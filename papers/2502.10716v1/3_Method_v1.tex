\section{Preliminaries}
% \subsection{Notation}
We first introduce the notations and basic concepts in the paper. We use calligraphic letters (i.e., $\mathcal{X}$) for spaces, upper case letters (i.e. $X$) for random variables, lower case letters (i.e. $x$) for their values and $\mathbb{P}$ for (observed) probability distributions.

\subsection{Problem Setup}
We consider a standard domain generalization setting with a potentially high-dimensional variable $X$ (e.g., an image), a label variable $Y$ and a discrete environment (or domain)
variable $E$ in the sample spaces $\mathcal{X}, \mathcal{Y}$, and $\mathcal{E}$, respectively. We consider the following family of distributions over the observed variables $(X,Y)$ given the environment $E=e \in \mathcal{E}$ where environment space under consideration $\mathcal{E} = \{ e \mid \mathbb{P}^e \in \mathcal{P} \}$:
\vspace{-2mm}
\begin{equation*}
    \mathcal{P}=\left \{\mathbb{P}^e(X, Y)=\int_{z_c}\int_{z_e}\mathbb{P}(X, Y, Z_c,Z_e, E=e)d z_c d z_e\right \}
\end{equation*}
The data generative process underlying every observed distribution $\mathbb{P}^e(X, Y)$ is characterized by a \textit{structural causal model} (SCM) over a tuple  $\left \langle V, U, \psi \right \rangle$ (See Figure~\ref{fig:graph}). The SCM consists of a set of \textit{endogenous} variables $V = \{X, Y, Z_c, Z_e, E\}$, a set of mutually independent \textit{exogenous} variables $U = \{U_x, U_y, U_{z_c}, U_{z_e}, U_e\}$ associated with each variable in $V$ and a set of deterministic equations $\psi = \{\psi_x, \psi_y, \psi_{z_c}, \psi_{z_e}, \psi_e\}$ representing the generative process for $V$. We note that this generative structure has been widely used and extended in several other studies, including \citep{chang2020invariant, mahajan2021domain, li2022invariant, zhang2023causal, lu2021invariant,liu2021heterogeneous}. 
% % \begin{figure}[h!]
% \begin{wrapfigure}{r!}{0.5\textwidth}
% \begin{centering}
% \subfloat{\centering{}\includegraphics[width=0.25\textwidth]{ICLR2025/Figures/SCM.png}}
% \par\end{centering}
% \caption{A directed acyclic graph (DAG) describing the causal relations among different factors producing data $X$ and label $Y$ in our SCM. Observed variables are shaded.\label{fig:graph}}
% \vspace{-2mm}
% \end{wrapfigure} 
% % \end{figure} 


The generative process begins with the sampling of an environmental variable $e$ from a prior distribution $\mathbb{P}(U_e)$\footnote{explicitly via the equation $e = \psi_e(u_e), u_e \sim P(U_e)$.}. We assume there exists a causal factor $z_c\in\mathcal{Z}_c$ determining the label $Y$ and a environmental feature $z_e\in\mathcal{Z}_e$ \textit{spuriously} correlated with $Y$. 
These two latent factors are 
generated from an environment $e$ via the mechanisms $z_c = \psi_{z_c}(e, u_{z_c})$ and $z_e = \psi_{z_e}(e, u_{z_e})$ with $u_{z_c} \sim \mathbb{P}(U_{z_c}), u_{z_e} \sim \mathbb{P}(U_{z_e})$. A data sample $x\in \mathcal{X}$ is generated from both the causal feature and the environmental feature i.e., $x = \psi_{x}(z_c, z_e, u_{x})$ with $u_x \sim \mathbb{P}(U_x)$. 


\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/SCM.png}
    \caption{A directed acyclic graph (DAG) describing the causal relations among different factors producing data $X$ and label $Y$ in our SCM. Observed variables are shaded.}
    \label{fig:graph}
\end{figure}

% \begin{figure}{h!}{0.5\linewidth}
% \vspace{-5mm}
% \begin{centering}
% \subfloat{\centering{}\includegraphics[width=0.28\textwidth]{Figures/SCM.png}}
% \par\end{centering}
% \caption{A directed acyclic graph (DAG) describing the causal relations among different factors producing data $X$ and label $Y$ in our SCM. \\Observed variables are shaded.\label{fig:graph}}
% \vspace{-5mm}
% \end{figure} 



% A causal factor $z_c$ determines a label $y\in \mathcal{Y}$ by $y = \psi_{y}(z_c, u_{y})$ with $u_y \sim \mathbb{P}(U_y)$. The presence of intrinsic noise $u_y$ is crucial, implying that a causal factor can be observed with different label categories. Therefore, without loss of generality, we can consider the label variable $y$ lying on the $C$-simplex, that is $\mathcal{Y} := \Delta_{C} = \left\{ \alpha\in\mathbb{R}^{C}:\norm\Vert\alpha\Vert_{1}=1\,\land\,\alpha\geq 0\right\}$. We note this is the point of difference of ours from prior works. As the ``soft" representation of $Y$ encapsulates the noise effect, the generative process for $y$ can now be written as $\psi_y(z_c) = y$.

Figure \ref{fig:graph} dictates that the joint distribution over $X$ and $Y$ can vary across domains resulting from the variations in the distributions of $Z_c$ and $Z_e$. Furthermore, \textit{both causal and environmental features are correlated with $Y$, but only $Z_c$ causally influences $Y$}.  However because $Y \perp\!\!\!\perp E | Z_c$, the conditional distribution of $Y$ given a specific $Z_c = z_c$ remains unchanged across different domains i.e., $\mathbb{P}^e(Y | Z_c = z_c) = \mathbb{P}^{e'}(Y | Z_c = z_c)$ $\forall e, e' \in \mathcal{E}$. For readability, we omit the superscript $e$ and denote this invariant conditional distribution as $\mathbb{P}(Y | Z_c = z_c)$. 

% Our data generation model accommodates the presence of intrinsic noise i.e., given sample $x = \psi_{x}(z_c, z_e, u_{x})$, the corresponding ground-truth label $\mathbb{P}(Y | Z_c = z_c)$ is distributed over $\mathcal{Y}$.

%\footnote{intrinsic noise in real-world settings: evidence from datasets such as TerraInc or DomainNet shows that it is impossible to achieve 100\% accuracy on the training partition when evaluated using one-hot labels. Meanwhile, \cite{zhang2021understanding} demonstrate that deep learning models trained with stochastic gradient descent can perfectly fit random labels, assuming the ground-truth label function is consistent.}


% Here, ${U_{z_c}, U_{z_e}, U_x, U_y}$ represent extraneous variables associated with each node, and they are mutually independent. 

% Given the joint distribution of these extraneous variable $\mathbb{P}(U_{z_c},U_{z_e},U_{x},U_{y})$ and set of functions $\{\psi_{z_c},\psi_{z_e},\psi_{x},\psi_{y}\}$ define a joint distribution $\mathbb{P}( X,Y,Z_c,Z_e,E)$. 

% We consider the following family of distributions over the observed variables $(X,Y)$ given the environment $E=e \in \mathcal{E}$ where  environment space under consideration $\mathcal{E} = \{ e \mid \mathbb{P}^e \in \mathcal{P} \}$:

% \begin{equation*}
%     \mathcal{P}=\left \{\mathbb{P}^e(X, Y)=\int_{z_c}\int_{z_e}\mathbb{P}(X, Y, Z_c,Z_e, E=e)d z_c d z_e\right \}
% \end{equation*}
% \[
% \mathcal{P}=\{\mathbb{P}^e(X, Y)=\int_{z_c}\int_{z_e}\mathbb{P}(X, Y, Z_c,Z_e, E=e)d z_c d z_e\}
% \]
%$\left \langle X,Y,E \right \rangle$: $\mathcal{P}=\{\mathbb{P}^e(X, Y)=\int_{z_c}\int_{z_e}\mathbb{P}(X, Y, Z_c,Z_e, E=e)d z_c d z_e\}$.

%The environment space under consideration is $\mathcal{E} = \{ e \mid \mathbb{P}^e \in \mathcal{P} \}$, acknowledging that any mixture of distributions from $\mathcal{E}$ also forms part of $\mathcal{E}$. 

% Figure \ref{fig:graph} dictates that the data input $X$ is the attributed to two factors: the characteristic feature $Z_c$ and the environmental feature $Z_e$. \textit{Both features are correlated with $Y$, but only $Z_c$ causally influences $Y$}.  Because $Y \perp\!\!\!\perp E | Z_c$, the conditional distribution of $Y$ given a specific $Z_c = z_c$ remains unchanged across different domains i.e., $\mathbb{P}^e(Y | Z_c = z_c) = \mathbb{P}^{e'}(Y | Z_c = z_c)$ $\forall e, e' \in \mathcal{E}$. For readability, we will omit the superscript $e$ and denote this conditional distribution as $\mathbb{P}(Y | Z_c = z_c)$. Different domains $\mathbb{P}^e$ may exhibit varied distributions for the causal features $Z_c$ and other features $Z_e$, implying that the marginal distribution of the observed sample $X$ and class label $Y$ can change across domains.
 
%Let us consider an example from the well-known Domain Generalization (DG) benchmark PACS dataset. Suppose $\mathcal{E}$ is a set of domains where images are collected. An image $x$ in this dataset, representing an object belonging to class $y$, is characterized by properties shared across domains $z_c$ (such as the object's shape) as well as other domain-specific latent features $z_e$ (such as texture, background, or color), which do not define the ``Y-ness" but may be spuriously correlated with $Y$. For instance, in a ``photograph domain", we often see \textit{horses on grass}, leading us to associate grass with the presence of a horse, whereas in ``sketch domain", grass is rarely observed with horses. Meanwhile, the causal factors like the shape a horse or parts of a horse remain consistent across all domains. 

\subsection{Revisiting Domain Generalization setting}

\textit{Notations}: 
We consider
the multi-class classification problem with the label set $\mathcal{Y}=\left[C\right]$,
where $C$ is the number of classes and $\left[C\right]:=\{1,\ldots,C\}$. Denote $\mathcal{Y}_{\Delta}:=\left\{ \alpha\in\mathbb{R}^{C}:\left \| \alpha \right \|_{1}=1\,\wedge\,\alpha\geq 0\right\} $
as the $C-$simplex, let $f:\mathcal{X}\mapsto\mathcal{Y}_{\Delta}$
be a hypothesis predicting a $C$-tuple $f\left(x\right)=\left[f\left(x\right)[i]\right]_{i=1}^{C}$,
whose element $f\left(x\right)[i]=p\left(y=i\mid x\right)$ 
is the probability to assign a data sample $x\sim\mathbb{P}$
to the class $i$ (i.e., $i\in\left\{ 1,...,C\right\} $). Let $l:\mathcal{Y}_{\Delta}\times\mathcal{Y}\mapsto\mathbb{R}$ be
a loss function, where $l\left(f\left(x\right),y\right)$ with
$\hat{f}\left(x\right)\in\mathcal{Y}_{\Delta}$ and $y\in\mathcal{Y}$
specifies the loss (e.g., cross-entropy, Hinge, L1, or L2 loss) to
assign a data sample $x$ to the class $y$ by the hypothesis $f$. The general 
loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}^e$ is:
\begin{equation}
\mathcal{L}\left(f,\mathbb{P}^e\right):=\mathbb{E}_{\left(x,y\right)\sim\mathbb{P}^e}\left[\ell\left(f\left(x\right),y\right)\right].   
\end{equation}

\color{black}

% \color{blue}
% Let $\mathcal{X}$ be a data space on which we endow a data distribution
% $\mathbb{P}$ with a corresponding density function $p(x)$. We consider
% the multi-class classification problem with the label set $\mathcal{Y}=\left[C\right]$,
% where $C$ is the number of classes and $\left[C\right]:=\{1,\ldots,C\}$.
% % represents the set of the first $C$ positive integer numbers.
% Denote
% $\mathcal{Y}_{\Delta}:=\left\{ \alpha\in\mathbb{R}^{C}:\left \| \alpha \right \|_{1}=1\,\wedge\,\alpha\geq 0\right\} $
% as the $C-$simplex, let $f:\mathcal{X}\mapsto\mathcal{Y}_{\Delta}$
% be a probabilistic labeling function returning a $C$-tuple $f\left(x\right)=\left[f\left(x,i\right)\right]_{i=1}^{C}$,
% whose element $f\left(x,i\right)=p\left(y=i\mid x\right)$ 
% % can be interpreted as 
% is the probability to assign a data sample $x\sim\mathbb{P}$
% to the class $i$ (i.e., $i\in\left\{ 1,...,C\right\} $). Moreover,
% a domain is denoted compactly as pair of data distribution and labeling
% function $\mathbb{D}:=\left(\mathbb{P},f\right)$. We note that
% given a data sample $x\sim\mathbb{P}$, its categorical label $y\in\mathcal{Y}$
% is sampled as $y\sim Cat\left(f\left(x\right)\right)$ which a categorical
% distribution over $f\left(x\right)\in\mathcal{Y}_{\Delta}$. 

% Let $l:\mathcal{Y}_{\Delta}\times\mathcal{Y}\mapsto\mathbb{R}$ be
% a loss function, where $l\left(f\left(x\right),y\right)$ with
% $f\left(x\right)\in\mathcal{Y}_{\Delta}$ and $y\in\mathcal{Y}$
% specifies the loss (e.g., cross-entropy, Hinge, L1, or L2 loss) to
% assign a data sample $x$ to the class $y$ by the hypothesis $f$.
% Moreover, given a prediction probability $\hat{f}\left(x\right)$
% w.r.t. the ground-truth prediction $f\left(x\right)$, we define the
% loss $\ell\left(\hat{f}\left(x\right),f\left(x\right)\right)=\mathbb{E}_{y\sim f\left(x\right)}\left[l\left(\hat{f}\left(x\right),y\right)\right]=\sum_{y=1}^{C}l\left(\hat{f}\left(x\right),y\right)f\left(x,y\right)$.
% We further define the general loss caused by using a classifier $\hat{f}:\mathcal{X}\mapsto\mathcal{Y}_{\Delta}$
% to predict $\mathbb{D}\equiv\left(\mathbb{P},f\right)$ as
% \begin{equation}
% \mathcal{L}\left(\hat{f},f,\mathbb{P}\right)=\mathcal{L}\left(\hat{f},\mathbb{D}\right):=\mathbb{E}_{x\sim\mathbb{P}}\left[\ell\left(\hat{f}(x),f(x)\right)\right].
% \end{equation}

\textit{Domain Generalization}: Given a set of training domains $\mathcal{E}_{tr}=\{e_1,...,e_K\} \subset \mathcal{E}$, the objective of DG is to exploit the `commonalities' present in the training domains to improve generalization to any domain of the population $e\in \mathcal{E}$. For supervised classification, the task is equivalent to seeking the set of \textbf{global optimal hypotheses} $\mathcal{F}^{*}$ 
%\footnote{under the assumptions of the data generation process, the set of global optimal hypotheses defined in Eq.~(\ref{eq:optimal}) is equivalent to the one defined by the worst-case domain: $\mathcal{F}^{*} = \underset{f'\in \mathcal{F}}{\text{argmin}}\sup_{{e}\in \mathcal{E}}\loss{f',\mathbb{P}^{e}}$ in the Appendix~\ref{apd:worstcase}.}
where every $f\in \mathcal{F}^*$ is locally optimal for every domain:
    \begin{equation}
    \mathcal{F}^{*} := \bigcap_{{e}\in \mathcal{E}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,\mathbb{P}^{e}}
    \label{eq:optimal}
\end{equation}
% where the hypothesis $f:\mathcal{X}\rightarrow\mathcal{Y}_{\Delta}$ is a map from the data space $\mathcal{X}$ to the the $C$-simplex label space $\mathcal{Y}_{\Delta}:=\left\{ \alpha\in\mathbb{R}^{\left | \mathcal{Y} \right |}:\norm\Vert\alpha\Vert_{1}=1\,\land\,\alpha\geq 0\right\}$.
% Let $\ell\left(f\left(x\right),y\right)$ be the loss incurred by using this hypothesis to predict $x\in\mathcal{X}$ with its true label $y\in \mathcal{Y}$. The general 
% loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}^e$ is: 
% \begin{equation}
% \mathcal{L}\left(f,\mathbb{P}^e\right):=\mathbb{E}_{\left(x,y\right)\sim\mathbb{P}^e}\left[\ell\left(f\left(x\right),y\right)\right].    
% \end{equation}

% \longvt{Let $\ell\left(f\left(x\right),y\right)$ be the loss incurred by using this hypothesis to predict $x\in\mathcal{X}$ with its true label $\mathbb{P}(Y\mid x)\in \mathcal{Y}_{\Delta}$. The general 
% loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}^e$ is:

% \begin{equation}
% \mathcal{L}\left(f,\mathbb{P}^e\right):=\mathbb{E}_{\left(x\right)\sim\mathbb{P}^e}\left[\ell\left(f\left(x\right),\mathbb{P}(Y\mid x)\right)\right].    
% \end{equation}
% }

\color{black}

We here examine the widely used {\it composite hypothesis} $f = h \circ g \in \mathcal{F}$, where $g : \mathcal{X} \rightarrow \mathcal{Z}$ belongs to a set of representation functions $\mathcal{G}$, mapping the data space $\mathcal{X}$ to a latent space $\mathcal{Z}$, and $h : \mathcal{Z} \rightarrow \mathcal{Y}_{\Delta}$ is the classifier in the space $\mathcal{H}$. For simplicity, we assume \(\mathcal{Z}_c, \mathcal{Z}_e \subseteq \mathcal{Z}\) in the following analyses.


\textbf{Presumption}. While our work considers limited 
and finite domains, we follow recent theoretical works \citep{wang2022provable, rosenfeld2020risks, kamath2021does, ahuja2021invariance, chen2022iterative} assuming the infinite data setting for every training environment. This assumption distinguishes DG literature from traditional generalization analysis (e.g., PAC-Bayes framework) that focuses on in-distribution generalization where the testing data are drawn from the same distribution. %\color{blue} Additionally, following prior works \cite{xu2017information, shwartz2018representation, kawaguchi2023does}, we consider the function spaces such as $\mathcal{F},\mathcal{G}$ and $\mathcal{H}$ to be a finite set of hypotheses. This assumption aligns naturally with the constraints of digital computers, which operate using finite-precision arithmetic (e.g., floating-point numbers).
\color{black}

% \longvt{ 

% We build on recent theoretical works \citep{wang2022provable, rosenfeld2020risks, kamath2021does, ahuja2021invariance, chen2022iterative}, which consider the ideal scenario of having infinite data for each training environment, thereby eliminating finite-sample effects. This approach allows us to concentrate on studying how domain generalization algorithms depend on different environments, distinguishing domain generalization from traditional generalization analysis (i.e., PAC framework), which focuses on a model's performance when trained on a given dataset and tested on unseen data drawn from the same distribution (i.e., the i.i.d. assumption).

% }


\section{Conditions for Generalization}\label{sec:main_conds}

In this section, we present the key assumptions about the data setting along with the necessary and sufficient conditions on the hypothesis and representation functions for achieving generalization defined in Eq. (\ref{eq:optimal}) (See Table \ref{tab:conditions} for summary). These conditions are critical to our analysis, where we first reveal that the existing DG methods aim to satisfy one or several of these necessary and sufficient conditions to achieve generalization. Following this section, we theoretically assess whether a method works effectively by to what extent the necessary conditions are met. 
% \begin{table}[h!]
% \caption{Summary of Conditions for Generalization (defined in Eq. (\ref{eq:optimal})  }
% \begin{centering}
% \resizebox{0.7\width}{!}{ %
% \begin{tabular}{llll}
% \toprule
% \textbf{Condition} & \textbf{Type}  & \textbf{Verifiable?} & \textbf{Corresponding DG approach}\\
% \midrule
% Label-identifiability (\ref{as:label_idf}) &Assumption & No (Data dependency)\\
% \midrule
% Causal support (\ref{as:sufficient_causal_support}) &  Assumption & No (Data dependency) \\
% \midrule
% Sufficient and diverse domains (\ref{thm:convergence}) & Sufficient & No & Augmentation\\
% \midrule
% Invariant Representation Function (\ref{thm:invariant_correlation}) &  Sufficient & No & Representation alignment\\
% \midrule
% Optimal hypothesis for training domains (\ref{def:joint_optimal})&Necessary & Yes & Invariant prediction\\
% \midrule
% \textbf{Sufficient Representation Function} (\ref{def:sufficient}) & \textbf{Necessary} & \textbf{constraint \Vy{No?}}& \textbf{Extra constraint to any approaches}\\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:conditions}
% \end{table}


\subsection{Assumptions on Data Setting}


We first establish crucial assumptions for the feasibility of generalization as described in Eq (\ref{eq:optimal}). These assumptions are essential for understanding the conditions under which generalization can be achieved. We also demonstrate that the first assumption is a necessary condition for the existence of global optimal hypotheses (Appendix \ref{thm:existence_apd}). 


%We note that the following discussion may not be entirely new and exist in some other forms dispersing across the literature. We here make the first contribution to unifying them into a systematic and comprehensive analysis. 

\begin{assumption} (Label-identifiability). We assume that for any pair $z_c, z^{'}_c\in \mathcal{Z}_c$,  $\mathbb{P}(Y|Z_c=z_c) = \mathbb{P}(Y|Z_c=z^{'}_c) \text{ if } \psi_x(z_c,z_e,u_x)=\psi_x(z_c',z'_e,u'_x)$ for some $z_e, z'_e, u_x, u'_x$
\label{as:label_idf}.
\end{assumption}

The causal graph indicates that $Y$ is influenced by $z_c$, making $Y$ identifiable over the distribution $\mathbb{P}(Z_c)$. This assumption implies that different causal factors $z_c$ and $z^{'}_c$ cannot yield the same $x$, unless the condition $\mathbb{P}(Y|Z_c=z_c) = \mathbb{P}(Y|Z_c=z^{'}_c)$ holds, or  the distribution $\mathbb{P}(Y\mid x)$ is stable. This assumption also can be view as covariate shift setting in OOD 
\citep{shimodaira2000improving}.
%\cite{shimodaira2000improving,bickel2009discriminative}. 

%\begin{theorem} $\mathcal{F}^*\neq \emptyset$ if and only if Assumption~\ref{as:label_idf} is hold. (Proof in Appendix \ref{thm:existence_apd}).  \label{thm:existence} \end{theorem}


\begin{assumption} (Causal support). We assume that the union of the support of \textit{causal} factors across training domains covers the entire causal factor space $\mathcal{Z}_c$: $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_c \right )\}=\mathcal{Z}_c$ where $\text{supp}(\cdot)$ specifies the support set of a distribution. 
\label{as:sufficient_causal_support}
\end{assumption}

% \begin{assumption} (Spurious support). We assume that the union of the support of \textit{spurious} factors across training domains covers the entire causal factor space $\mathcal{Z}_e$: $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_e \right )\}=\mathcal{Z}_e$ where $\text{supp}(\cdot)$ specifies the support set of a distribution. 
% \label{as:sufficient_spurious_support}
% \end{assumption}

This assumption holds significance in DG theories \citep{johansson2019support, ruan2021optimal, li2022sparse}, especially when we avoid imposing strict constraints on the target functions. Particularly, \citep{ahuja2021invariance} showed that without the support overlap assumption on the causal features, OOD generalization is impossible for such a simple model as linear classification. Meanwhile, for more complicated tasks, deep neural networks are typically employed, which, when trained via gradient descent however, cannot effectively approximate a broad spectrum of nonlinear functions beyond their support range \citep{xu2020neural}. It is worth noting that causal support overlap does not imply that the distribution over the causal features is held unchanged.





%It is worth noting that this assumption does not trivialize the problem, as identifying the causal factor is extremely challenging, if not  impossible, with finite samples.


% \Vy{While previous works assume sufficient causal support across \textit{all} domains \cite{mitrovic2020representation, ruan2021optimal, zhang2023causal}, 
% Assumption~\ref{as:sufficient_causal_support} is a more relaxed condition, which only requires sufficient causal support from a mixture of training domain distributions.}

% \Vy{The following theorem asserts that under Assumption~\ref{as:sufficient_causal_support}, it is feasible to learn a global optimal hypothesis from finite training domain data:}

\subsection{Conditions on Hypothesis}
% \subsection{Optimal hypothesis for training domains (NC)}
By definition, the global optimal hypothesis $f\in \mathcal{F}^*$ must also be the optimal solution for all training domains in $\mathcal{E}_{tr}$ which is defined as 

\begin{definition} \textit{(Optimal hypothesis for training domains) Given  $\mathcal{F}_{\mathbb{P}^e}=\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,\mathbb{P}^{e}}$ is set of optimal hypothesis for $\mathbb{P}^{e}$, the optimal hypothesis for all training domains $f\in\mathcal{F}_{\mathcal{E}_{tr}} = \bigcap_{{e}\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^e}$.}
\label{def:joint_optimal}
\vspace{-2mm}
\end{definition}

It is evident that a hypothesis being optimal for all training domains is a \textit{necessary condition} for achieving a global optimal hypothesis (if $f\in\mathcal{F}^*$ then  $f\in\mathcal{F}_{\mathcal{E}_{tr}}$). Since this condition is necessary, the reverse does not hold i.e., $f\in\mathcal{F}_{\mathcal{E}_{tr}}$ does not guarantee $f\in\mathcal{F}^*$. However, this condition remains essential for our theoretical analysis as it is the only condition that can be verified during training. We next present two sufficient conditions that lay the foundation for understanding DG algorithms. 

\begin{theorem} Under Assumption.\ref{as:label_idf} and \ref{as:sufficient_causal_support}, given a hypothesis $f=h\circ g$:

if $f\in \mathcal{F}_{\mathcal{E}_{tr}}$ and one of the following conditions holds:
\begin{enumerate}
    \item $g$ is an invariant representation function, i.e., $g\in \mathcal{G}_c$
    \item the union of the support of \textit{spurious} factors across training domains covers the entire causal factor space $\mathcal{Z}_e$: $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_e \right )\}=\mathcal{Z}_e$ where $\text{supp}(\cdot)$ specifies the support set of a distribution. 
    \item $f$ is also an optimal hypothesis on all augmented domains, i.e., $f\in \bigcap_{{e}\in \mathcal{E}_{tr}, T\in \mathcal{T}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,T\#\mathbb{P}^{e}}$
\end{enumerate}

Then $f\in \mathcal{F}^*$ (Proof in Appendix \ref{thm:single_generalization_apd})
\label{thm:single_generalization}
\end{theorem}



\subsection{Conditions on Training Domains}
We are thus motivated to study the properties of the training domains $\mathcal{E}_{tr}$ so that it is feasible to capture the global optimal hypothesis from these domains.

% \begin{theorem}
%  \label{thm:convergence} (Sufficient and diverse domains)
% Given sequence of training domains $\mathcal{E}_{tr}=\{e_1,...,e_K\} \subset \mathcal{E}$, denote $\Funion^{k}=\bigcap_{i=1}^{k}\mathcal{F}_{ \mathbb{P}^{e_i}}$. We consider $\mathcal{E}_{tr}$ to be \textbf{diverse} if for domain $e_k$, there exists at least one sample $x=\psi_x(z_c,z_e,u_x)\in\text{supp}\{\mathbb{P}^{e_k} \left (X \right )\}$ such that $\exists f  \in \Funion^{k-1} :f(x)\neq \mathbb{P}(Y\mid z_c)$. Given a set of diverse domains $\mathcal{E}_{tr}$, we have:
% \begin{equation*}
%  \Funion^1\supset  \Funion^2 \supset... \supset  \Funion^K   
% \end{equation*}
% and the number of training domains $\mathcal{E}_{tr}$ is sufficiently large:
% \begin{equation*}
% \lim_{\mathcal{E}_{tr}\rightarrow \mathcal{E}}\Funion^{ \left| \mathcal{E}_{tr} \right|} \rightarrow \mathcal{F}^*.
% \end{equation*}

% (Proof in Appendix~\ref{thm:convergence_apd})
% \end{theorem}

% \begin{theorem} (Sufficient and diverse domains)
%  \label{thm:convergence} 
% If Assumption~\ref{as:sufficient_causal_support} and Assumption~\ref{as:sufficient_spurious_support} are hold, then $\mathcal{F}_{\mathcal{E}_{tr}}\in \mathcal{F}^*$
% (Proof in Appendix~\ref{thm:convergence_apd})
% \end{theorem}


\color{blue}
Theorem with Causal-preserved transformation: show that $\mathcal{E} \in \{T(\mathcal{E}_{tr})\}_{T\in \mathcal{T}}$
\color{black}

Theorem~\ref{thm:convergence} dictates that having a sufficiently large and diverse set of training domains is a \textit{sufficient condition} for attaining the global optimal hypothesis. However, our theorem does not explicitly specify \textit{how large} the number of training domains must be. For a more in-depth study on this aspect, we refer readers to \citep{rosenfeld2020risks, arjovsky2020irm}.
In this work, we focus on the ``diversity" property since it is generally difficult to determine how many domains is enough but we can always attempt to make them diverse, as done by the family of augmentation-based DG algorithms (refer to Section~\ref{sec:augmentation}). These algorithms, such as \citep{mitrovic2020representation, wang2022out}, create augmented data that preserve the causal factor $z_c$ while varying the environment factor $z_e$ to encourage the classifier to focus on exploiting the causal factor $z_c$. It is worth noting that while these methods aim to achieve \textit{sufficient condition}~\ref{thm:convergence}, the condition is, in fact, theoretically non-verifiable without knowledge of the target domains.


\subsection{Conditions on Representation Function}

\begin{proposition} (Invariant Representation Function)
Under Assumption.\ref{as:label_idf}, there exists a set of deterministic representation function $(\mathcal{G}_c\neq \emptyset)\in \mathcal{G}$ such that for any $g\in \mathcal{G}_c$, $\mathbb{P}(Y\mid g(x)) = \mathbb{P}(Y\mid z_c)$ and $g(x)=g(x')$ holds true for all $\{(x,x',z_c)\mid  x= \psi_x(z_c, z_e, u_x), x'= \psi_x(z_c, z^{'}_e, u^{'}_x) \text{ for all }z_e,z^{'}_e, u_x, u^{'}_x\}$ (Proof in
Appendix \ref{thm:invariant_correlation_apd}).   
\label{thm:invariant_correlation}
\end{proposition}

\begin{definition} (Causal Invariant Transformation (CIT)). Denote $\mathcal{T}$ is set of all a causal invariant transformation  $T(\cdot)$ such that for any $T\in \mathcal{T}$: $(g\circ T)(\cdot)=g(\cdot)$.
\label{def:causal_transformation}
\vspace{-2mm}
\end{definition}


Assumption \ref{as:label_idf} gives rise to a family of invariant representation function $\mathcal{G}_c$, as stated in Proposition \ref{thm:invariant_correlation}. 
%\footnote{In such works as \citep{ruan2021optimal, zhang2023causal}, the existence of invariant representation function is directly assumed, without stating the conditions under which it holds. Assumption \ref{as:label_idf} thus sheds light on this.}. 
This discovery points to the presence of global optimal hypotheses i.e., $\mathcal{F}^*\neq \emptyset$. Furthermore, in the subsequent theorem, we demonstrate that with an understanding of the invariant correlation $g\in \mathcal{G}_c$, it is possible to learn these global optimal hypotheses from any training dataset $\mathbb{P}^e \sim \mathcal{P}$, given it exhibits \textit{sufficient causal support} (e.g., a mixture of training domains under Assumption~\ref{as:sufficient_causal_support}, where  $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_c \right )\}=\mathcal{Z}_c$).



\begin{theorem} Denote the set of \textbf{domain optimal hypotheses} of $\mathbb{P}^e$ induced by $g\in \mathcal{G}$: 
    \begin{equation*}
        \mathcal{F}_{\mathbb{P}^e,g}=\left \{h\circ g \mid h\in\underset{h'\in \mathcal{H}}{\rm{argmin }} \mathcal{L}\left ( h'\circ g, {\mathbb{P}^{e}} \right )  \right \}.
    \end{equation*} 
If $\text{supp}\{\mathbb{P}^e(Z_c)\}=\mathcal{Z}_c$ (Assumption~\ref{as:sufficient_causal_support}) and $g\in \mathcal{G}_c$, then $\mathcal{F}_{\mathbb{P}^e,g}  \subseteq \mathcal{F}^{*}$. (Proof in Appendix \ref{thm:single_generalization_apd})
\label{thm:single_generalization}
\end{theorem}

Theorem \ref{thm:single_generalization} demonstrates that under Assumption~\ref{as:label_idf} and Assumption~\ref{as:sufficient_causal_support}, $g\in \mathcal{G}_c$ is the \textit{sufficient condition} $f^*\in\mathcal{F}^*$ for learning global optimal hypothesis from finite number of training domains. 
% \textit{Necessary condition}~\ref{def:joint_optimal} is the objective of the invariant prediction strategy (refer to Section~\ref{sec:invariant_prediction})  in DG algorithms, as proposed by namely \citep{arjovsky2020irm,krueger2021out,rosenfeld2020risks}
This condition is what the family of \textit{representation alignment} and \textit{invariant prediction} methods strive for (refer to Section~\ref{sec:representation_alignment}). However, achieving $g\in \mathcal{G}_c$ is often infeasible in practice, because it requires the knowledge of \textit{all} domains. We therefore shift the attention to studying a new class of representation function that serves as a \textit{necessary condition} for global optimal hypothesis, which is defined as follows:  
% However, this concept later serves as a pivotal reference in defining a \textit{necessary condition} for $g$ which latter in the next section, being one of two key condition to enable us analyse the generalization ability of DG algorithm using one training domains.

\begin{definition}\textit{ (Sufficient Representation Function) A set of representation functions $\mathcal{G}_s\in \mathcal{G}$ is considered as sufficient representation functions if for any $g\in\mathcal{G}_s$, there exists a function $\phi: \mathcal{Z}\rightarrow \mathcal{Z}$ such that
$(\phi\circ g) \in \mathcal{G}_c$ (i.e., given $g\in \mathcal{G}_s$, $g(x)$ retains all information about causal feature of $x$). }
\label{def:sufficient}
\end{definition}
\color{black}
The following theorem shows that $g \in \mathcal{G}_s$ is necessary for achieving the global optimal hypothesis. 

\begin{theorem}

%$\mathcal{H}_{\mathbb{P}^e,g}=\underset{h\in \mathcal{H}}{\text{argmin }}\mathbb{E}_{\mathbb{P}^e}\left [ \mathcal{L}\left ( h\circ g(x),y \right ) \right ]$  to 

Considering the training domains $\mathbb{P}^e$ and representation function $g$, let $\mathcal{H}_{\mathbb{P}^e,g}=\underset{h\in \mathcal{H}}{\text{argmin }} \mathcal{L}\left ( h\circ g,\mathbb{P}^e \right )$ represent the set of optimal classifiers on $g\#\mathbb{P}^e$ (the push-forward distribution by applying $g$ on $\mathbb{P}^e$), \textbf{the best generalization classifier} from $\mathbb{P}^e$ to $\mathcal{P}$ is defined as 
\begin{equation}
\mathcal{F}^{B}_{\mathbb{P}^e,g}=\left \{ h\circ g \mid h \in \bigcap_{e'\in \mathcal{E}} \underset{ h'\in\mathcal{H}_{\mathbb{P}^e,g}}{\rm{argmin }}\mathcal{L}\left (  h'\circ g, \mathbb{P}^{e'} \right ) \right \}
\end{equation} 
Give representation function $g: \mathcal{X}\rightarrow \mathcal{Z}$ then $\forall \mathbb{P}^e\sim \mathcal{P}$ we have
$\left(\mathcal{F}^B_{ \mathbb{P}^e,g}\neq \emptyset \right) \subseteq \mathcal{F}^{*}$ if and only if $g\in \mathcal{G}_s$. (Proof in Appendix~\ref{thm:nacessary_apd})
\label{thm:nacessary}
\end{theorem}


\begin{theorem}
$\exist h: h\circ g\in \mathcal{F}^*$ if and only if $g\in \mathcal{G}_s$. (Proof in Appendix~\ref{thm:nacessary_apd})
\label{thm:nacessary}
\end{theorem}

This theorem demonstrates that if $g$ is not a sufficient representation i.e., $g\notin \mathcal{G}_s$, the best attainable hypothesis is surely not optimal i.e., $\mathcal{F}^B_{ \mathbb{P}^e,g} \cap \mathcal{F}^{*} = \emptyset$, implying that it is impossible to find any classifier $h$ such that $h\circ g\in \mathcal{F}^*$. In other words, $g\in \mathcal{G}_s$ is \textit{necessary} for $f\in \mathcal{F}^*$. This property plays a crucial role in understanding the generalization ability of DG algorithms.

\textbf{Remark:} An invariant representation function aims to extract representations that remain consistent across environments, ensuring that all optimal classifiers trained on top of these representations achieve optimal solutions. In contrast, a sufficient representation function captures representations containing causal information rather than strictly invariant features. This introduces the challenge of learning a model that avoids reliance on latent spurious features.

\subsection{Relationship between condition}
\subsection{Can DG algorithms generalize?}
\label{sec:efficacy_DG}

The majority of DG algorithms strive to satisfy one of the sufficient conditions to achieve generalization. However, the sufficient conditions are nearly non-verifiable when training domains are limited. 
% Corollary~\ref{thm:bad_domain_exist} indicates that if a hypothesis \(f = h \circ g\) satisfies all necessary conditions but fails to meet any sufficient condition, it may still perform poorly in many target domains. That is, there might exist $f\in \bigcap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{g, \mathbb{P}^e}$ but $f\notin \mathcal{F}^*$ and if $f\notin \mathcal{F}^*$, there are many "bad" domains $P^T$ for which loss $\loss{f,P^T}$ is arbitrary large (recall that $\mathcal{F}^*$ is set of globally optimal hypotheses).
% \begin{corollary}
% Given $g\in \mathcal{G}_s$, there exists $f = h\circ g\in \bigcap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{g, \mathbb{P}^e}$ such that for any $0\leq\delta \leq 1$, there are many undesirable target domains $\mathbb{P}^T \sim \mathcal{P}$ such that:
% \begin{align*}
%    \mathbb{E}_{(x,y)\sim\mathbb{P}^T} \left [ f(x) \neq f^*(x)  \right ]  \geq 1-\delta.
% \end{align*}  with  $f^* \in \mathcal{F}^*$.\footnote{This coincides with the ``no free lunch'' conclusion for learning representations in DG \citep{ruan2021optimal}.}
%     \label{thm:bad_domain_exist}
% (Proof in Appendix~\ref{thm:bad_domain_exist_apd})
% \end{corollary}
A natural question is to what extent DG algorithms are generalizable when the sufficient conditions cannot be guaranteed. To answer this question, we illustrate the relationship between sufficient and necessary conditions using Venn diagrams of hypothesis spaces corresponding to each condition, as shown in Figure \ref{fig:space}. 

By definition, $\mathcal{F}_{\mathcal{E}_{tr}}$ correspond to necessary condition~\ref{def:joint_optimal} presented in green area. We define $\mathcal{F}^S_{\mathcal{E}_{tr}}$ as set of optimal hypothesis for training domains and have sufficient representation function i.e., $\mathcal{F}^S_{\mathcal{E}_{tr}}$ 

% Additionally, for clarity in the following discussion, we use the notation $\mathcal{F}^*, \mathcal{F}_{\mathcal{E}_{tr}}$ as to represent the feasible global optimal hypothesis space and the joint optimal hypothesis space for training domains of algorithm $A$, respectively.

By definition of global optimal hypothesis, we have 
$\mathcal{F}^*\subseteq \mathcal{F}_{\mathcal{E}_{tr}}$. 

We further define $\mathcal{F}^S_{\mathcal{E}_{tr}}$ as optimal training and sufficient, then $\mathcal{F}^S_{\mathcal{E}_{tr}}\subseteq \mathcal{F}_{\mathcal{E}_{tr}}$

By theorem~\ref{thm:nacessary},  $\mathcal{F}^*\subseteq \mathcal{F}^S_{\mathcal{E}_{tr}}$

From theorem $\mathcal{F}^*\subseteq \mathcal{F}^S_{\mathcal{E}_{tr}}\subseteq \mathcal{F}_{\mathcal{E}_{tr}}$

By theorem~\ref{thm:}, Sufficient play as contraint to $\mathcal{F}_{\mathcal{E}_{tr}}$


% Given that a hypothesis $f\in \mathcal{F}^*$ must also be the optimal solution for all training domains in $\mathcal{E}_{tr}$, we deduce that  $\mathcal{F}^*\subseteq \mathcal{F}_{\mathbb{P}^e}$, $\forall e\in \mathcal{E}_{train}$, consequently, $\mathcal{F}^*\subseteq \mathcal{F}_{\mathcal{E}_{tr}}$. 

Denote  $\mathcal{F}_{\mathbb{P}^e}=\bigcup_{g\in \mathcal{G}_s}\mathcal{F}_{\mathbb{P}^e,g}$ as the set of hypotheses induced by an algorithm $A$ that satisfies both necessary conditions i.e., optimal for domain $e$ (\textit{Condition}~\ref{def:joint_optimal}) and $g \in \mathcal{G}_s$ is a \textit{sufficient representation} (\textit{Condition}~\ref{def:sufficient}).



\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/space_with_text3.png}
    \caption{The circles (\textcolor{brown}{brown}, \textcolor{blue}{blue}, \textcolor{red}{red}) denote the spaces of domain-optimal hypotheses \textcolor{brown}{$\mathcal{F}_{\mathbb{P}^{e_1}}$}, \textcolor{blue}{$\mathcal{F}_{\mathbb{P}^{e_2}}$}, \textcolor{red}{$\mathcal{F}_{\mathbb{P}^{e_3}}$} of training domains $e_1, e_2, e_3\in \mathcal{E}_{tr}$ respectively. The grey area indicates the space of global optimal hypotheses $\mathcal{F}^{*}$ while the green area represents the joint space of domain-optimal hypotheses $\mathcal{F}_{\mathcal{E}_{tr}}$.}
\vspace{-2mm}
\label{fig:space}
\end{figure}


% \begin{figure}[h!]
% \vspace{-2mm}
% \begin{centering}
% \subfloat{\centering{}\includegraphics[width=0.7\linewidth]{Figures/space_with_text3.png}}
% \par\end{centering}
% \caption{The circles (\textcolor{brown}{brown}, \textcolor{blue}{blue}, \textcolor{red}{red}) denote the spaces of domain-optimal hypotheses \textcolor{brown}{$\mathcal{F}_{\mathbb{P}^{e_1}}$}, \textcolor{blue}{$\mathcal{F}_{\mathbb{P}^{e_2}}$}, \textcolor{red}{$\mathcal{F}_{\mathbb{P}^{e_3}}$} of training domains $e_1, e_2, e_3$ respectively. The grey area indicates the space of global optimal hypotheses $\mathcal{F}^{*}$. An algorithm $A$ satisfying both conditions ~\ref{def:joint_optimal} and  \ref{def:sufficient} induces a non-empty grey area that lies within the green area - the joint space of domain-optimal hypotheses $\bigcap_{i\in \{1,2,3\}}\mathcal{F}_{ \mathbb{P}^{e_i}}$.}
% \vspace{-2mm}
% \label{fig:space}
% \end{figure} 

The Venn diagram reveals that any algorithm achieving \textit{Condition}~\ref{def:joint_optimal} can guarantee that its corresponding global optimal set is bounded by the feasible hypothesis set induced by the algorithm; in visual terms, the green area always cover the grey area. Apparently, "bad" domains occur for any learned hypothesis $f$ that falls outside of the grey area. Therefore, the more the green area collapses to the grey area, the higher chance generalization can be attained.

At a high level, a strategy to encourage both areas coincide is thus by reducing the size of the green area with additional constraints that can be met by all hypotheses in the grey area. Especially when either of the sufficient conditions is met, according to Theorem~\ref{thm:convergence}, the green area can always converge to the grey area under perfect optimization. Existing DG approaches are essentially seeking to reduce the green area. The \textit{augmentation}-based methods strive to achieve \textit{sufficient and diverse domains} (a sufficient condition) by generating augmented domains to challenge the hypothesis. Meanwhile, \textit{Representation alignment} or \textit{invariant prediction} strategies implicitly narrow down the green space by constraining the representation function space $\mathcal{G}$. 

While attempting to restrict the set of feasible solutions, a DG algorithm, with its extra constraints, may as well reduce the grey area, by restricting the global optimal set to only solutions that also meet the constraints. With arbitrary constraints, there is a possibility that the grey area shrinks to null. 
Interestingly, a key insight from Theorem~\ref{thm:nacessary} is that, under the \textit{Condition}~\ref{def:joint_optimal}, as long as the solution of an algorithm fulfills the \textit{sufficient representation function} constraint (\textit{Condition}~\ref{def:sufficient}), there exists a non-empty $\mathcal{F}^{*} \subseteq \mathcal{F}_{\mathcal{E}_{tr}}$; otherwise $\mathcal{F}^{*} = \emptyset$. In fact, that an algorithm meets a sufficient condition implies the satisfaction of \textit{Condition}~\ref{def:sufficient} by default.

\textbf{In summary}, an algorithm should be effectively designed to minimize the space $\mathcal{F}_{\mathcal{E}_{tr}}$ while maintaining the coverage of $\mathcal{F}^{*}$. \textit{Condition}~\ref{def:joint_optimal} ensures the \textit{green} area is non-empty i.e., $\mathcal{F}_{\mathcal{E}_{tr}} \ne \emptyset$ while  \textit{Condition}~\ref{def:sufficient} ensures the \textit{grey} one is non-empty i.e., $\mathcal{F}^{*} \ne \emptyset$. Satisfying both conditions further guarantees the existence of the global optimal solutions in $\mathcal{F}^{*} \subseteq \mathcal{F}_{\mathcal{E}_{tr}}$. In contrast, if both conditions are violated, the algorithm has zero chance of achieving generalization. Despite its significance, existing DG algorithms tends to overlook \textit{Condition}~\ref{def:sufficient}. From finite training domains, they thus cannot guarantee the possibility of searching for global optimal hypotheses. 



\section{Domain Generalization: A view of Necessity and Sufficiency}


% \subsection{Reviewing DG Literature} 
 \label{sec:discussion_DG}

It is clear that in order to be considered a globally optimal candidate, a learned hypothesis from finite domains must meet the necessary conditions ~\ref{def:joint_optimal} and  ~\ref{def:sufficient}. Following the previous analysis, we here review the popular classes of DG methods and discuss when they meet or fail these conditions.  

\subsection{Representation Alignment.} These approaches aim to learn a representation function $g$ for data $X$ such that $g(X)$ is invariant or consistent across different domains. Key studies like \citep{long2017conditional, ganin2016domain, li2018domain, nguyen2021domain, shen2018wasserstein, xie2017controllable, ilse2020diva} focus on learning such domain-invariant representations by reducing the divergence between latent marginal distributions $\mathbb{E}[g(X) | E]$ where $E$ represents a domain environment. Other methods seek to align the conditional distributions $\mathbb{E}[g(X) | Y=y, E]$ across domains as seen in \citep{li2018domain_b, tachet2020domain}. However, achieving true invariance is challenging and can be excessively limiting. In some instances, improved alignment of features leads to greater joint errors \citep{johansson2019support, zhao2019learning, phung2021learning, le2021lamda}.


\begin{theorem}
\label{theorem:single_tradeoff} \citep{zhao2019learning, phung2021learning, le2021lamda} Distance between two marginal distribution $\mathbb{P}^{e}_\mathcal{Y}$ and $\mathbb{P}^{e'}_\mathcal{Y}$ can be upper-bounded: 
\vspace{-1mm}
\begin{equation*}
\begin{aligned}
D\left(\mathbb{P}_{\mathcal{Y}}^{e},\mathbb{P}_{\mathcal{Y}}^{e'}\right) \leq 
D\left ( g_{\#}\mathbb{P}^{e},g_{\#}\mathbb{P}^{e'} \right )
+\mathcal{L}\left ( f,\mathbb{P}^{e} \right )
+
\mathcal{L}\left ( f,\mathbb{P}^{e'} \right )
\end{aligned}
\end{equation*}
where $g_{\#}\mathbb{P}(X)$ denotes representation distribution on  representation space $\mathcal{Z}$ induce by applying encoder with $g: \mathcal{X} \mapsto \mathcal{Z}$ on data distribution $\mathbb{P}$, $D$ can be $\mathcal{H}$-divergence \citep{zhao2019learning}, Hellinger distance \citep{phung2021learning} or Wasserstein distance \citep{le2021lamda} (Appendix~\ref{apd:tradeoff}).
\end{theorem}

Theorem~\ref{theorem:single_tradeoff} suggests that a substantial discrepancy in the label marginal distribution $D\left(\mathbb{P}_{\mathcal{Y}}^{e},\mathbb{P}_{\mathcal{Y}}^{e'}\right)$ across training domains may result in strong \textit{representation alignment} $D\left ( g_{\#}\mathbb{P}^{e},g_{\#}\mathbb{P}^{e'} \right )$ while increasing \textit{domain-losses} $\left ( \mathcal{L}\left ( f,\mathbb{P}^{e} \right ) + \mathcal{L}\left ( f,\mathbb{P}^{e'}\right )\right )$. It's important to recognize that while the \textit{representation alignment} strategy could challenge \textit{Condition}~\ref{def:joint_optimal}, this alignment constraint can help reduce the cardinality of $ \bigcap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{ \mathbb{P}^e}$. Thus, performance improvement is still attainable with careful adjustment of the alignment weight by exploiting the oracle knowledge of the target domain.
\label{sec:representation_alignment}

\subsection{Invariant Prediction.} These methods aim to learn a consistent optimal classifier across domains. For example, Invariant Risk Minimization (IRM) \citep{arjovsky2020irm} seeks to learn a representation function $g(x)$ with invariant predictors $\mathbb{E}[Y | g(x), E]$. This goal aligns with \textit{Condition}~\ref{def:sufficient} and encourages using invariant representations, without imposing restrictions that could affect \textit{Condition}~\ref{def:joint_optimal}. VREx \citep{krueger2021out} relaxes the IRM's constraint to enforce equal risks across domains, assuming that the optimal risks are similar across domains. If, however, the optimal solutions exhibit large loss variations, balancing risks could result in suboptimal performance for some domains, violating \textit{Condition}~\ref{def:joint_optimal}. Furthermore, with a limited number of training domains, both IRM and VREx may struggle to identify the optimal invariant predictor, as discussed by \citet{rosenfeld2020risks} and may not offer advantages over ERM, especially when representations from different domains occupy distinct regions in the representation space, as noted by \citep{ahuja2020empirical}. IIB \citep{li2022invariant} and IB-IRM \citep{ahuja2021invariance} integrate the information bottleneck principle with invariant prediction strategies. However, similar to IRM, these approaches only show benefits with a sufficient and diverse number of training domains. Otherwise, the information bottleneck even makes it susceptible to violating \textit{Condition}~\ref{def:sufficient}. See Appendix \ref{apd:relation_to_IDG} for further discussion.

\begin{figure}[h!]
    \centering
\includegraphics[width=1.0\linewidth]{Figures/information_min.png}
    \caption{Information diagrams of $X,Y,Z_c$ and $Z_{\text{min}} := g(X) \text{ s.t } g \in \{\underset{g\in \mathcal{G}}{\text{argmax }} I(g(X);Y) - I(g(X); X)\}$. In limited training domains, learning such minimal representation $Z_{\text{min}}$ would capture the least information about $Z_c$.}
    \label{fig:info_min}
\end{figure}

\paragraph{Information Bottleneck Theory.} We here  elucidate our claim in Section~\ref{sec:discussion_DG} that minimizing $I(g(X); X)$ can subject the model to violating \textit{Condition}~\ref{def:sufficient}. Whereas \cite{ahuja2021invariance} posits that information bottleneck aids generalization, such methods in fact assume sufficient and diverse domains (ref. their Theorem 1 or Assumption 5 and 6 about invariant and spurious feature support overlap), that is when a sufficient condition is met. In this case, the information about $Z_c$ is fully covered by the region $I(X;Y)$ and any $g = \underset{g\in \mathcal{G}}{\text{argmin }} I(g(X);X) \text{ s.t } g \in \{\underset{g\in \mathcal{G}}{\text{argmax }}I(g(X);Y)\}$ could guarantee all spurious features are discarded.



When the training domains are limited, the learned representations is however more likely to contain spurious correlations bad for prediction on unseen domains. Thus, minimizing $I(g(X);X)$ in fact would at most capture the shared information of $X$ and $Y$, thus yielding representations with the least information about $Z_c$. Therefore, such minimal representations are the least likely to meet the sufficient representation constraint in practice. Figure~\ref{fig:info_min} illustrates the difference between two learning scenarios.



\subsection{Augmentation.} Data augmentation \citep{mitrovic2020representation, wang2022out, shankar2018generalizing, zhou2020deep, zhou2021domain, xu2021fourier, zhang2017mixup, wang2020heterogeneous, zhao2020maximum, yao2022improving,carluccidomain, yao2022pcl} have long been applied to DG. This strategy is to utilize predefined or learnable transformations $T$ on the original sample $X$ or its features $g(x)$ to create augmented data $T(X)$ or $T(g(x))$. Applying various transformations during training effectively increases the training dataset, which, according to Theorem~\ref{thm:convergence}, should narrow the hypothesis space. However, it's crucial that transformation $T$ maintains the integrity of the causal factors. This implies a \textit{necessity for some knowledge of the target domain} to ensure the transformations do not alter the causal/invariant information \citep{gao2023out}, otherwise it risks violating \textit{Condition}~\ref{def:sufficient} (e.g., augmentation possibly introduces misleading information \citep{zhang2022rethinking}).\label{sec:augmentation}

\subsection{Ensemble Learning.} Ensemble learning \citep{zhou2012ensemble} refers to training multiple copies of the same architecture with different initializations or splits of the training data, then ensembling the individual models for prediction. This straightforward technique has been shown to outperform a single model across various applications, including DG \citep{zhou2021domain, ding2017deep, zhou2021domain, wang2020dofe, mancini2018best, cha2021swad, arpit2022ensemble}. Unlike explicit ensemble methods where multiple models (or model components) need to be trained, \citet{cha2021swad,rame2022diverse, wortsman2022robust} demonstrate that averaging model weights (WA) at different time steps during training to form a single model at test time \citep{izmailov2018averaging} can significantly enhance robustness under domain shift.  Different from the previous works, our analysis in Section \ref{sec:sufficient_constraint}) provides a new insight that ensemble-based methods can also encourage the learning of \textit{sufficient representation}  (\textit{Condition} \ref{def:sufficient}) to promote generalizability.

% The original explanation of WA is rooted in flatness-based analysis where flat minima is expected to generalize better \citep{cha2021swad}.
% However, \citet{rame2022diverse} argue that this analysis does not fully explain its success in OOD setting and provide an explanation via their proposed bias-variance-covariance-locality decomposition. 


% \section{Sufficient Representation Constraints}
\label{sec:sufficient_constraint}
In the previous section, we introduced a new \textit{necessary condition} called the "sufficient representation condition." In this section, we first explain why this condition has often been overlooked in prior research. We then establish a connection between this condition and the recent \textit{ensemble} strategy, demonstrating that \textit{ensemble} methods indeed encourage models to satisfy this condition.


% \begin{figure*}[!h]
% \subfloat[\label{fig:info}  Learning multiple representations $Z_{i},...,Z_{j}$ through ensemble learning where $ Z_i= g_i(X)\text{ s.t }g_i \in \{\underset{g\in \mathcal{G}}{\text{argmax }}I(g(X);Y)\}$ to maximize the shared information with $Z_c$.]
% {\begin{centering}
% \includegraphics[width=0.4\textwidth]{Figures/information_2.png}
% \par\end{centering}
%  }
%  % \hfill{}
% \subfloat[Information diagrams of $X,Y,Z_c$ and $Z_{\text{min}} := g(X) \text{ s.t } g \in \{\underset{g\in \mathcal{G}}{\text{argmax }} I(g(X);Y) - I(g(X); X)\}$. In limited training domains, learning such minimal representation $Z_{\text{min}}$ would capture the least information about $Z_c$.]
% {\begin{centering}
% \label{fig:info_min} 
% \includegraphics[width=0.4\textwidth]{Figures/information_min.png}
% \par\end{centering}
% }
% \caption{
% Information diagrams of $X, Y, Z_c$ and $Z=g(X)$.
% }
% \label{fig:tnse}
% %\vspace{-4mm}
% \end{figure*}

% \begin{figure}[h!]
%     \centering
% \includegraphics[width=1.0\linewidth]{Figures/information_min.png}
%     \caption{Information diagrams of $X,Y,Z_c$ and $Z_{\text{min}} := g(X) \text{ s.t } g \in \{\underset{g\in \mathcal{G}}{\text{argmax }} I(g(X);Y) - I(g(X); X)\}$. In limited training domains, learning such minimal representation $Z_{\text{min}}$ would capture the least information about $Z_c$.}
%     \label{fig:info_min}
% \end{figure}

  
% However, maximizing the lower bound $I(Z; Y)$ only guarantee $Z$ cover share information $I(X; Y)$. As demonstrated in Figure~\ref{fig:info}.(Right)), for a representation $Z_i$, learned by maximizing the lower bound $I(Z_i; Y)$ i.e., $Z_i=g(X) \mid g \in \{\argmax_{g}I(g(X);Y)\}_{i=1}^{M}$,it optimally cover share information $I(X; Y)$ and possibly some additional information about $Z_c$. To encourage representation capture more information from $Z_c$, we can further directly extend the approach to learn multi-version of representations through ensemble learning i.e., $Z^M = \{Z_i=g_i(X) \mid g_i \in \{\argmax_{g_i}I(g_i(X);Y)\}_{i=1}^{M}\}$, to capture as much information as possible about $Z_c$. This intuition aligns with the analysis on ensembles for OOD
%  generalization in \cite{rame2022diverse}. 
 \color{black}



% \cmt{For clarity, some might wonder why we're applying an information theory perspective here. A common question could be: if we extract spurious features that correlate with the label, and thus also carry information about the causal features, doesn't that make information theory irrelevant in this context? However, it's important to remember that in this section, our goal is to achieve \textbf{sufficient representation}. This means that spurious features can still be valuable if there exists a mapping that transforms these spurious features into the causal features.}


 
\paragraph{Connection to Ensemble strategy.} By definition, a representation function $g$ is considered as sufficient representation if there exists a function $\phi\in \Phi$ such that:
$\phi\circ g \in \mathcal{G}_c$. Our task can thus be translated into learning the representation $Z=g(X)$ that captures the most information about the causal factor $Z_c$. This motivates us to find $Z$ that maximizes the mutual information $I(Z; Z_c)$.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/information_2.png}
    \caption{Information diagrams of $X, Y, Z_c$ and $Z=g(X)$. Learning multiple representations $Z_{i},...,Z_{j}$ through ensemble learning where $ Z_i= g_i(X)\text{ s.t }g_i \in \{\underset{g\in \mathcal{G}}{\text{argmax }}I(g(X);Y)\}$ to maximize the shared information with $Z_c$.}
    \label{fig:info}
\end{figure}


Given a specific domain, recall our model $Z\leftarrow X \leftarrow Z_c \rightarrow Y$, where $Y$ is influenced by $Z_c$ (the latent cause) and $Z_c$ also affects $X$. Note that $X$ is also under the influence of $Z_e$, which we omit here for simplicity. Since $Z_c$ is unobserved, we cannot directly measure or learn from it. However, we can leverage $Y$, which inherits the causal information of $Z_c$. This intuition can be best understood via an information diagram. 

Let us examine Figure~\ref{fig:info}.(Left) that illustrates the mutual information of the $4$ variables. We have $I(X,Y \mid Z_c)=0$, meaning the causal features $Z_c$ must capture the shared information $I(X;Y)$. By Assumption~\ref{as:sufficient_causal_support} and Proposition~\ref{thm:invariant_correlation}, it follows that $X$ contains all information about $Z_c$.

By the chain rule of mutual information, we have that $I(Z; Z_c) \ge I(Z; Y)$. Thus, we resort to maximizing the lower bound $I(Z; Y)$ to increasing the chance of learning $Z$ that contains causal information $Z_c$. 
Recall that we use the cross-entropy loss \(\ell:\mathcal{Y}_{\Delta} \times \mathcal{Y} \mapsto \mathbb{R}\) to optimize the hypothesis for training domains. It is well-known that minimizing the cross-entropy loss is equivalent to maximizing the lower bound of \(I(Z; Y)\) \citep{qin2019rethinking, colombo2021novel}. In other words, hypotheses that are optimal on training domains (\textit{Condition}~\ref{def:joint_optimal}) also promote the sufficient representation function condition (\textit{Condition}~\ref{def:sufficient}). However, maximizing the lower bound \(I(Z; Y)\) only ensures that \(Z\) captures the shared information \(I(X; Y)\) and potentially some additional information about \(Z_c\) (as illustrated in Figure~\ref{fig:info} (Right)).

To encourage the representation \(Z\) to capture more information from \(Z_c\), this approach can be extended to learn multiple versions of representations through ensemble learning. Specifically, we can learn an \(M\)-ensemble of representations \(Z^M\):
\[
Z^M = \left\{Z_i = g_i(X) \mid g_i \in \underset{g\in \mathcal{G}}{\text{argmax }} I(g(X); Y) \right\}_{i=1}^{M},
\]
to capture as much information as possible about \(Z_c\). This intuition aligns with the analysis of ensembles for OOD generalization presented in \cite{rame2022diverse}.



\section{Sufficient Invariant Representation Learning}
\label{sec:main_proposed_method}

Section ~\ref{sec:discussion_DG} highlights that existing DG strategies attempt to maximize the likelihood of seeking a global optimal hypothesis from different directions yet with several drawbacks. Furthermore, that they all overlook \textit{Condition}~\ref{def:sufficient} poses a risk of landing in regions with empty solution set. Generally, an effective DG algorithm is one that strives to attain the sufficient conditions while guarantees the necessary conditions. Here we propose a method that exploits the joint effect of the two sets of conditions to boost generalization. 

In the following, we explain how to incorporate the sufficient representation constraint via \textit{ensemble learning} and present a novel \textit{representation alignment} strategy that can enforce the necessary conditions. We particularly do not consider \textit{invariant prediction} since it cannot substantiate its superiority over ERM with a potential of violating both necessary conditions. Meanwhile, \textit{data augmentation} typically provides significant benefits and can be integrated in a plug-and-play fashion. Since it requires prior knowledge, users should apply it carefully based on their expertise. 

% We here do not aim to propose a state-of-the-art method, rather introduce a foundational approach that can be incorporated into any existing DG framework that already targets sufficient conditions for enhancing their generalizability.  

% Here we propose an approach that can leverage the joint advantages of popular DG approaches while mitigating their respective weaknesses.




\subsection{Subspace Representation Alignment}

\textit{Representation Alignment} strategy helps reduce the cardinality of $\Funion$ but may compromise \textit{Condition}~\ref{def:joint_optimal} due to the potential trade-off between alignment constraints and domain losses (Theorem~\ref{theorem:single_tradeoff}). \textbf{However, we now show that with a more careful design, we can address the trade-off effectively}. Our proposed strategy, called \textit{Subspace Representation Alignment} (SRA), involves organizing training domains into distinct subspaces and aligning representations within these subspaces. 
This aims to diminish or completely remove differences in the marginal label distributions across these domains so that the search space can be reduced. 

% \color{blue}
% Merge to theorem, if $\Gamma=f^*$ them $D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0$
% \color{black}
% We consider \textit{subspace projector} $\Gamma: \mathcal{X}\rightarrow \mathcal{M}$, given a subspace index $m\in \mathcal{M}$, we denote $A_{m}=\Gamma^{-1}(m)=\left\{ x:\Gamma(x)=m\right\} $ is the region on data space which has the same index $m$.
% Let $\mathbb{P}_{m}^{e}$ be the distribution restricted by $\mathbb{P}^{e}$ over the set $A_{m}$. Eventually, we define $\mathbb{P}_{m}^{e}\left(y\mid x\right)$ as the probabilistic labeling
% distribution on the subspace $\left(A_{m},\mathbb{P}_{m}^{e}\right)$,
% meaning that if $x\sim\mathbb{P}_{m}^{e}$, $\mathbb{P}_{m}^{e}\left(y\mid x\right)=\mathbb{P}_{e}\left(y\mid x\right)$. Since each data point $x \in \mathcal{X}$ corresponds to only a single $\Gamma(x)$, the data space is partitioned into disjoint sets, i.e., $\mathcal{X} = \bigcup_{m=1}^{\mathcal{M}} A_{m}$, where $A_m \cap A_n = \emptyset, \forall m \neq n$. Consequently, 
% $\mathbb{P}^{e}:=\sum_{m\in\mathcal{M}}\pi^{e}_m\mathbb{P}_{m}^{e}$
% where $\pi^{e}_m=\mathbb{P}^{e}\left(A_{m}\right) \slash \sum_{m'\in\mathcal{M}}\mathbb{P}^{e}\left(A_{m'}\right)$.
\begin{theorem}
\label{theorem:multi_bound} Given \textit{subspace projector} $\Gamma: \mathcal{X}\rightarrow \mathcal{M}$, given a subspace index $m\in \mathcal{M}$, let $A_{m}=\Gamma^{-1}(m)=\left\{ x:\Gamma(x)=m\right\} $ is the region on data space which has the same index $m$ and $\mathbb{P}_{m}^{e}$ be the distribution restricted by $\mathbb{P}^{e}$ over the set $A_{m}$, if the loss function $\ell$ is upper-bounded by a positive
constant $L$, then:
(i)  The target general loss is upper-bounded: 
\begin{align*}
\left | \mathcal{E}_{tr} \right |\sum_{e\in \mathcal{E}_{tr}}\mathcal{L}\left ( f,\mathbb{P}^{e} \right )
&\leq
\sum_{e\in \mathcal{E}_{tr}} \sum_{m\in\mathcal{M}}\pi^{e}_m
\mathcal{L}\left ( f,\mathbb{P}^{e}_{m} \right ) \\+
&L\sum_{e, e'\in \mathcal{E}_{tr}}\sum_{m\in\mathcal{M}}\pi^{e}_{m}D\left ( g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}^{e'}_{m} \right ),
\end{align*}
(ii) Distance between two label marginal distribution $\mathbb{P}^{e}_{m}(Y)$ and $\mathbb{P}^{e'}_{m}(Y)$ can be upper-bounded: 
\begin{equation*}
\begin{aligned}
D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right) &\leq 
D\left ( g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}^{e'}_{m} \right )\\
+&\mathcal{L}\left ( f,\mathbb{P}^{e}_{m}\right )
+
\mathcal{L}\left ( f,\mathbb{P}^{e'}_{m} \right )
\end{aligned}
\end{equation*}
(iii) Construct the subspace projector $\Gamma$ as the optimal hypothesis for the source domains, i,e., \(\Gamma \in \cap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^{e}}\), which defines a set of subspace indices $\mathcal{M}=\{m=\hat{y}\mid \hat{y}=\Gamma(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}\subseteq \mathcal{Y}_\Delta$, then
\begin{equation*}
D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0
\end{equation*}

where $g_{\#}\mathbb{P}$ denotes representation distribution on $\mathcal{Z}$ induce by applying $g$ with $g: \mathcal{X} \mapsto \mathcal{Z}$ on data distribution $\mathbb{P}$, $D$ can be $\mathcal{H}$-divergence, Hellinger or Wasserstein distance. (Proof in Appendix~\ref{theorem:multi_bound_A})
\label{theorem:tradeoff}
\end{theorem}

In Theorem \ref{theorem:multi_bound}, (i) illustrates that \textit{domain-specific losses} can be broken down into \textit{losses} and \textit{representation alignments} within individual subspaces. Optimizing the subspace-specific losses across domains ensures optimizing the overall loss within the original domains are optimized. Meanwhile, (ii) demonstrates that the distance between the marginal label distributions is now grounded within subspaces, denoted as $d_{1/2}\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)$. Theorem~\ref{theorem:multi_bound} suggests that appropriately distributing training domains across subspaces can reduce both the upper and lower bounds. Particularly, for a given subspace index $m$, if $D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0$, we can jointly optimize both \textit{domains losses} $\mathcal{L}\left (f,\mathbb{P}^{e}_m \right ) + \mathcal{L}\left (  f,\mathbb{P}^{e'}_m \right )$ and \textit{representation alignment} $D\left ( g_{\#}\mathbb{P}^{e}_m,g_{\#}\mathbb{P}^{e'}_m \right )$. Consequently, optimizing the RHS of (ii) for all supspaces is equivalent to minimizing the RHS of (i).

Fortunately, Consequently, \(D\left(\mathbb{P}^{e}_{\mathcal{Y},m}, \mathbb{P}^{e'}_{\mathcal{Y},m}\right) = 0\) for all \(m \in \mathcal{M}\), allowing us to jointly optimize both \textit{domain losses} and \textit{representation alignment}.

% The question now is how we can manage the training distribution into a subspace such that $D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)$ is reduced, potentially even to zero. Fortunately, working within training domains, we anticipate that $f\in \cap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^{e}}$ will predict the ground truth label $f(x)=f^*(x)$ where $f^*\in \mathcal{F}^*$. We can define a projector \(\Gamma = f\), which induces a set of subspace indices $\mathcal{M}=\{m=\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}\subseteq \mathcal{Y}_\Delta$. As a result, given subspace index $m\in\mathcal{M}$, $\forall i \in \mathcal{Y}, \mathbb{P}^{e}_{\mathcal{Y},m}(Y=i) = \mathbb{P}^{e'}_{\mathcal{Y},m}(Y=i) = \sum_{x \in f^{-1}(m)}\mathbb{P}(Y=i\mid x) = m[i]$. Consequently, \(D\left(\mathbb{P}^{e}_{\mathcal{Y},m}, \mathbb{P}^{e'}_{\mathcal{Y},m}\right) = 0\) for all \(m \in \mathcal{M}\), allowing us to jointly optimize both \textit{domain losses} and \textit{representation alignment}.

The \textbf{final optimization objective}, encapsulating the constraints of optimal hypothesis for all training domain, ensemble for sufficient representation, and subspace representation alignment is given by:
\begin{align}
\min_{f} &\underset{\text{Subspace Representation Alignment}}{\underbrace{\sum_{e,e'\in \mathcal{E}_{tr}}\sum_{m\in \mathcal{M}}D\left( g\#\mathbb{P}_m^{e},g\#\mathbb{P}_m^{e'}\right)}}\\
&\text{ s.t. } \underset{\text{Training domain optimal hypothesis}}{\underbrace{f\in \bigcap_{e\in \mathcal{E}_{tr}}\underset{ f}{\text{argmin }} \mathcal{L}\left(f,\mathbb{P}^{e}\right)}} \nonumber
\label{eq:final_objective}
\end{align}

% \begin{align}
%     &\min_{f} \underset{\text{Sufficient Representation Constraint}}{\underbrace{\sum_{e\in \mathcal{E}_{tr}}-\mathcal{I}\left(X^{e},g(X^{e})\right )}}
% + \underset{\text{Subspace Representation Alignment}}{\underbrace{\sum_{e,e'\in \mathcal{E}_{tr}}\sum_{m\in \mathcal{M}}D\left( g\#\mathbb{P}_m^{e},g\#\mathbb{P}_m^{e'}\right)}}
% \label{eq:final_objective}
% \\
% &\text{subject to } \underset{\text{Training domain optimal hypothesis}}{\underbrace{f\in \bigcap_{e\in \mathcal{E}_{tr}}\underset{ f}{\text{argmin }} \mathcal{L}\left(f,\mathbb{P}^{e}\right)}}.\nonumber
% \end{align}

where $\mathcal{M}=\{\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}$ and $D$ can be $\mathcal{H}$-divergence, Hellinger distance, Wasserstein distance. We provide the details on the practical implementation of the proposed objective in Appendix~\ref{Sec:practical}.

\import{}{4_Experiments.tex}