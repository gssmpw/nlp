
% \subsection{Visualization}
% \label{sec:apd_visualization}

% To elucidate the interaction between latent representations and prototypes, we employ t-SNE~\citep{Maaten08visualizingdata} to visualize their distribution as obtained from our method dataset, depicted in Figure~\ref{fig:tsne}. 
% \begin{figure}[h!]
% \begin{centering}
% \subfloat{\centering{}\includegraphics[width=0.8\textwidth]{ICLR2025/Figures/tnse.png}}
% \par\end{centering}
% \caption{The t-SNE visualization of the learned representations and prototypes generated from our method.}
% \label{fig:tsne}
% \end{figure} 

% Observations from Figure~\ref{fig:tsne}.a reveal a homogeneous blend of representations across various domains, indicating an indistinct separation of samples from different domains, while maintaining clear class distinctions as shown in Figure~\ref{fig:tsne}.b. This highlights the efficacy of our subspace representation alignment approach.


% \begin{table}[h!]
% \caption{Classification Accuracy on PACS using ResNet50 with Prototype as classifier.}
% %\vspace{-0.5mm}
% \begin{centering}
% \resizebox{0.6\columnwidth}{!}{ %
% \begin{tabular}{lccccc}
% \toprule
% Algorithm  & \textbf{A} & \textbf{C} & \textbf{P} & \textbf{S} & \textbf{Avg}  \\
% \midrule
% Classifier & 90.2 $\pm$ 0.5 & 84.4 $\pm$ 0.3 & 97.9 $\pm$ 0.1 & 82.3 $\pm$ 0.2 & \textbf{88.7} \\
% Prototype & 90,9 $\pm$ 0.3 & 84.1 $\pm$ 0.3 & 97.5 $\pm$ 0.1 & 82.1 $\pm$ 0.2 & \textbf{88.7} \\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:PACS_r}
% \end{table}

% Additionally, consistent with our approach, prototypes are intended to capture class-discriminative information. Observations indicate that these prototypes are positioned within the clusters of their respective classes. To test their efficacy, we executed experiments in which prototypes are utilized as markers for class identification. In this process, a classifier initially predicts the class associated with a prototype, which is then used to determine the class label of a representation based on proximity to the nearest prototype. Results presented in Table \ref{tab:PACS_r} show that class labels can be accurately predicted using prototypes, sometimes even surpassing the effectiveness of direct classifier use.



% % \subsection{ Influence of Different Components}
% % \label{sec:apd_components}

% % \begin{table}[h!]
% % \caption{Classification Accuracy on PACS using ResNet50 with absence of different components.}
% % %\vspace{-0.5mm}
% % \begin{centering}
% % \resizebox{0.8\columnwidth}{!}{ %
% % \begin{tabular}{lccccc}
% % \toprule
% % Algorithm  & \textbf{A} & \textbf{C} & \textbf{P} & \textbf{S} & \textbf{Avg}  \\
% % \midrule


% % ERM & 89.3 $\pm$ 0.2 & 83.4 $\pm$ 0.6 & 97.3 $\pm$ 0.3 & 82.5 $\pm$ 0.5 & 88.1 \\
% % Our method w/o $\mathcal{L}_I$ & 89.9 $\pm$ 0.5 & 83.3 $\pm$ 0.4 & 97.9 $\pm$ 0.3 & 81.4 $\pm$ 0.6 & 88.1 \\
% % Our method w/o $\mathcal{L}_D$ & 89.7 $\pm$ 0.5 & 84.2$\pm$ 0.7 & 97.2 $\pm$ 0.3 &  81.5 $\pm$ 0.8 &  88.2\\
% % Our method & 90.2 $\pm$ 0.5 & 84.4 $\pm$ 0.3 & 97.9 $\pm$ 0.1 & 82.3 $\pm$ 0.2 & \textbf{88.7} \\
% % \bottomrule
% % \end{tabular}}
% % \par\end{centering}
% % \label{tab:PACS_componemt}
% % \end{table}
% % Table \ref{tab:PACS_componemt} demonstrates that removing any component significantly reduces performance. Specifically, omitting the subspace alignment constraint results in performance marginally better than the ERM baseline, whereas failing to maximize mutual information $\mathcal{I}(X,g(X))$ leads to performance on par with the baseline.

% \subsection{Ablation study on the number of Subspaces}
 
% Considering our data generation process, the number of distinct labels $\mathbb{P}(Y\mid x)$ reflects the number of distinct causal factors (denoted as $\left | \mathcal{Z} \right |$). If $\mathcal{M}\leq\left | \mathcal{Z} \right |$, samples with different labels may be projected into the same subspace, leading to discrepancies in the marginal label distribution within that subspace.

% We revisit the two key points in the previous discussion:

% \begin{itemize}
% \item Theorem~\ref{theorem:multi_bound} implies that projecting samples into the correct subspaces can significantly reduce or entirely eliminate marginal label shifts within those subspaces, assuming optimal projection for the sake of simplicity.
% \item As mentioned earlier, projecting samples with the same label $\mathbb{P}(Y\mid x)$ eliminates the discrepancy $d_{1/2}\left (\mathbb{P}^{e_i}_{\mathcal{Y},m}, \mathbb{P}^{e_i}_{\mathcal{Y},m} \right )$, reducing it to zero.
% \end{itemize}

% Increasing $\mathcal{M}$ reduces the likelihood of differently labeled samples being mapped to the same subspace, thus decreasing the discrepancy outlined in Theorem~\ref{theorem:multi_bound} (ii). Itâ€™s notable that the upper bound in (i) can be optimized to the limit defined by (ii) when the focus is only on training domains. This optimization, in turn, minimizes the bound (i).

% Rather than treating $\left | \mathcal{Z} \right |$ merely as a parameter for tuning, we delve further into analyzing the impact of varying $\left | \mathcal{Z} \right |$ values. In this ablation study, we test $\left | \mathcal{Z} \right |$ values of $[4,8,16,32]\times \left | \mathcal{C} \right |$, where $\left | \mathcal{C} \right |$ denotes the number of classes. 

% \label{sec:apd_subspaces}
% \begin{table}[h!]
% \caption{Classification Accuracy on PACS using ResNet50 with different number of subspaces (NoS) per class.}
% %\vspace{-0.5mm}
% \begin{centering}
% \resizebox{0.60\columnwidth}{!}{ %
% \begin{tabular}{lccccc}
% \toprule
% NoS \left | \mathcal{M} \right |  & \textbf{A} & \textbf{C} & \textbf{P} & \textbf{S} & \textbf{Avg}  \\
% \midrule
% ERM & 89.3 $\pm$ 0.2 & 83.4 $\pm$ 0.6 & 97.3 $\pm$ 0.3 & 82.5 $\pm$ 0.5 & 88.1 \\
% 4 & 90.2 $\pm$ 0.3 & 83.2 $\pm$ 0.7 & 97.9 $\pm$ 0.2 & 82.3 $\pm$ 1.5 & 88.2 \\

% 8 & 90.5 $\pm$ 0.8 & 83.8 $\pm$ 0.6 & 97.6 $\pm$ 0.3 &  82.1 $\pm$ 1.8 &  88.7\\

% 16 & 90.5 $\pm$ 0.5 & 83.4 $\pm$ 0.2 & 97.8 $\pm$ 0.1 & 83.2 $\pm$ 0.2 & 88.7 \\
% 32 & 90.2 $\pm$ 0.5&  83.8 $\pm$ 0.8 & 97.3 $\pm$ 0.4 & 82.0 $\pm$ 1.2 & 88.4\\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:PACS_prototype}
% \end{table}

% Table \ref{tab:PACS_prototype} reveals that performance generally improves with an increase in the number of prototypes. Nonetheless, a decline in performance is noted when $K$ becomes excessively large. We speculate this behavior is tied to the dataset's underlying causal factors; 
% specifically, if a limited number of causal factors generate the data, assigning a large number of prototypes to capture discriminative information might result in one causal factor being associated with multiple prototypes, thereby introducing ambiguity. This hypothesis, however, requires further investigation for confirmation, and we earmark it for future research.



% \subsection{Compare to other baselines}

% One of our key contributions is offering a new perspective on why domain generalization (DG) algorithms often fail to outperform the fundamental empirical risk minimization (ERM) approach on standard benchmarks, through an analysis of sufficient and necessary conditions. In the main paper, we compare our proposed SRA method with the two most related methods, DANN and CDANN, as they represent specific cases of our approach where the number of subspaces per class is set to 0 and 1, respectively. 

% In this section, we provide additional experimental results from various baselines, both with and without SWAD, on five datasets from the DomainBed benchmark \cite{gulrajani2020search}, to further support our discussion and analysis.


% \begin{table*}[h!]
% \caption{Classification accuracy (\%) for all algorithms across datasets.
% }
% \begin{centering}
% \resizebox{0.65\width}{!}{ %
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Algorithm}  & \textbf{VLCS} & \textbf{PACS} & \textbf{OfficeHome} & \textbf{TerraIncognita}  & \textbf{DomainNet} & \textbf{Avg} \\
% \toprule
% ERM~\citep{gulrajani2020search}  & 77.5 $\pm$ 0.4 & 85.5 $\pm$ 0.2 & 66.5 $\pm$ 0.3 & 46.1 $\pm$ 1.8 & 40.9 $\pm$ 0.1 & 63.3 \\
% GroupDRO~\citep{sagawa2019distributionally} & 76.7 $\pm$ 0.6 & 84.4 $\pm$ 0.8 & 66.0 $\pm$ 0.7 & 43.2 $\pm$ 1.1 & 33.3 $\pm$ 0.2 & 60.7 \\
% Mixup~\citep{wang2020heterogeneous}  & 77.4 $\pm$ 0.6 & 84.6 $\pm$ 0.6 & 68.1 $\pm$ 0.3 & 47.9 $\pm$ 0.8 & 39.2 $\pm$ 0.1 & 63.4 \\
% MLDG~\citep{li2017learning} & 77.2 $\pm$ 0.4 & 84.9 $\pm$ 1.0 & \textbf{66.8} $\pm$ 0.6 & 47.7 $\pm$ 0.9 & 41.2 $\pm$ 0.1 & 63.6 \\
% MTL~\citep{blanchard2021domain} & 77.2 $\pm$ 0.4 & 84.6 $\pm$ 0.5 & 66.4 $\pm$ 0.5 & 45.6 $\pm$ 1.2 & 40.6 $\pm$ 0.1 & 62.9 \\
% SagNet~\citep{nam2021reducing} & 77.8 $\pm$ 0.5 & \textbf{86.3} $\pm$ 0.2 & 68.1 $\pm$ 0.1 & 48.6 $\pm$ 1.0 & 40.3 $\pm$ 0.1 & 64.2 \\
% ARM~\citep{zhang2021adaptive} & 77.6 $\pm$ 0.3 & 85.1 $\pm$ 0.4 & 64.8 $\pm$ 0.3 & 45.5 $\pm$ 0.3 & 35.5 $\pm$ 0.2 & 61.7 \\
% RSC~\citep{huang2020self}  & 77.1 $\pm$ 0.5 & 85.2 $\pm$ 0.9 & 65.5 $\pm$ 0.9 & 46.6 $\pm$ 1.0 & 38.9 $\pm$ 0.5 & 62.7 \\
% IRM~\citep{arjovsky2020irm} & 78.5 $\pm$ 0.5 & 83.5 $\pm$ 0.8 & 64.3 $\pm$ 2.2 & 47.6 $\pm$ 0.8 & 33.9 $\pm$ 2.8 & 61.6 \\
% VREx~\citep{krueger2021out} & 78.3 $\pm$ 0.2 & 84.9 $\pm$ 0.6 & 66.4 $\pm$ 0.6 & 46.4 $\pm$ 0.6 & 33.6 $\pm$ 2.9 & 61.9 \\
% MMD~\citep{li2018domain} & 77.5 $\pm$ 0.9 & 84.6 $\pm$ 0.5 & 66.3 $\pm$ 0.1 & 42.2 $\pm$ 1.6 & 23.4 $\pm$ 9.5 & 58.8 \\
% CORAL~\citep{sun2016deep} & \textbf{78.8} $\pm$ 0.6 & 86.2 $\pm$ 0.3 & \textbf{68.7} $\pm$ 0.3 & 47.6 $\pm$ 1.0 & {41.5} $\pm$ 0.1 & {64.5} \\
% DANN~\citep{ganin2016domain} & 78.6 $\pm$ 0.4 & 83.6 $\pm$ 0.4 & 65.9 $\pm$ 0.6 & 46.7 $\pm$ 0.5 & 38.3 $\pm$ 0.1 & 62.6 \\
% CDANN~\citep{li2018domain}  & 77.5 $\pm$ 0.1 & 82.6 $\pm$ 0.9 & 65.8 $\pm$ 1.3 & 45.8 $\pm$ 1.6 & 38.3 $\pm$ 0.3 & 62.0 \\
% \midrule

% \textbf{Ours} (SRA)  & 76.4 $\pm$ 0.7 & \textbf{86.3} $\pm$ 1.1 & 66.4 $\pm$ 0.7 & \textbf{49.5} $\pm$ 1.0 & \textbf{44.5} $\pm$ 0.3 & \textbf{64.6} \\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:Averages_domainbed}
% \end{table*}



% \begin{table*}[h!]
% \caption{Classification accuracy (\%) for all algorithms across datasets.
% }
% \begin{centering}
% \resizebox{0.65\width}{!}{ %
% \begin{tabular}{lccccc}
% \toprule
% \textbf{Algorithm}  & \textbf{VLCS} & \textbf{PACS} & \textbf{OfficeHome} & \textbf{TerraIncognita}  & \textbf{Avg} \\
% \toprule
% SWAD~\citep{cha2021swad} & 79.1 $\pm$ 0.4 & 88.1 $\pm$ 0.4 & 70.6 $\pm$ 0.3 & 50.0 $\pm$ 0.4  & 72.0\\




% SWAD + IRM~\citep{arjovsky2020irm} & 78.8 $\pm$ 0.2 & 88.1 $\pm$ 0.4 & 70.4 $\pm$ 0.2 & 49.6 $\pm$ 1.7  & 71.7 \\
% SWAD + VREx~\citep{krueger2021out} & 78.1 $\pm$ 1.3 & 85.4 $\pm$ 0.5 & 69.9 $\pm$ 0.1 & 50.0 $\pm$ 0.2 & 70.9 \\
% SWAD +CORAL~\citep{sun2016deep} & \underline{78.9} $\pm$ 0.6 & 88.3 $\pm$ 0.5 & 71.4 $\pm$ 0.1 & 51.1 $\pm$ 0.9 & 72.4 \\
% SWAD +MMD~\citep{li2018domain} & 78.7 $\pm$ 0.1 & 88.3 $\pm$ 0.1 & 70.6 $\pm$ 0.4 & 49.6 $\pm$ 0.5  & 71.8 \\
% SWAD + DANN & 79.2 $\pm$ 0.0 & 87.9 $\pm$ 0.5 & 70.5 $\pm$ 0.1 & 50.6 $\pm$ 0.6  & 72.2\\
% SWAD + CDANN & 79.3 $\pm$ 0.2 & 87.7 $\pm$ 0.3 & 70.4 $\pm$ 0.1 & 50.7 $\pm$ 0.1  & 72.2\\
% \midrule

% \textbf{Ours} (SRA + SWAD) & \underline{79.4} $\pm$ 0.4 & \underline{88.7} $\pm$ 0.2 &  \underline{72.1} $\pm$ 0.5 &  \underline{51.6} $\pm$ 1.2 & \underline{73.0} \\
% \textbf{Ours} (SRA + SWAD + Ensemble) & \textbf{79.8} $\pm$ 0.0 & \textbf{89.2} $\pm$ 0.0 &  \textbf{73.2} $\pm$ 0.0 &  \textbf{52.2} $\pm$ 0.0 & \textbf{73.3} \\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:Averages_domainbed_swad}
% \end{table*}

% As observed in both Table~\ref{tab:Averages_domainbed} and Table~\ref{tab:Averages_domainbed_swad}, the baselines fail to consistently surpass the simple ERM baseline across all settings. While some methods perform well on certain datasets, they perform worse on others. However, the combination of our proposed method (SRA), which enforces strong sufficient conditions, and SWAD, which promotes necessary conditions, significantly improves generalization. This combination outperforms ERM and other baselines in all settings. These results support our analysis in Section~\ref{sec:discussion_DG}, indicating that existing methods often violate the necessary condition for effective domain generalization.
