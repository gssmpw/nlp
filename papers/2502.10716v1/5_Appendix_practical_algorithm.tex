
\section{Practical Methodology}
\label{Sec:practical}
In this section, we present the practical objectives to achieve Eq. (\ref{eq:final_objective}):

\begin{align}
\min_{f=h\circ g} \underset{\text{Subspace Representation Alignment}}{\underbrace{\sum_{e,e'\in \mathcal{E}_{tr}}\sum_{m\in \mathcal{M}}D\left( g\#\mathbb{P}_m^{e},g\#\mathbb{P}_m^{e'}\right)}}\text{ s.t. } \underset{\text{Training domain optimal hypothesis}}{\underbrace{f=h\circ g\in \bigcap_{e\in \mathcal{E}_{tr}}\underset{ f}{\text{argmin }} \mathcal{L}\left(f,\mathbb{P}^{e}\right)}}
   \label{eq:final_objective_apd}
\end{align}

where $\mathcal{M}=\{\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}$ and $D$ can be $\mathcal{H}$-divergence, Hellinger distance, Wasserstein distance.

In the following, we consider the encoder $g$, classifier $h$, domain discriminator $h_d$ and set of $K$ empirical training domains $\mathbb{D}^{e_i}=\{x_{j}^{e_i},y_{j}^{e_i}\}_{j=1}^{N_{e_i}}\sim [\mathbb{P}^{e_i} ]^{N_{e_i}}$, $i=1...K$.


\subsection{Optimal hypothesis across training domains}
For \textit{optimal hypothesis across training domains condition}, we simply adopting the objective set forth by ERM: 
\begin{align}
\label{eq:emp_IRM}
     \min_{f} \; \sum_{i=1}^K \mathcal{L}\left(f,\mathbb{D}^{e_i}\right)
\end{align} 


\subsection{Subspace Representation Alignment}
\paragraph{Subspace Modelling and Projection.} 
\label{sec:subspace_project_detail}
Our objective is to map samples $x$ from training domains with identical predictions $f(x) = m$ into a unified subspace, where $m\in \mathcal{M}=\{\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}$. Given that the cardinality of $\mathcal{M}$ can be exceedingly large, potentially equal to the total number of training samples if the output of $f(x)$ is unique for each sample, this makes the optimization process particularly challenging.

Drawing inspiration from the concept of prototypes \cite{snell2017prototypical}, we suggest representing $\mathcal{M}$ as a set of prototypes $\mathcal{M} = \{m_i\}_{i=1}^{M}$, where each $m_i$ is an element of $\mathcal{Z}$. Consequently, a sample $x$ is assigned to a subspace by selecting the nearest prototype $m_i$ i.e., $i=\underset{ i'}{\text{argmin }} \rho(g(x),m_{i'})$. Note that prototypes act as condensed representations of specific prediction outcomes. Consequently, samples assigned to the same prototype will receive the same prediction. Although this approach streamlines the subspace projection, it may lead to local optima as the mapping might favor a limited number of prototypes early in training \cite{vuong2023vector}. To mitigate this issue, we adopt a Wasserstein (WS) clustering approach \cite{vuong2023vector} to guide the mapping of latent features from each domain into the designated subspace more effectively.
We first endow a discrete distribution over the prototypes as $\mathbb{P}_{\mathcal{M},\pi}=\sum_{i=1}^{M}\pi_{i}\delta_{m_{i}}$
with the Dirac delta function $\delta$ and the weights $\pi\in\Delta_{M}= \{\pi'\geq \boldsymbol{0}: \Vert \pi'\Vert_1 =1\}$. 

Then we project each domain $\mathbb{P}^{e_i}$ in subspaces indexed by prototypes as follows:
\begin{equation}
\min_{\mathcal{M},\pi}\min_{g}\left \{\mathcal{L}_{P}=\sum_{i=1}^K\lambda\mathcal{W}_{\rho}\left(g\#\mathbb{P}^{e_i},\mathbb{P}_{\mathcal{M},\pi}\right)\right \},\label{eq:reconstruct_form_continuous}
\end{equation}
where:
\begin{itemize}
    \item Cost metric $\rho(z,m)=\frac{z^\top m}{\left \| z \right \|\left \| c \right \|}$ is the cosine similarity between the latent representation $z$ and the prototype $c$.
    \item  Wasserstein distance between source domain representation distribution and distribution over prototype $\mathbb{P}_{\mathcal{M},\pi}$:
\begin{align}
\mathcal{W}_{d}\left(g\#\mathbb{P}^{e_i}_{x},\mathbb{P}_{c,\pi}\right)
&=\mathcal{W}_{d}\left(\sum_{n=1}^{B}\frac{1}{B}g\left(x_{n}\right),\sum_{i=1}^{M}\pi_{i}\delta_{m_{i}}\right)\\
&=\frac{1}{B}\min_{\Gamma:\Gamma\#\left(g\#\mathbb{P}^{e_i}_{x}\right)=\mathbb{P}_{c,\pi}}\sum_{n=1}^{B}\rho\left(g\left(x_{n}\right),\Gamma\left(g\left(x_{n}\right)\right)\right)
\end{align}

Where $B$ is the batch size. This Wasserstein distance can be effectively compute by linear dynamic programming method, entropic dual form of optimal transport \citep{genevay2016stochastic} or Sinkhorn algorithm \cite{cuturi2013sinkhorn}.
\end{itemize}


\paragraph{Subspace Alignment Constraints}
Subspace alignment is achieved through a conditional adversarial training approach \cite{gan2016learning, li2018domain}. In this framework, the \textbf{subspace-conditional} domain discriminator $h_d$ aims to accurately predict the domain label ``$e_i$" based on the combined feature $[z,m]$, where $\{z=g(x), m=\Gamma(x)\}$. Concurrently, the objective for the representation function $g$ is to transform the input $x$ into a latent representation $z$ in such a way that $h_d$ is unable to determine the domain ``$e_i$" of $x$.  We employ the Gradient Reversal Layer (GRL) as introduced by\cite{ganin2016domain}, thereby simplifying the optimization process to:

\begin{equation}
    \min_{g, h_d} \left \{\mathcal{L}_{D}=-\sum_{i=1}^K\mathbb{E}_{x\sim\mathbb{D}^{e_i}}\left [ e_i\log h_d\left (\left [ \mathcal{R}\left ( g(x) \right ),m \right ]\right ) \right ] \right \}
\end{equation}

where $\mathcal{R}$ is gradient reversal layer.

% Such negative gradients
% contribute to making the learned features similar across domains. We propose a gradient-reversal layer (GRL) to update $g$\MH{check the latex here, it says theta f} by easily following \cite{ganin2016domain}. This gradient reversal layer does nothing and merely forwards the input to the following layer during forward propagation. However, it multiplies the gradient by âˆ’1 during the backpropagation to obtain a negative gradient from the domain classification.
% \subsection{Overall Framework}

\subsection*{Final objective}
Putting all together, we propose a joint optimization objective, which is given as  
\begin{equation}
\min_{\mathcal{M},\pi} \min_{g,h, h_d}  \left \{\mathcal{L}_{H}+\lambda_P\mathcal{L}_{P}+\lambda_D \mathcal{L}_{D}\right \},
\end{equation}
where $ \lambda_P$ is the subpsace projector hyper-parameter and $\lambda_D$ is the representation alignment hyper-parameter. 



We highlight that SRA is most similar to DANN and CDANN. Like these methods, SRA utilizes $\mathcal{H}$-divergence for alignment; however, the key distinction lies in the alignment strategy: 
\begin{itemize}
    \item DANN aligns the entire domain representation, 
    \item CDANN aligns class-conditional representations,
    \item while SRA employs subspace-conditional alignment.
\end{itemize}

It is also important to note that the representation alignment hyperparameter $\lambda_D$ is kept the same for DANN, CDANN, and SRA in our experiments. As discussed in Theorem~\ref{theorem:single_tradeoff}, DANN and CDANN potentially violate necessary conditions, whereas SRA does not (Theorem~\ref{theorem:multi_bound}), leading to improved performance.


