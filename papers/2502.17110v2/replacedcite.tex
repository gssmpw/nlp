\section{Related Work}
\subsection{GUI Agent}
To improve user experience, intelligent agent frameworks powered by Large Language Models (LLMs) are rapidly advancing in GUI operations ____. On the Web, HTML-based parsing dominates due to its interpretability, while some frameworks, such as ChatGPTâ€™s web assistant, leverage visual perception ____. In contrast, PC-based frameworks rely on system APIs or automation tools for enhanced control and flexibility ____. In the mobile domain, a key challenge is equipping intelligent agents with operational knowledge, which LLMs often lack. Existing approaches include: (1) training models on operational data, which is costly and lacks scalability ____; (2) enabling autonomous exploration, which is resource-intensive ____; and (3) manually generating knowledge, which is inefficient and relies on iterative human intervention ____.

\subsection{Video-guided Agent}
Video guidance has become a crucial modality for training intelligent agents, enabling them to understand and interact with dynamic environments efficiently. Early works focus on using large language models (LLMs) as central agents for video comprehension. Extending this idea, ____ improves long-term temporal comprehension. Beyond understanding, video guidance has been leveraged for real-world applications. ____ integrates LLMs into video editing workflows and automates language-based video descriptions and edits. Similarly, ____ introduces an efficient method to retrieve relevant video frames, enabling structured video processing. In robotics, ____ utilizes human demonstration videos to teach robots new manipulation skills without explicit supervision. These works demonstrate the growing role of video-guided agents, from video comprehension and retrieval to real-world task execution, forming the foundation for more advanced multimodal learning systems.