% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{bbding}
\usepackage{colortbl}
\usepackage{CJKutf8}
\usepackage{booktabs}
\usepackage{amsmath}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

% \author{Junyang Wang$^1$\thanks{Work done during internship at Alibaba Group.} \quad Haiyang Xu$^2$\thanks{Corresponding author} \quad Xi Zhang$^2$ \quad Ming Yan$^2$\footnotemark[2] \\ \textbf{Ji Zhang$^2$ \quad Fei Huang$^2$ \quad Jitao Sang$^1$\footnotemark[2]}\\
% {\texttt \{junyangwang, jtsang\}@bjtu.edu.cn \\ \{shuofeng.xhy, ym119608\}@alibaba-inc.com} \\
% $^1$Beijing Jiaotong University \quad $^2$Alibaba Group\\}

\author{
 \textbf{Junyang Wang\textsuperscript{1}\thanks{Work done during internship at Alibaba Group.}},
 \textbf{Haiyang Xu\textsuperscript{2}\thanks{Corresponding author}},
 \textbf{Xi Zhang\textsuperscript{2}},
 \textbf{Ming Yan\textsuperscript{2}\footnotemark[2]},
\\
 \textbf{Ji Zhang\textsuperscript{2}},
 \textbf{Fei Huang\textsuperscript{2}},
 \textbf{Jitao Sang\textsuperscript{1}\footnotemark[2]},
\\
 \textsuperscript{1}Beijing Jiaotong University,
 \textsuperscript{2}Alibaba Group,
\\
 \small{
   {\{junyangwang, jtsang\}@bjtu.edu.cn}
 }
\\
 \small{
   {\{shuofeng.xhy, ym119608\}@alibaba-inc.com}
 }
}

\begin{document}
\maketitle
\begin{abstract}
The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, a framework that leverages video guidance to provide rich and cost-effective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a sliding window strategy and incorporates a video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that Mobile-Agent-V achieves a 30\% performance improvement compared to existing frameworks. The code will be open-sourced at \url{https://github.com/X-PLUG/MobileAgent}.
\end{abstract}

\section{Introduction}

The reliance on mobile devices has increased, with users performing numerous operations daily, underscoring the need for streamlined interactions. Currently, artificial intelligence advances mobile automation, enhancing productivity research. Systems such as ChatGPT and Claude enable devices to autonomously handle tasks based on user input.

The development of Multimodal Large Language Models (MLLMs) has notably improved mobile device operating frameworks, using these models as intelligent agents \citep{liu2023visual,zhu2023minigpt,ye2023mplug,dai2023instructblip,liu2023improved,chen2023minigpt,bai2023qwen,ye2023mplugowl2,wang2023cogvlm,lu2024deepseek,ye2024mplug,wu2024deepseek}. These frameworks leverage agents' perception, decision-making, and reflection to perform complex tasks across multiple applications, thereby broadening mobile devices' autonomous capabilities.

Despite progress, existing approaches remain constrained by limited operational knowledge. As shown in Figure~\ref{fig:intro}, current agents struggle with tasks like disabling location recording during photography, even after extensive exploration. This limitation arises from the lack of comprehensive training data, the rapid obsolescence of learned knowledge due to app updates, and the inaccessibility of device-specific operational information. While methods such as Odyssey leverage external task paths, they face scalability and data collection challenges \citep{lu2024gui}. AppAgent's self-exploration is costly due to lengthy task sequences \citep{yang2023appagent}, and Mobile-Agent-V2's reliance on manual programming remains inefficient \citep{wang2024mobile2}. These challenges underscore the need for more scalable and adaptable solutions in mobile automation.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{intro.png}
    \caption{Comparison between a baseline agent, manually written knowledge, and Mobile-Agent-V. The baseline agent, lacking operation knowledge, struggles to complete the task, requiring excessive steps and still failing. Manually written knowledge requires documentation and iterative verification. In contrast, Mobile-Agent-V leverages operation videos, requiring only execution and recording, making knowledge injection far more efficient.}
    \label{fig:intro}
    \vspace{-3mm}
\end{figure}

To address these limitations, we propose video guidance as a solution. Unlike manually written knowledge, which requires extensive exploration, documentation, and iterative debugging, recording an operation video locally takes only about one minute. In contrast, an agent exploring the task requires numerous interaction steps and substantial time, while manually writing and verifying instructions demands both significant human effort and time. Thus, using videos as a means of knowledge injection offers greater scalability for large-scale deployment. As shown in Figuree~\ref{fig:intro}, under the guidance of the video, the agent can efficiently complete instructions that were originally impossible to complete. However, processing interaction videos poses significant challenges: the vast number of frames incurs high computational costs, user interactions are interwoven with dynamic Graphical User Interfaces (GUIs) changes, and redundant visual content makes it difficult to extract relevant information, requiring models to distinguish meaningful actions from background variations. Given that most MLLMs are optimized for static image inputs with tightly coupled vision-language alignment, direct video processing is impractical. Instead, we hope to avoid any complex video processing, relying solely on uniform sampling and similarity-based filtering to minimize computational overhead.

To achieve this, we introduce Mobile-Agent-V, a multi-agent collaboration framework designed to process operational video inputs, extract actionable knowledge, and apply it to mobile device interactions. To reduce keyframe redundancy while ensuring critical information is retained, we introduce a sliding window video input mechanism, where only a subset of keyframes is fed into the decision agent. A video agent analyzes the device's current state and adaptively shifts the window forward, ensuring that the selected frames remain relevant for decision-making. However, the resulting multi-frame input still poses challenges for MLLMs in maintaining contextual coherence. To improve accuracy, we introduce a reflection agent with long-chain-of-thought reasoning to analyze the video and refine the decision agent’s outputs, enabling error correction when necessary. Experimental results demonstrate that Mobile-Agent-V achieves a 30\% performance improvement over existing methods in tasks requiring operational knowledge.

Our summarized contributions are as follows:
\begin{itemize}
    \item We introduce Mobile-Agent-V, a novel framework that applies video guidance to enable autonomous mobile device operations.
    \item To address challenges in processing long-context video input, we propose a sliding window strategy alongside a video agent, ensuring effective guidance through keyframes.
    \item We enhance the decision accuracy by incorporating a deep-reflection agent. Experimental results show that Mobile-Agent-V achieves a performance improvement of 30\% compared to existing frameworks.
\end{itemize}

\section{Related Work}

\subsection{GUI Agent}
To improve user experience, intelligent agent frameworks powered by Large Language Models (LLMs) are rapidly advancing in GUI operations \citep{wang2024gui,liu2025llm}. On the Web, HTML-based parsing dominates due to its interpretability, while some frameworks, such as ChatGPT’s web assistant, leverage visual perception \citep{zhou2023webarena,deng2023mindweb,zheng2024gpt,he2024webvoyager,lu2024weblinx,yoran2024assistantbench,reddy2024infogent}. In contrast, PC-based frameworks rely on system APIs or automation tools for enhanced control and flexibility \citep{zhang2024ufo,tan2024towards,xie2024osworld}. In the mobile domain, a key challenge is equipping intelligent agents with operational knowledge, which LLMs often lack. Existing approaches include: (1) training models on operational data, which is costly and lacks scalability \citep{hong2023cogagent,cheng2024seeclick,you2024ferret,zhang2024android,chen2024octopus,lu2024gui,chai2024amex,rawles2024androidworld,xu2024androidlab,li2024effects,wan2024omniparser,xing2024understanding,liu2024autoglm}; (2) enabling autonomous exploration, which is resource-intensive \citep{yang2023appagent,wang2024mobile,li2024appagent,wang2025mobile}; and (3) manually generating knowledge, which is inefficient and relies on iterative human intervention \citep{wang2024mobile2}.

\subsection{Video-guided Agent}
Video guidance has become a crucial modality for training intelligent agents, enabling them to understand and interact with dynamic environments efficiently. Early works focus on using large language models (LLMs) as central agents for video comprehension. Extending this idea, \citep{wang2024videoagent} improves long-term temporal comprehension. Beyond understanding, video guidance has been leveraged for real-world applications. \citep{wang2024lave} integrates LLMs into video editing workflows and automates language-based video descriptions and edits. Similarly, \citep{zhang2024omagent} introduces an efficient method to retrieve relevant video frames, enabling structured video processing. In robotics, \citep{chane2023learning} utilizes human demonstration videos to teach robots new manipulation skills without explicit supervision. These works demonstrate the growing role of video-guided agents, from video comprehension and retrieval to real-world task execution, forming the foundation for more advanced multimodal learning systems.

\section{Mobile-Agent-V}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{framework.png}
    \caption{The framework of Mobile-Agent-V.}
    \label{fig:framework}
    \vspace{-3mm}
\end{figure*}

This section introduces Mobile-Agent-V, a framework that enhances mobile automation through video guidance. We outline its key components, including video processing, sliding window, video agent, deep-reflection agent, decision agent, and explain how they work together to improve operational efficiency and accuracy.

\subsection{Framework}
The overall workflow of Mobile-Agent-V is shown in Figure~\ref{fig:framework}. Given an input video $V$ that captures a demonstrated task, the system first extracts keyframes $F'$ through uniform sampling and redundancy removal. The execution begins with an initial sliding window positioned at the start of the keyframe sequence. At each iteration, the decision agent generates an action $O_i$ based on the current window, video instructions, and historical decisions. If the task is successfully completed, the process terminates. Otherwise, the deep-reflection agent validates and refines the action to ensure alignment with the demonstrated task. The refined decision $RO_i$ is then executed on the device, updating its state to $D_{i+1}$. The video agent subsequently determines the next window starting point $S_{i+1}$, facilitating a dynamic adjustment of the observation scope as the task progresses. This iterative procedure continues until the task is completed or the predefined maximum exploration limit is reached. The complete pipeline is outlined in Algorithm~\ref{pipeline}.

\subsection{Video Processing}
Traditional uniform sampling, commonly used in video understanding, is effective only for real-world videos with relatively static scenes and continuous motion between frames. However, in mobile recordings, most of the frames remain static, while the remaining frames change rapidly due to intermittent human interaction and fast device responses. This makes uniform sampling insufficient for mobile device videos.

To address this, we first uniformly sample the $V$ at a frequency $d$ to obtain the keyframe set $F$:
\begin{equation}
\label{uniform_sampling}
    F = \text{Uniform\_Sampling}(V, d)
\end{equation}
Next, we compute the similarity between consecutive keyframes and remove those with similarity above a threshold $s$, resulting in a reduced set $F_s$:
\begin{equation}
\label{similarity}
    F_s = \{ f_i \in F \mid \text{sim}(f_i, f_{i+1}) \leq s \}
\end{equation}
Finally, we filter out keyframes with temporal gaps smaller than a threshold $f_s$, yielding the final set of keyframes $F'$:
\begin{equation}
\label{key_frame}
    F' = \{ f_i \in F_s \mid t_{i+1} - t_i \geq d \}
\end{equation}
where $t_i$ represents the frame index of $f_i$.

\subsection{Sliding Window}
To improve video comprehension by MLLMs, we reduce the input length by selecting only the keyframes relevant to the current operation. This is achieved using a sliding window, where the keyframes between the window's start and end points $V_w$ serve as the input for decision-making:
\begin{equation}
\label{window}
    V_w = \{F'_k\}_{k=S_i}^{S_i+W}
\end{equation}
where the $w$ is the length of the window.

Ideally, with accurate keyframe extraction, the window size would be 2, covering the states before and after the operation to predict state transitions. However, for enhanced robustness, the window size is typically greater than 2, and the start point is shifted backward to capture previous states for better context.

\subsection{Decision Agent}
\noindent \textbf{Action Space.} The decision agent is responsible for generating actions that alter the device state. To ensure seamless execution via operational tools, we adopt an action space similar to existing frameworks. Mobile-Agent-V defines six fundamental actions: \textit{Click}, \textit{Scroll}, \textit{Type}, \textit{Back}, \textit{Home}, and \textit{Done}. A detailed description of the operating space is shown in the Appendix~\ref{action_space_detail}. These correspond to tapping a specific location, scrolling in a designated direction, entering text into an active input field, navigating to the previous page, returning to the home screen, and completing the task, respectively.

\noindent \textbf{Decision Making.} Unlike prior approaches that rely on internal operational knowledge, the decision agent in Mobile-Agent-V derives actions directly from video content. This imposes higher demands on contextual adherence. By leveraging the sliding window mechanism, we filter out irrelevant frames, reducing input length while preserving critical information. The $i$-th operation $O_i$ follows the steps outlined in the following equation:
\begin{equation}
    O_i = Da(Vw_i, I_v, D_i, I_u, \{O_k\}_{k=1}^{i-1})
\end{equation}
where $Da(\cdot)$ is the decision agent, $I_v$ is the instruction completed in the video, $D_i$ is the screenshot of the device during the $i$-th operation, and $I_u$ is the instruction that the user will complete on the current device. Besides this, to track the progress, we also provide the historical operations $\{O_k\}_{k=1}^{i-1}$ to the decision agent.

\subsection{Deep-Reflection Agent}
Even with a sliding window, handling low-quality keyframes necessitates increasing the window size, as a smaller window may be dominated by redundant frames, preventing critical keyframes from being included. In scenarios where perfect keyframe extraction cannot be ensured, the decision agent still faces challenges in reasoning over long multi-frame sequences. To address this, we introduce the deep-reflection agent, which performs in-depth validation and refinement of the decision agent’s outputs. Specifically, it follows a structured process: analyzing each operation in the video, identifying the current device state within the recorded sequence, verifying whether the decision agent’s action aligns with the corresponding operation in the video, and, if discrepancies are detected, refining the action based on the observed trajectory. This reflection mechanism enhances decision accuracy by ensuring strict adherence to the demonstrated operations, leading to a final refined decision $RO_i$, formulated as follows:
\begin{equation}
    RO_i = Ra(Vw_i, I_v, D_i, I_u, O_i)
\end{equation}

\subsection{Video Agent}
To dynamically adjust the sliding window throughout task execution, we introduce the video agent. Initially, the window spans from the first keyframe to the $W$-th keyframe. After each operation, the video agent analyzes the screenshots before and after the operation, keyframes within the current window, and user inputs to identify the corresponding keyframe. Then, it determines the updated window starting point, ensuring adaptive progression. The following is the formula for obtaining the starting point of the $i+1$-th sliding window:
\begin{equation}
    S_{i+1} = Va(Vw_i, I_v, R_i, I_u)
\end{equation}
where $Va(\cdot)$ is the video agent, and $R_i$ is the set of screenshots before and after the operation:
\begin{equation}
    R_i = \{D_k\}_{k=i}^{i+1}
\end{equation}

Moreover, the video agent handles anomalies such as incorrect transitions leading to unintended states or discrepancies caused by redundant or missing keyframes. To enhance reliability, it can flag inconsistencies and generate diagnostic feedback, facilitating error recovery and improving decision-making robustness.

\begin{algorithm}[!ht]
\caption{Mobile-Agent-V pipeline}
\label{pipeline}
\begin{flushleft}
\textbf{Input:}
Video $V$, Window length $W$, Video task $I_v$, User task $I_u$, Decision agent $Da$, Reflection agent $Ra$, Video agent $Va$, Max explorations $M_e$\\
\end{flushleft}
\begin{algorithmic}[1]
\State \textbf{Initialization:}
\State Obtain $F'$ from $V$ as Equ.~(\ref{uniform_sampling})~(\ref{similarity})~(\ref{key_frame})
\State $S_1 \gets$ 1
\For{$i = 1$ \textbf{to} $M_e$}
    \State Obtain $V_{w_i}$ from $F'_k$ as Equ.(~\ref{window})
    \State $O_i \gets Da(Vw_i, I_v, D_i, I_u, \{O_k\}_{k=1}^{i-1})$
    \If{$O_i$ == Done}
        \State break
    \EndIf
    \State $RO_i \gets Ra(Vw_i, I_v, D_i, I_u, O_i)$
    \State $D_{i+1} \gets $ Execute $RO_i$ on Device
    \State $R_i \gets \{D_k\}_{k=i}^{i+1}$
    \State $S_{i+1} \gets Va(Vw_i, I_v, R_i, I_u)$
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Experiments}
This section presents a comprehensive evaluation of Mobile-Agent-V. We first introduce the evaluation methodology. Next, we describe the experimental setup. We then report the main results. Finally, we conduct qualitative analyses and ablation studies to further examine the contributions of individual components.

\begin{table*}[t]
	\centering
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{6pt}
	\scalebox{0.85}{
	\begin{tabular}{l|c c c c c c c c c c c c}
        \hline
		\toprule
		\multicolumn{1}{l}{\multirow{2}{*}{\textbf{Method}}}&\multicolumn{4}{c}{\textbf{Basic Instruction}}&\multicolumn{4}{c}{\textbf{Normal Instruction}}&\multicolumn{4}{c}{\textbf{Advanced Instruction}}\\
        \cmidrule(lr){2-5}
        \cmidrule(lr){6-9}
        \cmidrule(lr){10-13}
        \multicolumn{1}{c}{}&\textbf{SR}&\textbf{CR}&\textbf{DA}&\textbf{Step}&\textbf{SR}&\textbf{CR}&\textbf{DA}&\textbf{Step}&\textbf{SR}&\textbf{CR}&\textbf{DA}&\textbf{Step}\\
        \midrule
        AppAgent&90&85.0&78.0&5.5&50&70.0&50.5&12.0&10&40.5&25.0&19.1\\
        Mobile-Agent-v1&80&86.5&79.5&5.3&40&72.5&48.0&11.5&10&43.0&27.8&19.8\\
        Mobile-Agent-v2&90&90.0&84.3&5.0&60&76.3&54.4&10.5&20&49.3&31.2&18.6\\
        \midrule
        Mobile-Agent-V&\textbf{100}&\textbf{100}&97.8&\textbf{4.5}&90&93.3&80.3&6.6&\textbf{70}&\textbf{86.8}&\textbf{60.1}&10.9\\
        $\Delta$&\textcolor{blue}{+10}&\textcolor{blue}{+10.0}&\textcolor{blue}{+13.5}&\textcolor{blue}{-0.5}&\textcolor{blue}{+30}&\textcolor{blue}{+17.0}&\textcolor{blue}{+25.9}&\textcolor{blue}{-3.9}&\textcolor{blue}{+50}&\textcolor{blue}{+37.5}&\textcolor{blue}{+18.9}&\textcolor{blue}{-7.7}\\
        \midrule
        \rowcolor{gray!15}
        Human-Know.&\textbf{100}&\textbf{100}&\textbf{98.9}&\textbf{4.5}&\textbf{100}&\textbf{95.2}&\textbf{83.7}&\textbf{6.2}&\textbf{70}&82.6&58.8&\textbf{10.7}\\
        \bottomrule
        \hline
	\end{tabular}
	}
    \caption{Evaluation results across different instructions, where Human-Know. denotes human-curated knowledge.}
	\label{tb:in-domain}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{CD.png}
    \caption{Comparison of video-misaligned instructions and video-aligned instructions. The in-domain means that the video instruction is consistent with the user instruction, and the cross-domain instruction is inconsistent.}
    \label{fig:CD}
    \vspace{-3mm}
\end{figure*}

\subsection{Evaluation}
\subsubsection{Benchmark}
To assess Mobile-Agent-V’s ability to acquire and apply operational knowledge from videos, we design a benchmark requiring extensive external knowledge. The complete instructions are shown in the Appendix~\ref{benchmark_detail}. The tasks involve device-specific operations that typically necessitate consulting manuals to determine optimal execution paths. Since such knowledge is rarely embedded in mainstream MLLMs, this benchmark effectively evaluates the knowledge augmentation provided by video guidance. It comprises three difficulty levels:  

\noindent \textbf{Simple instructions} involve common operations with minimal device-specific interactions, solvable through brief exploration even without prior knowledge. For example, turning off automatic brightness is a simple instruction, as most MLLMs are aware of the operation path.

\noindent \textbf{Normal instructions} include medium-frequency operations requiring some device-specific interactions. Successful completion demands either prior knowledge or basic exploration. For instance, disabling the status bar network speed display is a normal instruction—while the prompt clearly specifies that the setting is in the status bar options, the agent still needs to explore to locate the exact entry.

\noindent \textbf{Advanced instructions} consist of low-frequency, complex operations heavily dependent on device-specific knowledge. Without external knowledge, these tasks are difficult to complete through exploration. For example, enabling three-finger screenshot is an advanced instruction, as its setting is buried within a third-level menu, requiring multiple rounds of exploration to locate the entry. 

\subsubsection{Metrics}  
We evaluate Mobile-Agent-V using four key metrics: Success Rate (SR), Completion Rate (CR), Decision Accuracy (DA), and Step Count (Step).  

\begin{itemize}  
\item \textbf{Success Rate}: The proportion of fully completed instructions, measuring end-to-end task execution where all steps must be correct.  
\item \textbf{Completion Rate}: The percentage of completed steps, providing a finer-grained measure of task progress.  
\item \textbf{Decision Accuracy}: The ratio of correct decisions to total decisions, assessing the agent’s action selection accuracy.  
\item \textbf{Step Count}: The number of steps taken to complete an instruction, reflecting execution efficiency. If the task is incomplete when reaching the step limit, the count is set to this upper bound.  
\end{itemize}

\subsection{Setup}
\noindent \textbf{Baselines.} We compare Mobile-Agent-V with several open-source agent frameworks, including AppAgent~\cite{yang2023appagent}, Mobile-Agent~\cite{wang2024mobile}, and Mobile-Agent-v2~\cite{wang2024mobile2}. To assess its ability to learn operational knowledge from videos, we introduce a human-curated knowledge baseline, where an expert manually extracts key operational steps from the video and provides them as textual input. This text replaces the video input in Mobile-Agent-V.

\noindent \textbf{Models.} Both Mobile-Agent-V and baselines utilize GPT-4o as their MLLMs, ensuring consistency with the baselines. The model is accessed via the official API with default hyperparameters.

\noindent \textbf{Device and Interaction.} Experiments are conducted on a OnePlus 7 Pro smartphone using the Android Debug Bridge (ADB) for interaction, maintaining consistency with baselines. Clickable positions are extracted from the device’s XML hierarchy, visually marked on screenshots, and used by the agent for precise action selection.

\subsection{Main Results}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{window.png}
    \caption{Comparison of different sliding window sizes.}
    \label{fig:window}
    \vspace{-3mm}
\end{figure*}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.35\textwidth]{keyframe.png}
    \caption{Comparison of different keyframe quality.}
    \label{fig:keyframe}
    \vspace{-3mm}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{DR.png}
    \caption{Comparison of w/o DR and w/ DR across different instructions.}
    \label{fig:DR}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.85\textwidth]{case.jpg}
    \caption{A complete execution case of Mobile-Agent-V. The decision agent initially makes an incorrect action, but the deep-reflection agent verifies the operation video, compares the device state, and corrects the action.}
    \label{fig:case}
    \vspace{-3mm}
\end{figure*}

Table~\ref{tb:in-domain} shows that Mobile-Agent-V consistently outperforms baselines across various instruction difficulty levels. For basic instructions, while baselines generally achieve high success rates, Mobile-Agent-V reaches 100\% success with 97.8\% decision accuracy and an average of 4.5 steps. For Normal Instructions, it significantly improves both success and completion rates while reducing the step count. In Advanced Instructions, Mobile-Agent-V achieves a 70\% success rate—50\% higher than the best baseline—along with improved completion and decision accuracy, nearly halving the steps.

Further analysis indicates that baseline methods, despite their performance on Basic Instructions, suffer from lower decision accuracy that leads to redundant actions. In contrast, Mobile-Agent-V’s precise decision-making streamlines task execution, particularly for complex operations. By effectively extracting and applying video-based knowledge, Mobile-Agent-V overcomes the limitations of relying solely on built-in operational rules. Although human-curated knowledge still offers slight advantages for Advanced Instructions, the minimal performance gap demonstrates that video-based learning is a viable alternative to manual annotations, achieving near-human efficiency.

It is worth noting that using videos can significantly reduce the preparation time for running compared to manually writing knowledge. We show the specific time required to record a video of an instruction and compile the knowledge of an instruction in Appendix~\ref{cost_time}.

\subsection{Analysis}
\subsubsection{Generalization from Videos}
We constructed the Video-Misaligned task by modifying the original instructions so that the logic of the operations in the video aligns with the user task, but the specific actions differ. This setup aims to test whether Mobile-Agent-V can generalize operation types from a video demonstration. As shown in Figure~\ref{fig:CD}, Mobile-Agent-V shows a performance drop in the Video-Misaligned condition. While basic instructions remain stable, normal instructions and advanced instructions exhibit more significant declines in success rate and decision accuracy. Nevertheless, the system still completes tasks reasonably well in the misaligned condition, indicating its ability to generalize operational knowledge beyond direct instruction mapping. These results highlight the value of diverse video demonstrations to enhance cross-instruction generalization.

\subsubsection{Impact of Window Size}
Figure~\ref{fig:window} illustrates the effect of window size on task performance. Larger windows generally improve SR, CR, and DA while reducing steps, particularly for more complex tasks. However, beyond a certain threshold, further increasing the window size yields diminishing returns, with some metrics even declining. This decline is likely due to the introduction of irrelevant information, which interferes with decision-making. These findings highlight the importance of balancing temporal context to maximize efficiency.

\subsubsection{Impact of Keyframe Quality}
To investigate the impact of keyframe quality, we compare artificial sampling, where keyframes are manually selected to avoid redundancy and omission, with our uniform sampling and filtering strategy in Table~\ref{fig:keyframe}. As expected, manually chosen keyframes yield slightly better results, confirming that high-quality keyframes enhance performance. However, the gap between our method and manual selection remains small, demonstrating the effectiveness of our method in preserving essential task-relevant information.

\subsection{Ablation Study}
To evaluate the effectiveness of the deep-reflection Agent, we conduct an ablation study by comparing performance with and without deep reflection agent, as shown in Figure~\ref{fig:DR}. The results demonstrate that deep reflection agent consistently improves decision-making across various metrics. In cases where the SR and CR are already high, the improvements are marginal, as the decision agent is less prone to errors. However, for more complex tasks with lower baseline performance, incorporating deep reflection agent yields substantial gains, particularly in DA. This highlights its role in refining actions and mitigating inconsistencies in long multi-frame reasoning. The Step exhibits minor variations, suggesting that while deep reflection agent enhances precision, it does not significantly alter action efficiency. The deep reflection agent refines the decision agent’s outputs by correcting misalignments between predicted and actual actions, mitigating cascading errors in long-horizon tasks. It reduces reliance on perfect keyframe extraction, ensuring robustness in suboptimal visual conditions and improving overall reliability.

\subsection{Case Study}
Figure~\ref{fig:case} illustrates a multi-agent collaboration scenario within Mobile-Agent-V. The decision agent utilizes keyframes from a sliding window to determine the operation. However, an error occurs as it skips the "confirm contact" step, highlighting the challenge of multi-image action following.

To correct this, the deep-reflection agent identifies the misalignment and refines the decision, ensuring the correct operation is executed on the device. Meanwhile, the video agent observes the device state and anchors it to the fourth frame in the sliding window, subsequently shifting the window forward by two frames. This adjustment enables the system to display the next interaction, where the contact card is correctly selected.

This case demonstrates the seamless coordination within Mobile-Agent-V, effectively leveraging video-based knowledge to enhance task execution and ensure robust decision-making.

\section{Conclusion}
We introduce Mobile-Agent-V, a novel framework that leverages video guidance to enhance mobile automation by providing rich, cost-effective operational knowledge. To efficiently process video inputs, we propose a sliding window mechanism with a video agent that dynamically adjusts keyframes based on device states. Additionally, a deep-reflection agent improves decision accuracy through iterative reasoning. Experimental results show that Mobile-Agent-V outperforms existing frameworks by up to 30\%, demonstrating the effectiveness of video-based knowledge injection. In addition, video guidance can achieve an effect close to manually written knowledge while saving 80\% of the time. Mobile-Agent-V enables scalable conversion of videos into operational knowledge, providing a new pathway for agent learning.

\section{Limitations}
First, the reliance on video inputs introduces variability in data quality, as suboptimal recordings may affect knowledge extraction. Additionally, although the sliding window mechanism enhances efficiency, it may still overlook critical frames in complex interactions. Lastly, while our framework generalizes across various tasks, its effectiveness may vary depending on the diversity of available video demonstrations. Future work can explore adaptive mechanisms to further optimize efficiency and robustness.

\bibliography{reference}

\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Experimental Details}

\begin{table*}[h]
    \centering
    \renewcommand{\arraystretch}{1}
    \begin{tabular}{l|l|p{11cm}}
        \hline
		\toprule
        \textbf{Action} & \textbf{Parameter} & \textbf{Description} \\
        \hline
		\toprule
        Click & id & The "id" represents the numeric identifier of the detection box to be clicked. \\
        \hline
        Click\_text & text & The "text" specifies the target text to be clicked, used only when no detection box or corresponding ID exists at the target location. \\
        \hline
        Scroll & direction & The "direction" can be either "up" or "down," allowing the agent to scroll the screen accordingly. \\
        \hline
        Type & text & The "text" parameter defines the content to be entered into a text field. \\
        \hline
        Back & None & Returns to the previous screen. \\
        \hline
        Home & None & Navigates to the home screen. \\
        \hline
        Done & None & Signals task completion. \\
        \bottomrule
        \hline
    \end{tabular}
    \caption{Action space definition for Mobile-Agent-V.}
    \label{action_space}
\end{table*}

This section provides additional details regarding the experimental setup and implementation choices used in Mobile-Agent-V.

\subsubsection{Sliding Window Size Selection}
In our experiments, the sliding window size was set to 4. While increasing the window size to 5 is also feasible, experimental analysis demonstrated that the performance improvement was marginal, while the computational cost increased due to the higher token consumption. Therefore, we adopted a window size of 4 as a balanced trade-off between efficiency and performance.

\subsubsection{Video Similarity Computation}
To compute the similarity between video frames, we employed a simple yet effective approach based on pixel-wise differences. Given two frames $I_1$ and $I_2$, we first converted them to grayscale representations:
\begin{equation}
I'_1 = \text{grayscale}(I_1), \quad I'_2 = \text{grayscale}(I_2)
\end{equation}
Next, we computed the absolute difference between the two grayscale images:
\begin{equation}
D = \text{absdiff}(I'_1, I'_2)
\end{equation}
Finally, the similarity score $S$ was obtained by counting the number of nonzero pixels in $D$:
\begin{equation}
S = \frac{\text{np.count\_nonzero}(D)}{\text{total pixels}}
\end{equation}
This method effectively captures differences between frames while maintaining computational efficiency.

\subsubsection{Frame Similarity Threshold Selection}
As described in the main text, the similarity threshold $f_s$ was adjusted according to the characteristics of different applications. For instance, in the \textit{Settings} app, where UI changes are primarily text-based, we set $f_s=0.3$ to ensure that more informative frames were retained. Conversely, for the \textit{Weather} app, where UI elements exhibit significant visual variations, a higher threshold of $f_s=0.5$ was used to prevent excessive redundant frame extraction.

\subsubsection{Step Limitations and Task Termination Criteria}
To ensure fair evaluation and prevent infinite loops, we imposed an upper bound on the number of execution steps:
\begin{itemize}
    \item Basic tasks: 10-step limit.
    \item Standard tasks: 15-step limit.
    \item Complex tasks: 20-step limit.
\end{itemize}
If an agent reached the step limit without successfully completing the task, the attempt was deemed a failure. Additionally, if a framework executed the required action but continued performing unnecessary operations beyond the instruction’s scope, it was also considered a failure.

\subsubsection{Video Frame Concatenation for Visualization}
To facilitate interpretation, video frames were concatenated in a row-wise fashion. Each frame within the sliding window was annotated with an index to assist the video agent in tracking progress. If fewer than four frames were available in the window, only the existing frames (up to three) were concatenated. The final frame of each sequence was explicitly labeled as the termination state to guide the decision agent in stopping at the appropriate point.

\subsubsection{Action Space Definition}
\label{action_space_detail}
Mobile-Agent-V adopted the same action space as Mobile-Agent-V2. Unlike Mobile-Agent-V2, which utilized OCR and segmentation models to determine interaction coordinates, Mobile-Agent-V employed the SoM (Set of Mark) approach to reduce context length. Additionally, to mitigate potential XML parsing issues in certain UI pages, we introduced a supplementary click-by-text operation. The complete action space is detailed in Table~\ref{action_space}.

\subsection{Benchmark Details}
\subsubsection{Evaluation Instruction}
\label{benchmark_detail}
Table~\ref{benchmark} below provides a detailed breakdown of the benchmark tasks, categorized by application. This benchmark provides a structured way to evaluate Mobile-Agent-V’s ability to interpret, align, and execute user instructions of varying complexity. The distinction between video-aligned and video-misaligned instructions ensures robustness against linguistic variations, testing the framework’s adaptability to real-world user interactions.

\subsubsection{Screen Recording}
All videos were recorded using the built-in screen recording tool on a OnePlus 7 Pro test device. While the tool supports a maximum frame rate of 60 Hz, the actual frame rate varies between 30 Hz and 60 Hz depending on the extent of UI changes. All interactions were performed manually at an average frequency of one action every 1–2 seconds. The videos remain unprocessed, with no modifications such as acceleration, editing, or overlays, preserving the original recordings. Each benchmark instruction corresponds to a dedicated operation video, which demonstrates the optimal execution path for the task.

\subsection{Time Consumption}
\label{cost_time}
We measured the time required for recording a video and manually reviewing the video to document a complete operation sequence, as shown in Table~\ref{tab:time}. While video recording takes less than one minute on average, manual documentation requires approximately five minutes. Utilizing videos reduces time consumption by 80\% and eliminates the manual effort involved in writing instructions.

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \scalebox{0.95}{
    \begin{tabular}{l|c}
        \hline
        \toprule
        \textbf{Knowledge Injection Method} & \textbf{Average Time} \\
        \midrule
        Video Recording & <1 min \\
        Manually Writing Knowledge & \textasciitilde 5 min \\
        \bottomrule
        \hline
    \end{tabular}
    }
    \caption{Time consumption for video recording and manual documentation.}
    \label{tab:time}
\end{table}

\begin{table*}[h]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \scalebox{0.85}{
    \begin{tabular}{l|l|p{7cm}|p{7cm}}
        \hline
        \toprule
        APP & \textbf{Level} & \textbf{Video Instruction \& Video-Aligned User Instruction} & \textbf{Video-Misaligned User Instruction} \\
        \hline
        Phone & Basic & Help me dial 123. & Help me dial 321. \\
        & Normal & Please turn on the call recording for me. & Please view all call recording for me. \\
        & Advanced & Help me add the mobile number 1234567890 to the blacklist. & Help me add the mobile number 9876543210 to the whitelist. \\
        \hline
        Messages & Basic & Help me set up messages and notifications to be displayed together in Messages. & Help me set up messages and notifications not to be displayed together in Messages. \\
        & Normal & Please send a message to 123456 with text "Hello" & Please send a message to 9876543210 with text "Goodbye". \\
        & Advanced & Send a message to 123456 with my current location information. & Send a message to 987654 with my contact card. \\
        \hline
        Setting & Basic & Help me turn off the auto brightness in Setting. & Help me turn on the auto brightness in Setting. \\
        & Normal & Help me turn off the status bar network speed display. & Help me turn off the status bar NFC display. \\
        & Advanced & Help me open three-finger screenshots. & Help me open three-finger touch and hold. \\
        \hline
        Photo & Basic & Help me turn on the shared albums setting in Photos. & Help me turn off the shared albums setting in Photos. \\
        & Normal & Help me clear recently deleted photos. & Help me restore recently deleted photos. \\
        & Advanced & Help me set up not to record location when taking photos. & Help me set up not to record properties when taking photos. \\
        \hline
        Manager & Basic & Help me turn on the App cleaner reminder in Phone Manager. & Help me turn off the App cleaner reminder in Phone Manager. \\
        & Normal & Help me turn on the automatic phone call for help. & Help me turn on the automatic phone call for help and countdown sound. \\
        & Advanced & Help me clean up QQ's storage. & Help me clean up WhatsApp's storage. \\
        \hline
        Recorder & Basic & Help me start recording. & Help me stop recording. \\
        & Normal & Help me change the audio format of my recording. & Help me turn on the cloud recording. \\
        & Advanced & Help me show recently deleted recordings. & Help me show call recordings. \\
        \hline
        Files & Basic & Help me view photos in My Files. & Help me view videos in My Files. \\
        & Normal & Help me create a new tag named "test". & Help me create a new tag named "mobile". \\
        & Advanced & Help me turn on the option to show hidden files. & Help me turn off the option to show hidden files. \\
        \hline
        Clock & Basic & Help me start stopwatch in Clock. &  Help me reset stopwatch in Clock. \\
        & Normal & Help me set the gesture to turn off the alarm to swipe up. & Help me set the gesture to turn off the alarm to press button. \\
        & Advanced & Help me delete the last city of the current world clock and add London. & Help me delete the first city of the current world clock and add New York. \\
        \hline
        Weather & Basic & Help me turn on the meteorological alert setting in Weather. &  Help me turn off the meteorological alert setting in Weather. \\
        & Normal & Help me turn on the rain reminder. & Help me turn off the rain reminder. \\
        & Advanced & Help me turn on the UV intensity display and view the UV intensity at your current location. & Help me turn on the Sunset display and view the sunset at your current location. \\
        \hline
        Calendar & Basic & Help me turn on fixed time zone setting in Calendar. &  Help me turn off fixed time zone setting in Calendar. \\
        & Normal & Help me turn on calendar meeting reminders. & Help me turn on fixed time zone. \\
        & Advanced & Help me subscribe to horoscope and choose Aries. & Help me subscribe to today in history. \\
        \bottomrule
        \hline
    \end{tabular}
    }
    \caption{Benchmark tasks with different difficulty levels and instruction variations.}
    \label{benchmark}
    \label{tab:benchmark}
\end{table*}

\begin{table*}[t]
	\centering
	\renewcommand{\arraystretch}{1}
	\setlength{\tabcolsep}{8pt}
	\scalebox{0.9}{
	\begin{tabular}{p{17cm}}
        \hline
		\toprule
  \textbf{System}\\
  \hline
    You are a mobile phone operation assistant. Below is a description of this conversation.\\
    \\
    In the following part, I will upload a large image made up of many screenshots. These screenshots in this image are all from a screen recording of a mobile phone operation. I will tell you the task completed in the screen recording. You need to observe this screen recording.\\
    \\
    Then, you need to complete a new task, which is related to the task in the screen recording. You need to combine the operation experience provided by the screen recording and gradually complete this task. I will upload the current screenshot of the device. There will be many detection boxes on this screenshot, and there will be a number in the upper left and lower right corners of the detection box. You need to perform operations on the current page. In order to better operate the phone, the following are the operation tools you can use:\\
    - Click (id): The "id" is the numeric serial number of the detection box you need to click.\\
    - Click\_text (text): The "text" is the text you need to click. This is only used when the detection box and the corresponding id do not exist at the location to be clicked.\\
    - Scroll (direction): The "direction" selects from "up", "down", "left", and "right". You can scroll the page a certain distance in the specified direction.\\
    - Type (text): The "text" is the content you need to enter.\\
    - Back: You can use this operation to return to the previous page.\\
    - Home: You can use this operation to return to the home page.\\
    - Done: You can use this operation when the task is completed.\\
    \\
    You need to strictly follow the following json output format:\\
    {"Thought": You need to think about how to perform this operation on the current device based on the operation path in the video, "Operation": Select one from the operation tools, "Summary": Briefly summarize this operation}\\
  \midrule
    \textbf{User during the first operation}\\
    \hline
    The first image is the screen recording, in which the tasks are completed: \{$I_v$\}\\
    \\
    The second image is the screenshot of the current device, in which you need to complete the following tasks: \{$I_u$\}\\
    \\
    Note: You need to refer to the operation path in the video more than relying on your own operation experience. Because you may make mistakes.\\
    \\
    Note: You need to refer to the operation path in the video more than relying on your own operation experience. Because you may make mistakes."\\
    <image: $V_w$><image: $D_i$>\\
  \midrule
    \textbf{User during subsequent operations}\\
    \hline
    The first image is the screen recording, in which the tasks are completed: \{$I_v$\}\\
    \\
    The second image is the screenshot of the current device, in which you need to complete the following tasks: \{$I_u$\}\\
    \\
    Here is your operation history:\\
    Step-1: \{operation 1\}\\
    Step-2: \{operation 2\}\\
    ......\\
    Step-n: \{operation n\}\\
    \\
    Note: If the operation history and current device can infer that the task has been completed, use Done.\\
    \\
    Note: You need to refer to the operation path in the video more than relying on your own operation experience. Because you may make mistakes."\\
    <image: $V_w$><image: $D_i$>\\
    \bottomrule
    \hline
	\end{tabular}
	}
    \caption{The prompt for decision agent.}
	\label{tb:prompt_decision}
\end{table*}

\begin{table*}[t]
	\centering
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{8pt}
	\scalebox{1}{
	\begin{tabular}{p{15cm}}
        \hline
		\toprule
  \textbf{System}\\
  \hline
    You are an expert in mobile phone operation. I will upload two images below. The first image is a keyframe mosaic from an operation video, in which the completed task is "\{$I_v$\}"; the second image is a screenshot of the current status of the mobile phone.\\
    \\
    On the mobile phone shown in the second image, the task to be completed is: "\{$I_u$\}". The user will perform the following operation:\\
    \{Operation from decision agent\}\\
    \\
    Now please observe whether this operation conforms to the operation path shown in the first image. If it conforms, please output "True", otherwise please modify the operation content according to the above json format.\\
    \\
    The operation should be:\\
    - Click (id): The "id" is the numeric serial number of the detection box you need to click.\\
    - Click\_text (text): The "text" is the text you need to click. This is only used when the detection box and the corresponding id do not exist at the location to be clicked.\\
    - Scroll (direction): The "direction" selects from "up" and "down". You can scroll the page a certain distance in the specified direction.\\
    - Type (text): The "text" is the content you need to enter.\\
    - Back: You can use this operation to return to the previous page.\\
    - Home: You can use this operation to return to the home page.\\
    - Done: You can use this operation when the task is completed.\\
    Note: If the operation history and current device can infer that the task has been completed, use Done.\\
    \\
    You need to think in the following way:\\
    1. Observe the operation of each step in the video (especially frame-3 and frame-4).\\
    2. Anchor the position of the current device in the video.\\
    3. Complete the current step according to the operation in the video.\\
    Please output your thought about this step by step before you output your response.\\
  \midrule
    \textbf{User}\\
    \hline
    <image: $V_w$><image: $D_i$>\\
    \bottomrule
    \hline
	\end{tabular}
	}
    \caption{The prompt for deep-reflection agent.}
	\label{tb:prompt_reflection}
\end{table*}

\begin{table*}[t]
	\centering
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{8pt}
	\scalebox{1}{
	\begin{tabular}{p{15cm}}
        \hline
		\toprule
  \textbf{System}\\
  \hline
    You are a mobile phone operation assistant. I will provide you with two images. The first image is a long picture of key frames from a mobile phone operation video, which shows a correct operation trajectory to complete the task: \{$I_v$\}. The second image is two screenshots before and after an operation from the user. The user want to complete the task: \{$I_u$\}. Please note that these two images are not necessarily the complete operation trajectories, they may only be part of the continuous operation.\\
    \\
    Although the task shown in the video may not be exactly the same as the task the user needs to complete, there is a strong correlation between the two. So the user is referring to the operation in the video to complete this task.\\
    \\
    Now you need to determine which frame of the video the user is in after the device is operated. You need to use a number to represent it. If the device is in the state between two frames, the previous frame is output.\\
    If the device is not in any frame of the video, please output the number 0 to indicate an operation error and generate an error cause analysis.\\
    \\
    You need to output in the following json format:\\
    \{"Thought": Your thought of current question, "Frame": a number, "Analysis": If Frame is 0, generate an error cause analysis, otherwise output null, "Need\_Back": If Frame is 0, you need to think about how to get back on track. If you need to return to the previous page, please output true. If you need to continue to perform an operation on the current page to get back on track, please output false. If Frame is not 0, please output False directly.\}\\
  \midrule
    \textbf{User}\\
    \hline
    Here are the video and operation:\\
    <image: $V_w$><image: $D_i$>\\
    \bottomrule
    \hline
	\end{tabular}
	}
    \caption{The prompt for video agent.}
	\label{tb:prompt_video}
\end{table*}

\subsection{Prompt}
Table~\ref{tb:prompt_decision}, \ref{tb:prompt_reflection}, and \ref{tb:prompt_video} show the prompts of decision, deep-reflection, and video agent respectively.

\end{document}
