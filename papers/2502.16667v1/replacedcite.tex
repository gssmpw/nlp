\section{Related Work and Background}
\label{relatedwork}

Physicists have long utilized the Lagrangian and Hamiltonian formalisms of mechanics to study the dynamics of physical systems ____. Consider $\mathbf q=\left(q_1, \ldots, q_n\right)$ that represents the generalized coordinates of an $n$-dimensional configuration space, while ${\mathbf p}$ represents the corresponding generalized momenta. We can describe the Hamiltonian $H(\mathbf q, \mathbf p, t)$ as the total energy of the system. This leads to Hamilton's equations as follows:
\begin{equation}
    \dot{q}_i=\frac{\partial H}{\partial p_i}, \quad \dot{p}_i=-\frac{\partial H}{\partial q_i}, \quad i=1, \ldots, n.
\end{equation}

This formalism naturally imbues the phase space $(\mathbf q, \mathbf p) \in \mathbb{R}^{2 d}$ with geometric structures that are vital to the study of these physical systems. One such structure is the symplectic form $\omega$, given by,

\begin{equation}
    \omega=\sum_{i=1}^n \mathrm{~d} p_i \wedge \mathrm{~d} q_i
\end{equation}

Concretely, a map $\Phi_t:(\mathbf q(0), \mathbf p(0)) \mapsto(\mathbf q(t), \mathbf p(t))$ is said to be symplectic if $\Phi_t^* \omega=\omega$. This implies that the flow in the phase space preserves volume and the fundamental two-form $\omega$ ____. 

Inspired by such geometric formulations of mechanics, recent work such as Hamiltonian Neural Networks (HNNs) and Lagrangian Neural Networks (LNNs) have sought to embed physical priors into deep learning architectures ____. In particular, SympNets have emerged as structure-preserving neural architectures designed specifically for learning Hamiltonian systems, ensuring that their learned maps are intrinsically symplectic. They introduce modules that come in three variants: \emph{linear}$\left(\ell_{\text {up }}, \ell_{\text {low }}\right)$,
\emph{activation} $\left(N_{\text {up }}, N_{\text {low }}\right)$ and \emph{gradient}
$\left(G_{\text {up }}, G_{\text {low }}\right)$. These modules decompose symplectic flows into compositions of “unit triangular” symplectic transformations. Based on these modules, these authors propose two architectures, LA-SympNets that are compositions of \emph{linear} and \emph{activation} modules and G-Sympnets that based on \emph{gradient} modules exclusively. These architectures admit universal approximation theorems and crucially, this construction does not require solving ODEs or differentiating a candidate Hamiltonian during training and inference times, which often leads to more efficient optimization compared to other architectures such as HNNs or LNNs. The collection of all SympNets forms a group under composition, ensuring that every learned map is invertible with a closed-form inverse. However, due to the fundamental nature of Hamiltonian systems and symplectic forms, SympNets and similar architectures fail to generalize to dissipative systems where the symplectic structure is no longer preserved ____. While there have been attempts to reconcile dissipative dynamics and control inputs to model realistic physical systems by preserving the symplectic framework, such as the Dissipative SymODEN ____,  they often suffer from the lack of generalization to different systems ____. 

Generalization between different but related physical systems is also vital for deep learning methods to excel at physics modeling. Meta-learning serves as the natural avenue for exploring such strategies. Building on a series of meta-learning strategies ____, the iMODE framework represents a notable advancement in applying deep learning to families of dynamical systems. iMODE integrates multiple new components specifically tailored to continuous-time dynamics by parameterizing a continuous-time flow 
 $\dot{\mathbf x}(t) = \mathbf f_\theta(\mathbf{\mathbf x}(t), t)$ through a neural network similar to that of Neural ODEs (NODEs). When combined with MAML, iMODE introduces explicit \emph{adaptation parameters} that act as a latent space embedding for system-specific characteristics. In this setup, one set of parameters captures the universal dynamics shared across all systems in a family, while another set encodes the idiosyncratic physical parameters that differentiate one system instance from another. However, certain drawbacks still persist. Apart from the lack of physics priors, these existing meta-learning approaches such as iMODE suffer from a lack of scalability____.  

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.27]{schematic_corrected.png}}
\caption{Schematic representation of the \textbf{MetaSym} architecture. The conservative dynamics obtained from a physical system acts as the input to the SymplecticEncoder (blue-dashed box). This module consists of the symplectic network and its inverse, represented by the alternating light blue and dark blue dots. The forward network receives $( \mathbf{q}^{(i)}_t, \mathbf{p}^{(i)}_{t},\Delta t)$ that represents the position and momentum measurements at time $t$ for a given system $i$ during training, while the inverse network receives the position and momentum data from the future time step $(\mathbf q^{(i)}_{t+1},\mathbf p^{(i)}_{t+1},\Delta t)$. The SymplecticEncoder outputs the latent space $\mathbf z_c$. This process can be repeated for different systems. The sequence of the non-conservative canonical coordinates and control signals, $\left\{\mathbf{\tilde{q}}_{t:T}^{(i)}, \mathbf{\tilde{p}}_{t:T}^{(i)}, \Delta t, \mathbf{u}_{t:T}^{(i)}\right\}$,  act as inputs to the ControlNet (purple box) whose output is supplied to the ActiveDecoder (red dashed box). The ActiveDecoder consists of a layer normalization followed by a self attention layer that is then followed by layer normalization again. This is then provided to the cross attention layer which consists of keys, queries and values. The queries and values are finetuned based on a meta-update. This is finally followed by layer normalization and a multilayered perceptron (MLP) that outputs the position and momentum for the next time $(\hat{\mathbf q}^{(i)}_{t+1},\hat{\mathbf p}^{(i)}_{t+1})$. During training, we provide the ground truth sequence following a teacher-forcing paradigm (black arrow). During test time, we use the network's output for autoregressive prediction (black dotted line). } 
\label{schematic}
\end{center}
\vskip -0.3in
\end{figure*}