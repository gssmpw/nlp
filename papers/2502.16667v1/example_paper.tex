%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{MetaSym: A Symplectic Meta-learning Framework for Physical Intelligence}

\begin{document}

\twocolumn[
\icmltitle{MetaSym: A Symplectic Meta-learning Framework for Physical Intelligence}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Pranav Vaidhyanathan}{equal,oxford}
\icmlauthor{Aristotelis Papatheodorou}{equal,oxford}
\icmlauthor{Mark T. Mitchison}{dublin}
\icmlauthor{Natalia Ares}{oxford}
\icmlauthor{Ioannis Havoutis}{oxford}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{oxford}{University of Oxford, United Kingdom}
\icmlaffiliation{dublin}{Trinity College Dublin, Ireland}

\icmlcorrespondingauthor{Aristotelis Papatheodorou}{aristotelis@robots.ox.ac.uk}
\icmlcorrespondingauthor{Ioannis Havoutis}{ioannis@robots.ox.ac.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Scalable and generalizable physics-aware deep learning has long been considered a significant challenge with various applications across diverse domains ranging from robotics to molecular dynamics. Central to almost all physical systems are \emph{symplectic forms}—the geometric backbone that underpins fundamental invariants like energy and momentum. In this work, we introduce a novel deep learning architecture, \textbf{MetaSym}. In particular, MetaSym combines a strong symplectic inductive bias obtained from a symplectic encoder, and an autoregressive decoder with meta-attention. This principled design ensures that core physical invariants remain intact, while allowing flexible, data-efficient adaptation to system heterogeneities. We benchmark MetaSym on highly varied datasets such as a high-dimensional spring mesh system \cite{otness2021extensiblebenchmarksuitelearning}, an open quantum system with dissipation and measurement backaction, and robotics-inspired quadrotor dynamics. Our results demonstrate superior performance in modeling dynamics under few-shot adaptation, outperforming state-of-the-art baselines with far larger models. 
\end{abstract}

\section{Introduction}
\label{introduction}

Learning to predict the dynamics of physical systems is a fundamental challenge in scientific machine learning, with applications ranging from robotics, control, climate science, and quantum computing \cite{ghadami2022data, zhang2024artificialintelligencesciencequantum, alexeev2024artificialintelligencequantumcomputing}. Traditional approaches often rely on carefully derived differential equations that embed known conservation laws and geometric properties \cite{10801374} (e.g., Hamiltonian or Lagrangian mechanics). Although these classical models have been tremendously successful in capturing fundamental physics, they can become unwieldy or intractable when confronted with complex real-world phenomena—such as high-dimensional systems, intricate interactions, or partially unknown forces—that defy simple closed-form representations. Recent progress in deep learning has opened new avenues for data-driven modeling of dynamical systems, bypassing the need for complete analytical descriptions \cite{RevModPhys.91.045002}. Neural ODE frameworks \cite{chen2019neuralordinarydifferentialequations}, for instance, reinterpret dynamic evolution as a continuous function learned by a neural network, while operator-learning approaches such as Fourier Neural Operators (FNOs) \cite{li2021fourierneuraloperatorparametric} allow for flexible mappings from initial conditions to solutions of partial differential equations. Despite these advances, deep learning approaches often face two critical challenges:

\begin{itemize}
    \item \textbf{Preserving the underlying physical structure.} Standard networks, left unconstrained, may inadvertently violate symplectic forms, conservation of energy, or other geometric constraints intrinsic to physical dynamics \cite{chen2020symplecticrecurrentneuralnetworks}. These violations can accumulate over time, producing qualitatively incorrect long-horizon predictions—e.g., spurious energy drift in Hamiltonian systems.
    \item \textbf{Generalizing across related systems.} Many real-world applications involve entire families of similar yet distinct systems (e.g., variations of a robotic manipulator differing in load mass or joint friction, or molecular systems differing in exact bonding parameters). Training an entirely separate model for each variant is both data-inefficient and computationally expensive. Without mechanisms to share knowledge, a network trained on one system will typically fail to adapt efficiently to another, even if they share most of the same physics. Bridging this sim-2-real gap is crucial for a variety of tasks such as control and real-time prediction \cite{bai2024closesim2realgapphysicallybased}.
\end{itemize}

The tension between flexibility (i.e., capacity to learn diverse dynamics) and the enforcement of physical constraints can be addressed through specialized architectures that embed geometric priors \cite{chen2020symplecticrecurrentneuralnetworks, greydanus2019hamiltonianneuralnetworks}. Notably in the context of Hamiltonian mechanics, \emph{symplectic networks} (SympNets) incorporate symplectic forms directly into their design, guaranteeing that learned transformations preserve canonical transformations \cite{jin2020sympnetsintrinsicstructurepreservingsymplectic}. This preserves key invariants such as energy and momentum, mitigating error accumulation in long-horizon forecasts.

However, real-world systems also exhibit heterogeneity: varying parameters, boundary conditions, or even control signals that deviate from conservative dynamics. Thus, \emph{meta-learning} becomes a natural extension. By training on a set of related systems, meta-learning-based methods (e.g., Model-Agnostic Meta-Learning (MAML), interpretable Meta neural Ordinary Differential Equation (iMODE) and Fast Context Adaptation Via Meta-Learning (CAVIA) \cite{finn2017modelagnosticmetalearningfastadaptation, PhysRevLett.131.067301, CAVIA}) acquire high-level inductive biases that can be quickly adapted to novel systems using limited additional data. Consequently, when one faces a new variant of a familiar system, the network can fine-tune a handful of parameters, rather than retrain from scratch. This provides robust and scalable performance. 

\subsection{Contributions}
\label{contributions}
In this work, we introduce \textbf{MetaSym}, a deep-learning framework that addresses the major challenges of data-driven modeling of physical systems, i.e., preserving underlying geometric structures to ensure physically consistent behavior over long time horizons and rapidly adapting to system variations with minimal data as mentioned in Section \ref{introduction}. Our contributions are listed below:

\begin{itemize}
    \item \textbf{Symplectic Encoder for Structure Preservation:} We propose a specialized neural encoder (SymplecticEncoder) built on SympNet modules. The inherent structural invariants of the SympNets provide a strong inductive bias to the SymplecticEncoder, while our bi-directional training pipeline enforces Hamiltonian consistency, obtained by the canonical transformations that pertain to different systems. Hence, the SymplecticEncoder's output conserves key geometric invariants (e.g., energy and momentum), effectively minimizing error accumulation over long-term rollouts with minimal architecture size. 
    % By enforcing canonical transformations in each forward step and training the forward and the inverse map simultaneously, this encoder conserves key geometric invariants (e.g., energy, momentum), preventing error accumulation in long-term rollouts as well enforces consistency with the reversibility of such symplectic structures as well as providing a consistent and strong inductive bias. 
    \item \textbf{Autoregressive Decoder with Meta-Attention for Adaptation:} To handle nonconservative forces and variations in system parameters, we introduce ActiveDecoder, a transformer-inspired decoder module equipped with a meta-attention framework. This decoder incorporates control inputs, external forces, and per-system parameters enabling flexible modeling of real-world effects beyond ideal Hamiltonian dynamics while enabling autoregressive multistep prediction during inference time.
    \vskip 0.5in
    \item \textbf{Meta-Learning for Multi-System Generalization:} We adopt a meta-learning motivated inner-outer training scheme wherein system-specific parameters are fine-tuned in a few gradient steps, while global parameters remain shared across systems. This yields a single framework that quickly adapts to new or modified systems with minimal additional data.
    
\end{itemize}

Finally, we benchmark this bespoke smaller architecture against other state-of-the-art deep learning methods, including Dissipative Hamiltonian Neural Networks (DHNNs) \cite{sosanya2022dissipativehamiltonianneuralnetworks} and Transformers \cite{vaswani2017attention, Geneva_2022}, for modeling various physical systems in both classical and quantum regimes. These systems include a high-dimensional spring mesh system, an open quantum system whose dynamics are highly complex and counterintuitive in the classical regime, and a quadrotor with floating base dynamics. This provides evidence of far reaching implications in almost all aspects of physics modeling including challenging tasks like \emph{real-time} quantum dynamics prediction and simulating a variety of complex dynamics that typically require complex computational methods to solve. To the best of our knowledge, MetaSym is the first bespoke physics based deep learning model, to adapt and generalize well to both classical and non-unitary quantum dynamics. 

\section{Related Work and Background}
\label{relatedwork}

Physicists have long utilized the Lagrangian and Hamiltonian formalisms of mechanics to study the dynamics of physical systems \cite{kibble2004classical}. Consider $\mathbf q=\left(q_1, \ldots, q_n\right)$ that represents the generalized coordinates of an $n$-dimensional configuration space, while ${\mathbf p}$ represents the corresponding generalized momenta. We can describe the Hamiltonian $H(\mathbf q, \mathbf p, t)$ as the total energy of the system. This leads to Hamilton's equations as follows:
\begin{equation}
    \dot{q}_i=\frac{\partial H}{\partial p_i}, \quad \dot{p}_i=-\frac{\partial H}{\partial q_i}, \quad i=1, \ldots, n.
\end{equation}

This formalism naturally imbues the phase space $(\mathbf q, \mathbf p) \in \mathbb{R}^{2 d}$ with geometric structures that are vital to the study of these physical systems. One such structure is the symplectic form $\omega$, given by,

\begin{equation}
    \omega=\sum_{i=1}^n \mathrm{~d} p_i \wedge \mathrm{~d} q_i
\end{equation}

Concretely, a map $\Phi_t:(\mathbf q(0), \mathbf p(0)) \mapsto(\mathbf q(t), \mathbf p(t))$ is said to be symplectic if $\Phi_t^* \omega=\omega$. This implies that the flow in the phase space preserves volume and the fundamental two-form $\omega$ \cite{royer1991wigner}. 

Inspired by such geometric formulations of mechanics, recent work such as Hamiltonian Neural Networks (HNNs) and Lagrangian Neural Networks (LNNs) have sought to embed physical priors into deep learning architectures \cite{greydanus2019hamiltonianneuralnetworks,cranmer2020lagrangianneuralnetworks}. In particular, SympNets have emerged as structure-preserving neural architectures designed specifically for learning Hamiltonian systems, ensuring that their learned maps are intrinsically symplectic. They introduce modules that come in three variants: \emph{linear}$\left(\ell_{\text {up }}, \ell_{\text {low }}\right)$,
\emph{activation} $\left(N_{\text {up }}, N_{\text {low }}\right)$ and \emph{gradient}
$\left(G_{\text {up }}, G_{\text {low }}\right)$. These modules decompose symplectic flows into compositions of “unit triangular” symplectic transformations. Based on these modules, these authors propose two architectures, LA-SympNets that are compositions of \emph{linear} and \emph{activation} modules and G-Sympnets that based on \emph{gradient} modules exclusively. These architectures admit universal approximation theorems and crucially, this construction does not require solving ODEs or differentiating a candidate Hamiltonian during training and inference times, which often leads to more efficient optimization compared to other architectures such as HNNs or LNNs. The collection of all SympNets forms a group under composition, ensuring that every learned map is invertible with a closed-form inverse. However, due to the fundamental nature of Hamiltonian systems and symplectic forms, SympNets and similar architectures fail to generalize to dissipative systems where the symplectic structure is no longer preserved \cite{chen2020symplecticrecurrentneuralnetworks, cranmer2020lagrangianneuralnetworks,jin2020sympnetsintrinsicstructurepreservingsymplectic}. While there have been attempts to reconcile dissipative dynamics and control inputs to model realistic physical systems by preserving the symplectic framework, such as the Dissipative SymODEN \cite{zhong2020dissipativesymodenencodinghamiltonian,zhong2024symplecticodenetlearninghamiltonian},  they often suffer from the lack of generalization to different systems \cite{okamoto2024learningdeepdissipativedynamics}. 

Generalization between different but related physical systems is also vital for deep learning methods to excel at physics modeling. Meta-learning serves as the natural avenue for exploring such strategies. Building on a series of meta-learning strategies \cite{finn2017modelagnosticmetalearningfastadaptation,rajeswaran2019meta,zintgraf2019fast,nichol2018first}, the iMODE framework represents a notable advancement in applying deep learning to families of dynamical systems. iMODE integrates multiple new components specifically tailored to continuous-time dynamics by parameterizing a continuous-time flow 
 $\dot{\mathbf x}(t) = \mathbf f_\theta(\mathbf{\mathbf x}(t), t)$ through a neural network similar to that of Neural ODEs (NODEs). When combined with MAML, iMODE introduces explicit \emph{adaptation parameters} that act as a latent space embedding for system-specific characteristics. In this setup, one set of parameters captures the universal dynamics shared across all systems in a family, while another set encodes the idiosyncratic physical parameters that differentiate one system instance from another. However, certain drawbacks still persist. Apart from the lack of physics priors, these existing meta-learning approaches such as iMODE suffer from a lack of scalability~\cite{choe2023makingscalablemetalearning}.  

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.27]{schematic_corrected.png}}
\caption{Schematic representation of the \textbf{MetaSym} architecture. The conservative dynamics obtained from a physical system acts as the input to the SymplecticEncoder (blue-dashed box). This module consists of the symplectic network and its inverse, represented by the alternating light blue and dark blue dots. The forward network receives $( \mathbf{q}^{(i)}_t, \mathbf{p}^{(i)}_{t},\Delta t)$ that represents the position and momentum measurements at time $t$ for a given system $i$ during training, while the inverse network receives the position and momentum data from the future time step $(\mathbf q^{(i)}_{t+1},\mathbf p^{(i)}_{t+1},\Delta t)$. The SymplecticEncoder outputs the latent space $\mathbf z_c$. This process can be repeated for different systems. The sequence of the non-conservative canonical coordinates and control signals, $\left\{\mathbf{\tilde{q}}_{t:T}^{(i)}, \mathbf{\tilde{p}}_{t:T}^{(i)}, \Delta t, \mathbf{u}_{t:T}^{(i)}\right\}$,  act as inputs to the ControlNet (purple box) whose output is supplied to the ActiveDecoder (red dashed box). The ActiveDecoder consists of a layer normalization followed by a self attention layer that is then followed by layer normalization again. This is then provided to the cross attention layer which consists of keys, queries and values. The queries and values are finetuned based on a meta-update. This is finally followed by layer normalization and a multilayered perceptron (MLP) that outputs the position and momentum for the next time $(\hat{\mathbf q}^{(i)}_{t+1},\hat{\mathbf p}^{(i)}_{t+1})$. During training, we provide the ground truth sequence following a teacher-forcing paradigm (black arrow). During test time, we use the network's output for autoregressive prediction (black dotted line). } 
\label{schematic}
\end{center}
\vskip -0.3in
\end{figure*}


\section{Methods}
\label{methods}

In this section, we detail our pipeline for learning, adapting, and predicting the dynamics of physical systems using, MetaSym, our structure-preserving neural architecture and meta-learning framework. We describe both the high-level design of our \emph{encoder--decoder} model and the specialized training procedures implemented via \emph{meta-learning}. Figure~\ref{schematic} represents the overview of the architecture.

\subsection{SymplecticEncoder: Structure Preservation} \label{sec:encoder}

To ensure that predictions preserve fundamental geometric invariants, the encoder module is implemented as a SymplecticEncoder. Internally, it uses a \emph{symplectic neural network} (e.g., \texttt{LASympNet} \cite{jin2020sympnetsintrinsicstructurepreservingsymplectic}) consisting of sub-layers that update position $\mathbf{q}^{(i)}_t$ and momentum $\mathbf{p}^{(i)}_t$ for systems $i=1,...,n$ in a manner designed to approximate Hamiltonian flows and preserve symplecticity. Specifically, each sub-layer either performs an “\textit{up}” or “\textit{low}” transformation,
%
\begin{equation}
\begin{aligned}
    \text{(Up)}\quad\mathbf{q}^{(i)}_t \leftarrow \mathbf{q}^{(i)}_t \alpha(\mathbf{p}^{(i)}_t) \Delta t, \quad \mathbf{p}^{(i)}_t \leftarrow \mathbf{p}^{(i)}_t, \\
    \text{(Low)} \quad\mathbf{q}^{(i)}_t \leftarrow \mathbf{q}^{(i)}_t, \quad \mathbf{p}^{(i)}_t \leftarrow \mathbf{p}^{(i)}_t+\beta(\mathbf{q}^{(i)}_t) \Delta t.
\end{aligned}
\end{equation}
%
Here, $\alpha$ and $\beta$ can be \emph{linear}, \emph{activation}-based, or \emph{gradient}-based modules, ensuring we remain in the class of canonical (i.e., symplectic) maps. By stacking multiple up/low blocks, we achieve a deep network $\Phi_\theta:(\mathbf{q^{(i)}_t}, \mathbf{p^{(i)}_t}, \Delta t) \mapsto(\mathbf{q^{(i)}_{t+1}}, \mathbf{p^{(i)}_{t+1}})$.

A pivotal feature of Hamiltonian dynamics is \emph{time reversibility}: if we integrate the system forward from $(\mathbf{q^{(i)}_t},\mathbf{p^{(i)}_t)}$ to $(\mathbf{q^{(i)}_{t+1}}, \mathbf{p^{(i)}_{t+1}})$ over time $\Delta t$, then integrating backwards over $-\Delta t$ should return the system exactly to $(\mathbf{q^{(i)}_t}, \mathbf{p^{(i)}_t})$. This property lies at the heart of many physical invariants (energy, momentum, etc.) and is crucial for long-horizon stability.

To replicate this in the neural network, each forward pass provides the update for the forward and inverse network, where the forward network $\Phi_{\theta_{SE}}$ is characterized as:

\begin{equation}
    \Phi_{\theta_{SE}}(\mathbf{q^{(i)}_t}, \mathbf{p^{(i)}_t}, \Delta t)=[\mathbf{q^{(i)}_{t+1}}, \mathbf{p^{(i)}_{t+1}}],
\end{equation}
and the inverse network $ \Phi_{\theta_{SE}}^{-1}$ acts as not merely the computationally reversed pass of $\Phi_{\theta_{SE}}$. It also switches the sign of $\Delta t$ (i.e., steps ``backwards in time''). 
From a physical standpoint, preserving both a forward and inverse map enforces the time-reversal symmetry characterizing Hamiltonian flows. We \emph{train both directions} simultaneously—minimizing reconstruction errors for forward evolution and inverse evolution. As a result, the network is less susceptible to artificial energy drift and can better maintain conservation laws over extended forecasts. The learned symplectic map remains bijective, preserving volumes in phase space and the symplectic 2-form. Moreover, we can train both passes to minimize reconstruction errors in forward and backward directions, thereby further regularizing the network to approximate canonical transformations faithfully. This invertible design also allows consistent backward integration without introducing extraneous numerical artifacts.

During training the encoder is provided with a sequence of phase-space points and encodes them onto a conservative latent space $\mathbf z_c = [\mathbf{q_{enc}}, \mathbf{p_{enc}}] \in \mathbb{R}^{2 d \times T}$ that is ultimately used by the ActiveDecoder as its memory sequence.

Additionally, to ensure that our encoder can generalize quickly across multiple related but distinct physical systems, we adopt a MAML-style \cite{finn2017modelagnosticmetalearningfastadaptation} framework. For each system $i$ in a mini-batch, we split its trajectory into $\mathcal{I}_{adapt}$ and $\mathcal{I}_{meta}$ sets. During the fast adaptation loop, we optimize the parameters of the encoder $\theta_{SE}$ \emph{only} on $\mathcal{I}_{adapt}$, by minimizing the mean-squared error between its forward predictions and the ground-truth labels. This process simulates “specializing” the encoder to system $i$'s local dynamics using a simple loss function that avoids gradient-terms that may destabilize the subsequent meta-update of $\theta_{SE}$. For the outer loop, we perform a forward and an inverse pass of the SymplecticEncoder and subsequently we minimize a combined loss represented as,
\vskip-0.3in
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\mathrm{meta}} =\frac{1}{\mathcal{T}_{meta}N_{batch}}&\sum_{\substack{t \in \mathcal{I}_{\mathrm{meta}} \\ i \in N_{batch}}}\Bigl\|\Phi_{\theta_{SE}}\bigl(\mathbf{q}_{t}^{(i)}, \mathbf{p}_{t}^{(i)}; \boldsymbol{\theta_{SE}}^{*(i)}\bigr)\\
    &-\Phi_{\theta_{SE}}^{-1}\bigl(\mathbf{q}_{t+1}^{(i)}, \mathbf{p}_{t+1}^{(i)}; \boldsymbol{\theta_{SE}}^{*(i)}\bigr)\\ 
    &-[\mathbf{q}_{t+1}^{(i)} - \mathbf{q}_{t}^{(i)}, \mathbf{p}_{t+1}^{(i)} - \mathbf{p}_{t}^{(i)}] \Bigr\|^2,
\end{aligned}
\label{eq:encoder_meta_loss}
\end{equation}


using the adapted parameters $\theta^*_{SE}$, this loss measures how well the optimal adapted parameters $\theta^*_{SE}$ generalize beyond the adaptation subset. We accumulate gradients \emph{with respect to} $\theta_{SE}$ and sum across multiple batches to obtain a single update step.

\subsection{ActiveDecoder: Autoregressive Decoder}\label{decoder}
As mentioned in Section~\ref{contributions}, we train an autoregressive decoder to model non conservative and realistic forces such friction, or air resistance, that break pure Hamiltonian symmetries. Unlike the SymplecticEncoder, which strictly enforces canonical updates, the ActiveDecoder can incorporate these extraneous phenomena. While the former is trained in isolation, we freeze its parameters during the decoder's training.

We define $\mathbf{\hat{q}^{(i)}_t}$ and $\mathbf{\hat{p}^{(i)}_t}$ as the dissipative canonical coordinates, of a system $i$. An input consisting of $\{\mathbf{\tilde{q}^{(i)}_t},\mathbf{\tilde{p}^{(i)}_t},\Delta t,\mathbf{u^{(i)}_t}\} \in \mathbb{R}^{(2 d+ m +1)}$ is provided to a linear projection, where $\mathbf{u^{(i)}_t} \in \mathbb{R}^{m}$ represents the control input driving the system at time $t$. This linear layer which we call ``ControlNet'', serves to map the input to a latent vector $\mathbf z_d \in \mathbb{R}^{2d}$ representing the non-conservative part of the system's dynamics. We then apply a masked multi-head self-attention over the sequence $\{\mathbf z_d\}$ for autoregressive decoding. The masking allows us to model the causal dependencies of the decoder's input sequence. 

Next, we apply a cross-attention mechanism, augmented with a specialized \emph{meta-attention} design. To achieve multi-system generalization and fast adaptation, we use a meta-learning setup akin to a \emph{bi-level} optimization inspired by recent progress \cite{li2025meta}. Specifically, we separate the ActiveDecoder’s parameters into: global parameters, $\theta_{AD}$, that remain \emph{shared} across all systems, and local parameters, $\zeta_i$, that can be interpreted as system specific parameters.  Specifically in the meta-attention: 
\vskip-0.2in
\begin{itemize}[noitemsep]
\item \emph{Key parameters} remain global and are updated in the outer loop, common to all systems. 
\item \emph{Query/Value parameters} are re-initialized for each system and fine-tuned with a few iterations in the inner loop, allowing the decoder to discover per-system or per-task representations. 
\end{itemize} 
% We enforce this separation via gradient-masking \emph{hooks} such that queries/values only receive gradients during adaptation, whereas keys only receive gradients during the outer update. 
%
For each system $i$, we split the time-sequenced data,  $\{\mathbf{\tilde{q}^{(i)}_t},\mathbf{\tilde{p}^{(i)}_t},\mathbf{u^{(i)}_t}\}$ into an \emph{adaptation} set , $\mathcal{I}_{adapt}$, and a \emph{meta} set , $\mathcal{I}_{meta}$. During the \textit{inner loop} of the bi-level optimization, we hold $\theta_{AD}$ fixed and fine-tune $\zeta_i$ by performing a few steps of gradient descent. Each step iterates over $\mathcal{I}_{adapt}$ feeding the ActiveDecoder ($\Phi_{AD}$) with ground-truth $(\mathbf{q}_{t}^{(i)}, \mathbf{p}_{t}^{(i)})$ and control $\mathbf{u}_{t}^{(i)}$, and minimizing an inner MSE loss: 
\vskip-0.3in
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\mathrm{inner}}^{(i)} =\frac{1}{\mathcal T_{adapt}}\sum_{t \in \mathcal{I}_{\mathrm{adapt}}}\Bigl\|\Phi_{\text{AD}}&\bigl(\mathbf{q}_{t}^{(i)}, \mathbf{p}_{t}^{(i)}, \mathbf{u}_{t}^{(i)}; \boldsymbol{\theta_{AD}}, \boldsymbol{\zeta}_i\bigr)\\ 
    &-[\mathbf{q}_{t+1}^{(i)}, \mathbf{p}_{t+1}^{(i)}] \Bigr\|^2.
\end{aligned}
\end{equation}\label{eq:inner_loss_decoder}
\vskip-0.3in
This ensures that $\zeta_i$ adapts to system or task specific idiosyncrasies. 

Once the local parameters $\zeta_i$ have been adapted to the optimal value $\zeta^*_i$, we evaluate the model's performance on $\mathcal{I}_{meta}$ portion of the trajectory. The corresponding outer loss: 
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\mathrm{outer}}^{(i)} =\frac{1}{\mathcal{T}_{meta}}\sum_{t \in \mathcal{I}_{\mathrm{meta}}}\Bigl\|\Phi_{\text{AD}}&\bigl(\mathbf{q}_{t}^{(i)}, \mathbf{p}_{t}^{(i)}, \mathbf{u}_{t}^{(i)}; \boldsymbol{\theta_{AD}}, \boldsymbol{\zeta^*}_i\bigr)\\ 
    &-[\mathbf{q}_{t+1}^{(i)}, \mathbf{p}_{t+1}^{(i)}] \Bigr\|^2,
\end{aligned}
\end{equation}\label{eq:outer_loss_decoder}
\vskip-0.3in
propagates gradients \textit{only} to $\theta_{AD}$. The overall training objective is then the sum over $\mathcal{L}_{\mathrm{outer}}^{(i)}$ for all systems $i$, over the batch.


We finalize the decoder with a Multi-Layer Perceptron (MLP) that outputs the position and momentum for the next time $\{\mathbf{\hat{q}^{(i)}_{t+1}},\mathbf{\hat{p}^{(i)}_{t+1}}\}$. The architecture is then used autoregressively for future predictions.

% Algorithm~\ref{alg:active_decoder_meta} provides a high-level overview of how the ActiveDecoder is trained via meta-learning.

% \begin{algorithm}[t]
% \caption{Meta-Learning for the ActiveDecoder}
% \label{alg:active_decoder_meta}
% \textbf{Input:}  $\mathcal{D} = \{\mathcal{D}^{(1)},\dots,\mathcal{D}^{(N)}\}$: Training data from $N$ related systems. Each $\mathcal{D}^{(i)}$ is a trajectory of states $\bigl(\mathbf{q}_t^{(i)}, \mathbf{p}_t^{(i)}, \mathbf{u}_t^{(i)}\bigr)_{t=1}^T$.
% \begin{algorithmic}[1]
% \STATE

% \FOR{epoch $= 1 \to N_{\mathrm{epochs}}$}
%    \FOR{each mini-batch of systems $B \subseteq \{1,\dots,N\}$}
%       \STATE $\texttt{optimizer\_theta.zero\_grad()}$ 
%       \FOR{each system $i \in B$}
%          \STATE \textbf{Split the trajectory} $\mathcal{D}^{(i)}$ \textbf{into:}
%          \STATE \quad $\mathcal{I}_{\mathrm{adapt}} \subset \{1,\dots,T\}, \quad \mathcal{I}_{\mathrm{val}} = \{1,\dots,T\}\setminus \mathcal{I}_{\mathrm{adapt}}$.
         
%          \STATE \textbf{Inner loop: Adapt local parameters $\zeta_i$}
%          \FOR{$k = 1 \to K$}
%              \STATE $\texttt{inner\_optimizer.zero\_grad()}$
%              \STATE $\mathcal{L}_{\mathrm{inner}}^{(i)} \gets 0$
%              \FOR{$t \in \mathcal{I}_{\mathrm{adapt}}$}
%                 \STATE $\bigl(\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr)
%                 \;\leftarrow\;\mathrm{AD}\!\Bigl(\mathbf{q}^{(i)}_{t},\,\mathbf{p}^{(i)}_{t},\,\mathbf{u}^{(i)}_{t};\,\theta_{AD},\,\zeta_i\Bigr)$
%                 \STATE $\mathcal{L}_{\mathrm{inner}}^{(i)} \;+\!=\; \bigl\|\bigl[\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr]
%                 - [\mathbf{q}^{(i)}_{t+1}, \mathbf{p}^{(i)}_{t+1}] \bigr\|^2$
%              \ENDFOR
%              \STATE $\mathcal{L}_{\mathrm{inner}}^{(i)}.\texttt{backward()}$ \COMMENT{Eq.~\eqref{eq:inner_loss_decoder}}
%              \STATE $\texttt{inner\_optimizer.step()}$ \quad \COMMENT{Update $\zeta_i$ only}
%          \ENDFOR

%          \STATE \textbf{Outer loop: Validate \& update global parameters $\theta_{AD}$}
%          \STATE $\texttt{inner\_optimizer.zero\_grad()} \quad\;\;$ \COMMENT{No update to $\zeta_i$ now}
%          \STATE $\texttt{optimizer\_theta.zero\_grad()}$
%          \STATE $\mathcal{L}_{\mathrm{outer}}^{(i)} \gets 0$
%          \FOR{$t \in \mathcal{I}_{\mathrm{val}}$}
%             \STATE $\bigl(\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr)
%             \;\leftarrow\;\mathrm{AD}\!\Bigl(\mathbf{q}^{(i)}_{t},\,\mathbf{p}^{(i)}_{t},\,\mathbf{u}^{(i)}_{t};\,\theta_{AD},\,\zeta_i^*\Bigr)$
%             \STATE $\mathcal{L}_{\mathrm{outer}}^{(i)} \;+\!=\; \bigl\|\bigl[\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr]
%             - [\mathbf{q}^{(i)}_{t+1}, \mathbf{p}^{(i)}_{t+1}] \bigr\|^2$
%          \ENDFOR
%          \STATE $\mathcal{L}_{\mathrm{outer}}^{(i)}.\texttt{backward()}$ \COMMENT{Eq.~\eqref{eq:outer_loss_decoder}}
%          \STATE $\texttt{optimizer\_theta.step()}$ \quad \COMMENT{Update $\theta_{AD}$ only}
%       \ENDFOR
%    \ENDFOR
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}

\begin{figure*}[ht!]
\begin{center}
\centerline{\includegraphics[scale=0.265]{springmesh_corrected.png}}
\textbf{\vskip -0.3in}
\caption{\textbf{(Left)} Represents the trajectory of the position and the momentum for arbitrary masses from the spring mesh system. The actual  (orange line) and predicted (blue line) trajectories for the positions $q_x$ and $q_y$ along the $x$ and $y$ coordinates as well as the corresponding momenta ($p_x, p_z$) are plotted. \textbf{(Right)} Plots the mean squared error of the trajectories for five random systems from the test set, along with each phase-space component's median value.} 
\label{springmesh}
\end{center}
\vskip -0.2in
\end{figure*}


\subsection{Autoregressive Inference}
Once both the SymplecticEncoder and ActiveDecoder (Sections~\ref{sec:encoder} and \ref{decoder}) are trained, we generate future predictions by rolling out the model \emph{autoregressively} during inference time. Specifically, at test time, we are given initial phase-space measurements. We first map to the conservative portion of the phase-space, ($\mathbf q^{(i)}_t, \mathbf p^{(i)}_t$) through the SymplecticEncoder. The ActiveDecoder then produces the next predicted phase-space trajectory $(\mathbf{\hat{q}^{(i)}_{t+1}},\mathbf{\hat{p}^{(i)}_{t+1}})$, based on the pipeline highlighted in Section \ref{decoder}. However, we subsequently treat $(\mathbf{\hat{q}^{(i)}_{t+1}},\mathbf{\hat{p}^{(i)}_{t+1}})$ as inputs for the next timesteps. This is repeated over the context window provided to the decoder.

In summary, by pairing our SymplecticEncoder (to ensure structure preservation) that provides a strong \textbf{inductive bias} with an \emph{Autoregressive Transformer-style Decoder} (ActiveDecoder) equipped with \emph{meta-attention} (to handle individual system variations) and end-to-end training with metalearning, our network, MetaSym, can rapidly adapt to diverse physical systems while guaranteeing physically consistent core dynamics. The following section presents a range of experimental results demonstrating superior performance compared to state-of-the-art approaches, especially in long-horizon stability and few-shot generalization to new system instances of varying complexity.

\section{Results}
\label{results}




In this section, we evaluate MetaSym's performance, against a variety of State-of-The-Art (SoTA) models and benchmarks. In particular, we report the behavior of MetaSym against Transformers that have achieved impressive performance in modeling physical systems \cite{Geneva_2022} and DHNNs \cite{sosanya2022dissipativehamiltonianneuralnetworks} that model systems via the Hemholtz decomposition, although they require gradient information and a numerical integrator to obtain trajectory data. Our model predicts the subsequent timesteps directly, which poses a much harder task that does not require numerical integration, which can be time-consuming and may hinder real-time performance. These results also reflect on the effectiveness of our design choices. 

As mentioned in Section \ref{contributions}, we choose to benchmark MetaSym for three extremely challenging and diverse datasets. Notably, we choose the spring mesh system, which tests scalability and closely resembles predicting the dynamics modeled using finite element methods. This dataset has provided the state-of-art testbed for large dimensional systems  \cite{otness2021extensiblebenchmarksuitelearning}. We consider a novel dataset to demonstrate the robustness to noise and flexibility to model all types of physical systems with MetaSym by, considering an open quantum system undergoing heterodyne detection. Finally, we choose to predict the dynamics of a quadrotor, a long standing benchmark in robotics and challenging due to floating base dynamics \cite{bansal2016learning}.  The description of each experimental setup is laid in the next sections, however a more extensive description can be found in Appendix \ref{exp-setup}.



\subsection{Spring Mesh-System}
For our first benchmark, we consider a 400 dimensional spring mesh system initially proposed by Otness \textit{et al.} as part of the extensive benchmark for learning physical systems \cite{otness2021extensiblebenchmarksuitelearning}. The spring mesh system is provided as the most complex physical system among their benchmarks due to its high-dimensional state-space and dynamic couplings. This system is also used to model deformable volumes and the surfaces of complex objects that are vital to engineering tasks \cite{pfaff2021learningmeshbasedsimulationgraph}. In this setup, each spring position is discretized by nodes whose position is updated based on forces computed from spring extensions and compressions, thus subjecting the simulation to nontrivial communication and computation patterns between different node positions. For a more in-depth description of the benchmark, refer to Appendix~\ref{spring-mesh-appendix}.


\begin{figure*}[ht!]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.27]{quantum_corrected.png}}
\vskip -0.1in
\caption{ (\textbf{Left}) Represents the trajectory of the two measured quadratures obtained from heterodyne measurement for an arbitrary quantum system in the test set with measurement efficiency $\eta=0.86$ and measurement strength $\kappa$. The actual measurement trajectory (orange line) is plotted against the predicted measurement (blue line) by MetaSym. (\textbf{Right}) Plots the mean squared error of the trajectories along with the respective median value for five random systems with arbitrary initial states, and other randomized parameters from the test set.} 
\label{quantum}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Open Quantum System}
\label{quantumsys}
To benchmark MetaSym on a novel open quantum dynamics dataset, we consider a \emph{parametric oscillator} initialized in a coherent state~\cite{Gardiner2010}. The system Hamiltonian includes three contributions:
the harmonic term, $\hat{H}_{\mathrm{osc}}=\omega \hat{a}^{\dagger} \hat{a}$, the squeezing term, $\hat{H}_{\mathrm{sqz}}=\frac{i \chi}{2}\left(\hat{a}^{\dagger 2}-\hat{a}^2\right)$, and the cubic driving term, $\hat{H}_{\text {cubic }}=\beta\left(\hat{a}^3+\hat{a}^{\dagger 3}\right)$.
Here, $\hat{a}^{\dagger}$ and $\hat{a}$ represent creation and annihilation operators in a truncated Fock space of dimension $N$. Dissipation arises due to coupling with a thermal bath at rate $\gamma$ and average thermal occupation $\left\langle n_{\mathrm{th}}\right\rangle$. We assume that the system is continuously monitored via heterodyne detection, leading to a stochastic master equation for the conditional quantum state (refer to Appendix \ref{appendix: quantum}). The primary observables are the real and imaginary components (quadratures) of the heterodyne current, denoted $X$ and $P$, respectively. For this benchmark, we generate the dataset by numerically solving the stochastic master equation for the setup described \cite{Johansson_2012}. We produce a dataset with varying measurement efficiencies $\eta$, squeezing strength $\chi$, cubic term strength $\beta$, oscillator frequency $\omega$ and average thermal occupation $\langle n_{th} \rangle$ of the bath along with different initial states. This reflects real world experimental scenarios with variable $\eta$ due to sensors or non ideal measurement conditions. Refer to Appendix \ref{appendix: quantum} for a detailed description of the training setup and out-of-distribution inference setup that we used.
\begin{figure*}[ht]
%\vskip 0.2in
\begin{center}
 \centerline{\includegraphics[scale=0.267]{quadrotor_corrected.png}}

\caption{\textbf{(Left)} Represents the translational and angular phase-space evolution of the quadrotor, with training and test trajectories generated using the \emph{Crocoddyl} trajectory optimization package \cite{mastalli20crocoddyl}. Each task is initialized with randomized initial conditions and a randomized terminal position over a 1.5s horizon. The ground-truth test trajectory (orange line) is compared to MetaSym's predictions (blue line) indicating the good predictive capabilites of our model. \textbf{(Right)} Plots the average mean squared error of the trajectories for five randomly generated test-set systems with arbitrary initial conditions and terminal positions, along with randomized inertial system parameters.}
\label{quadrotor}
\end{center}
\vskip -0.2in
\end{figure*}
\subsection{Quadrotor}
For our final benchmark, we consider a quadrotor system, a well-known robotic platform with various applications across different domains, since the agility and speed of such systems are challenging for current SoTA data-driven methods \cite{sanyal2023rampnetrobustadaptivempc}. The complexity of the systems originate from the floating-base dynamics and the discontinuous orientation representation \cite{8953486}. The floating-base dynamics are prone to internal covariate shift during the training of data-driven models, since the magnitudes of the phase-space can vary across systems and time. Simultaneously, representing the quadrotor's orientation with Euler angles introduces singularities, while unit-quaternions have a nonlinear constraint on their total magnitude which most times is enforced using penalties in the loss function. This can often lead to local-minima that can dramatically slow down or stop the training progress. Our physics-informed SymplecticEncoder, along with the meta-learning capabilities of the ActiveDecoder, presents increased robustness across these problems. To avoid additional terms in the loss function, we project the quaternions representing the floating-base's orientation to their respective tangent-space. For a more elaborate description of our simulation setup, refer to Appendix \ref{quadrotor-appendix}.

\subsection{Benchmarks}
We evaluate our architecture's accuracy and memory footprint against two state-of-the-art methods and a naive baseline: an auto-regressive Transformer model \cite{vaswani2017attention}, a physics-informed DHNN model \cite{greydanus2019hamiltonianneuralnetworks} and a Multilayered Perceptron (MLP). Both MetaSym and the Transformer predict the next timestep autoregressively, while the DHNN estimates the symplectic and dissipative gradients and relies on an integrator to unroll the trajectory. Consequently, the DHNN is also unable to predict quadrature measurements since these derivatives are not well defined and involve a complex Wiener term \cite{PhysRevA.36.5271}. Additionally, the DHNN's training proved to be less stable as compared to MetaSym and the Transformer's convergence properties. Finally, all models were tested in an autoregressive manner, in which each prediction is provided again as input to the model for the next timestep. 
As seen in Table \ref{benchmarks}, this study exposes the true performance capability and scalability of our architecture with the superior error accumulation over long horizon prediction of MetaSym. We achieve superior performance with smaller model sizes for MetaSym especially for high dimensional systems eg. spring mesh system. For the quantum dynamics benchmark, smaller models are crucial to achieve real-time prediction and control. Hardware limitations for quantum control restrict the use of larger models with higher latency. We demonstrate superior performance compared to the Transformer, the current state-of-the-art model \cite{vaidhyanathan2024quantumfeedbackcontroltransformer}. Moreover, our model size falls below the threshold for real-time predictions of quantum devices \cite{Reuer_2023}.

\begin{table*}
  \centering
  \caption{Trajectory error and parameter count against SoTA baselines for three out-of-distribution datasets}
  \label{benchmarks}
  % Wrap the entire tabular in \resizebox
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{c ccc ccc ccc}
      \toprule
        & \multicolumn{2}{c}{Spring Mesh System} 
        & \multicolumn{2}{c}{Quantum System}
        & \multicolumn{2}{c}{Quadrotor} \\
      \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
      \textbf{Models}
         & \textbf{Param. Count} & \textbf{Trajectory MSE ($\pm \sigma$)}
        & \textbf{Param. Count} & \textbf{Trajectory MSE ($\pm \sigma$)}
        & \textbf{Param. Count} & \textbf{Trajectory MSE ($\pm \sigma$)} \\
      \midrule
      DHNNs
        & 3.0M  & 32.468 (28.086)
        & N/A & N/A
        & 3138  & 26.375 (9.160) \\
     Transformer 
        & 3.2M  & 36.653 (15.618)
        & 194  & 0.950 (0.239)
        & 4680  & 34.584 (14.063) \\
    Naive Baseline (MLP)
        & 3.5M & 323.199 (13.959)
        & 262 & 0.898 (0.226)
        &  4012 & 39.311 (13.937) \\
    MetaSym (ours)
        & \textbf{2.9M}  & \textbf{19.233 (15.673)}
        & \textbf{130}  & \textbf{0.859 (0.215)}
        &  \textbf{3036} & \textbf{25.889 (8.967)} \\


      \bottomrule
    \end{tabular}%
 }
\end{table*}




\section{Conclusion \& Future Work}
In this work, we introduced \textbf{MetaSym}, a novel deep-learning framework that combines structure-preserving symplectic networks with an autoregressive, decoder equiped with meta-learning for modeling a wide range of physical systems. The core idea rests on striking a balance between \emph{strong physical priors}—namely the intrinsic symplectic structure of Hamiltonian mechanics—and the \emph{flexibility} required to capture non-conservative effects and heterogeneous system parameters. Our experimental results across diverse domains—ranging from high-dimensional spring-mesh dynamics to open quantum systems and robotics-inspired quadrotor models—demonstrated that MetaSym outperforms state-of-the-art baselines in both long-horizon accuracy and few-shot adaptation with smaller model sizes.

From a geometric perspective, the SymplecticEncoder approximates canonical flows while preserving key invariants, significantly mitigating energy drift and ensuring robust long-term predictions. The encoder’s invertible design enforces time-reversal symmetry and reduces error accumulation. Meanwhile, the ActiveDecoder models departures from ideal Hamiltonian evolution through autoregressive prediction and meta-attention. The resulting architecture is computationally efficient, given that it does not require explicit numerical integration during inference, and—through meta-learning—readily adapts to system variations with minimal additional data. This approach offers a scalable and unified framework for high-fidelity physics modeling in realistic settings with provable near symplectic properties (see Appendix \ref{subsec:nearsymplectic-proof}).

Considering MetaSym’s promising performance, we seek to investigate several future directions such as incorporating a fully symplectic network for modeling realistic physics by exploiting the underlying structure of noise and our control signals. Another natural extension of few-shot adaptation is online learning and control. Since MetaSym can quickly adapt to new system configurations with minimal data, this capability can be leveraged in real-time control loops and model-based Reinforcement Learning (RL) algorithms. Future research can investigate how to integrate the decoder’s adaptation step with RL or adaptive Model Predictive Control (MPC) frameworks, effectively enabling self-tuning controllers in rapidly changing environments.



% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}


% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.


\section*{Acknowledgments}
The authors would like to thank Lucas Schorling, Federico Fedele and Vivek Wadhia for the useful discussions. P.V. is supported by the United States Army Research Office under Award No. W911NF-21-S-0009-2. A.P is supported by University of Oxford's Clarendon Fund. M.T.M. is supported by a Royal Society University Research Fellowship. N.A. acknowledges support from the European Research Council (grant agreement 948932) and the Royal Society (URF-R1-191150).

% UNCOMMENT <---------------------------------
% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{Meta Learning Setup}
\label{meta-appendix}

As mentioned in Section \ref{methods}, we elaborate on the algorithmic pipeline of our SymplecticEncoder and ActiveDecoder modules. In particular, we outline the MAML-based meta-learning strategy for the SymplecticEncoder and the inner-outer adaptation meta-learning for the ActiveDecoder. Figure \ref{inner-adapt} provides an interpretable insight into the effect of inner adaptation during the meta-update step and its effectiveness.


\subsection{Encoder}\label{meta-encoder}
Following Section \ref{sec:encoder}, Algorithm~\ref{alg:symplectic_encoder_meta} provides a high-level overview of how the SymplecticEncoder is trained via meta-learning.

\begin{algorithm}[htbp!]
\caption{Meta-Learning for the SymplecticEncoder}
\label{alg:symplectic_encoder_meta}
\begin{algorithmic}[1]
\REQUIRE  $\mathcal{D} = \{\mathcal{D}^{(1)},\dots,\mathcal{D}^{(N)}\}$: Training data from $N$ related systems. Each $\mathcal{D}^{(i)}$ is a trajectory of states $\bigl(\mathbf{q}_t^{(i)}, \mathbf{p}_t^{(i)}, \mathbf{u}_t^{(i)}\bigr)_{t=1}^T$.
\FOR{epoch $= 1 \to N_{\mathrm{epochs}}$}
   \FOR{each mini-batch of systems $B \subseteq \{1,\dots,N\}$}
      \STATE $\texttt{optimizer\_theta.zero\_grad()}$ 
      \STATE $\texttt{inner\_optimizer.zero\_grad()}$ 
      \FOR{each system $i \in B$}
         \STATE \textbf{Split the trajectory} $\mathcal{D}^{(i)}$ \textbf{into:}
         \STATE \quad $\mathcal{I}_{\mathrm{adapt}} \subset \{1,\dots,\mathcal{T}_i\}, \quad \mathcal{I}_{\mathrm{meta}} = \{1,\dots,\mathcal T_i\}\setminus \mathcal{I}_{\mathrm{adapt}}$.
         \STATE $\mathcal{L}_{\mathrm{meta}} \gets 0$
         \STATE \textbf{Fast Adaptation: Adapt system-specific parameters}
         \STATE $\theta_{SE}^{(i)} \leftarrow \theta_{SE}$\texttt{.clone().detach()} \COMMENT{Detach}
         \FOR{$k = 1 \to K$}
            \STATE $\texttt{inner\_optimizer.zero\_grad()}$
            \STATE $\mathcal{L}_{\mathrm{inner}}^{(i)} \leftarrow 0 $
             \FOR{$t \in \mathcal{I}_{\mathrm{adapt}}$}
                \STATE $\bigl(\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr)
                \;\leftarrow\;\Phi_{\mathrm{SE}}\!\Bigl(\mathbf{q}^{(i)}_{t},\,\mathbf{p}^{(i)}_{t},\,\mathbf{u}^{(i)}_{t};\,\theta_{SE}^{(i)}\Bigr)$
                \STATE $\mathcal{L}_{\mathrm{inner}}^{(i)} \;+=\; \bigl\|\bigl[\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr]
                - [\mathbf{q}^{(i)}_{t+1}, \mathbf{p}^{(i)}_{t+1}] \bigr\|^2$
             \ENDFOR
             \STATE $\mathcal{L}_{\mathrm{inner}}^{(i)}.\texttt{backward()}$
             \STATE $\texttt{inner\_optimizer.step()}$ \quad \COMMENT{Update $\theta_{SE}^{(i)}$ only}
         \ENDFOR

         \STATE \textbf{Meta Update: Update the global parameters $\theta_{AD}$}
         \FOR{$t \in \mathcal{I}_{\mathrm{meta}}$}
            \STATE $\theta_{SE} \leftarrow \theta_{SE}^{(i)^*}$\texttt{.clone()} \COMMENT{No Detach}
            \STATE $\bigl(\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr)
            \;\leftarrow\;\Phi_{\mathrm{SE}}\!\Bigl(\mathbf{q}^{(i)}_{t},\,\mathbf{p}^{(i)}_{t},\,\mathbf{u}^{(i)}_{t};\,\theta_{SE}\Bigr)$
            \STATE $\mathcal{L}_{\mathrm{meta}} \;+\!=\; \bigl\|\bigl[\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr]
            - [\mathbf{q}^{(i)}_{t+1}, \mathbf{p}^{(i)}_{t+1}] \bigr\|^2$
         \ENDFOR
      \ENDFOR
         \STATE $\mathcal{L}_{\mathrm{meta}}.\texttt{backward()}$ \COMMENT{Eq.~\eqref{eq:encoder_meta_loss}}
         \STATE $\texttt{optimizer\_theta.step()}$ \quad \COMMENT{Update $\theta_{SE}$ only}
   \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\newpage
\subsection{Decoder}
\label{meta-decoder}
Algorithm~\ref{alg:active_decoder_meta} provides a high-level overview of how the ActiveDecoder is trained via meta-learning.

\begin{algorithm}[htbp!]
\caption{Meta-Learning for the ActiveDecoder}
\label{alg:active_decoder_meta}
\begin{algorithmic}[1]
\REQUIRE $\mathcal{D} = \{\mathcal{D}^{(1)},\dots,\mathcal{D}^{(N)}\}$: Training data from $N$ related systems. Each $\mathcal{D}^{(i)}$ is a trajectory of states $\bigl(\mathbf{q}_t^{(i)}, \mathbf{p}_t^{(i)}, \mathbf{u}_t^{(i)}\bigr)_{t=1}^T$.
\FOR{epoch $= 1 \to N_{\mathrm{epochs}}$}
   \FOR{each mini-batch of systems $B \subseteq \{1,\dots,N\}$}
      \STATE $\texttt{optimizer\_theta.zero\_grad()}$ 
      \FOR{each system $i \in B$}
         \STATE \textbf{Split the trajectory} $\mathcal{D}^{(i)}$ \textbf{into:}
         \STATE \quad $\mathcal{I}_{\mathrm{adapt}} \subset \{1,\dots,\mathcal{T}_i\}, \quad \mathcal{I}_{\mathrm{meta}} = \{1,\dots,\mathcal T_i\}\setminus \mathcal{I}_{\mathrm{adapt}}$.
         
         \STATE \textbf{Inner loop: Adapt local parameters $\zeta_i$}
         \FOR{$k = 1 \to K$}
             % \STATE $\mathcal{L}_{\mathrm{inner}}^{(i)} \gets 0$
             \FOR{$t \in \mathcal{I}_{\mathrm{adapt}}$}
                \STATE $\texttt{inner\_optimizer.zero\_grad()}$
                \STATE $\zeta_i$.\texttt{randomize()} \COMMENT {Reinitialize $\zeta_i$}
                \STATE $\bigl(\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr)
                \;\leftarrow\;\Phi_{\mathrm{AD}}\!\Bigl(\mathbf{q}^{(i)}_{t},\,\mathbf{p}^{(i)}_{t},\,\mathbf{u}^{(i)}_{t};\,\theta_{AD},\,\zeta_i\Bigr)$
                \STATE $\mathcal{L}_{\mathrm{inner}}^{(i)} \;\!=\; \bigl\|\bigl[\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr]
                - [\mathbf{q}^{(i)}_{t+1}, \mathbf{p}^{(i)}_{t+1}] \bigr\|^2$
             \STATE $\mathcal{L}_{\mathrm{inner}}^{(i)}.\texttt{backward()}$ \COMMENT{Eq.~\eqref{eq:inner_loss_decoder}}
             \STATE $\texttt{inner\_optimizer.step()}$ \quad \COMMENT{Update $\zeta_i$ only}
             \ENDFOR
         \ENDFOR

         \STATE \textbf{Outer loop: Meta-update for the global parameters $\theta_{AD}$}
         \STATE $\texttt{optimizer\_theta.zero\_grad()}$ \COMMENT{No update to $\zeta_i$ now}
         \STATE $\mathcal{L}_{\mathrm{outer}}^{(i)} \gets 0$
         \FOR{$t \in \mathcal{I}_{\mathrm{meta}}$}
            \STATE $\bigl(\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr)
            \;\leftarrow\;\Phi_{\mathrm{AD}}\!\Bigl(\mathbf{q}^{(i)}_{t},\,\mathbf{p}^{(i)}_{t},\,\mathbf{u}^{(i)}_{t};\,\theta_{AD},\,\zeta_i^*\Bigr)$
            \STATE $\mathcal{L}_{\mathrm{outer}}^{(i)} \;+\!=\; \bigl\|\bigl[\hat{\mathbf{q}}^{(i)}_{t+1}, \hat{\mathbf{p}}^{(i)}_{t+1}\bigr]
            - [\mathbf{q}^{(i)}_{t+1}, \mathbf{p}^{(i)}_{t+1}] \bigr\|^2$
         \ENDFOR
         \STATE $\mathcal{L}_{\mathrm{outer}}^{(i)}.\texttt{backward()}$ \COMMENT{Eq.~\eqref{eq:outer_loss_decoder}}
         \STATE $\texttt{optimizer\_theta.step()}$ \quad \COMMENT{Update $\theta_{AD}$ only}
      \ENDFOR
   \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}


\begin{figure*}[htbp!]
\begin{center}
\centerline{\includegraphics[scale=0.2695]{inner_adapt.png}} 
\caption{Represents the change in loss between training steps due to the meta-update of parameters. }
\label{inner-adapt}
\end{center}
\vskip -0.2in
\end{figure*}




\section{Experimental Setup}
\label{exp-setup}

\subsection{Spring Mesh System}\label{spring-mesh-appendix}

Spring networks are a simple proxy for numerous physical scenarios involving deformable solids and cloth (in computer graphics, mechanics, or robotics). Each pair of connected particles exchanges spring forces that depend on displacements from rest lengths. Viscous damping terms further shape the evolution. While the individual force law (Hooke’s law) is straightforward, the combination of hundreds of coupled springs can give rise to complex large-scale deformations and oscillations. By benchmarking on a spring mesh, we can examine how MetaSym learns large deformations, wave propagation through a membrane, or the impact of damping. 

The training dataset consists of 25 distinct spring-mesh systems, each characterized by a unique set of physical parameters, including spring stiffness, damping coefficients, and initial conditions as indicated by Table \ref{indist-spring}. These parameters are sampled from a predefined distribution to ensure sufficient diversity within the training set. Each system is simulated over a time span of 2000 irregular timesteps, capturing the full trajectory of node displacements and momenta. The resulting dataset provides a rich representation of dynamical behaviors within the parameter space.

To assess generalization and robustness, we construct a validation dataset comprising 10 additional spring-mesh systems. Unlike the training set, the parameters for these systems are drawn from distributions that differ from those used during training as indicated by Table \ref{outdist-spring}, introducing a domain shift that mimics real-world variations. This OOD validation set enables a rigorous evaluation of the model’s ability to extrapolate beyond the observed training dynamics and adapt to unseen conditions. For further information regarding the spring-mesh benchmark refer to \cite{otness2021extensiblebenchmarksuitelearning}.

By incorporating both in-distribution training data and OOD validation data, this experimental setup ensures a comprehensive assessment of the model’s learning capacity, robustness, and generalization performance when applied to novel physical configurations.

\begin{table}[htbp!]
\centering
% First table in a minipage
\begin{minipage}[b]{0.45\textwidth}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Parameters} & \textbf{Values} \\
\hline
$\gamma_{decay}\; \left[kg/s\right]$ & $\mathrm{Uniform}(0.1,0.2)$ \\
\hline
mass $\left[kg\right]$ & $\mathrm{Uniform}(0.1,2.0)$ \\
\hline
$K_{spring}\; [N/m]$ & $\mathrm{Uniform}(0.001,0.5)$  \\
\hline
initial perturbations $[m]$ & $\mathrm{Uniform}(0,0.6)$\\
\hline
dt $[s]$ & $\mathrm{Uniform}(0.001,0.03)$ \\
\hline
\end{tabular}
\caption{In-distribution parameters $T=2000$.}
\label{indist-spring}
\end{minipage}
\quad
% Second table in another minipage
\begin{minipage}[b]{0.45\textwidth}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Parameters} & \textbf{Values} \\
\hline
$\gamma_{decay}\; [kg/s]$ & $\mathrm{Uniform}(0.01,0.05)$ \\
\hline
$mass\; [kg] $ & $\mathrm{Uniform}(3.0,5.0)$ \\
\hline
$K_{spring}\; [N/m]$& $\mathrm{Uniform}(1.0,3.0)$ \\
\hline
initial perturbations $[m]$ & $\mathrm{Uniform}(0.9,2.5)$ \\
\hline
dt $[s]$ & $\mathrm{Uniform}(0.1,0.3)$ \\
\hline
\end{tabular}
\caption{Out-of-distribution parameters, $T=2000$}
\label{outdist-spring}
\end{minipage}
\end{table}



\subsection{Open Quantum System Derivation}
\label{appendix: quantum}

Understanding the behavior of quantum systems plays a vital role in the development of promising technologies such as quantum computing, metrology, and sensing. While deep learning has found great success in several areas such as quantum control \cite{vaidhyanathan2024quantumfeedbackcontroltransformer}, error correction \cite{bausch2024learning} and tuning \cite{ares2021machine, gebhart2023learning}, predicting the measurement record based on modeling quantum dynamics has long remained elusive.  In many scenarios, the system of interest is \emph{open}: it couples to an environment (or bath) that can introduce thermal noise and dissipation. Furthermore, continuous monitoring (e.g., via homodyne detection) adds additional \emph{measurement backaction}, reflecting fundamental constraints from quantum mechanics \cite{Jacobs_2006}. Capturing these noise and measurement effects is pivotal for accurately predicting quantum trajectories and devising robust control protocols.

Unlike closed Hamiltonian evolutions, open quantum systems require one to solve Stochastic Master Equations (SMEs) incorporating decoherence and measurement terms. These equations produce trajectories of the (mixed) quantum state conditioned on the noisy measurement record. In many practical settings, however, we only have direct access to certain observables (e.g., position and momentum quadratures) rather than the full quantum state. Hence, training a deep learning network to model the quantum system and to predict future measurement outcomes becomes a natural and practically relevant challenge. The SME describes the evolution of the \emph{conditioned} quantum state $\rho_c(t)$ under the effect of environmental and measurement noise as~\cite{Jacobs_2006}
\begin{equation} \label{eq:SME}
d\rho_c(t) =-i\bigl[H,\rho_c(t)\bigr]dt +\sum_j \mathcal{D}\bigl[\hat{L}_j\bigr]\rho_c(t)dt +\sqrt{\eta}\mathcal{H}\bigl[\hat{M}\bigr]\rho_c(t)dW_t, \end{equation} 
where the \emph{dissipator} is $\mathcal{D}[\hat{L}] \rho=\hat{L} \rho \hat{L}^{\dagger}-\frac{1}{2}\left\{\hat{L}^{\dagger} \hat{L}, \rho\right\}$. Each Linblad operator, $\hat{L}_j$, represents a \emph{collapse operator} that encodes coupling to the environment (e.g., photon loss to a reservoir, thermal excitations, dephasing, etc.) \cite{Manzano_2020}. The stochastic backaction term, $\mathcal{H}[\hat{M}] \rho=\hat{M} \rho+\rho \hat{M}^{\dagger}-\operatorname{Tr}\left[\left(\hat{M}+\hat{M}^{\dagger}\right) \rho\right] \rho$, describes continuous monitoring of an observable $\hat{M}$, with $\eta$ the measurement efficiency ($0\leq \eta\leq 1$). The Wiener increment, $dW_t$, captures the randomness inherent in quantum measurement outcomes. %This stochastic master equation describes the evolution of state based on the dissipation and measurement backaction. 

\subsubsection{Setup for Parameteric Oscillator}

In Section \ref{quantumsys}, we focus on a single-mode bosonic system with annihilation operator $\hat{a}$ and creation operator $\hat{a}^{\dagger}$. The Hamiltonian is:

\begin{equation}
    H=\omega \hat{a}^{\dagger} \hat{a}+\frac{i \chi}{2}\left(\hat{a}^{\dagger 2}-\hat{a}^2\right)+\beta\left(\hat{a}^3+\hat{a}^{\dagger 3}\right),
\end{equation}

with $\omega$, $\chi$ and $\beta$ being the oscillator frequency, squeezing strength and cubic driving term respectively. We also include two Linblad operators:

\begin{equation}
    \hat{L}_1=\sqrt{\gamma\left(\bar{n}_{{th}}+1\right)} \hat{\boldsymbol{a}}, \quad \hat{L}_2=\sqrt{\gamma \bar{n}_{\mathrm{th}}} \hat{a}^{\dagger},
\end{equation}

where $\gamma$ is the coupling rate to the thermal bath, and $\bar{n}_{\mathrm{th}}$ is the average occupation number.

A common measurement technique often employed in experimental settings is called heterodyne measurement. Heterodyne detection continuously measures both quadratures of the output of our dissipative qubit by mixing it with a local oscillator at a slightly shifted frequency and then demodulating the resulting beat signal. This yields two simultaneous photocurrents, often referred to as the in-phase ($I$) and quadrature ($Q$) components. This reflects a key practical outcome of solving Eq.~\eqref{eq:SME} is that $\rho_c(t)$ depends on this random measurement trajectory. 

We employ heterodyne detection of the field operator $\hat{a}$. Based on this measurement scheme, we can get seperate the real and imaginary parts to obtain quadrature values that roughly correspond to $X$ and $P$ while adding quantum noise and measurement uncertainities due to quantum mechanical effect \cite{PhysRevX.10.011006}.

In the following Tables \ref{outdist-quantum} and \ref{indist-quantum}, we describe the parameters used to generate our dataset by solving the SME in order to generate training data. 
\begin{table}[htbp!]
\centering
% First table in a minipage
\begin{minipage}[t]{0.45\textwidth}
\vspace{0pt}
\centering
\begin{tabular}{{|p{3.5cm}|p{3cm}|}}
\hline
\textbf{Parameters} & \textbf{Values} \\
\hline
oscillator frequency $\omega$ & $\mathrm{Uniform}(0.5,1.0)$ \\
\hline
squeezing strength $\chi$& $\mathrm{Uniform}(0.1,0.4)$ \\
\hline
thermal occupation $\langle n_{th} \rangle$ & $\mathrm{Uniform}(0.1,0.5)$  \\
\hline
meas. efficiency $\eta$ & $\mathrm{Uniform}(0.7,1.0)$\\
\hline
\end{tabular}
\caption{In-distribution parameters $dt=0.5$, $T=600$.}
\label{indist-quantum}
\end{minipage}
\quad
\begin{minipage}[t]{0.45\textwidth}
\vspace{0pt}
\centering
\begin{tabular}{{|p{3.5cm}|p{3cm}|}}
\hline
\textbf{Parameters} & \textbf{Values} \\
\hline
oscillator frequency $\omega$ & $\mathrm{Uniform}(0.1,0.4)$ \\
\hline
squeezing strength $\chi$ & $\mathrm{Uniform}(0.5,0.8)$ \\
\hline
thermal occupation $\langle n_{th} \rangle$ & $\mathrm{Uniform}(0.6,0.7)$ \\
\hline
meas. efficiency $\eta$ & $\mathrm{Uniform}(0.4,0.6)$ \\
\hline
\end{tabular}
\caption{Out-of-distribution parameters $dt=0.5$, $T=600$}
\label{outdist-quantum}
\end{minipage}
\end{table}


\subsection{Quadrotor}\label{quadrotor-appendix}
The quadrotor system challenges data-driven methods with its floating-base dynamics. To generate the training and OOD validation datasets we use \emph{Crocoddyl} trajectory optimization package based on the dynamics model proposed by \cite{quad_citation}.
The training dataset comprises 30 systems with randomized parameters such as inertia, torque constant and rotor lengths as indicated in Table \ref{indist-quadrotor}. Each trajectory is generated from a randomized initial condition to a random terminal position with zero velocity, within a pre-set bounding box, to avoid unrealistic velocities.

In the same manner the OOD validation set contains 10 trajectories, each one corresponding to a system with parameters drawn from the distributions indicated in Table \ref{outdist-quadrotor}.

\begin{table}[ht!]
\centering
% First table in a minipage
\begin{minipage}[t]{0.45\textwidth}
\vspace{0pt}
\centering
\begin{tabular}{{|p{3.5cm}|p{3cm}|}}
\hline
\textbf{Parameters} & \textbf{Values} \\
\hline
Inertia $\mathcal{I}_{diag}$ & $\mathrm{Uniform}(0.1,0.2)$ \\
\hline
mass (m)& $\mathrm{Uniform}(0.5,3.0)$ \\
\hline
$d_{cogs}$ & $\mathrm{Uniform}(0.2,0.5)$  \\
\hline
$C_{torques}$ & $\mathrm{Uniform}(0.001,0.1)$\\
\hline
\end{tabular}
\caption{In-distribution parameters $dt=0.01$, $T=250$.}
\label{indist-quadrotor}
\end{minipage}
\quad
\begin{minipage}[t]{0.45\textwidth}
\vspace{0pt}
\centering
\begin{tabular}{{|p{3.5cm}|p{3cm}|}}
\hline
\textbf{Parameters} & \textbf{Values} \\
\hline
Inertia $\mathcal{I}_{diag}$ & $\mathrm{Uniform}(0.3,0.7)$ \\
\hline
mass (m) & $\mathrm{Uniform}(3.0,5.0)$ \\
\hline
$d_{cogs}$ & $\mathrm{Uniform}(0.5,0.7)$ \\
\hline
$C_{torques}$ & $\mathrm{Uniform}(0.1,0.2)$ \\
\hline
\end{tabular}
\caption{Out-of-distribution parameters $dt=0.01$, $T=500$}
\label{outdist-quadrotor}
\end{minipage}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Near Symplectic Architecture}
In this section, we prove that composing our SymplecticEncoder $\Phi_{\theta_{SE}}$ with our ActiveDecoder $\Phi_{\theta_{AD}}$ handling control signals and dissipative effects yields a near-symplectic map, with an explicit bound on preserving approximate symplectic geometry. This formalizes the realistic effects we can introduce to MetaSym and the inductive bias provided by symplectic invariants. 

\subsection{Overall Map}
The full transformation is
\[
(\mathbf q_t,\mathbf p_t)\;\mapsto\; \mathbf z_c = \Phi_{\theta_{SE}}(\mathbf q_t,\mathbf p_t)
\;\mapsto\;
\mathbf z_d = \Phi_{\theta_{AD}}(\mathbf z_c,\mathbf u_t, \mathbf d_t).
\]
Hence, at the \emph{global} level, we define
\[
\Phi_{\theta}(\mathbf q_t , \mathbf p_t,\mathbf u_t,\mathbf d_t)
:=
\Phi_{\theta_{AD}}\!\bigl(
 \Phi_{\theta_{SE}}(\mathbf q_t, \mathbf p_t),
 \;\mathbf u_t,\;
 \;\mathbf d_t
\bigr).
\]
We aim to show that if $\Phi_{\theta_{AD}}$ remains a \emph{small} (bounded) perturbation from identity in $\mathbf z_c$-space, then $\Phi_{\theta}$ preserves the symplectic structure up to a small error term. This is called \emph{near-symplecticity}. We assume that the dissipation $\mathbf d_t$ is seperable for the sake of this proof.
\subsection{SymplecticEncoder}
From Jin \textit{et al.}, we know that \emph{LA-SympNets} are fully symplectic due to their construction \cite{jin2020sympnetsintrinsicstructurepreservingsymplectic}. By extension, we can show that our SymplecticEncoder $\Phi_{{\theta}_{SE}}$.

\begin{definition}[Symplectic Property]
\label{theorem:symplectic}
Let $\mathcal{X} = \mathbb{R}^{2d}$ represent the canonical phase space with coordinates $\mathbf x \in (\mathbf q,\mathbf p)$. We write
\[
J \;=\;
\begin{pmatrix}
0 & I_d \\[3pt]
-I_d & 0
\end{pmatrix},
\]
the standard $2d\times2d$ symplectic matrix. A differentiable map $\Psi:\mathcal{X}\to\mathcal{X}$ is \emph{strictly symplectic} if
\[
\mathrm{d}\Psi(\mathbf x)^\top\,J\,\mathrm{d}\Psi(\mathbf x)
\;=\;
J
\quad
\forall\,\mathbf x\in\mathcal{X}.
\]
This implies $\det(\mathrm{d}\Psi)=1$ (volume preservation).
\end{definition}

We know 
\[
\Phi_{\theta_{SE}}:\;\mathcal{X}\;\to\;\mathcal{Z}\subseteq \mathbb{R}^{2d}
\]
is strictly symplectic, i.e.
\[
\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x)^\top\,J\,\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x) = J,\quad
\det(\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x))=1.
\]
Internally, since $\Phi_{\theta_{SE}}$ is a composition of symplectic sub-blocks (e.g.\ LA-SympNets). Its parameters, $\theta_{SE}$, can be partially meta-learned as long as the strict symplectic property is retained.

\subsection{Decoder with Control and Dissipation}

We define
\[
\Phi_{\theta_{AD}}: \quad \mathcal{Z}\times\mathcal{U}\times\mathcal{D} 
\;\to\;
\mathcal{Z},
\]
where:
\begin{itemize}
    \item \(\mathcal{Z} \subseteq\mathbb{R}^{2d}\) is the latent phase-space output by \(\Phi_{\theta_{SE}}\),
    \item \(\mathcal{U}\subseteq \mathbb{R}^{m}\) represents \emph{control signals} (bounded by \(\|\mathbf u_t\|\le U_{\max}\)),
    \item \(\mathcal{D}\subseteq \mathbb{R}^{r}\) represents \emph{dissipative parameters} (bounded by \(\|\mathbf d_t\|\le D_{\max}\)), which model forces that remove or drain energy (e.g.\ friction or drag).
\end{itemize}
The decoder modifies the latent state by
\[
\Phi_{\theta_{AD}}(\mathbf z_c, \mathbf u_t, \mathbf d_t)
\;=\;
\mathbf z_c + F_{\theta_{AD}}(\mathbf z_c, \mathbf u_t,\mathbf d_t),
\]
where \(F_{\theta_{AD}}\) can be cross-attention with magnitude modulated by \(\|\mathbf u_t\|\), or a damping formula modulated by \(\|\mathbf d_t\|\).

\subsection{Near-Symplectic Proof and Explicit Bound}
\label{subsec:nearsymplectic-proof}
In this section, we want to prove that if $\mathbf z_c\mapsto \mathbf z_c + F_{\theta_{AD}}(\mathbf z_c)$ is a \emph{bounded perturbation} from identity (in partial derivatives), then the composition with a strict symplectic map remains close to preserving $J$.


\paragraph{Bounded Perturbation Assumption.}
For the ActiveDecoder map, we assume the partial derivative w.r.t.\ $ \mathbf z_c$ in $F_{\theta_{AD}}$ is \emph{bounded} by a linear-type function of $(\|\mathbf{u}_t\|,\|\mathbf{d}_t\|)$:
\[
\Bigl\|
   \tfrac{\partial F_{\theta_{AD}}}{\partial \mathbf z_c}(\mathbf z_c,\mathbf{u}_t,\mathbf{d}_t)
\Bigr\|
\;\;\le\;
\alpha_0 \;+\;\alpha_u\,\|\mathbf{u}_t\| \;+\;\alpha_d\,\|\mathbf{d}_t\|,
\]
for some constants $\alpha_0,\alpha_u,\alpha_d \ge 0$.  This covers \emph{cross-attention} (scaled by $\|\mathbf{u}_t\|$) and \emph{dissipative} (scaled by $\|\mathbf{d}_t\|$) terms. Since $\|\mathbf{u}_t\|\le U_{\max}$ and $\|\mathbf{d}_t\|\le D_{\max}$, we define:
\[
\rho 
\;:=\;
\alpha_0
\;+\;
\alpha_u \,U_{\max}
\;+\;
\alpha_d \,D_{\max}.
\]
Hence, \(\max_{(\mathbf z_c,\mathbf{u}_t,\mathbf{d}_t)} \bigl\|\mathrm{d}_{\mathbf z_c}\Phi_{\theta_{AD}} - I\bigr\|\le \rho\). Equivalently,
\[
\Bigl\|\mathrm{d}_{\mathbf z_c}F_{\theta_{AD}}(\mathbf z_c,\mathbf{u}_t,\mathbf{d}_t)\Bigr\|
\;\le\;
\rho,
\quad
\forall (\mathbf z_c,\mathbf{u}_t,\mathbf{d}_t).
\]

\paragraph{The Composed Map.}
Recall we define the global map
\[
\Phi_{\theta}(\mathbf q_t,\mathbf p_t,\mathbf u_t,\mathbf d_t)
\;=\;
\Phi_{\theta_{AD}}\!\Bigl(
  \Phi_{\theta_{SE}}(\mathbf q_t,\mathbf p_t),
  \;\mathbf u_t,
  \;\mathbf d_t
\Bigr).
\]
Writing $\mathbf x=(\mathbf q_t,\mathbf p_t)\in\mathbb{R}^{2d}$ for convenience, we have
\[
\mathbf z_c 
= \Phi_{\theta_{SE}}(\mathbf x)
\quad\text{and}\quad
\mathbf z_d
= \Phi_{\theta_{AD}}\!\bigl(\mathbf z_c,\;\mathbf u_t,\;\mathbf d_t\bigr).
\]
To show near-symplecticity, we study
\[
\mathrm{d}\Phi_{\theta}(\mathbf x,\mathbf u_t,\mathbf d_t)
=\;
\underbrace{\mathrm{d}_{\mathbf z_c}\!\Phi_{\theta_{AD}}\bigl(\mathbf z_c,\mathbf{u}_t,\mathbf{d}_t\bigr)}_{\textstyle I + A,\;\|A\|\le\rho}
\;\;\times\;\;
\underbrace{\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x)}_{\textstyle \text{strictly symplectic.}}
\]

\begin{theorem}[Near-Symplectic Composition]
\label{thm:nearSymplectic}
Suppose $\Phi_{\theta_{SE}}$ is strictly symplectic, i.e.\ 
\[
\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x)^\top\;J\;\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x)
=\; J
\quad\text{and}\quad
\det(\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x))=1.
\]
Also assume $\mathrm{d}_{\mathbf z_c}\Phi_{\theta_{AD}}$ satisfies the bounded-perturbation condition $\max\|\mathrm{d}_{\mathbf z_c}\Phi_{\theta_{AD}}-I\|\le\rho$ over $\|\mathbf{u}_t\|\le U_{\max}$, $\|\mathbf{d}_t\|\le D_{\max}$. Then for the composed map $\Phi_{\theta}$, we have:
\[
\bigl\|
  \mathrm{d}\Phi_{\theta}(\mathbf x)^\top\,J\,\mathrm{d}\Phi_{\theta}(\mathbf x)
  - 
  J
\bigr\|
\;\le\;
C\,\rho,
\]
for a constant $C>0$ depending on the norm of $\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x)$. Hence $\Phi_{\theta}$ is $\epsilon$-symplectic with $\epsilon=C\,\rho$.
Furthermore,
\[
\det\bigl(\mathrm{d}\Phi_{\theta}(\mathbf x)\bigr)
= \mathbf 1 + O(\rho),
\]
implying near-volume preservation as well.
\end{theorem}

\begin{proof}[Sketch of Proof]
Let $\mathbf x=(\mathbf{q}_t,\mathbf{p}_t)$, $\mathbf z_c=\Phi_{\theta_{SE}}(\mathbf x)$, and $I+A=\mathrm{d}_{\mathbf z_c}\!\Phi_{\theta_{AD}}(\mathbf z_c,\mathbf{u}_t,\mathbf{d}_t)$ with $\|A\|\le\rho$. Then
\[
\mathrm{d}\Phi_{\theta}(\mathbf x,\mathbf u_t,\mathbf d_t)
=
(I + A)\,\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x).
\]
Hence
\[
\mathrm{d}\Phi_{\theta}^\top\,J\,\mathrm{d}\Phi_{\theta}
=
\mathrm{d}\Phi_{\theta_{SE}}^\top\,(I + A)^\top\,J\,(I + A)\,\mathrm{d}\Phi_{\theta_{SE}}.
\]
Expanding 
$(I + A)^\top J (I + A)
= J + A^\top J + J A + A^\top J A 
= J + O(\|A\|)$,
we substitute $\mathrm{d}\Phi_{\theta_{SE}}^\top\,J\,\mathrm{d}\Phi_{\theta_{SE}}=J$:
\[
\mathrm{d}\Phi_{\theta}^\top\,J\,\mathrm{d}\Phi_{\theta}
=
J + O(\|A\|)
=
J + O(\rho).
\]
In operator norm, $\|\mathrm{d}\Phi_{\theta}^\top\,J\,\mathrm{d}\Phi_{\theta}-J\|\le C\,\rho$. Since $\mathrm{d}\Phi_{\theta_{SE}}$ is volume-preserving, $\det(\mathrm{d}\Phi_{\theta})=\det(I + A)\times \mathbf 1= \mathbf 1+O(\rho)$. 
\end{proof}

\paragraph{Explicit Cross-Attention Bound.}
For instance, if $F_{\theta_{AD}}$ includes a \emph{cross-attention} term scaled by $\|\mathbf{u}_t\|$ plus a \emph{dissipative} term scaled by $\|\mathbf{d}_t\|$, we might write
\[
F_{\theta_{AD}}(\mathbf z_c,\mathbf{u}_t,\mathbf{d}_t)
=\;
\alpha\,\mathrm{CrossAttn}(\mathbf z_c,\mathbf{u}_t) 
\;-\;
\gamma\,(\mathbf d_t^\top\!\!\star \mathbf z_c),
\]
where $\mathbf d_t^\top\!\!\star \mathbf z_c$ indicates some parametric dissipator. If $\mathrm{CrossAttn}$ has partial derivative in $\mathbf z_c$ normed by $\|\mathbf{u}_t\|$, and $\mathbf d_t^\top\!\!\star 
\mathbf z_c$ is linear in $\|\mathbf{d}_t\|$, then
\[
\Bigl\|\tfrac{\partial F_{\theta_{AD}}}{\partial \mathbf z_c}\Bigr\|
\;\le\;
\alpha_0 \;+\;\alpha_u\,\|\mathbf{u}_t\|
\;+\;
\alpha_d\,\|\mathbf{d}_t\|,
\]
giving the same $\rho= \alpha_0 + \alpha_u U_{\max} + \alpha_d D_{\max}$. Provided $\rho\ll1$, $\Phi_{\theta}$ remains near-symplectic.




% \subsection{Overall Map}

% Define the composite map
% \[
% \Phi_{\theta}(x,u,d)
% \;=\;
% \Phi_{\theta_{AD}}\bigl(\,\Phi_{\theta_{SE}}(\mathbf x),\,u,\,d\bigr).
% \]
% we assume that the dissipation $d$ is seperable for the sake of notation.
% We wish to show that \(\Phi_\theta\) is \emph{$\epsilon$-symplectic}, i.e.\ 
% \[
% \bigl\|\mathrm{d}\Phi_\theta^\top(\mathbf x)\,J\,\mathrm{d}\Phi_\theta(\mathbf x) - J\bigr\|
% \;\le\;\epsilon,
% \]
% in direct proportion to the “strength” of the control/dissipation.

% \section{Bounding Condition}

% Assume that:
% \[
% \Bigl\|\mathrm{d}_zF_{\theta_{AD}}(z_c,u,d)\Bigr\|
% \;\;\le\;\;
% \alpha_0 \;+\; \alpha_u \,\|u\| \;+\; \alpha_d \,\|d\|,
% \]
% for fixed nonnegative constants \(\alpha_0,\alpha_u,\alpha_d\). If \(\|u\|\le U_{\max}\) and \(\|d\|\le D_{\max}\), then
% \[
% \max\Bigl\|\mathrm{d}_zF_{\theta_{AD}}(\cdot,u,d)\Bigr\|
% \;\le\;
% \alpha_0 + \alpha_u U_{\max} + \alpha_d D_{\max}
% \;\;=\;\;\rho,
% \]
% which is the worst-case derivative norm of the non-Hamiltonian portion. Hence
% \[
% \Bigl\|\mathrm{d}_z \Phi_{\theta_{AD}}(z,u,d) - I\Bigr\|
% =
% \Bigl\|\mathrm{d}_z F_{\theta_{AD}}(z,u,d)\Bigr\|
% \;\le\;\rho.
% \]

% \section{Main Theorem}

% \begin{theorem}[Near-Symplectic Composition]
% \label{thm:nearSymplectic}
% Let $\Phi_{\theta_{SE}}:\mathcal{X}\to\mathcal{Z}$ be strictly symplectic, and let $\Phi_{\theta_{AD}}:\mathcal{Z}\times \mathcal{U}\times \mathcal{D}\to \mathcal{Z}$ satisfy
% \[
% \max_{(z,u,d)}\;\Bigl\|\mathrm{d}_z \Phi_{\theta_{AD}}(z,u,d) - I\Bigr\|
% \;\;\le\;\;\rho
% =
% \alpha_0 
% \;+\;
% \alpha_u\,U_{\max}
% \;+\;
% \alpha_d\,D_{\max}.
% \]
% Then the composition
% \[
% \Phi_\theta(x,u,d)
% =
% \Phi_{\theta_{AD}}\Bigl(\Phi_{\theta_{SE}}(\mathbf x),\,u,\,d\Bigr)
% \]
% is $\epsilon$-symplectic with $\epsilon=C\,\rho$, where $C$ depends on $\sup_x \|\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x)\|^2$. Concretely,
% \[
% \bigl\|\mathrm{d}\Phi_\theta^\top\,J\,\mathrm{d}\Phi_\theta - J\bigr\|
% \;\le\;
% C\,\rho,
% \quad\text{and}\quad
% \det(\mathrm{d}\Phi_\theta)=1+O(\rho).
% \]
% Thus $\Phi_\theta$ preserves the symplectic form up to an $O(\rho)$ error and is volume-preserving up to factor $(1+O(\rho))$.
% \end{theorem}

% \begin{proof}[Proof (Sketch)]
% Using the chain rule on $x\mapsto\Phi_{\theta_{SE}}(\mathbf x)\mapsto \Phi_{\theta_{AD}}$, 
% \[
% \mathrm{d}\Phi_\theta(\mathbf x)
% =
% \mathrm{d}_z\Phi_{\theta_{AD}}\!\bigl(\Phi_{\theta_{SE}}(\mathbf x),u,d\bigr)
% \;\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x).
% \]
% Set $(I + A)=\mathrm{d}_z\Phi_{\theta_{AD}}$, with $\|A\|\le\rho$. Then
% \[
% (I + A)^\top J (I + A)
% =
% J + A^\top J + JA + A^\top J A
% =
% J + O(\|A\|).
% \]
% Since $\mathrm{d}\Phi_{\theta_{SE}}$ is strictly symplectic, 
% \[
% \mathrm{d}\Phi_{\theta_{SE}}(\mathbf x)^\top\,J\,\mathrm{d}\Phi_{\theta_{SE}}(\mathbf x)
% =
% J.
% \]
% Hence
% \[
% \mathrm{d}\Phi_\theta^\top\,J\,\mathrm{d}\Phi_\theta
% =
% \mathrm{d}\Phi_{\theta_{SE}}^\top\,
% \bigl(I + A\bigr)^\top J\bigl(I + A\bigr)\,
% \mathrm{d}\Phi_{\theta_{SE}}
% =
% \mathrm{d}\Phi_{\theta_{SE}}^\top\,
% \bigl[J + O(\|A\|)\bigr]\,
% \mathrm{d}\Phi_{\theta_{SE}}
% =
% J + O(\|A\|).
% \]
% Therefore, $\mathrm{d}\Phi_\theta^\top\,J\,\mathrm{d}\Phi_\theta=J + O(\rho)$ in operator norm, giving near-symplecticness with $\epsilon=C\rho$ for some $C$. The determinant $\det(\mathrm{d}\Phi_\theta)=\det(I + A)\times1=1+O(\rho)$.
% \end{proof}

% \section{Examples and Interpretation}

% \paragraph{1. Cross-Attention + Dissipation.}
% Let 
% \[
% F_{\theta_{AD}}(z,u,d)= \alpha \,\mathrm{CrossAttn}(z,u) - \gamma\, (d\!\star\! z),
% \]
% where $\mathrm{CrossAttn}$ scales with $\|u\|$, and $d\!\star\!z$ means $z$ is scaled or shaped by the parameter $d$. If each of those terms is bounded by $\alpha_u \|u\| + \alpha_d \|d\| + \alpha_0$, we get $\rho=\alpha_0 + \alpha_u U_{\max} + \alpha_d D_{\max}$. Provided $\rho\ll1$, $\Phi_\theta$ remains near-symplectic.

% \paragraph{2. Pure Hamiltonian vs.\ Large Dissipation.}
% If $U_{\max}$ or $D_{\max}$ is large, $\rho$ may approach or exceed 1, which breaks the near-identity assumption in $z$-space. Then the map can no longer be claimed near-symplectic. Conversely, if $U_{\max}=0$ and $D_{\max}=0$ (no forcing, no dissipation), $\rho=0$ and the composition is strictly symplectic.

% \paragraph{3. Meta-Learning.}
% If local parameters are adapted per system, we require each adapted $\theta_{SE}$ remain symplectic and each adapted $\theta_{AD}$ remain in a region ensuring $\|\mathrm{d}_z F_{\theta_{AD}}\|\le\rho$. Then the composition $\Phi_\theta$ is near-symplectic across tasks.

% \section{Conclusion}

% We have presented a \textbf{concrete bound} describing how a strictly symplectic encoder may be combined with a decoder that includes both external control and dissipative forces (i.e., friction, drag, or other forms of energy loss). By restricting the partial derivatives of the decoder to lie below a threshold $\rho$, which depends linearly (or in a controlled fashion) on the maximum control and dissipation amplitudes, the final composite map remains \emph{near-symplectic}, preserving Hamiltonian geometry up to $O(\rho)$ error. This result bridges the gap between purely Hamiltonian modeling and real-world settings involving moderate forcing and dissipation.





% \section*{Symplectic Encoder + Cross-Attention Decoder: \\ A Comprehensive Near-Symplectic Proof}

% \noindent
% \textbf{Notation and Overview.}\\
% \begin{itemize}
%     \item We have a phase-space state $(q,p)\in\mathbb{R}^{2d}$. 
%     \item \textbf{Symplectic Encoder} $E: \mathbb{R}^{2d}\to \mathbb{R}^{2d}$ is strictly symplectic, i.e.
%     \[
%       \mathrm{d}E_x^\top\,J\,\mathrm{d}E_x \;=\; J,\quad
%       \det(\mathrm{d}E_x)=1,
%     \]
%     for all $x=(q,p)$, where
%     \[
%     J \;=\;
%     \begin{pmatrix}
%     0 & I_d\\
%     -I_d & 0
%     \end{pmatrix}.
%     \]
%     \item \textbf{Cross-Attention Decoder} $D: \mathbb{R}^{2d} \times \mathbb{R}^m \to \mathbb{R}^{2d}$, which takes an encoded state $z \in \mathbb{R}^{2d}$ plus external or control data $u \in \mathbb{R}^m$ and returns the next state in latent space. We model it as
%     \[
%        D(z,u) 
%        \;=\; 
%        z \;+\; \alpha\,\mathrm{CrossAttn}(z,u),
%     \]
%     with $\alpha\in\mathbb{R}^+$ a scaling factor and $\mathrm{CrossAttn}(\cdot,\cdot)$ the cross-attention mechanism described below.

%     \item \textbf{Combined Map}: 
%     \[
%     \Phi(q,p,u) 
%     \;=\;
%     D\!\bigl(E(q,p),\,u\bigr).
%     \]
%     We claim $\Phi$ is \emph{near-symplectic}, meaning that its Jacobian $\mathrm{d}\Phi$ preserves the symplectic form $J$ up to an $\mathcal{O}(\epsilon)$ error. Equivalently, $\det(\mathrm{d}\Phi)=1+\mathcal{O}(\epsilon)$, so volumes are distorted by at most $1+\mathcal{O}(\epsilon)$.
% \end{itemize}

% \subsection*{1. Cross-Attention Mechanism and the Near-Identity Assumption}

% \paragraph{Single-Head Formulation.}
% To keep the notation simpler, consider a single-head cross-attention (extending to multi-head is straightforward by summation). Suppose:

% \[
% \mathrm{CrossAttn}(z,u)
% \;=\;
% \mathrm{softmax}\!\Bigl(\tfrac{Q(z)\,K(u)^\top}{\sqrt{d_k}}\Bigr)\;V(u),
% \]
% where 
% \[
% Q: \mathbb{R}^{2d}\!\to \mathbb{R}^{d_k}, \quad
% K: \mathbb{R}^{m}  \!\to \mathbb{R}^{d_k\times m}, \quad
% V: \mathbb{R}^{m}  \!\to \mathbb{R}^{d_v\times m}.
% \]
% In practice, $Q,K,V$ come from linear layers $W_Q,W_K,W_V$ applied to $z$ or $u$. Typically,
% \[
% Q(z) = W_Q\,z,\;\;
% K(u) = W_K\,u^\top,\;\;
% V(u) = W_V\,u^\top,
% \]
% for certain parameter matrices $W_Q,W_K,W_V$.

% \paragraph{Decoder as a Skip-Connection.}
% We define
% \[
% D(z,u)
% \;=\;
% z \;+\; \alpha\,\mathrm{CrossAttn}(z,u).
% \]
% Our \textbf{goal} is to show $\mathrm{d}D_{(z,u)}$ is close to the identity map (i.e.\ $\|\mathrm{d}D - I\|\le \epsilon$) provided $\alpha$ and $\|W_Q,W_K,W_V\|$ are sufficiently small or the domain is constrained.

% \begin{lemma}[Bounded Derivative of Cross-Attention]
% \label{lem:AttnBound}
% Assume each of $W_Q,W_K,W_V$ is bounded in operator norm, and $\mathrm{softmax}$ saturates at moderate logits. Then there exists a constant $C_{\mathrm{attn}}$ such that
% \[
%    \bigl\|\mathrm{d}\bigl[\mathrm{CrossAttn}(z,u)\bigr]\bigr\|
%    \;\le\;
%    C_{\mathrm{attn}},
% \]
% for all $(z,u)$ in a compact domain.
% \end{lemma}
% \begin{proof}[Sketch]
% Write
% \[
% \mathrm{CrossAttn}(z,u)
% \;=\;
% \mathrm{softmax}\!\Bigl(\tfrac{Q(z)K(u)}{\sqrt{d_k}}\Bigr)\,\bigl[V(u)\bigr] 
% \;\in \mathbb{R}^{d_{\mathrm{model}}}.
% \]
% Taking derivative w.r.t.\ $z$:
% \[
% \mathrm{d}_z\mathrm{CrossAttn}
% \;=\;
% \mathrm{d}_z\bigl(\mathrm{softmax}(\cdot)\bigr)\;\times\;\mathrm{d}_z\Bigl(\tfrac{Q(z)K(u)}{\sqrt{d_k}}\Bigr)
% \;+\;\dots
% \]
% Since $K(u),V(u)$ do not depend on $z$, the chain rule yields factors of $W_Q$ but no derivative in $W_K u^\top$. Meanwhile, $\mathrm{softmax}$ has a Jacobian bounded by some $C_{\mathrm{softmax}}$ if the logits stay within a bounded range. Combining these and bounding norms, we get an overall constant $C_{\mathrm{attn}}$. Full detail is standard in attention derivative analyses.
% \end{proof}

% \begin{corollary}[Decoder is Near-Identity]
% \label{cor:DecoderNearId}
% If 
% \(\displaystyle
% \alpha\,C_{\mathrm{attn}}\;\le\;\epsilon,
% \)
% then
% \[
% \bigl\|\mathrm{d}D_{(z,u)} - I_{2d}\bigr\|
% \;\le\;
% \alpha\,\|\mathrm{d}\,\mathrm{CrossAttn}\|
% \;\le\;
% \epsilon.
% \]
% Hence $D(z,u)$ is an $\epsilon$-near-identity map (in the $z$-space) on the relevant domain.
% \end{corollary}

% \subsection*{2. Composition with a Symplectic Encoder}

% We now form the pipeline 
% \[
% \Phi(q,p,u)
% \;=\;
% D\bigl(E(q,p),\,u\bigr).
% \]
% Since $E:\mathbb{R}^{2d}\to\mathbb{R}^{2d}$ is strictly symplectic and $D$ is $\epsilon$-near-identity, we claim $\Phi$ preserves the symplectic form $J$ up to $\mathcal{O}(\epsilon)$.

% \begin{theorem}[Near-Symplectic Encoder--Decoder Composition]
% \label{thm:EncDecSymp}
% Let $E:\mathbb{R}^{2d}\to \mathbb{R}^{2d}$ be a strictly symplectic diffeomorphism and let $D:\mathbb{R}^{2d}\times \mathbb{R}^m \to \mathbb{R}^{2d}$ satisfy $\|\mathrm{d}D_{(z,u)}-I\|\le \epsilon$ for $(z,u)$ in a compact set $\Omega$. Define
% \[
% \Phi(q,p,u)
% \;=\;
% D\!\bigl(E(q,p),\,u\bigr).
% \]
% Then for $(q,p,u)$ s.t.\ $\bigl(E(q,p),u\bigr)\in \Omega$, 
% \[
% \bigl\|
%    \mathrm{d}\Phi_{(q,p,u)}^\top\,J\,\mathrm{d}\Phi_{(q,p,u)}
%    \;-\; J
% \bigr\|
% \;\;\le\;\;C\,\epsilon,
% \]
% for some $C>0$ (depending on $\sup \|\mathrm{d}E\|$ but \emph{not} on $\epsilon$). Equivalently, $\Phi$ is \emph{$\epsilon$-symplectic}.
% \end{theorem}

% \begin{proof}
% By the chain rule,
% \[
% \mathrm{d}\Phi_{(q,p,u)}
% \;=\;
% \mathrm{d}D_{\bigl(E(q,p),u\bigr)} \;\mathrm{d}E_{(q,p)}.
% \]
% Let $A=\mathrm{d}D_{(z,u)} - I$ with $\|A\|\le \epsilon$. Then
% \[
% \mathrm{d}D_{(z,u)} = I + A,\quad
% \mathrm{d}\Phi = (I + A)\,\mathrm{d}E.
% \]
% Hence
% \[
% \mathrm{d}\Phi^\top\,J\,\mathrm{d}\Phi
% \;=\;
% \mathrm{d}E^\top\,(I + A)^\top\,J\,(I + A)\,\mathrm{d}E.
% \]
% Expand:
% \[
% (I + A)^\top\,J\,(I + A)
% \;=\;
% J + A^\top\,J + J\,A + A^\top\,J\,A.
% \]
% Each extra term is $\mathcal{O}(\|A\|)\le \epsilon$. Thus
% \[
% (I + A)^\top\,J\,(I + A)
% \;=\;
% J + O(\epsilon).
% \]
% Because $E$ is symplectic, $\mathrm{d}E^\top\,J\,\mathrm{d}E = J$. So
% \[
% \mathrm{d}\Phi^\top\,J\,\mathrm{d}\Phi - J
% \;=\;
% \mathrm{d}E^\top \bigl[
%     (I + A)^\top\,J\,(I + A) - J
% \bigr]\mathrm{d}E,
% \]
% which is $\mathcal{O}(\epsilon)$ in operator norm, possibly scaled by $\|\mathrm{d}E\|^2\le M^2$. Set $C=M^2\,\max(\ldots)$ to handle constants. Therefore
% \[
% \bigl\|\mathrm{d}\Phi^\top\,J\,\mathrm{d}\Phi - J\bigr\|
% \;\le\;C\,\epsilon,
% \]
% as desired.
% \end{proof}

% \subsection*{3. Measure-Theoretic (Volume-Preserving) Consequence}

% A strictly symplectic map $E$ satisfies $\det(\mathrm{d}E)=1$, so if $D$ has $\|\mathrm{d}D-I\|\le \epsilon$, then by standard perturbation of determinant, we also get
% \[
% \det(\mathrm{d}D_{(z,u)})
% \;=\;
% 1 + O(\epsilon).
% \]
% Hence
% \[
% \det(\mathrm{d}\Phi_{(q,p,u)})
% \;=\;
% \det\bigl(\mathrm{d}D_{E(q,p),\,u}\bigr)
% \;\times\;
% \det\bigl(\mathrm{d}E_{(q,p)}\bigr)
% \;=\;
% (1+O(\epsilon))\times 1 
% \;=\;
% 1 + O(\epsilon).
% \]
% Thus volumes in $\mathbb{R}^{2d}$ are preserved up to a factor $1+O(\epsilon)$, giving an \emph{$\epsilon$-measure-preserving} result. Over many time steps, one expects minimal drift in conserved quantities typical of Hamiltonian flows.

% \begin{corollary}[Near-Volume-Preserving]
% \label{cor:Volume}
% Under the same assumptions, $\Phi$ distorts the Liouville measure by $1+O(\epsilon)$,
% \[
% \mu\bigl(\Phi(A)\bigr)
% \;=\;
% \int_A \bigl|\det(\mathrm{d}\Phi)\bigr|\;\mathrm{d}\mu(\mathbf x)
% \;=\;
% (1+O(\epsilon))\,\mu(A).
% \]
% Hence $\Phi$ is $\epsilon$-measure-preserving.
% \end{corollary}

% \subsection*{4. Putting It All Together}

% \begin{itemize}
%     \item \textbf{Lemma~\ref{lem:AttnBound}} shows cross-attention $\mathrm{CrossAttn}(z,u)$ is bounded in derivative if the linear layer norms $\|W_Q\|,\|W_K\|,\|W_V\|$ and the softmax logits remain in a moderate range.
%     \item \textbf{Corollary~\ref{cor:DecoderNearId}} states that adding a skip-connection $\alpha\,\mathrm{CrossAttn}(z,u)$ makes $D(z,u)$ a small perturbation of the identity ($\|D'-I\|\le \alpha\,C_{\mathrm{attn}}$). 
%     \item \textbf{Theorem~\ref{thm:EncDecSymp}} (plus its measure-theoretic corollary) then applies: composing $E$ (strictly symplectic) with $D$ (near-identity) yields a map $\Phi$ that is \emph{$\epsilon$-symplectic}, i.e.\ 
%     \[
%        \mathrm{d}\Phi^\top\,J\,\mathrm{d}\Phi = J + O(\epsilon), 
%        \quad
%        \det(\mathrm{d}\Phi)=1+O(\epsilon).
%     \]
% \end{itemize}

% Hence the \textbf{Symplectic Encoder} imposes the canonical geometric structure, while the \textbf{Cross-Attention Decoder} modifies the embedding just enough to incorporate control or system-specific signals---without destroying Hamiltonian geometry. If the attention weights remain small in operator norm, the entire pipeline $\Phi$ exhibits near-Hamiltonian/near-symplectic behavior, preserving volumes and drastically reducing unphysical drifts in long-horizon predictions.

% \begin{remark}[Multi-Head and Deep Blocks]
% Multi-head attention is simply
% \[
% \sum_{h=1}^{H}
% \mathrm{softmax}\!\Bigl(\tfrac{Q_h(z)\,K_h(u)^\top}{\sqrt{d_k}}\Bigr)\;V_h(u),
% \]
% plus a possible feed-forward sub-block with skip connections. If each head and feed-forward portion is $\|\cdot\|\le \epsilon/H$, the total derivative remains $\le \epsilon$. Stacking multiple layers is handled similarly: each layer remains a near-identity block, so the entire composition remains near-identity. Thus the full Transformer-like decoder is near-symplectic after the symplectic encoder, provided the parameters are constrained or $\alpha$ is suitably small.
% \end{remark}

% \section*{Conclusion}
% This comprehensive proof integrates:
% \begin{enumerate}
% \item \textbf{Symplecticity} of $E$, ensuring $\mathrm{d}E^\top\,J\,\mathrm{d}E=J$. 
% \item \textbf{Near-Identity Cross-Attention} $D(z,u)=z+\alpha\,\mathrm{CrossAttn}(z,u)$ with bounded derivative. 
% \item \textbf{Composition Lemma}, guaranteeing $\Phi=D\circ E$ stays $\epsilon$-symplectic in operator norm and $1+O(\epsilon)$ measure-preserving.
% \end{enumerate}
% Thus, a learned \emph{Symplectic Encoder} can be augmented with flexible \emph{Cross-Attention} to handle external forcing or system-dependent variations without forsaking the long-term stability and geometric inductive biases characteristic of Hamiltonian systems.



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
