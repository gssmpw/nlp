\subsection{Experimental Setup}
\label{section:experimental_setup}
\textbf{Datasets:} Table~\ref{tab:datasets} provides a detailed breakdown of the SOTA intrusion datasets utilized in our study. 
%For each dataset we follow the data preparation steps outlined in section~\ref{section:data_preparation}. 
% \sean{is this section necessary with reduced page limit?}
% \begin{enumerate}
%     \item X-IIoTID \cite{al2021x}: The dataset consists of 59 features which are collected with the independence of devices and connectivity, generating a holistic intrusion data set to represent the heterogeneity of IIoT systems. It includes novel IIoT connectivity protocols, activities of various devices, and attack scenarios.  
%     \item WUSTL-IIoT \cite{zolanvari2021wustl}: WUSTL-IIoT aims to emulate real-world industrial systems. The dataset is deliberately unbalanced to imitate real-world industrial control systems, consisting of 41 features and 1,194,464 observations.
%     \item CICIDS2017 \cite{Sharafaldin2018TowardGA} The CICIDS2017 dataset includes a comprehensive collection of benign and malicious network traffic. It contains 80 features and represents a broad range of attacks, such as DoS, DDoS, Brute Force, XSS, and SQL Injection, across more than 2.8 million network flows. The dataset is widely used in evaluating intrusion detection systems.
%     \item UNSW-NB15 \cite{moustafa2015unsw, moustafa2016evaluation, moustafa2017novel, moustafa2017big, sarhan2020netflow} UNSW-NB15 is a comprehensive network intrusion dataset created by the University of New South Wales. It contains 49 features representing normal and malicious activities generated using IXIA's network traffic generator, covering a variety of contemporary attack types. 
% \end{enumerate}
For IIoT intrusion, we use IIoT datasets X-IIoTID \cite{al2021x} and WUSTL-IIoT \cite{zolanvari2021wustl}. We also include commonly used network intrusion datasets CICIDS2017 \cite{Sharafaldin2018TowardGA} and UNSW-NB15 \cite{moustafa2015unsw}. For X-IIoTID \cite{al2021x}, CICIDS2017 \cite{Sharafaldin2018TowardGA}, and UNSW-NB15 \cite{moustafa2015unsw}, we split the data across five experiences such that each experience contains two to four attacks. For WUSTL-IIoT \cite{zolanvari2021wustl}, we split the data across four experiences such that each experience contains one attack. We perform this data split to simulate an evolving data stream with emerging cyber attacks over time where each experience contains different attacks. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
    \caption{Selected Intrusion Datasets}
    \centering
    \label{tab:datasets}
    \resizebox{.99\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c}
    \hline
    Dataset    & Size      & Normal Data & Attack Data & Attack Types \\ 
    \hline
    X-IIoTID \cite{al2021x}   & 820,502   & 421,417     & 399,417     & 18           \\
    \hline
    WUSTL-IIoT \cite{zolanvari2021wustl} & 1,194,464 & 1,107,448   & 87,016      & 4       \\
    \hline
    CICIDS2017 \cite{Sharafaldin2018TowardGA} & 2,830,743 & 2,273,097 & 557,646 & 15 \\
    \hline
    UNSW-NB15 \cite{moustafa2015unsw}
 & 257,673 & 164,673 & 93,000 & 10 \\
    \hline
    \end{tabular}}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Baselines:} %Due to the novelty of this problem formulation, there are no directly comparable methods. However, the most similar widely studied problem would be unsupervised continual learning (UCL). Therefore, 
We evaluate our algorithm against two SOTA unsupervised continual learning (UCL) algorithms: the Autonomous Deep Clustering Network (\textbf{ADCN}) \cite{ashfahani2023unsupervised}, and an autoencoder paired with K-Means clustering. The autoencoder K-Means model is combined with Learning without Forgetting \cite{lwf2019Li} continual learning loss; we refer to this model as \textbf{LwF}. Note that both \textbf{ADCN} and \textbf{LwF} require a small amount of labeled normal and attack data to perform classification. We also compare our approach against SOTA ND methods: local outlier factor (\textbf{LOF})\cite{Faber_2024}, one-class support vector machine (\textbf{OC-SVM})\cite{Faber_2024}, principal component analysis (\textbf{PCA})\cite{rios2022incdfm}, and Deep Isolation Forest (\textbf{DIF}) \cite{xu2023deep}. 
%We train the ND algorithms on the clean subset of normal data, $N_c$, and evaluate their performance on the remainder of the dataset. 
Since these ND models cannot be retrained on unlabeled contaminated data, continual learning is not feasible for these methods.

%an autoencoder with K-Means clustering paired with SOTA Learning without Forgetting (LwF) continual loss (LwF) \cite{lwf2019Li}.
%Notably, many SOTA UCL algorithms rely on image-specific contrastive pairs, which is not directly applicable to intrusion detection \cite{madaan2022representational, yu2023scale, fini2022self, liu2024unsupervised}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
    \centering
    \includegraphics[width=.95\linewidth]{figures/cl_experiments.pdf}
    \caption{Continual learning metric results of ADCN\cite{ashfahani2023unsupervised}, LwF\cite{lwf2019Li}, and \Design{}}
    \label{fig:continual_methods_results}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Evaluation Metrics:} To evaluate the model performance, we report $F_{1}$ score. Since there is a class imbalance within these datasets, to simulate real world IDS, $F_{1}$ score gives an accurate idea on attack detection. For the continual learning methods, we evaluate their performance at the end of each training experience on all experience test sets. This generates a matrix of $F_{1}$ score results $R_{ij}$ such that $i$ is the current training experience, and $j$ is the testing experience. To summarize this matrix of results, we report widely used CL metrics \cite{diaz2018don}: average $F_{1}$ score on current experience (AVG), forward transfer (FwdTrans), and backward transfer (BwdTrans). For a matrix $R_{ij}$ with $m$ total experiences, our metrics are formulated as follows: $\text{AVG}_{F_1} = \frac{\sum_{i = j} R_{ij}}{m}$; $\text{FwdTrans}_{F_1} = \frac{\sum_{j>i} R_{ij}}{\frac{m * (m-1)}{2}}$; $\text{BwdTrans}_{F_1} = \frac{\sum_{i}^m R_{mi} - R_{ii}}{\frac{m * (m-1)}{2}}$.
AVG is the average performance on the current test experience at every point of training. FwdTrans is the average performance on ``future'' experiences, which simulates performance on zero-day attacks. Finally, BwdTrans is the average change in performance of ``past'' test experiences at a ``future'' point of training. A negative BwdTrans indicates catastrophic forgetting, whereas a positive BwdTrans  indicates the model actually improved performance on past experiences after learning a future experience. Overall, AVG measures seen attacks, FwdTrans measures zero-day attacks, and BwdTrans measures forgetting. For all metrics, a higher positive result indicates a better performance. 

We also report the threshold-free metric Precision-Recall Area Under the Curve (PR-AUC) \cite{praucDavid06}. Since \Design{} requires selecting a threshold, PR-AUC allows us to assess model performance independently of the threshold. We choose PR-AUC over Receiver Operating Characteristic Area Under the Curve (ROC-AUC) because ROC-AUC can give misleadingly high results in the presence of class imbalance \cite{praucDavid06}.

\textbf{Hyperparameters:} %For $L_{CND}$ hyperparameters are the number of K-Means clusters $K$, the reconstruction loss strength $\lambda_R$,  the continual learning loss strength $\lambda_{CL}$, and the cluster separation loss margin $m$. 
We utilize \textit{elbow method} \cite{han2011data} for determining the number of clusters $K$. 
%It tests a range of $K$ values and then selects the value   where there is a significant change in slope, called the elbow point. 
%This resulted in $K$ values between 100-500. 
We set $\lambda_R$ and $\lambda_{CL}$ to 0.1, and for $m$ we use 2 after careful experimentation. For the AE modules of \Design{}, we use 4-layer MLP with 256 neurons in the hidden layers. We train it using Adam optimizer \cite{kingma2017adammethods} with a learning rate of 0.001. For PCA, we use the explained variance method and set it to 95\% \cite{rios2022incdfm}.

\textbf{Hardware:} We run our experiments on NVIDIA GeForce RTX 3090 GPU, with a AMD EPYC 7343 16-Core processor.

\subsection{Results}

\textbf{Continual Learning Comparison:} Fig.~\ref{fig:continual_methods_results} presents the results of our approach \Design{} compared with ADCN\cite{ashfahani2023unsupervised} and LwF\cite{lwf2019Li}. \Design{} shows the best performance on both seen (AVG) and unseen (FwdTrans) attacks across all datasets. \Design{} also has the highest BwdTrans on all except one dataset (UNSW-NB15). The average BwdTrans of \Design{} (0.87\%) is higher than the average BwdTrans of both ADCN (-0.06\%) and LwF (0.09\%). Notably, the BwdTrans of \Design{} is positive for three datasets. Indicating past experiences actually improve after training on future experiences for these datasets. Given the high FwdTrans as well, our approach finds features that generalize well to future experiences. 

Table~\ref{tab:improvement} shows the improvement of \Design{} over the UCL baselines on all datasets. Bold and underlined cases indicate the best and the second best improvements with respect to each metric, respectively. These improvements were calculated by comparing the performance of \Design{} to the baselines, where the improvement values represent the proportional increase over the baseline performance. We do not include BwdTrans because a proportional increase does not make sense for a metric that can be negative. \Design{} has up to $4.50\times$ and $6.1\times$ AVG improvement on ADCN and LwF, respectively. In addition, \Design{} has up to $6.47\times$ and $3.47\times$ FwdTrans improvement on ADCN and LwF. Averaged across all datasets, \Design{} shows a $1.88\times$ and $1.78\times$ improvement on AVG, and a $2.63\times$ and $1.60\times$ improvement on FwdTrans, compared to ADCN and LwF, respectively. %These results underscore the benefit of our continual novelty detection method \Design{}. The notably high FwdTrans score emphasizes how novelty detection can be used to identify unseen anomalous data, thereby significantly enhancing performance on zero-day attacks.

Overall, these results highlight the benefit of continual ND over UCL methods for IDS. \Design{}, with its PCA-based novelty detector, excels by effectively harnessing the normal data to identify attacks. A key strength of our approach lies in the assumption that normal data forms a distinct class, while everything else is treated as anomalous. This assumption is particularly well-suited to IDS. In contrast, methods like ADCN and LwF do not make this distinction where they handle both normal and attack data similarly, limiting their ability to fully exploit the inherent structure of the data. 



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}[]
% \centering
% \caption{\Design{} Percentage Improvement over UCL Baselines on AVG and FwdTrans}
% \label{tab:improvement}
% \begin{tabular}{|c|c|c|c|}
% \hline
% Baseline      & Dataset    & AVG  & FwdTrans  \\ \hline
% ADCN\cite{ashfahani2023unsupervised}          & X-IIoTID   & 101.88\%        & 400.35\%        \\ \cline{2-4} 
%               & WUSTL-IIoT & 349.86\%        & 546.68\%        \\ \cline{2-4} 
%               & CICIDS2017 & 37.19\%         & 73.46\%         \\ \cline{2-4} 
%               & UNSW-NB15  & 29.25\%         & 43.90\%         \\ \hline
% LwF\cite{lwf2019Li} & X-IIoTID   & 46.43\%         & 35.39\%         \\ \cline{2-4} 
%               & WUSTL-IIoT & 510.92\%        & 246.81\%        \\ \cline{2-4} 
%               & CICIDS2017 & 92.72\%         & 163.81\%        \\ \cline{2-4} 
%               & UNSW-NB15  & 11.07\%         & 2.20\%          \\ \hline
% \end{tabular}
% \end{table}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[]
\centering
\caption{\Design{} Improvement over UCL Baselines}
\label{tab:improvement}
\scalebox{1}{
\begin{tabular}{|c|c|c|c|}
\hline
Baseline      & Dataset    & AVG  & FwdTrans  \\ \hline
ADCN\cite{ashfahani2023unsupervised}  & X-IIoTID   & $\underline{2.02\times}$  & $\underline{5.00\times}$   \\ \cline{2-4} 
                                      & WUSTL-IIoT & $\mathbf{4.50\times}$  & $\mathbf{6.47\times}$   \\ \cline{2-4} 
                                      & CICIDS2017 & $1.37\times$  & $1.73\times$   \\ \cline{2-4} 
                                      & UNSW-NB15  & $1.29\times$  & $1.44\times$   \\ \hline
LwF\cite{lwf2019Li}                   & X-IIoTID   & $1.46\times$  & $1.35\times$   \\ \cline{2-4} 
                                      & WUSTL-IIoT & $\mathbf{6.11\times}$  & $\mathbf{3.47\times}$   \\ \cline{2-4} 
                                      & CICIDS2017 & $\underline{1.93\times}$  & $\underline{2.64\times}$   \\ \cline{2-4} 
                                      & UNSW-NB15  & $1.11\times$  & $1.02\times$   \\ \hline
\end{tabular}}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Figure~\ref{fig:XIIoT_graph} shows the $F_{1}$ score of ADCN and \Design{} for each experience on both datasets. Similarly, we use green and red colors for \Design{} and ADCN respectively. Notably for \Design{}, the $F_{1}$ score of each experience has little change over training time. This highlights the strength of novelty detection for IDSs, as even before seeing attacks \Design{} has good performance. On the other hand, ADCN test experiences do not improve until the associated training experience, meaning ADCN does not have an ability to generalize to future attacks. ADCN utilizes a subset of labeled data to assign labels to clusters. This subset of labeled might be causing ADCN to overfit to the attacks within the current experience, therefore leading ADCN to not generalize well. We can also clearly see that our approach is consistently better (higher $F_{1}$ score) than the state-of-the-art ADCN. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[t]{\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/X-IIoTID-experiences.pdf}
%         \caption{X-IIoTID}
%         \label{fig:ADCN_XIIoT_results}
%     \end{subfigure}
%     \begin{subfigure}[t]{\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/WUSTL-IIoT-experiences.pdf}
%         \caption{WUSTL-IIoT}
%         \label{fig:WUSTL-}
%     \end{subfigure}
%     \caption{$F_1$ Score of ADCN and \Design{} of each test experience over training experiences.}
%     \label{fig:XIIoT_graph}
% \end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Novelty Detectors Comparison:} Fig.~\ref{fig:novelty_methods_results} compares LOF\cite{Faber_2024}, OC-SVM\cite{Faber_2024}, PCA\cite{rios2022incdfm}, and DIF \cite{xu2023deep} with \Design{} on all datasets. The average $F_{1}$ score of the novelty detection methods are compared to the AVG of \Design{}.  It can be seen \Design{} outperforms all other methods across all datasets. The two best performing methods are DIF and PCA. The average $F_{1}$ score improvement across all datasets of \Design{} is $1.16\times$ and $1.08\times$ over DIF and PCA, respectively. These results highlight the critical role of leveraging information from unsupervised data streams. Unlike these ND algorithms, \Design{} is capable of continuously learning from this unsupervised data, enabling it to enhance PCA reconstruction over time. By integrating evolving data patterns, \Design{} not only adapts to new anomalies but also improves its overall detection accuracy, demonstrating a clear advantage in dynamic environments.

%Given that \Design{} employs PCA detection, this indicates that the CFE effectively extracts useful features from the unlabeled training experiences. T

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/novelty_detectors_experiments.pdf}
    \caption{Average $F_1$ score on all experiences of \Design{} and novelty detection methods: LOF, OC-SVM, PCA, DIF}
    \label{fig:novelty_methods_results}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{figure}
    \centering
    \includegraphics[width=0.86\linewidth]{figures/novelty_detectors_pr_auc.pdf}
    \caption{Thresholding Free Evaluation of \Design{}}
    \label{fig:thresholding_free}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Pre-threshold Evaluation:} While thresholding plays a crucial role in attack decision-making, evaluating model prediction performance before applying threshold is also important. The UCL algorithms (ADCN\cite{ashfahani2023unsupervised} and LwF\cite{lwf2019Li}) do not output anomaly scores because they select classes based on the closest labeled cluster. Therefore we compare against the two best ND methods: DIF\cite{xu2023deep} and PCA\cite{rios2022incdfm}. Fig.~\ref{fig:thresholding_free} presents the PR-AUC values of DIF, PCA, and \Design{}. It can be seen that \Design{} provides the best threshold free results, which aligns with the threshold-based results presented earlier. The strong performance of \Design{} in both pre-threshold and threshold-based evaluations demonstrates that the model is robust regardless of the decision threshold. 

\subsection{Ablation Study}

To demonstrate the impact of our loss function components, we perform an ablation study. Table~\ref{tab:ablation_loss} shows the results of \Design{} with each loss function removed to demonstrate their individual effectiveness. Bold and underlined cases indicate the best and the second best performances with respect to each metric, respectively. \Design{} without reconstruction loss ($L_R$) and \Design{} without cluster separation loss ($L_{CS}$) performs worse in all categories. \Design{} without both $L_R$ and continual learning loss ($L_{CL}$) actually performs better AVG but has worse BwdTrans and FwdTrans. AVG does not account for past experiences, so the significantly negative BwdTrans indicates \Design{} w/o $L_R$ and $L_{CL}$ forgets, and therefore would perform worse on those experiences in the future. This would make sense as a regularization loss to improve continual learning would slightly decrease performance in non-continual scenario. Overall \Design{} has the best results when taking every metric category into account. Notably the low BwdTrans and FwdTrans of \Design{} (w/o $L_R$) showcases how the reconstruction loss helps \Design{} generalize better to unseen and past data. This highlights the power of $L_R$ to provide good features for continual learning. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[]
\caption{Ablation Study of \Design{} Loss Functions}
\label{tab:ablation_loss}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Strategy                         & AVG              & BwdTrans        & FwdTrans         \\ \hline
CND-IDS                          &\underline{76.92\%}    & \textbf{0.87\%} & \textbf{73.70\%} \\ \hline
CND-IDS (w/o $L_{CS}$)           & 66.23\%          & \underline{0.09\%}    & 70.26\%          \\ \hline
CND-IDS (w/o $L_R$)              & 72.86\%          & -5.44\%         & 67.82\%          \\ \hline
CND-IDS (w/o $L_R$ and $L_{CL}$) & \textbf{79.92\%} & -11.26\%        & \underline{71.01\%}    \\ \hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Overhead Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}[]
% \centering
% \caption{Average training time and inference time per sample across all datasets in milliseconds}
% \label{tab:overhead}
% \begin{tabular}{|c|c|c|}
% \hline
% Strategy               & Inference Time(ms) \\ \hline
% \Design{}                   & 0.0019             \\ \hline
% ADCN\cite{ashfahani2023unsupervised}    & 0.4061             \\ \hline
% LwF\cite{lwf2019Li}           & 0.0677             \\ \hline
% DIF\cite{xu2023deep}         & 1.0535             \\ \hline
% PCA\cite{rios2022incdfm}       & 0.0018             \\ \hline
% \end{tabular}
% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[]
\centering

\caption{Average inference time (in ms) per test sample}
\label{tab:overhead}
\scalebox{0.95}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Strategy           & \Design{} & ADCN   & LwF    & DIF    & PCA    \\ \hline
Inference Time (ms) & \underline{0.0019}                     & 0.4061 & 0.0677 & 1.0535 & \textbf{0.0018} \\ \hline
\end{tabular}}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Table~\ref{tab:overhead} evaluates the inference overhead of \Design{} compared to ADCN \cite{ashfahani2023unsupervised}, LwF \cite{lwf2019Li}, DIF \cite{xu2023deep}, and PCA \cite{rios2022incdfm}. %, excluding OC-SVM \cite{Faber_2024} and LOF \cite{Faber_2024} due to poor performance. 
\Design{} offers the fastest inference time among continual learning methods. Out of novelty detection methods, \Design{} is second only to PCA. We attribute the efficiency of \Design{} to avoiding the clustering classification used by LwF and ADCN. %\Design{} instead uses PCA reconstruction, which is much quicker than comparing data points to clusters. In addition, 
The difference between \Design{} and PCA is minimal, only 0.0001 milliseconds slower, due to the additional but lightweight step of encoding the data. Considering that the average median flow duration across datasets is 27.77 milliseconds, the overhead introduced by \Design{} is negligible in the context of real-time traffic flow.

%In this section we analyze the inference overhead of \Design{} compared to ADCN\cite{ashfahani2023unsupervised}, LwF\cite{lwf2019Li}, DIF\cite{xu2023deep}, and PCA\cite{rios2022incdfm}. We do not include OC-SVM\cite{Faber_2024} and LOF \cite{Faber_2024} due to weak performance. Table~\ref{tab:overhead} shows the average inference time in milliseconds per sample across all datasets. \Design{} has the best inference time besides PCA. We attribute this good inference time to \Design{} not using clustering classification like LwF and ADCN. Evidently, PCA reconstruction utilized by \Design{} is more time efficient than having to compare a data point to all saved clusters. Compared to pure PCA reconstruction, \Design{} is only 0.0001 ms slower. This small increase in inference time is due to the only added computation at inference is encoding the data with the encoder, which is simply a 4 layer MLP. Across all datasets, the average median travel flow duration is 27.77 ms, and the dataset with the quickest median travel flow is UNSW with 4.29 ms. Therefore the overhead introduced by \Design{} is irrelevant compared to the speed of the traffic flow. 

%\label{section:ablation_study}
%To assess the impact of our design choices, we perform an ablation study. Our goal is to analyze (i) threshold function evaluation, and (ii) novelty detection algorithm selection. 

 

%\textbf{Threshold Function Evaluation:} AE, PCA, and \Design{} all require a threshold to classify an anomaly based on the anomaly score. In all previously reported results, we select a widely used threshold that maximizes the $F_{1}$ score on the test set, i.e., Best-F. %This is not realistic but was used to compare the effectiveness of these methods. In this section 
%Here, we analyze three different threshold methods, which we denote: Best-F \cite{su2019robust}, Top-k \cite{zong2018deep}, and validation percentile (ValPer). Best-F uses the threshold that maximizes the $F_{1}$ score on test set. Top-k utilizes the contamination ratio $r$ of the test set, such that $r$ is the percentage of anomalies within the test set. Top-k selects a threshold so that the percentile of data within the test set classified as anomalies is equal to $r$. ValPer utilizes a validation set of normal data, and selects a threshold such that 99.7\% (3 standard deviations) of the normal data is within this threshold. 
%ValPer is the most realistic method as it does not rely on any information from the test set. 
%A breakdown of the $F_{1}$ score results for the different threshold methods is show in Table~\ref{tab:thresholding_results} where the best within each category is bolded. Overall Best-F performs significantly better than the other threshold methods, which is obvious as Best-F is an upper-bound for threshold selection. However the significant gap highlights the importance of threshold selection. Most importantly, \Design{} still performs better than PCA and AE through all threshold methods. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{table}[]
%    \centering
%    \caption{Threshold Function Evaluation}
%    \resizebox{.97\columnwidth}{!}{
%    \begin{tabular}{c|c|c|c|c}
%        \hline
%         Dataset & Stategy & Best-F & Top-k & ValPer\\
%         \hline
%         & PCA  & 70.9 & 4.03 & 3.56 \\
%         \cline{2-5}
%         X-IIoTID & AE  & 75.6 & 4.03 & 29.4 \\
%         \cline{2-5}
%         & \Design{} & \textbf{78.8} & \textbf{5.63} &  %\textbf{52.9} \\	
%         \hline
%        & PCA  & 85.6 &19.9 & 52.8\\
%         \cline{2-5}
%         WUSTL-IIoT & AE  & 79.6 &19.7 & 37.8\\
%         \cline{2-5}
%         & \Design{} & \textbf{88.2} & \textbf{21.1} & \textbf{55.6}\\	
%         \hline
%    \end{tabular}}
%    \label{tab:thresholding_results}
%\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/novelty_ablation.pdf}
%     \caption{Comparison of \Design{} with PCA and AE novelty detection models}
%     \label{fig:novelty_ablation_results}
% \end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \textbf{Novelty Detection Algorithm Selection:} For \Design{}, we select PCA as the novelty detection algorithm. As shown in Figure~\ref{fig:novelty_methods_results}, both PCA and AE perform well for detecting intrusions. Therefore, we test both AE and PCA as the novelty detection methods for \Design{}. Figure~\ref{fig:novelty_ablation_results} illustrates the AVG performance of \Design{} with AE and PCA as the novelty detection models. It is evident that PCA outperforms AE, justifying our selection of this algorithm for novelty detection. This could be because the CFE utilizes SAEs, which generate features based on the same reconstruction loss used by AE to classify anomalies. It may be beneficial to use PCA as it deconstructs the input in a different manner, thereby identifying different features and functioning better in conjunction with the SAE-based CFE.
