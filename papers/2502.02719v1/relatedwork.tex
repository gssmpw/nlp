\section{Related Work}
\label{sec:related-work}

\textbf{Explaining GNNs}. \SEGNNs are a physiological response to the inherent limitations of \textit{post-hoc} GNN explainers \citep{longa2024explaining, li2024underfire}.
%
\SEGNNs usually rely on regularization terms and architectural biases -- such as attention \citep{miao2022interpretable, miao2022interpretablerandom, lin2020graph, serra2022learning, wu2022discovering, chen2024howinterpretable}, prototypes \citep{zhang2022protgnn, ragno2022prototype, dai2021towards, dai2022towards}, or other techniques \citep{yu2020graph, yu2022improving, giunchiglia2022towards, ferrini2024self} -- to encourage the explanation to be human interpretable. 
%
Our work aims at understanding the formal properties of their explanations, which have so far been neglected.


\textbf{Beyond subgraph explanations.} 
%
% \citet{azzolin2022global} and \citet{armgaan2024graphtrail} proposed to extract post-hoc explanations in the form of logic formulas. However, those formulas express co-occurrence patterns of simpler subgraphs, making the final explanation still dependent on subgraphs.
%
\citet{pluska2024logical} and \citet{kohler2024utilizing} proposed to distill a trained GNN into an interpretable logic classifier.   Their approach is however limited to \textit{post-hoc} settings, and the extracted explanations are human-understandable only for simple datasets.
%
Nonetheless, this represents a promising future direction for integrating a logic-based rule extractor as a side channel in our \GLSSEGNNs framework.
%
\citet{Muller2023graphchef} and \citet{bechler-speicher2024gnan} introduced two novel interpretable-by-design GNNs that avoid extracting a subgraph explanation altogether, by distilling the GNN into a Decision Tree, or by modeling each feature independently via learnable shape functions, respectively.
%
However, we claim that subgraph-based explanations are desirable for existential motif-based tasks, and thus, we %aim to keep them in conjunction with an interpretable classifier to model both topological and non-topological reasoning.
strike for a middle ground between subgraph and non-subgraph-based explanations.


\textbf{Formal explainability}.  While GNN explanations are commonly evaluated in terms of faithfulness \citep{agarwal2023evaluating, christiansen2023faithful, azzolin2024perkspitfalls}, formal explainability has predominantly been studied for non-relational data, where it primarily focuses on PI explanations \citep{marques2023logic, darwiche2023complete, wang2021probabilistic}.
%
Important properties of PI explanations include sufficiency, minimality, and their connection to counterfactuals \citep{marques2022delivering}.  Simultaneously, PI explanations can be exponentially many (but can be summarized \citep{yu2023formal} for understanding) and are intractable to find and enumerate for general classifiers \citep{marques2023logic}.
%
Our work is the first to systematically investigate formal explainability for GNNs and elucidates the link between PI explanations and \SEGNNs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%