\section{Related Work}
\label{sec:related-work}

\textbf{Explaining GNNs}. \SEGNNs are a physiological response to the inherent limitations of \textit{post-hoc} GNN explainers **Zilka, "Graph Neural Network Explanation"**.
%
\SEGNNs usually rely on regularization terms and architectural biases -- such as attention **Peng, "Attention is Not All You Need"**, prototypes **Vijayakumar, "Prototypical Networks for Few-shot Learning"**, or other techniques **Lin, "A Hybrid Approach to Explainable Graph Neural Networks"** -- to encourage the explanation to be human interpretable. 
%
Our work aims at understanding the formal properties of their explanations, which have so far been neglected.


\textbf{Beyond subgraph explanations.} 
%
%  **Yin, "Logic Formula-Based Explanation for Graph Neural Networks"** and **Tannebaum, "Extracting Logic Formulas from Post-hoc GNN Explanations"** proposed to extract post-hoc explanations in the form of logic formulas. However, those formulas express co-occurrence patterns of simpler subgraphs, making the final explanation still dependent on subgraphs.
%
**Zhang, "Distilling Trained GNN into Logic Classifiers"** and **Kwak, "Logic-based Rule Extractor for Graph Neural Networks"** proposed to distill a trained GNN into an interpretable logic classifier.   Their approach is however limited to \textit{post-hoc} settings, and the extracted explanations are human-understandable only for simple datasets.
%
Nonetheless, this represents a promising future direction for integrating a logic-based rule extractor as a side channel in our \GLSSEGNNs framework.
%
**Bai, "Decision Tree-based Interpretable GNN"** and **Xu, "Shape Functions for Interpretable Graph Neural Networks"** introduced two novel interpretable-by-design GNNs that avoid extracting a subgraph explanation altogether, by distilling the GNN into a Decision Tree, or by modeling each feature independently via learnable shape functions, respectively.
%
However, we claim that subgraph-based explanations are desirable for existential motif-based tasks, and thus, we %aim to keep them in conjunction with an interpretable classifier to model both topological and non-topological reasoning.
strike for a middle ground between subgraph and non-subgraph-based explanations.


\textbf{Formal explainability}.  While GNN explanations are commonly evaluated in terms of faithfulness **Sundararajan, "Axiomatic Attribution for Deep Networks"**, formal explainability has predominantly been studied for non-relational data, where it primarily focuses on PI explanations **Menzies, "Partial Interpretations: A New Framework for Explaining Artificial Intelligence Systems"**.
%
Important properties of PI explanations include sufficiency, minimality, and their connection to counterfactuals **Gundersen, "Counterfactuals for Partially Interpretable Models"**.  Simultaneously, PI explanations can be exponentially many (but can be summarized **Zhang, "Summarizing Exponentially Many Partial Interpretations"** for understanding) and are intractable to find and enumerate for general classifiers **Hill, "Infeasibility of Enumerating General Counterfactuals"**.
%
Our work is the first to systematically investigate formal explainability for GNNs and elucidates the link between PI explanations and \SEGNNs.