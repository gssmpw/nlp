%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage[preprint]{LoG/log_2024}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{xcolor}
% For theorems and such
% \usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{researchpack}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{duckuments}
\usepackage[backref=page]{hyperref}
\usepackage[capitalise,nameinlink]{cleveref}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{nicefrac}
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{tablefootnote}
\usepackage{soul}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage[sort,round]{natbib}
% \usepackage{dirtytalk}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newif\ifshowcomments
% \showcommentstrue
\showcommentsfalse

\newcommand{\SM}[1]{%
  \ifshowcomments
    \textcolor{blue}{\textbf{[SM: #1]}}\xspace%
  \fi
}
\newcommand{\SA}[1]{%
  \ifshowcomments
    \textcolor{orange}{\textbf{[SA: #1]}}\xspace%
  \fi
}
\newcommand{\ST}[1]{%
  \ifshowcomments
    \textcolor{magenta}{\textbf{[ST: #1]}}\xspace%
  \fi
}
\newcommand{\AP}[1]{%
  \ifshowcomments
    \textcolor{red}{\textbf{[AP: #1]}}\xspace%
  \fi
}

\usepackage{thmtools, thm-restate}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\struct}{\omega}
\newcommand{\F}{\mathcal{F}}

\newcommand{\nentry}[2]{#1 \text{\tiny{$\pm$ #2}}}

\newcommand{\NEC}{\ensuremath{\mathsf{Nec}}\xspace}
\newcommand{\SUF}{\ensuremath{\mathsf{Suf}}\xspace}
\newcommand{\FAITH}{\ensuremath{\mathsf{Faith}}\xspace}
\newcommand{\ROB}{\ensuremath{\mathsf{Rob}}\xspace}
\newcommand{\UNF}{\ensuremath{\mathsf{Unf}}\xspace}
\newcommand{\FIDP}{\ensuremath{\mathsf{Fid}\scalebox{0.9}{+}}\xspace}
\newcommand{\FIDM}{\ensuremath{\mathsf{Fid}\scalebox{0.9}{-}}\xspace}
\newcommand{\RFIDP}{\ensuremath{\mathsf{RFid}\scalebox{0.9}{+}}\xspace}
\newcommand{\RFIDM}{\ensuremath{\mathsf{RFid}\scalebox{0.9}{-}}\xspace}
\newcommand{\PROBSUFF}{\ensuremath{\mathsf{PS}}\xspace}
\newcommand{\PROBNEC}{\ensuremath{\mathsf{PN}}\xspace}
\newcommand{\PR}{\ensuremath{p_{R}}\xspace}
\newcommand{\PC}{\ensuremath{p_{C}}\xspace}

\newcommand{\upd}{\ensuremath{\mathsf{update}}\xspace}
\newcommand{\aggr}{\ensuremath{\mathsf{aggr}}\xspace}
\newcommand{\MONO}{\ensuremath{g}\xspace}
\newcommand{\DET}{\ensuremath{q}\xspace}
\newcommand{\CLF}{\ensuremath{f}\xspace}

\newcommand{\DIGNN}{DI-GNN\xspace}
\newcommand{\DIGNNs}{DI-GNNs\xspace}
\newcommand{\SEGNN}{SE-GNN\xspace}
\newcommand{\SEGNNs}{SE-GNNs\xspace}
\newcommand{\GL}{Dual-Channel\xspace}
\newcommand{\GLS}{DC}
\newcommand{\GLSEGNN}{\GL GNN\xspace}
\newcommand{\GLSEGNNs}{\GL GNNs\xspace}
\newcommand{\GLSSEGNN}{\GLS-GNN\xspace}
\newcommand{\GLSSEGNNs}{\GLS-GNNs\xspace}
\newcommand{\SEGNNEXPL}{Trivial Explanation\xspace}
\newcommand{\SEGNNEXPLs}{Trivial Explanations\xspace}

\newcommand{\PIONE}{PI1\xspace}
\newcommand{\PITWO}{PI2\xspace}
\newcommand{\PITHREE}{PI3\xspace}

\newcommand{\LENs}{\texttt{LENs}\xspace}
\newcommand{\BLENs}{\texttt{(B)LENs}\xspace}
\newcommand{\GIN}{\texttt{GIN}\xspace}
\newcommand{\GIB}{\texttt{GIB}\xspace}
\newcommand{\VGIB}{\texttt{VGIB}\xspace}
\newcommand{\LRI}{\texttt{LRI}\xspace}
\newcommand{\GSAT}{\texttt{GSAT}\xspace}
\newcommand{\GLGSAT}{\texttt{\GLS-GSAT}\xspace}
\newcommand{\GISST}{\texttt{GISST}\xspace}
\newcommand{\GLGISST}{\texttt{\GLS-GISST}\xspace}
\newcommand{\RAGE}{\texttt{RAGE}\xspace}
\newcommand{\GMT}{\texttt{GMT}\xspace}
\newcommand{\LXGNN}{\texttt{L2XGNN}\xspace}
\newcommand{\SMGNN}{\texttt{SMGNN}\xspace}
\newcommand{\GLSMGNN}{\texttt{\GLS-SMGNN}\xspace}

\newcommand{\TopoC}{$\mathsf{Topo}$}
\newcommand{\FlatC}{$\mathsf{Rule}$}

\newcommand{\Motif}{\texttt{GOODMotif}\xspace}
\newcommand{\BAColor}{\texttt{RedBlueNodes}\xspace}
\newcommand{\TopoFeature}{\texttt{TopoFeature}\xspace}
\newcommand{\MUTAG}{\texttt{MUTAG}\xspace}
\newcommand{\BBBP}{\texttt{BBBP}\xspace}
\newcommand{\MNIST}{\texttt{MNIST75sp}\xspace}
\newcommand{\AIDS}{\texttt{AIDS}\xspace}
\newcommand{\AIDSC}{\texttt{AIDSC1}\xspace}
\newcommand{\SST}{\texttt{Graph-SST2}\xspace}
\newcommand{\BAMotif}{\texttt{BA2Motif}\xspace}

\newcommand{\cmark}{\textcolor{ForestGreen}{\ding{51}}\xspace}
\newcommand{\xmark}{\textcolor{BrickRed}{\ding{55}}\xspace}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\title[Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective]{Beyond Topological Self-Explainable GNNs: \\ A Formal Explainability Perspective}

\author[Steve. Azzolin et al.]{%
Steve Azzolin\thanks{Equal contribution.}\\
University of Trento\\
\email{steve.azzolin@unitn.it}\And
Sagar Malhotra\footnotemark[1]\\
TU Wien\\
\email{sagar.malhotra@tuwien.ac.at}\And
Andrea Passerini\\
University of Trento\\
\email{andrea.passerini@unitn.it}\And
Stefano Teso\\
University of Trento\\
\email{stefano.teso@unitn.it}
}

\begin{document}

\maketitle


\begin{abstract}
Self-Explainable Graph Neural Networks (SE-GNNs) are popular explainable-by-design GNNs, but the properties and the limitations of their explanations are not well understood.
Our first contribution fills this gap by formalizing the explanations extracted by SE-GNNs, referred to as Trivial Explanations (TEs), and comparing them to established notions of explanations, namely Prime Implicant (PI) and faithful explanations.
Our analysis reveals that TEs match PI explanations for a restricted but significant family of tasks. 
In general, however, they can be less informative than PI explanations and are surprisingly misaligned with widely accepted notions of faithfulness.
Although faithful and PI explanations are informative, they are intractable to find and we show that they can be prohibitively large.
Motivated by this, we propose Dual-Channel GNNs that integrate a white-box rule extractor and a standard SE-GNN, adaptively combining both channels when the task benefits.
Our experiments show that even a simple instantiation of Dual-Channel GNNs can recover succinct rules and perform on par or better than widely used SE-GNNs.
Our code can be found in the supplementary material.
\end{abstract}


\section{Introduction}

\textit{\textbf{Self-Explainable GNNs}} (\SEGNNs) are Graph Neural Networks \citep{scarselli2008graph, wu2020comprehensive} designed to combine high performance and \textit{ante-hoc} interpretability.
%
%
In a nutshell, a \SEGNN integrates two GNN modules:  an \textit{\textbf{explanation extractor}} responsible for identifying a class-discriminative subgraph of the input and a \textit{\textbf{classifier}} mapping said subgraph onto a prediction.
%
Since this subgraph, taken in isolation, is enough to infer the prediction, it plays the role of a local explanation thereof.
%
Despite the popularity of \SEGNNs \citep{miao2022interpretable, lin2020graph, serra2022learning, zhang2022protgnn, ragno2022prototype, dai2022towards}, little is known about the properties and limitations of their explanations.
%
Our work fills this gap.

Focusing on graph classification, we show that popular \SEGNNs are implicitly optimized for generating \textit{\textbf{\SEGNNEXPLs}} (TEs), \ie minimal subgraphs of the input that locally ensure the classifier outputs the target prediction.
%
Then we compare TEs with two other families of formal explanations: \textit{\textbf{Prime Implicant}} explanations\footnote{Also known as sufficient reasons \citep{darwiche2023complete}.} (PIs) and \textit{\textbf{faithful}} explanations.
%
Faithful explanations are subgraphs that are \textit{sufficient} and \textit{necessary} for justifying a prediction, \ie they capture all and only those elements that cause the predicted label \citep{yuan2022explainability, Juntao2022CF2, agarwal2023evaluating, azzolin2024perkspitfalls}.
%
PIs, instead, are minimally sufficient explanations extensively studied in formal explainability of tabular and image data \citep{marques2023logic, darwiche2023complete, wang2021probabilistic}.
% PI explanations, instead, have been extensively studied in formal explainability of tabular and image data \citep{marques2023logic, darwiche2023complete, wang2021probabilistic} and are sufficient by construction.  
They are also highly informative, 
% \eg the set of PIs is enough to reconstruct the decision surface of any tabular classifier\footnote{Any propositional formula can be rewritten in disjunctive normal form as a disjunction of implicants; the set of prime implicants covers the full set of implicants.}.
\eg for any propositional formula the set of PIs is enough to reconstruct the original formula \citep{Ignatiev2015PIrecoverformula}.
%
Moreover, both PI and sufficient explanations are tightly linked to counterfactuals \citep{beckers2022causal} and adversarial robustness \citep{ignatiev2019relating}.

% Our results highlight that TEs match PI explanations for the restricted but significant family of \textit{\textbf{motif-based prediction tasks}}, \ie those in which the label depends on the presence of topological motifs.
Our results show that TEs match PI explanations in the restricted but important family of \textbf{\textit{motif-based prediction tasks}}, where labels depend on the presence of topological motifs.
%
Although in these tasks TEs inherit all benefits of PIs, in general they are neither faithful nor PIs.


Motivated by this, we introduce \textit{\textbf{\GLSEGNNs}} (\GLSSEGNNs), a novel family of \SEGNNs that aim to extend the perks of motif-based tasks to more general settings.
%
\GLSSEGNNs combine a \SEGNN and a non-relational white-box predictor, adaptively employing one or both depending on the task.  Intuitively, the non-relational channel handles non-topological aspects of the input, leaving the \SEGNN free to focus on topological motifs with TEs. 
%
This setup encourages the corresponding TEs to be more compact, all while avoiding the (generally exponential \citep{marques2023logic}) computational cost of extracting PIs explicitly.
%
Empirical results on three synthetic and five real-world graph classification datasets highlight that \GLSSEGNNs perform as well or better than \SEGNNs by adaptively employing one channel or both depending on the task.


\textbf{Contributions}.  Summarizing, we:
%
\begin{itemize}[leftmargin=1.25em]

    \item Formally characterize the class of explanations that \SEGNNs optimize for, namely TEs.

    \item Show that TEs share key properties of PI and faithful explanations for motif-based prediction tasks.

    \item Propose \GLSEGNNs and compare them empirically to representative \SEGNNs, highlighting their promise in terms of explanation size and performance.
    
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Preliminaries}
\label{sec:background}

We will use $g$ to represent a \textit{\textbf{graph classifier}}, usually representing a GNN.
%
Given a graph $G = (V, E)$, we will use $g(G)$ to denote $g$'s predicted label on $G$.
%
Additionally, graphs can be annotated with edge or node features.
%
Throughout, we will use the notation $R \subset G$ to denote that $R$ is a \textit{\textbf{subgraph}} of $G$ and $R \subseteq G$ to denote that $R$ may be a subgraph or $G$ itself.  
Note that we assume that a subgraph $R \subseteq G$ can contain all, less, or none of the features of the nodes (or edges) in $G$.
%
We will use $|G|$ to indicate the size of the graph $G$.
%
The size may be defined in terms of the number of nodes, edges, features, or their combination, and the precise semantics will be clear from the context.


\textbf{Self-explainable GNNs}.  \SEGNNs are designed to complement the high performance of regular GNNs with \textit{ante-hoc} interpretability.
%
They pair an \textit{\textbf{explanation extractor}} $\DET$ mapping the input $G$ to a subgraph $\DET(G) = R \subseteq G$ playing the role of a local explanation, and a \textit{\textbf{classifier}} $\CLF$ using $R$ to infer a prediction:
%
\[
    g(G) = f(q(G))
\]
%
In practice, the explanation extractor $\DET$ outputs per-edge \textit{\textbf{relevance scores}} $p_{uv} \in \bbR$, which are translated into an edge-induced subgraph $R$ via thresholding \citep{yu2022improving} or top-$k$ selection \citep{miao2022interpretable, chen2024howinterpretable}.
%
%\ST{watch out:} Relevance scores are then used to weigh each edge's contribution during the classifier's aggregation.
%
While we focus on per-edge relevance scores due to their widespread use, our results equally apply to per-node relevance scores \citep{miao2022interpretable}.

Approaches for training \SEGNNs are designed to extract 
% A widespread approach for training \SEGNNs is to extract 
an interpretable subgraph that suffices to produce the target prediction, often formalized in terms of sparsity regularization (Sparsity) \citep{lin2020graph, serra2022learning} or the Information Bottleneck (IB) \citep{Tishby2000TheIB}.
%
Since the exact IB is intractable and difficult to estimate \citep{kraskov2004estimating, mcallester2020formal}, common approaches devise bounds %either based on the size of R \citep{lin2020graph, yu2021recognizing, serra2022learning} or 
on the divergence between the relevance scores and an uninformative prior controlled by the parameter $r \in [0,1]$ \citep{miao2022interpretable, miao2022interpretablerandom}.
%
% This work focuses on the two popular training objectives reported in \cref{tab:taxonomy-loss}.  \ST{reviewer 2: what about other SEGNNs?  why do we focus specifically on these?  we should say something either here or, better, in the limitations section}
This work focuses on representative training objectives for both Sparsity- and IB-based \SEGNNs, indicated in \cref{tab:taxonomy-loss}. %\ST{reviewer 2: what about other SEGNNs?  why do we focus specifically on these?  we should say something either here or, better, in the limitations section} \SA{Will write in limitations}

% Alternative formulations of \SEGNNs exist -- such as extracting prototypes \citep{zhang2022protgnn, ragno2022prototype, dai2021towards} or explanatory meta-paths \citep{ferrini2024self}, optimizing for the invariance of the extracted subgraph \citep{wu2022discovering} or optimizing to extract more faithful explanations \citep{deng2024sunny}.


\textbf{Logical classifiers}.  We will use basic concepts from First-Order Logic (FOL) as described in \citet{barcelo2020logical} and \citet{grohe2021logic}, and use $E(x,y)$ to denote an undirected edge between $x$ and $y$.
%
A FOL boolean classifier is a FOL sentence $\Phi$ that labels an instance $G$ positively if $G\models \Phi$ and negatively otherwise.
%
We present two examples of FOL classifiers that will be relevant to our discussion: $\exists x \exists y. E(x,y)$ classifies a graph positively if it has an edge, while $\forall x \exists y. E(x,y)$ when it has no isolated nodes.
%
Note that both can be expressed by a GNN, as they are in the logical fragment $\mathrm{C}^2$ (see Theorem IX.3 in \citet{grohe2021logic}). 
We say that two classifiers are distinct if there exists at least one instance where their predictions differ.
%
Although most of our results discuss general graph classifiers, they equally apply to the specific case of GNNs.


\begin{table}[!t]
    \centering
    \small
    \scalebox{0.8}{
    \begin{tabular}{llc}
        \toprule
        \textsc{Model}
            & \textsc{Group}
            & \textsc{Learning Objective}
        \\
        \midrule
        \GISST \citep{lin2020graph}
            & Sparsity
            & $\calL + \frac{\lambda_1}{|E|}\sum_{(u,v) \in E} p_{uv} + \frac{\lambda_2}{|E|}\sum_{(u,v) \in \calE} p_{uv}\log(p_{uv}) + (1-p_{uv})\log(1-p_{uv})$
        \\
        % \RAGE \citep{kosan2023robust} & Missing details in the paper
        % \\
       \makecell[l]{
            \GSAT, \LRI \citep{miao2022interpretable, miao2022interpretablerandom}, 
            \\
            \GMT \citep{chen2024howinterpretable}
        }
            & IB
            & $\calL + \lambda_1 \sum_{(u,v) \in E} p_{uv}\log(\frac{p_{uv}}{r}) + (1-p_{uv})\log(\frac{1-p_{uv}}{1-r})$
        \\
        % \LRI \citep{miao2022interpretablerandom}
        %     & "
        % \\
        % \VGIB \citep{yu2022improving} & Missing details in the paper
        % \\
        % \GMT \citep{chen2024howinterpretable}
        %     & InfoBott
        %     & $\calL + \lambda_1 \sum_{(u,v) \in \calE} p_{uv}\log(\frac{p_{uv}}{r}) + (1-p_{uv})\log(\frac{1-p_{uv}}{1-r})$
        % \\
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Training objectives of popular \SEGNNs}. $\calL$ is the cross-entropy loss between model predictions $\CLF(\DET(G))$ and target variable $Y$.  \cref{thm:segnnexpl-losses} shows that all of them optimize for generating \SEGNNEXPLs (\cref{def:segnn-explanations}).}
    \label{tab:taxonomy-loss}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{What are SE-GNN Explanations?}
\label{sec:trivial-explanations}

Despite being tailored for explainability, \SEGNNs lack a precise description of the properties of the explanations they extract.
%
In this section, we present a formal characterization (in \cref{def:segnn-explanations}) of explanations extracted by SE-GNNs, called \textit{\textbf{\SEGNNEXPLs}} (TEs).
%
In \cref{thm:segnnexpl-losses} we show that, for an \SEGNN with perfect predictive accuracy and a \emph{hard} explanation extractor, the loss functions in \cref{tab:taxonomy-loss} are minimal iff the explanation $R$ is a TE.


\begin{definition}[\SEGNNEXPLs]
\label{def:segnn-explanations}
    Let $g$ be a graph classifier and $G$ be an instance with predicted label $g(G)$, then $R$ is a Trivial Explanation for $g(G)$ if:%
    %
    \begin{enumerate}
        
        \item $R \subseteq G$
        
        \item $g(G) = g(R)$
        
        \item There does not exist an explanation $R' \subseteq G$ such that $|R'| < |R|$ and $g(G) = g(R')$. % (Minimum explanation)
    
    \end{enumerate}
    %
    \SEGNNEXPLs are not unique, and we use $\mathrm{TE}(g(G))$ to denote the set of all trivial explanations for $g(G)$.
\end{definition}


Since our goal is to formally analyze explanations extracted by \SEGNNs, we make idealized assumptions about the classifier $f$.
%
% Since, we want to fous only on the nature of explanations, we make idealized assumptions about the classifier. 
%
Hence, we assume that the \SEGNN is expressive enough and attains perfect predictive accuracy, \ie it always returns the correct label for $G$.
%
We also assume that it learns a \emph{hard} explanation extractor, \ie \DET outputs scores saturated\footnote{For IB-based losses in \cref{tab:taxonomy-loss} scores saturate to $\{r,1\}$.} to $\{0,1\}$ \citep{yu2020graph}.
% Note that this behavior is indeed a desired property of a well-learned subgraph extractor, i.e., it returns saturated $0/1$ relevance scores.
%
Under these assumptions, we show that \SEGNNs in \cref{tab:taxonomy-loss} optimize for generating TEs. 


\begin{restatable}{theorem}{segnnexpllosses}
    \label{thm:segnnexpl-losses}
    Let $\MONO$ be an \SEGNN with a ground truth classifier $\CLF$ (i.e., $\CLF(G)$ always returns the true label for $G$), a hard explanation extractor $\DET$, and perfect predictive accuracy. 
    %
    Then, $\MONO$ achieves minimal true risk (as indicated in Table \ref{tab:taxonomy-loss}) if and only if for any instance $G$, $\DET(G)$ provides Trivial Explanations for the predicted label $\MONO(G)$.
\end{restatable}
\begin{proof}[Proof Sketch]
        For a hard explanation extractor \DET, the risk terms in \cref{tab:taxonomy-loss} reduce to
        %
        $\calL ( \CLF(\DET(G)), Y ) + \lambda_1 |\DET(G)| / |E| \nonumber$
        %
        and
        %
        $\calL ( \CLF(\DET(G), Y ) + \lambda_1 |\DET(G)|\log(r^{-1})  \nonumber$.
        %
        Since the \SEGNN attains perfect predictive accuracy, $\calL (\CLF(\DET(G)), Y )$ is minimal. Hence, for both cases, the total risk is minimized when $\DET(G)$ is the smallest subgraph that preserves the label, i.e., a \SEGNNEXPL.
        % minimal. That is when $q$ returns the smallest subgraph $R$ that returns the same label as $g(G)$.
\end{proof}


Proofs relevant to the discussion are reported in the main text, while the others are available in \cref{sec:proofs}.
%
% Alternative formulations of \SEGNNs exist -- such as extracting prototypes \citep{zhang2022protgnn, ragno2022prototype, dai2021towards, dai2022towards} or explanatory meta-paths \citep{ferrini2024self}, optimizing for the invariance of the extracted subgraph \citep{wu2022discovering} or optimizing to extract more faithful explanations \citep{deng2024sunny} -- and will be discussed separately when relevant.
%
\begin{remark}
\label{rem:inform}
    A simple desideratum for any type of explanation is that it \textit{\textbf{gives information about the classifier beyond the predicted label}}. 
    %
    A weak formulation of this desideratum is that explanations for two distinct classifiers should differ for at least one instance where they predict the same label\footnote{This desideratum can be seen as the dual of the Implementation Invariance Axiom \citep{sundararajan2017axiomatic}.}. 
\end{remark}
%
The following theorem, however, shows that TEs can fail to satisfy this desideratum for general prediction tasks.

\begin{theorem}
\label{thm:TE_Inexpressive}
   % There exist two different GNNs $g$ and $g'$ (i.e., $g(B) \neq g'(B)$ for some instance $B$) such that for any $G$ where $g(G) = g'(G)$ we have that
   There exist two distinct classifiers $g$ and $g'$ such that for any $G$ where $g(G) = g'(G)$ we have that
   \begin{equation}
   \label{eq: same_TE}
        \mathrm{TE}(g(G)) =  \mathrm{TE}(g'(G))
   \end{equation}
\end{theorem}
\begin{proof}
    Let $g$ and $g'$ be boolean graph classifiers given by FOL formulas $g = \exists x \exists y. E(x,y)$ and $g' = \forall x \exists y. E(x,y)$ respectively.
    %
    Let $G$ be any positive instance for both $g$ and $g'$, and let $E$ and $V$ be the set of edges and nodes in $G$, respectively. 
    For any $e \in E$, we have that $e$ is a \SEGNNEXPL for both $g(G)$ and $g'(G)$ (e.g. see \cref{fig:examples_TE_PI_FAITH}).
    %
    % Furthermore, for any positive instance $G$, any edge constitutes a \SEGNNEXPL for both $g$ and $g'$. 
    %
    Hence, $\mathrm{TE}(g(G)) = \mathrm{TE}(g'(G)) = E$.
    %
    % Similarly, given a negative instance $G$ for both $g$ and $g'$, we have that a \SEGNNEXPL is a subgraph consisting of any single node from $G$, and $\mathrm{TE}(g(G)) = \mathrm{TE}(g'(G)) = V$.
    Similarly, if $G$ is a negative instance for both $g$ and $g'$, we have that a \SEGNNEXPL is a subgraph consisting of any single node from $G$, and $\mathrm{TE}(g(G)) = \mathrm{TE}(g'(G)) = V$.
    %
    % \textcolor{red}{Note that if the GNN is allowed to have an empty graph as an explanation, then $\mathrm{TE}(g(G)) = \mathrm{TE}(g'(G)) = \oldemptyset$. In either case, we have that for $g$ and $g'$, equation \eqref{eq: same_TE} holds, whereas $g \neq g'$ (as any graph with at least one isolated node and one edge is a positive instance for $g$, and a negative instance for $g'$).}
    %
\end{proof}

\cref{thm:TE_Inexpressive} highlights additional insights worth discussing.
Consider again the classifier $g' = \forall x \exists y. E(x,y)$, 
% Considering the classifiers $g = \exists x \exists y. E(x,y)$ and $g' = \forall x \exists y. E(x,y)$, \cref{thm:TE_Inexpressive} highlights additional insights worth discussing.
% \cref{thm:TE_Inexpressive} highlights additional insights worth discussing.
%
% Consider the classifier $g' = \forall x \exists y. E(x,y)$ 
and let $G$ be a negative instance for $g'$ composed of three nodes $\{u_1, u_2, u_3\}$, only two of which are connected by the edge $(u_1, u_2) \in E$.
%
In this case, a user may expect to see the isolated node $u_3$ as an explanation.
% In this case, the expected explanation is the isolated node $u_3$.
However, any subgraph consisting of a single isolated node from the set $\{u_1, u_2, u_3\}$ is a valid TE.
%
This highlights that as TEs focus only on the subgraphs allowing the model to reproduce the same prediction, they may lead to counter-intuitive explanations.


% Furthermore, for $g$ and $g'$ discussed above, \cref{thm:TE_Inexpressive} implies that
Furthermore, \cref{thm:TE_Inexpressive} implies that
\[
    \textstyle
    \bigcup_{G\in\Omega^{(y)}}\mathrm{TE}(g(G)) = \bigcup_{G\in\Omega^{(y)}}\mathrm{TE}(g'(G))
\]
where $\Omega^{(y)}$ is the set of all instances $G$ such that $g(G) = g'(G) = y$, $g = \exists x \exists y. E(x,y)$, and $g'$ is as above. 
%
Hence, there exist classifiers where model-level explanations built by aggregating over multiple local explanations \citep{setzu2021glocalx, azzolin2022global} may also not be informative w.r.t. \cref{rem:inform}. In the next section, we investigate a widely accepted formal notion of explanations, namely Prime Implicant explanations (PIs), for graph classifiers. We analyze the informativeness of TEs compared to PIs and characterize when they match.


%
% \begin{proof}[Sketch]
%     The corollary holds for the $g$ and $g'$ as constructed in proof of Theorem \ref{thm:TE_Inexpressive}.
% \end{proof}
% \begin{proof}
%     Let $E(G)$ ($V(G)$) be the set of edges (nodes) of G.
%     %
%     We proceed with $g$ and $g'$ as introduced in proof of \cref{thm:TE_Inexpressive}.
%     %
%     For the set of positive instances for both the classifiers $\bigcup_{G\in\Omega^{(+)}}\mathrm{TE}(g(G)) = \bigcup_{G\in\Omega^{(+)}}\mathrm{TE}(g'(G)) = \bigcup_{G\in\Omega^{(+)}}E(G)$.  
%     %
%     For the set of negative instances for both the classifiers $\bigcup_{G\in\Omega^{(-)}}\mathrm{TE}(g(G)) = \bigcup_{G\in\Omega^{(-)}}\mathrm{TE}(g'(G)) = \bigcup_{G\in\Omega^{(-)}}V(G)$. 
% \end{proof}


% In order to better understand TEs, we next contrast them with widely accepted formal notions of explanations, comparing their relative informativeness and analyzing cases in which they match.

% In the next Section, we investigate a widely accepted formal notion of explanations, namely Prime Implicant (PI) explanations, for graph classifiers. We analyze the informativeness of TEs compared to PI and characterize when they match. 

% discussing its implications for how explanations can be interpreted \SA{Refine this last sentence based on the focus of the next section}.

% \begin{example}
%     Let us have a classifier $g = \exists x \exists y. E(x,y)$ and a classifier $g' = \forall x \exists y. E(x,y)$, where $E(x,y)$ represents that node $x$ and $y$ are connected by an edge.
%     Hence, a graph is a positive instance of $g$ if it contains an edge, and is a positive instance of $g'$ if it contains no isolated nodes. 
% % Now, let $\Omega_{g}$ and $\Omega_{g'}$ be the (potentially infinite) set of positive instances for the two classifiers. 
% Now, note that for any positive instance $G$ of either $g$ or $g'$, any single edge of $G$ is a trivial explanation. 
% Furthermore, the trivial explanations for a negative instance of both the classifiers is any single node of the negative instance.
% Hence, trivial explanations provides no way of identifying the difference between the classifiers $g$ and $g'$. 
% \end{example}

%
% \begin{cor}
%     Let $R^*$ an optimal \SEGNNEXPL for a decision (G, $\hat{y}$), and let $R$ an explanation extracted by a \SEGNN outputting explanations such that $|R|=k|G|$ for a fixed $k$.
%     Then, $R$ is not a \SEGNNEXPL unless $k|G|$ matches the unknown $|R^*|$
%     \begin{proof}
%         Trivial. Follows from the definition of \SEGNNEXPL.
%     \end{proof}
% \end{cor}
%









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \section{\SEGNNEXPLs can match Prime Implicant explanations}
% \section{\SEGNNEXPLs vs Prime-Implicant Explanations}

\section{Trivial and Prime Implicant Explanations}

% \SM{We are using formulas very minimally now, only in proofs to argue that what we propose is actually expressible by GNNs, do you think we should still add a background on formulas?}

In this section, we provide a formal comparative analysis between Trivial and PI explanations for graph classifiers. 
%
While PIs are extensively studied for tabular and image-like data \citep{marquessilva20naivebayes, marques2023logic}, little investigation has been carried out for graphs.
%
Our analysis shows that TEs match PIs for a large class of tasks -- those based on the recognition of \textit{motifs} -- but they do not align in general.
%
We also show that PIs are more informative than TEs w.r.t the desideratum in \cref{rem:inform}.

Let us start by defining PIs for graph classifiers.


\begin{definition}[PI explanation]
\label{def:piexpl}
   Let $g$ be a classifier and $G$ be an instance with predicted label $g(G)$, then $R$ is a Prime Implicant explanation for $g(G)$ if:
   %
    \begin{enumerate}
            \item \label{PI1} $R \subseteq G$.
            
            \item \label{PI2} For all $R'$, such that $R \subseteq R' \subseteq G$, we have that ${g(G) = g(R')}$.
            
            \item \label{PI3} No other $R' \subset R$ satisfies both \eqref{PI1} and \eqref{PI2}.
    \end{enumerate}
    %
    Like TEs, PIs are not unique, and we use $\mathrm{PI}(g(G))$ to denote the set of all PIs for $g(G)$.
\end{definition}


\begin{figure}
    \centering
    \subfigure{\includegraphics[width=0.8\textwidth]{fig/trivial_explanations4.pdf}}
    
    \caption{Examples of a Trivial, PI, and faithful explanation (w.r.t \cref{prop:suf_equal_1} and \cref{prop:nec_above_0}) for the predictions of $\exists x \exists y.E(x,y)$ and $\forall x \exists y.E(x,y)$ on a triangle.
    }
    \label{fig:examples_TE_PI_FAITH}
\end{figure}


PIs feature several nice properties, in that they are guaranteed to be the minimal explanations that are provably sufficient for the prediction \citep{shih2018symbolic, beckers2022causal, darwiche2023complete}.
%
To illustrate the difference between TEs and PIs, we provide examples of both in \cref{fig:examples_TE_PI_FAITH} for two different classifiers on the same instance.
%
Note that for the classifier $\exists x \exists y. E(x,y)$, TEs match PIs.
%
This observation indeed generalizes to all existentially quantified FOL formulas, as shown next.


\subsection{TEs Match PI for Existential Classifiers}

\begin{restatable}{theorem}{segnnexplarepiexpl}
\label{prop:segnnexpl-are-piexpl}
    Given a classifier $g$ expressible as a purely existentially quantified first-order logic formula and a positive instance $G$ of any size, then a \SEGNNEXPL for $g(G)$ is also a Prime Implicant explanation for $g(G)$.
\end{restatable}

\begin{proof}[Proof sketch.] 
    A purely existentially quantified FOL formula $g$ is of the form $\exists x_1,\dots, \exists x_k. \Phi(x_1,\dots,x_k)$, where $\Phi$ is quantifier-free. For a positive instance $G$, a \SEGNNEXPL is the smallest subgraph $R$ induced by nodes in a tuple $\bar{a} = (a_1,\dots,a_k)$, such that $\Phi(\bar{a})$ holds. Now, any supergraph of $R$ necessarily contains $\bar{a}$, and hence witnesses $\exists x_1,\dots, \exists x_k. \Phi(x_1,\dots,x_k)$, while any smaller subgraph violates $\exists x_1,\dots, \exists x_k. \Phi(x_1,\dots,x_k)$, as $\bar{a}$ is minimal by construction. Hence, $R$ is a PI.
\end{proof}


Note that tasks based on the recognition of a topological motif (like the existence of a star) can indeed be cast as existentially quantified formulas ($\exists xyzw. E(x,y)\land E(x,z)\land E(x,w)\land x \ne y \ne z \ne w$), qualifying TEs as ideal targets for those tasks. %, as they inherit the properties of PIs.
%
% Such motif-based tasks are indeed widely present in datasets evaluating \SEGNNs (e.g., \BAMotif \citep{luo2020parameterized}).
%
% Albeit GNNs can express classifiers beyond purely existentially quantified tasks \citep{barcelo2020logical}, they are of high practical relevance. In fact, tasks based on the recognition of a topological motif (like the existence of a star) can indeed be cast as existentially quantified formulas ($\exists xyzw. E(x,y)\land E(x,z)\land E(x,w)\land x \ne y \ne z \ne w$).
% Such motif-based tasks are indeed widely present in datasets evaluating \SEGNNs (e.g., \BAMotif \citep{luo2020parameterized}).
% This qualifies TEs as ideal targets for motif-based tasks, as they inherit the properties of PIs.
% \cref{prop:segnnexpl-are-piexpl} has however limited practical benefits, as GNNs can express classifiers beyond purely existentially quantified, e.g., $\forall x.\exists y. E(x,y)$ \citep{barcelo2020logical}, and it is impossible to know beforehand whether the model has indeed learned an existential formula \citep{fontanesi2024xai}.
Motif-based tasks are indeed useful in a large class of practically relevant scenarios \citep{Sushko2012toxalerts, jin2020multi, chen2022learning,wong2024discovery}, and have been a central assumption in many works on GNN explainability \citep{ying2019gnnexplainer,miao2022interpretable, wu2022discovering}.
%
\cref{prop:segnnexpl-are-piexpl} theoretically %supports the use of \SEGNNs for such tasks and shows that \SEGNNs optimize for minimally sufficient explanations (PIs) for these cases.
supports using \SEGNNs for these tasks, and shows they optimize for minimally sufficient explanations (PIs).
%
However, global properties such as long-range dependencies \citep{gravina2022antisym} or classifiers like $\forall x\exists y E(x,y)$, cannot be expressed by purely existential statements.
%
In such scenarios, PIs can be more informative than TEs, as we show next.
%
% We now move to show that, in general, TEs are less informative than PI.


\subsection{TEs are Not More Informative than PIs}

We begin by showing that TEs are not more informative than PIs w.r.t. \cref{rem:inform}.

% Our goal is to present a formal comparison between PI and \SEGNNEXPLs.
% We first study whether PIs are better than TEs in satisfying the desideratum highlighted in \cref{sec:trivial-explanations}.
% %
% To do so, we first prove that once considering the set of PIs over all samples with the same label, TEs are subsumed by PIs.
% %
% Then, we provide examples of tasks where PIs differ but TEs do not, meaning that PIs can better identify the behavior of the model.


% We now show that even if a user sees all the Trivial Explanations for all the samples with a given label, then it will not have more information about the underlying classifier then had the user seen only Prime-Implicant explanation. 

\begin{restatable}{proposition}{piteequality}
\label{prop:PI_TE_equality}
    Let $g$ be a classifier and $y$ a given label. Let $\Omega^{(y)}_{g}$ be the set of all the finite graphs (potentially with a given bounded size) with predicted label $y$. Then,
    % If the classifiers $g$ and $g'$ share all the prime implicant explanations, for a given label $y$, then they also share all the trivial explanations for the label $y$.  
    %
    \begin{equation}
        \textstyle
        \bigcup_{G\in\Omega^{(y)}_{g}}\mathrm{TE}(g(G))
        \ \subseteq \ 
        \bigcup_{G\in\Omega^{(y)}_{g}}\mathrm{PI}(g(G))
    \end{equation}
\end{restatable}

% \textcolor{red}{The proof follows from showing that any \SEGNNEXPL, say $R$, for $g(G)=y$ is also a PI explanation for $g(R)$, and since  $R \in \Omega^{(y)}_{g}$, it is contained in the unions of PI.}

% The result above shows that TEs can not provide more information than PI for a given predicted label, as when considering the union of all finite graphs the latter subsumes the former.
This result shows that when considering the union of all finite graphs, PIs subsume TEs.
%
Hence, if two classifiers share all PIs then they necessarily share all TEs as well, meaning that TEs do not provide more information about the underlying classifier than PIs.
% Hence, if two classifiers share all PIs then they necessarily share all TEs as well.
%
% This implies that when considering all positively (resp. negatively) labeled instances from two distinct classifiers, TEs are not more informative than PIs in terms of distinguishing those classifiers, as the union of the former is subsumed by the latter.
%
Conversely, we show that there exist cases where PIs provide strictly more information about the underlying classifier than TEs. % , implying they can be more informative.


\begin{restatable}{theorem}{pimoreexpressive}
\label{prop:PI-more-expressive}
    There exist two distinct classifiers $g$ and $g'$ and a label $y$ such that:
    %
    \begin{enumerate}
      
      \item \label{prop_PI_more_1} For all graphs $G$
      %on which
      s.t. $g(G) = g'(G) = y$, we have 
      \[
        \mathrm{TE}\bigl(g(G)\bigr) \;=\; \mathrm{TE}\bigl(g'(G)\bigr).
      \]
      % (Hence \emph{Trivial Explanations} cannot distinguish $g$ from $g'$.)
      
      \item There exists \emph{at least one} graph $G^\ast$ with $g(G^\ast) = g'(G^\ast) = y$ such that 
      \[
        \mathrm{PI}\bigl(g(G^\ast)\bigr) 
        \;\neq\; 
        \mathrm{PI}\bigl(g'(G^\ast)\bigr).
      \]
      % (Hence \emph{Prime-Implicant Explanations} \emph{can} distinguish $g$ from $g'$.)
    \end{enumerate}
    % In this sense, prime-implicant explanations are strictly more expressive than trivial explanations.
\end{restatable}
% Indeed the classifiers $\exists x \exists y. E(x,y)$ and  $\forall x \exists y. E(x,y)$ shown in Figure \ref{fig:examples_TE_PI_FAITH} are two classifiers where TEs for any positively labeled (resp. negatively labeled) is an edge (resp. an isolated node) while they have different PIs. 
%
We provide an example in \cref{fig:examples_TE_PI_FAITH} of two classifiers where TEs are always equal for positive instances, but PIs are not. 

Summarizing, TEs match PIs for motif-based tasks, and as such they inherit desirable properties of PIs. In the other cases, TEs can generally be less informative.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{\SEGNNEXPLs can be Unfaithful}

A widespread approach to estimate the trustworthiness of an explanation %of the model's behavior
is faithfulness \citep{yuan2022explainability, amara2022graphframex, agarwal2023evaluating, longa2024explaining}, which consists in checking whether the explanation contains all and only the elements responsible for the prediction. %which aim to assess how closely the explanation matches the model's true behavior by checking whether the explanation contains all and only the elements responsible for the prediction. 
%
Clearly, unfaithful explanations fail to convey actionable information about what the model is doing to build its predictions \citep{agarwal2024faithfulness}.
%
This section reviews a general notion of faithfulness \citep{azzolin2024perkspitfalls} in \cref{def:faith}, and shows that faithfulness and TEs can overlap -- in some restricted cases -- but are generally fundamentally misaligned.


% \paragraph{Notation}
% %
% Let $\PR$ a distribution over $\{ G' = R \cup C' : C = G \setminus R \land C' \in \calO(C) \}$, and $\PC$ a distribution over $\{ G' = R' \cup C : R' \in \calO(R) \}$, where $\calO$ is an operator applying perturbations from a fixed set of allowed perturbations.
% %
% Also, let $\Delta_d(G, G') = d( p_\theta( \cdot \mid G) \ \| \ p_\theta( \cdot \mid G' ) )$
% measure the impact of replacing $G$ with $G'$ on the label distribution, according to a divergence of choice $d$.
% %
% For convenience, we will restrict our theoretical analysis to $d$ being $\Ind{\argmax_{y} p_\theta(y \mid G) \ne \argmax_{y} p_\theta(y \mid G')}$, which accounts for changes in the predicted class, and which will be henceforth referred to as $\Delta_{\Ind{}}$.
%
% In the following, we will consider a decision function $\psi$ that maps input instances into $\{ 0, 1 \}$. $\psi$ can either represent the ground truth function solving the target task or the function encoded by a neural model.
% %
% $\psi$ can be equivalently expressed in the form of a boolean classifier, for which we know a precise link to the expressive power of GNN \citep{barcelo2020logical}.\\

Intuitively, faithfulness metrics assess how much the prediction changes when perturbing either the complement -- referred to as \textit{sufficiency} of the explanation -- or the explanation itself -- referred to as \textit{necessity} of the explanation. % -- while leaving the rest unchanged.
%
Perturbations are sampled from a distribution of allowed modifications, which typically include edge and node removals.
%
Then, an explanation is said to be faithful when it is both sufficient and necessary.


\begin{definition}[\textit{Faithfulness}]
    \label{def:faith}
    Let $R \subseteq G$ be an explanation for $g(G)$ and $C = G \setminus R$ its complement.
    Let $\PR$ be a distribution over perturbations to $C$, and $\PC$ be a distribution over perturbations to $R$.
    %
    Also, let $\Delta(G, G') = \Ind{g(G) \ne g(G')}$ indicate a change in the predicted label.
    %
    Then, \SUF and $\NEC$ compute respectively the \textbf{\textit{degree of sufficiency}} and \textbf{\textit{degree of necessity}} of an explanation $R$ for a decision $g(G)$ as:
    %
    \begin{align}
       & \SUF(R) = \exp( -
            \mathbb{E}_{G' \sim \PR} [
                \Delta(G, G')
            ]),
        \label{eq:suf-subset}\\    
        & \NEC(R) = 1 - \exp( -
            \mathbb{E}_{G' \sim \PC} [
                \Delta(G, G')
            ]).
        \label{eq:nec-subset}
    \end{align}
    %
    The \textbf{\textit{degree of faithfulness}} $\FAITH(R)$ is then the harmonic mean of \SUF and \NEC.
\end{definition}

The (negated) exponential normalization ensures that \SUF (resp. \NEC) increases for more sufficient (resp. necessary) explanations.
%
Note that both \SUF and \NEC need to be non-zero for a \FAITH score above zero.
%
% \ST{repetition:} At a high level, \NEC replaces the original R with one of its subgraphs and evaluates whether the prediction changes.
%
According to this definition, the degree of necessity will be high when many perturbations of $R$ lead to a prediction change and is thus $1$ when all of them do so.
%
\SUF is analogous.


As the next theorem shows, TEs can surprisingly fail in being faithful even for trivial tasks.


\begin{restatable}{theorem}{segnnexplcanbeunfaith}
\label{prop:segnnexpl-can-be-unfaith}
    There exist tasks where \SEGNNEXPLs achieve a zero degree of faithfulness.
    %
    \begin{proof}[Proof]
        Let $\MONO = \exists x \exists y.E(x,y)$ and $G$ a positive instance with more than one edge. Then, a \SEGNNEXPL $R$ consisting of a single edge (see \cref{fig:examples_TE_PI_FAITH}) achieves a $\NEC(R)$ value of zero, 
        % as no perturbation to $R$ can lead to a prediction change. Hence $\FAITH(R)$ is also zero.
        as any perturbations to $R$ will leave $G \setminus R$ unchanged. Hence, $g$ remains satisfied, meaning that $\NEC(R)=0$ and thus $\FAITH(R)=0$.        
        %As removing $R$ will leave the other edges unchanged. And hence the formula $\exists x \exists y.E(x,y)$ still holds.  
    \end{proof}
\end{restatable}


% The implications of this result are far-reaching, in that, \SEGNNs may never achieve faithfulness unless specific training regimes are adopted \citep{deng2024sunny}.
%
% (OLD BUT KEEP) Given a black-box model and an unfaithful explanation, it is generally impossible to understand whether unfaithfulness comes from a flawed explainer or it is due to the underlying task the model has learned.
%
Note that $\exists x\exists y. E(x,y)$ achieves non-zero $\NEC(R)$ iff $R$ contains every edge, whereas $\SUF(R)=1$ is guaranteed by the presence of a single edge. 
These observations indeed generalize, as shown next. 
% In the following, we provide two conditions characterizing when explanations achieve non-trivial faithfulness scores. 

\begin{restatable}{proposition}{sufequalone}
\label{prop:suf_equal_1}
    An explanation $R \subseteq G$ for $g(G)$ has a maximal $\SUF(R)$ score if and only if there exists a PI explanation  $M \subseteq R$ for $g(G)$.
\end{restatable}

\begin{restatable}{proposition}{necabovezero}
\label{prop:nec_above_0}
    An explanation $R \subseteq G$ for $g(G)$ has a non-zero $\NEC(R)$ score if and only if it intersects every PI explanation $M \subseteq R$ for $g(G)$. 
\end{restatable}


Note that \cref{prop:suf_equal_1} (along with \cref{prop:segnnexpl-are-piexpl}) show that TEs are indeed maximally sufficient for existentially quantified FOL formulas. However, this might not be enough to ensure a non-zero \FAITH score, as shown in \cref{prop:nec_above_0}.
% Furthermore, \cref{prop:nec_above_0} shows that explanations with non-zero faithfulness scores may be prohibitively large. 
% This will indeed happen for $\exists x\exists y. E(x,y)$ (see \cref{fig 1}) , where for any positive instance only the whole graph will allow non-zero faithfulness score. 
% It is now immediate to link those conditions to \cref{prop:segnnexpl-are-piexpl}, implying that \SEGNNEXPL can satisfy both only in restricted cases.
%
% \begin{restatable}{proposition}{segnnexplarepiexpl}
% \label{prop:segnnexpl-are-piexpl}
%     Given a classifier $g$ expressible as a purely existentially quantified first-order logic formula and a positive instance $G$, then a \SEGNNEXPL for $g(G)$ has a faithfulness score $1$ iff it is the only witness to $g$.
% \end{restatable}
%
In general, TEs highlight a minimal class-discriminative subgraph, whereas faithful explanations highlight all possible causes for the prediction, and the two notions are fundamentally misaligned.


%Also, \label{prop:suf_equal_1} and \label{prop:suf_equal_1} hint at a hierarchical relationship between explanation sizes, as shown next:
%
% sup TE <= sup PI <= sup FAITH^{C1} <= sup FAITH^{C1 \land C2}
%
% sup TE <= min PI <= min FAITH (C1) <= min FAITH^{C1 \land C2}
%
% Partial order of explanation's cardinality
% TE <= PI <= FAITH^{C1} <= FAITH^{C1 \land C2}
% Proof:
% Any TE cannot be larger than the smallest PI, otherwise it is not a TE
% The smallest explanation satisfying C1 is trivially equal or larger than the smallest PI, and the largest explanation satisfying C1 is trivially the entire graph which is larger than the largest PI
% C1 \land C2 is trivially equal or larger than C1


% Faithful explanations are very informative of the model's predictive behavior and depict every possible cause for model predictions.
% %
% However, in doing so, may produce prohibitively large explanations, which in the worst case may include the entire graph.
% %
% Large explanations are, however, likely to be misunderstood by humans \citep{miller1956magical}.
% %
% For example, consider an explanation highlighting several input nodes contributing equally to the prediction. A human interpreter may still be confounded by a visually appealing motif surrounding those nodes, despite the prediction relying entirely on node features regardless of the underlying topology \citep{faber2021comparing, fontanesi2024xai}.


% Our analysis formalizes widely known phenomena that black-box models are hardly faithfully explained unless the explanation gets as complex as the model itself \citep{rudin2019stop}, hinting at a tension between the faithfulness of an explanation and its compactness.
% %
% We aim to go beyond this stalemate, by introducing a family of architectures that can adapt its explanations to the task, discussed next.




%%
% OLD VERSION OF FAITHFUL-TE-PI PROOF
% DO NOT DELETE
%%
% \begin{proposition}
%    A subgraph $R \subseteq G$\hspace{0.2em} is a maximally faithful explanation for $g(G)$, i.e., no $R' \subset R$ has a better faithfulness score, if: \AP{Why restricting to R' smaller than R?} \SA{What do you mean by 'restricting'?}\SM{Res}
%     \begin{enumerate}
%         % \item $R\subseteq G$ such that $g(R) = g(G)$
%         \item There exists an $M\subseteq R$ such that $M$ is a PI-explanation.
        
%         \SA{Minimal for optimizing \NEC, PI for optimizing \SUF}

        
%         \item For all $S' \subseteq G$, where $S' \neq M$ and $g(S') = g(G)$, then $S' \cap R \neq \oldemptyset$. 
%         \item No other $R' \subset R$ satisfies both $(1)$ and $(2)$. \AP{Not sure I get this}
%    \end{enumerate}   

%    \begin{proof}
%     Let us assume $R' \subset R$, but $R'$ has a larger faithfulness score. 
%     Using (3), we have that $R'$ either violates (1) or it violates (2).
%     If $R'$ violates (1), then there exists an extension of $R'$ such that the prediction changes, hence reducing the sufficiency. 
%     If $R'$ violates (2), then there exists an $S'$ such that $g(S') = g(G)$, where $S' \cap R = \oldemptyset$. Hence, $S'$ is in the complement $C$, which can always 
    
%     % if (2) is satisfied, then for sure removing R will change prediction
%     % if (2) is not satisfied, then there exists an S' in the complement such that g(S') = g(G). Then, removing R does not guarantee to change prediction
%     % NEC ~ num_subgraphs_chaning_prediction / num_subgraphs
%     % When putting S' back into C ->
%     %   NEC ~ num_subgraphs_chaning_prediction - 1 / num_subgraphs - 
    
%     % Intersecting every PI is necessary to have NEC > 0
%     % Including at least one PI is sufficient to have SUF=1
    
%     Then $R'$ either has a larger necessity score or a better sufficiency score. 
%     Now, $R'$ can not have a larger sufficiency score as $R$ is maximally sufficient.
%     Let us assume to the contrary that $R'$ has a larger necessity score.
%     However, note that $R'$ must satisfy (2) as otherwise it will have a necessity score of zero.
%     Hence, the only way for $R'$ to have a better necessity score is to have a edge/node removal that does not lead to prediction change, 
%     but this wi
    
%     Furthermore, $R'$ must satisfy (1) as otherwise there exists an expansion of $R'$, say $R''$ such that $g(R'') \neq g(G)$, which leads to a worse sufficiency score. 
    
%     But we $(3)$ requores that $R$ is minimal, which contradicts our assumption that $R'\subset R$
    
%     % Hence, 
%     % We assume that any explanation $R'$ such that $g(G) \neq g(R')$ has a faithfulness score of zero. 
%     % Now let us assume that $R'$ has the maximal faithfulness score 
    
%     % We will prove by contradiction that each of the three conditions  in the proposition are necessary for achieving an optimal faithfulness score. Note that maximal degree of faithfulness is obtained when all 
    
%     % We show that any subgraph $F\subseteq G$ that does not satisfy $(1), (2)$ or $(3)$ has a faithfulness score lesser than the maximally faithful explanation $R$.
%     % \begin{itemize}
%     %     \item $(1)$ is trivially true
%     %     \item if $(2)$ is not true then prediction never changes
%     %     \item if $(3)$ were not true then necessity would be worse than the minimal one.
%     % \end{itemize}
% \end{proof}
% \end{proposition}


%%
% EXAMPLE OF WHY FAITHFUL EXPLANATIONS CAN BE VERY INFORMATIVE
% DO NOT DELETE
%%
% The practical implications of \cref{prop:segnnexpl-can-be-unfaith} and \cref{prop:faith-include-all-segnnexpl} can be best seen in the following example, where faithful explanations are the only ones allowing to reliably spot when the classifier is relying on biased or protected features \citep{shih2018symbolic}:
% %
% \begin{example}
% \label{ex:faith_expl}
%     Given a \SEGNN whose classifiers encode the formula $\Phi \defeq \exists x.Red(x) \lor \exists x.Green(x)$, where green nodes are the protected features and can appear only when red nodes also do.
%     %
%     Then, for positive predictions, a \SEGNNEXPL will highlight a single node, either red or green.
%     %
%     Given that red nodes are present in every positive sample, whereas green nodes may not, the explanation extractor is likely to overfit and prefer to predict always explanations with red nodes.
%     %
%     Those explanations, however, do not allow a proper inspection of the model behavior, as green nodes are likely never considered relevant by \SEGNNEXPLs.
%     %
%     Also, they are not faithful, in that highlighting a single red node does not allow to change prediction when such node is removed\footnote{Unless the input graph contains a single red node}, meaning \NEC equals zero (\cref{prop:segnnexpl-can-be-unfaith}).
%     %
%     Nonetheless, a faithful explanation must highlight every possible cause of positive prediction, therefore including all red and all green nodes (\cref{prop:faith-include-all-segnnexpl}).
% \end{example}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Beyond (purely) Topological \SEGNNs}
\label{sec:method}

Our analysis highlights that explanations extracted by \SEGNNs can be, in general, both ambiguous (\cref{thm:TE_Inexpressive}) and unfaithful (\cref{prop:segnnexpl-can-be-unfaith}), potentially limiting their usefulness for debugging \citep{teso2023leveraging, bhattacharjee2024auditing, fontanesi2024xai}, scientific discovery \citep{wong2024discovery}, and Out-of-Distribution (OOD) generalization \citep{chen2022learning, gui2023joint}.
%
Nonetheless, we also showed that for a specific class of tasks, precisely those based on the presence of motifs, TEs are actually an optimal target, in that they match PIs and are the minimal explanations that are also provably sufficient (\cref{prop:segnnexpl-are-piexpl} and \cref{prop:suf_equal_1}). 

As PI and faithful explanations can generally be more informative than TEs (\cref{prop:PI_TE_equality} and \cref{prop:PI-more-expressive}), one might consider devising \SEGNNs that always optimize for them.
%
We argue, however, that aiming for PI or faithful explanations can be suboptimal, as they can be large and complex in the general case.
%
For example, a PI explanation for a positive instance of $\forall x \exists y. E(x,y)$ is an edge cover \citep{edgecover}, which grows with the size of the instance.
%
Similarly, the explanation with an optimal faithfulness score for $\exists x \exists y. E(x,y)$ is the whole graph. %\footnote{As an explanation with any edge makes $\SUF=1$, whereas excluding any edge makes $\NEC=0$.}.
%
Furthermore, finding and checking such explanations is inherently intractable, as both involve checking the model's output on all possible subgraphs \citep{yuan2021explainability}.
%
These observations embody the widely observed phenomenon that explanations can get as complex as the model itself \citep{rudin2019stop}. 


\begin{figure*}
    \centering
    \subfigure{\includegraphics[width=0.9\linewidth]{fig/dcgnn}}
    
    \caption{
        \textbf{Illustration of our \GL architecture} for a positive instance of \TopoFeature, where positive instances contain a cycle and at least two red nodes.
        %
        Numbers indicate edge relevance scores.
        %
        \SEGNNs may fail to highlight precisely the elements relevant to the prediction, as the task involves non-topological patterns.
        %
        \GLSSEGNNs, instead, provide more focused topological explanations by offloading part of the prediction to interpretable rules (cf. \cref{fig:expl_examples_topofeature_SMGNN} and \cref{fig:expl_examples_topofeature_GLSMGNN}).
    }
    \label{fig:architecture}
\end{figure*}


\subsection{\GLSEGNNs}

This motivates us to investigate \textit{\textbf{\GLSEGNNs}} (\GLSSEGNNs), a new family of \SEGNNs aiming to preserve the perks of TEs for motif-based tasks while freeing them from learning non-motif-based patterns, which would be inefficiently represented as a subgraph. 
%
To achieve this, \GLSSEGNNs pair an \SEGNN-based \emph{topological channel} -- named \TopoC -- with an \textit{interpretable channel} -- named \FlatC -- providing succinct non-topological rules.
%
To achieve optimal separation of concerns while striving for simplicity, we jointly train the two channels and let \GLSSEGNNs adaptively learn how to exploit them.



\begin{definition}
    (\GLSSEGNN) Let $g_{1}: G \mapsto [0,1]^{n_1}$ be a \SEGNN, $g_{2}: G \mapsto [0,1]^{n_2}$ an interpretable model of choice, and $\aggr: [0,1]^{n_1 + n_2} \mapsto Y$ an aggregation function.
    %
    A \GLSSEGNN is defined as:
    %
    \[
        \MONO(G) = \aggr \big ( g_{1}(G), g_{2}(G) \big ).
    \]
\end{definition}


While the second channel $g_{2}$ can be any interpretable model of choice, we will experimentally show that even a simple sparse linear model can bring advantages over a standard \SEGNN, leaving the investigation of a more complex architecture to future work.
%
Therefore, in practice, we will experiment with 
\[
    \textstyle
    g_{2}(G) = \sigma \big ( W \sum_{u \in G} x_u \big ),
\]
%
where $x_u \in \bbR^d$ is the node feature vector of $u$, $W \in \bbR^{n_2 \times d}$ the weights of the linear model, and $\sigma$ is an element-wise sigmoid activation function.
%
Also, we will fix $n_1=n_2$ equal to the number of classes of $Y$.
%
In the experiments, we will promote a sparse $W$ via weight decay.

A key component of \GLSSEGNNs is the \aggr function joining the two channels.
%
To preserve the interpretability of the overall model, \aggr should act as a gateway for making the final predictions based on a simple combination of the two channels.
%
To achieve this while ensuring differentiability, we considered an extension of Logic Explained Networks (\LENs) \citep{barbiero2022entropy}, described below.


\textbf{Implementing \aggr}.  \LENs are small Multi-layer Perceptrons (MLP) taking as input a vector of activations in $[0,1]$ \citep{barbiero2022entropy}.
%
The first layer of the MLP is regularized to identify the most relevant inputs while penalizing irrelevant ones via an attention mechanism.
%
To promote the network to achieve discrete activations scores without resorting to high-variance reparametrization tricks \citep{azzolin2022global, giannini2024interpretable},
we propose augmenting LENs with a progressive temperature annealing of the input vector.
%
We validate the effectiveness of this extension in preserving the semantics of each channel, together with a comparison to other baselines, in \cref{appx:ablation_aggr}.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Empirical Analysis}
\label{sec:experiments}

We empirically address the following research questions:
%
\begin{itemize}[leftmargin=2em]
    
    \item[\textbf{Q1}] Can \GLSSEGNNs match or surpass plain \SEGNNs?
    
    \item[\textbf{Q2}] Can \GLSSEGNNs adaptively select the best channel for each task?

    \item[\textbf{Q3}] Can \GLSSEGNNs extract more focused explanations?
    
\end{itemize}
%
Full details about the empirical analysis are in {\cref{appx:impl_details}}.
%



\textbf{Architectures}.  We test two representative \SEGNNs backbones from \cref{tab:taxonomy-loss}, namely \GISST \citep{lin2020graph} and \GSAT \citep{miao2022interpretable}, showing how adding an interpretable side channel can enhance them.
%
% We also propose an extension for \GISST to overcome its limitation of extracting explanation scores \AP{unclear, you mean rather than hard explanations?} \SA{Nope. GIIST extracts the explanations from raw input features, rather than a GNN-based explanation detector} from raw input features. The resulting model is named \SMGNN and uses the same explanation extractor as \GSAT.
% We also propose an extension of \GISST with a graph-aware explanation extractor, which otherwise extracts relevance scores from raw node features. The resulting model is named \SMGNN and uses the same explanation extractor as \GSAT.
We also propose a new \SEGNN named \SMGNN, which extends the graph-agnostic \GISST's explanation extractor with that of popular \SEGNNs \citep{miao2022interpretable, wu2022discovering}.
%
% Since \GISST extracts explanation scores from raw input features, which can be unsuitable for several tasks where input features are non-informative, we also consider an extension that overcomes this limitation.
%
% The resulting model is named \SMGNN and it shares the same topological explanation extraction and regularization as \GISST, but enhanced with a GNN-based explanation extractor just like other \SEGNNs \citep{miao2022interpretable, chen2024howinterpretable}.


\textbf{Datasets}. We consider three synthetic and five real-world graph classification tasks.
%
Synthetic datasets include \Motif \citep{gui2022good}, and two novel datasets. 
%
\BAColor contains random graphs where each node is either red or blue,  and the task is to predict which color is more frequent.
%
Similarly, \TopoFeature contains random graphs where each node is either red or uncolored, and the task is to predict whether the graph contains at least two red nodes \textit{and} a cycle, which is randomly attached to the base graph.
%
Both datasets contain two OOD splits where the number of total nodes is increased or the distribution of the base graph is changed. For \Motif we will use the original OOD splits \citep{gui2022good}.
%
Real-world datasets include \MUTAG \citep{debnath1991structure}, \BBBP \citep{morris2020tudataset}, \MNIST \citep{knyazev2019understanding}, \AIDS \citep{riesen2008iam}, and \SST \citep{yuan2022explainability}.


\begin{table}[t]
\centering
\footnotesize
\caption{
    \textbf{\GLSSEGNNs can reliably select the most appropriate channel based on the task.}
    %
    The table shows the test accuracy, the selected channel, and the test accuracy when removing one of the two channels.
    %
    \GISST is excluded from this analysis as it cannot extract meaningful explanations for \TopoFeature and \Motif.
}
\label{tab:synt_exp}
\scalebox{.7}{
    \begin{tabular}{lcccccccccccc}
        \toprule
         \textbf{Model} & \multicolumn{4}{c}{\textbf{\BAColor}} & \multicolumn{4}{c}{\textbf{\TopoFeature}} & \multicolumn{4}{c}{\textbf{\Motif}}\\

         \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
         & Acc & \makecell{Channel} & \makecell{Acc \\ w/o \TopoC} &  \makecell{Acc \\ w/o \FlatC} 
         & Acc & \makecell{Channel} & \makecell{Acc \\ w/o \TopoC} &  \makecell{Acc \\ w/o \FlatC} 
         & Acc & \makecell{Channel} & \makecell{Acc \\ w/o \TopoC} &  \makecell{Acc \\ w/o \FlatC} \\
        
        \midrule

        \GIN   
            & \nentry{99}{01} & - & - & - 
            & \nentry{96}{02} & - & - & -
            & \nentry{93}{00} & - & - & -\\

        \midrule
        
         \GSAT   
            & \nentry{99}{01} & - & - & - 
            & \nentry{99}{01} & - & - & -
            & \nentry{93}{00} & - & - & -\\

         \GLGSAT 
            & \nentry{100}{00} & \FlatC & \nentry{100}{01} & \nentry{50}{02}  
            & \nentry{100}{00} & $\mathsf{Both}$ & \nentry{50}{00} & \nentry{60}{20}
            & \nentry{93}{01}  & \TopoC & \nentry{34}{01} & \nentry{93}{00} \\

        \midrule
        
         \SMGNN 
            & \nentry{99}{00} & - & - & - 
            & \nentry{95}{01} & - & - & -
            & \nentry{93}{01} & - & - & -\\

         \GLSMGNN
            & \nentry{100}{00} & \FlatC & \nentry{99}{01} & \nentry{50}{02}
            & \nentry{100}{00} & $\mathsf{Both}$ & \nentry{50}{00} & \nentry{50}{00}
            & \nentry{93}{00}  & \TopoC & \nentry{34}{01} & \nentry{93}{00} \\

         \bottomrule         
    \end{tabular}
}
\end{table}


\begin{table}[t]
\centering
\footnotesize
\caption{
    \textbf{\GLSSEGNNs performs on par or better than a plain \SEGNN.}
    %
    The table shows the test accuracy and the selected channel for real-world experiments.
    %
    Channel relevance with ``*'' means that \GLSSEGNNs select the indicated channel in nine seeds over ten:
    %
    For \AIDS, that single seed uses the topological channel, as it can easily fit the task without incurring performance loss;
    %
    For \BBBP, instead, the \GLSSEGNN tries to use both channels but incurs performance loss.
    %
    ``Mix'' instead indicates that different seeds rely on different channels without incurring performance loss.
}
\label{tab:real_exp}
\scalebox{0.85}{
    \begin{tabular}{lcccccccccc}
        \toprule
         \textbf{Model} & 
         \multicolumn{2}{c}{\textbf{\AIDS}} & 
         % \multicolumn{2}{c}{\textbf{\AIDSC}} & 
         \multicolumn{2}{c}{\textbf{\MUTAG}} & 
         \multicolumn{2}{c}{\textbf{\BBBP}} & 
         \multicolumn{2}{c}{\textbf{\SST}} & 
         \multicolumn{2}{c}{\textbf{\MNIST}}\\

         % \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
         & F1 & \makecell{Channel}
         % & F1 & \makecell{Channel}
         & Acc & \makecell{Channel}
         & AUC & \makecell{Channel}
         & Acc & \makecell{Channel}
         & Acc & \makecell{Channel}\\
        
        \midrule

        \GIN
            & \nentry{96}{02} & -
            % & \nentry{85}{05} & -
            & \nentry{81}{02} & -
            & \nentry{68}{03} & -
            & \nentry{88}{01} & -
            & \nentry{92}{01} & -\\
        
        \midrule

         \GISST
            & \nentry{97}{02} & -
            % & \nentry{68}{06} & -
            & \nentry{78}{02} & -
            & \nentry{66}{03} & -
            & \nentry{85}{01} & -
            & \nentry{91}{01} & -\\

         \GLGISST
            & \nentry{99}{02} & $\mathsf{Mix}$
            % & \nentry{99}{02} & \FlatC
            & \nentry{79}{02} & \TopoC
            & \nentry{65}{02} & \TopoC
            & \nentry{87}{02} & $\mathsf{Mix}$
            & \nentry{91}{01} & \TopoC\\ 

        \midrule
        
         \GSAT
            & \nentry{97}{02} & -
            % & \nentry{70}{06} & -
            & \nentry{79}{02} & -
            & \nentry{66}{02} & -
            & \nentry{86}{02} & -
            & \nentry{94}{01} & -\\

         \GLGSAT
            & \nentry{99}{03} & \FlatC
            % & \nentry{99}{02} & \FlatC
            & \nentry{79}{02} & \TopoC
            & \nentry{65}{03} & \TopoC*
            & \nentry{87}{02} & \FlatC
            & \nentry{94}{02} & \TopoC\\

        \midrule
        
         \SMGNN
            & \nentry{97}{02} & -
            % & \nentry{68}{06} & -
            & \nentry{79}{02} & -
            & \nentry{66}{03} & -
            & \nentry{86}{01} & -
            & \nentry{92}{01} & -\\

         \GLSMGNN
            & \nentry{99}{01} & \FlatC*
            % & \nentry{99}{02} & \FlatC*
            & \nentry{80}{02} & \TopoC
            & \nentry{65}{05} & \TopoC*
            & \nentry{87}{01} & \FlatC
            & \nentry{93}{01} & \TopoC\\

        \bottomrule         
    \end{tabular}
}
\end{table}


\begin{table}[t]
\centering
\footnotesize
\caption{
    \textbf{\GLSSEGNNs can generalize better to OOD than plain \SEGNN.}
    %
    \GISST is excluded from this analysis as it cannot extract meaningful explanations for \TopoFeature and \Motif.
}
\label{tab:ood_exp}
\scalebox{0.9}{
    \begin{tabular}{lcccccc}
        \toprule
         \textbf{Model} & 
         \multicolumn{2}{c}{\textbf{\BAColor}} & 
         \multicolumn{2}{c}{\textbf{\TopoFeature}} & 
         \multicolumn{2}{c}{\textbf{Motif}}\\

         & $\text{OOD}_1$ & $\text{OOD}_2$
         & $\text{OOD}_1$ & $\text{OOD}_2$
         & $\text{OOD}_1$ & $\text{OOD}_2$\\
        
        \midrule

        \GIN
            & \nentry{94}{01} & \nentry{87}{05}
            & \nentry{61}{06} & \nentry{61}{04}
            & \nentry{63}{13} & \nentry{64}{03}\\

        \midrule
        
         \GSAT
            & \nentry{98}{01} & \nentry{98}{01}
            & \nentry{81}{08} & \nentry{87}{04}
            & \nentry{67}{12} & \nentry{57}{04}\\

         \GLGSAT
            & \nentry{100}{00} & \nentry{100}{00}
            & \nentry{87}{13} & \nentry{86}{12}
            & \nentry{72}{13} & \nentry{49}{05}\\

        \midrule
        
         \SMGNN
            & \nentry{97}{01} & \nentry{85}{05}
            & \nentry{55}{02} & \nentry{75}{03}
            & \nentry{65}{06} & \nentry{62}{06}\\

         \GLSMGNN
            & \nentry{99}{01} & \nentry{100}{00}
            & \nentry{93}{06} & \nentry{98}{01}
            & \nentry{82}{08} & \nentry{43}{04}\\
        \bottomrule         
    \end{tabular}
}
\end{table}



\textbf{A1: \GLSSEGNNs performs on par or better than plain \SEGNNs}.
%
We list in \cref{tab:synt_exp} the results for synthetic datasets and in \cref{tab:real_exp} those for real-world benchmarks.
%
In the former, \GLSSEGNNs always match or surpass the performance of plain \SEGNNs, and similarly, in the latter \GLSSEGNNs perform competitively with \SEGNNs baselines, and in some cases can even surpass them.
%
For example, \GLSSEGNNs consistently surpass the baseline on \AIDS and \SST by relying on the non-relational interpretable model.
%
These results are in line with previous literature highlighting that some tasks can be unsuitable for testing topology-based explainable models \citep{azzolin2024perkspitfalls}, and that graph-based architectures can overfit the topology to their detriment \citep{bechler2023graph}.


We further analyze their generalization abilities by reporting their performance on OOD splits in \cref{tab:ood_exp}.
%
Surprisingly, \GLSSEGNNs significantly outperformed baselines on $\text{OOD}_1$ of \Motif but underperformed on $\text{OOD}_2$, likely due to the intrinsic instability of OOD performance in models trained without OOD regularization \citep{chen2022learning, gui2023joint}.
%
For \BAColor and \TopoFeature instead, \GLSSEGNNs exhibit substantial gains in seven cases out of eight, achieving perfect extrapolation on \BAColor.
%
%\SA{@Sagar mention uniform expressivity argument here if space?}


\textbf{A2: \GLSSEGNNs can dependably select the appropriate channel(s) for the task}.
%
% \cref{tab:synt_exp} presents the results of \GSAT and \SMGNN together with their respective \GLSSEGNN augmentations for the three synthetic datasets.
%
Results in \cref{tab:synt_exp} clearly show that both \GLSSEGNNs correctly identify the appropriate channel(s) for all tasks.
%
In particular, they focus on the interpretable model for \BAColor as the label can be predicted via a simple comparison of node features. 
%
For \Motif, they rely on the underlying \SEGNN only, as the label depends on a topological motif. 
%
For \TopoFeature, instead, they combine both channels as the task can be decomposed into a motif-based sub-task and a comparison of node features, as expected from the dataset design. 
%
While the channel importance scores predicted by the \aggr are continuous, we report a discretized version indicating only the channel(s) achieving a non-negligible ($\ge 0.1$) score to avoid clutter. Raw scores are reported in \cref{tab:raw_channel_scores}.

To measure the dependability of the channel selection mechanism of \GLSSEGNNs, we perform an ablation study by setting to zero both channels independently.
We report the resulting accuracy in the last two columns for every dataset in \cref{tab:synt_exp}.
%
Results show that when the model focuses on just a single channel, removing the other does not affect performance.
%
Conversely, when the model finds it useful to mix information from both channels, removing either of them prevents the model from making the correct prediction, as expected.


\textbf{A3: \GLSSEGNNs can uncover high-quality rules and TEs}.
%
% One of the main benefits of augmenting a \SEGNN with an interpretable model is that the latter provides intelligible predictions, which translates into a weighted sum of input features in the case of our linear-based \GLSEGNN.
%
When the linear model $g_2$ is sparse, it is possible to interpret the weights as inequality rules enabling a full understanding of the predictions, cf. \cref{appx:rule-extraction}.
%
In fact, by focusing on model weights with non-negligible magnitude, we can extract the expected ground truth rule for \BAColor, whilst subgraph-based explanations fail to convey such a simple rule as shown in \cref{appx:expl_examples}.
%
At the same time, for \TopoFeature the interpretable model fires when the number of red nodes is at least two.
%
This result, together with the fact that the \GLSSEGNN is using both channels (see \cref{tab:synt_exp}), %confirms that the two channels cooperate in making the final prediction: 
confirms that the two channels are cooperating as expected: the interpretable model fires when at least two red nodes are present, and the \SEGNN is left to recognize when the motif is present, achieving more focused and compact explanations as depicted in \cref{fig:architecture}.
%
The alignment of those formulas to the ground truth annotation process is also reflected in better extrapolation performance, as shown in \cref{tab:ood_exp}.
%
We further discuss in \cref{appx:aidsc1} an additional experiment on \AIDS where we match the result of \citet{pluska2024logical}, showing that a GNN can achieve a perfect score by learning a simple rule on the number of nodes.
%
Those results highlight that better and more intuitive explanations can be obtained without relying on a subgraph of the input.
%
We further provide a quantitative (\cref{appx:exp_faith}) and qualitative (\cref{appx:expl_examples}) analysis showing how \GLSSEGNN can yield explanations that better reflect what the underlying \SEGNN is using for predictions.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Related Work}
\label{sec:related-work}

\textbf{Explaining GNNs}. \SEGNNs are a physiological response to the inherent limitations of \textit{post-hoc} GNN explainers \citep{longa2024explaining, li2024underfire}.
%
\SEGNNs usually rely on regularization terms and architectural biases -- such as attention \citep{miao2022interpretable, miao2022interpretablerandom, lin2020graph, serra2022learning, wu2022discovering, chen2024howinterpretable}, prototypes \citep{zhang2022protgnn, ragno2022prototype, dai2021towards, dai2022towards}, or other techniques \citep{yu2020graph, yu2022improving, giunchiglia2022towards, ferrini2024self} -- to encourage the explanation to be human interpretable. 
%
Our work aims at understanding the formal properties of their explanations, which have so far been neglected.


\textbf{Beyond subgraph explanations.} 
%
% \citet{azzolin2022global} and \citet{armgaan2024graphtrail} proposed to extract post-hoc explanations in the form of logic formulas. However, those formulas express co-occurrence patterns of simpler subgraphs, making the final explanation still dependent on subgraphs.
%
\citet{pluska2024logical} and \citet{kohler2024utilizing} proposed to distill a trained GNN into an interpretable logic classifier.   Their approach is however limited to \textit{post-hoc} settings, and the extracted explanations are human-understandable only for simple datasets.
%
Nonetheless, this represents a promising future direction for integrating a logic-based rule extractor as a side channel in our \GLSSEGNNs framework.
%
\citet{Muller2023graphchef} and \citet{bechler-speicher2024gnan} introduced two novel interpretable-by-design GNNs that avoid extracting a subgraph explanation altogether, by distilling the GNN into a Decision Tree, or by modeling each feature independently via learnable shape functions, respectively.
%
However, we claim that subgraph-based explanations are desirable for existential motif-based tasks, and thus, we %aim to keep them in conjunction with an interpretable classifier to model both topological and non-topological reasoning.
strike for a middle ground between subgraph and non-subgraph-based explanations.


\textbf{Formal explainability}.  While GNN explanations are commonly evaluated in terms of faithfulness \citep{agarwal2023evaluating, christiansen2023faithful, azzolin2024perkspitfalls}, formal explainability has predominantly been studied for non-relational data, where it primarily focuses on PI explanations \citep{marques2023logic, darwiche2023complete, wang2021probabilistic}.
%
Important properties of PI explanations include sufficiency, minimality, and their connection to counterfactuals \citep{marques2022delivering}.  Simultaneously, PI explanations can be exponentially many (but can be summarized \citep{yu2023formal} for understanding) and are intractable to find and enumerate for general classifiers \citep{marques2023logic}.
%
Our work is the first to systematically investigate formal explainability for GNNs and elucidates the link between PI explanations and \SEGNNs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusion and Limitations}

We have formally characterized the explanations given by \SEGNNs (TEs), and shown their relation to established notions of explanations, namely PI and faithful explanations.  Our analysis revealed that TEs match PIs and are sufficient for motif-based tasks.
%
However, in general TEs can be uninformative, whereas faithful and PI explanations can be large and intractable to find.
%
Motivated by this, we introduced \GLSSEGNNs, a new class of \SEGNNs adaptively providing either TEs, interpretable rules, or their combination.
We empirically validate \GLSSEGNNs, confirming their promise.

% Motivated by this, we introduced a new class of Dual-Channel \SEGNNs which exploit TEs when they are compact and informative, while resorting to an interpretable model otherwise. We also highlighted their promise empirically.

\textbf{Limitations}:
%
% Lack of a precise characterization of some behaviors due to example-based proofs. This is however not an issue as the task being explained is in general not known, making it impossible to draw conclusions apriori. Also, the tasks presented are fairly easy and represent a complexity that we expect truly trustworthy models shall achieve. In addition, they represent classes of problems that can be ubiquitously found in real-world datasets, like checking whether the dosage of a toxic compound is above an alert threshold, or checking whether an innocuous compound is more or less abundant than a toxic one \citep{Sushko2012toxalerts}.
Our theoretical analysis is focused on Sparsity- and IB-based \SEGNNs, which are popular choices due to their effectiveness and ease of training.
%
Alternative formulations optimize explanations for different objectives than minimality \citep{wu2022discovering,deng2024sunny}, and more work is needed to provide a unified formalization of this broader class of subgraph-based explanations.
%
Nonetheless, we expect our theoretical findings to also apply to many \textit{post-hoc} explainers, as they also optimize for the minimal subgraph explaining the prediction \citep{ying2019gnnexplainer, luo2020parameterized}.
%
Furthermore, despite showing that \GLSSEGNNs with a simple linear model has several benefits, we believe that more advanced interpretable channels should be investigated.
%
On this line, our baseline can serve as a yardstick for future developments of \GLSSEGNNs.



% \SA{Discuss the difference between edge and node removals. Under the assumption of extensions within the original graph, then every extension of a given R is equivalently represented as deletions from the entire model. Then, when a node becomes isolated, we can either remove it, or leave it there. Does this have some implications?}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \section*{Impact Statement}

% Our analysis highlights intrinsic limitations of explanations produced by a wide class of explainable-by-design GNNs, and in this sense serves as a warning for stakeholders from blindly trusting explanations produced by these models.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \section*{Acknowledgments}

% Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Health and Digital Executive Agency (HaDEA). Neither the European Union nor the granting authority can be held responsible for them. Grant Agreement no. 101120763 - TANGO.


\bibliography{reference, explanatory-supervision}
\bibliographystyle{ICML/icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\appendix
\onecolumn


\section{Proofs}
\label{sec:proofs}

\subsection{Proof of \cref{thm:segnnexpl-losses}}


\textbf{Preliminaries and Assumptions.}
%
Recall that an \SEGNN $g$ is composed of a GNN-based classifier $\CLF$ and an explanation extractor $\DET$.
%
Given an instance $G$, the explanation extractor $\DET$ returns a subgraph $\DET(G) = R \subseteq G$, and the classifier provides a label $g(G) = \CLF(\DET(G))$.    

Note that analyzing the losses presented in Table \ref{tab:taxonomy-loss} is challenging due to the potentially misleading minima induced by undesirable choices of $\lambda_1$ and $\lambda_2$.
%
For example, choosing the explanation regularization weight $\lambda_1$ and $\lambda_2$ in \cref{tab:taxonomy-loss} to be zero can trivially lead to correct predictions but with uninformative explanations. 
%
Equivalently, setting $\lambda_1$ and $\lambda_2$ too high yields models with very compact yet useless explanations, as the model may not converge to a satisfactory accuracy.
%
Since our goal is to analyze the nature of explanations extracted by $q$, we assume that \CLF expresses the ground truth function. %and thus always output the ground truth label for any input. , i.e., achieving minimal true risk. 
%
Also, we assume the \SEGNN to have perfect predictive accuracy, meaning that $\CLF(\DET(G))$ always outputs the ground truth label for any input.
%
Those two assumptions together allow us to focus only on the nature of the explanations extracted by $q$.



% thus excluding \SEGNNs, that may achieve optimal loss, for some hyper-parameterization, but have subotimal predicitve performance. 
% For example, choosing the explanation regularization weight $\lambda_1$ ($\lambda_2$) in \cref{tab:taxonomy-loss} to be zero trivially can lead to correct predictions but with uninformative explanations. 
%
% Equivalently, setting $\lambda_1$ ($\lambda_2$) too high yields models with very compact yet useless explanations, as the model may not converge to a satisfactory accuracy.


We also consider an \SEGNN with a hard explanation extractor. 
%
For sparsity-based losses (\cref{eq:proof-sparsity-trivialexpl-GISST}), this amounts to assigning a score equal to $1$ for edges in the explanation $R$, and equal to $0$ for edges in the complement $G \setminus R$.
%
For Information Bottleneck-based losses (\cref{eq:proof-sparsity-trivialexpl-GSAT}), instead, the explanation extractor assigns a score of $1$ for edges in the explanation $R$, and equal to $r$ for edges in the complement $G \setminus R$, where $r$ is the hyper-parameter chosen as the uninformative baseline for training the model \citep{miao2022interpretable}.
%
Therefore, an explanation $R$ is identified as the edge-induced subgraph where edges have a score $p_{uv} = 1$.


\segnnexpllosses*

\begin{proof}
    We proceed to prove the Theorem by analyzing two cases separately:
    
    \paragraph{Sparsity-based losses} Let us consider the following training objective of a prototypical sparsity-based \SEGNN, namely \GISST \citep{lin2020graph}. 
    Here we focus just on the sparsification of edge-wise importance scores, and we discuss at the end how this applies to node feature-wise scores:
    %
    \begin{equation}
    \label{eq:proof-sparsity-trivialexpl-GISST}
        \min \ \calL \big (\CLF(\DET(G)), Y \big ) + \lambda_1 \frac{1}{|E|}\sum_{(u,v) \in E} p_{uv} + \lambda_2 \frac{1}{|E|}\sum_{(u,v) \in E} p_{uv}\log(p_{uv}) + (1-p_{uv})\log(1-p_{uv})
    \end{equation}
    
    
    Given that the importance scores $p_{uv}$ can only take values in $\{0, 1\}$, the last term in \cref{eq:proof-sparsity-trivialexpl-GISST} equals to $0$.
    %
    Also, given that every edge outside of $\DET(G)$ has $p_{uv}=0$ and every edge in $\DET(G)$ has $p_{uv}=1$, we have that
    %
    \begin{equation}
      \sum_{(u,v) \in E} p_{uv}= |\DET(G)|
    \end{equation}
    
    Hence, the final minimization reduces to:
    %
    \begin{equation}
    \label{eq:proof-sparsity-trivialexpl-GISST-small}
        \min \ \calL \big (\CLF(\DET(G)), Y \big ) + \lambda_1 \frac{|\DET(G)|}{|E|}
    \end{equation}
    % f(q(G)) = f(R) = f(G) when R is TE
    % f=f^*
    % We first prove that for an \SEGNN with perfect predictive accuracy, the minimal true risk is obtained only if $\DET$ provides trivial explanations.
    % Recall that for a given instance $G$, $R = \DET(G) $ by construction of \SEGNN.
    % Let us assume to the contrary that $R$ is not a trivial explanation. Then either $\CLF(\DET(G)) \neq Y $, where $Y$ is the true label for $G$. However, this will contradict our assumption that the \SEGNN achieves perfect accuracy.
    % Or there exists $R'$, such that $|R'| < |R|$ and $\CLF(\DET(G)) = Y$, however, using equation \eqref{eq:proof-sparsity-trivialexpl-GISST-small}, this will mean that $R'$ leads to a smaller risk value, leading to a contradiction.
    % Hence, $R$ must be a trivial explanation.
    % %
    % %  (Rewriting the previous paragraph below)
    % %
    % We first prove that for an \SEGNN with perfect predictive accuracy, the minimal true risk is obtained only if $\DET$ provides \SEGNNEXPLs.
    % %
    % Let us assume to the contrary that $R$ is not a \SEGNNEXPL.
    % %
    % Then, at least one between condition 2 and condition 3 in \cref{def:segnn-explanations} must be violated. 
    % Let us consider the two possible cases independently:
    % %
    % \textcolor{red}{On one side, we get that $\CLF(\DET(R)) \neq \CLF(\DET(G))$. By the assumption of perfect accuracy, and letting $y$ be the true label for $G$, it follows that $\CLF(\DET(R)) \neq y$.
    % %
    % However, this contradicts our assumption that the \SEGNN achieves perfect accuracy.}
    % %
    % On the other, there exists $R'$ such that $|R'| < |R|$ and $\CLF(\DET(R')) = \CLF(\DET(G)) = y$, where $y$ is again the true label of $G$.
    % %
    % This however implies that \eqref{eq:proof-sparsity-trivialexpl-GISST-small} is not minimized by $R$, as $R'$ will lead to a smaller risk value, leading to a contradiction.
    % %
    % Hence, $R$ must be a \SEGNNEXPL.
    %
    %  (Fixing the proof)
    %

    
    \textbf{Minimal True Risk for \SEGNN $\Rightarrow$ \SEGNNEXPLs}\\
    %
    Given that $\CLF$ is the ground truth classifier, then due to perfect predictive accuracy for $\CLF(\DET(G))$ we have that $\CLF(\DET(G)) = \CLF(G) = y^*$, where $y^*$ is the ground truth label for $G$. 
    %
    This implies that $\calL(\CLF(\DET(G)), Y )$ is minimal.
    %
    Now, for the true risk to be minimal we must additionally have that  $\lambda_1 |\DET(G)| / |E|$ in \cref{eq:proof-sparsity-trivialexpl-GISST-small} is minimized as well.
    %
    Note that $q(G)$ returns the explanation $R$. Hence, minimizing $\lambda_1 |\DET(G)| / |E|$ in \cref{eq:proof-sparsity-trivialexpl-GISST-small} requires that we find the smallest $R \subseteq G$ such that $\calL(\CLF(\DET(G)), Y )$ is also minimal, hence $f(q(G)) = f(R) = y^*$. 
    %
    Also, note that $f(R) = f(q(R))$ by perfect predictive accuracy of $f(q(R))$ and $f$ being the ground truth classifier. 
    %
    Hence, we have that for an instance $G$, $R \subseteq G$ is the smallest subgraph such that  $f(q(G)) = f(q(R))$. 
    %
    Hence, $R$ is a \SEGNNEXPL for $f(q(G))$.

    
    % We first prove that for an \SEGNN with perfect predictive accuracy, the minimal true risk is obtained only if $\DET$ provides \SEGNNEXPLs.
    % For any instance $G$, let us define $g(G) = f(q(G))$.
    % For an \SEGNN to achieve minimal true risk 
    % Note that \SEGNNEXPLs are the smallest subgraphs $R$ of $G$ such that $g(G) = g(R)$. 
    % Also, we require that the explanation is returned by the subgraph extractor $q$.
    % We now show that if $f(q(G))$ returns the true label for $G$, and also attains the smallest possible true risk, as given in \eqref{eq:proof-sparsity-trivialexpl-GISST-small} then $q(G)$ must return a trivial explanation for $g(G)$. 
    % Now, $q(G)$ must return a subgraph $R$, such that $g(G)$
    


    
    % We first prove that for an \SEGNN with perfect predictive accuracy, the minimal true risk is obtained only if $\DET$ provides \SEGNNEXPLs.
    % %
    % Let us assume to the contrary that for a given instance $G$, the explanation $q(G) = R$ is not a \SEGNNEXPL and the \SEGNN achieves minimal true risk.
    % %
    % Then, at least one of condition 2 and condition 3 in \cref{def:segnn-explanations} must be violated, as condition 1 is satisfied by construction of $q$. 
    % Let us consider the two possible cases independently:
    
    % \textbf{Case 1:} If condition 2 is violated then we have that $\CLF(\DET(R)) \neq \CLF(\DET(G))$. 
    % \begin{align*}
    %     &\CLF(\DET(G))  = \CLF(R)  = y && \text{By construction of \DET and by minimal true risk}\\
    %     &\text{Hence, $R$ has label $y$. Also. Note that $q(R)$ returns the smallest subgraph of $R$, with label $y$ (minimization). Hence, $q(R) = R$?}\\
    %     &(\CLF(\DET(R)) \neq \CLF(\DET(G))) \rightarrow (\CLF(\DET(R)) \neq y) && \text{Which contradicts the assumption that the \SEGNN achieves minimum true risk}
    % \end{align*}
    
    % By the assumption of perfect accuracy, and letting $y$ be the true label for $G$, it follows that $\CLF(\DET(R)) \neq y$.
    % %
    % Let us define $R' \defeq \DET(R)$. 
    % %
    % Then, either $R' = R$, or $R' \subset R$.
    % %
    % If $R' = R$, then we have that $\CLF(\DET(R)) = \CLF(R) \neq \CLF(\DET(G)) \defeq \CLF(R)$, which is a contradiction.
    % %
    % \textcolor{red}{If $R' \subset R$, then we have that $\CLF(\DET(R)) = \CLF(R') \neq \CLF(\DET(G)) \defeq \CLF(R)$}

    
    % \textbf{Case 2:} If condition 3 is violated, there exists $R'$ such that $|R'| < |R|$ and $\CLF(\DET(R')) = \CLF(\DET(G)) = y$, where $y$ is again the true label of $G$.
    % %
    % This however implies that \eqref{eq:proof-sparsity-trivialexpl-GISST-small} is not minimized by $R$, as $R'$ will lead to a smaller risk value, leading to a contradiction.
    % %
    % Hence, $R$ must be a \SEGNNEXPL.

    


    \textbf{Minimal True Risk for \SEGNN $\Leftarrow$ Trivial Explanations}\\
    %
    We now show the other direction of the statement, i.e., if $\DET(G)$ provides \SEGNNEXPLs then an \SEGNN  $g$ (with ground truth graph classifier $f$ and perfect predictive accuracy) achieves minimal true risk.
    %
    Since, $g$ achieves perfect predictive accuracy, we have that $\calL (\CLF(\DET(G)), Y )$ is minimal.
    %
    Furthermore, by definition of trivial explanations and assumption of perfect predictive accuracy, we have that $R$ is the smallest subgraph such that $f(q(G)) = f(q(R)) = y^*$. 
    %
    Hence, $\lambda_1 |\DET(G)| / |E|$ can not be further minimized in \cref{eq:proof-sparsity-trivialexpl-GISST-small}.


    The same argument applies verbatim when adding the sparsification of (discrete) node feature explanations to \cref{eq:proof-sparsity-trivialexpl-GISST}, as prescribed in \citet{lin2020graph}.
    
    
    % We proceed by contradiction again. 
    % Let us assume that $\DET(G)$ provides a trivial explanation $R$ but the true risk is not minimal.
    % For the true risk to be smaller, we must either have a smaller $\calL \big (\CLF(\DET(G), Y \big )$ or a smaller $\lambda_1 \frac{|R|}{|\calE|}$ (or both). However, $\calL \big (\CLF(\DET(G), Y \big )$ can not be further minimized as the \SEGNN has perfect predictive accuracy by assumption. Also  $\lambda_1 \frac{|R|}{|\calE|}$ can not be further minimized as any smaller subgraph $R'$, such that $q(G)=R'$, \textcolor{red}{will necessarily lead to prediction change} due to $R$ being a trivial explanation. Which will again violate the assumption of perfect predictive accuracy.

    % Since $|\calE|$ is fixed, the previous minimization effectively optimizes for achieving high accuracy and low explanation size.
    % %
    % Now fix $R^*$ to be a \SEGNNEXPL.
    % %
    % By definition, we know that $R^*$ guarantees to make the correct prediction, at a fixed loss value $\calL \big (\CLF(R^*), Y \big ) = \epsilon$.
    % %
    % Also, every $R'^* \subset R^*$ is such that the prediction changes, meaning $\calL \big (\CLF(R^*), Y \big ) \gg \epsilon$.
    % Therefore, $R^*$ is the best trade-off for \cref{eq:proof-sparsity-trivialexpl-GISST-small}.
    % %
    % \ST{The above argument already applies to \GISST, right? It's not clear to me what's the difference with what follows} 
    % %
    % The same argument applies verbatim when adding the sparsification of node feature explanations to \cref{eq:proof-sparsity-trivialexpl-GISST}, as prescribed in \citet{lin2020graph}.


    \paragraph{Information Bottleneck-based losses} Let us consider the following training objective of a prototypical stochasticity injection-based \SEGNN, namely \GSAT \citep{miao2022interpretable}. The same holds for other models like \LRI \citep{miao2022interpretablerandom}, \GMT \citep{chen2024howinterpretable}, and \GIB\footnote{Even though it is not designed for interpretability, as it predicts importance scores at every layer making the resulting explanatory subgraph not intelligible, here we also include \GIB as a reference as it shares the same training objective.} \citep{wu2020graph}:
    \begin{equation}
    \label{eq:proof-sparsity-trivialexpl-GSAT}
        \min \calL \big (\CLF(\DET(G), Y \big ) + \lambda_1 \sum_{(u,v) \in E} p_{uv}\log(\frac{p_{uv}}{r}) + (1-p_{uv})\log(\frac{1-p_{uv}}{1-r})
    \end{equation}
    %
    By the hard explanation extractor assumption, $p_{uv} = 1$ when $(u,v) \in R$ and $r$ otherwise.
    %
    Then, we can differentiate the contribution of each edge to the second term separately, depending on its importance score:\\
    %
    For edges where $p_{uv} = 1$:
    %
    \begin{equation}
        \sum_{(u,v) \in E} p_{uv}\log(\frac{p_{uv}}{r}) + (1-p_{uv})\log(\frac{1-p_{uv}}{1-r}) = |\DET(G)|\log(\frac{1}{r})
    \end{equation}
    %
    For edges where $p_{uv} = r$:
    %
    \begin{equation}
        \sum_{(u,v) \in E} p_{uv}\log(\frac{p_{uv}}{r}) + (1-p_{uv})\log(\frac{1-p_{uv}}{1-r}) = 0
    \end{equation}
    %
    We can then rewrite the minimization as follows:
    %
    \begin{equation}
        \min \ \calL \big (\CLF(\DET(G), Y \big ) + \lambda_1 |\DET(G)|\log(\frac{1}{r})
    \end{equation}
    %
    which optimizes the same objective as \cref{eq:proof-sparsity-trivialexpl-GISST-small}, as the terms not in common are constants.
    %
    Thus, a similar argument to that of sparsity-based losses follows.


    % The insights above clearly hold only for appropriate choices of $\lambda_1$ and $r$ \ST{is $\lambda_2 > 0$ necessary for the relevance values to attain the extrema $\{0, 1\}$ and $\{0, r\}$?}.
    % %
    % % In fact, one can trivially set $\lambda_1=0$ or $r=1$ to avoid optimizing for explanation size. Similarly, setting $\lambda_1$ too high may yield very compact yet useless explanations, as the model might not reach a satisfactory accuracy.
    % %
    % To illustrate, we can analyze two extreme scenarios:
    % %
    % \paragraph{Case 1: $\lambda_1$ set too low.}
    % %
    % In this case, the optimization process essentially ignores explanation size, focusing entirely on classifier accuracy. As a result, the model may produce overly large or redundant explanations, failing to achieve the desired balance between accuracy and compactness. In the extreme case, setting $\lambda_1 = 0$, or equivalently $r=1$, avoids optimizing for explanation size altogether.

    % \paragraph{Case 2: $\lambda_1$ set too high.}
    % %
    % Conversely, if $\lambda_1$ is assigned an excessively high value, the optimization will heavily prioritize minimizing the explanation size, potentially at the expense of classifier performance. While the resulting explanations might be extremely compact, they could also lack meaning for the task.

    % These cases highlight the importance of carefully selecting hyperparameters to balance explanation compactness and classifier performance, which is achieved in practice by ablation studies.    
\end{proof}

% \begin{rem}
%     For proper choices of $\lambda$ and explanation thresholds $\tau$, optimizing \cref{eq:info_bottln} yields Trivial Explanations.
%     \begin{proof}
%         Consider the following three cases:
%         \begin{itemize}
            
%             \item $I(R; G)$ optimized via sparsity: Assuming to consider as relevant all the items with importance above a small constant $\tau > \epsilon$, then \cref{eq:info_bottln} yields explanations of minimum cardinality such as to maximize $I(Y; R)$, depending on the choice of $\lambda$.

%             \item $I(R; G)$ optimized via stochasticity: \cref{eq:info_bottln} pushes importance scores to either align to a predetermined noise level $r \in [0,1)$, or to the absence of noise (maximal relevance) equals $1$. 
%             %
%             If $r=0$, it boils down to the previous point.
%             %
%             Otherwise, by considering as relevant all items $\tau > r + \epsilon$, \cref{eq:info_bottln} yields explanations of minimum cardinality such as to maximize $I(Y; R)$, depending on the choice of $\lambda$

%             \item $I(R; G)$ constrained via hard size constraint: Optimizing \cref{eq:info_bottln} by imposing a fixed size on the explanation, e.g. TopK \citep{chen2022learning, wu2022discovering}, cannot guarantee to obtain a Trivial Explanation unless the fixed size matches the size of the (unknown) Trivial Explanation
%         \end{itemize}
%     \end{proof}
%     % A \SEGNN achieves optimal loss if R is a Trivial Explanation.
%     % \begin{proof}
%     %     Given a \SEGNN such that its detector \DET outputs a Trivial Explanation R, we want to show that it achieves optimal loss.
%     %     %
        
%     % \end{proof}
% \end{rem}

\subsection{Proof of \cref{prop:segnnexpl-are-piexpl}}

\segnnexplarepiexpl*

\begin{proof}
    Let $g$ be a classifier that can be expressed as a boolean FOL formula of the form
    \[
      \exists x_1 \, \exists x_2 \, \dots \exists x_k \;\; \Phi(x_1, x_2, \dots, x_k),
    \]
    where $\Phi$ is quantifier-free. A positive instance $G$ for $g$ is an instance such that $G \models g$.
    Now, let $R \subseteq G$ be a Trivial Explanation for $g(G)$, i.e, for $G\models \Phi$. We must show that $R$ is also a PI explanation. 
    % \SA{Maybe briefly discuss how it is possible to express a GNN in a formula referring to Barcelo e Grohe?}
    
We now show that $R$ satisfies conditions (1), (2) and (3) for PI Explanation as given in \cref{def:piexpl}
    \begin{enumerate}
    
        \item By definition of \SEGNNEXPL, we already have $R \subseteq G$. Hence, condition $(1)$ is satisfied.
        
        \item Since $R$ is a \SEGNNEXPL, we have that $R \models g$. 
        Also, $g$ is purely existential, hence there are specific elements $a_1, \dots, a_k \in R$ witnessing $g$; that is, $R \models \Phi(a_1, \dots, a_k)$. 
        Now if $\Gamma$ is a subgraph of $G$ such that $R \subseteq \Gamma$, then all $a_i$ remain in $\Gamma$. Hence $\Gamma$ also satisfies $g$ and therefore $\Gamma \models g$. This shows that every superset $\Gamma$ of $R$ inside $G$ satisfies $g$, satisfying the condition (2) in the PI explanation definition.
        \item Finally, we now show that $R$ is minimal. Note that since $R$ is a \SEGNNEXPL, there exists no $|R'| \leq |R|$, such that $R'\models g$. In particular, if there was a $R' \subset R$ such that $R' \models g$, that would contradict the minimality condition for a \SEGNNEXPL. Consequently, no such $R'$ can serve as a smaller PI explanation. This ensures condition (3).
    \end{enumerate}
\end{proof}

\textbf{Remark:} There exist classifiers that are not existentially quantified but \SEGNNEXPLs still equal PI explanations.
%
For instance, $\exists^{=1}x.Red(x)$ can not be expressed as a purely existentially quantified first-order logic formula for which \SEGNNEXPLs still equal PI explanations.





\subsection{Proof of \cref{prop:PI_TE_equality}}

\piteequality*

\begin{proof}
    Any \SEGNNEXPL $R$ for $g(G)$ is also an instance with a label $g(R) = g(G) =y$. 
    We now show that a \SEGNNEXPL for $g(R)$ is also a PI explanation for $g(R)$, and hence it is contained in $\bigcup_{G\in\Omega^{(y)}_{g}}\mathrm{PI}(g(G))$. For any $R' \subset R$, we have that $g(R') \neq g(G)$ (as $R$ is a \SEGNNEXPL for $g(G)$), and hence $g(R') \neq g(R)$. Furthermore, any extension of $R$ (within $R$) does not lead to prediction change, vacuously. Hence, $R$ is a PI explanation for $g(R)=g(G)=y$.
\end{proof}

% \subsection{Proof of \cref{prop:segnnexpl-can-be-unfaith}}

% \segnnexplcanbeunfaith*

% \begin{proof}
%         Let us consider the following examples:
%         %
%         \begin{itemize}
%             \item Given the task $\Phi_1 \defeq \exists x.Red(x)$ for an arbitrary graph G with $r$ red nodes and $|G| - r$ nodes of other colors, and a \SEGNNEXPL composed of a single red node $R = \{ Red(x_i) \}, i \in [1 \dots r]$. 
%             %
%             Then, evaluating \NEC according to \cref{def:faith} yields a necessity value of zero, and thus also a degree of faithfulness of zero, unless $r=1$.
%             %
%             This is because \NEC is computing the change in prediction after removing R, but if G contains multiple red nodes, then the removal of a single red node cannot change the prediction of $\Phi_1$.
%             %
%             In fact, it is easy to see that if $\Phi_1$ was instead $\Phi_1' \defeq \exists !x.Red(x)$, then R above for a positive instance $G'$ captures all and only the elements firing $\Phi_1'$, making R maximally \NEC and maximally \SUF. 
            

%             \item Given the task $\Phi_2 \defeq \exists xy.E(x,y)$ for an arbitrary graph G with $|\calE|$ edges, and a \SEGNNEXPL composed of a single edge $R = \{ E(u,v) \}, (u,v) \in \calE$.
%             %
%             Then, again \NEC yields a necessity value of zero, unless $|\calE|=1$.


%             \item Given the task $\Phi_3 \defeq \#Red \ge \#Blue$ which evaluates to true whenever the input graph has a number of red nodes greater or equal to the blue ones, and let G a positive instance with respectively $r$ red nodes and $b$ blue nodes.
%             %
%             The \SEGNNEXPL output by a \SEGNN now depends on the expressivity of the explanation extractor \DET. Consider in fact the following two cases:
%             \begin{itemize}
%                 \item If \DET is a standard node-wise message passing GNN without global aggregators, then node representations are unaware of the global count of red and blue nodes \citep{barcelo2020logical}. Therefore, \DET cannot reliably prune arbitrary input nodes without incurring a violation of the formula (think of a disconnected graph for example). Therefore, $R$ must contain every node in $G$. Then, \NEC can be arbitrarily low, as the number of subgraphs that can originate from G grows either as $r-b$ grows --for a fixed $b$-- or as $b < r$ --for a fixed $r$.

%                 \item If \DET contains global aggregators allowing the node representation to count the number of colored nodes in the entire graph, then the most compact explanation it can predict is composed of a single red node for samples of class 1, and a single blue node for samples of class 0.
%                 %
%                 % Such explanation is however little informative of the underlying task being solved, since it explains an easier "subtask", while the decision-making is spread across multiple modules.
%                 %
%                 Such an explanation has however a \NEC value of zero, unless $b=r$.
%             \end{itemize}
            
%             % Also, say that R>=B is anyways outside of the scope of the proposition of Sagar, highlighting holes in what Trivial Explanations can capture for arbitrary notions of "faithfulness"
            
%         \end{itemize}
% \end{proof}





\subsection{Proof of \cref{prop:PI-more-expressive}}

\pimoreexpressive*

\begin{proof}
    We continue with $g$ and $g'$ as given in the proof of \cref{thm:TE_Inexpressive}, i.e., $g = \exists x\exists y. E(x,y)$ and $g' = \forall x\exists y. E(x,y)$. 
    %
    As shown in \cref{thm:TE_Inexpressive}, condition $\ref{prop_PI_more_1}$ is true for $g$ and $g'$.
    %
    Now, for any positively labeled graph $G^{\ast}$, $\mathrm{PI}(g(G^\ast))$ is the set of edges in $G^{\ast}$, whereas $\mathrm{PI}(g'(G^\ast))$ is the set of edge covers. As shown in \cref{fig:examples_TE_PI_FAITH}, there exists a graph (say a triangle) such that the set of edge covers is different from the set of edges.
    %
    % Whereas, for any negatively labeled graph $G^{\ast}$, $\mathrm{PI}(g(G^\ast))$ consists of one explanation, containing all the nodes in $G^{\ast}$, whereas in $\mathrm{PI}(g'(G^\ast))$ each explanation is an isolated node of $G^\ast$.
    % As shown in \cref{fig:examples_TE_PI_FAITH}, there exists a graph (say a triangle) such that the set of edge covers is different from the set of edges.
\end{proof}




\subsection{Proof of \cref{prop:suf_equal_1}}

\sufequalone*

\begin{proof}
    An explanation $R$ is maximally sufficient if all possible edge deletions in $G \setminus R$ leave the predicted label $g(G')$ equal to $g(G)$, where $G'$ are the possible graphs obtained after perturbing $G \setminus R$. 
    %
    Equivalently, every possible extension $G'$ of $R$ in $G$ preserves the label. 
    %
    This is true if and only if $R$ is a PI explanation or there exists a subgraph $M \subset R$, which is a PI. 
\end{proof}



\subsection{Proof of \cref{prop:nec_above_0}}

\necabovezero*

\begin{proof}
    An explanation $R$  has zero necessity score if all possible edge deletions in $R$ leave the predicted label $g(G')$ equal to $g(G)$, where $G' \subseteq G$ are the possible subgraphs obtained after perturbations in $R$. 
    %
    Assume to the contrary that $R$ does not intersect a PI explanation $M$, and has a non-zero necessity score.
    %
    Then, there exists a graph $G'$ obtained by perturbing $R$ such that $g(G) \neq g(G')$. 
    %
    But note that $M \subseteq G'$ and $M$ is a PI by assumption, hence $g(G)$ must be equal to $g(G')$, leading to a contradiction. 
    %
    Therefore, $R$ must intersect all prime implicants to have a non-zero necessity score.
\end{proof}


% \subsection{Proof of \cref{prop:sufnec-are-piexpl}}

% \sufnecarepiexpl*

% \begin{proof}
%     \textbf{($\rightarrow$)}
%     %
%     Let $\Phi$ be the underlying task being explained such that G is a positive instance, and R an explanation $R \subseteq G$ such that:
%     %
%     \begin{itemize}
        
%         \item R has $\SUF=0$, meaning that for every $R'$ such that $R \subseteq R' \subseteq G$ the prediction remains the same, fulfilling condition (2) in \cref{def:piexpl}.

%         \item R has $\NEC=1$, meaning that for every $R''$ such that $R'' \subset R$ the prediction changes, implying that there always exists $\Gamma = R''$ such that $R'' \subseteq \Gamma \subseteq G \not \models \Phi$, fulfilling condition (3) in \cref{def:piexpl}
%     \end{itemize}
%     Since (1) is trivially satisfied, all conditions of \cref{def:piexpl} are satisfied.\\



%     \textbf{($\not \leftarrow$)}
%     %
%     Let $\Phi$ be the underlying task being explained, and R an explanation fulfilling all conditions in \cref{def:piexpl}.
%     %
%     Then, it is not possible to find $R' \subset R$ such that $R'$ is also sufficient as per condition (2).
%     %
%     Nonetheless, it is possible to have $R' \subset R$ such that $R' \models \Phi$.
%     %
%     The existence of such $R'$ makes however \NEC not maximal, as there exists at least one subgraph of R which does not lead to a prediction change.    
% \end{proof}




% \subsection{Proof of \cref{prop:segnnexpl-piexpl-may-not-match}}

% \segnnexplpiexplmaynotmatch*
%  \begin{proof}
%      If we prove that \cref{prop:segnnexpl-are-piexpl} is an \texttt{iff} condition, then this is a direct consequence of it. Otherwise, consider the following example:


     
%      %
%      Given the task $\Phi_3 \defeq \#Red \ge \#Blue$ which evaluates to true whenever the input graph has a number of red nodes greater or equal to the blue ones, and let G a positive instance with respectively $r$ red nodes and $b$ blue nodes.
%         %
%         The \SEGNNEXPL output by a \SEGNN now depends on the expressivity of the explanation extractor \DET. Consider in fact the following two cases:
%         %
%         \begin{itemize}
        
%             \item If \DET is a standard node-wise message passing GNN without global aggregators, then node representations are unaware of the global count of red and blue nodes \citep{barcelo2020logical}. Therefore, \DET cannot reliably prune arbitrary input nodes without incurring a violation of the formula (think of a disconnected graph for example). Therefore, $R$ must contain every node in $G$\footnote{This holds assuming no spurious correlations. For example, if a certain motif containing $k$ blue nodes appears only when $r - b \ge k$, then \DET might learn to discard such motif as it will never impact the underlying class, meaning the most compact explanation for a non-global \DET can be smaller than the entire graph.}. 
%             %
%             Then, condition (3) in \cref{def:piexpl} is not satisfied, as $R' = \{ r_i | i \in [1 \dots b]\} \subset R$ satisfies both (1) and (2).

            

%             \item If \DET contains global aggregators allowing the node representation to count the number of colored nodes in the entire graph, then the most compact explanation it can predict for class 1 is composed of a single red node $R = \{r_i\}$ for any $i \in [1 \dots r]$.
%             %
%             Then, condition (2) in \cref{def:piexpl} is not satisfied, as $\Gamma = R \cup \{ b_j, b_k \} \not \models \Phi_3$ for any $j,k \in [1 \dots b], j \ne k$.
            
%         \end{itemize}
%  \end{proof}









\section{Implementation Details}
\label{appx:impl_details}


\subsection{Datasets}
\label{appx:datasets}

In this study, we experimented on nine graph classification datasets commonly used for evaluating \SEGNNs. Among those, we also proposed two novel synthetic datasets to show some limitations of existing \SEGNNs.
%
More details regarding each dataset follow:
%
\begin{itemize}[leftmargin=1.25em]

    \item \BAColor (ours). Nodes are colored with a one-hot encoding of either red or blue.     The task is to predict whether the number of red nodes is larger or equal to the number of blue ones.    The topology is randomly generated from a Barab{\'a}si-Albert distribution \citep{barabasi1999emergence}. Each graph contains a number of total nodes in the range $[10, 100]$. We also generate two OOD splits, where respectively either the number of total nodes is increased to $250$ ($\text{OOD}_1$), or where the distribution of the base graph is switched to an Erdos-R{\'e}nyi distribution ($\text{OOD}_2$) \citep{erdds1959random}.

    \item \TopoFeature (ours).  Nodes are either uncolored or marked with a red color represented as one-hot encoding. The task is to predict whether the graph contains a certain motif together with at least two nodes.     The base graph is randomly generated from a Barab{\'a}si-Albert distribution \citep{barabasi1999emergence}. Each graph contains a number of total nodes in the range $[8, 80]$.   We also generate two OOD splits, where respectively either the number of total nodes is increased to $250$ ($\text{OOD}_1$), or where the distribution of the base graph is switched to an Erdos-R{\'e}nyi distribution ($\text{OOD}_2$) \citep{erdds1959random}

    \item \Motif \citep{gui2022good} is a three-classes synthetic dataset for graph classification where each graph consists of a basis and a special motif, randomly connected. The basis can be a ladder, a tree (or a path), or a wheel. The motifs are a house (class 0), a five-node cycle (class 1), or a crane (class 2). The dataset also comes with two OOD splits, where the distribution of the basis changes, whereas the motif remains fixed \citep{gui2022good}. In our work, we refer to the OOD validation split of \citet{gui2022good} as OOD$_1$, while to the OOD test split as OOD$_2$.

    \item \MUTAG \citep{debnath1991structure} is a molecular property prediction dataset, where each molecule is annotated based on its mutagenic effect. The nodes represent atoms and the edges represent chemical bonds. 

    \item \BBBP \citep{wu2018moleculenet} is a dataset derived from a study on modeling and predicting barrier permeability \citep{martins2012bayesian}.

    \item \AIDS \citep{riesen2008iam} contains chemical compounds annotated with binary labels based on their activity against HIV. Node feature vectors are one-hot encodings of the atom type.

    \item \AIDSC (ours) is an extension of \AIDS where we concatenate the value $1.0$ to the feature vector of each node.
    
    \item \MNIST \citep{knyazev2019understanding} converts the image-based digits inside a graph by applying a super pixelation algorithm. Nodes are then composed of superpixels, while edges follow the spatial connectivity of those superpixels.
    
    \item \SST is a sentiment analysis dataset based on the NLP task of sentiment analysis, adapted from the work of \citet{yuan2022explainability}. The primary task is a binary classification to predict the sentiment of each sentence.
\end{itemize}


\subsection{Architectures}
\label{appx:architectures}

\paragraph{\SEGNNs}
%
The \SEGNNs considered in this study are composed of an explanation extractor \DET and a classifier \CLF.
%
The explanation extractor is responsible for predicting edge (or equivalently node) relevance scores $p_{uv} \in [0,1]$, which indicate the relative importance of that edge. 
%
Scores are trained to saturate either to $1$ or to a predetermined value that is considered as the uninformative baseline. 
%
For IB-based losses, this value corresponds to the parameter $r$ \citep{miao2022interpretable, miao2022interpretablerandom}, whereas for Sparsity based it equals $0$ \citep{lin2020graph,yu2020graph}.
%
The classifier then takes as input the explanation and predicts the final label.
%
Generally, both the explanation extractor and the classifier are implemented as GNNs, and relevance scores are predicted by a small neural network over an aggregated representation of the edge, usually represented as the concatenation of the node representations of the incident nodes.
%
A notable exception is \GISST, using a \textit{shallow} explanation extractor directly on raw features.
%
Both explanation extractors and classifiers can then be augmented with other components to enhance their expressivity or their training stability like virtual nodes \citep{wu2022discovering,azzolin2024perkspitfalls} and normalization layers \citep{ioffe2015batch}.
%
Then, \SEGNNs may also differ in their training objectives, as shown in \cref{tab:taxonomy-loss}, or the type of data they are applied to \citep{miao2022interpretablerandom}.


We resorted to the codebase of \citet{gui2022good} for implementing \GSAT, which contains the original implementation tuned with the recommended hyperparameters.
%
\GISST is implemented following the codebase of \citet{christiansen2023faithful}.
%
When reproducing the original results was not possible, we manually tuned the parameters to achieve the best downstream accuracies.
%
We use the same explanation extractor for every model, implemented as a \GIN \citep{xu2018powerful}, and adopt Batch Normalization \citep{ioffe2015batch} when the model does not achieve satisfactory results otherwise.
%
Following \citet{azzolin2024perkspitfalls}, we adopt the \textit{explanation readout} mitigation in the final global readout of the classifier, so to push the final prediction to better adhere to the edge relevance scores.
%
This is implemented as a simple weighted sum of final node embeddings, where the weights are the average importance score for each incident edge to that node.
%
The only exceptions are \Motif, \BBBP, and \SST, where we use a simple mean aggregator as the final readout to match the results of original papers. 

To overcome the limitation of \GISST in extracting explanatory scores from raw input features, we propose an augmentation named Simple Modular GNN (\SMGNN).
%
\SMGNN adopts the same explanation extractor as \GSAT, which is trained, with the \GISST's sparsification loss after an initial warmup of 10 epochs.
%
Similarly to \GSAT, unless otherwise specified, the classifier backbone is shared with that of the explanation extractor, and composed of $5$ layers for \BBBP, and $3$ layers for all the other datasets. 


Regarding the choice of model hyper-parameter, we set the weight of the explanation regularization as follows:
%
For \GISST, we weight all regularization by $0.01$ in the final loss; 
%
For \SMGNN, we set $1.0$ and $0.8$ the $L_1$ and entropy regularization respectively;
%
For \GSAT, we set the value of $r$ to $0.7$ for \Motif, \MNIST, \SST, and \BBBP, to $0.5$ for \TopoFeature, \AIDS, \AIDSC, and \MUTAG, and to $0.3$ for \BAColor. 
%
Also, for \GSAT we set the decay of $r$ is set every $10$ step for every dataset, except for \SST and \Motif where it is set to $20$.
%
Then, the parameter $\lambda$ regulating the weight of the regularization is set to $0.001$ for all experiments with \SMGNN, while to $1$ for \GSAT on every dataset except for \BAColor.


For each model, we set the hidden dimension of GNN layers to be $64$ for \MUTAG, $300$ for \Motif, \BBBP, and \SST, and $100$ otherwise.
%
Similarly, we use a dropout value of $0.5$ for \Motif and \SST, of $0.3$ for \MNIST, \MUTAG, and \BBBP, and of $0.0$ otherwise.
%
% Also, for \GISST, we set a weight decay value of $0.0$ for \TopoFeature, \Motif, and \AIDS, a value of $0.001$ for \MNIST, \BAColor and \BBBP, and a value of $0.005$ for \SST, and $0.1$ for \AIDSC.
% Similarly, for \SMGNN, instead, we set a value of $0.0$ \TopoFeature, \BAColor, \Motif, and \AIDS, a value of $0.001$ for \MNIST and \BBBP, a value of $0.05$ for \SST and $0.1$ for \AIDSC.
% Finally, for \GSAT, we set a value of $$





\paragraph{\GLSEGNNs}
%
\GLSEGNNs are implemented as a reference \SEGNN of choice, whose architecture remains unchanged, and a linear classifier taking as input a global sum readout of node input features.
%
Both models have an output cardinality equal to the number of output classes, with a sigmoid activation function.
%
Then, the outputs of the two models are concatenated and fed to our \BLENs, described in \cref{appx:ablation_aggr}.
%
Following \citet{barbiero2022entropy}, we use an additional fixed temperature parameter with a value of $0.6$ to promote sharp attention scores of the underlying \LENs. 
%
Also, the number of layers of \BLENs and \LENs are fixed to $3$ including input and output layer, and the hidden size is set to $350$ for \MNIST, $64$ for \MUTAG and \BBBP, $30$ for \Motif, and $20$ otherwise.
%
The input layer of \BLENs and \LENs does not use the \textit{bias} parameter.
%
We adopt weight decay regularization to promote the sparsity of the 
linear model. For the additional experiment on \AIDSC (see \cref{appx:aidsc1}), a more stringent $L_1$ sparsification is applied.



\subsection{Extracting rules from linear classifiers}
\label{appx:rule-extraction}

Although linear classifiers do not explicitly generate rules, their weights can be interpreted as inequality-based rules. For this interpretation to be meaningful, the model should remain simple, with sparse weights that promote clarity. In this section, we review how this can be achieved.

A (binary) linear classifier makes predictions based on a linear combination of input features, as follows:
%
\begin{equation}
    y = w^T \vx + b
\end{equation}
%
where $\vx \in \bbR^d$ is a $d$-dimensional input vector , and $w \in \bbR^d$ and $b \in \bbR$ the learned parameters.
%
The decision boundary of the classifier corresponds to the hyperplane $w^T \vx + b = 0$, and the classification is then based on the sign of $y$. We will consider a classifier predicting the positive class when 
\begin{equation}
\label{eq:lin_clf}
    w^T \vx + b \ge 0.
\end{equation}
%
By unpacking the dot product, \cref{eq:lin_clf} corresponds to a weighted summation of input features, where weights correspond to the model's weights $w$:
%
\begin{equation}
\label{eq:lin_clf_unpacked}
    w_1 x_1 + \dots + w_d x_d + b \ge 0.
\end{equation}
%
For ease of understanding, let us commit to the specific example of \BAColor. There, the linear classifier in the side channel of \GLSSEGNNs takes as input the sum of node features for each graph. Therefore, our input vector will be $\vx = [x_r,x_b,x_u]$, where $x_r,x_b,x_u$ indicates the number of red, blue, and uncolored nodes, and a positive prediction is made when
%
\begin{equation}
\label{eq:lin_clf_unpacked_bacolor}
    w_r x_r + w_b x_b + w_u x_u + b \ge 0.
\end{equation}
%
If the model is trained correctly, and the training regime promotes enough sparsity -- e.g. via weight decay or $L_1$ sparsification -- we can expect to have $w_u \sim 0$ as there are no uncolored nodes and thus feature $x_u$ carries no information, and $b \sim 0$.
%
Then, we can rewrite \cref{eq:lin_clf_unpacked_bacolor} as 
%
\begin{equation}
\label{eq:lin_clf_unpacked_bacolor2}
    w_r x_r \ge  -w_b x_b
\end{equation}
%
Then, $w_r = -w_b$ is a configuration of parameters that allows to perfectly solve the task, yielding the formula
\begin{equation}
\label{eq:lin_clf_unpacked_bacolor3}
    x_r \ge x_b
\end{equation}

% We show in \cref{tab:exp_BACOLOR_weights}, that indeed our \GLSSEGNNs learned such configuration of parameters, resulting in a final intelligible formula like \cref{eq:lin_clf_unpacked_bacolor3}.
%
% Although the bias term $b$ in \cref{tab:exp_BACOLOR_weights} is non-zero, its contribution is very low, as the range of $x_r$ and $x_b$ is $[10,100]$.
%
We show in \cref{fig:dec_bound_bacolor} that indeed our \GLSSEGNNs learned such configuration of parameters by illustrating the decision boundary for the first two random seeds over the validation set. The figure confirms that the decision boundary is separating graphs based on the prevalence of red or blue nodes. 

We plot in \cref{fig:dec_bound_topofeat} a similar figure for \TopoFeature, where the decision boundary intersects the y-axis, on average over 10 seeds, at the value of $1.23$. By inspecting the model's weights, non-red nodes are assigned a sensibly lower importance magnitude (at least $10^2$ lower). Therefore, to convey a compact formula, we keep only the contribution of $x_r$, resulting on average in the final formula $x_r \ge 1.23$, which fires when at least two red nodes are present.



% Similarly, for \TopoFeature the input vector $\vx = [x_r,x_b,x_n]$, where $x_r,x_b,x_u$ indicates the number of red, blue, and uncolored nodes. Since there are no blue colors, $x_b$ will always be zero.
% Then \cref{eq:lin_clf_unpacked_bacolor2} writes as,
% %
% \begin{equation}
%     w_r x_r > - (b + w_u x_u).
% \end{equation}
% %
% By training the \GLSSEGNN and by inspecting model weights, we see that $x_u$ gets a significantly less magnitude than $x_r$, and thus for the sake of simplicity we will drop it from 



% \begin{table}[t]
% \centering
% \footnotesize
% \caption{
%     \GLSEGNN's linear classifier weights for the \GLSMGNN over \BAColor.
% }
% \label{tab:exp_BACOLOR_weights}
% \scalebox{0.99}{
%     \begin{tabular}{lcc}
%         \toprule
%          \textbf{Model} & 
%          \multicolumn{2}{c}{\textbf{\BAColor}}\\
%          \GLSMGNN's seed & Weights (W) & Bias (b)\\
        
% \midrule

%     seed 1
%         & \begin{tabular}{cc}
%             -0.515 & 0.516\\
%            \end{tabular} 
%         & -0.579\\

% % \midrule

%     seed 2
%         & \begin{tabular}{cc}
%             -0.412 & 0.412\\
%            \end{tabular} 
%         & -0.332\\

% % \midrule

%     seed 3
%         & \begin{tabular}{cc}
%             0.357 & -0.356\\
%            \end{tabular} 
%         & -0.183\\

% % \midrule

%     seed 4
%         & \begin{tabular}{cc}
%             0.474 & -0.472\\
%            \end{tabular} 
%         & -0.177\\

% % \midrule

%     seed 5
%         & \begin{tabular}{cc}
%             -0.492 & 0.493\\
%            \end{tabular} 
%         & -0.519\\

% % \midrule

%     seed 6
%         & \begin{tabular}{cc}
%             -0.483 & 0.484\\
%            \end{tabular} 
%         & -0.554\\

% % \midrule

%     seed 7
%         & \begin{tabular}{cc}
%             -0.420 & 0.421\\
%            \end{tabular} 
%         & -0.526\\

% % \midrule

%     seed 8
%         & \begin{tabular}{cc}
%             0.432 & -0.431\\
%            \end{tabular} 
%         & -0.084\\

% % \midrule

%     seed 9
%         & \begin{tabular}{cc}
%             -0.206 & 0.205\\
%            \end{tabular} 
%         & -0.262\\

% % \midrule

%     seed 10
%         & \begin{tabular}{cc}
%             0.523 & -0.524\\
%            \end{tabular} 
%         & -0.176\\

% \midrule

%     \end{tabular}
% }
% \end{table}


\begin{figure}
    \centering
        \subfigure[]{%
        \includegraphics[width=0.4\textwidth]{fig/rules/dec_boundary_1.png}
    }
    \subfigure[]{%
        \includegraphics[width=0.4\textwidth]{fig/rules/dec_boundary_2.png}
    }
    
    \caption{Decision boundary of the linear classifier of \GLSMGNN for \BAColor over the validation split (random seed 1 and 2).
    %
    The plot is in 2D since \BAColor contains only red or blue nodes.
    }
    \label{fig:dec_bound_bacolor}
\end{figure}

\begin{figure}
    \centering
        \subfigure[]{%
        \includegraphics[width=0.4\textwidth]{fig/rules/Topo_dec_boundary_1.png}
    }
    \subfigure[]{%
        \includegraphics[width=0.4\textwidth]{fig/rules/Topo_dec_boundary_2.png}
    }
    
    \caption{Decision boundary of the linear classifier of \GLSMGNN for \TopoFeature over the validation split (random seed 1 and 2). 
    %
    For seed 1, $w_r=0.89$, $w_b=4e^{-42}$, $w_u=-0.001$,  and $b=-1.3$.
    For seed 2, $w_r=0.92$, $w_b=-1e^{-33}$, $w_u=-0.002$,  and $b=-1.09$.
    %
    Since $x_b$ equals $0$ as there are no blue nodes in the dataset, we drop its visualization collapsing the plot to 2D. 
    %
    When providing the final interpretable formula, we drop $x_b$ and $x_u$ due to their sensible lower magnitude.
    %
    More aggressive sparsification can be applied to the training of the linear model to promote an even lower $w_u$.
    }
    \label{fig:dec_bound_topofeat}
\end{figure}


\subsection{Training and evaluation.}
Every model is trained for the same 10 random splits, and the optimization protocol is fixed across all experiments following previous work \citep{miao2022interpretable} and using the Adam optimizer \citep{kingma2015adam}.
%
Also, for experiments with \GLSEGNN, we fix an initial warmup of 20 epochs where the two channels are trained independently to output the ground truth label. 
%
After this warmup, only the overall model is trained altogether.
%
The total number of epochs is fixed to $100$ for every dataset except for \SST where it is set to $200$.

For experiments on \SST we forced the classifier of any \SEGNNs to have a single GNN layer and a final linear layer mapping the graph embedding to the output.
%
The parameters of the classifier are then different from those of the explanation extractor and trained jointly.

When training \SMGNN, to avoid gradient cancellation due to relevance scores approaching exact zero, we use a simple heuristic to push the scores higher when their average value in the batch is below $2e^{-9}$.
%
This is implemented by adding back to the loss of the batch the negated mean scaled by $0.1$.
%
This is similar to value clapping used by \GISST, but we found it to yield better empirical performances. 


\subsection{Software and hardware}

Our implementation is done using PyTorch 2.4.1 \citep{paszke2017automatic} and PyG 2.4.0 \citep{fey2019fast}.
%
Experiments are run on two different Linux machines, with CUDA 12.6 and a single NVIDIA GeForce RTX 4090, or with CUDA 12.0 and a single NVIDIA TITAN V.



\section{Additional Experiments}



\subsection{Ablation study on how to choose \aggr}
\label{appx:ablation_aggr}

\begin{table}[t]
\centering
\footnotesize
\caption{
    Raw channel relevance scores for the experiments in \cref{tab:synt_exp}.
    %
    The "Channel" column reports, the relative importance computed for each channel,
    For binary classification tasks, the first entry corresponds to the \SEGNN channel, whereas the last is to the interpretable model.
    For multi-class tasks, the first $|Y|$ entries correspond to the importance for each of the $|Y|$ \SEGNNs outputs, whereas the last $|Y|$ to that of the linear model. For \Motif, in particular, $|Y| = 3$.
}
\label{tab:raw_channel_scores}
\scalebox{0.8}{
    \begin{tabular}{lcccccc}
        \toprule
         \textbf{Model} & 
         \multicolumn{2}{c}{\textbf{\BAColor}} & 
         \multicolumn{2}{c}{\textbf{\TopoFeature}} & 
         \multicolumn{2}{c}{\textbf{\Motif}}\\

         & Acc & Channel & Acc & Channel & Acc & Channel \\
        
        \midrule

        $\GLGSAT$
            & \nentry{100}{00} & $[ 0.02, 0.98 ] \pm 0.05$
            & \nentry{100}{00} & $[ 0.47, 0.53 ] \pm 0.27$
            & \nentry{93}{01}  & \makecell{$[ 0.33, 0.36, 0.30, 4e^{-3}, 2e^{-4}, 1e^{-4}]$ \\ $\pm [0.33, 0.36, 0.29, 1e^{-2}, 2e^{-4}, 1e^{-4}]$}\\

        $\GLSMGNN$
            & \nentry{100}{00} & $[ 0.05, 0.95 ] \pm 0.10$
            & \nentry{100}{00} & $[ 0,31, 0,69] \pm 0.11$
            & \nentry{93}{00} & \makecell{$[ 0.27, 0.19, 0.53, 1e^{-4}, 3e^{-5}, 1e^{-3}]$ \\ $\pm [0.39, 0.29, 0.39, 0.0, 0.0, 0.0]$}\\

            

        \bottomrule         
    \end{tabular}
}
\end{table}


\begin{table}[t]
\centering
\footnotesize
\caption{
    Ablation study for the choice of \aggr.
    %
    The reference architecture in use is \SMGNN and the dataset used for the evaluation is \TopoFeature.
    %
    The "Channel" column reports, when \aggr supports it, the relative importance computed for each channel, where the first entry corresponds to the \SEGNN channel, whereas the last to the interpretable model.
}
\label{tab:exp_aggr}
\scalebox{0.99}{
    \begin{tabular}{lccc}
        \toprule
         \textbf{\aggr} & 
         \multicolumn{3}{c}{\textbf{\TopoFeature}}\\

         & Acc & Channel & Num red nodes $\ge$\\
        
        \midrule

        $\aggr_{\text{G{\"o}del}}$
            & \nentry{96}{03} & Not supported & -\\

        $\aggr_{\text{Product}}$
            & \nentry{96}{04} & Not supported & -\\

        $\aggr_{\text{Linear}}$
            & \nentry{79}{11} & $[ -1.22 , 1.77 ] \pm [1.65, 0.92]$ & $0.24 \pm 1.01$\\

        $\aggr_{\text{MLP}}$
            & \nentry{99}{02} & Not supported & -\\

        $\aggr_{\text{\LENs}}$
            & \nentry{95}{05} & $[ 0.62 , 0.38 ] \pm 0.42$ & $0.17 \pm 1.47$\\

        $\aggr_{\text{\LENs}}^{ST}$
            & \nentry{65}{07} & $[ 0.01 , 0.99 ] \pm 0.01$ & $0.70 \pm 0.96$\\

        $\aggr_{\text{\BLENs}}$ (ours)
            & \nentry{100}{00} & $[ 0.31 , 0.69 ] \pm 0.11$ & $1.23 \pm 0.17$\\
        
        \bottomrule         
    \end{tabular}
}
\end{table}

Our implementation of \aggr relies on \LENs to combine the two channels in an interpretable manner.
%
\LENs are however found to be susceptible to leakage \citep{azzolin2022global}, that is they can exploit information beyond those encoded in the activated class.
%
Leakage hinders the semantics of input activations, thus comprising the interpretability of the prediction \citep{margeloiu2021concept, havasi2022addressing}.
%
Popular remedies include binarizing the input activations \citep{margeloiu2021concept, azzolin2022global, giannini2024interpretable} so as to force the hidden layers of \LENs to focus on the activations themselves, rather than their confidence.


In our work, we adopt a different strategy to avoid leakage in LENs and propose to anneal the temperature of the input activations so as to gradually approach a binary activation during training, while ensuring a smooth differentiation.
%
Input activations are generally assumed to be continuous in $[0,1]$, and usually generated by a sigmoid activation function \citep{barbiero2022entropy}.
%
Therefore, our temperature annealing simply scales the raw activation before the sigmoid activation by a temperature parameter $\tau$, where $\tau$ is linearly annealed from a value of $1$ to $0.3$.
%
The resulting model is indicated with the name of (Binary)\LENs -- \BLENs in short.

In the following ablation study, we investigate different approaches for implementing \aggr, showing that only \BLENs reliably achieve satisfactory performances while preserving the semantics of each channel.
%
The baselines for implementing \aggr considered in our study are as follows:
% 
\begin{itemize}
    
    \item Logic combination based on T-norm fuzzy logics, like G{\"o}del $\aggr_{\text{G{\"o}del}}(A,B) = min(A,B)$ and Product logic $\aggr_{\text{Product}}(A,B) = A*B$ \citep{klement2013triangular}.

    \item Linear combination $\aggr_{\text{Linear}}(A,B) = W[A || B]$, where $||$ concatenates the two inputs.

    \item Multi-Layer Perceptron $\aggr_{\text{MLP}}(A,B) = W_3(\sigma(W_2(\sigma W_1[A || B])))$.

    \item Logic Explained Network (\LENs) $\aggr_{\text{\LENs}}(A,B) = \text{\LENs}(A || B)$.

    \item Logic Explained Network (\LENs) with discrete input $\aggr_{\text{\LENs}}^{ST}(A,B) = \text{\LENs}(ST(A) || ST(B))$.
    %
    The discreteness of the input activations is obtained using the Straight-Trough (ST) reparametrization \citep{jang2016categorical, azzolin2022global,giannini2024interpretable}, which uses the hard discrete scores in the forward step of the network, while relying on their soft continuous version for backpropagation.
    
\end{itemize}


We provide in \cref{tab:exp_aggr} the results for \GLSMGNN using different \aggr choices over \TopoFeature.
%
In this dataset, the expected rule to be learned by the interpretable channel is \textit{number of red nodes} $\ge$ \textit{2}.
%
Among the alternatives, $\aggr_{\text{G{\"o}del}}$, $\aggr_{\text{Product}}$, and $\aggr_{\text{MLP}}$ do not allow to understand how the channels are combined, resulting in unsuitable \aggr functions for our purpose.
%
$\aggr_{\text{Linear}}$ and $\aggr_{\text{\LENs}}^{ST}$, instead, fail in solving the task.
%
$\aggr_{\text{\LENs}}$, however, achieve both a satisfactory accuracy and an interpretable combination of channels.
%
Nonetheless, the presence of leakage hinders a full understanding of the interpretable rule, as the hidden layers of the \LENs are allowed to exploit the confidence of the interpretable channel's predictions in building an alternative rule, that it is now no longer intelligible.
%
Overall, \BLENs is the only choice that allows the combination of both accuracy and an interpretable combination of the two channels, all while preserving the semantics of the channels as testified by the rule \textit{number of red nodes} $\ge 1.23$ matching the expectations\footnote{Since the number of nodes is discrete, any threshold value in $[1+\epsilon, 2]$ corresponds to the rule $\ge 2$.}.





\subsection{More experiments on \AIDS}
\label{appx:aidsc1}



In \cref{tab:real_exp} we showed that a simple linear classifier on sum-aggregated node features can suffice for achieving the same, or better, performances than a plain \SEGNN.
%
Even if the linear classifier is promoted for sparsity via weight decay regularization, the resulting model is still difficult to be interpreted, as it assigns non-negligible scores to a multiple of input features, making it difficult to extract a simple rule.
%
For this reason, we extend the original set of node features by concatenating the value $1.0$ for each node. 
%
The role of this additional feature is to allow the linear classifier to easily access the graph-level feature \textit{number of nodes}.
%
We name the resulting updated dataset \AIDSC.


Then, we train the same models as \cref{tab:real_exp}, where we increase the sparsity regularization on both the \SEGNN and the linear model, to heavily promote sparsity.
%
This is achieved by setting to $0.1$ the weight decay regularization for the \SEGNN, and to $0.01$ a $L_1$ sparsification to the linear classifier.
%
The results are shown in \cref{tab:exp_AIDSC1}, and show that under this strong regularization, \SEGNN struggles to solve the task.
%
Conversely, the \GLSEGNN augmentation still solves the task, while providing intelligible predictions.
%
In fact, by inspecting the weights of the linear classifier when picked by the \GLSEGNN, the resulting model weights reveal that the model is mainly relying on the count of nodes in the graph, as pointed out in \citet{pluska2024logical}.
%
For completeness, we report in \cref{tab:exp_AIDSC1_weights} the full weight matrix for \GLSMGNN.



\begin{table}[]
\centering
\footnotesize
\caption{
    \GLSEGNNs solve \AIDSC even when prompted for strong sparsification, whereas plain \SEGNN achieves suboptimal predictions.
    %
    Results are averaged only over seeds where the \GLSEGNN selects the linear classifier as the main channel. We indicate with superscript numbers the seeds left out from the analysis.
    %
    The rule is extracted from the last column of \cref{tab:exp_AIDSC1_weights} and averaged across nine seeds.
}
\label{tab:exp_AIDSC1}
\scalebox{0.99}{
    \begin{tabular}{lccc}
        \toprule
         \textbf{Model} & 
         \multicolumn{3}{c}{\textbf{\AIDSC}}\\
         & F1 & \makecell{Channel} & \makecell{Rule}\\
        
        \midrule

        \GIN
            & \nentry{85}{05} & - & -\\
        
        \midrule

         \GISST
            & \nentry{68}{06} & - & -\\

         \GLGISST$^{2}$
            & \nentry{99}{02} & \FlatC & num nodes $\le 12.66 \pm 0.18$\\

        \midrule
        
         \GSAT
            & \nentry{70}{06} & - & -\\

         \GLGSAT$^{4,10}$
            & \nentry{99}{02} & \FlatC & num nodes $\le 13.64  \pm 1.41$\\

        \midrule
        
         \SMGNN
            & \nentry{68}{06} & - & -\\

         \GLSMGNN$^{10}$
            & \nentry{99}{02} & \FlatC & num nodes $\le 13.89  \pm 3.07$\\

        \bottomrule         
    \end{tabular}
}
\end{table}














\subsection{\GLSEGNN can improve the faithfulness of \SEGNNs}
\label{appx:exp_faith}

To measure the impact of the \GLSEGNN augmentation to plain \SEGNNs on the faithfulness of explanations, we compute \FAITH (\cref{def:faith}) for \SMGNN and \GSAT with their respective augmentations on \TopoFeature and \Motif.
%
Following \citet{christiansen2023faithful}, we compute \FAITH for both the actual explanation and a randomized explanation obtained by randomly shuffling the explanation scores before feeding them to the classifier, and computing their ratio.

\begin{equation}
\label{eq:faith_ratio}
    \text{\FAITH ratio} = \frac{\FAITH(\calE)}{\FAITH(R)}
\end{equation}

where $R$ is the original explanation, and $\calE$ a randomly shuffled explanation. 
%
The metric achieves a score of $1$ when the two values match, meaning the model is as faithful to the original explanation as a random explanation, whilst achieves a score of $0$ when the faithfulness of the original explanation is considerably higher than that of a random one.


We compute the metric over the entire validation splits, and extract hard explanations by applying a topK strategy as indicated in previous studies \citep{amara2022graphframex, longa2024explaining}, where $k \in [0.3,0.6,0.9]$ for \Motif, and $k \in [0.05, 0.1, 0.2, 0.4, 0.8]$ for \TopoFeature.
%
Perturbations are limited to edge removals, and we force isolated nodes to be removed from the explanation.
%
To compute \SUF and \NEC, we refer to the implementation of \citet{azzolin2024perkspitfalls} which requires a hyperparameter $b$ encoding the number of removals to apply at each perturbation.
%
To obtain more robust results, we vary $b \in [0.01, 0.05, 0.1]$, corresponding to a number of perturbations equal to a $b$ percentage of the graph size.
%
Then, the final \FAITH score is taken as the best \FAITH across $k$ and averaged across the values of $b$.


The final results are reported in \cref{tab:exp_faith} and highlight that for \TopoFeature, where both \GLSSEGNNs can exploit the interpretable channel (see \cref{tab:synt_exp}), \GLSEGNNs can achieve considerable gains in faithfulness.
%
\GLGSAT, in particular, achieves a better score but with a considerably higher standard deviation.
%
By inspecting the raw scores, however, we see that across the values $b$, \GLGSAT scores $[1.00, 0.05, 0.12]$ whereas \GSAT achieves $[0.50, 0.27, 0.48]$, indicating that $b = 0.01$ can be an unfortunate choice for this model as it may not bring enough perturbations to let the model change prediction.
%
On the other values of $b$, however, the model achieves a significant gain in faithfulness ratio of almost an order of magnitude.
%
On \Motif, instead, as the \GLSEGNNs does not have any advantage in using an interpretable model, we do not expect significant changes in the faithfulness scores as indicated in \cref{tab:exp_faith}, where the only gain is due to higher variance.


We argue that the substantial gains in faithfulness mainly come from the ability of \GLSEGNNs to delegate each sub-task to the channel that can best handle it, i.e., learning the motif for the \SEGNN and the "$\ge$" rule to the linear classifier.
%
In doing so, the underlying \SEGNN can better focus on highlighting the topological explanation, resulting in a more faithful explanation.
%
This insight is supported by the analysis of the compactness of explanations provided in \cref{appx:expl_examples}, showing that indeed those \SEGNNs can better focus on the topological sub-task.




\begin{table}[t]
\centering
\footnotesize
\caption{
    Faithfulness of \SEGNNs and their augmentations for \TopoFeature and \Motif.
    %
    Following \citep{christiansen2023faithful}, we report the ratio between \FAITH computed over randomly shuffled explanations and original ones.
    %
    Therefore, scores close to zero indicate better values.
}
\label{tab:exp_faith}
\scalebox{0.99}{
    \begin{tabular}{lcc}
        \toprule
         \textbf{Model} & 
         \multicolumn{1}{c}{\textbf{\TopoFeature}} & \multicolumn{1}{c}{\textbf{\Motif}}\\

         & \FAITH ratio ($\downarrow$) & \FAITH ratio ($\downarrow$)\\
        
        \midrule

        \GSAT
            & \nentry{0.42}{0.10} & \nentry{0.78}{0.08}\\

        \GLGSAT
            & \nentry{0.39}{0.43} & \nentry{0.69}{0.11}\\

        \midrule

        \SMGNN
            & \nentry{0.65}{0.28} & \nentry{1.00}{0.00}\\

        \GLSMGNN
            & \nentry{0.04}{0.04} & \nentry{1.00}{0.00}\\
        
        \bottomrule         
    \end{tabular}
}
\end{table}




\subsection{Plotting Explanations}
\label{appx:expl_examples}


In this section, we aim to provide examples of explanations of \SEGNNs and \GLSSEGNNs.
%
For visualization purposes, we rely on an importance threshold to plot the \textit{hard} explanatory subgraph over the entire graph.
%
Such threshold is picked by plotting the histogram of importance scores and chosen in such a way as to separate regions of higher scores from regions of lower scores.
%
We will analyze the following datasets:



\paragraph{\TopoFeature.}
%
\cref{fig:expl_histograms_gsat_topofeature} and \cref{fig:expl_histograms_smgnn_topofeature} present the histograms of explanation relevance scores for \GSAT and \SMGNN respectively.
%
Overall, \SMGNN achieves a better separation between higher and lower explanation scores, making it easier to select a proper threshold to plot explanations.
%
Therefore, we will proceed to show explanation examples in \cref{fig:expl_examples_topofeature_SMGNN} only for \SMGNN for seed 1, picking as threshold the value $0.8$.
%
Overall, the model succeeded in giving considerably higher relevance to edges in the motif, but failed in highlighting red nodes as relevant for predictions of class $1$, hindering a full understanding of the model's decision process.

We proceed now to analyze the explanations extracted for the same samples for \GLSMGNN.
%
First, we plot the histogram of explanation scores in \cref{fig:expl_histograms_glsmgnn_topofeature}, showing better sparsification than a plain \SMGNN.
%
For reference, we also plot the histogram for \GLGSAT scores in \cref{fig:expl_histograms_glgsat_topofeature}, where the same sparsification effect can be observed.
%
Then, we report in \cref{fig:expl_examples_topofeature_GLSMGNN} the explanations for the same graphs as in \cref{fig:expl_examples_topofeature_SMGNN}, showing that \GLSMGNN achieves better sparsification than a plain \SMGNN.
%
In fact, since the rule \textit{at least two red nodes} is learned by the interpretable model, the underlying \SMGNN now just looks at the topological motif, and indeed the substantially more sparse edge score reflects this behavior.
%
Overall, \GLSMGNN explanations reflect more closely the actual predictive behavior of the underlying \SEGNN.


\begin{figure}
    \centering
    \subfigure{\includegraphics[width=0.99\textwidth]{fig/explanations/TopoFeature_GSAT_histogram.pdf}}
    
    \caption{\textbf{Histograms of explanation relevance scores for \GSAT  on \TopoFeature} (validation set).
    %
    The model fails to reliably separate between relevant and non-relevant edges, making it difficult to select a proper relevance threshold.
    }
    \label{fig:expl_histograms_gsat_topofeature}
\end{figure}

\begin{figure}
    \centering
    \subfigure{\includegraphics[width=0.99\textwidth]{fig/explanations/TopoFeature_SMGNN_histogram.pdf}}
    
    \caption{\textbf{Histograms of explanation relevance scores for \SMGNN  on \TopoFeature} (validation set). 
    %
    The sparsification mechanism of \SMGNN better separates edges with higher importance than the rest of the graph.
    }
    \label{fig:expl_histograms_smgnn_topofeature}
\end{figure}

\begin{figure}[]
    \centering

    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/TopoFeature_SMGNN_graph0.pdf}
    }
    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/TopoFeature_SMGNN_graph1.pdf}
    }
    
    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/TopoFeature_SMGNN_graph2.pdf}
    }
    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/TopoFeature_SMGNN_graph8.pdf}
    }%
    
    \caption{\textbf{Examples of explanations for \SMGNN (seed $1$) over \TopoFeature}. 
    %
    Relevant edges are those with $p_{uv} \ge 0.8$ and are highlighted in red.
    %
    Edges are annotated with their respective $p_{uv}$ score.
    %
    Samples of class $1$ must have both a cycle and at least $2$ red nodes.
    }
    \label{fig:expl_examples_topofeature_SMGNN}
\end{figure}

\begin{figure}
    \centering
    \subfigure{\includegraphics[width=0.99\textwidth]{fig/explanations/TopoFeature_GLSMGNN_histogram.pdf}}
    
    \caption{\textbf{Histograms of explanation relevance scores for \GLSMGNN on \TopoFeature} (validation set). 
    %
    Since the underlying \SMGNN is now only looking for the topological motif (as the rule \textit{at least two red nodes} is learned by the interpretable model), the \GLSEGNN is allowed to sparsify all the other edges better, achieving more compact explanations.
    %
    For seed $1$, non-zero scores are clutter around $0.38$.
    }
    \label{fig:expl_histograms_glsmgnn_topofeature}
\end{figure}

\begin{figure}
    \centering
    \subfigure{\includegraphics[width=0.99\textwidth]{fig/explanations/TopoFeature_GLGSAT_histogram.pdf}}
    
    \caption{\textbf{Histograms of explanation relevance scores for \GLGSAT on \TopoFeature} (validation set).
    }
    \label{fig:expl_histograms_glgsat_topofeature}
\end{figure}

\begin{figure}[]
    \centering

    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/TopoFeature_GLSMGNN_graph0.pdf}
    }
    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/TopoFeature_GLSMGNN_graph1.pdf}
    }
    
    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/TopoFeature_GLSMGNN_graph2.pdf}
    }
    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/TopoFeature_GLSMGNN_graph8.pdf}
    }%
    
    \caption{\textbf{Examples of explanations for \GLSMGNN (seed $1$) over \TopoFeature}. 
    %
    Relevant edges are those with $p_{uv} \ge 0.2$ and are highlighted in red.
    %
    The threshold is picked by looking at the histogram in \cref{fig:expl_histograms_glsmgnn_topofeature}.
    %
    Edges are annotated with their respective $p_{uv}$ score.
    %
    Overall, \GLSMGNN achieves better sparsification than \SMGNN (\cref{fig:expl_examples_topofeature_SMGNN}).
    }
    \label{fig:expl_examples_topofeature_GLSMGNN}
\end{figure}




\paragraph{\BAColor.}
%
\cref{fig:expl_histograms_gsat_bacolor} and \cref{fig:expl_histograms_smgnn_bacolor} show the edge relevance scores for \GSAT and \SMGNN respectively.
%
Overall, the histograms show that both models fail to reliably identify a relevant subgraph with a consistently higher importance than irrelevant ones.
%
Examples of explanations for both models, plotted in \cref{fig:expl_examples_bacolor_GSAT} and \cref{fig:expl_examples_bacolor_SMGNN}, confirm that subgraph-based explanations fail to convey actionable insights into what the model is predicting, as no clear pattern emerges from explanations.
%
Conversely, both \GLGSAT and \GLSMGNN provide intelligible prediction by relying on a simple linear classifier encoding the ground truth rule \textit{number of red nodes $\ge$ blue nodes} (see \cref{tab:synt_exp} and \cref{fig:dec_bound_bacolor}).


\begin{figure}
    \centering
    \subfigure{\includegraphics[width=0.99\textwidth]{fig/explanations/BAColor_GSAT_histogram.pdf}}
    
    \caption{\textbf{Histograms of explanation relevance scores for \GSAT  on \BAColor} (validation set). 
    %
    The model fails to reliably separate between relevant and non-relevant edges, making it difficult to select a proper relevance threshold.
    %
    Specifically, most edges are assigned an importance close to $0.3$, which matches the uninformative prior $r$ selected during training.
    }
    \label{fig:expl_histograms_gsat_bacolor}
\end{figure}

\begin{figure}
    \centering
    \subfigure{\includegraphics[width=0.99\textwidth]{fig/explanations/BAColor_SMGNN_histogram.pdf}}
    
    \caption{\textbf{Histograms of explanation relevance scores for \SMGNN  on \BAColor} ((validation set)). 
    %
    The model assigns very cluttered scores to almost all edges, failing to highlight a subset that is reliably more relevant than the others, making it difficult to select an appropriate threshold for showing the explanations to consumers.
    }
    \label{fig:expl_histograms_smgnn_bacolor}
\end{figure}

\begin{figure}[]
    \centering

    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/BAColor_GSAT_graph3.pdf}
    }
    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/BAColor_GSAT_graph10.pdf}
    }
    
    \caption{\textbf{Examples of explanations for \GSAT (seed $1$) over \BAColor}. 
    %
    Relevant edges are those with $p_{uv} \ge 0.7$ and are highlighted in red.
    %
    Edges are not annotated with their respective $p_{uv}$ score to avoid excessive clutter.
    }
    \label{fig:expl_examples_bacolor_GSAT}
\end{figure}

\begin{figure}[]
    \centering

    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/BAColor_SMGNN_graph3.pdf}
    }
    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/BAColor_SMGNN_graph10.pdf}
    }
    
    \caption{\textbf{Examples of explanations for \SMGNN (seed $1$) over \BAColor}. 
    %
    Relevant edges are those with $p_{uv} \ge 0.2$ and are highlighted in red.
    %
    Edges are not annotated with their respective $p_{uv}$ score to avoid excessive clutter.
    }
    \label{fig:expl_examples_bacolor_SMGNN}
\end{figure}




\paragraph{\AIDS.}
%
Among each random seed, seed $8$ achieves a test F1 score of $1.0$, highlighting that the model is likely to have learned to just count the number of nodes in each graph, and to make the prediction based on such count \citep{pluska2024logical}.
%
This strategy is proven to be effective in this dataset, as highlighted in \cref{appx:aidsc1} and \citet{pluska2024logical}.
%
By plotting the histogram of explanatory scores in \cref{fig:expl_histograms_gsat_aids}, and some examples of explanations in \cref{fig:expl_examples_aids_GSAT}, we cannot 
unambiguously assess which rule the model is using for making predictions.
%
Conversely, as shown in \cref{appx:aidsc1}, \GLGSAT can achieve the same performances while declaring that only node count statistics are being used for prediction.



\begin{figure}
    \centering
    \subfigure{\includegraphics[width=0.99\textwidth]{fig/explanations/AIDS_GSAT_histogram.pdf}}
    
    \caption{\textbf{Histograms of explanation relevance scores for \GSAT  on \AIDS} (validation set).
    }
    \label{fig:expl_histograms_gsat_aids}
\end{figure}

\begin{figure}[]
    \centering

    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/AIDS_GSAT_graph4.pdf}%
    }
    \subfigure[]{%
        \includegraphics[width=0.45\textwidth]{fig/explanations/AIDS_GSAT_graph17.pdf}%
    }
    
    \caption{\textbf{Examples of explanations for \GSAT (seed $8$) over \AIDS}. 
    %
    Relevant edges are those with $p_{uv} \ge 0.8$ and are highlighted in red.
    %
    Edges are not annotated with their respective $p_{uv}$ score to avoid excessive clutter.
    }
    \label{fig:expl_examples_aids_GSAT}
\end{figure}

\begin{table}[t]
\centering
\footnotesize
\caption{
    \GLSEGNN's linear classifier weights for the \GLSMGNN experiment in \cref{tab:exp_AIDSC1}.
    %
    The $L_1$ sparsification effectively promotes the model to give considerably higher importance to the last input feature, corresponding to the number of nodes in the graph.
    %
    Seed 10 is omitted as the linear classifier is not in use.
}
\label{tab:exp_AIDSC1_weights}
\scalebox{0.75}{
    \begin{tabular}{lccc}
        \toprule
         \textbf{Model} & 
         \multicolumn{3}{c}{\textbf{\AIDSC}}\\
         \GLSMGNN's seed & Weights (W) & Bias (b) & - b / W[-1]\\
        
        \midrule

    seed 1
        & \begin{tabular}{ccccc}
            -6.1e-03 & -3.2e-02 & -9.5e-03 & -1.8e-02 & -3.8e-04\\
             7.8e-03 & -2.4e-04 & 7.0e-04 & 5.9e-04 & 1.4e-03\\
             3.9e-04 & 1.6e-02 & -4.4e-04 & 1.4e-03 & -6.9e-04 \\
             4.1e-05 & -2.6e-02 & -7.0e-04 & 3.1e-04 & -9.6e-04 \\
             -3.8e-04 & 8.9e-04 & 1.1e-04 & 3.7e-05 & -2.2e-04 \\
             -3.3e-04 & -3.3e-04 & 9.7e-04 & 4.3e-04 & 1.4e-03 \\
             -8.6e-05 & 3.4e-04 & 4.1e-04 & 3.6e-04 & 1.1e-03 \\
             -1.7e-03 & 1.7e-03 & 3.4e-04 & -2.9e-01
           \end{tabular} 
        & 3.64 & 12.45\\

\midrule

    seed 2
        & \begin{tabular}{ccccc}
            -2.0e-02 & -3.0e-02 & 8.0e-03 & 7.3e-03 & -7.3e-04\\
            2.7e-02 & -6.5e-04 & 2.0e-02 & -4.9e-05 & -9.4e-04\\
            7.2e-02 & -4.0e-04 & -1.4e-04 & -2.6e-04 & -3.4e-04\\
            1.1e-03 & 3.0e-04 & 8.4e-06 & 1.3e-04 & -9.4e-04\\
            -3.9e-05 & -9.3e-04 & -5.1e-04 & -1.4e-03 & 6.3e-04\\
            3.4e-04 & 1.3e-03 & 5.6e-04 & 1.2e-04 & -6.4e-04\\
            4.5e-04 & 3.8e-05 & 8.0e-05 & 1.6e-04 & -3.3e-04\\
            1.3e-04 & 7.1e-04 & 5.9e-04 & -2.9e-01
           \end{tabular} 
        & 3.63 & 12.57\\

\midrule

    seed 3
        & \begin{tabular}{ccccc}
            -1.2e-02 & -3.1e-02 & 3.2e-04 & 2.4e-04 & 3.6e-04\\
            2.4e-03 & -1.4e-04 & -4.4e-04 & 1.5e-06 & 1.8e-05\\
            6.1e-05 & 2.4e-04 & 6.5e-04 & -1.2e-04 & -2.3e-04\\
            -6.4e-04 & -3.1e-04 & 1.0e-03 & -2.8e-03 & -1.2e-03\\
            -3.9e-05 & 1.1e-03 & -5.2e-04 & -6.0e-04 & -1.1e-03\\
            -4.1e-05 & 2.0e-04 & -3.6e-04 & -9.2e-04 & -7.6e-04\\
            -3.4e-05 & -1.3e-04 & -8.7e-04 & -8.2e-04 & -1.4e-03\\
            5.4e-04 & 6.6e-04 & -3.3e-04 & -2.7e-01
           \end{tabular} 
        & 3.38 & 12.45\\

\midrule

    seed 4
        & \begin{tabular}{ccccc}
            -9.7e-03 & -1.6e-02 & -7.9e-03 & -2.0e-02 & 8.0e-04\\
            4.8e-03 & -5.9e-04 & 6.0e-05 & 3.9e-04 & -1.1e-03\\
            -3.6e-04 & 6.3e-04 & -1.7e-03 & 3.2e-04 & -1.0e-03\\
            4.4e-04 & -3.4e-02 & 7.9e-04 & -8.5e-04 & -6.0e-04\\
            4.7e-04 & -2.8e-04 & -9.4e-04 & -5.9e-04 & -1.0e-03\\
            1.2e-05 & -7.1e-05 & -5.1e-05 & 9.3e-04 & 1.3e-03\\
            1.5e-04 & -1.3e-04 & -4.9e-04 & 5.1e-04 & 7.9e-04\\
            2.5e-05 & -4.0e-04 & -4.4e-04 & -2.8e-01
           \end{tabular} 
        & 3.42 & 12.04\\

\midrule

    seed 5
        & \begin{tabular}{ccccc}
            -1.7e-01 & -2.0e-01 & -1.3e-01 & 1.2e-04 & -8.6e-04\\
            -1.2e-03 & 2.1e-04 & -2.0e-04 & -1.4e-03 & -5.7e-04\\
            1.3e-04 & -5.2e-04 & -3.5e-04 & -9.1e-04 & 4.6e-04\\
            -6.7e-04 & 3.6e-04 & 1.0e-03 & -6.8e-04 & -4.0e-04\\
            -4.7e-04 & -9.2e-04 & 8.1e-04 & -5.2e-04 & -1.9e-06\\
            -2.4e-04 & -5.2e-04 & 2.0e-04 & 4.5e-04 & -1.3e-03\\
            5.5e-04 & 2.5e-04 & -5.5e-04 & 6.2e-04 & -1.0e-03\\
            4.2e-04 & 2.1e-04 & 8.9e-04 & -2.0e-01
           \end{tabular} 
        & 4.48 & 22.18\\

\midrule

    seed 6
        & \begin{tabular}{ccccc}
            -1.0e-02 & -6.9e-02 & -2.0e-03 & -1.3e-02 & -4.7e-03\\
            1.5e-02 & -1.1e-03 & 5.1e-04 & 8.4e-04 & 7.4e-04\\
            -8.2e-04 & -3.4e-04 & -1.3e-03 & 6.7e-04 & -8.1e-07\\
            1.3e-03 & -1.7e-02 & 6.9e-04 & 2.7e-04 & 8.7e-04\\
            -8.5e-05 & 3.1e-04 & -3.1e-04 & -5.3e-04 & -9.0e-04\\
            -4.1e-04 & 1.2e-03 & 5.8e-04 & -4.2e-04 & 1.6e-03\\
            -5.1e-04 & 9.4e-04 & 4.4e-04 & 8.4e-04 & 6.4e-05\\
            2.4e-04 & -1.7e-04 & -9.2e-04 & -2.6e-01
           \end{tabular} 
        & 3.90 & 15.07\\

\midrule
        
    seed 7
        & \begin{tabular}{ccccc}
            -1.8e-02 & -2.7e-02 & -5.5e-03 & -1.6e-02 & 4.9e-04\\
            -4.3e-03 & -8.5e-04 & 1.6e-04 & -8.3e-04 & 9.1e-04\\
            -5.5e-04 & -1.1e-03 & -6.8e-04 & -1.6e-04 & -1.4e-03\\
            9.0e-04 & -3.5e-02 & 2.6e-04 & 1.3e-03 & 3.0e-04\\
            -5.3e-05 & -1.0e-03 & -1.0e-03 & -4.5e-04 & 7.9e-04\\
            6.3e-04 & 1.3e-03 & -5.1e-04 & 8.9e-04 & -4.7e-04\\
            2.0e-03 & 1.0e-03 & -3.9e-04 & -1.0e-03 & -1.0e-04\\
            4.6e-04 & 1.0e-03 & -6.5e-04 & -2.9e-01
           \end{tabular} 
        & 3.63 & 12.32\\

\midrule

    seed 8
        & \begin{tabular}{ccccc}
            -1.0e-02 & -2.2e-02 & 2.0e-03 & -1.6e-02 & 3.1e-04\\
            5.0e-04 & -1.2e-03 & -3.9e-04 & 7.8e-04 & -5.8e-04\\
            9.9e-05 & -8.4e-04 & 7.6e-04 & -3.9e-04 & 2.0e-04\\
            7.2e-04 & -2.3e-02 & -5.3e-04 & -4.5e-04 & 7.0e-04\\
            5.9e-04 & -1.1e-04 & 6.5e-04 & -6.3e-04 & 7.9e-06\\
            -5.8e-04 & -3.3e-04 & -5.5e-04 & -1.2e-03 & -9.8e-05\\
            -7.0e-05 & 3.1e-04 & -3.4e-04 & -2.0e-03 & 7.8e-05\\
            -3.5e-04 & -8.5e-04 & 3.3e-05 & -2.9e-01
           \end{tabular} 
        & 3.47 & 12.18\\

\midrule

    seed 9
        & \begin{tabular}{ccccc}
            -1.4e-02 & -3.1e-02 & 5.7e-03 & -2.1e-02 & -2.2e-04\\
            8.5e-03 & 3.0e-05 & 1.3e-04 & -6.1e-04 & 5.7e-04\\
            1.2e-04 & -2.5e-03 & -1.3e-03 & -1.8e-04 & 1.7e-03\\
            2.5e-04 & -2.7e-02 & 6.9e-04 & 2.5e-04 & 1.1e-03\\
            -5.1e-04 & -9.9e-04 & 1.5e-04 & 1.1e-04 & 2.4e-04\\
            -9.5e-04 & -8.2e-04 & 1.6e-04 & -3.8e-04 & -6.3e-04\\
            -5.5e-04 & 1.7e-03 & 1.0e-03 & -1.1e-04 & 6.4e-04\\
            7.1e-04 & -1.9e-04 & -4.5e-04 & -2.6e-01
           \end{tabular} 
        & 3.59 & 13.73\\  
    \end{tabular}
}
\end{table}


\end{document}