\section{Further Analyses}

This section presents a detailed analysis of VLM performance in the geolocation task, the hypotheses proposed to explain them, and preliminary experiments conducted to verify.

\subsection{Is There Data Leakage?}

\paragraph{Newer Version of Images}

Given the exceptional performance of VLMs, one might hypothesize that Google Street View images are included in their training data, leading to potential memorization of answers.
To investigate this, we supplement the 2019 version of Google Street View images used in the main experiments with a newer version from 2024 and an older version from 2014.
The 2024 images are not included in the training data of GPT-4o and Gemini-1.5-Pro, as their release dates postdate those of the models.
The inclusion of 2014 images aims to examine whether VLMs can recognize older views.
To minimize regional variability, we focused on identical locations across different temporal versions.
Given the limited availability of some versions in certain regions, we select three U.S. cities—Denver, Las Vegas, and New York—for this study.
For each city, we identify 10 locations, many of which exhibit changes over the selected timeframes, resulting in a total of 90 images.
Results show that, in terms of city-level accuracy, the 2019 images perform the best (84.6\%), followed by the 2024 images (82.5\%), with the 2014 images performing the worst (79.2\%).
These findings suggest that training data influence accuracy, though the effect is relatively small in the context of these U.S. cases.
% At the city level, GPT-4o achieved an accuracy of 79.1\% for the 2014 images, 89.1\% for the 2019 images, and 86.7\% for the 2024 images. In contrast, Gemini attained accuracies of 79.2\% for the 2014 images, 80.0\% for the 2019 images, and 78.3\% for the 2024 images.

\paragraph{Identifying User-Uploaded Images}

In addition to utilizing the latest version of Google Street View images, we incorporate images captured by the authors, ensuring that none have previously been published online.\footnote{All image providers (authors) have granted consent for the use of these images in this research and their publication in an open repository.}
The data include six cities worldwide: Bangkok, Chicago, Los Angeles, Mexico City, Shanghai, and Sydney, with 10 images collected per city.
We evaluate the accuracy of the VLMs using these user-provided images in comparison with Google Street View images from the same cities.
The results, presented in Table~\ref{tab:user-photo}, indicate that the VLM achieves higher accuracy on user-provided images, particularly for those from Shanghai.
This may be attributed to the broader field of view and richer contextual information in user-provided images compared to Google Street View.
This finding also highlights a potential privacy concern, as the VLMs could be used to identify locational information from user-uploaded images on the Internet.

\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        \bf Data & \bf Bangkok & \bf Chicago & \bf LA & \bf MC & \bf Shanghai & \bf Sydney \\
        \midrule
        \multicolumn{7}{c}{\bf GPT-4o} \\
        \hdashline
        Google & 63.3 & 73.3 & 76.7 & 73.3 & 36.7 & 90.0 \\
        User & 100.0 & 100.0 & 90.7 & 66.7 & 93.3 & 76.7 \\
        \midrule
        \multicolumn{7}{c}{\bf Gemini-1.5-Pro} \\
        \hdashline
        Google & 83.3 & 93.3 & 60.0 & 80.0 & 23.3 & 73.3 \\
        User & 100.0 & 100.0 & 70.7 & 47.6 & 70.0 & 73.3 \\
        \bottomrule
    \end{tabular}
    }
    \caption{City-level accuracy of GPT-4o and Gemini on Google Street View images and user-uploaded images. ``LA'' is Los Angeles while ``MC'' is Mexico City.}
    \label{tab:user-photo}
\end{table}

\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        \bf Model & \bf Bangkok & \bf Joburg & \bf Lima & \bf London & \bf NYC & \bf Sydney \\
        \midrule
        \bf GPT-4o & 90.0 & 56.7 & 96.7 & 86.7 & 100.0 & 100.0 \\
        \bf Gemini & 73.3 & 66.7 & 90.0 & 96.7 & 100.0 & 76.7 \\
        \bottomrule
    \end{tabular}
    }
    \caption{City-level accuracy of GPT-4o and Gemini on the Chinatown views. ``NYC'' is New York City. ``Joburg'' is Johannesburg.}
    \label{tab:chinatown}
\end{table}

\subsection{Is There Spurious Correlation?}

\paragraph{Specific Features}

Another hypothesis posits that VLMs may exploit superficial correlations in images to infer locations.
To examine the relationship between distinctive features and ground truths, we focus on Chinatowns across different cities, which share common visual elements such as Chinese characters and cultural decorations (\eg, red lanterns and Fai Chun).
For this experiment, one Chinatown is selected from each continent, with ten images sampled from each: Bangkok, Johannesburg, Lima, London, New York, and Sydney, all featuring established Chinatowns with significant Chinese communities.
Results from GPT-4o and Gemini-1.5-Pro, summarized in Table~\ref{tab:chinatown}, demonstrate strong performance by VLMs in identifying these Chinatown scenes.
This finding suggests that VLMs do not exclusively rely on obvious cues linking images to China but also leverage other nuanced features.

\paragraph{Style of City Views}

\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lp{8.6cm}}
    \toprule
    \rowcolor{mygray}
    \multicolumn{2}{l}{\textbf{CoT Example (1)}} \\
    \textsc{Image} & \includegraphics[width=0.5\linewidth]{Figures/cot-example-1.jpg} \\
    \midrule
    \textsc{Ground Truth} & Kano, Nigeria, Africa \\
    \textsc{Model} & Gemini-1.5-Pro \\
    \textsc{Analysis} & The image appears to be taken in a tropical or subtropical region due to the vegetation and sun angle. The architecture suggests a possibly African context. The road appears to be well-maintained. No license plates or signage are clearly visible to provide definitive clues. \\
    \textsc{Prediction} & Kano, Nigeria, Africa \\
    \bottomrule
    \end{tabular}
    }
\end{table}

In addition to specific features, we examine how the overall style of images influences predictions.
Specifically, we investigate whether VLMs exhibit biases, such as favoring developed cities for urban, modern street scenes and developing cities for rural, undeveloped environments.
For instance, as shown in Fig.~\ref{fig:cover}(c), GPT-4o predicts urban scenes from Cape Town, South Africa, as San Diego, USA, and Nice, France.
Conversely, for more rural images, Gemini-1.5-Pro misidentifies Moscow, Russia, as Kharkiv, Ukraine, and Madrid, Spain, as Seville, Spain.
Similarly, LLaMA demonstrates comparable errors: a clean, organized street scene from Bras\'ilia, Brazil, is predicted as Sydney, Australia, and a high-rise cityscape from Krasnoyarsk, Russia, is identified as New York, USA.
These findings reveal potential regional biases in VLMs when interpreting urban environments.

\subsection{Can CoT Help?}

To evaluate the performance of VLMs, we analyze their outputs using Chain-of-Thought (CoT)~\cite{kojima2022large, wei2022chain} prompts.
We present two example queries: one for Gemini and another for LLaMA.
The case study suggests that while CoT reasoning can appear logical, it is not consistently tied to the final answer.
In CoT Example (1), Gemini correctly identifies Africa’s surroundings but notes the absence of visible license plates or signs that could aid in further country or city analysis.
Despite this lack of evidence, the model still predicts the correct answer.
Conversely, in CoT Example (2), LLaMA identifies features typical of California but incorrectly predicts Santa Barbara instead of the correct answer, Los Angeles.
Across multiple examples, the elements cited in the CoT reasoning process often partially align with the final answer.
However, these elements are typically broad and fail to accurately pinpoint specific locations.
Relying solely on the reasoning process makes it challenging to determine the exact geographical location of an image.
We hypothesize that the model's responses are not derived from genuine reasoning based on image information but are instead influenced by its prior knowledge of geographical locations.

\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lp{8.6cm}}
    \toprule
    \rowcolor{mygray}
    \multicolumn{2}{l}{\textbf{CoT Example (2)}} \\
    \textsc{Image} & \includegraphics[width=0.5\linewidth]{Figures/cot-example-2.jpg} \\
    \midrule
    \textsc{Ground Truth} & Los Angeles, USA, North America \\
    \textsc{Model} & LLaMA-3.2-11B-Vision \\
    \textsc{Analysis} & The image shows a woman standing on a dirt path in a wooded area, holding a cell phone and wearing a pink t-shirt, shorts, and a white visor. She is looking out at a view of mountains and valleys in the distance. \\
    & *   The woman is wearing a pink t-shirt, shorts, and a white visor. \\
    & *   She is holding a cell phone in her right hand. \\
    & *   She is standing on a dirt path in a wooded area. \\
    & *   There are trees and bushes surrounding her. \\
    & *   In the background, there is a view of mountains and valleys. \\
    & *   The sky is blue with some clouds. \\
    & Based on the analysis, the street view appears to be located in a rural or natural area, possibly in a national park or nature reserve. The presence of mountains and valleys in the background suggests that the location is in a hilly or mountainous region. \\
    \textsc{Prediction} & Santa Barbara, USA, North America \\
    \bottomrule
    \end{tabular}
    }
\end{table}