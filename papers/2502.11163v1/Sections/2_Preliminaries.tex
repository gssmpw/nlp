\section{Related Work}

\subsection{Geo-Information with AI Models}

Recent advancements in geographical information processing have leveraged Large Language Models (LLMs) and VLMs to improve geolocation tasks.
Geo-seq2seq~\cite{zhang2023geo} and \citet{hu2023geo} develop models for extracting geographical information from social media, focusing on non-English texts and disaster-related content, respectively.
GPTGeoChat~\cite{mendes2024granular} fine-tunes VLMs or queries them with tailored prompts to responsibly disclose geographical information.
GPT4GEO~\cite{roberts2023gpt4geo} and \citet{bhandari2023large} explore LLMs' geographical knowledge, reasoning abilities, and spatial awareness.
K2~\cite{deng2024k2} fine-tunes LLMs for Earth Sciences applications.
GeoLM~\cite{li2023geolm} links textual data with spatial information from geographical databases for reasoning, while GeoLLM~\cite{manvi2024geollm} integrates OpenStreetMap data to improve geospatial prediction accuracy and scalability.
GeoLocator~\cite{yang2024geolocator} uses GPT-4 to infer location information from images and social media, highlighting geographical privacy risks.
PIGEON~\cite{haas2024pigeon} generalizes geolocation to unseen areas, and ETHAN~\cite{liu2024image} enhances image geolocation using LVLMs and contextual cues.
\citet{wazzan2024comparing} compare LLM-based search engines to traditional ones in image geolocation tasks.
While these works demonstrate significant progress in geolocation and spatial reasoning, they do not address biases in the geolocating ability of VLMs.

\subsection{Biases in AI Models}

Research has extensively documented biases in VLMs and text-to-image (T2I) models.
\citet{fraser2024examining} and \citet{ghosh2023person} analyze racial, gender, and national identity biases in AI-generated images, while \citet{wang2024new}, \citet{nakashima2023societal}, and BIGbench~\cite{luo2024bigbench} focus on gender, occupational biases, and debiasing techniques in T2I models.
Social biases in embedding spaces are explored by \citet{brinkmann2023multidimensional} and \citet{ross2021measuring}, who show that joint embeddings also exhibit biases.
\citet{zhang2022counterfactually}, \citet{srinivasan2022worst}, and \citet{ruggeri2023multi} use counterfactuals, masked prediction, and VQA to investigate gender and multi-dimensional biases.
BiasDora~\cite{raj2024biasdora} and \citet{sathe2024unified} analyze gender and professional biases across modalities, proposing metrics and frameworks for evaluation, while VisoGender~\cite{hall2023visogender} provides datasets for pronoun resolution and retrieval tasks. \citet{wolfe2023contrastive} reveal biases in emotional state perception and sexualized associations, and \citet{wolfe2022american} find a tendency for VLMs to associate whiteness with American identity. \citet{wan2023biasasker}, \citet{zhao2024gender} and \citet{du2025faircode} study gender and racial biases, while \citet{wan2024male} and \citet{huang2025fact} focus on gender biases in occupational contexts.
However, these studies do not address biases stemming from models' geolocation abilities.