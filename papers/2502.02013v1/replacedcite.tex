\section{Related Work}
\label{sec:related}

\paragraph{Understanding Neural Representations.}


A long line of research has aimed to understand \emph{how} deep neural networks encode and organize information. Early studies employed linear probes to interpret intermediate layers ____, while subsequent efforts introduced more sophisticated techniques such as SVCCA ____ to compare learned features across architectures and training regimes. Although these approaches shed light on representation dynamics, most focus on vision backbones or relatively shallow models. In contrast, our work extends layer-wise analysis to \emph{large-scale} language models, highlighting specific behaviors of intermediate layers in autoregressive Transformers, state-space models (SSMs), and beyond.



%Research on neural network representations has evolved from simple probing studies to sophisticated analysis frameworks. Early work by ____ introduced linear probes to analyze hidden representations, establishing foundational techniques for interpretation. Subsequent work like ____ developed more sophisticated tools such as Singular Vector Canonical Correlation Analysis (SVCCA) to compare representations across networks and layers. These methods revealed how networks progressively transform information but focused primarily on vision models and simple architectures.


\paragraph{Layer-wise Analysis in Language Models.}

Transformer-based LLMs have sparked significant interest in \emph{which} layers capture linguistic properties such as syntax and semantics ____. More recent work ____ has shown that mid-depth layers sometimes hold surprisingly robust features, challenging the typical focus on final layers. Our contribution unifies and expands these observations through a large-scale, theoretical--empirical framework that quantifies the quality of \emph{every} layer’s representation via information theory, geometry, and invariance metrics.



%The emergence of transformer-based language models prompted new investigations into layer-wise behavior. ____ demonstrated that lower layers encode primarily syntactic information while higher layers capture semantics. Recent work by ____ revealed that semantic concepts form primarily in intermediate layers, challenging the assumption that deeper layers necessarily capture more sophisticated features. Similarly, ____ found that certain spatial and temporal concepts are best represented in middle layers. Our work extends these findings by providing a systematic framework to quantify and explain these phenomena.


\paragraph{Architectural Comparisons.}
%While transformers have been extensively studied, newer architectures like those of  state space models (SSMs) remain less understood. ____ introduced the Mamba architecture, demonstrating competitive performance with more efficient sequence processing. However, comparative analyses of how different architectures organize information internally remain scarce. Our work provides a systematic comparisons between transformers and SSMs at the representational level.

Transformers remain the dominant architecture for NLP ____, but they come in multiple variants. Encoder-only models (e.g., BERT ____) typically use bidirectional attention and masked-language objectives, while decoder-only architectures (e.g., GPT ____) follow an autoregressive paradigm. Meanwhile, newer state-space models (SSMs) such as Mamba ____ use recurrent-style dynamics for efficient long-sequence processing. Although these designs differ significantly in attention mechanisms and sequence modeling strategies, there has been little direct comparison of \emph{hidden-layer} representations across them. In our work, we analyze Transformers (both encoder- and decoder-only) and SSMs under a common set of metrics, highlighting contrasts in how intermediate layers compress or preserve information and showing that intermediate-layer representations can excel across multiple architectures.






%Understanding representations in neural networks has been a topic of extensive research. ____ analyzed hidden representations to interpret neural networks' learning processes. ____ introduced Singular Vector Canonical Correlation Analysis (SVCCA) to compare representations across layers and networks, providing insights into learning dynamics. 
%In the context of Transformers, ____ studied the linguistic knowledge captured at different layers, finding that lower layers encode more syntactic information while higher layers capture semantic features. Similarly, ____ showed that semantic concepts are learned in intermediate layers and proposed a layer-wise probing technique to identify the specific layers where these concepts are formed. On the other hand, state-space models have been less explored in this regard. ____ introduced Mamba, an SSM architecture capable of handling long sequences efficiently. However, comparative studies between SSMs and Transformers at the representation level remain scarce. \dz{Dan: do we need to mention any works on distillation? some works have shown that you can throw away some layers of LLMs with minimal impact on performance}

\paragraph{Representation Quality Metrics.}


A variety of metrics have been proposed to quantify the ``quality'' of learned representations. We group them into three main categories:

\begin{itemize}[itemsep=1pt, topsep=0pt]
    \item \textbf{Information-theoretic measures} capture how much a model's internal representations compress or preserve relevant information. For example, the Information Bottleneck ____ analyzes whether intermediate layers discard noise while retaining essential features.
    
    \item \textbf{Geometric measures} focus on the structure of embeddings in high-dimensional space. Classical approaches include analyzing singular values or effective rank of the representation matrix ____, while more recent work explores curvature ____ to quantify how smoothly tokens are mapped across consecutive positions or time steps.
    
    \item \textbf{Task-based or invariance metrics} evaluate how well representations support downstream goals. For instance, augmentations-based approaches such as InfoNCE ____ and LiDAR ____ estimate invariance to perturbations, while methods like NESum or Self-Cluster ____ link closely to entropy. In computer vision, these scores often correlate strongly with downstream accuracy, highlighting how robust the embeddings are.
\end{itemize}

%Metrics like entropy and curvature have been used in other contexts to analyze representations. ____ discussed the Information Bottleneck principle, suggesting that networks learn to compress representations. ____ introduced curvature as a measure of representational dynamics in recurrent networks. Several works in the vision domain have proposed unsupervised representation quality metrics that are strongly correlated with accuracy on downstream tasks____. Notably, the RankMe measure from ____ can be shown to be a measure of entropy known as matrix-based entropy, which we use in our analysis.


Although these categories may appear distinct, we will show (Section~\ref{sec:framework}) that many can be unified under a single lens. This unification illuminates \emph{why} certain intermediate layers balance compression, geometry, and invariance so effectively, leading to better representations for downstream tasks.

\paragraph{Compression and Generalization.}

Multiple lines of research connect compression and generalization performance____. For instance, ____ demonstrated that discarding certain layers in self-supervised encoders can even \emph{improve} downstream accuracy, while ____ found that LLM embeddings often lie in low-dimensional manifolds. Our empirical study reinforces these ideas by demonstrating that many networks—especially autoregressive Transformers—naturally develop a mid-layer bottleneck that appears crucial for balancing “signal” versus “noise.” We show how intermediate layers can achieve optimal trade-offs between preserving task-relevant information and discarding superfluous detail.



%Recent theoretical work suggests connections between representation compression and generalization. ____ showed that removing layers can improve generalization in self-supervised learning, while ____ demonstrated that LLM representations often lie in low-dimensional subspaces. Our work provides empirical support for these theories by showing how intermediate layers achieve optimal trade-offs between compression and task performance.

Overall, our work bridges these overlapping threads by evaluating a range of architectures and training paradigms via a unified set of metrics. Beyond merely confirming that intermediate layers can be effective, we elucidate \emph{why} this happens, tying it to fundamental properties such as entropy, invariance, and geometry. This novel perspective provides an avenue for both finer-grained diagnostics of large language models and more deliberate design of mid-layer representations for downstream tasks.



%Our research bridges these various threads by providing a comprehensive framework for analyzing representations across different architectures, training regimes, and input conditions. Unlike previous work that focused on specific aspects or architectures, we offer a unified approach that reveals universal patterns in how neural networks organize information.


%Our work bridges these areas by applying and adapting such metrics to LLMs, providing a novel perspective on representation quality across architectures and training stages.


%Let $\mathbf{Z} \in \mathbb{R}^{N \times D}$ represent a batch of $N$ samples, each with dimensionality $D$. The vector $z_i$ denotes the $i$-th row of $Z$. We denote the $i$-th largest eigenvalue of a matrix $\mathbf{M}$ as $\lambda_i(\mathbf{M})$, and the trace of $\mathbf{M}$ by $\operatorname{tr}(\mathbf{M})$. Input sequences are denoted by $\mathbf{x} \in \mathbb{R}^{L \times d}$ and output sequences by $\mathbf{y} \in \mathbb{R}^{L \times d}$, where $L$ is the sequence length and $d$ is the feature dimension.