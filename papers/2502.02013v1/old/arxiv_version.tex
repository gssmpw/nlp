\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[preprint]{styles/neurips/neurips_2024}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{makecell}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\newcommand{\ravid}[2]{\textcolor{blue}{Ravid: #1}}

\newcommand\extrafootertext[1]{%
    \bgroup
    \renewcommand\thefootnote{\fnsymbol{footnote}}%
    \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
    \footnotetext[0]{#1}%
    \egroup
}

\newcommand{\SubItem}[1]{
    {\setlength\itemindent{15pt} \item[-] #1}
}


\title{Does Representation Matter? Exploring Intermediate Layers in Large Language Models}

\author{Oscar Skean$^{1*}$ \quad Md Rifat Arefin$^2$
\quad Yann LeCun$^{3, 4}$
\quad Ravid Shwartz-Ziv$^{3, 5}$ \\
  $^1$University of Kentucky  \quad $^2$Mila \quad   $^3$New York University \quad  
  $^4$Meta FAIR \quad  
  $^5$Wand.AI
}

\begin{document}
\maketitle


\begin{abstract}

Understanding what defines a ``good'' representation in large language models (LLMs) is fundamental to both theoretical understanding and practical applications. In the computer vision domain, representation evaluation metrics are well understood to correlate with downstream performance. We adapt and apply a suite of metrics- such as prompt entropy, curvature, and augmentation-invariance- and show significant differences in the behavior of these metrics between vision models and LLMs. Most notably, we find that intermediate layers often yield more informative representations for downstream tasks than the final layers. We also show 

Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer. Overall, our results illuminate the internal mechanics of LLMs and guide strategies for architectural optimization and training.


% Understanding what defines a ``good'' representation in large language models (LLMs) is fundamental to both theoretical understanding and practical applications. In this paper, we investigate the quality of intermediate representations in various LLM architectures, including Transformers and State Space Models (SSMs). We find that intermediate layers often yield more informative representations for downstream tasks than the final layers. To measure the representation quality, we adapt and apply a suite of metrics—such as prompt entropy, curvature, and augmentation-invariance—originally proposed in other contexts. Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer. Notably, we observe a bimodal pattern in the entropy of some intermediate layers and consider potential explanations tied to training data. Overall, our results illuminate the internal mechanics of LLMs and guide strategies for architectural optimization and training.



%Understanding what constitutes a ``good'' representation in large language models (LLMs) is a fundamental question in natural language processing. In this paper, we investigate the quality of representations at different layers of LLMs, focusing on Transformers and State Space Models (SSMs). Our findings indicate that intermediate layers consistently yield better representations for downstream tasks compared to final layers. To quantify representation quality, we adapt existing metrics from other contexts ---such as prompt entropy, curvature, and augmentation-invariance--to LLMs. Our experiments reveal significant differences between architectures, showcase how representations evolve during training, and illustrate the impact of input randomness and prompt length on different layers. Notably, we observe a bimodal behavior in entropy within intermediate layers and explore potential causes related to training data exposure. Our findings offer valuable insights into the internal workings of LLMs and offer guidance for optimizing their architectures and training processes.

\end{abstract}

\section{Introduction}
\label{sec:intro}

\extrafootertext{  *  correspondence to oscar.skean@uky.edu}



Large Language Models (LLMs) have revolutionized natural language processing by achieving remarkable performance across a wide range of tasks~\citep{muennighoff2022mteb, hendrycks2020mmlu}. Despite their success, understanding what constitutes a ``good'' representation within these models remains an open question. Specifically, how do representations at different layers contribute to downstream task performance, and how can we quantify their quality?


However, most previous studies have focused primarily on final-layer representations, often overlooking the potential of intermediate layers. Recent work suggests that intermediate layers may offer richer or more generalizable features for certain tasks~\citep{bordes2022guillotine, gurnee2023language, fan2024notalllayers}. These observations prompt a deeper investigation into the layer-wise behavior of LLMs.


In this paper, we explore the quality of representations across different layers of LLMs in various settings, including different model architectures (Transformers~\citep{vaswani2017attention} vs.\ State Space Models (SSMs)~\citep{mamba}), training checkpoints, input randomness, and prompt length. Our main contributions are:


\begin{itemize}
    \item We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.
    \item We apply and adapt existing metrics—such as prompt entropy, curvature, and augmentation-invariance metrics—to quantify representation quality in LLMs.
    \item We analyze how these metrics vary across different settings, including architectural differences, training progression, input randomness, and prompt length.
\end{itemize}



Furthermore, we uncover significant differences in the behavior of these metrics between Transformers and SSMs. Notably, we observe a bimodal distribution in entropy within intermediate layers and investigate potential causes, such as the influence of training data examples.


Ultimately, our findings provide a deeper understanding of how internal representations develop in LLMs and offer practical guidance for model optimization. By illuminating the intricacies of intermediate layers, we pave the way for improved architectures, better training strategies, and more efficient utilization of LLM representations.





\begin{table}[!t]
\centering
\caption{MTEB Downstream Task Performance Using Representations from Different Layers}
\label{tab:downstream_performance}
\scalebox{0.6}{
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{\makecell{Number of Tasks where Best Performance \\ is not in Last Layer}} & \textbf{Avg. Last Layer Performance} &  \textbf{Avg. Best Layer Performance}\\
\midrule
LLM2Vec 8B (Transformer) & 100\% & 64.7\% & 66.8\%\\
Pythia 410M (Transformer) & 96.6\% & 49.8\% & 53.3\% \\
Mamba 130M (SSM) & 100\% & 46.9\% & 50.9\% \\
\bottomrule
\end{tabular}
}
\end{table}


\section{Related Work}
\label{sec:related}

Understanding representations in neural networks has been a topic of extensive research. \citet{alain2016understanding} analyzed hidden representations to interpret neural networks' learning processes. \citet{raghu2017svcca} introduced Singular Vector Canonical Correlation Analysis (SVCCA) to compare representations across layers and networks, providing insights into learning dynamics. In the context of Transformers, \citet{liu2019linguistic} studied the linguistic knowledge captured at different layers, finding that lower layers encode more syntactic information while higher layers capture semantic features. Similarly, \citet{jin2024conceptdepth} showed that semantic concepts are learned in intermediate layers and proposed a layer-wise probing technique to identify the specific layers where these concepts are formed. On the other hand, state-space models have been less explored in this regard. \citet{mamba} introduced Mamba, an SSM architecture capable of handling long sequences efficiently. However, comparative studies between SSMs and Transformers at the representation level remain scarce.


Metrics like entropy and curvature have been used in other contexts to analyze representations. \citet{shwartz2017opening, shwartz2022information} discussed the Information Bottleneck principle, suggesting that networks learn to compress representations. \citet{hosseini2024curvature} introduced curvature as a measure of representational dynamics in recurrent networks. Several works in the vision domain have proposed unsupervised representation quality metrics that are strongly correlated with accuracy on downstream tasks~\citep{garrido2023rankme, agrawal2022alphareq, thilak2023lidar}. Notably, the RankMe measure from \citet{garrido2023rankme} can be shown to be a measure of entropy known as matrix-based entropy, which we use in our analysis.



Our work bridges these areas by applying and adapting such metrics to LLMs, providing a novel perspective on representation quality across architectures and training stages.

\section{Methodology}
\label{sec:methodology}

\subsection{Notation}
We consider a batch of $N$ samples, each represented by a $D$-dimensional vector. Let $\mathbf{Z} \in \mathbb{R}^{N \times D}$ be the matrix of representations, where $z_i$ denotes the $i$-th row of $\mathbf{Z}$. For a matrix $\mathbf{M}$, we use $\lambda_i(\mathbf{M})$ to denote its $i$-th largest eigenvalue, and $\operatorname{tr}(\mathbf{M})$ to denote its trace. When dealing with sequences, we let $\mathbf{x} \in \mathbb{R}^{L \times d}$ represent the input sequence and $\mathbf{y} \in \mathbb{R}^{L \times d}$ the output sequence, where $L$ is the sequence length and $d$ is the feature dimension.



%Let $\mathbf{Z} \in \mathbb{R}^{N \times D}$ represent a batch of $N$ samples, each with dimensionality $D$. The vector $z_i$ denotes the $i$-th row of $Z$. We denote the $i$-th largest eigenvalue of a matrix $\mathbf{M}$ as $\lambda_i(\mathbf{M})$, and the trace of $\mathbf{M}$ by $\operatorname{tr}(\mathbf{M})$. Input sequences are denoted by $\mathbf{x} \in \mathbb{R}^{L \times d}$ and output sequences by $\mathbf{y} \in \mathbb{R}^{L \times d}$, where $L$ is the sequence length and $d$ is the feature dimension. 

\subsection{Architectures}
\label{sec:architectures}

We compare two main types of architectures: Transformer-based models \citep{vaswani2017attention} and State Space Models (SSMs) \citep{mamba}.

\textbf{Transformers:} Transformers use self-attention layers to capture long-range dependencies within the input. By computing attention weights between tokens, they can integrate global context at every layer and scale effectively to large inputs.

\textbf{State Space Models (SSMs):} SSMs represent sequence processing using linear state transitions combined with gating mechanisms. They offer efficient handling of long sequences with linear time and memory complexity, making them a promising alternative to Transformers.

For further details on each architecture and their configurations, see Appendix~\ref{appendix:architectures}.





%In this study, we compare two prominent architectures: Transformer-based models \citep{vaswani2017attention} and State Space Models (SSMs) \citep{mamba}. Transformers utilize self-attention mechanisms to capture long-range dependencies within input sequences, enabling parallel processing and effective encoding of complex patterns. On the other hand, SSMs employ recurrent dynamics to handle sequential information with linear time and memory complexity, offering efficiency in processing longer sequences. Despite their differing approaches, both architectures aim to generate rich and meaningful representations across multiple layers. For detailed mathematical formulations and parameter configurations of each architecture, please refer to Appendix \ref{appendix:architectures}.




%We define $\mathbf{x} \in \mathbb{R}^{L \times d}$ and $\mathbf{y} \in \mathbb{R}^{L \times d}$ are the input and output sequences, respectively.

%Below we describe the building blocks of Transformer and State Space Models (SSMs).


\subsection{Representation Evaluation Metrics}
\label{sec:metrics}

We use two categories of metrics to evaluate representation quality: token embedding diversity metrics and augmentation-invariance metrics.


%To quantify the quality of representations across layers, we employ two categories of metrics: token embedding diversity metrics and augmentation-invariance metrics.

% Token embedding diversity metrics evaluate the variability and richness of the representations at the token level within a single sequence. We employ prompt entropy~\citep{wei2024large} and curvature~\citep{hosseini2024curvature}. Of particular interest is the prompt entropy, which measures the amount of compression in a prompt's token representations. 

% Augmentation-invariance metrics assess the robustness of representations to augmentations on the input prompt. We employ DiME~\citep{skean2023dime}, infoNCE~\citep{oord2018representation}, and LiDAR~\citep{thilak2023lidar}. We provide full details and examples of the augmentation process in Appendix~\ref{appendix:prompt-augmentation}.


% In this section, we introduce three categories of evaluation metrics to analyze the representations of LLMs. These categories are token embedding diversity metrics, batch embedding diversity metrics, and augmentation-invariance metrics.


% The first category of metrics we examine are token embedding diversity metrics, which quantify the information content \textit{at the prompt level} by assessing the representations of tokens within a prompt. These metrics seek to understand how LLMs transform the prompt during a forward pass.

\subsubsection{Token Embedding Diversity Metrics}
\label{sect:token-embedding-diversity-metrics}


Token embedding diversity metrics evaluate the variability and richness of the representations at the token level within a single sequence. These metrics are designed to capture how distinctively each token is represented within the context of the entire prompt, providing insight into how effectively the model encodes information and differentiates between different parts of the input.

\paragraph{Prompt Entropy:}



Following \citet{wei2024large}, we use the $\alpha$-order matrix-based entropy \citep{giraldo2014measures} as a surrogate for Rényi entropy. For a sequence of token representations $\mathbf{Z} \in \mathbb{R}^{L \times d}$, the Gram matrix is $\mathbf{K_Z} = \mathbf{Z}\mathbf{Z}^\top$. The entropy is:

\begin{equation}
\label{eq:matrix-based-entropy}
    S_{\alpha}(\mathbf{Z}) = \frac{1}{1 - \alpha} \log \left( \sum_{i=1}^{L} \left( \frac{\lambda_i(\mathbf{K_Z})}{\operatorname{tr}(\mathbf{K_Z})} \right)^{\alpha} \right).
\end{equation}



In this context, prompt entropy quantifies the degree of diversity and dispersion in token embeddings within a single sequence. Higher entropy values indicate that the model preserves more nuanced and varied token-level information. Conversely, lower entropy suggests that the model compresses the input representations into fewer dimensions or patterns. As such, prompt entropy provides a useful measure of how well the model maintains complexity and richness in its intermediate representations.

Unless otherwise specified, we use the limit case $\alpha=1$ in our calculations. At this limit, the metric is equivalent to the RankMe measure defined in \cite{garrido2023rankme}. We explore the effects of different $\alpha$ values in Appendix \ref{appendix:entropy}. For a more in-depth examination of prompt entropy, refer to Appendix~\ref{sect:appendix-prompt-entropy}.


\paragraph{Curvature}

As introduced by \citet{hosseini2024curvature}, curvature measures how rapidly the direction between two adjacent token embedding vectors changes. Define their difference as $\mathbf{v}_k = \mathbf{z}_{k+1} - \mathbf{z}_k$. The average curvature of a prompt is:

\begin{equation}
    \bar{C} = \frac{1}{L-2} \sum_{k=1}^{L-2} \arccos\left( \frac{\mathbf{v}_{k+1}^\top \mathbf{v}_k}{\|\mathbf{v}_{k+1}\| \|\mathbf{v}_k\|} \right).
\end{equation}



\subsubsection{Augmentation Invariance Metrics}

These metrics measure how consistently a model represents a prompt when it is perturbed or augmented. Because augmentations may change prompt length, we average all token embeddings to form a single vector per prompt.


 Because augmentation may change the prompt length, the token embedding diversity metrics described in~\ref{sect:token-embedding-diversity-metrics} are no longer suitable. Instead, we average all token embeddings to form a single vector per prompt and use the metrics described below to measure the similarity between two augmentations of the same prompt. 


 Let $Z_1 \in \mathbb{R}^{N \times D}$ and $Z_2 \in \mathbb{R}^{N \times D}$ represent two augmented sets of $N$ prompts, where the $i$-th row in both corresponds to the same original prompt. Details on the augmentation process are in Appendix~\ref{appendix:prompt-augmentation}.


% We refer to the two batches of augmented prompts as $Z_1 \in \mathbb{R}^{N \times D}$ and $Z_2 \in \mathbb{R}^{N \times D}$, where $N$ is the batch size and row $i$ in both matrices correspond to the same original prompt. We provide full details and examples of the augmentation process in Appendix~\ref{appendix:prompt-augmentation}.

\paragraph{InfoNCE}

InfoNCE~\citep{oord2018representation} provides a mutual information lower bound between paired augmentations. Lower InfoNCE loss suggests that augmentations of the same prompt map to similar representations, indicating invariance to perturbations. This loss is widely used to train augmentation-invariant networks in self-supervised learning for vision and is well-suited to capturing the semantic similarity underlying the augmented prompts~\citep{chen2020simclr, chen2020mocov2, shwartz2024compress, NEURIPS2023_b63ad8c2}.


\paragraph{DiME}

DiME~\citep{skean2023dime} compares the alignment of paired samples to that of randomly paired samples. Similar to InfoNCE, it is used to estimate the mutual information between two augmented sets of prompts. DiME is grounded in the matrix-based entropy defined in Eq.~\ref{eq:matrix-based-entropy}. In essence, it quantifies how closely the pairings in $(Z_1, Z_2)$ resemble each other, compared to pairings of $(Z_1, \Pi Z_2)$ for a permutation matrix $\Pi$. Higher DiME values imply that correct augmentation pairs yield representations that are significantly more similar than random pairings, indicating stronger augmentation invariance.

\paragraph{LiDAR}
LiDAR~\citep{thilak2023lidar} employs a linear discriminant analysis (LDA) framework to assess how well augmentations of a single prompt cluster together. Each prompt is considered a separate class, with its augmentations serving as class samples. By examining the variances of the linear discriminant components, LiDAR quantifies the tightness of these clusters. Higher LiDAR scores indicate that augmentations belonging to the same prompt form more coherent groups, reflecting stronger invariance.

To compute the LDA matrix, LiDAR uses augmentations to construct the class scatter matrix. In our setup, we use $N$ classes (one for each prompt) and $J=16$ samples per class. This is a larger sample size than the $J=2$ used in DiME or InfoNCE, reflecting the more complex requirements of computing the LDA matrix.





% \subsubsection{Batch Embedding Diversity Metrics}

% Batch embedding diversity metrics evaluate the diversity of representations across different input sequences within a batch. These metrics help in understanding how well the model captures the variety present in the dataset, reflecting the model's ability to generate distinct and meaningful embeddings for different inputs. High batch diversity suggests that the model effectively discriminates between different sequences, which is crucial for tasks that require nuanced understanding and differentiation between multiple contexts.


% The second category of metrics we examine are batch embedding diversity metrics, which quantify the information content \textit{at the batch level} by assessing the representations of prompts within a dataset. Such measures could be applied at the dataset level instead of the batch level, but a sufficiently large batch suffices and is more tractable. Assuming the batch (dataset) is diverse, these measures capture how well that diversity is captured by the model.


% \paragraph{Batch Entropy}
% We aggregate embeddings from all sequences in a batch and compute the matrix-based entropy similarly to prompt entropy.

% \subsubsection{Batch Entropy}
% Similarly to prompt entropy as described in Section \ref{sect:prompt-entropy}, we can apply matrix-based entropy at the batch level. To do so, 

\section{Experiments}
\label{sec:experiments}

\subsection{Intermediate Layers Provide Better Representations for Downstream Embedding Tasks}

We begin by evaluating representations at each model layer on a suite of downstream tasks from the Massive Text Embedding Benchmark (MTEB)~\citep{muennighoff2022mteb}. MTEB is designed to test the performance of LLMs on various embedded tasks. We chose 32 tasks covering classification, clustering, and
re-ranking. We use three models: Pythia 410M, Mamba 130M, and LLM2Vec-unsup-simcse~\citep{behnamghader2024llm2vec}. 



%First, we evaluate the performance of the representations of each layer in downstream tasks in the Massive Text Embedding Benchmark (MTEB)~\citep{muennighoff2022mteb}. This benchmark is designed to test the performance of LLMs on various embedded tasks. We chose 32 tasks that range from classification, clustering, and re-ranking. We evaluated each layer of Pythia 410M, Mamba 130M, and LLM2Vec-unsup-simcse~\citep{behnamghader2024llm2vec}.


Our findings indicate that intermediate layers consistently outperform the final layer across all three architectures (Table~\ref{tab:downstream_performance}). Selecting the best-performing intermediate layer yields at least a 2\% improvement in average accuracy compared to using the last layer. While prior work~\citep{fan2024notalllayers} noted similar trends for generation tasks, our results extend this observation to embedding-based evaluations.



%Interestingly, the intermediate layers consistently outperform the final layers in all architectures (Table~\ref{tab:downstream_performance}). Using the best-performing layer to compute the average accuracy yields at least a 2\% improvement. Similar findings were shown  in~\citep{fan2024notalllayers} for generation tasks, while our results are for embedding tasks.


\subsection{Downstream Performance and Entropy Are Negatively Correlated}

We next examine how prompt entropy relates to downstream performance on the Massive Multitask Language Understanding (MMLU) benchmark~\citep{hendrycks2020mmlu}, which tests comprehensive knowledge across 57 diverse subjects, covering topics from elementary mathematics to professional law.


We compare two similarly sized models, Llama3-8B and Mamba2-8B. Despite having the same parameter count, Llama3 achieves $63.85 \pm 0.38\%$ accuracy, far surpassing Mamba2’s $26.76 \pm 0.37\%$. We hypothesize that Llama3’s intermediate layers compress information more effectively, helping it discard irrelevant details and focus on task-relevant features. As shown in Figure~\ref{fig:metrics-across-architectures}, the correlation between intermediate-layer entropy and MMLU performance in Llama3 is strongly negative (-0.43 between the second and later layers) (Figure~\ref{fig:with_logit22}). In contrast, Mamba2 shows no such relationship, nor evidence of similar compression (Figure~\ref{fig:with_logit11}).



%We compared two models of the same parameter size, Llama3-8B and Mamba2-8B. Despite having the same parameter size, Llama3 $63.85 \pm0.38 \%$ outperforms Mamba2 $26.76\pm0.37$ significantly. We hypothesize that LLama3's superior performance is due to compression in its intermediate layers, as shown in Figure \ref{fig:metrics-across-architectures}, enabling it to filter irrelevant information, which is useful for tasks like MMLU. Additionally, we find a strong negative correlation (-0.43) between the second and later layers of LLama3's representations and MMLU task performances \ref{fig:with_logit22}. In contrast, Mamba2 shows neither such compression nor correlation with performance \ref{fig:with_logit11}.


\subsection{Experimental Setup for Evaluating Representation Quality}
\label{sect:metrics-experiments}

We now apply the metrics from Section~\ref{sec:metrics} to quantify representation quality layer-by-layer. Our experiments span both Transformers, SSMs, and Pythia~\citep{biderman2023pythia}, including various scales. We utilize two datasets: WikiText-103~\citep{merity2016pointer}, representing general text, and an instruction-based medical dataset~\citep{ruslanmv2024} for more specialized content. This setup allows us to probe how architectural choices and input complexity affect internal representations.



%Next, we apply the metrics described in Section~\ref{sec:metrics} to measure the quality of layer-wise representations. We conduct experiments on Transformers, SSMs and Pythia~\citep{biderman2023pythia} using models of varying sizes to analyze the impact of architecture and capacity. We utilize the WikiText-103 dataset \citep{merity2016pointer} and an instruction medical dataset \citep{ruslanmv2024} to test different input complexities.


%\subsection{Behavior of metrics across model architectures}
\subsubsection{Architectural Differences}
\label{sect:metrics-across-architectures}



Our analysis reveals notable differences in representation quality between Transformer-based architectures (e.g., Pythia) and SSMs (e.g., Mamba). Figure \ref{fig:metrics-across-architectures} compares entropy, InfoNCE, LiDAR, and DiME metrics as a function of model depth, normalized to allow fair comparisons across models with different numbers of layers.

%Our analysis reveals key differences in representation quality between Transformer-based architectures such as Pythia and SSMs such as Mamba across multiple metrics, including entropy, InfoNCE, LIDAR, and DiME. Figure \ref{fig:metrics-across-architectures} illustrates how these metrics vary as a function of model depth, represented as a percentage of the total number of layers, allowing for fair comparison between models of different depths.


For entropy and LiDAR, Pythia shows a pronounced decrease at intermediate layers, suggesting greater information compression and consolidation. In contrast, Mamba maintains more stable values, indicating less compression in its intermediate representations. Meanwhile, Mamba exhibits lower DiME and InfoNCE values than Pythia, implying reduced variability in its intermediate-layer representations.




%For entropy and LIDAR metrics, Pythia shows a significant reduction in values at intermediate layers, suggesting compression and information consolidation, while Mamba maintains more stable values, indicating less compression in intermediate representations. In contrast, Mamba exhibits lower values for the DiME and InfoNCE metrics than Pythia, implying less variability in intermediate representations.


Overall, these metric shifts are more pronounced in Pythia than in Mamba, suggesting that Pythia undergoes stronger representational transformations at intermediate depths. By comparison, Mamba’s representations remain more uniform across layers. These differences may influence how each model encodes and leverages information for downstream tasks.




%The effect and changes in these metrics across the intermediate layers are generally less pronounced in Mamba than in Pythia. This indicates that Mamba maintains more stable representations throughout its depth, whereas Pythia exhibits greater shifts and transformations in its intermediate representations, potentially leading to different strengths in how these models encode and utilize information for downstream tasks.

\iffalse
\begin{figure}[!t]
  \centering
\includegraphics[width=0.7\linewidth]{figures/metrics_comparison_pythia_mamba_llama.pdf}
  \caption{\textbf{Intermediate layers in Mamba show more stable representation values than Pythia, which exhibits more pronounced changes.} Representation evaluation metrics across layers in Pythia 410M and Mamba 370M architectures. The x-axis is the depth percentage of the model to allow fair comparison between models with different numbers of layers.}
  \label{fig:metrics-across-architectures}
\end{figure}
\fi



\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_comparison_pythia_mamba_llama_sentence-entropy.pdf}
        \caption{Prompt Entropy}
    \end{subfigure}%
    \hspace{0.04\textwidth}% Adjust spacing between subplots
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_comparison_pythia_mamba_llama_curvature.pdf}
        \caption{Curvature}
    \end{subfigure}%
    \hspace{0.04\textwidth}% Adjust spacing between subplots
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_comparison_pythia_mamba_llama_infonce.pdf}
        \caption{infoNCE}
    \end{subfigure}
    
    \vspace{0.5cm} % Adjust vertical spacing between rows
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_comparison_pythia_mamba_llama_lidar.pdf}
        \caption{LiDAR}
    \end{subfigure}%
    \hspace{0.04\textwidth}% Adjust spacing between subplots
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_comparison_pythia_mamba_llama_dime.pdf}
        \caption{DiME}
    \end{subfigure}%
    \hspace{0.04\textwidth}% Adjust spacing between subplots
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_comparison_pythia_mamba_llama_dime-normalized.pdf}
        \caption{DiME divided by Pr. Ent.}
    \end{subfigure}
  \caption{\textbf{Pythia’s intermediate layers show pronounced changes in representation quality metrics, while Mamba’s remain more stable.} Representation evaluation metrics across layers in Pythia 410M and Mamba 370M architectures. The x-axis denotes model depth as a percentage, allowing fair comparison between models with different layer counts.}
  \label{fig:metrics-across-architectures}
\end{figure}



\subsubsection{Impact of Training Progression}

To examine how representation quality evolves over the course of training, we analyze Pythia's representations at various checkpoints. Figure \ref{fig:metrics_across_training} reports several evaluation metrics across layers from the initial training step up to step 143k, sampled on a logarithmic scale.



%To understand how representation quality evolves during training, we use the training checkpoints provided by Pythia, examining how the metrics change across different layers as training progresses. Figure \ref{fig:metrics_across_training} shows the evaluation metrics for logarithmically spaced training checkpoints, from the initial step to the final step at 143k.


The results show that the most significant changes occur in the intermediate layers. As training progresses, prompt entropy in these layers decreases, indicating that the model is learning to compress and abstract input information more efficiently. In contrast, the InfoNCE metric peaks in the intermediate layers, suggesting that the representations become more distinct. Meanwhile, LiDAR and DiME values both decline, reflecting a reduction in variability along certain representational dimensions.




%The training dynamics reveal that the most significant changes occur in the intermediate layers. Specifically, the prompt entropy decreases in the middle layers during training, suggesting that the model learns to better compress and abstract the information within a prompt. This compression indicates that the model is becoming more efficient in representing complex information as training progresses. In contrast, the InfoNCE metric peaks in the middle layers, indicating increased distinctiveness of representations, while the LiDAR and DiME metrics both decrease, suggesting reduced variability in certain directions of the representation space.

Interestingly, the metrics for the earliest layers remain relatively stable throughout training. This observation aligns with the detokenization hypothesis proposed by \citep{lad2024remarkable}, which posits that initial layers primarily handle the mapping of raw input tokens into an initial embedding space. Their roles appear to solidify early on, exhibiting less ongoing change than the intermediate layers.



%Interestingly, the metrics in the initial layers remain relatively stable throughout the training, which we believe supports the detokenization hypothesis discussed in \citep{lad2024remarkable}. This indicates that the initial layers primarily focus on mapping input tokens to an initial embedding space, with little change in their representation dynamics during training.


% \begin{figure}[!t] \centering \includegraphics[width=0.8\linewidth]{figures/metrics_at_pythia_checkpoints.pdf} \caption{\textbf{Training effects are most pronounced in the intermediate layers.} Representation metrics across layers at different training checkpoints (steps 1 to 143k). The x-axis is the depth percentage of the model, showing how training influences different layers, particularly those at intermediate depths.} \label{fig:metrics_across_training} 
% \end{figure}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_at_pythia_checkpoints_sentence-entropy.pdf}
        \caption{Prompt Entropy}
    \end{subfigure}%
    \hspace{0.04\textwidth}% Adjust spacing between subplots
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_at_pythia_checkpoints_curvature.pdf}
        \caption{Curvature}
    \end{subfigure}%
    \hspace{0.04\textwidth}% Adjust spacing between subplots
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_at_pythia_checkpoints_infonce.pdf}
        \caption{infoNCE}
    \end{subfigure}
    
    \vspace{0.5cm} % Adjust vertical spacing between rows
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_at_pythia_checkpoints_lidar.pdf}
        \caption{LiDAR}
    \end{subfigure}%
    \hspace{0.04\textwidth}% Adjust spacing between subplots
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/metrics_at_pythia_checkpoints_dime.pdf}
        \caption{DiME}
    \end{subfigure}%
    \hspace{0.04\textwidth}% Adjust spacing between subplots
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/metrics_at_pythia_checkpoints_dime-normalized.pdf}
        \caption{DiME divided by Pr. Ent.}
    \end{subfigure}
    
    \caption{\textbf{Training effects are most pronounced in the intermediate layers.} Representation metrics across layers at different training checkpoints (steps 1 to 143k). The x-axis is the depth percentage of the model, showing how training influences different layers, particularly those at intermediate depths.}
    \label{fig:metrics_across_training}
\end{figure}


\iffalse
\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/metrics_at_pythia_checkpoints.pdf}
  \caption{\textbf{Training effects are most pronounced in the intermediate layers, with distinct dynamics for different metrics.} Representation evaluation metrics across layers at various training checkpoints, ranging from step 1 to the final step at 143k. The x-axis represents the depth percentage of the model, showing how training affects different layers, particularly in the intermediate stages.}
  \label{fig:metrics_across_training}
\end{figure}

\fi





\subsubsection{Prompt Entropy under Extreme Input Conditions}

To gain deeper insights into how prompt entropy behaves under various input perturbations, we investigate the impact of extreme prompt modifications on the model's internal representations. Specifically, we analyze how prompt entropy evolves across different layers of the Pythia 410M model when subjected to high levels of token repetition, randomness, or increased prompt length.



%To further understand how prompt entropy behaves under different input conditions, we examine the effect of increasingly extreme prompts on the model's representations. Specifically, we analyze how the prompt entropy changes across layers of the Pythia 410M model when the input prompts exhibit high levels of token repetition, randomness, or increased length.


We design three types of extreme prompts:

\begin{enumerate}

    \item \textbf{Prompts with Increasing Token Repetition}: We select 1,000 standard prompts from the WikiText dataset and randomly replace tokens with a fixed token from the prompt at varying probabilities $p$. As $p$ increases, the amount of repetition in the prompt increases.




    \item \textbf{Prompts with Increasing Token Randomness}: We introduce randomness by randomly substituting tokens in the prompts with arbitrary tokens from the vocabulary at varying probabilities $p$. Higher values of $p$ correspond to greater randomness in the prompts.
    

    \item \textbf{Random Prompts of Increasing Length}: We generate random prompts by sampling tokens uniformly from the vocabulary, creating prompts of varying lengths $T$.
\end{enumerate}

%\subsection{Behavior of prompt entropy for increasingly extreme prompts}

Figure \ref{fig:pythia-increasing-intensity} displays both normalized and unnormalized prompt entropy across different layers for each type of extreme prompt. The key observations from this analysis are:



%Figure \ref{fig:pythia-increasing-intensity} illustrates how the normalized and unnormalized prompt entropy changes across layers for these extreme prompts. Our key findings are as follows:


\textbf{1. Increasing token repetition reduces entropy in intermediate layers.} As the probability $p$ of token repetition rises, the model compresses redundant information, leading to lower entropy values in the middle layers. This compression indicates that the model effectively recognizes and encodes repetitive patterns within the input.


\textbf{2. Increasing token randomness elevates entropy, particularly in initial layers.} Introducing random tokens enhances the diversity of token representations, resulting in higher entropy values. The initial layers exhibit the most significant increases, suggesting that these layers are more sensitive to input noise and variability.



%\textbf{(2) Increasing token randomness results in higher entropy, especially in initial layers.} Introducing random tokens increases the diversity of token representations, leading to higher entropy. The initial layers are more affected, suggesting sensitivity to input noise.

\textbf{3. Prompt length influences entropy in Both normalized and unnormalized Forms.} Unnormalized entropy naturally grows with prompt length due to the increased number of tokens. Although not displayed, normalized entropy demonstrates sublinear growth, implying that each additional token contributes progressively less to the overall diversity as the prompt lengthens.



%\textbf{(3) Prompt length affects entropy in both normalized and unnormalized forms.} Unnormalized entropy naturally increases with prompt length due to more tokens. While not displayed, the normalized entropy shows sublinear growth, indicating that each additional token contributes less to the overall diversity as the prompt becomes longer.

These findings illustrate that extreme input conditions distinctly affect the model's internal representations, especially within intermediate layers. The varying compression and encoding behaviors based on the nature of input perturbations provide valuable insights into the model's processing mechanisms and its capacity to maintain or reduce information complexity under different scenarios.



%These observations highlight how extreme input conditions impact the model's internal representations, particularly in the intermediate layers. The model exhibits different compression and encoding behaviors depending on the nature of the input perturbations, which provides valuable insight into its processing mechanisms.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pythia_increasing_repetition.pdf}
        \caption{Repetition}
        \label{fig:pythia_increasing_repetition}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pythia_increasing_randomness.pdf}
        \caption{Randomness}
        \label{fig:pythia_increasing_randomness}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pythia_random_of_differing_lengths_raw.pdf}
        \caption{Random Prompt Length}
        \label{fig:prompt-random-raw}
    \end{subfigure}
    \caption{\textbf{Prompt entropy across layers of Pythia 410M under various extreme input conditions.} (a) Increasing token repetition leads to decreased entropy in intermediate layers. (b) Increasing token randomness results in higher entropy, especially in initial layers. (c) Unnormalized prompt entropy increases with prompt length due to the larger number of tokens. These results demonstrate how the model's internal representations adapt to different types of input perturbations.}
    \label{fig:pythia-increasing-intensity}   
\end{figure}



\subsection{Bimodal Behavior in Prompt Entropy}

During our analysis of average prompt entropy across different layers, we identified an intriguing phenomenon: a distinct bimodal distribution of entropy values in certain layers of Transformer models, which was absent in SSMs. Figure \ref{fig:all-models-bimodal} presents the entropy distributions for both the WikiText and AI-Medical-Chatbot datasets~\citep{ruslanmv2024}. Notably, the AI-Medical-Chatbot dataset exhibits a pronounced bimodal distribution in the middle layers of Transformer models. This suggests that the model processes some prompts in fundamentally different ways at these intermediate stages. To investigate the underlying causes of this bimodality, we conducted several experiments detailed in Appendix \ref{appendix:bimodal-investigation}. Our findings indicate that factors such as prompt length, semantic complexity, and overlap with training data do not account for this behavior. Consequently, the root cause of the bimodal entropy distribution remains an open question.








%While analyzing the average prompt entropy across different layers, we discovered an intriguing phenomenon: a clear bimodal distribution in the entropy values at certain layers in transformer models, but not SSMs. Figure \ref{fig:all-models-bimodal} shows the entropy distributions for WikiText and the ai-medical-chatbot datasets\citep{ruslanmv2024}. Notably, a pronounced bimodal distribution is observed in the middle layers for the ai-medical-chatbot dataset. This behavior suggests that the model processes some prompts fundamentally differently than others at these intermediate stages. We investigated the causes of this behavior in Appendix \ref{appendix:bimodal-investigation} and ruled out prompt length, semantic complexity, or overlap with training data. The underlying cause is currently an open question.

\section{Causal Vision Transformer}
\label{sec:causal_vision_transformer}

\begin{figure}[h]
  \centering
  \begin{minipage}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/apple_aim/d_normalized.pdf}
    % \caption{d normalized}
    \label{fig:figure1}
  \end{minipage}
  \begin{minipage}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/apple_aim/n_normalized.pdf}
    % \caption{n normalized}
    \label{fig:figure2}
  \end{minipage}
  \begin{minipage}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/apple_aim/unnormalized.pdf}
    % \caption{unnormalized}
    \label{fig:figure3}
  \end{minipage}
  \caption{Imagenet Layerwise entropy}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/apple_aim/Screenshot 2025-01-21 at 10.42.39 AM.png}
  \caption{\textcolor{red}{[TODO(Rifat): replace this with the reproduced results]}}
  % \label{fig:your_label}
\end{figure}




\section{Discussion and Conclusion}
\label{sec:discussion_conclusion}


In this work, we explored the representation quality of intermediate layers in LLMs, providing insights into their critical role in downstream task performance. By applying a diverse set of evaluation metrics, including prompt entropy, curvature, InfoNCE, LiDAR, and DiME, we highlighted distinct behaviors in Transformer-based architectures and SSMs. Our findings demonstrate that intermediate layers often outperform final layers in representation quality, underscoring their significance for feature extraction and transfer learning.






%In this study, we thoroughly examined the quality of layer-wise representations in LLMs, specifically comparing Transformer-based architectures and SSMs. Using a variety of evaluation metrics, including prompt entropy, curvature, InfoNCE, LIDAR, and DiME, we found several key insights into how these models process and encode information across different layers and under various conditions.


Transformers exhibited greater representational variability and information compression within intermediate layers, whereas SSMs displayed more stable and consistent representations. This suggests differing strategies in encoding information, with Transformers excelling in adaptability and SSMs prioritizing robustness. Furthermore, the training analysis revealed that the most substantial improvements in representation quality occur in intermediate layers, reinforcing their importance in learning dynamics.



%Our findings indicate that intermediate layers consistently provide superior representations for downstream tasks compared to final layers. This highlights the importance of using intermediate representations for feature extraction and transfer learning applications. Additionally, significant architectural differences were observed: Transformers exhibited more dynamic changes in metrics such as entropy and InfoNCE in their intermediate layers, suggesting a higher degree of information compression and variability. In contrast, SSMs maintained more stable representations, reflecting a different approach to information encoding that emphasizes consistency.

Our investigation into extreme input conditions revealed that intermediate layers play a pivotal role in adapting to diverse input scenarios, with distinct responses to token repetition, randomness, and prompt length. Additionally, the observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question, offering avenues for further research.




%During training progression, the most substantial changes in representation quality occurred in the intermediate layers, with prompt entropy decreasing and InfoNCE peaking, indicating enhanced compression and distinctiveness of representations. This underscores the critical role of intermediate layers in the learning process and suggests potential avenues for optimizing training strategies to further improve representation quality.




%LLMs demonstrated distinct behaviors under extreme input conditions, such as increased token repetition, randomness, and prompt length. Transformers showed significant variations in entropy and other metrics in response to input perturbations, particularly in intermediate layers, whereas SSMs maintained more stable representations. This suggests that transformers are more adaptable and sensitive to diverse input scenarios, while SSMs offer greater robustness and consistency. A particularly intriguing observation was the presence of bimodal entropy distributions in the intermediate layers, especially within the ai-medical-chatbot dataset. Despite extensive investigations, the cause of this bimodality remains unresolved. 

In conclusion, our research advances the understanding of internal representation dynamics in LLMs, highlighting the pivotal role of intermediate layers and the distinct behaviors of different architectures. These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application. Future work should delve deeper into the causes of phenomena such as bimodal entropy distributions and explore the development of new metrics specifically tailored to LLMs to further enhance representation evaluation.







%In conclusion, our research advances the understanding of internal representation dynamics in LLMs, highlighting the pivotal role of intermediate layers and the distinct behaviors of different architectures. These findings not only contribute to the theoretical knowledge of model representations, but also offer practical guidance for optimizing model design, training, and application. Future work should delve deeper into the causes of phenomena such as bimodal entropy distributions and explore the development of new metrics tailored specifically to LLMs to further enhance representation evaluation.




\bibliographystyle{styles/neurips/neurips_2024}
\bibliography{strings, references}

\newpage
 \appendix



% \section{Detailed Definitions}
% \label{appendix:definitions}

% Here, we provide comprehensive definitions and mathematical formulations for the metrics and concepts introduced in the main text. This includes the derivation of matrix-based entropy, curvature calculations, and other relevant measures used in our analysis.


\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.8\textwidth}
                \centering
                \includegraphics[width=\linewidth]{figures/Pythia_bimodal_entropies.pdf}
                \caption{Pythia 410M}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\textwidth}
              \centering
              \includegraphics[width=\linewidth]{figures/mamba_bimodal_entropies.pdf}
                \caption{Mamba 370M}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\textwidth}
              \centering
              \includegraphics[width=\linewidth]{figures/Llama3_bimodal_entropies.pdf}        
                \caption{Llama3 8B}
        \end{subfigure}
\caption{\textbf{Bimodal distribution of prompt entropies observed in intermediate layers.} The distributions of prompt entropies for WikiText and ai-medical-chatbot datasets are shown for Pythia, Mamba, and Llama3 models. The middle column highlights the layer with the highest Dip Test score \citep{hartigan1985dip}, which measures the degree of multimodality in the entropy distribution.}
\label{fig:all-models-bimodal}   
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/mmlu_llama3-8b.pdf}
  \caption{Entropy vs Accuracy of LLama3-8B on MMLU tasks. Each point represents a task in MMLU}
  \label{fig:with_logit22}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.58\linewidth]{figures/mmlu_mamba2-8b.pdf}
  \caption{Entropy vs Accuracy of Mamba2-8B on MMLU tasks}
  \label{fig:with_logit11}
\end{figure}

\section{Investigation into Bimodal Distribution of Entropies}
\label{appendix:bimodal-investigation}
To determine the underlying cause of this bimodal distribution of prompt entropies, we conducted several experiments to see if specific properties of the dataset could explain this phenomenon. Our goal was to understand whether the bimodality was related to characteristics such as prompt length, semantic complexity, or overlap with training data.

\paragraph{Effect of Prompt Length}

Initially, we hypothesized that the bimodality might be caused by variations in prompt length. If one mode corresponded to shorter prompts and the other to longer prompts, it could indicate different processing strategies. However, since the entropy values were normalized and theoretically invariant to length, this was unlikely. Upon further analysis, we confirmed that prompt length did not significantly correlate with the observed bimodality.

\paragraph{Manual Examination of Prompts}

We then manually examined prompts from each mode of the distribution to identify any distinguishing features, such as difficulty or specific types of medical terminology. Despite this effort, we found no significant differences between the prompts in either mode. Both modes contained a similar range of medical complexity and varied use of terminology, suggesting that the model's entropy was not merely a reflection of the difficulty or specificity of the input.

\paragraph{Training Set Overlap}

Next, we investigated whether the low entropy mode might be associated with prompts that were very similar to samples seen during training. Given that both the ai-medical-chatbot dataset and PILE \citep{gao2020pile} (which Mamba, Pythia, and possibly Llama3 were trained on) contained medical articles from PubMed, we hypothesized that overlap with training data could lead to more confident, lower-entropy representations. To test this, we implemented a BM25 index \citep{bm25s} to quickly search for identical or highly similar articles between the two datasets.

While we did find identical articles between the ai-medical-chatbot dataset and PILE, these articles were evenly distributed across both modes of the bimodal entropy distribution. This suggests that the presence of training set overlap does not explain the bimodal behavior, and the underlying cause remains an open question.


\section{Architectural Details}
\label{appendix:architectures}

In this section, we elaborate on the specific architectures of Transformers and State Space Models (SSMs). We outline the mathematical foundations, including the weight matrices, attention mechanisms for Transformers, and the state transition matrices for SSMs. Detailed equations and parameter configurations are provided to facilitate replication and deeper understanding.

\subsection{Transformer}
The Transformer architecture \citep{vaswani2017attention} utilizes self-attention mechanisms. Given an input $\mathbf{x}$, the key ($\mathbf{K}$), query ($\mathbf{Q}$), and value ($\mathbf{V}$) matrices are computed as:


\begin{equation}
    \mathbf{Q} = \mathbf{x}\mathbf{W}_Q, \quad \mathbf{K} = \mathbf{x}\mathbf{W}_K, \quad \mathbf{V} = \mathbf{x}\mathbf{W}_V,
\end{equation}

where $\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d \times d_k}$ and $\mathbf{W}_V \in \mathbb{R}^{d \times d_v}$ are learned weights.

The attention weights are calculated using:

\begin{equation}
    \mathbf{A} = \operatorname{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}} + \mathbf{M}\right),
\end{equation}

where $\mathbf{M}$ is a mask to enforce causality in autoregressive tasks.

The output is then:

\begin{equation}
    \mathbf{y} = \mathbf{A}\mathbf{V}.
\end{equation}

\section{Discussion on Prompt Entropy}
\label{sect:appendix-prompt-entropy}

The first measure of token embedding diversity we call prompt entropy. This entropy is measured on the intermediate tokens and captures how diverse the token representations are.

We follow the work of \cite{wei2024large} and use $\alpha$-order matrix-based entropy \cite{giraldo2014measures, skean2023dime, skean2024frossl}, which serves as a tractable surrogate for traditional Rényi’s $\alpha$-order entropy \cite{renyi1961measures}. The quantity is calculated using a similarity kernel $\kappa$ on a batch of samples drawn from a distribution, without making explicit assumptions on what the true distribution is. The choice of kernel $\kappa$ is flexible and can be any infinitely divisible kernel such as the Gaussian kernel, linear kernel, or Laplacian kernel, among others. For this work, we restrict ourselves to the linear kernel $\kappa(a, b) = a b^T$. This choice is motivated by the linear representation hypothesis \cite{parklinear2024} which finds that large language model representations encode high-level concepts such as truth \cite{burns2022dl}, honesty \cite{mallen2024eliciting}, and part-of-speech \cite{mamou2020emergence} in linearly separable manifolds.

 The equation for matrix-based entropy was previously defined in Eq. \ref{eq:matrix-based-entropy}. One way to interpret Eq. \ref{eq:matrix-based-entropy} is as the $\alpha$-order Rényi entropy of the Gram matrix eigenvalues\footnote{The non-zero eigenvalues of the Gram matrix $Z Z^T$ are equivalent to those of the covariance matrix $Z^T Z$. Using the covariance matrix instead of the Gram matrix in Eq. \ref{eq:matrix-based-entropy} makes no difference and is more computationally efficient if $D < N$.}. Notice how each eigenvalue is divided by $\textrm{tr}(\mathbf{K}_{\mathbf{Z}})$ before being raised to the $\alpha$ power. This is so that the eigenvalues of $\mathbf{K}_{\mathbf{Z}}$ sum to one (because  $\textrm{tr}(\cdot) = \sum_{i=1}^n \lambda_i(\cdot)$), which is a necessary condition to treat the eigenvalues as a probability distribution. Futhermore, each eigenvalue of $\mathbf{K}_{\mathbf{Z}}$ signifies the variance of samples in a particular principal component direction~\cite{scholkopf2018learning}. If entropy is low, then the eigenvalues form a heavy-tail distribution which implies that a few components dominate the variance of samples in $Z$. On the other hand, at maximum entropy, the eigenvalues form a uniform distribution and samples are spread equally in all directions. Matrix-based entropy is reminiscent of the LogDet entropy which uses the determinant of $\mathbf{K}_{\mathbf{Z}}$ to capture how much "volume" a dataset occupies~\cite{shwartz2023information, zhouyin2021understanding}. The LogDet entropy is given by $S_{\textrm{LogDet}}(Z) = \log \det (\mathbf{K}_{\mathbf{Z}}) - \log 2$. One can use Jensen's inequality to show that the LogDet entropy is a lower bound of Eq \ref{eq:matrix-based-entropy} when $\lim_{\alpha \rightarrow 1}$ (Appendix J.4 of~\cite{shwartz2023information}).
 
 Depending on the choice of $\alpha$, several special cases of matrix-based entropy can be recovered. In particular, when $\lim_{\alpha \rightarrow 1}$ it equals Shannon entropy (also referred to as von Neumann entropy in quantum information theory \cite{bach2022information, boes2019neumann}), and when $\alpha=2$ it equals collision entropy. Interestingly, the case of $\alpha=2$ can be calculated without explicit eigendecomposition \cite{skean2024frossl}. We show in the Appendix Figure \ref{fig:power_law_entropy} how varying values of $\alpha$ affect the matrix-based entropy of Gram matrices with eigenvalues distributed with a $\beta$-power law such that $\lambda_i = i^{-\beta}$. It is shown that for larger values of $\alpha$, smaller eigenvalues contribute more to the entropy.
 

%The self-attention block of the transformer ~\citep{vaswani2017attention} consists of key ($\mathbf K \in \mathbb{R}^{L \times d_k}$), query($\mathbf Q \in \mathbb{R}^{L \times d_k}$) and value ($\mathbf V \in \mathbb R^{L \times d_v}$) that are learned by transforming input $x$ using the following weight matrices $W_Q \in \mathbb{R}^{d \times d_k}$, $W_K \in \mathbb{R}^{d \times d_k}$ and $W_V \in \mathbb{R}^{d \times d_v}$. They are computed as follows:
%\begin{equation}
%    \mathbf{Q} = \mathbf{x}W_Q, \quad  \mathbf{K} = \mathbf{x}W_K, \quad \mathbf{V} = \mathbf{x}W_V.
%\end{equation}
%Keys, queries, and values are then combined in the attention block to produce the output
%Finally the attention output is calculated as:
%\begin{equation}\label{eqn:attention}
%    \mathbf{A} = softmax\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right),
%\end{equation}

%where $A$ is the attention weights which has a lower-triangular structure.

%And final outoput is calculated as:
%\begin{equation}\label{eqn:attention}
%    \mathbf{y} = A \mathbf{V},
%\end{equation}

\subsection{State Space Models}
\label{sec:ssm}

SSMs \citep{mamba} model sequences using recurrent dynamics. The hidden state $\mathbf{h}_t$ and output $\mathbf{y}_t$ at time $t$ are updated as:

\begin{align}
    \mathbf{h}_t &= \mathbf{A}\mathbf{h}_{t-1} + \mathbf{B}\mathbf{x}_t, \\
    \mathbf{y}_t &= \mathbf{C}\mathbf{h}_t + \mathbf{D}\mathbf{x}_t,
\end{align}

where $\mathbf{A} \in \mathbb{R}^{n \times n}$, $\mathbf{B} \in \mathbb{R}^{n \times d}$, $\mathbf{C} \in \mathbb{R}^{d \times n}$, and $\mathbf{D} \in \mathbb{R}^{d \times d}$ are learned parameters.



%In state space models~\citep{gu2023mamba}, the output $\mathbf{y}$ is computed based on a dynamic recurrence of the input at each time step $i$:

%\begin{align}
%    h_{i} &= A_{i}h_{i-1} + B_ix_i \\
%    y_i &= C_ih_i + D_i x_i,
%\end{align}

%where $h_i$ denotes the latent state of the system, and the dynamic matrices $A_i, B_i, C_i, D_i$ of corresponding dimensions represent the parameters learned by the model.




\section{Behavior of Matrix-based Entropy for different choices of $\alpha$}
\label{appendix:entropy}
 
 Depending on the choice of $\alpha$, several special cases of matrix-based entropy can be recovered. In particular, when $\lim_{\alpha \rightarrow 1}$ it equals Shannon entropy (also referred to as von Neumann entropy in quantum information theory \cite{bach2022information, boes2019neumann}), and when $\alpha=2$ it equals collision entropy. Interestingly, the case of $\alpha=2$ can be calculated without explicit eigendecomposition \cite{skean2024frossl}. We show in the Appendix Figure \ref{fig:power_law_entropy} how varying values of $\alpha$ affects the matrix-based entropy of Gram matrices with eigenvalues distributed with a $\beta$-power law such that $\lambda_i = i^{-\beta}$. It is shown that for larger values of $\alpha$, smaller eigenvalues contribute more to the entropy.
 


\begin{figure}[!b]
  \begin{center}
      \includegraphics[width=0.4\linewidth]{figures/power_law_entropy.pdf}
  \end{center}
  \caption{The behavior of Eq. \ref{eq:matrix-based-entropy} for varying values of $\alpha$ on Gram matrices with eigenvalues distributed with a $\beta$-power law such that $\lambda_i = i^{-\beta}$.}
  \label{fig:power_law_entropy}
\end{figure}

\section{Dataset Details}
\label{appendix:dataset-details}
\subsection{Wikitext Dataset}
We used the wikitext dataset \cite{merity2016pointer} for the majority of our experiments in Section \ref{sect:metrics-experiments}. This was downloaded from \textbf{Salesforce/wikitext} on huggingface. The dataset consists of 100 million tokens scraped from the Featured articles on wikipedia. We filtered out prompts which were less than 30 tokens or were wikipedia section headings.

\subsection{AI-Medical-Chatbot Dataset}
We also used the medical instruction dataset called ai-medical-chatbot \cite{ruslanmv2024} which downloaded from \textbf{ruslanmv/ai-medical-dataset} on HuggingFace. An example from this dataset is:

\begin{lstlisting}
    You are an AI Medical Assistant Chatbot, trained to answer medical questions. Below is an instruction that describes a task, paired with an response context. Write a response that appropriately completes the request.
    
    ### Instruction:
    What is the resurgent sodium current in mouse cerebellar Purkinje neurons?

    ### Context:
    FGF14 modulates resurgent sodium current in mouse cerebellar Purkinje neurons.
\end{lstlisting}

\section{Prompt Augmentations}
\label{appendix:prompt-augmentation}
For the augmentation-invariance metrics such as infoNCE, LiDAR, and DiME, we use the NLPAug library \cite{ma2019nlpaug} to augment our prompts. We use three types of augmentations.

\begin{itemize}
 \item The SplitAug augmentation randomly splits words into two parts by adding a space. 
 \item The RandomCharAug augmentation randomly inserts, substitutes, swaps, or deletes characters.
 \item The Keyboard augmentation randomly substitutes characters with other characters that are at a distance of one as measured on a QWERTY keyboard. For instance, the character "k" may be replaced with "i", "l", "m", or "j".
\end{itemize}

We use the pseudocode below to do our augmentations using three types of augmentations, using the default library settings for each type. When computing augmentation-invariance metrics like infoNCE or DiME, we use the two augmented prompts rather than using one augmented prompt alongside the original prompt. Note that these augmentations may change the token length $T$ of a prompt.

\begin{verbatim}
    
    aug = naf.Sequential([
        naw.SplitAug(p=0.3),
        nac.RandomCharAug(p=0.3),
        nac.KeyboardAug(p=0.3),
    ])
    (aug_A, aug_B) = aug.augment(prompt, num_augmentations=2)

    prompt -> "The quick brown fox jumps over the lazy dog."

    aug_A ->  "The quDUk b rown fox wEmps o ver the l azy dog."
    aug_B ->  "The qTuXi bro wn fox uVm)s ob3r the la_k dog."
\end{verbatim}

\section{Extreme Prompts}
\label{appendix:extreme-prompts}

\subsection{Increasing Repetition}
We take regular prompts from the wikitext dataset, tokenize them, and then for each token we randomly replace it with probability $p$. We draw replacements tokens by sampling a random token from within the prompt. We show examples below for varying levels of $p$.

\begin{itemize}
    \item ($p = 0$) \hspace{3pt} Mint records indicate the first gold dollars were produced on May 7...
    \item ($p = 0.1$) Mint records indicate the first gold dollars were Mint Mint May 7...
    \item ($p = 0.5$) Mint records Mint Mint Mint gold dollars were Mint Mint Mint 7...
    \item ($p = 1.0$) Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint...
\end{itemize}

\subsection{Increasing Randomness}
We take regular prompts from the wikitext dataset, tokenize them, and then for each token we randomly replace it with probability $p$. We draw replacements uniformly from the tokenizer distribution. We show examples below for varying levels of $p$. Unlike the character-level random noise added to prompts in Section {with random noise discussed in Appendix \ref{appendix:prompt-augmentation} which might change the number of tokens $T$ of the prompt, the token-level random noise used here does not do so.

\begin{itemize}
    \item ($p = 0$) \hspace{3pt} Mint records indicate the first gold dollars were produced on May 7...
    \item ($p = 0.1$) Mint records indicate salivary first gold dollars were produced on May NaCl...
    \item ($p = 0.5$) Mint records Dallas actively first dollars persufors on Mayder129 18...
    \item ($p = 1.0$) arf emulsion minorensteinorianmega\_TOStack potsRecip Installifykeeping...
\end{itemize}

\subsection{Random Prompts with Certain Length}

To make a random prompt of a specific length $T$, we sample $T$ tokens uniformly from the Pythia tokenizer distribution. Such a prompt may look like the following for $T=16$: "Proposition Sequencespecific Exp fibers brows Club overviewNos toss Thinking traderMulti indoorlis".

We show how random prompt representations evolve over Pythia training checkpoints in Figure \ref{fig:training_increasing_repetition}. The random prompts we use are of length 512 tokens. It is readily observed that the prompt entropy is flat across layers in the beginning of training. As training progresses, the model compresses more and more near the final layers.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/pythia_random_sentences_across_revisions.pdf}
  \caption{Behavior of random prompt representations as model is training}
\label{fig:training_increasing_repetition}
\end{figure}

\section{Fractal Metrics}


\begin{figure}[!ht]
\centering
\includegraphics[width=0.82\textwidth]{figures/2048_cumulative_normalized_surprisal.pdf}
\includegraphics[width=0.82\textwidth]{figures/500_750_cumulative_normalized_surprisal.pdf}
\caption{\textbf{(Top)} The integral process of the first 2048 tokens of an early version of this proposal document. The next-token probabilities from Pythia-410M are used to compute the surprisal. \textbf{(Bottom)} A zoomed-in view from tokens 500 to 750.}
\label{fig:fractal}
\end{figure}

A very interesting, recent line of work treats LLMs next-token probabilities as a 1-dimensional stochastic process, and analyzes this process with four standard fractal metrics~\cite{fractal-next-token}. These fractal metrics give insights to, among other things, the long-range dependency of token predictions. Furthermore, these metrics were shown to correlate well with accuracy on downstream tasks.

To create the LLM fractal, a list is constructed of negative log-likelihoods denoted as $(z_1, z_2, \cdots, z_n)$ where each list element is defined as:
\begin{equation}
    z_t = -\log p(w_t | w_{t-1}, w_{t-2}, \cdots, w_{1})
\end{equation}
The above equation denotes the "surprisal" of the t-th token given the previous tokens, where $w_t$ is the token probability of an LLM choosing the correct token. In information-theoretic terms, the surprisal corresponds to the number of bits needed to represent the t-th token. It should be noted that minimizing the average of this list is the standard objective function used to train LLMs~\cite{vaswani2017attention}. This method of generating a fractal is well-founded because the probabilities of next-token prediction in LLMs have been shown to be well-calibrated~\cite{llms-know-what-they-know}.

Once we have this list, we normalize it to have zero mean and unit variance. The resulting list is called the \textit{increment process}. Taking the cumulative summation of the increment process is called the \textit{integral process}. These two processes are our fractals on which we will compute metrics. Given these stochastic processes, several different fractal metrics can be calculated. The precise definitions of these metrics are quite complex, and we refer to~\cite{fractal-next-token} for the exact details of their computation.

To better understand the metrics we are about to describe, we plot the integral process of an early version of this document in Figure \ref{fig:fractal}. We filter out the Latex preamble and use the first 2048 tokens which covers from the beginning of the introduction to roughly the middle of Section 2. Pythia-410m~\cite{pythia} was used to generate the next-token probabilities. Recall that the integral process is the cumulative normalized surprisal of the next-token predictions. This implies that the lower the value on the y-axis in Figure \ref{fig:fractal}, the more confident the LLM is in a correct prediction. 

The first metric on the integral process we discuss is the Hölder exponent, also known as the self-similarity exponent, which measures how "self-similar" a process is across granularities. In this context, self-similarity of a process refers to how consistent its statistical properties are depending on the scale at which it is being examined. In other words a fractal with high self-similarity still looks like a fractal if you keep zooming in. If the self-similarity is low, then the jaggedness of the sequence smooths out as you zoom in because the statistical properties are different at this smaller scale. For a 1-dimensional process, the self-similarity exponent lies in the range $[0.5, 1]$, with 0.5 being perfectly self-similar and 1 being not self-similar at all. The self-similarity exponent for this document is $0.52$, and this behavior can be seen in the bottom of Figure \ref{fig:fractal} which still exhibits jagged behavior even at the smaller time scale. Self-similar processes arise in the natural world, such as in Ethernet traffic patterns~\cite{fractal-ethernet-traffic} and the British coastline~\cite{mandelbrot-british-coast}.

Secondly, the Hurst exponent~\cite{hurst-exponent} measures the predictability of a process. In this case, predictability refers to if knowledge a later portion of the sequence can be gleaned from examining an earlier portion of the sequence. If this is true, then we call the sequence predictable because it exhibits \textit{long-range dependence}. Unlike the self-similarity exponent, the Hurst exponent is calculated on the increment process. It is possible for a process to be very self-similar but have no predictability. The standard example of this is Brownian motion which has a perfect self-similarity of $0.5$ but offers no predictability or long-range dependence. Language, on the other hand, obviously exhibits predictability as adjacent clauses relate to each other, as do adjacent sentences, paragraphs, and so on. The Hurst exponent lies in a range of $[0, 1]$, where a process with a Hurst value above $0.5$ exhibits long-range dependence. The Hurst exponent of this document is $0.67$.

Thirdly, the fractal dimension metric measures the local complexity of the fractal. Any process can be called a fractal if its fractal dimension differs from its ambient dimension. For 1-dimensional processes, it is exactly defined as $D = 2 - S$, where $S$ is the self-similarity exponent~\cite{fractal-review}. Using the fractal dimension of a process, one can prove generalization bounds~\cite{fractal-generalization-bound}. However, recent work has shown counterexamples to strong bounds based on adversarial initialization and double descent~\cite{fractal-limitations}.

Fourthly, the Joseph effect quantifies how smooth local trends are. For example, it can capture if there are long periods of increases or decreases in the process. The bottom of Figure \ref{fig:fractal} displays a clear local trend, which would increase the score of the Joseph effect.

The authors of~\cite{fractal-next-token} examine if these four metrics are correlated with performance in downstream tasks. One dataset they examined is GSM8k~\cite{gsm8k} which is a set of 8000 math problems. They find that the Hurst exponent has a Pearson correlation with accuracy of 0.83, which is significantly more than the correlation with perplexity which is 0.67. The other fractal metrics exhibit positive correlation too, but do not exceed the perplexity. This finding shows that long-range dependence, as measured by the Hurst exponent, is both critical for and a great predictor of a LLM's performance on downstream tasks.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/fractal_across_sequence.png}
    \caption{The integral process for 2048 tokens of a scientific document. Three Pythia models are shown which are roughly an order of magnitude apart from each other. A higher y-axis value is better, as that means the model is less surprised by the input. The key takeaway is that the smallest model outperforms larger models in the beginning of the sequence.}
    \label{figure:fractal-different-sizes}
\end{figure}

% \section{Results}
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/without_logit.pdf}
%   \caption{Layer wise Entropy on Different datasets (Without Logit Layer)}
%   \label{fig:without_logit}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/with_logit.pdf}
%   \caption{Layer wise Entropy on Different datasets (With Logit Layer)}
%   \label{fig:with_logit3}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/Distillations.png}
%   \caption{Distilled models}
%   \label{fig:with_logit2}
% \end{figure}


% # MAMBA_MODELS_HF = ["mamba-130m-hf", "mamba-370m-hf", "mamba-1.4b-hf", "mamba-2.8b-hf"]
% # PYTHIA_MODELS = ["pythia-160m", "pythia-410m", "pythia-1.4b", "pythia-2.8b"]

% \begin{table}[h]
%   \centering
%   \begin{tabular}{c|c|c|c}
%     \hline
%     Model Name & Model Size & \# Layers & GSM8K \\ \hline
%     mamba-130m-hf & 130m & 24 & \\
%     mamba-370m-hf & 130m & 48 & \\
%     mamba-1.4b-hf & 130m & 48 & \\
%     mamba-2.8b-hf & 130m & 64 & \\ \hline
%     pythia-160m & 130m & 12 & \\
%     pythia-410m & 130m & 24 & \\
%     pythia-1.4b & 130m & 24 & \\
%     pythia-2.8b & 130m & 32 & \\
%   \end{tabular}
%   \caption{Results on different datasets}
%   \label{tab:your_label}
% \end{table}


% \begin{table}[h!]
% \centering
% \begin{tabular}{l|c|c|c}
% \hline
% % \textbf{Model}                    & \textbf{MMLU (5 shots)} & \textbf{AlpacaEval (LC win against GPT-4)} & \textbf{MT-Bench (scored by GPT-4)} \\ \hline
% \textbf{Model}                    & \textbf{MMLU (5 shots)} & \textbf{AlpacaEval} & \textbf{MT-Bench} \\ \hline
% Mamba (1/2 attention)             & 59.26                   & 29.61                                       & 7.35                                \\ 
% Mamba2 (1/2 attention)            & 56.67                   & 25.00                                       & 7.32                                \\ 
% Mamba (1/4 attention)             & 52.68                   & 25.85                                       & 6.86                                \\ 
% Mamba2 (1/4 attention)            & 53.94                   & 20.25                                       & 6.74                                \\
% Mamba (1/8 attention)             & 49.20                   & 20.76                                       & 6.46                                \\
% Mamba2 (1/8 attention)            & 50.85                   & 20.25                                       & 6.48                                \\ 
% Mamba2 (0 attention)              & 43.19                   & 14.49                                       & 5.64                                \\
% \end{tabular}
% \caption{Model performance comparison across MMLU, AlpacaEval, and MT-Bench.}
% \end{table}

\section{Things to Discuss}

\begin{itemize}
    \item Fractal Metrics
    \item intrinsic dimension
    \item Should we include vision models or just focus on LLMs? Vision models show similar behaviors, which is an interesting finding, but might distract
    \item More datasets from MTEB, maybe GSM8k
\end{itemize}

\section{List of Results}


\newpage
\subsection{Empirical}
\begin{itemize}
    \item Used every layer of several models for linear probing 32 MTEB (classification, reranking, retrieval) datasets. Found that in all but a few cases, an intermediate layer outperformed the last layer.
    \item Implemented 6 representation evaluation metrics: prompt entropy, curvature, LiDAR, DiME, infoNCE, and RankME 
    \SubItem{Looked at how these metrics evolve across layers, scale, and training. Found remarkable trends and differences between architectures}
    \SubItem{Found similar trends for vision models. Supervised compresses in the middle, CLIP compresses a bit less, DINO even less, and MAE/JEPA are flat.}
    \SubItem{Looked at how LLMs respond to perturbed inputs.}
    \SubItem{Bimodal behavior in prompt entropy for certain datasets}
    \SubItem{In vision, metrics are known to be well correlated with downstream performance. In our results, we looked at correlation between MTEB result and metrics. All metrics had weak correlation, except for DiME which had a Pearson correlation of 0.61. Weak correlations were also found in MMLU}

    \item (TODO) metrics with Chain-of-thought. math problems
    \item (TODO) Look for correlations between metrics and perplexity
    \item (TODO)  BERT, llama-70B. for MTEB
    \item (TODO) Look at vision downstream tasks. Classification, depth estimation, segmentation. MAE, JEPA, CLIP, DINO, supervised
\end{itemize}

\subsection{Theoretical}
\begin{itemize}
    \item Many representation metrics (RankMe, NESum, pseudo-condition number, etc) can be rewritten in terms of matrix-based entropy. Shows that there is not a big a diversity of evaluations as previously thought
\end{itemize}

\subsection{Order of Results}
\begin{itemize}
    \item First figure is performance in intermediate layers
    \item X-axis is downstream task performance, y-axis is normalized metric. Collect as many points as possible
\end{itemize}

\end{document}