\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage{style/neurips/neurips_2024}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{makecell}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\newcommand{\ravid}[2]{\textcolor{blue}{Ravid: #1}}

\title{Does Representation Matter? Exploring Intermediate Layers in Large Language Models}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
Understanding what constitutes a ``good'' representation in large language models (LLMs) is a fundamental question in natural language processing. In this paper, we investigate the quality of representations at different layers of LLMs, specifically Transformers and State Space Models (SSMs). We find that intermediate layers consistently provide better representations for downstream tasks compared to final layers. To quantify representation quality, we employ existing metrics from other contexts---such as prompt entropy, curvature, and augmentation-invariance---and apply them to LLMs. Our experiments reveal significant differences between architectures, showcase how representations evolve during training, and illustrate the impact of input randomness and prompt length on different layers. Notably, we observe a bimodal behavior in entropy within intermediate layers and explore potential causes related to training data exposure. Our findings offer valuable insights into the internal workings of LLMs and open avenues for optimizing their architectures and training processes.

\end{abstract}

\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) have revolutionized natural language processing by achieving remarkable performance across a variety of tasks~\citep{muennighoff2022mteb, hendrycks2020mmlu}. Despite their success, understanding what constitutes a ``good'' representation within these models remains an open question. Specifically, how do representations at different layers contribute to downstream task performance, and how can we quantify their quality?

Most previous studies have focused mainly on final-layer representations, often overlooking the potential of intermediate layers. However, recent work suggests that intermediate layers may offer richer or more generalizable features for certain tasks \citep{bordes2022guillotine, gurnee2023language, fan2024notalllayers}. This observation prompts a deeper investigation into the layer-wise behavior of LLMs.

In this paper, we explore the quality of representations across different layers of LLMs in various settings, including different model architectures (Transformers~\citep{vaswani2017attention} vs.\ State Space Models (SSMs)~\citep{mamba}), training checkpoints, input randomness, and prompt length. Our contributions are threefold:


\begin{itemize}
    \item We demonstrate that intermediate layers consistently yield much better representations for downstream tasks than final layers.
    \item We apply and adapt existing metrics—such as prompt entropy, curvature, and augmentation-invariance—to quantify representation quality in LLMs.
    \item We analyze how these metrics vary across different settings, including architectural differences (Transformers vs.\ SSMs), training progression, input randomness, and prompt length.
\end{itemize}



Furthermore, we uncover significant differences in the behavior of these metrics between Transformers and SSMs. For example, we observe a bimodal distribution in entropy within intermediate layers and investigate potential causes, such as the influence of training data examples.

Our findings provide new insights into the internal mechanisms of LLMs and offer practical guidance for model selection and architectural design in future research.


\begin{table}[!t]
\centering
\caption{MTEB Downstream Task Performance Using Representations from Different Layers}
\label{tab:downstream_performance}
\scalebox{0.6}{
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{\makecell{Number of Tasks where Best Performance \\ is not in Last Layer}} & \textbf{Avg. Last Layer Performance} &  \textbf{Avg. Best Layer Performance}\\
\midrule
LLM2Vec 8B (Transformer) & 100\% & 64.7\% & 66.8\%\\
Pythia 410M (Transformer) & 96.6\% & 49.8\% & 53.3\% \\
Mamba 130M (SSM) & 100\% & 46.9\% & 50.9\% \\
\bottomrule
\end{tabular}
}
\end{table}


\section{Related Work}
\label{sec:related}

Understanding representations in neural networks has been a topic of extensive research. \citet{alain2016understanding} analyzed hidden representations to interpret neural networks' learning processes. \citet{raghu2017svcca} introduced Singular Vector Canonical Correlation Analysis to compare representations across layers and networks. In the context of Transformers, \citet{liu2019linguistic} studied the linguistic knowledge captured at different layers, finding that lower layers encode more syntactic information while higher layers capture semantic features. A similar work~\citep{jin2024conceptdepth} showed that semantic concepts are learned in intermediate layers. They proposed a layer-wise probing technique to discover exactly in which layer concepts are formed. On the other hand, state-space models have been less explored in this regard. \citet{mamba} introduced MAMBA, an SSM architecture capable of handling long sequences efficiently. However, comparative studies between SSMs and Transformers at the representation level remain scarce.

Metrics like entropy and curvature have been used in other contexts to analyze representations. \citet{shwartz2017opening, shwartz2022information} discussed the Information Bottleneck principle, suggesting that networks learn to compress representations. \citet{hosseini2024curvature} introduced curvature as a measure of representational dynamics in recurrent networks. Several works in the vision domain proposed unsupervised representational quality metrics that are strongly correlated with accuracy on downstream tasks~\citep{garrido2023rankme, agrawal2022alphareq, thilak2023lidar}. The RankMe measure can be shown to be a measure of entropy known as matrix-based entropy, which we use in our subsequent analysis.

Our work bridges these areas by applying and adapting such metrics to LLMs, providing a novel perspective on representation quality across architectures and training stages.

\section{Methodology}
\label{sec:methodology}

\subsection{Definitions}

Let $\mathbf{Z} \in \mathbb{R}^{N \times D}$ represent a batch of $N$ samples, each with dimensionality $D$. The vector $z_i$ denotes the $i$-th row of $Z$. We denote the $i$-th largest eigenvalue of a matrix $\mathbf{M}$ as $\lambda_i(\mathbf{M})$, and the trace of $\mathbf{M}$ by $\operatorname{tr}(\mathbf{M})$. Input sequences are denoted by $\mathbf{x} \in \mathbb{R}^{L \times d}$ and output sequences by $\mathbf{y} \in \mathbb{R}^{L \times d}$, where $L$ is the sequence length and $d$ is the feature dimension. 

% For detailed mathematical formulations and parameter definitions, refer to Appendix \ref{appendix:definitions}.

%We define a batch $Z \in \mathbb{R} ^ {\textrm{N} \times \textrm{D}}$ consisting of $N$ samples that are $D$-dimensional.  Let $\lambda_i(\cdot)$ and $\textrm{tr}(\cdot)$ denote the $i$-th largest eigenvalue and trace of a matrix, respectively.

%We denote input sequences as $\mathbf{x} \in \mathbb{R}^{L \times d}$ and output sequences as $\mathbf{y} \in \mathbb{R}^{L \times d}$, where $L$ is the sequence length and $d$ is the feature dimension.

\subsection{Architectures}
\label{sec:architectures}

In this study, we compare two prominent architectures: Transformer-based models \citep{vaswani2017attention} and State Space Models (SSMs) \citep{mamba}. Transformers utilize self-attention mechanisms to capture long-range dependencies within input sequences, enabling parallel processing and effective encoding of complex patterns. On the other hand, SSMs employ recurrent dynamics to handle sequential information with linear time and memory complexity, offering efficiency in processing longer sequences. Despite their differing approaches, both architectures aim to generate rich and meaningful representations across multiple layers. For detailed mathematical formulations and parameter configurations of each architecture, please refer to Appendix \ref{appendix:architectures}.




%We define $\mathbf{x} \in \mathbb{R}^{L \times d}$ and $\mathbf{y} \in \mathbb{R}^{L \times d}$ are the input and output sequences, respectively.

%Below we describe the building blocks of Transformer and State Space Models (SSMs).


\subsection{Representation Evaluation Metrics}
% \label{sec:metrics}
% To quantify the quality of representations across layers, we employ two categories of metrics: token embedding diversity metrics and augmentation-invariance metrics. 
% We rigorously introduce each of the following metrics in Appendix \ref{sect:appendix-metrics}.

% Token embedding diversity metrics evaluate the variability and richness of the representations at the token level within a single sequence. We employ prompt entropy~\citep{wei2024large} and curvature~\citep{hosseini2024curvature}. Of particular interest is the prompt entropy, which measures the amount of compression in a prompt's token representations. 

% Augmentation-invariance metrics assess the robustness of representations to augmentations on the input prompt. We employ DiME~\citep{skean2023dime}, infoNCE~\citep{oord2018representation}, and LiDAR~\citep{thilak2023lidar}. We provide full details and examples of the augmentation process in Appendix~\ref{appendix:prompt-augmentation}. 


In this section, we introduce three categories of evaluation metrics to analyze the representations of LLMs. These categories are token embedding diversity metrics, batch embedding diversity metrics, and augmentation-invariance metrics.


The first category of metrics we examine are token embedding diversity metrics, which quantify the information content \textit{at the prompt level} by assessing the representations of tokens within a prompt. These metrics seek to understand how LLMs transform the prompt during a forward pass.

\subsubsection{Prompt Entropy}
\label{sect:prompt-entropy}

The first measure of token embedding diversity we call prompt entropy. We follow the work of \cite{wei2024large} and use $\alpha$-order matrix-based entropy \cite{giraldo2014measures, skean2023dime, skean2024frossl}, which serves as a tractable surrogate for traditional Rényi’s $\alpha$-order entropy \cite{renyi1961measures}. The quantity is calculated using a similarity kernel $\kappa$ on a batch of samples drawn from a distribution, without making explicit assumptions on what the true distribution is. The choice of kernel $\kappa$ is flexible and can be any infinitely divisible kernel such as the Gaussian kernel, linear kernel, or Laplacian kernel, among others. For this work, we restrict ourselves to the linear kernel $\kappa(a, b) = a b^T$. This choice is motivated by the linear representation hypothesis \cite{parklinear2024} which finds that large language model representations encode high-level concepts such as truth \cite{burns2022dl}, honesty \cite{mallen2024eliciting}, and part-of-speech \cite{mamou2020emergence} in linearly separable manifolds.


To compute matrix-based entropy, we first construct the Gram matrix $\mathbf{K}_{\mathbf{Z}} = \kappa \left(Z, Z\right) = Z Z^T \in \mathbb{R}^{\textrm{N} \times \textrm{N}}$ consisting of all pairwise evaluations of the $N$ points in $Z$. Then the matrix-based entropy of order $\alpha>0$ is defined as:

\begin{equation}
\label{eq:matrix-based-entropy}
S_{\alpha}\left(Z\right) = \frac{1}{1-\alpha}\log{\left[ \sum_{i=1}^N \left(\frac{\lambda_i(\mathbf{K}_{\mathbf{Z}})}{\textrm{tr}(\mathbf{K}_{\mathbf{Z}})} \right)^{\alpha}\right]}
\end{equation}


 One way to interpret Eq. \ref{eq:matrix-based-entropy} is as the $\alpha$-order Rényi entropy of the Gram matrix eigenvalues\footnote{The non-zero eigenvalues of the Gram matrix $Z Z^T$ are equivalent to those of the covariance matrix $Z^T Z$. Using the covariance matrix instead of the Gram matrix in Eq. \ref{eq:matrix-based-entropy} makes no difference and is more computationally efficient if $D < N$.}. Notice how each eigenvalue is divided by $\textrm{tr}(\mathbf{K}_{\mathbf{Z}})$ before being raised to the $\alpha$ power. This is so that the eigenvalues of $\mathbf{K}_{\mathbf{Z}}$ sum to one (because  $\textrm{tr}(\cdot) = \sum_{i=1}^n \lambda_i(\cdot)$), which is a necessary condition to treat the eigenvalues as a probability distribution. Futhermore, each eigenvalue of $\mathbf{K}_{\mathbf{Z}}$ signifies the variance of samples in a particular principal component direction~\cite{scholkopf2018learning}. If entropy is low, then the eigenvalues form a heavy-tail distribution which implies that a few components dominate the variance of samples in $Z$. On the other hand, at maximum entropy, the eigenvalues form a uniform distribution and samples are spread equally in all directions. Matrix-based entropy is reminiscent of the LogDet entropy which uses the determinant of $\mathbf{K}_{\mathbf{Z}}$ to capture how much "volume" a dataset occupies~\cite{shwartz2023information, zhouyin2021understanding}. The LogDet entropy is given by $S_{\textrm{LogDet}}(Z) = \log \det (\mathbf{K}_{\mathbf{Z}}) - \log 2$. One can use Jensen's inequality to show that the LogDet entropy is a lower bound of Eq \ref{eq:matrix-based-entropy} when $\lim_{\alpha \rightarrow 1}$ (Appendix J.4 of~\cite{shwartz2023information}).
 
 Depending on the choice of $\alpha$, several special cases of matrix-based entropy can be recovered. In particular, when $\lim_{\alpha \rightarrow 1}$ it equals Shannon entropy (also referred to as von Neumann entropy in quantum information theory \cite{bach2022information, boes2019neumann}), and when $\alpha=2$ it equals collision entropy. Interestingly, the case of $\alpha=2$ can be calculated without explicit eigendecomposition \cite{skean2024frossl}. We show in the Appendix Figure \ref{fig:power_law_entropy} how varying values of $\alpha$ affect the matrix-based entropy of Gram matrices with eigenvalues distributed with a $\beta$-power law such that $\lambda_i = i^{-\beta}$. It is shown that for larger values of $\alpha$, smaller eigenvalues contribute more to the entropy.


First introduced in \cite{hosseini2024curvature}, curvature is metric that is biologically inspired by how the brain represents sequential information \cite{henaff2019perceptual}. Specifically, curvature measures how the angle between adjacent sequence element representations are changing as the sequence progresses. For a sequence of token representations $z_1, z_2, \cdots, z_n$, and difference vectors $v_k = z_{k+1} - z_k$, the average curvature of the sequence is given by:

\begin{equation}
\bar{C} = \frac{1}{N} \sum_{i=1}^N \arccos \left( \frac{v_{k+1}^T v_k}{||v_{k+1}|| \cdot ||v_k||} \right)
\label{eq:average-curvature}
\end{equation}

\subsubsection{Batch Embedding Diversity Metrics}

Batch embedding diversity metrics evaluate the diversity of representations across different input sequences within a batch. These metrics help in understanding how well the model captures the variety present in the dataset, reflecting the model's ability to generate distinct and meaningful embeddings for different inputs. High batch diversity suggests that the model effectively discriminates between different sequences, which is crucial for tasks that require nuanced understanding and differentiation between multiple contexts.


The second category of metrics we examine are batch embedding diversity metrics, which quantify the information content \textit{at the batch level} by assessing the representations of prompts within a dataset. Such measures could be applied at the dataset level instead of the batch level, but a sufficiently large batch suffices and is more tractable. Assuming the batch (dataset) is diverse, these measures capture how well that diversity is captured by the model.


\paragraph{Batch Entropy}
We aggregate embeddings from all sequences in a batch and compute the matrix-based entropy similarly to prompt entropy.

\subsubsection{Batch Entropy}
Similarly to prompt entropy as described in Section \ref{sect:prompt-entropy}, we can apply matrix-based entropy at the batch level. To do so, 

\section{Experiments}
\label{sec:experiments}

\subsection{Intermediate Layers Yield Better Representations for Downstream Embedding Tasks}
First, we evaluate the performance of the representations of each layer in downstream tasks in the Massive Text Embedding Benchmark (MTEB)~\citep{muennighoff2022mteb}. This benchmark is designed to test the performance of LLMs on various embedded tasks. We chose 32 tasks that range from classification, clustering, and
re-ranking. We evaluated each layer of Pythia 410M, Mamba 130M, and LLM2Vec-unsup-simcse~\citep{behnamghader2024llm2vec}.

Interestingly, the intermediate layers consistently outperform the final layers in all architectures (Table~\ref{tab:downstream_performance}). Using the best-performing layer to compute the average accuracy yields at least a 2\% improvement. Similar findings were shown  in~\citep{fan2024notalllayers} for generation tasks, while our results are for embedding tasks.


\subsection{Downstream Performance is Negatively Correlated with Entropy}
We evaluate the relationship between the prompt entropy of different layers with the performance on downstream tasks in the Massive Multitask Language Understanding (MMLU)~\citep{hendryckstest2021} dataset. The MMLU is designed to assess the comprehensive knowledge and multitask accuracy of language models. It comprises a wide range of subjects organized into 57 tasks, covering topics from elementary mathematics to professional law.

We compared two models of the same parameter size, Llama3-8B and Mamba2-8B. Despite having the same parameter size, Llama3 $63.85 \pm0.38 \%$ outperforms Mamba2 $26.76\pm0.37$ significantly. We hypothesize that LLama3's superior performance is due to compression in its intermediate layers, as shown in Figure \ref{fig:metrics-across-architectures}, enabling it to filter irrelevant information, which is useful for tasks like MMLU. Additionally, we find a strong negative correlation (-0.43) between the second and later layers of LLama3's representations and MMLU task performances \ref{fig:with_logit22}. In contrast, Mamba2 shows neither such compression nor correlation with performance \ref{fig:with_logit11}.


\subsection{Experimental Setup for Quantifying Representation Quality}
\label{sect:metrics-experiments}

Next, we apply the metrics described in Section~\ref{sec:metrics} to measure the quality of layer-wise representations. We conduct experiments on Transformers, SSMs and Pythia~\citep{biderman2023pythia} using models of varying sizes to analyze the impact of architecture and capacity. We utilize the WikiText-103 dataset \citep{merity2016pointer} and an instruction medical dataset \citep{ruslanmv2024} to test different input complexities.


%\subsection{Behavior of metrics across model architectures}
\subsubsection{Differences Between Architectures}
\label{sect:metrics-across-architectures}

Our analysis reveals key differences in representation quality between Transformer-based architectures such as Pythia and SSMs such as Mamba across multiple metrics, including entropy, InfoNCE, LIDAR, and DiME. Figure \ref{fig:metrics-across-architectures} illustrates how these metrics vary as a function of model depth, represented as a percentage of the total number of layers, allowing for fair comparison between models of different depths.

For entropy and LIDAR metrics, Pythia shows a significant reduction in values at intermediate layers, suggesting compression and information consolidation, while Mamba maintains more stable values, indicating less compression in intermediate representations. In contrast, Mamba exhibits lower values for the DiME and InfoNCE metrics than Pythia, implying less variability in intermediate representations.

The effect and changes in these metrics across the intermediate layers are generally less pronounced in Mamba than in Pythia. This indicates that Mamba maintains more stable representations throughout its depth, whereas Pythia exhibits greater shifts and transformations in its intermediate representations, potentially leading to different strengths in how these models encode and utilize information for downstream tasks.

\begin{figure}[!t]
  \centering
\includegraphics[width=0.7\linewidth]{figures/metrics_comparison_pythia_mamba_llama.pdf}
  \caption{\textbf{Intermediate layers in Mamba show more stable representation values than Pythia, which exhibits more pronounced changes.} Representation evaluation metrics across layers in Pythia 410M and Mamba 370M architectures. The x-axis is the depth percentage of the model to allow fair comparison between models with different numbers of layers.}
  \label{fig:metrics-across-architectures}
\end{figure}

\subsubsection{Impact of Training Progression}

To understand how representation quality evolves during training, we use the training checkpoints provided by Pythia, examining how the metrics change across different layers as training progresses. Figure \ref{fig:metrics_across_training} shows the evaluation metrics for logarithmically spaced training checkpoints, from the initial step to the final step at 143k.

The training dynamics reveal that the most significant changes occur in the intermediate layers. Specifically, the prompt entropy decreases in the middle layers during training, suggesting that the model learns to better compress and abstract the information within a prompt. This compression indicates that the model is becoming more efficient in representing complex information as training progresses. In contrast, the InfoNCE metric peaks in the middle layers, indicating increased distinctiveness of representations, while the LiDAR and DiME metrics both decrease, suggesting reduced variability in certain directions of the representation space.

Interestingly, the metrics in the initial layers remain relatively stable throughout the training, which we believe supports the detokenization hypothesis discussed in \citep{lad2024remarkable}. This indicates that the initial layers primarily focus on mapping input tokens to an initial embedding space, with little change in their representation dynamics during training.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/metrics_at_pythia_checkpoints.pdf}
  \caption{\textbf{Training effects are most pronounced in the intermediate layers, with distinct dynamics for different metrics.} Representation evaluation metrics across layers at various training checkpoints, ranging from step 1 to the final step at 143k. The x-axis represents the depth percentage of the model, showing how training affects different layers, particularly in the intermediate stages.}
  \label{fig:metrics_across_training}
\end{figure}






\subsubsection{Prompt Entropy under Extreme Input Conditions}
To further understand how prompt entropy behaves under different input conditions, we examine the effect of increasingly extreme prompts on the model's representations. Specifically, we analyze how the prompt entropy changes across layers of the Pythia 410M model when the input prompts exhibit high levels of token repetition, randomness, or increased length.


We design three types of extreme prompts:

\begin{enumerate}
    \item \textbf{Prompts with Increasing Token Repetition}: We take 1000 regular prompts from the WikiText dataset and randomly replace tokens with a fixed token from the prompt at varying probabilities $p$. As $p$ increases, the amount of repetition in the prompt increases.

    \item \textbf{Prompts with Increasing Token Randomness}: We randomly replace tokens in the prompts with random tokens from the vocabulary at varying probabilities $p$. This introduces increasing levels of randomness into the prompts.

    \item \textbf{Random Prompts of Increasing Length}: We generate random prompts by sampling tokens uniformly from the vocabulary, creating prompts of varying lengths $T$.
\end{enumerate}

%\subsection{Behavior of prompt entropy for increasingly extreme prompts}
Figure \ref{fig:pythia-increasing-intensity} illustrates how the normalized and unnormalized prompt entropy changes across layers for these extreme prompts. Our key findings are as follows:

\textbf{(1) Increasing token repetition leads to a decrease in entropy in the intermediate layers.} As repetition increases, the model compresses redundant information, resulting in lower entropy values in the middle layers. This indicates that the model effectively identifies and encodes repetitive patterns.

\textbf{(2) Increasing token randomness results in higher entropy, especially in initial layers.} Introducing random tokens increases the diversity of token representations, leading to higher entropy. The initial layers are more affected, suggesting sensitivity to input noise.

\textbf{(3) Prompt length affects entropy in both normalized and unnormalized forms.} Unnormalized entropy naturally increases with prompt length due to more tokens. While not displayed, the normalized entropy shows sublinear growth, indicating that each additional token contributes less to the overall diversity as the prompt becomes longer.

These observations highlight how extreme input conditions impact the model's internal representations, particularly in the intermediate layers. The model exhibits different compression and encoding behaviors depending on the nature of the input perturbations, which provides valuable insight into its processing mechanisms.


\begin{figure}[!t]
        \centering
        \begin{subfigure}[b]{0.32\textwidth}
                \centering
                \includegraphics[width=\linewidth]{figures/pythia_increasing_repetition.pdf}
                \caption{Repetition}
                \label{fig:pythia_increasing_repetition}
        \end{subfigure}\hfill
        \begin{subfigure}[b]{0.32\textwidth}
              \centering
              \includegraphics[width=\linewidth]{figures/pythia_increasing_randomness.pdf}
                \caption{Randomness}
                \label{fig:pythia_increasing_randomness}
        \end{subfigure}\hfill
        \begin{subfigure}[b]{0.32\textwidth}
                \centering
                \includegraphics[width=\linewidth]{figures/pythia_random_of_differing_lengths_raw.pdf}
                \caption{Random Prompt Length}
                \label{fig:prompt-random-raw}
        \end{subfigure}
\caption{\textbf{Prompt entropy across layers of Pythia 410M under different extreme input conditions.} (a) Increasing token repetition leads to decreased entropy in intermediate layers. (b) Increasing token randomness results in higher entropy, especially in the initial layers. (c) Unnormalized prompt entropy of random prompts increases with prompt length due to the larger number of tokens. These results demonstrate how the model's internal representations adapt to different types of input perturbations.}
\label{fig:pythia-increasing-intensity}   
\end{figure}



\subsection{Bimodal Behavior in Prompt Entropy}
While analyzing the average prompt entropy across different layers, we discovered an intriguing phenomenon: a clear bimodal distribution in the entropy values at certain layers in transformer models, but not SSMs. Figure \ref{fig:all-models-bimodal} shows the entropy distributions for WikiText and the ai-medical-chatbot datasets\citep{ruslanmv2024}. Notably, a pronounced bimodal distribution is observed in the middle layers for the ai-medical-chatbot dataset. This behavior suggests that the model processes some prompts fundamentally differently than others at these intermediate stages. We investigated the causes of this behavior in Appendix \ref{appendix:bimodal-investigation} and ruled out prompt length, semantic complexity, or overlap with training data. The underlying cause is currently an open question.




\section{Discussion and Conclusion}
\label{sec:discussion_conclusion}

In this study, we thoroughly examined the quality of layer-wise representations in LLMs, specifically comparing Transformer-based architectures and SSMs. Using a variety of evaluation metrics, including prompt entropy, curvature, InfoNCE, LIDAR, and DiME, we found several key insights into how these models process and encode information across different layers and under various conditions.

Our findings indicate that intermediate layers consistently provide superior representations for downstream tasks compared to final layers. This highlights the importance of using intermediate representations for feature extraction and transfer learning applications. Additionally, significant architectural differences were observed: Transformers exhibited more dynamic changes in metrics such as entropy and InfoNCE in their intermediate layers, suggesting a higher degree of information compression and variability. In contrast, SSMs maintained more stable representations, reflecting a different approach to information encoding that emphasizes consistency.

During training progression, the most substantial changes in representation quality occurred in the intermediate layers, with prompt entropy decreasing and InfoNCE peaking, indicating enhanced compression and distinctiveness of representations. This underscores the critical role of intermediate layers in the learning process and suggests potential avenues for optimizing training strategies to further improve representation quality.

LLMs demonstrated distinct behaviors under extreme input conditions, such as increased token repetition, randomness, and prompt length. Transformers showed significant variations in entropy and other metrics in response to input perturbations, particularly in intermediate layers, whereas SSMs maintained more stable representations. This suggests that transformers are more adaptable and sensitive to diverse input scenarios, while SSMs offer greater robustness and consistency. A particularly intriguing observation was the presence of bimodal entropy distributions in the intermediate layers, especially within the ai-medical-chatbot dataset. Despite extensive investigations, the cause of this bimodality remains unresolved. 

In conclusion, our research advances the understanding of internal representation dynamics in LLMs, highlighting the pivotal role of intermediate layers and the distinct behaviors of different architectures. These findings not only contribute to the theoretical knowledge of model representations, but also offer practical guidance for optimizing model design, training, and application. Future work should delve deeper into the causes of phenomena such as bimodal entropy distributions and explore the development of new metrics tailored specifically to LLMs to further enhance representation evaluation.




\bibliographystyle{styles/neurips/neurips_2024}
\bibliography{references}

%\bibliographystyle{unsrt}
%\bibliography{references}
\newpage
 \appendix



% \section{Detailed Definitions}
% \label{appendix:definitions}

% Here, we provide comprehensive definitions and mathematical formulations for the metrics and concepts introduced in the main text. This includes the derivation of matrix-based entropy, curvature calculations, and other relevant measures used in our analysis.


\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.8\textwidth}
                \centering
                \includegraphics[width=\linewidth]{figures/Pythia_bimodal_entropies.pdf}
                \caption{Pythia 410M}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\textwidth}
              \centering
              \includegraphics[width=\linewidth]{figures/mamba_bimodal_entropies.pdf}
                \caption{Mamba 370M}
        \end{subfigure}
        \begin{subfigure}[b]{0.8\textwidth}
              \centering
              \includegraphics[width=\linewidth]{figures/Llama3_bimodal_entropies.pdf}        
                \caption{Llama3 8B}
        \end{subfigure}
\caption{\textbf{Bimodal distribution of prompt entropies observed in intermediate layers.} The distributions of prompt entropies for WikiText and ai-medical-chatbot datasets are shown for Pythia, Mamba, and Llama3 models. The middle column highlights the layer with the highest Dip Test score \citep{hartigan1985dip}, which measures the degree of multimodality in the entropy distribution.}
\label{fig:all-models-bimodal}   
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/mmlu_llama3-8b.pdf}
  \caption{Entropy vs Accuracy of LLama3-8B on MMLU tasks. Each point represents a task in MMLU}
  \label{fig:with_logit22}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.58\linewidth]{figures/mmlu_mamba2-8b.pdf}
  \caption{Entropy vs Accuracy of Mamba2-8B on MMLU tasks}
  \label{fig:with_logit11}
\end{figure}

\section{Investigation into Bimodal Distribution of Entropies}
\label{appendix:bimodal-investigation}
To determine the underlying cause of this bimodal distribution of prompt entropies, we conducted several experiments to see if specific properties of the dataset could explain this phenomenon. Our goal was to understand whether the bimodality was related to characteristics such as prompt length, semantic complexity, or overlap with training data.

\paragraph{Effect of Prompt Length}

Initially, we hypothesized that the bimodality might be caused by variations in prompt length. If one mode corresponded to shorter prompts and the other to longer prompts, it could indicate different processing strategies. However, since the entropy values were normalized and theoretically invariant to length, this was unlikely. Upon further analysis, we confirmed that prompt length did not significantly correlate with the observed bimodality.

\paragraph{Manual Examination of Prompts}

We then manually examined prompts from each mode of the distribution to identify any distinguishing features, such as difficulty or specific types of medical terminology. Despite this effort, we found no significant differences between the prompts in either mode. Both modes contained a similar range of medical complexity and varied use of terminology, suggesting that the model's entropy was not merely a reflection of the difficulty or specificity of the input.

\paragraph{Training Set Overlap}

Next, we investigated whether the low entropy mode might be associated with prompts that were very similar to samples seen during training. Given that both the ai-medical-chatbot dataset and PILE \citep{gao2020pile} (which Mamba, Pythia, and possibly Llama3 were trained on) contained medical articles from PubMed, we hypothesized that overlap with training data could lead to more confident, lower-entropy representations. To test this, we implemented a BM25 index \citep{bm25s} to quickly search for identical or highly similar articles between the two datasets.

While we did find identical articles between the ai-medical-chatbot dataset and PILE, these articles were evenly distributed across both modes of the bimodal entropy distribution. This suggests that the presence of training set overlap does not explain the bimodal behavior, and the underlying cause remains an open question.

\section{Representation Evaluation Metrics}
\label{sect:appendix-metrics}

\subsection{Token Embedding Diversity Metrics}
\label{sect:token-embedding-diversity-metrics}
Token embedding diversity metrics evaluate the variability and richness of the representations at the token level within a single sequence. These metrics are designed to capture how distinctively each token is represented within the context of the entire prompt, providing insight into how effectively the model encodes information and differentiates between different parts of the input.

\paragraph{Prompt Entropy}
Following \citet{wei2024large}, we use the $\alpha$-order matrix-based entropy \citep{giraldo2014measures} as a surrogate for Rényi entropy. For a sequence of token representations $\mathbf{Z} \in \mathbb{R}^{L \times d}$, the Gram matrix is $\mathbf{K_Z} = \mathbf{Z}\mathbf{Z}^\top$. The entropy is computed as:

\begin{equation}
\label{eq:matrix-based-entropy}
    S_{\alpha}(\mathbf{Z}) = \frac{1}{1 - \alpha} \log \left( \sum_{i=1}^{L} \left( \frac{\lambda_i(\mathbf{K_Z})}{\operatorname{tr}(\mathbf{K_Z})} \right)^{\alpha} \right).
\end{equation}

In this context, prompt entropy measures the diversity and dispersion of token embeddings within a given sequence. Higher entropy values imply a richer and more varied representation of the tokens, suggesting that the model captures more nuanced information across the sequence. This helps in understanding how effectively the model encodes diverse features and maintains the complexity of the input, making it a useful metric for evaluating the quality of intermediate layer representations. Unless otherwise specified, we use the limit case of $\alpha=1$ in our calculations. Details and behavior for different $\alpha$ are shown in Appendix \ref{appendix:entropy}.


\paragraph{Curvature}
As introduced by \citet{hosseini2024curvature}, curvature measures the change in direction between adjacent token embeddings. To calculate curvature, we first we calculate the difference between two adjacent vectors as $\mathbf{v}_k = \mathbf{z}_{k+1} - \mathbf{z}_k$. The average curvature of a prompt is:

\begin{equation}
    \bar{C} = \frac{1}{L-2} \sum_{k=1}^{L-2} \arccos\left( \frac{\mathbf{v}_{k+1}^\top \mathbf{v}_k}{\|\mathbf{v}_{k+1}\| \|\mathbf{v}_k\|} \right).
\end{equation}

\subsection{Augmentation Invariance Metrics}

These metrics assess the robustness of representations to input augmentations. Because augmentation may change the length of the tokenized prompt, the token embedding diversity metrics described in~\ref{sect:token-embedding-diversity-metrics} are no longer suitable. Instead, we average the tokens to get a single vector representing each prompt and use the metrics described below to measure the similarity between two augmentations of the same prompt. We refer to the two batches of augmented prompts as $Z_1 \in \mathbb{R}^{N \times D}$ and $Z_2 \in \mathbb{R}^{N \times D}$, where $N$ is the batch size and row $i$ in both matrices correspond to the same original prompt. We provide full details and examples of the augmentation process in Appendix~\ref{appendix:prompt-augmentation}.

\paragraph{InfoNCE}
We compute a mutual information lower bound using the InfoNCE loss \citep{oord2018representation} between two views. This loss is widely used to train augmentation-invariant networks in self-supervised learning for vision and is well-suited to capturing the semantic similarity underlying the augmented prompts~\citep{chen2020simclr, chen2020mocov2, shwartz2024compress, NEURIPS2023_b63ad8c2}.

\paragraph{DiME}
Similarly to infoNCE, the quantity DiME~\citep{skean2023dime,chen2023sudden} can be used to measure a mutual information-like between the two augmented batches of prompts. DiME is based on the matrix-based entropy described in Eq. \ref{eq:matrix-based-entropy}.  Roughly put, DiME measures the quality of the pairings between $Z_1$ and $Z_2$ as compared to between $Z_1$ and $\Pi Z_2$ for some permutation matrix $\Pi$. A low value of DiME means the row pairings between $Z_1$ and $Z_2$ are no better than random pairings.

\paragraph{LiDAR}
The LiDAR quantity~\citep{thilak2023lidar} was proposed to act as a representation quality metric. Unlike matrix-based entropy which looks at the principal component variances, LiDAR uses the linear discriminant component variances. To compute the linear discriminant analysis (LDA) matrix, LiDAR uses augmentations to construct the class scatter matrix. In our setting, we use $N$ classes (each corresponding to different prompt)  with $J$ samples per class (each sample within a class being a different augmentation of the same prompt). Due to the more complex requirements of computing the LDA matrix, we use $J=16$ rather than $J=2$ like in DiME or infoNCE.


\section{Architectural Details}
\label{appendix:architectures}

In this section, we elaborate on the specific architectures of Transformers and State Space Models (SSMs). We outline the mathematical foundations, including the weight matrices, attention mechanisms for Transformers, and the state transition matrices for SSMs. Detailed equations and parameter configurations are provided to facilitate replication and deeper understanding.

\subsection{Transformer}
The Transformer architecture \citep{vaswani2017attention} utilizes self-attention mechanisms. Given an input $\mathbf{x}$, the key ($\mathbf{K}$), query ($\mathbf{Q}$), and value ($\mathbf{V}$) matrices are computed as:


\begin{equation}
    \mathbf{Q} = \mathbf{x}\mathbf{W}_Q, \quad \mathbf{K} = \mathbf{x}\mathbf{W}_K, \quad \mathbf{V} = \mathbf{x}\mathbf{W}_V,
\end{equation}

where $\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d \times d_k}$ and $\mathbf{W}_V \in \mathbb{R}^{d \times d_v}$ are learned weights.

The attention weights are calculated using:

\begin{equation}
    \mathbf{A} = \operatorname{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}} + \mathbf{M}\right),
\end{equation}

where $\mathbf{M}$ is a mask to enforce causality in autoregressive tasks.

The output is then:

\begin{equation}
    \mathbf{y} = \mathbf{A}\mathbf{V}.
\end{equation}



%The self-attention block of the transformer ~\citep{vaswani2017attention} consists of key ($\mathbf K \in \mathbb{R}^{L \times d_k}$), query($\mathbf Q \in \mathbb{R}^{L \times d_k}$) and value ($\mathbf V \in \mathbb R^{L \times d_v}$) that are learned by transforming input $x$ using the following weight matrices $W_Q \in \mathbb{R}^{d \times d_k}$, $W_K \in \mathbb{R}^{d \times d_k}$ and $W_V \in \mathbb{R}^{d \times d_v}$. They are computed as follows:
%\begin{equation}
%    \mathbf{Q} = \mathbf{x}W_Q, \quad  \mathbf{K} = \mathbf{x}W_K, \quad \mathbf{V} = \mathbf{x}W_V.
%\end{equation}
%Keys, queries, and values are then combined in the attention block to produce the output
%Finally the attention output is calculated as:
%\begin{equation}\label{eqn:attention}
%    \mathbf{A} = softmax\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right),
%\end{equation}

%where $A$ is the attention weights which has a lower-triangular structure.

%And final outoput is calculated as:
%\begin{equation}\label{eqn:attention}
%    \mathbf{y} = A \mathbf{V},
%\end{equation}

\subsection{State Space Models}
\label{sec:ssm}

SSMs \citep{mamba} model sequences using recurrent dynamics. The hidden state $\mathbf{h}_t$ and output $\mathbf{y}_t$ at time $t$ are updated as:

\begin{align}
    \mathbf{h}_t &= \mathbf{A}\mathbf{h}_{t-1} + \mathbf{B}\mathbf{x}_t, \\
    \mathbf{y}_t &= \mathbf{C}\mathbf{h}_t + \mathbf{D}\mathbf{x}_t,
\end{align}

where $\mathbf{A} \in \mathbb{R}^{n \times n}$, $\mathbf{B} \in \mathbb{R}^{n \times d}$, $\mathbf{C} \in \mathbb{R}^{d \times n}$, and $\mathbf{D} \in \mathbb{R}^{d \times d}$ are learned parameters.



%In state space models~\citep{gu2023mamba}, the output $\mathbf{y}$ is computed based on a dynamic recurrence of the input at each time step $i$:

%\begin{align}
%    h_{i} &= A_{i}h_{i-1} + B_ix_i \\
%    y_i &= C_ih_i + D_i x_i,
%\end{align}

%where $h_i$ denotes the latent state of the system, and the dynamic matrices $A_i, B_i, C_i, D_i$ of corresponding dimensions represent the parameters learned by the model.




\section{Behavior of Matrix-based Entropy for different choices of $\alpha$}
\label{appendix:entropy}

One way to interpret Eq. \ref{eq:matrix-based-entropy} is as the $\alpha$-order Rényi entropy of the Gram matrix eigenvalues\footnote{The non-zero eigenvalues of the Gram matrix $Z Z^T$ are equivalent to those of the covariance matrix $Z^T Z$. Using the covariance matrix instead of the Gram matrix in Eq. \ref{eq:matrix-based-entropy} makes no difference and is more computationally efficient if $D < N$.}. Notice how each eigenvalue is divided by $\textrm{tr}(\mathbf{K}_{\mathbf{Z}})$ before being raised to the $\alpha$ power. This is so that the eigenvalues of $\mathbf{K}_{\mathbf{Z}}$ sum to one (because  $\textrm{tr}(\cdot) = \sum_{i=1}^n \lambda_i(\cdot)$), which is a necessary condition to treat the eigenvalues as a probability distribution. Furthermore, each eigenvalue of $\mathbf{K}_{\mathbf{Z}}$ signifies the variance of samples in a particular principal component direction~\cite{scholkopf2018learning}. If entropy is low, then the eigenvalues form a heavy-tail distribution which implies that a few components dominate the variance of samples in $Z$. On the other hand, at maximum entropy, the eigenvalues form a uniform distribution and samples are spread equally in all directions. Matrix-based entropy is reminiscent of the LogDet entropy which uses the determinant of $\mathbf{K}_{\mathbf{Z}}$ to capture how much "volume" a dataset occupies~\cite{shwartz2023information, zhouyin2021understanding}. The LogDet entropy is given by $S_{\textrm{LogDet}}(Z) = \log \det (\mathbf{K}_{\mathbf{Z}}) - \log 2$. One can use Jensen's inequality to show that the LogDet entropy is a lower bound of Eq \ref{eq:matrix-based-entropy} when $\lim_{\alpha \rightarrow 1}$ (Appendix J.4 of~\cite{shwartz2023information}).
 
 Depending on the choice of $\alpha$, several special cases of matrix-based entropy can be recovered. In particular, when $\lim_{\alpha \rightarrow 1}$ it equals Shannon entropy (also referred to as von Neumann entropy in quantum information theory \cite{bach2022information, boes2019neumann}), and when $\alpha=2$ it equals collision entropy. Interestingly, the case of $\alpha=2$ can be calculated without explicit eigendecomposition \cite{skean2024frossl}. We show in the Appendix Figure \ref{fig:power_law_entropy} how varying values of $\alpha$ affects the matrix-based entropy of Gram matrices with eigenvalues distributed with a $\beta$-power law such that $\lambda_i = i^{-\beta}$. It is shown that for larger values of $\alpha$, smaller eigenvalues contribute more to the entropy.
 


\begin{figure}[!b]
  \begin{center}
      \includegraphics[width=0.4\linewidth]{figures/power_law_entropy.pdf}
  \end{center}
  \caption{The behavior of Eq. \ref{eq:matrix-based-entropy} for varying values of $\alpha$ on Gram matrices with eigenvalues distributed with a $\beta$-power law such that $\lambda_i = i^{-\beta}$.}
  \label{fig:power_law_entropy}
\end{figure}

\section{Dataset Details}
\label{appendix:dataset-details}
\subsection{Wikitext Dataset}
We used the wikitext dataset \cite{merity2016pointer} for the majority of our experiments in Section \ref{sect:metrics-experiments}. This was downloaded from \textbf{Salesforce/wikitext} on huggingface. The dataset consists of 100 million tokens scraped from the Featured articles on wikipedia. We filtered out prompts which were less than 30 tokens or were wikipedia section headings.

\subsection{AI-Medical-Chatbot Dataset}
We also used the medical instruction dataset called ai-medical-chatbot \cite{ruslanmv2024} which downloaded from \textbf{ruslanmv/ai-medical-dataset} on HuggingFace. An example from this dataset is:

\begin{lstlisting}
    You are an AI Medical Assistant Chatbot, trained to answer medical questions. Below is an instruction that describes a task, paired with an response context. Write a response that appropriately completes the request.
    
    ### Instruction:
    What is the resurgent sodium current in mouse cerebellar Purkinje neurons?

    ### Context:
    FGF14 modulates resurgent sodium current in mouse cerebellar Purkinje neurons.
\end{lstlisting}

\section{Prompt Augmentations}
\label{appendix:prompt-augmentation}
For the augmentation-invariance metrics such as infoNCE, LiDAR, and DiME, we use the NLPAug library \cite{ma2019nlpaug} to augment our prompts. We use three types of augmentations.

\begin{itemize}
 \item The SplitAug augmentation randomly splits words into two parts by adding a space. 
 \item The RandomCharAug augmentation randomly inserts, substitutes, swaps, or deletes characters.
 \item The Keyboard augmentation randomly substitutes characters with other characters that are at a distance of one as measured on a QWERTY keyboard. For instance, the character "k" may be replaced with "i", "l", "m", or "j".
\end{itemize}

We use the pseudocode below to do our augmentations using three types of augmentations, using the default library settings for each type. When computing augmentation-invariance metrics like infoNCE or DiME, we use the two augmented prompts rather than using one augmented prompt alongside the original prompt. Note that these augmentations may change the token length $T$ of a prompt.

\begin{verbatim}
    
    aug = naf.Sequential([
        naw.SplitAug(p=0.3),
        nac.RandomCharAug(p=0.3),
        nac.KeyboardAug(p=0.3),
    ])
    (aug_A, aug_B) = aug.augment(prompt, num_augmentations=2)

    prompt -> "The quick brown fox jumps over the lazy dog."

    aug_A ->  "The quDUk b rown fox wEmps o ver the l azy dog."
    aug_B ->  "The qTuXi bro wn fox uVm)s ob3r the la_k dog."
\end{verbatim}

\section{Extreme Prompts}
\label{appendix:extreme-prompts}

\subsection{Increasing Repetition}
We take regular prompts from the wikitext dataset, tokenize them, and then for each token we randomly replace it with probability $p$. We draw replacements tokens by sampling a random token from within the prompt. We show examples below for varying levels of $p$.

\begin{itemize}
    \item ($p = 0$) \hspace{3pt} Mint records indicate the first gold dollars were produced on May 7...
    \item ($p = 0.1$) Mint records indicate the first gold dollars were Mint Mint May 7...
    \item ($p = 0.5$) Mint records Mint Mint Mint gold dollars were Mint Mint Mint 7...
    \item ($p = 1.0$) Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint...
\end{itemize}

\subsection{Increasing Randomness}
We take regular prompts from the wikitext dataset, tokenize them, and then for each token we randomly replace it with probability $p$. We draw replacements uniformly from the tokenizer distribution. We show examples below for varying levels of $p$. Unlike the character-level random noise added to prompts in Section {with random noise discussed in Appendix \ref{appendix:prompt-augmentation} which might change the number of tokens $T$ of the prompt, the token-level random noise used here does not do so.

\begin{itemize}
    \item ($p = 0$) \hspace{3pt} Mint records indicate the first gold dollars were produced on May 7...
    \item ($p = 0.1$) Mint records indicate salivary first gold dollars were produced on May NaCl...
    \item ($p = 0.5$) Mint records Dallas actively first dollars persufors on Mayder129 18...
    \item ($p = 1.0$) arf emulsion minorensteinorianmega\_TOStack potsRecip Installifykeeping...
\end{itemize}

\subsection{Random Prompts with Certain Length}

To make a random prompt of a specific length $T$, we sample $T$ tokens uniformly from the Pythia tokenizer distribution. Such a prompt may look like the following for $T=16$: "Proposition Sequencespecific Exp fibers brows Club overviewNos toss Thinking traderMulti indoorlis".

We show how random prompt representations evolve over Pythia training checkpoints in Figure \ref{fig:training_increasing_repetition}. The random prompts we use are of length 512 tokens. It is readily observed that the prompt entropy is flat across layers in the beginning of training. As training progresses, the model compresses more and more near the final layers.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/pythia_random_sentences_across_revisions.pdf}
  \caption{Behavior of random prompt representations as model is training}
\label{fig:training_increasing_repetition}
\end{figure}


% \section{Results}
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/without_logit.pdf}
%   \caption{Layer wise Entropy on Different datasets (Without Logit Layer)}
%   \label{fig:without_logit}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/with_logit.pdf}
%   \caption{Layer wise Entropy on Different datasets (With Logit Layer)}
%   \label{fig:with_logit3}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/Distillations.png}
%   \caption{Distilled models}
%   \label{fig:with_logit2}
% \end{figure}


% # MAMBA_MODELS_HF = ["mamba-130m-hf", "mamba-370m-hf", "mamba-1.4b-hf", "mamba-2.8b-hf"]
% # PYTHIA_MODELS = ["pythia-160m", "pythia-410m", "pythia-1.4b", "pythia-2.8b"]

% \begin{table}[h]
%   \centering
%   \begin{tabular}{c|c|c|c}
%     \hline
%     Model Name & Model Size & \# Layers & GSM8K \\ \hline
%     mamba-130m-hf & 130m & 24 & \\
%     mamba-370m-hf & 130m & 48 & \\
%     mamba-1.4b-hf & 130m & 48 & \\
%     mamba-2.8b-hf & 130m & 64 & \\ \hline
%     pythia-160m & 130m & 12 & \\
%     pythia-410m & 130m & 24 & \\
%     pythia-1.4b & 130m & 24 & \\
%     pythia-2.8b & 130m & 32 & \\
%   \end{tabular}
%   \caption{Results on different datasets}
%   \label{tab:your_label}
% \end{table}


% \begin{table}[h!]
% \centering
% \begin{tabular}{l|c|c|c}
% \hline
% % \textbf{Model}                    & \textbf{MMLU (5 shots)} & \textbf{AlpacaEval (LC win against GPT-4)} & \textbf{MT-Bench (scored by GPT-4)} \\ \hline
% \textbf{Model}                    & \textbf{MMLU (5 shots)} & \textbf{AlpacaEval} & \textbf{MT-Bench} \\ \hline
% Mamba (1/2 attention)             & 59.26                   & 29.61                                       & 7.35                                \\ 
% Mamba2 (1/2 attention)            & 56.67                   & 25.00                                       & 7.32                                \\ 
% Mamba (1/4 attention)             & 52.68                   & 25.85                                       & 6.86                                \\ 
% Mamba2 (1/4 attention)            & 53.94                   & 20.25                                       & 6.74                                \\
% Mamba (1/8 attention)             & 49.20                   & 20.76                                       & 6.46                                \\
% Mamba2 (1/8 attention)            & 50.85                   & 20.25                                       & 6.48                                \\ 
% Mamba2 (0 attention)              & 43.19                   & 14.49                                       & 5.64                                \\
% \end{tabular}
% \caption{Model performance comparison across MMLU, AlpacaEval, and MT-Bench.}
% \end{table}


\end{document}