@article{agrawal2022alphareq,
  title={$\alpha$-{ReQ}: Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay},
  author={Agrawal, Kumar K and Mondal, Arnab Kumar and Ghosh, Arna and Richards, Blake},
  journal=neurips,
  year={2022}
}

@article{alain2016understanding,
    title={Understanding intermediate layers using linear classifier probes},
    author={Guillaume Alain and Yoshua Bengio},
    year={2017},
    journal=iclr
}

@article{bordes2022guillotine,
  title={Guillotine regularization: Why removing layers is needed to improve generalization in self-supervised learning},
  author={Bordes, Florian and Balestriero, Randall and Garrido, Quentin and Bardes, Adrien and Vincent, Pascal},
  journal=tmlr,
  year={2023}
}

@inproceedings{deletanglanguage,
  title={Language Modeling Is Compression},
  author={Deletang, Gregoire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and others},
  booktitle=iclr,
  year=2024
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{fan2024notalllayers,
  title={Not all layers of llms are necessary during inference},
  author={Fan, Siqi and Jiang, Xin and Li, Xiang and Meng, Xuying and Han, Peng and Shang, Shuo and Sun, Aixin and Wang, Yequan and Wang, Zhongyuan},
  journal=arxiv,
  year={2024}
}

@inproceedings{garrido2023rankme,
  title={{RankMe}: Assessing the downstream performance of pretrained self-supervised representations by their rank},
  author={Garrido, Quentin and Balestriero, Randall and Najman, Laurent and Lecun, Yann},
  booktitle=icml,
  year={2023},
}

@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle =neurips,
 title = {Language Models are Few-Shot Learners},
 year = {2020}
}

@article{gurnee2023language,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal=arxiv,
  year={2023}
}

@article{hosseini2024curvature,
  title={Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language.},
  author={Hosseini, Eghbal and Fedorenko, Evelina},
  journal=neurips,
  year={2023}
}

@article{jin2024conceptdepth,
  title={Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?},
  author={Jin, Mingyu and Yu, Qinkai and Huang, Jingyuan and Zeng, Qingcheng and Wang, Zhenting and Hua, Wenyue and Zhao, Haiyan and Mei, Kai and Meng, Yanda and Ding, Kaize and others},
  journal=arxiv,
  year={2024}
}

@article{liu2019linguistic,
  title={Linguistic knowledge and transferability of contextual representations},
  author={Liu, Nelson F and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E and Smith, Noah A},
  journal=naccl,
  year={2019}
}

@article{mamba,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal=colm,
  year={2024}
}

@inproceedings{oord2018representation,
  title={Representation Learning with Contrastive Predictive Coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  booktitle=iclr,
  year={2018},
}

@article{park2024geometry,
  title={The Geometry of Categorical and Hierarchical Concepts in Large Language Models},
  author={Park, Kiho and Choe, Yo Joong and Jiang, Yibo and Veitch, Victor},
  journal={arXiv preprint arXiv:2406.01506},
  year={2024}
}

@article{raghu2017svcca,
  title={{SVCCA}: Singular vector canonical correlation analysis for deep learning dynamics and interpretability},
  author={Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
  journal=neurips,
  year={2017}
}

@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal=Entropy,
  year={2019}
}

@phdthesis{shwartz2022information,
  title={Information flow in deep neural networks},
  author={Shwartz-Ziv, Ravid},
  school={Hebrew University},
  year={2022}
}

@article{tenney2019bert,
  title={{BERT} rediscovers the classical NLP pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  journal=acl,
  year={2019}
}

@article{thilak2023lidar,
  title={{LiDAR}: Sensing Linear Probing Performance in Joint Embedding SSL Architectures},
  author={Thilak, Vimal and Huang, Chen and Saremi, Omid and Dinh, Laurent and Goh, Hanlin and Nakkiran, Preetum and Susskind, Joshua M and Littwin, Etai},
  journal=iclr,
  year={2024}
}

@article{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 journal = neurips,
 title = {Attention is All you Need},
 year = {2017}
}

@article{voita2019bottom,
  title={The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives},
  author={Voita, Elena and Sennrich, Rico and Titov, Ivan},
  journal=emnlp,
  year={2019}
}

