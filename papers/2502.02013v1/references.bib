@article{dcor2007,
Author = {Gábor J. Székely and Maria L. Rizzo and Nail K. Bakirov},
Title = {Measuring and testing dependence by correlation of distances},
Year = {2008},
journal = {Annals of Statistics},
}

@INPROCEEDINGS{effectiverank,
  author={Roy, Olivier and Vetterli, Martin},
  booktitle={2007 15th European Signal Processing Conference}, 
  title={The effective rank: A measure of effective dimensionality}, 
  year={2007},
  volume={},
  number={},
  pages={606-610},
  keywords={Entropy;Signal processing;Covariance matrices;Europe;Random processes;Matrix decomposition;Eigenvalues and eigenfunctions},
  doi={}}

@article{hao2024training,
  title={Training Large Language Models to Reason in a Continuous Latent Space},
  author={Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong},
  journal={arXiv preprint arXiv:2412.06769},
  year={2024}
}

@misc{yang2024qwen25mathtechnicalreportmathematical,
      title={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement}, 
      author={An Yang and Beichen Zhang and Binyuan Hui and Bofei Gao and Bowen Yu and Chengpeng Li and Dayiheng Liu and Jianhong Tu and Jingren Zhou and Junyang Lin and Keming Lu and Mingfeng Xue and Runji Lin and Tianyu Liu and Xingzhang Ren and Zhenru Zhang},
      year={2024},
      eprint={2409.12122},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.12122}, 
}

@article{qwen2.5,
    title   = {Qwen2.5 Technical Report}, 
    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
    journal = {arXiv preprint arXiv:2412.15115},
    year    = {2024}
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2501.12948}, 
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge university press}
}

@article{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 journal = neurips,
 title = {Attention is All you Need},
 year = {2017}
}

@misc{ruslanmv2024,
  title={AI Medical Chatbot dataset},
  author={Ruslan Magana Vsevolodovna},
  url={https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot},
 year={2024}
}
@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{behnamghader2024llm2vec,
  title={{LLM2Vec}: Large language models are secretly powerful text encoders},
  author={BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva},
  journal=colm,
  year={2024}
}

@article{park2024geometry,
  title={The Geometry of Categorical and Hierarchical Concepts in Large Language Models},
  author={Park, Kiho and Choe, Yo Joong and Jiang, Yibo and Veitch, Victor},
  journal={arXiv preprint arXiv:2406.01506},
  year={2024}
}

@article{mamba,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal=colm,
  year={2024}
}

@article{chen2020mocov2,
  title={Improved baselines with momentum contrastive learning},
  author={Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
  journal=arxiv,
  year={2020}
}

@article{hendrycks2020mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal=iclr,
  year={2021}
}

@article{bordes2022guillotine,
  title={Guillotine regularization: Why removing layers is needed to improve generalization in self-supervised learning},
  author={Bordes, Florian and Balestriero, Randall and Garrido, Quentin and Bardes, Adrien and Vincent, Pascal},
  journal=tmlr,
  year={2023}
}

@article{fan2024notalllayers,
  title={Not all layers of llms are necessary during inference},
  author={Fan, Siqi and Jiang, Xin and Li, Xiang and Meng, Xuying and Han, Peng and Shang, Shuo and Sun, Aixin and Wang, Yequan and Wang, Zhongyuan},
  journal=arxiv,
  year={2024}
}
@article{muennighoff2022mteb,
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  title = {{MTEB}: Massive Text Embedding Benchmark},
  journal=arxiv,  
  year = {2022}
}

@inproceedings{chen2020simclr,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle=icml,
  year={2020},
}

@article{liu2019linguistic,
  title={Linguistic knowledge and transferability of contextual representations},
  author={Liu, Nelson F and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E and Smith, Noah A},
  journal=naccl,
  year={2019}
}

@article{gurnee2023language,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal=arxiv,
  year={2023}
}


@article{alain2016understanding,
    title={Understanding intermediate layers using linear classifier probes},
    author={Guillaume Alain and Yoshua Bengio},
    year={2017},
    journal=iclr
}

@inproceedings{oord2018representation,
  title={Representation Learning with Contrastive Predictive Coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  booktitle=iclr,
  year={2018},
}


@article{raghu2017svcca,
  title={{SVCCA}: Singular vector canonical correlation analysis for deep learning dynamics and interpretability},
  author={Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
  journal=neurips,
  year={2017}
}
@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal=Entropy,
  year={2019}
}

@article{chen2023sudden,
  title={Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs},
  author={Chen, Angelica and Shwartz-Ziv, Ravid and Cho, Kyunghyun and Leavitt, Matthew L and Saphra, Naomi},
  journal=iclr,
  year={2024}
}

@inproceedings{NEURIPS2023_b63ad8c2,
 author = {Ben-Shaul, Ido and Shwartz-Ziv, Ravid and Galanti, Tomer and Dekel, Shai and LeCun, Yann},
 booktitle = neurips,
 title = {Reverse Engineering Self-Supervised Learning},
 year = {2023}
}

@article{shwartz2024compress,
  title={To compress or not to compress—self-supervised learning and information theory: A review},
  author={Shwartz Ziv, Ravid and LeCun, Yann},
  journal={Entropy},
  year={2024},
}


@phdthesis{shwartz2022information,
  title={Information flow in deep neural networks},
  author={Shwartz-Ziv, Ravid},
  school={Hebrew University},
  year={2022}
}

@misc{bm25s,
      title={BM25S: Orders of magnitude faster lexical search via eager sparse scoring}, 
      author={Xing Han Lù},
      year={2024},
      url=arxiv, 
}

@article{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2017},
    journal=iclr
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal=arxiv,
  year={2020}
}

@article{hartigan1985dip,
  title={The dip test of unimodality},
  author={Hartigan, John A and Hartigan, Pamela M},
  journal={The annals of Statistics},
  year={1985},
}

@article{burns2022dl,
  title={Discovering Latent Knowledge in Language Models Without Supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal=arxiv,
  year={2022}
}

@book{scholkopf2018learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Scholkopf, Bernhard and Smola, Alexander J},
  year={2018},
  publisher={MIT press}
}


@inproceedings{parklinear2024,
  title={The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  booktitle=icml,
 year={2024}
}

@article{boes2019neumann,
  title={Von Neumann entropy from unitarity},
  author={Boes, Paul and Eisert, Jens and Gallego, Rodrigo and M{\"u}ller, Markus P and Wilming, Henrik},
  journal={Physical review letters},
  year={2019},
}

@article{zhouyin2021understanding,
  title={Understanding neural networks with logarithm determinant entropy estimator},
  author={Zhouyin, Zhanghao and Liu, Ding},
  journal=arxiv,
  year={2021}
}

@article{shwartz2023information,
  title={An information theory perspective on variance-invariance-covariance regularization},
  author={Shwartz-Ziv, Ravid and Balestriero, Randall and Kawaguchi, Kenji and Rudner, Tim GJ and LeCun, Yann},
  journal=neurips,
  year={2023}
}

@article{bach2022information,
  title={Information theory with kernel methods},
  author={Bach, Francis},
  journal={IEEE Transactions on Information Theory},
  year={2022},
  publisher={IEEE}
}

@inproceedings{
mallen2024eliciting,
title={Eliciting Latent Knowledge from Quirky Language Models},
author={Alex Troy Mallen and Nora Belrose},
booktitle={ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models},
year={2024},
}

@inproceedings{mamou2020emergence,
  title={Emergence of separable manifolds in deep language representations},
  author={Mamou, Jonathan and Le, Hang and Del Rio, Miguel A and Stephenson, Cory and Tang, Hanlin and Kim, Yoon and Chung, SueYeon},
  booktitle=icml,
  year=2020
}


@inproceedings{skean2024frossl,
  title={{FroSSL}: Frobenius Norm Minimization for Self-Supervised Learning},
  author={Skean, Oscar and Dhakal, Aayush and Jacobs, Nathan and Giraldo, Luis Gonzalo Sanchez},
  booktitle=eccv,
  year={2024}
}

@article{skean2023dime,
  title={{DiME}: Maximizing mutual information by a difference of matrix-based entropies},
  author={Skean, Oscar and Osorio, Jhoan Keider Hoyos and Brockmeier, Austin J and Giraldo, Luis Gonzalo Sanchez},
  journal=arxiv,
  year={2023}
}
@article{thilak2023lidar,
  title={{LiDAR}: Sensing Linear Probing Performance in Joint Embedding SSL Architectures},
  author={Thilak, Vimal and Huang, Chen and Saremi, Omid and Dinh, Laurent and Goh, Hanlin and Nakkiran, Preetum and Susskind, Joshua M and Littwin, Etai},
  journal=iclr,
  year={2024}
}

@inproceedings{renyi1961measures,
  title={On measures of entropy and information},
  author={R{\'e}nyi, Alfr{\'e}d},
  booktitle={Proceedings of the fourth Berkeley symposium on mathematical statistics and probability},
  year={1961},
}


@article{giraldo2014measures,
  title={Measures of entropy from data using infinitely divisible kernels},
  author={Giraldo, Luis Gonzalo Sanchez and Rao, Murali and Principe, Jose C},
  journal={IEEE Transactions on Information Theory},
  year={2014},
}

@inproceedings{deletanglanguage,
  title={Language Modeling Is Compression},
  author={Deletang, Gregoire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and others},
  booktitle=iclr,
  year=2024
}

@article{hosseini2024curvature,
  title={Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language.},
  author={Hosseini, Eghbal and Fedorenko, Evelina},
  journal=neurips,
  year={2023}
}

@article{henaff2019perceptual,
  title={Perceptual straightening of natural videos},
  author={H{\'e}naff, Olivier J and Goris, Robbe LT and Simoncelli, Eero P},
  journal={Nature neuroscience},
  year={2019},
}

@misc{ma2019nlpaug,
  title={NLP Augmentation},
  author={Edward Ma},
  howpublished={https://github.com/makcedward/nlpaug},
  year={2019}
}

@article{wei2024large,
  title={Large language model evaluation via matrix entropy},
  author={Wei, Lai and Tan, Zhiquan and Li, Chenghai and Wang, Jindong and Huang, Weiran},
  journal=arxiv,
  year={2024}
}

@article{jin2024conceptdepth,
  title={Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?},
  author={Jin, Mingyu and Yu, Qinkai and Huang, Jingyuan and Zeng, Qingcheng and Wang, Zhenting and Hua, Wenyue and Zhao, Haiyan and Mei, Kai and Meng, Yanda and Ding, Kaize and others},
  journal=arxiv,
  year={2024}
}

@article{lad2024remarkable,
  title={The Remarkable Robustness of LLMs: Stages of Inference?},
  author={Lad, Vedang and Gurnee, Wes and Tegmark, Max},
  journal=arxiv,
  year={2024}
}

@inproceedings{garrido2023rankme,
  title={{RankMe}: Assessing the downstream performance of pretrained self-supervised representations by their rank},
  author={Garrido, Quentin and Balestriero, Randall and Najman, Laurent and Lecun, Yann},
  booktitle=icml,
  year={2023},
}

@article{agrawal2022alphareq,
  title={$\alpha$-{ReQ}: Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay},
  author={Agrawal, Kumar K and Mondal, Arnab Kumar and Ghosh, Arna and Richards, Blake},
  journal=neurips,
  year={2022}
}

@article{seqvcr,
  title={{Seq-VCR}: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning},
  author={Arefin, Md Rifat and Subbaraj, Gopeshh and Gontier, Nicolas and LeCun, Yann and Rish, Irina and Shwartz-Ziv, Ravid and Pal, Christopher},
  journal=arxiv,
  year={2024}
}


@article{llms-know-what-they-know,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal=arxiv,
  year={2022}
}

@article{palm2,
  title={{PaLM} 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal=arxiv,
  year={2023}
}

@article{hurst-exponent,
  title={Long-term storage capacity of reservoirs},
  author={Hurst, Harold Edwin},
  journal={Transactions of the American society of civil engineers},
  year={1951},
}

@inproceedings{pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle=icml,
  year={2023},
}

@article{fractal-review,
  title={Stochastic models that separate fractal dimension and the Hurst effect},
  author={Gneiting, Tilmann and Schlather, Martin},
  journal={SIAM review},
  year={2004},
  publisher={SIAM}
}

@inproceedings{fractal-generalization-bound,
  title={Generalization bounds using data-dependent fractal dimensions},
  author={Dupuis, Benjamin and Deligiannidis, George and Simsekli, Umut},
  booktitle=icml,
  year={2023},
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal=arxiv,
  year={2021}
}

@inproceedings{fractal-next-token,
  title={Fractal Patterns May Illuminate the Success of Next-Token Prediction},
  author={Alabdulmohsin, Ibrahim and Tran, Vinh Q and Dehghani, Mostafa},
  booktitle=neurips,
  year={2024}
}

@article{fractal-limitations,
  title={On the limitations of fractal dimension as a measure of generalization},
  author={Tan, Charlie B and Garc{\'\i}a-Redondo, In{\'e}s and Wang, Qiquan and Bronstein, Michael M and Monod, Anthea},
  journal=neurips,
  year={2024}
}

@misc{fractal-ethernet-traffic,
  title={Self-similar network traffic and performance evaluation},
  author={Park, Kihong and Willinger, Walter},
  year={2000},
}

@article{mandelbrot-british-coast,
  title={How long is the coast of Britain? Statistical self-similarity and fractional dimension},
  author={Mandelbrot, Benoit},
  journal={American Association for the Advancement of Science},
  year={1967},
}

@inproceedings{effective-rank,
  title={The effective rank: A measure of effective dimensionality},
  author={Roy, Olivier and Vetterli, Martin},
  booktitle={European signal processing conference},
  year={2007},
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal=arxiv,
  year={2024}
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle=cvpr,
  year={2009},
}

@article{aim,
  title={Scalable pre-training of large autoregressive image models},
  author={El-Nouby, Alaaeldin and Klein, Michal and Zhai, Shuangfei and Bautista, Miguel Angel and Toshev, Alexander and Shankar, Vaishaal and Susskind, Joshua M and Joulin, Armand},
  journal=icml,
  year={2024}
}

@inproceedings{pixelgpt,
  title={Generative pretraining from pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle=icml,
  year={2020},
  organization={PMLR}
}

@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal=iclr,
  year={2021}
}

@article{beit,
  title={{BeIT}: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal=iclr,
  year={2022}
}

@article{dinov2,
  title={{DINOv2}: Learning robust visual features without supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal=tmlr,
  year={2024}
}

@inproceedings{mae,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle=cvpr,
  year={2022}
}

@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle =neurips,
 title = {Language Models are Few-Shot Learners},
 year = {2020}
}

@article{alphacode,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  year={2022},
}

@article{tenney2019bert,
  title={{BERT} rediscovers the classical NLP pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  journal=acl,
  year={2019}
}

@article{voita2019bottom,
  title={The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives},
  author={Voita, Elena and Sennrich, Rico and Titov, Ivan},
  journal=emnlp,
  year={2019}
}

@article{marion2024implicit,
  title={Implicit regularization of deep residual networks towards neural ODEs},
  author={Marion, Pierre and Wu, Yu-Han and Sander, Michael E and Biau, G{\'e}rard},
  journal=iclr,
  year={2024}
}

@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}