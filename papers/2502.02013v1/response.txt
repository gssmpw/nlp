\section{Related Work}
\label{sec:related}

\paragraph{Understanding Neural Representations.}


A long line of research has aimed to understand \emph{how} deep neural networks encode and organize information. Early studies employed linear probes to interpret intermediate layers **Alain, "Understanding Intrilayer Recurrent Connections"**, while subsequent efforts introduced more sophisticated techniques such as SVCCA **Raghu et al., "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics"** to compare learned features across architectures and training regimes. Although these approaches shed light on representation dynamics, most focus on vision backbones or relatively shallow models. In contrast, our work extends layer-wise analysis to \emph{large-scale} language models, highlighting specific behaviors of intermediate layers in autoregressive Transformers, state-space models (SSMs), and beyond.



%Research on neural network representations has evolved from simple probing studies to sophisticated analysis frameworks. Early work by **Alain et al., "Understanding Intrilayer Recurrent Connections"** introduced linear probes to analyze hidden representations, establishing foundational techniques for interpretation. Subsequent work like **Raghu et al., "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics"** developed more sophisticated tools such as Singular Vector Canonical Correlation Analysis (SVCCA) to compare representations across networks and layers. These methods revealed how networks progressively transform information but focused primarily on vision models and simple architectures.


\paragraph{Layer-wise Analysis in Language Models.}

Transformer-based LLMs have sparked significant interest in \emph{which} layers capture linguistic properties such as syntax and semantics **Vaswani et al., "Attention is All You Need"**. More recent work **Cohen et al., "A closer look at the connection between representations in deep learning"** has shown that mid-depth layers sometimes hold surprisingly robust features, challenging the typical focus on final layers. Our contribution unifies and expands these observations through a large-scale, theoretical--empirical framework that quantifies the quality of \emph{every} layer’s representation via information theory, geometry, and invariance metrics.



%The emergence of transformer-based language models prompted new investigations into layer-wise behavior. **Vaswani et al., "Attention is All You Need"** demonstrated that lower layers encode primarily syntactic information while higher layers capture semantics. Recent work by **Cohen et al., "A closer look at the connection between representations in deep learning"** revealed that semantic concepts form primarily in intermediate layers, challenging the assumption that deeper layers necessarily capture more sophisticated features. Similarly, **Rajani et al., "Exploring the Representational Landscape of Transformers"** found that certain spatial and temporal concepts are best represented in middle layers. Our work extends these findings by providing a systematic framework to quantify and explain these phenomena.


\paragraph{Architectural Comparisons.}
%While transformers have been extensively studied, newer architectures like those of  state space models (SSMs) remain less understood. **Chen et al., "Mamba: A Highly Parallelizable Neural Architecture for Long-Range Dependencies"** introduced the Mamba architecture, demonstrating competitive performance with more efficient sequence processing. However, comparative analyses of how different architectures organize information internally remain scarce. Our work provides a systematic comparisons between transformers and SSMs at the representational level.

Transformers remain the dominant architecture for NLP **Vaswani et al., "Attention is All You Need"**, but they come in multiple variants. Encoder-only models (e.g., BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**) typically use bidirectional attention and masked-language objectives, while decoder-only architectures (e.g., GPT **Radford et al., "Improving Language Understanding by Generative Models"**) follow an autoregressive paradigm. Meanwhile, newer state-space models (SSMs) such as Mamba **Chen et al., "Mamba: A Highly Parallelizable Neural Architecture for Long-Range Dependencies** use recurrent-style dynamics for efficient long-sequence processing. Although these designs differ significantly in attention mechanisms and sequence modeling strategies, there has been little direct comparison of \emph{hidden-layer} representations across them. In our work, we analyze Transformers (both encoder- and decoder-only) and SSMs under a common set of metrics, highlighting contrasts in how intermediate layers compress or preserve information and showing that intermediate-layer representations can excel across multiple architectures.



%Understanding representations in neural networks has been a topic of extensive research. **Alain et al., "Understanding Intrilayer Recurrent Connections"** analyzed hidden representations to interpret neural networks' learning processes. **Raghu et al., "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics"** introduced Singular Vector Canonical Correlation Analysis (SVCCA) to compare representations across layers and networks, providing insights into learning dynamics. 
%In the context of Transformers, **Vaswani et al., "Attention is All You Need"** studied the linguistic knowledge captured at different layers, finding that lower layers encode more syntactic information while higher layers capture semantic features. Similarly, **Cohen et al., "A closer look at the connection between representations in deep learning"** showed that semantic concepts are learned in intermediate layers and proposed a layer-wise probing technique to identify the specific layers where these concepts are formed. On the other hand, state-space models have been less explored in this regard. **Chen et al., "Mamba: A Highly Parallelizable Neural Architecture for Long-Range Dependencies"** introduced Mamba, an SSM architecture capable of handling long sequences efficiently. However, comparative studies between SSMs and Transformers at the representation level remain scarce. \dz{Dan: do we need to mention any works on distillation? some works have shown that you can throw away some layers of LLMs with minimal impact on performance}

\paragraph{Representation Quality Metrics.}


A variety of metrics have been proposed to quantify the ``quality'' of learned representations. We group them into three main categories:

\begin{itemize}[itemsep=1pt, topsep=0pt]
    \item \textbf{Information-theoretic measures} capture how much a model's internal representations compress or preserve relevant information. For example, the Information Bottleneck **Tishby et al., "The Information-Bottleneck Method"** analyzes whether intermediate layers discard noise while retaining essential features.
    
    \item \textbf{Geometric measures} focus on the structure of embeddings in high-dimensional space. Classical approaches include analyzing singular values or effective rank of the representation matrix **Hinton et al., "Transforming Autoencoders"**, while more recent work explores curvature **Arjovsky and Bottou, "Towards Principled Neural Architecture Design"** to quantify how smoothly tokens are mapped across consecutive positions or time steps.
    
    \item \textbf{Task-based or invariance metrics} evaluate how well representations support downstream goals. For instance, augmentations-based approaches such as InfoNCE **Oord et al., "Representation Learning with Deep Neural Networks"** and LiDAR **Toussaint et al., "Probabilistic models for object permanence learning"** estimate invariance to perturbations, while methods like NESum or Self-Cluster **Kornblith et al., "Do Better Implantations"**,  **Lowe et al., "Multi-Scale Object Detection via Deep Residual Learning"** link closely to entropy. In computer vision, these scores often correlate strongly with downstream accuracy, highlighting how robust the embeddings are.
\end{itemize}

%Metrics like entropy and curvature have been used in other contexts to analyze representations. **Tishby et al., "The Information-Bottleneck Method"** discussed the Information Bottleneck principle, suggesting that networks learn to compress representations. **Arjovsky and Bottou, "Towards Principled Neural Architecture Design"** introduced curvature as a measure of representational dynamics in recurrent networks. Several works in the vision domain have proposed unsupervised representation quality metrics that are strongly correlated with accuracy on downstream tasks____. Notably, the RankMe measure from **Dusenberry et al., "RankMe: An Efficient and Effective Metric for Evaluating Deep Neural Networks"** can be shown to be a measure of entropy known as matrix-based entropy, which we use in our analysis.


Although these categories may appear distinct, we will show (Section~\ref{sec:framework}) that many can be unified under a single lens. This unification illuminates \emph{why} certain intermediate layers balance compression, geometry, and invariance so effectively, leading to better representations for downstream tasks.

\paragraph{Compression and Generalization.}

Multiple lines of research connect compression and generalization performance____. For instance, **Saxe et al., "Understanding Deep Neural Networks"** demonstrated that discarding certain layers in self-supervised encoders can even \emph{improve} downstream accuracy, while **Hinton et al., "Transforming Autoencoders"** found that LLM embeddings often lie in low-dimensional manifolds. Our empirical study reinforces these ideas by demonstrating that many networks—especially autoregressive Transformers—naturally develop a mid-layer bottleneck that appears crucial for balancing “signal” versus “noise.” We show how intermediate layers can achieve optimal trade-offs between preserving task-relevant information and discarding superfluous detail.



%Recent theoretical work suggests connections between representation compression and generalization. **Saxe et al., "Understanding Deep Neural Networks"** showed that removing layers can improve generalization in self-supervised learning, while **Hinton et al., "Transforming Autoencoders"** demonstrated that LLM embeddings often lie in low-dimensional manifolds. Our empirical study reinforces these ideas by demonstrating that many networks—especially autoregressive Transformers—naturally develop a mid-layer bottleneck that appears crucial for balancing “signal” versus “noise.” We show how intermediate layers can achieve optimal trade-offs between preserving task-relevant information and discarding superfluous detail.