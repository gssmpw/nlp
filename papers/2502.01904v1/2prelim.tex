\section{Preliminary}
\label{preliminary}
\begin{table}[htb]
\centering
\caption{Summary of Notations}
\myspace
\scalebox{0.9}{
    \begin{tabular}{c|c}
    \noalign{\hrule height 1pt}
    \cellcolor{gray!25} Notation & \cellcolor{gray!25} Definition \\ 
    \noalign{\hrule height 0.6pt}
    $G$ & a bipartite graph \\
    $\mathcal{A} $ & the adjacency matrix\\ 
    $\mathcal{A}_u$ & the neighbor list of the vertex $u$ \\  
    $N(u,G)$ & the neighbors of $u$ in $G$ \\
    $deg(u,G)$ & the degree of $u$ in $G$ \\
    $\varepsilon$ & a privacy budget \\ 
    $G'$ & a noisy bipartite graph\\
    % $\BTF_G$ & the number of butterflies in $G$ \\
    $\pqx$ & the number of common neighbors between $u$ and $w$\\
    $\Delta g$ & the global sensitivity of a function $g$\\
    % $\foneround$ & the one-round estimator\\
    $\fq$ & the single-source estimator based on $N(u, G)$\\
    $\fx$ & the single-source estimator based on $N(w, G)$\\
    $\fds$ & the double source estimator based on both $\fq$ and $\fx$\\
    \noalign{\hrule height 1pt}
    \end{tabular}
}
\label{tab:notation}
\myspace
\end{table}


\subsection{Problem definition}

We consider an unweighted bipartite graph $G(V=(U,L),E)$. $V=U\cup L$ denotes the set of vertices, where $U$ and $L$ represent the upper and lower layer, respectively. The vertices in $U$ and $L$ are called the upper vertices and the lower vertices. $E \subseteq U \times L$ denotes the set of edges. We use $n_1 = |U(G)|$ and $n_2 = |L(G)|$ to denote the number of upper and lower vertices, respectively, and $m$ = $|E|$ to represent the number of edges in $G$. The adjacency matrix $\mathcal{A}$ for $G$ is of dimensions $n \times n$, where $\mathcal{A}[u,v]=1$ if there exists an edge between the vertices $u$ and $v$ and $0$ otherwise. The $u$-th row of $\mathcal{A}$ (including both ``1'' and ``0'') is the {\em neighbor list} of $u$, denoted by $\mathcal{A}_u$. In addition, the set of neighbors of a vertex $u$ in $G$ is denoted by $N(u,G)$, and its degree is denoted by $deg(u,G)$ = $|N(u,G)|$. We use $d_{max}(U)$ and $d_{max}(L)$ to represent the maximum degree among the upper vertices and lower vertices, respectively. 
\begin{definition}
\label{def:pqx}
{\bf Common neighbors.} 
Let \vq and \vx be two vertices on the same layer of a bipartite graph $G$. 
The common neighbors of \vq and \vx are the vertices adjacent to both \vq and \vx in $G$, i.e., $N(u, G) \cap N(w, G)$. Here $N(u, G)$ represents the set of neighbors of vertex $u$ in graph $G$. 
We use $\pqx$ to denote the number of common neighbors of \vq and \vx, 
i.e., $\pqx = |N(u, G) \cap N(w, G)|$. 
\end{definition}

Classic differential privacy (DP) operates under a central model, where a central data curator manages a dataset $D$ \cite{dwork2014algorithmic}. $D$ and $D'$ are neighboring datasets if they differ by one data record. DP ensures that query outputs on these neighboring datasets are hard to distinguish. 
When extending DP under the central model to protect edge privacy in graphs, neighboring datasets refer to two graphs that vary by a single edge. 
However, the assumption with the central model that the data curator with access to the entire graph can be trusted can be impractical in real-world scenarios. 
Thus, we adopt the widely-used $\varepsilon$-edge local differential privacy (edge LDP), which enables each vertex to locally perturb its data before transmission to the data curator \cite{qin2017generating, zhang2018two, ye2020lf}. Under edge LDP, the neighboring datasets are two neighbor lists differing by one bit. 

\begin{definition} 
\label{def:ldp}
{\bf $\varepsilon$-edge local differential privacy.} 
Let $G$ be a bipartite graph and $\varepsilon>0$. 
For each vertex $u \in V(G)$, let $R_u$ with domain $\{0, 1\}^n$ be a randomized algorithm of vertex $u$. 
$R_u$ provides $\varepsilon$-edge LDP if for any two neighbor lists $\mathcal{A}_u$, $\mathcal{A}_u'$ that differ in one bit and any $S \subseteq Range(Ri)$: 
$$ Pr(R_u(\mathcal{A}_u) \in S  ) \leq e^{\varepsilon}  Pr(R_{u}(\mathcal{A}_u') \in S  ) $$
Here $\varepsilon$ is called the privacy budget. 
\end{definition}

\noindent
{\bf Problem Statement.}
Given a bipartite graph $G$, a privacy budget $\varepsilon$, and a pair of vertices \vq and \vx on the same layer of $G$, the {\em common neighborhood estimation} problem aims to estimate $\pqx$ while satisfying \epldp. 

{\color{black}Without loss of generality, we assume that $u$ and $w$ are in the lower layer of $G$.} In this paper, we use the {\em expected L2 loss} to evaluate the quality of estimates for the number of common neighbors. 
\begin{definition}
\label{def:l2}
{\bf Expected L2 loss.} % expected squared error
Given two vertices $u$ and $w$ on a bipartite graph $G$, and an estimate $f$ for the number of common neighbors between $u$ and $w$, the expected L2 loss of $f$ is the expected squared error between $\pqx$ and $f$, i.e., $$l2( \pqx, f ) = \mathbb{E}(( \pqx - f  )^2)$$
\end{definition}

\subsection{Warm up}
% Note that \epldp ensures that if two vertices have neighbor lists that differ by one bit, they cannot be reliably distinguished based on the outputs from the randomized algorithms. 
\epldp ensures that if two vertices have neighbor lists differing by just one bit, they cannot be reliably distinguished based on the outputs of the randomized algorithms. 
In this part, we introduce the most common methods for providing edge LDP, which are randomized responses and the Laplace mechanism. 
Then, we present a naive solution to the common neighborhood estimation problem. 


\begin{figure}[thb]
\includegraphics[width=0.40\textwidth]{figures/matrix.pdf}
\myspace
\caption{randomized responses on a bipartite graph. 
}
\label{fig:matrix}
\myspace
\end{figure}

\noindent
{\bf Warner's randomized response. }
One effective method for achieving \epldp is through randomized responses (RR), initially introduced as a survey technique to allow confidential answers to sensitive inquiries such as criminal or sexual activities \cite{warner1965randomized}. 
In essence, participants are asked to answer the questions honestly with probability. 
This concept has been adapted for graph applications to ensure edge local differential privacy \cite{qin_generating_2017, imola2021locally}. 
Specifically, each entry $x \in {0,1}$ of a neighbor list is perturbed with a probability of $\frac{1}{1+e^{\varepsilon}}$, where $\varepsilon$ denotes the privacy budget. 
$$
RR(x) = \begin{cases} 
1-x & \text{with probability } \frac{1}{1+e^{\varepsilon}} \\
x & \text{with probability } \frac{e^{\varepsilon}}{1+e^{\varepsilon}}
\end{cases}
$$
Current state-of-the-art methods on general graphs perturb each entry in the lower triangle of the adjacency matrix \cite{imola2021locally,imola2022communication}. 
However, to ensure that the resulting noisy graph $G'$ is bipartite, we only perturb entries in $\mathcal{A}$ that represent potential edges in a bipartite graph. 
Without loss of generality, we assume that the upper vertices have smaller IDs than the lower vertices. 
In this way, the adjacency matrix $\mathcal{A}$ can be divided into 4 blocks, where the ones on the diagonal are empty because the vertices of the same layer cannot be connected. 
%%%
In this paper, we only apply randomized responses to the neighbor lists of the query vertices \vq and \vx. 
When $u, w \in U(G)$, we flip the entries in $\mathcal{A}_u \cap L(G)$ and $\mathcal{A}_w \cap L(G)$. 
When $u, w \in L(G)$, we flip the entries in $\mathcal{A}_u \cap U(G)$ and $\mathcal{A}_w \cap U(G)$. 
In doing so, we avoid generating noisy edges not allowed in bipartite graphs. 
We denote the noisy graph with respect to a privacy budget $\varepsilon$ by $G'_{\varepsilon}$.
\begin{example}
We illustrate applying randomized responses to a bipartite graph with two upper vertices and four lower vertices in Fig.~\ref{fig:matrix}. 
The left shows the original adjacency matrix and the right side shows the matrix after applying randomized responses. 
Rows and columns are ordered with upper vertices preceding lower vertices. 
White squares indicate zeros, as edges within the same layer are not allowed in bipartite graphs. 
To find common neighbors of the upper vertices $u$ and $w$, we apply randomized responses to their neighbor lists, which affect the upper right block with eight grey squares. 
With the privacy budget \(\epsilon\), each grey block is flipped with probability \(\frac{1}{1 + e^{\epsilon}}\). Light grey squares outlined in red represent flipped entries.
\end{example}


\noindent
{\bf Calibrating noise with global sensitivity. }
To achieve \epldp, it is necessary to add noise to any data transmitted from a vertex to the data curator. The Laplace mechanism is used for this purpose, which calibrates the amount of noise with the {\em global sensitivity} of the transmitted data. 

\begin{definition}
\label{def:gs}
{\bf Global sensitivity.} 
Consider a bipartite graph $G$. 
Let $\mathcal{A}_u$ be the neighbor list of vertex $u$. 
Let $\mathcal{A}_u'$ be a neighbor list that differs from $\mathcal{A}_u$ in at most one entry. 
The global sensitivity of a function $f: \mathcal{A}_u \rightarrow \mathbb{R}$ is:
$$
\Delta f = max_{\mathcal{A}_u , \mathcal{A}_u'} | f(\mathcal{A}_u) - f(\mathcal{A}_u') |
$$
% Here $\mathcal{A}_u'$ is a neighbor list that differs from $\mathcal{A}_u$ in at most one entry. 
% $\Delta f$ represents the global sensitivity of $f$. 
\end{definition}
\begin{definition}
\label{def:lap}
{\bf The Laplace Mechanism.} 
Given a privacy budget $\varepsilon$ and any function $f$, the Laplace mechanism is defined as:
$$
\tilde{f} = f + \text{Lap}\left(\frac{\Delta f}{\varepsilon}\right)
$$
Here $\tilde{f}$ is the noisy version of $f$ and $Lap(\cdot)$ is the probability density function of the Laplace distribution. 
\end{definition}
By applying the Laplace mechanism, we allow the vertices to send local graph statistics with calibrated noise to the data curator while satisfying \epldp. 
% Note that edge LDP is immune to {\em post-processing}, allowing the data curator to apply any post-processing to the graph statistics it receives from the vertices without compromising the privacy guarantees \cite{imola2021locally}.



{\color{black}
Edge LDP can also be obtained via composition (i.e., combine multiple edge LDP algorithms). 
For instance, {\em sequential composition} \cite{jiang2021applications, qin_generating_2017} enables the sequential application of multiple edge LDP algorithms ($M_i$), each consuming some privacy budgets ($\varepsilon_i$), and ensures that the overall process satisfies $\sum_{i} \varepsilon_i$-edge LDP. 
{\em Parallel composition} states that if disjoint subsets of the graph are processed by different edge LDP algorithms ($M_i$) with privacy budget $\varepsilon_i$, then the overall mechanism running these algorithms in parallel satisfies $\max_i(\varepsilon_i)$-edge LDP \cite{yang2023local}.
% allows disjoint subsets of the graph to be processed independently by different mechanisms, enabling each mechanism to utilize the full privacy budget $\epsilon$ \cite{yang2023local}. 
%% for instance. 
Furthermore, edge LDP is immune to {\em post-processing} \cite{yang2023local, imola2021locally}, allowing the data curator to apply any post-processing to the graph statistics received from the vertices without compromising the privacy guarantees. }


\begin{algorithm}[tbh]
    \small
	\caption{\naive}
	\label{algo:naive}
	\LinesNumbered
	\KwIn{$G$: a bipartite graph; 
            $\varepsilon$: a privacy budget; 
            \vq, \vx: two query vertices from the same layer} 
	\KwOut{$\fnaive$: the naive estimator of $\pqx$}
         \tcp{\textbf{vertex side:}} 
        \input{RR1}
        \tcp{\textbf{curator side:}} 
        $\fnaive \gets |N(u, G_{\varepsilon}') \cap N(w, G_{\varepsilon}')|$;\\
        \textbf{return} $\fnaive$;\\
\end{algorithm}


\noindent
{\bf A naive approach.} 
Given that randomized responses preserve \epldp, a naive approach is to count the number of common neighbors of \vq and \vx on the noisy graph constructed by randomized responses, as outlined in Algorithm \ref{algo:naive}. 
Note that for our problem, we only need to apply randomized responses to \vq and \vx. 
Specifically, given a privacy budget $\varepsilon>0$, \naive flips the entries $A[i,j]$ with a probability $\frac{1}{1+e^{\varepsilon}}$, where $i \in \{u, w\}$ (Lines 1-4). 
The data curator collects the noisy edges from \vq and \vx and constructs a noisy graph $G_{\varepsilon}'$ (Line 5). 
\textcolor{black}{In this way, we do not need to analyze the global sensitivity for \naive because it does not involve the Laplace mechanism and only relies on randomized responses to provide edge LDP. }
Then, the naive estimator $\fnaive$ is calculated taking the intersection of the neighbors of \vq and \vx in $G_{\varepsilon}'$, i.e., $\fnaive = |N(u, G_{\varepsilon}') \cap N(w, G_{\varepsilon}')|$ (Line 6). 
However, since there are generally more ``0''s than ``1''s in the neighbor lists of \vq and \vx, applying randomized responses usually results in a much denser noisy graph $G_{\varepsilon}'$, which results in severe overcounting $\pqx$. 

\noindent
{\bf Theoretical analysis for \naive.} 
{\color{black}
Without loss of generality, we assume that the query vertices $u$ and $w$ are from the lower layer $L(G)$ when analyzing the time costs, communication costs, and expected L2 losses. 
%% time costs. 
The time costs are divided between the vertex side and the curator side.
On the vertex side, the time costs incurred by randomized responses is $O(n_1)$, where $n_1$ is the number of vertices in $U(G)$. 
On the curator side, the dominating cost is incurred by intersecting $N(u, G_{\varepsilon}')$ and $N(w, G_{\varepsilon}')$, which takes $O(min(deg(u,  G_{\varepsilon}'), deg(w,  G_{\varepsilon}')))$. Thus, the overall time complexity is $O(n_1)$. 
The communication costs are incurred only during randomized responses, where vertices $u$ and $w$ send noisy edges to the data curator. 
For vertex $u$, the expected number of noisy edges is $d_u \times (1-p) + (n_1 - d_u) \times p$, where $p = \frac{1}{1+e^{\varepsilon}}$. Similarly, for vertex $w$, the expected number of noisy edges is $d_w \times (1-p) + (n_1 - d_w) \times p$. 
Thus, the overall communication cost is $O\left(\frac{e^{\varepsilon}-1}{e^{\varepsilon}+1}(d_u + d_w) + \frac{2n_1}{1 + e^{\varepsilon}}\right)$.} 
In the following, we analyze the expected L2 loss of the estimator $\fnaive$ returned by \naive. 

\begin{theorem} 
\label{thm:f1}
{\color{black}
Given a bipartite graph $G$, a privacy budget $\varepsilon$, and a pair of query vertices \vq and \vx in $L(G)$, the expected L2-loss of the estimator for $\fnaive$ in Algorithm \ref{algo:naive} is $O(\frac{n_1^2}{(1+e^{\varepsilon})^4 }) $. 
Here, $n_1$ represents the number of vertices in $U(G)$. 
}

% Here $n$ represents the number of vertices in $G$. 
\end{theorem} 
\begin{proof}
{\color{black}
    The naive estimator $\fnaive = |N(u, G_{\varepsilon}')  \cap N(w, G_{\varepsilon}')|$. 
    Let $p = \frac{1}{1 + e^\varepsilon}$ be the flipping probability during the randomized responses. 
    Note that each entry $\mathcal{A}'[i,j]$ on the adjacency matrix of the noisy graph follows a Bernoulli distribution with a parameter $p$ (if $\mathcal{A}[i,j]=0$) or $1-p$ (if $\mathcal{A}[i,j]=1$). 
    Since $\fnaive\geq 0$, we have the following inequality:
        \begin{align*}
        & l2(\fnaive, \pqx)  
        % = \mathbb{E}( (\fnaive - \pqx)^2 )\\
        % & = \mathbb{E}( \fnaive^2 - 2\fnaive \pqx + \pqx^2  )  \\ 
        =  \mathbb{E}( \fnaive^2) -2 \pqx  \mathbb{E}(\fnaive) + \pqx^2
        % & \leq \mathbb{E}( \fnaive^2) + \pqx^2
        = O(\mathbb{E}( \fnaive^2)) 
        \end{align*}
        
Since $\fnaive = \sum_{v \in U(G) } \mathcal{A}'[u,v] \mathcal{A}'[v,w]$, we have: 
\begin{align*}
& \mathbb{E}( \fnaive^2) 
% = \mathbb{E}( (  \sum_{v \in U(G) } \mathcal{A}'[u,v] \mathcal{A}'[v,w]  )^2)\\
= \sum_{v \in U(G)} \mathbb{E}((\mathcal{A}'[u,v] \mathcal{A}'[v,w])^2) \\
& + 2 \sum_{v_i < v_j \in U(G)} \mathbb{E}( \mathcal{A}'[u,v_i] \mathcal{A}'[v_i,w]\mathcal{A}'[u,v_j] \mathcal{A}'[v_j,w]) \\ 
& \leq O({n_1 \choose 2 } (1-p)^4    ) = O(n_1^2 (1-p)^4 )= \errnaive
\end{align*}
% Thus, 
% \begin{align*}
% & l2(\fnaive, \pqx)  \leq \mathbb{E}( \fnaive^2) + \pqx^2 \\ 
% &= {\color{black}\mathbb{E}( (p(2p-3) \pqx + p(1-2p)|N(u, G)  \cup N(u, G)| + p^2 n_1)^2 )} \\
% & \leq 
% {\color{black}O((1-p)^4 n_1^2) = \errnaive }
% \end{align*}
The last step is due to $p = \frac{1}{1 + e^{\varepsilon}}$. 
}
% \begin{align*}
% & l2(\fnaive, \pqx)  = \mathbb{E}( (\fnaive - \pqx)^2 )\\
% &= {\color{black}\mathbb{E}( (p(2p-3) \pqx + p(1-2p)|N(u, G)  \cup N(u, G)| + p^2 n_1)^2 )} \\
% & \leq 
% {\color{black}O((1-p)^4 n_1^2) = \errnaive }
% \end{align*}
% The last step is due to $p = \frac{1}{1 + e^{\varepsilon}}$. 
\end{proof}

\begin{theorem}
{\color{black}
    The \naive algorithm satisfies \epldp. }
    % \textcolor{red}{TODO: provide formal proof. }
\end{theorem}
% \begin{proof}
% % The theorem follows because the randomized responses provide \epldp, which is immune to post-processing. 
% The randomized responses provide \epldp. Since edge LDP is immune to post-processing, the theorem holds. 
% % Since the randomized responses provide \epldp which is immune to post-processing, the theorem holds. 
% \end{proof}
\begin{proof}
{\color{black}
    Since the randomized responses provide \epldp \cite{imola2021locally, imola2022differentially}, 
    Lines 1-4 of the algorithm satisfy \epldp. 
    In addition, edge LDP is immune to post-processing, which means that any analysis (Lines 5-6) conducted on the noisy graph preserves edge LDP. 
    Thus, the theorem holds. 
}
\end{proof}