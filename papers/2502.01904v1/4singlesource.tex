\section{Multiple-round approaches}
\label{sec:adv}

% \textcolor{red}{TODO: add some illustrations}

% \textcolor{red}{Do we mention the multiple round framework}
The \bs algorithm reflects our first attempt at obtaining an unbiased estimate of $\pqx$. 
{However, as analyzed in Section \ref{sec:bs}, the L2 loss of \bs still contains a factor of $n_1$}, because \bs inevitably considers all potential vertices on the opposite layer from \vq and \vx as candidates for the common neighbors. 
% two hop paths $<u, v, w>$ where $v$ is any middle vertex on the opposite layer from \vq and \vx. 
% In the literature of motif counting under edge LDP, a classic approach is to adopt a multiple-round computing scheme, where we use one round to construct a noisy graph and we let the vertices combine their local neighborhood with the noisy graph and report useful graph statistics to be aggregated by the central data curator. 
To further improve data utility, in this section, we employ the classic multiple-round framework in the literature of graph analysis under edge LDP \cite{imola2021locally}. 
In the first round, we utilize a part of the privacy budget to construct a noisy graph via randomized responses. 
Then, we allow both \vq and \vx to download the noisy edges from each other and combine their local neighbors with the noisy edges to compute unbiased estimates of $\pqx$ locally. 
In the end, we use the remaining privacy budget to apply the Laplace mechanism to these unbiased estimates to comply with edge LDP. 
% where one round is dedicated to the noisy graph construction via randomized responses, after which the vertices combine their local neighborhoods with the noisy graph and report graph statistics with small variances. 
% In the literature on mot counting under edge LDP, a classic approach involves employing a multi-round framework. In this scheme, 
% one round is utilized to construct a noisy graph, after which vertices combine their local neighborhoods with this noisy graph. Subsequently, they report useful graph statistics to be aggregated by the central data curator. 
% In doing so, we can utilize the local neighbors of u and w and consider the two-hop paths with one edge in $G$.  
By adopting this multiple-round framework, we propose a \advsslong algorithm (\advss) where we only rely on the local view of \vq to estimate $\pqx$. 
Then, we introduce the \advdslong algorithm (\advds), which leverages the local neighborhoods of both \vq and \vx to optimize privacy budget allocation and balance the contribution of query vertices, resulting in minimized L2 loss.
% Then, we propose a \advdslong algorithm (\advds) that incorporates the local neighborhoods of both \vq and \vx, which optimizes the privacy budget allocation and balances the contribution of both query vertices for minimized L2 loss. 

% which further improves data utility by 
% (1) incorporating the local views of both \vq and \vx and 
% (2) adopting budget allocation optimization strategies to 
% distribute the privacy budget to noisy graph construction and Laplace mechanism for minimized L2 loss. 
% \textcolor{red}{mention something about the weighted average of the two.}
% constructing the noisy graph in the first round and the Laplace mechanism in the second round. 
% In addition, we discuss how our \advds algorithm can be further optimized for handling vertex pairs with imbalanced degrees. 

\begin{figure}[thb]
\centering  
\includegraphics[width=0.45\textwidth]{figures/adv.pdf}
\myspace
\caption{
% A bipartite graph and its corresponding noisy graph by applying randomized responses to $u_1$ and $u_2$. 
The construction of $\fq$ and $\fx$ based on the local neighborhoods of $u_1$ and $u_2$ ($u = u_1$, $w = u_2$).
% Two user-item networks and their corresponding noisy graphs constructed via randomized responses. 
}
\myspace
\myspace
% \myspace
\label{fig:adv}
\end{figure}
% \subsection{An unbiased estimate for $\pqx$} 

\subsection{A single-source estimator for $\pqx$} 
% Under the multiple-round framework, the privacy budget is divided into separate rounds. Within each round, the algorithm must adhere to $\varepsilon_k$-edge local differential privacy ($\varepsilon_k$-LDP). 
% Due to the sequential composition property of local differential privacy (LDP), the algorithm satisfies $\varepsilon$-LDP. 
In this part, we introduce a two-round algorithm for estimating $\pqx$. 
% where we allocate privacy budgets $\varepsilon_1$ to construct a noisy graph by applying randomized responses to \vq and \vx and $\varepsilon_2$ ($\varepsilon_1 + \varepsilon_2 = \varepsilon$). 
First, $\varepsilon_1$ is utilized to construct a noisy graph by applying randomized responses to \vq and \vx. 
Then, the data curator releases the noisy graph. In the second round, vertex \vq integrates its local neighbors with the noisy graph to derive an unbiased estimator for $\pqx$. Then, $\varepsilon_2$ is employed to apply the Laplace mechanism to add noise to this estimator. 

Now we assume that the noisy graph $G_{\varepsilon_1}'$ has already been constructed and discuss how to estimate $\pqx$ based on the local neighbors of \vq and the noisy neighbors of \vx. 
We start by noting that $\pqx$ can also be written as $\sum_{v \in N(u)} \mathcal{A}[v,w] $. 
Thus, when the neighbors of \vq are available, estimating $\pqx$ reduces to estimating $\mathcal{A}[v,w]$, which has already been addressed by $\phi(v, w) = \frac{\mathcal{A}'[v,w]-p}{1-2p}$ in Section \ref{sec:bs}. Here the flipping probability becomes $\frac{1}{1 + e^{\varepsilon_1}}$. 
Based on the above analysis, we derive the following unbiased estimate of $\pqx$ as 
\begin{align*}
   & f_u(u, w) = \sum_{ v \in N(u, G) } \phi(v, w)  = 
   \sum_{ v \in N(u, G) } \frac{\mathcal{A}'[v,w]-p}{1-2p} \\ 
   & = |N(u, G) \cap N(w, G_{\varepsilon_1}') | \frac{1-p}{1-2p} -|N(u, G) \setminus N(w,G_{\varepsilon_1}') |\frac{p}{1-2p} 
\end{align*}

At this point, $f_u(u, w)$ is computed locally based on the neighborhood of \vq. To release it under edge LDP, we analyze the global sensitivity of $f_u(u, w)$ and apply the Laplace mechanism. 

\noindent
{\bf Global Sensitivity Analysis.}
By Definition \ref{def:gs}, the global sensitivity of $f_u(u, w)$ is defined as $\Delta(f_u(u, w)) = \max_{u, u' \in V(G)} | f_u(u, w) - f_{u'}(u, w) |$, 
where $u'$ represents a hypothetical vertex differing from \vq in its neighbor list at one entry. 
It follows:
\begin{align*}
\Delta(f_u(u, w)) 
% & \leq \left| \sum_{v \in N(u, G)} \phi(v, w) - \sum_{v \in N(u', G)} \phi(v, w) \right| \\
\leq \max_{v'} | \phi(v', w) | = \frac{1-p}{1-2p}
\end{align*}

The last step is because the absolute value of $\phi(v', w)$ is either $\frac{1-p}{1-2p}$ or $\frac{p}{1-2p}$. Since $p = \frac{1}{1+ e ^{\varepsilon}} < \frac{1}{2}$, $\frac{1-p}{1-2p}$ is always larger than $\frac{p}{1-2p}$. 
This bound suggests that we must add Laplacian noise scaled to $\frac{1-p}{1-2p}$ before sending the estimator to the data curator. 
In other words, the data curator receives the noisy version of $f_u(u, w)$ denoted by 
\begin{align}
\fq(u,w) = \sum_{v \in N(u, G)} \frac{\mathcal{A}'[v,w]-p}{1-2p} + \text{Lap}\left(\frac{1-p}{(1-2p) \varepsilon_2}\right)
\label{eq:fq}
\end{align}

\begin{lemma}
\label{thm:fq.biase}
$\fq(u,w)$ in Equation \ref{eq:fq} is an unbiased estimate of $\pqx$, i.e., 
$\mathbb{E}(\fq(u,w)) = \pqx$. 
\end{lemma}
\begin{proof}
% Since $\phi(v,w) = \frac{\mathcal{A}'[v,w]-p}{1-2p}$ is an unbiased estimator for $\mathcal{A}[v,w]$, 
Since $\mathbb{E}(\frac{\mathcal{A}'[v,w]-p}{1-2p})  = \mathcal{A}[v,w]$, the expected value of the first term in $\fq$ is $\mathbb{E}( \sum_{v \in N(u, G)} \mathcal{A}[v,w]) = \pqx$. 
The second term represents the noise drawn from the Laplacian distribution with an expected value of zero. Thus, $\fq(u,w)$ is unbiased. 
\end{proof}

\begin{algorithm}[tbh]
    \small
	\caption{The \advss algorithm}
	\label{algo:adv1}
	\LinesNumbered
	\KwIn{$G$: a bipartite graph; 
            $\varepsilon$: a privacy budget; 
            \vq, \vx: two query vertices} 
         \KwOut{$\fq(u, w) $}
        split privacy budget $\varepsilon$ into $\varepsilon_1$ and $\varepsilon_2$ evenly;\\
        \tcp{\textbf{round 1:}}
        \input{RR-SS}
     \tcp{\textbf{round 2:}}
            $S_1 \gets 0$; $S_2 \gets 0$;\\
            \ForEach{$v \in N(u, G)$ }{
                \If{ $(v, w) \in E(G_{\varepsilon_1}')$  }{
                    $S_1 \gets S_1 + 1$:\\
                }
                \Else{
                    $S_2 \gets S_2 + 1$:\\
                }
            }
        $\fq(u, w) \gets S_1 \times \frac{1-p}{1-2p} - S_2 \times \frac{p}{1-2p}$;\\
	% \textbf{return} $\fq(u, w)$;\\
        $ \fq(u, w) \gets \fq(u, w) + \text{Lap}\left(\frac{1-p}{(1-2p) \varepsilon_2}\right)$;\\
	\textbf{return} $\fq(u, w) $;\\
\end{algorithm}

\noindent
{\bf The \advss algorithm.} 
In this part, we present the \advsslong algorithm (\advss) which involves two rounds of interaction between the vertices and the data curator and returns the unbiased estimator $\fq(u ,w)$ derived in Lemma \ref{thm:fq.biase}. The detailed steps are summarized in Algorithm \ref{algo:adv1}. 
Initially, \advss splits the privacy budget $\varepsilon$ into $\varepsilon_1$ and $\varepsilon_2$ evenly. 
In the first round, randomized responses are applied to both \vq and \vx to generate noisy edges, which are then transmitted to the data curator (Lines 3-6). Then, $G_{\varepsilon_1}'$ is constructed from these noisy edges. 
In the second round, \advss visits the neighbors of \vq on $G$ and counts how many are connected to \vx on $G_{\varepsilon_1}'$. 
Upon termination of the for-loop (Lines 9-13), $S_1$ represents the $|N(u, G) \cap N(w, G_{\varepsilon_1}') |$ and $S_2$ represents the $|N(u, G) \setminus N(w,G_{\varepsilon_1}') |$. 
Based on $S_1$ and $S_2$, \advss computes $\fq(u, w)$ and add Laplacian noise scaled to $\frac{1-p}{(1-2p) \varepsilon_2}$ (Lines 14, 15). 
Compared to \bs that considers all vertices on the opposite layer from \vq and \vx on the noisy graph, \advss limits the search scope for the common neighbors of \vq and \vx to the local neighbors of \vq, which results in a substantial reduction in L2 loss. 

\begin{example}
Consider the bipartite graph in Fig.~\ref{fig:motivation}. 
In Fig.\ref{fig:adv}, we illustrate the construction of the single-source estimators where $u = u_1$ and $w = u_2$. 
The outlines of $u_1$ and $w_2$ are highlighted in red. 
A privacy budget $\varepsilon_1$ is allocated to randomized responses for $u_1$ and $u2$. 
In Fig.~\ref{fig:adv}, each query vertex can download the noisy edges from the other query vertex and integrate them with its neighbors. 
In the local perspective of $u_1$, the solid lines represent edges between $u_1$ and its neighbors, while the dashed lines represent noisy edges from $u_2$. 
The vertices shaded in grey are candidates for common neighbors between $u_1$ and $u_2$ in \advss, which includes the neighbors of $u_1$ in the original graph. 
Note that this is much smaller than the candidate pool identified by \bs, which includes all vertices on the opposite layer from the query vertices. 
Based on the formula for $\fq$, it only relies on three Bernoulli variables: $\mathcal{A}'[v_1, u_2]$, $\mathcal{A}'[v_2, u_2]$, and $\mathcal{A}'[v_4, u_2]$. 
Similarly, we can derive $\fx$ based on four random variables: $\mathcal{A}'[v_1, u_2]$, $\mathcal{A}'[v_2, u_2]$, $\mathcal{A}'[v_4, u_2]$, and $\mathcal{A}'[v_{100}, u_2]$. 
The reliance on fewer random variables accounts for the smaller expected L2 loss of \advss compared to \bs.
\end{example}





\noindent
{\bf Theoretical analysis for \advss.} 
{
Without loss of generality, we assume that the query vertices $u$ and $w$ are in $L(G)$. 
First, we analyze the computational time complexity of Algorithm \ref{algo:adv1} (\advss). On the vertex side, the time costs incurred by randomized responses is $O(n_1)$, where $n_1 = |U(G)|$. On the curator side, visiting the neighbors of $u$ in $G$ to compute $\fq(u, w)$ takes $O(deg(u, G) $ time. The overall time complexity is $O(n_1)$. 
% The communication costs of \bs are incurred only during randomized responses, similar to \naive. The overall communication cost is $O\left(\frac{e^{\varepsilon}-1}{e^{\varepsilon}+1}(d_u + d_w) + \frac{2n_1}{1 + e^{\varepsilon}}\right)$. 
Then, we analyze the communications costs of \advss, which include (1) sending the noisy edges from $w$ and downloading them to vertex $u$ and (2) sending the single-source estimator $\fq(u, w)$ to the data curator. 
The dominating cost is incurred by the step (1). 
Note that the expected number of noisy edges from vertex $w$ is $d_w \times (1-p) + (n_1 - d_w) \times p$, where $p = \frac{1}{1 + e^{\varepsilon_1}}$. 
Thus, the overall communication cost is $O(\frac{e^{\varepsilon_1}-1}{e^{\varepsilon_1}+1}d_w + \frac{ n_1}{1 + e^{\varepsilon_1}} )$. 
In the following, we analyze the expected L2 loss of \advss. }

% We analyze the L2 loss of Algorithm \ref{algo:adv1} as follows. 
\begin{theorem}
\label{thm:fq}
The expected L2 loss of $\fq(u,w)$ in Equation \ref{eq:fq} is 
% $\frac{p(1-p)}{(1-2p)^2} d_u  + \frac{2(1-p)^2}{(1-2p)^2\varepsilon_2^2}$
$  O(\frac{ e^{\varepsilon_1}}{(1 - e^{\varepsilon_1})^2} (d_u + \frac{ 2 e^{\varepsilon_1}}{\varepsilon_2^2} ) )$. Here $d_u$ represents the degree of \vq in $G$. 
% $p = \frac{1}{1 + e^{\varepsilon_1}}$ represents the flipping probability from randomized responses. 
\end{theorem}
\begin{proof}
Since $\fq(u,w)$ is an unbiased estimator, its L2 loss equals its variance. 
The variance of $\fq(u,w)$ consists of two parts: $f_u(u,w)$ and the Laplacian noise. 
First, it immediately follows that the variance from $f_u(u,w)$ is $\frac{p(1-p)}{(1-2p)^2} d_u $ based on Equation \ref{eq:phi.var}. This is because each $\phi(u, w)$ is independent of each other and there is no covariance involved. 
In addition, the variance from the Laplacian noise is: 
$$
\mathrm{Var}(Lap(\frac{\Delta(\fq(u, w))}{\varepsilon_2})) = 2 (\frac{1-p}{(1-2p)\varepsilon_2})^2
 = \frac{2(1-p)^2}{(1-2p)^2\varepsilon_2^2}
$$
% Putting these together completes the proof. 
Since $f_u(u,w)$ and the Laplacian noise are independent, the expected L2 loss of $\fq$ is
$\frac{p(1-p)}{(1-2p)^2} d_u  + \frac{2(1-p)^2}{(1-2p)^2\varepsilon_2^2}$. 
Substituting $p = \frac{1}{1+ e^{\varepsilon_1}}$ into the above expression completes the proof. 
\end{proof}
{Since the L2 loss of $\fq(u,w)$ is no longer dependent on $n_1$, the data utility of \advss is significantly improved compared to \bs.} 
% local differential privacy inherits the sequential and parallel composition features mentioned in Section 2.4.
% In addition, to verify that the \advss algorithm satisfies edge LDP, we introduce the following {\em sequential composition} theorem. 
In addition, we check whether Algorithm \ref{algo:adv1} satisfies the privacy requirements of \epldp in the following theorem. 
%%% parallel composition theorem
% \begin{theorem}
% {\bf Sequential Composition \cite{jiang2021applications}.}
% \label{thm:sequentialcomposition}
% Consider $t$ edge LDP algorithms $Z_1, Z_2, \ldots, Z_t$, whose privacy budgets are denoted by $\varepsilon_1, \varepsilon_1, \ldots, \varepsilon_t$, respectively. We have the following property. 
% The composite algorithm obtained by sequentially applying $Z_1, Z_2, \ldots, Z_t$ on the bipartite graph $G$ provides $\sum_{i=1}^{t} \varepsilon_i$-differential privacy.
% \end{theorem}
%  which states that 
% when we sequentially apply various differential privacy algorithms to a 
% dataset
\begin{theorem} 
{Given a bipartite graph $G$ and a privacy budget $\varepsilon$, Algorithm \ref{algo:adv1} satisfies \epldp. }
% \textcolor{red}{TODO: provide formal proof. }
\end{theorem} 
\begin{proof}
{
We use the {\em Sequential Composition} theorem \cite{jiang2021applications} to prove that Algorithm \ref{algo:adv1} satisfies \epldp. 
% Since randomized responses provide \epldp, the first round of Algorithm \ref{algo:adv1} satisfies $\varepsilon_1$-edge LDP (Lines 2-5). 
In the first round, generating the noisy edges via randomized responses satisfies $\varepsilon_1$-edge LDP (Lines 2-6). 
In the second round, Lines 7-13 are conducted locally by the vertex $u$. 
Then, the Laplace mechanism (Line 14) is applied w.r.t. a privacy budget of $\varepsilon_2$ to construct the unbiased estimator $\fq$, which satisfies $\varepsilon_2$-edge LDP. 
% the query vertex $u$ constructs an unbiased estimator $\fq$ by applying the Laplace mechanism w.r.t. a privacy budget of $\varepsilon_2$, which satisfies $\varepsilon_2$-edge LDP. 
By the sequential composition property of edge LDP, Algorithm \ref{algo:adv1} satisfies \epldp ($\varepsilon = \varepsilon_1 + \varepsilon_2$). }
\end{proof}
