\subsection{A double-source estimator for $\pqx$} 
\begin{figure}[tb]
\centering  
\includegraphics[width=0.23\textwidth]{figures/5.0-10.0-2.0_same_plot.pdf}
\includegraphics[width=0.23\textwidth]{figures/5.0-100.0-2.0_same_plot.pdf}
% \includegraphics[width=0.4\textwidth]{figures/fig2.pdf}
% \myspace
\myspace
\myspace
\caption{Illustration of the L2 loss of $\fds$ when $\varepsilon = 2$.}
\label{fig:variancecompare}
\end{figure}

% \textcolor{red}{TODO: add some illustrations}


The single-source estimator $\fq(u,w)$ only involves the neighborhood of \vq. Similarly, we can develop another unbiased estimator, $\fx(u,w)= \sum_{ v \in N(w, G) } \phi(v, u)$, by applying the same process to the neighborhood of \vx. 
% \textcolor{red}{give the formula for $\fx$}
This raises a natural question: How can we integrate these estimators to further minimize L2 loss while maintaining unbiasedness? 
Examining the loss of L2 of $\fq(u,w)$ in Theorem \ref{thm:fq}, we can see that it consists of the first term representing the error incurred by randomized responses, and the second term representing the error incurred by Laplacian noise. 
On the one hand, if we only minimize the first term, we could always choose the estimator between $\fq$ and $\fx$ whose corresponding query vertex has a smaller degree. 
On the other hand, if we only focus on minimizing the second term, we could take an average of $\fq$ and $\fx$ and the Laplacian noise of the resulting estimator will be reduced by half. 
To balance both objectives, we propose a double-source estimator $f^*$ by taking a weighted average of $\fq$ and $\fx$, i.e., $f^* = \alpha \fq+ (1 - \alpha) \fx$ ($\alpha \in [0,1]$). 
Here $\alpha$ is the weighting parameter that adjusts the contribution of $\fq$ and $\fx$. 
By analyzing the L2 loss of $f^*$, we introduce the \advdslong algorithm (\advds), which enhances data utility by optimizing the allocation of privacy budget and balancing the contribution of $\fq$ and $\fx$. 

\noindent
{\bf Properties of the double-source estimator $\fds$.} 
% \noindent
% {\bf  Unbiasedness of the double-source estimator $\fds$.} 
Given that $\fds$ is a weighted average of $\fq$ and $\fx$, its unbiasedness directly stems from the principle of linearity in expected values, i.e., $\mathbb{E}(aX + bY) = a\mathbb{E}(X) + b\mathbb{E}(Y)$. 
Thus, based on the bias-variance decomposition, its L2 loss equals its variance, as analyzed in the following theorem. 
\begin{theorem}
\label{thm:balance}
The L2 loss of $f^* = \alpha \fq+ (1 - \alpha) \fx$ ($\alpha \in [0,1]$) is $\frac{ e^{\varepsilon_1}}{(1 - e^{\varepsilon_1})^2} \left( (\alpha^2 d_u+(1-\alpha)^2d_w)  + \frac{ 2(\alpha^2 + (1-\alpha)^2)e^{\varepsilon_1}}{\varepsilon_2^2} \right)
$.
% $\frac{p(1-p)}{(1-2p)^2} (\alpha^2 d_u + (1 - \alpha)^2 d_w) +
% \frac{2(1-p)^2}{(1-2p)^2\varepsilon_2^2}(\alpha^2 + (1 - \alpha)^2)$. 
Here $d_u$ and $d_w$ represent the degrees of \vq and \vx in $G$. 
% $p = \frac{1}{1 + e^{\varepsilon_1}}$ represents the flipping probability from randomized responses. 
\end{theorem}
\begin{proof}

\begin{align*}
    & l2(f^*, \pqx)  = \text{Var}(f^*) = \alpha^2 \text{Var}(\fq) + (1 - \alpha)^2 \text{Var}(\fx) \notag \\
    % & = \alpha^2\left( \frac{p(1-p)}{(1-2p)^2} d_u + \frac{2(1-p)^2}{(1-2p)^2\varepsilon_2^2} \right) + \\
    % & (1 - \alpha)^2 \left( \frac{p(1-p)}{(1-2p)^2} d_w+ \frac{2(1-p)^2}{(1-2p)^2\varepsilon_2^2} \right) \\
    & = \frac{p(1-p)}{(1-2p)^2} (\alpha^2 d_u + (1 - \alpha)^2 d_w) +
     \frac{2(1-p)^2}{(1-2p)^2\varepsilon_2^2}(\alpha^2 + (1 - \alpha)^2)
     \label{xxx}
\end{align*}  
{
Note that $\tilde{f_u}$ depends on the noisy edges connected to $w$, while $\tilde{f_w}$ depends on the noisy edges connected to $u$. Since they depend on disjoint edges in the noisy graph, $\tilde{f_u}$  and $\tilde{f_w}$ are independent, and their covariance $Cov(\fq, \fx)=0$. Thus, the first step holds. }
% Thus, $\tilde{f_u}$  and $\tilde{f_w}$  are independent and the first step holds. }
% (2) the noisy edges connected to u. 
% The first step holds because $\fq$ and $\fx$ are independent and their covariance $Cov(\fq, \fx)=0$. 
Also, the expected L2 loss of $\fx$ is the same as $\fq$ where $d_u$ is replaced by $d_w$. 
Substituting $p = \frac{1}{1+ e^{\varepsilon_1}}$ completes the proof. 
\end{proof}


{Based on Theorem \ref{thm:balance}, the L2 loss of $\fds$ is a function of $\varepsilon_1$ and $\alpha$, where $d_u$ and $d_w$ are constants. 
We denote it by $l2(f^*, \pqx) := F(\varepsilon_1, \alpha)$. 
To obtain estimates for $d_u$ and $d_w$, we can apply the Laplace mechanism in an additional round using a small privacy budget ($\varepsilon_0$). 
If the estimates of $d_u$ and $d_w$ are negative, we can estimate the average vertex degree in $L(G)$ and substitute them.} 
To minimize this loss, we seek values of $\varepsilon_1 \in (0, \varepsilon)$ and $\alpha \in [0,1]$ that minimize $F(\varepsilon_1, \alpha)$. 
We discover that $F$ reaches its global minimum if and only if its partial derivatives $\frac{\partial F}{\partial \alpha} = \frac{\partial F}{\partial \varepsilon_1} =0$. 
However, this results in a transcendental equation that lacks analytical solutions. 
Thus, we resort to Newton's method \cite{galantai2000theory} for high-precision approximate solutions. 
By optimizing $\varepsilon_1$ and $\alpha$, the resulting L2 loss of $\fds$ will be lower than that of both single-source estimators $\fq$ and $\fx$. 

{
We could also optimize the expected L2 loss for \advss, which is a function of $\epsilon_1$ and $deg(u)$. 
Specifically, we could spend a small privacy budget ($\varepsilon_0$) to estimate $deg(u)$ and then apply Newton's method to find the best privacy budgets ($\varepsilon_1$ and $\varepsilon_2$) that minimize the expected L2 loss of \advss ($\varepsilon = \sum_{i=0}^2 \varepsilon_i $). 
In practice, this implementation only outperforms the current \advss with an even separation of privacy budget ($\varepsilon_1 = \varepsilon_2$) when the degree of $u$ is large. 
In addition, it is a special case of \advds where $\alpha =1$. 
}


% \begin{theorem}
% \label{thm:compare}
% The minimum L2 loss incurred by the double-source estimator $\fds$ is less than or equal to the L2 loss incurred by both single-source estimators $\fq$ and $\fx$. 
% $$
% \min_{\varepsilon_1, \alpha} \ l2(\fds, \pqx) \leq \min(l2(\fq, \pqx), l2(\fx, \pqx))
% % \min_{\varepsilon_1, \alpha} \ \text{L2}(\mathcal{F}_{DS}, \mathcal{P}_{QX}) \leq \min(\text{L2}(\mathcal{F}_Q, \mathcal{P}_{QX}), \text{L2}(\mathcal{F}_X, \mathcal{P}_{QX}))
% $$
% % The L2 loss of $f^*$ is smaller that
% % $\frac{p(1-p)}{(1-2p)^2} (\alpha^2 d_u + (1 - \alpha)^2 d_w) +
% % \frac{2(1-p)^2}{(1-2p)^2\varepsilon_2^2}(\alpha^2 + (1 - \alpha)^2)$. 
% % Here $d_u$ and $d_w$ represents the degrees of \vq and \vx in $G$. 
% % $p = \frac{1}{1 + e^{\varepsilon_1}}$ represents the flipping probability from randomized responses. 
% \end{theorem}
% Theorem \ref{thm:compare} immediately follows because $\fq$ and $\fx$ are special cases of $\fds$ when $\alpha = 1$ and $\alpha = 0$, respectively. 
% % When $\varepsilon_1$  and $\alpha$ are found to minimize the L2 loss of $\fds$, it will be 
% To illustrate the comparison, we plot the L2 loss of $\fds$, $\fq$, and $\fx$ against varying $\varepsilon_1$ values in Fig.~\ref{fig:variancecompare}, where $\varepsilon=2$. 
% The blue curve labelled ``$\alpha = 0$'' represents the L2 loss of $\fx$, while the red curve labelled ``$\alpha = 1$'' represents the L2 loss of $\fq$. 
% The green curve labelled ``$\alpha = 0.5$''  represents the unbiased estimator $f' = (\fq + \fx)/2$. The grey horizontal line marks the global minimum of L2 loss for $\fds$. 
% On the left when $d_u = 5$ and $d_w = 10$, $f'$ outperforms $\fq$ and $\fx$ and reaches the global minimum. 
% On the right when $d_u$ and $d_w$ are more imbalanced, $\fq$ becomes the best-performing estimator that reaches the global minimum. 
% We can see that none of the single-source estimators or their average can consistently reach the minimized L2 loss of $\fds$ for all query vertex pairs. 
% This improvement is attributed to the flexibility of $\fds$, i.e., the ability to adjust the privacy budget allocation and the weighting of $\fq$ and $\fx$ in an ad-hoc manner. 

\begin{algorithm}[tbh]
    \small
	\caption{The \advds algorithm}
	\label{algo:adv2}
	\LinesNumbered
	\KwIn{
            $G$: a bipartite graph; 
            $\varepsilon$: a privacy budget; 
            \vq, \vx: two query vertices
        }
         \KwOut{$\fq(u, w) $}
        % split privacy budget $\varepsilon$ into  $\varepsilon_1$ and $\varepsilon_2$;\\
        \tcp{\textbf{round 1:}}
        $\varepsilon_0 \gets \varepsilon \times 0.05$;\\
        % $\varepsilon_0 \gets$ a small part of $\ep$;\\
        $ d_u \gets deg(u, G) + \text{Lap}(\frac{1}{\varepsilon_0})$;\\
        $ d_w \gets deg(w, G) + \text{Lap}(\frac{1}{\varepsilon_0})$; \\
        $d' \gets$ the average vertex degree on the same layer as \vq;\\
        correct $d_u$ and $d_w$ with $d'$;\\
        find $\alpha$ and $\varepsilon_1$ that minimizes $\text{Var}(f^*)$;\\
        \tcp{\textbf{round 2:}}
        \input{RR-DS}
         \tcp{\textbf{round 3:}}
            $\varepsilon_2 \gets \varepsilon - \varepsilon_0 -\varepsilon_1$;\\
            $\fq(u, w) \gets $ the estimator by running Lines 8-15 of Algorithm \ref{algo:adv1};\\
            $\fx(u, w) \gets $ the estimator by running Lines 8-15 of Algorithm \ref{algo:adv1} with \vq and \vx switched;\\                     
	\textbf{return} $\alpha \fq + (1 - \alpha) \fx$;\\
\end{algorithm}


\noindent
{\bf The \advds algorithm. }
In this part, we present the \advdslong algorithm (\advds) which uses an additional round compared to \advss to estimate $d_u$ and $d_w$ and estimate the L2 loss of $\fds$. 
The detailed steps are outlined in Algorithm \ref{algo:adv2}. 
% The \advss algorithm is designed to estimate $P_2(u, w)$ for all vertices \vx on the same layer as the query vertex \vq. It takes as input a bipartite graph $G$, a privacy budget $\varepsilon$, and the query vertex \vq. The output is $P_2(u, w)$ for all vertices \vx on the same layer as \vq.
In the first round, \advds uses a small privacy budget $\varepsilon_0$ and applies the Laplace mechanism to obtain unbiased estimates of $d_u$ and $d_w$ (Lines 1-3). 
{Here the global sensitivity of $d_u$ ($d_w$) is one because adding or deleting an edge from the neighbor list of \vq (\vx) changes $d_u$ ($d_w$) by at most one. }
Due to the Laplacian noise, the reported $d_u$ and $d_w$ could be negative. In this case, we correct for any negative value with the estimated average degree of the vertices on the same side as \vq and \vx (Lines 4, 5). 
Then, the \advds algorithm invokes Newton's method to find the pair of $\alpha$ and $\varepsilon_1$ that minimizes the estimated L2 loss of $\fds$. 
In the second round, randomized responses are applied to \vq and \vx with respect to $\varepsilon_1$, leading to the noisy graph $G_{\varepsilon_1}'$ (Lines 7-12). 
% In the third round, \advds uses the remaining privacy budget $\varepsilon_2$ to build the unbiased estimators $\fq$ and $\fx$ from the local neighborhoods of \vq and \vx. 
% Specifically, $\fq$ is obtained by executing Lines 8-15 of \advss. 
% Similarly, $\fx$ is computed by executing Lines 8-15 of \advss by visiting the neighbors of \vx in $G$. 
In the third round, \advds allocates the remaining privacy budget $\varepsilon_2$ to construct unbiased estimators $\fq$ and $\fx$ from the local neighborhoods of \vq and \vx. Specifically, $\fq$ is derived by executing Lines 8-15 of \advss, while $\fx$ is computed similarly by visiting the neighbors of \vx in $G$ (Lines 14-15). 
Note that when constructing $\fq$ and $\fx$, the global sensitivity analysis is the same as in \advss. In other words, the global sensitivity of $\frac{1-p}{1-2p}$ for each single-source estimator is applied to both $\fq$ and $\fx$ upon construction. 
In the end, \advds returns the weighted average of $\fq$ and $\fx$ where the parameter $\alpha$ is computed in the first round (Line 16). 
% Note that Algorithm \ref{algo:adv2} satisfies \epldp by the sequential composition property in Theorem \ref{thm:sequentialcomposition} ($\varepsilon = \varepsilon_0 +\varepsilon_1 + \varepsilon_2$). 





{
\noindent
{\bf Theoretical analysis for \advds.} 
Without loss of generality, we assume that $u$ and $w \in L(G)$. 
First, we analyze the computational time complexity of \advds. 
On the vertex side, estimating the average degree of the vertices in $L(G)$ takes $O(n_2)$ time. 
When constructing the noisy graph, the time costs incurred by the randomized responses are $O(n_1)$. 
On the curator side, visiting the neighbors of $u$ and $w$ to compute $\fq$ and $\fx$ takes $O(deg(u, G)  + deg(w, G) $ time. 
Thus, the overall time complexity is $O(n)$.}

{We then analyze the communication costs of \advds, which include: 
    (1) sending the noisy degree of all vertices in $L(G)$,
    (2) sending the noisy edges from $w$ and downloading them to vertex $u$,
    (3) sending the noisy edges from $u$ and downloading them to vertex $w$, and
    (4) sending two single-source estimators $\fq$ and $\fx$ to the data curator. 
Step (1) incurs communication costs of $O(n_2)$. 
The communication costs for Step (2) and Step (3) are proportional to the expected number of noisy edges from $u$ and $w$, which is $(d_u + d_w) \times (1-p) + 2(n_1 - d_w) \times p$, where $p = \frac{1}{1 + e^{\varepsilon_1}}$. 
Step (4) incurs a communication cost of $O(1)$. 
Thus, the overall communication cost is $O(n_2 + \frac{e^{\varepsilon_1}-1}{e^{\varepsilon_1}+1} (d_w+d_u) + \frac{ 2 n_1}{1 + e^{\varepsilon_1}})$. 
}

{
Since the expected L2 loss of \advds has been analyzed in Theorem \ref{thm:balance}, we compare it with the expected L2 loss of \advss in the following theorem. 
\begin{theorem}
\label{thm:compare}
The minimum L2 loss incurred by the double-source estimator $\fds= \alpha \fq + (1 - \alpha) \fx$ is less than or equal to the L2 loss incurred by both single-source estimators $\fq$ and $\fx$. 
$$
\min_{\varepsilon_1, \alpha} \ l2(\fds, \pqx) \leq \min(l2(\fq, \pqx), l2(\fx, \pqx))
$$
\end{theorem}
\begin{proof}
    Let $L^*$ be the minimized expected L2 loss of the double-source estimator $\fds$. 
    To prove the above inequality, we need to prove that $L^* \leq l2(\fq, \pqx)$ and $L^* \leq l2(\fx, \pqx)$. 
    By construction, $\fq$ is a special case of $\fds$ where $\alpha =1$, i.e., $\fq = \fds |_{\alpha =1}$. 
    Hence, for any privacy budget allocations ($\varepsilon_1$ and $\varepsilon_2$), we have $L^* \leq l2(\fq, \pqx)$. 
    Similarly, $\fx$ is also a special case of $\fds$ where $\alpha =0$, i.e., $\fq = \fds |_{\alpha =0}$. 
    We also obtain $L^* \leq l2(\fx, \pqx)$. 
    Combining these two inequalities completes the proof. 
\end{proof}
}

To illustrate the comparison, we plot the L2 loss of $\fds$, $\fq$, and $\fx$ against varying $\varepsilon_1$ values in Fig.~\ref{fig:variancecompare}, where $\varepsilon=2$. 
The blue curve labeled ``$\alpha = 0$'' represents the L2 loss of $\fx$. 
The red curve labeled ``$\alpha = 1$'' represents the L2 loss of $\fq$. 
The green curve labeled ``$\alpha = 0.5$''  represents the unbiased estimator $f' = (\fq + \fx)/2$. 
The grey horizontal line marks the global minimum L2 loss for $\fds$. 
On the left, when $d_u = 5$ and $d_w = 10$, $f'$ outperforms $\fq$ and $\fx$ and reaches the global minimum. 
On the right, when $d_u$ and $d_w$ are more imbalanced, $\fq$ becomes the best estimator, reaching the global minimum. 
None of the single-source estimators or their average can consistently reach the minimized L2 loss of $\fds$ for all query vertex pairs. 
This is due to the flexibility of $\fds$ in adjusting the privacy budget allocation and the weighting of $\fq$ and $\fx$.


In the following theorem, we verify the compliance of Algorithm \ref{algo:adv2} to \epldp. 
\begin{theorem} 
{
Given a bipartite graph $G$ and a privacy budget $\varepsilon$, Algorithm \ref{algo:adv2} satisfies \epldp. }
% \textcolor{red}{TODO: provide formal proof. }
\end{theorem} 
\begin{proof}
{
We use the {\em Sequential Composition} and {\em Parallel Composition} theorems for \epldp \cite{jiang2021applications}. 
Parallel composition theorem states that if different \epldp algorithms are applied to disjoint datasets with privacy budgets \(\varepsilon_i\), the composite algorithm satisfies \(\max_i \varepsilon_i\)-LDP. 
In the first round, each vertex reports its degree using the Laplace mechanism, achieving \(\varepsilon_0\)-edge LDP. 
By parallel composition, this round satisfies \(\varepsilon_0\)-edge LDP. 
In the second round, randomized responses provide \(\varepsilon_1\)-edge LDP. 
In the third round, building \(\fq\) and \(\fx\) satisfies \(\varepsilon_2\)-edge LDP. 
By parallel composition, this round satisfies \(\varepsilon_2\)-edge LDP. 
By sequential composition, Algorithm \ref{algo:adv2} satisfies \(\epsilon\)-LDP with \(\varepsilon = \varepsilon_0 + \varepsilon_1 + \varepsilon_2\). 
% Thus, the theorem holds.
}
\end{proof}


\noindent
{\bf Summary of the expected L2 losses of all algorithms.} 
{
% In Table \ref{tab:complexitycompare}, we summarize the computational time complexities, expected L2 losses, and communication costs of all privacy-preserving algorithms for the estimation of the common neighborhood. 
In Table \ref{tab:complexitycompare}, we summarize the expected L2 losses of all privacy-preserving algorithms for estimating the number of common neighbors. 
%%% compare the L2 loss of all algorithms 
% The expected L2 loss of \bs is smaller than \naive, being $O(n_1)$ instead of $O(n_1^2)$. 
The expected L2 loss of \bs is smaller than that of \naive, with \bs having an expected L2 loss of $O(n_1)$ compared to \naive's $O(n_1^2)$. 
In addition, the expected L2 losses of \advss and \advds are lower than those of \naive and \bs because they do not depend on the number of vertices in the graph. 
Between \advds and \advss, as analyzed in Theorem 9, the minimized loss for \advds is smaller than \advss because \advss is a special case of \advds where $\alpha = 0$ or $\alpha = 1$. 
% We also summarize the computational time complexities and communication costs of these algorithms. We will evaluate their performance through experiments.
Note that for \bs, \advss, and \advds, which are unbiased estimators, their expected L2 losses can offer insight into their deviation from the true value by applying \emph{Chebyshev's inequality} \cite{saw1984chebyshev}. 
For instance, for the \bs algorithm, we know $\mathbb{E}(\foneround(u, w)) = \pqx$ and that $\mathrm{Var}(\foneround(u, w)) = \frac{p^2(1-p)^2}{(1-2p)^4} n_1
    + \frac{p(1-p)}{(1-2p)^2}(d_w + d_w ) $. 
Chebyshev's inequality states that for any $k >0$: 
$$
P\left(|\foneround(u, w) - \pqx| \geq k \sqrt{\mathrm{Var}(\foneround(u, w))} \right) \leq \frac{1}{k^2}.
$$
Similar probabilistic bounds can be derived for the \advss and \advds algorithms based on their expected L2 losses.}
% Let $X$ be a random variable with finite expected value $\mu = \mathbb{E}[X]$ and finite variance $\sigma^2 = \mathbb{V}[X]$. For any $k > 0$,

% \begin{equation}
% \Pr(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}.
% \end{equation}



% \input{tabcomplexity}
\input{datatab}
\input{tabcomplex2}
