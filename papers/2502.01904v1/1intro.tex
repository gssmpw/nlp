\section{Introduction}
Bipartite graphs have been widely used to represent connections between two sets of entities. 
Examples of real-world bipartite graphs include user-item networks in E-commerce \cite{wang2006unifying, li2020hierarchical}, people-location networks in contact tracing \cite{chen2021efficiently}, and user-page networks in social analysis \cite{o2010essentials}. 
\input{motivation}
%% backbone
{
In bipartite graphs, finding the common neighbors of two vertices is a basic operation in many tasks. For example, the similarity between two vertices can be computed using Jaccard similarity, which is the ratio of the number of their common neighbors to their combined unique neighbors \cite{leicht2006vertex, tsourakakis2014toward, yang2022efficient}. 
Additionally, common neighbor counts can help prune unpromising vertices in $(p,q)$-biclique counting \cite{yang2021p, ye2023efficient}. 
Other tasks that benefit from counting common neighbors in bipartite graphs include anomaly detection \cite{sun2005neighborhood}, bipartite graph projection \cite{stankova2021node, zhang2023bipartite}, bipartite clustering coefficient computation \cite{aksoy2017measuring, huang2010link}, community search \cite{dong2021butterfly, abidi2022searching, DBLP:journals/pvldb/WangWLZZ24}, 
and wedge-based motif counting \cite{xu2022efficient, wang2023efficient}. }


Although computing common neighbors is straightforward in the conventional setting, it inevitably involves releasing the neighborhood information of the vertices, posing a significant privacy risk for users in real-world applications. For instance, in user-item networks, disclosing identical items in the shopping carts of two users in online shopping platforms (e.g., eBay and Amazon) significantly compromises users' privacy. 
Hence, it is crucial to estimate the common neighborhood in a privacy-preserving manner, which remains an unresolved research gap.

%%% DP is the gold standard for privacy-preserving
In the literature, {\em differential privacy} (DP) \cite{dwork2014algorithmic} has become the gold standard for privacy-preserving computation, which provides a mathematical framework to quantify permissible privacy loss. 
%%% for graph data, edge LDP is widely used. 
Among the various DP models, {\em edge local differential privacy} (edge LDP) \cite{qin_generating_2017, zhang2018two, ye2020lf} has been widely adopted to protect the user's private connections. 
Edge LDP is a robust privacy protection protocol that requires each vertex to perturb its local data (e.g., degrees and neighbors) before transmitting it to the data curator. 

In this paper, we study the problem of privacy-preserving common neighborhood estimation over bipartite graphs. Specifically, given a bipartite graph $G$ and two query vertices \vq and \vx, we aim to estimate the number of common neighbors of \vq and \vx in $G$ on the same vertex layer with edge LDP. 
%% what is edge LDP
A random algorithm satisfies edge LDP when the probabilities of observing its output from any two vertices, whose neighbor sets differ by one vertex, are indistinguishable within a factor of $e^{\varepsilon}$. This ensures deniability for the existence of the edge $(u,v)$.
%%% what is privacy budget
In this context, $\varepsilon$ represents the privacy budget, determining the acceptable level of privacy loss. Clients can adjust this parameter to tradeoff between privacy and data utility. 
{
In addition, common neighbor counting under edge LDP is the first step in addressing other problems under edge LDP, such as vertex similarity computation \cite{leicht2006vertex} and $(p,q)$-biclique counting \cite{yang2023p}. 
}


\noindent
{\bf Challenges.} 
In this paper, we aim to design accurate and efficient common neighbor estimation algorithms on bipartite graphs with edge LDP. We face the following three main challenges. 

\noindent 
{\em $\bullet$ Challenge 1:} 
In the literature, Randomized Response \cite{warner1965randomized} is widely employed to achieve edge LDP, flipping the entries of the adjacency matrix between ``0'' and ``1'' to construct a noisy graph. However, counting common neighbors on the noisy graph results in severe overcounting and bias, because the noisy graph is generally much denser. For instance, we examine the performance of this naive approach (\naive) across 1000 runs for a pair of query vertices on the dataset \texttt{rmwiki} in Fig..~\ref{fig:distribution}. 
The blue distribution, representing the estimates of \naive, deviates significantly to the right from the true count (indicated by the black dashed line). This substantial shift highlights the difficulty in accurately estimating the number of common neighbors using the noisy graph. 


% Some of the existing approaches \cite{imola2021locally, imola2022communication} adopt {\em Randomized Response} \cite{warner1965randomized} to provide edge LDP, which perturbs each vertex's edges and uses these noisy edges to construct a noisy graph. 
%% then briefly explain that RR will result in a very noisy graph 
% When constructing the noisy graph to satisfy edge LDP, randomized responses are applied to certain rows and columns of the adjacency matrix, which represents the connections between vertices in the bipartite graph. In the affected rows and columns, each entry is flipped (between ``0'' and ``1'') with a fixed probability. 
%% conclusion, this noisy graph presents challenges
% This process results in a much denser noisy graph, posing a challenge in accurately estimating common neighbors without bias.

\noindent 
{\em $\bullet$ Challenge 2:} 
Due to the constraints of edge LDP, we have to start with all vertices on the opposite layer of the query vertices as a candidate pool to estimate the common neighbors. 
This involves potentially $O(n)$ independent random variables, leading to a substantial margin of error. 
Reducing this candidate pool and developing an unbiased estimator that relies on fewer random variables for enhanced data utility presents a challenging task. 
% (1) the large candidate pool of common neighbors that includes all vertices on the opposite layer of the query vertices and 
% The candidate pool of the common neighbors includes all vertices on the opposite layer of the query vertices, leading to poor data utility. 
%%% challenge: how to construct an unbiased estimator that relies on fewer random variables, which is more likely to incur smaller variance. 

\noindent 
{\em $\bullet$ Challenge 3:} 
% \textcolor{red}{TO Do: elaborate}
% When the vertices and the data curator can interact for multiple rounds, determining the optimal way to distribute the privacy budget to different rounds based on the query vertices is difficult. 
% When the vertices and the data curator can interact for multiple rounds, the privacy budget needs to be split to be allocated to each round. The privacy budgets allocated to each round sum up to epsilon, meaning that if more is devoted to one round, less is available for others, making it challenging to manage errors through privacy budget distribution across multiple rounds.
% When vertices and the data curator interact over multiple rounds, the privacy budget must be divided and allocated to each round. 
% However, because the privacy budgets allocated to each round sum up to epsilon, allocating more to one round reduces the budget available for others, posing challenges in managing errors through privacy budget distribution across multiple rounds.
When the vertices and the data curator can interact for multiple rounds, the privacy budget must be divided among each round. However, allocating more budget to one round reduces the budget available for others. 
In addition, different pairs of query vertices likely need to be handled differently. 
% For instance, when the vertex degrees are highly imbalanced, how
Thus, finding the optimal allocation of privacy budgets to different rounds based on the query vertices requires special attention. 



\noindent
{\bf Our approaches.} 
To address Challenge 1, we propose a one-round algorithm \bs that obtains unbiased estimates of common neighbors by leveraging the probability at which the entries in the adjacency matrix are flipped to compensate for over-counting. 
Specifically, for each query vertex, \bs applies randomized responses to both query vertices to generate noisy edges. 
For every vertex $v$ on the opposite layer of the query vertices, we estimate whether $v$ is a common neighbor of $u$ and $w$. 
Then \bs aggregates these estimates to obtain an unbiased estimate. 
While \bs addresses the challenge of the dense noisy graph and generates unbiased estimates, it relies on a large pool of candidates, compromising the utility of the data. 
As shown in Fig.~\ref{fig:distribution}, the yellow distribution representing the estimates of \bs appears symmetrical around the true common neighbor count with fat tails on both sides. 
This implies that the estimates are unbiased but have high variance. 


To address Challenge 2, we propose a multiple round framework, allowing the query vertices to download the noisy edges from the previous round and reduce the candidate pool to their neighbors. 
Under this framework, we devise a single-source algorithm \advss, which returns an unbiased estimator for the number of common neighbors by leveraging the neighborhood of $u$. 
%% step 1. neighbor list not yet define, just say randomized response. 
Specifically, in the first round, \advss applies randomized responses to vertex $w$ to generate noisy edges. 
In the second round, the vertex $u$ retrieves these noisy edges from the data curator and then constructs a single-source estimator using its neighbors in the original graph and the neighbors of $w$ in the noisy graph. 
%%% the Laplacian step
To comply with edge LDP, the Laplace mechanism \cite{dwork2014algorithmic} is used to add noise to the single-source estimator before it is released to the data curator. 
This noise is scaled with the global sensitivity, defined as the maximum difference observed in the single-source estimator between two vertices whose neighbors differ by one edge. 
\advss achieves significantly better data utility compared to \bs by employing a multiple-round framework that reduces the candidate pool of common neighbors to the neighborhoods of the query vertices. 
As shown in Fig.~\ref{fig:distribution}, the green distribution represents the estimates of \advss, which preserves the unbiasedness and is more concentrated around the true value compared to \bs. 

\begin{figure}[thb]
\centering  
\includegraphics[width=0.38\textwidth]{figures/distribution.pdf}
\myspace
\caption{The estimate distribution on \texttt{rmwiki} when $\varepsilon=1$.
}
\myspace
\myspace
\label{fig:distribution}
\end{figure}

%% overview 
To tackle Challenge 3, we propose a double-source algorithm \advds under the multiple-round framework, which integrates two single-source estimators via a weighted average. 
In addition, we propose novel privacy budget allocation optimizations that allow \advds to dynamically adjust its privacy budgets for different rounds and the contribution of each single-source estimator for minimized L2 loss. 
%% steps
% By analyzing the L2 loss of the double-source estimator, we discover that it depends on the degrees of both query vertices, the privacy budget allocation to different rounds, and the weight of each single-source estimator. 
% Then, we utilize Newton's method to find the best privacy budgets for different rounds and the 
% impact 
% Thus, we utilize a small part of the privacy budget to estimate the query vertices' degrees and formulate the L2 loss as a multi-variable function, which can be minimized via Newton's method. 
%% describe the effects of these optimizations 
Specifically, when the incoming query vertices have large degrees, \advds tends to devote more privacy budget to noisy graph construction. %%% because?  
When the query vertices have very imbalanced degrees, \advds will favor the single-source estimator associated with the low-degree vertex, which depends on fewer random variables and induces less variance. 
%%% because the 
%%% we discover that the L2 loss of the double source estimator depends on the degrees of both query vertices, the privacy budget allocation, and the weight parameter. 
%%% Thus, we utilize a small proportion of the privacy budget to estimate the degrees of the query vertices and then leverage Newton's method to find epsilon 1 and alpha that minimize L2 loss. 
%%% specific steps (what it does)
% Specifically, in the second round, we build another unbiased estimator $\fx$ by switching $u$ and $w$. 
% Then, we build a double-source estimator $\fds$ by taking a weighted average of $\fq$ and $\fx$. 
%%% what it accomplishes
In doing so, \advds further reduces L2 loss compared to \advss and is more robust to query vertices with high degrees or unbalanced degrees. 
In Fig.~\ref{fig:distribution}, the red distribution depicts the estimates of \advds. 
Here the degrees of the query vertices are highly imbalanced ($556$ and $2$). 
In this case, \advds yields unbiased and more concentrated estimates compared to \advss. 

\noindent
{\bf Contributions.} Here we summarize our principal contributions. 

\noindent 
{\em $\bullet$} To the best of our knowledge, we are the first to study accurate and efficient common neighborhood estimation on bipartite graphs under edge LDP. 


\noindent 
{\em $\bullet$} 
To address the over-counting issue with the \naive algorithm, we propose a one-round algorithm \bs to return unbiased estimates, which leverages the probabilistic nature of the noisy graph to compensate for over-counting. 

\noindent 
{\em $\bullet$} 
% We adopt the multiple-round framework and propose the multiple-round single-source algorithm, which xxxx. 
We propose a multiple-round framework and devise a single-source algorithm (\advss), 
which allows one query vertex to download the noisy edges from the other query vertex and construct an unbiased estimator locally. 
This drastically reduces L2 loss because the search scope for the common neighbors is reduced to the neighborhood of one query vertex. 


\noindent 
{\em $\bullet$} 
Under the multiple-round framework, we propose a double-source algorithm (\advds) that constructs two unbiased estimators for each query vertex and combines them through a weighted average. \advds further reduces L2 loss by dynamically adjusting the allocation of the privacy budget and the contribution of two estimators based on the incoming query vertices. 


\noindent 
{\em $\bullet$}
We conduct extensive experiments on $15$ real-world datasets to evaluate the proposed algorithms. The multiple-round algorithms \advss and \advds produce significantly smaller mean relative errors than \naive and \bs across all datasets. 
\advds is especially robust to query vertices with imbalanced degrees. 
% In addition, \advds consistently achieves better data utility than \advss and is more robust to vertex pairs with imbalanced degrees. 


% \noindent
% {\bf Organization.} 
% The rest of the paper is organized as follows. 
% Section \ref{preliminary} formally defines the problem and presents the naive solution. 
% Section \ref{sec:bs} presents the one-round algorithm \bs that returns unbiased estimates. 
% Section \ref{sec:adv} presents the multiple-round algorithms \advss and \advds with improved data utility. 
% Section \ref{sec:exp} reports the experimental results. 
% Section \ref{sec:related} reviews the related work and Section \ref{sec:conclusion} concludes the paper. 


% Itâ€™s worth discussion how the proposed solution may be combined with other common operators to achieve overall effectiveness and privacy.
% Estimating the common neighbors of two vertices under edge LDP can benefit many other graph analytical tasks. 
%% for (p, q)-biclique counting 
% (1) For instance, once the common neighbor count of all pairs of vertices is computed, the number of $(2,k)$-bicliques in the bipartite graph for any $k$ can be easily determined. Similarly, by computing the common neighbors of all vertex triplets, we can ascertain the number of $(2,k)$-bicliques for any $k$.\\
%% for vertex similarity estimation, this can be 
% Consider a user-item bipartite network, where each edge represents that a user has bought an item. For two users, if we could estimate the number of common neighbors, i.e., the number of identical items in their shopping cart, we can compute their 
% For two vertices 
% For two vertice, it is easy to obtain estimate of their degrees. The number of common neighbors of two vertices indicate they have the same purchasing habits in a user-item network. 
%% for graph projection 
% (2) When designing graph neural networks for bipartite graphs, a widely adopted approach is to project the bipartite graph into a general graph. For each pair of vertices $u$ $w$, we include an edge with the number of common neighbors between $u$ and $w$ as the edge weight. Computing the number of common neighbors under edge LDP benefits the construction of graph neural networks on bipartite graphs under edge LDP. 
