\section{A one-round approach}
\label{sec:bs}

To address the problem of overcounting with the \naive approach, we propose a one-round algorithm \bs, which exploits the probabilistic nature of the noisy graph to obtain unbiased estimates of $\pqx$. 
Specifically, \bs leverages the flipping probability during randomized responses to construct an unbiased estimator for each vertex on the opposite layer of the query vertices. 
\text{In doing so, we do not need to analyze the global sensitivity for \bs, as it only relies on randomized responses to ensure edge LDP and does not involve the Laplace mechanism.} 
Then, \bs aggregates these estimates to obtain unbiased counts of common neighbors. 
% compute the probabilities at which each vertex on the opposite layer from the query vertices is a common neighbor. 
% the probability at which the entries on the adjacency matrix are flipped during randomized responses and estimates the likelihood of each vertex being a common neighbor of $u$ and $w$. 
% To get started, we first 
% To address Challenge 1, we propose a one-round algorithm \bs that obtains unbiased estimates of common neighbors by leveraging the probability at which the entries in the adjacency matrix are flipped to compensate for over-counting. 
First, we investigate the unbiased estimator of $\mathcal{A}[i,j]$ for two vertices $i$ and $j$ in the bipartite graph. 

% Note that for any two vertices $u$ and $v$, we can estimate the probability that the edge $(u,v)$ exists in the original graph. This also allows us to compute the probability for a given vertex to be a common neighbor of $u$ and $w$ by taking the product of the related edge existence probabilities. 


%%% an unbiased estimtor for A[i,j]
\subsection{An unbiased estimator for $\mathcal{A}[i,j]$}
Consider a bipartite graph $G$. Let $\varepsilon$ be the privacy budget. We use $G_{\varepsilon}'$ to represent the noisy graph from applying randomized responses to the edges in $G$. 
During randomized responses, each entry in the neighbor list is flipped with a probability $p  = \frac{1}{1+e^{\varepsilon}}$. 
% If $\mathcal{A}[i,j]$
Note that each entry $\mathcal{A}'[i,j]$ on the adjacency matrix of the noisy graph follows a Bernoulli distribution with a parameter $p$ (when $\mathcal{A}[i,j]=0$) or $1-p$ (when  $\mathcal{A}[i,j]=1$). 
% Note that the noisy edge $\mathcal{A}[i,j]'$ follows a Bernoulli distribution of $Bin(1-p)$ when $\mathcal{A}[i,j]=1$ and $Bin(p)$ when $\mathcal{A}[i,j]=0$. 
Based on this, we have the following equations which link the expected value of $\mathcal{A}[i,j]'$ and $\mathcal{A}[i,j]$: 
$$
\mathbb{E}(\mathcal{A}'[i,j] ) =
\begin{cases}
    p, & \text{if } \mathcal{A}[i,j] = 0 \\
    1-p & \text{if } \mathcal{A}[i,j]  = 1
\end{cases}
$$
This can be rearranged into one single equation: 
$
\mathbb{E}(\mathcal{A}'[i,j] ) =  \mathcal{A}[i,j] + p (1- 2 \mathcal{A}[i,j])$. 
Solving this equation for $\mathcal{A}[i,j]$ leads to: 
$$
E(\frac{\mathcal{A}'[i,j]  - p }{1-2p}) = \mathcal{A}[i,j]
$$
Let $\phi(i,j) = \frac{\mathcal{A}'[i,j]  - p }{1-2p}$. It immediately follows that $\phi(i,j)$ is an unbiased estimator of $\mathcal{A}[i,j]$. We can also analyze the variance of $\phi(i,j)$: 
\begin{align}
    \mathrm{Var}(\phi(i,j)) =  \mathrm{Var}(\frac{\mathcal{A}'[i,j]  - p }{1-2p}) 
    % = \frac{\mathrm{Var}(\mathcal{A}'[i,j])}{(1-2p)^2} \\ 
    = \frac{p(1-p)}{(1-2p)^2}   
    \label{eq:phi.var}
\end{align}
The last step is because $\mathcal{A}'[i,j]$ is a Bernoulli variable with a probability $p$ or $1-p$, which leads to a variance of $p(1-p)$. 
\begin{algorithm}[tbh]
    \small
	\caption{\bs}
	\label{algo:bs}
	\LinesNumbered
	\KwIn{$G$: a bipartite graph; 
            $\varepsilon$: a privacy budget; 
            \vq, \vx: two query vertices from the same layer} 
	\KwOut{$\foneround$: the one-round unbiased estimator for $\pqx$}
        \input{RR1}
        $\foneround \gets 0$; $p \gets \frac{1}{1 + e ^{\varepsilon}}$;\\
        \ForEach{{\color{black}$v \in V(G'_{\varepsilon})$ on the opposite layer as \vq and \vx }}{
            $\foneround \gets \foneround + (\mathcal{A}'[u, v] - p) (\mathcal{A}'[v, w] - p) / (1-2p)^2  $;\\
       }
        \textbf{return} $\foneround$;\\
\end{algorithm}

\subsection{An unbiased estimator for $\pqx$}
In this part, we derive an unbiased estimator for $\pqx$ based on the noisy graph from randomized responses. 
{\color{black}Without loss of generality, let's assume that both \vq and \vx are in $L(G)$. 
By definition, $\pqx = | N(u, G) \cap N(w, G)  | = 
\sum_{v \in U(G) } \mathcal{A}[u,v] \mathcal{A}[v,w]$. 
This implies that we need to estimate $\mathcal{A}[u,v] \mathcal{A}[v,w]$ for all $v \in U(G)$ in an unbiased way. 
Since $\mathcal{A}'[u,v]$ and $\mathcal{A}'[v,w]$ are independent of each other, we have $\mathbb{E}(\phi(u, v) \phi(v, w)) = \mathcal{A}[u,v] \mathcal{A}[v,w]$, which leads to the following estimator for $\pqx$. }

\begin{theorem}
\label{thm:f2}
Consider a bipartite graph $G$, a privacy budget $\varepsilon$, and two vertices \vq and \vx in $L(G)$. 
Let $G_{\varepsilon}'$ be the noisy graph after applying the randomized response to $G$ w.r.t. $\varepsilon$. 
Let $p = \frac{1}{1+e^{\varepsilon}}$ be the flipping probability. 
We have
$
\mathbb{E}( \foneround(u, w) ) = \pqx
$
where  
\begin{align}
    \foneround(u, w) = \sum_{ v \in U(G)}  \frac{(\mathcal{A}'[u,v] - p) (\mathcal{A}'[v,w] - p )}{(1-2p)^2}
    \label{eq:fbs}
\end{align}
is an unbiased estimate for $\pqx$. 
\end{theorem}

The proof of Theorem \ref{thm:f2} involves summing $\phi(u, v) \phi(v, w)$ over all $v \in U(G)$. 
In particular, when \vq and \vx belong to $U(G)$, similar results are derived with $v$ in $L(G)$. 
Based on Theorem \ref{thm:f2}, we design a one-round algorithm \bs, which estimates $\pqx$ based on the noisy graph generated from randomized responses, as outlined in Algorithm \ref{algo:bs}. In Lines 1-6, \bs constructs a noisy graph $G'_{\varepsilon}$ by applying randomized responses to \vq and \vx. 
%%%
Then, it computes $\foneround(u, w)$ by considering all vertices on the opposite layer (i.e., $U(G)$) as candidates for the common neighbors of \vq and \vx (Lines 7-8). 
%% note that this step can be implemented efficiency by computing the intersection and union of $N(u, G'_{\varepsilon})$ and $N(w, G'_{\varepsilon})$. 
In practice, to efficiently compute $\foneround(u, w)$ and avoid visiting all candidate vertices, the expression in Equation \ref{eq:fbs} can be expanded as such. 
% Note that the expression in Equation \ref{eq:fbs} for $\foneround(u, w)$ can be expanded in the following for efficient computation. 
{\color{black}
\begin{align*}
\foneround(u, w) & = N_1  \frac{{(1 - p)^2}}{{(1 - 2p)^2}} 
\hspace{-0.5mm} - (N_2 - N_1)  \frac{{(1 - p)  p}}{{(1 - 2p)^2}} 
\hspace{-0.5mm} + ( n_1-N_2 )  \frac{{p^2}}{{(1 - 2p)^2}}
\end{align*}
}
Here $N_1$ denotes the number of common neighbors of $u$ and $w$ in the noisy graph $G_{\varepsilon}'$. 
$N_2$ denotes size of the union of the neighbor sets of $u$ and $w$ in $G_{\varepsilon}'$. 
{\color{black}$n_1$ represents the number of vertices in $|U(G)|$ (i.e., the opposite layer of $u$ and $w$). }
In this way, we only need to compute the intersection and union of the neighbor sets of $u$ and $w$ in $G_{\varepsilon}'$ to efficiently obtain $\foneround(u, w)$. 


\begin{example}
% \textcolor{red}{TODO: need to mention here we use $n_2$}
We illustrate the construction of the unbiased estimator $\foneround$ returned by the \bs algorithm using the bipartite graph shown in Fig.\ref{fig:motivation}, focusing on $u_1$ and $u_2$ as query vertices with three common neighbors. 
The outline of the query vertices is highlighted in red. 
On the right side of Fig.\ref{fig:motivation}, we present the noisy graph constructed by applying randomized responses to $u_1$ and $u_2$. Note that we only need to apply randomized responses to $u_1$ and $u_2$. The dashed lines in the graph represent the resulting noisy edges. 
The vertices shaded in grey depict the candidate pool for common neighbors between $u_1$ and $u_2$ in \bs, including all vertices on the opposite layer from $u_1$ and $u_2$. 
According to Equation \ref{eq:fbs}, $\foneround$ relies on $n_2 = |L(G)|=100$ random variables: $\mathcal{A}'[v_i, u_2]$, where $i \in [1, 100]$. 
\end{example}


\noindent
{\bf Theoretical analysis for \bs.} %% time complexity and expected L2 loss
{\color{black}
Without loss of generality, we assume that the query vertices $u$ and $w$ are in $L(G)$ in the following analyses. 
On the vertex side, the time costs incurred by randomized responses is $O(n_1)$, where $n_1$ is the number of vertices in $U(G)$. 
On the curator side, the dominating cost is incurred by computing $\foneround(u, w)$ in Lines 7-8, which can be implemented in $O(deg(u, G'_{\varepsilon}) + deg(w, G'_{\varepsilon}))$ time by computing the intersection and the union of $N(u, G'_{\varepsilon})$ and $N(w, G'_{\varepsilon})$. The overall time complexity is $O(n_1)$. 
The communication costs of \bs are incurred only during randomized responses, similar to \naive. The overall communication cost is $O\left(\frac{e^{\varepsilon}-1}{e^{\varepsilon}+1}(d_u + d_w) + \frac{2n_1}{1 + e^{\varepsilon}}\right)$. }

In the following, we analyze the expected L2 loss of $\foneround$ returned by Algorithm \ref{algo:bs} in Theorem \ref{thm:f2.var}. 
% The steps on the vertex sides are the same as in \naive. The difference is on the data curator side, where in Lines xxxx the flipping probability $p$ is involved to correct for unbiased estimates. 




\begin{theorem}
\label{thm:f2.var}
{\color{black}
The L2 loss of $\foneround$ in Theorem \ref{thm:f2} is $O(\frac{ n_1  e^{\varepsilon}}{(1 - e^{\varepsilon})^4} )$. Here, $n_1$ represents the number of vertices in $U(G)$ (i.e., the opposite layer to \vq and \vx). }
\end{theorem}
\begin{proof}

Since $\foneround$ is unbiased, based on the bias-variance decomposition theorem \cite{bouckaert2008practical}, the L2 loss of $f_1$ equals its variance. 
{\color{black}Assume \vq and $w \in L(G)$. }
\begin{align*}
    & l_2( \foneround,  \pqx ) = \mathrm{Var}( \foneround(u,w))  \\
    % & = \hspace{-2mm} 
    % {\color{black}\sum_{v \in U(G)} \mathrm{Var}( \frac{(\mathcal{A}'[u,v] - p) (\mathcal{A}'[v,w] - p )}{(1-2p)^2} ) }\\
    & = {\color{black}\frac{1}{(1-2p)^4} \sum_{   v \in U(G)}  \mathrm{Var}( (\mathcal{A}'[u,v] - p) (\mathcal{A}'[v,w] - p ) ) }\\
\end{align*}
% Let $\eta =\mathcal{A}'[u,v] - p $ and $\theta = \mathcal{A}'[v,w] - p $. 
% We know $\mathbb{E}(\eta )  = 1-2p$ when $v \in N(u, G)$ and $\mathbb{E}(\eta )  = 0$ otherwise. 
% $\mathbb{E}(\theta )  = 1-2p$ when $v \in N(w, G)$ and $\mathbb{E}(\theta )  = 0$ otherwise. 
% This is because $\eta$ and $\theta$ are shifted Bernoulli variables with the same variances. 
% Also, we know $\mathrm{Var}(\eta ) = \mathrm{Var}(\theta) = p(1-p)$ because they are shifted Bernoulli variables with the same variances. 
{\color{black}Let \(\eta = \mathcal{A}'[u,v] - p\) and \(\theta = \mathcal{A}'[v,w] - p\). 
By construction, we know that \(\mathbb{E}(\eta) = 1 - 2p\) when \(v \in N(u, G)\) and \(\mathbb{E}(\eta) = 0\) otherwise. 
Similarly, \(\mathbb{E}(\theta) = 1 - 2p\) when \(v \in N(w, G)\) and \(\mathbb{E}(\theta) = 0\) otherwise. 
In addition, we have \(\mathrm{Var}(\eta) = \mathrm{Var}(\theta) = p(1 - p)\). 
This is because \(\eta\) and \(\theta\) are shifted Bernoulli variables with the same variance. }

\begin{align*}
& \mathrm{Var}( \foneround(u,w))  = \frac{1}{(1-2p)^4} \sum_{v\in U(G)}  \mathrm{Var}( \eta \theta ) \\
% &= \frac{1}{(1-2p)^4} \sum_{ v \in U(G)} (\mathrm{Var}(\eta)\mathrm{Var}(\theta) + \mathrm{Var}(\eta) \mathbb{E}(\theta)^2 + \mathrm{Var}(\theta) \mathbb{E}(\eta)^2   ) \\
% &= {\color{black} \frac{1}{(1-2p)^4} \sum_{  v \in U(G)}   p^2(1-p)^2 + p(1-p) ( \mathbb{E}(\theta)^2 +\mathbb{E}(\eta)^2 ) } \\ 
% &= {\color{red}\frac{1}{(1-2p)^4} \sum_{  v \in N(u) \cap N(w)} +  
%    \frac{1}{(1-2p)^4} \sum_{others}} \\ 
% & = {\color{red}\frac{1}{(1-2p)^4} \sum_{v \in N(u) \cap N(w) } p^2(1-p)^2 + 2 p(1-p) (1-2p)^2  + }\\
% &  {\color{red}\frac{1}{(1-2p)^4} \sum_{ otherwise} p^2(1-p)^2 +  p(1-p) (1-2p)^2 }\\
    % & = \frac{p^2(1-p)^2}{(1-2p)^4} |U(G)| + \frac{p(1-p)}{(1-2p)^2}(|U(G)|+\pqx) \\ 
&= {\color{black}\frac{p^2(1-p)^2}{(1-2p)^4} |U(G)|
    + \frac{p(1-p)}{(1-2p)^2}(deg(u, G) + deg(w, G)) } \\
&= {\color{black} O( \frac{p^2(1-p)^2}{(1-2p)^4}  |U(G)|  ) = \erroneround } 
\end{align*}
% \erroneround
The last step is due to $p = \frac{1}{1 + e^{\varepsilon}}$. 
\end{proof}


\begin{theorem}
{\color{black}
The \bs algorithm satisfies \epldp. 
}
\end{theorem}
\begin{proof}
% The theorem follows because the randomized responses provide \epldp, which is immune to post-processing. 
% The randomized responses provide \epldp. Since edge LDP is immune to post-processing, the theorem follows. 
{\color{black}
Since the randomized responses provide \epldp\cite{imola2021locally, imola2022differentially}, Lines 1-4 of the algorithm satisfy \epldp. 
Additionally, edge LDP is immune to post-processing, meaning that any analysis conducted on the noisy graph (Lines 5-8) preserves edge LDP. Thus, the theorem holds.
}
\end{proof}