 %%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\input{src/commands}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

% Authors
% Boyan, <huawei>, Ju, Nan, Yuyu

\begin{document}

\twocolumn[
% \icmltitle{Submission and Formatting Instructions for \\
%            International Conference on Machine Learning (ICML 2025)}
% \icmltitle{Alpha-SQL: Self-Supervised Text-to-SQL with Dynamic Path Exploration}

\icmltitle{Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search}
 % with LLM-as-Action-Model

 % Inference using Monte Carlo Tree Search
 
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Boyan Li}{1}
\icmlauthor{Jiayi Zhang}{1}
\icmlauthor{Ju Fan}{2}
\icmlauthor{Yanwei Xu}{3}
\icmlauthor{Chong Chen}{3}
\icmlauthor{Nan Tang}{1}
\icmlauthor{Yuyu Luo}{1}
\end{icmlauthorlist}

\icmlaffiliation{1}{The Hong Kong University of Science and Technology (Guangzhou)}
\icmlaffiliation{2}{Renmin University of China}
\icmlaffiliation{3}{Huawei Technologies Ltd}

\icmlcorrespondingauthor{Yuyu Luo}{yuyuluo@hkust-gz.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\nlsql, which enables natural language interaction with databases, serves as a pivotal method across diverse industries.
With new, more powerful large language models (LLMs) emerging every few months, fine-tuning has become incredibly costly, labor-intensive, and error-prone. As an alternative, \textit{zero-shot} Text-to-SQL, which leverages the growing knowledge and reasoning capabilities encoded in LLMs without task-specific fine-tuning, presents a promising and more challenging direction.
To address this challenge, we propose Alpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS) framework to iteratively infer SQL construction actions based on partial SQL query states. To enhance the framework’s reasoning capabilities, we introduce \textit{LLM-as-Action-Model} to dynamically generate SQL construction \textit{actions} during the MCTS process, steering the search toward more promising SQL queries. Moreover, Alpha-SQL employs a self-supervised reward function to evaluate the quality of candidate SQL queries, ensuring more accurate and efficient query generation.
%
%
%\nlsql, which enables natural language interaction with databases, serves as a pivotal method across diverse industries.
%With new, more powerful large language models (LLMs) emerging every few months, fine-tuning has become incredibly costly, labor-intensive, and error-prone.
%Instead, \textit{zero-shot} Text-to-SQL, which leverages the fast-growing knowledge and reasoning ability encoded in LLMs without task-specific fine-tuning, is promising to explore.
%In this paper, we formulate zero-shot \nlsql as a process of progressive SQL construction.
%, where queries are incrementally constructed step-by-step. 
%We model this process as a search problem over a tree-structured space, where nodes represent partial SQL query states and edges correspond to SQL construction actions.
%Based on this formulation, 
%We propose Alpha-SQL, a novel framework that employs \textit{LLM-as-Action-Model} to dynamically generate SQL construction \textit{actions} during the Monte Carlo Tree Search (MCTS) process. 
%By incorporating LLMs' step-by-step reasoning during query construction, Alpha-SQL ensures that each action is context-aware, guiding the search toward more promising SQL queries.
%The quality of candidate SQL queries is evaluated with a self-supervised reward function without the need for annotated data.
% 
Experimental results show that \sys achieves 69.7\% execution accuracy on the BIRD development set, using a 32B open-source LLM without fine-tuning. \sys outperforms the best previous zero-shot approach based on GPT-4o by 2.5\% on the BIRD development set.
% Experimental results show that \sys achieved 69.7\% execution accuracy on the BIRD development set, utilizing a 32B open-source LLM without fine-tuning.
% \sys outperforms current SOTA methods by a significant margin, surpassing the previous best zero-shot approach by 2.5\% on BIRD. 
%%%%%%%%FULL%%%%%%%%%%
% To address this challenge, we reformulate \nlsql task as a search problem over a tree-structured search space, where nodes represent partial SQL query states and edges correspond to SQL construction actions (\eg selecting a column).  A complete path from the root to a leaf node represents a reasoning trajectory, which corresponds to a candidate SQL query.

% To efficiently navigate this search space, We propose Alpha-SQL, a novel Text-to-SQL framework that leverages Monte Carlo Tree Search (MCTS). Alpha-SQL invokes LLMs at each step to perform reasoning and apply actions, progressively constructing candidate SQL queries along paths in the tree. By maintaining the reasoning process of LLMs within nodes, Alpha-SQL ensures that each decision is context-aware, enabling better exploration of the search space and reducing reliance on dense feedback signals. We evaluate the quality of candidate SQLs with a self-supervised self-consistency score and employ a hybrid voting mechanism for final output selection.

% Our method expands the range of candidate queries considered while incorporating the LLM’s reasoning process, leading to more accurate and semantically consistent SQL generation. 

% Empirical results demonstrate the effectiveness of \sys, achieving 95\% execution accuracy on the Spider dev set and 70\% on the BIRD dev set, utilizing a 32B open-source model without any fine-tuning.
% Notably, \sys outperforms current SOTA methods by a significant margin, surpassing the previous best zero-shot approach by 15\% on Spider and 10\% on BIRD. 
%%%%%%%%FULL%%%%%%%%%%
% 
% Text-to-SQL, the task of translating natural language queries into SQL, is a critical yet challenging problem.
% Recent advances fall into two categories: prompt-based approaches using closed-source large language models (LLMs) like Gemini 1.5 Pro, which raise concerns about privacy and computational costs; and fine-tuned open-source LLMs, which require extensive annotated datasets for training but often fail to deliver robust performance, especially on complex queries.
% % 
% To address these limitations, we investigate whether text-to-SQL performance can be enhanced using smaller open-source LLMs without fine-tuning. Inspired by how human experts iteratively construct and refine SQL queries, we propose Alpha-SQL, a framework that reformulates text-to-SQL as a search problem. Using Monte Carlo Tree Search, Alpha-SQL explores the SQL generation space by dynamically selecting and refining reasoning paths. Our framework generates multiple candidate SQL queries, evaluates their quality with a self-supervised self-consistency score, and employs a hybrid voting mechanism to select the best SQL query as the output.
% % 
% Our method achieves promising results, including 95\% execution accuracy on the Spider dev set and 70\% on the BIRD dev set, utilizing a 32B open-source model without fine-tuning. These findings suggest Alpha-SQL is a lightweight and cost-effective text-to-SQL solution.
\end{abstract}

% \begin{abstract}

% \bi 
% \item Text-to-SQL is an important but challenging task.
% \item Limitations of SOTA methods
% \bi 
% \item rely on large-scale data to fine-tuned specialized models (open-source LLMs)
% \item fixed pipeline or reason strategies to prompt LLMs, high cost.
% \ei
% \item can we boost Text-to-SQL with small size LLMs (open-source) without fine-tuning?
% \item Our key idea:
% \bi
% \item mimic human experts, write SQL query step-by-step with roll-back to fixed errors, i.e., generate multiple candidate SQL queries and then select the best one.
% \item To this end, we reformulate Text-to-SQL as a search problem: SQL clause and operators are the nodes.
% \item Our methods:
% \bi
% \item we propose Alpha-SQL, a Text-to-SQL method that efficiently explores the SQL generation space using Monte Carlo Tree Search
% \item explore this search space to select multiple paths (\ie SQL queries)
% \item automatically evaluates the quality of candidate SQl using a Self-consistency score.
% \item We propose a hybrid voting mechanism to select the best SQL query as the output.
% \ei
% \ei
% \item Extensive experiments show that Alpha-SQL achieves 95\% and 70\%
% on the Spider and BIRD dev sets, with 32B model
% \ei 
% \end{abstract}

\input{src/introduction}
\input{src/related}
\input{src/preliminary}
\input{src/method}
\input{src/experiments}
\input{src/conclusion}

% Acknowledgements should only appear in the accepted version.
% \input{src/acknowledgements}

% \input{src/statements}

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
% \clearpage
% \newpage
\section*{Impact Statement} 
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.
\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{src/appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}

