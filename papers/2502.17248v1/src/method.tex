%!TEX root = ../main.tex
\section{The Design Details of Alpha-SQL}
\label{sec:method}

% We first introduce LLM-as-Action-Model, where an LLM generates action outputs based on the query context, then define the SQL construction action space (Section~\ref{sub:llmaction}). Finally, we explain how MCTS integrates LLM-as-Action-Model to explore the SQL generation space (Section~\ref{sub:mcts}).




% \begin{figure*}
%     \centering    \includegraphics[width=0.8\textwidth]{AlphaSQL/figures/alpha-sql.pdf}
%     \vspace{-2em}
%     \caption{An Overview of Alpha-SQL. \boyan{todo:revise}}
% \end{figure*}

% Data preprocessing, no step x
% Step 3. SQL Selection

% \boyan{
% Redraw new figures here.
% }


% \textbf{Formulation.}
% Candidate SQL generation can be formalized as a multi-step reasoning process decomposed into simpler subtasks. We employ Monte Carlo Tree Search (MCTS) to enable the LLM to autonomously explore and generate reasoning trajectories. 
% Formally, given an initial input $x$ and language model $M$, MCTS iteratively constructs a search tree $\mathcal{T}$ by invoking $M$. The root node $(s_0)$ represents the initial input $x$, edges represent actions $a$, and each child node $s_i$ is the output state after $M$ executes action $a$. A complete path from root to terminal node $(s_t)$ forms a reasoning trajectory $\tau = s_0 \oplus \cdots \oplus s_t$, where $\oplus$ denotes concatenation.
% From the search tree $\mathcal{T}$, we obtain a set of reasoning trajectories $T = \{\tau_1, \tau_2, \dots, \tau_n\}$, which yields the candidate SQL set. 

\subsection{LLM-as-Action-Model}
\label{sub:llmaction}


% \paragraph{LLM-as-Action-Model.}

% 在推理过程的第i步（s_i），MCTS将从上述动作空间中选出一个动作a_i，基于之前的推理轨迹$s_0 \oplus \cdots \oplus s_i$, Prompting LLM执行a_i得到下一步状态s_(i+1)。值得注意的是，有些动作是存在前后顺序的，比如SQL Revision需要在SQL Generation之后，Table~\ref{tab:action-space}展示了基于前一步动作的下一步合法动作。此外，为了避免同一条推理轨迹中出现无限制的循环，我们限定同一个动作在一条推理轨迹中只能出现一次。

A key challenge in zero-shot text-to-SQL is the difficulty of transferring general knowledge from pre-trained language models to the specific task of SQL generation.

To enhance the reasoning capabilities of our framework, we propose the \textit{\textbf{LLM-as-Action-Model}}, which leverages LLMs to generate reasoning actions (\ie CoT Thoughts) dynamically based on the current context of the problem. As shown in Figure~\ref{fig:overview},
LLM-as-Action-Model empowers the LLMs to generate appropriate action outputs based on the question, database schema, and current partial SQL query state (including previous actions), enabling the model to build a valid SQL query in a step-by-step manner. 

Formally, at step $i$ of the reasoning process ($v_i$), MCTS selects an SQL construction action $a_i$ from the action space, which is defined later. Based on the reasoning trajectory $v_0 \oplus \cdots \oplus v_i$, the LLM is prompted to execute $a_i$ and generate the next state $v_{i+1}$:

\begin{equation*}
    \vspace*{-1em}\small
    v_{i+1} = LLM(q, \mathcal{D}, Actions(v_0,\cdots, v_i), Prompt(a_i)),
\end{equation*}

where $Actions(\cdot)$ refers to all previous reasoning steps in the trajectory, and $Prompt(\cdot)$ represents the prompt instruction for a specific action.

\textbf{SQL Construction Reasoning Action Space.}
The action space defines the set of potential reasoning steps an LLM can take to decompose and solve Text-to-SQL problems. It is crucial for the LLM-as-Action Model to guide the progressive construction of SQL queries by specifying possible actions at each stage.
% The SQL construction reasoning action space defines the set of potential reasoning actions that the LLM can take at each step, allowing it to decompose the original Text-to-SQL problem into multiple subtasks. This action space plays a crucial role in the LLM-as-Action-Model, as it defines the potential next steps the model can follow to progressively construct the SQL query. Each action in this space corresponds to a specific reasoning step, guiding the model through the query construction process.
% 推理动作空间决定了如何将原问题拆解问多个子任务，同时也是蒙特卡洛搜索过程的关键。(直接影响潜在推理轨迹空间的大小)
% The reasoning action space determines how to decompose the original problem into multiple subtasks while being crucial to the MCTS process.
% 先前的工作~\cite{CAHSE, chesssql, XiYan}使用有限的推理和固定的推理轨迹，有限的推理空间限制了模型的探索能力和在NL2SQL任务上的潜力。
% 
% While previous works~\cite{CHASE, chesssql, XiYan} employed limited reasoning actions and fixed reasoning pipelines, we recognize that this approach restricts the model's exploration capability in the Text-to-SQL task. 
% % 类比人的思考过程：一些人可能会直接思考问题答案，一些人则可能会先澄清问题意图，分解成多个子任务来解决，并根据当前的中间思考状态，动态选择下一步推理动作。
% Inspired by human thinking processes, some may directly contemplate the answer, while others might first clarify the question's intent, decompose it into subtasks, and dynamically select the next reasoning action based on their current intermediate state.
% 因此为了最大化模型的推理探索能力，我们在先前工作的动作空间基础上（如Schema Selection），还引入了问题改写（~\cite{RAP}）等新的推理动作，定义了更丰富的推理动作空间。我们还通过Section 5.6（~\ref{subsec:exp-action-space}）的消融实验，验证了动作空间的有效性。
% Therefore, to maximize the model's reasoning exploration capability, we introduce new reasoning actions such as question rewriting~\cite{rStar} in addition to the action space from previous works, defining a more comprehensive reasoning action space. 
% Ablation experiments in Section~\ref{subsec:exp-action-space} validate the effectiveness of our defined action space. 
% 
Previous works~\cite{CHASE, chesssql, XiYan} used limited actions and fixed pipelines, which restricted the model's ability to explore the full space of potential solutions. 

Inspired by human thinking, where some might jump straight to the answer while others first clarify the question and break it down into subtasks, we introduce new reasoning actions, such as question rewriting~\cite{rStar}, alongside existing ones. 
In total, our action space defines seven distinct reasoning actions.
The specific prompts for each action are illustrated in Appendix~\ref{sub:action-prompts}.

\uline{$A_1$: Question Rephrasing.} 
% 在Text-to-SQL真实应用场景下，不同的用户群里往往提出的问题风格不同，并带有不同程度歧义性～\cite{survey}，澄清用户问题的意图，对生成准确的SQL查询非常重要。
% In real-world Text-to-SQL applications, different user groups often pose questions with varying styles and degrees of ambiguity~\cite{nl2sql-survey}, making the clarification of user intent crucial for generating accurate SQL queries.
Text-to-SQL systems need to handle diverse question styles and ambiguities from different user groups~\cite{nl2sql-survey}. 
% 尽管先前的NL-Rewriter~\cite{nl-rewriter}提出了基于过往经验对问题进行重写，但是由于其需要训练数据进行经验积累，所以在无领域数据场景（Zero-shot场景）下应用受限。
% Although previous work NL-Rewriter~\cite{nl-rewriter} proposed question rewriting based on historical experience, its application is limited in zero-shot scenarios due to its dependency on training data for experience accumulation.
While NL-Rewriter~\cite{nl-rewriter} addresses this through experience-based question rewriting, it struggles in zero-shot scenarios where training data is unavailable.
% Follow rStar~\cite{rStar}，我们通过few-shot prompting的方式，将问题重写为（conditions list，问题）的更清晰的形式，Prompt如Figure~\ref{???}所示。
% Following rStar~\cite{rStar}, we employ few-shot prompting to reformulate questions into a clearer format of (conditions list, question).
Building on rStar~\cite{rStar}, we use few-shot prompting to decompose questions into a structured (conditions list, question) format.

\uline{$A_2$: Schema Selection.}
% 数据库中往往包含大量表格和复杂的关系连接（如外键），然而一条SQL查询只会用到很少的表和列，大量与当前用户问题无关的Schema增大了模型生成准确SQL查询的挑战～\cite{survey, supersql}。
% Databases typically contain numerous tables and complex relationships (\eg foreign keys); however, a single SQL query only utilizes a small subset of tables and columns. The abundance of schema elements irrelevant to the current user question increases the challenge for models to generate accurate SQL queries~\cite{nl2sql-survey, supersql}.
Databases often contain complex schemas, but individual SQL queries typically use only a small subset of available elements. This mismatch creates challenges for accurate SQL generation~\cite{nl2sql-survey}.
% While databases often contain many tables with complex relationships, each SQL query typically uses only a small subset of the available schema elements. This schema complexity poses a significant challenge for accurate query generation~\cite{nl2sql-survey}. 
% 先前工作~\cite{rslsql, CHASE, CHESS}都将Schema Selection作为一个SQL生成过程中的关键一环。我们follow～\cite{CHESS}，给定用户问题和数据库完整的Schema，使用CoT Prompting方式，让LLM输出与当前问题相关的schema子集，并作为本动作后续推理的schema context（替换原始完整的schema context）。具体的Prompt如Figure~\ref{???}所示。
% Previous works~\cite{rslsql, CHASE, chesssql} have identified schema selection as a crucial subtask in the SQL generation process. Following~\cite{chesssql}, we employ Chain-of-Thought (CoT) prompting to enable the LLM to output a subset of schema relevant to the current user question. The selected subset then serves as the schema context for subsequent reasoning.
Prior work has established schema selection as a critical component of SQL generation~\cite{rslsql, CHASE, chesssql}. Following~\cite{chesssql}, we use Chain-of-Thought (CoT) prompting to identify the relevant schema subset for each user question, which then guides subsequent query generation.

\uline{$A_3$: Column Value Identification.}
% Text-to-SQL真实场景下的SQL查询经常存在过滤条件，如“Bob在足球比赛中最好的排名是多少？”，SQL查询中需要对人物姓名“Bob”和比赛类型为“足球”进行过滤（e.g. “WHERE name = 'Bob' and match_type = 'football'”）。
% In real-world Text-to-SQL scenarios, SQL queries frequently incorporate filtering conditions. For instance, in the question ``What is Bob's best ranking in football matches?", the SQL query must filter based on the person's name ``Bob" and the match type ``football" (\eg \texttt{WHERE name = `Bob' AND match\_type = `football'}).
Text-to-SQL systems need to accurately identify filtering conditions in user questions. For example, ``What is Bob's best ranking in football matches?" requires filtering on both name (``Bob") and match type (``football") (\eg \texttt{WHERE name = `Bob' AND match\_type = `football'}).
% 先前的工作如CHESS~\cite{CHESS}在BIRD开发集上，约有20%的错误是因为选错过滤条件的列或者列值。
% Previous work such as CHESS-SQL~\cite{chesssql} demonstrated that approximately 20\% of errors on the BIRD development set occurred due to incorrect selection of filtering columns or column values.
CHESS-SQL~\cite{chesssql} found that 20\% of errors in the BIRD development set stem from incorrect filtering columns or value selection. 
% 为了缓解这一问题，我们设计了列值识别动作，旨在在模型生成最终SQL之前，增加对列值相关的思考。具体的Prompt如Figure~\ref{???}所示。
% To address this challenge, we designed a column value identification action aimed at enhancing the model's consideration of column values prior to generating the final SQL query.
To address this, we introduce a column value identification action that evaluates potential filtering values before SQL generation.

\uline{$A_4$: Column Function Identification.}
% Text-to-SQL真实场景下往往存在较为复杂的计算查询，需要使用数据库的聚合函数或标量函数。
% In some cases, complex queries require the use of database aggregate functions (\eg \texttt{COUNT}) or scalar functions (\eg \texttt{STRFTIME}).
Complex SQL queries often require aggregate functions (\eg \texttt{COUNT}) and scalar functions (\eg \texttt{STRFTIME}).
% 例如用户查询问题为“有多少人在2024年出生？”，SQL查询中可能会涉及日期计算（如“STRFTIME('%Y', people.date_of_birth) = '2024'”）以及聚合查询（如“COUNT（people.id）”）。
% For instance, when a user queries ``How many people were born in 2024?", the SQL query may involve date calculations (\eg \texttt{STRFTIME(`\%Y', people.date\_of\_birth) = `2024'}) and aggregate operations (\eg \texttt{COUNT(people.id)}).
For example, the question ``How many people were born in 2024?" necessitates both date manipulation (\texttt{STRFTIME(`\%Y', people.date\_of\_birth) = `2024'}) and aggregation (\texttt{COUNT(people.id)}). 
% 先前的工作如CHASE-SQL在BIRD开发集上约有19%的错误归咎于错误的函数使用。
% Previous work such as CHASE-SQL~\cite{CHASE} attributed approximately 19\% of errors on the BIRD development set to incorrect function usage.
Analysis by CHASE-SQL~\cite{CHASE} revealed that function-related errors account for 19\% of mistakes in the BIRD development set. 
% 为了促进模型在生成SQL时使用正确的函数计算，我们设计了列函数识别动作，增加模型在推理过程中对列函数使用的思考，具体的Prompt如Figure~\ref{???}所示。
% Thus, to promote the model's accurate function utilization in SQL generation, we designed a column function identification action during the inference process.
To improve function handling ability, we introduce a column function identification action during inference.

\uline{$A_5$: SQL Generation.}
% SQL生成是Text-to-SQL任务中必要且关键的一步，直接影响到系统整体的任务性能。CHASE-SQL~\cite{CHASE}使用了一种Divide-and-Conquer CoT策略，将原始复杂问题分解为多个更小的子问题，单独解决子问题后，通过组合子问题答案得到最终的预测SQL，这种Prompt策略在处理涉及嵌套查询等复杂场景下优势更加突出。
% SQL generation is a critical step in Text-to-SQL tasks, directly impacting the overall performance. CHASE-SQL~\cite{CHASE} implemented a Divide-and-Conquer Chain-of-Thought strategy that decomposes complex original problems into smaller subproblems. After solving these subproblems independently, it combines their solutions to obtain the final predicted SQL. This prompting strategy demonstrates particular advantages when handling complex scenarios involving nested queries.
SQL generation is the core component of Text-to-SQL systems. CHASE-SQL~\cite{CHASE} introduced a Divide-and-Conquer CoT strategy that breaks down complex queries into multiple subtasks, solves them independently, and combines solutions. This method particularly excels at handling nested queries. 
% 我们Follow了该策略，并加入了当前步骤之前的历史推理动作及其输出作为历史思考信息，具体的Prompt如Figure~\ref{???}所示。
% We adopted this strategy as the SQL generation approach for \sys.
We incorporate this strategy into our reasoning action space.

\uline{$A_6$: SQL Revision.}
% 在一些复杂SQL生成场景下，LLMs有可能生成语法错误的SQL查询~\cite{MAC-SQL, CHASE-SQL}。
% In complex SQL generation scenarios, LLMs may generate syntactically incorrect SQL queries~\cite{macsql, CHASE}.
LLMs can generate syntactically invalid SQL queries in complex scenarios~\cite{macsql, CHASE}. 
% 为了解决这个问题，类比人在修正SQL的过程会尝试执行SQL并根据执行结果反馈进行修正，我们输入LLM用户问题，数据库schema，错误的SQL，以及SQL在数据库的执行结果，让LLM根据反馈进行修正，并输出修正后的SQL。
% To address this issue, we are inspired by human SQL correction processes where one executes the SQL and makes corrections based on execution feedback. We input the user question, database schema, incorrect SQL, and corresponding database execution results to the LLM, enabling it to make corrections based on execution feedback and output the revised SQL.
Drawing inspiration from human debugging practices, we implement an execution-guided correction mechanism. Our approach provides the LLM with the user question, schema, incorrect SQL, and execution results to guide query revision. 
% 为了提高鲁棒性，我们使用多轮修正策略，直到SQL查询无执行错误，或者到达最大尝试次数$N_rev$，具体的Prompt如Figure~\ref{???}所示。
% To enhance robustness, we implement a multi-round correction strategy that continues until either the SQL query executes without errors or reaches the maximum attempt limit $N_{revision}$.
The system performs multiple correction rounds until either obtaining a valid SQL query or reaching a maximum attempt limit $N_{revision}$.

% TODO: 加一句话提Revision的作用？

\uline{$A_7$: Termination.}
% 当推理过程得到最终预测SQL时，将会执行Termination动作，用来标识一条推理轨迹的结束。我们规定终止动作出现在SQL Generation或SQL Revision之后。
The termination action is invoked when the reasoning process yields the final predicted SQL, signifying the conclusion of a reasoning trajectory. We specify that the termination action must occur following either SQL Generation or SQL Revision actions.

% order of actions, table
\begin{table}[t!]
    \centering
    \vspace{-1em}
    \caption{Action Space with Ordering.}
    \label{tab:action-space}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{c|c}
        \hline
        Previous Action & Valid Next Actions \\ \hline
        $-$ & $A_1, A_2, A_3, A_4, A_5$ \\ \hline
        $A_1$: Question Rephrasing & $A_2, A_3, A_4, A_5$ \\ \hline
        $A_2$: Schema Selection & $A_3, A_4, A_5$\\ \hline
        $A_3$: Column Value Identification & $A_2, A_4, A_5$\\ \hline
        $A_4$: Column Function Identification & $A_2, A_3, A_5$ \\ \hline
        $A_5$: SQL Generation & $A_6, A_7$ \\ \hline
        $A_6$: SQL Revision & $A_7$ \\ \hline
        $A_7$: Termination & $-$ \\ \hline
        \end{tabular}
    }
\vspace{-1.5em}
\end{table}

% \yuyu{It is calculated that there are over 3000 reasoning paths in the search space. However, we only performed 24 MCTS searches. Therefore, we believe that MCTS can efficiently explore significantly larger search spaces.}

\textbf{Action Ordering and Constraints.}
Each reasoning trajectory follows a structured order, ensuring logical coherence. For example, some actions, like SQL Revision, must occur only after SQL Generation.
Table~\ref{tab:action-space} defines the valid transitions between actions. To avoid infinite loops, we restrict each action to appear only once within a given process.

% Notably, certain actions follow a specific sequential order—for instance, SQL Revision must occur after SQL Generation. Table~\ref{tab:action-space} illustrates the valid subsequent actions based on the previous action. Furthermore, to prevent infinite loops within a single reasoning trajectory, we constrain each action to appear only once within a specific reasoning process.

\subsection{MCTS for Candidate SQL Generation}
\label{sub:mcts}



\textbf{Candidate SQL Generation with MCTS Rollout.}
% 我们通过执行多次MCTS Rollouts来探索高价值的Candidate SQLs。具体而言，每一次rollout包含四个阶段：Selection，Expnasion，Simulation以及Backpropagation。
We generate Candidate SQL through multiple MCTS rollouts. Specifically, each rollout includes four distinct phases: \textit{Selection}, \textit{Expansion}, \textit{Simulation}, and \textit{Backpropagation}.

\textit{(1) Selection.}
% 该阶段是为了选择最适合拓展的子节点。从根节点开始，递归地向下选择子节点，直到到达未扩展过的叶子结点或者Termination节点。为了平衡探索与利用，我们使用应用于树（UCT）的置信上限进行节点选择：
% This phase aims to select the most promising node for future expansion. Starting from the root node ($v_0$), the selection process recursively traverses downward, selecting child nodes until reaching either an unexpanded leaf node or a termination node. To maintain an optimal balance between exploration and exploitation, we employ the Upper Confidence Bound applied to Trees (UCT)~\cite{uct} for node selection:
This phase identifies promising nodes for expansion by traversing from the root node ($v_0$) to either an unexpanded leaf or termination node. We use Upper Confidence Bound applied to Trees (UCT)~\cite{uct} to balance exploration and exploitation during node selection:
$ UCT(v, a) = \frac{Q(v,a)}{N(v,a)} + c \sqrt{\frac{lnN(v)}{N(v,a)}}$, 
% N(v,a)是在之前的迭代中节点v选择动作a的次数，N(v)表示节点v的总访问次数，Q(v,a)是v选择动作a的估计奖励值，会通过反向传播进行更新。具有最高UCT分数的动作将会被选择，同时转移到v执行该动作之后的子节点。
% where $N(v,a)$ represents the number of times action $a$ has been visited from node $v$ in previous iterations, while $N(v)$ denotes the total number of visits to node $v$. $Q(v,a)$ represents the estimated reward value for selecting action $a$ from node $v$, which is updated through backpropagation. The action with the highest UCT score is selected, and the traversal proceeds to the corresponding child node by executing this action at node $v$.
where $N(v,a)$ counts visits to action $a$ from node $v$, and $N(v)$ tracks total visits to node $v$. $Q(v,a)$ is the estimated reward for action $a$ from node $v$, updated via backpropagation. We select the action with the maximum UCT score and move to the resulting child node.
% 值得注意的是，如果存在未访问过的子节点（即N(v,a)=0）,我们会优先选择这类子节点。
Notably, if there exist any unvisited child nodes (\ie $N(v, a) = 0$), we prioritize the selection of such nodes over UCT selection.

\textit{(2) Expansion.}
% 在这个阶段，被选择的节点将扩展其子节点。如Table 1所示，首先根据当前节点的类型确定合法的下一步动作集合，然后我们通过Prompting LLM的方式执行这些动作，生成动作对应的输出结果。值得注意的是，为了进一步增加推理的多样性，我们设定每一个动作都会采样N_sample次，从而获得共$N_{sample} * |\bbm{A}_valid|$个子节点。
% During this phase, the selected node expands its child nodes. As shown in Table 1, we first determine the set of valid next actions based on the current node type. These actions are then executed by prompting the LLM to generate corresponding output results. Notably, to enhance reasoning diversity, we sample each action $N_{expansion}$ times with sampling temperature $T_{expansion}$, resulting in a total of $N_{expansion} \times |{E}_{valid}|$ child nodes, where $|E_{valid}|$ is the size of valid actions.
The expansion phase generates child nodes from the selected node. Valid actions are determined by the node type (Table~\ref{tab:action-space}) and executed via LLM prompting. Each action is sampled $N_{expansion}$ times with temperature $T_{expansion}$, creating $N_{expansion} \times |{E}_{valid}|$ child nodes, where $|E_{valid}|$ represents the number of valid actions. This sampling approach enhances reasoning diversity.

\textit{(3) Simulation.}
% 一个完整的模拟过程由迭代的进行选择和扩展节点组成，直到到达Termination节点。其中，模拟过程中所有新扩展的子节点都将持久化保存在树中。
A complete simulation process consists of iterative node selection and expansion until reaching a termination node. Throughout the simulation process, all newly expanded child nodes are persistently maintained within the tree structure.

\textit{(4) Backpropagation.}
% 当simulation到达终止节点(v_t)后，会执行反向传播。首先，我们使用Section 3.2 中的自监督奖励函数对当前推理轨迹的预测SQL进行评分。
% Upon reaching a termination node ($v_t$) in the simulation, the backpropagation process begins. Initially, we evaluate the predicted SQL of the current reasoning trajectory using the self-supervised reward function described in Section~\ref{sub:overview}.
At a termination node ($v_t$), backpropagation begins by evaluating the predicted SQL using the self-supervised reward function from Section~\ref{sub:overview}. 
% 具体而言，我们找到当前轨迹中生成最终预测SQL的动作（SQL Generation / SQL Revision），然后重复多次Prompting LLM来采样多条SQL，并设定较高的温度采样参数以增加采样的多样性。我们计算采样SQL中与预测SQL的执行结果自洽分数，作为对预测SQL的奖励值r。
% Specifically, we identify the action (SQL Generation / SQL Revision) that produces the final predicted SQL in the current trajectory. Then, we repeatedly prompt the LLM to sample $N_{reward}$ SQL queries, employing a high temperature $T_{reward}$ to enhance sampling diversity. The reward value $r$ for the predicted SQL is computed based on the self-consistency score between the execution results of the sampled SQLs and the predicted SQL.
We identify the action ($A_5$ or $A_6$) that produced the final SQL, then sample $N_{reward}$ SQL queries with temperature $T_{reward}$ to ensure diversity. The reward value $r$ is calculated from the self-consistency score between the execution results of the sampled and predicted SQLs.
% 最后，我们从终止节点向父节点回溯，直到根节点。对于路径上的所有节点(v_0, ..., v_t)，我们按照以下公式更新Q和N值：
% Finally, we backtrack from the termination node to the parent nodes until reaching the root node. For all nodes along the path $v_0 \oplus \cdots \oplus v_t$, we update the $Q(v,a)$ and $N(v)$ values according to the following formula:
The process then backtracks from $v_t$ to the root node, updating $Q(v,a)$ and $N(v)$ values for all nodes along the path $v_0 \oplus \cdots \oplus v_t$ using: $
Q(v, a) = Q(v,a) + r,~N(v) = N(v) + 1$.
% 
% 值得注意的是，这些值将在下一次rollout过程中被UCT公式使用，以引导未来的搜索方向。
% Notably, these values are subsequently utilized by the UCT formula (Eq.~\ref{eq:uct}) during the next rollout process, thereby guiding the direction of future searching.
These updated values guide future search directions through the UCT formula in subsequent rollouts.

% 我们通过进行多次Rollouts，直到到达最大Rollout次数$N_rollout$。我们从树中收集所有推理轨迹，并过滤掉没有到达Termination节点的非法轨迹，便得到了待选SQL推理轨迹集合：$T = \{\tau_1, \tau_2, \dots, \tau_n\}$. 接下来，我们的目标是从$T$中选择一条作为最终的答案。
% We perform multiple rollouts until reaching the maximum rollout count $N_{rollout}$. By collecting all reasoning trajectories from the tree and filtering out invalid trajectories that have not reached termination nodes, we obtain a set of candidate SQL reasoning trajectories: $T = \{\tau_1, \tau_2, \dots, \tau_n\}$. 
After $N_{rollout}$ rollouts, we collect all complete trajectories that reach termination nodes, forming a set of candidate SQL reasoning trajectories $T = \{\tau_1, \tau_2, \dots, \tau_n\}$.
% Our subsequent objective is to select one trajectory from $T$ as the final answer.

% \textbf{Self-supervised Reward Function.}
% % 奖励函数是MCTS过程另一个关键组成，在搜索过程中，MCTS通过评估每一个action的奖励值，来选择更加promising的action，从而引导搜索朝向高价值的搜索空间部分。一些工作~\cite{?}通过训练Outcome Reward Model (ORM) 或者Progress Reward Model (PRM) 来作为奖励函数。然而，获取特定领域的奖励模型训练数据集以及训练一个有效的RM是具有挑战性的~\cite{}，使得这种方式难以快速应用到特定domain。
% The reward function constitutes another critical component of the MCTS process. During the search, MCTS evaluates the reward value of each action to select more promising options, thereby guiding the search toward high-value regions of the search space. Several studies~\cite{?} have employed the Outcome Reward Model (ORM) or Progress Reward Model (PRM) as reward functions. However, acquiring domain-specific Reward Model (RM) training datasets and training an effective RM presents significant challenges~\cite{?}, making this approach difficult to rapidly adapt to specific domains.
% % 我们回顾人类在完成任务时的行为：如果人类对当前任务的答案很自信，那么即使尝试多次仍会保持相同的答案，答案具有较高的置信度；反之，答案的置信度则比较低。答案的置信度一定程度上反映了质量的好坏~\cite{rStar}。因此，我们设计了一种基于Self-consistency的置信度的奖励函数，使用一种自监督的方式缓解了RM数据的获取和训练挑战（见Sec 3.2）。
% When examining human behavior in task completion, we observe that when individuals are confident in their answer to a given task, they tend to maintain the same response even after multiple attempts, indicating \textbf{high confidence} in their answer. Conversely, \textbf{low confidence} is exhibited when responses vary. The confidence level of an answer reflects its quality~\cite{rStar}. Based on this observation, we propose a self-consistency-based confidence reward function that leverages a self-supervised approach to address the challenges of RM data acquisition and training (see details in Section~\ref{sub:overview}).

\textbf{Final SQL Selection.} 
% In this phase, our goal is to select the optimal reasoning trajectory along with its final predicted SQL query from $T$. Previous approaches, such as CHASE-SQL~\cite{CHASE}, utilize the proprietary model Gemini-1.5-Flash with fine-tuning for SQL selection. However, the acquisition of extensive domain-specific data presents a significant challenge in real-world applications. In the Text-to-SQL task, although multiple reasoning trajectories may exist for a given question, they will converge to equivalent answers. Leveraging this property, we first execute all predicted SQL queries from different trajectories, then select the SQL query that demonstrates the highest consistency in execution results as the final prediction.
To select the optimal trajectory and SQL query from $T$, we leverage the convergent nature of Text-to-SQL: different reasoning paths yield equivalent SQL queries for a given question. We execute all predicted SQL queries and select the one with the highest execution result consistency as the final prediction. This approach differs from previous methods like CHASE-SQL~\cite{CHASE}, which rely on fine-tuned proprietary models such as Gemini-1.5-Flash, requiring extensive domain-specific data.

We show the pseudo-code of \sys in Appendix~\ref{app:code}.

\textbf{Pruning Strategies.}  
Alpha-SQL incorporates schema constraints and semantic rules into the search process to prune invalid paths early. A key aspect of our pruning strategy is the elimination of redundant nodes. For instance, when performing a Schema Selection action, we may sample the LLM $M$ multiple times (\eg 3 times). Although the Chain-of-Thought content generated by $M$ may differ in each sample, if the final selected schema subset is identical, we create only one child node instead of three duplicate nodes. This de-duplication significantly reduces the branching factor of the search tree without loss of information.



\subsection{Offline: Database Value Retrieval}
\label{sub:offline-preprocess}
% Database systems typically store large amounts of column values, but only a few values are used in one SQL query. These values predominantly appear in query filtering conditions (\eg \texttt{WHERE}) and directly determine the correctness of the SQL query. Moreover, there is often a semantic gap between the values described in user questions and their corresponding database representations; for instance, a user might reference ``America", while the database stores ``United States". Consequently, database value retrieval becomes a critical preliminary step, ensuring that relevant column values are available to support accurate SQL query generation. Following~\citet{chesssql}, we employ a similar strategy for database value retrieval. 
SQL queries typically reference a few column values from large databases, primarily in filtering conditions (\eg \texttt{WHERE}). These values are critical for query correctness but often face semantic gaps between user expressions and database values (e.g., ``America" vs. ``United States"). This makes accurate database value retrieval essential for SQL generation. Following~\citet{chesssql}, we implement value retrieval in two steps:
% 
% We extract entity keywords from user questions by prompting LLM with few-shot examples (\rev{Appendix}~\ref{sub:keyword-extraction-prompts}). Then, we use LSH~\cite{LSH} to retrieve the relevant set of values. Furthermore, given editing distance and semantic similarity thresholds $\epsilon_\text{edit}$ and $\epsilon_\text{semantic}$, we filter the most relevant values. For semantic similarity computation, we employ OpenAI's \textit{text-embedding-3-large} model to ensure accurate matching between question entities and database values. The relevant column values will be included as part of the database schema prompt when calling LLMs.
First, we extract keywords from questions using few-shot LLM prompts ({Appendix}~\ref{sub:keyword-extraction-prompts}). We then use LSH~\cite{LSH} to retrieve relevant values, filtering them based on editing similarity and semantic similarity thresholds ($\epsilon_\text{edit}$, $\epsilon_\text{semantic}$). The semantic matching employs OpenAI's \textit{text-embedding-3-large} model. 
The retrieved values will be used as part of the database schema prompt for our LLM-as-Action-Model module.

% algorithm (refer to RAP-MCTS)

% The MCTS exploration is guided by:
% \begin{equation}
%    a^* = argmax_a \{\frac{Q(s,a)}{N(s,a)} + c\sqrt{\frac{\ln N(s)}{N(s,a)}}\}
% \end{equation}

% where $Q(s,a)$ is the action value, $N(s)$ and $N(s,a)$ are visit counts, and $c$ is the exploration constant.



% \subsection{SQL Selection}



%%%%%%%%%% Draft %%%%%%%%%%
% RAG for Databases Values
% MCTS-based SQL Reasoning
% Reward-based SQL Selection
%%%%%%%%%%%%%%%%%%%%%%%%%%%