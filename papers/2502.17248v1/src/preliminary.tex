%!TEX root = ../main.tex
\section{Our \sys Approach}
\label{sec:preliminary}

% -------------
% \paragraph{Text-to-SQL Task.}
% Given a natural language question $Q$ and a database $\mathcal{D}$ consisting of tables $\mathcal{T} = \{t_1, t_2, ..., t_n\}$ with their corresponding columns $\mathcal{C} = \{c_1, c_2, ..., c_m\}$, the Text-to-SQL task aims to generate a syntactically correct and semantically accurate SQL query $Y$ that retrieves the desired information from the database.

% Formally, let $\mathcal{Q}$ denote the space of all possible natural language questions and $\mathcal{Y}$ denote the space of all valid SQL queries. The Text-to-SQL task can be formulated as finding a mapping $f: \mathcal{Q} \times \mathcal{D} \rightarrow \mathcal{Y}$. Here, $q \in \mathcal{Q}$ represents the input natural language question, $\mathcal{D} = (\mathcal{T}, \mathcal{C}, \mathcal{R})$ denotes the database where $\mathcal{R}$ represents the relationships between tables, and $y \in \mathcal{Y}$ is the target SQL query.

% This mapping $f$ can be realized through various approaches, including models with training, prompt-based methods, or hybrid approaches. The performance of a Text-to-SQL system is typically evaluated using the EX metric:

% \begin{equation}
%     \label{eq:ex}
%     \text{EX}(\hat{y}, y) = \mathbbm{1}[\text{Execute}(\hat{y}) = \text{Execute}(y)]
% \end{equation}

% where $\text{Execute}(\cdot)$ denotes the execution results of a SQL query in the database.


% NL2SQL Task
% 1. Introduction
% 2. Symbol Formulation
% 3. Optimization Goal

% We begin by introducing the Text-to-SQL task and reformulating it as a search problem (Section~\ref{sub:problem}). Next, we provide an overview of our solution, \sys (Section~\ref{sub:overview}).

\subsection{Zero-shot Text-to-SQL}
\label{sub:problem}

% \paragraph{Text-to-SQL Task.}

% Let $\mathcal{D} = (\mathcal{T}, \mathcal{C}, \mathcal{R})$ be a relational database, where:
% $\mathcal{T} = \{t_1, t_2, \dots, t_n\}$ is the set of tables, 
% $\mathcal{C} = \bigcup_{t \in \mathcal{T}} \mathcal{C}_t$ is the set of all columns, where $\mathcal{C}_t$ denotes the set of columns associated with table $t \in \mathcal{T}$, and 
% $\mathcal{R}$ represents the relationships between tables (\eg primary-foreign key constraints).
% Let $\mathcal{Q}$ denote the space of all possible natural language questions and $\mathcal{Y}$ denote the space of all valid SQL queries over $\mathcal{D}$. 

% The goal of the Text-to-SQL task is to find a mapping function $f: \mathcal{Q} \times \mathcal{D} \rightarrow \mathcal{Y}$ such that, for any given question $q \in \mathcal{Q}$, $f(q, \mathcal{D})$ produces a syntactically correct and semantically equivalent SQL $y \in \mathcal{Y}$ that retrieves the desired data from $\mathcal{D}$.
% Formally, the optimal mapping $f^*$ is defined as:

% \begin{equation}
% 	f^* = \arg\max_{f \in \mathcal{F}} \mathbb{E}_{q \sim P(q|\mathcal{D})}[\text{EX}(f(q, \mathcal{D}), y^*)]
% \end{equation}

% where $\mathcal{F}$ is the space of all possible mapping functions, 
% $P(q|\mathcal{D})$ is the distribution of questions conditioned on the database $\mathcal{D}$, 
% $y^*$ is the ground truth SQL query corresponding to $q$, and $\text{EX}(\cdot, \cdot)$ denotes the execution accuracy metric:

% \begin{equation}
% 	\label{eq:ex}
% 	\text{EX}(y, y^*) = \mathbbm{1}[\text{Execute}(y, \mathcal{D}) = \text{Execute}(y^*, \mathcal{D})]
% \end{equation}


% Here, $\text{Execute}(y, \mathcal{D})$ denotes the execution result of SQL query $y$ on the database $\mathcal{D}$, and $\mathbbm{1}[\cdot]$ is the indicator function that equals 1 if the execution results of $y$ and $y^*$ are identical, and 0 otherwise~\cite{DBLP:journals/pvldb/LiLCLT24}.

\textbf{Text-to-SQL Task.}
Let $\mathcal{D} = (\mathcal{T}, \mathcal{C}, \mathcal{R})$ represent a relational database, where $\mathcal{T}$ is the set of tables, $\mathcal{C}$ is the set of columns in those tables, and $\mathcal{R}$ denotes the relationships between tables (e.g., primary key-foreign key constraints). Let $\mathcal{Q}$ denote all well-specified natural language questions over $\mathcal{D}$, and $\mathcal{Y}$ all valid SQL queries over $\mathcal{D}$.

The goal is to find a mapping function $f$ such that for any given question $q \in \mathcal{Q}$, $f(q, \mathcal{D})$ produces a syntactically and semantically correct SQL query $y \in \mathcal{Y}$. % that retrieves the desired data from $\mathcal{D}$.

% Various approaches can be used to realize this mapping function $f$, including fine-tuning LLMs, employing prompt-based techniques, or leveraging hybrid methods.

In the \textbf{zero-shot setting}, the key challenge is to construct a mapping function $f$ \textit{without task-specific labeled data}. This requires the model to generalize across unseen SQL queries and databases, relying solely on pre-trained knowledge and the provided database schema. 
% The absence of fine-tuning or labeled data forces the model to leverage its inherent understanding of both language and databases, which is key to enabling practical and generalized text-to-SQL systems.

\textbf{Formulating Text-to-SQL as a Search Problem.}
We define the Text-to-SQL task as a search problem over a vast space of potential SQL queries, where the search space $\mathcal{S}$ consists of all valid SQL queries for a given database $\mathcal{D}$ and question $q$. To structure this space, we represent $\mathcal{S}$ as a tree $\Psi = (V, E)$, where:

(1) \textbf{Nodes ($V$)}: Each node $v \in V$ represents a partial SQL query state at a specific step in the query construction process. As shown in Figure~\ref{fig:enter-label}, The \textbf{root node} $v_0$ represents the initial empty query and contains the input question $q$ and database schema $\mathcal{D}$.
Intermediate nodes store incremental reasoning steps, such as identifying column values, selecting functions, or constructing SQL clauses.
A \textbf{leaf node} or \textbf{termination node} $v_t$ represents either \textit{a fully constructed SQL query} or  \textit{a state where a termination action is applied}.

(2) \textbf{Edges ($E$)}: Each edge $e \in E$ corresponds to an \textit{action} in the query construction process, such as selecting a table, adding a condition, or applying an aggregation function. These actions model transitions between intermediate query states in the search tree.

(3) \textbf{A Path from Root to Leaf Nodes (Candidate SQL)}: A path from the root node $v_0$ to a leaf node $v_t$ corresponds to a sequence of SQL construction actions that, when composed, forms a complete SQL query $y \in \mathcal{S}$. The SQL query $y$ can be expressed as: $y = v_0 \oplus v_1 \oplus \cdots \oplus v_t$, where $\oplus$ denotes the concatenation or composition of the actions represented by the nodes along the path.

Our goal is to identify an optimal \textbf{reasoning path} in the search tree that constructs an accurate SQL query for a given natural language question $q$ and database schema $\mathcal{D}$.

% Our goal is to efficiently and 
% effectively traverse this search space to find the optimal SQL query $y^* \in \mathcal{S}$ that correctly answers the input question $q$. 
% This can be formulated as follows:
% $y^* = \arg\max_{y \in \mathcal{S}} P(y|q, \mathcal{D})$, 
% 
% \begin{equation}
% 	y^* = \arg\max_{y \in \mathcal{S}} P(y|q, \mathcal{D})
% \end{equation}
% 
% 
% where $P(y|q, \mathcal{D})$ is the probability that $y$ is the correct SQL query given the input question $q$ and database $\mathcal{D}$.

% \nan{I would expect to see such a tree with termination nodes to better understand the problem.}

\begin{figure}[t!]
    \centering
\includegraphics[width=\linewidth]{figures/example.pdf}
\vspace{-2em}
    \caption{Example of the search tree formulation for Text-to-SQL.}
    \label{fig:enter-label}
    \vspace{-1em}
\end{figure}

\textbf{Complexity of the Search Space.}
Efficiently exploring the search space $\mathcal{S}$ for Text-to-SQL generation is a significant challenge due to its combinatorial nature. The size of $\mathcal{S}$, denoted as $|\mathcal{S}|$, grows exponentially with the complexity of the database schema $\mathcal{D}$ and the question $q$. 
% The challenge arises from the need to traverse a vast number of possible SQL queries, many of which may be syntactically valid but semantically incorrect.

% Efficiently exploring the search space $\mathcal{S}$ for text-to-SQL generation is a significant challenge due to its combinatorial nature. The size of $\mathcal{S}$, denoted as $|\mathcal{S}|$, grows exponentially with the complexity of the database schema $\mathcal{D}$ and the question $q$. 

% \yuyu{Here, we define a reasoning path as starting from NL (natural language) and DB (database) information, performing reasoning through different reasoning actions (subtasks), and continuing until the final predicted SQL is derived.}

\subsection{An Overview of \sys}
\label{sub:overview}

To address the challenges of navigating the exponential search space $\mathcal{S}$ and generating high-quality SQL queries from a natural language question $q$, we propose \textbf{Alpha-SQL}, a novel framework that leverages Monte Carlo Tree Search (MCTS)~\cite{mcts}. 



\textbf{MCTS-based Search with LLM-as-Action-Model.}
Building upon the search problem formulation in Section \ref{sub:problem}, Alpha-SQL employs MCTS to construct and explore the search tree $\Psi = (V, E)$. Given an input question $q$, database $\mathcal{D}$, and an LLM $M$, the MCTS process iteratively builds $\Psi$. The root node $v_0 \in V$ represents the initial state with an empty SQL query. The edges $e \in E$ represent an action $a$, where we invoke the LLM $M$ to select a SQL construction \textit{action} such as schema selection or column value identification, as shown in Figure~\ref{fig:enter-label}. The MCTS process iteratively invokes $M$ to apply actions during the search, exploring different paths. Each node $v_i \in V$ represents a partial SQL query state after applying a sequence of actions. %\yuyu{LLMs Thoughts / CoT / LLM-as-Action-Model}
A complete path from the root to a termination (leaf) node forms a reasoning trajectory corresponding to a candidate SQL query. The MCTS process generates multiple such trajectories, forming a set $T = \{\tau_1, \tau_2, \dots, \tau_n\}$ of candidate SQL queries. Therefore, \sys can efficiently explore the vast search space $\mathcal{S}$ defined in the problem formulation.

\begin{figure*}[t!]
    \centering    \includegraphics[width=\textwidth]{figures/Alpha-SQL-overview.pdf}
    \vspace{-2em}
    \caption{An Overview of Alpha-SQL.}
    \label{fig:overview}
    \vspace{-1em}
\end{figure*}
	
\textbf{Self-Supervised Rewards.}
The reward function plays a crucial role in the MCTS process by evaluating the utility of each action, guiding the search toward more promising SQL queries. Traditional methods like Outcome Reward Models~\cite{DBLP:conf/nips/ZelikmanWMG22} and Progress Reward Models~\cite{DBLP:journals/corr/abs-2211-14275} require domain-specific labeled data for training, making them difficult to generalize across different datasets~\cite{DBLP:journals/corr/abs-2406-03816}.

Inspired by human reasoning, we observe that individuals who are confident in their answers tend to consistently provide the same response across multiple attempts, indicating \textbf{high confidence} in their solution. Conversely, when responses vary, it suggests \textbf{low confidence}, implying uncertainty and lower reliability~\cite{rStar}. This intuition forms the foundation of our self-consistency-based reward function, where the confidence of an SQL query is determined by the consistency of its execution results across multiple sampled queries. 
The reward is computed as:


\begin{equation*}\vspace*{-1em} \small 
R(y, q, \mathcal{D}) = \frac{1}{N} \sum_{i=1}^N \mathbbm{1}[\text{Execute}(y, \mathcal{D}) = \text{Execute}(y_i, \mathcal{D})], \end{equation*}

where $y_i$ are sampled SQL queries, and $N$ is the number of samples. 
This formulation reinforces SQL queries that consistently yield stable execution results, enabling Alpha-SQL to prioritize reliable reasoning trajectories without requiring annotated data.
% 
% By leveraging self-consistency as a reward signal, our reward function eliminates the need for supervised reward model training, making Alpha-SQL adaptable across different datasets. This approach ensures robust query selection in a purely zero-shot setting, reinforcing SQL generation accuracy while maintaining generalizability.

% In the absence of labeled data, Alpha-SQL introduces a self-supervised reward function $R(y, q, \mathcal{D})$ to evaluate the quality of candidate SQL queries during the MCTS process. This reward is integrated into the MCTS algorithm to guide the selection of nodes, ensuring that high-quality paths are prioritized. For each leaf node, we compute:

% \begin{equation*}
% \small 
% R(y, q, \mathcal{D}) = \frac{1}{N} \sum_{i=1}^N \mathbbm{1}[\text{Execute}(y, \mathcal{D}) = \text{Execute}(y_i, \mathcal{D})],
% \end{equation*}
	
% where $y_i$ are sampled queries, and $N$ is the number of samples. This self-consistency-based reward mechanism enables Alpha-SQL to navigate the search space effectively.

% 奖励函数是MCTS过程另一个关键组成，在搜索过程中，MCTS通过评估每一个action的奖励值，来选择更加promising的action，从而引导搜索朝向高价值的搜索空间部分。一些工作~\cite{?}通过训练Outcome Reward Model (ORM) 或者Progress Reward Model (PRM) 来作为奖励函数。然而，获取特定领域的奖励模型训练数据集以及训练一个有效的RM是具有挑战性的~\cite{}，使得这种方式难以快速应用到特定domain。
% The reward function constitutes another critical component of the MCTS process. During the search, MCTS evaluates the reward value of each action to select more promising options, thereby guiding the search toward high-value regions of the search space. Several studies~\cite{?} have employed the Outcome Reward Model (ORM) or Progress Reward Model (PRM) as reward functions. However, acquiring domain-specific Reward Model (RM) training datasets and training an effective RM presents significant challenges~\cite{?}, making this approach difficult to rapidly adapt to specific domains.
% 我们回顾人类在完成任务时的行为：如果人类对当前任务的答案很自信，那么即使尝试多次仍会保持相同的答案，答案具有较高的置信度；反之，答案的置信度则比较低。答案的置信度一定程度上反映了质量的好坏~\cite{rStar}。因此，我们设计了一种基于Self-consistency的置信度的奖励函数，使用一种自监督的方式缓解了RM数据的获取和训练挑战（见Sec 3.2）。
% When examining human behavior in task completion, we observe that when individuals are confident in their answer to a given task, they tend to maintain the same response even after multiple attempts, indicating \textbf{high confidence} in their answer. Conversely, \textbf{low confidence} is exhibited when responses vary. The confidence level of an answer reflects its quality~\cite{rStar}. Based on this observation, we propose a self-consistency-based confidence reward function that leverages a self-supervised approach to address the challenges of RM data acquisition and training (see details in Section~\ref{sub:overview}).



% During the MCTS iterations, nodes with higher rewards are more likely to be selected for expansion, making the search towards more promising SQL query constructions. 
% This approach allows the algorithm to balance between exploring new paths and exploiting known good partial solutions, adapting dynamically to the specific question and database schema.






%Our proposed method, Alpha-SQL, employs Monte Carlo Tree Search (MCTS) to dynamically and adaptively explore the search space:
%
%\begin{equation}
%    \tau^* = \arg\max_{\tau \in \mathcal{T}} \text{MCTS}(\tau | q, \mathcal{D}, \theta)
%\end{equation}
%
%where $\tau^*$ is the optimal trajectory in the search tree, and $\theta$ represents the parameters of the underlying language model. This approach allows for more efficient exploration of the search space, potentially leading to more accurate and diverse SQL query generation in zero-shot settings.
%
%To further enhance the search process, we introduce a self-supervised reward function $R(\cdot)$ that evaluates the quality of candidate SQL queries during exploration:
%
%\begin{equation}
%    R(y, q, \mathcal{D}) = \frac{1}{N} \sum_{i=1}^N \mathbbm{1}[\text{Execute}(y) = \text{Execute}(y_i)]
%\end{equation}

%where $y_i$ are sampled SQL queries using high-temperature sampling, and $N$ is the number of samples. This reward function guides the MCTS process towards more promising regions of the search space without requiring labeled data.


% For neural approaches with training, the mapping function $f$ is parameterized by $\theta$ and optimized through:

% \begin{equation}
%     \theta^* = \mathop{\arg\min}\limits_{\theta}\mathbb{E}_{(q,\mathcal{D},y)\sim\Omega}[l(f_{\theta}(q, \mathcal{D}), y)]
% \end{equation}

% where $\mathcal{D}$ represents the training dataset containing natural language questions, database schemas, and their corresponding SQL queries.

% This formulation establishes the foundation for developing and evaluating NL2SQL systems, whether they are based on traditional machine learning approaches, pre-trained language models, or inference-time computation strategies.