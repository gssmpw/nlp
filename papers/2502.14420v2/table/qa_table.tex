\begin{table*}[tb]
  \centering
  \caption{\textbf{Understanding task:} Evaluation of MLLMs and VLAs on 6 Multimodal Understanding benchmarks and 7 VQA benchmarks. The bold denotes top-ranked methods, underlined entries signify secondary performers.} 
  \label{tbl:qa_table}
  \resizebox{1.0\linewidth}{!}{
      \begin{tabular}{c|c|cccccc|ccccccc}
        \toprule
        \multirow{2}{*}{Method} &\multirow{2}{*}{\#Params} & \multicolumn{6}{c|}{Multimodal Understanding Benchmarks} & \multicolumn{7}{c}{VQA Benchmarks} \\
               & & MMMU & MMStar & MME & OCRBench & HallBench & MMB % short for MMBench-EN
                & TextVQA & DocVQA & InfoVQA & AI2D & ChartQA & MTVQA & RealWorldQA \\
        \midrule
        \multicolumn{15}{c}{\textbf{Multimodal Large Language Models}} \\
        \midrule
        Janus~\cite{wu2024janus} & 1.3B & 30.5 & 37.6 & 1338.0 & 482 & 30.3 & 69.4 & — & — & — & 52.8 & — & — & — \\
        DeepSeek-VL~\cite{lu2024deepseek-vl} & 1.3B & 32.2 & 39.9 & — & 409 & 27.6 & 64.6 & — & — & — & 51.5 & — & — & — \\
        Qwen2-VL~\cite{wang2024qwen2}       & 2B & \textbf{41.1} &  \textbf{48.0} &  \textbf{1872.0} & \textbf{809} &  \textbf{41.7} & \underline{74.9} &  \textbf{79.7} &  \textbf{88.57} &  \textbf{61.37} & \underline{74.7} & \underline{73.5} &  \textbf{18.1} &  \textbf{62.9} \\
        SmolVLM~\cite{huggingface2023smolvlm} & 2.3B & 38.8 & 41.7 & — & 656 & 39.5 & — & \underline{72.7} & 81.6 & — & 64.2 & — & — & — \\
        LLaVA-Phi~\cite{llavaphi} & 2.7B & — & — & 1335.1 & — & — & 59.8 & 48.6 & — & — & — & — & — & — \\
        MobileVLM-V2~\cite{mobilevlmv2} & 3B & — & — & 1440.5 & — & — & 63.2 & 57.5 & — & — & — & — & — & — \\
        MoE-LLaVA~\cite{moe-llava} & 3.6B & — & — & 1431.3 & — & — & 68 & 57 & — & — & — & — & — & — \\
        Phi-3-Vision~\cite{abdin2024phi} & 4.2B& \underline{40.4} & — & — & — & — & 80.5 & 70.9 & — & — &  \textbf{76.7} &  \textbf{81.4} & — & — \\
        LLaVA-1.5~\cite{llava1.5} & 7B & 34.2 & — & \underline{1510.7} & — & — & 64.3 & 58.2 & — & — & 63.1 & 55.0 & — & — \\
        DeepSeek-VL~\cite{lu2024deepseek-vl} & 7B & 36.6 & — & — & 456 & — & 73.2 & — & — & — & — & — & — & — \\
        LLaVA-Next~\cite{li2024llavanext-strong} & 8B & 36.4 & — & — & — & — &  \textbf{79.7} & 55.7 & — & — & 66.9 & 65.8 & — & — \\
        \midrule
        \multicolumn{15}{c}{\textbf{Vision-Language-Action Models}} \\
        \midrule
        OpenVLA~\cite{kim24openvla} & 7B & 0 & 0& 0& 0& 0& 0& 0& 0& 0 & 0& 0& 0& 0\\
        %CoAVLA & 2B\\
        ECoT~\cite{ecot}     & 7B & 5.4 & 0 & 0 & 12 & 0.9 & — & 0 & 0 & 0 & 0 & 0 & 1.7 & 0 \\
        DiVLA~\cite{diffusionvla} & 2B & 17.2 & 21.1 & 186.5 & 294 & 9.0 & — & 7.5 & 15.2 & 14.7 & 43.1 & 17.2 & 6.2 & 25.2\\
       \textbf{ChatVLA(Ours)} & 2B & \textbf{37.4} & \textbf{\underline{47.2}} & \textbf{1435.2} & \textbf{\underline{729}} & \textbf{\underline{39.9}} & \textbf{69.0} & \textbf{71.2} & \textbf{\underline{83.3}} & \textbf{\underline{53.3}} & \textbf{67.6} & \textbf{59.9} & \textbf{\underline{11.5}} & \textbf{\underline{57.0}} \\
        \bottomrule
      \end{tabular}
  }
\end{table*}
