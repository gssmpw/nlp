\begin{figure*}[t]
    \centering
    \includegraphics[width=.9\linewidth]{picture/chatvla_analyse.pdf}
    \caption{\textbf{Analysis of how training data influences VLA performance on control and understanding tasks.} (a) We use three different sets of training data, corresponding to the three main training approaches for VLA models. (b) The experimental results are presented for five real-world robot tasks across three settings. (c) The results on VQA and multimodal understanding benchmarks.}
    \label{fig:setting}
\end{figure*}


\section{Methodology}

This section provides a thorough discussion of our framework. Section~\ref{sec:definition} presents formal definitions. Section~\ref{sec:motivation} details our motivation and empirical results demonstrating how existing vision-language-action models (VLAs) suffer from catastrophic forgetting and task interference, thus hindering the unification of multimodal understanding and robot control. Section~\ref{sec:architecture_data} proposes a simple solution to address this problem.



\subsection{Formal Definition}\label{sec:definition}
Consider two distinct scenarios: robot control and multimodal understanding. In the context of robot control, we typically construct a dataset of demonstrations $D_{robot} = \{\tau_{i}\}_{i=1}^{N}$, where each demonstration $\tau_{i}$ comprises a sequence of state-action pairs.  The state $s$ consists of an observation (image) $v$ and an instruction (text) $t$, such that $s = (v, t)$.  We can represent the sequence of state-action pairs as $\tau_{i} = \{((v_{1},t_{1}),a_{1}), ((v_{2},t_{2}),a_{2}), \dots, ((v_{T},t_{T}),a_{T})\}$, where each tuple $((v_j, t_j), a_j)$ represents the state at timestep $j$ and the corresponding action taken, and $T$ is the length of the demonstration. These demonstrations are typically provided by a human expert.

For multimodal understanding and visual conversation tasks, we have a dataset $D_{v-t} = \{\phi_{i}\}_{i=1}^{M}$, where each data sample $\phi_{i}$ consists of a visual image $v_i$ and a corresponding question (or caption) in textual form $t_i$, i.e., $\phi_{i} = \{(v_i, t_i)\}$. Here, $M$ represents the total number of such image-text pairs. The notation $v-t$ denote visual-text data.

The overarching goal of our work is to develop a general model $\pi$ capable of addressing both embodied control and multimodal understanding.  For embodied control, this involves learning a policy that models the joint distribution of robot actions given the current visual observation and textual instruction: $\pi(a_{t}|v_{t},t_{t})$.  Simultaneously, for multimodal understanding and visual question answering, the model should capture the distribution of the text (answer or caption) given the visual input: $\pi(t|v)$.  Our objective is to create a unified model that can effectively learn both distributions, enabling it to perform well in both robot control tasks and multimodal understanding scenarios.

Current VLA research focuses on developing more robust and generalizable models for learning visuomotor policies~\cite{kim24openvla,[pi0,wen2024tinyvla}. Some approaches explore chain-of-thought-like reasoning to improve policy generation~\cite{ecot,diffusionvla,coa-vla}, while others investigate co-training VLA models with visual-text and robot data~\cite{pertsch2025fast}. In particular, some studies report benefits from co-training with visual-text data in laboratory settings~\cite{rt-2}, while others find it less effective in real-world scenarios~\cite{ecot}. Although a few works suggest that VLA can maintain conversational ability~\cite{diffusionvla,rt-2}, none have thoroughly investigated how this ability, along with general multimodal understanding, is preserved after applying the VLA training paradigm. In the following section, we analyze different training data setups for VLA, focusing specifically on the resulting model's performance in both multimodal understanding and real-world robot control. Our goal is to provide practical guidance for building unified models capable of both.



\subsection{Analysis}\label{sec:motivation}
To understand the capabilities of existing VLA models in terms of multimodal understanding and embodied control, we investigate three distinct training paradigms, each utilizing a different dataset: 1) training solely with robot data, the most prevalent approach in VLA~\cite{[pi0,openflamingo,kim24openvla,wen2024tinyvla}, primarily focused on optimizing robot control performance; 2) augmenting robot data with chain-of-thought-like reasoning, aiming to provide auxiliary information that improves both model generalization and robot task performance~\cite{diffusionvla,ecot}; and 3) co-training with both visual-text data and robot data. This latter paradigm was pioneered by RT-2~\cite{rt-2}; however, due to proprietary data and model details, exact replication is challenging. Following RT-2, we used a 3:1 ratio of robot data to visual-text data in this experiment.

In this section, we analyze these three training data setups for VLA models. Specifically, we utilize DiffusionVLA~\cite{diffusionvla}, a representative VLA model that supports both language output via autoregression and action generation via a diffusion model. We evaluate performance on six representative benchmarks: four focused on visual question answering and two providing a broader evaluation of multimodal large language models, encompassing tasks like math and OCR.  Furthermore, we assess performance on five real-world robot tasks covering diverse skills, including hanging, pulling, picking, and placing.  Following the methodology of DiffusionVLA, we generate robot reasoning data. For visual-text data, we randomly sample 54k image-text pairs from LLaVA. Further details regarding experimental setup and data processing are provided in the Appendix.


\textbf{Results on multimodal understanding and question-answering benchmark.} The experimental results are presented in Figure 2. The bottom-right portion of the figure displays performance on six benchmarks, encompassing both visual question answering (VQA) and general understanding tasks. The top-right portion of Figure 2 shows the average success rate across a total of 112 trials conducted on five real-world robot tasks. 

The bottom-right table includes results for the base model, Qwen2-VL~\cite{wang2024qwen2}. Some results are intuitive. For example, training the model solely on robot data yields a performance of 0 across all benchmarks. This model completely loses its conversational ability, exhibiting only murmuring when asked a question. As expected, the smallest performance drop compared to the base model occurs when training uses both visual-text pairs and robot data. Interestingly, training with robot data including reasoning also boosts performance from 0 to a non-negligible level, despite the highly structured, template-driven nature of the reasoning phrases within that data. Even though the reasoning phrases are similar and structured, explicitly allowing the model to ``speak out" significantly improves performance on question answering and even general understanding.

\textbf{\textit{Conclusion 1.}} Our observations suggest that the pre-trained VLM component suffers from what appears to be catastrophic forgetting. Training solely with robot data causes the model to lose previously acquired conversational and understanding abilities. However, our experiments indicate that this isn't necessarily a complete loss of knowledge, but rather a misalignment caused by the robot data. Training with a fixed reasoning template seems to ``reactivate" the visual-text alignment, enabling the model to engage in conversation and demonstrate understanding. In Section~\ref{sec:appendix}, we will delve into the specific knowledge that is reactivated and discuss how future work can further bridge the gap between the base VLM and the VLA model. We term this phenomenon ``spurious forgetting."


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{picture/chatvla_moe.pdf}
    \caption{\textbf{Training strategy.} Our framework is initially trained on robot data with action trajectories, then co-trained with visual-text and robot data to maintain performance in both domains.}
    \label{fig:stage}
\end{figure}
\textbf{Results on real robot multi-task settings.} We further evaluated different approaches to our real robot setup. All methods were trained on 25 real robot tasks, and we selected five diverse tasks, covering skills like pushing, picking, and hanging, for comparison.  Details, including the number of trials for each experiment, can be found in the Appendix.  Surprisingly, training with only robot data yielded worse performance than incorporating reasoning. This confirms previous findings that leveraging either visual or textual chain-of-thought enhances the generalization of robot models.  Intriguingly, co-training robot data with visual-text data resulted in a significant performance drop in real-world task success rates.

\textbf{\textit{Conclusion 2.}} The initial observation that incorporating reasoning into robot data improves performance aligns with Dual Coding Theory~\cite{paivio1991dual}.  This theory posits that physical motor skills and visual-linguistic understanding are not mutually exclusive but rather interconnected, offering overlapping benefits. However, the performance of robot control dramatically decreased when visual-text pairs were added to the training data. This suggests that the distinct representations required for action generation and understanding may compete within the shared parameter space. This phenomenon, we named as \textbf{partial task interference}, requires careful resolution. A unified system should connect the two data types while simultaneously enabling separable representation learning for each task.


\begin{figure}[t]
    \centering
    \includegraphics[width=.9\linewidth]{picture/chatvla_em.pdf}
    \caption{\textbf{Illustration of the Mixture-of-Experts component of ChatVLA.} Two distinct expert types process robot data and visual-text data separately, while shared self-attention layers facilitate knowledge transfer between the two domains.}
    \label{fig:framework}
    \vspace{-1mm}
\end{figure}

\input{table/qa_table}
\subsection{Method: ChatVLA}
\label{sec:architecture_data}
As discussed above, training on robot policy data can interfere with learning of visual-text relationships.  Furthermore, training exclusively on robot data can diminish visual-text alignment, leading to a degradation of the model's conversational abilities. Therefore, addressing these two challenges is crucial for successfully unifying both perspectives within a single VLA model. We will first describe the training strategy used to address spurious forgetting, and then outline the general architecture of our method to tackle the second challenge.




\textbf{Phased alignment training.} Previously, we identified that spurious forgetting is a key factor in causing VLA to lose its ability to chat and understand scenes. Since the pre-trained VLM is well-trained and excels at visual-related tasks, it is intuitive that the ability to chat and understand scenes can be reactivated with a small amount of visual-text pair data. In contrast, robot control tasks are much more complex to train, so the priority should be to develop an excellent model that excels at embodied control tasks. Our training strategy is straightforward yet effective. We first train the VLA model on robot data. During this training, we also include reasoning data to ensure continuous alignment between the visual and text components. Once the robot data is trained, we co-train both visual-text and robot data to help the model retain proficiency in both tasks.



\input{table/ts_longhorizon}
\input{table/ts_compo}
\input{table/ts_other}
\textbf{Mixture of experts.} The previous section demonstrated the use of phased alignment training to address the spurious forgetting problem, enabling the model to retain knowledge from the previously trained VLM. However, this approach does not fully resolve task interference issues, as the model still requires co-training on both visual-text and robot data. We introduce the mixture-of-expert to resolve the problem, which is in Figure~\ref{fig:framework}. Specifically, given $x^l$ be the input of the $l$-th block. The input can either belong to the $D_{robot}$ or $D_{v-l}$. Notably, we design a dual router, the one to deal with tasks regarding multimodal understanding and conversational ($f(\text{FFN}_{v-l})$), and the other learn representation on robot control ($f(\text{FFN}_{robot})$). The input is first coming through a multi-head self-attention $x^{l'} = \text{MHA}(x^{l-1})+x^{l-1}$, where $\text{MHA}(\cdot)$ represents multi-head self attention. It is then fed into the mixture-of-expert layer, which can be represented as:
\begin{align*}
MoE(x^{l'})&=\begin{cases}
f(\text{FFN}_{v-l})(x^{l'}),&m = 0\\
f(\text{FFN}_{robot})(x^{l'}),&1\leq m\leq M_r
\end{cases}
\end{align*}
This is then added with input from skip connection $x^{l} = x^{l'} + MoE(x^{l'})$. Notice that in stage 1 training, only the control expert is activated. 

To differentiate task outputs, we employ distinct system prompts, such as ``Answer based on question" for understanding and conversation tasks, and ``Predict robot action" for control tasks. Intuitively, a static MoE architecture applied to the MLP layers can be viewed as a high-dimensional feature extractor that partitions the shared parameter space. This allows each task (e.g., understanding and control) to utilize a substantial portion of dedicated neurons, enabling the model to excel at both. A key advantage of this MoE-like architecture is that during inference, only one path is activated, preserving the model parameters of the base model. Our results demonstrate that this straightforward approach leads to simultaneous improvements in understanding, conversation, and control performance. 



\begin{figure*}[t]
   \centering
   \includegraphics[width=0.85\linewidth]{picture/chatvla_finegrain_mmmu_results.pdf}
   \caption{Comparison with Qwen2-VL on $MMMU_{val}$.}
   \label{fig:mmmu_result}
\end{figure*}

\textit{Why sharing self-attention layers?} A prevailing solution is a use mixture of attention to learn task-specific representation. However, based on our experiments (detailed in Section~\ref{sec:exp}), we believe that understanding and robot control tasks share representations that are beneficial to both. For example, a typical robot control scenario requires the model to understand the scene, recognize objects, determine their locations, and then translate this information into actions.  These high-dimensional representations share similar semantic concepts. Therefore, the interconnected nature of these two tasks is crucial for simultaneously improving performance on both understanding and control.









