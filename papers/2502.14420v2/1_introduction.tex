\section{Introduction}
Recent advancements in Vision-Language-Action (VLA)~\cite{[pi0, kim24openvla, wen2024tinyvla, wen2025dexvla} models have largely prioritized robotic action mastery. While models trained on robotic control tasks excel at low-level manipulation and physical interaction, they often struggle to interpret and reason about multimodal data like images and text. This is paradoxical, as modern VLA architectures build upon pre-trained vision-language models (VLMs). Conversely, VLMs trained on visual-text pairs demonstrate impressive multimodal scene understanding but lack the ability to physically interact with the environment. This duality highlights a critical challenge: unifying embodied control and multimodal understanding by aligning these disparate data sources (robotic actions and visual-text semantics) without sacrificing performance in either domain.

This work investigates how to unify a single end-to-end neural network capable of multimodal scene understanding, conversational ability, and physical interaction. We first explore existing training paradigms to assess their feasibility for unification. Specifically, we examine three data settings for VLA training: 1) training solely on expert demonstration data containing robot action trajectories (the most common approach, e.g., OpenVLA~\cite{kim24openvla}, TinyVLA~\cite{wen2024tinyvla}, $\pi_0$~\cite{[pi0}); 2) augmenting robot data with reasoning phrases to guide action (similar to ECoT~\cite{ecot} and DiffusionVLA~\cite{diffusionvla}); and 3) co-training with both visual-text pairs and robot data (as in RT-2~\cite{rt-2}). We analyze how each configuration impacts the model's ability to balance control and understanding. Our experiments reveal that training solely with robot data erodes conversational ability entirely; adding reasoning data partially preserves multimodal understanding; and introducing visual-text pairs significantly weakens control capabilities.  This suggests two key challenges: (1) VLA models suffer from \textbf{spurious forgetting}~\cite{zheng2025spurious, zhai2023investigating, luo2023empirical}, where performance degradation may not reflect complete knowledge loss from pre-trained VLMs, but rather a shift in how the model aligns its internal representations with different tasks. The alignment between robot actions and visual-text data appears fragile and susceptible to being overwritten during fine-tuning. (2) \textbf{Task interference}~\cite{wang2021afec,ahn2025prevalence} arises, where the conflicting parameter spaces of control and understanding tasks, sharing overlapping representations, cause mutual performance degradation when trained simultaneously. 

To address these challenges, we present ChatVLA, a simple yet effective framework—in terms of both neural architecture and training strategy—for enabling a single neural network to master both understanding and manipulation. We propose Phased Alignment Training, a two-stage strategy inspired by curriculum learning. The model first masters embodied control before incrementally integrating multimodal data to ``reactivate" frozen alignment links. Furthermore, we introduce a Mixture-of-Experts (MoE) on the MLP layers. This allows the two tasks to share attention layers (for cross-task knowledge transfer) while isolating task-specific MLPs (to minimize interference). This design is motivated by Dual Coding Theory~\cite{paivio1991dual}, which posits that human minds process information through two separate but interconnected systems: one for physical skills and the other for verbal and visual practice. The shared attention layers in ChatVLA facilitate the exchange of mutually beneficial knowledge between understanding and control tasks, while the separate MLP layers process learned knowledge independently.

We evaluate ChatVLA across three dimensions: conversational ability (visual question answering), general multimodal understanding, and general robot control. Specifically, we assess its conversational ability on established datasets like TextVQA~\cite{textvqa} and DocVQA~\cite{mathew2021docvqa}, where it achieves competitive performance compared to existing VLMs. Furthermore, ChatVLA demonstrates strong multimodal understanding capabilities on general visual and textual benchmarks, including MMMU~\cite{yue2023mmmu}, MME~\cite{mme}, and MMStar~\cite{chen2024we}. Notably, compared to state-of-the-art VLA methods like ECoT, our method achieves a 6x performance improvement on MMMU and boosts performance on MMStar from 0 to 47.2, using 3.5x fewer parameters in the VLM backbone. Finally, we evaluate ChatVLA on 25 real-world robot tasks encompassing diverse skills like picking, placing, pushing, and hanging, across multiple environments such as bathrooms, kitchens, and tabletops. In this multi-task setting, our method outperforms state-of-the-art VLA methods like OpenVLA. These results validate the effectiveness of our approach, showcasing the potential of a single unified method for both multimodal understanding and robot control.

In summary, our contributions are the following:
\begin{itemize}
    \item We provide an in-depth analysis of existing VLA approaches under rigorous settings, demonstrating their limitations in achieving satisfactory performance across both multimodal understanding and robot control.
    \item We introduce ChatVLA, a simple yet effective framework that unifies conversational ability, multimodal understanding, and robot control within a single neural network.
    \item We conduct extensive experiments to evaluate ChatVLA's performance on various question-answering and general understanding benchmarks. 
    \item We perform extensive real-world robot experiments, encompassing 25 diverse tasks in realistic home environments (tabletop, kitchen, and bathroom), demonstrating ChatVLA's superior performance in real-world robot control scenarios.
\end{itemize}

