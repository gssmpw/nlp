
\section{Experiment}
\label{sec:exp}
In this section, we conduct a series of experiments to evaluate the performance of ChatVLA across a range of embodied control and multimodal understanding tasks. 


\subsection{Results on Multimodal Understanding and Visual-Question Answering}

We evaluate the visual question answering abilities of ChatVLA using Vlmevalkit~\cite{duan2024vlmevalkit}  on TextVQA ~\cite{singh2019towards}, DocVQA~\cite{mathew2021docvqa}, InfoVQA~\cite{9706887}, AI2D~\cite{kembhavi2016diagram}, ChartQA~\cite{masry-etal-2022-chartqa}, MTVQA~\cite{tang2024mtvqa}, and RealworldQA~\cite{RealWorldQA}.
We also tested against more challenging benchmarks designed for MLLMs, i.e.,
MMMU~\cite{yue2023mmmu}, MMStar~\cite{chen2024we}, MME~\cite{mme}, OCRBench~\cite{Liu_2024}, HallBench~\cite{Guan_2024_CVPR} and MMBench~\cite{mmbench}. 
As delineated in Table \ref{tbl:qa_table}, ChatVLA demonstrates competitive performance relative to existing VLMs across multiple benchmarks. Notably, in VQA tasks, our framework achieves a notable performance of 71.2 on TextVQA, surpassing current SOTA VLAs by substantial margins. Specifically, it outperforms ECoT and DiVLA by relative improvements of 9.2x and 9.5x over these baseline models. The model exhibits particularly strong capabilities in multimodal reasoning tasks requiring complex cross-modal integration. On the MMStar benchmark, ChatVLA attains a score of 37.4, demonstrating 2.2x and 6.9x performance enhancements over DiVLA and ECoT respectively. 


\subsection{Results on Real Robot Tasks}

The embodied control performance of ChatVLA is evaluated on 25 realworld manipulation tasks. All these evaluated tasks can be divided into three categories according to the granularity of the language instructions. A more detailed description of these tasks can be found in the Appendix (Section \ref{sec:appendix}). We conducted 528 trials on a real robot to evaluate the model's ability.

\textbf{Long-horizon tasks with direct prompting.} The model is asked to executing tasks directly from language instruction(e.g., ``Sort toys"). The four tasks we evaluated were all completed within a toy scenario constructed on a desktop setup.
Challenging tasks of this category include Task 1, where all toys are randomly positioned in varying poses, and Task 3, which demands the integration of three distinct skills: opening, picking, and closing.
Our method demonstrates substantial advantages in executing tasks directly from high-level descriptions across all evaluated scenarios. The approach maintains consistent performance in multi-step sequences, achieving a 0.54 average success length in Task 1 (6.75x higher than Octo) and perfect success rates throughout Task 3â€™s three-step sequence.


\textbf{Long-horizon tasks with high-level planner.} The model receives intermediate commands that specify the current sub-task objectives (e.g., ``pick object and place to target location"). The primary challenge in this evaluation stems from the substantial variations between sub-tasks, which involve: (1) diverse object types (e.g., plates, cups, bread), (2) multiple required skills (e.g., pick-place,flip), (3) varying location heights (e.g. top/bottom shelf positions) as visually demonstrated in the bottom-right panel of Fig.\ref{fig:firstfig}. These variations collectively create a testbed for evaluating the model's compositional reasoning capability - specifically, its capacity to integrate object manipulation, spatial reasoning, and interference adaptation. This requirement is clearly reflected in the experimental results shown in Table \ref{tbl:taskscore_compositional}, where our method outperforms OpenVLA and Octo across all task configurations.

\textbf{Cross-skill multi-tasking.} These tasks require the integration of multiple manipulation skills (e.g., picking, placing, pushing, and hanging) across various real-world environments, specifically categorized into three test domains: bathroom scenarios (Tasks 14-17), kitchen environments (Tasks 18-19), and tabletop configurations (Tasks 20-25). As demonstrated in Table \ref{tbl:taskscore_other}, ChatVLA achieves superior performance compared to both Octo and OpenVLA across all task categories. The model exhibits particularly strong performance in challenging bathroom and kitchen tasks, where robotic arm operations are constrained to a severely limited spatial range. This experimental setup inherently introduces substantial safety considerations during model evaluation, consequently establishing rigorous requirements for the operational precision and system robustness of the assessed models.

\subsection{Ablation Study}


\textbf{What vision-language data are preferred?}
% \begin{figure*}[t]
%    \centering
%    \includegraphics[width=0.85\linewidth]{picture/chatvla_finegrain_mmmu_results.pdf}
%    \caption{Comparison with Qwen2-VL on $MMMU_{val}$.}
%    \label{fig:mmmu_result}
% \end{figure*}
In stage 2, we employed the LLaVA-1.5~\cite{llava1.5} dataset for co-training, which allowed the model to achieve compatible results on both VQA and MLLM benchmarks compared to Qwen2-VL. However, we argue that the remaining performance gap is attributed to the limitations of the visual-text data used. To explore this further, we conducted an in-depth analysis of the results between ChatVLA and Qwen2-VL on the MMMU dataset, as illustrated in Fig. \ref{fig:mmmu_result}.

The MMMU dataset is divided into six categories, and ChatVLA's performance is slightly lower than Qwen2-VL in three of them: art, medicine, and social science. A closer inspection of the results for the corresponding subcategories reveals that the performance discrepancies primarily occur in five specific domains: art theory, lab medicine, pharmacy, literature, and psychology. These fields involve relatively limited specialized knowledge that is difficult to obtain. Upon reviewing the composition of the LLaVA dataset, we were surprised to find that its subdatasets, including COCO, GQA, OCR-VQA, TextVQA, and VisualGenome, lack the expert knowledge required for these domains, which likely contributed to the observed performance drop. A more detailed description of the composition of these datasets can be found in the Section~\ref{sec:llava_dataset}.

This finding also highlights the considerable potential of our model: with more appropriate professional data for training, we believe that we can achieve significantly better performance in multimodal understanding.

\textbf{What is the appropriate ratio of visual-text data to robot data?}
While co-training with visual-text data, we followed the settings discussed in ECoT~\cite{ecot} and set the overall visual-text data to robot data ratio at 1:3. However, whether other data ratios are beneficial or detrimental to multimodal understanding and robot tasks still requires attention. Therefore, under the same number of steps, we modified the ratio of visual-text data to robot data in co-training to 1:1 and 3:1, respectively. The results of the three setups are shown in the Table~\ref{tbl:ratio}.
\begin{table*}[tb]
  \centering
  \caption{\textbf{Understanding task.} Evaluation of MLLMs and VLAs on 6 multimodal understanding benchmarks and 7 VQA benchmarks. We use bold to denote top-ranked methods, and underlined entries signify secondary performers.} 
  \label{tbl:ratio}
  \resizebox{1.0\linewidth}{!}{
      \begin{tabular}{c|ccccc|ccccccc}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{5}{c|}{Multimodal Understanding Benchmarks} & \multicolumn{7}{c}{VQA Benchmarks} \\
               & MMMU & MMStar & MME & OCRBench & HallBench  
                & TextVQA & DocVQA & InfoVQA & AI2D & ChartQA & MTVQA & RealWorldQA \\
        \midrule
        
        1:1& 36.1&44.7&1426.9&691&36.2&72.6&82.9&54.0&65.382&62.6&10.0&57.9\\
        3:1&35.3&45.3&1399.5&726&36.4&72.7&83.6&54.3&67.0&63.2&10.3&58.8\\
       
      1:3 & 37.4 &47.2 & 1435.2 & 729 & 39.9 & 71.2 & 83.3 & 53.3 & 67.6 & 59.9 & 11.5 & 57.0 \\
        \bottomrule
      \end{tabular}
  }
\end{table*}

Surprisingly, a smaller amount of visual-text data resulted in better performance. This aligns with the analysis in the previous subsection~\ref{sec:motivation} in the paper, which suggests that even a limited amount of visual-text data is sufficient to reactivate visual-text alignment and bridge the real-world interacting capacity gap between the base VLM and the VLA model.

