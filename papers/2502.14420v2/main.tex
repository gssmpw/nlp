% ICCV 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{iccv}              % To produce the CAMERA-READY version
% \usepackage[review]{iccv}      % To produce the REVIEW version
\usepackage[pagenumbers]{iccv} % To force page numbers, e.g. for an arXiv version
\definecolor{deepblue}{HTML}{27a2c3}
% Import additional packages in the preamble file, before hyperref


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{iccvblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=iccvblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{*****} % *** Enter the Paper ID here
\def\confName{ICCV}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{ChatVLA: Unified Multimodal Understanding and Robot Control \\with Vision-Language-Action Model}

%%%%%%%%% AUTHORS - PLEASE UPDATE
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\author{
Zhongyi Zhou$^{1,2*}$ \quad Yichen Zhu$^{1*\dagger}$\quad Minjie Zhu$^{2}$\quad Junjie Wen$^{2}$\quad Ning Liu$^{4}$ \quad Zhiyuan Xu$^{4}$\\ 
Weibin Meng$^{5}$\quad Ran Cheng$^{1}$\quad Yaxin Peng$^{3}$\quad Chaomin Shen$^{2}$\quad Feifei Feng$^{1}$ \vspace{0.03in} \\
$^1$Midea Group \quad $^2$East China Normal University \quad $^3$Shanghai University
\\ \quad $^4$ Beijing Innovation Center of Humanoid Robotics \quad $^5$Tsinghua University \\
\thanks{$*$ Co-first author. $\dagger$ Corresponding author.}
\vspace{-0.1in} \\
\href{https://chatvla.github.io}{\color{deepblue}\textbf{chatvla.github.io}\xspace} \vspace{-0.3in}
}


\begin{document}

\makeatletter
\let\@oldmaketitle\@maketitle%
\renewcommand{\@maketitle}{\@oldmaketitle
    \begin{center}
        \captionsetup{type=figure}
        \centering
    \includegraphics[width=1.02\textwidth]{picture/chatvla_framework.pdf}
    \caption{\textbf{ChatVLA is the first work to unify multimodal understanding and embodied control.} We conduct extensive evaluations on VQA and multimodal understanding benchmarks to demonstrate that robot foundation models can also engage in chat. Furthermore, we evaluate our approach on diverse real-world robot tasks.}
    \label{fig:firstfig}
    \end{center}
    %\vspace{-1em}
}
\def\thanks#1{\protected@xdef\@thanks{\@thanks
        \protect\footnotetext{#1}}}
\makeatother
\maketitle
\input{0_abstract}
\input{1_introduction}
\input{2_related_work}
\input{3_method}
\input{4_experiment}
\input{5_conclusion}

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdin et~al.(2024)Abdin, Aneja, Awadalla, Awadallah, Awan, Bach, Bahree, Bakhtiari, Bao, Behl, et~al.]{abdin2024phi}
Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar~Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et~al.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock \emph{arXiv preprint arXiv:2404.14219}, 2024.

\bibitem[Ahn et~al.(2025)Ahn, Hyeon, Oh, Hwang, and Moon]{ahn2025prevalence}
Hongjoon Ahn, Jinu Hyeon, Youngmin Oh, Bosun Hwang, and Taesup Moon.
\newblock Prevalence of negative transfer in continual reinforcement learning: Analyses and a simple baseline.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025.

\bibitem[Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu, Marathe, Bitton, Gadre, Sagawa, et~al.]{openflamingo}
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et~al.
\newblock Openflamingo: An open-source framework for training large autoregressive vision-language models.
\newblock \emph{arXiv preprint arXiv:2308.01390}, 2023.

\bibitem[Black et~al.(2023{\natexlab{a}})Black, Janner, Du, Kostrikov, and Levine]{black2023training}
Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine.
\newblock Training diffusion models with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2305.13301}, 2023{\natexlab{a}}.

\bibitem[Black et~al.(2023{\natexlab{b}})Black, Nakamoto, Atreya, Walke, Finn, Kumar, and Levine]{black2023zero}
Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine.
\newblock Zero-shot robotic manipulation with pretrained image-editing diffusion models.
\newblock \emph{arXiv preprint arXiv:2310.10639}, 2023{\natexlab{b}}.

\bibitem[Black et~al.(2024)Black, Brown, Driess, Esmail, Equi, Finn, Fusai, Groom, Hausman, Ichter, Jakubczak, Jones, Ke, Levine, Li-Bell, Mothukuri, Nair, Pertsch, Shi, Tanner, Vuong, Walling, Wang, and Zhilinsky]{[pi0}
Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy~Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky.
\newblock $\pi_0$: A vision-language-action flow model for general robot control, 2024.

\bibitem[Brohan et~al.(2023{\natexlab{a}})Brohan, Brown, Carbajal, Chebotar, Chen, Choromanski, Ding, Driess, Dubey, Finn, et~al.]{brohan2023rt-2}
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et~al.
\newblock Rt-2: Vision-language-action models transfer web knowledge to robotic control.
\newblock \emph{arXiv preprint arXiv:2307.15818}, 2023{\natexlab{a}}.

\bibitem[Brohan et~al.(2023{\natexlab{b}})Brohan, Brown, Carbajal, Chebotar, Chen, Choromanski, Ding, Driess, Dubey, Finn, et~al.]{rt-2}
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et~al.
\newblock Rt-2: Vision-language-action models transfer web knowledge to robotic control.
\newblock \emph{arXiv preprint arXiv:2307.15818}, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Li, Dong, Zhang, Zang, Chen, Duan, Wang, Qiao, Lin, et~al.]{chen2024we}
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et~al.
\newblock Are we on the right way for evaluating large vision-language models?
\newblock \emph{arXiv preprint arXiv:2403.20330}, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Wang, Cao, Liu, Gao, Cui, Zhu, Ye, Tian, Liu, et~al.]{chen2024expanding}
Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et~al.
\newblock Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling.
\newblock \emph{arXiv preprint arXiv:2412.05271}, 2024{\natexlab{b}}.

\bibitem[Chen et~al.(2024{\natexlab{c}})Chen, Wu, Wang, Su, Chen, Xing, Zhong, Zhang, Zhu, Lu, et~al.]{chen2024internvl}
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et~al.
\newblock Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 24185--24198, 2024{\natexlab{c}}.

\bibitem[Chi et~al.(2023)Chi, Feng, Du, Xu, Cousineau, Burchfiel, and Song]{diffusion-policy}
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song.
\newblock Diffusion policy: Visuomotor policy learning via action diffusion.
\newblock \emph{arXiv preprint arXiv:2303.04137}, 2023.

\bibitem[Chu et~al.(2024)]{mobilevlmv2}
Xiangxiang Chu et~al.
\newblock Mobilevlm v2: Faster and stronger baseline for vision language model.
\newblock \emph{arXiv preprint arXiv:2402.03766}, 2024.

\bibitem[Dai et~al.(2023)]{instructblip}
W Dai et~al.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning.
\newblock \emph{arXiv preprint arXiv:2305.06500}, 2023.

\bibitem[Dasari et~al.(2024)Dasari, Mees, Zhao, Srirama, and Levine]{dasari2024ingredients}
Sudeep Dasari, Oier Mees, Sebastian Zhao, Mohan~Kumar Srirama, and Sergey Levine.
\newblock The ingredients for robotic diffusion transformers.
\newblock \emph{arXiv preprint arXiv:2410.10088}, 2024.

\bibitem[Ding et~al.(2024)Ding, Zhao, Zhang, Song, Zhang, Huang, Yang, and Wang]{ding2024quar}
Pengxiang Ding, Han Zhao, Wenjie Zhang, Wenxuan Song, Min Zhang, Siteng Huang, Ningxi Yang, and Donglin Wang.
\newblock Quar-vla: Vision-language-action model for quadruped robots.
\newblock In \emph{European Conference on Computer Vision}, pages 352--367. Springer, 2024.

\bibitem[Duan et~al.(2024)Duan, Yang, Qiao, Fang, Chen, Liu, Dong, Zang, Zhang, Wang, et~al.]{duan2024vlmevalkit}
Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et~al.
\newblock Vlmevalkit: An open-source toolkit for evaluating large multi-modality models.
\newblock In \emph{Proceedings of the 32nd ACM International Conference on Multimedia}, pages 11198--11201, 2024.

\bibitem[Face(2023)]{huggingface2023smolvlm}
Hugging Face.
\newblock Smolvl: Scaling vision-language models to tiny sizes, 2023.

\bibitem[Fu et~al.(2023)Fu, Chen, Shen, Qin, Zhang, Lin, Yang, Zheng, Li, Sun, et~al.]{mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et~al.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2306.13394}, 2023.

\bibitem[Guan et~al.(2024)Guan, Liu, Wu, Xian, Li, Liu, Wang, Chen, Huang, Yacoob, Manocha, and Zhou]{Guan_2024_CVPR}
Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou.
\newblock Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 14375--14385, 2024.

\bibitem[Huang et~al.()Huang, Yong, Ma, Linghu, Li, Wang, Li, Zhu, Jia, and Huang]{leo3d}
Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang.
\newblock An embodied generalist agent in 3d world.
\newblock In \emph{ICLR 2024 Workshop: How Far Are We From AGI}.

\bibitem[Karamcheti et~al.(2024)Karamcheti, Nair, Balakrishna, Liang, Kollar, and Sadigh]{karamcheti2024prismatic}
Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh.
\newblock Prismatic vlms: Investigating the design space of visually-conditioned language models.
\newblock \emph{arXiv preprint arXiv:2402.07865}, 2024.

\bibitem[Kembhavi et~al.(2016)Kembhavi, Salvato, Kolve, Seo, Hajishirzi, and Farhadi]{kembhavi2016diagram}
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock A diagram is worth a dozen images.
\newblock In \emph{Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14}, pages 235--251. Springer, 2016.

\bibitem[Kim et~al.(2024)Kim, Pertsch, Karamcheti, Xiao, Balakrishna, Nair, Rafailov, Foster, Lam, Sanketi, Vuong, Kollar, Burchfiel, Tedrake, Sadigh, Levine, Liang, and Finn]{kim24openvla}
{Moo Jin} Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn.
\newblock Openvla: An open-source vision-language-action model.
\newblock \emph{arXiv preprint arXiv:2406.09246}, 2024.

\bibitem[Lauren{\c{c}}on et~al.(2023)Lauren{\c{c}}on, Saulnier, Tronchon, Bekman, Singh, Lozhkov, Wang, Karamcheti, Rush, and Kiela]{idefics}
H Lauren{\c{c}}on, L Saulnier, L Tronchon, S Bekman, A Singh, A Lozhkov, T Wang, S Karamcheti, A Rush, and D Kiela.
\newblock Obelisc: An open web-scale filtered dataset of interleaved image-text documents.
\newblock \emph{arXiv preprint arXiv:2306.16527}, 2023.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Zhang, Zhang, Guo, Zhang, Li, Zhang, Liu, and Li]{li2024llavanext-strong}
Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li.
\newblock Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Li, Savarese, and Hoi]{blip-2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock \emph{arXiv preprint arXiv:2301.12597}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Zhu, Tang, Wen, Zhu, Liu, Li, Cheng, Peng, and Feng]{coa-vla}
Jinming Li, Yichen Zhu, Zhibin Tang, Junjie Wen, Minjie Zhu, Xiaoyu Liu, Chengmeng Li, Ran Cheng, Yaxin Peng, and Feifei Feng.
\newblock Improving vision-language-action models via chain-of-affordance.
\newblock \emph{arXiv preprint arXiv:2412.20451}, 2024{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Liu, Zhang, Yu, Xu, Wu, Cheang, Jing, Zhang, Liu, et~al.]{roboflamingo}
Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et~al.
\newblock Vision-language foundation models as effective robot imitators.
\newblock \emph{arXiv preprint arXiv:2311.01378}, 2023{\natexlab{b}}.

\bibitem[Lin et~al.(2024{\natexlab{a}})]{moe-llava}
Bin Lin et~al.
\newblock Moe-llava: Mixture of experts for large vision-language models.
\newblock \emph{arXiv preprint arXiv:2401.15947}, 2024{\natexlab{a}}.

\bibitem[Lin et~al.(2024{\natexlab{b}})Lin, Hu, Sheng, Wen, You, and Gao]{lin2024datascalinglawsimitation}
Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, and Yang Gao.
\newblock Data scaling laws in imitation learning for robotic manipulation, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee]{llava1.5}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock \emph{arXiv preprint arXiv:2310.03744}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee]{llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu, et~al.]{mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock \emph{arXiv preprint arXiv:2307.06281}, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(2024)Liu, Li, Huang, Yang, Yu, Li, Yin, Liu, Jin, and Bai]{Liu_2024}
Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai.
\newblock Ocrbench: on the hidden mystery of ocr in large multimodal models.
\newblock \emph{Science China Information Sciences}, 67\penalty0 (12), 2024.

\bibitem[Lu et~al.(2024)Lu, Liu, Zhang, Wang, Dong, Liu, Sun, Ren, Li, Yang, et~al.]{lu2024deepseek-vl}
Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et~al.
\newblock Deepseek-vl: towards real-world vision-language understanding.
\newblock \emph{arXiv preprint arXiv:2403.05525}, 2024.

\bibitem[Luo et~al.(2024)Luo, Yang, Dou, Wang, Dai, Qiao, and Zhu]{luo2024mono}
Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu.
\newblock Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training.
\newblock \emph{arXiv preprint arXiv:2410.08202}, 2024.

\bibitem[Luo et~al.(2023)Luo, Yang, Meng, Li, Zhou, and Zhang]{luo2023empirical}
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang.
\newblock An empirical study of catastrophic forgetting in large language models during continual fine-tuning.
\newblock \emph{arXiv preprint arXiv:2308.08747}, 2023.

\bibitem[Ma et~al.(2024)Ma, Liu, Chen, Liu, Wu, Wu, Pan, Xie, Zhang, Zhao, et~al.]{ma2024janusflow}
Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et~al.
\newblock Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation.
\newblock \emph{arXiv preprint arXiv:2411.07975}, 2024.

\bibitem[Masry et~al.(2022)Masry, Long, Tan, Joty, and Hoque]{masry-etal-2022-chartqa}
Ahmed Masry, Do Long, Jia~Qing Tan, Shafiq Joty, and Enamul Hoque.
\newblock {C}hart{QA}: A benchmark for question answering about charts with visual and logical reasoning.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2022}, pages 2263--2279, Dublin, Ireland, 2022. Association for Computational Linguistics.

\bibitem[Mathew et~al.(2021)Mathew, Karatzas, and Jawahar]{mathew2021docvqa}
Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.
\newblock Docvqa: A dataset for vqa on document images.
\newblock In \emph{Proceedings of the IEEE/CVF winter conference on applications of computer vision}, pages 2200--2209, 2021.

\bibitem[Mathew et~al.(2022)Mathew, Bagal, Tito, Karatzas, Valveny, and Jawahar]{9706887}
Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and C.~V. Jawahar.
\newblock Infographicvqa.
\newblock In \emph{2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, pages 2582--2591, 2022.

\bibitem[{Octo Model Team} et~al.(2024){Octo Model Team}, Ghosh, Walke, Pertsch, Black, Mees, Dasari, Hejna, Xu, Luo, Kreiman, Tan, Chen, Sanketi, Vuong, Xiao, Sadigh, Finn, and Levine]{octo}
{Octo Model Team}, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, {You Liang} Tan, Lawrence~Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine.
\newblock Octo: An open-source generalist robot policy.
\newblock In \emph{Proceedings of Robotics: Science and Systems}, Delft, Netherlands, 2024.

\bibitem[Paivio(1991)]{paivio1991dual}
Allan Paivio.
\newblock Dual coding theory: Retrospect and current status.
\newblock \emph{Canadian Journal of Psychology/Revue canadienne de psychologie}, 45\penalty0 (3):\penalty0 255, 1991.

\bibitem[Pertsch et~al.(2025)Pertsch, Stachowicz, Ichter, Driess, Nair, Vuong, Mees, Finn, and Levine]{pertsch2025fast}
Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine.
\newblock Fast: Efficient action tokenization for vision-language-action models.
\newblock \emph{arXiv preprint arXiv:2501.09747}, 2025.

\bibitem[Prasad et~al.(2024)Prasad, Lin, Wu, Zhou, and Bohg]{prasad2024consistencypolicy}
Aaditya Prasad, Kevin Lin, Jimmy Wu, Linqi Zhou, and Jeannette Bohg.
\newblock Consistency policy: Accelerated visuomotor policies via consistency distillation.
\newblock \emph{arXiv preprint arXiv:2405.07503}, 2024.

\bibitem[{RealWorld Team}(2024)]{RealWorldQA}
{RealWorld Team}.
\newblock {RealWorldQA: A Comprehensive Real-World Question Answering Dataset}, 2024.

\bibitem[Reuss et~al.(2024)Reuss, Ya{\u{g}}murlu, Wenzel, and Lioutikov]{multimodal_diffusion_transformer}
Moritz Reuss, {\"O}mer~Erdin{\c{c}} Ya{\u{g}}murlu, Fabian Wenzel, and Rudolf Lioutikov.
\newblock Multimodal diffusion transformer: Learning versatile behavior from multimodal goals.
\newblock \emph{Robotics: Science and Systems}, 2024.

\bibitem[Singh et~al.(2019{\natexlab{a}})Singh, Natarajan, et~al.]{textvqa}
Amanpreet Singh, Vivek Natarajan, et~al.
\newblock Towards vqa models that can read.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8317--8326, 2019{\natexlab{a}}.

\bibitem[Singh et~al.(2019{\natexlab{b}})Singh, Natarjan, Shah, Jiang, Chen, Batra, Parikh, and Rohrbach]{singh2019towards}
Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 8317--8326, 2019{\natexlab{b}}.

\bibitem[Tang et~al.(2024)Tang, Liu, Ye, Lu, Wei, Lin, Li, Mahmood, Feng, Zhao, et~al.]{tang2024mtvqa}
Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz~Bin Mahmood, Hao Feng, Zhen Zhao, et~al.
\newblock Mtvqa: Benchmarking multilingual text-centric visual question answering.
\newblock \emph{arXiv preprint arXiv:2405.11985}, 2024.

\bibitem[Uehara et~al.(2024{\natexlab{a}})Uehara, Zhao, Black, Hajiramezanali, Scalia, Diamant, Tseng, Biancalani, and Levine]{uehara2024fine}
Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel~Lee Diamant, Alex~M Tseng, Tommaso Biancalani, and Sergey Levine.
\newblock Fine-tuning of continuous-time diffusion models as entropy-regularized control.
\newblock \emph{arXiv preprint arXiv:2402.15194}, 2024{\natexlab{a}}.

\bibitem[Uehara et~al.(2024{\natexlab{b}})Uehara, Zhao, Black, Hajiramezanali, Scalia, Diamant, Tseng, Levine, and Biancalani]{uehara2024feedback}
Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel~Lee Diamant, Alex~M Tseng, Sergey Levine, and Tommaso Biancalani.
\newblock Feedback efficient online fine-tuning of diffusion models.
\newblock \emph{arXiv preprint arXiv:2402.16359}, 2024{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, Zhang, Jia, Li, Bao, Ma, Zhu, and Zhong]{wang2021afec}
Liyuan Wang, Mingtian Zhang, Zhongfan Jia, Qian Li, Chenglong Bao, Kaisheng Ma, Jun Zhu, and Yi Zhong.
\newblock Afec: Active forgetting of negative transfer in continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 22379--22391, 2021.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Bai, Tan, Wang, Fan, Bai, Chen, Liu, Wang, Ge, et~al.]{wang2024qwen2}
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et~al.
\newblock Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.
\newblock \emph{arXiv preprint arXiv:2409.12191}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Zhang, Huo, Tian, Zhang, Xie, Xu, Ji, Zhan, Ding, et~al.]{wang2024sparse-dp}
Yixiao Wang, Yifei Zhang, Mingxiao Huo, Ran Tian, Xiang Zhang, Yichen Xie, Chenfeng Xu, Pengliang Ji, Wei Zhan, Mingyu Ding, et~al.
\newblock Sparse diffusion policy: A sparse, reusable, and flexible policy for robot learning.
\newblock \emph{arXiv preprint arXiv:2407.01531}, 2024{\natexlab{b}}.

\bibitem[Wen et~al.(2024{\natexlab{a}})Wen, Zhu, Zhu, Tang, Li, Li, Zhou, Liu, Shen, Peng, and Feng]{diffusionvla}
Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Chengmeng Li, Zhongyi Zhou, Xiaoyu Liu, Chaomin Shen, Yaxin Peng, and Feifei Feng.
\newblock Diffusionvla: Scaling robot foundation models via unified diffusion and autoregression.
\newblock 2024{\natexlab{a}}.

\bibitem[Wen et~al.(2024{\natexlab{b}})Wen, Zhu, Zhu, Tang, Li, Zhou, Li, Liu, Peng, Shen, et~al.]{wen2024diffusionvla}
Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, et~al.
\newblock Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression.
\newblock \emph{arXiv preprint arXiv:2412.03293}, 2024{\natexlab{b}}.

\bibitem[Wen et~al.(2024{\natexlab{c}})Wen, Zhu, Li, Zhu, Wu, Xu, Liu, Cheng, Shen, Peng, et~al.]{wen2024tinyvla}
Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, et~al.
\newblock Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation.
\newblock \emph{arXiv preprint arXiv:2409.12514}, 2024{\natexlab{c}}.

\bibitem[Wen et~al.(2025)Wen, Zhu, Li, Tang, Shen, and Feng]{wen2025dexvla}
Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng.
\newblock Dexvla: Vision-language model with plug-in diffusion expert for general robot control.
\newblock \emph{arXiv preprint arXiv:2502.05855}, 2025.

\bibitem[Wu et~al.(2024)Wu, Chen, Wu, Ma, Liu, Pan, Liu, Xie, Yu, Ruan, et~al.]{wu2024janus}
Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et~al.
\newblock Janus: Decoupling visual encoding for unified multimodal understanding and generation.
\newblock \emph{arXiv preprint arXiv:2410.13848}, 2024.

\bibitem[Yue et~al.(2024)Yue, Ni, Zhang, Zheng, Liu, Zhang, Stevens, Jiang, Ren, Sun, Wei, Yu, Yuan, Sun, Yin, Zheng, Yang, Liu, Huang, Sun, Su, and Chen]{yue2023mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.
\newblock Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.
\newblock In \emph{Proceedings of CVPR}, 2024.

\bibitem[Zawalski et~al.(2024)Zawalski, Chen, Pertsch, Mees, Finn, and Levine]{ecot}
Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine.
\newblock Robotic control via embodied chain-of-thought reasoning.
\newblock \emph{arXiv preprint arXiv:2407.08693}, 2024.

\bibitem[Zhai et~al.(2023)Zhai, Tong, Li, Cai, Qu, Lee, and Ma]{zhai2023investigating}
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong~Jae Lee, and Yi Ma.
\newblock Investigating the catastrophic forgetting in multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2309.10313}, 2023.

\bibitem[Zhao et~al.(2024)Zhao, Tompson, Driess, Florence, Ghasemipour, Finn, and Wahid]{aloha_unleashed}
Tony~Z Zhao, Jonathan Tompson, Danny Driess, Pete Florence, Seyed Kamyar~Seyed Ghasemipour, Chelsea Finn, and Ayzaan Wahid.
\newblock Aloha unleashed: A simple recipe for robot dexterity.
\newblock In \emph{8th Annual Conference on Robot Learning}, 2024.

\bibitem[Zheng et~al.(2025)Zheng, Cai, Qiu, and Ma]{zheng2025spurious}
Junhao Zheng, Xidi Cai, Shengjie Qiu, and Qianli Ma.
\newblock Spurious forgetting in continual learning of language models.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025.

\bibitem[Zhou et~al.(2024)Zhou, Yu, Babu, Tirumala, Yasunaga, Shamis, Kahn, Ma, Zettlemoyer, and Levy]{zhou2024transfusion}
Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy.
\newblock Transfusion: Predict the next token and diffuse images with one multi-modal model.
\newblock \emph{arXiv preprint arXiv:2408.11039}, 2024.

\bibitem[Zhu et~al.(2024{\natexlab{a}})Zhu, Chen, Shen, Li, and Elhoseiny]{minigpt4}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Mini{GPT}-4: Enhancing vision-language understanding with advanced large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{a}}.

\bibitem[Zhu et~al.(2024{\natexlab{b}})Zhu, Zhu, Li, Wen, Xu, Liu, Cheng, Shen, Peng, Feng, et~al.]{zhu2024scalingdp}
Minjie Zhu, Yichen Zhu, Jinming Li, Junjie Wen, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, et~al.
\newblock Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation.
\newblock \emph{arXiv preprint arXiv:2409.14411}, 2024{\natexlab{b}}.

\bibitem[Zhu et~al.(2024{\natexlab{c}})Zhu, Zhu, Liu, Liu, Xu, Shen, Peng, Ou, Feng, and Tang]{zhu2024comprehensive}
Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, and Jian Tang.
\newblock A comprehensive overhaul of multimodal assistant with small language models.
\newblock \emph{arXiv preprint arXiv:2403.06199}, 2024{\natexlab{c}}.

\bibitem[Zhu et~al.(2024{\natexlab{d}})Zhu, Zhu, Liu, Ou, Mou, and Tang]{llavaphi}
Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang.
\newblock Llava-phi: Efficient multi-modal assistant with small language model.
\newblock \emph{arXiv preprint arXiv:2401.02330}, 2024{\natexlab{d}}.

\end{thebibliography}

    % \bibliography{main}
}

\input{X_supplementary}

\end{document}
