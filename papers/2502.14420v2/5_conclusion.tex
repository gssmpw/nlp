\section{Conclusion}
Integrating embodied control and multimodal understanding in Vision-Language-Action (VLA) models is challenging, as current methods often compromise one for the other. We identified key limitations: robot-only training degrades conversational ability, while visual-text co-training diminishes control performance due to spurious forgetting and task interference. To address this, we introduce ChatVLA, a unified framework combining Phased Alignment Training (prioritizing control before multimodal linking) and a Mixture-of-Experts architecture. ChatVLA achieves competitive VQA and general understanding performance while excelling at real-world robot control (25 tasks across diverse scenes), outperforming OpenVLA and ECoT with 3.5x fewer parameters. These results demonstrate that a single network can effectively harmonize multimodal reasoning, conversation, and physical interaction.