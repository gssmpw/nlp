The success of Transformer in speech enhancement mainly stems from the ability of self attention to model the long-range context of speech effectively. However, they can not characterise the salient T-F speech distribution. Although study [39] perform self attention along time axis and frequency axis, the mechanism of self attention determines that they can not characterise the salient T-F speech distribution. Thus our TFA work is motivated by this.

They just perform self attention on both time and frequency axis, so they called it as TFA. In fact, they are quiet different. 

In addition, we focus on a lightweight flexible module, whereas the study [39] just perform self attention along two dimensions. It will never be a lightweight module.