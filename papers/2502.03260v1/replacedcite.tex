\section{Related Work}
\label{sec2}

\textcolor{black}{In Figure~\ref{fig:sr_overview}, we briefly outline audio/speech representation methods, including hand-crafted fixed filterbanks, lightweight learnable front-ends (not adaptive), our dynamically adaptive front-end (Ada-FE), and audio pre-training methods.}

\subsection{\textcolor{black}{Lightweight Learnable Front-ends}} 
There have been many studies on learnable front-ends as drop-in replacements of fixed Mel-filterbanks. Some learn front-ends from low-level spectral features such as STFT. For instance, Sainath~\textit{et al.}____ take the power spectral feature as the input and explore using a filterbank layer to learn the band-pass filters of Mel-filterbanks. The filterbank layer is initialized with the weights of the Mel-filters and optimized jointly with the rest of the network. Seki~\textit{et al.}____ exploit the parameterized Gaussian filters to learn the band-pass filterbanks from the power spectrogram. Won~\textit{et al.}____ propose parameterized harmonic band-pass filterbanks for learnable front-end that preserves spectro-temporal locality with harmonic structures, where harmonic filters are learned from the STFT spectrogram.


% \begin{figure*}[!ht]
%  \centering
%   \includegraphics[width=0.90\linewidth]{AdaFEV2.pdf}
%   % \vspace{-0.5em}
%     \caption{Illustrations of (a) the overview diagram of the adaptive front-end (Ada-FE) and (b) the simplified adaptive front-end (Ada-FE-S), where the fixed level-dependent adaptation function module is removed and the adaptive Q value is completely tuned by the adaptive feedback controller.}
%   % \caption{Illustrations of (a) The overview diagram of the adaptive front-end (adopted from ____), (b) The energy-based level-dependent adaptation, and (c) the adaptive feedback controller.}
%   \label{fig2}
%   \vspace{-1.5em}
% \end{figure*}

% \begin{figure*}[!ht]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.91\linewidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{./AdaFE.pdf}}
% % \vspace{-0.5em}
% \caption{The adaptive front-end (Ada-FE) is optimized jointly with the back-end classifier.}
% \label{fig2:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.91\linewidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{./AdaFES.pdf}}
% % \vspace{-0.5em}
% \caption{The simplified adaptive front-end (Ada-FE-S) is optimized jointly with the back-end classifier.}
% \label{fig2:2}
% \end{subfigure}
% \caption{\textcolor{black}{Illustrations of (a) the overall diagram of the adaptive front-end (Ada-FE)____ and (b) the simplified adaptive front-end (Ada-FE-S), where the hand-crafted level-dependent adaptation function module is removed and the adaptive Q value is completely controlled by the neural adaptive feedback controller (blue box).}}
% % \vspace{-0.8em}
% \label{fig2}
% \end{figure*}

% (adopted from ____)

Learning representations directly from the raw waveforms in an end-to-end manner has attracted great research interest, which avoids the design of hyper-parameters (such as frame length, hop length, and the typology of the window function) used to extract the appropriate low-level spectrum. In an early attempt ____, Jailty and Hinton proposed pre-training a generative restricted Boltzmann machine (RBM) to learn features from raw speech signals and show that the learned features yield promising performance in phoneme recognition on the TIMIT benchmark. Later, Palaz \textit{et al.}____ proposed using a convolutional neural network (CNN) to learn representation from raw waveforms for phoneme recognition and suggest that the first layer tends to learn a set of band-pass filters. Similarly, Sainath~\textit{et al.}____ and Hoshen \textit{et al.}____ employed CNNs for feature learning from single- and multi-channel raw waveforms respectively, where the CNN filters are initialized as Gammaton-filters and trained jointly with the deep classifier. In____, a convolutional long short-term memory deep neural network (CLDNN) is trained on raw waveforms for spoofing speech detection. Along this line of research, Zeghidour \textit{at al.}____ proposed learnable time-domain filterbanks (TD-fbanks) for phone recognition as an approximation to Mel-filterbanks.

% and fine-tuned with the remaining CNN without constraints.

Parameterized neural filters have been explored to facilitate CNNs to discover meaningful filters. Ravanelli and Bengio____ propose the SincNet layer for speaker recognition, comprised of a set of parameterized Sinc filters that approximate band-pass filters, where the low and high cut-off frequencies are the only parameters learned from data. The recent LEAF____ employs two parameterized Gabor filter layers to build a learnable front-end, with learnable filtering, pooling, and compression, demonstrating impressive results in a broad range of audio and speech tasks. The very recent works____ demonstrate that LEAF lacks learning as there is no substantial movement between initial filters and learned filters.

The Gabor filter, in contrast to the Sinc filter, shows characteristics similar to those of auditory filters and is optimally localized in time and frequency. This motivates us to explore parameterized Gabor filters for our adaptive front-end, where our Ada-FE learns the Q-factor to tune the filters instead of the center frequency and bandwidth used in LEAF. Additionally, unlike all prior learnable front-ends that perform feature extraction with fixed filter weights at inference time, to our knowledge, Ada-FE is the only front-end that can dynamically tune the filters frame by frame to perform adaptive inference with respect to the input.

% \textcolor{blue}{More recently, studies____ show that LEAF does not learn as it shows no substantial difference between learned and initialized filters, except the compression layer.} 

\subsection{\textcolor{black}{Audio Pre-training Methods}} 
\textcolor{black}{\textcolor{black}{As a parallel line of research, audio pre-training has recently fostered prominent success in audio representation learning, which involves supervised pre-training and self-supervised pre-training____}. These methods typically pre-train a large neural network model on a great amount of external data to extract high-level feature representations from spectral features (e.g., AST____, SSAST____, and BYOL-A____) or raw audio waveforms (e.g., HuBERT____, WavLM____, and Wav2vec2____).} In contrast, lightweight learnable audio front-ends only perform low-level spectral feature learning (spectral decomposition) with a small number of trainable parameters. A recent study____ suggests that \textcolor{black}{lightweight learnable front-ends (i.e., SincNet and LEAF) could benefit self-supervised audio pre-training. \textcolor{black}{These pre-trained models are learnable but not adaptive.}} 


% \textcolor{black}{In parallel, representation learning based on pre-training has enabled the recent progress in audio processing, where a large DNN is pre-trained on massive external data to extract high-level representation from raw waveforms (e.g., Wav2vec2____ and WavLM____) or spectrograms (e.g., AST____ and BYOL-A____). It is unlike the lightweight learnable front-ends that only involve spectral decomposition and low-level spectral feature learning, with fewer parameters.}

% (as well as pre-training models

% Unlike LEAF which learns center frequency and bandwidth to tune the filters, our proposed method only uses the Q-factor to tune the filters.
% Furthermore, Gabor filter provides a straightforward way to dynamically tune the filter weights via Q-factor

% \begin{figure}[!htbp]
%  \centering
%   \includegraphics[width=0.57\linewidth]{AdaptiveQ.pdf}
%   % \vspace{-0.5em}
%   \caption{Illustrations of how Q values are adjusted in frame-by-frame fashion in Ada-FE.}
%   \label{fig_struc}
%   % \vspace{-1.5em}
% \end{figure}


% \IEEEPARstart{S}{peech} signals in a real-world acoustic environment are inevitably corrupted by background noise, which can severely degrade speech quality and intelligibility. Speech enhancement seeks to separate the target speech signal from the background noise. It is an essential component in a number of speech processing systems, such as hearing aids, automatic speech recognition (ASR), speaker verification, and the brain-computer interface. Monaural speech enhancement represents one of the challenges. Traditional signal processing-based methods have been extensively studied for a long time, mainly including spectral subtraction ____, Wiener filtering ____, and statistical model-based methods ____. These methods perform well for stationary noise, however, fail to handle non-stationary noise. 

% With the advent of deep learning, speech enhancement has made remarkable progress____. Techniques can be grouped into time-frequency (T-F) domain methods and time-domain methods, according to the way input signals are handled. Specifically, time-domain methods perform speech enhancement directly on the raw waveform domain, where a deep neural network (DNN) is optimized to learn the mapping from the noisy raw waveform to the clean one ____ \textcolor{black}{via some latent feature representation. T-F domain methods typically transform the noisy raw waveform into a T-F representation or spectrogram first before mapping to the clean one with well-designed training objectives.} The most popular T-F domain training objectives include ideal ratio mask (IRM) ____, spectral magnitude mask (SMM) ____, complex IRM (cIRM) ____, phase-sensitive mask (PSM) ____, target magnitude spectrum ____, and log-power spectrum ____. 
% More recently, the instantaneous \textit{a priori} signal-to-noise ratio (SNR), termed Xi, is proposed as a \textcolor{black}{training objective} to bridge the gap between deep learning and traditional statistical model-based methods ____. In this study, we adopt a novel T-F domain method, which allows for intuitive time-frequency analysis. We also adopt the three most widely used \textcolor{black}{training objectives}, i.e., IRM, SMM, PSM and a recent one (Xi) in the experiments.

%\textcolor{blue}{Although the difficult phase estimation problem is avoided, time-domain methods also give up the benefit of easily distinguishable speech and noise patterns on a T-F spectrogram. In this study, we focus on T-F domain method and adopt three most widely used training objectives, i.e., IRM, SMM, PSM, and a recent one (Xi) in the experiments. }
%\textcolor{red}{Could this paragraph explain that what we choose T-F method first?}}
%\textcolor{blue}{Or just need to say: In this study, we focus on T-F domain method and adopt three most widely used training objectives, i.e., IRM, SMM, PSM, and a recent one (Xi) in the experiments.

%\textcolor{red}{we need to justify what we choose T-F method first (time-domain vs T-F), then explain why IRM, SMM, PSM and Xi are used. (predicting mask directly vs indirectly via spectrum targets.) masks are  actually not better than magnitude or spectrum targets when optimizing network. }

%\textcolor{blue}{As many masks are proposed for speech enhancement, we select four of them in order to perform extensive experiments to demonstrate our method is solid. In some studies, masking methods show better performance, in some other studies, magnitude mapping may be better. I think there is no a accurate definition about the superiority mask over magnitude.} \textcolor{red}{i am not comparing magnitude mapping vs masking. I am comparing two ways to learn the masks. One is to use the masks as the target (objective) directly, another is to use the output features as the target (objective) while optimizing the masks as an intermediate layer. The second one is always better.}

%\textcolor{blue}{Thank you Prof. Li. Now I understand what you said, the first one is mask approximation, the second one is the signal approximation (the mask is used as an intermediate). In fact both of thees two methods are widely used in existing methods and following the previous works (conference paper), in this study we use the first one in our experiment. I think that we can choose anyone to reach the same comparison results or conclusion. We just need to keep all models using the same objective loss function and describe clearly the used objective function in experiment setup part.}

 % Time-domain methods ____ optimize a DNN directly  recover the raw waveform of clean speech from noisy raw waveform, and are not able to utilize the benefit that the harmonic structure of speech and noise pattern are easily distinguishable in T-F spectrogram.
% Although time domain methods achieve the state-of-the-art performance in speaker separation task, the superiority is not obvious in speech enhancement filed. Taking the speech quality, robustness, and computational efficiency into consideration, the T-F domain methods remain the most attractive solution today.

% Advanced speech enhancement algorithms depend on a strong backbone network architecture. Multi-layer perceptrons (MLPs) are the most widely adopted backbone network architecture in early studies. Furthering the idea of T-F masking in the computational auditory scene analysis, Wang \textit{et al.} ____ proposed to employ an MLP to predict the ideal binary mask (IBM) ____ to separate the speech from background noise. Subsequently, Xu \textit{et al.} ____ adopted an MLP as a regression function to learn the mapping from the noisy log-power spectra to the clean one. 
% In ____, Chen \textit{et al.} formulated speech enhancement as a sequence-to-sequence mapping, that effectively addresses speaker generalization issue, and employed a recurrent neural network (RNN) with four long short-term memory (LSTM) layers to model the long-range contextual information of the speech. The LSTM-RNN model demonstrates substantial performance improvement over MLPs____. However, deep LSTM-RNN network architectures involve a large number of parameters, which significantly limits its scope of applications. 

% Deep convolutional neural networks (CNNs) represent another successful backbone network architecture. Unlike RNN that involves a sequential process, CNN  performs the filter processing on speech frames in parallel. Meanwhile, CNN captures the contextual information by stacking multiple layers. Recently, the residual temporal convolution networks \textcolor{black}{(ResTCNs)} ____, which employ 1-D dilated convolutional modules and residual skip connections, have demonstrated impressive performance in modeling long-term dependencies and outperformed RNN across a broad range of sequence modeling tasks. \textcolor{black}{ResTCNs} have gained considerable success in speech enhancement ____ and speaker separation ____ as well. Self-attention based \textcolor{black}{Transformer} backbone network ____ has achieved state-of-the-art performance on many natural language processing tasks. More recently, Transformers have been successfully adopted for speech enhancement ____ and many other speech processing-related tasks such as speech synthesis and voice conversion. As the key component of \textcolor{black}{Transformers}, the multi-head self-attention \textcolor{black}{(MHA)} mechanism processes the whole sequence at once and computes the similarity between all time-steps to obtain the new representation, allowing the \textcolor{black}{Transformer} model for modeling long-term dependencies more efficiently. In ____ Zhao \textit{et al.} proposed to employ a \textcolor{black}{MHA} module to produce dynamic representations followed by a \textcolor{black}{ResTCN} model to learn a nonlinear mapping for speech dereverberation.

% The generation of human speech is subject to the constraint of the physiological structure of vocal production, and the phonetic and phonotactic rules of a spoken language. The success of \textcolor{black}{ResTCN} and \textcolor{black}{Transformer} in speech enhancement mainly stems from their ability to effectively model the long-range temporal context of speech. 
% %The spectral density and the time-frequency (T-F) joint energy distribution of speech signals are equally informative for speech perception. However, the T-F representation of speech is not adequately exploited by existing methods for speech enhancement.
% We also note that the energy concentration of a speech utterance in time or frequency varies from utterance to utterance. To preserve such a speech formant structure, we expect that a speech enhancement model performs according to the energy concentration in the T-F plane of a spectrogram. We are motivated to investigate a dedicated mechanism to characterise the salient T-F speech distribution.

%  (We are motivated to investigate a different aspect of the architecture design, T-F attention, for modeling the T-F distribution characteristic of speech.)
 
% The idea of attention has been well studied for the network to learn to attend to the salient features in computer vision ____, speech emotion recognition ____, and speaker verification ____.  In a preliminary study ____, we investigated an attention mechanism to model the speech distribution along the frequency dimension and demonstrate its efficacy. We proposed a functional neural module, termed T-F attention \textcolor{black}{(TFA)}, as part of the backbone networks to attend to the salient T-F representation for speech enhancement ____. The proposed \textcolor{black}{TFA module} consists of two parallel attention branches, i.e., time-dimension \textcolor{black}{(TA)} and frequency-dimension attention \textcolor{black}{(FA)} that produce two 1-D attention maps to guide the models to focus on `where' (which time frames) and `what' (which frequency-wise channels), respectively. Then the \textcolor{black}{TA} and \textcolor{black}{FA} branches are combined to generate the final 2-D attention map, which assigns differentiated attention weights for each T-F spectral component, allowing the networks to capture the speech distribution in the T-F representation. In this paper, we further study the \textcolor{black}{TFA module}____ across different backbone network architectures and \textcolor{black}{training objectives}, and evaluates its efficacy for a robust ASR system.

% We evaluate the proposed TFA module as part of two widely used backbone networks, i.e., ResTCN and Transformer, in both speech enhancement and speech recognition experiments.}

%\textcolor{red}{There have been studies on T-F attention____ in speech enhancement, which attempts to capture the T-F contextual correlations____ by applying self-attention along the T-F dimensions. In those studies, the attention mechanisms were optimized directly for enhanced speech outputs. In this paper, we propose a novel T-F attention module that is optimized for T-F mask-based training objectives. The proposed T-F attention module is different from that in____ both in terms of motivation and actual implementation.} 

% \textcolor{black}{There have been attempts to capture the long-range correlations in the T-F representation by applying self-attention operation along the time and frequency axes____, which was referred to as T-F attention____. In this paper, we use T-F attention to refer to a dedicated mechanism that especially models the salient T-F speech distribution of speech signals. Such a T-F attention module can be used to augment existing neural speech enhancement solutions. Therefore, it is different from that in____ either in terms of motivation or implementation.}

% \textcolor{black}{There have been attempts to capture long-range correlations in the T-F representation by applying self-attention operation along the time and frequency axes____, which was referred to as T-F attention____. In this paper, \textcolor{black}{T-F attention (TFA) refers to a dedicated mechanism different from self-attention. It models the salient T-F speech distribution of speech signals. In particular,} the T-F attention____ is based on self-attention, and the learned attention scores represent the similarity among T-F vectors. However, the differentiated attention weights learned by our TFA represent how informative each T-F spectral component is. Such a T-F attention module can be used to augment existing neural speech enhancement solutions including self-attention. Therefore, it is different from that in____ either in terms of motivation or implementation.}
% \textcolor{black}{The main contributions of this work are as follows:
% \begin{itemize}
%   \setlength\itemsep{0.5pt}
%   \item We propose a simple yet very effective network module (TFA) to characterise the salient T-F distribution for speech enhancement. It can be flexibly integrated with existing backbone networks to improve performance.
%   \item We design time-dimension (TA) and frequency-dimension (FA) attention to enable the models to focus on informative frames and frequency-wise channels, respectively. Comprehensive ablation studies validate the efficacy.
%   \item We extensively evaluate the TFA module across different backbone networks and training objectives. The results confirm that our TFA module consistently provides significant performance gains in speech enhancement, as well as in robust speech recognition.
% \end{itemize}}


%\textcolor{red}{to qiquan: we need to find a way to explain the difference. I don't agree with your arguments. I think that our ResTCN and MHANet both handle contextual information just like any Transformer model [41]. They also selectively pay attention to T-F bins, we need to explain why they have not done enough to motivate the dedicated TFA we propose. I understand perfectly that we do NOT do self-attention, the ResTCN/MHANet does, and we only do TF bin masking in this paper. But we cannot say that the Transformer-based model does not select TF bins. they do. Our paper will go to the reviewers who are good at Transformer-based research. We will get negative review if we say that. however, we can nicely explain why the transformer-based system is not good at doing this. By showing better results cannot be the proof because we use more parameters in an additional module. You know that in NN, when you add more parameters, you gain performance most of the time.}

%\textcolor{blue}{I think that we do not need to describe the differences of training objectives. We just need to say that they are different, as self-attention only model the correlations although it is extended to frequency axis. We focus on generating a 2-D attention map to locate informative T-F component, which is completely unrelated to self-attention mechanism.} \textcolor{red}{in [39], actually they also locate the informative T-F components. I have read [39] multiple times and conclude that the difference is only the training objectives. the technique in [39] can also be used for our task.}

% \textcolor{blue}{I will send [39-41] to you for checking.} \textcolor{blue}{Dear Prof. Li, First, our paper is quite different from study [39] with respects to both idea and motivation. Study [39] is still focus on the dependencies (or correlation) and simply use self attention operation on frequency axis and temporal axis sequentially, instead of focus on which T-F component is informative. The mechanism is quite different. Here, the reason why I describe this study [39] is that they also use same term T-F attention (the name is same as ours), and I want to describe clearly to avoid the misunderstanding. In fact, I argue that It is quite OK that we do not describe the study [39] here.}
%the spectral components in the T-F representation, which is different from the T-F attention in____ both in terms of motivation and actual implementation.


%\textcolor{red}{we should cite TF attention papers for speech recognition here. There are many. you can find some references from this paper --- Lili Guo, Longbiao Wang*, Chenglin Xu, Jianwu Dang*, Eng Siong Chng and Haizhou Li, "Representation Learning with Spectro-Temporal-Channel Attention for Speech Emotion Recognition," Proc. of ICASSP 2021: 6304-6308 -- Recently, attention mechanisms ____ have been extensively studied in computer vision, and achieved substantial performance improvement.} \textcolor{blue}{I cite a T-F attention paper on speech enhancement. I did not cite it before as it is quiet different from our idea (just same name). I already cite the papers about T-F attention in speech emotion attention and speaker verification.}

%Our proposed TFA module is a lightweight and plug-and-play module that can be easily integrated into existing architectures.

%Considering the distribution characteristic of speech energy in T-F domain and inspired by the attention in computer vision, 

% We propose a time-frequency attention (TFA) module to guide model `where' (which time segments) and `what' (which spectral components in frequency dimension) to attend and also increase the representation capability of the model. The TFA is a lightweight and plug-play module that can be easily integrated into existing architectures, which consists of two components, time-dimension attention (TA) and frequency-dimension attention (FA). TA and FA learn to generate differentiated weights for the time-frequency bins according to the training objectives, and assign the differentiated weights dynamically during run-time inference. TFA combines TA and FA by a matrix multiplication operation to generate the T-F attention weights to each T-F spectral component. We evaluate the proposed TFA module as part of two widely used backbone networks, i.e., ResTCN and Transformer, in both speech enhancement and speech recognition experiments. 
% objectives. In addition, we further investigate the effect of TFA module on downstream ASR task. It is observed that the introduce of the TFA module demonstrates significant performance improvements to backbone networks with negligible parameter and computational overheads.

% The remainder of this paper is organized as follows. In Section \ref{sec:2}, we formulate the research problem. In Section \ref{sec:3}, we propose a novel time-frequency attention mechanism for speech enhancement. In Section \ref{sec:4}, we describe the experimental setup. The experimental results are presented in Section \ref{sec:5}. Finally, Section \ref{sec:6} concludes this study.

% \textcolor{blue}{\textbf{Explain the differences with study ____}}
% \begin{figure}[t]
% \vskip -0.1in
% \begin{center}
% \centerline{\includegraphics[width=1\columnwidth]{diagram/TFA_2020.png}}
% \caption{}
% \label{fig1}
% \end{center}
% \vskip -0.2in
% \end{figure}

%\textcolor{blue}{Dear Prof. Li, I explain that study ____ is quite different from ours. Here, we first gives the motivation in original paper ____ ``\textbf{For speech processing tasks, the contextual information plays a key role. To leverage such information, a general way is to employ an attention mechanism. Note that the contextual information is not limited to the time dimension. For speech, the frequency dimension should be also taken into account.}''}

%\textcolor{blue}{1./ From the motivation in original paper, we can find that the TFA attention in ____ is still focus on modeling long-range context information, and they just extend self attention to frequency axis. \textcolor{red}{to qiquan: in [41], the attention is also over frequency axis. why  you say that they did not? self-attention is a mechanism to learn context information, our multi-layer network expands the receptive field to cover long-range context information as well. they are just different way of doing the same thing.}\\
%\textbf{\textcolor{blue}{to Prof. Li, yeah, they indeed employ self attention along frequency axis. However, self-attention just computes the similarity between all frequency bands/channels to model the correlations along frequency dimension. What I mean is that the mechanism of self-attention does not allow model to locate which T-F feature is informative, which is what we do in this study. \textcolor{red}{if the self-attention mechanism is applied with our mask-based objective function, the self-attention can also learn to focus on informative T-F features. This is not the problem of self-attention.}They just first capture long-range correlations of time frames then capture the long range correlations of frequency bands. Our TFA employ a time attention to locate the informative speech frames and a frequency-channel attention to locate informative frequency channel. In other words, out attention focuses on \textcolor{red}{Locating informative T-F component} while self-attention focuses on \textcolor{red}{Capturing correlation}. The two ideas are completely different or unrelated.}}} \textcolor{red}{in [41], they use contextual correlation information to locate the informative T-F components. because [41] optimizes directly for a target reference, the masking results are not easily explainable. But we cannot say that [41] doesn't focus on locating T-F components. They do!}

%\textcolor{blue}{2./ From the diagram of TFA attention, we can find that it is based on the self-attention (Q, K, and V) attention mechanism, which can only model the long-range correlation but can not tell model to focus on which informative T-F
%components. For the above point, we already explain it in introduction part, which is also the reason why we propose our TFA attention.\\
%From above two points, it is clearly that our work is quite different (has no relation) from study ____. \textcolor{red}{in [39], optimization is done using the target speech as the reference target, while we use `mask' as the target. this is different. The former usually gives better results for domain specific data, the latter works better for cross-domain applications. We follow the latter. But this is not a big novelty because both techniques were used many times in the past. }\\
%\textcolor{blue}{To Prof. Li: Our novelty is not the T-F masks. The mechanism of our work is quiet from self-attention. In introduction part, we introduce that the success of Transformer in speech enhancement mainly stems from the ability of self attention to model the long-range context of speech effectively. However, they can not characterise the salient T-F speech distribution. Although study ____ perform self attention along time axis and frequency axis, the mechanism of self attention determines that they can not characterise the salient T-F speech distribution. Thus our TFA work is motivated by this. They just perform self attention on both time and frequency axis, so they called it as TFA. In fact, they are quiet different.}\\
%3. In addition, we focus on a lightweight flexible module, whereas the study ____ just perform self attention along two dimensions. It will never be a lightweight module. \textcolor{red}{the lightweight was introduced in [39] (see section 2.3 in [39], we don't want to claim novelty in our paper - not to invite criticism. please revise to reduce such claims. but rather cite [39] at the appropriate place.}}\\

%\textcolor{blue}{Dear Prof. Li, I have revised them and cite study ____ at the appropriate place.}

%\textcolor{blue}{\textbf{According above analysis, we can easily find that our work is unrelated to ____ with respect to motivation, implementation and any others.}}


% /* demo file is intended to serve as a ``starter file''
% for IEEE journal papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8b and later.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
% I wish you the best of success./*

% \hfill mds

% \hfill August 26, 2015

% \subsection{Subsection Heading Here}
% Subsection text here.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

% \subsubsection{Subsubsection Heading Here}
% Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/sconferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.