
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}



\usepackage[colorlinks, citecolor=blue]{hyperref}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
\usepackage[table]{xcolor}
\usepackage{url,hyperref,booktabs}

% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
\usepackage{pifont}
\usepackage{makecell}






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
  \usepackage{epstopdf}
%   \usepackage{subfigure}
  \usepackage{subcaption}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{amssymb}

% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath

\usepackage{multirow}
% \usepackage{textcase}
% \usepackage[tablename=TABLE]{caption}
\usepackage[switch]{lineno}
% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
% \ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
% \else
%  \usepackage[caption=false,font=footnotesize]{subfig}
% \fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
% \fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )
\usepackage[symbol]{footmisc}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\makeatletter
\def\endthebibliography{%
  \def\@noitemerr{\@latex@warning{Empty `thebibliography' environment}}%
  \endlist
}
\makeatother

\begin{document}
% \linenumbers
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
% \title{A Time-Frequency Attention Module for Neural Speech Enhancement}
% \title{Ada-FE: A Bio-Inspired Adaptive Front-End With Dynamic Neural Control for Audio Processing}
\title{\textcolor{black}{Should Audio Front-ends be Adaptive? \textcolor{black}{Comparing} Learnable and Adaptive Front-ends}}
%Ada-FE: An Adaptive Front-End With Neural Feedback Control for Audio Processing}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
% Programmatic Grant No. A18A2b0046 from the Singapore Government’s Research, Innovation and Enterprise 2020 plan (Advanced Manufacturing and Engineering domain)
\author{Qiquan~Zhang,~\IEEEmembership{Member,~IEEE,}
        Buddhi Wickramasinghe, %~\IEEEmembership{Member,~IEEE,}
        Eliathamby Ambikairajah,~\IEEEmembership{Life Senior Member,~IEEE,}
        Vidhyasaharan Sethu,~\IEEEmembership{Member,~IEEE,}
        % Eliathamby Ambikairajah,~\IEEEmembership{Senior Member,~IEEE,} 
        and Haizhou Li,~\IEEEmembership{Fellow,~IEEE}
\thanks{
Manuscript received date; revised date. 
% This work was supported in part by ARC Discovery Grant DP1900102479, in part by the Research Foundation of Guangdong Province under Grant 2019A050505001, in part by the Internal Project of Shenzhen Research Institute of Big Data under Grant T00120220002, and in part by the Guangdong Provincial Key Laboratory of Big Data Computing under the Grant B10120210117-KP02. 
% in part by University Development Fund under Grants UDF01002333 and UF02002333, {in part by the Research Foundation of Guangdong Province under Grant 2019A050505001}, in part by the Science and Engineering Research Council, Agency for Science, Technology and Research (A*STAR), Singapore, through the National Robotics Program under Human-Robot Interaction Phase 1 under Grant 1922500054, and in part by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy (University Allowance, EXC 2077, University of Bremen). \textit{(Corresponding Author: Xinyuan Qian)}.
}
\thanks{Qiquan Zhang, Eliathamby Ambikairaiah, and Vidhyasaharan Sethu are with the School of Electrical Engineering and Telecommunications, The University of New South Wales, Sydney, 2052, Australia~(e-mail: {qiquan.zhang}@unsw.edu.au; {e.ambikairajah}@unsw.edu.au).}
\thanks{Buddhi Wickramasinghe is with the School of Electrical and Computer Engineering, Purdue University, West Lafayette IN, USA~(e-mail: {wwickram}@purdue.edu).}
% \thanks{Qiquan Zhang is also with the Department of Electrical and Computer Engineering, National University of Singapore, 119077, Singapore.}
% \thanks{Xinyuan Qian is with the School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083, China, and also with The Chinese University of Hong Kong, Shenzhen 518172, China (e-mail: qianxy@ustb.edu.cn)}
% \thanks{Zhaoheng Ni is with Meta AI, New York, 10003, United States of America (e-mail: {zni}@fb.com).}
% \thanks{Aaron Nicolson is with Australian e-Health Research Centre, CSIRO, Australia (e-mail: {aaron.nicolson}@csiro.au).}
% \thanks{Eliathamby Ambikairaiah and Vidhyasaharan Sethu are with the School of Electrical Engineering and Telecommunications at the University of New South Wales, Sydney, 2052, Australia (e-mail: {e.ambikairajah}@unsw.edu.au).}
\thanks{Haizhou Li is with the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong (Shenzhen), 518172 China, and also with Shenzhen Research Institute of Big data, Shenzhen, 51872 China (e-mail: {haizhouli}@cuhk.edu.cn). 
% also with the Department of Electrical and Computer Engineering, National University of Singapore, 119077, Singapore; also with the University of Bremen, 28359 Germany and also with Kriston AI Lab, Xiamen, China (e-mail: {haizhouli}@cuhk.edu.cn).
}
% \thanks
}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}
% \cortext[cor1]{Corresponding author}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2019}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
% Speech enhancement plays an essential role in a wide range of speech processing applications. Recent studies on speech enhancement tend to investigate how to effectively capture the long-term contextual dependencies of speech signals to boost performance. However, these studies generally neglect the time-frequency (T-F) distribution information of speech spectral components, which is equally important for speech enhancement. In this paper, we propose a simple yet very effective network module, which we term the T-F attention \textcolor{black}{(TFA) module}, that uses two parallel attention branches, i.e., time-frame attention and frequency-channel attention, to explicitly exploit position information to generate a 2-D attention map to characterise the salient T-F speech distribution. We validate our \textcolor{black}{TFA module} as part of two widely used backbone networks (residual temporal convolution network and Transformer) and conduct speech enhancement with four most popular training objectives. Our extensive experiments demonstrate that our proposed \textcolor{black}{TFA module} consistently leads to substantial enhancement performance improvements in terms of the five most widely used objective metrics, with negligible parameter overheads. In addition, we further evaluate the efficacy of speech enhancement as a front-end for a downstream speech recognition task. Our evaluation results show that the \textcolor{black}{TFA module} significantly improves the robustness of the system to noisy conditions. 

\textcolor{black}{Hand-crafted features, such as Mel-filterbanks, have traditionally been the choice for many audio processing applications. Recently, there has been a growing interest in learnable front-ends that extract representations directly from the raw audio waveform. \textcolor{black}{However, both hand-crafted filterbanks and current learnable front-ends lead to fixed computation graphs at inference time, failing to dynamically adapt to varying acoustic environments, a key feature of human auditory systems.} To this end, we explore the question of whether audio front-ends should be adaptive by comparing the Ada-FE front-end (a recently developed adaptive front-end that employs a neural adaptive feedback controller to dynamically adjust the Q-factors of its spectral decomposition filters) to established learnable front-ends. Specifically, we systematically investigate learnable front-ends and Ada-FE across two commonly used back-end backbones and a wide range of audio benchmarks including speech, sound event, and music. The comprehensive results show that our Ada-FE outperforms advanced learnable front-ends, and more importantly, it exhibits impressive stability or robustness on test samples over various training epochs.
}

 %we explored an adaptive front-end (Ada-FE) based on parameterized Gabor filters, which employs a neural adaptive feedback controller (AFC) to dynamically adjust the Q-factor of filters in a frame-wise manner to mimic the active process of cochlear. \textcolor{blue}{Should audio front-ends be adaptive?

% \textcolor{black}{\textcolor{red}{R2:} Hand-crafted features, such as Mel-filterbanks, have long been the choice for many audio processing applications until the recent interest in learnable front-ends that extract representation directly from the raw audio waveform. However, both of these approaches lead to fixed computation graphs that fail to dynamically adapt to varying acoustic environments, a key feature of human auditory systems. To this end, we explored an adaptive front-end (Ada-FE) based on parameterized Gabor filters, which employs a neural adaptive feedback controller (AFC) to dynamically adjust the Q-factor of filters in a frame-wise manner to mimic the active process of cochlear. \textcolor{blue}{Should audio front-ends be adaptive? This paper systematically investigates learnable front-ends and Ada-FE across two commonly used back-end backbones and a wide range of audio benchmarks including speech, acoustic scenes, and music. The comprehensive results show that our Ada-FE outperforms advanced learnable front-ends, and more importantly, it exhibits impressive stability or robustness on test samples over various training epochs.}}

% \textcolor{black}{Hand-crafted features, such as Mel-filterbanks, have long been the choice for many audio processing applications until the recent interest in learning a front-end directly from the raw audio waveform. However, both of these approaches lead to fixed systems that fail to dynamically adapt to varying acoustic environments, a key feature of human auditory systems. To this end, we explore an adaptive front-end (Ada-FE) based on parameterized Gabor filters, which employs a neural adaptive feedback controller (AFC) to dynamically adjust the Q-factor of filters in a frame-wise manner to mimic the active process of cochlear. In this paper, we investigate the Ada-FE on two commonly used back-end backbones, across a wide range of audio benchmarks including speech, acoustic scenes, and music. The extensive results show that our Ada-FE outperforms advanced baselines, and more importantly, it exhibits impressive stability or robustness on test samples over various training epochs.}

% \textcolor{black}{Hand-crafted features, such as Mel-filterbanks, have long been the choice for many audio processing applications until the recent interest in training a front-end directly from the raw audio waveform. However, both of these approaches lead to fixed systems that do not dynamically adapt to the environment, a key feature of human auditory systems that makes them significantly more robust to noise and interference. To this end, we explore an adaptive front-end (Ada-FE) based on parameterized Gabor filters, which employs a neural adaptive feedback controller (AFC) to dynamically adjust the Q-factor of filters in a frame-wise manner to mimic the active process of cochlear. In this paper, we investigate the Ada-FE on two commonly used back-end backbones, across a wide range of audio benchmarks including speech, acoustic scenes, and music. The results show that our Ada-FE outperforms advanced baselines, and more importantly, it exhibits impressive stability or robustness on test samples over various training epochs.}

%However, the design of these features draws inspiration from human auditory perception and there is no evidence that they are optimal for the audio task at hand. Learning a front-end directly from the raw waveform as an alternative to fixed Mel-filterbanks has recently gained significant interest. However, despite much progress, existing learnable front-ends still do not match the capabilities (the active or adaptive process) of the human auditory system to adapt to varying acoustic environments. Specifically, they fail to dynamically adjust the learned neural filters with respect to the input signal level at inference time, once training is completed.

% under a variety of noisy environments and 



% through extensive experiments with two state-of-the-art backbone networks on
% which generates a 2-D attention map to guide the model to selectively emphasize the important features in the T-F representation. 
%The recent successes of residual temporal convolution network (ResTCN) and self-attention based Transformer model on speech enhancement mainly benefit from their remarkable ability to model the long-term temporal dependencies of speech signals.

% \newline
% \textit{Availability}: Deep xi framework is available at: \url{https://github.com/anicolson/Deepxi}.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Representation learning, Audio front-end, Adaptive inference, Gabor filters 
% \textcolor{blue}{Noisy environments}
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}\label{sec:1}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
%
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
%
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
%
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
%
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.

\IEEEPARstart{T}{he} \textcolor{black}{last decade has witnessed substantial success in deep learning to speech and audio processing. More recently, audio representation learning has received increasing interest, which seeks to extract discriminative and robust audio features for downstream tasks. It has demonstrated impressive advances in a variety of audio tasks, such as phone recognition~\cite{zeghidour2018learning}, automatic speech recognition (ASR)~\cite{asr2015}, speaker recognition~\cite{sincnet}, spoofing speech detection~\cite{dinkel2017end}, sound event classification~\cite{esc50}, music understanding~\cite{wang2022towards}, and speech separation~\cite{pariente2020filterbank}.}

% Feature extraction is one critical step for building a deep learning-based audio processing system.

\begin{figure}[!t]
% \vspace{-1.3em}
\centering
\begin{subfigure}[t]{0.97\columnwidth}
\centerline{\includegraphics[width=0.99\columnwidth]{./Fixed_systemV6.pdf}}
\caption{\textcolor{black}{Fixed during training and inference.}}
\hfill
\label{fig1:1}
\end{subfigure}
\begin{subfigure}[t]{0.98\columnwidth}
\centerline{\includegraphics[width=0.99\columnwidth]{./Learnable_systemV8.pdf}}
\caption{\textcolor{black}{Learnable during training, fixed at inference time}}
\hfill
\label{fig1:2}
\end{subfigure}
\begin{subfigure}[t]{0.98\columnwidth}
\centerline{\includegraphics[width=0.99\columnwidth]{./Adaptive_systemV8.pdf}}
\caption{\textcolor{black}{Learnable during training, adaptive at inference time}}
\label{fig1:3}
\end{subfigure}

\caption{\textcolor{black}{
\textcolor{black}{\textcolor{black}{Illustration of (a) fixed, (b) learnable but non-adaptive, and (c) learnable and adaptive audio front-ends.} In real-world acoustic scenarios, speech and audio signals are inevitably shaped by varying acoustic conditions during transmission. The input speech level, for instance, varies with the distance between the speakers and the microphone and the voice levels of the speakers. (a) Fixed (hand-crafted) front-ends extract feature using fixed filters during training and inference regardless of the acoustic scene. (b) Existing neural front-ends learn a common model to deal with the different acoustic scenes present in the training data but cannot change to compensate for previously unseen acoustic scenarios at test time}. (c) Our adaptive front-end employs a neural controller to dynamically adjust the filter response (or shape) to varying acoustic conditions during training and inference.
% Illustration of neural audio front-ends. In real-world acoustic scenarios, speech and audio signals are inevitably shaped by varying acoustic conditions during transmission. The input speech level, for instance, varies with the distance (related to the transfer function) between the speakers and the microphone and the voice levels of the speakers. (a) Existing learnable front-ends are fixed at inference time, regardless of the input speech level. (b) Our adaptive front-end employs a feedback controller to dynamically adjust the filter response (or shape) to varying acoustic conditions during training and inference.
} 
% \textcolor{red}{Redraw this figure to emphasize noisy environment instead of low/loud voice}
}
\vspace{-0.8em}
\label{fig1}
\end{figure}

% The hand-crafted Mel-filterbank features have established remarkable success in multiple application domains and remained the features of choice for many state-of-the-art audio processing systems~\cite{conformer}. Mel-filterbanks first apply the short-term Fourier transform (STFT) to the raw waveform, followed by a squared modulus operation. The resulting STFT power spectrogram is then passed through a triangular band-pass filterbank spaced according to the Mel-scale to model the non-linear human perception, which provides robustness and shifts-invariance to deformations~\cite{anden2014deep}. Finally, a logarithmic compression function is typically employed to reduce the dynamic range of filterbank energy to match the human response to loudness.

\textcolor{black}{Until fairly recently audio front-ends have been fixed (Figure 1a), with the Mel filterbank being the front-end of choice} across numerous audio processing systems~\cite{conformer}. Mel-filterbanks first apply the short-term Fourier transform (STFT) to the raw waveform, followed by a squared modulus operation. The resulting STFT power spectrogram is then passed through a triangular band-pass filterbank spaced according to the Mel-scale to model the non-linear human perception, which provides robustness and shifts-invariance to deformations~\cite{anden2014deep}. Finally, a logarithmic compression function is employed to reduce the dynamic range of filterbank energy to match the human response to loudness. \textcolor{black}{However, Mel-filterbanks suffer from limitations of fixed representations, such as a lack of robustness to wide variations in loudness.} Similarly, Mel-filterbanks are incapable of providing robust pitch harmonics to children's ASR systems~\cite{pitch}. To address these limitations, there has been an increased popularity of employing neural filter layers to learn a front-end directly from raw waveforms to replace hand-crafted filterbanks~\cite{he2016deep,luo2019conv,demucs}. Such a way allows for the optimization of neural filterbanks specifically for the audio tasks at hand in an end-to-end fashion.

% The hand-crafted representations of the audio signal, such as Mel- and Gammatone-filterbanks, have established remarkable success in multiple application domains and remained the features of choice for many state-of-the-art audio processing systems~\cite{conformer}. Mel-filterbanks first apply the short-term Fourier transform (STFT) to the raw audio waveform, followed by a squared modulus operation. The resulting STFT power spectrogram is then passed by a triangular band-pass filterbank spaced according to the Mel-scale to model the non-linear human perception, which provides robustness and shifts-invariance to deformations~\cite{anden2014deep}. Finally, a logarithmic compression function is typically employed to reduce the dynamic range of filterbank energy to match the human response to loudness.
%However, the design of Mel-filterbanks is inspired by perceptual evidence from the human auditory system, which is not necessarily the best for the downstream audio processing tasks. 
% \textcolor{blue}{However, Mel-filterbanks suffer from limitations inherent in fixed representations, such as the logarithmic compression, which has been shown not to be robust to loudness variations~\cite{wang2017trainable}.} Similarly, Mel-filterbanks are incapable of providing robust pitch harmonics to children's ASR systems~\cite{pitch}. To address these limitations, there has been an increased popularity of employing neural filter layers to learn a front-end directly from raw waveforms to replace hand-crafted filterbanks~\cite{he2016deep,luo2019conv,demucs}. Such a way allows for the optimization of neural filterbanks specifically for the audio tasks at hand in an end-to-end fashion.

% In contrast, learning representations directly from the raw waveform potentially allows the model to better extract low-level features to attain optimal performance on a specific downstream task at hand. Meanwhile, the success of end-to-end modeling schemes~\cite{he2016deep,luo2019conv,demucs}, where neural networks learn discriminative features from raw data, also inspires us to explore learning front-ends as an alternative to hand-crafted filterbanks.

% However, the design of each module in Mel-filterbanks is intended to emulate the human auditory perceptual system~\cite{davis1980comparison} therefore not necessarily guarantee the best performance or optimal representation for downstream audio processing tasks. Mel-filterbanks, for instance, are incapable of providing robust pitch harmonics for children's ASR systems~\cite{pitch}. Furthermore, Mel-filterbanks suffer from limitations inherent in fixed representations, such as the Mel-scale has been modified several times and the logarithmic compression has been shown to be not robust to loudness variations~\cite{wang2017trainable}. In contrast, learning representations directly from the raw waveform potentially allows the model to better extract low-level features to attain optimal performance on a specific downstream task at hand. Meanwhile, the success of end-to-end modeling schemes~\cite{he2016deep,luo2019conv,demucs}, where neural networks learn discriminative features from raw data, also inspires us to explore learning front-ends as an alternative to hand-crafted filterbanks.

% Meanwhile, the success of end-to-end modeling schemes~\cite{he2016deep,luo2019conv,demucs}, where neural networks learn discriminative features from raw data, also inspires us to explore learning front-ends from raw waveform as an alternative to hand-crafted filterbanks.

% In addition, Mel-filterbanks also present inherent issues of fixed representations, such as the Mel-scale has been modified several times and logarithmic compression has been shown to be not robust to loudness variation~\cite{wang2017trainable}

% Instead, learning representations directly from the raw waveform enables the model to extract low-level features to attain optimal performance on each specific downstream task. Meanwhile, the success of end-to-end modeling schemes~\cite{luo2019conv,he2016deep}, where the network learns the discriminative features from the raw data, also inspires us to explore learning front-ends from raw waveform as an alternative to hand-crafted filterbanks. 

% \textcolor{black}{Some previous works propose replacing the fixed logarithmic compression with a differentiable non-linear compression operation (e.g., per-channel energy normalization~\cite{wang2017trainable}) to build learnable audio front-ends~\cite{lostanlen2018per}. Most recent studies focus on the use of learnable neural filter layers for learnable front-ends, including standard convolution neural layers \cite{conv15,7178847}, dilated convolution layers~\cite{schneider2019wav2vec}, and the recent popular one, i.e., parameterized known filters (e.g., Gammatone~\cite{sainath15_interspeech}, Sinc~\cite{pariente2020filterbank,sincnet}, Gaussian~\cite{gauss}, and Gabor filters~\cite{zeghidour2018learning,leaf}). In contrast to standard convolution filter layers, these parameterized filters are shaped by only a few learnable parameters with clear physical meaning. In this way, one can force the neural filter layer to learn physically meaningful filters and still provide considerable learning flexibility. Moreover, researchers also attempt to learn all the operations (filtering, compression, and pooling) in front-ends~\cite{leaf}.}


\textcolor{black}{In recent years, multiple types of neural filter layers have been investigated to build learnable front-ends, involving standard convolutional layers~\cite{conv15,7178847}, dilated convolutional layers~\cite{schneider2019wav2vec}, and parametric neural filters such as Gaussian~\cite{gauss}, Gammatone~\cite{sainath15_interspeech}, Sinc~\cite{sincnet,pariente2020filterbank}, and Gabor filters~\cite{leaf}.} Parametric neural filters, in comparison to standard convolutional layers that learn all the filter parameters, only involve several trainable shape parameters with a clear physical meaning (e.g., cut-off frequency, bandwidth, and center frequency). In this way, the number of parameters is drastically reduced and the learned feature representations are more interpretable while maintaining considerable learning flexibility. There are some attempts to learn the non-linear compression component~\cite{wang2017trainable} or learn all these components (i.e., filtering, compression, and pooling)~\cite{leaf}.




% In contrast to standard convolutional neural layers, these parametric filter layers are shaped by only a few learnable parameters with clear physical meanings. In such a way, one can force neural filter layers to learn physically meaningful filters, while still providing considerable learning flexibility. Moreover, some researchers propose learning the compression~\cite{wang2017trainable} or all the operations (filtering, compression, and pooling) to build learnable front-ends~\cite{leaf}.


% \textcolor{black}{There has been an increased popularity of employing neural filter layers to learn filterbanks, including standard convolution neural layers \cite{conv15,7178847}, dilated convolution layers~\cite{schneider2019wav2vec}, and the parameterized neural filters such as Gammatone~\cite{sainath15_interspeech}, Sinc~\cite{pariente2020filterbank,sincnet}, Gaussian~\cite{gauss}, and Gabor filters~\cite{zeghidour2018learning,leaf}. In contrast to standard convolution neural layers, these parameterized filters are shaped by only a few learnable parameters with clear physical meanings. In such a way, one can force the neural filter layer to learn physically meaningful filters and still provide considerable learning flexibility. Moreover, researchers also propose learning the compression~\cite{wang2017trainable} or all the operations (filtering, compression, and pooling) in front-ends~\cite{leaf}.}

% \textcolor{black}{There has been an increased popularity of employing neural filter layers to learn filterbanks, including standard convolution neural layers \cite{conv15,7178847}, dilated convolution layers~\cite{schneider2019wav2vec}, and the parameterized neural filters such as Gammatone~\cite{sainath15_interspeech}, Sinc~\cite{pariente2020filterbank,sincnet}, Gaussian~\cite{gauss}, and Gabor filters~\cite{zeghidour2018learning,leaf}. In contrast to standard convolution neural layers, these parameterized filters are shaped by only a few learnable parameters with clear physical meanings. In such a way, one can force the neural filter layer to learn physically meaningful filters and still provide considerable learning flexibility. Moreover, researchers also propose learning the compression~\cite{wang2017trainable} or all the operations (filtering, compression, and pooling) in front-ends~\cite{leaf}.}

As illustrated in Figure~\ref{fig1}\,(b), these resulting neural front-ends are learned jointly along with the subsequent back-end network model, such that the representations are optimized for the downstream task at hand. However, prior neural front-ends perform feature extraction in a static fashion at the inference stage, i.e., the filter parameters are fixed once trained. That is, they can not dynamically adjust the filters conditioned on the input (e.g., varying acoustic environments) during inference. However, the evidence from psychoacoustic research has demonstrated that the mammalian cochlea performs an active mechanism exploiting metabolic energy via the outer hair cells (OHCs), which dynamically tunes its response conditioned on the input~\cite{lyon1990automatic,elliott2012cochlea}. This adaptive mechanism enables the mammalian cochlea extraordinary auditory sensitivity and selectivity, as well as a wide dynamic range of hearing~\cite{hudspeth2014integrating}. \textcolor{black}{Inspired by the mammalian auditory model, the zero-crossing with peak-amplitudes (ZCPA) model~\cite{zcpa} and power-normalized cepstral coefficients (PNCC)~\cite{PNCC} have been proposed for robust ASR.}


% \textcolor{black}{These resulting front-ends are optimized jointly with the back-end classifier to learn the representations that are customized to a specific task at hand (Figure~\ref{fig1}(a)). However, once the training is completed, the filter weights or coefficients of all these learnable front-ends are fixed at the inference stage. In other words, they do not dynamically adjust in response to varying acoustic conditions. However, psychoacoustic research has demonstrated that the outer hair cells (OHCs) of mammalian cochlea exploit metabolic energy to perform an active mechanism that dynamically adjusts its response with respect to input signal levels~\cite{lyon1990automatic,elliott2012cochlea}. It is known that such adaptive mechanisms have led to remarkable auditory sensitivity and selectivity, and a wide dynamic range of hearing~\cite{hudspeth2014integrating}.}

\textcolor{black}{In a preliminary study~\cite{wickramasinghe2023dnn}, we \textcolor{black}{introduced} a novel adaptive front-end termed Ada-FE to emulate the adaptive mechanism in the mammalian cochlea for audio spoofing detection. \textcolor{black}{Ada-FE exploits a neural feedback controller to perform adaptive inference by dynamically tuning the filter weights in a frame-wise fashion.}} The diagram of the proposed framework is shown in Figure~\ref{fig1}\,(c). In particular, our Ada-FE learns the audio representation with a set of parameterized Gabor filters that perform band-pass filtering. Ada-FE consists of one fixed Gabor filter layer and one adaptive Gabor filter layer. The adaptive Gabor filter layer exploits two parallel modules (i.e., level-dependent adaptation and adaptive neural feedback controller) to dynamically tune the filters frame by frame. The characteristic of the Gabor filter is controlled by the bandwidth and center frequency. Ada-FE tunes the shape or selectivity of filters via the quality factor (Q-factor), which is defined as the ratio of the center frequency and bandwidth. 

% To this end, we study a novel adaptive front-end with a neural feedback controller, referred to as Ada-FE, which performs adaptive inference by dynamically tuning the filter weights in a frame-wise fashion. The diagram of the proposed framework is shown in Figure~\ref{fig1}\,(b). In particular, our Ada-FE learns the feature representation with a set of parameterized Gabor filters that perform band-pass filtering. Ada-FE consists of one fixed Gabor filter layer and one adaptive Gabor filter layer. The adaptive Gabor filter layer exploits two parallel modules (i.e., level-dependent adaptation and adaptive neural feedback controller) to dynamically tune the filters frame by frame. The characteristic of the Gabor filter is controlled by the bandwidth and center frequency. Ada-FE tunes the shape or selectivity of filters via the quality factor (Q-factor), which is defined as the ratio of the center frequency and bandwidth. 

% Our goal is to explore a way to derive a neural front-end that mimics the adaptive mechanism of the mammalian cochlea. To this end, we study a novel adaptive front-end with a neural feedback controller, referred to as Ada-FE, which performs adaptive inference by dynamically tuning the filter weights in a frame-wise fashion. The diagram of the proposed framework is shown in Figure~\ref{fig1}\,(b). In particular, our Ada-FE learns the feature representation with a set of parameterized Gabor filters that perform band-pass filtering. Ada-FE consists of one fixed Gabor filter layer and one adaptive Gabor filter layer. The adaptive Gabor filter layer exploits two parallel modules (i.e., level-dependent adaptation and adaptive neural feedback controller) to dynamically tune the filters frame by frame. The characteristic of the Gabor filter is controlled by the bandwidth and center frequency. Ada-FE tunes the shape or selectivity of filters via the quality factor (Q-factor), which is defined as the ratio of the center frequency and bandwidth. 

% is the only parameter that tunes the shape or selectivity of filters
% dynamically tune the filters frame by frame
% In this paper, we explore further simplifying Ada-FE and different design choices with the simplified Ada-FE (Ada-FE-S). Our experiments are carried out on two widely used back-end backbone classifiers and across a diverse range of audio-related tasks, involving speech, acoustic scenes, and music tasks, to evaluate the Ada-FE and Ada-FE-S. The experimental results show that the Ada-FE and Ada-FE-S converge faster and exhibit better performance than advanced baseline systems. More notably, our adaptive front-ends demonstrate extremely excellent stability on the test set (unseen data). The workflow of the Ada-FE and Ada-FE-S models are described in Section~\ref{sec3}.

\textcolor{black}{\textcolor{black}{Should audio front-ends be adaptive? In this paper, we conduct a systematic study on the Ada-FE across two widely used back-end classifiers and a diverse range of speech and audio tasks, involving sound event classification, non-semantic speech~\cite{shor2020towards} (keyword spotting, emotion recognition, speaker identification), and music tasks (genre classification). 
% A single-task setting is adopted for evaluation.
} Furthermore, we explore simplifying the adaptive control mechanism of the Ada-FE, and the adaptive front-end with only one adaptive filter layer to study the role of fixed Gabor filters. For the neural feedback controller, we further probe the effects of different input choices. The workflows of the Ada-FE and the simplified Ada-FE (Ada-FE-S) models are detailed in Section~\ref{sec3}.}

% In this paper, we comprehensively study the Ada-FE across two widely used back-end backbone classifiers and a diverse range of audio and speech tasks, involving acoustic scene classification, keyword spotting, emotion recognition, speaker identification, and music genre classification tasks. Furthermore, we explore simplifying the adaptive control mechanism of the Ada-FE, and the adaptive front-end with only one adaptive filter layer to study the role of fixed Gabor filters. For the neural feedback controller, we further probe the effects of different input choices. The workflows of the Ada-FE and the simplified Ada-FE (Ada-FE-S) models are detailed in Section~\ref{sec3}.



% \textcolor{blue}{In this paper, we explore further simplifying Ada-FE and more design choices with the simplified Ada-FE (Ada-FE-S). Our experiments are carried out on two widely used back-end backbone classifiers and across a diverse range of audio-related tasks, involving speech, acoustic scenes, and music tasks, to evaluate the Ada-FE and Ada-FE-S. The experimental results show that the Ada-FE and Ada-FE-S converge faster and exhibit better performance than advanced baseline systems.} 
% More notably, our adaptive front-ends demonstrate extremely excellent stability on the test set (unseen data). The workflow of the Ada-FE and Ada-FE-S models are described in Section \ref{sec3}.

\textcolor{black}{In summary, the main contribution of this work is two-fold as follows:}
\begin{itemize}
    \item \textcolor{black}{We explore the adaptive front-end (Ada-FE)~\cite{wickramasinghe2023dnn}, where a neural feedback controller is designed to dynamically tune the filter weights frame by frame at run time to emulate the active mechanism of the mammalian cochlea. Specifically, we compare Ada-FE to a range of learnable front-ends across two back-end classifiers and diverse audio and speech tasks.}
    % to emulate the active mechanism of the mammalian cochlea, which exploits a neural feedback controller to dynamically tune the filter weights in a frame-wise manner at run time. 
    \item \textcolor{black}{We further simplify the adaptive control mechanism to enable the adaptive filter layer adjusted completely by the neural feedback controller and study Ada-FE with only one adaptive filter layer. Built upon this, we probe different design choices on the input to the neural controller.}
    % We further attempt to simplify the Ada-FE by removing the control of hand-crafted level-dependent adaptation. In Ada-FE-S, the adaptive Q value is only adjusted by the neural feedback controller. Built upon Ada-FE-S, we explore more design choices on the input to the feedback controller. 
   % \item We design two modules, i.e., level-dependent adaptation and adaptive feedback controller, to dynamically learn Gabor filter layers. Comprehensive ablation studies confirm the efficacy of our design choices.
%\textcolor{black}{The comprehensive experiment results confirm the superiority of Ada-FE over the state-of-the-art front-ends in recognition accuracy and stability. Compared to Ada-FE, Ada-FE-S exhibits comparable or better performance.}
\end{itemize}

 % We comprehensively evaluate Ada-FE and Ada-FE-S across eight widely used audio/speech benchmarks and two widely used back-end backbone networks. The evaluation results show that the Ada-FE-S achieves comparable or better performance than Ada-FE. Compared to existing state-of-the-art audio front-ends, Ada-FE and Ada-FE-S demonstrate better accuracy.
 
\textcolor{black}{The remainder of this paper is structured as follows. In Section~\ref{sec2}, we first describe the related works. In Section~\ref{sec3}, we detail the proposed adaptive front-end with neural feedback control. In Section~\ref{sec4}, we present the experimental setup. Section~\ref{sec5} provides experimental results and discussions. Finally, in Section~\ref{sec6}, we conclude this paper.}


\begin{figure*}[!ht]
 \centering
  % \includegraphics[width=0.99\linewidth]{front-end_overview.pdf}
  \includegraphics[width=0.994\linewidth]{./AdaFE_Fig/fig2.pdf}
    \caption{Illustration of audio/speech representation methods, which mainly include Fixed, hand-crafted features (e.g., Mel- and Gammaton-filterbanks), lightweight learnable front-ends (e.g., TD-fbanks, LEAF, and SincNet), our dynamically adaptive front-end (Ada-FE), and pre-trained audio models (e.g., Wav2Vec, AST, WavLM, and HuBERT).
    }
  \label{fig:sr_overview}
  % \vspace{-1.0em}
\end{figure*}

\section{Related Work}\label{sec2}

\textcolor{black}{In Figure~\ref{fig:sr_overview}, we briefly outline audio/speech representation methods, including hand-crafted fixed filterbanks, lightweight learnable front-ends (not adaptive), our dynamically adaptive front-end (Ada-FE), and audio pre-training methods.}

\subsection{\textcolor{black}{Lightweight Learnable Front-ends}} 
There have been many studies on learnable front-ends as drop-in replacements of fixed Mel-filterbanks. Some learn front-ends from low-level spectral features such as STFT. For instance, Sainath~\textit{et al.}~\cite{sainath2013learning} take the power spectral feature as the input and explore using a filterbank layer to learn the band-pass filters of Mel-filterbanks. The filterbank layer is initialized with the weights of the Mel-filters and optimized jointly with the rest of the network. Seki~\textit{et al.}~\cite{gauss} exploit the parameterized Gaussian filters to learn the band-pass filterbanks from the power spectrogram. Won~\textit{et al.}~\cite{won2020data} propose parameterized harmonic band-pass filterbanks for learnable front-end that preserves spectro-temporal locality with harmonic structures, where harmonic filters are learned from the STFT spectrogram.


% \begin{figure*}[!ht]
%  \centering
%   \includegraphics[width=0.90\linewidth]{AdaFEV2.pdf}
%   % \vspace{-0.5em}
%     \caption{Illustrations of (a) the overview diagram of the adaptive front-end (Ada-FE) and (b) the simplified adaptive front-end (Ada-FE-S), where the fixed level-dependent adaptation function module is removed and the adaptive Q value is completely tuned by the adaptive feedback controller.}
%   % \caption{Illustrations of (a) The overview diagram of the adaptive front-end (adopted from \cite{wickramasinghe2021replay}), (b) The energy-based level-dependent adaptation, and (c) the adaptive feedback controller.}
%   \label{fig2}
%   \vspace{-1.5em}
% \end{figure*}

% \begin{figure*}[!ht]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.91\linewidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{./AdaFE.pdf}}
% % \vspace{-0.5em}
% \caption{The adaptive front-end (Ada-FE) is optimized jointly with the back-end classifier.}
% \label{fig2:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.91\linewidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{./AdaFES.pdf}}
% % \vspace{-0.5em}
% \caption{The simplified adaptive front-end (Ada-FE-S) is optimized jointly with the back-end classifier.}
% \label{fig2:2}
% \end{subfigure}
% \caption{\textcolor{black}{Illustrations of (a) the overall diagram of the adaptive front-end (Ada-FE)~\cite{wickramasinghe2023dnn} and (b) the simplified adaptive front-end (Ada-FE-S), where the hand-crafted level-dependent adaptation function module is removed and the adaptive Q value is completely controlled by the neural adaptive feedback controller (blue box).}}
% % \vspace{-0.8em}
% \label{fig2}
% \end{figure*}

% (adopted from \cite{wickramasinghe2021replay})

Learning representations directly from the raw waveforms in an end-to-end manner has attracted great research interest, which avoids the design of hyper-parameters (such as frame length, hop length, and the typology of the window function) used to extract the appropriate low-level spectrum. In an early attempt \cite{jaitly2011learning}, Jailty and Hinton proposed pre-training a generative restricted Boltzmann machine (RBM) to learn features from raw speech signals and show that the learned features yield promising performance in phoneme recognition on the TIMIT benchmark. Later, Palaz \textit{et al.}~\cite{palaz2013estimating} proposed using a convolutional neural network (CNN) to learn representation from raw waveforms for phoneme recognition and suggest that the first layer tends to learn a set of band-pass filters. Similarly, Sainath~\textit{et al.}~\cite{sainath15_interspeech} and Hoshen \textit{et al.}~\cite{hoshen2015speech} employed CNNs for feature learning from single- and multi-channel raw waveforms respectively, where the CNN filters are initialized as Gammaton-filters and trained jointly with the deep classifier. In~\cite{dinkel2017end}, a convolutional long short-term memory deep neural network (CLDNN) is trained on raw waveforms for spoofing speech detection. Along this line of research, Zeghidour \textit{at al.}~\cite{zeghidour2018learning} proposed learnable time-domain filterbanks (TD-fbanks) for phone recognition as an approximation to Mel-filterbanks.

% and fine-tuned with the remaining CNN without constraints.

Parameterized neural filters have been explored to facilitate CNNs to discover meaningful filters. Ravanelli and Bengio~\cite{sincnet} propose the SincNet layer for speaker recognition, comprised of a set of parameterized Sinc filters that approximate band-pass filters, where the low and high cut-off frequencies are the only parameters learned from data. The recent LEAF~\cite{leaf} employs two parameterized Gabor filter layers to build a learnable front-end, with learnable filtering, pooling, and compression, demonstrating impressive results in a broad range of audio and speech tasks. The very recent works~\cite{anderson2023learnable,meng23c_interspeech} demonstrate that LEAF lacks learning as there is no substantial movement between initial filters and learned filters.

The Gabor filter, in contrast to the Sinc filter, shows characteristics similar to those of auditory filters and is optimally localized in time and frequency. This motivates us to explore parameterized Gabor filters for our adaptive front-end, where our Ada-FE learns the Q-factor to tune the filters instead of the center frequency and bandwidth used in LEAF. Additionally, unlike all prior learnable front-ends that perform feature extraction with fixed filter weights at inference time, to our knowledge, Ada-FE is the only front-end that can dynamically tune the filters frame by frame to perform adaptive inference with respect to the input.

% \textcolor{blue}{More recently, studies~\cite{anderson2023learnable,meng23c_interspeech} show that LEAF does not learn as it shows no substantial difference between learned and initialized filters, except the compression layer.} 

\subsection{\textcolor{black}{Audio Pre-training Methods}} 
\textcolor{black}{\textcolor{black}{As a parallel line of research, audio pre-training has recently fostered prominent success in audio representation learning, which involves supervised pre-training and self-supervised pre-training~\cite{audiomae,chen2023beats,byol}}. These methods typically pre-train a large neural network model on a great amount of external data to extract high-level feature representations from spectral features (e.g., AST~\cite{gong21b_interspeech}, SSAST~\cite{gong2022ssast}, and BYOL-A~\cite{byol}) or raw audio waveforms (e.g., HuBERT~\cite{hubert}, WavLM~\cite{wavlm}, and Wav2vec2~\cite{wav2vec2}).} In contrast, lightweight learnable audio front-ends only perform low-level spectral feature learning (spectral decomposition) with a small number of trainable parameters. A recent study~\cite{YadavZ22} suggests that \textcolor{black}{lightweight learnable front-ends (i.e., SincNet and LEAF) could benefit self-supervised audio pre-training. \textcolor{black}{These pre-trained models are learnable but not adaptive.}} 


% \textcolor{black}{In parallel, representation learning based on pre-training has enabled the recent progress in audio processing, where a large DNN is pre-trained on massive external data to extract high-level representation from raw waveforms (e.g., Wav2vec2~\cite{wav2vec2} and WavLM~\cite{wavlm}) or spectrograms (e.g., AST~\cite{gong21b_interspeech} and BYOL-A~\cite{byol}). It is unlike the lightweight learnable front-ends that only involve spectral decomposition and low-level spectral feature learning, with fewer parameters.}

% (as well as pre-training models

% Unlike LEAF which learns center frequency and bandwidth to tune the filters, our proposed method only uses the Q-factor to tune the filters.
% Furthermore, Gabor filter provides a straightforward way to dynamically tune the filter weights via Q-factor

% \begin{figure}[!htbp]
%  \centering
%   \includegraphics[width=0.57\linewidth]{AdaptiveQ.pdf}
%   % \vspace{-0.5em}
%   \caption{Illustrations of how Q values are adjusted in frame-by-frame fashion in Ada-FE.}
%   \label{fig_struc}
%   % \vspace{-1.5em}
% \end{figure}


% \IEEEPARstart{S}{peech} signals in a real-world acoustic environment are inevitably corrupted by background noise, which can severely degrade speech quality and intelligibility. Speech enhancement seeks to separate the target speech signal from the background noise. It is an essential component in a number of speech processing systems, such as hearing aids, automatic speech recognition (ASR), speaker verification, and the brain-computer interface. Monaural speech enhancement represents one of the challenges. Traditional signal processing-based methods have been extensively studied for a long time, mainly including spectral subtraction \cite{boll1979}, Wiener filtering \cite{wiener1996}, and statistical model-based methods \cite{mmse,mmse2017,zhang2019,8740919}. These methods perform well for stationary noise, however, fail to handle non-stationary noise. 

% With the advent of deep learning, speech enhancement has made remarkable progress~\cite{overview2018}. Techniques can be grouped into time-frequency (T-F) domain methods and time-domain methods, according to the way input signals are handled. Specifically, time-domain methods perform speech enhancement directly on the raw waveform domain, where a deep neural network (DNN) is optimized to learn the mapping from the noisy raw waveform to the clean one \cite{SEGAN,2018raw,TCNN,convtasnet,kolbaek2020loss} \textcolor{black}{via some latent feature representation. T-F domain methods typically transform the noisy raw waveform into a T-F representation or spectrogram first before mapping to the clean one with well-designed training objectives.} The most popular T-F domain training objectives include ideal ratio mask (IRM) \cite{targets}, spectral magnitude mask (SMM) \cite{targets}, complex IRM (cIRM) \cite{cirm}, phase-sensitive mask (PSM) \cite{psm}, target magnitude spectrum \cite{tms2013}, and log-power spectrum \cite{yongxu2015}. 
% More recently, the instantaneous \textit{a priori} signal-to-noise ratio (SNR), termed Xi, is proposed as a \textcolor{black}{training objective} to bridge the gap between deep learning and traditional statistical model-based methods \cite{nicolson2019deep,DeepMMSE}. In this study, we adopt a novel T-F domain method, which allows for intuitive time-frequency analysis. We also adopt the three most widely used \textcolor{black}{training objectives}, i.e., IRM, SMM, PSM and a recent one (Xi) in the experiments.

%\textcolor{blue}{Although the difficult phase estimation problem is avoided, time-domain methods also give up the benefit of easily distinguishable speech and noise patterns on a T-F spectrogram. In this study, we focus on T-F domain method and adopt three most widely used training objectives, i.e., IRM, SMM, PSM, and a recent one (Xi) in the experiments. }
%\textcolor{red}{Could this paragraph explain that what we choose T-F method first?}}
%\textcolor{blue}{Or just need to say: In this study, we focus on T-F domain method and adopt three most widely used training objectives, i.e., IRM, SMM, PSM, and a recent one (Xi) in the experiments.

%\textcolor{red}{we need to justify what we choose T-F method first (time-domain vs T-F), then explain why IRM, SMM, PSM and Xi are used. (predicting mask directly vs indirectly via spectrum targets.) masks are  actually not better than magnitude or spectrum targets when optimizing network. }

%\textcolor{blue}{As many masks are proposed for speech enhancement, we select four of them in order to perform extensive experiments to demonstrate our method is solid. In some studies, masking methods show better performance, in some other studies, magnitude mapping may be better. I think there is no a accurate definition about the superiority mask over magnitude.} \textcolor{red}{i am not comparing magnitude mapping vs masking. I am comparing two ways to learn the masks. One is to use the masks as the target (objective) directly, another is to use the output features as the target (objective) while optimizing the masks as an intermediate layer. The second one is always better.}

%\textcolor{blue}{Thank you Prof. Li. Now I understand what you said, the first one is mask approximation, the second one is the signal approximation (the mask is used as an intermediate). In fact both of thees two methods are widely used in existing methods and following the previous works (conference paper), in this study we use the first one in our experiment. I think that we can choose anyone to reach the same comparison results or conclusion. We just need to keep all models using the same objective loss function and describe clearly the used objective function in experiment setup part.}

 % Time-domain methods \cite{SEGAN,convtasnet,TCNN} optimize a DNN directly  recover the raw waveform of clean speech from noisy raw waveform, and are not able to utilize the benefit that the harmonic structure of speech and noise pattern are easily distinguishable in T-F spectrogram.
% Although time domain methods achieve the state-of-the-art performance in speaker separation task, the superiority is not obvious in speech enhancement filed. Taking the speech quality, robustness, and computational efficiency into consideration, the T-F domain methods remain the most attractive solution today.

% Advanced speech enhancement algorithms depend on a strong backbone network architecture. Multi-layer perceptrons (MLPs) are the most widely adopted backbone network architecture in early studies. Furthering the idea of T-F masking in the computational auditory scene analysis, Wang \textit{et al.} \cite{wang2013} proposed to employ an MLP to predict the ideal binary mask (IBM) \cite{ibm} to separate the speech from background noise. Subsequently, Xu \textit{et al.} \cite{yongxu2015} adopted an MLP as a regression function to learn the mapping from the noisy log-power spectra to the clean one. 
% In \cite{chenlstm}, Chen \textit{et al.} formulated speech enhancement as a sequence-to-sequence mapping, that effectively addresses speaker generalization issue, and employed a recurrent neural network (RNN) with four long short-term memory (LSTM) layers to model the long-range contextual information of the speech. The LSTM-RNN model demonstrates substantial performance improvement over MLPs~\cite{chenlstm,2014lstm,2015lstm}. However, deep LSTM-RNN network architectures involve a large number of parameters, which significantly limits its scope of applications. 

% Deep convolutional neural networks (CNNs) represent another successful backbone network architecture. Unlike RNN that involves a sequential process, CNN  performs the filter processing on speech frames in parallel. Meanwhile, CNN captures the contextual information by stacking multiple layers. Recently, the residual temporal convolution networks \textcolor{black}{(ResTCNs)} \cite{TCN2018}, which employ 1-D dilated convolutional modules and residual skip connections, have demonstrated impressive performance in modeling long-term dependencies and outperformed RNN across a broad range of sequence modeling tasks. \textcolor{black}{ResTCNs} have gained considerable success in speech enhancement \cite{TCNN,DeepMMSE,GRN,restcnsa} and speaker separation \cite{convtasnet,zhiqiang2019} as well. Self-attention based \textcolor{black}{Transformer} backbone network \cite{attention2017} has achieved state-of-the-art performance on many natural language processing tasks. More recently, Transformers have been successfully adopted for speech enhancement \cite{restcnsa,mha_se,mhanet} and many other speech processing-related tasks such as speech synthesis and voice conversion. As the key component of \textcolor{black}{Transformers}, the multi-head self-attention \textcolor{black}{(MHA)} mechanism processes the whole sequence at once and computes the similarity between all time-steps to obtain the new representation, allowing the \textcolor{black}{Transformer} model for modeling long-term dependencies more efficiently. In \cite{restcnsa} Zhao \textit{et al.} proposed to employ a \textcolor{black}{MHA} module to produce dynamic representations followed by a \textcolor{black}{ResTCN} model to learn a nonlinear mapping for speech dereverberation.

% The generation of human speech is subject to the constraint of the physiological structure of vocal production, and the phonetic and phonotactic rules of a spoken language. The success of \textcolor{black}{ResTCN} and \textcolor{black}{Transformer} in speech enhancement mainly stems from their ability to effectively model the long-range temporal context of speech. 
% %The spectral density and the time-frequency (T-F) joint energy distribution of speech signals are equally informative for speech perception. However, the T-F representation of speech is not adequately exploited by existing methods for speech enhancement.
% We also note that the energy concentration of a speech utterance in time or frequency varies from utterance to utterance. To preserve such a speech formant structure, we expect that a speech enhancement model performs according to the energy concentration in the T-F plane of a spectrogram. We are motivated to investigate a dedicated mechanism to characterise the salient T-F speech distribution.

%  (We are motivated to investigate a different aspect of the architecture design, T-F attention, for modeling the T-F distribution characteristic of speech.)
 
% The idea of attention has been well studied for the network to learn to attend to the salient features in computer vision \cite{senet,cbam,ecanet}, speech emotion recognition \cite{2020SER,2021SER}, and speaker verification \cite{2021spr}.  In a preliminary study \cite{tcn_se}, we investigated an attention mechanism to model the speech distribution along the frequency dimension and demonstrate its efficacy. We proposed a functional neural module, termed T-F attention \textcolor{black}{(TFA)}, as part of the backbone networks to attend to the salient T-F representation for speech enhancement \cite{tfa}. The proposed \textcolor{black}{TFA module} consists of two parallel attention branches, i.e., time-dimension \textcolor{black}{(TA)} and frequency-dimension attention \textcolor{black}{(FA)} that produce two 1-D attention maps to guide the models to focus on `where' (which time frames) and `what' (which frequency-wise channels), respectively. Then the \textcolor{black}{TA} and \textcolor{black}{FA} branches are combined to generate the final 2-D attention map, which assigns differentiated attention weights for each T-F spectral component, allowing the networks to capture the speech distribution in the T-F representation. In this paper, we further study the \textcolor{black}{TFA module}~\cite{tfa} across different backbone network architectures and \textcolor{black}{training objectives}, and evaluates its efficacy for a robust ASR system.

% We evaluate the proposed TFA module as part of two widely used backbone networks, i.e., ResTCN and Transformer, in both speech enhancement and speech recognition experiments.}

%\textcolor{red}{There have been studies on T-F attention~\cite{zhao2020} in speech enhancement, which attempts to capture the T-F contextual correlations~\cite{2021interactive,tftnet} by applying self-attention along the T-F dimensions. In those studies, the attention mechanisms were optimized directly for enhanced speech outputs. In this paper, we propose a novel T-F attention module that is optimized for T-F mask-based training objectives. The proposed T-F attention module is different from that in~\cite{zhao2020} both in terms of motivation and actual implementation.} 

% \textcolor{black}{There have been attempts to capture the long-range correlations in the T-F representation by applying self-attention operation along the time and frequency axes~\cite{2021interactive,tftnet}, which was referred to as T-F attention~\cite{zhao2020}. In this paper, we use T-F attention to refer to a dedicated mechanism that especially models the salient T-F speech distribution of speech signals. Such a T-F attention module can be used to augment existing neural speech enhancement solutions. Therefore, it is different from that in~\cite{zhao2020} either in terms of motivation or implementation.}

% \textcolor{black}{There have been attempts to capture long-range correlations in the T-F representation by applying self-attention operation along the time and frequency axes~\cite{2021interactive,tftnet}, which was referred to as T-F attention~\cite{zhao2020}. In this paper, \textcolor{black}{T-F attention (TFA) refers to a dedicated mechanism different from self-attention. It models the salient T-F speech distribution of speech signals. In particular,} the T-F attention~\cite{zhao2020} is based on self-attention, and the learned attention scores represent the similarity among T-F vectors. However, the differentiated attention weights learned by our TFA represent how informative each T-F spectral component is. Such a T-F attention module can be used to augment existing neural speech enhancement solutions including self-attention. Therefore, it is different from that in~\cite{zhao2020} either in terms of motivation or implementation.}
% \textcolor{black}{The main contributions of this work are as follows:
% \begin{itemize}
%   \setlength\itemsep{0.5pt}
%   \item We propose a simple yet very effective network module (TFA) to characterise the salient T-F distribution for speech enhancement. It can be flexibly integrated with existing backbone networks to improve performance.
%   \item We design time-dimension (TA) and frequency-dimension (FA) attention to enable the models to focus on informative frames and frequency-wise channels, respectively. Comprehensive ablation studies validate the efficacy.
%   \item We extensively evaluate the TFA module across different backbone networks and training objectives. The results confirm that our TFA module consistently provides significant performance gains in speech enhancement, as well as in robust speech recognition.
% \end{itemize}}


%\textcolor{red}{to qiquan: we need to find a way to explain the difference. I don't agree with your arguments. I think that our ResTCN and MHANet both handle contextual information just like any Transformer model [41]. They also selectively pay attention to T-F bins, we need to explain why they have not done enough to motivate the dedicated TFA we propose. I understand perfectly that we do NOT do self-attention, the ResTCN/MHANet does, and we only do TF bin masking in this paper. But we cannot say that the Transformer-based model does not select TF bins. they do. Our paper will go to the reviewers who are good at Transformer-based research. We will get negative review if we say that. however, we can nicely explain why the transformer-based system is not good at doing this. By showing better results cannot be the proof because we use more parameters in an additional module. You know that in NN, when you add more parameters, you gain performance most of the time.}

%\textcolor{blue}{I think that we do not need to describe the differences of training objectives. We just need to say that they are different, as self-attention only model the correlations although it is extended to frequency axis. We focus on generating a 2-D attention map to locate informative T-F component, which is completely unrelated to self-attention mechanism.} \textcolor{red}{in [39], actually they also locate the informative T-F components. I have read [39] multiple times and conclude that the difference is only the training objectives. the technique in [39] can also be used for our task.}

% \textcolor{blue}{I will send [39-41] to you for checking.} \textcolor{blue}{Dear Prof. Li, First, our paper is quite different from study [39] with respects to both idea and motivation. Study [39] is still focus on the dependencies (or correlation) and simply use self attention operation on frequency axis and temporal axis sequentially, instead of focus on which T-F component is informative. The mechanism is quite different. Here, the reason why I describe this study [39] is that they also use same term T-F attention (the name is same as ours), and I want to describe clearly to avoid the misunderstanding. In fact, I argue that It is quite OK that we do not describe the study [39] here.}
%the spectral components in the T-F representation, which is different from the T-F attention in~\cite{zhao2020} both in terms of motivation and actual implementation.


%\textcolor{red}{we should cite TF attention papers for speech recognition here. There are many. you can find some references from this paper --- Lili Guo, Longbiao Wang*, Chenglin Xu, Jianwu Dang*, Eng Siong Chng and Haizhou Li, "Representation Learning with Spectro-Temporal-Channel Attention for Speech Emotion Recognition," Proc. of ICASSP 2021: 6304-6308 -- Recently, attention mechanisms \cite{senet,cbam,ecanet} have been extensively studied in computer vision, and achieved substantial performance improvement.} \textcolor{blue}{I cite a T-F attention paper on speech enhancement. I did not cite it before as it is quiet different from our idea (just same name). I already cite the papers about T-F attention in speech emotion attention and speaker verification.}

%Our proposed TFA module is a lightweight and plug-and-play module that can be easily integrated into existing architectures.

%Considering the distribution characteristic of speech energy in T-F domain and inspired by the attention in computer vision, 

% We propose a time-frequency attention (TFA) module to guide model `where' (which time segments) and `what' (which spectral components in frequency dimension) to attend and also increase the representation capability of the model. The TFA is a lightweight and plug-play module that can be easily integrated into existing architectures, which consists of two components, time-dimension attention (TA) and frequency-dimension attention (FA). TA and FA learn to generate differentiated weights for the time-frequency bins according to the training objectives, and assign the differentiated weights dynamically during run-time inference. TFA combines TA and FA by a matrix multiplication operation to generate the T-F attention weights to each T-F spectral component. We evaluate the proposed TFA module as part of two widely used backbone networks, i.e., ResTCN and Transformer, in both speech enhancement and speech recognition experiments. 
% objectives. In addition, we further investigate the effect of TFA module on downstream ASR task. It is observed that the introduce of the TFA module demonstrates significant performance improvements to backbone networks with negligible parameter and computational overheads.

% The remainder of this paper is organized as follows. In Section \ref{sec:2}, we formulate the research problem. In Section \ref{sec:3}, we propose a novel time-frequency attention mechanism for speech enhancement. In Section \ref{sec:4}, we describe the experimental setup. The experimental results are presented in Section \ref{sec:5}. Finally, Section \ref{sec:6} concludes this study.

% \textcolor{blue}{\textbf{Explain the differences with study \cite{zhao2020}}}
% \begin{figure}[t]
% \vskip -0.1in
% \begin{center}
% \centerline{\includegraphics[width=1\columnwidth]{diagram/TFA_2020.png}}
% \caption{}
% \label{fig1}
% \end{center}
% \vskip -0.2in
% \end{figure}

%\textcolor{blue}{Dear Prof. Li, I explain that study \cite{zhao2020} is quite different from ours. Here, we first gives the motivation in original paper \cite{zhao2020} ``\textbf{For speech processing tasks, the contextual information plays a key role. To leverage such information, a general way is to employ an attention mechanism. Note that the contextual information is not limited to the time dimension. For speech, the frequency dimension should be also taken into account.}''}

%\textcolor{blue}{1./ From the motivation in original paper, we can find that the TFA attention in \cite{zhao2020} is still focus on modeling long-range context information, and they just extend self attention to frequency axis. \textcolor{red}{to qiquan: in [41], the attention is also over frequency axis. why  you say that they did not? self-attention is a mechanism to learn context information, our multi-layer network expands the receptive field to cover long-range context information as well. they are just different way of doing the same thing.}\\
%\textbf{\textcolor{blue}{to Prof. Li, yeah, they indeed employ self attention along frequency axis. However, self-attention just computes the similarity between all frequency bands/channels to model the correlations along frequency dimension. What I mean is that the mechanism of self-attention does not allow model to locate which T-F feature is informative, which is what we do in this study. \textcolor{red}{if the self-attention mechanism is applied with our mask-based objective function, the self-attention can also learn to focus on informative T-F features. This is not the problem of self-attention.}They just first capture long-range correlations of time frames then capture the long range correlations of frequency bands. Our TFA employ a time attention to locate the informative speech frames and a frequency-channel attention to locate informative frequency channel. In other words, out attention focuses on \textcolor{red}{Locating informative T-F component} while self-attention focuses on \textcolor{red}{Capturing correlation}. The two ideas are completely different or unrelated.}}} \textcolor{red}{in [41], they use contextual correlation information to locate the informative T-F components. because [41] optimizes directly for a target reference, the masking results are not easily explainable. But we cannot say that [41] doesn't focus on locating T-F components. They do!}

%\textcolor{blue}{2./ From the diagram of TFA attention, we can find that it is based on the self-attention (Q, K, and V) attention mechanism, which can only model the long-range correlation but can not tell model to focus on which informative T-F
%components. For the above point, we already explain it in introduction part, which is also the reason why we propose our TFA attention.\\
%From above two points, it is clearly that our work is quite different (has no relation) from study \cite{zhao2020}. \textcolor{red}{in [39], optimization is done using the target speech as the reference target, while we use `mask' as the target. this is different. The former usually gives better results for domain specific data, the latter works better for cross-domain applications. We follow the latter. But this is not a big novelty because both techniques were used many times in the past. }\\
%\textcolor{blue}{To Prof. Li: Our novelty is not the T-F masks. The mechanism of our work is quiet from self-attention. In introduction part, we introduce that the success of Transformer in speech enhancement mainly stems from the ability of self attention to model the long-range context of speech effectively. However, they can not characterise the salient T-F speech distribution. Although study \cite{zhao2020} perform self attention along time axis and frequency axis, the mechanism of self attention determines that they can not characterise the salient T-F speech distribution. Thus our TFA work is motivated by this. They just perform self attention on both time and frequency axis, so they called it as TFA. In fact, they are quiet different.}\\
%3. In addition, we focus on a lightweight flexible module, whereas the study \cite{zhao2020} just perform self attention along two dimensions. It will never be a lightweight module. \textcolor{red}{the lightweight was introduced in [39] (see section 2.3 in [39], we don't want to claim novelty in our paper - not to invite criticism. please revise to reduce such claims. but rather cite [39] at the appropriate place.}}\\

%\textcolor{blue}{Dear Prof. Li, I have revised them and cite study \cite{zhao2020} at the appropriate place.}

%\textcolor{blue}{\textbf{According above analysis, we can easily find that our work is unrelated to \cite{zhao2020} with respect to motivation, implementation and any others.}}


% /* demo file is intended to serve as a ``starter file''
% for IEEE journal papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8b and later.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
% I wish you the best of success./*

% \hfill mds

% \hfill August 26, 2015

% \subsection{Subsection Heading Here}
% Subsection text here.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

% \subsubsection{Subsubsection Heading Here}
% Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/sconferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.



\section{Ada-FE: Adaptive Front-end}\label{sec3}

% \subsection{Overall Architecture}
% Prior audio front-ends perform feature extraction with fixed filer weights at inference time. Our adaptive front-end (Ada-FE) involves a neural feedback controller, in which a feed-forward neural network dynamically tunes the filter weights in a frame-by-frame fashion. 

% \textcolor{black}{In Figure~\ref{fig2}(a), we illustrate the overall architecture of our Ada-FE. Specifically, Ada-FE includes two Gabor filterbanks, where the first Gabor filterbank (purple box in Figure~\ref{fig2}(a)) employs fixed filter coefficients to maintain band-pass filtering, and the second Gabor filterbank (yellow box in Figure~\ref{fig2}(a)) consists of a set of adaptive filters. The shape of each adaptive filter is adjusted by its Q-factor defined by the ratio of the center frequency and bandwidth. The Q-factor is the only shape parameter of the neural Gabor filters learned from the data, which is produced frame by frame.}

\begin{figure*}[!ht]
% \vspace{-1.3em}
\centering
\captionsetup[sub]{font=large}
\begin{subfigure}[t]{0.95\linewidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{./AdaFEv7.pdf}}
\centerline{\includegraphics[width=0.99\columnwidth]{./AdaFE_Fig/fig3a1.pdf}}
% \vspace{-0.5em}
\caption{}
% \caption{The adaptive front-end (Ada-FE) is optimized jointly with the back-end classifier.}
\label{fig2:1}
\end{subfigure}
\begin{subfigure}[t]{0.95\linewidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{./AdaFES_V5.pdf}}
\centerline{\includegraphics[width=0.99\columnwidth]{./AdaFE_Fig/fig3b1.pdf}}
% \vspace{-0.5em}
\caption{}
% \caption{The simplified adaptive front-end (Ada-FE-S) is optimized jointly with the back-end classifier.}
\label{fig2:2}
\end{subfigure}
\caption{\textcolor{black}{Illustrations of (a) the overall diagram of the adaptive front-end (Ada-FE)~\cite{wickramasinghe2023dnn} and \textcolor{black}{(b) the simplified adaptive front-end (Ada-FE-S), where the hand-crafted level-dependent adaptation function module is removed and the adaptive Q value is completely controlled by the neural adaptive feedback controller (orange box).}}}
\vspace{-0.1em}
\label{fig2}
\end{figure*}

% \textcolor{black}{In Figure~\ref{fig2}\,(a), we illustrate the overall architecture of our Ada-FE. In particular, Ada-FE contains one fixed Gabor filter layer (purple box in Figure~\ref{fig2}\,(a)) and one adaptive Gabor filter layer (yellow box in Figure~\ref{fig2}\,(a)). The fixed Gabor filter layer maintains band-pass filtering with fixed weights. The adaptive filter layer dynamically tunes the filters by two parallel modules (via the Q-factor), i.e., level-dependent adaptation (LDA, green box) and an adaptive feedback controller (AFC, blue box). The Q-factor, which is defined by the ratio of the center frequency and bandwidth, is the only shape parameter of the adaptive Gabor filters learned from the data. For each adaptive filter, its Q value is attained by the summation of $Q^{\text{E}}$ and $Q^{\text{FM}}$ calculated from the LDA and the AFC modules respectively: $Q\{x\}\!=\!Q^{\text{E}}\! +\! Q^{\text{FM}}$. The LDA module employs a hand-crafted piecewise function to calculate the Q value according to the energy of each subband, which is described in detail in Section~\ref{sec3.2}. The AFC exploits a two-layer fully-connected neural network to dynamically tune the Q factors in a frame-wise manner, where the Q factors for the current time frame are calculated from the last time frame, behaving like a recurrent neural network.} 

In Figure~\ref{fig2}\,(a), we illustrate the overall architecture of our Ada-FE~\cite{wickramasinghe2023dnn}, which contains one fixed Gabor filter layer (purple box in Figure~\ref{fig2}\,(a)) and one adaptive Gabor filter layer (yellow box in Figure~\ref{fig2}\,(a)). \textcolor{black}{There is no other processing between these two Gabor filterbanks.} The fixed Gabor filter layer maintains band-pass filtering with fixed weights. The adaptive filter layer dynamically tunes the filters by two parallel modules (via the Q-factor), i.e., level-dependent adaptation (LDA, green box) and an adaptive feedback controller (AFC, blue box). The Q-factor, which is defined by the ratio of the center frequency and bandwidth, is the only shape parameter of the adaptive Gabor filters learned from the data. For each adaptive filter, its Q value is attained by the summation of \textcolor{black}{$\textbf{Q}^{\text{E}}$ and $\textbf{Q}^{\text{FM}}$ calculated from the LDA and the AFC modules respectively: $\textbf{Q}\!=\!\textbf{Q}^{\text{E}}\! +\! \textbf{Q}^{\text{FM}}$. The LDA module employs a hand-crafted piecewise function to calculate the Q value according to the energy of each subband, which is described in detail in Section~\ref{sec3.2}. The AFC exploits a two-layer fully-connected neural network to dynamically tune the Q-factors in a frame-wise manner, where the Q-factors for the current time frame are calculated from the last time frame, behaving like a recurrent neural network.} 

\textcolor{black}{In addition to the broader question of whether front-ends should be adaptive, in this paper we also explore if the role of the feedforward level-dependent adaptation can be subsumed into the adaptive neural feedback controller, i.e., AFC. }
%in this paper, we are particularly interested in learning the adaptive Q value with only the adaptive neural control module. i.e., AFC. 
To this end, we attempt to further simplify Ada-FE by removing the hand-crafted LDA module. Figure~\ref{fig2}\,(b) shows the architecture of the simplified Ada-FE (Ada-FE-S). In this way, the adaptive Q value is completely adjusted by the neural AFC. Exploiting the parameterized Gabor filters, where only the Q-factor is the tunable parameter to adjust the filter response, can place the filters on an appropriate frequency scale and still allow considerable learning flexibility. Moreover, this solution also mitigates the overfitting and stability issues encountered by training unconstrained filters~\cite{sincnet,leaf}. 



% \textcolor{black}{The tunable Q value of the adaptive Gabor filters is controlled by both the feedback flow from the back-end classifier and the input signal level within that subband. Specifically, for each adaptive Gabor filter, two components, $Q^{\text{E}}$ and $Q^{\text{FM}}$, are derived from two parallel modules, i.e, the level-dependent adaption module (LDA) and the adaptive feedback controller (AFC), respectively. Then they are summed to attain the final Q value, i.e., $Q\{x\}\!=\!Q^{\text{E}} + Q^{\text{FM}}$. The $Q^{\text{E}}$ and $Q^{\text{FM}}$are calculated from the outputs of the first and the second Gabor filterbanks, respectively. LDA module is a hand-crafted piecewise function for calculating the Q value according to the energy of each subband, which is described in detail in Section~\ref{sec3.2}. In this paper, we are particularly interested in learning the adaptive Q value without the hand-crafted control module. We attempt to further simplify the Ada-FE by removing the hand-crafted LDA module. Figure~\ref{fig2}(b) shows the architecture of the simplified Ada-FE (Ada-FE-S). In this way, the adaptive Q value is completely adjusted by the neural feedback controller.}

% \textcolor{black}{In Figure \ref{fig2}, we illustrate the overall architecture of the proposed adaptive front-end with a feedback controller, called Ada-FE. Unlike the existing front-ends, it involves a feedback controller, where a neural network is employed to dynamically tune (in a frame-by-frame fashion) the filter weights of the front-end with respect to the input signal level. Figure \ref{fig_struc} displays the diagram of how Q values are dynamically tuned, which shows that the Q values for the current frame are calculated from the last frame. To be specific, the Ada-FE comprises two Gabor filterbanks, where the filters of the first Gabor filterbank (purple box in Figure \ref{fig2}(a)) employ fixed coefficients to maintain band-pass filtering, and the second Gabor filterbank (yellow box in Figure \ref{fig2}(a)) consists of a set of adaptive filters. The shape of each filter is controlled by its Q-factor defined by the centre frequency and bandwidth. The Q-factor is the only parameter of the neural filters learned from the data.}

% \textcolor{black}{The tunable Q value of the adaptive filters is controlled by both the feedback flow from the back-end and the input signal power within that subband. For each adaptive filter, two components, $Q^{\text{E}}\!\{E\!\{x\}\!\}$ and $Q^{\text{FM}}\!\{G\!\{x\}\!\}$, are derived from two paths to obtain the final Q value, i.e., $Q\{x\}\!=\!Q^{\text{E}}\{E\{x\}\}+Q^{\text{FM}}\!\{G\{x\}\}$. The $Q^{\text{E}}\!\{E\!\{x\}\!\}$ is calculated from the energy of the spatially differentiated output, and the $Q^{\text{FM}}\!\{G\!\{x\}\!\}$ is calculated from a neural network based adaptive controller.}



\textcolor{black}{The neural AFC is jointly trained with the parameters of the back-end classifier using gradient-based optimization methods. However, this joint training is not straightforward since the direction of information flow through the neural AFC runs counter to the rest of the neural network. In particular, the input to the controller is from a later stage of the network while the output of the controller feeds to an earlier stage of the network (i.e., information flows from the output of the neural network toward the input through the feedback controller). Consequently, the paths of gradient flow for backpropagation are also split into two and are depicted as Backprop1 and Backprop2 in Figure~\ref{fig2}. This joint optimization solution allows the AFC to be updated without the need for labeled Q-factor values, instead updating with the objective of reducing the classification error.}

% \textcolor{black}{Exploiting the parameterized Gabor filters, where only the Q-factor is the tunable parameter to adjust the filter response, can place the filters on an appropriate frequency scale and still allows considerable flexibility. Moreover, this solution drastically reduces the number of learnable parameters and also mitigates the overfitting and stability issues encountered by training unconstrained filters \cite{sincnet,leaf}. All components involved in Ada-FE are differentiable, and the adaptive feedback controller is jointly trained with the parameters of the back-end classifier using gradient-based optimization methods. However, this joint training is not straight forward since the direction of information flow through the adaptive controller runs counter to the rest of the neural network since the input to the controller is from a later stage of the network and the output of the controller feeds to an earlier stage of the network (i.e., information flows from the output of the neural network towards the input through the feedback controller). Consequently, the paths of gradient flow for backpropagation are also split into two and are depicted as Backprop1 and Backprop2 in Figure 2. This joint optimization solution allows the feedback controller to be updated without the need for labeled Q-factor values, instead updating with the objective of reducing the classification error.}

%not straight forward \textcolor{red}{(I don't understand this sentence.)}and the paths of gradient flow to optimize both the back-end and the adaptive controller are depicted as Backprop1 and Backprop2 in Fig. 2. \textcolor{red}{(backprop happens during training, it updates the weights. however, in this paper, backprop is used as a feedback to adaptive controller? i am not clear here.)} \textcolor{blue}{To Prof. Li: Prof. Ambi said that he will explain this to you.} \textcolor{red}{(please add some text to explain backprop1 and backprop2)}



\textcolor{black}{During training, the adaptive controller is expected to dynamically tune the filters to better extract spectral features for downstream tasks. Since the features that need to be enhanced and the factors that affect this vary across time. i.e., certain bands need to be enhanced at certain times, the Q-factor value must be dynamically adaptive. Each component of the Ada-FE architecture is detailed in the following Sections.}

% \begin{figure}[!htbp]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.85\columnwidth}
% \centerline{\includegraphics[width=0.97\columnwidth]{Gabortime.pdf}}
% \caption{Time impulse responses of Gabor filters}
% \label{fig3:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.88\columnwidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{Gaborfreq.pdf}}
% \caption{Frequency responses of Gabor filters}
% \label{fig3:2}
% \end{subfigure}
% \caption{Illustrations of the time impulse and frequency responses of three Gabor filters, where $f_{c}\!=\!\left\{1000, 2500, 3500\right\}$ and $Q\!=\!\left\{1.0, 1.5, 2.5\right\}$, respectively.}
% % \vspace{-1.0em}
% \label{fig3}
% \end{figure}

\subsection{Gabor Filter Layer}


\textcolor{black}{\textcolor{black}{Let $x$ denote the input waveform which is filtered frame-wise with the first Gabor filter layer comprising} of $N$ fixed band-pass filters, $\textbf{W}^{1}\!=\!\{W^{1}_{1}, W^{1}_{2}, ...,W^{1}_{N}\}$. This produces a feature map with $N$ feature channels, denoted as $\textbf{Y}\in\mathbb{R}^{T\!\times\! N\times\! F}$, where $T$ and $F$ represent the number of time frames and frame length, respectively. \textcolor{black}{Following} this, a $k^{th}$ order spatial differentiation operation~\cite{WickramasingheA19} is applied to $\textbf{Y}$. For uniformly spaced filters, first-order spatial differentiation is implemented by taking the difference between two adjacent feature channels, and higher-order spatial differentiations can be implemented by repeating this operation. The spatial differentiation has been shown to increase the sharpness of filter transition bands and reduce their overlap, which reduces the correlation between adjacent feature coefficients~\cite{wickramasinghe2021replay}.}

\textcolor{black}{The spatial differentiation produces a feature map denoted as $\textbf{S}\!=\!\{\textbf{S}_{i}\}$, where \textcolor{black}{$\mathbf{S}_{i}\!\in\!\mathbb{R}^{(N-k)\!\times\!F}$} represents the feature map for $i^{th}$ time frame, with $i\!\in\!\{1,..., T\}$. The feature map $\mathbf{S}$ is then filtered frame-wise by the second Gabor filter layer, comprising \textcolor{black}{$N\!-\!k$} adaptive band-pass filters, \textcolor{black}{$\textbf{W}^{2}\!=\!\{W^{2}_{1}, W^{2}_{2}, ...,W^{2}_{N-k}\}$.} Here, $W^{1}_{i}\!\in\!\mathbb{R}^{P}$ and $W^{2}_{i}\!\in\!\mathbb{R}^{P}$ denote the $i^{th}$ filters in the first and the second Gabor filter layers, respectively, where $P$ is filter length. Each output frequency channel from the first layer is processed by the corresponding channel in the second layer. The center frequencies of second-layer filters are calculated to match the center frequency shift produced by spatial differentiation operation. 
}

% $\textbf{S}\!\in\!\mathbb{R}^{T\!\times (N-k)\!\times\!F}$ is then passed through the second Gabor filter layer, which consists of $M$ adaptive band-pass filters, $\textbf{W}^{2}\!=\!\{W^{2}_{1}, W^{2}_{2}, ...,W^{2}_{M}\}$, where $M\!=\!N\!-\!k$. Here, $W^{1}_{i}\!\in\!\mathbb{R}^{L}$ and $W^{2}_{i}\!\in\!\mathbb{R}^{P}$ denote the $i^{th}$ filters in the first and the second Gabor filter layers, respectively, where $P$ is filter length. Each output frequency channel from the first layer is processed by the corresponding channel in the second layer. The center frequencies of second-layer filters are calculated to match the center frequency shift produced by spatial differentiation operation. 


% \textcolor{blue}{The output of spatial differentiation $\textbf{S}\!\in\!\mathbb{R}^{T\!\times (N-k)\!\times\!F}$ is then passed through the second Gabor filter layer, which consists of $M$ adaptive band-pass filters, $\textbf{W}^{2}\!=\!\{W^{2}_{1}, W^{2}_{2}, ...,W^{2}_{M}\}$, where $M\!=\!N\!-\!k$. Here, $W^{1}_{i}\!\in\!\mathbb{R}^{L}$ and $W^{2}_{i}\!\in\!\mathbb{R}^{P}$ denote the $i^{th}$ filters in the first and the second Gabor filter layers, respectively, where $P$ is filter length. Each output frequency channel from the first layer is processed by the corresponding channel in the second layer. The center frequencies of second-layer filters are calculated to match the center frequency shift produced by spatial differentiation operation. }


% followed by a $k^{th}$ order spatial differentiation operation~\cite{WickramasingheA19}. The resulting output $S\{x\}$ is then passed through the second Gabor filter layer, which consists of $M$ adaptive band-pass filters, $\textbf{W}^{2}\!=\!\{W^{2}_{1}, W^{2}_{2}, ...,W^{2}_{M}\}$, where $M\!=\!N\!-\!k$. Here, $W^{1}_{i}\!\in\!\mathbb{R}^{L}$ and $W^{2}_{i}\!\in\!\mathbb{R}^{L}$ denote the $i^{th}$ filters in the first and the second Gabor filter layers, respectively, where $L$ is filter length. Each output frequency channel from the first layer is processed by the corresponding channel in the second layer. The center frequencies of second-layer filters are calculated to match the center frequency shift produced by spatial differentiation operation.

% Let $x[n]$ denotes the input signal, it was filtered frame-wise with the first Gabor filter layer and the energy $E\{x\}$ corresponding to each subband or frequency channel was calculated over the spatially differentiated filter output $S\{x\}$. For uniformly spaced filters, first-order spatial differentiation is implemented by taking the difference between two adjacent output channels, and higher-order spatial differentiations can be implemented by repeating this operation. The spatial differentiation has been shown to increase the sharpness of filter transition bands and reduce their overlap, which reduces the correlation between adjacent feature coefficients~\cite{wickramasinghe2021replay}.

% The first Gabor filter layer consists of $N$ fixed band-pass filters, $\textbf{W}^{1}\!=\!\{W^{1}_{1}, W^{1}_{2}, ...,W^{1}_{N}\}$, followed by a $k^{th}$ order spatial differentiation operation \cite{WickramasingheA19}. The resulting output $S\{x\}$ is then passed through the second Gabor filter layer, which consists of $M$ adaptive band-pass filters, $\textbf{W}^{2}\!=\!\{W^{2}_{1}, W^{2}_{2}, ...,W^{2}_{M}\}$, where $M\!=\!N\!-\!k$. Here, $W^{1}_{i}\!\in\!\mathbb{R}^{L}$ and $W^{2}_{i}\!\in\!\mathbb{R}^{L}$ denote the $i^{th}$ filters in the first and the second Gabor filter layers, respectively, where $L$ is filter length. Each output frequency channel from the first layer is processed by the corresponding channel in the second layer. The center frequencies of second-layer filters are calculated to match the center frequency shift produced by spatial differentiation operation.

% \begin{figure}[!tbp]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.85\columnwidth}
% \centerline{\includegraphics[width=0.97\columnwidth]{Gabortime.pdf}}
% \caption{Time impulse responses of Gabor filters}
% \label{fig3:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.88\columnwidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{Gaborfreq.pdf}}
% \caption{Frequency responses of Gabor filters}
% \label{fig3:2}
% \end{subfigure}
% \caption{Illustrations of the time impulse and frequency responses of three Gabor filters, where $f_{c}\!=\!\left\{1000, 2500, 3500\right\}$ and $Q\!=\!\left\{1.0, 1.5, 2.5\right\}$, respectively.}
% % \vspace{-1.0em}
% \label{fig3}
% \end{figure}

% \textcolor{black}{The first Gabor filter layer consists of $N$ band-pass filters, $\textbf{W}^{1}\!=\!\{W^{1}_{1}, W^{1}_{2}, ...,W^{1}_{N}\}$, and the second Gabor filter layer consists of $M$ adaptive filters, $\textbf{W}^{2}\!=\!\{W^{2}_{1}, W^{2}_{2}, ...,W^{2}_{M}\}$. A $k^{th}$ order spatial differentiation operation is conducted between the two layers. Here, $W^{1}_{i}\!\in \mathbb{R}^{L}$ and $W^{2}_{i}\!\in \mathbb{R}^{L}$ denote the $i^{th}$ filters in the first and second Gabor filter layers, respectively, where $L$ is filter length. $N$ and $M$ denote the number of filters in each layer, where $M\!=\!N\!-\!k$. Each output channel from the first layer is processed by the corresponding channel in the second layer. The centre frequencies of second-layer filters are calculated to match the centre frequency shift produced by spatial differentiation operation.}

% and exhibits characteristics similar to those of auditory filters
\begin{figure}[!tbp]
% \vspace{-1.3em}
\centering
\begin{subfigure}[t]{0.85\columnwidth}
\centerline{\includegraphics[width=0.97\columnwidth]{GaborT.pdf}}
\caption{Time impulse responses of Gabor filters}
\label{fig3:1}
\end{subfigure}
\begin{subfigure}[t]{0.85\columnwidth}
\centerline{\includegraphics[width=0.99\columnwidth]{GaborF.pdf}}
\caption{Frequency responses of Gabor filters}
\label{fig3:2}
\end{subfigure}
\caption{Illustrations of the time impulse and frequency responses of three Gabor filters, \textcolor{black}{where $f_{c}\!=\!3000$ and $Q\!=\!\left\{1.5, 2.0, 2.5\right\}$, respectively.}
% $f_{c}\!=\!\left\{1000, 2500, 3500\right\}$ and $Q\!=\!\left\{1.0, 1.5, 2.5\right\}$, respectively.
}
\vspace{-1.0em}
\label{fig3}
\end{figure}

\textcolor{black}{At training time, the first Gabor filter layer, $\textbf{W}^{1}$, is fixed, while the second Gabor filter layer, $\textbf{W}^{2}$, is adaptive. Gabor filter provides a straightforward relationship between its Q-factor and filter response or shape. The time impulse response of the chosen Gabor filter is defined as \cite{gaborfilter}:
% \vspace{-0.5em}
\begin{equation}
W[z]=e^{-(b z)^2} \cos \left(\Omega_c z\right)
\label{eq1}
\end{equation}
where $\Omega_{c}\!=\!2\pi\frac{f_{c}}{f_{s}}$, $b\!=\!{\sqrt{2\pi}}\frac{BW}{2f_{s}}$, and $z\!\in\!\{1,...,\textcolor{black}{P}\}$. $f_{c}$ and $f_{s}$ denote the center frequency and the sampling frequency, respectively, and $BW$ denotes the 3-dB bandwidth, given as $BW\!=\!{\frac{f_{c}}{Q}}$.} \textcolor{black}{Let $\Omega\!=\!\Omega_{c}$, the relationship between the Q-factor and its frequency response at the center frequency is given by 
\begin{equation}
  \left|W\left(\Omega_c\right)\right|=\frac{\sqrt{2} \pi \cdot Q}{\Omega_c}(1+e^{-8 \pi Q^2}).  
\end{equation}
When $Q\!\geq\!0.5$, the factor $e^{-8 \pi Q^2}$ is very small and we obtain $\left|W\left(\Omega_c\right)\right| \approx \frac{\sqrt{2} \pi \cdot Q}{\Omega_c}$. \textcolor{black}{In Figure~\ref{fig3}\,(a) and (b), we illustrate the time impulse responses and frequency responses of three Gabor filters, respectively, \textcolor{black}{where $f_{c}\!=\!3000$ and $Q\!=\!\{1.5,2.0,2.5\}$. It visualizes how the selectivity (gain) and sensitivity (bandwidth) of filters are adjusted via the Q-factor.}}}
% with $f_{c}\!=\!\{1000,2500,3500\}$ and $Q\!=\!\{1.0,1.5,2.5\}$. It visualizes how the selectivity (gain) and sensitivity (bandwidth) of filters are adjusted via the Q-factor.


\textcolor{black}{As shown in Figure~\ref{fig2}\,(a), the implementation of the adaptive Q-factor in Ada-FE involves level-dependent adaptation (LDA) and neural adaptive feedback controller (AFC). The adaptive controller dynamically tunes the Q factors frame by frame. At frame $t-1$, with the input as \textcolor{blue}{$\mathbf{S}_{t-1}\!\in\!\mathbb{R}^{(N-k)\times F}$}, LDA yields the Q value $\mathbf{Q}_{t-1}^{\text{E}}\!\in\!\mathbb{R}^{(N-k)\times1}$. The AFC takes the input as \textcolor{blue}{the output of the adaptive filter layer $\mathbf{C}_{t-1}\!\in\!\mathbb{R}^{(N-k)\!\times\!F}$} and produces the Q value $\mathbf{Q}_{t-1}^{\text{FM}}\!\in\!\mathbb{R}^{(N-k)\times 1}$. The adaptive Q value for $t^{th}$ frame is updated as $\mathbf{Q}_{t}\!=\!\mathbf{Q}_{t-1}^{\text{E}}+\mathbf{Q}_{t-1}^{\text{FM}}$ for \textcolor{black}{$\!N-\!k$} filters.}



\subsection{Adaptive Controller} \label{sec3.2}

% \textcolor{black}{In Ada-FE (Figure~\ref{fig2}\,(a)), the implementation of the adaptive Q-factor involves level-dependent adaptation (LDA) and neural adaptive feedback controller (AFC).}

\begin{figure}[!ht]
 \centering
  % \includegraphics[width=0.91\linewidth]{LDAv3.pdf}
  \includegraphics[width=0.94\linewidth]{./AdaFE_Fig/fig5.pdf}
    \caption{Illustration of the LDA module.}
  \label{fig4}
  \vspace{-0.5em}
\end{figure}

\textbf{Level-Dependent Adaptation}. In our previous study~\cite{WickramasingheA19}, we propose a feed-forward adaptive front-end consisting of two filterbanks for spoofed speech detection, where an LDA module is employed to provide variable selective gain via Q value according to the input levels. As shown in Figure \ref{fig4}, the energy $\mathbf{E}$ corresponding to each subband or frequency channel was calculated over the spatially differentiated filter output $\mathbf{S}$. The Q value $\textbf{Q}^{\text{E}}\{\mathbf{E}\}$ is low for a high input level $\mathbf{E}$, resulting in a filter with decreased gain while increasing the bandwidth, which enables the filter to be less selective and compresses the input signal. \textcolor{black}{In contrast, $\mathbf{Q}^{\text{E}}$ is high for a low input level, which results in a filter with increased gain and reduced bandwidth. It enables the filter to be more selective.} 

% \textcolor{red}{The original description: ``In contrast, $\mathbf{Q}^{\text{E}}$ is high for a low input level, which results in a filter with increased gain while reducing the bandwidth, enabling the filter more selective and amplifying the signal.''}

% In Ada-FE, we replace the filterbanks with the parameterized Gabor filterbanks, which enables the model to dynamically adjust Q via gradient optimization.

% \textbf{Level-Dependent Adaptation}. \textcolor{black}{In our previous study~\cite{WickramasingheA19}, we propose a feed-forward adaptive front-end consisting of two filterbanks for spoofed speech detection, where an LDA module is employed to provide variable selective gain via Q value according to the input levels.} As shown in Figure \ref{fig4}, the Q value $\textbf{Q}^{\text{E}}\{\mathbf{E}\}$ is low for a high input level $\mathbf{E}$, resulting in a filter with decreased gain while increasing the bandwidth, which enables the filter to be less selective and compresses the input signal. In contrast, $\mathbf{Q}^{\text{E}}$ is high for a low input level, which results in a filter with increased gain while reducing the bandwidth, enabling the filter more selective and amplifying the signal. \textcolor{black}{In Ada-FE, we replace the filterbanks with the parameterized Gabor filterbanks, which enables the model to dynamically adjust Q via gradient optimization. }

% \textbf{Level-Dependent Adaptation}. \textcolor{black}{In our previous study~\cite{WickramasingheA19}, we propose a feed-forward adaptive front-end consisting of two filterbanks for spoofed speech detection, where an LDA module is employed to provide variable selective gain via Q value according to the input levels.} As shown in Figure \ref{fig4}, the Q value $Q^{E}\{E\{x\}\}$ is low for a high input level $E\{x\}$, resulting in a filter with decreased gain while increasing the bandwidth, which enables the filter to be less selective and compresses the input signal. In contrast, $Q^{E}$ is high for a low input level, which results in a filter with increased gain while reducing the bandwidth, enabling the filter more selective and amplifying the signal. \textcolor{black}{In Ada-FE, we replace the filterbanks with the parameterized Gabor filterbanks, which enables the model to dynamically adjust Q via gradient optimization. }

% \textbf{Level-Dependent Adaptation} In a recent study\cite{WickramasingheA19}, we proposed a feed-forward adaptive front-end consisting of two filterbanks for spoofed speech detection, which provides variable selective gain via Q value according to the input levels. As shown in Figure 2(b), the Q value $Q^{E}\{E\{x\}\}$ is low for a high input level $E\{x\}$, resulting in a filter with decreased gain while increasing the bandwidth, which enables the filter to be less selective and compresses the input signal. In contrast, $Q^{E}$ is high for a low input level, which results in a filter with increased gain while reducing the bandwidth, enabling the filter more selective and amplifying the signal. Here, we replace the filterbanks with the parameterized Gabor filterbanks, which enables the model to dynamically adjust Q via gradient optimization.

% Let $x[n]$ denotes the input signal, it was filtered frame-wise with the first Gabor filter layer and the energy $E\{x\}$ corresponding to each subband or frequency channel was calculated over the spatially differentiated filter output $S\{x\}$. For uniformly spaced filters, first-order spatial differentiation is implemented by taking the difference between two adjacent output channels, and higher-order spatial differentiations can be implemented by repeating this operation. The spatial differentiation has been shown to increase the sharpness of filter transition bands and reduce their overlap, which reduces the correlation between adjacent feature coefficients~\cite{wickramasinghe2021replay}.

\begin{figure}[!t]
 \centering
  % \includegraphics[width=0.99\linewidth]{AFCv3.pdf}
    \includegraphics[width=0.99\linewidth]{./AdaFE_Fig/fig6.pdf}
  % \vspace{-0.5em}
    \caption{Illustration of the AFC module.}
  \label{fig5}
  % \vspace{-0.9em}
\end{figure}

\textbf{Neural Adaptive Feedback Controller}. \textcolor{black}{As illustrated in Figure~\ref{fig5} (blue dotted box), the neural AFC takes the frame-averaged frequency modulation (FM) component $\mathbf{G}$ of one given subband as the input~\cite{wickramasinghe2021replay}. The input is first processed by a batch normalization (BN) operation and then flows into a two-layer fully-connected (FC) network \textcolor{black}{of size \textcolor{black}{$N-k$}}, with ReLU and Tanh activation respectively. The resulting Q value $\mathbf{Q}^{\text{FM}}$ is added with the Q value $\mathbf{Q}^{\text{E}}$ that depends on input energy (relating to amplitude modulation (AM)) to obtain the final Q value to update the Gabor filters. This is motivated by evidence in psychoacoustic research, which shows that the FM component is complementary to the AM component within the auditory system and captures spectral information for accurate signal-noise separation for speech perception~\cite{nie2004encoding}. Additionally, phase information carried in the FM component reflects reverberation effects and spectral envelope changes due to channel characteristics. The FM component $\mathbf{G}$ is computed from the output of the second Gabor filter layer. For FM calculation, we employ the spectral centroid deviation method~\cite{fm2018}. In this paper, we explore different design choices for the input to the AFC module, which will be described in Section~\ref{sec:5.2}}


% \textbf{Neural Adaptive Feedback Controller}. As illustrated in Figure~\ref{fig5} (blue dotted box), the neural AFC takes the frame-averaged frequency modulation (FM) component $G\{x\}$ of one given subband as the input~\cite{wickramasinghe2021replay}. The input is first processed by a batch normalization (BN) operation and then flows into a two-layer fully-connected (FC) network \textcolor{black}{of size $M$}, with ReLU and Tanh activation respectively. The resulting Q value $Q^{\text{FM}}$ is added with the Q value $Q^{E}$ that depends on input energy (relating to amplitude modulation (AM)) to obtain the final Q value to update the Gabor filters. This is motivated by evidence in psychoacoustic research, which shows that the FM component is complementary to the AM component within the auditory system and captures spectral information for accurate signal-noise separation for speech perception~\cite{nie2004encoding}. Additionally, phase information carried in the FM component reflects reverberation effects and spectral envelope changes due to channel characteristics. The FM component $G\{x\}$ is computed from the output of the second Gabor filter layer. For FM calculation, we employ the spectral centroid deviation method~\cite{fm2018}. In this paper, we explore different design choices for the input to the AFC module, which will be described in Section~\ref{sec:5.2} 

% \textcolor{black}{Given a subband $j\!\in\!\{1,2..,M\}$ with centre frequency $f^{j}_{c}$, $\text{FM}_{j}$ is calculated as:
% \begin{equation}
% \text{FM}_j=\left|\text{SCF}_j-f^{j}_{c}\right|
% \end{equation}
% \vspace{-0.5em}
% \begin{equation}
% \text{SCF}_j=\frac{\sum_{f=f_l}^{f_h} f|Z[f]|}{\sum_{f=f_l}^{f_h}|Z[f]|}
% \end{equation}
% where $\text{SCF}_{j}$ denotes the spectral centroid frequency (SCF) of the subband $j$, $f_{h}$ and $f_{l}$ denote the lowest and highest frequency of subband $j$ and $Z[f]$ the spectral magnitude at frequency $f$. SCF calculates the ``centre of gravity'' of the subband, thus $\text{FM}_{j}$ reflects the changes in selectivity.}

\textcolor{black}{Since an increase of Q value increases the ringing in the filter, some constraints should be considered to ensure the ringing is not too high as it may degrade the performance. To limit the maximum Q values learned by FC1 in a differentiable manner, we employ a scaled and shifted version of an FC layer (FC2) with a diagonal weight matrix and biases followed by the Tanh activation~\cite{wickramasinghe2023dnn}.} \textcolor{black}{Frame-wise processing of the whole utterance is followed by the feature computation module and a back-end classifier.}

% \textcolor{black}{Since an increase of Q value increases the ringing in the filter, some constraints should be considered to ensure the ringing is not too high as it may degrade the performance. To limit the maximum Q values learned by FC1 in a differentiable manner, we employ a scaled and shifted version of an FC layer (FC2) with a diagonal weight matrix and biases followed by the Tanh activation~\cite{wickramasinghe2023dnn}.} \textcolor{black}{The adaptive controller dynamically tunes the Q factors frame by frame. Frame-wise processing of the whole utterance is followed by the feature computation module and a back-end classifier.}

\subsection{Feature Computation}

The adaptive Gabor filter layer is followed by a feature calculation operation, which involves subband energy and spectral envelop centroid magnitude. \textcolor{black}{Information about subband energy can be discerned from the mean magnitude of the Fourier transform of the subband signal.}
%Subband energy can be viewed as the mean magnitude of the Fourier transform of a subband signal. 
The spectral envelope centroid magnitude (CM), which is defined as the weighted average magnitude of the spectral envelope of a given subband signal, is introduced for spoofing attack detection~\cite{8683693}. In this study, we divide the full-band subband spectral envelope (from 0 to 8 kHz) into five octaves, and the CM feature is extracted from each octave. CM from $j^{th}$ octave of the subband signal of a given frame is
% In this study, we divide the full-band subband spectral envelope (from 0 to 8 kHz) into five octaves, and the CM feature is extracted from each octave as the equation (8) in~\cite{8683693}. CM from $j^{th}$ octave of the subband signal of a given frame is calculated as: 
% \vspace{-0.6em}
\begin{equation}
\text{CM}_j=\frac{\sum_{f=f_l^{(j)}}^{f_u^{(j)}} f\cdot|E[f]|}{\sum_{f=f_l^{(j)}}^{f_u^{(j)}} f}
% \vspace{-0.7em}
\end{equation}
\textcolor{black}{where $|E[f]|$ denotes the spectral envelope, $f$ denotes the frequency, and $f_{l}^{(j)}$ and $f_{u}^{(j)}$ denote the lower and upper-frequency bounds of the $j^{th}$ octave  respectively.}

\input{dataset.tex}

\section{Experimental Setup}\label{sec4}

\subsection{Benchmarks and Datasets}
% \textcolor{black}{\textcolor{black}{\textcolor{blue}{To perform a comprehensive investigation, audio front-ends should be evaluated across a diverse and wide range of audio signals~\cite{gong21b_interspeech,gong2022ssast,byol}. To this end, we evaluate the front-ends on eight popular audio and speech datasets, across acoustic scene classification, non-semantic speech task (speech emotion classification, keyword spotting, speaker identification), and music tasks (genre classification).}} For all the tasks, we report classification accuracy as the evaluation metric. Table~\ref{dataset} shows the eight datasets for evaluation and the descriptions are given below.}
\textcolor{black}{\textcolor{black}{\textcolor{black}{To perform a comprehensive investigation, audio front-ends should be evaluated across a diverse and broad range of audio signals. To this end, we evaluate the audio front-ends on eight audio and speech classification benchmarks that are widely used in previous studies~\cite{chen2023beats,audiomae,gong21b_interspeech,gong2022ssast,byol}, including sound event classification, non-semantic speech~\cite{byol,shor2020towards}, and music tasks (genre classification). The speech tasks involve keyword spotting, emotion classification, and speaker identification, which belong to three different aspects of speech~\cite{superb}, i.e., content, paralinguistics, and speaker, respectively.}} For all the tasks, we report classification accuracy as the evaluation metric. Table~\ref{dataset} shows the eight datasets for evaluation and the descriptions are given below.}
\begin{itemize}
\item \textcolor{black}{\textbf{Environmental Sound Classification} (ESC-50) dataset \cite{esc50} is employed for the sound event classification task. \textcolor{black}{ESC-50 comprises $2\,000$ environmental audio recordings grouped into 50 classes, where each recording is 5 seconds long with a sampling rate of $44.1$ kHz} and labeled with only one class. We perform the standard five-fold cross-validation to evaluate performance and report the accuracy~\cite{gong21b_interspeech}.}

\item \textcolor{black}{\textbf{GTZAN} We use the GTZAN dataset~\cite{gtzan} for the music genre classification task. \textcolor{black}{GTZAN comprises $1\,000$ single-label music clips of 10 genre classes, and each audio clip is 30 seconds long with a sampling rate of $22.05$ kHz.} We follow the fault-filtered split setting provided in~\cite{tmm15,byol} to evaluate models and report the accuracy.}

\item \textcolor{black}{\textbf{Free Music Archive} (FMA) dataset~\cite{fma} is employed for music information analysis. In this paper, we also use the FMA dataset for the music genre classification task. \textcolor{black}{Here, we use the small subset of FMA (FMA-S), which consists of $8\,000$ 30-second-long audio clips (sampled at $44.1$ kHz) of 8 genres ($1\,000$ clips per genre).} We follow the standard split provided in the dataset to evaluate models and report the accuracy.}

\item \textcolor{black}{\textbf{CREMA-D} \textcolor{black}{We use the CREMA-D dataset~\cite{cremad} for the speech emotion task, comprising $7\,442$ audio clips (sampled at $16$ kHz) from 43 female and 48 male speakers.} For CREMA-D, we follow the split setting employed in~\cite{byol}, which uses the 7:1:2 split to assign speakers into training, validation, and testing subsets (without duplication) to evaluate models and report accuracy.
}

\item \textcolor{black}{\textbf{IEMOCAP} We also employ the commonly used IEMOCAP dataset \cite{iemocap} for the speech emotion recognition task, which comprises approximately 12 hours of speech clips from 10 speakers. For IEMOCAP, we adopt the SUPERB evaluation benchmark~\cite{superb} that follows the conventional evaluation protocol, where the unbalanced emotion classes are dropped to leave the four classes \textcolor{black}{(angry, happy, sad, and neutral)} with a similar amount of speech clips. The standard five-fold cross-validation evaluation is performed and the accuracy is reported. \textcolor{black}{All audio samples are at a sampling rate of $16$ kHz.}}

\item \textcolor{black}{\textbf{Speech Commands V2 (SPC-V2)} We use the Google SPC-V2 dataset \cite{GSCv2} for the keyword spotting task. SPC-V2 contains a total of $105\,829$ 1-second-long utterances from $2\,618$ speakers, which are recorded at a sampling rate of 16 kHz and labeled with one of the 35 common speech commands. Following the standard split setting provided in the dataset~\cite{GSCv2}, we split the dataset into training, validation, and test sets, with $84\,843$, $9\,981$, and $11\,005$ utterances, respectively. Here, we focus on the 35-class classification task and repost the accuracy, no class re-balancing operation is applied during testing and training.}

\item \textcolor{black}{\textbf{Speech Commands V1 (SPC-V1)} We also employ the Google SPC-V1 \cite{GSCv2} for the keyword spotting task. \textcolor{black}{SPC-V1 contains a total of $64\,721$ 1-second-long utterances from $1\,881$ speakers, which are recorded at a sampling rate of $16$ kHz and labeled with one of the 30 common speech commands.} We follow the official split setting provided in the dataset~\cite{GSCv2} and focus on the 30-class classification task. We report the accuracy, no class re-balancing operation is applied during testing and training.}

\item \textcolor{black}{\textbf{VoxCeleb1} We employ the VoxCeleb1 dataset~\cite{voxceleb1} for the speaker identification (speaker ID) task, which consists of a total of $153\,516$ recordings from $1\,251$ speakers. All the speech signals are sampled at 16 kHz. Following the default identification split provided by the VoxCeleb1, the dataset is split into a training set with $138\,316$ utterances, a development set with $6\,904$ utterances, and a test set with $8\,251$ utterances, respectively. All the recordings of the three sets are from the same $1\,251$ speakers.}
\end{itemize}


\subsection{Implementation Details}

\textcolor{black}{Following~\cite{chen2023beats,audiomae,gong21b_interspeech,gong2022ssast,byol}, the raw audio signals are resampled to $16$ kHz.} We segment audio and speech recordings into 11 ms long non-overlapping time frames for frame-wise \textcolor{black}{adaptation}. For the fixed Gabor filter layer, we employ $N\!=\!40$ normalized Gabor filters (in Hz frequency scale) with a length of $L\!=\!150$ samples. The filtered signal is then passed through a first-order ($k\!=\!1$) spatial differentiation operation~\cite{8683693}, which generates \textcolor{black}{$N\!-\!1=39$} output channels for each time frame. They are then fed into the adaptive Gabor filter layer comprised of \textcolor{black}{$N-1\!=\!39$} filters to tune Q values on a frame-by-frame basis.

% The fixed Gabor filterbank contains $N\!=\!40$ normalized Gabor filters (in Hz frequency scale) of length $L\!=\!150$ coefficients. The first-order ($k\!=\!1$) spatial differentiation is then applied to the filtered signal. The resulting 39 ($M\!=\!N\!-\!k$) output channels per framed signal are fed into the adaptive Gabor filter layer containing $M\!=\!39$ filters.

\textcolor{black}{In our experiments, we compare our Ada-FE and Ada-FE-S with recent TD-fbanks and LEAF, where an independent front-end is trained jointly with the back-end for each task.} \textcolor{black}{Since LEAF demonstrates state-of-the-art results over Mel-filterbanks and prior learnable front-ends in diverse audio and speech tasks, we mainly focus on the comparison to LEAF.} For the TD-fbansks baseline, here we report the best results listed in the work~\cite{leaf}. We use the official code implementation of the LEAF baseline to run all the experiments. We follow LEAF~\cite{leaf} evaluating front-ends with the same common network architecture, which comprises a front-end followed by a back-end classifier. To ensure fair comparisons, our comparative study keeps the same experiment settings.

% Our experiments employ the TD-fbanks and LEAF as the baselines. Here, we mainly focus on the comparison of the Ada-FE and Ada-FE-S with LEAF, since LEAF demonstrates state-of-the-art performance in diverse audio and speech tasks. For TD-fbansks, the best results reported in \cite{leaf} are listed here. Our experiments adopt the official code implementation for the LEAF baseline model and keep the same experiment settings for a fair comparison. Consistent with LEAF, we employ the same common network architecture, which consists of a front-end and a back-end classifier. 

\textbf{EfficientNets}~\cite{efficientnet} is a family of CNNs, that demonstrate substantial superiority in accuracy and efficiency over previous convolutional network architectures. The superiority mainly comes from two design choices: 1) EfficientNets scale up neural networks by all three dimensions (i.e., input resolution, width, and depth) with a compound coefficient, which performs better than scaling only one dimension; 2) EfficientNets use an efficient building block, i.e., mobile inverted bottleneck convolution (MBConv) with squeeze-and-excitation optimization~\cite{senet}. Following LEAF paper~\cite{leaf}, we use EfficientNet-B0, i.e., a basic version of EfficientNets, to perform the back-end classifier.

% and involves 5.3M parameters

\textbf{MobileNetV2}~\cite{mobilenetv2} is a family of highly efficient mobile convolutional networks. The core layer module of the MobileNetV2 is the inverted residual with linear bottleneck, which allows one to significantly reduce memory requirement at inference time. To conduct comprehensive comparison experiments, we also study LEAF and Ada-FE (as well as Ada-FE-S) on the MobileNetV2 backbone network. In our experiments, we employ the MobileNetV2-100 as the back-end classifier\footnote{The code implementation for MobileNetV2-100 is adopted from PyTorch Image Models (timm) library, which is available here \url{https://github.com/huggingface/pytorch-image-models}.}.

% MobileNetV2 is a family of highly efficient mobile convolutional network models. The core layer module of MobileNetV2 is the inverted residual with linear bottleneck, which allows one to significantly reduce memory requirement at inference time. In this study, we employ the MobileNetV2-100\footnote{The code implementation for MobileNetV2-100 is adopted from PyTorch Image Models (timm) library, which is available here \url{https://github.com/huggingface/pytorch-image-models}.} as the back-end classifier.

\textbf{Training details}. We employ the cross-entropy as the loss function and the~\textit{Adam} optimizer for the gradient optimization, with the parameters~\cite{Adam}, i.e., $\!\beta_{1}\!=\!0.9$, $\!\beta_{2}\!=\!0.98$, and $\epsilon\!=\!10^{-9}$. The models are trained for 150 epochs, with a batch size of 64 utterances. The initial learning rate is set to $1.0\!\times\!10^{-4}$, with a weight decay of $1.0\!\times\! 10^{-4}$. To handle the variable duration of the input audio and speech recordings, following LEAF~\cite{leaf} we employ randomly sampled 1-second clips for training. At the inference stage, we use the entire recordings for prediction, dividing them into consecutive non-overlapping 1-second clips and taking the average of the output logits over clips to attain the final prediction or classification results. Moreover, as a more common design choice in speaker identification~\cite{voxceleb1}, we also train models on randomly sampled 3-second segments and employ the same method to perform inference. It is worth noting that no data augmentation is applied to the raw audio waveforms at training time for fair comparisons. \textcolor{black}{We repeat each experiment three times and report the mean results.}

% The experiments were conducted on an NVIDIA Tesla P100-SXM2-16GB GPU.


% The models are trained for 150 epochs, with a batch size of 64 utterances. For each epoch, we randomly shuffle the order of the audio samples. The cross-entropy is used as the loss function. The \textit{Adam} algorithm is used for the gradient optimization, with the parameters \cite{Adam}, i.e., $\!\beta_{1}\!=\!0.9$, $\!\beta_{2}\!=\!0.98$, and $\epsilon\!=\!10^{-9}$. The initial learning rate is set to $1.0\!\times\!10^{-4}$, with a weight decay of $1.0\!\times\! 10^{-4}$. \textcolor{black}{To address the variable length of the input speech utterances, following LEAF~\cite{leaf} we employ randomly selected 1-second segments for training. At inference time, the full-length utterances are split into consecutive non-overlapping 1-second segments and the output logits over segments are averaged to obtain the final classification results. In addition, as a more common setting choice in speaker identification \cite{voxceleb1}, here we also employ randomly selected 3-second segments for training, and the same inference method is used. It should be noted that no data augmentation or pre-processing step is applied to the raw speech waveforms at training time for fair comparisons. The experiments were conducted on an NVIDIA Tesla P100-SXM2-16GB GPU.}


% To address the variable length of the input speech utterances for the speaker identification task, following \cite{leaf} we employ randomly selected 1-second segments for training. For inference, the full-length utterances are split into consecutive non-overlapping 1-second segments and the output logits over segments are averaged to obtain the final classification results. In addition, as a more common setting choice in speaker identification~\cite{voxceleb1}, here we also employ randomly selected 3-second segments for training, and the same inference method is used. It should be noted that no data augmentation or pre-processing step is applied to the raw speech waveforms at training time for fair comparisons. The experiments were conducted on an NVIDIA Tesla P100-SXM2-16GB GPU.

\section{Experimental Results}\label{sec5}

\subsection{Q-Value Analysis}

 
% is consistent with the response of cochlear to varying input levels, 

% which is consistent with those shown in Figures 4-6.

% \begin{figure}[!htbp]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.50\columnwidth}
% \centerline{\includegraphics[width=1.0\columnwidth]{E_SPCV2.pdf}}
% \caption{The Energy curve}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.48\columnwidth}
% \centerline{\includegraphics[width=0.98\columnwidth]{Q_SPCV2.pdf}}
% \caption{The Q value curve}
% \label{fig1:2}
% \end{subfigure}
% \caption{Illustration of how (a) energy and (b) Q value change frame by frame for the Gabor filter with the center frequency of $3.27$ KHz at inference time. Ada-FE infers one 1-second speech utterance on the keyword spotting (SPC-V2).}
% \vspace{-0.5em}
% \label{figQ1}
% \end{figure}

\begin{figure}[!h]
% \vspace{-1.3em}
\centering
\begin{subfigure}[t]{0.49\columnwidth}
\centerline{\includegraphics[width=1.0\columnwidth]{VoxCeleb1_E4.pdf}}
\caption{The Energy curve}
\label{fig1:1}
\end{subfigure}
\begin{subfigure}[t]{0.49\columnwidth}
\centerline{\includegraphics[width=0.98\columnwidth]{Voxceleb1_Q4_V1.pdf}}
\caption{The Q value curve}
\label{fig1:2}
\end{subfigure}
\caption{\textcolor{black}{Illustration of how (a) energy and (b) Q value change frame by frame for the Gabor filter with a center frequency of $1.12$ kHz (low). Ada-FE infers one 1-second speech segment on the speaker ID task (VoxCeleb1).}}
\vspace{-0.5em}
\label{figQ1}
\end{figure}

\begin{figure}[!h]
% \vspace{-1.3em}
\centering
\begin{subfigure}[t]{0.505\columnwidth}
\centerline{\includegraphics[width=1.0\columnwidth]{E_Voxceleb1.pdf}}
\caption{The Energy curve}
\label{fig1:1}
\end{subfigure}
\begin{subfigure}[t]{0.48\columnwidth}
\centerline{\includegraphics[width=0.99\columnwidth]{Q_Voxceleb1.pdf}}
\caption{The Q value curve}
\label{fig1:2}
\end{subfigure}\caption{\textcolor{black}{Illustration of how (a) energy and (b) Q value change frame by frame for the Gabor filter with a center frequency of $3.46$ kHz (mid). Ada-FE infers one 1-second speech segment on the speaker ID task (VoxCeleb1).}}
\vspace{-0.5em}
\label{figQ2}
\end{figure}

% \begin{figure}[!h]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.49\columnwidth}
% \centerline{\includegraphics[width=1.0\columnwidth]{VoxCeleb1_E21.pdf}}
% \caption{The Energy curve}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.49\columnwidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{Voxceleb1_Q21.pdf}}
% \caption{The Q value curve}
% \label{fig1:2}
% \end{subfigure}
% \caption{\textcolor{blue}{Illustration of how (a) energy and (b) Q value change frame by frame for the Gabor filter with the center frequency of $4.44$ kHz. Ada-FE infers one 1-second speech segment on the speaker ID task (VoxCeleb1).}}
% \vspace{-0.5em}
% \label{figQ21}
% \end{figure}

% \begin{figure}[!h]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.49\columnwidth}
% \centerline{\includegraphics[width=1.0\columnwidth]{VoxCeleb1_E25.pdf}}
% \caption{The Energy curve}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.49\columnwidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{Voxceleb1_Q25.pdf}}
% \caption{The Q value curve}
% \label{fig1:2}
% \end{subfigure}
% \caption{\textcolor{blue}{Illustration of how (a) energy and (b) Q value change frame by frame for the Gabor filter with the center frequency of $5.81$ kHz. Ada-FE infers one 1-second speech segment on the speaker ID task (VoxCeleb1).}}
% % \vspace{-0.5em}
% \label{figQ21}
% \end{figure}

% \begin{figure}[!h]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.49\columnwidth}
% \centerline{\includegraphics[width=1.0\columnwidth]{VoxCeleb1_E26.pdf}}
% \caption{The Energy curve}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.49\columnwidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{Voxceleb1_Q26.pdf}}
% \caption{The Q value curve}
% \label{fig1:2}
% \end{subfigure}
% \caption{\textcolor{blue}{Illustration of how (a) energy and (b) Q value change frame by frame for the Gabor filter with the center frequency of $5.42$ kHz. Ada-FE infers one 1-second speech segment on the speaker ID task (VoxCeleb1).}}
% % \vspace{-0.5em}
% \label{figQ21}
% \end{figure}

\begin{figure}[!h]
% \vspace{-1.3em}
\centering
\begin{subfigure}[t]{0.49\columnwidth}
\centerline{\includegraphics[width=1.0\columnwidth]{VoxCeleb1_E30_V5.pdf}}
\caption{The Energy curve}
\label{fig1:1}
\end{subfigure}
\begin{subfigure}[t]{0.49\columnwidth}
\centerline{\includegraphics[width=0.99\columnwidth]{Voxceleb1_Q30_V2.pdf}}
\caption{The Q value curve}
\label{fig1:2}
\end{subfigure}
\caption{\textcolor{black}{Illustration of how (a) energy and (b) Q value change frame by frame for the Gabor filter with a center frequency of $5.81$ kHz (high). Ada-FE infers one 1-second speech segment on the speaker ID task (VoxCeleb1).}}
\vspace{-0.5em}
\label{figQ21}
\end{figure}

% \begin{figure}[!h]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.5\columnwidth}
%     \centerline{\includegraphics[width=1.0\columnwidth]{VoxCeleb1_E34.pdf}}
% \caption{The Energy curve}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.485\columnwidth}
% \centerline{\includegraphics[width=0.98\columnwidth]{Voxceleb1_Q34.pdf}}
% \caption{The Q value curve}
% \label{fig1:2}
% \end{subfigure}
% \caption{\textcolor{blue}{Illustration of how (a) energy and (b) Q value change frame by frame for the Gabor filter with the center frequency of $6.98$ kHz. Ada-FE infers one 1-second speech segment on the speaker ID task (VoxCeleb1).}}
% \vspace{-0.5em}
% \label{figQ22}
% \end{figure}

% \begin{figure}[!h]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.49\columnwidth}
%     \centerline{\includegraphics[width=1.0\columnwidth]{Voxceleb2_AveE.pdf}}
% \caption{The Energy curve}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.49\columnwidth}
% \centerline{\includegraphics[width=0.98\columnwidth]{Voxceleb1_AveQ1.pdf}}
% \caption{The Q value curve}
% \label{fig1:2}
% \end{subfigure}
% \caption{\textcolor{blue}{Illustration of how (a) mean energy and (b) Q value (averaged across all frequency feature channels) change frame by frame. Ada-FE infers one 1-second audio segment on the the speaker ID task (VoxCeleb1).}}
% % \vspace{-0.5em}
% \label{figQ3}
% \end{figure}


% \begin{figure}[!h]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.505\columnwidth}
%     \centerline{\includegraphics[width=1.0\columnwidth]{FMA_E.pdf}}
% \caption{The Energy curve}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.48\columnwidth}
% \centerline{\includegraphics[width=0.98\columnwidth]{FMA_Q.pdf}}
% \caption{The Q value curve}
% \label{fig1:2}
% \end{subfigure}
% \caption{\textcolor{blue}{Illustration of how (a) energy and (b) Q value change frame by frame for the Gabor filter with the center frequency of $4.25$ KHz. Ada-FE infers one 1-second audio segment on the music genre classification task (FMA-Small).} \textcolor{red}{if the Figure 9 is necessary?}}
% % \vspace{-0.5em}
% \label{figQ41}
% \end{figure}

% \begin{figure}[!h]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.505\columnwidth}
%     \centerline{\includegraphics[width=1.0\columnwidth]{FMA_AveE.pdf}}
% \caption{The Energy curve}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.48\columnwidth}
% \centerline{\includegraphics[width=0.98\columnwidth]{FMA_AveQ.pdf}}
% \caption{The Q value curve}
% \label{fig1:2}
% \end{subfigure}
% \caption{\textcolor{blue}{Illustration of how (a) energy and (b) Q value (averaged across all frequency feature channels) change frame by frame at inference time. Ada-FE infers one 1-second audio segment on the music genre classification task (FMA-Small).}}
% % \vspace{-0.5em}
% \label{figQ4}
% \end{figure}



% \begin{figure}[!h]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.49\columnwidth}
%     \centerline{\includegraphics[width=1.0\columnwidth]{test.pdf}}
% \caption{Initialized response}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.49\columnwidth}
% \centerline{\includegraphics[width=0.98\columnwidth]{test1.pdf}}
% \caption{Learned response}
% \label{fig1:2}
% \end{subfigure}
% % \hspace{1cm}
% \begin{subfigure}[t]{0.5\columnwidth}
% \centerline{\includegraphics[width=0.99\columnwidth]{GTZAN_Learned.pdf}}
% \caption{Learned response}
% \label{fig1:2}
% \end{subfigure}
% \caption{\textcolor{blue}{Illustration of frequency responses of (a) the initialized and (b) the learned Gabor filers in LEAF. Center frequency and bandwidth are respectively represented by the solid line and the shaded area.} \textcolor{red}{if the Figure 11(c) is necessary?}}
% % \vspace{-0.5em}
% \label{figQ6}
% \end{figure}

% \begin{figure}[!htbp]
%  \centering
%   \includegraphics[width=0.55\linewidth]{LEAFQ.pdf}
%   % \vspace{-0.5em}
%     \caption{\textcolor{blue}{The curves of initialized and learned Q values for LEAF front-end. Unlike Ada-FE, the trainable shape parameters of the Gabor filters in LEAF are center frequency and bandwidth. The Q value for each Gabor filter is calculated by dividing the center frequency (initialized or learned) by the bandwidth (initialized or learned). \textcolor{red}{Which one of Figure 10 and Figure 11 will be better?}}}
%   \label{figQ5}
%   % \vspace{-0.5em}
% \end{figure}

% \begin{figure}[!h]
% % \vspace{-1.3em}
% \centering
% \begin{subfigure}[t]{0.505\columnwidth}
%     \centerline{\includegraphics[width=1.0\columnwidth]{E_FMA.pdf}}
% \caption{The Energy curve}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{0.48\columnwidth}
% \centerline{\includegraphics[width=0.98\columnwidth]{Q_FMA.pdf}}
% \caption{The Q value curve}
% \label{fig1:2}
% \end{subfigure}
% \caption{Illustration of how (a) energy and (b) Q value change frame by frame for the Gabor filter with the center frequency of $4.25$ KHz at inference time. Ada-FE infers one 1-second audio segment on the music genre classification task (FMA-Small).}
% \vspace{-0.5em}
% \label{figQ3}
% \end{figure}

% \begin{figure}[!htbp]
% % \vspace{-1.3em}
% \centering
% \captionsetup[sub]{font=normalsize}
% \begin{subfigure}[t]{\columnwidth}
%     \centerline{\includegraphics[width=0.95\columnwidth]{AFC2v3.pdf}}
% \caption{}
% % \caption{AFC with the energy of each subband or channel as the input.}
% \vspace{1.5em}
% \label{fig1:1}
% \end{subfigure}
% \begin{subfigure}[t]{\columnwidth}
% \centerline{\includegraphics[width=0.95\columnwidth]{AFCSv4.pdf}}
% % \caption{AFC with the concatenation of the energy and FM as the input.}
% \caption{}
% \label{fig1:2}
% \end{subfigure}
% \caption{\textcolor{black}{The illustrations of the AFC with (a) the energy of each subband (AFC-Energy) and (b) the concatenation of the energy and FM as the input (AFC-Energy+FM), respectively. $\bigoplus$ represents the concatenation operation.}}
% % \vspace{-1.5em}
% \label{figafc}
% \end{figure}

\textcolor{black}{In Figures~\ref{figQ1}-\ref{figQ21}, we illustrate three examples to visualize how the value of Q-factor changes as the energy frame by frame at inference time, where our Ada-FE (with EfficientNet-B0 back-end classifier) infers 1-second audio segments for speaker ID (VoxCeleb1). The Q value and energy curves displayed in Figures~\ref{figQ1}-\ref{figQ21} correspond to the adaptive Gabor filters with center frequencies of $1.12$ kHz (low), $3.46$ kHz (mid), and $5.81$ kHz (high), respectively. It can be clearly observed that, overall, the Q-factor curve shows an opposite trend to the energy curve across different frequency channels. Given a frame with a low energy value, we expect Ada-FE to dynamically produce a corresponding high Q-factor value, leading to a high gain (sensitivity) and a low bandwidth (selectivity), and conversely, expect a low Q value, which is consistent with those shown in Figures~\ref{figQ1}-\ref{figQ21}. These observations about the relationship between energy and Q values over time illustrate how the proposed model adaptively responds to time-varying signal characteristics.}

\begin{figure}[!htbp]
% \vspace{-1.3em}
\centering
\captionsetup[sub]{font=normalsize}
\begin{subfigure}[t]{\columnwidth}
    % \centerline{\includegraphics[width=0.95\columnwidth]{AFC2v3.pdf}}
    \centerline{\includegraphics[width=0.95\columnwidth]{./AdaFE_Fig/fig10a.pdf}}
\caption{}
% \caption{AFC with the energy of each subband or channel as the input.}
\vspace{1.5em}
\label{fig1:1}
\end{subfigure}
\begin{subfigure}[t]{\columnwidth}
% \centerline{\includegraphics[width=0.95\columnwidth]{AFCSv4.pdf}}
\centerline{\includegraphics[width=0.95\columnwidth]{./AdaFE_Fig/fig10b.pdf}}
% \caption{AFC with the concatenation of the energy and FM as the input.}
\caption{}
\label{fig1:2}
\end{subfigure}
\caption{\textcolor{black}{The illustrations of the AFC with (a) the energy of each subband (AFC-Energy) and (b) the concatenation of the energy and FM as the input (AFC-Energy+FM), respectively. $\bigoplus$ represents the concatenation operation.}}
% \vspace{-1.5em}
\label{figafc}
\end{figure}

% \textcolor{black}{In Figure~\ref{figQ2}, we illustrate an example that visualize how the value of Q-factor changes as the energy frame by frame at inference time, where our Ada-FE (with EfficientNet-B0 back-end classifier) infers 1-second audio segments for speaker ID (VoxCeleb1) task. The Q-factor value and energy curves displayed in Figure~\ref{figQ2} correspond to the adaptive Gabor filter with center frequencies of 3.46 KHz. It can be clearly observed that overall, the Q-factor curve shows an opposite trend to the energy curve. Given a frame with a low energy value, we expect Ada-FE to dynamically produce a corresponding high Q-factor value, leading to a high gain (sensitivity) and a low bandwidth (selectivity), and conversely, expect a low Q value, which is consistent with those shown in Figures~\ref{figQ2}. The relationship established in energy curves and Q value curves confirms the effectiveness of our model design.}

% \textcolor{blue}{In Figures~\ref{figQ1}-\ref{figQ3}, we illustrate four examples that visualize how the value of Q-factor changes as the energy frame by frame at inference time, where our Ada-FE (with EfficientNet-B0 back-end classifier) infers 1-second audio segments for speaker ID (VoxCeleb1). The Q value and energy curves displayed in Figures~\ref{figQ1}-\ref{figQ21} correspond to the adaptive Gabor filters with center frequencies of $1.12$ kHz (low), $3.46$ kHz (mid), and $5.81$ kHz (high), respectively. Figures~\ref{figQ3} exhibit the curves of the  mean Q value and energy (averaged across all frequency feature channels). It can be clearly observed that, overall, the Q-factor curve shows an opposite trend to the energy curve across different frequency channels. Given a frame with a low energy value, we expect Ada-FE to dynamically produce a corresponding high Q-factor value, leading to a high gain (sensitivity) and a low bandwidth (selectivity), and conversely, expect a low Q value, which is consistent with those shown in Figures~\ref{figQ1}-\ref{figQ3}. The relationship established in energy and Q value curves confirms the effectiveness of our model design.}

% Figures~\ref{figQ3}-\ref{figQ4} exhibit the curves of the Q value and energy (averaged across all frequency feature channels) for speaker ID and music genre classification, respectively. It can be clearly observed that overall, the Q-factor curve shows an opposite trend to the energy curve. Given a frame with a low energy value, we expect Ada-FE to dynamically produce a corresponding high Q-factor value, leading to a high gain (sensitivity) and a low bandwidth (selectivity), and conversely, expect a low Q value, which is consistent with those shown in Figures~\ref{figQ2}-\ref{figQ4}. The relationship established in energy and Q value curves confirms the effectiveness of our model design. Figure~\ref{figQ5} shows the curves of initialized and learned Q values. Similar to~\cite{anderson2023learnable}, we observe that the learned filters do not differ substantially from their initialized values.}

% In Figures~\ref{figQ1}-\ref{figQ3}, we illustrate three examples that visualize how the value of Q-factor changes as the energy frame by frame at inference time, where our Ada-FE (with EfficientNet-B0 back-end classifier) respectively infers 1-second audio segments for keyword spotting (SPC-V2), speaker ID (VoxCeleb1), and music genre classification (FMA-Small) tasks. The Q value and energy curves displayed in Figures~\ref{figQ1}-\ref{figQ3} correspond to the adaptive Gabor filters with center frequencies of 3.27 KHz, 3.46 KHz, and 4.25 KHz, respectively. It can be clearly observed that overall, the Q value curve shows an opposite trend to the energy curve. Given a frame with a low energy value, we expect Ada-FE to dynamically produce a corresponding high Q value, leading to a high gain (sensitivity) and a low bandwidth (selectivity), and conversely, expect a low Q value, which is consistent with those shown in Figures~\ref{figQ1}-\ref{figQ3}. The relationship established in energy curves and Q value curves confirms the effectiveness of our model design.

\input{Ablation}
% \subsection{Ablation Studies}
\subsection{\textcolor{black}{\textcolor{black}{Analyses of} Simplified Ada-FE}}\label{sec:5.2}

% \textcolor{black}{In this section, we perform comprehensive experiments to evaluate our Ada-FE and simplified Ada-FE (Ada-FE-S) across six audio and speech \textcolor{blue}{datasets}, with the EfficientNet-B0 as the back-end. For Ada-FE-S, we explore different design choices for the input to the adaptive feedback controller (AFC). We refer to the Ada-FE-S with the AFC that takes the original FM input (shown in Figure \ref{fig5}) as the Ada-FE-S-F. As shown in Figures \ref{figafc}(a) and (b), we also attempt to employ the energy of each subband and the concatenation of the energy and FM as the input to AFC, which are referred as Ada-FE-E and Ada-FE-EF, respectively.} 

In this section, we comprehensively evaluate the simplified Ada-FE (Ada-FE-S) across sound event classification, music genre classification, speech emotion classification, and keyword spotting tasks, with EfficientNet-B0 as the back-end. Built upon Ada-FE-S, we probe different design choices for the input to the neural adaptive feedback controller (AFC). We refer to the Ada-FE-S with the original AFC that takes the FM input (shown in Figure~\ref{fig5}) as the \textcolor{black}{Ada-FE-S-FM}. \textcolor{black}{As shown in Figure~\ref{figafc}\,(a) and (b), we also attempt to employ the energy of each subband and the concatenation of the energy and FM as the input to the AFC, which are referred to as \textcolor{black}{Ada-FE-S-Eg and Ada-FE-S-EgFM}, respectively.}

\textcolor{black}{The evaluation results are given in Table~\ref{ablation}, where Top-1 and Top-5 accuracy are presented. Top-1 and Top-5 accuracy respectively measure the proportion of times the top prediction matches the true label and the proportion of times true label is among the top five predictions. We can observe that Ada-FE-S-FM, proposed simplified (S) model with Frequency Modulation (FM) features as input, achieves comparable or better performance than the original Ada-FE on all the tasks, demonstrating that the Q-factor adaptive learning in Ada-FE benefits from removing the hand-crafted module. Among Ada-FE-S-FM (with FM input), Ada-FE-S-Eg (with Energy input, Figure~\ref{figafc}\,(a)), and Ada-FE-S-EgFM (with Energy and FM inputs, Figure~\ref{figafc}\,(b)), overall, Ada-FE-S-FM and Ada-FE-S-EgFM are superior to Ada-FE-S-Eg. We also can observe that the fusion of energy and FM features is not consistently beneficial for Ada-FE-S. Ada-FE-S-EgFM outperforms Ada-FE-S-FM on CREMA-D, SPC-V1, and SPC-V2 benchmarks, while Ada-FE-S-FM achieves better performance on ESC-50, GTZAN, and FMA-Small benchmarks.}

% We can observe that Ada-FE-S-F achieves comparable or better performance than Ada-FE on all the tasks, demonstrating that the Q-factor adaptive learning in Ada-FE can benefit from removing the hand-crafted module. \textcolor{black}{Among Ada-FE-S-F, Ada-FE-S-E, and Ada-FE-S-FE, overall, Ada-FE-S-F and Ada-FE-S-EF are superior to Ada-FE-S-E and the fusion of energy and FM features is not consistently beneficial for Ada-FE-S.} \textcolor{black}{Ada-FE-S-EF outperforms Ada-FE-S-F on CREMA-D, SPC-V1, and SPC-V2 datasets, while Ada-FE-S-F achieves better performance on ESC-50, GTZAN, and FMA-Small datasets.}

% in Ada-FE can benefit from removing the hand-crafted LDA module.

\textcolor{black}{Further, an interesting question arises: ``May we only use a single adaptive Gabor filter layer for Ada-FE?''. To answer this question, we explore removing the fixed Gabor filters (purple box in Figure~\ref{fig2}\,(a)) to study the effect of this change to assess its efficacy. Similarly, we employ the FM, the energy, and the concatenation of the energy and FM as the input to AFC. The evaluation results are denoted in gray (Table~\ref{ablation}). The comparison results to Ada-FE-S confirm the role of the fixed Gabor filters. Again, it can be observed that the FM and the concatenation input of energy and FM provide better performance than the energy input.} %\textcolor{black}{In addition, we can find that the adaptive front-end with only a single adaptive Gabor filter layer consistently benefits from the fusion of FM and energy input on all datasets.}


% \textcolor{black}{Built upon Ada-FE-S, we attempt to further remove the first fixed Gabor filters (purple box in Figure \ref{fig2} (a)) to study the effect of this change to assess the efficacy. Similarly, we adopt FM, energy, and the concatenation of the energy and FM as the input to AFC. The evaluation results are denoted in gray. The comparison results to Ada-FE-S confirm the effectiveness of the first fixed Gabor filters. Again, it can be observed that the FM input and the concatenation input of energy and FM provide better performance than energy input. In addition, we can find that the front-end with one adaptive Gabor filter layer benefits from the fusion of FM and energy input on all datasets. }

% \textcolor{black}{Here, we perform in-depth ablation studies to demonstrate the efficacy of our Ada-FE design. In the Ada-FE, the Q value for the adaptive Gabor filter layer is contributed by its core component, an adaptive feedback controller (AFC), as well as a level-dependent adaptation (LDA) module. To assess the contribution or necessity of LDA, we perform experiments to study the effect of removing the LDA module (green box in Figure \ref{fig2}) from the Ada-FE.}



% \input{./Top1Acc}
% \input{./Top5Acc}

% \textcolor{black}{Before the adaptive Gabor filter layer, the input time frame is passed through a fixed Gabor filter layer followed by a spatial differentiation operation. Further, we remove the fixed Gabor filter layer (including the spatial differentiation) to study the effect of this difference to assess its contribution. In addition, we explore more design choices for the adaptive front-end that only involves one adaptive Gabor filter layer (without fixed Gabor filters). For the original AFC (shown in Figure \ref{fig3} (c)), the frequency modulation (FM) component of the filtered signal is designed as the input. Here we also explore replacing the FM with the energy feature and the FM concatenated with the energy feature (Energy+FM), respectively.}


\subsection{\textcolor{black}{Comparative Studies}}

In this section, we compare Ada-FE and \textcolor{black}{Ada-FE-S-FM} with recent TD-fbanks and LEAF. In Table~\ref{top1}, we report the Top-1 test accuracy of the models across \textcolor{black}{all} eight audio and speech datasets and two back-end classifier backbone networks (i.e., MobileNetV2-100 and EfficientNet-B0). In addition, we report the results (marked in gray) of several advanced state-of-the-art audio representation learning models based on pre-training. Ada-FE and \textcolor{black}{Ada-FE-S-FM} (as well as LEAF and TD-fbanks) only involve spectral decomposition and low-level feature extraction, with a small number of trainable parameters. Note that they are not directly comparable to pre-training models that exploit a large DNN to capture high-level features and are pre-trained on massive external data. AST-Scratch denotes the audio spectrogram Transformer (AST) model~\cite{gong21b_interspeech} trained from scratch without pre-training. AST-AS and AST-AS+IM denote the AST models pre-trained on AudioSet~\cite{audioset} and AudioSet+ImageNet, respectively. PANNs~\cite{kong2020panns} is the CNN14 pre-trained on AudioSet. Wav2Vec2-LS960~\cite{wav2vec2} denotes the Wav2Vec2 model pre-trained on LibriSpeech-960 corpus~\cite{librispeech}. AST, PANNs, and Wav2Vec2 involve \textcolor{black}{87.0M}, 79.7M, and \textcolor{black}{315.4M} parameters, respectively. The model size of the back-end classifier varies with the size (number of classes) of the classification head on the top. From IEMOCAP (four classes) to VoxCeleb1 ($1\,251$ classes), the sizes of MobileNetV2-100 and EfficientNet-B0 range from \textcolor{black}{2.23M} and \textcolor{black}{4.01M} to \textcolor{black}{3.83M} and \textcolor{black}{5.61M}, respectively. \textcolor{black}{In contrast, Ada-FE (1.51K parameters), LEAF (0.32K parameters), and TD-fbanks (32K parameters) have negligible parameters compared to the back-ends.}

\input{./Top1Acc}
\input{./Top5Acc}

% \textcolor{blue}{In this section, we compare the Ada-FE and Ada-FE-S with recent TD-fbanks and LEAF. In Table~\ref{top1}, we report the Top-1 test accuracy of Ada-FE, Ada-FE-S, LEAF, and TD-fbanks, across eight datasets (five audio tasks) and two back-end classifier backbone networks (i.e., MobileNetV2-100 and EfficientNet-B0). In addition, we list the results (marked in gray) of several recent state-of-the-art audio representation methods based on pre-training. Ada-FE as well as LEAF and TD-fbanks only involve spectral decomposition and low-level feature extraction, and have a small number of trainable parameters. Note that they are not directly comparable to pre-training models that exploit a large DNN to capture high-level features and are trained on massive external pre-training data. AST-Scratch denotes the audio spectrogram Transformer (AST) model~\cite{gong21b_interspeech} trained from scratch without pre-training. AST-AS and AST-AS+IM denote the AST models pre-trained on AudioSet~\cite{audioset} and AudioSet+ImageNet, respectively. PANNs~\cite{kong2020panns} is the CNN14 pre-trained on AudioSet. Wav2Vec2-LS960~\cite{wav2vec2} denotes the Wav2Vec2 model pre-trained on LibriSpeech-960 corpus~\cite{librispeech}. AST, PANNs, and Wav2Vec2 involve 87.0 M, 79.7M, and 315.4 M parameters, respectively. The model size of the back-end classifier varies with the size (number of classes) of the classification head on the top. From IEMOCAP (four classes) to VoxCeleb1 ($1\,251$ classes), the sizes of MobileNetV2-100 and EfficientNet-B0 range from 2.23 M and 4.01 M to 3.83 M and 5.61 M, respectively. Ada-FE (1.51 K), LEAF (0.32 K), and TDfbanks (32 K) have negligible parameters compared to back-ends.}

% MobileNetV2-100 has a 2.23 M 2.23 M 2.23 M 2.24 M 2.26 M 2.27 M 2.29 3.83 M.\\
% EfficientNet-B0 has  4.01 M, 4.01 M, 4.02 M, 4.02 M, 4.05 M, 4.05 M, 4.07 M, 5.61 M.



\textcolor{black}{From Table~\ref{top1}, we observe that Ada-FE and \textcolor{black}{Ada-FE-S-FM} outperform LEAF and TD-fbanks, across all the tasks and two back-end classifiers (MobileNetV2-100 and EfficientNet-B0). In music genre classification (GTZAN), for instance, Ada-FE and \textcolor{black}{Ada-FE-S-FM} provide 17.1\% and 19.68\%, and 14.4\% and 18.71\% relative Top-1 accuracy improvements over LEAF on MobileNetV2-100 and EfficientNet-B0, respectively. In speech emotion recognition (IEMOCAP), Ada-FE and \textcolor{black}{Ada-FE-S-FM} showcase 13.51\% and 16.95\%, and 6.37\% and 8.71\% relative Top-1 accuracy gains over LEAF, on MobileNetV2-100 and EfficientNet-B0 respectively. Compared to the other speech and audio tasks, VoxCeleb1 speaker ID ($1\,251$ classes) is a more challenging task. In speaker ID, with 1-second segments for training, Ada-FE and \textcolor{black}{Ada-FE-S-FM} improve LEAF with relative Top-1 accuracy of 34.06\% and 30.89\%, and 5.64\% and 2.63\% on MobileNetV2-100 and EfficientNet-B0, respectively. In addition, Ada-FE and \textcolor{black}{Ada-FE-S-FM} (with MobileNet-V2-100 and EfficientNet-B0 back-ends) exhibit better performance than the AST-Scratch (without pre-training), with significantly lower parameter overheads.}

% and 14.4\%, and 3.18\% and 9.19\% relative Top-1 accuracy improvements over LEAF on MobileNetV2-100 and EfficientNet-B0, on GTZAN and FMA-Small datasets respectively. Compared to the other speech and audio tasks, VoxCeleb1 speaker ID ($1\,251$ classes) is a more challenging task. In the speaker ID, with 1-second segments for training, Ada-FE improves LEAF with relative Top-1 accuracies of 34.06\% and 5.64\%, on MobileNetV2-100 and EfficientNet-B0, respectively. In addition, Ada-FE and Ada-FE-S-F (with MobileNet-V2-100 and EfficientNet-B0 back-ends) exhibit better performance than the AST-Scratch (without pre-training), with significantly lower parameter overheads.

% In music genre classification, for instance, Ada-FE provides 17.1\% and 14.4\%, and 3.18\% and 9.19\% relative Top-1 accuracy improvements over LEAF on MobileNetV2-100 and EfficientNet-B0, on GTZAN and FMA-Small datasets respectively. Compared to the other speech and audio tasks, VoxCeleb1 speaker ID ($1\,251$ classes) is a more challenging task. In the speaker ID, with 1-second segments for training, Ada-FE improves LEAF with relative Top-1 accuracies of 34.06\% and 5.64\%, on MobileNetV2-100 and EfficientNet-B0, respectively. In addition, Ada-FE and Ada-FE-S-F (with MobileNet-V2-100 and EfficientNet-B0 back-ends) exhibit better performance than the AST-Scratch (without pre-training), with significantly lower parameter overheads.

\textcolor{black}{Table~\ref{top5} reports the Top-5 accuracy of Ada-FE, \textcolor{black}{Ada-FE-S-FM}, and LEAF across seven datasets (except for IEMOCAP with four classes), on two back-ends. We can observe a performance trend similar to that shown in Table~\ref{top1}. In sound event classification (ESC-50) and speech emotion recognition (CREMA-D), Ada-FE attains 15.74\% and 9.51\%, 0.72\% and 0.89\% relative Top-5 accuracy improvements on MobileNetV2-100 and EfficientNet-B0, respectively. For VoxCeleb1 speaker ID, with 1-second training segments, Ada-FE improves LEAF with relative Top-5 accuracy of 20.08\% and 2.06\%, on the two back-ends respectively.}



% \textcolor{black}{Table I reports the Top-1 and Top-5 test accuracies (\%) of Ada-FE and LEAF, and Top-1 results of TD-fbank. It can be observed that Ada-FE demonstrates the best performance on two tasks. In the keyword spotting task, Ada-FE provides a relative Top-1 accuracy increase of 1.04\% over LEAF. Compared to keyword spotting (35 classes), VoxCeleb1 speaker Id. (1251 classes) is a more challenging task. In speaker Id. with 1-second segments for training, Ada-FE improves LEAF with relative Top-1 and Top-5 accuracies of 5.44\% and 2.36\%, respectively.}

% \vspace{-0.9em}

\textcolor{black}{Figures~\ref{fig:esc50mbnetv2}-\ref{fig:scv2} display the learning curves of our Ada-FE and \textcolor{black}{Ada-FE-S-FM} compared with that of the LEAF, on keyword spotting (SPC-V2), speaker ID (VoxCeleb1), speech emotion recognition (CREMA-D), and keyword spotting (SPC-V2), respectively. It can be easily observed that our Ada-FE and \textcolor{black}{Ada-FE-S-FM} demonstrate a faster increase in recognition accuracy, across MobileNetV2-100 and EfficientNet-B0 back-ends. As shown in Figure~\ref{fig:scv2}, for instance, Ada-FE and \textcolor{black}{Ada-FE-S-FM} with EfficientNet-B0 attain over 56\% accuracy with one-epoch training and converges well after 10-epoch training. The LEAF shows less than 10\% accuracy with one-epoch training. We also can find that the best performance attained by Ada-FE and \textcolor{black}{Ada-FE-S-FM} is better than that attained by LEAF.} 

\begin{figure}[!t]
% \vspace{-0.7em}
  \centering
  \includegraphics[width=0.85\linewidth]{ESC50_mbnetv2_V2_1.pdf}
  % \vspace{-1.8em}
  \caption{Top-1 accuracy (\%) of the LEAF, Ada-FE, and \textcolor{black}{Ada-FE-S-FM} (with the MobileNetV2-100 back-end) on sound event classification task (ESC-50 dataset), over various training epochs. Averaged accuracy over five-fold validation sets is reported.}
  \label{fig:esc50mbnetv2}
  % \vspace{-1.5em}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.85\linewidth]{GTZAN_mbnetv2_V2_1.pdf}
  % \vspace{-0.8em}
  \caption{Top-1 accuracy (\%) of the LEAF, Ada-FE, and \textcolor{black}{Ada-FE-S-FM} (with the MobileNnetV2 back-end) on music genre classification task (GTZAN dataset), over various training epochs.}
  \label{fig:vox1mbnetv2}
  % \vspace{-0.9em}
\end{figure}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.85\linewidth]{CREMAD_enetb0_V2_1.pdf}
  % \vspace{-0.8em}
  \caption{Top-1 accuracy (\%) of the LEAF, Ada-FE, and \textcolor{black}{Ada-FE-S-FM} (with the EfficientNet-B0 back-end) on speech emotion recognition (CREMA-D dataset), over various training epochs.}
  \label{fig:cremad}
  \vspace{-1.5em}
\end{figure}

\begin{figure}[!hpbt]
  \centering
  \includegraphics[width=0.85\linewidth]{SPCV2_enetb0_V2_1.pdf}
  % \vspace{-0.8em}
  \caption{Top-1 accuracy (\%) of the LEAF, Ada-FE, and \textcolor{black}{Ada-FE-S-FM} (with the EfficientNet-B0 back-end) on keyword spotting task with the SPC-V2 dataset, over various training epochs.}
  \label{fig:scv2}
  \vspace{-0.9em}
\end{figure}

% \textcolor{black}{Figures~\ref{fig:esc50mbnetv2}-\ref{fig:scv2} display the learning curves of our Ada-FE and \textcolor{black}{Ada-FE-S-FM} compared with that of the LEAF, on keyword spotting (SPC-V2), speaker ID (VoxCeleb1), speech emotion recognition (CREMA-D), and keyword spotting (SPC-V2), respectively. It can be easily observed that our Ada-FE and \textcolor{black}{Ada-FE-S-FM} demonstrate a faster increase in recognition accuracy, across MobileNetV2-100 and EfficientNet-B0 back-ends. As shown in Figure~\ref{fig:scv2}, for instance, Ada-FE and \textcolor{black}{Ada-FE-S-FM} with EfficientNet-B0 attain over 56\% accuracy with one-epoch training and converges well after 10-epoch training. The LEAF shows less than 10\% accuracy with one-epoch training. We also can find that the best performance attained by Ada-FE and \textcolor{black}{Ada-FE-S-FM} is better than that attained by LEAF.} 


% \textcolor{black}{Figures \ref{fig:esc50mbnetv2}-\ref{fig:scv2} show the learning curves of our Ada-FE compared with that of the LEAF, across keyword spotting and speaker identification, respectively. It can be easily observed that our proposed Ada-FE demonstrates a faster increase in recognition accuracy on both keyword spotting and speaker identification. As shown in Figure \ref{fig:scv2}, for instance, the Ada-FE attains over 45\% accuracy with one-epoch training and converges well after 10-epoch training. The LEAF shows less than 10\% accuracy with one-epoch training. We also can find that the best performance attained by Ada-FE is better than that attained by LEAF.} 


% \begin{figure}[!h]
%   \centering
%   \includegraphics[width=0.85\linewidth]{CREMAD_enetb0_V2.pdf}
%   % \vspace{-0.8em}
%   \caption{Top-1 accuracy (\%) of the LEAF, Ada-FE, and Ada-FE-S-F (with the EfficientNet-B0 back-end) on speech emotion recognition (CREMA-D dataset), over various training epochs.}
%   \label{fig:cremad}
%   \vspace{-1.5em}
% \end{figure}

% \begin{figure}[!hpbt]
%   \centering
%   \includegraphics[width=0.85\linewidth]{SPCV2_enetb0_V2.pdf}
%   % \vspace{-0.8em}
%   \caption{Top-1 accuracy (\%) of the LEAF and our Ada-FE (with the EfficientNet-B0 back-end) on keyword spotting task with the SPC-V2 dataset, over various training epochs.}
%   \label{fig:scv2}
%   % \vspace{-0.9em}
% \end{figure}

% \begin{figure}[!hbpt]
%   \centering
%   \includegraphics[width=0.9\linewidth]{VoxCeleb1.pdf}
% \vspace{-0.8em}
%   \caption{Top-1 accuracy (\%) of the LEAF and our Ada-FE on speaker identification task with the VoxCeleb1 dataset, over various training epochs.}
%   \label{fig:voxceleb1}
% \end{figure}

Beyond the performance superiority, and perhaps more importantly, we find that our Ada-FE and \textcolor{black}{Ada-FE-S-FM} show much better stability over unseen test samples when compared with LEAF. This can be highly advantageous in a data-driven learning task. In real-world applications, for instance, speech signals captured by microphones are inevitably shaped by transfer channels between speakers and microphones. Transfer channels vary with the distance between speakers and microphones, as well as the acoustic scenarios (such as car space, church, hall, and outdoor). The learning curves of LEAF suggest that LEAF is quite sensitive to varying acoustic conditions. In contrast, the stability or robustness exhibited by the Ada-FE is significantly better and bodes well for practical applications.


\section{Conclusion}\label{sec6}

\textcolor{black}{\textcolor{black}{In this paper, we investigate the Ada-FE across two back-end classifiers and eight audio and speech classification benchmarks (including sound event classification, emotion recognition, keyword spotting, speaker identification, and music genre classification tasks).} Ada-FE consists of one fixed Gabor filter layer and an adaptive Gabor filter layer. The adaptive filters are dynamically tuned by hand-crafted level-dependent adaption and neural adaptive feedback controller. We further simplify the adaptive control process to enable the adaptive filters completely adjusted by the neural feedback controller.
% Furthermore, we study the adaptive front-end with only one adaptive Gabor filter layer.
}


% In this study, we explore the audio front-end that replicates the active process of the mammalian cochlea for general-purpose audio applications. We present the Ada-FE, a novel adaptive front-end, which employs a feedback controller to dynamically adjust the characteristics of the filterbank in a frame-by-frame fashion at inference time. Ada-FE consists of two Gabor filter layers, where the first layer is with fixed weights and the second one is adaptive. For the adaptive filter layer, we design two modules, i.e., level-dependent adaption and adaptive feedback controller, to dynamically infer the Q values to adjust the selectivity and sensitivity of filters.

% \textcolor{red}{Ada-FE is evaluated on a benchmark composed of eight speech and audio datasets, across various tasks, i.e., acoustic scene classification, music genre classification, speech emotion recognition, keyword spotting, and speaker identification. Extensive ablation studies confirm the effectiveness of the design choices for Ada-FE and clarify the contributions of Ada-FE components. Additionally, Ada-FE demonstrates better performance over the recent state-of-the-art front-end, LEAF. The experiment results suggest that using an adaptive front-end offers advantages over using a fixed front-end. Beyond the performance superiority, Ada-FE also shows significantly better stability than the baseline model on the test set, over various epochs. In future work, we will further incorporate the active process into audio representation learning, such as pre-training models. We will also explore the suitability of Ada-FE for other audio tasks such as speech recognition, speech enhancement, and speech separation.}

Comprehensive experimental results confirm the performance superiority of Ada-FE over the recent state-of-the-art LEAF and suggest that using an adaptive front-end offers advantages over using a fixed front-end. Additionally, The simplified Ada-FE (\textcolor{black}{Ada-FE-S-FM}) provides highly competitive or better performance than Ada-FE, demonstrating hand-crafted LDA is quite essential. Beyond the performance superiority, Ada-FE and \textcolor{black}{Ada-FE-S-FM} show significantly better stability than the baseline on the test set, over various epochs. This paper demonstrates the advantages of employing feedback control in the front-end to respond to varying conditions by suitably altering filter characteristics, opening up new avenues for future research in adaptive speech and audio representations. In future work, we will further incorporate the active process into audio representation learning, such as pre-training models. 
%We will also explore the suitability of Ada-FE for other audio tasks such as speech recognition, speech enhancement, and speech separation.

% \input{reverb}
% \input{results_dB}

% \input{TFA}
% The experiment results suggest that using an adaptive front-end offers advantages over using a fixed front-end. Ada-FE is evaluated on a benchmark composed of eight speech and audio datasets, across various tasks, i.e., acoustic scene classification, music genre classification, speech emotion recognition, keyword spotting, and speaker identification. Extensive ablation studies confirm the effectiveness of the design choices for Ada-FE and clarify the contributions of Ada-FE components. Additionally, Ada-FE demonstrates better performance over the recent state-of-the-art front-end, LEAF. The experiment results suggest that using an adaptive front-end offers advantages over using a fixed front-end. Ada-FE demonstrates the advantages of employing feedback control in the front-end to respond to varying conditions by suitably altering filter characteristics, opening up new avenues for future research in adaptive speech and audio representations.Beyond the performance superiority, Ada-FE also shows significantly better stability than the baseline model on the test set, over various epochs.

% \section{Problem Formulation}\label{sec:2}

% \subsection{Signal Model}\label{sec:2.1}
% Let a noisy speech signal be $x[n]$,
% \begin{equation} \label{equ:b}
% x[n] = s[n] + d[n],
% \end{equation}
% where $s[n]$ and $d[n]$ denote clean speech and uncorrelated additive noise, respectively, and $n$ denotes the discrete-time index. The noisy speech, $x[n]$, is then analysed frame-wise using the short-time Fourier transform (STFT):
% \begin{equation}
% X[l,k] = S[l,k] + D[l,k],
% \end{equation}
% where $X[l,k]$, $S[l,k]$, and $D[l,k]$ denote the complex-valued STFT coefficients of the noisy speech, the clean speech, and the noise components at time-frame index $l$ and discrete-frequency index $k$. 

% \subsection{Training Objectives}\label{sec:2.2}
% A backbone network architecture is trained to optimize the designed \textcolor{black}{training objectives} for speech enhancement. Studies show that by optimizing the network with respect to the T-F mask, we improve the intelligibility and quality of speech in speech enhancement. Without loss of generality, we have chosen four widely used \textcolor{black}{training objectives} in this study, as summarized next.

% \subsubsection{Ideal Ratio Mask}

% The ideal ratio mask \textcolor{black}{(IRM)} \cite{targets} is one of the most popular masking-based \textcolor{black}{training objectives}, and it is defined as:
% \begin{equation}\label{IRM}
% \text{IRM}[l,k]=\sqrt{\frac{|S[l, k]|^{2}}{|S[l, k]|^{2}+|D[l, k]|^{2}}}
% \end{equation}
% where $\left|S\left[l,k\right]\right|$ and $\left|D\left[l,k\right]\right|$ denote the spectral magnitudes of clean speech and noise, respectively. The value of \textcolor{black}{IRM} ranges from 0 to 1. 

% \subsubsection{Spectral Magnitude Mask}

% The spectral magnitude mask \textcolor{black}{(SMM)} \cite{targets} is defined on the STFT magnitude of clean speech and noisy speech:
% \begin{equation}\label{SMM}
% \text{SMM}[l,k]=\frac{\left|S[l, k]\right|}{|X[l, k]|}
% \end{equation}
% where $\left|X\left[l,k\right]\right|$ denotes the spectral magnitude of the noisy speech.

% \subsubsection{Phase-Sensitive Mask}

% The phase-sensitive mask \textcolor{black}{(PSM)} \cite{psm} is an extension of the \textcolor{black}{SMM}, which introduces a phase error item to compensate for the use of noisy speech phases:
% \begin{equation}\label{PSM}
% \text{PSM}[l,k]=\frac{\left|S[l, k]\right|}{|X[l, k]|}\cos[\theta_{S-X}]
% \end{equation}
% where $\theta_{S-X}$ denotes the difference between the clean speech phase and noisy speech phase.

% From equations (\ref{SMM}) and (\ref{PSM}), we can find that the upper bound of \textcolor{black}{SMM} and \textcolor{black}{PSM} values exceeds 1. To fit the output range of the sigmoid activation function, we \textcolor{black}{clip} the \textcolor{black}{SMM} and \textcolor{black}{PSM} to between 0 and 1.

% \subsubsection{Instantaneous \textit{a Priori} SNR}

% The instantaneous \textit{a priori} SNR \textcolor{black}{(Xi)} was proposed as the \textcolor{black}{training objective} \cite{nicolson2019deep,DeepMMSE} and is employed by statistical model-based methods \cite{DeepMMSE}. To form the \textcolor{black}{training objective}, the normal cumulative distribution function (CDF) is used to map $\xi_{\text{dB}}[l,k]= 10\log_{10}{\xi[l,k]}$ to the interval $[0, 1]$, where $ \xi[l,k] = \frac{|S[l, k]|^{2}}{|D[l, k]|^{2}}$ \cite{doi:10.1121/10.0004823}:
% \begin{equation} \label{equa}
% \bar{\xi}[l,k] = \frac{1}{2}\Bigg[1 + \textrm{erf}\Bigg( \frac{\xi_{\text{dB}}[l,k] - \mu_k}{\sigma_k\sqrt{2}} \Bigg) \Bigg],
% \end{equation}
% where $\rm erf$ denotes the error function and mean $\mu_{k}$ and variance $\sigma^{2}_{k}$ are calculated from the training set (over $1000$ randomly selected samples in this study). During inference, the \textit{a priori} SNR estimate is computed as follows:
% \begin{equation}
% \hat{\xi}[l,k] = 10^{\big(\big(\sigma_k\sqrt{2}\textrm{erf}^{-1}\big(2\hat{\bar{\xi}}_l[k] - 1\big) + \mu_k\big)/10\big)}
% \end{equation}
% {With \textcolor{black}{IRM, SMM, and PSM as training objectives}, we train DNNs to produce masks at run time. We then apply the resulting masks on the STFT spectral magnitude of noisy speech to obtain a clean version. The enhanced magnitude is then used with the noisy speech phase to reconstruct the clean speech waveform. For \textcolor{black}{Xi}, we adopt the minimum mean-square error log-spectral amplitude (MMSE-LSA) estimator as the statistical model, which uses $\hat{\xi}[l,k]$ to compute the spectral magnitude of enhanced speech \cite{logmmse}}. 

%\textcolor{blue}{To Prof Li: it is mapped version of priori SNR not directly use the instantaneous priori SNR. The processing is as follow: The priori SNR is mapped to [0,1] as training target. Then the obtained estimate of mapped priori SNR is transformed back to instantaneous priori SNR, which is used in a gain function derived from statistical model to obtain the enhanced magnitude.
%They are different. For SMM, PSM ,and IRM, we just need to use the mask to multiply the noisy spectrogram to obtain the clean version. For Magxi, it needs to be integrated into any/a statistical model-based gain function for speech enhancement.}  
%\subsection{Neural Speech Enhancement Framework}\label{sec:2.2}

%\textcolor{red}{a) please add a subsection to explain how a neural speech enhancement work, to bring out the concept of T-F tensor, and explain why a T-F attention map can improve such T-F tensor. This is a very important part of this paper. Readers are not interested in your workflow, they are only interested in why you think the T-F attention map is useful for neural speech enhancement. For this, you need to explain how a general speech enhancement framework looks like, and motivate your idea. If some of the sentences are already somewhere else, please move them here, and delete them from the original places.  This will give readers the idea how they can learn from your idea for their own work in the future. b) another issue of this paper is that you repeat the same statement in many places, please note that one statement can only be made in ONE place, please find the best and most logical place to put the statement, but do not repeat it again. i have cleaned up your introduce by removing many repeating statements.}

% \textcolor{blue}{Speech spectral components are not uniformly distributed over the entire T-F representation. Specifically, the energy distribution of the speech signal in an utterance is concentrated in certain frames and frequency bands, which vary with utterances. To preserve the speech formant structure better, an efficient speech enhancement model should pay more effort or attention to the T-F regions where speech energy is concentrated. A natural question to ask is whether it is possible to investigate a different aspect of the architecture design that enables the network to capture the speech distribution characteristic on T-F representation. (I also put this paragraph in introduction part for checking whether it is suitable.) }

%\textcolor{blue}{Dear Prof. Li. Could the above paragraph explain our motivation? But  I feel confused that we place our motivation in this subsection. Could we place this section in introduction section?}

%\textcolor{blue}{}

%\textcolor{red}{In introduction, we talk about the motivation; in this subsection, we talk about a general neural speech enhancement framework to prepare for next section. the next section talks about 2 different network architecture, we give a high level general framework here, and we don't need to repeat in the next paragraph.}


%\textcolor{blue}{Thank you Prof. Li. I will check according your comments. \textcolor{red}{please draw one single architecture diagram here that covers both ResTCN and Transformer.}} \textcolor{blue}{Thank you Prof. Li. I give the plot of the overall architecture in Fig. 1.}

% The performance of statistical model-based speech enhancement methods is dominantly impacted by the \textit{a priori} SNR. 
% The Deep Xi framework \cite{nicolson2019deep,DeepMMSE} proposed the mapped \textit{a priori} SNR (MagXi) as the training objective to bridge the gap between deep learning and statistical model-based methods, which is a mapped version of the instantaneous \textit{a priori} SNR in dB, $\xi_{\text{dB}}[l,k] = 10\log_{10}{\xi[l,k]}$, where $ \xi[l,k] = \frac{|S[l, k]|^{2}}{|D[l, k]|^{2}}.$
% It was found that the distribution of $\xi_{\text{dB}}[l,k]$ follows the normal distribution. 
% % In order to improve the convergence rate of the gradient descent algorithm, 
% The cumulative distribution function (CDF) is used as compression function to map the $\xi_{\text{dB}}[l,k]$ to the interval $[0, 1]$. It was assumed that $\xi_{\text{dB}}[l,k]$ is distributed normally with mean $\mu_{k}$ and variance $\sigma^{2}_{k}$: $\xi_{\text{dB}}[l,k]\sim\mathcal{N}(\mu_k,\sigma^2_k)$. The compression function, i.e., the CDF function of $\xi_{\text{dB}}$ is given by: 
% \begin{equation} \label{equa}
% \bar{\xi}[l,k] = \frac{1}{2}\Bigg[1 + \textrm{erf}\Bigg( \frac{\xi_{\text{dB}}[l,k] - \mu_k}{\sigma_k\sqrt{2}} \Bigg) \Bigg]
% \end{equation}
% where $\bar{\xi}[l,k]$ is the mapped a \textit{priori} SNR (MagXi) and erf denotes the error function. The mean and variance of $\xi_{\text{dB}}[l,k]$ are calculated over $1000$ randomly selected samples from training set.
% In the inference stage, the a \textit{priori} SNR estimate is computed from the MagXi estimate:
% \begin{equation}
% \hat{\xi}[l,k] = 10^{\big(\big(\sigma_k\sqrt{2}\textrm{erf}^{-1}\big(2\hat{\bar{\xi}}_l[k] - 1\big) + \mu_k\big)/10\big)}
% \end{equation}

% In Deep Xi framework, the obtained \textit{a priori} SNR estimate $\hat{\xi}[l,k]$ is applied in a gain function derived from statistical model to produce the spectral magnitude estimate of clean speech and then used with noisy phase to reconstruct the enhanced speech waveform. 
% In this study the minimum mean-square error log-spectral (MMSE-LSA) magnitude estimator \cite{logmmse} is used to obtain the enhanced magnitude. 


% given by:
% \begin{equation}
% G_{\mathrm{MMSE}-\mathrm{LSA}}[l,k]=\frac{\xi[l, k]}{\xi[l, k]+1} \exp \left\{\frac{1}{2} \int_{v[l, k]}^{\infty} \frac{e^{-t}}{t} d t\right\},
% \end{equation}
% where $v[l, k]=(\xi[l, k] /(\xi[l, k]+1)) \gamma[l, k]$ and $\gamma[l,k]$ denotes the \textit{a posteriori} SNR. The maximum likelihood \textit{a posteriori} SNR estimator is used, i.e., $\hat{\gamma}[l, k]=\hat{\xi}[l, k]+1$.


% \section{Time-Frequency Attention for Speech Enhancement}\label{sec:3}

% \subsection{\textcolor{black}{Time-Frequency Attention}}\label{sec3.1}

%\textcolor{blue}{I have corrected them. I keep them consistent: TFA module. And I have a question about accepted conference version of our paper (ICASSP 2022 paper). Do we need to add this reference in the introduction section.} \textcolor{red}{yes, you MUST cite and explain the difference.} \textcolor{blue}{Dear Prof. Li, I have added few lines to explain the differences in introduction part (blue colored part).}

% \textcolor{black}{A TFA module is a computational unit, which takes an intermediate T-F representation $\textbf{Y}\!\in\!\mathbb{R}^{L\times d_{model}}$ as the input, i.e., $L$ frames of $d_{model}$ frequency-wise feature channels, and generates an enhanced representation $\widetilde{\textbf{Y}}\!\in \!\mathbb{R}^{L\times d_{model}}$ with differentiated T-F attention. The diagram of the proposed TFA module is illustrated in Figure \ref{TFA}.}

% \textcolor{black}{The distribution of speech signals over the T-F plane is defined by its time-frame index and frequency-wise channel index. We would  generate a position-aware attention map, which assigns differentiated weights to position-specific speech components. In practice, we employ two parallel attention branches, termed TA and FA, which produces a 1-D time-frame attention map $\textbf{T}_{\textbf{A}}\!\in\!\mathbb{R}^{1 \times L}$ and a 1-D frequency-dimension attention map $\textbf{F}_{\textbf{A}}\!\in\!\mathbb{R}^{d_{model}\times 1}$. Then, the two 1-D attention maps are combined via a tensor multiplication operation, resulting in a position-aware 2-D T-F attention map $\textbf{TF}_{\textbf{A}}\!\in\!\mathbb{R}^{L \times d_{model}}$.}

% \begin{figure*}[!ht]
% % \vskip -0.2in
% \begin{center}
% \centerline{\includegraphics[width=1.56\columnwidth]{./diagram/TFA.pdf}}
% \caption{A diagram of the proposed TFA module, where the TA and FA modules are shown in blue dotted box and black dotted box, respectively. Here, AvgPool and Conv1D represent the average pooling and dilated 1-D convolution operation, respectively. $\bigotimes$ and $\bigodot$ denote the matrix multiplication and element-wise product, respectively.}
% \label{TFA}
% \end{center}
% \vskip -0.3in
% \end{figure*}

%, where each element reflects whether the informative speech spectral component exists in the corresponding frame and frequency-wise channel.

% \textcolor{black}{We next provide the detailed working of the proposed TFA module. As the long-range context correlations as shown in T-F representation are essential to locate the informative T-F regions, each attention branch adopts a two-step strategy to capture such correlations to generate the attention map: information aggregation and attention generation.}

% \textcolor{blue}{To enable models to accurately locate the position of the informative speech spectral component to focus on, our TFA module utilizes two parallel attention branches to effectively integrate positional information for the generation of a position-aware attention map.} 

% \textcolor{blue}{The diagram of the proposed TFA module is illustrated in Figure \ref{TFA}. The two parallel attention branches produce a 1-D time-frame attention map $\textbf{T}_{\textbf{A}}\!\in\!\mathbb{R}^{L \times 1}$ and a 1-D frequency-dimension attention map $\textbf{F}_{\textbf{A}}\!\in\!\mathbb{R}^{1 \times d_{model}}$, which explicitly help models to locate the informative speech frames and the informative frequency-wise channels, respectively. Then the two 1-D attention maps are effectively combined via a tensor multiplication operation, resulting in a 2-D T-F attention map $\textbf{TF}_{\textbf{A}}\!\in\!\mathbb{R}^{L \times d_{model}}$, where each element reflects whether the informative speech spectral component exists in the corresponding frame and frequency-wise channel. The output of our TFA module, i.e., the augmented T-F attention $\widetilde{\textbf{Y}}$ is written as
% \begin{equation}
%     \widetilde{\textbf{Y}} = \textbf{Y} \odot \textbf{TF}_{\textbf{A}},
% \end{equation}
% where $\odot$ denotes an element-wise multiplication. In the following, we will describe the detailed calculation process of the proposed TFA attention module. Both the time-frame attention map $\textbf{T}_{\textbf{A}}$ and the frequency-dimension attention map $\textbf{F}_{\textbf{A}}$ are produced with two steps: global information aggregation and attention generation.}
% The proposed TFA module is illustrated in Figure \ref{TFA}. 
% Given a transformed T-F representation $\textbf{Y}\!\in \!\mathbb{R}^{L\times d_{model}}$ as the input \textcolor{black}{of $L$ frames and $d_{model}$ frequency-wise feature channels}, the TFA module first generates a 1-D frequency-dimension attention map $\textbf{F}_{\textbf{A}}\!\in\!\mathbb{R}^{1 \times d_{model}}$ and a 1-D time-frame attention map $\textbf{T}_{\textbf{A}}\!\in\!\mathbb{R}^{L \times 1}$ in parallel, then combines them with a tensor multiplication operation to obtain the final 2-D T-F attention map $\textbf{TF}_{\textbf{A}}\!\in\!\mathbb{R}^{L \times d_{model}}$. The output with differentiated attention can be expressed as:
% \begin{equation}
%     \widetilde{\textbf{Y}} = \textbf{Y} \odot \textbf{TF}_{\textbf{A}},
% \end{equation}
% where $\odot$ denotes an element-wise multiplication. The detailed calculation process of the proposed TFA attention module is given below.
%The spectral energy distribution of speech in T-F domain shows obvious time-frame dependencies and frequency-dimension dependencies, which is essential to produce accurate attention map. 
% Each of the 1-D attention maps, $\textbf{F}_{\textbf{A}}$ and $\textbf{T}_{\textbf{A}}$,  are obtained in two steps: global information aggregation and attention generation. 

% \textcolor{black}{\textbf{Information Aggregation.}
% The TA and FA branches aggregate the whole utterance information along the time and frequency dimensions, respectively. The global average pooling and max pooling are typical techniques to aggregate global spatial information \cite{senet,cbam}. We adopt the global average pooling, which produces the global information descriptors that are expressive and general for the entire utterance. Specifically, the TA branch takes global average pooling along the frequency dimension on the given input $\textbf{Y}$ and generates a time-frame-wise statistic $\textbf{Z}_{\textbf{T}}\in \mathbb{R}^{1\times L}$ as follows:
% \begin{equation}
%     \textbf{Z}_{\textbf{T}}(l) = \frac{1}{d_{model}}\sum_{k=1}^{d_{model}} \textbf{Y}(l,k), 
% \end{equation}
% where $\textbf{Z}_{\textbf{T}}(l)$ is the \textit{l}-th element of $\textbf{Z}_{\textbf{T}}$. Similarly, the FA branch applies global average pooling along the time-frame dimension on the input $\textbf{Y}$ and generates a frequency-wise statistic $\textbf{Z}_{\textbf{F}}\in \mathbb{R}^{d_{model} \times 1}$. The \textit{k}-th element of $\textbf{Z}_{\textbf{F}}$ is given as:
% \vspace{-0.5em}
% \begin{equation}
%     \textbf{Z}_{\textbf{F}}(k) = \frac{1}{L}\sum_{l=1}^{L} \textbf{Y}(l,k). 
% \vspace{-0.1em}
% \end{equation}}


%\textcolor{red}{The above two operations, which capture the long-range correlations along frequency and time-frame dimensions, aggregate the global utterance information and generate two descriptors, i.e., $\textbf{Z}_{\textbf{T}}$ and $\textbf{Z}_{\textbf{F}}$.}

% achieve the global information aggregation and the capture of the long-range correlations along frequency and time-frame dimensions, respectively. The two statistics $\textbf{Z}_{\textbf{T}}$ and $\textbf{Z}_{\textbf{F}}$ can be viewed as two descriptors of the speech distributions in the time-frame dimension and frequency dimension, respectively.


% \textcolor{black}{\textbf{Attention Generation.} 
% A two-layer fully-connected (FC) network is often used to provide channel attention~\cite{senet,cbam}. However, the use of FC layer brings a large parameter overhead especially for long-duration speech. Alternatively, it has been suggested that an effective channel attention can be implemented in a more efficient way via 1-D convolution \cite{ecanet}.}

% \textcolor{black}{We adopt two stacked dilated 1-D convolution layers of kernel size $k_{t\!f\!a}$ to capture the dependencies in the descriptors and learn a nonlinear interaction to produce the attention map. Specifically, given the descriptor $\textbf{Z}_{\textbf{T}}$, the attention map in the TA branch is calculated as:
% \vspace{-0.05em}
% \begin{equation}
%     \textbf{T}_{\textbf{A}} = \sigma(f_{2}^{T\!A}(\delta (f_{1}^{T\!A}(\textbf{Z}_{\textbf{T}})))).
% \end{equation}
% where $f$ denotes a dilated 1-D convolution operation, $\delta$ and $\sigma$ refer to the rectified linear module (ReLU) and the sigmoidal activation functions, respectively. The dilation rate $d$ is set to 1 and 2 for the first and second convolution modules, respectively.
% A similar process is applied to the FA branch for generating the frequency-wise channel attention map:
% \begin{equation}
%     \textbf{F}_{\textbf{A}} = \sigma(f_{2}^{F\!A}(\delta (f_{1}^{F\!A}(\textbf{Z}_{\textbf{F}}))))
% \end{equation}
% Then, the attention maps obtained from the two attention branches interact with a tensor multiplication operation, resulting in our final 2-D T-F attention map $\textbf{TF}_{\textbf{A}}$ written as:
% \vspace{-1.0em}
% \begin{equation}
%     \textbf{TF}_{\textbf{A}} = \textbf{T}_{\textbf{A}} \otimes \textbf{F}_{\textbf{A}},
% \end{equation}
% where $\otimes$ denotes the tensor multiplication operation. The $(l,k)$-th element of the final 2-D attention map $\textbf{TF}_{\textbf{A}}$ is computed as:
% \vspace{-0.5em}
% \begin{equation}
%     \textbf{TF}_{\textbf{A}}(l,k) = \textbf{T}_{\textbf{A}}(l)\times \textbf{F}_{\textbf{A}}(k)
% \end{equation}
% where $\textbf{T}_{\textbf{A}}(l)$ and $\textbf{F}_{\textbf{A}}(k)$ denote the $l$-th element of $\textbf{T}_{\textbf{A}}$ and the $k$-th element of $\textbf{F}_{\textbf{A}}$, respectively. The output of our TFA module $\widetilde{\textbf{Y}}$ is written as:
% \vspace{-0.5em}
% \begin{equation}
%     \widetilde{\textbf{Y}} = \textbf{Y} \odot \textbf{TF}_{\textbf{A}},
% \end{equation}
% where $\odot$ denotes an element-wise multiplication.}

% \textcolor{blue}{\textbf{Causal Setting}. Since the global average pooling operation in the FA branch relies on future time frames, the proposed TFA module is non-causal. For causal configuration, we can adopt a causal average pooling operation to perform frame-wise average pooling, which only relies on the current and past frames:
% \vspace{-0.5em}
% \begin{equation}
%     \textbf{Z}_{\textbf{F}}(l,k) = \frac{1}{l}\sum_{i=1}^{l} \textbf{Y}(i,k). 
% \vspace{-0.5em}
% \end{equation}}
% \vspace{-0.8em}

% \begin{figure}[t]
% \vskip -0.1in
% \begin{center}
% \centerline{\includegraphics[width=0.4\columnwidth]{diagram/framework.pdf}}
% \caption{Overall diagram of backbone networks.}
% \label{fig1}
% \end{center}
% \vskip -0.3in
% \end{figure}

% \subsection{\textcolor{black}{Network Architecture}}\label{sec3.0}
%\textcolor{red}{Dear Prof. Li, our two backbone ResTCN (TASLP 2020) and MHANet (Speech Communication 2020) are competitive. SOTA speaker separation model ConvTasNet are also based on ResTCN network. In introduction, we may already describe their success on speech enhancement (page 2, paragraph 2)? Here, we describe them and cite the corresponding references. In addition, we also compare ResTCN+TFA with ResTCN+SA baseline (TASLP 2020) in Table IV, which further confirms the efficiency and efficacy of TFA. As our paper focus on a module to improve existing models not developing a sota SE system, I did not report the comparison results with other sota models before. If it is necessary, we can provide more comparisons with other speech enhancement systems as in Table VIII of section IV-D. I will further describe the results if needed.}

% \textcolor{black}{Our proposed \textcolor{black}{TFA module} is applicable to general neural speech enhancement architecture. To set the stage for this study, in Figure \ref{fig1}, we illustrate a typical neural solution to speech enhancement, it is referred to as the backbone network in this paper. Here, two recently proposed backbone networks, \textcolor{black}{ResTCN} \cite{DeepMMSE} and \textcolor{black}{Transformer} \cite{mhanet,attention2017}, are employed. As the key component of the \textcolor{black}{Transformer} layer \cite{attention2017} is the \textcolor{black}{MHA} module, we term the \textcolor{black}{Transformer} layer as a \textcolor{black}{MHANet} block. The network takes the noisy spectral magnitude as input, $|\textbf{X}|\!\in\! \mathbb{R}^{L \times K}$, where $L$ and $K$ denote the number of time frames and frequency bins, respectively. The first layer consists of a 1-D convolution layer with a frame-wise layer normalization followed by the ReLU activation function, that encodes the input into a latent T-F representation of $\mathbb{R}^{L \times d_{model}}$. The output of the first layer is then fed into $B$ stacked ResTCN or MHANet blocks to perform T-F feature transformation. Following the last transformation block is the output layer, which is a 1-D convolution layer with a sigmoidal activation function that generates the estimates of IRM, SMM, PSM, and Xi.}

% \textcolor{blue}{We propose a \textcolor{blue}{TFA module} that is applicable to general neural speech enhancement architecture. To set the stage for this study, in Figure \ref{fig1}, we illustrate a typical neural solution to speech enhancement, it is referred to as the backbone network in this paper. Here, two recently proposed backbone networks, \textcolor{blue}{ResTCN} \cite{DeepMMSE} and \textcolor{blue}{Transformer} \cite{mhanet,attention2017}, are employed. As the key component of the \textcolor{blue}{Transformer} layer \cite{attention2017} is the \textcolor{blue}{MHA} module, we term the \textcolor{blue}{Transformer} layer as a \textcolor{blue}{MHANet} block. The network takes a noisy spectral magnitude as input, $|\textbf{X}|\!\in\! \mathbb{R}^{L \times K}$, where $L$ and $K$ denote the number of time frames and frequency bins, respectively. The first layer consists of a 1-D convolution layer with a frame-wise layer normalization followed by the ReLU activation function, that encodes the input into a latent T-F representation of $\mathbb{R}^{L \times d_{model}}$. The output of the first layer is then fed into $B$ stacked ResTCN or MHANet blocks to perform T-F feature transformation. Following the last transformation block is the output layer, which is a 1-D convolution layer with a sigmoidal activation function that generates the estimates of IRM, SMM, PSM, and Xi.} 

% \begin{figure}[!ht]
% \vskip -0.1in
% \centering
% \begin{subfigure}[t]{0.52\columnwidth}
% \centerline{\includegraphics[width=0.93\columnwidth]{./diagram/ResTCN.pdf}}
% \caption{The original ResTCN block}
% \end{subfigure}\hfill
% \begin{subfigure}[t]{0.4\columnwidth}
% \centerline{\includegraphics[width=0.77\columnwidth]{./diagram/ResTCN_TFA.pdf}}
% \caption{The proposed ResTCN block with TFA module}
% \end{subfigure}
% \caption{Illustration of (a) the ResTCN block and (b) the ResTCN block with the proposed TFA module. $\bigoplus$ denotes the element-wise summation operation.}
% \label{ResTCN}
% \vspace{-1.0em}
% \end{figure}

% \begin{figure}[!ht]
% \vskip -0.1in
% \centering
% \begin{subfigure}[t]{0.42\columnwidth}
% \centerline{\includegraphics[width=0.77\columnwidth]{./diagram/MHANet.pdf}}
% \caption{The MHANet block (Transformer layer)}
% \end{subfigure}
% \hfill
% % \quad
% \begin{subfigure}[t]{0.44\columnwidth}
% \centerline{\includegraphics[width=0.76\columnwidth]{./diagram/MHA_TFA.pdf}}
% \caption{The proposed MHANet block with TFA module}
% \end{subfigure}
% \caption{Illustrations of (a) the MHANet block and (b) our proposed MHANet block with the TFA module.}
% \label{MHA}
% \vspace{-0.5em}
% \end{figure}

% \subsubsection{\textcolor{black}{TFA in ResTCN}}\label{resnet_model}

% \textcolor{black}{In Figure \ref{ResTCN}, we illustrate a backbone network based on a \textcolor{black}{ResTCN} block \cite{DeepMMSE} and  how the proposed TFA module works inside the \textcolor{black}{ResTCN} block. As shown in Figure \ref{ResTCN}(a), the ResTCN block consists of three 1-D causal dilated convolutional modules. Each convolutional module employs a pre-activation design, where the input is pre-activated using frame-wise layer normalization followed by the ReLU activation function. We denote the kernel size, number of filters, and dilation rate for each convolutional module as a tuple of three elements. The first and third convolutional modules have a kernel size of 1, whilst the second convolutional module has a kernel size of $k$. The number of filters is $d_{f}$ for the first and second convolutional modules, and $d_{model}$ for the third convolutional module. The dilation rate, $d$, is employed in the second convolutional module, providing a contextual field over previous speech frames. The dilation rate is cycled as the block index $b\!=\!\left\{1,2,3,...,B\right\}$ increases: $d=2^{\left(b-1 \bmod \left(\log _{2}(D)+1\right)\right.}$, where mod is the modulo operation and $D\!=\!16$ is the maximum dilation rate.} 

% \textcolor{black}{As shown in Figure \ref{ResTCN}(b), the ResTCN block is augmented by a \textcolor{black}{TFA module} to attend to the salient T-F representation. The residual connection \cite{resnet} is applied between the input and output of the block to facilitate gradient optimization.}


% \subsubsection{\textcolor{black}{TFA in MHANet}}\label{mha_model}

% \textcolor{black}{In Figure \ref{MHA}, we illustrate how a TFA module is incorporated into the MHANet backbone \cite{mhanet}. As shown in Figure \ref{MHA}(a), the MHANet block comprises two sub-blocks. The first is an MHA module, and the second is a two-layer fully connected feed-forward network (FFN). A residual connection is applied in each sub-block, followed by frame-wise layer normalization. To capture the T-F energy distribution of speech, as shown in Figure \ref{MHA}(b), we propose to incorporate a TFA module into the MHA module.}

% The proposed \textcolor{blue}{MHANet} block with our \textcolor{blue}{TFA module} is described as follows.

% In the MHA module, a total of $H$ attention heads are employed to enable the model to pay attention to different aspects of information, where $h\!=\!\left\{1,2,3,...,H\right\}$ is the head index. 
% For $h$-th attention head, given an intermediate latent T-F tensor $\textbf{U}\!\in\!\mathbb{R}^{L\times d_{model}}$ as the input to the block, the MHA module first projects the input $\textbf{U}$ to queries ($\textbf{Q}_{h}\!\in \!\mathbb{R}^{L\times d_{k}}$), keys ($\textbf{K}_{h}\!\in\!\mathbb{R}^{L\times d_{k}}$), and values ($\textbf{V}_{h}\!\in\!\mathbb{R}^{L\times d_{v}}$):
% $\textbf{Q}_{h}\!=\!\textbf{U}\textbf{W}_{h}^{Q}$, $\textbf{K}_{h}\!=\!\textbf{U}\textbf{W}_{h}^{K}$, and $\textbf{V}_{h}\!=\!\textbf{U}\textbf{W}_{h}^{V}$, 
% where $\{\textbf{W}_{h}^{Q}, \textbf{W}_{h}^{K}\} \in \mathbb{R}^{d_{model}\times d_{k}}$, and $\textbf{W}_{h}^{V}\in \mathbb{R}^{d_{model}\times d_{v}}$ are different, learned linear projections. 
% The scaled dot-product attention is applied to each head in parallel, and the output is given as:
% \begin{equation}
%     \textbf{A}_{h} = \text{Attention}\left(\textbf{Q}_{h}, \textbf{K}_{h}, \textbf{V}_{h}\right) = \text{softmax}\left(\frac{\textbf{Q}_{h}\textbf{K}_{h}^\top}{\sqrt{d_{k}}}\right)\textbf{V}_{h},
% \end{equation}
% where $d_{k}=d_{v} = d_{model}/{H}$. An upper triangular mask is used to mask out the similarities that include future frames. For more detailed descriptions about attention function, we refer the reader to the original study \cite{attention2017}. The outputs for each attention head are concatenated and linearly projected again, yielding the output of the MHA module as:
% \begin{equation}
%     \text{MHA}\left(\textbf{U}\right) = \text{Concat}(\textbf{A}_{1},...,\textbf{A}_{H})\textbf{W}^{O},
% \end{equation}
% where $\textbf{W}^{O}\!\in\!\mathbb{R}^{d_{model}\times d_{model}}$. The TFA module takes the output from the previous MHA module, and conducts a T-F attention operation (described in Section \ref{sec3.1}) to tell the model to focus on the informative spectral components, resulting in an augmented T-F representation. 

% \textcolor{black}{Given an intermediate latent T-F tensor $\textbf{U}\!\in\!\mathbb{R}^{L\times d_{model}}$ as the input to the block, the MHA module first projects the input $\textbf{U}$ to queries ($\textbf{Q}\!\in \!\mathbb{R}^{L\times d_{model}}$), keys ($\textbf{K}\!\in\!\mathbb{R}^{L\times d_{model}}$), and values ($\textbf{V}\!\in\!\mathbb{R}^{L\times d_{model}}$):
% $\textbf{Q}\!=\!\textbf{U}\textbf{W}^{Q}$, $\textbf{K}\!=\!\textbf{U}\textbf{W}^{K}$, and $\textbf{V}\!=\!\textbf{U}\textbf{W}^{V}$, where 
% $\left\{\textbf{W}^{Q}, \textbf{W}^{K}, \textbf{W}^{V}\right\}\in \mathbb{R}^{d_{model}\times d_{model}}$ are different, learned linear projections. Then they are split into $H$ attention heads, indexed by $h=\left\{1,2,3,...,H\right\}$, and with dimensions $d_{k}$, $d_{k}$, and $d_{v}$, respectively, which enables the model to pay attention to different aspects of information. The scaled dot-product attention is applied to each head in parallel to generate the output,
% \begin{equation}
%     \textbf{A}_{h} = \text{Attention}\left(\textbf{Q}_{h}, \textbf{K}_{h}, \textbf{V}_{h}\right) = \text{softmax}\left(\frac{\textbf{Q}_{h}\textbf{K}_{h}^\top}{\sqrt{d_{k}}}\right)\textbf{V}_{h},
% \end{equation}
% where $d_{k}=d_{v} = d_{model}/{H}$, and $\textbf{K}_{h}^\top$ denotes the transpose of the $h$-th head of keys, $\textbf{K}_{h}$. An upper triangular mask is used to mask out the similarities that include future frames. For more detailed descriptions about attention function, we refer the reader to the original study \cite{attention2017}. The outputs for each attention head are concatenated and linearly projected again, yielding the output of the MHA module as:
% \begin{equation}
%     \text{MHA}\left(\textbf{Q}, \textbf{K}, \textbf{V}\right) = \text{Concat}(\textbf{A}_{1},...,\textbf{A}_{H})\textbf{W}^{O},
% \end{equation}
% where $\textbf{W}^{O}\!\in\!\mathbb{R}^{d_{model}\times d_{model}}$. The TFA module takes the output from the previous MHA module, and conducts a T-F attention operation (described in Section \ref{sec3.1}) to tell the model to focus on the informative spectral components, resulting in an augmented T-F representation.}

% \textcolor{black}{The two-layer FFN takes the output $\textbf{U}^{'}\!\in\! \mathbb{R}^{L\times d_{model}}$ from the first sub-block, and performs two linear transformations with a ReLU activation in the first layer:
% \begin{equation}
%     \text{FFN}(\textbf{U}^{'}) = \text{max}(0, \textbf{U}^{'}\textbf{W}^{1} + \textbf{b}^{1})\textbf{W}^{2} + \textbf{b}^{2},
% \end{equation}
% where $\textbf{W}^{1}\in \mathbb{R}^{d_{model}\times d_{f\!f}}$, $\textbf{b}^{1}\in \mathbb{R}^{d_{f\!f}}$, $\textbf{W}^{2}\in \mathbb{R}^{d_{f\!f}\times d_{model}}$, and $\textbf{b}^{2}\in \mathbb{R}^{d_{model}}$. The size of input and output is $d_{model}$, and the inner-layer has a size of $d_{f\!f}$.}

% \textbf{\textcolor{black}{Model Configuration.}}
% \textcolor{black}{For ResTCN backbone, we adopt the parameter settings as in \cite{DeepMMSE} to build the network: $d_{model}\!=\!256$, $B\!=\!40$, $d_{f}\!=\!64$, and $k\!=\!3$.
% For the MHANet backbone, we follow the parameter settings in \cite{mhanet}: $B\!=\!5$, $d_{model}\!=\!256$, $H\!=\!8$, and $d_{f\!f}\!=\!1024$. For our proposed TFA module, a kernel size $k_{t\!f\!a}\!=\!17$ is adopted in the dilated 1-D convolution.} 


%\textcolor{red}{we need to justify why we use two 1D map to form a 2D map? why not directly estimate a 2D map?} \\
%\textcolor{red}{There are multiple ways to compute a 2D attention map. the most straightforward method is to apply a 2D attention directly. because time-frequency bins are correlated, when we compute 1D along time axis and 1D along freq axis, basically we ignore the time-frequency relationship which is against our own idea. }

%\textcolor{red}{This paper is written like a technical report, there is no justification nor substantiation. we simply say we do this, we do that. this is not the way to write a scientific paper. for example, we say that 1-D attention first uses a global average pooling, ... who proved this is the best, and there is not alternative? is it because there is no other way to do the same thing, or because it is already proven by a reference (cite the paper) that this works better than other alternative? for example, why don't we do a 2D attention map directly? we cannot simply exclude other options in the theoretical explanation. this section is for us to debate what is the best solution, and is NOT for us to tell what we do. only after we debate what is the best, then we can zoom into a solution. otherwise, the paper is seen as a technical report, simply reporting what we have done. Readers are not interested in what we do at all, 99\% of readers/reviewers learn how we motivate our idea and algorithms, and use our ideas in their work. No one will repeat our experiments unless they agree that the method is absolutely the best way to the problem, and it is our job to prove that we offer the best way.  ---- please justify why neural speech enhancement solution needs a TFA module in Section II.C!n i don't see anything in introduction that justify the need of a TFA module in neural speech enhancement architecture. please point to me which line? in the introduction, we only say the T-F attention is not used, but there is no justification that TFA like the one we propose may lead to an improvement.}
%\textcolor{blue}{Thank you Prof. Li. I have revised the description in this section.}

% \subsection{Implementation}\label{sec3.2}
% The proposed time-frequency attention is applicable to neural speech enhancement architecture in general. Here we integrate it into two widely used backbones, i.e., ResTCN and Transformer. As the MHA module is the  key component of the Transformer layer, we term the Transformer layer as a MHANet block. 


% In this subsection, we will give detailed descriptions of the architecture configuration of models. All models employ the architecture as shown in figure \ref{architecture}. The input is the noisy spectral magnitude, $|\textbf{X}|\in \mathbb{R}^{L \times K}$, where $L$ and $K$ denote the number of time frames and frequency bins, respectively. The first layer projects the input to an embedding with a size of $d_{model}$, where a fully-connected layer that includes frame-wise layer normalization followed by the ReLU activation function is employed. The output of the first layer is then fed into $B$ stacked transformation blocks (shown in Figure \ref{ResTCN} and \ref{MHA}). Following the last block is the output layer, which is a fully-connected layer with a sigmoidal activation function. 

% \begin{figure}[!ht]
% % \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.4\columnwidth]{diagram/framework.pdf}}
% \caption{Architecture of models.}
% \label{architecture}
% \end{center}
% \vskip -0.2in
% \end{figure}


% \section{Experimental Setup}\label{sec:4}

% \subsection{Datasets and Feature Extraction}\label{sec:4.1}

% First, we describe the clean speech and noise data in this study. For clean speech recordings, we use the \textit{train-clean-100} set from the Librispeech corpus \cite{Librispeech} as the training set, which includes $28\,539$ utterances spoken by $251$ speakers. The noise recordings in the training set are taken from the following datasets: the QUT-NOISE dataset \cite{QUT}, the Nonspeech dataset \cite{Nonspeech}, the Environmental Background Noise dataset \cite{ENV1,ENV2}, the RSG-10 dataset \cite{RSG} (\textit{voice babble}, \textit{F16}, and \textit{factory welding} are excluded for testing), the Urban Sound dataset \cite{Urban} (\textit{street music} recording no $26\,270$ is excluded for testing), the noise set from the MUSAN corpus \cite{MUSAN}, and colored noise recordings (with an $\alpha$ value ranging from $-2$ to $2$ in increments of 0.25). Noise recordings that are over 30 seconds in length are split into segments of $30$ seconds or less. This gives a total of $6\,809$ noise recordings, each with a length less than or equal to $30$ seconds. For validation experiments, we randomly select $1\,000$ clean speech and noise recordings (without replacement) and remove them from the aforementioned clean speech and noise sets. Each clean speech recording is mixed with a random section of one noise recording at a randomly selected SNR level between $-10$ dB and $20$ dB in 1 dB increments. This generates $1\,000$ noisy speech signals as the validation set. 

% For evaluation experiments, we adopt the recordings of four real-world noise sources (excluded from training set) including two non-stationary and two coloured. The two non-stationary noise sources are the \textit{voice babble} from the the RSG-10 noise dataset \cite{RSG} and \textit{street music} from the Urban Sound dataset \cite{Urban}. The two colored noise sources are \textit{F16} and \textit{factory welding} from RSG-10 noise dataset \cite{RSG}. For each of the four noise recordings, ten clean speech recordings (without replacement) randomly selected from the \textit{test-clean-100} of Librispeech corpus \cite{Librispeech} are mixed with a random segment of the noise recordings at the following SNR levels: $\left\{ -5 \text{ dB}, 0 \text{ dB}, 5 \text{ dB}, 10 \text{ dB}, 15 \text{ dB} \right\}$. This generates $200$ noisy mixtures for evaluation.

% In this study, a square-root-Hann window function is used for spectral analysis and synthesis, with a frame length of 32 ms ($512$ samples) and a frame-shift of 16 ms ($256$ samples). \textcolor{black}{The 257-point single-sided STFT magnitude spectrum of noisy speech, which includes both the DC frequency component and the Nyquist frequency component, is used as the input.}

% \subsection{Training Methodology}
% Here, we describe the details of training methodology used in this study. A mini-batch size of $10$ noisy speech utterances is used for each training iteration. The noisy speech signals are created as follows: each clean speech recording selected for the mini-batch is mixed with a random section of a randomly selected noise recording at a randomly selected SNR level (-10 dB to 20 dB, in 1 dB increments). The selection order for the clean speech recordings is randomised for each epoch. For the three masking-based training objectives (i.e., IRM, SMM, and PSM), we adopt the mask approximation to learn the mask, where the mean-square error (MSE) is the loss function. For Xi, the cross-entropy is employed as the loss function \cite{nicolson2019deep,DeepMMSE}. Each utterance in a mini-batch is padded with zeros, giving it the same number of time frames as the longest noisy utterance. 

% All models are trained from scratch. For ResTCN \cite{DeepMMSE} and its TFA augmented variants, the \textit{Adam} algorithm with default hyper-parameters \cite{Adam} and a learning rate of $0.001$ is used for gradient descent optimisation. For MHANet \cite{mhanet} and its TFA augmented variant, the \textit{Adam} algorithm with parameters as in \cite{attention2017}, i.e., $\beta_{1}=0.9$, $\beta_{2}=0.98$, and $\epsilon=10^{-9}$ is used for training. The gradient clipping technique is used for all models, where the gradients are clipped between $[-1,1]$. As the training of MHANet is sensitive to the learning rate \cite{mhanet,attention2017}, we adopt the warm-up training strategy in \cite{attention2017}, where the learning rate is adjusted during the training process according to the rule:
% \begin{equation}
% \vspace{-0.2em}
%     lr = d_{model}^{-0.5}\cdot \textrm{min} \left(n\_step^{-0.5}, n\_step \cdot w\_steps^{-1.5}\right)
% \end{equation}
% where $n\_step$ and $w\_steps$ denote the number of training steps and warm-up training steps, respectively. Following \cite{mhanet}, the number of steps $w\_steps\!=\!40\,000$ is adopted for warm-up training in this work.


% \subsection{Evaluation Metrics}
% In our experiments, five widely used metrics are adopted for extensive speech enhancement evaluations, including perceptual evaluation of speech quality (PESQ) \cite{PESQ}, extended short-time objective intelligibility (ESTOI) \cite{estoi}, and three composite metrics \cite{composite}. For the PESQ metric, we adopt the wide-band PESQ \cite{PESQ}, which typically produces a lower score than the narrow-band counterpart \cite{restcnsa}. The value range of PESQ is $[-0.5, 4.5]$ and the value of ESTOI is typically in $[0, 1]$. The three composite metrics are mean opinion score (MOS) predictors of the signal distortion (CSIG) \cite{composite}, the background-noise intrusiveness (CBAK) \cite{composite}, and the overall signal quality (COVL) \cite{composite}, respectively. The value range of the three composite metrics is $[0, 5]$. For all of the above five metrics, a higher score indicates better enhancement performance. The word error rate (WER\%) is adopted to evaluate the downstream ASR performance, and a lower WER\% score means better speech recognition performance.

% \subsection{Comparative Models}
% We evaluate the proposed TFA module as part of two backbone networks (ResTCN \cite{DeepMMSE} and MHANet \cite{mhanet}) on four training objectives. The proposed ResTCN and MHANet with the TFA module are denoted by ``ResTCN+TFA'' and ``MHANet+TFA'', respectively. In addition, we conduct an ablation study to validate the efficacy of each component (the TA and FA) in the TFA module. Similarly, we denote the ResTCN and MHANet with the TA and FA by ``ResTCN+TA'', ``MHANet+TA'',  ``ResTCN+FA'',  and ``MHANet+FA'', respectively. All models in this study are implemented using Tensorflow 1.13. \textcolor{black}{The experiments were conducted on an NVIDIA Tesla V100 graphics processing unit (GPU) and an Intel Xeon Platinum 8163 CPU at 2.50 GHz (96 logical processors).}

% % \textcolor{blue}{To Prof. Li, we use the platform equipped with an NVIDIA Tesla V100 GPU and an Intel Xeon Platinum 8163 CPU at 2.50 GHz. For RTF test, we use GPU to conduct it.}

% \begin{figure}[!b]
% \vskip -0.1in
% \centering
% \begin{subfigure}[t]{0.495\columnwidth}
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/resnet_irm_train_symbol.pdf}}
% \caption{}
% \end{subfigure}\hfill
% \begin{subfigure}[t]{0.490\columnwidth}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/resnet_irm_val_symbol.pdf}}
% \caption{}
% \end{subfigure}
% \caption{The (a) training error and (b) validation error of ResTCN and proposed models with ResTCN as backbone on IRM training objective. }
% \vskip -0.2in
% \label{resnet_irm}
% \end{figure}

% \begin{figure}[!htbp]
% % \vskip 0.1in
% \centering
% \begin{subfigure}[t]{0.495\columnwidth}
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/mha_irm_train_symbol.pdf}}
% \caption{}
% \end{subfigure}\hfill
% \begin{subfigure}[t]{0.490\columnwidth}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/mha_irm_val_symbol.pdf}}
% \caption{}
% \end{subfigure}
% \caption{The (a) training error and (b) validation error of MHANet and proposed models with MHANet as backbone on IRM training objective.}
% \vskip -0.2in
% \label{mha_irm}
% \end{figure}

% \begin{figure}[!h]
% % \vskip 0.1in
% \centering
% \begin{subfigure}[t]{0.495\columnwidth}
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/resnet_train_smm_symbol.pdf}}
% \caption{}
% \end{subfigure}\hfill
% \begin{subfigure}[t]{0.495\columnwidth}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/resnet_val_smm_symbol.pdf}}
% \caption{}
% \end{subfigure}
% \caption{The (a) training error and (b) validation error of ResTCN and proposed models with ResTCN as backbone on SMM training objective.}
% \vskip -0.2in
% \label{resnet_smm}
% \end{figure}

% \begin{figure}[!h]
% % \vskip 0.1in
% \centering
% \begin{subfigure}[t]{0.495\columnwidth}
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/mha_train_smm_symbol.pdf}}
% \caption{}
% \end{subfigure}\hfill
% \begin{subfigure}[t]{0.495\columnwidth}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/mha_val_smm_symbol.pdf}}
% \caption{}
% \end{subfigure}
% \caption{The (a) training error and (b) validation error of MHANet and proposed models with MHANet as backbone on SMM training objective.}
% \vskip -0.2in
% \label{mha_smm}
% \end{figure}

% \begin{figure}[!h]
% % \vskip 0.1in
% \centering
% \begin{subfigure}[t]{0.495\columnwidth}
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/resnet_psm_train_symbol.pdf}}
% \caption{}
% \end{subfigure}\hfill
% \begin{subfigure}[t]{0.495\columnwidth}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/resnet_psm_val_symbol.pdf}}
% \caption{}
% \end{subfigure}
% \caption{Training error (a) and validation error (b) of ResTCN and the proposed models with ResTCN as backbone on PSM training objective.}
% \vskip -0.2in
% \label{resnet_psm}
% \end{figure}

% \begin{figure}[!h]
% % \vskip 0.1in
% \centering
% \begin{subfigure}[t]{0.495\columnwidth}
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/mha_psm_train_symbol.pdf}}
% \caption{}
% \end{subfigure}\hfill
% \begin{subfigure}[t]{0.495\columnwidth}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/mha_psm_val_symbol.pdf}}
% \caption{}
% \end{subfigure}
% \caption{Training error (a) and validation error (b) of MHANet and proposed models with MHANet as backbone on PSM training objective.}
% \vskip -0.2in
% \label{mha_psm}
% \end{figure}

% \begin{figure}[!ht]
% % \vskip 0.1in
% \centering
% \begin{subfigure}[t]{0.495\columnwidth}
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/resnet_magXi_train_symbol.pdf}}
% \caption{}
% \end{subfigure}\hfill
% \begin{subfigure}[t]{0.495\columnwidth}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/resnet_magXi_val_symbol.pdf}}
% \caption{}
% \end{subfigure}
% \caption{Training error (a) and validation error (b) of ResTCN and the proposed models with ResTCN as backbone on Xi training objective.}
% \vskip -0.2in
% \label{resnet_magXi}
% \end{figure}

% \begin{figure}[!ht]
% % \vskip 0.1in
% \centering
% \begin{subfigure}[t]{0.495\columnwidth}
% \captionsetup{justification=centering}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/mha_magXi_train_symbol.pdf}}
% \caption{}
% \end{subfigure}\hfill
% \begin{subfigure}[t]{0.495\columnwidth}
% \centerline{\includegraphics[width=\columnwidth]{./loss_curves/mha_magXi_val_symbol.pdf}}
% \caption{}
% \end{subfigure}
% \caption{The training error (a) and validation error (b) of MHANet and the proposed models with MHANet as backbone on Xi training objective.}
% \vskip -0.3in
% \label{mha_magXi}
% \end{figure}


% \section{Experimental Results}\label{sec:5}
% \subsection{Training and Validation Error}
% We first observe the training and validation errors across the models. The error curves produced by each of the models on the four training objectives (i.e., IRM, SMM, PSM, and Xi) are shown in Figures \ref{resnet_irm}-\ref{mha_irm}, Figures \ref{resnet_smm}-\ref{mha_smm}, Figures \ref{resnet_psm}-\ref{mha_psm}, and Figures \ref{resnet_magXi}-\ref{mha_magXi}, respectively, where each model is trained for 250 epochs. We observe similar trends of error curves on different training objectives. The purple curves are for ResTCN and MHANet, and the red curves are for ResTCN+TFA and MHANet+TFA. It can be easily observed that ResTCN+TFA and MHANet+TFA yield significantly lower training and validation errors than ResTCN and MHANet, which demonstrates the effect of the TFA module. In addition, the ablation study also confirms the efficacy of the FA and TA modules. One can observe that ResTCN+FA (blue curves) and ResTCN+TA (yellow curves) produce significantly lower training and validation error as compared to ResTCN. ResTCN+FA yields close error curves with ResTCN+TA. MHANet+FA (blue curves) and MHANet+TA (yellow curves) also achieve an obvious lower training and validation error than MHANet. For MHANet, applying the TA module achieves lower training and validation error than the FA module. Among the TFA, TA, and FA modules, the TFA module consistently produces the lowest training and validation error across different training objectives.
% \input{pesq.tex}

% \input{stoi.tex}

% \input{csig.tex}
% \input{cbak.tex}
% \input{covl.tex}

% \subsection{Experiment on Enhancement Performance}
% Tables \ref{tab:pesq} and \ref{tab:estoi} list the wide-band PESQ and ESTOI scores obtained by each of the models in all noisy test conditions, respectively, in four training objectives. The highest PESQ and ESTOI scores for each condition are highlighted in boldface. Compared to unprocessed noisy recordings, for all of the training objectives, our proposed models provide substantial improvements in terms of both PESQ and ESTOI scores. Taking the \textit{street music} noise with SNR of 5 dB as a case, ResTCN+TFA and MHANet+TFA with the SMM achieve 0.70 and 0.65 gains on PESQ, and 19.40\% and 19.43\% gains on ESTOI, respectively. Among the four training objectives, overall, PSM and Xi show better performance than IRM and SMM in terms of PESQ. The obvious superiority in terms of ESTOI is not observed on any of the training objectives. 

% It is also easy to observe that for all training objectives, applying the TFA module significantly improves the PESQ and ESTOI scores of ResTCN and MHANet backbones with negligible parameter overheads ($2.72 \text{K}$ and $0.34 \text{K}$), demonstrating its effectiveness on speech enhancement. In the \textit{F16} noise with SNR of 5 dB case, for instance, ResTCN+TFA and MHANet+TFA with the IRM provide 0.24 and 0.19 PESQ improvements, 3.98\% and 2.84\% ESTOI improvements, respectively, over the corresponding baselines. From the comparison results, among the two baselines, ResTCN benefits more from the TFA module in most cases. 

% In addition, the performance evaluations of the TA and FA modules are also reported in the ablation study. The TA and FA modules produce two 1-D attention maps to model the energy distribution of speech along time and frequency dimensions, respectively. As shown in Tables \ref{tab:pesq} and \ref{tab:estoi}, \textcolor{black}{both ResTCN and MHANet achieve performance gains, in terms of PESQ and ESTOI, due to the TA and FA modules in most cases. Overall, the TA module provides more PESQ and ESTOI gains than the FA module.} \textcolor{black}{This could be explained by the fact that the temporal attention mechanism assigns the differentiated attention weights along the time axis, acting like a soft voice activity detector (VAD). The temporal information could be more informative than the spectral one in speech enhancement.} The TFA module effectively combines the TA and FA modules to produce a 2-D attention map for modeling the T-F distribution of speech spectral components, which attains the highest PESQ and ESTOI scores in almost all cases. 

% Tables \ref{tab:csig}-\ref{tab:covl} report the average CSIG, CBAK, and COVL scores for each of the SNR levels (covering four noise sources), respectively, and the highest scores are highlighted in boldface. It is obvious that applying the TFA module to ResTCN and MHANet significantly improves their performance in terms of the three composite metrics, across different training objectives. In the -5 dB SNR case, for instance, ResTCN+TFA and MHANet+TFA with the IRM improve CSIG by 0.23 and 0.17, CBAK by 0.1 and 0.1, and COVL by 0.17 and 0.12, respectively. Again, compared to MHANet, ResTCN benefits more from the TFA module. 

% The TA and FA modules also provide substantial improvements to baselines in the three metrics. For instance, in the 5 dB case, the SMM is used as the training objective. For ResTCN applying FA and TA modules improves CISG by 0.21 and 0.21, CBAK by 0.12 and 0.12, and COVL by 0.19 and 0.20, respectively. For MHANet, applying FA and TA modules improves CISG by 0.07 and 0.12, CBAK by 0.08 and 0.10, and COVL by 0.09 and 0.13, respectively.  Overall, the TA module performs slightly better than the FA. In the case of Xi as training objective and the SNR of 15 dB, MHANet+FA obtains the same CSIG scores (4.18) with MHANet+TFA. In all other cases, the TFA module obtains the highest CSIG, CBAK, and COVL scores. 

% \input{eval_size.tex}
% \subsubsection{The Number of Building Blocks} We evaluate the effectiveness of the TFA module across different numbers of building blocks with IRM as the training objective. ResTCN-20 and ResTCN-30 denote ResTCN models with 20 and 30 ResTCN blocks, respectively. MHANet-4 and MHANet-6 denote MHANet models with 4 and 6 MHANet blocks, respectively. The experimental results are given in Table \ref{tab7}. It can be seen that the TFA module consistently affords substantial improvements to both ResTCN and MHANet. Here, we also report the evaluation results of ResTCN with self-attention (ResTCN+SA) \cite{restcnsa}, which further demonstrates the efficacy and efficiency of our TFA module. ResTCN+SA \cite{restcnsa} employs a multi-head self-attention module as a pre-processing module followed by a ResTCN model. Compared to ResTCN+SA, ResTCN+TFA shows substantial superiority in terms of performance scores and parameter efficiency. In addition, we also study our TFA module in the recent Conformer \cite{conformer-asr,conformer-se} and the self-attentive TCN (SA-TCN) \cite[Fig. 1]{sa-tcn} network architectures. Here, we adopt the Conformer model (Conformer-5) with 5 Conformer building blocks \cite[Fig. 1 (right)]{conformer-asr} and the 2-stage SA-TCN (2S-SA-TCN) \cite{sa-tcn} as the baseline backbones. A Conformer block consists of four modules stacked together, i.e., an FNN module, a self-attention module, a convolution module, and a second FNN module. Each stage of 2S-SA-TCN includes a self-attention module, followed by 24 ResTCN blocks. For Conformer, the TFA module is incorporated into the convolution module (following the third convolution unit) and the self-attention module (as shown in Fig. \ref{MHA}(b)). For 2S-SA-TCN, the TFA module is incorporated into the self-attention module and the ResTCN block as shown in Fig. \ref{MHA}(b) and Fig. \ref{ResTCN}(b), respectively. It is clear that the TFA module consistently provides significant improvements to both Conformer and 2S-SA-TCN.

% \input{wer.tex}

% \subsection{Experiment on ASR Performance}
% In real-world environments, speech enhancement is often used as a front-end to improve the noise robustness of an ASR system. In this section, we investigate the effectiveness of our proposed model as the front-end for a robust ASR system. DeepSpeech\footnote{The implementation of the DeepSpeech ASR system is available at: https://github.com/mozilla/DeepSpeech. The latest release model (0.9.3) is used in our experiment.} \cite{deepspeech}, an open-source ASR system developed using end-to-end deep learning technique, is used in this study to conduct ASR experiments for evaluating front-end performance. The RNN-based acoustic model and language model are used in DeepSpeech. Here, we treat the DeepSpeech ASR system as a black box, without fine-tuning during the experiment.

% Table \ref{tab:wer} presents the average WER\%\footnote{The code implementation for WER\% calculation is available at: https://github.com/jitsi/jiwer. Several pre-processing steps are applied before WER\% calculation, including transforming a sentence into a list of words, removing multiple spaces between words and empty strings, and removing all leading and trailing spaces.} scores attained by all the models for each SNR condition and the WER\% scores averaged across all SNR conditions. 
% It can be seen that all front-end models achieve performance gains substantially in terms of WER\% compared to the ASR performance on unprocessed noisy recordings. Overall, for two backbones (ResTCN and MHANet) and four training objectives, our proposed TFA module attains the lowest average WER\% scores over all conditions, and performs the best under most SNR conditions. 
% The evaluation shows that the proposed TFA module demonstrates substantial performance improvement to the two baselines, i.e., ResTCN and MHANet. For the four training objectives similar performance trends are observed, and on average, the IRM achieves the best performance. With the IRM as the training objective, the TFA module improves the ResTCN and MHANet baselines, with a relative  WER\% reduction of 15.78\% and 8.58\% over all conditions, respectively. In addition, the FA and TA modules provide a relative WER\% reduction of 10.46\% and 7.40\% to the ResTCN, and that of 4.89\% and 4.84\% to the MHANet. The ablation results on ASR performance also illustrate the efficacy of the TA and FA modules. 

% \input{./execution.tex}
% \textcolor{blue}{In Table \ref{exet}, we compare the execution times of the models (ResTCN, ResTCN+TFA, MHANet, and MHANet+TFA) on the test set (200 noisy speech) described in Section \ref{sec:4.1}, averaged over 5 executions. Each noisy speech is processed individually and the total duration of the noisy speech in the test set is $1\,515.15$ seconds. The execution times are measured on an NVIDIA Tesla P100 GPU. It can be found that the introduction of the TFA module does not significantly increase the computation burden.}

% \textcolor{blue}{In Table \ref{exet}, we compare the inference speed of the models (ResTCN, ResTCN+TFA, MHANet, and MHANet+TFA) on the test set (200 noisy speech) described in Section \ref{sec:4.1}. The inference speed is measured in the real-time factor (RTF), which is the time taken to process certain speech divided by the total duration of the speech. Each noisy speech is processed individually and the total duration of the noisy speech in the test set is $1\,515.15$ seconds. The RTFs are measured on an NVIDIA Tesla V100 GPU, averaged over 5 executions. It can be observed that the introduction of the TFA module does not significantly increase the computation burden.} 

% \textcolor{black}{In Table \ref{exet}, we compare the computation required by the models (ResTCN, ResTCN+TFA, MHANet, and MHANet+TFA), in terms of real-time factor (RTF) \cite{cleanunet}, which is the ratio of the time taken to process a speech utterance to the duration of the utterance. The RTFs are measured on an NVIDIA Tesla V100 GPU, averaged over 10 executions. We use a batch size of 20 noisy mixtures with a length of 7 seconds. It can be observed that the introduction of the TFA module does not significantly increase the computational cost.} 

% \input{./demand.tex}
% \subsection{Comparative Study} 
% % \subsection{Comparison to State-of-the-Art Methods} 

% In this section, we compare our proposed method with multiple state-of-the-art systems on the Voicebank-DEMAND dataset \cite{demand}. As reported in Table \ref{demand}, it can be observed that our proposed model \textcolor{black}{ResTCN+TFA} with \textcolor{black}{Xi} training objective (ResTCN+TFA-Xi) demonstrates highly competitive performance to those methods with respect to the five evaluation metrics. More significantly, our TFA module provides a simple and flexible way to improve existing network architectures for speech enhancement. It is worth noting that we focus on a small module rather than a system beyond existing enhancement systems.

%\textcolor{red}{to Qiquan: the terminology of this paper needs an overall re-write. The title of this paper is TFA-SE. however, i don't know what TFA-SE refers to? on the other hand, this paper introduces many names that are not consistent with the opening of this paper. about half-way, i got totally lost. I don't think any reviewers can understand this paper in the current form. and i find that this paper is now in a worst shape than few weeks ago.}
 
%\textcolor{blue}{Dear Prof.Li, Thank you for your comments. 1./ TFA-SE means that we use TFA for speech enhancement, it does not denote a specific model. 2./ I will check the names and keep consistent. 3./ I just rename MagXi as Xi and do not change other contents compare to the before version. I check them and keep these terminology consistent in the whole paper.}

%\textcolor{blue}{Dear Prof. Li, I have marked all the terminology with blue color and kept them consistent.}


% \section{Conclusion}\label{sec:6}
% In this study, we propose the TFA module, a lightweight and flexible attention module designed to model the distribution of speech components in the T-F representation, improving the representational power of a network. Our TFA module consists of two parallel attention branches, i.e., the TA and FA modules, which produce two attention maps to model the speech distribution along time-frame and frequency dimensions, respectively. We evaluate the TFA module as part of ResTCN and Transformer backbone networks and adopt four widely used training objectives to conduct extensive speech enhancement experiments.

% Our experimental results demonstrate that the TFA module consistently provides significant improvements to the baseline networks in terms of five metrics (PESQ, ESTOI, CSIG, CBAK, and COVL). Moreover, the evaluation results on the downstream ASR also demonstrate the effectiveness of the TFA module. This reveals the importance of the \textit{priori} about the energy distribution of speech for speech enhancement, and the inability of the previous models to capture that \textit{priori}. We believe that the success of the TFA module provides a new idea for the design of network architecture to boost speech enhancement. \textcolor{black}{In future studies, we plan to further investigate the effectiveness of the TFA module across other commonly used datasets and other speech processing tasks such as speech recognition.} In addition, we will also try to extend our proposed TFA module to multi-channel scenarios. 


% The most straightforward way to obtain the T-F attention is to adopt convolution to exploit the position information and compute a 2-D attention map. However, the position information exploited by convolution is inherently implicit and local.  


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% use section* for acknowledgment
% \section*{Acknowledgement}
% The authors thank the Circuit and Systems (signal processing) Group at Delft University of Technology for providing an implementation of the MMSE speech spectrum estimator with generalised Gamma priors.

% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
% \bibliography{IEEEabrv,../bib/paper}
% \scriptsize
% \newpage
\bibliography{IEEEabrv,./myreference}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{99}
% \end{thebibliography}

% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}
% [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{QiquanZhang}}]{Qiquan Zhang}
% received the B.S. and Ph.D degrees in electronic science \& technology from Harbin Institute of Technology, China, in 2015 and 2020, respectively. He is currently a postdoctoral research fellow with the Human Language Technology Lab, National University of Singapore, Singapore. His research interests include digital signal processing, speech processing, speech enhancement, machine learning, and deep learning.
% \end{IEEEbiography}

% \newpage
% \newpage
% \newpage
% if you will not have a photo at all:
% \vspace{-13em} 
% \newpage
% \newpage

% \begin{IEEEbiographynophoto}{Qiquan Zhang} (Member IEEE) received the B.Sc. and Ph.D. degrees in electronic science \& technology from Harbin Institute of Technology, China, in 2015 and 2020, respectively. Since Mar. 2021, he has been a postdoctoral research fellow with the Human Language Technology Lab in the Department of Electrical and Computer Engineering, National University of Singapore (NUS) under the supervision of Prof. Haizhou Li. He is currently a postdoctoral research associate with the Signal Processing Lab, University of New South Wales (UNSW), Sydney, Australia under the supervision of Prof. Eliathamby Ambikairajah and Prof. Haizhou Li. His research interests include statistical signal processing, speech processing, audio-visual speech processing, speech enhancement, noise estimation, machine learning, and deep learning.
% \end{IEEEbiographynophoto}

% \begin{IEEEbiographynophoto}{Buddhi Wickramesinghe} received the B.Sc. (Hons.) degree in Computer Engineering from the University of Peradeniya, Peradeniya, Sri Lanka in 2016 and the Ph.D. degree in Electrical Engineering from the University of New South Wales, Sydney, Australia in 2021. She recently completed her postdoctoral studies at Purdue University, West Lafayette, IN, United States. Her research interests include neuromorphic computing, explainable machine learning and auditory and speech signal processing.
% \end{IEEEbiographynophoto}

% \vspace{-1mm}
% \begin{IEEEbiography}
% [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Xinyuan_Qian.png}}]{Xinyuan Qian} (Member IEEE) 
% received the B.Eng. (with First Class Hons.) and M.Sc. (with Distinction) degrees both from the University of Edinburgh, Edinburgh, U.K, She received the Ph.D. degree from Queen Mary, University of London, London, U.K.. She has been a research fellow with National University of Singapore since Feb. 2020 and then become a visiting researcher with The Chinese University of Hong Kong, Shenzhen, China. She is currently an associate professor with the University of Science and Technology Beijing, China. Her research interests include speech processing, audio-visual sound localization, speech enhancement, and deep learning.
% \end{IEEEbiography}
% \vspace{-1mm}
% \begin{IEEEbiography}
% [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Zhaoheng_Ni.png}}]{Zhaoheng Ni}
% received the B.Sc. degree in Biology and Medical Engineering from Beihang University and M.S.c degree in Computer Science from City University of New York. He is currently a Research Scientist at Meta AI, United States. His research interests include single-channel and multi-channel speech enhancement, speech separation, and Automatic Speech Recognition.
% \end{IEEEbiography}
% \vspace{-1mm}

% \begin{IEEEbiography}
% [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Aaron_Nicolson.png}}]{Aaron Nicolson} received his BEng degree (Class 1A Hons.) from Griffith University, in 2016. In 2020, he completed his PhD at the Signal Processing Laboratory of Griffith University, Brisbane, Australia under the supervision of Kuldip K. Paliwal. He is currently a postdoctoral research fellow at the Commonwealth Scientific and Industrial Research Organisation (CSIRO), stationed at the Australia e-Health Research Centre in Brisbane, Australia, since 2020. He is also a part of CSIRO's machine learning \& Artificial Intelligence Future Science Platform. His research interests include speech processing, speech enhancement, and medical image analysis.
% \end{IEEEbiography}

% \begin{IEEEbiographynophoto}
% % \begin{IEEEbiography}
% % [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Eliathamby_Ambikairajah.jpg}}]
% {Eliathamby Ambikairajah} (Senior Member IEEE) received his BSc (Eng) (Hons) degree from the University of Sri Lanka, and received his PhD degree in Signal Processing from Keele University, UK. He was appointed as Head of Electronic Engineering and later Dean of Engineering at the Athlone Institute of Technology in the Republic of Ireland from 1982 to 1999. His key publications led to his repeated appointment as a short-term Invited Research Fellow with the British Telecom Laboratories, U.K., for ten years from 1989 to 1999. Professor Ambikairajah served as the Acting Deputy Vice-Chancellor Enterprise during 2020, after previously serving as the Head of School of Electrical Engineering and Telecommunications, University of New South Wales (UNSW), Australia from 2009 to 2019. His research interests include speaker and language recognition, emotion detection and biomedical signal processing. He has authored and co-authored approximately 300 journal and conference papers and is the recipient of many competitive research grants. He was a Faculty Associate with the Institute of Infocomm Research (A*STAR), Singapore from 2010 to 2018, and was an Advisory Board member of the AI Speech Lab at AI Singapore (2019-2021). Professor Ambikairajah was an Associate Editor for the IEEE Transactions on Education from 2012 to 2019. He received the UNSW Vice-Chancellor’s Award for Teaching Excellence in 2004 for his innovative use of educational technology and innovation in electrical engineering teaching programs, and in 2014 he received the UNSW Excellence in Senior Leadership Award. In 2019 was the recipient of the People’s Choice Award as part of the UNSW President’s Awards. Professor Ambikairajah was an APSIPA Distinguished Lecturer for the 2013-14 term. He is a Fellow and a Chartered Engineer of the IET UK and Engineers Australia (EA) and is a life Senior Member of the IEEE and a Life Member of APSIPA.
% % \end{IEEEbiography}
% \end{IEEEbiographynophoto}
% \vspace{-2mm}

% \begin{IEEEbiographynophoto}
% % \begin{IEEEbiography}
% % [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{vidhya.jpg}}]
% {Vidhyasaharan Sethu} (Member, IEEE) received the BE degree from Anna University, Chennai, India, in 2005, the MEngSc degree in signal processing, and the PhD degree in speech signal processing from the University of New South Wales (UNSW), Sydney, Australia, in 2006 and 2010, respectively. He is currently a senior lecturer with the School of Electrical Engineering and Telecommunications, UNSW, Sydney, 2052, Australia. From 2010 to 2013, he was a postdoctoral fellow with the Speech Processing Research Group, UNSW. He has coauthored approximately 100 publications and serves on the editorial board of Computer Speech and Language. His research interests include the application of machine learning to speech processing and affective computing, speaker recognition, and computational paralinguistics.
% % \end{IEEEbiography}
% \end{IEEEbiographynophoto}

% \begin{IEEEbiography}
% \begin{IEEEbiographynophoto}
% % [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Haizhou_Li.jpg}}]
% {Haizhou Li} (Fellow, IEEE) received the B.Sc., M.Sc., and Ph.D. degrees in electrical and electronic engineering from South China University of Technology, Guangzhou, China in 1984, 1987, and 1990 respectively. He is currently a Presidential Chair Professor with the School of Data Science, The Chinese University of Hong Kong, Shenzhen, China. He is also an Adjunct Professor with the Department of Electrical and Computer Engineering, National University of Singapore (NUS). Prior to that, he taught in the University of Hong Kong (1988-1990) and South China University of Technology (1990-1994). He was a Visiting Professor at CRIN in France (1994-1995), Research Manager at the AppleISS Research Centre (1996-1998), Research Director in Lernout \& Hauspie Asia Pacific (1999-2001), Vice President in InfoTalk Corp. Ltd. (2001-2003), and the Principal Scientist and Department Head of Human Language Technology in the Institute for Infocomm Research, Singapore (2003-2016). His research interests include automatic speech recognition, speaker and language recognition, and natural language processing. Dr Li was the Editor-in-Chief of IEEE/ACM Transactions on Audio, Speech and Language Processing (2015-2018), a Member of the Editorial Board of Computer Speech and Language since 2012, an elected Member of IEEE Speech and Language Processing Technical Committee (2013-2015), the President of the International Speech Communication Association (2015-2017), the President of Asia Pacific Signal and Information Processing Association (2015-2016), and the President of Asian Federation of Natural Language Processing (2017-2018). He was the General Chair of ACL 2012, INTERSPEECH 2014, ASRU 2019 and ICASSP 2022. Dr Li is a Fellow of the ISCA, and a Fellow of the Academy of Engineering Singapore. He was the recipient of the National Infocomm Award 2002, and the President’s Technology Award 2013 in Singapore. He was named one of the two Nokia Visiting Professors in 2009 by the Nokia Foundation, and U Bremen Excellence Chair Professor in 2019.
% \end{IEEEbiographynophoto}
% \end{IEEEbiography}


% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% Author contribution
% Qiquan Zhang: Idea, Writing - review & editing, Methodology, Software, Experiments, Data curation.
% Xinyuan Qian: Software, Experiments, Writing - review & editing.
% Zhaoheng Ni: Writing - review & editing.
% Aaron Nicolson: Writing - review & editing.
% Eliathamby Ambikairajah: Writing - review & editing, Supervision.
% Haizhou Li: Writing - review & editing, Supervision.
% All authors discussed the results and commented on the manuscript.

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


