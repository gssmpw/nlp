@misc{achintalwar2024alignment,
      title={Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations}, 
      author={Swapnaja Achintalwar and Ioana Baldini and Djallel Bouneffouf and Joan Byamugisha and Maria Chang and Pierre Dognin and Eitan Farchi and Ndivhuwo Makondo and Aleksandra Mojsilovic and Manish Nagireddy and Karthikeyan Natesan Ramamurthy and Inkit Padhi and Orna Raz and Jesus Rios and Prasanna Sattigeri and Moninder Singh and Siphiwe Thwala and Rosario A. Uceda-Sosa and Kush R. Varshney},
      year={2024},
      eprint={2403.09704},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{aggarwal2023lets,
      title={Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs}, 
      author={Pranjal Aggarwal and Aman Madaan and Yiming Yang and Mausam},
      year={2023},
      eprint={2305.11860},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bai2022constitutional,
      title={Constitutional AI: Harmlessness from AI Feedback}, 
      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
      year={2022},
      eprint={2212.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bhardwaj2024language,
      title={Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic}, 
      author={Rishabh Bhardwaj and Do Duc Anh and Soujanya Poria},
      year={2024},
      eprint={2402.11746},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{elazar_measuring_2021,
	title = {Measuring and {Improving} {Consistency} in {Pretrained} {Language} {Models}},
	doi = {10.1162/tacl_a_00410},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and others},
	year = {2021},
	file = {Full Text:/Users/domenicrosati/Zotero/storage/GFGV9RY3/Elazar et al. - 2021 - Measuring and Improving Consistency in Pretrained .pdf:application/pdf},
}

@inproceedings{fierro_factual_2022,
	title = {Factual {Consistency} of {Multilingual} {Pretrained} {Language} {Models}},
	doi = {10.48550/arXiv.2203.11552},
	booktitle = {{FINDINGS}},
	author = {Fierro, Constanza and Søgaard, Anders},
	year = {2022},
}

@misc{ge2023mart,
      title={MART: Improving LLM Safety with Multi-round Automatic Red-Teaming}, 
      author={Suyu Ge and Chunting Zhou and Rui Hou and Madian Khabsa and Yi-Chia Wang and Qifan Wang and Jiawei Han and Yuning Mao},
      year={2023},
      eprint={2311.07689},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gou2021knowledge,
title={Knowledge distillation: A survey},
author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
journal={International Journal of Computer Vision},
volume={129},
pages={1789--1819},
year={2021},
publisher={Springer}
}

@article{hinton2015distilling,
title={Distilling the knowledge in a neural network},
author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
journal={arXiv preprint arXiv:1503.02531},
year={2015}
}

@article{jang_accurate,
  author    = {Myeongjun Jang and
               Deuk Sin Kwon and
               Thomas Lukasiewicz},
  title     = {Accurate, yet inconsistent? Consistency Analysis on Language Understanding
               Models},
  journal   = {CoRR},
  volume    = {abs/2108.06665},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.06665},
  eprinttype = {arXiv},
  eprint    = {2108.06665},
  timestamp = {Wed, 18 Aug 2021 19:45:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-06665.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kassner_beliefbank_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{BeliefBank}: {Adding} {Memory} to a {Pre}-{Trained} {Language} {Model} for a {Systematic} {Notion} of {Belief}},
	shorttitle = {{BeliefBank}},
	url = {https://aclanthology.org/2021.emnlp-main.697},
	doi = {10.18653/v1/2021.emnlp-main.697},
	urldate = {2022-09-12},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kassner, Nora and Tafjord, Oyvind and Schütze, Hinrich and Clark, Peter},
	month = nov,
	year = {2021},
	pages = {8849--8861},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/XR3KSK34/Kassner et al. - 2021 - BeliefBank Adding Memory to a Pre-Trained Languag.pdf:application/pdf},
}

@misc{keleg2023dlama,
      title={DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models}, 
      author={Amr Keleg and Walid Magdy},
      year={2023},
      eprint={2306.05076},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kuhn2023semantic,
      title={Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation}, 
      author={Lorenz Kuhn and Yarin Gal and Sebastian Farquhar},
      year={2023},
      eprint={2302.09664},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{mitchell-etal-2022-enhancing,
    title = "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference",
    author = "Mitchell, Eric  and
      Noh, Joseph  and
      Li, Siyan  and
      Armstrong, Will  and
      Agarwal, Ananth  and
      Liu, Patrick  and
      Finn, Chelsea  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    pages = "1754--1768"
}

@misc{newman_p-adapters_2022,
	title = {P-{Adapters}: {Robustly} {Extracting} {Factual} {Information} from {Language} {Models} with {Diverse} {Prompts}},
	shorttitle = {P-{Adapters}},
	url = {http://arxiv.org/abs/2110.07280},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Newman, Benjamin and Choubey, Prafulla Kumar and Rajani, Nazneen},
	month = apr,
	year = {2022},
	note = {arXiv:2110.07280 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/RMDRRQ5M/Newman et al. - 2022 - P-Adapters Robustly Extracting Factual Informatio.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/NL44RZ3I/2110.html:text/html},
}

@inproceedings{park2019relational,
title={Relational Knowledge Distillation},
author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
pages={3967--3976},
year={2019}
}

@inproceedings{petroni,
  author    = {Fabio Petroni and
               Tim Rockt{\"{a}}schel and
               Sebastian Riedel and
               others},
  editor    = {Kentaro Inui and
               Jing Jiang and
               Vincent Ng and
               Xiaojun Wan},
  title     = {Language Models as Knowledge Bases?},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural
               Language Processing and the 9th International Joint Conference on
               Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China,
               November 3-7, 2019},
  pages     = {2463--2473},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/D19-1250},
  doi       = {10.18653/v1/D19-1250},
  timestamp = {Thu, 07 Apr 2022 09:14:07 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/PetroniRRLBWM19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rabinovich-etal-2023-predicting,
    title = "Predicting Question-Answering Performance of Large Language Models through Semantic Consistency",
    author = "Rabinovich, Ella  and
      Ackerman, Samuel  and
      Raz, Orna  and
      Farchi, Eitan  and
      Anaby Tavor, Ateret",
    editor = "Gehrmann, Sebastian  and
      Wang, Alex  and
      Sedoc, Jo{\~a}o  and
      Clark, Elizabeth  and
      Dhole, Kaustubh  and
      Chandu, Khyathi Raghavi  and
      Santus, Enrico  and
      Sedghamiz, Hooman",
    booktitle = "Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.gem-1.12",
    pages = "138--154"
}

@misc{raj2023measuring,
      title={Measuring Reliability of Large Language Models through Semantic Consistency}, 
      author={Harsh Raj and Domenic Rosati and Subhabrata Majumdar},
      year={2022},
      eprint={2211.05853},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sahu2022unpacking,
      title={Unpacking Large Language Models with Conceptual Consistency}, 
      author={Pritish Sahu and Michael Cogswell and Yunye Gong and Ajay Divakaran},
      year={2022},
      eprint={2209.15093},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{samvelyan2024rainbow,
      title={Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts}, 
      author={Mikayel Samvelyan and Sharath Chandra Raparthy and Andrei Lupu and Eric Hambro and Aram H. Markosyan and Manish Bhatt and Yuning Mao and Minqi Jiang and Jack Parker-Holder and Jakob Foerster and Tim Rocktäschel and Roberta Raileanu},
      year={2024},
      eprint={2402.16822},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tam2022evaluating,
      title={Evaluating the Factual Consistency of Large Language Models Through Summarization}, 
      author={Derek Tam and Anisha Mascarenhas and Shiyue Zhang and Sarah Kwan and Mohit Bansal and Colin Raffel},
      year={2022},
      eprint={2211.08412},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wang_self-consistency_2022,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	doi = {10.48550/arXiv.2203.11171},
	journal = {ArXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, D. and Le, Quoc and Chi, Ed and Zhou, Denny},
	year = {2022},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/VY8PZPL2/Wang et al. - 2022 - Self-Consistency Improves Chain of Thought Reasoni.pdf:application/pdf},
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2024reft,
      title={ReFT: Representation Finetuning for Language Models}, 
      author={Zhengxuan Wu and Aryaman Arora and Zheng Wang and Atticus Geiger and Dan Jurafsky and Christopher D. Manning and Christopher Potts},
      year={2024},
      eprint={2404.03592},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhou,
  author = {Zhou, Chunting and He, Junxian and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  title = {Prompt Consistency for Zero-Shot Task Generalization},
  publisher = {arXiv},
  year = {2022}
}

