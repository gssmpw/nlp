[
  {
    "index": 0,
    "papers": [
      {
        "key": "petroni",
        "author": "Fabio Petroni and\nTim Rockt{\\\"{a}}schel and\nSebastian Riedel and\nothers",
        "title": "Language Models as Knowledge Bases?"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "elazar_measuring_2021",
        "author": "Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and others",
        "title": "Measuring and {Improving} {Consistency} in {Pretrained} {Language} {Models}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "fierro_factual_2022",
        "author": "Fierro, Constanza and S\u00f8gaard, Anders",
        "title": "Factual {Consistency} of {Multilingual} {Pretrained} {Language} {Models}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "keleg2023dlama",
        "author": "Amr Keleg and Walid Magdy",
        "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "jang_accurate",
        "author": "Myeongjun Jang and\nDeuk Sin Kwon and\nThomas Lukasiewicz",
        "title": "Accurate, yet inconsistent? Consistency Analysis on Language Understanding\nModels"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhou",
        "author": "Zhou, Chunting and He, Junxian and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham",
        "title": "Prompt Consistency for Zero-Shot Task Generalization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "newman_p-adapters_2022",
        "author": "Newman, Benjamin and Choubey, Prafulla Kumar and Rajani, Nazneen",
        "title": "P-{Adapters}: {Robustly} {Extracting} {Factual} {Information} from {Language} {Models} with {Diverse} {Prompts}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "tam2022evaluating",
        "author": "Derek Tam and Anisha Mascarenhas and Shiyue Zhang and Sarah Kwan and Mohit Bansal and Colin Raffel",
        "title": "Evaluating the Factual Consistency of Large Language Models Through Summarization"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "elazar_measuring_2021",
        "author": "Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and others",
        "title": "Measuring and {Improving} {Consistency} in {Pretrained} {Language} {Models}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "raj2023measuring",
        "author": "Harsh Raj and Domenic Rosati and Subhabrata Majumdar",
        "title": "Measuring Reliability of Large Language Models through Semantic Consistency"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "rabinovich-etal-2023-predicting",
        "author": "Rabinovich, Ella  and\nAckerman, Samuel  and\nRaz, Orna  and\nFarchi, Eitan  and\nAnaby Tavor, Ateret",
        "title": "Predicting Question-Answering Performance of Large Language Models through Semantic Consistency"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "sahu2022unpacking",
        "author": "Pritish Sahu and Michael Cogswell and Yunye Gong and Ajay Divakaran",
        "title": "Unpacking Large Language Models with Conceptual Consistency"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "kuhn2023semantic",
        "author": "Lorenz Kuhn and Yarin Gal and Sebastian Farquhar",
        "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wei2023chainofthought",
        "author": "Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "kassner_beliefbank_2021",
        "author": "Kassner, Nora and Tafjord, Oyvind and Sch\u00fctze, Hinrich and Clark, Peter",
        "title": "{BeliefBank}: {Adding} {Memory} to a {Pre}-{Trained} {Language} {Model} for a {Systematic} {Notion} of {Belief}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "mitchell-etal-2022-enhancing",
        "author": "Mitchell, Eric  and\nNoh, Joseph  and\nLi, Siyan  and\nArmstrong, Will  and\nAgarwal, Ananth  and\nLiu, Patrick  and\nFinn, Chelsea  and\nManning, Christopher",
        "title": "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wang_self-consistency_2022",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, D. and Le, Quoc and Chi, Ed and Zhou, Denny",
        "title": "Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}"
      },
      {
        "key": "aggarwal2023lets",
        "author": "Pranjal Aggarwal and Aman Madaan and Yiming Yang and Mausam",
        "title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "wu2024reft",
        "author": "Zhengxuan Wu and Aryaman Arora and Zheng Wang and Atticus Geiger and Dan Jurafsky and Christopher D. Manning and Christopher Potts",
        "title": "ReFT: Representation Finetuning for Language Models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "bhardwaj2024language",
        "author": "Rishabh Bhardwaj and Do Duc Anh and Soujanya Poria",
        "title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "ge2023mart",
        "author": "Suyu Ge and Chunting Zhou and Rui Hou and Madian Khabsa and Yi-Chia Wang and Qifan Wang and Jiawei Han and Yuning Mao",
        "title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "samvelyan2024rainbow",
        "author": "Mikayel Samvelyan and Sharath Chandra Raparthy and Andrei Lupu and Eric Hambro and Aram H. Markosyan and Manish Bhatt and Yuning Mao and Minqi Jiang and Jack Parker-Holder and Jakob Foerster and Tim Rockt\u00e4schel and Roberta Raileanu",
        "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "bai2022constitutional",
        "author": "Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "achintalwar2024alignment",
        "author": "Swapnaja Achintalwar and Ioana Baldini and Djallel Bouneffouf and Joan Byamugisha and Maria Chang and Pierre Dognin and Eitan Farchi and Ndivhuwo Makondo and Aleksandra Mojsilovic and Manish Nagireddy and Karthikeyan Natesan Ramamurthy and Inkit Padhi and Orna Raz and Jesus Rios and Prasanna Sattigeri and Moninder Singh and Siphiwe Thwala and Rosario A. Uceda-Sosa and Kush R. Varshney",
        "title": "Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "hinton2015distilling",
        "author": "Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff",
        "title": "Distilling the knowledge in a neural network"
      },
      {
        "key": "gou2021knowledge",
        "author": "Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng",
        "title": "Knowledge distillation: A survey"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "park2019relational",
        "author": "Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu",
        "title": "Relational Knowledge Distillation"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "raj2023measuring",
        "author": "Harsh Raj and Domenic Rosati and Subhabrata Majumdar",
        "title": "Measuring Reliability of Large Language Models through Semantic Consistency"
      }
    ]
  }
]