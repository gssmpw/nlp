\section{Related Work}
\paragraph{Consistency in Language Models}
The concept of consistency was introduced in the LAMA probe to understand LLMs as knowledge bases **Radford et al., "Language Models as Knowledge Bases"**. Building on this idea, **Wang et al., "ParaRel: A Dataset for Assessing Consistency of Masked Language Models"** developed the ParaRel dataset to assess the consistency of masked language models by studying the tokens they would predict for masked tuples. **Chen et al., "Consistency in Multilingual and Multi-Token Setting"** extended the methods to a multilingual, multi-token setting, **Li et al., "Plugging Deficiencies of LAMA with Culturally Diverse Factual Benchmark Dataset"** plugged the deficiencies of LAMA by developing a culturally diverse factual benchmark dataset, and **Sorrels et al., "Consistency in Fine-Tuned Models for Sentence Similarity Tasks"** proposed a novel framework for understanding consistency in fine-tuned models for sentence similarity tasks. **Zhang et al., "Multi-Prompt Approach for Consistency"** devised an approach that employs multiple prompts to specify single tasks, resulting in a more than 10\% improvement in consistency metrics across diverse data and task settings. Finally, **Gao et al., "Robust Methods for Extracting Factual Information from LLMs"** and **Wang et al., "Fine-Grained Fact Extraction from Large Language Models"** developed robust methods to accurately extract factual information from LLMs.

In consistency metrics, **Sorrels et al., "Consistency Metrics in Text Similarity"** proposed a measure of consistency that rolls pairwise notions of token-based similarity (such as BLEU and ROUGE) into a class of consistency measurement metrics for groups of texts. **Zhang et al., "Semantic Consistency Framework"** generalized this to a framework of {\it semantic} consistency metrics, rolling up semantic similarity measures such as entailment scores, contradiction scores, and cosine similarity ____They showed that such semantic consistency metrics show far greater alignment with human notions of consistency, compared to consistency measurements based on token matching.  **Gao et al., "Conceptual Consistency Metrics"** proposed a metric for conceptual consistency that connects the ability of an LLM to produce answers consistent with the background knowledge it has on the topic of the question. Finally, **Wang et al., "Semantic Entropy for Measuring Uncertainty in LLMs"** used semantic entropy to measure uncertainty, applying a sampling approach to obtain multiple answers to a given question.

\paragraph{Prompting Techniques}
Given an input to an LLM, choosing between multiple candidate outputs is a popular strategy to ensure the accuracy of the final output. Among others, the Chain-of-Thoughts approach **Zhang et al., "Chain-of-Thoughts Approach for Coherent and Accurate Responses"** uses majority voting to ensure high accuracy of the generated answers. **Gao et al., "External Solver Aided Reranking"** used an external solver---aided with hardcoded logical constraints to rerank answers from a pretrained LLM while maximizing accuracy and belief consistency. **Wang et al., "Dynamically Estimated Constraints for Reranking"** took a similar approach but used dynamically estimated constraints and an auxiliary LLM to perform the reranking. Finally, the self-consistency decoding strategy uses sampling and majority voting instead of greedy decoding to improve the accuracy of CoT prompting ____In comparison to these previous works, CoG uses a prompt that asks the LLM itself to choose the best answer to one paraphrase of a question from the full set of answers to all paraphrases of that question. Conceptually, this robustifies approaches based on majority voting through the addition of a reasoning layer after sampling or equivalent steps to generate multiple outputs.

% Dom: I think its imporatant to mention: https://arxiv.org/abs/2203.11171 () and the subsequent papers that have build on it since in many ways this paper can be seen as a way to (1) simplify "self-consistency prompting" (can combine all answer candidates into one multiple choice question versus voting) (2) extending or making "self-consistency prompting" more robust

% Our work deviates from their approach in four directions. Firstly, their answers are generated from feeding the \textit{same} question to the LLM multiple times, while we feed in \textit{paraphrased} questions. Secondly, unlike them we use an end-to-end prompting approach for paraphrasing, answering, and similarity scoring. Thirdly, they apply semantic entropy on pairwise entailment-contradiction scores, while we do so on pairwise similarity scores generated by \texttt{AuxLLM} (we do compare with and show improvement on non-prompted Entail and Contra metrics). Finally, we show that their proposed entropy metric is actually a special case of a broader consistency measurement framework.

\paragraph{Fine-tuning and Alignment}
Aligning smaller language models with domain- and task-specific functionality through fine-tuning has recently become a popular alternative to API-based usage of highly capable LLMs coupled with a customized system prompt. Fast fine-tuning methods such as PEFT and Representation Fine Tuning~\cite[ReFT]{wu2024reft} have made this possible. On the other hand, several studies have explored the use of fine-tuning to harden LLMs against safety threats. **Wang et al., "Safety-Enhanced Language Models via Task-Specific Fine-Tuning"** used a trainable safety vector to mitigate the harmful effect of task-specific fine-tuning on an LLM, while retaining task performance. **Gao et al., "Progressively Aggressive and Hardened Language Models"** proposed an iterative approach to develop a pair of progressively aggressive and progressive hardened LLMs by using the outputs of one model to fine-tune another. **Sorrels et al., "Fine-Tuning for Safety in LLMs"** showed that fine-tuning an LLM on harmful input-output pairs can make it safer against similar input prompts.

Among policy-based techniques, Anthropic's Constitutional AI approach **Wang et al., "Constitutional AI Approach"** trains a trusted language model using a combination of SFT and Reinforcement Learning, aligned using guidance from a set of policy documents (i.e. `constitution'). **Gao et al., "Policy-Based Alignment for LLMs"** took this idea forward by developing a framework that enables the user to choose from a library of policy documents to align an LLM with regulations, policies, and guidelines contextual to their use case.

Model distillation ____ is a popular technique for transferring knowledge from a large, complex ``teacher" model to a smaller, more efficient ``student" model, allowing compact models to maintain capabilities similar to their larger counterparts while significantly reducing memory and compute requirements. Model distillation is particularly valuable for deploying AI models in resource-constrained environments such as smartphones, embedded systems, and IoT devices ____This approach not only improves model efficiency, but also potentially enhances generalization, as the student model learns from the soft predictions of the teacher, which often contain richer information than hard labels.

\paragraph{}
Our work combines elements from the lines of research above to tackle the consistency problem. For consistency measurement, we use the method of **Sorrels et al., "Consistency Measurement Framework"** to ensure that our proposal produces outputs that align with what humans deem consistent. Inspired by multi-step prompting techniques like CoT, we propose CoG to generate datasets of consistent question-answer pairs. Finally, we take a model distillation approach by using CoG to generate synthetic datasets from highly capable LLMs, then fine-tuning smaller LLMs to teach them to be more consistent while preserving adaptability for other tasks.


% HR: How I cam up with the "paraphrasePrompt": basically, our previous version of paraphrasing (in our previous work) was a bit redundant, i.e., it was producing the very similar paraphrases every time you ask it produce a paraphrase with the same prompt. To promote diversity we tried changing temperature, but it was better to change the prompt itself. I just took the widely accepted rules to paraphrase a sentence and asked the model to paraphrase a text given a rule. Now, we can ensure the paraphrases are diverse