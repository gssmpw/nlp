\section{Related Work}
\paragraph{Consistency in Language Models}
The concept of consistency was introduced in the LAMA probe to understand LLMs as knowledge bases ____. Building on this idea, ____ developed the ParaRel dataset to assess the consistency of masked language models by studying the tokens they would predict for masked tuples. ____ extended the methods to a multilingual, multi-token setting, ____ plugged the deficiencies of LAMA by developing a culturally diverse factual benchmark dataset, and ____ proposed a novel framework for understanding consistency in fine-tuned models for sentence similarity tasks. ____ devised an approach that employs multiple prompts to specify single tasks, resulting in a more than 10\% improvement in consistency metrics across diverse data and task settings. Finally, ____ and ____ developed robust methods to accurately extract factual information from LLMs.

In consistency metrics, ____ proposed a measure of consistency that rolls pairwise notions of token-based similarity (such as BLEU and ROUGE) into a class of consistency measurement metrics for groups of texts. ____ generalized this to a framework of {\it semantic} consistency metrics, rolling up semantic similarity measures such as entailment scores, contradiction scores, and cosine similarity ____. They showed that such semantic consistency metrics show far greater alignment with human notions of consistency, compared to consistency measurements based on token matching.  ____ proposed a metric for conceptual consistency that connects the ability of an LLM to produce answers consistent with the background knowledge it has on the topic of the question. Finally, ____ used semantic entropy to measure uncertainty, applying a sampling approach to obtain multiple answers to a given question. 

\paragraph{Prompting Techniques}
Given an input to an LLM, choosing between multiple candidate outputs is a popular strategy to ensure the accuracy of the final output. Among others, the Chain-of-Thoughts approach____ uses majority voting to ensure high accuracy of the generated answers. ____ used an external solver---aided with hardcoded logical constraints to rerank answers from a pretrained LLM while maximizing accuracy and belief consistency. ____ took a similar approach but used dynamically estimated constraints and an auxiliary LLM to perform the reranking. Finally, the self-consistency decoding strategy uses sampling and majority voting instead of greedy decoding to improve the accuracy of CoT prompting ____. In comparison to these previous works, CoG uses a prompt that asks the LLM itself to choose the best answer to one paraphrase of a question from the full set of answers to all paraphrases of that question. Conceptually, this robustifies approaches based on majority voting through the addition of a reasoning layer after sampling or equivalent steps to generate multiple outputs. 
% Dom: I think its imporatant to mention: https://arxiv.org/abs/2203.11171 () and the subsequent papers that have build on it since in many ways this paper can be seen as a way to (1) simplify "self-consistency prompting" (can combine all answer candidates into one multiple choice question versus voting) (2) extending or making "self-consistency prompting" more robust

% Our work deviates from their approach in four directions. Firstly, their answers are generated from feeding the \textit{same} question to the LLM multiple times, while we feed in \textit{paraphrased} questions. Secondly, unlike them we use an end-to-end prompting approach for paraphrasing, answering, and similarity scoring. Thirdly, they apply semantic entropy on pairwise entailment-contradiction scores, while we do so on pairwise similarity scores generated by \texttt{AuxLLM} (we do compare with and show improvement on non-prompted Entail and Contra metrics). Finally, we show that their proposed entropy metric is actually a special case of a broader consistency measurement framework.

\paragraph{Fine-tuning and Alignment}
Aligning smaller language models with domain- and task-specific functionality through fine-tuning has recently become a popular alternative to API-based usage of highly capable LLMs coupled with a customized system prompt. Fast fine-tuning methods such as PEFT and Representation Fine Tuning~\cite[ReFT]{wu2024reft} have made this possible. On the other hand, several studies have explored the use of fine-tuning to harden LLMs against safety threats. ____ used a trainable safety vector to mitigate the harmful effect of task-specific fine-tuning on an LLM, while retaining task performance. ____ proposed an iterative approach to develop a pair of progressively aggressive and progressive hardened LLMs by using the outputs of one model to fine-tune another. ____ showed that fine-tuning an LLM on harmful input-output pairs can make it safer against similar input prompts.

Among policy-based techniques, Anthropic's Constitutional AI approach ____ trains a trusted language model using a combination of SFT and Reinforcement Learning, aligned using guidance from a set of policy documents (i.e. `constitution'). ____ took this idea forward by developing a framework that enables the user to choose from a library of policy documents to align an LLM with regulations, policies, and guidelines contextual to their use case.

Model distillation ____ is a popular technique for transferring knowledge from a large, complex ``teacher" model to a smaller, more efficient ``student" model, allowing compact models to maintain capabilities similar to their larger counterparts while significantly reducing memory and compute requirements. Model distillation is particularly valuable for deploying AI models in resource-constrained environments such as smartphones, embedded systems, and IoT devices ____. This approach not only improves model efficiency, but also potentially enhances generalization, as the student model learns from the soft predictions of the teacher, which often contain richer information than hard labels.

\paragraph{}
Our work combines elements from the lines of research above to tackle the consistency problem. For consistency measurement, we use the method of ____ to ensure that our proposal produces outputs that align with what humans deem consistent. Inspired by multi-step prompting techniques like CoT, we propose CoG to generate datasets of consistent question-answer pairs. Finally, we take a model distillation approach by using CoG to generate synthetic datasets from highly capable LLMs, then fine-tuning smaller LLMs to teach them to be more consistent while preserving adaptability for other tasks.


% HR: How I cam up with the "paraphrasePrompt": basically, our previous version of paraphrasing (in our previous work) was a bit redundant, i.e., it was producing the very similar paraphrases every time you ask it produce a paraphrase with the same prompt. To promote diversity we tried changing temperature, but it was better to change the prompt itself. I just took the widely accepted rules to paraphrase a sentence and asked the model to paraphrase a text given a rule. Now, we can ensure the paraphrases are diverse