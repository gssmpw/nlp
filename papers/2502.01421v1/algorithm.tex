\section{The Algorithm}







In this section, we explain our fully dynamic algorithm for maintaining a \SpectralHypersparsifier{} of a hypergraph \( \hypergraph{H} = (V, E, \vect{w}) \).
Following \cite{Bansal:2019aa}, we describe our algorithm under the assumption that every hyperedge is of size \( (r/2, r] \).
At the end of the section, we will generalize the algorithm to hypergraphs of rank \( r \) using the lemma below.





\begin{lemma} \label{lem:r/2}
Let \( \varepsilon > 0 \) and suppose that there is an algorithm which, given any \( m \)-hyperedge \( n \)-vertex hypergraph \hypergraph{H'} with hyperedges of size \( (r/2, r ] \), computes a \SpectralHypersparsifier{} of \hypergraph{H'} with size  
\( 
 r^{\ConstantR} \varepsilon^{ - \ConstantVar } \cdot S(m, n) \)
 in \( T(m, n, r, \varepsilon ^{-1}) \) time, where \( S \) and \( T \) are  monotone non-decreasing functions.
Then, for any \( m \)-hyperedge \( n \)-vertex hypergraph \hypergraph{H} of rank \( r \), there is an algorithm for computing a \SpectralHypersparsifier{} of \hypergraph{H} with size \atmost{r^{\ConstantR} \varepsilon^{ - \ConstantVar } \cdot S(m, n)} in \atmost{T(m, n, r, \varepsilon ^{-1}) \log r} time.
\end{lemma}

The exponents of \( r \) and \( \varepsilon \) in \Cref{lem:r/2} are adjusted for our later use. 
Before proving the lemma, we first prove the decomposability of spectral hypersparsifiers below.
This will be used in both the proof of \Cref{lem:r/2} and in the proofs that follow. 


\begin{lemma}[Decomposability] \label{lem:decomposability}
Let \( \varepsilon > 0 \), \( \hypergraph{H} = (V, E, \vect{w}) \) be a hypergraph, and \( E_1, E_2, \dots, E_k \) be a partition of \( E \).
For \( 1 \leq i \leq k \), define \( \hypergraph{H_i} = (V, E_i, \vect{w}_i ) \) as the sub-hypergraph of \hypergraph{H} induced by \( E_i \).
Let \hypergraph{\sparsifier{H_i}} be a \SpectralHypersparsifier{} of \hypergraph{H_i}.
Then, the hypergraph \( \hypergraph{\sparsifier{H}} = \cup_{i = 1} ^k \hypergraph{\sparsifier{H_i}} \) is a \SpectralHypersparsifier{} of \hypergraph{H}.
\end{lemma}
\begin{proof}
By \Cref{eq:spectral_hypergraph}, for every integer \( 1 \leq i \leq k \) and every vector \( \vect{x} \in \mathbb R ^n \), we have
\begin{equation*}
(1 - \varepsilon) Q_{\hypergraph{\sparsifier{H_i}}}(\vect{x}) \leq Q_{\hypergraph{H_i}}(\vect{x}) \leq (1 + \varepsilon) Q_{\hypergraph{\sparsifier{H_i}}}(\vect{x}).
\end{equation*}
Summing over all \( i \), we have
\begin{equation*}
(1 - \varepsilon) \sum_{i = 1} ^k Q_{\hypergraph{\sparsifier{H_i}}}(\vect{x}) \leq \sum_{i = 1} ^k Q_{\hypergraph{H_i}}(\vect{x})
\leq (1 + \varepsilon) \sum_{i = 1} ^k Q_{\hypergraph{\sparsifier{H_i}}}(\vect{x}).
\end{equation*}
Since \( E_1, E_2, \dots, E_k \) is a partition of \( E \), \( \sum_{i = 1} ^k Q_{\hypergraph{\sparsifier{H_i}}}(\vect{x}) = Q_{\hypergraph{\sparsifier{H}}}(\vect{x}) \) and \( \sum_{i = 1} ^k Q_{\hypergraph{H_i}}(\vect{x}) = Q_{\hypergraph{H}}(\vect{x}) \), thereby completing the proof.
\end{proof}





\begin{proof}[Proof of \Cref{lem:r/2}]
Suppose that 
For \( 1 \leq i \leq \log r \), let \hypergraph{H_i} be the sub-hypergraph containing the hyperedges of \hypergraph{H} of size \( (2^{i - 1} , 2^i] \).
i.e., \( r_i =  2^i\).
Let 
\hypergraph{\sparsifier{H_i}} be a \( (1 \pm \varepsilon ) \) spectral hypersparsifier of \hypergraph{H_i} of size \atmost{r _i ^{\ConstantR} \varepsilon ^{ - \ConstantVar } \cdot S(m, n)}, computed in time \atmost{T(m, n, \varepsilon ^{-1} )}.
We show that the hypergraph \( \hypergraph{\sparsifier{H}} = \cup _{i = 1} ^{\log r} \hypergraph{\sparsifier{H_i}} \) has the desired properties.

Since every \hypergraph{\tilde H_i} is a \SpectralHypersparsifier{} of \hypergraph{H_i}, it follows from \Cref{lem:decomposability} that \hypergraph{\sparsifier{H}} is a \SpectralHypersparsifier{} of \hypergraph{H}.
To bound the size of \hypergraph{\sparsifier{H}}, we have
\begin{equation*}
\atmost{  \sum_{i = 1} ^ {\log r} r _i ^{\ConstantR} \varepsilon ^{ - \ConstantVar } \cdot S(m, n) } 
= 
\atmost{  \sum_{i = 1} ^ {\log r} 2 ^{\ConstantR i} \varepsilon ^{ - \ConstantVar } 
 \cdot S(m, n)  }
= 
\atmost{ r^{\ConstantR} \varepsilon^{ - \ConstantVar } \cdot S(m, n) }.
\end{equation*}
The bound on time follows immediately from the construction of \hypergraph{\sparsifier{H}}.
\end{proof}



The rest of this section is divided into two parts.
In \Cref{subsec:static}, we explain our algorithm in the static setting and prove its correctness and guarantees.
In \Cref{subsec:dynamic}, we explain how to transform the static algorithm into a dynamic one.

\subsection{Static \( (1 \pm \varepsilon) \)-Spectral Hypersparsifier} \label{subsec:static}
Our static algorithm consists of two parts (\Cref{alg:light_spectral_sparsify,alg:spectral_sparsify}).
Our algorithm uses the same approach as \cite{Koutis:2016aa,Abraham:2016aa,Oko:2023aa} by using \( t \)-bundle \( \alpha \)-hyperspanners to establish a bound on the effective resistances.
In \Cref{alg:light_spectral_sparsify}, we compute a ``slightly sparser'' \SpectralHypersparsifier{} of the input hypergraph.
Then, in \Cref{alg:spectral_sparsify}, we use this procedure recursively to ``peel off'' the hypergraph until we obtain the desired sparsity.


\begin{lemma}[Adapted from \cite{Oko:2023aa}] \label{lem:effective_resistance}
Let \( \hypergraph{H} = (V, E, \vect{w}) \) be a hypergraph with hyperedges of size \( (r/2, r] \), and let \hypergraph{B} be a \( t \)-bundle \( \alpha \)-hyperspanner of \hypergraph{H}.
Then, for every hyperedge \( e \) of \( \hypergraph{H} \setminus \hypergraph{B} \) and every pair of vertices \( u,v \in e \),
\begin{equation*}
w_e \cdot R_{G_H}(u, v) \leq \frac{4 \alpha}{rt},
\end{equation*}
where \( R_{G_H}(u, v) \)  is the effective resistance of the pair \( u, v \) in the associated graph \( \associated{\hypergraph{H}} \) of \hypergraph{H}.
\end{lemma}
\begin{proof}[Proof sketch]
Let \( e \) be a hyperedge in \( \hypergraph{H} \setminus \hypergraph{B} \), let \(u, v\) be any pair of vertices from $e$, and let \( \hypergraph{B} = \hypergraph{T_1} \cup \dots \cup \hypergraph{T_t} \).
By the definition of \( \hypergraph{B} \), for each \( 1 \leq i \leq t \), there exists a hyperpath \( \pi _i \) in \hypergraph{T_i} of length at most \( \alpha \cdot 1/ w_e \) that connects \( u \) and \( v \) (recall that the length of an edge \( e \) is \( 1/w_e \)).
Thus, there are at least \( t \) hyperedge-disjoint paths connecting \( u \) and \( v \) in \hypergraph{H}.

In the associated graph \associated{\hypergraph{H}}, each path \( \pi _i \) is represented as a sequence of intersecting cliques.
Since \( |e| \in (r/2, r] \) for every \( e \in E \), there are at least \( r / 2 \) disjoint paths between each pair of vertices within the cliques, and each of these paths contain at most two edges.
Consequently, each path \( \pi _i \) represents at least \( r/2 \) edge-disjoint parallel paths of length at most \( \alpha \cdot 2/ w_e \) in \associated{\hypergraph{H}}.

We conclude that there are at least \( rt / 2 \) edge-disjoint parallel paths in \associated{\hypergraph{H}} of length at most \( \alpha \cdot 1/ w_e \) connecting \( u \) and \( v \).
Using the parallel decomposition rule of resistances to compute the effective resistance between \( u \) and \( v \), we have
\begin{equation*}
R_{G_H}(u, v) \leq \frac{\alpha \cdot 2 / w_e}{rt / 2} = \frac{4 \alpha}{rt} \cdot \frac{1}{w_e},
\end{equation*}
as desired.
\end{proof}

By using the proposition below, combined with the bound on \( R_{G_H}(u, v) \) obtained in \Cref{lem:effective_resistance}, we can determine a suitable value for the number of spanners \( t \) of a bundle as specified in \Cref{alg:light_spectral_sparsify}.



\begin{proposition}[Adapted from \cite{Bansal:2019aa}] \label{prop:constant_probability}

Let \( \varepsilon > 0 \), \( \gamma \geq 1 \) be a constant, and \( \hypergraph{H} = (V, E, \vect{w}) \) be an \( m \)-hyperedge \( n \)-vertex hypergraph with hyperedges of size \( (r/2, r] \).
Let \( \hypergraph{\sparsifier{H}} = (V, \tilde{E}, \tilde{\vect{w}} ) \) be a hypergraph formed by independently sampling each hyperedge \( e \in E \) with probability
\begin{equation*}
\min \left\{ 1, \ConstantProp r^4 \varepsilon ^{-2} \log n \cdot w_e \left( \max_{u, v \in e} R_{G_H}(u, v) \right) \right\} \leq p_e \leq 1
\end{equation*}
and assigning \( \tilde{w}_e = w_e \cdot 1/p_e  \) if \( e \in \tilde{E} \), where \ConstantProp is a constant depending on \( \gamma \), and \( R_{G_H}(u, v) \) is the effective resistance of the pair \( u, v \) in the associated graph \associated{\hypergraph{H}} of \hypergraph{H}.
Then \hypergraph{\sparsifier{H}} is a \SpectralHypersparsifier{} with probability at least \( 1 - \atmost{1/n^{\gamma + 2}} \).
\end{proposition}



\begin{algorithm}

\KwIn{\( \varepsilon > 0 \), and hypergraph \( \hypergraph{H} = (V, E, \vect{w}) \)}
\KwOut{a ``slightly sparser'' \SpectralHypersparsifier{} \( \hypergraph{\sparsifier{H}} = (V, \tilde{E}, \tilde{\vect{w}}) \) and a \( t \)-bundle \( \alpha \)-hyperspanner \hypergraph{B} of \hypergraph{H}}

\( t \gets 16 \alpha \ConstantProp r^3 \varepsilon ^{-2} \log n \)
\tcc*{\ConstantProp is from \Cref{prop:constant_probability}}

Compute a \( t \)-bundle \( \alpha \)-hyperspanner \hypergraph{B} of \hypergraph{H}

\( \hypergraph{\sparsifier{H}} \gets \hypergraph{B} \)

\For{each hyperedge \( e \) of \( \hypergraph{H} \setminus \hypergraph{B} \) 
}{
with probability \( 1/4 \), add \( e \) to \( \tilde{E} \) and set \( \tilde{w}_e \gets 4 w_e \)
}

\Return{\( (\hypergraph{\sparsifier{H}}, \hypergraph{B}) \)}

\caption{\textsc{Slight-Spectral-Sparsify(\( \hypergraph{H}, \varepsilon \))}}
\label{alg:light_spectral_sparsify}
\end{algorithm}


In the next lemma, we bound the size of the spectral hypergraph \hypergraph{\sparsifier{H}} computed by \Cref{alg:light_spectral_sparsify} and prove that the chosen value for \( t \) is sufficient.
Note that \hypergraph{\sparsifier{H}} is not sparse and therefore cannot be directly considered as the desired \SpectralHypersparsifier{} we are looking for.

\begin{lemma} \label{lem:light-spectral-sparsify}
Let \( \varepsilon > 0 \) and let \( \hypergraph{H} = (V, E, \vect{w}) \) be an \( m \)-hyperedge \( n \)-vertex hypergraph with hyperedges of size \( (r/2, r] \).
Then, with probability at least \( 1 - \atmost{1/n^2} \), \Cref{alg:light_spectral_sparsify} returns a \SpectralHypersparsifier{} \( \hypergraph{\sparsifier{H}} = (V, \tilde{E} , \tilde{\vect{w}}) \) with \( |\tilde{E}| \leq |E(\hypergraph{B})| + |E| /2 \).
\end{lemma}
\begin{proof}
We substitute the upper bound on \( R_{G_H}(u, v) \) from \Cref{lem:effective_resistance} into \Cref{prop:constant_probability}.
Consequently, we obtain
\begin{equation*}
\ConstantProp r^4 \varepsilon ^{-2} \log n \cdot w_e \left( \max_{u, v \in e} R_{G_H}(u, v) \right) \leq \frac{2 \alpha \ConstantProp r^3 \varepsilon ^{-2} \log n}{t} = \frac{1}{4}.
\end{equation*}
Thus, it follows from \Cref{prop:constant_probability} that the hypergraph \hypergraph{\sparsifier{H}} is a \SpectralHypersparsifier{}.


To prove the upper bound on \( | \tilde{E} | \), we use a standard Chernoff bound argument.
Without loss of generality, assume \( |E| > \ConstantSize n \) for a constant \( \ConstantSize > 0 \); otherwise, the algorithm simply returns a sparse hypergraph. 
For each hyperedge \( e \in H \setminus B \), 
 define the random variable \( X_e \), where \( X_e = 1 \) with probability \( 1/4 \) and  \( X_e = 0 \) with probability \( 3/4 \).
Let \( X = \sum _{e \in E} X_e \).
Clearly, \( \mu = \mathbb E[X] = |E|/4 \).
By choosing \( \delta = 1 \) in \Cref{eq:Chernoff}, we have
\begin{equation} \label{eq:whp_light_spectral_sparsify}
\mathbb P \left[X \geq \frac{|E|}{2}\right] \leq \frac{1}{e^{ |E| / 12}} = \frac{1}{e^{ \ConstantSize m^\star / 12}} < \frac{1}{e^{ \ConstantSize n / 12}}  \leq \frac{\ConstantSize}{n^2},
\end{equation}
for a sufficiently large constant \ConstantSize.
\end{proof}


We now focus on the second part of the algorithm (\Cref{alg:spectral_sparsify}).
\Cref{lem:spectral_sparsify} establishes its correctness and provides the guarantees we will use later in the dynamic setting.

\begin{algorithm}

\KwIn{\( \varepsilon > 0 \), hypergraph \( \hypergraph{H} = (V, E, \vect{w}) \), reduction parameter \( \rho \), size threshold \( m ^\star \), and constant \( \ConstantSize > 0 \)}
\KwOut{a \SpectralHypersparsifier{} \( \hypergraph{\sparsifier{H}} = (V, \tilde{E}, \tilde{\vect{w}}) \)}


\( i \gets 0 \)

\( k \gets \lceil \log \rho \rceil \)

\( \hypergraph{H_0} \gets \hypergraph{H} \)

\( \hypergraph{B_0} \gets (V, \emptyset, \vect{0}) \)


\While(
){\( i < k \) and \( |E(\hypergraph{H_i})| \geq \ConstantSize m^\star \)}{


\( (\hypergraph{\sparsifier{H_{i + 1}}}, \hypergraph{B_{i + 1}} ) \gets \textsc{Slight-Spectral-Sparsify(\( \hypergraph{H_i}, \varepsilon / (2k) \))} \)

\( \hypergraph{H_{i+1}} \gets \hypergraph{\sparsifier{H_{i+1}}} \setminus \hypergraph{B_{i+1}} \)

\( i \gets i + 1 \)
}

\( \ilast \gets i \)

\( \hypergraph{\sparsifier{H}} \gets \bigcup _{j = 1} ^ \ilast \hypergraph{B_j} \cup \hypergraph{H_\ilast} \)

\Return{(\( \hypergraph{\sparsifier{H}}
\))
}

\caption{\textsc{Spectral-Sparsify(\( \hypergraph{H}, \varepsilon \))}}
\label{alg:spectral_sparsify}
\end{algorithm}


\begin{lemma} \label{lem:spectral_sparsify}
Let \( \varepsilon > 0 \), \( \rho \) be a positive integer, \( \hypergraph{H} = (V, E, \vect{w}) \) be an \( m \)-hyperedge \( n \)-vertex hypergraph with hyperedges of size \( (r/2, r] \), and \( m^\star \geq n \) be an integer.
Then, with probability at least \( 1 - \atmost{ \lceil \log \rho \rceil /n^2} \), \Cref{alg:spectral_sparsify} returns a \SpectralHypersparsifier{} \( \hypergraph{\sparsifier{H}} = (V, \tilde{E}, \tilde{\vect{w}}) \).
Moreover, the number of iterations before the algorithm terminates is at most
\begin{equation*}
\ilast = \min\left\{ \lceil \log \rho \rceil, \lceil \log \left( m / m^\star \right) \rceil \right\},
\end{equation*}
and the size of \hypergraph{\sparsifier{H}} is 
\begin{equation} \label{eq:SizeSparsifier}
\atmost{ \sum _{j = 1} ^ \ilast |\hypergraph{B_j}| +  \ConstantSize m^\star + m / \rho }.
\end{equation}


\end{lemma}
\begin{proof}


Note that after the algorithm terminates, \ilast is the number of iterations, and we always have \( \ilast \leq k \), where \( k = \lceil \log \rho \rceil \).

We first prove that the returned hypergraph \( \hypergraph{\sparsifier{H}} = \bigcup _{j = 1} ^\ilast \hypergraph{B_j} \cup \hypergraph{H_\ilast} \) is a \SpectralHypersparsifier{} of \hypergraph{H} with probability at least \( 1 - \atmost{\ilast/n^2} \).
To do so, we use induction to show that, for every integer \( 1 \leq p \leq \ilast \), the inequality
\begin{equation} \label{eq:upper_bound}
Q_{\hypergraph{H_{\ilast - p }}}( \vect{x} ) 
\leq
\left( 1 + \varepsilon / (2k) \right)^{p} Q_{\hypergraph{I_{p}}}( \vect{x} )
\end{equation}
holds for \( \hypergraph{I_p} =  \bigcup _{j = \ilast - p + 1} ^ \ilast \hypergraph{B_j} \cup \hypergraph{H_\ilast} \) with probability at least \( (1 \pm \varepsilon /(2k) )^{p} \).
We will use this later to prove that
 \( \hypergraph{I_\ilast} =  \bigcup _{j = 1} ^ \ilast \hypergraph{B_j} \cup \hypergraph{H_\ilast} \) is a 
 \SpectralHypersparsifier{} of \( \hypergraph{H_{0}} = \hypergraph{H} \) with probability at least \( 1 - \atmost{p/n^2} \).

For \( p = 1 \), it follows from \Cref{lem:light-spectral-sparsify}.
Suppose that the claim is true for \( p = l \).
i.e., \( Q_{\hypergraph{H_{\ilast - l }}}( \vect{x} ) \leq \left( 1 + \varepsilon / (2k) \right)^{l} Q_{\hypergraph{I_{l}}}( \vect{x} ) \)
 with probability at least \( 1- \atmost{l/n^2} \).
Since \hypergraph{\sparsifier{H_{\ilast-l}}} consists of \hypergraph{B_{\ilast-l}} and \hypergraph{H_{\ilast-l}}, for every vector \( \vect{x} \in \mathbb R ^n \) we have
\begin{align*}
Q_{ \hypergraph{\sparsifier{H_{\ilast-l}}} } (\vect{x}) 
&= Q_{\hypergraph{B_{\ilast- l }}}( \vect{x} ) 
+ Q_{\hypergraph{H_{\ilast- l }}}( \vect{x} )
\\
&\leq 
 Q_{\hypergraph{B_{\ilast- l }}}( \vect{x} ) 
+ 
\left( 1 + \varepsilon / (2k) \right)^{l}
Q_{\hypergraph{I_l}}( \vect{x} )
\\
&\leq
\left( 1 + \varepsilon / (2k) \right)^{l} 
\left( 
 Q_{\hypergraph{B_{\ilast-l} }}( \vect{x} )
 +
 Q_{\hypergraph{I_l }}( \vect{x} ) 
\right)
\\
&= \left( 1 + \varepsilon / (2k) \right)^{l} Q_{\hypergraph{I_{l + 1 }}}( \vect{x} ) ,
\end{align*}
where the last line followed from the fact that \hypergraph{I_{l + 1}} consists of \hypergraph{B_{\ilast- l}} plus the hyperedges in \hypergraph{I_{l}}.
By \Cref{alg:light_spectral_sparsify}, \hypergraph{\sparsifier{H_{\ilast - l}}} is a \( \left( 1 + \varepsilon / (2k) \right) \) spectral hypersparsifier for \hypergraph{H_{\ilast - (l+ 1) }} with probability at least \( 1 - \atmost{1/n^2} \).
Thus, for every vector \( \vect{x} \in \mathbb R ^n \) we have
\begin{equation} \label{eq:spectral_proof_1}
Q_{\hypergraph{H_{\ilast - (l+ 1) }}}( \vect{x} ) 
\leq
\left( 1 + \varepsilon / (2k) \right) Q_{ \hypergraph{\sparsifier{H_{\ilast-l}}} }( \vect{x} ) 
\leq 
\left( 1 + \varepsilon / (2k) \right)^{l+ 1} Q_{\hypergraph{I_{l + 1}}}( \vect{x} ),
\end{equation}
with probability at least \( 1 - \atmost{(l + 1)/n^2} \), proving \Cref{eq:upper_bound} with the desired probabilily.


By symmetry, 
\begin{equation} \label{eq:spectral_proof_2}
\left( 1 - \varepsilon / (2k) \right)^{l+ 1} Q_{\hypergraph{I_{l + 1 }}} ( \vect{x} ) \leq Q_{\hypergraph{H_{\ilast - (l+ 1) }}} ( \vect{x} )
\end{equation}
with probability at least \( 1 - \atmost{(l + 1)/n^2} \).

It follows from \Cref{eq:spectral_proof_1,eq:spectral_proof_2} that \hypergraph{I_{l+1}} is a \( (1 \pm \varepsilon /(2k) )^{l+1} \) spectral hypersparsifier of \hypergraph{H_{\ilast-(l+1)}}
with probability at least \( 1 - \atmost{(l + 1)/n^2} \).
Since \( ( 1 + \varepsilon / (2k) )^{\ilast} \leq ( 1 + \varepsilon / (2k) )^{k} \leq  1 + \varepsilon \) and \( 1 - \varepsilon \leq ( 1 - \varepsilon / (2k) )^{k} \leq ( 1 - \varepsilon / (2k) )^{\ilast} \), it follows that \( \hypergraph{\sparsifier{H}} = \hypergraph{I_\ilast} \) is a \SpectralHypersparsifier{} of \hypergraph{H} with probability at least \( 1 - \atmost{\ilast / n^2} \).

\underline{Number of iterations:}
note that the loop halts either when \( i = k \) or \( |E(\hypergraph{H_i})| \geq \ConstantSize m^*  \).
The first condition provides the first constraint on the number of iterations.
For the latter, by \Cref{lem:light-spectral-sparsify}, \( |E(\hypergraph{H_i})| \leq |E(\hypergraph{H_{i - 1}})| /2 \) with probability at least \( 1 - \atmost{1/n^2} \) for each \( 1 \leq i \leq k \).
Thus, at each iteration, at least half of the edges are removed until fewer than \( c_2 m ^\star \) edges remain. This means that the number of iterations is at most \( \lceil \log \left( m / m^\star \right) \rceil \) with probability at least \( 1 - \atmost{k/n^2} \), as \( k \) is an upper bound on the number of iterations.

\underline{Size of \hypergraph{\sparsifier{H}}:}
we only need to discuss the size of \hypergraph{H_i}, which is either bounded by \( \ConstantSize m^\star \), or is the hypergraph \hypergraph{H_k} returned by \Cref{alg:light_spectral_sparsify} in the \( k \)th iteration.
As discussed in the previous paragraph, with probability at least \( 1 - \atmost{k/n^2} \), the size of \hypergraph{H_k} is bounded by 
\begin{equation*}
\frac{|E(\hypergraph{H_0})|}{2^k} = \frac{m}{2^{\lceil \log \rho \rceil}} = \atmost{m / \rho},
\end{equation*}
thus establishing the bound of \atmost{\ConstantSize m^\star + m/\rho}.
\end{proof}






\subsection{Dynamic \( (1 \pm \varepsilon) \)-Spectral Hypersparsifier} \label{subsec:dynamic}




We now explain the dynamic algorithm for maintaining a \SpectralHypersparsifier{}.
Similar to \cite{Abraham:2016aa}, we first design a decremental algorithm, then in \Cref{subsec:fully_dynamic}, we generalize it to a fully dynamic algorithm using \Cref{lem:turn_to_fully_dynamic}.

The decremental algorithm is the decremental version of \Cref{alg:light_spectral_sparsify,alg:spectral_sparsify} explained in \Cref{subsec:decremetal_light_spectral_sparsify,subsec:decremental_spectral_sparsify}.
Before that, we first design a decremental algorithm for maintaining \( t \)-bundle \( \alpha \)-hyperspanners in \Cref{subsec:t-bundle}, where \( \alpha = \atmost{\log n} \).




\begin{lemma} \label{lem:turn_to_fully_dynamic}
Let \( \varepsilon > 0 \) and let
\( \mathcal A \) be a decremental algorithm that, for any \( n \)-vertex hypergraph \( \hypergraph{H'} = (V, E', \vect{w}') \) with \( m' \) initial hyperedges  and the weight ratio \( W = \max _{i,j} w_i / w_j \), maintains a \SpectralHypersparsifier{} \( \hypergraph{\tilde H'} \) of \( \hypergraph{H'} \) with probability at least \( 1 - \atmost{1 / n^{2}} \) with size \( S(m', n, \varepsilon ^{-1}, W) \) in amortized update time \( T(m', n, \varepsilon ^{-1}, W) \).



Then, there is a fully dynamic algorithm that, with probability at least \( 1 - \atmost{ \lceil \log m \rceil /n^{2}} \), maintains a \SpectralHypersparsifier{} of \( \hypergraph{H} = (V, E, \vect{w}) \) of size \atmost{S(m, n, \varepsilon ^{-1}, W) \log m} in amortized update time \atmost{T(m, n, \varepsilon ^{-1}, W) \log m}, where \( m \) is an upper bound on the number of the hyperedges of \( H \) at any point.
\end{lemma}
\begin{proof}

Let \( k = \lceil \log{m} \rceil \).
The fully dynamic algorithm utilizes \( \hypergraph{\mathcal A_1}, \hypergraph{\mathcal A_2}, \dots, \hypergraph{\mathcal A_k} \) to decrementally maintain  \SpectralHypersparsifier{} on sub-hypergraphs \( \hypergraph{H_1}, \hypergraph{H_2}, \dots, \hypergraph{H_k} \) with the set of hyperedges \( \hypergraph{E_1}, \hypergraph{E_2}, \dots, \hypergraph{E_k} \), respectively.
The algorithm also maintains a counter \( t \) which tracks the number of insertions in \hypergraph{H}.
In the following, we first explain how the algorithm handles insertions and deletions, and then discuss its guarantees.

\underline{Insertions:}
upon an insertion of a hyperedge \( e \) into \hypergraph{H}, the algorithm first updates the counter \( t \) by incrementing it by one.
Let \( j \) be the highest bit in the counter \( t \) that is flipped after \( t \) is updated, or alternatively,
\begin{equation*}
j = \max \left\{ 1 \leq i \leq k \mid \text{\( t \) is divisible by \( 2^{i - 1} \)} \right\}.
\end{equation*}
The algorithm then sets \( E_j \gets \{ e \} \bigcup \cup_{i = 1} ^j E_i \) and reinitializes the decremental algorithm \( \mathcal A_i \) on \hypergraph{H_j}.
For \( 1 \leq i \leq j - 1 \), the algorithm sets \( E_i \gets \emptyset \).

\underline{Deletions:}
upon a deletion of a hyperedge \( e \) from \hypergraph{H}, the algorithm simply passes the deletion of \( e \) to the decremental algorithm \( \mathcal A_j \) on the hypergraph \hypergraph{H_j} that contains \( e \), maintaining a \SpectralHypersparsifier{} \hypergraph{\sparsifier{H_j}} of \hypergraph{H_j} after the deletion of \( e \).
Note that \( E_i \cap E_j = \emptyset \) for every \( i \neq j \).

\underline{Guarantees:}
We now prove that \( \hypergraph{\sparsifier{H}} = \cup_{i = 1} ^k \hypergraph{\sparsifier{H_i}} \), which is implicitly maintained by the algorithm, satisfies the desired guarantees.

Since \( \hypergraph{E_1}, \hypergraph{E_2}, \dots, \hypergraph{E_k} \) is a partition of \( E \), by \Cref{lem:decomposability}, \hypergraph{\sparsifier{H}} is a \SpectralHypersparsifier{}.

The bound on the size of \hypergraph{\sparsifier{H}} follows from the fact that the sparsifier \hypergraph{\tilde H} consists of the edges that appear in \( k = \lceil \log m \rceil \) sparsifiers, each of which of size \( S(m, n, W) \).

To bound the update time, note that for each \hypergraph{H_i}, the decremental algorithm \( \mathcal A_i \) is reinitialized and maintained at most \( m / 2^{i - 1} \) times as the reinitialization of \( \mathcal A_i \) happens when \( t \leq m \) is divisible by \( 2^{i - 1} \).

Note that, at any point, \( |E_i| \leq 2^{i - 1} \).
To prove this fact, we show that after setting \( E_i \gets \emptyset \), \( E_i \) will contain at most \( 2^{i - 1} \) edges before it is set to \( E_i \gets \emptyset \) again.
By the discussion above, \( E_i \) is set to \( \emptyset \) at time \( t \) only if the value of \( j \) at time \( t \), denoted \( j_t \), is greater than \( i \).
In this case, we set \( E_1, E_2, \dots, E_{j_t} \) to \( \emptyset \).
By the definition of \( j_t \), \( t = c 2^{j - 1} \) for some positive integer \( c \).
We set \( E_i \) to  \( \emptyset \) again after at most \( 2^{i - 1} \) insertions since for time \( t' = c 2^{j - 1} + 2^{i - 1} \), we have \( t' = \left( c 2^{j - i} + 1 \right) 2^{i - 1} \), and thus \( j_{t'} \geq i \).
Since there are at most \( 2^{i - 1} \) insertions, \( |E_i| \leq |E_1| + |E_2| + \dots + |E_i| \leq 2^{i - 1} \).


It follows that the total update time of \( \mathcal A_i \) after each initialization is bounded by \( |E_i| \cdot T(|E_i|, n, W) \leq 2^{i - 1} T(m, n, W) \).
Therefore, the total update time for maintaining \hypergraph{\sparsifier{H_i}} throughout the whole sequence of updates is \( \atmost{(m / 2^{i - 1}) \cdot 2^{i - 1} T(m, n, W)}  = \atmost{m \cdot T(m, n, W)} \).
Since \( k = \lceil \log m \rceil \), the total update time of the algorithm is \atmost{m \cdot T(m, n, W)  \log m}.

The guarantee on the probability simply follows from the fact that each hypersparsifier \hypergraph{\sparsifier{H_i}} is correctly maintained with probability at least \( 1 - \atmost{1/n^2} \) and that \( \hypergraph{\sparsifier{H}} = \cup_{i = 1} ^k \hypergraph{\sparsifier{H_i}} \).
\end{proof}









Recall that, in \Cref{subsec:static}, we assumed the hyperedges in \hypergraph{H} are of size \( (r/2, r ] \).
By \Cref{lem:r/2}, static algorithms on \( H \) can be generalized to hypergraphs of rank \( r \) with an \atmost{\log r} overhead in the running time.
In the lemma below, we prove that this approach continues to hold in the decremental setting. 


\begin{lemma} \label{lem:r/2_dynamic}
Let \( \varepsilon > 0 \) and
suppose that there is a decremental algorithm which, given any \( n \)-vertex hypergraph \hypergraph{H'} with \( m \) initial hyperedges of size \( (r/2, r ] \), maintains a \SpectralHypersparsifier{} of \hypergraph{H'} with size \atmost{r^{\ConstantR} \varepsilon^{ - \ConstantVar } \cdot S(m, n)} in \( T(m, n, r, \varepsilon ^{-1}) \) total update time.
Then, for any \( n \)-vertex hypergraph \hypergraph{H} with \( m \) initial hyperedges of rank \( r \), there is a decremental algorithm for maintaining a \SpectralHypersparsifier{} of \hypergraph{H} with size \atmost{r^{\ConstantR} \varepsilon^{ - \ConstantVar } \cdot S(m, n)} in \atmost{T(m, n, r, \varepsilon ^{-1} ) \log r} total update time.
\end{lemma}
\begin{proof}
Similar to the proof of \Cref{lem:r/2}, we partition the hyperedges of \hypergraph{H} as follows.
For \( 1 \leq i \leq \log r \), let \hypergraph{H_i} be the sub-hypergraph containing the hyperedges of \hypergraph{H} of size \( (2^{i - 1} , 2^i] \).
i.e., \( r_i =  2^i\).
Let 
\hypergraph{\sparsifier{H_i}} be a \( (1 \pm \varepsilon ) \) spectral hypersparsifier of \hypergraph{H_i} maintained by the decremental algorithm with size \atmost{r _i ^{\ConstantR} \varepsilon ^{ - \ConstantVar } \cdot S(m_i, n)} in \atmost{T(m_i, n, \varepsilon ^{-1} )} total update time, where \( m_i \) is the number of hyperedges of \hypergraph{H_i}.

After each hyperedge deletion from \hypergraph{H}, we pass the deletion to the decremental algorithm of the corresponding sub-hypergraph containing the deleted hyperedge, thereby maintaining each \hypergraph{\tilde H_i} as a \SpectralHypersparsifier{} of \hypergraph{H_i}.

By \Cref{lem:decomposability}, the hypergraph \( \hypergraph{ \tilde H = \cup _{i = 1} ^{\log r}} \hypergraph{\sparsifier{H_i}} \) is a \SpectralHypersparsifier{} of \hypergraph{H}.
To bound the size of \hypergraph{\sparsifier{H}}, we have
\begin{equation*}
\atmost{  \sum_{i = 1} ^ {\log r} r _i ^{\ConstantR} \varepsilon ^{ - \ConstantVar } \cdot S(m_i, n) } 
= 
\atmost{  \sum_{i = 1} ^ {\log r} 2 ^{\ConstantR i} \varepsilon ^{ - \ConstantVar } 
 \cdot S(m, n)  }
= 
\atmost{ r^{\ConstantR} \varepsilon^{ - \ConstantVar } \cdot S(m, n) }.
\end{equation*}
The bound on time follows immediately from the construction of \hypergraph{\sparsifier{H}}.
\end{proof}

The rest of this section is dedicated to designing decremental implementation of \Cref{alg:light_spectral_sparsify,alg:spectral_sparsify}.
In the following paragraphs, we briefly discuss the main challenge in designing such algorithms.

Recall that the \SpectralHypersparsifier{} \hypergraph{\tilde H} of the hypergraph \hypergraph{H} returned by \Cref{alg:spectral_sparsify} consists of \( t \)-bundle \( \alpha \)-hyperspanners and some additional sampled hyperedges.
Assume that \hypergraph{\tilde H} contains the \( t \)-bundle \( \alpha \)-hyperspanner  \( \hypergraph{B} = \hypergraph{T_1} \cup \hypergraph{T_2} \cup \dots \cup \hypergraph{T_t} \), and that a deletion of an edge from \hypergraph{H} has caused the removal of another hyperedge \( e \in T_j \) from \( B \) to maintain \hypergraph{B} as a \( t \)-bundle \( \alpha \)-hyperspanner.

Since \( T_{j+1} \) is an \( \alpha \)-spanner of \( H \setminus \cup _{i = 1} ^j T_i \), if \( e \) is not removed from \hypergraph{H}, it must appears in \( H \setminus \cup _{i = 1} ^j T_i \).
The same holds for \( T_{j+2} \) if \( e \) is not selected to be in \( T_{j+1} \) after the update, as \( T_{j+2} \) is an \( \alpha \)-spanner of \( H \setminus \cup _{i = 1} ^{j + 1} T_i \), and so on.
Thus, the deletion of \( e \) from \( T_j \) could potentially result in its insertion into the underlying hypergraphs used to maintain \( T_{j + 1}, T_{j + 2}, \dots, T_{t} \).
This is expensive for two reasons: (1) handling the insertion of \( e \) into the underlying hypergraphs requires designing a fully dynamic algorithm to maintain the \( \alpha \)-spanners, and (2) the insertion would need to be passed to \atmost{t} algorithms maintaining \( T_{j + 1}, T_{j+2}, \dots, T_{t} \).

To overcome this challenge, we ensure that \( e \) is removed from \hypergraph{T_j} \textit{only if} it is removed from \hypergraph{H}.
This property is called the \textit{monotonicity property}, and we refer to an algorithm that supports this property as a \textit{monotone algorithm}.
This addresses the concerns raised in the previous paragraph: (1) since the deletion of \( e \) from \( T_j \) does not result in the insertion of \( e \) into \( \hypergraph{H} \setminus \cup_{i = 1} ^j \hypergraph{T_i}  \), we only need to design a decremental algorithm to maintain each \( T_i \), and (2) the deletion does not affect the underlying hypergraphs used to maintain \( T_{j + 1}, T_{j+2}, \dots, T_{k} \).  




\subsubsection{Decremental Monotone \( t \)-Bundle \( \atmost{\log n} \)-Hyperspanners} \label{subsec:t-bundle}

To enforce the monotonicity property, we first reduce the problem of maintaining a \( t \)-bundle \( \alpha \)-hyperspanner of \hypergraph{H} to maintaining an \( \alpha \)-spanner \( G_\hypergraph{H}' \) on its associated graph \associated{\hypergraph{H}}, as shown in the following lemma.
We then apply the decremental monotone algorithm on graphs from \cite{Abraham:2016aa}, which is summarized in \Cref{lem:Abraham_decremental_t-bundle}.


\begin{lemma}[\cite{Oko:2023aa}] \label{lem:Oko_associated_graph}
Let \( \alpha \geq 1 \), \hypergraph{H = (V, E, \vect{w})} be a hypergraph, and \( \associated{\hypergraph{H}} = (V, E_\hypergraph{H}, \vect{w}_\hypergraph{H} ) \) be its associated graph with the function \( f \colon E_\hypergraph{H} \to E \) mapping each edge \( e_\hypergraph{H} \in E_\hypergraph{H}  \) to the corresponding hyperedge \( e \in E \).
If \( G_\hypergraph{H}' \) is an \( \alpha \)-spanner of \associated{\hypergraph{H}}, then the sub-hypergraph \( \hypergraph{H}' \) of \hypergraph{H} with the set of hyperedges \( E' = \{ f(e') \mid e' \in E(G_\hypergraph{H}')  \} \) is an \( \alpha \)-hyperspanner of \hypergraph{H}.
\end{lemma}

\begin{proof}
Since \( G_\hypergraph{H}' \) is an \( \alpha \)-spanner of \associated{\hypergraph{H}}, for every vertex \( u, v \in V \), there exists a \( (u, v) \)-path \( P \) in \( G_\hypergraph{H}' \) such that \( \sum_{e \in P} 1/w_e \leq \alpha d_\associated{\hypergraph{H}}(u, v) \), as \( \vect{w}_H \) matches \vect{w} on \( E_H \).
Using the map \( f \colon E_\hypergraph{H} \to E \), the path \( f(P) = \cup _{e \in P} f(e) \) is a \( (u, v) \)-path in \hypergraph{H'} with the length
\begin{equation*}
\sum_{e \in f(P)} \frac{1}{w_e}
\leq
\sum_{e \in P} \frac{1}{w_e}
\leq
\alpha d_\associated{\hypergraph{H}}(u, v)
= \alpha d_\hypergraph{H}(u, v),
\end{equation*}
where the last equation follows from the fact that \( d_\associated{\hypergraph{H}}(u, v)
= d_\hypergraph{H}(u, v) \) by the construction of the associated graph.
\end{proof}




\begin{restatable}[\cite{Abraham:2016aa}]{lemma}{abraham}\label{lem:Abraham_decremental_t-bundle}
Let \( G = (V, E, \vect{w}) \) be an \( n \)-vertex graph with \( m \) initial edges and the weight ratio \( W = \max _{i,j} w_i / w_j \), and \( t \geq 1 \) be an integer.
Then, there exists a decremental monotone algorithm against an oblivious adversary that maintains a \( t \)-bundle \atmost{\log n}-spanner \( B \) of \( G \) with an expected size of \atmost{t n \log ^3 n \log W} and an expected total update time of \atmost{t m \log ^3 n}.
\end{restatable}

\begin{remark}
While \cite{Abraham:2016aa} obtained the guarantees when \( G \) was a simple graph, we use \Cref{lem:Abraham_decremental_t-bundle} on associated graphs which are multi-graphs.
See \Cref{app:abraham} for an explanation of their algorithm and its extension to multi-graphs.
\end{remark}


Since the associated graph \associated{\hypergraph{H}} contains \atmost{m r^2} edges, directly applying \Cref{lem:Abraham_decremental_t-bundle} on \associated{\hypergraph{H}} results in a decremental monotone algorithm with an expected total update time of \atmost{t m  r^2 \log ^3 n}.
In the following lemma, we show how to reduce the expected total update time to \atmost{ t m r \log ^3 n}.


\begin{lemma} \label{lem:decremental_t-bundle}
Let \( \hypergraph{H} = (V, E, \vect{w}) \) be an \( n \)-vertex hypergraph of rank \( r \) with \( m \) initial hyperedges and weight ratio \( W = \max _{i,j} w_i / w_j \), and \( t \geq 1 \) be an integer.
Then, there exists a decremental monotone algorithm against an oblivious adversary that maintains a \( t \)-bundle \atmost{\log n}-hyperspanner \hypergraph{B} of \hypergraph{H} with an expected size of \atmost{ t n \log ^3 n \log W} and an expected total update time of \atmost{t m r  \log ^3 n}.
\end{lemma}
\begin{proof}
To achieve the desired update time,
we use a standard technique as follows.
Instead of applying the algorithm of \Cref{lem:Abraham_decremental_t-bundle} directly on the associated graph \associated{\hypergraph{H}}, we apply it on the star graph \stargraph{\hypergraph{H}} of \hypergraph{H}, defined as follows: for each hyperedge \( e \in E \), instead of adding a clique \( C(e) \) and forming the associated graph \associated{\hypergraph{H}}, replace each clique \( C(e) \) with a star centered at an arbitrarily chosen vertex of \( e \).
Note that \stargraph{\hypergraph{H}} is not unique.

By \Cref{lem:Oko_associated_graph} and the fact that \stargraph{\hypergraph{H}} is a \( 2 \)-spanner of \associated{\hypergraph{H}}, it is straightforward to see that for every \( \alpha \)-spanner \( S_\hypergraph{H}^\star \) of \stargraph{\hypergraph{H}}, the sub-hypergraph \hypergraph{H^\star} of \hypergraph{H} with hyperedges \( E^\star = \{ f(e^\star) \mid e^\star \in E(S_\hypergraph{H}^\star) \} \) is an \( 2 \alpha \)-hyperspanner of \hypergraph{H}.

Thus, without any asymptotic loss in the quality of the \atmost{\log n}-spanner, we can apply \Cref{lem:Abraham_decremental_t-bundle} on \stargraph{\hypergraph{H}}, which has \atmost{mr} edges, rather than using the associated graph \associated{\hypergraph{H}}.
The expected total update time follows accordingly.
\end{proof}



\subsubsection{Decremental Implementation of \Cref{alg:light_spectral_sparsify}} \label{subsec:decremetal_light_spectral_sparsify}

With the decremental monotone algorithm for maintaining \( t \)-bundle \( \atmost{\log n} \)-hyperspanners in place, we are now ready to explain the decremental implementation of \Cref{alg:light_spectral_sparsify}.

\begin{lemma} \label{lem:decremental_light-spectral-sparsify}
Let \( 0 < \varepsilon \leq 1 \),
 \( \gamma \geq 1 \) be a constant, and \( \hypergraph{H} = (V, E, \vect{w}) \) be an \( n \)-vertex hypergraph with \( m \) initial hyperedges of size \( (r/2, r] \).
Then, with probability at least \( 1 - \atmost{1 /n^2} \) against an oblivious adversary, the decremental implementation of \Cref{alg:light_spectral_sparsify} maintains a \SpectralHypersparsifier{} \( \hypergraph{\sparsifier{H}} = (V, \tilde{E}, \tilde{\vect{w}}) \) of expected size \( |\tilde{E}| \leq |E(B)| + |E| /2 \) in expected total update time \atmost{ t m r \log ^3 n}, where \( t = 16 \alpha \ConstantProp r^3 \varepsilon ^{-2} \log n \), over any arbitrary sequence of \atmost{n^\gamma} hyperedge deletions.
\end{lemma}
\begin{proof}
We first explain how we maintain the \( t \)-bundle \atmost{\log n}-hyperspanner \hypergraph{B} and the set of sampled hyperedges in \( \hypergraph{\sparsifier{H}} \setminus \hypergraph{B} \) after each deletion in \hypergraph{H}.


To maintain \hypergraph{B}, we use the decremental algorithm of \Cref{subsec:t-bundle}.
Specifically, after each deletion in \hypergraph{H}, we first pass the deletion to the algorithm of \Cref{lem:decremental_t-bundle} and update \hypergraph{B} accordingly.
Since this algorithm is monotone, a hyperedge is deleted from \hypergraph{B} only if it is removed from \hypergraph{H}.
Thus, \hypergraph{B} and \( \hypergraph{H} \setminus \hypergraph{B} \) remain decremental after each update.

Next, we check whether the deletion in \hypergraph{H} or the update in \hypergraph{B} resulted in hyperedge deletions in \( \hypergraph{H} \setminus \hypergraph{B} \): if a hyperedge is removed from \( \hypergraph{H} \setminus \hypergraph{B} \), we simply remove it from  \( \hypergraph{\sparsifier{H}} \setminus \hypergraph{B} \).
Recall that a hyperedge \( e \) in \( \hypergraph{\tilde H} \setminus \hypergraph{B} \) is sampled from \( \hypergraph{H} \setminus \hypergraph{B} \) with probability \( 1/4 \) and has weight \( 4 w_e \).
Since the deletion was done by an oblivious adversary, the updated \( \hypergraph{\tilde H} \setminus \hypergraph{B} \) still contains the sampled hyperedges of \( \hypergraph{H} \setminus \hypergraph{B} \), and no further action is required.



To prove that the guarantees hold with probability at least \( 1 - \atmost{1/n^2} \), first note that in the decremental setting, the probability of failure accumulates over at most \( c n^\gamma \) deletions, for a sufficiently large constant \( c \).
Thus, by using \Cref{prop:constant_probability}, we have
\begin{equation*}
\mathbb P \left[ \text{ \( \hypergraph{\sparsifier{H}} \setminus \hypergraph{B} \) is not correctly maintained } \right] \leq   \frac{c n^\gamma}{n^{\gamma + 2}} \leq  \frac{c}{n^2},
\end{equation*}
which means that \( \hypergraph{\sparsifier{H}} \) is a \SpectralHypersparsifier{} of \hypergraph{H} with probability at least \( 1 - c/n^2 \).
Moreover, by using \Cref{eq:whp_light_spectral_sparsify},
\begin{equation*}
\mathbb P \left[ \left|\hypergraph{\sparsifier{H}} \setminus \hypergraph{B} \right| \geq \frac{|E|}{2}\right] \leq \frac{c n^\gamma}{e^{ |E| / 12}} \leq \frac{c n^\gamma}{e^{ \ConstantSize n}}  \leq \frac{d}{n^2},
\end{equation*}
for a sufficiently large constant \( d \).
Therefore,  with probability at least \( 1 - \atmost{1/n^2} \), \hypergraph{\tilde H} is a \SpectralHypersparsifier{} of \hypergraph{H} with expected size \( |\tilde E| \leq |E(B)| + |E|/2 \).



As explained above, we only need to maintain \hypergraph{B} and do not need to inspect \( \hypergraph{H} \setminus \hypergraph{B} \) after each update.
Moreover, \hypergraph{B} is easily accessible since we explicitly maintain each spanner that forms \hypergraph{B}.
Thus, the update time of the algorithm is dominated by the update time of \hypergraph{B}, by \Cref{lem:decremental_t-bundle}, the expected total update time is \atmost{t m r \log ^3 n}.
\end{proof}




\subsubsection{Decremental Implementation of \Cref{alg:spectral_sparsify}} \label{subsec:decremental_spectral_sparsify}

We now explain the decremental algorithm for maintaining a \SpectralHypersparsifier{} of \hypergraph{H}.


\begin{lemma} \label{lem:decremental_spectral_sparsify}
Let \( 0 < \varepsilon \leq 1 \), \( \gamma \geq 1 \) be a constant,  \( \hypergraph{H} = (V, E, \vect{w}) \) be an \( n \)-vertex hypergraph with \( m \) initial hyperedges of size \( (r/2, r] \) and the weight ratio \( W = \max _{i,j} w_i / w_j \), and let \( 1 \leq \rho \leq m \) and \( m^\star \geq n \) be integers.
Then, there is a decremental algorithm that, with probability at least \( 1 - \atmost{ \lceil \log \rho \rceil /n^2} \) against an oblivious adversary, maintains a \SpectralHypersparsifier{} with an expected size of 
\begin{equation*}
\atmost{
\min\left\{ \lceil \log \rho \rceil, \lceil \log m / m^\star \rceil \right\}
\cdot
t n \log ^3 n \log W + \ConstantSize m^\star + m/\rho
}
\end{equation*}
and an expected total update time of 
\begin{equation*}
\atmost{
\min\left\{ \lceil \log \rho \rceil, \lceil \log m / m^\star \rceil \right\}
\cdot
t m r \log ^3 n 
}
\end{equation*}
over any arbitrary sequence of \atmost{n^\gamma} hyperedge deletions.
\end{lemma}
\begin{proof}
We implement a decremental version of \Cref{alg:spectral_sparsify} by decrementally maintaining the hypergraphs \( \hypergraph{H_0}, \hypergraph{H_1}, \dots, \hypergraph{H_k} \), \( \hypergraph{B_1}, \dots, \hypergraph{B_k} \), and \( \hypergraph{\sparsifier{H_1}}, \dots, \hypergraph{\sparsifier{H_k}} \).
For \( 1 \leq i \leq k \), we maintain \hypergraph{B_i} and \hypergraph{\sparsifier{H_i}} by using the algorithms of \Cref{lem:decremental_light-spectral-sparsify,lem:decremental_t-bundle}, respectively.
Since each \hypergraph{B_i} is maintained by a monotone algorithm, no hyperedge is inserted in \( \hypergraph{H_i} = \hypergraph{\sparsifier{H_i}} \setminus \hypergraph{B_i} \) for every \( 1 \leq i \leq k \).
Note that we can easily check whether \( |E_i| \leq \ConstantSize m^\star \) by counting the hyperedges in \hypergraph{H_i} during the initialization step.

By \Cref{lem:spectral_sparsify}, \Cref{alg:spectral_sparsify} terminates after \( \min\left\{ \lceil \log \rho \rceil, \lceil \log m / m^\star \rceil \right\} \) iterations.
This means that the number of \hypergraph{B_i}s is bounded by \( \min\left\{ \lceil \log \rho \rceil, \lceil \log m / m^\star \rceil \right\} \).
By \Cref{lem:decremental_t-bundle}, \( |\hypergraph{B_i}| = \atmost{t n  \log ^2 n \log W} \) for every \( 1 \leq i \leq k \), and the expected size of \hypergraph{\sparsifier{H}} directly follows by substituting the upper bound on the size of \hypergraph{B_i}s in \Cref{eq:SizeSparsifier}.

By \Cref{lem:decremental_t-bundle}, the expected total update time for maintaining each \hypergraph{B_i} is \atmost{tmr \log ^3 n}.
Since there are \( \min\left\{ \lceil \log \rho \rceil, \lceil \log m / m^\star \rceil \right\} \) such \hypergraph{B_i}s to be maintained, the expected total update time follows.

The guarantee on probability follows from the upper bound on the number of \hypergraph{B_i}s.
\end{proof}









Finally, we set the parameters for our algorithm.
Note that \( \alpha = \atmost{\log n} \) as the algorithm of \Cref{lem:decremental_light-spectral-sparsify} maintains \( t \)-bundle \atmost{\log n}-hyperspanners, and so \( t = \atmost{ r^3 \varepsilon ^{-2} \log ^2 n} \).
We set \( \rho =  \Theta\left( m \right) \) and \( m^\star = n = \poly{n} \), and so
\begin{equation*}
\min\left\{ \lceil \log \rho \rceil, \lceil \log m / m^\star \rceil \right\} = \lceil \log m \rceil.
\end{equation*}
We conclude our discussion on the decremental algorithm in the following corollary.

\begin{corollary} \label{cor}
Let \( 0 < \varepsilon \leq 1 \), \( \gamma \geq 1 \) be a constant, \( 1 \leq \rho \leq m \), and  \( \hypergraph{H} = (V, E, \vect{w}) \) be an \( n \)-vertex hypergraph with \( m \) initial hyperedges of size \( (r/2, r] \) and the weight ratio \( W = \max _{i,j} w_i / w_j \).
Then, there is a decremental algorithm that, with probability at least \( 1 - \atmost{ \lceil \log m \rceil /n^2} \) against an oblivious adversary, maintains a \SpectralHypersparsifier{} with an expected size of 
\begin{equation*}
\atmost{ n r^3 \varepsilon ^{-2} \log m \log ^5 n \log W }
\end{equation*}
and an expected total update time of 
\begin{equation*}
\atmost{ m r^4 \varepsilon ^{-2} \log m \log ^5 n }
\end{equation*}
over any arbitrary sequence of \atmost{n^\gamma} hyperedge deletions.
\end{corollary}




\subsubsection{The Fully Dynamic Algorithm} \label{subsec:fully_dynamic}


We extend the decremental algorithm of \Cref{cor} on hypergraphs with hyperedges of size \( (r/2, r] \) to general hypergraphs of rank \( r \), using \Cref{lem:r/2_dynamic}.
This results in an \atmost{\log r} overhead in the update time.

To extend the decremental algorithm to a fully dynamic one, we use \Cref{lem:turn_to_fully_dynamic}, which adds an \atmost{\log m} overhead to both the size and the update time.
Since \Cref{cor} holds for \atmost{n^\gamma} hyperedge deletions, we assume that the fully dynamic algorithm undergoes arbitrary hyperedge insertions but \atmost{n^\gamma} hyperedge deletions.
This ensures that each decremental algorithm in \Cref{lem:turn_to_fully_dynamic} undergoes \atmost{n^\gamma} hyperedge deletions, and so \( T\left( m, n, W \right) = \atmost{ m r^4 \varepsilon ^{-2} \log m \log ^5 n } \).
Note that, instead of restricting the number of deletions, we could reinitialize each decremental algorithm after \atmost{n^\gamma}deletions.
However, this could result in \atmost{2^n/n^\gamma} reinitializations, which adds an exponential overhead to \( T\left( m, n, W \right) \). 

The resulting guarantees are summarized below.

\main*







