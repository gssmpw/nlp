\section{Introduction}


Sparsifying large graphs into smaller ones while provably retaining key graph-centric properties is a powerful algorithmic framework at the heart of scalable graph processing. A notable example is spectral sparsification, where reweighted subgraphs preserve essential spectral properties of the original graph. Rooted in the breakthrough work on ultra-fast solvers for Laplacian systems~\cite{Spielman:2004aa}, a series of advancements led to the development of essentially linear-time algorithms for computing spectral sparsifiers~\cite{Spielman:2008aa,Spielman:2011aa,Batson:2012aa}. Since then, these sparsifiers have been successfully applied to obtain significantly faster algorithms for a myriad of machine learning primitives, such as spectral clustering and graph partitioning~\cite{Lee:2014aa,Peng:2017aa,Feng:2018aa}, pruning of neural network~\cite{Hoang:2023aa,Laenen:2023aa}, and graph learning~\cite{Calandriello:2018aa}, to name a few.

In many downstream machine learning tasks, a common assumption is to endow the objects of interest with pair-wise relationships. While these relationships are naturally represented as graphs, they often fail to capture the complex interdependencies found in real-world networks, such as biological interactions or social connections~\cite{Bonacich:2004aa}. As highlighted in~\cite{Zhou:2006aa}, modeling these relationships using \emph{hypergraphs} has a host of advantages, often leading to improved performance over graph-based methods in fundamental learning tasks like clustering, classification, and embedding. This raises the following natural question -- an analogue of spectral sparsification for graphs: \emph{Do hypergraphs admit spectral sparsifiers?} Recent research has increasingly focused on this question, leading to algorithms that achieve near-optimal sparsifier sizes~\cite{Kapralov:2021aa,Kapralov:2021ab,Oko:2023aa,Jambulapati:2023aa,Lee:2023aa}.

In this work, we study spectral sparsification through the lens of an emerging and important aspect of large-scale hypergraph processing: \emph{dynamism}. Concretely, given a hypergraph that undergoes an intermixed updates of hyperedge insertions and deletions, the goal is to efficiently maintain a sparsifier that spectrally approximates the input hypergraph under these updates. Dynamic maintenance of spectral sparsifiers is particularly relevant to real-world applications, where hypergraph data is constantly changing. Recomputing solutions to hypergraph-based optimization problems from scratch after each update is often computationally prohibitive, making efficient dynamic algorithms essential.

Our main contribution is the first fully dynamic algorithm for maintaining spectral sparsifiers of hypergraphs, with strong theoretical guarantees, as summarized in the theorem below.



\begin{restatable}{theorem}{main}
\label{th:main}
Let \( 0 < \varepsilon \leq 1 \), 
\( \gamma \geq 1 \) be a constant, and \( \hypergraph{H} = (V, E, \vect{w}) \) be an initially empty \( n \)-vertex hypergraph guaranteed to have at most \( m \) hyperedges of rank 
at most \( r \) and weight ratio \( \max _{i,j} w_i / w_j \leq W \) throughout any sequence of hyperedge insertions and \atmost{n^\gamma} hyperedge deletions.
Then, there is a fully dynamic algorithm that, with probability at least \( 1 - \atmost{ \lceil \log m \rceil /n^2} \) against an oblivious adversary, maintains a \SpectralHypersparsifier{} of \( H \) with an expected size of 
\begin{equation*}
\atmost{ n r^3 \varepsilon ^{-2} \log ^2 m \log ^5 n \log W }
\end{equation*}
and an expected amortized update time of 
\begin{equation*}
\atmost{ r^4 \varepsilon ^{-2} \log ^2 m \log ^5 n \log r }.
\end{equation*}
\end{restatable}


To put our result in context, note that reading the set of vertices that define a single hyperedge of rank $r$ inherently requires $\Omega(r)$ time, implying the update time must necessarily depend on $r$. When $r = 2$, this result precisely recovers the dynamic spectral algorithm of ordinary graphs by Abraham et al.~\cite{Abraham:2016aa}. In fact, when $r$ is a big constant -- a setting  which may be relevant to many applications, our algorithm achieves both nearly optimal sparsifier size and poly-logarithmic update time in the input parameters of the problem.












\subsection{Technical Overview}




The starting point of our algorithm is the now standard sampling technique that constructs a \SpectralHypersparsifier{} \( \hypergraph{\tilde H} = (V, \tilde E, \tilde{\vect{w}}) \) of a hypergraph \( \hypergraph{H} = (V, E, \vect{w}) \) by sampling each hyperedge \( e \in E \) with probability \( p_e \) and then setting \( \tilde w_e \gets  w_e/p_e \). 
While the sampling itself can be implemented in \atmost{m} time, the challenge lies in computing \( p_e \) for each \( e \in E \) efficiently. 

The sampling probability \( p_e \) is typically inversely proportional to the effective resistance of hyperedge  \( e \) \cite{Bansal:2019aa}. As a result, if we can upper bound the effective resistance for all hyperedges, we can avoid computing \( p_e \) for every \( e \in E \).
This enables us to sample each hyperedge with constant probability \( p \) and then rescale its weight to \( \tilde w_e \gets  w_e/p \).

Motivated by \cite{Abraham:2016aa,Oko:2023aa}, which both build on the work of~\cite{Koutis:2016aa}, our algorithm uses a set of spanners \hypergraph{B} to bound the effective resistance of \( \hypergraph{H} \setminus \hypergraph{B} \), and then return \hypergraph{B} along with the sampled edges from \( \hypergraph{H} \setminus \hypergraph{B} \) (with constant probability \( p \)) as the \SpectralHypersparsifier{} of \hypergraph{H}.
More precisely, we find a sequence of \( \alpha \)-hyperspanners \( T_1, T_2, \dots, T_t \), where \( T_1 \) is an \( \alpha \)-hyperspanners of \( H \), \( T_2 \) is an \( \alpha \)-hyperspanners of \( H \setminus T_1 \), and so on.
We call \( B = T_1 \cup T_2 \cup \dots \cup T_t \) a \( t \)-bundle \( \alpha \)-hyperspanner of \( H \). 
The hypergraph $B$ guarantees that every hyperedge \( e \in H \setminus B \) has effective resistance of at most \( 4 \alpha / rt \).
By choosing a proper value for \( t \), we ensure that \( p = 1/4 \) for every hyperedge \( e \in \hypergraph{H} \setminus \hypergraph{B} \).


 







A priori, maintaining such a nested bundle of hyperspanners in a dynamic setting is not straightforward. To tackle this, we first design a decremental algorithm that handle only hyperedge deletions. It is not hard to see that this algorithm can be generalized to a fully dynamic one at the cost of paying a \( \log r \) overhead in the update time, where \( r \) is the rank of \hypergraph{H}.
Next, we outline the high-level idea behind our decremental algorithm.




Assume that \hypergraph{\tilde H} contains the \( t \)-bundle \( \alpha \)-hyperspanner  \( \hypergraph{B} = \hypergraph{T_1} \cup \hypergraph{T_2} \cup \dots \cup \hypergraph{T_t} \), and that a deletion of an edge from \hypergraph{H} has caused the removal of another hyperedge \( e \in T_j \) from \( B \) to maintain \hypergraph{B} as a \( t \)-bundle \( \alpha \)-hyperspanner.
Since \( T_{j+1} \) is an \( \alpha \)-spanner of \( H \setminus \cup _{i = 1} ^j T_i \), if \( e \) is not removed from \hypergraph{H}, it must appears in \( H \setminus \cup _{i = 1} ^j T_i \).
The same holds for \( T_{j+2} \) if \( e \) is not selected to be in \( T_{j+1} \) after the update, as \( T_{j+2} \) is an \( \alpha \)-spanner of \( H \setminus \cup _{i = 1} ^{j + 1} T_i \), and so on.
Thus, the deletion of \( e \) from \( T_j \) could potentially result in its insertion into the underlying hypergraphs used to maintain \( T_{j + 1}, T_{j + 2}, \dots, T_{t} \).
This is expensive for two reasons: (1) handling the insertion of \( e \) into the underlying hypergraphs requires designing a fully dynamic algorithm to maintain the \( \alpha \)-hyperspanners, and (2) the insertion would need to be passed to \atmost{t} algorithms maintaining \( T_{j + 1}, T_{j+2}, \dots, T_{t} \).

To overcome this challenge, we ensure that \( e \) is removed from \hypergraph{T_j} \textit{only if} it is removed from \hypergraph{H}.
This property is called the \textit{monotonicity property}, and we refer to an algorithm that supports this property as a \textit{monotone algorithm}.
This addresses the concerns raised in the previous paragraph: (1) since the deletion of \( e \) from \( T_j \) does not result in the insertion of \( e \) into \( \hypergraph{H} \setminus \cup_{i = 1} ^j \hypergraph{T_i}  \), we only need to design a decremental algorithm to maintain each \( T_i \), and (2) the deletion does not affect the underlying hypergraphs used to maintain \( T_{j + 1}, T_{j+2}, \dots, T_{k} \).

\subsection{Related Work}
Because of foundational importance and practical relevance of spectral-based sparsification on graphs, various variants of such sparsifiers have been studied across different computational models. Some examples include, dynamic sparsifiers~\cite{Bernstein:2022aa}, streaming sparsifiers~\cite{Kelner:2013aa,Kapralov:2017aa,Kapralov:2019aa}, distributed and parallel sparsifiers,~\cite{Koutis:2016aa}, dynamic vertex sparsifiers~\cite{Goranci:2018aa,Durfee:2019aa,Chen:2020ac,Gao:2021aa,Axiotis:2021aa,Brand:2022aa,Dong:2022aa} and distributed vertex sparsifiers~\cite{Zhu:2021aa,Forster:2021aa}.










