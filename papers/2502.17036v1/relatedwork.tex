\section{Related Work}
The goal of using a re-ranker in an information retrieval context is to refine the outputs of an initial retrieval step based on a lexicographical or semantic database search. LM-based re-rankers are more expensive to run compared to simpler methods based on lexical matching, like BM25, but are expected to increase the performance of the overall retrieval system thanks to their semantic understanding~\citep{glass-etal-2022-re2g,li2023making}. 
\citet{sun-etal-2023-chatgpt} also showed how standard LLMs, like GPT-4, can be used as re-rankers.

Two popular benchmarks for re-rankers are the BEIR and KILT benchmarks by \citet{thakur2021beir,petroni-etal-2021-kilt}. 
Compared to our work, these benchmarks focus on high-level re-ranker performance and do not consider fine-grained aspects of difficulty for re-rankers. 

Similarly to our work, \citet{sturua2024jina} identify and investigate fine-grained aspects of difficulty for their \texttt{jina} models, of which one is \emph{misleading syntactic similarities}. This describes the case when passages with high syntactic similarity to the query are favoured over gold documents with lower syntactic overlap. Henceforth referred to as \emph{distractors}. 
\citet{wang-etal-2024-dapr} instead consider an aspect of difficulty related to \emph{missing document context}, for which a re-ranker may fail to identify a gold passage if its identification hinges on knowing that the passage comes from a relevant document or webpage. By prepending page titles to passages they were able to alleviate this issue on NQ. 

In contrast to these works, we expand on the analysis of distractors and missing document context to include multiple SOTA re-rankers, datasets from diverse domains and better tuned metrics. We also tie these aspects of difficulty to a more fundamental question of whether LM re-rankers are steered by lexical similarities. To measure this, we develop a new metric which allows us to identify problematic samples.