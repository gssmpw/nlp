
\documentclass{article}


\usepackage{arxiv}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
% \usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{cleveref}       % smart cross-referencing
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{adjustbox} % For better figure positioning and bounding box
\usepackage{tcolorbox} % For enclosing figures in a box
\usepackage{multicol}  % For multi-column layouts

\usepackage[square,authoryear]{natbib}



% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}

\usepackage{arxiv}

\newcommand{\RETURN}{\textbf{return} }


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\title{Scalable Differentially Private Bayesian Optimization}

\newif\ifuniqueAffiliation
\uniqueAffiliationtrue

\ifuniqueAffiliation 
\author{ 
    {\hspace{1mm}Getoar Sopa}\thanks{Equal contribution} \\
    Department of Statistics\\
    Columbia University\\
    New York, NY \\
    \And
    {\hspace{1mm}Juraj Marusic}\footnotemark[1]\\
    Department of Statistics\\
    Columbia University\\
    New York, NY \\
    \And
    {\hspace{1mm}Marco Avella-Medina} \\
    Department of Statistics\\
    Columbia University\\
    New York, NY \\
    \And
    {\hspace{1mm}John P. Cunningham} \\
    Department of Statistics \\
    Columbia University \\ 
    New York, NY \\ \\
}

\begin{document}

\maketitle
\let\thefootnote\relax\footnotetext{\noindent\small Emails: 
\texttt{\{gs3222, jm5692, ma3874, jpc2181\}@columbia.edu}}


\begin{abstract}
In recent years, there has been much work on scaling Bayesian Optimization to high-dimensional problems, for example hyperparameter tuning in large neural network models. These scalable methods have been successful, finding high objective values much more quickly than traditional global Bayesian Optimization or random search-based methods. At the same time, these large neural network models often use sensitive data, but preservation of Differential Privacy has not scaled alongside these modern Bayesian Optimization procedures. Here we develop a method to privately estimate potentially high-dimensional parameter spaces using Gradient Informative Bayesian Optimization. Our theoretical results prove that under suitable conditions, our method converges exponentially fast to a ball around the optimal parameter configuration. Moreover, regardless of whether the assumptions are satisfied, we show that our algorithm maintains privacy and empirically demonstrates superior performance to existing methods in the high-dimensional hyperparameter setting.

\end{abstract}



\section{Introduction}
\label{introduction}

Differentially Private Machine Learning methods have increasingly seen use by practitioners due to the sensitive nature of the data used in some machine learning applications. Examples include healthcare data \citep{kourou2015machine}, data collected at scale by companies that must comply with privacy regulations \citep{cummings2018role}, and more. Differential Privacy allows practitioners to reason about the amount of 'data leakage' to an individual by the inclusion of one data point to the input of an algorithm \citep{dwork2006differential}. 

Independent of this body of research, there have been many recent advances in optimizing black box functions using Bayesian Optimization (BO) \citep{shahriari2015taking}. Often, these black box functions appear in the optimization of hyperparameters of machine learning algorithms \citep{snoek2012practical}. These methods allow for the efficient evaluation of points in the domain space, and therefore often find 'good' hyperparameters in many fewer rounds of training than classical grid search methods. However, many Bayesian Optimization methods suffer from the curse of dimensionality, a phenomenon that is alleviated in local Bayesian Optimization approaches such as TuRBO \citep{eriksson2019scalable} and Gradient Informative Bayesian Optimization (GIBO) \citep{muller2021local}. GIBO avoids the curse of dimensionality by using Bayesian Optimization to approximate the function gradient, and then descending along this approximate gradient path.

How then can we tune hyperparameters in a differentially private way?
 Recent work has mostly considered nonadaptive, random grid search methods \citep{papernot2021hyperparameter, mohapatra2022role,liu2019private}. These papers have developed powerful techniques for ensuring a constant privacy cost for training the model for given hyperparameter configurations, even as a random - potentially large - number of configurations are tested. Recently, this line of work was extended to adaptive hyperparameter search strategies by \cite{wang2024dp}.
Independently, \cite{kusner2015differentially} devised a framework for selecting candidate hyperparameter configurations using Bayesian Optimization, while maintaining privacy in the validation dataset. Making BO differentially private was a significant development, and the authors show that their algorithm provided rapid convergence to a minimizing hyperparameter configuration.  However, as this work was done before Bayesian Optimization techniques were scaled to high-dimensional problems, it too suffers from the curse of dimensionality.

Here we introduce a Differentially Private variant of GIBO, which can be considered a black box analog of Noisy Gradient Descent when gradients are unknown or prohibitively expensive to compute. Doing so enables efficient, private optimization over high-dimensional spaces, thus filling the gap left by traditional global Bayesian Optimization or random search routines. The tuning of continuous hyperparameters of large-scale models in particular falls into this paradigm. 
We prove convergence guarantees of our proposed algorithm. Formally, for sufficiently well-behaved validation loss functions, we prove that the suboptimality gap induced by our algorithm converges exponentially fast to a ball whose size depends on both the magnitude of the privacy noise injected into the algorithm and the gradient bias induced by the gradient approximation step. We demonstrate that this gradient bias becomes small compared to the privacy noise after only a few iterations of the algorithm. Furthermore, we empirically show that even if the assumptions on the validation loss function are not met, our algorithm can quickly find near-optimal parameters. In simple settings, we show that our algorithm performs similar to the Noisy Gradient Descent, while outperforming current state-of-art private Bayesian Optimization methods in high-dimensional hyperparameter tuning.


\section{Background}
\label{preliminaries}

Since this approach combines results from both Bayesian Optimization and Differential Privacy, we present a brief overview of results we will be using from both areas. Furthermore, we will introduce the problem setting and connect our framework to the related work in both fields. 

\subsection{Notation}
For a vector $v$, $\| v \|$ without a subscript refers to the Euclidean norm $\| v \|_2 = \sqrt{v^\top v}$. For two vectors $\mathbf{x} = (x_1, .., x_n)$, $\mathbf{x}' = (x_1', .., x_n')$ in $\mathcal{X}^n$, we define their Hamming distance $d_H(\mathbf{x}, \mathbf{x}') := |i: x_i \neq x'_i |$. Next, for a function $f : \Theta \to \mathbb{R}$, and a collection of points $\mathcal{D}:= \{ \theta_i\}_{i=1}^n$, where $\theta_i \in \Theta \quad \forall i$, we define $f(\mathcal{D}) := \begin{bmatrix}
    f(\theta_1)&
    \cdots &
    f(\theta_n)
\end{bmatrix}^\top.$ Also, for a function $k: \Theta \times \Theta \to \mathbb{R}$, let $\nabla k(x, y) := \partial_x k(x,y)$, and $k(x, y) \nabla ^ \top := \partial_y k(x,y)$. Finally, for $\mathcal{D}':= \{y_i\}_{i=1}^m$, similarly as before define $k(\mathcal{D}, {\mathcal{D}}')_{ij} := k(x_i, y_j)$.

\subsection{Differential Privacy}
There are multiple mathematical frameworks for formalizing the concept of Differential Privacy, but we will use the concept of Gaussian Differential Privacy (GDP) in this paper \citep{dong2022gaussian}. To this end, let $\mathcal{M}: \mathcal{X}^n \to \mathcal{Y}$ denote a (randomized) mechanism, taking as input a set $\mathbf{x} = (x_1, .., x_n) \in \mathcal{X}^n$ and yielding some output in $\mathcal{Y}$.  Datasets $\mathbf{x}$ and $\mathbf{x}'$ are neighboring datasets if they differ in exactly one entry, i.e. if $d_H(\mathbf{x},\mathbf{x}')=1$. Before we can formally define Gaussian Differential Privacy, we must first define the trade-off function.
\begin{definition} [\cite{dong2022gaussian}]
     Given two probability distributions $P$ and $Q$ and a rejection rule $0\leq \phi \leq 1$, the trade-off function is defined as
    \[
    T(P,Q)(\alpha) = \inf \{1 - E_Q[\phi]:  E_P[\phi] \leq \alpha\},
    \]
    where the supremum is taken over the set of measurable rejection rules.
\end{definition}
Informally, the trade-off function characterizes how difficult it is to distinguish distribution $P$ from $Q$ through hypothesis testing. The following definition characterizes GDP.
\begin{definition}[\cite{dong2022gaussian}]
    
    Let $G_\mu = T\left(N(0,1), N(\mu,1)\right)$, i.e. $G_\mu(\alpha)=\Phi(\Phi^{-1}(1-\alpha)-\mu)$ for $\alpha\in[0,1]$, where $\Phi(\cdot)$ denotes the CDF of a standard Gaussian random variable. A randomized mechanism $\mathcal{M}$ is $\mu$-GDP if for all neighboring datasets $\mathbf{x}$ and $\mathbf{x}'$, the mechanism satisfies
    \[
    T\left(\mathcal{M}(\mathbf{x}), \mathcal{M}(\mathbf{x}')\right)(\alpha) \geq G_\mu(\alpha), \quad\forall \alpha \in[0,1].
    \]
\end{definition}
In other words, a mechanism is $\mu$-GDP if distinguishing  the output of the mechanism with two different inputs which differ in only one entry is at least as difficult as statistically distinguishing a draw from a $N(\mu, 1)$ random variable from a draw from a $N(0, 1)$ random variable. This framework of differential privacy therefore allows for a clear hypothesis testing interpretation of the privacy guarantees of a mechanism \citep{wasserman2010statistical}.

Our approach for ensuring Gaussian Differential Privacy in our algorithm is to make use of the Gaussian Mechanism. This is a simple way to make a deterministic statistic $h(\mathbf{x})$ differentially private by introducing an additive noise calibrated according to the global sensitivity $GS(h)$ of the function $h$, defined as follows.
\begin{definition}
    The global sensitivity $GS(h)$ of a $d-$dimensional function $h$ is defined as 
    \[
    GS(h) = \sup_{\mathbf{x}, \mathbf{x}'}\|h(\mathbf{x}) - h(\mathbf{x}')\|,
    \]
    where the supremum is taken over all  neighboring datasets $\mathbf{x}$ and $\mathbf{x}'$. 
\end{definition}
The following theorem presents the Gaussian Mechanism's privacy guarantee.
\begin{theorem}[\cite{dong2022gaussian}]
    Let $h$ be a deterministic function with finite global sensitivity $GS(h)$. The randomized function $\tilde{h}(x) = h(x) + \frac{GS(h)}{\mu}Z$, where $Z \sim N(0,I_d)$, is $\mu$-GDP. 
    \label{gaussianmechanism}
\end{theorem}
A final result of Gaussian Differential Privacy that we will use in this work relates to the composition of multiple GDP algorithms.
\begin{corollary}[\cite{dong2022gaussian}]
    If the algorithms $\mathcal{A}_t$ are $\mu_t$-GDP for $1\leq t\leq T$, then the $T$-fold composition of $\mathcal{A}_1$, ..., $\mathcal{A}_T$ is $\sqrt{\sum_{t=1}^T \mu_t^2}$-GDP.
    \label{composition}
\end{corollary}
\subsection{Bayesian Optimization}
The objective in Bayesian Optimization is to minimize some black box function $f$, and to find the input value that achieves this minimum. Bayesian Optimization strategies find a minimum by constructing a stochastic surrogate model and iteratively sampling new points, informed by the current properties of the surrogate model, in an attempt to minimize this surrogate function. If an appropriate surrogate model is chosen, the location of the minimum in the surrogate function should align with the location of the minimum in the true function. The surrogate model used in this paper is a Gaussian Process.

A thorough introduction to Bayesian Optimization can be found, for example, in \cite{garnett2023bayesian} and \cite{frazier2018tutorialbayesianoptimization}.
\subsection{Gaussian Processes}
A Gaussian Process $GP(m, k)$ is a stochastic process characterized by a mean function $m: \Theta \to \mathbb{R}$ and a kernel function $k: \Theta \times \Theta \to \mathbb{R}$. 
It has the property that if the function $f$ is a draw from the Gaussian Process $GP(m, k)$, then for any finite collection of points $\boldsymbol{\theta} \in \Theta^b$, we have that $f(\boldsymbol{\theta}) \sim N\left(\mu(\boldsymbol{\theta}), k(\boldsymbol{\theta}, \boldsymbol{\theta})\right)$, where $k(\boldsymbol{\theta},\boldsymbol{\theta})_{ij} =k(\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)$.

An appealing property of Gaussian Processes is conditioning on data: if $f$ is a $GP(m, k)$, then conditioned on the function observations at the collection of points $\mathcal{D}$, we have that $f|(\mathcal{D}, f(\mathcal{D})) = GP(m_\mathcal{D}, k_\mathcal{D})$, where
\[
m_\mathcal{D}(\theta) = m(\theta) + k(\theta, \mathcal{D}) k(\mathcal{D}, \mathcal{D})^{-1} (f(\mathcal{D}) - m(\mathcal{D}) ),
\]
and 
\[
k_\mathcal{D}(\theta, \theta') = k(\theta, \theta') - k(\theta, \mathcal{D})k(\mathcal{D}, \mathcal{D})^{-1}k(\mathcal{D}, \theta')
\]
Finally, our analysis makes use of the crucial fact that if $f$ is a GP with a kernel function that is at least twice differentiable, then the gradient of $f$ is also a GP. More precisely, we have the following joint distribution over $f$ and its gradient $\nabla f$
\[
\begin{pmatrix}
    f \\
    \nabla f
\end{pmatrix} \sim GP\left(\begin{pmatrix}
    m \\
    \nabla m
\end{pmatrix}, \begin{pmatrix}
    k & k \nabla^\top \\
    \nabla k & \nabla k \nabla^\top
\end{pmatrix}\right),
\]
and specifically, we have for any $\theta \in \Theta$,
\[
\nabla {f(\theta)}|\mathcal{D} \sim N(\nabla m_\mathcal{D}(\theta), \nabla k_\mathcal{D}(\theta, \theta) \nabla^\top).
\]
%
Throughout the remainder of this work, we will assume that the prior mean $m(\cdot) \equiv 0$.

\subsection{Problem setting}
We have a (validation) data space $\mathcal{X} \subset \mathbb{R}^p$, wherein our sensitive user data lies. Furthermore, we have a parameter space $\Theta \subset \mathbb{R}^d$ which we seek to optimize over. The function $f_{\mathbf{x}}: \Theta \to \mathbb{R}$ we aim to optimize is of the form
\begin{equation*}
    f_{\mathbf{x}}(\theta) := f(\theta, \mathbf{x}) = \frac{1}n\sum_{i=1}^n \mathcal{L}(\theta, x_i),
\end{equation*}
where $\mathbf{x}:= (x_1, .., x_n)\in \mathcal{X}^n$ represents the $p-$dimensional data of $n$ individuals and $\theta \in \Theta$, and $\mathcal{L}(\cdot, x): \Theta \to \mathbb{R}$ is a black box function that depends on user data and the parameters. Note that here we treat $\mathbf{x}$ as intrinsic to the function, and we assume that given $x \in \mathcal{X}$ and $\theta \in \Theta$, we can query $\mathcal{L}(\theta, x)$ but not its gradient. Moreover, we assume that our function evaluations are entirely noiseless. Our objective is to find $\theta^* := \arg \min_\theta f_\mathbf{x}(\theta)$. 
\begin{remark}
    In the context of hyperparameter tuning, our assumption of noiseless function evaluation implies that only the validation data - and not the training data - is sensitive. This implication is similar to the assumption maintained in the work of  \cite{kusner2015differentially} that first introduced a Differentially Private Bayesian Optimization procedure, where only validation data is explicitly privatized. This approach by itself is reasonable because their framework - as well as ours - is agnostic to the training routine used, and private training routines can be accounted for by modeling the validation loss as noisy outcomes, which we discuss in Section \ref{section:noisy}.
\end{remark}
As our approach consists of using a surrogate Gaussian Process to approximate the gradients of $f_\mathbf{x}$ with, our theoretical analysis requires assumptions that guarantee some closeness between our objective function's gradients and the approximating gradients. The following assumption, similar to the assumption made in \cite{wu2024behavior}, suffices to accomplish this. 

\begin{assumption}
    There is a twice continuously differentiable, positive definite kernel $k$ and a constant $C_{\mathcal{X}}$ such that $\mathcal{L} (\cdot, x) \in \mathcal{H} := \text{RKHS}(k)$ and $\|\mathcal{L} (\cdot, x)\|_{\mathcal{H}} \leq C_{\mathcal{X}}, \forall x \in \mathcal{X}$.
    \label{rkhsassumption} 
\end{assumption}

Moreover, for our gradient descent approach to be able to guarantee convergence to a minimum, we also need the following smoothness assumption on our objective function, which is standard in gradient descent analysis.

\begin{assumption}
        $\mathcal{L}(\cdot, x)$ is $L$-smooth for all $x \in \mathcal{X}$, i.e.  for all $\theta, \tilde{\theta} \in \Theta$,
        \[
        \mathcal{L}(\theta, x) \leq \mathcal{L}(\tilde{\theta}, x) + \langle \nabla_\theta \mathcal{L}(\tilde{\theta}, x), \theta - \tilde{\theta}\rangle + \frac{L}{2}\|\theta - \tilde{\theta}\|^2.
        \]
        \label{smoothness}
\end{assumption}
As we seek to privatize the iterations of our gradient descent procedure, we also need to bound the sensitivity of our descent steps. To that end, we assume that our objective function has bounded gradients.

\begin{assumption}
    We have that $ \|\nabla_\theta \mathcal{L}(\theta, x)\| \leq B \; \forall x \in \mathcal{X}, \theta \in \Theta$. 
    \label{GradientBound}
\end{assumption}

Although this assumption may seem less natural than in standard noisy gradient descent analyses such as \cite{avella2023differentially}, where the assumption is grounded in choosing classes of M-estimators with bounded influence functions, we note that for many classes of kernels, Assumption \ref{rkhsassumption} implies Assumption \ref{GradientBound}. For example, there is an equivalence between the RKHS of Matérn kernels and Sobolev spaces \citep{kanagawa2018gaussian}, such that a bounded RKHS norm in the Matérn case of a sufficiently large order necessarily has gradients bounded in $L_2$. A similar implication holds for Assumption  \ref{smoothness} -  if $k$ is chosen to be four times continuously differentiable, then $\mathcal{L}$ is necessarily twice continuously differentiable, and hence if $\Theta$ is compact, $\mathcal{L}$ is $L$-smooth for some $L$.


Finally, some of our results will require our function $f_\mathbf{x}$ to be strongly convex.
\begin{assumption}
    \label{assumption:strongconvexity}
    The function $f_\mathbf{x}$ is $\tau$-strongly convex, i.e, for all $\theta, \tilde{\theta} \in \Theta$, $f_\mathbf{x}$ satisfies
    \[
    \langle \nabla f_\mathbf{x}(\theta)- \nabla f_\mathbf{x}(\tilde{\theta}), \theta - \tilde{\theta}\rangle \geq \tau\|\theta - \tilde{\theta}\|^2.
    \]
    % Equivalently,
    % \[
    % f_\mathbf{x}(\theta) - f_\mathbf{x}(\tilde{\theta}) \geq \langle\nabla f_\mathbf{x}(\tilde{\theta}), \theta - \tilde{\theta}\rangle + \tau\|\theta - \tilde{\theta}\|^2.
    % \]
\end{assumption}

\subsection{Related work}

Using a zeroth-order Bayesian Optimization framework to perform approximate gradient descent along black box functions is a recent idea that was introduced by \cite{muller2021local}. This paper uses GIBO, which attempts to learn the gradient of the black box function by using an acquisition function that samples evaluation points so as to minimize the uncertainty about the gradient. Certain properties of GIBO were proved in \cite{wu2024behavior}, under the assumption that the black-box function lies in the RKHS generated by the kernel of the surrogate Gaussian process with which the function is modeled. They show that the algorithm converges to a region with gradient norm bounded by a constant, which is determined by properties of a kernel. We improve upon their results by showing that under certain regimes, this upper bound on the norm of the gradient vanishes and GIBO actually converges to a local solution. 

On the other hand, gradient descent algorithms are among the algorithms that have received the most attention in the Differential Privacy literature. Noisy (Stochastic) Gradient Descent algorithms are among the most widely used Differentially Private empirical risk minimizers \citep{privateSGD, privateGD2, privateGD3}. Recent works such as \cite{avella2023differentially} derive error bounds, convergence rates and statistical properties  for the results of first- and second-order noisy gradient descent algorithms. 


\section{Differentially Private Gradient Informative Bayesian Optimization}
\label{DPLBO}

\subsection{Algorithm}
Our procedure, detailed in Algorithm \ref{algorithm}, uses Bayesian Optimization to approximate the underlying function gradient by the gradient of the surrogate model, performs an appropriate clipping step, adds noise, and performs gradient descent along this direction. The acquisition function we use in this Bayesian Optimization framework is the following:
\[
\alpha(\mathbf{z}; \mathcal{D}, \theta) = \text{Tr}(\nabla k_{\mathcal{D} \cup \mathbf{z}} (\theta, \theta) \nabla^\top),
\]
where $\mathbf{z}$ is a set containing points in $\Theta$.
Intuitively, this acquisition function aims to minimize the per-step uncertainty in the posterior gradient covariance. This acquisition function can later also be seen to be a greedy optimizer of the high probability sub-optimality gap proved in Theorem \ref{thm:SubOptimGap}.
\begin{algorithm}[H]
\caption{A Differentially Private Local Bayesian Optimization Algorithm}
\begin{algorithmic}[1]
\STATE \textbf{Input:} A dataset $x \in \mathcal{X}^n$, corresponding functions $\mathcal{L}(\cdot, x_i)$, a bound $B$ from Assumption \ref{GradientBound}, a point sampling batch size $b$, and a privacy parameter $\mu$
\STATE Initialize at a non-data dependent $\theta^{(0)} \in \Theta$, initialize evaluation set $\mathcal{D}_{-1} = \emptyset$
\FOR{$t = 0, \dots, T-1$}
    \STATE Find the set of $b$ points minimizing the acquisition function, \[\mathbf{z}' = \arg\min_{\mathbf{z}: |\mathbf{z}| = b} \alpha(\mathbf{z}; \mathcal{D}_{t-1}, \theta^{(t)})\]
    \STATE Update $\mathcal{D}_{t} = \mathcal{D}_{t-1} \cup \mathbf{z}'$
    \STATE Denote the per-user surrogate posterior mean gradient at $\theta^{(t)}$ as 
    \[
    g_t^{(i)}:=\nabla k(\theta^{(t)},\mathcal{D}_t) k(\mathcal{D}_t, \mathcal{D}_t)^{-1}\mathcal{L}(\mathcal{D}_t, x_i)
    \]    
    \STATE Clip and aggregate these to get a full approximate gradient 
    \[
    g_t := \frac{1}{n} \sum_{i=1}^n g_t^{(i)}\cdot \min\{1, \frac{B}{\|g_t^{(i)}\|}\}
    \]
    \STATE Perform the approximate noisy gradient descent step 
    \[
    \theta^{(t+1)} = \theta^{(t)} - \eta_t (g_t + \frac{2B\sqrt{T}}{n\mu}w_t),
    \]
    where $w_t \overset{iid}{\sim} N(0, I_d)$
\ENDFOR
\STATE \textbf{Return} $\theta^{(T)}$
\end{algorithmic}
\label{algorithm}
\end{algorithm}

It might be difficult to verify whether Assumption \ref{GradientBound} holds in practice, but our algorithm preserves privacy even if the bound $B$ is misspecified, or even if such a bound does not exist. Under these circumstances, our algorithm can still perform well. We demonstrate the algorithm's performance in these regimes in Section \ref{examples}.

\subsection{Theoretical guarantees}
In this section we present the theoretical guarantees for the estimates obtained by Algorithm \ref{algorithm}. Our first lemma shows that the algorithm indeed satisfies $\mu$-GDP under no assumptions on the model. 
\begin{lemma}
\label{lem:privacy}
The statistic released by Algorithm \ref{algorithm} is $\mu$-GDP.
\label{privacylemma}
\end{lemma}
The detailed proof of Lemma \ref{privacylemma}, as well as the proofs of the other results presented in this section, can be found in Appendix \ref{appendix:proofs}. 


We are now ready to state Algorithm \ref{algorithm}'s first convergence guarantee.
To that extent, denote $\Psi_T = \frac{1}{T} \sum_{t=0}^{T-1} \|\nabla f_\mathbf{x}(\theta^{(t)})\|^2$, and denote $F_t := f_\mathbf{x}(\theta^{(t)}) - f_\mathbf{x}(\theta^{*})$.
The first result, Theorem \ref{thm:Goodconvergence}, is on the average squared gradient norm $\Psi_T$, and relies on results in the analysis of biased stochastic gradient descent \citep{ajalloeian2021convergencesgdbiasedgradients}.


\begin{theorem}
    \label{thm:Goodconvergence}
    Under Assumptions \ref{rkhsassumption}-\ref{GradientBound}, we have with probability at least $1-\delta$, for $\eta \leq \frac{1}{L}$,
    \begin{align*}
        \Psi_T
        \leq 
        \frac{2 F_0L}{T} 
        + C_{\mathcal{X}}^2 \frac{\sum_{t=0}^{T-1} \text{Tr} \big( \nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)}) \nabla^\top \big)}{T} 
        + 2 \mathcal{V},
    \end{align*}
    where $\mathcal{V} := M(2 +  \frac{M}{2B^2})$ for $$M:=  \frac{2B^2\sqrt{T}(4\sqrt{d} + 2\sqrt{2\log\frac{T}{\delta}} )}{n\mu}.$$

\end{theorem}
In the above, note that if $n = \Omega(\frac{\sqrt{Td} + \sqrt{T\log\frac{T}{\delta}}}{\mu})$, then $\mathcal{V} = O(\frac{B^2\sqrt{T}(\sqrt{d} + \sqrt{\log\frac{T}{\delta}})}{n\mu})$.

We can also give results about the suboptimality gap, again in probability. This theorem is a simple extension of Theorem \ref{thm:Goodconvergence}, under the addition of a strong convexity assumption.

\begin{theorem}
    \label{thm:SubOptimGap}
    Under Assumptions \ref{rkhsassumption}-\ref{assumption:strongconvexity}, we have, with probability at least $1-\delta$, for $\eta \leq \frac{1}{L}$,
    \begin{align*}
        & F_T \leq  (1 - \eta \tau)^T F_0 + r_{geom}
        \\
        % & \frac{ C_\mathcal{X}^2\eta}{2} \sum_{t=0}^{T-1} (1 - \eta \tau)^{T-1-t}  \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top) + \frac{\mathcal{V}}{ \tau},
    \end{align*}
    where 
    \begin{align*} r_{geom} &= \frac{ C_\mathcal{X}^2\eta}{2} \sum_{t=0}^{T-1} (1 - \eta \tau)^{T-1-t}  \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top)  +\frac{\mathcal{V}}{ \tau},
    \end{align*} with  $\mathcal{V}$ defined in Theorem \ref{thm:Goodconvergence}.
\end{theorem}

A discussion on the averaging trace terms that appear in Theorems \ref{thm:Goodconvergence} and \ref{thm:SubOptimGap} is in order. First, note that for $b>d$, each term in the sum equals zero \citep{wu2024behavior}. Otherwise, note that for $n$ large enough, we have $\|\theta^{(t+1)} - \theta^{(t)} \| = O(\eta B)$ with high probability. Therefore, for $\eta B$ small, and by twice continuous differentiability of $k$, we have $\text{Tr}(\nabla k_{\mathcal{D}_t}(\theta^{(t)}, \theta^{(t)})\nabla^\top) \approx \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t+1)}, \theta^{(t+1)})\nabla ^\top)$. 
Therefore, much gradient information is kept from one iteration to another, such that the average trace generally decreases as $t$ progresses. Furthermore, we note that no matter the information available about $f$ or $\mathcal{L}$, one can easily calculate $\text{Tr}(\nabla k_{\mathcal{D}_t}(\theta^{(t)}, \theta^{(t)})\nabla^\top)$ after each iteration, allowing one to gauge the magnitude of the bias error at the end of the algorithm run.

We empirically demonstrate the behavior of these bias terms in Example \ref{example:normalmodel}, where we show that the error induced by the gradient bias quickly becomes of a smaller order than the error induced by the privacy noise.  Moreover, in Theorem \ref{thm:convergenceproof} in Appendix \ref{appendix:proofs}, we include a derivation that shows that given sufficient iterations and samples, Algorithm \ref{algorithm} with initially restricted sampling can achieve any suboptimality gap level. 

Finally, we can give a result on the convergence of the parameter estimate obtained by Algorithm \ref{algorithm}. 
\begin{corollary}
    \label{thm:parameter_error}
    Under Assumptions \ref{rkhsassumption}-\ref{assumption:strongconvexity}, we have, with probability at least $1-\delta$, for $\eta \leq \frac{1}{L}$,
    \begin{align*}
        \|\theta^{(T)} - \theta^*\| \leq \sqrt{\frac{(1-\eta \tau)^T}{\tau}F_0 + \frac{r_{geom}}{\tau}},
    \end{align*}
    with $r_{geom}$ as in Theorem \ref{thm:SubOptimGap}.
\end{corollary}


\begin{remark}
    Our theory assumes that our function is globally $L$-smooth and $\tau$-strongly convex, and produces estimates of the global minimum. If these assumptions hold locally, then one may instead use the tools in this paper to show convergence to a local minimum.
\end{remark}

\subsection{Improvements with varying batch size}
Note that in Algorithm \ref{algorithm}, we have used a constant batch size $b$ when choosing new query points in every iteration. This might not be the most efficient way of using a fixed computational budget. For instance, if we are trying to estimate a gradient close to a point we have already visited in the past, it might be wise to use less than $b$ new samples in evaluating the gradient. On the other hand, if we end up at a point whose neighborhood we haven't explored in the past, we might want to use more than $b$ new samples in estimating the gradient in the uncertain surrounding. To that end, we propose a slight modification of Algorithm \ref{algorithm}, which takes this idea into account.
\begin{algorithm}[H]
\caption{Adaptation with varying batch size}
\begin{algorithmic}[1]
\STATE \textbf{Input:} A dataset $x \in \mathcal{X}^n$, corresponding functions $\mathcal{L}(\cdot, x_i)$, a bound $B$ from Assumption \ref{GradientBound}, an error tolerance $\varepsilon$, and a privacy parameter $\mu$
\STATE Initialize at a non-data dependent $\theta^{(0)} \in \Theta$, initialize evaluation set $\mathcal{D}_{-1} = \emptyset$
\FOR{$t = 0, \dots, T-1$}
    \STATE $b = \min \left \{ b : \min_{\mathbf{z}: |\mathbf{z}| = b} \alpha(\mathbf{z}; \mathcal{D}_{t-1}, \theta^{(t)}) \leq \varepsilon \right \}$
    \STATE $\mathbf{z} = \arg\min_{\mathbf{z}: |\mathbf{z}| = b} \alpha(\mathbf{z}; \mathcal{D}_{t-1}, \theta^{(t)})$
    \STATE Update $\mathcal{D}_t = \mathcal{D}_{t-1} \cup \mathbf{z}$
    \STATE Proceed with lines 6-8 in Algorithm \ref{algorithm}
\ENDFOR
\STATE \textbf{Return} $\theta^{(T)}$
\end{algorithmic}
\label{algorithm2}
\end{algorithm}
Note that the only difference between the two proposed algorithms is that in Line 4 of Algorithm \ref{algorithm2}, we choose $b$ in an adaptive way. The cost of finding this $b$ in each iteration is negligible compared to the cost of training the model, and we certainly have $b \leq d+1$ in each iteration, therefore bounding the number of function evaluations performed in the full routine by $(d+1)T$. However, it can be much smaller. By making this change, we are able to control the bias term from the previous theorems.

\begin{corollary}
    \label{thm:GoodConvergenceEps}
    Under Assumptions \ref{rkhsassumption}-\ref{GradientBound}, for a given tolerance level $\varepsilon > 0$, after running Algorithm \ref{algorithm2} for $T$ iterations, we have, with probability at least $1-\delta$, for $\eta \leq \frac{1}{L}$,
    \begin{align*}
        &\Psi_T
        \leq 
        \frac{2 F_0L}{T} 
        + \varepsilon C_{\mathcal{X}}^2
        + 2 \mathcal{V}, 
        \end{align*}
        If, additionally, we make Assumption \ref{assumption:strongconvexity}, then also,
        \begin{align*}F_T \leq (1 - \eta \tau)^TF_0 + \frac{C_\mathcal{X}^2\varepsilon}{2\tau} + \frac{\mathcal{V}}{\tau},
    \end{align*}
    where $\mathcal{V}$ is defined in Theorem \ref{thm:Goodconvergence}.

\end{corollary}

\subsection{Noisy Function Observations}
\label{section:noisy}
In the preceding sections, we have assumed that we can observe $f_\mathbf{x}(\theta)$ without noise for any $\theta \in \Theta$. There are, however, situations in which this does not hold. In reinforcement learning settings, this may be caused by inherently noisy sensors or human variation \citep{wang2020reinforcement}. In the hyperparameter tuning setting, this occurs if the training routine is also privatized. Although applying Algorithm \ref{algorithm} in this case still yields private estimates, its performance can become arbitrarily poor depending on the magnitude of the noise. 

Specifically, assume that one wants to preserve $\mu$-GDP in both the training and validation datasets. In this case, by standard GDP composition, one would have to ensure $\frac{\mu}{b\sqrt{T}}$-GDP in each individual training routine, yielding noisy model parameters for any given hyperparameter configuration and, as a result, noisy validation losses.

To reconcile this setting with our algorithms, we can model our noisy observations to be of the form
\[
\tilde{f}_\mathbf{x}(\theta^{(t)}) = f_\mathbf{x}(\theta^{(t)}) + v_t,
\]
where $v_t \overset{iid}{\sim} N(0, \sigma^2)$. Note that no matter the kernel $k$ chosen, the noise ensures that almost surely $\tilde{f}_\mathbf{x} \notin \mbox{RKHS}(k)$, and thus our usual theory does not apply. As in \cite{wu2024behavior}, we can instead proceed by assuming that $f$ is a sample from a GP with mean $\mu$ and kernel $k$.  Denoting $\mathcal{J} = \begin{bmatrix} \tilde{f}_\mathbf{x}(\boldsymbol{\theta})^\top ; & \nabla f_\mathbf{x}(\theta)^\top  \end{bmatrix}^\top$, we then have the joint distribution
\begin{align*}
    %\begin{pmatrix}
     %   \tilde{f}_\mathbf{x}(\boldsymbol{\theta}) \\
      %  \nabla f_\mathbf{x}(\theta)
   % \end{pmatrix} 
  \mathcal{J}  \sim N( & \begin{pmatrix}
        \mu(\boldsymbol{\theta}) \\
        \nabla \mu(\theta)
    \end{pmatrix},   \begin{pmatrix}
        k(\boldsymbol{\theta}, \boldsymbol{\theta}) + \sigma^2I & k(\boldsymbol{\theta}, \theta)\nabla^\top \\
        \nabla k(\theta, \boldsymbol{\theta)} & \nabla k(\theta, \theta)\nabla^\top
    \end{pmatrix}),
\end{align*}
such that Algorithm \ref{algorithm} may be adjusted by accounting for the $\sigma^2I$ term in line 6 of the algorithm. Note that by choosing $\sigma^2$ too small relative to the true noise, the algorithm will place too much trust in noisy observations when approximating the gradient, whereas by choosing $\sigma^2$ too large, the model will be conservative in approximating the gradient. 

%In Section \ref{examples}, we include an example where our observations are noisy, and demonstrate that Algorithm \ref{algorithm} still performs well relative to competing methods.



\section{Examples}
\label{examples}
We now present four numerical experiments to illustrate the behavior of our proposed algorithm and to compare it to other private algorithms and to non-private Bayesian Optimization procedures.  The first two examples are theoretical, where we examine the performance of our algorithm on models that satisfy our main assumptions, allowing us to compare the results to the ground truth. In the remaining two examples, we study hyperparameter tuning in higher dimensions, where current private Bayesian Optimization methods struggle. More detailed discussion about these examples, along with additional studies in case of noisy observations can be found in Appendix \ref{appendix:examples}.

    
\subsection{Location of the Normal model}
\label{example:normalmodel}
Let $x_1, \ldots, x_n \sim N(\theta, I)$ where $\theta = [1, 1, 1, 1, 1]^\top \in \mathbb{R}^5$. We are interested in finding a private estimate $\hat{\theta}$ of $\theta$ by noisily minimizing the negative log-likelihood (up to a constant):
\[
f_{\mathbf{x}}(\theta) = \frac{1}{2n} \sum_{i=1}^n \|x_i - \theta\|^2.
\]
Here, we choose our kernel to be the polynomial kernel of degree 2, i.e. $k(x,y) = (x^\top y + 1)^2$. By doing this, we assure that $f_{\mathbf{x}}$ is in the RKHS generated by $k$ for every $\mathbf{x}$. 

Figure \ref{fig:normal}(Top) shows that both the $\mu = 2$ and $\mu = 0.5$ private estimates move to the actual value of the Maximum Likelihood Estimate, but due to the bigger variance of the noise in the $\mu = 0.5$ case, the oscillations around the true value are bigger, as expected. The same effect can be seen on the function values as well, as in Figure \ref{fig:normal}(Middle). Finally, in Figure \ref{fig:normal}(Bottom), we can see that the size of the gradient bias is negligible compared to the size of the noise added in every iteration of the algorithm.

\subsection{Linear regression}
Here we explore the proposed algorithm on simulated data from a linear regression model and compare the results to noisy gradient descent, a widely used private ERM algorithm. To that end, let the data$\{ (x_i, y_i) \}_{i=1}^n$ be generated by the model $y_i = x_i^\top \theta + \varepsilon_i$, where $\varepsilon \overset{\text{i.i.d.}}{\sim} N(0, 1)$ and the covariates are generated by $x_i \overset{\text{i.i.d.}}{\sim} N(0, \mathbb{I}_4)$. We are minimizing the loss given by
\[
f_x(\theta) = \frac{1}{n} \sum_{i=1}^n \rho_c \left (y_i - x_i^\top \theta \right ) \cdot \min \left( 1, \frac{2}{\| x_i \| ^ 2} \right ),
\]
where $\rho_c$ is the Huber loss with tuning parameter $c$, which we set to $c=1$. By considering this loss, the gradient with respect to $\theta$ has bounded global sensitivity. 

Setting $\theta = [1, 1, 1, 1]^\top$, Figure \ref{fig:linear_regression}(Top) shows that both estimates move toward the true parameter value, but a slight lag can be noticed in the DP-GIBO(Algorithm \ref{algorithm}) estimates, which are due to bad estimates of the gradient in the first few iterations ($b = 2 < d + 1 = 5$). The same phenomena can be observed in Figure \ref{fig:linear_regression}(Bottom), where in the start, the norm of the estimated gradient is below the actual value. Nevertheless, after several iterations, both estimates perform equally well, which is expected since after some time, we have evaluated the function at enough points to be able to successfully estimate the gradient in the neighborhood of those points. Note that while non-private gradient descent converges to the true value, both of our private estimates alter around that value, which is due to the noise being added to every iteration of the algorithm.

The gradient trajectories in Figure \ref{fig:linear_regression}(Bottom) align with theoretical results of geometrically vanishing errors, as in Theorem \ref{thm:Goodconvergence}.


\subsection{Hyperparameter tuning in Gaussian Process Regression}
\label{example:GP}
In this example, we consider the problem of tuning kernel length-scales in Gaussian Process Regression. The data is generated from a 10-dimensional Gaussian Process with RBF kernel where the true data generating length scales were uniformly drawn from $[0,5]^{10}$, which is also the space over which we optimize the validation loss. We compare our private algorithm to both random search and Upper Confidence Bound Bayesian Optimization, as suggested in the previous work on Differentially Private Bayesian Optimization \citep{kusner2015differentially}. 

In Figure \ref{fig:GP_Regression}(Top) we can see that Vanilla BO struggles in $d = 10$ dimensions with only 265 function evaluations, and our method performs significantly better. It is worth noting that the two alternative methods aren't privatized, and our method preserves $\mu = 1$ level of GDP. 


\begin{remark}
    Instead of standard gradient descent updates, we use AdaGrad updates in Line 8 of Algorithm \ref{algorithm} with privatized gradients, maintaining the same level of privacy.
\end{remark}


\subsection{Hyperparameter tuning of an SVM}
We study a hyperparameter tuning problem with Kernel Support Vector Machine (SVM) trained on a MRI regression dataset \citep{dua2017uci}. We define this as an optimization problem in $d = 33$ dimensions, including 3 regularization parameters and 30 kernel length-scales. In Figure \ref{fig:SVM}, we compare the performance of DP-GIBO to the non-private (standard) version of GIBO and random search.


Both the privatized and non-privatized versions of Algorithm \ref{algorithm} outperform non-private random search on this dataset. Moreover, we can see that, as predicted by Theorem \ref{thm:SubOptimGap}, the suboptimality gap in the privatized case is constantly larger than the suboptimality gap in the non-privatized case, driven by the standard cost of differential privacy.

\begin{tcolorbox}[colframe=white!95, colback=white!95, sharp corners, boxrule=0.5mm, width=\textwidth, arc=3mm]

\begin{multicols}{2}

    % First figure
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{Pictures/Pic1_combined.png}
        \caption{\textbf{Top}: Estimates of a single coordinate of the model; \textbf{Middle}: The optimization path of the negative-log likelihood; \textbf{Bottom}: Comparison between the norm of the bias of the gradient estimate and the norm of the added noise for $\mu = 0.5$ case. }
        \label{fig:normal}
    \end{figure}

    \addtocounter{figure}{1}
    % Third figure
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{Pictures/combined_plot_SVM1.png}
        \caption{Comparison of DP-GIBO on $\mu = 1$ level of privacy to the non-private (standard) GIBO (i.e. DP-GIBO with $\mu = \infty$) and Random search.}
        \label{fig:SVM}
    \end{figure}

    \addtocounter{figure}{-2}
    % Second figure
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{Pictures/Pic2_combined.png}
        \caption{\textbf{Top}: Estimates of a single coordinate of the regression parameter; \textbf{Bottom}: Gradient of the loss function at the current iteration, on a log scale.}
        \label{fig:linear_regression}
    \end{figure}

    \addtocounter{figure}{+1}

    % Fourth figure
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{Pictures/combined_plot_d=10_FULL2.png}
        \caption{\textbf{Top}: Comparison of DP-GIBO to the Random search and Vanilla Bayesian Optimization with upper confidence bound as acquistion function; \textbf{Bottom}: Comparison of DP-GIBO for different levels of privacy.}
        \label{fig:GP_Regression}
    \end{figure}
\end{multicols}

\end{tcolorbox}


\section{Discussion}
High-dimensional black-box optimization poses significant challenges and has been an active area of research in recent years. Simultaneously, ensuring dataset privacy throughout the optimization process has become increasingly important. Previous efforts to privatize Bayesian Optimization have primarily relied on global approaches, which suffer significantly from the curse of dimensionality.

We introduced a private variant of the local Bayesian Optimization method that scales effectively with dimensionality. Under suitable conditions, we proved that our method converges to a region around the optimal parameters while preserving privacy. Moreover, our empirical results show that even when our assumptions are not fully met, our approach outperforms both random-search based methods and global Bayesian Optimization techniques.

Finally, we believe there are several avenues for further work in the direction of this paper:
\begin{itemize}
    \item Providing parallel theoretical guarantees in the case of noisy observation as discussed in Section \ref{section:noisy},
    \item As mentioned in Section \ref{section:noisy}, using standard GDP composition, each training call has to be $\frac{\mu}{b\sqrt{T}}$-GDP to guarantee overall $\mu$-GDP in Algorithm \ref{algorithm}. If the training sample size is small, this can cause the validation loss observations to be excessively noisy, and our algorithm to converge too slowly. Recent work by \cite{wang2024dp} allows for a constant training privacy cost over multiple iterations of an adaptive tuning algorithm. It may be possible to incorporate such an approach into our algorithm to yield an efficient, comprehensive framework for hyperparameter tuning while guaranteeing both training and validation differential privacy,
    \item 
    Developing private alternatives for other widely used local Bayesian Optimization methods such as TuRBO and SAASBO \cite{eriksson2019scalable, highdimBOsparse}.
\end{itemize}


\bibliography{main}
\bibliographystyle{plainnat}


\appendix
\onecolumn
\newpage
\section{Supporting Lemmas}
The following two lemmas will be used in proofs of theorems.
First, we show that, under our modeling assumptions, clipping the per-sample gradients cannot hurt us. To that end, for a vector $v$ define $\Pi_B (v) = v \cdot \min \{ 1, \frac{B}{\| v \|}\}$, a projection onto a ball of radius B around the origin, $B_B(0)$. Similar to the definitions in Algorithm \ref{algorithm}, and recalling that $\mathcal{D}$ is a set containing points in $\Theta$, define
\[
g_{\mathcal{D}}^{(i)}(\theta) := \nabla k(\theta,\mathcal{D}) k(\mathcal{D}, \mathcal{D})^{-1}\mathcal{L}(\mathcal{D}, x_i),
\]
and
\[
g_{\mathcal{D}}(\theta) = \frac{1}{n} \sum_{i=1}^n g_{\mathcal{D}}^{(i)}(\theta) \cdot \min \left \{1 , \frac{B}{\|g_{\mathcal{D}}^{(i)}(\theta)\|} \right \},
\]
our projected gradient.

\begin{lemma}

\label{lem:projection}
        Under Assumption \ref{GradientBound}, we have, for any $\theta \in \Theta$, $i \in [n]$,
        \[
        \left \| \Pi_{B}\left (g_{\mathcal{D}}^{(i)}(\theta)\right) - \nabla_{\theta}\mathcal{L}(\theta, x_i) \right \|  \leq \left \| g_\mathcal{D}^{(i)}(\theta) - \nabla_{\theta} \mathcal{L}(\theta, x_i) \right \| .
        \]
\end{lemma}


\begin{proof}
    First, note that $\Pi_B: \mathbb{R}^d \mapsto \mathbb{R}^d$ is a projection onto a closed convex set, the $L_2$ ball of radius B around the origin. 
    By definition of the projection onto a closed set, we have that $\Pi_B(x) = \arg \min_{x' \in B_B(0)} \|x - x' \|$. Now, let $y \in B_B(0)$. Since $B_B(0)$ is convex, we have that for any $0 < \gamma <1$, $z:=\gamma y + (1-\gamma)\Pi_B(x) = \Pi_B(x) + \gamma(y - \Pi_B(x)) \in B_B(0)$.  Thus, we have
    \begin{align*}
        \|x - \Pi_B(x)\|^2 \leq &\|x - z\|^2 = \|x - \Pi_B(x) - \gamma(y - \Pi_B(x))\|^2\\ =&  \|x - \Pi_B(x)\|^2 + \gamma^2 \|y - \Pi_B(x)\|^2 - 2\gamma \langle x - \Pi_B(x), y - \Pi_B(x)\rangle,
    \end{align*}
    where in the inequality we used that $\Pi_B(x)$ is the closest point in $B_B(0)$ to $x$ by definition. As a consequence, we get
    \begin{align*}
        \langle x - \Pi_B(x),\Pi_B(x) - y\rangle + \frac{\gamma}{2}\|y - \Pi_B(x)\|^2 \geq 0.
    \end{align*}
    As $0 < \gamma<1$ is arbitrary, we obtain
    \begin{align*}
        \langle x - \Pi_B(x), \Pi_B(x) - y\rangle = \lim_{\gamma \to 0^+} \langle x - \Pi_B(x),\Pi_B(x) - y\rangle + \frac{\gamma}{2}\|y - \Pi_B(x)\|^2 \geq 0
    \end{align*}
    for all $y \in B_B(0)$.
    Using this, we can derive
    \begin{align*}
        \left \| \nabla\mu_\mathcal{D}^{(i)}(\theta) - \nabla \mathcal{L}(\theta, x_i) \right \|^2 &=   \left \| \nabla\mu_\mathcal{D}^{(i)}(\theta) - \Pi_{B}(\nabla \mu_{\mathcal{D}}^{(i)}(\theta)) + \Pi_{B}(\nabla \mu_{\mathcal{D}}^{(i)}(\theta)) - \nabla \mathcal{L}(\theta, x_i) \right \|^2 \\
        &= \left \| \nabla\mu_\mathcal{D}^{(i)}(\theta) - \Pi_{B}(\nabla \mu_{\mathcal{D}}^{(i)}(\theta)) \right \|^2 + \left \| \Pi_{B}(\nabla \mu_{\mathcal{D}}^{(i)}(\theta)) - \nabla \mathcal{L}( \theta, x_i) \right \|^2 +\\
        & \quad + 2 \left < \nabla\mu_\mathcal{D}^{(i)}(\theta) - \Pi_{B}(\nabla \mu_{\mathcal{D}}^{(i)}(\theta)), \ \Pi_{B}(\nabla \mu_{\mathcal{D}}^{(i)}(\theta)) - \nabla \mathcal{L}(\theta, x_i)\right > \\
        & \geq \left \| \Pi_{B}(\nabla \mu_{\mathcal{D}}^{(i)}(\theta)) - \nabla \mathcal{L}(\theta, x_i) \right \|^2,
    \end{align*}
    where the final inequality follows from the fact that the first and last terms on the right hand side are positive.
\end{proof}
Next, we will give an upper bound on the bias of our estimated gradient. 
Using the previous result and Lemma 1 from \cite{wu2024behavior}, we derive the following:
\begin{lemma}
\label{lem:bias_gradient}
    Under Assumptions \ref{rkhsassumption} and \ref{GradientBound}, we have that,
    \[
    \left \| g_{\mathcal{D}}(\theta) - \nabla f_\mathbf{x}(\theta) \right \| \leq C_\mathcal{X} \sqrt{\text{Tr}(\nabla k_{\mathcal{D}} (\theta, \theta)\nabla^\top)}
    \]
    \label{BiasLemma}
\end{lemma}

\begin{proof}
    We start by using the triangle inequality to go to the user-level gradients, then use Lemma \ref{lem:projection} to remove the projection operator, and lastly apply Lemma 1. from \cite{wu2024behavior}:
    \begin{align*}
        \left \| g_{\mathcal{D}}(\theta) - \nabla f_x(\theta) \right \| &= \left \| \frac{1}{n} \sum_{i=1}^n \Pi_B \left ( g_{\mathcal{D}}^{(i)}(\theta) \right ) - \frac{1}{n} \sum_{i=1}^n \nabla \mathcal{L}(\theta, x_i)  \right \| \\
        & \leq \frac{1}{n} \sum_{i=1}^n \left \| \Pi_B \left ( g_{\mathcal{D}}^{(i)}(\theta) \right ) - \nabla \mathcal{L}(\theta, x_i)  \right \| \\
        & \leq \frac{1}{n} \sum_{i=1}^n \left \| g_{\mathcal{D}}^{(i)}(\theta) - \nabla \mathcal{L}(\theta, x_i)  \right \| \\
        & \leq \frac{1}{n} \sum_{i=1}^n C_\mathcal{X} \sqrt{\text{Tr}(\nabla k_{\mathcal{D}} (\theta, \theta)\nabla^\top)} \\
        &= C_\mathcal{X} \sqrt{\text{Tr}(\nabla k_{\mathcal{D}} (\theta, \theta)\nabla^\top)}.
    \end{align*}
    
\end{proof}



\begin{lemma}
    Let $w_1, .., w_T \overset{iid}{\sim} N(0, I_d)$. Then, we have, with probability at least $1-\delta$,
    \[
    \max_{t\leq T} \|w_t\|_2 \leq 4\sqrt{d} + 2\sqrt{2\log\frac{T}{\delta}}
    \]
    \label{subGaussianProof}
\end{lemma}

\begin{proof}
    The proof follows from \cite{rigollet2023high}'s Theorem 1.19 and an application of the union bound.
\end{proof}






\begin{lemma}
    Under Assumptions \ref{rkhsassumption}-\ref{assumption:strongconvexity}, there exists a radius $r<\infty$ such that with probability at least $1-\xi$, if in Algorithm \ref{algorithm}, $\theta^{(0)} \in B_r(\theta^*)$, then $\theta^{(t)} \in B_r(\theta^*) \; \forall t \leq T$.
    \label{balllemma}
\end{lemma}

\begin{proof}
    First, we condition on
    \begin{align}
    \max_{t\leq T} \|w_t\| \leq 4\sqrt{d} + 2\sqrt{2\log\frac{T}{\xi}},
    \end{align}
    which, by Lemma \ref{subGaussianProof}, has probability at least $1-\xi$ of occurring. Then, by Lemma \ref{DescentLemma}, we have that $f(\theta^{(t+1)}) \leq f(\theta^{(t)})$ if \begin{align}
        \|\nabla f(\theta^{(t)})\|  \geq\sqrt{C_\mathcal{X}^2\text{Tr}(\nabla k_{\mathcal{D}_t}(\theta^{(t)}, \theta^{(t)})\nabla^\top) + \frac{2}{\eta}\mathcal{V}}.
    \end{align}
    By strong convexity and Cauchy-Schwarz, this holds if 
    \[
    \|\theta^{(t)} - \theta^* \| \geq r_{total} := \frac{1}{\tau}\sqrt{C_\mathcal{X}^2E_{d,k}(b) + \frac{2}{\eta}\mathcal{V}},
    \]
    where $E_{d,k}(b) = \min_{\mathbf{z}: |\mathbf{z}| = b} \text{Tr}(\nabla k_{\mathbf{z}}(0, 0)\nabla^\top)$.
    So in this case, $\|\theta^{(t+1)} - \theta^*\| \leq \|\theta^{(t)} - \theta^*\|$. On the other hand, if $\|\theta^{(t)} - \theta^*\| < r_{total}$, then we have 
    \begin{align*}
    \|\theta^{(t+1)} - \theta^*\| &\leq \|\theta^{(t+1)} - \theta^{(t)}\| + \|\theta^{(t)} - \theta^*\| \leq r_{total} + \eta (\|g_t\| +  \frac{2B\sqrt{T}}{n\mu}\|w_t\|) \\
    &\leq r:=  r_{total} + \eta B\bigl(1 + \frac{2\sqrt{T}}{n\mu}(4\sqrt{d} + 2\sqrt{2\log\frac{T}{\xi}})\bigr)
    \end{align*}
    Thus, if $\theta^{(0)} \in B_r(\theta^*)$, then w.p. at least $1-\xi$, $\theta^{(t)} \in B_r(\theta^*)$ $\forall t \leq T$.
\end{proof}


\newpage
\section{Proofs of Main Results}
\label{appendix:proofs}
% \begin{lemma}[Privacy of Algorithm 1]
%     The statistic released by Algorithm 1 is $\mu$-GDP.
% \end{lemma}
\subsection*{Proof of Lemma \ref{lem:privacy}}
\begin{proof}
    The result follows a standard argument that combines guarantees of the Gaussian mechanism, post-processing and composition.  Let $\mu > 0$ be the desired level of privacy. We will first prove that each iteration of Algorithm \ref{algorithm} is $\frac{\mu}{\sqrt{T}}$-GDP if we consider $\theta^{(t)}$ and $\mathcal{D}$. To that end, take two datasets $x$ and $x'$ such that $d_H(x, x') = 1$. Without loss of generality, we can assume that $x_1 \neq x_1'$ and $x_i = x_i'$ for all $2 \leq i \leq n$. Fix $t > 0$, a current iteration of the algorithm. Let 
    \[
    g_t^{(i)} := \nabla k(\theta^{(t)}, \mathcal{D})k(\mathcal{D}, \mathcal{D})^{-1} \mathcal{L}(\mathcal{D}, x_i),
    \]
    and 
    \[
    \tilde{g}_t^{(i)} := \nabla k(\theta^{(t)}, \mathcal{D})k(\mathcal{D}, \mathcal{D})^{-1} \mathcal{L}(\mathcal{D}, x_i').
    \]
    Our estimated gradients at the $t$-th iteration, for both datasets are as follows
    \[
    g_t =\frac{1}{n} \sum_{i = 1}^n g_t^{(i)} \cdot \min \left \{ 1, \frac{B}{\|g_t^{(i)}\|} \right \},
    \]
    and 
    \[
    \Tilde{g}_t = \frac{1}{n} \sum_{i = 1}^n \Tilde{g}_t^{(i)} \cdot \min \left \{ 1, \frac{B}{\|\Tilde{g}_t^{(i)}\|} \right \}.
    \]
    Hence, by noting that $g_t^{(i)} = \Tilde{g}_t^{(i)}$ for all $2 \leq i \leq n$, we derive that the global sensitivity of the estimated gradient at time $t$ is
    \begin{align*}
        \|g_t - \Tilde{g}_t \| &= \left \| \frac{1}{n}\sum_{i = 1}^n g_t^{(i)} \cdot \min \left \{ 1, \frac{B}{\|g_t^{(i)}\|} \right \} - \frac{1}{n} \sum_{i = 1}^n \Tilde{g}_t^{(i)} \cdot \min \left \{ 1, \frac{B}{\|\Tilde{g}_t^{(i)}\|} \right \} \right \| \\
        &= \frac{1}{n} \left \| g_t^{(1)} \cdot \min \left \{ 1, \frac{B}{\|g_t^{(1)}\|} \right \} -  \Tilde{g}_t^{(1)} \cdot \min \left \{ 1, \frac{B}{\|\Tilde{g}_t^{(1)}\|} \right \}  \right \| \\
        &\leq \frac{1}{n} \left ( \left \| g_t^{(i)} \cdot \min \left \{ 1, \frac{B}{\|g_t^{(i)}\|} \right \} \right \| + \left \| \Tilde{g}_t^{(1)} \cdot \min \left \{ 1, \frac{B}{\|\Tilde{g}_t^{(1)}\|} \right \} \right \| \right ) \\
        &\leq \frac{1}{n} (B + B) = \frac{2B}{n}.
    \end{align*}
    Hence, since we are adding $\frac{2B\sqrt{T}}{n\mu}w_t$, where $\ w_t \overset{iid}{\sim} N(0, I_d)$, noise, by Theorem \ref{gaussianmechanism}, every iteration of the algorithm is $\frac{\mu}{\sqrt{T}}$-GDP. Finally, since by post-processing [Proposition 4 in \cite{dong2022gaussian}], our whole algorithm is a T-fold composition of $\frac{\mu}{\sqrt{T}}$-GDP mechanisms, we conclude that Algorithm \ref{algorithm} is $\mu$-GDP by Corollary \ref{composition}.
\end{proof}








\subsection*{Proof of Theorem \ref{thm:Goodconvergence}}

We will need the following auxiliary lemma to prove this result


\begin{lemma}
    With probability at least $1-\delta$, we have, for each $t\leq T$, and conditional on the sample path and evaluated points up to time $t-1$, and assuming $\eta \leq \frac{1}{L}$, that
    \[
    f(\theta^{(t+1)}) \leq  f(\theta^{(t)}) - \frac{\eta}{2} \| \nabla f(\theta^{(t)})\|^2 + \frac{\eta}{2}\left(C_\mathcal{X}^2 \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top)  \right) + \eta \mathcal{V},
    \]
    where $\mathcal{V} \leq  \frac{2B^2\sqrt{T}(4\sqrt{d} + 2\sqrt{2\log\frac{T}{\delta}} )}{n\mu}(2 +   \frac{2\sqrt{T}(4\sqrt{d} + 2\sqrt{2\log\frac{T}{\delta}} )}{n\mu})$.
    \label{DescentLemma}
\end{lemma}
\begin{proof}
    We have by L-smoothness of the function and Lemma \ref{subGaussianProof}, that with probability at least $1-\delta$, simultaneously for all $t \leq T$,
    \begin{align*}
        f(\theta^{(t+1)})  &\leq f(\theta^{(t)}) + \langle \nabla f(\theta^{(t)}), \theta^{(t+1)} - \theta^{(t)}\rangle + \frac{L}{2}\|\theta^{(t+1)} - \theta^{(t)}\|^2 \\
        &\leq f(\theta^{(t)}) - \eta \langle \nabla f(\theta^{(t)}), g_{\mathcal{D}_t}(\theta^{(t)}) + \frac{2B\sqrt{T}}{n\mu}w_t\rangle + \frac{L\eta^2}{2}\left(\|g_{\mathcal{D}_t}(\theta^{(t)}) \|^2 + \left \|\frac{2B\sqrt{T}}{n\mu}w_t \right \|^2 + \frac{4B\sqrt{T}}{n\mu}\langle g_{\mathcal{D}_t}(\theta^{(t)}),w_t \rangle  \right) \\
        &\leq f(\theta^{(t)}) - \eta \langle \nabla f(\theta^{(t)}), g_{\mathcal{D}_t}(\theta^{(t)}) \rangle + \eta \frac{2B^2\sqrt{T}(4\sqrt{d} + 2\sqrt{2\log\frac{T}{\delta}} )}{n\mu} + \\
        &\quad + \frac{L \eta^2}{2} \left(\|g_{\mathcal{D}_t}(\theta^{(t)})\|^2 +\frac{4B^2T(4\sqrt{d} + 2\sqrt{2\log\frac{T}{\delta}})^2}{n^2\mu^2} + \frac{4B^2\sqrt{T}(4\sqrt{d} + 2\sqrt{2\log\frac{T}{\delta}})}{n\mu}\right).
    \end{align*}
    Now, using that $\eta \leq \frac{1}{L}$, we can denote $M:=  \frac{2B^2\sqrt{T}(4\sqrt{d} + 2\sqrt{2\log\frac{T}{\delta}} )}{n\mu}$, and $\mathcal{V} := M(2 +  \frac{M}{2B^2})$. Then, denoting the bias $b_t := g_{\mathcal{D}_t} - \nabla f$, we get the following bound on the right hand side of the last display
    \begin{align*}
         &f(\theta^{(t)}) - \eta \langle \nabla f(\theta^{(t)}), g_{\mathcal{D}_t}(\theta^{(t)})\rangle + \frac{L \eta^2}{2} \|g_{\mathcal{D}_t}(\theta^{(t)})\|^2 + \eta \mathcal{V} \\
        \leq\,& f(\theta^{(t)}) - \eta \langle \nabla f(\theta^{(t)}), g_{\mathcal{D}_t}(\theta^{(t)})\rangle + \frac{\eta}{2}\|g_{\mathcal{D}_t}(\theta^{(t)}) \|^2 + \eta \mathcal{V} \\
        = \, &f(\theta^{(t)}) - \eta \| \nabla f(\theta^{(t)})\|^2 - \eta \langle \nabla f(\theta^{(t)}), b_t(\theta^{(t)})\rangle + \frac{\eta}{2} \| \nabla f(\theta^{(t)}) + b_t(\theta^{(t)}) \|^2 + \eta \mathcal{V} \\
        \leq \,&f(\theta^{(t)}) - \eta \| \nabla f(\theta^{(t)})\|^2 - \eta \langle \nabla f(\theta^{(t)}), b_t(\theta^{(t)})\rangle + \frac{\eta}{2} \| \nabla f(\theta^{(t)}) \|^2 + \frac{\eta}{2} \|b_t(\theta^{(t)})\|^2 + \eta \langle \nabla f(\theta^{(t)}), b_t(\theta^{(t)})\rangle + \eta \mathcal{V} \\
        \leq\,& f(\theta^{(t)}) - \frac{\eta}{2} \| \nabla f(\theta^{(t)})\|^2 
        + \frac{\eta}{2}\left(C_\mathcal{X}^2 \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top)  \right) + \eta \mathcal{V}.
        \end{align*}
\end{proof}
We are now ready to prove the theorem.
\begin{proof}
    As a direct result of Lemma \ref{DescentLemma}, we have that, with probability at least $1-\delta$, for each $t \leq T$,
    \begin{align*}
        \| \nabla f(\theta^{(t)})\|^2 \leq 2\frac{F_t - F_{t+1}}{\eta} + C_\mathcal{X}^2 \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top)+ 2\mathcal{V}.
    \end{align*}
    Then, taking the sum over $t$ and dividing by $T$, we get
    \begin{align*}
        \frac{1}{T} \sum_{t=0}^{T-1} \| \nabla f(\theta^{(t)})\|^2 &\leq 2\frac{F_0 - F_T}{\eta T} + C_{\mathcal{X}}^2 \frac{\sum_{t=0}^{T-1}\text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top) }{T} + 2 \mathcal{V} \\
        & \leq 2\frac{F_0}{\eta T} + C_{\mathcal{X}}^2 \frac{\sum_{t=0}^{T-1}\text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top) }{T} + 2\mathcal{V}.
    \end{align*}
\end{proof}

\subsection*{Proof of Theorem \ref{thm:SubOptimGap}}


\begin{proof}
    By Lemma \ref{DescentLemma}, we have
    \begin{align*}
    F_{t+1} \leq F_{t} - \frac{\eta}{2}\| \nabla f(\theta^{(t)})\|^2 + \frac{\eta}{2}\left(C_\mathcal{X}^2 \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top)  \right) + \eta \mathcal{V}.
    \end{align*}
    Now, we can use strong convexity to get,
    \begin{align}
        \leq F_{t} -  \eta \tau F_t + \frac{\eta}{2}\left(C_\mathcal{X}^2 \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top)  \right) + \eta \mathcal{V}\\
        = F_t(1 - \eta \tau) + \frac{\eta}{2}\left(C_\mathcal{X}^2 \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top)  \right) + \eta \mathcal{V}.
    \end{align}
    We can unroll this recursion to retrieve
    \begin{align*}
    F_{T} & \leq (1 - \eta \tau)^T F_0 + \frac{ C_\mathcal{X}^2\eta}{2} \sum_{t=0}^{T-1} (1 - \eta \tau)^{T-1-t}  \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top) + \eta \mathcal{V}\sum_{t=0}^{T-1}(1-\eta\tau)^t\\
    & \leq (1 - \eta \tau)^T F_0 + \frac{ C_\mathcal{X}^2\eta}{2} \sum_{t=0}^{T-1} (1 - \eta \tau)^{T-1-t}  \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top) + \frac{\mathcal{V}}{\tau},
   \end{align*}
    where we used an infinite geometric series to bound the last term. 
\end{proof}

\subsection*{Proof of Corollary \ref{thm:parameter_error}}
\begin{proof}
    The inequality 
    \[
    \|\theta^{(T)} - \theta^* \| \leq \sqrt{\frac{(1-\eta \tau)^T}{\tau} F_0 + \frac{r_{geom}}{\tau}}
    \]
    follows immediately from Theorem \ref{thm:SubOptimGap} and strong convexity of $f$. 
\end{proof}

\subsection*{Proof of Corollary \ref{thm:GoodConvergenceEps}}
  
\begin{proof}
    This follows immediately from Theorems \ref{thm:Goodconvergence} and \ref{thm:SubOptimGap} and noting that by the construction of sampled points in every iteration, we have that 
    \[
        \text{Tr}(\nabla k_{\mathcal{D}_t} (\theta^{(t)}, \theta^{(t)})\nabla^\top) \leq \varepsilon. 
    \]
\end{proof}
\subsection*{Convergence of $F_T$}
\begin{theorem}
\label{thm:convergenceproof}
        Under Assumptions \ref{rkhsassumption}-\ref{assumption:strongconvexity}, and if $k$ is additionally stationary, then, for any $\varepsilon>0$, there exists a finite set $P_k(\varepsilon, r)$ such that by changing the policy search in line 4 of the algorithm to initially only sample points $z \in P_{k}(\varepsilon,r)$,  and if $T = \Omega(d^{d + 1}  + \log\frac{dC_\mathcal{X}^2}{\varepsilon}+ \log\frac{F_0}{\varepsilon})$ and $n = \Omega(\frac{B^2\sqrt{T}(\sqrt{d} + \sqrt{\log\frac{T}{\xi}}}{\mu\varepsilon})$, then, with probability at least $1-\xi$, Algorithm \ref{algorithm} achieves
    \[
    F_T< \varepsilon.
    \]
    \label{convergencetozerothm2}
\end{theorem}
\begin{proof}
    We again condition on the event in Lemma \ref{subGaussianProof}. Let $\mathcal{D}$ be some minimal size set of points such that $\text{Tr}(\nabla k_\mathcal{D}(0, 0)\nabla^\top) <\frac{\varepsilon'}{2}$. Let $l := |\mathcal{D}| \leq d+1$. Then by twice continuous differentiability of $k$, there exists $ \delta>0$ such that, if $\|\theta\|<\delta$, then $\text{Tr}(\nabla k_{\mathcal{D}}(\theta, \theta)\nabla^\top)<\varepsilon'$. Now consider a minimal cover $Q_k(\varepsilon',r) = \{Q_m: m =1, ..., M\}$ of $B_{2r}(\theta^{(0)})$ by balls of size $\delta$, where $r$ is as in Lemma \ref{balllemma}, and let $c_i$ denote the centers of $Q_i$.

    An upper bound on \( M \) is \( \left(\frac{r\sqrt{d}}{\delta}\right)^d \). To see this, note that the largest \( d \)-cube contained in \( B_\delta(\cdot) \) has sides of length \( \frac{2\delta}{\sqrt{d}} \). At the same time, the smallest \( d \)-cube that entirely contains \( B_{2r}(\cdot) \) has sides of length \( 4r \). Thus, 

    \[
    \left(\frac{4r}{\frac{2\delta}{\sqrt{d}}}\right)^d = \left(\frac{2r\sqrt{d}}{\delta}\right)^d
    \]
    
    smaller cubes suffice to fully cover the larger cube, and hence at most that many radius \(\delta\) \( d \)-balls are required to fully cover the radius \(2r\) \( d \)-ball.

    
    Now, let $\mathcal{D}_i := \mathcal{D} + c_i$, which is the set of points in $\mathcal{D}$ shifted by $c_i$. Finally, let $P_k(\varepsilon',r) = \cup_{i=1}^M \mathcal{D}_i$, which has at most $lM$ points. By first sampling points only from $P_k(\varepsilon',r)$, which will be done in at most $lM$ iterations, we guarantee that $\text{Tr}(\nabla k_{\mathcal{D}_t}(\theta^{(t)}, \theta^{(t)})\nabla^\top) <\varepsilon'$ for all $t>lM$. Furthermore, for the first $lM$ iterations, the trace is bounded by $W:= \text{Tr}(\nabla k(0,0)\nabla^\top) = O(d)$. 
    
    Thus, for $T> T_{\varepsilon,d} := (d+1)(\frac{2r\sqrt{d}}{\delta})^d$, we have
    \[
    \frac{C_\mathcal{X}^2\eta}{2}\sum_{t=0}^{T-1}(1- \eta \tau)^{T-1-t}\text{Tr}(\nabla k_{\mathcal{D}_t}(\theta^{(t)}, \theta^{(t)})\nabla^\top) \leq \frac{C_\mathcal{X}^2\eta}{2}(W\sum_{t=0}^{T_{\varepsilon,d}}(1-\eta\tau)^{T-1-t} + \varepsilon'\sum_{t = T_{\varepsilon,d} + 1}^{T-1} (1 - \eta\tau)^{T-1-t}). \]
    Taking $\varepsilon' = \frac{\varepsilon\tau\eta}{3\eta C_\mathcal{X}^2}$, we see that the second sum, by bounding it by an infinite geometric sum, is always smaller than $\varepsilon/6$. Moreover, the first sum is smaller than $\varepsilon/6$ if $T \geq T_1 :=  T_{\varepsilon, d} + 1 + \frac{\log(\frac{\varepsilon \tau}{3C_\mathcal{X}^2W})}{\log(1  - \eta\tau)}$.

    For the exponential decay term, we need $(1 - \eta\tau)^TF_0 \leq \frac{\varepsilon}{3}$, for which it suffices that $T> T_2:= \frac{\log\frac{3F_0}{\varepsilon}}{\log\frac{1}{1-\tau\eta}}$.

    Finally, for the third term, we require $\frac{\mathcal{V}}{\tau} \leq \frac{\varepsilon}{3}$. This is achieved if $n>\frac{18B^2\sqrt{T}(4\sqrt{d} + \sqrt{2\log\frac{T}{\xi}})}{\tau\mu\varepsilon}$.

    Thus, it suffices to have $T = \Omega(d^{d + 1}  + \log\frac{dC_\mathcal{X}^2}{\varepsilon}+ \log\frac{F_0}{\varepsilon})$, and $n = \Omega(\frac{B^2\sqrt{T}(\sqrt{d} + \sqrt{\log\frac{T}{\xi}}}{\mu\varepsilon})$, to achieve $F_T \leq \varepsilon$.

    
    
\end{proof}

\newpage
\section{Examples}
\label{appendix:examples}
\subsection{Location of the Normal model}
We ran Algorithm \ref{algorithm} twice to obtain two estimates, one with privacy level $\mu = 0.5$ and the other $\mu = 2$ private. In both runs, we set $B = 1$, $b = 3$, $n = 50$, and we ran the algorithm for $T = 150$ iterations. We started the optimization from $\theta^{(0)} = \mathbf{0}$.
\subsection{Linear regression}
For this example, we used $B = 1$ (true value), $n = 100$, $b = 2$, and we ran the optimization for $T = 100$ iterations. Setting $\theta = [1, 1, 1, 1]^\top$, Figure \ref{fig:linear_regression} compares the performance of the proposed algorithm (DP-GIBO) to the standard noisy gradient descent (DP-GD), where both are obtaining $\mu = 1$ private estimates. We started the optimization from $\theta^{(0)} = \mathbf{0}$ and used the Radial Basis Function kernel. Note that in this example the loss function is not in the RKHS generated by the RBF kernel.
\subsection{Hyperparameter tuning in Gaussian Process
Regression}
Throughout this example, we used RBF kernel with batch size $b = 11$, we used $n=4500$ datapoints, set $B = 1$, and ran our algorithm for $T = 25$ iterations, resulting in 265 function evaluations. For all of the methods, we do 10 independent replications, with random restarts inside the optimization area. Finally, instead of doing standard descent update, we have used AdaGrad updates with our privatized estimate of the gradient.
\subsection{Hyperparameter tuning of an SVM}
We randomly choose 5000 training and 5000 validation points from the "CT slice" UCI dataset \citep{dua2017uci}. We then randomly choose 30 features and perform SVM regression on the training data. We then compute the validation loss (mean squared error on the validation data), which is the value we wish to minimize. The log length-scales were optimized over interval $[-2, 2]$, while regularization parameters \texttt{epsilon}, \texttt{C} and \texttt{gamma} were optimized over $[0.01, 1.0]$, $[0.1, 3.0]$ and $[0.01, 5.0]$ respectively. We performed 5 independent replications, where all methods were initialized uniformly at random inside the optimization domain. As in the previous example, we used AdaGrad updates throughout the optimization, set $B = 1$, and used batch size $b = 33 + 1 = 34$ for DP-GIBO.
\subsection{Noisy observations}
\label{example:noise}
We once again consider the Gaussian Process Regression setup from Example \ref{example:GP}. The only difference here is that we do not observe the actual validation losses \( f \) but instead observe \( f + \lambda Z \), where \( Z \sim N(0, 1) \) and we set \( \lambda = 0.01 \). We then proceed to privately (\(\mu=1\)) find the best hyperparameter using Algorithm \ref{algorithm}, adapted for the noisy case as described in Section \ref{section:noisy}.  

We run the procedure three times: first with the correct estimate of noise (\(\sigma = 0.01\)), then with an overestimate (\(\sigma = 1.0\)), and finally with an underestimate (\(\sigma = 0.0001\)). As shown in Figure \ref{fig:noiseGP}, the correct choice of the noise parameter results in the best average performance. The \(\sigma = 1.0\) setting makes our estimate conservative but still viable, whereas underestimating the noise parameter leads to worse performance than a random search.


\begin{figure}[H]
    \centering
    \includegraphics[width=.9\textwidth]{Pictures/noisy_pic.png}
    \caption{\textbf{Left:} We used the correct variance of the noise; \textbf{Middle:} We overestimated the variance of the noise; \textbf{Right:} We underestimated the variance of the noise}
    \label{fig:noiseGP}
\end{figure}

\end{document}



