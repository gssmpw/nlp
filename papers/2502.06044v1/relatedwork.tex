\section{Related work}
Using a zeroth-order Bayesian Optimization framework to perform approximate gradient descent along black box functions is a recent idea that was introduced by \cite{muller2021local}. This paper uses GIBO, which attempts to learn the gradient of the black box function by using an acquisition function that samples evaluation points so as to minimize the uncertainty about the gradient. Certain properties of GIBO were proved in \cite{wu2024behavior}, under the assumption that the black-box function lies in the RKHS generated by the kernel of the surrogate Gaussian process with which the function is modeled. They show that the algorithm converges to a region with gradient norm bounded by a constant, which is determined by properties of a kernel. We improve upon their results by showing that under certain regimes, this upper bound on the norm of the gradient vanishes and GIBO actually converges to a local solution. 

On the other hand, gradient descent algorithms are among the algorithms that have received the most attention in the Differential Privacy literature. Noisy (Stochastic) Gradient Descent algorithms are among the most widely used Differentially Private empirical risk minimizers \citep{privateSGD, privateGD2, privateGD3}. Recent works such as \cite{avella2023differentially} derive error bounds, convergence rates and statistical properties  for the results of first- and second-order noisy gradient descent algorithms.