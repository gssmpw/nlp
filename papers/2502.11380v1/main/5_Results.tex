% \section{Results and Analysis}
% \label{sec: results}
% In this section, we first build the conceptual space based on the minimum connectivity mentioned in Section~\ref{Sec: CS}. Then we evaluate the space in three scenarios.

% \subsection{Graph Construction}
% \label{section:GC}
% \paragraph{The choice of $K$.}
% To guarantee the least edges while keeping the graph connected, we extract edges of top $K$ ratio where the weights on the edges are obtained by cosine similarity. We incrementally increase the value of $K$ while calculating the number of connected components (CC), as illustrated in the Figure~\ref{fig: K_ratio}. When the log number of CC reaches zero, the graph first becomes connected. In our experiments, we chose 0.002 as $K$, when \textit{both} models first becomes connected rather than different values of $K$ for different models. This guarantees the same edges for both models to keep the comparison fair.

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figs/NCC_K.pdf}
%     \caption{Log number of connected components with the increase of Top K ratio for Llama2-7B and Llama2-70B.}
%     \label{fig: K_ratio}
% \end{figure}

\section{Results and Analysis}
\label{sec:results}
In this section, we first construct the conceptual space based on the minimum connectivity approach described in Section~\ref{Sec: CS}. We then evaluate the space in three distinct scenarios.

\subsection{Graph Construction}
\label{section:GC}
\paragraph{Choice of $K$.}
To ensure the graph is minimally connected, we extract the top $K$ ratio of edges, where the edge weights are determined by cosine similarity. We incrementally increase the value of $K$ while monitoring the number of connected components (CC), as shown in Figure~\ref{fig:K_ratio}. The graph first becomes connected when the log of the number of CC reaches zero. In our experiments, we selected $K=0.002$, at which point both models become connected. This choice ensures that the same number of edges are used for both models, making the comparison fair.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/NCC_K.pdf}
    \caption{Logarithm of the number of connected components as the top $K$ ratio increases for Llama2-7B and Llama2-70B. A value of zero indicates a fully connected network, while the dotted line marks the first ratio at which both models become connected.}
    \label{fig:K_ratio}
\end{figure}


% \paragraph{Global Statistics.}
% We obtain the statistics of conceptual spaces for both models, as listed in Table~\ref{tab:statistics}. These are divided into three parts. \textbf{Basic} one includes the basic connections and information on nodes and edges, such as the number of nodes (\#Nodes) and edges(\#Edges), and the average (Avg. Degree) and standardization derivation (Std. Degree) of degrees. \textbf{Weighted} version calculates the average and std of weighted degree (also called Strength or Traffic). We also consider the equivalent threshold, the least value of weight in the final graph $G$. The last part considers statistics related to \textbf{small-world} effects. Global clustering coefficient (GCC) computes graph transitivity, the fraction of actual triangle connections in all possible triangles present in G. Meanwhile, the local version of GCC calculates the fraction of actual connections in possible ones within the neighbor nodes for a specific node. We obtain the average one (ALCC) across all the nodes. And the diameter of a network is the length of the longest path among any pair of two nodes. The smaller the diameter or the larger the other two indicators are, the small-word effects are intense. We also calculate the averaged shortest path length (ASPL) for both models.

% Llama2-7B and Llama2-70B have the similar basic configurations of the graph due to the the constraint of the same edges. The averaged degree is 64, indicating one node has 64 neighbors on average. This moderate coverage is rather rational. Note that 7B has a more flat distribution with a larger std. As for weighted version, 7B has a smaller and concentrated degrees, which is slightly different from unweighted one. Figure~\ref{fig:degree_dist} in Appendix~\ref{app: dd} shows the distribution of the overall degree. It shows the 7B has a more long-tailed distribution, indicating a less number of high-degree nodes (also called central nodes). They both shows a second-order Zipf's Law, which is natural for a distribution. % 有些不是很清楚

% LLMs show a strong small-world clustering effect, indicated by large GCC and ALCC and a short diameter. This is compared with a random network with the same number of edges. The first one has random values in embeddings with 0.0032 of GCC and ALCC; the second one has a random connection with 0.0020 of GCC and ALCC. These are much lower than LLM-generated network. Note the number of 6 is somewhat consistent with the theory of Six Degrees of Separation~\cite{Milgram1967small}, which has been widely verified in social networks~\cite{Watts1998collective}, website pages~\cite{Albert1999internet} and so on.

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Statistics} & \textbf{Llama2-7B} & \textbf{Llama2-70B} \\
\midrule
\multicolumn{3}{c}{\textbf{Basic}} \\  
\cmidrule(lr){1-3}
\#Nodes & 32,000 & 32,000 \\
\#Edges & 1,024,000 & 1,024,000 \\
Avg. Degree & 64 & 64 \\
Std. Degree & 68.39 & 58.96 \\
\midrule
\multicolumn{3}{c}{\textbf{Weighted}} \\  
\cmidrule(lr){1-3}
Avg. Degree\_W & 8.76 & 12.23 \\
Std. Degree\_W & 13.78 & 14.02 \\
Threshold & 0.095 & 0.147 \\
\midrule
\multicolumn{3}{c}{\textbf{Small-world}} \\  
\cmidrule(lr){1-3}
GCC ($\uparrow$) & 0.325 & 0.215 \\
ALCC ($\uparrow$) & 0.183 & 0.174 \\
Diameter ($\downarrow$) & 6 & 6 \\
ASPL ($\downarrow$) & 3.392 & 3.353 \\
\bottomrule
\end{tabular}
\caption{Statistics for Llama2-7B and Llama2-70B. In the ``Small-world'' section, GCC refers to global clustering coefficient, ALCC to average local clustering coefficient, and Diameter and ASPL refer to the longest and average shortest path lengths, respectively.}
\label{tab:statistics}
\end{table}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figs/models_map_ALL.pdf}
    \caption{Shortest path lengths among semantic groups for Llama2-7B (left) and Llama2-70B (right).}
    \label{fig:shortest_path}
\end{figure*}

\paragraph{Global Statistics.}
We present the statistics of the conceptual spaces for both models in Table~\ref{tab:statistics}, divided into three parts. The \textbf{Basic} section includes the number of nodes (\#Nodes), edges (\#Edges), and the average (Avg. Degree) and standard deviation (Std. Degree) of degrees. The \textbf{Weighted} section calculates the average and standard deviation of weighted degrees (also called Strength or Traffic), along with the equivalent threshold—the minimum weight value in the final graph $G$. The last section focuses on \textbf{Small-world} effects. The global clustering coefficient (GCC) measures graph transitivity—the fraction of actual triangles among all possible triangles in $G$. The local clustering coefficient (ALCC) is calculated as the average of actual connections within neighbors for all nodes. The network diameter is the longest path between any two nodes. Smaller diameters and larger GCC/ALCC values indicate stronger small-world effects. We also calculate the average shortest path length (ASPL) for both models.

% Llama2-7B and Llama2-70B share similar basic graph configurations, as they use the same number of edges. The average degree is 64, meaning each node has 64 neighbors on average, which is reasonable. However, 7B has a flatter degree distribution with a larger standard deviation. In the weighted version, 7B shows a smaller and more concentrated degree distribution compared to the unweighted version. Figure~\ref{fig:degree_dist} in Appendix~\ref{app: dd} shows the degree distribution, where 7B exhibits a long-tailed distribution, indicating fewer high-degree nodes (central nodes).

% Both models exhibit strong small-world clustering effects, as evidenced by high GCC and ALCC values and a short diameter. This is in contrast to random networks with the same number of edges. For random networks, GCC and ALCC values are 0.0032 and 0.0020, respectively, which are much lower than those of the LLM-generated networks. The diameter of 6 is consistent with the Six Degrees of Separation theory~\cite{Milgram1967small}, widely observed in social networks~\cite{Watts1998collective} and web pages~\cite{Albert1999internet}.



Llama2-7B and Llama2-70B share similar graph structures, both using the same number of edges. The average degree is 64, but 7B has a flatter degree distribution with a larger standard deviation. In the weighted version, 7B's degree distribution is more concentrated. As shown in Figure~\ref{fig:degree_dist} (Appendix~\ref{app: dd}), 7B exhibits a long-tailed distribution, indicating fewer high-degree (central) nodes.  

Both models exhibit strong small-world clustering, with high GCC and ALCC values and a short diameter. In contrast, random networks with the same edge count have much lower GCC (0.0032) and ALCC (0.0020). The observed diameter of 6 aligns with the Six Degrees of Separation theory~\cite{Milgram1967small}, commonly found in social networks~\cite{Watts1998collective} and web structures~\cite{Albert1999internet}.



% \begin{table}[h!]
% \centering
% \begin{tabular}{lcc}
% \toprule
% \textbf{Statistics} & \textbf{Llama2-7B} & \textbf{Llama2-70B} \\
% \midrule
% \multicolumn{3}{c}{\textbf{Basic}} \\  
% \cmidrule(lr){1-3}
% \#Nodes & 32,000 & 32,000 \\
% \#Edges & 1,024,000 & 1,024,000 \\
% Avg. Degree & 64 & 64 \\
% Std. Degree & 68.39 & 58.96 \\
% \midrule
% \multicolumn{3}{c}{\textbf{Weighted}} \\  
% \cmidrule(lr){1-3}
% Avg. Degree\_W & 8.76 & 12.23 \\
% Std. Degree\_W & 13.78 & 14.02 \\
% Threshold & 0.095 & 0.147 \\
% \midrule
% \multicolumn{3}{c}{\textbf{Small-world}} \\  
% \cmidrule(lr){1-3}
% GCC $\uparrow$ & 0.325 & 0.215 \\
% ALCC $\uparrow$ & 0.183 & 0.174 \\
% Diameter $\downarrow$ & 6 & 6 \\
% ASPL $\downarrow$ & 3.392 & 3.353 \\
% \bottomrule
% \end{tabular}
% \caption{Statistics for Llama2-7B and Llama2-70B with three parts. In the bottom part of ``Small-world'', GCC represents global clustering coefficient; ALCC represents averaged local clustering coefficient; Diameter and ASPL shows the largest and averaged shortest path length.}
% \label{tab:statistics}
% \end{table}


% \subsection{Scenario 1}
% In the first scenario, we evaluate the conceptual spaces of both models by the length of the shortest path of any pair with intra or inner semantic groups, before we calculate the average length across instances.The shortest path is computed via the Dijkstra algorithm in the package of networkx~\footnote{\url{https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path.html}}. The averaged lengths of 7B and 70B are shown in Figure~\ref{fig:shortest_path}, while the difference heatmap with the averaged length of 70B minus that of 70B is illustrated in Figure~\ref{fig:difference_map}. The figure also indicates the significance of difference by the number of ``*'', where one, two, three stars represents the p values in t-test of 0.05, 0.01 and 0.001, respectively. 
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/difference_map_ALL.pdf}
    \caption{Shortest Path Length Difference (Llama2-70B Minus Llama2-7B) Across Semantic Groups. The number of stars indicate the degree of  the significance level of the difference.}
    \label{fig:difference_map}
\end{figure}



\subsection{Scenario 1}
In this scenario, we evaluate the conceptual spaces of both models by calculating the shortest path length between any pair of nodes within and between semantic groups. The shortest path is computed using Dijkstra's algorithm from the NetworkX package~\footnote{\url{https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html}}
. The average lengths for Llama2-7B and Llama2-70B are shown in Figure~\ref{fig:shortest_path}. Figure~\ref{fig:difference_map} presents the difference heatmap, where the difference is computed as the average length of Llama2-70B minus that of Llama2-7B. The significance of the differences is indicated by the number of stars (one, two, or three), corresponding to p-values of 0.05, 0.01, and 0.001 in the t-test, respectively.





% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figs/models_map_ALL.pdf}
%     \caption{Length of the shortest path among semantic gropus for Llama2-7B (left) and Llama2-70B (right).}
%     \label{fig:shortest_path}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figs/difference_map_ALL.pdf}
%     \caption{Length difference map among semantic groups of the shortest path when 70B minus 7B.}
%     \label{fig:difference_map}
% \end{figure}

% From the result, we can observe in general, 70B has a significantly longer length than 7B, no matter within the semantic group or not, except RANDOM vs. NAME or PLACE. This indicates 70B tends to have a complex path to discover the potential relation. This is illustrated by an example of the six shortest paths from ``bar'' to ``library'' for both models, shown in Figure~\ref{fig:example_7B} and Figure~\ref{fig:example_70B}, respectively. The shortest path (marked as red) in 70B has more nounced relation change such as the word form and conjugation than 7B. Nodes in other paths for 70B have multilingual instances and have more diverse connections, which disambiguates the word. For example, ``library'' has the meaning of building storing book and the tools in software. If we dive into the result per group, both models show a shorter path when compared within the same group and they have the similar trend when coming to different group pair. Among them, instances within COLOR shows the shortest path while FURNITURE the longest. This is may because instances in FURNITURE may be more polysemous, such as chair both has the meaning of furniture and human.
% % (\textcolor{red}{Some examples. TBD.}). 
% This is also verified by the longer path when comparing FURNITURE to other groups. And NATION and CITY tends to be shorter.  



% From the results, we observe that, in general, Llama2-70B has significantly longer path lengths than Llama2-7B, both within and between semantic groups, except in the RANDOM vs. NAME or PLACE comparison. This suggests that Llama2-70B tends to take a more complex path to uncover potential relationships. This is illustrated in the example of the six shortest paths from ``bar'' to ``library'' for both models, shown in Figures~\ref{fig:example_7B} and~\ref{fig:example_70B}. The shortest path (marked in red) in Llama2-70B exhibits more pronounced changes in word form and conjugation compared to Llama2-7B. In addition, nodes in other paths for Llama2-70B include multilingual instances and more diverse connections, which help disambiguate the word. For example, ``library'' can refer to both a building that stores books and a set of tools in software. 

Our results show that Llama2-70B generally has longer path lengths than Llama2-7B, both within and between semantic groups, except in the RANDOM vs. NAME or PLACE comparison. This suggests that Llama2-70B follows more complex paths to uncover relationships. Figures~\ref{fig:example_7B} and~\ref{fig:example_70B} illustrate this with the six shortest paths from “bar” to “library.” The shortest path (red) in Llama2-70B shows greater variation in word form and conjugation. Additionally, its other paths include multilingual instances and diverse connections, aiding disambiguation—for example, distinguishing “library” as a building from its software-related meaning.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/library_bar_Llama2-7B_SP.pdf}
    \caption{Paths between ``bar'' and ``library'' for Llama2-7B. Edge width reflects weight values. Nodes and edges along the shortest path in terms of the summed weights are marked in \textcolor{red}{red}.}
    \label{fig:example_7B}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/library_bar_Llama2-70B_SP.pdf}
    \caption{Paths between ``bar'' and ``library'' for Llama2-70B. Edge width represents weight values. Nodes and edges in the shortest path are highlighted in \textcolor{red}{red}. The paths illustrate a complex relationship involving similar word forms, multilingual links, and disambiguation groups.}
    \label{fig:example_70B}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figs/set_collection_Llama2-7B_SP.pdf}
    \caption{Paths between ``collection'' and ``set'' in Llama2-7B. The \textcolor{red}{red} line represents the shortest path in terms of the summed weights.}
    \label{fig:example_hyp_7B}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figs/set_collection_Llama2-70B_SP.pdf}
    \caption{Paths between ``collection'' and ``set'' in Llama2-70B. The \textcolor{red}{red} line indicates the shortest path, which follows a more logical transition from a singular concept to a plural one.}
    \label{fig:example_hyp_70B}
\end{figure}

When analyzing per group, both models show shorter paths within the same group, and the trends are similar for different group pairs. Among the groups, COLOR shows the shortest paths, while FURNITURE exhibits the longest. This may be due to polysemy in FURNITURE, where terms like ``chair'' refer to both furniture and a human (e.g., a person referred to as a ``chair''). This is further supported by the longer paths when comparing FURNITURE to other groups. Conversely, NATION and CITY tend to have shorter paths.


% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figs/▁library_▁bar_Llama2-7B_SP.pdf}
%     \caption{Paths between ``bar'' and ``library'' for Llama2-7B. Width of edges show the value of weights. Nodes and edges along with the shortest path are marked as red.}
%     \label{fig:example_7B}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figs/▁library_▁bar_Llama2-70B_SP.pdf}
%     \caption{Paths between ``bar'' and ``library'' for Llama2-70B. The width of edges show the value of weights. Nodes and edges along with the shortest path are marked as red.}
%     \label{fig:example_70B}
% \end{figure}




% \subsection{Scenario 2}
% For the second scenario, we evaluate the network for the pair of words in different relation types, as listed in Table~\ref{tab:relation}. Similarly, we obtain the shortest path for each word pair and show the length for both models. We also calculate the significance of such differences, indicated by the number of stars. The results are illustrated in Figure~\ref{fig:wordnet_path}.
% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figs/wordnet_path.pdf}
%     \caption{Averaged length of the shortest path across instances in the different relation types for two models. The number of stars shows the difference significance between these models.}
%     \label{fig:wordnet_path}
% \end{figure}

% We draw the similar conclusion to Scenario 1: Llama2-70B tends to have longer path than Llama2-7B. Nearly for all of the relations, the length of 70B outperforms 7B significantly except two types related to the word form, where both models have a direct edge for the word pair. The longer path appears in \textit{member of domain topic}, \textit{verb group} and \textit{hypernym}, indicating a longer relation chain. We illustrate an example of ``set'' and ``collection'' in the relation \textit{hypernym}, where ``set'' is a hypernym instance of ``collection''. Figure~\ref{fig:example_7B} and Figure~\ref{fig:example_70B} show the spaces constructed by Llama2-7B and Llama2-70B, respectively. Similar to cases in the last scenario, Llama2-70B tends to exploit a more complicated connection, gradually from a single concept to a plural one with some conjugation correlation.



\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/wordnet_path.pdf}
    \caption{Averaged shortest path length across relation types for both models. The number of stars indicates the significance of the difference.}
    \label{fig:wordnet_path}
\end{figure}

\subsection{Scenario 2}
In the second scenario, we evaluate the network for word pairs from different relation types, as shown in Table~\ref{tab:relation}. For each pair, we compute the shortest path and display the length for both models. The significance of the differences is indicated by the number of stars. The results are shown in Figure~\ref{fig:wordnet_path}.





The results are similar to Scenario 1: Llama2-70B generally has longer paths than Llama2-7B. For most relations, the length of 70B is significantly longer, except for those related to word form, where both models have direct edges for the word pair. Longer paths appear in \textit{member of domain topic}, \textit{verb group}, and \textit{hypernym} relations, indicating a more complex connection chain. For example, in the \textit{hypernym} relation between ``set'' and ``collection'', Llama2-70B shows a more intricate connection, gradually linking the singular ``set'' to the plural ``collection'' with some conjugation correlation, as seen in Figures~\ref{fig:example_hyp_7B} and~\ref{fig:example_hyp_70B}.


% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figs/▁set_▁collection_Llama2-7B_SP.pdf}
%     \caption{Shortest paths between ``collection'' and ``set'' for the space by Llama2-7B. Red line shows the shortest one.}
%     \label{fig:example_hyp_7B}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figs/▁set_▁collection_Llama2-70B_SP.pdf}
%     \caption{Shortest paths between ``collection'' and ``set'' for the space by Llama2-70B. Red line shows the shortest one.}
%     \label{fig:example_hyp_70B}
% \end{figure}

% \subsection{Scenario 3}
% For the third Scenario, we build a conceptual space for qualitative words, with a reference (GT) from a cross-lingual research. The statistics of spaces constructed by Llama2-7B (7B), Llama2-70B (70B), with that from GT are listed in Table~\ref{tab:SMM_stats}.



\subsection{Scenario 3}
In Scenario 3, we construct a conceptual space for qualitative words, with a reference (GT) from cross-lingual research. The statistics for the spaces built by Llama2-7B (7B), Llama2-70B (70B), and GT are shown in Table~\ref{tab:SMM_stats}.


% \begin{table}[h]
% \centering
% \begin{tabular}{lccc}
% \toprule
% \textbf{Statistics} & \textbf{7B} & \textbf{70B} & \textbf{GT} \\
% \midrule
% \multicolumn{4}{c}{\textbf{Basic}} \\  
% \cmidrule(lr){1-4}
% \#Nodes & 75 & 75 & 75 \\
% \#Edges & 293 & 130 & 37 \\
% Avg. Degree & 7.813 & 3.467 & 0.987 \\
% Std. Degree & 5.724 & 3.021 & 0.959 \\
% \midrule
% \multicolumn{4}{c}{\textbf{Weighted}} \\  
% \cmidrule(lr){1-4}
% Avg. Degree\_W & 0.963 & 0.611  & - \\
% Std. Degree\_W & 0.736 &0.546 & - \\
% Avg. Weight & 0.123 & 0.176 & - \\
% \midrule
% \multicolumn{4}{c}{\textbf{Connectivity}} \\  
% \cmidrule(lr){1-4}
% \#Component & 8 & 17 & 39 \\
% \#Single &7  & 16 & 26 \\
% \midrule
% \multicolumn{4}{c}{\textbf{Reference with GT}} \\
% \cmidrule(lr){1-4}
% Correlation $\uparrow$ & 0.466 & 0.449 & 1 \\
% Recall $\uparrow$ & 0.568 & 0.378 & - \\
% Precision $\uparrow$ & 0.072 & 0.108 & - \\
% \bottomrule
% \end{tabular}
% \caption{Statistics for Llama2-7B, Llama2-70B and GT with four dimensions. ``\#Component'' represents the number of connected components. ``\#Single'' means the number of nodes with the zero degree. In the bottom part, we calculate the degree correlation across samples, recall and precision.}
% \label{tab:SMM_stats}
% \end{table}

% In general, spaces built from the models have more edges than that from human experts, with much more edges, and less connected components and single nodes. This is mainly because experts construct the graph by collecting corpus and add an edge only when at least three languages meets the requirement of concept co-occurrence. This is rather difficult for humans, especially for the low-resource languages. In contrast, vectorized concepts from models can form a much dense graph. Furthermore, 70B tends to be more sparse than its 7B counterpart. When comparing it to the GT, the automatic space has a good alignment, with a moderately high correlation of degrees and coverage (indicated by recall). They have a low precision due to a large number edges, though. This indicates we could use embedding to initially construct a space before linguists re-analyze it.



\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Statistics} & \textbf{7B} & \textbf{70B} & \textbf{GT} \\
\midrule
\multicolumn{4}{c}{\textbf{Basic}} \\  
\cmidrule(lr){1-4}
\#Nodes & 75 & 75 & 75 \\
\#Edges & 293 & 130 & 37 \\
Avg. Degree & 7.813 & 3.467 & 0.987 \\
Std. Degree & 5.724 & 3.021 & 0.959 \\
\midrule
\multicolumn{4}{c}{\textbf{Weighted}} \\  
\cmidrule(lr){1-4}
Avg. Degree\_W & 0.963 & 0.611  & - \\
Std. Degree\_W & 0.736 & 0.546 & - \\
Avg. Weight & 0.123 & 0.176 & - \\
\midrule
\multicolumn{4}{c}{\textbf{Connectivity}} \\  
\cmidrule(lr){1-4}
\#Component ($\downarrow$) & 8 & 17 & 39 \\
\#Single ($\downarrow$) &7  & 16 & 26 \\
\midrule
\multicolumn{4}{c}{\textbf{Reference with GT}} \\
\cmidrule(lr){1-4}
Correlation ($\uparrow$) & 0.466 & 0.449 & 1 \\
Recall ($\uparrow$) & 0.568 & 0.378 & - \\
Precision ($\uparrow$) & 0.072 & 0.108 & - \\
\bottomrule
\end{tabular}
\caption{Statistics for Llama2-7B, Llama2-70B, and GT across four dimensions. ``\#Component'' represents the number of connected components, and ``\#Single'' indicates the number of nodes with zero degree. In the bottom section, we report degree correlation, recall, and precision.}
\label{tab:SMM_stats}
\end{table}



In general, the spaces constructed by the models have more edges than those built by human experts, with fewer connected components and isolated nodes. This is because experts create the graph by collecting a corpus and adding an edge only when at least three languages exhibit concept co-occurrence. This process is especially challenging for low-resource languages. In contrast, the vectorized concepts from models generate a much denser graph. Furthermore, Llama2-70B tends to be sparser than Llama2-7B. Compared to GT, the automatic space aligns well, showing moderate correlation and coverage (indicated by recall). However, the precision is lower due to the larger number of edges, suggesting that embeddings could be used to initially construct a space, which linguists could later refine.
