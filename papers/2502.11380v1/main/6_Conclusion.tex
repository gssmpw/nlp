% \section{Conclusion}
% This paper investigates the method to construct conceptual spaces via input embeddings from LLMs. Then we analyze and compare the network properties from two LLMs of different scales in three scenarios. We conclude that conceptual spaces can be effectively constructed by the embeddings and they demonstrate a small-world clustering effect. Besides, the model with more parameters tends to explore a longer and more complicated path between two concepts. This could partly justify the power of ``scaling law''. Our study also provides a potential efficient way to construct a conceptual space, which may assist the area of language typology or cognitive science. 

% \section{Limitations}
% We acknowledge that our work has several limitations. First, we represent each concept as a word without context. However, individual words can be ambiguous, especially for homonymous ones where different unrelated meanings or concepts appear into one word form. Second, we only evaluate on limited models, i.e., Llama2-7B and Llama2-70B. Conclusions may change for models with different architectures and scales of parameters. Besides, only input embeddings are investigated and we do not probe the properties of output embeddings which also can represent an individual word. Finally, the length of the shortest path in conceptual spaces is not a constant metric to measure the quality of embeddings, and we will explore more elaborate metrics to reflect the features of the space.

% \section{Ethics Statement}
% In general, we do not foresee any immediate ethical consequences of our research. However, there may exist some unethical connections between concepts, like genders and rank of jobs. This could be caused by potentially biased embeddings~\cite{Bordes2013wn18,bolukbasi2016man}.

\section{Conclusion}
This paper investigates the construction of conceptual spaces using input embeddings from large language models (LLMs). We analyze and compare the network properties of two LLMs with different scales across three scenarios. Our findings show that conceptual spaces can be effectively constructed from embeddings, which exhibit a small-world clustering effect. Additionally, models with more parameters tend to explore longer and more complex paths between concepts, partially supporting the ``scaling law''~\cite{kaplan2020scaling}. This study also provides an efficient approach to constructing conceptual spaces, potentially benefiting fields such as language typology and cognitive science.

\section{Limitations}
We acknowledge several limitations in our work. First, we represent each concept as a word without considering context. However, words can be ambiguous, particularly for homonyms that encompass multiple unrelated meanings. Second, our evaluation is limited to two models, Llama2-7B and Llama2-70B. Results may differ with models of different architectures or parameter scales. Additionally, we focus only on input embeddings and do not explore the properties of output embeddings, which may also capture individual word representations. Finally, the length of the shortest path in conceptual spaces is not a definitive metric for embedding quality, and we plan to explore more sophisticated metrics to better reflect the characteristics of these spaces.

\section{Ethics Statement}
We do not foresee immediate ethical concerns arising from our research. However, there may be unintended biases in the connections between concepts, such as those involving gender and job ranks. These biases may stem from biased embeddings~\cite{Bordes2013wn18, bolukbasi2016man}.
