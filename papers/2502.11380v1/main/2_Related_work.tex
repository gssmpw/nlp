\section{Related Work}
\label{related work}

\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{figs/intro.pdf}
    \caption{Outline of our conceptual space construction. First, we extract the input word embeddings ($\mathcal{E}$) for the LLM vocabulary ($\mathcal{V}$). Next, we build a complete graph $\mathcal{C}$ by calculating the cosine similarity between all embedding pairs. Finally, we retain edges based on similarity, from highest to lowest, until the graph $\mathcal{G}$ is connected. We then focus on specific connected subgraphs $\mathcal{G'}$ representing certain domains at a local level.}
    \label{fig:intro}
\end{figure*}

% \subsection{Conceptual space modeling}
% % Representation
% % Method
% % Network Analysis
% Conceptual space modeling has been widely studied and applied in cognitive science~\cite{gardenfors2000conceptual,gardenfors2014geometry,nosofsky1986attention,nosofsky1992similarity}, linguistic typology~\cite{croft2001radical,haspelmath2003geometry}, neuroscience~\cite{caglar2021conceptual} and so on. One popular framework (CSF) proposed by~\citet{gardenfors2000conceptual,gardenfors2014geometry} introduces the basic notions of similarity space and conceptual space, where the latter is a prototypical realization based on the former one. A series work following that focus on the construction of similarity space including the representation of instances and distance metrics among them. Mutidimensional scaling~\cite{BorgGroenen1999} and the spatial arrangement method~\cite{goldstone1994efficient} are two common approaches based on pairwise similarity judgments for a set of items. However, these methods are always manual with the high costs associated with extensive data collection or cognitively demanding. Alternatively, a more efficient way leverages language models and word embeddings, such as word2vec, fasttext, BERT, even LLMs. Notably, embeddings directly from LLMs performs much worse correlation in a human-annotated dataset than prompt guided construction. Our paper also focuses on the embeddings from LLMs, but with more systematic evaluations and much more concepts~\footnote{The term ``concept'' in our paper refers to a word without context, which is similar to the instances in CFS. Thus our conceptual spaces are more like the similarity spaces in the context of CFS.}.

\subsection{Conceptual Space Modeling}
Conceptual space modeling has been extensively studied and applied in fields such as cognitive science~\cite{gardenfors2000conceptual,gardenfors2014geometry,nosofsky1986attention,nosofsky1992similarity}, linguistic typology~\cite{croft2001radical,haspelmath2003geometry}, and neuroscience~\cite{caglar2021conceptual}. One widely used framework, the Conceptual Space Framework (CSF) proposed by~\citet{gardenfors2000conceptual,gardenfors2014geometry}, introduces the basic concepts of similarity space and conceptual space, with the latter being a prototypical realization based on the former. Subsequent work has focused on constructing similarity spaces, including the representation of instances and the distance metrics among them. Multidimensional scaling~\cite{BorgGroenen1999} and spatial arrangement methods~\cite{goldstone1994efficient} are two common approaches based on pairwise similarity judgments for a set of items. However, these methods are often manual and incur high costs due to the need for extensive data collection or the cognitive demands they impose. Alternatively, a more efficient approach leverages language models and word embeddings, such as word2vec~\cite{Mikolov2013distributed}, fastText~\cite{Bojanowski2017subword}, BERT~\cite{devlin2019bert}, and even LLMs~\cite{touvron2023llama}. Notably, embeddings directly from LLMs exhibit weaker correlation with human-annotated datasets compared to prompt-guided constructions. Our paper also focuses on embeddings from LLMs, but with more systematic evaluations and a broader set of concepts~\footnote{In this paper, the term ``concept'' refers to a word without context, similar to the instances in CSF. Thus, our conceptual spaces are more akin to similarity spaces within the CSF framework.}.


% \subsection{Semantic Map Models}
% Semantic map modeling is another framework to construct a conceptual space based on the cross-lingual co-occurrence of concepts in a linguistic form. These forms may include content words~\cite{guo2012adjectives, cysouw2007building,perrin2010polysemous}, function words~\cite{zhang2017semantic}, or constructions~\cite{malchukov2007ditransitive}. The concepts are always represented by the grammatical~\cite{zhang2017semantic} or content~\cite{guo2012adjectives} meanings of that form. The conceptual space can be build in a bottom-up or top-down manner. A classical bottom-up approach is based on connectivity hypothesis~\cite{croft2001radical,haspelmath2003geometry,ma2015semantic_en},, which states that nodes of concepts involved into a single linguistic form should be connected in the corresponding subgraph. The overall space thus is built edge by edge incrementally. Another top-down perspective~\cite{liu2024top} first constructs a similarity graph according to the strength of co-occurrence. Then the graph is sparsified in terms of a refined connectivity constraint. Our paper regard a concept as a single token without explicitly matching a form-meaning pair and adopts an efficient top-down manner with slightly different constraints to build the conceptual space.

\subsection{Semantic Map Models}
Semantic map modeling is another framework for constructing conceptual spaces based on cross-lingual co-occurrence of concepts in linguistic forms. These forms can include content words~\cite{guo2012adjectives,cysouw2007building,perrin2010polysemous}, function words~\cite{zhang2017semantic}, or constructions~\cite{malchukov2007ditransitive}. The concepts are typically represented by the grammatical~\cite{zhang2017semantic} or content~\cite{guo2012adjectives} meanings of these forms. The conceptual space can be constructed in either a bottom-up or top-down manner. A classical bottom-up approach is based on the connectivity hypothesis~\cite{croft2001radical,haspelmath2003geometry,ma2015semantic_en}, which posits that concepts involved in a single linguistic form should be connected within the corresponding subgraph. The overall space is then built incrementally, edge by edge. Alternatively, a top-down approach~\cite{liu2024top} first constructs a similarity graph based on the strength of co-occurrence, and then sparsifies the graph according to a refined connectivity constraint. In this paper, we treat a concept as a single token without explicitly matching form-meaning pairs, and we adopt an efficient top-down approach with slightly modified constraints to build the conceptual space.



% \subsection{Word Embedding and Representation}
% Contemporary language models utilize a continuous vector to represent a word or subtoken, based on distributed semantics~\cite{boleda2020distributional}. These highly dimensional vectors can be static~\cite{Mikolov2013distributed,Bojanowski2017subword} or context-sensitive~\cite{devlin2019bert}, effective but lack of interpretability compared to its linguistic feature counterpart. They are shown to have an elegant linear relations~\cite{Mikolov2013distributed}, high similarity alignment with humans~\cite{vulic2020probing}, and meaning representation~\cite{Turney2010vector}. Among them, the static embedding is an ideal agent for an offline word without context, especially for monosemous word. These embeddings are arranged in the input and output layers in LLMs. Researchers have studied the linear property~\cite{han2024word}, conceptual space construction~\cite{moullec2025cheaper} and
% so on.
% % \textcolor{red}{interpretability TBD.} 
% for these parts. We also concentrate on the input embeddings for LLMs but provides a more variety and systematic evaluation by leveraging conceptual spaces. 



\subsection{Word Embedding and Representation}
Contemporary language models represent words or subtokens using continuous vectors based on distributed semantics~\cite{boleda2020distributional}. These high-dimensional vectors can be static~\cite{Mikolov2013distributed,Bojanowski2017subword} or context-sensitive~\cite{devlin2019bert}. While effective, they lack the interpretability of linguistic feature-based representations~\cite{petersen2023lexical}. Word embeddings exhibit elegant linear relationships~\cite{Mikolov2013distributed}, high similarity with human judgments~\cite{vulic2020probing}, and meaningful representations~\cite{Turney2010vector}. Static embeddings are particularly suitable for offline, context-free words, especially monosemous ones. These embeddings are used in the input and output layers of LLMs. Previous research has explored their linear properties~\cite{han2024word}, conceptual space construction~\cite{moullec2025cheaper}, and other aspects. In this paper, we focus on LLM input embeddings, offering a more diverse and systematic evaluation through the lens of conceptual spaces.



