% \section{Experimental Design}
% \label{sec:experiment}

% % model
% \subsection{Large Language Models}
% We adopt the Llama series as our LLMs, including Llama2-7B and Llama2-70B~\cite{touvron2023llama}. The dimension of the input embedding is 4096 and 8192, for Llama2-7B and Llama2-70B respectively. 
% % Besides, 
% \lky{Also}, they share the vocabulary for both models, with the size of vocabulary 32,000. The tokens in the vocabulary are obtained by Byte Pair Encoding~\cite{sennrich2016neural}, merging the frequent characters. Thus, many tokens are part of a whole word. Besides, tokens with a whitespace or appearing at the beginning of a sentence are different from those in other places, i.e., the end part of a word. For example, ``man'' and ``man'' in ``policeman'' are different units in the vocabulary. We identify the token appearing the end part of a word by add ``\#'' at the beginning of the token, such as ``\#man''.

\section{Experimental Design}
\label{sec:experiment}

% model
\subsection{Large Language Models}
We adopt the Llama series as our LLMs, including Llama2-7B and Llama2-70B~\cite{touvron2023llama}. The dimension of the input embedding is 4096 and 8192, for Llama2-7B and Llama2-70B respectively. 
% Besides, 
Also, they share the vocabulary for both models, with the size of vocabulary 32,000. The tokens in the vocabulary are obtained by Byte Pair Encoding~\cite{sennrich2016neural}, merging the frequent characters. Thus, many tokens are part of a whole word. Besides, tokens with a whitespace or appearing at the beginning of a sentence are different from those in other places, i.e., the end part of a word. For example, 
% ``man'' and ``man'' in ``policeman'' 
``man'' in ``policeman'' and ``man''
are different units in the vocabulary. We identify the token appearing the end part of a word by add ``\#'' at the beginning of the token, such as ``\#man''.


% % three scenarios
% \subsection{Scenario 1: Common Concepts}
% We collect nine semantic groups to represent common concepts, each with 10 tokens or concepts. These includes NUMBER, NAME, MONTH, COLOR, CITY, NATION, PLACE, HUMAN and FURNITURE. Besides, a semantic group with random concepts is also included. The specific concepts are list in Appendix~\ref{sec:sce_1}. This scenario mainly examines the clustering effective with inner- and intra- semantic group.

% \subsection{Scenario 2: WordNet Relations}
% In scenario 2, we observe the subgraph consisting of instances with WordNet relations and explore the structural relatedness. We extract a subset of the public dataset WN18~\cite{Bordes2013wn18}, which includes 40,943 WordNet synsets and 18 relation types among them. We filter the whole dataset including train, dev and test dataset by several conditions: (1) After converting the synset into its words, the word has to be the token into the Llama vocabulary; (2) The synset has to be the first sense listed in the WordNet, so that the meaning is the most stereotypical one; (3) The nubmer of words in a relation is no less than 10. (4) We only reserve one relation type when it has a symmetric one, such as hypernym and hyponym. Then we create two other relations related to wordform: \textit{tokenization variant} considers a token in the vocabulary with and without a beginning of ``\_', indicating whether the word is a suffix or not. And uppercase variant considers words with or without a capitalized beginning letter, such as ``red'' and ``Red''. Overall are eight relation types, which are listed in Table~\ref{tab:relation}.

\subsection{Scenario 1: Common Concepts}
We collect nine semantic groups to represent common concepts, each containing 10 tokens or concepts: NUMBER, NAME, MONTH, COLOR, CITY, NATION, PLACE, HUMAN, and FURNITURE. Additionally, we include a semantic group of RANDOM concepts. A full list of the concepts is provided in Table~\ref{tab:concepts_SC} in Appendix~\ref{sec:sce_1}. This scenario primarily examines the length of the shortest path within and between semantic groups.

\subsection{Scenario 2: WordNet Relations}
In Scenario 2, we explore a subgraph of WordNet instances and their structural relationships. We extract a subset from the public WN18 dataset~\cite{Bordes2013wn18}, consisting of 40,943 WordNet synsets and 18 relation types. The dataset is filtered by the following conditions: (1) after converting synsets to words, the words must be in the Llama vocabulary; (2) only the first sense of each synset is retained for its stereotypical meaning; (3) a relation must involve at least 10 words; (4) symmetric relation pairs, such as hypernym and hyponym, are merged. Additionally, we define two wordform relations: \textit{tokenization variant}, distinguishing tokens with and without a leading underscore (e.g., ``man'' vs. ``\#man''), and \textit{uppercase variant}, differentiating capitalized and non-capitalized forms (e.g., ``red'' vs. ``Red''). In total, we consider eight relation types, listed in Table~\ref{tab:relation}.


% \begin{table}[h!]
% \centering
% \begin{tabular}{ccc}
% \toprule
% Index & Relation Type & Count \\
% \midrule
 
%  A & member of domain topic & 51 \\
%  B & verb group & 10 \\
%  C & hypernym & 464 \\
%  D & has part & 22 \\
%  E & also see & 95 \\
%  F & derivationally related form & 388 \\
%  G & tokenization variant & 1685 \\
%  H & uppercase variant & 1788 \\
 
% \bottomrule
% \end{tabular}
% \caption{WordNet relations with counts of instances}
% \label{tab:relation}
% \end{table}



% \subsection{Scenario 3: An SMM of qualitative words}
% In the third scenario, we utilize a cross-lingual semantic map on adjectives and qualitative words~\cite{perrin2010polysemous}. These include twenty-two African languages, French and English. When a polysemy in a specific languages has several concepts, these concepts are connected to indicate a high proximity. The domain includes dimension, age, value, colour and so on. Each has several concepts represented by capitalized words mainly in English. For example in the domain of ``dimension'', they use BIG, SMALL, LONG, SHORT, WIDE, DEEP to name a few. Again, we filter the words which are not in the Llama vocabulary and obtain 75 concepts. The final concepts and the human-annotated graph are shown in Appendix~\ref{app:CS}.

\begin{table}[h!]
\centering
\begin{tabular}{ccc}
\toprule
Index & Relation Type & Count \\
\midrule
A & Member of Domain Topic & 51 \\
B & Verb Group & 10 \\
C & Hypernym & 464 \\
D & Has Part & 22 \\
E & Also See & 95 \\
F & Derivationally Related Form & 388 \\
G & Tokenization Variant & 1685 \\
H & Uppercase Variant & 1788 \\
\bottomrule
\end{tabular}
\caption{WordNet Relations and Instance Counts}
\label{tab:relation}
\end{table}

\subsection{Scenario 3: SMM of Qualitative Words}
In Scenario 3, we utilize a cross-lingual semantic map for adjectives and qualitative words~\cite{perrin2010polysemous}, which includes 22 African languages, French, and English. Polysemes in each language are connected to indicate conceptual proximity. The domain spans dimension, age, value, color, etc., with capitalized English words representing concepts, such as BIG, SMALL, LONG, SHORT, WIDE, and DEEP for dimension. We filter out words not present in the Llama vocabulary, resulting in 75 concepts. The final set of concepts and the human-annotated graph are presented in Appendix~\ref{app:CS}.

