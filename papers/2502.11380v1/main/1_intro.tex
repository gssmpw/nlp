\section{Introduction}
\label{introduction}

% Conceptual Space and Semantic Maps
% The conceptual space framework, casting concepts (i.e., indicated by a word) as nodes and connecting the pair of nodes via their proximity, has been widely utilized under the area of cognitive science, typology linguistics and beyond~\cite{gardenfors2000conceptual,gardenfors2014geometry,nosofsky1986attention,nosofsky1987attention,nosofsky1992similarity,shepard1964attention,shepard1987universal,croft2003typology,haspelmath2003geometry}. This framework attempts to discover the alignment between surface-level languages and deep-level human brain and provide a spacial visualization for brain activities. 

The conceptual space framework, which represents concepts (i.e., words) as nodes and connects them based on their proximity, has been extensively applied in cognitive science, typological linguistics, and related fields~\cite{gardenfors2000conceptual,gardenfors2014geometry,nosofsky1986attention,nosofsky1987attention,nosofsky1992similarity,shepard1964attention,shepard1987universal,croft2003typology,haspelmath2003geometry}. This framework aims to uncover the alignment between surface-level linguistic structures and deep cognitive processes, offering a spatial visualization of brain activities or concept representations.

% Methods including manual, traditional embeddings and their drawbacks
% Researchers leverages manual and automatic methods to represent the nodes and the edges and construct the conceptual space. For manual methods, linguists either directly connect the edges where nodes appear in the same form based on connectivity hypothesis~\cite{croft2001radical}, or calculate the number of co-occurrence of two concepts as the similarity of two nodes and generate a low-dimensional network by leveraging PPrincipal Component Analysis (PCA) or MDS~\cite{goldstone1994efficient}. However, these manual methods are demanding due to the large workload and unable to obtain large-scale conceptual space.
% \lky{Existing efforts leverage manual and automatic methods to represent the nodes and edges, thereby constructing the conceptual space. For manual methods, linguists either directly connect two nodes appearing in the same form based on connectivity hypotheses~\cite{croft2001radical}, or calculate the co-occurrence frequency of two concepts as their similarity, generating a low-dimensional network via Principal Component Analysis (PCA)~\cite{abdi2010principal} or Multidimensional Scaling (MDS)~\cite{goldstone1994efficient}. However, these manual methods are labor-intensive and fail to produce large-scale conceptual spaces.}

Existing approaches employ both manual and automated methods to define nodes and edges, thereby constructing the conceptual space. In manual methods, linguists either establish direct connections between nodes based on connectivity hypotheses~\cite{croft2001radical} or determine similarity by computing the co-occurrence frequency of two concepts~\cite{cysouw2007building}, subsequently generating a low-dimensional network using techniques such as Principal Component Analysis (PCA)~\cite{abdi2010principal} or Multidimensional Scaling (MDS)~\cite{goldstone1994efficient}. However, these manual approaches are labor-intensive and inadequate for constructing large-scale conceptual spaces.

% A more efficient way is to utilize embedding models. Traditional models include stactic word2vec, which targets at the objective to represent the word as a dense vector. However, they only use data of small scale and fail to capture the naunced meaning of a word. Pretrained models like BERT~\cite{devlin2019bert} are shown~\cite{devlin2019bert,tenney2019bert}, to be more effective to represent the word and have been used to the construction~\cite{moullec2025cheaper}. This is beneficial to its consistent objective of encoder-only models to restore the current masked words. 
% \lky{To address mentioned limitations, automatic methods employ embedding models to enhance efficiency. Traditional embedding models like static word2vec~\cite{church2017word2vec} can represent words as dense vectors but rely on small-scale data and struggle to capture subtle word meanings. In contrast, pre-trained models such as BERT~\cite{devlin2019bert} have proven more effective in word representation~\cite{devlin2019bert,tenney2019bert} and are widely used in relevant fields~\cite{moullec2025cheaper}. As a pure encoder model, BERT aims to recover masked words, giving it a significant edge in constructing conceptual spaces.}

To address the aforementioned limitations, automated methods utilize embedding models to improve efficiency. Traditional embedding models, such as static word2vec~\cite{church2017word2vec}, represent words as dense vectors. However, these models rely on limited datasets and often fail to capture nuanced word meanings. In contrast, pre-trained models like BERT~\cite{devlin2019bert} have demonstrated greater effectiveness in word representation~\cite{devlin2019bert,tenney2019bert} and have become widely adopted across related fields~\cite{moullec2025cheaper}. As a purely encoder-based model, BERT focuses on recovering masked words, which gives it a distinct advantage in constructing conceptual spaces.


% LLMs and their specials (Motivations)
% Word embeddings from large language models (LLMs) are less studied, although LLMs have received extraordinary performance in a range of understanding and generative tasks~\cite{openai2023gpt4}. This is possibly because most of LLMs are decoder-only with the objective of next token prediction. Thus, to what extent the embeddings reserve the meaning of current token rather than ``transfer'' to the next token is unclear~\cite{liu-etal-2024-fantastic}. On the other hand, LLMs, as the most advanced language model, still lack of interpretability. A conceptual space built by their initial input representation and relations between each other could provide a cognitive perspective to explain LLMs. 
% \lky{Recently, Large Language Models (LLMs) have excelled in a range of understanding and generation tasks~\cite{openai2023gpt4}. However, research on word embeddings for LLMs remains limited. 
% Typically, LLMs are decoder-only with the objective of next token prediction. As a result, }to what extent the embeddings reserve the meaning of current token rather than ``transfer'' to the next token is unclear~\cite{liu-etal-2024-fantastic}. 
% \lky{Moreover, as state-of-the-art language models, LLMs still fall short in terms of interpretability. Overall, we aim to provide a cognitive perspective for interpreting LLMs by constructing a conceptual space based on initial input representations and their interrelationships.}

Recently, Large Language Models (LLMs) have demonstrated remarkable performance across a variety of understanding and generation tasks~\cite{openai2023gpt4}. However, research on word embeddings within LLMs remains relatively limited. Typically, LLMs are decoder-only models trained with the objective of next-token prediction. Consequently, it remains unclear to what extent the embeddings capture the meaning of the current token, as opposed to simply transferring information to the next token~\cite{liu-etal-2024-fantastic}. Furthermore, despite their success, LLMs still face challenges in terms of interpretability~\cite{zou2023representation}. In this context, we aim to offer a cognitive perspective for interpreting LLMs comparatively by constructing a conceptual space based on initial input representations from LLMs of different scales. 



% What we have done... and contributions
% In the paper, we build a conceptual space by leveraging input embeddings from LLMs and explore the properties from global and local views. Specially, we regard the vocabulary of LLMs as the set of concepts and respective input embeddings as the representation of nodes. With a similarity metrics, we first construct a complete graph and then sparsify it based on the minimum connectivity hypothesis motivated by the theory of semantic map models~\cite{haspelmath2003geometry,croft2003typology}. Then, we compare the conceptual spaces from LLMs with the similar architecture but different scales of parameters by calculating the global statistics of the network. We found that LLMs embeddings form a small-world network with a high clustering coefficient and low distances. Besides, a network with more parameters tends to have a more complex network with longer paths and relations. To evaluate the space, we then extract the local subgraph in different designated scenarios: (1) common concepts, (2) wordnet relations, (3) a case study on cross-lingual qualitative words. The alignment with human perceptions show the effectiveness and efficiency of the construction.
% \lky{In this paper, we construct a conceptual space based on the input embeddings of LLMs and explore its properties from both global and local perspectives. Specifically, we treat the LLM's vocabulary as a set of concepts and the corresponding input embeddings as node representations. With similarity metrics, we first create a complete graph and then sparsify it based on the minimal connectivity hypothesis motivated by the theory of semantic map models~\cite{haspelmath2003geometry,croft2003typology}. Next, we compare the conceptual spaces of LLMs with similar architectures but different parameter scales by calculating global statistics of the network. The study reveals that LLM embeddings form a small-world network characterized by high clustering coefficients and low average path lengths. Moreover, LLMs with larger parameter scales typically exhibit more complex network structures, with longer paths and richer relationships. To evaluate the practicality of the conceptual space, we extract local subgraphs from different specified scenarios, including: (1) common concepts, (2) WordNet relations, and (3) a case study on cross-lingual qualitative words. The high consistency of these subgraphs with human perception validates the effectiveness and efficiency of our constructed conceptual space.}

In this paper, we construct a conceptual space based on LLM input embeddings and analyze its properties comparatively from both global and local perspectives. Specifically, we treat the LLM vocabulary as a set of concepts, with the input embeddings serving as node representations. Using similarity metrics, we first build a complete graph, which we then sparsify based on the minimal connectivity hypothesis inspired by semantic map models~\cite{haspelmath2003geometry,croft2003typology}. We compare the conceptual spaces of LLMs with similar architectures but different parameter scales by calculating global network statistics. Our findings show that LLM embeddings form a small-world network with high clustering coefficients and low average path lengths. LLMs with larger parameter scales tend to have more complex structures, with longer paths and richer relationships. To assess the practical utility of the conceptual space, we extract local subgraphs for scenarios such as: (1) common concepts, (2) WordNet relations, and (3) a cross-lingual case study on qualitative words. The consistency of these subgraphs with human annotations confirms the effectiveness of our constructed conceptual space.
% Our contributions are listed as followed:
% \begin{itemize}
%     \item We construct a conceptual space based on the connectivity hypothesis for input embeddings of LLMs.
%     \item We evaluate the space from global and local perspective, with the comparison of models of 
%     % different
%     varying
%     scales.
%     \item The conceptual spaces show some alignment with human perception and can be as an efficient representation to concepts and their relations.
% \end{itemize}
In conclusion, our contributions are as follows:
\begin{itemize}
    \item We propose a comparative study on conceptual spaces constructed from LLMs of different scales based on the proposed connectivity hypothesis.
    \item We design three distinct scenarios and conduct an extensive evaluation of the conceptual space, considering both global and local perspectives, while comparing models of different scales.
    \item We demonstrate that the conceptual spaces align with human perception and provide an effective representation of concepts and their relationships.
\end{itemize}
