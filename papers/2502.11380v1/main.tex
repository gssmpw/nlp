% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{authblk}
\usepackage{graphicx}
% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}
% \usepackage[review]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{hyperref}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% \usepackage{authblk}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Data and Model Uncertainty Estimation for Word Sense Disambiguation}
% \title{Lower Layers Encode Lexical Semantics: Investigating Layer-wise Semantic Dynamics on LLMs}

\title{Exploring the Small World of Word Embeddings: A Comparative Study on Conceptual Spaces from LLMs of Different Scales}

% \author[1]{\textbf{Xingtai Lv}\thanks{\hspace{0.2em}\texttt{Corresponding author}}}
% \author[2]{\textbf{Ning Ding}\thanks{\hspace{0.2em}\texttt{Corresponding author}}}
% \author[2]{\textbf{Yujia Qin}}
% \author[2,3,4,5]{\textbf{Zhiyuan Liu}\thanks{\hspace{0.2em}\texttt{Corresponding author}}}
% \author[2,3,4,5]{\textbf{Maosong Sun}\thanks{\hspace{0.2em}\texttt{Corresponding author}}}

% \affil[1]{Department of Electronic Engineering, Tsinghua University}
% \affil[2]{Department of Computer Science and Technology, Tsinghua University}
% \affil[3]{BNRIST, Tsinghua University}
% \affil[4]{Institute for Artificial Intelligence, Tsinghua University}
% \affil[5]{International Innovation Center of Tsinghua University, Shanghai}

% \affil[ ]{\texttt{lvxt20, dingn18, qyj20@mails.tsinghua.edu.cn}}
% \affil[ ]{\texttt{liuzy, sms@tsinghua.edu.cn}}

\renewcommand\Authands{, } % 去掉默认的 "and"

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Zhu Liu, Cunliang Kong, Ying Liu\thanks{\hspace{0.5em} Corresponding author} \and Maosong Sun \\
%         Tsinghua University, China \\ liuzhu22@mails.tsinghua.edu.cn \\
%         cunliang.kong@outlook.com \\
%         \{yingliu,sms\}@tsinghua.edu.cn }

\author[1]{\textbf{Zhu Liu}}
\author[1]{\textbf{Ying Liu}}
\author[2]{\textbf{Kangyang Luo}}
\author[2]{\textbf{Cunliang Kong}}
\author[2]{\textbf{Maosong Sun}}

% \author[ \hspace{0.2em}1]{\textbf{Ying Liu}\thanks{\hspace{0.5em} Corresponding author}}

\affil[1]{School of Humanities, Tsinghua University}
\affil[2]{Department of Computer Science and Technology, Tsinghua University}

% \affil[ ]{\nolinkurl{{liuzhu22, luoky}@mails.tsinghua.edu.cn}} 
% \affil[ ]{\nolinkurl{{ luoky}@mail.tsinghua.edu.cn}} 
% \affil[ ]{\nolinkurl{{liuzhu22, yingliu,sms}@mails.tsinghua.edu.cn}} 
% \affil[ ]{\nolinkurl{cunliang.kong@outlook.com}}
\affil[ ]{\nolinkurl{liuzhu22@mails.tsinghua.edu.cn}}

\renewcommand\Authands{, } % 去掉默认的 "and"
        
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Zhu Liu \\
%   Tsinghua University \\
%   School of Humanities \\
%   \texttt{liuzhu22@mails.tsinghua.edu} \\
%   \And
%   Ying Liu \\
%   Tsinghua University  \\
%   School of Humanities\\
%   \texttt{yingliu@tsinghua.edu.cn} \\}

%------------------MY PACKAGE------------------ 
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{color,xcolor} % delete later.
\usepackage{xspace}
\usepackage{adjustbox}
\usepackage{multirow}
\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\newcommand*{\etc}{etc.\@\xspace}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{xcolor,colortbl}
\newcommand{\lky}[1]{{\color{blue}#1}}

\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\mathrm{\expandafter\@slowromancap\romannumeral #1@}}
\makeatother 


\begin{document}

\setcounter{page}{1}



\maketitle

\vspace{2cm} % 增加标题和正文之间的垂直间距

\begin{abstract}
% A conceptual space takes concepts as nodes and semantic relatedness as edges. 
% Word embeddings, as representation of concepts, along with a similarity metrics provide an efficient approach to construe the space. 
% % These embeddings are often extracted by traditional distributed models or encoder-only pretrained models due to the consistent objective.
% \lky{Typically, these embeddings are extracted} by traditional distributed models or encoder-only pretrained models due to the consistent objective.
% However, word embeddings from decoder-only and larger-scale large language models (LLMs) 
% % are less explored. 
% \lky{remain underexplored.}
% In this paper, we build a conceptual space by LLM word embeddings and investigate the properties of the space. 
% Specifically, we first construct a network using embeddings from LLMs based on a connectivity hypothesis motivated by linguistic typology. 
% % We then investigate the global statistics of the network and compare the differences between LLMs of different scales. 
% \lky{We then delve into the global statistics of the network and compare differences between LLMs of varying scales.}
% Afterwards, in a local view, we 
% % show 
% \lky{explore}
% different conceptual pairs belonging to various wordnet relations.
% Finally, we extract a cross-lingual semantic network relative to qualitative words.
% We conclude that the space is a small-world network characterized by a high clustering coefficient and low distances. 
% % Besides, a network with more parameters tends to have a more complex network with longer paths and relations. 
% % Finally, the network can be regarded as an agent to cross-lingual semantic maps.
% \lky{Additionally, networks derived from LLMs with more parameters tend to be more complex, featuring longer paths and richer relations.
% Importantly, the network can serve as an agent to cross-lingual semantic maps. }

% A conceptual space represents concepts as nodes and semantic relatedness as edges. Word embeddings, serving as representations of concepts, along with a similarity metric, provide an efficient approach to constructing such a space. 
% % Typically, these embeddings are extracted using traditional distributed models or encoder-only pretrained models due to their consistent objectives. 
% Typically, these embeddings are extracted using traditional distributed models or encoder-only pretrained models, as their objectives ensure a direct representation of the current token’s meaning, whereas decoder-only models including large language models (LLMs) are trained to predict the next token, making their representations less directly tied to the current token’s semantics.
% % However, word embeddings derived from decoder-only  large language models (LLMs) remain underexplored. 
% In this paper, we construct a conceptual space using word embeddings from LLMs and investigate its properties. 
% Specifically, we build a network based on a connectivity hypothesis inspired by linguistic typology, analyze its global statistics, and compare LLMs of varying scales. From a local perspective, we explore conceptual pairs corresponding to various common concepts, WordNet relations{, and} extract a cross-lingual semantic network related to qualitative words. %%加个逗号
% Our findings suggest that the space exhibits small-world properties, with a higher clustering coefficient and shorter path lengths. Additionally, spaces from larger LLMs tend to be more complex, featuring longer paths and richer relational structures. Furthermore, the network can serve as an efficient agent for cross-lingual semantic maps.

% A conceptual space represents concepts as nodes and semantic relatedness as edges. Word embeddings, paired with a similarity metric, offer an efficient way to construct such a space.
% Typically, these embeddings come from traditional distributed models or encoder-only pretrained models, as their objectives directly capture the current token’s meaning. In contrast, decoder-only models, including large language models (LLMs), predict the next token, making their embeddings less directly tied to the current token’s semantics. Furthermore, a comparative study on LLMs of different scales is less studied.
% This paper constructs a conceptual space using word embeddings from LLMs of varying scales and explores their properties comparatively. We build a network based on a linguistic typology-inspired connectivity hypothesis, analyze global statistics, and compare LLMs of different scales. Locally, we examine conceptual pairs, WordNet relations, and a cross-lingual semantic network for qualitative words.
% Our results show that the space exhibits small-world properties, with a high clustering coefficient and short path lengths. Larger LLMs produce more complex spaces, characterized by longer paths for instances of richer relational structures. Additionally, the network serves as an efficient tool for cross-lingual semantic maps.

A conceptual space represents concepts as nodes and semantic relatedness as edges. Word embeddings, combined with a similarity metric, provide an effective approach to constructing such a space. Typically, embeddings are derived from traditional distributed models or encoder-only pretrained models, whose objectives directly capture the meaning of the current token. In contrast, decoder-only models, including large language models (LLMs), predict the next token, making their embeddings less directly tied to the current token's semantics. Moreover, comparative studies on LLMs of different scales remain underexplored.
In this paper, we construct a conceptual space using word embeddings from LLMs of varying scales and comparatively analyze their properties. We establish a network based on a linguistic typology-inspired connectivity hypothesis, examine global statistical properties, and compare LLMs of varying scales. Locally, we analyze conceptual pairs, WordNet relations, and a cross-lingual semantic network for qualitative words.
Our results indicate that the constructed space exhibits small-world properties, characterized by a high clustering coefficient and short path lengths. Larger LLMs generate more intricate spaces, with longer paths reflecting richer relational structures and connections. Furthermore, the network serves as an efficient bridge for cross-lingual semantic mapping.



\end{abstract}

\input{main/1_intro}

\input{main/2_Related_work}

% \input{main/3_1_preliminary}

\input{main/3_Approach}

\input{main/4_Experiment}

\input{main/5_Results}

\input{main/6_Conclusion}




% \section*{Acknowledgements}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for 
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan, 
% NAACL 2017 by Margaret Mitchell, 
% ACL 2012 by Maggie Li and Michael White, 
% ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
% ACL 2002 by Eugene Charniak and Dekang Lin, 
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% % Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
\bibliographystyle{acl_natbib}
\clearpage

\input{main/Appendix}
% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
