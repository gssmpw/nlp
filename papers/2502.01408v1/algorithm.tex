\section{Algorithm for finding tree}\label{sec:algorithm}

In this section we first show that our problem is \np-hard and
present our greedy method for finding trees.

\subsection{Computational complexity of \prblagy}

Finding rank $r$ minimizing $\score{G, r}$ can be done
in polynomial time. However, our problem, that is, finding label tree
minimizing $\labelscore{G, T}$ turns out to be \np-hard.

\begin{proposition} 
\label{prop:np}
\prblagy is \np-hard.
\end{proposition} 

See Appendix (in the extended version of the paper) for all the proofs.

The proof of Proposition~\ref{prop:np} shows
that deciding whether there is a tree $T$ yielding
$\labelscore{G, T} = 0$ is \np-complete. This immediately implies that
there is no approximation algorithm for \prblagy since any approximation
algorithm should be able to find optimal tree $T$ if there is a solution
yielding $\labelscore{G, T} = 0$.


\begin{corollary}
\prblagy is inapproximable, unless $\poly=\np$.
\end{corollary}

The proof of
Proposition~\ref{prop:np} relied that we set cardinality constraint $k$.
It turns out that the problem is also \np-hard even if the constraint
is not enforced.

\begin{proposition} 
\label{prop:np2}
\prblagy is \np-hard, even with $k = \infty$.
\end{proposition} 


\subsection{Greedy algorithm}

% Since we cannot solve--or even approximate the constrained version---our
% problem, we need to resort to heuristics.
Since we cannot solve \prblagy efficiently  or even approximate the constrained version of our
problem, we need to resort to heuristics.

We approach the problem with a greedy method: starting from an
empty tree, we find the best possible split, perform the split
if there is a gain in score, and recurse on both leaves.
The pseudo-code for the algorithm is given in Algorithm~\ref{alg:parttion-fast}.

Here, we ignore any cardinality constraint. We will address the constraint in Section~\ref{sec:card}.

\algpartition uses two subroutines: \algsplit to compute the gain
of split candidate, and \algleft to update the tree.
In the next two sections, we discuss how these two subroutines
can be implemented efficiently.


\begin{algorithm}[ht!]
\caption{$\algpartition(\alpha)$. Calls \algsplit to find the best split. Calls \algleft to update the structures. Recurses to \algpartition for further splits.}
\label{alg:parttion-fast}

find optimal label $t$ and criterion $c$ using \algsplit\;
$\Delta \define $ reduction in score when splitting with $(t, c)$\;


\If {$\Delta < 0$} {
	
	${\beta,\gamma} \define \algleft(\alpha, t, c)$\;
	$\gain{\alpha} \define \Delta $\;
	
% 	Remove $t_{min}$ from $\labeldic{\alpha}$\;
	$\algpartition(\beta)$; $\algpartition(\gamma)$\;
   
}

\end{algorithm}

\subsection{Computing gain}

The computational bottleneck of the greedy algorithm is finding the next split:
Given a rank function, computing agony from scratch requires $\bigO{m}$ time.
Moreover, this calculation needs to be done for every viable label.

Luckily, we can speed this process by adopting the ideas presented
by~\citet{nikolaj2017tiers}, where the authors propose a divide-and-conquer algorithm
for finding ranks with low agony $\score{G, r}$. Since the original algorithm does not
use any label information, we will modify the approach so that it can be used for our problem.

More formally, we will maintain several counters that allow us to compute
agony without enumerating over the edges.

Let $T$ be a current tree and let $\alpha$ be a leaf in $T$ with a rank $i$.
Let $U$ be the vertices with smaller rank, that is,
\[
	U = \set{v \in U \mid r(T, L(v)) < i}\quad.
\]
Similarly, let $W$ be the vertices with the larger rank
\[
	W = \set{v \in U \mid r(T, L(v)) > i}\quad.
\]

We maintain 3 counters for each leaf and each vertex: Firstly, we maintain the total weight of backward
edges that go over $\alpha$,
\begin{equation}
	\back{\alpha} = \sum_{e \in E(W, U)} w(e),
	\label{eq:count1}
\end{equation}
secondly we maintain the total weight of backward edges from higher ranks to $V(\alpha)$,
\begin{equation}
	\inback{\alpha} = \sum_{\mathclap{v \in V(\alpha)}}\inback{v}, \quad \text{where}\quad  \inback{v} = \sum_{\mathclap{e \in E(W, v)}} w(e),
	\label{eq:count2}
\end{equation}
and we maintain the total weight of backward edges from $V(\alpha)$ to lower ranks,
\begin{equation}
	\outback{\alpha} = \sum_{\mathclap{v \in V(\alpha)}}\outback{v}, \quad \text{where}\quad  \outback{v} = \sum_{\mathclap{e \in E(v, U)}} w(e)\ .
	\label{eq:count3}
\end{equation}

Finally, for each vertex $v$ in $\alpha$ we maintain the total weight of backward incoming edges
minus the total weight of backward outgoing edges,
\begin{equation}
	\diff{v} = \sum_{e \in E(W \cup V(\alpha), v)} w(e) - \sum_{e \in E(v, U \cup V(\alpha))} w(e)\ .
	\label{eq:count4}
\end{equation}

We will use the following result to compute the gain.


\begin{proposition}[Proposition~8~in~\citep{nikolaj2017tiers}]
\label{prop:split}
Let $\alpha$ be a leaf of a tree $T$. 
Assume a new tree $T'$, where we have split $\alpha$ to two leaves. Let $Y_1$ be the
vertex set of the new left leaf, and $Y_2$ the vertex set of the new right leaf. Then
the score difference is
\begin{equation}
\label{eq:gain1}
	\score{T'} - \score{T} = \back{\alpha} + \inback{\alpha} - \sum_{y \in Y_2} \diff{y} 
\end{equation}
that can be rewritten as
\begin{equation}
\label{eq:gain2}
	\score{T'} - \score{T} = \back{\alpha} + \outback{\alpha} + \sum_{y \in Y_1} \diff{y}\quad. 
\end{equation}
\end{proposition}

We make two observations:
(1) We do not need to enumerate over edges.
(2) We can test both cases for split $(t, c)$ using only $\labeldic{\alpha, t}$:
if $c = \mathit{true}$,
then we set $Y_1 = \labeldic{\alpha, t}$ and use Eq.~\ref{eq:gain2},
if $c = \mathit{false}$,
then we set $Y_2 = \labeldic{\alpha, t}$ and use Eq.~\ref{eq:gain1}.
The pseudo-code for this procedure is given in Algorithm~\ref{alg:split}.


\begin{algorithm}[ht!]
\caption{$\algsplit(\alpha,t)$, computes the gain difference of $\alpha$ due to
split based on label $t$ and criterion $c$. Tests both criteria $c = \mathit{true}$
and $c = \mathit{false}$ and returns the better.
}
\label{alg:split}
	%$U \define \labeldic{\alpha,t}$\;
	$\Delta_1 \define \back{\alpha} + \outback{\alpha}  + \sum_{y \in \labeldic{\alpha,t}} \diff{y; \alpha}$\; 
	$\Delta_2 \define \back{\alpha} + \inback{\alpha} - \sum_{y \in \labeldic{\alpha,t}} \diff{y; \alpha}$\; 
	\Return $\min (\Delta_1, \Delta_2)$, $\Delta_1 \leq \Delta_2$\;
\end{algorithm}


\subsection{Maintaining the tree}

Once we have found the best candidate for splitting $\alpha$, we need to split
the leaf $\alpha$ into new leaves $\beta$, and $\gamma$.

More specifically, we need to update the counters $\back{\cdot}$,
$\inback{\cdot}$, $\outback{\cdot}$, and $\diff{\cdot}$, as well as compute
$V(\beta)$ and $V(\gamma)$. Here, we modify the algorithm in~\citep{nikolaj2017tiers} to suit
our needs.

Computing the vertex sets $V(\beta)$ and $V(\gamma)$ can be done
in $\bigO{\abs{V(\alpha, t)}}$ time by first moving $V(\alpha)$
to one of the child, say $\gamma$, in constant time,
and then moving the extra nodes in $\bigO{\abs{V(\alpha, t)}}$ time.

In order to update the counters we will need to iterate over the cross-edges between
$\beta$ and $\gamma$. We can do this iteration either by going over the edges
adjacent to $V(\beta)$, or by going over the edges adjacent to $V(\gamma)$. 

Both approaches will yield the same result, so we use the one that
visits fewer unnecessary edges. More specifically: if $\abs{V(\beta)} + \abs{E(\beta)} \leq \abs{V(\gamma)} + \abs{E(\gamma)}$,
enumerate over $V(\beta)$, otherwise we enumerate over $V(\gamma)$.

The pseudo-code for \algleft is given in Algorithm~\ref{alg:left}.
A straightforward calculation, which we will omit, starting from Eqs.~\ref{eq:count1}--\ref{eq:count4}
demonstrates that Algorithm~\ref{alg:left} maintains the counters correctly.
Moreover, since we need the cross-edges between $\beta$ and $\gamma$ only
once, we will delete them as we are processing them.




\begin{algorithm}[ht!]
\caption{$\algleft(\alpha)$, splits $\alpha$}
\label{alg:left}
	$N \define \labeldic{\alpha, t, c}$\;
	$P \define V(\alpha) \setminus N$\;
	
	create a new leaf $\beta$ with  $V(\beta) = N$\; 

	create a new leaf $\gamma$ with  $V(\gamma) = P$\;

	\uIf {$\abs{V(\beta)} + \abs{E(\beta)} \leq \abs{V(\gamma)} + \abs{E(\gamma)}$} {

		$\inback{\beta} \define \sum_{x \in N} \inback{x}$;
		$\inback{\gamma} \define \inback{\alpha} - \inback{\beta}$\;
		$\outback{\beta} \define \sum_{x \in N} \outback{x}$;
		$\outback{\gamma} \define \outback{\alpha} - \outback{\beta}$\;
		$\back{\beta} \define \back{\alpha} + \outback{\gamma}$;
		$\back{\gamma} \define \back{\alpha} + \inback{\beta}$\;

		\ForEach {$x \in N$} {
			\ForEach {$e = (z, x)$ such that $z \in P$} {
				add $w(e)$ to $\inback{x}$, $\inback{\beta}$, $\outback{z}$, $\outback{\gamma}$\;
				delete $e$\;
			}
			\ForEach {$e = (x, z)$ such that $z \in P$} {
				decrease $\diff{x}$, $\diff{z}$ by $w(e)$\;
				delete $e$\;
			}
		}
	}
	\Else {
		$\inback{\gamma} \define \sum_{x \in P} \inback{x}$;
		$\inback{\beta} \define \inback{\alpha} - \inback{\gamma}$\;
		$\outback{\gamma} \define \sum_{x \in P} \outback{x}$;
		$\outback{\beta} \define \outback{\alpha} - \outback{\gamma}$\;
		$\back{\beta} \define \back{\alpha} + \outback{\gamma}$;
		$\back{\gamma} \define \back{\alpha} + \inback{\beta}$\;


		\ForEach {$z \in P$} {
			\ForEach {$e = (z, x)$ such that $x \in N$} {
				add $w(e)$ to $\inback{x}$, $\inback{\beta}$, $\outback{z}$, $\outback{\gamma}$\;
				delete $e$\;
			}
			\ForEach {$e = (x, z)$ such that $x \in N$} {
				decrease $\diff{x}$, $\diff{z}$ by $w(e)$\;
				delete $e$\;
			}
		}
	}
    \Return $\beta,\gamma$ \;
\end{algorithm}


Next we prove the computational complexity of \algpartition.


\begin{proposition}
\label{prop:time}
The running time of
\algpartition is in $\bigO{(n + m) \log n + \ell R}$,
where $R = \sum_v \abs{L(v)}$ is the number of node-label pairs in $G$,
and $\ell$ is the number of nodes in the resulting label tree.
\end{proposition}

\subsection{Applying cardinality constraint by pruning}
\label{sec:card}

As our last step, we look on how to enforce possible cardinality constraint.
Given the cardinality constraint $k$ and a label tree $T$ produced by \algpartition, we can reduce the
number of leaves by pruning some branches of $T$.  The optimal subtree can be
obtained using dynamic programming, as proposed by~\citet{nikolaj2017tiers}.

Let us define $\opt{\alpha; h}$ be the optimal gain that obtained in the branch in $T$ starting from the node $\alpha$
by using only $h$ leaves (and pruning the remaining nodes).

If $\alpha$ is the root of $T$, then $\opt{\alpha; k}$
is the optimal agony achieved by pruning $T$ to have only $k$ leaves.
We initialize the array by setting  $\opt{\alpha; 1} = 0$ for any $\alpha$, and $\opt{\alpha; h} = 0$ if $\alpha$ is a leaf in $T$. If $\alpha$ is a non-leaf and $k > 1$, then we use the identity.
\[
	\opt{\alpha; h} = \gain{\alpha} + \min_{1 \leq \ell \leq h - 1} \opt{\beta; \ell} + \opt{\gamma; h - \ell},
\]
where $\beta$ is the left child and $\gamma$ is the right child, and $\gain{\alpha}$ is the improvement in agony as recorded in \algpartition.
To perform the trace-back, we further record the optimal index $\ell$ in a different table. Computing  $\opt{\alpha; h}$ requires $\bigO{k}$ time. We compute
at most $\bigO{nk}$ entries which leads to $\bigO{nk^2}$ running time.
