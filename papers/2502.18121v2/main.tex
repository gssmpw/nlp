\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{caption}
\usepackage[subrefformat=parens]{subcaption}
\usepackage{adjustbox}


\pdfinfo{
   % /Author (Anonymous)
   /Author (Ryo Takizawa, Izumi Karino, Koki Nakagawa, Yoshiyuki Ohmura, Yasuo Kuniyoshi)
   /Title  (Enhancing Reusability of Learned Skills for Robot Manipulation via Gaze and Bottleneck)
   /CreationDate (D:20241227165300)
   /Subject (Imitation Learning for Robot Manipulation)
   /Keywords (Robot Manipulation, Deep Imitation Learning, Generalization, Gaze, Eye-hand Coordination, Autonomous Agent)
}

\begin{document}

% paper title
\title{Enhancing Reusability of Learned Skills for Robot Manipulation via Gaze and Bottleneck}

% % You will get a Paper-ID when submitting a pdf file to the conference system
% \author{Author Names Omitted for Anonymous Review. Paper-ID 72}

\author{
\authorblockN{Ryo Takizawa\authorrefmark{1},
Izumi Karino,
Koki Nakagawa, 
Yoshiyuki Ohmura and
Yasuo Kuniyoshi}
\authorblockA{The University of Tokyo}
\authorblockA{\authorrefmark{1}Indicates Corresponding Author}
}




\maketitle

    


\begin{abstract}
Autonomous agents capable of diverse object manipulations should be able to acquire a wide range of manipulation skills with high reusability. Although advances in deep learning have made it increasingly feasible to replicate the dexterity of human teleoperation in robots, generalizing these acquired skills to previously unseen scenarios remains a significant challenge.
In this study, we propose a novel algorithm, Gaze-based Bottleneck-aware Robot Manipulation (GazeBot), which enables high reusability of the learned motions even when the object positions and end-effector poses differ from those in the provided demonstrations. By leveraging gaze information and motion bottlenecks—both crucial features for object manipulation—GazeBot achieves high generalization performance compared with state-of-the-art imitation learning methods, without sacrificing its dexterity and reactivity. Furthermore, the training process of GazeBot is entirely data-driven once a demonstration dataset with gaze data is provided.
Videos and code are available at \href{https://crumbyrobotics.github.io/gazebot}{https://crumbyrobotics.github.io/gazebot}.
\end{abstract}






\IEEEpeerreviewmaketitle









\section{Introduction} 
Recent advancements utilizing powerful neural networks such as Transformers have made deep imitation learning increasingly capable of reproducing dexterity to a certain extent \cite{Zhao2023, Chi2023, Kim2024}.
However, significant issues persist regarding their generalization capabilities. 
Although generalization in object manipulation occurs at multiple levels, even the most fundamental aspects, such as changes in object position and the end-effector pose, are known to cause drastic reductions in success rates with variations of just a few centimeters \cite{Cheng2024}.
For instance, ACT \cite{Zhao2023}, a model recognized for its strong dexterous capabilities, has only been validated with objects placed on white tape with an accuracy of approximately 5 cm. 
Although ACT demonstrated high success rates under these specific conditions in our experiments, it was unable to reach objects placed in unseen positions (Figure \ref{fig:overview}), highlighting the poor generalization capabilities acquired through this method.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/overview2.pdf}
    {\setlength{\belowcaptionskip}{-2pt}
    \caption{\textbf{GazeBot} achieves high reusability of learned skills for unseen object positions and end-effector poses. Demonstrations collected within restricted ranges of object positions and end-effector poses, and then the success rate is evaluated for in-distribution (\textbf{ID}) cases within these ranges and out-of-distribution (\textbf{OOD}) cases outside them.}
    \label{fig:overview}}
\end{figure}

The purpose of this study is to achieve imitation that can accurately perform demonstrated object manipulations under various object positions and initial end-effector poses. 
Owing to the limited generalization capability of conventional imitation learning, exhaustive demonstration collection is currently required to ensure that the robot behaves correctly under various object positions and end-effector poses \cite{Brohan2022, Kim2024b, Moo2024}. 
From a short-term perspective such as automating a certain task, it might be sufficient to collect data until the desired capability is achieved.
However, for enabling robots to autonomously and adaptively expand human-aligned behaviors in unknown environments, it is essential that actions learned from limited demonstrations can be reused in slightly different situations.
To address this need, we propose Gaze-based Bottleneck-aware Robot Manipulation (\textbf{GazeBot}), an object manipulation imitation method that enables the reuse of acquired skills even with object positions and end-effector poses not included in the provided demonstrations.
As illustrated in Figure \ref{fig:overview}, the object positions and initial end-effector poses used in the demonstrations are restricted to a designated region to evaluate the generalization performance.
In this setting, GazeBot can accurately perform the task not only under in-distribution (\textbf{ID}) conditions (i.e., within the designated region) but also under out-of-distribution (\textbf{OOD}) conditions (i.e., outside the designated region).

To reuse skills learned within the ID domain in OOD situations, it is necessary to (1) establish an object representation that is robust to changes in object position and (2) develop an action policy architecture capable of accurate control under previously unseen end-effector poses. 
To achieve this, we draw inspiration from human object manipulation, where \textbf{gaze} on the target object not only provides a visual representation independent of the object's absolute position but also exhibits strong gaze–hand coordination during end-effector movements such as reaching \cite{Johansson2001, Hayhoe2003}. 
Concretely, we first represent the entire field of view using a 3D point cloud and then crop a cubic region around the gaze position---referred to as \textbf{gaze-centered point cloud}---from the entire point cloud.
By using this gaze-centered point cloud as input to the action prediction, we realize an object representation that is robust to variations in object position.
Next, based on the action predictivity in the gaze-centered point cloud, we perform a data-driven action segmentation of the overall manipulation into (1) a reaching motion to the vicinity of the gaze position and (2) a gaze-centered dexterous action. 
We define the temporal boundary between these motions as a \textbf{bottleneck}, and predict the \textbf{bottleneck pose}, the end-effector pose at this bottleneck, from the 3D gaze position and gaze-centered point cloud. 
This approach allows accurate prediction of the bottleneck pose even for unseen object positions, and this prediction is independent of the current end-effector pose.
Consequently, GazeBot can handle various object positions and end-effector poses by first executing a rough reaching motion to the bottleneck pose, then performing a gaze-centered dexterous action using only the gaze-centered point cloud. This approach enables the reuse of the learned motion in unseen conditions. 
Here, reaching the bottleneck is achieved by generating an end-effector trajectory that smoothly connects the current end-effector pose and the bottleneck pose using a first-order Bézier curve, which provides sufficient expressive power for the reaching motion while avoiding unnecessary complexity that could compromise the model's generalization. 
Furthermore, GazeBot updates all actions at every step, and a fully parametric method based on a Transformer is used to directly output subsequent end-effector poses for the gaze-centered dexterous actions. 
This approach ensures that dexterity and reactivity are not sacrificed in the pursuit of generalization.

In summary, our key results and contributions are as follows:
\begin{itemize}
    \item We propose a \textbf{gaze-centered point cloud} as a visual representation that is invariant to the object position.
    \item We introduce a method for data-driven action segmentation based on \textbf{bottleneck} determination that clarifies which actions can be reused.
    \item We implement a reaching method to the bottleneck pose that remains accurate even with unseen object positions and end-effector poses.
    \item We develop a novel imitation learning model, \textbf{GazeBot}, which demonstrates superior generalization performance compared with state-of-the-art models by leveraging gaze and bottlenecks.
\end{itemize}













\section{Related Work}
\textbf{Gaze-based Object Manipulation.} 
Some works have previously proposed imitation learning methods using gaze data collected from a remote human operator during teleoperated demonstrations for action prediction \cite{Kim2020, Kim2021, Kim2024}. These gaze-based methods have previously exhibited advantages such as enhanced robustness by disregarding task-irrelevant objects \cite{Kim2020} and improved dexterity by focusing on task-relevant regions of visual inputs \cite{Kim2021, Kim2024}. 
However, because these methods rely on image cropping for gaze-centered images, they are susceptible to visual variations caused by changes in object position.
In this study, we employ a gaze-centered point cloud that is robust to positional changes, and our method further improves the reusability of acquired skills for unseen object positions and end-effector poses, in addition to the conventional benefits of gaze.

\textbf{Data-driven Action Segmentation.} 
Segmenting actions into reaching motions and dexterous actions has been proposed to improve dexterity \cite{Kim2021}, increase success rates for long-horizon tasks \cite{Kim2024, Belkhale2023, Sundaresan2024}, and enable high generalization capabilities \cite{Johns2021, Sundaresan2024}. 
Kim et al. proposed data-driven segmentation methods based on end-effector velocity \cite{Kim2021} or the visibility of the end-effector within a gaze-centered image \cite{Kim2024}. However, these approaches often fail in tasks where high dexterity is not required or when the end-effector is not visible during dexterous actions, such as manipulating with a long stick.
In contrast, our approach segments motions at bottlenecks, which are determined based on action predictivity in the gaze-centered point cloud. 
This action predictivity-based approach offers a more general and data-driven segmentation scheme compared with these previous methods.

\textbf{Generalization to Out-of-Distribution.} 
Generalization to out-of-distribution scenarios is required across diverse levels and factors \cite{Bharadhwaj2023}, including adaptation to a variety of object poses and unseen objects in the same category \cite{Simeonov2021, Wang2024, Gao2024}, to changes in the environment such as varying backgrounds, camera positions, or distractor objects \cite{Zhu2023, Yu2023, Xie2024}, and to entirely novel objects and tasks \cite{Shen2023, Stone2023}. 
However, to the best of our knowledge, there have been no studies examining enhanced generalization to out-of-distribution object positions or end-effector poses, which is the main focus of this study.
In other words, our proposed GazeBot is the first method in the general deep imitation learning framework to demonstrate the high reusability of imitated skills even under unseen object positions and end-effector poses.
Although several methods exhibit similarities with GazeBot, they have not successfully demonstrated such high reusability, primarily owing to issues in the design of the action policy. 
Hydra \cite{Belkhale2023} and SPHINX \cite{Sundaresan2024}, for instance, segment actions into a reaching phase and a dexterous action phase, using sparse action representations similar to a bottleneck pose for the reaching motion. 
However, in contrast to GazeBot, these sparse representations are directly estimated by a neural network from entire images and the end-effector poses, and dexterous action prediction also relies on absolute information such as the end-effector poses or entire images. As we will see in Section \ref{sec:experiments}, these design changes hinder the accurate extrapolation of the reaching motions and reduce the reusability of the learned dexterous actions when faced with unseen object positions or end-effector poses.













\section{Gaze-based Bottleneck-aware Robot Manipulation}
\label{sec:method}
In this section, we propose a novel policy model named GazeBot that improves the reusability of learned skills while retaining dexterity and reactivity. 
We begin by explaining how gaze and bottlenecks contribute to the enhanced reusability of imitated motions (Section \ref{sec:gaze-bottleneck}). We then detail the data-driven action segmentation based on gaze and bottlenecks for decomposing demonstrations (Section \ref{sec:segmentations}). Finally, we describe the design of the policy model (Section \ref{sec:model}).

\subsection{Gaze and Bottleneck}
\label{sec:gaze-bottleneck}

\subsubsection{Gaze-based Visual Representation}
Unlike a standard camera, human vision does not uniformly perceive the entire visual field, but instead distinguishes between a high-resolution foveal (central) region and a lower-resolution peripheral region \cite{Paillard1996}. This foveal region can be seen as a 3D attention mechanism that selects a specific portion of space for detailed processing. 
Kim et al. previously proposed an imitation learning method that implements a foveal vision system for robot manipulation \cite{Kim2020}. In that approach, gaze data of a remote human operator are measured during teleoperated demonstrations, and a portion of the image input is cropped around the operator’s gaze position to simulate human-like gaze-based vision. During the inference phase, a gaze prediction model trained on the measured gaze data provides the online gaze control.

However, the conventional image-cropping approach lacks 3D-awareness, causing substantial changes in visual representation when the object position is varied (Figure \ref{fig:gaze-vision}).
To address this, we extract a region of the point cloud around the 3D gaze coordinates rather than cropping a 2D image. Here, we use stereo vision to estimate depth and then convert the pixel coordinates of the gaze to 3D coordinates. As shown in Figure \ref{fig:gaze-vision}b, this gaze-centered point cloud reduces the sensitivity to changes in object positions and end-effector poses compared with conventional 2D cropping methods (Figure \ref{fig:gaze-vision}a), thereby enabling neural networks to make predictions independent of object position and end-effector pose.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/gaze-based-vision.pdf}
    \caption{Although both the left and right scenes represent a similar state, their positions on the table differ. In conventional gaze-centered image (a), which lacks 3D-awareness, the scenes appear substantially different, whereas in our proposed \textbf{gaze-centered point cloud} (b), their underlying three-dimensional structure is captured as similar.}
    \label{fig:gaze-vision}
\end{figure}

\subsubsection{Bottleneck-aware Action Segmentation}
\label{sec:bottleneck}
% Conventional methods for robot object manipulation often suffer from limited generalization. One contributing factor is the lack of inductive bias in the architectures used: most of today’s widely adopted policy models are designed to handle diverse tasks ranging from object manipulation to locomotion and navigation \cite{Muhammad2022, Doshi2024}. Yet, just as biological species evolve specialized characteristics, it is crucial to incorporate suitable biases into the policy model so that learning for object manipulation can be done more efficiently—without sacrificing generality.

In human object manipulation, gaze position and hand movement are strongly coupled both temporally and spatially \cite{Johansson2001, Paillard1996}.
As illustrated in Figure \ref{fig:bottleneck}, a gaze-centered point cloud makes it possible to segment the movement into two distinct phases: (1) a reaching motion toward the vicinity of the gaze, and (2) a gaze-centered dexterous action. This segmentation is possible because gaze-centered vision captures only the object during the reaching phase, whereas it captures both the object and end-effector during the dexterous action phase.
Here, we define the temporal boundary between these two actions as a bottleneck.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/bottleneck.pdf}
    \caption{By observing object manipulation in gaze-centered point cloud, an action can be segmented into two phases at the \textbf{bottleneck}: (1) the reaching motion and (2) the gaze-centered dexterous action. The motion after the bottleneck is reusable irrespective of the object position and the initial end-effector pose.}
    \label{fig:bottleneck}
\end{figure}

While fully parametric methods, in which neural networks directly predict action trajectories, can achieve high dexterity and reactivity, it has been challenging to extrapolate learned behaviors to unseen object positions and end-effector poses.
To address this, we restrict the use of fully parametric action prediction to the gaze-centered dexterous actions derived from the aforementioned bottleneck-aware action segmentation. In this context, the robot first reaches the bottleneck pose in a goal-fixed manner using Bézier curve approximation, and then switches to the fully parametric approach for the gaze-centered dexterous action.
Here, if (A) the bottleneck estimation is independent of both object position and end-effector pose, and (B) the fully parametric gaze-centered dexterous action prediction relies solely on the relative spatial relationships between the object and end-effector (or the grasped object), it becomes possible to accurately perform the learned motions even under previously unseen object positions and initial end-effector poses without sacrificing dexterity or reactivity.

As the bottleneck pose is defined in a gaze-centered manner, it can be described as an offset from the 3D gaze position:
\begin{align}
    p_b &= p_{\text{gaze}} + p^{\text{relative}}_b \label{eq:bottleneck1}, \\ 
        &= p_{\text{gaze}} + f(g) \label{eq:bottleneck2}, 
\end{align}
where $p_b, p_{\text{gaze}},$ and $p^{\text{relative}}_b$ denote the bottleneck pose, the 3D gaze position in the end-effector coordinate frame, and the offset of the bottleneck pose from the gaze position, respectively, and $f(\cdot)$ represents a neural network that takes the gaze-centered point cloud $g$ as input.
Here, the bottleneck pose is estimated as an offset from the gaze position rather than being directly predicted based on the gaze-centered point cloud (Eq. \ref{eq:bottleneck2}).
Notably, this offset is independent of the object position. 
As a result, the bottleneck pose can be accurately estimated even when the object position is unseen. Moreover, as Eq. \ref{eq:bottleneck1} does not involve the current end-effector pose, the accuracy of bottleneck estimation is not affected even with unseen end-effector poses.

\subsection{Data-driven Demonstration Segmentation}
\label{sec:segmentations}

We first temporally segment the provided demonstrations in a data-driven manner based on gaze and bottlenecks before training an action policy.
Specifically, we decompose the demonstrations into multiple sub-tasks (Section \ref{sec:sub-task}), and then further segment each sub-task into reaching motions and gaze-centered dexterous actions according to bottlenecks (Section \ref{sec:bottleneck-determination}). 
The corresponding action policy modules are trained using the sets of reaching motions and gaze-centered dexterous actions obtained through these processes.
The decomposition into sub-tasks is performed so that each sub-task includes exactly one bottleneck by using the task decomposition method based on gaze transitions \cite{Takizawa2024}. 
The proposed method for demonstration segmentation is summarized in Algorithm \ref{alg:segmentations}.

\subsubsection{Gaze-Based Sub-task Segmentation}
\label{sec:sub-task}
A typical object manipulation task comprises multiple bottlenecks. For example, in a standard pick-and-place task, there is usually one bottleneck for the “pick” phase and another for the “place” phase. In such cases, before segmenting the motions using bottlenecks, it is necessary to decompose a sequence of object manipulation behaviors into multiple smaller sub-tasks (e.g., pick/place in this example).

As a robust and simple way to achieve such segmentation in a data-driven manner, previous work proposed a gaze-based task decomposition method for object manipulation \cite{Takizawa2024}. This approach exploits the pattern of gaze during object manipulation, where the human teleoperator fixates on specific task-relevant gaze landmarks. By simply detecting transitions in this gaze behavior, the object manipulation task can be segmented so that each fixation period corresponds to a distinct sub-task. In this work, we first apply this method to decompose expert demonstrations into multiple sub-tasks.

\subsubsection{Bottleneck Determination}
\label{sec:bottleneck-determination}
Once the demonstrations are segmented into sub-tasks, we need to determine the time step at which each sub-task reaches its bottleneck pose. 
To do so, we propose a bottleneck determination approach based on action predictivity from a gaze-centered point cloud.
In each sub-task, predicting actions solely from a gaze-centered point cloud exhibits low accuracy when the end-effector or grasped object is not visible within the gaze-centered point cloud.
Therefore, the transition from this low-predictivity phase to a high-predictivity phase can serve as the segmentation point, which we define as the bottleneck (Figure \ref{fig:bottleneck-determination}).

In this approach, we use a policy model $a_t = h(g_t)$ that predicts the action solely from the gaze-centered point cloud $g_t$. This policy model is trained via behavior cloning \cite{Zhang2018}, that is, by minimizing $\|h(g_t) - a_t^*\|$ to replicate the actions in the demonstration, where $a_t^*$ denotes the expert action recorded in the demonstration. 
Once this policy $h$ has been trained, we compute the action-prediction loss $\|h(g_t) - a_t^*\|$ at every time step of each demonstration. 
As illustrated in Figure \ref{fig:bottleneck-determination}, we split each sub-task based on the median of this action prediction loss, enabling entirely data-driven segmentation for every sub-task.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/bottleneck-determination.pdf}
    \caption{We can determine the bottleneck based on the action predictivity by training a model to predict actions solely from the gaze-centered point cloud.}
    \label{fig:bottleneck-determination}
\end{figure}

\begin{algorithm}[t]
\caption{Data-driven Demonstration Segmentation}
\label{alg:segmentations}
\begin{algorithmic}[1]
  \Statex
  \textbf{Given:} Demonstrations 
  \[
  D = \{(o_t^{(i)}, p_t^{(i)}, g_t^{(i)})\}_{i=1}^{N},\quad t\in[0,T^{(i)}],
  \]
  where $o_t$, $p_t$, $g_t$ denote observation (point cloud), end-effector pose, and gaze position.
  \Statex
  \textbf{Notation:} For the $k$-th sub-task of the $i$-th demonstration: 
  \[
  s^{(i)}_k \text{ (start)},\quad e^{(i)}_k \text{ (end)},\quad b^{(i)}_k \text{ (bottleneck)}
  \]
  
  \State \textbf{(1) Gaze-based Action Segmentation:}
  \For{each demonstration \(i\)}
    \State Partition \([0, T^{(i)}]\) into \([s^{(i)}_0,e^{(i)}_0],\dots,[s^{(i)}_K,e^{(i)}_K]\)
  \EndFor
  
  \State \textbf{(2) Train Action Prediction Model $h_\theta$:}
  \[
  \theta^*=\arg\min_\theta \sum_{(o_t,a_t,g_t)\in D}\|a_t-h_\theta(\text{crop}(o_t,g_t))\|^2
  \]
  
  \State \textbf{(3) Compute Action Predictivity:}
  \For{each demonstration \(i\) and sub-task \(k\)}
    \State \(\text{scores}_k=\{\|a_t-h_{\theta^*}(\text{crop}(o_t,g_t))\|^2\}_{t=s^{(i)}_k}^{e^{(i)}_k}\)
  \EndFor
  
  \State \textbf{(4) Detect Bottleneck:}
  \For{each sub-task \([s^{(i)}_k,e^{(i)}_k]\)}
    \State Partition \([s^{(i)}_k,e^{(i)}_k]\) into \([s^{(i)}_k, b^{(i)}_k-1]\) and \([b^{(i)}_k, e^{(i)}_k]\)
  \EndFor

  \Statex 
  \textbf{Return:} $D$ and $\{s^{(i)}_k, e^{(i)}_k, b^{(i)}_k\}_{i,k}$
\end{algorithmic}
\end{algorithm}

\subsection{Policy Design}
\label{sec:model}

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\linewidth]{figures/model-architecture.pdf}
        \caption{\textbf{GazeBot} architecture. (left) The gaze prediction model is trained to estimate the gaze position across the entire image as a classification problem. (right) The action policy model achieves robust reaching motion by estimating the bottleneck pose and the shape of the trajectory up to the bottleneck, and uses a Transformer to predict the gaze-centered dexterous action in a full-parametric manner. Both actions and gaze transitions are predicted by the gaze-centered point cloud to improve the reusability.}
        \label{fig:model-architecture}
    \end{center}
\end{figure*}

At every time step, GazeBot first predicts the gaze position from the entire image using the gaze prediction model (Section \ref{sec:gaze-model}). It then uses the gaze-centered point cloud at the predicted gaze position, along with the current end-effector pose, as inputs to the action policy model (Section \ref{sec:policy-model1} and \ref{sec:policy-model2}), which reactively predicts the subsequent end-effector poses.
The action policy model consists of two main modules: (i) a module that predicts gaze-centered dexterous actions solely from gaze-centered point clouds, and (ii) a module that predicts bottleneck poses based on gaze-centered point clouds and 3D gaze positions, as well as \textit{control points} of first-order Bézier curves to connect the current end-effector and the predicted bottleneck pose. 
These modules are trained using, respectively, (i) post-bottleneck motions and (ii) reaching trajectories leading to the bottleneck pose, both obtained from demonstrations as described in the previous section \ref{sec:bottleneck-determination}. 

\subsubsection{Gaze Prediction Model}  
\label{sec:gaze-model}
For gaze-based object manipulation, it is necessary for the robot to acquire gaze information during both training and inference. 
In response, Kim et al. proposed a method that takes advantage of the common practice of collecting real-robot demonstrations via human teleoperation \cite{Kim2020}. 
During demonstration collection, the teleoperator’s gaze is measured simultaneously, and this gaze data is then used as supervision for a gaze prediction model, which predicts gaze positions across the entire image. The gaze positions estimated by the gaze prediction model are used during both training and inference.

We adopt a similar approach in this study. In particular, we employ an architecture similar to the one used in DAA \cite{Kim2024b}. As shown in the left side of Figure \ref{fig:model-architecture}, we replace the image token extraction via EfficientNetV2 with DINOv2 \cite{Oquab2023}, a newer and more powerful feature extraction model, and replace the cross-attention layers (originally intended for language inputs) with a set of four-layer multilayer perceptrons (MLPs) that map each of the 768-dimensional tokens to a single probability value.
To intentionally transition the gaze to the next gaze landmark upon completing each sub-task, we prepare one such MLP for each sub-task and switch these MLPs based on the sub-task index $i_{\mathrm{seg}}$.  
At inference time, the sub-task index $i_{\mathrm{seg}}$ is initialized to zero at the beginning and incremented based on the progress $c_t$ output by the action policy model, indicating the completion of each sub-task. 
By managing gaze transitions using $c_t$ estimated from the gaze-centered point cloud, which solely captures the relative spatial relationships between the object and end-effector (or grasped object), the robot can perform gaze transitions at the correct timing even under unseen conditions.

\subsubsection{Action Policy Model (i)}  
\label{sec:policy-model1}
The pixel-space gaze position predicted by the gaze prediction model is transformed into 3D coordinates using stereo-based depth estimation. 
From the stereo-derived point cloud, we then crop a cubic region (20 cm on each side in this study) centered on the 3D gaze coordinates, which is referred to as the gaze-centered point cloud. 
This gaze-centered point cloud is embedded as a sequence of tokens using a PointTransformer \cite{Zhao2020} and then passed to a Transformer encoder (Figure \ref{fig:model-architecture} right). 
The Transformer encoder includes a CLS token (\textit{classification token}, similar to ViT \cite{Dosovitskiy2020}) that aggregates information from all point-cloud tokens into a feature vector $f_{\mathrm{pcd}} \in \mathbb{R}^{512}$. 
All output tokens, including the CLS token, are then fed into a Transformer decoder, which generates both the action sequence $a_{t:t+H} \in \mathbb{R}^{H\times14}$ for the left and right end-effector over $H$ future time steps and the progress $c_t \in \mathbb{R}^{N_{seg}}$ for controlling gaze transitions in each sub-task. Here, each end-effector has 7 degrees of freedom including gripper angle, and $N_{seg}$ denotes the number of sub-tasks. 
As the input vision is gaze-centered, the output actions are represented as relative end-effector poses, with the current end-effector pose serving as the origin of the coordinate frame. 
By computing these actions in relative coordinates, the learned dexterous actions remain reusable even when the absolute position of the object changes.

\subsubsection{Action Policy Model (ii)}  
\label{sec:policy-model2}
The feature vector $f_{\mathrm{pcd}}$ is used to predict the bottleneck poses for each end-effector. 
As described in Section \ref{sec:bottleneck}, the bottleneck pose is estimated by first predicting its offset relative to the 3D gaze position, then adding this offset to the 3D coordinates of the gaze. The module depicted in the top-right of Figure \ref{fig:model-architecture} implements this procedure and enables significantly more accurate extrapolation of the bottleneck pose under unseen object positions.

After predicting the bottleneck pose, the corresponding reaching trajectory is generated. 
In this context, a simple straight-line movement sometimes suffices for the reaching motion, because the movement from the current end-effector pose to the bottleneck pose is non-contact and relatively coarse.
However, for some tasks the reaching trajectory consistently takes a particular shape. For example, when placing an object into a deep container, the reaching motion toward the container consistently forms an upwardly convex curve.
To capture such a characteristic shape without introducing unnecessary complexity that could compromise the model’s generalization, we generate the reaching trajectory from the current end-effector pose to the bottleneck pose by using a first-order Bézier curve. 
Furthermore, because the endpoint of the Bézier curve is fixed at the bottleneck pose, the robot can accurately reach the bottleneck without incurring a compounding error \cite{Zhao2023} that accumulates over time.

As illustrated in the lower-right portion of Figure \ref{fig:model-architecture}, we predict a 7-dimensional \textit{bezier vector} that determines the shape of the first-order Bézier curve for each end-effector.
A first-order Bézier curve is defined by its two endpoints and a single \textit{control point}. 
Because the curve lies in a 7-dimensional end-effector pose space, the combined control points for both arms are in 14 dimensions.
Rather than predicting the control point directly, we first estimate the displacement---bezier vector---from the mean pose of the start (current end-effector pose) and end (bottleneck pose) poses, and then add this bezier vector to the mean pose to obtain the control point (Figure \ref{fig:model-architecture} bottom-right).
For the training, we approximate each reaching motion in the demonstrations by fitting them with first-order Bézier curves, and then use the resulting bezier vectors from these curves for supervision.













\section{Experiments}
\label{sec:experiments}
\subsection{Robot System}
We used a dual-arm robot system designed for imitation learning via human teleoperation. %\cite{Kim2020}. 
In this system, a human operator remotely controls the robot while observing its surrounding environment through a head-mounted display (HMD). During this operation, the operator’s gaze data were recorded in sync with the video feed displayed on the HMD. The system is compatible with both a physical dual-arm robot—consisting of two UR5 (Universal Robots Inc.) arms—and its simulated counterpart, allowing the collection of teleoperated demonstration data in both real and virtual environments. % \cite{Hamano2022}. 
The recorded gaze data were output as pixel coordinates corresponding to the images displayed on the HMD. The same single stereo camera (ZED Mini, Stereolabs Inc.) was used for both the teleoperation and inference phases, and we directly utilized the depth images generated by the deep learning–based depth estimation algorithm provided by ZED SDK (Stereolabs Inc.). The time-series demonstration data used for training were recorded at 10 Hz.

\subsection{Task Setup}
\label{sec:data}
In this study, we conducted imitation learning experiments on three tasks (two real and one simulated) to evaluate the reusability of skills acquired by GazeBot in comparison with conventional models.
To quantitatively assess the generalization performance, we collected demonstrations under controlled conditions by restricting the object positions and initial end-effector poses within predefined regions, thus clearly defining ID and OOD situations:
\begin{itemize}
    \item \textbf{PenInCup} (real, 109 demos): The robot, operating on a bare tabletop (no tablecloth), picks up a blue marker pen with its left arm and places it into a red cup. Using a gaze-based action segmentation method, this series of actions is divided into two sub-tasks. The pen is always placed such that the part without the cap remains within a 15 cm square area on the left side of the table, and the cup is placed so that part of its base overlaps a 5 cm area on the right side of the table. The robot’s initial pose starts with its left and right end-effectors positioned above the left and right edges of the table, respectively, oriented inward. The object positions and orientations, as well as the robot’s initial pose, are manually randomized within their respective areas. All other factors (table location, background, etc.) are maintained as consistent as possible during the demonstration collection. To avoid introducing bias in the training images, we marked the boundaries of the object placement areas in a way that would be minimally visible.
    \item \textbf{OpenCap} (real, 110 demos): The robot, operating on a tabletop covered with a green tablecloth, holds an upright empty plastic bottle in place with its right arm and removes the cap with its left arm. This series of actions is divided into two sub-tasks. The bottle is always placed within the right side of the table. All other conditions are identical to those described in the PenInCup task.
    \item \textbf{PileBox} (sim, 100 demos): The robot, operating on a bare tabletop, picks up a red box with its left arm and stacks it on top of a green box. This series of actions is divided into two sub-tasks. The red box is always placed such that its center remains within a 10 cm × 20 cm rectangular region on the left side of the table, and the green box is placed such that its center remains within a 10 cm square region on the right side of the table. The robot’s initial pose starts with its left and right end-effectors positioned above the left and right edges of the table, respectively, oriented inward. Because this task is performed in a simulation environment, the object positions and orientations are fully randomized according to an uniform distribution, whereas the initial robot pose, table position, and background are all fixed.
\end{itemize}

\subsection{Evaluation of Generalization and Ablation}
To verify the improved reusability of learned skills provided by GazeBot, we first trained GazeBot and its ablation models, including conventional models (ACT \cite{Zhao2023}, DAA \cite{Kim2024b}), using the demonstrations collected as previously described in Section \ref{sec:data}.
We then measured their success rates for scenarios within the training distribution (ID) and those outside it (OOD), as shown in Figure \ref{fig:eval-generalization}. 
In the experiment, we standardized the initial conditions for all ID and OOD trials to enable an accurate comparison of each model’s generalization performance. 
Consequently, both GazeBot and the seven ablation models performed the task under conditions as similar as possible. 
In the OOD trials, we first evaluated the cases where each object was individually placed in unseen positions, and then examined the cases where all objects were simultaneously placed in unseen positions (Figure \ref{fig:eval-generalization}). Finally, we conducted trials involving unseen initial end-effector poses. All initial conditions used in the ID and OOD trials are provided in Appendix \ref{appendix-experiment}.

The overall success rates in ID and OOD trials for the PenInCup, OpenCap, and PileBox tasks are presented in Table \ref{tab:eval-generalization}.
Here, the reusability of acquired skills is evaluated by how well the model achieves a success rate as high as possible on OOD while maintaining a high success rate on ID.
GazeBot incorporates several design choices and components, whose contributions can be assessed by comparing its performance with that of the ablation models.

\begin{figure*}
    \begin{center}
        \includegraphics[width=\linewidth]{figures/eval-generalization.pdf}
        \caption{Examples of \textbf{ID} and \textbf{OOD} trials in the PenInCup, OpenCap, and PileBox tasks, where object positions and the initial end-effector poses are controlled. The images show the initial states of each trial. The checkboxes correspond to the method order in Table \ref{tab:eval-generalization} and indicate whether each method succeeded in the task from that initial state.}
        \label{fig:eval-generalization}
    \end{center}
\end{figure*}

\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccccccccc}
    \toprule
        & \multicolumn{4}{c}{\textbf{PenInCup} (real)} 
        & \multicolumn{4}{c}{\textbf{OpenCap} (real)} 
        & \multicolumn{4}{c}{\textbf{PileBox} (sim)} \\
    \cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
        & \multicolumn{2}{c}{ID} & \multicolumn{2}{c}{OOD} 
        & \multicolumn{2}{c}{ID} & \multicolumn{2}{c}{OOD} 
        & \multicolumn{2}{c}{ID} & \multicolumn{2}{c}{OOD} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
    \cmidrule(lr){6-7}\cmidrule(lr){8-9}
    \cmidrule(lr){10-11}\cmidrule(lr){12-13}
        \textbf{Method} 
        & Pick & Put & Pick & Put 
        & Hold & Open 
        & Hold & Open 
        & Lifted & Pile 
        & Lifted & Pile \\
    \midrule
        GazeBot (Ours) 
            & \textbf{92} & \textbf{75} & \textbf{83} & \textbf{83} 
            & \textbf{100} & \textbf{83} 
            & \textbf{100} & \textbf{75} 
            & \textbf{100} & \textbf{100} 
            & 80 & \textbf{75} \\
        - w/o point cloud (Ablation1) 
            & 75 & \textbf{75} & 42 & 42 
            & \textbf{100} & 67 
            & 58 & 42 
            & 95 & 65 
            & 70 & 40 \\
        - w/ state input partially (Ablation2)
            & 50 & 33 & 25 & 8 
            & \textbf{100} & 50
            & 58 & 42 
            & \textbf{100} & 90 
            & \textbf{85} & 50 \\
        - w/ state input (Ablation3)
            & 75 & 67 & 17 & 0 
            & \textbf{100} & 50 
            & 50 & 33 
            & \textbf{100} & 80 
            & 65 & 10 \\
        - w/o relative bottleneck (Ablation4)
            & 58 & 50 & 42 & 17 
            & \textbf{100} & 67 
            & 67 & 42 
            & \textbf{100} & 95 
            & 65 & 35 \\
        DAA 
            & 67 & 50 & 17 & 0 
            & 75 & 67 
            & 8 & 8 
            & \textbf{100} & 60 
            & 15 & 0 \\
        - w/o local action 
            & 67 & 50 & 17 & 0 
            & 67 & 50 
            & 8 & 8 
            & \textbf{100} & 40 
            & 20 & 0 \\
        ACT 
            & 75 & 67 & 8 & 0 
            & \textbf{100} & 75 
            & 25 & 25 
            & \textbf{100} & 80 
            & 40 & 15 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Comparison of success rates (\%) for ID and OOD trials in PenInCup, OpenCap and PileBox tasks. Our method is compared with seven ablation models, two of which are conventional baselines. We conducted 12 trials for PenInCup and OpenCap, and 20 trials for PileBox. Notably, only the proposed method maintains a high success rate in OOD situations for both tasks.}
    \label{tab:eval-generalization}
\end{table*}


\subsubsection{3D Awareness of Gaze-based Vision (Ablation1)}
One core feature of GazeBot is the ability to produce visual representations robust to changes in object location, achieved by the gaze-centered point cloud. In Ablation1, we replaced the gaze-centered point cloud with the conventional image-cropping approach. The cropped left and right images were tokenized via ResNet18 \cite{He2015}, following ACT, and then fed into the Transformer encoder. 
Although Ablation1 maintained a relatively high success rate for ID trials, as presented in Table \ref{tab:eval-generalization}, it suffered from lower accuracy in OOD owing to the absence of 3D awareness, which led to degraded performance when the objects appeared differently under unseen object positions.

\subsubsection{Inputs to the Policy (Ablation2, Ablation3)}
In GazeBot, the action policy model is designed so that the current end-effector pose is used as input only during the estimation of the bezier vector. 
In other words, bottleneck estimation relies solely on the 3D gaze position and the gaze-centered point cloud, and the prediction of the gaze-centered dexterous actions after the bottlenecks also relies only on the gaze-centered point cloud.

In Ablation3, we added tokens representing the current left and right end-effector poses and the 3D gaze position (3 × 512-dimensions) to the sequence of 3D point cloud tokens fed to the Transformer encoder. This addition prevented the model from accurately estimating the bottleneck in unseen initial poses. Even when the robot successfully reached the correct bottleneck for unseen object positions, the subsequent gaze-centered dexterous actions degraded because the bottleneck pose itself is an untrained input for the policy network (Table \ref{tab:eval-generalization}).

In Ablation2, we similarly added tokens but then applied an attention mask so that the CLS token $f_{pcd}$ in the Transformer encoder did not attend to these additional tokens. As a result, $f_{pcd}$ aggregated only information from the 3D point cloud, enabling accurate bottleneck reaching for unseen end-effector poses. Nonetheless, as mentioned in Ablation3, the accuracy of the gaze-centered dexterous actions still decreased even after the bottleneck was reached correctly (Table \ref{tab:eval-generalization}), often causing the end-effector to be “pulled” back toward the ID region.

\subsubsection{Bottleneck Estimation Method (Ablation4)}
GazeBot estimates the bottleneck pose by first predicting an offset from the 3D gaze position based on the gaze-centered point cloud and then adding that offset to the 3D gaze position. Another implementation could be considered, where these inputs are fed into a neural network that directly outputs the bottleneck pose. However, under unseen object positions, Ablation4 struggled to extrapolate the bottleneck and even failed to reach the object correctly. By contrast, our method successfully extrapolated the bottleneck pose in almost all OOD cases (Table \ref{tab:eval-generalization}).

\subsubsection{Design of the Bottleneck Reaching Module (DAA)}
In GazeBot, we achieve bottleneck reaching by first predicting a bottleneck pose and then generating a first-order Bézier curve as a trajectory leading to the bottleneck.
One could adopt a fully parametric approach that directly outputs an action sequence leading to the bottleneck pose.
DAA, for instance, proposes switching between global and local actions—similar to our segmentation by bottlenecks—and employs a fully parametric approach for the global actions by directly predicting the time series of end-effector poses.
In our tasks, however, DAA exhibited a significant drop in success rates from ID to OOD (Table \ref{tab:eval-generalization}). In most OOD trials, the model even failed to perform the reaching motion itself, reflecting the issues seen in Ablation4.

\subsubsection{Gaze-Based Vision (ACT, DAA w/o local action)}
Lastly, we compared two baselines: (1) ACT, which is effectively equivalent to removing all proposed modules from GazeBot, and (2) DAA w/o local action, which can be interpreted as applying a gaze-based image-cropping strategy to ACT.
As can be observed from Table \ref{tab:eval-generalization}, both approaches exhibited a sharp decline in success rates when moving from ID to OOD.
In OOD trials, whether local action was present or not had little impact on the success rate of DAA because it already failed during the global (reaching) phase.
Moreover, ACT slightly outperformed DAA overall. 
In DAA, because only a small, gaze-centered region is input to the model and 3D awareness is nearly absent, the input varies significantly with the object position, undermining data efficiency. Consequently, with approximately 100 demonstrations—as in this experiment—it was not possible to achieve the improvements in dexterity previously reported \cite{Kim2021}.
Meanwhile, GazeBot, which also adopted gaze-based vision, outperformed ACT (Table \ref{tab:eval-generalization}). This demonstrates that our approach additionally contributed to improved data efficiency of gaze-based models.











\section{Limitations and Future Directions}
In this study, we proposed GazeBot, an imitation learning method for robotic object manipulation that significantly improves the reusability of learned skills without sacrificing dexterity and reactivity. 
GazeBot demonstrates a significantly high generalization capability, particularly for unseen object positions and unseen end-effector poses, compared to state-of-the-art models. However, several challenges remain.

\textbf{Precise and Flexible Gaze Control.} While GazeBot demonstrated the importance of three-dimensional gaze control in object manipulation, robotic gaze prediction is still in its infancy. 
Robots not only require task-specific gaze data from human teleoperators for supervision but also lack the flexibility to adjust their gaze positions when objects are partially or fully obscured. Moreover, they struggle to align their gaze \textit{exactly} on tiny targets, such as a needle in midair.

\textbf{More Flexible and Adaptive Segmentations.} In this study, we achieved highly reusable imitation by employing gaze and bottlenecks to spatially and temporally segment vision and action. However, our current approach uses rigid segmentation along explicit boundaries. One limitation concerns the gaze-centered point cloud. In our method, gaze-based vision only captures information corresponding to foveal vision, whereas human vision simultaneously benefits from peripheral vision, which can roughly capture a wider range of information. Another limitation involves bottleneck determination. The proposed approach assumes that each sub-task has a single bottleneck, yet some tasks involve sub-tasks with multiple bottlenecks. Future work could explore more flexible segmentation methods to enable more adaptive learning.

\textbf{Integrating Advanced Trajectory Planning.} For simplicity, the reaching trajectories in this work are generated by connecting the current end-effector pose and the bottleneck pose with a first-order Bézier curve. To further enhance generalization, we could replace these simple connections with trajectories provided by advanced planning methods that support collision avoidance and bimanual coordination. Such improvements would enable more robust reusability of learned motions.






% \section*{Acknowledgments}

%% Use plainnat to work nicely with natbib. 

\input{main.bbl}
% \bibliographystyle{plainnat}
% \bibliography{references}

\appendix

\subsection{Implementation Details}
\label{appendix-implement}

% We summarize training and inference of GazeBot in Algorithms \ref{alg:model-training} and \ref{alg:model-inference}.

% \begin{algorithm}
% \caption{Training Algorithm}
% \label{alg:model-training}
% \begin{algorithmic}
% \Require $n \geq 0$
% \Ensure $y = x^n$
% \State $y \gets 1$
% \State $X \gets x$
% \State $N \gets n$
% \While{$N \neq 0$}
% \If{$N$ is even}
%     \State $X \gets X \times X$
%     \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
% \ElsIf{$N$ is odd}
%     \State $y \gets y \times X$
%     \State $N \gets N - 1$
% \EndIf
% \EndWhile
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% \caption{Inference Algorithm}
% \label{alg:model-inference}
% \begin{algorithmic}
% \Require $n \geq 0$
% \Ensure $y = x^n$
% \State $y \gets 1$
% \State $X \gets x$
% \State $N \gets n$
% \While{$N \neq 0$}
% \If{$N$ is even}
%     \State $X \gets X \times X$
%     \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
% \ElsIf{$N$ is odd}
%     \State $y \gets y \times X$
%     \State $N \gets N - 1$
% \EndIf
% \EndWhile
% \end{algorithmic}
% \end{algorithm}

% デモの例を見せる
We collected demonstrations through human teleoperation, and the teleoperation system is summarized in Figure \ref{fig:teleop-system}. 

Examples of the collected demonstrations for the PenInCup, OpenCap, and PileBox tasks are shown in Figure \ref{fig:demos}.
The images were captured using a ZED Mini camera mounted on the robot. These images correspond to both the perspective observed by the remote operator through the HMD and the viewpoint used by the robot during inference.
The white circles in the images indicate the gaze positions of the remote operator measured at the respective moments.

% Hypara一覧表を加える
The hyperparameters used for GazeBot and the seven ablation models are summarized in Table \ref{tab:hyperparameters-models}.
We found that the same hyperparameters yielded good results across all models.
We also present the hyperparameters used for task decomposition based on gaze transitions \cite{Takizawa2024} in Table \ref{tab:hyperparameters-seg}. This method successfully segmented all demonstrations into sub-tasks consistently and accurately.

\subsection{Experiment Details}
\label{appendix-experiment}
% すべてのtrial（ID・OOD）の初期状態一覧
In the experiment, we standardized the initial conditions for all ID and OOD trials to enable an accurate comparison of each model’s generalization performance. Both GazeBot and the seven ablation models performed the task under conditions as similar as possible. Figure \ref{fig:init-states} illustrates all the initial conditions used in the ID and OOD trials. 
In the OOD trials, we first evaluated the cases where each object was individually placed in unseen positions. We then examined the cases where all objects were simultaneously placed in unseen positions. Finally, we conducted trials involving unseen initial end-effector poses.



\newpage



\begin{figure*}[b]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/teleop.pdf}
    \caption{The robot system used in this work: a dual-arm robot (right side of the image) and a teleoperation system (left side of the image).}
    \label{fig:teleop-system}
\end{figure*}

\begin{table*}[b]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Parameter Name} & \textbf{Value} & \textbf{Description} \\ 
\midrule
image size & 1280 x 720 & Size of the entire image before converting it to point cloud. \\ 
num points (real) & 10000 & Number of points included in the gaze-centered point cloud in real tasks. \\ 
num points (sim) & 4000 & Number of points included in the gaze-centered point cloud in simulator tasks. \\
crop size (point cloud) & 0.2 & Size of the cubic region in meters cropped by the entire point cloud . \\ 
crop size (image) & 250 & Size of the square region in meters cropped by the entire image (used in Ablation1 and DAA). \\ 
progress thresh & 0.9 / 5 & The sub-task index is incremented when the progress advances by 0.9 continuously over five time steps. \\
learning rate & 1e-5 & Learning rate of training (the same for all models). \\ 
weight decay & 1e-4 & Weight decay of training (the same for all models). \\ 
batch size & 8 & Batch size of training (the same for all policy models). \\ 
hidden dimension & 512 & Hidden dimensions used for the Transformer. \\ 
num heads & 8 & Number of heads used for transformer. \\ 
feedforward dimension & 3200 & Feedforward dimensions used for the Transformer. \\ 
num enc layers & 4 & Number of encoder layers of the Transformer. \\ 
num dec layers & 4 & Number of decoder layers of the Transformer. \\ 
hidden dimension & 512 & Hidden dimensions used for the Transformer. \\ 
dropout & 0.1 & Dropout used for the Transformer. \\
batch size (gaze) & 16 & Batch size of training in the gaze model. \\
upsample & 4 & Upsampling of the number of tokens in the gaze model. \\
\bottomrule
\end{tabular}
\caption{Hyperparameters.}
\label{tab:hyperparameters-models}
\end{table*}
% gaze分節化のやつも

\begin{table*}[b]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Parameter Name} & \textbf{Value} & \textbf{Description}\\ 
\midrule
init thresh (gaze position) & 50 & Initial threshold of gaze position for change detection. \\ 
init thresh (gaze feature) & 0.05 & Initial threshold of gaze feature for change detection. \\ 
crop size (in 1280x720) & 256 & Size of the cropped image around the gaze position to extract gaze features.  \\
window size & 20 & Window size for median filtering. \\ 
\bottomrule
\end{tabular}
\caption{Hyperparameters for gaze-based task decomposition \cite{Takizawa2024}.}
\label{tab:hyperparameters-seg}
\end{table*}

\begin{figure*}[b]
    \begin{tabular}{c}
        \vspace{0.3cm}
        \begin{minipage}[c]{0.95\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figures/demo-a.png}
            \subcaption{PenInCup}
        \end{minipage}
        \\ \vspace{0.3cm}
        \begin{minipage}[c]{0.95\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figures/demo-b.png}
            \subcaption{OpenCap}
        \end{minipage}
        \\ 
        \begin{minipage}[c]{0.95\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figures/demo-c.png}
            \subcaption{PileBox}
        \end{minipage}
    \end{tabular}
    \caption{Examples of the demonstrations collected by a human teleoperator. White circles indicate the remote operator's gaze position.}
    \label{fig:demos}
\end{figure*}



\begin{figure*}[b]
    \begin{tabular}{c}
        \vspace{0.2cm}
        \begin{minipage}[c]{0.95\linewidth}
            \centering
            \includegraphics[width=0.7\linewidth]{figures/init-states-a.pdf}
            \subcaption{ID trials (PenInCup)}
        \end{minipage}
        \\ \vspace{0.2cm}
        \begin{minipage}[c]{0.95\linewidth}
            \centering
            \includegraphics[width=0.7\linewidth]{figures/init-states-b.pdf}
            \subcaption{OOD trials (PenInCup)}
        \end{minipage}
        \\ \vspace{0.2cm}
        \begin{minipage}[c]{0.95\linewidth}
            \centering
            \includegraphics[width=0.7\linewidth]{figures/init-states-c.pdf}
            \subcaption{ID trials (OpenCap)}
        \end{minipage}
        \\ \vspace{0.2cm}
        \begin{minipage}[c]{0.95\linewidth}
            \centering
            \includegraphics[width=0.7\linewidth]{figures/init-states-d.pdf}
            \subcaption{OOD trials (OpenCap)}
        \end{minipage}
        \\  \vspace{0.2cm}
        \begin{minipage}[c]{0.95\linewidth}
            \centering
            \includegraphics[width=0.7\linewidth]{figures/init-states-e.pdf}
            \subcaption{ID trials (PileBox)}
        \end{minipage}
        \\ \vspace{0.2cm}
        \begin{minipage}[c]{0.95\linewidth}
            \centering
            \includegraphics[width=0.7\linewidth]{figures/init-states-f.pdf}
            \subcaption{OOD trials (PileBox)}
        \end{minipage}
    \end{tabular}
    \caption{Initial states of all trials in ID and OOD.}
    \label{fig:init-states}
\end{figure*}


\end{document}