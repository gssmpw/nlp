\section{Fundamental Analysis of Self-Consistency}

In this section, we first present a distribution alignment perspective on how self-consistency works with specific true answer distributions, supported by experimental evidence to substantiate this viewpoint. Building upon this foundation, we proceed to provide both a formal definition of self-consistency convergence and practical criteria for assessment. 
\subsection{Why Self-Consistency Works: A Distributional Perspective}

Self-Consistency is a widely-used decoding method for improving reasoning performance by aggregating multiple stochastic samples. 
By applying a majority voting scheme, it mitigates issues such as local optima and high variance that arise from relying on a single sample. Formally, it can be expressed as:

\begin{equation}
    \hat{y}_{SC} = \arg\max_y \left( \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(y_i = y) \right)
\end{equation}
where $y_i$ is the $i$-th sampled answer, and $\mathbb{I}(y_i = y)$ is the indicator function that equals 1 if $y_i$ matches the candidate answer $y$, and 0 otherwise. The result, $\hat{y}_{SC}$, is the answer with the highest number of votes (the top-1 answer).

From a probabilistic perspective, self-consistency can be seen as a \textit{Monte Carlo estimator} of the true answer distribution $p(y \mid \mathbf{x})$. As the number of samples increases, the empirical distribution formed by the samples approximates the true distribution, and the most frequent answer aligns with the true distribution:
\begin{equation}
\begin{aligned}
\hat{p}_{SC}(y) &= \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(y_i = y) \\
&\to p(y \mid \mathbf{x}), \quad \text{as} \quad n \to \infty
\end{aligned}
\end{equation}
As the number of samples increases, the estimation becomes more reliable, and the voting mechanism converges towards the true answer.

\paragraph{Experimental Analysis}
\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{figs/Top1_Matching_Probability_Accuracy.pdf}
\caption{Top-1 answer matching probability (a) and accuracy (b) both improve as the sampling number increases.}
\label{fig:top1}
\end{figure} 
To validate this viewpoint, we analyzed the top-1 answer match rate as a function of the sample size. The true top-1 answer is simulated by drawing from a large sample to approximate the true distribution. 
Results from Figure~\ref{fig:top1} reveals \textbf{Findings 1}: As the sample size increased, the top-1 answer match rate gradually approaches 100\% with the accuracy consistently improves.
Based the observation, we derive the following insight: 
\textit{\textbf{Insights 1}: The improvement in self-consistency performance stems from the fact that, 
the top-1 answer in the sampling distribution gradually aligns with the true distribution, ultimately enhancing accuracy to match the true distribution's level.}


\subsection{Convergence Analysis of Answer Aggregation}
According to \textit{\textbf{Insights 1}}, since the accuracy of the true distribution is fixed, the performance of self-consistency is guaranteed to converge.
To further investigate it, we provide the following definition according to the Cauchy convergence criterion:
\begin{definition}
Let $f^M(i) = \sum_{l=1}^M \mathbb{I}(\hat{y_l} = i)$, where $\hat{y_l}$ represents the set of answers generated by the model, and $ M $ is the number of samples. For any given $ \epsilon > 0 $, there exists a positive integer $ L $ such that for $ N, M > L $, if the following holds:
\begin{align}
\left|\; \underset{i}{argmax}\; f^M(i) - \underset{i}{argmax}\; f^N(i) \;\right| < \epsilon
\end{align}
we can conclude that self-consistency has converged.
\label{def:sc}
\end{definition}

Based on Definition \ref{def:sc}, we prove that self-consistency also converges in terms of the accuracy on the dataset:

\begin{theorem}
Let $ Acc_{D}^M = \frac{1}{|D|}\sum_{j\in D}\mathbb{I}[\underset{i}{argmax}\; f^M(i)=gt_j] $ denote the accuracy of self-consistency when a single question is sampled $ M $ times on dataset $ D $, where $ gt_j $ represents the correct answer to the $ j $-th question. If Definition 1 holds, then for any given $ \epsilon > 0 $, there exists a positive integer $ L $ such that when $ N, M > L $, the following holds:
\begin{align}
\left| \; Acc_{D}^M - Acc_{D}^N \;\right| < \epsilon
\end{align}
\label{the:sc}
\end{theorem}
The Proof of Theorem~\ref{the:sc} is in Appendix~\ref{app:proof}.
By setting $\epsilon$ to $\frac{1}{|D|}$, the following definition is established:
\begin{definition}
If the following holds on dataset $D$:
\begin{align}
\left| \; Acc_{D}^M - Acc_{D}^{M-5} \;\right| < \frac{1}{|D|}
\end{align}
we can consider self-consistency to have converged at a sample size of $M$.
 
\label{def:sc_D}
\end{definition}
\paragraph{Experimental Analysis}
\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figs/convergence_gsm8k.pdf}
\caption{Self-consistency convergence plots under different temperature (0.4 and 0.8) settings.}
\label{fig:convergence}
\end{figure} 
Figure~\ref{fig:convergence} depicts the convergence behavior of various models, with the accuracy curves plotted up to the convergence point according to Definition~\ref{def:sc_D}, from where we can get:
\textbf{Findings 2}: The convergence speed exhibits a positive correlation with accuracy.
\textbf{Findings 3}: The convergence speed is inversely correlated with temperature. 
\textbf{Findings 4}: The final converged accuracy varies across different temperature settings.
Based on them, we derive \textit{\textbf{Insights 2}: Sampling diversity will affect the true distribution, impacting both the convergence accuracy and the convergence speed of self-consistency.}

\section{Diversity Trade-offs for Self-Consistency}
\label{sec:diversity}
\subsection{Sampling Diversity Affection}
According to \textbf{\textit{Insight 2}}, to gain a deeper understanding of the impact of diversity on self-consistency, we investigate how accuracy varies with temperature changes in increments of 0.1. The study is divided into two parts: convergence analysis and finite-sample analysis.
\paragraph{Converge Condition}

\begin{figure}[t]
\centering
\includegraphics[width=0.75\linewidth]{figs/Qwen2.5-7B_gsm8k_inf_tem.pdf}
\caption{The accuracy curve with varying temperature under convergence.}
\label{fig:inf_tem}
\end{figure} 
Figure~\ref{fig:inf_tem} indicates \textbf{Findings 5}: As the temperature increases, the accuracy of single samples exhibits a declining trend, while the accuracy of self-consistency after convergence shows an increasing trend (the optimal point is often near 1.0\footnote{We speculate that this may be related to the training temperature being typically set to 1.0. We leave the study of the optimal temperature as future work.}). Please refer to Appendix~\ref{app:diversity} for more results. The disagreement resolution theorem in ensemble learning provides a potential explanation, suggesting that the overall performance of an ensemble is determined by the trade-off between the accuracy of individual models and the diversity among them. From this trend and \textit{\textbf{Insights 1}}, we gain \textit{\textbf{Insights 3}: When the sample size is sufficient, the temperature should be increased to better explore the true distribution with higher accuracy.}

\paragraph{Finite-Sample Condition}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figs/Qwen2.5-7B_gsm8k_100_vllm_heatmap.pdf}
\caption{The accuracy heatmap with varying temperature with finite sample size.}
\label{fig:limit_tem}
\end{figure} 
Figure~\ref{fig:limit_tem} indicates \textbf{Findings 6}: When the sample size is limited, the optimal temperature gradually shifts toward lower values as the sample size decreases.
Please refer to Appendix~\ref{app:diversity} for more results.
This findings and \textit{\textbf{Insights 1}} leads us to \textit{\textbf{Insights 4}: Sample size determines the maximum top-1 confidence level that can be reliably modeled. True distributions with lower confidence require larger data volumes to ensure that the sampled top-1 answer aligns with the converged result.}

By combining \textit{\textbf{Insights 3}} and \textit{\textbf{4}}, we can derive \textit{\textbf{Insights 5}: The effectiveness of self-consistency depends on dynamically aligning the confidence of the sampling distribution with the inherent uncertainty of the true answer distribution.}

\subsection{Chain-of-thought Affection}
\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figs/cot_fsd_comparison.pdf}
\caption{FSD (Equation~\ref{eq:fsd}) \citep{FSD} is employed as the confidence metric to quantify the gap between top two candidates.}
\label{fig:cot_fsd}
\end{figure} 

Besides the sampling diversity decided by temperature, Chain-of-Thought \citep{COT} is also a key factor. From Figure~\ref{fig:cot_fsd} we can get \textbf{Findings 7}: Using CoT prompt leads to higher confidence compared to not using it.
A deeper \textbf{\textit{Insight 6}} emerges: \textit{Chain-of-thought (CoT) reasoning narrows the output space and reduces diversity, thereby increasing answer confidence.} However, investigating this phenomenon is not the focus of this paper, and we leave it for future work.

