\section{From Static to Adaptive: Confidence-Driven Optimization of Self-Consistency Distributions}
\subsection{Motivation}
Traditional self-consistency methods rely on static sampling strategies with fixed sample sizes and temperatures, which face limitations:
When the confidence of the sampling distribution is too low, a limited sample size struggles to accurately capture the true top-1 answer, restricting the performance gains of self-consistency (\textbf{\textit{Insight 4}}). Conversely, when the confidence is too high, the model fails to explore potentially better distributions at higher temperatures (\textbf{\textit{Insight 3}}).
Our method addresses these issues through adaptive confidence-distribution alignment (\textbf{\textit{Insight 5}}). By dynamically adjusting the sampling distribution's diversity based on real-time confidence levels, we optimize alignment by proactively adapting to the evolving gap between model confidence and true distribution uncertainty. This dynamic mechanism enables efficient convergence to the correct answer even under limited sample sizes while facilitating exploration of better true distributions when needed. Through this approach, we enhance both the accuracy and robustness of self-consistency across diverse conditions.
\subsection{Diversity Control Strategy}

\paragraph{Dynamic Temperature Adjustment}
We introduce a confidence-driven diversity optimization mechanism to dynamically align the sampling distribution with the latent answer distribution. First-Second Distance (FSD) \citep{FSD} is employed as the confidence metric to quantify the gap between top candidates. Formally, at decoding step $t$:

\begin{equation}
     \text{FSD}^{(t)} = {p_1}^{(t)} - {p_2}^{(t)}
\end{equation}
where ${p_1}^{(t)}$ and ${p_2}^{(t)}$ are the probabilities of the top two answers from the first $t$ samples. This metric directly reflects the modelâ€™s uncertainty in distinguishing between the dominant candidates.

To ensure stable optimization, we design a conservative adjustment rule with a dead zone around confidence threshold $\tau$.
The temperature $T$ is updated based on the FSD metric:
\begin{align}
    T^{(t+1)} =
\begin{cases}
T^{(t)} - 0.1 & \text{if } \text{FSD}^{(t)} < \tau - \epsilon, \\
T^{(t)} + 0.1 & \text{if } \text{FSD}^{(t)} > \tau + \epsilon, \\
T^{(t)} &  \text{otherwise,}
\end{cases}
\label{eq:fsd}
\end{align}
where $\epsilon$ is a stability margin, which we set to 0.05 for simplicity. Temperature $T$ is clamped to [0.1, 1.0] to avoid extreme values.

\paragraph{Phased Sampling Strategy}
To balance exploration and efficiency, we employ a three-phase sampling protocol:
\begin{itemize}
    \item Exploration Phase: Collect small number of samples ($n_1 = 5$) with preset $T^{(1)}$ as a window to estimate initial $\text{FSD}^{(1)}$.
    \item Adaptive Phase: Adjust $T^{(2)}$ through Equation~\ref{eq:fsd}, then generate $n_2 = 0.5N - n_1$ ($N$: total budget) additional samples.
    \item Exploitation Phase: Finalize $T^{(3)}$ and generate the remaining $n_3 = 0.5 N$ samples.
\end{itemize}
The phased approach progressively shifts from broad exploration to focused exploitation. Finally, the accuracy is calculated by majority voting from the total of $N$ samples. 

In summary, our method dynamically adjusts the sampling diversity by monitoring the confidence levels, allowing for more efficient exploration and convergence. This adaptive mechanism ensures better alignment with the true answer distribution under sampling constraints to improve accuracy.


\subsection{Theoretical Analysis}
\label{sec:threshold}


To ensure a rational and effective selection of the FSD threshold $\tau$, we construct a one-sided z-test for analysis. The test employs the null hypothesis as follows:

$H_0$: The current sampled top-1 answer is not the true answer for the given question under infinite sampling.

To simplify this problem, we assume that only the current top-2 answer could potentially become the true answer under infinite sampling. Consequently, it is natural to focus on the relationship between FSD and confidence. Therefore, this hypothesis can be described as:
\begin{equation}
    z = \frac{ \hat{d} - d_{\mu} }{ SE }
\end{equation}
 where $\hat{d}$ represents the observed FSD from actual sampling, $d_{\mu}$ denotes the FSD under the true distribution of the model, and $SE$ stands for the standard error. Based on the null hypothesis $H_0$, it is clear that $d_{\mu}<0$. We adopt the critical condition $d_{\mu}=0$:

\begin{equation}
    z \geq \frac{ \hat{d} - 0}{ SE } = \frac{ \hat{p}_1 - \hat{p}_2 }{ SE }
\label{eq:z}
\end{equation}

Assuming that the current sample size approaches infinity and that the sampling between the two categories can be considered independent, according to the multinomial distribution and Jensen's inequality (in the case of a concave function), we have:
\begin{align}
    SE &=\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{N}+\frac{\hat{p}_2(1-\hat{p}_2)}{N}} \notag\\
    &\leq \sqrt{\frac{2p(1-p)}{N}} 
\label{eq:SE}
\end{align}
where $p = \frac{\hat{p}_1+\hat{p}_2}{2} \in (0,0.5]$, substituting Equation \ref{eq:SE} into Equation \ref{eq:z}, we can derive the theoretical lower bound of $z$:
\begin{equation}
z \geq \frac{\hat{p}_1-\hat{p}_2}{\sqrt{\frac{2p(1-p)}{N}}} \geq \hat{d}\sqrt{2N}
\end{equation}

Setting $z = 1.64$, the corresponding $p$-value is approximately 0.05, indicating strong statistical evidence that the current top-1 answer is indeed the top-1 answer under the true model distribution. Therefore, the FSD threshold can be set as:
\begin{align}
    \tau = \frac{1.16}{\sqrt{N}}
\end{align}