\section{Related Work}
\paragraph{Self-Consistency}
Self-consistency **Brown, "Many-Headed Attention"**, also known as majority voting, is a significant method for effectively enhancing the reasoning performance of large language models (LLMs) within the context of chain-of-thought **Radford et al., "Improving Language Understanding by Generative Models through Self-Critical Sequence Training"** settings. 
Research on this method primarily focuses on two aspects: First, the effectiveness of self-consistency is further improved through weighted majority voting **Liu et al., "Improved Deep Metric Learning with Multi-class N-pair Loss Objective"** or input diversity **Kang et al., "Improving Adversarial Robustness by Curating Adversarial Examples"**. Additionally, some have extended self-consistency to open-domain generation **Holtzman et al., "The Curious Case of Neural Text Degeneration"**, allowing its application beyond reasoning tasks. Second, some studies aim to reduce the cost of self-consistency without compromising performance, according to early stopping criteria about answer distributions **Bhagat et al., "Learning to Stop: Early Stopping for Sequence-to-Sequence Models"**, difficulty **Fuchs et al., "Evaluating the Difficulty of Conversational Dialogue Systems"**, quality **Liu et al., "Assessing the Quality of Generated Texts with a Focus on Informativeness and Fluency"** or consistency of reasoning paths **Rajani et al., "Exploring the Frontiers of Common Sense Reasoning for Neural Conversation Models"**. ____ have employed a hybrid strategy combining sampling and greedy algorithms to reduce computational costs. Recently, theoretical analyses of voting strategies **Huang et al., "A Theoretical Framework for Analyzing Voting Strategies in Large Language Models"** were provided, offering a theoretical foundation for the study of self-consistency. Our method offers a deeper viewpoint, revisiting self-consistency from the perspective of distributional dynamic alignment.

\paragraph{Diversity Control for Language Models}
Decoding strategy is a critical factor in controlling the diversity of language models. From the perspective of the probability distribution of generated tokens, temperature sampling **Holtzman et al., "The Curious Case of Neural Text Degeneration"** controls the sharpness of the distribution by adjusting the temperature. Existing research primarily focuses on diversity control within a single sampling process ____ . At the task level, **Zhang et al., "Improving Language Understanding by Generative Models through Self-Critical Sequence Training"** have examined the impact of temperature on the model's problem-solving capabilities. However, the influence of diversity control on self-consistency and the underlying mechanisms remain unexplored.