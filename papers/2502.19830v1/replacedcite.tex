\section{Related Work}
\paragraph{Self-Consistency}
Self-consistency ____, also known as majority voting, is a significant method for effectively enhancing the reasoning performance of large language models (LLMs) within the context of chain-of-thought ____ settings. 
Research on this method primarily focuses on two aspects: First, the effectiveness of self-consistency is further improved through weighted majority voting ____ or input diversity ____. Additionally, some have extended self-consistency to open-domain generation ____, allowing its application beyond reasoning tasks. Second, some studies aim to reduce the cost of self-consistency without compromising performance, according to early stopping criteria about answer distributions ____, difficulty ____, quality ____ or consistency of reasoning paths ____. ____ have employed a hybrid strategy combining sampling and greedy algorithms to reduce computational costs. Recently, theoretical analyses of voting strategies ____ were provided, offering a theoretical foundation for the study of self-consistency. Our method offers a deeper viewpoint, revisiting self-consistency from the perspective of distributional dynamic alignment.

\paragraph{Diversity Control for Language Models}
Decoding strategy is a critical factor in controlling the diversity of language models. From the perspective of the probability distribution of generated tokens, temperature sampling ____ controls the sharpness of the distribution by adjusting the temperature. Existing research primarily focuses on diversity control within a single sampling process ____. At the task level, ____ have examined the impact of temperature on the model's problem-solving capabilities. However, the influence of diversity control on self-consistency and the underlying mechanisms remain unexplored.