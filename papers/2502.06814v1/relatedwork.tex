\section{Related Work}
\label{sec:related_work}

\emph{We find that the gap in VLM alignment partly stems from the technological trajectories pursued over the past half-decade.}  
One key VLM milestone was Flamingo \cite{alayrac2022flamingo}, which laid the foundation for modern VLMs. In its design, images and text are processed by separate encoders, unified through a perceiver resampler \cite{jaegle2021perceiver}, and passed through deep transformer layers combining cross-attention and self-attention. Flamingo's elegant architecture established a new standard, influencing a range of subsequent models \cite{li2022blip, li2023blip, you2023ferret}.  
%
The importance of aligning vision-text correlations is evident in the design of Llama 3.2 \cite{dubey2024llama}, released two years later, which adopts Flamingo’s strategy of using a dedicated cross-attention module for effective interaction handling. However, training a VLM with a dedicated cross-attention module end-to-end requires substantial data and computational resources. These models are typically pre-trained on millions or even billions of image-text pairs and interleaved image-text datasets \cite{zhu2024multimodal}. 
Similar challenges apply to broader multimodal models beyond vision and language \cite{lu2024unified}.

Unlike VLMs, single-modality large language models (LLMs) have scaled more rapidly \cite{ouyang2022training, brown2020language, chowdhery2023palm}, often consuming over 100 million examples spanning 1,800 tasks \cite{longpre2023flan}. VLMs, however, face a training data gap due to the high cost of acquiring paired image-text datasets.  
%
To address this, researchers proposed leveraging scaled LLMs by instruction fine-tuning them on as little as 150k paired visual question answering (VQA) data using an autoregressive loss. This approach, pioneered by \citet{zhu2023minigpt, dai2023instructblipgeneralpurposevisionlanguagemodels, liu2024visual}, aligns text and image tokens through fine-tuning connectors such as MLPs, encoders, or decoders connecting to the LLM, providing an efficient pathway to integrate vision with language models for diverse tasks \cite{wang2024qwen2, li2024llava, koh2024generating, chen2025sharegpt4v, wang2024visionllm, chen2023tem, huang2024segment, liu2024universal, gao2023llama, cha2024honeybee}.

However, the community soon recognised that the vision capabilities integrated through these small adapter layers outside the LLM (in LLaVA-like approaches) remain insufficient \cite{tong2024eyes}. To address this gap, \citet{covert2024locality, karamcheti2024prismatic} refined vision encoders to align more closely with pretrained vision models.  
\citet{jiang2023clip, kar2025brave} proposed merging multiple visual encoders with projection layers before feeding them into the LLM.  
Separately, \citet{tong2024cambrian, shi2024eagle, zong2024mova} explored merging a larger number of diverse vision expert models with fine-tuned projection layers, integrating the combined vision features either before the LLM or within the LLM transformer layers, respectively.  

Diffusion Models (DMs), as vision experts, have gained recent attention. \citet{wang2024diffusion} use DM’s image generation loss to enhance the visual encoder. Other approaches merge VLMs with DM’s image generation, either by fine-tuning DMs with VLM outputs \cite{tong2024metamorph, hernandez2024generative}, sharing a central transformer \cite{shi2025lmfusionadaptingpretrainedlanguage, chen2025janusprounifiedmultimodalunderstanding}, or integrating DMs with LLM transformers \cite{zhou2024transfusion}.

Despite attempts to integrate DMs and VLMs with minimal modifications to their internal architectures, \emph{one overlooked aspect in prior work is the role of self- and cross-attention layers within DM and LLM Transformers}. These layers govern the interplay between multi-modal tokens, yet have not been closely examined. Although DMs and the LLM component in VLMs share the same foundational Transformer architecture \cite{vaswani2017attention, dosovitskiy2020image}, they exhibit markedly different text-to-region alignment due to their distinct optimization objectives. Notably, DMs demonstrate stronger alignment than VLMs, as shown in \cref{fig:empirical}.