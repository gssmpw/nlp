[
  {
    "index": 0,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "jaegle2021perceiver",
        "author": "Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao",
        "title": "Perceiver: General perception with iterative attention"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2022blip",
        "author": "Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",
        "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
      },
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      },
      {
        "key": "you2023ferret",
        "author": "You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei",
        "title": "Ferret: Refer and ground anything anywhere at any granularity"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhu2024multimodal",
        "author": "Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin",
        "title": "Multimodal c4: An open, billion-scale corpus of images interleaved with text"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lu2024unified",
        "author": "Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha",
        "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "chowdhery2023palm",
        "author": "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others",
        "title": "Palm: Scaling language modeling with pathways"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "longpre2023flan",
        "author": "Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others",
        "title": "The flan collection: Designing data and methods for effective instruction tuning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      },
      {
        "key": "dai2023instructblipgeneralpurposevisionlanguagemodels",
        "author": "Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi",
        "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
      },
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wang2024qwen2",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      },
      {
        "key": "li2024llava",
        "author": "Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others",
        "title": "Llava-onevision: Easy visual task transfer"
      },
      {
        "key": "koh2024generating",
        "author": "Koh, Jing Yu and Fried, Daniel and Salakhutdinov, Russ R",
        "title": "Generating images with multimodal language models"
      },
      {
        "key": "chen2025sharegpt4v",
        "author": "Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua",
        "title": "Sharegpt4v: Improving large multi-modal models with better captions"
      },
      {
        "key": "wang2024visionllm",
        "author": "Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and others",
        "title": "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks"
      },
      {
        "key": "chen2023tem",
        "author": "Chen, Guangyi and Liu, Xiao and Wang, Guangrun and Zhang, Kun and Torr, Philip HS and Zhang, Xiao-Ping and Tang, Yansong",
        "title": "Tem-adapter: Adapting image-text pretraining for video question answer"
      },
      {
        "key": "huang2024segment",
        "author": "Huang, Xiaoke and Wang, Jianfeng and Tang, Yansong and Zhang, Zheng and Hu, Han and Lu, Jiwen and Wang, Lijuan and Liu, Zicheng",
        "title": "Segment and caption anything"
      },
      {
        "key": "liu2024universal",
        "author": "Liu, Yong and Zhang, Cairong and Wang, Yitong and Wang, Jiahao and Yang, Yujiu and Tang, Yansong",
        "title": "Universal segmentation at arbitrary granularity with language instruction"
      },
      {
        "key": "gao2023llama",
        "author": "Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others",
        "title": "Llama-adapter v2: Parameter-efficient visual instruction model"
      },
      {
        "key": "cha2024honeybee",
        "author": "Cha, Junbum and Kang, Wooyoung and Mun, Jonghwan and Roh, Byungseok",
        "title": "Honeybee: Locality-enhanced projector for multimodal llm"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "tong2024eyes",
        "author": "Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining",
        "title": "Eyes wide shut? exploring the visual shortcomings of multimodal llms"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "covert2024locality",
        "author": "Covert, Ian and Sun, Tony and Zou, James and Hashimoto, Tatsunori",
        "title": "Locality Alignment Improves Vision-Language Models"
      },
      {
        "key": "karamcheti2024prismatic",
        "author": "Karamcheti, Siddharth and Nair, Suraj and Balakrishna, Ashwin and Liang, Percy and Kollar, Thomas and Sadigh, Dorsa",
        "title": "Prismatic vlms: Investigating the design space of visually-conditioned language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "jiang2023clip",
        "author": "Jiang, Dongsheng and Liu, Yuchen and Liu, Songlin and Zhao, Jin'e and Zhang, Hao and Gao, Zhen and Zhang, Xiaopeng and Li, Jin and Xiong, Hongkai",
        "title": "From clip to dino: Visual encoders shout in multi-modal large language models"
      },
      {
        "key": "kar2025brave",
        "author": "Kar, O{\\u{g}}uzhan Fatih and Tonioni, Alessio and Poklukar, Petra and Kulshrestha, Achin and Zamir, Amir and Tombari, Federico",
        "title": "Brave: Broadening the visual encoding of vision-language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "tong2024cambrian",
        "author": "Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and Wang, Austin and Fergus, Rob and LeCun, Yann and Xie, Saining",
        "title": "{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}"
      },
      {
        "key": "shi2024eagle",
        "author": "Shi, Min and Liu, Fuxiao and Wang, Shihao and Liao, Shijia and Radhakrishnan, Subhashree and Huang, De-An and Yin, Hongxu and Sapra, Karan and Yacoob, Yaser and Shi, Humphrey and others",
        "title": "Eagle: Exploring the design space for multimodal llms with mixture of encoders"
      },
      {
        "key": "zong2024mova",
        "author": "Zong, Zhuofan and Ma, Bingqi and Shen, Dazhong and Song, Guanglu and Shao, Hao and Jiang, Dongzhi and Li, Hongsheng and Liu, Yu",
        "title": "Mova: Adapting mixture of vision experts to multimodal context"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wang2024diffusion",
        "author": "Wang, Wenxuan and Sun, Quan and Zhang, Fan and Tang, Yepeng and Liu, Jing and Wang, Xinlong",
        "title": "Diffusion feedback helps clip see better"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "tong2024metamorph",
        "author": "Tong, Shengbang and Fan, David and Zhu, Jiachen and Xiong, Yunyang and Chen, Xinlei and Sinha, Koustuv and Rabbat, Michael and LeCun, Yann and Xie, Saining and Liu, Zhuang",
        "title": "MetaMorph: Multimodal Understanding and Generation via Instruction Tuning"
      },
      {
        "key": "hernandez2024generative",
        "author": "Hernandez, Jefferson and Villegas, Ruben and Ordonez, Vicente",
        "title": "Generative Visual Instruction Tuning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "shi2025lmfusionadaptingpretrainedlanguage",
        "author": "Weijia Shi and Xiaochuang Han and Chunting Zhou and Weixin Liang and Xi Victoria Lin and Luke Zettlemoyer and Lili Yu",
        "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation"
      },
      {
        "key": "chen2025janusprounifiedmultimodalunderstanding",
        "author": "Xiaokang Chen and Zhiyu Wu and Xingchao Liu and Zizheng Pan and Wen Liu and Zhenda Xie and Xingkai Yu and Chong Ruan",
        "title": "Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhou2024transfusion",
        "author": "Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer",
        "title": "Transfusion: Predict the next token and diffuse images with one multi-modal model"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      },
      {
        "key": "dosovitskiy2020image",
        "author": "Dosovitskiy, Alexey",
        "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
      }
    ]
  }
]