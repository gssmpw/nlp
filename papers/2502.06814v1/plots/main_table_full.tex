\definecolor{tablegray}{gray}{0.9} % This is correct as gray is a valid color model
\definecolor{color_lora_auto}{rgb}{0.85, 0.90, 0.98}
\definecolor{color_lora_lav}{rgb}{0.88, 0.83, 0.90}
\definecolor{color_full_auto}{rgb}{0.66, 0.76, 0.92}
\definecolor{color_full_lav}{rgb}{0.76, 0.67, 0.81}


\begin{table*}[ht]
    \vspace{-0.5em}
    \linespread{1}
    \aboverulesep = 0.2em 
    \belowrulesep = 0.2em
    \scriptsize
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccc|cccccccccccccccc} 
        Model & \hspace{-0.9em}Base & \hspace{-0.9em}FT Data & \rotatebox[origin=lb]{90}{\smash{AI2D}} & \rotatebox[origin=lb]{90}{\smash{CCBench}} & \rotatebox[origin=lb]{90}{\smash{DocVQA}} & \rotatebox[origin=lb]{90}{\smash{InfoVQA}} & \rotatebox[origin=lb]{90}{\smash{MMBench$^{EN}$}} & \rotatebox[origin=lb]{90}{\smash{MME}} & \rotatebox[origin=lb]{90}{\smash{MMMU-val}} & \rotatebox[origin=lb]{90}{\smash{MMStar}} & \rotatebox[origin=lb]{90}{\smash{OCRBench}} & \rotatebox[origin=lb]{90}{\smash{OCRVQA$^{TQ}$}} & \rotatebox[origin=lb]{90}{\smash{POPE}} & \rotatebox[origin=lb]{90}{\smash{R.W.QA}} & \rotatebox[origin=lb]{90}{\smash{SEED-IMG}} & \rotatebox[origin=lb]{90}{\smash{ScienceQA}} & \rotatebox[origin=lb]{90}{\smash{TextVQA}} & \rotatebox[origin=lb]{90}{\smash{Hallu.Bench}} \\
        \midrule
        \multicolumn{19}{c}{\textbf{Small Budget-Constrained Models (self-attention only)}} \\
        \midrule
        LLaVA-1.5-7B & \hspace{-0.9em}Vicuna-7B & \hspace{-0.9em}0.15M & \hspace{-0.5em}55.5 & \hspace{-0.9em}17.8 & \hspace{-0.9em}28.1 & \hspace{-0.9em}25.8 & \hspace{-0.9em}66.5 & \hspace{-0.9em}1510.0 & \hspace{-0.9em}35.7 & \hspace{-0.9em}33.1 & \hspace{-0.9em}318.0 & \hspace{-0.9em}60.6 & \hspace{-0.9em}86.1 & \hspace{-0.9em}54.8 & \hspace{-0.9em}58.6 & \hspace{-0.9em}69.2 & \hspace{-0.9em}58.2 & \hspace{-0.9em}27.6 \\ 
        LLaVA-NeXt-7B & \hspace{-0.9em}Vicuna-7B & \hspace{-0.9em}0.76M & \hspace{-0.5em}67.0 & \hspace{-0.9em}24.3 & \hspace{-0.9em}74.4 & \hspace{-0.9em}37.1 & \hspace{-0.9em}67.4 & \hspace{-0.9em}1519.0 & \hspace{-0.9em}37.6 & \hspace{-0.9em}37.6 & \hspace{-0.9em}532.0 & \hspace{-0.9em}63.8 & \hspace{-0.9em}87.5 & \hspace{-0.9em}57.8 & \hspace{-0.9em}70.2 & \hspace{-0.9em}70.3 & \hspace{-0.9em}64.9 & \hspace{-0.9em}27.6 \\ 
        Mini-Gemini-7B & \hspace{-0.9em}Qwen-7B & \hspace{-0.9em}1.5M & \hspace{-0.5em}- & \hspace{-0.9em}- & \hspace{-0.9em}- & \hspace{-0.9em}- & \hspace{-0.9em}65.8 & \hspace{-0.9em}1523.0 & \hspace{-0.9em}36.8 & \hspace{-0.9em}- & \hspace{-0.9em}477.0 & \hspace{-0.9em}- & \hspace{-0.9em}- & \hspace{-0.9em}- & \hspace{-0.9em}- & \hspace{-0.9em}71.1 & \hspace{-0.9em}65.2 & \hspace{-0.9em}- \\ 
        Cambrian-1-7B & \hspace{-0.9em}Vicuna-7B & \hspace{-0.9em}10M & \hspace{-0.5em}74.6 & \hspace{-0.9em}23.7 & \hspace{-0.9em}47.9 & \hspace{-0.9em}40.8 & \hspace{-0.9em}74.6 & \hspace{-0.9em}1802.9 & \hspace{-0.9em}41.8 & \hspace{-0.9em}\textbf{50.7} & \hspace{-0.9em}614.0 & \hspace{-0.9em}66.0 & \hspace{-0.9em}86.4 & \hspace{-0.9em}60.0 & \hspace{-0.9em}73.3 & \hspace{-0.9em}81.0 & \hspace{-0.9em}77.1 & \hspace{-0.9em}30.6 \\ 
        Eagle-X5-7B & \hspace{-0.9em}Vicuna-7B & \hspace{-0.9em}0.93M & \hspace{-0.5em}73.6 & \hspace{-0.9em}28.4 & \hspace{-0.9em}\textbf{86.6} & \hspace{-0.9em}- & \hspace{-0.9em}68.8 & \hspace{-0.9em}1866.0 & \hspace{-0.9em}37.6 & \hspace{-0.9em}41.7 & \hspace{-0.9em}551 & \hspace{-0.9em}64.3 & \hspace{-0.9em}\textbf{89.3} & \hspace{-0.9em}63.8 & \hspace{-0.9em}73.6 & \hspace{-0.9em}71.2 & \hspace{-0.9em}71.9 & \hspace{-0.9em}35.4 \\ 
        LLaVA-1.5-8B & \hspace{-0.9em}Lama3-8B & \hspace{-0.9em}0.15M & \hspace{-0.5em}69.9 & \hspace{-0.9em}27.8 & \hspace{-0.9em}32.4 & \hspace{-0.9em}27.5 & \hspace{-0.9em}- & \hspace{-0.9em}1825.5 & \hspace{-0.9em}39.2 & \hspace{-0.9em}46.1 & \hspace{-0.9em}420.0 & \hspace{-0.9em}61.0 & \hspace{-0.9em}87.3 & \hspace{-0.9em}56.7 & \hspace{-0.9em}70.1 & \hspace{-0.9em}72.2 & \hspace{-0.9em}- & \hspace{-0.9em}28.7 \\ 
        LLaVA-Next-8B & \hspace{-0.9em}Lama3-8B & \hspace{-0.9em}0.76M & \hspace{-0.5em}72.8 & \hspace{-0.9em}32.7 & \hspace{-0.9em}78.5 & \hspace{-0.9em}38.2 & \hspace{-0.9em}74.8 & \hspace{-0.9em}1908 & \hspace{-0.9em}43.1 & \hspace{-0.9em}43.9 & \hspace{-0.9em}531.0 & \hspace{-0.9em}60.7 & \hspace{-0.9em}87.1 & \hspace{-0.9em}58.4 & \hspace{-0.9em}72.5 & \hspace{-0.9em}73.1 & \hspace{-0.9em}65.3 & \hspace{-0.9em}33.1 \\ 
        Cambrian-1-8B & \hspace{-0.9em}Lama3-8B & \hspace{-0.9em}10M & \hspace{-0.5em}73.0 & \hspace{-0.9em}- & \hspace{-0.9em}77.8 & \hspace{-0.9em}42.6 & \hspace{-0.9em}75.9 & \hspace{-0.9em}1547.0 & \hspace{-0.9em}42.7 & \hspace{-0.9em}50.7 & \hspace{-0.9em}624.0 & \hspace{-0.9em}66.0 & \hspace{-0.9em}73.0 & \hspace{-0.9em}64.2 & \hspace{-0.9em}\textbf{74.7} & \hspace{-0.9em}73.1 & \hspace{-0.9em}71.7 & \hspace{-0.9em}30.6 \\ 
        \midrule
        MiniCPM-V-2.5 & \hspace{-0.9em}Lama3-8B & \hspace{-0.9em}0.08M & \hspace{-0.5em}\textbf{78.1} & \hspace{-0.9em}45.5 & \hspace{-0.9em}84.6 & \hspace{-0.9em}52.1 & \hspace{-0.9em}76.4 & \hspace{-0.9em}2009.1 & \hspace{-0.9em}43.1 & \hspace{-0.9em}50.3 & \hspace{-0.9em}718.0 & \hspace{-0.9em}68.7 & \hspace{-0.9em}86.6 & \hspace{-0.9em}63.9 & \hspace{-0.9em}71.9 & \hspace{-0.9em}88.8 & \hspace{-0.9em}76.6 & \hspace{-0.9em}41.9 \\ 
        \rowcolor{color_lora_auto}
        +Lora FT & \hspace{-0.9em}Lama3-8B & \hspace{-0.9em}0.08M & \hspace{-0.5em}77.9 & \hspace{-0.9em}44.9 & \hspace{-0.9em}84.6 & \hspace{-0.9em}52.2 & \hspace{-0.9em}76.4 & \hspace{-0.9em}\textbf{2014.2} & \hspace{-0.9em}43.1 & \hspace{-0.9em}50.6 & \hspace{-0.9em}717.0 & \hspace{-0.9em}68.8 & \hspace{-0.9em}86.2 & \hspace{-0.9em}64.1 & \hspace{-0.9em}71.9 & \hspace{-0.9em}88.9 & \hspace{-0.9em}76.6 & \hspace{-0.9em}41.8 \\ 
        \rowcolor{color_lora_lav}
        +Lavender FT & \hspace{-0.9em}Lama3-8B & \hspace{-0.9em}0.08M & \hspace{-0.5em}77.9 & \hspace{-0.9em}\textbf{46.3} & \hspace{-0.9em}84.6 & \hspace{-0.9em}\textbf{52.3} & \hspace{-0.9em}\textbf{76.9} & \hspace{-0.9em}1990.2 & \hspace{-0.9em}\textbf{45.0} & \hspace{-0.9em}50.4 & \hspace{-0.9em}\textbf{721.0} & \hspace{-0.9em}\textbf{69.0} & \hspace{-0.9em}86.5 & \hspace{-0.9em}\textbf{64.3} & \hspace{-0.9em}72.1 & \hspace{-0.9em}\textbf{89.7} & \hspace{-0.9em}\textbf{76.9} & \hspace{-0.9em}\textbf{42.0} \\
        \midrule
        \multicolumn{19}{c}{\textbf{Llama-3.2-11B (cross-attention)}} \\
        \midrule
        Llama-3.2-11B & \hspace{-0.9em}LM32-11B & \hspace{-0.9em}N.A. & \hspace{-0.5em}78.7 & \hspace{-0.9em}30.6 & \hspace{-0.9em}82.8 & \hspace{-0.9em}59.0 & \hspace{-0.9em}70.8 & \hspace{-0.9em}1692.9 & \hspace{-0.9em}48.0 & \hspace{-0.9em}48.3 & \hspace{-0.9em}754.0 & \hspace{-0.9em}67.7 & \hspace{-0.9em}86.3 & \hspace{-0.9em}61.4 & \hspace{-0.9em}72.9 & \hspace{-0.9em}83.6 & \hspace{-0.9em}80.8 & \hspace{-0.9em}36.3 \\ 
        \rowcolor{color_lora_auto}
        + AutoR. Lora-FT & \hspace{-0.9em}LM32-11B & \hspace{-0.9em}0.13M & \hspace{-0.5em}77.8 & \hspace{-0.9em}\textbf{39.8} & \hspace{-0.9em}86.0 & \hspace{-0.9em}61.0 & \hspace{-0.9em}73.6 & \hspace{-0.9em}1664.6 & \hspace{-0.9em}43.7 & \hspace{-0.9em}45.1 & \hspace{-0.9em}733.0 & \hspace{-0.9em}70.0 & \hspace{-0.9em}86.9 & \hspace{-0.9em}60.7 & \hspace{-0.9em}73.6 & \hspace{-0.9em}74.6 & \hspace{-0.9em}81.1 & \hspace{-0.9em}36.6 \\ 
        \rowcolor{color_lora_lav}
        + Lavender Lora-FT & \hspace{-0.9em}LM32-11B & \hspace{-0.9em}0.13M & \hspace{-0.5em}\textbf{79.0} & \hspace{-0.9em}39.2 & \hspace{-0.9em}\textbf{90.3} & \hspace{-0.9em}\textbf{65.1} & \hspace{-0.9em}\textbf{81.3} & \hspace{-0.9em}\textbf{1871.5} & \hspace{-0.9em}46.3 & \hspace{-0.9em}48.7 & \hspace{-0.9em}\textbf{764.0} & \hspace{-0.9em}\textbf{71.5} & \hspace{-0.9em}\textbf{88.1} & \hspace{-0.9em}62.1 & \hspace{-0.9em}\textbf{74.1} & \hspace{-0.9em}\textbf{84.7} & \hspace{-0.9em}\textbf{88.1} & \hspace{-0.9em}\textbf{41.3} \\ 
        \rowcolor{color_full_auto}
        + AutoR. Full-FT & \hspace{-0.9em}LM32-11B & \hspace{-0.9em}0.13M & \hspace{-0.5em}76.7 & \hspace{-0.9em}37.6 & \hspace{-0.9em}89.8 & \hspace{-0.9em}64.3 & \hspace{-0.9em}77.5 & \hspace{-0.9em}1697.4 & \hspace{-0.9em}45.6 & \hspace{-0.9em}46.8 & \hspace{-0.9em}748.0 & \hspace{-0.9em}68.6 & \hspace{-0.9em}87.3 & \hspace{-0.9em}59.4 & \hspace{-0.9em}71.8 & \hspace{-0.9em}77.1 & \hspace{-0.9em}81.5 & \hspace{-0.9em}37.7 \\ 
        \rowcolor{color_full_lav}
        + Lavender Full-FT & \hspace{-0.9em}LM32-11B & \hspace{-0.9em}0.13M & \hspace{-0.5em}78.7 & \hspace{-0.9em}38.6 & \hspace{-0.9em}81.6 & \hspace{-0.9em}65.0 & \hspace{-0.9em}80.0 & \hspace{-0.9em}1695.9 & \hspace{-0.9em}\textbf{49.1} & \hspace{-0.9em}\textbf{49.8} & \hspace{-0.9em}686.0 & \hspace{-0.9em}70.1 & \hspace{-0.9em}87.6 & \hspace{-0.9em}\textbf{62.2} & \hspace{-0.9em}73.1 & \hspace{-0.9em}84.3 & \hspace{-0.9em}84.9 & \hspace{-0.9em}39.5 \\ 
        \midrule
        \multicolumn{19}{c}{\textbf{Small Data-Heavy SOTA Models ($<$20 B) with Massive FT Data ($\geq$5M)}} \\
        \midrule
        \rowcolor{tablegray}
        L.OneVision-7B & \hspace{-0.9em}Qwen-7B & \hspace{-0.9em}5.2M & \hspace{-0.5em}82.4 & \hspace{-0.9em}54.9 & \hspace{-0.9em}87.5* & \hspace{-0.9em}68.8* & \hspace{-0.9em}83.2 & \hspace{-0.9em}1993.6 & \hspace{-0.9em}47.9 & \hspace{-0.9em}\textbf{61.9} & \hspace{-0.9em}622.0 & \hspace{-0.9em}64.7 & \hspace{-0.9em}\textbf{88.4} & \hspace{-0.9em}\textbf{69.9} & \hspace{-0.9em}\textbf{76.7} & \hspace{-0.9em}95.4 & \hspace{-0.9em}78.3* & \hspace{-0.9em}31.6 \\ 
        \rowcolor{tablegray}
        InternVL2-8B & \hspace{-0.9em}InternVL2-8B & \hspace{-0.9em}5M & \hspace{-0.5em}83.6 & \hspace{-0.9em}\textbf{77.1} & \hspace{-0.9em}91.6* & \hspace{-0.9em}74.8* & \hspace{-0.9em}81.7* & \hspace{-0.9em}2215.1 & \hspace{-0.9em}51.2 & \hspace{-0.9em}61.5 & \hspace{-0.9em}794.0 & \hspace{-0.9em}42.6 & \hspace{-0.9em}84.2 & \hspace{-0.9em}64.2 & \hspace{-0.9em}75.4 & \hspace{-0.9em}\textbf{97.1} & \hspace{-0.9em}77.4* & \hspace{-0.9em}45.0 \\ 
        \rowcolor{tablegray}
        Qwen2-VL-7B & \hspace{-0.9em}Qwen2-7B & \hspace{-0.9em}\textasciitilde50M & \hspace{-0.5em}83.0 & \hspace{-0.9em}65.7 & \hspace{-0.9em}\textbf{94.5}* & \hspace{-0.9em}\textbf{76.5}* & \hspace{-0.9em}\textbf{83.0}* & \hspace{-0.9em}\textbf{2276.3} & \hspace{-0.9em}\textbf{53.7} & \hspace{-0.9em}60.7 & \hspace{-0.9em}\textbf{843.0} & \hspace{-0.9em}\textbf{67.5} & \hspace{-0.9em}\textbf{88.4} & \hspace{-0.9em}68.5 & \hspace{-0.9em}76.0 & \hspace{-0.9em}85.5 & \hspace{-0.9em}\textbf{84.3}* & \hspace{-0.9em}\textbf{50.4} \\ 
        \rowcolor{tablegray}
        Molmo-7B-O & \hspace{-0.9em}Qwen2-7B & \hspace{-0.9em}\textasciitilde35M & \hspace{-0.5em}\textbf{90.7} & \hspace{-0.9em}20.6 & \hspace{-0.9em}90.8 & \hspace{-0.9em}70.0 & \hspace{-0.9em}69.1 & \hspace{-0.9em}1714.7 & \hspace{-0.9em}39.3 & \hspace{-0.9em}50.1 & \hspace{-0.9em}666.0 & \hspace{-0.9em}15.1 & \hspace{-0.9em}86.7 & \hspace{-0.9em}67.5 & \hspace{-0.9em}72.7 & \hspace{-0.9em}88.8 & \hspace{-0.9em}80.4 & \hspace{-0.9em}42.5 \\ 
        \rowcolor{tablegray}
        Pixtral-12B & \hspace{-0.9em}Nemo-12B & \hspace{-0.9em}N.A. & \hspace{-0.5em}79.0 & \hspace{-0.9em}37.6 & \hspace{-0.9em}90.7 & \hspace{-0.9em}50.8 & \hspace{-0.9em}77.9 & \hspace{-0.9em}1921.7 & \hspace{-0.9em}52.5 & \hspace{-0.9em}54.5 & \hspace{-0.9em}685.0 & \hspace{-0.9em}64.7 & \hspace{-0.9em}84.2 & \hspace{-0.9em}65.4 & \hspace{-0.9em}71.5 & \hspace{-0.9em}87.2 & \hspace{-0.9em}75.7 & \hspace{-0.9em}47.0 \\ 
        \midrule
        \multicolumn{19}{c}{\textbf{Large State-of-the-Art Models ($>$20 B) with Massive FT Data ($\geq$5M)}} \\
        \midrule 
        \rowcolor{tablegray}
        Cambrian-1-34B & \hspace{-0.9em}Yi-34B & \hspace{-0.9em}10M & \hspace{-0.5em}79.7 & \hspace{-0.9em}49.2 & \hspace{-0.9em}75.5 & \hspace{-0.9em}46.0 & \hspace{-0.9em}81.4 & \hspace{-0.9em}1689.0 & \hspace{-0.9em}49.7 & \hspace{-0.9em}54.2 & \hspace{-0.9em}600.0 & \hspace{-0.9em}68.2 & \hspace{-0.9em}79.7 & \hspace{-0.9em}67.8 & \hspace{-0.9em}75.3 & \hspace{-0.9em}76.8 & \hspace{-0.9em}76.7 & \hspace{-0.9em}41.6 \\ 
        \rowcolor{tablegray}
        L.OneVision-72B & \hspace{-0.9em}Qwen2-72B & \hspace{-0.9em}5.2M & \hspace{-0.5em}85.6 & \hspace{-0.9em}63.9 & \hspace{-0.9em}91.3 & \hspace{-0.9em}74.9 & \hspace{-0.9em}85.8 & \hspace{-0.9em}2257.4 & \hspace{-0.9em}56.8 & \hspace{-0.9em}65.8 & \hspace{-0.9em}741.0 & \hspace{-0.9em}- & \hspace{-0.9em}86.6 & \hspace{-0.9em}71.9 & \hspace{-0.9em}77.5 & \hspace{-0.9em}90.2 & \hspace{-0.9em}80.5 & \hspace{-0.9em}47.9 \\ 
        \rowcolor{tablegray}
        Qwen2-VL-72B & \hspace{-0.9em}Qwen2-72B & \hspace{-0.9em}\textasciitilde50M & \hspace{-0.5em}88.1 & \hspace{-0.9em}69.8 & \hspace{-0.9em}\textbf{96.5} & \hspace{-0.9em}\textbf{84.5} & \hspace{-0.9em}\textbf{86.5} & \hspace{-0.9em}\textbf{2482.7} & \hspace{-0.9em}64.5 & \hspace{-0.9em}\textbf{68.3} & \hspace{-0.9em}\textbf{877.0} & \hspace{-0.9em}\textbf{73.7} & \hspace{-0.9em}87.2 & \hspace{-0.9em}\textbf{77.8} & \hspace{-0.9em}\textbf{77.9} & \hspace{-0.9em}\textbf{91.2} & \hspace{-0.9em}\textbf{85.5} & \hspace{-0.9em}\textbf{58.1} \\ 
        \rowcolor{tablegray}
        Molmo-72B & \hspace{-0.9em}Qwen2-72B & \hspace{-0.9em}\textasciitilde35M & \hspace{-0.5em}\textbf{96.3} & \hspace{-0.9em}- & \hspace{-0.9em}93.5 & \hspace{-0.9em}81.9 & \hspace{-0.9em}79.4 & \hspace{-0.9em}1992.0 & \hspace{-0.9em}54.1 & \hspace{-0.9em}63.3 & \hspace{-0.9em}701.0 & \hspace{-0.9em}- & \hspace{-0.9em}- & \hspace{-0.9em}75.2 & \hspace{-0.9em}- & \hspace{-0.9em}- & \hspace{-0.9em}83.1 & \hspace{-0.9em}46.6 \\ 
        \rowcolor{tablegray}
        Claude-3 Haiku & \hspace{-0.9em}N.A. & \hspace{-0.9em}N.A. & \hspace{-0.5em}86.7 & \hspace{-0.9em}24.5 & \hspace{-0.9em}88.8 & \hspace{-0.9em}56.1 & \hspace{-0.9em}60.7 & \hspace{-0.9em}1920.0 & \hspace{-0.9em}50.2 & \hspace{-0.9em}38.1 & \hspace{-0.9em}658.0 & \hspace{-0.9em}- & \hspace{-0.9em}74.4 & \hspace{-0.9em}45.5 & \hspace{-0.9em}63.3 & \hspace{-0.9em}- & \hspace{-0.9em}67.3 & \hspace{-0.9em}39.2 \\ 
        \rowcolor{tablegray}
        Claude-3.5 Sonnet & \hspace{-0.9em}N.A. & \hspace{-0.9em}N.A. & \hspace{-0.5em}94.7 & \hspace{-0.9em}54.1 & \hspace{-0.9em}95.2 & \hspace{-0.9em}74.3 & \hspace{-0.9em}79.7 & \hspace{-0.9em}1920.0 & \hspace{-0.9em}68.3 & \hspace{-0.9em}62.2 & \hspace{-0.9em}788.0 & \hspace{-0.9em}- & \hspace{-0.9em}73.6 & \hspace{-0.9em}60.1 & \hspace{-0.9em}72.2 & \hspace{-0.9em}88.9 & \hspace{-0.9em}74.1 & \hspace{-0.9em}49.9 \\ 
        \rowcolor{tablegray}
        GPT-4V (0409) & \hspace{-0.9em}N.A. & \hspace{-0.9em}N.A. & \hspace{-0.5em}89.4 & \hspace{-0.9em}57.3 & \hspace{-0.9em}87.2 & \hspace{-0.9em}75.1 & \hspace{-0.9em}81.0 & \hspace{-0.9em}2070.2 & \hspace{-0.9em}63.1 & \hspace{-0.9em}56.0 & \hspace{-0.9em}656.0 & \hspace{-0.9em}- & \hspace{-0.9em}81.8 & \hspace{-0.9em}61.4 & \hspace{-0.9em}73.0 & \hspace{-0.9em}84.8 & \hspace{-0.9em}78.0 & \hspace{-0.9em}43.9 \\ 
        \rowcolor{tablegray}
        GPT-4o (0513) & \hspace{-0.9em}N.A. & \hspace{-0.9em}N.A. & \hspace{-0.5em}94.2 & \hspace{-0.9em}\textbf{71.2} & \hspace{-0.9em}92.8 & \hspace{-0.9em}79.2 & \hspace{-0.9em}83.4 & \hspace{-0.9em}2310.3 & \hspace{-0.9em}\textbf{69.1} & \hspace{-0.9em}63.9 & \hspace{-0.9em}736.0 & \hspace{-0.9em}- & \hspace{-0.9em}85.6 & \hspace{-0.9em}75.4 & \hspace{-0.9em}77.1 & \hspace{-0.9em}90.7 & \hspace{-0.9em}77.4 & \hspace{-0.9em}55.0 \\ 
        \rowcolor{tablegray}
        Gemini 1.5 Pro & \hspace{-0.9em}N.A. & \hspace{-0.9em}N.A. & \hspace{-0.5em}94.4 & \hspace{-0.9em}28.4 & \hspace{-0.9em}93.1 & \hspace{-0.9em}81.0 & \hspace{-0.9em}73.9 & \hspace{-0.9em}2110.6 & \hspace{-0.9em}62.2 & \hspace{-0.9em}59.1 & \hspace{-0.9em}754.0 & \hspace{-0.9em}12.3 & \hspace{-0.9em}\textbf{88.2} & \hspace{-0.9em}64.1 & \hspace{-0.9em}76.0 & \hspace{-0.9em}85.7 & \hspace{-0.9em}78.7 & \hspace{-0.9em}55.9 \\ 
        \rowcolor{tablegray}
        Llama-3.2-90B & \hspace{-0.9em}Llama-3.1-70B & \hspace{-0.9em}N.A. & \hspace{-0.5em}92.3* & \hspace{-0.9em}54.1 & \hspace{-0.9em}85.7 & \hspace{-0.9em}- & \hspace{-0.9em}80.4 & \hspace{-0.9em}1741.0 & \hspace{-0.9em}60.3 & \hspace{-0.9em}55.3 & \hspace{-0.9em}783.0 & \hspace{-0.9em}- & \hspace{-0.9em}86.3 & \hspace{-0.9em}68.2 & \hspace{-0.9em}76.8 & \hspace{-0.9em}87.1 & \hspace{-0.9em}- & \hspace{-0.9em}44.1 \\ 
        \bottomrule
        \end{tabular}
        }
    \caption{\textbf{Zero-shot accuracy of various fine-tuned models across 16 VLM benchmarks.} 
    Results are grouped into four sections based on base model size and the scale of the fine-tuning dataset. The top score for each benchmark within each group is highlighted in bold.
    Scores for MiniCPM-V-2.5, Llama-3.2-11B, and their autoregressive (AutoR.) and Lavender fine-tuned variants are locally evaluated using OpenCompass Vlmevalkit \cite{duan2024vlmevalkit} with `gpt-4o' as the evaluator. All other scores are sourced from the OpenCompass Multi-Modal Leaderboard, evaluated with the same Vlmevalkit. When leaderboard results are unavailable, the models' published numbers are cited, marked with a * notation if provided.
    Our observations are  categorized as follows:  
    %
    1) \textit{Small Budget-Constrained Models.}  
    This group is most comparable to Lavender in terms of parameter size and fine-tuning data scale. Both Lavender versions outperform the majority of benchmarks with significant margins over the second-best-performing external models, achieving improvements of up to 40\% on CCBench. On a few benchmarks (e.g., MMStar, POPE, and SEED-IMG), Lavender is surpassed by baseline models, though the difference is within 4\% for SEED-IMG. Lavender-Llama-3.2 occasionally underperforms on MME due to privacy protection constraints (see \cref{tab:mme_failure}). 
    %
    2) \textit{Small Data-Heavy SOTA Models.}  
    The primary argument for Lavender in this work is its ability to achieve improvements over the autoregressive fine-tuning baseline. Lavender implementations on MiniCPMv2.5 and Llama-3.2-11B are not designed to beat state-of-the-art results due to the limited fine-tuning data scale (0.13M), which is significantly smaller than the 5M to 50M datasets used for this group, approximately 38x to 384x larger than that used for Lavender.  
    %
    We simplify the comparison by excluding pretraining dataset sizes for base models. In this group, Qwen2-VL-7B and LLaVA-OneVision-7B are the top performers, surpassing Lavender on most benchmarks. However, we note that the performance gap is likely influenced by the composition of fine-tuning datasets, as discussed in \cref{sec:correlation}.  
    %
    3) \textit{Large SOTA Models.}  
    Finally, we include results from the latest state-of-the-art models, which are at least 20B in size (or unreleased models typically exceeding 100B) and fine-tuned on datasets of at least 5M samples.  
    %
    Despite Qwen2-VL-72B outperforming all other models on 18/20 benchmarks, Lavender pushes the boundaries of Llama-3.2-11B, achieving performance comparable to certain closed-source models that are at least an order of magnitude larger (e.g., Claude-3.5 Sonnet, GPT-4o, and Gemini 1.5 Pro) on benchmarks such as TextVQA, POPE, RealWorld, and DocVQA.
    }
    \label{tab:main_table_full}
\end{table*}
