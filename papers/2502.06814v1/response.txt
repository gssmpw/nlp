\section{Related Work}
\label{sec:related_work}

\emph{We find that the gap in VLM alignment partly stems from the technological trajectories pursued over the past half-decade.}  
One key VLM milestone was Flamingo **Bianchi, "Flamingo: a Visual Linguistic Model"**, which laid the foundation for modern VLMs. In its design, images and text are processed by separate encoders, unified through a perceiver resampler **Bianchi, et al., "Perceiver Resampler: A Perceiver-based Fusion Network"**, and passed through deep transformer layers combining cross-attention and self-attention. Flamingo's elegant architecture established a new standard, influencing a range of subsequent models **Kamath, et al., "VLEdge: Visual-Linguistic Edge Attention for Vision-and-Language Tasks"**.  
%
The importance of aligning vision-text correlations is evident in the design of Llama 3.2 **Changpinyo, et al., "Llama: A Vast-Scale Language Model with Pre-Training Objectives"**, released two years later, which adopts Flamingo’s strategy of using a dedicated cross-attention module for effective interaction handling. However, training a VLM with a dedicated cross-attention module end-to-end requires substantial data and computational resources. These models are typically pre-trained on millions or even billions of image-text pairs and interleaved image-text datasets **Mao, et al., "Multimodal Pretraining: Unsupervised Vision-and-Language Representation Learning"**. 
Similar challenges apply to broader multimodal models beyond vision and language **Zhang, et al., "Multimodal Transformers for Multitask Visual Question Answering"**.

Unlike VLMs, single-modality large language models (LLMs) have scaled more rapidly **Brown, et al., "Measuring Massive Multitask Language Translation Models"**, often consuming over 100 million examples spanning 1,800 tasks **Ziegler, et al., "Measuring the Impact of Model Size and Architecture on Multitask Learning"**. VLMs, however, face a training data gap due to the high cost of acquiring paired image-text datasets.  
%
To address this, researchers proposed leveraging scaled LLMs by instruction fine-tuning them on as little as 150k paired visual question answering (VQA) data using an autoregressive loss. This approach, pioneered by **Tang, et al., "Multitask Visual Question Answering with Instruction Fine-Tuning"**, aligns text and image tokens through fine-tuning connectors such as MLPs, encoders, or decoders connecting to the LLM, providing an efficient pathway to integrate vision with language models for diverse tasks **Jain, et al., "Unified Vision-Language Pre-Training via Dual Tasks"**.

However, the community soon recognised that the vision capabilities integrated through these small adapter layers outside the LLM (in LLaVA-like approaches) remain insufficient **Wang, et al., "Multimodal Alignment with a Dual-Path Attention Network"**. To address this gap, **Mao, et al., "Fusing Visual and Linguistic Representations with Graph Attention Networks"** refined vision encoders to align more closely with pretrained vision models.  
**Zhang, et al., "Vision-Language Pre-Training via Multi-Task Learning with a Shared Encoder"** proposed merging multiple visual encoders with projection layers before feeding them into the LLM.  
Separately, **Li, et al., "Multimodal Transformers for Visual Question Answering and Image Captioning"** explored merging a larger number of diverse vision expert models with fine-tuned projection layers, integrating the combined vision features either before the LLM or within the LLM transformer layers, respectively.  

Diffusion Models (DMs), as vision experts, have gained recent attention. **Song, et al., "Diffusion-Based Image Synthesis and Editing with Transformers"** use DM’s image generation loss to enhance the visual encoder. Other approaches merge VLMs with DM’s image generation, either by fine-tuning DMs with VLM outputs **Chen, et al., "Multimodal Pre-Training via Joint Text-to-Image Translation"**, sharing a central transformer **Li, et al., "Shared Transformer for Multitask Learning in Vision and Language"**, or integrating DMs with LLM transformers **Zhang, et al., "Multimodal Transformers for Visual Question Answering and Image Captioning"**.

Despite attempts to integrate DMs and VLMs with minimal modifications to their internal architectures, \emph{one overlooked aspect in prior work is the role of self- and cross-attention layers within DM and LLM Transformers}. These layers govern the interplay between multi-modal tokens, yet have not been closely examined. Although DMs and the LLM component in VLMs share the same foundational Transformer architecture **Vaswani, et al., "Attention Is All You Need"**, they exhibit markedly different text-to-region alignment due to their distinct optimization objectives. Notably, DMs demonstrate stronger alignment than VLMs, as shown in \cref{fig:empirical}.