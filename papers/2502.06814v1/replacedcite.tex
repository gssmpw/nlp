\section{Related Work}
\label{sec:related_work}

\emph{We find that the gap in VLM alignment partly stems from the technological trajectories pursued over the past half-decade.}  
One key VLM milestone was Flamingo ____, which laid the foundation for modern VLMs. In its design, images and text are processed by separate encoders, unified through a perceiver resampler ____, and passed through deep transformer layers combining cross-attention and self-attention. Flamingo's elegant architecture established a new standard, influencing a range of subsequent models ____.  
%
The importance of aligning vision-text correlations is evident in the design of Llama 3.2 ____, released two years later, which adopts Flamingo’s strategy of using a dedicated cross-attention module for effective interaction handling. However, training a VLM with a dedicated cross-attention module end-to-end requires substantial data and computational resources. These models are typically pre-trained on millions or even billions of image-text pairs and interleaved image-text datasets ____. 
Similar challenges apply to broader multimodal models beyond vision and language ____.

Unlike VLMs, single-modality large language models (LLMs) have scaled more rapidly ____, often consuming over 100 million examples spanning 1,800 tasks ____. VLMs, however, face a training data gap due to the high cost of acquiring paired image-text datasets.  
%
To address this, researchers proposed leveraging scaled LLMs by instruction fine-tuning them on as little as 150k paired visual question answering (VQA) data using an autoregressive loss. This approach, pioneered by ____, aligns text and image tokens through fine-tuning connectors such as MLPs, encoders, or decoders connecting to the LLM, providing an efficient pathway to integrate vision with language models for diverse tasks ____.

However, the community soon recognised that the vision capabilities integrated through these small adapter layers outside the LLM (in LLaVA-like approaches) remain insufficient ____. To address this gap, ____ refined vision encoders to align more closely with pretrained vision models.  
____ proposed merging multiple visual encoders with projection layers before feeding them into the LLM.  
Separately, ____ explored merging a larger number of diverse vision expert models with fine-tuned projection layers, integrating the combined vision features either before the LLM or within the LLM transformer layers, respectively.  

Diffusion Models (DMs), as vision experts, have gained recent attention. ____ use DM’s image generation loss to enhance the visual encoder. Other approaches merge VLMs with DM’s image generation, either by fine-tuning DMs with VLM outputs ____, sharing a central transformer ____, or integrating DMs with LLM transformers ____.

Despite attempts to integrate DMs and VLMs with minimal modifications to their internal architectures, \emph{one overlooked aspect in prior work is the role of self- and cross-attention layers within DM and LLM Transformers}. These layers govern the interplay between multi-modal tokens, yet have not been closely examined. Although DMs and the LLM component in VLMs share the same foundational Transformer architecture ____, they exhibit markedly different text-to-region alignment due to their distinct optimization objectives. Notably, DMs demonstrate stronger alignment than VLMs, as shown in \cref{fig:empirical}.