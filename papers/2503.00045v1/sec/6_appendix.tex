\newpage
\section{Appendix}
\subsection{Preliminary: Latent Diffusion Model}
The diffusion model~\citep{ho2020denoising} is a sophisticated probabilistic approach for image generation.  It typically encompasses two phases: the forward diffusion process in which the data is gradually injected with Gaussian noise, and the reverse process that learns to reconstruct the original data from the noise.

Specifically, in the forward diffusion process, the data $\mathbf{x}_0$ is transformed into a sequence of latent variables $\mathbf{x}_1, \dots, \mathbf{x}_T$ following a predefined noise schedule $\beta_1, \dots, \beta_T$:
\begin{equation}
    q(\mathbf{x}_{1:T} | \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t | \mathbf{x}_{t-1}), \quad 
    q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t;\sqrt{1-\beta_t}\mathbf{x}_{t-1},\beta_t \mathbf{I}).
\end{equation}

Suppose that $p(\bx_T)=\mathcal{N}(\bx_T; \bzero, \bI)$. The reverse process is modeled as a Markov chain with learned transitions $\theta$, aiming to recover the original data from the noisy latent variables:
\begin{equation}
    p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T)\prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t), \quad
    p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)).
\end{equation}

To maintain generation quality with limited computational resources, latent diffusion models like Stable Diffusion~\citep{rombach2022high} perform the forward and reverse processes in the latent space of pretrained VAE~\citep{kingma2013auto}. As in Fig.~\ref{fig:main_arch}, the encoder function $\mathcal{E}$ maps the input data $\mathbf{x}_0$ to a latent space representation $\mathbf{z}_0 = \mathcal{E}(\mathbf{x}_0)$, and the decoder $\mathcal{D}$ reconstructs the data from the latent space: $\mathbf{x}_0 = \mathcal{D}(\mathbf{z}_0)$.

\subsection{More Experiments and Visualization Results}
\input{table/6_ms_training}
\textbf{Impact of multi-scale training.} Following the training configuration established in Panacea, multi-scale training is adopted as the default strategy. Specifically, the original images are randomly resized to scales ranging from 0.3 to 0.36 of their original dimensions. As demonstrated in Tab.~\ref{tab:ms_training}, multi-scale training exhibits negligible effects on both perception and generation performance. To streamline the implementation, our proposed framework, Glad, retains multi-scale training as its default configuration.

\input{table/7_new_traj_av2}
\textbf{Enhancing model performance with external trajectory generation.} We explore cross-domain scenario generation by incorporating trajectories from the Argoverse 2 (AV2) dataset~\citep{wilson2023argoverse}, which contains 360-degree perception data and diverse driving scenarios. Using Far3D~\citep{jiang2024far3d} as the unified evaluation framework (compatible with both AV2 and nuScenes datasets), we observe 1.06 improvement in terms of NDS on the nuScenes validation set (Tab.~\ref{tab:external_traj_av2}). This demonstrates that trajectory-based scene generation can enhance model generalization despite domain gaps.

\input{table/8_train_gene_only}
\textbf{Performance comparision with exclusively generated data.} In Tab.~\ref{tab:train_gene_only}, we present the performance of Glad only using the generated data for training. When trained solely on synthetic data, Glad outperforms Panacea by 4.6 in terms of mAP and 4.7 in terms of NDS.

\input{table/9_per_cls_det}
\textbf{Detail of the detection performance.} In Tab.~\ref{tab:per_class_det}, the detailed detection performance of the 10 classes can reflect the generation quality of each category. Since Panacea does not provide results broken down by category, we compare our results with the Oracle results from StreamPETR. It can be observed that common categories such as Car and Traffic cone have higher generation quality compared to rare categories like Construction Vehicles and Trailer. 

\input{figs/7_object_statics}
\textbf{Object continuous existence frames analysis.} We analyze object persistence in frames to understand why using more video frames (such as 16 frames instead of 8) does not further improve downstream task performance. As shown in Fig.~\ref{fig:obj_stat}, our analysis of the nuScenes dataset reveals that 96.2\% of objects appear in fewer than 8 consecutive frames, and 99.7\% appear in fewer than 16 consecutive frames.

\input{figs/8_noise_prior}
\input{table/10_error_accu}
\input{table/11_temporal}

\textbf{Ablation study on error accumulation.} As shown in Fig.~\ref{fig:noise_prior}, we evaluate error accumulation by comparing three distinct methods of modifying the noise prior. In the Tab.~\ref{tab:error_accu}, using denoised latent features achieves the best performance in all evaluation metrics. Therefore, we utilize the denoised latent feature. 

\textbf{Ablation study on temporal attention layer.} As shown in the Tab.~\ref{tab:temporal}, using the temporal layer for propagating temporal information yields an FID score of 17.91 and an FVD score of 183. While the FID is slightly worse than that of latent variable propagation (LVP), the FVD is marginally better. This suggests that the temporal layer more effectively models temporal propagation but has limitations in spatial modeling of single frames. Overall, LVP achieves a better balance in both spatial modeling and temporal propagation. Moreover, the approach using the temporal layer, which requires modeling multiple frames simultaneously, is considerably more expensive in terms of memory consumption and training time. 

\textbf{Visualization results on corner case scenes.}  To generate corner cases, we utilize the CODA~\citep{li2022coda} dataset, which is specifically designed for real-world road corner cases. CODA encompasses three major autonomous driving datasets, including 134 scenes from nuScenes. We focus on this subset of 134 nuScenes scenes. After filtering to identify which scenes belong to the validation set, we retain 30 valid scenes. These 30 scenes serve as our corner case examples, and we present selected generation results in Fig.~\ref{fig:corner_cases}.
\newpage
\input{figs/9_corner_cases}