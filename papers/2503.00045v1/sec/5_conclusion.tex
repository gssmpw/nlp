\section{Conclusion}
In this paper, we introduce a simple framework, named \modelname, to generate video data in an online framework. Our proposed \modelname extends Stable Diffusion for video generation by introducing two novel modules, including latent variable propagation and streaming data sampler. The latent variable propagation views the denoised latent features of previous frame as noise prior for image generation at current frame, leading to maintain a good temporal consistency. The streaming data sampler samples the video frame orderly at continuous iterations, enabling efficient training. We perform the experiments on the widely-used dataset nuScenes, which demonstrates the efficacy of our proposed method as a strong baseline for generation tasks.


\noindent\textbf{Limitations and future work:} We observe that our proposed method struggles to generate high-quality video data under high dynamic scenes. In these scenes,  the temporal consistency of objects still needs to be improved. In future, we will explore improving high dynamic object generation. 

\section{Acknowledge}
The work was supported by National Science and Technology Major Project of China (No. 2023ZD0121300), and also supported by National Natural Science Foundation of China (No. 62271346).