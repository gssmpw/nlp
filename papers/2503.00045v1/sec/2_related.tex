\section{Related works}
\subsection{Video Generation Models}
Recent years have seen considerable advancements in video generation, focusing on improving the quality, diversity, and controllability of generated content. Initial methods \citep{yu2022generating,clark2019adversarial,tulyakov2018mocogan} extend the success of Generative Adversarial Networks (GANs) from image synthesis to video generation. However, these approaches often face challenges in maintaining temporal consistency.

The introduction of diffusion models~\citep{ho2020denoising,rombach2022high} represents a paradigm shift, providing a robust alternative for high-fidelity video synthesis. VDM~\citep{ho2022video} utilizes factorized spatio-temporal attention blocks to generate 16-frame, 64$\times$64 pixels videos, which are then upscaleable to $128\times128$ pixels and 64 frames using an enhanced model. ImagenVideo~\citep{ho2022imagen} further progresses the field through a cascaded diffusion process, beginning with a base model that produces videos of 40$\times$24 pixels and 16 frames, and sequentially upsampling through six additional diffusion models to reach $1280\times768$ pixels and 128 frames. MagicVideo~\citep{zhou2022magicvideo} employs Latent Diffusion Models (LDM) to enhance efficiency in video generation. VideoLDM~\citep{blattmann2023align}, leveraging a similar architecture, incorporates temporal attention layers into a pre-trained text-to-image diffusion model, excelling in text-to-video synthesis. TRIP~\citep{zhang2024trip} proposes a new recipe of image-to-video diffusion paradigm that pivots on image noise prior derived from static image. In contrast, our Glad uses latent features from the previous frame as noise prior, replacing Gaussian noise, while TRIP predicts a "reference noise" for the entire clip, risking inaccuracy over time. Vista~\citep{gao2024vista} presents a generalizable driving world model with high fidelity and versatile controllability. Additionally, Sora~\citep{sora} significantly enhances video quality and diversity, while introducing capabilities for text-prompt driven control of video generation.

Despite these advancements, video generation still faces several challenges, including maintaining temporal consistency, generating longer videos, and reducing computational costs. A streaming video generation pipeline could present an elegant solution. Notably, several advanced works \citep{yan2021videogpt,hong2022cogvideo,huang2022single,henschel2024streamingt2v} have adopted an autoregressive approach to generate video frames. VideoGPT~\citep{yan2021videogpt} predicts the current latent code from the previous frame, Autoregressive GAN~\citep{huang2022single} generates frames based on a single static frame, and CogVideo~\citep{hong2022cogvideo} employs a GPT-like transformer architecture. 
ART$\cdot$V~\citep{weng2024art} firstly explores autoregressive text-to-video generation and  adopts multi-reference frames and anchor frame as condition, while our \modelname achieves streaming video generation by using previous latents as initial noise in the diffusion process. Nonetheless, these methods do not yet fully satisfy the stringent requirements for generation quality, controllability, and motion dynamics essential in autonomous driving applications.

\subsection{Driving Diffusion Models}
In the realm of driving scenario generation, early research predominantly focuses on synthesizing individual images.  BEVGen~\citep{yan2024street} explores the generation of multi-view street images from a BEV layout. BEVControl~\citep{yang2023bevcontrol} incorporates cross-view attention to enhance visual consistency and ensure a cohesive scene representation. MagicDrive~\citep{gao2023magicdrive} highlights the challenges associated with the loss of 3D geometric information after the projection from 3D to 2D. In addition to the cross-view consistency, cross-frame consistency remains crucial for temporal modeling. Based on this point, DrivingDiffusion~\citep{li2023drivingdiffusion} and Panacea~\citep{wen2023panacea} introduce sequences of BEV layouts to generate complex urban videos in a controllable manner. However, their sliding-window approach to video generation proves inefficient and unsuitable for extended durations. Moreover, DriveDreamer-2~\citep{zhao2024drivedreamer} and ADriver-I~\citep{jia2023adriver} use the initial frame as a reference, demonstrating significant potential in autonomous driving simulation by predicting subsequent video sceness. GAIA-I~\citep{hu2023gaia} generates long-duration, realistic video data conditioned on diverse inputs such as images, text, and action signals. Unfortunately, the capability to manipulate motion trajectories of other vehicles through control signals is limited in~\citep{hu2023gaia}. To meet the demands of data generation and simulation, the challenges such as extended video length, controllability, and consistency still hold significant value.












