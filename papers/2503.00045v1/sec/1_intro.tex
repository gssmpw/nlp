\section{Introduction}
 
Autonomous driving tasks are usually data-intensive, relying on vast amounts of data to learn informed models. In recent years, autonomous driving has achieved considerable progress, particularly in the field of Bird's Eye View (BEV) perception~\citep{huang2021bevdet,liu2022petr,li2022bevformer,liu2023petrv2,li2023bevdepth,liao2022maptr} and end-to-end planning~\citep{shi2016uniad,jiang2023vad,chen2024vadv2}, thanks to the availability of public datasets such as nuScenes~\citep{nuscenes2019}, CARLA~\citep{dosovitskiy2017carla}, Waymo~\citep{ettinger2021large}, and ONCE~\citep{mao2021one}. However, these real-world driving datasets exist several limitations. On one hand, it is expensive and labor-intensive to collect large-scale real-world driving data. On the other hand, although corner-cases comprise only a small portion of dataset, they are more important for evaluating the safety of autonomous driving. Therefore, the scale and diversity of real-world datasets constrain the further development of autonomous driving.

\input{figs/1_intro}

Instead of collecting large-scale real-world datasets, researchers have explored generating synthetic street-view data. Some approaches focus on leveraging Neural Radiance Fields (NeRF)~\citep{yang2023emernerf,yan2024oasim,yang2023unisim,tonderski2023neurad} and 3D Gaussian Splatting (3DGS)~\citep{yan2024street} for rendering the novel views in driving scenes. These cutting-edge approaches offer a powerful tool for rendering highly realistic and detailed images from various viewpoints. However, as shown in Fig. \ref{fig:comp_arch}(a), these approaches struggle to reconstruct the non-seen streets when the source trajectory and simulated trajectory are different. Recently, some approaches~\citep{gao2023magicdrive,yang2023bevcontrol,swerdlow2024street,li2023drivingdiffusion,jia2023adriver,hu2023gaia} attempt to employ Stable Diffusion~\citep{rombach2022high} to generate the synthetic data of driving scenes. For instance, Panacea~\citep{wen2023panacea} and DriveDreamer-2~\citep{zhao2024drivedreamer} have delved into the ability of Stable Diffusion to generate synthetic video data from a given initial frame. However, as shown in Fig.~\ref{fig:comp_arch}(b),  these diffusion models (e.g., Panacea) are limited to offline video generation and require high memory consumption. Moreover, it fails to dynamically generate video data in response to the changes in simulated trajectory. For a flexible generator, it requires the ability of generating dynamic scenes corresponding to the simulated trajectory.

To address the issues mentioned above, we propose \modelname, a simple yet effective framework that generates and forecasts synthetic video data  in an online (i.e., frame-by-frame) manner. Our  \modelname is based on Stable Diffusion, and introduces the latent variable propagation (LVP) to maintain temporal consistency when performing frame-by-frame generation. In LVP, the latent features of previous frame are viewed as the  noise prior injected into the latent features of current frame. In addition, we introduce a streaming data sampler (SDS), which aims to keep the consistency between training and inference, and present an efficient data sampling for training. In SDS, we sample original frames in video clip one by one at continuous iterations, and save the generated latent features in cache used for the noise prior of next iteration.

With such streaming generation manner, our  \modelname is capable of generating videos of arbitrary lengths in theory. Specifically, it can generate the videos of the novel scene from the noise when serving as the data generator. In addition, given the reference frame, it can produce the video data of specific scene. We perform the experiments on the public autonomous driving dataset nuScenes, which demonstrates the efficacy of our \modelname. The contributions and merits can be summarized as follows:

\begin{itemize}
\item 
A simple yet effective framework, named \modelname, is proposed to generate  video data in an online  manner, instead of offline fixed-length manner. Theoretically, our proposed \modelname is able to generate videos of arbitrary lengths, showcasing substantial potential in data generation and simulation.

\item A latent variable propagation strategy is introduced to ensure the temporal consistency for frame-by-frame video generation, where the latent features of previous frame are viewed as the noise fed to the latent features of current frame.


\item We further design a streaming data sampler to provide efficient training. In the streaming data sampler, we sample the frame in video clip one by one at several continuous iterations, and save the latent features in cache for next iteration.

\item The experiments are performed on the widely-used dataset nuScenes. Our \modelname  is able to generate high-quality video data  as generator. Further, our generated video data can significantly improve the perception, tracking, and HD map construction performance. 
\end{itemize}