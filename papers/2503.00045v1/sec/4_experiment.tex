\section{Experiments}
\subsection{Datasets and Implementation Details}
\noindent\textbf{nuScenes dataset.} The nuScenes dataset was collected from 1000 different driving scenes in Boston and Singapore. These scenes are split into training, validation, and test sets. Specifically, the training set contains 700 scenes, the validation set contains 150 scenes, and the test set contains 150 scenes. The dataset has totally 10 object classes, and provides accurate 3D bounding-box annotations for each object. In every scene, there are 6 camera views and each view records a length of about 20 second driving video.

\noindent\textbf{Evaluation metrics.}  
We  evaluate the quality of the generation and simulation, respectively. To quantitatively evalute the generation quality, we adopt the frame-wise Fréchet Inception Distance (FID) \citep{parmar2022aliased} and Fréchet Video Distance (FVD) \citep{unterthiner2018towards}, which  calculate the Fréchet distance between generated content (image or video) and ground truth. To evaluate the model's ability to adhere the given input conditions, such as the BEV layout, we follow~\citep{liao2022maptr} and report the performance of online vectorized HD map construction. This includes the average precision (AP) for constructing pedestrian crossings (AP$_{ped}$), lane dividers (AP$_{divider}$), road boundaries (AP$_{boundary}$), and their weighted average (mAP).
To evaluate the potentiality of \modelname as simulator, we assess the 3D detection performance on the nuScenes dataset, including nuScenes Detection Score (NDS) and mean Average Precision (mAP). To further evaluate the temporal consistency of the generated videos, we also report the multi-object tracking results, including average multiple object tracking accuracy (AMOTA) and average multiple object tracking precision (AMOTP).

\noindent\textbf{Implementation details.}
Our \modelname is implemented based on Stable Diffusion 2.1~\citep{rombach2022high}. We train our models on 8 NVIDIA A100 GPUs with the mini-batch of 2 images. During training, we first perform image-level pre-training. % on our in-house image dataset
Constant learning rate $4 \times 10^{-5}$ has been adopted, and there are 1.25M iterations totally. Afterwards, we fine-tune our \modelname on nuScenes dataset with same  settings for 48 epochs. We split each video into 2 clips to balance video length and data diversity. During inference, we utilize the DDIM~
\citep{song2020denoising} sampler with 25 sampling steps and scale of the CFG as 5.0. The image is generated at a spatial resolution of $256 \times 3072$ pixels with 6 different views, and split it to  6 images of $256 \times 512$ pixels for evaluation. We adopt StreamPETR~\citep{wang2023exploring} with  ResNet-50~\citep{he2016deep} backbone as perception model. 
Regarding the map construction model, we employ MapTR~\citep{liao2022maptr} with ResNet-50 and retrain it under a $512 \times 256$ pixels resolution setting.
The inference time of complete denoising process is reported in single NVIDIA A100 GPU.

\subsection{Main Results}

\input{table/1_fid}

\noindent\textbf{Generation quality.} 
We first online generate video data on nuScenes validation set, and employ the sliding window strategy to evaluation synthetic video quality similar to existing approach~\citep{wen2023panacea}. Tab.~\ref{tab:sota} compares data generation quality of our proposed \modelname and some state-of-the-art approaches. We present the results of our \modelname respectively using Gaussian noise or from a given reference frame. When generating video using  Gaussian noise, \modelname achieves an FID of 12.57 and an FVD of 207. Furthermore, when generating videos starting from reference frame, \modelname achieves a lower FID of 11.18 and FVD of 188. Compared to the offline approaches, our online \modelname achieves promising performance in terms of both FID and FVD. For instance, our \modelname without reference frame outperforms MagicDrive by 3.63 and 11 in terms of FID and FVD respectively. 
We evaluate the generation speed of our method and some open-source methods. To generate multi-view frames, BEVGen requires 6.6s, MagicDrive takes 11.2s, and our \modelname takes 9.4s. Namely, our \modelname has a comparable generation speed with these approaches, but has better generation quality.

\input{table/2_nuscenes_val}
\noindent\textbf{Practicality for generation.} 
To validate the practicality of our method  in driving scenes, we generate all frames in the nuScenes validation set using BEV layout. Instead of using sliding window strategy, we generate the video per 8 frames. We employ the video-based 3D object detection approach, StreamPETR~\citep{wang2023exploring}, to perform 3D object detection on both ground-truth and synthetic validation set. Tab.~\ref{tab:valset} reports 3D detection and multi-object tracking results. It can be observed that, compared to Panacea \citep{wen2023panacea}, our \modelname performs better on all 3D object detection metrics, which reflect that our \modelname can generate more realistic videos for 3D object detection. Further, our \modelname achieves an mAP of 28.3 and an NDS of 41.3, which correspond to 74.9\% and 83.6\% of the Oracle, respectively. We also report the results when generating the video per 16 frames, the performance of the 3D object detection and tracking drop slightly.
\input{table/3_nuscenes_train}

Another crucial application is the expansion of  driving dataset to boost the performance of perception model. We first generate synthetic nuScenes training set using Gaussian noise as the reference frame and BEV layout. Then, we  first pre-train StreamPETR on this synthetic training set, and then fine-tune it on real training set. Tab.~\ref{tab:augment} presents the results of baseline, Panacea \citep{wen2023panacea}, and our \modelname. Compared to the offline Panacea, our online approach \modelname achieves the comparable performance, significantly outperforming the baseline. We further validate the effectiveness using nuImage pre-training that can provide a better  initialization. The baseline achieves the NDS of 49.4 and AMOTA of 33.9, having 2.5 and 3.7 improvements compared to using ImageNet~\citep{deng2009imagenet} pre-training, respectively. Compared to the stronger baseline, our \modelname also has 1.9 improvement in terms of NDS, and 2.6 improvement in terms of AMOTA. 

\input{table/4_maptr}
Furthermore, we report the performance of our model on online vectorized HD map construction. As shown in Tab.~\ref{tab:maptr}(a), for the generated validation set, our \modelname achieves an mAP of 32.0, while the Oracle performance is 47.2. In Tab.~\ref{tab:maptr}(b), after pre-training on the generated training set, compared to the baseline, our \modelname demonstrates an improvement of 4.9 in mAP.

\input{figs/4_simulation}
\noindent\textbf{Potentiality for simulation.}  To assess the capability of \modelname  as a simulator, we design an experiment to evaluate simulation stability over time. Specifically, we generate video data using reference frame, and perform 3D detection from frame 1 to frame $N$ respectively, and compare the detection performance on simulated data and real data. Fig.~\ref{fig:simu} gives the relative detection performance between simulated data and real data. The lower relative detection performance is, the simulated data  collapse. Although the performance begins to decline, it quickly converges at a high percentage, such as the NDS around 85\%. It demonstrates that our \modelname has a good potentiality for simulation. 


\subsection{Ablation Study}
\input{table/5_ablation}
Here we employ StreamPETR \citep{wang2023exploring} to evaluate detection and tracking performance for ablation study.

\noindent\textbf{On the LVP module.} 
Tab. \ref{tab:ablation-b}(a) compares  baseline and our \modelname. The baseline directly generates video data from Gaussian noise, while our Glad uses latent variable propagation (LVP). Compared to the baseline, our Glad with LVP has the improvement of 1.3 and 1.5 in terms of NDS and mAP. We also report the quality of image generation, where the FID reduces from 20.85 to 12.57. These results demonstrate that the simple yet effective LVP module is vital for our \modelname.

\noindent\textbf{On the SDS module.} 
Tab.~\ref{tab:ablation-a}(b) compares memory usage on single NVIDIA A100 GPU of streaming data sampler (SDS) in our \modelname and sliding window sampler (SWS) in Panecea. Compared to SWS, our SDS does not increase  memory usage with the increasing video length. The reason is that we sample video orderly at continuous iterations.

\noindent\textbf{Impact of the number of video chunks during training.} During training, we split original training video into several video chunks to balance video length and data diversity. Tab.~\ref{tab:ablation-c}(c) gives the impact of different length of video chunks. When the number of chunks is equal to 2, it has the best performance.

\noindent\textbf{Impact of the length of  generated video during inference.} During inference, we can generate video data of different lengths. Tab.~\ref{tab:ablation-d}(d) gives the impact of different video lengths during inference. We observe that it has the best performance when the length is equal to 8.  When the length is larger, the performance  slightly drops.

\noindent\textbf{Impact of LN Layer.} We employ a LN layer to normalize the latent features as in Eq. \ref{eq:noise}. Without LN layer, we observe the training loss escalating to NAN,  highlighting the importance of LN layer.
 
\input{figs/5_vis_noise}
\input{figs/6_vis_bev}
\subsection{Qualitative Analysis}

Here we provide multi-view generation examples of \modelname. As in Fig.~\ref{fig:vis_noise}, as a generator, our \modelname generates multi-view images (corresponding to each row) with strong cross-view consistency. Moreover, \modelname  maintains strong temporal consistency from frame 1 to frame 8. In Fig.~\ref{fig:vis_bev}, we demonstrate the capability of generating outputs under edited input conditions using our \modelname. The first row displays the ground truth as a reference. Subsequent rows show frames selected from a video based on the same reference frame, with modified conditions for each current frame. In the second row, frames are generated under original conditions. In the third row, a car following the ego is removed. In the fourth row, a new branch road is added, and in the last row, the ego vehicle switch lanes to the adjacent one. Our \modelname effectively manages these conditions while maintaining consistency across frames under the same reference.