\section{Method}

\input{figs/2_main_arch}

Here we introduce our proposed \modelname, a simple yet effective approach that denoises the current frame from the fully denoised latent feature of the previous frame instead of Gaussian noise, which generates and simulates video data in an online manner. The proposed \modelname is based on Stable Diffusion \citep{rombach2022high}, and introduces two novel modules for generating video data, including latent variable propagation and streaming data sampler. The latent variable propagation  aims to maintain temporal consistency when performing frame-by-frame generation, while streaming data sampler provides an efficient data sampling pipeline and keeps data consistency between training and inference. Our proposed \modelname is capable of both generating an entirely new scene from  Gaussian noise and simulating  a specific scene based on a given reference frame. 

\noindent\textbf{Overview.} Fig.~\ref{fig:main_arch} presents the overall architecture of our proposed \modelname. Given the Gaussian noise or latent features $\mathbf{z}^0$ of the reference frame, we introduce latent variable propagation (LVP) to process it first and then view it as the noise-added latent features $\mathbf{z}^1$ of frame 1. Afterwards, we employ denoising UNet to recover the  latent features $\mathbf{z}^1$, which are fed to VAE decoder for image generation and LVP module for latent features propagation. By analogy, we can generate synthetic images from frame 1 to frame $N$, which together form a continuous video sequence. Compared to the fixed-length video generation in  existing approaches \citep{wen2023panacea,zhao2024drivedreamer}, our frame-by-frame video generation approach is more efficient and flexible, which can theoretically generate the videos of arbitrary  lengths. In addition, similar to most existing approaches, we employ ControlNet \citep{zhang2023adding} and CLIP \citep{Radford_2021_CLIP} to condition feature extraction of denoising UNet  by given text prompt and BEV semantic layout.

\subsection{Latent Variable Propagation}

Our latent variable propagation (LVP) aims to maintain the temporal consistency in frame-by-frame video generation. The distribution of noise plays an essential role in conditioning  image synthesis of diffusion models.  When extending image-level diffusion model Stable Diffusion \citep{rombach2022high} to generate video sequences, how to generate the noise distribution across time  is crucial to maintain video temporal consistency. 
One straightforward way is to sample the noise from the same distribution for image generation at different frames similar to offline approaches \citep{wen2023panacea}. However it cannot maintain temporal consistency in our frame-by-frame generation design. To address this issue, our proposed LVP  views the denoised latent features at previous frame as the noise prior for image generation at current frame. Based on the LVP module, we can orderly generate video sequences from frame 1 to frame $N$, while maintaining good temporal consistency.

Specifically, we perform the forward diffusion process of frame $n$ based on the latent features of frame $n-1$ as 
\begin{equation}
    q(\mathbf{z}_{1:T}^n | \mathbf{z}_0^n) = \prod_{t=1}^T q(\mathbf{z}_t^n | \mathbf{z}_{t-1}^n),~
    q(\mathbf{z}_t^n|\mathbf{z}_{t-1}^n) = \mathcal{N}(\mathbf{z}_t^n;\sqrt{1-\beta_t}\mathbf{z}_{t-1}^n,\beta_t \varphi(\mathbf{z}^{n-1}_0)), ~n=1,...,N
    \label{eq:noise}
\end{equation}
where $\mathbf{z}_t^n$ represents the noisy latent features at frame $n$ at time-step $t$, and $\mathbf{z}^{n-1}=\mathbf{z}_0^{n-1}$ represents the denoised latent features at frame $n-1$ generated by denoising UNet. The function $\varphi$ aims to normalize the latent features $\mathbf{z}^{n-1}$ as the noise, where we adopt layer normalization operation.  The reverse process at frame $n$ can be written as 
\begin{equation}
    p_\theta(\mathbf{z}_{0:T}^n) = p(\mathbf{z}_T^n)\prod_{t=1}^T p_\theta(\mathbf{z}_{t-1}^n|\mathbf{z}_t^n),~
    p_\theta(\mathbf{z}_{t-1}^n|\mathbf{z}_t^n) = \mathcal{N}(\mathbf{z}_{t-1}^n; \boldsymbol{\mu}_\theta(\mathbf{z}_t^n, t), \boldsymbol{\Sigma}_\theta(\mathbf{z}_t^n, t)), n=1,...,N
\end{equation}
where $p(\mathbf{z}_T^n)=p(\mathbf{z}_0^{n-1})$ is the Gaussian noise for generator or latent features of reference frame  for simulator when $n=1$, and represents the denoised latent features $\mathbf{z}^{n-1}$ of generated image at frame $n-1$ when $n=2,...,N$.

\input{figs/3_streaming}
\subsection{Streaming Data Sampler}
As mentioned above, our proposed \modelname requires the denoised latent features  $\mathbf{z}^{n-1}$ at  frame $n-1$ as noise prior to generate the image at frame $n$. To ensure the consistency between training and inference, we need to calculate the denoised latent features $\mathbf{z}^{n-1}$ at frame $n-1$  when performing  image generation at frame $n$ during training. However, it is time-consuming and unnecessary to generate the denoised latent features $\mathbf{z}^{n-1}$ at frame $n-1$ from scratch (\textit{i.e.,} frame 1). To address this issue, we introduce a streaming data sampler that  samples video clip orderly at continuous iterations, which can  improve the efficiency of training. 

Fig. \ref{fig:streaming} presents the pipeline of our streaming data sampler module.
Given a video clip having  $N$ frames for each clip, we orderly sample the frame one by one  at $N$  continuous iterations. At each iteration, we employ diffusion model to generate denoised latent features at  current frame, and save the denoised latent features  at current frame in the cache. The saved denoised latent features at current iteration will be used as the noise prior  at  next iteration. In this streaming way, we just generate the synthetic image of each frame in video clip only once, resulting in more efficient training.

\subsection{Training and Inference Strategy}
\textbf{Layout and text control.} It is crucial to freely manipulate the agents within the scenes when performing image generation. Following existing works~\citep{wen2023panacea,gao2023magicdrive}, we employ BEV  layout and text prompt to  precisely control the scene composition, which includes 10 common object categories and 3 different types of map elements. 
For  BEV layout, we firstly convert them into camera view perspective and extract the control elements as object bounding boxes, object depth maps, road maps, and camera pose embeddings. Afterwards, we integrate them into denoising UNet using the ControlNet~\citep{zhang2023adding} framework. The text prompts are fed to denoising UNet via pre-trained CLIP \citep{Radford_2021_CLIP}.

\textbf{Training.} Inspired by Panacea \citep{wen2023panacea}, we adopt a two-stage training strategy and employ the same data and processing rules. First, we perform image-level pre-training. Our \modelname can also be seen as an image synthesis model, where we only employ Stable Diffusion architecture to generate the image of one frame from Gaussian noise. Second, we perform video-level fine-tuning on the autonomous driving dataset nuScenes by using our proposed streaming data sampler. To obtain images with a resolution of 512$\times$256 pixels, we randomly resize the original images to a proportion of 0.3 to 0.36, and then perform a random crop. This ensures consistency in processing with the downstream StreamPETR method.

\textbf{Inference.}
Our \modelname can be used as generator and simulator, which respectively generate video sequence from Gaussian noise and reference frame. Specifically, given the reference frame or Gaussian noise, we first employ VAE encoder to generate the latent features. 
Subsequently, we feed it to the latent variable propagation module as noise prior, and employ denoising UNet and VAE decoder to predict the denoised latent features  and generate the image at frame 1. Then, the  denoised latent features at frame 1 are fed to the latent variable propagation module for image generation at next frame. In this way, our \modelname is able to generate a video sequence of arbitrary lengths.