\section{Related Work}
In this section, we review existing publicly available datasets for structured web data extraction, along with modern solutions for this task, with a particular focus on BERT-like transformer models.

\subsection{Datasets}

There are several publicly available datasets, which provide a rich source of data for researchers engaged in the task of extracting website attributes. Different approaches have unique dataset requirements that are challenging to implement within a single one: diversity of websites, the number of pages for each, storage formats and attribute sets.

One of the most prevalent datasets in the task is SWDE ____ (2011), which is extensively utilized for evaluating the quality of structured data extraction. It includes 8 subject domains (verticals), each consisting of 10 sites, ranging from 4,405 to 20,000 pages per vertical. The total number of pages in the dataset is 124,291. Each vertical has an average of 4 attributes. SWDE includes HTML files of the pages and attribute data stored separately and not tied to any specific nodes. All websites are in English.

Klarna Product Page Dataset ____ (2021) consists of web pages in MHTML format, their screenshots and snapshots\footnote{https://github.com/klarna-incubator/webtraversallibrary}. It includes 8,175 sites with 51,701 e-commerce product pages in total. The sites of 8 languages are presented: DE, US, GB, FI, AT, SE, NO, NL. Each page contains 5 labeled attributes: buy button, cart button, product price, name and image.

The CoVA dataset ____ (2022) is designed for approaches based on visual page analysis instead of HTML code. It includes web page images and node characteristics: node boundaries, the number of words in the node text, and the presence of special characters. The CoVA dataset comprises 408 e-commerce websites in various languages, totaling 7,740 pages, and includes the following attributes: product price, title, and image.

Researchers utilize datasets with limited page numbers yet diverse websites to assess information extraction method quality. For instance, the Zyte Article Extraction Benchmark\footnote{https://github.com/scrapinghub/article-extraction-benchmark} (2020) contains 181 HTML files from unique websites, their url, screenshot, and article text. The quality of article text extraction is evaluated. The Zyte Product Extraction Benchmark\footnote{https://github.com/scrapinghub/product-extraction-benchmark} (2021) contains 140 products pages from different websites with labeled price, stock-keeping unit (SKU), availability, in-stock status, and out-of-stock status. These benchmarks are used to evaluate both open-source libraries and commercial services, such as Diffbot and Zyte Automatic Extraction.

The article ____ (2022) describes the creation of a dataset of Russian-language news web pages. It consists of 722 news pages from 112 websites. The pages are annotated with 9 attributes: title, subtitle, publication and modification dates, text, tags, category, author, source. Text, XPath of start and end HTML nodes, start and end text offsets within these nodes, and global offsets in HTML code are provided for each attribute.

Available datasets statistics are summarized in table \ref{table:datasets}.

\begin{table}[ht!]
    \caption{Datasets for information extraction from web pages}
    \begin{center}
    \input{tables/datasets.tex}
    \end{center}
    \label{table:datasets}
\end{table}

\subsection{Extraction Methods}
Automated methods allow to extract attributes from any website within a specific domain without having to manually create a data collection algorithm for each site. These methods can be broadly classified into two primary categories: heuristic tools and approaches based on neural network models. We will consider BERT-based transformer models, which are among the most promising approaches in this area.

MarkupLM ____ (2022) is a pre-trained model designed specifically for document understanding tasks that utilize markup languages, such as HTML, where text and markup information are jointly pre-trained. Text is encoded by RoBERTa ____ model. XPath expressions of HTML nodes form the relationship between document elements. MarkupLM model was pre-trained on 24M English web pages from CommonCrawl\footnote{https://commoncrawl.org/} dataset with three strategies: Masked Markup Language Modeling, Node Relation Prediction and Title Page Matching. The pre-trained MarkupLM model has been made available for fine-tuning by the developers, with a focus on two specific downstream tasks: information extraction and reading comprehension. The information extraction task is formulated as token classification into $n+1$ classes ($n$ is the number of extracted attributes, additional category is \textit{None}). 

DOM-LM ____ (2022) is also RoBERTa-based pre-trained model, but unlike MarkupLM, DOM-LM considers various structural features of nodes instead of relying on XPath expressions. Authors propose the DOM Tree Processor algorithm for splitting DOM tree into subtrees that preserve local context. Concatenation of its HTML tag, attribute markups and textual content, which is called node representation, is encoded by RoBERTa. Node depth and index, parent node index and some other characteristics are used as tree position features. DOM-LM was pre-trained on over 120,000 English web pages from SWDE using an adapted Masked Language Modeling strategy.

Structor ____ (2023) is based on MarkupLM and incorporates site-level information and attribute patterns by regular expressions for each DOM node. The authors retrive a node in the same position from another DOM tree on the same website and then incorporate it into the input sequence, approximating character patterns of DOM nodes using regular expressions and integrating them into the neural networks logits.

WebLM ____ (2024) is a multimodal pre-trained network designed to address the limitations of solely modeling text and structure modalities of HTML. It integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents, significantly outperforming previous state-of-the-art models on SWDE dataset.

\begin{table}[ht!]
\caption{Extraction performance of existing models on SWDE (F1-score)}
\label{table:existing_models_compare}
\begin{center}
\input{tables/existing_models_compare.tex}
\end{center}
\end{table}

Table \ref{table:existing_models_compare} shows comparative testing results of some models on SWDE with metrics from related papers. For each SWDE vertical, the models were fine-tuned on $k=2$ and $k=5$ random sites and then evaluated on the remaining $10 - k$ sites. Ten training samples were constructed by cyclically shifting the list of sites.