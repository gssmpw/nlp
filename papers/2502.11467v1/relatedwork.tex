\section{Related Works}
As this research concerns universal approximation, we discuss several related works on this topic.

\subsubsection{Approximation Theory of Neural Networks}
The universal approximation theorem was established by works such as \cite{Cyb89} and \cite{Hor91}. These demonstrate that single-layer neural networks with a general activation function can approximate any continuous function on compact domains to an arbitrary degree of precision. However, they focus solely on the existence of neural networks that approximate continuous functions and do not specify their exact architecture.

In recent years, the empirical success of deep neural networks in tasks such as image recognition and object detection has sparked significant interest in their representational abilities. \cite{Yar17} showed that deep neural networks can approximate smooth functions with fewer parameters than shallow ones. Further developments include the results of \cite{Lu17}, which showed that ReLU FNNs with a fixed width and arbitrary depth can approximate any Lebesgue-integrable function to an arbitrary degree of precision in the sense of \(L^1\). \cite{Lu21} proved that \(C^s\)-functions can be uniformly approximated with an error that decreases at a polynomial rate with respect to the number of layers and width. In addition, \cite{Lu21} showed that the polynomial order of the uniform approximation error is optimal, except for a logarithmic factor.

As a more applied approximation theory, \cite{schmidt2017nonparametric} clarified the approximation performance of functions with composite structures and demonstrated the suitability of neural networks. \cite{petersen2018optimal} and \cite{imaizumi2018deep,imaizumi2022advantage} analyzed the approximation rate of neural networks for non-differentiable functions and showed that neural networks with more layers can achieve better approximation rates than conventional methods.
\cite{nakada2020adaptive} and \cite{chen2019efficient} demonstrated the approximation performance of neural networks adapted to manifold structures, showing that the order of approximation error can be fully described in terms of the manifold dimension. \cite{suzuki2018adaptivity} and \cite{hayakawa2020minimax} elucidated the approximation performance in the general function space such as the Besov space, demonstrating that deep neural networks can achieve optimal approximation rate even for functions that conventional methods fail to approximate optimally.

\subsubsection{Approximation Theory of Symmetric Neural Networks}
Discussions on the symmetry of neural networks have also advanced. In tasks such as image recognition, it is often desirable for the network output to be invariant to transformations such as parallel shifts. Neural networks that are inherently symmetric can be advantageous for this reason. Additionally, imposing symmetry can reduce the number of parameters, which is particularly valuable, especially given that recent models have a significant number of parameters.  
\cite{Zah17} considered neural networks defined on sets. Since sets do not take the order of their elements into account, this can be regarded as a type of symmetric neural network in the paper. Research on symmetric neural networks has also progressed. \cite{Yar18} showed that any permutation invariant function \( f: \mathbb{R}^{d \times n} \to \mathbb{R} \) with \( d, n \in \mathbb{N} \) can be uniformly approximated to arbitrary precision on any compact set using a two-layer neural network that is permutation invariant with respect to its input columns. Additionally, \cite{Mar19} extended these results to the general case, demonstrating that functions invariant under specific permutations are universal approximators. Furthermore, \cite{San19} proved the permutation equivariant case and showed that imposing symmetry on ReLU FNNs reduces the number of parameters compared to the case without symmetry. These approaches provide an effective framework for approximating symmetric functions by leveraging inherent symmetries.

\subsubsection{Universal Approximation of Transformers}
The universal approximation theorem for Transformers, proved by \cite{Yun19}, states that any continuous permutation equivariant function on $[0,1]^{d \times n}$ can be approximated to an arbitrary precision by Transformers. Additionally, it demonstrates that if positional encoding (a method for embedding positional information into input data) is employed, the same result holds even when the target function is not permutation equivariant. \cite{Tak23} showed that specific shift equivariant functions (i.e., functions equivariant to column shifts) can be approximated by a one-layer Transformer with positional encoding. Later, \cite{Kaj24} showed that by directly using the softmax function—in contrast to \cite{Yun19}, which used the hardmax function—a Transformer with a single-head attention layer serves as a universal approximator for permutation equivariant continuous functions. Moreover, several other universal approximation theorems have been established, considering different cases. \cite{Zah20} demonstrated that Transformers with sparse attention layers are universal approximators while reducing computational complexity in the attention layers. \cite{Yun20} investigated a more general case of universal approximation with sparse attention layers. \cite{Kra22} examined universal approximation under constraints, where the outputs of both the target function and the approximating Transformer lie within a specific convex set.

\subsubsection{Approximation Efficiency of Neural Networks and Transformers}

There are studies related to the expressivity of neural networks and Transformers beyond universal approximation.
Let $d$ and $n$ be the input dimension and sequence length of a Transformer (i.e., the number of input tokens).
\cite{Bho20} proved that certain matrices cannot be expressed as the output of the softmax function in the attention layer of Transformers when $d < n$.
\cite{Lik21} demonstrated that the number of columns required to approximate sparse matrices is significantly lower than the total number of columns.

Another topic related to efficiency is memorization capacity, which focuses on fitting $N$ input-output pairs.
\cite{Par21} showed that ReLU FNNs with $\tilde{O}(N^{2/3})$ parameters can memorize $N$ pairs, where $\tilde{O}(\cdot)$ is Landau's Big-O notation which omits constants and logarithmic factors.
\cite{Var22} improved the rate to $\tilde{O} (\sqrt{N})$, which is optimal when ignoring logarithmic factors, according to \cite{Gol93}.
The case for Transformers was shown in \cite{Kim23}, showing that Transformers with $\tilde{O} (d + n + \sqrt{dN})$ parameters can memorize $N$ input-output mappings, where the inputs belong to $\mathbb{R}^{d\times n}$.