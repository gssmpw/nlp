\section{RELATED WORK}
\subsection{EHR Retrieval Benchmarks}
Typically, a retrieval benchmark has three components: the query set, the corpus, and the relevance judgments.
We review existing EHR retrieval benchmarks from these three perspectives.

\subsubsection{Query set}
According to the downstream task of interest, there are various types of query sets used in EHR retrieval, among which the most popular one is patient cohort criteria **[1]**. For example, TREC released a total of $85$ queries in 2011 and 2012 Medical Record tracks **[2];**  Li et al. employed $56$ real criteria from the Mayo Clinic and Oregon Health \& Science University (OHSU);  Chen et al. curated $113$ queries focusing on $6$ oncology use cases.
Such queries can be quite complex, potentially including multiple medical entities and complicated logical conditions, such as "Patients taking atypical antipsychotics without a diagnosis schizophrenia or bipolar depression".
Though faithfully reflecting the patient cohort selection scenario, these queries lack generalizability in downstream applications of EHR retrieval, and are actually often decomposed to single entities for better retrieval performance **[3]**.

Recently, using medical entities as queries has gained more and more research interest due to their versatility and their alignment with physicians' practices **[4]**.
Zhang et al. utilized $8$ cardiovascular disease terms, such as "hypertension" and "palpitation", as queries;
Huang et al. incorporated $20$ stroke-related concepts including diseases, symptoms, medications, and procedures;  Liang et al. extracted $26$ disease mentions from radiology reports as the query set.
However, all existing works used a quite small number of queries, with limited diversity, often focusing on specific information types or even specific diseases.
The absence of a large-scale and diverse query set may hinder a comprehensive assessment of the retrieval performance.

\subsubsection{EHR corpus}
Almost all existing evaluations on EHR retrieval use proprietary corpus **[5]**.
To the best of our knowledge, Wang et al. are the only ones attempting to utilize publicly available EHR corpus, MIMIC-III, to establish an EHR retrieval benchmark.
However, the annotated dataset is not released, and a total of only $50$ patients are incorporated in the evaluation, potentially undermining the robustness of the evaluation.
Besides, previous benchmarks generally adopt either Single-Patient **[6]** or Multiple-Patient setting **[7]**.
None has assessed the model performance under different retrieval settings.

\subsubsection{Relevance judgment}
Traditionally, relevance judgment can be obtained solely from human experts **[8]**, which is prohibitive to scale.
In terms of positive labels, TREC annotated a total of $5,895$ relevant pairs in two years, and Chen et al. included $5,815$ positive annotations in their benchmark.
Recently, researchers begin to explore automatic annotation methods to overcome the limitations of dataset scale.
Chen et al. integrates Hypercube **[9]**, a deterministic reasoning engine based on ontology, in the annotation pipeline and manages to achieve a Cohen's Kappa efficient of $1$ with medical experts.
However, the annotation process still relies on manually reading every patient record and extracting clinical facts regarding the queries, which is not generalizable and scalable.
Liang et al. automatically annotated three specific types of mentions (diagnosis, surgeries, and antibiotics) using regular expressions and the UMLS knowledge graph.
Despite being totally untethered from intensive human labor, the annotation quality is strictly constrained by exact match, which is known to have a low recall **[10]**.
This limitation may particularly underestimate more advanced methods such as dense retrieval **[11]**.
In addition, none of existing benchmarks has incorporated detailed assessments on semantic match.


\subsection{EHR Retrieval Applications}
A recent survey **[12]** identifies the primary applications of EHR retrieval as patient chart review (36\%), patient cohort selection (29\%), and disease prevalence prediction (21\%).
Additionally, EHR retrieval is often regarded as a preliminary step for EHR QA.

\subsubsection{Patient chart review}
Patient chart review refers to the process that clinicians go through a patient's notes to find specific information **[13]**.
For example, TREC defines three chart review tasks as retrieving all information related to acute myocardial infarction, Crohnâ€™s disease, and diabetes from the patient notes.
This process can be quite time-consuming due to the vast volume of even one patient's notes.
Therefore, the retrieval step is necessary.
Traditional patient chart review systems, such as EMERSE **[14]**, relies on exact match methods.
Recently, word embeddings have been applied to enhance the retrieval by providing query recommendation and query expansion **[15]**.
Notably, Wang et al. proposed a medical term embedding model trained on real clinical notes, and showed significant improvement over general domain embeddings for retrieval performance.

\subsubsection{Patient cohort selection}
In patient cohort selection, physicians or researchers aims to select a group of patients satisfying certain criteria for clinical researches or risk identification **[16]**.
This task can go beyond the domain of text retrieval since some criteria are related to structured patient characteristics such age and gender. 
For example, Cohort Retrieval Enhanced by Analysis of Text from Electronic Health Records (CREATE) is a system that performs cohort selection on both structured and unstructured data using the OMOP Common Data Model and Elasticsearch **[17]**.
Still, the retrieval process relies largely on fixed vocabulary and Named Entitiy Recognition (NER) tools such as cTAKES **[18]**.
Facilitated by the data provided by TREC Medical Record tracks, the development of this area has been pushed relatively farther.
More advanced methods including Siamese network **[19]** and Transformer-based language model **[20]** has been applied.


\subsubsection{Disease prevalence prediction}
EHR retrieval can be also applied to predict the prevalence of certain conditions in a population.
For example, Zhang et al. managed to increase the accuracy of identifying veterans with suicide attempts via searching the medical record database.
Similarly, Chen et al. leveraged EHR retrieval to identify 41 additional cases of drug-induced anaphylaxis besides the 25 cases already identified.

\subsubsection{EHR QA}
EHR QA has been an indispensable component in clinical practice, and EHR retrieval has great potential of significantly improving the efficiency and effectiveness of EHR QA systems **[21]**.
Chen et al. formally defined the task of EHR retrieval in the context of EHR QA, and evaluated the impacts of various retrieval methods on the performance of downstream QA task using emrQA dataset **[22]**.
They also investigated the effects of different chunking strategies and chunking lengths.

\subsection{Dense Retrieval}
With the rapid development of Pre-trained Language Models (PLMs), dense retrieval has become the predominant retrieval method **[23]**.
It leverages dense vector representations of queries and documents generated by PLMs to perform efficient and effective similarity match in high-dimensional embedding spaces **[24]**.
Dense retrievers have been shown to acquire zero-shot abilities through contrastive learning on large-scale paired data **[25]**, and are thus widely adopted in both academic and industrial scenarios.
One notable example is the \texttt{bge} series models **[26]**, which leverage vast amount of unsupervised pair data mined from web corpus, and are trained with a three-stage pipeline: RetroMAE pre-training **[27]**, unsupervised contrastive learning, and fine-tuning on manually annotated data.
Among them, \texttt{bge-base-en-v1.5}, a BERT-based **[28]** model with $110$M parameters, demonstrates remarkable capacities on MTEB **[29]**, the most authoritative embedding benchmark.

Recently, LLMs present unprecedentedly capacities on various language generation tasks, and researchers start to explore their potential as an embedding model by taking the last token embedding for representation **[30]**.
Generally, these embedding models are initialized from powerful LLMs of various sizes (mostly $1.5\sim 7$B), and are equipped with bidirectional attention and instruct-tuning **[31]**.
The training pipeline usually involves contrastive learning with large-scale unsupervised data, high-quality supervised data, and LLM synthetic data **[32]**.
Some well-known examples include the \texttt{gte-Qwen2} series **[33]** initialized from \texttt{Qwen2} **[34]**, and \texttt{NV-Embed-v2} initialized from \texttt{Mistral-7B} **[35]**.
The latter is currently the best performing open-source embedding model on MTEB.

In biomedical domain, the development of generalizable dense retrievers has been limited by the lack of high-quality paired data **[36]**.
Prior to the advent of LLMs, Wang et al. introduced MedCPT, which was trained on PubMed user logs and thus specialized in biomedical retrieval.
At the time, MedCPT significantly outperformed general domain encoders in a wide range of biomedical retrieval tasks.
Recently, leveraging powerful LLMs like GPT-3.5 and GPT-4 for data synthesizing, Zhang et al. introduced BMRetriever, a series of LLM-based embedding model ($410$M, $2$B, and $7$B), further improving the performance on these tasks.
However, the effectiveness of these models on the task of EHR retrieval remains unclear.