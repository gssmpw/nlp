\section{RELATED WORK}
\subsection{EHR Retrieval Benchmarks}
Typically, a retrieval benchmark has three components: the query set, the corpus, and the relevance judgments.
We review existing EHR retrieval benchmarks from these three perspectives.

\subsubsection{Query set}
According to the downstream task of interest, there are various types of query sets used in EHR retrieval, among which the most popular one is patient cohort criteria ____.
For example, TREC released a total of $85$ queries in 2011 and 2012 Medical Record tracks ____; 
____ employed $56$ real criteria from the Mayo Clinic and Oregon Health \& Science University (OHSU); 
____ curated $113$ queries focusing on $6$ oncology use cases.
Such queries can be quite complex, potentially including multiple medical entities and complicated logical conditions, such as "Patients taking atypical antipsychotics without a diagnosis schizophrenia or bipolar depression".
Though faithfully reflecting the patient cohort selection scenario, these queries lack generalizability in downstream applications of EHR retrieval, and are actually often decomposed to single entities for better retrieval performance ____.

Recently, using medical entities as queries has gained more and more research interest due to their versatility and their alignment with physicians' practices ____.
____ utilized $8$ cardiovascular disease terms, such as "hypertension" and "palpitation", as queries;
____ incorporated $20$ stroke-related concepts including diseases, symptoms, medications, and procedures; ____ extracted $26$ disease mentions from radiology reports as the query set. 
However, all existing works used a quite small number of queries, with limited diversity, often focusing on specific information types or even specific diseases. 
The absence of a large-scale and diverse query set may hinder a comprehensive assessment of the retrieval performance.

\subsubsection{EHR corpus}
Almost all existing evaluations on EHR retrieval use proprietary corpus ____.
To the best of our knowledge, ____ are the only ones attempting to utilize publicly available EHR corpus, MIMIC-III, to establish an EHR retrieval benchmark.
However, the annotated dataset is not released, and a total of only $50$ patients are incorporated in the evaluation, potentially undermining the robustness of the evaluation.
Besides, previous benchmarks generally adopt either Single-Patient ____ or Multiple-Patient setting ____.
None has assessed the model performance under different retrieval settings.

\subsubsection{Relevance judgment}
Traditionally, relevance judgment can be obtained solely from human experts ____, which is prohibitive to scale.
In terms of positive labels, TREC annotated a total of $5,895$ relevant pairs in two years, and ____ included $5,815$ positive annotations in their benchmark.
Recently, researchers begin to explore automatic annotation methods to overcome the limitations of dataset scale.
____ integrates Hypercube ____, a deterministic reasoning engine based on ontology, in the annotation pipeline and manages to achieve a Cohen's Kappa efficient of $1$ with medical experts.
However, the annotation process still relies on manually reading every patient record and extracting clinical facts regarding the queries, which is not generalizable and scalable.
____ automatically annotated three specific types of mentions (diagnosis, surgeries, and antibiotics) using regular expressions and the UMLS knowledge graph.
Despite being totally untethered from intensive human labor, the annotation quality is strictly constrained by exact match, which is known to have a low recall ____.
This limitation may particularly underestimate more advanced methods such as dense retrieval ____.
In addition, none of existing benchmarks has incorporated detailed assessments on semantic match.


\subsection{EHR Retrieval Applications}
A recent survey ____ identifies the primary applications of EHR retrieval as patient chart review (36\%), patient cohort selection (29\%), and disease prevalence prediction (21\%). 
Additionally, EHR retrieval is often regarded as a preliminary step for EHR QA.

\subsubsection{Patient chart review}
Patient chart review refers to the process that clinicians go through a patient's notes to find specific information ____.
For example, ____ defines three chart review tasks as retrieving all information related to acute myocardial infarction, Crohnâ€™s disease, and diabetes from the patient notes.
This process can be quite time-consuming due to the vast volume of even one patient's notes.
Therefore, the retrieval step is necessary.
Traditional patient chart review systems, such as EMERSE ____, relies on exact match methods.
Recently, word embeddings have been applied to enhance the retrieval by providing query recommendation and query expansion ____.
Notably, ____ proposed a medical term embedding model trained on real clinical notes, and showed significant improvement over general domain embeddings for retrieval performance.

\subsubsection{Patient cohort selection}
In patient cohort selection, physicians or researchers aims to select a group of patients satisfying certain criteria for clinical researches or risk identification ____.
This task can go beyond the domain of text retrieval since some criteria are related to structured patient characteristics such age and gender. 
For example, Cohort Retrieval Enhanced by Analysis of Text from Electronic Health Records (CREATE) is a system that performs cohort selection on both structured and unstructured data using the OMOP Common Data Model and Elasticsearch ____. 
Still, the retrieval process relies largely on fixed vocabulary and Named Entitiy Recognition (NER) tools such as cTAKES ____.
Facilitated by the data provided by TREC Medical Record tracks, the development of this area has been pushed relatively farther.
More advanced methods including Siamese network ____ and Transformer-based language model ____ has been applied.


\subsubsection{Disease prevalence prediction}
EHR retrieval can be also applied to predict the prevalence of certain conditions in a population.
For example, ____ managed to increase the accuracy of identifying veterans with suicide attempts via searching the medical record database. 
Similarly, ____ leveraged EHR retrieval to identify 41 additional cases of drug-induced anaphylaxis besides the 25 cases already identified. 

\subsubsection{EHR QA}
EHR QA has been an indispensable component in clinical practice, and EHR retrieval has great potential of significantly improving the efficiency and effectiveness of EHR QA systems ____.
____ formally defined the task of EHR retrieval in the context of EHR QA, and evaluated the impacts of various retrieval methods on the performance of downstream QA task using emrQA dataset ____.
They also investigated the effects of different chunking strategies and chunking lengths.

\subsection{Dense Retrieval}
With the rapid development of Pre-trained Language Models (PLMs), dense retrieval has become the predominant retrieval method ____.
It leverages dense vector representations of queries and documents generated by PLMs to perform efficient and effective similarity match in high-dimensional embedding spaces ____.
Dense retrievers have been shown to acquire zero-shot abilities through contrastive learning on large-scale paired data ____, and are thus widely adopted in both academic and industrial scenarios.
One notable example is the \texttt{bge} series models ____, which leverage vast amount of unsupervised pair data mined from web corpus, and are trained with a three-stage pipeline: RetroMAE pre-training ____, unsupervised contrastive learning, and fine-tuning on manually annotated data.
Among them, \texttt{bge-base-en-v1.5}, a BERT-based ____ model with $110$M parameters, demonstrates remarkable capacities on MTEB ____, the most authoritative embedding benchmark. 

Recently, LLMs present unprecedentedly capacities on various language generation tasks, and researchers start to explore their potential as an embedding model by taking the last token embedding for representation ____.
Generally, these embedding models are initialized from powerful LLMs of various sizes (mostly $1.5\sim 7$B), and are equipped with bidirectional attention and instruct-tuning ____.
The training pipeline usually involves contrastive learning with large-scale unsupervised data, high-quality supervised data, and LLM synthetic data ____.
Some well-known examples include the \texttt{gte-Qwen2} series ____ initialized from \texttt{Qwen2} ____, and \texttt{NV-Embed-v2} initialized from \texttt{Mistral-7B} ____.
The latter is currently the best performing open-source embedding model on MTEB.

In biomedical domain, the development of generalizable dense retrievers has been limited by the lack of high-quality paired data ____.
Prior to the advent of LLMs, ____ introduced MedCPT, which was trained on PubMed user logs and thus specialized in biomedical retrieval.
At the time, MedCPT significantly outperformed general domain encoders in a wide range of biomedical retrieval tasks.
Recently, leveraging powerful LLMs like GPT-3.5 and GPT-4 for data synthesizing, ____ introduced BMRetriever, a series of LLM-based embedding model ($410$M, $2$B, and $7$B), further improving the performance on these tasks.
However, the effectiveness of these models on the task of EHR retrieval remains unclear.