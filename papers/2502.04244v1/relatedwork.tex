\section{RELATED WORK}
\label{sec:related-works}

% maneuver detection
\subsection{Maneuver Detection}
%  3D CNNs
% slow and heavy for running in device (add refs)
Vehicle maneuver detection is a complex task involving the analysis of temporal sequences -- usually video and sometimes other sensor data (GPS, IMU, Lidar) -- of importance for road scene understanding.
% 1 or 2 methods with 3D conv + 1 or 2 methods with RNNs + CNNs

In \cite{singh2023road}, the authors propose a convolutional neural network (CNN) named \emph{3D-RetinaNet} to classify road agent actions from videos captured by a vehicle-mounted camera, building on the success of video recognition networks for human action recognition tasks like SlowFast \cite{Feichtenhofer2019slowfast}. This model receives a sequence of video frames as input and builds a feature pyramid using either a 3D or a 2D CNN as backbone; these features are then fed to two subnetworks, in order to locate other road agents and classify their actions. An additional classification head outputs ego-vehicle action classification. This architecture is fairly complex and targets a more general scenario (not only ego-vehicle maneuvers, but generic road agent actions) than ours. In addition, the authors do not report good results on ego-vehicle overtaking and moving to the sides.

Recurrent neural networks (RNNs) in combination with 2D CNNs have also been explored as a way to combine spatio-temporal information for the maneuver detection task. In \cite{peng2018driving}, features extracted from video frames through a \mbox{VGG-19} CNN are combined with handcrafted features from GPS and IMU data and fed to a LSTM-based RNN to perform ego-vehicle maneuver classification. The targeted maneuvers are left and right turns, lane changes and straight driving. While the authors report high accuracy results, this model is computationally heavy as it requires to run a deep feature extractor on each video frame, making it unsuitable for real-time execution.

Zekany \etal \cite{zekany2019classifying} explicitly target ego-vehicle maneuver classification. They propose an approach based on deep learning visual odometry to estimate ego-vehicle localization and use dynamic time warping to match the obtained trajectory against a set of reference maneuvers, namely left, right, U and K turns, stops, reverse and straight driving. This method is thus only partially applicable to our target maneuvers; moreover, the system is not designed for real-time operations and works offline, as it peeks into future frames to generate its output. Further work from the authors \cite{zekany2022finding} expands the methodology to additional maneuvers such as lane changes, decelerations, highway merges and exits by introducing individual classifiers and techniques to combine them. While this produces more general results, it also makes the system more complex, retaining the limitations for its real time operation.

While targeting tasks similar to ours, these methods rely on processing each video frame, making them computationally heavy and mostly suited for offline execution. In this work we focus on methods that use a condensed video representation and are suitable for deployment on resource constrained devices.

\subsection{Motion Profile}

The idea behind motion profiles is introduced for the first time by \kilicarslan \etal 2014a \cite{kilicarslan2014visualizing},
where the authors propose to look at a single strip of pixels for each frame at an intermediate distance between the horizon line and the vehicle hood for each frame, 
and stack them into a single image that they call road profile.
Although such representation maintains several interesting properties of the road, like pedestrian crossings and lanes, it produces several artifacts corresponding to the objects in the road scene, \eg vehicles.
For this reason, in the same paper the authors propose to look at a set of pixels under the horizon rather than a single line, to vertically average them and to form a stacked representation that they name motion profile.
They claim that in this second representation, a wide set of events relative to the motion of the entities in the scene is recognizable, 
like ego-vehicle moving, stopping, turning or changing lanes, 
as well as other vehicles passing and being passed, the presence of the leading vehicle and pedestrians; at the same time, the sharp artifacts that characterize the road profile are blurred and blend with the background.
In \kilicarslan \etal 2014b \cite{kilicarslan2014detecting} the authors address pedestrian detection and propose to consider a belt of pixels slightly below the horizon, to detect artifacts similar to chains due to the motion of the human legs; they also propose a method based on HOG to identify the nodes of such chain-like artifacts.

In \kilicarslan \etal 2016, 2017 \cite{kilicarslan2016bridge, kilicarslan2017direct} the authors use the motion profile for Time-to-collision (TTC) estimation without the need for a vehicle detection module. They propose to compute the gradient in the motion profile in the central zone of the profile, to identify the edges relative to the leading vehicle, and use convergence/divergence factors to estimate the TTC.

The first approach that tries to use motion profiles for maneuver detection is Wang \etal \cite{wang2020detecting}. 
In the paper, the authors identify a set of maneuvers and events observable in the driving scene and associate to each of them the ideal artifact that they would generate in the motion profile.
Such set includes cut-in, overtaking, being overtaken, oncoming vehicles, and left or right turns performed by the ego-vehicle.
The authors also propose a way to identify such maneuvers, by computing the gradient of the motion profile, extracting the punctual angular value of the detected traces and then computing a 1D vertical Laplacian filter into specific location of the image, \eg the center or the left or right side.
They propose to use the output of the Laplacian filter to identify the presence of a trace, and the angle value to identify its type (distinguishing for instance between passed, passing and oncoming vehicle).
Another approach for maneuver detection is presented in \kilicarslan \etal 2022 \cite{kilicarslan2022motion}.
The authors propose to consider a vehicle detected into consecutive frames at timestamp $t$ and $t + d_t$ and to compute the motion profile between the two frames, with $d_t$ reasonably small. Then, they build an image by concatenating vertically a patch with the upper part of the vehicle at frame $t$, the motion profile, and the lower part of the vehicle at frame $t + d_t$.
Finally, they use a YOLOv3 detector on the generated patches to identify both the vehicles and the traffic flow, \ie distinguishing between passed, passing and following vehicles.
Despite their approach being interesting, it does not extend to a motion profile of arbitrary length, \eg a few minutes, if not by running the inference every $d_t$ frames, which is not ideal for a lightweight setup.