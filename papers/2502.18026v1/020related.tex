% \subsection{Biological Network Inference}
% \label{subsec:networkinference}

Existing methods for analyzing the structure of biological networks typically represent these networks as graphs, aiming to infer subgraphs or extract relevant interactions, and can generally be categorized into statistical topology-driven and data-driven deep graph learning methods.\\
\textbf{Topology-driven methods} utilize statistical metrics on structural properties of graphs, such as node degrees \cite{RSS,RSS1}, centrality \cite{MDS,MDS1,NACHER201657}, betweenness, or PageRank scores \cite{PMID:21149343,PPR1} to infer which substructures or edges exert a more significant influence on the overall topology, thereby identifying more targeted interactions among genes or proteins.\\
\textbf{Deep graph learning methods} incorporate experimental data during the learning process by embedding data as node representations.
They train graph neural networks (GNNs) with suitable objectives, such as link prediction or graph reconstruction \cite{GNNB2,GNNB1,GNNB3}, and the links that contribute most to these objectives can be considered the targeted interactions.
For instance, the classic GCN algorithm, GraphSAGE \cite{GraphSAGE}, has been validated on protein-protein interaction (PPI) datasets to predict protein functions within networks.
The work of \cite{GCN1} introduced a GCN-based method for predicting protein functions, leveraging sequence features derived from a protein language model alongside structural information.
Chen et al. \cite{GAT1} employed a graph attention network to extract drug and protein AA-Seq features for predicting drug-target interactions.
Moreover, GNN models have been applied to incorporate RNA-Seq data, for tasks like predicting disease states and cell-cell relationships \cite{GNNS1, GNNS2}.
% \textbf{Key Limitations of Previous Works.}
% The topology-driven methods cannot incorporate experimental data to infer biological networks, and they are often computationally intractable for large networks \cite{BG_1}.
% While GNN-based methods generate targeted interactions in a data-driven manner, their objectives do not explicitly focus on inferring networks and are typically task-specific. 
% In contrast, our method focuses on directly explaining graph representations of general networks under specific experimental data. 
% We are unaware of any published results that address a similar question.


% \textbf{Conventional Method.}
% %Topology-Driven Methods
% Random subgraph sampling (RSS) \cite{RSS} represents one of the most fundamental, conventional approaches.
% It randomly selects nodes and edges to create smaller subgraphs from the original network. 
% RSS is a well-used, earlier method of analyzing biological networks due to its computational efficiency and ease of implementation \cite{RSS1,RSS2}.
% However, its random nature often leads to subgraphs that lack biological significance or focus.
% The outstander of more complex methods, including Minimum Dominating Set (MDS) \cite{MDS}.
% The MDS is a more structured method that selects a subset of nodes so that every other node in the network is either in this subset or directly connected to it. 
% It focuses on the most influential nodes, which helps identify key parts that influence the behavior of other elements in the biological networks \cite{MDS1, NACHER201657}. 
% Despite its advantages, solving MDS is computationally challenging for large networks due to its NP-hard nature, often requiring heuristic or approximation algorithms for practical applications.
% The recent approach includes variants of methods analyzing general networks, such as Personalized PageRank (PPR) \cite{PMID:21149343,PPR1} to identify key pathways and protein interactions.
% It represents the advanced, probabilistic methods for subgraph extraction. 
% This algorithm extends the classical PageRank by biasing the random walk toward specific seed nodes, ensuring that the extracted subgraph is closely related to the biological elements of interest. 
% PPR achieves a balance of local and global network information, but its performance depends on careful parameter tuning and seed node selection, which may require domain expertise.

% \subsection{GNN Method}\label{subsec:GNN4bio}
% Deep learning methods, especially GNN-based approaches, have emerged as promising new tools to tackle classical questions in biological network analysis. 
% Unlike task-specific traditional methods, GNNs provide a flexible framework for learning and extracting deep features from biological networks, enabling their application to various downstream tasks such as link prediction, network reconstruction, and classification \cite{GNNB1,GNNB2,GNNB3}.
% For instance, GligorijeviÄ‡ et al. \cite{GCN1} introduced a GCN-based method for predicting protein functions using sequence features derived from a protein language model alongside structural information. 
% Similarly, the classic GCN algorithm, GraphSAGE \cite{GraphSAGE}, has been validated on protein-protein interaction (PPI) datasets to predict the function of proteins within the network.
% Furthermore, attention mechanisms have been integrated into GNNs to enhance their ability to capture tiered network relationships.
% For example, Chen et al. \cite{GAT1} proposed a method employing a graph attention network (GAT) with multi-head self-attention to extract features of drugs and proteins for predicting drug-target interactions.
% Moreover, GNN models have also been applied to incorporate non-graph data, such as sequencing data, for tasks like predicting disease states and cell-cell relationships \cite{GNNS1, GNNS2}.



% \subsection{Explanations for Graph Neural Networks}\label{subsec:GNNexplainer}
% % The growing importance of graph learning has highlighted the critical need for explainability. Broad research efforts have been dedicated to understanding the interpretability of graph structures.
% The growing importance of graph learning has highlighted the critical need for explaining graph structures. 
% % Model-agnostic statistical methods, including graph-theoretical algorithms, provided foundational interpretability through combinatorial optimization and stochastic analysis \cite{10.1093/bioinformatics/bth163, NACHER201657}. Despite their computational efficiency, these approaches fail to account for node features and the dynamics of neural networks, limiting their alignment with GNN behaviors.
% % The advent of gradient-based techniques \cite{saliency, InputXGradient, guidedbackpropagation, deconvolustion} addressed this limitation by leveraging differential signals from trained models. 
% Gradient-based methods \cite{saliency, InputXGradient, guidedbackpropagation, deconvolustion} use gradient propagation through network layers or feature attribution via perturbation analysis to deliver data-driven explanations that reflect both input features and model parameters. However, their emphasis on local feature importance often neglects the relational inductive biases that are central to graph learning.
% To overcome these challenges, GNN-specific explanation methods have been developed, focusing on explicitly modeling the message-passing mechanisms and topological dependencies inherent to GNNs. GNNExplainer \cite{gnnexplainer} optimize for both structural relevance and feature importance through differentiable masks or parametric explanation generators \cite{pgexplainer}, producing explanations that respect graph connectivity and neural transformations. 



