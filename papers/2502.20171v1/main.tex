\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{booktabs}
\usepackage{pdfpages}
\usepackage{pax}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\pgfplotsset{every tick label/.append style={font=\scriptsize}}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{url}

%mathieu{titel capitalisatie?}
\title{Representing Signs as Signs: One-Shot ISLR to Facilitate Functional Sign Language Technologies}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Toon Vandendriessche, Mathieu De Coster, Annelies Lejon \& Joni Dambre \\
IDLab-AIRO -- Ghent University -- imec\\
Ghent, Belgium \\
\texttt{\{firstname.lastname\}@ugent.be} \\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand\joni[1]{\textcolor{red}{[JONI: #1]}}
\newcommand\toon[1]{\textcolor{blue}{[TOON: #1]}}
\newcommand\mathieu[1]{\textcolor{green}{[MATHIEU: #1]}}
\newcommand\annelies[1]{\textcolor{purple}{[ANNELIES: #1]}}


\newcommand{\PFASL}{PF-ASL}%{PoseFormer\textsubscript{ASL}}
\newcommand{\PFVGT}{PF-VGT}%{PoseFormer\textsubscript{VGT}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
% \mathieu{mooi abstract!}
    Isolated Sign Language Recognition (ISLR) is crucial for scalable sign language technology, yet language-specific approaches limit current models. To address this, we propose a one-shot learning approach that generalises across languages and evolving vocabularies. Our method involves pretraining a model to embed signs based on essential features and using a dense vector search for rapid, accurate recognition of unseen signs. We achieve state-of-the-art results, including 50.8\% one-shot MRR on a large dictionary containing 10,235 unique signs from a different language than the training set. Our approach is robust across languages and support sets, offering a scalable, adaptable solution for ISLR. Co-created with the Deaf and Hard of Hearing (DHH) community, this method aligns with real-world needs, and advances scalable sign language recognition.
  % This paper presents a new baseline for a simple, yet highly effective, method for one-shot isolated sign language recognition. The simplicity of the method stems from the fact that the one-shot classifier is a frozen neural network and only requires a trivial initialisation but no weight updates through training. Yet, at the same time, the method is highly effective, as we achieve high recall despite having only one training example per sign. We show that, with a sufficiently large and varied pretraining set, it is possible to classify unseen signs for which only one training example is available. This is possible even if the sign originates from a different sign language than the language used for pretraining. This method enables search functionality for online sign language dictionaries, as well as large-vocabulary sign language recognition for sign languages for which no extensive datasets are available.
  % \keywords{Sign language recognition \and One-shot classification \and Human pose estimation}
\end{abstract}


\section{Introduction}
\label{sec:intro}

Isolated sign language recognition (ISLR) is a crucial first step toward achieving full sign language translation (SLT). Despite notable progress, most advances in ISLR have been confined to specific languages and respective datasets, limiting the range of vocabularies that can be recognised in real-world applications. In addition, with sign languages constantly evolving, this limitation becomes increasingly problematic. More flexible approaches are required -- ones that allow for the expansion of vocabularies and are robust enough to handle large-scale dictionaries for SLT.

To overcome these challenges, it is essential to shift the focus away from predefined vocabularies and instead emphasise the intrinsic features of signs. Once an \textit{effective} representation of a specific sign is established, the surrounding context can be integrated afterwards. An \textit{effective} sign representation requires two key elements: (1) each sign is uniquely identifiable, and (2) only the essential features, i.e. sign phonemes, are captured to ensure reliable discrimination and adaptability across varied signing conditions.
% \mathieu{to ensure clarity and precision: dit is nietszeggend vind ik, zou ik weglaten of vervangen door iets concreters}

Following this idea, we propose leveraging one-shot learning for sign language recognition. One-shot learning is particularly well-suited for sign language recognition since it allows the model to generalise from limited examples, making it possible to recognise new signs without extensive retraining or data collection. By focusing on embedding signs efficiently, this method enables accurate, context- and even language-independent isolated sign recognition.

Our approach consists of two steps: (1) pretraining a model that can reliably embed signs, and (2) using this frozen pretrained model for a dense vector search. In the first step, we train the model on a diverse set of signs to capture their essential features. This creates embeddings representing signs as points in a high-dimensional space. In the second step, a dense vector search matches new, unseen variations of signs to the closest embedding in this space, enabling rapid and accurate recognition without requiring further retraining.

We evaluate both the pretraining and one-shot classification, achieving state-of-the-art results. Notably, we attain a 50.8\% one-shot MRR on an extremely large dictionary from a language entirely different from the pretraining data. Our method also demonstrates strong robustness across multiple languages, even when introducing variations in the set of exemplary signs. These findings underscore that ISLR systems can be generalised across languages, independent of the pretraining data. This enables recognition of vast vocabularies while adapting to the evolving nature of sign languages. This adaptability positions our approach as a progressive solution for enhancing the scalability and effectiveness of sign language technology in diverse contexts.
% \mathieu{die laatste zin vind ik er lichtjes over en ruikt een beetje naar generative AI (sorry als dat niet zo is :p) vind ik, persoonlijk zou ik die weglaten}

The development of this paper was guided by a clear message from the DHH community: sign language technology can be initially suboptimal but should be improvable through active feedback, to provide more usable tools for the DHH community \emph{in the short term}. This goal was emphasised at a workshop on sign language research in AI \citep{bragg2019sign}. Bearing this in mind, the method of this paper was developed in a co-creation strategy, where the needs and desires of the DHH community were assessed frequently. Ultimately, this led to an openly available tool\footnote{\url{https://woordenboek.vlaamsegebarentaal.be/signBuddy}}.

% \textbf{[AUTHORS: In the light of this paper, we integrated a publically available application into an online sign language dictionary. This application allows the user to search for a sign in the dictionary by simply performing it in front of a webcam. We can not provide any more details, since this conflicts with the double-blind review policy of ICLR. This paragraph is a placeholder for a more detailed description after publication.]}
% \mathieu{is dit oke volgens de ICLR guidelines om er zo'n paragraaf in te zetten? Plus het wordt vrij obvious dat het over VGT gaat omdat dat de enige dictionary is die besproken wordt in deze paper} \toon{paragraaf wegdoen, zinnetje toevoegen aan bovenstaande en voetnoot met link.}

\textbf{The contributions of this paper are the following.} 
\textbf{(a)} This paper presents the first robust approach to one-shot sign language recognition, demonstrating its language-independent capabilities and its applicability to larger vocabularies.
\textbf{(b)} We achieve state-of-the-art results in sign language recognition.
\textbf{(c)} Our method demonstrates strong generalisation across multiple languages and proves robust even when variations are introduced in the exemplary signs.
\textbf{(d)} We developed an application using a co-creation strategy in close collaboration with the DHH community, ensuring that sign language research output meets their needs and can be continuously improved through active feedback.
% \textbf{(c)} We introduce an expanded evaluation set to benchmark the performance of our one-shot sign language recognition model. This evaluation set underscores the critical role of the composition and diversity of sign language datasets in achieving accurate and reliable recognition results.


% Isolated sign language recognition (ISLR) relates to the classification of signs, represented as video data.
% It is a crucial first step towards automatic sign language translation from signed to spoken languages.
% Furthermore, ISLR also enables applications for sign language learning (e.g., PopSign \cite{popsign}) and
% search functionality for sign language dictionaries \cite{de2023querying,fink2023sign}.
% Data collection for sign languages is expensive and requires effort by a small group of the population,
% namely Deaf and Hard of Hearing (DHH) people, whom are often requested to participate in research
% organised by hearing researchers. Especially when this research does not lead to useful applications
% in the short term, this can lead to frustration of the DHH with regards to artificial
% intelligence ``solutions'' \citep{de2021good}.

% Recent advances in sign language recognition technology are now leading to applications such as
% PopSign targeted at hearing learners of American Sign Language (ASL). The creation of such applications
% also requires the collection of datasets. In this paper, we propose a method for one-shot sign
% classification which might reduce the burden of data collection, since only one training example per sign
% is required. This method also opens doors for extremely large vocabulary sign language recognition,
% which is required for modelling the full established vocabularies of sign languages. The
% largest datasets for ISLR contain just shy of 3,000 sign categories;
% our method supports an arbitrary number of sign categories (we consider 10,235 categories in this paper).
% %It can distinguish between 10,235 signs with 0.3743 Recall@1 and 0.6704 Recall@5.
% It does not require fine-tuning or retraining to transfer it to another language
% and adding additional signs is trivial.
% It is important to remark that our evaluation set does not cover all 10,235 signs, but only a smaller subset. However, every evaluation example is classified as one of the 10,235 signs.

% The proposed method enables search functionality for sign language dictionaries, which typically have extremely large vocabularies and cannot yet be queried with sign input.

% Our method is based on previous research \citep{de2023querying} in the space of sign language dictionaries, but we make two key modifications which are crucial to achieving high recall. Our first modification is made because signs are the combination of five components or phonemes. These five sign-phonemes include the handshape, movement, place of articulation, hand orientation and non-manual features \citep{stokoe1960sign,battison1978lexical}. Recognising these independent components proficiently is thus essential for identifying a wide variety of signs. 

% The method described by \citet{de2023querying} used the Flemish Sign Language (VGT) corpus \cite{van2015het} to pretrain a sign language
% recognition model that is then used for downstream one-shot classification on the VGT dictionary. While using the same language for pretraining and the downstream task is indeed sensible, other research highlights a particular characteristic of the VGT corpus, which makes it unfit for one-shot classification pretraining: its class distribution follows a Zipfian pattern \cite{de2023towards}. 
% The majority of the examples in the dataset correspond to just three pointing signs, which have a simple and identical handshape. When a model trained on such a dataset, with limited handshape variety, is used to classify previously unseen signs, it is likely that it has never seen the handshapes used in these signs before. Consequently, it will be unable to properly classify them.

% Our paper demonstrates that pretraining on a more varied dataset, containing a wider variety
% of manual parameters, enables higher downstream one-shot performance. This is the case even if the pretraining
% dataset contains only signs that belong to another language. We do this by pretraining on ASL Citizen
% \cite{desai2024asl}, which has multiple examples for 2,731 unique signs and an almost uniform class distribution.

% Our second modification concerns the one-shot strategy. After the pretraining phase, the model is frozen and the one-shot inference takes place, relying on the learnt intermediate representations or embeddings. Previous research \cite{de2023querying} proposed a one-nearest-neighbour search using the embeddings based on Euclidean distance. Although this method can be highly effective, it has a significant drawback: the resulting latent distances lack interpretability. To address this issue, we utilise matching networks \cite{vinyals2016matching}. Matching networks apply a softmax function over the predictions, which allows them to be interpreted as probabilities. These probabilities provide clearer insights into the model's predictions and their associated confidence levels. Consequently, this enhanced interpretability improves downstream decision-making by allowing for more informed and accurate assessments based on the model's outputs.


% Although it is indeed sensible to use the same language for pretraining and the downstream task, other research illustrates a problem for one-shot classification pretraining with the VGT corpus: its class distribution is Zipfian \cite{de2023towards}.



% Our method is based on previous research \cite{de2023querying} in the space of sign language
% dictionaries, but we make several changes which are crucial to achieving high recall. First,
% previous research used the Flemish Sign Language (VGT) corpus \cite{van2015het} to pretrain a sign language
% recognition model; this model was then frozen and used for one-shot classification on previously unseen signs from the VGT dictionary.
% It is indeed sensible to use the same language for pretraining and the downstream task.
% However, other research illustrates a problem that occurs for one-shot classification pretraining with the VGT corpus: its class distribution is Zipfian \cite{de2023towards}.
% The majority of the examples in the dataset correspond to just three pointing signs, which have a simple and identical handshape. The handshape is one of the five components that make up a sign, the other four being
% hand movement, place of articulation, hand orientation, and non-manual features \cite{stokoe1960sign,battison1978lexical}. Being able to recognise a wide variety of handshapes (and movements, places of articulation, hand orientations, and non-manual features) is thus a necessary condition for being able to recognise a wide variety of signs.
% When a model trained on such a dataset, with limited handshape variety, is used to classify previously unseen signs, it is likely
% that it has never before seen the handshapes used in these signs. It will be unable to properly
% classify them.
% Our paper demonstrates that pretraining on a more varied dataset, containing a wider variety
% of manual parameters, enables higher downstream one-shot performance. This is the case even if the pretraining
% dataset contains only signs that belong to another language. We do this by pretraining on ASL Citizen
% \cite{desai2024asl}, which has multiple examples for 2,731 unique signs and an almost uniform class distribution.

% Sign language datasets are video datasets, and models trained on visual data can exacerbate visual bias. We therefore use keypoint-based models as preprocessing: MediaPipe in particular has been trained on people from several nations, of several genders and ethnicities \cite{mediapipemodelcard}. Moreover, when we envision a sign language dictionary search application, we want this application to work anywhere. Robust keypoint estimators enable this and relieve sign language researchers of the task of creating a visually varied dataset.

% \mathieu{positionality statement toevoegen?}
% The development of this paper was guided by a clear message from the DHH community: sign language technology can be initially suboptimal but should be improvable through active feedback, to provide more usable tools for the DHH community \emph{in the short term}. This goal was emphasised at a workshop on sign language research in AI \cite{bragg2019sign}. Bearing this in mind, the method of this paper was developed in a co-creation strategy, where the needs and desires of the DHH community were assessed frequently. 

% \textbf{The contributions of this paper are the following.} 
% \textbf{(a)} This paper presents the first robust approach to one-shot sign language recognition, demonstrating its language-independent capabilities and its applicability to larger vocabularies.
% \textbf{(b)} We apply and extend a proven model, achieving state-of-the-art results in sign language recognition.
% \textbf{(c)} We introduce an expanded evaluation set to benchmark the performance of our one-shot sign language recognition model. This evaluation set underscores the critical role of the composition and diversity of sign language datasets in achieving accurate and reliable recognition results.


\section{Related Work}

\subsection{Isolated Sign Language Recognition}
\label{sec:islr}

ISLR is a classification problem. Traditionally, a system analyses a video depicting an isolated sign and aims to predict the corresponding label or gloss. These input videos can be processed in several ways. Recent advancements \citep{papadimitriou2023sign, chen2022two} have demonstrated a positive impact using pose estimation models, such as MediaPipe Holistic \citep{grishchenko2020mediapipe} (henceforth MediaPipe) and OpenPose \citep{cao2017realtime}. These models transform input videos into sequences of skeletal representations, capturing ``keypoints'' or ``landmarks'' of the human pose in 2D or 3D Cartesian coordinates. By removing all information about a person's appearance, these tools enhance the generalisability to downstream tasks. This transformation allows ISLR models to focus solely on the structural aspect of sign videos: how people move their arms, hands, and face.

% \mathieu{Volgende paragraaf begint alsof het over MediaPipe's voordelen gaat gaan, maar is dan
% terug algemeen. Ik zou de eerste zin ofwel naar boven verplaatsen, ofwel de paragraaf meer over
% deze ene tool laten gaan en niet in het algemeen spreken. Kaggle was ten slotte ook MediaPipe}
MediaPipe has significantly advanced the current state-of-the-art of ISLR, but there is still considerable room for improvement. \citet{moryossef2021evaluating} argued that this tool is not directly applicable to fine-grained tasks like sign language recognition. Although the keypoint estimator is generally accurate, MediaPipe struggles when two body parts interact. Since this interaction is elemental to sign language, crucial information is lost. However, recent Kaggle competitions \citep{asl-signs, chow2023google} based on keypoint estimation using MediaPipe present a different perspective, showing promising results in SLR using keypoint estimation. A key component appears to be the addition of a frame embedding that is not present in the work by Moryossef et al., but present in all top Kaggle competition entries. This frame embedding allows the network to learn the non-linear relationships between keypoints \citep{de2023towards}.

% Overall, MediaPipe provides the optimal balance between accuracy and computational efficiency \cite{de2023towards}, while it also provides the best estimation of the hands. 

% \mathieu{Misschien hier nog vermelden: a key component appears to be the addition frame embedding that is not present in the work by Moryossef et al., but in all top Kaggle competition entries. This frame embedding allows the network to learn
% the non-linear relationships between keypoints [6]. En ook dat MediaPipe beter is dan OpenPose voor handen, bv eerste zin van deze paragraaf naar hier verplaatsen}

% \mathieu{De eerste zin mag concreter. Niet "there are many decisions to be made", maar je mag gewoon zeggen dat de architectuur een grote impact heeft.}
Not only the preprocessing of sign language videos is essential. The architecture of the models also has a great impact. Until 2020, deep learning approaches to ISLR
% \mathieu{Initial vind ik niet ideaal als woord. Misschien "deep learning approaches until 2020"} deep learning approaches 
primarily used variations of Recurrent Neural Networks \citep{koller2016deep, koller2017re, ye2018recognizing}. The common factor of these models is that they are proficient at handling sequential data and dealing with temporal dependencies between different poses. The introduction of transformers \citep{vaswani2017attention} in 2017 initiated a paradigm shift. 
% The first applications of transformers in the domain of ISLR and sign language translation were proposed in 2020: \cite{de2020sign}, respectively \cite{camgoz2020sign}. 
% \mathieu{Misschien hier ook verwijzen naar Sign Language Transformers van Camgoz voor translation, anders zitten we echt wel praktisch enkel mij te citeren en wordt het obvious :p} 
% In the competitions mentioned above, models employing attention mechanisms also performed significantly better.
The combination of keypoints and attention leads to powerful models for ISLR: the top scoring on the Kaggle ASL ISLR competition \citep{asl-signs} method achieved 89.3\% test set accuracy on 250 sign categories using keypoint data.




\subsection{Few-Shot Sign Language Recognition}

% Sign languages are natural languages, and as such, they evolve. This evolution necessitates the development of flexible techniques for ISLR. 
The evolving nature of sign languages motivates the need for flexible ISLR techniques.
The traditional 
% \mathieu{Leuke intro, maar misschien hier kort zeggen wat je bedoelt met traditional (verwijzen naar vorige sectie)} 
sign language classification models described in the previous section lack this flexibility, as they are limited to the glossary provided in the training set and require a large number of examples for every sign. 
% To address this limitation many approaches resort to one- or few-shot learning.\mathieu{De vorige zin "to adress" zou ik weglaten, de volgende is een betere introductie tot dit concept. Nu komt het uit het niets, terwijl de volgende zin de context geeft waarom few of one shot relevant is} 
\citet{fei2006one} argued that ``one can take advantage of knowledge coming from previously learned categories, no matter how different these
categories might be.''  This insight led to the introduction of few-shot learning, where fewer examples per category are required.

There are various gradations to few-shot learning, including zero-shot learning.
%In this approach, the model has never seen an example of the task at hand and completely relies on its prior knowledge.
In zero-shot learning, the model has never seen an example for a given category and relies on its
prior knowledge and some form of description of the category.
Zero-shot learning was first introduced to SLR by \citet{bilge2019zero}. Their method involved matching textual descriptions to video inputs, utilising BERT text embeddings \citep{devlin2018bert} and 3D CNNs alongside bidirectional LSTMs for video processing. More recently, \citet{rastgoo2021zs} approached zero-shot ISLR with a similar technique. This method combined BERT text embeddings with pose estimators and vision transformers, followed by LSTMs for temporal modelling. These zero-shot methods rely on the presence of a textual description of a sign, which is not always available. 

% For languages like VGT a dictionary is often available, a collection of every recognised sign and one corresponding video. 
More often, there are one or multiple example videos available for one sign, which can be used for few-shot learning. If none are available for a given sign, recording one example is easier than accurately describing the sign through text.
% While zero-shot learning offers significant advantages\mathieu{welke advantages? komt niet echt naar boven uit het vorige. Ik zou zeggen dat het uitdagend is, en dat je bv textual descriptions nodig hebt wat je ook niet voor elk gebaar wil doen. Een, of een klein aantal gebaren, verzamelen, kan makkelijker zijn en schaalbaarder. Dus kijken we naar one shot}
 % there are cases where some examples can be beneficial. 
\citet{wang2021cornerstone}, for example, leverage multiple examples (i.e., they perform few-shot learning) of one sign to perform K-means clustering and a custom matching algorithm. For some sign languages, a dictionary is available. In the case of VGT, the dictionary contains exactly one example per unique sign, which is ideal for one-shot learning. \citet{de2023querying}  in essence performed one-shot learning to recognise signs in the VGT dictionary \citep{van2004woordenboek}. In their work, a pretrained model outputs embeddings, which are used in a Euclidean distance-based vector search. However, despite being pretrained on VGT data, the modelâ€™s performance on dictionary lookup was suboptimal. This shortfall can be attributed to the Zipfian class distribution in the dataset and the fact that the examples come from continuous signing rather than isolated signs.

\section{Methodology}
\subsection{One-Shot Sign Language Recognition}
\label{sec:oneshot}
% \mathieu{als je de vergelijking met mijn paper nog minder obvious wilt maken, zou je kunnen zeggen dat dictionary search 1 applicatie is van one-shot ISLR (en er hier nog over zwijgen maar dit vermelden op punt waar ik een comment geplaatst heb met note HIERDICTIONARY}

% We adapt the two-stage method proposed by De Coster and Dambre \cite{de2023querying}, to perform one-shot sign language recognition. This approach involves an initial supervised pretraining phase, where the model learns to produce internal sign representations or embeddings. These embeddings are then utilised in the one-shot inference phase. While De Coster and Dambre employed Euclidean distance-based one-nearest-neighbour search for this purpose, we propose instead to leverage matching networks, described by Vinyals et al. \cite{vinyals2016matching}. 
% This method, illustrated in \cref{fig:matching}, offers an alternative approach that provides more interpretable results and a better fit for downstream tasks, all without compromising accuracy.

% \mathieu{je verwijst hier niet naar je figuur ;)}
% \mathieu{support set = exemplar set? beter om 1 en dezelfde term te gebruiken doorheen de paper dan}
Our one-shot inference exists out of two steps, namely: initialisation and inference. These steps are illustrated in \cref{fig:matching}. In the initialisation step -- illustrated with solid lines -- the frozen model converts all dictionary videos to embeddings. This collection of embeddings is known as the support set, which serves as a reference for subsequent comparisons during the inference phase. During the inference step -- depicted with dashed lines -- the model processes an input query video and generates its corresponding embedding. This embedding is then compared against the support set to determine the most similar entry. The comparison utilises the attention mechanism as described by \citep{bahdanau2014neural}, which effectively identifies the closest match from the support set solely based on the embeddings.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/Matching2.pdf}
    \caption{We perform one-shot sign classification to search through a dictionary with a query video.
    Solid arrows: a sign language dictionary is mapped to a support set of embeddings by an SLR model. This is done once.
    Dashed arrows: we can classify a new example (query the dictionary) by also mapping the example to an embedding with the same model and using attention to obtain probabilities for every label in the support set. This can be done without regenerating the support set.}
    \label{fig:matching}
\end{figure}

In essence, this method performs \emph{search} as described by \citet{mittal2021compositional}, who formulated it as the ``selection of a relevant entity from a set via \emph{query-key} interactions''. In our case, the queries and keys are the query videos and the support set respectively. Furthermore, this approach applies the softmax function, and hence the output can be interpreted as probabilities. These probabilities provide a measure of confidence in the match, which in turn allows for more nuanced decision-making, ultimately improving the system's interpretability and reliability.

We evaluate the one-shot classification method using two approaches. The first evaluation involves a limited query set with a large support set, assessing the model's ability to handle applications requiring extensive vocabularies, such as SLT. The second experiment introduces variability by randomly selecting different instances from a sign language dataset multiple times, effectively reconstructing the same classes within the support set. By measuring the spread of evaluation metrics across these variations, we investigate how the variability within sign categories and the selection of support set instances influence our approach.
% \mathieu{deze zin: ik zou verduidelijken wat je bedoelt met sign language training dataset, en dat het over de support set gaat. dus dat je een andere dictionary opstelt als het ware}


% \mathieu{goede segue naar datasets, maar ik vind de laatste zin vreemd}
Although this one-shot technique is remarkably elegant, the performance heavily relies on how the manual sign features are represented within the embedding. Two main factors independently impact this objective: the pretraining and the utilised datasets. We employ isolated sign language recognition for pretraining since it aligns best with our downstream goal. Furthermore, both the quality and quantity of the used pretraining data significantly impact the results of the downstream tasks, as the results in \cref{sec:oneshotresults} show.
% \mathieu{laatste zin: is dit iets dat we aantonen met experimenten? dan zou ik dat ook zeggen, bv. "as the results in section/table xyz show"}

% Although this is an integral part of the method's success, in our case, the selection of the correct dataset proved to be more meaningful.




% \subsection{One-Shot Sign Language Recognition}
% % \begin{figure}
% %     \centering
% %     \includegraphics[width=1\textwidth,clip, trim=8pt 8pt 8pt 8pt]{fig/Matching.pdf}
% %     \caption{One-shot classification}
% %     \label{fig:matching}
% % \end{figure}




% To perform one-shot sign language recognition, a model needs to be able to accurately recognise the
% five components that make up signs: handshape, movement, place of articulation, orientation, and non-manual features.

% We adapt the two-step methodology laid out by De Coster and Dambre \cite{de2023querying} to learn the recognition of these components implicitly through training an SLR model. However, we use a significantly larger dataset and a slightly different look-up method. This more varied dataset boosts the learning of the five components, while the revised look-up method
% produces not distances, but the more interpretable probabilities.
% \mathieu{was: produces probabilities, rather than distances. Ga je akkoord Toon? Ik heb deze aanpassing gemaakt om meteen ook te argumenteren waarom probabilities een pluspunt zijn}
% We specify two distinct steps: pretraining a model and the actual one-shot inference. 

% % \begin{figure}[ht!]
% %     \centering
% %     \includesvg[width=\textwidth]{fig/Matching2.svg}
% %     \caption{We perform one-shot sign classification to search through a dictionary with a query video.
% %     Solid arrows: the dictionary is mapped to a support set of embeddings by an SLR model. This is done once.
% %     Dashed arrows: we can classify a new example (query the dictionary) by also mapping the example to an embedding with the same model and using attention to obtain probabilities for every label in the support set. This can be done without regenerating the support set.}
% %     \label{fig:matching}
% % \end{figure}

% \cref{fig:matching} demonstrates the one-shot classification as proposed by Vinyals et al. \cite{vinyals2016matching}. Our methodology's initial step involves a pretrained SLR model encoding each dictionary video to its corresponding embedding, thereby establishing our support set. This process is illustrated with solid lines. Subsequently, the inference itself can take place, which is illustrated with dashed lines. This process happens as follows.
% The identical pretrained SLR model encodes an incoming query video. Based on this query, we calculate the attention score \cite{bahdanau2014neural} for the whole support set. Essentially, the inner product of the query and every element of the support set is taken, and a softmax is computed over the resulting output. This computation yields a probability for each element in the support set. Higher probabilities indicate both a stronger match between the two embeddings and greater certainty in the prediction itself. 
% \mathieu{Toon, als we plaats hebben, zou ik bovenstaande eventueel formeler (met formules) willen beschrijven}

% Although this one-shot method is conceptually straightforward, it depends on certain assumptions made during the pretraining. Primarily, it assumes that the embeddings---the model's internal representations of signs---are sufficiently robust to uniquely and meaningfully describe all signs in the dictionary.
% %The collection of all embeddings is called the embedding space.
% To create such a rich embedding space, a comprehensive and effective pretraining phase is essential.

% \mathieu{@Toon, welke pretraining methods nog? Self-supervised bedoel je dan?
% Ik zou ook niet classification uitleggen, wie deze paper leest zal weten wat classification is.
% Deze paragraaf moeten we misschien nog es samen uitwerken}
% Different pretraining methods could be considered, but we chose classification. The goal of classification is to predict the correct label from a fixed set. Our implementation used the SLR model, with an additional linear layer tailored to this fixed set. The output layer predicts probabilities per label. The choice of one linear layer is far from trivial, this ensures that the desired features are represented linearly separable in the embedding. Ultimately, this implementation ensures that the embedding is by definition a meaningful representation of a given sign. This pretraining step is explained in more detail in \cref{sec:pretraining}.


% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{fig/IMG_0020.jpeg}
%     \caption{Citation Mediocrates}
%     \label{fig:mediocrates}
% \end{figure}

% We follow the two step methodology laid out by De Coster and Dambre \cite{de2023querying}
% to learn the recognition of these components implicitly through training a sign language recognition
% model. However, we use a significantly larger and more varied dataset to boost the learning of
% these components.
% The two steps are: pretraining a model and embedding sign examples with this model.

% \mathieu{afbeelding? Plus, misschien eerst het one shot gedeelte voorop, en dan pas over pretraining praten.}

% First, a pretraining step initialises the model that will be used for one-shot sign language
% recognition. The pretraining step involves training a model with the traditional isolated
% sign language recognition objective. That is, given a set of videos, each containing a single sign,
% classify these videos by predicting the corresponding sign labels. This pretraining step is
% explained in more detail in \cref{sec:pretraining}.

% Second, once the model is pretrained, it is frozen and the classification head is removed. We will be
% evaluating the model with unseen classes and are therefore not interested in the classification
% output, but rather in the internal representation learned in the pretraining step.
% The frozen model is used to embed the dictionary onto the space learned in the pretraining step.
% As a result, every input video is summarised into a single vector, the dimensionality of which is determined by the architecture of the pretrained model.

% After these two steps, the model is finished and can be used for inference. Using the same approach as in step 2, a query video is embedded onto the embedding space. One-shot classification
% is performed by finding the nearest entry in the embedding space and predicting the corresponding label as the
% label for the query. In other words, during inference, classification is performed by simply
% comparing the query to all dictionary entries and returning the label of the closest
% (or most similar) dictionary entry. This method can trivially be extended to few-shot classification
% by considering a $k$-nearest neighbour search, but for our purposes we only have a single example
% so we are limited to one-shot classification in this paper.

% \mathieu{deze paragraaf voelt hier niet echt thuis. misschien kunnen we die ergens anders kwijt of schrappen}
% This approach is completely dependent on the richness of the embedding space and the capability
% of the pretraining step to capture the components of signs in a manner that generalises to unseen
% signs. However, this approach allows the recognition of entirely new signs by simply embedding a single, new, high quality video and appending it to the support set. This is especially useful for sign language dictionaries, as they can be continuously updated without requiring models to be retrained \emph{and} without requiring technical deep learning knowledge.
% \toon{dit hoort hier niet echt, maar had dit idee van argumentering en wou het ergens neerschrijven:}
% This method offers both flexibility and robustness in a elegant fashion. 
% In essence, this method solely performs search, as described by Mittal et al. \cite{mittal2021compositional}, but can be used for any form of retrieval. In this case, we only retrieve labels, but also embeddings for LLMs could be considered for SLT. 


\subsection{Datasets}
Several kinds of datasets are required to perform our experiments. First, we need one or more
pretraining datasets, on which we can train a model for ISLR.
Second, datasets for the one-shot ISLR evaluation are needed. These datasets include both real-world sign language dictionaries and existing isolated sign language datasets that we have adapted for the one-shot task.

\subsubsection{Pretraining Datasets} 
\label{sec:pretrainingdatasets}
% \mathieu{HIERDICTIONARY (zie comment ergens hierboven) - hier zou je kunnen zeggen dat dictionary search 1 vd applicaties is van one-shot, en dat je daarom met de2023querying vergelijkt}
One primary example of one-shot ISLR is dictionary retrieval, like the study by \cite{de2023querying}. To facilitate comparisons with this work, the same pretraining dataset is utilised. This dataset is derived from the VGT corpus
\citep{van2015het}. However, the limited number of classes and the severe class imbalance in this dataset may have contributed to suboptimal performance. Therefore, we look for a larger and more varied dataset for ISLR: we choose ASL Citizen \citep{desai2024asl} for its sizeable vocabulary and because DHH signers recorded the signs.

\begin{figure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/dist_vgt.pdf}
        \caption{Corpus VGT \citep{van2015het}}
        \label{fig:distvgt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/dist_asl.pdf}
        \caption{ASL-Citizen \citep{desai2024asl}}
        \label{fig:distasl}
    \end{subfigure}
    \caption{Class distributions of pretraining datasets, sorted by descending sample count.}
    \label{fig:distributions}
\end{figure}

The VGT dataset consists of 24,967 examples for 292 signs, but these examples are not uniformly
distributed (\cref{fig:distvgt}). Since the dataset is derived from a corpus that contains spontaneous language use,
the distribution is rather Zipfian. The majority of examples represent only a minority of the signs. The examples are cut from continuous signing, which means that every sign execution
is influenced by preceding and subsequent signs, a phenomenon referred to as co-articulation.

The ASL Citizen dataset is richer, containing 83,399 examples for 2,731 signs. These examples
are more uniformly distributed, with an average of 30 instances per sign (\cref{fig:distasl}).
Since the dataset entries were recorded in isolation, no co-articulation occurred. This more closely matches
the task of dictionary search. In fact, the ASL Citizen dataset was envisioned as a dataset of
``in-the-wild dictionary queries'' \citep{desai2024asl}.

% Both datasets are signer-independent, i.e., there is no overlap between the training, validation, and test sets in terms of the signers appearing in those sets. As such, data leakage is avoided and metric based evaluation gives a proper estimate of the performance of the models in the wild, when it is unlikely that someone from the training set will use the model.

%\toon{Misschien iets toevoegen over ASL-LEX, want dit haal ik later opnieuw aan in de limitaties.}
%\mathieu{vind ik hier niet per se nodig, maar doe maar als jij dit wel belangrijk vindt}

\subsubsection{Evaluation datasets} 
One application of our work is enabling native and non-native signers to search through sign language dictionaries by recording or uploading a video of a sign. To evaluate the performance of a 
% \mathieu{hier ook: mss zwijgen over dictionary search en enkel one shot? of expliciet vermelden dat het een toepassing is van one-shot}
dictionary search system (one-shot classifier) we always require two sets: dictionary entries, referred to as the support set, and dictionary queries, which serve as test examples. 
Both sets consist of short videos containing individual signs that mirror the entries
found in the dictionary. Ideally, these signs are performed in citation form, meaning that each sign starts and ends in a resting position. 

Due to sign language dialects and synonyms, multiple signs may correspond to a single word
or concept. However, we do not consider this as the same sign and instead label signs by unique IDs. In this work, we focus on retrieving the exact sign matching the query video, rather than its meaning in spoken language.

For the first evaluation set, we collect a set of dictionary queries in VGT for 30 distinct sign categories (\cref{fig:frequency}). The 10 categories used in prior dictionary retrieval research \citep{de2023querying} are a subset of our set of 30. As the first support set, we use a pre-existing sign language dictionary, that is, the VGT dictionary \citep{van2004woordenboek}.\footnote{The dictionary videos can be downloaded from this website: \url{https://taalmaterialen.ivdnt.org/download/woordenboek-vgt/}.}
This dictionary contains 10,235 unique signs at the time of writing. Like many other sign language dictionaries, every sign has exactly one example, which is why we opt for one-shot ISLR.


% Plot met count per gloss in evaluatie set
\input{plot.tex}


The second set of evaluation datasets includes AUTSL \citep{sincan2020autsl} and WLASL \citep{li2020word}, which consist of Turkish and American Sign Language respectively. The WLASL dataset provides splits for various dictionary sizes -- 100, 300, 1000, and 2000 signs -- offering a robust evaluation for scaling dictionary sizes. AUTSL, on the other hand, comprises a vocabulary of 226 independent signs. For all experiments using these datasets, the test set serves as queries, while the support set is constructed by randomly selecting one entry per class from the training set for 100 different times. To ensure these sets were different, seeds were used. Random sampling provides a more rigorous assessment of the model's performance, demonstrating that its performance does not depend on the quality or consistency of individual dictionary entries.




% \mathieu{@Toon, jij hebt de URLs van al deze videos in je broncode staan vermoed ik? Kunnen we deze in appendix toevoegen aan de paper? Zoals in mijn PhD, gewoon per gloss de link naar de video in het WDB}

% \mathieu{@Toon, kan je hier in een tabel de statistieken van hoeveel voorbeelden we hebben per gebaar oplijsten? aangezien we toch zoveel plaats hebben, is 1 key frame per gebaar in een figuur
% misschien ook wel tof, daar kan ik evt voor zorgen}

% \begin{figure}[ht!]
%     \centering
%     % \includesvg[width=\textwidth]{fig/frequency2_0.svg}
%     \includegraphics[width=\textwidth]{fig/frequency2_0_axes_1.pdf}
%     \caption{The number of dictionary queries per gloss is distributed approximately uniformly
%     with mean 11.93.}
%     \label{fig:frequency}
% \end{figure}





\subsection{Pretraining: Keypoint-based Sign Language Recognition}
\label{sec:pretraining}

To perform one-shot sign language recognition, we first need to pretrain a sign language recognition model. We choose to use a keypoint-based model. By performing human keypoint
estimation on the sign language videos, the sign language recognition task is facilitated.
Moreover, we hypothesise that using a keypoint-based model is beneficial to the one-shot
classification task, because it reduces the impact of the visual properties (background, lighting,
clothing, etc.) of the query and the dictionary videos, and it better aligns the input
distributions of the data used in the pretraining step and the querying step. Another advantage of using keypoints is the ability to integrate publicly available estimators, such as MediaPipe, into client-side applications, ensuring both privacy and low-latency communication.

More specifically, we choose to employ MediaPipe \citep{grishchenko2020mediapipe} for the reasons listed in \cref{sec:islr}. MediaPipe predicts the pose, hands and face keypoints. We only utilise the pose and hand information. Thus, our approach solely focuses on the manual components of signs: handshape, movement, place of articulation, and orientation. These manual features are transferable across sign languages. 
% Mouthings are more sign language specific \citep{bank2015alignment}; we do not integrate them in this work. 
% Models processing the mouthings were trained, optimized and tested, but lowered our one-shot results. 
We also trained, optimised and tested embedding models that use mouth keypoints, but the results for one-shot recovery were worse.
This decrease can be attributed to the fact that mouthings are more language-specific \citep{bank2015alignment}. Therefore, we do not deem them essential to this work, but acknowledge their importance in broader sign language processing.

% We choose the ISLR model from the SignON research project \citep{holmes2023scarcity}, because
% training and evaluation code are available under the Apache 2.0 licence.\footnote{\url{https://github.com/signon-project/wp3-slr-pipeline} and \url{https://github.com/m-decoster/VGT-SL-Dictionary}} The weights
% are also available online.\footnote{\url{https://huggingface.co/signon-project/slr-poseformer-vgt}}
% The name of this model is ``PoseFormer''.
% Previous research with similar models has also illustrated that their constituent sub-networks
% can be transferred to different languages without finetuning \citep{de2023towards}, which is
% a useful property for our use case.

We build on the architectural style of the SignON research project \citep{holmes2023scarcity}, making slight modifications to the number of layers and blocks to better fit the requirements of our task. A schematic overview of this so-called PoseFormer network is shown in \cref{fig:pf}. The network integrates dense and convolutional blocks, followed by a multi-head attention mechanism.

\begin{figure}[ht!]
    \centering
    \includegraphics[clip,trim=2pt 10pt 10pt 10pt,width=\textwidth]{fig/Poseformer2.pdf}
    \caption{The PoseFormer model, represented by solid lines, consists of several blocks. The 1D convolutions process data along the temporal axis, while the frame embedding block handles individual frames. Finally, the multi-head attention block extracts relevant features. After training, the classification head, consisting of a single linear layer (depicted with a dashed line), is removed.}
    \label{fig:pf}
\end{figure}

In this model, a sequence of keypoint coordinates representing human poses serves as input. Initially, 1D convolutions are used for temporal smoothing, capturing short-term dependencies in the sequence. The output is then fed into a multi-layer dense sub-network, which processes each frame individually to extract non-linear representations of the pose. These frame-level representations are subsequently passed through a convolutional block that focuses on learning local temporal context. Finally, a multi-head self-attention mechanism captures global temporal dependencies across the sequence. The resulting vector, representing each sign, is input into a linear classifier for final prediction.

\begin{table}[ht!]
    \caption{We optimise hyperparameters for both models}
    \label{tab:hyperparams}
    \centering
    \begin{tabular}{l|ll}
         \hline
         \textbf{Hyperparameter} & \textbf{ASL} & \textbf{VGT} \\
         \hline
         Batch size & 64 & 128 \\
         Learning rate & 0.0003 & 0.0003 \\
         Representation size & 160 & 192 \\
         Attention layers & 4 & 4 \\
         Attention heads & 8 & 8 \\
         Dropout & 0.2 & 0.2 \\
         \hline
    \end{tabular}
\end{table}

As noted in \cref{sec:pretrainingdatasets}, we use two pretraining datasets with different properties and compare results obtained with them. When we pretrain the PoseFormer with the VGT dataset, we refer to this model as \PFVGT, and when we pretrain it with the ASL Citizen dataset, we refer to it as \PFASL. 
% We optimise hyperparameters for the \PFASL. 
% For the \PFVGT, since we use the pretrained checkpoint from the SignON project, we do not optimise hyperparameters. 
The used hyperparameters for both datasets are represented in \cref{tab:hyperparams}.

% It is a well-known best practice to use separate training, validation, and test sets in deep learning. To report the pretraining stage, we adhere to the predefined splits of the provided dataset. However, for our dictionary search task, we do not rely on the ASL Citizen test set for evaluation. Instead, we add it to our training set to maximally leverage the available data, nearly doubling the size of the training set, from 40,154 to 73,095.

\subsection{Evaluation Metrics}
Several evaluation metrics are used throughout the entire pipeline to ensure a robust evaluation of the model. First, we report the mean Recall@$K$ for $K \in [1, 5, 10]$. Furthermore, two ranking metrics are employed: mean reciprocal rank (MRR) and normalised discounted cumulative gain (nDCG). Given that there is only one relevant item per prediction (its label), the interpretations of MRR and nDCG are similar in this scenario. The key difference is that the inverse of the MRR is the harmonic mean of the ranks of all predictions, which provides an alternative view of the results. For all three metrics, higher is better and a value of one indicates optimal performance. All three metrics -- Recall@$K$, MRR, and nDCG -- are used to evaluate the pretraining, to allow for comparison with a baseline ISLR model. For the evaluation of the one-shot methods, reporting only the Recall@$K$ and the MRR is deemed sufficient, since the interpretation of MRR and nDCG is very similar. 


% Let $r_i$ be the rank for the $i$\textsuperscript{th} test example, that is, the one-based index of the ground truth label in the ordered list of model predictions. Then, for a set of $N$ test examples,
% \begin{equation}
%     \mathrm{MRR} = \frac{1}{N}\sum_{i=1}^{N}\frac{1}{r_i}.
% \end{equation}

% nDCG is similar to MRR, but it considers the ranks of all correct predictions, favouring relevant results that occur earlier in the ordered list of model predictions. Since in our case search results are either correct or incorrect,
% we define the relevance of a search result as a binary value: it is $1$ when the prediction $\hat{y}$
% is equal to the ground truth label $y$, and $0$ otherwise.
% The DCG can be computed for a single test example by considering the ordered list of $M$ predictions.
% For an expected ground truth label $y$, the DCG is equal to
% \begin{equation}
%     \mathrm{DCG} = \sum_{j=1}^{M}\frac{\mathbbm{1}_{\hat{y}_j = y}}{\log_2(j+1)}.
%     \label{eq:dcg}
% \end{equation}
% nDCG is the normalised version of DCG. This normalisation is done by dividing the DCG by the ideal DCG (IDCG). IDCG is the DCG of the optimal ranking. In our specific case, with only one relevant item, the IDCG is always equal to one. Therefore the nDCG simplifies directly to DCG:
% \begin{equation}
%     \mathrm{nDCG} = \frac{\mathrm{DCG}}{\mathrm{IDCG} } = \mathrm{DCG}.
%     \label{eq:ndcg}
% \end{equation}
% We compute the DCG for all $N$ test examples and report the mean.



\section{Results}
\label{sec:results}

\subsection{Pretraining}

The pretraining stage results are summarised in table \ref{tab:asl_results}. The I3D model \citep{desai2024asl} is considered the baseline: it is, as of writing, the best-performing model in the scientific literature for ASL Citizen.
We compare the PoseFormer model with this baseline. 
% Since this model has additional components compared to the Poseformer Transformer Network (PTN) used by \citet{de2023querying} for one-shot ISLR, 
% We perform a limited ablation study to determine the importance of the individual components of the network.
% \mathieu{vind je deze ablation nog nuttig? indien plaats tekort vind ik dit een kandidaat om te schrappen}
To further assess the PoseFormerâ€™s architecture, we conduct a limited ablation study, isolating the impact of its key components. 
Specifically, these components are: the input convolutions and intermediate convolutions, which
appear respectively before and after the frame embedding sub-network. We remove these components
individually and measure the impact with respect to our metrics.
% \mathieu{@Toon: wat mij hier niet duidelijk is is, drop je telkens 1 deel of drop je er meerdere (cumulatief)? Ik vind ook die embedding size discussie hier niet super relevant, ik zou gewoon de beste houden}
%\toon{Enkel poseformer 160 behouden, maar die voor 192 staan ook nog in latex moesten we ze nog nodig hebben. Je weet maar nooit.}

\begin{table}[ht!]
    \caption{The PoseFormer outperforms the I3D baseline on the pretraining task (ASL Citizen). The ablation study
    illustrates the importance of the frame embedding and convolutions in the PoseFormer.}
    \label{tab:asl_results}
    \centering
    \begin{tabularx}{\textwidth}{l | X X | X X X}
         \hline
         \textbf{Model} & $\uparrow$ \textbf{MRR} & $\uparrow$ \textbf{nDCG} & $\uparrow$ \textbf{Rec{@}1} & $\uparrow$ \textbf{Rec{@}5} & $\uparrow$ \textbf{Rec{@}10}\\
         \hline
         % Poseformer 192: & 0.8287 & 0.8672 & 0.7456 & 0.9287 & 0.9531\\
         % \hspace{4mm}Input convolution & 0.8229 & 0.8624 & 0.7401 & 0.9239 & 0.9489 \\
         % \hspace{4mm}Frame embedding & 0.8151 & 0.8564 & 0.7289 & 0.9194 & 0.9470 \\
         % \hspace{4mm}Intermediate convolution & 0.8222 & 0.8621 & 0.7362 & 0.9271 & 0.9514 \\
         Poseformer & \textbf{0.833} & \textbf{0.870} & \textbf{0.751} & \textbf{0.932} & 0.955 \\
         \hspace{2mm}No input conv. & 0.831 & 0.869 & 0.749 & 0.931 & \textbf{0.956} \\
         \hspace{2mm}No frame embedding & 0.811 & 0.854 & 0.723 & 0.920 & 0.946 \\
         \hspace{2mm}No intermediate conv. & 0.819 & 0.860 & 0.733 & 0.926 & 0.951 \\
         \hline
         I3D \citep{desai2024asl} & 0.733 & 0.791 & 0.631 & 0.861 & 0.909\\
         \hline
         % \hline
    \end{tabularx}
\end{table}




% \begin{table}[ht!]
%     \caption{The PoseFormer outperforms the I3D baseline on the pretraining task (ASL Citizen). The ablation study
%     illustrates the importance of the frame embedding and convolutions in the PoseFormer.}
%     \label{tab:asl_results}
%     \centering
%     \begin{tabularx}{\textwidth}{l | X X | X X X}
%          \hline
%          \textbf{Model} & $\uparrow$ \textbf{MRR} & $\uparrow$ \textbf{DCG} & $\uparrow$ \textbf{Rec{@}1} & $\uparrow$ \textbf{Rec{@}5} & $\uparrow$ \textbf{Rec{@}10}\\
%          \hline
%          % Poseformer 192: & 0.8287 & 0.8672 & 0.7456 & 0.9287 & 0.9531\\
%          % \hspace{4mm}Input convolution & 0.8229 & 0.8624 & 0.7401 & 0.9239 & 0.9489 \\
%          % \hspace{4mm}Frame embedding & 0.8151 & 0.8564 & 0.7289 & 0.9194 & 0.9470 \\
%          % \hspace{4mm}Intermediate convolution & 0.8222 & 0.8621 & 0.7362 & 0.9271 & 0.9514 \\
%          Poseformer & \textbf{0.8325} & \textbf{0.8702} & \textbf{0.7509} & \textbf{0.9319} & 0.9552 \\
%          \hspace{2mm}No input conv. & 0.8311 & 0.8691 & 0.7492 & 0.9306 & \textbf{0.9555} \\
%          \hspace{2mm}No frame embedding & 0.8114 & 0.8535 & 0.7231 & 0.9196 & 0.9461 \\
%          \hspace{2mm}No intermediate conv. & 0.8193 & 0.8599 & 0.7329 & 0.9260 & 0.9507 \\
%          \hline
%          Baseline: I3D \cite{desai2024asl} & 0.7332 & 0.7913 & 0.6310 & 0.8609 & 0.9086\\
%          \hline
%          % \hline
%     \end{tabularx}
% \end{table}

\Cref{tab:asl_results} illustrates the PoseFormer's significant improvement over the I3D baseline \citep{desai2024asl} (+ 0.0993 MRR and + 0.1199 Recall@1). As of writing, our results are the state of
the art on the ASL Citizen dataset.
The ablation study emphasises the importance of the components of the PoseFormer.
The input convolutions, which act as temporal filters on the raw keypoint coordinate features, have limited impact on the scores. The frame embedding is also present in the original network, and has a
larger impact (+ 0.0211 MRR and + 0.0278 Recall@1). The intermediate convolutions, which appear in the network between
the frame embedding and self-attention blocks, also have a larger impact than the input convolutions
(+ 0.0132 MRR and + 0.018 Recall@1). These results confirm the relevance of the individual components of the PoseFormer Network.

% The intermediate convolutions have a larger impact (
% Furthermore, each component plays a demonstrable role. The intermediate convolution and frame embedding significantly impact performance. The frame embedding captures crucial pose information for accurate sign recognition. The intermediate convolution smoothes these representations, ultimately mitigating poor landmark representations. The input convolution layer seems less critical. This input filtering might be more relevant for noisier datasets, like corpus-VGT \cite{van2015het}. However, its inclusion allows for a fair comparison to the results of the model used by De Coster and Dambre \cite{de2023querying} in the following section.

\subsection{One-Shot Sign Language Recognition} 
\label{sec:oneshotresults}

There are two sets of results for the one-shot ISLR. The first considers the \PFASL model's capabilities to handle extremely large dictionaries. The second set evaluates both models' resilience to the selection of support set samples. For this set of experiments, we only consider the Recall@$K$ and the MRR, since the interpretation of MRR and nDCG is very similar. 
% \mathieu{dit vond ik niet zo heel duidelijk uit het methodology gedeelte, kan je dat daar nog duidelijker maken wat er precies gebeurt? ik heb daar ook een opmerking gezet denk ik}

\subsubsection{Large dictionaries}

\Cref{fig:top_1_vs_top_5} displays the relation between the dictionary size and Recall@$K$. We refer to the results of the independent models using the names from \cref{sec:pretraining}: \PFVGT~and \PFASL~(i.e., the PoseFormer pretrained on VGT and ASL respectively). For a dictionary of 100 signs, \PFASL~has a Recall@1 of 0.888. \PFVGT, despite being pretrained on the language of this dictionary lookup task, achieves a score of only 0.506.
% \mathieu{ik heb de volgende zin uitgedaan, want ik vind het wat appels met peren vergelijken. Ik snap wat je ermee wil aantonen. Ik zou het dan eerder bij MRR bespreken, waar je kan zeggen dat
% MRR ASL @ 10235 > MRR VGT @ 100. dan heb je het over dezelfde metriek en niet Recall@1 bij het ene model en Recall@2 bij het andere}
% To put this into perspective, the proposed method scores 0.5028 top-2 recall for dictionary size 10235.
For the full dictionary of 10,235 signs, \PFASL~achieves a Recall@1 score of 0.374, which is a substantial improvement over the 0.089 score obtained by \PFVGT. To put these results into perspective, a random search through the 10,235 classes would yield a Recall@1 of only 9.77e-5. These results underscore the impressive performance of \PFASL~in accurately predicting the correct class from a vast dictionary.

\begin{figure}[ht!]
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/compare_acc.pdf}
        \caption{Recall@1 and Recall@5 for large dictionary search}
        \label{fig:top_1_vs_top_5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/paper_mrr.pdf}
        \caption{MRR for large dictionary search}
        \label{fig:mrr}
    \end{subfigure}
    \caption{Metrics for one-shot evaluation}
    \label{fig:ranking_metrics}
\end{figure}

For the \PFASL, although not optimal, the MRR (\cref{fig:mrr}) remains relatively high at 0.508 across 10,235 categories. This suggests that, on average, the correct prediction ranks second. On the other hand, Recall@5 offers a complementary perspective: for the same 10,235 categories, the correct result is among the top five predictions 67\% of the time. This demonstrates the model's ability to consistently rank the correct prediction within a small set of top choices.

% The MRR is shown in \cref{fig:mrr}. For the \PFASL, it is not perfect, but high (still 0.508 for 10,235 categories). This illustrates that on average, the correct prediction appears at rank 2. The Recall@5 gives a different interpretation of this: for 10,235 categories, the correct result is in the top 5 predictions 67\% of the time. 



% \begin{figure}[ht!]
%         \centering
%         \includesvg[width=0.7\textwidth]{fig/compare_acc.svg}
%         \caption{Recall@1 and Recall@5 for large dictionary search}
%         \label{fig:top_1_vs_top_5}
% \end{figure}

% \begin{figure}
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includesvg[width=\textwidth]{fig/top_1_acc.svg}
%         \caption{Recall@1 for dictionary search}
%         \label{fig:top_1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includesvg[width=\textwidth]{fig/top_5_acc.svg}
%         \caption{Recall@5 for dictionary search}
%         \label{fig:top_5}
%     \end{subfigure}
%     \caption{Recall@$K$ accuracy for dictionary search}
%     \label{fig:dict_search}
% \end{figure}

% \mathieu{TODO: EXACTE NUMMERS 0.5 en 80\%}

% \toon{top 1-10 op 10235: [37.4301676 , 50.27932961, 58.10055866, 64.24581006, 66.75977654,69.55307263, 72.34636872, 74.58100559, 77.37430168, 78.49162011]}




\subsubsection{Support set perturbation}

The results on the support set perturbation are given in table \ref{tab:pertubation_results}. We only report the Recall@1 and MRR for both the \PFVGT~and \PFASL. Once again, these results highlight that the \PFASL~is a robuster model than the \PFVGT, because the pretraining dataset better aligns with the downstream task and contains more different signs.
% \mathieu{"despite..." -> in plaats daarvan zegggen dat het door de pretraining dataset komt "because the pretraining dataset better aligns with the downstream task and is more general" of zo}
On the AUTSL dataset, we achieved a Recall@1 of 58.1\%, while the \PFVGT~only achieved a score as high as 40.1\%. Although the results differ by almost 20\%, the spread on both results is about 3\%. The same can be observed in the MRR.
% \mathieu{waar ik het zag heb ik kommagetallen van bv 58,1 aangepast naar 58.1 maar een final pass zou nog goed kunnen zijn}


\begin{table}[ht!]
    \caption{To account for the evolving nature of sign languages, the PoseFormers were tested across various languages. The table also presents the standard deviation associated with perturbations in the dictionary.}
    \label{tab:pertubation_results}
    \centering
    % \scriptsize
    \begin{tabularx}{\textwidth}{l | X X | X X}
         \hline
         % &&&&\\[-1.8mm]
         & \multicolumn{2}{c|}{\PFASL} & \multicolumn{2}{c}{\PFVGT}  \\   
         \hline
         % &&&&\\[-1.8mm]
         \textbf{Dataset} & $\uparrow$ \textbf{Recall@1} & $\uparrow$ \textbf{MRR} & $\uparrow$ \textbf{Recall@1} & $\uparrow$ \textbf{MRR} \\
         \hline 
         % &&&&\\[-2mm]
         WLASL100 & 0.614 $\pm$ 0.024 & 0.692 $\pm$ 0.020 & 0.384 $\pm$ 0.017 & 0.485 $\pm$ 0.017 \\
         WLASL300 & 0.558 $\pm$ 0.011 & 0.646 $\pm$ 0.010 & 0.298 $\pm$ 0.008 & 0.397 $\pm$ 0.009 \\
         WLASL1000 & 0.470 $\pm$ 0.006 & 0.569 $\pm$ 0.005 & 0.225 $\pm$ 0.003 & 0.308 $\pm$ 0.004 \\
         WLASL2000 & 0.391 $\pm$ 0.004 & 0.500 $\pm$ 0.004 & 0.198 $\pm$ 0.002 & 0.269 $\pm$ 0.003 \\
         \hline
         AUTSL & 0.581 $\pm$ 0.036 & 0.687 $\pm$ 0.032 & 0.401 $\pm$ 0.034 & 0.533 $\pm$ 0.033 \\
         \hline
         %    % WLASL &&&&\\
         %    WLASL100 & 0.614 $\pm$ 0.024 & 0.692 $\pm$ 0.020 & 0.755 $\pm$ 0.016 & 0.384 $\pm$ 0.017 & 0.485 $\pm$ 0.017  & 0.586 $\pm$ 0.014 \\
         %    WLASL300 & 0.558 $\pm$ 0.011 & 0.646 $\pm$ 0.010 & 0.715 $\pm$ 0.009 & 0.298 $\pm$ 0.008 & 0.397 $\pm$ 0.009  & 0.508 $\pm$ 0.008 \\
         %    WLASL1000 & 0.470 $\pm$ 0.006 & 0.569 $\pm$ 0.005 & 0.651 $\pm$ 0.004 & 0.225 $\pm$ 0.003 & 0.308 $\pm$ 0.004  & 0.425 $\pm$ 0.003 \\
         %    WLASL2000 & 0.391 $\pm$ 0.004 & 0.500 $\pm$ 0.004 & 0.593 $\pm$ 0.003 & 0.198 $\pm$ 0.002 & 0.269 $\pm$ 0.003  & 0.386 $\pm$ 0.002 \\
         % \hline
         % &&&&\\[-1.8mm]
         %    AUTSL & 0.581 $\pm$ 0.036 & 0.687 $\pm$ 0.032 & 0.755 $\pm$ 0.026 & 0.401 $\pm$ 0.034 & 0.533 $\pm$ 0.033 & 0.633 $\pm$ 0.027 \\
         % \hline
    \end{tabularx}
\end{table}

Secondly, we evaluated the WLASL dataset across all predefined dictionary sizes. As expected, test performance diminished with increasing dictionary sizes, with Recall@1 dropping from 0.611 to 0.391. On the largest dictionary size, we also achieved an MRR of 0.500. This means that, on average, the correct prediction ranks second. In comparison, \PFVGT~places the correct prediction at rank 4.

The results also show a consistently small standard deviation across both datasets, models, and all dictionary sizes, indicating stable performance regardless of dataset variability. This consistency suggests that one-shot ISLR classification is robust to differences in data, maintaining reliable performance across various scenarios. Interestingly, the standard deviation decreases as dictionary size increases, likely due to the larger evaluation set offering a more comprehensive assessment of model accuracy. These findings reinforce the robustness of the models and their strong generalisation capabilities, making them well-suited for real-world applications across diverse settings.

% \toon{vind dit misschien een abrupt einde, weet niet wat nog toe te voegen. Misschien nog iets zeggen over de stds?}\mathieu{ja, want dat is eigenlijk het punt dat je wilt maken: de spread is zeer klein dus is het model robust voor dictionary entry selection}

\section{Discussion}


The results reveal four key insights. First, we confirm that keypoint-based models can achieve state-of-the-art results on the challenging isolated sign language recognition task. By utilising keypoints, the one-shot classification task becomes more feasible, as the pose estimator eliminates visual differences between datasets. Second, we demonstrate that the size, vocabulary, and class distribution of the dataset are critical for the pretraining phase, significantly influencing downstream performance in one-shot classification. Next, we find that alignment between the pretraining and downstream languages is less important than these dataset characteristics. Finally, our results highlight that representing individual signs, rather than relying on translations, is feasible and also crucial for creating future-proof sign language technologies. We detail these insights below.

%\subsubsection{Keypoint-Based Models Achieve State-of-the-art Performance}
Indeed, the keypoint-based PoseFormer achieves state-of-the-art results on the challenging large-vocabulary ASL Citizen dataset. It outperforms the I3D baseline by 0.120 Recall@1 (a 19\% increase). Moreover, the model also transfers seamlessly to downstream tasks on different languages, such as vector-based dictionary search for VGT, as our results illustrate.

%\subsubsection{The Pretraining Step is Crucial for the One-Shot Task}
The model's performance in the one-shot setting depends on the richness of the pretraining data.
Two specific traits of the ASL Citizen dataset enable the high performance of our one-shot
classification approach.  First, it encompasses a broad vocabulary of 2,731 unique glosses.
% , compared to just 250 glosses in other datasets like ASL Signs \cite{asl-signs}\mathieu{misschien wat raar om dit te vermelden aangezien we verder niks doen met ASL Signs. Gewoon zeggen: broad vocabulary punt}. Despite having a similar number of examples -- 83,399 for ASL Citizen versus 94,477 for ASL Signs -- the larger vocabulary in ASL Citizen results in fewer samples per class. 
This large and varied gloss set ensures exposure to a diverse range of signs, which is critical for robust model performance.
Second, the ASL Citizen dataset maintains a uniform class distribution, offering sufficient examples across different handshapes and movements. In contrast, the VGT Corpus \citep{van2015het}, used in prior one-shot research \citep{de2023querying}, follows a Zipfian distribution, where most samples feature the simple pointing handshape. As a result, models trained on VGT struggle to generalise to unseen signs that involve more complex handshapes, as the dataset lacks the necessary variety.
This difference in data diversity explains the significantly higher performance of our \PFASL~model compared to \PFVGT. The PoseFormer model, trained on ASL Citizen, benefits from encountering a wide array of handshapes, enabling it to learn more transferable representations for unknown signs.


%\subsubsection{Pretraining Dataset Variety is More Important than Language}
The variety of the ASL Citizen dataset plays a crucial role in the success of our model in one-shot classification. By ensuring exposure to a wide range of signs, the model minimises the chance of encountering unfamiliar signs, leading to better generalisation. Importantly, our results demonstrate that the diversity within a dataset is more critical than consistency in the language used during pretraining. Even though WLASL \citep{li2020word} and ASL Citizen \citep{desai2024asl} both represent ASL, their partially shared, but differing vocabularies yield markedly different results in pretraining and one-shot classification. This suggests that limiting pretraining to a single dataset or language may not capture the full complexity of an entire sign language, and future models should prioritise dataset variety to enhance robustness in real-world applications.

% subsection about the representation of signs.
Sign representation plays a pivotal role in achieving robust performance in one-shot sign language classification. By focusing on pose-based embeddings, our approach abstracts away from surface-level visual differences, emphasising core elements of sign structure like handshapes, movement, and orientation. This abstraction enables models to generalise across different languages and datasets, which addresses the challenges posed by variations in signer appearance, environment, and video quality. As sign languages evolve and new signs emerge, systems that rely on such representations rather than static translations are better equipped to adapt and remain relevant, paving the way for scalable and inclusive sign language technologies.

% Second, the uniformity of ASL Citizen's class distribution ensures that the model
% is exposed to sufficient examples of a variety of handshapes and movements. This is then in contrast with the VGT Corpus \citep{van2015het} used in prior one-shot sign language recognition research \citep{de2023querying}, for which the class distribution is Zipfian and the majority of examples are for signs that use the simple pointing handshape. The PoseFormer cannot learn
% from such a dataset a robust representation that will transfer to unknown signs, since it will not have
% seen many of the different handshapes that will occur in the set of unknown signs.
% This explains we observe the significantly higher scores with the \PFASL~model,
% when comparing it to the \PFVGT.

\section{Future work}
Despite our considerable gains in ISLR performance and the first results of one-shot ISLR, there are several promising directions for future research. One such direction involves leveraging the lexical information from the ASL-LEX database \citep{caselli2017asl} when using the ASL-citizen dataset. Additionally, extending the existing data with resources like the Sem-Lex benchmark dataset \citep{kezar2023sem} could further enhance recognition performance. However, we argue that the key to achieving more robust sign recognition lies not just in more data but in the diversity of signs it contains. Therefore, multilingual training, where a shared sign representation is used to recognise signs across different languages, may be a more effective approach. Such an approach could vastly expand the potentially recognisable glossary of signs, contributing to more versatile and scalable models.

Finally, the proposed method not only enables highly accurate dictionary search applications but also creates opportunities for other downstream tasks. As mentioned in \cref{sec:oneshot}, \citet{mittal2021compositional} formally described \emph{search}, but also introduced \emph{retrieval} as the extraction of relevant features. We envision the combination of the \emph{search} technique proposed in this paper for the \emph{retrieval} of token embeddings for large language models. The integration into LLMs could facilitate a sign language translation tool or even a sign language-enabled virtual assistant. In summary, due to its effectiveness and lower data requirements, this technique has the potential to significantly advance sign language research, which is currently constrained by data availability. This could lead to the development of more practical tools for the DHH community in the near future.

% % \mathieu{@toon hier kunnen we het hebben over die lexicale dingen en mouthings}
% Despite our notable gains in performance, we acknowledge the limitations of both our approach and evaluation. At this moment, some relevant information that could be used, remains untouched. We expect a performance gain when face keypoints and/or lexical information are used. Additionally, our evaluation strategy has its limitations. However, we also identify several opportunities for this method for other branches of sign language research, like translation. 

% %\subsubsection{The face keypoints are not utilised}
% As discussed in \cref{sec:pretraining}, the facial keypoints are excluded from the pose sequences. This omission results in the loss of a fraction of the crucial information. Notably, certain signs in VGT are identical, except for the mouthings (minimal pairs). In such cases, it is essential to effectively assess these mouthings. For future work, we propose the integration of an auxiliary channel in the network, dedicated to independently processing mouthings from our pose sequences.
% We leave this for future work because more research into the accurate recognition of mouthings is required \cite{albanie2020bsl}.
% %\mathieu{weet niet of deze zinnen hier heel nuttig zijn. uiteindelijk: het is extra informatie die ontbreekt, dus zou het moeten helpen.} This module would generate \toon{self-contained/separate/independent} embeddings. When uncertain, the model can leverage these embeddings, to get a more accurate prediction.

% %\subsubsection{The lexical information is not utilised}
% Another important aspect is the fact that not all available labels are utilised. The ASL Citizen dataset is linked to ASL-LEX \cite{caselli2017asl}, a dataset containing lexical information about each sign. Some examples are the handshape, type of movement and place of articulation.
% These lexical features are the exact information that should be embedded in our embedding space.
% We predict that including these labels will benefit the performance of downstream tasks.
% This lexical information is learned \emph{implicitly} in our proposed method, whereas we expect that learning it \emph{explicitly} will help encode it in the embedding space.% Moreover, if these markers are represented in a linearly separable way in the embedding, we expect more structure to the topology of our embedding space.\mathieu{deze laatste zin vind ik wat vaag en ik vrees dat reviewers hierover zouden kunnen struikelen}

% %\subsubsection{Our test set is limited}
% Furthermore, our evaluation set is limited.
% We evaluate our method on the full VGT dictionary, i.e., when we execute a query, we compare the query with all 10,235 dictionary entries. However, our evaluation is only performed for examples corresponding to a set of 30 sign categories. We emphasise that these signs are not cherry picked to be easily recognised by the model,
% but nonetheless they provide only partial insight into its performance.
% We are currently collecting a larger evaluation set, containing more signs \emph{and} more examples
% for every sign, such that a more representative evaluation can be performed. Since data collection
% takes a considerable amount of time, this will be reported on in future work. In the mean time, our method can already be applied to other one-shot ISLR tasks or for other sign languages.

\section{Conclusion}
Sign language recognition models based on keypoints and self-attention, trained on large-vocabulary
datasets, can classify unknown signs in a different language with just one training example.
We leverage the proven PoseFormer model, pretrain it on the ASL Citizen dataset, and use it in a
one-shot classification setting by leveraging the attention mechanism in the embedding
space of the internal representations learnt by the PoseFormer. The PoseFormer achieves
state-of-the-art sign language recognition on the ASL Citizen dataset (a 19\% increase in Recall@1 compared to previous work)
and on one-shot sign classification (0.508 MRR and 0.374 Recall@1 on 10,235 signs). For the first time, large vocabulary ISLR is enabled thanks to the one-shot classification approach. Furthermore, we prove that the method generalises to different languages and is independent of the used sign-variations inside the support set. Despite the multi-lingual evaluation, we leave the multi-lingual pretraining for one-shot ISLR for future research. Finally, the results of this paper led to the development of a publicly available dictionary look-up application for the DHH community.

% \mathieu{aangezien we enkel over VGT dictionary spreken geeft dit eigenlijk wel weg wie we zijn. willen we trouwens ergens iets zeggen over dat we die one shot datasets ook beschikbaar maken?}
% The method does not yet utilise facial keypoints, which can be important for distinguishing signs based on mouthings. 
% This is left for future research. Also left for future research is the collection of a larger, more representative test set, which is currently in progress.



% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliography{main}
\bibliographystyle{iclr2025_conference}

\end{document}