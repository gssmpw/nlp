\section{Related Work}
\subsection{Isolated Sign Language Recognition}
\label{sec:islr}

ISLR is a classification problem. Traditionally, a system analyses a video depicting an isolated sign and aims to predict the corresponding label or gloss. These input videos can be processed in several ways. Recent advancements **Zhou, "Sign Language Recognition with Keypoint Estimation"** have demonstrated a positive impact using pose estimation models, such as MediaPipe Holistic **Cao et al., "MediaPipe: A Framework for Building High-Quality Video Analysis Models"** (henceforth MediaPipe) and OpenPose **Rogez et al., "OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields"**. These models transform input videos into sequences of skeletal representations, capturing ``keypoints'' or ``landmarks'' of the human pose in 2D or 3D Cartesian coordinates. By removing all information about a person's appearance, these tools enhance the generalisability to downstream tasks. This transformation allows ISLR models to focus solely on the structural aspect of sign videos: how people move their arms, hands, and face.

% \mathieu{Volgende paragraaf begint alsof het over MediaPipe's voordelen gaat gaan, maar is dan
% terug algemeen. Ik zou de eerste zin ofwel naar boven verplaatsen, ofwel de paragraaf meer over
% deze ene tool laten gaan en niet in het algemeen spreken. Kaggle was ten slotte ook MediaPipe}
MediaPipe has significantly advanced the current state-of-the-art of ISLR, but there is still considerable room for improvement. **Haque et al., "A Comprehensive Study on Sign Language Recognition using Deep Learning"** argued that this tool is not directly applicable to fine-grained tasks like sign language recognition. Although the keypoint estimator is generally accurate, MediaPipe struggles when two body parts interact. Since this interaction is elemental to sign language, crucial information is lost. However, recent Kaggle competitions **Kaggle, "Sign Language Recognition using MediaPipe"** based on keypoint estimation using MediaPipe present a different perspective, showing promising results in SLR using keypoint estimation. A key component appears to be the addition of a frame embedding that is not present in the work by Moryossef et al., but present in all top Kaggle competition entries. This frame embedding allows the network to learn the non-linear relationships between keypoints **Gupta et al., "Sign Language Recognition using Transformers"**.

% Overall, MediaPipe provides the optimal balance between accuracy and computational efficiency ____, while it also provides the best estimation of the hands. 

% \mathieu{Misschien hier nog vermelden: a key component appears to be the addition frame embedding that is not present in the work by Moryossef et al., but in all top Kaggle competition entries. This frame embedding allows the network to learn
% the non-linear relationships between keypoints [6]. En ook dat MediaPipe beter is dan OpenPose voor handen, bv eerste zin van deze paragraaf naar hier verplaatsen}

% \mathieu{De eerste zin mag concreter. Niet "there are many decisions to be made", maar je mag gewoon zeggen dat de architectuur een grote impact heeft.}
Not only the preprocessing of sign language videos is essential. The architecture of the models also has a great impact. Until 2020, deep learning approaches to ISLR
% \mathieu{Initial vind ik niet ideaal als woord. Misschien "deep learning approaches until 2020"} deep learning approaches 
primarily used variations of Recurrent Neural Networks **Graves et al., "Supervised Sequence Labelling with Recurrent Neural Networks"**. The common factor of these models is that they are proficient at handling sequential data and dealing with temporal dependencies between different poses. The introduction of transformers **Vaswani et al., "Attention Is All You Need"** in 2017 initiated a paradigm shift. 
% The first applications of transformers in the domain of ISLR and sign language translation were proposed in 2020: ____, respectively ____. 
% \mathieu{Misschien hier ook verwijzen naar Sign Language Transformers van Camgoz voor translation, anders zitten we echt wel praktisch enkel mij te citeren en wordt het obvious :p} 
% In the competitions mentioned above, models employing attention mechanisms also performed significantly better.
The combination of keypoints and attention leads to powerful models for ISLR: the top scoring on the Kaggle ASL ISLR competition **Kaggle, "ASL ISLR Competition"** method achieved 89.3\% test set accuracy on 250 sign categories using keypoint data.




\subsection{Few-Shot Sign Language Recognition}

% Sign languages are natural languages, and as such, they evolve. This evolution necessitates the development of flexible techniques for ISLR. 
The evolving nature of sign languages motivates the need for flexible ISLR techniques.
The traditional 
% \mathieu{Leuke intro, maar misschien hier kort zeggen wat je bedoelt met traditional (verwijzen naar vorige sectie)} 
sign language classification models described in the previous section lack this flexibility, as they are limited to the glossary provided in the training set and require a large number of examples for every sign. 
% To address this limitation many approaches resort to one- or few-shot learning.\mathieu{De vorige zin "to adress" zou ik weglaten, de volgende is een betere introductie tot dit concept. Nu komt het uit het niets, terwijl de volgende zin de context geeft waarom few of one shot relevant is} 
**Santoro et al., "One-shot Learning with Self-supervised Imitation"** argued that ``one can take advantage of knowledge coming from previously learned categories, no matter how different these
categories might be.''  This insight led to the introduction of few-shot learning, where fewer examples per category are required.

There are various gradations to few-shot learning, including zero-shot learning.
%In this approach, the model has never seen an example of the task at hand and completely relies on its prior knowledge.
In zero-shot learning, the model has never seen an example for a given category and relies on its
prior knowledge and some form of description of the category.
Zero-shot learning was first introduced to SLR by **Camgoz et al., "Sign Language Recognition using Zero-Shot Learning"**. Their method involved matching textual descriptions to video inputs, utilising BERT text embeddings **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and 3D CNNs alongside bidirectional LSTMs for video processing. More recently, **Xu et al., "Zero-Shot Sign Language Recognition using Vision Transformers"** approached zero-shot ISLR with a similar technique. This method combined BERT text embeddings with pose estimators and vision transformers, followed by LSTMs for temporal modelling. These zero-shot methods rely on the presence of a textual description of a sign, which is not always available. 

% For languages like VGT a dictionary is often available, a collection of every recognised sign and one corresponding video. 
More often, there are one or multiple example videos available for one sign, which can be used for few-shot learning. If none are available for a given sign, recording one example is easier than accurately describing the sign through text.
% While zero-shot learning offers significant advantages\mathieu{welke advantages? komt niet echt naar boven uit het vorige. Ik zou zeggen dat het uitdagend is, en dat je bv textual descriptions nodig hebt wat je ook niet voor elk gebaar wil doen. Een, of een klein aantal gebaren, verzamelen, kan makkelijker zijn en schaalbaarder. Dus kijken we naar one shot}
 % there are cases where some examples can be beneficial. 
**Wang et al., "One-Shot Learning for Sign Language Recognition"**, for example, leverage multiple examples (i.e., they perform few-shot learning) of one sign to perform K-means clustering and a custom matching algorithm. For some sign languages, a dictionary is available. In the case of VGT, the dictionary contains exactly one example per unique sign, which is ideal for one-shot learning. **Camgoz et al., "One-Shot Sign Language Recognition using Dictionary Lookup"** in essence performed one-shot learning to recognise signs in the VGT dictionary ____. In their work, a pretrained model outputs embeddings, which are used in a Euclidean distance-based vector search. However, despite being pretrained on VGT data, the modelâ€™s performance on dictionary lookup was suboptimal. This shortfall can be attributed to the Zipfian class distribution in the dataset and the fact that the examples come from continuous signing rather than isolated signs.