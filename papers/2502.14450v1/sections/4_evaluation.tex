\section{Evaluation}
\label{sec:eva}

We evaluate \sysname{} in two different ways:
First, we demonstrate the feasibility of \sysname{} with a proof-of-concept prototype (\cref{sec:eva:poc}).
Second, we explore to which degree our approach can generate ready-to-use applications.
For this, we collect a dataset of natural language application descriptions from a group of non-technical users, which we use on our prototype to evaluate the efficacy of \sysname{} (\cref{sec:eva:study-design}).
We present the evaluation results in \cref{sec:eva:results} and discuss our findings in \cref{sec:eva:discussion}.

\begin{figure*}
    \centering
    \subfloat[Syntactic success rate\label{fig:eva:syntactic}]{
        \includegraphics[width=0.48\linewidth]{graphs/baseline_v2_plot/compile_df_barplot.pdf}
    }
    \hfill
    \subfloat[Semantic success rate\label{fig:eva:semantic}]{
        \includegraphics[width=0.48\linewidth]{graphs/baseline_v2_plot/ready_to_use_df_barplot.pdf}
    }
    
    \caption{
        Syntactic and semantic success rates across 26 user answers for tasks of varying complexity. We compare \sysname{} prototype with GPT-4o to a baseline without FaaS.
    }
    \label{fig:eva-llm4faas-baseline}
\end{figure*}


\subsection{Proof-of-Concept Implementation}
\label{sec:eva:poc}

Our prototype of \sysname{} is a Python application that implements the bridge component outlined in \cref{sec:architecture}.
We use OpenAI's GPT-4o model~\cite{openai2024} as an LLM through the API, with the model temperature set to 0.7 and maximum token length set to 1,500.
We provide details on the system prompt in Appendix~\ref{sec:appendix:prompt}.
Our prototype uses the open-source \emph{tinyFaaS}~\cite{paper_pfandzelter2020_tinyfaas} FaaS platform to deploy and manage functions.
In our implementation, we use the tinyFaaS Python runtime and, accordingly, instruct the LLM to produce code in Python.


\subsection{Study Design}
\label{sec:eva:study-design}

Our evaluation is based on a dataset of natural language application descriptions provided by real users, which we use to run a number of experiments on our \sysname{} prototype.

\subsubsection{Dataset}

Our dataset was collected using a questionnaire answered by 26 users without prior programming experience, asking them to provide natural language descriptions of how they would instruct a smart home to automate four tasks with increasing complexity.
We denote these tasks as \emph{easy}, \emph{medium}, \emph{advanced}, and \emph{complex} in the remainder of this paper.
We provide a detailed description on the design of our questionnaire and data collection in Appendix~\ref{sec:appendix:questionnaire}.
All natural language descriptions provided by users are originally collected in Chinese language.
We do not translate these answers to preserve their intended meaning and nuance, yet we are aware that this can impact our results (see our discussion in \cref{sec:eva:discussion}).
In our system prompt, we combine these answers with API descriptions for our fictional smart home.



\subsubsection{Experiments}

We use our dataset of natural language descriptions to evaluate the efficacy of \sysname{}.
For each of our four tasks, we consider two success metrics:
First, we consider the result of \sysname{} for a single task description \emph{syntactically correct} if the resulting application can run without errors or exceptions.
Second, we also consider it \emph{semantically correct} if it successfully passes our tests and implements the task functionality we outline in our questionnaire.
Note that (i)~syntactic correctness is a prerequisite for semantic correctness and (ii)~a result may also be semantically incorrect if the user fails to understand or articulate the task functionality.
This is by design: Although this would not be the fault of the LLM, we do consider the \sysname{} approach to fail if the user is unable to build the application they desire.
We further discuss this in \cref{sec:eva:discussion}.

To isolate the impact of the abstractions of FaaS, we also run our experiments against a baseline without FaaS.
Here, we modify the prompt to have our LLM also generate boilerplate code to run the application, including generating command line instructions to run the application.
While this is unrealistic, as it requires manual work by us to build and run the applications, it allows us to evaluate to what extent FaaS can actually improve the output and reduce operational complexity of LLM-generated code.

Finally, note that we repeat a subset of experiments to ensure stable results.


\subsection{Results}
\label{sec:eva:results}


We show the syntactic and semantic success rate of \sysname{} and our baseline (without FaaS) in \cref{fig:eva-llm4faas-baseline}.
\sysname{} shows an average syntactic success rate of 87.58\% across all tasks, with 91.30\% and 90.77\% for the easy and medium tasks, respectively.
This performance is similar to the baseline, with a mean 88.42\% success across all tasks.

\sysname{} achieves an average semantic success rate of 71.47\%, with the easy and medium task reaching 86.96\% and 81.54\%, respectively.
In contrast, the baseline exhibits a marked decline in semantic success rate, with an average of 43.48\%.

\paragraph{Syntactical Failure Reasons}
\label{sec:eva:result:syntactic-error}
\sysname{} and the baseline exhibit similar rates of syntactical failures, primarily caused by import errors, improper data handling, and missing code. 
Import errors dominate, representing 81.82\% of \sysname{} errors and 57.14\% of baseline errors. 
For \sysname{} errors, 13.64\% are due to improper data handling, and 4.55\% are due to missing code.
In contrast, 28.57\% of baseline errors are from missing code and 14.29\% due to improper data handling.


\begin{figure*}
    \centering
    \subfloat[Syntactic repeat success rate\label{fig:eva:repeat_syntactic}]{
        \includegraphics[width=0.48\linewidth]{graphs/baseline_v2_plot/repeat_barplot_syntactic.pdf}
    }
    \hfill
    \subfloat[Semantic repeat success rate\label{fig:eva:repeat_semantic}]{
        \includegraphics[width=0.48\linewidth]{graphs/baseline_v2_plot/semantic_repeat_barplot.pdf}
    }
    
    \caption{
        Success rates of ten repetitions of \sysname{} code synthesis with three user answers.
        Syntactic success does not significantly change between repeat LLM invocations, either nine or ten out of ten correct in all cases.
        For the complex task, repeating an identical invocation can lead to different results.
    }
    \label{fig:eva:repeat}
\end{figure*}

\paragraph{Repeat Experiment}
\label{sec:eva:result:repeat}

We use three random responses to evaluate performance variation in \sysname{}.
Specifically, we try to quantify to what extent randomness in LLM code synthesis affects the performance of \sysname{}.
To that end, we invoke \sysname{} with identical natural language descriptions ten times, observing syntactic and semantic success for each repetition.

The results in \cref{fig:eva:repeat} show that syntactic success remains stable even across repetitions, with at most one unsuccessful invocation out of ten.
Semantic success, however, has mixed results for more complex tasks.
This shows how randomness in LLM responses even with identical requirements can impact the performance of \sysname{}.
Note that for semantic correctness especially it is not feasible to simply repeat an invocation until it succeeds, as there exists no way to automatically confirm correctness (unlike with syntactic correctness, where, e.g., import errors can be detected).




\subsection{Discussion}
\label{sec:eva:discussion}

Our prototype implementation and experimental results demonstrate that the \sysname{} approach is feasible and is ready for adoption, especially for tasks of low complexity.

\subsubsection{Impact of FaaS}
While the use of FaaS in \sysname{} has no impact of the syntactic correctness of LLM-generated code in our experiments, we do see a significant improvement in semantic correctness.
This shows that the reduced complexity of the FaaS programming model allows the LLM to focus its performance on understanding and correctly implementing user intentions rather than boilerplate code that addresses operational concerns.

\subsubsection{Task Complexity}
Our results also clearly show the impact of task complexity on the semantic success rate of \sysname{}.
We consider two possible causes:
First, the increased task complexity provides a challenge for the LLM, which must produce more complex code with more opportunity for failure.
Second, increased complexity can also be challenging for our users, who must understand and articulate more complex requirements, a particular challenge for less experienced users.
In future work, it would be interesting to compare human software developers as a baseline to determine if a semantic error is caused by incorrect understanding by the LLM or an insufficient user description.

\subsubsection{Model Selection}
We chose GPT-4o as the primary LLM for \sysname{} due to its advanced performance and strong performance in non-English languages~\cite{openai2024}.
\sysname{} exhibits practical performance with GPT-4o, even with more complex tasks.
Nevertheless, advances in LLMs will likely improve the performance of \sysname{}.
In future work, we plan to further evaluate our approach with different LLMs, including models specifically trained for software development.

\subsubsection{User Description Language}

The natural language application descriptions collected from our users are all in Chinese language, which may negatively impact the performance of the LLM.
While we cannot expect users to learn English to use a no-code development platform -- in the same way that we cannot expect them to learn a programming language -- we should be aware of the impact of input languages.
In future work, we plan to further investigate this impact.
We first plan to explore more language-specific models, e.g., those trained on mostly Chinese-language texts.
Second, we also plan to evaluate the feasibility of adding a separate translation step, which may carry the cost of lost nuance in the descriptions.

\subsubsection{Programming Language}

Similar to user description language, the choice of Python as a programming language in our evaluation could also have an impact on \sysname{} prototype performance.
We choose Python as it is widely used and LLMs are likely to perform well with it, yet we also plan to explore other language performance in future work.

\subsubsection{Feasibility of Feedback Loops}

In \sysname{}, we give LLMs only a single opportunity to generate function code before we decide whether it is syntactically and semantically correct.
However, LLMs are known to perform well with feedback, e.g., in a chat.
It may be equally possible to provide feedback to the LLM on generated code, both from the FaaS platform for syntax errors, e.g., missing imports, and from the user for semantic errors, e.g., clarifying application logic.
