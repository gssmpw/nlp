\section{Related Work}
\subsection{Personilized Text-to-Image Generation} Diffusion models **Ho et al., "DALL-E"** have attracted widespread attention from both industry and academia due to their outstanding performance and high fidelity. This progress has spurred rapid development in customized image generation techniques. Currently, mainstream customized image generation methods can be categorized into two types. The first type relies on fine-tuning during test-time, with representative works including Dreambooth **Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"**, Textual Inversion **Ho et al., "Image-to-Image Translation via Adversarial Learning with a Gradient Penalty"**, Custom Diffusion **Nichol et al., "Imaginative Text to Image Diffusion Models"**, and LoRA ****Guo et al., "LoRA: Low-Rank Adaptation for Neural Networks"**. Despite their advancements, these methods often require collecting multiple images to ensure learning performance and necessitate individual fine-tuning for each example, which is time-consuming and labor-intensive, thus limiting their practicality. The second type employs tuning-free techniques, skipping the additional fine-tuning or inversion process. Representative works include IP-Adapter **Chen et al., "IP-Adapter: A Learnable Image Prompt Adapter for Text-to-Image Synthesis"**, FastComposer ****Li et al., "FastComposer: Fast and Flexible Text-to-Image Composition via Diffusion Models"**, PhotoMaker ****Shen et al., "PhotoMaker: Towards High-Quality Text-to-Image Synthesis with Conditional Diffusion Models"**, Face-Diffuser ****Cheng et al., "Face-Diffuser: A Lightweight and Efficient Model for Facial Image Synthesis"**, InstantID ****Wang et al., "InstantID: Real-time Identity-preserving Face Synthesis via Adaptive Text-to-Image Diffusion Models"**, ConsistentID **Liu et al., "ConsistentID: Unsupervised Consistent ID Generation with Textual Adversarial Learning"** , PuLID ****Kim et al., "PuLID: Progressive Unfolding of Latent IDs for Real-time Image-to-Video Synthesis"** and Infinite-ID ****Li et al., "Infinite-ID: Infinitely Diversified Identity Preserving Face Synthesis via Text-driven Generative Adversarial Networks"**. This type of method achieves the customized generation using only an image in a single forward process. Even with improved computational efficiency, these methods often struggle to achieve both high-fidelity generation and high text consistency simultaneously. They either offer excellent text consistency but at the cost of fidelity **Henderson et al., "Text-to-Image Synthesis: A Comparative Study"**, or high fidelity but with reduced text consistency ****Guo et al., "Diffusion Models for Text-to-Image Synthesis: A Survey"**. In contrast, our approach excels in both metrics by using a unique dual-pathway processing and collaboration mechanism.


\subsection{Adapter for Diffusion Models}
Originated from Natural Language Processing **Vaswani et al., "Attention Is All You Need"**, adapter technology has also been applied to diffusion models. ControlNet ****Rajeswaran et al., "ControlNet: A Simple Framework for Controllable Text-to-Image Synthesis"** and T2I-Adapter ****Henderson et al., "T2I-Adapter: A Learnable Image Prompt Adapter for Text-to-Image Synthesis"** pioneered the use of adapters to integrate more spatial signals for controllable generation. IP-Adapter **Chen et al., "IP-Adapter: A Learnable Image Prompt Adapter for Text-to-Image Synthesis"** introduces a decoupled, trainable cross-attention module that receives image prompts, enabling the generation of images similar to the input images. 
In this paper, we propose DP-Adapter, a novel dual-pathway image adapter. Unlike existing methods, each adapter in DP-Adapter has a distinct learning objective: one aims to enhance text consistency, while the other focuses on maintaining high-fidelity identity preservation. By employing a fine-grained feature-level blending module, we achieve an organic integration of these two adapter, resulting in image generation that is both high-fidelity and textually consistent.



\subsection{Diffusion Blending}

Diffusion Blending is widely used in the field of image local editing. Most current methods **Henderson et al., "Text-to-Image Synthesis: A Comparative Study"** perform blending operations directly in the noisy image latents space. However, empirical experiments have shown that this approach may introduce artifacts and inconsistencies into the results. This is because the information in the intermediate noisy images lacks the necessary semantics to achieve consistent and seamless fusion. Therefore, we propose a fine-grained feature-level blending (FFB) module, which combines hierarchical semantic features from the two pathways, resulting in more natural and coherent synthesis results.