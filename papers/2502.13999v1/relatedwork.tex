\section{Related Work}
\subsection{Personilized Text-to-Image Generation} Diffusion models \cite{rombach2022high,nichol2021glide,podell2023sdxl,peebles2023scalable} have attracted widespread attention from both industry and academia due to their outstanding performance and high fidelity. This progress has spurred rapid development in customized image generation techniques. Currently, mainstream customized image generation methods can be categorized into two types. The first type relies on fine-tuning during test-time, with representative works including Dreambooth \cite{ruiz2023dreambooth}, Textual Inversion \cite{gal2022image}, Custom Diffusion \cite{kumari2023multi}, and LoRA \cite{hu2021lora}. Despite their advancements, these methods often require collecting multiple images to ensure learning performance and necessitate individual fine-tuning for each example, which is time-consuming and labor-intensive, thus limiting their practicality. The second type employs tuning-free techniques, skipping the additional fine-tuning or inversion process. Representative works include IP-Adapter \cite{ye2023ip}, FastComposer \cite{xiao2023fastcomposer}, PhotoMaker \cite{li2024photomaker}, Face-Diffuser \cite{wang2024high}, InstantID \cite{wang2024instantid}, ConsistentID  \cite{huang2024consistentid}, PuLID \cite{guo2024pulid} and Infinite-ID \cite{wu2024infinite}. This type of method achieves the customized generation using only an image in a single forward process. Even with improved computational efficiency, these methods often struggle to achieve both high-fidelity generation and high text consistency simultaneously. They either offer excellent text consistency but at the cost of fidelity \cite{li2024photomaker}, or high fidelity but with reduced text consistency \cite{huang2024consistentid,wang2024instantid,ye2023ip}. In contrast, our approach excels in both metrics by using a unique dual-pathway processing and collaboration mechanism.


\subsection{Adapter for Diffusion Models}
Originated from Natural Language Processing \cite{houlsby2019parameter}, adapter technology has also been applied to diffusion models. ControlNet \cite{zhang2023adding} and T2I-Adapter \cite{mou2024t2i} pioneered the use of adapters to integrate more spatial signals for controllable generation. IP-Adapter \cite{ye2023ip} introduces a decoupled, trainable cross-attention module that receives image prompts, enabling the generation of images similar to the input images. 
In this paper, we propose DP-Adapter, a novel dual-pathway image adapter. Unlike existing methods, each adapter in DP-Adapter has a distinct learning objective: one aims to enhance text consistency, while the other focuses on maintaining high-fidelity identity preservation. By employing a fine-grained feature-level blending module, we achieve an organic integration of these two adapter, resulting in image generation that is both high-fidelity and textually consistent.



\subsection{Diffusion Blending}

Diffusion Blending is widely used in the field of image local editing. Most current methods \cite{lugmayr2022repaint, avrahami2023blended, avrahami2022blended,couairon2022diffedit,zhao2023magicfusion} perform blending operations directly in the noisy image latents space. However, empirical experiments have shown that this approach may introduce artifacts and inconsistencies into the results. This is because the information in the intermediate noisy images lacks the necessary semantics to achieve consistent and seamless fusion. Therefore, we propose a fine-grained feature-level blending (FFB) module, which combines hierarchical semantic features from the two pathways, resulting in more natural and coherent synthesis results.