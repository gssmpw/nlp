\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/method.pdf}
    \caption{The DP-Adapter framework utilizes a region-separate processing approach, dividing the target image generation into visually sensitive and text-sensitive regions. For visually sensitive regions, an identity-enhancing adapter is employed to ensure high-fidelity face generation. For text-sensitive regions, a textual-consistency adapter is used to maintain semantic accuracy. To seamlessly blend these regions, we introduce a fine-grained feature-level blending module, which effectively integrates the different areas and minimizes artifacts.
    For simplicity, the textual injection branch is not displayed.}
    \label{fig:method_framework}
\end{figure*}


\section{Method}


In this section, we first introduce the preliminaries of Stable Diffusion \cite{rombach2022high} and image adapter techniques. We then introduce our proposed dual-pathway image adapter model. 
\subsection{Preliminary}

\noindent{\textbf{Stable Diffusion}}.  As a state-of-the-art text-to-image generation model, it begins by encoding an input image $x$ into a latent representation $z$ using a VAE encoder. Noise $N$ is then introduced at time step $t$ to create a noisy latent $z_t$. To guide the generation process with text conditions, Stable Diffusion \cite{rombach2022high} incorporates a CLIP text encoder $\tau$ to encode textual prompts $C$, which are integrated into the cross-attention layers for interaction with the noisy latents. Finally, a conditional U-Net backbone $\epsilon_\theta$ is trained to predict the noise. The training objective is as follows:
\begin{equation}
    L_{SD}(\theta) := \mathbb{E}_{t,x_0,\epsilon} \left[ \lVert N - \epsilon_\theta(z_t, t, C) \rVert^2 \right] \label{eq:LLDM}
\end{equation}

% Preliminary

\noindent{\textbf{Image Adapter}}. 
Image adapter techniques, as presented in \citet{ye2023ip, song2024moma, mou2024t2i}, leverage lightweight adapter networks to process visual condition inputs. These techniques enhance pre-trained text-to-image diffusion models by incorporating image prompt capabilities. They typically employ a trainable cross-attention module that processes image conditions and merge the output with the text cross-attention branch, as detailed below:
\begin{equation}
\operatorname{Attn}_{final} = \operatorname{Attn}_{text} + \alpha \cdot \operatorname{Attn}_{image}
\end{equation}
where $\alpha$ is a weight that controls the intensity of image condition injection. 


\subsection{DP-Adapter Framework}

The pipeline of DP-Adapter includes three parts: (a) Generation of visually sensitive regions
(b) Generation of text-sensitive regions, and (c) fine-grained feature-level blending.  In the following sections, we will provide a detailed exposition of the functions and characteristics of these important components.




\noindent{\textbf{Generation of Visually Sensitive Regions}}. As shown in the upper branch of Figure \ref{fig:method_framework}.a, the objective is to achieve high-fidelity human generation, with a specific focus on the facial region. Specifically, we crop the face image from the original image and utilize a pre-trained visual encoder to extract the input face embedding. Subsequently, a trainable MLP is employed to project the visual embedding. To accommodate the visual embedding, we construct a lightweight identity-enhancing adapter (IEA) comprising a trainable cross-attention module. Finally, the output of the IEA is added to that of the original text cross-attention, in the same manner as IP-Adapter \cite{ye2023ip}. To achieve precise control over the regions where visual conditions are applied, inspired by the principle of division of labor, we propose a region-based denoising loss function to replace the traditional global denoising. We focus on the denoising learning of Visually Sensitive Regions (i.e., the facial region). Such a specific learning objective allows for precise control over the influence of the IEA, thereby significantly improving the identity fidelity of the generated images. Specifically, we use a face region mask to impose constraints on the original denoising loss function, ensuring that the model optimizes only for the designated region during the training process. The formulation of loss function is defined as follows:
\begin{equation}
    L_{IEA}(\theta) = \lVert M * (N - N^{IEA})\rVert^2 
    \label{eq:IEA}
\end{equation}
where, $M$ is the face region mask, $N^{IEA} = \epsilon_\theta(z_t, t, C, I_f)$ is the predicted noise, $I_f$ is the cropped face image.

\noindent{\textbf{Generation of Text-Sensitive Regions}}. The goal of generating text-sensitive regions is to enhance the semantic consistency between the generated image and the corresponding text prompt. As shown in the lower branch of the Figure \ref{fig:method_framework}.b, we propose the textual-consistency adapter (TCA). While TCA shares the same structure as IEA, their responsibilities differ. TCA employs a softened visual injection method to process face embeddings rather than completely dropping the visual condition. This approach mitigates the intrusion and interference of visual signals on text-sensitive regions, thereby enhancing the expression of textual signals. Specifically, we employ a simple yet effective method to achieve visual signal softening by lowering the $\alpha$ coefficient during the merging of visual and text cross-attention outputs, as illustrated below:

\begin{align}
    \operatorname{Attn}_{final} &= \operatorname{Attn}_{text} + \alpha \cdot \operatorname{Attn}_{image}, \alpha = 0.5
     \label{eq:alpha}
\end{align}
this straightforward adjustment effectively reduces visual interference, significantly improving textual semantic consistency. Moreover, to minimize the interference of textual signals on visually sensitive regions, TCA also adopts a region-based denoising loss function. During training, we focus on denoising learning in Text-Sensitive Regions, i.e., non-face regions, to ensure the model accurately applies textual semantics to the correct positions. The formulation of this loss function is defined as follows:


\begin{equation}
    L_{TCA}(\theta) = \lVert (1-M) * (N - N^{TCA})\rVert^2 
    \label{eq:IEA}
\end{equation}

where $N^{TCA}$ is the noise predicted by this pathway.




\noindent{\textbf{Fine-grained Feature-level Blending Module}}.
To obtain the final prediction, we need to merge the regions generated by IEA and TCA. A straightforward approach is to merge and blend at the noise latents level based on the mask. However, this simple blending results in numerous artifacts in the generated images. This is because the noise latent space lacks sufficient semantics to achieve consistent fusion, leading to degraded generation results. Conversely, the deep features of diffusion models contain rich semantic information. Therefore, we propose the fine-grained feature-level blending module, as illustrated in Figure \ref{fig:method_framework}.c. Specifically, we fuse the output features of corresponding blocks from the two branches using a mask and input the fused features into the next block. Since the size of the feature maps varies between blocks, we downsample or upsample the binary mask to match the resolution of the features. We then perform weighted fusion of the feature maps from the two branches, using the adjusted mask to control their respective contributions. 
This fusion process ensures that the features from different regions are effectively combined, thereby maintaining consistent semantic information and generating more natural and coherent synthesis results.
Finally, we merge the noise predictions from the two pathways based on the mask to obtain the global prediction and calculate the overall denoising loss, defined as follows:
\begin{align}
    N^{f} = (1-M)*N^{TCA} + M * N^{IEA} \\
    L_{fusion}(\theta) = \lVert (N - N^{f})\rVert^2 
     \label{eq:alpha}
\end{align}

% 需要写个总的损失表达式
\noindent{\textbf{Loss Function}}. The loss function of DP-Adapter is defined as follows:
\begin{equation}
    Loss = L_{IEA} + L_{TCA} + L_{fusion}
    \label{eq:fusion}
\end{equation}
where $L_{IEA}$ is dedicated to facial areas, refining the fidelity of facial features during the generation. $L_{TCA}$ focuses on optimizing non-facial regions to enhance the consistency between the generated image and its textual description. $L_{fusion}$, which is applied in the final stage after feature blending, performs a global optimization of the predictions. This step ensures that the generated images exhibit a natural harmony and visual cohesion.





\noindent{\textbf{Inference of DP-Adapter}}. During the inference stage, it is common to lack masks as auxiliary information. Traditional methods \cite{gu2024mix,wang2024instantid} typically rely on manually defined bounding boxes (bbox), poses, and other spatial control information to guide image generation. While this provides some guidance, it limits the diversity of generated images and the flexibility of text prompts. To overcome this limitation, we propose a mask generation method based on layout priors, which automatically generates masks by leveraging internal model layout information. Specifically, our TCA possesses robust text semantic understanding capabilities, enabling it to generate image content that is highly consistent with the layout, background, and pose described in the text input. Based on this, we extract cross-attention maps corresponding to visual prompts from the TCA, as shown in Figure \ref{fig:inference_mask}.b. By applying a threshold, we filter out high-response areas, such as the face region, while excluding low-response areas, as demonstrated in Figure \ref{fig:inference_mask}.c. To further enhance mask quality, we introduce a largest region filtering technique. This technique identifies the largest contiguous area in the image and retains only the responses within this region, effectively excluding irrelevant responses outside of it, as shown in Figure \ref{fig:inference_mask}.d. Finally, through binarization, we obtain a region mask, providing more precise face position guidance for image generation (see Figure \ref{fig:inference_mask}.e). For further details, please refer to the supplementary material.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/mask.pdf} % 图片路径
    \caption{The process of generating face mask images during the inference stage.} % 图片说明
    \label{fig:inference_mask} % 引用标签
\end{figure}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/single_compare.pdf}
    \caption{Qualitative comparison with several SOTA human image customized generation methods.}
    \label{fig:single_compare}
\end{figure*}