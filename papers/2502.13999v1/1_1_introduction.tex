\section{Introduction}

\begin{figure}[t]
    \centering
    % \includegraphics[width=\linewidth]{imgs/single_compare_figure.pdf} % 图片路径
    \includegraphics[width=\linewidth]{imgs/teaser.pdf}
    \caption{
Compared to other methods, our approach excels in preserving human identities while ensuring textual semantic consistency.} % 图片说明
    \label{fig:teaser} % 引用标签
\end{figure}

In today's digital age, the creation and sharing of personal content have become ubiquitous. AI-powered methods for generating human-centric content provide individuals with innovative avenues for self-expression and presentation. Customized human image generation \cite{li2024photomaker, xiao2023fastcomposer, wang2024high, wang2024instantid, huang2024consistentid,wu2024infinite,guo2024pulid}—which focuses on creating entirely new images of individuals based on reference images and textual prompts—has garnered significant scholarly interest. This technology offers diverse applications, including virtual try-on, advertising design, and artistic creation, making it a valuable tool in various creative and commercial fields.


Current methods for customized human image generation can be categorized into two types, based on whether fine-tuning is required during testing. The first type involves methods that fine-tune pre-trained text-to-image (T2I) models, such as Stable Diffusion \cite{rombach2022high}, using multiple images of the same identity (ID). Notable examples include Dreambooth \cite{ruiz2023dreambooth}, Textual Inversion \cite{gal2022image}, and Low-Rank Adaptation (LoRA) \cite{hu2021lora}. While these approaches are effective in achieving high fidelity, the fine-tuning process is resource-intensive and time-consuming, which limits their practicality. Additionally, the need for multiple images per ID poses challenges in scenarios where data is limited.

In a parallel line of research, tuning-free methods typically utilize a pre-trained image encoder to extract reference image embeddings as visual guidance, enabling image generation in a single forward pass. Prominent examples include FastComposer \cite{xiao2023fastcomposer}, PhotoMaker \cite{li2024photomaker}, Face-Diffuser \cite{wang2024high}, ConsistentID \cite{huang2024consistentid}, InstantID \cite{wang2024instantid}, and IP-Adapter \cite{ye2023ip}. Although these methods are computationally efficient, they often struggle to balance high-fidelity image generation with maintaining strong text consistency. As illustrated in Figure \ref{fig:teaser}, methods such as ConsistentID \cite{huang2024consistentid}, InstantID \cite{wang2024instantid}, and IP-Adapter \cite{ye2023ip} can produce high-fidelity images of individuals, but frequently lose key textual elements, such as Superman outfits or autumn trees, leading to reduced semantic consistency with text prompts. On the other hand, PhotoMaker \cite{li2024photomaker} demonstrates a better understanding of textual semantics, but tends to generate images with lower fidelity, such as altering the reference person's hairstyle (see Figure \ref{fig:teaser}, third row).

The primary reason these methods struggle to overcome the aforementioned challenges is the lack of effective constraints during the joint integration of visual and textual conditions. Traditionally, these approaches either directly blend visual features with corresponding text token features \cite{xiao2023fastcomposer,li2024photomaker}, or use a trainable module to process visual features separately \cite{ye2023ip,huang2024consistentid}. However, both techniques fail to impose sufficient supervision and constraints upon the interaction between text and visual information. This lack of constraints allows visual information to encroach upon areas where textual information should dominate, and vice versa, leading to severe harmful interference between the two modalities, ultimately resulting in the suboptimal expression of both.

Recent study, Face-Diffuser \cite{wang2024high}, suggests that visual and textual conditions influence the generation of different image regions in distinct ways. Specifically, two distinct types of region can be identified: visually sensitive regions and text-sensitive regions. Visually sensitive regions, such as facial areas, are primarily influenced by visual conditions, which are crucial to accurately depicting detailed features such as face shape and skin tone. In contrast, text-sensitive regions, such as the background, actions, and poses, are predominantly guided by textual conditions, with visual conditions serving a supplementary role in these areas.  To leverage this property, Face-Diffuser \cite{wang2024high} proposes to tune two independent models for subject and scene generation; however, it sacrifices training efficiency and is prone to introducing unnatural qualities or even irrationality during regional fusion.

Unlike Face-Diffuser \cite{wang2024high}, we propose a Dual-Pathway image prompt Adapter  (DP-Adapter) module to enhance text consistency and improve image fidelity. For visually sensitive regions, such as facial features, we implement an Identity-Enhancing Adapter (IEA) that leverages the original visual conditions to ensure a detailed and accurate representation. For text-sensitive regions, including backgrounds and poses, we use a Textual-Consistency Adapter (TCA) to minimize the impact of visual conditions, thereby reducing visual intrusion and preserving textual semantics.

After processing the visual and textual prompts separately, we merge the information from both conditions using a Fine-grained Feature-level Blending (FFB) module. Unlike most methods \cite{avrahami2022blended, zhao2023magicfusion, wang2024high}, which rely on direct blending in the noise space and are prone to introducing visual artifacts, our FFB module combines hierarchical semantic features from both pathways. This approach ensures a smooth and artifact-free integration of visual and textual conditions, resulting in more coherent and high-quality generated images.

Based on both quantitative and qualitative results, our approach not only achieves high-fidelity human image personalization but also excels at maintaining text consistency compared to current state-of-the-art (SOTA) methods. Our key contributions can be summarized as follows:
\begin{itemize}
\item We introduce a novel dual-pathway image prompt adapter for human image customization, featuring an Identity-Enhancing Adapter (IEA) to emphasize the processing of visual prompts for visually sensitive regions, and a Textual-Consistency Adapter (TCA) to prioritize text prompts for text-sensitive regions. This approach effectively mitigates the harmful mutual interference between visual and textual conditions seen in existing methods, leading to significant improvements in both human identity preservation and text consistency.

\item We present a fine-grained feature-level blending module that hierarchically combines visual and semantic information across multiple feature levels. This approach greatly reduces artifacts in the generated images, resulting in more natural and coherent synthesis outcomes.

\item Extensive experiments across various application scenarios demonstrate that our method surpasses other state-of-the-art approaches, delivering superior performance and robust generation capabilities.
\end{itemize}
