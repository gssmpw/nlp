
\section{Experiments}

\subsection{Implementation Details} 
We adopt SDXL as our base model and refer to the source code of IP-Adapter \cite{ye2023ip} for implementation and use the FFHQ dataset processed by FastComposer \cite{xiao2023fastcomposer} for training. We use cropped face images as image prompts for IEA and TCA. For each cropped face image, we apply random flipping and rotation for data augmentation. Our model is trained on a single NVIDIA A6000 GPU with a batch size of 4. The training process takes approximately 6 hours to complete. We utilize the Adam optimizer with a learning rate set to 1e-5. During the training process, only the parameters of IEA and TCA are updated. During inference, we use the DDIM sampler for 50 steps, with the guidance scale set to 5.0. The generated image size is 1024×1024.

\subsection{Experiment Setting}

\noindent{\textbf{Comparison Methods}}. We compare our approach with several SOTA methods: FastComposer \cite{xiao2023fastcomposer}, FaceDiffuser \cite{wang2024high}, IP-Adapter \cite{ye2023ip}, PhotoMaker \cite{li2024photomaker}, InstantID \cite{wang2024instantid}, ConsistentID \cite{huang2024consistentid}.




\noindent{\textbf{Evaluation Metrics}}.
To comprehensively evaluate the performance of our method, we employed two widely used metrics in related works \cite {li2024photomaker,wang2024instantid,huang2024consistentid,xiao2023fastcomposer}: the CLIP image-text similarity score (CLIP-IT) and the face similarity score (face score). These metrics assess the text consistency and facial fidelity of the generated images, respectively. Moreover, we introduced two human preference metrics, PickScore \cite{Kirstain2023PickaPicAO} and HPS \cite{wu2023better}, to assess whether the generated results align with human aesthetic preferences.



\noindent{\textbf{Test Data}}. We collected 30 individual image samples. The dataset was curated to ensure balanced gender and age distribution, thereby enhancing its representativeness and diversity. To comprehensively evaluate the performance of our method, we prepared 40 prompts that cover actions, clothing, accessories, and scenes. Additionally, we combined these prompts to generate complex text prompts for further testing. For each text prompt, we generated four images, resulting in a total of 4.8K images for evaluation. More details are listed in the  supplementary material.



\subsection{Comparison with SOTA Methods}


\noindent{\textbf{Qualitative Results}}.
Figure \ref{fig:single_compare} presents a qualitative comparison between our method and other approaches. We showcase the generated results across five categories of textual descriptions: accessory, clothing, action, action + clothing, and action + clothing + background. Although InstantID \cite{wang2024instantid} and ConsistentID \cite{huang2024consistentid} tend to generate high-fidelity portraits, they often lose consistency with textual semantics. IP-Adapter \cite{ye2023ip} improves text consistency to some extent but still loses some textual elements, especially under composite multi-category textual conditions (e.g., "riding horse" in the third row and "spacesuit" in the fourth row). PhotoMaker \cite{li2024photomaker} exhibits good text consistency, and the generated images align well with the textual prompts. However, it fails to produce high-fidelity human images. In contrast, our method not only generates high-fidelity human images but also maintains consistency with textual prompts. Additionally, our approach can generate images with complex and realistic backgrounds, further demonstrating its diverse generation capabilities.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{imgs/ablation.pdf} % 图片路径
    \caption{Qualitative ablation study for core components of DP-Adapter.} % 图片说明
    \label{fig:ablation} % 引用标签
\end{figure}


\noindent{\textbf{Quantitative Results}}. 
In Table \ref{tab:compare}, we present the quantitative results of our method compared with other state-of-the-art (SOTA) methods. It is evident that these SOTA methods struggle to achieve both high-fidelity generation and text semantic consistency. For instance, PhotoMaker achieves the highest image-text similarity but has poor face fidelity, with a Face Score of 65.42. While InstantID, ConsistentID, and IP-Adapter demonstrate high Face Scores, their text semantic consistency is notably poor, with a maximum CLIP-IT score of only 22.76. FastComposer and Face-Diffuser perform poorly on both metrics. In contrast, our method achieves the highest Face Score (81.06) and the second-highest CLIP-IT score (25.07, just behind PhotoMaker), indicating that our approach successfully enhances both high fidelity and text semantic consistency. Furthermore, when evaluating alignment with human aesthetic preferences, our method outperforms all compared approaches, as evidenced by the highest PickScore (21.97) and HPS (21.31). This further demonstrates its superiority in generating images that align with human aesthetics and underscores its practical applicability.


% \input{Tabs/pick}

\input{Tabs/table1}

\input{Tabs/ablation}

\subsection{Ablation Study}

Table 2 illustrates the efficacy of our method's components under different configurations. Employing the IEA alone results in high facial fidelity (79.72) yet low text consistency (19.05). The TCA, when used alone, improves text consistency (24.25) but reduces facial fidelity (64.48). Combining IEA and TCA enhances both metrics, increasing CLIP-IT to 24.81 and Face Score to 80.26. The addition of the FFB component further optimizes performance, with CLIP-IT rising to 25.07 and Face Score to 81.06. The aforementioned results confirm the effectiveness of the IEA, TCA, and FFB components.


We also present qualitative ablation results, as shown in Figure \ref{fig:ablation}. It can be observed that while IEA cannot generate content corresponding to the text prompt, it can produce high-fidelity human images, indicating that IEA focuses on improving human fidelity. In contrast, TCA can generate highly detailed scenes, such as basketball courts, but the facial details are poor. Naively blending IEA and TCA with a hard mask results in many artifacts. However, FFB, through fine-grained feature blending, effectively eliminates these artifacts, producing natural human images that maintain both high fidelity and text consistency.



\subsection{Applications}

\noindent{\textbf{Controllable Headshot-to-Full-Body Portrait Generation}}.
Most current methods lack the flexibility to control the generation from headshot to full-body images. In contrast, our method allows for controllable generation of headshot, half-body, and full-body images by adjusting the $\alpha$ weight: see Figure \ref{fig:p2f}. This capability  is beyond the reach of most existing methods.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/p2f.pdf} % 图片路径
    \caption{Applications of controllable headshot-to-full- body portrait generation.} % 图片说明
    \label{fig:p2f} % 引用标签
\end{figure}




\noindent{\textbf{Age Editing}}.
As illustrated in Figure \ref{fig:age}, our method allows for editing the age of the person in the reference image. 


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/age.pdf} % 图片路径
    \caption{Applications of changing human age.} % 图片说明
    \label{fig:age} % 引用标签
\end{figure}

\noindent{\textbf{Old-Photo to Reality}}.
Our method also supports bringing person from old photographs into real life. As shown in Figure \ref{fig:old2real}, we can generate images of Newton in various real-life scenarios.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/old2real.pdf} % 图片路径
    \caption{Applications of bringing old photos to life.} % 图片说明
    \label{fig:old2real} % 引用标签
\end{figure}




% \noindent{\textbf{Identity Mixing}}.
% As shown in Figure \ref{fig:mixing}, our method also supports blending two faces to create a new one, and based on this new identity, we can use text-driven prompts to generate diverse human images.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{imgs/identity_mixing.pdf} % 图片路径
%     \caption{Applications of identity mixing.} % 图片说明
%     \label{fig:mixing} % 引用标签
% \end{figure}


\noindent{\textbf{Expression Editing}}.
As shown in Figure \ref{fig:expression}, our method also supports specifying facial expressions, such as smiling, surprised, or angry, during customized generation.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{imgs/expression.pdf} % 图片路径
    \caption{Applications of expression editing.} % 图片说明
    \label{fig:expression} % 引用标签
\end{figure}





\subsection{Ethics Check}
We place a high priority on ethical considerations. Like other methods \cite{ruiz2023dreambooth, li2024photomaker, wang2024instantid}, our approach faces certain ethical risks. However, its primary aim is to provide users with a respectful and meaningful personalization solution, such as creating commemorative items or enhancing interpersonal relationships. To prevent misuse of the method, we have implemented stringent security checks. We strictly prohibit any form of abuse or generation of harmful content.
