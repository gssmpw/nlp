\section{Introduction}

% Opening
Customized human image generation, which attracts considerable scholarly interest \cite{ye2023ip,xiao2023fastcomposer,wu2024infinite,wang2024instantid,valevski2023face0,ruiz2023dreambooth,li2024photomaker,huang2024consistentid,gal2022image,chen2023photoverse}, has spurred the development of various applications such as virtual try-on, advertising design, and artistic creation. 

% Research Background--ã€‹research gap or limitations --> question 
Current research in personalized human image generation can be broadly divided into two categories: tuning-based \cite{ruiz2023dreambooth,valevski2023face0,kumari2023multi,gal2022image} and tuning-free \cite{ye2023ip,xiao2023fastcomposer,li2024photomaker,wang2024instantid,huang2024consistentid,wu2024infinite,chen2023photoverse}. Within the tuning-based category, several pioneering methods have been proposed, such as Dreambooth \cite{ruiz2023dreambooth}, Textual Inversion \cite{gal2022image}, and Custom-Diffusion \cite{kumari2023multi}. Despite their high-fidelity image generation capabilities, these methods require the manual collection of multiple identity (ID) images and necessitate individual fine-tuning for each ID, leading to significant training costs.


In a parallel line of research, tuning-free methods have been introduced \cite{li2024photomaker, wang2024instantid, ye2023ip, huang2024consistentid}. These methods typically fall into two categories based on how they utilize ID embeddings. In the first type, exemplified by PhotoMaker \cite{li2024photomaker} and FastComposer \cite{xiao2023fastcomposer}, ID embeddings are integrated by mapping them to the CLIP text space and fusing them with corresponding text token embeddings. However, straightforward mapping results in the loss of detailed ID embedding information, reducing face fidelity and affecting the semantic expression of the original text, thereby impairing text-image semantic alignment. In the second type, illustrated by IP-Adapter \cite{ye2023ip} and ConsistentID \cite{huang2024consistentid}, a trainable cross-attention module is designed to directly handle ID information, significantly enhancing face fidelity. Nonetheless, under mixed conditioning, the visual ID conditioning may interfere with or weaken text conditioning, compromising textual semantic consistency. 

To improve text alignment, some works \cite{pan2023kosmos, li2024blip,song2024moma} have started combining multimodal large language models (MLLMs) with diffusion models for joint training. These approaches treats images as a form of language, fully fusing and aligning image and text features to effectively bridge modalities.
However, incorporating MLLMs such as LLaVA-7B \cite{liu2023llava} on the basis of large-scale diffusion models undoubtedly further increases the training costs.

Despite the advancements in customized human image generation, a crucial question remains unanswered: Can there be a method that, without requiring additional training, enhances text semantic consistency while maintaining high fidelity? In this paper, we aim to address the aforementioned challenge by focusing on the following objectives: 1) enhancing text alignment while maintaining high fidelity to achieve an optimal balance. 2) developing a training-free method to reduce training costs.

Therefore, we propose a simple yet effective training-free framework for customized human image generation. Specifically, we observed that when visual and textual conditions are applied together, the visual conditions often interfere with the expression of text semantics. Reducing the intensity of visual conditions can enhance text semantic consistency but at the cost of facial fidelity. Based on this observation, we introduce a self diffusion fusion method, which combines different versions of the same diffusion model, each tailored to different levels of visual signal injection. We incorporate a weaker visual condition model specialized for text alignment alongside a stronger visual condition model aimed at preserving identity fidelity. For efficient fusion of the two versions, we propose a novel region-guided dual-path inference paradigm to decouple foreground and background noise prediction in the denoising process adaptively.

Experimental results demonstrate that our method significantly enhances text semantic alignment at zero training cost, achieving a superior balance between text alignment and identity fidelity. It supports a wide range of applications, including controllable portrait-to-full-body image generation, changing expression, and identity mixing. Moreover, it excels not only in single-person synthesis, but also seamlessly extends to dual-person scenarios, demonstrating robust generalization capabilities.

Our main contributions can be summarized as follows:

\begin{itemize}
    \item We introduce a novel training-free framework for customized human image generation, which enhances text semantic consistency while preserving high fidelity.
    \item We propose self diffusion fusion method that integrates region-guided dual path inference, leveraging multiple versions of a diffusion model to balance visual signal injection.
    \item Extensive experiments and various application scenarios demonstrate that our method outperforms other state-of-the-art (SOTA) methods in terms of superior performance and robust generation capabilities.
\end{itemize}



% Challenge ---> our goal


