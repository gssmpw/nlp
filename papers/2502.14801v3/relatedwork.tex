\section{Related Work}
\subsection{Interpretability and Accident Analysis in Autonomous Driving}

Interpretability is crucial in autonomous driving systems for enhancing user trust and ensuring transparent decision-making. Traditional methods primarily leverage vision-based and LiDAR-based technologies. Vision-based systems employ visual attention mechanisms and segmentation maps to provide insights into vehicle behavior based on its surroundings~\cite{kim2017interpretable}. LiDAR-based systems generate three-dimensional maps and detect objects to improve spatial awareness and interaction understanding~\cite{zeng2019end}. However, these techniques often fail to deliver explanations in natural language, making it challenging for non-expert users to comprehend the vehicle's decisions.

To address this, Kim et al.~\cite{kim2018textual} introduced textual explanations, translating vehicle behaviors and decision-making processes into comprehensible language. Building on this, the Action-aware Driving Caption Transformer (ADAPT) framework~\cite{jin2023adapt} utilizes a transformer-based architecture to elucidate the ‘why’ and ‘how’ behind autonomous decisions. Our AVD2 framework advances this approach by integrating Self-critical Sequence Training (SCST)-based contextual analysis~\cite{bujimalla2020b}, further enhancing the clarity and accessibility of accident explanations. Recognizing the complexity of real-world traffic scenarios, existing methods, such as Video Question Answering (VQA) models, attempt to analyze accident causes by combining visual data with structured queries~\cite{xu2021sutd,liu2023cross}. However, these approaches often struggle to capture the complexity and variability of real-world traffic scenarios~\cite{le2020hierarchical}. By leveraging the strengths of ADAPT, AVD2 significantly enhances the precision and contextual relevance of accident explanations. AVD2 generates detailed descriptions, analyzes underlying causes, and provides actionable prevention strategies, setting a new standard for explainability and reliability in autonomous driving systems~\cite{nahata2021assessing,hwang2024safe,ding2024hint,li2023understanding}.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{images/AVD2.pdf} % 替换为实际图像路径
\caption{\textbf{The Framework Architecture of AVD2 system.} {\footnotesize The frame diagram demonstrates a visual language system for generating descriptions and obstacle avoidance cues from video. SwinBERT processes the video input, converts frames into video tags, and outputs descriptions and obstacle avoidance suggestions via a text generation module. The description includes the driving situation of the vehicle, and the obstacle avoidance section gives safety suggestions. Visual-language Transformer extracts text and image features and optimizes the generation with SCST.}}
\label{fig:method}
\vspace{-1em}
\end{figure*}

\subsection{Video Captioning for Autonomous Driving}

Video captioning converts video content into natural language by integrating computer vision and natural language processing. Early research focused on generating sentences with specific syntactic structures by filling recognized elements into fixed templates~\cite{inoue2023towards}, which lacked flexibility and richness. As the field progressed, sequence learning approaches were introduced, allowing for the generation of more natural sentences with flexible syntactic structures~\cite{venugopalan2015sequence}. Further developments included the use of object-level representations to capture fine-grained object-aware interactions in videos~\cite{gao2023retrieval}. More recently, Transformer-based models like SWINBERT have been introduced, employing sparse attention mechanisms to reduce redundant information and enhance the descriptive precision of video captions~\cite{wang2021end,lin2022swinbert,liu2023delving,zheng2024monoocc}.

However, current models struggle with accurately representing complex actions, especially in autonomous driving scenarios where capturing nuanced details is crucial. This limitation hampers their effectiveness in scenarios requiring detailed analysis, such as accident understanding and prevention. To address these challenges, we introduce the AVD2 framework, designed to enhance accident comprehension. AVD2 enhances the precision and contextual relevance of video captions, particularly in complex scenarios like traffic accidents, providing actionable insights for prevention.

\subsection{Video Generation}

Video generation utilizes computational techniques and models to produce video content through advanced methodologies ~\cite{vondrick2016generating,cai2018deep,huang2024vbench,chen2023pixart}. Key methods include image-to-video transformation \cite{hussain2017automatic,karras2023dreampose}, text-to-video synthesis \cite{karras2023dreampose}, and the reconfiguration of existing video materials into new segments~\cite{jiang2023text2performer}. Among these, text-to-video generation is particularly important in autonomous driving \cite{wen2024panacea,yuan2024magictime}, where creating realistic and diverse scenarios is essential for developing and testing system modules. Recent advancements in large language models (LLMs)
% \cite{lian2023llm}
, have enabled the generation of highly detailed and realistic scenes~\cite{zhang2024ctrl, xu2024diffusion, gao2024scp, li2024fairdiff, gu2025text2street}.
% ~\cite{brown2020language}
 Techniques like scenario prompt engineering, scenario generation through LLMs, and iterative evaluation feedback have been employed to improve these models~\cite{chang2024llmscenario}.
%Additionally, scoring functions are now employed to evaluate generated scenarios based on realism and uniqueness, ensuring high-quality outputs.

Despite these efforts, generating accurate and contextually appropriate accident videos remains a challenge. Previous research has made strides in video generation, yet these models often struggle with the complexity and unpredictability of real-world traffic accidents fully~\cite{tulyakov2018mocogan,pang2021image}. Moreover, the application of text-to-video generation specifically for accident scenarios is still underexplored~\cite{denton2018stochastic}. To address these gaps, we propose a novel framework that leverages Open-Sora 1.2 to generate high-fidelity accident videos. By fine-tuning pre-trained models with MM-AU raw data, our approach achieves the first successful generation of detailed and realistic accident scenarios~\cite{gupta2018social}. This innovation sets a new benchmark in autonomous driving research, significantly improving the quality and diversity of training datasets and providing more robust evaluation and training tools for autonomous driving systems.