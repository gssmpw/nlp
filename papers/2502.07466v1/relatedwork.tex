\section{Related Works}
\textbf{Stylized Image Generation}
Stylized image generation, commonly referred to as image style transfer, involves transferring the stylistic or aesthetic attributes from a reference image to a target image. 
Thanks to the significant advancements in diffusion models \citep{DDPM, SDXL, DDIM, li2024blip, rombach2022high, Classifier-Free, ramesh2022hierarchical, saharia2022photorealistic, nichol2021glide},
numerous methods \citep{sun2023sgdiff, xu2024freetuner, lu2023specialist, lin2024ctrl} have been developed to ensure style consistency across images generated. Among inversion-based approaches \citep{InST, gal2022imageworthwordpersonalizing, StyleAligned} project style images into a learnable embedding in the text token space to guide style-specific generation. Unfortunately, these methods can lead to information loss due to the mapping from visual to text modalities.
Cross-attention manipulation \citep{le2022styleid, StyleAligned, chung2024style, StyleAligned, StyleAdapter} is another method for achieving style transfer, involving the manipulation of features within self-attention layers. 
In contrast, IP-Adapter \citep{ye2023ip} and Style-Adapter \citep{StyleAdapter} introduce a distinct cross-attention mechanism that de-couples the attention layers for text and image features, allowing for a coarse control over the style transfer process. 
Although these methods have achieved significant advancements, they often struggle with content leakages from style-reference images.


\textbf{Methods Addressing Content Leakages}
Some approaches \citep{zhang2018separating, zhang2018unifiedframeworkgeneralizablestyle, qi2024deadiff} aim to tackle the content leakages issue by constructing paired datasets where images share the same subject matter but exhibit distinct styles, facilitating the extraction of disentangled style and content representations. DEADiff \citep{qi2024deadiff} stands out by extracting disentangled representations of content and style using a paired dataset, facilitated by the Q-Former \citep{li2023blip} technique.
Other works \citep{sohn2024styledrop, liu2023stylecrafter} optimize some or all model parameters using extensive style images, embedding their visual style into the model’s output domain. However, the inherently underdetermined nature of style makes the creation of large-scale paired datasets or style datasets both resource-intensive and limited in the diversity of styles it can capture. To address this issue, InstantStyle \citep{wang2024instantstyle}, a recent innovation, employs block-specific injection and feature subtraction techniques to implicitly achieve decoupling of content and style, offering a nuanced approach to style transfer. In the context of image-driven style transfer, InstantStyle-Plus \citep{wang2024instantstyle-plus} further proposed several techniques to prioritize the integrity of the content image while seamlessly integrating the target style. 
Although the InstantStyle approach achieved significant advancements, feature manipulation across different modalities inevitably introduces the image-text misalignment issue \citep{Misalign, gordon2023mismatch}, which hinders accurate disentanglement of content and style. 
StyleDiffusion \citep{wang2023stylediffusion} introduced a CLIP-based style disentanglement loss coordinated with a style reconstruction to decouple content from style in the CLIP image space. However, this framework required a training process to disentangle style from each style image, achieving this by providing approximately 50 content images for training.
DiffuseIT \citep{kwon2022diffusion} introduced a novel diffusion-based unsupervised image translation method for decoupling content from style, but it also requires complex loss regularization.
\textcolor{black}{More recent and stronger models, such as RB-Modulation \citep{rout2024rb}, have been proposed to alleviate the content leakage problem. RB-Modulation uses attention-based feature aggregation and different descriptors to decouple content and style. It is training-free and is reported to outperform InstantStyle. CSGO \citep{xing2024csgo} is another recent approach that employs a separately trained style projection layer to mitigate content leakage.}
Additionally, Zhao et al. \citep{zhao2024identifying} proposed a method to identify and address the issue of conditional content leakage in image-to-video (I2V) generation. Several studies \citep{motamed2023lego, huang2024learning, le2022styleid} focus on concept disentanglement, but they are not specifically aimed at style transfer.

\textcolor{black}{\textbf{Masking Mechanism in Synthesizing High-Quality Images}
Although several studies \citep{couairon2022diffedit, gao2023masked, hansen2024unified, pan2023masked, lei2023masked} have explored the effectiveness of masking mechanisms, our method differs from these approaches in several key aspects:
1) No coupled denoising processes: Our method avoids the need for two denoising processes, thus saving computational resources. For instance, the DIFFEDIT method \citep{couairon2022diffedit} requires two denoising processes—one conditioned on the query text and the other conditioned on a reference text. By contrasting the predictions of the two diffusion models, DIFFEDIT generates a mask that locates the regions needing editing to match the query text.
2) Masking in the latent space: Unlike DIFFEDIT \citep{couairon2022diffedit}, which operates on the pixel level to generate a mask highlighting the regions of the input image that need editing, our method performs masking in the latent space, bypassing pixel-level operations and patch-level manipulations.
3) Focus on content leakage in style transfer: While the MDT method \citep{gao2023masked} introduces a latent masking scheme to enhance the DPMs' ability to learn contextual relations among object semantics in an image, it focuses on predicting randomly masked tokens from unmasked ones. In contrast, our method targets content leakage in style transfer. We mask feature elements that are related to unwanted content from the style reference, guided by clustering results on the element-wise product. 
% The ``From Test to Mask'' method leverages the rich multi-modal knowledge embedded in diffusion models to perform segmentation. By comparing different correlation maps in the denoising U-Net, it generates the final segmentation mask.
}