\documentclass[twocolumn]{IEEEtran}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{color}
\usepackage{tikz}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{array} % Add this in your preamble
% \usetikzlibrary{shapes.multipart, positioning, arrows.meta}

% % Define styles
% \tikzset{
%   header/.style={rectangle, rounded corners, draw=black, thick, fill=orange!50, text width=6cm, align=center, font=\bfseries},
%   subheader/.style={rectangle, draw=black, thick, fill=cyan!20, text width=5cm, align=center},
%   leaf/.style={rectangle, draw=black, thick, fill=yellow!20, text width=4.5cm, align=center},
%   connector/.style={->, thick, >=Stealth},
% }


% \usepackage{xcolor}
% \usepackage{makecell}


\usepackage{pgfplots}
\usetikzlibrary{dateplot} % Load the library for date handling

\usepackage{pgfplots} % Load the pgfplots package
\pgfplotsset{compat=1.17} % Ensure compatibility with newer features of pgfplots
\usepackage{hyperref} 
\newcommand*{\aq}[1]{\textcolor{magenta}{AQ: #1}}
\newcommand*{\jq}[1]{\textcolor{magenta}{JQ: #1}}
\newcommand\uk[1]{\textbf{\textcolor{red}{Uk: #1}}	}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%



\title{Ethics at the Scalpel's Edge: Trustworthy Surgical Video Analysis in the Era of Foundation Models}

\title{Trustworthy Surgical Scene Understanding in the Era of Foundation Models: A Survey}

\title{Deep Learning for Surgical Scene Understanding: A Survey}

\title{Deep Learning for Surgical Scene Understanding using Endoscopic Videos: A Survey}

\title{Using Foundation AI Models for Surgical Scene Understanding in Endoscopic Videos: A Survey}

\title{Surgical Scene Understanding in the Age of Foundation AI Models: A Comprehensive Review}

\title{Surgical Scene Understanding in the Era of Foundation AI Models: A Comprehensive Review}

%\title{The Foundation Model Revolution in Surgical Scene Understanding: A Survey}


\author{Ufaq Khan, Umair Nawaz, Adnan Qayyum, Shazad Ashraf, Muhammad Bilal, Junaid Qadir%~\IEEEmembership{Staff,~IEEE,}
        % <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem  U.Khan and U.Nawaz are with the Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE. \protect \\
E-mail: ufaq.khan@mbzuai.ac.ae
% <-this % stops a space
\IEEEcompsocthanksitem A.Qayyum is with the HBKU, Education City, Qatar.
\IEEEcompsocthanksitem S.Ashraf is with the University Hospitals Birmingham, Birmingham, United Kingdom.
\IEEEcompsocthanksitem  M.Bilal is with the Birmingham City University, United Kingdom.
\IEEEcompsocthanksitem  J.Qadir is with the Qatar University, Qatar.


}
}



\begin{document}

\maketitle

\begin{abstract}
% Recent advancements in machine learning (ML) and deep learning (DL) have provided significant opportunities to enhance surgical scene understanding within the field of minimally invasive surgery (MIS). This paper provides a comprehensive survey of the state-of-the-art ML and DL technologies that are currently being integrated into the surgical workflow, focusing on their applications in surgical endoscopic video analysis. We review several core technologies including Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and the innovative Segment Anything Model (SAM), highlighting their roles in improving segmentation accuracy, instrument tracking, and phase recognition. Our analysis covers the challenges these models face, such as data variability, computational demands, and the need for model transparency. We also discuss the ethical considerations and integration hurdles of deploying these AI tools in clinical settings. The survey aims to bridge the gap between technological capabilities and clinical needs, suggesting pathways for future research to improve the adaptability, efficiency, and ethical alignment of AI applications in surgery. Our findings indicate that, while substantial progress has been made, more focused work is required to achieve seamless integration of AI technologies into existing clinical workflows. The key is to complement surgical practice by improving precision, reducing risks, and optimizing patient outcomes.





Recent advancements in machine learning (ML) and deep learning (DL), particularly through the introduction of foundational models (FMs), have significantly enhanced surgical scene understanding within minimally invasive surgery (MIS). This paper surveys the integration of state-of-the-art ML and DL technologies, including Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and foundational models like the Segment Anything Model (SAM), into surgical workflows. These technologies improve segmentation accuracy, instrument tracking, and phase recognition in surgical endoscopic video analysis. The paper explores the challenges these technologies face, such as data variability and computational demands, and discusses ethical considerations and integration hurdles in clinical settings. Highlighting the roles of FMs, we bridge the technological capabilities with clinical needs and outline future research directions to enhance the adaptability, efficiency, and ethical alignment of AI applications in surgery. Our findings suggest that substantial progress has been made; however, more focused efforts are required to achieve seamless integration of these technologies into clinical workflows, ensuring they complement surgical practice by enhancing precision, reducing risks, and optimizing patient outcomes.
\end{abstract}

\section{Introduction}
\label{s:intro}
% \uselengthunit{in}\printlength{\textwidth}
Artificial intelligence (AI) has the potential to have a profound impact in the field of surgery. This is particularly relevant in the field of ``vision'', where improving the understanding of complex surgical scenes through advanced imaging and analysis techniques can complement surgical actions \cite{liu2024evolution}. Minimally invasive surgery (MIS) has become the benchmark for advanced surgical procedures, offering numerous benefits over traditional open surgery, such as reduced trauma and quicker recovery times. These procedures rely heavily on the surgeon's ability to interpret high definition (HD) video feeds that capture the dynamic and often unpredictable environment within the surgical field of view (SFOV). The complexity of these SFOV is characterized by variable lighting conditions, motion blur from rapid instrument movements, and hampered SFOV caused by blood, intracorporeal surgical ``smoke'' or other fluids. Traditional image processing methods often struggle with this variability, which limits their effectiveness in real-time applications\cite{bohnen2020trainee}.
% \subsection{Advances in AI-Based Surgical Scene Understanding}


\begin{figure}[hbtp]
  \centering
  \includegraphics[width=\columnwidth]{Images/Surgical-Scene.png}
  %\caption{ Surgical scene understanding components, emphasizing task interconnections for enhanced training and safety.}
  \caption{{Key components in surgical scene understanding: we explore the use of Surgical AI for \textit{Tool Detection and Tracking} in Section \ref{sec:tool_recognition}, \textit{Workflow Recognition} in Section \ref{sec:workflow} and \textit{Surgical Training and Simulation} in Section} \ref{sec:surgical_training}.}
  \label{fig:scene-understanding}
\end{figure}


\begin{figure*}[h!]
  \centering
  \includegraphics[width=\textwidth]{Images/flowchart.png}
  \caption{Organization of the review paper on surgical scene understanding categorized into seven sections, detailing the evolution of deep learning models in surgical applications, advancements in surgical tool recognition, and comprehensive analysis of surgical workflows, training, and simulation. The paper provides a structured roadmap through various technical discussions, highlights relevant datasets, and concludes with insights into open issues and future research directions in the domain of surgical scene understanding.}

  % Following are the abbreviations used: Surg.: Surgical; FMs: Foundational Models; Dif: Diffusion; Ethi: Ethical; Reg.: Regulatory; Segm: Segmentation; Det.: Detection; DL: Deep Learning; Ext.: Extraction; LLFMs: Large Language Foundational Models; Vid.: Video; VLFMS: Vision Language Foundational Models.
  
  \label{fig:organization}
\end{figure*}

% \begin{figure}
% \centering
% \resizebox{.43\textwidth}{!}{ % Resize content to fit half the page width
% \begin{tikzpicture}[timeline/.style={line width=2pt, draw=blue},
%                     section/.style={font=\bfseries\Large\color{Maroon}, anchor=west, align=left, text decoration={underline}},
%                     subsection/.style={font=\bfseries\itshape\color{blue}, anchor=west, align=left},
%                     title/.style={font=\bfseries\small},
%                     description/.style={font=\footnotesize, align=left}]

%     % Draw timeline
%     \draw[timeline] (0, 1) -- (0, -17); % Adjust timeline length to align with content

%     % SECTION I: INTRODUCTION
%     \node[section] at (0.2, 0.8) {SECTION I.};
%     \node[title, anchor=west] at (0.2, 0.4) {INTRODUCTION};

%     % SECTION II: BACKGROUND AND CHALLENGES
%     \node[section] at (0.2, -0.2) {SECTION II.};
%     \node[title, anchor=west] at (0.2, -0.6) {BACKGROUND AND CHALLENGES};
%     \node[subsection, anchor=west] at (0.2, -1.2) {A. Surgical Video Analytics};
%     \node[subsection, anchor=west] at (0.2, -1.8) {B. Deep Learning Models in Surgical Video Analysis};
%     \node[subsection, anchor=west] at (0.2, -2.4) {C. Foundation Models};
%     \node[subsection, anchor=west] at (0.2, -3.0) {D. Challenges in Surgical Scene Understanding};

%     % SECTION III: MACHINE LEARNING APPLICATIONS
%     \node[section] at (0.2, -3.8) {SECTION III.};
%     \node[title, anchor=west] at (0.2, -4.2) {ML/DL APPLICATIONS IN SURGICAL TOOL RECOGNITION};
%     \node[subsection, anchor=west] at (0.2, -4.8) {A. Instrument Segmentation};
%     \node[subsection, anchor=west] at (0.2, -5.4) {B. Instrument Detection};
%     \node[subsection, anchor=west] at (0.2, -6.0) {C. Instrument Tracking};
%     \node[subsection, anchor=west] at (0.2, -6.6) {D. Instrument Pose Estimation};
%     \node[subsection, anchor=west] at (0.2, -7.2) {E. Anomaly Detection and Safety Measures};

%     % SECTION IV: SEGMENTATION USING FOUNDATION MODELS
%     \node[section] at (0.2, -8.0) {SECTION IV.};
%     \node[title, anchor=west] at (0.2, -8.4) {SEGMENTATION USING FOUNDATION MODELS};
%     \node[subsection, anchor=west] at (0.2, -9.0) {A. The Segment Anything Model (SAM)};
%     \node[subsection, anchor=west] at (0.2, -9.6) {B. Variants of SAM};
%     \node[subsection, anchor=west] at (0.2, -10.2) {C. Endoscopic and Laparoscopic Tool Detection Datasets};

%     % SECTION V: ETHICAL CONSIDERATIONS
%     \node[section] at (0.2, -11.0) {SECTION V.};
%     \node[title, anchor=west] at (0.2, -11.4) {ETHICAL AND REGULATORY CONSIDERATIONS};
%     \node[subsection, anchor=west] at (0.2, -12.0) {A. Ensuring AI Safety};
%     \node[subsection, anchor=west] at (0.2, -12.6) {B. Bias and Fairness in AI};
%     \node[subsection, anchor=west] at (0.2, -13.2) {C. Ethical Dilemmas in Automation};

%     % SECTION VI: OPEN ISSUES AND FUTURE WORK
%     \node[section] at (0.2, -14.0) {SECTION VI.};
%     \node[title, anchor=west] at (0.2, -14.4) {OPEN ISSUES AND FUTURE DIRECTIONS};
%     \node[subsection, anchor=west] at (0.2, -15.0) {A. Developing More Robust AI Models};
%     \node[subsection, anchor=west] at (0.2, -15.6) {B. Expanding AI-driven Approaches in Medicine};

%     % SECTION VII: CONCLUSIONS
%     \node[section] at (0.2, -16.4) {SECTION VII.};
%     \node[title, anchor=west] at (0.2, -16.8) {CONCLUSIONS};

% \end{tikzpicture}}

% \caption{Outline of the Paper.}
% \label{fig:outline}
% \end{figure}

AI advancements such as deep learning (DL) have opened new possibilities for understanding surgical scenes. Unlike traditional methods that rely on predefined features, these AI models learn to recognize patterns and features directly from data, improving their ability to generalize across different settings. These capabilities are crucial for tasks such as distinguishing anatomical structures from surgical tools, navigate obscured views, and predict the presence of abnormalities or complications in real time \cite{maier2017surgical}. In this context, the precision and accuracy of imaging and visualization technologies become paramount.

The development of state-of-the-art imaging techniques has significantly improved the ability of MIS surgeons to plan and execute complex surgeries with greater confidence and improved outcomes \cite{zaffino2020review}. A crucial component of these advances is medical image segmentation, which involves partitioning digital images into distinct regions that represent different tissues, structures, or devices. Segmentation is essential for numerous medical applications, allowing improved visualization, perception, and interpretation of complex anatomical and pathological information \cite{cai2020review, siddique2021u, wang2022medical}. In surgical contexts, effective segmentation ensures that surgical professionals accurately identify and differentiate between various anatomical features and surgical tools, thereby prompting the surgeon to precisely dissect in correct surgical planes thus avoiding collateral damage to adjacent structures. For example, the left ureter can sometimes be damaged when releasing the sigmoid colon off the retroperitoneal structures in rectal cancer surgery (anterior resection). This focus on developing surgical safety systems, analogous to   ``satellite navigation'', has the potential to highlight potential hazards, minimize complications and therefore improve patient outcomes.



% Segmentation is just one part of the broader field of surgical scene understanding. Other critical components include:
%The following are the major tasks involved in surgical scene understanding (as shown in Fig.~\ref{fig:scene-understanding}). 

The key tasks of surgical scene understanding as shown in Fig.~\ref{fig:scene-understanding}, are outlined next.

\begin{itemize}
    \item \textit{Surgical Tool and Object Detection and Tracking}: Identifying anatomical objects and following the movement of instruments within the surgical field provides context-aware assistance and ensures patient safety. For example, identifying the proximity of a surgical instrument close to a blood vessel or adjacent organ would enhance situational awareness within the surgical field \cite{bouget2017vision}.
    
    \item \textit{Surgical Workflow Recognition}:  Identifying different phases of a procedure enables real-time documentation, skill assessment, and context-aware assistance \cite{zia2018surgical}. Additionally, recognizing surgeon gestures and actions supports training, performance evaluation, and automation. This serves as a foundation for converting surgical videos into structured formats, enabling large-scale indexing and advanced data analysis \cite{yengera2018less}.
    
    % \item \textit{Action Recognition}: Understanding the surgeon's gestures and actions can be used for training, assessment, and automation purposes \cite{yengera2018less}. \aq{actions can overlap with the workflow recognition, please see if we need to describe it separately, how literature defines it? }
    
    \item \textit{Surgical Training \& Simulation}: Understanding the surgeon's gestures and actions can be used for training, assessment, and automation purposes \cite{yengera2018less}.  Detecting unexpected events (for example, bleeding or damage to adjacent tissue or organs) \cite{georgescu2021anomaly}, generating synthetic videos \cite{li2024endora}, and performing question-answering feedback tasks \cite{bai2025surgical} can also lead to enhanced surgical experience from the perspective of clinicians.  
\end{itemize}


% \subsection{Core Components of Surgical Scene Understanding}
% These components are essential, especially in minimally invasive surgery \cite{omisore2020review}, where direct visualization is replaced by imaging technology. Real-time segmentation and identification on video feeds influence the precision of surgical maneuvers and procedure safety \cite{twinanda2017endonet}. Previously, these tasks were performed manually by radiologists, making the process labor-intensive and prone to variability due to human interpretation \cite{ihdayhid2020influence, soomro2022image}. The advent of computer-assisted technologies has standardized and objectified these tasks, originally handled by rule-based or statistical methods with limited flexibility \cite{shehab2022machine}.




% \section{Introduction}

% Artificial Intelligence (AI) has profoundly impacted the field of surgery, particularly in enhancing the understanding of complex surgical scenes through advanced imaging and analysis techniques \cite{liu2024evolution}. The precision required in surgeries, especially in minimally invasive procedures, demands an acute interpretation of the surgical environment, which AI is increasingly capable of providing. Minimally invasive surgery (MIS) has become the benchmark for advanced surgical procedures, offering numerous benefits over traditional open surgery, such as reduced trauma and quicker recovery times. These procedures rely heavily on the surgeon's ability to interpret endoscopic video feeds that capture the dynamic and often unpredictable environment within the surgical site. The complexity of these videos is characterized by variable lighting conditions, motion blur from rapid instrument movements, and occlusions caused by blood or other fluids. Traditional image processing methods often struggle with such variability, limiting their effectiveness in real-time applications\cite{bohnen2020trainee}.
% The advent of deep learning has led to a new era of possibilities for such surgical scene understanding. Unlike traditional methods that rely on predefined features, AI models, particularly those driven by deep learning, learn to recognize patterns and features directly from data, improving their ability to generalize across different settings. These capabilities are crucial for tasks such as distinguishing anatomical structures from surgical tools, navigating through obscured views, and predicting the presence of abnormalities or complications\cite{maier2017surgical}.
% In this context, the precision and accuracy of imaging and visualization technologies become paramount. The beginning of state-of-the-art imaging techniques has significantly enhanced the ability of surgeons to plan and execute complex surgeries with greater confidence and improved outcomes \cite{zaffino2020review}. A crucial component of these advancements is medical image segmentation, which involves partitioning digital images into distinct regions representing different tissues, structures, or devices. Segmentation is essential for numerous medical applications, enabling enhanced visualization and interpretation of complex anatomical and pathological information \cite{cai2020review,siddique2021u,wang2022medical}. In surgical contexts, effective segmentation ensures that medical practitioners accurately identify and differentiate between various anatomical features and surgical tools, thereby facilitating precise interventions and improving patient outcomes.



% However, segmentation is only one aspect of surgical scene understanding. Other critical components include surgical tool detection and tracking, surgical workflow recognition, action recognition, and anomaly detection. Surgical tool detection and tracking involve identifying and following the movement of instruments within the surgical field, which is essential for providing context-aware assistance and ensuring patient safety \cite{bouget2017vision}. Surgical workflow recognition aims to automatically identify the different phases or steps of a surgical procedure, enabling applications such as automated documentation, skill assessment, and context-aware assistance \cite{zia2018surgical}. Action recognition focuses on understanding the surgeon's gestures and actions, which can be used for training, assessment, and automation purposes \cite{yengera2018less}. Anomaly detection involves identifying unexpected events or complications during surgery, such as bleeding or tissue damage, allowing for timely interventions and improved patient safety \cite{georgescu2021anomaly}.
% %%%%%%%%%%%%%%%%%%%%%%




% The significance of these components is particularly pronounced in the field of minimally invasive surgery (MIS) \cite{omisore2020review}, where the surgeon's direct visualization of the operative field is replaced by imaging technology. In these scenarios, the ability to accurately segment and identify tools and tissues on a real-time video feed is essential, as it directly influences the precision of surgical maneuvers and the safety of the procedure \cite{twinanda2017endonet}. Enhanced segmentation techniques improve the efficacy of these surgeries by providing more precise, well-detailed visual information, which in turn reduces operative risks and optimizes outcomes. Previously, tasks such as segmentation, tool detection, and workflow recognition were predominantly performed manually by skilled radiologists, a process that was both labor-intensive and prone to variability due to human interpretation \cite{ihdayhid2020influence,soomro2022image}. The advent of computer-assisted technologies has revolutionized this practice, introducing more standardized and objective methods of image analysis. Initial computational approaches were rule-based or utilized simple statistical methods and were limited by their inflexibility and the high degree of variability in medical images \cite{shehab2022machine}.

% The integration of more recent deep learning techniques has further transformed the landscape of surgical scene understanding \cite{wang2022medical}. These approaches leverage large datasets of annotated images and videos to learn complex patterns and features, enabling the automated segmentation of medical images, detection of surgical tools, recognition of surgical phases, and detection of anomalies with high accuracy. Deep learning models, particularly Convolutional Neural Networks (CNNs), have become the standard in the field due to their ability to process spatial hierarchies in images and videos, making them highly effective for tasks involving visual data \cite{litjens2017survey}. The emergence of advanced models such as Transformers \cite{dosovitskiy2021imageworth16x16words}, which avoid the localized receptive fields of CNNs for global context via self-attention mechanisms, represented the next evolution in surgical scene understanding. These models address some of the limitations of CNNs, including their inadequate handling of large context areas and complex spatial relationships, which are prevalent in medical imaging \cite{dosovitskiy2020image}.


% Furthermore, the development of models like the Segment Anything Model (SAM) \cite{kirillov2023segment} underscores a move towards more adaptable and generalized approaches in medical image segmentation. SAM and similar models are designed to handle a variety of imaging conditions and segmentation tasks without extensive retraining, making them particularly valuable in dynamic surgical environments where conditions and requirements can change rapidly \cite{zhou2019fast}. Advancements in segmentation technology not only enhance current medical practices but also pave the way for innovative surgical techniques and treatments. As accuracy in segmentation, tool detection, and workflow recognition improves, it enables the development of augmented reality (AR) systems for surgery \cite{jud2020applicability}. These AR systems can project segmented images and contextual information directly onto the patient's body during surgery, providing real-time, context-sensitive information that enhances the surgeon's spatial understanding and precision \cite{kunz2022augmented}.


% \begin{table*}[ht]
% % Can also add multi-modalities? 

% \centering
% \caption{Comparative performance and limitations of existing DL-based medical image segmentation survey papers from recent years.}
% \label{tab:comparison}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}c>{\raggedright\arraybackslash}p{2.5cm}cc>{\raggedright\arraybackslash}p{2.5cm}>{\raggedright\arraybackslash}p{2.5cm}>{\raggedright\arraybackslash}p{2.5cm}c>{\raggedright\arraybackslash}p{2.5cm}>{\raggedright\arraybackslash}p{5cm}@{}}



% \toprule


% Reference & Scope of Review & Surgical Procedures Detail & Surgical Dataset Coverage & Methodology of Review & Algorithm Detailing & Performance Metrics & Results Analysis & Future Directions Suggested & Unique Contributions \\ \midrule


% Rivas-Blanco et al. \cite{rivas2021review} & DL models in minimally invasive surgery & \xmark & \checkmark & Systematic literature search & CNN, RNN, ANN, HMM & Varied metrics & \xmark & Integration of different DL models & Deep insights into automation of surgical tasks \\

% Fu et al. \cite{fu2021future} & Endoscopic navigation technologies & \xmark & \checkmark & Broad literature survey & Augmented reality, various imaging modalities & Varied, not specified & \checkmark & Challenges in integrating advanced imaging techniques & Detailed exploration of endoscopic vision technologies \\


% Rueckert et al. \cite{rueckert2024methods} & Instrument segmentation in surgery & \checkmark & \checkmark & Systematic search & Deep learning, CNN & Accuracy, precision & \checkmark & Spatial-temporal segmentation & Comprehensive review on surgical instrument segmentation \\


% Zhang et al. \cite{zhang2024segment} & SAM in medical image segmentation & \xmark & \xmark & Comprehensive literature review & SAM, foundation models & Accuracy, adaptability & \xmark & Application of SAM to diverse medical scenarios & Exploration of SAM's extension to medical segmentation \\


% Azad et al. \cite{azad2024medical} & Evolution and success of U-Net in medical imaging & \xmark & \checkmark & Thorough review & U-Net and its variants & Varied, comprehensive & \checkmark & Adaptation of U-Net to various medical modalities & In-depth analysis of U-Net variants across modalities \\


% Schmidt et al. \cite{schmidt2024tracking} & Tracking and mapping in medical CV & \xmark & \checkmark & Detailed literature review & Nonrigid tracking, SLAM & Precision, robustness & \checkmark & Challenges in nonrigid environments & Insightful review of tracking/mapping in deformable tissues \\


% Upadhyay et al. \cite{upadhyay2024advances} & DL methods to overcome data scarcity & \xmark & \checkmark & Systematic literature search & CNN, U-Net, GAN & Various & \checkmark & Emphasizes semi-supervised learning & Extensive review on overcoming data scarcity \\

% \vspace{0.5mm}
% Zhou et al. \cite{zhou2020application} & AI in surgery integration & \checkmark & \checkmark & Narrative review & CNN, RNN & Accuracy, Sensitivity & \checkmark & Focus on autonomous robotic surgery & Comprehensive integration of AI in surgery \\

% \vspace{0.5mm}
% Morris et al. \cite{morris2023deep} & DL applications in sub-specialties & \xmark & \xmark & Expert opinion & CNN, RNN & Efficiency, Performance & \xmark & Calls for real-time decision-making tools & Insights into practical DL applications in surgery \\

% \vspace{0.5mm}
% Li et al. \cite{li2024deep} & Surgical workflow recognition & \checkmark & \checkmark & Comprehensive bibliographic databases search & CNN, LSTM & Accuracy, Real-time performance & \checkmark & Proposes large language models as assistants & Detailed surgical workflow recognition analysis \\

% \vspace{0.5mm}
% Garrow et al. \cite{garrow2021machine} & Automated phase recognition & \xmark & \xmark & Systematic review according to PRISMA & HMM, ANN & Accuracy, Real-time feedback & \xmark & Advocates for standardization in phase definition & Highlight on phase recognition automation in surgery \\



% This Paper & \textcolor{red}{TBA} & \checkmark & \checkmark & \textcolor{red}{TBA} & \textcolor{red}{TBA} & \textcolor{red}{TBA} & \checkmark & \textcolor{red}{TBA} & \textcolor{red}{TBA} \\ 

% \bottomrule
% \end{tabular}
%  }
% % \end{table}
% \end{table*}



% Moreover, the integration of surgical scene understanding with robotic surgery systems represents another frontier. Robotic systems equipped with advanced algorithms can perform complex surgical tasks with enhanced precision and control, reducing human error and patient recovery times \cite{penza2017envisors}. This integration optimizes surgical outcomes and minimizes the invasiveness of procedures, leading to quicker patient recovery and less postoperative pain. Additionally, the data generated from surgical scene understanding contribute to the growing field of surgical data science, which aims to improve the quality and outcomes of surgery through data-driven methods and insights. This interdisciplinary field uses data such as segmented images, tool trajectories, and surgical phase annotations to develop predictive models and decision-support systems to anticipate complications and suggest optimal surgical approaches \cite{maier2022surgical}. 

% As machine learning and computational power continue to advance, the future of medical image segmentation looks promising. Techniques requiring significant computational resources may become more accessible, enabling real-time analysis during surgeries in varied clinical settings \cite{nespolo2023feature}. This progression will likely lead to more personalized surgical interventions tailored to the unique anatomical features of each patient, thereby maximizing the effectiveness of treatments and minimizing risks.
% This paper provides a comprehensive review of foundational AI models and their application in surgical scene understanding, with a particular focus on interpreting endoscopic video data for minimally invasive surgery (MIS). By exploring the integration of advanced AI techniques, this review highlights the transformative impact of these technologies on surgical practice, aiming to improve accuracy and outcomes in MIS. The review also delves into the evolution and current landscape of various techniques within medical imaging, including segmentation, tool detection, workflow recognition, and anomaly detection, specifically their utility in surgical environments. It traces the historical development of these technologies and examines contemporary advancements, emphasizing their critical role in augmenting surgical precision and improving patient outcomes.




% \subsection{Related Surveys}

% In the rapidly growing area of deep learning (DL) applications in medical imaging and surgery, many reviews have sought to summarize progress and pinpoint emerging trends. This section pulls together insights from these reviews to create a context for our own work, outlining the main focus areas and methods that dominate the field. Rivas-Blanco et al. \cite{rivas2021review} concentrate on DL models specifically designed for minimally invasive surgery, highlighting the integration of multiple DL frameworks such as CNNs, Recurrent Neural Networks (RNNs), and Hidden Markov Models (HMMs) and offering profound insights into the automation of surgical tasks. Contrarily, Fu et al. \cite{fu2021future} explore the specialized domain of endoscopic navigation technologies, emphasizing the challenges related to the integration of augmented reality and various imaging modalities to enhance navigational efficacy in endoscopic procedures. Focusing on the segmentation of surgical instruments, Rueckert et al. \cite{rueckert2024methods} deliver a systematic search that encapsulates the accuracy and precision of instrument detection, underscoring the value of spatial-temporal segmentation in surgical procedures. Zhang et al. \cite{zhang2024segment} explore the applicability of the Segment Anything Model (SAM) to medical image segmentation, critically assessing its adaptability across diverse medical scenarios.

% In a recent survey, Azad et al. \cite{azad2024medical} trace the evolution and widespread adoption of the U-Net architecture across various medical imaging modalities. Their comprehensive analysis not only covers the architectural enhancements but also extends to practical applications and variant developments. Similarly, Schmidt et al. \cite{schmidt2024tracking} delve into the complexities of tracking and mapping in medical computer vision, particularly in deformable environments such as soft tissue and organs, which are pivotal for advancing surgical automation and precision.

% This survey builds upon these foundational works, aiming to carve a niche by focusing on specific aspects, e.g., the integration of DL models in live surgical environments. Unlike existing surveys, this paper integrates recent advancements in surgical scene understanding, offering a forward-looking perspective that emphasizes specific future directions, e.g., real-time decision-making tools in surgery. Our analysis not only highlights current applications but also sets the stage for upcoming innovations that promise to redefine operational workflows and patient outcomes in surgical practices.


% \subsection{Organization of This Paper}

% \jq{TODO Write this and refer to \cite{fig:tba}.}




% \begin{tikzpicture}[node distance=1.5cm, level distance=1.5cm]

% % Root Node
% \node[header] (root) {Paper Organization};

% % Level 1 Sections
% \node[header, below=of root, xshift=-10cm] (sec2) {Sec. II \\ Background};
% \node[header, below=of root, xshift=-6cm] (sec3) {Sec. III \\ Surgical Video Analytics};
% \node[header, below=of root, xshift=-2cm] (sec4) {Sec. IV \\ Instrument Understanding};
% \node[header, below=of root, xshift=2cm] (sec5) {Sec. V \\ Workflow Analysis};
% \node[header, below=of root, xshift=6cm] (sec6) {Sec. VI \\ Scene Understanding};
% \node[header, below=of root, xshift=10cm] (sec7) {Sec. VII \\ Available Datasets};
% \node[header, below=of root, xshift=14cm] (sec8) {Sec. VIII \\ Open Issues and Future Research Directions};
% \node[header, below=of root, xshift=18cm] (sec9) {Sec. IX \\ Conclusion};

% % Level 2: Subsections
% % Section II
% \node[subheader, below=of sec2] (sec2a) {Deep Learning Models};
% \node[leaf, below=of sec2a] (sec2b) {Convolutional Neural Networks};
% \node[leaf, below=of sec2b] (sec2c) {Segmentation Models};
% \node[leaf, below=of sec2c] (sec2d) {Vision Transformers (ViTs)};
% \node[leaf, below=of sec2d] (sec2e) {Diffusion Models};

% \node[subheader, right=1.5cm of sec2a] (sec2f) {Foundation Models: Background};
% \node[leaf, below=of sec2f] (sec2g) {Segment Anything Model (SAM)};
% \node[leaf, below=of sec2g] (sec2h) {Surgical-DeSAM};
% \node[leaf, below=of sec2h] (sec2i) {AdaptiveSAM};

% % Section III
% \node[subheader, below=of sec3] (sec3a) {Challenges in Surgical Scene Understanding};
% \node[leaf, below=of sec3a] (sec3b) {Technical Challenges};
% \node[leaf, below=of sec3b] (sec3c) {Ethical, Regulatory, Privacy};

% \node[subheader, right=1.5cm of sec3a] (sec3d) {Applications of ML};
% \node[leaf, below=of sec3d] (sec3e) {Overview of Key ML Applications};
% \node[leaf, below=of sec3e] (sec3f) {Taxonomy of ML Applications};

% % Section IV
% \node[subheader, below=of sec4] (sec4a) {Instrument Segmentation};
% \node[leaf, below=of sec4a] (sec4b) {Min-max Similarity};

% \node[subheader, below=of sec4a] (sec4c) {Instrument Detection};
% \node[leaf, below=of sec4c] (sec4d) {Traditional Tracking};

% % Section V
% \node[subheader, below=of sec5] (sec5a) {Anomaly Detection};
% \node[leaf, below=of sec5a] (sec5b) {Complication Detection};

% \node[subheader, right=1.5cm of sec5a] (sec5c) {Phase Recognition};
% \node[leaf, below=of sec5c] (sec5d) {Real-Time CNN};

% % Section VI
% \node[subheader, below=of sec6] (sec6a) {Surgical Video Summarization};
% \node[leaf, below=of sec6a] (sec6b) {Automatic Summarization};

% \node[subheader, right=1.5cm of sec6a] (sec6c) {Anatomical Recognition};
% \node[leaf, below=of sec6c] (sec6d) {Unsupervised Anomaly Detection};

% % Section VII
% \node[subheader, below=of sec7] (sec7a) {Robotic Surgery Datasets};
% \node[leaf, below=of sec7a] (sec7b) {ATLAS Dione};

% % Section VIII
% \node[subheader, below=of sec8] (sec8a) {Open Issues};
% \node[leaf, below=of sec8a] (sec8b) {Data Constraints};
% \node[leaf, below=of sec8b] (sec8c) {Integration with Clinical Workflows};

% % Section IX
% \node[leaf, below=of sec9] (sec9a) {Conclusion};

% % Connections: Level 1
% \draw[connector] (root) -- (sec2);
% \draw[connector] (root) -- (sec3);
% \draw[connector] (root) -- (sec4);
% \draw[connector] (root) -- (sec5);
% \draw[connector] (root) -- (sec6);
% \draw[connector] (root) -- (sec7);
% \draw[connector] (root) -- (sec8);
% \draw[connector] (root) -- (sec9);

% % Section II connections
% \draw[connector] (sec2) -- (sec2a);
% \draw[connector] (sec2a) -- (sec2b);
% \draw[connector] (sec2b) -- (sec2c);
% \draw[connector] (sec2f) -- (sec2g);

% % Section VIII connections
% \draw[connector] (sec8) -- (sec8a);
% \draw[connector] (sec8a) -- (sec8b);

% \end{tikzpicture}

% \begin{tikzpicture}[node distance=0.8cm, every node/.style={font=\small}]

% % Root Node
% \node[header] (root) {Paper Organization};

% % Left Column: Sections II to V
% \node[header, below=1.5cm of root, anchor=west, xshift=-4.5cm] (sec2) {Sec. II \\ Background};
% \node[header, below=of sec2] (sec3) {Sec. III \\ Surgical Video Analytics};
% \node[header, below=of sec3] (sec4) {Sec. IV \\ Instrument Understanding};
% \node[header, below=of sec4] (sec5) {Sec. V \\ Workflow Analysis};

% % Right Column: Sections VI to IX
% \node[header, below=1.5cm of root, anchor=east, xshift=4.5cm] (sec6) {Sec. VI \\ Scene Understanding};
% \node[header, below=of sec6] (sec7) {Sec. VII \\ Available Datasets};
% \node[header, below=of sec7] (sec8) {Sec. VIII \\ Open Issues and Research Directions};
% \node[header, below=of sec8] (sec9) {Sec. IX \\ Conclusion};

% % Subsections for Sec. II
% \node[subheader, right=0.8cm of sec2] (sec2a) {Deep Learning Models};
% \node[leaf, below=0.3cm of sec2a] (sec2b) {Convolutional Neural Networks};
% \node[leaf, below=0.3cm of sec2b] (sec2c) {Segmentation Models};
% \node[leaf, below=0.3cm of sec2c] (sec2d) {Vision Transformers (ViTs)};

% % Subsections for Sec. III
% \node[subheader, right=0.8cm of sec3] (sec3a) {Challenges in Surgical Scene Understanding};
% \node[leaf, below=0.3cm of sec3a] (sec3b) {Technical Challenges};
% \node[leaf, below=0.3cm of sec3b] (sec3c) {Ethical, Regulatory, Privacy Issues};

% % Subsections for Sec. VI
% \node[subheader, left=0.8cm of sec6] (sec6a) {Anatomical Recognition};
% \node[leaf, below=0.3cm of sec6a] (sec6b) {Anomaly Detection};
% \node[subheader, below=0.3cm of sec6b] (sec6c) {Surgical Video Summarization};

% % Subsections for Sec. VIII
% \node[subheader, left=0.8cm of sec8] (sec8a) {Open Issues};
% \node[leaf, below=0.3cm of sec8a] (sec8b) {Data Constraints};
% \node[leaf, below=0.3cm of sec8b] (sec8c) {Integration with Clinical Workflows};
% \node[leaf, below=0.3cm of sec8c] (sec8d) {Ethical Considerations};

% % Connections: Root to Sections
% \draw[connector] (root) -| (sec2);
% \draw[connector] (root) -| (sec6);

% % Left Column Connections
% \draw[connector] (sec2) -- (sec2a);
% \draw[connector] (sec3) -- (sec3a);

% % Right Column Connections
% \draw[connector] (sec6) -- (sec6a);
% \draw[connector] (sec8) -- (sec8a);

% \end{tikzpicture}





\begin{figure*}[hbtp]
  \centering
  \includegraphics[width=\textwidth]{phases.png}
  \caption{{An in-depth examination of the intricate tasks involved in navigating and controlling surgical tools, highlighting the technical and operational processes that enable precision and effectiveness in modern surgical procedures}}
  \label{fig:tool-centric}
\end{figure*}


\subsection{Comparison with Related Surveys}
DL applications in medical imaging and surgery have been extensively reviewed, with numerous studies summarizing advancements and identifying emerging trends. This survey provides a different perspective  by focusing on the predominant areas and methodologies shaping the domain of understanding surgical video scenes. In Table \ref{tab:comparisonrelated}, we present a comprehensive comparison of this article with existing surveys and review articles that have a similar focus. Specifically, a comparison is provided in terms of scope, procedural coverage, datasets utilized, methodological approaches, algorithmic details, performance metrics, and unique contributions.

% Rivas et al. \cite{rivas2021review} focused on DL architectures tailored for minimally invasive surgeries, integrating CNNs, RNNs, and HMMs to facilitate surgical task automation. Fu et al. \cite{fu2021future} examined challenges in adopting augmented reality and diverse imaging modalities within endoscopic navigation. Rueckert et al. \cite{rueckert2024methods} presented a systematic review on the precision of instrument segmentation, advocating for the importance of spatial-temporal approaches in surgical contexts. Zhang et al. \cite{zhang2024segment} critically evaluated the flexibility of the SAM across various medical scenarios. Furthermore, Azad et al. \cite{azad2024medical} focused on the evolution of U-Net across different imaging modalities, discussing architectural improvements and application breadth. Schmidt et al. \cite{schmidt2024tracking} explored tracking and mapping technologies in deformable surgical environments, crucial for enhancing surgical automation and precision.

% Building upon these foundational insights, our survey uniquely focuses on DL model integration within live surgical settings, underscoring real-time decision-making tools that could potentially revolutionize surgical workflows and improve patient outcomes. \aq{We need to revise this and provide a more detailed description of how our paper is unique from existing articles.}

Compared to the existing literature, this survey adopts a more comprehensive and integrative approach and attempts to address various methodologies and applications within the domain of understanding the surgical scene. Previous works, such as those of Rivas et al. \cite{rivas2021review} and Rueckert et al. \cite{rueckert2024methods}, have predominantly focused on specific tasks such as automation in MIS and instrument segmentation, respectively. In contrast, this survey expands the focus to encompass a broader spectrum of challenges, including segmentation, tracking, and workflow recognition. Moreover, unlike the studies by Fu et al. \cite{fu2021future} and Azad et al. \cite{azad2024medical} that mainly focuses on endoscopic navigation and U-Net variants, respectively, and explore individual methodologies, this paper emphasizes the applicability of advanced models such as Vision Transformers (ViTs), Large Vision-Language Models (LVLMs), and foundation models like SAM. These models are particularly highlighted for their potential to assist and enable real-time decision making and address the complexities associated with multimodal data. Furthermore, based on the work of Li et al. \cite{li2024deep}, this survey enhances existing data set analyzes and presents an updated and detailed catalog specifically tailored for surgical scene understanding tasks. By synthesizing these contributions, this survey not only consolidates current knowledge but also identifies key trends, such as the integration of multimodal AI and foundation models for surgical applications. It also underscores the pressing need for real-time decision-making tools that meet the dynamic and  ``high-stake'' demands of live surgical environments, providing a robust framework to advance AI-driven innovations in surgical practice.





\begin{table*}[!h]
\centering
\caption{Comparative performance and limitations of DL-based medical image segmentation surveys.}
\label{tab:comparisonrelated}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}p{1.8cm}p{2.5cm}p{2.5cm}p{2cm}p{2cm}p{2.5cm}p{1.5cm}p{1.8cm}p{1.5cm}p{2cm}p{2cm}p{2cm}p{1cm}@{}}
\toprule
\textbf{Reference} & \textbf{Scope} & \textbf{Contributions} & \textbf{Algorithms} & \textbf{Generative Models} & \textbf{Vision-Language Models} & \textbf{Foundational Models} & \textbf{Ethical and Regulatory Insights} & \textbf{Dataset Coverage} & \textbf{Comprehensive Surgical Tasks} & \textbf{Foundational Models Adaptation} & \textbf{Clinical Perspective} & \textbf{Open Issues} \\ \midrule

Rivas-Blanco et al. \cite{rivas2021review} & DL models in minimally invasive surgery & Deep insights into automation of surgical tasks & CNN, RNN, ANN, HMM & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \centering\xmark & \centering\xmark & \centering\xmark & \xmark \\

Fu et al. \cite{fu2021future} & Endoscopic navigation technologies & Detailed exploration of endoscopic vision technologies & Augmented reality, various imaging modalities & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \xmark \\

Rueckert et al. \cite{rueckert2024methods} & Instrument segmentation in surgery & Comprehensive review on surgical instrument segmentation & Deep learning, CNN & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \centering\checkmark & \centering\xmark & \centering\xmark & \xmark \\

Zhang et al. \cite{zhang2024segment} & SAM in medical image segmentation & Exploration of SAM's extension to medical segmentation & SAM, foundation models & \centering\xmark & \centering\xmark & \centering\checkmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \centering\checkmark & \checkmark \\

Azad et al. \cite{azad2024medical} & Evolution and success of U-Net in medical imaging & In-depth analysis of U-Net variants across modalities & U-Net and its variants & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \centering\xmark & \centering\xmark & \centering\checkmark & \checkmark \\

Schmidt et al. \cite{schmidt2024tracking} & Tracking and mapping in medical CV & Insightful review of tracking/mapping in deformable tissues & Nonrigid tracking, SLAM & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \centering\xmark & \centering\xmark & \centering\checkmark & \checkmark \\

Upadhyay et al. \cite{upadhyay2024advances} & DL methods to overcome data scarcity & Extensive review on overcoming data scarcity & CNN, U-Net, GAN & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \centering\checkmark & \centering\xmark & \centering\xmark & \centering\xmark & \xmark \\

Zhou et al. \cite{zhou2020application} & AI in surgery integration & Comprehensive integration of AI in surgery & CNN, RNN & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \centering\xmark & \centering\xmark & \centering\checkmark & \checkmark \\

Morris et al. \cite{morris2023deep} & DL applications in sub-specialties & Insights into practical DL applications in surgery & CNN, RNN & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \xmark \\

Li et al. \cite{li2024deep} & Surgical workflow recognition & Detailed surgical workflow recognition analysis & CNN, LSTM & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \centering\checkmark & \centering\xmark & \centering\xmark & \centering\checkmark & \checkmark \\

Garrow et al. \cite{garrow2021machine} & Automated phase recognition & Highlight on phase recognition automation in surgery & HMM, ANN & \centering\xmark & \centering\xmark & \centering\xmark & \centering\xmark & \centering\checkmark & \centering\xmark & \centering\xmark & \centering\xmark & \xmark \\

This Paper & Surgical Scene Understanding & Detailed model review, applications overview, dataset analysis & CNNs, ViTs, LVLMs, GANs, FMs & \centering\checkmark & \centering\checkmark & \centering\checkmark & \centering\checkmark & \centering\checkmark & \centering\checkmark & \centering\checkmark & \centering\checkmark & \checkmark \\

\bottomrule
\end{tabular}
}
\end{table*}





% \begin{table*}[!h]
% \centering
% \caption{\textbf{Comparative performance and limitations of DL-based medical image segmentation surveys.}}
% \label{tab:comparisonrelated}
% \resizebox{\textwidth}{!}{%
% \begin{tabular} {@{}p{2.5cm}p{2.5cm}p{3.5cm}p{2.5cm}p{2cm}p{2.5cm}p{1.5cm}p{1.8cm}p{2.5cm}p{3.5cm}@{}}
% \toprule
% \textbf{Reference} & \textbf{Scope} & \textbf{Contributions} & \textbf{Methodology} & \textbf{Algorithms} & \textbf{Metrics} & \textbf{Foundational Models} & \textbf{Ethical Consideration} & \textbf{Dataset Coverage} & \textbf{Future Work} \\ \midrule

% Rivas-Blanco et al. \cite{rivas2021review} & DL models in minimally invasive surgery & Deep insights into automation of surgical tasks & Systematic literature search & CNN, RNN, ANN, HMM & Varied metrics & \centering\xmark & \centering\xmark & \centering\checkmark & Integration of different DL models \\

% Fu et al. \cite{fu2021future} & Endoscopic navigation technologies & Detailed exploration of endoscopic vision technologies & Broad literature survey & Augmented reality, various imaging modalities & Varied, not specified & \centering\xmark & \centering\checkmark & \centering\checkmark & Challenges in integrating advanced imaging techniques \\

% Rueckert et al. \cite{rueckert2024methods} & Instrument segmentation in surgery & Comprehensive review on surgical instrument segmentation & Systematic search & Deep learning, CNN & Accuracy, precision & \centering\xmark & \centering\checkmark & \centering\checkmark & Spatial-temporal segmentation \\

% Zhang et al. \cite{zhang2024segment} & SAM in medical image segmentation & Exploration of SAM's extension to medical segmentation & Comprehensive literature review & SAM, foundation models & Accuracy, adaptability & \centering\xmark & \centering\xmark & \centering\xmark & Application of SAM to diverse medical scenarios \\

% Azad et al. \cite{azad2024medical} & Evolution and success of U-Net in medical imaging & In-depth analysis of U-Net variants across modalities & Thorough review & U-Net and its variants & Varied, comprehensive & \centering\xmark & \centering\checkmark & \centering\checkmark & Adaptation of U-Net to various medical modalities \\

% Schmidt et al. \cite{schmidt2024tracking} & Tracking and mapping in medical CV & Insightful review of tracking/mapping in deformable tissues & Detailed literature review & Nonrigid tracking, SLAM & Precision, robustness & \centering\xmark & \centering\checkmark & \centering\checkmark & Challenges in nonrigid environments \\

% Upadhyay et al. \cite{upadhyay2024advances} & DL methods to overcome data scarcity & Extensive review on overcoming data scarcity & Systematic literature search & CNN, U-Net, GAN & Various & \centering\xmark & \centering\checkmark & \centering\checkmark & Emphasizes semi-supervised learning \\

% Zhou et al. \cite{zhou2020application} & AI in surgery integration & Comprehensive integration of AI in surgery & Narrative review & CNN, RNN & Accuracy, Sensitivity & \centering\xmark & \centering\checkmark & \centering\checkmark & Focus on autonomous robotic surgery \\

% Morris et al. \cite{morris2023deep} & DL applications in sub-specialties & Insights into practical DL applications in surgery & Expert opinion & CNN, RNN & Efficiency, Performance & \centering\xmark & \centering\xmark & \centering\xmark & Calls for real-time decision-making tools \\

% Li et al. \cite{li2024deep} & Surgical workflow recognition & Detailed surgical workflow recognition analysis & Comprehensive bibliographic databases search & CNN, LSTM & Accuracy, Real-time performance & \centering\xmark & \centering\checkmark & \centering\checkmark & Proposes large language models as assistants \\

% Garrow et al. \cite{garrow2021machine} & Automated phase recognition & Highlight on phase recognition automation in surgery & Systematic review according to PRISMA & HMM, ANN & Accuracy, Real-time feedback & \centering\xmark & \centering\xmark & \centering\xmark & Advocates for standardization in phase definition \\

% This Paper & Surgical Scene Understanding & Detailed model review, applications overview, dataset analysis & Literature Survey of SOTA solutions & CNNs, ViTs, LVLMs, GANs, SAM & Variety of metrics & \centering\checkmark & \centering\checkmark & \centering\checkmark & Highlighted future trends \\

% \bottomrule
% \end{tabular}
% }
% \end{table*}

\subsection{Contributions of This Paper} 
This survey provides a focused and contemporary review of understanding of the surgical scene based on AI, emphasizing the transformative role of foundation AI models in the field. Unlike traditional reviews that primarily cover conventional ML and DL techniques, we delve into the unique advancements enabled by foundation models, Vision Transformers (ViTs), pre-trained multimodal architectures, and generative AI systems.  Our contributions are as follows:




\begin{itemize}  
    \item \textit{Contemporary Focus on Foundation AI Models:} This survey uniquely highlights how foundation models are reshaping surgical scene understanding, particularly through their ability to generalize across tasks and modalities. We analyze their applications in real-time surgical workflows for tasks such as robust tool segmentation, fine-grained workflow recognition, and anomaly detection, emphasizing their scalability and adaptability in MIS.

    \item \textit{Critical Insights into Transformative Use Cases:} By focusing on endoscopic video analysis, we underscore how foundation models outperform traditional approaches in addressing challenges like variability in surgical scenes, occlusions, and inter-patient heterogeneity. The survey provides critical insights into their utility in tasks such as automated annotation, video summarization, and cross-modal reasoning, bridging gaps in surgical data analysis with unprecedented efficiency.

    \item \textit{Evaluation of Evolving Datasets and Benchmarks:} We offer an in-depth evaluation of emerging datasets and benchmarks specifically designed for foundation model training and validation in surgical applications. This includes an analysis of large-scale, multimodal datasets that enable transfer learning and fine-tuning for surgical scene understanding, addressing gaps in data availability and variability.

\end{itemize}  





By centering on foundation AI models, this survey represents a timely and significant advancement in the literature, offering a forward-looking perspective on their role in revolutionizing surgical practices. It serves as a critical resource for researchers and clinicians aiming to leverage state-of-the-art AI to enhance surgical scene understanding in MIS, a fundamental step in improving patient outcomes.

\subsection{Organization of This Paper}

This paper provides a comprehensive review of foundational AI models and their application in surgical scene understanding, with a particular focus on interpreting endoscopic video data for minimally invasive surgery. By exploring the integration of advanced AI techniques, this review highlights the transformative impact of these technologies on surgical practice, with the aim of improving precision and outcomes in MIS. The review also delves into the evolution and current landscape of various techniques within medical imaging, including segmentation, tool detection, workflow recognition, and anomaly detection, specifically their utility in surgical environments. It traces the chronological development of these technologies and examines contemporary advancements, emphasizing their critical role in augmenting surgical precision and improving patient outcomes. The organization of the paper is shown in Fig.~\ref{fig:organization}, which provides the visual presentation of all the sections and sub-sections. 


\section{Background and Challenges}
\label{sec:back}


% Deep learning algorithms, by analyzing hundreds of surgical video lectures, have the capability to identify intricate patterns and subtle nuances in surgical techniques \cite{maier2017surgical}. This analytical prowess significantly enhances training programs and promotes consistency in procedural execution. Such technologies not only support the education of new surgeons but also aid experienced professionals in refining their skills by providing detailed, data-driven insights into their practice.

% As a pivotal component of modern surgical data science, it utilizes advanced computational models to elevate the understanding and operational efficiency of surgical practices. By synthesizing elements from machine learning, computer vision, and data analytics, this interdisciplinary field integrates and interprets diverse data types, ranging from high-resolution video feeds to synchronized audio inputs and physiological signals to enable a holistic analysis of surgical procedures.


% \subsection{Introduction to Surgical Video Analytics}

% Surgical video analytics represents a transformative approach at the intersection of computer vision, machine learning, and surgical practice, designed to enhance the efficacy and safety of surgical procedures.  Utilizing cutting-edge techniques from image processing and AI, surgical video analytics analyzes video footage captured during surgeries. Algorithms capable of detecting, segmenting, and tracking surgical instruments and maneuvers transform complex video data into precise and actionable insights, as can be seen visually in Fig.~\ref{fig:tool-centric}. Moreover, surgical video analytics enables various applications, from real-time decision support where surgeons receive augmented overlays that guide procedural steps to comprehensive post-operative analyses that assess procedural efficacy and inform future interventions. The field also plays a crucial role in surgical training and education, providing quantitative assessments of surgical skills based on objective data rather than subjective evaluations. This feedback is vital for training surgeons to meet standardized levels of proficiency. Additionally, automated documentation generated from surgical videos can significantly reduce the administrative load on surgical teams, allowing them to focus more on patient care.

% The advent of 3D augmentation technologies transforms traditional two-dimensional video streams into dynamic, real-time three-dimensional reconstructions of the surgical site \cite{yuk2021current}. This technological enhancement offers surgeons an immersive and intuitive view, greatly simplifying complex tasks such as navigating around critical anatomical structures or manipulating delicate tissues. The depth and clarity provided by 3D visualizations help in accurately identifying and avoiding vital structures, thereby minimizing surgical trauma and enhancing patient safety. These modular and scalable 3D systems not only refine the surgeon's perceptual and spatial awareness but also pave the way for more precise and minimally invasive interventions \cite{kim2015design}. By reducing the physical impact on the patient, these advancements hold the potential to decrease intraoperative risks and improve overall patient outcomes. 


% \subsection{Overview of Surgical Scene Understanding}
% Surgical scene understanding is a critical aspect of modern surgery, enabling enhanced visualization, improved decision-making, and better patient outcomes. Machine learning (ML) techniques have been increasingly applied to interpret and analyze surgical scenes, providing valuable assistance to surgeons during procedures. This section explores the various applications of ML in surgical scene understanding, highlighting key advancements and their impact on surgical practice. The taxonomy of ML applications within the domain of surgical scene understanding, depicted in Fig.~\ref{fig:Tax}, the categorization is structured into three pivotal segments: Instrument Understanding, Surgical Workflow Analysis, and Scene Understanding. Each segment addresses distinct but interconnected aspects of the surgical environment, harnessing the capabilities of advanced ML techniques to significantly enhance operational efficiency, accuracy, and safety.

% However, the implementation of surgical video analytics is not without challenges. Variability in surgical procedures, differences in patient anatomy, and the unstructured nature of surgical settings add layers of complexity to algorithm development. The high dimensionality of video data also demands robust computational resources, which can be a barrier to real-time processing applications. Furthermore, strict privacy and security measures are imperative when handling sensitive medical footage to comply with health data regulations.

% As technology advances, the future of surgical video analytics is set to integrate more deeply with artificial intelligence, improving the granularity and accuracy of data analysis. Innovations such as edge computing are anticipated to facilitate real-time analytics without extensive bandwidth, making sophisticated tools accessible in diverse clinical settings, including those with limited resources. The integration of multi-modal data combining video analytics with patient vitals and other intra-operative data promises a more holistic approach to monitoring and improving surgical care. These advancements suggest a promising horizon for surgical video analytics, poised to redefine norms in surgical precision, training, and patient outcomes.




\subsection{Surgical Video Analytics and Scene Understanding}  
Surgical video analytics and scene understanding represent transformative advances at the intersection of computer vision, ML, and surgical practice, with the aim of improving the safety, efficiency, and precision of surgical procedures. These technologies leverage cutting-edge techniques from image processing and ML / DL to analyze videos captured during surgery, providing actionable insights in real time and impact on postoperative events (for example, increased bleeding events may lead to increased length of hospital stay; increased intraoperative bowel handling may lead to bowel paralysis and higher rates of post-operative nausea and vomiting). Algorithms for detecting, segmenting, and tracking surgical instruments and maneuvers transform complex video data into precise information that supports decision making, as illustrated in Fig.~\ref{fig:tool-centric}.

Surgical scene understanding complements video analytics by focusing on the interpretation of surgical environments, including instrument recognition, workflow analysis, and scene segmentation. These capabilities enable real-time decision support through augmented overlays that guide procedural steps, comprehensive postoperative analyzes to assess surgical efficacy, and automated documentation to reduce administrative burden on surgical teams, particularly relevant after a procedure that may take several hours. The latter would allow clinicians to focus on other clinical tasks such as informing relatives of patients about the procedure or assisting in the safe transfer of patients out of the operating room. In addition, it is likely to describe the ground truth of operative events without bias, a highly relevant point in governance. In Fig.~\ref{fig:intraoperative}, the three stages of surgical procedures are depicted, illustrating the comprehensive workflow from pre-operative planning through intra-operative execution to post-operative recovery and evaluation.

Furthermore, the advent of 3D augmentation technologies has revolutionized surgical video analytics by converting traditional 2D video streams into dynamic 3D reconstructions, providing surgeons with an immersive and intuitive view of the surgical site \cite{yuk2021current}. These systems improve spatial awareness, simplify navigation around critical anatomical structures, and enable more precise interventions \cite{kim2015design}, minimizing patient trauma and improving outcomes.

Despite these advances, numerous challenges persist, including variability in surgical procedures, differences in patient anatomy, and the unstructured nature of surgical environments, which collectively complicate algorithm development. Moreover, high-dimensional video data demands substantial computational resources for real-time processing, while stringent privacy and security measures remain critical to safeguarding sensitive medical footage. The future of surgical video analytics and scene understanding lies in deeper integration with AI and multimodal data, combining video analysis with patient vitals and other intraoperative information. Innovations like edge computing promise real-time analytics in resource-constrained settings, whereas enhanced algorithms can provide greater granularity and accuracy in data interpretation. These advancements are poised to redefine surgical precision, training, and patient care, ultimately paving the way for safer and more effective surgical practices worldwide.



\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{Images/IntraOperative.png}
  \caption{Sequential stages of a surgical procedure. {The \textit{Pre-Operative stage} involves patient assessment, detailed imaging (and scene understanding), and surgical planning (based on better understanding). The \textit{Intra-Operative stage} focuses on the actual surgical procedure itself, including anesthesia administration, correct patient positioning and adherence to safety protocols. The \textit{Post-Operative stage} covers both initial and long-term recovery, concluding with follow-up and outcome evaluations to ensure the effectiveness of the procedure.}}
  \label{fig:intraoperative}
\end{figure*}



\subsection{DL Models in Surgical Video Analysis}
% The use of DL in analyzing surgical videos has brought major advancements to the field of surgery, offering new ways to understand procedures and enhancing the abilities of surgical teams. However, applying DL to surgical video analysis comes with several significant challenges. These challenges include differences in video quality and format, the high computational power required, concerns about patient privacy and ethical issues, and the need for seamless integration with existing medical systems. In the following sections, we will explore these challenges in detail and discuss how they impact the development and use of advanced DL systems in surgical environments. This overview provides a basis for understanding the current state of surgical video analysis and highlights how innovative DL techniques can help overcome these hurdles.

% \subsection{Applications of ML in Surgical Scene Understanding}
% \begin{figure*}[hbtp]
%   \centering
%   \includegraphics[width=\textwidth]{TaxonSurgical.png}
%   \caption{Taxonomy of Machine Learning Applications in Robo-Assisted Surgery}
%   \label{fig:Tax}
% \end{figure*}

% \begin{figure}[hbtp]
%   \centering
%   \includegraphics[width=\columnwidth]{ABC.png}
%   \caption{TBA}
%   \label{fig:Scene}
% \end{figure}


\subsubsection{Convolutional Neural Networks}
CNNs are foundational to modern video analysis, particularly due to their ability to efficiently extract high-level features from visual data. Structured as a series of convolutional layers, CNNs capture spatial hierarchies in images, rendering them extremely effective in tasks such as object detection and scene classification. These networks perform convolutions across image pixels to generate feature maps that summarize the presence of specific features at various locations in the image. This ability makes CNNs particularly adept at processing visual inputs \cite{khan2023pd} that are common in video data, such as frames from surgical procedures. In surgical video analysis, CNNs are utilized not only for object identification but also for understanding the interaction and relative positioning of various surgical tools and anatomical structures. For example, through frame-by-frame analysis of video data, CNNs can detect environmental changes, track the movement of surgical instruments, and monitor the progress of surgical interventions and any potential hazards, and predict the end of the surgical procedure (this helps in high-level management of theater lists and helps improve case throughput and theatre time utilization). Such capabilities are crucial for developing real-time feedback systems that can alert surgeons about critical events or anomalies detected during surgery, as well as giving theatre teams a high-level overview of surgical procedures \cite{garcia2017toolnet}.

% \subsubsection{Segmentation Models}
% U-Net, DeepLabv3, etc. 
\subsubsection{Segmentation Models}
Segmentation models are crucial in medical image analysis, providing detailed pixel-level annotations that are essential for both diagnostic and interventional procedures. Two of the most influential segmentation models in this domain are U-Net and DeepLabv3, each known for its effectiveness in handling the complexities of medical imagery.
\begin{itemize}
    \item {\textit{U-Net}}: Developed specifically for biomedical image segmentation, U-Net features a symmetric architecture with a downsampling path to capture context and a precisely corresponding upsampling path to allow for precise localization \cite{ronneberger2015u}. This network architecture is particularly well suited for medical applications because of its efficiency in using a limited number of training samples to produce high-resolution segmentations. The ability of U-Net to perform well even with small amounts of data and its robustness against noise in images make it a preferred choice for segmenting surgical video frames, where precise delineation of organs and surgical instruments is critical.
    \item {\textit{DeepLabv3}}: Building on earlier versions, DeepLabv3 incorporates \textit{atrous convolutions}, a technique that increases the receptive field of filters by inserting spaces between kernel elements, to capture richer contextual information without compromising the sharpness of the image segmentation \cite{chen2017rethinking}. This model utilizes an atrous spatial pyramid pooling module to robustly segment objects at multiple scales, a feature particularly useful for the varied scales seen in surgical videos. DeepLabv3's ability to effectively segment fine structures at different depths and scales is invaluable in surgeries, providing clear delineations that can guide surgical interventions and improve outcomes. 
\end{itemize}

\begin{figure*}[hbtp]
  \centering
  \includegraphics[scale=0.55]{foundational.png}
  \caption{{Overview of foundational models applied in surgical settings, illustrating their roles in various applications such as report generation, treatment planning, and surgical assistance.}}
  \label{fig:foundational}
\end{figure*}
% Both models, U-Net and DeepLabv3, are instrumental in advancing the capabilities of surgical video analysis by enhancing the accuracy and precision of segmentations. This allows for a more nuanced understanding of the surgical field, supporting surgeons in real-time decision-making and contributing to more refined and safer surgical procedures.

\subsubsection{Vision Transformers (ViT)}
ViT represents a significant paradigm shift in how image data is processed for complex tasks like video analysis, including in high-stakes environments such as operation theaters. Originally adapted from the transformer architecture, which has revolutionized natural language processing, ViTs apply the principles of self-attention to visual contexts, allowing them to learn contextual relationships between different parts of an image \cite{dosovitskiy2020image}. In surgical video analysis, ViTs have shown great promise in accurately segmenting and identifying critical structures within surgical scenes, outperforming traditional CNNs in tasks that require understanding complex spatial dependencies and long-range interactions \cite{carion2020end}. This makes ViTs particularly suitable for applications where precision and context-aware decision-making are crucial, such as real-time surgical guidance and post-operative analysis.

\subsubsection{Diffusion Models}
Diffusion models are a class of generative models that have attracted significant attention for their ability to generate high-quality, detailed images from a learned distribution of training data. These models work by initially learning to gradually add noise to an image, transforming it into a Gaussian noise distribution, and then learning to reverse this process to reconstruct the original image from the noise \cite{ho2020denoising}. This forward and reverse process enables diffusion models to effectively model the probability distribution of training data, making them extremely powerful for generating or reconstructing images. In surgical video analysis, diffusion models can be utilized to enhance low-resolution or noisy surgical videos, providing clearer visualizations of surgical procedures . They can also generate synthetic surgical images for training purposes, allowing for the creation of diverse scenarios that help train surgical staff without the need for extensive real-life video collections. Additionally, diffusion models, with their generative capabilities, hold significant potential for simulating possible surgical outcomes based on intra-operative videos, aiding surgeons in planning and decision-making during complex procedures. These applications demonstrate the versatility and potential of diffusion models in transforming surgical education for future generations of surgical trainees and practice by enhancing the quality and utility of surgical imagery. Ultimately, a library of surgical videos that create all potential scenarios or hazards that can occur during surgery will lead to better prepared surgeons and improved patient outcomes. 



\subsection{Foundation Models}


Foundation models (FMs) represent a significant breakthrough in machine learning, characterized by their extensive pre-training on large-scale datasets across a diverse range of tasks and domains \cite{10.1093/bjs/znae090}. Although these models are generally designed for broad use cases, researchers are increasingly applying and fine-tuning them in specialized fields like medical imaging. In the realm of medical imaging, FMs have revolutionized the approach to complex image data interpretation. These models leverage the massive amount of pre-training to develop a deep and nuanced understanding of image features, which is crucial for tasks such as medical image segmentation, classification, and anomaly detection. The application of these models in medical image analysis has led to substantial advancements, particularly in enhancing the accuracy and efficiency of segmentation processes used to identify and delineate anatomical structures and pathological conditions from medical imaging data \cite{hosny2018artificial}.


Despite these benefits, there are notable challenges when using FMs in healthcare. First, patient privacy must be upheld, which can limit the volume and variety of data available for training \cite{zhang2022privacy}. Second, the black box nature of deep learning can make it difficult to interpret model decisionsan issue of particular concern in medical settings, where transparency and accountability are paramount \cite{rudin2019stop}. Third, many clinical environments lack the substantial computational resources or extensive datasets needed to fine-tune large models \cite{topol2019high}. Further complicating matters are rigorous data protection laws, such as HIPAA in the United States and GDPR in the European Union, which limit data sharing across organizations and regions.


Nevertheless, FMs also open exciting new possibilities. They can integrate multiple data types, such as combining medical images with patient health records to create more comprehensive diagnostics \cite{rajpurkar2018deep}. By refining these models through detailed customization, it is possible to address specific needs in areas like surgical video analysis and disease detection. They introduce a paradigm shift in medical image analysis by leveraging large-scale datasets to capture a broad spectrum of features and patterns \cite{zhang2023challenges}. Ultimately, while FMs hold great promise, ongoing research is needed to adapt them effectively to diverse medical tasks, ensure interpretability, and comply with strict regulatory standards \cite{kaur2019systematic}.

% However, the deployment of FMs also presents challenges, particularly regarding data privacy, model interpretability, and the need for extensive computational resources. Moreover, the specificity and variability of medical data necessitate careful adaptation and validation of these models to ensure their effectiveness and reliability in clinical settings \cite{topol2019high}. New perspectives offered by FMs include the ability to integrate multimodal data sources, such as combining imaging data with electronic health records, to provide a more comprehensive diagnostic analysis. Furthermore, the ongoing evolution of these models promises even greater personalization of diagnostic and therapeutic approaches, potentially transforming patient care in profound ways \cite{rajpurkar2018deep}. FMs introduce a paradigm shift in medical image analysis by leveraging large-scale datasets to capture a broad spectrum of features and patterns \cite{zhang2023challenges}. However, their application in medicine also presents unique challenges, such as model interpretability, data privacy, and the need for customization to specific medical contexts. These models require significant adaptation to align their broad learning capabilities with the nuanced requirements of medical diagnostics and surgical interventions. One of the key challenges is the interpretability of FM, particularly in critical domains such as surgery, where understanding the decision-making process is essential in developing a world-class environment that is transparent, accountable, and lends itself to constant improvement following critical feedback \cite{rudin2019stop}. 

% There exists a variety of challenges related to the development of a foundational model. Unlike more transparent models, the ``black box'' nature of DL poses difficulties for clinicians who must trust the outcomes of these systems. Additionally, data privacy and regulatory issues also create barriers to the widespread adoption of FMs in medical applications \cite{zhang2022privacy}. Balancing model compliance with stringent regulations such as the Health Insurance Portability and Accountability Act (HIPAA) in the United States and the General Data Protection Regulation (GDPR) in the European Union while still enabling effective learning from large datasets remains a significant challenge. Furthermore, the performance of FMs often depends heavily on the quality and diversity of the training data. In many medical settings, data is heterogeneous and limited, which can hinder the model's ability to generalize \cite{kaur2019systematic}. 

Finally, while foundation models hold great promise for broad applications, their effectiveness in specialized medical contexts such as surgical video analysis and diagnostics relies heavily on customization and domain-specific fine-tuning \cite{azizi2021big}. This section examines models like SAM, CLIP, DINO, BERT, and GPT, which showed instrumental outcomes in advancing data processing and decision-making in the medical domains, as shown in Fig. \ref{fig:foundational}. 



% Their applications in surgical settings are further detailed in Section\ref{sec:fmapp}, highlighting their transformative impact on tasks such as real-time analysis and workflow optimization, ultimately redefining precision and efficiency in medical practice.

%   \centering
%   \includegraphics[width=\textwidth]{SAM.png}
%   \caption{ Chronological Timeline of SAM, SAM2 and Its Variants (2023-2024) for medical image segmentation}
%   \label{fig:chrono}
% \end{figure*}





\subsubsection{Segment Anything Model (SAM)}

The SAM \cite{kirillov2023segment} was developed to perform general-purpose image segmentation with minimal task-specific retraining using over 11 million images. Its adaptability extends to a wide array of applications, including autonomous driving, medical imaging, and satellite image analysis. Despite its non-medical origin, SAM shows considerable promise in healthcare settings. By combining robust CNN features and deep learning techniques, SAM can quickly adapt to different segmentation tasks, such as identifying tumors in CT scans, surgical tool recognition, or delineating organs in MRI data. This ability is referred to as zero-shot segmentation, as it allows SAM to perform inference on new imaging problems without extensive domain-specific training.

The subsequent iterations and specialized adaptations of SAM further illustrate its effectiveness and flexibility. For example, SAM3D adapts its architecture for 3D imaging tasks (e.g., CT, MRI, PET), using specialized 3D CNN layers \cite{bui2024sam3d}. In SAM, further innovations have been introduced that integrate multimodal imaging data, such as MRI and ultrasound, to achieve comprehensive diagnostic precision \cite{tan2024novelsam}. Further adaptations have been developed for the medical domain, such as the Medical SAM Adapter (MSA), which fine-tunes the SAM using adapter layers for higher segmentation precision in medical contexts \cite{chen2024ma}. SAM2 \cite{ravi2024sam}, which is a recent advanced version of SAM, has shown significant improvements in performing real-time segmentation in videos, which demonstrates its enhanced capabilities in complex scenarios such as tumor identification and organ delineation \cite{ma2024benchmark}. Lastly, many different efforts are being made to incorporate domain-specific knowledge into these models to capture the nuances of medical images more effectively using limited data \cite{azad2023foundational}. 



\begin{figure*}[hbtp]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Challanges/Challange-a.png}
        \caption{Instrument flare}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Challanges/Challange-b.png}
        \caption{Partial occlusions by blood}
        \label{fig:sub2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Challanges/Challange-c.png}
        \caption{Smoke-induced occlusion}
        \label{fig:sub3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Challanges/Challange-d.png}
        \caption{Underexposed regions}
        \label{fig:sub4}
    \end{subfigure}

    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Challanges/Challange-e.png}
        \caption{Motion blur}
        \label{fig:sub5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Challanges/Challange-f.png}
        \caption{Multiple instruments}
        \label{fig:sub6}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Challanges/Challange-g.png}
        \caption{Partial occlusion by organ}
        \label{fig:sub7}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Challanges/Challange-h.png}
        \caption{Transparent instrument}
        \label{fig:sub8}
    \end{subfigure}
\caption{\textit{Key challenges in surgical video analysis} include photometric artifacts (e.g., blurriness, specular reflections), occlusions from blood and tissue deformation, limited camera view, and tool similarity. These complexities hinder robust localization and segmentation, emphasizing the need for advanced foundation models. Image adapted from \cite{ceron2022real}.}
    \label{fig:chal}
\end{figure*}

%\subsubsection{ChatGPT (Chat Generative Pre-trained Transformer)}

\subsubsection{Generative Pre-trained Transformer (GPT)}  
The GPT \cite{brown2020language} foundation model, developed by OpenAI, serves as a versatile backbone for numerous AI applications due to its exceptional generalization and natural language understanding capabilities. ChatGPT is the most prominent development by OpenAI, which is an instruction-tuned conversational model that adapts GPT's capabilities for interactive and user-centric applications. While GPT functions as a general-purpose foundation model capable of handling a wide range of language tasks, ChatGPT is fine-tuned to specialize in conversational contexts, making it particularly effective in medical settings. It enables the automation of routine inquiries, enhances patient engagement through interactive dialogues, and streamlines clinical workflows by converting unstructured data into accurate and detailed medical reports. Moreover, its ability to assimilate and articulate complex medical literature makes it an invaluable tool for supporting research and continuing education for healthcare professionals. ChatGPT's integration into clinical practice not only improves operational efficiency but also facilitates personalized patient care by providing timely and relevant medical information. However, deploying ChatGPT in healthcare requires rigorous attention to data privacy, ethical considerations, and model reliability to ensure secure and responsible use in sensitive medical environments \cite{radford2019language}.


% Looking forward, there is a focus on enhancing the efficiency of these models, reducing computational demands, and improving generalization from limited annotated medical datasets. The integration of semi-supervised and unsupervised learning techniques is also explored to leverage unlabeled medical data effectively \cite{cheplygina2019not}.



% \end{itemize}

\subsubsection{Bidirectional Encoder Representations from Transformers (BERT)}
BERT is a pioneering language model that has significantly advanced natural language processing through its ability to understand context bidirectionally using a transformer-based architecture \cite{kenton2019bert}. In the surgical domain, the complexity and specificity of medical language require sophisticated models such as BERT, which are adept at capturing the nuanced semantics and intricate terminologies inherent in surgical texts. BERT's pre-training on extensive and diverse datasets enables it to develop a deep comprehension of language patterns, making it well-suited to address the rigorous demands of surgical data analysis \cite{bombieri2024surgicberta}. Its architecture facilitates an effective contextual representation, which is critical for accurately interpreting the multifaceted information present in surgical documentation and communication. Moreover, the adaptability of BERT through fine-tuning processes ensures that it can be tailored to meet the specific linguistic and contextual requirements of the surgical field, thereby enhancing the robustness and precision of language understanding in medical contexts. Although primarily focused on textual data, BERT also supports the integration and interpretation of findings from various medical imaging modalities, such as MRI or CT, by aiding in the automatic extraction and structuring of textual reports generated from these images, thus bridging the gap between radiological findings and surgical planning. This would be hugely useful in large-scale rapid automated reporting and would help alleviate the pressures human radiologists face when reporting large volumes of scans under the Faster Diagnostic Standards (FDS). In the UK, the FDS framework is a 28-day national health service (NHS) target to diagnose or exclude cancer in patients. These attributes position BERT as a foundational model in integrating advanced language processing capabilities within surgical data systems, contributing to the advancement of data-driven practices in surgical environments.


\subsubsection{Self-Distillation with No Labels (DINO)}

DINO \cite{caron2021emerging} exemplifies a groundbreaking approach in self-supervised learning, initially designed for general computer vision tasks. Nevertheless, its ability to learn from unlabeled data makes it especially appealing for medical applications, where annotated data can be hard to obtain. By employing a teacher-student training framework, DINO captures meaningful image features from different augmented views of the same input without needing pre-existing annotations. This is very important for the medical domain as the data is often scarce and hard to annotate. 

In surgical contexts, for example, DINO could learn to recognize and segment important structures in endoscopic videos even if ground truth labels are scarce. This feature is crucial in clinical environments, where the time and expertise required to annotate large volumes of surgical footage may be limited. Many researchers have utilized DINO for different applications in several innovative ways. For instance, the SurgVID \cite{koksal2024surgivid} framework uses DINO's self-supervised learning to efficiently segment surgical tools and anatomical structures from video data, thus reducing the need for extensive manual annotations. This approach has proven to be nearly as effective as fully-supervised methods, showcasing the potential of self-supervised techniques in surgical applications.

Another adaptation, known as SurgicalDINO \cite{cui2024surgical}, modifies DINO for depth estimation in robotic surgery. This version incorporates Low-Rank Adaptation (LoRA) layers \cite{hu2021lora}, allowing the model to be fine-tuned for specific surgical applications without the need to retrain the entire model. This method demonstrates how DINO can be tailored to surgical navigation and 3D reconstruction. Additionally, Ramesh et al. \cite{ramesh2023dissecting} compared various self-supervised learning models, including DINO, on surgical datasets like Cholec80. They explored how well these models perform in recognizing surgical phases and detecting tools, indicating that self-supervised learning holds promise for enhancing surgical computer vision.


Furthermore, DINO's scalability and efficiency make it well-suited for medical centers that lack extensive computational resources. Through these advantages, DINO holds considerable promise in supporting tasks like surgical planning, intraoperative guidance, and postoperative analysis.



% DINO \cite{caron2021emerging} represents a groundbreaking approach in the field of medical imaging \aq{DINO was not originally proposed for medical imaging.}, particularly in surgical applications, through its advanced self-supervised learning techniques. Developed primarily for Vision Transformers (ViTs), DINO utilizes knowledge distillation where a student model learns from a teacher model without the need for labeled data, a method particularly advantageous in the medical field where labeled datasets can be scarce and expensive to procure. This technique allows DINO to develop autonomously a subtle understanding of complex medical images, such as those generated during surgical procedures. By processing different augmented views of the same image, DINO effectively captures essential semantic features that are crucial for tasks such as surgical planning and intraoperative guidance. The model's capability to generalize from unlabeled data indicates that it can assist in identifying and segmenting surgical targets from complex anatomical backgrounds. In addition, the scalability and efficiency of DINO make it suitable for deployment in varied surgical settings, potentially reducing reliance on extensive annotated medical data sets and accelerating the adoption of advanced AI tools in resource-constrained environments. By pushing the boundaries of self-supervised learning in surgery, DINO paves the way for more innovative and accessible applications in surgical practice.



\subsubsection{Large Language Model Architecture (LLaMA)}
LLaMA \cite{touvron2023llama}, a high-capacity language model, leverages transformer-based mechanisms to understand and generate human language with remarkable accuracy and depth. Developed to facilitate advanced natural language processing tasks, LLaMA is designed for a wide array of applications ranging from automated text generation to complex query handling \cite{shao2024survey}. In the medical field, LLaMA's capabilities are particularly beneficial, as the model can interpret and synthesize medical literature, patient reports, and clinical guidelines with high precision. By training in various medical texts, LLaMA can help healthcare professionals by providing diagnostic suggestions, summarizing patient histories, and even generating informational content for patient education. Its ability to process and generate medical text effectively makes it an invaluable tool for enhancing clinical decision-making and improving patient outcomes. Moreover, LLaMA integration into clinical information systems can streamline workflows by automating documentation processes and extracting useful information from vast datasets, allowing medical personnel to focus more on patient care than administrative tasks \cite{nazi2024large}. As the demand for efficient and accurate processing of medical information grows, LLaMA stands out as a transformative model capable of revolutionizing various aspects of healthcare delivery.

\subsubsection{Contrastive Language-Image Pre-training (CLIP)}
CLIP \cite{radford2021learning} is a novel model introduced by OpenAI that revolutionizes the way machines understand images and text together. CLIP is trained in a contrastive manner where the images are paired with the corresponding text, learning from a wide variety of publicly available images and captions. This unique training approach allows CLIP to generalize across a wide array of visual concepts in a zero-shot manner. It can understand and categorize images that it has never seen before based on textual descriptions alone \cite{hafner2021clip}. In the medical and surgical fields, CLIP's capabilities can be particularly transformative. Its ability to interpret and correlate complex medical imagery with corresponding clinical notes or annotations without direct supervision makes it an excellent tool for diagnostic imaging. For example, CLIP can help radiologists and surgeons quickly identify relevant features in medical scans, such as magnetic resonance images or CT images, that correspond to textual descriptions found in case reports or diagnostic criteria. This could significantly speed up the diagnosis process, an imperative in the NHS FDS pathway, and improve the accuracy of identifying and classifying pathological characteristics. Moreover, CLIP's robust generalization ability allows it to adapt to diverse medical datasets, potentially reducing the time and resources required for model training and fine-tuning in specialized medical applications\cite{sun2023eva}.


% Deep learning, particularly the use of convolutional neural networks (CNNs), has revolutionized surgical scene understanding \cite{wang2022medical}. CNNs have become the standard for visual data tasks due to their proficiency in processing spatial hierarchies, allowing them to effectively capture patterns and features in medical images \cite{litjens2017survey}. However, the recent introduction of Transformer models \cite{dosovitskiy2021imageworth16x16words} has brought new possibilities through self-attention mechanisms that provide global context, addressing CNNs' limitations in managing complex spatial relationships in medical imaging \cite{dosovitskiy2020image}.

% Building on these advancements, models like the Segment Anything Model (SAM) \cite{kirillov2023segment} represent a move toward flexible and adaptable segmentation solutions. SAM and similar models are designed for dynamic surgical environments, capable of handling varied imaging conditions and diverse segmentation tasks with minimal retraining \cite{zhou2019fast}. Such adaptability is critical for enabling real-time applications in surgery, where conditions can change rapidly and a reliable, consistent model response is essential.

% A notable application of these segmentation advancements is in augmented reality (AR) systems for surgery. By projecting segmented images and contextual data directly onto the patient's body in real time, AR systems can enhance spatial awareness and precision for surgeons, providing valuable, context-sensitive information that guides surgical decisions and improves outcomes \cite{kunz2022augmented}. As segmentation accuracy continues to improve, the potential for AR integration in surgical procedures will further elevate the precision and safety of complex interventions.






% \section{Surgical Video Analytics: Introduction}
% Surgical video analytics represents a transformative approach at the intersection of computer vision, machine learning, and surgical practice, designed to enhance the efficacy and safety of surgical procedures. This emerging field utilizes cutting-edge techniques from image processing and artificial intelligence to analyze video footage captured during surgeries. By applying algorithms capable of detecting, segmenting, and tracking surgical instruments and maneuvers, surgical video analytics transforms complex video data into precise, actionable insights. These capabilities enable a range of applications from real-time decision support---where surgeons receive augmented overlays that guide procedural steps---to comprehensive post-operative analyses that assess procedural efficacy and inform future interventions.

% The field also plays a crucial role in surgical training and education, providing quantitative assessments of surgical skills based on objective data rather than subjective evaluations. This feedback is vital for training surgeons to meet standardized levels of proficiency. Additionally, automated documentation generated from surgical videos can significantly reduce the administrative load on surgical teams, allowing them to focus more on patient care.

% However, the implementation of surgical video analytics is not without challenges. Variability in surgical procedures, differences in patient anatomy, and the unstructured nature of surgical settings add layers of complexity to algorithm development. The high dimensionality of video data also demands robust computational resources, which can be a barrier to real-time processing applications. Furthermore, strict privacy and security measures are imperative when handling sensitive medical footage to comply with health data regulations.

% As technology advances, the future of surgical video analytics is set to integrate more deeply with artificial intelligence, improving the granularity and accuracy of data analysis. Innovations such as edge computing are anticipated to facilitate real-time analytics without extensive bandwidth, making sophisticated tools accessible in diverse clinical settings, including those with limited resources. The integration of multi-modal data---combining video analytics with patient vitals and other intra-operative data---promises a more holistic approach to monitoring and improving surgical care. These advancements suggest a promising horizon for surgical video analytics, poised to redefine norms in surgical precision, training, and patient outcomes.



% Surgical video analytics, a pivotal component of modern surgical data science, utilizes advanced computational models to elevate the understanding and operational efficiency of surgical procedures. This interdisciplinary field synthesizes elements from machine learning, computer vision, and data analytics to integrate and interpret diverse data types. These range from high-resolution video feeds to synchronized audio inputs and physiological signals, enabling a holistic analysis of surgical practices. DL algorithms, by analyzing hundreds of surgical video lectures, have the capability to identify intricate patterns and subtle nuances in surgical techniques. This analytical prowess significantly enhances training programs and promotes consistency in procedural execution \cite{maier2017surgical}. Such technologies not only support the education of new surgeons but also aid experienced professionals in refining their skills by providing detailed, data-driven insights into their practice.

% The advent of 3D augmentation technologies marks a significant evolution in surgical video analytics. This innovative approach transforms traditional two-dimensional video streams into dynamic, real-time three-dimensional reconstructions of the surgical site. This technological enhancement offers surgeons an immersive and intuitive view, greatly simplifying complex tasks such as navigating around critical anatomical structures or manipulating delicate tissues \cite{yuk2021current}. The depth and clarity provided by 3D visualizations help in accurately identifying and avoiding vital structures, thereby minimizing surgical trauma and enhancing patient safety. These modular and scalable 3D systems not only refine the surgeon's perceptual and spatial awareness but also pave the way for more precise and minimally invasive interventions. By reducing the physical impact on the patient, these advancements hold the potential to decrease intraoperative risks and improve overall patient outcomes\cite{kim2015design}.



% \begin{figure*}[h!]
%   \centering
%   \includegraphics[width=\textwidth]{challenges_image.png}
%   \caption{The various challenges posed by surgical tool images \cite{ceron2022real}: \textbf{(a)} partial occlusion by organs, \textbf{(b)} motion blur, \textbf{(c)} smoke-induced occlusion, \textbf{(d)} instrument flare, \textbf{(e)} transparent tools, \textbf{(f)} presence of multiple different instruments in a single scene, \textbf{(g)} underexposed areas containing instruments, and \textbf{(h)} partial occlusions caused by blood.}
%   \label{fig:chal}
% \end{figure*}


% \begin{figure*}[hbtp]
%     \centering
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Challanges/Challange-a.png}
%         \caption{Instrument flare}
%         \label{fig:sub1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Challanges/Challange-b.png}
%         \caption{Partial occlusions by blood}
%         \label{fig:sub2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Challanges/Challange-c.png}
%         \caption{Smoke-induced occlusion}
%         \label{fig:sub3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Challanges/Challange-d.png}
%         \caption{Underexposed regions}
%         \label{fig:sub4}
%     \end{subfigure}

%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Challanges/Challange-e.png}
%         \caption{Motion blur}
%         \label{fig:sub5}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Challanges/Challange-f.png}
%         \caption{Multiple instruments}
%         \label{fig:sub6}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Challanges/Challange-g.png}
%         \caption{Partial occlusion by organ}
%         \label{fig:sub7}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Challanges/Challange-h.png}
%         \caption{Transparent instrument}
%         \label{fig:sub8}
%     \end{subfigure}
% \caption{\textbf{Key challenges in surgical video analysis} include photometric artifacts (e.g., blurriness, specular reflections), occlusions from blood and tissue deformation, limited camera view, and tool similarity. These complexities hinder robust localization and segmentation, emphasizing the need for advanced foundation models.}
%     \label{fig:chal}
% \end{figure*}




\subsection{Challenges in Surgical Scene Understanding}
\label{sec:challenges}

Surgical scene understanding presents a diverse array of challenges that stem from the complexity and unpredictability of surgical environments, coupled with the technical limitations of current AI models. To advance AI-assisted surgical systems, the following challenges must be carefully addressed.



\subsubsection{Technical Challenges}

The domain of surgical video analysis presents unique challenges that hinder robust localization and segmentation of surgical instruments, as shown in Fig.~\ref{fig:chal}. Unlike natural images and videos, surgical frames are characterized by high tissue deformations and frequent occlusions caused by the presence of blood and multiple artifacts on the instruments. Photometric artifacts, as identified by \cite{ni2020pyramid}, can significantly degrade the performance of segmentation models. Some of the additional complexities include:
\begin{itemize}
 \item[a)] \textit{Subtle Variance and Limited View}: Surgical procedures may involve subtle interphase or intraphase variances that are difficult to capture consistently. The limited field of view offered by surgical visualization cameras further complicates visual assessment, while restricting the visual context available for decision-making \cite{hesamian2019deep}.
    \item[b)] \textit{Blurriness and Specular Reflection}:
Camera motion and the gas emissions from surgical tools often cause blurriness, while specular reflections and scale variations, as discussed by Baumhauer et al. \cite{baumhauer2008navigation}, can lead to poor segmentation accuracy.
\item[c)] \textit{Tool Similarity and Edge Presence}: The segmentation of multiple instruments is particularly challenging due to the appearance and shape similarity between different tools. Instruments located on the edge of video frames are especially difficult to detect and segment reliably \cite{karpat2008mechanics}. Moreover, the variations in the instrument pose may also alter the perceived geometry or shape, depending on the surgical camera's field of view.
\end{itemize}

\begin{table*}[ht]
\centering
\caption{Overview of Machine Learning Applications in Surgery}
\label{tab:mlapplicationssurgery}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}p{3.5cm}p{2.5cm}p{2.5cm}p{3cm}p{3cm}p{5cm}@{}}
\toprule
\textbf{Application} & \textbf{Method} & \textbf{Model} & \textbf{Data} & \textbf{Metrics} & \textbf{Limitations} \\ \midrule
Instrument Segmentation \cite{ronneberger2015u} & DL & U-Net & Surgical videos & Accuracy, IoU & High computational cost \\ 
Tissue Classification \cite{litjens2017survey} & Supervised Learning & CNNs & Histopathological images & Precision, Recall & Data variability \\ 
Phase Recognition \cite{twinanda2017endonet} & Temporal Analysis & LSTM & Time-series data & Accuracy, F1-Score & Long training times \\ 
Anomaly Detection \cite{schlegl2017unsupervised} & Anomaly Detection & Autoencoders & Physiological signals & AUC, ROC & False positives \\ 
Skill Assessment \cite{ahmidi2017dataset} & Pattern Recognition & SVM & Motion tracking data & Accuracy, Error rate & Subjective bias \\ 
Organ Segmentation \cite{milletari2016v} & DL & V-Net & CT scans, MRI & Dice coefficient & Requires large datasets \\ 
Error Prediction \cite{korbar2017deep} & Predictive Modeling & Random Forest & Operational data & Sensitivity, Specificity & Overfitting \\ 
Depth Estimation \cite{laina2016deeper} & Computer Vision & Stereo Vision & 3D imaging & Mean error & Calibration issues \\ 
Segmentation \cite{rueckert2024methods} & DL & CNNs & Endoscopic Videos & Accuracy, IoU & Computational demand \\ 
Medical Image Segmentation \cite{zhang2024segment} & Deep Learning, Foundation Models & SAM & CT, MRI & Dice Coefficient, IoU & Poor zero-shot performance, requires fine-tuning \\ 
Medical Image Segmentation \cite{azad2024medical} & DL & U-Net & CT, MRI, US, X-ray, OCT, PET & IoU & High computational cost, limited data \\ 
Biomedical Image Segmentation \cite{lee2024foundation} & Deep Learning, Zero-Shot Learning & SAM & Medical images across various modalities & Accuracy, Dice Coefficient, IoU & Segmentation in complex anatomical regions, adaptation to domain-specific challenges \\ 
Tracking and Mapping \cite{schmidt2024tracking} & Computer Vision, SLAM & Various, including SLAM and SfM & Medical Imaging (Endoscopy, Colonoscopy) & Accuracy, Precision & Low texture, light reflections, organ deformation \\ 
3D Reconstruction and Localization \cite{lin2016video} & Computer Vision & Laparoscope Localization & Surgical Videos & Accuracy, Precision & Needs high computational power, complex setup \\ 
3D vs. 2D Vision in Laparoscopy \cite{sorensen2016three} & Systematic Review & Not Applicable & Surgical Training & Performance Time, Error Reduction & High cost, visual discomfort, fatigue \\ 
Surgical Tool Detection \cite{bouget2017vision} & Review, Machine Learning & Not Applicable & Surgical Tools & Accuracy, Recall, Precision & Requires robust feature extraction and tracking methods, sensitive to visual occlusions and tool coverage \\ 
Endoscopic Video Processing \cite{munzer2018content} & Review & Various, Computer Vision & Endoscopic Videos & Accuracy, Precision & Requires robust computational techniques, sensitive to video quality and consistency \\ 
Tracking \cite{bouget2017vision} & Comparative Study, DL & CNNs, RF & Surgical videos & Accuracy, Precision, Recall & Complexity of dynamic environments, inconsistent annotations \\ 
\bottomrule
\end{tabular}
}
\end{table*}

\subsubsection{Lack of Representative Datasets}
The application of foundational models in surgical scene understanding faces significant challenges due to the scarcity of labeled data essential for training supervised learning methods. This scarcity is exacerbated by class imbalance issues, notably between foreground (surgical instruments) and background, where instruments occupy fewer image pixels compared to the predominantly background pixels. Current state-of-the-art approaches focus on pixel-wise classification for instrument segmentation but fail to account for global semantic correlations across images, potentially leading to inaccurate feature distributions and compromised segmentation accuracy. To overcome these challenges, it is imperative not only to enhance the robustness and efficiency of AI models, but also to innovate in data set creation and enhancement. This will ensure the availability of high-quality and balanced data that is necessary to effectively train foundational models in the complex setting of surgical operations.

%  The effectiveness of DL in surgical navigation is further constrained by data-specific challenges, including a general scarcity of labeled data necessary for training supervised learning methods. This scarcity is compounded by class imbalance issues, which may manifest as imbalances between foreground-background classes or among the foreground instances themselves. The foreground-background imbalance is particularly pronounced given that the small-sized instruments represent fewer image pixels, while most of each image is occupied by background pixels. Current state-of-the-art (SOTA) approaches primarily address the instrument segmentation problem through pixel-wise classification. However, they largely overlook the global semantic correlations among pixels across subsequent images. This oversight can lead to an imprecise distribution of features, undermining the potential for accurate segmentation.

% Addressing these challenges requires not only advancements in the robustness and efficiency of AI models but also innovations in dataset creation and enhancement to ensure high-quality, balanced data for training sophisticated DL models in the demanding environment of surgical operations.

\subsubsection{Ethical, Regulatory, and Privacy Challenges}

In addition to technical barriers, significant ethical, regulatory and privacy concerns must be addressed when applying AI to the understanding of the surgical scene. These considerations are essential to ensure that AI systems remain safe, reliable, and equitable in clinical settings. Below are some of the key challenges that require attention:

\begin{itemize}
    \item[a)] \textit{Patient Privacy and Data Protection:} The use of AI in surgical procedures frequently involves sensitive patient data, raising major concerns about privacy and data security. The protection of this information is vital to maintain trust among patients, clinicians, and institutions \cite{anderson2002ethics}.
    
    \item[b)] \textit{Regulatory Compliance and Bias:} AI systems in healthcare must comply with stringent regulatory standards, which continue to evolve \cite{char2018implementing}. Furthermore, AI models may introduce bias, especially if they are trained on nonrepresentative datasets, leading to disparities in care across different patient groups.
    
    \item[c)] \textit{Surgeon Accountability and Trust:} The introduction of AI into surgical decision making requires clear guidelines on surgeon accountability \cite{luxton2015artificial}. Although AI can assist in real-time decision making, surgeons must retain ultimate responsibility for patient outcomes. Ensuring that AI systems act as supportive tools rather than autonomous decision makers is critical to preserving trust and ethical responsibility.
    
\end{itemize}

% These ethical, regulatory, and privacy challenges emphasize the importance of developing AI tools that are not only effective but also safe, transparent, and aligned with clinical standards.




% \begin{table*}[ht]
% % \begin{table}[ht]

% % Can also add multi-modalities? 

% \centering
% \caption{Overview of Machine Learning Applications in Surgery}
% \label{tab:mlapplicationssurgery}
% \resizebox{\textwidth}{!}{%

% \begin{tabular}{@{}cccccp{5cm}@{}}
% \toprule
% \textbf{Application} & \textbf{Method} & \textbf{Model} & \textbf{Data} & \textbf{Metrics} & \textbf{Limitations} \\ \midrule
% \vspace{5mm}
% Instrument Segmentation\cite{ronneberger2015u} & DL & U-Net & Surgical videos & Accuracy, IoU & High computational cost \\
% \vspace{5mm}
% Tissue Classification \cite{litjens2017survey} & Supervised Learning & CNNs & Histopathological images & Precision, Recall & Data variability \\ 
% \vspace{5mm}
% Phase Recognition\cite{twinanda2017endonet} & Temporal Analysis & LSTM & Time-series data & Accuracy, F1-Score & Long training times \\ 
% \vspace{5mm}
% Anomaly Detection\cite{schlegl2017unsupervised} & Anomaly Detection & Autoencoders & Physiological signals & AUC, ROC & False positives \\ 
% \vspace{5mm}
% Skill Assessment\cite{ahmidi2017dataset} & Pattern Recognition & SVM & Motion tracking data & Accuracy, Error rate & Subjective bias \\
% \vspace{5mm}
% Organ Segmentation\cite{milletari2016v} & DL & V-Net & CT scans, MRI & Dice coefficient & Requires large datasets \\ 
% \vspace{5mm}
% Error Prediction\cite{korbar2017deep} & Predictive Modeling & Random Forest & Operational data & Sensitivity, Specificity & Overfitting \\ 
% \vspace{5mm}
% Depth Estimation \cite{laina2016deeper} & Computer Vision & Stereo Vision & 3D imaging & Mean error & Calibration issues \\ 
% \vspace{5mm}
% Segmentation\cite{rueckert2024methods} & DL & CNNs & Endoscopic Videos & Accuracy, IoU & Computational Demand \\
% \vspace{5mm}
% Medical Image Segmentation \cite{zhang2024segment} & Deep Learning, Foundation Models & SAM & CT, MRI & Dice Coefficient, IoU & Poor zero-shot performance, requires fine-tuning \\ 
% \vspace{5mm}
% Medical Image Segmentation\cite{azad2024medical} & DL & U-Net & CT, MRI, US, X-ray, OCT, PET & IoU & High computational cost, limited data \\
% \vspace{5mm}
% Biomedical Image Segmentation \cite{lee2024foundation} & Deep Learning, Zero-Shot Learning & SAM & Medical images across various modalities & Accuracy, Dice Coefficient, IoU & Segmentation in complex anatomical regions, adaptation to domain-specific challenge \\ 
% \vspace{3mm}
% Tracking and Mapping\cite{schmidt2024tracking} & Computer Vision, SLAM & Various, including SLAM and SfM & Medical Imaging (Endoscopy, Colonoscopy) & Accuracy, Precision & Low texture, light reflections, organ deformation \\ 
% \vspace{3mm}
% 3D Reconstruction and Localization\cite{lin2016video} & Computer Vision & Laparoscope Localization & Surgical Videos & Accuracy, Precision & Needs high computational power, complex setup \\ 
% \vspace{5mm}
% 3D vs. 2D Vision in Laparoscopy\cite{sorensen2016three} & Systematic Review & Not Applicable & Surgical Training & Performance Time, Error Reduction & High cost, visual discomfort, fatigue \\ 
% \vspace{3mm}
% Surgical Tool Detection\cite{bouget2017vision} & Review, Machine Learning & Not Applicable & Surgical Tools & Accuracy, Recall, Precision & Requires robust feature extraction and tracking methods, sensitive to visual occlusions and tool coverage \\ 
% \vspace{5mm}
% Endoscopic Video Processing\cite{munzer2018content} & Review & Various, Computer Vision & Endoscopic Videos & Accuracy, Precision & Requires robust computational techniques, sensitive to video quality and consistency \\
% \vspace{5mm}
% Tracking\cite{bouget2017vision} & Comparative Study, DL & CNNs, RF & Surgical videos & Accuracy, Precision, Recall & Complexity of dynamic environments, inconsistent annotations \\




% \bottomrule
% \end{tabular}
%  }
% % \end{table}
% \end{table*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{ML/DL Applications in Surgical Tool Recognition}
\label{sec:tool_recognition}
The integration of ML technologies into surgical practices has catalyzed significant advances in computer-assisted interventions, improving both the precision and efficiency of procedures. These technologies facilitate a variety of applications, from diagnostic imaging to real-time operational guidance, fundamentally transforming the surgical landscape. The systematic deployment of various ML methodologies has great potential in improving surgical outcomes as well as streamlining both preoperative planning and postoperative care. As detailed in Table~\ref{tab:mlapplicationssurgery}, ML applications in surgery use a diverse range of techniques, including supervised learning for tissue classification, unsupervised anomaly detection, and DL for intricate tasks such as instrument segmentation and surgical skill evaluation. These techniques employ various models such as CNN for image-based analysis, SAM for zero-shot segmentation, and RNN for time series data, addressing specific needs within the surgical workflow. Table~\ref{tab:mlapplicationssurgery} categorizes these applications by specifying the ML methods used with the corresponding models and the data types utilized, ranging from histopathological images to surgical video data. It also outlines the metrics used to evaluate performance, such as accuracy, precision, recall, and the intersection over union (IoU). In particular, the table acknowledges the limitations inherent to each application, such as the high computational costs and the need for extensive data annotation, which are critical considerations for future research directions.

\begin{table*}[ht]
\centering
\caption{Comprehensive Summary of Tool Navigation Datasets (Chronologically Sorted)}
\label{tab:datasetcomp}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}p{0.8cm}p{2.5cm}p{2.2cm}p{2.5cm}p{1.5cm}p{1.8cm}p{3cm}p{3.2cm}@{}}
\toprule
\textbf{Year} & \textbf{Collection} & \textbf{Data Volume} & \textbf{Surgical Type} & \textbf{Access} & \textbf{Instrument Type} & \textbf{Label Types} & \textbf{Operational Tasks} \\ \midrule
2015 & FetalFlexTool & 21 Videos & Fetal Surgery & Public & Rigid & Bounding-box & Detection \\
2015 & NeuroSurgicalTools & 2476 Images & Neurosurgery & Public & Robotic & Bounding-box & Detection \\
2015 & EndoVis 15 & 9K Images & Colorectal Surgery & Public & Rigid & Pixel-wise, 2D pose & Segmentation, Tracking \\
2016 & Cholec80 & 80 Videos & Cholecystectomy & Public & Rigid & Bounding Box & Detection, Activity, Skills \\ 
2016 & M2CAI16-tool & 16 Videos & Cholecystectomy & Public & Rigid & Tool presence & Detection, Phase \\ 
2017 & Hamlyn & 2 Phantom & Cardiac & Public & NA & Depth map & Tracking \\ 
2017 & ATLAS Dione & 8 Videos & In-vitro Experiments & Public & Robotic & Bounding Box & Detection, Activity \\ 
2017 & EndoVis 17b & 10 Videos & Porcine & Public & Robotic & Pixel-wise & Segmentation, Binary, Parts \\
2018 & m2cai16-tool locations & 16 Videos & Cholecystectomy & Public & Rigid & Bounding-box & Detection \\ 
2018 & LapGyn4 & 55K Images & Gynecologic Surgery & Public & Rigid & No annotation & Multiple \\ 
2018 & EndoVis 18d & 14 Videos & Nephrectomy & Public & Robotic & Pixel-wise mask & Scene Segmentation \\ 
2019 & SCAREDf & 27 Videos & Porcine & NA & NA & Depth + mask & Depth estimation \\ 
2019 & Cata7 & 7 Videos & Cataract Surgery & Private & Rigid & Pixel-wise mask & Segmentation, 3D Reconstruction \\ 
2019 & UCL & 16016 Synthetic Images & Sigmoid resection, colonoscopy & NA & NA & Depth map & Depth estimation \\
2019 & ROBUST-MIS19 & 30 Videos & Proctocolectomy, rectal & Public & Rigid & Instances & Segmentation(Binary,Parts) \\ 
2020 & LapSig300 & 300 Videos & Colorectal Surgery & Private & Rigid & Pixel-wise mask & Segmentation, Parts, Action Recognition \\ 
2020 & Sinus Surgery-L & 3 Videos & Sinus-Live & Public & Rigid & Pixel-wise mask & Segmentation, Phase, Action Recognition \\ 
2020 & Sinus Surgery-C & 10 Videos & Sinus-Cadaver & Public & Rigid & Pixel-wise mask & Segmentation(Binary) \\ 
2020 & UCL dVRK & 20 Videos + Kinematic Data & Ex-Vivo & Public & Robotic & Camera parameters & Segmentation(Binary) \\ 
2020 & HeiSurF & 24 Videos, 9 Test Surgeries & Laparoscopic gallbladder resections & Public & Rigid & Segmentation masks, Instrument classes & Full scene segmentation, Workflow analysis \\
2021 & KvasirInstrument & 590 endoscopic frames & Gastrointestinal Endoscopy & Public & Rigid & Binary masks, Bounding boxes & Tool Segmentation \\ 
2021 & RoboTool & 514 Video Frames, 14720 Synthetic Images & Various Surgical & Public & Robotic & Binary labels, segmentation masks & Synthetic dataset creation \\ 
2021 & I2I Translation & 200,000 Images & Laparoscopic Surgery & NA & Rigid & Style transformation masks & Image style translation \\ 
2021 & SCARED & 9 Datasets & Porcine & Public & Robotic & Depth & Depth estimation \\ 
2021 & CaDTD & 50 Videos & Cataract Surgery & Private & Rigid & Bounding-box & Detection \\
2021 & dVPN & 48702 Images & Nephrectomy & NA & NA & TP, A, SC, Ph & Detection, Action Recognition \\ 
2021 & EndoVis 21 & 33 Videos & Cholecystectomy & Public & Rigid & TP, A, SC, Ph & Detection, Phase, Action Recognition \\ 
2021 & AutoLaparo & 21 Videos & Laparoscopic Hysterectomies & Public & Rigid & Annotated for uterus and instruments & Workflow recognition, Motion prediction, Anatomy segmentation \\ 
2021 & ART-Net & 29 Procedures & Laparoscopic Hysterectomies & Public & Non-robotic & Binary segmentation, tool presence & Instrument segmentation, geometry annotation \\
\bottomrule
\end{tabular}
}
\end{table*}







% \section{Instrument Understanding}
% Instrument Understanding in surgical scene understanding employs machine learning techniques to enhance the recognition and manipulation of surgical instruments within the operating theater. This category is crucial for supporting both manual and robotic-assisted surgeries, improving safety, efficiency, and precision. The sub-categories within Instrument Understanding include:

% \subsection{Surgical Instrument Recognition}


% % \subsubsection{EndoVis Challenge Series} 
% Since 2015, the MICCAI Endoscopic Vision (EndoVis) challenges have significantly contributed to advancing AI research in endoscopic and laparoscopic tool detection. The EndoVis series includes datasets tailored to different tasks, such as segmentation, tracking, and classification. The inaugural EndoVis-2015 challenge \cite{bouget2015detecting} focused on segmenting and tracking rigid surgical instruments. Later challenges, including EndoVis-2017, introduced videos for binary and multi-class instrument segmentation. By 2018, the challenges included entire scene segmentation tasks, requiring the segmentation of robotic and non-robotic tools along with anatomical structures \cite{allan20202018}. The 2019 Robust-MIS challenge \cite{ross2021comparative} included a diverse dataset with 30 surgical procedures to enhance robustness and generalization, focusing on binary segmentation and instance segmentation of surgical tools. The SCARED (Stereo Correspondence and Reconstruction of Endoscopic Data) sub-challenge emphasized depth estimation, while the 2020 and 2021 challenges tackled domain adaptation and workflow recognition, respectively.



% \subsubsection{M2CAI16 Challenge} 
% The M2CAI16 Challenge features two datasets for surgical workflow and tool detection. The m2cai-tool dataset \cite{twinanda2016endonet} includes 15 cholecystectomy videos10 for training and 5 for testingcaptured at the University Hospital of Strasbourg. It provides binary indicators for the presence of seven surgical tools. Later, the dataset was expanded to m2cai-tool-locations \cite{jin2018tool}, adding spatial annotations for 2,532 frames to facilitate tool localization.

% % \subsubsection{Cholec80 and Extensions} 
% Developed by Twinanda et al. \cite{twinanda2016endonet}, the Cholec80 dataset consists of 80 cholecystectomy videos performed by 13 surgeons, with annotations for tool presence and surgical phase recognition. The dataset includes 86,000 annotated images, with tool bounding boxes available for 10 videos. A later extension, the ITEC Smoke Cholec80 Image dataset \cite{aksamentov2017deep}, adds 100,000 frames annotated for smoke and non-smoke scenarios, specifically curated for research on smoke removal.

% % \subsubsection{KvasirInstrument Dataset} 
% The Kvasir-Instrument dataset \cite{jha2021kvasir} includes 590 annotated endoscopic frames from gastroscopies and colonoscopies, with binary segmentation masks and bounding boxes for diagnostic and therapeutic tools. The image resolutions range from 720576 to 12801024, covering instruments such as snares, balloons, and biopsy forceps.

% % \subsubsection{ART-Net} 
% The ART-Net dataset \cite{hasan2021detection} contains 29 videos of laparoscopic hysterectomy procedures using non-robotic instruments, recorded at a resolution of 19201080. In addition to binary segmentation annotations, the dataset includes geometric data for each instrument, enhancing its utility for tool presence detection.

\subsection{Instrument Segmentation}
Instrument segmentation is crucial in medical image analysis, especially in robotic-assisted surgery, where it involves accurately differentiating surgical instruments from the background and other elements within the scene. The effectiveness of robotic surgery systems hinges on precise visual information provided by segmentation algorithms, primarily based on CNNs. These algorithms process images in real-time to deliver clear outlines of surgical instruments, which is essential for tasks like automated tool tracking and interaction handling in robotic surgeries.

A significant contribution to this field is detailed by Garca-Peraza-Herrera et al. \cite{garcia2017real}, who explore CNN-based methods for real-time surgical tool segmentation in laparoscopic videos. This study underscores the improvements DL models bring to surgical tool segmentation, improving clarity and operational efficiency in robotic surgeries, which are critical to ensuring safety and precision.

In Table~\ref{table:instrumentsegmentation}, the advanced models are listed for instrument segmentation, covering a spectrum from supervised learning with models like FCN and U-Net to complex methods involving adversarial networks and domain adaptation. These models are evaluated on datasets like EndoVis and various private datasets, utilizing techniques that combine CNNs with RNNs and residual networks.

Transitioning from traditional segmentation techniques, the Min-max Similarity model introduces a novel contrastive and semi-supervised learning framework that marks a significant advancement in medical imaging. This model addresses the challenge of limited annotated data \cite{lou2023min} by maximizing the similarity between representations of similar objects while minimizing those of dissimilar ones. The innovative approach improves the ability to distinguish between surgical tools and complex backgrounds, leveraging unlabeled data effectively to overcome the scarcity of labeled datasets. Its semi-supervised nature facilitates generalization across different surgical tools and procedures, reducing the need for extensive retraining. This method not only advances technical capabilities, but also provides substantial clinical benefits by enhancing the autonomy of robotic surgical systems and potentially reducing the cognitive load on surgeons, thus increasing surgical safety.

Building on these advances, the development of Efficient Class-Promptable Surgical Instrument Segmentation represents a further evolution in the field. Models like the SAM prompting system can be directed toward segment-specific classes of surgical instruments through simple prompts or minimal user input such as drawing a bounding box or a point prompt of the target object \cite{yue2024surgicalsam}. This capability is crucial in surgical settings, where the types of tools and the visual environment can vary significantly between procedures. Such models allow for rapid adaptation to new instruments without the need for extensive retraining, improving their practical utility in the operating room. This adaptability not only increases segmentation accuracy but also reduces computational demands, making real-time implementation more feasible \cite{twinanda2016endonet}. 

Further enriching the field, advances in foundational models, particularly those based on Transformer architectures, have significantly enhanced surgical image segmentation. These models utilize self-attention mechanisms to effectively manage complex spatial relationships in medical imaging, complementing traditional techniques like CNNs \cite{dosovitskiy2021imageworth16x16words, dosovitskiy2020image}. Innovations such as the SAM demonstrate the adaptability required for dynamic surgical environments, crucial for applications where rapid changes demand reliable model performance \cite{kirillov2023segment, zhou2019fast}. Moreover, the integration of these advanced models into augmented reality systems provides surgeons with context-sensitive real-time information that improves both precision and safety during surgical procedures \cite{kunz2022augmented}. These would act as clinical decision support tools that highlight features that link real-time images with preoperative CT or MRI scans. For example, pre-operative CT detection of an enlarged lymph node with potential cancer cells in the mesentery next to the ileo-colic or middle colic artery could be highlighted in real time when performing an extended right hemicolectomy (removal of the right and transverse colon). This would help the surgeon achieve a clear surgical margin and reduce the risk of cancer recurrence. 

The recent development of the Video-Instrument Synergistic Network (VISN) underscores a significant advancement in leveraging machine learning for robotic surgeries, as explained by Wang et al. \cite{wang2024video}. VISN is specifically designed to handle the complexities of video-based instrument segmentation in this field. Taking advantage of synergistic interactions between video frames and instrument-specific features, it significantly enhances segmentation accuracy. VISN leverages the dynamic relationships and spatial-temporal consistency of surgical instruments within video sequences, achieving precise segmentation critical for automating robotic surgery. Its core strength lies in its ability to parse and integrate contextual information from continuous video feeds, seamlessly adapting to the variable and visually complex environment of robotic surgeries. This adaptation ensures high accuracy in instrument detection and segmentation despite challenges like variable lighting, occlusions, and the presence of blood and tissue fluids. VISN not only enhances the safety and efficacy of robotic-assisted surgeries by integrating into surgical workflows but also reduces surgeons' cognitive load, potentially improving patient outcomes and surgical efficiency.


% Instrument segmentation is crucial in medical image analysis, especially in robotic surgeries, where it entails accurately differentiating surgical instruments from the background and other elements within the scene. The effectiveness of robotic surgery systems hinges on precise visual information provided by segmentation algorithms, primarily based on CNNs. These algorithms process images in real-time to deliver clear outlines of surgical instruments, which is essential for tasks like automated tool tracking and interaction handling in robotic surgeries.

% A significant contribution to this field is detailed by Garca-Peraza-Herrera et al. \cite{garcia2017real}, who explore CNN-based methods for real-time surgical tool segmentation in laparoscopic videos. This study underscores the enhancements DL models bring to surgical tool segmentation, improving clarity and operational efficiency in robotic surgeries, which are critical for ensuring safety and precision.

% In Table~\ref{table:instrumentsegmentation}, the advanced models are listed for instrument segmentation, covering a spectrum from supervised learning with models like FCN and U-Net to complex methods involving adversarial networks and domain adaptation. These models are evaluated on datasets like EndoVis and various private datasets, utilizing techniques that combine CNNs with RNNs and residual networks. The subsequent sections will delve further into these surgical tool segmentation approaches.











\begin{table}[h!]
  \centering
  \caption{State-of-the-Art Models for Surgical Instrument Segmentation of surgical tools and instruments within various surgical environments, as seen in robotic and laparoscopic surgeries.}
  \label{table:instrumentsegmentation}
  \begin{tabular}{>{\raggedright\arraybackslash}p{0.20\columnwidth} >{\raggedright\arraybackslash}p{0.20\columnwidth} >{\raggedright\arraybackslash}p{0.20\columnwidth} >{\raggedright\arraybackslash}p{0.20\columnwidth}}
    \hline
    \textbf{Reference} & \textbf{Dataset} & \textbf{Model} & \textbf{Technique} \\
    \hline
    Garcia-Peraza-Herrera et al. \cite{garcia2017real}& EndoVis15, NST, FFT & FCN-8s+ & Supervised\\
    Attia et al. \cite{attia2017surgical} & EndoVis15 & CNN + RNN & Supervised \\
    Garcia-Peraza-Herrera et al. \cite{garcia2017toolnet} & DVR & ToolNet & Supervised\\
    Shvets et al. \cite{shvets2018automatic} & EndoVis-17 & Ternaus11, Ternaus16, LinkNet34 & Supervised\\
    Milletari et al. \cite{milletari2018cfcm} & EndoVis15 & ResNet +, Conv LSTM & Supervised \\
    Ross et al. \cite{ross2018exploiting} & EndoVis17 & ResNet, U-Net & Semi-Supervised \\
    Pakhomov et al. \cite{pakhomov2019deep} & EndoVis15 & ResNet+atrous & Supervised \\
    Islam et al. \cite{islam2019real} & EndoVis17 & CNN+Residual & Auxilary, Adversarial \\
    Lee et al. \cite{lee2019weakly} & Private & DCNN & Weakly-Supervised \\
    Fuentes-Hurtado et al. \cite{fuentes2019easylabels} & EndoVis15 & DeepLabv3+ & Weakly-Supervised \\
    Mohammed et al. \cite{mohammed2019streoscennet} & EndoVis17 & StreoScenNet & Supervised \\
    Jin et al. \cite{jin2019incorporating} & EndoVis17 & MF-TAPNet & Supervised, Self-supervised \\
    Yu et al. \cite{yu2020holistically} & EndoVis17 & Modified U-Net & Supervised \\
    Gonzalez et al. \cite{gonzalez2020isinet} & EndoVis17,18 & ISINet & Supervised \\
    Ni et al. \cite{ni2020barnet} & Cata7, EndoVis17 & BarNet & Supervised \\
    Sahu et al. \cite{sahu2020endo} & Sim, Sim Cholec80, EndoVis15 & DNN+, Ternaus11 & Unsupervised Domain Adaptation\\
    Zhao et al. \cite{zhao2021anchor} & Davis16, EndoVis17,18, HKPWH & AOMA & Meta-Learning \\
    Peng et al. \cite{peng2024reducing} & UW Sinus, EndoVis17 & DeepLabv3+, MobileNet & Active-Learning \\
    Su et al. \cite{su2021local} & Sinus Surgery & CycleGAN & Adversarial\\
    Zhang et al. \cite{zhang2021surgical} & Private, EndoVis17 Synthetic & U-Net+PatchGAN & Adversarial\\
    Colleoni et al. \cite{colleoni2020synthetic} & Custom & FCNN & Supervised\\
    Lin et al. \cite{lin2020lc} & Sinus Surgery & LC-GAN & Adversarial\\
    \hline
  \end{tabular}
\end{table}


% \subsubsection{Min-max Similarity Segmentation}



% The Min-max Similarity model marks a significant advancement in medical imaging by introducing a contrastive and semi-supervised learning framework for surgical tool segmentation, addressing challenges like limited annotated data \cite{lou2023min}. This model employs contrastive learning principles alongside semi-supervised techniques to enhance segmentation performance by maximizing the similarity between representations of similar objects while minimizing those of dissimilar ones. This approach sharpens the ability to distinguish between surgical tools and complex backgrounds, effectively leveraging unlabeled data to overcome the common obstacle of scarce labeled datasets. Its contrastive nature not only facilitates learning from labeled data but also captures intrinsic patterns from unlabeled data, boosting its learning efficacy. The model's semi-supervised nature allows it to generalize across various surgical tools and procedures, minimizing the need for extensive retraining. This innovation not only propels technical advancements but also offers substantial clinical benefits, enhancing robotic surgical systems' capabilities and supporting the development of more autonomous surgical assistance, thereby potentially reducing surgeons' cognitive load and increasing surgical safety.






% \subsubsection{Efficient Class-Promptable Surgical Instrument Segmentation}
% Recent advancements in medical image analysis have emphasized enhancing the efficiency and adaptability of segmentation models for surgical instruments. By developing models like the SAM prompting, they can be directed to segment specific classes of surgical instruments through simple prompts or minimal user input like drawing a bounding box or a point prompt of target object \cite{yue2024surgicalsam}. This capability is crucial in surgical settings, where the types of tools and the visual environment can vary significantly between procedures. Such promptable models allow for rapid adaptation to new instruments without the need for extensive retraining, improving their practical utility in the operating room. This adaptability not only increases segmentation accuracy but also reduces computational demands, making real-time implementation more feasible \cite{twinanda2016endonet}. The efficient class promptable segmentation models can enhance surgical navigation systems, robotic surgery, and augmented reality applications by providing precise instrument localization and tracking. Many researchers are exploring techniques such as transfer learning, interactive segmentation methods, and incorporation of domain-specific knowledge to optimize these models \cite{gao2022rgb}. The goal is to develop segmentation tools that are both highly accurate and capable of operating within the practical constraints of surgical environments.


% \subsubsection{Emerging Models and Techniques in Surgical Image Segmentation}
% Advancements in foundational models, particularly those based on Transformer architectures, have significantly enhanced surgical image segmentation. These models utilize self-attention mechanisms to manage complex spatial relationships in medical imaging effectively, complementing traditional techniques like CNNs \cite{dosovitskiy2021imageworth16x16words, dosovitskiy2020image}. Innovations such as the SAM demonstrate the adaptability required for dynamic surgical environments, crucial for applications where rapid changes demand reliable model performance \cite{kirillov2023segment, zhou2019fast}. Moreover, the integration of these advanced models into augmented reality systems provides surgeons with real-time, context-sensitive information that enhances both precision and safety during surgical procedures \cite{kunz2022augmented}.


% \subsubsection{Video-instrument Segmentation}

% The Video-Instrument Synergistic Network (VISN) represents a significant advancement in machine learning applications within robotic surgery, specifically designed to handle the complexities of this field. Thoroughly explained by Wang et al. \cite{wang2024video}, VISN leverages the synergistic interactions between video frames and instrument-specific features to enhance segmentation accuracy significantly. It capitalizes on the dynamic relationships and spatial-temporal consistency of surgical instruments within video sequences, achieving precise segmentation critical for robotic surgery automation. Its core strength lies in its ability to parse and integrate contextual information from continuous video feeds, adapting seamlessly to the variable and visually complex environment of robotic surgeries. This adaptation ensures high accuracy in instrument detection and segmentation despite challenges like variable lighting, occlusions, and the presence of blood and tissue fluids. VISN not only enhances the safety and efficacy of robotic-assisted surgeries by integrating into surgical workflows but also reduces surgeons' cognitive load, potentially improving patient outcomes and surgical efficiency.



% The integration of machine learning into robotic surgery has reached an advanced stage with the development of the Video-Instrument Synergistic Network (VISN), a model crafted specifically to handle the complexities of robotic surgical applications. This innovative network leverages synergistic interactions between video frames and instrument-specific features to significantly enhance segmentation accuracy, as detailed in a recent study by Wang et al. \cite{wang2024video}. By capitalizing on the dynamic relationships and spatial-temporal consistency of surgical instruments within video sequences, VISN achieves precise segmentation crucial for automating tasks in robotic surgery.

% The core strength of the Video-Instrument Synergistic Network lies in its ability to parse and integrate contextual information from continuous video feeds, allowing it to adapt seamlessly to the highly variable and visually complex environment typical in robotic surgeries. This capability is paramount, as it ensures that the network can maintain high accuracy in instrument detection and segmentation despite the inherent challenges of surgical settings, such as varying lighting conditions, occlusions, and the presence of blood and tissue fluids.

% Furthermore, VISN represents a significant advancement in surgical tool segmentation technology, providing a more sophisticated, accurate, and adaptable system suitable for a wide range of surgical applications. As these technologies continue to evolve, they promise to further integrate into surgical workflows, thereby enhancing both the safety and efficacy of robotic-assisted surgeries. This continuous integration not only improves the operational aspects of surgeries but also reduces the cognitive load on surgeons, potentially leading to better patient outcomes and more efficient surgical procedures.

\begin{table}[h!]
  \centering
  \caption{Comprehensive Overview of State-of-the-Art Models for Instrument Detection for detecting surgical tools in dynamic surgical settings, showcasing their application on different datasets.}
  \label{table:instrumentdetection}
  \begin{tabular}{>{\raggedright\arraybackslash}p{0.20\columnwidth} >{\raggedright\arraybackslash}p{0.20\columnwidth} >{\raggedright\arraybackslash}p{0.20\columnwidth} >{\raggedright\arraybackslash}p{0.20\columnwidth}}
    \hline
    \textbf{Reference} & \textbf{Dataset} & \textbf{Model} & \textbf{Technique} \\
    \hline
    Twinanda et al. \cite{twinanda2016endonet}&  Cholec80, EndoVis15 &  EndoNeT & Supervised\\
    Teevno et al. \cite{teevno2023semi}&  m2cai16 &  Teacher-Student & Semi-Supervised\\
    Zhang et al. \cite{zhang2021surgical}&  AJU-Set, m2cai16-tool- locations &  FasterRCNN+, Region proposal Network & Supervised\\
    Namazi et al. \cite{namazi2022contextual}&  M2CAI16, Cholec-80 &  RCNN & Supervised\\
    Hu et al. \cite{hu2017agnet}& m2cai16-tool &   AGNet & Supervised\\
    Mishra et al. \cite{mishra2017learning}& m2cai16-tool &   CNN+LSTM & Supervised\\
    Kurmann et al. \cite{kurmann2017simultaneous}& RMIT, EndoVis15 &   CNN & Supervised\\
    Sarikaya et al. \cite{sarikaya2017detection}& ATLASDione &    CNN + RPN & Supervised\\
    Vardazaryan et al. \cite{vardazaryan2018weakly}& Cholec80 &   FCN & Weakly-supervised\\ 
    Yoon et al. \cite{yoon2020semi}& Private &  Faster, CascadeRCNN & Semi-supervised\\
    Kondo et al. \cite{kondo2021lapformer}&Cholec80 & CNN + Transformer & Supervised\\
    Alshirbaji et al. \cite{alshirbaji2021deep}&Cholec80 & CNN+LSTM & Supervised\\
    \hline
  \end{tabular}
\end{table}


\subsection{Instrument Detection}
Instrument detection in surgical settings is crucial for identifying surgical tools' presence and location within images or video frames, setting the foundation for tracking movements and recognizing surgical activities. Most of the detection models leverage sophisticated frameworks like Faster R-CNN or YOLO for efficient multi-object detection in the dynamic environments of surgical scenes.

Twinanda et al. \cite{twinanda2016endonet} introduced the EndoNet architecture, combining tool detection with phase recognition, exemplifying how multitask learning enhances surgical scene understanding by simultaneously tackling related tasks, thus improving the robustness and accuracy of tool identification and contextual understanding. In open surgery, DL models detect and localize tools within dynamic environments, which helps postoperative analysis and real-time decision making \cite{fujii2022surgical}. This capability not only improves surgical safety and efficiency by ensuring the correct use and placement of instruments, but also enhances surgical training by providing real-time feedback on tool handling.

Furthermore, integrating instrument detection with phase recognition, as further developed by Twinanda et al. \cite{twinanda2017endonet}, demonstrates the potential of combining multiple analytical tasks for an understanding of surgical workflow. Such systems contribute to automated documentation, improved situational awareness, and improved coordination of surgical procedures. Table \ref{table:instrumentdetection} summarizes various state-of-the-art models for instrument detection, including techniques and models like EndoNet, Faster R-CNN, and Cascade R-CNN in datasets such as Cholec80, EndoVis15, RMIT, and m2cai16-tool. This table illustrates the evolution of instrument detection technology, emphasizing advancements towards sophisticated, multitask learning frameworks that promise greater precision in real-time surgical tool detection.
% Instrument detection focuses on identifying the presence and precise location of surgical tools within an image or video frame. This task is foundational for subsequent processes such as tracking tool movements and recognizing surgical activities. Detection models typically employ advanced frameworks like Faster R-CNN or YOLO (You Only Look Once) to efficiently identify multiple objects (i.e., instruments) in the complex and dynamic environments characteristic of surgical scenes.

% Twinanda et al. \cite{twinanda2016endonet} introduced the EndoNet architecture, which integrates tool detection with phase recognition. This approach exemplifies how multitask learning can enhance the overall performance of surgical scene understanding systems by simultaneously addressing multiple related tasks, thereby improving the robustness and accuracy of tool identification and contextual understanding during surgical procedures.

% An exemplary implementation of instrument detection technology is demonstrated in the study of surgical tool detection in open surgery videos, where DL models are trained to detect and localize tools within the unstructured and dynamic environment of open surgical procedures \cite{fujii2022surgical}. In this application, the models analyze video data to recognize various tool types and their interactions with the surgical environment, providing valuable insights for both post-operative analysis and real-time decision support. This capability not only enhances the safety and efficiency of surgical operations by ensuring the appropriate use and placement of instruments but also supports surgical training by offering trainees real-time feedback on tool handling and usage, thereby facilitating the development of better surgical skills and practices.

% Furthermore, the integration of instrument detection with phase recognition, as demonstrated by Twinanda et al. \cite{twinanda2017endonet} underscores the potential of combining multiple analytical tasks to achieve a more comprehensive understanding of the surgical workflow. By accurately detecting tools and simultaneously recognizing the surgical phase, such systems can contribute to automated documentation, enhance situational awareness for surgical teams, and improve the overall coordination and efficiency of surgical procedures.
% Table \ref{table:instrumentdetection} details various state-of-the-art (SOTA) models for instrument detection in surgical environments, showcasing a variety of techniques and models, such as EndoNet, Faster R-CNN, and Cascade R-CNN applied across different datasets like Cholec80 and m2cai16-tool. This table highlights the evolution of instrument detection technology by listing both the techniques employed and the models developed for enhancing detection accuracy in varied surgical settings. Each entry reflects significant strides made in the field, emphasizing a move towards more sophisticated, multi-task learning frameworks that promise greater precision in real-time surgical tool detection.
% \begin{table}[h!]
%   \centering
%   % \caption{ Instrument Detection  State-of-the-Art  Models}
%   \caption{Comprehensive Overview of State-of-the-Art Models for Instrument Detection for detecting surgical tools in dynamic surgical settings, showcasing their application on different datasets.}

%   \label{tab:related_work}
%   \begin{tabular}{|>{\raggedright\arraybackslash}p{0.20\columnwidth}|>{\raggedright\arraybackslash}p{0.20\columnwidth}|>{\raggedright\arraybackslash}p{0.20\columnwidth}|>{\raggedright\arraybackslash}p{0.20\columnwidth}|}
%     \hline
%     \textbf{Reference} & \textbf{Dataset} & \textbf{Model} & \textbf{Technique} \\
%     \hline
%      Twinanda et al. \cite{twinanda2016endonet}&  Cholec80, EndoVis15
%  &  EndoNeT
%  & Supervised\\

%  Teevno et al. \cite{teevno2023semi}&  m2cai16
%  &  Teacher-Student
%  & Semi-Supervised\\

%   Zhang et al. \cite{zhang2021surgical}&  AJU-Set,
%    m2cai16-tool- locations
%  &  FasterRCNN+,
%  Region proposal 
%  Network
%  & Supervised\\
 
%  Namazi et al. \cite{namazi2022contextual}&  M2CAI16,
%  Cholec-80
%  &  RCNN
%  & Supervised\\

%   Hu et al. \cite{hu2017agnet}& m2cai16-tool
%  &   AGNet
%  & Supervised\\

%   Mishra et al. \cite{mishra2017learning}& m2cai16-tool
%  &   CNN+LSTM
%  & Supervised\\
 
%   Kurmann et al. \cite{kurmann2017simultaneous}& RMIT,
%    EndoVis15
%  &   CNN
%  & Supervised\\

%   Sarikaya et al. \cite{sarikaya2017detection}& ATLASDione
%  &    CNN + RPN
%  & Supervised\\

%   Vardazaryan et al. \cite{vardazaryan2018weakly}& Cholec80
%  &   FCN
%  & Weakly-supervised\\ 

%   Yoon et al. \cite{yoon2020semi}& Private
%  &  Faster,
%  CascadeRCNN
%  & Semi-supervised\\

% Kondo et al. \cite{kondo2021lapformer}&Cholec80 & CNN + Transformer
%  & Supervised\\

%   Alshirbaji et al. \cite{alshirbaji2021deep}&Cholec80 & CNN+LSTM
%  & Supervised\\
% \hline
%    \end{tabular}
% \end{table}


In addition to these applications, instrument detection technologies are instrumental in advancing robotic-assisted surgeries. Accurate detection and localization of surgical tools enable precise control and manipulation by robotic systems, thereby enhancing the surgeon's capabilities and reducing the likelihood of errors. The ability to monitor tool positions in real-time also facilitates the implementation of safety protocols, such as preventing unintended movements and ensuring that instruments are used within their designated areas.

\subsection{Instrument Tracking}
Instrument tracking in surgical environments involves continuous monitoring of surgical tools across video frames, critical for the effective operation of robotic systems and assistive technologies. This capability ensures accurate real-time localization of instruments, adapting seamlessly to surgeon actions and enhancing procedural safety and efficiency.

Table \ref{table:instrumenttracking} provides an overview of advanced state-of-the-art models for instrument tracking, including LinkNet and ST-MTL, which excel in complex tasks combining detection, segmentation, and tracking, essential for precise real-time instrument tracking in modern surgeries. Other models like U-Net and FCN are noted for their robust detection and accurate instrument positioning, which are crucial for maintaining performance in dynamic surgical settings. The table details the effectiveness of these models across diverse surgical environments, listing the specific datasets validated along with the tracking techniques used. This comprehensive information highlights how each model contributes to instrument tracking, offering insights into their operational efficiencies and applications.

\begin{table}[hbtp]
  \centering
  \caption{State-of-the-Art Models for Instrument Tracking of surgical instruments across video frames, highlighting their effectiveness in dynamic surgical environments.}
  \label{table:instrumenttracking}
  \begin{tabular}{>{\raggedright\arraybackslash}p{0.20\columnwidth} >{\raggedright\arraybackslash}p{0.20\columnwidth} >{\raggedright\arraybackslash}p{0.20\columnwidth} >{\raggedright\arraybackslash}p{0.20\columnwidth}}
    \hline
    \textbf{Reference} & \textbf{Dataset} & \textbf{Model} & \textbf{Technique} \\
    \hline
    Zhang et al. \cite{zhang2017real}&   m2cai16-tool & LinkNet & Detection and Segmentation\\
    Islam et al. \cite{islam2021st}&   EndoVis17 & ST-MTL & Tracking, Segmentation\\
    Allan et al. \cite{allan2015image}&  Self & Decision tree, Optical flow & Features\\
    Zhao et al. \cite{zhao2019real}& Private & CNN & Detection \\
    Sarikaya et al. \cite{sarikaya2017detection}&  ATLASDione & CNN+RPN, Fast RCNN & Detection \\
    Lejeune et al. \cite{lejeune2018iterative}& BRATS, EndoVis15,Cochlea & U-Net & Features\\
    Du et al. \cite{du2018articulated}& RMIT, EndoVis15 & FCN & Detection - Regression\\
    Nwoye et al. \cite{nwoye2019weakly}& Cholec80 & FCN+ ConvLSTM & Detection\\
    \hline
  \end{tabular}
\end{table}


% Instrument tracking extends beyond mere detection by continuously monitoring the movement of surgical tools across a sequence of video frames. This capability is essential in dynamic surgical environments, where the precise and real-time localization of instruments is critical for the effective functioning of robotic systems and assistive technologies. Accurate tracking ensures that these systems can seamlessly adapt to the surgeon's actions, enhancing both the safety and efficiency of surgical procedures.





% Table \ref{table:instrumenttracking} presents a detailed overview of state-of-the-art models for instrument tracking, showcasing their application across various dynamic surgical environments. This table includes advanced models such as LinkNet and ST-MTL, which are notable for their capabilities in handling complex tasks that combine detection, segmentation, and tracking. These models are crucial for providing precise real-time tracking of surgical instruments, an essential feature for modern surgical procedures. Other significant models listed, like U-Net and FCN, are recognized for their robust detection capabilities and precise regression of instrument positions, which is vital for maintaining accuracy in rapidly changing surgical scenes.

% The models and techniques outlined in Table \ref{table:instrumenttracking} have been selected for their demonstrated effectiveness in diverse settings, reflecting the latest advancements in the field. Each entry in the table not only specifies the model used but also details the datasets on which these models have been validated, along with the specific tracking techniques employed. This comprehensive approach ensures a deep understanding of how each model contributes to the field of instrument tracking, providing valuable insights into their operational efficiencies and areas of application.

\paragraph{Traditional Tracking Methods}
Traditional tracking methods such as the Kalman Filter are widely used in dynamic systems tracking due to their ability to predict state variable changes based on velocity and acceleration measurements. This method excels in environments with linear motion profiles and Gaussian noise, which can be reliably estimated under these specific conditions \cite{kalman1960new}. It uses a predictive model that iteratively updates state estimates, making it suitable for applications such as surgical tool tracking, where motions are typically linear. However, surgical instruments often display non-linear movement patterns with sudden changes in direction and speed, challenging traditional tracking methods. The assumptions of linearity and Gaussian noise integral to Kalman Filters may not apply in surgical contexts, leading to potential inaccuracies and impairing the filter's effectiveness in complex surgical maneuvers \cite{dissanayake2001solution}.



% Traditional tracking methodologies, like the Kalman Filter, have been extensively utilized in dynamic systems tracking due to their proficiency in predicting the temporal evolution of state variables based on current measurements of velocity and acceleration. The Kalman Filter is particularly adept in environments characterized by linear motion profiles and Gaussian-distributed noise, offering reliable estimations under these conditions \cite{kalman1960new}. This filtering technique constructs a predictive model by iteratively updating the state estimates of a system, making it well-suited for tracking applications where surgical tool dynamics can be closely represented with linear approximations.

% Nevertheless, the movement patterns of surgical instruments often exhibit non-linear characteristics and experience abrupt directional and speed variations, which pose substantial challenges for traditional tracking approaches. The fundamental assumptions of linearity and normality in Kalman Filters may not hold in surgical settings, leading to potential inaccuracies. Such deviations from expected motion patterns significantly impair the filter's ability to provide consistent and precise tracking in complex surgical maneuvers \cite{dissanayake2001solution}.


\paragraph{DL-based Methods}
To address the limitations of traditional tracking methods, advanced DL-based methodologies using neural networks have been developed, utilizing temporal consistencies and contextual data to improve the accuracy of the tracking. DL architectures like RNNs and Long Short-Term Memory (LSTM) networks excel in capturing complex temporal dependencies and adapting to the unpredictable dynamics of surgical tool movements \cite{liu2023reproducing}. These models trained on extensive datasets of annotated surgical activities, generalize well across different surgical scenarios and instrument types. A key development by Allan et al. \cite{iovene2024hybrid} uses DL for real-time surgical tool tracking, combining CNNs with LSTM layers to effectively model the spatial and temporal aspects of surgical instruments. This hybrid approach adeptly manages rapid movements and visual occlusions, which are common challenges in surgical settings, enhancing both the functionality of surgical assistive technologies and the precision of robotic instrument control, thus reducing operational errors and improving outcomes.


Furthermore, recent studies have explored Transformer-based models, which are renowned for handling long-range dependencies within sequential data \cite{vaswani2017attention}. These models, with their scalability and adaptability, are well-suited for the complex data involved in surgical instrument tracking. Transformers employ attention mechanisms to selectively focus on relevant data segments, significantly enhancing tracking accuracy across extended periods and diverse surgical environments.
% To overcome the constraints inherent in traditional tracking techniques, advanced methodologies utilizing neural networks have been developed. These approaches capitalize on temporal consistencies and contextual information to enhance tracking accuracy. Particularly, DL architectures such as RNNs and Long Short-Term Memory (LSTM) networks have demonstrated proficiency in capturing complex temporal dependencies and adjusting to the unpredictable dynamics of surgical tool movements \cite{liu2023reproducing}. These models are trained on extensive datasets of annotated surgical activities, enabling them to generalize across diverse surgical scenarios and various types of instruments.

% A seminal work by Allan et al.\ \cite{iovene2024hybrid} showcased the application of DL for real-time surgical tool tracking. Their method integrates CNNs with LSTM layers to concurrently model the spatial and temporal characteristics of surgical instruments. This hybrid model effectively addresses rapid movements and visual occlusions of tools---prevalent challenges in surgical environments. The improvements in tracking accuracy and system responsiveness reported by Allan et al. not only enhance the functionality of surgical assistive technologies but also augment the precision in robotic instrument control, thereby diminishing the risk of operational errors and improving surgical outcomes.

% Moreover, contemporary research has investigated the application of Transformer-based models, noted for their ability to handle long-range dependencies within sequential data \cite{vaswani2017attention}. These architectures provide superior scalability and adaptability, suitable for managing the intricate and voluminous data typical in surgical instrument tracking. By employing attention mechanisms, Transformers can selectively concentrate on pertinent segments of the input data, significantly boosting their tracking accuracy over prolonged durations and in varied surgical settings.




\paragraph{Sensor-Based and Integrated Tracking Systems}
Sensor-based tracking systems augment vision-centric methodologies by incorporating data from electromagnetic trackers or Inertial Measurement Units (IMUs), adding dimensions of information that improve the robustness of the tracking \cite{nakawala2017toward}. These multimodal approaches counteract the limitations of vision-based systems, such as occlusions and variable lighting, by providing additional reliable sources of positional data. Integration of instrument tracking with detection and segmentation technologies is also critical, as it facilitates consistent and accurate representations of the surgical environment and allows functionalities such as automated tool re-identification, workflow analysis, and Augmented Reality (AR) overlays for surgical navigation \cite{ayobi2024pixel}. Despite advances, tracking in dynamic and cluttered surgical settings remains challenging due to factors such as tool occlusions, variable lighting, and the presence of biological tissues, along with the need for real-time processing, which imposes directions on computational efficiency. Therefore, developing sophisticated models that balance tracking accuracy with computational speed is essential to maintain robust performance without disrupting surgical workflows.



\begin{figure*}[hbtp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/SAM-Variants/DeSAM.png}
        \caption{DeSAM \cite{sheng2024surgical}}
        \label{figSAM:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/SAM-Variants/AdaptiveSAM.png}
        \caption{AdaptiveSAM \cite{paranjape2024adaptivesam}}
        \label{figSAM:sub2}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/SAM-Variants/CycleSAM.png}
        \caption{CycleSAM \cite{murali2024cyclesam}}
        \label{figSAM:sub3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/SAM-Variants/Surgical-SAM2.png}
        \caption{Surgical SAM2 \cite{liu2024surgical}}
        \label{figSAM:sub4}
    \end{subfigure}
    %\caption{Architectural overview of SAM variants for surgical video analytics, each tailored for specific segmentation and analysis tasks in surgical environments. Each image is adapted from corresponding references.}
\caption{Segmentation tasks in surgical video analytics addressed by SAM variants: (a) precise tool localization via detection and segmentation integration (\textit{DeSAM}); (b) improved segmentation in complex scenarios with task-specific prompts and multi-head attention (\textit{AdaptiveSAM}); (c) temporal consistency through reference-based segmentation (\textit{CycleSAM}); and (d) efficient video processing with memory-based frame pruning (\textit{Surgical SAM2}). These adaptations highlight SAM's versatility in tackling the challenges of surgical environments. Images adapted from corresponding references.}    
\label{fig:SAM}
\end{figure*}



% Beyond vision-centric methodologies, sensor-based tracking techniques have been investigated to augment and bolster the reliability of tracking systems. Integration of data from electromagnetic trackers or Inertial Measurement Units (IMUs) introduces supplementary dimensions of information, significantly enhancing the robustness of tracking systems \cite{nakawala2017toward}. These multimodal approaches address the inherent shortcomings of purely vision-based systems, such as occlusions and fluctuating lighting conditions, by providing additional, reliable sources of positional data.

% Furthermore, the synthesis of instrument tracking with detection and segmentation technologies is critical for achieving a comprehensive understanding of the surgical scene. This integration facilitates the maintenance of a consistent and accurate representation of the surgical environment, enabling advanced functionalities such as automated tool re-identification, workflow analysis, and Augmented Reality (AR) overlays for surgical navigation \cite{ayobi2024pixel}. Such integrated systems are instrumental in developing intelligent surgical assistants that can predict surgeon intentions, deliver real-time feedback, and improve coordination within the operating room.

% Despite these advancements, tracking surgical instruments remains fraught with challenges. The dynamic and cluttered nature of surgical settings, combined with variables such as tool occlusions, variable lighting, and the presence of biological tissues, significantly complicates the tracking process. Additionally, the necessity for real-time processing in surgical applications imposes stringent demands on the computational efficiency of tracking algorithms. To overcome these obstacles, it is imperative to develop sophisticated models that adeptly balance tracking accuracy with computational speed, ensuring robust performance without introducing latency that could disrupt the surgical workflow.

% \begin{table}[h!]
%   \centering
%   % \caption{ Instrument Tracking State-of-the-Art  Models}
%   \caption{State-of-the-Art Models for Instrument Tracking of surgical instruments across video frames, highlighting their effectiveness in dynamic surgical environments.}

%   \label{tab:related_work}
%   \begin{tabular}{|>{\raggedright\arraybackslash}p{0.20\columnwidth}|>{\raggedright\arraybackslash}p{0.20\columnwidth}|>{\raggedright\arraybackslash}p{0.20\columnwidth}|>{\raggedright\arraybackslash}p{0.20\columnwidth}|}
%     \hline
%     \textbf{Reference} & \textbf{Dataset} & \textbf{Model} & \textbf{Technique} \\
%     \hline
    
%     Zhang et al. \cite{zhang2017real}&   m2cai16-tool
%  &  LinkNet
%  & Detection and Segmentation\\
%  \vspace{0.5mm}
%  Islam et al. \cite{islam2021st}&   EndoVis17
%  & ST-MTL
%  & Tracking,
%  Segmentation\\
%  \vspace{0.5mm}
% Allan et al. \cite{allan2015image}&  Self
   
%  & Decision tree,
%  Optical flow
 
 
%  & Features\\
%   \vspace{0.5mm}
 
%  Zhao et al. \cite{zhao2019real}& Private
 
%  & CNN 
%  & Detection \\
%   \vspace{0.5mm}
 
%   Sarikaya et al. \cite{sarikaya2017detection}&  ATLASDione
%  &   CNN+RPN,
%  Fast RCNN
%  & Detection \\
%   \vspace{0.5mm}

%   Lejeune et al. \cite{lejeune2018iterative}& BRATS,
%    EndoVis15,Cochlea
%  &  U-Net 
%  & Features\\
 
 
%  Du et al. \cite{du2018articulated}& RMIT,
%  EndoVis15
%  &  FCN 
%  & Detection - Regression\\
 

%  Nwoye et al. \cite{nwoye2019weakly}& Cholec80 &   FCN+ ConvLSTM
 
 
 
%  & Detection\\


% \hline
%    \end{tabular}
% \end{table}



\subsection{Instrument Pose Estimation}

% Instrument Pose Estimation determines the orientation and precise position of surgical tools, which is essential for accurate manipulations, particularly in minimally invasive surgeries. Accurate pose estimation enhances the control and coordination of surgical instruments, thereby improving the effectiveness and safety of surgical procedures. This task utilizes various techniques, including regression models and pose detection frameworks, to provide spatial coordinates and orientation details of instruments relative to the surgical site \cite{kehl2017real}.

% \subsubsection{Techniques and Methodologies}
Pose estimation in surgical settings involves traditional computer vision and modern deep learning-based methods. Traditional techniques using feature detection and geometric modeling often employ regression-based approaches such as Support Vector Regression (SVR) and Random Forests to predict pose parameters directly from images \cite{kehl2017real}. However, these methods may struggle with the variability of the surgical environment, such as instrument occlusions and various orientations. Deep learning advances have greatly improved the accuracy and robustness of pose estimation. Fully Convolutional Networks (FCNs) are used for articulated pose estimation, detecting multiple key points and their spatial relationships \cite{du2018articulated}. Furthermore, RNNs and Long Short-Term Memory (LSTM) networks also incorporate temporal information from video sequences to enhance real-time pose tracking \cite{laina2017concurrent}. These architectures manage temporal dependencies and motion dynamics, ensuring consistent pose estimations across frames, even in the presence of motion blur and occlusions.
% Pose estimation techniques can be broadly categorized into traditional computer vision methods and modern deep learning-based approaches. Traditional methods often rely on feature detection and geometric modeling. Regression-based approaches, such as Support Vector Regression (SVR) and Random Forests, predict pose parameters directly from input images by learning the mapping between visual features and pose coordinates \cite{kehl2017real}. While effective, these methods can struggle with the variability and complexity of surgical environments, where instruments may be occluded or appear in diverse orientations.

% Advancements in DL have significantly enhanced pose estimation accuracy and robustness. Fully Convolutional Networks (FCNs) have been employed to perform articulated pose estimation of surgical tools, enabling the detection of multiple keypoints and their spatial relationships \cite{du2018articulated}. Du et al.\ \cite{du2018articulated} demonstrated that FCN-based models could accurately predict the poses of multi-jointed instruments, thereby improving surgical precision. Additionally, RNNs and Long Short-Term Memory (LSTM) networks have been utilized to incorporate temporal information from video sequences, enhancing pose tracking in real-time applications \cite{laina2017concurrent}. These architectures capture temporal dependencies and motion dynamics, maintaining consistent pose estimations across consecutive frames despite challenges like motion blur and occlusions.

% \subsubsection{Applications and Impact on Surgical Outcomes}
Accurate instrument pose estimation is critical to improving various aspects of surgical practice. In robotic-assisted surgeries, it provides precise control of robotic arms, allowing surgeons to execute complex maneuvers with a reduced risk of instrument clash \cite{cepolina2022introductory}. Pose estimation also supports AR systems in operating rooms by overlaying essential information onto the surgeon's field of view, thus improving situational awareness and navigation \cite{chidambaram2021applications}. Furthermore, in surgical training and simulation, it enables the creation of realistic simulators that offer real-time feedback on instrument handling and techniques, improving training effectiveness \cite{zhao2019real}. Despite its benefits, pose estimation in surgical settings faces challenges such as tool occlusions, variable lighting, and complex backgrounds. Real-time performance without compromising accuracy remains essential for practical application. Future research aims to enhance the robustness and adaptability of pose estimation systems through the use of unsupervised and semi-supervised learning techniques, the integration of multimodal data, and the incorporation of domain-specific knowledge to meet the unique requirements of surgical environments \cite{nakawala2017toward}.

% Accurate instrument pose estimation has profound implications for various aspects of surgical practice. In robotic-assisted surgeries, precise pose information is crucial for the accurate control and manipulation of robotic arms, enabling surgeons to perform intricate maneuvers with reduced risk of error \cite{cepolina2022introductory}. Pose estimation also facilitates Augmented Reality (AR) systems in the operating room by overlaying critical information onto the surgeon's field of view, enhancing situational awareness and navigation \cite{chidambaram2021applications}. Furthermore, in surgical training and simulation, accurate pose estimation enables the development of realistic simulators that provide trainees with real-time feedback on instrument handling and technique, thereby improving the training process \cite{zhao2019real}.

% Despite these advancements, instrument pose estimation in surgical environments faces challenges such as tool occlusions, varying lighting conditions, and the presence of complex backgrounds. Achieving real-time performance without compromising accuracy remains a critical requirement for practical applications. Future research is likely to focus on enhancing the robustness and adaptability of pose estimation systems by leveraging unsupervised and semi-supervised learning techniques, integrating multimodal data sources, and incorporating domain-specific knowledge to better address the unique demands of surgical environments \cite{nakawala2017toward}.



\subsection{Anomaly Detection and Safety Measures}
Anomaly detection and safety measures use ML to identify deviations from normal operations that can indicate hazardous situations, in order to enhance surgical safety through alerts or corrective actions. Kayan et al. \cite{kayan2024casper} explored how the integration of anomaly detection systems in robotic surgery can preemptively address risks by recognizing unusual tool movements or unsafe interactions.


% Anomaly Detection and Safety Measures involve using machine learning to recognize patterns or actions that deviate from normal operations, potentially indicating hazardous situations. These systems are designed to enhance surgical safety by providing alerts or initiating corrective actions when anomalies are detected.
% Kayan et al. \cite{kayan2024casper} discuss the integration of anomaly detection systems in robotic surgery, highlighting how these systems can preemptively address risks by recognizing unusual tool movements or unsafe interactions.
\subsubsection{Complication Detection}
% Complication Detection within the framework of Instrument Understanding in surgical scene understanding focuses on the proactive identification of potential complications during surgical procedures. This critical task leverages machine learning technologies to analyze surgical data in real-time, detecting signs that may precede complications, such as unexpected bleeding, tissue distress, or improper instrument handling. Ruan et al. \cite{ruan2022real} discuss the use of machine learning in the operating room to monitor the surgical process continuously, detecting anomalies that could lead to complications. By integrating data from various sources, including surgical tools and physiological monitors, these models provide a holistic view of the surgery, enhancing the detection capabilities.

Complication Detection focuses on identifying potential complications during surgeries, such as unexpected bleeding or improper instrument handling, using machine learning to analyze real-time surgical data. Ruan et al. \cite{ruan2022real} highlight continuous monitoring of the surgical process, integrating data from surgical tools and physiological monitors to enhance the detection capabilities of complications.
\subsubsection{Error Prediction} 
% Error Prediction in Instrument Understanding involves forecasting potential errors in the handling or functioning of surgical instruments before they occur. This aspect of machine learning in surgery aims to enhance surgical safety and efficiency by preventing errors that could compromise the procedure or patient safety. Miao et al. \cite{miao2024predictive}, DL models were employed to predict errors in real-time during robotic surgeries. The study showcased how predictive analytics could effectively reduce error rates by providing early warnings to surgeons about potential instrument misuse or failure.

Error Prediction forecasts potential errors in the handling or functioning of surgical instruments to enhance safety and efficiency. Miao et al. \cite{miao2024predictive} demonstrate how DL models can predict errors during robotic surgeries in real time, effectively reducing error rates by alerting surgeons to potential instrument misuse or failure before they compromise the procedure.

%this change


\subsection{Segmentation using Foundation Models}

Building upon the foundation laid by traditional ML and DL techniques in surgical applications, this section shifts focus to foundational models, specifically the Segment Anything Model (SAM). In surgical applications, SAM is being refined for precise tool segmentation. This involves fine-tuning the model with surgical-specific datasets and incorporating domain-specific knowledge to enhance precision without sacrificing generalization across different surgical setups \cite{oguine2024generalization}. High-resolution surgical images and detailed annotations are used in training to help the model learn the nuanced features of surgical instruments and tissues \cite{shvets2018automatic}. Techniques such as transfer learning and domain adaptation adjust the model's parameters for surgical contexts, with few-shot learning enabling performance improvements with limited data \cite{twinanda2016endonet}. These advancements contribute to more precise segmentation outputs, which is crucial for real-time instrument tracking, surgical navigation, and augmented reality applications in the operating room \cite{deng2023segment}. Below are some variants of the SAM architecture as shown in Fig \ref{fig:SAM}:

\subsubsection{Surgical-DeSAM}
The Surgical-DeSAM explores the decoupling of the SAM's components to better suit robotic surgical environments, where precision and reliability are paramount. By modularizing the segmentation tasks, DeSAM enhances the robustness and accuracy of instrument segmentation, which is critical for automated or semi-automated robotic surgeries \cite{sheng2024surgical}. Decoupling allows each module to be fine-tuned individually, enabling more precise control over the segmentation process and facilitating the integration of domain-specific knowledge relevant to robotic surgery \cite{qu2021surgical}. In such robotic surgeries, accurate instrument segmentation is essential for tasks such as motion tracking, collision avoidance, and providing visual feedback to the surgeon \cite{lin2020lc}. The complexity of surgical scenes, combined with the variability of instruments and tissues, presents significant challenges for segmentation models. By decoupling SAM's components, Surgical-DeSAM addresses these challenges by allowing for specialized processing of different aspects of the segmentation task. For instance, separate modules can handle the unique visual features of robotic instruments or adapt to varying lighting conditions in the surgical environment \cite{ross2018exploiting}. This modular approach also facilitates easier updates and maintenance of the system, as individual components can be improved or replaced without affecting the entire model \cite{eppler2023automated}.

\subsubsection{AdaptiveSAM}
AdaptiveSAM \cite{paranjape2024adaptivesam} represents an evolution in the application of the SAM for surgical segmentation, focusing on dynamically adjusting the segmentation parameters in response to the specific needs of each surgical video frame. This adaptability ensures high precision and relevance of the segmentation output, adapted to the specific characteristics of the surgical regions \cite{paranjape2024adaptivesam}. In surgical environments, conditions such as lighting, tissue appearance, and instrument presence can change rapidly, making static segmentation models less effective \cite{twinanda2017endonet}. AdaptiveSAM addresses this challenge by incorporating mechanisms that fine-tune the model on the fly, enhancing its responsiveness to the dynamic surgical scene. One of the key features of AdaptiveSAM is its ability to efficiently utilize computational resources while maintaining high segmentation accuracy \cite{yang2022tmf}. By employing techniques like transfer learning and incremental learning, the model can adapt to new data without retraining from scratch \cite{ouyang2022self}. This is particularly important in surgical settings where real-time performance is crucial and computational resources may be limited. AdaptiveSAM's efficient tuning process allows it to provide immediate feedback to surgeons, assisting in tasks such as instrument tracking, tissue identification, and navigation \cite{maier2017surgical}.


\subsubsection{CycleSAM}
CycleSAM introduces a novel approach that leverages cycle-consistent feature matching to enable one-shot learning within the SAM, allowing the model to perform accurate segmentation from a single annotated example. This capability is particularly valuable in surgical settings where obtaining comprehensive training data is challenging due to the scarcity of annotated images and the high cost of expert labeling \cite{murali2024cyclesam}. In traditional segmentation models, a large amount of annotated data is required to achieve high accuracy, which is often impractical in medical contexts. CycleSAM addresses this limitation by using cycle-consistent adversarial networks to align features between the source domain (annotated examples) and the target domain (unlabeled surgical images) \cite{zhu2017unpaired}. This method ensures that the learned features are robust and transferable, enabling the model to generalize from a single example to a variety of surgical scenes. The loss of cycle consistency enforces a bidirectional mapping between the domains, preserving the structural integrity of surgical images while adapting to new contexts \cite{hoffman2018cycada}. This approach effectively mitigates the domain shift problem commonly encountered in medical image analysis, where variations in imaging conditions can significantly impact model performance \cite{chen2019synergistic}. By incorporating cycle-consistent feature matching, CycleSAM enhances SAM's ability to perform accurate segmentation without extensive retraining or additional data collection. This one-shot learning capability is crucial in surgical environments, where real-time decision-making is essential and the availability of annotated data is limited. CycleSAM's efficiency enables quicker deployment of segmentation models, facilitating applications such as surgical navigation, instrument tracking, and intraoperative guidance \cite{maier2017surgical}.  

\subsubsection{Surgical SAM 2}
Advancing the capabilities of the SAM, recent iterations focus on the real-time segmentation of surgical videos by employing efficient frame pruning techniques. These innovations reduce the computational load by selectively processing frames that contain significant changes or relevant surgical actions, enabling SAM to operate effectively in real-time surgical scenarios \cite{liu2024surgical}. This is crucial in dynamic surgical environments where rapid processing is essential for assisting surgeons without disrupting the workflow. By minimizing latency and ensuring timely feedback, efficient frame pruning enhances the practicality of SAM in the fast-paced setting of an operating room. Efficient frame pruning works by identifying and discarding redundant frames that do not contribute new information, thereby optimizing resource utilization without compromising segmentation accuracy \cite{gao2022rgb}. This approach allows the model to focus computational efforts on critical moments within the surgical video, improving both speed and efficiency. Researchers are also exploring the integration of motion detection algorithms and temporal consistency models to further enhance the performance of SAM in real-time applications \cite{twinanda2016endonet}. These advancements make it feasible to deploy sophisticated segmentation models in surgical settings, facilitating better surgical navigation and instrument tracking and potentially improving patient outcomes.
\subsection{Endoscopic and Laparoscopic Tool Detection Datasets}
The MICCAI Endoscopic Vision (EndoVis) challenge initiated in 2015 have played a crucial role in propelling AI research in endoscopic and laparoscopic tool detection. These challenges offer datasets for diverse tasks like segmentation, tracking, and classification. The availability of high quality datasets is vital in driving advances in the field.  Starting with rigid surgical instrument segmentation and tracking in 2015 \cite{bouget2015detecting}, subsequent challenges expanded to include binary and multi-class instrument segmentation, and by 2018, comprehensive scene segmentation tasks involving both robotic and non-robotic tools as well as anatomical structures were introduced \cite{allan20202018}. The 2019 Robust-MIS challenge elevated this with a dataset of 30 surgical procedures aimed at improving robustness and generalization in binary and instance segmentation of surgical tools \cite{ross2021comparative}. Additional focuses in recent years have included depth estimation in the SCARED sub-challenge, as well as domain adaptation and workflow recognition in the 2020 and 2021 challenges, respectively.
The M2CAI16 Challenge presents two datasets for surgical workflow and tool detection: the m2cai-tool dataset \cite{twinanda2016endonet} annotating the presence of seven surgical tools that feature 15 cholecystectomy videos from the University Hospital of Strasbourg where 10 are used for training and 5 for testing purpose. This dataset was later expanded to m2cai-tool-locations \cite{jin2018tool}, adding spatial annotations for 2,532 frames to aid tool localization. The Cholec80 dataset, developed by Twinanda et al. \cite{twinanda2016endonet}, includes 80 cholecystectomy videos by 13 surgeons, with 86,000 annotated images for tool presence and surgical phase recognition, including tool bounding boxes for 10 videos. It was extended by the ITEC Smoke Cholec80 Image dataset \cite{aksamentov2017deep}, which adds 100,000 frames annotated to differentiate smoke from non-smoke scenarios, aiding research on smoke removal in surgical environments. The Kvasir-Instrument dataset \cite{jha2021kvasir} comprises 590 endoscopic frames from gastroscopies and colonoscopies, providing binary segmentation masks and bounding boxes for various instruments at resolutions from 720576 to 12801024. Additionally, the ART-Net dataset \cite{hasan2021detection} includes 29 laparoscopic hysterectomy procedure videos recorded in 19201080 resolution, featuring binary segmentation and geometric data for non-robotic instruments, enhancing its application in tool presence detection.
All these details of the datasets are summarized in Table \ref{tab:datasetcomp}.


\section{ML/DL Applications in Surgical Workflow Analysis}
\label{sec:workflow}
Surgical Workflow Analysis critically employs ML to enhance the understanding and efficiency of surgical procedures by automating the recognition of phases and steps within surgeries. This analysis is key to improving efficiency, safety, and quality in surgical settings. Recent advancements in DL have significantly strengthened the ability to perform nuanced analyses of complex surgical activities. A detailed review by Demir et al. \cite{demir2023deep} highlights the crucial role of DL techniques in recognizing surgical phases and steps, employing advanced architectures such as CNNs, RNNs, and transformers to process sequential video data for identifying critical surgical actions and transitions. These models are particularly effective in complex environments rich in visual data.

The efficacy of these models in surgical workflow analysis is rigorously evaluated using benchmarks such as the HeiChole benchmark, which provides a standardized dataset from cholecystectomy procedures to validate and compare algorithms \cite{wagner2023comparative}. Such comparative analyses are vital for identifying superior computational approaches and fostering ongoing improvements in the field. In addition, there is an increasing emphasis on the application of these advanced analytical methods in specific surgical specialties, including open orthopedic surgery. This involves analyzing intraoperative video data to understand workflows, crucial for training, planning, and providing real-time operational support. Adapting DL models to the unique visual and procedural nuances of specialties like orthopedic surgery presents significant challenges, given the specialized instruments and complex movements involved.

% Surgical Workflow Analysis is a crucial domain within surgical scene understanding that leverages machine learning to analyze and optimize the sequence of events in surgical procedures. This analysis is pivotal for improving the efficiency, safety, and overall quality of surgeries. Surgical workflow analysis aims to enhance the understanding and efficiency of surgical procedures through the automated recognition of distinct phases and steps within a surgery. This field benefits significantly from advancements in machine learning, especially deep learning, which enables more nuanced and precise analysis of complex surgical activities.
% A comprehensive review of contemporary methodologies in surgical workflow analysis highlights the pivotal role of DL techniques in surgical phase and step recognition \cite{demir2023deep}. This review elucidates how advanced neural network architectures---including CNNs, RNNs, and transformers---are employed to process sequential video data, enabling the identification of critical surgical actions and transitions. These models exhibit exceptional effectiveness in complex surgical environments where visual data is abundant and intricate. The efficacy of various machine learning algorithms in surgical workflow analysis has been rigorously evaluated using benchmarks such as the HeiChole benchmark \cite{wagner2023comparative}. This benchmark provides a standardized dataset derived from cholecystectomy procedures, facilitating the validation and comparison of algorithms in terms of accuracy, efficiency, and applicability in real-world surgical settings. Such comparative studies are essential for pinpointing the most effective computational approaches and promoting continuous advancements in surgical workflow analysis technologies. Transitioning toward practical applications, there is a growing focus on implementing these advanced analytical techniques in specific surgical specialties, such as open orthopedic surgery. These applications involve the analysis of intraoperative video data to comprehend surgical workflows, which is crucial for surgical training, procedural planning, and real-time assistance during operations. A significant challenge lies in adapting DL models to accommodate the unique visual and procedural nuances of orthopedic surgery, which often involves specialized instruments and complex movements. 


% % \subsubsection{ATLASDione} 
% Compiled by Sarikaya et al. \cite{sarikaya2017detection}, the ATLASDione dataset comprises 99 videos documenting six robotic tasks conducted by 10 clinicians at the Roswell Park Cancer Institute using the da Vinci Surgical System. Annotations include tool bounding boxes, action types, durations, and the surgeons' skill levels, aiding in both tool detection and skill assessment.

% % \subsubsection{dVPN Dataset} 
% The dVPN dataset, compiled by Ye et al. \cite{ye2017self}, originates from da Vinci partial nephrectomy procedures. It provides 34,320 pairs of rectified stereo images for training and 14,382 pairs for testing, supporting stereo vision and depth estimation in robotic surgeries.

% % \subsubsection{AutoLaparo Dataset} 
% The AutoLaparo dataset \cite{wang2022autolaparo} includes 21 videos of laparoscopic hysterectomy procedures, each recorded in high definition (19201080). It contains sub-datasets for workflow recognition, laparoscope motion prediction, and segmentation of instruments and anatomical structures. Tool types annotated include grasping forceps, ligasure, dissecting forceps, and electric hooks.

% % \subsubsection{NeuroSurgicalTools} 
% The NeuroSurgicalTools dataset \cite{bouget2015detecting} consists of 14 videos recorded with Zeiss OPMI Pentero microscopes at 720x576 resolution. Captured during actual neurosurgeries at CHU Pontchaillou, Rennes, it features seven instruments with annotations that account for challenges such as occlusions, overlapping tools, blurring, and reflections.

% % \subsubsection{FetalFlexTool} 
% Developed by Garca-Peraza-Herrera et al. \cite{garcia2017real}, the FetalFlexTool dataset is an ex-vivo fetal surgery dataset with 21 training images and a 10-second testing video, annotated for tool segmentation. The training images were captured in air, while the testing video was recorded underwater to introduce varied lighting and backgrounds.

% % \subsection{Synthetic and Simulated Datasets}

% % \subsubsection{UCL and UCL dVRK} 
% The UCL dataset \cite{rau2019implicit} is synthetic, derived from a human CT colonography scan. Using segmentation and meshing, it offers 16,000 images and corresponding depth maps rendered with the Unity game engine. The UCL dVRK dataset \cite{colleoni2020synthetic} includes 14 videos, each containing 300 frames, featuring diverse animal tissues under varied lighting and backgrounds, with fractional Brownian motion applied to simulate noise.

% % \subsubsection{Laparoscopic Image-to-Image Translation Dataset}
% This dataset \cite{pfeiffer2019generating} includes 20,000 synthetic images derived from 3D laparoscopic simulations based on CT scans. Using style transformation, the dataset comprises 100,000 images that mimic various visual styles, enabling the testing of models under different scenarios.


% % \subsubsection{Sinus Surgery Dataset} 
% The Sinus Surgery Dataset \cite{qin2020towards} includes 10 cadaveric and 3 live videos, captured at 19201080 resolution. It presents complex scenarios such as occlusions, tissue interactions, and specular reflections, with frames manually annotated for tool segmentation.

% % \subsubsection{Cata7} 
% Introduced by Ni et al. \cite{ni2019raunet}, Cata7 is the first dataset for cataract surgery, featuring seven videos from Beijing Tongren Hospital. Each video is split into high-resolution images with detailed annotations for tool edges and segmentation masks.

% % \subsubsection{CaDTD Dataset} 
% The CaDTD dataset \cite{jiang2021semi}, derived from the CATARACTS dataset \cite{al2019cataracts}, contains 50 videos of cataract surgeries. Half of the videos are annotated with bounding boxes for tool heads and handles, while the others are unlabeled, providing a semi-supervised learning resource.

% % \subsubsection{RoboTool Dataset} 
% The RoboTool dataset \cite{garcia2021image} comprises 514 frames from robotic surgical videos, manually annotated for tool segmentation. Additionally, a synthetic dataset with 14,720 images provides segmentation masks for surgical tools against various backgrounds.

%The sub-categories within Surgical Workflow Analysis include: hopaedic surgery, which often involves unique instruments and movements.

\subsection{Phase Recognition}
Surgical phase recognition is a key aspect of surgical process modeling, with the aim of automatically identifying different stages of a procedure from video data. This capability is crucial to improving surgical training, automating documentation, and supporting real-time decision making in operating rooms. Recent advances in deep learning have notably advanced the accuracy and real-time functionality of phase recognition systems. Machine learning models, especially those employing temporal classification networks like LSTM networks, effectively recognize and segment different surgical phases from video data, structuring the surgical workflow to facilitate relevant information and tool availability at each stage and improving surgical efficiency and safety assessments. Twinanda et al. \cite{twinanda2016endonet} introduced EndoNet, a DL architecture that performs phase recognition in surgical scenarios, demonstrating its utility in automating and improving understanding of surgical procedures.
% Surgical phase recognition is a crucial component of surgical process modeling, which aims to automatically identify different stages of a surgical procedure from video data. This capability is instrumental in enhancing surgical training, automating documentation, and providing real-time decision support in operating rooms. Recent advancements in machine learning, particularly deep learning, have significantly improved the accuracy and real-time applicability of phase recognition systems.
% Phase Recognition involves identifying and segmenting different stages of a surgical procedure. This process is essential for structuring the surgical workflow, enabling automated systems to provide relevant information and tools at each stage, and aiding in the comprehensive assessment of surgical efficiency and safety. Machine learning models, particularly those based on temporal classification networks like LSTM networks, are used to recognize distinct phases from video data.
% Twinanda et al. \cite{twinanda2016endonet} developed EndoNet, a DL architecture that combines tool detection with phase recognition, highlighting its effectiveness in improving the automation and understanding of surgical procedures.


 % \subsubsection{Systematic Review and Current Approaches}

 A systematic review by Garrow et al. \cite{garrow2021machine} examined the transition from traditional ML methods to advanced DL models in surgical phase recognition, tracing the evolution of increasingly sophisticated algorithms that utilize extensive datasets and complex model architectures \cite{padoy2012statistical}. This progression has significantly enhanced accuracy and real-time applicability. Early methods involved simpler classifiers like Support Vector Machines (SVM) and Hidden Markov Models (HMM), but these were often constrained by their linear decision boundaries and basic assumptions about data sequence dependencies \cite{padoy2012statistical}. The early advancements techniques favor DL methods, particularly CNNs, and RNNs, which effectively manage the spatial and temporal complexities of surgical videos \cite{twinanda2016endonet, zia2021surgical}. Moreover, the development of multi-stage temporal convolutional networks (TCNs), such as the ``Tecno'' model, offers a robust solution that bypasses the limitations of recurrent architectures by using temporal convolutions to capture sequential dependencies more efficiently \cite{czempiel2020tecno, lea2017temporal}. These TCNs are particularly adept at processing sequential data, making them ideal for real-time applications that require instant feedback and recognition during surgical procedures \cite{czempiel2020tecno}. The comprehensive and fine understanding of these models significantly improves the automation of surgical documentation and aids in training surgical residents by offering objective and consistent phase identification \cite{lea2017temporal}.
% A systematic review of machine learning techniques for surgical phase recognition \cite{garrow2021machine} provides a comprehensive analysis of the progression from traditional machine learning methods to advanced DL models in this field. The review critically assesses the landscape of surgical phase recognition, highlighting key milestones and benchmark studies that have significantly advanced the understanding and application of these technologies in clinical environments. The evolution of surgical phase recognition has been marked by the adoption of increasingly sophisticated algorithms that leverage large datasets and complex model architectures to achieve high accuracy and real-time performance. Early works primarily utilized simpler classifiers such as Support Vector Machines (SVM) and Hidden Markov Models (HMM) \cite{padoy2012statistical}. These methods, however, were often limited by the linear nature of their decision boundaries or the simplistic assumptions about data sequence dependencies.

% Recent advancements have embraced DL techniques, particularly CNNs and RNNs, which are better suited for handling the spatial and temporal complexities of surgical videos \cite{twinanda2016endonet}. CNNs have been effective in extracting rich feature representations from video frames, while RNNs, especially LSTM networks, have provided the means to capture dynamic temporal relationships across different phases of surgery. Moreover, integrated models that combine CNNs with RNNs have demonstrated superior capability by leveraging both spatial and temporal data dimensions, leading to more robust and accurate phase recognition systems \cite{zia2021surgical}. These hybrid models not only enhance the granularity of phase detection but also improve the overall understanding of surgical workflows, thereby facilitating better training, documentation, and real-time assistance in surgical settings.


% % \subsubsection{Tecno: Multi-Stage Temporal Convolutional Networks}
% A significant advancement in surgical phase recognition is the development of ``Tecno,'' a model that capitalizes on the strengths of multi-stage temporal convolutional networks (TCNs) \cite{czempiel2020tecno}. This model departs from traditional recurrent architectures by utilizing temporal convolutions to effectively capture the sequential dependencies inherent in surgical activities. Temporal Convolutional Networks are particularly well-suited for processing sequential data due to their ability to handle long-range dependencies with reduced computational overhead compared to recurrent models like LSTMs or GRUs. The multi-stage design of Tecno allows it to perform hierarchical processing, where each stage captures increasingly abstract representations of the data. This methodology facilitates a deep and nuanced understanding of the temporal patterns in surgical videos, enhancing the model's ability to predict surgical phases accurately \cite{lea2017temporal}.

% Moreover, Tecno's architecture is designed to operate in real-time, providing instant feedback and recognition capabilities during surgical procedures. This is crucial for applications such as intra-operative decision support and immediate surgical assessment, where timely and accurate phase recognition can significantly impact surgical outcomes. Studies have shown that TCNs, when properly configured and trained on sufficient data, outperform many traditional methods in both speed and accuracy, making them ideal for integration into modern surgical systems \cite{czempiel2020tecno}. The real-world application of Tecno has demonstrated its potential in improving the automation of surgical documentation and assisting in the training of surgical residents by providing objective and consistent identification of different surgical phases, thereby supporting a standardized approach to surgical training and evaluation \cite{lea2017temporal}.

% % One notable development is ``Tecno,'' a model that utilizes multi-stage temporal convolutional networks (TCNs) for phase recognition \cite{czempiel2020tecno}. This approach leverages temporal convolutions to capture the sequential dependencies of surgical activities, allowing for more precise and dynamic recognition of surgical phases. The model's ability to process and learn from time-series data in surgery videos makes it highly effective for real-time applications.
% % \subsubsection{Real-Time CNN-based Approaches}
% The integration of CNNs into real-time surgical phase recognition represents a major technological breakthrough, particularly in complex procedures such as laparoscopic sigmoidectomy. This advancement leverages the robust feature extraction capabilities of CNNs to analyze intraoperative video streams, enabling the dynamic segmentation and classification of different surgical phases as they unfold \cite{twinanda2016endonet}.CNNs are adept at handling high-dimensional image data, extracting salient features critical for recognizing distinct surgical phases. The real-time application of these models is facilitated by their ability to operate efficiently under the constraints of streaming video data, providing immediate feedback vital for supporting the surgical team in a high-stakes, dynamic environment \cite{quellec2014real}. By processing visual information on-the-fly, CNN-based systems can assist in enhancing situational awareness and decision-making during surgery.

% Moreover, the development of real-time CNN-based approaches has been significantly bolstered by advancements in computational hardware and optimized neural network architectures. These enhancements allow the models to achieve lower latency and higher throughput, essential for deployment in clinical settings where delay can impact surgical outcomes. Studies have demonstrated that with adequate training data, these CNN models can achieve a high degree of accuracy in phase recognition, comparable to or exceeding that of manual phase annotation by clinical experts \cite{czempiel2020tecno}. The practical implications of deploying real-time CNN-based phase recognition systems are profound. They include improvements in surgical documentation accuracy, enhanced training for surgical residents, and the potential for automated, context-sensitive support that adjusts to the needs of the surgical team throughout the procedure \cite{yang2020image}.


% % \subsubsection{Deep Learning in Surgical Workflow Analysis}
% Using DL in surgical workflow analysis has significantly broadened the scope of automated recognition, moving beyond mere phase recognition to encompass comprehensive workflow comprehension. Reviews of both phase and step recognition using DL models \cite{demir2023deep} highlight the transformative impact these technologies have on understanding surgical processes. These models provide granularity that allows for intricate insights into the complexities of various surgical procedures. DL approaches, especially those leveraging architectures like CNNs and RNNs, have demonstrated remarkable success in dissecting the intricate sequences of surgical workflows. These technologies parse through high-dimensional video data, extracting and learning from patterns that might elude even experienced surgeons. As such, they enable the development of systems that can assist in real-time decision-making and retrospective analysis by identifying not just phases, but also critical steps and decision points within those phases \cite{twinanda2016endonet}.

% Moreover, integrating DL into surgical workflow analysis fosters a more proactive approach to surgical training and evaluation. By providing detailed breakdowns of surgical actions, these systems can pinpoint areas of improvement for training surgeons, facilitating a targeted and efficient learning process. Furthermore, the ability to automatically document detailed steps of surgical procedures aids in standardizing surgical practice and enhancing procedural consistency across varying clinical settings \cite{quellec2014real}.

% The future of surgical workflow analysis is likely to see greater adoption of these advanced DL models, driven by their ability to adapt and learn from a growing corpus of surgical data. This ongoing refinement and adaptation promise even more accurate and helpful systems, capable of contributing to safer, more efficient, and more predictable surgical outcomes \cite{czempiel2020tecno}.

% % \subsubsection{Hybrid Models and Transformers}
% Recent advancements in surgical phase recognition include the development of hybrid models like ``Trans-svnet,'' which integrates hybrid embeddings with an aggregation transformer to enhance phase recognition accuracy \cite{li2024dual}. This model uses CNNs for spatial feature extraction and transformers for temporal data aggregation, effectively combining spatial and temporal dimensions for improved contextual understanding. The transformer component in ``Trans-svnet'' employs attention mechanisms to selectively focus on relevant features, maintaining high accuracy across complex surgical procedures with varying phases. This dynamic focus captures subtle nuances, significantly enhancing model robustness in diverse surgical environments \cite{vaswani2017attention}. The integration of CNNs and transformers addresses data heterogeneity in surgical videos, ensuring robust performance across various surgical settings. The deployment of such models promises enhanced surgical documentation, improved real-time decision support, and better training tools, contributing to safer, more efficient surgical practices \cite{zhang2022towards}.



% % \subsubsection{Opera: Attention-Regularized Transformers}
% The introduction of ``Opera,'' an attention-regularized transformer model, marks a significant advancement in the field of surgical phase recognition \cite{czempiel2020tecno}. This model epitomizes the cutting-edge application of attention mechanisms, designed to focus intensively on the most relevant features within surgical video frames while concurrently regulating and filtering out less informative data through attention gates. Opera's attention regularization is crucial for handling the vast amounts of data inherent in video streams, ensuring that the model prioritizes critical information that impacts phase recognition accuracy. By selectively attending to specific areas within the video, Opera minimizes computational waste on non-essential parts, thereby enhancing the efficiency and speed of phase recognition \cite{guo2022attention}.

% This model leverages a sophisticated transformer architecture that incorporates multi-headed attention layers, allowing it to simultaneously process multiple aspects of the video data. This multi-faceted approach not only improves the model's ability to discern subtle distinctions between surgical phases but also ensures a more robust performance across diverse and complex surgical procedures \cite{touvron2021training}. The practical deployment of Opera in clinical settings could substantially refine surgical workflow analysis, providing surgeons with precise, real-time feedback on the current phase of surgery. This capability is particularly valuable for educational purposes, surgical planning, and post-operative analysis, where understanding the exact sequence of events can lead to better outcomes and more informed surgical interventions \cite{shi2022recognition}.


\subsection{Tissue and Organ Segmentation}
Tissue and Organ Segmentation is vital in medical imaging, enabling precise identification and delineation of anatomical structures for presurgical planning and intraoperative guidance. DL models, especially U-Nets and Fully Convolutional Networks (FCNs), have dramatically improved the segmentation of medical images, offering high accuracy and real-time processing \cite{ronneberger2015u, long2015fully}. The V-Net architecture, a three-dimensional variant of the U-Net developed by Milletari et al. \cite{milletari2016v}, excels in volumetric segmentation, processing entire data volumes simultaneously to effectively capture spatial hierarchies of tissues. The use of volumetric convolutions and skip connections in this model helps to maintain segment boundary accuracy during up-sampling, which is crucial for surgeries requiring high precision. Furthermore, recent enhancements incorporate Generative Adversarial Networks (GANs) and attention mechanisms, which improve the model's ability to differentiate between similar tissues and enhance boundary contrast in complex scenarios \cite{oktay2018attention}. 


% Tissue and Organ Segmentation is a critical component of medical imaging, involving the precise identification and delineation of various tissues and organs within surgical images. This process is indispensable for pre-surgical planning and intraoperative guidance, as it enables surgeons to navigate complex anatomical structures safely and effectively. ML models, particularly those employing DL architectures such as U-Nets or fully convolutional networks (FCNs), have revolutionized this field by providing the capability to segment medical images with high accuracy and in real-time \cite{ronneberger2015u}. The advent of DL in medical imaging has led to the development of specialized neural network architectures tailored for segmentation tasks. Among these, the V-Net, a variant of the U-Net presented by Milletari et al. \cite{milletari2016v} (2016), stands out due to its efficacy in volumetric segmentation of medical images. V-Net has been specifically designed to handle three-dimensional data, making it exceptionally useful in distinguishing different tissues and organs during surgical procedures. The model incorporates volumetric convolutions that allow it to process entire volumes at once, thereby capturing spatial hierarchies in tissues more effectively than traditional two-dimensional approaches.

% Also, the integration of skip connections within the V-Net architecture helps in recovering spatial information lost during down-sampling processes, which is crucial for maintaining the accuracy of segment boundaries in the up-sampling phase. This capability ensures that even subtle anatomical features are accurately captured, which is vital for operations requiring high precision \cite{long2015fully}. Recent advancements have also seen the application of generative adversarial networks (GANs) and attention mechanisms to further refine the segmentation process, focusing on improving the model's ability to distinguish between similar tissues and enhancing the contrast of boundaries in challenging scenarios \cite{oktay2018attention}. These innovations have significantly improved the robustness and reliability of segmentation models in clinical settings.

\subsection{Anatomical Structure Recognition}
Anatomical Structure Recognition is an essential component of advanced surgical systems, focusing on the accurate identification and classification of anatomical structures within the surgical field. This capability is crucial to improving the functionality of automated surgical systems and providing surgeons with real-time decision support. Using advanced ML techniques, particularly CNNs, these systems can efficiently recognize and label complex anatomical features, helping to improve the precision and safety of surgical interventions. Oh et al. \cite{oh2024real} demonstrated the effectiveness of DL models in real-time recognition of anatomical structures, significantly improving both the accuracy and safety of surgical procedures. These models are trained on large datasets of annotated images, allowing them to develop a nuanced understanding of various anatomical nuances that are critical during operations.


 \subsection{Action and Task Recognition}

Action and Task Recognition in surgery identifies specific actions and tasks within phases, crucial for training, performance evaluation, and protocol adherence. Advanced ML techniques, such as CNN and sequence modeling, analyze surgical videos to detect subtle surgeon activities, thus enhancing the monitoring and standardization of practices \cite{brandao2018towards}. Some of the recent developments include a computer vision platform that uses ML algorithms to recognize actions and highlight crucial events during surgeries such as laparoscopic cholecystectomy, contributing to procedural standardization and training improvement \cite{mascagni2021computer}. Additionally, gesture recognition technologies in robotic surgery improve interactions between surgeons and robotic systems, enhancing efficiency and outcomes by providing intuitive control of surgical robots \cite{van2021gesture}.

Autonomous Instruments Control uses ML to automate the control of surgical instruments, improve precision, and minimize human errors by replicating expert maneuvers, offering consistency in intricate tasks, and reducing surgeon fatigue \cite{kassahun2016surgical, shademan2016supervised}. Anomaly Detection and Safety Monitoring employs ML to identify deviations from standard procedures, enhancing safety with real-time alerts and interventions, which are essential in high-stakes environments \cite{wagner2023comparative, maier2018surgical}. It is imperative that these alerts do not overload clinical staff, resulting in  ``notification fatigue". Striking a balance is key. 

Augmented Reality (AR) and Navigation Assistance technologies use ML to overlay critical information directly on the surgical field and provide navigational signals during procedures, significantly impacting surgical planning and execution \cite{bernhardt2017status, cutolo2024augmented, maier2017surgical}. Error detection and feedback mechanisms use ML to monitor surgeries and provide corrective feedback in real time, thereby improving quality, safety and supporting surgical training by providing real-time guidance and post-procedure analysis to beginner surgeons \cite{shademan2016supervised, twinanda2016endonet, lajczak2024md}.


\subsection{Downstream Tasks using Multimodality}
% \subsubsection{BERT (Bidirectional Encoder Representations from Transformers)}
BERT is a foundational language model renowned for its proficiency in understanding and generating human language. In the surgical domain, BERT has been leveraged to improve various aspects of clinical practice, documentation, and decision-making processes. One of the primary applications of BERT in surgery involves tasks of natural language processing (NLP), such as the automatic extraction and interpretation of information from surgical reports, electronic health records (EHR) and operative notes \cite{zhong2024improving}. By accurately parsing complex medical terminology and contextual nuances, BERT-based models facilitate the creation of structured data sets from unstructured textual data, which can be instrumental for clinical research and patient management.

In addition, BERT has been integrated into decision support systems to help surgeons make informed decisions during procedures. For example, by analyzing historical surgical data and outcomes, BERT models can provide predictive insights and recommendations tailored to specific patient cases, thus improving surgical planning and risk assessment \cite{li2023interpretable}. Furthermore, BERT improves communication within surgical teams by enabling more efficient information retrieval and summarization, ensuring that all team members have access to relevant and up-to-date patient information in real-time. This  approach to key information enables high performance within the team setting. 

In the realm of surgical training and education, BERT-powered applications offer personalized learning experiences for surgical trainees. These applications can analyze trainee performance reports, identify areas for improvement, and suggest targeted educational resources, thus fostering continuous professional development \cite{ray2024large}. Furthermore, BERT facilitates the development of intelligent virtual assistants that can support surgeons by answering questions, providing procedural guidelines, and managing administrative tasks, ultimately contributing to increased operational efficiency and reduced cognitive load during surgeries.

%\subsection{Large Language Model} 
LLaMA is renowned for its natural language processing prowess and is enhancing surgical and healthcare operations through a series of specialized systems. LLaVA-Surg, as detailed by \cite{li2024llava}, uses LLaMA to integrate and interpret multimodal data from surgical procedures. This system helps surgical teams by providing dynamic support during operations, helping to identify crucial procedural steps and potential deviations, thus improving surgical safety and efficiency. Following this, Surgical-LLaVA focuses on deepening the understanding of surgical scenarios. Jin et al. \cite{jin2024surgical} highlight how this system utilizes large language and vision models to process complex surgical data, thus improving the precision and responsiveness of surgical interventions.

Furthermore, LlamaCare by \cite{sun2024llamacare} leverages LLaMA's capabilities to improve healthcare knowledge sharing. This system improves the dissemination of medical information and best practices throughout the healthcare community, facilitating better communication between healthcare providers and improving patient care through access to updated and comprehensive medical knowledge.

ChatGPT has also shown significant promise as an intraoperative and educational tool in various surgical disciplines, as highlighted in recent studies. Atkinson et al. \cite{atkinson2024artificial} and Araji et al. \cite{araji2024evaluating} demonstrate how ChatGPT helps decision making during complex surgeries such as Deep Inferior Epigastric Perforator (DIEP) flap procedures and enhances learning experiences for surgical clerkships. These studies illustrate ChatGPT's ability to deliver real-time, accurate responses and educational support, improving surgical safety and training outcomes. Extending its applications, the review by Goglia et al. \cite{goglia2024artificial} further emphasizes ChatGPT's role in abdominal and pelvic surgery, showing its effectiveness in preoperative planning, intraoperative decisions, and patient communication. Together, these studies reflect a transformative shift towards integrating AI in surgical practices, underscoring ChatGPT's potential to optimize surgical workflows, precision, and educational protocols while also highlighting the need for ongoing refinement and cautious integration into clinical settings.


% Action and Task Recognition categorizes and identifies specific surgical actions and tasks within each phase. Accurate recognition of these elements is vital for training purposes, performance evaluation, and ensuring adherence to surgical protocols. Advanced machine learning techniques, including convolutional neural networks and sequence modeling, are employed to analyze complex surgical videos and detect nuanced surgeon activities. Brando et al. \cite{brandao2018towards} used DL to automatically detect and classify surgical actions in video data, demonstrating how these technologies can enhance the monitoring and standardization of surgical practices.


% % \subsubsection{A computer vision platform to automatically locate critical events in surgical videos}
% One significant advancement in this domain is the development of a computer vision platform specifically designed to locate and document critical events during surgeries such as laparoscopic cholecystectomy. This system utilizes machine learning algorithms to analyze surgical footage, recognize specific actions, and highlight moments of interest that are crucial for safety and training purposes \cite{mascagni2021computer}. By providing real-time analysis and feedback, such platforms can significantly contribute to the standardization of surgical procedures and enhancement of surgical training.

% % \subsubsection{Gesture recognition in robotic surgery}
% Additionally, robotic surgery has greatly benefited from the integration of gesture recognition technologies, enabling more intuitive control of surgical robots and enhancing the understanding of surgical workflows. A comprehensive review of gesture recognition techniques in robotic surgery underscores the importance of these technologies in enhancing the interaction between surgeons and robotic systems, thereby improving the efficiency and outcomes of surgical procedures \cite{van2021gesture}. Gesture recognition systems analyze the surgeon's hand movements and translate them into precise commands for the robotic instruments, facilitating smoother and more natural surgical operations.


% % \subsubsection{Autonomous Instruments Control}
% Autonomous Instruments Control represents a critical development in surgical robotics, where ML algorithms are employed to facilitate automated control of surgical instruments. This technological advancement aims to enhance surgical precision and minimize human errors by augmenting or replacing human control with accurate, machine-driven actions \cite{kassahun2016surgical}. These systems integrate various ML techniques, such as reinforcement learning and supervised learning, to understand and mimic expert surgical maneuvers. The goal is to achieve a level of dexterity and decision-making akin to or surpassing that of the best human surgeons. By doing so, these autonomous systems can perform repetitive or highly intricate tasks with consistent precision, reducing the physical strain and fatigue experienced by human surgeons during long and complex procedures \cite{shademan2016supervised}.

% Kassahun et al. \cite{{kassahun2016surgical}} explored the effectiveness of different ML approaches in enhancing the capabilities of surgical robots. Their research highlights how advancements in algorithmic design and data processing have contributed to significant improvements in the autonomy of surgical instruments, emphasizing the potential to achieve increased precision and operational efficiency in the operating room.

% Moreover, the integration of DL models has further propelled the field forward, allowing robots to process visual and sensory data in real-time to make informed decisions during surgeries \cite{oleari2019enhancing}. These models leverage large datasets of surgical procedures to learn complex patterns and predict the necessary adjustments required during an operation, thereby improving the safety and outcomes of surgical interventions \cite{vitiello2012emerging}.




% % \subsubsection{Anomaly Detection and Safety Monitoring}
% Anomaly Detection and Safety Monitoring in surgical environments primarily focuses on identifying deviations from standard surgical procedures that could potentially pose risks to patient safety. Integrating ML techniques for real-time monitoring ensures that any anomalies or irregularities are promptly detected and addressed, significantly enhancing the safety and reliability of surgical operations \cite{wagner2023comparative}. ML algorithms are adept at analyzing large volumes of data generated during surgeries, such as video feeds, sensor data, and surgeon movements, to learn typical patterns and behaviors. By employing these algorithms, the system can automatically flag activities or metrics that deviate from the norm, facilitating immediate intervention. This capability is critical in high-stakes environments like the operating room, where rapid response to unexpected events can be lifesaving \cite{maier2018surgical}.

% Wagner et al. \cite{wagner2023comparative} specifically highlighted the efficacy of ML in monitoring surgical workflows and detecting anomalies. They demonstrated how AI technologies could continuously learn from vast datasets of surgical procedures, thereby progressively improving their accuracy and reliability in anomaly detection. These systems not only support the surgical team by providing a second layer of monitoring but also contribute to training and procedural refinement by identifying and logging unusual events for further analysis.

% % \subsubsection{Augmented Reality (AR) and Navigation Assistance}
% Augmented Reality (AR) and Navigation Assistance are transformative technologies that significantly enhance surgical procedures by integrating ML within the broader context of surgical workflow analysis. These innovative technologies augment the surgeon's perception by overlaying real-time, context-sensitive information directly onto the surgical field, thereby enhancing interaction with the surgical environment \cite{bernhardt2017status}. AR systems in surgery use computer vision and ML algorithms to process images and data from the surgical site, allowing the system to provide visual aids such as annotations, highlighting critical structures, or displaying diagnostic data that are not visible to the naked eye. This integration of virtual elements into the real world not only improves the accuracy of surgical interventions but also significantly reduces the risk of errors, potentially lowering the likelihood of postoperative complications \cite{cutolo2024augmented}.

% Moreover, Navigation Assistance technologies leverage sophisticated ML models to interpret complex datasets from diagnostic tools like MRI, CT scans, and real-time ultrasound. These models assist by providing navigational cues that guide surgeons during intricate procedures, ensuring precise targeting and minimizing invasiveness. The application of deep learning, particularly convolutional neural networks, has been pivotal in refining these navigational aids, enabling them to adapt dynamically to changes in the surgical landscape and offer updated guidance based on surgical progress \cite{maier2017surgical}. The integration of AR and navigation systems into surgical workflows represents a significant step forward in medical technology, merging real-world practice with advanced computational capabilities. This synergy not only enhances the surgical precision but also contributes to training and planning phases, where surgeons can rehearse complex operations in a virtual environment, thus gaining familiarity with specific patient anatomy before actual surgery \cite{bin2020virtual}.


% % \subsubsection{Error Detection and Feedback}
% Error Detection and Feedback mechanisms are integral components of modern surgical systems, aimed at maintaining the highest standards of quality and safety during surgical procedures. These systems utilize advanced ML algorithms to continuously monitor ongoing surgeries, analyzing a wide range of data inputs, including tool usage, surgeon movements, and physiological signals from the patient \cite{maier2017surgical}.

% ML models, particularly those based on supervised learning and anomaly detection, are trained on vast datasets of surgical procedures to identify patterns and deviations that may indicate potential errors. Once an error is detected, the system can provide real-time alerts and corrective feedback to the surgical team. This immediate response is crucial for mitigating risks and preventing complications during surgery \cite{shademan2016supervised}. Furthermore, these error detection systems are enhanced by incorporating feedback loops that not only alert to immediate errors but also contribute to long-term improvements in surgical practices. By analyzing the outcomes and incorporating surgeon responses, the systems adapt and refine their predictive capabilities, which can lead to more personalized and precise interventions over time \cite{twinanda2016endonet}. The integration of error detection and feedback mechanisms into surgical workflows not only enhances patient safety but also supports surgical training. Novice surgeons can benefit from real-time guidance and post-procedure analysis, which help in honing their skills and reducing the learning curve associated with complex surgical techniques \cite{lajczak2024md}.

\subsection{Workflow Analysis Datasets}
Workflow analysis in surgical settings benefits from a variety of specialized datasets, each contributing unique insights and challenges to the field. The ATLASDione dataset, compiled by Sarikaya et al. \cite{sarikaya2017detection}, includes 99 videos from robotic tasks performed by clinicians, annotated with tool bounding boxes, action types, durations, and surgeon's skill levels. The dVPN data set from Ye et al. \cite{ye2017self} provides 34,320 pairs of stereo images for training and 14,382 pairs for testing, improving stereo vision and depth estimation in robotic surgeries. The AutoLaparo dataset \cite{wang2022autolaparo} contains 21 high-definition videos of laparoscopic hysterectomy procedures, annotated for tool and anatomy segmentation and workflow recognition. The NeuroSurgicalTools data set \cite{bouget2015detecting} offers 14 neurosurgery videos with detailed instrument annotations, addressing visual challenges like occlusions and reflections. The FetalFlexTool dataset by Garca-Peraza-Herrera et al. \cite{garcia2017real} provides ex-vivo fetal surgery images and videos, annotated for tool segmentation under varied conditions.

Additional synthetic and simulated data sets include the UCL data sets \cite{rau2019implicit, colleoni2020synthetic}, offering synthetic images and videos rendered to simulate various surgical scenarios. The Laparoscopic Image-to-Image Translation dataset \cite{pfeiffer2019generating} includes 100,000 images derived from CT scans, adapted to various visual styles for model testing. The Sinus Surgery Dataset \cite{qin2020towards} features videos of sinus surgeries annotated for tool segmentation in complex environments. Cata7 \cite{ni2019raunet} focuses on cataract surgery, providing high-resolution images with detailed annotations, while the CaDTD data set \cite{jiang2021semi} extends the CATARACTS data set \cite{al2019cataracts} with semi-supervised learning techniques for cataract surgeries. Lastly, the RoboTool dataset \cite{garcia2021image} includes real and synthetic images for training in robust tool segmentation. These diverse data sets collectively improve the development and testing of machine learning models in various surgical specialties and conditions.


\section{ML/DL Applications in Surgical Training and Simulation}
\label{sec:surgical_training}
% Surgical training and simulation within the domain of surgical scene understanding plays a crucial role in comprehensively interpreting and analyzing the entire surgical environment. This encompasses not only the directions toward surgical education but also the broader operational context by generating new content in terms of visual and textual information, including patient anatomy and the interactions between surgical tools and tissues. ML algorithms are central to enhancing the perception and contextual awareness necessary for effective surgical interventions.

ML and DL are significantly enhancing surgical training and simulation, offering profound insights and enhancements in the understanding of surgical scenes. These technologies help create realistic simulations of surgical procedures, allowing trainees to practice and learn in a safe environment. ML and DL tools generate detailed visuals and provide information about patient anatomy and how surgical tools interact with tissues. This helps surgeons understand and navigate the surgical environment better, making it easier for them to perform successful operations. Overall, using ML and DL in surgical training helps improve a surgeon's skills and knowledge, which is crucial for effective and safe surgery. Following are some of the key components that correspond to designing efficient tools and techniques.


\subsection{Key Frame Extraction}

The extraction of key frames from surgical videos is a critical process for efficient video analysis, allowing the identification of important moments without the need to review entire video sequences. An advanced method, proposed by Ma et al. \cite{ma2020keyframe}, uses a diverse and weighted dictionary selection algorithm to identify key frames based on their representativeness and uniqueness. This approach ensures that the selected frames capture critical phases of the surgery, enhancing their educational and analytical value. Another technique, described by Tan et al. \cite{tan2024large}, leverages large-scale DL models to extract sequential key frames, accurately summarizing surgical procedures. This method emphasizes the extraction of key stages of the surgery, providing a concise and informative visual summary, particularly useful for surgical training and procedural reviews.

\subsection{Tissue Classification}
Tissue Classification is crucial in surgical procedures for tasks such as cancer excision, involving the differentiation of tissue types based on characteristics such as appearance, texture and morphology. Machine learning, particularly DL-based techniques, excels in analyzing surgical images and videos to classify tissues with high accuracy, aiding surgeons in making decisions during complex interventions. Cekic et al. \cite{cekic2024deep} introduced a DL approach using CNNs to classify tissue types in surgical videos in real time, effectively distinguishing between healthy and pathological tissues. This precision is vital in oncological surgeries to ensure complete removal of malignant cells while conserving healthy tissue. The field has seen further advancements by integrating more sophisticated architectures like Residual Networks (ResNets) and Inception networks. These networks provide deeper and more complex models that capture a broader range of features from surgical images, significantly enhancing the accuracy of classification \cite{hu2024advancing}.
% Tissue Classification is an essential function in surgical procedures, particularly for critical tasks such as tumor excision or targeted therapy. This process involves differentiating between various types of tissues based on a myriad of characteristics, including appearance, texture, and other morphological features. ML algorithms, especially those employing DL techniques, are well-suited to analyze surgical images and videos to classify tissue types with high accuracy, thereby supporting the surgeon's decision-making process during intricate surgical interventions.

% In \cite{cekic2024deep} presented a novel DL approach that specifically addresses the classification of tissue types in surgical videos. Their model utilizes CNNs to process video frames in real-time, effectively distinguishing between healthy and pathological tissues with significant precision. This capability is particularly beneficial in surgeries involving cancerous tumors, where precise tissue classification can guide the surgeon in removing all malignant cells while preserving as much healthy tissue as possible. Further advancements in this field have incorporated the use of more sophisticated neural network architectures, such as Residual Networks (ResNets) and Inception networks, which provide deeper and more complex models for image analysis. These networks are capable of capturing an extended range of features from surgical images, leading to improved classification outcomes \cite{hu2024advancing}.





\subsection{Depth Estimation and 3D Reconstruction}
Depth Estimation and 3D Reconstruction technologies play a crucial role in providing three-dimensional perspectives of the surgical field from two-dimensional images or video feeds, particularly in MIS. These technologies, leveraging ML models such as stereo vision algorithms and structured light approaches, enhance depth perception and spatial understanding, thus improving the precision and safety of surgical interventions. Guni et al. \cite{guni2024artificial} highlighted the effectiveness of ML techniques in producing accurate 3D reconstructions that significantly aid in planning and executing complex surgical procedures. These models are trained on large datasets and identify and interpret depth signals to create detailed 3D maps of the operative area. Recent advancements include using DL frameworks like CNNs and Generative Adversarial Networks (GANs) to improve the accuracy and detail of 3D reconstructions. CNNs process large amounts of image data to detect crucial edges and contours for depth estimation, while GANs enhance detail in partially obscured areas \cite{huang2024real}. Furthermore, the use of time-of-flight (ToF) cameras and laser triangulation with ML algorithms facilitates real-time 3D reconstruction during surgeries, providing immediate feedback for on-the-fly adjustments, crucial for successful surgical outcomes \cite{baptista2024structured}.







% Depth Estimation and 3D Reconstruction technologies are instrumental in providing a three-dimensional perspective of the surgical field from two-dimensional images or video feeds. These advancements are particularly crucial in the context of minimally invasive surgery, where spatial cues are inherently limited, and the detailed visualization of the operative area is essential. ML models, especially those employing stereo vision algorithms and structured light approaches, play a pivotal role in reconstructing the 3D environment of the surgical site. These technologies enhance the surgeon's depth perception and spatial understanding, thereby improving the precision and safety of surgical interventions. In \cite{guni2024artificial}have demonstrated the effectiveness of ML techniques in creating accurate 3D reconstructions of the surgical site, which significantly aid in the planning and execution of complex surgical procedures. By leveraging large datasets of surgical images, these models are trained to identify and interpret depth cues, enabling them to generate highly detailed 3D maps of the operative field.

% Moreover, recent advancements have incorporated DL frameworks such as CNNs and generative adversarial networks (GANs) to further enhance the accuracy and detail of 3D reconstructions. For instance, CNNs are utilized to process vast amounts of image data to detect edges and contours that are crucial for depth estimation, while GANs have been explored for their ability to fill in details in areas where data might be missing or obscured \cite{huang2024real}. Furthermore, the integration of time-of-flight (ToF) cameras and laser triangulation techniques with ML algorithms has opened new avenues for real-time 3D reconstruction during surgeries. These technologies provide instant feedback to the surgical team, allowing for adjustments to be made on-the-fly, which is vital for the success of surgical procedures \cite{baptista2024structured}.










% \begin{table*}[ht]
% \centering
% \caption{Comprehensive Summary of Tool Navigation Datasets}
% \label{tab:datasetcomp}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}cccccccc@{}}
% \toprule
% \textbf{Year} & \textbf{Collection} & \textbf{Data Volume} & \textbf{Surgical Type} & \textbf{Access} & \textbf{Instrument Type} & \textbf{Label Types} & \textbf{Operational Tasks}
%  \\ \midrule

% \vspace{3mm}
% 2021 & KvasirInstrument & 590 endoscopic frames & Gastrointestinal Endoscopy & Public & Rigid & Binary masks, Bounding boxes & Tool Segmentation \\ 
% \vspace{3mm}
% 2021 & RoboTool & 514 video frames, 14720 synthetic images & Various surgical & Public & Robotic & Binary labels, segmentation masks & Synthetic dataset creation \\ 
% \vspace{3mm}
% 2021 & I2I Translation & 200,000 images & Laparoscopic surgery & NA & Rigid & Style transformation masks & Image style translation \\ 
% \vspace{3mm}

% 2021 & SCARED & 9 datasets & Porcine & Public & Robotic & depth & Depth estimation \\ 
% \vspace{3mm}

% 2021 & CaDTD & 50 Videos & Cataract Surgery & Private & Rigid & Bounding-box & Detection \\
% \vspace{3mm}
% 2021 & dVPN & 48702 images & nephrectomy & NA & NA & TP, A, SC, Ph & Detection, Action Rec. \\ 
% \vspace{3mm}
% 2021 & EndoVis 21 & 33 Videos & cholecystectomy & Public & Rigid & TP, A, SC, Ph & Detection, Phase, Action Rec. \\ 
% \vspace{3mm}
% 2020 & LapSig300 & 300 Videos & Colorectal Surgery & Private & Rigid & Pixel-wise mask & Segmentation, Parts, Action Rec. \\ 
% \vspace{3mm}
% 2020 & Sinus Surgery-L & 3 Videos & Sinus-Live & Public & Rigid & Pixel-wise mask & Segmentation, Phase, Action Rec. \\ 
% \vspace{3mm}
% 2020 & Sinus Surgery-C & 10 Videos & Sinus-Cadaver & Public & Rigid & Pixel-wise mask & Segmentation(Binary) \\ 
% \vspace{3mm}
% 2020 & UCL dVRK & 20 Videos+Kinematic Data & Ex-Vivo & Public & Robotic & Camera parameters & Segmentation(Binary) \\ 
% \vspace{3mm}
% 2019 & SCAREDf & 27 Videos & Porcine & NA & NA & Depth + mask & Depth estimation \\ 
% \vspace{3mm}
% 2019 & Cata7 & 7 Videos & Cataract Surgery & Private & Rigid & Pixel-wise mask & Segmentation, 3D Reconstruction \\ 
% \vspace{3mm}
% 2019 & UCL & 16016 Synthetic images & sigmoid resection, colonoscopy & NA & NA & Depth map & Depth estimation \\
% \vspace{3mm}
% 2019 & ROBUST-MIS19 & 30 videos & proctocolectomy, rectal & Public & Rigid & Instances & Segmentation(Binary,Parts) \\ 
% \vspace{3mm}
% 2018 & m2cai16-tool locations & 16 Videos & cholecystectomy & Public & Rigid & Bounding-box & Detection \\ 
% \vspace{3mm}
% 2018 & LapGyn4 & 55K images & Gynecologic Surgery & Public & Rigid & No annotation & Multiple \\ 
% \vspace{3mm}
% 2018 & EndoVis 18d & 14 Videos & Nephrectomy & Public & Robotic & Pixel-wise mask & Scene Segmentation \\ 
% \vspace{3mm}
% 2017 & Hamlyn & 2 Phantom & Cardiac & Public & NA & Depth map & Tracking \\ 
% \vspace{3mm}
% 2017 & ATLAS Dione & 8 Videos & In-vitro Experiments & Public & Robotic & Bounding Box & Detection, Activity \\ 
% \vspace{3mm}
% 2017 & EndoVis 17b & 10 Videos & Porcine & Public & Robotic & Pixel-wise & Segmentation, Binary, Parts \\
% \vspace{3mm}
% 2016 & Cholec80 & 80 Videos & cholecystectomy & Public & Rigid & Bounding Box & Detection, Activity, Skills \\ 
% \vspace{3mm}
% 2016 & M2CAI16-tool & 16 Videos & cholecystectomy & Public & Rigid & Tool presence & Detection, Phase \\ 
% \vspace{3mm}
% 2015 & FetalFlexTool & 21 videos & Fetal Surgery & Public & Rigid & Bounding-box & Detection \\
% \vspace{3mm}
% 2015 & NeuroSurgicalTools & 2476 images & Neurosurgery & Public & Robotic & Bounding-box & Detection \\
% \vspace{3mm}
% 2015 & EndoVis 15 & 9K images & colorectal surgery & Public & Rigid & Pixel-wise,2D pose & Segmentation, Tracking \\
% \vspace{3mm}
% 2021 & AutoLaparo & 21 videos & Laparoscopic Hysterectomies & Public & Rigid & Annotated for uterus and instruments & Workflow recognition, Motion prediction,
% Anatomy segmentation \\ 
% \vspace{3mm}
% 2020 & HeiSurF & 24 videos, 9 test surgeries & Laparoscopic gallbladder resections & Public & Rigid & Segmentation masks, Instrument classes & Full scene segmentation, Workflow analysis \\
% \vspace{3mm}
% 2021 & ART-Net & 29 procedures & Laparoscopic hysterectomies & Public & Non-robotic & Binary segmentation, tool presence & Instrument segmentation, geometry annotation \\
% \bottomrule
% \end{tabular}
% }
% % \end{table}
% \end{table*}











\subsection{Surgical Video Generation}
The generation of realistic surgical video through ML models opens new possibilities for training, simulation, and research. A recent work designed SurGen \cite{cho2024surgen}, an innovative approach to surgical video generation which is a text-guided diffusion model designed to create detailed and accurate surgical videos from textual descriptions. This model leverages the capabilities of diffusion models, which construct images by gradually refining patterns of noise into structured visuals. By integrating text input, SurGen allows users to specify what the video should depict, making it a powerful tool to create customized training materials or to simulate surgical scenarios to study possible outcomes and strategies, as shown in Fig.~\ref{fig:surgical}.


% \subsubsection{SurGen: Text-Guided Diffusion Model for Surgical Video Generation}
% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=\columnwidth]{surgical_video.png} % Set the width to \columnwidth for one-column width
%   \caption{High-definition images generated from surgical videos by SurGen\cite{cho2024surgen} a) Gallbladder Dissection. b) Clipping and cutting. c) Preparation. d) Calot's triangle dissection.}
%   \label{fig:surgical} 
% \end{figure}


% \begin{figure}[ht!]
%     \centering
%     \begin{subfigure}[b]{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/SurGen/SurGen-a.png}
%         \caption{Gallbladder dissection}
%         \label{fig:sub1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/SurGen/SurGen-b.png}
%         \caption{Clipping and cutting}
%         \label{fig:sub2}
%     \end{subfigure}
    
%     \begin{subfigure}[b]{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/SurGen/SurGen-c.png}
%         \caption{Preparation}
%         \label{fig:sub3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/SurGen/SurGen-d.png}
%         \caption{Calot's triangle dissection}
%         \label{fig:sub4}
%     \end{subfigure}
%     \caption{High-definition images generated from surgical videos by SurGen\cite{cho2024surgen}. All images are adapted from \cite{cho2024surgen}.}
%     \label{fig:surgical}
% \end{figure}

\begin{figure}[hbtp]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/SurGen/SurGen-a.png}
        \caption{Gallbladder dissection.}
        \label{fig:sub1-Surgen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/SurGen/SurGen-b.png}
        \caption{Clipping and cutting.}
        \label{fig:sub2-Surgen}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/SurGen/SurGen-c.png}
        \caption{Preparation for removal.}
        \label{fig:sub3-Surgen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/SurGen/SurGen-d.png}
        \caption{Calot's triangle dissection.}
        \label{fig:sub4-Surgen}
    \end{subfigure}
    \caption{The use of \textit{SurGen} \cite{cho2024surgen}, a \textit{text-guided diffusion model for surgical video synthesis}, to generate key stages of gallbladder surgery. These images highlight the potential of foundation models to improve surgical scene understanding by generating realistic visuals for training and planning. Images adapted from \cite{cho2024surgen}.}
    \label{fig:surgical}
\end{figure}



Recent advancements in the field have shown that video generation models can also leverage temporal coherence and multi-modal data to enhance the realism of generated surgical procedures. For example, the work by Wang et al. \cite{wang2024leo} introduced a method for surgical video synthesis that maintains temporal consistency across frames, which is crucial for realistic simulations. Similarly, Yamada et al. \cite{yamada2024multimodal} explored the integration of multi-modal inputs such as surgical instrument tracking data, further improving the fidelity of generated videos by aligning visual and kinematic information.



\subsection{Anomaly Detection}
Anomaly detection in surgical settings is critical for ensuring patient safety and enhancing surgical precision. This technology leverages advanced ML techniques to identify deviations from normal procedures, which can indicate potential risks or complications that might occur when the position of the surgical dissection plane alters.

% \subsubsection{Unsupervised Anomaly Detection for a Smart Autonomous Robotic Assistant Surgeon (SARAS)}
In recent developments, unsupervised learning techniques have been applied to robotic surgery through the use of deep residual autoencoders. These models learn to reconstruct normal surgical activities by training on large datasets of standard operations. During actual procedures, significant deviations from the expected reconstruction are detected and flagged as anomalies, highlighting potential issues. The unsupervised nature of this approach makes it particularly valuable in environments where labeled data is scarce or unavailable, making it an ideal tool for real-time monitoring of complex robotic surgeries \cite{li2024advances}. This method ensures that deviations from expected surgical patterns are identified, improving surgical accuracy and patient safety.


% \subsubsection{Learning Spatiotemporal Features for Esophageal Abnormality Detection from Endoscopic Videos}
This method focuses on capturing temporal and spatial abnormalities in endoscopic videos of the esophagus, as shown in Fig.~\ref{fig:endo}. By employing CNNs that track changes over time, the system can detect subtle anomalies that may be indicative of esophageal diseases, such as early-stage cancers or dysplasia \cite{ghatwary2020learning}. This method demonstrates how ML can improve diagnostic precision in challenging clinical contexts.
\begin{figure}[h!]
  \centering
  \includegraphics[width=\columnwidth]{endoscopic.png} % Set the width to \columnwidth for one-column width
  \caption{Detection results from complex endoscopic frames, highlighting challenges in identifying esophageal abnormalities in surgical scenes: (a) tool occlusion, (b) bubbles, (c) motion blur, and (d) fog. Red boxes indicate AI model predictions, and blue boxes represent ground truth annotations. Images from \cite{ghatwary2020learning}.}
  \label{fig:endo} 
\end{figure}

%Examples of detection results from complex endoscopic frames that obscure the region of esophageal abnormality. a) Tool appearance, b) bubbles, c) blurry, d) fog. Images are retrieved from \cite{ghatwary2020learning}


% \subsubsection{Deep Learning for Medical Anomaly Detection - A Survey \cite{fernando2021deep}}
Moreover, \cite{fernando2021deep} provides a comprehensive review of DL applications in the detection of medical anomalies, highlighting key methodologies, successes, and ongoing challenges. It underscores the importance of DL in enhancing the diagnostic capabilities of medical imaging technologies. The survey highlights several critical advancements in the field, such as the ability of DL models to automatically learn and extract intricate features from large medical datasets, significantly improving the accuracy and efficiency of anomaly detection compared to traditional methods. For example, CNNs have been widely adopted for detecting anomalies in medical imaging, particularly for identifying abnormalities in radiological scans, such as tumors in CT or MRI images. RNNs and LSTM networks, on the other hand, are particularly effective in sequential medical data, such as electrocardiograms (ECG) and electroencephalograms (EEG), where temporal dependencies are crucial to detect irregularities.



\subsection{Surgical Video Summarization}

Surgical video summarization is crucial in the medical field for education, documentation, and preoperative planning, enabling efficient review of lengthy and complex surgeries. ML and DL techniques play a vital role in this process by identifying key events, actions, and phases in surgical videos to create concise summaries without losing essential information. One prominent method is the automatic summarization of endoscopic surgical videos, where systems like those developed by King et al. \cite{king2022automatic} automatically generate summaries by identifying critical surgical phases, condensing lengthy recordings into essential, manageable segments. This allows surgeons and trainees to quickly review procedures, capturing major events such as the start and end of critical phases. Zisimopoulos et al. \cite{xiao2019deep} introduced another method using hierarchical DL models that capture both temporal and spatial information to accurately identify important segments. This technique uses a combination of convolutional and recurrent neural networks to provide contextually rich summaries.

Esteva et al. \cite{esteva2019guide} suggest combining video summarization with real-time feedback systems to enhance surgery decision-making, potentially integrating with surgical robots or augmented reality platforms for real-time procedural guidance. Advanced techniques further refine surgical video summarization such as King et al. \cite{king2023automatic}, they utilized object detection and Hidden Markov Models (HMMs) to segment and summarize skull base surgeries by highlighting key instruments and actions. In addition, live tags from surgical teams mark important moments, enabling collaborative summarization. A deep multi-scale pyramidal features network dynamically summarizes complex surgeries by capturing hierarchical structures in the content, suitable for procedures requiring multiple detail levels \cite{hashimoto2018artificial}. Furthermore, techniques such as deep feature matching and motion analysis specifically tailor summarization for wireless capsule endoscopy by focusing on areas of pathological interest \cite{garcia2023videosum}. These innovations collectively improve the field of surgical video summarization, providing powerful tools for education, documentation, and clinical review.







% Surgical video summarization has become an increasingly important tool in the medical field, particularly for education, documentation, and preoperative planning. As surgical procedures are often long and complex, summarization techniques allow clinicians, students, and researchers to quickly review the most critical parts of surgery, significantly reducing the time required for video analysis. These summaries offer concise representations of essential steps, enabling more efficient learning and decision-making, especially in high-stakes medical environments. ML and DL techniques have been instrumental in automating this process by identifying key events, actions, and phases in surgical videos and summarizing them without losing essential information.

% % \subsubsection{Automatic Summarization of Endoscopic Surgical Videos}
% One prominent approach in surgical video summarization is the automatic summarization of endoscopic surgical videos, as demonstrated by King et al. \cite{king2022automatic}. This system employs advanced algorithms to automatically generate video summaries by identifying critical phases of the procedure. It effectively condenses lengthy endoscopic recordings into shorter, more manageable segments while retaining all essential surgical steps. This technology benefits both surgeons and trainees, allowing them to quickly review procedures without sacrificing the quality of instruction or key insights. The algorithm focuses on detecting major events, such as the start and end of critical phases like tissue dissection or suturing, ensuring that the summary includes all the necessary information for a comprehensive understanding of the procedure. Other notable works in surgical video summarization include the framework developed by Zisimopoulos et al. \cite{xiao2019deep}, which uses hierarchical DL models to capture both temporal and spatial information in surgical videos. This method employs a combination of convolutional and recurrent neural networks to automatically identify important segments, providing an accurate and informative summary of the video. The ability to recognize spatiotemporal patterns enables the system to produce high-quality summaries that are not only concise but also contextually rich.
% Furthermore, combining video summarization with real-time feedback systems, as suggested by Esteva et al. \cite{esteva2019guide}, could provide actionable insights during surgeries, enhancing decision-making capabilities. The integration of these systems into surgical robots or augmented reality platforms may also represent the next frontier, where video summarization plays a role in guiding surgeons during procedures in real-time.

% % \subsubsection{Advanced Techniques for Surgical Video Summarization}
% Surgical video summarization has seen significant advancements through the integration of various ML techniques. One approach utilizes object detection combined with HMMs to segment and summarize skull base surgeries, identifying key surgical instruments and actions to highlight the most critical segments for educational and review purposes \cite{king2023automatic}. Another innovative method involves the use of live tags by surgical teams to mark important moments or complications during procedures, enabling collaborative summarization that incorporates direct input from the operating team. In a different technique, a deep multi-scale pyramidal features network dynamically summarizes videos by capturing hierarchical structures in the content, which is particularly useful for complex surgeries where multiple levels of detail are essential. A comprehensive analysis of DL models used in video summarization highlights the varying effectiveness of these techniques across surgical contexts, offering insights into their operational mechanisms \cite{hashimoto2018artificial}. Additionally, surgical video captioning with mutual-modal concept alignment enhances summarization by aligning spoken commands with visual data, ensuring a contextually enriched summary. For wireless capsule endoscopy, a specialized method applies deep feature matching and motion analysis to condense hours of video into concise segments, focusing on areas of pathological interest \cite{garcia2023videosum}. These approaches collectively advance the field of surgical video summarization, offering improved tools for education, documentation, and clinical review.


\subsection{Surgical Skills Assessment}
Surgical skills assessment is a fundamental aspect of maintaining high standards in surgical training and practice, and recent advancements in ML have led to the development of innovative and objective methods for evaluating these skills. One such approach is the Contrastive Regression Transformer model, which is designed to assess surgical skills during robotic surgeries by analyzing video data. This model captures subtle movements and decision-making processes to provide a quantitative assessment that aids in improving surgical performance \cite{anastasiou2023keep}. Additionally, video-based analysis of recognized surgical gestures and skill levels has been explored in the work by Wang et al. \cite{wang2020towards}, where ML techniques are used to correlate specific actions with skill levels, offering a more detailed and interpretable evaluation of surgical proficiency. This method enhances the feedback given to surgeons during training, ensuring a more tailored and effective learning process. Furthermore, the link between a surgeon's technical skills and patient outcomes has been well-established, as highlighted by Stulberg et al. \cite{stulberg2020association}, emphasizing the critical impact that high surgical proficiency has on improving clinical outcomes. These developments demonstrate the growing role of ML in refining surgical training and ensuring optimal patient care.

\begin{table*}[hbtp]
\centering
\caption{Overview of Foundation Models in Medical Imaging across Various Modalities and Tasks. This table summarizes key methodologies, their model specifications, modalities, downstream tasks, and primary findings. The abbreviations here are Seg: Segmentation, Cls: Classification, Det: Detection, IE: Image Enhancement}
\label{tab:flapplications}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}p{0.5cm}p{1.7cm}>{\centering\arraybackslash}m{1cm}p{2cm}p{2.3cm}p{1.7cm}p{5.8cm}@{}}
\toprule
\textbf{Year} & \textbf{Method} & \textbf{Code} & \textbf{Model} & \textbf{Modality} & \textbf{Downstream Task} & \textbf{Key Finding}\\
\midrule
2023 & MA-SAM \cite{chen2024ma} & \textcolor{blue}{\checkmark} & ViT (SAM) & CT, MRI, Endoscopy & Seg & Improved robustness across modalities \\ 
2023 & Endo-FM \cite{wang2023foundation} & \textcolor{blue}{\checkmark} & ViT & Endoscopy & Seg, Cls, Det & Surpasses SOTA with significant margins in pre-training and transfer learning\\
2023 & VisionFM \cite{qiu2023visionfmmultimodalmultitaskvision} & \textcolor{blue}{\xmark} & - & Multimodal images &  Cls &  Delivers expert-level diagnostic accuracy in ophthalmology, surpassing traditional models and specialists with its modality-agnostic approach\\
2023 & Polyp SAM++ \cite{article} & \textcolor{blue}{\checkmark} & ViT (SAM) & Endoscopy & Seg & Advances segmentation using text-guided SAM, enhancing localization and accuracy, effectively handling complex colonoscopy images\\
2024 & SP-SAM \cite{yue2024surgicalpartsam} & \textcolor{blue}{\checkmark} & ViT & Endoscopy & Seg & Integrates part-level prompts with image embeddings, enhancing detailed structure segmentation, with minimal parameters\\
2024 & Polyp-SAM\cite{li2024polyp} & \textcolor{blue}{\checkmark} & ViT (SAM) & Endoscopy & Seg & Utilizes fine-tuned SAM for polyp segmentation, achieving state-of-the-art results with superior generalization, Dice scores \\
2024 & Swinsam\cite{feng2025swinsam} & \textcolor{blue}{\checkmark} & ViT (SAM) & Endoscopy & Seg & Integrates Swin Transformer decoder with SAM encoder, improving segmentation detail significantly, enhancing performance metrics  \\
2024 & SurgicalSAM\cite{yue2024surgicalsam} & \textcolor{blue}{\checkmark} & ViT (SAM) & Endoscopy & Seg & Introduces efficient tuning for SAM with a class prompt encoder, achieving SOTA performance with reduced parameter complexity and improved pipeline simplicity \\
2024 & USFM\cite{jiao2024usfm} & \textcolor{blue}{\xmark} & - & US & Seg, Cls, IE & Demonstrates adaptability across organs and tasks with minimal annotations, enhancing robust feature extraction with spatial-frequency dual masking\\
2024 & LVM-Med\cite{mh2024lvm} & \textcolor{blue}{\checkmark} & ResNet, ViT & Multimodal images  & Seg, Cls, Det & Sets new benchmarks in medical imaging by leveraging large-scale self-supervised learning and second-order graph matching to enhance feature learning and adaptability \\
2024 & VoCo\cite{wu2024voco} & \textcolor{blue}{\checkmark} & Swin & CT  & Seg, Cls &   Utilizes volume contrastive learning to encode high-level semantics, significantly enhancing performance on segmentation and classification tasks\\
2024 & RudolfV\cite{dippel2024rudolfv} & \textcolor{blue}{\xmark} & ViT(DINO-v2) & Pathology  & Cls &  Utilizes pathologist-informed design and semi-automated data curation from a diverse dataset, significantly enhancing diagnostic accuracy and robustness across various benchmarks \\
2024 & AFTerSAM\cite{yan2024after} & \textcolor{blue}{\xmark} & ViT (SAM) & CT  & Seg &  Enhances SAM with Axial Fusion Transformer, improving 3D medical image segmentation by integrating intra-slice and inter-slice contextual information with minimal training data\\
2024 & PUNETR\cite{fischer2024prompt} & \textcolor{blue}{\checkmark} & - & CT  & Seg &  Introduces prompt tuning for efficient semantic segmentation, achieving significant parameter efficiency with robust performance on medical imaging datasets \\

\midrule

\end{tabular}
}
\end{table*}

\subsection{Vision Language Foundation Model}
DINO \cite{caron2021emerging} has shown promise in revolutionizing surgical imaging by enabling improved interpretation and segmentation of medical images through its self-supervised learning approach. In surgical settings, DINO's ability to learn detailed visual representations from unlabeled surgical imagery can significantly improve the accuracy and efficiency of surgical planning and intraoperative guidance. For example, DINO has been applied to automated segmentation of tumor boundaries in real-time during oncological surgeries, helping surgeons achieve more precise excision and margin control. This technology also facilitates the development of advanced diagnostic tools that can automatically differentiate between healthy and pathological tissues, enhancing the surgeon's ability to make informed decisions during complex procedures \cite{koksal2024surgivid}. In addition, by improving the quality and usability of intraoperative images, such as in laparoscopic video feeds, DINO contributes to safer and more efficient surgical workflows. This capability is crucial for MIS procedures, where the clarity and detail of visual information directly impacts precision surgery and therefore reduced postoperative complications. 

Recently, ``Surgical-DINO,'' an adaptation of the DINO model, has been specifically designed to enhance depth estimation in endoscopic surgery, demonstrating the adaptability of foundational models to specialized surgical tasks \cite{cui2024surgical}. Moreover, the SURGIVID project leverages DINO's self-supervised learning framework for annotation-efficient surgical video object discovery, further highlighting its utility in reducing the labor-intensive process of video annotation in surgical training and analysis \cite{koksal2024surgivid}. Furthermore, DINO's capabilities could be instrumental in analyzing datasets like EgoSurgery-Tool, which focuses on detecting surgical tools and hands from egocentric perspectives in open surgery videos, potentially enhancing the training and performance of AI models in recognizing and interacting with complex surgical environments \cite{fujii2024egosurgery}. The adoption of DINO in these surgical applications not only streamlines the surgical process but also opens possibilities for more adaptive and responsive surgical systems, potentially reducing the incidence of complications and improving overall surgical outcomes\cite{rabbani2024can}.


%\subsection{Vision Language Model}
CLIP and other vision-language foundation models (VLMs) are also reshaping the landscape of medical imaging by effectively linking textual descriptions to visual data. This capability is pivotal for diagnosing radiological images, enhancing surgical planning, and facilitating educational initiatives. The zero-shot learning capability of CLIP introduced by Radford et al. \cite{radford2021learning} allows it to recognize and categorize medical conditions across various imaging modalities, such as X-rays and MRIs, without direct training on specific medical datasets. This cross-modal understanding is especially beneficial in scenarios where large annotated datasets are scarce. Building on this application, Kerdegari et al. \cite{kerdegari2024foundational} demonstrated how foundational models could be adapted to improve the detection and categorization of pathologies in specialized medical imagery, such as endoscopy images for the diagnosis of gastric inflammation (gastritis). This study illustrates CLIP's potential to enhance diagnostic accuracy in complex imaging scenarios.

Further expanding the scope of foundational models in healthcare, Sun et al. \cite{sun2024medical} provides a comprehensive examination of medical multimodal foundation models in clinical diagnosis and treatment. Their research focuses on various applications, challenges, and future directions of these technologies in healthcare. They highlight how these models integrate diverse data types by combining visual, textual, and possibly even genomic information to offer more nuanced diagnostics and treatment strategies. This integration facilitates a deeper understanding of patient data, crucial for personalized medicine and advanced treatment planning. Zhao et al. \cite{zhao2023clip} also contribute to this body of knowledge by surveying the deployment of CLIP in medical fields, revealing its transformative impact from routine diagnostics to complex surgical interventions. Table \ref{tab:flapplications} provides a detailed overview of these foundational models, outlining their specifications, modalities, and key advancements in enhancing precision and robustness across various medical imaging and clinical tasks.



% \section{Applications using Foundational Models}
% \label{sec:fmapp}
% Foundational models have emerged as transformative tools in medical imaging and surgical data science, addressing tasks ranging from segmentation and classification to natural language processing and multi-modal learning. This section explores their diverse applications, highlighting segmentation models for precise delineation of anatomical structures, large language models for generating clinical narratives, vision-language models for cross-modal tasks, and generative AI models in surgical data science. 
% Foundational models have emerged as transformative tools in medical imaging and surgical data science, addressing tasks ranging from segmentation and classification to natural language processing and multi-modal learning. 

%Recent foundational models have been used for many different applications ranging from tool segmentation to describing operative notes. Table \ref{tab:flapplications} provides a comprehensive overview of various foundational models used in medical imaging, detailing their specifications, modalities, and primary tasks such as segmentation, classification, and detection. It highlights key methodologies and advancements across various models and tasks. It emphasizes their applicative roles in enhancing precision and robustness in medical image analysis across different modalities like CT, MRI, and endoscopy.
% highlighting advancements in AI technologies tailored for different medical modalities and tasks.











%\subsection{Generative Pretraining Model}

% \subsection{GenAI/Foundation Models in Surgical Data Science}

%old one 
\subsection{Visual Question Answering}

\begin{table*}[h!]

\centering
\caption{Summary of recent advancements in Visual Question Answering for surgical applications}
\begin{tabular}{p{0.15\linewidth} p{0.3\linewidth} p{0.15\linewidth} p{0.15\linewidth} p{0.15\linewidth}}
\toprule
\textbf{Paper} & \textbf{Key Findings} & \textbf{Dataset Used} & \textbf{Model Type} & \textbf{Application Area} \\ \midrule
Surgical VQA \cite{seenivasan2022surgical} & Introduced transformer-based Surgical-VQA for integrating visual-text information in training. & Surg-VQA & Transformer-based & Surgical training \\ 
LV-GPT \cite{seenivasan2023surgicalgpt} & Developed LV-GPT for enhanced word-vision token sequencing in VQA. & Surg-QA & Language-Vision GPT & Robotic surgery assistance \\ 
SSG-VQA-Net \cite{yuan2024advancing} & Used scene graph-based model for scene-aware reasoning in surgical VQA. & SSG-QA & Scene Graph Network & Scene understanding \\ 
Surgical-VQLA++ \cite{bai2025surgical} & Improved VQA robustness via adversarial contrastive learning. & Surg-LA & Contrastive Learning & Error detection \\ 
Surgical-LVLM \cite{wang2024surgical} & Applied specialized perception blocks for complex surgical scenes. & Surg-LVLM & Vision-Language Model & Complex surgical environments \\ 
Surgical-VQLA \cite{bai2023surgical} & Used gated vision-language embedding for localized answering. & Robo-Surg & Gated V-L Embedding & Robotic surgery \\ 
Surgical-LLaVA \cite{jin2024surgical} & Tailored large language and vision model for multi-modal chat in surgery. & LLaVA-Surg & LLaVA Model & Multimodal assistance \\ 
LLaVA-Surg \cite{li2024llava} & Advanced multimodal surgical assistant capabilities with Surg-QA. & Surg-QA & Vision-Language & Video question answering \\ 
GP-VLS \cite{schmidgall2024gp} & General-purpose VQA model with comprehensive surgical datasets. & GP-VLS & General Purpose V-L Model & Surgical evaluation \\ 
CAT-ViL \cite{bai2023cat} & Co-attention gated model for effective training in robotic surgery. & Surg-CAT & Co-Attention Model & Surgical training \\ 
\bottomrule
\end{tabular}
\label{tab:vqa_summary}
\end{table*}




% \begin{table}[h!]
% \centering
% \caption{Summary of recent advancements in Visual Question Answering for surgical applications}
% \begin{tabular}{p{0.3\linewidth}p{0.6\linewidth}}
% \toprule
% \textbf{Paper} & \textbf{Key Findings} \\ \midrule
% Surgical VQA \cite{seenivasan2022surgical} & Introduced Surgical-VQA using transformer models, enhancing the integration of visual-text information for surgical education and training. \\ 
% LV-GPT \cite{seenivasan2023surgicalgpt} & Developed LV-GPT, a Language-Vision GPT model for surgical VQA, demonstrating superior performance by carefully sequencing word and vision tokens. \\ 
% SSG-VQA-Net \cite{yuan2024advancing} & Advanced VQA with a scene graph-based model, SSG-VQA-Net, incorporating scene-aware reasoning for better understanding complex, surgical scenes. \\ 
% Surgical-VQLA++ \cite{bai2025surgical} & Introduced Surgical-VQLA++, leveraging adversarial contrastive learning to improve robustness against image corruptions in VQA tasks. \\ 
% Surgical-LVLM \cite{wang2024surgical} & Proposed Surgical-LVLM, employing specialized visual perception blocks for improving VQA accuracy and grounding in complex surgical environments. \\ 

% Surgical-vqla \cite{bai2023surgical} & Developed Surgical-VQLA using gated vision-language embedding to facilitate localized answering in robotic surgery, improving integration of visual and textual information without the need for object detection models. \\ 
% Surgical-LLaVA \cite{jin2024surgical} & Introduced Surgical-LLaVA, a large language and vision model tailored for surgical scenarios, demonstrating superior performance in multi-modal chat abilities and instruction-following within surgical contexts. \\ 
% LLaVA-Surg \cite{li2024llava} & Created LLaVA-Surg and the Surg-QA dataset to advance multimodal surgical assistant capabilities, showing significant improvements in zero-shot surgical video question-answering tasks. \\ 
% GP-VLS \cite{schmidgall2024gp} & Proposed GP-VLS, a general-purpose vision language model for surgery, which significantly outperforms other models in surgical vision-language tasks with new datasets for comprehensive evaluation. \\ 
% CAT-ViL \cite{bai2023cat} & Proposed CAT-ViL, a co-attention gated vision-language model for localized answering in robotic surgery, which provides enhancements in surgical training and understanding through effective multimodal embedding. \\ 


% \bottomrule
% \end{tabular}

% \label{tab:vqa_summary}
% \end{table}


% Visual Question Answering (VQA) in surgery utilizes artificial intelligence to analyze visual data from surgical videos and provide context-based answers to support decision-making and enhance learning. One approach, Surgment, employs segmentation techniques to improve VQA functionality, enabling users to query specific visual components in surgical videos and receive detailed explanations, thus advancing video-based surgery learning \cite{wang2024surgment}. Another method, Surgical-VQA, utilizes transformer architectures to deliver accurate and contextually aware responses to questions about surgical scenes, fostering a more interactive and immersive learning environment \cite{seenivasan2022surgical}.

Visual Question Answering (VQA) in surgery uses AI to analyze visual data from surgical videos, enhancing decision-making and educational experiences for medical professionals. This application interprets dynamic visual content in real time, deepening understanding of complex surgical procedures, and serves as a crucial tool for training and operational assistance. The Surgment system pioneers this field with advanced segmentation techniques, enabling specific queries about visual elements in surgical videos, such as identifying tools or recognizing procedural actions, thereby enriching video-based surgical training \cite{wang2024surgment}. The Surgical-VQA method utilizes transformer architectures, adept at processing video streams, to provide precise, contextually relevant answers, enhancing real-time surgical decision-making \cite{seenivasan2022surgical}.

Further extending these capabilities, the Language-Vision GPT (LV-GPT) model integrates visual data processing with traditional GPT-2, using a vision tokenizer and vision token embeddings designed for surgical VQA. This model outperforms existing frameworks on datasets like the Endoscopic Vision Challenge Robotic Scene Segmentation 2018 and CholecTriplet2021, setting new benchmarks for AI-driven VQA applications in surgery \cite{seenivasan2023surgicalgpt}. Furthermore, VQA technologies significantly aid postoperative analysis, allowing surgical teams to review complex procedures with AI-enhanced visual data analysis, potentially improving surgical techniques and outcomes. Integrating VQA with augmented reality (AR) and virtual reality (VR) technologies could further revolutionize surgical training and operative workflows by providing AI-generated annotations and insights during procedures, enhancing surgeons' field of view, reducing errors, and improving outcomes. Recent advances in VQA for surgical applications include transformer-based models and sophisticated systems that improve the robustness and precision of VQA tasks in complex scenarios. These developments underscore the increasing importance of VQA systems in surgical training, decision support, and procedural guidance, promising to transform surgical education and operations by improving the cognitive capabilities of surgical systems.



Table \ref{tab:vqa_summary} summarizes recent advancements in VQA for surgical applications, showcasing a variety of models, datasets, and key findings that improve tasks such as scene understanding, error detection, and surgical training. It highlights innovative approaches like transformer-based models and graph networks, which enhance the interpretability and accuracy of surgical VQA systems.
% Visual Question Answering (VQA) in surgery leverages artificial intelligence to analyze visual data from surgical videos, providing context-based answers that aid decision-making and enhance educational experiences for medical professionals. By interpreting dynamic visual content in real-time, this innovative application of AI facilitates a deeper understanding of complex surgical procedures. It serves as an invaluable tool for both training and operational assistance in surgical environments, transforming how surgeons learn and perform operations.

% A pioneering approach in this domain is the Surgment system, which incorporates advanced segmentation techniques to enhance the functionality of VQA systems. This technology allows users to make specific queries about different visual elements within surgical videossuch as identifying surgical tools, understanding tissue types, or recognizing standard and anomalous procedural actions. By providing detailed and accurate explanations in response to these queries, Surgment significantly enriches the educational value of video-based surgical training \cite{wang2024surgment}. It transforms passive video observation into an interactive learning platform where every frame can be queried and understood in depth, bridging the gap between theoretical learning and practical, hands-on experience.

% Additionally, the Surgical-VQA method leverages the power of transformer architectures, which are particularly effective for processing sequential data like video streams. These architectures excel in handling the complexities of surgical video analysis by providing precise and contextually relevant responses to inquiries about ongoing procedures \cite{seenivasan2022surgical}. This capability makes Surgical-VQA an essential tool for enhancing real-time decision-making during surgeries. By enabling more interactive and engaging learning experiences, it helps both trainees and seasoned surgeons refine their skills and decision-making processes under the guidance of AI-driven insights.

% Extending these advancements, the Language-Vision GPT (LV-GPT) model enhances traditional GPT-2 capabilities by incorporating visual data processing through a vision tokenizer and vision token embeddings, specifically designed for VQA in surgical contexts. By sequencing textual queries prior to visual data, LV-GPT mimics human cognitive processes to enhance the accuracy and relevance of its responses. Demonstrating significant progress, the LV-GPT model outperforms existing VQA frameworks on surgical datasets such as the Endoscopic Vision Challenge Robotic Scene Segmentation 2018 and CholecTriplet2021, establishing a new benchmark for AI-driven VQA applications in robotic surgery \cite{seenivasan2023surgicalgpt}.

% Beyond training, VQA technologies play a critical role in post-operative analysis and review. They allow surgical teams to revisit complex procedures and learn from actual practices with the benefit of AI-enhanced visual data analysis. This retrospective capability can lead to improvements in surgical techniques and patient outcomes by identifying nuances and subtleties that might not be captured through human observation alone.

% Furthermore, the integration of VQA into surgical practice is anticipated to evolve with the adoption of augmented reality (AR) and virtual reality (VR) technologies. These technologies can simulate surgical environments for training purposes or overlay critical information during actual procedures. This integration could dramatically enhance the surgeon's field of view with AI-generated annotations and insights, potentially reducing errors and improving surgical outcomes.

% In recent years, significant advances have been made in VQA specifically tailored for surgical applications. These developments encompass a range of methodologiesfrom transformer-based models that enhance text-visual data integration to sophisticated systems that improve the robustness and accuracy of VQA tasks under complex surgical scenarios. Table \ref{tab:vqa_summary} summarizes the key findings from recent research works, illustrating diverse approaches and their contributions to the field. These advancements underscore the growing importance of VQA systems in surgical training, decision support, and real-time procedural guidance. They promise to revolutionize aspects of surgical education and operative workflows by enhancing the cognitive capabilities of surgical systems.





\subsection{Surgical Video Retrieval}
Efficient retrieval of surgical video content is vital in education, training, and clinical review, allowing users to quickly access relevant video segments based on specific queries. One notable method involves unsupervised feature disentanglement, which separates and identifies key features within surgical videos to improve the accuracy of retrieval systems in MIS \cite{wang2022unsupervised}. This unsupervised approach improves the ability to search large volumes of video data (analogous to a comprehensive surgical video library) without relying on extensive labeling, thus streamlined access to critical information during surgical review and training \cite{mantri2024intelligent}.
% \subsection{Concluding Insights and Implications from DL Advances in Surgery}

\begin{table*}[!t]
\centering
\caption{Comparative analysis of deep learning models for real-time segmentation and instrument identification in various surgical procedures.}
\label{tab:comparison-models}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c>{\raggedright}p{3cm}>{\raggedright}p{2.5cm}>{\raggedright}p{3cm}>{\raggedright}p{3cm}>{\raggedright}p{1.5cm}>{\raggedright}p{3cm}>{\raggedright\arraybackslash}p{3cm}@{}}
\toprule
\textbf{Reference} & \textbf{Procedure} & \textbf{Dataset} & \textbf{Model} & \textbf{Application} & \textbf{Modality} & \textbf{Task/Tool} & \textbf{Results} \\ \midrule

\vspace{0.5mm}
\cite{wang2019graph}  & Cholecystectomy & m2cai16-tool + Cholec80 & 3D DenseNet+GCN & Instrument Classification & Images & Tool & 90.2\% + 90.13\% (mAP)  \\

\vspace{0.5mm}
\cite{namazi2019attention}  & Cholecystectomy & Cholec80 & LSTM & Phases Boundary Detection & Images & Task & 48 s (MAE)  \\

\vspace{0.5mm}
\cite{qin2020temporal}  & In-vitro experiments & JIGSAWS + NPA & VGG16 & Gesture Segmentation & Images + Kinematics data + events & Task & 86.3\% + 89.4\% (acc) \\

\vspace{0.5mm}
\cite{nwoye2020recognition}  & Cholecystectomy & Cholec80 & ResNet18 & Fine-grained activities & Images & Task & 24.8\% (acc) \\

\vspace{0.5mm}
\cite{zhao2018fast}  & In-vitro experiments & JIGSAWS & Dense CNN & Trajectory Segmentation & Images + Kinematics data & Task & 70.6\% (mAP) \\

\vspace{0.5mm}
\cite{jin2020multi}  & Cholecystectomy + Colorectal surgery & Cholec80 & VGG-50 + LSTM & Phases Recognition & Images & Task & 89.2\% (acc) \\

\vspace{0.5mm}
\cite{aksamentov2017deep}  & Cholecystectomy & Cholec120 & ResNet-152 & Surgery Time Prediction & Images & Task & 460 s (MAE) \\

\vspace{0.5mm}
\cite{petscharnig2018early}  & Gynecology & NPA & GoogleNet & Phases Recognition & Images & Task & 79.6\% (AP) \\

\vspace{0.5mm}
\cite{kannan2019future}  & Nine different surgeries & NPA & VGG16 + LSTM & Surgery Type Recognition & Images & Task & 75\% (acc) \\

\vspace{0.5mm}
\cite{colleoni2019deep}  & Colorectal Surgery & EndoVis 2015 & 3D FCNN & Instrument Detection & Images & Tool & 85.1\% (Dice) \\

\vspace{0.5mm}
\cite{pakhomov2020searching}  & Porcine Procedures & EndoVis 2017 & ResNet18 & Instrument Segmentation (Binary + Parts) & Images & Tool & 89.6\% + 76.4\% (Dice) \\

\vspace{0.5mm}
\cite{liu2020anchor}  & In-vitro experiments & Atlas Dione + EndoVis 15 & Stacked Hourglass & Instrument Detection & Images & Tool & 98.5\% + 100\% (mAP) \\

\vspace{0.5mm}
\cite{petscharnig2018learning}  & Gynechologic surgeries & NPA & GoogleNet & Anatomical Structures Classification & Images & Task & 78.1\% (acc) \\

\vspace{0.5mm}
\cite{kadkhodamohammadi2019feature}  & Sleeve Gastrectomy & EndoVis 2015 & Xception & Surgical Scenario Segmentation & Images & Task & 98.44\% (Dice) \\

\vspace{0.5mm}
\cite{petscharnig2017deep}  & Colonoscopy & EndoVis 2015 & VGG16 & Polyp Detection & Images & Task & 80\% (Dice) \\

\vspace{0.5mm}
\cite{lee2019weakly}  & Phantom \% Porcine Tissue & NPA & LinkNet-152 & Binary Instrument Segmentation & Images & Tool & 88.9\% (Dice) \\

\cite{marullo2023multi} & Laparoscopic Surgery & Surgical videos with RGB images & Multi-task CNN with U-Net encoder & Blood accumulation detection and tool semantic segmentation & Video & Tool segmentation and event detection & 81.89\% + 90.63\% (Dice) \\

\cite{baby2023forks} & Minimally Invasive Surgeries & EndoVis2017, EndoVis2018 & Custom network with classification module & Instance segmentation of surgical instruments & Image & Instrument segmentation & Improves SOTA by at least 12 points on EndoVis2017 \\

\cite{yue2024surgicalsam} & Surgical instrument segmentation & EndoVis2018, EndoVis2017 & SurgicalSAM with prototype-based class prompt encoder & Segmentation using SAM with enhanced class prompting & Image & Instrument segmentation & SOTA performance with minimal tunable parameters \\

\cite{wang2024video} & Robotic Surgery & Custom surgery video datasets & VIS-Net with Graph-based Relation-aware Module & Referring video instrument segmentation using text & Video & Instrument segmentation based on language descriptions & Significantly outperforms existing methods \\

\cite{kolbinger2023anatomy} & Laparoscopic Surgery & Dresden Surgical Anatomy Dataset with 13,195 images & DeepLabv3, SegFormer & Anatomy segmentation & Image & Semantic segmentation of anatomical structures & 0.23 to 0.85 (IoU); outperforms human experts \\

%%%

\cite{kolbinger2024strategies} & Laparoscopic Surgery & DSAD & SegFormer, DeepLabv3 & Anatomy segmentation, enhance real-world applicability & Image & Semantic segmentation of anatomical structures & Improved IoU, accuracy, precision, recall, F1 score, specificity, Hausdorff Distance, and ASSD \\

\cite{mao2024pitsurgrt} & Pituitary Surgery & Custom Dataset & PitSurgRT with HRNet & Real-time landmark detection and semantic segmentation of critical anatomical structures & Video & Instrument segmentation, anatomy localization & Improved landmark detection mean error; IoU increased by 4.39\% \\

\cite{park2024deep} & Prostate Surgery & Intraoperative Videos of RALP & Reinforcement U-Net & Real-time semantic segmentation in robotic prostatectomy & Video & Segmentation of instruments, bladder, prostate, seminal vesiclevas deferens & Dice score 0.96, 0.74, 0.85, 0.84 respectively; IoU 0.77 \\

\cite{grammatikopoulou2024spatio} & Laparoscopic and Partial Nephrectomy & CholecSeg8k, Private Dataset & Spatio-temporal network & Enhance temporal consistency in video semantic segmentation & Video & Surgical scene segmentation & Improved temporal consistency and mean IoU by 1.30\% and 4.27\% on datasets \\

\cite{chen2024asi} & General Surgery & EndoVis2018, EndoVis2017 & ASI-Seg with SAM & Audio-driven instrument segmentation with surgeon intention understanding & Image & Semantic segmentation and intention-oriented segmentation & 82.37\% + 71.64\% (IoU) \\

\bottomrule
\end{tabular}
 }
% \end{table}
\end{table*}



\subsection{Concluding Insights and Implications}
%from Foundational Model Advances in Surgery}

This review has delved into a broad range of methodologies and applications in the realm of surgical video analysis and instrument detection, as illustrated in Table \ref{tab:comparison-models}. The exploration underscores the rapid progress in deploying FMs, such as advanced neural networks, to enhance surgical outcomes and support real-time decision making. 



These technologies have evolved from basic instrument classification to sophisticated dynamic phase recognition, showcasing the substantial potential to increase the precision and efficiency of surgical procedures. Adopting state-of-the-art neural architectures and segmentation techniques has particularly enhanced the sensitivity and specificity of these models, making them essential tools in the operating room. This progression demonstrates how foundational models not only refining current surgical practices but also setting the stage for future innovations in medical technology.


\subsection{Computational Complexity of FMs in Surgical Applications}

FMs have revolutionized the field of surgical AI by offering robust generalization capabilities and exceptional performance across a variety of tasks \cite{khan2025comprehensive}. However, these models typically require vast amounts of data and considerable computational resources for training, making it challenging for deployment in clinical environments. This section explores effective strategies to mitigate the computational demands of foundational models, ensuring their practical applicability in surgical settings.

Foundational models like SAM \cite{kirillov2023segment} and CLIP \cite{radford2021learning} encompass diverse tasks and modalities, which require extensive training on large datasets. This extensive training enables them to perform well across different tasks but also makes them resource-intensive. In surgeries, where responses need to be immediate and accurate, the heavy computational demand of these models can be a significant limitation. For this purpose, several innovative techniques have been developed to adapt foundational models to be more resource-efficient, making them suitable for real-time surgical applications:

\begin{itemize}
    \item \textit{Adapters:} Adapters are small modules added to a pre-trained model that can be adjusted to new tasks. They require changing only a few parts of the model, which reduces the computational load significantly. In surgery, adapters can help tailor models to recognize specific instruments or actions using limited data without needing extensive retraining.

    \item \textit{Low-Rank Adaptation (LoRA): }LoRA modifies a model's deep structures in a way that reduces the amount of data they handle at once, which cuts down on the computing power required. This technique is useful for refining models to perform specific tasks like VQA and grounding for intricate surgical contexts \cite{wang2024surgical}.

    \item \textit{Prompt Tuning: }This method tweaks the inputs given to the model to guide it toward a particular function using the model's existing knowledge base. This approach is computationally light and can be used to adjust models for specific tasks, such as analyzing surgical videos, without extensive reprogramming \cite{wang2023sam}.

    \item \textit{Knowledge Distillation: }This process involves training a smaller, more manageable model to mimic a larger one. The smaller model retains much of the larger model's effectiveness but uses less computational power, making it better suited for use in surgeries where fast processing is crucial \cite{ding2022free}.

    \item \textit{Quantization and Pruning: }These techniques reduce the model's size and speed up its operations. Quantization decreases the precision of the numbers the model uses, and pruning removes parts of the model that have little impact on its performance \cite{kuzmin2023pruning}. Both adjustments help the model run faster and more efficiently, which is essential in a surgical setting.

    
\end{itemize}




% In this review, we have explored a diverse array of methodologies and applications within the field of surgical video analysis and instrument detection, as depicted in Table \ref{tab:comparison-models}. These studies highlight the rapid advancements in the application of DL models for enhancing surgical outcomes and real-time decision-making. From intricate instrument classification to dynamic phase recognition, the evolution of these technologies demonstrates significant potential for improving the accuracy and efficiency of surgical procedures. Notably, the integration of advanced neural architectures and segmentation techniques has led to notable improvements in the sensitivity and specificity of these models, making them invaluable tools in the operating room.









% \section{Available Datasets}

% Methods based on ML are heavily reliant on extensive training data, with their performance typically scaling with the quantity of available examples. This highlights the crucial role of dataset availability in constructing effective DL models. Many of the datasets for surgical tool navigation are freely accessible, allowing researchers who lack proprietary data to engage in model building and facilitating the comparison of methodologies. Table \ref{tab:datasetcomp}provides a comprehensive overview of various state-of-the-art models for instrument tracking, highlighting the diverse range of datasets these models have been trained and validated on. This table serves as a crucial resource for understanding the scope and application of different DL models in dynamic surgical environments. Models like LinkNet and ST-MTL, highlighted in the table, are utilized for complex tasks that integrate detection, segmentation, and tracking, demonstrating their efficacy in ensuring precise and real-time tracking of surgical instruments While most datasets provide video data, some also include kinematic data, and it is expected that future datasets will incorporate robotic kinematics to enable more sophisticated model architectures. Below, we categorize the main datasets used in this domain.


% \subsection{Endoscopic and Laparoscopic Tool Detection and Workflow Analysis}

% \subsubsection{EndoVis Challenge Series} Since 2015, the MICCAI Endoscopic Vision (EndoVis) challenges have significantly contributed to advancing AI research in endoscopic and laparoscopic tool detection. The EndoVis series includes datasets tailored to different tasks, such as segmentation, tracking, and classification. The inaugural EndoVis-2015 challenge \cite{bouget2015detecting} focused on segmenting and tracking rigid surgical instruments. Later challenges, including EndoVis-2017, introduced videos for binary and multi-class instrument segmentation. By 2018, the challenges included entire scene segmentation tasks, requiring the segmentation of robotic and non-robotic tools along with anatomical structures \cite{allan20202018}. The 2019 Robust-MIS challenge \cite{ross2021comparative} included a diverse dataset with 30 surgical procedures to enhance robustness and generalization, focusing on binary segmentation and instance segmentation of surgical tools. The SCARED (Stereo Correspondence and Reconstruction of Endoscopic Data) sub-challenge emphasized depth estimation, while the 2020 and 2021 challenges tackled domain adaptation and workflow recognition, respectively.

% \subsubsection{M2CAI16 Challenge} The M2CAI16 Challenge features two datasets for surgical workflow and tool detection. The m2cai-tool dataset \cite{twinanda2016endonet} includes 15 cholecystectomy videos10 for training and 5 for testingcaptured at the University Hospital of Strasbourg. It provides binary indicators for the presence of seven surgical tools. Later, the dataset was expanded to m2cai-tool-locations \cite{jin2018tool}, adding spatial annotations for 2,532 frames to facilitate tool localization.

% \subsubsection{Cholec80 and Extensions} Developed by Twinanda et al. \cite{twinanda2016endonet}, the Cholec80 dataset consists of 80 cholecystectomy videos performed by 13 surgeons, with annotations for tool presence and surgical phase recognition. The dataset includes 86,000 annotated images, with tool bounding boxes available for 10 videos. A later extension, the ITEC Smoke Cholec80 Image dataset \cite{aksamentov2017deep}, adds 100,000 frames annotated for smoke and non-smoke scenarios, specifically curated for research on smoke removal.

% \subsubsection{KvasirInstrument Dataset} The Kvasir-Instrument dataset \cite{jha2021kvasir} includes 590 annotated endoscopic frames from gastroscopies and colonoscopies, with binary segmentation masks and bounding boxes for diagnostic and therapeutic tools. The image resolutions range from 720576 to 12801024, covering instruments such as snares, balloons, and biopsy forceps.

% \subsubsection{ART-Net} The ART-Net dataset \cite{hasan2021detection} contains 29 videos of laparoscopic hysterectomy procedures using non-robotic instruments, recorded at a resolution of 19201080. In addition to binary segmentation annotations, the dataset includes geometric data for each instrument, enhancing its utility for tool presence detection.

% \subsection{Robotic Surgery Datasets}

% \subsubsection{ATLASDione} Compiled by Sarikaya et al. \cite{sarikaya2017detection}, the ATLASDione dataset comprises 99 videos documenting six robotic tasks conducted by 10 clinicians at the Roswell Park Cancer Institute using the da Vinci Surgical System. Annotations include tool bounding boxes, action types, durations, and the surgeons' skill levels, aiding in both tool detection and skill assessment.

% \subsubsection{dVPN Dataset} The dVPN dataset, compiled by Ye et al. \cite{ye2017self}, originates from da Vinci partial nephrectomy procedures. It provides 34,320 pairs of rectified stereo images for training and 14,382 pairs for testing, supporting stereo vision and depth estimation in robotic surgeries.

% \subsubsection{AutoLaparo Dataset} The AutoLaparo dataset \cite{wang2022autolaparo} includes 21 videos of laparoscopic hysterectomy procedures, each recorded in high definition (19201080). It contains sub-datasets for workflow recognition, laparoscope motion prediction, and segmentation of instruments and anatomical structures. Tool types annotated include grasping forceps, ligasure, dissecting forceps, and electric hook.

% \subsection{Specialized Surgical Domains}

% \subsubsection{NeuroSurgicalTools} The NeuroSurgicalTools dataset \cite{bouget2015detecting} consists of 14 videos recorded with Zeiss OPMI Pentero microscopes at 720x576 resolution. Captured during actual neurosurgeries at CHU Pontchaillou, Rennes, it features seven instruments with annotations that account for challenges such as occlusions, overlapping tools, blurring, and reflections.

% \subsubsection{FetalFlexTool} Developed by Garca-Peraza-Herrera et al. \cite{garcia2017real}, the FetalFlexTool dataset is an ex-vivo fetal surgery dataset with 21 training images and a 10-second testing video, annotated for tool segmentation. The training images were captured in air, while the testing video was recorded underwater to introduce varied lighting and backgrounds.

% \subsection{Synthetic and Simulated Datasets}

% \subsubsection{UCL and UCL dVRK} The UCL dataset \cite{rau2019implicit} is synthetic, derived from a human CT colonography scan. Using segmentation and meshing, it offers 16,000 images and corresponding depth maps rendered with the Unity game engine. The UCL dVRK dataset \cite{colleoni2020synthetic} includes 14 videos, each containing 300 frames, featuring diverse animal tissues under varied lighting and backgrounds, with fractional Brownian motion applied to simulate noise.

% \subsubsection{Laparoscopic Image-to-Image Translation Dataset} This dataset \cite{pfeiffer2019generating} includes 20,000 synthetic images derived from 3D laparoscopic simulations based on CT scans. Using style transformation, the dataset comprises 100,000 images that mimic various visual styles, enabling the testing of models under different scenarios.

% \subsection{General Surgical Tool Segmentation Datasets}

% \subsubsection{HeiSurF Dataset} The HeiSurF dataset \cite{bodenstedt2018comparative} features images from laparoscopic gallbladder resections with full-scene segmentation and workflow analysis annotations. It includes actions and surgical phase data, with images annotated every two minutes for 24 procedures.

% \subsubsection{LapSig300} Developed by Kitaguchi et al. \cite{kitaguchi2020automated}, LapSig300 is a comprehensive dataset of 300 colorectal surgery videos with 82,623,098 frames annotated for phase recognition and tool segmentation, covering five commonly used instruments.

% \subsubsection{LapGyn4} LapGyn4 consists of over 55,000 images from gynecological surgeries. It includes detailed segmentations for surgical actions, anatomical structures, and visible tools, facilitating tool detection and workflow analysis.

% \subsection{Other Specialty Datasets}

% \subsubsection{Sinus Surgery Dataset} The Sinus Surgery Dataset \cite{qin2020towards} includes 10 cadaveric and 3 live videos, captured at 19201080 resolution. It presents complex scenarios such as occlusions, tissue interactions, and specular reflections, with frames manually annotated for tool segmentation.

% \subsubsection{Cata7} Introduced by Ni et al. \cite{ni2019raunet}, Cata7 is the first dataset for cataract surgery, featuring seven videos from Beijing Tongren Hospital. Each video is split into high-resolution images with detailed annotations for tool edges and segmentation masks.

% \subsubsection{CaDTD Dataset} The CaDTD dataset \cite{jiang2021semi}, derived from the CATARACTS dataset \cite{al2019cataracts}, contains 50 videos of cataract surgeries. Half of the videos are annotated with bounding boxes for tool heads and handles, while the others are unlabeled, providing a semi-supervised learning resource.

% \subsubsection{RoboTool Dataset} The RoboTool dataset \cite{garcia2021image} comprises 514 frames from robotic surgical videos, manually annotated for tool segmentation. Additionally, a synthetic dataset with 14,720 images provides segmentation masks for surgical tools against various backgrounds.


% % \section{Available Datasets}

% % Methods based on machine learning are heavily reliant on extensive training data, with their performance typically scaling with the quantity of available examples. This highlights the crucial role of dataset availability in constructing effective deep learning models. A detailed overview of the datasets employed for surgical tool navigation is provided in Table\ref{tab:comparison}. It should be noted that although the MICCAI EndoVis challenges include various sub-challenges, this summary focuses only on those relevant to the current study. Many datasets in this area are freely accessible, serving two primary functions: they enable researchers who lack their own data to engage in model building and facilitate the comparison of different methodologies reported in the academic literature. While the majority of these datasets offer video data, one also includes kinematic data. With the ongoing advancements in robotic surgery, there is an expectation that future video datasets will also incorporate robotic kinematics to foster the development of more sophisticated architectures. The Medical Image Computing and Computer Assisted Intervention (MICCAI) society has significantly contributed to the progress of Minimally Invasive Surgery (MIS) and the adoption of data-driven approaches in interventional healthcare. Since 2015, MICCAI has annually organized challenges that have substantially advanced AI-based research and the implementation of computer vision in computer-assisted interventions. Detailed descriptions of each dataset are included below:


% % \textbf{M2CAI16 Challenge}:
% % This competition features two datasets designed for distinct tasks: surgical workflow and tool detection. The m2cai-tool1 dataset \cite{twinanda2016endonet} consists of 15 cholecystectomy videos---10 for training and 5 for testing---gathered in partnership with the University Hospital of Strasbourg. The primary objective was to detect the types of surgical instruments appearing in the videos. Each surgical video was marked with binary indications of the presence of seven different tools. The m2cai16-tool dataset was later expanded to include spatial annotations for 2,532 frames within the initial 10 videos out of the total collection of 15. This expanded dataset, referred to as m2cai-tool-locations \cite{jin2018tool}, enhances the utility of the dataset by facilitating the localization of surgical tools alongside their classification.


% % \textbf{NeuroSurgicalTools}: This dataset for surgical tool detection \cite{bouget2015detecting} includes 14 monocular videos recorded with ``Zeiss OPMI Pentero classic'' microscopes. These videos are captured at a resolution of 720x576 pixels and a frame rate of 25 frames per second (fps), during actual neurosurgical procedures at CHU Pontchaillou in Rennes. The dataset is rich in challenging scenarios involving surgical tools, such as occlusions by organs or the hands of surgeons, overlapping of tools, tools smeared with blood, blurring, and specular reflections. It features seven different surgical instruments, each annotated with a bounding polygon. Additionally, each part of the instrument is assigned a multi-class label, and data about the tool's orientation, width, and tip are included.

% % \textbf{FetalFlexTool}: Developed by Garca-Peraza-Herrera et al. \cite{garcia2017real}, this ex-vivo fetal surgery dataset comprises 21 images for model training and a 10-second video for testing purposes. It was created using non-rigid McKibben artificial muscle actuation. The training images were taken in the air, while the testing video was filmed underwater to introduce variations in lighting conditions and backgrounds. Manual annotations were made for tool segmentation masks.

% % \textbf{AutoLaparo dataset}
% % The AutoLaparo dataset \cite{wang2022autolaparo} comprises sub-datasets dedicated to tasks such as workflow recognition, laparoscope motion prediction, and segmentation of instruments and critical anatomical features. It includes 21 videos of laparoscopic hysterectomy procedures, each recorded in high definition with a resolution of 1920  1080. The dataset not only provides segmentation of the uterus based on anatomy but also identifies four specific types of surgical instruments within the footage: grasping forceps, ligasure, dissecting and grasping forceps, and electric hook.

% % \textbf{EndoVis Challenge}: Since 2015, the MICCAI society has annually hosted the Endoscopic Vision Challenges, each year themed differently. The EndoVis datasets encompass a range of surgical tools, including both rigid and robotic instruments, complete with annotations. The inaugural challenge concentrated on the segmentation and tracking of both rigid and articulated surgical instruments. Subsequent challenges expanded in scope; the 2017 challenge introduced videos of robotic tools for both binary and multi-class instrument segmentation. By 2018, the complexity escalated to encompass entire surgical scene segmentation, where participants were tasked with segmenting robotic, non-robotic, and anatomical elements within video frames. The 2019 Robust-MIS challenge, focusing on robustness and generalization, gathered a dataset from thirty varied surgical procedures and aimed at binary segmentation of instruments, as well as parts and instance segmentation. That same year, the SCARED (Stereo Correspondence and Reconstruction of Endoscopic Data) sub-challenge concentrated on depth estimation within surgical scenes. In 2020, the focus was on visual domain adaptation within surgical settings, and in 2021, the challenge tackled surgical workflow recognition.

% % \textbf{HeiSurF dataset}: The ``HeiChole Surgical Workflow Analysis and Full Scene Segmentation (HeiSurF)'' challenge \cite{bodenstedt2018comparative} was implemented, and the accompanying dataset was released. This dataset features laparoscopic gallbladder resections and is specifically tailored for full-scene segmentation and detailed surgical workflow analysis. It includes annotations for the actions taken and the specific phases of the surgery, along with segmentation ground truth. The training set is divided into two segments: the first features images annotated every two minutes across 24 procedures, and the second includes short video sequences annotated at a rate of 1 FPS. The test set comprises nine previously unreleased surgical videos. The video resolutions range from 720  576 to 1920  1080. The annotations cover various categories, including tissues and organs, and specific classes for surgical instruments, drains, clips, trocars, and specimen bags.


% % \textbf{ATLASDione}: Compiled by Sarikaya et al. \cite{sarikaya2017detection}, this dataset features 99 surgical videos that document six distinct tasks conducted by ten clinicians at the Roswell Park Cancer Institute in Buffalo, NY. These procedures were carried out using the da Vinci Surgical System. The dataset includes videos that illustrate both basic robotic surgical skills, categorized under Fundamental Skills of Robotic Surgery (FSRS), and specialized skills required for the Robotic Anastomosis Competency Evaluation (RACE). Annotations within the dataset cover a range of data, including tool bounding boxes, the types of surgical actions performed, their durations, and the skill levels of the surgeons involved.


% %  \textbf{LapSig300}: Authored by Kitaguchi et al. \cite{kitaguchi2020automated}, the LapSig300 dataset is a comprehensive compilation of 300 laparoscopic colorectal surgery videos, collected in cooperation with 19 high-volume Japanese medical institutions. It encompasses a total of 82,623,098 frames annotated for the recognition of surgical phases and actions, and 4,243 frames specifically annotated for the semantic segmentation of surgical tools. The dataset focuses on five frequently used tools for segmentation: grasper, dissector, linear dissector, Maryland, and clipper. Access to this dataset is provided upon request from the authors.

% % \textbf{Cholec80}: Developed by Twinanda et al. \cite{twinanda2016endonet}, Cholec80 is a dataset featuring 80 cholecystectomy procedure videos performed by 13 clinicians. It includes annotations for tool presence and surgical phase recognition, specifying that a tool is considered present if at least half of its tip is visible in a video frame. The video capture rate is 25 fps, but for processing purposes, it's downsampled to 1 fps. Cholec80 is divided into two parts, each containing 40 videos. The first part, referred to as the fine-tuning subset, includes 86,000 annotated images, with 10 videos additionally annotated with tool bounding boxes. The second part, known as the evaluation subset, is utilized for assessing models' capabilities in detecting tool presence and recognizing surgical phases.

% % Further expansions of Cholec80 include the addition of 40 more annotated cholecystectomy videos by Aksamentov et al. \cite{aksamentov2017deep}. Additionally, the ITEC Smoke Cholec80 Image extension comprises 100,000 frames from the original Cholec80 set, specifically curated for research on smoke removal, with annotations distinguishing between smoke and non-smoke scenarios.


% % \textbf{LapGyn4}: Consisting of more than 55,000 images, LapGyn4 is a segmented Gynecological surgery dataset drawn from 500 surgical interventions. It includes images of surgical scenes that detail surgical actions, anatomical structures, visible surgical tools, and specific actions conducted on particular anatomical parts.


% % \textbf{dVPN Dataset}: Compiled by Ye et al. in 2017, the in-vivo dVPN dataset originates from da Vinci partial nephrectomy procedures. It includes 34,320 pairs of rectified stereo images for training purposes and 14,382 pairs for testing. This dataset does not come with any ground truth labels.


% % \textbf{UCL}:
% %  UCL is a synthetic
% %  dataset generated from a human CT colonography
% %  (CTC) scan\cite{rau2019implicit}. Manual segmentation and meshing are used to extract surface mesh. To render
% %  endoscopic images with their depth information, the Unity game engine application is used. A virtual camera with two light sources runs through the virtual model, producing different images containing various illumination scenarios. The
% %  virtual materials contain various textures to make the dataset more versatile. Overall, the dataset
% %  consists of 16,000 images along with as many depth maps.


% % \textbf{Cata7}: Introduced by Ni et al. \cite{ni2019raunet}, Cata7 is the inaugural dataset for cataract surgery, featuring recordings from Beijing Tongren Hospital. It comprises 7 videos, each documenting an entire cataract surgery. These videos have been split into images with a resolution of 19201080 pixels and are downsampled from 25 fps to 1 fps to minimize redundancy. The dataset includes detailed annotations identifying types of surgical instruments and outlining their precise edges.


% % \textbf{Sinus Surgery Dataset}: Sinus Surgery Dataset encompasses both cadaver-based and live surgical data. The Cadaver section includes 10 videos of sinus surgeries conducted on 5 cadavers by 9 different surgeons, with operations performed on both the right and left nasal cavities of each subject\cite{qin2020towards}. These videos vary in length from 5 to 23 minutes and are captured at a resolution of 320240 pixels at 30 fps. The Live dataset section contains 3 videos, ranging from 12 to 66 minutes in duration, with a higher resolution of 19201080 pixels. This dataset presents a variety of complex scenarios, including blurry frames, smoke, shadowed instruments, tissue occlusions, and specular reflections. All frames in the dataset have been center-cropped to 240240 pixels and manually annotated to highlight the surgical tools in the foreground.

% % \textbf{SCARED Dataset}:
% %  SCARED dataset\cite{allan2021stereo} was introduced during the EndoVis sub-challenge, Stereo Correspondence and Reconstruction of Endoscopic Data (SCARED), at the MICCAI 2019 conference. This dataset includes 7 training and 2 testing sets, each recorded using the Da Vinci Xi surgical robot. The datasets feature structured light data from individual porcine subjects. Depth labels are provided for all keyframes within the dataset.


% % \textbf{CaDTD Dataset}: CaDTD is a dataset\cite{jiang2021semi} derived from the CATARACTS dataset \cite{al2019cataracts}. It comprises 50 videos of cataract surgeries conducted by expert clinicians at Brest University Hospital. Originally captured at 30 fps with a resolution of 1920 $\times$ 1080, the videos have been modified for more efficient use. Half of the videos come without labels, while the other half includes annotations for tool bounding boxes. To enhance usability, the videos were downsampled to 720 $\times$ 540 resolution, and frames were spaced three seconds apart. The dataset categorizes surgical tools in two ways: one category identifies entire tools, including both the head and handle, while the other category focuses solely on the tool heads.

% % \textbf{UCL dVRK Dataset}:
% %  UCL dVRK dataset \cite{colleoni2020synthetic} features 14 videos, each containing 300 frames with a resolution of 720x576. These were recorded using the da Vinci Research Kit. The data capture included a variety of animal tissues---chicken breast and back, lamb and pork loin, beef sirloin---set against diverse backgrounds and lighting conditions to enrich the dataset. To increase the complexity of the test set, elements such as lamb kidneys and blood were included in the background scenes, and fractional Brownian motion was introduced to the test frames to simulate noise. The dataset includes surgical tool annotations in the form of segmentation masks and also provides data on robot kinematics.

% % \textbf{KvasirInstrument Dataset}:
% % Released in 2021, ``Kvasir-Instrument: Diagnostic and Therapeutic Tool Segmentation Dataset\cite{jha2021kvasir} in Gastrointestinal Endoscopy'' encompasses annotations for 590 endoscopic frames from gastroscopies and colonoscopies. The dataset features images that include both diagnostic and therapeutic tools, with resolutions ranging from 720 $\times$ 576 to 1280 $\times$ 1024. The authors have provided binary segmentation masks and bounding boxes for these tools. Representative instruments in the dataset include snares, balloons, and biopsy forceps.


% % \textbf{RoboTool Dataset}: The creation of the RoboTool dataset \cite{garcia2021image}involved the manual annotation of 514 video frames from 20 publicly available surgical procedures featuring exclusively robotic instruments. The resolution of these video frames corresponds to that of the original video recordings. The dataset includes only binary labels to distinguish whether a pixel belongs to an instrument or to the background. Additionally, the dataset facilitates the creation of a synthetic dataset through the provision of 14,720 foreground images. These images display between one and three surgical robotic instruments (from a total of 17 different types) against a green screen background. Of these images, 13,613 have a resolution of 4032 $\times$ 3024, while the remainder have a resolution of 3360 $\times$ 2240. Each foreground image is accompanied by a binary segmentation mask of the instruments. The dataset also includes 6,130 background images of varying resolutions that depict human tissue from 50 publicly available surgical frames where no instruments are visible. To assist with dataset utilization, the authors provide source code for merging the foreground images with the background images.

% % \textbf{Laparoscopic Image-to-Image (I2I) Translation Dataset}: This dataset\cite{pfeiffer2019generating} comprises 20,000 synthetic images that have been automatically annotated, derived from 3D laparoscopic simulations based on CT scans from ten patients. These images represent laparoscopic scenes as viewed from various randomly positioned camera angles. The authors developed a translation network to convert these rendered images into different styles. Each image was transformed using five unique style vectors, generating a collection of 100,000 images. Additionally, all images were processed to mimic five styles from the Cholec80 dataset, which includes images from actual in-vivo laparoscopic procedures. This process produced another set of 100,000 images that emulate the visual characteristics of the Cholec80 dataset. The resolution of all images is 452  $\times$  256. The dataset primarily features two traditional surgical instruments, the grasper and hook, with annotations distinguishing the shafts and tips as individual instrument subclasses.

% % \textbf{ART-Net}: The ``Augmented Reality Tool Network (ART-Net)'' dataset \cite{hasan2021detection} features 29 laparoscopic hysterectomy procedures conducted using non-robotic instruments. Along with binary segmentation of surgical instruments, annotations also include the presence of tools and specific geometric characteristics of each instrument. Each video in the dataset is recorded at a resolution of 1920  $\times$  1080.

% \begin{figure}
% \centering
% \begin{tikzpicture}
% \label{fig:baro}
% \begin{axis}[
%     ybar,
%     enlarge x limits=0.15,
%     ymin=0, ymax=70, % Adjust the maximum y value to provide some space above the highest bar
%     symbolic x coords={EndoVis-2017, Private Datasets, Others, EndoVis-2015, EndoVis-2018, Kvasir Instrument, EndoVis-2019, Sinus-Surgery-C/L, Lap. I2I Translation, UCL dVRK, RoboTool},
%     xtick=data,
%     x tick label style={rotate=45, anchor=east, align=right},
%     ylabel={Frequency of Use},
%     nodes near coords,
%     nodes near coords align={vertical},
%     bar width=15pt,
%     axis line style={draw=none}, % Removes axis lines
%     tick style={draw=none} % Removes ticks
%  ]
% \addplot coordinates {
%     (EndoVis-2017, 62)
%     (Private Datasets, 40)
%     (Others, 22)
%     (EndoVis-2015, 14)
%     (EndoVis-2018, 14)
%     (Kvasir Instrument, 14)
%     (EndoVis-2019, 7)
%     (Sinus-Surgery-C/L, 5)
%     (Lap. I2I Translation, 3)
%     (UCL dVRK, 3)
%     (RoboTool, 3)
% };
% \end{axis}
% \end{tikzpicture}
% \caption{Visualization of commonly used datasets along with their frequency of use in recent publications, displayed as a vertical bar chart. (Should We Add such figures)}
% \end{figure}
% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=\columnwidth]{graphdata.png} % Set the width to \columnwidth for one-column width
%   \caption{Visualization of commonly used datasets along with their frequency of use in recent publications, displayed as a vertical bar chart.}
%   \label{fig:dataa} 
% \end{figure}
% % In Figure~\ref{fig:baro} provides a comprehensive overview of dataset utilization across 123 examined studies, involving a total of 187 distinct datasets. These datasets fall into three main categories: nine primary datasets that meet specific relevance criteria, a group of non-public (private) datasets, and those that don't satisfy the established relevance criteria.

% % To be classified as ``relevant,'' a dataset must include surgical instruments, provide segmentation ground truths for these instruments, and contain endoscopic images or videos. Those datasets that aren't publicly accessible are collectively referred to as ``Private Datasets.'' The ``Others'' category encompasses all publicly available datasets that fail to meet these relevance standards. Notably, the EndoVis-2017 dataset \cite{twinanda2017endonet} emerges as the most extensively employed, featured in 62 publications. It is followed by private datasets, which appear in 40 publications. The next most utilized datasets include EndoVis-2015 \cite{bouget2015detecting}, EndoVis-2018 \cite{allan20202018}, and Kvasir Instrument \cite{jha2021kvasir}, each with 14 uses. EndoVis-2019 \cite{ross2021comparative}follows with seven mentions. Additionally, the Sinus Surgery-C/L dataset \cite{qin2020towards} was cited seven times. Other datasets, including the Laparoscopic Image-to-Image Translation \cite{pfeiffer2019generating}, UCL dVRK \cite{colleoni2020synthetic}, and RoboTool \cite{garcia2021image}, were each referenced three times in various publications.





% \section{Open Issues and Future Research Directions}
% \section{Open Issues and Future Research Directions in Surgical Scene Understanding}
% Surgical scene understanding has made significant strides, yet numerous challenges and open issues persist that require ongoing research and innovative solutions as shown in  Fig \ref{fig:openissues}. This section outlines these challenges in detail, categorizing them into specific areas 
% that are crucial for the advancement of the field.

% \subsection{Technical Challenges}

% \subsubsection {Data and Annotation Constraints}
% One of the persistent challenges in surgical video analytics is the scarcity and variability of annotated surgical data. To address this, future research should focus on developing robust semi-supervised and unsupervised learning algorithms that can effectively leverage unlabeled data \cite{baradaran2024critical}. This reduces the dependency on extensive annotated datasets, which are costly and time-consuming to produce. Additionally, enhancing collaborations across medical institutions can facilitate the sharing of surgical videos and annotations under strict privacy regulations, thus broadening the datasets available for the training and validation of AI models.

% \subsubsection {Integration with Clinical Workflows}
% While AI models offer promising improvements in surgical scene understanding, their integration into clinical workflows remains limited. Future research should explore the development of user-friendly interfaces and real-time analysis tools that seamlessly integrate with existing surgical equipment and protocols \cite{byrd2024artificial}. This integration should also consider the ergonomic and cognitive load on surgeons to ensure that AI tools augment rather than hinder surgical performance.

% \subsubsection {Robustness and Generalization}
% The variability in surgical environments and practices across different hospitals and geographic locations raises significant concerns about the robustness and generalization of AI models \cite{matta2024systematic}. Future studies should focus on developing adaptive models that can adjust to new environments and conditions without requiring extensive retraining. Techniques such as domain adaptation and federated learning could play crucial roles in addressing these issues.

% \subsection {Explainability and Trust}
% The ``black box'' nature of many deep learning models used in surgery poses significant challenges in terms of explainability and trust. Future models should aim to incorporate explainable AI principles that make it possible to understand and trust AI decisions and predictions \cite{samek2019towards}. This is critical not only for gaining the trust of medical professionals but also for meeting regulatory requirements.

% \subsubsection {Advancing Robotic Surgery}
% AI-driven insights from surgical videos hold the potential to revolutionize robotic surgery by providing more intuitive and responsive controls \cite{bekbolatova2024transformative}. Future research should explore deeper integration of AI analytics with robotic control systems, enabling more sophisticated instrument handling and decision-making support.

% \subsection{Ethical and Regulatory Considerations}

% \subsubsection{Ethical Considerations}
% As AI systems become more integral to surgical procedures, addressing ethical implications and ensuring compliance with evolving regulatory standards will be imperative \cite{o2019legal}. Future research directions should include the development of guidelines and standards for ethical AI use in surgery, emphasizing patient safety, data privacy, and bias mitigation.

% \subsubsection{Regulatory Considerations}

% \subsection{Multi-modal Data Fusion and Integration}

% \subsection {Multi-modal Data Fusion}
% Combining information from surgical videos with other data types, such as patient medical records, real-time sensor data, and histopathological images, can provide a more comprehensive understanding of surgical scenes \cite{xu2024comprehensive}. Future innovations could focus on multi-modal AI systems that leverage this holistic data integration to enhance surgical precision and outcomes.

% \subsection {Personalized Feedback}
% AI models can significantly enrich surgical training and simulation by providing objective assessments and personalized feedback \cite{guerrero2023advancing}. Further advancements in this area might include the development of virtual reality (VR) or augmented reality (AR) platforms that utilize AI to simulate a variety of surgical scenarios and complications, enhancing training efficacy.



% By addressing these open issues and harnessing upcoming technological advancements, the field of surgical scene understanding can significantly enhance both the efficacy and safety of surgeries, ultimately improving patient outcomes.

% % As surgical scene understanding evolves, it is confronted by a myriad of unresolved issues as 
% %  shown in Fig.~\ref{fig:chall} and emerging opportunities that can significantly influence future research directions:


% % \begin{itemize}
% %     \item Data and Annotation Constraints:
% %    One of the persistent challenges in surgical video analytics is the scarcity and variability of annotated surgical data. Future efforts should focus on developing robust semi-supervised and unsupervised learning algorithms that can effectively leverage unlabeled data, thereby reducing the dependency on extensive annotated datasets. Additionally, collaborations across medical institutions can be enhanced to facilitate the sharing of surgical videos and annotations under strict privacy regulations, broadening the datasets available for training and validating AI models.
   
% %  \item Integration with Clinical Workflows: While AI models offer promising improvements in surgical scene understanding, their integration into clinical workflows remains limited. Future research should explore the development of user-friendly interfaces and real-time analysis tools that seamlessly integrate with existing surgical equipment and protocols. This integration should also consider the ergonomic and cognitive load on surgeons to ensure that AI tools augment rather than hinder surgical performance.
% %   \item  Explainability and Trust: The black box nature of many DL models used in surgery poses significant challenges in terms of explainability and trust. Future models should aim to incorporate explainable AI  principles that make it possible to understand and trust AI decisions and predictions. This will be critical not only for gaining the trust of medical professionals but also for meeting regulatory requirements.

% % \begin{figure}[hbtp]
% %   \centering
% %   \includegraphics[width=\columnwidth]{issues.png} % Set the width to \columnwidth for one-column width
% %   \caption{Challenges in Surgical scene Understanding}
% %   \label{fig:chall} 
% % \end{figure}

% %   \item Robustness and Generalization: The variability in surgical environments and practices across different hospitals and geographic locations raises significant concerns about the robustness and generalization of AI models. Future studies should focus on developing adaptive models that can adjust to new environments and conditions without requiring extensive retraining. Techniques such as domain adaptation and federated learning could play crucial roles in addressing these issues.
  
% %   \item Ethical and Regulatory Considerations: As AI systems become more integral to surgical procedures, addressing ethical implications and ensuring compliance with evolving regulatory standards will be imperative. Future research directions should include the development of guidelines and standards for ethical AI use in surgery, emphasizing patient safety, data privacy, and bias mitigation.

% %   \item Advancing Robotic Surgery: AI-driven insights from surgical videos hold the potential to revolutionize robotic surgery by providing more intuitive and responsive controls. Future research should explore deeper integration of AI analytics with robotic control systems, enabling more sophisticated instrument handling and decision-making support.

% %    \item Multi-modal Data Fusion: Combining information from surgical videos with other data types, such as patient medical records, real-time sensor data, and histopathological images, can provide a more comprehensive understanding of surgical scenes. Future innovations could focus on multi-modal AI systems that leverage this holistic data integration to enhance surgical precision and outcomes.

% %   \item Surgical Training and Simulation: AI models can significantly enrich surgical training and simulation by providing objective assessments and personalized feedback. Further advancements in this area might include the development of virtual reality (VR) or augmented reality (AR) platforms that utilize AI to simulate a variety of surgical scenarios and complications, enhancing training efficacy.
% % \end{itemize}
% % By addressing these open issues and harnessing upcoming technological advancements, the field of surgical scene understanding can significantly enhance both the efficacy and safety of surgeries, ultimately improving patient outcomes
% % 

% % Despite significant advancements, there remain several open research issues in the application of AI in surgery, including the need for real-time trust metrics, uncertainty quantification, human-in-the-loop systems, and trustworthiness in model design. These issues address the critical aspects of safety, reliability, and ethics in surgical AI, pointing towards a future where these technologies are seamlessly integrated into clinical practice, enhancing both the efficiency and outcomes of surgeries. This section outlines the current gaps in the research, critiques existing methodologies, and proposes promising directions for future exploration.


% % \subsection{Gap Analysis}
% % \subsubsection{Data Quality and Availability}
% % One of the primary challenges in advancing surgical scene understanding is the variability and quality of the data. Surgical videos vary greatly due to differences in surgical procedures, equipment used, and patient-specific factors. Additionally, there is a scarcity of large, well-annotated surgical video datasets that are publicly available due to privacy concerns and the extensive effort required in annotating these videos. Developing strategies to enhance data collection and annotation, possibly through semi-supervised learning or synthetic data generation, could provide a pathway for future research.

% % \subsubsection{Real-time Processing} 
% % Current deep learning models, while effective, often require substantial computational resources, which can limit their use in real-time applications. The latency in processing can be detrimental in a surgical context where decisions need to be made swiftly. Future research should focus on optimizing algorithms for faster processing without sacrificing accuracy, potentially through model pruning, efficient network architectures like MobileNets, or hardware-specific optimizations for surgical systems.

% % \subsubsection{Generalization Across Different Surgical Procedures}
% % Most existing models are trained and tested on specific types of surgeries, which limits their applicability across the diverse spectrum of surgical procedures. The ability of models to generalize across different surgical contexts without extensive retraining remains a significant challenge. Research into transfer learning and domain adaptation techniques could help in developing more versatile models.

% % \subsection{Pitfalls and Problems with Current Approaches}
% % \subsubsection{Over-reliance on Supervised Learning}
% % The majority of current systems depend heavily on supervised learning, which requires large amounts of labeled data. This dependency is a major bottleneck due to the labor-intensive nature of labeling surgical videos. Exploring unsupervised and self-supervised learning paradigms could mitigate this issue by reducing the reliance on extensive annotated datasets.

% % \subsubsection{Integration with Surgical Workflows}
% % Another critical issue is the integration of AI systems into existing surgical workflows without disrupting the surgical team or requiring significant changes to surgical practice. Future systems must be designed to seamlessly integrate with the surgical environment, enhancing their adoption and utility.

% % \subsection{Promising Directions for Future Work}
% % \subsubsection{Enhanced Augmented Reality (AR) Systems}
% % There is considerable potential in enhancing AR systems with advanced ML techniques to provide real-time, context-sensitive information overlaid directly on the surgeon's field of view. This could include real-time segmentation, anomaly detection, and even predictive analytics to foresee potential complications.

% % \subsubsection{Multi-modal Data Integration}
% % Combining different types of data (e.g., video, audio, and physiological signals) could significantly enhance the robustness and accuracy of surgical scene understanding systems. Future research should explore the integration of these diverse data streams to provide a more holistic view of the surgical environment.

% % \subsubsection{Adaptive and Personalized AI Systems}
% % Developing adaptive AI systems that can learn and adjust to individual surgeon's styles and preferences could personalize the surgical assistance, leading to better outcomes. Such systems would use machine learning to adapt in real-time, providing personalized insights and support during surgeries.





\section{Open Issues and Future Research Directions in Surgical Scene Understanding}

Surgical scene understanding has made significant strides, yet numerous challenges and open issues persist that require ongoing research and innovative solutions, as shown in Fig.~\ref{fig:openissues}. Addressing these issues is critical for advancing the field and realizing the full potential of AI-driven surgical tools. This section categorizes these challenges into specific areas that are crucial for the development and deployment of surgical AI systems.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\columnwidth]{openissues_research.png}
  \caption{An illustration of potential challenges and future research directions surgical AI development.}
  \label{fig:openissues}
\end{figure}


\subsection{Technical Challenges}

\subsubsection{Data and Annotation Constraints}
One of the most persistent challenges in surgical video analytics is the scarcity and variability of annotated surgical data. Developing robust semi-supervised \cite{han2024deep}, weakly supervised \cite{khan2024fetr}, and unsupervised learning \cite{stan2024unsupervised} algorithms that can effectively leverage unlabeled data remains an essential area of future research \cite{baradaran2024critical}. Such methods reduce the dependency on large annotated datasets \cite{khan2024ultraweak}, which are costly and time-consuming to produce. Furthermore, fostering collaborations across medical institutions can enable secure sharing of surgical videos and annotations under stringent privacy regulations. These efforts will expand the availability of diverse datasets for training and validating AI models, improving their robustness and generalization.

\subsubsection{Integration with Clinical Workflows}
Although AI models hold great promise for surgical scene understanding, their integration into clinical workflows remains limited due to technical and ergonomic barriers. Future research should prioritize the development of user-friendly interfaces and real-time analysis tools that seamlessly integrate with existing surgical equipment and protocols \cite{byrd2024artificial}. Consideration must also be given to minimizing the ergonomic and cognitive load on surgeons to ensure that AI systems serve as effective tools that enhance, rather than disrupt, surgical performance.

\subsubsection{Robustness and Generalization}
AI models used in surgical scene understanding often struggle with robustness and generalization due to the variability in surgical environments, practices, and equipment across hospitals and geographic regions \cite{matta2024systematic}. Adaptive models that can generalize to new settings without extensive retraining are critical for broader adoption. Promising techniques such as domain adaptation and federated learning offer potential solutions to address these challenges and enhance the reliability of AI models in diverse surgical contexts.

\subsubsection{Explainability and Trust}
The ``black-box'' nature of many deep learning models used in surgery poses significant challenges in terms of explainability and trust \cite{khan2024guardian}. Future research should focus on incorporating explainable AI principles that provide transparent and interpretable insights into AI decision-making processes \cite{samek2019towards}. The enhancement of the explainability of the model will be instrumental in gaining the trust of medical professionals and meeting regulatory requirements, ensuring that AI systems are reliable and safe for use in clinical settings.

\subsubsection{Advancing Robotic Surgery}
AI-driven insights derived from surgical videos have the potential to revolutionize robotic surgery by enabling more intuitive and responsive control systems \cite{bekbolatova2024transformative}. Future research should focus on a deeper integration of AI analytics with robotic systems to support more sophisticated instrument handling, decision making, and task execution. This will improve the precision and safety of robotic-assisted surgeries while enabling novel applications in complex procedures.

\subsection{Ethical and Regulatory Considerations}

\subsubsection{Balancing Automation and Human Skill Development}
The increasing integration of AI and robotics in surgery, while enhancing precision and efficiency, raises critical concerns about the atrophy of human skills. As Beane notes in the article ``Today's Robotic Surgery Turns Surgical Trainees into Spectators'' \cite{beane2022today}, surgical trainees are often relegated to passive observers in robotic surgeries, significantly limiting their opportunities to develop essential cognitive and motor skills. This overreliancen automation risks creating a generation of surgeons proficient in operating machines but ill-prepared for handling emergencies or unanticipated complications. To mitigate this, AI systems should prioritize human augmentation by enabling active trainee participation and providing real-time feedback, fostering skill development alongside technological progress \cite{euchner2024designing} \cite{beane2024skill}.

\subsubsection{Ethical Considerations}
As AI becomes increasingly integral to surgical workflows, addressing ethical challenges such as patient safety, data privacy, and bias mitigation will be crucial \cite{o2019legal}. Future research should emphasize the development of robust ethical guidelines and frameworks to ensure that AI systems align with principles of fairness, accountability, and transparency. These guidelines will play a critical role in fostering trust and adoption among clinicians and patients.

From an ethical point of view, the shift towards automation introduces questions of accountability, transparency, and sustainability. As highlighted in The Skill Code \cite{beane2024skill}, surgical AI must be designed to preserve human expertise rather than replace it, ensuring that technology complements the role of the surgeon. Over-standardization and dependency on AI could undermine creativity and adaptability in surgery, diminishing the human element essential for patient-centered care. Addressing these challenges requires a human-centered approach to AI that balances innovation with ethical responsibility, safeguarding both the art and science of surgery.

\subsubsection{Regulatory Considerations}
The rapid advancement of surgical AI requires robust and evolving regulatory frameworks to ensure safety, transparency, and compliance. Existing guidelines, such as AI as a medical device legislation from regulatory bodies such as the FDA and EU MDR, emphasize risk management, performance validation, and post-market surveillance \cite{solaiman2024research}. To align with these standards, surgical AI systems must ensure data security, model interpretability, and clinical efficacy while incorporating mechanisms for continuous learning and adaptation.

Collaboration between AI researchers, medical professionals, and regulatory bodies is essential to establish comprehensive standards that balance innovation with rigorous oversight. These regulations should prioritize patient safety, fairness, and accountability to mitigate biases and unintended consequences. By fostering trust and ethical responsibility, such frameworks will support the sustainable integration of AI into surgical workflows, ensuring both efficacy and safety in clinical applications.
\subsection{Multi-modal Data Fusion and Integration}

\subsubsection{Multi-modal Data Fusion}
Combining information from surgical videos with other types of data, such as patient medical records, real-time sensor data, and histopathological images, can provide a more comprehensive understanding of surgical scenes \cite{xu2024comprehensive}. Multimodal AI systems capable of leveraging this holistic integration will not only improve surgical precision and outcomes, but also enable more personalized and context-aware decision making.

\subsubsection{Personalized Feedback}
AI systems have significant potential to enrich surgical training and simulation through personalized feedback mechanisms. Future advancements could focus on the development of virtual reality (VR) and augmented reality (AR) platforms powered by AI to simulate diverse surgical scenarios and complications. These platforms can provide objective assessments and customized feedback to enhance surgeon training skills \cite{guerrero2023advancing}.

% \section{Conclusions}
% \label{sec:concs}
\section{Conclusion}
\label{sec:concs}
 The deployment of machine learning and deep learning technologies in the realm of surgical scene understanding marks a significant paradigm shift in minimally invasive surgery. By leveraging sophisticated models like CNNs, ViTs, and FMs, these technologies will enhance the precision of surgical interventions, improve real-time decision-making, and ensure patient safety. However, foundational models have evolved the way these models operate as they achieve more diverse tasks and employ different applications like accurate segmentation and recognition of surgical instruments and tissues, which in turn reduces surgical risks and improves outcomes. Challenges remain in the field, such as handling diverse and complex surgical scenarios, ensuring model transparency, and integrating these technologies into clinical workflows. Future research should focus on developing more adaptable, efficient, and ethically aligned models that can be seamlessly integrated into clinical settings. Advances in the field will continue to push the boundaries of what is possible in surgery, potentially leading to groundbreaking surgical techniques and transforming surgical training and patient care. Moreover, the excitement about AI's impact on healthcare is growing at the highest levels, with global healthcare leaders increasingly recognizing its transformative potential.
 %Excitement about the potential impact of AI abounds at the highest level of leadership and is set out in the recently published plans around NHS reform.
 



\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}
