\section{Related Work}

\paragraph{Retrieval-augmented Generation}
To address the issues of hallucinations~\cite{xu24hallucinationsurvey,liu24hallucinationsurvey} due to a lack of corresponding knowledge or containing outdated knowledge, retrieval-augmented generation (RAG)~\cite{gao23ragsurvey,fan24ragsurvey} has been proposed for retrieving relevant chunks from a pool of candidate documents to assist LLM generation.

In a typical RAG system~\cite{patrick20rag}, the documents are first segmented into chunks based on lengths and structures, and then encoded with an embedding model~\cite{zach24nomic,li24mxbai} and indexed for efficient retrieval.
Inspired by the idea of sliding windows~\cite{jiao06slidingwindows}, sentence window retrieval~\cite{jiang23arag,matous24aragog} fetches the neighboring chunks around the retrieved chunks and concatenates them into a single larger chunk for context enrichment.
However, sentence window retrieval only considers the physical proximity of text chunks within the same document.
Different from existing studies, \modelname performs retrieval expansion based on factual associations among chunks that may be across multiple documents.

Reranking~\cite{nicholas24improving,michael22re2g} is a critical technique in information retrieval~\cite{mandalay62irsurvey,kuo24irsurvey}. In RAG systems, feeding the retrieved chunks along with the queries into a deep learning-based cross-encoder~\cite{xiao23cpack} can measure the semantic relevance more precisely, thereby enhancing both the retrieval and generation quality.
\modelname organizes the retrieved chunks into paragraphs with KGs as the skeleton, allowing a fine-grained measurement of paragraph-level relevance to queries.


\paragraph{LLMs with Knowledge Graph}
LLM~\cite{li24llmsurvey,ren24llmsurvey} is one of the most representative achievements of contemporary artificial intelligence (AI).
KGs~\cite{ji22kgsurvey}, as graph-structured relational databases, serve as a crucial data infrastructure for AI applications.
Research indicates that LLMs have the potential to address tasks related to KGs, such as knowledge graph completion~\cite{liu24finetuning} and knowledge graph question answering~\cite{sen23knowledge}.

Recently, the research community begins to explore how KGs can be used to enhance the generation capability of LLMs~\cite{wang24kgp,darren24graphrag,xu24retrieval}.
For example,
KGP~\cite{wang24kgp} constructs a document KG consisting of page and passage nodes, and links passage nodes with TF-IDF. The document KG is employed for retrieval expansion. The document KG constructed by KGP is based on sentence-level text similarity, which essentially functions similarly to simply expanding the context window.
GraphRAG~\cite{darren24graphrag} targets at query-focused summarization tasks. GraphRAG extracts KGs automatically from the document base with an LLM and analyzes the semantic structure of the dataset before querying, by splitting the KG from different level and detecting linked nodes hierarchically.
Different from previous studies, \modelname aims to enhance RAG with the fact-level structure and factual knowledge of KGs.