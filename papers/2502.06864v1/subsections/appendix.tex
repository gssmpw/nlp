\clearpage
\appendix

%==============================
\section{Additional Experimental Results}

%--------------------
\subsection{Results on More Datasets}
\label{app:more_datasets}
We conduct additional experiments on two different datasets to confirm the effectiveness and generality of \modelname in various scenarios.

As shown in Table~\ref{tab:results_musique}, \modelname maintains superiority on the widely-used MuSiQue dataset~\cite{harsh22musique} in response F1 score, response exact match (EM) rate, and retrieval F1 score.

\begin{table}[!ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
Methods & Response F1 & Response EM & Retrieval F1 \\
\midrule
LLM-only & 0.075 & 0.025 & - \\
Semantic RAG & 0.367 & 0.248 & 0.365 \\
\quad +\,Rerank & 0.380 & 0.249 & 0.372 \\
Hybrid RAG & 0.380 & 0.250 & 0.364 \\
LightRAG & 0.248 & 0.170 & 0.289 \\
GraphRAG & 0.231 & 0.156 & 0.273 \\
\modelname & \textbf{0.419} & \textbf{0.303} & \textbf{0.451} \\
\bottomrule
\end{tabular}
}
\caption{Comparison results on MuSiQue.}
\label{tab:results_musique}
\end{table}

Also, we conduct experiments on the typical long-context dataset TriviaQA~\cite{joshi17triviaqa}. On average, each document in this dataset contains 2,895 words. For comparison, documents in HotpotQA have an average of 917 words. The experimental results are shown in Table~\ref{tab:results_trivia}, which confirms the effectiveness of \modelname in a typical long-context setting.

\begin{table}[!ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
Methods & Response F1 & Response Prec. & Response Recall \\
\midrule
LLM-only & 0.182 & 0.303 & 0.144 \\
Semantic RAG & 0.259 & 0.413 & 0.211 \\
\quad +\,Rerank & 0.265 & 0.409 & 0.235 \\
Hybrid RAG & 0.262 & 0.415 & 0.229 \\
LightRAG & 0.118 & 0.157 & 0.237 \\
GraphRAG & 0.127 & 0.193 & 0.225 \\
\modelname & \textbf{0.273} & \textbf{0.416} & \textbf{0.240} \\
\bottomrule
\end{tabular}
}
\caption{Comparison results on TriviaQA.}
\label{tab:results_trivia}
\end{table}

%--------------------
\subsection{Efficiency Analysis}
\label{app:efficiency_analysis}
We compare the KG construction cost of \modelname with two other KG-enhanced RAG approaches: LightRAG~\cite{guo2024lightrag} and GraphRAG~\cite{darren24graphrag}. The results, as summarized in Table~\ref{tab:kg_efficiency}, demonstrate that \modelname is more efficient in terms of token cost, the number of LLM calls, and time cost.

\begin{table}[!ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccc}
\toprule
& $\#$Input tokens & $\#$Output tokens & $\#$LLM calls & Extraction time \\
\midrule
LightRAG & 1,269 & 381 & 1 & 3s \\
GraphRAG & 2,791 & 629 & 5 & 6s \\
\modelname & \ \ \ 561 & \ \ 22 & 1 & 1s \\
\bottomrule
\end{tabular}
}
\caption{Comparison of average LLM and time cost per chunk during KG construction.}
\label{tab:kg_efficiency}
\end{table}

We calculate the average retrieval time and generation time of \modelname compared to LightRAG and GraphRAG. The results in Table~\ref{tab:rag_efficiency} indicate that \modelname requires less time for both retrieval and response generation than LightRAG and GraphRAG, and is very close to Semantic RAG. Note that \modelname might need a lower time for response generation using a condensed and informative context as input.

\begin{table}[!ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc}
\toprule
Method & Avg. retrieval time & Avg. generation time \\
\midrule
Semantic RAG & 21ms & 2,500ms \\
LightRAG & 40ms & 5,600ms \\
GraphRAG & 42ms & 5,500ms \\
\modelname & 25ms & 2,300ms \\
\bottomrule
\end{tabular}
}
\caption{Comparison of average retrieval and generation time per query.}
\label{tab:rag_efficiency}
\end{table}
