\section{Methodology}
\label{sec:method}

An overview of the workflow of \modelname is illustrated in Fig.~\ref{fig:pipeline}. 
In the following subsections, we provide more details following the workflow of \modelname, including document offline processing (Sec.~\ref{subsec:offline}), KG-enhanced chunk retrieval (Sec.~\ref{subsec:retrieval}), and KG-based context organization (Sec.~\ref{subsec:context}).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/extract_prompt_new.pdf}
\caption{The prompt for triplet extraction.}
\label{fig:extract_prompt}
\end{figure}

\subsection{Document Offline Processing}
\label{subsec:offline}
Following the existing studies in RAG~\cite{patrick20rag,gao23ragsurvey,fan24ragsurvey}, all documents are first split into $n$ chunks based on the structure of sentences and paragraphs given a predefined chunk size, which can be given as $\mathcal{D} = \{c_1,\ldots,c_n\}$.
These chunks can be further processed, for example, by adding relevant context~\cite{jiang23arag,matous24aragog}, extracting meta-information~\cite{laurent24meta} (e.g., title, abstract), and generating corresponding questions~\cite{ma23queryrewriting,wang24maferw}. 
Since these chunk-enhancing techniques are orthogonal to the proposed method in this paper, we recommend referring to the original paper for more details.
Hereafter, we continue to denote the processed chunks as $\mathcal{D} = \{c_1,\ldots,c_n\}$.

To capture the rich fact-level relationships among these chunks, we associate them with a KG,  which can be implemented via the following approaches. In cases where a KG is available, such as in WebQSP~\cite{tih16webqsp} and CWQ~\cite{alon18cwq}, the chunk-KG association can be performed through entity and relation recognition and linkage algorithms~\cite{zhao23improving,tian24generating}. 
Another approach involves directly extracting multiple entities and relations from the chunks to form subgraphs, which can be used to combine into a complete graph. 
In this paper, to avoid reliance on existing KGs, we adopt the latter approach, implementing it by providing appropriate prompts (refer to Fig.~\ref{fig:extract_prompt}) to LLMs.

After this process, we provide linkages between chunks and a specific KG, which can be given as
\begin{equation}
    \mathcal{G} = \{(h,r,t,c)\,|\,c\in \mathcal{D}\}, 
\end{equation}
where $h$, $r$, and $t$ denote the head entity, relation, and tail entity, respectively, and $c$ denotes the chunk that derives the triplets.
Note that the chunk-KG association process is query-independent, which implies that it can be performed offline, only needs to be constructed once for all documents, and supports incremental updates for new documents.
As the document offline processing aligns with what vanilla RAG does, \modelname naturally supports adding new documents to or removing documents from the existing knowledge base and KG efficiently.

\subsection{KG-enhanced Chunk Retrieval}
\label{subsec:retrieval}
Given the chunks $\mathcal{D}$ and the associated KG $\mathcal{G}$, the proposed \modelname suggests a two-stage retrieval process, including semantic-based retrieval and graph-guided expansion. 

\paragraph{Semantic-based Retrieval}
During the semantic-based retrieval process, the semantic similarities between a user query $q$ and all the chunks can be measured as
\begin{equation}
\mathcal{S} = \{s(q,c)\,|\,c\in\mathcal{D}\},
\label{eq:embedding_retrieval}
\end{equation}
where the similarity function $s(\cdot)$ employs an embedding model~\cite{zach24nomic,li24mxbai} to transfer the query and chunks into high-dimensional representations, followed by computing their cosine similarity.

The chunks with the top-$k$ highest similarities to the query are selected as the retrieved chunks, denoted by $\mathcal{D}_q$. These retrieved chunks can be integrated into the prompts as context and fed into LLMs for RAG. 
As discussed in Sec.~\ref{sect:intro}, relying solely on semantic-based retrieval may result in isolated chunks, missing crucial factual knowledge and the intrinsic connections among the chunks.
To tackle this, we regard the retrieved chunks $\mathcal{D}_q$ as seed chunks, and propose a graph-guided expansion process.


\paragraph{Graph-guided Expansion}
During communication and thinking processes, people often connect one event to others as these events involve the same entities, such as persons and places.
For example, \textit{Capitol Hill, Washington, D.C.} connects our impressions of \textit{Barack Obama}, \textit{Donald Trump}, and \textit{Joe Biden}, as they all delivered their presidential inaugural speeches there in 2013, 2017, and 2021, respectively.
Shed light by such insights, \modelname suggests linking one chunk to other chunks through the overlapping or connected entities that they contain for retrieved chunk expansion.

Specifically, given the retrieved chunks $\mathcal{D}_q\subseteq\mathcal{D}$ and the KG $\mathcal{G}=\{(h,r,t,c)\,|\,c\in\mathcal{D}\}$, we first get the relevant subgraph of $\mathcal{D}_q$ as follows:
\begin{equation}
\label{eq:subgraph}
\mathcal{G}_q^0 = \{(h,r,t,c)\,|\,c\in\mathcal{D}_q\}\subseteq\mathcal{G}.
\end{equation}

After that, we traverse the $m$-hop neighborhood of $\mathcal{G}_q$ to get the expanded subgraph $\mathcal{G}_q^m$, which can be given as
\begin{equation}
\label{eq:traverse}
\mathcal{G}_q^m = \text{traverse}(\mathcal{G},\mathcal{G}_q^0,m),
\end{equation}
where $\text{traverse}(\cdot)$ can be implemented with the breadth-first search (BFS) algorithm,
% serving as a function that captures the $m$-hop neighborhood of $\mathcal{G}_q^0$.
serving as a function that captures all entities in $\mathcal{G}_q^0$, corresponding $m$-hop neighboring entities, and all edges linking these entities to form an expanded subgraph.

Given the expanded subgraph $\mathcal{G}_q^m$, we can readout all the chunks associated with the graph (i.e., containing facts corresponding to the triplets in this graph) as follows:
\begin{equation}
\label{eq:expansion}
\mathcal{D}_q^m = \{c\,|\,(h,r,t,c)\in\mathcal{G}_m^q\}\subseteq\mathcal{D},
\end{equation}
where $\mathcal{D}_q^m$ is referred to as the expanded chunks.

\paragraph{Discussions}
Several semantic-based and context-based approaches can also achieve chunk expansion. For example, one can increase the value of $k$ in the aforementioned similarity-based retrieval process, or apply a context window expansion~\cite{jiang23arag} (i.e., when a chunk is retrieved, the chunks within the context window are also recalled together). Different from these approaches, the proposed graph-guided expansion gathers chunks that contain the same or related entities or triplets, without requiring these expanded chunks to have high semantic similarity to the query or to be located around the retrieved chunks. 
Such a design of graph-guided expansion helps prevent redundancy and excessive homogeneity among the retrieved and expanded chunks, leading to greater diversity and the development of a more comprehensive knowledge network.
We provide some empirical evidence to further confirm the effectiveness of the proposed graph-guided expansion in Sec.~\ref{sec:discussions}.

\subsection{KG-based Context Organization}
\label{subsec:context}
After the KG-enhanced chunk retrieval, \modelname incorporates a post-processing stage before response generation of LLMs, motivated by the following two considerations.

Firstly, the number of expanded chunks through the graph-guided expansion is tied to the triplets contained in the expanded subgraph, which can be too large, potentially exceeding the context length and introducing noise that may obscure helpful information.
Secondly, inspired by human reading habits and previous studies~\cite{li23promptsurvey,liu24lost}, providing semantically coherent and well-organized materials as context makes positive impacts on the understanding and generation performance of LLMs.
As a result, we propose a KG-based context organization module in \modelname, which serves as both a filter and an arranger to meet these requirements.

\paragraph{Serving as a Filter}
Specifically, we first calculate the semantic similarities between the expanded chunks with the user query, according to Eq.~\eqref{eq:embedding_retrieval}. Based on these similarities, the expanded subgraph $\mathcal{G}_q^m$ can be transformed into an undirected weighted graph as follows:
\begin{equation}
\label{eq:undirect_graph}
\begin{aligned}
\mathcal{U}_q^m=\{&(h\leftrightarrow t, \text{rel}:r, \text{src}:c, \text{weight}:s(q,c))\\
&\,|\,(h,r,t,c)\in\mathcal{G}_q^m\},
\end{aligned}
\end{equation}
where $h\leftrightarrow t$ represents an undirected edge, attached with the corresponding relation and the source chunk as meta information.
We reuse the semantic similarities calculated in Sec.~\ref{subsec:retrieval} to save computing resources.

Due to the cohesive nature of knowledge, $\mathcal{U}_q^m$ can naturally be divided into $p$ connected components, denoted by $\mathcal{B}_i,1\leq i\leq p$, where nodes within each connected component $\mathcal{B}_i$ represent entities from the KG. Note that multiple edges may connect a pair of nodes due to redundant knowledge, which promotes us to generate the maximum spanning tree (MST) of each connected component for filtering. This can be formulated as
\begin{equation}
\label{eq:mst}
\mathcal{T}_i = \text{MST}(\mathcal{B}_i).
\end{equation}
Through such a filtering process, we retain only the most relevant linking information between entities and eliminate redundant edges, thereby enhancing the informativeness of the retrieved chunks.

\paragraph{Serving as an Arranger}
With the KG-based context organization module, we aim to integrate the retrieved chunks into intrinsically related and self-consistent paragraphs with the KG as the skeleton.

To achieve this, we provide two representations for each generated MST $\mathcal{T}_i$, including a text representation and a triplet representation. For the text representation, we pick the edge with the highest weight as the root, and concatenate all the chunks linked to the edges using a depth-first search (DFS) algorithm to form a coherent paragraph. For the triplet representation, we concatenate all the edges in the form of $<h,r,t>$ within the MST.

We calculate the relevance scores between MSTs and the user query based on their triplet representations using a cross-encoder reranking function~\cite{xiao23cpack}:
\begin{equation}
\label{eq:rerank}
R(q,\mathcal{T}_i)=C(q,\text{conc}(\mathcal{T}_i)),
\end{equation}
where $C(\cdot)$ is the cross-encoder reranking function and $\text{conc}(\cdot)$ is used to obtain the triplet representations. We use triplet representations instead of text representations because triplets provide a concise and structured refinement of the key information associated with the corresponding chunks, allowing relevance matching to focus on key information.

After computing the relevance scores, we sort the MSTs $\{\mathcal{T}_i\,|\,1\leq i\leq p\}$ according to their relevance $\{R(q,\mathcal{T}_i)\}$ to the user query $q$ in descending order. Then, we include their text representations in order until the top-$k$ constraint on the number of chunks has been reached. Finally, these selected chunks are fed into the LLMs along with the user query for response generation.
