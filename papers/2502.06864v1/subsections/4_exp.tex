\begin{table*}[!ht]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccc}
\toprule
\multirowcell{2.4}{Methods} & \multicolumn{3}{c}{Hotpot-Dist} & \multicolumn{3}{c}{Hotpot-Full} & \multicolumn{3}{c}{Shuffle-Hotpot-Dist} & \multicolumn{3}{c}{Shuffle-Hotpot-Full} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
& F1 & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall \\
\midrule
LLM-only & 0.237 & 0.259 & 0.234 & 0.237 & 0.259 & 0.234 & 0.158 & 0.175 & 0.158 & 0.158 & 0.175 & 0.158 \\
Semantic RAG & 0.617 & 0.646 & 0.643 & 0.528 & 0.558 & 0.535 & 0.508 & 0.533 & 0.524 & 0.422 & 0.449 & 0.433 \\
\quad +\,Rerank & 0.652 & 0.685 & 0.665 & 0.587 & 0.613 & 0.603 & 0.532 & 0.560 & 0.546 & 0.447 & 0.476 & 0.456 \\
Hybrid RAG & 0.653 & 0.676 & 0.655 & 0.551 & 0.582 & 0.558 & 0.520 & 0.548 & 0.534 & 0.443 & 0.473 & 0.446 \\
LightRAG & 0.293 & 0.288 & 0.480 & 0.261 & 0.259 & 0.364 & 0.285 & 0.284 & 0.404 & 0.202 & 0.199 & 0.293 \\
GraphRAG & 0.400 & 0.408 & 0.491 & 0.169 & 0.157 & 0.429 & 0.351 & 0.365 & 0.401 & 0.163 & 0.155 & 0.362 \\
\modelname & \textbf{0.663} & \textbf{0.690} & \textbf{0.683} & \textbf{0.631} & \textbf{0.665} & \textbf{0.643} & \textbf{0.545} & \textbf{0.572} & \textbf{0.566} & \textbf{0.507} & \textbf{0.539} & \textbf{0.512} \\
\bottomrule
\end{tabular}
}
\caption{Comparisons in terms of response quality between \modelname and baselines.}
\label{tab:results_answer}
\end{table*}

\begin{table*}[!ht]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccc}
\toprule
\multirowcell{2.4}{Methods} & \multicolumn{3}{c}{Hotpot-Dist} & \multicolumn{3}{c}{Hotpot-Full} & \multicolumn{3}{c}{Shuffle-Hotpot-Dist} & \multicolumn{3}{c}{Shuffle-Hotpot-Full} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
& F1 & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall \\
\midrule
Semantic RAG & 0.343 & 0.206 & 0.894 & 0.300 & 0.178 & 0.790 & 0.321 & 0.201 & 0.837 & 0.268 & 0.167 & 0.708 \\
\quad +\,Rerank & 0.357 & 0.224 & \textbf{0.932} & 0.306 & 0.197 & 0.833 & 0339 & 0.213 & \textbf{0.886} & 0.286 & 0.179 & 0.754 \\
Hybrid RAG & 0.354 & 0.222 & 0.921 & 0.302 & 0.189 & 0.795 & 0.334 & 0.210 & 0.837 & 0.279 & 0.174 & 0.739 \\
LightRAG & 0.234 & 0.150 & 0.638 & 0.132 & 0.083 & 0.340 & 0.227 & 0.148 & 0.535 & 0.116 & 0.073 & 0.295 \\
GraphRAG & 0.255 & 0.167 & 0.594 & 0.180 & 0.113 & 0.470 & 0.210 & 0.138 & 0.482 & 0.199 & 0.126 & 0.510 \\
\modelname & \textbf{0.436} & \textbf{0.301} & 0.908 & \textbf{0.310} & \textbf{0.203} & \textbf{0.838} & \textbf{0.405} & \textbf{0.279} & 0.840 & \textbf{0.305} & \textbf{0.193} & \textbf{0.790} \\
\bottomrule
\end{tabular}
}
\caption{Comparisons in terms of retrieval quality between \modelname and baselines.}
\label{tab:results_sps}
\end{table*}

\section{Experiments}

\subsection{Experiment Setup}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figs/triplet_distribution.pdf}
\caption{Statistics of triplet extraction.}
\label{fig:triplet_distribution}
\end{figure}

\paragraph{Datasets}
We conduct experiments on the benchmark dataset HotpotQA~\cite{yang18hotpotqa}, where each query can be associated with several materials (e.g., relevant content in Wikipedia) to help in response generation. 
The HotpotQA dataset consists of two settings, named {\bf HotpotQA-Dist} and {\bf HotpotQA-Full}. In the distractor setting, a total of ten documents are provided as supporting materials, including all useful knowledge as well as some irrelevant content. In the fullwiki setting, it is required to identify useful knowledge from the entire 66,581 documents extracted from Wikipedia.

For the KG-chunk association, we provide a manual prompt to Llama-3~\cite{dubey24llama3} for extracting entities and relations from the 66,581 documents of HotpotQA, resulting in a total of 211,356 triplets consisting of 98,226 entities and 19,813 relations. Each triplet in the constructed KG is linked to its source chunk. 
We record the number of triplets extracted from each chunk and document, and plot the corresponding distributions of chunks and documents in Fig.~\ref{fig:triplet_distribution}, which shows a long-tail phenomenon.

Furthermore, to alleviate the dependence on prior knowledge during the generation process (i.e., the training corpus of LLMs may contain Wikipedia content) and to better demonstrate the effects of RAG, we construct variants of HotpotQA. Specifically, for each entity, we randomly replace it with another entity in the same category, and then update the queries, triplets, and documents accordingly. For example, the entity \textit{Family Guy} can be replaced with \textit{Rick and Morty}, and all instances of \textit{Family Guy} contained in queries, triplets, and documents would be updated to \textit{Rick and Morty}. Therefore, LLMs have to identify and extract relevant content from the documents rather than relying on prior knowledge about \textit{Family Guy} from training data to correctly answer the queries.
Note there might generate lots of new triplets such as (\textit{Rick and Morty}, \textit{language}, \textit{French}), as the original tail entity can be also transformed from \textit{English} to \textit{French}.
The produced variant datasets are denoted by {\bf Shuffle-HotpotQA-Dist} and {\bf Shuffle-HotpotQA-Full}, respectively.

\paragraph{Evaluation Metrics}
We compare \modelname with existing RAG-based methods in terms of response quality and retrieval quality, which can be influenced by both the retrieved chunks and context organization.
For retrieval quality, we use the evaluation script provided by HotpotQA to measure the F1 score, precision, and recall between the retrieved chunks and referenced facts.
For response quality, we adopt the F1 score, precision, and recall as metrics, comparing the generated responses against ground truth answers.

\paragraph{Baselines}
In the experiments, we compare \modelname with the following baseline methods:
\begin{itemize}
    \item \textit{LLM-only}, which directly instructs LLMs to generate responses to user queries without any additional retrieval mechanisms.
    \item \textit{Semantic RAG}~\cite{jiang23arag}, which employs a semantic-based approach to retrieve relevant chunks. These chunks are concatenated into the prompt and fed into the LLMs for response generation. For more details, please refer to Sec.~\ref{subsec:retrieval}.
    \item \textit{Hybrid RAG}~\cite{gao21complement}, which combines a semantic-based retrieval method with a keyword-based retrieval method (e.g., BM25~\cite{arian23bm25}) for chunk retrieval. The retrieved chunks are subsequently merged through a cross-encoder reranker.
    \item \textit{GraphRAG}~\cite{darren24graphrag}, which constructs a graph-based index with an LLM. GraphRAG derives a knowledge graph from the source documents and pre-generates community summaries for clustered entities. Given a query, it generates partial responses with each related community summary and aggregates them into the final answer.
    \item \textit{LightRAG}~\cite{guo2024lightrag}, which acts as a lightweight version of GraphRAG. LightRAG extracts entities and relations from the source documents and generates a short description of each entity for retrieval. The retrieved information is unified with the query and fed into the LLM for generation.
\end{itemize}

For \modelname and all baseline methods, we use LLaMA3-8B~\cite{dubey24llama3} as the LLM for KG construction and response generation, mxbai-embed-large~\cite{li24mxbai} as the embedding model, and bge-reranker-large~\cite{xiao23cpack} as the cross-encoder reranker for both Hybrid RAG and \modelname. The value of $k$ is set to 10 unless otherwise specified.

\begin{table*}[!ht]
\centering
% \resizebox{\columnwidth}{!}{
{\small
\begin{tabular}{lccccccr}
\toprule
\multirowcell{2.4}{} & \multicolumn{3}{c}{Response Quality} & \multicolumn{4}{c}{Retrieval Quality} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-8}
& F1 & Precision & Recall & F1 & Precision & Recall & $\#$Avg.\\
\midrule
\modelname & 0.663 & 0.690 & 0.683 & 0.436 & 0.301 & 0.908  & 8.11 \\
\quad w/o organization & 0.660 & 0.678 & 0.679 & 0.259 & 0.153 & 0.963 & 16.76 \\
\quad w/o expansion & 0.626 & 0.653 & 0.645 & 0.473 & 0.341 & 0.842 & 4.41 \\
% \quad w/o triplet representation & 0.634 & 0.670 & 0.645 & 0.396 & 0.263 & 0.870 & 8.23 \\
\bottomrule
\end{tabular}
}
\caption{Experimental results of an ablation study conducted on HotpotQA in the distractor setting.}
\label{tab:raw_distractor_ablation}
\end{table*}

\begin{table*}[!ht]
\centering
% \resizebox{\columnwidth}{!}{
{\small
\begin{tabular}{lccccccr}
\toprule
\multirowcell{2.4}{} & \multicolumn{3}{c}{Response Quality} & \multicolumn{4}{c}{Retrieval Quality} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-8}
& F1 & Precision & Recall & F1 & Precision & Recall & $\#$Avg.\\
\midrule
\modelname & 0.545 & 0.572 & 0.566 & 0.405 & 0.279 & 0.840 & 8.09 \\
\quad w/o organization & 0.538 & 0.563 & 0.560 & 0.182 & 0.102 & 0.962 & 24.56 \\
\quad w/o expansion & 0.474 & 0.503 & 0.485 & 0.511 & 0.458 & 0.656 & 3.82 \\
% \quad w/o triplet representation & 0.532 & 0.565 & 0.545 & 0.432 & 0.306 & 0.810 & 6.87 \\
\bottomrule
\end{tabular}
}
\caption{Experimental results of an ablation study conducted on Shuffle-HotpotQA in the distractor setting.}
\label{tab:pu_distractor_ablation}
\end{table*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.85\linewidth]{figs/vary_k.pdf}
\caption{Experimental results with varying top-$k$ on HotpotQA in distractor setting.}
\label{fig:vary_k}
\end{figure*}

\begin{table*}[t]
\centering
% \resizebox{\columnwidth}{!}{
{\small
\begin{tabular}{lccccccr}
\toprule
\multirowcell{2.4}{} & \multicolumn{3}{c}{Response Quality} & \multicolumn{4}{c}{Retrieval Quality} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-8}
& F1 & Precision & Recall & F1 & Precision & Recall & $\#$Avg.\\
\midrule
$m=1$ & 0.663 & 0.690 & 0.683 & 0.436 & 0.301 & 0.908 & 8.11 \\
$m=2$ & 0.656 & 0.681 & 0.674 & 0.420 & 0.291 & 0.917 & 8.53 \\
$m=3$ & 0.658 & 0.678 & 0.675 & 0.421 & 0.284 & 0.924 & 8.19 \\
\bottomrule
\end{tabular}
}
\caption{Experimental results on HotpotQA in the distractor setting with varying $m$.}
\label{tab:vary_m}
\end{table*}

\begin{table*}[t]
\centering
% \resizebox{\columnwidth}{!}{
{\small
\begin{tabular}{lcccccc}
\toprule
\multirowcell{2.4}{} & \multicolumn{3}{c}{Response Quality} & \multicolumn{3}{c}{Retrieval Quality} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
& F1 & Precision & Recall & F1 & Precision & Recall \\
\midrule
Hybrid RAG & 0.653 & 0.676 & 0.655 & 0.354 & 0.222 & 0.921 \\
\modelname & 0.663 & 0.690 & 0.683 & 0.436 & 0.301 & 0.908 \\
\quad $-5\%$ & 0.662 & 0.681 & 0.676 & 0.434 & 0.306 & 0.898 \\
\quad $-10\%$ & 0.654 & 0.688 & 0.682 & 0.432 & 0.305 & 0.890 \\
\bottomrule
\end{tabular}
}
\caption{Experimental results on HotpotQA in the distractor setting with triplets dropped.}
\label{tab:drop_triplets}
\end{table*}

\subsection{Comparisons and Analyses}
\label{sec:comparison}

\paragraph{Response Quality}
The comparisons in terms of response quality between \modelname and the baselines are shown in Table~\ref{tab:results_answer}. From the table, we can observe that all methods utilizing RAG achieve significant improvements compared to the LLM-only approach, exceeding 29.1\% improvements in F1 scores on the original HotpotQA and 26.4\% improvements in F1 scores on Shuffle-HotpotQA. Among these RAG-based methods, \modelname achieves consistent outperformance, especially in the fullwiki setting and on the Shuffle-HotpotQA dataset.

In the fullwiki setting, a large pool of candidate documents (thousands of times more than in the distractor setting) is provided to LLMs, necessitating high-quality retrieval results and effective context organization. In such a challenging setup, our proposed method \modelname achieves at least 8\% improvements compared to baselines, demonstrating that \modelname enhances chunk retrieval through KG-guided approaches that surpass semantic-based and keyword-based methods. Besides, on the Shuffle-HotpotQA dataset, where LLMs should rely more on RAG rather than prior knowledge, our proposed method achieves at least 2.5\% and 6.4\% improvements in the distractor and fullwiki settings, respectively.

\paragraph{Retrieval Quality}
The experimental results are shown in Table~\ref{tab:results_sps}, which demonstrate that \modelname strikes a favorable balance between retrieval precision and recall, highlighting the effectiveness of KG-guided expansion and context organization. In the distractor setting, where irrelevant chunks are limited, our proposed method achieves similar performance in recall but significantly better performance in precision (more than 7.9\% and 6.9\% on HotpotQA and Shuffle-HotpotQA, respectively). 
In the fullwiki setting where identifying relevant chunks is more challenging, our proposed method achieves consistent improvements in both precision and recall compared to other RAG-based methods. These results further confirm the effectiveness of \modelname in providing high-quality retrieval results with the help of KG.


\subsection{Further Discussions}
\label{sec:discussions}

\paragraph{Ablation Study}
We conduct an ablation study to demonstrate the contributions of different modules in \modelname, including KG-guided expansion and KG-based context organization. The experimental results on the HotpotQA and Shuffle-HotpotQA datasets in the distractor setting are shown in Tables~\ref{tab:raw_distractor_ablation} and \ref{tab:pu_distractor_ablation}, where we also report the average number of retrieved chunks.

From these results, we can observe that using only KG-guided expansion without KG-based context organization (denoted by ``w/o organization'' in the table), \modelname achieves similar performance in terms of answer quality but significantly worse retrieval quality. The reason is that, without the KG-based context organization module, the number of retrieved chunks can be noticeably larger, potentially containing irrelevant chunks that do not contribute positively to performance but consume additional tokens. These findings confirm the contribution of the KG-based context organization module in effectively selecting and organizing retrieved chunks to preserve relevant information. 

With only the KG-based context organization module (denoted by ``w/o expansion'' in the table), \modelname achieves high retrieval precision and F1 score with a significantly smaller number of chunks, but fails to provide better responses, as some necessary chunks may not be retrieved using only semantic-based approaches.
These results confirm the importance of the KG-guided expansion module in successfully leveraging KG to capture fact-level relationships between chunks and retrieve key information that might be missed by semantic-based approaches.

\paragraph{Performance w.r.t. Varying $k$}
We conduct experiments with varying top-$k$ values on HotpotQA in the distractor setting. The experimental results are shown in Fig.~\ref{fig:vary_k}. 
From these figures, we can observe that \modelname maintains superior performance compared to baselines with different $k$. When $k$ is set to a suitable value (e.g., 5 or 10), \modelname ensures the efficient retrieval of high-quality chunks, thereby providing coherent and contextually consistent contexts for generating high-quality responses.

However, when $k$ is set to a too large value (e.g., 15), although the retrieval recall significantly improves, the quality of the generated responses does not increase proportionally, which indicates simply increasing the number of chunks cannot always result in a better retrieval recall ratio and response quality. 
\modelname exhibits the least sensitivity to the hyperparameter $k$ compared to baselines, which makes the RAG process robust.

\paragraph{Performance w.r.t. Varying $m$}
In \modelname, $m$ serves as the hyperparameter for graph expansion, balancing the trade-off between retrieval precision and recall. We set the $m$-hop value to 1 in the previous experiments. 
To further explore the effects of $m$, we conduct experiments with varying $m$ on HotpotQA dataset. The results are shown in Table~\ref{tab:vary_m}. These results indicate that setting $m=1$ is appropriate for the experiments, and KG$^2$RAG shows low sensitivity to the hyperparameter $m$.

\paragraph{Robustness Analysis}
To further confirm the robustness of \modelname with quality-limited KGs, we randomly drop 5\% or 10\% of the triplets from the constructed KG, and show the experimental results in Table~\ref{tab:drop_triplets}. The results demonstrate that \modelname maintains robust performance even with quality limitations and outperforms the baselines.