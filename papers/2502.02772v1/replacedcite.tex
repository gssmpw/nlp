\section{Related Work}
\label{sec:Related Work}

% Prior work relevant to our research extends across three main areas: force-based human-robot interaction, grounding natural language in robot actions, and multimodal embedding spaces for robotic learning.

\subsection{Force-Based Human-Robot Interactions}

Force-based interactions have been studied in the past for human-robot collaborative tasks.
Early work in ____  demonstrated automatic program generation from force-based teaching data. Furthermore, ____ established the significance of force feedback in human-robot interfaces, putting down the groundwork for new interaction paradigms. Recent work has significantly improved our understanding of force-based manipulation, with ____ demonstrating planning for tool use under force constraints and ____ further extending this to robust multi-stage manipulation tasks.

Research on using force sensing for improved physical human-robot interaction was explored in ____  showing methods for learning from demonstration using force data ____ and explaining human intent from contact forces ____. However, these works are applicable to tasks under specific conditions; broader task variability, diverse conditions and contexts, and subtle nuance that language can describe are not considered. %mostly focus on direct force control or planning instead of creating shared representations between forces and language.

% \begin{figure*}[H]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/architecture_level_0.png}
%     \caption{Our model is made up of 2 autoencoders. One is responsible for encoding and decoding forces to and from the shared latent space, and the other is responsible for encoding and decoding phrases to and from the latent space. They are trained with data collected from human participants to represent corresponding inputs similarly in the shared latent space. To achieve translation between modalities, embed the input into the shared latent space using its respective encoder and decode using the decoder of the other modality. TODO: replace cross recon outputs with real outputs of model (hopefully they are similar)}
%     \label{fig:architecture_level_0}
% \end{figure*}

\subsection{Grounding Natural Language in Robot Actions} 
Natural language has been investigated in literature for grounding language phrases to robot actions and behaviors.  ____ developed probabilistic approaches for mapping natural language instructions to robot trajectories. 
Building on this, ____ showed methods for learning semantic parsers that ground language to robot control policies. Recent work has shown the use of large language models to improve language understanding for robotics ____ ____ . While these approaches map language to robot actions, the tasks are mostly pick-and-place, and more complex manipulative tasks that involve contact forces are excluded. % they generally do not consider force-based interactions and mapping language phrases to forces in a particular direction and magnitude with a duration of time.  

\subsection{Multimodal Embeddings in Robotics}
Research in learning shared embedding spaces between different modalities for robotic learning has been explored in the past. ____ developed cross-modal embeddings between visual and force data for manipulation tasks. ____ showed learning joint embeddings of language and vision for a robot instruction navigation task. 
Although, these approaches have demonstrated the potential of multimodal embeddings in robotics, none have specifically addressed the challenge of creating shared embeddings between force trajectories and natural language descriptions. 

The current work aims to fill this gap by developing and providing a framework of bidirectional translation between physical forces and their linguistic descriptions. Inspired by physical therapists' interactions with patients, we will address the needs for unified representation of language and force profiles and effectiveness of force-language cross-modality embedding to better understand how these strikingly distinct modalities can be integrated.
% This framework will enable future HRI researchers and developers to generate appropriate forces from language commands or generate natural language descriptions from observed forces. Such capabilities could serve as building blocks for more sophisticated human-robot interaction architectures, allowing robots to both understand force-based commands and communicate their intended actions more naturally.

%