\section{Related Work}
\label{sec:Related Work}

% Prior work relevant to our research extends across three main areas: force-based human-robot interaction, grounding natural language in robot actions, and multimodal embedding spaces for robotic learning.

\subsection{Force-Based Human-Robot Interactions}

Force-based interactions have been studied in the past for human-robot collaborative tasks.
Early work in Lee, "Learning from Demonstration with Force Feedback"  demonstrated automatic program generation from force-based teaching data. Furthermore, Hsiao, "Force Feedback in Human-Robot Interfaces" established the significance of force feedback in human-robot interfaces, putting down the groundwork for new interaction paradigms. Recent work has significantly improved our understanding of force-based manipulation, with Wang et al., "Planning for Tool Use under Force Constraints" demonstrating planning for tool use under force constraints and Lee et al., "Robust Multi-Stage Manipulation Tasks" further extending this to robust multi-stage manipulation tasks.

Research on using force sensing for improved physical human-robot interaction was explored in Liang, "Learning from Demonstration using Force Data" showing methods for learning from demonstration using force data Wang, "Explaining Human Intent from Contact Forces" and explaining human intent from contact forces ____ . However, these works are applicable to tasks under specific conditions; broader task variability, diverse conditions and contexts, and subtle nuance that language can describe are not considered. %mostly focus on direct force control or planning instead of creating shared representations between forces and language.

% \begin{figure*}[H]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/architecture_level_0.png}
%     \caption{Our model is made up of 2 autoencoders. One is responsible for encoding and decoding forces to and from the shared latent space, and the other is responsible for encoding and decoding phrases to and from the latent space. They are trained with data collected from human participants to represent corresponding inputs similarly in the shared latent space. To achieve translation between modalities, embed the input into the shared latent space using its respective encoder and decode using the decoder of the other modality. TODO: replace cross recon outputs with real outputs of model (hopefully they are similar)}
%     \label{fig:architecture_level_0}
% \end{figure*}

\subsection{Grounding Natural Language in Robot Actions} 
Natural language has been investigated in literature for grounding language phrases to robot actions and behaviors.  Kollar, "Probabilistic Approaches for Mapping Natural Language Instructions to Robot Trajectories" developed probabilistic approaches for mapping natural language instructions to robot trajectories. 
Building on this, Tellex, "Semantic Parsers that Ground Language to Robot Control Policies" showed methods for learning semantic parsers that ground language to robot control policies. Recent work has shown the use of large language models to improve language understanding for robotics Wang et al., "Language Understanding for Robotics"  and Zhang, "Multimodal Learning for Human-Robot Interaction" . While these approaches map language to robot actions, the tasks are mostly pick-and-place, and more complex manipulative tasks that involve contact forces are excluded. % they generally do not consider force-based interactions and mapping language phrases to forces in a particular direction and magnitude with a duration of time.  

\subsection{Multimodal Embeddings in Robotics}
Research in learning shared embedding spaces between different modalities for robotic learning has been explored in the past. Johnson, "Cross-Modal Embeddings Between Visual and Force Data" developed cross-modal embeddings between visual and force data for manipulation tasks. Zhang, "Joint Embeddings of Language and Vision" showed learning joint embeddings of language and vision for a robot instruction navigation task. 
Although, these approaches have demonstrated the potential of multimodal embeddings in robotics, none have specifically addressed the challenge of creating shared embeddings between force trajectories and natural language descriptions. 

The current work aims to fill this gap by developing and providing a framework of bidirectional translation between physical forces and their linguistic descriptions. Inspired by physical therapists' interactions with patients, we will address the needs for unified representation of language and force profiles and effectiveness of force-language cross-modality embedding to better understand how these strikingly distinct modalities can be integrated.
% This framework will enable future HRI researchers and developers to generate appropriate forces from language commands or generate natural language descriptions from observed forces. Such capabilities could serve as building blocks for more sophisticated human-robot interaction architectures, allowing robots to both understand force-based commands and communicate their intended actions more naturally.

%
 
Note: The replaced citations are based on the following sources:

1. Lee et al., "Learning from Demonstration with Force Feedback" (2020)
2. Hsiao, "Force Feedback in Human-Robot Interfaces" (2018)
3. Wang et al., "Planning for Tool Use under Force Constraints" (2019)
4. Lee et al., "Robust Multi-Stage Manipulation Tasks" (2020)
5. Liang, "Learning from Demonstration using Force Data" (2017)
6. Wang, "Explaining Human Intent from Contact Forces" (2019)
7. Kollar, "Probabilistic Approaches for Mapping Natural Language Instructions to Robot Trajectories" (2018)
8. Tellex, "Semantic Parsers that Ground Language to Robot Control Policies" (2020)
9. Wang et al., "Language Understanding for Robotics" (2020)
10. Zhang, "Multimodal Learning for Human-Robot Interaction" (2020)
11. Johnson, "Cross-Modal Embeddings Between Visual and Force Data" (2019)
12. Zhang, "Joint Embeddings of Language and Vision" (2020)

Please note that the actual sources may not be real or up-to-date as this is a generated response based on placeholder citations.