\documentclass[conference]{IEEEtran}
\usepackage{times}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% paper title
% \title{Demonstrating Shared Force-Language Embeddings for Natural Human-Robot Communication}
\title{Cross-Modality Embedding of Force and Language for Natural Human-Robot Communication}
% \title{Force-Language, Cross-Modality Embedding for Synergistic Verbal-Haptic Communication in Physical Interactions between a Human and a Robot}


% \author{Ravi Tejwani$^{1}$, Chengyuan Ma$^{1}$, Paco Gomez-Paz$^{2}$, Paolo Bonato$^{3}$ and H. Harry Asada$^{4}$,$~\IEEEmembership{Fellow,~IEEE}$%
% \thanks{$^{1}$Ravi Tejwani and Chengyuan Ma are with Dept. of Electrical Engineering and Computer Science (EECS), Massachusetts Institute of Technology,
% Cambridge, MA 02142 USA
% {\tt\small \{tejwanir, macy404 \}@mit.edu}}%
% \thanks{$^{2}$Paco Gomez-Paz is with Dept. of Mathematics, Massachusetts Institute of Technology,
% Cambridge, MA 02142 USA
%         {\tt\small pjgomez@@mit.edu}}%
% \thanks{$^{3}$Paolo Bonato is with Harvard Medical School Department of Physical Medicine and Rehabilitation at Spaulding Rehabilitation Hospital, 
% Charlestown, MA 02129 USA
%         {\tt\small pbonato@mgh.harvard.edu}}%
% \thanks{$^{4}$H. Harry Asada is with Dept. of Mechanical Engineering, Massachusetts Institute of Technology,
% Cambridge, MA 02142 USA
%         {\tt\small asada@mit.edu}}%
% }


% You will get a Paper-ID when submitting a pdf file to the conference system
% \author{Author Names Omitted for Anonymous Review. Paper-ID [add your ID here]}

\author{\authorblockN{Ravi Tejwani}
\authorblockA{Electrical Engineering and\\Computer Science\\
Massachusetts Institute of Technology\\
Cambridge, MA 02139\\
tejwanir@mit.edu}
\and
\authorblockN{Karl Velazquez}
\authorblockA{Electrical Engineering and\\Computer Science\\
Massachusetts Institute of Technology\\
Cambridge, MA 02139\\
kvelaz@mit.edu}
\and
\authorblockN{John Payne}
\authorblockA{Electrical Engineering and\\Computer Science\\
Massachusetts Institute of Technology\\
Cambridge, MA 02139\\
johnpayn@mit.edu}
\and
\authorblockN{Paolo Bonato}
\authorblockA{Department of Physical Medicine, Harvard Medical School\\
Rehabilitation, Spaulding Rehabilitation Hospital\\
Charlestown, MA 02129 USA\\
pbonato@mgh.harvard.edu}
\and
\authorblockN{Harry Asada}
\authorblockA{Mechanical Engineering\\
Massachusetts Institute of Technology\\
Cambridge, MA 02139\\
asada@mit.edu}
}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3}, 
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\maketitle

\begin{abstract}
A method for cross-modality embedding of force profile and words is presented for synergistic coordination of verbal and haptic communication. When two people carry a large, heavy object together, they coordinate through verbal communication about the intended movements and physical forces applied to the object. This natural integration of verbal and physical cues enables effective coordination. Similarly, human-robot interaction could achieve this level of coordination by integrating verbal and haptic communication modalities. This paper presents a framework for embedding words and force profiles in a unified manner, so that the two communication modalities can be integrated and coordinated in a way that is effective and synergistic. Here, it will be shown that, although language and physical force profiles are deemed completely different, the two can be embedded in a unified latent space and proximity between the two can be quantified. In this latent space, a force profile and words can a) supplement each other, b) integrate the individual effects, and c) substitute in an exchangeable manner. First, the need for cross-modality embedding is addressed, and the basic architecture and key building block technologies are presented. Methods for data collection and implementation challenges will be addressed, followed by experimental results and discussions.
\end{abstract}

\IEEEpeerreviewmaketitle

% \section{Intended Demonstration}
% \label{sec:Intended Demonstration}
% As a physical therapist performs a therapeutic exercise (Fig. \ref{fig:therapist-patient}), the therapist verbally instructs a patient and physically moves the patient's limb. 
% Experienced therapists effectively engage patients through cross-modality interactions based on both language and physical force. This paper intends to demonstrate that force-language cross-modality embedding can represent language and force profiles in a unified manner, which will facilitate coordination of force and language, as experienced therapists do. 
% % Two interaction scenarios will demonstrate this.

% The live demonstration will involve attendees interacting with a UR robot \cite{ur5} in two distinct ways: In the first scenario, attendees physically guide the robot's end-effector while varying their applied forces. The system monitors the force profiles, maps them to our cross-modality embedding, and generates appropriate verbal descriptions like ``gently right" or ``sharply forward" in real-time.
% In the second interaction scenario, participants experience reverse translation by providing verbal commands to the robot such as ``gently backwards", and the robot executes the motion with corresponding force profiles. For example, guiding their hand backwards with a gentle force that matches the verbal instruction.

% Throughout both scenarios, displays show real-time visualizations of the interaction's force components ($F_x$, $F_y$, $F_z$) alongside the verbal descriptions. This will allow participants to observe how their physical motions and force profiles are being interpreted in the language domain, and conversely, how their verbal commands are being transformed into physical force profiles in real time. 




\footnotetext{Detailed therapy sessions can be seen in our online appendix: \url{https://shared-language-force-embedding.github.io/therapy-sessions/}}


\section{Introduction}
\label{sec:Introduction}
As stipulated in Asimov's laws, robots must perform tasks based on human instruction. A central challenge in robotics has been developing effective ways for humans to communicate and give instructions to robots. 
Early roboticists attempted to develop robot languages, e.g. VAL \cite{mcgraw1982val}, to describe desired robot actions and tasks. However, despite some success, it became apparent that certain motions and behaviors are difficult, inefficient, or impossible to describe through language alone. This limitation is particularly evident in tasks requiring environmental contact and force/compliance control, where forces and moments are not directly visible. Such manipulative tasks, now termed contact-rich manipulation \cite{pang2023global, beltran2020learning}, often involve subconscious knowledge that humans find difficult to articulate through any form of language. To address this challenge, roboticists developed alternative approaches that bypass the need to translate subconscious skills into language. These approaches include teaching by demonstration \cite{atkeson1997robot}, programming by demonstration \cite{cypher1993watch}, skill acquisition \cite{liu1992transferring} and, more recently, imitation learning \cite{ross2010efficient}.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/pt-f-lang-spaulding.png}
    \caption{
    Physical therapist from Spaulding Rehabilitation Hospital is seen demonstrating 'hamstring curl' therapy on the patient with neurological injuries 
    \protect\footnotemark . She is instructing a patient to move 'gently forward' while providing an assistive force. }
    \label{fig:therapist-patient}
\end{figure}

With recent advances in natural language processing, language-grounded robot control has gained significant momentum \cite{tellex2011understanding, matuszek2018grounded}. This approach will play a crucial role in scenarios in which robots and humans interact closely. However, a fundamental question persists: Can language alone effectively convey human intentions, instructions, and desired behaviors to robots?

Consider situations where a human wants a robot to gently touch an object or their body. Such actions are difficult to describe verbally; instead, humans prefer to physically demonstrate the desired gentleness of touch. Yet, physical demonstration alone cannot convey important context, nuance, and reasoning behind the action. This illustrates how language and touch/force are complementary modalities that must be integrated and coordinated for effective human-robot communication.



% To illustrate this challenge, we collaborated with physical therapists from Spaulding Rehabilitation Hospital, where they demonstrated a therapy for turning on the hamstring on a patient with neurological injury (Fig. ~\ref{fig:therapist-patient}). 

To study this problem, we collaborated with physical therapists from Spaulding Rehabilitation Hospital who demonstrated various therapeutic techniques. Figure ~\ref{fig:therapist-patient} shows one such demonstration, where the therapist first explains the  procedure verbally and then demonstrates by gently turning the patient's leg. If a robot were to suddenly move the patient's leg, the patient would likely feel uncomfortable or frightened. 
On the other hand, the verbal explanation, ``I will lift your leg gently", is ambiguous to the patient, wondering how gentle is gentle. They may be anxious to see whether it is painful. The therapist starts pushing the leg immediately after giving the brief explanation, demonstrating what she means by lifting the leg gently. This example highlights how language and physical touch/force serve as two distinct but complementary modalities for describing tasks and communicating intended behaviors. The challenge lies in integrating them effectively.  

% In addressing the integration of language and force modalities, there are two fundamental perspectives to consider. The heterogeneous approach emphasizes the distinct characteristics of each modality - language and force profiles have unique features that complement each other. This naturally suggests a sequential integration where verbal communication precedes physical interaction. Conversely, the homogeneous approach focuses on finding common ground between modalities, enabling their representation in a unified space where they become interchangeable and mutually reinforcing. Our framework adopts this homogeneous perspective, embedding both force profiles and language in a shared latent space to enable seamless bidirectional translation.

The goal of the current work is to establish a unified method for representing language and force that facilitates their integration and coordination. We make the following contributions: 
\begin{enumerate}
    \item A framework for cross-modality embedding of force profiles and words, enabling translation between physical force curves and natural language descriptions;
    \item A paired data collection methodology with 10 participants performing language-to-force and force-to-language translations, capturing human intuition about force-language relationships;
    \item Evaluation metrics and results validating the framework's effectiveness and generalization on unseen data;
\end{enumerate}

% \footnotetext{Open Source Framework: \url{https://shared-language-force-embedding.github.io/}}

% \section{Introduction}
% \label{sec:Introduction}

% \textbf{Motivation}
% Imagine a physical therapist working with a patient during a rehabilitation session. Their interaction happen through the interplay of assistive forces and language phrases. The therapist applies an assistive upward force to lift the patient's leg while instructing "gently lift upwards" (Fig. ~\ref{fig:therapist-patient}). The verbal instructions maps directly to the physical force being applied: a controlled 5N force in the upward direction with gradual onset. This coordinated translation between forces and verbal descriptions is natural for humans but remains a significant challenge for robots in human-robot interactions.
% % Imagine two humans moving a couch through a narrow hallway and up a flight of stairs. One person applies a gentle upward force to the left side of the couch. The other, feeling this change in force, says: "I feel you pushing up to left - are we turning here?" This intuitive translation between physical forces and verbal descriptions continues throughout their task - forces are felt and communicated through language, while language instructions are seamlessly converted back into appropriate forces.  This integration of applied forces and natural language descriptions enables humans to effectively collaborate on complex physical tasks. 
% % However, achieving such natural force-language coordination remains a significant challenge in human-robot interaction.

% \textbf{Challenges}
% As robots increasingly work alongside humans in collaborative physical tasks, they need to both understand and communicate about forces with humans. A robot helping a human in tasks like physical therapy should be able to perform bidirectional translation between forces and language. For example, when a robot observes a force trajectory from a human - a 2-3N force applied in the Z-axis for 4-5 seconds with gradual damping - it must learn to map this physical signal to natural language descriptions like "gently upwards". Conversely, for a verbal instruction like " firmly forward", the robot should generate appropriate force profiles - such as 10N in the Y-axis with a rapid onset and 1-second duration. While many approaches exist for human-robot interaction, the fundamental challenge of creating these mappings between force trajectories and their natural language descriptions remains largely unaddressed.


% \textbf{Gaps}
% This gap stems from the inherent complexity of representing and mapping between these two distinct modalities. Force is a physical quantity characterized by three main elements: magnitude, direction, and duration, typically represented as force profiles in 3D space \cite{craig2009introduction, featherstone2014rigid}. These force profiles capture how the magnitude and direction of applied forces change over time, essential for describing physical interactions. Language on the other hand exists in a high-dimensional semantic space which can be represented through word embeddings like GloVe \cite{pennington2014glove} or Word2Vec \cite{mikolov2013efficient}. These embeddings represent words as dense vectors that encode semantic relationships and meaning.

% \textbf{Goal}
% Our research addresses this gap by developing a framework for learning shared force-language embeddings that enable bidirectional translation between physical forces and their natural language descriptions. 

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/pt-f-lang.png}
%     \caption{A physical therapist\protect\footnotemark is seen instructing a patient to 'gently lift upwards' while applying an upward force to the leg. This illustrates the synergistic interplay between forces and language in human to human interactions. The physical forces are often communicated through natural language descriptions.}
%     \label{fig:therapist-patient}
% \end{figure}

% \footnotetext{Physical Therapy Professional from Upstream Rehabilitation}

% \textbf{Adapting Exisiting Methods} Although traditional multimodal vision based translation frameworks such as image-to-text or image captioning methods \cite{xu2015show, anderson2018bottom}  have shown promise in many domains, force-language mapping presents unique challenges that prevent direct adaptation of these methods. Force signals essentially  have temporal dynamics and physics based constraints which as lacking in image data. Furthermore, force profile profiles have significant user variability - the same verbal description (e.g., "gently left") could have different force profile trajectories for different users.  Existing video-to-language models \cite{krishna2017dense, wang2019learning, pan2020spatio}, despite being effective in learning spatial relationships, lack methods for handling these temporal dynamics and user-dependent physical variations. These distinct challenges calls for the development of a specialized framework for force-language descriptors mapping rather than adapting to the existing architectures.

% \textbf{Vision}
% The proposed framework will serve as a foundation for future HRI researchers and developers to build interaction models that can seamlessly integrate force and language understanding in diverse HRI systems - from collaborative manufacturing where robots need to communicate force adjustments, to rehabilitation robotics where precise force-based feedback is crucial, to household assistance where natural force-language communication could help robots better assist in daily tasks. By open-sourcing this capability, we aim to accelerate development of more intuitive and responsive human-robot interaction systems.

% \textbf{Contributions}
% We make the following contributions: 
% \begin{enumerate}
%     \item An open-source framework\protect\footnotemark for bidirectional force-language mapping, enabling translation between physical force profiles and natural language descriptions;
%     \item A paired data collection methodology with 10 participants performing language-to-force and force-to-language translations, capturing human intuition about force-language relationships;
%     \item Evaluation metrics and results validating the framework's effectiveness and generalization on unseen data points;

% \end{enumerate}


%\section{QA}
%\label{sec:qa}

%Why only use force to describe motion? Why not include position, velocity, and even image data?

%Why aren't rule-based or simple networks sufficient? What does a shared embedding uniquely enable?

%Why was the focus only on simple hand motions?

%What is the novelty of the paper?

%Why isn't the architecture taking advantage of the many more advanced deep learning models?

% rt-1 paper: https://arxiv.org/abs/2212.06817

%What language even describes the force behind a motion? Why was the specific vocabulary chosen?

\section{Related Work}
\label{sec:Related Work}

% Prior work relevant to our research extends across three main areas: force-based human-robot interaction, grounding natural language in robot actions, and multimodal embedding spaces for robotic learning.

\subsection{Force-Based Human-Robot Interactions}

Force-based interactions have been studied in the past for human-robot collaborative tasks.
Early work in \cite{asada1989automatic}  demonstrated automatic program generation from force-based teaching data. Furthermore, \cite{kazerooni1993human} established the significance of force feedback in human-robot interfaces, putting down the groundwork for new interaction paradigms. Recent work has significantly improved our understanding of force-based manipulation, with \cite{holladay2019force} demonstrating planning for tool use under force constraints and \cite{holladay2024robust} further extending this to robust multi-stage manipulation tasks.

Research on using force sensing for improved physical human-robot interaction was explored in \cite{haddadin2017robot}  showing methods for learning from demonstration using force data \cite{lee2015learning} and explaining human intent from contact forces \cite{peternel2018robot}. However, these works are applicable to tasks under specific conditions; broader task variability, diverse conditions and contexts, and subtle nuance that language can describe are not considered. %mostly focus on direct force control or planning instead of creating shared representations between forces and language.

% \begin{figure*}[H]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/architecture_level_0.png}
%     \caption{Our model is made up of 2 autoencoders. One is responsible for encoding and decoding forces to and from the shared latent space, and the other is responsible for encoding and decoding phrases to and from the latent space. They are trained with data collected from human participants to represent corresponding inputs similarly in the shared latent space. To achieve translation between modalities, embed the input into the shared latent space using its respective encoder and decode using the decoder of the other modality. TODO: replace cross recon outputs with real outputs of model (hopefully they are similar)}
%     \label{fig:architecture_level_0}
% \end{figure*}

\subsection{Grounding Natural Language in Robot Actions} 
Natural language has been investigated in literature for grounding language phrases to robot actions and behaviors.  \cite{tellex2011understanding} developed probabilistic approaches for mapping natural language instructions to robot trajectories. 
Building on this, \cite{matuszek2018grounded} showed methods for learning semantic parsers that ground language to robot control policies. Recent work has shown the use of large language models to improve language understanding for robotics \cite{andreas2014grounding} \cite{lynch2020language} . While these approaches map language to robot actions, the tasks are mostly pick-and-place, and more complex manipulative tasks that involve contact forces are excluded. % they generally do not consider force-based interactions and mapping language phrases to forces in a particular direction and magnitude with a duration of time.  

\subsection{Multimodal Embeddings in Robotics}
Research in learning shared embedding spaces between different modalities for robotic learning has been explored in the past. \cite{lee2019making} developed cross-modal embeddings between visual and force data for manipulation tasks. \cite{zhang2019neural} showed learning joint embeddings of language and vision for a robot instruction navigation task. 
Although, these approaches have demonstrated the potential of multimodal embeddings in robotics, none have specifically addressed the challenge of creating shared embeddings between force trajectories and natural language descriptions. 

The current work aims to fill this gap by developing and providing a framework of bidirectional translation between physical forces and their linguistic descriptions. Inspired by physical therapists' interactions with patients, we will address the needs for unified representation of language and force profiles and effectiveness of force-language cross-modality embedding to better understand how these strikingly distinct modalities can be integrated.
% This framework will enable future HRI researchers and developers to generate appropriate forces from language commands or generate natural language descriptions from observed forces. Such capabilities could serve as building blocks for more sophisticated human-robot interaction architectures, allowing robots to both understand force-based commands and communicate their intended actions more naturally.

% \section{Design Considerations}
% \label{sec:designconsiderations}

% We discuss key design considerations for the framework development. These considerations address fundamental questions about our methodology and architectural choices.

% \subsection{Shared Representation}
% Direct mappings between forces and language through classifiers or rules fail to capture the rich relationships between physical forces and natural language descriptions. The shared representation enables bidirectional translation through a unified latent space where similar forces and their linguistic descriptions are embedded close together. This approach generalizes to novel force-language combinations by capturing underlying semantic patterns. For example, after training on "gently left" and "quickly up", the method can generate appropriate descriptions like "gently up" for a new slow upward force, despite never seeing this exact combination. Force profiles combining multiple known patterns could elicit emergent descriptions like "firmly up and gradually right" by leveraging the shared space's semantic structure. This common embedding grounds both robot control and human communication, bridging the gap between force measurements and natural language.

% \subsection{Architectural Choices}
% While more complex architectures like transformers \cite{vaswani2017attention, devlin2018bert} or graph neural networks \cite{kipf2016semi, velivckovic2017graph} could be applied, we deliberately chose a basic encoder-decoder design. The architecture's simplicity allows us to establish clear baselines and focus on the core challenge of force-language mapping without conflating architectural complexity with framework effectiveness. This choice prioritizes interpretability and requires less training data while maintaining efficiency. Our contribution addresses the fundamental challenge of creating meaningful force-language associations rather than architectural innovation.

% \subsection{Input Modality and Scope}

% Our framework specifically focuses on force data and simple hand motions for several reasons. The force-only input directly captures human intention during physical interaction while minimizing confounding variables during initial framework validation. By focusing solely on force data before incorporating additional modalities like position or vision, we establish core capabilities that can be extended in future work. Similarly, our choice of simple hand motions provides fundamental interaction primitives that are both common in everyday tasks and allow for controlled data collection with clear ground truth.

% \subsection{Language Structure Design}

% The language structure implements a carefully designed vocabulary that decomposes into \textit{<direction>} and \textit{<modifier>} components. This decomposition naturally maps to how humans describe forces while maintaining sufficient expressiveness for practical applications. Our vocabulary covers essential force characteristics - magnitude, direction, and duration - while balancing complexity with usability. Terms were selected based on common usage in human force descriptions and their clear semantic distinctions.

% \subsection{Framework Novelty}

% While individual components of our framework may use established techniques, our primary contributions are threefold: (1) the first comprehensive framework for bidirectional force-language mapping in human-robot interaction, (2) a novel methodology for collecting and utilizing paired force-language data, and (3) an open-source implementation that enables further research in this domain. These contributions address a significant gap in human-robot communication that has remained largely unexplored.

% Through these design considerations, we developed a framework that prioritizes practical effectiveness and natural human-robot communication. Our choices reflect a balance between theoretical soundness and practical applicability, establishing a foundation for future work in force-based human-robot interaction.


\section{Preliminaries}
\label{sec:Preliminaries}



\subsection{Coordinate System}
\label{sec:Preliminaries:CoordinateSystem}

We first introduce a coordinate system to consistently define spatial directions and interpret force profiles (Table \ref{tab:direction_axis_mapping}). Each force measurement is a vector $\vec{F}(t)\in\mathbb{R}^3$ with components $(F_x(t),F_y(t),F_z(t))$. Fig. \ref{tab:direction_axis_mapping} shows linguistic direction to its corresponding axis:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.60\linewidth]{figures/coordinate_system.pdf}
    \caption{Coordinate system mapping direction words to spatial axes for interpreting force profiles.}
    \label{tab:direction_axis_mapping}
\end{figure}


% \begin{table}[H]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% \textbf{Direction} & \textbf{Axis} \\
% \hline Left & -x \\ Right & +x \\ Backward & -y \\ Forward & +y \\ Down & -z \\ Up & +z \\ \hline \end{tabular} \caption{Mapping between directional words and spatial axes.} \label{tab:direction_axis_mapping} \end{table}


\subsection{Force Profile}
\label{sec:Preliminaries:Force}
We record time-varying force data using a force-torque sensor mounted on the UR robot's end-effector. For a recording $T$ seconds, we store each timestamp $t_i\in[0,T]$ along with the measured force vector $\vec{F}(t_i)$. We refer to the set of samples, ordered chronologically as a \textit{force profile}. Formally, a force profile is represented as a $4\times\mathcal{N}$ tensor:

\begin{equation}
    \text{Force Profile}=
    \begin{bmatrix}
    t_0 & t_1 & \dots & t_{\mathcal{N}-1} \\
    F_x(t_0) & F_x(t_1) & \dots & F_x(t_{\mathcal{N}-1}) \\
    F_y(t_0) & F_y(t_1) & \dots & F_y(t_{\mathcal{N}-1}) \\
    F_z(t_0) & F_z(t_1) & \dots & F_z(t_{\mathcal{N}-1})
    \end{bmatrix}
\end{equation}

where $t_0=0$ and $t_{\mathcal{N}-1}=T$. Figure \ref{fig:force_profile_examples} shows examples of force profiles, paired with textual instructions.

To interpret forces quantitatively, we adopt Newton's second law of motion:

\begin{equation}
   \vec{p}(t)-\vec{p}(0)=\int_0^T\vec{F}(t)dt=\vec{J}(t)
\end{equation}

where at time $t$, $\vec{p}(t)$ is the momentum, $\vec{F}(t)$ is the applied force, and $\vec{J}(t)$ denotes impulse. This unlocks an intuition of the elementary pillars that describe force profiles: \textit{direction} $\hat{F}(t)$, \textit{magnitude} $||\vec{F}(t)||$, and \textit{duration} $T$.

%According to Newton's second law of motion, force is the reason why the motion, or more precisely momentum, of an object changes:

%\begin{equation}
%    \vec{p}(t)-\vec{p}(0)=\int_0^T\vec{F}(t)dt=\vec{J}(t)
%\end{equation}

% % Where $\vec{p}(t)$, $\vec{F}(t)$, and $\vec{J}(t)$ are the momentum, force, and impulse at time $t$. $T$ is the total time the force was exerted. Momentum, force, and impulse are vectors in $\mathbb{R}^3$, as in they are made up of 3 components along the $x$, $y$, and $z$ axes. Impulse is simply another way to refer to the change of momentum; they have the same units.

% % Force exerted by a human can be measured using a force sensor (\ref{sec:Architecture:Data Collection:Phrase-to-Force}). Force readings are recorded at a sampling rate depending on the sensor and execution environment. For some given motion that lasts for $T$ seconds, this results in a list of samples of $\vec{F}(t)$ in terms of its $x$, $y$, and $z$ components, and each sample is tagged with a timestamp of when the sample was taken. We will refer to this collection of samples going forward as a force profile:

% \begin{equation}
%     \text{Force Profile}=
%     \begin{bmatrix}
%     0 & t_2 & \dots & T \\
%     F_{x,0} & F_{x,t_2} & \dots & F_{x,T} \\
%     F_{y,0} & F_{y,t_2} & \dots & F_{y,T} \\
%     F_{z,0} & F_{z,t_2} & \dots & F_{z,T}
%     \end{bmatrix}
% \end{equation}

% The force profile is a $4\times\mathcal{N}$ tensor. $\mathcal{N}$ is the number of samples. Each sample is a column containing the timestamp $t_i$, and the $x$, $y$, and $z$ components of force $F_{x,t_i}$, $F_{y,t_i}$, and $F_{z,t_i}$ respectively. The samples are ordered in increasing timestamp order. The first sample is timestamped at time $0$ and the last sample is timestamped at time $T$, and all other timestamps $t_i$ are some value in between.

% \ref{fig:force_profile_examples} displays several examples of force profiles alongside their corresponding phrase.

% \ref{sec:Architecture:Model:Phrase Input} details further into how the force profile tensor is preprocessed into a $769\times1$ vector before feeding it into the model.

\subsection{Language}
\label{sec:Preliminaries:Language}

Throughout this paper, we define a \textit{phrase} as an ordered list of words describing a motion or force profile (e.g. ``\textit{slowly forward}", ``\textit{quickly right and up}"). To handle these numerically, we consider two distinct vocabularies.

\subsubsection{Minimal Viable Vocabulary} 
\label{sec:Preliminaries:Language:Binary}
This vocabulary contains 18 direction words (e.g. \textit{left}, \textit{right}, \textit{forward-down}) and 12 modifier words (e.g. \textit{slowly}, \textit{quickly}, \textit{harshly}) that describe variations in force magnitude and duration \cite{irie2021examining}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Direction} & \textbf{Modifier} \\
        \hline
        backward & slightly \\
        backward-down & greatly \\
        backward-left & smoothly \\
        backward-right & sharply \\
        backward-up & slowly \\
        down & quickly \\
        down-forward & lightly \\
        down-left & significantly \\
        down-right & softly \\
        forward & harshly \\
        forward-left & gradually \\
        forward-right & immediately \\
        forward-up & \\
        left & \\
        left-up & \\
        right & \\
        right-up & \\
        up & \\
        \hline
    \end{tabular}
    \caption{The minimal viable vocabulary. Direction words describe the overall direction of the force profile, while modifier words describe the magnitude and duration.}
    \label{tab:vocabulary}
\end{table}

% Phrases in this vocabulary are comprised of two words, a modifier and a direction. Each word is represented as a \textbf{binary phrase vector}: the direction word is 18-dimensional vector and the modifier word is a 12-dimensional vector. These two word vectors are then combined to create a 30-dimensional vector representing the complete phrase.

\textbf{Binary Phrase Vectors:} The direction words require 18 dimensions and modifier words require 12 dimensions (as shown in Table \ref{tab:vocabulary}).
Each word is encoded as a 31-dimensional basis vector, where the additional dimension represents an empty or null word. These binary phrase vectors are then concatenated to form a 62-dimensional vector, where exactly two positions contain 1 - one in the first 31 dimensions identifying the modifier (or empty modifier) and one in the second 31 dimensions identifying the direction (or empty direction). This binary encoding scheme ensures consistent representation while allowing for partial or incomplete phrases through the accomodation of empty words.

\subsubsection{Extended GloVe Vocabulary} 
\label{sec:Preliminaries:Language:GloVe}
\textbf{GloVe Embeddings: }We make use of the GloVe (Global Vectors for Word Representation) word embeddings \cite{pennington2014glove}, a pretrained model containing 20,000 words where each word is mapped to a 50-dimensional vector that captures semantic relationships. For phrases of up to three words, we concatenate the embeddings (with padding if needed) to form a $150$-dimensional phrase vector. This continuous representation encodes rich semantic relationships and supports similarity-based operations within the embedding space.


% In this setting, each word is mapped to a 50-dimensional vector, and phrases containing up to three words (to accommodate cases with compound directions, for example, “quickly right up”) are represented by concatenating the corresponding embeddings. When a phrase contains fewer than three words, a special \textit{PAD} token—represented as a 50-dimensional zero vector—is appended to ensure a fixed 150-dimensional input. 

% This continuous representation encodes rich semantic relationships and supports similarity-based operations within the embedding space.

% We also leverage a larger set of 20,000 pretrained GloVe word embeddings \cite{pennington2014glove}, where each word is mapped to a $50$-dimensional vector. 

% There are over 400,000 words in the English language (\cite{pennington2014glove}), let alone the practically infinite number of combinations and orderings of these words to produce rich and meaningful language.

% Most of this language is not necessarily meant to specifically describe motions and forces, therefore we constructed a minimal viable vocabulary of words for the purposes of demonstrating our method. We ensured it was capable of qualitatively describing the quantitative dimensions of motion.

% \ref{sec:Preliminaries:Force} established these dimensions: the force exerted over time $\vec{F}(t)$ and the total duration it is exerted for $T$. As $\vec{F}(t)$ is a vector quantity, it can be further split into its direction $\hat{F}(t)$ and magnitude $||\vec{F}(t)||$ components. This unlocks an intuition for what quantitative pillars our minimal viable vocabulary should qualitatively describe: magnitude, direction, and duration.

% It is straightforward to find a set of words that describe direction, as there exist words that directly correspond to each spatial direction. \ref{tab:direction_axis_mapping} displays and \ref{tab:vocabulary} lists out the 6 direction words we used. Distinct direction words can also be combined to refer to new directions beyond the 6 basis ones.

% Finding a set of words that describe the other two pillars, magnitude and duration, is not as straightforward. Unlike direction, there does not exist a set of words that correspond one-to-one with magnitude or duration. This is a significant issue that motivates the cultivation of a shared embedding between language and force because it is the underlying semantics of the word that can describe magnitude and duration.

% \cite{irie2021examining} demonstrated the presence of adverbs such as \textit{slowly} and \textit{quickly} was enough to affect the hand motions of participants when asked to react to them. We use this finding to justify a selection of 12 common adverbs that we refer to as modifiers because they modify the default force exertion magnitude and duration behaviors of a person. \ref{tab:vocabulary} lists out these modifier words.

% Vector operations cannot be performed directly on the text of phrases, so they need to be embedded into some vector representation. We evaluated our method on two types of phrase embeddings. One was a $31\times1$ binary phrase vector representation of the phrase, and it can be obtained by treating phrases as a multilabel of direction and modifier classes. The other leveraged pretrained GloVe word embeddings (\cite{pennington2014glove}) to create a $150\times1$ vector representation. \ref{sec:Architecture:Model:Phrase Input} goes further into detail on each representation.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/conceptual_shared_latent_space.pdf}
    \caption{Conceptual illustration of the desired properties of the cross-modality latent space. A pair of corresponding force profile and phrase should be located near each other, measured by a distance metric like Euclidean distance or cosine similarity. However, force profiles and phrases that do not correspond should be positioned far away. This demonstrates that similar inputs would be close together and dissimilar inputs would be far apart in the latent space.}
    \label{fig:conceptual_shared_latent_space}
\end{figure}


\begin{figure*}[t]
    \centering
    % First row - (a) and (b)
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/example_force_profile_a.png} 
        \caption{}
        \label{fig:example_force_profile_a}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/example_force_profile_b.png} 
        \caption{}
        \label{fig:example_force_profile_b}
    \end{subfigure}
    
    \vspace{2em}  % Add vertical space between rows
    
    % Second row - (c) and (d)
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/example_force_profile_c.png} 
        \caption{}
        \label{fig:example_force_profile_c}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/example_force_profile_d.png} 
        \caption{}
        \label{fig:example_force_profile_d}
    \end{subfigure}
    
    \caption{Examples of corresponding force profiles and phrase pairs. (a,b) Basic motions in forward and backward directions, showing dominant positive and negative y-components respectively. (c,d) Effect of adding modifiers ('softly' and 'greatly') to forward motion, demonstrating how they alter force magnitude while maintaining direction.}
    \label{fig:force_profile_examples}
\end{figure*}


% \begin{figure*}
%     \centering
    
%     \begin{subfigure}{0.49\linewidth}
%         \centering
%         \includegraphics[width=0.75\linewidth]{figures/example_force_profile_a.png} 
%         \caption{}
%         \label{fig:example_force_profile_a}
%     \end{subfigure}
%     \begin{subfigure}{0.49\linewidth}
%         \centering
%         \includegraphics[width=0.75\linewidth]{figures/example_force_profile_b.png} 
%         \caption{}
%         \label{fig:example_force_profile_b}
%     \end{subfigure}
    
%     \begin{subfigure}{0.49\linewidth}
%         \centering
%         \includegraphics[width=0.75\linewidth]{figures/example_force_profile_c.png} 
%         \caption{}
%         \label{fig:example_force_profile_c}
%     \end{subfigure}
%     \begin{subfigure}{0.49\linewidth}
%         \centering
%         \includegraphics[width=0.75\linewidth]{figures/example_force_profile_d.png} 
%         \caption{}
%         \label{fig:example_force_profile_d}
%     \end{subfigure}

%     \caption{Examples of force profiles and their corresponding phrase pairs demonstrated by a user participant. (a) Shows a basic motion in the forward direction, which is evident from the dominant positive y-component in the force profile (see coordinate system in Table. \ref{tab:direction_axis_mapping}). (b) Shows the opposite motion (backward direction) with a dominant negative y-component. (c) and (d) Demonstrate how adding modifiers ('softly' and 'greatly' respectively) to the forward motion affects the magnitude and profile of the applied forces while maintaining the same primary direction.}
%     \label{fig:force_profile_examples}
% \end{figure*}


\subsection{Cross-Modality Embedding (Shared Latent Space)}
\label{sec:Preliminaries:Shared Latent Space}

We learn the cross-modality embedding as a shared latent space $\mathcal{Z}\subset \mathbb{R}^{16}$ to align force profiles and phrases. 
Rather than treating phrases and force profiles as purely distinct modalities, we emphasize on their common representational ground. In this unified latent space, certain force profiles can be naturally described by words (e.g., ``gentle push"), and conversely, phrases can be manifested as force trajectories. 
% This interchangeability enables more sophisticated coordination than would be possible by focusing solely on the modalities' differences. 
Specifically, we define encoders $E_{\text{force}}$, $E_{\text{phrase}}$ that map force profiles and phrases, respectively, to a shared embedding $z\in\mathcal{Z}$. We also define decoders $D_{\text{force}}$, $D_{\text{phrase}}$ that map shared embeddings back to forces and phrases, respectively. A contrastive learning objective \cite{hadsell2006dimensionality} encourages embeddings of paired force–phrase data to lie close together in $\mathcal{Z}$ while pushing apart non-matching pairs. This alignment supports:

\begin{itemize}
    \item Force-to-Language Translation: Observed force profiles can be decoded into textual instructions.
    \item Language-to-Force Translation: Written phrases can be transformed into corresponding force trajectories for robotic execution.
\end{itemize}

Together, these capabilities enable more natural interactions in human-robot collaboration by unifying physical force signals and language instructions within a single latent representation \cite{radford2021learning}.

% A latent space is a lower dimensional abstract representation of data that captures meaningful "hidden" or "latent" variables that describe the data. A good latent space effectively represents these latent variables such that similar inputs are mapped similarly in the latent space. This enables potential to generalize because the latent space abstracts away fine grain details of inputs to understand the patterns and roles different latent variables play.

% A model can be trained to learn a latent representation of inputs. This can either be done through the architecture of the model itself (\cite{kramer1991nonlinear}) or through coercing the model to optimize a loss function that directly influences the structure of the latent space (\cite{hadsell2006dimensionality}, \cite{radford2021learning}, \cite{oord2018representation}).

% We were interested in architecting a model that can learn a shared latent space. Usually, latent spaces capture latent variables of the input of a single modality. On the other hand, a latent space that captures latent variables of inputs of multiple modalities must have some understanding of shared characteristics between the two modalities. This enables cross modal understanding and in turn translation between modalities.

% The latent space our model learns should be able to relate corresponding force profiles and phrases together. \ref{fig:conceptual_shared_latent_space} illustrates our desired properties of the latent space. The model should have some notion that a force profile of a motion to the right is abstractly similar to the phrase \textit{right}, however not similar to the phrase \textit{backward}.

% The vector representations of force profiles and phrases however exist in different dimensional spaces. In order to share a latent space, they must be compressed into the same space. We chose latent space vectors to be $16\times1$ because it is roughly half of the binary phrase vector representation of phrases, which is $31\times1$, the smallest of the other representations of force profiles and phrases. This ensures all representations are being compressed into a lower dimensional space.






\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/architecture_level_1_without_compass.pdf}
    \caption{Architecture diagram of the cross-modality  dual autoencoder that represents phrases as a concatenation of three $50\times 1$ GloVe (\cite{pennington2014glove}) word embeddings. The phrase embedding input is then passed into the phrase autoencoder, which encodes it into the shared latent space. There the $16\times 1$ embedding can take 2 paths: either be decoded back into a phrase or be translated into a force profile by using the force profile decoder. Force profile inputs are first preprocessed to extract meaningful features for the force profile autoencoder to digest. They are then encoded into the shared latent space. Like phrase inputs, they can either be decoded back into a force profile or be translated into a phrase by using the phrase decoder.}
    \label{fig:architecture_level_1}
\end{figure*}

\section{System Overview}
\label{sec:System Overview}

In this work, we aim to develop a unified representation for physical forces and natural language phrases. Our primary goal is to learn a shared embedding space that allows robots to translate human-applied force profiles into linguistic descriptions and, conversely, generate appropriate force outputs from language instructions. 

% Unlike prior methods that treat force-based control and language-conditioned actions as separate problems, we integrate both into a single framework.

We use a UR \cite{ur5} robot for the magnitude, direction, and duration of applied forces over time. To build a dataset that naturally pairs force signals with language, we designed two human-participant procedures. In the Phrase-to-Force procedure (Sec. \ref{sec:Architecture:Data Collection:Phrase-to-Force}), participants receive a brief textual phrase and physically move the robot arm. In the Force-to-Phrase procedure (Sec. \ref{sec:Architecture:Data Collection:Force-to-Phrase}), participants observe an externally applied force trajectory and then describe it in natural language from our minimal viable vocabulary (e.g., “gradually left”). By collecting these paired samples, we obtain a diverse dataset for learning force-language correspondences.

Our proposed model is a dual autoencoder architecture (Fig. \ref{fig:architecture_level_1}), which processes both time-series force profiles and textual phrases. We encode each force profile into a latent representation and similarly embed each phrase into a matching latent space. We train the model with three core objectives that facilitate robust multimodal alignment: (1) reconstruction, which ensures that both forces and phrases can be faithfully recovered from their respective embeddings, (2) contrastive learning, which encourages correct force–phrase pairs to be close in latent space while pushing apart mismatched pairs, and (3) translation, which enables the network to generate a force profile from a given phrase and to describe a given force profile with a textual output.

By optimizing these objectives, the system learns to embed semantically related force and language inputs in close proximity. During inference, a robot can interpret a previously unseen force in linguistic terms or synthesize an appropriate force response for a phrase. We describe the details of the training procedure, architecture, and data preprocessing in Sec. \ref{sec:Architecture}. In subsequent sections, we evaluate how well the learned embeddings capture force-language relationships and demonstrate the system’s capability to perform bidirectional translation between physical forces and natural language.

% v0:
% The goal of this system is to develop a shared embedding space between force and language, allowing bidirectional translation between physical forces and natural language descriptions. This enables robots to interpret human-applied forces in linguistic terms and, conversely, generate appropriate force responses from language instructions. Unlike prior approaches that focus on either force-based control or language-conditioned robotic behaviors in isolation, this system integrates both modalities into a unified representation.

% To achieve this, the system processes two primary input types: force profiles and language phrases. A force profile captures the magnitude, direction, and duration of an applied force over time, represented as a structured sequence of force measurements. A language phrase, by contrast, consists of direction and modifier terms that qualitatively describe motion. These two input types exist in inherently different spaces—one in a physical signal domain and the other in a high-dimensional semantic space. The system learns to map both into a shared latent representation through a dual autoencoder architecture.

% The model undergoes multitask learning to ensure robust force-language mapping. Three key objectives drive training: (1) contrastive learning, which encourages force profiles and corresponding phrases to be mapped closely in the latent space while pushing apart non-matching pairs, (2) reconstruction, which ensures that both force profiles and phrases can be faithfully reconstructed from their latent embeddings, and (3) translation, which trains the system to generate a reasonable force profile given a phrase and vice versa. These objectives collectively shape a latent space where semantically related forces and phrases share similar embeddings.

% Data to train the model was collected through human-participant procedures designed to capture natural correspondences between force and language. In the Phrase-to-Force procedure (\ref{sec:Architecture:Data Collection:Phrase-to-Force}), participants were given a phrase and asked to apply a corresponding force to a UR10 robot arm, generating labeled force profiles. In the Force-to-Phrase procedure (\ref{sec:Architecture:Data Collection:Force-to-Phrase}), participants were instead presented with an externally applied force trajectory and asked to describe it in natural language. The resulting dataset consists of paired force profiles and language descriptions, providing a foundation for learning bidirectional mappings.

% The following section details the system’s architecture, including the preprocessing of force and language inputs, the structure of the dual autoencoder model, and the training objectives that enable force-language translation.


\section{Architecture}
\label{sec:Architecture}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/architecture_level_0.png}
%     \caption{High level overview of the framework. }
%     \label{fig:architecture_level_0}
% \end{figure}



\subsection{Model}
\label{sec:Architecture:Model}

\subsubsection{Force Profile Input}
\label{sec:Architecture:Model:Force_Profile_Input}

Each raw force profile consists of time-series measurements for the $x$, $y$, and $z$ components of force, recorded at potentially irregular intervals and for varying durations. To create a uniform representation, we first resample each force profile to $\mathcal{N}=256$ evenly spaced time steps spanning a fixed duration $T=4$. This yields a $3\times256$ tensor

\begin{equation}
    \mathbf{F}=
    \begin{bmatrix}
    F_{x,0} & F_{x,1} & \dots & F_{x,255} \\
    F_{y,0} & F_{y,1} & \dots & F_{y,255} \\
    F_{z,0} & F_{z,1} & \dots & F_{z,255}
    \end{bmatrix}
\end{equation}

where $F_{x,i}, F_{y,i}, F_{z,i}$ denote the resampled forces at the $i$-th time step in each axis. Next, we integrate each axis of $\mathbf{F}$ over time to obtain an impulse profile

\begin{equation}
    \mathbf{J}=
    \begin{bmatrix}
    J_{x,0} & J_{x,1} & \dots & J_{x,255} \\
    J_{y,0} & J_{y,1} & \dots & J_{y,255} \\
    J_{z,0} & J_{z,1} & \dots & J_{z,255}
    \end{bmatrix}
\label{eq:impulse_profile}
\end{equation}

where $J_{a,i}=\int_0^{t_i}F_a(t_i)dt$ and $t_i$ is the time associated with the $i$-th sampled resampled point. $\mathbf{J}$ is flattened to form a $768$-dimensional vector $\begin{bmatrix}\mathbf{J}_x&\mathbf{J}_y&\mathbf{J}_z\end{bmatrix}\in\mathbb{R}^{768}$ which serves as input to the force encoder.

% Neural networks, including the force autoencoder in our model, require a fix sized input of features in order to process them. A raw force profile comprises $\mathcal{N}$ samples for each component $x$, $y$, and $z$ of force, and $\mathcal{N}$ timestamps for each sample. $\mathcal{N}$ is not guaranteed to be fixed across multiple force profiles due to the differing total durations $T$ each force profile may last for, as well as the variable sampling rates of force sensors. Furthermore, samples are not guaranteed to be uniformly spaced across time (i.e. the delta time between samples of an individual force profile is not constant) due to the variable sampling rates of force sensors mentioned before. This can result in the model being confused due to the inconsistent timings between samples.

% We take a force profile and for each of its $x$, $y$, and $z$ components resample $256$ evenly spaced samples across the total duration $T$ of the force profile. The resultant $3\times256$ tensor can be thought of as an approximation of the force profile with constant delta time between samples equaling the original total duration divided by the number of resamples: $\frac{T}{256}$. This tensor does not contain information on the original total duration of the force profile, however. Without this critical feature, the tensor simply represents samples across normalized time.

% The total duration $T$ of the force profile is obtained by reading the timestamp of the last sample. This is the last feature that completes the description of the force profile. In order to include it with the $3\times256$ tensor of force samples (and to make it compatible with the force autoencoder that expects a features vector--not tensor--input), the $3\times256$ tensor is flattened into a $768\times1$ vector and is concatenated with the total duration (which can be thought as a $1\times1$ vector/tensor). This results in a $769\times1$ force profile features vector input compatible with the force autoencoder.

\subsubsection{Phrase Input}
\label{sec:Architecture:Model:Phrase Input}

Since neural networks cannot directly process raw text, each phrase must be converted into a fixed-size vector representation that preserves its semantics. We consider one of the two embedding approaches: Binary Phrase Vectors from the Minimal Viable Vocabulary, or GloVe embeddings from Extended Glove Vocabulary (Section ~\ref{sec:Preliminaries:Language}).  

% This discrete representation allows us to treat force profiles as categorical labels with respect to the corresponding modifier and direction.

% Decoding from the continuous embedding space back to discrete words is achieved by identifying the best matching word in the vocabulary (Minimal Viable or Extended Glove) using, e.g. cosine similarity. Specifically, the predicted word embedding is compared against the pretrained embeddings, and the word corresponding to the vector with the highest cosine similarity is selected as the decoder’s output.


% \paragraph{Minimum Viable Vocabulary} In our minimal viable vocabulary, each phrase is composed of exactly two words: a modifier and a direction. Each word is represented as a 31-dimensional binary phrase vector. The modifier and direction vectors are then concatenated to yield a 62-dimensional phrase vector. This discrete representation allows us to treat force profiles as categorical labels with respect to their corresponding modifier and direction.

% \paragraph{Pretrained GloVe Embeddings} Alternatively, we utilize a continuous embedding based on pretrained GloVe word vectors \cite{pennington2014glove}. In this setting, each word is mapped to a 50-dimensional vector, and phrases containing up to three words (to accommodate cases with compound directions, for example, “quickly right up”) are represented by concatenating the corresponding embeddings. When a phrase contains fewer than three words, a special \textit{PAD} token—represented as a 50-dimensional zero vector—is appended to ensure a fixed 150-dimensional input. This continuous representation encodes rich semantic relationships and supports similarity-based operations within the embedding space.

% Neural networks cannot simply process a string of characters of some text. Words, and by extension phrases, need to be embedded into some vector form that best preserves the semantics of the text, i.e. their meaning. We used and later evaluated on two methods: modifier plus direction binary phrase vectors, and pretrained GloVe word embeddings (\cite{pennington2014glove}).

% binary phrase vectors: used to represent a selection of a distinct discrete category. Each category is mapped to a distinct element of the vector, and to select a specific category the corresponding element is marked with $1$ and the other elements are set to $0$.

% Phrases are made up of a unique direction word(s) (\textit{left}, \textit{forward and down}) and an optional unique modifier word (\textit{slightly}, \textit{quickly}). This enables treating them as categorical labels of force profiles, although in this case it takes two labels (a direction label and modifier label) to categorize a single force profile.

% There are 13 modifier categories that force profiles fall into. 12 are for each of the 12 modifier words present in the vocabulary, and an additional category for the no-modifier case since they are optional to begin with. This makes the modifier binary phrase vector $13\times1$.

% A similar process is done for the directions. There are the 6 direction words in the vocabulary that each refer to a distinct spatial direction, however 2 direction words can be combined to refer to a new direction. Order of direction words does not change the direction their composition refers to (\textit{left and up} and \textit{up and left} are equivalent), so we focus on combinations and not permutations. There are ${{6}\choose{2}}=15$ ways to combine 2 direction words, however 3 of these combinations are invalid because they combine opposite directions (\textit{left and right}, \textit{forward and backward}, \textit{up and down}). This results in a total of 18 categories of directions for force profiles, making the direction binary phrase vector $18\times1$.

% Neural networks, including the phrase autoencoder of our model, expect a single vector input. We concatenate the direction and modifier binary phrase vectors to create a $31\times1$ phrase binary phrase vector\footnote{We avoid calling it a binary phrase vector and instead a binary phrase vector because binary phrase vectors are expected to only have one of their elements set to $1$ and the rest $0$. Concatenating binary phrase vectors means the resultant vector will always have more than 1 element set to $1$. At the very least, it is a binary phrase vector since its elements can only take on the values $0$ or $1$}.

% Pretrained GloVe word embeddings (\cite{pennington2014glove}): a mapping of words to vectors that was obtained by executing the aforementioned GloVe (\cite{pennington2014glove}) algorithm on a corpus of Wikipedia and news articles for a set of 400,000 words. The algorithm uses distinct word-word co-occurance statistics to develop vectors that encode the semantics of words. The resultant vector space features interesting properties like similarly meaning words being mapped distance-wise closer together.

% Word embeddings fundamentally change the problem from a classification of distinct categories to a similarity search of viable options. This is because binary/binary phrase vectors discretizes the vector space, while the word embedding space is continuous\footnote{Not exactly because eventually this continuous vector has to be converted back to an actual word which comes from a discrete set. This inverse embedding operation is not unique to our use case, and our model is trained to construct word embedding vectors and not necessarily perfectly convert them back to words.}

% We specifically use the pretrained $50\times1$ word embedding vectors for the top most frequent 20,000 words. This representative sample of words includes word embedding vectors for all the vocabulary words we use, and of course orders of magnitude many more words not present in the data. To convert word embedding vectors back into words\footnote{Another common technique is to find which pretrained word embedding vector has the highest cosine similarity or lowest Euclidean distance with the input word embedding vector. The word corresponding to that pretrained word embedding vector would be chosen word.}, we trained a single linear layer ($50\times20,000$ matrix) to take in $50\times1$ word embedding vectors and convert them into a $20,000\times1$ logits vector which after applying standard soft-max gives us a probability distribution of what words the input word embedding vector is potentially referring to. We arg-max this probability distribution vector to find the element with the highest probability, which corresponds with the decoder's most confident word guess as to what the input word embedding vector is referring to.

% Each word corresponds to a fixed sized $50\times1$ word embedding vector. Phrases however are not fixed lengthen. To provide a single fixed sized input to the phrase autoencoder, we concatenate the word embedding vectors of each word. The maximum possible phrase length in the data is 3 words (2 direction words and a modifier), which means the resultant phrase embedding vector is $150\times1$. Since not all phrases are of length 3, we introduce a special \textit{PAD} token word embedding vector which is just a $50\times1$ vector with all elements set to $0$\footnote{An alternative to setting the vector to $0$ is to take the average vector of all the pretrained word embedding vectors. This is typically done for the special \textit{UNKOWN} token which is a catch all word used to substitute out-of-vocabulary words.}. We pad the end of a phrase this special \textit{PAD} token to ensure every phrase is length 3.

\subsubsection{Dual Autoencoders}
\label{sec:Architecture:Model:Dual Autoencoders}

%% TABLE OF DIMENSIONS OF LAYERS OF AUTOENCODERS

% \begin{figure*}
%     \centering
%     \begin{tabular}{|c|c|c|c|}
%         \hline
%         \textbf{Input Type} & \textbf{Flattened Force Profile Features} & \textbf{Binary Phrase Vector} & \textbf{GloVe Embedding (\cite{pennington2014glove}) Phrase Vector} \\
%         \hline
%         \hline
%         \textbf{Input Vector} & $\boldsymbol{769\times1}$ & $\boldsymbol{31\times1}$ & $\boldsymbol{150\times1}$ \\
%         \hline
%         Encoder Layer 1 & $769\times512$ & $31\times512$ & $150\times512$ \\
%         Encoder Activation 1 & ReLU & ReLU & ReLU \\
%         Encoder Layer 2 & $512\times128$ & $512\times128$ & $512\times128$ \\
%         Encoder Activation 2 & ReLU & ReLU & ReLU \\
%         Encoder Layer 3 & $128\times16$ & $128\times16$ & $128\times16$ \\
%         \hline
%         \textbf{Latent Space Vector} & $\boldsymbol{16\times1}$ & $\boldsymbol{16\times1}$ & $\boldsymbol{16\times1}$ \\
%         \hline
%         Decoder Layer 1 & $16\times128$ & $16\times128$ & $16\times128$ \\
%         Decoder Activation 1 & ReLU & ReLU & ReLU \\
%         Decoder Layer 2 & $128\times512$ & $128\times512$ & $128\times512$ \\
%         Decoder Activation 2 & ReLU & ReLU & ReLU \\
%         Decoder Layer 3 & $512\times769$ & $512\times31$ & $512\times150$ \\
%         \hline
%         \textbf{Output Vector} & $\boldsymbol{769\times1}$ & $\boldsymbol{31\times1}$ & $\boldsymbol{150\times1}$ \\
%         \hline
%     \end{tabular}
%     \caption{Detailed break down of the dimensions of the inputs, layers, and outputs that comprise each different autoencoder that is responsible for encoding and decoding their assigned input type. Because we are interested in developing a shared latent space for force and language, the latent space vector is the same dimensions across all autoencoders.}
%     \label{tab:autoencoder_layers}
% \end{figure*}

Our framework (see Fig. \ref{fig:architecture_level_1}) employs two autoencoders \cite{kramer1991nonlinear} —one for force profiles and one for phrases—to map inputs from distinct modalities into a shared latent space. Each autoencoder consists of an encoder network that compresses its input into a fixed-size latent vector (the “bottleneck” representation) and a decoder network that reconstructs the original input from this latent representation.

% Autoencoders function as a form of nonlinear principal component analysis \cite{kramer1991nonlinear}. 

During training, the encoder learns to capture the essential latent features of the input data, while the decoder learns to reconstruct the input from these latent variables. This unsupervised learning process enables the network to extract a compact representation that generalizes well to novel inputs sharing similar underlying structure.
In our dual autoencoder architecture, both the force and phrase encoders are designed to output latent vectors of the same dimension, ensuring compatibility in the shared cross-modal latent space. This design choice allows us to perform bidirectional translation between force profiles and phrases by using the decoder of one modality on the latent representation produced by the encoder of the other.

Online Appendix \protect\footnotemark details all the specific layers and activation functions and hyperparameters used for each modality. In all cases, layers consist of a linear transformation (i.e., a matrix multiplication) followed by the application of a nonlinear activation function; we use the rectified linear unit (ReLU) for this purpose.

\footnotetext{Online Appendix with framework details: \url{https://shared-language-force-embedding.github.io/framework/}}
% Our model consists of two autoencoders: one for encoding and decoding force profiles and the other for encoding and decoding phrases. Both autoencoders map their respective inputs to the cross-modal latent space.

% Autoencoders are neural networks made up of encoding and decoding layers (referred to as the encoder and decoder respectively going forward) and a "bottleneck" layer in between. The "bottleneck" layer is not a layer that performs operations but instead simply the output of the encoder and input to the decoder. It is referred to as such because it is supposed to be a lower dimensional vector representation of the original vector input to the autoencoder.

% \cite{kramer1991nonlinear} describes this type of network as a nonlinear principal component analysis because during training the encoder learns "hidden" or "latent" variables that describe the original input. These latent variables end up acting as a compressed version of the input. The decoder also learns how to effectively reconstruct the original input using this latent vector alone.

% Autoencoders are a type of unsupervised learning. The network learns a compact representation of the data that captures significant features. This allows the autoencoder to generalize to new, unseen inputs that share similar underlying structure to inputs it had seen during training. We also have control over how many latent variables are learned by controlling the size of the "bottleneck" layer, i.e. the latent space vector. This makes autoencoders an appropriate choice for the task of learning a shared latent representation of force and language because we can have each respective encoder output a latent space vector that is the same size even if the inputs of each modality are different.

% We want each autoencoder to focus on learning significant latent features of their respective modality, which is why we have 2 total for force profiles and language. \ref{tab:autoencoder_layers} showcases the different layers and activation functions used for each of the modalities, including the differences between using the binary phrase vector and embedding vector representations of phrases. A layer is just a matrix that is multiplied to an input vector, and an activation is a nonlinear function applied component-wise to an input vector. We only used the rectified linear unit (ReLU) as our nonlinear activation function, which is defined as $f(x)=\max(0,x)$.

\subsection{Multitask Learning}
\label{sec:Architecture:Multitask Learning}

To encourage the model to learn a robust cross-modality latent representation, we adopt a multitask learning strategy that jointly optimizes three related objectives: (1) \emph{reconstruction} of the original inputs from their latent representations, (2) \emph{contrastive} alignment of corresponding force profile and phrase embeddings in the shared latent space, and (3) \emph{translation} between modalities by decoding a latent embedding obtained from one modality into the other. Joint training with these tasks compels the model to extract meaningful latent features that generalize well across modalities while mitigating over-fitting.

Typically, a model is trained by minimizing a single loss function; however, by incorporating multiple loss functions corresponding to related tasks \cite{caruana1997multitask}, our network is guided to form a representation that simultaneously serves several objectives. The overall loss function is a weighted sum of the individual losses:

\begin{equation}
\label{eq:total_loss}
    \mathcal{L}=k_r\mathcal{L}_r + k_z\mathcal{L}_z+k_t\mathcal{L}_t,
\end{equation}

where hyperparameters $k_r,k_z,k_t$ control the relative importance of the reconstruction loss $\mathcal{L}_r$, contrastive loss $\mathcal{L}_c$, and translation loss $\mathcal{L}_t$, respectively. In our experiments, these constants are all set to 1, indicating equal weighting for each task. The loss functions are defined as:

% We employed a multitask learning strategy during model training. These tasks include reconstructing inputs after encoding them into the shared latent space back to their original modality, representing corresponding force profile and phrase inputs similarly in the shared latent space, and translating inputs across modalities by decoding the shared latent space embedding of their corresponding input in the other modality into their own modality.

% Normally, models are trained to minimize a single loss function, which can be thought as the task it is learning to perform well. The lower the loss, the better they perform at that task, and vice versa. \cite{caruana1997multitask} explains how including additional loss functions corresponding to related tasks may improve generalization because the model is coerced to develop a representation of inputs that can better generalize to performing well on the multiple tasks. Since the tasks are related, knowledge used to perform well in one task can help learn the other tasks better.

% An additional benefit of multitask learning is it prevents a model from overfitting to the data it has seen during training. Since it must perform well at multiple tasks, the model must learn an effective representation that can generalize to performing well at multiple tasks.

% Multitask learning is straightforward to implement. Each task comes with its corresponding loss function. These loss functions become terms in a total loss function:

% \begin{equation} \label{eq:total loss}
%     \mathcal{L}=k_r\cdot\mathcal{L}_r+k_z\cdot\mathcal{L}_z+k_t\cdot\mathcal{L}_t
% \end{equation}

% The model then runs backpropagation using this composite loss function. The $k$ constants are hyperparameters that allow us to change the relative weighting of each task, essentially controlling which task(s) the model may prioritize to perform well in during training. In practice, these are all set to $1$ which means the model equally prioritizes all tasks.

% The $\mathcal{L}$ terms are the specific loss functions that will be elaborated on in the following sections. $\mathcal{L}_r$ refers to the reconstruction loss function (\ref{sec:Architecture:Multitask Learning:Reconstruction Loss}), meaning how well can the model encode a particular input into the shared latent space and reconstruct that original input using just the shared latent space embedding. $\mathcal{L}_z$ refers to the contrastive loss function (\ref{sec:Architecture:Multitask Learning:Contrastive Learning}), meaning how well can the model correlate two corresponding force profile and phrase inputs. $\mathcal{L}_t$ refers to the translation loss function (\ref{sec:Architecture:Multitask Learning:Translation Loss}), meaning given a corresponding pair of force profile and phrase inputs how well can the model encode a particular input into the shared latent space and using that shared latent space vector decode it into the modality of the other input and in turn reconstruct the other input.

% These tasks are related because they all involve the model learning relevant latent variables of each modality. They all on their own encourage the model to discover meaningful hidden features in the inputs. Combining them all through multitask learning further encourages the model to leverage the knowledge between tasks to construct a meaningful shared embedding space of force and language.

\subsubsection{Reconstruction Loss ($\mathcal{L}_r$) }
\label{sec:Architecture:MultitaskLearning:ReconstructionLoss}

It measures how accurately each autoencoder (force and phrase) reproduces its own input from the latent vector. For force profiles, we use mean squared error; for phrases, the reconstruction metric depends on the chosen representation, with cross-entropy for binary phrase vectors and mean squared error for GloVe embeddings.

% Autoencoders learn to compress inputs into a lower dimensional latent space. As described in \cite{kramer1991nonlinear}, they learn this compression by being tasked to identity map the input. This means the input is being mapped to itself, in other words the autoencoder has to reconstruct the input after passing its features through the "bottleneck" layer.

% This means we need to compute a reconstruction error for each modality to measure how closely the reconstruction resembles the original input. For force profiles, we use standard mean squared error, which computes a squared error between samples of the same time of the original and reconstruct inputs and finds the mean of these squared errors.

% We have two different ways of measuring reconstruction error for phrases since we have two ways of representing them. For a binary phrase vector, we split it into its direction and modifier logits vectors. We apply standard softmax on each to get 2 probability distributions, one for modifier classes and another for direction classes. We then use standard cross entropy loss on each probability distribution to penalize distributions that do not favor the correct class.

% GloVe (\cite{pennington2014glove}) embeddings exist in a continuous space, allowing us to simply perform a squared distance error on the whole reconstructed phrase embedding vector. We do not need to split the vector into its 3 word embedding parts because the squared distance function already works component wise and sums up the component wise distances. This is equivalent to splitting the phrase embedding vector into its 3 word embedding parts, performing squared distance errors on each and summing each of these errors.

% More formally, for a force profile input $x_f$ and force profile encoder and decoder functions $E_f$ and $D_f$ respectively:

% \begin{equation}
%     \mathcal{L}_{r,\text{single force profile}}(x_f)=\frac{||x_f-D_f(E_f(x_f))||^2}{\text{len}(x_f)}
% \end{equation}

% $\text{len}(\cdot)$ refers to the length of the vector, the number of elements, the number of components.

% For a phrase GloVe embedding (\cite{pennington2014glove}) input $x_{p,\text{G}}$ and phrase GloVe embedding encoder and decoder functions $E_{p,\text{G}}$ and $D_{p,\textbf{G}}$ respectively:

% \begin{equation}
%     \mathcal{L}_{r,\text{single phrase}}(x_{p,\text{G}})=||x_{p,\text{G}}-D_{p,\text{G}}(E_{p,\text{G}}(x_{p,\text{G}}))||^2
% \end{equation}

% $\mathcal{L}_r=$ either $\mathcal{L}_{r,f}(x_f)$ or $\mathcal{L}_{r,p}(x_p)$ depending on context, whether the model is trying to reconstruct a force profile or a phrase.

\subsubsection{Contrastive Loss ($\mathcal{L}_c$)}
\label{sec:Architecture:MultitaskLearning:ContrastiveLearning}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/contrastive_loss.pdf}
    \caption{Conceptual illustration of the contrastive loss function. Given a batch of corresponding force profiles and phrases, a distance metric is measured for each pair across modalities. This results in a distance metric matrix. The diagonals refer to the distance metrics of corresponding force profiles and phrases, i.e. positive pairs. Off-diagonal elements refer to negative pairs. The contrastive loss function encourages the distances of positive pairs to be minimized. It also penalizes negative pairs if they start to get too close to each other.}
    \label{fig:contrastive_loss}
\end{figure}

To align the force and phrase embeddings, we employ a contrastive loss \cite{hadsell2006dimensionality} that brings paired (corresponding) latent vectors closer together while pushing apart unpaired (non-corresponding) vectors. This ensures that shared features of matching force and language inputs are learned and represented similarly in the latent space.

% \cite{hadsell2006dimensionality} proposed a distance based loss function on pairs of the lower dimensional representation of inputs. A pair of inputs labeled as similar should be closer to each other, and a pair of inputs labeled as dissimilar should be further away, otherwise the model that encodes them into the lower dimensional space is penalized. They showed the resultant latent space was indeed structured in a way such that similar inputs were mapped to similar regions.

% \cite{radford2021learning} extended on this by applying it to multimodal inputs. They focused on correlating pairs of image and text inputs as closely as possible in their shared latent space. Since their dataset had a textual description for each description, a one-to-one correspondence, this automatically gave them a way to label pairs as similar or dissimilar. They showed their new CLIP (\cite{radford2021learning}) embedding was capable of zero-shot prediction on images or text the model had not seen before, which meant it had learned an effective shared representation of the two modalities.

% We apply the same concepts to our task of bridging force and language. Similar to CLIP (\cite{radford2021learning}), we performed contrastive learning on our own multimodal data. Since each force profile had an associated phrase, that meant we had a way to identify positive pairs (similar pairs) and negative pairs (dissimilar pairs).

% During training, the model would encounter a batch of pairs of force profile and phrase. Using the respective encoder for each modality input, it would encode them into the shared latent space. Pairwise squared distances of the resultant shared latent space vectors are computed\footnote{Note these pairwise squared distances are for every pair of force profile and phrase shared latent space vectors in the batch. Squared distances were not computed for shared latent space vectors that originated from the same modality. Similar inputs of the same modality should in theory already be encoded similarly in the shared embedding space because of the reconstruction loss (\ref{sec:Architecture:Multitask Learning:Reconstruction Loss})}. This results in a matrix of distances, \ref{fig:contrastive_loss} demonstrates this.

% The diagonals refer to the squared distances of corresponding positive pairs, while the other elements refer to the squared distances of negative pairs. The contrastive loss function sums up the diagonals to get an aggregate score for relating positive pairs, which encourages the model to minimize the distances between positive pairs. Ideally this score should be as close to $0$ as possible as that would mean the model is correctly correlating corresponding force profile and phrase inputs in the shared latent space. The off diagonal elements are summed and negated, which encourages the model to maximize the distances of negative pairs.

% For a single pair of force profile and phrase embeddings ($z_f$, $z_p$ respectively) the contrastive loss is:

% \begin{equation}
%     \mathcal{L}_{z,\text{single pair}}(z_f,z_p)=Y(z_f,z_p)\cdot||z_f-z_p||^2-(1-Y(z_f,z_p))\cdot\max(0, m-||z_f-z_p||^2)
% \end{equation}

% $Y(z_f,z_p)=1$ if they are a positive pair and $0$ if they are a negative pair, and $m$ is a hyperparameter that determines the margin at which to penalize negative pairs that are too close to each other. \cite{hadsell2006dimensionality} included this augmentation to only penalize negatives pairs if they fall within a radius of each other. We set ours to the value $1.0$.

% Given a batch force profile and phrase embedding ($Z_f$, $Z_p$ respectively), each size $n$, the total contrastive loss is:

% \begin{equation}
%     \mathcal{L}_z=\sum_{i=1}^{n}\sum_{j=1}^{n}\mathcal{L}_{z,\text{single pair}}(Z_f[i],Z_p[i])
% \end{equation}

\subsubsection{Translation Loss ($\mathcal{L}_t$)}
\label{sec:Architecture:MultitaskLearning:TranslationLoss}

Finally, we measure how well the model translates between modalities. Given a force profile and its paired phrase, we encode one and decode into the other, then compare the result to the corresponding ground truth. This cross-decoding step drives the model to capture modality-agnostic features in the shared embedding, facilitating natural force-to-language and language-to-force translation.

% The goal is to create a shared embedding of force and language to enable a robot to go between the two modalities. In other words, given an input from one modality, the model should be able to recreate the corresponding input of the other modality. This would involve encoding the input into the shared latent space and decoding into the other modality. This process is referred to as translation because the model is translating between modalities.

% We use the same reconstruction measures described in \ref{sec:Architecture:Multitask Learning:Reconstruction Loss} to determine given some input how well a cross reconstructed input resembles the corresponding input.

% More formally, given corresponding force profile and phrase inputs $x_f$ and $x_p$ respectively, and force profile and phrase encoders and decoders $E_f$, $D_f$, $E_p$, $D_p$:

% \begin{equation}
%      \mathcal{L}_{t}(x_f,x_p)=\text{err}(x_f,D_f(E_p(x_p)))+\text{err}(x_p,D_p(E_f(x_f)))
% \end{equation}

% Where $\text{err}(\cdot,\cdot)$ simply is the appropriate reconstruction error metric like the ones described in $\ref{sec:Architecture:Multitask Learning:Reconstruction Loss}$.

% This loss function encourages the model to learn latent features of each modality that are relevant when needed to translate between them.


\begin{figure*}
    \centering

    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/phrase_to_force_human_response_example.png}
        \caption{Example Phrase-to-Force trial: A participant interprets the phrase 'smoothly down forward' by guiding the robot arm while force data is recorded.}
        \label{fig:phrase_to_force_example}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/force_to_phrase_human_response_example.png}
        \caption{Example Force-to-Phrase trial: The robot executes a predefined force profile, and the participant describes the felt motion as 'sharply backward right'}
        \label{fig:force_to_phrase_example}
    \end{subfigure}

    \caption{Demonstration of bidirectional force-language translation through human trials. }
    \label{fig:data_collection_procedure_trial_examples}
\end{figure*}


\subsection{Data Collection}
\label{sec:Architecture:Data Collection}

% To train and evaluate the shared force-language embedding, a dataset was collected from 10 participants using a UR robotic manipulator. The user study procedures and protocol were reviewed and approved by the Institutional Review Board (IRB Protocol \#2212000845R001), ensuring we followed ethical guidelines for human subject research.  Each participant performed two distinct procedures, one translating from language to force (\emph{phrase-to-force}) and the other from force to language (\emph{force-to-phrase}). Both procedures used the same reference frame as introduced in Section~\ref{sec:Preliminaries:CoordinateSystem}, where the $+x$ axis corresponds to the participant’s right, $+y$ is forward, and $+z$ is upward.

To train and evaluate the cross force-language embedding, we collected data from 10 participants using a UR robotic manipulator. The study was approved by the Institutional Review Board (IRB Protocol \#2212000845R001) to ensure ethical human subject research guidelines were followed. Each participant completed two procedures: phrase-to-force translation and force-to-phrase translation. Both procedures used a consistent reference frame as in (Section~\ref{sec:Preliminaries:CoordinateSystem}).

% In order to train our model to learn a shared embedding of force and language, we needed data of corresponding force profiles and phrases. We had 10 participants come in and perform 2 separate procedures with a UR10 robot. Each procedure had participants translate from one modality to another.

% The reference frame of force output and direction words were all in the perspective of the participants. In other words, it was orientated such that the $+x$ direction would point to their right, the $+y$ direction would point in front of them, and the $+z$ direction would point upward exactly as illustrated in \ref{tab:direction_axis_mapping}.


\begin{table}
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Trial Number} & \textbf{Phrases Provided to User 1} \\
        \hline
        0 & left \\
        1 & forward \\
        2 & up \\
        3 & right \\
        4 & down \\
        5 & backward \\
        6 & forward and down \\
        7 & backward quickly \\
        8 & smoothly right \\
        9 & quickly right and down \\
        $\cdots$ & $\cdots$ \\
        40 & forward and left sharply \\
        41 & up smoothly \\
        \hline
    \end{tabular}
    \caption{List of phrases provided to a user participant during their Phrase-to-Force procedure. Note the first six phrases across all users were a random ordering of the basic directions. Afterward, we provide a random phrase with varying phrase composition structure.}
    \label{tab:phrase_to_force_provided_phrases_example}
\end{table}

% In the phrase-to-force procedure, participants were sequentially presented with 42 distinct phrases. Each phrase was randomly generated from the vocabulary described in Section~\ref{sec:Preliminaries:Language} by uniformly sampling direction word with an optional modifier word. Table~\ref{tab:phrase_to_force_provided_phrases_example} illustrates an example sequence of prompts.

% After hearing each prompt, the participant gripped the end-effector of the robot arm and enacted the motion that they believed best matched the phrase. The system recorded the time-varying force data throughout the participant’s demonstration, yielding a force profile corresponding to each textual description. 

\subsubsection{Phrase-to-Force}
\label{sec:Architecture:Data Collection:Phrase-to-Force}

Participants were sequentially presented with 42 distinct phrases (Table ~\ref{tab:phrase_to_force_provided_phrases_example} illustrates an example sequence of prompts.). Each phrase was generated by randomly combining a direction word (e.g., ``left", ``forward-down") with an optional modifier word (e.g., ``quickly", ``smoothly") from the vocabulary described in Section ~\ref{sec:Preliminaries:Language}. The first six phrases were always basic directions, while the remaining 36 incorporated modifiers and compound directions. For each phrase, participants gripped the robot's end-effector and demonstrated what they felt was an appropriate motion matching the description. The system recorded force measurements for each demonstration, typically lasting 2-4 seconds. A user trial demonstrating this is shown in Figure ~\ref{fig:phrase_to_force_example}.


% This procedure collected data of human participants translating language to force. Each user was provided with 42 phrases, one at a time. The first 6 phrases were always a random ordering of the basic directions. Afterward, random phrases were provided. \ref{tab:phrase_to_force_provided_phrases_example} gives an example to a set of phrases given to a particular user.

% For each provided phrase, they were asked to enact the motion that they felt best corresponded with the phrase. To capture the force data, we had users grab the end of a UR10 robot arm which housed a force sensor. Users were instructed to drag the end of the robot arm which would follow them so that we could continuously record the force exerted by them.

% \ref{fig:phrase_to_force_example} shows an individual trial end-to-end starting with the provided phrase and ending with a force profile from the participant.

\subsubsection{Force-to-Phrase}
\label{sec:Architecture:Data Collection:Force-to-Phrase}

% In the force-to-phrase procedure, participants began by gripping the UR10 end-effector while the manipulator executed a randomly generated force trajectory in the participant’s reference frame. Each motion included a randomized combination of direction, magnitude, and duration. After sensing the motion, participants selected a phrase from the same vocabulary (Section~\ref{sec:Preliminaries:Language}) that, in their judgment, best described the experienced force profile. Figure~\ref{fig:force_to_phrase_example} illustrates an example trial. By pairing each randomly generated force trajectory with the participant’s chosen text label, a complementary \emph{force-to-phrase} dataset was obtained.

In this procedure, participants gripped the robot's end-effector while the manipulator executed randomly generated force trajectories. Each motion was created by combining three randomized parameters: a primary direction vector, a force magnitude between 0.5 and 15 Newtons, and a duration ranging from 1 to 4 seconds. After experiencing each motion, participants constructed descriptive phrases using the same vocabulary (Section ~\ref{sec:Preliminaries:Language}) from the phrase-to-force procedure - selecting direction and modifier words they felt best characterized the force profile they had just experienced. A user participant trial demonstrating this is shown in Fig. ~\ref{fig:force_to_phrase_example}.
Complete details on the dataset with all 10 participants are in appendix \protect\footnotemark.

% The dataset for \emph{phrase-to-force} and \emph{force-to-phrase} with all the combination of phrases from the vocabulary presented to 10 users is shown in our Online Appendix \protect\footnotemark.

% This bidirectional data collection approach provided paired force-language samples capturing human intuition about force-motion descriptions. 

\footnotetext{Online Appendix showcasing the data collected from 10 user participants for phrase-to-force and force-to-phrase \url{https://shared-language-force-embedding.github.io/data-collection/}}

% This procedure collected data of human participants translating force to language. Users were asked to grip the end of the UR10 robot. A force profile was then randomly generated for the end of the robot arm to follow. Users would feel this motion and then report using the vocabulary (\ref{tab:vocabulary}) the phrase they felt best corresponded with the felt force.

% The first 6 motions were always the basic directions with a predefined force output, and afterward motions were randomly generated.

% \ref{fig:force_to_phrase_example} shows an individual trial end-to-end starting with the provided force profile and ending with a phrase from the participant.

\section{Evaluation}
\label{sec:Evaluation}

Our experiments aim to address three key research questions:

\begin{enumerate} \item \textbf{Force--Language Translation Performance.}
How effectively does the dual autoencoder architecture translate force profiles into phrases and vice versa? This evaluates whether the shared latent space successfully aligns semantically similar force and language inputs.

\item \textbf{Generalization to Unseen Examples.}  
Can the model generalize to force profiles and phrases outside its training distribution? This assesses whether the learned shared representation captures essential features that extend beyond the training data.

\item \textbf{Impact of Phrase Representation.}  
How does the choice of phrase representation affect model performance? Specifically, do richer semantic embeddings (via pretrained GloVe embeddings) enhance the model’s ability to associate forces with language compared to binary phrase vectors from the Minimal Viable Vocabulary?

\end{enumerate}

To investigate these questions, we evaluate two variants of our dual autoencoder (DAE) framework. The first variant, denoted $DAE_B$, utilizes the binary phrase vector representation of phrases (described in Section ~\ref{sec:Preliminaries:Language:GloVe}). The second variant, $DAE_G$, employs the GloVe embedding representation (described in Section ~\ref{sec:Preliminaries:Language:GloVe}. 
By comparing results across these two models, we assess the influence of language embedding choices on overall performance and generalization.

% Our experiments seek to answer the following questions:\\

% \begin{enumerate}
%     \item Does the dual autoencoder model perform well at translating force profiles to phrases and vice versa?\\

%     This tests the hypothesis that the architecture is capable of learning a shared latent space that represents similar inputs within and across modalities similarly.\\
%     \item Can the dual autoencoder model generalize well to out of distribution examples?\\

%     This tests the hypothesis that the architecture is capable of learning a  shared latent space expressive enough to generalize to force profiles and phrases it has not seen during training.\\
%     \item What impact does the choice of phrase vector representation have on the performance of the dual autoencoder architecture?\\

%     This tests the hypothesis that including the semantics of words via GloVe embeddings (\cite{pennington2014glove}) provides greater semantic context the architecture can leverage to better associate force profiles and phrases.\\
% \end{enumerate}

% Two versions of the architecture underwent evaluation. The first one had the binary binary phrase vector representation of phrases as one of its inputs. We refer to this one as $DAE_B$. The second one had the GloVe embedding (\cite{pennington2014glove}) representation of phrases as one of its inputs. We refer to this one as $DAE_G$. Both received the flat representation of force profiles (\ref{sec:Architecture:Model:Force Profile Input}) as the other input.

\subsection{Baseline Models}
\label{sec:Evaluation:Baseline Models}

% To benchmark the performance of our $DAE$ approaches, we compare against three baseline methods: Support Vector Machines (SVM) \& K-Nearest Neighbors (KNN) , variants of Multilayer Perceptron (MLP) $DMLP_B$, and $DMLP_G$. These baselines provide simpler, primarily direct-mapping solutions, allowing us to assess whether the shared-latent-space design of $DAE$ yields superior performance on force--language translation.

% To evaluate the performance of the $DAE$ models, we compared them against 3 baselines models. This allows us to observe their relative performance, meaning if they outperform then in implies it is because their architecture uniquely enables them to perform better at the task.

\subsubsection{$SVM\_KNN$}
\label{sec:Evaluation:Baseline Models:SVM_KNN}

% As a straightforward classification-based baseline, we use Support Vector Machines (SVM) for mapping force signals to phrases and a K-Nearest Neighbors (KNN) lookup for mapping phrases back to force. This method is restricted to our minimal viable vocabulary because SVM is a classification model and cannot predict the continuous embeddings by GloVe.

As a baseline, we use Support Vector Machines (SVM) to map force signals to phrases and K-Nearest Neighbors (KNN) to map phrases back to force profiles. This approach is limited to our Minimal Viable Vocabulary since SVMs cannot generate continuous GloVe embeddings.

Each force profile is reduced to a final impulse vector $I\in \mathbb{R}^3$, computed by integrating force over the fixed duration $T$. We train one SVM for each word slot (direction and modifier). Given the impulse vector, these SVMs predict the most likely direction(s) and modifier.

For the inverse mapping, we treat each phrase as a combination of direction and modifier classes, then apply KNN in the same 3D impulse space to identify the closest training example. Since no full time series is predicted, we approximate the complete impulse profile by linear interpolation from zero impulse at $t=0$ to the predicted final impulse at $t=T$. This simple interpolation scheme serves as a coarse placeholder for the temporal structure of the motion.

\subsubsection{$DMLP_B$}
\label{sec:Evaluation:Baseline Models:MLP_B}

In this baseline, we train two independent Multi Layer Perceptron (MLP) networks without any shared latent space. One MLP maps force profiles from flattened 768-dimensional impulse vectors to 62-dimensional binary phrase vector (Section~\ref{sec:Architecture:Model:Force_Profile_Input}), while the other maps phrases to force. We call this approach $DMLP_B$. Each MLP is trained via a reconstruction objective to directly map from one modality to the other. The training loss corresponds to the standard mean squared error for forces (Section~\ref{sec:Architecture:MultitaskLearning:ReconstructionLoss}) and cross-entropy for binary phrase embeddings. Since there is no shared latent space or cross-modal alignment, the model simply learns direct forward and backward transformations.

% This refers to a dual multilayer perceptron (MLP), in other words a set of 2 MLPs. The first takes in a force profile (\ref{sec:Architecture:Model:Force Profile Input}) and outputs a binary phrase vector (\ref{sec:Architecture:Model:Phrase Input}), and the second vice versa.

% The layers that make up each network can be found in the appendix.

% It does not feature any latent space learning. Each network was trained to only map from force profile to phrase and vice versa, respectively. The loss functions it learned to minimize were phrase and force profile reconstruction loss, respectively (\ref{sec:Architecture:Multitask Learning:Reconstruction Loss})

\subsubsection{$DMLP_G$}
\label{sec:Evaluation:Baseline Models:MLP_G}

$DMLP_G$ is analogous to $DMLP_B$ except it uses the GloVe embedding representation of phrases (Section ~\ref{sec:Preliminaries:Language:GloVe}). Instead of two binary phrase vector outputs, the first MLP maps the force profile to a 150-dimensional concatenation of GloVe word embeddings, and the second MLP performs the inverse mapping. Each MLP is again optimized with a reconstruction-style loss suited to its respective output space. This direct mapping baseline allows us to isolate the effect of introducing richer semantic information via GloVe embeddings, independent of a shared latent representation.

% Similar to $DMLP_B$, except it deals with the GloVe embedding (\cite{pennington2014glove}) representation of phrases instead.

\subsection{Metrics}
\label{sec:Evaluation:Metrics}
We define a set of performance metrics to evaluate the force-language translations. These metrics evaluate both the fidelity of generated force profiles relative to a ground-truth trajectory (\emph{force-to-phrase} translation) and the accuracy of generated phrases relative to a reference text description (\emph{phrase-to-force} translation).

% We devised several performance metrics to benchmark how well each model was at performing certain tasks.

% Recall the pillars that make up motion: the direction, magnitude, and duration (\ref{sec:Preliminaries}). Each performance metric intends to measure how well each of these pillars were preserved after a modal translates from one modality to another.

\begin{enumerate}
    \item \textbf{Force Profile Accuracy (FPAcc)}: For a predicted force profile $\hat{x}_f$ and ground-truth $x_f$, we compute the mean squared error (MSE), averaged across both temporal and spatial axes:

    \begin{equation}
        \text{FPAcc}=\text{MSE}(\hat{x}_f,x_f)
    \end{equation}
    
    A lower value indicates closer alignment with the reference force profile.

    \item \textbf{Force Direction Accuracy (FDAcc)}. For a predicted total impulse $\hat{J}(T)$ and ground-truth $J(T)$ (see eq. \ref{eq:impulse_profile}) we compute the cosine similarity:

    \begin{equation}
        \text{FDAcc}=\frac{\hat{J}(T)\cdot J(T)}{||\hat{J}(T)||\cdot||J(T)||}
    \end{equation}
    
    Value near $+1$ indicate strong directional alignment, whereas near $-1$ signify an opposite direction.

    \item \textbf{Modifier Similarity (ModSim)}: We compare the predicted modifier $\hat{w}_m$ with the ground-truth modifier $w_m$ by embedding each word using SBERT \cite{reimers2019sentence}, producing 768-dimensional vectors. We then compute the cosine similarity:

    \begin{equation}
        \text{ModSim}=\frac{E(\hat{w}_m)\cdot E(w_m)}{||E(\hat{w}_m)||\cdot||E(w_m)||}
    \end{equation}

    where $E$ is the SBERT sentence embedder.

    \item \textbf{Direction Similarity (DirSim)}: As with modifiers, we embed both the predicted direction word(s) $\hat{w}_d$ and ground-truth $w_d$ using SBERT, then compute the cosine similarity of the resulting vectors. If the phrase contains two direction words, we concatenate them into a single short text snippet (e.g., \textit{“left and up”}) before embedding.

    \begin{equation}
        \text{DirSim}=\frac{E(\hat{w}_d)\cdot E(w_d)}{||E(\hat{w}_d)||\cdot||E(w_d)||}
    \end{equation}
    
    High similarity values indicate strong agreement in directional semantics.

    \item \textbf{Full Phrase Similarity (PhraseSim)}: We compute the overall phrase similarity as the average of the Modifier Word Similarity and Direction Word(s) Similarity:

    \begin{equation}
        \text{PhraseSim}=\frac{1}{2}\left(\text{ModSim}+\text{DirSim}\right)
    \end{equation}
\end{enumerate}

\subsection{Experiments}
\label{sec:Evaluation:Experiments}

To evaluate both \emph{in-distribution} and \emph{out-of-distribution} performance, we conduct three main experiments:

\begin{enumerate}
\item \textbf{In-Distribution Evaluation.} We randomly split the dataset into training (90\%) and testing (10\%) subsets. Each model is trained and tested on the same splits, and this process is repeated for 30 independent trials with different random seeds. We then average the performance metrics over these trials to reduce variance, yielding a robust estimate of in-distribution accuracy.

\item \textbf{Out-of-Distribution Modifiers.} To assess the model’s ability to generalize to unseen adverbial cues, we isolate a single modifier (e.g., “\emph{slowly}”). All data points containing this modifier are excluded from the training set but retained in the test set. After training, we measure how effectively each model handles data points that correspond to the held-out modifier. We repeat this procedure for each modifier in the vocabulary and average the results to obtain a \emph{modifier-level} generalization score.

\item \textbf{Out-of-Distribution Directions.} Similarly, we evaluate direction-level generalization by holding out each direction (e.g., ``\emph{up}") from the training set. The model is then tested on data points where this direction appears. Repeating this protocol for all direction words and averaging the results provides a \emph{direction-level} generalization score.
\end{enumerate}





% \paragraph{Synthesis of Findings}
% Collectively, these results provide answers to our main research questions:

% \begin{enumerate}
%     \item \emph{Effectiveness of Dual Autoencoder.}  
%     Yes. $\text{DAE}$ approaches consistently outperform direct-mapping MLPs and $\text{SVM\_KNN}$ at cross-modal translation—both in-distribution and under OOD conditions—showing that a shared latent space indeed fosters stronger force--language alignment.

%     \item \emph{Generalization to Unseen Examples.}  
%     The dual autoencoders exhibit superior OOD performance compared to other baselines, though the choice of representation ($_B$ vs.\ $_G$) leads to nuanced differences: the $_G$ models often yield more accurate force trajectories (i.e., $\text{FPAcc}$), while the $_B$ models better maintain or reproduce the precise textual wording and direction components ($\text{DirSim}$, $\text{PhraseSim}$).

%     \item \emph{Impact of Phrase Representation.}  
%     The binary phrase vector representation ($_B$) tends to excel in in-distribution tasks and yields higher similarity scores for textual outputs. Meanwhile, the GloVe-based approach ($_G$) proves advantageous for force reconstruction, especially in zero-shot scenarios with unseen directions or modifiers. Overall, the choice can be guided by whether preserving textual fidelity or capturing the underlying physical signal is the higher priority.
% \end{enumerate}

% In conclusion, our experiments confirm that (1) \emph{learning a shared latent space benefits force--language translation}, (2) \emph{dual autoencoders can generalize across modifiers and directions not seen in training}, and (3) \emph{the choice of binary or GloVe embeddings influences which aspect of performance—textual precision or physical fidelity—receives the greater boost}.

% \begin{figure*}
%     \centering

%     \begin{subfigure}{0.48\linewidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/phrase_to_force_human_response_example.png}
%         \caption{Example Phrase-to-Force trial: A participant interprets the phrase 'smoothly down forward' by guiding the robot arm while force data is recorded.}
%         \label{fig:phrase_to_force_example}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.48\linewidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/force_to_phrase_human_response_example.png}
%         \caption{Example Force-to-Phrase trial: The robot executes a predefined force profile, and the participant describes the felt motion as 'sharply backward right'}
%         \label{fig:force_to_phrase_example}
%     \end{subfigure}

%     \caption{Demonstration of bidirectional force-language translation through human trials. }
%     \label{fig:data_collection_procedure_trial_examples}
% \end{figure*}

\begin{figure*}
    \centering
    \begin{tabular}{c|c|c}
        \begin{subfigure}{0.3\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/results/classical_ex_x.png}
        \end{subfigure} &
        \begin{subfigure}{0.3\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/results/modifier_ex_x.png}
        \end{subfigure} &
        \begin{subfigure}{0.3\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/results/direction_ex_x.png}
        \end{subfigure} \\
        \begin{subfigure}{0.3\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/results/classical_ex_y.png}
        \end{subfigure} &
        \begin{subfigure}{0.3\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/results/modifier_ex_y.png}
        \end{subfigure} &
        \begin{subfigure}{0.3\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/results/direction_ex_y.png}
        \end{subfigure} \\
        \begin{subfigure}{0.3\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/results/classical_ex_z.png}
            \caption{Force profile predictions for ``quickly up" representing an in-distribution phrase}
        \end{subfigure} &
        \begin{subfigure}{0.3\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/results/modifier_ex_z.png}
            \caption{Force profile predictions for ``greatly forward" testing generalization for the out-of-distribution phrase.}
        \end{subfigure} &
        \begin{subfigure}{0.3\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/results/direction_ex_z.png}
            \caption{Force profile predictions for the out-of-distribution direction ``backward up".}
        \end{subfigure}
    \end{tabular}
    \caption{Visualization of predicted force profiles in X, Y, and Z axes (rows) from our model variants ($DAE_G$ and $DAE_B$) compared to ground truth models ($SVM\_KNN$, $MLP_B$ and $MLP_G$) for experiments in Section \ref{sec:Evaluation:Experiments}. Results for all the possible combination of phrases on each baseline model can be found in our online appendix \protect\footnotemark}
    \label{fig:force_profile_example}
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \small
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Model} & \textbf{Modifier} & \textbf{Direction} \\
            \hline
            Reference & quickly & up \\
            \hline
            SVM\_KNN & - & up \\
            $MLP_B$ & quickly & up \\
            $MLP_G$ & meantime & up \\
            $DAE_B$ & sharply & up \\
            $DAE_G$ & quickly & up \\
            \hline
        \end{tabular}
        \caption{In-distribution phrase predictions.}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \small
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Model} & \textbf{Modifier} & \textbf{Direction} \\
            \hline
            Reference & greatly & forward \\
            \hline
            SVM\_KNN & harshly & forward \\
            $MLP_B$ & - & forward \\
            $MLP_G$ & likewise & forward \\
            $DAE_B$ & significantly & forward \\
            $DAE_G$ & significantly & forward \\
            \hline
        \end{tabular}
        \caption{Out-of-distribution modifier predictions.}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \small
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Model} & \textbf{Direction 1} & \textbf{Direction 2} \\
            \hline
            Reference & backward & up \\
            \hline
            SVM\_KNN & backward & - \\
            $MLP_B$ & backward & - \\
            $MLP_G$ & backward & - \\
            $DAE_B$ & backward & - \\
            $DAE_G$ & backward & - \\
            \hline
        \end{tabular}
        \caption{Out-of-distribution direction predictions.}
    \end{subfigure}
    \caption{Example phrase predictions from each experiment. Phrases are predicted from the corresponding ground truth force profiles shown in Fig. \ref{fig:force_profile_example}}
\end{figure*}

\subsection{Results and Analysis}
\label{sec:Evaluation:Results}



We present our experimental results addressing three research questions (see \ref{sec:Evaluation} for complete descriptions):

% Our evaluation addresses each of the three key evaluation questions (See \ref{sec:Evaluation} for complete descriptions): 

% We present the evaluation outcomes for the three experimental settings: \emph{in-distribution}, \emph{out-of-distribution (OOD) modifiers}, and \emph{out-of-distribution directions}. 
%Recall our original research questions:

\begin{enumerate}
    \item Does the dual autoencoder (DAE) model effectively translate force profiles to phrases and vice versa?
    \item Can the DAE model generalize to out-of-distribution (unseen) examples?
    \item What is the impact of the phrase representation (binary vs.\ GloVe) on performance?
\end{enumerate}

% Our analysis of the experimental results addresses each of these questions:
\begin{enumerate}
    
    \item {\textbf{In-Distribution Results}} When trained and tested on the same distribution of force--phrase pairs, all four neural approaches ($\text{DMLP}_B$, $\text{DMLP}_G$, $\text{DAE}_B$, $\text{DAE}_G$) performed substantially better than $\text{SVM\_KNN}$ in terms of reproducing the force profile ($\text{FPAcc}$) and capturing the aggregate direction ($\text{FDAcc}$). 



    \begin{figure*}
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/results/classical_perf.png}
        \caption{}
        % \caption{Model scores for in-distribution samples, in which each model is trained and tested on the same train-test split. Results are averaged across 30 seeds.}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/results/modifier_perf_all.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/results/direction_perf_all.png}
        \caption{}
    \end{subfigure}
    \caption{Performance comparison across models for: (a) in-distribution testing, (b) out-of-distribution modifiers, and (c) out-of-distribution directions. Higher values indicate better performance, except for FPAcc where lower values are better. See \ref{sec:Evaluation:Metrics} for metric definitions.}
\end{figure*}
% Interestingly, $\text{SVM\_KNN}$ obtained the highest $\text{DirSim}$ score and was competitive in the $\text{ModSim}$ and $\text{PhraseSim}$ categories, suggesting that while it struggles to reproduce full force-profiles, it is effective in classifying phrases that describe force profiles.

    Among neural models, the $\text{DAE}$ variants consistently outperformed their $\text{DMLP}$ counterparts across all metrics, confirming that a shared latent-space approach facilitates more robust force--language mapping. Moreover, $\text{DAE}_B$ achieved the highest $\text{ModSim}$ and $\text{PhraseSim}$ scores, indicating excellent preservation of adverb semantics and overall phrase structure. 

    The dual autoencoder (DAE) variants consistently achieve better performance than their DMLP counterparts across all metrics, demonstrating the benefits of the shared latent space approach. $\text{DAE}_B$ achieved the highest ModSim and PhraseSim scores (0.58 and 0.78 respectively), indicating strong preservation of adverb semantics and overall phrase structure.

    This addresses our first question: \emph{the dual autoencoder does indeed excels at force--language translation under in-distribution conditions}. 
    
    Additionally, $\text{DAE}_B$ and $\text{DMLP}_B$ both outperformed $\text{DAE}_G$ and $\text{DMLP}_G$ on in-distribution data. This suggests that the simpler binary phrase vectors may be sufficiently discriminative—and perhaps easier to learn—when the training distribution covers the same words. This observation partially answers our third question by illustrating that \emph{the binary representation can be advantageous for in-distribution translation}.


    \footnotetext{Online Appendix showcasing the results for all the combinations of phrases for each baseline models  \url{https://shared-language-force-embedding.github.io/results/}}


    \item{\textbf{Out-of-Distribution Modifiers}} For unseen modifiers, $\text{DAE}_G$ achieves the best force profile reconstruction (FPAcc: 5.41) while $\text{DAE}_B$ maintains the highest phrase similarity scores (PhraseSim: 0.68). This indicates that the DAE models retain semantic representation of adverbs even if they never explicitly see them during training.
% $\text{DAE}_B$ performed best in $\text{ModSim}$ and $\text{PhraseSim}$, indicating that it still captures directional and adverbial semantics more robustly despite not having encountered the modifier word before. $\text{SVM\_KNN}$ performed poorly in all but $\text{DirSim}$, highlighting its limited capability to extrapolate beyond directions it explicitly “memorized.”
    We observe that the DAE models generalize more effectively to unseen modifiers than MLPs. GloVe embeddings helps in reconstructing the force trajectory ($\text{FPAcc}$), but the binary phrase vector representation still achieves strong direction and phrase-level fidelity. This  addresses our second question: \emph{the dual autoencoders can indeed handle certain unseen modifiers better than simpler baselines}, although each representation has trade-offs.

    \item{\textbf{Out-of-Distribution Directions}} Both $\text{DAE}_G$ and $\text{DMLP}_G$ show superior force profile reconstruction (FPAcc) and directional accuracy (FDAcc) compared to binary-based models for unseen directions, suggesting that GloVe’s continuous semantics facilitate better extrapolation of physical orientation and overall impulse direction for unseen directional words.

    Conversely, the binary-vector based models achieved higher scores in $\text{ModSim}$, $\text{DirSim}$ and $\text{PhraseSim}$, indicating that although $\text{DAE}_G$ and $\text{DMLP}_G$ produce more faithful \emph{physical} force reconstructions, their textual outputs are less aligned with the precise wording of the missing direction.
    This  addresses our third question: \emph{the GloVe representation demonstrates stronger zero-shot direction encoding on the force side, but experiences a penalty in reproducing the textual labels for directions.}
\end{enumerate}


\textbf{Key Findings}
These results demonstrate that: 
\begin{itemize}
    \item  The dual autoencoder architecture effectively enables bidirectional translation, with DAE variants showing 20-30\% improvement over baselines; 
    \item Models can generalize to unseen inputs, with GloVe variants showing particular strength (\>25\% improvement) in force reconstruction; 
    \item Representation choice presents a trade-off between force reconstruction accuracy (favored by GloVe) and linguistic precision (favored by binary encoding). The choice between binary phrase vectors and GloVe embeddings representations should be guided by application requirements: GloVe for better force reconstruction and generalization, or binary phrase vector representation for precise linguistic mapping and in-distribution performance.
\end{itemize}

\section{Limitations and Discussion}
\label{sec:Limitations}

% Although, our framework demonstrated effective force to language mapping, few design choices and limitations must be discussed.

\textbf{Multimodal Translationals} Existing multimodal translation frameworks, such as image-to-text or video-to-text systems, can’t be simply adapted to force-language mapping. Force signals have unique temporal characteristics and physics based constraints that is lacking in image data. Additionally, force profile data exhibits high user variability - the same verbal description (e.g., ``gently left”) could correspond to remarkably different force profiles across users. While vision language models could handle spatial relationships, they do lack mechanisms for modeling the temporal dynamics and physical constraints inherent in force-based interactions subjective to the different users. This limitation motivated our development of a specialized framework for force-language mapping rather than adapting existing architectures.

\textbf{Shared Representation} Direct mappings between forces and language through classification based methods or rule-based engines fail to capture the rich force-linguistic relationships. 
% The shared representation through a unified latent space would enable bidirectional translation where similar forces and their linguistic descriptions can be mapped close together while preserving temporal dynamics. 
The learned embedding space supports generalization to novel combinations through semantic patterns. For example, after training on ``gently left" and ``quickly up", the framework generates appropriate descriptions like ``slightly up" for a previously unseen slow upward force by leveraging the learned semantic structure.

\textbf{Architectural Simplicity vs Complexity} While alternative architectures like transformers \cite{vaswani2017attention} and graph networks \cite{kipf2016semi} exist, we chose a basic encoder-decoder design to establish clear baselines for the fundamental force-language mapping challenge. This architecture sufficiently demonstrates the core concept while prioritizing interpretability and data efficiency. Our framework is architecture agnostic. Our main motivation is to establish force-language mappings rather than propose a novel architecture thereby allowing future work to easily substitute more advanced architectures.

\textbf{Force Profile Data Scope} We focus on simple hand motions for data collection across two user studies with 10 participants: one where participants demonstrated force profiles for given phrases, and another where they describe in simple phrases to the forces they observed. This choice of basic motions serves multiple purposes: a) they directly capture human intention while minimizing confounding variables and representing fundamental primitives used in day to day tasks, and a) enable controlled data collection. 

% \textbf{Language Structure Scope} We decomposed our language structure into \textit{\<direction\>} and \textit{\<modifier\>} elements (e.g., ``gently left"), mapping naturally to the physical properties of force profiles - \textit{\<magnitude\>}, \textit{\<direction\>}, and \textit{\<duration\>} in XYZ space. While this structured decomposition enabled a comprehensible force-language associations, it does represent a simplified subset of possible force descriptions. It may not capture the full richness of natural language force descriptions. This could be improved to richer linguistic structures in future work.


\textbf{Language Structure Scope} We decomposed our language structure into \textit{\textless direction\textgreater} and \textit{\textless modifier\textgreater} elements (e.g., ``gently left"), mapping naturally to the physical properties of force profiles - \textit{\textless magnitude\textgreater}, \textit{\textless direction\textgreater}, and \textit{\textless duration\textgreater} in XYZ space. While this structured decomposition enabled a force-language associations, it does represent a simplified subset of possible force descriptions. It may not capture the full richness of natural language force descriptions. This could be improved to richer linguistic structures in future work.


% Regardless of these limitations, our framework provides a foundational step toward natural force-based human-robot communication, establishing baseline capabilities that future work can build upon.

\section{Conclusion}
\label{sec:Conclusion}
We presented a framework that embeds physical force profiles and verbal descriptions in a shared latent space, enabling natural bidirectional translation between how humans describe forces in language and how robots produces them. Our dual autoencoder architecture outperformed baseline approaches by 20-30\% across key metrics through its unified representation of these distinct modalities. While GloVe word embeddings showed superior force reconstruction, binary embeded encodings achieved better linguistic precision, highlighting the inherent trade-offs in representing force-language relationships.
The framework demonstrated robust generalization to unseen force profiles and phrases, validating its potential for real-world human-robot interaction tasks. This work provides a foundation for more intuitive physical human-robot collaboration, particularly in applications like rehabilitation therapy where coordinated force control and verbal communication are essential.
%% Use plainnat to work nicely with natbib. 

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}