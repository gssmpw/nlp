
\section{Background}
\label{background}

\fakeparagraph{Large Language Models and Alignment} LLMs are deep neural networks designed to model the probability distribution of text sequences. One particular training objective is next-token prediction, where, given a sequence of tokens, the model will predict the most likely token to appear next. 
Broadly, most models have two objectives: the primary training phase focuses on \textit{generation} of plausible and meaningful text, while the second objective, \textit{alignment}, aims to make sure that the generated text is ethical and aligns with user-intended goals~\citep{ouyang2022traininglanguagemodelsfollow, wei2022finetunedlanguagemodelszeroshot}.
Like other neural-network-based models, these generative systems are susceptible to adversarial attacks, where carefully crafted inputs exploit model vulnerabilities to produce incorrect or unintended outputs~\citep{szegedy2014intriguingpropertiesneuralnetworks, biggioadversarial}. 
An attack that aims to subvert the model's alignment is called a \textit{jailbreak}~\citep{wallace2021nlpattacks, ebrahimi2018hotflip, jia-liang-2017-adversarial}. 
% \cite{wei2023jailbrokendoesllmsafety} find that even state-of-the-art models exhibit the training deficits that lead to competing objectives, which in turn lead to susceptibility to jailbreaks. 
Initial jailbreaks were crafted manually and largely found on internet forums~\citep{shen2024dan}; nowadays there exists a myriad of algorithmic textual jailbreak methods~\citep{yi2024jailbreakattacksdefenses}. At the time of writing, the state-of-the-art white-box jailbreak method is Greedy Coordinate Gradient~\citep{zou2023gcg}, which iteratively identifies and modifies input tokens to maximize the likelihood of bypassing the model's alignment constraints by leveraging gradient information. This technique permitted the generation of a `universal' or `prompt-agnostic' jailbreak prefix, that is, a prefix which subverts the model's safety constraints no matter which harmful prompt it is pre-pended to. The `greedy' aspect refers to the difficulty of discrete optimization - each update has to select the nearest token representation in a continuous search space. It is difficult to make textual jailbreaks stealthy, as the attack text appears clearly unnatural to a reader.

\fakeparagraph{Vision Language Model Jailbreaks} VLMs process both visual and textual information. Typically, this is achieved by encoding images and text into a shared representation space (which is often the representation space of an underlying language model). However, VLMs open up an attack vector on the underlying aligned LM~\cite{carlini2024alignedneuralnetworksadversarially}. One is able to generate images, e.g.\ using projected gradient descent, that jailbreak-aligned LMs where purely-textual methods fail to do so. This is a specific example of a broader range of soft-embedding attacks. 
\citet{qi2023visualadversarialexamplesjailbreak} generate adversarial visual perturbations in VLMs that constitute a \textit{universal} image jailbreak that can effectively break alignment on any malicious prompt. The authors find a surprising efficacy even in misalignment categories that the image was not explicitly optimized for. To date, this type of universal jailbreak has not been proven to exist in the audio modality.

\fakeparagraph{Attacks on Automatic Speech Recognition Systems} While audio classification systems were shown to be, unsurprisingly, vulnerable to adversarial attacks~\cite{FlowMur}, a more commonly-occurring audio task is Automatic Speech Recognition (ASR). 
Initial works demonstrated untargeted attacks on ASR that reduced the general transcription quality~\citep{gonguntargetedasr, wuuntargetedasr}, with targeted attacks soon to follow: \citet{carlini2018audioadversarialexamplestargeted}~show how to craft adversarial perturbations on an arbitrary audio sample to evoke a transcription of choice; \citet{qin2019imperceptiblerobusttargetedasr}~augment this method to introduce properties desirable in a real-world attack scenario such as robustness to degradation and imperceptibility. 
Nevertheless, ASR systems and ALMs have different architectures and training heuristics. In particular, ASR systems do not combine audio and text, but rather train the model to extract spoken words from the raw signal directly, whereas ALMs perform the embedding projection and cross attention on all audio and text tokens equally for next token prediction. This means that the target task can be arbitrary within the realm of text generation.
% Some popular multimodal systems such as GPT 4o use a middle way: they transcribe speech in audio input to text and combine this with a flexible textual prompt. This permits a conversational storytelling, DAN-style voice jailbreaking approach as demonstrated in \cite{shen2024voicejailbreakattacksgpt4o}, but fundamentally, this is still a discrete optimization.

\fakeparagraph{Practical Audio Attacks} In this work, we explore audio jailbreaks not only as a theoretical failure mode for ALMs, but also as a practical threat with real-world safety and security implications. To account for this, we incorporate ideas about stealth and psychacoustics in audio signals from~\citet{sch√∂nherr2018adversarialattacksautomaticspeech}.
%Although most prominent jailbreaking papers focus on large, state-of-the-art chatbot models such as GPT, Gemini or Claude, generative AI will likely underpin many innocuous specialized applications in the coming years~\citep{FuiHoon}.
There have been many works showing attacks on deployed systems via the audio modality, for example on personal assistants~\citep{commercialattack}, spoken assessment~\citep{rainaassessment}, and speaker verification systems~\citep{speakerverification}. 
Interestingly, select works have shown that it is possible to craft image and audio adversarial examples that can easily be reproduced by humans~\citep{khachaturov2023humanproducible, ahmed2023tubesusanalogattack}, which implies that these attacks could be instantiated in a natural environment. 

\fakeparagraph{Audio Language Model Jailbreaks} At the time of writing, there are very few works on jailbreaks for ALMs. Some approaches vocalize harmful textual responses in the audio~\cite{yang2024audioachillesheelred, shen2024voicejailbreakattacksgpt4o}.
% and make audio jailbreaks by vocalizing harmful requests using text-to-speech systems and perturbing these to make the request unintelligible to humans while remaining intelligible to the target model. This is a per-prompt approach and produces an audio that cannot be identified as a jailbreak but is unnatural. 
A recent work proposes \textit{Best-of-N-Jailbreaking}: a cross-modal per-prompt black-box jailbreak method which works by repeatedly applying random modality-specific augmentations to a harmful request until a harmful response is achieved~\cite{hughes2024bestofnjailbreaking}. 
% It achieves reliable effectiveness but requires a large number of variations (around 10,000) and thus a large number of inferences. It is also a per-prompt approach. 
\citet{kang2024advwavestealthyadversarialjailbreak} use a dual-phase optimization framework, first optimizing discrete latent representations of audio tokens to bypass model safeguards, then refining the corresponding audio waveform while ensuring it remains stealthy and natural through adversarial and retention loss constraints.
% This framework can successfully generate ALM jailbreaks; however, it provides limited insight into how the model processes or interprets the jailbreak.

% Our novel contribution is that proposing a method for generating a prompt-agnostic audio jailbreak which transcends the content of the optimization corpus and captures the notion of toxicity as a whole. We explore the effects of different constraints on this optimization process, together with realistic degradation of the audio, and the resulting jailbreak effectiveness. Most importantly, we share insights regarding the meaning and characteristics of the produced jailbreak audios, which has not been explored thus far.


Our novel contribution is that we first show it is possible to generate a prompt-agnostic audio jailbreak which transcends the content of the optimization corpus and captures the notion of toxicity as a whole. We explore the effects of different constraints on this optimization process, realistic degradation of the audio, and the resulting jailbreak effectiveness. Secondly, we share insights regarding the meaning and characteristics of the produced jailbreak audios, which has not been explored thus far.


% Our work is different to previous works in that it shows it is possible to construct a powerful prompt-agnostic audio jailbreak which transcends the content of the optimization corpus and captures the notion of toxicity as a whole. We ex- plore the effects of different constraints on this optimization

% process, degradation of the audio, and the resulting jailbreak effectiveness. Most importantly, we share insights regarding the meaning and characteristics of the produced jailbreak audios, which has not been explored thus far.