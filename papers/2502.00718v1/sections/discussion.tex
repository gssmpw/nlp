
\section{Conclusion and Future Work}

We have shown that the audio modality can be use to subvert the alignment of a language model, and highlights that unconstrained audio optimization on a few-shot corpus perturbs the base audio to encode a first-person speech snippet containing negative or sinister language. However, aside from this, the language model seems highly sensitive and brittle even in the face of random noise, minimally perturbed (stealthy) jailbreaks and audio with inconspicuous features - and different degradation methods are also not reliable in reducing its effect. The surmise that it would be possible to perform this kind of optimization on any target corpus highlights practical dangers, particularly in for example in AI-powered robotics. 

% These features are different to the textual jailbreaks we have otherwise seen (\cite{zou2023gcg, chao2024jailbreakbench}), and curiously, image jailbreaks have not been shown to include similar features (the equivalent would be the image coming to include written jailbreak text).

\fakeparagraph{Future Work} Our work aims to unveil how language models consume jailbreaks in audio by looking at a representative ALM. It would be interesting to repeat these experiments both with other audio-language models \cite{chu2023qwenaudioadvancinguniversalaudio, alayrac2022flamingovisuallanguagemodel} and other optimization goals to see how this affects features of the produced jailbreak. Unlike audio jailbreaks, image-based jailbreaks do not typically incorporate textual content. Future research could explore whether similar meaningfulness methods in the visual domain lead to textual content emergence. Prior work found that image jailbreaks transfer poorly between models, unlike textual ones \cite{schaeffer2024transferability}; interesting follow up work could look at whether audio jailbreaks then do transfer because they assume textual properties. Additionally, it could be insightful to study how different jailbreak generation methods \cite{ying2024bimodal, shayegani2023jailbreakpieces, ma2024diffusion} or target corpi influence the meaningfulness of generated jailbreaks. Lastly, our stealth experiments suggest an information-theoretic perspective: it is unclear how to find the minimum number of optimized bits or the required $L_\infty$ perturbation size required to encode an instruction capable of triggering a jailbreak.

% This work sheds light on the potential and limitations of the audio modality for subverting alignment of a language model. We show that it is possible to craft a jailbreak audio which is agnostic to the category of misalignment, the specific prompt and even the base audio that it is added to. Our analysis highlights that unconstrained audio optimization on a few-shot corpus perturbs the base audio to encode a textual jailbreak which is frequently a first-person speech snippet containing negative or sinister language. These jailbreaks are different to typical textual jailbreaks we have otherwise seen (\cite{chao2024jailbreakbench}). Notably, random noise itself is already effective as a jailbreak 

\fakeparagraph{Defenses} Our findings suggest that while unconstrained optimization may generate conspicuous transcriptions or labels, relying on textual prompt filtering as a safeguard in transcription is not a reliable way to detect jailbreak audios, as demonstrated by the stealthy jailbreak results. These results also have implications for output filtering in audio synthesis, showing that harmful signals can exist without producing an obvious textual transcription or a clearly identifiable sound classification.

% \fakeparagraph{Future Work}
% Our work aims to unveil how language models consume jailbreaks in different modalities in order to inform defenses for practical deployments. It would be interesting to repeat these experiments both with other audio-language models such as \cite{chu2023qwenaudioadvancinguniversalaudio} and \cite{alayrac2022flamingovisuallanguagemodel} to see whether the results hold, and moreover, to test whether audio jailbreaks are transferable between models (contrary to what we have observed with visual jailbreaks (\cite{schaeffer2024transferability}). It would also be interesting to extend the analysis to other jailbreak generation methods (\cite{ying2024bimodal, shayegani2023jailbreakpieces, ma2024diffusion}) and see whether the optimization method, or indeed the optimization corpus, changes the meaningfulness of the jailbreak produced.  



% An interesting follow-up work could build on our meaningfulness results in order to find a thorough method of identifying an unsafe audio signal without performing inference on a language model.

% \isha{Adversarial images don’t contain text when classified? worth looking into whether audio jailbreaks then do transfer. Conclusion: at the core, these models are extremely brittle. They should not be deployed for safety reasons without guardrails }

% \rob{could we also use such an approach to reliably exaggerate the output of the model, i.e. not necessarily generating toxic output but recommending a more or less extreme course of action etc.?} \isha{the goal can essentially be anything}
