\section{Experimental Design}

\subsection{Threat Models}
\label{section:threat}
%We consider two threat models in our experiments.
\fakeparagraph{Dual Control} The adversary can control the audio \textit{and} the textual input. In this scenario the adversary is using the audio channel to provoke misbehavior that cannot be elicited solely via the textual channel, i.e.\ the audio input essentially `unlocks' the unaligned or toxic output. \textit{Example:} A customer support chatbot for a financial service company integrates speech-to-text and text processing capabilities to assist users with account-related queries. A malicious user enters a question about another user (personal information attack) which the model should, according to it's alignment, not provide. The textual input alone is declined by the model. Accompanied by a jailbreak-optimized audio however, the user is able to elicit the undesirable response. We also consider a scenario where \textbf{stealth} is required. \textit{Example:} Consider the same malicious user having to use the system at a public booth. The adversary can avoid suspicious behavior by using an audio that cannot be identified as malicious.

\fakeparagraph{Single Control} Interaction with the ALM is performed exclusively via an audio channel. The textual system prompt is fixed. This would apply to, e.g.\ call center bots or voice-controlled IoT devices. \textit{Example:} A customer calls a banking hotline with the system prompt: \texttt{You are a helpful but harmless and unopinionated assistant to a bank customer...}. Using a special jailbreak audio input, the customer is able to subvert the model's alignment to produce discriminatory content regarding the plausibility of financial loans for certain demographic groups, which the company may liable for. Again, we also consider the case where the audio input has to be crafted to be \textbf{stealthy}. \textit{Example:} This might occur if the malicious user is trying to evade surveillance/fraud detection.

\subsection{Audio-Language Model}

We run our experiments on the \texttt{SALMON-N 7B} parameter model developed by Tsinghua University and ByteDance~\cite{tang2024salmonn}. \texttt{SALMON-N} consumes audio by extracting both BEATs features (labels such as `Snicker', `Drip' or `Human Sounds') and Whisper features (which are used for speech transcription) from the audio spectogram. These are then combined by dividing the signal into overlapping chunks and fusing them using a Q-former such that these signals are aligned to the language model input space~\cite{kim2024efficientvisuallanguagealignmentqformer}. The textual input is processed and embedded for the same language model, with the audio and textual tokens concatenated with a delimiter. Thus, the model performs the cross-attention mechanism on these concatenated mixed-modality input tokens. The underlying language model is \texttt{Vicuna-7Bv1.5}, a chatbot trained by fine-tuning \texttt{LLaMA-7B} on user-shared conversations with ChatGPT~\cite{vicuna2023}. \texttt{Vicuna} thus mimics the alignment of GPT-4, which is trained using RLHF according to OpenAI's usage guidelines. \texttt{Vicuna} does not incorporate any further input/output filtering to ensure safety and is vulnerable to many jailbreak generation methods~\cite{chao2024jailbreakbench}. We choose to run our experiments on the \texttt{SALMON-N} model due to its open-source nature and its state-of-the-art performance on a range of tasks such as auditory information-based question answering, emotion recognition, and music and audio captioning. Moreover, \texttt{SALMON-N} includes two popular audio feature extractors in a standard configuration with \texttt{Vicuna}. Other ALMs such as Pengi~\cite{deshmukh2024pengiaudiolanguagemodel} or Qwen~\cite{chu2023qwenaudioadvancinguniversalaudio} use a very similar architecture. The dual speech and non-speech feature extractor offers high explainability and lends itself well to our meaningfulness exploration.

% \rob{It feels like you could add a statement about (1) how representative and (2) how good this model is, it might help avoid comments that you need to generate results for many different models, i.e. is this state-of-the-art? why did you choose it etc.? Do your attacks potentially transfer to other models, e.g. Llama or others?}

\subsection{Audio Samples}

We use a selection of base audio files which we optimize to form jailbreaks. These are taken from the \texttt{SALMON-N} repository, each in WAV format, sampled at~\SI{16000}{\hertz}. We provide a brief summary of the characteristics of these audio files in~\Cref{apx:baseaudios}. 

\subsection{Jailbreak Generation}

Our method for generating audio jailbreaks was inspired by~\citet{qi2023visualadversarialexamplesjailbreak}. Given a base audio $x_0$, a target corpus $t = \{t_0..t_n\}$, and a fully differentiable model $f$, we perform gradient descent on $x_0$ to maximize the probability of the output $t$ by minimizing the cross-entropy loss between the predicted distribution and the target outputs:\begin{gather*}
x_{adv} = \arg\min_{x} -\sum_{i=0}^n t_i \log P_f(t_i | x)
\end{gather*}

$\log P_f(t_i | x)$ is the probability of the model generating the target sentence $t$ with the input audio $x$. During optimization we use an empty textual prompt, a deliberate decision to make the audio jailbreak prompt-agnostic. 
% During each step of gradient descent, the audio $x$ is updated as follows:

% \[
% x_{t+1} = x_t - \eta \nabla_x \mathcal{L}(x_t, t)
% \]

% where $\nabla_x \mathcal{L}(x_t, t)$ is the gradient of the cross entropy loss $\mathcal{L} = -\sum_{i=0}^n t_i \log P_f(t_i | x)$ with respect to $x_t$. $\eta$ is the learning rate, controlling the step size; we use $\eta = 0.01$ unless specified otherwise.
As the target corpus $t$, we use a collection of $66$ derogatory sentences directed towards a victim demographic, a victim gender, and the human race in general~\cite{qi2023visualadversarialexamplesjailbreak}. In each epoch, $8$ of these target sentences are optimized on. The fundamental research questions we pose at this stage is \textit{can we optimize any base audio such that, when combined with a harmful textual prompt, it reliably circumvents the model’s alignment mechanisms and elicits toxic output?}

\subsubsection{Stealth}

Some of the scenarios described in~\Cref{section:threat} require us to augment the optimization formula to generate stealthy audio inputs. This raises the question \textit{how does the efficacy of the optimized jailbreak change as we impose stealth constraints?} We investigate three approaches to stealth:

\begin{itemize}
    \item \textbf{Epsilon-Constrained \cite{qi2023visualadversarialexamplesjailbreak}} Here we constrain the absolute change in each audio value, effectively performing bounded gradient descent. In every epoch update, we clip the modified audio such that \begin{gather*}
    \forall i: x_{t+1}[i] = \text{clip}(x_t[i] - \eta \nabla_x \mathcal{L}(x_t, t), \\ x_0[i] - \epsilon, x_0[i] + \epsilon)
    \end{gather*}
    We take $\epsilon \in \{0.1, 0.01, 0.001, 0.0001\}$.
    \item \textbf{Frequency-Hiding \cite{schönherr2018adversarialattacksautomaticspeech}} A normal human hearing range is around 20-20000Hz. However, when adding noise to audio files, it is of course possible to encode information outside of these boundaries. To hide information in specific frequency ranges we use a band-stop filter, removing frequencies between a lower bound $b_l$ and an upper bound $b_u$:
    \begin{gather*}
        \hat{x}[f] = 
        \begin{cases} 
        x[f], & \text{if } f < b_l \text{ or } f > b_u, \\
        0, & \text{if } b_l \leq f \leq b_u,
        \end{cases}
    \end{gather*} where $x[f]$ is the frequency component of the audio at frequency $f$, found using a Fourier transform. We experiment with $(b_l, b_u) \in \{(1000, 8000), (100, 10000), (40, 20000), (50, 15000)\}$.
    \item \textbf{Prepend \cite{rainamuting}} In this scenario, instead of optimizing noise within the audio, we freeze the base audio and optimize a short, unconstrained audio snippet $p$ which is added as a \textit{prefix}. The loss is calculated on the output resulting from the concatenation of the prefix and the base audio. Given the length $d$ of the prepend snippet in seconds, we now optimize:
    \begin{gather*}
        p^* = \arg\min_{p \in [-1, 1]^{16000d}} \mathcal{L}([p \| x], t)
    \end{gather*}

    We randomly initialize the prefix $p$ and experiment with $d\in\{2, 1, 0.1, 0.01\}$. 
\end{itemize}
% \rob{Stealthiness - I guess we could also overlay any recognisable noise in the hope that this was
% interpreted as "background noise", e.g. low-level conversations, people moving about, chairs moving, dogs barking etc. etc. and this could still be completely natural, but in practice added by the attacker. I suppose this is less constrained than an image where the noise has to be random or changes to the image have to be consistent with the image presented (although I guess you could add objects etc., do people do this?). I guess David did work on human reproducible attacks like this.}

\subsubsection{Audio-Agnostic Jailbreak Noise}

Thus far we have discussed techniques to generate adversarial perturbations tailored to a specific input. A natural question to ask is \textit{can we make the adversarial noise not only prompt-agnostic but also audio-agnostic?} To this end, we optimize our adversarial noise on several audio files simultaneously, to evaluate whether the resulting perturbation retains its effectiveness when applied to entirely new, unseen audio. Using our $n$ base audios $B = \{x^1..x^n\}$, we optimize a prepend snippet $p$ using the following loss function:\begin{gather*}
  \mathcal{L}_{\text{total}} = \frac{1}{|B|} \sum_{x \in B} \mathcal{L}([p \| x], t)  
\end{gather*}

Each gradient update step optimizes the perturbation by backpropagating the total loss across all base audios, ensuring that the resulting adversarial noise is generalizable. We choose to optimize a prepended snippet instead of overlay noise due to different base audio lengths. In order to evaluate the effectiveness of this approach, we perform the above optimization on four out of five of the audio files and use the fifth as a holdout to which we prepend the resulting snippet and test the attack efficacy. We initialize the prepend section as a short $\SI{1}{\second}$ snippet of noise, constraining the perturbation at each sample to an absolute value of $0.1$. This is perceived as an ephemeral burst of noise to a listener.

\subsubsection{Robustness}

From a practical perspective, we aim to explore the effect of different audio degradations on a selection of generated jailbreaks. Specifically, we consider the following types of degradation: \textbf{Over-the-Air Recording} Here we make a recording of the jailbreak audio with an iPhone 12 at a distance of $\SI{4}{\second}$ to the speaker in a quiet environment, and then pass it to the language model. This mimics how such an attack could be deployed practically. \textbf{Intermittent Silence Masking} This involves zero-ing out short segments of the signal at random or predefined intervals. These interruptions mimic real-world signal loss or editing artifacts that can obscure portions of the audio content. \textbf{Gaussian Noise Removal} Gaussian noise is removed from the audio using a denoising algorithm. This could be a naive first attempt at defending against adversarial noise. \textbf{Band Pass Filter} All frequencies above and below a particular range are removed from the audio, which is again a simple defense that could be built into the audio pre-processing pipeline to eliminate stealthy noise. Using these techniques, we attempt to answer the question \textit{are optimized audio jailbreaks naturally robust to practical and naive degradations?} 

\subsubsection{Meaningfulness}

Our optimized adversarial noise is specifically designed to alter the model's behavior, however, it remains largely incomprehensible or devoid of meaning to a human listener. This prompts an intriguing question: \textit{how does the language model interpret the jailbreak, and what do its features mean?} The architecture of the \texttt{SALMON-N} model offers itself well to such an analysis - at every epoch, we extract and log two types of meaningfulness features. \textit{BEATS features} are discrete label predictions produced by the BEATS model which describe the audio (e.g. hammer, recording, shuffle). We record the top five most likely BEATS features at every epoch. Additionally, we log the \textit{Whisper transcription}, a representation of spoken words detected in the input audio. 
%For meaningfulness logging, we add two further steps: we use the Whisper language model itself to predict the transcribed tokens and the decoder to parse this into human-readable language. Thus in each epoch, we transcribe the linguistic content of the audio file as perceived by the Vicuna model, which is not directly understandable to the human ear.

\subsection{Evaluation}
\label{section:eval}

An important aspect of assessing the effect of the jailbreak audio on the model is the evaluation set used. As a non-jailbroken \texttt{Vicuna} model answers some harmful prompts, we craft an evaluation dataset of $140$ harmful prompts which the model declines to respond to without jailbreaks. These are equally split into seven categories, namely Target Hate (biased, derogatory prompts regarding the specific demographic and gender in the optimization dataset), Non-Target Hate (biased, derogatory prompts regarding any other group of people), Hate Speech (derogatory, insulting prompts not targeting a group but humans in general or individuals), Illegal Activity, Physical Violence, Sexual Content, or Other (which includes misinformation, political content, or personal information). These were compiled by recording the clean model's responses to prompts from~\citet{qi2023visualadversarialexamplesjailbreak}'s Manual Harmful Instructions and Red Teaming Prompts,~\citet{gehman2020realtoxicityprompts}'s Real Toxicity Prompts, and a few were prepared manually or generated by ChatGPT and adjusted.

We also have a control set of 20 logic/reasoning questions from ARC-Easy~\citep{clarkarc}, which the clean model answers correctly, to measure the effect of the jailbreak on neutral non-toxic tasks. We denote the model's performance $logic(f, x)$ as a percentage on these 20 logical tasks, given an audio input $x$.

We partition our dataset of $140$ harmful prompts into two sets: the reduced evaluation set comprises only the categories `Target Hate', `Non-Target Hate' and `Hate Speech'. Prompts in these categories pertain to the type of misalignment that the jailbreak was optimized on. The full evaluation set, which we use for select experiments, includes all the categories and reflects the transferability of the jailbreak to different types of misalignment.

When evaluating the effect of the jailbreak, we record the output of the model up to $150$ tokens when prompted with the jailbreak audio and each of the harmful prompts. That is, for a jailbreak $x$ and for each harmful prompt $h_i \in H = \{h_0...h_n\}$, we record $f(x, h_i)$. We then use the Detoxify API~\citep{Detoxify} to assign the model output toxicity scores. We use \texttt{Mixtral8x7B-Instruct}~\citep{jiang2024mixtralexperts} as a judge to label each output as toxic or non-toxic according to the types of alignment we attempt to evade, which we denote by $toxic(f(x, h_i)) \in {0, 1}$, and also manually audit. The judge prompt used can be found in~\Cref{apx:judgeprompt}.