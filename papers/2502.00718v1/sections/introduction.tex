\section{Introduction}

Large Language Models (LLMs) have proven useful beyond a doubt across various domains since their widespread deployment, significantly enhancing productivity in tasks such as natural language processing, code generation, and creative content creation~\citep{brown2020languagemodelsfewshotlearners, openai2024}. 
However, their vast capabilities pose a considerable challenge in balancing usefulness and harmlessness~\citep{bommasani2022opportunitiesrisksfoundationmodels}. 
One prominent aspect of AI safety is \textit{alignment}: ensuring that generated content corresponds to the functional objectives and ethical ideals of human users, minimizing risks of harm, bias, or misuse in real-world applications~\citep{weidinger2021ethicalsocialrisksharm, russelaicontrol}. 
Despite the development of various methods for alignment, such as reinforcement learning from human feedback \cite{rlhf} and rule-based constraints \cite{mu2024rulebasedrewardslanguage}, LLM alignment has been shown to be inherently brittle and easy to circumvent using adversarial prompts, jailbreak techniques, or context manipulation~\citep{perez2022redteaminglanguagemodels, liu2024autodangeneratingstealthyjailbreak, wei2023jailbrokendoesllmsafety,xu2024comprehensivestudyjailbreakattack}. 

Humans naturally interact with their surroundings not only through written word, but more commonly via visual and spoken cues. This motivates the development of multimodal models, which integrate information from various modalities - such as text, images, and audio - to more effectively simulate human-like understanding and improve interaction with users~\citep{multimodality}. A plethora of multimodal models have been released in recent years~\citep{alayrac2022flamingovisuallanguagemodel, liu2023visualinstructiontuning, driess2023palmeembodiedmultimodallanguage}. Two such kinds of models are Vision Language Models (VLMs)~\citep{vlms} and Audio Language Models (ALMs)~\citep{chu2023qwenaudioadvancinguniversalaudio}, which respectively take image or audio and textual input simultaneously. Naturally, the introduction of an additional input channel opens a conspicuous attack vector through which a model can be deceived into undesirable output~\citep{eykholt2018physicaladversarialexamplesobject, roboticarmadversarial}. Adversarial example generation for plain language models is computationally expensive due to the discrete nature of the input space - specifically, gradient-based optimization methods do not directly map onto valid textual tokens, making it difficult to manipulate the input effectively~\citep{carlini2018audioadversarialexamplestargeted}. In contrast, when working with continuous signals, gradient manipulations can directly influence the input, enabling the generation of adversarial examples without the constraints imposed by discrete token boundaries.

As Vision-Language Models (VLMs) have become mainstream, with numerous commercial and open-source implementations available such as BLIP~\citep{li2022blip}, Flamingo~\citep{alayrac2022flamingovisuallanguagemodel} and CLIP~\citep{radford2021clip}, there has been extensive research into various types of attacks targeting the visual modality~\citep{goodfellow2014explaining, eykholt2018robustphysical, shafahi2018poisonfrog, hosseini2017semanticattack}, particularly regarding visual jailbreaks~\citep{carlini2024alignedneuralnetworksadversarially, qi2023visualadversarialexamplesjailbreak, li2024imagesachillesheelalignment, feng2024jailbreaklensvisualanalysisjailbreak}. Just as visual adversarial attacks have been observed to differ practically and mechanistically from textual attacks~\citep{schaeffer2024transferability, wallace2021nlpattacks}, we argue that audio attacks deserve to be studied separately to image attacks.  Image and audio input signals fundamentally differ in that audio signals are inherently sequential and require temporal context, unlike static, frame-based image signals, and thus audio requires time-frequency representations like spectrograms whereas images are processed as 2D spatial data. Moreover, human perceptual tolerance for perturbations in audio and image is given through different sets of constraints. Audio is of particular practical interest as vocal interaction with digital assistants is more natural than typed; indeed the nature of chatbot-based communication emulates a transcribed oral conversation. This gives rise to endless meaningful deployments, such as real-time speech analysis for courtrooms, voice biometrics for authentication, audio-based emotion analysis for mental health monitoring or LLM-powered smart home voice assistants~\cite{Mahmood_2025, koffi2023voice}. 

\fakeparagraph{Contributions} 
In this paper, we offer first results of an extensive exploration of ALM jailbreaks on the SALMONN-7B language model~\citep{tang2024salmonn}. The goal of this research is to deliver a range of empirical results regarding the potential and limitations of jailbreaks in the audio modality, and most importantly, to offer novel insights into the \textit{meaning} of these in the textual space. We establish an experimental framework to facilitate the study of audio jailbreaks and design a meaningful evaluation dataset for the selected adversarial task. We show that this method permits highly potent jailbreaks which generalize across different content dimensions. We investigate the striking characteristics of the interpretations of these jailbreaks. We discuss the limitations of the jailbreaks and the significance of the selection of the base audio in the optimization process.

% \isha{CHANGE FOR CURRENT PAPER} that although our optimized audio jailbreaks are particularly potent, even random noise, stealthy jailbreaks, or degraded jailbreaks are able to subvert the model's alignment training when paired with any type of toxic prompt. Curiously, our meaningfulness analysis reveals that the optimized jailbreaks are frequently interpreted by the model as first-person speech containing negative or dark content.

% \begin{itemize}
%     \item We establish an experimental framework to facilitate the study of audio jailbreaks and design a meaningful evaluation dataset for the selected adversarial task;
%     \item We confirm that using a gradient-based optimization method that has been applied to create visual adversarial examples can construct a universal audio jailbreak (\cite{qi2023visualadversarialexamplesjailbreak}); \isha{this sentence needs revision} \rob{Never use cites as nouns and add some context: "We show that experiments using visual adversarial inputs~\cite{Qi...} can be adapted to construct a universal audio jailbreak." ??? -- perhaps you need to add the generation of universal here too (i.e. prompt agnostic)}
%     \item We explore the effects of stealth constraints and whether we can make the adversarial noise not only prompt-agnostic but also agnostic to the audio input;
%     \item We study the robustness of our attacks to realistic distortions of the audio input;
%     \item We report findings on the transferability of the jailbreak attack to other attack objectives, for example toxicity against non-target demographics or the elicitation of sexual content;
%     \item We provide results on the meaningfulness of the different jailbreaks we produce in each of these settings in the textual space and characterize an effective jailbreak.
% \end{itemize}
