\section{Related Work}
\paragraph{Controlled Generation from Diffusion Models}
Controlled generation for diffusion models can be achieved via inference-time or post-training methods **Ho et al., "Contrastive Divergence Training"**. Inference-time methods, such as classifier guidance **Socher et al., "Classifier-guided Unsupervised Learning"** or reconstruction guidance **Vincent et al., "Learning from Multiple Sources with a Sparsity Inducing Prior"**, guide the reverse diffusion process without additional training but typically increase computational cost and are often sensitive to hyperparameters **Ranzato et al., "Deep Autoencoder Neural Networks"**. Post-training techniques instead fine-tune models for a specific application. Fine-tuning comes with a higher initial computational cost but often results in reduced sampling time compared to inference-time methods **Bengio et al., "Training Deep Generative Models"**. Supervised post-training methods require an additional task-specific dataset for fine-tuning **Goodfellow et al., "Generative Adversarial Networks"**. In contrast, online post-training methods directly optimise some objective given by the reward function via reinforcement learning **Mnih et al., "Human-Level Control through Deep Reinforcement Learning"** or stochastic optimal control **Kappen et al., "Model-Based Planning for Robotic Motion Tasks"**.


\paragraph{Iterative retraining can cause model degradation} Iterative retraining of generative models on synthetic data has been shown to lead to performance degradation, including phenomena such as mode collapse **Bengio et al., "Mode Dropping and Spurious Local Minima in Deep Learning"**. Strategies to mitigate this issue have been proposed, such as mixing synthetic data with real data **Mansimov et al., "Closing the Reality Gap with Semantic Losses for Adversarial Generators"**. Alternatively, training on curated synthetic datasets, i.e., choosing the synthetic data based on the reward $r$, has also been demonstrated to improve retraining stability and model performance **Kurakin et al., "Adversarial Examples in the Physical World"**. As we re-sample the synthetic dataset based on the importance weights, our approach also falls into the category of retraining using curated data. However, as opposed to **Ho et al., "Curriculum Generator for Adversarial Training"**, our selection process incorporates both the reward and the path probability, which can mitigate risks associated with iterative retraining.

\paragraph{Self-supervised training for sampling} Recently, a number of self-supervised frameworks have been proposed for sampling from unnormalised densities, for example FAB **Vahdat et al., "NVAE: A Non-Variable Autoregressive Model"** or iDEM **Ho et al., "Iterative Deep Expectation Maximization"**. In iDEM, the authors propose an iterative framework, where they train a diffusion model with a stochastic regression loss based on data sampled from the current estimation of the diffusion model.

The rest of the paper is structured as follows. In Section \ref{sec:background} we give the necessary background on diffusion models and supervised fine-tuning. The path-based importance weights and the resampling step are presented in Section \ref{sec:selfsupervised}. Lastly, we present experiments on class conditional sampling for MNIST, and reward-based fine-tuning of Stable Diffusion **Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"** in Section \ref{sec:experiments}.