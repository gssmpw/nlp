\section{Related Work}
\paragraph{Controlled Generation from Diffusion Models}
Controlled generation for diffusion models can be achieved via inference-time or post-training methods \citep{uehara2025reward}. Inference-time methods, such as classifier guidance \citep{dhariwal2021diffusion} or reconstruction guidance \citep{chung2022diffusion}, guide the reverse diffusion process without additional training but typically increase computational cost and are often sensitive to hyperparameters \citep{song2023solving}. Post-training techniques instead fine-tune models for a specific application. Fine-tuning comes with a higher initial computational cost but often results in reduced sampling time compared to inference-time methods \citep{denker2024deft}. Supervised post-training methods require an additional task-specific dataset for fine-tuning \citep{ruiz2022dreambooth,zhang2023adding,xu2024imagereward}. In contrast, online post-training methods directly optimise some objective given by the reward function via reinforcement learning \citep{venkatraman2024amortizing,clark2023directly,fan2024reinforcement,black2024training} or stochastic optimal control \citep{denker2024deft,domingo2024adjoint}. 


\paragraph{Iterative retraining can cause model degradation} Iterative retraining of generative models on synthetic data has been shown to lead to performance degradation, including phenomena such as mode collapse \citep{alemohammad2023self,shumailov2023curse}. Strategies to mitigate this issue have been proposed, such as mixing synthetic data with real data \citep{bertrand2023stability}. Alternatively, training on curated synthetic datasets, i.e., choosing the synthetic data based on the reward $r$, has also been demonstrated to improve retraining stability and model performance \citep{ferbach2024self}. As we re-sample the synthetic dataset based on the importance weights, our approach also falls into the category of retraining using curated data. However, as opposed to \cite{ferbach2024self} our selection process incorporates both the reward and the path probability, which can mitigate risks associated with iterative retraining.
 
\paragraph{Self-supervised training for sampling} Recently, a number of self-supervised frameworks have been proposed for sampling from unnormalised densities, for example FAB \citep{midgley2022flow} or iDEM \citep{akhound2024iterated}. In iDEM, the authors propose an iterative framework, where they train a diffusion model with a stochastic regression loss based on data sampled from the current estimation of the diffusion model.  

The rest of the paper is structured as follows. In Section \ref{sec:background} we give the necessary background on diffusion models and supervised fine-tuning. The path-based importance weights and the resampling step are presented in Section \ref{sec:selfsupervised}. Lastly, we present experiments on class conditional sampling for MNIST, and reward-based fine-tuning of Stable Diffusion \citep{rombach2022high} in Section \ref{sec:experiments}.