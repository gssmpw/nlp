\section{Related Work}
\paragraph{Controlled Generation from Diffusion Models}
Controlled generation for diffusion models can be achieved via inference-time or post-training methods ____. Inference-time methods, such as classifier guidance ____ or reconstruction guidance ____, guide the reverse diffusion process without additional training but typically increase computational cost and are often sensitive to hyperparameters ____. Post-training techniques instead fine-tune models for a specific application. Fine-tuning comes with a higher initial computational cost but often results in reduced sampling time compared to inference-time methods ____. Supervised post-training methods require an additional task-specific dataset for fine-tuning ____. In contrast, online post-training methods directly optimise some objective given by the reward function via reinforcement learning ____ or stochastic optimal control ____. 


\paragraph{Iterative retraining can cause model degradation} Iterative retraining of generative models on synthetic data has been shown to lead to performance degradation, including phenomena such as mode collapse ____. Strategies to mitigate this issue have been proposed, such as mixing synthetic data with real data ____. Alternatively, training on curated synthetic datasets, i.e., choosing the synthetic data based on the reward $r$, has also been demonstrated to improve retraining stability and model performance ____. As we re-sample the synthetic dataset based on the importance weights, our approach also falls into the category of retraining using curated data. However, as opposed to ____ our selection process incorporates both the reward and the path probability, which can mitigate risks associated with iterative retraining.
 
\paragraph{Self-supervised training for sampling} Recently, a number of self-supervised frameworks have been proposed for sampling from unnormalised densities, for example FAB ____ or iDEM ____. In iDEM, the authors propose an iterative framework, where they train a diffusion model with a stochastic regression loss based on data sampled from the current estimation of the diffusion model.  

The rest of the paper is structured as follows. In Section \ref{sec:background} we give the necessary background on diffusion models and supervised fine-tuning. The path-based importance weights and the resampling step are presented in Section \ref{sec:selfsupervised}. Lastly, we present experiments on class conditional sampling for MNIST, and reward-based fine-tuning of Stable Diffusion ____ in Section \ref{sec:experiments}.