\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{bm}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\usepackage{graphicx,xcolor}
\newcommand{\Alex}[1]{\leavevmode \textcolor{blue}{Alex: #1}}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{bm}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{tikz}
\usepackage[framemethod=tikz]{mdframed}
\usetikzlibrary{calc}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\newcommand{\Hightlight}[1]{\leavevmode \textcolor{blue}{#1}}

\input{commands/math.tex}
\input{commands/notation.tex}

\title{Simulation-free training of DEFT}
\author{}

\begin{document}
\maketitle

See \url{https://hackmd.io/W4_w3DwdRDiW2pPYMP3bEg} \\
See \url{https://hackmd.io/@franciscovargas/rJxl3SExke} \\
See \url{https://hackmd.io/Ux2XRGX-SiKXX5nJ7nFf0Q}


\section{Self-Supervised DEFT}

\textbf{Approach:} Use the reverse SDE 
\begin{align} 
\label{eq:rev_sde}
    d x_t = \left( f_t(x_t) - \sigma_t^2 \left( \nabla_{x_t} \ln p_t(x_t) + h_t^*(x_t) \right)\right)\,d t + \sigma_t\ dw, 
\end{align}
with $h_t^*(x_t)$ as the generalised $h$-transform. 

\begin{theorem}[Supervised fine-tuning]
We can recover the $h$-transform as the minimiser of 
\begin{align}
\label{eq:supervised_deft}
    h^* = \arg \min_h \mathop{\mathbb{E}}_{\substack{ x_0 \sim p_\text{tilted}(x_0) \\ t\sim \mathrm{U}(0,T), x_t \sim p_{t|0}(x_t|x_0)}  }\left[ \left\| \left({h_t(x_t)}\!+\!\nabla_{x_t} \ln p_t(x_t) \right)\!-\!\nabla_{x_t}\ln p_{t|0}(x_t  |x_0) \right\| ^2\right] 
\end{align}
For score-matching with the VP-SDE this is just:
\begin{align}
        h^* = \arg \min_h \mathop{\mathbb{E}}_{\substack{ x_0 \sim p_\text{tilted}(x_0) \\ t\sim \mathrm{U}(0,T), x_t \sim p_{t|0}(x_t|x_0)}  }\left[ \left\| \left({h_t(x_t)}\!+\!s_\theta(x_t,t) \right)- z \right\| ^2\right], 
\end{align}
where $x_t = x_0 + \nu_t z, z \sim \mathcal{N}(0,I)$.
\end{theorem}

One big drawback is that we need to be able to sample from the tilted distribution $x_0 \sim p_\text{tilted}(x_0)$. This is actually the hard part, which we want to learn. In \cite{denker2024deft}, we applied this framework to inverse problems and amortised over measurements $y$ by using a small fine-tuning dataset $\{x_0^{(i)}, y^{(i)} \}_{i=1}^N$. However, in the general context of sampling from a tilted distribution $p_\text{tilted}(x_0) = p_\text{data}(x_0) \exp(r(x_0))$, this is not possible.





One can think of a self-supervised framework:
\begin{itemize}
    \item 0. Initialise $h$-transform $h_t$ with some neural network
    \item 1. Sample a dataset $\{ x_0^{(i)}\}, i=1,\dots,N$ from the reverse SDE in Equation \eqref{eq:rev_sde} with the current estimation to the $h$-transform
    \item 2. Compute scaled Importance Weights $w_i = w(x_0^{(i)})$ (scaled in such a way that a certain percentage will be accepted) 
    \item 3. Use the IS weights to resample $\{ x_0^{j}\}, j \in I \subset \{1, \dots, N\}$
    \item 4. Use this subset to estimate the $h$-transform using Equation \eqref{eq:supervised_deft}  with the resampled dataset $\{ x_0^{j}\}, j \in I \subset \{1, \dots, N\}$
    \item 5. Go back to 1. and repeat 
\end{itemize}

\newpage 
One can think of a self-supervised framework to learn $q(x)$:
\begin{itemize}
    \item 0. Initialise $p_\theta$
    \item 1. Sample a dataset $\{ x_0^{(i)}\}, i=1,\dots,N$ from current model $p_\theta$
    \item 2. Compute Importance Weights $w_i = q(x) / p_\theta(x)$  
    \item 3. Use the IS weights to resample $\{ x_0^{j}\}, j \in I \subset \{1, \dots, N\}$
    \item 4. Re-train $p_\theta$ with the resampled dataset $\{ x_0^{j}\}, j \in I \subset \{1, \dots, N\}$
    \item 5. Go back to 1. and repeat 
\end{itemize}

What we wrote here is also really similar to FAB?

Similar ideas have been explored for sampling from unnormalised densities. In our application ''conditional sampling from unconditional diffusion models'' it might work better, as we already have an good starting value:
\begin{itemize}
    \item \cite{akhound2024iterated} (or the same for flow models: \cite{woo2024iterated})
    \item \cite{midgley2022flow}
\end{itemize}

\section{Learning to Sample}
We want to sample from a distribution
\begin{align}
    p_\text{target}(x) = \frac{\rho_\text{target}(x)}{Z}, \quad Z = \int \rho_\text{target}(x) dx, 
\end{align}
where we only can evaluate the unnormalised density $\rho_\text{target}$ and its gradient. A lot of sampling approaches are based on transporting a tractable initial density function to the target. Recently, an interesting PDE perspective has been seen increasing attention \cite{mate2023learning,sun2024dynamical,berner2022optimal}. In the deterministic setting, the goal is to transport a prior density to the target density via some ODE. Here, we consider
\begin{align}
    d x_t = f(x_t, t) dt, \quad x_0 \sim p_\text{prior},
\end{align}
and learn the drift $f(x_t, t)$ such that $x_t \sim q_\theta \approx p_\text{target}$. This setting was for example considered for continuous normalising flows (CNFs) \cite{chen2018neural,grathwohl2018ffjord}. For CNFs, we try to minimise the reverse KL divergence 
\begin{align*}
    \min_f D_\text{KL}(q_\theta, p_\text{target}) = \min_f \mathbb{E}_{x \sim q_\theta}[\log q_\theta - \log \rho_\text{target}(x)] + Z.
\end{align*}
However, there are (at least) three drawbacks of this:
\begin{itemize}
    \item[1.] We have to simulate the full trajectory to sample from $q_\theta$, 
    \item[2.] and also backpropagate through this function or solve the corresponding adjoint ODE,
    \item[3.] further the reverse KL is prone to mode collapse. 
\end{itemize}
We can rewrite the reverse KL as 
\begin{align*}
     \min_f D_\text{KL}(q_\theta, p_\text{target})  \Leftrightarrow \min_f \mathbb{E}_{x \sim q_\theta(x) }[-\log \rho_\text{target}(x)] - \mathcal{H}(q_\theta),
\end{align*}
where $\mathcal{H}(q_\theta) = \mathbb{E}_{x \sim q_\theta(x) }[- \log q_\theta(x)]$ is the entropy of $q_\theta$, i.e., we want to maximise the log-likelihood of $\rho_\text{target}$ under our model and have a large entropy (dont simply get stuck in the maximum of $\rho_\text{target}$).

\subsection{Controlling a pre-trained NeuralODE/CNF/Flow Matching Model}
Assume we already have access to a pre-trained model 
\begin{align*}
    d x_t = f_\theta^*(x_t, t) dt, \quad x_0 \sim p_\text{prior}, x_T \sim q_\theta ( = p_\text{target})
\end{align*}
Can we control this model to sample from some tilted distribution or posterior
\begin{align*}
    p_\text{post}(x) = q_\theta(x) \exp(r(x)),
\end{align*}
with some reward function $r$?

If we follow a similar line as in DEFT and learn an additional drift $h_\phi$, i.e.,
\begin{align*}
    d x_t = (f_\theta^*(x_t, t) + h_\phi(x_t,t)) dt, \quad x_0 \sim p_\text{prior}, x_T \sim p_\phi
\end{align*}
Can we think of $h_\phi$ again as the $h$-transform? What is the mathematical meaning of $h_\phi$? Does it depend on how the original model $f_\theta^*$ was trained, i.e., if $f_\theta^*$ is an OT velocity field? Further, how can we train $h_\phi$ such that $p_\phi \approx p_\text{post}$?

For the training, look at
\begin{align}
    \min_\phi D_\text{KL}(p_\phi || p_\text{post}) \Leftrightarrow \min_\phi D_\text{KL}(p_\phi || q_\theta) - \mathbb{E}_{x \sim p_\phi}[r(x)]
\end{align}

Is the first KL divergence again similar to 
\begin{align}
    D_\text{KL}(p_\phi || q_\theta) \Rightarrow \mathbb{E}_{x_t }\left[\int_0^T \| h_\phi(x_t, t) \|_2^2  dt\right] ?
\end{align}

\section{Goal}
We are working in the same setting as \cite{denker2024deft}, i.e., we assume that we have a set of forward and backward SDEs 
\begin{align}
    d x_t &= f_t(x_t) dt + \sigma_t dw_t,  \quad x_0 \sim p_0 := \pi \\
    d x_t &= \left(f_t(x_t) - \sigma_t^2 \nabla_{x_t} \ln p_t(x_t)\right) dt + \sigma_t dw_t,  \quad x_T \sim p_T := \mathcal{N}(0,I) \label{eq:reverse_SDE},
\end{align}
to sample from a distribution $\pi$ (by simulating \eqref{eq:reverse_SDE} from $t=T$ to $t=0$). 

Our goal is to \textit{control} this reverse SDE to satisfy additional constraints. For inverse problems this means that we want to sample from the posterior 
\begin{align*}
    p(x | y ) = \pi(x) p(y|x) / Z,
\end{align*}
where $p(y|x)$ is the likelihood. We assume that we can evaluate $p(y|x), \log p(y|x)$ and $\nabla_x \log p(y|x)$.

In \cite{denker2024deft} we show that we can control the SDE by adding an additional drift term, resulting in 
\begin{align}
    d x_t &= \left(f_t(x_t) - \sigma_t^2 \nabla_{x_t} \ln p_t(x_t) - \sigma_t^2 h_t(x_t,y) \right) dt + \sigma_t dt,  \quad x_T \sim p_T := \mathcal{N}(0,I) \label{eq:reverse_SDE_cond},
\end{align}

One way to learn this $h_t$ is by solving an stochastic optimisation problem: 

\begin{theorem}[Proposition~3.2 from \cite{denker2024deft}]
For a given $y \sim p(y|x)$ we can recover $h_t$ as the minimiser of 
\begin{align}
    \min_{h_t} \mathbb{E}_{\{ x_t \}_t}\left[\frac{1}{2} \int_0^T \sigma_t^2 \| h_t(x_t) \|_2^2 dt\right] - \mathbb{E}_{x_0}[\ln p(y|x_0)],
\end{align}
where the expectation is over trajectories of 
\begin{align}
    d x_t = (f_t(x_t) - \sigma_t^2 (\nabla_{x_t} \ln p_t(x_t) + h_t(x_t)) dt + \sigma_t dw_t.
\end{align}
\end{theorem}

This way of learning $h_t$ works and we have some small experiments in our paper. However, it is computationally very expensive as you have to simulate the full trajectory for every optimisation step, and in the naive way also backpropagate through the full chain.

\section{Simulation-free Loss Function}
Lets consider a slightly different parametrisation of the reverse SDE in \eqref{eq:reverse_SDE_cond}:
\begin{align}
        d x_t &= \underbrace{\left(f_t(x_t) - \sigma_t^2 \nabla_{x_t} \ln p_t(x_t) - \sigma_t^2 \nabla \log \Tilde{h}_t(x_t,y) \right)}_{:= \Tilde{f}(x_t, t)} dt + \sigma_t dt,  \quad x_T \sim p_T := \mathcal{N}(0,I) \label{eq:reverse_SDE_cond_v2},
\end{align}
where $h_t = \nabla \log \Tilde{h}_t$. Then, we can characterise $\Tilde{h}_t$ as the solution of the backward Kolmogorov equation
\begin{align}
    \partial_t \Tilde{h} + \mathcal{L} \Tilde{h} = 0, \\
    \Tilde{h}_0(x) = p(y | x),
\end{align}
with 
\begin{align}
    \mathcal{L} \Tilde{h}_t(x) = - \text{div}( \Tilde{f}(x, t) \Tilde{h}_t(x)) + \frac{1}{2} \sigma_t^2 \text{Tr}(\nabla^2 \Tilde{h}_t(x)),
\end{align}
see also Eqn. (3) in \cite{heng2021simulating}. Note, that $\text{Tr}(\nabla^2 \Tilde{h}_t(x)) = \Delta \Tilde{h}_t(x)$. 

We can find a solution to this PDE by applying PINNs and minimise:
\begin{align}
    L(\Tilde{h}_t) = \mathbb{E}_{t, x_t}[ \| \partial_t \Tilde{h}_t + \mathcal{L} \Tilde{h}_t  \|^2] + \lambda \mathbb{E}_{x_0}[ \| \ln \Tilde{h}_0(x_0) - \ln p(y|x_0) \|^2]
\end{align}
The advantage of this formulation is that the collocation points $x_t$ are not required to the sampled from the reverse SDE. 

Of course, by constructing a suitable method to draw collocation points this loss should be easier to minimise and the PINN easier to train.


\section{Does this also work for ODEs?}
In the ODE setting, we can work with the continuity equation instead of Fokker-Planck. This means, that we get around the term $\text{Tr}(\nabla^2 \Tilde{h}_t)$ in the PDE, which would reduce the computational cost. In the unconditional case (i.e., learning to sample) this connection can be seen in \cite{sun2024dynamical}.

We assume we have a probability flow ODE 
\begin{align}
    d x_t = \mu(x_t, t) dt, \quad x_T \sim p_0 = \mathcal{N}(0,I),
\end{align}
where $\mu(\cdot, t)$ is trained such that $x_T \sim p_T = \pi$. We want to control this ODE such that $x_T \sim p(x_T | y) = \pi(x_T) p(y | x_T) / Z$. For this, we add an additional drift term
\begin{align}  
    \label{eq:controlled_ode}
    d x_t = (\mu(x_t, t) + h(x_t, t)) dt, \quad x_0 \sim p_0 = \mathcal{N}(0,I).
\end{align}
Note that I changed the notation here: In this section $h$ is already $\nabla \log \tilde{h}$.
We know that the density $p_t$ of the controlled ODE \eqref{eq:controlled_ode} satisfies the continuity equation
\begin{align}
    \partial_t p_t + \text{div}(p_t (\mu + h)) = 0, 
\end{align}
with boundary conditions $p_0 = \mathcal{N}(0,I)$. We want to learn $h$ such that $p_T =  \pi(x_0) p(y | x_0) / Z$ is satisfied. We can also look at the log-transformed continuity equation, with $V_t := \log p_t$ 
\begin{align}
    \mathcal{R}_{\text{log}}(V, h) =  \partial_t V_t + \text{div}(\mu + h) + \nabla V_t \cdot (\mu + h) = 0
\end{align}
This gives us the loss function for $V$ and $h$:
\begin{align}
    \mathcal{L}(V, h) = \mathbb{E}[ ( \mathcal{R}_{\text{log}}(V, h))^2]. 
\end{align}



Note the div(u + h) term is as expensive as the Laplacian term when it comes to high dimensions so this brings little,  note to learn the transform we need:

$$
V_t = \log p_t^{\mathrm{ref}} -  \ln \tilde{h}_t 
$$


so to learn the true h-transform one still needs the scalar parameterization of the network which is not a huge problem the really expensive term is the divergence.


\subsection{Derivation of the Marginal Density for Deft}

Let us relable 

\begin{align}
    r(x_0) = \ln p(y|x_0)
\end{align}

From results in control (Tzen and Raginsky) we can show that the transition density of the optimally controlled distribution (can also be proved more intuitively  using h-transform results I think , without any stochastic control references)


\begin{align}
    p_{t|s}(y|x) = p_{t|s}^{\mathrm{ref}}(y|x) e^{\tilde{h}_s(x)-\tilde{h}_t(y)} 
\end{align}

So then we can marginalise with respect to the $p_0(x) = Q_t[p(x_0|y)]$ distribution : 

\begin{align}
  \int   p_{t|0}(y|x) p_0(x) dx &=  \int p_{t|s}^{\mathrm{ref}}(y|x) e^{\tilde{h}_0(x)-\tilde{h}_t(y)}  p_0(x) dx  \\
  &= e^{-\tilde{h}_t(y)} \int p_{t|T}^{\mathrm{ref}}(y|x) e^{\tilde{h}_T(x)-\tilde{h}_t(y)}  p_t(x) dx  \\ 
  &=  e^{-\tilde{h}_t(y)} p_t^{\mathrm{ref}}(y)
\end{align}


Where we have used (This can be proved for the h-transform/value function its true)

$$
e^{\tilde{h}_T(x)}  =\frac{p_T^{\mathrm{ref}}(x)}{p_t(x) }
$$

\section{Self-supervised Finetuning}

Let $q(x)\propto p_\text{diff}(x)\exp(r(x))$ for some $r\colon\mathbb{R}^d\to\mathbb{R}$ and assume that we are given a diffusion model
\begin{align*}
    d x_t &= f_t(x_t) dt + \sigma_t dw_t,  \quad x_0 \sim p_0 := \pi \\
    d x_t &= \left(f_t(x_t) - \sigma_t^2 \nabla_{x_t} \ln p_t(x_t)\right) dt + \sigma_t dw_t,  \quad x_T \sim p_T := \mathcal{N}(0,I),
\end{align*}
which generates the density $p_\text{diff}$.
Like in \cite{denker2024deft}, we aim to learn an additional drift term $h_t$ such that $q(x)$ is approximately generated by the SDE
\begin{align*}
    d x_t &= \left(f_t(x_t) - \sigma_t^2 \nabla_{x_t} \ln p_t(x_t) - \sigma_t^2 h_t(x_t,y) \right) dt + \sigma_t dt,  \quad x_T \sim p_T := \mathcal{N}(0,I).
\end{align*}
To this end, employ the following iterative rejection procedure (``supervised fine-tuning'').
\begin{enumerate}
\item Generate a batch of samples $x_1,...,x_N$ from the current approximation
\item Reweight or reject/resample them based on some importance-based scheme
\item Update the parameters of $h_t$ based on the score-matching loss. {\color{blue} Would you re-weight the supervised score matching loss using the importance weights?}
\end{enumerate}
The crucial step here, will be the implementation of the importance-based reweighting or rejection/resampling. 

\paragraph{Approximate Importance Weights} Basically all importance/reweighting/rejection/resampling steps
need to compute (or approximate) the importance weights $\frac{q(x)}{p(x)}$ up to a multiplicative constant, where $p$ is the density of the distribution generated by our current approximation. To this end we note that $\frac{q(x)}{p(x)}=\frac{q(x)}{p_\text{diff}(x)}\frac{p_\text{diff}(x)}{p(x)}\propto\exp(r(x))\frac{p_\text{diff}(x)}{p(x)}$.
Since we cannot compute the quotiont $\frac{p_\text{diff}(x)}{p(x)}$ exactly, we approximate it by the ELBO from \cite{denker2024deft} given by
\begin{align*}
\frac{p_\text{diff}(x)}{p(x)}&=\exp\left(-\log\left(\frac{p(x)}{p_\text{diff}(x)}\right)\right)\approx \exp(-\log(\frac{\mathbb{P}(x_{0:T})}{\mathbb{P}_\text{diff}(x_{0:T})}))=\exp \left( - \int_0^T \sigma_t^2\| h_t(x_t) \|_2^2  dt - \int_0^T \sigma_th_t(x_t)^\top dW_t\right)
%\exp(-D_\text{KL}(p || p_\text{diff}))\approx \exp(-D_\text{KL}(\mathbb{P} || \mathbb{P}_\text{diff}))\\
%&=\exp\left(-\mathbb{E}_{x_t \sim \mathbb{P}}\left[\int_0^T \sigma_t^2\| h_t(x_t) \|_2^2  dt\right]\right),
\end{align*}
where $\mathbb{P}$ and $\mathbb{P}_\text{diff}$ are the path densities for the SDEs corresponding to the prior and adjusted diffusion model. 
{\color{blue}the expression written there before was nonsense, I corrected it according to Franciscos comment.}
Note that in practice, we are doing of course all computations in the log-space and we will also only use the log-values of them in the following paragraph (otherwise this would be numerically incredible instable).


One can compute path-wise importance weights of the form (with some error on signs/powers of $\sigma_t$): 
\begin{align}
    \frac{p^{\mathrm{h}}(x_{0:T})}{p_\text{diff}(x_{(0:T]})e^{r(x_0)}} = \exp \left( -r(x_0) -  \int_0^T \sigma_t^2\| h_t(x_t) \|_2^2  dt + \int_0^T \sigma_th_t(x_t)^\top dW_t\right)
\end{align}

I think we want (only true when $x_{(0:T]}\sim p^h$
\begin{align}
    \frac{p_\text{diff}(x_{(0:T]})e^{r(x_0)}}{p^{\mathrm{h}}(x_{0:T})} = \exp \left( r(x_0) -  \int_0^T \sigma_t^2\| h_t(x_t) \|_2^2  dt + \int_0^T \sigma_th_t(x_t)^\top dW_t\right)
\end{align}

then if interested in taking expectations with respect to $p e^r$ for example one can do
\begin{align}
  \mathbb{E}_{X_{0:T} \sim p^{\mathrm{h}}} \left[ \frac{p_\text{diff}(x_{(0:T]})e^{r(x_0)}}{p^{\mathrm{h}}(x_{0:T})} f(X_{0:T})\right]
\end{align}
if we wanted to compute approximate marginal IS weights we would need to estimate the marginal $p_\text{diff}(x_0)$ on a sample $x_0$ coming from a particular It\^{o} process, say the one we are learning, for example, approximations for this can be derived using It\^{o}s lemma as done here \url{https://arxiv.org/pdf/2411.01293} or here \url{https://hackmd.io/@franciscovargas/rJxl3SExke}, here the error in the approximation is controlled by how good our score approximation is. {\color{blue} Can we also compute the marginals via the probability flow ODE and see continuos change of variables?}
{\color{blue}JH: I see, that the approximation with the path densities instead of the marginals is pretty coarse. However, I would conjecture that the rejection sampling steps are minimizing the same objective as in your DEFT paper.}


then if interested in taking expectations with respect to $p e^r$ for example one can do
\begin{align}
  \mathbb{E}_{X_{0:T} \sim p^{\mathrm{h}}} \left[ \frac{p_\text{data}(x_{(0:T]})e^{r(x_0)}}{p_h(x_{0:T})} f(X_{0:T})\right]
\end{align}
if we wanted to compute approximate marginal IS weights we would need to estimate the marginal $p_\text{diff}(x_0)$ on a sample $x_0$ coming from a particular It\^{o} process, say the one we are learning, for example, approximations for this can be derived using It\^{o}s lemma as done here \url{https://arxiv.org/pdf/2411.01293} or here \url{https://hackmd.io/@franciscovargas/rJxl3SExke}, here the error in the approximation is controlled by how good our score approximation is. {\color{blue} Can we also compute the marginals via the probability flow ODE and see continuos change of variables?}
{\color{blue}JH: I see, that the approximation with the path densities instead of the marginals is pretty coarse. However, I would conjecture that the rejection sampling steps are minimizing the same objective as in your DEFT paper.}


\paragraph{Relaxed Rejection/Resampling Step}
Now, one possibility for implementing the second step from above efficiently is the relaxed rejection/resampling step which we proposed in \cite{hertrich2024importance}. It consists out of the following two steps and (for exact importance weights) has the unique fixed point given by the target distribution.
\begin{itemize}
\item Compute acceptance rate $\alpha(x_i)=\min\{1,\frac{q(x)}{cp(x)}\}$, where $c$ is a hyperparameter
\item With probability $\alpha(x_i)$ accept $x_i$, otherwise resample $x_i$ from $p$.
\end{itemize}
In practice, the hyper parameter $c$ can easily be chosen such that a certain rate $r$ of the samples is rejected (I think we used $r=0.2$), which worked well. Moreover, one can compute the density ratio between the distribution before and after the rejection step such that several of these steps can be stacked.

\section{Trajectory Balance on Transition Denisty}
\cite{rector2024steering}
and see \url{https://hackmd.io/Ux2XRGX-SiKXX5nJ7nFf0Q}

\section{MMD Flow}
\cite{galashov2024deep}, \cite{du2023nonparametric}, \cite{hertrich2023generative}, \cite{hagemann2023posterior}

\section{TODOs:}

General:
\begin{itemize}
    \item Replay buffer (for better exploring): cheap buffer (unprioritized) vs. (maybe later) weighted by the importance weights 
    \item Check the importance weights 
\end{itemize}

Experiments:
\begin{itemize}
    \item Simple 2D Gaussian Mixture example: \Hightlight{This works as a toy example}
    \item Inpainting on MNIST $28 \times 28$: 
    \item Class Conditional Sampling for Cifar10 $3\times 32\times 32$:
    \begin{itemize}
        \item TODO: Get diffusion model for Cifar10
        \item TODO: Get classifier for Cifar10
    \end{itemize}
    \item (Inpainting for CelebA $3 \times 128 \times 128$)
    \item Reward Fine-tuning using Stable Diffusion (text-to-images) $3\times 512 \times 512$
\end{itemize}

Stable Diffusion
\begin{itemize}
    \item Use version 1.5 \url{https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5}
    \item Write a forward pass function \texttt{model.forward(xt, t)} which gives the noise as output 
    \item Use this in forward in our sampling 
    \item Scheduling: It is possible to use different schedulers. We can just use the usual DDPM scheduling.  
    \item What happens to the importance weights for the $\epsilon$ matching instead of the score model? Currently we have $s_\theta(x_t,t) + h_\phi(x_t, t)$. Should we rewrite the Stable Diffusion $\epsilon$ matching model as a score model or should we rewrite our importance weights? 
\end{itemize}


\section{Stable Diffusion}
I want to repeat the DDPM forward and reverse process. Given a schedule for $\beta_t$ we have for the forward process 
\begin{align*}
    x_{t} = \sqrt{1 - \beta_t} x_{t-1} + \sqrt\beta_t z, \quad z \sim \mathcal{N}(0,I)
\end{align*}
or, given $x_0$
\begin{align*}
    x_t = \sqrt{\bar \alpha_t} x_0 + \sqrt{1 - \bar \alpha_t} z, \quad z \sim \mathcal{N}(0,I)
\end{align*}
with $\alpha_t = 1 - \beta_t$ and $\bar \alpha_t = \prod_{i=1}^t \alpha_i$. For the reverse process we have 
\begin{align*}
    x_{t-1} = \frac{1}{\sqrt \alpha_t} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar \alpha_t}} \epsilon_\theta(x_t,t) \right) + \sqrt{\frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_t} \beta_t} z, \quad z \sim \mathcal{N}(0,I)
\end{align*}
which can also be written as 
\begin{align*}
    x_{t-1} = \frac{\sqrt{\bar \alpha_{t-1}}\beta_t}{1 - \bar \alpha_t} x_0(x_t, \theta) + \frac{\sqrt{\alpha_t}(1 - \bar \alpha_{t-1})}{1 - \bar \alpha_t} x_t + \sqrt{\frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_t} \beta_t} z, \quad z \sim \mathcal{N}(0,I),
\end{align*}
with the Tweedie estimate
\begin{align*}
     x_0(x_t, \theta) = \frac{x_t - \sqrt{1 - \bar \alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar \alpha_t}}.
\end{align*}
With the additional h-transform (trained in the epsilon matching objective) this becomes:
\begin{align*}
     x_0^\text{cond}(x_t, \theta) = \frac{x_t - \sqrt{1 - \bar \alpha_t} \left(\epsilon_\theta(x_t, t) + h_\epsilon(x_t, t)\right)}{\sqrt{\bar \alpha_t}}
\end{align*}
For the fine-tuning, we should initialise $h_\epsilon(x_t, t)$ as 
\begin{align*}
    h_\epsilon(x_t, t) = \text{NN}_1(x_t, t) - \text{NN}_2(t) \sqrt{1 - \bar \alpha_t} \nabla_{x_0} r(x_0),
\end{align*}
due to the correspondence of score to noise as 
\begin{align*}
    \text{noise} = - \sqrt{1 - \bar \alpha_t} \text{score}.
\end{align*}

\subsection{Fine-tuning using relative Trajectory Balance}
In this section I briefly want to write down the relative trajectory balance loss \cite{venkatraman2024amortizing} for a diffusion model with DDPM schedule. Let $r(x_0)$ be the reward and we want to sample from $p^\text{post}(x) \propto p(x) \exp(r(x))$. Then the RTB loss is given as 
\begin{align}
    \mathcal{L}_\text{RTB}(x_0, \dots, x_T;\phi) = \left( \log \frac{Z_\phi p_\phi^\text{post}(x_0, \dots, x_T)}{\exp(r(x_0)) p_\theta(x_0, \dots, x_T)} \right)^2,
\end{align}
where $p_\phi^\text{post}$ is the path probability of the posterior (fine-tuned) model and $p_\theta$ the path probability of the prior (pre-trained) model. If we assume that both start at the same distribution, i.e., $p_\theta(x_T) = p_\phi^\text{post}(x_T)$ (which is often true, as both are standard Gaussian), this reduces to
\begin{align}
    \left( \log \frac{Z_\phi}{\exp(r(x_0))} + \sum_{t=1}^T \log \frac{p_\phi^\text{post}(x_{t-1}|x_t)}{p_\theta(x_{t-1}|x_t)} \right)^2,
\end{align}
where the second term contains log ratios of the backward steps. For DDPM the backward steps are given as 
\begin{align}
    \label{eq:backward_dens_prior}
    p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}| \frac{1}{\sqrt \alpha_t} ( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar \alpha_t}} \epsilon_\theta(x_t,t)); \tilde \beta_t^2 I), \\ \label{eq:backward_dens_post}
    p_\phi^\text{post}(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}| \frac{1}{\sqrt \alpha_t} ( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar \alpha_t}} \tilde\epsilon_\phi(x_t,t)); \tilde \beta_t^2 I).
\end{align}
Both are Gaussians and only have a different mean. Here, $\tilde \beta_t = \sqrt{\frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_t} \beta_t}$. 

The log ratio of two Gaussian reduces to 
\begin{align}
    \log \frac{\mathcal{N}(x; \mu_1, \Sigma_1)}{\mathcal{N}(x; \mu_2, \Sigma_2)} = - \frac{1}{2} [(x - \mu_1)^T \Sigma_1^{-1}(x - \mu_1) - (x - \mu_2)^T \Sigma_2^{-1}(x - \mu_2)] + \frac12 \log \frac{|\Sigma_2|}{|\Sigma_1|},
\end{align}
so for the log ratio in RTB we get 
\begin{align}
    \log \frac{p_\phi^\text{post}(x_{t-1}|x_t)}{p_\theta(x_{t-1}|x_t)} = - \frac{1}{2 \tilde \beta_t^2} \left[ \| x_{t-1} - \mu_\phi^\text{post}(x_t,t) \|_2^2 - \| x_{t-1} - \mu_\theta(x_t,t) \|_2^2 \right],
\end{align}
where $\mu_\phi^\text{post}$ and $\mu_\theta$ are the means in Eqn.~\eqref{eq:backward_dens_post} and Eqn.~\eqref{eq:backward_dens_prior}. We can even simplify this expression a bit 
\begin{align}
    \log \frac{p_\phi^\text{post}(x_{t-1}|x_t)}{p_\theta(x_{t-1}|x_t)} = - \frac{1}{2 \tilde \beta_t} \left[ 2 (\mu_\theta(x_t, t) - \mu_\phi^\text{post})^T x_t + (\| \mu_\phi^\text{post} \|_2^2 - \| \mu_\theta \|_2^2)  \right] 
\end{align}
If we assume that the posterior mean is the original mean plus a delta, i.e., $\mu_\phi^\text{post} = \mu_\theta + h_\phi$, we even get 
\begin{align}
    \log \frac{p_\phi^\text{post}(x_{t-1}|x_t)}{p_\theta(x_{t-1}|x_t)} = - \frac{1}{2 \tilde \beta_t} \left[ -2 (h_\phi)^T x_t + 2 (\mu_\theta)^T x_t+ \| h_\phi \|_2^2)  \right] 
\end{align}



\textbf{Note, that the RTB loss function only needs trajectories. They do not have to come from the posterior and further, we do not have to backpropagate through the trajectory.}

\subsection{Computation of Importance Weights}
The controlled reverse SDE 
\begin{align}
    \label{eq:controlled_back_sde}
    \dd \bX_t &= \left( f_t(\bX_t) - \sigma_t^2 ( \nabla_{\bX_t} \ln p_t(\bX_t) + h_t(\bX_t)) \right) \,\dd t + \sigma_t \bwd{ \dd \rv{W}}_t, \quad \bX_T \sim  \distT, 
\end{align}
induces some distribution $p^h$ at $t=0$. We are interested in importance weights of the form 
\begin{align}
    w(\vx) = \frac{p_\text{tilted}(\vx)}{p^h(\vx)}
\end{align}
However, calculating these weights is expensive and requires the simulation of the probability flow ODE. Instead, we use approximated importance weights. 



Let $\mathbb{P}_\text{data}$ be the path probability of 
\begin{align}
    \dd \bX_t &= \left( f_t(\bX_t) - \sigma_t^2 \nabla_{\bX_t} \ln p_t(\bX_t)  \right) \,\dd t + \sigma_t \bwd{ \dd \rv{W}}_t, \quad \bX_T \sim  \distT, 
\end{align}
and $\mathbb{P}_h$ the path probability of 
\begin{align}
    \dd \bH_t &= \left( f_t(\bH_t) - \sigma_t^2( \nabla_{\bH_t} \ln p_t(\bH_t) + h_t(\bH_t)) \right) \,\dd t + \sigma_t \bwd{ \dd \rv{W}}_t, \quad \bH_T \sim  \distT. 
\end{align}
For the approximated importance weights we require the RND of $\mathbb{P}_h$ with respect to $\mathbb{P}_\text{data}$ evaluated at trajectories from $\mathbb{P}_h$. The drift difference evaluates as 
\begin{align}
     \left( f_t(\vx_t) - \sigma_t^2( \nabla_{\vx_t} \ln p_t(\vx_t) + h_t(\vx_t)) \right) - \left( f_t(\vx_t) - \sigma_t^2 \nabla_{\vx_t} \ln p_t(\vx_t) \right) = - \sigma_t^2 h_t(\vx_t),
\end{align}
this gives us
\begin{align}
   \ln  \left( \frac{d \bwd{\sP}_\text{h}}{d \bwd{\sP}_\text{data}} \right)(\bH) = - \sigma_t \int_0^T  h_t(\bH_t) d \bwd{\bW}_t + \frac12 \int_0^T \sigma_t^2 \| h_t(\bH_t) \|^2 dt
\end{align}
as we assume that both SDE start with the same initial distribution. \Alex{Are the signs correct? We are looking at reverse SDEs here. Looking at equation (64) in \cite{nusken2024transport}, we get} 
\begin{align}
    \ln \left( \frac{d \bP_{\mu, a}}{d \bP_{\nu, b}} \right)(\bY) = \ln\left( \frac{\dd \mu}{\dd \nu} \right)(\bY_T) + \frac{1}{\sigma_t^2} \int_0^T (a_t - b_t)(\bY_t) \cdot \dd \bwd{\bY_t} + \frac{1}{2 \sigma_t^2} \int_0^T (b_t^2 - a_t^2)(\bY_t) \dd t, 
\end{align}
for path probabilities $\bP_{\mu, a}$ and $\bP_{\nu, b}$.
\Alex{In this setting, what exactly are the SDEs corresponding to these path probabilities? Can you put this in?}
\begin{itemize}
    \item $\bP_{\mu, a}$ is the path probability of 
        \begin{align}
             \dd \bX_t = \dots 
        \end{align}
    \item $\bP_{\nu, b}$ is the path probability of
    \begin{align}
            \dd \bX_t = \dots 
    \end{align}
\end{itemize}


\section{Applications of Controlled Generation}
We discuss possible applications of controlled generation in the following paragraphs.

\paragraph{Statistical inverse problems} The goal in inverse problems is to reconstruct an image $\vx \in \R^n$ from indirect and noisy measurements $\m \in \R^m$, where the relationship is given by 
\begin{align}
    \m = \Pm(\vx) + \eta,
\end{align}
with a forward operator $\Pm: \R^n \to \R^m$ and $\eta \sim \mathcal{N}(0, \sigma_\eta^2 \mathbf{I}_m)$ additive Gaussian noise. Here, we are interested in sampling from the posterior distribution $p_\text{post}(\vx|\m) \propto p_\text{data}(\vx) p_\text{lkhd}(\m | \vx)$. For the additive Gaussian noise model the log-likelihood is given by 
\begin{align}
    \ln p_\text{lkhd}(\m | \vx) = - \frac{1}{2 \sigma_\eta^2} \| \Pm(\vx) - \m \|_2^2 + \text{constant}. 
\end{align}
Considering the setting in \eqref{eq:tilted_dist}, we can set $r(\vx) = - \frac{1}{2 \sigma_\eta^2} \| \Pm(\vx) - \m \|_2^2$ to enable posterior sampling for inverse problems.




\section{Old Version of Section~\ref{sec:selfsupervised} \Alex{Can be deleted?}}

To circumvent the requirement for a fine-tuning dataset, we propose a self-supervised method for fine-tuning diffusion models. For this, we combine the recent rejection sampling steps from \cite{hertrich2024importance} with the supervised fine-tuning from Theorem~\ref{th:deft} (ii).

In particular, let $h^{\theta}_t(\vx_t)$ be the current estimation to the $h$-transform. We iteratively refine the model using synthetic data sampled from the reverse SDE
\begin{align}
    \label{eq:back_sde_h}
    \dd \bH_t &= \left( f_t(\bH_t) - \sigma_t^2 s_t(\bH_t) - \sigma_t^2 h^\theta_t(\bH_t) \right) \,\dd t + \sigma_t \bwd{ \dd \rv{W}}_t. 
\end{align}
Starting with some initial approximation $h^\theta_t$, we sample a batch $\{ \vx_0^{(i)}\}, i=1,\dots,N$ from this reverse SDE. For each sample, we compute approximate importance weights 
\begin{align}
   \alpha_i = \alpha(\vx_0^{(i)}) \propto \frac{p_\text{tilted}(\vx_0^{(i)})}{p_{h}(\vx_0^{(i)})},
\end{align}
with $p_h$ as the marginal of the current model estimate. Using these weights, we perform a weighted resampling of the batch, favouring samples with higher $\alpha_i$. The $h$-transform is then updated using the supervised loss derived in Theorem~\ref{th:deft} (ii)
\begin{align}
\label{eq:self_supervised_deft}
    \min_h \mathop{\mathbb{E}}_{\substack{ \bX_0 \sim \tilde p_h(\vx_0) \\ t\sim \mathrm{U}(0,T), \bH_t \sim \fwd{p}_{t|0}(\vx_t|\vx_0)}  }\left[ \left\| \left({h_t(\bH_t)}\!+\!s_t(\bH_t) \right)\!-\!\nabla_{\bH_t}\ln \fwd{p}_{t|0}(\bH_t  |\bX_0) \right\| ^2\right], 
\end{align}
where $\tilde p_h(\vx_0)$ denotes the distribution after resampling.

As motivation for this, we show in Theorem 2 that resampling reduces the loss in Equation~\eqref{eq:tilted_dist_opt}.

The entire process is iterated: sampling from the reverse SDE with the updated $h$-transform, recalculating importance weights, resampling, and fine-tuning. The full algorithm is given in Algorithm~\ref{alg:self_supervised}. This iterative refinement reduces the divergence between the model distribution and the target tilted distribution. \Alex{Hope we can show this.}

%We further add accepted samples $\vx_0^{(i)}$ to an unpriotised replay buffer. We set a maximum size for the buffer. Once this size is reached the oldest samples will be discarded. The full method is given in Algorithm \ref{alg:self_supervised}.





\subsection{Approximate Importance Weights}
Basically all resampling steps need to compute the importance weights $\frac{p_\text{tilted}(\vx)}{p_h(\vx)}$ up to a multiplicative constant, where $p_h$ is the density of the distribution induced by the reverse SDE with the current estimation of the $h$-transform. To this end we note that for $p_\text{tilted}(\vx) \propto p_\text{data}(\vx) \exp(r(\vx))$ we have
\begin{align}
    \frac{p_\text{tilted}(\vx)}{p_h(\vx)}=\frac{p_\text{tilted}(\vx)}{p_\text{data}(\vx)}\frac{p_\text{data}(\vx)}{p_h(\vx)}\propto\exp(r(\vx))\frac{p_\text{data}(\vx)}{p_h(\vx)}
\end{align}
Since we cannot compute the quotient $\frac{p_\text{data}(\vx)}{p^h(\vx)}$ exactly, we approximate it by the ELBO from \cite{denker2024deft}, given by
\begin{align}
\frac{p_\text{data}(\vx)}{p_h(\vx)}=\exp\left(-\log\left(\frac{p_h(x)}{p_\text{data}(\vx)}\right)\right) &\approx \exp\left(-\log\left(\frac{\dd \sP_h}{\dd \sP_\text{data}}(\vx_{0:T})\right)\right) \\ &=\exp \left( - \frac12 \int_0^T \sigma_t^2\| h_t(\vx_t) \|_2^2  dt + \int_0^T \sigma_t h_t(\vx_t)^\top dW_t\right),\label{eq:rnd}
%\exp(-D_\text{KL}(p || p_\text{diff}))\approx \exp(-D_\text{KL}(\sP || \sP_\text{diff}))\\
%&=\exp\left(-\mathbb{E}_{x_t \sim \sP}\left[\int_0^T \sigma_t^2\| h_t(x_t) \|_2^2  dt\right]\right),
\end{align}
where $\sP_h$ and $\sP_\text{data}$ are the path densities for the SDEs corresponding to the prior and adjusted diffusion model. See also Appendix \ref{app:RND_SDE} for a derivation, both in continuous and in discrete time. In particular, we use the framework presented in \cite{nusken2024transport} to compute the Radon-Nikodym derivative. Note that in practice, we are doing all computations in the log-space and we will also only use the log-values of them in the following paragraph.

In total, we compute path-wise importance weights of the form 
\begin{align}
    \frac{p_\text{data}(\vx_{[0:T]})e^{r(\vx_0)}}{p_h(\vx_{[0:T]})} = \exp \left( r(\vx_0) - \frac12 \int_0^T \sigma_t^2\| h_t(\vx_t) \|_2^2  dt + \int_0^T \sigma_t h_t(\vx_t)^\top dW_t\right),
\end{align}
where $\vx_{[0:T]} \sim \sP_h$. These path-wise importance weights can be computed concurrently to sampling without any computational overhead.

\subsection{Resampling Step}
Now, one possibility to implement the second step from above efficiently is the relaxed rejection / resampling step proposed in \cite{hertrich2024importance}. It consists out of the following two steps and (for exact importance weights) has the unique fixed point given by the target distribution.
\begin{itemize}
\item Compute acceptance rate $\alpha(x_i)=\min\{1,\frac{q(x)}{cp(x)}\}$, where $c$ is a hyperparameter
\item With probability $\alpha(x_i)$ accept $x_i$, otherwise resample $x_i$ from $p$. \Alex{is this necessary for the proof?} {\color{green}JH: No. In the theorem we just skip the resampling.}
\end{itemize}
In practice, the hyper parameter $c$ can easily be chosen such that a certain rate of the samples is rejected (I think we used $20\%$), which worked well. Moreover, one can compute the density ratio between the distribution before and after the rejection step such that several of these steps can be stacked.

The following theorem is analogously to Proposition 6 in \cite{hertrich2024importance}. For completeness we include the proof in Appendix \ref{app:proof_resampling}.
\begin{theorem}\label{the:app}
Let $\alpha(\vx_{[0:T]})=\min\left(1,\frac{\dd \sP_\text{data}}{\dd \sP_h}(\vx_{[0,T]}) \frac{\exp(r(\vx_0))}{c}\right)$ be the acceptance probability of a sample $\vx_{[0,T]}\sim \sP_h$ and denote by $\tilde{\sP}_h$ the distribution of the accepted paths.
Then, the following holds true:
\begin{itemize}
    \item[(i)] $\frac{\dd \tilde{\sP}_h}{\dd  \sP_h}(\vx_{[0,T]}) = \frac{\alpha(\vx_{[0,T]})}{\mathbb{E}_{\vx_{[0,T]}\sim\sP_h}[\alpha(\vx_{[0,T]})]}$
    \item[(ii)] $\mathcal F(\tilde{\sP}_h)\leq \mathcal F(\sP_h)$, where $\mathcal F(\sP)=\KL(\sP,\sP_\mathrm{data})-\mathbb{E}_{\vx_{[0,T]}\sim \sP}[r(\vx_0)]$
    \item[(iii)] Let $h_t^*\in\argmin_{g_t}\mathcal L_{FT}(g_t)$, where
\begin{align}\label{eq:ft_score_matching}
    \mathcal L_{FT}(g_t)= \mathop{\mathbb{E}}_{\substack{ \vx_0 \sim \tilde{\sP}_h^0 \\ t\sim \mathrm{U}(0,T), \vx_t \sim \fwd{p}_{t|0}(\cdot|\vx_0)}  }\left[ \left\| \left({g_t(\vx_t)}\!+\!s_t(\vx_t) \right)\!-\!\nabla_{\vx_t}\ln \fwd{p}_{t|0}(\vx_t  |\vx_0) \right\| ^2\right].
\end{align}
    and let $\sP_{h^*}$ be the path measure of the corresponding SDE.
    Then it holds 
    $
    \mathcal F(\sP_{h^*})\leq\mathcal F(\tilde{\sP}_h)\leq\mathcal F(\sP_h).
    $
    \item[(iv)] {\color{red} Do we have $\KL(\tilde{\sP}_h^0, p_\text{tilted}) \le \KL(\sP_h^0, p_\text{tilted})$ for the marginals at $t=0$ or $\KL(\tilde{\sP}_h, \sP_\text{tilted}) \le \KL(\sP_h, \sP_\text{tilted})$ for the complete path?}
\end{itemize}
\end{theorem}


\begin{proof}

Via doing score matching we are minimising:

\begin{align}
   \min_\theta \mathrm{KL}\left(\sP_{\mathrm{data}} \frac{\tilde{\sP}_h^0}{p_{\mathrm{data}}} \Big|\Big|  \mathbb{Q}_\theta\right)
\end{align}

Let $\sP^* = \sP_{\mathrm{data}} \frac{\tilde{\sP}_h^0}{p_{\mathrm{data}}} $  and $\sP^{\mathrm{tilt}} = \sP^{\mathrm{data}}e^r$ then we want to compare $\KL(\sP^{\mathrm{tilt}} || \sP^*)$ to  $\KL(\sP^{\mathrm{tilt}} || \sP^h)$ then

\begin{align}
    \KL(\sP^* ||\sP^{\mathrm{tilt}}  ) &=  \E\left[\ln \frac{  \dd \sP_{\mathrm{data}} \frac{\tilde{\sP}_h^0}{p_{\mathrm{data}}}} {\dd \sP^{\mathrm{data}}e^r}\right] \\
    &=\E\left[\ln \frac{\tilde{\sP}_h^0}{e^r p_{\mathrm{data}}}\right]  = \KL ( \tilde{\sP}_h^0 || p^{\mathrm{tilted}})
\end{align}
and 
\begin{align}
    \KL(\sP^h || \sP^{\mathrm{tilt}} ) = \E\left[\int_0^T\sigma_t^{2}\vert\vert h(\vx_t) + (\nabla_{x_t} \ln p^h_t(\vx_t)  - \nabla_{x_t} \ln p^{\mathrm{data}}_t(\vx_t) )\vert\vert^2\right] + \KL(\sP^h_0 \vert\vert p_{\mathrm{data}} e^r)
\end{align}
Where $\E\left[\int_0^T\sigma_t^{2}\vert\vert h(\vx_t) + (\nabla_{x_t} \ln p^h_t(\vx_t)  - \nabla_{x_t} \ln p^{\mathrm{data}}_t(\vx_t) )\vert\vert^2\right] \geq 0$ thus if $\KL(\sP^h_0 \vert\vert p^{\mathrm{tilted}}) > \KL ( \tilde{\sP}_h^0 || p^{\mathrm{tilted}})$ which I think it is by Theorem 2 ii) then we are done.


Is this true ?
$$\KL(\sP^h_0 \vert\vert p^{\mathrm{tilted}}) > \KL ( \tilde{\sP}_h^0 || p^{\mathrm{tilted}})$$
% \begin{align}
%     \KL(\sP^h || \sP^{\mathrm{tilt}} ) = \E\left[\int_0^T\sigma_t^{2}\vert\vert h(\vx_t) + (\nabla_{x_t} \ln p^h_t(\vx_t)  - \nabla_{x_t} \ln p^{\mathrm{data}}_t(\vx_t) )\vert\vert^2\right] + \E\left[ \ln \frac{\sP^h_0}{p_{\mathrm{data}} e^r} \right]
% \end{align}
\end{proof}



\bibliographystyle{apalike}
\bibliography{bibliography}

\end{document}