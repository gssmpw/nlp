\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
%\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{graphicx,xcolor}
\usepackage{booktabs}

% Useful packages
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
%\usepackage{bm}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{tikz}
\usepackage[framemethod=tikz]{mdframed}
\usetikzlibrary{calc}
\usepackage{subcaption}
\usepackage{mathtools}

\usepackage{algorithm}
\usepackage{algorithm}
\usepackage[noEnd=true, indLines=true, spaceRequire=false, italicComments=false, rightComments=true, commentColor=gray]{algpseudocodex}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}


\newcommand{\Alex}[1]{\leavevmode \textcolor{red}{Alex: #1}}

\newcommand{\Shreyas}[1]{\leavevmode \textcolor{brown}{Shreyas: #1}}



\mdfdefinestyle{highlightedBox}{
    linecolor=gray!30,          % Border color
    backgroundcolor=gray!10,   % Background color
    innertopmargin=5pt,       % Space before the start of the content
    innerbottommargin=2pt,    % Space after the end of the content
    roundcorner=4pt            % Round corner radius
    innerleftmargin=0pt,
    innerrightmargin=3pt,
    rightmargin=-0.2cm,
    leftmargin=-0.2cm
}

\input{commands/math.tex}
\input{commands/notation.tex}
% Self-Supervised
\title{Iterative Importance Fine-tuning \\ of Diffusion Models}
% Iterative Importance Fine-tuning of Diffusion Models
% Iterative Importance-Based Refinement of Diffusion Models for Conditional Sampling ? JH: skip the "-Based"?
% Conditional Sampling via Iterative Refinement of Diffusion Models with Path-Based Importance Weights ? 
% Fine-tuning Diffusion Models for Conditional Sampling without Ground truth data --> JH: Here I don't like that the iterative nature is not represented (you could also write this title for the stochastic control loss from your DEFT paper)
% Importance-based estimation of the h-transform for conditional sampling with diffusion models --> JH: people would not know what the h-transform is...
% ...
\author{Alexander Denker \hspace{2.65cm} Shreyas Padhy\\
University College London \hspace{1.58cm} University of Cambridge\\
\texttt{a.denker@ucl.ac.uk} \hspace{1.6cm} \texttt{sp2058@cam.ac.uk} 
%\And
%Shreyas Padhy \\
%University of Cambridge \\
%\texttt{sp2058@cam.ac.uk} 
\AND
Francisco A. Vargas Palomo\\
University of Cambridge \\
\texttt{fav25@cam.ac.uk} 
\And  
Johannes Hertrich \\
UniversitÃ© Paris Dauphine-PSL \\
\texttt{johannes.hertrich@dauphine.psl.eu}  
}

\iclrfinalcopy 
\begin{document}
\maketitle

% TLDR: An iterative refinement framework for fine-tuning diffusion models using path-based importance-weighted resampling without requiring additional datasets
\begin{abstract}
Diffusion models are an important tool for generative modelling, serving as effective priors in applications such as imaging and protein design. A key challenge in applying diffusion models for downstream tasks is efficiently sampling from resulting posterior distributions, which can be addressed using the $h$-transform. This work introduces a self-supervised algorithm for fine-tuning diffusion models by estimating the $h$-transform, enabling amortised conditional sampling. Our method iteratively refines the $h$-transform using a synthetic dataset resampled with path-based importance weights. We demonstrate the effectiveness of this framework on class-conditional sampling and reward fine-tuning for text-to-image diffusion models.  
\end{abstract} % Submission deadline is 3 February, 2025.

\section{Introduction}
Diffusion models have emerged as a powerful tool for generative modelling \citep{ho2020denoising,dhariwal2021diffusion}. As training these models is expensive and requires large amount of data, fine-tuning existing models for new tasks is of interest. In particular, given large pre-trained foundation models such as Stable Diffusion \citep{rombach2022high} for images or RFDiffusion \citep{watson2023novo} for protein generation, the goal is to use the model as a prior for various downstream applications. For example, in imaging applications, the same diffusion model might be used as a prior for inpainting, deblurring or super-resolution tasks \citep{rout2024solving}. % This adaptation is often referred to as controlled generation or alignment. 

Sampling from the resulting conditional distribution can be interpreted as sampling from a tilted distribution \citep{domingo2024adjoint}, where the distribution defined by the pre-trained diffusion models $p_\text{data}(\vx)$ is tilted by some reward or likelihood function $r:\R^n \to \R$. Then, we define the tilted distribution as
\begin{align}
    \label{eq:tilted_dist}
    p_\text{tilted}(\vx) \propto p_\text{data}(\vx) \exp\left(\frac{r(\vx)}{\lambda} \right),
\end{align}
with a temperature $\lambda > 0$. This general framework includes several applications, such as statistical inverse problems with $r(\vx) = \ln p(\vy|\vx)$ as the log-likelihood given observed measurements $\vy$; class conditional sampling with $r(\vx)=\ln p(c|\vx)$ as the log-class probabilities for a class $c$ or reward fine-tuning where $r(\vx)$ is learned explicitly as an image reward \citep{xu2024imagereward}. The tilted distribution can also be expressed as the minimiser of an optimisation problem, i.e.,
\begin{align}
    \label{eq:tilted_dist_opt}
    p_\text{tilted} = \arg \min_p \left\{ - \E_{\vx \sim p}[r(\vx)] + \lambda \KL(p, p_\text{data}) \right\}.
\end{align} 
Here, the first term maximises the reward, and the second term acts as a regulariser with the temperature $\lambda$ as the regularisation strength. In the following, we directly incorporate $\lambda$ into the reward $r$ for an easier notation. 

We can sample from the tilted distribution by adding an additional term, the generalised $h$-transform, to the drift function of the reverse SDE \citep{vargas2023bayesian}. The $h$-transform is in general intractable in closed form and thus many works propose approximations, see for example  \citep{jalal2021robust,chung2022diffusion,rout2024solving}. As an alternative, \cite{denker2024deft} propose a supervised framework to estimate the $h$-transform given a small dataset from the tilted distribution. However, this limits the method to domains where such a dataset is available. In this work, we propose a self-supervised framework for estimating the $h$-transform without access to samples from the tilted distribution. In particular, we iteratively sample from the diffusion model, resample on path-based importance weights and fine-tune the model using the resampled data. 

%\subsection{Related Work}

\paragraph{Controlled Generation from Diffusion Models}
Controlled generation for diffusion models can be achieved via inference-time or post-training methods \citep{uehara2025reward}. Inference-time methods, such as classifier guidance \citep{dhariwal2021diffusion} or reconstruction guidance \citep{chung2022diffusion}, guide the reverse diffusion process without additional training but typically increase computational cost and are often sensitive to hyperparameters \citep{song2023solving}. Post-training techniques instead fine-tune models for a specific application. Fine-tuning comes with a higher initial computational cost but often results in reduced sampling time compared to inference-time methods \citep{denker2024deft}. Supervised post-training methods require an additional task-specific dataset for fine-tuning \citep{ruiz2022dreambooth,zhang2023adding,xu2024imagereward}. In contrast, online post-training methods directly optimise some objective given by the reward function via reinforcement learning \citep{venkatraman2024amortizing,clark2023directly,fan2024reinforcement,black2024training} or stochastic optimal control \citep{denker2024deft,domingo2024adjoint}. 


\paragraph{Iterative retraining can cause model degradation} Iterative retraining of generative models on synthetic data has been shown to lead to performance degradation, including phenomena such as mode collapse \citep{alemohammad2023self,shumailov2023curse}. Strategies to mitigate this issue have been proposed, such as mixing synthetic data with real data \citep{bertrand2023stability}. Alternatively, training on curated synthetic datasets, i.e., choosing the synthetic data based on the reward $r$, has also been demonstrated to improve retraining stability and model performance \citep{ferbach2024self}. As we re-sample the synthetic dataset based on the importance weights, our approach also falls into the category of retraining using curated data. However, as opposed to \cite{ferbach2024self} our selection process incorporates both the reward and the path probability, which can mitigate risks associated with iterative retraining.
 
\paragraph{Self-supervised training for sampling} Recently, a number of self-supervised frameworks have been proposed for sampling from unnormalised densities, for example FAB \citep{midgley2022flow} or iDEM \citep{akhound2024iterated}. In iDEM, the authors propose an iterative framework, where they train a diffusion model with a stochastic regression loss based on data sampled from the current estimation of the diffusion model.  

The rest of the paper is structured as follows. In Section \ref{sec:background} we give the necessary background on diffusion models and supervised fine-tuning. The path-based importance weights and the resampling step are presented in Section \ref{sec:selfsupervised}. Lastly, we present experiments on class conditional sampling for MNIST, and reward-based fine-tuning of Stable Diffusion \citep{rombach2022high} in Section \ref{sec:experiments}.

\section{Background}
\label{sec:background}
Let us first recap the score-based generative modelling framework of \cite{song2021Scorebased}; we start with a forward SDE, which progressively transforms the target distribution $p_{\mathrm{data}}$
\begin{align}
    \dd \bX_t &= f_t(\bX_t) \,\dd t + \sigma_t \fwd{ \dd \rv{W}}_t, \quad \bX_0 \sim  p_{\mathrm{data}},
\end{align}
with drift $f_t$ and diffusion $\sigma_t$. As usual, we denote by $p_t$ the density of the solution $\bX_t$ at time $t$ and by $\fwd{p}_{t_1|t_2}(\vx_{t_1}|\vx_{t_2})$ the conditional densities of $\bX_{t_1}$ given $\bX_{t_2}$. Under some regularity assumptions, there exists by \cite{anderson1982reverse} a corresponding reverse SDE, that allows sampling from $\sP_T$ (typically $\gN(0,\mathbf{I}_n)$) and denoising them to generate samples from $p_{\mathrm{data}}$. The reverse SDE is given~by 
\begin{align}
    \label{eq:back_sde}
    \dd \bX_t &= \left( f_t(\bX_t) - \sigma_t^2 s_t(\bX_t) \right) \,\dd t + \sigma_t \bwd{ \dd \rv{W}}_t, \quad \bX_T \sim  \sP_T, 
\end{align}
where the time flows backwards and $s_t(\vx)=\nabla_\vx\ln p_t(\vx)$ is the score function.

To sample from the tilted distribution $p_\mathrm{tilted}$ instead of $p_\mathrm{data}$, we consider an additional guidiance term in the reverse SDE. More precisely, we consider the SDE
\begin{align}
    \label{eq:rev_sde2trans_2}
    \dd \bH_t &= \left( f_t(\bH_t) - \sigma_t^2 \left(s_t(\bH_t)+h_t(\bH_t)\right) \right) \,\dd t + \sigma_t \bwd{ \dd \rv{W}}_t,
\end{align}
where the time flows again backwards and we use $\bH_t$ for the guided reverse SDE. 
To ensure that $\bH_0\sim p_\mathrm{tilted}$, \citet{denker2024deft,vargas2023bayesian} considered a generalisation of Doob's $h$-transform. In particular, they proved the following theorem. 

%\begin{mdframed}[style=highlightedBox]
\begin{theorem}[Proposition 2.2 and Theorem 3.1 in \citealp{denker2024deft}]\label{th:deft}
~\newline 
Assume $Z_r\coloneqq\int \exp(r(\vx_0))\d \vx_0<\infty$. Further, let $Q_T^{f_t}[p_\mathrm{tilted}]\coloneqq\int \fwd{p}_{T|0}(\vx| \vx_0) p_\mathrm{tilted}(\vx_0) \d \vx_0$ and 
\begin{equation}\label{eq:gen_h_trafo}
h_t^*(\vx)=\nabla_\vx \ln p_t^r(\vx),\quad\text{where}\quad p^r_{t}(\vx) = \int \frac{\exp(r(\vx_0))}{Z_r} \bwd{p}_{0|t}(\vx_0 |\vx) \dd \vx_0
.
\end{equation}
Then, the following holds true:
\begin{itemize}
\item[(i)] Let $(\bH_t)_t$ be the solution of the SDE \eqref{eq:rev_sde2trans_2} with $h_t=h_t^*$ given in \eqref{eq:gen_h_trafo} and $\bH_T\sim Q_T^{f_t}[p_\mathrm{tilted}]$. Then, it holds $\bH_0\sim p_\mathrm{tilted}$.
\item[(ii)] It holds that $h_t^*$ is the unique minimiser of the loss function
\begin{align}
\label{eq:supervised_deft}
    \mathcal L_{SM}(h_t)= \mathop{\mathbb{E}}_{\substack{ \vx_0 \sim p_\mathrm{tilted} \\ t\sim \mathrm{U}(0,T), \vx_t \sim \fwd{p}_{t|0}(\cdot|\vx_0)}  }\left[ \left\| \left({h_t(\vx_t)}\!+\!s_t(\vx_t) \right)\!-\!\nabla_{\vx_t}\ln \fwd{p}_{t|0}(\vx_t  |\vx_0) \right\| ^2\right].
\end{align}
\item[(iii)] It holds that
\begin{equation}
\label{eq:control_deft}
h_t^*\in\argmin_{h_t}\mathcal F(\sP_h),\quad\text{where}\quad\mathcal F(\sP)=\KL(\sP,\sP_\mathrm{data})-\mathbb{E}_{\vx_{[0,T]}\sim \sP}[r(\vx_0)]
\end{equation}
and where $\sP_h$ and $\sP_\mathrm{data}$ are the path measures of the corresponding SDEs \eqref{eq:rev_sde2trans_2} and \eqref{eq:back_sde}.
\end{itemize}
\end{theorem}
%\end{mdframed}

%Conditioning the reverse SDE to sample from the tilted distribution can be achieved by using the generalised $h$-transform \citep{denker2024deft,vargas2023bayesian}. Here, we state the result again for completeness. Note, that we will refer to the conditional process with $\bH_t$ and to the unconditional process with $\bX_t$. 

%\begin{mdframed}[style=highlightedBox]
%\begin{proposition}[Generalised $h$-transform \citep{denker2024deft}]
%\label{prop:noisy_cond}
%Given the following reverse SDE with marginals $p_t$
%\begin{align} 
%    \dd \bX_t &=  \bwd{b}_t(\bX_t) \,\dd t + \sigma_t\; \bwd{ \dd \rv{W}}_t,  \quad \bX_T \sim  \sP_T ,\label{eq:rev_sde2trans_21}
%\end{align}
%then it follows that the backward SDE 
%\begin{align} 
%    \bH_T &\sim  Q_T^{f_t} [p_\mathrm{tilted}(\vx_0) ] = \int \fwd{p}_{T|0}(\vx| \vx_0) p_\mathrm{tilted}(\vx_0) \dd \vx_0 \nonumber\\
%    \dd \bH_t &= \left( \bwd{b}_t(\bH_t) - \sigma^2_t  \cbox{\nabla_{\bH_t} \ln p^r_{t}(\bH_t)}\right)\,\dd t + \sigma_t\; \bwd{ \dd \rv{W}}_t, \label{eq:rev_sde2trans_2}
%\end{align}
%satisfies $\law{\bH_0} = p_\mathrm{tilted}(\vx_0)$
%with $p^r_{t}(\vx) = \int \frac{\exp(r(\vx_0))}{Z_r} \bwd{p}_{0|t}(\vx_0 |\vx) \dd \vx_0$, where $Z_r$ is the normalisation constant of $\exp(r(\vx))$. We recover guidance based diffusions via $\bwd{b}_t(\bH_t) = f_t(\bH_t) - \sigma_t^2 \nabla_{\bH_t} \ln p_t(\bH_t)$. 
%
%\end{proposition}
%\end{mdframed}

%Given access to samples $\vx_0 \sim p_\text{tilted}(\vx)$, the $h$-transform can be recovered using a supervised score matching loss function. This is stated in the following Theorem: 

%\begin{mdframed}[style=highlightedBox]
%\begin{theorem}[Supervised fine-tuning \citep{denker2024deft}] \label{th:supervised_finetuning}
%We can recover the generalised $h$-transform as the minimiser of 
%\begin{align}
%\label{eq:supervised_deft}
%    h^* = \arg \min_h \mathop{\mathbb{E}}_{\substack{ \bX_0 \sim p_\mathrm{tilted}(\vx_0) \\ t\sim \mathrm{U}(0,T), \bH_t \sim \fwd{p}_{t|0}(\vx_t|\vx_0)}  }\left[ \left\| \left({h_t(\bH_t)}\!+\!\nabla_{\bH_t} \ln p_t(\bH_t) \right)\!-\!\nabla_{\bH_t}\ln \fwd{p}_{t|0}(\bH_t  |\bX_0) \right\| ^2\right] 
%\end{align}
%\end{theorem}
%\end{mdframed}

\begin{remark}
In Theorem~\ref{th:deft} (i) the terminal distribution is given as $Q_T^{f_t}[p_\mathrm{tilted}]$ instead of $\sP_T$ in Equation~\eqref{eq:back_sde} to remove the value function bias. In \cite{domingo2024adjoint}, the authors deal with the value function bias by altering the noise of the controlled SDE. However, due to the mixing time in the commonly used VP-SDE the discrepancy, measured w.r.t.~the total variation distance, decays exponentially \citep[Proposition G.2]{denker2024deft} and we approximate $Q_T^{f_t}[p_\mathrm{tilted}] \approx \sP_T \approx \mathcal{N}(0,\mathbf{I}_n)$ for all numerical experiments.
\end{remark}

Part (i) of Theorem \ref{th:deft} states conditions on $h_t$ such that $\bH_0\sim p_\mathrm{tilted}$ in \eqref{eq:rev_sde2trans_2} and the (ii), (iii)  provide loss functions to learn such a $h_t$. For the commonly used VP-SDE \citep{song2021Scorebased}, the loss \eqref{eq:supervised_deft} in part (ii) reduces to
\begin{align}
       \mathcal L_{SM}(h_t)= \mathop{\mathbb{E}}_{\substack{  \vx_0 \sim p_\mathrm{tilted} \\ t\sim \mathrm{U}(0,T), \vz\sim\mathcal N(0,I)}}\left[ \left\| \left({h_t(\gamma_t \vx_0+\nu_t\vz)}\!+\!s_t(\gamma_t \vx_0+\nu_t\vz) \right)- \vz / \nu_t \right\| ^2\right], 
\end{align}
where the parameters $\gamma_t, \nu_t$ are specified by the drift and diffusion coefficients. However, samples from the tilted distribution are often not available, making this approach limited in practice. 
Moreover, the loss function in \eqref{eq:control_deft} can be reformulated as
\begin{align}
\label{eq:online_loss}
\mathcal L_{SC}(h_t)=\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\frac12\int_0^T\sigma_t^2\|h_t(\vx_t)\|^2\d t-r(\vx_0)\right].
\end{align}
Differentiating $\mathcal L_{SC}$ requires differentiating through the sample generation of the SDE. Even though \citet{denker2024deft} reduce the computational cost of this step by using the VarGrad \citep{richter2020vargrad} or trajectory balance loss \citep{malkin2022trajectory}, the memory requirements for optimising  $\mathcal L_{SC}$ still scale linearly with the number of steps in the SDE generation. %In particular, this loss cannot be used for too large models.


\section{Self-Supervised Importance Fine-tuning}
\label{sec:selfsupervised}
To circumvent the limitations of the loss functions from \citet{denker2024deft}, we propose in this section a self-supervised method for fine-tuning diffusion models. To this end, we combine the recently proposed rejection sampling steps from \cite{hertrich2024importance} with the supervised fine-tuning from Theorem~\ref{th:deft} (ii) and prove that this defines a descent algorithm for the loss function $\mathcal{F}$ from Theorem~\ref{th:deft}.

\subsection{Importance-based Rejection}

A basic tool for reweighting and fine-tuning generative models are importance weights. To this end, let $\vx_0$ be generated from the backward SDE \eqref{eq:rev_sde2trans_2} for some guidance term $h$. We assign to it the weight $\frac{p_\mathrm{tilted}(\vx_0)}{p_h(\vx_0)}$, where $p_h$ is the density of the generated distribution. Then, samples with a large importance weights are under-represented and samples with small importance weights are over-represented in $p_h$ for approximating $p_\mathrm{tilted}$.

Unfortunately, we do not have access to the explicit value of $p_h$. Therefore, we rewrite the importance weight with $p_\text{tilted}(\vx) \propto p_\text{data}(\vx) \exp(r(\vx))$ as
\begin{align}
    \frac{p_\text{tilted}(\vx)}{p_h(\vx)}=\frac{p_\text{tilted}(\vx)}{p_\text{data}(\vx)}\frac{p_\text{data}(\vx)}{p_h(\vx)}\propto\exp(r(\vx))\frac{p_\text{data}(\vx)}{p_h(\vx)}.
\end{align}
Now we approximate the quotient $\frac{p_\text{data}(\vx)}{p^h(\vx)}$ by the ELBO from \cite{denker2024deft}, such that
\begin{align}\label{eq:approx_iw}
\frac{p_\text{tilted}(\vx)}{p_h(\vx)}\propto\exp(r(x))\frac{p_\text{data}(\vx)}{p_h(\vx)}\approx \exp(r(\vx_0))\frac{\dd \sP_\text{data}}{\dd \sP_h}(\vx_{[0,T]}) 
%\exp(-D_\text{KL}(p || p_\text{diff}))\approx \exp(-D_\text{KL}(\sP || \sP_\text{diff}))\\
%&=\exp\left(-\mathbb{E}_{x_t \sim \sP}\left[\int_0^T \sigma_t^2\| h_t(x_t) \|_2^2  dt\right]\right),
\end{align}
where $\sP_h$ and $\sP_\text{data}$ are the path measures for the SDEs corresponding to the prior and adjusted diffusion model and $\vx_{[0:T]}=(\vx_t)_{t\in[0,T]}$ is the whole generated trajectory. The Radon-Nikodym derivative $\frac{\dd \sP_\text{data}}{\dd \sP_h}(\vx_{[0,T]})$ can now be computed based on the framework from \cite{nusken2024transport}. We summarise the result in the following lemma. The derivations are included in Appendix~\ref{app:RND_SDE}, both in continuous and in discrete time.
\begin{lemma} \label{lemma:rnd}
The right side of \eqref{eq:approx_iw} can be rewritten as
\begin{align}
    \exp \left( r(\vx_0) - \frac12 \int_0^T \sigma_t^2\| h_t(\vx_t) \|_2^2  dt + \int_0^T \sigma_t h_t(\vx_t)^\top dW_t\right),
\end{align}
\end{lemma}
These path-wise importance weights can be computed concurrently with sampling without any computational overhead. For numerical stability, we perform these computations in the log-space.

In practice, importance weights have the disadvantage that they often suffer from highly imbalanced weights.
As a remedy, \citet{hertrich2024importance} proposed a rejection sampling algorithm based on relaxed importance weights. Plugging in our approximation \eqref{eq:approx_iw}, this rejection sampling step amounts to computing, for some generated trajectory $\vx_{[0,T]}$, the acceptance probability
\begin{equation}\label{eq:acceptance_prob}
\alpha(\vx_{[0:T]})=\min\left(1,\frac{\dd \sP_\text{data}}{\dd \sP_h}(\vx_{[0,T]}) \frac{\exp(r(\vx_0))}{c}\right),
\end{equation}
where $c$ is a hyper-parameter. In practice, we use an adaptive choice of $c$ such that a certain rate of samples is accepted, see \citep[Rem 10]{hertrich2024importance}.  We will show in the next subsection that the distribution of accepted samples is closer to the tilted distribution than the distribution of the initially generated samples.

\subsection{Self-supervised Importance Fine-Tuning}

\begin{algorithm}[t]
\caption{Self-supervised estimation of the $h$-transform for DDPM}
\label{alg:self_supervised}
\begin{algorithmic}[1]
\Require Pre-trained noise prediction model $\epsilon^\theta_t(\vx_t)$, noise schedule $\bar \alpha_t$
\Require Number of outer training steps $K$, number of inner training steps $M$, number of samples $N$, batch size $m$, buffer with maximum length
\Require Rate of accepted samples $r$
\For{$k = 1$ to $K$}
    \State Sample $\vx_{[0:T]}^{(1:N)} \sim \sP_{h_\varphi}$ \Comment{Sample paths with current model} 
    \For{$i=1$ to $N$} \Comment{Resample using rejection sampling}
    \State Compute acceptance probability $\alpha(\vx_{[0:T]}^{(i)})$
    \State Sample $u \sim U([0,1])$
    \State If $u \le \alpha(\vx_{[0:T]}^{(i)})$, add $\vx_{[0:T]}^{(i)}$ to buffer
    \EndFor
    \For{$1$ to $M$ } \Comment{Inner training loop using buffer}
    \State Sample $\vx^{(1:m)}$ from buffer 
    \State $t \sim U(\{1, \dots, T\})$
    \State $\epsilon \sim \mathcal{N}(0,I)$
    \State $\vx_t^{(1:m)} \gets \sqrt{\bar \alpha_t} \vx^{(1:m)} + \sqrt{1 - \bar \alpha_t} \epsilon$
    %\STATE $\hat{\epsilon} \gets \epsilon^\theta_t(\vx_t^{(1:M)})$
    %\STATE $\hat{h} \gets h^\varphi_t(\vx_t^{(1:M)}) $
    \State $\ell \gets \left\| \left((\epsilon^\theta_t(\vx_t^{(1:m)}) + h^\varphi_t(\vx_t^{(1:m)})\right) - \epsilon \right\|_2^2 $
    \State $\varphi \gets \text{Optim}(\varphi, \nabla_\varphi \ell)$
    \EndFor    
\EndFor
\end{algorithmic}
\end{algorithm}


Next, we will combine the importance-based rejection steps with the supervised loss function from Theorem~\ref{th:deft} (ii) to obtain a tractable fine-tuning algorithm.
More precisely, starting with an initial guidance term $h^0$ we construct a sequence of guidance terms $h^k$ for $k=1,2,...$ by the following steps.

First, we sample a batch $\{ \vx_0^{(i)}\}_{i=1,\dots,N}$ from the reverse SDE \eqref{eq:rev_sde2trans_2} with $h=h^k$ and compute the corresponding acceptance probabilities \smash{$\alpha_i=\alpha(\vx_{[0,T]}^{(i)})$} from \eqref{eq:acceptance_prob}.
Second, we keep any sample from the batch with probability $\alpha_i$ and discard the rest. We denote the distribution of remaining samples by $\tilde{\sP}_{h^k}^0$.
Finally, we update the guidance term $h^k$ by using the supervised loss function for the $h$-transform from Theorem~\ref{th:deft} (ii). That is, we define $h^{k+1}\in\argmin_g \mathcal L_{FT}(g)$, where
\begin{align}\label{eq:supervised_finetuning_loss}
\mathcal L_{FT}(g)=\mathop{\mathbb{E}}_{\substack{ \vx_0 \sim \tilde{\sP}_{h^k}^0 \\ t\sim \mathrm{U}(0,T), \vx_t \sim \fwd{p}_{t|0}(\cdot|\vx_0)}  }\left[ \left\| \left({g_t(\vx_t)}\!+\!s_t(\vx_t) \right)\!-\!\nabla_{\vx_t}\ln \fwd{p}_{t|0}(\vx_t  |\vx_0) \right\| ^2\right].  
\end{align}

We summarise our self-supervised importance fine-tuning in Algorithm~\ref{alg:self_supervised}. 

Recall that the generalised $h$-transform from Theorem~\ref{th:deft} minimises the function $\mathcal F(\sP_h)=\KL(\sP_h,\sP_\mathrm{data})-\mathbb{E}_{\vx_{[0,T]}\sim \sP_h}[r(\vx_0)]$. The following theorem proves that our self-supervised importance fine-tuning is a descent algorithm for $\mathcal F$. More precisely, it holds that $\mathcal F(\sP_{h^{k+1}})\leq \mathcal F(\sP_{h^{k}})$ for all $k=1,2,...$.
We include the proof in Appendix \ref{app:proof_resampling}.
\begin{theorem}\label{the:}
Let $\alpha(\vx_{[0:T]})=\min\left(1,\frac{\dd \sP_\text{data}}{\dd \sP_h}(\vx_{[0,T]}) \frac{\exp(r(\vx_0))}{c}\right)$ be the acceptance probability of a sample $\vx_{[0,T]}\sim \sP_h$ and denote by $\tilde{\sP}_h$ the distribution of the accepted paths.
Then, the following holds true:
\begin{itemize}
    \item[(i)] $\frac{\dd \tilde{\sP}_h}{\dd  \sP_h}(\vx_{[0,T]}) = \frac{\alpha(\vx_{[0,T]})}{\mathbb{E}_{\vx_{[0,T]}\sim\sP_h}[\alpha(\vx_{[0,T]})]}$
    \item[(ii)] $\mathcal F(\tilde{\sP}_h)\leq \mathcal F(\sP_h)$, where $\mathcal F(\sP)=\KL(\sP,\sP_\mathrm{data})-\mathbb{E}_{\vx_{[0,T]}\sim \sP}[r(\vx_0)] \propto \KL(\sP, \sP_{\mathrm{tilde}})$
    \item[(iii)] Let $h_t^*\in\argmin_{g_t}\mathcal L_{FT}(g_t)$ with $\mathcal L_{FT}$ from \eqref{eq:supervised_finetuning_loss}
    and let $\sP_{h^*}$ be the path measure of the corresponding SDE.
    Then it holds 
    $
    \mathcal F(\sP_{h^*})\leq\mathcal F(\tilde{\sP}_h)\leq\mathcal F(\sP_h).
    $
\end{itemize}
\end{theorem}

\subsection{Training and Network Parametrisation}

\paragraph{Replay buffer} Sampling from the current model is the most computationally expensive part of the algorithm. Motivated by \cite{midgley2022flow,sendera2024improved}, we make use of an un-prioritised replay buffer with a fixed length. We save the accepted samples from the current model and append them the buffer. Once the fixed length is reached, we discard the oldest samples. During training, we randomly sample batches from the buffer.   


\paragraph{Network parametrisation} The parametrisation of the $h$-transform has a crucial effect on performance and convergence speed. Motivated by the network parametrisation in sampling applications with diffusion \citep{vargas2023denoising,zhang2021path} and fine-tuning approaches \citep{denker2024deft,venkatraman2024amortizing}, we make use of a \textit{reward-informed inductive bias} given as 
\begin{align}
\label{eq:lkhd_informed_model}
    h^\theta_t(\vx_t) = \text{NN}_1(\vx_t, t) + \text{NN}_2(t) \nabla_{\hat \vx_0} r(\hat \vx_0),
\end{align}
where $\hat \vx_0$ is the Tweedie estimate given the pre-trained unconditional diffusion model, $\text{NN}_1$ is a vector-valued and $\text{NN}_2$ a scalar-valued neural network. %For the text-to-image models, we make use of prior work, see e.g. \citep{venkatraman2024amortizing, fan2024reinforcement}, and use the parameter efficient LoRA method \citep{hu2021lora}. 
% \paragraph{Relaxed Importance Weights} To enhance diversity following the resampling step, we mix the importance weights with a uniform distribution. In particular, we define the adjusted weights as:
% \begin{align}
% w'_i = \lambda \frac{w_i}{ \sum_{i=1}^N w_i} + (1 - \lambda) \frac{1}{N},
% \end{align}
% where $\lambda = 0.975$. This approach is useful for small batch sizes, where a single sample with a high importance weight can disproportionately dominate the resampling process. By mixing with the uniform distribution, we ensure that the resampling step retains more diversity and avoids being overly influenced by a single dominant sample. \Alex{This was only neccessary for the multinominal resampling. With the rejection step from Johannes this is not neccessary.}

\paragraph{KL-Regularisation} Similar to \cite{fan2024reinforcement}, we found that using a KL regulariser was useful to improve diversity in the supervised setting. The KL divergence between $p_h$ and $p_\text{data}$ can be bounded as 
\begin{align}
    D_\text{KL}(p_h,p_\text{data}) \le D_\text{KL}(\sP_h,\sP_\text{data}) = \mathbb{E}_{\bH_t \sim \sP_h}\left[ \int_0^T \sigma_t^2 \| h_t(\bH_t) \|_2^2 d t \right], 
\end{align}
i.e., the norm of the $h$-transform over trajectories, see Appendix \ref{app:RND_SDE} for a derivation. The naive addition of this term to the supervised training loss is expensive, as one has to  backpropagate through the full trajectory. Instead, we estimate the integral using a single random time step. %For the VP-SDE \citep{song2021Scorebased}, the final objective in each inner iterations reads 
%\begin{align}
%        \min_h \mathop{\mathbb{E}}_{\substack{ \bX_0 \sim \tilde{p}_h(\vx_0) \\ t\sim \mathrm{U}(0,T), \bH_t \sim \fwd{p}_{t|0}(\vx_t|\vx_0)}  }\left[ \left\| \left({h_t(\bH_t)}\!+\!s^\theta_t(\bH_t) \right)- \vz / \nu_t \right\| ^2 + \alpha_\text{KL} \nu_t^2 \| h_t(\bH_t) \|_2^2 \right], 
%\end{align}
%where $\bH_t = \gamma_t \bX_0 + \nu_t \vz, \vz \sim \mathcal{N}(0,I)$, $s_\theta(\bH_t,t) \approx \nabla_{\bH_t} \ln p_t(\bH_t) $ is the pre-trained score model and $\tilde{p}_h(\vx_0)$ is the distribution induced by the current estimate of the $h$-transform after the resampling step. %Instead using regularising the norm of $h_t$ one can also use weight decay.


\section{Experiments}
\label{sec:experiments}
In this section, we present a toy example on a 2D dataset, class conditional sampling for MNIST and finally preliminary results for reward fine-tuning of text-to-image diffusion models. 
%In this section, we present a toy example on a 2D dataset, class conditional sampling for MNIST and CIFAR-10 and finally preliminary results for reward fine-tuning of text-to-image diffusion models. %We start by discussing important implementation details, such as the replay buffer, the network parametrisation and KL-regularisation.  


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth,trim=0 120 0 110,clip]{imgs/2d_GMM_results_new2.pdf} % ,trim=0 22 0 10,clip
    \caption{2D Toy example for fine-tuning diffusion models. We show the prior the log-probability of the tilted distribution and samples from the conditional diffusion model using classifier guidance \citep{dhariwal2021diffusion}, online fine-tuning \citep{fan2024reinforcement}, our importance fine-tuning and a variation with only reward-based importance weights.}
    \label{fig:2g_gmm}
\end{figure}



\subsection{2D Toy example}
As an initial example we make use of a diffusion trained on samples of a Gaussian mixture model (GMM) with $25$ modes, arranged on a grid. The goal is to sample from the tilted distribution $p_\text{data}(\vx)\exp(r(\vx))$, where the reward $r$ is defined as the log-likelihood of a GMM with a reweighted subset of the modes, see Appendix~\ref{app:toy_details} for the detailed setup. Figure \ref{fig:2g_gmm} illustrates both the prior and the density of the tilted distribution. We compare against Classifier Guidance \citep{dhariwal2021diffusion}, an inference time method, where the $h$-transform is approximated by the gradient of the reward $h_t(\vx_t) = \gamma \nabla_{\vx_t} r(\vx_t)$. Further, we compare against online fine-tuning, where we directly optimise Equation~\eqref{eq:tilted_dist_opt}. Instead of employing the gradient calculation of DPOK \citep{fan2024reinforcement}, we backpropagate gradients through the entire trajectory for this toy example. Further, we evaluate our importance fine-tuning and a variation where importance weights are calculated solely based on the reward, referred to as reward-only fine-tuning.

In Figure~\ref{fig:2g_gmm}, we observe that classifier guidance is able to find all the modes of the posterior, fails to capture the correct weighting. In contrast, both online fine-tuning and importance fine-tuning yield samples that more accurately represent the target distribution. Lastly, the reward-only fine-tuning method collapses to the highest mode.

%We train a diffusion model on samples of a Gaussian Mixture Model (GMM) with $25$ modes, arranged on a grid $\{-2.5, -1.25, 0, 1.25, 2.5\} \times \{-2.5, -1.25, 0, 1.25, 2.5\}$. As the reward function, we choose a GMM with $5$ nodes on the diagonal $\{(-2.5, -2.5), (-1.25, -1.25), (0,0), (1.25, 1.25), (2.5, 2.5) \}$. Here, the goal is to fine-tune the diffusion model to sample from this subset. 


\subsection{Class-conditional sampling}
As another application, we consider class conditional sampling. Let $c$ be the class and $p(c|\vx)$ the log class probabilities of a pre-trained classifier. The goal is to sample from the tilted distribution with the reward as $r(x) = \ln p(c | \vx)$. We consider both the MNIST dataset and pre-train unconditional diffusion models using the U-Net architecture from \cite{ho2020denoising}. We use a convolutional classifier with a $98.6\%$ accuracy. We use the reward-informed architecture as in Eqn.~\eqref{eq:lkhd_informed_model}.

%We consider both the MNIST and CIFAR-10 dataset. For both models we pre-train unconditional diffusion models using the U-Net architecture from \cite{ho2020denoising}. We use standard classifiers for both dataset, i.e., a small convolutional network for MNIST and ResNet18 \citep{he2016deep} for CIFAR-10. We use the reward-informed architecture as in Eqn.~\eqref{eq:lkhd_informed_model}.

%\paragraph{MNIST}
We use importance fine-tuning to learn the posterior for different MNIST classes and for even/odd numbers. We show posterior prior samples in Figure~\ref{fig:mnist_class_cond}, see Figure~\ref{fig:mnist_class_cond_full} for all classes. We compare both against classifier guidance and online fine-tuning. For online fine-tuning we optimise Equation~\eqref{eq:online_loss} using VarGrad \citep{richter2020vargrad} to reduce the memory cost. In Table \ref{tab:mnist_results} we present the expectation of the reward and the FID \citep{heusel2017gans} based on the features of the penultimate layer of the pre-trained classifier. For single-class posteriors, online fine-tuning achieves a higher expected reward but results in a lower actual reward. A similar trend is observed for even/odd tasks. This indicates that online fine-tuning is more effective at maximising reward, although at the expense of reduced sample diversity, as reflected by the FID score.

%\paragraph{CIFAR-10}

\begin{figure*}[t]
\centering
\begin{subfigure}[t]{.24\textwidth}
  \includegraphics[width=\linewidth]{imgs/prior_samples_mnist.png}
  \caption*{Prior samples}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.24\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_six.png}
    \captionsetup{justification=centering}
  \caption*{Posterior: Six}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.24\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_even.png}
  \caption*{Posterior: Even numbers}
\end{subfigure}%
\hfill
\caption{Class conditional sampling for MNIST. Left: Sampled from unconditional model. Middle: Samples for class 'six'. Right: Samples for even numbers.} \label{fig:mnist_class_cond}
\end{figure*}

\begin{table}[]
\centering
\caption{Expected reward and FID scores for both the single class and the even/odd task on MNIST. FID is computed for the features of the penultimate layer of the pre-trained classifier for $1024$ samples. We compare against classifier guidance with scale $\gamma=4.5$, online fine-tuning and our importance fine-tuning.}
\begin{tabular}{lcccc}
\toprule                       & \multicolumn{2}{c}{\textbf{Single class}} & \multicolumn{2}{c}{\textbf{Even / Odd}} \\
                 & $\E[r(\vx)]$ $(\uparrow)$  & FID $(\downarrow)$ & $\E[r(\vx)]$  $(\uparrow)$ & FID $(\downarrow)$ \\ \midrule
Classifier Guidance    &  $-1.495$ &	$117.241$   &   $-2.572$     & $193.984$  \\
Online Fine-tuning    & $-0.110$  & $31.994$    &  $-0.065$      &   $198.871$ \\
Importance Fine-tuning &      $-0.152$     &  	$30.814$    &            $-0.878$      &    $110.403$        \\ \bottomrule
\end{tabular}
\label{tab:mnist_results}
\end{table}


%\begin{figure*}[t]
%\centering
%\begin{subfigure}[t]{.24\textwidth}
%  \includegraphics[width=\linewidth]{imgs/prior_samples_cifar10.png}
%  \caption*{Prior samples}
%\end{subfigure}%
%\hfill
%\begin{subfigure}[t]{.24\textwidth}
%  \includegraphics[width=\linewidth]{imgs/cond_samples.png}
%    \captionsetup{justification=centering}
%  \caption*{Posterior: Dog}
%\end{subfigure}%
%\hfill
%\begin{subfigure}[t]{.24\textwidth}
%  \includegraphics[width=\linewidth]{imgs/cond_samples.png}
%  \caption*{Posterior: Horse}
%\end{subfigure}%
%\hfill
%\begin{subfigure}[t]{.24\textwidth}
%  \includegraphics[width=\linewidth]{imgs/cond_samples.png}
%    \caption*{Posterior: Car}
%end{subfigure}%
%\hfill
%\caption{\Alex{Test how large these images are. Here, we will test different conditional sampling tasks. Currently, we show $32$ samples}} \label{fig:cifar10_class_cond}
%\end{figure*}



\begin{figure*}[t]
\centering
\begin{subfigure}[t]{.49\textwidth}
  \includegraphics[width=\linewidth]{imgs/samples_green_rabbit_seed11.png}
    \captionsetup{justification=centering}
  \caption*{Base Model}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.49\textwidth}
  \includegraphics[width=\linewidth]{imgs/lora_samples_green_rabbit_seed11.png}
    \captionsetup{justification=centering}
  \caption*{Fine-tuned Model}
\end{subfigure}%
\caption{Samples for the base model and the fine-tuned model for the prompt ''A green colored rabbit.``. Images were generated using the same seed.} \label{fig:stable_diff_greenrabbit}
\end{figure*}


\subsection{Text-to-Image Reward Fine-tuning}
Despite the progress in training text-to-image diffusion models, samples not always align with human preferences. In particular, the model can struggle with unnatural prompts such as ``A green rabbit'' \citep{venkatraman2024amortizing,fan2024reinforcement}. To alleviate this problem, the diffusion model is fine-tuned to maximise some reward model, trained to imitate human preferences. We use the latent diffusion model Stable Diffusion-v1.5 \citep{rombach2022high} and align it using the ImageReward-v1.0 \citep{xu2024imagereward} reward model. We refer to Appendix~\ref{app:rewardfinetuning} for experimental details. In Figure~\ref{fig:stable_diff_greenrabbit}, we show examples for the prompt ``A green rabbit''. Here, we see that in contrast to the pre-trained model, we obtain samples which align better with the prompt. This can also be seen in the mean reward value of $-0.18$ for the base model and $0.67$ for the fine-tuned model, evaluated over $60$ images. However, for some samples the visual quality also degrades. Specifically, in the middle image of the bottom row, the output shows oversaturation in the green colour. 


\section{Conclusion}
We propose an iterative approach for fine-tuning diffusion models for conditional sampling tasks. As the naive iterative retraining of generative models often lead to performance degradation \citep{shumailov2023curse}, we introduce an additional resampling step based on path-based importance weights. We show initial evaluations for class conditional sampling and reward fine-tuning of text-to-image diffusion models. 

Online fine-tuning methods often require additional tricks to increase training stability, e.g., \cite{venkatraman2024amortizing} use loss clipping and disregard low reward trajectories or \cite{fan2024reinforcement} employ variance reduction techniques by additionally learning the value function. In contrast our importance fine-tuning method is trained using a score matching loss, see Theorem~\ref{th:deft}, leading to a more stable training.  


\paragraph{Limitations} In our iterative refinement method, the model is fine-tuned using ''good``, i.e., having a high importance weight, samples from the pre-trained model. However, these samples necessarily lie in the support of the pre-trained model and we rely on the fact that such ''good`` samples exist. Thus, our refinement approach might struggle on domains where the distribution of the pre-trained model is sparse and high reward samples are are. One possibility to alleviate this problem is to make use of ''off-policy`` samples \citep{venkatraman2024amortizing}, which are obtained by some other method. For off-policy samples, we loose the compact formulation of the RND from Lemma~\ref{lemma:rnd}. However, we can still compute importance weights using the RND in Proposition~\ref{prop:rnd_between_sde}. 

%\paragraph{Further work} We provide an initial evaluation of our refinement approach. Comparisons against other fine-tuning approaches, such as DPOK \citep{fan2024reinforcement} or adjoint matching \citep{domingo2024adjoint}, are necessary to fully evaluate our importance fine-tuning method.

% Only for arxiv 
\subsubsection*{Acknowledgments}
Alexander Denker acknowledges support by the EPSRC programme grant EP/V026259/1. Johannes Hertrich acknowledges funding by the German Research Foundation (DFG) within the Walter Benjamin Programme with project number 530824055. Shreyas Padhy is funded by the University of Cambridge Harding Distinguished Postgraduate Scholars
Programme.

\bibliographystyle{iclr2025_conference}
\bibliography{bibliography}

\newpage
\appendix
\section{Radon-Nikodym derivative betweens SDEs.}
\label{app:RND_SDE}
To calculate the approximated importance weights, we make use of the RND between SDEs. In particular, we use the result from \citep{nusken2021solving,nusken2024transport}.

\begin{proposition}(RND between SDEs \citep{nusken2021solving,nusken2024transport})
\label{prop:rnd_between_sde}
Given the following SDEs 
\begin{align} 
    \dd \bY_t &=  a_t(\bY_t) \,\dd t + \sigma_t(\bY_t)\; \fwd{ \dd \rv{W}}_t,  \quad \bY_0 \ \sim  \mu , \\
    \dd \bX_t &=  b_t(\bX_t) \,\dd t + \sigma_t(\bX_t) \; \fwd{ \dd \rv{W}}_t,  \quad \bX_0 \ \sim  \nu ,
\end{align}
with path probabilities $\fwd{\sP}_a$ and $\fwd{\sP}_b$, we get 
\begin{align} 
    \ln \left( \frac{d \fwd{\sP}_a}{d\fwd{\sP}_b}\right)(\bZ) = \ln \left( \frac{d \mu}{d \nu} \right)(\bZ_0) + \int_0^T \sigma_t^{-2} (a_t - b_t)(\bZ_t) d \fwd{\bZ}_t + \frac12 \int_0^T \sigma_t^{-2} (b_t^2 - a_t^2)(\bZ_t) dt,
\end{align}
and in particular, when evaluated on $\bY$:
\begin{align} 
    \ln \left( \frac{d \fwd{\sP}_a}{d\fwd{\sP}_b}\right)(\bY) = \ln \left( \frac{d \mu}{d \nu} \right)(\bY_0) + \int_0^T \sigma_t^{-1} (a_t - b_t)(\bY_t) d \fwd{\bW}_t + \frac12 \int_0^T \sigma_t^{-2} \|b_t - a_t\|^2(\bZ_t) dt.
\end{align}
\end{proposition}

Let $\sP_\text{data}$ be the path probability of 
\begin{align}
    \dd \bX_t &= \left( f_t(\bX_t) - \sigma_t^2 \nabla_{\bX_t} \ln p_t(\bX_t)  \right) \,\dd t + \sigma_t \bwd{ \dd \rv{W}}_t, \quad \bX_T \sim  \distT, 
\end{align}
and $\sP_h$ the path probability of 
\begin{align}
    \dd \bH_t &= \left( f_t(\bH_t) - \sigma_t^2( \nabla_{\bH_t} \ln p_t(\bH_t) + h_t(\bH_t)) \right) \,\dd t + \sigma_t \bwd{ \dd \rv{W}}_t, \quad \bH_T \sim  \distT. 
\end{align}
For the approximated importance weights we require the RND of $\sP_h$ with respect to $\sP_\text{data}$ evaluated at trajectories from $\sP_h$. Let the drift of $\sP_\text{data}$ be given as $a_t(\vx) = f_t(\vx) - \sigma_t^2 \nabla_{\vx} \ln p_t(\vx)$ and the drift of $\sP_h$ as $b_t(\vx) = f_t(\vx) - \sigma_t^2 \nabla_{\vx} \ln p_t(\vx) - \sigma_t^2 h_t(\vx) $. In particular, we have $a_t - b_t = \sigma_t^2 h_t$. For readability, we do not omit the dependence on $\vx$ for the drift in the following. Using Proposition \ref{prop:rnd_between_sde} we get,
\begin{align}
    \ln\left( \frac{\dd \sP_\text{data}}{\dd \sP_h} \right)(\bH) &= \int_0^T \sigma_t^{-2} (a_t - b_t) \dd \bwd{\bH}_t + \frac12 \int_0^T \sigma_t^{-2} (b_t^2 - a_t^2) \dd t \\ 
    &= \int_0^T \sigma_t^{-2}(a_t-b_t)b_t \dd t + \int_0^T \sigma_t^{-2} (a_t - b_t) \sigma_t d \bwd{\bW}_t + \frac12 \int_0^T \sigma_t^{-2} (b_t^2 - a_t^2) \dd t \\
    &= \frac12 \int_0^T \sigma_t^{-2} [2a_t b_t - 2b_t^2 +  b_t^2 -  a_t^2] \dd t + \int_0^T \sigma_t h_t \dd \bwd{\bW}_t \\ 
    &= - \frac12 \int_0^T \sigma_t^{-2} (b_t - a_t)^2 \dd t + \int_0^T \sigma_t h_t \dd \bwd{\bW}_t \\
    & = - \frac12 \int_0^T \sigma_t^2 \| h_t \|_2^2 \dd t + \int_0^T \sigma_t h_t \dd \bwd{\bW}_t, \label{eq:rnd_final}
\end{align}
evaluated on a trajectory $\bH$ from $\sP_h$.

\subsection{Discrete Version for DDPM}
We can also express the RND in the discrete setting. For this derivation, we make use of the DDPM schedule \citep{ho2020denoising}. The generalisation to different schedules is straightforward. The path probability of the pre-trained model is given as 
\begin{align}
    p_\theta^\text{data}(\vx_0, \dots, \vx_T) = p_T(\vx_T) \prod_{t=1}^T p_\theta^\text{data}(\vx_{t-1} | \vx_t), \quad p_\theta^\text{data}(\vx_{t-1}|\vx_t) = \mathcal{N}(\vx_{t-1}| \mu_\theta(\vx_t, t) ; \tilde \beta_t^2 I)
\end{align}
where the mean is parametrised as $\mu_\theta(\vx_t, t) =\frac{1}{\sqrt \alpha_t} ( \vx_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar \alpha_t}} \epsilon_\theta(\vx_t,t))$. Similar, we have path probabilities for the fine-tuned model as 
\begin{align}
        p_\varphi^\text{h}(\vx_0, \dots, \vx_T) = p_T(\vx_T) \prod_{t=1}^T p_\varphi^\text{h}(\vx_{t-1} | \vx_t), \quad p_\varphi^h(\vx_{t-1}|\vx_t) = \mathcal{N}(\vx_{t-1}| \mu_h(\vx_t, t) ; \tilde \beta_t^2 I),
\end{align}
with mean $\mu_h(\vx_t, t) = \mu_\theta(\vx_t, t) + \Delta_h(\vx_t, t)$ is the original mean plus a delta given by the $h$-transform. We use $\tilde \beta_t = \sqrt{\frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_t} \beta_t}$ for the standard deviation of the reverse kernel. Using this setting, we can write the RND as
\begin{align}
    \ln \left( \frac{p_\theta^\text{data}(\vx_0, \dots, \vx_T)}{p_\varphi^\text{h}(\vx_0, \dots, \vx_T)} \right) = \sum_{t=1}^T  \log \left( \frac{p_\theta^\text{data}(\vx_{t-1}|\vx_t)}{p_\varphi^h(\vx_{t-1}|\vx_t)} \right),
\end{align}
where we assumed that the terminal distribution $p_T$ is the same for both diffusion models. The log ratio of two Gaussian reduces to 
\begin{align}
    \log \left( \frac{\mathcal{N}(x; \mu_1, \Sigma_1)}{\mathcal{N}(x; \mu_2, \Sigma_2)} \right) = - \frac{1}{2} [(x - \mu_1)^T \Sigma_1^{-1}(x - \mu_1) - (x - \mu_2)^T \Sigma_2^{-1}(x - \mu_2)] + \frac12 \log \frac{|\Sigma_2|}{|\Sigma_1|},
\end{align}
which gets us
\begin{align}
    \label{eq:log_ratio_kernel}
    \log \left( \frac{p_\theta^\text{data}(\vx_{t-1}|\vx_t)}{p_\varphi^h(\vx_{t-1}|\vx_t)} \right) = - \frac{1}{2 \tilde \beta_t^2} \left[ \| \vx_{t-1} - \mu_\theta(\vx_t,t) \|_2^2 - \| \vx_{t-1} - \mu_h(\vx_t,t) \|_2^2 \right].
\end{align}
To further simplify these terms, we need some information about the trajectory $\vx_0, \dots, \vx_T$. In particular, we assume that we have a \textit{trajectory sampled from the fine-tuned model}, i.e.,
\begin{align}
    \vx_{t-1} = \mu_h(\vx_t, t) + \tilde \beta_t \epsilon, \quad \epsilon \sim \mathcal{N}(0,I).
\end{align}
Given the parametrisation of the mean in the diffusion model
\begin{align}
    \mu_\theta(\vx_t,t) &=\frac{1}{\sqrt \alpha_t} \left( \vx_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar \alpha_t}} \epsilon_\theta(\vx_t,t)\right), \\
    \mu_h(\vx_t,t) &=\frac{1}{\sqrt \alpha_t} \left( \vx_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar \alpha_t}} \epsilon_\theta(\vx_t,t) - \frac{1 - \alpha_t}{\sqrt{1 - \bar \alpha_t}} h_\varphi(\vx_t,t)\right), \label{eq:ddpm_mean_h}
\end{align}
we can reduce the terms on the RHS of Equation \eqref{eq:log_ratio_kernel} to
\begin{align}
    & \| \vx_{t-1} - \mu_\theta(\vx_t,t) \|_2^2 -  \| \vx_{t-1} - \mu_h(\vx_t,t) \|_2^2 \\ &= \| \mu_h(\vx_t, t) + \tilde \beta_t \epsilon - \mu_\theta(\vx_t, t) \|_2^2 - \| \mu_h(\vx_t, t) + \tilde \beta_t \epsilon - \mu_h(\vx_t, t) \|_2^2 \\
    & = \|  \Delta_h(\vx_t, t) + \tilde \beta_t \epsilon \|_2^2 - \| \tilde \beta_t  \epsilon \|_2^2. \label{eq:mean_difference}
\end{align}
Combining Equation~\eqref{eq:mean_difference} and Equation~\eqref{eq:log_ratio_kernel}, we obtain
\begin{align}
     \log \left( \frac{p_\theta^\text{data}(\vx_{t-1}|\vx_t)}{p_\varphi^h(\vx_{t-1}|\vx_t)} \right) &= - \frac{1}{2 \tilde \beta_t^2} \left[ \| \vx_{t-1} - \mu_\theta(\vx_t,t) \|_2^2 - \| \vx_{t-1} - \mu_h(\vx_t,t) \|_2^2 \right] \\
    & = - \frac{1}{2 \tilde \beta_t^2} \left[\|  \Delta_h(\vx_t, t) + \tilde \beta_t \epsilon \|_2^2 - \| \tilde \beta_t \epsilon \|_2^2\right] \\
    & = - \frac{1}{2 \tilde \beta_t^2} \left[ \| \Delta_h(\vx_t, t) \|_2^2 + 2 \Delta_h(\vx_t, t)^\top \tilde \beta_t \epsilon + \| \tilde \beta_t \epsilon \|_2^2 - \| \tilde \beta_t \epsilon \|_2^2    \right] \\
    & = - \frac{1}{2 \tilde \beta_t^2} \| \Delta_h(\vx_t, t) \|_2^2 - \frac{1}{\tilde \beta_t} \Delta_h(\vx_t, t)^\top \epsilon.
\end{align}
For the full RND we obtain
\begin{align}
    \label{eq:discrete_iw_ddpm_coef}
     \sum_{t=1}^T \log \left( \frac{p_\theta^\text{data}(\vx_{t-1}|\vx_t)}{p_\varphi^h(\vx_{t-1}|\vx_t)} \right) = \sum_{t=1}^T \left[ -\frac{1}{2} \tilde \beta^{-2} \| \Delta_h(\vx_t, t) \|_2^2 + \tilde \beta^{-1} \Delta_h(\vx_t, t)^\top \epsilon \right],
\end{align}
which mimics a discretised version of the continuous RND in Equation~\eqref{eq:rnd_final}. 



%Given the mean parametrisation in Equation~\eqref{eq:ddpm_mean_h}, we can identify $\Delta_h$ as 
%\begin{align}
%     \Delta_h(\vx_t, t) = - \frac{1 - \alpha_t}{\sqrt{\alpha_t} \sqrt{1 - \bar \alpha_t}} h_\varphi(\vx_t, t),
%\end{align}
%which finally gives us
%\begin{align}
%    \label{eq:discrete_iw_ddpm}
%     \sum_{t=1}^T \log \left( \frac{p_\theta^\text{data}(\vx_{t-1}|\vx_t)}{p_\varphi^h(\vx_{t-1}|\vx_t)} \right) = \sum_{t=1}^T \left[ -\frac{1}{2} \sigma_t^2 \| h_\varphi(\vx_t, t) \|_2^2 + \sigma_t h_\varphi(\vx_t,t)^\top \epsilon \right],
%\end{align}
%with $\sigma_t = \frac{1}{\sqrt{\alpha_t - \bar \alpha_t}}$. \Alex{How can this $\sigma_t$ be interpreted? Or do i have a mistake with the constants?} If we keep the coefficients inside the norm, we have 
%\begin{align}
%        \label{eq:discrete_iw_ddpm_coef}
%     \sum_{t=1}^T \log \left( \frac{p_\theta^\text{data}(\vx_{t-1}|\vx_t)}{p_\varphi^h(\vx_{t-1}|\vx_t)} \right) = \sum_{t=1}^T \left[ -\frac{1}{2} \tilde \beta^{-2} \| \frac{1 - \alpha_t}{\sqrt{\alpha_t} \sqrt{1 - \bar \alpha_t}} h_\varphi(\vx_t, t) \|_2^2 + \tilde \beta^{-1} (\frac{1 - \alpha_t}{\sqrt{\alpha_t} \sqrt{1 - \bar \alpha_t}} h_\varphi(\vx_t,t))^\top \epsilon \right],
%\end{align}
%which then mimics a discretised version of the continuous RND in Equation~\eqref{eq:rnd_final}.

%In the continuous case in Appendix \ref{prop:rnd_between_sde}, we use the score-based parametrisation, while the DDPM often uses a noise prediction network. However, we can derive the score estimation from the noise estimation by
%\begin{align}
%    h_\varphi(\vx_t, t) = - \sqrt{1 - \bar \alpha_t} h_\varphi^\text{score}(\vx_t, t). 
%\end{align}
%Using this score parametrisation we get the same equation as in Equation~\eqref{eq:discrete_iw_ddpm} with different constants $\sigma_t = \sqrt{\frac{1 - \bar \alpha_t}{(1 - \bar \alpha_{t-1}) \alpha_t}} \approx \alpha_t^{-1/2}$ for a large number of time-steps $t$ and the sign of the second term would change. Note, that we forward process in DDPM is given by $\vx_t = \sqrt{\alpha_t} \vx_{t-1} + \sqrt{1 - \alpha_t} \epsilon$ with $\epsilon \sim \mathcal{N}(0,I)$. %We should see that the discrete weights converge to the continuous ones, as recently shown for a lot of different objective functions by \cite{berner2025discrete}. \Alex{In particular, see Lemma B.7 in \citep{berner2025discrete}.}

%Further, for the KL regulariser from \cite{fan2024reinforcement}, we get 
%\begin{align}
%    \mathbb{E}_{(\vx_0, \dots, \vx_T) \sim p_\varphi^h}\left[ \sum_{t=1}^T \log \left( \frac{p_\varphi^h(\vx_{t-1}|\vx_t)}{p_\theta^\text{data}(\vx_{t-1}|\vx_t)} \right) \right] = \mathbb{E}_{(\vx_0, \dots, \vx_T) \sim p_\varphi^h}\left[ \sum_{t=1}^T \frac{1}{2 \tilde \beta_t}  \|\Delta_h(\vx_t, t) \|_2^2  \right]
%\end{align}
%Note, that here the numerator and denominator are changed. This term is a regularisation for the norm of the distance to the original mean. If we use this regulariser in the supervised training setting, we obtain a similar strategy to Section 4.2 in \cite{fan2024reinforcement}.

\section{Resampling Step}

\subsection{Proof to Theorem \ref{the:}}
\label{app:proof_resampling}
\begin{proof}
Part (i) is a Bayes theorem.

The proof of part (ii) follows similar ideas as \citep[Prop 8 (ii)]{hertrich2024importance}. To this end, let $Z=\mathbb{E}_{\vx_{[0,T]}\sim \sP_h}[\alpha(\vx_{[0,T]})]$.
Then, we can estimate by Jensens' inequality that
\begin{align*}
-\log(Z)&=-\log\left(\mathbb{E}_{\vx_{[0,T]}\sim \sP_h}\left[\min\left(\frac{\dd \sP_\mathrm{data}}{\dd\sP_h}(\vx_{[0,T]})\frac{\exp(r(\vx_0))}{c},1\right)\right]\right)\\
&\leq -\mathbb{E}_{\vx_{[0,T]}\sim \sP_h}\left[\log\left(\min\left(\frac{\dd \sP_\mathrm{data}}{\dd\sP_h}(\vx_{[0,T]})\frac{\exp(r(\vx_0))}{c},1\right)\right)\right]\\
&= -\mathbb{E}_{\vx_{[0,T]}\sim \sP_h}\left[\min\left(\log\left(\frac{\dd \sP_\mathrm{data}}{\dd\sP_h}(\vx_{[0,T]})\frac{\exp(r(\vx_0))}{c}\right),0\right)\right]\\
&=\mathbb{E}_{\vx_{[0,T]}\sim \sP_h}\left[\max\left(\log\left(\frac{\dd \sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}\right),0\right)\right]
\end{align*}
On the other side, we have by definition that
\begin{align*}
\mathrm{KL}(\tilde{\sP}_h,\sP_\mathrm{data})-\mathbb{E}_{\vx_{[0:T]}\sim\tilde{\sP}_h}[r(\vx_0)]&=\mathbb{E}_{\vx_{[0:T]}\sim\tilde{\sP}_h}\left[\log\left(\frac{\dd \tilde{\sP}_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\right)\right]-\mathbb{E}_{\vx_{[0:T]}\sim\tilde{\sP}_h}[r(\vx_0)]\\
&=\mathbb{E}_{\vx_{[0:T]}\sim\tilde{\sP}_h}\left[\log\left(\frac{cZ}{\exp(r(\vx_0))}\frac{\dd \tilde{\sP}_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\right)\right]-\log(cZ).
\end{align*}
By taking the expectation over $\sP_h$ instead of $\tilde{\sP}_h$ this is equal to
\begin{align*}
\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\frac{\dd \tilde{\sP}_h}{\dd \sP_h}(\vx_{[0,T]})\log\left(\frac{cZ}{\exp(r(\vx_0))}\frac{\dd \tilde{\sP}_h}{\dd\sP_h}(\vx_{[0,T]})\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\right)\right]-\log(cZ).
\end{align*}
Inserting the formula from part (i) and then the definition of $\alpha$, this is equal to
\begin{align*}
&\qquad\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\frac{\alpha(\vx_{[0,T]})}{Z}\log\left(\frac{c}{\exp(r(x))}\alpha(\vx_{[0,T]})\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\right)\right]-\log(cZ)\\
&=\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\frac{\alpha(\vx_{[0,T]})}{Z}\log\left(\frac{c}{\exp(r(x))}\min\left(1,\frac{\dd \sP_\text{data}}{\dd \sP_h}(\vx_{[0,T]}) \frac{\exp(r(\vx_0))}{c}\right)\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\right)\right]-\log(cZ)\\
&=\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\frac{\alpha(\vx_{[0,T]})}{Z}\log\left(\min\left(\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}, 1\right)\right)\right]-\log(cZ)\\
&=\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\frac{\alpha(\vx_{[0,T]})}{Z}\min\left(\log\left(\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}\right), 0\right)\right]-\log(cZ)\\
&=\frac1Z\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\min\left(\alpha(\vx_{[0,T]})\log\left(\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}\right), 0\right)\right]-\log(cZ)\\
&=\frac1Z\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\min\left(\min\left(1,\frac{\dd \sP_\text{data}}{\dd \sP_h}(\vx_{[0,T]}) \frac{\exp(r(\vx_0))}{c}\right)\log\left(\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}\right), 0\right)\right]-\log(cZ).
\end{align*}
Since $1\leq \frac{\dd \sP_\text{data}}{\dd \sP_h}(\vx_{[0,T]}) \frac{\exp(r(\vx_0))}{c}$ if and only if $\log\left(\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}\right)\leq 0$ the minimum is attained either for both $\min$ in the above formula in the first argument or it is attained for both $\min$ in the second argument. Consequently the above formula is equal to
\begin{align*}
\frac1Z\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\min\left(\log\left(\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}\right), 0\right)\right]-\log(cZ)\\
\leq \mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\min\left(\log\left(\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}\right), 0\right)\right]-\log(Z)-\log(c),
\end{align*}
where the inequality comes from the fact that $Z\in(0,1]$ and that the expectation is non-positive (since the integrand is non-positive). Inserting the formula of $-\log(Z)$ from the beginning of the proof, this is equal to
\begin{align*}
&\quad\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\min\left(\log\left(\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}\right), 0\right)\right]\\
&\qquad+\mathbb{E}_{\vx_{[0,T]}\sim \sP_h}\left[\max\left(\log\left(\frac{\dd \sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}\right),0\right)\right]-\log(c)\\
&=\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\log\left(\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\frac{c}{\exp(r(\vx_0))}\right)\right]-\log(c)\\
&=\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}\left[\log\left(\frac{\dd\sP_h}{\dd\sP_\mathrm{data}}(\vx_{[0,T]})\right)\right]-\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}[r(\vx_0)]\\
&=\mathrm{KL}(\sP_h,\sP_\mathrm{data})-\mathbb{E}_{\vx_{[0:T]}\sim\sP_h}[r(\vx_0)].
\end{align*}
This concludes the proof of part (ii).

For part (iii), we observe that the loss in  \eqref{eq:supervised_finetuning_loss} is learning the marginal score of a forward SDE (in our case a VP-SDE intiliased at $\tilde{\sP}_h^0$, by construction the associated path measure of this process is given by (See appendix A \citep{vargas2023denoising} for a similar sketch) \footnote{Note we slightly abuse notation here as these equalities are meant to be understood in an RND sense but we omit the full ratio notation for brevity.}:
\begin{align}
 \sP_{h^*}(\cdot) &=  \sP_{\mathrm{forward}}(\cdot | \vx_0)  \tilde{\sP}_h^0(\vx_0) \\
 &=  \sP_{\mathrm{forward}}(\cdot | \vx_0) \sP_{\mathrm{data}}(\vx_0) \cdot \frac{\tilde{\sP}_h^0}{p_{\mathrm{data}}}(\vx_0) \\
 & = \frac{\tilde{\sP}_h^0}{p_{\mathrm{data}}} \sP_{\mathrm{data}} (\cdot)
\end{align}
Thus, the path measure minimising the score-matching loss satisfies 
\begin{align}
     \sP_{h^*} = \frac{\tilde{\sP}_h^0}{p_\mathrm{data}} \sP_{\mathrm{data}} 
\end{align}
% and thus the score matching loss \eqref{eq:supervised_finetuning_loss} has the same minimiser as
%     \begin{align}
% \min_{g_t}\mathrm{KL}\left(\frac{\tilde{\sP}_h^0}{p_{\mathrm{data}}} \sP_{\mathrm{data}}  \Big|\Big|  \sP_g\right)
%     \end{align}
% such that $\sP_{h^*}=\frac{\tilde{\sP}_h^0}{p_{\mathrm{data}}} \sP_{\mathrm{data}}$.
Then it follows that
\begin{align}
\mathrm{KL}(\sP_{h^*}||\sP_\mathrm{data})&=\mathbb{E}_{\vx_{[0:T]}\sim\sP_{h^*}}\left[\ln\left(\frac{\dd \sP_\mathrm{data}}{\dd \sP_\mathrm{data}}(\vx_{[0:T]})\frac{\tilde{\sP}_h^0(\vx_0)}{p_\mathrm{data}(\vx_0)}\right)\right]\\
&=\mathbb{E}_{\vx_{[0:T]}\sim\frac{\tilde{\sP}_h^0}{p_{\mathrm{data}}} \sP_{\mathrm{data}}}\left[\ln\left(\frac{\tilde{\sP}_h^0(\vx_0)}{p_\mathrm{data}(\vx_0)}\right)\right]\\
&=\mathbb{E}_{\vx_{0}\sim\tilde{\sP}_h^0}\left[\ln\left(\frac{\tilde{\sP}_h^0(\vx_0)}{p_\mathrm{data}(\vx_0)}\right)\right]=\mathrm{KL}(\tilde{\sP}_h^0, p_\mathrm{data})
\end{align}
Via the disintegration theorem \citep[Theorem 1.6. and Theorem 2.4]{leonard2014some}, the KL divergence can be decomposed as
\begin{align}
    \KL(\tilde{\sP}_h,\sP_\mathrm{data}) = \E[\KL(\tilde{\sP}_h(\cdot|\vx_0), \sP_\mathrm{data}(\cdot|\vx_0))] + \KL(\tilde{\sP}_h^0, p_\mathrm{data})
\end{align}
Where $\E[\KL(\tilde{\sP}_h(\cdot|\vx_0),\sP_\mathrm{data}(\cdot|\vx_0))] \geq 0$ and thus
$$
\mathrm{KL}(\sP_{h^*},\sP_\mathrm{data})=\mathrm{KL}(\tilde{\sP}_h^0, p_\mathrm{data})\leq\mathrm{KL}(\tilde{\sP}_h, \sP_\mathrm{data}).
$$

Since the marginals of $\sP_{h^*}$ and $\tilde{\sP}_h$ at time $0$ coincide, we obtain
\begin{align}
    \mathcal F(\sP_{h^*})&=\KL(\sP_{h^*}, \sP_\text{data}) -\mathbb{E}_{\vx_{[0,T]}\sim \sP_{h^*}}[r(\vx_0)]  \\
    &= \KL(\sP_{h^*}, \sP_\text{data}) - \mathbb{E}_{\vx_{[0,T]}\sim \tilde{\sP}_{h}}[r(\vx_0)] \\
    &\le \KL(\tilde{\sP}_{h}, \sP_\text{data}) - \mathbb{E}_{\vx_{[0,T]}\sim \tilde{\sP}_{h}}[r(\vx_0)]=\mathcal F(\tilde{\sP}_{h}),
\end{align}
which shows the first inequality from the claim. The second inequality was proven in part (ii).  
\end{proof}


\section{Experimental Details and Additional Results}

\subsection{2D Toy Example}\label{app:toy_details}

We train the initial diffusion model on a Gaussian mixture model with modes $25$ equally weighted modes where the set of means is defined by $\{-2.5,-1.25,0,1.25,2.5\}^2$ and the covariance matrix is given by $0.1 I$. The reward function $r$ is defined as the negative log likelihood function of the GMM with four modes with means $(-2.5,1.25)$, $(-1.25,2.5)$, $(1.25,0)$ and $(2.5,-1.25)$, covariance matrix $0.1 I$ and mode weights $\frac18$, $\frac18$, $\frac58$ and $\frac18$. 
For classifier guidance, we choose the parameter $\gamma$ by maximising the expected reward, which is $\gamma=0.3$.
For the importance and reward-only fine-tuning we use a batch size of $4096$, a buffer size of $6000$ and a KL regularisation with $\alpha_\text{KL}= 0.2$. We initialise the buffer with samples from the initial score-model.
Then we perform $40$ fine-tuning steps with $200$ gradient updates per iteration. We choose $c$ such that $30\%$ of the samples will be rejected.
The total training time is approximately $1.5$min on a single NVIDIA GeForce RTX~4090.

\subsection{Class conditional sampling}

%\paragraph{MNIST} 
We pre-train a score-based diffusion model on the MNIST dataset with the VP-SDE with $\beta_\text{min}=0.1$ and $\beta_\text{max}=20.0$. We parametrise the score model using a small Attention UNet \citep{dhariwal2021diffusion} with approx.~$1.1$M parameters. We parametrise the $h$-transform as in Equation~\eqref{eq:lkhd_informed_model} with approx. $0.8$M parameters. For all experiments, we use a batch size of $256$, a buffer size of $2048$ and a reward scaling $\lambda = 4.0$. We perform $50$ importance fine-tuning iterations with $50$ gradient updates per iterations. We use the KL regulariser with $\alpha_\text{KL}= 0.001$. Lastly, we adaptively choose $c$ such that $10\%$ of the samples will be accepted for the first $10$ steps. After the initial $10$ steps, we accept $30\%$ of the samples. We start with $10\%$ as MNIST has $10$ classes with roughly similar class probabilities. The total training time is about $10$min on a single NVIDIA GeForce RTX~4090.

For the comparison methods in Table~\ref{tab:mnist_results}, we use classifier guidance and online fine-tuning. For classifier guidance, we used a scaling $\gamma =4.5$ maximising sample quality. For online fine-tuning we directly minimise Equation~\eqref{eq:online_loss} using VarGrad \citep{richter2020vargrad}. VarGrad enables us to detach the trajectory from the backpropagation, thus saving memory cost and enabling us to train with a larger batch size. In particular, we use the formulation in \cite{denker2024deft} as 
\begin{align}
\label{eq:var_grad_stochastic_control}
  D_{\mathrm{logvar}}(\sP_h, \sP_\text{data}; \mathbb{W}) = \text{Var}_{\bH^{g_t}_{0:T} \sim \mathbb{W}} \left[\ln   \frac{\dd  \sP_h }{\dd \sP_\text{data}}(\bH^{g_t}_{0:T})\right], 
\end{align}
evaluated at a reference process $\mathbb{W} = \mathrm{Law}(\mH^{g_t}_{0:T})$, given by
\begin{align} 
\bH_T &\sim Q_T^{f_t}[p_\text{tilted}] \nonumber\\
     \dd \bH_t &= \left( f_t(\bH_t) - \sigma^2_t (s_t(\bH_t) + g_t(\bH_t)) \right)\,\dd t + \sigma_t\; \bwd{ \dd \rv{W}}_t.
\end{align}
In particular, if we choose $g_t=\mathrm{stop\_grad}(h_t)$  as a detached copy of the current estimation $h_t$, we obtain
\begin{align}
\label{eq:rnd_vargrad}
\ln   \frac{\dd  \sP_h }{\dd  \sP_\text{data}}(\mH^{g_t}_{0:T}) & =  -\frac{1}{2}\int_0^T \sigma_t^{2} \| h_t(\mH_t^{g_t}) \|^2 \dd t  +   \int_0^T \sigma_t^2 (g_t^\top h_t) (\mH_t^{g_t}) \dd t - r( \vx_0^{g_t} )  \nonumber\\
&+  \int_0^T \sigma_t h_t^\top (\mH_t^{g_t})\bwd{ \dd \rv{W}}_t,
\end{align}
using the RND for time reverse SDEs see Equation 64 in \citep{nusken2024transport}. We use the same reward-informed network architecture to parametrise $h_t$.

In Figure~\ref{fig:mnist_class_cond_full} we show the posterior for all $10$ classes. For all experiments, we chose the same hyperparameters as well as the same initial seed for the samples presented. We observe that these settings work for most classes. However, for class "Four" we see a single digit "Six" inthe image, giving evidence that this particular model has not fully converged. 



%\paragraph{CIFAR10}


\begin{figure*}[t]
\centering
\begin{subfigure}[t]{.19\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_zero.png}
  \caption*{Posterior: Zero}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.19\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_one.png}
    \captionsetup{justification=centering}
  \caption*{Posterior: One}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.19\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_two.png}
  \caption*{Posterior: Two}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.19\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_three.png}
  \caption*{Posterior: Three}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.19\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_four.png}
    \captionsetup{justification=centering}
  \caption*{Posterior: Four}
\end{subfigure}%

\begin{subfigure}[t]{.19\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_five.png}
  \caption*{Posterior: Five}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.19\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_six.png}
  \caption*{Posterior: Six}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.19\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_seven.png}
    \captionsetup{justification=centering}
  \caption*{Posterior: Seven}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.19\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_eight.png}
  \caption*{Posterior: Eight}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{.19\textwidth}
  \includegraphics[width=\linewidth]{imgs/cond_samples_nine.png}
  \caption*{Posterior: Nine}
\end{subfigure}%
\caption{Class conditional sampling for MNIST for all classes. We used the same initial seed. The model for "Posterior: Four" has apparently not fully converged, as we still see a "6" in the image.} \label{fig:mnist_class_cond_full}
\end{figure*}

\subsection{Reward Fine-tuning}
\label{app:rewardfinetuning}
Diffusion models can be used for conditional generation via classifier free guidance \citep{ho2022classifier}. In classifier free guidance both an unconditional $\epsilon^\theta_t(\vx_t)$ and a conditional $\epsilon^\theta_t(\vx_t, c)$ noise-prediction model are learned by randomly masking out the text prompt $c$. During sampling the linear combination $\bar{\epsilon}^\theta_t(\vx_t, t) = (1 + w) \epsilon^\theta_t(\vx_t, c) - w \epsilon^\theta_t(\vx_t)$, with a guidance scale $w$, is used. Despite the progress in training text-to-image diffusion models, the samples are not always aligned with human preferences. For the alignment we make use of \texttt{ImageReward-v1.0}, a human preference reward model \citep{xu2024imagereward}. These reward models $r(\vx; c)$ are trained to produce a reward from a given text prompt $c$ and an image $\vx$, corresponding to human preferences. Here, we learn the tilted distribution for a given text prompt and directly fine-tune the classifier free model $\bar{\epsilon}^\theta_t(\vx_t, t)$ using the LoRA parametrisation \citep{hu2021lora} instead of the reward-informed parametrisation. We use a LoRA for all attention layers with a rank of $4$. We used a buffer of $200$ images and sample $32$ new images at every iteration, the parameter $c$ was chosen such at $40\%$ of the images were accepted. We used a total of $50$ iteration, where we did $60$ gradient descent steps with a batch size of $4$ at each iteration. Fine-tuning took about $4$ hours on a single NVIDIA Geforce RTX 4090. We used the AdamW optimiser \citep{loshchilov2017decoupled} with a learning rate of $1 \times 10^{-4}$ and a cosine decay to $1 \times 10^{-5}$. For the final sampling evaluation we used the DDIM scheduler with $50$ time steps. We used the default guidance scale of $7.5$ for Stable Diffusion. 

\end{document}
