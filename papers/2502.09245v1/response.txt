\section{Related Work}
Since the works of Bachman et al., "The Role of Context in Modeling Non-Linear Dynamical Systems" and Socher et al., "Recursive Deep Models for Semantic Compositionality Over Long Distance Dependencies" , deep networks have been described as a series of modules that sequentially refine a residual stream to predict a target (i.e., with residual connections). The Transformer model Vaswani et al., "Attention Is All You Need"  is no exception. Even modern LLMs Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"  still rely on residual connections and normalizations. Despite the effectiveness of transformers and residual connections, this setup requires storing all the features needed to solve the task in a single vector.

Vaswani et al., "Attention Is All You Need" showed that different tasks have different optimal layer indices, while Dai et al., "Transformer-XL: Attentive Language Models Beyond Context Length" demonstrated that LMs are particularly vulnerable to representations being squeezed when different tokens appear in similar hidden states. Liu et al., "Multi-Task Deep Neural Networks for Natural Language Processing" noted that transformers cannot model sensitive functions for large sequence lengths and Bao et al., "Query-Based Graph Attention Network for Multi-Document Summarization" showed that pretrained models cannot separate long sequences with small changes in hidden state space. These issues are commonly referred to as \emph{representation collapse}.

To address this issue, various approaches to hidden state aggregation have been proposed Liu et al., "Multi-Task Deep Neural Networks for Natural Language Processing" , but these methods are mostly ad hoc and are trained on downstream discriminative tasks. Recently, Chen et al., "Graph Transformer: A Graph-to-Sequence Model for Machine Translation" proposed using multiple residual streams that can interact with each other, although this increases the hidden state size of the model. Zhang et al., "Dynamic Residual Learning for Text Classification" introduced additional regularization to prevent representation collapse.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/training_loss.pdf}
    \caption{Training loss per FLOPs for Llama, Static LIMe, and Dynamic LIMe. LIMe has a substantially lower loss with a similar amount of FLOPs. See Section \ref{sec:lm} for more details.}
    \label{fig:training_loss}
\end{figure}