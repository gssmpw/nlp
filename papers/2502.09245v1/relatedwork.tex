\section{Related Work}
Since the works of \citet{srivastava2015highway} and \citet{he2015deep}, deep networks have been described as a series of modules that sequentially refine a residual stream to predict a target (i.e., with residual connections). The Transformer model \citep{vaswani2017attention} is no exception. Even modern LLMs \citep{grattafiori2024llama, jiang2023mistral, qwen2024qwen25, deepseek-ai2024deepseekv2} still rely on residual connections and normalizations. Despite the effectiveness of transformers and residual connections, this setup requires storing all the features needed to solve the task in a single vector.

\citet{tenney-etal-2019-bert} showed that different tasks have different optimal layer indices, while \citet{voita-etal-2019-bottom} demonstrated that LMs are particularly vulnerable to representations being squeezed when different tokens appear in similar hidden states. \citet{hahn-rofin-2024-sensitive} noted that transformers cannot model sensitive functions for large sequence lengths and \citet{barbero2024transformers} showed that pretrained models cannot separate long sequences with small changes in hidden state space. These issues are commonly referred to as \emph{representation collapse}.

To address this issue, various approaches to hidden state aggregation have been proposed \citep{cl_attn, DBLP:conf/interspeech/YangCCLLLLSCLHT21}, but these methods are mostly ad hoc and are trained on downstream discriminative tasks. Recently, \citet{zhu2024hyperconnections} proposed using multiple residual streams that can interact with each other, although this increases the hidden state size of the model. \citet{arefin2024seqvcr} introduced additional regularization to prevent representation collapse.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/training_loss.pdf}
    \caption{Training loss per FLOPs for Llama, Static LIMe, and Dynamic LIMe. LIMe has a substantially lower loss with a similar amount of FLOPs. See Section \ref{sec:lm} for more details.}
    \label{fig:training_loss}
\end{figure}