%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{listings}
\usepackage{xcolor}

\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\bfseries\color{blue},
    frame=tb,
    numbers=left,
    numberstyle=\tiny,
    showstringspaces=false
}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{You Do Not Fully Utilize Transformer's Representation Capacity}

\begin{document}

\twocolumn[
\icmltitle{You Do Not Fully Utilize Transformer's Representation Capacity}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Gleb Gerasimov}{equal,ttech,hse}
\icmlauthor{Yaroslav Aksenov}{equal,ttech}
\icmlauthor{Nikita Balagansky}{ttech,mipt}
\icmlauthor{Viacheslav Sinii}{ttech}
\icmlauthor{Daniil Gavrilov}{ttech}
\end{icmlauthorlist}

\icmlaffiliation{ttech}{T-Tech}
\icmlaffiliation{hse}{HSE University}
\icmlaffiliation{mipt}{Moscow Institute of Physics and Technology}

\icmlcorrespondingauthor{Yaroslav Aksenov}{y.o.aksenov@tbank.ru}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\DeclareRobustCommand{\v}[1]{\ensuremath{\mathbf{#1}}}

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
In contrast to RNNs, which compress previous tokens into a single hidden state, Transformers can attend to all previous tokens directly. However, standard Transformers only use representations from the immediately preceding layer. In this paper, we show that this design choice causes representation collapse and leads to suboptimal performance. To address this issue, we introduce \emph{Layer-Integrated Memory} (LIMe), a simple yet powerful approach that preserves the model’s overall memory footprint while expanding its representational capacity by allowing access to hidden states from earlier layers. Through extensive experiments across various architectures and different lookup mechanisms, we demonstrate consistent performance improvements on a wide range of tasks. Moreover, our analysis of the learned representation dynamics and our exploration of depthwise circuits reveal how LIMe integrates information across layers, pointing to promising directions for future research.
\end{abstract}


\section{Introduction}
\label{sec:introduction}

Transformers \citep{vaswani2017attention} have become a central architecture in modern machine learning, powering state-of-the-art solutions in language modeling, computer vision, and beyond. Their ability to capture complex patterns arises from deeply stacked layers that refine contextual representations. However, despite their success, standard Transformer decoders maintain a single residual stream per layer, forcing the model to compress all previously learned features into the immediately preceding hidden state \citep{srivastava2015highway, he2015deep}. This design choice can lead to \emph{representation collapse} — a phenomenon in which different tokens or features become indistinguishable in deeper layers \citep{voita-etal-2019-bottom, barbero2024transformers, arefin2024seqvcr}. The problem is particularly pronounced when learning from lengthy sequences, where subtle token distinctions risk being squeezed out by limited floating-point precision and finite hidden-state capacity.

In this paper, we propose \emph{Layer-Integrated Memory}{\footnote{Source code is available by the link \href{https://github.com/corl-team/lime}{https://github.com/corl-team/lime}}} (\textbf{LIMe}), a simple yet powerful extension to masked multi-head self-attention that enables the model to retrieve and integrate representations from all earlier layers — rather than relying solely on the most recent hidden state. LIMe accomplishes this by learning a special routing mechanism that efficiently blends multi-layer features for both keys and values, all while preserving the core Transformer structure and adding negligible overhead.

Through extensive language modeling experiments and in-depth analysis, we demonstrate three main contributions. \textbf{First}, LIMe consistently outperforms standard Transformer baselines \citep{grattafiori2024llama} and other state-of-the-art modifications \citep{zhu2024hyperconnections} across a diverse range of one-shot benchmarks. \textbf{Second}, our empirical investigations show that LIMe effectively counters representation collapse: it preserves higher entropy in deeper layers, increases separability for closely related tokens, and improves overall representational diversity. \textbf{Third}, we provide insights into the \emph{depthwise circuits} LIMe learns, revealing how crucial lexical or syntactic cues from earlier layers are seamlessly reintroduced in later layers. Together, these findings indicate that explicitly integrating multi-layer memory not only enhances performance but also yields richer, more interpretable internal representations.

Our results suggest a promising direction for building deeper and more robust Transformers. By decoupling the burden of storing all relevant context from the single residual stream, LIMe opens the door to a range of architectural advances that harness earlier-layer features more effectively.

\section{Related Work}

Since the works of \citet{srivastava2015highway} and \citet{he2015deep}, deep networks have been described as a series of modules that sequentially refine a residual stream to predict a target (i.e., with residual connections). The Transformer model \citep{vaswani2017attention} is no exception. Even modern LLMs \citep{grattafiori2024llama, jiang2023mistral, qwen2024qwen25, deepseek-ai2024deepseekv2} still rely on residual connections and normalizations. Despite the effectiveness of transformers and residual connections, this setup requires storing all the features needed to solve the task in a single vector.

\citet{tenney-etal-2019-bert} showed that different tasks have different optimal layer indices, while \citet{voita-etal-2019-bottom} demonstrated that LMs are particularly vulnerable to representations being squeezed when different tokens appear in similar hidden states. \citet{hahn-rofin-2024-sensitive} noted that transformers cannot model sensitive functions for large sequence lengths and \citet{barbero2024transformers} showed that pretrained models cannot separate long sequences with small changes in hidden state space. These issues are commonly referred to as \emph{representation collapse}.

To address this issue, various approaches to hidden state aggregation have been proposed \citep{cl_attn, DBLP:conf/interspeech/YangCCLLLLSCLHT21}, but these methods are mostly ad hoc and are trained on downstream discriminative tasks. Recently, \citet{zhu2024hyperconnections} proposed using multiple residual streams that can interact with each other, although this increases the hidden state size of the model. \citet{arefin2024seqvcr} introduced additional regularization to prevent representation collapse.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/training_loss.pdf}
    \caption{Training loss per FLOPs for Llama, Static LIMe, and Dynamic LIMe. LIMe has a substantially lower loss with a similar amount of FLOPs. See Section \ref{sec:lm} for more details.}
    \label{fig:training_loss}
\end{figure}

\section{Preliminary}
\label{sec:preliminary}

The decoder-only Transformer architecture comprises $N$ identical layers. Each layer includes a masked multi-head self-attention sub-layer followed by a position-wise feed-forward network. Residual connections and layer normalization are applied around each sub-layer to facilitate training.

\subsection{Attention Mechanism}

The attention mechanism enables the model to assign different levels of importance to tokens when encoding a particular token. Scaled dot-product attention is central to the Transformer model.

Given queries \(\mathbf{Q} \in \mathbb{R}^{n\times d}\), keys \(\mathbf{K} \in \mathbb{R}^{n\times d}\), and values \(\mathbf{V} \in \mathbb{R}^{n\times d}\), where \(n\) is the sequence length, the attention function is:
\begin{equation*}
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\Bigl(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}}\Bigr)\mathbf{V}.
\end{equation*}
The factor \(\sqrt{d}\) helps mitigate large dot-product values, stabilizing gradients during training.

\begin{table*}[ht!]
    \centering
    \small
    \caption{LM Evaluation Harness benchmarks (accuracies in \%) on 1.2B models with num-fewshots 1. The rightmost column shows average accuracy across the tasks. Proposed methods outperform both LLaMA and HyperConnections \citep{zhu2024hyperconnections} baselines. See Section \ref{sec:lm} for more details.}
    \label{tab:lm_eval}
    \begin{tabular}{r|cccccccc||c}
    \toprule
    \textbf{Model} & \textbf{ARC-E} & \textbf{ARC-C} & \textbf{Winogrande} & \textbf{COPA} & \textbf{MultiRC} & \textbf{RTE} & \textbf{HellaSwag} & \textbf{PIQA} & \textbf{Avg} \\
    \midrule
    LLaMA 
      & 69.5 
      & 38.7 
      & 55.2 
      & 75.0 
      & 42.8 
      & 54.5 
      & 53.1 
      & 72.5 
      & 57.7 \\
    HC 
      & 70.1 
      & 38.4 
      & 53.0 
      & 77.0 
      & 42.9 
      & 51.6 
      & 54.4 
      & \textbf{73.5} 
      & 57.6 \\
    \textbf{LIMe Dynamic} 
      & \textbf{72.7} 
      & \textbf{39.5} 
      & 53.1 
      & \textbf{79.0} 
      & 43.0 
      & 52.4 
      & \textbf{54.4} 
      & 72.9 
      & \textbf{58.4} \\  % Highest average
    \textbf{LIMe Static} 
      & 71.1 
      & 39.3 
      & \textbf{56.2} 
      & 75.0 
      & \textbf{43.1} 
      & \textbf{55.2} 
      & 53.9 
      & 72.2 
      & 58.3 \\
    \bottomrule
    \end{tabular}
\end{table*}

To prevent the model from accessing future tokens during training, masking is applied in self-attention so that each position only attends to past outputs.

The multi-head self-attention mechanism is defined as:
\begin{equation*}
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \dots, \text{head}_H)\,\mathbf{W}^{(O)},
\end{equation*}
where each head is computed by
\begin{equation*}
\text{head}_i = \text{Attention}\bigl(\mathbf{Q}\,\mathbf{W}_i^{(Q)},\ \mathbf{K}\,\mathbf{W}_i^{(K)},\ \mathbf{V}\,\mathbf{W}_i^{(V)}\bigr).
\end{equation*}
Here, \(\mathbf{W}_i^{(Q)} \in \mathbb{R}^{d\times d_h}\), \(\mathbf{W}_i^{(K)} \in \mathbb{R}^{d\times d_h}\), \(\mathbf{W}_i^{(V)} \in \mathbb{R}^{d\times d_h}\), and \(\mathbf{W}^{(O)} \in \mathbb{R}^{H d_h \times d}\) are learned projection matrices, \(i \in 1\dots H\), \(d_h\) is the dimensionality of each head (usually \(d_h < d\)), and \(H\) is the number of heads. The concatenation of heads is performed along the last dimension.

\subsection{Gated Feed-Forward Networks}
In the LLaMA architecture, each decoder layer contains a Gated Feed-Forward Network that is applied identically to every position. The Gated MLP can be written as:
\begin{equation*}
\mathrm{FFN}(x) \;=\;
\Bigl[\,\bigl(x\,W_{g} + b_{g}\bigr)
  \;\odot\;
  \mathrm{SiLU}\!\bigl(x\,W_{f} + b_{f}\bigr)
\Bigr]\,W_{o} \;+\; b_{o}.
\end{equation*}
where \(\odot\) denotes element-wise multiplication and 
\(W_{g}, W_{f}, W_{o}, b_{g}, b_{f}, b_{o}\) are learned parameters.

\subsection{Residual Connections}
Residual connections are crucial in Transformers, helping address vanishing gradients and enabling deeper networks to be trained effectively. In both encoder and decoder blocks, each sub-layer (e.g., multi-head attention or feed-forward) incorporates a residual connection.

Formally, let \(\mathbf{x}\) be the input to a sub-layer and \(\mathcal{F}(\mathbf{x})\) its function (e.g., multi-head attention or a feed-forward network). The output \(\mathbf{y}\) of the sub-layer with a residual connection is:
\begin{equation*}
    \mathbf{y} = \mathbf{x} + \mathcal{F}\bigl(\text{LayerNorm}(\mathbf{x})\bigr).
    \label{eq:residual_connection}
\end{equation*}
These connections provide each sub-layer access to both the transformed output and the original inputs, promoting more stable and efficient training.

For the following sections, let \(\mathbf{X}_0\) be the sequence embeddings and  \(\mathbf{X}_\ell\) be the hidden representation \(\mathbf{y}\) after the \(\ell\)-th decoder layer.


\section{Method}
\label{sec:method}

We introduce a mechanism called \emph{Layer-Integrated Memory} (LIMe) to augment Transformer decoders with multi-layer memory. Starting from the \emph{second} decoder layer (\(\ell = 2\)), we replace \emph{all} Masked Multi-Head Attention (MHA) heads with specialized heads that attend to a learnable convex combination of the representations from all earlier layers (including the original embeddings). We describe below how LIMe forms these combinations and parameterizes their computation, either \emph{statically} or \emph{dynamically}.

\subsection{LIMe Overview}
\label{sec:lime}

In a standard Transformer decoder, each \(\ell\)-th layer (\(1 \le \ell \le L\)) takes as input a residual stream \(\mathbf{X}_{\ell-1}\) and produces updated representations \(\mathbf{X}_\ell\). The MHA sub-layer typically derives queries, keys, and values from \(\mathbf{X}_{\ell-1}\). In contrast, LIMe modifies the \emph{key-value} side of attention while keeping queries the same. Specifically, queries $\mathbf{Q}$ remain a projection of \(\mathbf{X}_{\ell-1}\); keys $\mathbf{K}$ and values $\mathbf{V}$ are generated from a convex combination of all preceding layer outputs \(\{\mathbf{X}_{0}, \dots, \mathbf{X}_{\ell-1}\}\), where \(\mathbf{X}_{0}\) is the initial token embedding.


In order to simplify our notation, we denote LIMe layers that \emph{retrieve} information as $r$ (with $2 \leq r \leq L$). 
Meanwhile, we denote layers that \emph{provide “memorized” features} as $m$ (with $0 \leq m \leq r - 1$).

\subsection{LIMe Router}
\label{sec:lime_router}

To form keys and values from multiple past representations, LIMe uses an \emph{inter-layer router}. For each layer \(r \ge 2\), a unique router outputs a weight matrix 
\(
R \in \mathbb{R}^{H \times r},
\)
where \(H\) is the number of attention heads.

\paragraph{Forming Convex Mixtures.}
Let \(\mathbf{X}_{r - 1} \in \mathbb{R}^{t \times d}\) be the residual stream after layer \(r - 1\). We want to compute the next layer’s output \(\mathbf{X}_{r}\) by aggregating features from \(\{\mathbf{X}_{0}, \dots, \mathbf{X}_{r-1}\}\). We define the Key-Value projection inputs for head $h$ as

\begin{equation}
\label{eq:kv_lime_input}
\mathbf{Z}_{h}^{(r)} 
=\; 
\mathbb{E}_{m\sim p(m|h, r)}\bigl[X_m\bigr] \in \mathbb{R}^{t \times d},
\end{equation}

where \(p(m \mid h, r) = \alpha^{(r)}_{h,m}\) are per-level head-specific routing coefficients from a weight matrix $R$, softmax-normalized to sum to 1 per head to form valid discrete distributions, giving each LIMe router the ability to emphasize higher- or lower-level features. In $\mathbf{Z}_{h}^{(r)}$, the superscript \(r\) indicates that these features are computed at layer \(r\). Henceforth, to simplify notation, we omit the superscript in subsequent derivations.

\paragraph{Key-Value Projections.}
Each head \(h\) maps \(\mathbf{Z}_{h}\) to keys \(\mathbf{K}_{h}\) and values \(\mathbf{V}_{h}\) via learnable projection matrices $\mathbf{W}_{h}^{(K)} \in \mathbb{R}^{d \times d_h}$ and $\mathbf{W}_{h}^{(V)} \in \mathbb{R}^{d \times d_h}$:
\[
\mathbf{K}_{h} 
\;=\; 
\mathbf{Z}_{h}\,\mathbf{W}_{h}^{(K)},
\quad
\mathbf{V}_{h}
\;=\;
\mathbf{Z}_{h}\,\mathbf{W}_{h}^{(V)}.
\]
Thus, instead of relying solely on \(\mathbf{X}_{r-1}\), each head can attend to the specific learned mixture of all prior layers.

\paragraph{Query Projection.}
Queries remain a projection of the residual stream via 
\(\mathbf{W}^{(Q)} \in \mathbb{R}^{d \times (h \times d_h)}\), 
split into separate queries per head:
\[
\mathbf{Q}_h \;=\; \mathbf{X}_{r-1}\,\mathbf{W}_{h}^{(Q)}.
\]
This preserves standard decoder attention on the query side, while the key-value side gains deeper contextual information.

By using the LIMe Router block, LIMe heads can draw from any prior layer’s representations, creating a \emph{layer-wise memory} in the decoder. Unlike standard skip connections or naive averaging, LIMe learns distinct weightings over past states per head, capturing richer dependencies. This design resembles Ladder Networks \citep{Rasmus2015SemisupervisedLW} in that it can “forget” information that can be recovered later via routers. Compared to naive multi-head attention, LIMe adds negligible overhead that is linear in the sequence length, which can be further minimized with pruning techniques as we discuss in Section~\ref{subsection:deep_nets}. Note that this architecture can be viewed as a modification of the input to the attention block, preserving compatibility with methods for efficient MHA computation \citep{dao2022flashattention, dao2023flashattention2, shah2024flashattention3fastaccurateattention}. For more details on method efficiency, refer to Section~\ref{appendix:efficiency}.

\subsection{Static and Dynamic Routers}
\label{sec:routers}

The LIMe Router's coefficients \(R_{h,m}\) in layer $r$ can be parameterized in two ways.

\paragraph{Static Router.}
In the \emph{static} router, \(R\) is a learned matrix for each layer, \(R \in \mathbb{R}^{H \times r}\), which is the same for all tokens in a batch. We apply softmax over the entries of \(R\) to obtain
\[
\alpha^{(r)}_{h,m}
\;=\; 
\frac{\exp(R_{h,m})}{\sum_{j=0}^{r - 1} \exp(R_{h,j})}, 
\quad m \in 0\dots r - 1.
\]
This approach is simple and adds minimal overhead.

\paragraph{Dynamic Router.}
In the \emph{dynamic} router, \(R\) is computed by a linear function of \(\mathbf{X}_{r-1}\). For \(\mathbf{X}_{r-1} \in \mathbb{R}^{t \times d}\), we define
\[
R(\mathbf{X}_{r-1}) 
\;:=\; 
\mathrm{DynamicRouter}(\mathbf{X}_{r-1}),
\]
where \(\mathrm{DynamicRouter}: \mathbb{R}^{t \times d} \to \mathbb{R}^{t \times H \times r}\). Here, each token can produce its own routing matrix, making it more flexible, but adding some computational and memory costs.
The pseudocode for LIMe is available in \cref{appendix:pseudocode}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/mean_weight_for_layers.pdf}
    \vspace{-25pt}
    \caption{Mean retrieval weight for each representation ($m$) among later layers ($r$). In both cases, in the last layers, models tend to retrieve information from previous layers rather than from the current one. In the case of Dynamic LIMe, there is a clear bump in retrieving from the first layer. See Section \ref{subsec:analysing_routings} for more details.}
    \label{fig:mean_weight_of_repr}
\end{figure}

\section{Experiments}
\subsection{Language Modeling}
\label{sec:lm}

We evaluate the effectiveness of \emph{LIMe} by comparing two variants---\textbf{LIMe Static} and \textbf{LIMe Dynamic}---against two baselines: \textbf{LLaMa}~\citep{grattafiori2024llama} and \textbf{Hyper Connections}~\citep{zhu2024hyperconnections}. 

\paragraph{Training Setup.}
All models have approximately 1.2B parameters and share the same underlying transformer architecture (see \cref{tab:model_arch}). Each model is trained from scratch on a \emph{FineWeb Edu} subset with about 50B tokens. The full training setup can be found in \cref{appendix:training_setup}.

\paragraph{Results and Evaluation.}
Figure~\ref{fig:training_loss} displays the training loss curves, demonstrating that both \textit{Dynamic LIMe} and \textit{Static LIMe} converge more rapidly than LLaMa. When normalized by floating-point operations (FLOPs), both LIMe variants achieve lower perplexities than LLaMa, thereby indicating improved parameter efficiency. Table~\ref{tab:lm_eval} presents results on the one-shot LM Eval Harness benchmarks, which further highlight the advantages conferred by LIMe. Overall, these results suggest that LIMe’s layer-integrated memory mechanism consistently provides benefits over baseline methods. In the next section, we go deeper into the factors driving these gains. For detailed efficiency information, see Section~\ref{appendix:efficiency}.

\subsection{Analysing Learned Routings in LIMe}
\label{subsec:analysing_routings}

One of the goals of our work is to illuminate \emph{how} LIMe routes information across multiple prior layers, thereby mitigating representation collapse. To this end, we examine the learned routing weights (both static and dynamic) and analyze the resulting ``semantic circuits'' that emerge within each Transformer layer.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/previous_repr_weight_heads.pdf}
    \caption{Self Retrieval weights for each head of Static and Dynamic LIMe. Both models assign higher weights to the latest representation in the middle layers, but tend to retrieve lower-level features later. The depicted weights decrease significantly in almost all heads, although some of them still use self-retrieval paths, suggesting the outputs' refinement stage. Moreover, we can see that Dynamic LIMe's first layers heavily rely on low-level features due to their sequence conditioning. See Section \ref{subsec:analysing_routings} for more details.}
    \label{fig:prev_repr_heads}
\end{figure}


In the static case, each head’s weights are shared across the entire batch, whereas the dynamic router can modulate its mixture coefficients on a per-token basis. To approximate these dynamic priors, we aggregated routing outputs over 50{,}000 sequences from FineWeb Edu. For a full, detailed view of LIMe Dynamic and Static Router distributions, see Appendix Figure~\ref{fig:weights_heatmaps}.

Appendix Figure~\ref{fig:prev_repr_mean} further summarizes how much weight is allocated specifically to the final residual stream, averaged over all heads. Notably, even in the deepest LIMe layers, a subset of heads still attends to lower-level representations, which helps prevent crucial lexical distinctions from being compressed out.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/level_expected.pdf}
    \caption{Expected retrieved representation for each LIMe layer ($r$). Both static and dynamic variants tend to retrieve information from early layers. See Section \ref{subsec:analysing_routings} for more details.}
    \label{fig:level_expected}
\end{figure}

\paragraph{Layer-Wise Mixtures and Emphasis on Shallow Representations.}
Figure~\ref{fig:mean_weight_of_repr} illustrates how much each LIMe layer relies on the most recent residual stream versus lower-level features. Under standard self-attention, this weight would be fixed at 1.0. More detailed per-head Self Retrieval weights are visualized in Figure~\ref{fig:prev_repr_heads}. In LIMe we see that early layers strongly incorporate embeddings or low-level features, suggesting specialized ``morphological circuits'' that cluster tokens by subword or lexical patterns. Moreover, in the \emph{static} router, layers~11--16 distinctly favor layer~10, hinting at next-token prediction circuits that consistently re-use mid-level context. Turning to the \emph{dynamic} router, we observe that middle layers, although similar to self-attention, allocate a substantial fraction of their prior to representations from the first layer. The final layer places its heaviest emphasis on layers~2--4 instead, possibly to refine earlier morphological or syntactic cues before producing the output logits. Remarkably, Dynamic LIMe's first layers hardly rely on embeddings' features, in contrast to Static, due to their ability to fine-tune weights conditioned on the sequence.
\paragraph{Expected Retrieved Representation.}
To quantify how each LIMe layer distributes attention across all prior representations (for notation, please refer to \ref{sec:lime}), we define the joint distribution
\(
p(m, h \mid r) = p(m \mid h, r) \, p(h \mid r),
\)
where the term \(p(h \mid r)\) is obtained by normalizing the output-norm contribution of each head in the MHA output projection (i.e., a normalization over the per-head slices of \(\mathbf{W}^{(O)}\)). Here, \(r\) is the layer from which we consider the lookup of the previous layers. We then compute the expected retrieved representation $m$ as
\begin{equation}
\label{eq:exp_level_joint}
\mathbb{E}\bigl[m|r\bigr]
\;=\;
\sum_{h=1}^{H} 
p(h\mid r)\,
\sum_{j=0}^{r-1} 
p\bigl(j \mid h, r\bigr)\cdot j.
\end{equation}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/entropy_values.pdf}
    \caption{Values' matrix entropy on FineWeb Edu subset by layers. Both Dynamic and Static LIMe have more diverse values than LLaMa, which indicates more information stored in LIMe.}
    \label{fig:entropy_values}
\end{figure}
Figure~\ref{fig:level_expected} shows that in early layers, both LIMe Static and LIMe Dynamic closely track standard self-attention, as their expected representation levels lie near the diagonal. However, at greater depths, these LIMe variants diverge more substantially from self-attention, indicating heavier reliance on lower-level features. This selective retrieval from earlier layers alleviates the need to preserve every relevant feature in a single residual stream. By layer 16, LIMe's expected retrieved representation~($m$) is markedly lower than that of standard self-attention (e.g., $m = r - 1$), suggesting that LIMe routes a nontrivial portion of attention to preceding layers rather than focusing solely on the immediately previous hidden state. This flexibility helps leverage earlier-layer nuances and mitigates representation collapse.



\paragraph{Interpreting Dynamic Router embeddings.}
To shed light on the semantic roles of dynamic LIMe routers, we draw inspiration from the Logit Lens approach \citep{nostalgebraist2020logitlens}. We represent each router’s learned transformation as an embedding and retrieve its nearest neighbors (top-30) from the token embedding matrix. The decoded tokens offer clues about the \emph{semantic route} that particular router emphasizes---for instance, punctuation marks, function words, or morphological fragments.


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/values_clouds.pdf}
    \caption{t-SNE of similar tokens' values among layers. Values obtained from LIMe are separable in later layers, while values in LLaMA become mixed and lose information about tokens. See Section \ref{subsection:representation} for more details.}
    \label{fig:values_clouds}
\end{figure*}

Table~\ref{table:tokens_example} illustrates a subset of these \emph{semantic circuits}: for example, one circuit focuses on English suffixes (layers~$4$/$9$), another on intensifiers and comparative modifiers (layer~$15$), and another on subordinating conjunctions (layer~$10$). These findings align with our broader hypothesis that LIMe’s multi-layer routing fosters more specialized token pathways, alleviating the pressure on later layers to maintain all salient information in a single residual vector.


Overall, our analysis indicates that LIMe leverages earlier-layer features in a more flexible and targeted fashion compared to conventional self-attention. By explicitly allocating heads to retrieve distinct representations, LIMe not only reduces the risk of collapse but also exposes interpretable circuits that capture morphological, lexical, and syntactic patterns at varying depths. Exploring more fine-grained interpretability approaches (e.g., sparse autoencoders) remains a promising direction for future research.

\begin{table*}[ht]
\centering
\small
\caption{Table with examples of tokens where semantic circuits activate in Dynamic LIMe. L -- layer which makes the query, R -- level of queried representation, H -- head number. This result indicates that the model learns depthwise circuits to bypass information without change to a further layer. See Section \ref{subsec:analysing_routings} for more details.}
\label{table:tokens_example}
\begin{tabular}{|r|r|r|p{7cm}|p{7cm}|}
\hline
\textbf{L} & \textbf{R} & \textbf{H} & \textbf{Semantic circuit description} & \textbf{Tokens examples} \\
\hline
4 & 0 & 23 &
Primarily partial word segments that illustrate English morphological composition. &
lex, ache, isters, ique, ley, elling, ets, ry. \\
\hline
9 & 1 & 3 &
A range of English suffixes or near-suffix fragments that highlight morphological building blocks and transformations. &
ist, ised, ishing, osed, ized, ense, istic, ish, ened, inch. \\
\hline
8 & 0 & 10 &
Primarily affixes and stems that indicate morphological processes in English. &
izing, ically, ified, ission, ational, ist, ering. \\
\hline
15 & 1 & 23 &
A collection of intensifiers, qualifiers, and comparative modifiers that adjust tone and degree in writing. &
very, various, respective, relatively, highly, latter, largely, particularly. \\
\hline
10 & 1 & 18 &
Primarily subordinating conjunctions and discourse markers for conditions or reasons, illustrating causation, contingency, and contrast. &
Because, If, Although, While, There, According, Unlike, However, It, Even. \\
\hline
\end{tabular}
\end{table*}




\subsection{Representation collapse analysis}
\label{subsection:representation}

Recent work has shown that large language models (LLMs) can suffer from \emph{representation collapse} when representing long sequences, forcing subtle token distinctions to become inseparable in deeper layers~\citep{voita-etal-2019-bottom, arefin2024seqvcr}. We investigate this phenomenon by comparing LLaMa~\citep{grattafiori2024llama}, LIMe (Static), and LIMe (Dynamic) via two complementary approaches: (i)~quantifying hidden-states and values diversity with \emph{matrix-based R\'enyi entropy}~\citep{arefin2024seqvcr} and (ii)~measuring and visualizing linear separability of layer-wise embeddings of closely related tokens (\texttt{is}, \texttt{are}, \texttt{was}, \texttt{were}) \citep{voita-etal-2019-bottom}. These two methodologies directly measure representation collapse in language models;

\paragraph{Matrix-Based R\'enyi Entropy.}
Following \citet{arefin2024seqvcr}, we measure the diversity of hidden representations at layer~\(\ell\) by forming the Gram matrix 
\(\mathbf{K} = Z^{(\ell)}\,{Z^{(\ell)}}^{\top} \in \mathbb{R}^{T\times T}\),
where \(Z^{(\ell)}\) contains the \(d\)-dimensional representations of~\(T\) tokens. Let \(\{\lambda_i(\mathbf{K})\}_{i=1}^T\) be the eigenvalues of~\(\mathbf{K}\). We define the \(\alpha\)-order R\'enyi entropy as
\(
S_\alpha\bigl(Z^{(\ell)}\bigr) \;=\; 
\frac{1}{\,1-\alpha\,}\;\log\biggl[\;
\sum_{i=1}^T\Bigl(\tfrac{\lambda_i(\mathbf{K})}{\mathrm{tr}(\mathbf{K})}\Bigr)^\alpha
\biggr].
\)
Each eigenvalue is normalized by \(\mathrm{tr}(\mathbf{K})\), ensuring the probabilities sum to~1. Higher \(S_\alpha\) indicates greater variance (i.e., lower collapse). Although \citet{arefin2024seqvcr} measured matrix entropy on gathered hidden states instead of values from MHA to evaluate models' representation collapse, we compare LLaMa and LIMe on values representations. LIMe can store and retrieve information outside of the current representation; that's why its entropy measured on hidden states can be statistically the same as LLaMa's, but LIMe can still distinguish tokens by its values. Figure~\ref{fig:entropy_values} shows that both LIMe router parameterizations yield significantly higher matrix entropy of gathered MHA values compared to LLaMa. For visualized entropy on hidden states refer to Appendix Figure~\ref{fig:entropy_hiddens}.



\paragraph{Layer-Wise Token Separability.}
To more directly assess representation collapse, we replicate the methodology of \citet{voita-etal-2019-bottom}, extracting 1668 occurrences each of \texttt{is}, \texttt{are}, \texttt{was}, \texttt{were} from the \emph{FineWeb Edu} corpus. At each layer $\ell$, we record \emph{Value embeddings} (i.e.\ the output of the linear projection of the attention values), and \emph{Hidden states} (i.e.\ the residual stream \(\mathbf{X}_\ell\)).

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/values_acc.pdf}
    \caption{Values classification accuracy measured with standard deviation over 5 cross-validation folds. Values in later layers obtained from LIMe can be linearly separated with nearly 1.0 accuracy, while accuracy for values from LLaMA is much lower. See Section \ref{subsection:representation} for more details.}
    \label{fig:values_acc}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/training_loss_deep.pdf}
    \caption{Training losses for deep architectures. The LIMe architecture significantly outperforms the baseline, especially in the case of $128$ layers. See Section \ref{subsection:deep_nets} for more details.}
    \label{fig:deep_losses}
\end{figure}

We then project these representations into two-dimensional space via t-SNE and visualize how well the tokens cluster. For the same reason as in Matrix-Based R\'enyi Entropy, we compare LLaMa and LIMe on values' visualizations in Figure~\ref{fig:values_clouds}. In LLaMa, deeper-layer representations for such similar tokens often collapse into overlapping regions, reflecting the model’s inclination to compress all relevant information into a single representation. For hidden states' clusters comparison, refer to Appendix Figure~\ref{fig:hidden_states_clouds}.



\paragraph{Linear Classification Results.}
To additionally quantify these observations, we train a linear four-way classifier (for \texttt{is}, \texttt{are}, \texttt{was}, \texttt{were}) on layer-wise representations. Figure~\ref{fig:values_acc} shows mean classification accuracies (with five-fold cross-validation) for value embeddings layer by layer. We observe that LIMe consistently exhibits higher classification accuracy than LLaMa, confirming that LIMe’s value representations avoid collapse. Dynamic LIMe shows better representation capacity than Static due to its conditioning on the sequence. For classification results on hidden states see Appendix Figure~\ref{fig:hidden_acc}.



Together, these results corroborate our theoretical motivation: by allowing each head to attend directly to earlier-layer representations, LIMe expands the overall representational capacity. This multi-layer routing reduces collapse in the \emph{values} while freeing deeper \emph{hidden} states from the burden of storing all lexical nuances — leading to higher overall entropy on values (Figure~\ref{fig:entropy_values}) and improved model performance (Table~\ref{tab:lm_eval}).

\subsection{Deep networks performance}
\label{subsection:deep_nets}



\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/deep_repr_weights.pdf}
    \caption{Retrieval weights statistics for a 128-layer LIMe model trained with top-$p$ pruning. The mean retrieval weight from subsequent layers (green curve) displays several distinct peaks, indicating that the model acquires multiple information streams in a self-supervised fashion. The mean self-retrieval weight (orange curve), where 1.0 denotes self-attention, decreases across later layers, forming three consecutive layer groups with different information-processing patterns. See Section~\ref{subsection:deep_nets} for further details.}
    \label{fig:deep_repr_weights}
\end{figure}

Scaling Transformers to large depths often causes representation collapse, thus motivating our investigation of LIMe in 32-, 64-, and 128-layer architectures. We compare Static LIMe to the baseline LLaMA, each configured with eight attention heads per layer. To mitigate computational overhead, we apply a top-$p$ pruning strategy to the LIMe router weights at every layer, while it retains sufficient flexibility to outperform LLaMA at all tested depths (Figure~\ref{fig:deep_losses}). See Section~\ref{appendix:pruning} for pruning details.

Empirically, LIMe exhibits superior scaling behavior relative to LLaMA as the model depth increases, suggesting that LIMe’s ability to directly route information from earlier layers effectively enlarges the representational capacity, while LLaMA’s single residual stream struggles to maintain fine-grained features from distinct layers.

In Figure~\ref{fig:deep_repr_weights}, one observes several distinct peaks in the average weight distribution, signifying that certain mid-level layers act as "information streams" reused by multiple subsequent layers. It also reveals that while early layers rely heavily on their immediate predecessors (i.e., strong self-attention to layer $i-1$), deeper layers revisit earlier representations more frequently. Collectively, these routing patterns form three distinct stages in the network’s depth: 

\begin{enumerate}
    \item \emph{Shallow processing}, where initial layers focus on lexical cues and local context (mostly retrieving first-layer features).
    \item \emph{Mid-layer aggregation}, where representations converge around local layers, presumably capturing more abstract semantics.
    \item \emph{Late-stage refinement}, where the final layers selectively gather both low-level embeddings and mid-layer feature maps, often skipping the immediately preceding layers.
\end{enumerate}
For the full distribution of the 128-layer LIMe Router, see Figure~\ref{fig:deep_all_weights}. Overall, these experiments' results indicate that the optimal scaling architecture is probably deeper than that of transformer models. We leave these experiments for future work.




\section{Conclusion and Future Work}
\label{section:conclusion}

In this work, we introduced \emph{Layer-Integrated Memory (LIMe)}, a simple yet effective modification to Multi-Head Attention that mitigates \emph{representation collapse} in Transformer decoders. By allowing each attention head to form \emph{convex combinations} of all previous layer outputs, LIMe addresses the fundamental constraint of finite hidden-state capacity. Empirically, this mechanism elevates the model’s overall representational power with negligible parameter and computational overhead. This flexible retrieval capability eases the burden on later layers, enabling them to focus on integrative decision making rather than continually preserving fine lexical or syntactic cues in a single residual stream.

While LIMe demonstrates consistent gains in both performance and representational diversity, our studies also raise broader questions for future research: (1) at which "width to depth" ratio LIMe models scale the best, due to their observed ability to form information circuits; (2) how sparse LIMe Routers can be and still keep LIMe's representational superiority.

Our findings highlight that preserving some intermediate representations can unlock greater flexibility and improve language modeling performance. We hope these insights stimulate further exploration of multi-layer memory mechanisms in large-scale sequence modeling, guiding the design of more robust, interpretable, and effective Transformer architectures.


\section{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Training Setup}
\label{appendix:training_setup}

Both \(\text{Static LIMe}\) and \(\text{Dynamic LIMe}\) apply the router from the second decoder layer onward, as described in \cref{sec:method}. For \(\text{Dynamic LIMe}\), we use weight decay only on the router parameters, whereas for \(\text{Static LIMe}\) we omit weight decay on the router. We found that increasing the router learning rate to \(1 \times 10^{-2}\) boosts model performance by speeding up router convergence and circuit formation. We initialize the static router weights \(\mathbf{R}\) with a standard normal distribution scaled by 0.1 to encourage a near-uniform mixture of previous layers. In the dynamic case, we use simple linear layers without bias to produce the routing coefficients.

\begin{table}[ht!]
    \centering
    \caption{Key training hyperparameters used in all experiments.}
    \label{tab:training_hparams}
    \begin{tabular}{ll}
    \toprule
    \textbf{Hyperparameter} & \textbf{Value} \\
    \midrule
    Optimizer & AdamW \\
    Learning Rate & 0.001 \\
    LIMe Router Learning Rate & 0.01 \\
    Weight Decay & 0.1 \\
    \(\beta_1\) & 0.9 \\
    \(\beta_2\) & 0.95 \\
    \(\epsilon\) & \(1\times10^{-8}\) \\
    Scheduler & cosine \\
    Warmup Steps & 200 \\
    Min LR & \(1\times10^{-6}\) \\
    Mixed Precision & bf16 \\
    Gradient Clipping & 1.0 \\
    \midrule
    Sequence Length & 2048 \\
    Batch Size & 1024 \\
    Training Steps & 20,000 \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[ht!]
    \centering
    \caption{Model architecture for all variants (LLaMa, Hyper Connections, and LIMe) at the 1.2B scale.}
    \label{tab:model_arch}
    \begin{tabular}{ll}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Vocab Size & 50{,}257 \\
    Hidden Size & 2048 \\
    Intermediate Size & 8192 \\
    Number of Hidden Layers & 16 \\
    Number of Attention Heads & 32 \\
    Tie Word Embeddings & True \\
    \bottomrule
    \end{tabular}
\end{table}

\section{Pruning details}
\label{appendix:pruning}

After softmax-normalizing the router weights, we sort them in descending order along the layer dimension and select the minimal subset of weights whose cumulative sum exceeds $p=0.9$; the remaining weights are interpreted as zero weights. Table~\ref{tab:top_p_stats} summarizes the number of pruned router weights at $p=0.9$ for different models' depths.

\begin{table}[ht!]
\centering
\caption{Number of pruned LIMe Router's weights at top-$p=0.9$ for various model depths. As we can see in deep models' training results (\ref{subsection:deep_nets}), retrieval paths pruning does not stop LIMe from being superior compared to LLaMA.}
\label{tab:top_p_stats}
\resizebox{0.5\columnwidth}{!}{%
\begin{tabular}{l|c|c}
\toprule
\textbf{Model Depth} & \textbf{Total Router Weights} & \textbf{Pruned Weights (\%)} \\
\midrule
32-layer  & 7{,}936   & 1{,}845 (23\%) \\
64-layer  & 32{,}256  & 6{,}795 (21\%) \\
128-layer & 130{,}048 & 24{,}632 (19\%) \\
\bottomrule
\end{tabular}%
}
\end{table}

\section{Additional LIMe Analysis}
\label{appendix:lime_figures}

Router weights, both static and dynamic, suggest that the model reuses previous hidden states, especially in later layers. As shown in Figure~\ref{fig:weights_heatmaps}, early layers prioritize the most recent representation (high diagonal values), resembling standard self-attention. However, from layer 12 onward, this pattern shifts, with lower weights assigned to recent states in favor of earlier ones. Figure~\ref{fig:prev_repr_mean} confirms this trend, showing higher weights on the latest hidden state in early layers. Figure~\ref{fig:prev_repr_heads} further illustrates that while most heads follow this pattern, some in later layers resemble self-attention, likely to update retrieved representations.

Figures~\ref{fig:values_clouds} and \ref{fig:hidden_states_clouds} show that LIMe layers yield more separable value states than LLaMA, while hidden states are mixed. Despite collapsed hidden states in both models, the LIMe model suffers from this to a lesser extent, as it attends to past representations as well. Figures~\ref{fig:entropy_hiddens} and \ref{fig:entropy_values} suggest this applies to entire contexts, albeit less distinctly.

Scaling LIMe to 128 layers reinforces these retrieval patterns. As seen in Figure~\ref{fig:deep_all_weights}, the first 80 layers mainly perform self-attention (strong diagonal weights), while later layers distribute attention across earlier representations. Distinct vertical stripes indicate that certain representations remain consistently attended to across layers.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/previous_repr_weight.pdf}
    \caption{Self Retrieval weights averaged across heads for each LIMe layer.}
    \label{fig:prev_repr_mean}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/entropy_hiddens.pdf}
    \caption{Hiddens' matrix entropy on FineWeb Edu subset by layers. We can see that hidden states in LIMe can be not very diverse for the model to provide better performance on language tasks. For details, see Section~\ref{subsection:representation}.}
    \label{fig:entropy_hiddens}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/hidden_states_acc.pdf}
    \caption{Hidden states classification accuracy measured with standard deviation over 5 cross-validation folds. Although LLaMa’s deeper layers maintain stronger linear separability, LIMe’s hidden states become slightly harder to cluster in later layers due to its ability to smoothly move on to predicting the next token using the full hidden states’ dimensionality.}
    \label{fig:hidden_acc}
\end{figure}


\twocolumn
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth, height=0.9\textheight, keepaspectratio]{figures/weights_heatmaps.pdf}
    \caption{Learned static weights and dynamic prior distribution calculated on a subset of Fineweb Edu. Each cell represents retrieval probability for each layer in the specific head.}
    \label{fig:weights_heatmaps}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth, height=0.9\textheight, keepaspectratio]{figures/deep_all_weights.pdf}
    \caption{All weights for deep static LIMe with 128 layers. We can see explicitly the repeated routing patterns resembling a refinement process.}
    \label{fig:deep_all_weights}
\end{figure}
\onecolumn

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/hidden_states_clouds.pdf}
    \caption{t-SNE of similar tokens' hidden states among layers. Although hidden states are not separable in later layers for both models, unlike LLaMA, LIMe can make updates attending to the previous representations, which leads to high values' separability. See Section \ref{subsection:representation} for more details.}
    \label{fig:hidden_states_clouds}
\end{figure*}


\begin{table*}[ht!]
\centering
\small
\caption{Comparing efficiency for all $1.2$B models: both Dynamic and Static LIMe enjoy negligible parameter and FLOPs increase, and smaller peak memory than HC during training. When the Key-Value cache is utilized, this memory advantage extends to inference as well (*). H -- number of heads, L — number of layers, T — sequence length, D — hidden dimension, R — Hyper Connections \citep{zhu2024hyperconnections} expansion rate.}
\label{tab:efficiency_table}
\begin{tabular}{r|c|c|c|cc}
\toprule
\textbf{Model} 
  & \textbf{\# Parameters (B)}
  & \textbf{FLOPs (T)} 
  & \multicolumn{2}{c}{\textbf{Peak Memory Overhead over LLaMA excluding parameters}} \\ 
\cmidrule(lr){4-5}
 & & & \textbf{Train} & \textbf{Inference} \\
\midrule
\textbf{LLaMA}       
  & $1.1767$
  & $2.97$
  & 0 
  & 0 \\
\textbf{LIMe Static} 
  & $1.1768$ (+0.008\%) 
  & $2.98$ (+0.3\%)
  & $(L-1)BTHD$
  & $BTHD \; ^{(*)}$ \\
\textbf{LIMe Dynamic}
  & $1.1856$ (+0.075\%) 
  & $3.01$ (+1.3\%) 
  & $BTH \left(\frac{L(L+1)}{2} - 1\right) + (L-1)BTHD$
  & $LBTH + BTHD \; ^{(*)}$ \\
\textbf{HC Dynamic}
  & $1.1771$ (+0.030\%)  
  & $2.98$ (+0.3\%) 
  & $2LBT[(R - 1)D + R(R+2))]$
  & $BT[(R - 1)D + R(R+2))]$ \\
\bottomrule
\end{tabular}
\end{table*}

\section{Efficiency}
\label{appendix:efficiency}

We evaluate \emph{LIMe} in terms of parameter usage, memory overhead, and computational cost in comparison to LLaMA~\citep{grattafiori2024llama} and Hyper-connections (HC)~\citep{zhu2024hyperconnections}. As shown in Table~\ref{tab:efficiency_table}, LIMe adds only a marginal number of extra parameters and forward FLOPs relative to LLaMA, while also avoiding the substantial memory overhead incurred by approaches that maintain multiple copies of residual streams (e.g., HC). In particular, during \emph{training}, LIMe can reuse the intermediate layer outputs that are already stored for backpropagation, thereby imposing negligible extra cost. During \emph{inference}, memory usage remains minimal as all the required tensors are already stored in the key-value cache. Computational overhead can be further reduced by decreasing the number of activated routes to several dozen, retaining only the most critical circuits (e.g., through top-$p$ pruning or $\ell_0$ regularization during training). These optimizations can be efficiently implemented using Triton/CUDA kernels.

\newpage
\section{LIMe Pseudocode}
\label{appendix:pseudocode}

\begin{lstlisting}[language=Python]
class StaticRouter(nn.Module):
    def __init__(self, num_heads, n_repr, init_coef):
        self.weights = nn.Parameter(torch.randn(num_heads, n_repr) * init_coef)

    def forward(self, stacked_hiddens):  # stacked_hiddens: [n_repr, batch, time, d]
        probs = self.weights.softmax(dim=-1)
        return torch.einsum('rl,lbtd->brtd', probs, stacked_hiddens)

class DynamicRouter(nn.Module):
    def __init__(self, hidden_size, num_heads, n_repr):
        self.proj = nn.Sequential(
            nn.Linear(hidden_size, num_heads * n_repr, bias=False),
            Reshape(batch, time, num_heads, n_repr)
        )

    def forward(self, stacked_hiddens):  # stacked_hiddens: [n_repr, batch, time, d]
        last = stacked_hiddens[-1]  # [batch, time, hidden_size]
        probs = self.proj(last).softmax(dim=-1)
        return torch.einsum("btrl,lbtd->brtd", probs, stacked_hiddens)

class LIMeAttentionProjection(nn.Module):
    def __init__(self, num_heads, hidden_size, head_dim):
        self.proj = nn.Parameter(torch.empty(1, num_heads, hidden_size, head_dim))
        nn.init.kaiming_uniform_(self.proj, a=math.sqrt(5))

    def forward(self, x):  # x: [batch, heads, time, hidden_size]
        return x @ self.proj

class LIMeSdpaAttention(BaseSelfAttention):
    def __init__(self, hidden_size, num_heads, head_dim):
        self.k_proj_lime = LIMeAttentionProjection(num_heads, hidden_size, head_dim)
        self.v_proj_lime = LIMeAttentionProjection(num_heads, hidden_size, head_dim)
        self.q_proj_default = nn.Linear(hidden_size, num_heads * head_dim)

    def forward(self, last_hidden, lime_representations, **kwargs):
        key_states = self.k_proj_lime(lime_representations)
        value_states = self.v_proj_lime(lime_representations)
        query_states = self.q_proj_default(last_hidden).permute(batch, num_heads, time, head_dim)
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, **kwargs)
        attn_output = scaled_dot_product_attention(query_states, key_states, value_states, **kwargs)
        return self.o_proj(attn_output.transpose(1, 2).reshape(batch, time, -1))

class LIMeLayer(BaseDecoderLayer):
    def __init__(self, hidden_size, lime_config, layer_idx, num_heads, head_dim):
        router_cls = DynamicRouter if lime_config.dynamic else StaticRouter
        self.attention_router = router_cls(**lime_config, n_repr=layer_idx)
        self.self_attn = LIMeSdpaAttention(hidden_size, num_heads, head_dim)
        self.lime_layernorm = LlamaRMSNorm(hidden_size)

    def forward(self, last_hidden, past_hiddens, **kwargs):
        residual = last_hidden
        last_hidden = self.input_layernorm(last_hidden)
        lime_representations = self.attention_router(self.lime_layernorm(past_hiddens))
        attn_out = self.self_attn(last_hidden, lime_representations, **kwargs)
        last_hidden = residual + attn_out

        # ... standard LLaMA MLP block ...

        return last_hidden

\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
