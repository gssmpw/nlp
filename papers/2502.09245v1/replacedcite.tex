\section{Related Work}
Since the works of ____ and ____, deep networks have been described as a series of modules that sequentially refine a residual stream to predict a target (i.e., with residual connections). The Transformer model ____ is no exception. Even modern LLMs ____ still rely on residual connections and normalizations. Despite the effectiveness of transformers and residual connections, this setup requires storing all the features needed to solve the task in a single vector.

____ showed that different tasks have different optimal layer indices, while ____ demonstrated that LMs are particularly vulnerable to representations being squeezed when different tokens appear in similar hidden states. ____ noted that transformers cannot model sensitive functions for large sequence lengths and ____ showed that pretrained models cannot separate long sequences with small changes in hidden state space. These issues are commonly referred to as \emph{representation collapse}.

To address this issue, various approaches to hidden state aggregation have been proposed ____, but these methods are mostly ad hoc and are trained on downstream discriminative tasks. Recently, ____ proposed using multiple residual streams that can interact with each other, although this increases the hidden state size of the model. ____ introduced additional regularization to prevent representation collapse.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/training_loss.pdf}
    \caption{Training loss per FLOPs for Llama, Static LIMe, and Dynamic LIMe. LIMe has a substantially lower loss with a similar amount of FLOPs. See Section \ref{sec:lm} for more details.}
    \label{fig:training_loss}
\end{figure}