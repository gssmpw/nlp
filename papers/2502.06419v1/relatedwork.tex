\section{RELATED WORK}
\subsection{Multimodal Large Language Model}

Recent advancements in Multimodal Large Language Models (MLLMs) have sparked interest by combining the advanced reasoning capabilities of LLMs with image, video, and audio data~\cite{li2023blip,zhu2023minigpt,liu2023visual,lu2024gpt}. These models have shown remarkable proficiency in tasks such as zero-shot and few-shot image classification, segmentation, and object detection by leveraging the synergy between visual and textual data. In the context of autonomous driving, LLMs address a critical gap by enhancing scene understanding, providing richer semantic context, and facilitating decision-making processes, which current systems lack.
Several methods have been proposed to leverage LLMs in autonomous driving. Vision-based approaches, such as DriveGPT4, interpret video inputs to generate driving-related textual responses~\cite{chen2023driving}, while models like HiLM-D enhance hazard identification and intention prediction through high-resolution visual data~\cite{ding2023hilm}. Lidar-based methods utilize vectorized visual embeddings to equip LLMs with environmental perception capabilities, enabling detailed analysis of the driving scene~\cite{fu2023driving}.

% Despite these advancements, existing methods often require significant computational resources and lack explainability in intermediate reasoning steps. For example, Talk2BEV combines Bird's Eye View (BEV) maps with linguistic context to enable visual-linguistic reasoning in autonomous vehicles~\cite{dewangan2023talk2bev}, yet the complexity of integrating multiple modalities presents challenges in real-time applications. Moreover, generative models like GAIA-1 highlight the potential of LLMs to anticipate various outcomes based on vehicle maneuvers but underscore the computational demands of such sophisticated models~\cite{hu2023gaia}.


\subsection{Occupancy}

% The majority of prevalent 3D perception methods, whether leveraging LiDAR sweeps, multi-view images, or multi-modal data, construct BEV (Bird's Eye View) feature representations before performing various downstream tasks in the BEV space \cite{bevdet, li2022bevdepth, li2022bevformer, liu2022bevfusion, liang2022bevfusion,lu2023towards,lu2024scaling}. 
Recently, 3D semantic occupancy provides a more detailed representation of the environment by explicitly modeling the occupancy status of each voxel within a 3D grid. SSCNet \cite{song2017semantic} was the first to introduce the task of semantic scene completion, integrating geometric and semantic information. Subsequent works commonly utilize geometric inputs with explicit depth information \cite{lmscnet, aicnet, js3cnet, sketch}. MonoScene \cite{cao2022monoscene} proposed the first monocular approach for semantic scene completion, using a 3D UNet \cite{ronneberger2015u} to process voxel features generated through sight projection. Various networks based on the transfer architecture have been designed~\cite{huang2023tri,huang2023tri,Zhang_2023_ICCV}. Additionally, several concurrent works have focused on proposing surrounding-view benchmarks for 3D semantic occupancy prediction, contributing to the rapid advancement of the occupancy community~\cite{wang2023openoccupancy,wang2023openoccupancy, wei2023surroundocc, tong2023scene, tian2023occ3d}. OccWorld learns a world model based on 3D occupancy, which has attracted much attention with its interpretability and efficiency. Further, this paper attempts to use the large language model as a bridge to unify occupancy tasks.


% TPVFormer \cite{huang2023tri} introduced a tri-perspective view representation for describing 3D scenes in semantic occupancy prediction. VoxFormer \cite{li2023voxformer} presented a two-stage transformer-based semantic scene completion framework capable of producing complete 3D volumetric semantics from only 2D images. OccFormer \cite{Zhang_2023_ICCV} introduced a dual-path transformer network to effectively process 3D volumes in semantic occupancy prediction, achieving long-range, dynamic, and efficient encoding of camera-generated 3D voxel features. 

% bevdet fb-bev bevformer-occ