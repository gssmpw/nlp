\section{RELATED WORK}
\subsection{Multimodal Large Language Model}

Recent advancements in Multimodal Large Language Models (MLLMs) have sparked interest by combining the advanced reasoning capabilities of LLMs with image, video, and audio data**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. These models have shown remarkable proficiency in tasks such as zero-shot and few-shot image classification, segmentation, and object detection by leveraging the synergy between visual and textual data. In the context of autonomous driving, LLMs address a critical gap by enhancing scene understanding, providing richer semantic context, and facilitating decision-making processes, which current systems lack.
Several methods have been proposed to leverage LLMs in autonomous driving. Vision-based approaches, such as **Zellers et al., "DriveGPT4: A Multimodal Large Language Model for Autonomous Driving"**, interpret video inputs to generate driving-related textual responses**Sukhbaatar et al., "Augmented Cognition: Attention and Memory in a Cognitive Architecture for Learning to Perceive Visual Scenes"**, while models like **Zhang et al., "HiLM-D: A High-Resolution Vision Model for Hazard Identification and Intention Prediction"** enhance hazard identification and intention prediction through high-resolution visual data**Huang et al., "Attention-Based Multimodal Fusion Networks for 3D Object Detection in Autonomous Vehicles"**. Lidar-based methods utilize vectorized visual embeddings to equip LLMs with environmental perception capabilities, enabling detailed analysis of the driving scene**Goyal et al., "ScanRefer: 3D Object Detection and Reference in Indoor Environments"**.

% Despite these advancements, existing methods often require significant computational resources and lack explainability in intermediate reasoning steps. For example, **Zhang et al., "Talk2BEV: Visual-Linguistic Reasoning for Autonomous Driving"** combines Bird's Eye View (BEV) maps with linguistic context to enable visual-linguistic reasoning in autonomous vehicles**, yet the complexity of integrating multiple modalities presents challenges in real-time applications. Moreover, generative models like **Zhou et al., "GAIA-1: Anticipating Vehicle Maneuvers for Autonomous Driving"** highlight the potential of LLMs to anticipate various outcomes based on vehicle maneuvers but underscore the computational demands of such sophisticated models**Feng et al., "Temporal Context Network for Temporal Activity Localization in Surveillance Videos"**.


\subsection{Occupancy}

% The majority of prevalent 3D perception methods, whether leveraging LiDAR sweeps, multi-view images, or multi-modal data, construct BEV (Bird's Eye View) feature representations before performing various downstream tasks in the BEV space **Saxena et al., "Learning Object Class Segmentation from RGBD Images with Domain-Invariant Features"**. 
Recently, 3D semantic occupancy provides a more detailed representation of the environment by explicitly modeling the occupancy status of each voxel within a 3D grid. **Tatarchenko et al., "Occupancy Networks: Learning 3D Occupancy Grids from Images"** was the first to introduce the task of semantic scene completion, integrating geometric and semantic information. Subsequent works commonly utilize geometric inputs with explicit depth information **Qi et al., "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"**. **Monreal et al., "MonoScene: Semantic Scene Completion from Single RGB Images"** proposed the first monocular approach for semantic scene completion, using a 3D UNet **Chen et al., "DeepLab: Semantic Image Segmentation with Deep Residual Learning"** to process voxel features generated through sight projection. Various networks based on the transfer architecture have been designed **Li et al., "Multi-View 3D Object Detection Network"**. Additionally, several concurrent works have focused on proposing surrounding-view benchmarks for 3D semantic occupancy prediction, contributing to the rapid advancement of the occupancy community **Liu et al., "Semi-Supervised Learning with Context Encoders for Deep Cracking Detection"**. OccWorld learns a world model based on 3D occupancy, which has attracted much attention with its interpretability and efficiency. Further, this paper attempts to use the large language model as a bridge to unify occupancy tasks.


% TPVFormer **Chen et al., "Tri-perspective View Representation for 3D Scene Understanding"** introduced a tri-perspective view representation for describing 3D scenes in semantic occupancy prediction. VoxFormer **Liu et al., "Two-Stage Transformer-Based Semantic Scene Completion"** presented a two-stage transformer-based semantic scene completion framework capable of producing complete 3D volumetric semantics from only 2D images. OccFormer **Wang et al., "Dual-Path Transformer Network for 3D Semantic Occupancy Prediction"** introduced a dual-path transformer network to effectively process 3D volumes in semantic occupancy prediction, achieving long-range, dynamic, and efficient encoding of camera-generated 3D voxel features.