\section{Related Work}
\label{sec:related_work_extended}

\paragraph{Preference Optimization in RLHF.}
Direct Preference Optimization (DPO) is a collection of techniques for fine-tuning language models based on human preferences \cite{rafailov2024direct}. Several variants of DPO have been developed to address specific challenges and improve its effectiveness \cite{ethayarajh2024kto,zeng2024token,dong2023raft,yuan2023rrhf}. For example, KTO and TDPO focus on different aspects of preference optimization, while RAFT and RRHF utilize alternative forms of feedback. Other variants, such as SPIN, CPO, ORPO, and SimPO, introduce additional objectives or regularizations to enhance the optimization process \cite{chen2024self,xu2024contrastive,hong2024orpo,meng2024simpo}.

Further variants, including R-DPO, LD-DPO, sDPO, IRPO, OFS-DPO, and LIFT-DPO, address issues like length bias, training strategies, and specific reasoning tasks. These diverse approaches demonstrate the ongoing efforts to refine and enhance DPO, addressing its limitations and expanding its applicability to various tasks and domains \cite{park2024disentangling,liu2024iterative,pang2024iterative,qi2024online, yuan2024following}.
% Reinforcement Learning from Human Feedback (RLHF) \citep{christiano2017deep, ziegler2019fine, ouyang2022training} typically focuses on pairwise preference comparisons between model outputs, training a reward model to distinguish “better” vs.\ “worse” responses and then fine-tuning a policy to maximize that reward. This preference-based approach has been widely adopted in language model alignment \citep{bai2022training, stiennon2020learning}, but it can oversimplify the range of human preferences by reducing them to binary comparisons.

\paragraph{Multi-Preference Approaches.}
Recent work extends standard RLHF to consider entire \emph{sets} of responses at once, enabling more nuanced feedback signals \citep{rafailov2024direct, cui2023ultrafeedback, chen2024noise}. Group-based objectives capture multiple acceptable (and multiple undesirable) answers for each query, rather than only a single “better vs.\ worse” pair. \citet{gupta2024swepo} propose a contrastive formulation, \swepo, that jointly uses multiple “positives” and “negatives.” Such multi-preference methods can reduce label noise and better reflect the complexity of real-world tasks, but their computational cost grows if one attempts to incorporate all generated outputs \citep{cui2023ultrafeedback, chen2024noise}.

\paragraph{On-Policy Self-Play.}
A key advancement in reinforcement learning has been \emph{self-play} or on-policy generation, where the model continuously updates and re-generates data from its own evolving policy \citep{silver2016mastering, silver2017mastering}. In the context of LLM alignment, on-policy sampling can keep the training set aligned with the model’s current distribution of outputs \citep{christiano2017deep, wu2023fine}. However, this approach can significantly inflate the number of candidate responses, motivating the need for selective down-sampling of training examples.

\paragraph{Active Learning for Policy Optimization.}
The notion of selectively querying the most informative examples is central to \emph{active learning} \citep{cohn1996active, settles2009active}, which aims to reduce labeling effort by focusing on high-utility samples. Several works incorporate active learning ideas into reinforcement learning, e.g., uncertainty sampling or diversity-based selection \citep{sener2017active, zhang2022active}. In the RLHF setting, \citet{christiano2017deep} highlight how strategic feedback can accelerate policy improvements, while others apply active subroutines to refine reward models \citep{wu2023fine}. By picking a small yet diverse set of responses, we avoid both computational blow-ups and redundant training signals.

\paragraph{Clustering and Coverage-Based Selection.}
Selecting representative subsets from a large dataset is a classic problem in machine learning and combinatorial optimization. \emph{Clustering} techniques such as $k$-means and $k$-medoids \citep{hartigan1979algorithm} aim to group points so that distances within each cluster are small. In the RLHF context, embedding model outputs and clustering them can ensure \emph{coverage} over semantically distinct modes \citep{har2004coresets, cohen2022improved}. These methods connect to the \emph{facility location} problem \citep{oh2017deep}—minimizing the cost of “covering” all points with a fixed number of centers—and can be addressed via coreset construction \citep{feldman2020core}. 

\paragraph{Min-Knapsack and Integer Programming.}
When picking a subset of size $k$ to cover or suppress “bad” outputs, one may cast the objective in a \emph{min-knapsack} or combinatorial optimization framework \citep{kellerer2004introduction}. For instance, forcing certain outputs to zero probability can impose constraints that ripple to nearby points in embedding space, linking coverage-based strategies to integer programs \citep{chen2020big}. \citet{cohen2022improved} and \citet{har2004coresets} demonstrate how approximate solutions to such subset selection problems can achieve strong empirical results in high-dimensional scenarios. By drawing from these established concepts, our method frames the selection of negative samples in a Lipschitz coverage sense, thereby enabling both theoretical guarantees and practical efficiency in multi-preference alignment.


Collectively, our work stands at the intersection of \emph{multi-preference alignment} \citep{gupta2024swepo, cui2023ultrafeedback}, \emph{on-policy data generation} \citep{silver2017mastering, ouyang2022training}, and \emph{active learning} \citep{cohn1996active, settles2009active}. We leverage ideas from \emph{clustering} (k-means, k-medoids) and \emph{combinatorial optimization} (facility location, min-knapsack) \citep{kellerer2004multidimensional, cacchiani2022knapsack} to construct small yet powerful training subsets that capture both reward extremes and semantic diversity. The result is an efficient pipeline for aligning LLMs via multi-preference signals without exhaustively processing all generated responses.
