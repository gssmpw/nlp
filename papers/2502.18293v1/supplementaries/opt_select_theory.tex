\section{Extended Theoretical Analysis of \textsc{Opt-Select}}
\label{sec:theory_opt_select_extended}

In this appendix, we present a more detailed theoretical treatment of $\ampoos$. We restate the core problem setup and assumptions, then provide rigorous proofs of our main results. Our exposition here augments the concise version from the main text.

\subsection{Problem Setup}

Consider a single prompt (query) \(x\) for which we have sampled \(n\) candidate responses \(\{\,y_1,\,y_2,\,\ldots,\,y_n\}\). Each response \(y_i\) has:
\begin{itemize}[itemsep=0.5em, leftmargin=1em]
    \item A scalar reward \(r_i \in [0,1]\).
    \item An embedding \(\mathbf{e}_i \in \mathbb{R}^d.\)
\end{itemize}
We define the distance between two responses \(y_i\) and \(y_j\) by
\begin{equation}
\label{eq:appdistdef}
A_{i,j} \;=\; \|\mathbf{e}_i \,-\, \mathbf{e}_j\|.
\end{equation}
We wish to learn a \emph{policy} \(\{p_i\}\), where \(p_i \ge 0\) and \(\sum_{i=1}^n p_i = 1\). The policy's \emph{expected reward} is
\begin{equation}
\label{eq:appexprew}
\mathrm{ER}(p) 
\;=\; 
\sum_{i=1}^n r_i \,p_i.
\end{equation}

\paragraph{Positive and Negative Responses.}
We designate exactly one response, denoted \(y_{i_{\mathrm{top}}}\), as a \emph{positive} (the highest-reward candidate). All other responses are potential ``negatives.'' Concretely:
\begin{itemize}[itemsep=0.5em, leftmargin=1em]
    \item We fix one index \(i_{\mathrm{top}}\) with \(\displaystyle i_{\mathrm{top}} \;=\; \arg \max_{i\in\{1,\dots,n\}}\,r_i.\)
    \item We choose a subset \(\mathcal{S}\subseteq \{1,\dots,n\}\setminus\{i_{\mathrm{top}}\}\) of size \(k\), whose elements are forced to have \(p_j=0\). (These are the ``negatives.'')
\end{itemize}

\subsubsection{Lipschitz Suppression Constraint}
\label{subsec:LipschitzConstraintApp}

We assume a mild Lipschitz-like rule:
\begin{enumerate}[label=(A\arabic*), itemsep=0.5em]
    \item\label{asmp:Lipschitz} \textbf{\(L\)-Lipschitz Constraint.} If \(p_j = 0\) for some \(j\in \mathcal{S}\), then for every response \(y_i\), we must have
    \begin{equation}
    \label{eq:LipschitzConstraintApp}
    p_i 
    \;\le\; 
    L\, A_{i,j}
    \;=\;
    L\,\|\mathbf{e}_i \,-\,\mathbf{e}_j\|.
    \end{equation}
\end{enumerate}
The effect is that whenever we force a particular negative \(j\) to have \(p_j=0\), any response \(i\) near \(j\) in embedding space also gets \emph{pushed down}, since \(p_i \le L\,A_{i,j}\). By selecting a set of $k$ negatives covering many ``bad'' or low-reward regions, we curb the policy's probability of generating undesirable responses.

\paragraph{Goal.} 
Define the feasible set of distributions:
\begin{equation}
\label{eq:feasibleRegionApp}
\mathcal{F}(\mathcal{S}) 
\;=\; 
\Bigl\{\,
\{p_i\}\colon p_j=0 \ \forall\,j\in \mathcal{S}, \ 
p_i \le L\, \min_{j\in \mathcal{S}} A_{i,j}\ \forall\,i\notin\{\,i_{\mathrm{top}}\}\cup\mathcal{S}
\Bigr\}.
\end{equation}
We then have a two-level problem:
\begin{align}
\nonumber
&\max_{\,\substack{\mathcal{S}\,\subseteq \{1,\dots,n\}\setminus\{i_{\mathrm{top}}\}\\ |\mathcal{S}|=k}}  
\quad
\max_{\substack{\{p_i\}\in \mathcal{F}(\mathcal{S}) \\ \sum_i p_i = 1,\;p_i\ge 0}}
\quad
\sum_{i=1}^n r_i\, p_i,
\\[0.75em]
&\text{subject to}\quad p_{i_{\mathrm{top}}}\text{ is unconstrained (no Lipschitz bound).}
\label{eq:lip_mainObjApp}
\end{align}
We seek \(\mathcal{S}\) that \emph{maximizes} the best possible Lipschitz-compliant expected reward.

\subsection{Coverage View and the MIP Formulation}

\paragraph{Coverage Cost.}
To highlight the crucial role of ``covering'' low-reward responses, define a weight
\begin{equation}
\label{eq:appWeightDef}
w_i 
\;=\;
\exp\bigl(\,\overline{r} - r_i\bigr),
\end{equation}
where \(\overline{r}\) can be, for instance, the average reward \(\frac{1}{n}\sum_{i=1}^n r_i\). 
Then a natural \emph{coverage} cost is
\begin{equation}
\label{eq:coverageCostApp}
\mathrm{Cost}(\mathcal{S})
\;=\;
\sum_{i=1}^n
  w_i
  \,\min_{j\in \mathcal{S}}
    A_{i,j}.
\end{equation}
A small \(\min_{j\in \mathcal{S}} A_{i,j}\) means response \(i\) is ``close'' to at least one negative center \(j\). If \(r_i\) is low, then \(w_i\) is large, so we put higher penalty on leaving \(i\) uncovered. Minimizing \(\mathrm{Cost}(\mathcal{S})\) ensures that \emph{important} (low-reward) responses are forced near penalized centers, thus \emph{suppressing} them in the policy distribution.

\paragraph{MIP \(\mathcal{P}\) for Coverage Minimization.}

We can write a mixed-integer program:

\begin{align}
\label{eq:covMIPApp}
\nonumber
\textbf{Problem } \mathcal{P}:\;
&\min_{\,\substack{x_j \in \{0,1\}\\ z_{i,j}\in \{0,1\}\\ y_i \ge 0}} 
\sum_{i=1}^n 
  w_i\,y_i,
\\
&\text{subject to}
\begin{cases}
\displaystyle
\sum_{j=1}^n x_j = k, 
\\[0.2em]
z_{i,j}\le x_j,\quad 
\sum_{j=1}^n z_{i,j} = 1,\quad \forall\,i,
\\[0.2em]
y_i\le A_{i,j} + M\,(1 - z_{i,j}),
\\[0.2em]
y_i\ge A_{i,j} - M\,(1 - z_{i,j}),\quad \forall\,i,j,
\end{cases}
\end{align}
where \(M = \max_{i,j} A_{i,j}\). Intuitively, each \(x_j\) indicates if \(j\) is chosen as a negative; each \(z_{i,j}\) indicates whether \(i\) is ``assigned'' to \(j\). At optimality, \(y_i = \min_{j\in \mathcal{S}} A_{i,j}\), so the objective 
\(\sum_i w_i\,y_i\) is precisely \(\mathrm{Cost}(\mathcal{S})\). Hence solving \(\mathcal{P}\) yields \(\mathcal{S}^*\) that \emph{minimizes} coverage cost~\eqref{eq:coverageCostApp}.

\subsection{Key Lemma: Equivalence of Coverage Minimization and Lipschitz Suppression}

\begin{lemma}[Coverage $\Leftrightarrow$ Suppression]
\label{lem:appCoverageLemma}
Assume \ref{asmp:Lipschitz} (the \(L\)-Lipschitz constraint, \eqref{eq:LipschitzConstraintApp}) and let \(i_{\mathrm{top}}\) be a highest-reward index. Suppose \(\mathcal{S}\subseteq\{1,\dots,n\}\setminus\{i_{\mathrm{top}}\}\) is a subset of size~\(k\). Then:
\begin{enumerate}[label=(\roman*), leftmargin=1.25em, itemsep=0.5em]
    \item Choosing \(\mathcal{S}\) that \emph{minimizes} \(\mathrm{Cost}(\mathcal{S})\) yields the strongest suppression of low-reward responses and thus the best possible \emph{feasible} expected reward under the Lipschitz constraint.
    \item Conversely, any set \(\mathcal{S}\) achieving the \emph{highest} feasible expected reward necessarily \emph{minimizes} \(\mathrm{Cost}(\mathcal{S})\).
\end{enumerate}
\end{lemma}

\begin{proof}
\textbf{(i) Minimizing \(\mathrm{Cost}(\mathcal{S})\) improves expected reward.}\\
Once we pick \(\mathcal{S}\), we set \(p_j=0\) for all \(j\in \mathcal{S}\). By \ref{asmp:Lipschitz}, any \(y_i\) is then forced to satisfy \(p_i \le L\,A_{i,j}\) for all \(j\in \mathcal{S}\). Hence
\[
p_i 
\;\le\; 
L \,\min_{j\in \mathcal{S}} A_{i,j}.
\]
If \(\min_{j\in \mathcal{S}} A_{i,j}\) is large, then \(p_i\) could be large; if it is small (particularly for low-reward \(r_i\)), we effectively suppress \(p_i\). By weighting each \(i\) with \(w_i = e^{\overline{r}-r_i}\), we see that leaving low-reward \(y_i\) \emph{far} from all negatives raises the risk of high \(p_i\). Minimizing \(\sum_i w_i\,\min_{j\in \mathcal{S}} A_{i,j}\) ensures that any \(i\) with large \(w_i\) (i.e.\ small \(r_i\)) has a small distance to at least one chosen center, thus bounding its probability more tightly. 

Meanwhile, the best candidate \(i_{\mathrm{top}} \in \{1,\dots,n\}\) remains unconstrained, so the policy can always place mass \(\approx 1\) on \(i_{\mathrm{top}}.\) Consequently, a set \(\mathcal{S}\) that better ``covers'' low-reward points must yield a higher feasible expected reward \(\sum_i r_i p_i\). 

\textbf{(ii) Necessity of Minimizing \(\mathrm{Cost}(\mathcal{S})\).}\\
Conversely, if there were a set \(\mathcal{S}\) that \emph{did not} minimize \(\mathrm{Cost}(\mathcal{S})\) but still provided higher feasible expected reward, that would imply we found a distribution \(\{p_i\}\) violating the Lipschitz bound on some low-reward region. Formally, \(\mathcal{S}\) that yields strictly smaller coverage cost would impose stricter probability suppression on harmful responses. By part~(i), that coverage-lowering set should then yield an even higher feasible reward, a contradiction.
\end{proof}

\subsection{Main Theorem: Optimality of \(\mathcal{P}\) for Lipschitz Alignment}

\begin{theorem}[Optimal Negative Set via \(\mathcal{P}\)]
\label{thm:appOptNegatives}
Let \(\mathcal{S}^*\) be the solution to the MIP \(\mathcal{P}\) in \eqref{eq:covMIPApp}, i.e.\ it \emph{minimizes} \(\mathrm{Cost}(\mathcal{S})\). Then \(\mathcal{S}^*\) also \emph{maximizes} the objective \eqref{eq:lip_mainObjApp}. Consequently, picking \(\mathcal{S}^*\) and allowing free probability on \(i_{\mathrm{top}} \approx \arg\max_i\, r_i\) yields the \emph{optimal} Lipschitz-compliant policy.
\end{theorem}

\begin{proof}
By construction, solving \(\mathcal{P}\) returns \(\mathcal{S}^*\) with
\(\displaystyle
\mathrm{Cost}(\mathcal{S}^*)
\;=\;
\min_{|\mathcal{S}|=k}\,\mathrm{Cost}(\mathcal{S}).
\)
Lemma~\ref{lem:appCoverageLemma} then states that such an \(\mathcal{S}^*\) simultaneously \emph{maximizes} the best possible feasible expected reward. Hence \(\mathcal{S}^*\) is precisely the negative set that achieves the maximum of \eqref{eq:lip_mainObjApp}.
\end{proof}

\paragraph{Interpretation.} 
Under a mild Lipschitz assumption in embedding space, penalizing (assigning zero probability to) a small set \(\mathcal{S}\) \emph{and} forcing all items near \(\mathcal{S}\) to have small probability is equivalent to a \emph{coverage} problem. Solving (or approximating) \(\mathcal{P}\) selects negatives that push down low-reward modes as effectively as possible.

\vspace{-0.05in}
\subsection{Discussion and Practical Implementation}

\vspace{-0.05in}
\textsc{Opt-Select} thus emerges from optimizing coverage: 
\begin{enumerate}[leftmargin=1.25em, itemsep=0.5em]
    \item \textbf{Solve or approximate} the MIP \(\mathcal{P}\) to find the best subset \(\mathcal{S}\subseteq\{1,\dots,n\}\setminus\{i_{\mathrm{top}}\}\).
    \item \textbf{Force} \(p_j=0\) for each \(j\in \mathcal{S}\); \textbf{retain} \(i_{\mathrm{top}}\) with full probability (\(p_{i_{\mathrm{top}}}\approx 1\)), subject to normalizing the distribution. 
\end{enumerate}
In practice, local search or approximate clustering-based approaches (e.g.\ Weighted \(k\)-Medoids) can find good solutions without exhaustively solving \(\mathcal{P}\). The method ensures that near any chosen negative \(j\), all semantically similar responses \(i\) have bounded probability \(p_i \le L\,A_{i,j}\). Consequently, \textsc{Opt-Select} \emph{simultaneously} covers and suppresses undesired modes while preserving at least one high-reward response unpenalized.

\vspace{-0.05in}
\paragraph{Additional Remarks.}

\vspace{-0.15in}
\begin{itemize}[leftmargin=1.25em, itemsep=0.5em]
    \item The single-positive assumption reflects a practical design where one high-reward response is explicitly promoted. This can be extended to multiple positives, e.g.\ top \(m^+\) responses each unconstrained.
    \item For large \(n\), the exact MIP solution may be expensive; local search (see Appendix~\ref{sec:local_search_kmedoids}) still achieves a constant-factor approximation.
    \item The embedding-based Lipschitz constant \(L\) is rarely known exactly; however, the coverage perspective remains valid for “sufficiently smooth” reward behaviors in the embedding space.
\end{itemize}

Overall, these results solidify \textsc{Opt-Select} as a principled framework for negative selection under Lipschitz-based alignment objectives.
