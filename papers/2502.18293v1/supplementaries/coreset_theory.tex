
% \section{CoreSets for Representative Selection}
% \label{sec:coreset_subsection}

\section{Constant-Factor Approximation for Subset Selection Under Bounded Intra-Cluster Distance}
\label{sec:constant_factor_subset_selection}

\noindent
The term \emph{coreset} originates in computational geometry and machine learning, referring to a subset of data that \emph{approximates} the entire dataset with respect to a particular objective or loss function \citep{bachem2017practical,feldman2020turning}. More precisely, a coreset $\mathcal{C}$ for a larger set $\mathcal{X}$ is often defined such that, for any model or solution $w$ in a hypothesis class, the loss over $\mathcal{C}$ is within a small factor of the loss over $\mathcal{X}$. 

In the context of \textsc{AMPO-Coreset}, the $k$-means clustering subroutine identifies \emph{representative} embedding-space regions, and by choosing a single worst-rated example from each region, we mimic a coreset-based selection principle: our selected negatives approximate the \emph{distributional diversity} of the entire batch of responses. In essence, we seek a small but well-covered negative set that ensures the model receives penalizing signals for all major modes of undesired behavior. 

Empirically, such coverage-driven strategies can outperform purely score-based selection (Section \ref{sec:ampo_bottomk}) when the reward function is noisy or the model exhibits rare but severe failure modes. By assigning at least one negative from each cluster, \textsc{AMPO-Coreset} mitigates the risk of ignoring minority clusters, which may be infrequent yet highly problematic for alignment. As we show in subsequent experiments, combining \emph{coreset-like coverage} with \emph{reward-based filtering} yields robust policy updates that curb a wide range of undesirable outputs.



We give a simplified theorem showing how a local-search algorithm can achieve a fixed (constant) approximation factor for selecting \(k\) ``negative'' responses. Our statement and proof are adapted from the classical \emph{Weighted \(k\)-Medoids} analysis, but use simpler notation and explicit assumptions about bounded intra-cluster distance.

% \subsection{Problem Statement and Assumptions}

% \noindent
% \textbf{Weighted k-medoids setup Setup.}  
% We have \(n\) items, indexed by \(i=1,\dots,n\). Each item \(i\) has a \emph{weight} \(w_i \ge 0\) and lies in a metric space with distance \(d(i,j)\). We want to choose a subset \(\mathcal{S} \subseteq \{1,\dots,n\}\) of size \(k\) (i.e., \(|\mathcal{S}|=k\)) that minimizes
% \[
%   \mathrm{Cost}(\mathcal{S})
%   \;=\;
%   \sum_{i=1}^n
%     w_i \,\min_{\,j \in \mathcal{S}} d(i,j).
% \]
% This is precisely the \emph{Weighted \(k\)-Medoids} objective. The points \(j\in \mathcal{S}\) can be viewed as ``negatives'' or ``centers,'' penalizing nearby items by forcing them to remain close (hence incurring a smaller distance cost).

% We aim to present a theorem here with strong assumptions, that may not hold truly in practice, but would be indicative of a performance guarantee, should such assumptions hold.

\noindent
\subsection{Additional Assumptions:}
\textbf{Assumption 1: Bounded number of clusters k.}  
We assume that the data partitions into natural clusters such that the number of such clusters is equal to the number of examples we draw from the negatives. It is of course likely that at sufficiently high temperature, an LLM may deviate from such assumptions, but given sufficiently low sampling temperature, the answers, for any given query, may concentrate to a few attractors.

\textbf{Assumption 2: Bounded Intra-Cluster Distance.}  
We assume that the data can be partitioned into natural clusters of bounded diameter \(d_{\max}\). This assumption helps us simplify our bounds, towards rigorous guarantees, and we wish to state that such an assumption may be too strict to hold in practice, especially in light of Assumption 1.




Given these assumptions, We present a distribution-dependent coreset guarantee for selecting a small ``negative'' subset of responses for a given query, thus enabling the policy to concentrate probability on the highest-rated responses. Unlike universal coreset theory, we only require that this negative subset works well for typical distributions of responses, rather than for every conceivable set of responses.

\subsection{Setup: Queries, Responses, and Ratings}

\noindent
\textbf{Queries and Candidate Responses.}  
We focus on a single \emph{query} \(x\), which admits a finite set of \(m\) candidate responses 
\[
  \{\,y_1,\dots,y_m\}.
\]
Each response \(y_i\) has a scalar rating \(r_i \in [0,1]\). For notational convenience, we assume \(r_i\) is normalized to \([0,1]\). A larger \(r_i\) indicates a better (or more desirable) response.

\vspace{0.5em}
\noindent
\textbf{Negative Ratings via Exponential Weights.}  
Let 
\begin{align}
\overline{r} 
\;=\; 
\frac{1}{m}\sum_{i=1}^m r_i
\quad\text{(the mean rating)}, 
\quad
w_i 
\;=\; 
\exp\bigl(\overline{r}-r_i\bigr).
\label{eq:neg_weight}
\end{align}
Then \(w_i\) is larger when \(r_i\) is smaller. One may also employ alternative references (\(\max r_i\) instead of \(\overline{r}\)), or re-scaling to maintain bounded ranges.

\subsection{Policy Model and Subset Selection}

\noindent
\textbf{Policy Distribution Over Responses.}  
A policy \(P_\theta(y \mid x)\) assigns a probability \(p_i \ge 0\) to each response \(y_i\), satisfying 
\(\sum_{i=1}^m p_i = 1\). The \emph{expected rating} is 
\[
  \mathrm{ER}(p_1,\dots,p_m)
  \;=\;
  \sum_{i=1}^m p_i\,r_i.
\]

\vspace{0.5em}
\noindent
\textbf{Negative Subset and Probability Suppression.}  
We aim to choose a small subset \(\mathcal{S}\subseteq\{\,1,\dots,m\}\) of size \(k\), each member of which is assigned probability zero: 
\[
   p_j = 0,\quad\forall j\in \mathcal{S}.
\]
In addition, we impose a \textit{Lipschitz-like} rule that if \(p_j=0\) for \(j\in\mathcal{S}\), then any response \(y_i\) ``close'' to \(y_j\) in some embedding space must also have probability bounded by 
\[
  p_i 
  \;\le\;
  L\,\|\mathbf{e}_i - \mathbf{e}_j\|,
\]
where \(\mathbf{e}_i\) is an embedding of \(y_i\). If \(y_j\) is \emph{negatively rated}, then forcing \(p_j=0\) also forces small probability on responses near \(y_j\). This ensures undesired modes get suppressed.

\vspace{0.5em}
\noindent
\textbf{Concentrating Probability on Top Responses.}  
We allow the policy to place nearly all probability on a small handful of high-rated responses, so that the expected rating \(\sum_{i=1}^m p_i r_i\) is maximized. Indeed, the policy will try to push mass towards the highest \(r_i\) while setting \(p_j=0\) on low-rated responses in \(\mathcal{S}\).

\noindent
\textbf{Sampling Response-Sets or ``Solutions.''}  
We suppose that the set \(\{y_1,\dots,y_m\}\) with ratings \(\{r_i\}\) arises from some distributional process (for instance, \(\mathcal{D}\) might represent typical ways the system could generate or rank responses). Denote a random draw by 
\[
  \bigl(\{y_1,\dots,y_m\},\,\{r_i\}\bigr)
  \;\sim\; 
  \mathcal{D}.
\]
We only require that our negative subset \(\mathcal{S}\) yield a near-optimal Lipschitz-compliant policy \emph{for a typical realization from \(\mathcal{D}\)}, rather than for every possible realization.

\vspace{0.5em}
\noindent
\textbf{Clustering in Embedding Space.}  
Let \(\mathbf{e}_i\in\mathbb{R}^d\) be an embedding for each response \(y_i\). Suppose we partition \(\{1,\dots,m\}\) into \(k\) clusters \(C_1,\dots,C_k\) (each of bounded diameter at most \(d\)), and within each cluster \(C_j\), pick exactly one ``negative'' index \(i_j^- \in C_j\). This yields 
\[
   \mathcal{S} 
   \;=\; 
   \{\, i_1^-, \dots, i_k^-\}.
\]
We then penalize each \(y_{i_j^-}\) by setting \(p_{\,i_j^-}=0\). Consequently, for any \(y_i \in C_j\), the Lipschitz suppression condition forces \(p_i \le L\,d\).  






\subsection{A Distribution-Dependent Coreset Guarantee}

We now state a simplified theorem that, under certain conditions on the distribution \(\mathcal{D}\), ensures that for most draws of queries and responses, the chosen subset \(\mathcal{S}\) yields a policy whose expected rating is within \((1\pm \varepsilon)\) of the optimal Lipschitz-compliant policy of size \(k\).

\begin{theorem}[Distribution-Dependent Negative Subset]
\label{thm:distribution_coreset_responses}
Let \(\mathcal{D}\) be a distribution that generates query-response sets \(\{y_1,\dots,y_m\}\), each with ratings \(\{r_i\}\subset [0,1]\). Assume we cluster the \(m\) responses into \(k\) groups \(C_1,\dots,C_k\) of diameter at most \(d\) in the embedding space, and choose exactly one ``negative'' index \(i_j^-\in C_j\). Let \(\mathcal{S}=\{\,i_1^-,\dots,i_k^-\}\). Suppose that:
\[
   \max_{i\in C_j}\,
   \|\mathbf{e}_i-\mathbf{e}_{\,i_j^-}\|
   \;\le\;d,
   \quad
   \forall\,j=1,\dots,k.
\]
Assume a Lipschitz constant \(L\), so that penalizing \(y_{i_j^-}\) (i.e.\ \(p_{\,i_j^-}=0\)) enforces \(p_i \le L\,d\) for all \(i\in C_j\). Then, under a sufficiently large random sample of queries/responses (or equivalently, a large i.i.d.\ sample from \(\mathcal{D}\) to refine the clustering), with high probability over that sample, for at least a \(\bigl(1-\delta\bigr)\) fraction of newly drawn query-response sets from \(\mathcal{D}\), the set \(\mathcal{S}\) induces a Lipschitz-compliant policy whose expected rating is within a factor \((1\pm \varepsilon)\) of the best possible among all $k$-penalized subsets.
\end{theorem}

\begin{proof}[Proof Sketch]
We give a high-level argument:

\textbf{1. Large Sample Captures Typical Configurations.} By drawing many instances of responses $\{y_i\}$, $\{r_i\}$ from $\mathcal{D}$, we can cluster them in such a way that \emph{any new} draw from $\mathcal{D}$ is, with probability at least $1-\delta$, either (a) close to one of our sampled configurations or (b) has measure less than $\delta$.

\textbf{2. Bounded-Diameter Clusters.} Suppose each cluster $C_j$ has diameter at most $d$, and we pick $i_j^- \in C_j$ as the ``negative.'' This implies every response $y_i$ in that cluster is at distance $\le d$ from $y_{i_j^-}$.

\textbf{3. Lipschitz Suppression.} If $p_{\,i_j^-}=0$, then $p_i \le L\,\|\mathbf{e}_i - \mathbf{e}_{\,i_j^-}\|\le L\,d$ for all $i \in C_j$. This ensures that the entire cluster $C_j$ cannot accumulate large probability mass on low-rated responses. Consequently, we push the policy distribution to concentrate on higher-rated responses (e.g.\ those \emph{not} near a penalized center).

\textbf{4. Near-Optimal Expected Rating.} For any typical new draw of $\{y_i\}$, $\{r_i\}$, a $k$-penalized Lipschitz policy can be approximated by using the same $k$ negatives $\mathcal{S}$. Because we ensure that the new draw is close to one of our sampled draws, the coverage or cluster assignment for the new $\{y_i\}$ is accurate enough that the resulting feasible policy is within a multiplicative $(1\pm \varepsilon)$ factor of the best possible $k$-subset. This completes the distribution-dependent argument.

\end{proof}

