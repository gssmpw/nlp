\vspace{-0.15in}
\section{Active Subset Selection Strategies}
\label{sec:subset_selection_strategies}

\vspace{-0.1in}
In this section, we present two straightforward yet effective strategies for actively selecting a small set of \emph{negative} responses in the \textsc{AMPO} framework. First, we describe a simple strategy, \emph{\textbf{AMPO-BottomK}}, that directly picks the lowest-rated responses. Second, we propose \emph{\textbf{AMPO-Coreset}}, a clustering-based method that selects exactly one negative from each cluster in the embedding space, thereby achieving broad coverage of semantically distinct regions. In Section \ref{sec:constant_factor_subset_selection}, we connect this clustering-based approach to the broader literature on \emph{coreset construction}, which deals with selecting representative subsets of data.

\vspace{-0.1in}
\subsection{AMPO-BottomK}
\label{sec:ampo_bottomk}

\vspace{-0.05in}
\noindent
\emph{AMPO-BottomK} is the most direct approach that we use for comparison: given $N$ sampled responses and their scalar ratings $\{r_i\}_{i=1}^N$, we simply pick the $k$ lowest-rated responses as negatives. This can be expressed as:

\vspace{-0.25in}
\begin{align}
\label{eq:bottomk_negatives}
S^- \;=\; \mathrm{argtopk}_{i}(-\,r_i,\,k),
\end{align}

\vspace{-0.1in}
which identifies the $k$ indices with smallest $r_i$. Although conceptually simple, this method can be quite effective when the reward function reliably indicates “bad” behavior. 
Furthermore to break-ties, we use minimal cosine similarity with the currently selected set.


% \vspace{0.5em}

% \begin{algorithm}[tb]
% \caption{$\ampobk$($\{r_i\}_{i=1}^N$, $k$)}
% \label{alg:bottom_k_negatives}
% \begin{algorithmic}[1]
%     \STATE {\bfseries Input:} 
%     \STATE (1) A set of $N$ responses with associated scalar ratings $\{r_i\}_{i=1}^N$
%     \STATE (2) Desired number of negatives $k$
%     \STATE 
%     \STATE {\bfseries Step 1:} Rank by rating
%     \STATE Sort indices $\{1,\dots,N\}$ so that $r_{(1)} \,\le\, r_{(2)} \,\le\, \dots \,\le\, r_{(N)}$,
%     \STATE where $r_{(1)}$ is the lowest rating
%     \STATE
%     \STATE {\bfseries Step 2:} Select bottom $k$
%     \STATE $S^- \;\leftarrow\; \{ (1),\, (2),\,\dots,\,(k)\}$
%     \STATE
%     \RETURN the $k$ indices in $S^-$ as the set of negatives
% \end{algorithmic}
% \end{algorithm}

\vspace{-0.1in}
\subsection{AMPO-Coreset (Clustering-Based Selection)}
\label{sec:ampo_coreset}

\vspace{-0.05in}
\noindent
$\ampobk$ may overlook problematic modes that are slightly better than the bottom-k, but fairly important to learn on. A diversity-driven approach, which we refer to as $\ampocs$, explicitly seeks coverage in the embedding space by partitioning the $N$ candidate responses into $k$ clusters and then selecting the lowest-rated response within each cluster. Formally:

\vspace{-0.15in}
\[
\label{eq:clustering_negatives}
i^-_j 
\;=\;
\arg\min_{\,i \,\in\,C_j}\; r_i, 
\,
j = 1,\dots,k, 
\,
S^- \;=\;\bigl\{\,i^-_1,\dots,i^-_k\bigr\}
\]

\vspace{-0.15in}
where $C_j$ is the set of responses assigned to cluster $j$ by a $k$-means algorithm (\citealt{har2004coresets,cohen2022improved}; see also Section \ref{sec:constant_factor_subset_selection}). The pseudo-code is provided in Algorithm \ref{alg:cluster_negatives}.


\begin{algorithm}[tb]
\caption{\textcolor{titlecolor}{$\ampocs$ via k-means}}
\label{alg:cluster_negatives}
\begin{algorithmic}[1]
    \STATE \textcolor{inputcolor}{\textbf{Input:}}
    \STATE \textcolor{inputcolor}{(1) $N$ responses, each with embedding $\mathbf{e}_i \in \mathbb{R}^d$ and rating $r_i$}
    \STATE \textcolor{inputcolor}{(2) Desired number of negatives $k$}
    \STATE
    \STATE \textcolor{stepcolor}{\textbf{Step 1:} \textit{Run $k$-means on embeddings}}
    \STATE \textcolor{mathcolor}{Initialize $\{\mathbf{c}_1,\dots,\mathbf{c}_k\} \subset \mathbb{R}^d$ (e.g., via $k$-means++)}
    \REPEAT
        \STATE \textcolor{mathcolor}{$\pi(i) = \arg\min_{1 \le j \le k} \|\mathbf{e}_i - \mathbf{c}_j\|^2$, \quad $i = 1,\dots,N$}
        \STATE \textcolor{mathcolor}{$\mathbf{c}_j = \frac{\sum_{i:\pi(i)=j}\mathbf{e}_i}{\sum_{i:\pi(i)=j}1}$, \quad $j = 1,\dots,k$} \label{eq:vanilla_kmeans}
    \UNTIL{convergence}
    \STATE
    \STATE \textcolor{stepcolor}{\textbf{Step 2:} \textit{In each cluster, pick the bottom-rated response}}
    \STATE \textcolor{mathcolor}{For each $j \in \{1,\dots,k\}$, define $C_j = \{\, i \mid \pi(i) = j \}$}
    \STATE \textcolor{mathcolor}{Then $i_j^- = \arg\min_{i\in C_j} r_i$, \quad $j = 1,\dots,k$}
    \STATE
    \STATE \textcolor{stepcolor}{\textbf{Step 3:} Return negatives}
    \STATE \textcolor{mathcolor}{$S^- = \{\, i_1^-,\, i_2^- ,\dots, i_k^- \}$}
    \RETURN \textcolor{outputcolor}{$S^-$ as the set of $k$ negatives}
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}[tb]
% \caption{$\ampocs$($\{\mathbf{e}_i,r_i\}_{i=1}^N$, $k$)}
% \label{alg:cluster_negatives}
% \begin{algorithmic}[1]
%     \STATE {\bfseries Input:} 
%     \STATE (1) $N$ responses, each with embedding $\mathbf{e}_i \in \mathbb{R}^d$ and rating $r_i$
%     \STATE (2) Desired number of negatives $k$
%     \STATE
%     \STATE {\bfseries Step 1:} Run $k$-means on embeddings
%     \STATE Initialize $\{\mathbf{c}_1,\dots,\mathbf{c}_k\} \subset \mathbb{R}^d$ (e.g., via $k$-means++)
%     \REPEAT
%         \STATE $\pi(i) = \arg\min_{1 \le j \le k} \|\mathbf{e}_i - \mathbf{c}_j\|^2$, \quad $i = 1,\dots,N$
%         \STATE $\mathbf{c}_j = \frac{\sum_{i:\pi(i)=j}\mathbf{e}_i}{\sum_{i:\pi(i)=j}1}$, \quad $j = 1,\dots,k$ \label{eq:vanilla_kmeans}
%     \UNTIL{convergence}
%     \STATE
%     \STATE {\bfseries Step 2:} In each cluster, pick the bottom-rated response
%     \STATE For each $j \in \{1,\dots,k\}$, define $C_j = \{\, i \mid \pi(i) = j \}$
%     \STATE Then $i_j^- = \arg\min_{i\in C_j} r_i$, \quad $j = 1,\dots,k$
%     \STATE
%     \STATE {\bfseries Step 3:} Return negatives
%     \STATE $S^- = \{\, i_1^-,\, i_2^- ,\dots, i_k^- \}$
%     \RETURN $S^-$ as the set of $k$ negatives
% \end{algorithmic}
% \end{algorithm}



This approach enforces that each cluster---a potential ``mode'' in the response space---contributes at least one negative example. Hence, \textsc{AMPO-Coreset} can be interpreted as selecting \emph{representative} negatives from diverse semantic regions, ensuring that the model is penalized for a wide variety of undesired responses.

