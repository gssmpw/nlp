

\vspace{-0.1in}
\section{Discussion \& Future Work}
\label{sec:discussion_future_work}

\vspace{-0.1in}
\paragraph{Iteration via Active Synthetic Data Generation.}
% Our active sampling approach naturally enables synthetic data generation. By combining reward and embedding signals, the model surfaces its own uncertain or error-prone outputs during on-policy training. This process efficiently covers the answer space while generating both positive (high-probability, high-reward) and negative (high-probability, low-reward) examples. The resulting synthetic dataset is both representative of model behavior and instructive for iteratively improving both the policy and reward model.
When we combine reward signals and output-embedding signals in active sampling, we naturally create a pathway to \emph{synthetic data} creation. Through multi-preference optimization on diverse queries, the model continually improves itself by receiving feedback on different modes of failure (and success). Crucially, because this process is \emph{on-policy}, the model directly surfaces new candidate answers for which it is most uncertain or prone to errors. The selection for coverage ensures that we efficiently address a large portion of the measurable answer space, rather than merely focusing on obvious or extreme failures.

% Beyond updating the policy itself, we can view the high-probability, high-reward subset of answers as ``positive synthetic examples'' and the high-probability, low-reward subset as ``negative synthetic examples,'' thereby constructing a labeled dataset that is simultaneously \emph{representative} (capturing many modes of the modelâ€™s actual behavior) and \emph{instructive} (contrasting good vs.\ bad). 
Over multiple epochs, such a growing corpus of synthetic data can be used to refine or re-check the reward model, establishing a feedback loop between policy improvement and reward-model improvement. We believe this to be an important direction of future work.

