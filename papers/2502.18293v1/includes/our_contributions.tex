\subsection{Our Contributions}
\begin{itemize}[leftmargin=1em]
    \item \textbf{Algorithmic Novelty:} We propose \emph{Active Multi-Preference Optimization} (\ampo), an on-policy framework that blends group-based preference alignment with active subset selection without exhaustively training on all generated responses. This opens out avenues for research on how to select for synthetic data, as we outline in Sections \ref{sec:subset_selection_strategies} and \ref{sec:discussion_future_work}.
    \item \textbf{Theoretical Insights:} Under mild Lipschitz assumptions, we show that coverage-based negative selection can systematically suppress low-reward modes and maximizes expected reward. This analysis (in Sections \ref{sec:opt_select} and \ref{sec:theory_main}) connects our method to the weighted $k$-medoids problem, yielding performance guarantees for alignment.
    \item \textbf{State-of-the-Art Results:} Empirically, \ampo\ sets a new benchmark on \textit{AlpacaEval} with Llama 8B, surpassing strong baselines like $\simpo$ by focusing on a small but strategically chosen set of responses each iteration (see Section \ref{sec:experimental setup}).
    \item \textbf{Dataset Releases:} We publicly release our \href{https://huggingface.co/datasets/Multi-preference-Optimization/AMPO-Coreset-selection}{\texttt{AMPO-Coreset-Selection}} 
    and \href{https://huggingface.co/datasets/Multi-preference-Optimization/AMPO-OPT-Selection}{\texttt{AMPO-Opt-Selection}} datasets on Hugging Face. These contain curated response subsets for each prompt, facilitating research on multi-preference alignment.
\end{itemize}
