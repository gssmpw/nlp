

\vspace{-0.1in}
\section{Notations and Preliminaries}
\label{sec:notations_preliminaries}

\vspace{-0.1in}
We focus on aligning a \emph{policy model} to human preferences in a single-round (one-shot) scenario. Our goal is to generate multiple candidate responses for each prompt, then actively select a small, high-impact subset for alignment via a group-contrastive objective.

\vspace{-0.1in}
\paragraph{Queries and Policy.}
Let $\mathcal{D} = \{x_1, x_2, \ldots, x_M\}$ be a dataset of $M$ \emph{queries} (or \emph{prompts}), each from a larger space $\mathcal{X}$. We have a policy model $P_\theta(y \mid x)$, parameterized by $\theta$, which produces a distribution over possible responses $y \in \mathcal{Y}$. To generate diverse answers, we sample from $P_\theta(y \mid x)$ at some fixed \emph{temperature} (e.g., $0.8$). Formally, for each $x_i$, we draw up to $N$ responses,

\vspace{-0.1in}
\begin{equation}
   \{y_{i,1}, y_{i,2}, \dots, y_{i,N}\}, 
\end{equation}

\vspace{-0.1in}
from $P_\theta(y \mid x_i)$. Such an \textbf{on-policy} sampling, ensures, we are able to provide preference feedback on queries that are chosen by the model.

\vspace{-0.1in}
For simplicity of notation, we shall presently consider a single query (prompt) \(x\) and sampled responses \(\{y_1,\dots,y_N\}\) from \(P_\theta(\cdot \mid x)\), from the autoregressive language model.

\vspace{-0.1in}
Each response \(y_i\) is assigned a scalar reward

\vspace{-0.1in}
\begin{equation}
r_i \;=\; \mathcal{R}(x,\,y_i) \;\in\; [0,1],
\end{equation}

\vspace{-0.1in}
where \(\mathcal{R}\) is a fixed reward function or model (not optimized during policy training). We also embed each response via \(\mathbf{e}_i = \mathcal{E}(y_i)\in \mathbb{R}^d\), where \(\mathcal{E}\) might be any sentence or document encoder capturing semantic or stylistic properties.

\vspace{-0.1in}
Although one could train on all \(N\) responses, doing so is often computationally expensive. We therefore \emph{select} a subset \(\mathcal{S}\subset \{1,\dots,N\}\) of size \(\lvert\mathcal{S}\rvert = K < N\) by maximizing some selection criterion (e.g.\ favoring high rewards, broad coverage in embedding space, or both). Formally,

\vspace{-0.15in}
\begin{equation}
\label{eq:subset_selection}
\mathcal{S}
\;=\;
\arg\max_{\substack{\mathcal{I}\subset\{1,\dots,N\} \\ \lvert\mathcal{I}\rvert = K}}
\,\mathcal{U}\Bigl(\{y_i\}_{i\in\mathcal{I}},\, \{r_i\}_{i\in\mathcal{I}},\, \{\mathbf{e}_i\}_{i\in\mathcal{I}}\Bigr),
\end{equation}

\vspace{-0.1in}
where \(\mathcal{U}\) is a \emph{utility function} tailored to emphasize extremes, diversity, or other alignment needs.

\vspace{-0.1in}
Next, we split \(\mathcal{S}\) into a \emph{positive} set \(\mathcal{S}^+\) and a \emph{negative} set \(\mathcal{S}^-\). For example, let 

\vspace{-0.05in}
\[
\overline{r} \;=\;
\frac{1}{K}\,\sum_{i\in \mathcal{S}}\,r_i
\]
\vspace{-0.05in}
be the average reward of the chosen subset, and define

\vspace{-0.05in}
\[
\mathcal{S}^+
\;=\;
\{\,i\in \mathcal{S}\,\mid\,r_i > \overline{r}\},
\quad
\mathcal{S}^- 
\;=\;
\{\,i\in \mathcal{S}\,\mid\,r_i \le \overline{r}\}.
\]

\vspace{-0.05in}
Hence, \(\mathcal{S} = \mathcal{S}^+\cup \mathcal{S}^-\) and \(\lvert \mathcal{S}^+\rvert + \lvert \mathcal{S}^-\rvert = K\).

We train \(\theta\) via a reference-free \emph{group-contrastive} objective known as \(\textsc{refa}\) \citep{gupta2024refa}. Concretely, define

\vspace{-0.15in}
\begin{equation}
L_{\text{swepo}}(\theta)
\;=\;
-\,\log\!\Biggl(\!
  \frac{
    \displaystyle
    \sum_{\,i \,\in\, \mathcal{S}^+}\;
    \exp\Bigl[
      s'_\theta\bigl(y_i \mid x\bigr)
    \Bigr]
  }{
    \displaystyle
    \sum_{\,i\,\in\,(\mathcal{S}^+\cup \mathcal{S}^-)}\;
    \exp\Bigl[
      s'_\theta\bigl(y_i \mid x\bigr)
    \Bigr]
  }
\Biggr),
\end{equation}

\vspace{-0.05in}
where

\vspace{-0.25in}
\[
s'_\theta\bigl(y_i \mid x\bigr)
  =
  \log P_\theta(y_i\mid x)
  +
  \alpha \,\bigl(r_i - \overline{r}\bigr).
\]

\vspace{-0.05in}
Here, \(P_{\mathrm{ref}}\) is a reference policy (e.g.\ an older snapshot of \(P_\theta\) or a baseline model), and \(\alpha\) is a hyperparameter scaling the reward difference. In words, \(\textsc{swepo}\) encourages the model to increase the log-probability of \(\mathcal{S}^+\) while decreasing that of \(\mathcal{S}^-\), all in a single contrastive term that accounts for multiple positives and negatives simultaneously.

Although presented for a single query \(x\), this procedure extends straightforwardly to any dataset \(\mathcal{D}\) by summing \(L_{\text{swepo}}\) across all queries. In subsequent sections, we discuss diverse strategies for selecting \(\mathcal{S}\) (and thus \(\mathcal{S}^+\) and \(\mathcal{S}^-\)), aiming to maximize training efficiency and alignment quality.



