\vspace{-0.1in}
\section{Theoretical Results: Key Results}
\label{sec:theory_main}

\vspace{-0.05in}
In this section, we present the core theoretical statements used throughout the paper. Full extended proofs appear in Appendices~\ref{sec:theory_opt_select_extended}--\ref{sec:constant_factor_subset_selection}. 

\vspace{-0.15in}
\subsection{Setup and Assumptions}

\vspace{-0.05in}
\paragraph{(A1) $L$-Lipschitz Constraint.}
When a response $y_j$ is penalized (probability $p_j = 0$), any other response $y_i$ within embedding distance $A_{i,j}$ must satisfy $p_i \le L \, A_{i,j}$. 

\vspace{-0.1in}
\paragraph{(A2) Single Positive Enforcement.}
We allow one highest-reward response $y_{i_{\mathrm{top}}}$ to be unconstrained, i.e.\ $p_{i_{\mathrm{top}}}$ is not pulled down by the negatives. 

\vspace{-0.1in}
\paragraph{(A3) Finite Support.}
We focus on a finite set of $n$ candidate responses $\{y_1,\dots,y_n\}$ and their scalar rewards $\{r_i\}$, each embedded in $\mathbb{R}^d$ with distance $A_{i,j} = \|\mathbf{e}_i-\mathbf{e}_j\|$. 

\vspace{-0.1in}
\subsection{Optimal Negatives via Coverage}

\begin{theorem}[Optimality of \textsc{Opt-Select}]
\label{thm:opt_select_main}
Under assumptions (A1)--(A3), let $\mathcal{S}^*$ be the set of $k$ “negative” responses that \emph{minimizes} the coverage cost

\vspace{-0.2in}
\begin{equation}
      \mathrm{Cost}(\mathcal{S})
  \;=\;
  \sum_{i=1}^n
    \exp(\,\overline{r}-r_i)
    \;\min_{j\in \mathcal{S}}
       A_{i,j},
\end{equation}

where $\overline{r}$ is a reference reward (e.g.\ average of $\{r_i\}$). Then $\mathcal{S}^*$ also \emph{maximizes} the expected reward among all Lipschitz-compliant policies of size $k$ (with a single positive). Consequently, selecting $\mathcal{S}^*$ and allowing $p_{i_{\mathrm{top}}} \approx 1$ is optimal.
\end{theorem}

\paragraph{Sketch of Proof.}
(See Appendix~\ref{sec:theory_opt_select_extended} for details.) We show a one-to-one correspondence between minimizing coverage cost $\sum_i w_i \min_{j\in \mathcal{S}} A_{i,j}$ and maximizing the feasible expected reward $\sum_i r_i p_i$ under the Lipschitz constraint. Low-reward responses with large $w_i$ must lie close to at least one negative $j\in \mathcal{S}$; otherwise, they are not sufficiently suppressed. A mixed-integer program encodes this cost explicitly, and solving it yields the unique $\mathcal{S}^*$ that maximizes reward.

\subsection{Local Search for Weighted $k$-Medoids}
\label{subsec:local_search_statement}

\paragraph{(A4) Weighted $k$-Medoids Setup.}
We have $n$ points $\{1,\dots,n\}$ in a metric space with distance $d(\cdot,\cdot)\ge0$, each with weight $w_i\ge0$. Our goal is to find a subset $\mathcal{S}$ of size $k$ minimizing 

\vspace{-0.2in}
\[
  \mathrm{Cost}(\mathcal{S})
  \;=\;
  \sum_{i=1}^n
    w_i
    \,\min_{j\in \mathcal{S}}
       d(i,j).
\]

\vspace{-0.1in}
\begin{theorem}[Local Search Approximation]
\label{thm:local_search_main}
Suppose we apply a $1$-swap local search algorithm to select $k$ medoids. Let $\widehat{\mathcal{S}}$ be the resulting local optimum and let $\mathcal{S}^*$ be the globally optimal subset. Then

\vspace{-0.1in}
\[
  \mathrm{Cost}\bigl(\widehat{\mathcal{S}}\bigr)
  \;\le\;
  5
  \,\times\,
  \mathrm{Cost}\bigl(\mathcal{S}^*\bigr).
\]
\end{theorem}
\vspace{-0.15in}
The running time is polynomial in $n$ and $k$. 
% We note that a tighter constant factor can be proved.

\vspace{-0.15in}
\paragraph{Sketch of Proof.}
(See Appendix~\ref{sec:local_search_kmedoids} for a complete proof.) Assume by contradiction that $\mathrm{Cost}(\widehat{\mathcal{S}}) > 5\, \mathrm{Cost}(\mathcal{S}^*)$. We then show there exists a profitable swap (removing some $j\in\widehat{\mathcal{S}}$ and adding $j^*\in\mathcal{S}^*$) that strictly decreases cost, contradicting the local optimality of $\widehat{\mathcal{S}}$.

\vspace{-0.05in}
\subsection{Coreset and Bounded Intra-Cluster Distance}
\label{subsec:coreset_bounded_statement}

\vspace{-0.05in}
\paragraph{(A5) Bounded-Diameter Clusters.}
We assume $k$ clusters of diameter at most $d_{\max}$ in $\ampocs$.

% \vspace{-0.1in}
\begin{theorem}[Distribution-Dependent Coreset Guarantee]
\label{thm:coreset_main}
Suppose we form $k$ clusters each of diameter $\le d_{\max}$ in an embedding space $\mathbb{R}^d$, and from each cluster we pick one response as a “negative.” Under a Lipschitz constant $L$, this subset induces $p_i \le L\,d_{\max}$ for all $i$ in each cluster. With sufficiently many samples (drawn from a distribution of prompts/responses), with probability at least $1-\delta$, for a $(1-\delta)$ fraction of new draws, the resulting Lipschitz-compliant policy is within a factor $(1\pm \varepsilon)$ of the optimal $k$-subset solution.
\end{theorem}

\vspace{-0.15in}
\paragraph{Sketch of Proof.}
(See Appendix~\ref{sec:constant_factor_subset_selection}.) By choosing one representative from each of $k$ bounded-diameter clusters, we ensure that all similar (i.e.\ same-cluster) responses are suppressed. A uniform convergence argument shows that, with enough sampled data, responses from the same distribution are close to one of the clusters, guaranteeing approximate coverage and thus near-optimal expected reward.



\begin{table*}[!tb]
\centering
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard} & \textbf{MT-Bench} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
 & \textbf{LC (\%)} & \textbf{WR (\%)} & \textbf{WR (\%)} & \textbf{GPT-4} \\ \midrule
Base & 28.4 & 28.4& 26.9 & 7.93 \\
Best-vs-worst (\simpo) & 47.6 & 44.7 & 34.6 & 7.51 \\
\ampo-Bottomk & 50.8 & 50.5 & 35.3 & \underline{8.11} \\
\ampo-Coreset & \textbf{52.4} & \textbf{52.1} & \textbf{39.4} & \textbf{8.12} \\
\ampo-Opt-Select & \underline{51.6} & \underline{51.2} & \underline{37.9} & 7.96 \\
\bottomrule
\end{tabular}
}
\caption{Comparison of various preference optimization baselines on AlpacaEval, Arena-Hard, and MT-Bench benchmarks for Llama-3-Instruct (8B). LC-WR represents length-controlled win rate, and WR represents raw win rate. Best results are in \textbf{bold}, second-best are \underline{underlined}. Our method ($\ampo$) achieves SOTA performance across all metrics, with different variants achieving either best or second-best results consistently.}
\label{tab:llama3-results}
\end{table*}

% \section{Theoretical Analysis of \textsc{Opt-Select}}
% \label{sec:theory_opt_select}

% We now show how \textsc{Opt-Select} emerges from the goal of \emph{maximizing expected reward} under a Lipschitz constraint on the policy. 
% For brevity, we assume the basic notation (e.g.\ rewards \(\{r_i\}\), embeddings \(\{\mathbf{e}_i\}\), distances \(A_{i,j} = \|\mathbf{e}_i - \mathbf{e}_j\|\), and subset size \(k\)) has already been defined in prior sections.

% \subsection{Maximizing Expected Reward Under a Lipschitz Constraint}

% Let \(i_{\mathrm{top}}\) be an index of a \emph{highest-reward} response. We partition the responses into two groups:
% \begin{itemize}[leftmargin=1.3em]
% \item \textbf{Positive:} \(i_{\mathrm{top}}\), whose probability \(p_{i_{\mathrm{top}}}\) is unconstrained.
% \item \textbf{Negatives:} a subset \(\mathcal{S}\subset\{1,\dots,n\}\setminus\{i_{\mathrm{top}}\}\) of size \(k\), each forced to have \(p_j=0\).
% \end{itemize}
% We require a policy to be \(L\)-Lipschitz in the sense that if \(p_j=0\) for \(j\in\mathcal{S}\), then every other response \(i\) must have
% \[
%   p_i \;\le\;
%   L\,
%   \min_{\,j\in\mathcal{S}}\!
%      A_{i,j}.
% \]
% Our objective is to maximize the \emph{expected reward} \(\sum_{i=1}^n r_i\,p_i\) subject to these constraints:
% \begin{align}
% \label{eq:lip_max_problem}
% &\max_{\substack{\mathcal{S}\,\subseteq \{1,\dots,n\} \\ |\mathcal{S}|=k}}
% \quad
% \max_{\substack{p_i\ge 0,\ \sum_i p_i=1}}
% \quad
% \sum_{i=1}^n 
%   r_i \, p_i\\
% &\text{subject to}
% \begin{cases}
% p_j = 0,\ \forall j\in\mathcal{S},\\
% p_i \le L\,\min_{j\in\mathcal{S}}\,A_{i,j},\ \forall\, i\notin \mathcal{S}\cup\{i_{\mathrm{top}}\}.
% \end{cases}
% \end{align}

% \subsection{Coverage Cost Formulation}

% To see why \emph{coverage} arises, let us define a weight for each response,
% \[
%    w_i
%    \;=\;
%    \exp\!\bigl(\,\overline{r}\;-\;r_i\bigr),
% \]
% where \(\overline{r}\) is a baseline (e.g.\ mean or max reward). Lower-reward \(y_i\) have larger \(w_i\).  
% Next, for a candidate negative set \(\mathcal{S}\),
% \[
%   \mathrm{Cost}(\mathcal{S})
%   \;=\;
%   \sum_{i=1}^n
%     w_i 
%     \,\min_{\,j \in \mathcal{S}}\;
%       A_{i,j}.
% \]
% Minimizing \(\mathrm{Cost}(\mathcal{S})\) ``covers'' low-reward points tightly with at least one negative center \(j\in\mathcal{S}\).  Because \emph{any} \(i\) near a negative \(j\) is forced to have \(p_i\le L\,A_{i,j}\), choosing \(\mathcal{S}\) that yields a small coverage cost effectively suppresses low-reward regions.

% \subsection{MIP Formulation for Selecting Negatives}

% We can find \(\mathcal{S}\) that minimizes \(\mathrm{Cost}(\mathcal{S})\) via a mixed-integer program (MIP).  
% Let \(x_j\in\{0,1\}\) indicate whether \(j\in\mathcal{S}\).  Then \(\sum_{j=1}^n x_j=k\).  Also let \(z_{i,j}\in\{0,1\}\) choose \emph{which} center covers \(i\).  Introducing slack variables \(y_i\ge 0\), we write:
% \begin{align*}
% \textbf{Problem } \mathcal{P}: \quad
% &\min_{\substack{x_j \in \{0,1\},\ z_{i,j}\in\{0,1\},\ y_i\ge 0}} 
%  \sum_{i=1}^n w_i \,y_i 
% \\
% \text{s.t.}\quad
% & \sum_{j=1}^n x_j = k, 
% z_{i,j}\le x_j,
% \sum_{j=1}^n z_{i,j} = 1,\quad \forall\,i,
% \\
% & y_i \le A_{i,j} + M\,(1 - z_{i,j}), \quad\\
% &y_i \ge A_{i,j} - M\,(1 - z_{i,j}),\quad\forall\,i,j,
% \end{align*}
% where \(M=\max_{i,j}A_{i,j}\).  At optimality, we get
% \(\displaystyle
% y_i
% =
% \min_{j\in\mathcal{S}}A_{i,j}
% \).
% Thus solving \(\mathcal{P}\) yields the set \(\mathcal{S}^*\) that minimizes coverage cost.


% \subsection{Key Assumptions}
% \label{subsec:assumptions}

% \begin{itemize}
% \item \textbf{(A1) $L$-Lipschitz Policy Constraint.} Suppose whenever a response \(y_j\) has its probability \(p_j=0\), any response \(y_i\) at distance \(A_{i,j}\) from \(y_j\) must satisfy
% \[
%    p_i \;\le\; L \; A_{i,j}.
% \]
% Intuitively, forcing \(p_j=0\) for some \(j\) ``pulls down'' the allowable probability of any nearby \(y_i\).

% \item \textbf{(A2) Single Positive Enforcement.} We allow full probability mass on exactly one \emph{best} response \(y_{i_{\mathrm{top}}}\), unconstrained by the negatives. This captures the idea that we explicitly reward at least one high-quality response.

% \item \textbf{(A3) Finite Support.} We focus on the finite set \(\{y_1,\dots,y_n\}\). The reasoning extends to continuous or larger spaces by discretizing or bounding neighborhoods.
% \end{itemize}


% \subsection{Lemma and Main Theorem}

% \begin{lemma}[Coverage $\Leftrightarrow$ Lipschitz Suppression]
% \label{lem:coverage_lipschitz}
% Under the Lipschitz constraint in \eqref{eq:lip_max_problem}, minimizing \(\mathrm{Cost}(\mathcal{S})\) enforces maximal suppression of low-reward responses.  In particular, among all subsets of size \(k\), the one that achieves 
% \(\min_{\mathcal{S}}\mathrm{Cost}(\mathcal{S})\)
% \emph{maximizes} the feasible expected reward in \eqref{eq:lip_max_problem}.
% \end{lemma}

% \begin{proof}
% \noindent
% \textbf{1. Feasible Probabilities.}
% Once \(\mathcal{S}\) is selected, for every \(j\in \mathcal{S}\) we set \(p_j=0\). By \textbf{(A1)}, any response \(y_i\) at distance \(A_{i,j}\) to \(y_j\) satisfies \(p_i\le L\,A_{i,j}\). Therefore,
% \[
%   p_i \;\le\; L\,\min_{j\in\mathcal{S}}A_{i,j}
%   \quad
%   \text{for all }
%   i\notin\{i_{\mathrm{top}}\}\cup\mathcal{S}.
% \]
% \noindent
% \textbf{2. Concentrating on a Single Positive.}
% Because we allow unconstrained probability on \(y_{i_{\mathrm{top}}}\), the \emph{best} distribution to maximize expected reward is to assign
% \[
%    p_{i_{\mathrm{top}}} 
%    \;=\; 
%    1 \;-\;\sum_{i\neq i_{\mathrm{top}}} p_i,
% \]
% subject to \(p_i \le L\,\min_{j\in\mathcal{S}}A_{i,j}\).  Since \(r_i\le 1\) for all \(i\) and \(r_{i_{\mathrm{top}}}\) is largest, we want to push as much mass as possible onto \(y_{i_{\mathrm{top}}}\).  

% \noindent
% \textbf{3. Weighted Coverage Interpretation.}
% To prevent low-reward responses \(y_i\) from ``using up'' large probability, \(\min_{j\in\mathcal{S}}A_{i,j}\) should be small for those \(i\) with \(\exp(\overline{r}-r_i)\) large (i.e.\ small \(r_i\)). Formally, it can be shown that the maximum of \(\mathrm{ER}(p)\) is inversely related to 
% \(\sum_{i=1}^n w_i\,\min_{j\in\mathcal{S}}A_{i,j}\). Minimizing this coverage cost (via \(\mathcal{P}\)) yields the best possible negative set \(\mathcal{S}\) for Lipschitz suppression.
% \end{proof}


% \begin{theorem}[Optimal Negatives via \textsc{Opt-Select}]
% \label{thm:optimality_of_opt_select}
% Let \(\mathcal{S}^*\) be the solution to \(\mathcal{P}\) that minimizes \(\mathrm{Cost}(\mathcal{S})\).  Then \(\mathcal{S}^*\) also maximizes the objective \eqref{eq:lip_max_problem}, i.e.\ it is the optimal negative subset for \emph{Lipschitz-compliant} policy alignment.
% \end{theorem}

% \begin{proof}
% \noindent
% \textbf{(1) Equivalence to Coverage Minimization.}
% By Lemma~\ref{lem:coverage_lipschitz}, the highest attainable expected reward under Lipschitz suppression is achieved by a negative set \(\mathcal{S}\) that covers low-reward responses as tightly as possible. The coverage cost
% \begin{align}
% \mathrm{Cost}(\mathcal{S})
% \;=\;
% \sum_{i=1}^n 
%   w_i\,
%   \min_{j\in \mathcal{S}}A_{i,j},
% \quad
% w_i = \exp(\overline{r}-r_i),
% \end{align}
% reflects how strongly we need to suppress each \(y_i\).  

% \noindent
% \textbf{(2) MIP $\mathcal{P}$ Yields Minimizing $\mathcal{S}^*$.}
% The Mixed-Integer Program \(\mathcal{P}\) exactly encodes \(\mathrm{Cost}(\mathcal{S})\). Hence solving it yields the global minimizer \(\mathcal{S}^*\).  

% \noindent
% \textbf{(3) Optimal Subset for Expected Reward.}
% Once \(\mathcal{S}^*\) is chosen, we assign \(p_{i_{\mathrm{top}}}\approx 1\) and \(p_i = 0\) for \(i\in \mathcal{S}^*\), plus negligible mass on others (bounded by \(L \min_{j\in \mathcal{S}^*} A_{i,j}\)). This distribution \(\{p_i\}\) is Lipschitz-compliant and maximizes 
% \(\sum_{i=1}^n r_i p_i\). Consequently, \(\mathcal{S}^*\) is indeed the negative subset that achieves the maximum of \eqref{eq:lip_max_problem}, completing the proof.
% \end{proof}


% Under a mild Lipschitz condition, forcing probability mass on a small ``negative'' subset and its neighbors is tantamount to \emph{covering} low-reward responses.  Solving \(\mathcal{P}\) (i.e.\ minimizing the coverage cost) directly yields the negative set that \emph{maximizes} expected reward.  Implementing this procedure is precisely the essence of \textsc{Opt-Select}.


