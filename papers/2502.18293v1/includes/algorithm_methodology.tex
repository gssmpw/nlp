\section{Algorithm and Methodology}
\label{sec:methodology}

We outline a one-vs-$k$ selection scheme in which a single \emph{best} response is promoted (positive), while an \emph{active} subroutine selects $k$ negatives from the remaining $N-1$ candidates. This setup highlights the interplay of three main objectives:

\vspace{-0.1in}
\begin{description}[leftmargin=1em, itemsep=0pt]
   \item[Probability:] High-probability responses under $P_\theta(y\mid x)$ can dominate even if suboptimal by reward.
   \item[Rewards:] Simply selecting extremes by reward misses problematic "mediocre" outputs.
   \item[Semantics:] Diverse but undesired responses in distant embedding regions must be penalized.
\end{description}

\vspace{-0.15in}
While positives reinforce a single high-reward candidate, active negative selection balances probability, reward and diversity to systematically suppress problematic regions of the response space.

% \begin{description}[leftmargin=1em, itemsep=1pt]
%     \item[Probability (Model Likelihood):] Responses that have high probability under $P_\theta(y\mid x)$ can dominate the model’s output distribution; even if such responses are not the absolute worst by reward, leaving them unpenalized can hamper alignment.
%     \item[Rewards (Quality):] Purely selecting the top- or bottom-ranked by reward is insufficient when the model generates a variety of “mediocre” or “niche” outputs that still require demotion.
%     \item[Semantics (Diversity/Coverage):] Some undesired responses may differ substantially from the mainstream modes (e.g., lying in a distant embedding region). Penalizing them is vital, else the model remains vulnerable to sporadic but harmful outputs.
% \end{description}

% Balancing these three factors (model likelihood, reward, and diversity) is the cornerstone of \emph{active} negative selection. While the \emph{positive} ensures strong reinforcement of one high-reward candidate, the \emph{negatives} promote a broader shaping of the policy to down-weight potentially problematic regions. 

% \medskip
\noindent
\textbf{Algorithm.}
Formally, let $\{y_1,\dots,y_N\}$ be the sampled responses for a single prompt $x$. Suppose we have:\\
1. A reward function $r_i = \mathcal{R}(x,y_i)\in [0,1]$.\\
2. An embedding $\mathbf{e}_i = \mathcal{E}(y_i)$.\\
3. A model probability estimate $\pi_i = P_\theta(y_i\mid x)$.
% \begin{enumerate}
% \item 
% \item .
% \item 
% \end{enumerate}

Selection algorithms may be \textit{rating-based} selection (to identify truly poor or excellent answers) with \textit{coverage-based} selection (to explore distinct regions in the embedding space), we expose the model to both common and outlier responses. This ensures that the \textsc{swepo} loss provides strong gradient signals across the spectrum of answers the model is prone to generating. In Algorithm \ref{alg:one_vs_k_active}, $\textsc{ActiveSelection}(\cdot)$ is a generic subroutine that selects a set of $k$ “high-impact” negatives. We will detail concrete implementations (e.g.\ bottom-$k$ by rating, clustering-based, etc.) in later sections.



\begin{algorithm}[t]
\caption{\textcolor{titlecolor}{\textbf{$\ampo$: One-Positive vs.\ $k$-Active Negatives}}}
\label{alg:one_vs_k_active}
\begin{algorithmic}[1]
    \STATE \textcolor{inputcolor}{\textbf{Input:} (1) A set of $N$ responses $\{y_i\}$ sampled from $P_{\theta}(y\mid x)$; (2) Their rewards $\{r_i\}$, embeddings $\{\mathbf{e}_i\}$, and probabilities $\{\pi_i\}$; (3) Number of negatives $k$, reference policy $P_{\mathrm{ref}}$, and hyperparameter $\alpha$}
    \STATE \textcolor{outputcolor}{\textbf{Output:} (i) Positive $y_{+}$; (ii) Negatives $\{y_j\}_{j \in S^-}$; (iii) Updated parameters $\theta$ via \textsc{swepo}}
    \STATE \textcolor{stepcolor}{\textit{1. Select One Positive (Highest Reward)}}
    \STATE \textcolor{mathcolor}{$i_{+} \leftarrow \arg\max_{i=1,\dots,N} r_i$, \quad $y_{+} \leftarrow y_{\,i_{+}}$}
    \STATE \textcolor{stepcolor}{\textit{2. Choose $k$ Negatives via Active Selection}}
    \STATE \textcolor{mathcolor}{$\Omega \leftarrow \{1,\dots,N\}\setminus\{i_{+}\}$}
    \STATE \textcolor{mathcolor}{$S^- \leftarrow \textsc{ActiveSelection}(\Omega,\{r_i\},\{\mathbf{e}_i\},\{\pi_i\},k)$}
    \STATE \textcolor{stepcolor}{\textit{3. Form One-vs.-$k$ \textsc{swepo} Objective}}
    \STATE \textcolor{mathcolor}{$\overline{r} \leftarrow \frac{r_{\,i_{+}} + \sum_{j\,\in\,S^-} r_j}{1 + k}$}
    \STATE For each $y_i$:
    \STATE \textcolor{mathcolor}{$s'_\theta(y_i) = \log P_\theta(y_i \mid x) + \alpha(r_i - \overline{r})$}
    \STATE \textcolor{mathcolor}{$L_{\text{swepo}}(\theta) = -\log\!\Biggl(\frac{\exp\!\bigl[s'_\theta(y_{+})\bigr]}{\exp\!\bigl[s'_\theta(y_{+})\bigr] + \sum_{\,j \,\in\, S^-}\exp\!\bigl[s'_\theta(y_j)\bigr]}\Biggr)$}
    \STATE \textcolor{stepcolor}{\textit{4. Update Model Parameters:}} \textcolor{mathcolor}{$\theta \leftarrow \theta - \eta\,\nabla_\theta L_{\text{swepo}}(\theta)$}
    \RETURN The chosen positive $y_{+}$, the negative set $\{y_j\}_{j \in S^-}$, and the updated parameters $\theta$
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}[t]
% \caption{\textbf{One-Positive vs.\ $k$-Active Negatives for group contrastive alignment via $\swepo$ loss}}
% \label{alg:one_vs_k_active}
% \begin{algorithmic}[1]
%     \STATE {\bfseries Input:} (1) A set of $N$ responses $\{y_i\}$ sampled from $P_{\theta}(y\mid x)$; (2) Their rewards $\{r_i\}$, embeddings $\{\mathbf{e}_i\}$, and probabilities $\{\pi_i\}$; (3) Number of negatives $k$, reference policy $P_{\mathrm{ref}}$, and hyperparameter $\alpha$
%     \STATE {\bfseries Output:} (i) Positive $y_{+}$; (ii) Negatives $\{y_j\}_{j \in S^-}$; (iii) Updated parameters $\theta$ via \textsc{swepo}
%     \STATE {\bfseries 1. Select One Positive (Highest Reward)}\\
%     $i_{+} \leftarrow \arg\max_{i=1,\dots,N} r_i$, \quad $y_{+} \leftarrow y_{\,i_{+}}$
%     \STATE {\bfseries 2. Choose $k$ Negatives via Active Selection}\\
%     $\Omega \leftarrow \{1,\dots,N\}\setminus\{i_{+}\}$\\ $S^- \leftarrow \textsc{ActiveSelection}(\Omega,\{r_i\},\{\mathbf{e}_i\},\{\pi_i\},k)$
%     \STATE {\bfseries 3. Form One-vs.-$k$ \textsc{swepo} Objective}\\
%     $\overline{r} \leftarrow \frac{r_{\,i_{+}} + \sum_{j\,\in\,S^-} r_j}{1 + k}$
%     \STATE For each $y_i$:\\ $s'_\theta(y_i) = \log P_\theta(y_i \mid x) - \log P_{\mathrm{ref}}(y_i \mid x) + \alpha(r_i - \overline{r})$
%     \STATE $L_{\text{swepo}}(\theta) = -\log\!\Biggl(\frac{\exp\!\bigl[s'_\theta(y_{+})\bigr]}{\exp\!\bigl[s'_\theta(y_{+})\bigr] + \sum_{\,j \,\in\, S^-}\exp\!\bigl[s'_\theta(y_j)\bigr]}\Biggr)$
%     \STATE {\bfseries 4. Update Model Parameters:} $\theta \leftarrow \theta - \eta\,\nabla_\theta L_{\text{swepo}}(\theta)$
%     \RETURN The chosen positive $y_{+}$, the negative set $\{y_j\}_{j \in S^-}$, and the updated parameters $\theta$
% \end{algorithmic}
% \end{algorithm}


\vspace{-0.1in}
\noindent
\subsection{Detailed Discussion of Algorithm \ref{alg:one_vs_k_active}}

\vspace{-0.1in}
The algorithm operates in four key steps: First, it selects the highest-reward response as the positive example (lines 3-4). Second, it actively selects $k$ negative examples by considering their rewards, probabilities $\pi_i$, and embedding distances $\mathbf{e}_i$ to capture diverse failure modes (lines 5-7). Third, it constructs the \textsc{swepo} objective by computing normalized scores $s'_\theta$ using the mean reward $\overline{r}$ and forming a one-vs-$k$ contrastive loss (lines 8-12). Finally, it updates the model parameters to increase the probability of the positive while suppressing the selected negatives (line 13). This approach ensures both reinforcement of high-quality responses and systematic penalization of problematic outputs across the response distribution.

% Step 1 ensures that at least one high-quality response is explicitly reinforced. Step 2 removes this positive from the candidate pool, then invokes an \emph{active} subroutine that must simultaneously account for reward (so that sufficiently poor answers are penalized), probability (so that common but flawed responses do not slip through), and diversity (so that rare but semantically distinct mistakes are also exposed). In Step 3, the one-vs.-$k$ \textsc{swepo} loss encourages the model to push up the log-probability of the positive while pushing down that of the $k$ negatives. Finally, Step 4 updates the parameters based on this contrast. 

% By highlighting a single “best” response and a carefully chosen set of “bad” or “undesired” responses, the algorithm provides a strong training signal across the reward distribution. Unlike naive strategies that choose negatives purely by lowest reward, our active-selection perspective seeks to uncover distinct semantic modes (via $\mathbf{e}_i$) or infrequent but still plausible outputs (via $\pi_i$). In this way, the model is given contrasting examples that refine its probability distribution to \emph{both} improve high-reward regions and suppress a diverse set of negative modes. 
% By coupling a single, clearly good response with an actively selected set of negative answers, the approach sculpts the model’s distribution towards several desirable outcomes in a single optimization step.


