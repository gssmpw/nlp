\section{Opt-Select: Active Subset Selection by Optimizing Expected Reward}
\label{sec:opt_select}

In this section, we propose \emph{Opt-Select}: a strategy for choosing $k$ \emph{negative} responses (plus one \emph{positive}) so as to \emph{maximize} the policy’s expected reward under a Lipschitz continuity assumption. Specifically, we model the local “neighborhood” influence of penalizing each selected negative and formulate an optimization problem that seeks to suppress large pockets of low-reward answers while preserving at least one high-reward mode. We first describe the intuition and objective, then present two solution methods: a \emph{mixed-integer program} (MIP) and a \emph{local search} approximation.

\subsection{Lipschitz-Driven Objective}
\label{subsec:lipschitz_objective}

Let $\{y_i\}_{i=1}^n$ be candidate responses sampled on-policy, each with reward $r_i \in [0,1]$ and embedding $\mathbf{e}_i \in \mathbb{R}^d$. Suppose that if we \emph{completely suppress} a response $y_j$ (i.e.\ set its probability to zero), all answers within distance $\|\mathbf{e}_i - \mathbf{e}_j\|$ must also decrease in probability proportionally, due to a Lipschitz constraint on the policy. Concretely, if the distance is $d_{i,j} = \|\mathbf{e}_i - \mathbf{e}_j\|$, and the model’s Lipschitz constant is $L$, then the probability of $y_i$ cannot remain above $L\,d_{i,j}$ if $y_j$ is forced to probability zero.

From an \emph{expected reward} perspective, assigning zero probability to \emph{low-reward} responses (and their neighborhoods) improves overall alignment. To capture this rigorously, observe that the \emph{penalty} from retaining a below-average answer $y_i$ can be weighted by:
\begin{align}
\label{eq:weight_w_i}
    w_i 
    \;=\;
    \exp\bigl(\,\overline{r} \;-\; r_i\bigr),
\end{align}
where $\overline{r}$ is (for instance) the mean reward of $\{r_i\}$. Intuitively, $w_i$ is larger for lower-reward $y_i$, indicating it is more harmful to let $y_i$ and its neighborhood remain at high probability.

Next, define a distance matrix 
\begin{align}
\label{eq:distance_matrix}
  A_{i,j} \;=\;
  \bigl\|\mathbf{e}_i - \mathbf{e}_j\bigr\|_2,
  \quad
  1 \le i,j \le n.
\end{align}
Selecting a subset $S\subseteq \{1,\dots,n\}$ of “negatives” to penalize suppresses the probability of each $i$ in proportion to $\min_{j \in S} A_{i,j}$. Consequently, a natural \emph{cost} function measures how much “weighted distance” $y_i$ has to its closest chosen negative:
\begin{align}
\label{eq:weighted_distance_cost}
    \text{Cost}(S)
    \;=\;
    \sum_{i=1}^n 
    w_i 
    \;\min_{\,j \in S}\;
    A_{i,j}.
\end{align}
Minimizing \eqref{eq:weighted_distance_cost} yields a subset $S$ of size $k$ that “covers” or “suppresses” as many low-reward responses (large $w_i$) as possible. We then \emph{add} one \emph{positive} index $i_{\mathrm{top}}$ with the highest $r_i$ to amplify a top-quality answer. This combination of \emph{one positive} plus \emph{$k$ negatives} provides a strong signal in the training loss.

\paragraph{Interpretation and Connection to Weighted k-medoids.}
If each negative $j$ “covers” responses $i$ within some radius (or cost) $A_{i,j}$, then \eqref{eq:weighted_distance_cost} is analogous to a weighted \emph{$k$-medoid} objective, where we choose $k$ items (negatives) to minimize a total weighted distance. Formally, this can be cast as a mixed-integer program (MIP) (Problem~\ref{eq:problem_P} below). For large $n$, local search offers an efficient approximation.

\subsection{Mixed-Integer Programming Formulation}

Define binary indicators $x_j = 1$ if we choose $y_j$ as a negative, and $z_{i,j} = 1$ if $i$ is assigned to $j$ (i.e.\ $\min_{j\in S} A_{i,j}$ is realized by $j$). We write:

\vspace{-0.15in}
\begin{align}
\label{eq:problem_P}
\textbf{Problem } \mathcal{P}: \quad
&\min_{\substack{x_j \in \{0,1\},\ z_{i,j}\in\{0,1\},\ y_i\ge 0}} 
 \sum_{i=1}^n w_i \,y_i 
\\
\text{s.t.}\quad
& \sum_{j=1}^n x_j = k, 
z_{i,j}\le x_j,
\sum_{j=1}^n z_{i,j} = 1, \forall\,i,\nonumber\\
& y_i \le A_{i,j} + M\,(1 - z_{i,j}), \nonumber\\
&y_i \ge A_{i,j} - M\,(1 - z_{i,j}),\quad\forall\,i,j,
\end{align}

\vspace{-0.1in}
where $M=\max_{i,j} A_{i,j}$. In essence, each $i$ is forced to \emph{assign} to exactly one chosen negative $j$, making $y_i = A_{i,j}$, i.e. the distance between the answer embeddings for answer $\{i,j\}$. Minimizing $\sum_i w_i\,y_i$ (i.e.\ \eqref{eq:weighted_distance_cost}) then ensures that low-reward points ($w_i$ large) lie close to at least one penalized center.

\vspace{-0.1in}
\paragraph{Algorithmic Overview.}
Solving $\mathcal{P}$ gives the $k$ negatives $S_{\mathrm{neg}}$, while the highest-reward index $i_{\mathrm{top}}$ is chosen as a positive. The final subset $\{i_{\mathrm{top}}\}\cup S_{\mathrm{neg}}$ is then passed to the \textsc{swepo} loss (see Section \ref{sec:methodology}). Algorithm~\ref{alg:opt_select} outlines the procedure succinctly.


\begin{algorithm}[t]
\caption{\textcolor{titlecolor}{$\ampoos$ via Solving MIP}}
\label{alg:opt_select}
\begin{algorithmic}[1]
    \STATE \textcolor{inputcolor}{\textbf{Input:} Candidates $\{y_i\}_{i=1}^n$ with $r_i, \mathbf{e}_i$; integer $k$}
    \STATE \textcolor{mathcolor}{Compute $i_{\mathrm{top}} = \arg\max_i\,r_i$}
    \STATE \textcolor{mathcolor}{Let $w_i = \exp(\,\overline{r} - r_i)$ with $\overline{r}$ as mean reward}
    \STATE \textcolor{mathcolor}{Solve Problem~\eqref{eq:problem_P} to get $\{x_j^*\}, \{z_{i,j}^*\}, \{y_i^*\}$}
    \STATE \textcolor{mathcolor}{Let $S_{\mathrm{neg}} = \{\,j \mid x_j^*=1\}$ (size $k$)}
    \RETURN \textcolor{outputcolor}{$\{\,i_{\mathrm{top}}\}\cup S_{\mathrm{neg}}$ for \textsc{swepo} training}
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}[t]
% \caption{$\ampoos$ via Solving MIP}
% \label{alg:opt_select}
% \begin{algorithmic}[1]
%     \STATE {\bfseries Input:} Candidates $\{y_i\}_{i=1}^n$ with $r_i, \mathbf{e}_i$; integer $k$
%     \STATE Compute $i_{\mathrm{top}} = \arg\max_i\,r_i$
%     \STATE Let $w_i = \exp(\,\overline{r} - r_i)$ with $\overline{r}$ as mean reward
%     \STATE Solve Problem~\eqref{eq:problem_P} to get $\{x_j^*\}, \{z_{i,j}^*\}, \{y_i^*\}$
%     \STATE Let $S_{\mathrm{neg}} = \{\,j \mid x_j^*=1\}$ (size $k$)
%     \RETURN $\{\,i_{\mathrm{top}}\}\cup S_{\mathrm{neg}}$ for \textsc{swepo} training
% \end{algorithmic}
% \end{algorithm}

\vspace{-0.1in}
\subsection{Local Search Approximation}

\vspace{-0.1in}
For large $n$, an exact MIP can be expensive. A simpler \emph{local search} approach initializes a random subset $S$ of size $k$ and iteratively swaps elements in and out if it lowers the cost \eqref{eq:weighted_distance_cost}. In practice, this provides an efficient approximation, especially when $n$ or $k$ grows.

\begin{algorithm}[t]
\caption{\textcolor{titlecolor}{$\ampoos$ via Coordinate Descent}}
\label{alg:opt_select_local_search}
\begin{algorithmic}[1]
    \STATE \textcolor{inputcolor}{\textbf{Input:} Set $I = \{1,\dots,n\}$, integer $k$, distances $A_{i,j}$, rewards $\{r_i\}$}
    \STATE \textcolor{mathcolor}{Find $i_{\mathrm{top}} = \arg\max_{i}\, r_i$}
    \STATE \textcolor{mathcolor}{Compute $w_i = \exp(\,\overline{r} - r_i)$ and $d_{i,j}=A_{i,j}$}
    \STATE \textcolor{mathcolor}{Initialize a random subset $S \subseteq I\setminus\{i_{\mathrm{top}}\}$ of size $k$}
    \WHILE{improving}
        \STATE \textcolor{mathcolor}{Swap $j_{\mathrm{out}} \in S$ with $j_{\mathrm{in}} \notin S$ if it decreases $\sum_{i \in I} w_i\,\min_{j \in S} d_{i,j}$}
    \ENDWHILE
    \RETURN \textcolor{outputcolor}{$S_{\mathrm{neg}}=S$ (negatives) and $i_{\mathrm{top}}$ (positive)}
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}[t]
% \caption{$\ampoos$ via Coordinate Descent}
% \label{alg:opt_select_local_search}
% \begin{algorithmic}[1]
%     \STATE {\bfseries Input:} Set $I = \{1,\dots,n\}$, integer $k$, distances $A_{i,j}$, rewards $\{r_i\}$
%     \STATE Find $i_{\mathrm{top}} = \arg\max_{i}\, r_i$
%     \STATE Compute $w_i = \exp(\,\overline{r} - r_i)$ and $d_{i,j}=A_{i,j}$
%     \STATE Initialize a random subset $S \subseteq I\setminus\{i_{\mathrm{top}}\}$ of size $k$
%     \WHILE{improving}
%         \STATE Swap $j_{\mathrm{out}} \in S$ with $j_{\mathrm{in}} \notin S$ if it decreases $\sum_{i \in I} w_i\,\min_{j \in S} d_{i,j}$
%     \ENDWHILE
%     \RETURN $S_{\mathrm{neg}}=S$ (negatives) and $i_{\mathrm{top}}$ (positive)
% \end{algorithmic}
% \end{algorithm}

\paragraph{Intuition.}
If $y_i$ is far from all penalized points $j\in S$, then it remains relatively “safe” from suppression, which is undesirable if $r_i$ is low (i.e.\ $w_i$ large). By systematically choosing $S$ to reduce $\sum_i w_i\,\min_{j\in S}d_{i,j}$, we concentrate penalization on high-impact, low-reward regions. The local search repeatedly swaps elements until no single exchange can further reduce the cost.

\subsection{Why ``Opt-Select''? A Lipschitz Argument for Expected Reward}

We name the procedure ``Opt-Select'' because solving \eqref{eq:problem_P} (or its local search variant) directly approximates an \emph{optimal} subset for improving the policy's expected reward. Specifically, under a Lipschitz constraint with constant $L$, assigning zero probability to each chosen negative $y_j$ implies \emph{neighboring answers} $y_i$ at distance $d_{i,j}$ cannot exceed probability $L\,d_{i,j}$. Consequently, their contribution to the ``bad behavior'' portion of expected reward is bounded by
\[
   \exp\bigl(r_{\max} - r_i\bigr)\,\bigl(\,L\,d_{i,j}\bigr),
\]
where $r_{\max}$ is the rating of the best-rated response. Dividing by a normalization factor (such as $\exp(r_{\max} - \overline{r})\,L$), one arrives at a cost akin to $w_i\, d_{i,j}$ with $w_i = \exp(\overline{r}-r_i)$. 
% Hence, minimizing \eqref{eq:weighted_distance_cost} ensures that \emph{low-reward} points do not remain at large distance from any chosen negative (i.e.\ they get suppressed). 
This aligns with classical \emph{min-knapsack} of minimizing some costs subject to some constraints, and has close alignment with the \emph{weighted $k$-medoid} notions of “covering” important items at minimum cost. %By combining this with a single top-reward positive, Opt-Select systematically pushes probability mass toward high-reward modes while penalizing low-reward regions.
