\begin{abstract}
Multi-preference optimization enriches language-model alignment beyond pairwise preferences by contrasting entire sets of helpful and undesired responses, enabling richer training signals for large language models. During self-play alignment, these models often produce numerous candidate answers per query, making it computationally infeasible to include all of them in the training objective. We propose \textit{Active Multi-Preference Optimization} ($\ampo$), which combines \emph{on-policy} generation, a multi-preference \emph{group-contrastive} loss, and \emph{active} subset selection. Specifically, we score and embed large candidate pools of responses, then pick a small but informative subset—covering reward extremes and distinct semantic clusters—for preference optimization. 
The resulting contrastive~training scheme identifies not only the best and worst answers but also subtle, underexplored modes crucial for robust alignment.  
Theoretically, we provide guarantees of expected reward maximization using our active selection method.
Empirically, AMPO achieves state-of-the-art results on \textit{AlpacaEval} with Llama 8B. 
We release our datasets at \href{https://huggingface.co/datasets/Multi-preference-Optimization/}{huggingface/MPO}.
\end{abstract}
