\section{Experimental Setup}
\label{sec:experimental setup}

\begin{figure*}[!thbp]
    \centering
    \includegraphics[width=1.0\textwidth]{images/tsne_plot_with_scores_35914.pdf}
    \vspace{-0.25in}
    % \includegraphics[draft]{images/Sweppo_illustration.pdf}
    \caption{t-SNE visualization of projected high-dimensional response embeddings into a 2D space, illustrating the separation of actively selected responses. (a) AMPO-BottomK (baseline). (b) AMPO-Coreset (ours). (c) Opt-Select (ours). We see that the traditional baselines select many responses close to each other, based on their rating. This provides insufficient feedback to the LLM during preference optimization. In contrast, our methods simultaneously optimize for objectives including coverage, generation probability as well as preference rating.}
\label{fig:summation_logps_analysis}
\end{figure*}




\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard} & \textbf{MT-Bench} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
 & \textbf{LC (\%)} & \textbf{WR (\%)} & \textbf{WR (\%)} & \textbf{GPT-4} \\ \midrule
Base & 28.4 & 28.4& 26.9 & 7.93 \\
Best-vs-worst & 47.6 & 44.7 & 34.6 & 7.51 \\
\ampo-Bottomk & 50.8 & 50.5 & \textbf{44.8} & \textbf{8.11} \\
\ampo-Coreset & \textbf{52.4} & \textbf{52.1} & \underline{39.4} & \underline{8.05} \\
\ampo-Opt-Select & \underline{51.6} & \underline{51.2} & 37.9 & 7.96 \\
\bottomrule
\end{tabular}
}
\caption{Comparison of various preference optimization baselines on AlpacaEval, Arena-Hard, and MT-Bench benchmarks for Llama-3-Instruct (8B). LC-WR represents length-controlled win rate, and WR represents raw win rate. Best results are in \textbf{bold}, second-best are \underline{underlined}. Our method ($\ampo$) achieves SOTA performance across all metrics, with different variants achieving either best or second-best results consistently.}
\label{tab:llama3-results}
\end{table}


\paragraph{Model and Training Settings:}
For our experiments, we utilized the Ultrafeedback Dataset \cite{cui2023ultrafeedback}, an instruction-following benchmark annotated by GPT-4. This dataset consists of approximately 64,000 instructions, each paired with four responses generated by different language models. GPT-4 assigned scalar rewards on a 0-to-10 scale for each response, which prior research has shown to correlate strongly with human annotations. This establishes GPT-4 ratings as a reliable and cost-efficient alternative to manual feedback.

In our broader framework, we first trained a base model (mistralai/Mistral-7B-v0.1) on the UltraChat-200k dataset to obtain an SFT model. This SFT model, trained on open-source data, provides a transparent starting point. Subsequently, we refined the model by performing preference optimization on the UltraFeedback dataset. Once fine-tuned, the model was used for alignment. This two-step process ensures the model is well-prepared for tasks.

In our experiments, we observed that tuning hyperparameters is critical for optimizing the performance of all offline preference optimization algorithms, including DPO, SimPO, \refa-InfoNCA, \refa-1-vs-all and \refa-dynamic. Carefully selecting hyperparameter values significantly impacts the effectiveness of these methods across various datasets.

For \refa-InfoNCA, we found that setting the beta parameter in the range of 2.0 to 2.5 consistently yields strong performance. Similarly, for \refa-1-vs-all and \refa-dynamic, optimal results were achieved with beta values between 2.0 and 4.0, while tuning the gamma parameter within the range of 0.7 to 1.4 further improved performance. These observations highlight the importance of systematic hyperparameter tuning to achieve reliable outcomes across diverse datasets.

\paragraph{Evaluation Benchmarks}
We evaluate our models using three widely recognized open-ended instruction-following benchmarks: MT-Bench, AlpacaEval 2, AlpacaEval and Arena-Hard v0.1. These benchmarks are commonly used in the community to assess the conversational versatility of models across a diverse range of queries.

AlpacaEval 2 comprises 805 questions sourced from five datasets, while MT-Bench spans eight categories with a total of 80 questions. The recently introduced Arena-Hard builds upon MT-Bench, featuring 500 well-defined technical problem-solving queries designed to test more advanced capabilities.

We adhere to the evaluation protocols specific to each benchmark when reporting results. For AlpacaEval 2, we provide both the raw win rate (WR) and the length-controlled win rate (LC), with the latter being designed to mitigate the influence of model verbosity. For Arena-Hard, we report the win rate (WR) against a baseline model. For MT-Bench, we present the scores as evaluated by GPT-4-Preview-1106, which serve as the judge model.


\begin{figure}
    \centering
\includegraphics[width=1.0\columnwidth]{images/active_selection_optimization_samp_temp_variation.pdf}
    \vspace{-0.25in}
    \caption{Effect of Sampling Temperature on different baselines for on the AlpacaEval 2 Benchmark: (a) Length-Controlled Win Rate (LC) and (b) Overall Win Rate (WR).}
    \label{fig:samp-temp-analysis}
\end{figure}


\paragraph{Baselines}
We compare our approach against several established offline preference optimization methods, summarized in Table . Among these are RRHF \cite{yuan2023rrhf} and SLiC-HF \cite{zhao2023slic}, which employ ranking loss techniques. RRHF uses a length-normalized log-likelihood function, akin to the reward function utilized by SimPO \cite{meng2024simpo}, whereas SLiC-HF directly incorporates log-likelihood and includes a supervised fine-tuning (SFT) objective in its training process.

IPO \cite{azar2023general} presents a theoretically grounded approach that avoids the assumption made by DPO, which treats pairwise preferences as interchangeable with pointwise rewards. CPO \cite{guo2024controllable}, on the other hand, uses sequence likelihood as a reward signal and trains jointly with an SFT objective.

ORPO \cite{hong2024orpo} introduces a reference-free odds ratio term to directly contrast winning and losing responses using the policy model, also incorporating joint training with the SFT objective. R-DPO \cite{Park2024DisentanglingLF} extends DPO by adding a regularization term that mitigates the exploitation of response length.

InfoNCA \cite{chen2024noise}, which introduces a K-category cross-entropy loss, reframes generative modeling problems as classification tasks by contrasting multiple data points. It computes soft labels using dataset rewards, applying a softmax operation to map reward values into probability distributions.

Lastly, SimPO \cite{meng2024simpo} leverages the average log probability of a sequence as an implicit reward, removing the need for a reference model. It further enhances performance by introducing a target reward margin to the Bradley-Terry objective, significantly improving the algorithm's effectiveness.


