\vspace{-0.15in}
\section{Introduction}


\begin{figure}[!thbp]
    \centering
    \includegraphics[width=\linewidth]{images/AMPO_illustration.pdf}
    \vspace{-0.15in}
    \caption{Overview of the Active Multi-Preference Optimization framework. Given a query, the LLM generates diverse responses, which are evaluated by a rater model. Selected responses with different ratings and semantics are then used to train and align the LLM through preference optimization. Active selection of the preferences to optimize over improves training dynamics.}
\label{fig:ampo_illustration}
\end{figure}

\vspace{-0.05in}
Preference Optimization (PO) has become a standard approach for aligning large language models (LLMs) with human preferences \citep{christiano2017deep, ouyang2022training, bai2022training}. Traditional alignment pipelines typically rely on pairwise or binary preference comparisons, which may not fully capture the subtleties of human judgment \citep{rafailov2024direct, liu2024tis, korbak2023pretraining}. As a remedy, there is increasing interest in \emph{multi-preference} methods, which consider entire sets of responses when providing feedback \citep{cui2023ultrafeedback, chen2024noise, gupta2024swepo}. By learning from multiple “good” and “bad” outputs simultaneously, these approaches deliver richer alignment signals than purely pairwise methods. At the same time, an important trend in alignment is the shift to \emph{on-policy} or “self-play” data generation, where the policy learns directly from its own distribution of outputs at each iteration \citep{chen2024self, kumar2024training, wu2023fine, wu2024self}. This feedback loop can accelerate convergence ensuring that the training data stays relevant to the model’s behavior.


\vspace{-0.05in}
However, multi-preference alignment faces a serious bottleneck: modern LLMs can easily generate dozens of candidate responses per query, and incorporating \emph{all} of these into a single training objective can become computationally infeasible \citep{askell2021general}. Many of these sampled responses end up being highly similar or near-duplicates, providing limited additional information for gradient updates \citep{long2024llms}. Consequently, naive attempts to process all generated responses cause both memory blow-ups and diminishing returns in training \citep{dubey2024llama}. Given these constraints, identifying a \emph{small yet highly informative} subset of candidate responses is critical for effective multi-preference learning.



One way to conceptualize the problem is through an “\textit{island}” metaphor (See Figure \ref{fig:ampo_illustration}). Consider each prompt’s answer space as a set of semantic islands, where certain clusters of responses (islands) may be exceptionally good (tall peaks) or particularly poor (flat plains). Focusing only on the tallest peaks or the worst troughs can cause the model to overlook crucial middle-ground modes—“islands” that might harbor subtle failure modes or moderate-quality answers. Therefore, an ideal subset selection strategy should \emph{cover} the landscape of responses by sampling from each island \citep{yu2024large}. In this paper, we show that selecting representatives from all such “islands” is not only about diversity but can also be tied to an \emph{optimal} way of suppressing undesired modes under a mild Lipschitz assumption (see Section \ref{sec:theory_main}).

\vspace{-0.05in}
Fundamentally, the process of deciding which responses deserve feedback naturally evokes the lens of \emph{active learning}, where we “actively” pick the most informative data samples
\citep{cohn1996active, ceravolo2024active, xiao2023freeal}. By selecting a small yet diverse subset of responses, the model effectively creates a \emph{curriculum} for itself. Rather than passively training on random or exhaustively sampled data, an active learner \emph{queries} the examples that yield the greatest improvement when labeled. In our context, we actively pick a handful of responses that best illustrate extreme or underexplored behaviors—whether very good, very bad, or semantically distinct \citep{wu2023fine}. This helps the model quickly eliminate problematic modes while reinforcing the most desirable responses. Crucially, we remain on-policy: after each update, the newly refined policy generates a fresh batch of responses, prompting another round of active subset selection \citep{liu2021self}.

\vspace{-0.05in}
We propose \textbf{Active Multi-Preference Optimization (AMPO)}, a framework that unifies (a) on-policy data generation, (b) group-based preference learning, and (c) \emph{active} subset selection. Specifically, we adopt a reference-free group-contrastive objective known as \(\refa\) \citep{gupta2024refa}, which jointly leverages multiple “positive” and “negative” responses in a single loss term. On top of this, we explore various active selection schemes—ranging from simplest bottom-$K$ ranking \citep{meng2024simpo} to coreset-based clustering \citep{cohen2021new, cohen2022improved, huang2019coresets} and a more theoretically grounded “Opt-Select” method that ties coverage to maximizing expected reward. Our contributions are: 
(i) a unifying algorithmic pipeline for multi-preference alignment with active selection, 
(ii) theoretical results demonstrating that coverage of distinct clusters 
à la k-medoids, can serve as an \emph{optimal} negative-selection strategy, 
and (iii) empirical evaluations showing that $\ampo$ achieves state of the art results compared to strong alignment baselines like \simpo.
% more robust alignment with lower computational overhead compared to naive baselines. 
Altogether, we hope this approach advances the state of multi-preference optimization, enabling models to learn more reliably from diverse sets of model behaviors.

\vspace{-0.15in}
\paragraph{Related Works:}
We provide a detailed description of our related work in Appendix  \ref{sec:related_work_extended} covering other multi-preference optimization methods, on-policy alignment, coverage-based selection approaches.
