
\section{Experiments}

\subsection{Experimental Setup}
\label{sec:experimental setup}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{images/tsne_plot_with_scores_35914.pdf}
    \vspace{-0.25in}
    % \includegraphics[draft]{images/Sweppo_illustration.pdf}
    \caption{t-SNE visualization of projected high-dimensional response embeddings into a 2D space, illustrating the separation of actively selected responses. (a) AMPO-BottomK (baseline). (b) AMPO-Coreset (ours). (c) Opt-Select (ours). We see that the traditional baselines select many responses close to each other, based on their rating. This provides insufficient feedback to the LLM during preference optimization. In contrast, our methods simultaneously optimize for objectives including coverage, generation probability as well as preference rating.}
\label{fig:tsne_analysis}
\end{figure*}





\paragraph{Model and Training Settings:}
For our experiments, we utilize a pretrained instruction-tuned model (\href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{meta-llama/MetaLlama-3-8B-Instruct}), as the SFT model. These models have undergone extensive instruction tuning, making them more capable and robust compared to the SFT models used in the Base setup. However, their reinforcement learning with human feedback (RLHF) procedures remain undisclosed, making them less transparent.

To reduce distribution shift between the SFT models and the preference optimization process, we follow the approach in \cite{tran2023iterative}  and generate the preference dataset using the same SFT models. This ensures that our setup is more aligned with an on-policy setting. Specifically, we utilize prompts from the UltraFeedback dataset \cite{cui2023ultrafeedback} and regenerate the resonses using the SFT models. For each prompt x, we produce 32 responses by sampling from the SFT model with a sampling temperature of 0.8. We then use the reward model (\href{https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B-v0.2}{Skywork/Skywork-Reward-Llama-3.1-8B-v0.2}) \cite{liu2024skywork} to score all the 32 responses. Then the response are selected based on the Active Subset selection strategies a.) \textbf{AMPO-Bottomk} b.) \textbf{AMPO-Coreset} c.) \textbf{AMPO-OptSelect}

In our experiments, we observed that tuning hyperparameters is critical for optimizing the performance . Carefully selecting hyperparameter values significantly impacts the effectiveness of these methods across various datasets.We found that setting the $\beta$ parameter in the range of 5.0 to 10.0 consistently yields strong performance, while tuning the $\gamma$ parameter within the range of 2 to 4 further improved performance. These observations highlight the importance of systematic hyperparameter tuning to achieve reliable outcomes across diverse datasets.

\paragraph{Evaluation Benchmarks}
We evaluate our models using three widely recognized open-ended instruction-following benchmarks: MT-Bench \cite{zheng2023judging}, AlpacaEval2 \cite{dubois2024length}, and Arena-Hard v0.1. These benchmarks are commonly used in the community to assess the conversational versatility of models across a diverse range of queries.

AlpacaEval 2 comprises 805 questions sourced from five datasets, while MT-Bench spans eight categories with a total of 80 questions. The recently introduced Arena-Hard builds upon MT-Bench, featuring 500 well-defined technical problem-solving queries designed to test more advanced capabilities.

We adhere to the evaluation protocols specific to each benchmark when reporting results. For AlpacaEval 2, we provide both the raw win rate (WR) and the length-controlled win rate (LC), with the latter being designed to mitigate the influence of model verbosity. For Arena-Hard, we report the win rate (WR) against a baseline model. For MT-Bench, we present the scores as evaluated by GPT-4-Preview-1106, which serve as the judge model.


\begin{figure}[!b]
    \centering
\includegraphics[width=1.0\columnwidth]{images/active_selection_optimization_samp_temp_variation.pdf}
    \vspace{-0.25in}
    \caption{Effect of Sampling Temperature on different baselines for on the AlpacaEval 2 Benchmark: (a) Length-Controlled Win Rate (LC) and (b) Overall Win Rate (WR).}
    \label{fig:samp-temp-analysis}
\end{figure}

\begin{figure}[!b]
    \centering
\includegraphics[width=1.0\columnwidth]{images/active_selection_optimization_gamma_variation.pdf}
    \vspace{-0.15in}
    \caption{Effect of Gamma on AlpacaEval2 for Active Subset Selection Strategies.}
    \label{fig:samp-gamma-analysis}
\end{figure}

% \paragraph{Baselines}
% We compare our approach against several established offline preference optimization methods, summarized in Table . Among these are RRHF \cite{yuan2023rrhf} and SLiC-HF \cite{zhao2023slic}, which employ ranking loss techniques. RRHF uses a length-normalized log-likelihood function, akin to the reward function utilized by SimPO \cite{meng2024simpo}, whereas SLiC-HF directly incorporates log-likelihood and includes a supervised fine-tuning (SFT) objective in its training process.

% IPO \cite{azar2023general} presents a theoretically grounded approach that avoids the assumption made by DPO, which treats pairwise preferences as interchangeable with pointwise rewards. CPO \cite{guo2024controllable}, on the other hand, uses sequence likelihood as a reward signal and trains jointly with an SFT objective.

% ORPO \cite{hong2024orpo} introduces a reference-free odds ratio term to directly contrast winning and losing responses using the policy model, also incorporating joint training with the SFT objective. R-DPO \cite{Park2024DisentanglingLF} extends DPO by adding a regularization term that mitigates the exploitation of response length.

% InfoNCA \cite{chen2024noise}, which introduces a K-category cross-entropy loss, reframes generative modeling problems as classification tasks by contrasting multiple data points. It computes soft labels using dataset rewards, applying a softmax operation to map reward values into probability distributions.

% Lastly, SimPO \cite{meng2024simpo} leverages the average log probability of a sequence as an implicit reward, removing the need for a reference model. It further enhances performance by introducing a target reward margin to the Bradley-Terry objective, significantly improving the algorithm's effectiveness.

\subsection{Experimental Result}

\paragraph{Impact of Selection Strategies on Diversity.}
Figure~\ref{fig:tsne_analysis} shows a t-SNE projection of response embeddings, highlighting how each selection method samples the answer space:\\
\textbf{AMPO-BottomK}: Tends to pick a tight cluster of low-rated responses, limiting coverage and redundancy in feedback.\\
\textbf{AMPO-Coreset}: Uses coreset-based selection to cover more diverse regions, providing coverage of examples.\\
\textbf{Opt-Select}: Further balances reward extremity, and embedding coverage, yielding well-separated response clusters and more effective supervision for preference alignment.


% \paragraph{Impact of Selection Strategies on Response Diversity}
% To further investigate the effectiveness of our active selection strategies, we visualize the t-SNE projection of high-dimensional response embeddings into a 2D space in Fig. \ref{fig:tsne_analysis}. This visualization highlights the spatial distribution of selected responses compared to rejected and non-selected responses across different selection strategies.
% \begin{itemize}
%     \item \textbf{AMPO-BottomK (Ours)}: In this approach we primarily bottom k responses that are closely clustered together when observed during evaluation, leading to redundancy in feedback. This selection bias arises from a heavy reliance on rating scores, which limits diversity in sampled responses and provides insufficient gradient signals for optimizing model preferences.
%     \item \textbf{AMPO-Coreset (Ours)}: Our method leverages a coreset-based selection strategy, which ensures broader coverage of the response space. This enables the model to learn from a more representative subset, capturing diverse response characteristics that improve optimization robustness.
%     \item \textbf{Opt-Select (Ours)}: Our optimized selection strategy further enhances response diversity by jointly optimizing for preference ratings, generation probability, and embedding coverage. This results in a well-separated distribution of selected responses, ensuring more effective supervision signals for preference alignment.
% \end{itemize}




\vspace{-0.1in}
\textbf{Key analysis} from Fig. \ref{fig:tsne_analysis} demonstrate that our selection strategies significantly improve response diversity compared to traditional baselines. By actively optimizing for coverage-aware selection, our methods mitigate redundancy in selected responses, leading to better preference modeling and enhanced LLM alignment.

\vspace{-0.1in}
\paragraph{Impact of Temperature Sampling for Different Active Selection Approaches}
To analyze the impact of temperature-controlled response sampling on different active selection approaches, we conduct an ablation study by varying the sampling temperature from 0 to 1.0 in increments of 0.25 on AlpacaEval2 benchmark as demonstrated in Figure \ref{fig:samp-temp-analysis}. We evaluate our active selection strategies observe a general trend of declining performance with increasing temperature. \textbf{Key observation:} \ampo-Coreset and \ampo-OptSelect demonstrate robustness to temperature variations, whereas WR-SimPO and bottom-k selection are more sensitive.

\vspace{-0.1in}
\paragraph{Effect of gamma for Active Selection Approaches}
To further investigate the sensitivity of core-set selection to different hyper-parameter settings, we conduct an ablation study on the impact of varying the gamma parameter as show in Figure \ref{fig:samp-gamma-analysis}. As gamma increases from 1 to 3, we observe a consistent improvement in both LC-WR and WR scores. \textbf{Key findings} highlight the importance of tuning gamma appropriately to maximize the effectiveness of active-selection approaches.

