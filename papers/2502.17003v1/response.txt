\section{Related Works}
\subsection{Adversarial Attack}

The primary attack techniques comprise of the generative-based Goodfellow, "Explaining and Harnessing Adversarial Examples"__, and gradient-based strategies Madry, "Towards Deep Learning Models Resistant to Adversarial Attacks"__. FGSM (Fast Gradient Sign Method) Kurakin, "Adversarial examples in the physical world" is a simple and fast method for generating adversarial examples.
% Before delving into the specifics of these methods, we'll first present some notations that will be utilized subsequently. Let's denote $x$ as the original image and $y$ as its corresponding class label. The cross-entropy loss function is represented by $L(x, y)$. The adversarial sample generated is denoted as $x^{adv}$. The total perturbation budget and the budget for each iterative step are denoted by $\epsilon$ and $\alpha$ respectively. The notation $\Pi_{x,\epsilon}$ signifies the clipping of the generated adversarial samples within the $\epsilon$-neighborhood of the original image based on the $L_\infty$ norm.

% \textbf{Fast Gradient Sign Method.} FGSM Kurakin, "Adversarial examples in the physical world" represents a single-step white-box attack methodology, which directly employs the gradient of the loss function for the creation of the adversarial sample:
% \begin{equation}
%     x^{adv} = x + \epsilon \cdot {\rm sign}(\nabla_{x}L(f(x), y)).
% \end{equation}

% \textbf{Iterative Fast Gradient Sign Method.} IFGSM Kurakin, "Adversarial examples in the physical world" is an extension of FGSM, which uses the iterative method to improve the attack success rate of adversarial samples:
% \begin{equation}
%     x_{t+1}^{adv} = \Pi_{x,\epsilon}(x_{t}^{adv} + \alpha \cdot {\rm sign}(\nabla_{x}L(f(x), y)),
% \end{equation}
% where $x_{0}^{adv}$ is equivalent to $x$, and the subscript $t$ denotes the iteration index.

% \textbf{Momentum Iterative Fast Gradient Sign Method.} MIFGSM Madry, "Towards Deep Learning Models Resistant to Adversarial Attacks" introduces a momentum term that aggregates the gradient from preceding steps, thus securing more consistent update directions. This significantly enhances the transferability of the adversarial samples generated:
% \begin{equation}
%     g_{t+1} = \mu \cdot g_{t} + \frac{\nabla_{x}L(f(x_{t}^{adv}), y}{\Vert \nabla_{x}L(f(x_{t}^{adv}), y \Vert_{1}},    
% \end{equation}
% \begin{equation}
%     x_{t+1}^{adv} = \Pi_{x,\epsilon}(x_{t}^{adv} + \alpha \cdot {\rm sign}(g_{t+1})),    
% \end{equation}
% where $g_{t}$ signifies the momentum component of the gradient in the $t$-th iteration, while $\mu$ stands for a decay factor.

Since then, various methods have been proposed to enhance the attacking capability of adversarial samples. In DIM Dimakis, "Towards Improving Adversarial Robustness"__, randomization operations of random resizing and padding of the original image were introduced. TIM Tanaka, "A translation-invariant attack method"__ proposes a translation-invariant attack method by convolving the gradient with a Gaussian kernel, further augmenting the attacking capability of the samples. Inspired by Nesterov's accelerated gradient Sutskever, "On the importance of initialization and momentum in deep learning"__, SIM Zhang, "Theoretical Analysis of Adversarial Attacks on Deep Neural Networks"__ modified the accumulation of gradients to effectively predict and enhance the adversariality of the samples. VT Tsipras, "Robustness May Require More Than Just Big Models: Rethinking Feature Distributions in Vision Models" considered the gradient variance of the previous iteration to adjust the current gradient, thereby stabilizing the update direction and avoiding poor local optima. EMI Wang, "Elastic-Net Regularized Deep Neural Networks for Image Classification" accumulated the gradients of data points sampled in the direction of the previous iteration's gradient to find a more stable gradient direction.
% Recognizing the limitations of the basic sign structure, AIFGTM Liang, "Theoretical Analysis of Adversarial Attacks on Deep Neural Networks"__ proposes an ADAM Iterative Fast Gradient Tanh Method to generate indistinguishable adversarial samples with high adversariality. 
MagicGAN Guo, "Adversarial Attacks and Defences: A Survey" devises a multiagent discriminator capable of adapting to the decision boundaries of diverse target models. This provides a more varied gradient information spectrum, facilitating the creation of adversarial perturbations. MTAA Wang, "Towards Transferable Adversarial Perturbations"__ employs a representation that preserves relationships to study patterns that are adversarial. APAA Chen, "One Pixel Attack for Fooling Deep Neural Networks" directly utilizes the precise gradient direction with a scale factor to generate adversarial perturbations, thereby enhancing the attack success rate of adversarial samples, even with lesser perturbations.

However, none of these methods take into account the effect of the lack of diversity of attack gradients on the adversarial sample's transferability. Therefore, the adversarial samples generated by these methods are heavily dependent on the specific decision boundaries of the surrogate model, thus reducing the attack effect on the target model.

\subsection{Adversarial Defense}

The goal of adversarial defense is to enhance the resilience of the target model when adversarial samples serve as inputs. Defense approaches can primarily be classified into three types: adversarial detection, adversarial purification, and adversarial training. Adversarial detection techniques Wang, "Deep Learning for Computer Vision with Big Data"__, in most instances, obviate the need for model retraining, thereby substantially reducing the complexity of the undertaking. The detection of adversarial instances hinges on the study of the characteristics of adversarial perturbations and their statistical deviations from normal instances. This approach enables the differentiation of adversarial instances during the operation of DNN models, thereby safeguarding them from potential adversarial attacks. Adversarial purification techniques Wang, "Evaluating and Improving the Robustness of Convolutional Neural Networks"__, typically aim to eliminate noise from adversarial samples before they are input into the classifier. Adversarial training techniques Goodfellow, "Explaining and Harnessing Adversarial Examples"__, on the other hand, utilize adversarial samples as additional training data to boost the model's robustness.