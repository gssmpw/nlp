\section{Related Works}
\subsection{Adversarial Attack}

The primary attack techniques comprise of the generative-based~\cite{zhao2018generating,song2018constructing,joshi2019semantic,qiu2020semanticadv,xiao2021improving}, and gradient-based strategies~\cite{goodfellow2014explaining,madry2017towards,dong2018boosting,xie2019improving,dong2019evading,lin2019nesterov}. FGSM (Fast Gradient Sign Method)~\cite{goodfellow2014explaining} is a simple and fast method for generating adversarial examples.
% Before delving into the specifics of these methods, we'll first present some notations that will be utilized subsequently. Let's denote $x$ as the original image and $y$ as its corresponding class label. The cross-entropy loss function is represented by $L(x, y)$. The adversarial sample generated is denoted as $x^{adv}$. The total perturbation budget and the budget for each iterative step are denoted by $\epsilon$ and $\alpha$ respectively. The notation $\Pi_{x,\epsilon}$ signifies the clipping of the generated adversarial samples within the $\epsilon$-neighborhood of the original image based on the $L_\infty$ norm.

% \textbf{Fast Gradient Sign Method.} FGSM~\cite{goodfellow2014explaining} represents a single-step white-box attack methodology, which directly employs the gradient of the loss function for the creation of the adversarial sample:
% \begin{equation}
%     x^{adv} = x + \epsilon \cdot {\rm sign}(\nabla_{x}L(f(x), y)).
% \end{equation}

% \textbf{Iterative Fast Gradient Sign Method.} IFGSM~\cite{kurakin2016adversarial} is an extension of FGSM, which uses the iterative method to improve the attack success rate of adversarial samples:
% \begin{equation}
%     x_{t+1}^{adv} = \Pi_{x,\epsilon}(x_{t}^{adv} + \alpha \cdot {\rm sign}(\nabla_{x}L(f(x), y)),
% \end{equation}
% where $x_{0}^{adv}$ is equivalent to $x$, and the subscript $t$ denotes the iteration index.

% \textbf{Momentum Iterative Fast Gradient Sign Method.} MIFGSM~\cite{dong2018boosting} introduces a momentum term that aggregates the gradient from preceding steps, thus securing more consistent update directions. This significantly enhances the transferability of the adversarial samples generated:
% \begin{equation}
%     g_{t+1} = \mu \cdot g_{t} + \frac{\nabla_{x}L(f(x_{t}^{adv}), y}{\Vert \nabla_{x}L(f(x_{t}^{adv}), y \Vert_{1}},    
% \end{equation}
% \begin{equation}
%     x_{t+1}^{adv} = \Pi_{x,\epsilon}(x_{t}^{adv} + \alpha \cdot {\rm sign}(g_{t+1})),    
% \end{equation}
% where $g_{t}$ signifies the momentum component of the gradient in the $t$-th iteration, while $\mu$ stands for a decay factor.

Since then, various methods have been proposed to enhance the attacking capability of adversarial samples. In DIM~\cite{xie2019improving}, randomization operations of random resizing and padding of the original image were introduced. TIM~\cite{dong2019evading} proposes a translation-invariant attack method by convolving the gradient with a Gaussian kernel, further augmenting the attacking capability of the samples. Inspired by Nesterov's accelerated gradient~\cite{nesterov1983method}, SIM~\cite{lin2019nesterov} modified the accumulation of gradients to effectively predict and enhance the adversariality of the samples. VT~\cite{wang2021enhancing} considered the gradient variance of the previous iteration to adjust the current gradient, thereby stabilizing the update direction and avoiding poor local optima. EMI~\cite{wang2021boosting} accumulated the gradients of data points sampled in the direction of the previous iteration's gradient to find a more stable gradient direction.
% Recognizing the limitations of the basic sign structure, AIFGTM~\cite{zou2022making} proposes an ADAM Iterative Fast Gradient Tanh Method to generate indistinguishable adversarial samples with high adversariality. 
MagicGAN~\cite{chen2022magicgan} devises a multiagent discriminator capable of adapting to the decision boundaries of diverse target models. This provides a more varied gradient information spectrum, facilitating the creation of adversarial perturbations. MTAA~\cite{chen2023learning} employs a representation that preserves relationships to study patterns that are adversarial. APAA~\cite{yuan2024adaptive} directly utilizes the precise gradient direction with a scale factor to generate adversarial perturbations, thereby enhancing the attack success rate of adversarial samples, even with lesser perturbations.

However, none of these methods take into account the effect of the lack of diversity of attack gradients on the adversarial sample's transferability. Therefore, the adversarial samples generated by these methods are heavily dependent on the specific decision boundaries of the surrogate model, thus reducing the attack effect on the target model.

\subsection{Adversarial Defense}

The goal of adversarial defense is to enhance the resilience of the target model when adversarial samples serve as inputs. Defense approaches can primarily be classified into three types: adversarial detection, adversarial purification, and adversarial training. Adversarial detection techniques~\cite{wang2019adversarial,meng2017magnet,liang2018detecting,zheng2018robust}, in most instances, obviate the need for model retraining, thereby substantially reducing the complexity of the undertaking. The detection of adversarial instances hinges on the study of the characteristics of adversarial perturbations and their statistical deviations from normal instances. This approach enables the differentiation of adversarial instances during the operation of DNN models, thereby safeguarding them from potential adversarial attacks. Adversarial purification techniques~\cite{liao2018defense,liu2019feature,jia2019comdefend}, typically aim to eliminate noise from adversarial samples before they are input into the classifier. Adversarial training techniques~\cite{madry2017towards,tramer2017ensemble,pang2020boosting}, on the other hand, utilize adversarial samples as additional training data to boost the model's robustness.