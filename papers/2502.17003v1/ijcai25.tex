%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow} % Required for multirows
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{float}
\usepackage{subfig}
\usepackage{overpic}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation}


% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Wenyuan Wu$^1$
\and
Zheng Liu$^2$\and
Yong Chen$^3$\and
Chao Su$^1$\and
Dezhong Peng$^1$\And
Xu Wang$^1$\thanks{Corresponding author.}\\
\affiliations
$^1$College of Computer Science, Sichuan University, China\\
$^2$Sichuan Newstrong UHD Video Technology Company Ltd., China\\
$^3$Institute of Optics and Electronics, Chinese Academy of Sciences, China\\
\emails
wuwenyuan97@gmail.com,
liuzheng@uptcsc.com,
cy1415926@gmail.com,
suchao.ml@gmail.com,
pengdz@scu.edu.cn,
wangxu.scu@gmail.com
}
% \fi

\begin{document}

\maketitle

\begin{abstract}
% In recent years, with the rapid advancement of deep neural networks, there has been increasing attention on the security of deep learning models. Although existing adversarial attack algorithms have shown success in enhancing adversarial transferability, their performance can still be improved, as they fail to account for the differences between the target and source models. In this paper, we propose a simple and effective inverse knowledge distillation (IKD) method to enhance adversarial transferability. Specifically, when using gradient-based attack methods to generate adversarial samples, IKD is integrated into the modelâ€™s loss function to effectively increase the diversity of attack gradients, thereby preventing adversarial samples from being limited to a single model architecture. This significantly improves the transferability of adversarial samples. Additionally, our approach can be seamlessly integrated with existing gradient-based attacks to further boost their performance. Extensive experiments on the ImageNet dataset validate the effectiveness of our method.
In recent years, the rapid development of deep neural networks has brought increased attention to the security and robustness of these models.  While existing adversarial attack algorithms have demonstrated success in improving adversarial transferability, their performance remains suboptimal due to a lack of consideration for the discrepancies between target and source models.  To address this limitation, we propose a novel method, Inverse Knowledge Distillation (IKD), designed to enhance adversarial transferability effectively. IKD introduces a distillation-inspired loss function that seamlessly integrates with gradient-based attack methods, promoting diversity in attack gradients and mitigating overfitting to specific model architectures. By diversifying gradients, IKD enables the generation of adversarial samples with superior generalization capabilities across different models, significantly enhancing their effectiveness in black-box attack scenarios. Extensive experiments on the ImageNet dataset validate the effectiveness of our approach, demonstrating substantial improvements in the transferability and attack success rates of adversarial samples across a wide range of models.
\end{abstract}

\section{Introduction}
The rapid advancements and significant achievements in deep learning over the past few years have led to an increased focus on its security aspects. A key security concern is the susceptibility of these systems to minimal, nearly undetectable adversarial noise~\cite{szegedy2013intriguing}. This vulnerability suggests a high risk of deliberate attacks, particularly in technologies such as facial recognition and autonomous driving. Although it is crucial to research methods to bolster deep learning models against adversarial attacks, it is of equal importance to investigate strategies for launching attacks on these models.

Current attack strategies create an adversarial sample by incorporating elaborate adversarial perturbations into the input. These perturbations are usually generated by generating networks~\cite{zhao2018generating,joshi2019semantic,qiu2020semanticadv,xiao2021improving} or gradient-based optimization techniques~\cite{goodfellow2014explaining,madry2017towards,dong2018boosting,xie2019improving,dong2019evading,lin2019nesterov}. The latter, the gradient-based approach, is the mainstream. The central idea behind these methods is to generate adversarial perturbations through gradients, which are calculated by maximizing the loss function associated with the target task.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{explain.pdf}
    \caption{\textbf{The distinction between Inverse Knowledge Distillation (IKD) method and existing gradient-based attack approaches.} The blue arrow indicates the gradient direction of traditional gradient-based attack methods, which are heavily reliant on the decision boundary of the surrogate model (i.e., closely correlated with the parameters of the surrogate model). As a result, these methods may fail to effectively attack the target model, as they often cannot generate adversarial samples capable of crossing the decision boundary of the target model, represented by the orange curve. In contrast, by optimizing the attack gradient direction (indicated by the orange dashed line), our IKD method not only ensures that the adversarial sample successfully crosses the surrogate model's decision boundary, but also increases the likelihood of the adversarial sample overcoming the target model's decision boundary. Consequently, our IKD method demonstrates a higher success rate in transfer-based attacks.}
    \label{fig:toy}
\end{figure}

% Existing attack methods show high effectiveness in white-box deep learning models, but in black-box models, the difficulty of attack is greatly increased because of the inability to use the information related to the internal structure of the model, which becomes a huge challenge. To deal with this challenge, a series of studies have been carried out to try~\cite{li2022approximated,sun2023multi,tang2023adversarial,tang2024adversarial,tang2023natural,zhao2023minimizing}, these studies can be generally summarized into two categories: query-based attack and transfer-based attack. Query-based attack methods generate adversarial samples by sending a large number of queries to the black-box model. However, such methods are usually accompanied by high computational cost and long time overhead. In contrast, transfer-based attacks take full advantage of the transferability of adversarial samples by generating adversarial samples on the surrogate model and migrating them to the black-box model for attack. This approach is not only more efficient but also relatively more feasible in real-world applications. Therefore, this paper mainly focuses on transfer-based attacks and aims to improve the transferability of adversarial attacks to enhance their attack effect in the black-box model.
Existing attack methods demonstrate high efficiency in white-box scenarios but face significant challenges in black-box models, where internal model information is inaccessible. This limitation significantly increases the difficulty of attacks. To address this issue, research efforts are primarily categorized into two approaches: query-based attacks and transfer-based attacks. Query-based attacks generate adversarial samples through extensive queries but incur high computational costs and time overhead. In contrast, transfer-based attacks leverage the transferability of adversarial samples, generating them on surrogate models for application to black-box models, offering greater efficiency and practicality. Consequently, this paper focuses on transfer-based attacks, aiming to enhance the transferability of adversarial samples and improve their effectiveness in black-box attack scenarios.

At present, transfer-based attack methods have covered a variety of technologies, including input transformation-based attacks~\cite{dong2019evading,liang2023styless,wang2021admix,xie2019improving}, advanced gradient attacks~\cite{dong2018boosting,li2023adaptive,lin2019nesterov,wang2021enhancing}, ensemble attacks~\cite{qian2023lea2,tramer2017ensemble,xiong2022stochastic}, feature-based attacks~\cite{ganeshan2019fda,wang2023improving,zhang2022improving}, and so on. Although these methods can improve the transferability of attacks to some extent, they usually come with high computational costs.
% For example, ensemble attacks require loading multiple models, which consumes significant memory resources. In contrast, advanced gradient attacks require additional computational overhead to use the neighbor gradient to reorient the attack and thus correct the update of the attack gradient. 
% In addition, these methods may face high time and resource consumption in practical applications, which makes their application in specific scenarios limited. Among them, the attack based on input transformation, as a simple and effective method to enhance adversarial transferability, has received wide attention. By transforming or perturbing the input data, this method enhances the adversarial sample's transferability and improves the attack's success rate. However, this method does not fully consider the differences between the surrogate model and the target model, and the generated adversarial perturbations often show better results on the surrogate model, but may not maintain the same effectiveness on the target model. Specifically, the attack based on input transformation makes it easy to overfit the characteristics of the surrogate model, which limits its transfer ability to the target model and reduces the overall effect and success rate of the attack.
Moreover, existing methods often fail to adequately account for the differences between surrogate models and target models. As a result, the generated adversarial perturbations tend to perform better on surrogate models but may not retain the same effectiveness when applied to target models. Specifically, these methods suffer from a lack of diversity in attack gradients, relying heavily on a single fixed direction to generate adversarial samples while neglecting other potential directions. This fixed direction is typically determined by the computations of the surrogate model, causing the generated adversarial samples to overfit the surrogate model and limiting their effectiveness in attacking the target model.

% In order to solve the problem of single directivity of attack gradient and overfitting of the surrogate model with adversarial samples, a simple and effective inverse knowledge distillation (IKD) attack method is proposed. By adding regularization term based on knowledge distillation into the loss function of gradient attack, the method aims to increase the diversity of attack gradient, overcome the problem that adversarial samples only fit the surrogate model, and improve the transferability of adversarial samples. Specifically, the inverse knowledge distillation (IKD) method enhances the generalization ability of adversarial perturbations by introducing a distillation process to adjust the gradient during optimization, and improves the transfer performance of adversarial perturbations with little additional computational cost.

% To address this challenge, we propose Inverse Knowledge Distillation (IKD), a simple yet effective method designed to mitigate overfitting by enhancing gradient diversity. 
% By incorporating the inverse knowledge distillation process into the loss calculation of gradient-based attack methods, our approach not only targets the specified label but also optimizes the difference in output feature distributions between adversarial and benign samples on the surrogate model. This process introduces richer gradient information, alleviating the limitations of fixed gradient directions and enhancing gradient diversity. Furthermore, maximizing the distribution difference reduces reliance on the specific decision boundary of the surrogate model. This forces the optimization process to prioritize more general perturbations, effectively preventing adversarial samples from overfitting to the surrogate model and significantly improving their transferability, as shown in~\Cref{fig:toy}.
To address this challenge, we propose Inverse Knowledge Distillation (IKD), a novel and effective method aimed at mitigating overfitting by enhancing gradient diversity. IKD integrates a distillation-inspired mechanism into the loss computation of gradient-based attack methods, optimizing not only the alignment with the specified label but also the divergence in output feature distributions between adversarial and benign samples on the surrogate model. This approach introduces richer gradient information, breaking the constraints of fixed gradient directions and significantly enhancing gradient diversity. As illustrated in ~\Cref{fig:toy}, by maximizing the difference in feature distributions, IKD reduces dependence on the specific decision boundaries of surrogate models, compelling the optimization process to prioritize more generalized perturbations. This effectively prevents adversarial samples from overfitting to surrogate models and substantially improves their transferability.
% Firstly, the effects of different distillation methods on the performance of transfer-based attack are analyzed. The experimental results show that the KL divergence-based distillation method can effectively improve the attack performance, while the common mean square error (MSE) and cross-entropy (CE) do not show the same effect. The KL divergence-based distillation method can inverse knowledge distillation (IKD) on the softmax feature and promote the diversity of gradients in the optimization process, which effectively avoids the structure of adversarial samples only applicable to a specific model, thus enhancing the transferability of adversarial samples between different models. In this way, the inverse knowledge distillation (IKD) method effectively solves the limitations of the traditional attack method in the transfer process. Finally, a new inverse distillation attack is proposed by combining inverse distillation regularization with gradient-based attack methods. Through a large number of experiments, the results show that the proposed method has significant advantages in improving the attack success rate and the transferability of adversarial samples.

The main contributions of our work can be summarized as follows:
\begin{itemize}
    \item We propose a simple and effective adversarial attack method: Inverse Knowledge Distillation (IKD) attack. IKD can effectively reduce the overfitting problem and enhance the adversarial sample's transferability. Our Inverse Knowledge distillation (IKD) method is compatible and easy to integrate with gradient-based attack methods to improve attack effectiveness.
    % We propose a simple and effective adversarial attack method: inverse knowledge distillation (IKD) attack. Introducing
    % % regularization terms based on 
    % inverse knowledge distillation can effectively reduce the overfitting problem and enhance the adversarial sample's transferability. Our inverse knowledge distillation (IKD) method has good compatibility and can be used in combination with gradient-based attack methods, and it is easy to integrate with existing gradient-based attack methods to improve the attack effect.
    \item We reveal that different distillation methods have significant differences on the transferability of adversarial perturbations. Compared with mean square error (MSE) and cross-entropy (CE) loss, the KL divergence based distillation method shows the best effect in improving the transferability of the adversarial sample.
    \item We conducted a large number of experiments on the ImageNet dataset to verify the validity of the proposed IKD method. The experimental results show that the proposed method significantly improves the success rate of almost all attack methods, which proves the potential and advantage of IKD in practice.
\end{itemize}

\section{Related Works}
\subsection{Adversarial Attack}

The primary attack techniques comprise of the generative-based~\cite{zhao2018generating,song2018constructing,joshi2019semantic,qiu2020semanticadv,xiao2021improving}, and gradient-based strategies~\cite{goodfellow2014explaining,madry2017towards,dong2018boosting,xie2019improving,dong2019evading,lin2019nesterov}. FGSM (Fast Gradient Sign Method)~\cite{goodfellow2014explaining} is a simple and fast method for generating adversarial examples.
% Before delving into the specifics of these methods, we'll first present some notations that will be utilized subsequently. Let's denote $x$ as the original image and $y$ as its corresponding class label. The cross-entropy loss function is represented by $L(x, y)$. The adversarial sample generated is denoted as $x^{adv}$. The total perturbation budget and the budget for each iterative step are denoted by $\epsilon$ and $\alpha$ respectively. The notation $\Pi_{x,\epsilon}$ signifies the clipping of the generated adversarial samples within the $\epsilon$-neighborhood of the original image based on the $L_\infty$ norm.

% \textbf{Fast Gradient Sign Method.} FGSM~\cite{goodfellow2014explaining} represents a single-step white-box attack methodology, which directly employs the gradient of the loss function for the creation of the adversarial sample:
% \begin{equation}
%     x^{adv} = x + \epsilon \cdot {\rm sign}(\nabla_{x}L(f(x), y)).
% \end{equation}

% \textbf{Iterative Fast Gradient Sign Method.} IFGSM~\cite{kurakin2016adversarial} is an extension of FGSM, which uses the iterative method to improve the attack success rate of adversarial samples:
% \begin{equation}
%     x_{t+1}^{adv} = \Pi_{x,\epsilon}(x_{t}^{adv} + \alpha \cdot {\rm sign}(\nabla_{x}L(f(x), y)),
% \end{equation}
% where $x_{0}^{adv}$ is equivalent to $x$, and the subscript $t$ denotes the iteration index.

% \textbf{Momentum Iterative Fast Gradient Sign Method.} MIFGSM~\cite{dong2018boosting} introduces a momentum term that aggregates the gradient from preceding steps, thus securing more consistent update directions. This significantly enhances the transferability of the adversarial samples generated:
% \begin{equation}
%     g_{t+1} = \mu \cdot g_{t} + \frac{\nabla_{x}L(f(x_{t}^{adv}), y}{\Vert \nabla_{x}L(f(x_{t}^{adv}), y \Vert_{1}},    
% \end{equation}
% \begin{equation}
%     x_{t+1}^{adv} = \Pi_{x,\epsilon}(x_{t}^{adv} + \alpha \cdot {\rm sign}(g_{t+1})),    
% \end{equation}
% where $g_{t}$ signifies the momentum component of the gradient in the $t$-th iteration, while $\mu$ stands for a decay factor.

Since then, various methods have been proposed to enhance the attacking capability of adversarial samples. In DIM~\cite{xie2019improving}, randomization operations of random resizing and padding of the original image were introduced. TIM~\cite{dong2019evading} proposes a translation-invariant attack method by convolving the gradient with a Gaussian kernel, further augmenting the attacking capability of the samples. Inspired by Nesterov's accelerated gradient~\cite{nesterov1983method}, SIM~\cite{lin2019nesterov} modified the accumulation of gradients to effectively predict and enhance the adversariality of the samples. VT~\cite{wang2021enhancing} considered the gradient variance of the previous iteration to adjust the current gradient, thereby stabilizing the update direction and avoiding poor local optima. EMI~\cite{wang2021boosting} accumulated the gradients of data points sampled in the direction of the previous iteration's gradient to find a more stable gradient direction.
% Recognizing the limitations of the basic sign structure, AIFGTM~\cite{zou2022making} proposes an ADAM Iterative Fast Gradient Tanh Method to generate indistinguishable adversarial samples with high adversariality. 
MagicGAN~\cite{chen2022magicgan} devises a multiagent discriminator capable of adapting to the decision boundaries of diverse target models. This provides a more varied gradient information spectrum, facilitating the creation of adversarial perturbations. MTAA~\cite{chen2023learning} employs a representation that preserves relationships to study patterns that are adversarial. APAA~\cite{yuan2024adaptive} directly utilizes the precise gradient direction with a scale factor to generate adversarial perturbations, thereby enhancing the attack success rate of adversarial samples, even with lesser perturbations.

However, none of these methods take into account the effect of the lack of diversity of attack gradients on the adversarial sample's transferability. Therefore, the adversarial samples generated by these methods are heavily dependent on the specific decision boundaries of the surrogate model, thus reducing the attack effect on the target model.

\subsection{Adversarial Defense}

The goal of adversarial defense is to enhance the resilience of the target model when adversarial samples serve as inputs. Defense approaches can primarily be classified into three types: adversarial detection, adversarial purification, and adversarial training. Adversarial detection techniques~\cite{wang2019adversarial,meng2017magnet,liang2018detecting,zheng2018robust}, in most instances, obviate the need for model retraining, thereby substantially reducing the complexity of the undertaking. The detection of adversarial instances hinges on the study of the characteristics of adversarial perturbations and their statistical deviations from normal instances. This approach enables the differentiation of adversarial instances during the operation of DNN models, thereby safeguarding them from potential adversarial attacks. Adversarial purification techniques~\cite{liao2018defense,liu2019feature,jia2019comdefend}, typically aim to eliminate noise from adversarial samples before they are input into the classifier. Adversarial training techniques~\cite{madry2017towards,tramer2017ensemble,pang2020boosting}, on the other hand, utilize adversarial samples as additional training data to boost the model's robustness.

\section{Method}
% In this section, we first define the problem in~\Cref{section:Problem Definition}. Next, in~\Cref{section:Inverse Knowledge Distillation Attack}, we provide a detailed explanation of the proposed Inverse Knowledge Distillation (IKD) attack. In~\Cref{section:Enhanced Gradient Diversity}, we discuss how IKD enhances gradient diversity. Finally, in~\Cref{section:Attack Algorithm}, we present an attack algorithm that integrates the IKD method.
% In this section, we first outline the problem definition in~\Cref{section:Problem Definition}. We then provide a detailed explanation of the proposed inverse knowledge distillation attack in~\Cref{section:Inverse Knowledge Distillation Attack}. Finally, in~\Cref{section:Attack Algorithm}, we present the attack algorithm that integrates the inverse knowledge distillation method.

\subsection{Problem Definition}\label{section:Problem Definition}
The task of adversarial attack involves making subtle modifications to the original image by introducing imperceptible noise, to cause the target model to misclassify the resulting adversarial samples. For instance, a gradient-based approach generates adversarial samples by maximizing the cross-entropy loss function, and its optimization objective can be expressed as:
\begin{equation}\label{equation:optimal advsample}
    \begin{aligned}
        &\mathop{\arg\max}\limits_{x^{adv}}\mathcal{L}(f_\theta(x^{adv}),y),&s.t.\Vert x^{adv}-x\Vert_p\leq\epsilon,
    \end{aligned}
\end{equation}
where $x^{adv}$ denote the adversarial sample, $x^{adv}=x+\delta$, with $x$ representing the benign sample and $\delta$ the adversarial perturbation. $y$ represents the ground truth label. $f_\theta$ is a well-trained classification model parameterized by $\theta$, and $\mathcal{L}(\cdot,\cdot)$ denotes the cross-entropy loss in the classification task. $\Vert\cdot\Vert_p$ represents the $l_p$-norm. In this study, we continue the previous work~\cite{dong2019evading,li2023adaptive,xie2019improving} and use the $l_\infty$ norm to measure the size of the adversarial perturbation. In this framework, the maximum allowable correction of the perturbation $\delta$ is controlled by $\epsilon$, and the perturbation satisfies a specific constraint, that is, it is in the $l_\infty$ sphere with radius $\epsilon$ centered on $x$. Specifically, the disturbance $\delta$ must satisfy $\Vert\delta\Vert_p\leq\epsilon$, which ensures that the adversarial perturbation does not deviate too far from the original sample $x$, thus ensuring that the generated adversarial sample is reasonable in the input space, while maintaining the effectiveness of the attack.

However, the problem in~\Cref{equation:optimal advsample} can only be solved if the classification model is directly accessible, i.e., in a white-box setting. In a black-box scenario, where the target model $f_\theta$ is not directly accessible, this method cannot be directly applied for attacks. In such cases, directly solving the problem becomes infeasible. To address this limitation, a potential solution is to generate adversarial samples on a surrogate model $f_\varphi$ and exploit their transferability to attack the inaccessible target model $f_\theta$. Given the inherent differences between the surrogate and target models, improving the transferability of adversarial samples generated by the surrogate model $f_\varphi$ is crucial. This is because the transferability of adversarial samples determines their effectiveness across different models. In black-box attacks, for instance, highly transferable adversarial samples can successfully bypass the defenses of the target model. Therefore, the primary focus of this paper is to enhance the transferability of adversarial samples generated by surrogate models.
  
% \subsection{Knowledge Distillation}\label{section:Knowledge Distillation}
% Knowledge distillation (KD) is a model compression technique that transfers the knowledge of a large, complex model (referred to as the "teacher model") to a smaller, simpler model (referred to as the "student model"), thereby reducing the computational complexity of the student model while maintaining performance that is close to that of the teacher model. This method guides the training of the student model by learning from either the output probability distribution or the intermediate representations of the teacher model. The knowledge distillation process can be described by the following steps:

% Let $T$ and $S$ denote the teacher and student models, respectively. The teacher model $T$ is a large, complex network that has been fully trained and achieves high precision, while the student model $S$ is a smaller, simpler network designed to approximate the performance of the teacher model through knowledge distillation. The goal of knowledge distillation is for the student model $S$ to learn the output of the teacher model $T$ (referred to as "soft labels"), rather than simply learning the hard labels (i.e., category labels). The predicted output of the teacher model $T$ on input data $x$ is $\hat{y}_T=T(x)$, representing the output probability distribution of the model. Similarly, the output of the student model is $\hat{y}_S=S(x)$, which corresponds to the output probability distribution of the student model. We define a "distillation loss" to quantify the difference between the output of the student model and that of the teacher model. Given the output $T(x)$ of the teacher model and $S(x)$ of the student model, the soft label loss, using Kullback-Leibler (KL) divergence as an example, can be expressed as:
% \begin{equation}
%     \mathcal{L}_{soft}=KL(T(x)\Vert S(x))=\sum_iT(x)_i\log(\frac{T(x)_i}{S(x)_i}),    
% \end{equation}
% where $T(x)_i$ and $S(x)_i$ represent the probabilities for class $i$ output by the teacher and student models, respectively. The total loss function typically combines the distillation loss with the traditional supervision loss (also known as the hard label loss). Hard label losses, such as cross-entropy loss, are used to quantify the difference between the predictions of the student model and the true labels:
% \begin{equation}
%     \mathcal{L}_{hard}=-\sum_iy_i\log S(x)_i,    
% \end{equation}
% where $y_i$ is the ground-truth label for the training data.

% The final total loss function is:
% \begin{equation}
%     \mathcal{L}_{total}=\beta\mathcal{L}_{hard}+(1-\beta)\mathcal{L}_{soft},    
% \end{equation}
% where $\beta$ is a hyperparameter that balances the two losses. In general, $\alpha$ serves as a weight that controls the relative importance of the hard label loss and the soft label loss.

% During the training process, the student model $S$ updates its parameters by minimizing the total loss function $\mathcal{L}_{total}$, thereby gradually acquiring the knowledge from the teacher model $T$. This enables the student model to approximate the performance of the teacher model while maintaining a smaller number of parameters.

\subsection{Inverse Knowledge Distillation Attack}\label{section:Inverse Knowledge Distillation Attack}
Knowledge distillation (KD) is a model compression technique that transfers the knowledge of a large, complex model (referred to as the ``teacher model") to a smaller, simpler model (referred to as the ``student model"), thereby reducing the computational complexity of the student model while maintaining performance that is close to that of the teacher model~\cite{hinton2015distilling}. This method guides the training of the student model by learning from either the output probability distribution or the intermediate representations of the teacher model.

Inspired by knowledge distillation techniques, we propose the Inverse Knowledge Distillation (IKD) method. Unlike knowledge distillation, our IKD attack method does not involve the concepts of teacher and student models, nor does it require the student model to learn from the teacher model. Instead, our approach aims to maximize the disparity between the predictive output $f_\varphi(x)$ of the surrogate model $f_\varphi$ on benign input data $x$ and the predictive output $f_\varphi(x^{adv})$ of the surrogate model on adversarial input data $x^{adv}$, without compromising the performance of the white-box adversarial attack. In this case, the optimization objective in~\Cref{equation:optimal advsample} becomes the following:
\begin{equation}\label{equation:IKD optimal}
    \mathop{\arg\max}\limits_{x^{adv}}(\mathcal{L}_{hard}(f_\varphi(x^{adv}),y)+\gamma\cdot\mathcal{L}_{soft}(f_\varphi(x^{adv}),f_\varphi(x))),    
\end{equation}
where $\gamma$ is the distillation weight. The objective is to minimize the amount of knowledge contained in the adversarial sample $x^{adv}$ that is related to the surrogate model $f_\varphi$, in order to avoid overfitting of the adversarial sample $x^{adv}$ to the surrogate model $f_\varphi$ and to enhance the transferability of the adversarial sample. 

% The soft label loss in~\Cref{equation:IKD optimal}, $\mathcal{L}_{soft}(f_\varphi(x^{adv}),f_\varphi(x))$, is defined as the Kullback-Leibler (KL) Divergence between the softmax output distributions of the benign sample $x$ and the adversarial example $x^{adv}$ in the surrogate model $f_\varphi$:
The soft label loss $\mathcal{L}_{soft}$, defined in~\Cref{equation:IKD optimal}, can take various common forms, such as Kullback-Leibler (KL) divergence, mean square error (MSE), and cross-entropy (CE). In IKD, we select KL divergence as the soft label loss, formulated as:
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{soft}(f_\varphi(x^{adv}),f_\varphi(x))&=KL(f_\varphi(x)\Vert f_\varphi(x^{adv})) \\
        &=\sum_if_\varphi(x)(i)\log\frac{f_\varphi(x)(i)}{f_\varphi(x^{adv})(i)},
    \end{aligned}
\end{equation}
The KL divergence measures how much information is lost when $f_\varphi(x^{adv})$ is used to approximate $f_\varphi(x)$. It provides unique advantages in quantifying the difference between the probability distributions of benign samples $f_\varphi(x)$ and adversarial samples $f_\varphi(x^{adv})$.

% There are several reasons for selecting KL divergence over MSE or CE. 
The primary reasons for selecting KL divergence over MSE or CE as the soft label loss are as follows. First, the gradient of KL divergence with respect to $f_\varphi(x^{adv})$ is given by:
\begin{equation}
    \frac{\partial KL(f_\varphi(x)\Vert f_\varphi(x^{adv}))}{\partial f_\varphi(x^{adv})(i)}=-\frac{f_\varphi(x)(i)}{f_\varphi(x^{adv})(i)},
\end{equation}
which exhibits two key characteristics: asymmetric sensitivity and probabilistic emphasis. Asymmetric sensitivity refers to the fact that KL divergence penalizes larger deviations of $f_\varphi(x^{adv})$ from $f_\varphi(x)$ more heavily. This asymmetry is crucial for capturing directional differences in the probability distributions, ensuring that adversarial samples move away from the benign sample distribution and, consequently, the decision boundary of the surrogate model. Probabilistic emphasis, on the other hand, means that the gradient is weighted by $f_\varphi(x)$, prioritizing high-probability regions of the benign distribution. This alignment enhances the goal of modifying the adversarial sampleâ€™s predictions.

Second, the gradient of MSE with respect to $f_\varphi(x^{adv})$ is given by:
\begin{equation}
    \frac{\partial MSE(f_\varphi(x)\Vert f_\varphi(x^{adv}))}{\partial f_\varphi(x^{adv})(i)}=\frac{2}{N}(f_\varphi(x^{adv})-f_\varphi(x)).
\end{equation}
Compared to KL divergence, MSE has two main drawbacks: symmetric penalization and gradient uniformity. Symmetric penalization refers to the fact that MSE treats overestimation and underestimation of probabilities symmetrically, which is suboptimal for adversarial attacks. Gradient uniformity means that MSE does not account for the relative importance of the probability $f_\varphi(x^{adv})$. As a result, MSE applies uniform penalties across all categories, diminishing its effectiveness in optimizing the high-probability regions of the target distribution. Consequently, MSE lacks the probabilistic emphasis and asymmetric sensitivity needed for generating effective adversarial attacks.

Finally, the gradient of Cross-Entropy (CE) with respect to $f_\varphi(x^{adv})$ is given by:
\begin{equation}
    \frac{\partial CE(f_\varphi(x)\Vert f_\varphi(x^{adv}))}{\partial f_\varphi(x^{adv})(i)}=-\frac{f_\varphi(x)(i)}{f_\varphi(x^{adv})(i)}.
\end{equation}
Although both CE and KL divergence share the same gradient structure, they differ in their original forms. In CE, the rightmost logarithmic term is $\log f_\varphi(x^{adv})$, whereas in KL divergence, it is $\log\frac{f_\varphi(x)}{f_\varphi(x^{adv})}$. This difference introduces two key disadvantages for CE. First, CE cannot capture the relationship between $f_\varphi(x)$ and $f_\varphi(x^{adv})$ when $f_\varphi(x)$ is close to zero. In such cases, CE fails to fully account for the deviation of $f_\varphi(x^{adv})$, potentially leading to suboptimal gradient updates. Second, CE only focuses on the direction from $f_\varphi(x^{adv})$ to $f_\varphi(x)$. In contrast, the mutual penalization in KL divergence ensures that both $f_\varphi(x)$ and $f_\varphi(x^{adv})$ are pushed away from each other in a balanced manner, a property that CE lacks.

Additionally, a series of experiments are conducted to validate the effectiveness of selecting KL divergence as the soft label loss, as detailed in~\Cref{section:Influence of soft label loss selection on IKD}.

\subsection{Enhanced Gradient Diversity}\label{section:Enhanced Gradient Diversity}
The introduction of the IKD enhances the diversity of gradients used during the adversarial example generation. Traditional gradient-based attack methods, such as FGSM and I-FGSM, often rely on the gradient of the loss with respect to the input, which can lead to similar perturbations along specific directions of the input space.

To address this issue, we introduce IKD into the loss calculation. This modification ensures that, in addition to the gradient of the classification loss with respect to the input, the optimization process now incorporates the gradient of the divergence between the distributions of benign and adversarial examples. This additional term encourages the attack to move not only along the steepest descent direction of the adversarial loss but also in directions that maximize the difference between the output distributions of the benign and adversarial samples. As a result, the adversarial example is pushed to explore a broader range of directions in the input space, leading to greater gradient diversity.

This increased gradient diversity prevents the optimization from getting stuck in local minima and allows the adversarial perturbations to better generalize across different models, which is essential for improving transferability.
 
\subsection{Attack Algorithm}\label{section:Attack Algorithm}
Similar to previous work~\cite{xie2019improving,long2022frequency}, the proposed method can be seamlessly integrated with any gradient-based attack technique. For instance, using MIFGSM as an example, we combine inverse knowledge distillation (IKD) with MIFGSM to introduce a new attack method, named MIFGSM-IKD. In this process, inverse distillation is incorporated into the gradient update rules of the MIFGSM attack to improve the transferability of adversarial samples. Specifically, by introducing IKD into the loss function, inverse distillation adjusts the gradient update direction, enabling the generated adversarial samples to not only effectively attack the surrogate model but also exhibit enhanced cross-model transferability. We propose a new adversarial sample update formula for the MIFGSM-IKD method, as follows:
\begin{equation}
    \begin{aligned}
        &x_0^{adv}=x,g_0=0, \\
        &\mathcal{L}_{total}=\mathcal{L}_{hard}(f_\varphi(x^{adv}),y)+\gamma\mathcal{L}_{soft}(f_\varphi(x^{adv}),f_\varphi(x)), \\
        &g_{t+1}=\mu\cdot g_t+\frac{\nabla_x(\mathcal{L}_{total})}{\Vert\nabla_x(\mathcal{L}_{total})\Vert_1}, \\
        &x_{t+1}^{adv}=x_t^{adv}+\alpha\cdot sign(g_{t+1}),
    \end{aligned}
\end{equation}
where $g_t$ and $x_t^{adv}$ represent the gradient and the adversarial sample generated at the $t$-th iteration, respectively. The parameter $\alpha$ denotes the attack step size and controls the magnitude of the update to the adversarial sample in each iteration. $\mu$ is an attenuation factor used to adjust the influence of historical information during gradient updating. $\Vert\cdot\Vert_1$ represents the $L_1$ norm. Thus, the IKD can be applied as an enhancement to the gradient-based attack method to improve its effectiveness in black-box settings.

\section{Experiments}
\renewcommand{\dblfloatpagefraction}{.9}
\begin{table*}[htbp]
    \centering
    \caption{Comparison results of various attack methods in terms of attack success rate ($\%$). The bold item indicates the best one. Item with $\star$ superscript is white-box attacks, and the others is black-box attacks. AVG column indicates the average attack success rate on black-box models.}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{c|l|cccccccccccc}
    \hline
        \multirow{2}{*}{Source Model} & \multirow{2}{*}{Method} & \multicolumn{12}{c}{Target Model} \\ \cline{3-14}
         &  & RN50 & DN121 & RNX50 & VGG19BN & IncRes-v2 & Inc-v3 & Inc-v4 & RN101 & RN152 & Inc-v3$adv$ &  IncRes-V2$adv,ens$ & AVG \\ \hline
        \multirow{8}{*}{RN50} & MIFGSM & 99.9$\star$ & 65.7  & 75.2  & 73.2  & 38.9  & 52.6  & 46.0  & 68.2  & 62.1  & 30.6  & 16.0  & 52.9  \\ 
        ~ & MIFGSM-IKD & 99.9$\star$ & \textbf{68.4}  & \textbf{79.4}  & \textbf{76.0}  & \textbf{39.9}  & \textbf{54.4}  & \textbf{49.6}  & \textbf{73.1}  & \textbf{67.6}  & \textbf{32.1}  & \textbf{16.3}  & \textbf{55.7}  \\ \cline{2-14}
        ~ & DIFGSM & \textbf{99.9$\star$} & 82.9  & 88.8  & 84.7  & 61.7  & 70.3  & 69.4  & 86.1  & 82.1  & 35.9  & 18.4  & 68.0  \\
        ~ & DIFGSM-IKD & 99.8$\star$ & \textbf{84.9}  & \textbf{90.6}  & \textbf{84.8}  & \textbf{62.9}  & \textbf{70.8}  & \textbf{69.6}  & \textbf{86.5}  & \textbf{83.3}  & \textbf{38.4}  & \textbf{18.9}  & \textbf{69.1}  \\ \cline{2-14}
        ~ & TIFGSM & 99.0$\star$ & 68.4  & 65.4  & \textbf{71.3}  & 51.1  & 63.9  & 60.3  & 55.6  & \textbf{49.8}  & 52.8  & 43.0  & 58.2  \\
        ~ & TIFGSM-IKD & \textbf{99.2$\star$} & \textbf{69.1}  & \textbf{67.1}  & 70.8  & \textbf{52.1}  & \textbf{64.5}  & \textbf{61.4}  & \textbf{57.9}  & 49.5  & 52.8  & \textbf{43.8}  & \textbf{58.9}  \\ \cline{2-14}
        ~ & NIFGSM & 99.9$\star$ & 71.6  & 80.0  & 76.5  & 43.4  & 55.2  & 49.2  & 75.9  & 70.7  & 31.6  & \textbf{16.0}  & 57.0  \\
        ~ & NIFGSM-IKD & \textbf{100.0$\star$} & \textbf{76.5}  & \textbf{85.0}  & \textbf{79.3}  & \textbf{44.6}  & \textbf{57.8}  & \textbf{52.5}  & \textbf{81.1}  & \textbf{74.7}  & \textbf{32.0}  & 15.4  & \textbf{59.9}  \\ \hline
        \multirow{8}{*}{DN121} & MIFGSM & 75.7  & 100.0$\star$ & 71.6  & 86.3  & 52.2  & 65.4  & 58.4  & 61.4  & 55.4  & 38.2  & 20.9  & 58.6  \\
        ~ & MIFGSM-IKD & \textbf{78.3}  & 100.0$\star$ & \textbf{76.6}  & \textbf{88.5}  & \textbf{55.7}  & \textbf{70.9}  & \textbf{64.1}  & \textbf{68.1}  & \textbf{62.8}  & \textbf{38.6}  & \textbf{21.0}  & \textbf{62.5}  \\ \cline{2-14}
        ~ & DIFGSM & 87.2  & 100.0$\star$ & 84.5  & 92.0  & 74.3  & \textbf{82.8}  & 80.2  & 77.8  & 72.5  & \textbf{48.5}  & 29.0  & 72.9  \\
        ~ & DIFGSM-IKD & \textbf{89.0}  & 100.0$\star$ & \textbf{88.5}  & \textbf{93.9}  & \textbf{77.3}  & 82.7  & \textbf{82.9}  & \textbf{81.9}  & \textbf{78.6}  & 48.3  & 29.0  & \textbf{75.2}  \\ \cline{2-14}
        ~ & TIFGSM & 58.4  & 99.7$\star$ & 52.8  & \textbf{82.0}  & 60.4  & \textbf{72.8}  & 66.2  & 40.7  & 34.1  & 64.5  & 54.4  & 58.6  \\
        ~ & TIFGSM-IKD & \textbf{60.5}  & \textbf{99.8$\star$} & \textbf{56.1}  & 81.7  & \textbf{62.1}  & 72.4  & \textbf{67.2}  & \textbf{41.3}  & \textbf{37.8}  & \textbf{65.7}  & \textbf{57.5}  & \textbf{60.2}  \\ \cline{2-14}
        ~ & NIFGSM & 79.0  & 100.0$\star$ & 77.9  & 86.1  & 55.3  & 68.2  & 62.1  & 67.3  & 60.4  & 39.0  & 20.4  & 61.6  \\
        ~ & NIFGSM-IKD & \textbf{82.3}  & 100.0$\star$ & \textbf{80.0}  & \textbf{89.4}  & \textbf{57.9}  & \textbf{70.}5  & \textbf{64.8}  & \textbf{71.9}  & \textbf{66.4}  & \textbf{39.6}  & \textbf{20.6}  & \textbf{64.3}  \\ \hline
        % \multirow{8}{*}{RNX50} & MIFGSM & 68.2  & 60.6  & 98.7$\star$ & \textbf{72.8}  & 31.3  & \textbf{46.1}  & 40.1  & 61.8  & 55.2  & 35.7  & \textbf{15.5}  & 48.7  \\
        % ~ & MIFGSM-IKD & \textbf{72.0}  & \textbf{63.3}  & \textbf{98.9$\star$} & 72.2  & \textbf{34.0}  & 45.1  & \textbf{41.6}  & \textbf{64.8}  & \textbf{57.9}  & \textbf{35.9}  & 14.9  & \textbf{50.2}  \\ \cline{2-14}
        % ~ & DIFGSM & 79.0  & 75.9  & 94.6$\star$ & 78.5  & 52.2  & 59.7  & \textbf{62.1}  & 75.8  & 71.9  & 36.3  & 16.1  & 60.8  \\
        % ~ & DIFGSM-IKD & \textbf{81.3}  & \textbf{76.3}  & \textbf{96.1$\star$} & \textbf{81.9}  & \textbf{55.1}  & \textbf{60.2}  & 62.0  & \textbf{77.2}  & \textbf{73.1}  & \textbf{37.4}  & \textbf{17.5}  & \textbf{62.2}  \\ \cline{2-14}
        % ~ & TIFGSM & 54.6  & \textbf{58.0}  & \textbf{85.6$\star$} & 67.5  & 42.2  & 54.6  & 52.1  & \textbf{45.8}  & \textbf{41.2}  & \textbf{42.0}  & 33.1  & 49.1  \\
        % ~ & TIFGSM-IKD & 54.6  & 56.6  & 85.5$\star$ & \textbf{68.0}  & \textbf{42.6}  & \textbf{55.0}  & \textbf{52.2}  & 45.2  & 41.1  & 41.8  & \textbf{35.6}  & \textbf{49.3}  \\ \cline{2-14}
        % ~ & NIFGSM & 74.8  & 67.8  & \textbf{99.9$\star$} & 73.9  & 35.5  & 47.0  & \textbf{46.3}  & 67.4  & 61.0  & \textbf{37.6}  & \textbf{14.8}  & 52.6  \\
        % ~ & NIFGSM-IKD & \textbf{78.5}  & \textbf{70.2}  & 99.8$\star$ & \textbf{77.7}  & \textbf{37.1}  & \textbf{49.3}  & 44.2  & \textbf{69.6}  & \textbf{63.8}  & 36.0  & 13.6  & \textbf{54.0}  \\ \hline
        \multirow{8}{*}{VGG19BN} & MIFGSM & 47.9  & 58.9  & 43.4  & 100.0$\star$ & 30.1  & 46.8  & 42.9  & 33.3  & 28.9  & 26.6  & \textbf{15.2}  & 37.4  \\
        ~ & MIFGSM-IKD & \textbf{55.2}  & \textbf{63.8}  & \textbf{50.2}  & 100.0$\star$ & \textbf{34.3}  & \textbf{52.5}  & \textbf{49.0}  & \textbf{39.3}  & \textbf{36.4}  & \textbf{27.1}  & 14.9  & \textbf{42.3}  \\ \cline{2-14}
        ~ & DIFGSM & 63.3  & 75.0  & 58.3  & 100.0$\star$ & 46.7  & 64.5  & 61.2  & 46.1  & 43.1  & 32.7  & 18.6  & 51.0  \\
        ~ & DIFGSM-IKD & \textbf{71.1}  & \textbf{80.3}  & \textbf{67.9}  & 100.0$\star$ & \textbf{55.8}  & \textbf{69.4}  & \textbf{69.5}  & \textbf{57.4}  & \textbf{52.7}  & \textbf{35.4}  & 18.6  & \textbf{57.8}  \\ \cline{2-14}
        ~ & TIFGSM & 40.2  & 59.8  & 34.6  & 100.0$\star$ & 40.5  & 62.6  & 56.8  & 25.2  & 20.9  & 44.1  & 33.8  & 41.9  \\
        ~ & TIFGSM-IKD & \textbf{44.8}  & \textbf{66.2}  & \textbf{42.2}  & 100.0$\star$ & \textbf{48.2}  & \textbf{65.9}  & \textbf{61.6}  & \textbf{30.1}  & \textbf{26.3}  & \textbf{48.8}  & \textbf{36.5}  & \textbf{47.1}  \\ \cline{2-14}
        ~ & NIFGSM & 54.3  & 62.9  & 47.7  & 100.0$\star$ & 34.4  & 50.6  & 45.2  & 39.7  & 33.9  & 25.9  & 14.2  & 40.9  \\
        ~ & NIFGSM-IKD & \textbf{57.8}  & \textbf{65.8}  & \textbf{51.5}  & 100.0$\star$ & \textbf{36.4}  & \textbf{53.5}  & \textbf{50.0}  & \textbf{41.8}  & \textbf{37.0}  & \textbf{26.}3  & \textbf{14.7}  & \textbf{43.5}  \\ \hline
        \multirow{8}{*}{IncRes-v2} & MIFGSM & 54.8  & 64.4  & 55.2  & 78.6  & 98.9$\star$ & 74.1  & 70.9  & 46.4  & 44.2  & 44.8  & 28.0  & 56.1  \\
        ~ & MIFGSM-IKD & \textbf{58.1}  & \textbf{68.7}  & \textbf{57.5}  & \textbf{79.5}  & 98.9$\star$ & \textbf{77.6}  & \textbf{75.0}  & \textbf{51}.4  & \textbf{47.8}  & \textbf{46.3}  & \textbf{30.4}  & \textbf{59.2}  \\ \cline{2-14}
        ~ & DIFGSM & 64.8  & 75.2  & 65.4  & 82.5  & \textbf{98.6$\star$} & 84.5  & 83.9  & 58.9  & 54.6  & 54.7  & 39.0  & 66.4  \\
        ~ & DIFGSM-IKD & \textbf{69.7}  & \textbf{79.4}  & \textbf{69.3}  & \textbf{85.3}  & 98.3$\star$ & \textbf{85.7}  & \textbf{86.5}  & \textbf{64.3}  & \textbf{58.4}  & \textbf{58.1}  & \textbf{39.5}  & \textbf{69.6}  \\ \cline{2-14}
        ~ & TIFGSM & 40.9  & 59.1  & 39.4  & 68.0  & \textbf{97.7$\star$} & 74.3  & 70.1  & 31.5  & 27.4  & 67.6  & 63.9  & 54.2  \\
        ~ & TIFGSM-IKD & \textbf{45.0}  & \textbf{60.4}  & \textbf{40.9}  & \textbf{68.6}  & 97.6$\star$ & \textbf{76.1}  & \textbf{73.1}  & \textbf{35.6}  & \textbf{29.3}  & \textbf{70.8}  & \textbf{68.2}  & \textbf{56.8}  \\ \cline{2-14}
        ~ & NIFGSM & 55.8  & 65.9  & 56.2  & 78.1  & 99.3$\star$ & 74.1  & 70.5  & 48.6  & 45.5  & 43.3  & 26.6  & 56.5  \\
        ~ & NIFGSM-IKD & \textbf{60.5}  & \textbf{70.3}  & \textbf{59.7}  & \textbf{80.4}  & \textbf{99.4$\star$} & \textbf{76.6}  & \textbf{73.3}  & \textbf{52.4}  & \textbf{50.1}  & \textbf{46.9}  & \textbf{27.0}  & \textbf{59.7}  \\ \hline
        % \multirow{8}{*}{Inc-v3} & MIFGSM & 47.7  & \textbf{60.9}  & 44.7  & 72.3  & \textbf{58.3}  & 99.1$\star$ & 61.8  & 38.3  & 35.9  & 41.5  & 21.4  & 48.3  \\
        % ~ & MIFGSM-IKD & \textbf{48.6}  & 60.0  & \textbf{45.1}  & \textbf{72.4}  & 58.1  & \textbf{99.7$\star$} & \textbf{62.8}  & \textbf{38.6}  & \textbf{36.6}  & \textbf{41.8}  & \textbf{22.6}  & \textbf{48.7}  \\ \cline{2-14}
        % ~ & DIFGSM & 56.6  & 68.6  & 53.3  & 78.7  & 70.1  & 99.1$\star$ & 73.8  & 47.2  & 42.0  & 49.5  & 26.9  & 56.7  \\
        % ~ & DIFGSM-IKD & \textbf{57.8}  & \textbf{69.5}  & \textbf{56.1}  & \textbf{79.8}  & \textbf{74.9}  & \textbf{99.3$\star$} & \textbf{77.2}  & \textbf{50.9}  & \textbf{45.2}  & \textbf{54.0}  & \textbf{29.0}  & \textbf{59.4}  \\ \cline{2-14}
        % ~ & TIFGSM & 32.7  & 49.8  & 28.6  & 64.2  & 51.2  & \textbf{98.8$\star$} & 60.0  & 21.0  & 18.8  & 56.0  & 42.4  & 42.5  \\
        % ~ & TIFGSM-IKD & \textbf{33.9}  & \textbf{50.4}  & \textbf{30.9}  & \textbf{66.6}  & \textbf{55.9}  & 98.7$\star$ & \textbf{62.4}  & \textbf{23.0}  & \textbf{20.3}  & \textbf{60.3}  & \textbf{44.9}  & \textbf{44.9}  \\ \cline{2-14}
        % ~ & NIFGSM & 55.4  & 69.4  & 52.0  & 76.5  & 64.9  & 99.4$\star$ & 70.0  & 47.7  & 40.8  & 42.2  & 21.6  & 54.1  \\
        % ~ & NIFGSM-IKD & \textbf{57.6}  & \textbf{71.5}  & \textbf{56.3}  & \textbf{78.4}  & \textbf{69.5}  & \textbf{99.8$\star$} & \textbf{72.7}  & \textbf{48.1}  & \textbf{45.6}  & \textbf{44.3}  & \textbf{23.0}  & \textbf{56.7}  \\ \hline
        % \multirow{8}{*}{Inc-v4} & MIFGSM & 53.3  & 62.4  & 52.8  & 76.5  & 60.4  & 69.5  & 97.9$\star$ & 45.0  & 42.6  & 37.1  & \textbf{20.4}  & 52.0  \\
        % ~ & MIFGSM-IKD & \textbf{54.1}  & \textbf{64.2}  & \textbf{53.7}  & \textbf{78.9}  & \textbf{62.2}  & \textbf{71.8}  & \textbf{99.0$\star$} & \textbf{48.3}  & \textbf{44.7}  & \textbf{38.4}  & 20.1  & \textbf{53.6}  \\ \cline{2-14}
        % ~ & DIFGSM & 62.9  & 70.9  & 62.1  & 82.4  & 76.2  & 79.8  & 98.0$\star$ & 55.6  & 52.4  & 45.2  & 25.6  & 61.3  \\
        % ~ & DIFGSM-IKD & \textbf{66.2}  & \textbf{74.0}  & \textbf{66.7}  & \textbf{84.4}  & 76.2  & \textbf{81.6}  & \textbf{98.1$\star$} & \textbf{59.4}  & \textbf{55.7}  & \textbf{47.7}  & \textbf{26.7}  & \textbf{63.9}  \\ \cline{2-14}
        % ~ & TIFGSM & 39.3  & 51.4  & 37.2  & 67.7  & 55.9  & 67.5  & \textbf{97.7$\star$} & 27.8  & 24.1  & 53.1  & \textbf{44.5}  & 46.9  \\
        % ~ & TIFGSM-IKD & \textbf{40.0}  & \textbf{52.2}  & \textbf{38.0}  & \textbf{69.1}  & \textbf{57.8}  & \textbf{70.1}  & 97.1$\star$ & \textbf{28.9}  & \textbf{25.7}  & \textbf{55.7}  & 44.2  & \textbf{48.2}  \\ \cline{2-14}
        % ~ & NIFGSM & 58.5  & 66.4  & 58.3  & 82.5  & 61.6  & 72.3  & 98.9$\star$ & 49.6  & 45.3  & 37.4  & 18.9  & 55.1  \\
        % ~ & NIFGSM-IKD & \textbf{60.8}  & \textbf{69.1}  & \textbf{60.2}  & \textbf{82.6}  & \textbf{65.7}  & \textbf{74.8}  & \textbf{99.8$\star$} & \textbf{52.7}  & \textbf{48.9}  & \textbf{39.3}  & \textbf{20.7}  & \textbf{57.5}  \\ \hline
        \multirow{8}{*}{RN101} & MIFGSM & 72.5  & 66.3  & 70.4  & 74.8  & 36.9  & 49.3  & 44.0  & 97.4$\star$ & 70.1  & 29.4  & 14.5  & 52.8  \\
        ~ & MIFGSM-IKD & \textbf{75.3}  & \textbf{66.9}  & \textbf{72.4}  & \textbf{75.1}  & \textbf{37.3}  & \textbf{50.3}  & \textbf{44.4}  & \textbf{97.7$\star$} & \textbf{71.7}  & \textbf{30.3}  & \textbf{14.9}  & \textbf{53.9}  \\ \cline{2-14}
        ~ & DIFGSM & 77.7  & \textbf{75.4}  & 77.6  & \textbf{80.6}  & \textbf{56.6}  & 63.6  & 61.0  & 93.2$\star$ & 76.6  & \textbf{35.1}  & \textbf{17.9}  & 62.2  \\
        ~ & DIFGSM-IKD & \textbf{78.5}  & 75.0  & \textbf{80.1}  & 80.3  & 56.0  & \textbf{64.5}  & \textbf{61.7}  & \textbf{93.3$\star$} & \textbf{78.4}  & 32.3  & 17.7  & \textbf{62.5}  \\ \cline{2-14}
        ~ & TIFGSM & \textbf{53.6}  & \textbf{56.0}  & \textbf{54.0}  & \textbf{64.6}  & \textbf{44.6}  & 55.0  & 52.9  & 77.9$\star$ & 49.7  & 42.9  & \textbf{36.3}  & \textbf{51.0}  \\
        ~ & TIFGSM-IKD & 53.2  & 54.7  & 53.2  & 64.3  & 44.3  & \textbf{56.5}  & \textbf{53.5}  & \textbf{79.8$\star$} & \textbf{50.6}  & \textbf{43.5}  & 35.6  & 50.9  \\ \cline{2-14}
        ~ & NIFGSM & 79.7  & 72.2  & 77.0  & 78.2  & 37.9  & 51.5  & 45.6  & \textbf{99.2$\star$} & 74.1  & \textbf{30.1}  & 14.3  & 56.1  \\
        ~ & NIFGSM-IKD & \textbf{80.3}  & \textbf{72.8}  & \textbf{77.7}  & \textbf{79.2}  & \textbf{39.9}  & \textbf{52.4}  & \textbf{47.6}  & 99.0$\star$ & \textbf{75.1}  & 29.5  & \textbf{15.7}  & \textbf{57.0}  \\ \hline
        % \multirow{8}{*}{RN152} & MIFGSM & 69.6  & 62.9  & 66.3  & 74.1  & 35.9  & 48.7  & 42.2  & 68.8  & 96.2$\star$ & 26.2  & 14.8  & 51.0  \\
        % ~ & MIFGSM-IKD & \textbf{71.2}  & \textbf{63.5}  & \textbf{68.8}  & \textbf{74.8}  & \textbf{38.1}  & \textbf{49.7}  & \textbf{44.7}  & \textbf{70.3}  & \textbf{96.4$\star$} & \textbf{27.4}  & \textbf{14.9}  & \textbf{52.3}  \\ \cline{2-14}
        % ~ & DIFGSM & 75.2  & \textbf{72.3}  & 75.5  & \textbf{79.0}  & \textbf{55.4}  & \textbf{62.7}  & 60.5  & 78.0  & 91.2$\star$ & \textbf{33.7}  & 17.6  & 61.0  \\
        % ~ & DIFGSM-IKD & \textbf{76.3}  & 72.0  & \textbf{77.6}  & 78.0  & 55.2  & 62.5  & \textbf{60.8}  & \textbf{78.7}  & \textbf{91.8$\star$} & 32.9  & \textbf{17.8}  & \textbf{61.2}  \\ \cline{2-14}
        % ~ & TIFGSM & 49.4  & \textbf{52.4}  & 50.5  & \textbf{64.1}  & 42.1  & \textbf{54.5}  & 49.0  & 49.4  & 74.4$\star$ & \textbf{42.6}  & \textbf{36.8}  & 49.1  \\
        % ~ & TIFGSM-IKD & \textbf{50.8}  & 51.4  & \textbf{51.6}  & 62.8  & \textbf{42.2}  & 53.2  & \textbf{50.0}  & \textbf{51.0}  & \textbf{74.9$\star$} & 42.4  & 35.1  & 49.1  \\ \cline{2-14}
        % ~ & NIFGSM & 77.3  & 68.0  & 74.5  & 77.4  & 37.0  & \textbf{51.1}  & 44.3  & 76.4  & 99.1$\star$ & \textbf{27.4}  & \textbf{15.6}  & 54.9  \\
        % ~ & NIFGSM-IKD & \textbf{78.8}  & \textbf{70.7}  & \textbf{75.0}  & \textbf{79.5}  & \textbf{38.4}  & 51.0  & \textbf{45.6}  & \textbf{77.5}  & 99.1$\star$ & 26.6  & 15.1  & \textbf{55.8} \\ \hline
    \end{tabular}
    }
  \label{tab:ngmasr}%
\end{table*}

\begin{table*}[htbp]
  \centering
  \caption{Evaluation results of the advanced attacks combined with IKD in terms of attack success rate (\%). The bold item indicates the best one. Item with $\star$ superscript is white-box attacks, and the others is black-box attacks. AVG column indicates the average attack success rate on black-box models.}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{c|l|cccccccccccc}
        \hline
        \multirow{2}{*}{Source Model} & \multirow{2}{*}{Method} & \multicolumn{12}{c}{Target Model} \\
        \cline{3-14}
        & & RN50  & DN121 & RNX50 & VGG19BN & IncRes-v2 & Inc-v3 & Inc-v4 & RN101 & RN152 & Inc-v3$adv$ &  IncRes-V2$adv,ens$ & AVG \\
        \hline
        \multirow{6}{*}{RN50} & SINIFGSM & 99.9$\star$ & 81.7  & 85.0  & 84.5  & 53.2  & 67.0  & 59.3  & 80.1  & 75.4  & 37.6  & 17.1  & 64.1  \\
        & SINIFGSM-IKD & \textbf{100.0$\star$} & \textbf{88.5 } & \textbf{91.3 } & \textbf{88.7 } & \textbf{58.0 } & \textbf{69.9 } & \textbf{64.5 } & \textbf{87.9 } & \textbf{84.1 } & \textbf{38.2 } & \textbf{17.7 } & \textbf{68.9 } \\
        \cline{2-14}
        & VMIFGSM & 99.9$\star$ & 84.7  & 88.6  & \textbf{86.5 } & 64.8  & \textbf{73.6 } & \textbf{73.5 } & 85.5  & 84.9  & 43.8  & \textbf{24.2 } & 71.0  \\
        & VMIFGSM-IKD & 99.9$\star$ & \textbf{86.1 } & \textbf{89.3 } & 86.1  & \textbf{65.6 } & 72.6  & 72.6  & \textbf{87.1 } & \textbf{85.3 } & \textbf{44.7 } & 23.4  & \textbf{71.3 } \\
        \cline{2-14}
        & VNIFGSM & \textbf{99.9$\star$} & 86.2  & 89.2  & 87.0  & 68.7  & 74.1  & 73.1  & 87.2  & 86.0  & \textbf{45.0 } & 23.0  & 72.0  \\
        & VNIFGSM-IKD & 99.8$\star$ & \textbf{86.6 } & \textbf{91.1 } & \textbf{88.2 } & 68.7  & \textbf{75.3 } & \textbf{74.0 } & \textbf{88.8 } & \textbf{88.0 } & 44.3  & \textbf{24.2 } & \textbf{72.9 } \\
        \hline
        \multirow{6}{*}{DN121} & SINIFGSM & 81.2  & 100.0$\star$ & 77.0  & 92.7  & 63.0  & 77.7  & 70.6  & 69.8  & 64.6  & 47.8  & 27.8  & 67.2  \\
        & SINIFGSM-IKD & \textbf{87.3 } & 100.0$\star$ & \textbf{85.5 } & \textbf{94.2 } & \textbf{68.2 } & \textbf{79.5 } & \textbf{75.4 } & \textbf{77.0 } & \textbf{73.0 } & \textbf{49.2 } & \textbf{29.3 } & \textbf{71.9 } \\
        \cline{2-14}
        & VMIFGSM & 87.3  & 100.0$\star$ & 84.6  & 92.6  & 73.3  & \textbf{82.6 } & 79.4  & 79.5  & 76.7  & 49.7  & 34.0  & 74.0  \\
        & VMIFGSM-IKD & \textbf{88.7 } & 100.0$\star$ & \textbf{87.3 } & \textbf{93.8 } & \textbf{74.4 } & 82.4  & \textbf{81.7 } & \textbf{82.5 } & \textbf{78.9 } & \textbf{50.1 } & \textbf{34.9 } & \textbf{75.5 } \\
        \cline{2-14}
        & VNIFGSM & 87.7  & 100.0$\star$ & 86.2  & 93.5  & 74.1  & \textbf{83.7 } & 80.6  & 81.1  & 78.7  & 49.6  & 34.1  & 74.9  \\
        & VNIFGSM-IKD & \textbf{89.2 } & 100.0$\star$ & \textbf{88.0 } & \textbf{93.9 } & \textbf{75.0 } & 82.9  & \textbf{81.3 } & \textbf{82.1 } & \textbf{80.7 } & \textbf{49.8 } & \textbf{34.8 } & \textbf{75.8 } \\
        \hline
        % \multirow{6}{*}{RNX50} & SINIFGSM & 91.0  & \textbf{88.5 } & 99.9$\star$ & 86.8  & 61.2  & \textbf{70.0 } & \textbf{70.1 } & 87.5  & 82.9  & \textbf{39.3 } & \textbf{18.5 } & 69.6  \\
        % & SINIFGSM-IKD & \textbf{91.7 } & 87.6  & 99.9$\star$ & \textbf{87.0 } & \textbf{61.9 } & 69.8  & 68.9  & \textbf{87.9 } & \textbf{85.0 } & 38.2  & 18.3  & 69.6  \\
        % \cline{2-14}
        % & VMIFGSM & \textbf{81.5 } & 77.8  & 97.1$\star$ & \textbf{81.1 } & 59.2  & 66.3  & 66.9  & 77.9  & 75.3  & \textbf{40.0 } & \textbf{23.9 } & 65.0  \\
        % & VMIFGSM-IKD & 81.4  & \textbf{78.1 } & \textbf{98.1$\star$} & 80.7  & 59.2  & \textbf{67.2 } & \textbf{67.5 } & \textbf{78.8 } & \textbf{76.6 } & 39.8  & 23.0  & \textbf{65.2 } \\
        % \cline{2-14}
        % & VNIFGSM & \textbf{82.4 } & \textbf{79.8 } & 97.1$\star$ & \textbf{82.5 } & \textbf{61.9 } & 68.8  & \textbf{68.1 } & \textbf{78.8 } & \textbf{77.2 } & \textbf{41.9 } & \textbf{23.5 } & \textbf{66.5 } \\
        % & VNIFGSM-IKD & 82.0  & 79.5  & \textbf{98.0$\star$} & 80.9  & 60.6  & \textbf{70.3 } & 66.9  & 78.4  & 77.1  & 40.3  & 22.3  & 65.8  \\
        % \hline
        \multirow{6}{*}{VGG19BN} & SINIFGSM & 55.3  & 66.0  & 49.5  & 100.0$\star$ & 38.3  & 57.4  & 50.6  & 39.3  & 33.9  & \textbf{31.2 } & \textbf{16.6 } & 43.8  \\
        & SINIFGSM-IKD & \textbf{61.6 } & \textbf{71.9 } & \textbf{55.5 } & 100.0$\star$ & \textbf{39.3 } & \textbf{59.8 } & \textbf{54.2 } & \textbf{44.7 } & \textbf{40.9 } & 28.9  & 15.5  & \textbf{47.2 } \\
        \cline{2-14}
        & VMIFGSM & 69.9  & 80.7  & 63.5  & 100.0$\star$ & 53.4  & 70.9  & 67.5  & 52.3  & 48.3  & 37.1  & \textbf{20.5 } & 56.4  \\
        & VMIFGSM-IKD & \textbf{73.9 } & \textbf{83.2 } & \textbf{67.6 } & 100.0$\star$ & \textbf{54.2 } & \textbf{71.5 } & \textbf{69.6 } & \textbf{57.4 } & \textbf{53.2 } & \textbf{37.2 } & 19.7  & \textbf{58.8 } \\
        \cline{2-14}
        & VNIFGSM & 71.5  & 81.2  & 63.7  & 100.0$\star$ & 53.6  & 71.9  & 68.4  & 54.0  & 49.8  & 37.0  & \textbf{20.7 } & 57.2  \\
        & VNIFGSM-IKD & \textbf{76.4 } & \textbf{85.3 } & \textbf{71.5 } & 100.0$\star$ & \textbf{56.9 } & \textbf{74.0 } & \textbf{71.7 } & \textbf{62.1 } & \textbf{56.0 } & \textbf{39.3 } & 20.1  & \textbf{61.3 } \\
        \hline
        \multirow{6}{*}{IncRes-v2} & SINIFGSM & 64.9  & 76.1  & 62.6  & 85.9  & 99.8$\star$ & 88.0  & 79.6  & 55.7  & 52.4  & 61.2  & 40.9  & 66.7  \\
        & SINIFGSM-IKD & \textbf{72.9 } & \textbf{82.8 } & \textbf{69.8 } & \textbf{90.5 } & \textbf{99.9$\star$} & \textbf{90.8 } & \textbf{86.4 } & \textbf{64.2 } & \textbf{62.4 } & \textbf{66.0 } & \textbf{44.6 } & \textbf{73.0 } \\
        \cline{2-14}
        & VMIFGSM & 63.1  & 72.3  & 63.1  & 81.6  & 98.8$\star$ & 82.8  & 82.4  & 57.0  & 56.2  & 57.2  & 42.7  & 65.8  \\
        & VMIFGSM-IKD & \textbf{69.7 } & \textbf{77.0 } & \textbf{69.9 } & \textbf{85.2 } & \textbf{98.9$\star$} & \textbf{86.6 } & \textbf{86.6 } & \textbf{63.5 } & \textbf{63.2 } & \textbf{61.0 } & \textbf{46.9 } & \textbf{71.0 } \\
        \cline{2-14}
        & VNIFGSM & 66.3  & 75.1  & 65.1  & 82.6  & \textbf{99.5$\star$} & 83.9  & 84.5  & 59.1  & 56.2  & 57.8  & 41.9  & 67.3  \\
        & VNIFGSM-IKD & \textbf{71.9 } & \textbf{79.1 } & \textbf{72.2 } & \textbf{85.7 } & 99.4$\star$ & \textbf{88.9 } & \textbf{86.7 } & \textbf{66.4 } & \textbf{64.7 } & \textbf{63.0 } & \textbf{46.3 } & \textbf{72.5 } \\
        \hline
        % \multirow{6}{*}{Inc-v3} & SINIFGSM & 56.3  & 71.2  & 53.5  & 83.4  & 67.6  & 100.0$\star$ & 72.5  & 45.7  & 42.6  & 50.2  & \textbf{26.2 } & 56.9  \\
        % & SINIFGSM-IKD & \textbf{62.4 } & \textbf{74.4 } & \textbf{57.7 } & \textbf{84.4 } & \textbf{74.7 } & 100.0$\star$ & \textbf{78.1 } & \textbf{50.9 } & \textbf{47.1 } & \textbf{54.7 } & 25.3  & \textbf{61.0 } \\
        % \cline{2-14}
        % & VMIFGSM & 55.8  & 68.8  & 53.9  & 78.2  & 73.0  & 99.4$\star$ & 73.4  & 48.7  & 43.4  & 52.9  & 29.5  & 57.8  \\
        % & VMIFGSM-IKD & \textbf{58.9 } & \textbf{70.7 } & \textbf{54.4 } & \textbf{79.2 } & \textbf{75.0 } & 99.4$\star$ & \textbf{78.4 } & \textbf{51.3 } & \textbf{47.5 } & \textbf{56.7 } & \textbf{30.4 } & \textbf{60.3 } \\
        % \cline{2-14}
        % & VNIFGSM & 61.3  & 72.2  & 57.5  & 80.2  & 75.2  & 99.7$\star$ & 77.7  & 50.4  & 47.4  & 53.6  & 31.3  & 60.7  \\
        % & VNIFGSM-IKD & \textbf{64.7 } & \textbf{76.4 } & \textbf{61.9 } & \textbf{82.9 } & \textbf{79.9 } & \textbf{100.0$\star$} & \textbf{83.2 } & \textbf{56.4 } & \textbf{52.5 } & \textbf{59.0 } & \textbf{33.1 } & \textbf{65.0 } \\
        % \hline
        % \multirow{6}{*}{Inc-v4} & SINIFGSM & 67.3  & 79.6  & 66.0  & 90.0  & 80.2  & 87.9  & 100.0$\star$ & 61.3  & 57.5  & 59.3  & 34.9  & 68.4  \\
        % & SINIFGSM-IKD & \textbf{73.6 } & \textbf{83.0 } & \textbf{71.3 } & \textbf{92.3 } & \textbf{85.7 } & \textbf{90.2 } & 99.9$\star$ & \textbf{66.2 } & \textbf{62.7 } & \textbf{61.0 } & \textbf{35.0 } & \textbf{72.1 } \\
        % \cline{2-14}
        % & VMIFGSM & 60.9  & 69.7  & 60.0  & 81.3  & 73.6  & 78.9  & \textbf{98.6$\star$} & 55.9  & 54.5  & 50.5  & 29.7  & 61.5  \\
        % & VMIFGSM-IKD & \textbf{65.7 } & \textbf{73.1 } & \textbf{65.6 } & \textbf{83.4 } & \textbf{79.6 } & \textbf{82.9 } & 98.5$\star$ & \textbf{60.2 } & \textbf{60.3 } & \textbf{52.0 } & \textbf{31.3 } & \textbf{65.4 } \\
        % \cline{2-14}
        % & VNIFGSM & 65.3  & 73.5  & 63.8  & 85.0  & 77.6  & 82.2  & 99.5$\star$ & 56.4  & 55.6  & 52.1  & 29.8  & 64.1  \\
        % & VNIFGSM-IKD & \textbf{70.3 } & \textbf{77.0 } & \textbf{70.7 } & \textbf{87.3 } & \textbf{82.1 } & \textbf{85.3 } & \textbf{99.8$\star$} & \textbf{64.6 } & \textbf{62.3 } & \textbf{53.5 } & \textbf{31.9 } & \textbf{68.5 } \\
        % \hline
        \multirow{6}{*}{RN101} & SINIFGSM & 90.7  & 87.5  & 90.6  & 88.3  & 64.4  & 71.8  & 71.6  & \textbf{99.9$\star$} & 89.9  & \textbf{39.0 } & 18.8  & 71.3  \\
        & SINIFGSM-IKD & \textbf{91.4 } & \textbf{88.0 } & \textbf{90.9 } & \textbf{88.4 } & \textbf{65.1 } & \textbf{73.2 } & \textbf{71.9 } & 99.7$\star$ & \textbf{90.4 } & 38.1  & \textbf{19.7 } & \textbf{71.7 } \\
        \cline{2-14}
        & VMIFGSM & 78.9  & 74.8  & 77.9  & \textbf{80.0 } & 58.4  & 65.3  & 64.1  & 95.9$\star$ & 78.3  & \textbf{38.2 } & 23.5  & 63.9  \\
        & VMIFGSM-IKD & \textbf{79.2 } & \textbf{74.9 } & \textbf{78.4 } & 79.5  & \textbf{59.0 } & \textbf{66.3 } & 64.1  & \textbf{96.1$\star$} & \textbf{78.4 } & 37.9  & \textbf{23.7 } & \textbf{64.1 } \\
        \cline{2-14}
        & VNIFGSM & 79.9  & 76.0  & 80.5  & \textbf{81.8 } & 59.9  & 67.2  & 64.3  & 96.8$\star$ & 80.4  & 37.6  & \textbf{24.0 } & 65.2  \\
        & VNIFGSM-IKD & \textbf{81.5 } & \textbf{78.1 } & \textbf{81.4 } & 81.3  & \textbf{60.7 } & \textbf{67.5 } & \textbf{65.2 } & 96.8$\star$ & \textbf{80.7 } & \textbf{39.8 } & 23.3  & \textbf{66.0 } \\
        \hline
        % \multirow{6}{*}{RN152} & SINIFGSM & 91.1  & 87.0  & 88.4  & \textbf{87.4 } & \textbf{65.1 } & 71.8  & \textbf{70.7 } & \textbf{91.4 } & \textbf{99.5$\star$} & \textbf{40.4 } & \textbf{20.1 } & \textbf{71.3 } \\
        % & SINIFGSM-IKD & \textbf{91.5 } & \textbf{88.1 } & \textbf{89.2 } & 86.4  & 63.6  & \textbf{71.9 } & 69.6  & 90.7  & 99.2$\star$ & 39.0  & 19.3  & 70.9  \\
        % \cline{2-14}
        % & VMIFGSM & 78.5  & 72.0  & 76.8  & \textbf{79.0 } & \textbf{58.1 } & 63.5  & \textbf{62.9 } & 77.0  & \textbf{95.8$\star$} & \textbf{40.0 } & 23.4  & 63.1  \\
        % & VMIFGSM-IKD & \textbf{78.6 } & \textbf{72.2 } & \textbf{77.7 } & 78.8  & 57.6  & \textbf{64.7 } & 62.0  & \textbf{78.0 } & 95.7$\star$ & 39.7  & \textbf{23.7 } & \textbf{63.3 } \\
        % \cline{2-14}
        % & VNIFGSM & 81.6  & 76.4  & 81.2  & 82.0  & \textbf{61.1 } & 65.7  & \textbf{66.2 } & 80.3  & 97.4$\star$ & \textbf{40.0 } & 24.0  & 65.9  \\
        % & VNIFGSM-IKD & \textbf{82.1 } & \textbf{78.0 } & \textbf{81.3 } & \textbf{82.1 } & 61.0  & \textbf{67.5 } & 65.9  & \textbf{81.5 } & \textbf{98.0$\star$} & 39.9  & \textbf{24.8 } & \textbf{66.4 } \\
        % \hline
    \end{tabular}%
    }
  \label{tab:agmasr}%
\end{table*}%

\subsection{Experiments Setting}
\subsubsection{Dataset}
We use a subset\footnote{\url{https://drive.google.com/drive/folders/1CfobY6i8BfqfWPHL31FKFDipNjqWwAhS}} of the ImageNet dataset~\cite{russakovsky2015imagenet} for the experiment. This subset consists of 1,000 images, covering nearly all the major categories in ImageNet, and has been widely used in previous related studies. In the experimental setup, we choose a pixel value range of 0-255 and set the maximum perturbation budget to 16, using the $L_\infty$ norm for the disturbance. Specifically, the perturbation size is constrained such that the adversarial perturbation must satisfy $\Vert\delta\Vert_\infty\leq16$, ensuring that the generated adversarial sample does not deviate significantly from the original sample. Additionally, to match the standardized input format, we adjust the image resolution to $3\times224\times224$, which complies with the standard preprocessing requirements of the ImageNet dataset.

\subsubsection{Evaluation Models}
We evaluate all attack methods using conventional training models and defense models. A total of 9 standard models and 2 defense models, provided by the timm package~\cite{rw2019timm}, are assessed. Specifically, the models used include: ResNet50~\cite{he2016deep}, DenseNet121~\cite{huang2017densely}, ResNeXt50~\cite{xie2017aggregated}, VGG19BN~\cite{simonyan2014very}, InceptionResNet-v2~\cite{szegedy2017inception}, Inception-v3~\cite{szegedy2016rethinking}, Inception-v4~\cite{szegedy2017inception}, ResNet101~\cite{he2016identity}, ResNet152~\cite{he2016identity}, Inception-v3$\rm _{adv}$~\cite{tramer2017ensemble} and InceptionResNet-v2$\rm _{adv,ens}$~\cite{tramer2017ensemble}.

\subsubsection{Metrics}
To evaluate the effectiveness of different attack methods, we use the attack success rate (ASR) for both white-box and black-box models as measurement indicators.
% To ensure fairness and comparability in the evaluations, all methods are tested under the same $L_\infty$ perturbation budget constraint, meaning the perturbation size of the adversarial sample is strictly limited to the range $\Vert\delta\Vert_\infty\leq\epsilon$.

\subsubsection{Baselines}
To thoroughly evaluate our proposed approach, we selected several existing baseline attack methods for comparison, including MIFGSM~\cite{dong2018boosting}, DIFGSM~\cite{xie2019improving}, TIFGSM~\cite{dong2019evading}, NIFGSM~\cite{lin2019nesterov}, SINIFGSM~\cite{lin2019nesterov}, VMIFGSM~\cite{wang2021enhancing}, and VNIFGSM~\cite{wang2021enhancing}. These methods encompass various types of gradient-based attacks, providing a broad frame of reference to effectively demonstrate the performance of our approach against multiple attack strategies.
% To ensure that the generated adversarial samples remain within a reasonable image range, we clip the pixel values of the adversarial samples after each iteration, ensuring that they stay within the standard image range of 0 to 255.

\subsubsection{Implementation Details}
To ensure the comparability and consistency of the experiments, we set the maximum permissible perturbation $\epsilon=16$, the number of iterations $T=10$, the attack step size $\alpha=\frac{2}{255}$, and the momentum term attenuation factor $\mu=1.0$, following previous research~\cite{dong2019evading,wang2021enhancing,xie2019improving}. These parameter settings are consistent with those commonly used in the literature, enabling a fair comparison of different attack methods. To facilitate the implementation and comparison of attack strategies, we employ the attack toolkit Torchattacks~\cite{kim2020torchattacks} and retain its default parameters, with the exception of the custom settings for $\epsilon$ and $T$. This ensures that all methods are compared within the same attack framework, minimizing the influence of other factors on the experimental outcomes. All experiments are implemented in the PyTorch framework and executed on one NVIDIA GeForce RTX 3090 GPU.

\subsection{Experiments Results}
In this section, we integrate the proposed Inverse Knowledge Distillation (IKD) method with existing attack strategies to investigate potential improvements in attack performance. The resulting combined methods are denoted by the suffix ``-IKD", such as MIFGSM-IKD. The comparison methods include classical attacks (e.g., MIFGSM and NIFGSM), attacks based on input transformations (e.g., DIFGSM and TIFGSM), and advanced gradient-based attacks (e.g., SINIFGSM, VMIFGSM, and VNIFGSM).

\subsubsection{Combination with classical attacks and attacks based on input transformations}
As shown in~\Cref{tab:ngmasr}, our method outperforms the comparison methods on the vast majority of black-box models. Specifically, we observe a significant improvement in average transferability across almost all eleven models tested. Notably, our method achieves a $6.8\%$ increase in average transferability compared to DIFGSM on the VGG19BN model, with our DIFGSM-IKD method attaining an average transferability of $57.8\%$, while DIFGSM alone achieves $51.0\%$. Furthermore, our approach also outperforms most comparison methods under defense models in terms of average attack success rate, demonstrating the effectiveness of our method in attacking defensive mechanisms. Additionally, we find that DenseNet121 is the most vulnerable model, exhibiting the highest average transferability, which challenges the conventional belief that deeper models are inherently more robust than shallower ones. This observation suggests that model robustness is more closely linked to architectural design than to depth alone. Finally, we acknowledge that in some cases, our method may show slightly lower performance than comparison methods in either white-box or black-box settings. This may be due to the introduction of the IKD, which could slightly interfere with the generation of adversarial perturbations. Additional experimental results can be found in the supplementary material.

\subsubsection{Combination with Advanced gradient Attack}
In~\Cref{tab:agmasr}, we examine the potential enhancement of attack performance by integrating our approach with advanced gradient-based attacks. On one hand, our method increases the average transferability of the original attack in the vast majority of cases. Specifically, we observe 
%an average transferability improvement of $2.3\%$, with %
a maximum improvement of $6.3\%$ when using SINIFGSM on the InceptionResNet-v2 model. On the other hand, the performance improvement varies across different attacks and models. For instance, among the eleven models, ResNet50 exhibits the largest increase in attack performance, with a $4.5\%$ improvement. SINIFGSM-IKD achieves a $4.8\%$ increase, while VMIFGSM-IKD shows a modest $0.3\%$ improvement. In contrast, the smallest difference in performance improvement is observed with ResNet101, where the improvement is only $0.6\%$. Additionally, VNIFGSM-IKD demonstrates the largest performance improvement of $3.4\%$ against DenseNet121 (the best-performing model), and a smaller improvement of $1.6\%$ against VGG19BN (the least improved model) among the four attacks. In comparison, VMIFGSM-IKD shows the smallest variation in performance improvement, with a $0.8\%$ difference between VGG19BN ($2.3\%$) and DenseNet121 ($1.5\%$). This discrepancy may be attributed to the fact that VMIFGSM has less room for improvement relative to other attacks. Additional experimental results can be found in the supplementary material.

\subsection{Ablation Study}
To thoroughly investigate the potential factors that may influence the performance of our method, we conduct two ablation experiments in this section: (1) the effect of IKD's soft label loss selection in inverse knowledge distillation, and (2) the impact of the weight of the IKD.
\renewcommand{\floatpagefraction}{.99}
\begin{figure}[htbp]
	\centering
	\subfloat[Classical attacks and input transformation-based attacks.]{\includegraphics[width=0.48\columnwidth]{ngmasr_ikd.pdf}}
	\subfloat[Advanced gradient attacks.]{\includegraphics[width=0.48\columnwidth]{agmasr_ikd.pdf}}
	\caption{Effect of different IKD methods on the transfer-based average attack success rate.}
    \label{fig:regularization}
\end{figure}
\begin{figure}[htbp]
	\centering
	\subfloat[Classical attacks and input transformation-based attacks.]{\includegraphics[width=0.48\columnwidth]{ngmasr_weight.pdf}}
	\subfloat[Advanced gradient attacks.]{\includegraphics[width=0.48\columnwidth]{agmasr_weight.pdf}}
	\caption{Effect of IKD weight on the transfer-based average attack success rate.}
    \label{fig:weight}
\end{figure}

\subsubsection{Influence of soft label loss selection on IKD}\label{section:Influence of soft label loss selection on IKD}
To investigate the effect of different types of IKD's soft label loss on adversarial transferability, we employ Mean Squared Error (MSE), Cross-Entropy (CE), and Kullback-Leibler (KL) Divergence for inverse knowledge distillation. Specifically, we conduct experiments on ResNet50 (RN50) using MSE, CE, and KL divergence as the soft label loss in IKD. The average evaluation results are presented in~\Cref{fig:regularization}. From the results, it is evident that KL divergence demonstrates superior adversarial transferability on RN50 compared to the other two methods. For RN50, the mean transferability with MSE, CE, and KL divergence as the IKD's soft label loss were $63.2\%$, $63.4\%$, and $65.2\%$, respectively. The transferability across different soft label loss varies significantly. Based on these findings, we opt to use KL divergence as the soft label loss for IKD. For more detailed experimental results, please refer to the supplementary material.

\subsubsection{Influence of weight of IKD}
The inverse knowledge distillation method exhibits varying influences on the gradient direction based on the weight parameter, which in turn affects the adversarial transferability. To investigate this relationship, we perform a grid search over the weight parameter $\gamma$ of inverse knowledge distillation in the range $\{0.001,0.01,0.1,1,10,100,1000\}$, using ResNet50 (RN50) as the surrogate model. The average evaluation results are presented in~\Cref{fig:weight}. As observed, with the increase in the IKD weight, the average transferability of RN50 initially remains stable within a certain range, and then gradually declines. This suggests that the inverse knowledge distillation method proposed in this paper is not confined to a specific weight value. Based on these findings, we select $\gamma=0.01$ as the weight for the inverse knowledge distillation attack. For more detailed experimental results, please refer to the supplementary material.

\section{Conclusion}
In this paper, we propose an inverse knowledge distillation (IKD) method aimed at significantly improving the transferability of adversarial samples. Specifically, we enhance the diversity of attack gradients by incorporating the IKD's soft label loss into the model's loss function. IKD not only encourages the model to focus on the classification error of the target label during adversarial sample generation but also optimizes the gradient update direction through the inverse knowledge distillation process. As a result, our approach effectively mitigates overfitting and enhances the transferability of adversarial samples. The proposed IKD method integrates seamlessly with existing gradient-based attack techniques, thereby boosting attack performance. Experimental results demonstrate the efficacy of the proposed method, with extensive evaluations on the ImageNet dataset confirming significant improvements in attack success rates across several baseline attack strategies.

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}


\appendix

% \section*{Ethical Statement}
% There are no ethical issues.

% \section*{Acknowledgments}
\section*{Supplementary Material}

% The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
% files that implement them was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Preparation of the Microsoft Word file was supported by IJCAI.  An
% early version of this document was created by Shirley Jowell and Peter
% F. Patel-Schneider.  It was subsequently modified by Jennifer
% Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
% Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
% Francisco Cruz-Mencia and Edith Elkind.


\section{Algorithm}
Taking MIFGSM as an example, the specific algorithmic workflow of MIFGSM-IKD is detailed in ~\Cref{alg:algorithm}. By incorporating the momentum term and an appropriate IKD strategy, adversarial samples are iteratively updated to achieve effective attacks on the target model.
\begin{algorithm}[htbp]
    \caption{Example algorithm}
    \label{alg:algorithm}
    \textbf{Input}: A classifier $f$ with loss function $\mathcal{L}$; a benign sample $x$ and ground-truth label $y$;\\
    \textbf{Input}: The size of perturbation $\epsilon$; iterations $T$ and decay factor $\mu$\\
    \textbf{Output}: An adversarial sample $x^{adv}$ with $\Vert x^{adv}-x\Vert_\infty\leq\epsilon$.
    \begin{algorithmic}[1] %[1] enables line numbers
        \STATE $\alpha=\frac{\epsilon}{T}$;
        \STATE $g_0=0;x_0^{adv}=x$;
        \FOR{$t=0$ to $T-1$}
        \STATE Input $x_t^{adv}$ to $f$ and obtain the hard label loss $\mathcal{L}_{hard}(f_\varphi(x^{adv}),y)$;
        \STATE Input $x_t^{adv}$ and $x$ to $f$ and obtain the soft label loss $\mathcal{L}_{soft}(f_\varphi(x^{adv}),f_\varphi(x))$;
        \STATE Calculate the total loss
        \begin{equation}
            \mathcal{L}_{total}=\mathcal{L}_{hard}(f_\varphi(x^{adv}),y)+\gamma\mathcal{L}_{soft}(f_\varphi(x^{adv}),f_\varphi(x));
            \nonumber
        \end{equation}
        \STATE Obtain the gradient $\nabla_x(\mathcal{L}_{total})$;
        \STATE Update $g_{t+1}$ by accumulating the velocity vector in the gradient direction as
        \begin{equation}
            g_{t+1}=\mu\cdot g_t+\frac{\nabla_x(\mathcal{L}_{total})}{\Vert\nabla_x(\mathcal{L}_{total})\Vert_1};
            \nonumber
        \end{equation}
        \STATE Update $x_{t+1}^{adv}$ by applying the sign gradient as
        \begin{equation}
            x_{t+1}^{adv}=x_t^{adv}+\alpha\cdot sign(g_{t+1});
            \nonumber
        \end{equation}
        \ENDFOR
        \STATE \textbf{return} $x^{adv}=x_t^{adv}$.  
    \end{algorithmic}
\end{algorithm}

\section{Experiments}
\Cref{tab:ngmasr_appendix} demonstrates the effects of IKD on classical and input transformation-based attacks when using ResNeXt50, Inception-v3, Inception-v4, and ResNet152 as surrogate models.
\begin{table*}[htbp]
    \centering
    \caption{Comparison results of various attack methods in terms of attack success rate ($\%$). The bold item indicates the best one. Item with $\star$ superscript is white-box attacks, and the others is black-box attacks. AVG column indicates the average attack success rate on black-box models.}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{c|l|cccccccccccc}
    \hline
        \multirow{2}{*}{Source Model} & \multirow{2}{*}{Method} & \multicolumn{12}{c}{Target Model} \\ \cline{3-14}
         &  & RN50 & DN121 & RNX50 & VGG19BN & IncRes-v2 & Inc-v3 & Inc-v4 & RN101 & RN152 & Inc-v3$adv$ &  IncRes-V2$adv,ens$ & AVG \\ \hline
        % \multirow{8}{*}{RN50} & MIFGSM & 99.9$\star$ & 65.7  & 75.2  & 73.2  & 38.9  & 52.6  & 46.0  & 68.2  & 62.1  & 30.6  & 16.0  & 52.9  \\ 
        % ~ & MIFGSM-IKD & 99.9$\star$ & \textbf{68.4}  & \textbf{79.4}  & \textbf{76.0}  & \textbf{39.9}  & \textbf{54.4}  & \textbf{49.6}  & \textbf{73.1}  & \textbf{67.6}  & \textbf{32.1}  & \textbf{16.3}  & \textbf{55.7}  \\ \cline{2-14}
        % ~ & DIFGSM & \textbf{99.9$\star$} & 82.9  & 88.8  & 84.7  & 61.7  & 70.3  & 69.4  & 86.1  & 82.1  & 35.9  & 18.4  & 68.0  \\
        % ~ & DIFGSM-IKD & 99.8$\star$ & \textbf{84.9}  & \textbf{90.6}  & \textbf{84.8}  & \textbf{62.9}  & \textbf{70.8}  & \textbf{69.6}  & \textbf{86.5}  & \textbf{83.3}  & \textbf{38.4}  & \textbf{18.9}  & \textbf{69.1}  \\ \cline{2-14}
        % ~ & TIFGSM & 99.0$\star$ & 68.4  & 65.4  & \textbf{71.3}  & 51.1  & 63.9  & 60.3  & 55.6  & \textbf{49.8}  & 52.8  & 43.0  & 58.2  \\
        % ~ & TIFGSM-IKD & \textbf{99.2$\star$} & \textbf{69.1}  & \textbf{67.1}  & 70.8  & \textbf{52.1}  & \textbf{64.5}  & \textbf{61.4}  & \textbf{57.9}  & 49.5  & 52.8  & \textbf{43.8}  & \textbf{58.9}  \\ \cline{2-14}
        % ~ & NIFGSM & 99.9$\star$ & 71.6  & 80.0  & 76.5  & 43.4  & 55.2  & 49.2  & 75.9  & 70.7  & 31.6  & \textbf{16.0}  & 57.0  \\
        % ~ & NIFGSM-IKD & \textbf{100.0$\star$} & \textbf{76.5}  & \textbf{85.0}  & \textbf{79.3}  & \textbf{44.6}  & \textbf{57.8}  & \textbf{52.5}  & \textbf{81.1}  & \textbf{74.7}  & \textbf{32.0}  & 15.4  & \textbf{59.9}  \\ \hline
        % \multirow{8}{*}{DN121} & MIFGSM & 75.7  & 100.0$\star$ & 71.6  & 86.3  & 52.2  & 65.4  & 58.4  & 61.4  & 55.4  & 38.2  & 20.9  & 58.6  \\
        % ~ & MIFGSM-IKD & \textbf{78.3}  & 100.0$\star$ & \textbf{76.6}  & \textbf{88.5}  & \textbf{55.7}  & \textbf{70.9}  & \textbf{64.1}  & \textbf{68.1}  & \textbf{62.8}  & \textbf{38.6}  & \textbf{21.0}  & \textbf{62.5}  \\ \cline{2-14}
        % ~ & DIFGSM & 87.2  & 100.0$\star$ & 84.5  & 92.0  & 74.3  & \textbf{82.8}  & 80.2  & 77.8  & 72.5  & \textbf{48.5}  & 29.0  & 72.9  \\
        % ~ & DIFGSM-IKD & \textbf{89.0}  & 100.0$\star$ & \textbf{88.5}  & \textbf{93.9}  & \textbf{77.3}  & 82.7  & \textbf{82.9}  & \textbf{81.9}  & \textbf{78.6}  & 48.3  & 29.0  & \textbf{75.2}  \\ \cline{2-14}
        % ~ & TIFGSM & 58.4  & 99.7$\star$ & 52.8  & \textbf{82.0}  & 60.4  & \textbf{72.8}  & 66.2  & 40.7  & 34.1  & 64.5  & 54.4  & 58.6  \\
        % ~ & TIFGSM-IKD & \textbf{60.5}  & \textbf{99.8$\star$} & \textbf{56.1}  & 81.7  & \textbf{62.1}  & 72.4  & \textbf{67.2}  & \textbf{41.3}  & \textbf{37.8}  & \textbf{65.7}  & \textbf{57.5}  & \textbf{60.2}  \\ \cline{2-14}
        % ~ & NIFGSM & 79.0  & 100.0$\star$ & 77.9  & 86.1  & 55.3  & 68.2  & 62.1  & 67.3  & 60.4  & 39.0  & 20.4  & 61.6  \\
        % ~ & NIFGSM-IKD & \textbf{82.3}  & 100.0$\star$ & \textbf{80.0}  & \textbf{89.4}  & \textbf{57.9}  & \textbf{70.}5  & \textbf{64.8}  & \textbf{71.9}  & \textbf{66.4}  & \textbf{39.6}  & \textbf{20.6}  & \textbf{64.3}  \\ \hline
        \multirow{8}{*}{RNX50} & MIFGSM & 68.2  & 60.6  & 98.7$\star$ & \textbf{72.8}  & 31.3  & \textbf{46.1}  & 40.1  & 61.8  & 55.2  & 35.7  & \textbf{15.5}  & 48.7  \\
        ~ & MIFGSM-IKD & \textbf{72.0}  & \textbf{63.3}  & \textbf{98.9$\star$} & 72.2  & \textbf{34.0}  & 45.1  & \textbf{41.6}  & \textbf{64.8}  & \textbf{57.9}  & \textbf{35.9}  & 14.9  & \textbf{50.2}  \\ \cline{2-14}
        ~ & DIFGSM & 79.0  & 75.9  & 94.6$\star$ & 78.5  & 52.2  & 59.7  & \textbf{62.1}  & 75.8  & 71.9  & 36.3  & 16.1  & 60.8  \\
        ~ & DIFGSM-IKD & \textbf{81.3}  & \textbf{76.3}  & \textbf{96.1$\star$} & \textbf{81.9}  & \textbf{55.1}  & \textbf{60.2}  & 62.0  & \textbf{77.2}  & \textbf{73.1}  & \textbf{37.4}  & \textbf{17.5}  & \textbf{62.2}  \\ \cline{2-14}
        ~ & TIFGSM & 54.6  & \textbf{58.0}  & \textbf{85.6$\star$} & 67.5  & 42.2  & 54.6  & 52.1  & \textbf{45.8}  & \textbf{41.2}  & \textbf{42.0}  & 33.1  & 49.1  \\
        ~ & TIFGSM-IKD & 54.6  & 56.6  & 85.5$\star$ & \textbf{68.0}  & \textbf{42.6}  & \textbf{55.0}  & \textbf{52.2}  & 45.2  & 41.1  & 41.8  & \textbf{35.6}  & \textbf{49.3}  \\ \cline{2-14}
        ~ & NIFGSM & 74.8  & 67.8  & \textbf{99.9$\star$} & 73.9  & 35.5  & 47.0  & \textbf{46.3}  & 67.4  & 61.0  & \textbf{37.6}  & \textbf{14.8}  & 52.6  \\
        ~ & NIFGSM-IKD & \textbf{78.5}  & \textbf{70.2}  & 99.8$\star$ & \textbf{77.7}  & \textbf{37.1}  & \textbf{49.3}  & 44.2  & \textbf{69.6}  & \textbf{63.8}  & 36.0  & 13.6  & \textbf{54.0}  \\ \hline
        % \multirow{8}{*}{VGG19BN} & MIFGSM & 47.9  & 58.9  & 43.4  & 100.0$\star$ & 30.1  & 46.8  & 42.9  & 33.3  & 28.9  & 26.6  & \textbf{15.2}  & 37.4  \\
        % ~ & MIFGSM-IKD & \textbf{55.2}  & \textbf{63.8}  & \textbf{50.2}  & 100.0$\star$ & \textbf{34.3}  & \textbf{52.5}  & \textbf{49.0}  & \textbf{39.3}  & \textbf{36.4}  & \textbf{27.1}  & 14.9  & \textbf{42.3}  \\ \cline{2-14}
        % ~ & DIFGSM & 63.3  & 75.0  & 58.3  & 100.0$\star$ & 46.7  & 64.5  & 61.2  & 46.1  & 43.1  & 32.7  & 18.6  & 51.0  \\
        % ~ & DIFGSM-IKD & \textbf{71.1}  & \textbf{80.3}  & \textbf{67.9}  & 100.0$\star$ & \textbf{55.8}  & \textbf{69.4}  & \textbf{69.5}  & \textbf{57.4}  & \textbf{52.7}  & \textbf{35.4}  & 18.6  & \textbf{57.8}  \\ \cline{2-14}
        % ~ & TIFGSM & 40.2  & 59.8  & 34.6  & 100.0$\star$ & 40.5  & 62.6  & 56.8  & 25.2  & 20.9  & 44.1  & 33.8  & 41.9  \\
        % ~ & TIFGSM-IKD & \textbf{44.8}  & \textbf{66.2}  & \textbf{42.2}  & 100.0$\star$ & \textbf{48.2}  & \textbf{65.9}  & \textbf{61.6}  & \textbf{30.1}  & \textbf{26.3}  & \textbf{48.8}  & \textbf{36.5}  & \textbf{47.1}  \\ \cline{2-14}
        % ~ & NIFGSM & 54.3  & 62.9  & 47.7  & 100.0$\star$ & 34.4  & 50.6  & 45.2  & 39.7  & 33.9  & 25.9  & 14.2  & 40.9  \\
        % ~ & NIFGSM-IKD & \textbf{57.8}  & \textbf{65.8}  & \textbf{51.5}  & 100.0$\star$ & \textbf{36.4}  & \textbf{53.5}  & \textbf{50.0}  & \textbf{41.8}  & \textbf{37.0}  & \textbf{26.}3  & \textbf{14.7}  & \textbf{43.5}  \\ \hline
        % \multirow{8}{*}{IncRes-v2} & MIFGSM & 54.8  & 64.4  & 55.2  & 78.6  & 98.9$\star$ & 74.1  & 70.9  & 46.4  & 44.2  & 44.8  & 28.0  & 56.1  \\
        % ~ & MIFGSM-IKD & \textbf{58.1}  & \textbf{68.7}  & \textbf{57.5}  & \textbf{79.5}  & 98.9$\star$ & \textbf{77.6}  & \textbf{75.0}  & \textbf{51}.4  & \textbf{47.8}  & \textbf{46.3}  & \textbf{30.4}  & \textbf{59.2}  \\ \cline{2-14}
        % ~ & DIFGSM & 64.8  & 75.2  & 65.4  & 82.5  & \textbf{98.6$\star$} & 84.5  & 83.9  & 58.9  & 54.6  & 54.7  & 39.0  & 66.4  \\
        % ~ & DIFGSM-IKD & \textbf{69.7}  & \textbf{79.4}  & \textbf{69.3}  & \textbf{85.3}  & 98.3$\star$ & \textbf{85.7}  & \textbf{86.5}  & \textbf{64.3}  & \textbf{58.4}  & \textbf{58.1}  & \textbf{39.5}  & \textbf{69.6}  \\ \cline{2-14}
        % ~ & TIFGSM & 40.9  & 59.1  & 39.4  & 68.0  & \textbf{97.7$\star$} & 74.3  & 70.1  & 31.5  & 27.4  & 67.6  & 63.9  & 54.2  \\
        % ~ & TIFGSM-IKD & \textbf{45.0}  & \textbf{60.4}  & \textbf{40.9}  & \textbf{68.6}  & 97.6$\star$ & \textbf{76.1}  & \textbf{73.1}  & \textbf{35.6}  & \textbf{29.3}  & \textbf{70.8}  & \textbf{68.2}  & \textbf{56.8}  \\ \cline{2-14}
        % ~ & NIFGSM & 55.8  & 65.9  & 56.2  & 78.1  & 99.3$\star$ & 74.1  & 70.5  & 48.6  & 45.5  & 43.3  & 26.6  & 56.5  \\
        % ~ & NIFGSM-IKD & \textbf{60.5}  & \textbf{70.3}  & \textbf{59.7}  & \textbf{80.4}  & \textbf{99.4$\star$} & \textbf{76.6}  & \textbf{73.3}  & \textbf{52.4}  & \textbf{50.1}  & \textbf{46.9}  & \textbf{27.0}  & \textbf{59.7}  \\ \hline
        \multirow{8}{*}{Inc-v3} & MIFGSM & 47.7  & \textbf{60.9}  & 44.7  & 72.3  & \textbf{58.3}  & 99.1$\star$ & 61.8  & 38.3  & 35.9  & 41.5  & 21.4  & 48.3  \\
        ~ & MIFGSM-IKD & \textbf{48.6}  & 60.0  & \textbf{45.1}  & \textbf{72.4}  & 58.1  & \textbf{99.7$\star$} & \textbf{62.8}  & \textbf{38.6}  & \textbf{36.6}  & \textbf{41.8}  & \textbf{22.6}  & \textbf{48.7}  \\ \cline{2-14}
        ~ & DIFGSM & 56.6  & 68.6  & 53.3  & 78.7  & 70.1  & 99.1$\star$ & 73.8  & 47.2  & 42.0  & 49.5  & 26.9  & 56.7  \\
        ~ & DIFGSM-IKD & \textbf{57.8}  & \textbf{69.5}  & \textbf{56.1}  & \textbf{79.8}  & \textbf{74.9}  & \textbf{99.3$\star$} & \textbf{77.2}  & \textbf{50.9}  & \textbf{45.2}  & \textbf{54.0}  & \textbf{29.0}  & \textbf{59.4}  \\ \cline{2-14}
        ~ & TIFGSM & 32.7  & 49.8  & 28.6  & 64.2  & 51.2  & \textbf{98.8$\star$} & 60.0  & 21.0  & 18.8  & 56.0  & 42.4  & 42.5  \\
        ~ & TIFGSM-IKD & \textbf{33.9}  & \textbf{50.4}  & \textbf{30.9}  & \textbf{66.6}  & \textbf{55.9}  & 98.7$\star$ & \textbf{62.4}  & \textbf{23.0}  & \textbf{20.3}  & \textbf{60.3}  & \textbf{44.9}  & \textbf{44.9}  \\ \cline{2-14}
        ~ & NIFGSM & 55.4  & 69.4  & 52.0  & 76.5  & 64.9  & 99.4$\star$ & 70.0  & 47.7  & 40.8  & 42.2  & 21.6  & 54.1  \\
        ~ & NIFGSM-IKD & \textbf{57.6}  & \textbf{71.5}  & \textbf{56.3}  & \textbf{78.4}  & \textbf{69.5}  & \textbf{99.8$\star$} & \textbf{72.7}  & \textbf{48.1}  & \textbf{45.6}  & \textbf{44.3}  & \textbf{23.0}  & \textbf{56.7}  \\ \hline
        \multirow{8}{*}{Inc-v4} & MIFGSM & 53.3  & 62.4  & 52.8  & 76.5  & 60.4  & 69.5  & 97.9$\star$ & 45.0  & 42.6  & 37.1  & \textbf{20.4}  & 52.0  \\
        ~ & MIFGSM-IKD & \textbf{54.1}  & \textbf{64.2}  & \textbf{53.7}  & \textbf{78.9}  & \textbf{62.2}  & \textbf{71.8}  & \textbf{99.0$\star$} & \textbf{48.3}  & \textbf{44.7}  & \textbf{38.4}  & 20.1  & \textbf{53.6}  \\ \cline{2-14}
        ~ & DIFGSM & 62.9  & 70.9  & 62.1  & 82.4  & 76.2  & 79.8  & 98.0$\star$ & 55.6  & 52.4  & 45.2  & 25.6  & 61.3  \\
        ~ & DIFGSM-IKD & \textbf{66.2}  & \textbf{74.0}  & \textbf{66.7}  & \textbf{84.4}  & 76.2  & \textbf{81.6}  & \textbf{98.1$\star$} & \textbf{59.4}  & \textbf{55.7}  & \textbf{47.7}  & \textbf{26.7}  & \textbf{63.9}  \\ \cline{2-14}
        ~ & TIFGSM & 39.3  & 51.4  & 37.2  & 67.7  & 55.9  & 67.5  & \textbf{97.7$\star$} & 27.8  & 24.1  & 53.1  & \textbf{44.5}  & 46.9  \\
        ~ & TIFGSM-IKD & \textbf{40.0}  & \textbf{52.2}  & \textbf{38.0}  & \textbf{69.1}  & \textbf{57.8}  & \textbf{70.1}  & 97.1$\star$ & \textbf{28.9}  & \textbf{25.7}  & \textbf{55.7}  & 44.2  & \textbf{48.2}  \\ \cline{2-14}
        ~ & NIFGSM & 58.5  & 66.4  & 58.3  & 82.5  & 61.6  & 72.3  & 98.9$\star$ & 49.6  & 45.3  & 37.4  & 18.9  & 55.1  \\
        ~ & NIFGSM-IKD & \textbf{60.8}  & \textbf{69.1}  & \textbf{60.2}  & \textbf{82.6}  & \textbf{65.7}  & \textbf{74.8}  & \textbf{99.8$\star$} & \textbf{52.7}  & \textbf{48.9}  & \textbf{39.3}  & \textbf{20.7}  & \textbf{57.5}  \\ \hline
        % \multirow{8}{*}{RN101} & MIFGSM & 72.5  & 66.3  & 70.4  & 74.8  & 36.9  & 49.3  & 44.0  & 97.4$\star$ & 70.1  & 29.4  & 14.5  & 52.8  \\
        % ~ & MIFGSM-IKD & \textbf{75.3}  & \textbf{66.9}  & \textbf{72.4}  & \textbf{75.1}  & \textbf{37.3}  & \textbf{50.3}  & \textbf{44.4}  & \textbf{97.7$\star$} & \textbf{71.7}  & \textbf{30.3}  & \textbf{14.9}  & \textbf{53.9}  \\ \cline{2-14}
        % ~ & DIFGSM & 77.7  & \textbf{75.4}  & 77.6  & \textbf{80.6}  & \textbf{56.6}  & 63.6  & 61.0  & 93.2$\star$ & 76.6  & \textbf{35.1}  & \textbf{17.9}  & 62.2  \\
        % ~ & DIFGSM-IKD & \textbf{78.5}  & 75.0  & \textbf{80.1}  & 80.3  & 56.0  & \textbf{64.5}  & \textbf{61.7}  & \textbf{93.3$\star$} & \textbf{78.4}  & 32.3  & 17.7  & \textbf{62.5}  \\ \cline{2-14}
        % ~ & TIFGSM & \textbf{53.6}  & \textbf{56.0}  & \textbf{54.0}  & \textbf{64.6}  & \textbf{44.6}  & 55.0  & 52.9  & 77.9$\star$ & 49.7  & 42.9  & \textbf{36.3}  & \textbf{51.0}  \\
        % ~ & TIFGSM-IKD & 53.2  & 54.7  & 53.2  & 64.3  & 44.3  & \textbf{56.5}  & \textbf{53.5}  & \textbf{79.8$\star$} & \textbf{50.6}  & \textbf{43.5}  & 35.6  & 50.9  \\ \cline{2-14}
        % ~ & NIFGSM & 79.7  & 72.2  & 77.0  & 78.2  & 37.9  & 51.5  & 45.6  & \textbf{99.2$\star$} & 74.1  & \textbf{30.1}  & 14.3  & 56.1  \\
        % ~ & NIFGSM-IKD & \textbf{80.3}  & \textbf{72.8}  & \textbf{77.7}  & \textbf{79.2}  & \textbf{39.9}  & \textbf{52.4}  & \textbf{47.6}  & 99.0$\star$ & \textbf{75.1}  & 29.5  & \textbf{15.7}  & \textbf{57.0}  \\ \hline
        \multirow{8}{*}{RN152} & MIFGSM & 69.6  & 62.9  & 66.3  & 74.1  & 35.9  & 48.7  & 42.2  & 68.8  & 96.2$\star$ & 26.2  & 14.8  & 51.0  \\
        ~ & MIFGSM-IKD & \textbf{71.2}  & \textbf{63.5}  & \textbf{68.8}  & \textbf{74.8}  & \textbf{38.1}  & \textbf{49.7}  & \textbf{44.7}  & \textbf{70.3}  & \textbf{96.4$\star$} & \textbf{27.4}  & \textbf{14.9}  & \textbf{52.3}  \\ \cline{2-14}
        ~ & DIFGSM & 75.2  & \textbf{72.3}  & 75.5  & \textbf{79.0}  & \textbf{55.4}  & \textbf{62.7}  & 60.5  & 78.0  & 91.2$\star$ & \textbf{33.7}  & 17.6  & 61.0  \\
        ~ & DIFGSM-IKD & \textbf{76.3}  & 72.0  & \textbf{77.6}  & 78.0  & 55.2  & 62.5  & \textbf{60.8}  & \textbf{78.7}  & \textbf{91.8$\star$} & 32.9  & \textbf{17.8}  & \textbf{61.2}  \\ \cline{2-14}
        ~ & TIFGSM & 49.4  & \textbf{52.4}  & 50.5  & \textbf{64.1}  & 42.1  & \textbf{54.5}  & 49.0  & 49.4  & 74.4$\star$ & \textbf{42.6}  & \textbf{36.8}  & 49.1  \\
        ~ & TIFGSM-IKD & \textbf{50.8}  & 51.4  & \textbf{51.6}  & 62.8  & \textbf{42.2}  & 53.2  & \textbf{50.0}  & \textbf{51.0}  & \textbf{74.9$\star$} & 42.4  & 35.1  & 49.1  \\ \cline{2-14}
        ~ & NIFGSM & 77.3  & 68.0  & 74.5  & 77.4  & 37.0  & \textbf{51.1}  & 44.3  & 76.4  & 99.1$\star$ & \textbf{27.4}  & \textbf{15.6}  & 54.9  \\
        ~ & NIFGSM-IKD & \textbf{78.8}  & \textbf{70.7}  & \textbf{75.0}  & \textbf{79.5}  & \textbf{38.4}  & 51.0  & \textbf{45.6}  & \textbf{77.5}  & 99.1$\star$ & 26.6  & 15.1  & \textbf{55.8} \\ \hline
    \end{tabular}
    }
  \label{tab:ngmasr_appendix}%
\end{table*}

\Cref{tab:agmasr_appendix} illustrates the impact of IKD on advanced gradient attacks using ResNeXt50, Inception-v3, Inception-v4, and ResNet152 as surrogate models.
\begin{table*}[htbp]
  \centering
  \caption{Evaluation results of the advanced attacks combined with IKD in terms of attack success rate (\%). The bold item indicates the best one. Item with $\star$ superscript is white-box attacks, and the others is black-box attacks. AVG column indicates the average attack success rate on black-box models.}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{c|l|cccccccccccc}
        \hline
        \multirow{2}{*}{Source Model} & \multirow{2}{*}{Method} & \multicolumn{12}{c}{Target Model} \\
        \cline{3-14}
        & & RN50  & DN121 & RNX50 & VGG19BN & IncRes-v2 & Inc-v3 & Inc-v4 & RN101 & RN152 & Inc-v3$adv$ &  IncRes-V2$adv,ens$ & AVG \\
        \hline
        % \multirow{6}{*}{RN50} & SINIFGSM & 99.9$\star$ & 81.7  & 85.0  & 84.5  & 53.2  & 67.0  & 59.3  & 80.1  & 75.4  & 37.6  & 17.1  & 64.1  \\
        % & SINIFGSM-IKD & \textbf{100.0$\star$} & \textbf{88.5 } & \textbf{91.3 } & \textbf{88.7 } & \textbf{58.0 } & \textbf{69.9 } & \textbf{64.5 } & \textbf{87.9 } & \textbf{84.1 } & \textbf{38.2 } & \textbf{17.7 } & \textbf{68.9 } \\
        % \cline{2-14}
        % & VMIFGSM & 99.9$\star$ & 84.7  & 88.6  & \textbf{86.5 } & 64.8  & \textbf{73.6 } & \textbf{73.5 } & 85.5  & 84.9  & 43.8  & \textbf{24.2 } & 71.0  \\
        % & VMIFGSM-IKD & 99.9$\star$ & \textbf{86.1 } & \textbf{89.3 } & 86.1  & \textbf{65.6 } & 72.6  & 72.6  & \textbf{87.1 } & \textbf{85.3 } & \textbf{44.7 } & 23.4  & \textbf{71.3 } \\
        % \cline{2-14}
        % & VNIFGSM & \textbf{99.9$\star$} & 86.2  & 89.2  & 87.0  & 68.7  & 74.1  & 73.1  & 87.2  & 86.0  & \textbf{45.0 } & 23.0  & 72.0  \\
        % & VNIFGSM-IKD & 99.8$\star$ & \textbf{86.6 } & \textbf{91.1 } & \textbf{88.2 } & 68.7  & \textbf{75.3 } & \textbf{74.0 } & \textbf{88.8 } & \textbf{88.0 } & 44.3  & \textbf{24.2 } & \textbf{72.9 } \\
        % \hline
        % \multirow{6}{*}{DN121} & SINIFGSM & 81.2  & 100.0$\star$ & 77.0  & 92.7  & 63.0  & 77.7  & 70.6  & 69.8  & 64.6  & 47.8  & 27.8  & 67.2  \\
        % & SINIFGSM-IKD & \textbf{87.3 } & 100.0$\star$ & \textbf{85.5 } & \textbf{94.2 } & \textbf{68.2 } & \textbf{79.5 } & \textbf{75.4 } & \textbf{77.0 } & \textbf{73.0 } & \textbf{49.2 } & \textbf{29.3 } & \textbf{71.9 } \\
        % \cline{2-14}
        % & VMIFGSM & 87.3  & 100.0$\star$ & 84.6  & 92.6  & 73.3  & \textbf{82.6 } & 79.4  & 79.5  & 76.7  & 49.7  & 34.0  & 74.0  \\
        % & VMIFGSM-IKD & \textbf{88.7 } & 100.0$\star$ & \textbf{87.3 } & \textbf{93.8 } & \textbf{74.4 } & 82.4  & \textbf{81.7 } & \textbf{82.5 } & \textbf{78.9 } & \textbf{50.1 } & \textbf{34.9 } & \textbf{75.5 } \\
        % \cline{2-14}
        % & VNIFGSM & 87.7  & 100.0$\star$ & 86.2  & 93.5  & 74.1  & \textbf{83.7 } & 80.6  & 81.1  & 78.7  & 49.6  & 34.1  & 74.9  \\
        % & VNIFGSM-IKD & \textbf{89.2 } & 100.0$\star$ & \textbf{88.0 } & \textbf{93.9 } & \textbf{75.0 } & 82.9  & \textbf{81.3 } & \textbf{82.1 } & \textbf{80.7 } & \textbf{49.8 } & \textbf{34.8 } & \textbf{75.8 } \\
        % \hline
        \multirow{6}{*}{RNX50} & SINIFGSM & 91.0  & \textbf{88.5 } & 99.9$\star$ & 86.8  & 61.2  & \textbf{70.0 } & \textbf{70.1 } & 87.5  & 82.9  & \textbf{39.3 } & \textbf{18.5 } & 69.6  \\
        & SINIFGSM-IKD & \textbf{91.7 } & 87.6  & 99.9$\star$ & \textbf{87.0 } & \textbf{61.9 } & 69.8  & 68.9  & \textbf{87.9 } & \textbf{85.0 } & 38.2  & 18.3  & 69.6  \\
        \cline{2-14}
        & VMIFGSM & \textbf{81.5 } & 77.8  & 97.1$\star$ & \textbf{81.1 } & 59.2  & 66.3  & 66.9  & 77.9  & 75.3  & \textbf{40.0 } & \textbf{23.9 } & 65.0  \\
        & VMIFGSM-IKD & 81.4  & \textbf{78.1 } & \textbf{98.1$\star$} & 80.7  & 59.2  & \textbf{67.2 } & \textbf{67.5 } & \textbf{78.8 } & \textbf{76.6 } & 39.8  & 23.0  & \textbf{65.2 } \\
        \cline{2-14}
        & VNIFGSM & \textbf{82.4 } & \textbf{79.8 } & 97.1$\star$ & \textbf{82.5 } & \textbf{61.9 } & 68.8  & \textbf{68.1 } & \textbf{78.8 } & \textbf{77.2 } & \textbf{41.9 } & \textbf{23.5 } & \textbf{66.5 } \\
        & VNIFGSM-IKD & 82.0  & 79.5  & \textbf{98.0$\star$} & 80.9  & 60.6  & \textbf{70.3 } & 66.9  & 78.4  & 77.1  & 40.3  & 22.3  & 65.8  \\
        \hline
        % \multirow{6}{*}{VGG19BN} & SINIFGSM & 55.3  & 66.0  & 49.5  & 100.0$\star$ & 38.3  & 57.4  & 50.6  & 39.3  & 33.9  & \textbf{31.2 } & \textbf{16.6 } & 43.8  \\
        % & SINIFGSM-IKD & \textbf{61.6 } & \textbf{71.9 } & \textbf{55.5 } & 100.0$\star$ & \textbf{39.3 } & \textbf{59.8 } & \textbf{54.2 } & \textbf{44.7 } & \textbf{40.9 } & 28.9  & 15.5  & \textbf{47.2 } \\
        % \cline{2-14}
        % & VMIFGSM & 69.9  & 80.7  & 63.5  & 100.0$\star$ & 53.4  & 70.9  & 67.5  & 52.3  & 48.3  & 37.1  & \textbf{20.5 } & 56.4  \\
        % & VMIFGSM-IKD & \textbf{73.9 } & \textbf{83.2 } & \textbf{67.6 } & 100.0$\star$ & \textbf{54.2 } & \textbf{71.5 } & \textbf{69.6 } & \textbf{57.4 } & \textbf{53.2 } & \textbf{37.2 } & 19.7  & \textbf{58.8 } \\
        % \cline{2-14}
        % & VNIFGSM & 71.5  & 81.2  & 63.7  & 100.0$\star$ & 53.6  & 71.9  & 68.4  & 54.0  & 49.8  & 37.0  & \textbf{20.7 } & 57.2  \\
        % & VNIFGSM-IKD & \textbf{76.4 } & \textbf{85.3 } & \textbf{71.5 } & 100.0$\star$ & \textbf{56.9 } & \textbf{74.0 } & \textbf{71.7 } & \textbf{62.1 } & \textbf{56.0 } & \textbf{39.3 } & 20.1  & \textbf{61.3 } \\
        % \hline
        % \multirow{6}{*}{IncRes-v2} & SINIFGSM & 64.9  & 76.1  & 62.6  & 85.9  & 99.8$\star$ & 88.0  & 79.6  & 55.7  & 52.4  & 61.2  & 40.9  & 66.7  \\
        % & SINIFGSM-IKD & \textbf{72.9 } & \textbf{82.8 } & \textbf{69.8 } & \textbf{90.5 } & \textbf{99.9$\star$} & \textbf{90.8 } & \textbf{86.4 } & \textbf{64.2 } & \textbf{62.4 } & \textbf{66.0 } & \textbf{44.6 } & \textbf{73.0 } \\
        % \cline{2-14}
        % & VMIFGSM & 63.1  & 72.3  & 63.1  & 81.6  & 98.8$\star$ & 82.8  & 82.4  & 57.0  & 56.2  & 57.2  & 42.7  & 65.8  \\
        % & VMIFGSM-IKD & \textbf{69.7 } & \textbf{77.0 } & \textbf{69.9 } & \textbf{85.2 } & \textbf{98.9$\star$} & \textbf{86.6 } & \textbf{86.6 } & \textbf{63.5 } & \textbf{63.2 } & \textbf{61.0 } & \textbf{46.9 } & \textbf{71.0 } \\
        % \cline{2-14}
        % & VNIFGSM & 66.3  & 75.1  & 65.1  & 82.6  & \textbf{99.5$\star$} & 83.9  & 84.5  & 59.1  & 56.2  & 57.8  & 41.9  & 67.3  \\
        % & VNIFGSM-IKD & \textbf{71.9 } & \textbf{79.1 } & \textbf{72.2 } & \textbf{85.7 } & 99.4$\star$ & \textbf{88.9 } & \textbf{86.7 } & \textbf{66.4 } & \textbf{64.7 } & \textbf{63.0 } & \textbf{46.3 } & \textbf{72.5 } \\
        % \hline
        \multirow{6}{*}{Inc-v3} & SINIFGSM & 56.3  & 71.2  & 53.5  & 83.4  & 67.6  & 100.0$\star$ & 72.5  & 45.7  & 42.6  & 50.2  & \textbf{26.2 } & 56.9  \\
        & SINIFGSM-IKD & \textbf{62.4 } & \textbf{74.4 } & \textbf{57.7 } & \textbf{84.4 } & \textbf{74.7 } & 100.0$\star$ & \textbf{78.1 } & \textbf{50.9 } & \textbf{47.1 } & \textbf{54.7 } & 25.3  & \textbf{61.0 } \\
        \cline{2-14}
        & VMIFGSM & 55.8  & 68.8  & 53.9  & 78.2  & 73.0  & 99.4$\star$ & 73.4  & 48.7  & 43.4  & 52.9  & 29.5  & 57.8  \\
        & VMIFGSM-IKD & \textbf{58.9 } & \textbf{70.7 } & \textbf{54.4 } & \textbf{79.2 } & \textbf{75.0 } & 99.4$\star$ & \textbf{78.4 } & \textbf{51.3 } & \textbf{47.5 } & \textbf{56.7 } & \textbf{30.4 } & \textbf{60.3 } \\
        \cline{2-14}
        & VNIFGSM & 61.3  & 72.2  & 57.5  & 80.2  & 75.2  & 99.7$\star$ & 77.7  & 50.4  & 47.4  & 53.6  & 31.3  & 60.7  \\
        & VNIFGSM-IKD & \textbf{64.7 } & \textbf{76.4 } & \textbf{61.9 } & \textbf{82.9 } & \textbf{79.9 } & \textbf{100.0$\star$} & \textbf{83.2 } & \textbf{56.4 } & \textbf{52.5 } & \textbf{59.0 } & \textbf{33.1 } & \textbf{65.0 } \\
        \hline
        \multirow{6}{*}{Inc-v4} & SINIFGSM & 67.3  & 79.6  & 66.0  & 90.0  & 80.2  & 87.9  & 100.0$\star$ & 61.3  & 57.5  & 59.3  & 34.9  & 68.4  \\
        & SINIFGSM-IKD & \textbf{73.6 } & \textbf{83.0 } & \textbf{71.3 } & \textbf{92.3 } & \textbf{85.7 } & \textbf{90.2 } & 99.9$\star$ & \textbf{66.2 } & \textbf{62.7 } & \textbf{61.0 } & \textbf{35.0 } & \textbf{72.1 } \\
        \cline{2-14}
        & VMIFGSM & 60.9  & 69.7  & 60.0  & 81.3  & 73.6  & 78.9  & \textbf{98.6$\star$} & 55.9  & 54.5  & 50.5  & 29.7  & 61.5  \\
        & VMIFGSM-IKD & \textbf{65.7 } & \textbf{73.1 } & \textbf{65.6 } & \textbf{83.4 } & \textbf{79.6 } & \textbf{82.9 } & 98.5$\star$ & \textbf{60.2 } & \textbf{60.3 } & \textbf{52.0 } & \textbf{31.3 } & \textbf{65.4 } \\
        \cline{2-14}
        & VNIFGSM & 65.3  & 73.5  & 63.8  & 85.0  & 77.6  & 82.2  & 99.5$\star$ & 56.4  & 55.6  & 52.1  & 29.8  & 64.1  \\
        & VNIFGSM-IKD & \textbf{70.3 } & \textbf{77.0 } & \textbf{70.7 } & \textbf{87.3 } & \textbf{82.1 } & \textbf{85.3 } & \textbf{99.8$\star$} & \textbf{64.6 } & \textbf{62.3 } & \textbf{53.5 } & \textbf{31.9 } & \textbf{68.5 } \\
        \hline
        % \multirow{6}{*}{RN101} & SINIFGSM & 90.7  & 87.5  & 90.6  & 88.3  & 64.4  & 71.8  & 71.6  & \textbf{99.9$\star$} & 89.9  & \textbf{39.0 } & 18.8  & 71.3  \\
        % & SINIFGSM-IKD & \textbf{91.4 } & \textbf{88.0 } & \textbf{90.9 } & \textbf{88.4 } & \textbf{65.1 } & \textbf{73.2 } & \textbf{71.9 } & 99.7$\star$ & \textbf{90.4 } & 38.1  & \textbf{19.7 } & \textbf{71.7 } \\
        % \cline{2-14}
        % & VMIFGSM & 78.9  & 74.8  & 77.9  & \textbf{80.0 } & 58.4  & 65.3  & 64.1  & 95.9$\star$ & 78.3  & \textbf{38.2 } & 23.5  & 63.9  \\
        % & VMIFGSM-IKD & \textbf{79.2 } & \textbf{74.9 } & \textbf{78.4 } & 79.5  & \textbf{59.0 } & \textbf{66.3 } & 64.1  & \textbf{96.1$\star$} & \textbf{78.4 } & 37.9  & \textbf{23.7 } & \textbf{64.1 } \\
        % \cline{2-14}
        % & VNIFGSM & 79.9  & 76.0  & 80.5  & \textbf{81.8 } & 59.9  & 67.2  & 64.3  & 96.8$\star$ & 80.4  & 37.6  & \textbf{24.0 } & 65.2  \\
        % & VNIFGSM-IKD & \textbf{81.5 } & \textbf{78.1 } & \textbf{81.4 } & 81.3  & \textbf{60.7 } & \textbf{67.5 } & \textbf{65.2 } & 96.8$\star$ & \textbf{80.7 } & \textbf{39.8 } & 23.3  & \textbf{66.0 } \\
        % \hline
        \multirow{6}{*}{RN152} & SINIFGSM & 91.1  & 87.0  & 88.4  & \textbf{87.4 } & \textbf{65.1 } & 71.8  & \textbf{70.7 } & \textbf{91.4 } & \textbf{99.5$\star$} & \textbf{40.4 } & \textbf{20.1 } & \textbf{71.3 } \\
        & SINIFGSM-IKD & \textbf{91.5 } & \textbf{88.1 } & \textbf{89.2 } & 86.4  & 63.6  & \textbf{71.9 } & 69.6  & 90.7  & 99.2$\star$ & 39.0  & 19.3  & 70.9  \\
        \cline{2-14}
        & VMIFGSM & 78.5  & 72.0  & 76.8  & \textbf{79.0 } & \textbf{58.1 } & 63.5  & \textbf{62.9 } & 77.0  & \textbf{95.8$\star$} & \textbf{40.0 } & 23.4  & 63.1  \\
        & VMIFGSM-IKD & \textbf{78.6 } & \textbf{72.2 } & \textbf{77.7 } & 78.8  & 57.6  & \textbf{64.7 } & 62.0  & \textbf{78.0 } & 95.7$\star$ & 39.7  & \textbf{23.7 } & \textbf{63.3 } \\
        \cline{2-14}
        & VNIFGSM & 81.6  & 76.4  & 81.2  & 82.0  & \textbf{61.1 } & 65.7  & \textbf{66.2 } & 80.3  & 97.4$\star$ & \textbf{40.0 } & 24.0  & 65.9  \\
        & VNIFGSM-IKD & \textbf{82.1 } & \textbf{78.0 } & \textbf{81.3 } & \textbf{82.1 } & 61.0  & \textbf{67.5 } & 65.9  & \textbf{81.5 } & \textbf{98.0$\star$} & 39.9  & \textbf{24.8 } & \textbf{66.4 } \\
        \hline
    \end{tabular}%
    }
  \label{tab:agmasr_appendix}%
\end{table*}%

\Cref{tab:regularization} presents the impact of different IKD methods on the success rate of transfer-based attacks, with ResNet50 serving as the surrogate model.
\begin{table*}[htbp]
  \centering
  \caption{Comparison results of various attack methods under different IKD methods regarding attack success rate (\%). The bold item indicates the best, and the underlined item indicates the suboptimal. An item with $\star$ superscript is a white-box attack; the others are black-box attacks. The AVG column indicates the average attack success rate on black-box models. The source model is ResNet50 and the IKD weight is 0.01.}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|ccccccccccc}
        \hline
        \multirow{2}{*}{Method} & \multicolumn{11}{c}{Target Model} \\
        \cline{2-12}
        & RN50  & DN121 & RNX50 & VGG19BN & IncRes-v2 & Inc-v3 & Inc-v4 & RN101 & RN152 & Inc-v3$adv$ &  IncRes-V2$adv,ens$ \\
        \hline
        MIFGSM & 99.9* & 65.7  & \underline{75.2}  & 73.2  & \underline{38.9}  & 52.6  & 46.0  & 68.2  & 62.1  & 30.6  & 16.0  \\
        MIFGSM-IKD(CE) & 99.9* & \underline{66.6}  & 74.2  & 73.0  & 38.8  & \underline{53.6}  & 46.0  & \underline{69.4}  & \underline{63.6}  & 30.7  & \underline{16.2}  \\
        MIFGSM-IKD(MSE) & 99.9* & 65.4  & 74.1  & \underline{73.6}  & 37.2  & 53.2  & \underline{46.2}  & 68.0  & 62.4  & \underline{31.3}  & 15.9  \\
        MIFGSM-IKD(KL) & 99.9* & \textbf{68.4 } & \textbf{79.4 } & \textbf{76.0 } & \textbf{39.9 } & \textbf{54.4 } & \textbf{49.6 } & \textbf{73.1 } & \textbf{67.6 } & \textbf{32.1 } & \textbf{16.3 } \\
        \hline          
        DIFGSM & \textbf{99.9*} & 82.9  & 88.8  & \underline{84.7}  & \underline{61.7}  & 70.3  & \underline{69.4}  & \underline{86.1}  & \underline{82.1}  & 35.9  & 18.4  \\
        DIFGSM-IKD(CE) & \underline{99.8*} & \underline{83.9}  & \underline{88.9}  & 83.4  & 61.0  & \underline{70.5}  & 68.4  & 83.5  & 80.2  & \underline{38.1}  & 18.4  \\
        DIFGSM-IKD(MSE) & \underline{99.8*} & 83.8  & 88.4  & 83.4  & 60.4  & 70.1  & 69.0  & 83.9  & 80.7  & 36.2  & \textbf{19.0 } \\
        DIFGSM-IKD(KL) & \underline{99.8*} & \textbf{84.9 } & \textbf{90.6 } & \textbf{84.8 } & \textbf{62.9 } & \textbf{70.8 } & \textbf{69.6 } & \textbf{86.5 } & \textbf{83.3 } & \textbf{38.4 } & \underline{18.9}  \\
        \hline          
        TIFGSM & 99.0* & \underline{68.4}  & \underline{65.4}  & \underline{71.3}  & 51.1  & 63.9  & \underline{60.3}  & 55.6  & \textbf{49.8 } & \textbf{52.8 } & \underline{43.0}  \\
        TIFGSM-IKD(CE) & 98.9* & 67.2  & 64.6  & \textbf{71.8 } & \underline{51.6}  & \underline{64.2}  & 59.6  & 54.5  & 47.0  & 50.7  & 41.7  \\
        TIFGSM-IKD(MSE) & \underline{99.1*} & 67.2  & 64.9  & 70.9  & 50.9  & 63.3  & 59.1  & \underline{55.8}  & 47.8  & \underline{51.3}  & 42.2  \\
        TIFGSM-IKD(KL) & \textbf{99.2*} & \textbf{69.1 } & \textbf{67.1 } & 70.8  & \textbf{52.1 } & \textbf{64.5 } & \textbf{61.4 } & \textbf{57.9 } & \underline{49.5}  & \textbf{52.8 } & \textbf{43.8 } \\
        \hline          
        NIFGSM & \underline{99.9*} & 71.6  & 80.0  & 76.5  & 43.4  & 55.2  & \underline{49.2}  & 75.9  & 70.7  & \underline{31.6}  & \underline{16.0}  \\
        NIFGSM-IKD(CE) & \textbf{100.0*} & 71.2  & 80.7  & \underline{77.4}  & 42.3  & \underline{57.2}  & 49.1  & 76.6  & 71.6  & \textbf{32.0 } & \textbf{16.4 } \\
        NIFGSM-IKD(MSE) & \underline{99.9*} & \underline{72.7}  & \underline{81.0}  & 77.2  & \underline{44.2}  & 55.6  & 48.9  & \underline{76.8}  & \underline{71.7}  & 31.1  & \textbf{16.4 } \\
        NIFGSM-IKD(KL) & \textbf{100.0*} & \textbf{76.5 } & \textbf{85.0 } & \textbf{79.3 } & \textbf{44.6 } & \textbf{57.8 } & \textbf{52.5 } & \textbf{81.1 } & \textbf{74.7 } & \textbf{32.0 } & 15.4  \\
        \hline          
        SINIFGSM & \underline{99.9*} & 81.7  & 85.0  & 84.5  & 53.2  & 67.0  & 59.3  & \underline{80.1}  & 75.4  & 37.6  & 17.1  \\
        SINIFGSM-IKD(CE) & \underline{99.9*} & \underline{83.4}  & \underline{85.3}  & 84.7  & \underline{54.3}  & 67.1  & \underline{60.8}  & \underline{80.1}  & 74.9  & 36.5  & \textbf{17.8 } \\
        SINIFGSM-IKD(MSE) & 99.8* & 82.6  & 84.4  & \underline{85.4}  & 53.4  & \underline{67.2}  & 60.5  & 79.2  & \underline{76.6}  & \underline{37.9}  & 17.5  \\
        SINIFGSM-IKD(KL) & \textbf{100.0*} & \textbf{88.5 } & \textbf{91.3 } & \textbf{88.7 } & \textbf{58.0 } & \textbf{69.9 } & \textbf{64.5 } & \textbf{87.9 } & \textbf{84.1 } & \textbf{38.2 } & \underline{17.7}  \\
        \hline          
        VMIFGSM & \textbf{99.9*} & 84.9  & \underline{88.6}  & \textbf{86.5 } & 64.8  & \underline{73.6}  & \textbf{73.5 } & \underline{85.5}  & 84.9  & 43.8  & \textbf{24.2 } \\
        VMIFGSM-IKD(CE) & \textbf{99.9*} & \underline{85.9}  & 88.2  & \underline{86.4}  & \textbf{66.4 } & \textbf{73.7 } & 72.9  & \underline{85.5}  & \textbf{85.5 } & \underline{44.5}  & \underline{23.8}  \\
        VMIFGSM-IKD(MSE) & \underline{99.8*} & \underline{85.9}  & 88.2  & 86.1  & 65.2  & 73.1  & \underline{73.0}  & 85.3  & 85.2  & 44.0  & 23.2  \\
        VMIFGSM-IKD(KL) & \textbf{99.9*} & \textbf{86.1 } & \textbf{89.3 } & 86.1  & \underline{65.6}  & 72.6  & 72.6  & \textbf{87.1 } & \underline{85.3}  & \textbf{44.7 } & 23.4  \\
        \hline          
        VNIFGSM & \textbf{99.9*} & \underline{86.2}  & 89.2  & 87.0  & \textbf{68.7 } & 74.1  & \underline{73.1}  & 87.2  & 86.0  & \textbf{45.0 } & 23.0  \\
        VNIFGSM-IKD(CE) & \textbf{99.9*} & \underline{86.2}  & 89.9  & \underline{87.5}  & 67.5  & \textbf{75.4 } & 73.0  & \underline{87.6}  & \underline{86.5}  & 44.2  & \underline{23.4}  \\
        VNIFGSM-IKD(MSE) & \textbf{99.9*} & 86.0  & \underline{90.5}  & 86.7  & \underline{68.0}  & 74.6  & 72.6  & 87.5  & 85.9  & 44.1  & 23.3  \\
        VNIFGSM-IKD(KL) & \underline{99.8*} & \textbf{86.6 } & \textbf{91.1 } & \textbf{88.2 } & \textbf{68.7 } & \underline{75.3}  & \textbf{74.0 } & \textbf{88.8 } & \textbf{88.0 } & \underline{44.3}  & \textbf{24.2 } \\
        \hline
    \end{tabular}%
    }
  \label{tab:regularization}%
\end{table*}%

~\Cref{tab:ngmweight,tab:agmweight} presents the effect of IKD weight on the transfer-based attack success rate.
\begin{table*}[htbp]
  \centering
  \caption{Comparison results of various attack methods under different IKD weights regarding attack success rate (\%). The bold item indicates the best, and the underlined item indicates the suboptimal. An item with $\star$ superscript is white-box attack; the others are black-box attacks. The source model is ResNet50 and the distillation method is KL divergence.}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|r|ccccccccccc}
        \hline
        \multirow{2}{*}{Method} & \multicolumn{1}{c|}{\multirow{2}{*}{Weight}} & \multicolumn{11}{c}{Target Model} \\
        \cline{3-13}
        &  & RN50  & DN121 & RNX50 & VGG19BN & IncRes-v2 & Inc-v3 & Inc-v4 & RN101 & RN152 & Inc-v3$adv$ &  IncRes-V2$adv,ens$ \\
        \hline
        MIFGSM & /     & \textbf{99.9$\star$} & 65.7  & 75.2  & 73.2  & 38.9  & 52.6  & 46.0  & 68.2  & 62.1  & 30.6  & 16.0 \\
        \multirow{7}{*}{MIFGSM-IKD} & 1000  & 81.2$\star$ & 58.9  & 65.6  & 65.9  & 33.9  & 47.3  & 41.8  & 62.0  & 58.4  & 30.7  & 15.3 \\
        & 100   & \underline{94.2$\star$} & 66.1  & 74.1  & 71.7  & 39.4  & 51.1  & 48.1  & 69.4  & 65.8  & \textbf{32.1 } & 15.5 \\
        & 10    & \textbf{99.9$\star$} & \textbf{69.5 } & 78.5  & \underline{75.7}  & 40.2  & \textbf{54.6 } & 49.2  & 73.3  & \textbf{68.5 } & 31.3  & \underline{16.7} \\
        & 1     & \textbf{99.9$\star$} & \underline{69.1}  & 79.3  & 75.1  & \underline{40.7}  & 53.9  & 48.6  & \textbf{74.3 } & 66.9 & \textbf{32.1 } & 16.5 \\
        & 0.1   & \textbf{99.9$\star$} & 68.5  & 79.3  & \textbf{76.0 } & \textbf{41.0 } & 53.8  & \textbf{49.8 } & 72.4  & 67.4  & 31.1  & 15.7 \\
        & 0.01  & \textbf{99.9$\star$} & 68.4  & \underline{79.4}  & \textbf{76.0 } & 39.9  & \underline{54.4}  & \underline{49.6}  & 73.1  & \underline{67.6}  & \textbf{32.1 } & 16.3 \\
        & 0.001 & \textbf{99.9$\star$} & \textbf{69.5 } & \textbf{79.8 } & 74.6  & 40.5  & 52.6  & 49.2  & \underline{73.3}  & 67.4  & \underline{31.5}  & \textbf{17.1 } \\
        \hline          
        DIFGSM & /     & \textbf{99.9$\star$} & 82.9  & 88.8  & \underline{84.7}  & 61.7  & 70.3  & 69.4  & \underline{86.1}  & 82.1  & 35.9  & 18.4 \\
        \multirow{7}{*}{DIFGSM-IKD} & 1000  & 67.2$\star$ & 58.2  & 61.0  & 64.1  & 43.2  & 51.2  & 47.7  & 58.2  & 57.2  & 30.2  & 14.7 \\
        & 100   & 74.4$\star$ & 63.8  & 67.6  & 68.2  & 47.0  & 54.3  & 52.5  & 64.9  & 62.8  & 31.9  & 16.2 \\
        & 10    & 78.8$\star$ & 68.0  & 70.0  & 71.5  & 50.2  & 56.9  & 56.2  & 67.8  & 66.3  & 31.6  & 17.2 \\
        & 1     & 89.1$\star$ & 76.7  & 81.0  & 77.8  & 56.0  & 63.8  & 61.8  & 77.2  & 75.0  & 34.7  & 16.8 \\
        & 0.1   & 98.1$\star$ & 83.9  & 88.7  & 82.8  & 62.2  & \underline{71.6}  & 69.4  & 85.8  & 82.7  & 37.5  & \underline{18.7} \\
        & 0.01  & \underline{99.8$\star$} & \underline{84.9}  & \textbf{90.6 } & \textbf{84.8 } & \underline{62.9}  & 70.8  & \underline{69.6}  & \textbf{86.5 } & \underline{83.3}  & \textbf{38.4 } & \textbf{18.9 } \\
        & 0.001 & 99.7$\star$ & \textbf{85.0 } & \underline{89.8}  & 84.3  & \textbf{63.7 } & \textbf{72.5 } & \textbf{70.5 } & \textbf{86.5 } & \textbf{83.7 } & \underline{37.6}  & 18.5 \\
        \hline          
        TIFGSM & /     & 99.0$\star$ & 68.4  & 65.4  & 71.3  & 51.1  & 63.9  & 60.3  & 55.6  & \textbf{49.8 } & \textbf{52.8 } & \underline{43.0} \\
        \multirow{7}{*}{TIFGSM-IKD} & 1000  & 66.6$\star$ & 47.0  & 46.6  & 55.1  & 37.0  & 47.9  & 44.1  & 38.8  & 35.1  & 38.4  & 31.2 \\
        & 100   & 73.4$\star$ & 52.1  & 50.8  & 59.1  & 40.5  & 49.8  & 46.4  & 41.5  & 37.8  & 41.3  & 33.1 \\
        & 10    & 78.0$\star$ & 54.4  & 53.3  & 62.2  & 41.6  & 52.3  & 49.7  & 44.1  & 39.1  & 43.6  & 33.8 \\
        & 1     & 88.3$\star$ & 61.6  & 60.1  & 66.0  & 46.4  & 58.9  & 55.0  & 50.3  & 43.5  & 46.9  & 38.5 \\
        & 0.1   & 98.1$\star$ & 68.2  & \underline{66.8}  & \underline{71.4}  & \underline{52.8}  & \underline{64.0}  & 60.2  & 55.4  & 48.4  & 52.3  & 42.7 \\
        & 0.01  & \textbf{99.2$\star$} & \textbf{69.1 } & \textbf{67.1 } & 70.8  & 52.1  & \textbf{64.5 } & \underline{61.4}  & \textbf{57.9 } & \underline{49.5}  & \textbf{52.8 } & \textbf{43.8 } \\
        & 0.001 & \underline{99.1$\star$} & \underline{68.6}  & \underline{66.8}  & \textbf{71.6 } & \textbf{53.3 } & 64.0  & \textbf{61.7 } & \underline{57.5}  & 48.4  & \underline{52.6}  & 42.2 \\
        \hline          
        NIFGSM & /     & \underline{99.9$\star$} & 71.6  & 80.0  & 76.5  & 43.4  & 55.2  & 49.2  & 75.9  & 70.7  & 31.6  & \textbf{16.0 } \\
        \multirow{7}{*}{NIFGSM-IKD} & 1000  & 87.8$\star$ & 66.6  & 74.1  & 71.9  & 40.0  & 51.7  & 47.5  & 71.2  & 65.3  & 30.4  & 15.1 \\
        & 100   & 96.2$\star$ & 72.6  & 80.6  & 75.0  & 43.5  & 55.7  & 51.8  & 77.9  & 72.7  & 31.6  & \underline{15.6} \\
        & 10    & \underline{99.9$\star$} & 75.8  & 83.8  & \textbf{79.8 } & \underline{45.0}  & \textbf{59.0 } & 53.1  & \textbf{81.8 } & \textbf{76.0 } & 31.9  & 14.9 \\
        & 1     & \underline{99.9$\star$} & 75.3  & \underline{84.8}  & 78.5  & \underline{45.0}  & 58.1  & 53.0  & 80.7  & 74.2  & \textbf{32.4 } & 15.5 \\
        & 0.1   & \underline{99.9$\star$} & \underline{76.4}  & 84.6  & 79.0  & 44.1  & 57.9  & \underline{53.7}  & 80.8  & \textbf{76.0 } & 31.6  & 15.4 \\
        & 0.01  & \textbf{100.0$\star$} & \textbf{76.5 } & \textbf{85.0 } & \underline{79.3}  & 44.6  & 57.8  & 52.5  & \underline{81.1}  & 74.7  & \underline{32.0}  & 15.4 \\
        & 0.001 & \textbf{100.0$\star$} & 75.2  & \textbf{85.0 } & 79.0  & \textbf{45.9 } & \underline{58.8}  & \textbf{54.2 } & 79.1  & \underline{75.7}  & 31.8  & \textbf{16.0 } \\
        \hline
    \end{tabular}%
    }
  \label{tab:ngmweight}%
\end{table*}%

\begin{table*}[htbp]
  \centering
  \caption{Evaluation results of the advanced attacks under different IKD weights regarding attack success rate (\%). The bold item indicates the best, and the underlined item indicates the suboptimal. An item with $\star$ superscript is white-box attack, the others are black-box attacks. The source model is ResNet50 and the IKD method is KL divergence.}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|r|ccccccccccc}
        \hline
        \multirow{2}{*}{Method} & \multicolumn{1}{c|}{\multirow{2}{*}{Weight}} & \multicolumn{11}{c}{Target Model} \\
        \cline{3-13}          
        & & RN50  & DN121 & RNX50 & VGG19BN & IncRes-v2 & Inc-v3 & Inc-v4 & RN101 & RN152 & Inc-v3$adv$ &  IncRes-V2$adv,ens$ \\
        \hline
        SINIFGSM & /     & \underline{99.9$\star$} & 81.7  & 85.0  & 84.5  & 53.2  & 67.0  & 59.3  & 80.1  & 75.4  & \underline{37.6}  & 17.1 \\
        \multicolumn{1}{l|}{\multirow{7}{*}{SINIFGSM-IKD}} & 1000  & 97.2$\star$ & 84.1  & 84.5  & 87.5  & 52.8  & 66.9  & 62.3  & 82.3  & 78.3  & 35.6  & 17.6 \\
        & 100   & 97.2$\star$ & 82.8  & 85.3  & 87.0  & 51.6  & 65.4  & 62.2  & 81.7  & 77.2  & 34.7  & 17.0 \\
        & 10    & 97.7$\star$ & 84.2  & 85.6  & 87.2  & 53.8  & 66.5  & 63.5  & 81.6  & 77.3  & 35.2  & 16.6 \\
        & 1     & \underline{99.9$\star$} & 85.5  & 88.8  & 88.4  & 55.1  & 69.0  & \textbf{65.8 } & 84.8  & 81.1  & 36.1  & 17.0 \\
        & 0.1   & 99.8$\star$ & 85.9  & 90.0  & 87.7  & 57.6  & \textbf{70.3 } & 64.2  & \underline{87.0}  & 82.7  & 36.3  & \textbf{18.4 } \\
        & 0.01  & \textbf{100.0$\star$} & \textbf{88.5 } & \underline{91.3}  & \textbf{88.7 } & \textbf{58.0 } & \underline{69.9}  & 64.5  & \textbf{87.9 } & \underline{84.1}  & \textbf{38.2 } & 17.7 \\
        & 0.001 & \underline{99.9$\star$} & \underline{87.2}  & \textbf{91.7 } & \underline{88.5}  & \underline{57.9}  & \textbf{70.3 } & \underline{65.5}  & \textbf{87.9 } & \textbf{84.5 } & 37.5  & \underline{18.0} \\
        \hline          
        VMIFGSM & /     & \textbf{99.9$\star$} & 84.9  & 88.6  & \textbf{86.5 } & 64.8  & \textbf{73.6 } & \underline{73.5}  & 85.5  & 84.9  & 43.8  & \textbf{24.2 } \\
        \multirow{7}{*}{VMIFGSM-IKD} & 1000  & 91.0$\star$ & 78.0  & 82.7  & 79.2  & 57.9  & 65.4  & 64.7  & 80.0  & 78.0  & 38.3  & 19.4 \\
        & 100   & 98.2$\star$ & 82.6  & 87.5  & 83.8  & 59.7  & 67.9  & 66.5  & 84.7  & 82.9  & 39.3  & 20.1 \\
        & 10    & \underline{99.7$\star$} & 83.4  & 88.7  & 83.5  & 60.3  & 68.0  & 68.3  & 86.7  & 84.6  & 38.7  & 20.4 \\
        & 1     & \underline{99.7$\star$} & 83.9  & \underline{90.5}  & 84.5  & 61.8  & 69.0  & 68.6  & 86.5  & 84.5  & 40.2  & 20.4 \\
        & 0.1   & \textbf{99.9$\star$} & 85.0  & \textbf{90.6 } & 85.4  & 64.3  & 71.0  & 70.8  & 86.9  & 84.9  & 42.5  & 21.9 \\
        & 0.01  & \textbf{99.9$\star$} & \underline{86.1}  & 89.3  & 86.1  & \underline{65.6}  & 72.6  & 72.6  & \underline{87.1}  & \underline{85.3}  & \textbf{44.7 } & \underline{23.4} \\
        & 0.001 & \textbf{99.9$\star$} & \textbf{86.9 } & 89.1  & \underline{86.2}  & \textbf{66.9 } & \underline{73.3}  & \textbf{73.6 } & \textbf{87.3 } & \textbf{85.9 } & \underline{44.4}  & 23.3 \\
        \hline          
        VNIFGSM & /     & \textbf{99.9$\star$} & 86.2  & 89.2  & 87.0  & \textbf{68.7 } & 74.1  & 73.1  & 87.2  & 86.0  & \underline{45.0}  & 23.0 \\
        \multirow{7}{*}{VNIFGSM-IKD} & 1000  & 93.8$\star$ & 83.5  & 87.2  & 83.4  & 62.1  & 70.9  & 70.4  & 85.5  & 82.8  & 40.1  & 18.3 \\
        & 100   & 98.3$\star$ & 86.3  & 92.1  & 85.6  & 64.3  & 72.3  & 72.2  & 89.1  & 86.2  & 39.8  & 19.4 \\
        & 10    & 99.7$\star$ & 87.0  & \textbf{92.6 } & 86.6  & 65.5  & 74.0  & 72.0  & 89.6  & 87.8  & 40.5  & 19.1 \\
        & 1     & \underline{99.8$\star$} & \textbf{87.6 } & \underline{92.5}  & 87.4  & 66.2  & 73.8  & 73.6  & \textbf{90.4 } & \underline{88.3}  & 40.2  & 20.6 \\
        & 0.1   & \textbf{99.9$\star$} & \underline{87.4}  & 92.2  & \underline{87.7}  & 68.3  & \textbf{76.3 } & 73.9  & \underline{90.1}  & \textbf{88.9 } & 42.2  & 21.8 \\
        & 0.01  & \underline{99.8$\star$} & 86.6  & 91.1  & \textbf{88.2 } & \textbf{68.7 } & \underline{75.3}  & \underline{74.0}  & 88.8  & 88.0  & 44.3  & \textbf{24.2 } \\
        & 0.001 & 99.7$\star$ & 87.2  & 90.9  & \underline{87.7}  & \underline{68.5}  & 75.0  & \textbf{74.5 } & 88.1  & 87.4  & \textbf{45.4 } & \underline{23.5} \\
        \hline
    \end{tabular}%
    }
  \label{tab:agmweight}%
\end{table*}%

\end{document}

