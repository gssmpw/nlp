
\section{Related works}

\paragraph{LLM alignment.}
Pretrained LLMs demonstrate remarkable capabilities across a broad spectrum of tasks \citep{brown2020language}.
Their performance at downstream tasks, such as conversational modeling, is significantly enhanced through alignment with human preferences \citep{ouyang2022training, bai2022training}. 
RLHF \citep{christiano2017deep} has emerged as a foundational framework for this alignment, typically involving learning a reward function via a preference model, often using the Bradley-Terry model \citep{bradley1952rank}, and tuning the LLM using reinforcement learning (RL) to optimize this reward. 
Despite its success, RLHF's practical implementation is notoriously complex, requiring multiple LLMs, careful hyperparameter tuning, and navigating challenging optimization landscapes.

Recent research has focused on simplifying this process. A line of works studies the direct alignment algorithms \citep{zhao2023slic, rafailov2024direct, azar2024general}, which directly optimize the LLM in a supervised manner without first constructing a separate reward model. In particular, the representative DPO \citep{rafailov2024direct} attracts significant attention in both academia and industry. After these, SimPO \citep{meng2024simpo} simplifies DPO by using length regularization in place of a reference model. 
% However, these approaches are primarily offline and can suffer from performance degradation when facing distribution shifts during deployment, yielding poorer generalization.
% Iterative DPO \citep{xiong2024iterative} attempts to address this by enabling iterative optimization for improved alignment.

% These challenges can be potentially tackled by connecting LLM alignment with IR.
Although LLMs are adopted for IR \citep{tay2022transformer}, there is a lack of study to improve direct LLM alignment with IR principles.
% While significant progress has been made in LLM alignment, the connection with IR remains largely unexplored.
% However, LiPO does not explore the potential of online optimization methods inspired by IR. 
This paper fills this gap by establishing a systematic link between LLM alignment and IR methodologies, and introducing a novel iterative LLM alignment approach that leverages insights from retriever optimization to advance the state of the art.
The most related work is LiPO \citep{liu2024lipo}, which applies learning-to-rank objectives.
% However, LiPO focuses on offline settings and it is unclear how to obtain list-wise data for it.
However, LiPO relies on off-the-shelf listwise preference data, which is hard to satisfy in practice.

\paragraph{Language models for information retrieval.}
Language models (LMs) have become integral to modern IR systems \citep{zhu2023large}, particularly after the advent of pretrained models like BERT \citep{kenton2019bert}.  
A typical IR pipeline employs retrievers and rerankers, often based on dual-encoder and cross-encoder architectures, respectively \citep{humeau2019poly}. 
Dense Passage Retrieval (DPR) \citep{karpukhin2020dense} pioneered the concept of dense retrieval, laying the groundwork for subsequent research. 
Building on DPR, studies have emphasized the importance of hard negatives in training \citep{zhan2021optimizing, qu2020rocketqa} and the benefits of online retriever optimization \citep{xiong2020approximate}.

In the realm of reranking, \citep{nogueira2019passage} were among the first to leverage pretrained language models for improved passage ranking. 
This was followed by MonoT5 \citep{nogueira2020document}, which scaled rerankers using large encoder-decoder transformer architectures, and RankT5 \citep{zhuang2023rankt5}, which introduced pairwise and listwise ranking objectives. 
Recent work has also highlighted the importance of candidate list preprocessing before reranking \citep{meng2024ranked}.

Despite the pervasive use of LMs in IR, the interplay between LLM alignment and IR paradigms remains largely unexplored. 
This work aims to bridge this gap, establishing a strong connection between LLM alignment and IR, and leveraging insights from both fields to advance our understanding of LLM alignment from an IR perspective.
