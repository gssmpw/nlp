
\newpage
\appendix
% \onecolumn

% \section{Pseudo-code for \Ours}

\section{LLM inference strategy and IR pipelines}

\begin{table}[h]
\caption{Correspondence between LLM inference and IR pipelines.}
  \label{tb:llm-retriever-reranker}
  \centering
%   \small
  \scalebox{0.8}{\begin{tabular}{lccc}
    \toprule
    Method & Retriever & Reranker & Pipeline       \\
    \midrule
    Greedy decoding     & LLM &  $\emptyset$ & Retriever-only  \\
    \midrule
    Best-of-N \citep{stiennon2020learning} & LLM & Reward model & Retriever-reranker  \\
    \midrule
    Majority voting  \citep{wang2022self}  & LLM & Majority & Retriever-reranker  \\
    \midrule
    Iterative refinement \citep{madaan2024self} & LLM & $\emptyset$ & Iterative retrieval  w. query rewriting \\
    \bottomrule
  \end{tabular}}
\end{table}


\section{How can SFT and preference optimization help the LLM from an IR perspective?}\label{apx:sft-rlhf-empirical}


We assess how well LLMs perform at two tasks: fine-grained reranking (using greedy decoding accuracy) and coarse-grained retrieval (using Recall@$N$).  
We focus on how SFT and DPO, affect these abilities.  
Using the Mistral-7b model, we evaluate on the GSM8k and MATH datasets with two approaches: SFT-only, and SFT followed by DPO (SFT $\rightarrow$ DPO).

In the SFT phase, the model is trained directly on correct answers. 
For DPO, we generate 20 responses per prompt and created preference pairs by randomly selecting one correct and one incorrect response.  
We use hyperparameter tuning and early stopping to find the best model checkpoints (see Appendix \ref{apx:sec:sft-rlhf} for details).


\begin{table}[h]
\caption{Retrieval (Recall@N) and reranking (greedy accuracy) metrics across dataset and training strategies, with Mistral-7b as the LLM. 0.7 is used as the temperature. Recall@N can also be denoted as pass@N.}\label{tb:sft-rlhf-result}
\vskip 1em
\centering
\small
\begin{tabular}{llcccc}
    \toprule
     & Metric & \textbf{init model} & \textbf{SFT} & \textbf{SFT $\rightarrow$ DPO} \\
    \midrule
    \multirow{4}{*}{\rotatebox{90}{GSM8K}} 
    & Greedy Acc & 0.4663 & 0.7680 & 0.7991  \\
    & Recall@20 & 0.8347 & 0.9462 & 0.9545  \\
    & Recall@50 & 0.9090 & 0.9629 & 0.9727  \\
    & Recall@100 & 0.9477 & 0.9735 & 0.9826   \\
    \midrule
    \multirow{4}{*}{\rotatebox{90}{Math}} 
    & Greedy Acc & 0.1004 & 0.2334 & 0.2502 \\
    & Recall@20 & 0.2600 & 0.5340 & 0.5416  \\
    & Recall@50 & 0.3354 & 0.6190 & 0.6258  \\
    & Recall@100 & 0.4036 & 0.6780 & 0.6846  \\
    \bottomrule
\end{tabular}
\end{table}

The results are shown in Table \ref{tb:sft-rlhf-result}.  
We observe that both SFT and DPO improve both retrieval and reranking, with SFT being more effective. Adding DPO after SFT further improves performance on both tasks.  
This is consistent with information retrieval principles that both direct retriever optimization and reranker-retrieval distillation can enhance the retriever performance, while the latter on top of the former can further improve the performance. Further discussions can be found in Appendices \ref{apx:discuss1} and \ref{apx:discuss2}.


\section{Discussion on the connection and difference between SFT and direct retriever optimization}\label{apx:discuss1}

As discussed in Section \ref{sec:llm-tuning-retriever}, the direct retriever optimization goal with InfoNCE is shown as:
\begin{gather*}
    \max \log P(d_{\text{gold}}|q) = \max \log \frac{\text{Enc}_d(d_{\text{gold}}) \cdot\text{Enc}_q(q)}{\sum^{|C|}_{j=1} \text{Enc}_d(d_j) \cdot\text{Enc}_q(q)},
\end{gather*}
while the SFT optimization goal is shown as:
\begin{gather}
    \max \log P(y_{\text{gold}}|x) = \max \log \prod^{|y_{\text{gold}}|}_i P(y_{\text{gold}}(i)|z_i) 
    = \max \sum^{|y_{\text{gold}}|}_i \log \frac{\text{Emb}(y_{\text{gold}}(i)) \cdot\text{LLM}(z_i)}{\sum^{|V|}_{j=1} \text{Emb}(v_j) \cdot\text{LLM}(z_i)}. \label{apx:eq:sft}
\end{gather}

As a result, the SFT objective can be seen as a summation of multiple retrieval optimization objectives, where $\text{LLM}(\cdot)$ and word embedding $\text{Emb}(\cdot)$ are query encoder and passage encoder respectively.

However, for direct retriever optimization with InfoNCE, $\text{Enc}_d(\cdot)$ is usually a large-scale pretrained language model which is computationally expensive on both time and memory.
In this case, it is unrealistic to calculate the $\text{Enc}_d(d_j)$ for all $d_j\in C$, when $C$ is large, because of the time constrain and GPU memory constrain.
As a result, a widely-adopted technique is to adopt ``in-batch negatives'' with ``hard negatives'' to estimate the $\log P(d_{\text{gold}}|q)$ function:
\begin{gather*}
    \max \log P(d_{\text{gold}}|q) = \max \log \frac{\text{Enc}_d(d_{\text{gold}}) \cdot\text{Enc}_q(q)}{\sum^{|C|}_{j=1} \text{Enc}_d(d_j) \cdot\text{Enc}_q(q)} \\
    \sim \max \log \frac{\text{Enc}_d(d_{\text{gold}}) \cdot\text{Enc}_q(q)}{\sum^{|B|}_{i=1} \text{Enc}_d(d_i) \cdot\text{Enc}_q(q) + \sum^{|H|}_{j=1} \text{Enc}_d(d_j) \cdot\text{Enc}_q(q)},
\end{gather*}
where $B$ is the in-batch negative set and $H$ is the hard negative set.
Note that $B\bigcup H \subset C$.
This objective is more efficient to optimize but is not the original optimization goal. As a result, the learned model after direct retriever optimization is not optimal.
It is also found that the hard negatives $H$ is the key to estimate the original optimization goal \citep{zhan2021optimizing}.
Thus, reranker-retriever distillation can further improve the retriever by introducing more hard negatives.

On the other hand, LLM optimization, as shown in Eq. (\ref{apx:eq:sft}), can be seen as a summation of multiple retrieval optimization function.
In each retrieval step, the passage can be seen as a token and the corpus is the vocabulary space $V$.
Given that the passage encoder $\text{Emb}(\cdot)$ (word embedding) here is cheap to compute and the vocabulary space $V$ ($<$100k) is usually not as large as $C$ ($>$1M) in IR, the objective in Eq. (\ref{apx:eq:sft}) can be directly optimized without any estimation.
In this case, the LLM as a retriever is more sufficiently trained compared with the retriever training in IR.


\section{Discussion on the connection and difference between preference optimization and reranker-retriever distillation}\label{apx:discuss2}

As discussed in Section \ref{sec:llm-tuning-retriever}, preference optimization with an online reward model $f_{\text{reward-model}}(\cdot) \overset{r}{\rightarrow} \text{data} \overset{g(\cdot)}{\rightarrow}  f_{\text{LLM}}(\cdot)$ can be seen as a reranker to retriever distillation process $f_{\text{rerank}}(\cdot) \overset{r}{\rightarrow} \text{data}\overset{g(\cdot)}{\rightarrow}   f_{\text{retrieval}}(\cdot)$, where the reward model is the reranker (\textit{i.e.}, cross-encoder) and the LLM is the retriever (\textit{i.e.}, bi-encoder).

However, there are two slight differences here:
\begin{itemize}[leftmargin=*]
\item The LLM after SFT is more sufficiently trained compared to a retriever after direct optimization. As discussed in Appendix \ref{apx:discuss1}, the SFT optimization function is not an estimated retriever optimization goal compared with the direct retrieval optimization. As a result, the LLM after SFT is suffienctly trained. In this case, if the reward model (reranker) cannot provide information other than that already in the SFT set (\textit{e.g.}, using the SFT prompts), this step may not contribute to significant LLM capability improvement.
\item The reward model may introduce auxiliary information than the reranker in IR. For a reranker in IR, it captures a same semantic with the retriever: semantic similarity between the query and the passage. However, in LLM post-training, the goal and data in SFT and preference optimization can be different. For example, the SFT phase could have query/response pairs which enable basic chat-based retrieval capability for the LLM. While the reward model may contain some style preference information or safety information which do not exist in SFT data. In this case, the preference optimization which is the reranker to retriever distillation step could also contribution to performance improvement.
\end{itemize}


\section{Evaluate LLMs as retrievers}\label{apx:llm-as-retriever}

In addition to Mathstral-7b-it on GSM8K in Figure \ref{fig:mathstral-gsm8k-infer}, we conduct extensive experiments to both Mistral-7b-it and Mathstral-7b-it on GSM8K and MATH. The results are shown in Figure \ref{apx:fig:empirical-llm-retriever}.
We have similar findings as in Figure \ref{fig:mathstral-gsm8k-infer} that:
(1) As $N$ increases, Recall@$N$ improves significantly, indicating that retrieving a larger number of documents increases the likelihood of including a correct one within the set.
(2) For smaller values of $N$ (e.g., $N=1$), lower temperatures yield higher Recall@$N$. This is because lower temperatures reduce response randomness, favoring the selection of the most relevant result.
(3) Conversely, for larger $N$ (e.g., $N>10$), higher temperatures enhance Recall@$N$. Increased temperature promotes greater response diversity, which, when combined with a larger retrieval set, improves the chances of capturing the correct answer within the results.

\begin{figure*}[h]
    \centering
    \subfigure[Mistral-7b-it on GSM8k]{\includegraphics[width=0.45\textwidth]{figure/LLM_alignment_gsm8k_mathstral7b_infer.pdf}}
    \subfigure[Mistral-7b-it on GSM8k]{\includegraphics[width=0.45\textwidth]{figure/LLM_alignment_gsm8k_mistral7b_infer.pdf}}
    \subfigure[Mathstral-7b-it on MATH]{\includegraphics[width=0.45\textwidth]{figure/LLM_alignment_math_mathstral7b_infer.pdf}} 
    \subfigure[Mistral-7b-it on MATH]{\includegraphics[width=0.45\textwidth]{figure/LLM_alignment_math_mistral7b_infer.pdf}}
    % \vspace{-0.1in}
    \vskip -1em
    \caption{Evaluate the LLM as a retriever with Recall@N (Pass@N). As the number (N) of retrieved responses increases, the retrieval recall increases. The higher the temperature is, the broader spectrum the retrieved responses are, and thus the higher the recall is.}\label{apx:fig:empirical-llm-retriever}
\end{figure*}


% \subsection{How SFT and RLHF benefit the LLM retriever?}\label{apx:sft-rlhf}

% In addition to the experiments with Gemma-1-7b-it in Table \ref{tb:sft-rlhf-result}, we also conduct experiments to study the effect of SFT and DPO on Deepseek-math-7b-base model \citep{shao2024deepseekmath}.
% The results on MATH dataset are shown in Table \ref{apx:tb:sft-rlhf-result}, where we have similar discovery with that in Table \ref{tb:sft-rlhf-result}:
% (1) Both SFT and DPO can improve the retrieval capability of the LLM, while SFT is more effective.
% (2) On top of SFT, DPO can slightly improve the reranking capability (greedy accuracy) but not the general retrieval capability.

% \begin{table}[h]
% \caption{Retrieval (Recall@N) and reranking (greedy accuracy) metrics across dataset and training strategies. LLM: Deepseek-math-7b. Temperature: 0.7. Recall@N can also be denoted as pass@N.}\label{apx:tb:sft-rlhf-result}
% \vskip 1em
% \centering
% \scalebox{0.8}{
% \begin{tabular}{llcccc}
%     \toprule
%      & Metric & \textbf{init model} & \textbf{DPO} & \textbf{SFT} & \textbf{SFT $\rightarrow$ DPO} \\
%     \midrule
%     \multirow{4}{*}{\rotatebox{90}{Math}} 
%     & Greedy Acc & 0.0972 & 0.1164 & 0.3078 & 0.312 \\
%     & Recall@20 & 0.4914 & 0.5136 & 0.6524 & 0.6558 \\
%     & Recall@50 & 0.6058 & 0.6278 & 0.7332 & 0.736 \\
%     & Recall@100 & 0.6728 & 0.6976 & 0.7844 & 0.7828 \\
%     \bottomrule
% \end{tabular}}
% \end{table}



\section{\Ours retriever optimization objective}\label{apx:proofs}

We provide the proof for different variants of \Ours's objective functions.

\subsection{Contrastive ranking}\label{apx:proof:contrastive}

\begin{theorem}
Let \( x \) be a prompt and \( (y_w, y^{(1)}_l, ..., y^{(m)}_l)  \) be the responses for \( x \) under the contrastive assumption (Eq.(\ref{eq:contrastive-assumption})).
Then the objective function to learn the LLM \( \pi_\theta \):
\end{theorem}

\begin{equation}
    \begin{aligned}
    \mathcal{L}_{\text{con}} = -\mathbb{E} & \biggl[
    \log \frac{\exp\bigl(\gamma(y_w \mid x)\bigr)}{
        \exp\bigl(\gamma(y_w \mid x)\bigr) + \sum_{i=1}^m \exp\bigl(\gamma(y_l^{(i)} \mid x)\bigr)}
    \biggr], \\
    \text{where } &\quad \gamma(y \mid x) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}.
\end{aligned}\label{eq:contrastive}
\end{equation}

\textit{Proof.}
From \citep{rafailov2024direct}, we know that
\begin{gather}
    r(x, y) = \beta \text{log} \frac{\pi_{\text{llm}}(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \text{log} Z,
\end{gather}
where $Z = \sum_{y'} \pi_{\text{ref}}(y'|x) \text{exp}(\frac{1}{\beta} r(x, y'))$.

Then,
\begin{equation}\label{eq:1-n}
\begin{aligned}
\mathbb{P}\text{r}(y_w & \succeq y^{(1)}_l, ..., y_w \succeq y^{(m)}_l) 
= \text{softmax}(r(x, y_w)) \\
&= \frac{\text{exp}(r(x,y_w))}{\text{exp}(r(x,y_w)) + \sum^m_{i=1}\text{exp}(r(x,y^{(i)}_l))} \\
&= \frac{1}{1 + \sum^m_{i=1}\text{exp}(r(x,y^{(i)}_l)-r(x,y_w))} \\
&= \frac{1}{1 + \sum^m_{i=1}\text{exp}(\gamma(y^{(i)}_l \mid x) + \beta \text{log} Z - \gamma(y_w \mid x) - \beta \text{log} Z)} \\
&= \frac{1}{1 + \sum^m_{i=1}\text{exp}(\gamma(y^{(i)}_l \mid x) - \gamma(y_w \mid x))} \\
&= \frac{\exp\bigl(\gamma(y_w \mid x)\bigr)}{
        \exp\bigl(\gamma(y_w \mid x)\bigr) + \sum_{i=1}^m \exp\bigl(\gamma(y_l^{(i)} \mid x)\bigr)}
\end{aligned}
\end{equation}

We can learn $\pi_\theta$ by maximizing the logarithm-likelihood: 
\begin{gather}
\max \log \mathbb{P}\text{r}(y_w \succeq y^{(1)}_l, \dots, y_w \succeq y^{(m)}_l) \Leftrightarrow 
\min - \log \mathbb{P}\text{r}(y_w \succeq y^{(1)}_l, \dots, y_w \succeq y^{(m)}_l) = \mathcal{L}, \\
 \therefore \mathcal{L}_{\text{con}} = -\mathbb{E} \biggl[
    \log \frac{\exp\bigl(\gamma(y_w \mid x)\bigr)}{
        \exp\bigl(\gamma(y_w \mid x)\bigr) + \sum_{i=1}^m \exp\bigl(\gamma(y_l^{(i)} \mid x)\bigr)}
    \biggr], \\
\text{where} \quad \gamma(y \mid x) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}.
\end{gather}



\subsection{LambdaRank ranking}\label{apx:proof:lambdarank}

\begin{theorem}
Let \( x \) be a prompt and \( (y_1, ..., y_m)  \) be the responses for \( x \) under the LambdaRank assumption (Eq.(\ref{eq:lambdarank-assumption})).
Then the objective function to learn the LLM \( \pi_\theta \):
\end{theorem}

% \begin{gather}
%     \mathcal{L}_{\text{lamb}}=-\mathbb{E}\;\biggl[ \sum_{1<i<j<m}
%   w_{ij}\log \sigma\Bigl(
%      \gamma(y_i \mid x)-
%      \gamma(y_j \mid x)
%   \Bigr)
% \biggr]
% \end{gather}
\begin{gather}
    \mathcal{L}_{\text{lamb}}=-\mathbb{E}\;\biggl[ \sum_{1<i<j<m}
   \log \sigma\Bigl(
     \gamma(y_i \mid x)-
     \gamma(y_j \mid x)
   \Bigr)
\biggr].
\end{gather}
% where $w_{ij}$ is an adjustable weight.

\textit{Proof.}
\begin{equation}
\begin{aligned}
\mathbb{P}\text{r}(y_1 & \succeq ... \succeq y_m)
= \prod_{1<i<j<m} \sigma(r(x,y_i) - r(x,y_j)) \\
&= \prod_{1<i<j<m} \sigma(\gamma(x,y_i) + \beta \text{log} Z - \gamma(x,y_j) - \beta \text{log} Z)  \\
&= \prod_{1<i<j<m} \sigma(\gamma(y_i \mid x)-
     \gamma(y_j \mid x)).
\end{aligned}
\end{equation}

We can learn $\pi_\theta$ by maximizing the logarithm-likelihood: 
\begin{gather}
\max \log \mathbb{P}\text{r}(y_w \succeq y^{(1)}_l, \dots, y_w \succeq y^{(m)}_l) \Leftrightarrow 
\min - \log \mathbb{P}\text{r}(y_w \succeq y^{(1)}_l, \dots, y_w \succeq y^{(m)}_l) = \mathcal{L}, \\
 \therefore \mathcal{L}_{\text{lamb}}=-\mathbb{E}\;\biggl[ \sum_{1<i<j<m}
   \log \sigma\Bigl(
     \gamma(y_i \mid x)-
     \gamma(y_j \mid x)
   \Bigr)
\biggr], \\
\text{where} \quad \gamma(y \mid x) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}.
\end{gather}
% $w_{ij}$ can be added to control the weight of each pair in the candidate list.


\subsection{ListMLE ranking}\label{apx:proof:listmle}

\begin{theorem}
Let \( x \) be a prompt and \( (y_1, ..., y_m)  \) be the responses for \( x \) under the ListMLE assumption (Eq.(\ref{eq:listmle-assumption})).
Then the objective function to learn the LLM \( \pi_\theta \):
\end{theorem}

\begin{equation}
\begin{aligned}
    \mathcal{L}_{\text{lmle}} &= -\mathbb{E} \biggl[
    \sum^m_{i=1} \log \frac{\exp\bigl(\gamma(y_i \mid x)\bigr)}{
        \exp\bigl(\gamma(y_i \mid x)\bigr) + \sum_{j=i}^m \exp\bigl(\gamma(y_j \mid x)\bigr)}
    \biggr].
\end{aligned}
\end{equation}

\textit{Proof.}
From Eq.(\ref{eq:1-n}),
\begin{gather}
\begin{aligned}
    \mathbb{P}\text{r}(y_1 & \succeq ... \succeq y_m) = \prod^m_{i=1} \mathbb{P}\text{r}(y_i \succeq y_{i+1}, ..., y_i \succeq y_m)  \\
    & = \prod^m_{i=1} \frac{\text{exp}(\gamma(y_i \mid x))}{\text{exp}(\gamma(y_i \mid x)) + \sum^m_{j=i+1}\text{exp}(\gamma(y_j \mid x))}
\end{aligned}.
\end{gather}
% The derivation above uses the result from Eq.(\ref{eq:1-n}).

We can learn $\pi_\theta$ by maximizing the logarithm-likelihood: 
\begin{gather}
\max \log \mathbb{P}\text{r}(y_w \succeq y^{(1)}_l, \dots, y_w \succeq y^{(m)}_l) \Leftrightarrow 
\min - \log \mathbb{P}\text{r}(y_w \succeq y^{(1)}_l, \dots, y_w \succeq y^{(m)}_l) = \mathcal{L}, \\
 \therefore \mathcal{L}_{\text{lmle}} = -\mathbb{E} \biggl[
    \sum^m_{i=1} \log \frac{\exp\bigl(\gamma(y_i \mid x)\bigr)}{
        \exp\bigl(\gamma(y_i \mid x)\bigr) + \sum_{j=i}^m \exp\bigl(\gamma(y_j \mid x)\bigr)}
    \biggr], \\
\text{where} \quad \gamma(y \mid x) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}.
\end{gather}


\section{Baselines}\label{apx:sec:baselines}

We conduct detailed illustrations on the baselines compared with \Ours in Section \ref{sec:main-result} below.

\begin{itemize}[leftmargin=*]
  \item RRHF \citep{yuan2023rrhf} scores responses via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss.
  \item SLiC-HF \citep{zhao2023slic} proposes a sequence likelihood calibration method which can learn from human preference data.
  \item DPO \citep{guo2024direct} simplifies the PPO \citep{ouyang2022training} algorithms into an offline direct optimization objective with the pairwise Bradley-Terry assumption.
  \item IPO \citep{azar2024general} theoretically grounds pairwise assumption in DPO into a pointwise reward.
  \item CPO \citep{xu2024contrastive} adds a reward objective with sequence likelihood along with the SFT objective.
  \item KTO \citep{ethayarajh2024kto} adopts the Kahneman-Tversky model and proposes a method which directly maximizes the utility of generation instead of the likelihood of the preferences.
  \item RDPO \citep{park2024disentangling} modifies DPO by including an additional regularization term to disentangle the influence of length.
  \item SimPO \citep{meng2024simpo} further simplifies the DPO objective by using the average log probability of a sequence as the implicit reward and adding a target reward margin to the Bradley-Terry objective.
  \item Iterative DPO \citep{xiong2024iterative} identifies the challenge of offline preference optimization and proposes an iterative learning framework.
\end{itemize}


\section{Experiment settings}\label{apx:sec:main-result-setting}

\subsection{Table \ref{tab:main-performance}}\label{apx:sec:main}

We conduct evaluation on two widely used benchmark: AlpacaEval2 \citep{dubois2024length} and MixEval \citep{ni2024mixeval}.
We consider two base models: Mistral-7b-base and Mistral-7b-it. For Mistral-7b-base, we first conduct supervised finetuning following \citet{meng2024simpo} before the preference optimization.

The performance scores for offline preference optimization baselines are from SimPO \citep{meng2024simpo}.
To have a fair comparison with these baselines, we adopt the same off-the-shelf reward model \citep{jiang2023llm} as in SimPO for the iterative DPO baseline and \Ours.

For the iterative DPO baseline, we generate 2 responses for each prompt, score them with the off-the-shelf reward model and construct the preference pair data to tune the model.

For \Ours (contrastive $\mathcal{L}_{\text{con}}$), we generate 10 responses each iteration and score them with the reward model. The top-1 ranked response and the bottom-3 ranked responses are adopted as the chose response and rejected responses respectively.
Generation temperature is selected as 1 and 0.8 for Mistral-7b-base and Mistral-7b-it respectively (we search it among 0.8, 0.9, 1.0, 1.1, 1.2).

For \Ours (LambdaRank $\mathcal{L}_{\text{lamb}}$), we generate 10 responses each iteration and score them with the reward model. The top-2 ranked response and the bottom-2 ranked responses are adopted as the chose response and rejected responses respectively.
Generation temperature is selected as 1 and 0.8 for Mistral-7b-base and Mistral-7b-it respectively (we search it among 0.8, 0.9, 1.0, 1.1, 1.2).

For \Ours (ListMLE $\mathcal{L}_{\text{lmle}}$), we generate 10 responses each iteration and score them with the reward model. The top-2 ranked response and the bottom-2 ranked responses are adopted as the chose response and rejected responses respectively.
Generation temperature is selected as 1 and 0.8 for Mistral-7b-base and Mistral-7b-it respectively (we search it among 0.8, 0.9, 1.0, 1.1, 1.2).

\Ours can achieve even stronger performance with stronger off-the-shelf reward model \citep{dong2024rlhf}.
% Results with stronger a reward model can be found in Appendix \ref{apx:sec:stronger-rm}.

\subsection{Table \ref{tab:objective}}\label{apx:sec-objective-setting}

We conduct experiments on both Gemma2-2b-it \citep{team2024gemma} and Mistral-7b-it \citep{jiang2023mistral}.
Following \citet{Tunstall_The_Alignment_Handbook} and \citet{dong2024rlhf}, we perform training on UltraFeedback dataset for 3 iterations and show the performance of the final model checkpoint.
We use the pretrained reward model from \citet{dong2024rlhf}.
The learning rate is set as 5e-7 and we train the LLM for 2 epochs per iteration.

For the pairwise objective, we generate 2 responses for each prompt and construct the preference pair data with the reward model.
For the others, we generate 4 responses per prompt and rank them with the reward model.
For the contrastive objective, we construct the 1-vs-N data with the top-1 ranked response and the other responses.
For the listMLE and lambdarank objective, we take the top-2 as positives and the last-2 as the negatives.
Experiments with opensource LLM as the evaluator (\texttt{alpaca\_eval\_llama3\_70b\_fn}) can be found in Table \ref{tab:objective2}.

\input{tables/objective-table2}

\subsection{Table \ref{fig:list-study}}\label{apx:sec-list-setting}

We adopt Gemma2-2b-it as the initial model. All the models are trained with iterative DPO for 3 iterations. We use the off-the-shelf reward model \citep{dong2024rlhf}.
We generate 2 responses for each prompt in each iteration.
For ``w. current'', we only use the scored responses in the current iteration for preference optimization data construction.
For ``w. current + prev'', we rank the responses in the current iteration and the previous one iteration, and construct the preference pair data with the top-1 and bottom-1 ranked responses.
For ``w. current + all prev'', we rank all the responses for the prompt in the current and previous iterations and construct the preference pair data.
For ``single temperature'', we only adopt temperature 1 and generate 2 responses for reward model scoring.
For ``diverse temperature'', we generate 2 responses with temperature 1 and 0.5 respective and rank the 4 responses to construct the preference data with the reward model.

\subsection{Table \ref{tb:sft-rlhf-result}}\label{apx:sec:sft-rlhf}

We use mistral-7b-it \citep{jiang2023mistral} as the initial model to alleviate the influence of the math related post-training data of the original model.
% For SFT, we conduct training on the training set of MATH \citep{hendrycks2021measuring} and GSM8K \citep{cobbe2021training} respectively.
For SFT, we conduct training on the meta-math dataset \citep{yu2023metamath}.
For DPO, we use the prompts in the training set of the two dataset and conduct online iterative preference optimization with the binary rule-based reward (measure if the final answer is correct or not with string match). 
The evaluation is performed on the test set of MATH and GSM8K respectively.
% For both SFT and DPO, we conduct careful hyper-parameter search.
For SFT, we follow the same training setting with \citet{yu2023metamath}.
For DPO, we search the learning rate in 1e-7, 2e-7, 5e-7, 2e-8, 5e-8 and train the LLM for 5 iterations with early stop (1 epoch per iteration for MATH and 2 epoch per iteration for GSM8K). The learning rate is set as 1e-7 and we select the checkpoint after the first and fourth iteration for GSM8K and MATH respectively.

\subsection{Figure \ref{fig:merge-study}(a)}\label{apx:sec-hard-neg-setting}

We conduct training with the prompts in the training set of GSM8K and perform evaluation on GSM8K testing set.
We conduct learning rate search and finalize it to be 2e-7.
The learning is performed for 3 iterations.

We make explanations of how we construct the four types of negative settings:
For (1) a random response not related to the given prompt, we select a response for a random prompt in Ultrafeedback.
For (2) a response to a related prompt, we pick up a response for a different prompt in the GSM8K training set.
For (3) an incorrect response to the given prompt with high temperature, we select the temperature to be 1.
For (4) an incorrect response to the given prompt with low temperature, we select the temperature to be 0.7.

\begin{figure}[t]
\centering
\includegraphics[scale=0.4]{figure/LLM_alignment_gemma_temperature_study.pdf}
\vskip -1em
\caption{Training temperature study with $\mathcal{L}_{\text{pair}}$ on Gemma2-2b-it and Alpaca Eval 2. Within a specific range ($>$ 0.9), lower temperature leads to harder negative and benefit the trained LLM. However, temperature lower than this range can cause preferred and rejected responses non-distinguishable and lead to degrade training.}\label{apx:tab:temp-hard}
\end{figure}

\subsection{Figure \ref{fig:merge-study}(b)}\label{apx:sec-hard-neg-setting-temp}

We conduct experiments on both Gemma2-2b-it and Mistral-7B-it models.
For both LLMs, we conduct iterative DPO for 3 iterations and report the performance of the final model.
We perform evaluation on Alpaca Eval2 with \texttt{alpaca\_eval\_llama3\_70b\_fn} as the evaluator.

For temperature study, we find that under a specific temperature threshold, repeatedly generated responses will be large identical for all LLMs and cannot be used to construct preference data, while the threshold varies for different LLMs.
% As a result, we select temperatures above the threshold for robust experiments.
The ``low'' and ``high'' refer to the value of those selected temperatures.
% For Gemma2-2b-it, we use temperature as 0.2, 0.5 and 0.7 to generate the responses, score the responses by the reward model and train the LLM with the newly labeled data.
% For Mistral-7b-it, we set the temperature as 1, 1.1 and 1.2 respectively.
We also conduct experiments on Gemma2-2b-it model and show the results in Figure \ref{apx:tab:temp-hard}.


\subsection{Figure \ref{fig:merge-study}(c)}\label{apx:sec-length-setting}

We adopt Mistral-7b-it as the initial LLM and the contrastive objective (Eq. \ref{eq:contrastive}) in iterative preference optimization.
We generate 4/6/8/10 responses with the LLM and score the responses with the off-the-shelf reward model \citep{dong2024rlhf}.
The top-1 scored response is adopted as the positive response and the other responses are treated as the negative responses to construct the 1-vs-N training data.
The temperature is set as 1 to generate the responses.


% \newpage
% \section{\Ours with a stronger reward model}\label{apx:sec:stronger-rm}

% In Section \ref{sec:lrpo}, we show the results with LLM-Blender \citep{jiang2023llm} as the reward model to have a fair comparison with the baseline methods.
% In this section, we would like to show that \Ours can achieve even stronger performance with stronger off-the-shelf reward model \citep{dong2024rlhf}.
% The results are shown in Table \ref{apx:tab:main-performance}, where we can find that a stronger reward model can further improve the performance of \Ours.


% \begin{table*}[h]
%     \centering
%     \caption{Method evaluation on AlpacaEval 2 and MixEval. LC WR and WR denote length-controlled win rate and win rate respectively. Offline baseline performances on AlpacaEval 2 are from \citept{meng2024simpo} with LLM-Blender reward model \citep{jiang2023llm}.}\label{apx:tab:main-performance}
%     \scalebox{0.78}{
%     \begin{tabular}{lcccccccccc}
%         \toprule
%         Model & \multicolumn{4}{c}{Mistral-Base (7B)} & \multicolumn{4}{c}{Mistral-Instruct (7B)} \\
%         \cmidrule(lr){2-5} \cmidrule(lr){6-9}
%         & \multicolumn{2}{c}{Alpaca Eval 2}  & \multirow{1}{*}{MixEval} & \multirow{1}{*}{MixEval-Hard} & \multicolumn{2}{c}{Alpaca Eval 2}  & \multirow{1}{*}{MixEval} & \multirow{1}{*}{MixEval-Hard} \\
%         \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}  \cmidrule(lr){6-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9}
%         & LC WR & WR  & Score & Score & LC WR  & WR & Score & Score \\
%         \midrule
%         SFT    & 8.4  & 6.2   &  0.602  & 0.279  & 17.1 & 14.7  & 0.707 & 0.361 \\
%         RRHF   & 11.6 & 10.2   &  0.600  & 0.312  & 25.3 & 24.8  &   0.700    & 0.380 \\
%         DPO    & 15.1 & 12.5  &  0.686  &  0.341 & 26.8 & 24.9  & 0.702 & 0.355 \\
%         KTO    & 13.1 & 9.1    & \textbf{0.704}  & 0.351   & 24.5 & 23.6  &   0.692    & 0.358 \\
%         RDPO   & 17.4 & 12.8  & 0.693  & 0.355   & 27.3 & 24.5  &   0.695    & 0.364 \\
%         SimPO  & 21.5 & 20.8  &  0.672  &  0.347 & 32.1 & 34.8  & 0.702  & 0.363 \\
%         Iterative DPO  & 18.9  & 16.7  & 0.660   & 0.341  & 20.4 & 24.84  & 0.719  & 0.389 \\
%         \midrule
%         \multicolumn{9}{c}{Reward model: LLM-Blender \citep{jiang2023llm}}  \\
%         \midrule
%         \Ours (contrastive) & 31.6 & 30.8  &   0.703 & 0.409  & 32.7 & 38.6  &  0.718 & \textbf{0.418} \\
%         \Ours (LambdaRank) &  \textbf{34.9} & \textbf{37.2} & 0.695 &  \textbf{0.452}  & \textbf{32.9} & \textbf{38.9}   & \textbf{0.720} & 0.417  \\
%         \Ours (ListMLE) & 31.1  &  32.1   &  0.669  & 0.390  &  29.7 & 36.2    & 0.709  & 0.397 \\
%         \midrule
%         \multicolumn{9}{c}{Reward model: FsfairX \citep{dong2024rlhf}}  \\
%         \midrule
%         \Ours (contrastive) & \textbf{41.5} & \textbf{42.9} & 0.718 & 0.417    & \textbf{43.0}  & \textbf{53.8} & 0.718 & 0.425   \\
%         \Ours (LambdaRank) & 35.8 & 34.1 & 0.717 & 0.431   & 41.9  & 48.1 & \textbf{0.740} & \textbf{0.440}  \\
%         \Ours (ListMLE) & 36.6 & 37.8 & \textbf{0.730} & \textbf{0.423}   & 39.6  & 48.1 & 0.717 & 0.397   \\
%         \bottomrule
%     \end{tabular}}
% \end{table*}
