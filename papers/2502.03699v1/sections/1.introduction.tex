
\section{Introduction}
Large Language Models (LLMs) \citep{achiam2023gpt,team2024gemini} have demonstrated remarkable capacities in a wide range of fields including conversational modeling \citep{zhao2023survey}, reasoning \citep{wei2022chain} and code generation \citep{jiang2024survey}.  
Unlocking the full potential of LLMs while ensuring their ethical, safe, and high-quality performance hinges on effective alignment \citep{wang2023aligning}. 
However, existing reinforcement learning-based LLM alignment methods (\textit{e.g.}, PPO \citep{ouyang2022training}) involve multi-stage training and are challenging to optimize.
To this end, direct LLM preference optimization methods (\textit{e.g.}, DPO \citep{rafailov2024direct}) are proposed to simplify the alignment process.
% To this end, direct LLM preference optimization methods (\textit{e.g.}, DPO \citep{rafailov2024direct}) focus on offline settings which suffer from constrained performance.
% Yet, achieving successful alignment remains a significant hurdle, plagued by challenges of data scalability \citep{cao2024towards}, overfitting and sensitivity to distribution shift \citep{zou2024improving}.  

In this work, we further enhance direct LLM preference optimization, focusing on bringing Information Retrieval (IR) perspectives \citep{tay2022transformer}.
% Towards these challenges to obtain improved LLM alignment, we focus on bringing Information Retrieval (IR) perspectives.
Striking parallels exist between IR methodologies and LLM alignment techniques \citep{lin2022pretrained}. 
For example, IR's retriever-reranker framework, which uses a retriever for broad semantic matching to generate a candidate set and a reranker for fine-grained refinement, offers a compelling analogy to the Best-of-N approach in LLM alignment \citep{dong2023raft, sessa2024bond}. 
In this analogy, the LLM acts as the retriever, while the reward model serves as the reranker.
%IR's core retriever-reranker framework, where the retriever provides a broad set of relevant documents through coarse semantic matching followed by the reranker's fine-grained refinement, mirrors the Best-of-N approach \citep{dong2023raft, sessa2024bond} of LLM alignment, with the LLM acting as the retriever and the reward model acting as the reranker.
Furthermore, the common use of dual-encoder architectures in both LLM generation and IR retrievers, coupled with the reliance on cross-encoder architectures in reward models and IR rerankers, further underscores this synergy.  
% By leveraging established IR techniques, we can significantly enhance both the scalability and interpretability of LLM alignment as retriever optimization.
%By leveraging established IR techniques, we can potentially propose new LLM alignment methods which are easy to implement, grounded in IR and contributed to enhanced quality.
Leveraging established IR techniques offers the potential to develop novel, easily implementable LLM alignment methods grounded in IR principles, leading to improved alignment quality.

Despite the promising connections between LLM alignment and IR, a systematic exploration of this synergy remains lacking. 
Specifically, three key gaps exist: 
(1) a clear mapping between LLM alignment mechanisms and core IR principles has not been established; 
(2) empirical evaluations of LLMs through an IR lens are scarce; and 
(3) proven IR techniques like retriever optimization, hard negative mining, and candidate list construction are underutilized for LLM alignment.
This paper directly addresses these gaps by systematically bridging LLM alignment and IR methodologies.
Our contributions are fourfold:
\begin{itemize}[leftmargin=*]
\item We introduce a comprehensive framework that connects LLM alignment techniques with the established IR principles, providing a new perspective on LLM alignment.
\item We demonstrate the significance of three key IR principles  \text{-} retriever optimization objectives, hard negative mining, and candidate list construction \text{-} for improving LLM alignment.
\item Building on these insights, we propose a novel alignment method, \textbf{L}LM \textbf{A}lignment as \textbf{R}etriever \textbf{P}reference \textbf{O}ptimization (\Ours), which demonstrably enhances alignment quality, with 38.9 \% and 13.7 \% relative averaged improvement on AlpacaEval2 and MixEval-Hard.
\item We conduct further empirical studies to evaluate LLM performance using IR metrics, analyzing the impact of various post-training techniques.
\end{itemize}
In summary, this work establishes a crucial link between IR and LLM alignment, offering both novel insights and practical methods for advancing the field.
