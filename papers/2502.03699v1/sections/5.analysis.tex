
% \section{Empirical Analyses}\label{sec:analysis}
\section{Analyses}\label{sec:analysis}

% As discussed in the previous section, we can clearly see the connection between LLM community and IR community, where the LLMs can be seen as retrievers.
% In this section, we conduct experiments to analyze LLM from the three perspective in IR:
% (1) retrieval quality; (2) hard negatives in training data; (3) retrieval list size.

This section provides empirical analyses of the three factors identified in Section \ref{sec:proposal}.  
% Building upon these findings, Section \ref{sec:lrpo} introduces new preference optimization methods that leverage insights from the field of Information Retrieval.

\input{tables/objective-table}

\subsection{Retriever optimization objective}

\paragraph{Experimental setting.}
Iterative preference optimization is performed on LLMs using the different learning objectives outlined in Section \ref{sec:retrieval-obj}.
Alignment experiments are conducted using the Gemma2-2b-it \citep{team2024gemma} and Mistral-7b-it \citep{jiang2023mistral} models, trained on the Ultrafeedback dataset \citep{cui2024ultrafeedback}. 
Following the methodology of \citep{dong2024rlhf}, we conduct three iterations of training and report the performance of the final checkpoint in Table \ref{tab:objective}.  
Model evaluations are performed on AlpacaEval2 \citep{dubois2024length} and MixEval \citep{ni2024mixeval}. 
% For AlpacaEval2, we employed both the open-source LLM evaluator \texttt{alpaca\_eval\_llama3\_70b\_fn} and the GPT4 evaluator \texttt{alpaca\_eval\_gpt4\_turbo\_fn}.
Detailed settings can be found in Appendix \ref{apx:sec-objective-setting}.


\paragraph{Observation.}
Table \ref{tab:objective} presents the results, from which we make the following observations: 
(1) Contrastive optimization generally outperforms pairwise optimization (\textit{e.g.}, DPO), likely due to its ability to incorporate more negative examples during each learning step. 
(2) Listwise optimization methods, including ListMLE and LambdaRank, generally demonstrate superior performance compared to both pairwise and contrastive approaches. 
This is attributed to their utilization of a more comprehensive set of preference information within the candidate list.


\input{tables/merge_study}

\subsection{Hard negatives}

\paragraph{Experimental setting.}
The Mathstral-7b-it model is trained on the GSM8k training set and evaluated its performance on the GSM8k test set. 
Iterative DPO is employed as the RLHF method, with the gold or correct response designated as the positive example. 
The impact of different hard negative variants is investigated, as described in Section \ref{sec:hard-negative}, with the results presented in Figure \ref{fig:merge-study}(a). 
Additionally, the influence of temperature on negative hardness with Lambdarank objective are examined using experiments on the AlpacaEval 2 dataset, with results shown in Figure \ref{fig:merge-study}(b).
Detailed settings are in Appendix \ref{apx:sec-hard-neg-setting} and \ref{apx:sec-hard-neg-setting-temp}.

% \input{tables/hard-negative-fig}

\paragraph{Observation.}
Figure \ref{fig:merge-study}(a) illustrates that the effectiveness of the final LLM is directly correlated with the hardness of the negatives used during training. 
Harder negatives consistently lead to a more performant LLM.  
Figure \ref{fig:merge-study}(b) further demonstrates that, within a specific range, lower temperatures generate harder negatives, resulting in a more effective final trained LLM. 
% However, much lower temperature could also affect the quality of the chosen responses, make the chose and rejected responses non-distinguishable and finally lead to performance drop.
However, much lower temperature could lead to less diverse responses and finally lead to LLM alignment performance drop.
% Definition of temperatures can be found in Appendix \ref{apx:sec-hard-neg-setting-temp}.


\subsection{Candidate List}

\paragraph{Experimental setting.}
To investigate the impact of inclusiveness and memorization on LLM alignment, experiments are conducted using Gemma2-2b-it, employing the same training settings as in our objective study. 
For the inclusiveness study, the performance of the trained LLM is evaluated using varying numbers of candidates in the list.
For the memorization study, three approaches are compared: (i) using only the current iteration's responses, (ii) using responses from the current and previous iteration, and (iii) using responses from the current and all previous iterations. 
% Finally, for the temperature diversity study, the effect of employing different sampling temperatures is examined during response generation.
Detailed settings can be found in Appendix \ref{apx:sec-length-setting} and \ref{apx:sec-list-setting}.


% \begin{figure}[h!]
% \centering
% \includegraphics[scale=0.3]{figure/LLM_alignment_mistral_length_study.pdf}
% \vskip -1em
% \caption{Candidate list size study with $\mathcal{L}_{\text{con}}$ on Mistral-7b-it. As the candidate list size increases, alignment performance improves.}\label{fig:length-study}\vspace{-10pt}
% \end{figure}


\begin{table}[t]
    \centering
    % \vspace{-0.15in}
    \caption{Candidate list study with $\mathcal{L}_{\text{pair}}$ on Gemma2-2b-it. Previous iteration responses enhance performance.}\label{fig:list-study}
    \small
    \scalebox{0.99}{\begin{tabular}{lcc}
        \toprule
        & \multicolumn{2}{c}{Alpaca Eval 2} \\
         \cmidrule(r){2-3}
        Method & LC Winrate & Winrate \\
        \midrule
         SFT & 47.03 & 48.38 \\
        \cmidrule{1-3}
        Alignment (w. current)  & 55.06 & 66.56 \\
        Alignment (w. current + prev) & 55.62 & 70.92 \\
        Alignment (w. current + all prev) & 56.02 & 72.50  \\
        % \cmidrule{1-3}
        % Alignment (single temperature)  & 55.06 & 66.56 \\
        % Alignment (diverse temperature)  & 59.36 & 73.47  \\
        \bottomrule
    \end{tabular}}
    % \vspace{-0.15in}
\end{table}



\paragraph{Observation.}
Figure \ref{fig:merge-study}(c) illustrates the significant impact of candidate list size on LLM alignment performance.
As the candidate list size increases, performance improves, albeit with a diminishing rate of return. 
This is intuitive, given that a bigger candidate list size can contribute to more hard negatives and potentially benefit the model learning \citep{qu2020rocketqa}.
Table \ref{fig:list-study} demonstrates that incorporating responses from previous iterations can enhance performance.
This is potentially because introducing previous responses can make the candidate list more comprehensive and lead to better preference signal capturing.
More explanations are in Appendix \ref{apx:sec-list-setting}.


% \begin{table}[t]
%     \centering
%     % \renewcommand{\arraystretch}{1.2}
%     \begin{tabular}{lcc}
%         \toprule
%         & \multicolumn{2}{c}{Alpaca Eval 2} \\
%          \cmidrule(r){2-3}
%         Method & LC Winrate & Winrate \\
%         \midrule
%          SFT & 27.04 & 17.41 \\
%         \cmidrule{1-3}
%         RLHF (4 responses) & 50.02 & 61.72 \\
%         RLHF (6 responses) & 52.56 & 63.59 \\
%         RLHF (8 responses) & 55.21 & 64.88  \\
%         RLHF (10 responses) & 55.52 & 64.42  \\
%         \bottomrule
%     \end{tabular}
%     \caption{Candidate list size study for Mistral-7b-It. We conduct RLHF (iterative InfoPO) for 3 iterations. We use \texttt{alpaca\_eval\_llama3\_70b\_fn} as the evaluator.}
% \end{table}

