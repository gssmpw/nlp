
\section{Iterative LLM alignment as retriever optimization}\label{sec:proposal}
% \section{Methodology: \Ours}\label{sec:proposal}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.45]{figure/online-workflow-one-column.pdf}
\caption{The connection between iterative LLM alignment \citep{xiong2024iterative} and iterative retriever optimization \citep{xiong2020approximate}}\label{fig:llm-online}
% \vspace{-0.2in}
\end{figure}

Iterative learning is a common technique in retriever optimization \citep{xiong2020approximate}, where results from the newly-trained model are used to generate new training data, as illustrated in Figure \ref{fig:llm-online}(a). 
Similarly, for LLM alignment, iterative preference optimization has been shown to enhance performance \citep{xiong2024iterative, xu2024bpo, guo2024direct} (Figure \ref{fig:llm-online}(b)). 
Drawing inspirations from retriever optimization, we re-examine iterative LLM preference optimization, focusing on three key aspects: 
(1) the optimization objective; 
(2) the use of hard negatives; and
(3) the candidate list construction.
Based on these aspects, we propose a new LLM alignment with an IR perspective, \Ours.

\input{tables/assumption-objective}

\subsection{Retriever optimization objective}\label{sec:retrieval-obj}
Typical objectives for retriever optimization include pairwise, contrastive and listwise objectives \citep{zhao2024dense}.
In this section, we discuss preference optimization variants \citep{wang2023aligning} corresponding to different retriever optimization objectives.
The optimization objective for preference optimization is given as:
\begin{gather*}
    \max_{\pi_{\text{LLM}}} \mathbb{E}_{x, y \thicksim\pi_{\text{LLM}}(\cdot|x)}[r(x,y)] - \beta \text{KL}(\pi_{\text{LLM}}(\cdot|x) || \pi_{\text{ref}}(\cdot|x)).
\end{gather*}
As discussed in \citep{rafailov2024direct}, the equation above has the optimal solution as:
\begin{gather}
    r(x, y) = \beta \text{log} \frac{\pi_{\text{LLM}}(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \text{log} Z,
\end{gather}
where $Z=\sum_{y'} \pi_{\text{ref}}(y'|x) \text{exp}(\frac{1}{\beta} r(x, y'))$ is the normalization constant and $r(\cdot)$ is the reward model which can also be seen as a reranker.
According to different assumption for $r(x, y)$ from IR, we can obtain different training objectives as shown in Table \ref{tb:all-llm-objective}, with proofs in Appendix \ref{apx:proofs}.

\paragraph{Pairwise ranking.} 
Under the pairwise (Bradley-Terry) assumption $ \mathbb{P}\text{r}(y_w \succeq y_l) = \sigma(r(x,y_w) - r(x,y_l))$,
% \begin{gather}
%     \mathbb{P}\text{r}(y_w \succeq y_l) = \sigma(r(x,y_w) - r(x,y_l)),
% \end{gather}
the policy objective becomes DPO \citep{rafailov2024direct} $\mathcal{L}_{\text{pair}}$.
% \begin{gather}
% \mathcal{L}_{\text{pair}}=-\mathbb{E}\;\biggl[
%   \log \sigma\Bigl(
%      \beta \log\!\tfrac{\pi_\theta(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)}-
%      \beta \log\!\tfrac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)}
%   \Bigr)
% \biggr].
% \end{gather}

\paragraph{Contrastive ranking.} Another widely used objective for ranking is contrastive learning \citep{oord2018representation}:
% \begin{gather}
%     \mathbb{P}\text{r}(y_w \succeq y^{(1)}_l, ..., y_w \succeq y^{(m)}_l) = \text{softmax}(r(x, y_w)) \\
%     = \frac{\text{exp}(r(x,y_w))}{\text{exp}(r(x,y_w)) + \sum^m_{i=1}\text{exp}(r(x,y^{(i)}_l))}
% \end{gather}
\begin{equation}\label{eq:contrastive-assumption}
\begin{aligned}
\mathbb{P}\text{r}(y_w & \succeq y^{(1)}_l, ..., y_w \succeq y^{(m)}_l) 
= \text{softmax}(r(x, y_w)) = \frac{\text{exp}(r(x,y_w))}{\text{exp}(r(x,y_w)) + \sum^m_{i=1}\text{exp}(r(x,y^{(i)}_l))}.
\end{aligned}
\end{equation}
It handles multiple negatives in a single step, allowing the model to learn more robust representations for retrieval and ranking.
It is widely used for dense retriever training \citep{karpukhin2020dense}.
Under this ranking assumption, the policy objective becomes $\mathcal{L}_{\text{con}}$ as shown in Table \ref{tb:all-llm-objective}.
% Under this ranking assumption, the policy objective becomes:
% \begin{equation}
% \begin{aligned}
%     \mathcal{L}_{\text{con}} = -\mathbb{E} & \biggl[
%     \log \frac{\exp\bigl(\gamma(y_w \mid x)\bigr)}{
%       \exp\bigl(\gamma(y_w \mid x)\bigr) + \sum_{i=1}^m \exp\bigl(\gamma(y_l^{(i)} \mid x)\bigr)}
%     \biggr], \\
%     \text{where } &\quad \gamma(y \mid x) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}.
% \end{aligned}\label{eq:contrastive}
% \end{equation}
% The proof can be found in Appendix \ref{apx:proof:contrastive}.

% \paragraph{Listwise ranking.}
\paragraph{LambdaRank.}
% Another widely used retriever training objective is listwise ranking objective.
In addition to pairwise and contrastive learning, list-wise ranking is widely adopted to sufficiently utilize the comprehensive information in candidate list.
Inspired by LambdaRank \citep{burges2010ranknet, zeng2022curriculum}:
\begin{gather}\label{eq:lambdarank-assumption}
    \mathbb{P}\text{r}(y_1 \succeq ... \succeq y_m) = \prod_{1<i<j<m} \sigma(r(x,y_i) - r(x,y_j)),
\end{gather}
the policy optimization objective becomes $\mathcal{L}_{\text{lamb}}$ (Table \ref{tb:all-llm-objective}).
% \begin{gather}
%     \mathcal{L}_{\text{lamb}}=-\mathbb{E}\;\biggl[ \sum_{1<i<j<m}
%   \log \sigma\Bigl(
%      \gamma(y_i \mid x)-
%      \gamma(y_j \mid x)
%   \Bigr)
% \biggr],
% \end{gather}
% The proof can be found in Appendix \ref{apx:proof:lambdarank}.


\paragraph{ListMLE.}
Another list-wise ranking assumption is the ListMLE assumption \citep{xia2008listwise}, which provides theoretical grounding and global optimization perspective:
\begin{gather}\label{eq:listmle-assumption}
\begin{aligned}
    \mathbb{P}\text{r}(y_1 & \succeq ... \succeq y_m) = \prod^m_{i=1} \text{softmax}^m_i(r(x, y_i))  = \prod^m_{i=1} \frac{\text{exp}(r(x,y_i))}{\text{exp}(r(x,y_i)) + \sum^m_{j=i+1}\text{exp}(r(x,y_j))}
\end{aligned}
\end{gather}
In this case, the objective becomes $\mathcal{L}_{\text{lmle}}$ shown in Table \ref{tb:all-llm-objective}.

% In this case, the policy optimization objective becomes:
% \begin{equation}
% \begin{aligned}
%     \mathcal{L}_{\text{lmle}} &= -\mathbb{E} \biggl[
%     \sum^m_{i=1} \log \frac{\exp\bigl(\gamma(y_i \mid x)\bigr)}{
%         \exp\bigl(\gamma(y_i \mid x)\bigr) + \sum_{j=i}^m \exp\bigl(\gamma(y_j \mid x)\bigr)}
%     \biggr].
% \end{aligned}
% \end{equation}
% The proof can be found in Appendix \ref{apx:proof:listmle}.

\subsection{Hard negatives}\label{sec:hard-negative}
Hard negatives are crucial for effective retriever training \citep{zhan2021optimizing, qu2020rocketqa}, as learning to distinguish harder negatives potentially lead to more powerful retrievers \citep{xiong2020approximate}. 
In LLM alignment, negatives correspond to unpreferred responses ($y_l$) for a given prompt ($x$). 
In iterative on-policy training, various types of negatives can be identified, ordered by increasing difficulty: 
(1) \textbf{Easiest}: A random, unrelated response to $x$;
(2) \textbf{Easy}: A response to a related but different prompt ($x'$); 
(3) \textbf{Hard}: An incorrect response to $x$ generated with a high temperature; 
(4) \textbf{Hardest}: An incorrect response to $x$ generated with a low temperature.

Note that, assuming a well-initialized policy LLM, as indicated by Figure \ref{fig:mathstral-gsm8k-infer}(b) ($N=1$), low temperatures tend to produce harder negatives, yielding the above ranking.
%Therefore, the hardness of these negative types can be ranked as (4) $>$ (3) $>$ (2) $>$ (1). 
% The impact of negative hardness on LLM alignment remains largely unexplored.
According to \citet{zhan2021optimizing}, hardest negatives could be most important to LLM alignment. 

\subsection{Candidate list}
In iterative retriever optimization, construction of the candidate list $[d_1, ..., d_m]$, which is used by the reranker to generate data for the next iteration, is crucial. 
Prior research \citep{zeng2022curriculum} has identified factors such as list size and candidate selection as being particularly important. 
Similarly, in iterative preference optimization, construction of the candidate response list $Y=[y_1, ..., y_m]$ is critical. 
We identify two key factors influencing the quality of $Y$: inclusiveness and memorization.
\begin{enumerate}[label=(\arabic*), leftmargin=*]
    \item \textbf{Inclusiveness} \citep{qu2020rocketqa} refers to the \textbf{size} of the response list $Y$. A larger $Y$ potentially encompasses more information.
    \item \textbf{Memorization} \citep{zeng2022curriculum} refers whether previously generated responses $Y'$ are included in the current list $Y$ to preserve past results.
    % \item \textbf{Diversity} \citep{lin2023train} relates to the sampling strategy used to generate responses. Diverse sampling temperatures can enhance response diversity.
\end{enumerate}
Given their importance in IR \citep{qu2020rocketqa, zeng2022curriculum}, the impact of these factors on LLM alignment, however, remains largely under-explored.


% \subsection{\Ours}
\section{The Proposed Solution: \Ours}\label{sec:proposal1}

\begin{algorithm}[t]
\caption{\Ours: LLM alignment as iterative retriever preference optimization.}\label{alg-ours}
\begin{algorithmic}[1]
\REQUIRE Number of iterations $T$, number of new data per annotation phase $M$, number of generated responses for each prompt $k$, temperature for each iteration $\{t_i\}^T_{i=0}$, prompt dataset $\mathcal{D}_\mathcal{X} = \{x_i\}_{i=1}^N$, policy LLM $\pi_{\theta_0}$, reward model $r$, learning rate $\gamma$, a ranking-based objective function $\mathcal{L}_{\text{rank}}$.
\ENSURE Aligned LLM $\pi_{\theta_T}$.

\FOR{$s := 0$ to $T$}
    % \IF{$t \bmod K = 0$}
        \STATE Update behavior LLM: $\pi_\beta \leftarrow \pi_{\theta_s}$
        \STATE Preference dataset $\mathcal{D}_s = \{\}$
        \FOR{$i := 1$ to $M$}
            \STATE Sample prompt $x \sim \mathcal{D}_\mathcal{X}$
            \STATE // \textcolor{myblue}{candidate list construction}
            \STATE Sample $y_1, ..., y_k \sim \pi_{\beta}(\cdot|x)_{t_s}$  
            \STATE // \textcolor{myblue}{hard negatives}
            \STATE Rank $\{y_i\}$ with $r$: $Y_x= \{ y^{(r)}_j \}$, where $(r(y^{(r)}_a) > r(y^{(r)}_b)), a<b$ 
            \STATE $\mathcal{D}_s \leftarrow \mathcal{D}_s \cup \{(x, Y_x)\}$
        \ENDFOR
    % \ENDIF
    % \STATE $\mathcal{D} \leftarrow  \bigcup^s_{i=1} \mathcal{D}_s$ ~~~~~~~~~// \textcolor{myblue}{candidate list construction}
    \STATE // \textcolor{myblue}{candidate list construction}
    \STATE $\mathcal{D} \leftarrow  \text{Merge}^s_{i=0} \mathcal{D}_s$ 
    \WHILE{$\mathcal{D} \neq \emptyset$} 
    % \FOR{a batch $(x, Y_x)$ from $\mathcal{D}_s$}
    \STATE Sample a batch $(x, Y_x)$ from $\mathcal{D}$
    \STATE Update $\mathcal{D} \leftarrow \mathcal{D} \setminus \{(x, Y_x)\}$
    \STATE // \textcolor{myblue}{retriever optimization objective}
    \STATE $\theta_{s} \leftarrow \theta_s - \gamma \cdot \nabla_\theta \mathcal{L}_{\text{rank}}(x, Y_x, \pi_\theta; \pi_\beta)$
    \ENDWHILE
    \STATE $\theta_{s+1} \leftarrow \theta_s$
    % \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

Motivated by iterative retriever optimization pipeline as shown in Figure \ref{fig:llm-online}(a) and the three key points in IR, we introduce \Ours, a novel approach to LLM alignment formulated as iterative retriever preference optimization.  
The algorithmic details are provided in Algorithm \ref{alg-ours}.  
Specifically, our experimental setup explores the following key aspects:
(1) \textbf{Optimization objective}: We evaluate three distinct loss functions as the ranking objective ($\mathcal{L}_{\text{rank}}$): $\mathcal{L}_{\text{con}}$, $\mathcal{L}_{\text{lamb}}$, and $\mathcal{L}_{\text{lmle}}$.
(2) \textbf{Hard negatives}: For a given prompt, hard negative samples are constructed by selecting less preferred responses generated with an appropriate temperature through parameter search.
More details of how the temperature are available in Appendix \ref{apx:sec:main}.
(3) \textbf{Candidate list}: In each iteration, we generate multiple (10) candidate responses considering inclusiveness.  
In terms of memorization, the candidate pool for subsequent iterations includes all previously generated responses. 
% An investigation of response temperature diversity is deferred to future work.