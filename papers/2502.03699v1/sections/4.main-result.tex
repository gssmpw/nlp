

\section{Main Results}\label{sec:main-result}

\paragraph{Baselines.}
We evaluate the performance of \Ours against a range of established preference optimization methods, encompassing both offline and online approaches.
Our offline comparison set includes RRHF \citep{yuan2023rrhf}, SLiC-HF \citep{zhao2023slic}, DPO \citep{guo2024direct}, IPO \citep{azar2024general}, CPO \citep{xu2024contrastive}, KTO \citep{ethayarajh2024kto}, RDPO \citep{park2024disentangling} and SimPO \citep{meng2024simpo}.
For online methods, we compare with iterative DPO \citep{xiong2024iterative}.
The baseline checkpoints are from \citep{meng2024simpo}.
Further details regarding these baselines and our experimental setup are provided in Appendix \ref{apx:sec:baselines}.
Both baselines and \Ours are trained on Ultrafeedback dataset \citep{cui2024ultrafeedback} for fair comparison.

\paragraph{Datasets.} We conduct evaluation on two widely used benchmarks AlpacaEval2 \citep{dubois2024length} and MixEval \citep{ni2024mixeval}.  
These benchmarks are designed to assess the conversational capabilities of models across a diverse range of queries. AlpacaEval2 comprises 805 questions sourced from five datasets, while MixEval includes 4000 general and 1000 hard questions.
Evaluation follows the established protocols for each benchmark. For AlpacaEval 2, we report both the raw win rate (WR) and the length-controlled win rate (LC). These benchmarks collectively provide a comprehensive assessment of the models' instruction-following and problem-solving capabilities.

\paragraph{Results.}
% We assess the performance of \Ours on two established benchmarks: AlpacaEval2 \citep{dubois2024length} and MixEval \citep{ni2024mixeval}. 
The baseline performances on AlpacaEval 2 are directly from \citet{meng2024simpo}, while the performances on MixEval is evaluated by ourselves with the opensourced checkpoints.
We adopt the same LLM-Blender \citep{jiang2023llm} reward model for a fair comparison with the baselines and also explore stronger reward model: FsfairX \citep{dong2024rlhf}.
The results, presented in Table \ref{tab:main-performance}, show that \Ours consistently outperforms the competitive baseline methods on both datasets, with 38.9 \% and 13.7 \% averaged relative improvements, on AlpacaEval2 and MixEval-Hard respectively, with the same reward model as the baselines.
With a stronger reward model, we can further improve \Ours by 25.8 \% on the challenging AlpacaEval2 dataset.
Additional details regarding our experimental setup are available in Appendix \ref{apx:sec:main}.

\input{tables/main-result-table}

