
% \section{LLM alignment as retriever optimization}
\section{An Information Retrieval Perspective on LLMs}\label{sec:understanding}


\begin{figure*}
\centering
\includegraphics[scale=0.53]{figure/arch2.pdf}
\caption{Architecture connection between retriever/LLM (bi-encoder) and reranker/reward model (cross-encoder). 
Bi-encoder models process each query/prompt and passage/response separately and often calculate their alignment score via a dot product operator, while cross-encoder models take both query/prompt and passage/response as input and score them directly.
Bi-encoder models can be more efficient (\textit{i.e.}, large-scale text matching) but the interaction between the two information unit is only captured by a dot production operation where their effectiveness can be constrained. Cross-encoder models can be more effective (\textit{i.e.}, deeper interaction calculation with transformer architecture \citep{vaswani2017attention}) but less efficient. Although LLM involves auto-regressive token matching, which is different from retriever, some insights from IR can be borrowed to enhance LLM alignment as shown in the following sections.}\label{fig:bi-cross-encoder}
% \vspace{-0.1in}
\end{figure*}

% \paragraph{Primer on information retrieval}
\subsection{Primer on information retrieval}

Information retrieval systems \citep{zhu2023large} typically employ a two-stage process involving retrievers \citep{zhao2024dense} and rerankers \citep{lin2022pretrained}.
The retriever, often implemented as a bi-encoder (Figure \ref{fig:bi-cross-encoder}), efficiently identifies a large set of ($K$) potentially relevant passages, denoted as $D_{\text{retrieval}}$, from a corpora $C$ given a query $q$.
% This is achieved using a coarse-grained similarity function, $f_{\text{retrieval}}(q, d)=\text{Enc}^T_q(q) \cdot \text{Enc}_d(d)$, where $\text{Enc}_q$ and $\text{Enc}_d$ represent the query and passage encoders respectively:
This is achieved using a coarse-grained similarity function, $p_{\text{retrieval}}(d|q)=\text{Enc}^T_q(q) \cdot \text{Enc}_d(d)$, where $\text{Enc}_q$ and $\text{Enc}_d$ represent the query and passage encoders respectively:
\begin{gather}\label{eq:retriever}
    D_{\text{retrieval}}(q) = \{ d \in C \;|\; \max_{\text{top-}K} ~ p_{\text{retrieval}}(\cdot | q)\}.
\end{gather}
However, due to the scale of the corpus, retrievers might not accurately capture fine-grained query-passage similarity with the simple dot production interaction function. 
Therefore, rerankers, typically implemented with cross-encoder (Figure \ref{fig:bi-cross-encoder}), are employed to refine the ranking of the retrieved passages $D_{\text{retrieval}}$.
% The reranker produces a smaller set ($k$) of top-ranked passages, $D_{rank}$, using a fine-grained similarity function, $f_{\text{rank}}(q,d)=w^T \cdot \text{Enc}(q, d)$.
The reranker produces a smaller set ($k$) of top-ranked passages, $D_\text{rank}$, using a fine-grained similarity function, $r_{\text{rank}}(q,d)=w \cdot \text{Enc}(q, d)$, where $w$ is a learnable linear layer. 
Here, reranker adopts cross-encoder with both query/passage as inputs and encoded together while retriever adopts dual encoder for separate query/passage encoding.
% \begin{gather}
%     D_{\text{rank}}(q) =  \max_{\text{top-}k, d\in D_{\text{retrieval}}} f_{\text{rank}}(q, d),
% \end{gather}
\begin{gather}\label{eq:reranker}
    D_{\text{rank}}(q) = \{ d \in D_{\text{retrieval}}(q) \;|\; \max_{\text{top-}k} r_{\text{rank}}(q, \cdot)\}.
\end{gather}
% The resulting ranked passages are ordered such that $D_{\text{rank}}(q)=\{d_1, d_2, \ldots, d_k\}$ where $f_{\text{rank}}(q, d_1)\ge f_{\text{rank}}(q, d_2)\ge\cdots\ge f_{\text{rank}}(q, d_k)$.
The resulting ranked passages are ordered such that $D_{\text{rank}}(q)=\{d_1, d_2, \ldots, d_k\}$ where $r_{\text{rank}}(q, d_1)\ge r_{\text{rank}}(q, d_2)\ge\cdots\ge r_{\text{rank}}(q, d_k)$.
%If a single output passage is required, the top-ranked passage, $d_1$, is selected.

\subsection{LLMs as retrievers. Reward models as rerankers}
During inference, an LLM generates a response $y$ given an input prompt $x$ by modeling the probability distribution $p_{\text{LLM}}(y|x)$.
Assuming a fixed maximum sequence length $L$ and a vocabulary space $V$ \citep{li2024matching}, the set of all possible responses can be defined as  $Y=\{y:y(1)y(2)...y(L) | y(i) \in V\} \subseteq V^L$.

We can conceptualize this process through an IR lens \citep{tay2022transformer}. The prompt $x$ can be viewed as analogous to a query $q$, the set of all possible responses $Y$ can be treated as the corpus $C$, and the generated response $y$ can be considered as the retrieved passage $d$.
Thus, given a prompt $x$, the LLM effectively acts as a retriever, searching for the most probable responses $D_{\text{LLM}}(x)$ from response space $Y$:
% \begin{gather}
%     D_{\text{LLM}}(x) =  \max_{\text{top-}K, y\in Y} f_{\text{LLM-retrieval}}(x, y),
% \end{gather}
\begin{gather}\label{eq:retriever2}
    D_{\text{LLM}}(x) = \{ y \in Y \;|\; \max_{\text{top-}K} ~ p_{\text{LLM}}(\cdot | x)\}.
\end{gather}
% where $f_{\text{LLM-retrieval}}(x, y)=p_{\text{LLM}}(y|x)$.
where $p_{\text{LLM}}(y|x)$ is analogous to $p_{\text{retrieval}}(d|q)$ in IR.

This analogy is further supported by the LLMs' architecture. 
As illustrated in Figure \ref{fig:bi-cross-encoder}, the generative modeling with LLMs can be interpreted as the matching process of a bi-encoder model. 
The prompt is encoded into a vector representation by LLM, while response tokens are represented as token embedding vectors. 
For each token position decoding, prompt embedding (obtained often from the hidden state of the last layer of the LLM) and vocabulary token embeddings are compared with a dot product, to determine the likelihood of a selected token for the response.
% These representations are then compared, often using a dot product, to determine the likelihood of a given response.

Furthermore, reward models $r_\text{rm}(x,y)$ \citep{lambert2024rewardbench}, which take both the prompt and response as input, function similarly to cross-encoders (\textit{i.e.}, rerankers $r_\text{rank}(q,d)$ \citep{zhuang2023rankt5}) in IR.
To enhance LLM performance, various inference-time strategies have been developed, including Best-of-N sampling \citep{stiennon2020learning} and majority voting \citep{wang2022self}.  
These can be interpreted as different configurations of retrievers and rerankers, as summarized in Appendix Table \ref{tb:llm-retriever-reranker}.


% \paragraph{LLM tuning as retriever optimization.}
\subsection{LLM tuning as retriever optimization}\label{sec:llm-tuning-retriever}

\paragraph{Supervised fine-tuning as direct retriever optimization.}
Retriever training, aiming for accurate retrieval, often employs contrastive learning with the InfoNCE loss \citep{oord2018representation} to maximize $P(d_{\text{gold}}|q)$ of retrieving the ground truth passage $d_{\text{gold}}$ given a query $q$. This can be expressed as:
% and encourage the similarity score between the query $q$ and ground truth passage $d_{\text{gold}}$ to be higher than that between $q$ and \textbf{all} other passages:
% \begin{gather*}
%     \forall d' \in C \setminus d_{\text{gold}}, \max P(f_{\text{retrieval}}(q, d_{\text{gold}}) > f_{\text{retrieval}}(q, d')).
% \end{gather*}
\begin{gather*}
    \max \log P(d_{\text{gold}}|q) = \max \log \frac{\text{Enc}_d(d_{\text{gold}}) \cdot\text{Enc}_q(q)}{\sum^{|C|}_{j=1} \text{Enc}_d(d_j) \cdot\text{Enc}_q(q)}.
\end{gather*}
In the context of LLM alignment, supervised fine-tuning (SFT) aims to quickly adapt the model to a target task using prompt-response pairs ($x, y_{\text{gold}}$). 
SFT maximizes the conditional probability $P(y_{\text{gold}}|x)$ as:
\begin{gather*}
    \max \log P(y_{\text{gold}}|x) = \max \log \prod^{|y_{\text{gold}}|}_i P(y_{\text{gold}}(i)|z_i) 
    = \max \sum^{|y_{\text{gold}}|}_i \log \frac{\text{Emb}(y_{\text{gold}}(i)) \cdot\text{LLM}(z_i)}{\sum^{|V|}_{j=1} \text{Emb}(v_j) \cdot\text{LLM}(z_i)},
\end{gather*}
where $y(i)$ is the $i$-th token of $y$, $z_i=[x, y_{\text{gold}}(1:i-1)]$ represent the concatenation of the prompt $x$ and the preceding tokens of $y_{\text{gold}}$, $\text{LLM}(\cdot)$ produces a contextualized representation, and $\text{Emb}(\cdot)$ is the token embedding function.

Consequently, the SFT objective can be interpreted as a composite of multiple retrieval optimization objectives. In this analogy, $\text{LLM}(\cdot)$ acts as the query encoder and $\text{Emb}(\cdot)$ serves as the passage (or, in this case, token) encoder.

% As for LLM alignment, supervised fine-tuning (SFT) helps the LLM learn basic instruction/task-solving capabilities.
% Given a prompt $x$ and the golden response $y_\text{gold}$, SFT maximizes $p_{\text{LLM}}(y_{\text{gold}}|x)$, which is equivalent to optimize the following objective:
% \begin{gather*}
%     \forall y' \in Y \setminus y_{\text{gold}}, \max P(p_{\text{LLM}}(y_{\text{gold}}|x) > p_{\text{LLM}}(y'|x)).
% \end{gather*}
% In this case, SFT is conducted retrieval capability training to let LLM be able to retrieve the proper response from the large response set.

% \begin{figure}
% \centering
% \small
% \includegraphics[scale=0.3]{figure/LLM_alignment_gsm8k_mathstral7b_infer.pdf}
% \vskip -1em
% \caption{Evaluation of the LLM as a retriever with Recall@N (Pass@N) with Mathstral-7b-it as the LLM. As the number of retrieved responses (N) increases, the retrieval recall increases. The higher the temperature is, the broader spectrum the retrieved responses are, and thus the higher the recall is.}\label{fig:mathstral-gsm8k-infer}
% \end{figure}




\paragraph{Preference optimization as reranker-retriever distillation.}
In retriever training, optimizing solely based on query/ground-truth document pairs can be suboptimal, particularly when using in-batch negatives for efficiency. Performance can be enhanced by distilling knowledge from a more powerful reranker to retriever \citep{qu2020rocketqa,zeng2022curriculum}. This distillation process can be represented as $f_{\text{rerank}}(\cdot) \overset{r}{\rightarrow} \text{data}   \overset{g(\cdot)}{\rightarrow}  f_{\text{retrieval}}(\cdot)$, where new data, generated by the reranker $f_{\text{rerank}}(\cdot)$ based on a rule $r$, is used to optimize the retriever $f_{\text{retrieval}}(\cdot)$ with an objective $g(\cdot)$.

Similarly, in LLM alignment, a preference alignment phase often follows supervised fine-tuning (SFT) to further enhance the model using an external reward model to absorb preferential supervision effectively.
Methods like PPO \citep{schulman2017proximal} and iterative DPO \citep{guo2024direct} exemplify this approach.  
Here, the LLM (considered acting as the retriever) generates responses that are then scored by the reward model (considered acting as the reranker). 
These scores are used to create new training data, effectively performing distillation from the reward model into the LLM: 
$f_{\text{reward-model}}(\cdot) \overset{r}{\rightarrow}  \text{data} \overset{g(\cdot)}{\rightarrow} f_{\text{LLM}}(\cdot)$. 
Thus, preference optimization can be viewed as a form of reranker-to-retriever distillation, analogous to the process used in traditional IR.

We conduct empirical studies to understand SFT and preference optimization from IR perspective in Appendix \ref{apx:sft-rlhf-empirical} and have further discussion in Appendices \ref{apx:discuss1} and \ref{apx:discuss2}.

% \textcolor{blue}{add a sentence that we also conduct experiments to study SFT and PO in appendix}

\subsection{Empirical insights into LLMs as IR models}\label{sec:empirical}

\paragraph{Evaluating LLMs as retrievers.}
A common metric for evaluating retrievers is Recall@$N$, which assesses whether the top-$N$ retrieved passages include any relevant passages for a given query. 
In the context of LLMs, this translates to evaluating whether the top-$N$ generated responses contain a suitable response to the prompt, analogous to Pass@$N$ \citep{chen2021evaluating}.

\begin{figure}[h!]
    \centering
    \subfigure[Retriever]{\includegraphics[width=0.45\textwidth]{figure/LLM_alignment_nq_e5_pass.pdf}}
    \hspace{1cm}
    \subfigure[LLM]{\includegraphics[width=0.45\textwidth]{figure/LLM_alignment_gsm8k_mathstral7b_pass_multi.pdf}}
    \caption{Analogy between evaluating retriever with Recall@N and LLM with Pass@N. As the number (N) of retrieved passages/generated responses increases, the retriever and LLM have a similar increasing trend. This highlights the importance of inference time scaling (\textit{e.g.}, Best-of-N) for LLM similar to retriever-reranker scaling in IR. Retriever: e5; LLM: Mathstral-7b-it.}\label{fig:mathstral-gsm8k-infer}
\end{figure}

To draw the empirical connection between LLM and retrievers, we conduct an experiment on the GSM8K dataset \citep{cobbe2021training} using Mathstral-7b-it \citep{mathstral2025} and an experiment on the NQ dataset \citep{kwiatkowski2019natural} using e5 retriever.
%, to study the correlation between Recall@N/Pass@N with N.
Figure \ref{fig:mathstral-gsm8k-infer} illustrates that increasing N can contribute to improved performance for both retriever and LLM. Detailed analysis can be found in Appendix \ref{apx:llm-as-retriever}.

% To investigate the retrieval capabilities of LLMs, we conduct an experiment on the GSM8K dataset \citep{cobbe2021training} using Mathstral-7b-it \citep{mathstral2025}.  
% We focus on two key parameters: the number of generated responses ($N$) and the decoding temperature, which influence the retrieval size and the diversity of the retrieved set, respectively.  
% Figure \ref{fig:mathstral-gsm8k-infer} illustrates the following key findings: 
% (1) Recall@$N$ increases significantly with $N$, indicating that retrieving more responses increases the likelihood of obtaining a correct one. 
% (2) For small $N$ (\textit{e.g.}, 1), lower temperatures yield higher Recall@$N$, as higher temperatures introduce randomness that can hinder the retrieval of the most probable response. 
% (3) For large $N$ (\textit{e.g.}, $>$ 10), higher temperatures lead to improved Recall@$N$, likely because increased diversity among the responses improves the chances of including the correct answer within a larger retrieved set.

Greedy decoding, equivalent to $N=1$, is a prevalent LLM inference strategy. 
However, as shown in Figure \ref{fig:mathstral-gsm8k-infer}(b), Pass@1 is often suboptimal, and thus increasing $N$ can substantially improve performance. 
This highlights the importance of inference-time scaling techniques like Best-of-N \citep{stiennon2020learning} in LLM similar to retriever-reranker scaling \citep{zhuang2023rankt5} in IR.
More results and analyses can be found in Appendix \ref{apx:llm-as-retriever}.
