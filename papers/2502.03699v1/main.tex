% This class has a lot of options, so please check deepmind.cls for more details.
% This is a minimal set for most needs.
\documentclass[11pt, a4paper, logo, copyright]{googlecloud}

% Omit dates for reproducibility.
\pdfinfoomitdate 1
\pdftrailerid{redacted}

% This avoids duplicate hyperref bookmark entries when using \bibentry (e.g. via \citeas).
\makeatletter
\renewcommand\bibentry[1]{\nocite{#1}{\frenchspacing\@nameuse{BR@r@#1\@extra@b@citeb}}}
\makeatother

\usepackage{kantlipsum, lipsum}
\usepackage{dsfont}
% \usepackage{gdm-colors}

% Sometimes you will get errors about pdflink ending up in diffrent position. Try this and
% comment it out again when you are done with your document.
%\hypersetup{draft}

% Set the bibliography options here.
\usepackage[authoryear, sort&compress, round]{natbib}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{hyperref}       % hyperlinks
\usepackage[nameinlink]{cleveref}
\usepackage{bbm}
\usepackage{multirow}
% \usepackage{subfig}
\usepackage{soul}
\usepackage{floatrow}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{blindtext}
\usepackage{tablefootnote}
% \usepackage{fdsymbol}. % cause an error
\usepackage{amsfonts}
\usepackage[flushleft]{threeparttable}
% \usepackage{colortbl}
\usepackage{bbding}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{bm}
\usepackage{arydshln}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{setspace}
% \usepackage{mathabx}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{longtable}
% \usepackage{booktabs}

\usepackage[normalem]{ulem}
\usepackage{ulem}
\usepackage[nomargin,inline,marginclue,draft]{fixme}
\usepackage{balance}
\usepackage{verbatim}
\usepackage{diagbox}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{array}   % Optional: Improves table column alignment

%%%%%%%%%%% mboratko: added these, feel free to adjust
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%%%%%%%%%%

\newcommand{\Ours}{\textsc{LarPO}\xspace}

\definecolor{myred}{rgb}{1, 0, 0}
\definecolor{myblue}{rgb}{0, 0, 1}
\definecolor{myblack}{rgb}{1, 1, 1}

\newcommand{\bmh}{{\bm h}}
\newcommand{\bmW}{{\bm W}}
\newcommand{\bmz}{{\bm z}}
\newcommand{\bmH}{{\bm H}}
\newcommand{\bme}{{\bm e}}
\newcommand{\bmQ}{{\bm Q}}
\newcommand{\bmK}{{\bm K}}
\newcommand{\bmV}{{\bm V}}
\newcommand{\bmc}{{\bm c}}
\newcommand{\bmE}{{\bm E}}
\newcommand{\bmd}{{\bm d}}
\newcommand{\bmy}{{\bm y}}
\newcommand{\bmx}{{\bm x}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% \input{commands}

% Images will be looked for in this path, removes need for explicit path when including images.
\graphicspath{{figures/}}

% Important Information about your paper.
\title{LLM Alignment as Retriever Optimization: An Information Retrieval Perspective}

% Can leave this option out if you do not wish to add a corresponding author.
\correspondingauthor{bowenj4@illinois.edu}

% Remove these if they are not needed
% \keywords{\LaTeX, Publications process, tools}
% \paperurl{arxiv.org/abs/123}

% Use the internally issued paper ID, if there is one
% \reportnumber{001} % Leave blank if n/a

% Assign your own date to the report.
% Can comment out if not needed or leave blank if n/a.
\renewcommand{\today}{}

% Can have as many authors and as many affiliations as needed. Best to indicate joint
% first-authorship as shown below.
\author[1 2 *]{Bowen Jin}
\author[1]{Jinsung Yoon}
\author[3]{Zhen Qin}
\author[2]{Ziqi Wang}
\author[2]{Wei Xiong}
\author[4]{Yu Meng}
\author[2]{Jiawei Han}
\author[1]{Sercan Ö. Arık\hspace{-0.4ex}}

% Affiliations *must* come after the declaration of \author[]
% \affil[*]{Equal contributions}
\affil[1]{Google Cloud}
\affil[2]{University of Illinois at Urbana-Champaign}
\affil[3]{Google DeepMind}
\affil[4]{University of Virginia}

\begin{abstract}
Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. 
Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse.
While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative.
In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. 
We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. 
Building on this foundation, we propose \textbf{L}LM \textbf{A}lignment as \textbf{R}etriever \textbf{P}reference \textbf{O}ptimization (\Ours), a new alignment method that enhances overall alignment quality.
Extensive experiments validate \Ours's effectiveness with 38.9\% and 13.7\% averaged improvement on AlpacaEval2 and MixEval-Hard respectively.
Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.
\end{abstract}

\begin{document}

\maketitle

\input{sections/1.introduction}
\input{sections/2.understanding}
\input{sections/3.method}
\input{sections/4.main-result}
\input{sections/5.analysis}
\input{sections/6.related_work}

\section{Conclusions}
This paper investigates the impact of increasing the number of retrieved passages on the performance of long-context LLMs in retrieval-augmented generation (RAG) systems.
Contrary to expectations, we observe that performance initially improve but then degrade as more passages are included.
This phenomenon is attributed to the detrimental influence of retrieved "hard negatives".
To mitigate this issue, we propose and evaluate three solutions: training-free retrieval reordering, RAG-specific implicit LLM fine-tuning, and RAG-oriented LLM fine-tuning with intermediate reasoning.
A systematic analysis of the training-based methods explores the effects of data distribution, retriever for training, and training context length.
Interesting future directions include exploring (automated) position optimization with more advanced retrieval ordering methods, and fine-tuning the LLMs for RAG with more fine-grained and multi-step reasoning chains.

% Bibliography components
\bibliographystyle{abbrvnat}
\nobibliography*
\bibliography{main}

\clearpage
% Some other useful sections you might consider having in your report.
% \section*{Citing this work}
% This is a free, open access paper provided by Google DeepMind.
% The final version of this work was published online (provide venue, date and digital object
% identifier (doi, if available). \textit{Cite as:} \citeas{lee2024gecko}.

% If you add a bibtex entry of your own paper (this paper), you can
% show its full citation inline using \citeas, as above. Note that
% this citation removes the trailing full stop. To make \citeas work,
% you need to load the bibliography data. This can be done in two
% ways:
%
%    1. If you already have a printed bibliography with \bibliography{...},
%       then add the command "\nobibliography*", no arguments, before that.
%    2. If you don't otherwise print a bibliography, add the command
%       \nobibliography{...} at the end of your document.


% \section*{Author Contributions}

% \section*{Acknowledgements}

% \section*{Funding}
% This research was funded by Google.

% \section*{Competing interests}
% The authors declare no competing financial interests. Related
% patent number here if applicable.

% \section*{Data availability}
% The datasets used in the experiments have been made available
% for download at IF AVAILABLE.

\input{sections/7.appendix}

\end{document}
