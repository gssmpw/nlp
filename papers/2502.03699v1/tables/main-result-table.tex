

\begin{table*}
    \centering
    \caption{Evaluations on AlpacaEval 2 and MixEval. LC WR and WR denote length-controlled win rate and win rate respectively. Offline baseline performances on AlpacaEval 2 are from \citep{meng2024simpo}. We use LLM-blender \citep{jiang2023llm} as the reward model for a fair comparison with the baselines and also report the result with a stronger reward model FsfairX \citep{dong2024rlhf}}\label{tab:main-performance}
    \scalebox{0.87}{
    \begin{tabular}{lcccccccccccc}
        \toprule
        Model & \multicolumn{4}{c}{Mistral-Base (7B)} & \multicolumn{4}{c}{Mistral-Instruct (7B)} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9}
        & \multicolumn{2}{c}{Alpaca Eval 2}  & \multirow{1}{*}{MixEval} & \multirow{1}{*}{MixEval-Hard} & \multicolumn{2}{c}{Alpaca Eval 2}  & \multirow{1}{*}{MixEval} & \multirow{1}{*}{MixEval-Hard} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9}
        & LC WR & WR & Score & Score & LC WR & WR & Score & Score \\
        \midrule
        SFT    & 8.4  & 6.2    &  0.602  & 0.279  & 17.1 & 14.7  & 0.707 & 0.361 \\
        \midrule
        \multicolumn{9}{c}{Reward model: LLM-Blender \citep{jiang2023llm}}  \\
        \midrule
        RRHF   & 11.6 & 10.2  &  0.600  & 0.312  & 25.3 & 24.8  &   0.700    & 0.380 \\
        SLiC-HF & 10.9 & 8.9    & 0.679  &   0.334 & 24.1 & 24.6  &   0.700    & 0.381 \\
        DPO    & 15.1 & 12.5  &  0.686  &  0.341 & 26.8 & 24.9  & 0.702 & 0.355 \\
        IPO    & 11.8 & 9.4   &  0.673  & 0.326  & 20.3 & 20.3  & 0.695 & 0.376 \\
        CPO    & 9.8  & 8.9    & 0.632   &  0.307 & 23.8 & 28.8  & 0.699 & 0.405 \\
        KTO    & 13.1 & 9.1   & \textbf{0.704}  & 0.351   & 24.5 & 23.6  &   0.692    & 0.358 \\
        % ORPO   & 14.7 & 12.2 & 7.0  &   &    & 24.5 & 24.9 & 20.8 &  0.703     & 0.378 \\
        RDPO   & 17.4 & 12.8   & 0.693  & 0.355   & 27.3 & 24.5  &   0.695    & 0.364 \\
        SimPO  & 21.5 & 20.8 &  0.672  &  0.347 & 32.1 & 34.8  & 0.702  & 0.363 \\
        Iterative DPO  & 18.9  & 16.7  & 0.660   & 0.341  & 20.4 & 24.8  & 0.719  & 0.389 \\
        \midrule
        \Ours (Contrastive) & 31.6 & 30.8  &   0.703 & 0.409  & 32.7 & 38.6  &  0.718 & \textbf{0.418} \\
        \Ours (LambdaRank) &  \textbf{34.9} & \textbf{37.2} & 0.695 &  \textbf{0.452}  & \textbf{32.9} & \textbf{38.9}   & \textbf{0.720} & 0.417  \\
        \Ours (ListMLE) & 31.1  &  32.1   &  0.669  & 0.390  &  29.7 & 36.2    & 0.709  & 0.397 \\
        \midrule
        \multicolumn{9}{c}{Reward model: FsfairX \citep{dong2024rlhf}}  \\
        \midrule
        \Ours (Contrastive) & \textbf{41.5} & \textbf{42.9} & 0.718 & 0.417    & \textbf{43.0}  & \textbf{53.8} & 0.718 & 0.425   \\
        \Ours (LambdaRank) & 35.8 & 34.1 & 0.717 & 0.431   & 41.9  & 48.1 & \textbf{0.740} & \textbf{0.440}  \\
        \Ours (ListMLE) & 36.6 & 37.8 & \textbf{0.730} & \textbf{0.423}   & 39.6  & 48.1 & 0.717 & 0.397   \\
        \bottomrule
    \end{tabular}}
    % \vspace{-0.1in}
\end{table*}
