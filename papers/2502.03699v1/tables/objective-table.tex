
% \begin{table*}[t]
%     \centering
%     % \renewcommand{\arraystretch}{1.2}
%     \caption{Preference optimization objective study on AlpacaEval2 and MixEval. For AlpacaEval2, we report the result with both opensource LLM evaluator \texttt{alpaca\_eval\_llama3\_70b\_fn} and GPT4 evaluator \texttt{alpaca\_eval\_gpt4\_turbo\_fn}. SFT corresponds to the initial chat model.}\label{tab:objective}
%     \small
%     \scalebox{0.85}{\begin{tabular}{llccccccccc}
%         \toprule
%         & & \multicolumn{2}{c}{AlpacaEval 2 (opensource LLM)} & \multicolumn{2}{c}{AlpacaEval 2 (GPT-4)} & \multicolumn{1}{c}{MixEval} & \multicolumn{1}{c}{MixEval-Hard} \\
%          \cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-7} \cmidrule(r){8-8}
%         & Method & LC Winrate & Winrate & LC Winrate & Winrate & Score & Score \\
%         \midrule
%         \multirow{6}{*}{\rotatebox{90}{Gemma2-2b-it}} & SFT & 47.03 & 48.38 & 36.39 & 38.26 & 0.6545 & 0.2980 \\
%         \cmidrule{2-8}
%         & pairwise & 55.06 & 66.56 & 41.39 & 54.60 & 0.6740 & 0.3375 \\
%         & contrastive & 60.44 & 72.35 & 43.41 & 56.83 & 0.6745 & 0.3315 \\
%         & ListMLE & 63.05 & 76.09 & 49.77 & 62.05 & 0.6715 & 0.3560 \\
%         & LambdaRank & 58.73 & 74.09 & 43.76 & 60.56 & 0.6750 & 0.3560 \\
%         \midrule
%         \multirow{6}{*}{\rotatebox{90}{Mistral-7b-it}} & SFT & 27.04 & 17.41 & 21.14 & 14.22 & 0.7070 & 0.3610 \\
%         \cmidrule{2-8}
%         & pairwise & 49.75 & 55.07 & 36.43 & 41.86 & 0.7175 & 0.4105 \\
%         & contrastive & 52.03 & 60.15 & 38.44 & 42.61 & 0.7260 & 0.4340 \\
%         & ListMLE & 48.84 & 56.73 & 38.02 & 43.03 & 0.7360 & 0.4200 \\
%         & LambdaRank & 51.98 & 59.73 & 40.29 & 46.21 & 0.7370 & 0.4400 \\
%         \bottomrule
%     \end{tabular}}
% \end{table*}

\begin{table}[t]
    \centering
    % \renewcommand{\arraystretch}{1.2}
    % \vspace{-0.1in}
    \caption{Preference optimization objective study on AlpacaEval2 and MixEval. SFT corresponds to the initial chat model.}\label{tab:objective}
    \small
    \scalebox{0.99}{\begin{tabular}{llcccccc}
        \toprule
        & & \multicolumn{2}{c}{AlpacaEval 2} & \multicolumn{1}{c}{MixEval} & \multicolumn{1}{c}{MixEval-Hard} \\
         \cmidrule(r){3-4} \cmidrule(r){5-5} \cmidrule(r){6-6}
        & Method & LC Winrate & Winrate & Score & Score \\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{Gemma2-2b-it}} & SFT & 36.39 & 38.26 & 0.6545 & 0.2980 \\
        \cmidrule{2-6}
        & pairwise & 41.39 & 54.60 & 0.6740 & 0.3375 \\
        & contrastive & 43.41 & 56.83 & 0.6745 & 0.3315 \\
        & ListMLE & \textbf{49.77} & \textbf{62.05} & 0.6715 & \textbf{0.3560} \\
        & LambdaRank & 43.76 & 60.56 & \textbf{0.6750} & \textbf{0.3560} \\
        \midrule
        \midrule
        \multirow{6}{*}{\rotatebox{90}{Mistral-7b-it}} & SFT & 21.14 & 14.22 & 0.7070 & 0.3610 \\
        \cmidrule{2-6}
        & pairwise & 36.43 & 41.86 & 0.7175 & 0.4105 \\
        & contrastive & 38.44 & 42.61 & 0.7260 & 0.4340 \\
        & ListMLE & 38.02 & 43.03 & 0.7360 & 0.4200 \\
        & LambdaRank & \textbf{40.29} & \textbf{46.21} & \textbf{0.7370} & \textbf{0.4400} \\
        \bottomrule
    \end{tabular}}
    % \vspace{-0.1in}
\end{table}
