\section{Background and related work}
\label{Sec: background}

\paragraph{(Blind) image steganography} refers to the technique of hiding secret but retrievable messages in an image with minimal change to the image~\citep{zhu2018hidden}. Given an arbitrary natural image $I \in \gI$, where $\gI$ denotes the set of natural images, and an arbitrary $n$-bit message $\mb w \in \{0, 1\}^n$, an image steganography system typically consists of an encoder $E$---which takes any image $I$ and any message $\mb w$ and produces an encoded image, a decoder $D$---which takes any image and produces an informative message, and its system goal: ($\circ$ means function composition)  
\begin{subequations}
\label{eq:goal-stega}
\begin{align}
\label{eq: steganography def 1}
& (D \circ E) (I, \mb w) = \mb w, \; & \forall I \in \gI, & \; \forall \mb w \in \{0, 1\}^n, & \quad \text{(correctly encode and decode $\mb w$)} \\
% \label{eq: watermark def}
\label{eq: steganography def 2}
& D(I) = \emptyset, \; & \forall I \in \gI, & & \quad \text{(no useful message decoded from a clean image)}\\
\label{eq: steganography def 3}
& E(I, \mb w) \approx I, \; & \forall I \in \gI,
 & \; \forall \mb w \in \{0, 1\}^n. & \quad \text{(minimal encoding distortion to the image)} 
\end{align}
\end{subequations}
Existing steganography methods differ by whether the encoder and decoder are manually designed or learned from data. Manually designed encoder-decoder pairs rely on ideas such as manipulating the least significant bit (LSB) \citep{tirkel1993electronic}, template matching in the Fourier domain \citep{pereira2000robust}, discrete wavelet transform (DWT), discrete cosine transform (DCT), and singular value decomposition (SVD) \citep{bi2007robust, pereira2000robust, navas2008dwt}. In contrast, training-based methods often learn DNN-based encoder-decoder pairs from data, based on variants of a model formulation derived from the goal stated in \cref{eq:goal-stega}: 
\begin{gather}
\begin{aligned}
\min_{\mb \phi, \mb \theta} ~ & \bb E_{\mb w, I} \ell_{m}[\mb w, (D_{\mb \theta} \circ E_{\mb \phi}) (I, \mb w)] & \text{(to ensure \cref{eq: steganography def 1})} \\
\st \; & \ell_{q} (I, E_{\mb \phi} (I, \mb w)) \leq \delta,  \quad \forall I \in \gI, ~ \forall \mb w \in \{0, 1\}^n, ~~ & \text{(to ensure \cref{eq: steganography def 3})}
% \st \; & D_{\theta} (I) \ne \mb w, \;  
% & \forall I \in \gI, ~ \forall \mb w \in \{0, 1\}^n, ~~ & \text{(to achieve \cref{eq: watermark def})}
\label{eq: learning-based steganography obj}
\end{aligned}   
\end{gather}
where $\mb \phi$ and $\mb \theta$ are learnable weights of the DNNs in $E$ and $D$, respectively; $\ell_m$ and $\ell_q$ are two losses measuring the error of \textbf{m}essage recovery and the \textbf{q}uality distortion to the image, respectively; and $\delta$ is the maximally allowed perturbation to the image caused by watermarking embedding. Representative training-based methods include HIDDEN and its variants \citep{zhu2018hidden,wen2019romark,luo2020distortion}, SteganoGAN \citep{zhang2019steganogan}, Stable Signature \citep{fernandez2023stable}, rivaGAN \citep{zhang2019robust}, StegaStamp \citep{tancik2020stegastamp}, Mbrs \citep{jia2021mbrs} and TrustMark \citep{bui2023trustmark}. These methods typically also incorporate regularization terms to encourage the distribution of the encoded images to be close to that of the original images based on generative adversarial networks (GAN) \citep{goodfellow2014generative}. SSL \citep{fernandez2022watermarking} and RoSteALS \citep{bui2023rosteals} are similar in spirit but perform learning in different spaces. In theory, solving \cref{eq: learning-based steganography obj} with a reasonably small $\delta$ can always produce distortion patterns that are \emph{invisible} to human eyes. However, existing methods typically work with heuristic penalty or regularization forms of \cref{eq: learning-based steganography obj} and therefore do not necessarily find feasible solutions to \cref{eq: learning-based steganography obj}, e.g., \citep{zhu2018hidden}. In addition, the choice of $\ell_q$ can differ, e.g., mean squared error (MSE)~\citep{zhu2018hidden,zhang2019steganogan}, LPIPS distance~\citep{zhang2018unreasonable}. Therefore, different methods lead to different levels of visible distortions. \cref{Fig: watermark vis} visualizes several popular training-based methods (together with the manually designed DwtDctSVD) and highlights the different distortion levels that they lead to.
\input{tex-figure/Fig-watermark-vis}

\paragraph{(Blind) image watermarking}
Based on steganography, a natural way to trace AI-generated images is to assign \emph{a fixed message} $\mb w$ as the signature of the content owner (e.g., a company) and apply steganography to generate images containing the signature, i.e., watermarked images, to achieve~\citep{jiang2023evading}:
\begin{subequations}
\label{eq: watermark def}
\begin{align}
& D(E(I, \mb w)) = \mb w, \; & \forall I \in \gI, & \quad \text{($\mb w$ is a fixed message representing the signature)} \\
& D(I) \ne \mb w, \; & \forall I \in \gI, & \quad \text{(messages decoded from unwatermarked images should not be $\mb w$)}  \\
& E(I, \mb w) \approx I, \; & \forall I \in \gI.
& \quad \text{(minimal encoding distortion to the image)} 
\end{align}   
\end{subequations}
In practice, whether an image $I$ is watermarked by $\mb w$ can be detected by comparing the decoded message with $\mb w$:
\begin{equation}
\label{Eq: detection bar}
    \mathbbm{1} [ BA(D(I), \mb w) > \gamma],
\end{equation} where $BA$ denotes the bitwise accuracy and $\gamma$ is a preset task-dependent threshold~\citep{jiang2023evading,fernandez2023stable,yu2021artificial}.  Due to the similarity between \cref{eq: watermark def} and \cref{eq:goal-stega}, most existing work considers image watermarking as a special application of steganography, e.g., \cite{zhu2018hidden,tancik2020stegastamp,an2024benchmarking,Zhao2024SoKWF}. As a result, the learning formulation in \cref{eq: learning-based steganography obj} is also widely adopted in works that only focus on watermarking systems, e.g., \cite{zhang2019robust,fernandez2023stable}. 
% However, we have a different idea---solving \cref{eq: learning-based steganography obj} for a watermarking system can be an overkill. Since the role of a watermark is a signature of the content owner, a fixed bitstring $\mb w$ can already be sufficient. In fact, evaluating watermarking systems with a fixed $\mb w$ is common practice, e.g., \cite{jiang2023evading,fernandez2022watermarking,an2024benchmarking}. Therefore, training a watermarking system can be modified accordingly with a fixed $\mb w$ instead of considering all $\mb w \in \{0, 1\}^n$, as shown below:
% \begin{gather}
% \begin{aligned}
% & \min_{\phi, \theta} ~ \bb E_{I} [\ell_{D}(\mb w, D_{\theta}(E_{\phi} (I, \mb w)))], & \text{(using a fixed watermark $\mb w$)}\\
% & \st \; \ell_{s} (I, E_{\phi} (I, \mb w)) \leq \delta,  \quad \forall I \in \gI. ~~ & \text{(to ensure \cref{eq: steganography def 3})}
% \label{eq: modified learning-based watermark obj}
% \end{aligned}   
% \end{gather}
% which is a strictly easier problem to solve than \cref{eq: learning-based steganography obj},\footnote{For example, the state space for $\mb w$ in \cref{eq: learning-based steganography obj} is $2^{32}$ if $\mb w$ is 32-bit. This requires enormous samples ($\sim 10^{11}$) for term $\bb E_{\mb w}$, which is not satisfied by any of the existing methods listed above in their experiments.} and may lead to improved training efficiency and system reliability. Since our focus in this paper is on the evaluation side of watermarking systems, we leave further discussion related to \cref{eq: modified learning-based watermark obj} in future work.

The above watermark methods are \emph{post-processing} in nature, as the watermark is embedded on any given image $I$ that is already generated. There is an emerging line of \emph{in-processing} watermark methods that directly modify the image generation process~\citep{Zhao2024SoKWF,an2024benchmarking}, including TreeRing watermark~\citep{wen2023tree}, stable signature~\citep{fernandez2023stable}, Gaussian shading watermark~\citep{Yang2024GaussianSP}, and pseudorandom error-correcting code watermark~\cite{Gunn2024AnUW}. In these in-processing methods, 
there is no notion of ``clean images'' $I$ and every generated watermark image exhibits a semantic shift compared to the image generated without the in-process watermarking; see \cref{Fig: watermark vis treering} for an example generated by the TreeRing watermark. \textbf{In this paper, we focus on post-processing watermark methods due to their flexibility, as they are agnostic to the image generation process}.  

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-2em}
    \includegraphics[width=0.5\textwidth]{source-figures/watermark-vis/treering_watermark.jpg}%
    \vspace{-2em}
    \caption{Visualization of a TreeRing watermark example, where \textbf{(a)} and \textbf{(b)} are images generated from the same text prompt input using the same image diffusion model without and with the in-process watermarking, respectively.}%
    \label{Fig: watermark vis treering}
    \vspace{-1em}
\end{wrapfigure}



% \paragraph{Image watermarks and existing methods}
% A watermarking system is typically composed of an encoder $E$, a decoder $D$ and a watermark $\mb w$ in terms of a $n$-bit bitstring, e.g., $\mb w = 1011 \ldots 110$. Formally, a watermarked image $I_{w}$ is produced by passing an image $I$ and the watermark $\mb w$ as inputs to the encoder: $I_{w} = E(I, \mb w)$; the decoder can retrieve a bitstring $\mb w_{I}$ from an arbitrary image $I$: $w_{I} = D(I)$. The ideal watermarking system should ensure:
% % \begin{gather}
% % \begin{aligned}
% % & D(E(I, \mb w)) = \mb w, \; \forall I \in \gI, & \quad \text{(correctly encode and decode the watermark)} \\
% % & D(I) \ne \mb w, \; \forall I \in \gI, & \quad \text{($\mb w$ should not be decoded from unwatermarked images)} 
% % \label{eq: watermark def}
% % \end{aligned}   
% % \end{gather}
% % \begin{align}
% % \label{eq: watermark def 1}
% % & D(E(I, \mb w)) = \mb w, \; \forall I \in \gI, & \quad \text{(correctly encode and decode the watermark)} \\
% % \label{eq: watermark def}
% % & D(I) \ne \mb w, \; \forall I \in \gI, & \quad \text{($\mb w$ should not be decoded from unwatermarked images)} 
% % \end{align}
% where $\gI$ is a distribution of images. In practice, whether an image $I'$ contains watermark $\mb w$ is detected by calculating the bitwise accuracy ($BA$): $\mathbbm{1} [ BA(D(I'), \mb w) > \gamma]$, where $\gamma$ is a preset task-dependent threshold.
% Existing watermarks can be grouped into two categories: non-training-based and training-based. Watermarks such as the translucent overlaid layer of logos that can be read directly by humans \citep{kankanhalli1999adaptive}, and invisible watermarks based on least significant bit (LSB) \citep{tirkel1993electronic}, template matching in Fourier domain \citep{pereira2000robust}, discrete wavelet transform (DWT), discrete cosine transform (DCT), and singular value decomposition (SVD) \citep{bi2007robust, pereira2000robust, navas2008dwt} are all based on manually designed rules. For training-based watermarks, the encoder and decoder are automatically learned from the data using DNNs. Existing methods typically treat the watermarking system as a task of steganography and follow the training goal:
% \begin{gather}
% \begin{aligned}
% \min_{\phi, \theta} ~ & \bb E_{\mb w, I} [\ell_{D}(\mb w, D_{\theta}(E_{\phi} (I, \mb w)))] & & \text{(to achieve \cref{eq: watermark def 1})} \\
% \st \; & D_{\theta} (I) \ne \mb w, \;  
% & \forall I \in \gI, ~ \forall \mb w \in \{0, 1\}^n, ~~ & \text{(to achieve \cref{eq: watermark def})} \\
% & \ell_{s} (I, E_{\phi} (I, \mb w)) \leq \delta,  \; & \forall I \in \gI, ~ \forall \mb w \in \{0, 1\}^n, ~~ & \text{(to ensure the quality of $I_w$)} 
% \label{eq: learning-based watermark obj}
% \end{aligned}   
% \end{gather}
% where $\phi$ and $\theta$ are parameters of the encoder $E$ and decoder $D$ networks; $\ell_D$ and $\ell_s$ are two distance functions; and $\delta$ is a maximally allowed budget for the watermark pattern embedded to the original image $I$. And all use the encoder-decoder structure as \cref{Fig: watermark system vis}. The $\ell_s(\cdot)$ constraint in \cref{eq: learning-based watermark obj} is necessary to ensure that the generated $I_w$ is useful---it does not differ from $I$ too much and preserves the original image content. Methods such as HIDDEN and its variants \citep{zhu2018hidden,wen2019romark,luo2020distortion}, SteganoGAN \citep{zhang2019steganogan}, Stable Signature \citep{fernandez2023stable}, rivaGAN \citep{zhang2019robust}, StegaStamp \citep{tancik2020stegastamp}, UDH \citep{zhang2020udh} and Mbrs \citep{jia2021mbrs} all belong to this group. Within the training-based approaches, SSL \citep{fernandez2022watermarking} is slightly different, where $I_w$ and $\theta$ are the optimization variables instead of $\theta$ and $\phi$, and the encoder $E$ is fixed with a heuristic data hiding rule. Theoretically, solving \cref{eq: learning-based watermark obj} with a reasonably small $\delta$ can always produce watermarks that are \emph{invisible} to human eyes. However, existing methods typically use penalty/regularization forms and do not strictly find feasible solutions to \cref{eq: learning-based watermark obj} \citep{zhu2018hidden}. 
% In addition, the choice of $\ell_s$ can differ, e.g., using mean sure error (MSE) \citep{zhu2018hidden,zhang2019steganogan}, LPIPS distance \citep{zhang2018unreasonable}, etc. As a result, different watermarking methods are observed with different visibility levels in practice. \cref{Fig: watermark vis} visualizes four popular training-based watermarks (together the non-training-based DwtDctSVD) and shows their different visibility levels. On the other hand, it is worth noting that solving \cref{eq: learning-based watermark obj} is an overkill for watermarking systems---only one or a limited number of $w$ is sufficient for watermarking purposes; and there is no need to cover the entire space of $w \in \{0, 1\}^n$. Thus, it is sufficient to consider the following objective for watermarking:
% \begin{gather}
% \begin{aligned}
% & \min_{\phi, \theta} ~ \bb E_{I} [\ell_{D}(\mb w, D_{\theta}(E_{\phi} (I, \mb w)))] & \quad & \text{(using a fixed watermark $\mb w$)},
% \label{eq: modified learning-based watermark obj}
% \end{aligned}   
% \end{gather}
% which is much easier to solve than \cref{eq: learning-based watermark obj},\footnote{For example, the state space for $\mb w$ in \cref{eq: learning-based watermark obj} is $2^{32}$ if $\mb w$ is 32-bit. This requires enormous samples ($\sim 10^{11}$) for term $\bb E_{\mb w}$, which is not satisfied by any of the existing methods listed above in their experiments.} and may lead to improved training efficiency and system reliability. In the case where several watermarks are necessary, it is also sufficient to train several systems independently via \cref{eq: modified learning-based watermark obj}. Since our focus in this paper is on the evaluation side of watermarking systems, we leave further discussion related to \cref{eq: modified learning-based watermark obj} in future work.

\paragraph{Robustness of watermarking systems}
% \label{subsec: watermark evasion}
Robustness of an image watermarking system refers to the extent of the watermark to remain detectable by the decoder $D$ when the watermarked image is manipulated (also called ``evaded'' if such manipulation is a deliberate attack). Thus, robustness is typically associated with the potential of a watermarking system to be applied to copyright protection and misinformation detection. To stress test the robustness of watermark systems, various watermark evasion techniques have been proposed. These techniques are broadly classified into white-box and black-box evasions, depending on whether any component of the watermark system is known to the evader~\citep{an2024benchmarking}. \textbf{In this paper, we focus on black-box evasions where nothing about the watermark system is known}, as in practice, companies tend to keep their watermark system private. Existing black-box evasions can be classified into two groups: \textbf{Corruption methods} try to distort watermarked images so that watermarks become corrupted and undetectable. Classical digital editing (e.g., applying Gaussian noise, Gaussian blur, JPEG compression, etc. \citep{voyatzis1999protecting}), the query-based adversarial attack (WevadeBQ) in \cite{jiang2023evading} and the surrogate attack in \cite{saberi2023robustness} belong to this group; \textbf{Purification methods} treat embedded watermark patterns as noise signals and attempt to remove them using denoising and regeneration techniques, such as BM3D \citep{dabov2007image}, diffusion models \citep{saberi2023robustness,zhao2023invisible} and VAE \citep{zhao2023invisible}. The rationale behind the purification methods is rooted in \cref{eq: watermark def}, where the original image, if recovered successfully, is always an evasion. In addition, the original image will be the ultimate threat to any watermarking system---the watermark is evaded by an image without any loss of quality.


\section{Our method: DIP for black-box watermark evasion}
\label{Sec: methods}

\subsection{Watermark evasion via DIP-based blind denoising}
\begin{wrapfigure}{r}{0.4\textwidth}
    \vspace{-4em}
    \includegraphics[width=0.4\textwidth]{source-figures/DIP-illustration/dip-illustration.png}
    \vspace{-2em}
    \caption{A typical image quality (measured by peak signal-to-noise ratio, PSNR) vs. iteration curve when DIP is used in blind denoising tasks. An early stopping method is used to detect the iteration achieving the peak performance (red dashed line). (Figure adapted from \cite{wang2021early} under the Creative Commons 4.0 license)}
    \label{Fig: DIP demo}
    % \vspace{1em}
\end{wrapfigure}
\paragraph{Deep Image Prior (DIP)}
\label{subsec: DIP background} refers to the technique of using \textbf{untrained} DNN as an implicit prior for natural images in solving image recovery problems, \textbf{without training on massive data}: for any natural image $I$, DIP parametrizes it as $I = G_{\mb \theta}(\mb z)$, where $G_{\mb \theta}$ is typically a trainable convolutional neural network (CNN) and $\mb z$ is a fixed input (typically randomly drawn). Now consider the canonical optimization-based formulation for image recovery problems: 
\begin{align}
    \min\nolimits_{I} \; \ell\paren{\mb y, f(I)} + \lambda R(I),  
\end{align}
where $\mb y \approx f(I)$ is the observation model, $\ell(\mb y, f(I))$ measures the recovery error, and $R(\cdot)$ denotes regularization on $I$. DIP transforms the formulation into 
\begin{align}
      \min\nolimits_{\mb \theta} \; \ell\paren{\mb y, f(G_{\mb \theta}(\mb z))} + \lambda R(G_{\mb \theta}(\mb z)). 
\end{align}
Note that the optimization is \emph{only} with respect to the CNN weights $\mb \theta$. The resulting formulation is then solved by first-order optimizers, such as ADAM~\citep{kingma2014adam}. Such a simple strategy, in combination with appropriate early stopping methods~\citep{li2021self,wang2021early,shi2022measuring} that pick the best intermediate recovered images, has proved highly successful in solving a wide range of image recovery problems, from simple denoising~\citep{ulyanov2018deep} to advanced scientific and medical reconstruction problems~\citep{Tirer2023DeepIL,DBLP_journals_pami_QayyumISBBQ23,zhuang2023advancing,zhuang2024blind,zhuang_practical_2023,jdd_doubledip,li_random_2023,li2023deep}.

\paragraph{Watermark evasion via DIP-based blind denoising}
\input{tex-algo/algo-dip-evasion}
Now, consider an arbitrary watermarked image $I_{\mb w} \doteq E(I, \mb w)$, where we do not know $E$ or $\mb w$. Since $I_{\mb w} \approx I$ as required in \cref{eq: watermark def} and $I$ is clearly a successful invasion, it is sensible to try to ``purify'' or ``denoise'' $I_{\mb w}$ toward $I$---the intuition behind all purification methods for black-box evasion~\citep{dabov2007image,saberi2023robustness,zhao2023invisible}. 

DIP has proven effective in single-image denoising, e.g., with Gaussian, impulse, shot noise, etc., when combined with appropriate early stopping strategies~\citep{mataev2019deepred,DBLP_conf_iccv_JoCC21,li2021self,wang2021early,li2023deep,li_random_2023,jdd_doubledip}. In particular, when the noise level is low---which is true for typical watermarking systems as $I_{\mb w}$ is supposed to be very close to $I$, a simple formulation with the standard mean-squared-error (MSE) loss and the additive noise model can perform ``blind'' simultaneous denoising for multiple types of noise~\cite{li2021self,wang2021early}. Inspired by this, we propose a simple DIP-based blind watermark-evasion formulation 
\begin{align} \label{eq:main-form}
    \min\nolimits_{\mb \theta} \; \norm{I_{\mb w} - G_{\mb \theta}(\mb z)}_2^2,  \quad (\textbf{DIP-based watermark evasion})
\end{align}
for any given watermarked image $I_{\mb w}$. Unlike DIP-based blind denoising which requires appropriate early stopping strategies to find optimal denoising, we only need to check the evasion success of all iterates when iteratively solving \cref{eq:main-form} by querying the watermark decoder. Our whole algorithm pipeline is summarized in \cref{alg: DIP evasion}. While we do not invent DIP-based blind denoising, we are the first to explore it for blind evasion of invisible watermarks. \cite{braindotai2021watermark} also performs DIP-based watermark removal, but solves it as \emph{image inpainting} which requires \textbf{known watermark patterns and locations}. 

\subsection{Why including DIP-based evasion as a baseline method? }
\label{sec:why-include-dip}
% \input{tex-figure/Fig-watermark-fourier}
The primary obstacle for purification methods is that the ``noise'' patterns induced by watermark methods, especially those training-based ones, may not follow any simple noise model. Consequently, classical denoising methods that target specific noise types, such as BM3D, may struggle to remove the watermark. The recent regeneration techniques via diffusion models or VAE effectively perform learned denoising. However, a priori, it is unclear they can generalize well to novel watermark patterns. \textbf{In contrast, our DIP-based evasion operates on a different principle}: it leverages the different rates at which different frequency components are captured during DIP learning through \cref{eq:main-form}, which has been consistently observed in prior DIP literature~\citep{ulyanov2018deep,DBLP_journals_ijcv_ShiMMS22,li2021self,wang2021early}. Specifically, the $G_{\mb \theta}(\mb z)$ term in \cref{eq:main-form} picks up the low-frequency components---which dominate natural images, \textbf{much faster} than picking up the high-frequency components---which tend to be noise-induced, likely watermark-induced for our case. 
\begin{SCfigure}[][!htbp]
    % \vspace{-1.5em}
    \includegraphics[width=0.65\textwidth]{source-figures/watermark-fourier/fourier.jpg}
    % \vspace{-2em}
    \caption{Visualization of 2D Fourier spectra of the clean and watermarked images from \cref{Fig: watermark vis} (magnitudes visualized in $\log$ scale). The histogram below each spectrum plot shows the band-wise energy distribution in the radial direction (i.e., the dashed arrow direction), where the $y$-axis is in $\log$ scale.}
    \label{Fig: watermark fourier vis}
    \vspace{-1em}
\end{SCfigure}
\begin{itemize}[nosep,leftmargin=1em]
    \item In \cref{Fig: watermark fourier vis}, we compare the Fourier spectrum of the clean image from \cref{Fig: watermark vis} to those of various watermarked versions: clearly, the spectrum of the clean image concentrates on low frequencies, but different watermark methods reshape the Fourier spectrum by mostly changing the mid-to-high frequencies. For example, RoSteALS and StegaStamp mostly affect the low-mid frequencies, DwtDctSVD and SSL mostly focus on mid-high frequencies, and rivaGAN alters high frequencies. The only exception is TrustMark, whose watermark pattern is hard to observe from the frequency spectrum. 

    \item Next, in \cref{Fig: fourier band error}, we visualize the different learning paces of DIP across different frequency bands, by tracking the frequency band errors (FBEs) similar to \cite{wang2024dmplug,li2023deep,zhuang2024blind}: we first compute the relative per-frequency error in the frequency domain, i.e., $|\mathcal{F}(I_w) - \mathcal{F}(G_{\mb \theta^{(i)}}) / |\mathcal{F} (I_{\mb w})| $, where $\mathcal{F} (\cdot)$ is the discrete Fourier transform, then divide all frequencies into five radial bands from the lowest (1) to the highest (5), and compute the mean errors within each band. It is evident that the lower the frequencies, the faster the FBEs decay. 
    \begin{SCfigure}[][!htbp]
    \includegraphics[width=0.65\textwidth]{source-figures/watermark-fourier/Fourier_band_error.jpg}
    \caption{Evolution of the Fourier band errors (FBEs) of DIP's intermediate iterates ($x$-axis: iteration count; $y$-axis: relative band error). We visualize FBEs by dividing all frequency components into five different bands in the radial direction, from the lowest (1) to hightest (5).}
    \label{Fig: fourier band error}
    \vspace{-1em}
\end{SCfigure}
\end{itemize}
Such disparate learning paces imply that certain intermediate iterates enjoy sufficient separation of the low-frequency image content and high-frequency watermark patterns, hence the potential for successful evasion. In summary, compared to alternative purification methods that depend on either explicit noise modeling or data-driven priors, our DIP-based evasion relies on frequency separation that is noise-agnostic and distribution-free. Hence, our DIP-based evasion method largely complements the existing ones. 

\subsection{Remarks on improving the query efficiency}
\label{sec:query-effi}
\cref{alg: DIP evasion} often takes up to $\sim 1000$ iterative steps to solve \cref{eq:main-form}. Querying the decoder using all intermediate iterates, as described in \cref{alg: DIP evasion}, may be costly, and the decoder server may also limit query frequency for individual users to ensure safety and fairness. To reduce the number of queries and thereby improve query efficiency, it makes sense to only send high-quality iterates to the decoder---again, we emphasize both image quality and evasion success in this paper. To this end, although the clean image $I$ is unknown, the watermarked image $I_{\mb w}$ is very close to $I$ by design (i.e., per \cref{eq: watermark def}). Therefore, we can take $I_{\mb w}$ as a proxy reference to estimate the image quality of intermediate iterates; see \cref{App: psnr-ba} for exemplar trajectories of image quality and watermark detectability on different watermarks by different evasion methods. 