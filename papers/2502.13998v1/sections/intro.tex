\section{Introduction}
\label{Sec: Intro}
In this prosperous era of generative AI, the traceability of AI-generated content (e.g., language, images, and videos) to its source has been frequently mentioned as a promising solution to promote the responsible use of generative AI \citep{fan2023trustworthiness}, e.g., to protect copyright or to curb misinformation. In particular, the traceability of AI-generated images has become increasingly urgent, as many AI products, such as DALL-E \citep{ramesh2022hierarchical} and Stable Diffusion \citep{rombach2022high}, can create highly photorealistic and artistic images that are hard to distinguish from natural photos or human drawings. Unsurprisingly, some AI-generated images have caused false beliefs on social media \citep{bbc2024ai}. Thus, major tech companies such as Google, OpenAI \citep{rueters2023openai} have recently opted to incorporate watermarks into their image generation products to improve traceability and promote responsible use. 


\begin{figure*}[!htbp]
\centering
\begin{tabular}{ccc}
\includegraphics[width=0.23\textwidth]{source-figures/watermark-vis/clean.png}
&\includegraphics[width=0.23\textwidth]{source-figures/watermark-vis/visible-watermark.png}
&\includegraphics[width=0.23\textwidth]{source-figures/watermark-vis/stegastamp.png}\\
\small{\textbf{(a)}}%
&\small{\textbf{(b)}}%
&\small{\textbf{(c)}}%
\end{tabular}
\caption{Example of (a) a clean image, (b) an image with an overlaid logo watermark and (c) an image with steganography watermark.}%
\vspace{-0.5em}
\label{Fig: watermark vis intro}
\end{figure*}
\paragraph{Manually designed vs. training-based watermarks}
Watermarking methods can be generally divided into two categories: \textbf{(i)} \emph{non-blind} methods \citep{cox1997secure,hsieh2001hiding,pereira2000fast} and \textbf{(ii)} \emph{blind} methods \citep{bi2007robust}, which are divided by whether access to original clean images is required to correctly decode the watermarked images~\citep{Zhao2024SoKWF}. In what follows, we focus only on blind methods (and refer to them as \emph{watermarks}), as they do not require access to clean images and fit better in large-scale application scenarios, such as tracing AI-generated content. Watermarks are typically embedded in images in terms of an overlaid logo or steganography\footnote{Steganography: detectable messages embedded in an image but are invisible to human eyes.} \citep{morkel2005overview}; see \cref{Fig: watermark vis intro} for an example. The embedded watermarks then are typically detected by human eyes or by algorithmic decoders \citep{voyatzis1999protecting,zhu2018hidden}. In early research, various manually designed watermarks were proposed and applied to copyright protection, e.g., visible watermarks (detectable by human eyes) such as an overlaid layer \citep{kankanhalli1999adaptive} or a color code signature (such as the example in \cref{App: DALLE-2 Example}); invisible watermarks (detectable by algorithmic decoders) such as \cite{tirkel1993electronic,pereira2000robust,navas2008dwt}. However, these manually designed watermarks are challenged by robustness concerns, i.e., imperceptible corruption, digital editing, or deliberate attacks to the watermarked image can make them undetectable~\citep{remove2022remove,zhao2023invisible}; see also \cref{Fig: watermark system vis}. To address these challenges, recent work has shifted to training-based watermarking systems using deep neural networks (DNNs). By incorporating ideas from data augmentation \citep{mumuni2022data} and adversarial training \citep{goodfellow2014explaining}% which have been shown to be effective in boosting the robustness of DNN classifiers
, the robustness of training-based watermarks against common digital corruptions consistently beats that of manually designed ones~\citep{zhu2018hidden,zhang2019robust,tancik2020stegastamp,jia2021mbrs}. 

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{source-figures/watermark-system-vis/watermark-evasion-v0.png}
\caption{Illustration of the watermarking system and the robustness concern---watermarks may be removed with little loss of image quality.}%
\vspace{-1em}
\label{Fig: watermark system vis}
\end{figure*}

\paragraph{Evaluating the robustness of watermark systems} 
Robustness evaluation of watermark systems often operates under either the white-box or the black-box evasion (i.e., attack) setting~\citep{Zhao2024SoKWF,an2024benchmarking}. In 
white-box evasion, the decoder and the groundtruth watermark are accessible to a dedicated evader~\citep{jiang2023evading}, which easily leads to a high threat. In contrast, the threat can be greatly reduced by maintaining secrecy of the system \citep{an2024benchmarking}, i.e., under the \emph{black-box} model, where the decoder and the groundtruth watermark remain unknown to the evader. Nowadays, it is common for companies not to open-source their generative products, e.g., DALLE-3 from OpenAI, Midjourney, Imagen from Google. Thus, being robust against potential black-box evasions is of higher priority in practical scenarios. 

The main focus of this paper is to explore \emph{black-box} evasion techniques that can generate evasion images with the best possible quality. We stress the image quality alongside the evasion success, because for a dedicated evader, the higher the quality of the evasion images they can produce, the more benefit they can potentially gain, e.g., breaking the copyright protection without compromising the image quality or misleading people to use highly naturally-looking fake images. Recently, researchers are putting effort into standardizing the robustness benchmark of image watermarks \citep{an2024benchmarking}. Although the selected black-box evasion methods are effective in bypassing watermark detection, their evasion images can still introduce visible defects; see \cref{Sec: exp and results} and \cref{Fig: watermark evasion vis stegaStamp}. Thus, the search for new methods to provide stronger stress tests of watermarking systems is still a pressing problem.
\input{tex-figure/Fig-watermark-system}

\paragraph{Our contributions} 
In this paper, we propose a new black-box watermark evasion method using the Deep Image Prior (DIP) \citep{ulyanov2018deep}---an untrained DNN-based prior that proves powerful in solving single-image blind denoising and numerous single-instance inverse problems~\citep{DBLP_journals_pami_QayyumISBBQ23,Tirer2023DeepIL,zhuang2023advancing}. Our main contributions include: \textbf{(i)} We show that DIP-based blind denoising can be used to generate high-quality evasion images effective against many existing invisible watermarks, both training-based and manually designed; \textbf{(ii)} We elucidate the principle behind DIP's evasion performance---its faster rate in picking up low frequencies than high ones empowers its image-agnostic watermark purification ability. Due to \textbf{(i)} and \textbf{(ii)}, we advocate including DIP evasion as an integral component in the robustness evaluation of watermarking systems~\citep{saberi2023robustness,an2024benchmarking}; \textbf{(iii)} Based on our analysis, we further recommend that to counteract black-box watermark evasions, a reliable watermark scheme should focus on modifying low-frequency components and have a reasonable magnitude. 