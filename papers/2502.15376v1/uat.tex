\dpinline{I have gone through the proof and made some minor changes and clarifications, mainly to make it a bit more readable.}
\lhnote{Thanks!}
Here we give the complete proof of Theorem \ref{UAT}, or, rather its stronger form in Theorem \ref{main}.

    We consider the network architecture GEBLNet. Given the flux tensor $W_k$, we stack the identity and its Herimitian conjugate to a second channel as 
    $$W'^\gamma_k =(W_{k,0}, W_{k,1}, W_{k, -1}):=(I,W_k, W_k^{-1}).$$
    Afterwards we put it through several blocks, each containing three layers: GEBL, GEAct and TrNorm. In this section, we ignore \trnorm\ Layers, since they are introduced to boost training results. We call each block a \textbf{``packed layer"}. 
    
    After several packed layers we calculate the trace per-channel and add a linear layer (the ``Dense layer") in the end. The Dense layer acts on the real and imaginary parts separately. Then we take the sum over the site index (to calculate the topological invariant).
    
    So the outputs have the following form:
    $$W_k\longmapsto w\cdot \hat{\sigma}\circ \text{GEBL}_n\circ \cdots \circ\hat{\sigma}\circ \text{GEBL}_1(W_k)) + b.$$
    Where $\hat{\sigma}(W^\gamma_k)=\sigma(\tr W^\gamma_k) W^\gamma_k$. We denote the set of these functions by $\mathcal{BLN}_{\sigma} (G)$, where the subscript $\sigma$ indicates the choice of activation function.\dpnote{Why introduce $\sigma$ instead of just $\sigma$? The hatted version is not defined.} We further denote by $\mathcal{BLN}_{\sigma}^k(G)$ the subset of $ \mathcal{BLN}_{\sigma}(G)$ with $k$ packed layers.

    % Furthermore, we denote a certain type of \textbf{"channel-fast-descending"} functions as $\mathcal{BLN}(G)\subset \mathcal{BLN}(G)$. By that we mean $n_{ch}$ of the $i$th GEBL is larger than $2n_{ch}$ of the $(i+1)$th GEBL. 
    Since we attempted to learn local quantities $F(W_k)$, we omit the subscript $k$. Furthermore, we treat the flux $W$ as an abstract element in the Lie group $G$, denoted as $g$. In this case where the input channel size is one, we propose the main result:
    

    \begin{theorem}[Universal Approximation Theorem] For any activation function $\sigma=\tils\circ \re$, where $\tils$ is bounded and non-decreasing, $\mathcal{BLN}_{\sigma}(G)$ is dense in $L^2_{class}(G)$.
    \label{main}
    \end{theorem}

    The proof of this will require the following lemma.
    \begin{lemma}
$\mathcal{BLN}_{\sigma}^k(G)$ is dense in $\{f(p_1,\cdots,p_{2^k}):\|f\|_\infty<\infty\}\subset L^\infty(G)$, where $p_i=\tr g^i$.
\label{fundamentallemma}
    \end{lemma}
    \begin{proof}
        We prove this lemma by induction. For $k=1$, the output has the following form
        $$g\longmapsto \left(\sum_{i=0}^2 \alpha_i^t g^t\right)_i\quad  \longmapsto\quad \omega_i \tr\sigma\left(\sum_{i=0}^2 \alpha_i^t g^t\right)_i + b.$$
        Note that $\tr\hat{\sigma}(\sum_{i=0}^2 \alpha_i^t g^t)=\sigma(\re \alpha_i^tp_t)\alpha_i^tp_t$.
        For any channel index $i$, when taking only the real part (in other words, forcing $w_{i,\text{Im}}$ in the dense layer to be zero), the output is simply\dpnote{$\hat\sigma$ missing on the LHS of (31)?}
        \begin{equation}
            \sigma\left(\sum_t\re \alpha_i^t \re p_t - \im \alpha_i^t \im p_t \right)\left(\sum_t\re \alpha_i^t \re p_t - \im \alpha_i^t \im p_t \right)\\
            =\hat{\sigma}\left(\sum_i\re \alpha_i^t \re p_t - \im \alpha_i^t \im p_t\right)
        \end{equation}
        Therefore, it is essentially a one-hidden-layer fully connected network on 
        $\{(p_1, p_2)\}\simeq\Rb^4$ . Thus the set is dense.
        
        Assume this is the case for $n$, and we would like to prove the lemma for $n+1$. We denote $2^n=N$. Then the layer input has the following form:
        $$\Tilde{\sigma}\left(\sum_{t=0}^{N}a_i^t(p_0,\cdots,p_{N/2})p_t\right)\left(\sum_{t=0}^{N}a_i^t(p_0,\cdots,p_{N/2})g^t\right),$$

        Now the new ``$a^t_i$''(denoted as $b^t_i$) takes the following form:
        $$b_i^t=\sum_{p+q=t}\sum_{j,k}\alpha_{ijk}\Tilde{\sigma}(a_j^tp_t)\Tilde{\sigma}(a_k^tp_t)a_j^p a_k^q.$$
        Consider the bijection $F:\Cb^{N+1}\ra \mathrm{P}_N(\Cb)$, given by $F(\Vec{a})=\sum_{t}a_tz^t$. Using this we define 
        $$\Vec{a} * \Vec{b} = F^{-1}(F(\Vec{a})F(\Vec{b})).$$
        Then $$\Vec{b_i}=\alpha_{ijk}\Tilde{\sigma}(\Vec{a_j}\cdot \Vec{p})\Tilde{\sigma}(\Vec{a_k}\cdot \Vec{p})\Vec{a_j}\Vec{a_k}=\alpha_{ijk}H(p, \Vec{a_j}, \Vec{a_k}).$$
        This forms a linear space $\mathcal{B}^{n+1}\subset (L^\infty(K_{n+1}))^{2N+1}$. For simplicity we henceforth omit the subscript on $K_{n+1}$. 
        
        We assume $(a^t_i)_{t=0}^N$ could approximate any constant function of $p_1, \cdots, p_{N/2}$. This is trivially true when $n=1$, since it is a function on a constant and takes arbitrary constant values.

        Denoting $e_0=F^{-1}(1/d)$, where $d=\dim G$, we have $e_0\cdot p = 1$, $\forall p \in K$. Since $K$ is compact, there exists an open set $U$ s.t. $e_0\in \partial U$, and $b\cdot p \in (1,+\infty)$, $\forall b \in U, p\in K$. 
        
        On the other hand, it is easy to see that $\left\{b*b:b=\begin{pmatrix}
            1,z,\cdots,z^N
        \end{pmatrix}\right\}$ is linearly independent as a subset. This way we could choose $2N+1$ elements $\{\bzt\}_{t=0}^{2N}$ from its intersection with $U$, such that $\text{span}\{\bzt*\bzt\}=\Cb^{2N+1}$. 

        Now given a constant vector $\Vec{b}=(b_0,\cdots,b_{2N})$, there exits $\{\alpha_t\}$ such that $\Vec{b}=\alpha_t \bzt*\bzt$. We want to show that $\Vec{b}$ can be approximated by any precision $\epsilon$.
        
        Without loss of generality, assume $\sup \Tilde{\sigma}=1$ and  $\inf\Tilde{\sigma}=0$. Then, for all $\epsilon$, there exists $M_0>0$ such that for all $x>M_0/2$, $\Tilde{\sigma}(x)\in(\sqrt{1-\epsilon}, 1)$.  This gives 
        $$\left|\frac{d^2}{M^2} H\left(p, M e_0, Me_0\right)-1\right|=|1-\Tilde{\sigma}(M)^2|<\epsilon,\quad \forall M>M_0.$$

        By induction, there exists $a_t$ such that $\|a_t-\bzt\|_\infty<\min\{\epsilon, M/2\}$. Consider $$\Vec{b'}=\alpha_t \frac{1}{M^2}H(p, a_t, a_t) = \alpha_t  \Tilde{\sigma}(M\alpha_t\cdot p)^2a_t*a_t.$$
        Then \begin{eqnarray}
        |\Vec{b'}-\Vec{b}|&=&|\alpha_t (\Tilde{\sigma}(M\alpha_t\cdot p)^2 -1)\bzt*\bzt 
        + \Tilde{\sigma}(M\alpha_t\cdot p)(\bzt*\bzt-\alpha_t*\alpha_t)|\nonumber \\ \nonumber
        &\leq &\alpha_t \epsilon |\bzt*\bzt| + 2\epsilon |\bzt| + \epsilon ^2 \\ 
        &\leq& C(\Vec{b}, N)\epsilon.
        \end{eqnarray}

        % For clarity, we introduce a few notations, along with properties for calculation:
        % \begin{enumerate}
        %         \item given $b^1=
        %     (b^1_0,\cdots,b^1_{N})
        % ,\ b^2=
        %     (b^2_0,\cdots,b^2_{N})
        % $, \\$b^1*b^2\triangleq 
        %     (b^1_0b^2_0,\cdots,\sum_{k}b^1_kb^2_{t-k},\cdots,b^1_Nb^2_N)
        % $; therefore $b^1 * b^2 = b^2 * b^1$.
        %         \item $b_{m} = b + {m} e_0 =
        %     (b_0 + {m},\cdots,b_{N})
        % $, $\breve{\sigma}[m](x)=\Tilde{\sigma}(x+m) - \Tilde{\sigma}(x)$, here $m\in\Rb$.
        %         \item For $p=\begin{pmatrix}
        %     p_1,\cdots,p_{N}
        % \end{pmatrix}$, $b\otimes p = \re(b_0 + \sum_{t=1}^N b_ip_i)$; therefore $b_m\otimes p = b\otimes p + m e_0\otimes p$. 
        %         \item $\tils^1_{m} = \tils(b^1_{m}\otimes p), \bres[m]^1_n=\bres[m](b^1_n\otimes p)$; therefore $\tils^1_m=\bres[m]^1+\tils^1$.
        %     \end{enumerate}

        When the coefficient functions approximate constants, the last layer is essentially a one-hidden-layer fully connected network over $p_1,\cdots,p_{2N}$. Similar to the $N=1$ case, as the width grows larger the network can approximate any function $f(p_1,\cdots,p_{2^N})$. the concludes the proof of the lemma. \qedhere
        % This is clearly another linear space of functions on $p_0,\cdots,p_{N}$, so we only need to prove that $\int_K \Tilde{\sigma}(\cdots)_j\Tilde{\sigma}(\cdots)_k \sum_{p+q=t}a_j^p a_k^q d\mu(x)=0\Ra \mu=0$. 

        % Since we assumed that $a_j^p$ could approximate any function, they could approximate any constant. For two arbitrary constant tuples $\{a_j^p\},\ \{a_k^q\}$, by a perturbation we could assume $\sum_{p+q=t}a_j^p a_k^q\neq 0$, so 
        % $$\int_K \Tilde{\sigma}(a_j^tp_t)\Tilde{\sigma}(a_k^tp_t) d\mu(x)=0$$
        % If $n_{ch}$ is large enough for the $n$th GEBL, the above statement is true for $\{a_j^p\},\ \{a_k^q\}$ a.e., therefore is true for any $\{a_j^p\},\ \{a_k^q\}$ by uniform convergence theorem. In this case we could chose $a_k^q\neq 0$ iff. $q=0$, and $a_k^q=1$. Therefore we have:
        % $$\int_K \Tilde{\sigma}(a^t_jp_t)d\mu(x)=0$$
        % Similar to Corollary \ref{coro:ReLU*x}, we get $\mu=0$.

        % Now we have the new "$a_i^t$''approximate any function of $p_1, \cdots, p_{N}$, we want them to approximate arbitrary constants. For the final dense layer we have the output:
        % $$\omega_i \hat{\sigma}(\sum_0^{2N}"a_i^t"p_t)+b$$
        % Similar to the case where $n=1$, this approximate any function on $p_1, \cdots, p_{2N}$. \qedhere
    \end{proof}

    We may now complete the proof of Theorem \ref{main}.
    \begin{proof}(Proof of Theorem \ref{main})\ \\
        Recall that by Theorem \ref{PW} the space of class functions $L^2_{class}(G)$ is spanned by symmetric polynomials in the eigenvalues of group elements. Since these symmetric polynomials can be expressed in terms of traces $\tr(g), \tr(g^n), \dots, \tr(g^M)$ it follows that any class function can be written as a function of these traces. Now, since $G$ is compact, we have $L^2(G)\supset L^\infty(G)$ and  $\|f\|_2\geq C\|f\|_\infty$.
        Therefore, for all $f\in L^2_\text{class}(G)$, and for any $\epsilon>0$, there exists $$ f_n=f_n(p_1,\cdots,p_n)\in L^\infty(K)$$ such that $\|f-f_n\|_2<1/2\epsilon$. By Lemma \ref{fundamentallemma} the function class $\mathcal{BLN}^k_{\hat{\sigma}}(G)$, consisting of neural networks with $k$ gauge equivariant bilinear layers, can approximate any function $f(p_1, \dots, p_k)$ arbitrarily well, provided $k$ is large enough. We deduce that there exists $g\in \mathcal{BLN}^n_{\hat{\sigma}}(G)\subset L^\infty(G)$\ such that $\|g-f_n\|_\infty<1/2C \epsilon$. Therefore $$\|g-f\|_2<(C\cdot 1/2C + 1/2)\epsilon=\epsilon.$$ This concludes the proof of the main theorem.
    \end{proof}
    % We could further generalize to the case where the input data has more than one channels. In other words, the function $f(g_1,\dots, g_n)$ we attempt to learn has the following property,
    % \begin{equation}
    %     f(gh_1g^{-1},\dots,gh_ng^{-1})=f(h_1,\dots,h_n),\forall g, h_i\in G.\label{eq:multi-invariant}
    % \end{equation}
    % For inputs with $n$ channels, we denote the set of functions that could be represented by GEBLNet as $\mathcal{BLN}_{\hat{\sigma}}(G^n)$.
    % \begin{theorem}
    %     For any activation function $\hat{\sigma}$ that is bounded and non-decreasing, $\mathcal{BLN}_{\hat{\sigma}}(G^n)$ could approximate any function with the property specified in~\eqref{eq:multi-invariant} in the $L^2$ norm.
    % \end{theorem}
    % Noticing that $\mathcal{BLN}_{\hat{\sigma}}(G^n)$ is a linear space, we only need to show it could approximate $$\delta_{\hat{h}_1,\dots,\hat{h}_n}(h_1,\dots,h_n)=\begin{cases}
    %     1,\ &h_i=g\hat{h}_ig^{-1}\\
    %     0,\ &\text{Otherwhise}
    % \end{cases}$$ for any tuple $(\hat{h}_1,\dots,\hat{h}_n)$. 

    