\documentclass[11pt, a4paper, twocolumn]{article}

% Packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}
% \usepackage{physics}
\usepackage[svgnames]{xcolor}

\usepackage{tabularx}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{thm-restate}

\usepackage{authblk}
\makeatletter
\def\th@plain{%
  \thm@notefont{}% same as heading font
  \itshape % body font
}
\makeatother

\theoremstyle{plain}
\declaretheorem[name=Theorem]{theorem}
\declaretheorem[name=Lemma,sibling=theorem]{lemma}
\declaretheorem[name=Corollary,sibling=theorem]{corollary}
\declaretheorem[name=Proposition,sibling=theorem]{proposition}

\theoremstyle{remark}
\declaretheorem[name=Remark,sibling=theorem]{remark}

\theoremstyle{definition}
\declaretheorem[name=Definition]{definition}

\usepackage{bbold}
\DeclareSymbolFont{bbsymbol}{U}{bbold}{m}{n}
\DeclareMathSymbol{\unitmatrix}{\mathbin}{bbsymbol}{"31}


% use times new roman font
\usepackage{newtxtext}
\usepackage{newtxmath}

\usepackage{siunitx}

% Geometry settings
\geometry{
    a4paper,
    left=20mm,
    right=20mm,
    top=25mm,
    bottom=25mm
  }
\setlength{\columnsep}{10mm}


\hypersetup{
  colorlinks = true,
  allcolors = DarkRed,
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.75em}


\usepackage[backend=bibtex,style=numeric,sorting=none]{biblatex}
\addbibresource{cite.bib}
% Allow for more flexible URL breaks in bibliography
\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}


% Redefine urldate field to be empty
\DeclareFieldFormat{urldate}{}

% Remove the primaryclass field from all entries
\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map{
      \step[fieldset=primaryclass, null]
    }
  }
}

\input{todonotes}
\input{notations}
% \newcommand{\ZZ}{\mathbb{Z}}
% \newcommand{\EE}{\mathbb{E}}
% \newcommand{\RR}{\mathbb{R}}
% \newcommand{\rhoreg}{\rho_{\mathrm{reg}}}
% \newcommand{\fanin}{n_{\mathrm{fan\,in}}}
% \newcommand{\ncin}{n_{c_{\mathrm{in}}}}
% \newcommand{\sk}{S_{\kappa}}
% \newcommand{\kk}{K_{\kappa}}
% \newcommand{\nk}{N_{\kappa}}
% \newcommand{\nfc}[1][]{\mathcal{N}^{\mathrm{FC}#1}}
% \newcommand{\ngcnn}[1][]{\mathcal{N}^{\mathrm{GC}#1}}
% \newcommand{\nkn}[1][]{\mathcal{N}^{K\ltimes N#1}}
% \newcommand{\nn}[1][]{\mathcal{N}^{N#1}}
% \newcommand{\naug}[1][]{\mathcal{N}^{\mathrm{aug}#1}}
% \newcommand{\nnaug}[1][]{\mathcal{N}^{#1}}
% \newcommand{\tfc}[1][]{\Theta^{\mathrm{FC}#1}}
% \newcommand{\tgcnn}[1][]{\Theta^{\mathrm{GC}#1}}
% \newcommand{\tkn}[1][]{\Theta^{K\ltimes N#1}}
% \newcommand{\tn}[1][]{\Theta^{N#1}}
% \newcommand{\taug}[1][]{\Theta^{\mathrm{aug}#1}}
% \newcommand{\tnaug}[1][]{\Theta^{#1}}
% \newcommand{\kfc}[1][]{K^{\mathrm{FC}#1}}
% \newcommand{\kfcdot}[1][]{\dot{K}^{\mathrm{FC}#1}}
% \newcommand{\kgcnn}[1][]{K^{\mathrm{GC}#1}}
% \newcommand{\kkn}[1][]{K^{K\ltimes N#1}}
% \newcommand{\kn}[1][]{K^{N#1}}
% \newcommand{\kaug}[1][]{K^{\mathrm{aug}#1}}
% \newcommand{\knaug}[1][]{K^{#1}}
% \newcommand{\xfc}[1][]{\Xi^{\mathrm{FC}#1}}
% \newcommand{\xgcnn}[1][]{\Xi^{\mathrm{GC}#1}}
% \newcommand{\xn}[1][]{\Xi^{N#1}}
% \newcommand{\xkn}[1][]{\Xi^{K\ltimes N#1}}
% \newcommand{\mgcnn}[1][]{\mu^{\mathrm{GC}#1}}
% \newcommand{\mfc}[1][]{\mu^{\mathrm{FC}#1}}
% \newcommand{\mkn}[1][]{\mu^{K\ltimes N#1}}
% \newcommand{\mn}[1][]{\mu^{N#1}}
% \newcommand{\maug}[1][]{\mu^{\mathrm{aug}#1}}
% \newcommand{\mnaug}[1][]{\mu^{#1}}
% \newcommand{\scirc}{\hspace{-1pt}\circ\hspace{-1pt}}
% \newcommand{\ub}[1]{\underbracket[0.5pt][2pt]{#1}}
% \newcommand{\wig}{\ensuremath{\mathcal{D}}}
% \DeclareMathOperator{\SO}{SO}
% \DeclareMathOperator{\supp}{supp}
% \DeclareMathOperator{\vol}{vol}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\renewcommand*{\Affilfont}{\small\normalfont}
% \renewcommand*{\Authfont}{\bfseries}    % make author names boldface    
\setlength{\affilsep}{2em}


% Redefine symbols for \thanks
\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or a\or b\or
   c\or d\or e\or f\or g
   \or h \else\@ctrerr\fi}}
\renewenvironment{abstract}{
  \small
  \begin{center}
    \bfseries \abstractname\vspace{-1.5em}\vspace{\z@}
  \end{center}
  \begin{center}
    \begin{minipage}{0.43\textwidth} % Adjusts the width of the abstract
}{
    \end{minipage}
  \end{center}
  \endquotation
}
\makeatother

% Title and author
\title{ Learning Chern Numbers of Topological Insulators\\with Gauge Equivariant Neural Networks}
% \author{
%     Philipp Misof\,\thanks{Department of Mathematical Sciences, Chalmers University of Technology and the University of Gothenburg, SE-412 96 Gothenburg, Sweden.} \and
%     Pan Kessel\,\thanks{Prescient Design, Genentech Roche, Basel, Switzerland.\\ Emails: misof@chalmers.se, pan.kessel@roche.com, gerken@chalmers.se} \and
%     Jan E.\ Gerken\,\footnotemark[1]
%     }

% Format:
% \author[#affiliation index]{#name \thanks{#email}}
% One author could have multiple affiliations, just write \author[1,3,5,...]{...}.

\author[1]{Longde~Huang~\thanks{longde@chalmers.se}}
\author[2]{Oleksandr~Balabanov}
\author[1,4]{Hampus~Linander}
\author[3]{Mats~Granath}
\author[1]{Daniel~Persson}
\author[1]{Jan~E.~Gerken~\thanks{gerken@chalmers.se}}

\affil[1]{Department of Mathematical Sciences, Chalmers University of Technology and University of Gothenburg,\newline SE-412 96 Gothenburg, Sweden.}
\affil[2]{Department of Physics, Stockholm University, AlbaNova University Center, SE-106 91 Stockholm, Sweden}
\affil[3]{Department of Physics, University of Gothenburg, SE-412 96 Gothenburg, Sweden}
\affil[4]{VERSES AI Research Lab, Los Angeles, USA}

\date{}

\begin{document}

\maketitle
\vspace{-2em}

\begin{abstract}
  Equivariant network architectures are a well-established tool for predicting invariant or equivariant quantities. However, almost all learning problems considered in this context feature a global symmetry, i.e. each point of the underlying space is transformed with the same group element, as opposed to a local ``gauge'' symmetry, where each point is transformed with a different group element, exponentially enlarging the size of the symmetry group. Gauge equivariant networks have so far mainly been applied to problems in quantum chromodynamics. Here, we introduce a novel application domain for gauge-equivariant networks in the theory of topological condensed matter physics. We use gauge equivariant networks to predict topological invariants (Chern numbers) of multiband topological insulators. The gauge symmetry of the network guarantees that the predicted quantity is a topological invariant. We introduce a novel gauge equivariant normalization layer to stabilize the training and prove a universal approximation theorem for our setup. We train on samples with trivial Chern number only but show that our models generalize to samples with non-trivial Chern number. We provide various ablations of our setup. Our code is available at \url{https://github.com/sitronsea/GENet/tree/main}.
\end{abstract}
\vspace{-2em}
\section{Introduction}
\vspace{-1em}
    % \begin{itemize}
    %     \item Introduce Topological Matter and Insulator (concisely: Physical Meaning of Invariance)
    %     \item Problem setting: Multidimensional multiband insulator
    %     \item Geometric Deep Learning
    %     \item Global vs. Local Invariance
    %     \item Model and features: \\
    %     equivariant
    %     bilinear\\
    %     Conv Layer
    %     \item in QCD as well, connection with physics; failed to learn with traditional network
    %     \item summary of results; main contributions: learning the invariant, training on zero labels (generalization), theory
    % \end{itemize}

Geometric deep learning is a subfield of machine learning that takes advantage of the geometric and topological structures inherent in complex data to construct more efficient neural network architectures~\cite{gerken2023}.   
%Geometric deep learning is a subfield of machine learning that integrates deep neural networks with the geometric and topological structures inherent in complex data.
%Inspired by theoretical physics\jgnote{Add 5G ref}, such approaches leverage symmetries to guide the design of network architectures.
This approach has been successfully applied in a variety of domains, from medical imaging~\cite{bekkers2018} to high-energy physics~\cite{pmlr-v119-bogatskiy20a} and quantum chemistry~\cite{duval2023a}. As we show in this paper, this perspective is particularly valuable for studying topological insulators, a class of materials which has been one of the main areas of interest in condensed matter physics over the last two decades~\cite{moore2010birth,RevModPhys.82.3045},  with a broad range of applications, including spintronics and magnetoelectronics \cite{he2022topological}, photonics \cite{Lu2014photonics}, quantum devices \cite{Jin2023devices}, and quantum computing \cite{zanardi1999holonomic, Nayak2008tqc, Beenakker2013tqc}. % whose behavior cannot be captured by traditional, purely local, quantities but instead depends on the global state of the system.

The mathematical field of topology studies objects which cannot be deformed continuously into each other. For instance, a doughnut is topologically equivalent to a coffee cup (the hole in the doughnut becoming the hole in the handle) but not to a ball (which does not have a hole). Topological insulators are materials whose interior is insulating yet whose surface or boundary supports robust conduction. This behavior arises from the different topologies of the electronic band structures inside and outside of the material. The topologies are characterized by quantities which do not change under continuous deformations respecting the underlying system's physical symmetries; these are known as topological invariants. 

%In this article,\obnote{I attempted to write something up. Please feel free to make any changes.}\jgnote{Short summary of our discussion for your restructuring, Daniel: (1) We should state clearly that the Chern number is a toy problem from the physics perspective (no non-trivial physical symmetries) but it's still very challenging to learn. (2) The extension to non-trivial physical symmetries are the motivation to study this toy problem, but the discussion of this should be localized to one paragraph so that ML people can easily skip it} we present a novel way of learning these topological invariants. Deep learning algorithms can serve as a powerful, systematic tool for exploring topological insulators with various global (time-reversal and particle-hole) and local (crystalline) symmetries, providing insights into the construction of their topological indices. Previous theoretical studies have examined various symmetry classes of topological insulators, employing advanced mathematical tools such as K-theory to target strong topological indices in the infinite-band limit. Deep learning methods can automate and verify these approaches numerically, potentially extending them to more complex systems like dissipative (non-Hermitian) insulators, periodically driven (Floquet) insulators, and beyond. They can also relax certain assumptions—such as restricting attention to infinite-band or “strong” topologies—and illuminate new features of topological index construction. Crucially, such methods may enable an automated, systematic sweep through a broad range of cases. We here emphasize that learning topological invariants becomes particularly challenging for multi-band systems, even in the absence of symmetries. We therefore focus on this setting, discuss why it is difficult, and propose an architecture that effectively solves this learning problem. Our contribution marks a step forward toward realizing the broader systematic classification of topological phases envisioned above.

In the context of topological insulators, the deformations we are interested in only affect the phase of the electron wavefunction and therefore leave the underlying physical system unchanged, corresponding to a so-called gauge symmetry. Even though these phase changes are locally unphysical, they accumulate over momentum space into the \emph{Chern number}. This is a physically relevant topological invariant and the learning target of the models studied here. We construct a neural network which predicts the Chern number given a specific discretized version of the wave function as detailed below. In particular, our focus will be on so-called multi-band topological insulators in which several wave functions are combined into a vector.
%In a system with $N$ filled bands, the unphysical phase results in a $\mathrm{U}(N)$ symmetry.
Previous works could construct deep learning systems which predict Chern numbers for materials with only 1 filled band%
%, i.e.\ U$(1)$ gauge symmetry,  
~\cite{PhysRevB.98.085402,PhysRevResearch.2.013354}. Perhaps surprisingly, these models fail to learn Chern numbers for higher-band systems, pointing to a fundamental challenge in the multi-band regime. In contrast, our model is able to predict Chern numbers of materials with at least 7 filled bands.

We stress that learning the Chern number may be viewed as a toy model for more interesting topological insulators. However, since even learning the Chern number is challenging, this is a stepping stone towards more sophisticated physical systems exhibiting richer topological—and consequently physical—properties.

We identify the \emph{gauge symmetry} of the system as the central reason for the failure of traditional approaches in the high-band setting and instead propose to use a gauge equivariant network for learning topological invariants such as the Chern number. Usual group equivariant networks $\mathcal{N}:X\rightarrow Y$ satisfy the constraint $\mathcal{N}(\rho_X(g) x)=\rho_Y(g)\mathcal{N}(x)$ $\forall g \in G$ with symmetry group $G$ and representations $\rho_{X,Y}$ on the input- and output spaces, respectively. In contrast, a gauge equivariant network satisfies a much stronger condition in which the group element $g$ can depend on the input $x$: $\mathcal{N}(\rho_X(g_x) x)=\rho_Y(g_x)\mathcal{N}(x)$. In our case, the input space $X$ is the Fourier transform of the position, the so-called \emph{Brillouin zone} of the material, and $G=\mathrm{U}(N)$ for a system with $N$ filled bands.
%The gauge transformation then corresponds to a change of phase of the wave-function-vector for each position in the Brillouin zone independently.

In a discretized input domain $X$ in $d$ dimensions with $p$ points per dimension, a gauge symmetry effectively means that the total symmetry group of the problem consists of $p^d$ copies of $G$. This enormous enlargement of the symmetry group explains why non-equivariant networks are often unable to learn tasks which feature a gauge symmetry. We therefore claim that gauge-equivariant networks are necessary to learn high-band Chern numbers of topological insulators. This is a novel application for gauge equivariant neural networks, which thus far have mainly found applications within the realm of lattice gauge theories for quantum chromodynamics. In particular, we cast the problem at hand in a form in which we can use an adapted version of the Lattice Gauge Equivariant Convolutional Neural Networks (LGE-CNNs)~\cite{favoni2022} to learn multiband Chern numbers.

%Our model features the gauge-equivariant bilinear- and nonlinear layers from LGE-CNNs, but not their convolutional layers as our network performs only local transformations. Since the Chern number is given by an integral over the Brillouin zone, these are then summed up over lattice points to obtain the Chern number. This setup allows for a training on trivial Chern numbers which nevertheless generalizes to non-trivial Chern numbers. We stabilize the training by introducing a novel gauge-equivariant normalization layer.\jgnote{Add more contributions.}

%\jgnote{Mention that the gauge group is very large}An important example of topological materials are topological insulators. These are phases of matter whose interior is insulating yet whose surface or boundary supports robust conduction. In contrast to ordinary insulators, topological insulators cannot be continuously deformed into trivial states without crossing a conducting phase. Their defining feature is a global, nontrivial topology of the quantum-mechanical wave functions; to wit, a “twisting” of energy bands that cannot be “untwisted” without closing the band gap. Moreover, these conductive surface states are symmetry-protected, meaning they remain stable under local perturbations as long as the relevant symmetries (e.g., time-reversal symmetry) are preserved. Mathematically, the space of all topological insulators can be identified with a certain space of vector bundles, the topology of which characterizes the material. In brief, the base of the bundle is the Brillouin zone and the fiber over each point (i.e. a choice of momentum $\mathbf{k}$) is the associated vector space of occupied states. In particular, the number of connected components of the space of vector bundles correspond to the various non-trivial ``islands’’ of insulators. If an insulator lies in the component containing the vacuum state it is considered trivial; all the others have non-trivial topology. 

%It is important to distinguish between \emph{global} versus \emph{local} symmetries. A global symmetry corresponds to a transformation that acts uniformly on the entire system without changing its physical properties, i.e. leaves the Hamiltonian invariant. A prime example is the time-reversal symmetry, which is an anti-unitary operator that reverses momentum and spin. The Altland-Zirnbauer 10-fold way classification of topological insulators is based on global symmetries. On the other hand, one can have local symmetries that redefine the basis of wave functions at each point in momentum space. According to Bloch's theorem the electron wave function of a crystal is of the form $\psi_{n\mathbf{k}}(\mathbf{r})=e^{i\mathbf{k}\cdot \mathbf{r}} u_{n\mathbf{k}}(\mathbf{r})$, where $n$ is the band index, $\mathbf{k}$ the momentum and $\mathbf{r}$ a lattice vector. However, the choice of overall phase $e^{i\mathbf{k}\cdot \mathbf{r}} $ is not unique; we can redefine the wave function by a $\mathrm{U}(N)$ gauge transformation that depends on $\mathbf{k}$ (but is independent of the lattice point $\mathbf{r}$). 

%Probing and classifying topological phases typically requires computing topological invariants, such as the Chern number, which characterizes a particular connected component in the space of insulators. Physically, the Chern number correspond to an aggregate of the underlying Berry curvature or Berry flux. The Berry curvature tracks how the gauge choice vary  across the Brillouin zone. For systems like multi-band Chern insulators, the Chern number is thus obtained by summing local Berry fluxes. Although deep neural networks are in principle universal function approximators, standard architectures such as residual and convolutional networks often fail to learn the mapping from Bloch Hamiltonians to topological invariants, particularly when there are more than two occupied bands.

%To overcome these obstacles, we employ {\it gauge equivariant neural networks} which inherently take into account the gauge symmetry of the system. In effect, they extend and generalize group equivariant convolutional neural networks (GCNNs), which rely on global symmetries, to more complex settings where the input data may lie on arbitrary manifolds with local symmetry transformations. In this paper, we apply these ideas to multi-band Chern insulators. We demonstrate how gauge equivariant networks can more effectively learn the relevant topological invariants by leveraging the geometric constraints of the problem. This is a novel application for gauge equivariant neural networks, which thus far have mainly found applications within the realm of lattice gauge theories for QCD. 

%Our model consists of gauge equivariant bilinear convolutional layers…. MORE DETAILS HERE!

%\dpinline{References to appear}


%\subsection{Summary of results}

Our main contributions are as follows:

\begin{itemize}
    \item We provide a novel application of $\mathrm{U}(N)$-gauge equivariant neural networks to the task of learning higher-band Chern numbers. The resulting model can predict Chern numbers in two dimensions for systems with at least $N=7$ filled bands. In contrast, previous models could only handle the trivial case of a $\mathrm{U}(1)$-symmetry.
    \item Our model is also the first to be able to learn higher-dimensional ($D=4$) Chern numbers of multi-band topological insulators.
    \item We prove a universal approximation theorem for our architecture in this context which shows that our model can approximate all $\mathrm{U}(N)$ gauge invariants arbitrarily well. We perform ablations over different gauge equivariant architectures motivated by this theoretical investigation.
    \item To stabilize the training, we introduce a new gauge equivariant normalization layer. By training our purely local network with a novel combinations of loss functions, we obtain a model which generalizes from trivial to non-trivial Chern numbers and to unseen lattice sizes. 
\end{itemize}


    
\section{Literature Review}
Gauge equivariant networks have been considered in two different settings. In the first setting, the gauge symmetry concerns local coordinate changes in the domain of the feature maps~\cite{bronstein2021,weiler2023EquivariantAndCoordinateIndependentCNNs,gerken2023}. This case was first studied theoretically in~\cite{cheng2019} and models respecting this symmetry were introduced in~\cite{cohen2019,dehaan2020}. Applications of gauge equivariant networks to lattice quantum chromodynamics (QCD) fall into the second setting, where the gauge transformations act on the co-domain of the feature maps. An important problem in lattice QCD is sampling configurations from the lattice action, a problem for which gauge equivariant normalizing flows~\cite{kanwar2020,boyda2021,nicoli2021,bacchio2023,abbott2023} as well as gauge equivariant neural-network quantum states~\cite{luo2021a} have been used. In contrast, our model is based on a gauge equivariant prediction model developed for lattice QCD~\cite{favoni2022}.


Machine learning for quantum physics has seen an explosive development over the last decade with applications in condensed matter physics, materials science, quantum information and quantum computing to name a few~\cite{Carleo_2019,Carrasquilla_2020,Krenn,dawid2023modernapplicationsmachinelearning}.  In this brief overview we focus on machine learning of topological states of matter. Early ground-breaking work in this area include~\cite{Carrasquilla_2017} that used supervised learning on small convolutional neural networks for identifying the ground state of the Ising lattice gauge theory, as well as~\cite{van_Nieuwenburg_2017} that developed an unsupervised method “learning by confusion” to study phase transitions including topological order of the Kitaev chain. Unsupervised approaches include the use of diffusion maps~\cite{PhysRevB.102.134213} and topology preserving data generation~\cite{PhysRevLett.124.226401}. Of particular relevance to our work are the papers~\cite{Zhang_2018,PhysRevB.98.085402} that used convolutional neural networks and supervised learning to predict $\mathrm{U}(1)$ topological invariants.
%The authors showed that the convolutional filters learned to calculate an accurate local measure of the curvature on a discrete lattice which aggregates to the appropriate topological index.
This work was later extended to an unsupervised setting in~\cite{PhysRevResearch.2.013354,balabanov2020unsupervised} by incorporating the scheme of learning by confusion and augmenting data using topology preserving deformations. 
%In all of these works (and others, e.g.\ \cite{Zhang}) the network architecture is designed to support learning of the relevant physical quantity, which also allows for generalization. Clearly, the challenge for the network to learn the appropriate local physical quantity will increase with the complexity of the problem, which will require more sophisticated network architectures. 
\section{Learning Multiband Chern\\ Numbers}
\label{sec:chern_intro}
The band structure of the topological insulators we want to consider here is described by so-called Bloch Hamiltonians $H(k)$ which are maps from the Brillouin zone to the space of $M\times M$ complex Hermitian matrices. The Brillouin zone is the space of momenta $k$ of the electrons which is periodic in each dimension. At each point $k$ in the Brillouin zone, we consider the eigenvectors $v_n(k)\in\mathbb{C}^{M}$, $n=1,\dots,N$, of the Bloch Hamiltonian with negative eigenvalues since these correspond to the bands occupied by electrons in the material.
    
%\jgnote{Add a few sentences explaining what a Bloch Hamiltonian is}Consider a class of noninteracting gapped space-periodic systems, described by Bloch Hamiltonians $H({k})$ for each momentum ${k}$. The Bloch Hamiltonians $H({k})$ define maps from Brillouin Zone to the space of $2N\times 2N$-matrices for an $N$-band system.

\subsection{Chern numbers for topological \\ insulators}\label{sec:chern}
A nontrivial Chern number means that it is impossible to find smoothly varying eigenvectors over the entire Brillouin zone. In two dimensions, it is defined via
\begin{align} 
C = \frac{1}{2 \pi i}\int_{BZ} \, \, \mathrm{Tr} \, [\mathcal{F}(k)]\,d^2 k\,,
\label{eq:C1}
\end{align}
where $\mathcal{F}$ is an $N\times N$-matrix known as the non-abelian Berry curvature defined by
\begin{align} 
\begin{split} 
\mathcal{F} &= \partial_{k_x} \mathcal{A}_y (k) - \partial_{k_y} \mathcal{A}_x (k) + [\mathcal{A}_x (k), \mathcal{A}_y (k)]\,.
\end{split}
\label{eq:F1}
\end{align}
Here, $\mathcal{A}_\mu(k)$, $\mu=x,y$ is the non-abelian Berry connection, another $N\times N$-matrix whose components are $\mathbb{C}^2$-vectors given by
\begin{align} 
\begin{split} 
[\mathcal{A}_\mu(k)]_{n,m} & = v_n(k)^\top \partial_{k_\mu} v_m(k)
\end{split}
\label{eq:A1}
\end{align}
in terms of the eigenvectors of the Bloch Hamiltonian with negative eigenvalues.

It can be shown that the Chern number defined by~\eqref{eq:C1} is an integer and there are generalizations to higher dimensional Brillouin zones, on which we performed experiments, see Section~\ref{sec:expr-results}.

    %Chern number is a topological invariance of multiband insulator (defined as such...). Specifically, with a known analytic expression for the Hamiltonian on the Brillouin zone $T^{2m}$, the corresponding Chern number could be calculated from the given integral:\jgnote{Write explicit simple expression for 2d case and relegate general case to appendix, stress that it is a local quantity integrate over the base space}
    %\[c_m = \frac{1}{(2\pi i)^m}\int  \tr F^m\]
    %Where $F^m$ is the $m$th wedge product of the $\mathfrak{u}(n)$-valued berry curvature. 
   % It is integer-valued and gauge invariant (a class function). 
   % \textit{(Detailed Reasoning in Appendix.)}

In practice, we consider a discretization of the Brillouin zone into a rectangular grid with periodic boundary conditions, of which there are a total of $N_x \times N_y=N_\text{site}$ grid points. On this grid, one can define the following discrete, integer Chern number $\tilde{C}$ which converges to $C$ for vanishing grid spacing~\cite{fukui2005chern}
\begin{equation}
\tilde{C} = \sum_{(i, j)} \mathrm{Im} \, \mathrm{Tr} \, \log \, W_{i, j}\,, 
\label{eq:C2}
\end{equation}
where $W_{i, j}\in\mathrm{U}(N)$ is the Wilson loop at grid point $\Vec{k}=(i,j)$, defined by
\begin{equation}
    W_k=W_{i, j} = U^x_{i, j} U^y_{i - 1, j} U^x_{i - 1, j - 1} U^y_{i, j - 1} \label{eq:fluxdef}
\end{equation}
in terms of the link matrices $U^x_{i, j},U^y_{i, j}\in\mathbb{C}^{N\times N}$. These links capture the overlap between the eigenvectors of the Bloch Hamiltonians of neighboring grid points and have components
\begin{align} 
 [U^x_{i, j}]_{m,n} &= v_m(k_{i,j})^\top v_n(k_{i-1,j})\\
 [U^y_{i, j}]_{m,n} &= v_m(k_{i,j})^\top v_n(k_{i,j-1})\,.
\label{eq:U1}
\end{align}
The links are discrete analogous of the operator $\exp(i\mathcal{A}(k) d{k})$. The Wilson loops correspond to closed $1\times 1$ loops of the link variables. In higher dimensions, there are several Wilson loops $W^\gamma_k$ per grid point $k$ which are aligned with different directions $\gamma$ in the lattice.

The learning task we will study in this article is to predict the discrete Chern number $\tilde{C}\in\mathbb{Z}$ given the Wilson loops $W_{i,j}\in\mathbb{C}^{N\times N}$ on the lattice. One of our models also uses the links as additional input. Although the Chern number is given by the innocuous-looking equation~\eqref{eq:C2}, learning it is not straightforward as detailed in Section~\ref{sec:res_nets}. We will show that the main reason for the difficulty of learning~\eqref{eq:C2} lies in the gauge invariance of $\tilde{C}$.

\subsection{Gauge symmetry}
\label{sec:gauge_sym}
All topological indices, including the Chern number, are invariant under the gauge group $\mathrm{U}(N)$. Gauge transformations are local symmetries, i.e.\ at each lattice point $i,j$ they act with a different group element $\Omega_{i,j}\in\mathrm{U}(N)$ by
  \begin{align} U^{x,y}_{i,j}&\rightarrow (\Omega_{i,j})^\dagger U^{x,y}_ {i,j}\Omega_{i+1,j}\label{eq:gauge_link}\\
 W^{\gamma}_{i,j}&\rightarrow \Omega_{{i,j}}^\dagger 
    W^{\gamma}_{i,j}\Omega_{{i,j}}\,,\label{eq:gauge_flux}
    \end{align}
where $\dagger$ denotes Hermitian conjugation. Hence, the total symmetry group is $U(N)^{N_\text{site}}$.

    %Recall that these determine the curvature through the equation $F=\text{Im} (\log( \det W))$.

    The gauge symmetry of the system implies the equivalence relation
    \begin{equation}
        W\sim \Omega^\dagger W\Omega\quad\forall\,\Omega\in\mathrm{U}(N)\label{eq:equivalence}
    \end{equation}
    on the set of Wilson loops at each grid point. According to the spectral theorem, there is exactly one diagonal matrix in each equivalence class (group orbit) with elements in $\mathrm{U}(1)$, up to reordering of the diagonal elements. Therefore, the set of equivalence classes for this relation is given by $\mathrm{U}(1)^N/_{S_N}$, the set of diagonal unitary $N\times N$-matrices up to permutation of the diagonal elements. This fact has been used in the construction of gauge equivariant spectral flows~\cite{boyda2021}. We will exploit it below to prove our universal approximation theorem and simplify the data augmentation process.
    

% On the Bloch eigenstates the gauge action is given by   $|a; k \rangle^\prime = \sum_b \Omega_{a,b}(k)| b; k \rangle$ for any $n$ by~$n$ unitary matrix $\Omega(k)$. It also follows that $(\langle a; k |)^\prime = \sum_b \Omega^*_{a,b}(k)\langle b; k|$ under a gauge transformation associated with $\Omega(k)$. This implies that the link matrices $U^x_{i, j}$ transform as 
% \begin{align} 
% \begin{split} 
%  U^x_{i, j} &=  [\Omega^T(i, j)]^\dagger U^x_{i, j} [\Omega^T(i-1, j)]. \\
% \end{split}
% \label{eq:UEq2}
% \end{align}
% \begin{align} 
% \begin{split} 
%  U^y_{i, j} &=  [\Omega^T(i, j)]^\dagger U^y_{i, j} [\Omega^T(i, j - 1)]. \\
% \end{split}
% \label{eq:Uy}
% \end{align}
% \lhinline{What's the difference between Eq8 and Eq11?}

  
    
\subsection{Learning Chern numbers using ResNets}
\label{sec:res_nets}
Using the fact that $\mathrm{Tr}\log(X) = \log\det(X)$, it follows that the Chern number defined in \eqref{eq:C2} can be written as a sum over $\im\log\det(W)$. As a warmup to predicting Chern numbers, we start by considering the simpler problem of predicting determinants of $N\times N$ real matrices $A$.

We construct a dataset containing $N \times N$ matrices with elements sampled from a uniform distribution on the unit interval $[0, 1]$.
As a baseline, we use a naive multi layer perceptron (MLP) with residual connections $f: \mathbb{R}^{N^2}
\rightarrow \mathbb{R}$, taking the matrix elements as input, and predicting the
determinant value.

The determinant of an $N\times N$ matrix can be expressed as an order $N$ polynomial in the matrix elements. For example, the determinant of a $4\times 4$ matrix $A_{ij}$ is given by the forth order expression
\begin{equation}
	\det(A)=\sum_{i,j,k,l=1}^4\epsilon^{ijkl} A_{1i}A_{2j}A_{3k}A_{4l},
	\label{eq:determinant}
\end{equation}
where $\epsilon$ is the totally antisymmetric Levi-Cevita tensor.

Inspired by the functional form of the determinant in equation \eqref{eq:determinant}, we consider higher order layers with structure 
\begin{equation}
	A^{\text{out}}_{i j} = \sum_{k_1,\dots,k_{2R}}\theta_{i j}{}^{k_1 \dots k_{2 R}}A^{\text{in}}_{k_1 k_2}\dots A^{\text{in}}_{k_{2R-1} k_{2R}},
\end{equation}
where $R$ is the number of factors of $A$ in the layer, and $\theta_{i j}^{k_1 \dots k_{2R}}$ are learnable parameters.
An architecture containing layers with $R=2$ will be referred to as bilinear. As the number of parameters grows quickly with order $R$, we use layers of order $R \leq 4$ and construct higher order terms by composition of multiple layers. For example, two layers of order $R=3$ and $R=2$ will contain terms of order $2, 3$ and $6$ when using residual connections.
%An architecture with two linear layers (i.e. $R=1$) will be denoted by $(1,1)$ corresponding exactly to a standard 2-layer MLP. \hlnote{Probably to detailed for the main manuscript?}
%To model the determinant of a $4 \times 4$ matrix we use architectures $(2,2)$, $(2,2,2)$ and $(4)$.

Predicted determinants $f(A)$ are evaluated against the target determinant value $\det(A)$ using absolute relative error $\delta = \left|f(A) - \det(A)\right| / \left|\det(A)\right|$.
\begin{table}
\centering
\caption{Relative error $\delta$ for linear MLP and bilinear residual architectures predicting the determinant of real $4 \times 4$ matrices with uniform random elements in $[0,1]$. Standard MLP architecture fails to learn the determinant relation.  \label{tbl:det4x4}}
\vspace{10pt}
\begin{tabular}{llr}
\toprule
Architecture & Layers & $\delta$ \\
\midrule
MLP & 2 & 1.02 \\
 & 3 & 1.02 \\
 \hline

Bilinear & 2 & 0.01 \\
 & 3 & 0.01 \\
\bottomrule
\end{tabular}
\end{table}
Table~\ref{tbl:det4x4} shows the failure of a residual MLP architecture to learn the determinant of $4\times4$ real matrices with uniformly distributed elements on $[0,1]$, whereas a bilinear architecture with layers of order 2 can achieve low relative error.

Even though these higher order layers provide an architecture that is expressive enough for determinants in small dimensions, they quickly run into issues for larger matrices. Figure~\ref{fig:relative_error_matrix_size} shows that for matrix sizes corresponding to band size $\geq 4$, learning the determinant becomes prohibitively hard without better model priors.

 \begin{figure}
 \centering
	\includegraphics[width=0.5\textwidth]{Figures_arxiv/detnxn.pdf}
	\vspace{-10pt}\caption{Best relative error of predicted matrix determinants for polynomial architectures as a function of increasing matrix size. The ablations include layers up to order 4. Dashed line indicates relative error of a mean predictor. Architectures considered include layers of order $\leq 4$, and depth $\leq 4$, containing terms of up to order $16$ by composition.
    \label{fig:relative_error_matrix_size}}
    %\jginline{Can we say something along the lines of: Considering depth and width of the layers, they should be able to represent determinants of matrices of size $X$ ($X>8)$?}
    \vspace{-10pt}
\end{figure}


%\subsection{Using gauge symmetry in neural networks}
%\lhinline{I attempted to put this part entirely to the beginning of Section 4, please share opinions.}
%Traditional methods for equivariant neural networks, including group convolution, fails in our case due to the massiveness of the symmetry group. The basic input data in our network is the set of discretized Wilson loops (or plaquettes) $W$. Recall that these determine the curvature through the equation $F=\text{Im} (\log( \det W))$. 
%Now, the gauge symmetry of the system implies that any two samples $(W, \tilde{W})$ are equivalent if 
% $$W= \Omega^\dagger\Tilde{W}\Omega$$
%for all $\Omega\in U(n)$. Mathematically, the fluxes are thus organized into a set of equivalence classes given by $U(n)/\text{Ad}$. 
%The spectral theorem for $SU(n)$ then ensures that any $u\in U(n)$ there exists a $g\in U(n)$ such that $gug^{\dagger}$ is a diagonal matrix with components given by pure phases, i.e a collection of numbers $(z_1, \dots, z_n)\in U(1)^n$. But we still have the freedom to permute these diagonal elements with an element of $S_n$, the permutation group of $n$ elements. We conclude that the set of conjugacy classes, i.e. physically relevant choices, of the unitary matrix $W$ is given by a set of unordered $n$-tuples of phases. This is the orbit space $U(1)^n/S_n$.

%This means we can generate plaquettes $W$ among diagonal matrices. The gauge invariance further implies that our network is built from $U(n)$ \emph{class functions}, i.e. functions $f:G\rightarrow \mathbb{R}$ (or $\mathbb{C}$) such that: $$f(ghg^{-1})=f(h),\ \forall g,\ h\in U(n)$$





%\dpinline{How much detail on class functions etc do we need here? Maybe put most of this in an appendix?}

%    \jginline{Have a look in Section~\ref{sec:model_structure}}

\section{Network Architecture}
%\lhinline{...And here we write: ResNet fail due to huge symmetry group hence we have these building blocks and these structures.}
%\jginline{The reason that ResNets fail is due to huge symmetry group. Hence, use gauge equivariant networks}
% \lhinline{To-do list Jan 26th-27th:
% \begin{itemize}
%     \item Rewrite network architecture section, specifically write ''use only fluxes by default, links just for convlayer''; split a 'theory' chapter from it, relocate class function discussions completely there. 2hr
%     \item Make new plots of architecture, similar to the Favoni paper style, or add a table of it for comparison. ?
%     \item Explain data generation scheme, add discussions on distribution (Haar measure). 1hr
%     \item adjust sample plot; equal size, left to right. 10min
%     \item write the experiment part. (text), think about how to organize discussions on loss function (std); adjust accuracy figure (color). 3hr?
%     \item Redo the stats per layer figure, remove insignificant data. (perhaps log the stats for future adjustions?) 2hr?
%     \item Add more loss curves, e.g. global loss, total loss, mae for higher dimensions, etc. Think about it. Put the variations of TrNorm to appendix. 2hr?
% \end{itemize}
% }
    As demonstrated in the previous section, even hand-crafted polynomial architectures fail to learn a simplified version of the Chern number. Motivated by the performance of our gauge equivariant networks, we propose that this is due to the size of the gauge symmetry present in this problem. Since the Wilson loops at each site can be transformed independently, the total symmetry group is $U(N)^{N_\text{site}}$, a exponentially larger group than even for more traditional group equivariant networks.

    The input data in our network is the set of discretized Wilson loops $W^\gamma_k\in\mathrm{U}(N)$ and all equivariant layers in our setup operator on tensors of this form. The index $\gamma$ counts the number of different orientations of the Wilson loops per site (in 2D, there is only one) for the input and serves as a general channel index in deeper layers. Hence, our layers operate on complex tensors of the shape $N_\text{ch}\times N_\text{sites} \times N\times N$.

%In $2D$ cases, there is only one Wilson loop per site $W_k$, as defined in~\eqref{eq:fluxdef}. In other words, the input per site is a single matrix $W_k\in U(n)$.
    
%For simplicity, we flatten the grid indices $(i,j)$ for the flux $W^{\gamma}_{i,j}$, denoting it as $W^\gamma_k$. Similarly, we denote the link $U^\mu_{i,j}$ as $U^\mu_k$, for $\mu=x,y$. We also define the expression $k+d\hat{\mu}$ as the flattened index of $(i,j)+de_\mu$, where $e_\mu$ is the unit vector at direction $\mu$ and $d\in\Zb$. Therefore, the flux data should have a form of 
%\[(W^\gamma_k)=(w_{k\gamma ij})\in \Cb^{N_\text{sites}\times N_\text{ch}} \times \Cb^{N\times N}\]
%Where $k$ represents the flattened grid index, $\gamma$ has a physical meaning only for input data, and represents a generic channel index for following outputs. 
    %We therefore design an architecture which respects the 
    
    %Traditional methods for equivariant neural networks, including group convolution, fails in our case due to the massiveness of the symmetry group. Hence, we construct networks that are manifestly gauge equivariant, with the building blocks introduced below.
    
\subsection{Gauge equivariant layers}
    
\label{sec:model_structure}
        
    \begin{figure*}
        \centering
        \includegraphics[width=1\linewidth]{Figures_arxiv/GEBLNet-arch.pdf}
        % \vspace{-0.8cm}
        \caption{Architecture of GEBLNet. In this figure, the rectangles represent the spatial grid, and the number of layers ($N_\text{ch}$) represents the number of channels ($\gamma$). Each circle represents a site on the grid, and quantities on different sites do not interact with each other, until the last summation on grids.}
        % \jginline{Extend caption}
        % \lhinline{Compact labels (closer to layers)}
        % \jginline{I was thinking about the space between the figure and the caption, but I could fix it with negative vspace, so no worries about this, looks good.}
        \label{fig:gebl-arch}
    \end{figure*}
   
    Our model is composed of the following equivariant layers which were introduced in \cite{favoni2022} as well as our new gauge equivariant normalization layer.
%, through any of which the tuple structure is preserved:

    \paragraph{GEBL (Gauge Equivariant Bilinear Layers)}
    Given an input tensor $W^\gamma_{k}$, the layer computes a local quantity per site as
    \begin{equation}
        {W'^\gamma_k}= \sum_{\mu,\nu}\alpha_{\gamma\mu\nu}W^\mu_{k} W^\nu_{k}\,,
 % + \beta_{\gamma \mu} W^\mu_k + \tilde{\beta}_{\gamma \mu} \tilde{W}^\mu_k +\theta_\gamma I.
    \end{equation}
    where $W'$ has $N_\text{out}$ channels and $\alpha_{\gamma\mu\nu}\in\Cb^{N_{\text{in}}\times N_{\text{in}}\times N_{\text{out}}}$ are trainable parameters. Using~\eqref{eq:gauge_flux}, it can easily be checked that this layer is equivariant. In practice, GEBL includes also a linear and a bias term which are obtained by enlarging $W$ with its Hermitian conjugate and the identity matrix.
    In order to merge two branches of the network, two different $W$ can also be used on the right-hand side. 

    

    \paragraph{GEAct (Gauge Equivariant Activation Layers)}
    Given a tensor $W^\gamma_{k}$, the layer maintains channel size $N_\text{in}$ and serves as an equivariant nonlinearity defined by
    \begin{equation}
       {W'^\gamma_k} = \sigma(\tr W^\gamma_k) W^\gamma_k\,,
    \end{equation}
    where $\sigma$ is a usual activation function. In Section~\ref{sec:uat}, we prove a universal approximation theorem for a certain type of $\sigma$. In practice, we use $\sigma(z)=\relu(\re z)$ to avoid gradient vanishing, hence referring to this layer also as GEReLU.

    \paragraph{GEConv (Gauge Equivariant Convolution Layers)} 
    This is the only layer in our setup which introduces interactions between neighboring points. It is also the only layer for which the link variables $U^\gamma_k$ are used. Given a tuple $(U^\mu_{k}, W^\gamma_{k})$, the layer performs a convolution as
    \begin{equation} 
        {W'^\gamma_k}= \sum_{\mu,d}\sum_{\sigma}\omega_{\mu d\gamma \sigma}(U^{d\mu}_{{k} + d\hat{\mu}})^\dagger W^\sigma_{{k} + d\hat{\mu}}U^{d\mu}_{{k}}\,,
    \end{equation}
    where $U^{d\mu}_{{k}} = U^\mu_{k} \dots U^{\mu}_{{k}+(d-1)\hat{\mu}}$ and $d\hat{\mu}$ is the length-$d$ vector in the $\mu$ direction in the lattice. The output $W'$ of this layer has the shape $N_{\text{out}}\times N_{\text{site}}\times N\times N$ and $\omega$ are trainable weights of shape $\text{dim}\times d \times N_\text{out} \times N_\text{in}$. Note that this layer does not update the links. Using the transformations~\eqref{eq:gauge_link} and \eqref{eq:gauge_flux}, one can check that this layer is equivariant as well. When we take a zero convolution kernel size, the layer degenerates into an equivariant linear layer that is completely local.
    
    %outputting a tuple $(U^\mu_{k}, W^\gamma_{k})$, where $W^\gamma_{k}$ has the shape $N_{\text{out}}\times N_{\text{site}}\times N\times N$, and $U^\mu_k$ remains unchanged. Here, for a given integer $d$, $k+d\hat{\mu}$ is defined as the flattened index of $(i,j)+de_\mu$, where $e_\mu$ is the unit vector at direction $\mu$, and $U^{d\mu}_{{k}} = U^\mu_{k} \dots U^{\mu}_{{k}+(d-1)\hat{\mu}}$.% When we take a zero convolution kernel size, the layer degenerates into a equivariant linear layer that is completely local, in another words only the fluxes are taken as inputs.

    %What follows is a trace layer, which extracts the trace of the flux variables $W$, rendering the outputs essentially gauge invariant:
    
    \paragraph{Trace Layer}
    Given a tensor $W^\gamma_{k}$, this layer maintains the channel size and takes the trace of the fluxes as
    \begin{equation} 
        T^\gamma_k =\tr W^\gamma_k\,.
    \end{equation}
    Since the trace is invariant under the transformations~\eqref{eq:gauge_flux}, this layer renders the features gauge invariant. The output has the shape $N_{\text{in}}\times N_{\text{site}}$.
    %We denote the vector slice $(\tr W^\gamma_k)_{\gamma \in \Gamma}$ of tensor $\tr W^\gamma_k$ as $\vec{T}_k$.
    
    \paragraph{Dense Layer} 
    After the trace layer, we perform a real valued linear layer on $T_k$ as
    \begin{equation}
       T'_k=w_{\text{Re}}\cdot \re(T_k) + w_{\text{Im}}\cdot \im(T_k) + b\,,
    \end{equation}
    where $w_{\text{Re}}$, $w_{\text{Im}}$ and $b$ are trainable parameters. The output features have shape $N_\text{out}\times N_\text{site}$. Note that we only transform gauge invariant features with this layer since it does not respect the gauge symmetry.
    %Outputting a vector of length $N_\text{site}$.  %Then we predict the Chern number by taking the summation over the grid points.
    
    % \lhinline{We have a main model structure composed of GEBL, GCReLU, TrNorm, Trace and Dense. \textit{(Detailed Reasoning in Appendix.)} Motivation: using local structure and topological generalization Traing setup}

    \paragraph{TrNorm (Trace Normalization Layers)}
    The bilinear \gebl-layers introduced above quickly lead to training instabilities when stacked deeply, as demonstrated in Section~\ref{sec:experiments} below. To solve this problem, we introduce a novel gauge-equivariant normalization layer which we insert after the nonlinearities. Given an input tensor $W^\gamma_{k}$, this layer maintains the channel size and performs a channel-wise normalization as
    \begin{equation}
        {W'^\gamma_k}= \frac{1}{|\text{mean}_{\gamma}\{\tr W^\gamma_k\}|}W^\gamma_k\,.       
    \end{equation}
    This operation is gauge equivariant since the prefactor is gauge invariant. After the normalization, the output features $W'$ satisfy $\text{mean}_{\gamma}\{\tr W'^\gamma_k\} = e^{i\phi_k}$ for some $\phi_k\in\mathbb{R}$.

    %We introduce this novel layer to address training instability on trivial samples, further discussed in Section~\ref{sec:expr-results}.

    

    \subsection{Network Architecture}
    We now use the layers introduced in the previous section to construct three different equivariant network architectures.
    
    \paragraph{GEBLNet (Gauge Equivariant Bilinear Network)}
    GEBLNet is a model that only operates locally, i.e.\ the inputs are the fluxes $W^\gamma_k$ alone, and features at different sites only interact with each other in the final sum. It processes the fluxes through repeated blocks of \gebl, \geact, and \trnorm\   layers. The outputs are then aggregated through a \trlayer layer, followed by a dense and a summation over sites to produce the prediction on the Chern number. This is our primary model to study. An example structure is shown in Figure~\ref{fig:gebl-arch}.
    
    \paragraph{GEConvNet (Gauge Equivariant Convolutional Network)}
    GEConvNet is a model that features \geconv\ layers, therefore takes both the links $(U^\mu)$ and the fluxes $(W^\gamma)$ as input. Each \gebl-\geact-\trnorm\  block, similar to that of \geblnet, is paralleled by a \geconv-\geact-\trnorm\  block, with their outputs combined through a subsequent \gebl\  layer. This process is repeated through several iterations, after which the resulting output is passed to the \trlayer- \dense- and sum sequence to produce the final prediction. An example structure is shown in Figure~\ref{fig:conv-arch} in Appendix~\ref{appendix:conv}.
    
    \paragraph{TrMLP (Trace Multilayer Perceptron)}
    As a baseline for two-dimensional grids, we also introduce a Trace Multilayer Perceptron. This model takes only $W^\gamma$ as inputs, extracts a characteristic vector $\left(\tr(W^\gamma_k)^1,\dots,\tr(W^\gamma_k)^N \right)$ for some $N$ per site and then applies an MLP on this vector, followed by a summation over sites for prediction. Its motivation and feasibility will be discussed in Section~\ref{sec:uat}.
    
\section{Theoretical Foundations of the Model}\label{sec:uat}
In this section we will present a universal approximation theorem for our models. We focus on \geblnet, whose inputs are solely $W_k$. 
Recalling the equivalence relation~\eqref{eq:equivalence}, the local quantity is in fact a class function on the gauge group $U(n)$, which is defined as follows:
\begin{definition}
    A (complex) class function on a Lie group is a function $f:G\ra \Cb$ such that:
    \begin{equation}
        f(ghg^{-1})=f(h),\qquad \forall g, h \in G
    \end{equation}
\end{definition}
For compact Lie groups any continuous function on $G$ is automatically square integrable due to the finiteness of the Haar measure. Hence, the space of class functions is a subset of $L^2(G)$. We will henceforth denote this subspace by $L^2_{class}(G)$.

We now present our main theoretical result, which shows our model's capability to learn an arbitrary class function.
%\subsection{Universal Approximation Theorem for gauge equivariant networks}\label{sec:uat}
\begin{theorem}[Universal Approximation Theorem]
    For a compact Lie group $G$, and with the nonlinearity $\sigma$ in GEAct taking the form $\tils\circ \re$, where $\sigma$ is bounded and non-decreasing, GEBLNet could approximate any class function on $G$.
    \label{UAT}
\end{theorem}
The full proof is given in Appendix~\ref{appendix:proof}. Here we present a brief sketch of it. To this end we begin by introducing some properties of class functions.
\begin{theorem}
    The space of symmetric polynomials $\{s_k\}$ over eigenvalues $\{\lambda_k\}$ forms an orthonormal basis of $L^2_\text{class}(G)$.  
    \label{PW}
\end{theorem}
\begin{proof} Follows directly from the Peter-Weyl theorem and the expansion of class functions in terms of irreducible characters.
\end{proof}
Furthermore, consider the set of polynomials over eigenvalues of $g$: $\{p_k(\lambda_1,\dots,\lambda_n)=\sum_i \lambda_i^k\}$. In our setting these polynomials can be identified with the set of traces of group elements of the form $\tr g^k$. Using Newton's identities for symmetric polynomials,
    \begin{equation}
        ke^k = \sum_{j=1}^k(-1)^{j-1}e_{k-j}p_j\,,\quad e_k=\hspace{-2pt}\sum_{\sum_{n}k_n=k}\prod\lambda_i^{k_i},
    \end{equation} 
one may deduce the following
\begin{corollary}\label{cor:trgn}
    The space $\bigcup_M\{f(\tr g, \dots, \tr g^M)\}\cap L^2(G)$ is dense in $L^2_\text{class}(G)$.
\end{corollary}
We are now ready to  sketch the proof of Theorem~\ref{UAT}.
\begin{proof}[Sketch of Proof]
    Consider a network with $M$ \gebl\  layers and sufficient width. One can show that the output of such a network can approximate any function of the form
    \begin{equation}w_{i}\sigma(\re \sum_{j=0}^{2^M}\alpha_{ij}\tr g^i)(\sum_{j=0}^{2^M}\alpha_{ij}\tr g^i)+b.
\label{UATfunction}
    \end{equation}
    However, this is equivalent to a one-layer MLP on the polynomials $(\tr g, \dots, \tr g^{2^M})$. Therefore it can approximate any function in $\{f(\tr g, \dots, \tr g^{2^M})\}\cap L^2(G)$. By Corollary~\ref{cor:trgn}, we conclude that eq.(\ref{UATfunction}) can  approximate any functon in $L^2_{class}(G)$. \qedhere
\end{proof}
We refer to Appendix~\ref{appendix:proof} for the complete proof.
% It is straightforward to generalize the theorem to higher dimensions, where the network takes multiple $g\in G$ as inputs, and satisfies
% $$f(gh_1g^{-1},\dots,gh_ng^{-1})=f(h_1,\dots,h_n),\forall g, h_i\in G.$$
% We discuss this further in Appendix~\ref{appendix:proof}.

\section{Experiments}
\label{sec:experiments}
To assess the performance of our gauge-equivariant neural network, we conducted experiments on synthetic datasets with varying grid sizes and distributions. Unless specified, the dataset is based on $2D$ grids. In this case, sample data shall have the form $(U^x_k, U^y_k, W_k)_k$.
\subsection{Data Generation}

\paragraph{General Dataset} The data generation pipeline consists of two main steps. Given a grid size \( N_x \times N_y \), we first generate random link variables using the following algorithm:

\begin{enumerate}
    \item For \( \mu = x, y \), draw \( A^\mu_k \sim \mathcal{N}(0, 1)^{N \times N} \).
    \item Perform a QR decomposition on \( A^\mu_k \), decomposing it into the product of a unitary matrix \( U^\mu_k \) and a semi-definite matrix \( \Sigma^\mu_k \), \( A^\mu_k = U^\mu_k \Sigma^\mu_k \).
    \item Use \( U^\mu_k \) as the link variable for site \( k \) in direction \( \mu \).
\end{enumerate}

It can be shown that the distribution of the links generated in this way is uniform on \( \mathrm{U}(N) \), i.e.\ the random variables \( U \) and \( gU \) are identically distributed for all \( g \in \mathrm{U}(N) \). See Appendix~\ref{appendix:data} for details.

In the next step, we compute the fluxes \( W_k \) using \eqref{eq:fluxdef} and the discrete Chern number \( \tilde{C} \) using \eqref{eq:C2}. Ultimately, this yields a dataset of data-value pairs \( ((U_k^\mu, W_k^\gamma), \tilde{C}) \). We generate the training samples continuously during training to avoid overfitting.

\paragraph{Diagonal Dataset}
% \jgnote{Please check}
For some of our experiments, we require control over the distribution of the Chern numbers in our training data. To this end, we employ a different data generation strategy. Due to the invariance of our model under gauge transformations, training samples which lie in the same gauge orbit are equivalent in the sense that the parameter updates they induce are the same. This implies that we can select an arbitrary element along the gauge orbit for training. As discussed in Section~\ref{sec:gauge_sym}, there is always a diagonal matrix with $\mathrm{U}(1)$-valued components in the orbit. Therefore, by training on these matrices in $\mathrm{U}(1)^N$ and manipulating the distribution of the diagonal values, we can generate datasets with different distributions of Chern numbers. Note that networks trained on these datasets of course generalize to non-diagonal Wilson loops since they are gauge invariant. Detailed discussions are provided in Appendix~\ref{appendix:data}.

\subsection{Training and Evaluation}

We adopt two loss functions for our training: the global loss $L_{\mathrm{g}}$ and the standard deviation loss $L_{\mathrm{std}}$. Specifically, given the network $f(W)$ and the Chern number $\tilde{C}$, $L_{\mathrm{g}}$ calculates $\|f(W)-\tilde{C}\|_1$. Meanwhile, $L_{\mathrm{std}}$ evaluates the entrywise standard deviation of the network output after the dense layer, denoted as $(g(W_k))_k$, namely $\|\max (\{\mathrm{std}(g(W_k)_k),\delta\}) - \delta\|_1$, where $\delta$ is a hyperparameter, set to 0.5 by default.
The standard deviation loss is necessary to prevent the model from collapsing to only zero outputs, since it forces the model to output locally differed quantities. This is particularly relevant for the training on trivial topologies only, as described below.
The total loss function $L_\text{total}$ adds these two terms, $L_\text{total}=L_{\mathrm{g}}+L_\mathrm{std}$.

% \paragraph{Learning Rate} We adopt a default initial learning rate of $10^{-4}$, and an Adam optimizer with scheduling.\mgnote{Adam with scheduling? not for}

For evaluation, we compute the accuracy by rounding the network output $f(W)$ to the nearest integer and comparing it with the Chern number $\tilde{C}$, unless otherwise specified.
\paragraph{Baseline Model Configuration}
For the main architecture \geblnet, we set a baseline model configuration, whose \gebl layers and hyperparameters are listed in Table~\ref{tab:baseline} in the appendix.

\subsection{Experimental Results}\label{sec:expr-results}
%We now show our experiment results.
\begin{table}[!]
        \centering
        \caption{Accuracy of GEBLNet trained and evaluated on a $5^2$ grid. }
        \label{tab:bands-acc}
        \vspace{10pt}
        \begin{tabularx}{0.5\textwidth}{cccccc}
            \toprule
             Bands& 4& 5&6&7&8 \\
             \midrule
             Accuracy ($\%$)&$95.9$& $94.0$& $93.8$& $91.7$& $52.5$\\
             \bottomrule
        \end{tabularx}
\end{table}
\begin{figure}[!]
        \centering
        \includegraphics[width=0.45\textwidth]{Figures_arxiv/LayerStats.pdf}
        \vspace{-10pt}\caption{Comparison of statistics of $\|\tr W'^\gamma_k\|$ across each layer, between two training runs on a $5\times 5$ grid, with $4$ filled bands. The two runs have identical configurations, except for the implementation of \trnorm\  Layers.}
        \label{fig:stats-trnorm}
        \vspace{-10pt}
\end{figure}
\begin{figure*}
    \centering 
    \includegraphics[width=0.48\textwidth]{Figures_arxiv/loss_curve_trivial_naive.pdf}    \includegraphics[width=0.48\textwidth]{Figures_arxiv/loss_curve_trivial_norm.pdf}
    \vspace{-10pt}
    \caption{Comparison of global and standard deviation loss on validation data between the same two runs learning on only zero Chern numbers, as shown in Figure~\ref{fig:stats-trnorm}. The former, without \trnorm layers, collapses to zero local quantities everywhere, hence having a lower $L_{\mathrm{g}}$ on trivial samples than the latter with \trnorm\ layers. Nevertheless, the former could not generalize to nontrivial cases.
    In contrast, the latter, with \trnorm layers, succeeds in learning global quantities and local differences simultaneously. }
    \label{fig:loss-trivial}
    \vspace{-10pt}
    % \jginline{Emphasize that this is for learning trivial chern numbers only and that the model above has collapsed to zero outputs, explaining its lower loss, however won't generalize to non-zero Chern numbers.}
\end{figure*}
\paragraph{Model Comparison}
For a basic model comparison, we train \geblnet, \geconvnet, and \trmlp\  on 2D grids to learn Chern numbers by training on non-diagonal data. We find that \geblnet  outperforms the other models in both accuracy and robustness, see Figure~\ref{fig:complexity-acc} in  Appendix~\ref{appendix:conv}.

Using a benchmark grid size of $5 \times 5$ and $N=4$ filled bands, \geblnet\  achieved approximately $95\%$ accuracy across different seeds, demonstrating strong robustness. In contrast, \geconvnet\  struggled to learn correct results with positive kernel sizes, likely due to redundant information. We also tested a degenerate \geconvnet with kernel size $0$ (a local network) which performed better than its non-local counterpart, but it remained less robust than \geblnet. \trmlp\ was highly sensitive to initialization, frequently encountering gradient explosions, and most successful runs exhibited low accuracy.

A complexity-accuracy comparison in Figure~\ref{fig:complexity-acc} in the appendix demonstrates the balance achieved by the baseline model, which performed consistently well while maintaining efficiency. Additionally, tests on increasing band sizes (Table~\ref{tab:bands-acc}) show that the baseline model effectively learns Chern numbers up to 7 bands, retaining high accuracy and robustness.

\begin{figure}
        \centering 
        \includegraphics[width=0.3\textwidth]{Figures_arxiv/local_quantity_comp.pdf}
        \vspace{-10pt}\caption{Comparison of rescaled local outputs with local true values. Points closer to the reference line $y = x$ indicate higher accuracy in capturing local quantities.}
        \label{fig:local-quantity}
\end{figure}
\begin{figure}
        \centering
        \includegraphics[width=0.45\textwidth]{Figures_arxiv/grid_acc_comparison.pdf}
        \vspace{-10pt}
        \caption{Comparison of model accuracy across different grid sizes. Each run, represented by markers of the same color, has identical configurations, but is trained on grids of a different size. Each line represents a linear regression on the corresponding run.}
        \label{fig:grid-acc}\vspace{-10pt}
\end{figure}

\paragraph{Training on Trivial Topologies}
In order to test the generalization properties of our model, we train exclusively on topologically trivial samples. To achieve this, non-trivial samples were manually filtered out during data generation, turning $L_{\mathrm{g}}$ effectively into $\|f(W)\|_1$. 

A naive \geblnet\  model without \trnorm layers can learn the Chern number for up to 3 bands but fails for 4 bands and above, mostly outputting zero local quantities except for a few random seeds. Analyzing the statistics of output traces per layer (Figure~\ref{fig:stats-trnorm}) reveals that variance accumulates across layers, reaching $10^5$ after the final \geact\  layer, causing numerical instability and vanishing gradients.

Introducing \trnorm\  layers mitigates this issue, stabilizing the variance and enabling the model to learn Chern number of systems with more than 4 bands. Figure~\ref{fig:loss-trivial} compares the loss curves between models with and without \trnorm\  layers, demonstrating the effectiveness of this modification.
    % \begin{figure}
    %     \centering
    %     \includegraphics[width=0.4\textwidth]{Figures_arxiv/LayerStats without norm.png}
    %     \vspace{-10pt}\caption{}
    %     \label{fig:stats-naive}
    % \end{figure}

Furthermore, training solely on trivial samples limits the model to learning the Chern number up to a global, sample independent, rescaling factor, i.e.\ $f(W) \approx k\tilde{C}$ for some $k$. To evaluate on general samples, we compute the rescaling factor $R_\text{scale} = \text{mean}(\tilde{C})/\text{mean}(f(W))$ using a large set of non-trivial samples and scale the output as $R_\text{scale}f(W)$. The training results demonstrated prediction accuracy comparable to that of training on general datasets, as detailed in Table~\ref{tab:diag-acc} in Appendix~\ref{appendix:data}, we obtain an accuracy of approximately $94.1\%$ on four bands, comparable to $95.9\%$ for training on data which includes non-trivial Chern numbers. Meanwhile, Figure~\ref{fig:local-quantity} shows the model's ability to capture local quantities accurately.
    
\paragraph{Larger grids}
Due to its local structure, \geblnet\  can process samples of arbitrary grid sizes. We evaluate its generalization abilities by testing samples on larger grids using a baseline model trained on smaller grids. The results, shown in Figure~\ref{fig:grid-acc}, indicate that our models show excellent generalization ability to larger grid sizes. The moderate accuracy decrease we observe is approximately linearly with the number of grid sites, likely due to accumulating errors. Training on larger grids slightly improves performance but comes at a considerable computational cost.

% In addition, models trained on smaller grids achieve similar accuracy to those trained on larger grids, demonstrating the generalization capability of our model.

\paragraph{Learning higher-dimensional Chern numbers}We extended the task to learning Chern numbers to $4D$ grids, whose definitions are significantly more complex than the $2D$ case (see Appendix~\ref{appendix:chern_highdim}). Furthermore, instead of a single flux $W_k$, there are $C^2_4=6$ Wilson loops per site in $4D$. Since for higher dimensions, the discrete approximation to the Chern number is not necessarily an integer, we cannot use the accuracy for evaluation. Therefore, we use the MAE (global loss $L_{\mathrm{g}}$) instead, see Figure~\ref{fig:loss-highdim} in Appendix~\ref{appendix:conv}. With an MAE of around $0.25$, our models can predict these higher-dimensional Chern numbers well within rounding errors.
\begin{figure}
        \centering
        \includegraphics[width=0.3\textwidth]{Figures_arxiv/local_quantity_comp_highdim.pdf}
        \vspace{-10pt}
        \caption{Comparison of rescaled local outputs with local true values.}
        \label{fig:local-quantity-highdim}
    \end{figure}
Figure~\ref{fig:local-quantity-highdim} demonstrates the predictions of local quantities, showing good agreement with the targets.


%\section{Conclusion}
%In this work, we introduced a $U(N)$-gauge equivariant neural networks to predict Chern numbers in multiband topological insulators, addressing the limitations of traditional methods in high-band scenarios. With inherent gauge equivariance, our model accurately learns the topological invariant, outperforming previous approaches and extending to at least 7 filled bands. A key strength of our model is its ability to generalize -- both from topological trivial to nontrivial samples, but also to larger grid sizes. We further established a universal approximation theorem, proving that our architecture can approximate any $U(N)$ gauge-invariant function. 

%Despite the progress, challenges remain, including extensions to more complex symmetry groups, higher dimensions, and [...]. Future work will focus on applying this framework to more complex multiband systems and validating it with extensive experiments.
\newpage
\newpage

\section*{Acknowledgments}
We want to thank David Müller and Daniel Schuh for inspiring discussions.
The work of J.G.\ and D.P.\ is supported by the Wallenberg AI, Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg (KAW) Foundation. M.G.\ acknowledges support from KAW through the Wallenberg Centre for Quantum Technology (WACQT). The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council through grant agreement no. 2022-06725. 

\renewcommand*{\bibfont}{\normalfont\footnotesize}
\printbibliography

\newpage
\onecolumn
\appendix
\section{Higher Order Chern Numbers}
    In~\ref{sec:chern}, we defined in~\eqref{eq:C2} for two dimensional Brillouin zones the non-abelian Berry curvature as
    \begin{equation*}
        \mathcal{F} = \partial_{k_x} \mathcal{A}_y (k) - \partial_{k_y} \mathcal{A}_x (k) + [\mathcal{A}_x (k), \mathcal{A}_y (k)]\,.
    \end{equation*}
    For $2n$ dimensional Brillouin zones, which are topologically equivalent to $\Rb^{2n}/_{\Zb^{2n}}$, there are $P^2_{2n}$ different oriented planes, i.e. for every two directions $k_\mu,k_\nu$, there is a planar flux
    \begin{equation}
        W_{k}^{\mu,\nu}=U_{k}^\mu U_{k+\hat{\mu}}^\nu (U_{k+\hat{\nu}}^\mu)^\dagger (U_{k}^\nu)^\dagger.
    \end{equation}
    It is easy to verify that $W_{k}^{\mu,\nu}=(W_{k}^{\nu,\mu})^\dagger$.
     Similarly, there is a planar curvature
    \begin{equation}
        \mathcal{F}_{\mu,\nu} = \partial_{k_\mu} \mathcal{A}_\nu (k) - \partial_{k_\nu} \mathcal{A}_\mu (k) + [\mathcal{A}_\mu (k), \mathcal{A}_\nu (k)]\,.
    \end{equation}
    Where $\mathcal{A}_\mu$ is similarly defined as in~\eqref{eq:A1}.
    We showed in \ref{sec:chern} the definition of Chern numbers on a $2D$ Brillouin zone in~\eqref{eq:C1}. For a $2n$ dimensional Brillouin zone, a $n_{th}$ order Chern number is defined as
    \begin{equation}
        C_n = \frac{1}{ n!\left(2 \pi i\right)^n}\int_{BZ} \, \, \mathrm{Tr} \, [\mathcal{F}(k)^n]\,d^{2n} k\,.
    \end{equation}
    Here, $\mathcal{F}(k)^n$ represents a wedge product of differential forms $\mathcal{F}_{\mu,\nu}(k)\,d k_\mu\, dk_\nu$, which could be written equivalently as 
    \begin{equation}
        \frac{1}{2^n}\sum_{\mu_1,\mu_2,\dots,\mu_{2n-1},\mu_{2n}}\epsilon_{\mu_1,\mu_2,\dots,\mu_{2n-1},\mu_{2n}}\prod_{t=1}^n\mathcal{F}_{\mu_{2t-1},\mu_{2t}}(k).
    \end{equation}
    It could be shown that $C$ is always an integer, $\forall n\geq 1$.
    
    In practice, since the fluxes $W_{k}^{\mu,\nu}$ is an approximation of $\exp(\mathcal{F}_{\mu,\nu})$, we calculate the discrete version of higher order Chern numbers with the following equation
    \begin{equation}
        \tilde{C}_n=\frac{1}{n!(2\pi i)^n2^n}\sum_k\sum_{\mu_1,\mu_2,\dots,\mu_{2n-1},\mu_{2n}}\tr\epsilon_{\mu_1,\mu_2,\dots,\mu_{2n-1},\mu_{2n}} \prod_{t=1}^n \log W^{\mu_{2t-1},\mu_{2t}}_k.\label{eq:discrete_chern}
    \end{equation}
    When taking $n=1$, Equation~\eqref{eq:discrete_chern} coincides with~\eqref{eq:C2}. Since $\log$ function is analytical, which means could be represented by a power series, and $(\Omega^\dagger W \Omega)^n=\Omega^\dagger W^n \Omega$, we have
    \[ \tilde{C}(W^{\mu_{2t-1},\mu_{2t}}_k)=\tilde{C}(\Omega^\dagger W^{\mu_{2t-1},\mu_{2t}}_k\Omega),\ \forall \Omega\in U(N)\]
    This discretized Chern number is an integer only in the continuum limit, therefore we use the MAE
(global loss $L_{\mathrm{g}}$) instead for evaluation.
\label{appendix:chern_highdim}
\newpage
\section{Data Generation}\label{appendix:data}
    \subsection{Uniform Distribution on $U(N)$ with QR Decomposition}
        In the experiments we generate $U(N)$ with QR Decomposition on a matrix $A\in \Cb^{N\times N}$, whose entries have i.i.d. $\mathcal{N}(0,1)$ real and imaginary parts. We assume the algorithm to generate $U$ from $A$ is single-valued, i.e. $U=f(A)$ for some function $f:\Cb^{N\times N}\ra U(N)$. We show the ''left invariance'' of the random variable $U$. 
        \begin{proposition}
            $U$ and $gU$ are identically distributed, $\forall g\in G$.
        \end{proposition}
        \begin{proof}
        By definition, $gU = f(gA)$. Then it suffices to show $gA$ and $A$ are identically distributed.

        For complex matrices, we consider the two bijections. The first one is  $p:A\ra \begin{pmatrix}
            \re A\\\im A
        \end{pmatrix}$. Then $p(gA)=\begin{pmatrix}
            \re g &-\im g\\
            \im g & \re g
        \end{pmatrix}=\hat{g} p(A) $. It is easy to verify $\hat{g}$ is orthogonal. We then flatten the matrix with a vec operator
        \[\text{vec}(A)=(A_{11}, A_{12},\dots, A_{1N},\dots,A_{M1},\dots,A_{MN})\]
        The following property is well known.
        \begin{proposition}[Vec Operator Identity]
            $\text{vec}(gA)=(g\otimes I_N) \text{vec}(A)$
        \end{proposition}
        Where $\otimes$ is the Kronecker product.
        Then $\text{vec}(p(gA)) = (\hat{g}\otimes I_{N})\text{vec}(p(A))$. However, $\text{vec}(p(A))$ is just $(\re A_{ij}, \im A_{ij})$, which follows the distribution $\mathcal{N}(0,I_{2N^2})$, and $\hat{g}\otimes I_{N}$ is still orthogonal, it follows that $$\text{vec}(p(gA))\sim \mathcal{N}(0,(\hat{g}\otimes I_{N})I_{2N^2}(\hat{g}\otimes I_{N})^T)=\mathcal{N}(0,I_{2N^2})$$ Therefore $\text{vec}(p(gA))\sim \text{vec}(p(A))$. By bijectivity, $gA\sim A$. \qedhere
        \end{proof}
    \subsection{Diagonal Dataset}
        \input{diagonaldata}
\newpage
\section{Proof of the Universal Approximation Theorem}
    \label{appendix:proof}\input{uat} 

\newpage
\section{Further Details regarding the Network Structure}\label{appendix:conv}
    \paragraph{Architecture of \geconvnet}
    Figure~\ref{fig:complexity-acc} is a demonstration of the architecture of \geconvnet. It has \geconv layers implemented inside, therefore takes the links $U_k^\mu$ as inputs, beside the fluxes $W_k$, if the kernel size is set as positive. 
    \begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{Figures_arxiv/GEConvNet-arch.pdf}
        \vspace{-10pt}\caption{Architecture of GEConvNet. In this figure, the rectangles represent the spatial grid, the arrows on the grid represent links, and the number of layers ($N_{ch}$) represents the number of channels ($\gamma$). Each circle represents a site on the grid, and quantities on different sites do not interact with each other, except for the \geconv\ Layers, and the last summation on grids.}\label{fig:conv-arch}
    \end{figure}
    \paragraph{Configuration of the baseline model}
    Table~\ref{tab:baseline} lists the hyperparameters for \gebl layers in the baseline model. Since the \gebl layers are consecutive with channel size-maintaining layers, \geact\ and \trnorm\ layers in between, the former layer's output channel equals to the latter's input channel.
    \begin{table}[]
    \centering
    \caption{Configuration of \gebl layers for the baseline model. See its architecture in Figure~\ref{fig:gebl-arch}. In addition, every \gebl layer is followed by a \geact layer and a \trnorm layer that maintains channel size.}
    \vspace{10pt}
    \label{tab:baseline}
    \begin{tabular}{ccc}
         \toprule
         Layer & Input Channel & Output Channel\\
         \midrule
        \gebl\  1& 1&32\\
        \gebl\  2 & 32 & 16\\
        \gebl\  3 & 16 & 8\\
        \bottomrule
    \end{tabular}
\end{table}
\paragraph{Complexity-Accuracy Comparison}
Figure~\ref{fig:complexity-acc} is a comparison of model complexity and accuracy across different models. All the models are all trained on a $5\times 5$ grid with $4$ filled bands. Due to the instability of \trmlp, there are only $3$ success runs with this model.
\begin{figure}
        \centering
        % \includegraphics[width=0.5\textwidth]{Figures_arxiv/runtime_acc_comparison.pdf}
        \includegraphics[width=0.7\textwidth]{Figures_arxiv/complexity_acc_comparison.pdf}
        \vspace{-10pt}
        \caption{Comparison of model complexity and accuracy across different architectures. Complexity is measured by the number of learnable parameters, and $s_k$ denotes the kernel size for \geconv\   layers. Each marker represents a training run with variations in models, learnable parameters, and random seeds.}
        \label{fig:complexity-acc}
    \end{figure}
\paragraph{Global Loss Curve for Training on Higher Order Chern Numbers}
Figure~\ref{fig:loss-highdim} shows the global loss curve for training on second order Chern numbers. Since we adopt the $L_1$-norm $\|\cdot\|_1$ here, $L_{\mathrm{g}}$ essentially measure the mean absolute error of global outputs.
\begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{Figures_arxiv/loss_curve_highdim.pdf}
        \vspace{-10pt}\caption{Global validation loss curve of the baseline model, trained on a $3^4$ grid, with $3$ filled bands to learn the second order Chern number $\tilde{C}_2$.}
        \label{fig:loss-highdim}
    \end{figure}


%\section*{Learning determinants}

%An architecture with two linear layers (i.e. $R=1$) will be denoted by $(1,1)$ corresponding exactly to a standard 2-layer MLP. 
%To model the determinant of a $4 \times 4$ matrix we use architectures $(2,2)$, $(2,2,2)$ and $(4)$.
% Non-linear target functions are typically modeled by artificial neural networks
% using piecewise interpolation. For example, a single hidden layer MLP with
% rectified linear activation provides a piecewise-linear approximation. These
% types of approximations seldom provide good generalization outside of the training
% domain.

    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
