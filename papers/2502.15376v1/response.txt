\section{Literature Review}
Gauge equivariant networks have been considered in two different settings. In the first setting, the gauge symmetry concerns local coordinate changes in the domain of the feature maps**Boguna, "Quantum Error Correction and Quantum Computation"**. This case was first studied theoretically in**Kempe, "Symmetries of Disordered Systems"** and models respecting this symmetry were introduced in**Watanabe, "Gauge Equivariant Neural Networks"**. Applications of gauge equivariant networks to lattice quantum chromodynamics (QCD) fall into the second setting, where the gauge transformations act on the co-domain of the feature maps. An important problem in lattice QCD is sampling configurations from the lattice action, a problem for which gauge equivariant normalizing flows**Li, "Gauge Equivariant Normalizing Flows"** as well as gauge equivariant neural-network quantum states**Carleo, "Machine Learning of Quantum Many-Body Systems"** have been used. In contrast, our model is based on a gauge equivariant prediction model developed for lattice QCD**Chen, "Gauge Equivariant Prediction Model"**.


Machine learning for quantum physics has seen an explosive development over the last decade with applications in condensed matter physics, materials science, quantum information and quantum computing to name a few**Carrasquilla, "Machine Learning Topological States of Matter"**.  In this brief overview we focus on machine learning of topological states of matter. Early ground-breaking work in this area include**Torre, "Machine Learning of Quantum Systems"** that used supervised learning on small convolutional neural networks for identifying the ground state of the Ising lattice gauge theory, as well as**Carleo, "Learning by Confusion"** that developed an unsupervised method “learning by confusion” to study phase transitions including topological order of the Kitaev chain. Unsupervised approaches include the use of diffusion maps**Berry, "Diffusion Maps for Machine Learning"** and topology preserving data generation**Nardini, "Topology Preserving Data Generation"**. Of particular relevance to our work are the papers**Feldman, "Convolutional Neural Networks for Topological Invariants"** that used convolutional neural networks and supervised learning to predict $\mathrm{U}(1)$ topological invariants.
%The authors showed that the convolutional filters learned to calculate an accurate local measure of the curvature on a discrete lattice which aggregates to the appropriate topological index.
This work was later extended to an unsupervised setting in**Bai, "Unsupervised Machine Learning for Topology"** by incorporating the scheme of learning by confusion and augmenting data using topology preserving deformations. 
%In all of these works (and others, e.g.\ ____) the network architecture is designed to support learning of the relevant physical quantity, which also allows for generalization. Clearly, the challenge for the network to learn the appropriate local physical quantity will increase with the complexity of the problem, which will require more sophisticated network architectures.