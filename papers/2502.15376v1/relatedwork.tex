\section{Literature Review}
Gauge equivariant networks have been considered in two different settings. In the first setting, the gauge symmetry concerns local coordinate changes in the domain of the feature maps~\cite{bronstein2021,weiler2023EquivariantAndCoordinateIndependentCNNs,gerken2023}. This case was first studied theoretically in~\cite{cheng2019} and models respecting this symmetry were introduced in~\cite{cohen2019,dehaan2020}. Applications of gauge equivariant networks to lattice quantum chromodynamics (QCD) fall into the second setting, where the gauge transformations act on the co-domain of the feature maps. An important problem in lattice QCD is sampling configurations from the lattice action, a problem for which gauge equivariant normalizing flows~\cite{kanwar2020,boyda2021,nicoli2021,bacchio2023,abbott2023} as well as gauge equivariant neural-network quantum states~\cite{luo2021a} have been used. In contrast, our model is based on a gauge equivariant prediction model developed for lattice QCD~\cite{favoni2022}.


Machine learning for quantum physics has seen an explosive development over the last decade with applications in condensed matter physics, materials science, quantum information and quantum computing to name a few~\cite{Carleo_2019,Carrasquilla_2020,Krenn,dawid2023modernapplicationsmachinelearning}.  In this brief overview we focus on machine learning of topological states of matter. Early ground-breaking work in this area include~\cite{Carrasquilla_2017} that used supervised learning on small convolutional neural networks for identifying the ground state of the Ising lattice gauge theory, as well as~\cite{van_Nieuwenburg_2017} that developed an unsupervised method “learning by confusion” to study phase transitions including topological order of the Kitaev chain. Unsupervised approaches include the use of diffusion maps~\cite{PhysRevB.102.134213} and topology preserving data generation~\cite{PhysRevLett.124.226401}. Of particular relevance to our work are the papers~\cite{Zhang_2018,PhysRevB.98.085402} that used convolutional neural networks and supervised learning to predict $\mathrm{U}(1)$ topological invariants.
%The authors showed that the convolutional filters learned to calculate an accurate local measure of the curvature on a discrete lattice which aggregates to the appropriate topological index.
This work was later extended to an unsupervised setting in~\cite{PhysRevResearch.2.013354,balabanov2020unsupervised} by incorporating the scheme of learning by confusion and augmenting data using topology preserving deformations. 
%In all of these works (and others, e.g.\ \cite{Zhang}) the network architecture is designed to support learning of the relevant physical quantity, which also allows for generalization. Clearly, the challenge for the network to learn the appropriate local physical quantity will increase with the complexity of the problem, which will require more sophisticated network architectures.