\begin{abstract}
\iffalse
Large Language Models (LLMs) have achieved remarkable success in recent years, owing to their impressive generalization capabilities and rich world knowledge. To capitalize on the potential of using LLMs as recommender systems, mainstream approaches typically focus on two strategies: employing in-context learning to utilize LLMs without fine-tuning or aligning LLMs with recommendation tasks through fine-tuning on recommendation data. However, we argue that fine-tuning-free methods often perform inadequately in recommendation tasks. While fine-tuning approaches can lead to the diminution of the generalization capabilities in the original pre-trained models, potentially resulting in suboptimal outcomes in scenarios such as cold-start and cross-domain recommendations. 
%This creates a barrier to 

To tackle these issues, we propose a generalizable LLM-based recommendation framework named MoLoRec.
%which is a parameter-efficient tuning method designed for LLM-based recommender systems. 
Our approach starts by individually fine-tuning domain-generalizable LoRA adapters and domain-specific LoRA adapters. We then show that a highly efficient and effective linear arithmetic operation can merge these LoRA adapters within the weight space, allowing MoLoRec to maintain strong recommendation performance across both specific domains and out-of-distribution scenarios. To further enhance the merging process of LoRA adapters, we introduce an adaptive LoRA weight merging method guided by entropy minimization during test time.
Extensive experiments validate the effectiveness and generalizability of our MoLoRec framework. Our codes are available at \url{https://anonymous.4open.science/r/MoLoRec}.
\fi



Large Language Models (LLMs) have achieved remarkable success in recent years, owing to their impressive generalization capabilities and rich world knowledge. To capitalize on the potential of using LLMs as recommender systems, mainstream approaches typically focus on two paradigms. The first paradigm designs multi-domain or multi-task instruction data for generalizable recommendation, so as to align LLMs with general recommendation areas and deal with cold-start recommendation. The second paradigm enhances domain-specific recommendation tasks with parameter-efficient fine-tuning techniques, in order to improve models under the warm recommendation scenarios.  While most previous works treat these two paradigms separately, we argue that they have complementary advantages, and combining them together would be helpful.

To that end, in this paper, we propose a generalizable and efficient LLM-based recommendation framework \shortname. Our approach starts by parameter-efficient fine-tuning a domain-general module with general recommendation instruction data, to align LLM with recommendation knowledge. Then, given users' behavior of a specific domain, we construct a domain-specific instruction dataset and apply efficient fine-tuning to the pre-trained LLM. After that, we provide approaches to integrate the above domain-general part and domain-specific part with parameters mixture. Please note that, \shortname~is efficient with plug and play, as the domain-general module is trained only once, and any domain-specific plug-in can be efficiently merged with only domain-specific fine-tuning. Extensive experiments on multiple datasets under both warm and cold-start recommendation scenarios validate the effectiveness and generality of the proposed \shortname. 
Codes are available at \url{https://anonymous.4open.science/r/MoLoRec}.

%Our approach starts by individually fine-tuning domain-generalizable LoRA adapters and domain-specific LoRA adapters. We then show that a highly efficient and effective linear arithmetic operation can merge these LoRA adapters within the weight space, allowing MoLoRec to maintain strong recommendation performance across both specific domains and out-of-distribution scenarios. To further enhance the merging process of LoRA adapters, we introduce an adaptive LoRA weight merging method guided by entropy minimization during test time.


\end{abstract}