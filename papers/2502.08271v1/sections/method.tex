\section{Methodology}
\iffalse
\begin{figure}[b]
    \centering
     \includegraphics[width=0.48\textwidth]{figures/phase1_3.pdf}
    %\vspace{-0.4cm}
    \caption{Illustration of aligning LLM with recommendation task.}
    \label{fig:phase1}
    \Description{}
\end{figure}
\fi
\begin{figure*}[ht]
    \centering \includegraphics[width=0.98\textwidth]{figures/framework9.pdf}
    \vspace{-0.2cm}
    \caption{Illustration of our proposed \shortname~framework.
    Firstly, to align LLM with any recommendation task, we construct a generalizable base model. Secondly, we fine-tune LLM to get domain-specific LoRA plugins. Finally, we perform a highly efficient and effective linear arithmetic operation to merge these LoRA adapters within the weight space, allowing \shortname~to maintain strong recommendation performance across both specific domains and out-of-distribution scenarios.}
    \label{fig:framework}
    \Description{}
\end{figure*}
In this section, we propose \shortname, a generalizable, effective, and efficient LLM-based recommendation framework. As shown in Figure \ref{fig:framework}, \shortname~operates through three key stages. 
Firstly, we align the LLM with any recommendation task and train a domain-general LoRA module~(Section \ref{sec:stage1}). Secondly, to adapt the framework to specific domains, we fine-tunes the LLM to obtain domain-specific LoRA modules (Section \ref{sec:stage2}). Subsequently, \shortname~performs an efficient linear arithmetic operation to merge these LoRA adapters within the weight space, enabling it to maintain strong recommendation performance across both specific domains and out-of-distribution scenarios (Section \ref{sec:stage3_1}). Notably, our framework is designed for ease of use, offering plug-and-play integration, where the domain-general module is trained once, and domain-specific adaptations are incorporated with minimal fine-tuning. \shortname~ is capable of significantly enhancing the recommendation performance and generalization across various domains at a relatively low cost.

\iffalse
First, the recommendation ability alignment stage aligns the pre-trained LLM with the general characteristics of recommendation tasks using an autoregressive objective (Section \ref{sec:stage1}). Next, the domain-specific fine-tuning stage incorporates specialized knowledge from specific domains (e.g., movies, beauty products) into the pre-trained LLM (Section \ref{sec:stage2}). Finally, we introduce the Mixture-of-LoRA method to address specific recommendation scenarios by adaptively integrating general and domain-specific knowledge. This approach significantly enhances the model's recommendation performance and generalization ability without increasing inference time or the number of model parameters (Section \ref{sec:stage3_1}).
\fi

%\subsection{Aligning LLM with Recommendation Task}
\subsection{Training Generalizable Base Recommender}
\label{sec:stage1}
Pre-trained LLMs often exhibit suboptimal performance when directly applied to recommendation tasks~\cite{bao2023tallrec,10.1145/3678004}.
This limitation arises because their pre-training on general-purpose datasets fails to capture the specialized knowledge required for understanding user preferences, behaviors, and the contextual nuances essential to building effective recommender systems.
To bridge this gap, in this subsection, we align the pre-trained LLM with the recommendation task, as shown in Figure \ref{fig:framework}(a). Specifically, we first construct large-scale instruction data by mixing users' behaviors from multiple domains.
The pre-trained LLM is then fine-tuned on this dataset using the LoRA technique, equipping it with general knowledge to tackle recommendation tasks.

\noindent$\bullet$ \quad\textbf{Instruction Data Construction.}
Given $N$ recommendation domains (i.e., $\mathcal{D}^1$, $\mathcal{D}^2$,..., $\mathcal{D}^N$), denotes $\mathcal{U}^n$, $\mathcal{I}^n$, and $\mathcal{S}^n$ as the user set, item set and user interaction sequence set of domain $n$, respectively.
To provide general user modeling and recommendation knowledge, we combine data from all $N$ domains and design instruction templates to convert them into a text format. Note that the choice of recommendation domains and instruction templates can be arbitrary. 
%In this study, we collect user recommendation data from 8 domains of the Amazon Review dataset\footnote{\url{https://cseweb.ucsd.edu/~jmcauley/datasets.html}}, which include Clothing, Cell, Grocery, Health, Home, Pet, Tools, and Videos. The statistics of the 8 datasets are listed in Table \ref{tab:stage1_data}.
As illustrated in Figure \ref{fig:framework}(a), we transform the recommendation data into instruction data $\mathcal{D}_{g}=\{(\textbf{x}, \textbf{y})\}$, where $\textbf{x}$ and $\textbf{y}$ denote the instruction input and output, respectively. The instruction input includes the task description (which explains the recommendation task), the userâ€™s historical interactions, and the item candidate set, all in natural language. Here, the items are represented by their titles. The candidate set consists of one ground-truth item and some randomly selected negative samples. The instruction output is designed to rank the user's next most likely products.


\iffalse
We denote the mixed user, item, and the interaction set as $\mathcal{U}$, $\mathcal{I}$, $\mathcal{S}$, respectively.
For each user interaction sequence $\mathcal{S}_u=\left[i_u^1, i_u^2, \ldots, i_u^L\right]$ 

For the instruction template, 

 
To provide general user modeling and recommendation knowledge, we mix user behaviors $\mathcal{S}_u=\left[i_u^1, i_u^2, \ldots, i_u^L\right]$ from multiple domains and reconstruct a recommendation instruction dataset $\mathcal{D}_{g}=\{(\textbf{x},\textbf{y})\}$, where $\textbf{x}$ and $\textbf{y}$ denote the instruction input and output, respectively. The choice of the instruction format and the original user behaviors data can be arbitrary. In this work, we collect 8 domains' user recommendation data from Amazon Review dataset\footnote{\url{https://cseweb.ucsd.edu/~jmcauley/datasets.html}}, including Clothing, Cell, Grocery, Health, Home, Pet, Tools, and Videos. Specifically, as shown in the left part of Figure \ref{fig:phase1}, the instruction input starts from a system prompt that defines the LLM's role-"\textit{You are a helpful recommendation assistant}"
\fi

\noindent$\bullet$ \quad\textbf{Tuning Domain-General LoRA Module.}
Given the instruction data $\mathcal{D}_{g}$, we apply LoRA fine-tuning to adapt the pre-trained LLM for general recommendation tasks. The pre-trained model parameters are kept frozen, while trainable low-rank decomposition matrices are introduced into each layer of the Transformer architecture, enabling efficient and lightweight tuning. Formally,
\begin{equation}
\max _{\Delta\Theta_g} \sum_{(\mathbf{x},\mathbf{y}) \in \mathcal{D}_{g}} \sum_{t=1}^{|\mathbf{y}|} \log P_{\Theta_{\text{pre}} +\Delta\Theta_g}\left(\mathbf{y}^t \mid \mathbf{y}^{<t}, \mathbf{x}\right),
\end{equation}
where $\Theta_{\text{pre}}$ is the parameters of the pre-trained LLM, and $\Delta\Theta_g$ denotes the set of parameters of LoRA fine-tuning. By undergoing this fine-tuning step, $\Delta\Theta_g$ is now enriched with extensive general knowledge relevant to the field of recommendations.


\subsection{Training Domain-Specific Plugin}
\label{sec:stage2}

Each recommendation domain exhibits unique user behavior patterns, making the acquisition of domain-specific knowledge essential for delivering accurate recommendations. For instance, in the clothing recommendation domain, user behavior is predominantly driven by style preferences. In contrast, when purchasing electronic products, users tend to prioritize compatibility with their existing products. To address these domain-specific needs, we construct an instruction dataset $\mathcal{D}_s$ tailored to the specific new recommendation domain $s$ and apply LoRA fine-tuning to the pre-trained LLM, resulting in the LoRA modules enhanced with domain-specific expertise. 
As shown in Figure \ref{fig:framework}(b), the template for constructing the instruction dataset and LoRA fine-tuning method are similar to those outlined in Section \ref{sec:stage1}. We formally define the process of training the domain-specific LoRA module $\Delta\Theta_s$ as:
\begin{equation}
\max _{\Delta\Theta_s} \sum_{(\mathbf{x},\mathbf{y})\in \mathcal{D}_s} \sum_{t=1}^{|\mathbf{y}|} \log P_{\Theta_{\text{pre}} +\Delta\Theta_s}\left(\mathbf{y}^t \mid \mathbf{y}^{<t}, \mathbf{x}\right).
\end{equation}

\subsection{Mixture-of-LoRA for Plug-and-Play.}
\label{sec:stage3_1}
\begin{figure}[b]
\vspace{-8mm}
    \centering \includegraphics[width=0.4\textwidth]{figures/task_arithmetic_3.pdf}
    \vspace{-0.2cm}
    \caption{Illustration of task arithmetic~\cite{ilharco2023editing}. (a) A task vector is obtained by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning. (b) Adding task vectors together improves the performance of the pre-trained model on the tasks under consideration.}
    \label{fig:task_arithmetic}
    \Description{}
\end{figure}


After instruction fine-tuning in stages 1 and 2, we obtained a domain-general model that captures general recommendation knowledge and a domain-specific model that incorporates domain-specific insights. In this subsection, we propose integrating these two parts to improve recommendation accuracy and enhance generalization capabilities simultaneously.
Natural questions arise: could this goal be achieved by applying traditional ensemble learning methods that combine the outputs of multiple models? Or could we integrate general and domain-specific knowledge by directly performing second-round fine-tuning on the domain-general LoRA module with the domain-specific dataset?
Unfortunately, the answer is no. Since LLMs generate natural language text, ensembling their outputs can introduce semantic inconsistencies or ambiguities, while also increasing inference time and GPU memory usage. Meanwhile, performing a second round of fine-tuning risks catastrophic forgetting~\cite{kirkpatrick2017overcoming}, causing the model to collapse.

\noindent$\bullet$ \quad\textbf{Mixture-of-LoRA.} We propose a simple yet effective method called Mixture-of-LoRA, which linearly merges the model parameters of the domain-general LoRA module $\Delta\Theta_g$ and the domain-specific LoRA module $\Delta\Theta_s$.
Formally, given the domain-general LoRA module $\Delta\Theta_g =\{\boldsymbol{A}_g^l,\boldsymbol{B}_g^l\}_{l=1}^L$ and the domain-specific LoRA module $\Delta\Theta_s=\{\boldsymbol{A}_s^l,\boldsymbol{B}_s^l\}_{l=1}^L$, we define the mixture-of-LoRA operator $\oplus$ as:
\begin{equation}
    \Delta\Theta_m = (\lambda_1\Delta\Theta_g) \oplus (\lambda_2\Delta\Theta_s) = \{\boldsymbol{A}_m^l, \boldsymbol{B}_m^l\}_{l=1}^L,
\label{eq:merge}
\end{equation}
\begin{equation}
    \boldsymbol{A}_m^l = \lambda_1\boldsymbol{A}_g^l + \lambda_2\boldsymbol{A}_s^l,
\end{equation}
\begin{equation}
    \boldsymbol{B}_m^l = \lambda_1\boldsymbol{B}_g^l + \lambda_2\boldsymbol{B}_s^l,
\end{equation}
where the coefficients $\lambda_1$ and $\lambda_2$ represents the importance of merging. We constraint $\lambda_1 + \lambda_2=1$ and $0 <= \lambda_1,\lambda_2 <=1$. They can be considered hyperparameters and selected using the validation data. Please note that the mixed $\Delta\Theta_m$ maintains the same total number of parameters as one standard LoRA, making our Mixture-of-LoRA method simple, fast, and effective. There is no extra cost at inference time in terms of memory or compute, since we only do element-wise operations on model weights. In addition, the domain-general LoRA module is reusable. When facing a new domain, it is only necessary to retrain a domain-specific LoRA module.

Mixture-of-LoRA is inspired by recent studies~\cite{ilharco2023editing,wortsman2022model} on the linear connectivity of trained models in a full finetuning setting. These studies suggest that parameters of tuned models can be directly added to improve generalization, provided they are initialized from the same pre-trained model checkpoint. 
Specifically, as shown in Figure \ref{fig:task_arithmetic}, recent research~\cite{ilharco2023editing} defines the concept of "task vector". A task vector $\Delta\Theta$ specifies a direction in the weight space of a pre-trained model , such that movement in that direction improves performance on the task. It is built by subtracting the weights of a pre-trained model ($\Delta\Theta_{pre}$) from the weights of the same model after fine-tuning ($\Delta\Theta_{ft}$) on a task. Adding task vectors together can improve performance on multiple tasks at once.
The underlying hypothesis is that two models finetuned from the same pre-trained checkpoint often lie in the same error basin~\cite{neyshabur2020being,zhang2023composing}, and thus the parameters could be directly added.
Extending this property to the context of LoRA, we hypothesize that LoRA modules can also be linearly combined. This is because a LoRA module can be considered as the difference between a fine-tuned LLM and its pre-trained counterpart, making it analogous to a task vector.


\noindent$\bullet$ \quad\textbf{Entropy-Guided Adaptive Mixture-of-LoRA.}
As shown in Figure \ref{fig:framework}(c), in this subsection, we provide an efficient and automatic way to better choose mixture coefficients $\lambda_1$ and $\lambda_2$. As discussed in the previous subsection, $\lambda_1$ and $\lambda_2$ can be chosen by employing the grid-search in the validation data. Nevertheless, (1) it is still lacking a guiding principle. (2) When the distribution of the inference data differs significantly from that of the validation set, the chosen coefficients may perform poorly.
To this end, we introduce entropy minimization on the unlabeled test samples an optimization surrogate objective to update $\lambda_1$ and $\lambda_2$.
Specifically, the Shannon entropy~\cite{shannon1948mathematical} is a well-known measure of uncertainty. For a sample $\mathbf{x}_i$, the predicted output of a neural network $\mathcal{F}_\theta(\mathbf{x}_i)$ is $\hat{\mathbf{y}}_i$, the Shannon entropy is calculated as $H(\hat{\mathbf{y}}_i) = -\sum_c^C p\left(\hat{\mathbf{y}}_{i, c}\right) \log p\left(\hat{\mathbf{y}}_{i, c}\right)$, where $p\left(\hat{\mathbf{y}}_{i, c}\right)$ denotes the probability that the input $\mathbf{x}_i$ is predicted to be the $c$-th class. 
Lower entropy indicates that the model has lower uncertainty about its predictions, meaning the model is more confident in its outputs. 
Therefore, the intuition behind our method is that the good coefficients $\lambda_1$ and $\lambda_2$ for the test inputs should make the mixed model more confident in its prediction, that is, it should lead to lower model entropy over the input~\cite{wang2021tent,wang2021emea,yang2024adamerging}.
Formally, we collect a set of unlabeled test samples $\mathcal{D}_t$, i.e., some instruction inputs in the test time. We fix the $\Delta\Theta_g$, $\Delta\Theta_s$, $\Theta_\text{pre}$, and using the following entropy minimization loss to update coefficients $\lambda_1$ and $\lambda_2$:
\begin{align}
\min _{\lambda_1, \lambda_2} 
& \sum_{\mathbf{x}_i \in \mathcal{D}_t} 
H\left(\mathcal{F}_{\Theta_\text{MoLo}}\left(\mathbf{x}_i\right)\right), \\
& \text{where } \Theta_{\text{MoLo}} = 
\Theta_{\text{pre}} + (\lambda_1\Delta\Theta_g) \oplus (\lambda_2\Delta\Theta_s).
\end{align}
For the LLM, the output of $\mathcal{F}_{\Theta_\text{MoLo}}$ is a sentence. Since our instruction is to select a title from a given candidate set, the first few tokens output by the model are more important because after deciding on them, the subsequent tokens are more certain.
So in practice, we only calculate the average entropy of the first three tokens in the sentence to represent $H\left(\mathcal{F}_{\Theta_\text{MoLo}}\right)$.
Besides, we do not need all test data to be available. Even if only 50 unlabeled tests are available, our method can have significant performance improvements.


\subsection{Discussion}
\noindent$\bullet$\quad\textbf{Key Advantages of \shortname.} 
\underline{1) Generalization.}
%\noindent$\bullet$\quad\textbf{Generalization.}
\shortname~is generalizable to various recommendation scenarios.
For example, when facing a new recommendation domain, it only needs to train a new domain-specific LoRA module, enabling rapid generalization. Even in extreme cases where no training data is available for the new domain, \shortname~can still work using the generalizable base model. For the new user or new item recommendation scenario, \shortname~dynamically balances generalization and domain-specific specialization to deliver accurate recommendations.
Additionally, leveraging the in-context learning capabilities of LLMs, \shortname~naturally exhibits task generalization. E.g., it can generate explainable recommendation results.
\underline{2) Efficiency.}
%\noindent$\bullet$\quad\textbf{Efficiency.}
The proposed \shortname~paradigm is designed for ease of use, offering plug-and-play integration, where the domain-general module is trained once, and the domain-specific plugin is incorporated with minimal fine-tuning. 

\noindent$\bullet$\quad\textbf{Comparision to Existing Methods.}
Traditional sequential recommendation models (e.g., GRU4Rec~\cite{DBLP:journals/corr/HidasiKBT15}, SASRec~\cite{kang2018self}) typically rely on explicit item IDs for modeling, which restricts their generalization ability across new domains or platforms. To this end, \underline{transferable methods} have been explored for universal sequence representation learning. For instance, VQ-Rec~\cite{10.1145/3543507.3583434}) and UniSRec~\cite{hou2022towards} leverage text to represent items and employ contrastive pre-training strategies on language models to enhance transferability.
However, these pretraining methods cannot be directly applied to LLMs.
With the rise of LLMs, their strong domain and task generalization capabilities present new opportunities for improving the generalization of recommender systems. We categorize these approaches as \underline{breadth-oriented methods}. For example, P5~\cite{geng2022recommendation} designs prompts to unify five recommendation tasks, establishing a unified text-to-text recommendation paradigm.
In contrast, \underline{depth-oriented methods} focus on deeply aligning LLMs with specific domains. For instance, TallRec~\cite{bao2023tallrec} employs LoRA for efficient domain adaptation. Another research direction explores aligning collaborative signals with LLMs, as seen in works like LLaRA~\cite{10.1145/3626772.3657690} and iLoRA~\cite{kong2024customizing}. While incorporating collaborative signals significantly enhances performance in warm-start scenarios, it comes at the cost of reduced generalization, making adaptation to new domains and cold-start situations more challenging.


\iffalse
\subsubsection{Model Generalization and Efficiency}
The proposed \shortname\newline paradigm is designed for ease of use, offering plug-and-play integration, where the domain-general module is trained once, and the domain-specific plugin is incorporated with minimal fine-tuning. \shortname~is highly efficient, as the mixture-of-LoRA method merges the weights, incurring no additional memory or computational cost during inference.
\noindent$\bullet$\quad\textbf{Generalization.}
\shortname~is generalizable to various recommendation scenarios.
For example, when facing a new recommendation domain, it only needs to train a new domain-specific LoRA module, enabling rapid generalization. Even in extreme cases where no training data is available for the new domain, \shortname~can still work using the generalizable base model. For the new user or new item recommendation scenario, \shortname~dynamically balances generalization and domain-specific specialization to deliver accurate recommendations.
Additionally, leveraging the in-context learning capabilities of LLMs, \shortname~naturally exhibits task generalization. For example, it can generate explainable recommendation results.

\noindent$\bullet$\quad\textbf{Efficiency.}
\shortname~is highly efficient, as the mixture-of-LoRA method merges the weights, incurring no additional memory or computational cost during inference.
\fi
    










%\subsubsection{Comparisons to LLM-Based Recommendation Models}
