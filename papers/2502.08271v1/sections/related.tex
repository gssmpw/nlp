\vspace{-4mm}
\section{Related Works}
\noindent$\bullet$\quad\textbf{Sequential Recommendation.}
Sequential recommendation utilizes users' interaction histories to predict the next relevant item. Deep learning-based methods, such as RNNs~\cite{DBLP:journals/corr/HidasiKBT15,RNN2}, GNNs~\cite{graph_1,graph_2}, and attention mechanisms~\cite{kang2018self,S3-Rec,CL4SRec}, have become mainstream but rely solely on item IDs, limiting their adaptability to new scenarios. To improve transferability, transferable sequential recommendation studies~\cite{hou2022towards,10.1145/3543507.3583434,10.1145/3580305.3599519,MoRec} explore leveraging textual features to enhance item representations. These approaches improve the transferability and robustness of recommender systems.
Nowadays, LLMs offer new opportunities for sequential modeling, promising more robust and generalizable recommender systems.


\noindent$\bullet$\quad\textbf{LLM-Based Recommendation.}
%\subsection{LLM-Based Recommendation}
With the rise of LLMs, interest in LLM-based recommender systems has grown, leveraging LLMs as core engines. Early studies~\cite{dai2023uncovering,sanner2023large,wang-etal-2023-rethinking-evaluation} explore their zero-shot/few-shot potential via in-context learning~\cite{dong2024survey}. However, the gap between LLMs' pretraining on general text and recommendation-specific needs leads to suboptimal performance. To address this, recent research follows two paradigms: breadth-oriented and depth-oriented.
The former integrates multi-domain~\cite{10.1145/3705727,peng2024ecellm} or multi-task~\cite{geng2022recommendation,10.1145/3708882,cui2022m6,peng2024ecellm} recommendation data to construct extensive recommendation world knowledge, paving the way for developing a generalizable LLM-based recommender. For example, Peng et al.\cite{peng2024ecellm} build large-scale e-commerce instruction dataset ECInstruct and develop generalist LLM for e-commerce. P5~\cite{geng2022recommendation} designs prompts to unify 5 recommendation tasks and presents a unified text-to-text recommendation paradigm.
Depth-oriented paradigm seeks to enable LLMs to deeply comprehend recommendation tasks within specific domains. 
Key areas of focus include: 1) the in-depth alignment of domain-specific recommendation knowledge, such as collaborative signals~\cite{lin2024bridging,10.1145,10.1145/3626772.3657690,10.1145/3589334.3645458,kong2024customizing}.
The introduction of collaborative signals effectively improves model performance in warm-start scenarios. However, this comes at the cost of reduced generalization, making it challenging to adapt to new domains and cold-start situations.
Another research focus is 2) the development of efficient alignment methods between large language models and recommendation tasks. These methods include leveraging~\cite{bao2023tallrec,kong2024customizing} LoRA fine-tuning techniques and designing data-efficient fine-tuning strategies~\cite{10.1145/3626772.3657807, zheng2024harnessing}.
In fact, the two paradigms have complementary advantages. In this work, we investigate how to integrate the advantages of both paradigms to simultaneously achieve both breadth and depth.



\iffalse

Existing LLM-based recommendation methods can be broadly classified into two categories based on the specific emergent capabilities of LLMs they prioritize. (1) The first category aims to build general-purpose recommender systems by harnessing the generalizability of LLMs~\cite{10.1145/3705727,geng2022recommendation,10.1145/3708882,peng2024ecellm,dai2023uncovering}.
This line of research reformulates data from diverse recommendation tasks or domains into unified textual prompts, which are then used as input for LLMs.
Early works leveraging LLMs for recommender systems, aiming to leverage the LLMs' rich world knowledge, strong reasoning, and generalization abilities.





Existing research on LLMs for recommendation can be primarily divided into two categories: (1) LLM-enhanced recommendations, which treat LLMs as powerful feature extractors to enrich user and item representations. (2) LLM-based recommendations, which directly employ LLMs as recommender systems. Along the second line, early works xxxxxx
\fi


\noindent$\bullet$\quad\textbf{Model Merging.}
%\subsection{Model Merging}
Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs~\cite{yang2024modelmergingllmsmllms}.
Model merging techniques are applied in various scenarios such as unlearning old-knowledges in LLMs~\cite{zaman-etal-2024-fuse}, understanding content across multiple modalities~\cite{aiello2024jointly,chen-etal-2024-model}, generating images with different styles or achieving image-style transformation~\cite{biggs2024diffusion}.
Previous attempts involve merging multiple models, all initially trained on the same task, with the aim of enhancing the modelâ€™s overall generalization~\cite{Gupta2020Stochastic,wang2022meta}. 
Inspired by these works, we apply model merging to the LoRA modules, leveraging their ability to integrate domain-general and domain-specific knowledge effectively.
