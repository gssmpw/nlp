\section{Experiments}
\iffalse
In this section, we conduct experiments on three real-world datasets to answer the following research questions: 
\begin{itemize}[leftmargin=*]
\item \textbf{RQ1:} How does our proposed \shortname~perform compared to traditional, transferable, and LLM-based recommenders? How does \shortname~ perform in terms of generalization in scenarios such as the cold-start setting?
\item \textbf{RQ2:} How does each component of \shortname~ affect its effectiveness? Can the mixture-of-LoRA method integrate domain-specific and domain-general knowledge effectively?
\item \textbf{RQ3:} Is MoLoRec robust to hyperparameter selection and instruction input?
\end{itemize}
\fi
\subsection{Experimental Settings}

\begin{table*}[h]
\small
\centering
\caption{Performance Comparison Across Datasets in Warm Start I.I.D Scenario (Beauty, Toys, Sports, and MovieLens-1M).}
%\vspace{-2mm}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{ll|cc|cc|cc||cc}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{} & \multicolumn{2}{c|}{\textbf{Beauty}} & \multicolumn{2}{c|}{\textbf{Toys}} & \multicolumn{2}{c||}{\textbf{Sports}} & \multicolumn{2}{c}{\textbf{MovieLens-1M}} \\ 
 & & \textbf{NDCG@1} & \textbf{NDCG@3} & \textbf{NDCG@1} & \textbf{NDCG@3} & \textbf{NDCG@1} & \textbf{NDCG@3} & \textbf{NDCG@1} & \textbf{NDCG@3} \\ 
\midrule
\multirow{4}{*}{\textbf{Traditional}} 
& \textbf{BPR-MF} &  0.1630 & 0.2588 & 0.1276 & 0.2056 & 0.1496 & 0.2338 & 0.1724 & 0.4185 \\ 
& \textbf{GRU4Rec} & 0.1672 & 0.2752 & 0.1320 & 0.2243 & 0.1787 & 0.2829 & 0.1724 &  0.4423\\ 
& \textbf{SASRec} & 0.2410 & 0.3284 & 0.2223 & 0.3105 & 0.1957 & 0.2967 & 0.2257 & 0.4708 \\ 
& \textbf{FMLP-Rec}& 0.2988 & 0.4000 & 0.2994 & 0.3990 & 0.2645 & 0.3812 & 0.2410 & 0.5515 \\ 
\midrule
\multirow{2}{*}{\textbf{Transferable}} 
& \textbf{UniSRec} & 0.2654 & 0.4089 & 0.2612 & 0.3998 & 0.2341 & 0.3721 & 0.2615 & 0.5594 \\ 
& \textbf{VQ-Rec} & 0.2714 & 0.4157 & 0.2715 & 0.4119 & 0.2476 & \underline{0.3944} & 0.2805 & 0.5745 \\ 
\midrule
\multirow{4}{*}{\textbf{LLM-Based}} 
& \textbf{Qwen2-7B} & 0.0300 & 0.0394 & 0.0843 & 0.1062 & 0.0170 & 0.0242 & 0.0814 & 0.1057  \\ 
& \textbf{RecFormer} & 0.2858 & 0.3840 & 0.3001 & 0.3880 & 0.2667 & 0.3885 & 0.2743 & 0.5701 \\ 
& \textbf{P5} & 0.1775 & 0.2482 & 0.1171 & 0.1709 & 0.1860 & 0.2674 & 0.2046 & 0.2947\\ 
& \textbf{TALLRec} & 0.3208 & 0.3479 & 0.3308 & 0.3583 & 0.3002 & 0.3274 & 0.4759 & 0.4971\\ 
\midrule
\multirow{3}{*}{\textbf{Ours}} 
& \textbf{MoLoRec-G} & 0.3081 & 0.3316 & 0.2957 & 0.3209 & 0.2750 & 0.2998 & \underline{0.5680} & \underline{0.5918} \\
& \textbf{MoLoRec-S} & \underline{0.4079} & \underline{0.4291} & \underline{0.4076} & \underline{0.4314} & \underline{0.3735} & 0.3925 & 0.5460 & 0.5703\\
& \cellcolor[gray]{0.9}\textbf{MoLoRec} 
& \cellcolor[gray]{0.9}\textbf{0.4132*} 
& \cellcolor[gray]{0.9}\textbf{0.4350*} 
& \cellcolor[gray]{0.9}\textbf{0.4097*} 
& \cellcolor[gray]{0.9}\textbf{0.4334*} 
& \cellcolor[gray]{0.9}\textbf{0.3754*} 
& \cellcolor[gray]{0.9}\textbf{0.3944*} 
& \cellcolor[gray]{0.9} \textbf{0.5783*} 
& \cellcolor[gray]{0.9} \textbf{0.6023*}\\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\label{table:performance_comparison}
\end{table*}



\begin{table*}[h]
\small
\centering
\caption{Performance Comparison Across Datasets in Cold-Start Item O.O.D Scenario~(Beauty, Toys, Sports, and MovieLens-1M).}
%\vspace{-2mm}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{ll|cc|cc|cc||cc}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{} & \multicolumn{2}{c|}{\textbf{Beauty}} & \multicolumn{2}{c|}{\textbf{Toys}} & \multicolumn{2}{c||}{\textbf{Sports}} & \multicolumn{2}{c}{\textbf{MovieLens-1M}} \\ 
 & & \textbf{NDCG@1} & \textbf{NDCG@3} & \textbf{NDCG@1} & \textbf{NDCG@3} & \textbf{NDCG@1} & \textbf{NDCG@3} & \textbf{NDCG@1} & \textbf{NDCG@3} \\ 
\midrule
\multirow{4}{*}{\textbf{Traditional}} 
& \textbf{BPR-MF} & 0.0306 & 0.0688 & 0.0333 & 0.0765 & 0.0350 & 0.0739 & 0.0723 & 0.1421 \\ 
& \textbf{GRU4Rec} & 0.0562 & 0.1063 & 0.0447 & 0.0926 & 0.0640 & 0.0996 & 0.0798 & 0.1489 \\ 
& \textbf{SASRec} & 0.0656 & 0.1368 & 0.0670 & 0.1210 & 0.0547 & 0.1203 & 0.0912 & 0.1891 \\ 
& \textbf{FMLP-Rec} & 0.0587 & 0.1229 & 0.0537 & 0.1117 & 0.0545 & 0.1236 & 0.1145 & 0.1947 \\ 
\midrule
\multirow{2}{*}{\textbf{Transferable}} 
& \textbf{UniSRec} & 0.0957 & 0.1457 & 0.0814 & 0.1559 & 0.0832 & 0.1408 & 0.0985 & 0.1343 \\ 
& \textbf{VQ-Rec} & 0.1189 & 0.1589 & 0.0957 & 0.1603 & 0.0985 & 0.1463 & 0.1025 & 0.1412\\ 
\midrule
\multirow{4}{*}{\textbf{LLM-Based}} 
& \textbf{Qwen2-7B} & 0.0187 & 0.0260 & 0.0293 & 0.0356 & 0.0213 & 0.0273 & 0.0318 & 0.0407 \\ 
& \textbf{RecFormer} & 0.1051 & 0.1687 & 0.0913 & 0.1592 & 0.0922 & 0.1489 & 0.1108 & 0.1547\\ 
& \textbf{P5} & 0.0871 & 0.1466 & 0.0755 & 0.1358 & 0.0758 & 0.1355 & 0.0957 & 0.1319 \\ 
& \textbf{TALLRec} & 0.1415 & 0.1674 & 0.1251 & 0.1524 & 0.1226 & 0.1486 & 0.1458 & 0.1668 \\ 
\midrule
\multirow{3}{*}{\textbf{Ours}} 
& \textbf{MoLoRec-G} & \underline{0.1746} & \underline{0.2072} & 0.1474 & 0.1710 & \underline{0.1581} & \underline{0.1868} & 0.1455& 0.1890\\
& \textbf{MoLoRec-S} & 0.1603 & 0.1863 & \underline{0.1504} & \underline{0.1764} & 0.1564 & 0.1867 & \underline{0.1636} & \underline{0.2597}\\
& \cellcolor[gray]{0.9}\textbf{MoLoRec}
& \cellcolor[gray]{0.9}\textbf{0.1825*} 
& \cellcolor[gray]{0.9}\textbf{0.2145*} 
& \cellcolor[gray]{0.9}\textbf{0.1614*} 
& \cellcolor[gray]{0.9}\textbf{0.1821*} 
& \cellcolor[gray]{0.9}\textbf{0.1646*} 
& \cellcolor[gray]{0.9}\textbf{0.1921*} 
& \cellcolor[gray]{0.9}\textbf{0.1818*} 
& \cellcolor[gray]{0.9}\textbf{0.2755*} \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\label{table:performance_comparison_cold_start}
\end{table*}


\iffalse
\begin{table}[ht]
\small
\centering
\caption{Statistics of the datasets for stage 1.}
\begin{tabular}{lcccc}
\toprule[1pt]  % Make the top border thicker
\textbf{Datasets} & \textbf{\# Users} & \textbf{\# Items} & \textbf{\# Interactions} & \textbf{Density(\%)} \\
\midrule[0.8pt]     % Optional: Adjust midrule thickness if needed
Clothing & 39,387 & 23,033 & 278,677 & 0.0307 \\
Cell & 27,879 & 10,429 & 194,439 & 0.0669 \\
Grocery & 14,681 & 8,713 & 151,254 & 0.1182 \\
Health & 38,609 & 18,534 & 346,355 & 0.0484 \\
Home & 66,519 & 28,237 & 551,682 & 0.0294 \\
Pet & 19,856 & 8,510 & 157,836 & 0.0934 \\
Tools & 16,638 & 10,217 & 134,476 & 0.0791 \\
Videos & 24,303 & 10,672 & 231,780 & 0.0894 \\
\midrule
Total & 247,872 & 118,354 & 2,046,499 & - \\
\bottomrule[1pt]  % Make the bottom border thicker
\end{tabular}
\label{tab:stage1_data}
\end{table}
\fi


\iffalse
\begin{table}[ht]
\small
\centering
\caption{Statistics of the four datasets.}
\begin{tabular}{lcccc}
\toprule[1pt]  % Make the top border thicker
\textbf{Specific Datasets} & \textbf{\# Users} & \textbf{\# Items} & \textbf{\# Interactions} & \textbf{Density(\%)} \\
\midrule[0.8pt]     % Optional: Adjust midrule thickness if needed
Beauty & 22,363 & 12,101 & 198,502 & 0.0734 \\
Toys & 19,412 & 11,924 & 167,597 & 0.0724 \\
Sports & 35,598 & 18,357 & 296,337 & 0.0453 \\
Movielens-1M & 6,040 & 6,883 & 1,000,209 & 0.2410 \\
\bottomrule[1pt]  % Make the bottom border thicker
\end{tabular}
\label{tab:data}
\end{table}
\fi
\subsubsection{\textbf{Datasets.}}
We conduct experiments on e-commerce and movie recommendation scenarios. 
For the e-commerce recommendation scenario, the domain-general instruction tuning dataset is conducted using seven e-commerce domains in Amazon\footnote{https://jmcauley.ucsd.edu/data/amazon/.} and validated on three domain-specific datasets in Amazon~(Beauty, Toys, Sports). 
For the movie recommendation scenario, the domain-general dataset is built using MovieLens-10M\footnote{\url{https://grouplens.org/datasets/movielens/}} and validated on the domain-specific dataset MovieLens-1M.


For all datasets, items are represented using their textual "title" information. To prevent data leakage, we carefully removed the overlapping portions between the domain-general dataset and the domain-specific datasets. We consider two recommendation settings:
1) \textbf{Warm-Start Setting} keeps the five-core dataset and filters users and items with fewer than five interactions for all datasets. Following \cite{geng2022recommendation,lin2024bridging}, we adopt the leave-one-out strategy to split the filtered dataset. More concretely, we split the last interaction of each user into the test set, the second-to-last one into the validation set, and the rest into the training data. 
2) \textbf{New-Item Setting} uses the same training and validation sets as the warm-start setting, but replaces the items in the test set with those that never appear in the training or validation sets.
(See Appendix \ref{sec:appendix_dataset} for dataset statistics.)

\iffalse

We conduct experiments on four real-world benchmark datasets, which consist of comprehensive textual information including "title". The statistics of the datasets after preprocessing are shown in Table \ref{tab:stage1_and_specific_data}. We describe the dataset details  as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Beauty}
is the collection of user interactions with beauty products from Amazon review datasets\footnote{https://jmcauley.ucsd.edu/data/amazon/.}.
    \item \textbf{Toys} is also one representative recommendation dataset drawn from Amazon review datasets, where each toy product has substantial meta information.
    \item \textbf{Sports} is an e-commerce dataset, that reflects user preference for purchasing sports and outdoor products on the Amazon platform.
    \item \textbf{Movielens-1M}\footnote{\url{https://grouplens.org/datasets/movielens/}} is a popular benchmark containing about 1 million pieces of movie ratings, ranging from 1 to 5. 
\end{itemize}
\fi
\iffalse
1) \textbf{Beauty}
is the collection of user interactions with beauty products from Amazon review datasets\footnote{https://jmcauley.ucsd.edu/data/amazon/.}.
2) \textbf{Toys} is also one representative recommendation dataset drawn from Amazon review datasets, where each toy product has substantial meta information.
3) \textbf{Sports} is an e-commerce dataset, that reflects user preference for purchasing sports and outdoor products on the Amazon platform.
4) \textbf{Movielens-1M}\footnote{\url{https://grouplens.org/datasets/movielens/}} is a popular benchmark containing about 1 million pieces of movie ratings, ranging from 1 to 5. 
\fi

\iffalse
In the downstream recommendation scenarios for Beauty, Toys, and Sports, the datasets listed in Table \ref{tab:stage1_data} are used to train the domain-general LoRA module. For the Movielens-1M dataset, the training is conducted using the Movielens-10M\footnote{\url{https://grouplens.org/datasets/movielens/}} dataset. To prevent data leakage, we carefully removed the overlapping portions between the domain-general dataset and the domain-specific datasets.

We consider two recommendation settings:
1) \textbf{Warm-Start Setting} keeps the five-core dataset and filters users and items with fewer than five interactions for all datasets. Following \cite{geng2022recommendation,lin2024bridging}, we adopt the leave-one-out strategy to split the filtered dataset. More concretely, we split the last interaction of each user into the test set, the second-to-last one into the validation set, and the rest into the training data. 
2) \textbf{New-Item Setting} uses the same training and validation sets as the warm-start setting, but replaces the items in the test set with those that never appear in the training or validation sets.
\fi
\iffalse
We first sort all interactions by the timestamps. Following \cite{geng2022recommendation,lin2024bridging}, we adopted the leave-one-out strategy to split the datasets. More concretely, we split the last interaction of each user into the test set, the second-to-last one into the validation set, and the rest into the training data. 
In addition, we consider two settings: 1) \textbf{Warm-Start Setting} keeps the five-core datasets and filter users and items with fewer than five interactions for all
datasets. 
2) \textbf{Cold-Start Setting} replaces the test set of the warm start setting with items that have been interacted with by users but less than five cores of the original dataset, while keeping the training set and validation set unchanged. This setting is referred to the completely cold-start item scenario~\cite{bai2024unified}.
\fi
\subsubsection{\textbf{Baselines.}} 
We compare \shortname~ with traditional recommendation methods (BPR-MF~\cite{10.5555/1795114.1795167}, GRU4Rec~\cite{DBLP:journals/corr/HidasiKBT15}, SASRec~\cite{kang2018self}, and FMLP-Rec)~\cite{10.1145/3485447.3512111}, transferable sequential recommenders (UniSRec~\cite{hou2022towards}, VQ-Rec~\cite{10.1145/3543507.3583434}), LLM-based recommenders (Qwen2-7B~\cite{qwen2}, RecFormer~\cite{10.1145/3580305.3599519}, P5~\cite{geng2022recommendation}, TALLRec~\cite{bao2023tallrec}) and our ablation counterparts (MoLoRec-G, MoLoRec-S). (See Appendix \ref{sec:appendix_baselines} for more details of these baselines.)
\iffalse
\begin{itemize}[leftmargin=*]
    \item \textbf{BPR-MF}~\cite{10.5555/1795114.1795167} is one of the most representative collaborative filtering models.
    \item \textbf{GRU4Rec}~\cite{DBLP:journals/corr/HidasiKBT15} is a seminal method that uses RNNs to model user action sequences for session-based recommendation. 
    \item \textbf{SASRec}~\cite{kang2018self} is a representative sequential recommender model that adopts a self-attention mechanism to learn the item dependency from user interactions.
    \item \textbf{FMLP-Rec}~\cite{10.1145/3485447.3512111} is an all-MLP model with learnable filters for sequential recommendation tasks.
    \item \textbf{UniSRec}~\cite{hou2022towards} equips textual item representations with an MoE-enhanced adaptor for domain fusion and adaptation. Both item-sequence and sequence-sequence contrastive learning tasks are designed for pre-training transferable sequence representations.
    \item \textbf{VQ-Rec}~\cite{10.1145/3543507.3583434} learns vector-quantized item representations for transferable sequential Recommenders.
    \item \textbf{Qwen2-7B}\footnote{\url{https://huggingface.co/Qwen/Qwen2-7B-Instruct}} is a well-known open-source LLM. In our experiments, we choose it as \shortname~'s LLM backbone.
    \item \textbf{RecFormer}~\cite{10.1145/3580305.3599519} models user preferences and item features using the LongFormer~\cite{beltagy2020longformer} backbone, transforming sequential recommendation into a task of predicting the next item as if predicting the next sentence, by converting item attributes into a sentence format.
    \item \textbf{P5}~\cite{geng2022recommendation} is a unified LLM-based recommendation framework. It is built on T5 by fine-tuning with multiple recommendation tasks.
    \item \textbf{TALLRec}~\cite{bao2023tallrec} learns the recommendation task based on prompts consisting solely of text and fine-tunes the LLMs using the LoRA.
    \item \textbf{MoLoRec-G} is an ablation counterpart of our proposed framework. It only underwent stage 1, utilizing only the domain-general LoRA module.
    \item \textbf{MoLoRec-S} is an ablation counterpart of our proposed framework. It only underwent stage 2, utilizing only the domain-general LoRA module.
\end{itemize}
\fi
\iffalse
1) \textbf{BPR-MF}~\cite{10.5555/1795114.1795167} is one of the most representative collaborative filtering models.
2) \textbf{GRU4Rec}~\cite{DBLP:journals/corr/HidasiKBT15} is a seminal method that uses RNNs to model user action sequences for session-based recommendation. 
3) \textbf{SASRec}~\cite{kang2018self} is a representative sequential recommender model that adopts a self-attention mechanism to learn the item dependency from user interactions.
4) \textbf{FMLP-Rec}~\cite{10.1145/3485447.3512111} is an all-MLP model with learnable filters for sequential recommendation tasks.
5) \textbf{UniSRec}~\cite{hou2022towards} equips textual item representations with an
MoE-enhanced adaptor for domain fusion and adaptation. Both
item-sequence and sequence-sequence contrastive learning tasks
are designed for pre-training transferable sequence representations.
6) \textbf{VQ-Rec}~\cite{10.1145/3543507.3583434} learns vector-quantized item
representations for transferable sequential Recommenders.
7) \textbf{Qwen2-7B}\footnote{\url{https://huggingface.co/Qwen/Qwen2-7B}} is a well-known open-source LLM. In our experiments, we choose it as \shortname~'s LLM backbone.
8) \textbf{RecFormer}~\cite{10.1145/3580305.3599519} models user preferences and item features using the LongFormer~\cite{beltagy2020longformer} backbone, transforming sequential recommendation into a task of predicting the next item as if predicting the next sentence, by converting item attributes into a sentence format.
9) \textbf{P5}~\cite{geng2022recommendation} is a unified LLM-based recommendation framework. It is built on T5 by fine-tuning with multiple recommendation tasks.
10) \textbf{TALLRec}~\cite{bao2023tallrec} learns the recommendation task based on prompts consisting solely of text and fine-tunes the LLMs using the LoRA.
11) \textbf{MoLoRec-G} is an ablation counterpart of our proposed framework. It only underwent stage 1, utilizing only the domain-general LoRA module.
12) \textbf{MoLoRec-S} is an ablation counterpart of our proposed framework. It only underwent stage 2, utilizing only the domain-general LoRA module.
\fi


\subsubsection{\textbf{Evaluation Setting.}} Following some previous LLM-based recommendation works~\cite{10.1145/3708882,kim2024large}, to evaluate the performance of the sequential recommendation models, we add 29 randomly selected non-interacted items to the test set, so that the test set of each user contains 1 positive item and 29 negative items. For quantitative comparison, we employ 
widely used ranking-based metrics, NDCG@1 and NDCG@3 for all experiments. All metrics are "the higher, the better". For all tables in the following, \textbf{bold*} numbers refer to the best performance, while \underline{underlined} numbers indicate the second-best performance.

\noindent(See Appendix \ref{sec:appendix_implementation} for implementation details.)

\subsection{Overall Performance}
We comprehensively compare \shortname~ against traditional, transferable, and LLM-based recommenders. The experimental results on the warm-start I.I.D setting and the cold-start item out-of-distribution settings are shown in Table \ref{table:performance_comparison} and \ref{table:performance_comparison_cold_start}, respectively. From the experimental results, we have the following observations:
\begin{itemize}[leftmargin=*]
    \item The proposed \shortname~ consistently achieves the best performance across all I.I.D and O.O.D scenarios on the four datasets, with a t-test at p<0.05 level.
    Specifically, in the warm-start scenario, \shortname~ achieves notable improvements in NDCG@1 over the best baseline methods (excluding our ablation counterparts), with performance gains of 28.8\%, 23.85\%, 25.05\%, and 21.52\% across the Beauty, Toys, Sports, and Movielens-1M, respectively. In the cold-start O.O.D scenario, the improvements are 28.98\%, 29.02\%, 34.26\%, and 24.69\%. The outstanding performance of MoLoRec in both I.I.D and O.O.D scenarios demonstrates its ability to efficiently capture domain-specific knowledge while exhibiting exceptional generalization capabilities.
    \item Qwen2-7B demonstrates limited performance across all scenarios. However, the LLMs trained via TALLRec achieved significant improvements. This is because there is a gap between pre-training general text corpus of LLMs and the recommendation task, showing the importance of using recommendation knowledge for instruction fine-tuning on pre-trained LLMs.
    \item Traditional recommendation methods and the ID-based LLM recommendation method P5 perform poorly in cold-start scenarios. Relying heavily on collaborative filtering information reduces the model's generalization capability.
    %\item Among traditional recommenders, sequential methods (GRU4Rec, SASRec, FMLP-Rec) surpass non-sequential methods (BPR-MF) on 4 datasets in the warm-start scenario. The better performance stems from the sequential modeling of the userâ€™s interaction sequence, which captures dynamic shifts in user interests and intricate item dependencies. We find that TALLRec, when using only textual information, outperforms traditional recommendation methods, highlighting the potential of large language model-based recommendation approaches.
\end{itemize}


\subsection{In-Depth Analysis}
\subsubsection{\textbf{Ablation Study}}
From the performance of \shortname, \shortname-G, \shortname-S in the Table \ref{table:performance_comparison} and \ref{table:performance_comparison_cold_start}, we can find that:
1) \shortname~ consistently surpasses its ablation counterparts in all scenarios, which indicates that it is crucial for LLM to comprehend both general recommendation world knowledge and domain-specific knowledge. These two types of knowledge can complement each other. It also validates that the mixture-of-LoRA method can effectively integrate both types of knowledge. 2) Note that the \shortname-G has not been exposed to training data from the Beauty, Toys, Sports, or Movielens-1M domains in any experimental setting. However, it still achieves commendable performance in such zero-shot settings, demonstrating that it has effectively learned generalizable knowledge in the recommendation domain. This strong performance underscores its robust generalization ability.

\subsubsection{\textbf{Performance in Few-Shot Training Setting}}
We further conduct experiments in scenarios with limited domain-specific training data. Specifically, we adopt a few-shot training setup on MovieLens-1M, where only a small percentage of samples are randomly selected from the training set for model training. We compare \shortname~with TallRec and the results of second-round fine-tuning on the generalizable base model. The experimental results are shown in Table \ref{tab:few-shot}.
We find that the optimization approach of second-round fine-tuning led to catastrophic forgetting. It fails to generate output in the specified instruction format. Experimental results demonstrate that MoloRec maintains strong performance even in few-shot scenarios.

\subsubsection{\textbf{Analysis of the Coefficients $\lambda_1$ and $\lambda_2$}}
In Figure \ref{fig:coeff_1} and \ref{fig:coeff_2}, we investigate the coefficients $\lambda_1$ and $\lambda_2$ calculated by entropy-guided adaptive mixture-of-LoRA in the warm-start scenario and the cold-start scenario, respectively. $\lambda_1$ and $\lambda_2$ represent the respective weights assigned to the domain-general and domain-specific LoRA module during their mixture. We observe that in the warm-start scenario, $\lambda_2$ is relatively large, reflecting a greater reliance on domain-specific LoRA. Conversely, in the cold-start scenario, the weight of $\lambda_1$ increases significantly, emphasizing the importance of domain-general knowledge. This result is reasonable and aligns with the differing requirements of these two scenarios. 
This result also demonstrates the effectiveness of our entropy-guided adaptive mixture-of-LoRA method.

\subsubsection{\textbf{Analysis of the Number of Unlabeled Test Data}}
The number of unlabeled test data is one of the hyperparameters. Figure \ref{fig:test_data} illustrates the impact of different numbers on the NDCG@1 performance. The entropy-guided learning method converges rapidly, experimental results indicate that setting the number to 50 or 100 achieves a model fusion weight with optimal performance, this configuration proves effective across the majority of experiments conducted on the Beauty, Toys, Sports and MovieLens-1M datasets.
Therefore, a limited amount of training data is sufficient to learn suitable model fusion weights, thereby significantly reducing the data requirements and computation resource costs.

\iffalse
\subsubsection{\textbf{The Impact of Language Model Size.}}
\fi
\subsubsection{\textbf{Case Study}}
We further conduct a case study to delve deeper into the recommendation results of \shortname. 
We randomly selected a user from the Movielens-1M test set, provided their historical viewing records and a candidate set, and asked both our model and ChatGPT to make movie recommendations along with explanations for their choices.
The outputs are shown in Figure \ref{fig:case}.
We find that \shortname~ successfully generalizes to the explainable recommendation task.
It accurately captured the user's preference for action movies from their historical viewing records and leveraged world knowledge to provide an accurate interpretation of the plot of Die Hard. 
In contrast, GPT-4 lacks domain-specific knowledge in the recommendation, incorrectly associating the action movie Die Hard with science fiction films like Star Wars, Alien, and Terminator, resulting in unreasonable explanation outcomes.
This case study further highlights MoLoRec's task generalization capability and its deep understanding of recommendation knowledge.

\begin{table}[!t]
\small
\centering
\caption{NDCG@1 performance in few-shot training setting on Movielens-1M.}
\vspace{-2mm}
\label{tab:few-shot}
\begin{tabular}{ccccc}
\toprule
\textbf{Scenario} & \textbf{Sample \%} & \textbf{TallRec} & \textbf{2nd Finetune} & \textbf{MoLoRec} \\
\midrule
\multirow{3}{*}{\textbf{Warm-Start}} 
& \textbf{10\%}  & 0.3957  & 0.0704 & 0.5353 \\
& \textbf{20\%}  & 0.4298  & 0.0790 & 0.5454 \\
& \textbf{30\%}  & 0.4563  & 0.0540 & 0.5498 \\
\midrule
\multirow{3}{*}{\textbf{Cold-Start}} 
& \textbf{10\%}  & 0.1091  & 0.0000 & 0.1455 \\
& \textbf{20\%}  & 0.1091  & 0.0182 & 0.1636 \\
& \textbf{30\%}  & 0.1273  & 0.0000 & 0.1636 \\
\bottomrule
\vspace{-5mm}
\end{tabular}
\end{table}





\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
    \includegraphics[width=1\textwidth,height=0.16\textheight]{figures/warm_start.pdf}
        \caption{Warm-Start Scenario}
        \label{fig:coeff_1}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[t]{0.23\textwidth}
\includegraphics[width=1\textwidth,height=0.16\textheight]{figures/cold_start.pdf}
        \centering
        %\vspace{-5pt}
        \caption{Cold-Start Scenario}
        \label{fig:coeff_2}
    \end{subfigure}
    % \caption{Impact of different $\Delta$ on model performance for the Waterbirds dataset}
    %\vspace{-10pt}
    %\caption{Analysis of debiasing at different levels of bias in CelebA dataset. The x-axis represents the proportions of each group, (blond hair, Male) : (blond hair, Female) : (black hair, Male) : (black hair, Female). The y-axis represents the values of the Bias metric.}
    \vspace{-2mm}
\caption{The coefficients $\lambda_1$ and $\lambda_2$ calculated by entropy- guided adaptive mixture-of-LoRA.}
\label{fig:ts}
    % \Description{}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
    \includegraphics[width=1\textwidth,height=0.16\textheight]{figures/test_data_warm_start.pdf}
        \caption{Warm-Start Scenario}
        \label{fig:111}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[t]{0.23\textwidth}
\includegraphics[width=1\textwidth,height=0.16\textheight]{figures/test_data_cold_start.pdf}
        \centering
        %\vspace{-5pt}
        \caption{Cold-Start Scenario}
        \label{fig:222}
    \end{subfigure}
    % \caption{Impact of different $\Delta$ on model performance for the Waterbirds dataset}
    %\vspace{-10pt}
    %\caption{Analysis of debiasing at different levels of bias in CelebA dataset. The x-axis represents the proportions of each group, (blond hair, Male) : (blond hair, Female) : (black hair, Male) : (black hair, Female). The y-axis represents the values of the Bias metric.}
    \vspace{-2mm}
\caption{Impact of the number of unlabeled test data in entropy-guided adaptive mixture-of-LoRA.}
\label{fig:test_data}
    % \Description{}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/case_3.pdf}
    \vspace{-0.4cm}
    \caption{Case study of \shortname~ and GPT-4 explainable recommendation results.}
    \vspace{-0.4cm}
    \label{fig:case}
    \Description{}
\end{figure}







