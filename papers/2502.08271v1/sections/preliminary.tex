\section{Preliminary}
In this section, we introduce key concepts underpinning our methodology. First, we cover the task formulation and instruction tuning for LLM-based recommender models. Next, we highlight the use of LoRA for parameter-efficient fine-tuning of LLMs.
\subsection{LLM-Based Recommender Models}
$\bullet$ \quad\textbf{Task Formulation.}
We mainly focus on the sequential recommendation task, which holds significant practical importance. Let $\mathcal{U}$ and $\mathcal{I}$ represent the sets of users and items, respectively. The historical interaction sequence of a user $u \in \mathcal{U}$ is denoted as $\mathcal{S}_u=\left[i_u^1, i_u^2, \ldots, i_u^L\right]$, arranged in chronological order, where $i_u \in \mathcal{I}$ and $L=|\mathcal{S}_u|$. The goal is to predict this user's next liked item $i_u^{L+1} \in \mathcal{I}$ based on the historical interactions.

\noindent$\bullet$ \quad\textbf{Instruction Tuning for LLM-Based Recommendation.}
For LLM-based recommendation, instruction tuning~\cite{wei2022finetuned} is the key step to bridge the gap between the next-word prediction objective of LLMs and the recommendation task~\cite{bao2023tallrec,kong2024customizing,10.1145/3626772.3657807}.
Formally, instruction tuning involves fine-tuning LLMs using training data organized as explicit instruction pairs $\{(\mathbf{x}_u, \mathbf{y}_u)|u \in \mathcal{U}\}$. Here, $\mathbf{x}_u$ represents a detailed textual instruction that encapsulates the interaction sequences $\mathcal{S}_u$ and the recommendation task, while $\mathbf{y}_u$ corresponds to the textual description of the predicted item $i_u^{L+1}$. The instruction fine-tuning process is guided by minimizing the following autoregressive loss function:
\begin{equation}
\mathcal{L}_\Theta^{L L M}=-\sum_{u} \sum_{t=1}^{|\mathbf{y}_u|} \log P_\Theta\left(y_u^t \mid \mathbf{y}^{<t}_u, \mathbf{x}_u\right),
\label{eq:ft}
\end{equation}
where $y^t_u$ denotes the $t$-th token of $\mathbf{y}_u$, $\mathbf{y}^{<t}_u$ is the token sequence preceding $y^t_u$, and $\Theta$ is the LLM's parameters.



\subsection{Low-Rank Adaptation~(LoRA)}
In traditional fine-tuning as described in Eqn. (\ref{eq:ft}), updating all parameters makes the process highly computationally intensive, particularly for LLMs.
To address this issue, parameter-efficient methods are designed to fine-tune LLMs while updating only a small subset of parameters.
Low-Rank Adaptation (LoRA)~\cite{hu2022lora} is the mainstream approach. LoRA addresses this issue by introducing low-rank matrices that are trained alongside the frozen original model weights. This allows the model to adapt to specific tasks by learning a small number of additional parameters, without requiring modifications to the entire model.

Specifically, for any pre-trained weight matrics $\boldsymbol{W}_0 \in \mathbb{R}^{d \times k}$ in the transformer block of the LLM, which takes an input $\boldsymbol{x} \in \mathbb{R}^k$ and output $\boldsymbol{h}$. LoRA modifies  $\boldsymbol{h} = \boldsymbol{W}_0 \boldsymbol{x}$ to:
\begin{equation}
    \boldsymbol{h} = \boldsymbol{W}_0 \boldsymbol{x} + \boldsymbol{B}\boldsymbol{A}\boldsymbol{x},
\end{equation}
where $\boldsymbol{B}\in \mathbb{R}^{d \times r}$, $\boldsymbol{A}\in \mathbb{R}^{r \times k}$ are the low-rank projection matrices. Notably, the rank $r \ll \min (d, k)$, ensuring that the number of parameters introduced by $\boldsymbol{B}\boldsymbol{A}$ is significantly fewer than those of $\boldsymbol{W}_0$, as $dr + rk \ll dk$. During fine-tuning, only $\boldsymbol{A}$ and $\boldsymbol{B}$ are updated, while $\boldsymbol{W}_0$ remains fixed. In a similar way, LoRA adapter is generally applicable to any LLM layer desired for updating. The training objective of LoRA fine-tuning can be formulated as:
\begin{equation}
\max _{\Delta\Theta} \sum_{u} \sum_{t=1}^{|\mathbf{y}_u|} \log P_{\Theta_{\text{pre}} +\Delta\Theta }\left(\mathbf{y}_u^t \mid \mathbf{y}^{<t}_u, \mathbf{x}_u\right).
\end{equation}
Here, $\Theta_{\text{pre}}$ is the parameters of the pre-trained LLM. $\Delta\Theta=\{\boldsymbol{A}^l,\boldsymbol{B}^l\}_{l=1}^L$ denotes the set of parameters of LoRA fine-tuning, and $L$ represents the number of LoRA modules.

