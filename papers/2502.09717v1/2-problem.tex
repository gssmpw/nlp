This section formalizes the carbon-aware scheduling problem and motivates insights to contextualize 
our desiderata.

\vspace{-0.5em}
\subsection{Carbon-aware DAG scheduling problem}

Each job %
is represented as a directed acyclic graph (DAG) $\mathcal{J} = \{ \mathcal{V}, \mathcal{E} \}$, where each node in $\mathcal{V}$ is one of $n$ tasks, and each edge in $\mathcal{E}$ encodes precedence constraints between tasks -- e.g., for tasks $j, j' \in \mathcal{V}$, an edge $j \to j'$ indicates that $j'$ cannot start until after $j$ has completed.
A typical data processing cluster includes $K \geq 1$ machines (or executors).  More than one job can simultaneously run on a cluster -- e.g., given a set of current jobs $\{ \mathcal{J} \}$, the scheduler assigns tasks to machines over time while respecting precedence and capacity constraints.  We index continuous time by $t \geq 0$.

The goal of a typical scheduler is \textit{performance}, e.g., in terms of throughput, utilization, and average job completion time.  In this work, we additionally consider the goal of \textit{carbon-awareness} -- with respect to a time varying carbon signal given by a function $c(t) : t \geq 1$, a carbon-aware scheduler's objective is to minimize a combination of typical metrics (i.e., job completion time) and the overall carbon footprint (on both a per-job and a global, cluster basis).

Although future values of this carbon signal are unknown to the scheduler, in the rest of the paper, we follow prior work~\cite{Lechowicz:23, Bostandoost:24} and assume that it is bounded by constants $L$ and $U$ that are known to the scheduler, where $L \leq c(t) \leq U$.  In practice, the values of $L$ and $U$ can capture e.g., short-term forecasts of the best and worst carbon conditions on a given electric grid over the next 24 or 48 hours.




\vspace{-0.5em}
\subsection{Prior work and motivation}\label{sec:motiv}

Scheduling directed acyclic graphs (DAGs), or more broadly, precedence-constrained tasks, has been extensively studied.\\ 
Classic results establish the difficulty of this problem: even in its simplest forms, DAG scheduling is NP-hard~\cite{Lenstra:78}. 
To address this, prior work has developed heuristic methods and approximation algorithms~\cite{Su:24:Tompecs, Chudak:99, Lassota:23, Li:17, Davies:20, Davies:21, Maiti:20, Su:23}, ranging from the well-known list scheduling algorithm~\cite{Graham:66}, priority-based algorithms~\cite{Sels:12:Priority}, to more complex approaches such as genetic programming~\cite{Cheng:96:Genetic, Pezzella:08:Genetic, Davis:14:Genetic}. These methods often rely on simplifying assumptions, such as fixed task durations or centralized knowledge of the task graph.  %


In recent years, DAG scheduling has become %
a key problem in \textit{data processing frameworks} such as Apache Airflow, Beam, and Spark, which use DAGs to represent workflows. 
In Spark, each node of a job's DAG corresponds to a \textit{stage}, which encapsulates operations (\textit{tasks}) that can be executed in parallel over partitions of input data. Inter-stage dependencies impose precedence constraints: a stage can only begin once all ``parent'' stages have completed. 
Frameworks such as Spark typically implement simple scheduling strategies such as first-in, first-out (FIFO) and fair-share scheduling~\cite{SparkScheduling} -- these are explainable and efficient in terms of overhead, but suboptimal in terms of job completion time.

Recent works that revisit scheduling %
for data processing have explored %
learning-based techniques, such as reinforcement learning (RL) methods that dynamically learn scheduling policies~\cite{Hongzi:2019:Decima, Wu:18, Li:23, Grinsztajn:20, Zhou:22, Islam:21}.  Although these methods outperform default policies and hand-tuned heuristics in terms of job completion time, theoretical results for these techniques have proven difficult to obtain.



Carbon awareness adds a new dimension to the DAG scheduling problem -- an online scheduler must consider the time-varying carbon intensity while choosing to assign resources to specific nodes in the job DAG(s), with an overarching goal of reducing carbon footprint, combined with traditional metrics such as job completion time -- see \autoref{fig:motivation} for an illustration of this desired behavior for \DANISH, FIFO, and optimal schedules.
As discussed above, the state-of-the-art for carbon-agnostic DAG scheduling falls into two categories: theoretical models that focus on provably near-optimal schedules under idealized assumptions, and heuristic or learning-based methods that do not provide theoretical bounds but perform well in practice.
In adding carbon-awareness to the problem, we consider a \textit{middle ground} that balances between design goals of simplicity, interpretability, configurability, and performance. 
In particular, we seek a carbon-aware scheduler that is tractable for theoretical insight, offering provable bounds on, e.g., the trade-off between carbon and job completion time while not sacrificing the efficiency gains that come from, e.g., learning DAG structure. 












