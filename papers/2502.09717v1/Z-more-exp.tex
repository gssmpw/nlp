
In this section, we give additional experiment setup details and results that are deferred from the main body (i.e., in \autoref{sec:eval})



\subsection{Deferred setup details} \label{apx:extra-details}

\subsubsection{\textbf{GreenHadoop~\cite{Goiri:2012:GreenHadoop} adaptation and implementation}} \label{apx:greenhadoop}

This implementation begins by considering green (renewable) energy and brown (non-renewable) energy values available in the carbon traces obtained from Electricity Maps~\cite{electricity-map}. The system derives a "green window" by iterating over future minutes and summing the portion of executor capacity that can be powered purely by renewable energy sources until it meets the outstanding workload. Similarly, a "brown window" is derived by computing the number of executor minutes to finish the outstanding workload, assuming the system is run at maximum executor capacity. These two windows bracket the best-case scenarios of optimizing purely for carbon and for time. 

To balance carbon usage and JCT, the system derives a final window size via a convex combination of the green and brown windows, modulated by a tunable parameter, $\theta$, which details the carbon-awareness of the system. By default, $\theta$ is set to $0.5$, which balances between $0$ (carbon-agnostic) and $1$ (fully-carbon-aware) At each scheduling decision, the system utilizes all of the available green energy in the future timesteps, and computes the fraction of available brown energy required to complete all jobs by the end of our convex window. Within that determined executor limit, tasks are dispatched using a standard FIFO queue, similar to how inter-job dependencies are managed in Hadoop. While this approach separates the carbon-aware resource provisioning from the job ordering logic, it does not embed the carbon-importance of tasks within each job. 





\subsubsection{\textbf{Differences between Spark standalone FIFO baseline and default Spark / Kubernetes behavior}} \label{apx:fifo-vs-k8s}

In the main body, in \autoref{sec:eval-proto}, we discuss some notable  differences between the \textit{baselines} in the prototype experiments and the simulator experiments.
In the simulator experiments, we use the default Spark standalone cluster scheduler (i.e., FIFO) as a baseline.  In contrast, as the prototype is implemented on top of Spark and Kubernetes, we use the combined default behavior of the Spark DAG scheduler and Kubernetes scheduler as a baseline.  


The difference between these baselines is particularly pronounced when comparing e.g., the relative carbon footprint of Decima against the primary baseline -- in the simulator, Decima's carbon footprint is a 21.5\% improvement over FIFO, while in the prototype Decima improves on the default behavior by only 1.2\% -- similar trends are evident in average job completion time.  This difference in performance can be attributed to a difference in how per-job parallelism limits are set by the FIFO scheduler in the simulated Spark standalone environment vs. the Spark/Kubernetes behavior configured on our prototype cluster.  In the Spark standalone mode of operation, the default FIFO behavior assigns up to $N$ executors to each stage of a job, where $N$ is the number of tasks within said stage.  In contrast, our Spark/Kubernetes cluster scheduler is configured to assign up to $25$ executors across all stages of any job to avoid an issue where executors from previously completed stages continue to use cluster resources, causing Spark to hang.  

\begin{figure*}[h]
    \vspace{-1em}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fifo_vs_k8s_timeseries.png} \vspace{-1em}
    \caption{ Executor usage (left) and number of jobs in the system (right) for an identical group of 50 TPC-H jobs in both the simulator (FIFO) and prototype (Spark / Kubernetes default) clusters, both with a maximum of 100 executors. }
    \label{fig:fifo-vs-k8s-timeseries}
\end{figure*}

In \autoref{fig:fifo-vs-k8s-timeseries}, we illustrate the granular effect of these different policies for an experiment of 50 TPC-H jobs with identical ordering and identical interarrival times in the simulator and prototype, respectively.  The left-hand side plots the number of busy executors, while the right-hand side plots the number of jobs in the system.  Note that the number of busy executors in the Spark / Kubernetes prototype more frequently drops below 100, particularly when the number of jobs in the system is low (e.g., between 1000 - 1200 seconds).  This corresponds to the executor cap configured for the Spark / Kubernetes default behavior, which results in more efficient executor usage and reduced blocking (i.e., when a job enters the system, it is more likely that there will be free executors to work on it).  As a result, the Spark / Kubernetes behavior improves on the Spark standalone FIFO scheduler by 18.8\% in carbon footprint and 22.1\% in average job completion time in an experiment with 50 TPC-H jobs, mirroring the broader trends observed in our simulator and prototype experiments.  


\subsection{Deferred experiments} \label{apx:exp}

In this section, we present the results of additional experiments in the simulator and the prototype that are omitted from the main body due to space considerations.


\subsubsection{\textbf{Impact of total number of jobs}}

In the main results presented in \autoref{sec:eval}, we evaluate the performance of tested algorithms under experiments with 25, 50, and 100 continuously arriving jobs.  We explore the impact of varying this number of jobs in each experiment below, with \DANISH and \CAP configured to be moderately carbon-aware, in the \verb|DE| grid region.

\autoref{fig:num-jobs-sim} plots the average carbon reduction, end-to-end completion time, and average per-job completion time with respect to FIFO for \DANISH, Decima, and \CAP on top of FIFO in the simulator environment, using experiments with 12, 25, 50, 100, and 200 jobs.  We find that the relative ordering of all three techniques largely stays constant, although measuring results for a small number of jobs (e.g., 12, 25) is intuitively prone to more variance as the end-to-end results are more easily impacted by e.g., one or two random jobs.  Out of the three metrics, carbon footprint is the most stable.  As the number of jobs increases, all metrics ``converge'' to some average behavior.  One interesting exception is \CAP-FIFO -- due to the ``blocking'' behavior of the FIFO scheduler in the simulator that is more prone to queue build-up (e.g., see \autoref{apx:fifo-vs-k8s}), a larger total number of jobs results in longer job completion times for \CAP-FIFO.


\begin{figure*}[h]
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/sim/num-jobs-v-carbon-sim.png} \vspace{-1em}
    {\centering \textbf{\textit{(a)}}}
\end{minipage} \hfill
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/sim/num-jobs-v-time-sim.png} \vspace{-1em}
    {\centering \textbf{\textit{(b)}}}
\end{minipage} \hfill
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/sim/num-jobs-v-per-job-time-sim.png} \vspace{-1em}
    {\centering \textbf{\textit{(c)}}}
\end{minipage}
\caption{  \textbf{\textit{(a)}} Carbon reduction, \textbf{\textit{(b)}} end-to-end completion time, and \textbf{\textit{(c)}} average job completion time achieved by \DANISH, \CAP-FIFO, and Decima (relative to FIFO) in a single grid region for varying experiment sizes.  Shaded regions denote the standard deviation across the entire carbon trace.  } \label{fig:num-jobs-sim}
\end{figure*}
\begin{figure*}[h]
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/proto/num-jobs-v-carbon-proto.png} \vspace{-1em}
    {\centering \textbf{\textit{(a)}}}
\end{minipage} \hfill
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/proto/num-jobs-v-time-proto.png} \vspace{-1em}
    {\centering \textbf{\textit{(b)}}}
\end{minipage} \hfill
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/proto/num-jobs-v-per-job-time-proto.png} \vspace{-1em}
    {\centering \textbf{\textit{(c)}}}
\end{minipage} 
\caption{ \textbf{\textit{(a)}} Carbon reduction, \textbf{\textit{(b)}} end-to-end completion time, and \textbf{\textit{(c)}} average job completion time achieved by \DANISH, \CAP, and Decima (relative to the Spark/Kubernetes default) in a single grid region for varying experiment sizes.  Shaded regions denote the standard deviation across 10 random trials. } \label{fig:num-jobs-proto}
\end{figure*}

In \autoref{fig:num-jobs-proto}, we plot the same metrics with respect to the default Spark/Kubernetes scheduler in the prototype implementation for \DANISH, Decima, and \CAP, using experiments with 25, 50, and 100 jobs.  These results generally mirror those shown in the simulator above, although \CAP implemented on top of the default Spark/Kubernetes behavior does not exhibit the same increase in per-job completion time for larger experiment sizes that \CAP-FIFO does in the simulator -- this is because the blocking behavior of FIFO is more pronounced than in the default Spark/Kubernetes scheduler (e.g., see \autoref{apx:fifo-vs-k8s}).


\subsubsection{\textbf{Impacts of submission rate}}
In the main results presented in \autoref{sec:eval}, we evaluate the performance of tested algorithms under continuous job arrivals following a Poisson process with an average interarrival time of $\nicefrac{1}{\lambda} = 30$ minutes ($30$ seconds in real time).  In what follows, we explore the impact of varying this interarrival time below, where note that smaller interarrival times imply that the cluster is more heavily utilized.  \DANISH and \CAP algorithms are both configured to be moderately carbon-aware, and the tested grid region is \verb|DE|.

In \autoref{fig:lambda-sim}, we plot the average carbon reduction, end-to-end completion time, and average per-job completion time with respect to FIFO for \DANISH, Decima, and \CAP-FIFO in the simulator environment.  We find that the relative ordering of algorithm performance remain largely the same, but differences arise particularly for small interarrival times -- in these scenarios where the cluster is more heavily utilized, we find that both \DANISH and Decima benefit from more intelligent scheduling decisions with respect to FIFO, which manifests in higher carbon reductions relative to FIFO and lower end-to-end completion times.


\begin{figure*}[h]
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/sim/carbon-v-lambda-sim.png} \vspace{-1em}
    {\centering \textbf{\textit{(a)}}}
\end{minipage} \hfill
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/sim/time-v-lambda-sim.png} \vspace{-1em}
    {\centering \textbf{\textit{(b)}}}
\end{minipage}\hfill
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/sim/per-job-time-v-lambda-sim.png} \vspace{-1em}
    {\centering \textbf{\textit{(c)}}}
\end{minipage}
\caption{ \textbf{\textit{(a)}} Carbon reduction, \textbf{\textit{(b)}} end-to-end completion time, and \textbf{\textit{(c)}} average job completion time achieved by \DANISH, \CAP-FIFO, and Decima (relative to FIFO) in a single grid region for varying Poisson interarrival times.  Shaded regions denote the standard deviation across the entire carbon trace. } \label{fig:lambda-sim}
\end{figure*}

In \autoref{fig:lambda-proto}, we plot the same metrics with respect to the default Spark/Kubernetes scheduler in the prototype implementation for \DANISH, Decima, and \CAP.  These results largely mirror those shown in the simulator above -- the most notable change is the improved performance of \DANISH and Decima (in terms of both end-to-end completion time and carbon footprint) for small interarrival times, i.e., when the cluster is heavily utilized.


\begin{figure*}[h]
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/proto/carbon-v-lambda-proto.png} \vspace{-1em}
    {\centering \textbf{\textit{(a)}}}
\end{minipage} \hfill
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/proto/time-v-lambda-proto.png} \vspace{-1em}
    {\centering \textbf{\textit{(b)}}}
\end{minipage}\hfill
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/proto/per-job-time-v-lambda-proto.png} \vspace{-1em}
    {\centering \textbf{\textit{(c)}}}
\end{minipage}
\caption{ \textbf{\textit{(a)}} Carbon reduction, \textbf{\textit{(b)}} end-to-end completion time, and \textbf{\textit{(c)}} average job completion time achieved by \DANISH, \CAP, and Decima (relative to the Spark/Kubernetes default) in a single grid region for varying Poisson interarrival times.  Shaded regions denote the standard deviation across 10 random trials. } \label{fig:lambda-proto}
\end{figure*}


\subsubsection{\textbf{Carbon-awareness logic latency}}
The logic of \DANISH and \CAP naturally introduce latency due to the overhead required to make carbon-aware decisions.  We quantify this \textit{scheduling overhead} for FIFO, Decima, \CAP-FIFO, and \DANISH in the simulator below, in  the \verb|DE| grid region over 1000 simulated trials.  Note that we  report the latency in real-time (i.e.,  without the scaling mentioned  in \autoref{sec:carbon-traces}).  These measurements do not include the latency of e.g., calls to an external carbon intensity API.

In \autoref{fig:latency}(a), we plot the average latency (in milliseconds) of invoking each scheduler once, given that there is exactly $\{1, 5, 10, 25, 50, 75, 100\}$ outstanding TPC-H jobs in the queue.  We find that simple decision rule policies (FIFO and \CAP-FIFO) exhibit consistently low latencies below 5 milliseconds, where \CAP adds an overhead of a few milliseconds.  In contrast, Decima and \PCAPS, which both use a GNN+RL model to run inference at each invocation, intuitively exhibit latency that grows as a function of the number of outstanding jobs.  Relative to Decima, \PCAPS adds an overhead of a few milliseconds at each invocation, and this overhead is constant (i.e., it does not grow with the number of jobs in the queue).

In \autoref{fig:latency}(b), we plot the average latency over a full experiment, where the initial number of jobs in the queue is one of $\{1, 5, 10, 25, 50, 75, 100\}$ -- note that this latency goes down over time as jobs are completed.  It is computed as a ratio between the total time spent in the scheduler and the number of scheduler invocations over the experiment length.  The findings in this metric are largely similar, with lower overall averages due to the averaging over a full experiment (as opposed to averaging over a single queue length).
Overall, these results imply that the latency impact of carbon-awareness is minimal -- in the context of long-running big-data workloads, the sub-20 millisecond latencies observed are likely to be insignificant compared to other overheads on the cluster.

\begin{figure*}[h]
\hfill 
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/sim/latency-v-queue-length.png} \vspace{-1em}
    {\centering \textbf{\textit{(a)}}}
\end{minipage} \hfill
\begin{minipage}{0.32\linewidth}
        \centering
    \includegraphics[width=\linewidth]{figures/sim/latency-v-num-jobs.png} \vspace{-1em}
    {\centering \textbf{\textit{(b)}}}
\end{minipage} 
\hfill \hfill
\caption{  \textbf{\textit{(a)}} Average latency with $N$ jobs in the queue and \textbf{\textit{(b)}} average normalized time  in the scheduler for \PCAPS, \CAP-FIFO, Decima, and FIFO in a single grid region for varying experiment sizes.  Shaded region denotes the standard deviation across all 1000 trials. } \label{fig:latency}
\end{figure*}




