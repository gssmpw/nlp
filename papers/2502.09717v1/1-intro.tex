Concerns about the climate impact of machine learning (ML) and artificial intelligence (AI) have primarily revolved around the carbon footprint during the training phase~\cite{Hanafy:23:CarbonScaler, Wiesner:2021:WaitAwhile} or, in some cases, the inference phase~\cite{Baolin:2023:Clover} of their life cycle. 
However, as the data requirements of foundation models have ballooned, the \textit{data processing} tasks that must be completed before training %
account for almost one-third of the cumulative computation for an AI model during its life cycle~\cite{WU:2022:SustainableAI}. 
Furthermore, foundation model \textit{finetuning} generally trains a model on a narrower data set that may require additional data processing~\cite{Mosbach:21:FinetuningBERT} -- as the finetuning of general purpose models (e.g., Llama) for specific tasks~\cite{Liu:24:TaskLLMFinetuning, Lin:24:DataEfficientFinetuning} has gained traction, the comparative fraction of computational demand borne by data processing tasks is expected to grow.

Therefore, efforts towards responsible and sustainable development in AI must consider and optimize the carbon footprint of data processing. 
Even beyond sustainability, companies such as Microsoft have implemented \textit{internal carbon pricing} for short- and long-term decisions~\cite{Microsoft:19, McKinsey:21} that put financial responsibility on business divisions for each metric ton of operational CO$_2$ that they emit.  In the data center context, most current schedulers do not consider the time-varying aspect of carbon intensity and the resulting compute-carbon impact -- this must change to accommodate additional operational concerns such as carbon pricing. 


Data processing frameworks (e.g., Apache Spark) ingest workloads that are composed of \textit{precedence-constrained tasks}, where e.g., the outputs of one operation are the inputs to another~\cite{Zaharia:12}.  
There is a rich literature studying scheduling algorithms for this case of precedence-constrained tasks (e.g., represented as a directed acyclic graph (DAG)) that characterize large-scale data processing.  From the theoretical side, optimal scheduling of precedence-constrained tasks (in terms of total completion time) is known to be NP-hard~\cite{Lenstra:78}.  Although there has been progress in approximation techniques~\cite{Su:24:Tompecs, Chudak:99, Lassota:23, Li:17, Davies:20, Davies:21, Maiti:20, Su:23}, the hardness of the problem necessitates simple settings with relatively strong assumptions. From an experimental perspective, there have been several studies proposing data-driven and/or evolutionary approaches for scheduling, both in the general precedence-constrained tasks case and the specific data processing case~\cite{Hongzi:2019:Decima, Wu:18, Li:23, Grinsztajn:20, Zhou:22, Cheng:96:Genetic, Pezzella:08:Genetic, Davis:14:Genetic, Islam:21}.  In recent years, such works have leveraged learning techniques such as graph neural networks (GNNs) and reinforcement learning (RL) to learn an improved scheduling policy, showing significant improvements in experiments.  However, owing to the complexities of these approaches, theoretical guarantees for learning-based approaches have proven difficult to obtain.




Beyond the singular objective of job completion time, a select few works have considered settings that are closer to the carbon-aware problem we study in this paper~\cite{Su:24:Tompecs, Goiri:2012:GreenHadoop, Liu:23}.  These \textit{multi-objective} scheduling environments balance the objectives of e.g., reducing job completion time alongside another metric of interest, such as cost.  For instance, several works have considered energy efficiency in tandem with job completion time, from both theoretical and experimental perspectives.  \citet{Su:24:Tompecs} study \textit{energy-aware} list scheduling for precedence constrained tasks, %
giving theoretical bounds for an combined objective of energy consumption and performance.
GreenHadoop~\cite{Goiri:2012:GreenHadoop} is a MapReduce framework for data centers with local renewable sources that predicts the future availability of carbon-free (``green'') electricity and schedules jobs accordingly, subject to deadlines for individual jobs.  
\citet{Liu:23} consider job scheduling for low-carbon data center operation in a general model with both DAG and non-DAG jobs -- they develop an RL-based scheduler that focuses primarily on increasing energy-efficiency.

Despite these previous works, focusing on \textit{carbon-efficiency} rather than energy-efficiency requires different techniques.  In particular, while carbon-efficiency and energy-efficiency are sometimes complementary objectives, they are often contradictory~\cite{Hanafy:23} -- for instance, due to the \textit{time-varying} nature of carbon intensity, it may be advantageous to scale up during low-carbon periods (i.e., sacrificing energy efficiency) in exchange for the ability to scale down during high-carbon periods.
Works that \textit{do} consider carbon emissions (e.g., GreenHadoop) use abstractions, such as job-level deadlines and ``green'' vs. ``brown'' energy, that do not adequately model the current m.o. in data centers.

To address this multi-objective 
setting while catering to realistic scenarios, we propose that a middle-ground approach is needed -- namely, by drawing on techniques from the theoretical literature for precedence-constrained and carbon-aware scheduling, and simultaneously considering experimental advances, we seek a simple and interpretable framework that comes with guarantees in terms of the \textit{trade-off} between job completion time and carbon savings.



In this paper, we propose \PCAPS
(\textbf{P}recedence- and \textbf{C}arbon-\textbf{A}ware \textbf{P}rovisioning and \textbf{S}cheduling), a carbon-aware scheduler for data processing clusters.  \PCAPS is theoretically-inspired, leveraging a paradigm of interpretable and configurable \textit{threshold-based design} that informs decisions at each time step based on e.g., the current carbon intensity and/or carbon price.  In keeping with this inspiration, we give analytical results that characterize the trade-off between job completion time and carbon savings.
\PCAPS is also practically relevant, drawing on recent insights from ML-based DAG schedulers (e.g., Decima~\cite{Hongzi:2019:Decima}, LACHESIS~\cite{Zhou:22}, and others~\cite{Wu:18, Li:23, Grinsztajn:20}).  By interfacing with a score or probability distribution over available tasks, \PCAPS defines a notion of \textit{relative importance} (i.e., compared to other tasks) -- this allows it to make fine-grained carbon-aware decisions that take the DAG's structure into account, such as continuing to schedule bottleneck tasks even if carbon intensity is high.  See \autoref{sec:danish-design} for a formal description of \PCAPS's design.

As a simplification of \PCAPS, we additionally propose and study \CAP (\textbf{C}arbon-\textbf{A}ware \textbf{P}rovisioning), which takes the provisioning ideas of \PCAPS and generalizes them to interoperate with any carbon-agnostic scheduler.  
Without explicitly considering inter-task dependencies, \CAP changes the resources available to the cluster, capturing an intuition that clusters should \textit{throttle down} during high-carbon periods and vice versa~\cite{Hanafy:23:CarbonScaler, radovanovic2022carbon} -- see \autoref{sec:cap-design} for a description.





\begin{figure*}[t]
    \includegraphics[width=\linewidth]{figures/motivation/motivation.png}
    \vspace{-1em}
    \caption{ Four scheduling policies for a motivating DAG and 18-hour-long carbon intensity trace (on the left hand side).  
    Compared to a carbon-agnostic FIFO scheduler, the time-optimal approach (\texttt{T-OPT}) prioritizes starting the green and purple stages early to reduce completion time.  A carbon-aware-optimal approach (\texttt{C-OPT}) with a \textit{deadline} to finish the DAG within 18 hours reduces carbon emissions by 51.2\%, at the expense of increasing time by 28.5\% compared to FIFO.  
    By prioritizing green and purple stages during high-carbon periods, \PCAPS reduces carbon emissions by 23.1\% and still completes the job 7\% earlier compared to FIFO.} 
    \label{fig:motivation} \vspace{-1em}
\end{figure*}

We have implemented \PCAPS and \CAP as modules for Spark on Kubernetes and as extensions for a high-fidelity simulator~\cite{Hongzi:2019:Decima}.  Our experiments consider real and synthetic data processing workloads from Alibaba traces and TPC-H~\cite{TPCH:18, Alibaba:18}, alongside real carbon intensity traces from six power grids~\cite{electricity-map}.  
In our prototype implementation, we evaluate \PCAPS and \CAP on a 100-node Spark cluster.
We report the impact of carbon-aware policies on both job completion time and carbon savings, showing that \PCAPS and \CAP's configurable design enables notable carbon reduction for mild increases in \textit{end-to-end completion time}, which measures the total time to complete all jobs in a given experiment, measuring the system's overall throughput and efficiency.
We summarize our key contributions as follows:

\begin{enumerate}[itemsep=0.1cm]
    \item \PCAPS, 
    a carbon-aware scheduler that interfaces with a probability distribution over stages of a DAG, such as those provided by ML schedulers.  \PCAPS incorporates carbon into decisions that arbitrage between stages of a job at a granular level, obtaining a favorable trade-off between carbon savings and job completion time.
    \item \CAP, %
    a carbon-awareness module that dynamically adjusts cluster resources without replacing an existing scheduler (see \autoref{sec:cap-design}).  Compared to \PCAPS, it obtains a worse trade-off between carbon and completion time in exchange for flexibility and ease of implementation.
    \item  We analyze the \textit{carbon stretch factor} for \PCAPS and \CAP, which bounds the increase in job completion time due to carbon-aware actions (e.g., see \sref{Theorems}{thm:ksMakespan} and \ref{thm:danishMakespan}).
    \item  We implement \PCAPS and \CAP as extensions for a high-fidelity Spark simulator, alongside proof-of-concept prototypes for Spark on Kubernetes (see \autoref{sec:impl}).  We evaluate our proposed carbon-aware schedulers against baselines and a state-of-the-art ML scheduler (see \autoref{sec:eval}). \blfootnote{Our experiment code is available at \url{https://github.com/umass-solar/carbon-aware-dag/}.}
\end{enumerate}


