\section{Prior work and motivation}
\label{sec:motiv}

Scheduling directed acyclic graphs (DAGs), or more broadly, precedence-constrained tasks, has been extensively studied.\\ 
Classic results establish the difficulty of this problem: even in its simplest forms, DAG scheduling is NP-hard~\cite{Lenstra:78}. 
To address this, prior work has developed heuristic methods and approximation algorithms~\cite{Su:24:Tompecs, Chudak:99, Lassota:23, Li:17, Davies:20, Davies:21, Maiti:20, Su:23}, ranging from the well-known list scheduling algorithm~\cite{Graham:66}, priority-based algorithms~\cite{Sels:12:Priority}, to more complex approaches such as genetic programming~\cite{Cheng:96:Genetic, Pezzella:08:Genetic, Davis:14:Genetic}. These methods often rely on simplifying assumptions, such as fixed task durations or centralized knowledge of the task graph.  %


In recent years, DAG scheduling has become %
a key problem in \textit{data processing frameworks} such as Apache Airflow, Beam, and Spark, which use DAGs to represent workflows. 
In Spark, each node of a job's DAG corresponds to a \textit{stage}, which encapsulates operations (\textit{tasks}) that can be executed in parallel over partitions of input data. Inter-stage dependencies impose precedence constraints: a stage can only begin once all ``parent'' stages have completed. 
Frameworks such as Spark typically implement simple scheduling strategies such as first-in, first-out (FIFO) and fair-share scheduling~\cite{SparkScheduling} -- these are explainable and efficient in terms of overhead, but suboptimal in terms of job completion time.

Recent works that revisit scheduling %
for data processing have explored %
learning-based techniques, such as reinforcement learning (RL) methods that dynamically learn scheduling policies~\cite{Hongzi:2019:Decima, Wu:18, Li:23, Grinsztajn:20, Zhou:22, Islam:21}.  Although these methods outperform default policies and hand-tuned heuristics in terms of job completion time, theoretical results for these techniques have proven difficult to obtain.



Carbon awareness adds a new dimension to the DAG scheduling problem -- an online scheduler must consider the time-varying carbon intensity while choosing to assign resources to specific nodes in the job DAG(s), with an overarching goal of reducing carbon footprint, combined with traditional metrics such as job completion time -- see \autoref{fig:motivation} for an illustration of this desired behavior for \DANISH, FIFO, and optimal schedules.
As discussed above, the state-of-the-art for carbon-agnostic DAG scheduling falls into two categories: theoretical models that focus on provably near-optimal schedules under idealized assumptions, and heuristic or learning-based methods that do not provide theoretical bounds but perform well in practice.
In adding carbon-awareness to the problem, we consider a \textit{middle ground} that balances between design goals of simplicity, interpretability, configurability, and performance. 
In particular, we seek a carbon-aware scheduler that is tractable for theoretical insight, offering provable bounds on, e.g., the trade-off between carbon and job completion time while not sacrificing the efficiency gains that come from, e.g., learning DAG structure.