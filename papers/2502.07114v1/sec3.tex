
\section{Assumptions and Asymptotic Normality}\label{sec:3}

In this section, we introduce assumptions and present the asymptotic normality guarantee for~sketched Newton methods. Throughout the paper, we let $\mF_t = \sigma(\{\xi_i\}_{i=0}^t)$, for any $t\geq 0$, be the filtration~of~$\sigma$-algebras generated by the sample sequence $\xi_0, \xi_1,\xi_2\ldots$.

\subsection{Assumptions}\label{sec:3.1}

We first impose a Lipschitz continuity condition on the objective Hessian $\nabla^2  F(\bx)$, which is~standard in existing literature \citep{Bercu2020Efficient, Cenac2020efficient, Na2022Statistical}.

\begin{assumption}\label{ass:1}
We assume $F(\bx)$ is twice continuously differentiable and its Hessian $\nabla^2F(\bx)$~is~$\Upsilon_{L}$-Lipschitz continuous. In particular, for any $\bx$ and $\bx^{\prime}$, we have
\begin{equation*}
\|\nabla^2 F(\bx) - \nabla^2 F(\bx^{\prime})\|\leq \Upsilon_{L}\|\bx-\bx^{\prime}\|.
\end{equation*}
\end{assumption}


The next assumption regards the noise in stochastic gradients.~We assume that the fourth~conditional moment of the gradient noise satisfies a growth condition. This assumption aligns with~existing literature on covariance estimation for SGD \citep{Chen2020Statistical, Zhu2021Online}.


\begin{assumption}\label{ass:2}

We assume the function $f(\bx;\xi)$ is twice continuously differentiable with respect~to $\bx$ for any $\xi$, and $\|\nabla f(\bx;\xi)\|$ is uniformly integrable for any $\bx$. This implies \mbox{$\mathbb{E}[\bar{g}_t\mid \mathcal{F}_{t-1}]=\nabla F_t$}.~Furthermore, there exist constants $C_{g,1}, C_{g,2}>0$ such that
\begin{equation}\label{ass:2:4th}
\mE[\|\bar{g}_t - \nabla F_t\|^4\mid\mathcal{F}_{t-1}]\leq C_{g,1}\|\bx_t-\bx^\star\|^4 + C_{g,2}, \quad\quad \forall t\geq 0.
\end{equation}
\end{assumption}


The above growth condition is weaker than the bounded moment condition $\mE[\|\bar{g}_t-\nabla F_t\|^4\mid~\mF_{t-1}]\leq C$ assumed for the plug-in estimator in \cite{Na2022Statistical}. In fact, the bounded fourth moment can be relaxed to a bounded $(2+\epsilon)$-moment for establishing asymptotic normality of SGD~and~Newton methods, but it is widely imposed for limiting covariance estimation. 


The next assumption imposes lower and upper bounds for stochastic Hessians, with a growth~condition on the Hessian noise.

\begin{assumption}\label{ass:3}
There exist constants $\Upsilon_H>\gamma_H>0$ such that for any $\xi$ and any $\bx$,
\begin{equation}\label{ass:3:a}
\gamma_H\leq\lambda_{\min}(\nabla^2 f(\bx;\xi))\leq \lambda_{\max}(\nabla^2 f(\bx;\xi))\leq \Upsilon_H,
\end{equation}
which implies $\mathbb{E}[\bar{H}_t\mid \mathcal{F}_{t-1}]=\nabla^2 F_t$. Furthermore, there exist constants $C_{H,1}, C_{H,2}>0$ such that
\begin{equation}\label{ass:3:4th}
\mE\left[\|\bar{H}_t-\nabla^2 F_t\|^4\mid\mF_{t-1}\right]\leq C_{H,1}\|\bx_t-\bx^\star\|^4 + C_{H,2}, \quad\quad \forall t\geq 0.
\end{equation}

\end{assumption}

The condition \eqref{ass:3:a} is widely used in the literature on stochastic second-order methods \citep{Byrd2016Stochastic, Berahas2016Multi, Moritz2016Linearly}.~By the averaging structure of~$B_t$,~we~know~\eqref{ass:3:a}~implies
\begin{equation}\label{ass:3:c2}
\gamma_H\leq\lambda_{\min}(B_t)\leq\lambda_{\max}(B_t)\leq\Upsilon_H.
\end{equation}
Moreover, as shown in \cite[Lemma 3.1]{Chen2020Statistical}, the condition \eqref{ass:3:a} together with the~bounded $\mE[\|\nabla f(\bx^\star;\xi)\|^4]$ implies \eqref{ass:2:4th}.~The growth condition on the Hessian noise in \eqref{ass:3:4th} is analogous~to~that~on the gradient noise in \eqref{ass:2:4th}. 


We finally require the following assumption regarding the sketching distribution.

\begin{assumption}\label{ass:4}
For $t\geq 0$, we assume the sketching matrix $S_{t,j}\stackrel{iid}{\sim}S$ satisfies 
$\mE[B_tS(S^TB_t^2S)^{\dagger}S^TB_t \mid \mF_{t-1}]\succeq \gamma_S I$ and $\mE[\|S\|^2\|S^{\dagger}\|^2]\leq \Upsilon_S$ for some constants $\gamma_S, \Upsilon_S>0$.

\end{assumption}


The above two expectations are taken over the randomness of the sketching matrix $S$. The lower bound of the projection matrix $B_tS(S^TB_t^2S)^{\dagger}S^TB_t$ is commonly required by sketching solvers to ensure convergence \citep{Gower2015Randomized}. We trivially have $\gamma_S\leq 1$. The~bounded~second moment of the condition number of $S$ is necessary to analyze the difference between two projection matrices, $\|B_tS(S^TB_t^2S)^{\dagger}S^TB_t-B^{\star}S(S^T(B^{\star})^2S)^{\dagger}S^TB^{\star}\|$ \cite[Lemma 5.2]{Na2022Statistical}.~Under \eqref{ass:3:c2}, both conditions easily hold for various sketching distributions, such as \mbox{Gaussian}~\mbox{sketching}~$S\sim \mN(\0, \Sigma)$ and Uniform sketching $S\sim \text{Unif}(\{\be_i\}_{i=1}^d)$, where $\be_i$ is the $i$-th canonical basis of $\mR^d$ (called randomized Kaczmarz method \citep{Strohmer2008Randomized}).



\subsection{Almost sure convergence and asymptotic normality}\label{sec3:subsec2}

We review the almost sure convergence and asymptotic normality of the sketched Newton method as established in \cite[Theorems 4.7 and 5.6]{Na2022Statistical}. We emphasize that our growth~conditions~on the gradient and Hessian noises are weaker than those assumed in \cite{Na2022Statistical}, and by a sharper analysis, our assumption on $\chi_t$ is relaxed from $\chi>1$ to $\chi>0.5(\beta+1)$. We~show that all their results still hold, with proofs deferred to Appendix \ref{pf:sec3:subsec2} for completeness.$\hskip3cm$


\begin{theorem}[Almost sure convergence]\label{sec3:thm1}

Consider the iteration scheme \eqref{nequ:7}. Suppose Assumptions \ref{ass:1} -- \ref{ass:4} hold, the number of sketches satisfies $\tau\geq \log(\gamma_H/4\Upsilon_H)/\log \rho$ with $\rho=1-\gamma_S$,~and~the~stepsize parameters satisfy $\beta\in(0.5,1]$, $\chi>0.5(\beta+1)$, and $c_{\beta}, c_{\chi}>0$. Then, we have $\bx_t\rightarrow \tx$ as~$t\rightarrow\infty$ almost surely.

\end{theorem}


To present the normality result, we need to introduce some additional notation.~Let~\mbox{$B^\star = \nabla^2F(\tx)$}. For $S_1,\ldots,S_\tau\stackrel{iid}{\sim}S$, we define the product of $\tau$ projection matrices as
\begin{equation}\label{exp:Cstar}
\tilde{C}^\star = \prod_{j=1}^\tau(I - B^\star S_j(S_j^T(B^\star)^2S_j)^\dagger S_j^TB^\star)
\end{equation}
and let $C^\star=\mE[\tilde{C}^\star]$. Then, we denote the eigenvalue decomposition of $I-C^\star$ as
\begin{equation}\label{eigendecomp}
I-C^\star = U\Sigma U^T\quad\quad\text{with}\quad\quad \Sigma=\text{diag}(\sigma_1,\dots,\sigma_d).
\end{equation}
We also define
\begin{equation}\label{exp:Omegastar}
\Omega^\star = (B^\star)^{-1}\mE[\nabla f(\bx^\star;\xi)\nabla^T f(\bx^\star;\xi)](B^\star)^{-1}.
\end{equation} 

With the above notation, we have the following normality guarantee for the scheme \eqref{nequ:7}.


\begin{theorem}[Asymptotic normality]\label{sec3:thm2}

Suppose Assumptions \ref{ass:1} -- \ref{ass:4} hold, the number of~sketches satisfies $\tau\geq \log(\gamma_H/4\Upsilon_H)/\log \rho$ with $\rho = 1-\gamma_S$, and the~stepsize parameters satisfy~$\beta\in(0.5,1]$,~$\chi>1.5\beta$, and $c_{\beta}>1/\{1.5(1-\rho^\tau)\}$ for $\beta=1$. Then, we have 
\begin{equation*}
\sqrt{1/\baralpha_t}(\bx_t-\bx^\star)\stackrel{d}{\longrightarrow}\mN(\mathbf{0},\Xi^\star),
\end{equation*}
where $\Xi^{\star}$ is the solution to the following Lyapunov equation:
\begin{equation}\label{equ:lyap}
\rbr{\cbr{1 - \frac{\b1_{\{\beta=1\}}}{2c_\beta} }I - C^\star}\Xi^{\star} + \Xi^{\star}\rbr{\cbr{1 - \frac{\b1_{\{\beta=1\}}}{2c_\beta}}I - C^\star} = \mathbb{E}\big[(I-\widetilde{C}^{\star}) \Omega^{\star}(I-\widetilde{C}^{\star})^T\big].
\end{equation}
\end{theorem}


In fact, the limiting covariance $\Xi^{\star}$ has an explicit form as:
\begin{equation}\label{exp:Xistar}
\Xi^\star = U\big(\Theta\circ U^T\mE[(I-\tilde{C}^*)\Omega^\star(I-\tilde{C}^*)^T]U\big)U^T\quad\text{with}\quad [\Theta]_{k,l}= \frac{1}{\sigma_k + \sigma_l - \b1_{\{\beta=1\}}/c_\beta},
\end{equation}
where $\circ$ denotes the matrix Hadamard product. There exists a degenerate case. When the~Newton~systems are exactly solved ($\tau=\infty$), then $\tilde{C}^\star = C^\star = \0$, $\Sigma=I$, and $\Xi^\star = \Omega^\star/(2- \b1_{\{\beta=1\}}/c_\beta)$.~In~this case, we have $\Xi^\star = \Omega^\star/2$ for $\beta\in(0.5,1)$ and $\Xi^\star = \Omega^\star$ for $\beta=c_{\beta}=1$. For the latter setup, we know $\Xi^\star = \Omega^\star$ achieves the asymptotic minimax lower bound \citep{Duchi2021Asymptotic}.




