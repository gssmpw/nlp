
\section{Online Sketched Newton Method}\label{sec:2}

At a high level, the online sketched Newton method takes the following update scheme:
\begin{equation}\label{nequ:7}
\bx_{t+1} = \bx_t + \baralpha_t \barDelta\bx_t,
\end{equation}
where $\barDelta\bx_t$ \textit{approximately} solves the Newton system $B_t\Delta\bx_t = - \nabla f(\bx_t; \xi_t)$ via the sketching~solver~(see \eqref{nequ:8}) and $\baralpha_t$ is an \textit{adaptive}, \textit{potentially random} stepsize (see \eqref{nequ:6}). 

More precisely, given the current iterate $\bx_t$, we randomly generate a sample $\xi_t\sim \P$ and obtain~the gradient and Hessian estimates:
\begin{equation*}
\bar{g}_t = \nabla f(\bx_t;\xi_t)\hskip1.3cm \text{ and }\hskip1.3cm \bar{H}_t = \nabla^2 f(\bx_t;\xi_t).
\end{equation*}
Then, we define $B_t$ to be the Hessian average using samples $\{\xi_i\}_{i=0}^{t-1}$, expressed as
\begin{equation}\label{nequ:5}
B_t = \frac{1}{t}\sum_{i=0}^{t-1} \barH_i \hskip2cm \stackrel{\text{online update}}{\Longrightarrow} \hskip0.5cm B_t = \frac{t-1}{t}B_{t-1} + \frac{1}{t}\barH_{t-1}.
\end{equation}
In this paper, we use $\bar{(\cdot)}$ to denote a random quantity that depends on the current sample $\xi_t$. Note~that the estimate $\barH_t$ is only used in the $(t+1)$-th iteration; thus $B_t$ is deterministic conditional on $\bx_t$ (this is why we do not use the notation $\bar{B}_t$).~The Hessian average is widely used~in~\mbox{Newton}~methods to accelerate the convergence rate \citep{Na2022Hessian}. In certain problems, $B_t$ can be expressed~as~the sum~of rank-1 matrices, allowing its inverse to be updated online in~a~manner similar to \eqref{nequ:5}~\citep{Bercu2020Efficient, Cenac2020efficient, Boyer2022asymptotic, Leluc2023Asymptotic}.~\mbox{However},~solving the Newton system $B_t\Delta\bx_t = -\barg_t$ for a generic stochastic function can be expensive.


We now employ the sketching solver to approximately solve $B_t\Delta\bx_t = -\bar{g}_t$. At each inner~iteration $j$, we generate a sketching matrix/vector $S_{t,j}\in\mathbb{R}^{d\times q}\sim S$ for some $q\geq 1$ and solve the subproblem:
\begin{equation}\label{sec2:equ1}
\Delta\bx_{t,j+1} = \argmin_{\Delta\bx}\|\Delta\bx-\Delta\bx_{t,j}\|^2,\quad\quad\text{ s.t. }\quad \;\; S_{t,j}^TB_t\Delta\bx = -S_{t,j}^T\bar{g}_t.
\end{equation}
In particular, we only aim to solve the sketched Newton system $S_{t,j}^TB_t\Delta\bx = -S_{t,j}^T\bar{g}_t$ at the $j$-th inner iteration, but we prefer the solution that is as close as possible to the current solution approximation $\Delta\bx_{t,j}$. The closed-form recursion of \eqref{sec2:equ1} is  ($\Delta\bx_{t,0}=\0$):
\begin{equation}\label{sec2:equ2}
\Delta\bx_{t,j+1} = \Delta\bx_{t,j} - B_tS_{t,j}(S_{t,j}^TB_t^2S_{t,j})^{\dagger}S_{t,j}^T(B_t\Delta\bx_{t,j}+\bar{g}_t),
\end{equation}
where $(\cdot)^{\dagger}$ denotes the Moore-Penrose pseudoinverse.~When we employ a sketching vector ($q=1$), the pseudoinverse reduces to the reciprocal, meaning that solving the Newton system is \textit{matrix-free} --- no matrix factorization is needed. We refer to \cite{Strohmer2008Randomized, Gower2015Randomized} for specific examples of choosing dense/sparse sketching matrices and their trade-offs.


For a deterministic integer $\tau$, we let
\begin{equation}\label{nequ:8}
\barDelta\bx_t = \Delta\bx_{t, \tau},
\end{equation}
and then we update the iterate $\bx_t$ as in \eqref{nequ:7} with a potentially random stepsize $\baralpha_t$ satisfying
\begin{equation}\label{nequ:6}
\beta_{t}\leq \baralpha_t\leq \beta_{t}+\chi_t \quad\quad\quad \text{with}\quad\quad \beta_t=\frac{c_{\beta}}{(t+1)^{\beta}} \quad\text{and}\quad \chi_t=\frac{c_{\chi}}{(t+1)^{\chi}}.
\end{equation}
The motivation for using a well-controlled random stepsize is to enhance the adaptivity of the method without compromising the asymptotic normality guarantee. Particularly, different directions~may prefer different stepsizes, so that $\baralpha_t$ depends on $\barDelta\bx_t$ and is random.
\cite{Berahas2021Sequential, Berahas2023Stochastic, Curtis2024Stochastic} have proposed various adaptive stepsize selection schemes for~\mbox{Newton}~\mbox{methods}~on~constrained problems that precisely satisfy the condition in \eqref{nequ:6}.

