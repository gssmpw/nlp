
\section{Online Covariance Matrix Estimation}\label{sec:4}

In this section, we build upon the results in Section \ref{sec:3} and construct an online estimator for~the~limiting covariance matrix $\Xi^\star$. With the covariance estimator, we are then able to perform online~\mbox{statistical}~inference, such as constructing asymptotically valid confidence intervals for model parameters. 


\subsection{Weighted sample covariance estimator}\label{sec:4.1}

Let $\varphi_t = \beta_t + \chi_t/2$ be the centered stepsize.~Our weighted sample covariance estimator is defined as$\;\;$ 
\begin{equation}\label{exp:Xihat}
\hat{\Xi}_t = \frac{1}{t}\sum_{i=1}^t \frac{1}{\varphi_{i-1}}(\bx_i-\bar{\bx}_t)(\bx_i-\bar{\bx}_t)^T\quad\quad\quad\text{with}\quad\quad\quad
\bar{\bx}_t = \frac{1}{t}\sum_{i=1}^t\bx_i.
\end{equation}
This estimator can be rewritten as
\begin{equation*}
\hat{\Xi}_t = W_t - \bv_t\bar{\bx}_t^T - \bar{\bx}_t\bv_t^T + a_t\bar{\bx}_t\bar{\bx}_t^T,
\end{equation*}
where
\begin{equation}\label{nequ:9}
W_t = \frac{1}{t}\sum_{i=1}^t \frac{1}{\varphi_{i-1}}\bx_i\bx_i^T,\quad \quad\quad\bv_t = \frac{1}{t}\sum_{i=1}^t \frac{1}{\varphi_{i-1}}\bx_i,\quad\quad\quad a_t = \frac{1}{t}\sum_{i=1}^t\frac{1}{\varphi_{i-1}}.
\end{equation}
We mention that $W_t, \bv_t, \bar{\bx}_t, a_t$ can all be updated recursively, meaning that $\hat{\Xi}_t$ can be computed~in a fully online fashion. The detailed steps are shown in Algorithm \ref{alg:2}.

\begin{algorithm}[!t]
\caption{Construction of Weighted Sample Covariance Estimator}	\label{alg:2}
\begin{algorithmic}[1]
\State\textbf{Input:} initial iterate $\bx_0$, positive sequences $\{\beta_t, \chi_t\}$, an integer $\tau>0$, $B_0 = I$;
\State\textbf{Initialize:} $W_0 = \boldsymbol{0}\in\mathbb{R}^{d\times d}$, $\bv_0 = \bar{\bx}_0 = \boldsymbol{0}\in\mathbb{R}^d$, $a_0 = 0$
\For{$t = 0,1,2,\ldots$}
\State Obtain the sketched Newton iterate $\bx_{t+1}$ and let $\varphi_t = \beta_t+\chi_t/2$;
\State Update the quantities as:
\begin{align*}
W_{t+1} & = \frac{t}{t+1}W_t + \frac{1/\varphi_t}{(t+1)}\;\bx_{t+1}\bx_{t+1}^T,\quad\quad && \bv_{t+1}  = \frac{t}{t+1}\bv_t + \frac{1/\varphi_t}{t+1}\;\bx_{t+1};\\
\bar{\bx}_{t+1} & = \frac{t}{t+1}\bar{\bx}_t + \frac{1}{t+1}\bx_{t+1},\quad\quad && a_{t+1} = \frac{t}{t+1}a_t + \frac{1/\varphi_t}{t+1};
\end{align*}
\EndFor
\State\textbf{Output:} Covariance estimator $\hat{\Xi}_t = W_t - \bv_t\bar{\bx}_t^T - \bar{\bx}_t\bv_t^T + a_t\bar{\bx}_t\bar{\bx}_t^T$.
\end{algorithmic}
\end{algorithm}


We note that the estimator $\hat{\Xi}_t$ is in a similar flavor to batch-means covariance estimators~designed for first-order online methods. In particular, \cite{Chen2020Statistical, Zhu2021Online} grouped~SGD~iterates into multiple batches and estimated the covariance $\tOmega$ in \eqref{nequ:2} by computing the~\mbox{sample}~\mbox{covariance} among batches.~However, a significant difference from those batch-means estimators is that our~estimator $\hat{\Xi}_t$ is \textit{batch-free}.~Specifically, batch-means estimators~rely~on~\mbox{additional}~batch~size~\mbox{sequences}~that must satisfy certain conditions and largely affect both the theoretical and empirical performance~of~the estimators.~In contrast, we assign proper weights to the iterates based on the \mbox{stepsizes},~\mbox{eliminating}~the need to tune any extra parameters beyond those required by the online method itself. That \mbox{being}~said, 
we observe that the memory and computational complexities of inference based on sketched Newton methods are comparable to those of first-order methods. The memory complexity is dominated by storing $B_t$ and $W_t$, incurring a cost of $O(d^2)$, independent of the sample size $t$. The computational complexity includes $O(\tau\cdot\texttt{nnz(S)}d)$ flops for computing the sketched Newton direction and $O(d^2)$~flops for updating $\hat{\Xi}_t$. For instance, when $S\sim \text{Unif}(\{\be_i\}_{i=1}^d)$, \cite{Na2022Statistical} showed~$\tau = O(d)$, suggesting that the overall computational complexity of sketched Newton inference~is $O(d^2)$. This order precisely matches the complexity in \cite{Chen2020Statistical, Zhu2021Online}.


\begin{remark}\label{rem:1}
We compare $\hat{\Xi}_t$ with the plug-in estimator proposed in \cite{Na2022Statistical}.~Due to significant challenges in estimating sketch-related quantities in \eqref{equ:lyap}, \cite{Na2022Statistical}~simply neglected all those quantities and estimated $\tOmega/(2-\b1_{\{\beta=1\}}/c_\beta)$ instead. Their plug-in \mbox{estimator}~is~defined as:
\begin{equation}\label{exp:PI}
\tilde{\Xi}_t = \frac{1}{2 - \b1_{\{\beta=1\}}/c_\beta} \cdot B_t^{-1}\Big(\frac{1}{t}\sum_{i=1}^t\bar{g}_i\bar{g}_i^T\Big)B_t^{-1}.
\end{equation}
Comparing $\tilde{\Xi}_t$ with $\hat{\Xi}_t$, we clearly see that $\tilde{\Xi}_t$ is not matrix-free as it involves the inverse of~$B_t$~(i.e.,~$O(d^3)$ flops), which contradicts the spirit of using sketching solvers. Furthermore, $\tilde{\Xi}_t$ is a biased estimator~of $\Xi^\star$, leading to invalid confidence coverage even as $t\rightarrow\infty$.
\end{remark}



\subsection{Convergence rate of the estimator }\label{sec4:subsec1}

To establish the convergence rate of $\hat{\Xi}_t$, we first present a preparation result that provides \mbox{error}~bounds for the Newton iterate $\bx_t$ and the averaged Hessian $B_t$. We show that the fourth moments of~$\|\bx_t-\tx\|$ and $\|B_t- B^\star\|$ scale as $O(\beta_t^2+\chi_t^4/\beta_t^4)$. 
When $\chi_t \gtrsim \beta_t^{1.5}$, the error $\chi_t^4/\beta_t^4$ incurred by the adaptivity of stepsize dominates. In contrast, when $\chi_t\lesssim \beta_t^{1.5}$, adaptive stepsizes lead only to a higher-order~error.~A matching error bound (for the iterate $\bx_t$) has been established for SGD methods~with~$\chi_t=0$~\citep{Chen2020Statistical}. That said, our analysis is more involved~due~to~\mbox{higher-order}~methods,~sketching~components, and randomness in stepsizes. 


\begin{lemma}[Error bounds of $\bx_t$ and $B_t$]\label{sec4:lem1}

Suppose Assumptions \ref{ass:1} -- \ref{ass:4} hold, the \mbox{number}~of~sketches satisfies $\tau\geq \log(\gamma_H/4\Upsilon_H)/\log \rho$ with $\rho = 1-\gamma_S$, and the stepsize parameters satisfy $\beta\in(0,1)$,~$\chi>\beta$, and $c_{\beta}, c_{\chi}>0$. Then, we have
\begin{equation*}
\mE\big[\|\bx_t-\bx^\star\|^4\big]\lesssim \beta_t^2 + \frac{\chi_t^4}{\beta_t^4}\quad\quad\text{ and }\quad\quad \mE\big[\|B_t-B^\star\|^4\big]\lesssim \beta_t^2 + \frac{\chi_t^4}{\beta_t^4}.
\end{equation*}

\end{lemma}


With the above lemma, we show the convergence rate of $\hat{\Xi}_t$ in the following theorem.


\begin{theorem}\label{sec4:thm1}
Under the conditions in Lemma \ref{sec4:lem1}, except for strengthening $\chi>\beta$ to $\chi>1.5\beta$, the covariance estimator $\hat{\Xi}_t$ defined in \eqref{exp:Xihat}~satisfies
\begin{equation*}
\mE\sbr{\big\|\hat{\Xi}_t-\Xi^\star\big\|}\lesssim \left\{\begin{aligned} &\sqrt{\beta_t} + \chi_t/\beta_t^{1.5}\quad &\text{ for } 0<\beta\leq 0.5,\\
&1/\sqrt{t\beta_t} + \chi_t/\beta_t^{1.5}\quad &\text{ for } 0.5<\beta<1.
\end{aligned}\right.
\end{equation*}
\end{theorem}


Since $\sqrt{\beta_t} \vee \chi_t/\beta_t^{1.5}\rightarrow 0$ and $t\beta_t\rightarrow \infty$ as $t\rightarrow\infty$ (because $\chi>1.5\beta$), Theorem \ref{sec4:thm1} states that $\hat{\Xi}_t$ is an (asymptotically) consistent estimator of $\Xi^{\star}$. Note that $\chi>1.5\beta$ is already \mbox{required}~by~the~asymptotic normality guarantee (cf. Theorem \ref{sec3:thm2}). Similar to Lemma \ref{sec4:lem1}, $\chi\geq2\beta$ (i.e., $\chi_t\lesssim \beta_t^2$)~makes~the adaptivity error $\chi_t/\beta_t^{1.5}$ higher order.


When we suppress the sketching solver, the limiting covariance $\tXi = \tOmega/2$, meaning that $2\hat{\Xi}_t$ is a consistent estimator of $\tOmega$. Notably, this result suggests that we can estimate the optimal covariance matrix $\tOmega$ without grouping the iterates, computing the batch means, and tuning batch size sequences, which significantly differs and simplifies the estimation procedure in first-order~\mbox{methods}.~This advantage is indeed achieved by leveraging Hessian estimates; however, we preserve the computation~and memory costs as low as those of first-order methods. We defer~a~comprehensive discussion of~Theorem \ref{sec4:thm1} to Section \ref{sec:4.3}.


Theorem \ref{sec4:thm1} immediately implies the following corollary, which demonstrates the construction~of confidence intervals/regions. 


\begin{corollary}\label{sec4:cor1}

Let us set the coverage probability as $1-q$ with $q\in (0,1)$. Consider performing the online scheme \eqref{nequ:7} and computing the covariance estimator \eqref{exp:Xihat}.~Suppose Assumptions \mbox{\ref{ass:1} -- \ref{ass:4}}~hold, the number of sketches satisfies $\tau\geq \log(\gamma_H/4\Upsilon_H)/\log \rho$ with $\rho = 1-\gamma_S$, and the~stepsize parameters satisfy $\beta\in(0.5,1)$, $\chi> 1.5\beta$, and $c_{\beta},c_{\chi}>0$. Then, we have 
\begin{equation*}
P\rbr{\tx \in \E_{t,q}} \rightarrow 1 - q\quad\quad \text{ as }\quad t\rightarrow\infty,
\end{equation*}
where $\E_{t,q} = \{\bx\in\mR^d: (\bx-\bx_t)^T\hat{\Xi}_t^{-1}(\bx-\bx_t)/\baralpha_t \leq \chi_{d,1-q}^2\}$. Furthermore, for any direction $\bw\in\mR^d$,
\begin{equation*}
P\rbr{\bw^T\tx \in \sbr{\bw^T\bx_t \pm z_{1-q/2}\sqrt{\baralpha_t\cdot \bw^T\hat{\Xi}_t\bw}}} \rightarrow 1-q \quad\quad \text{ as }\quad t\rightarrow\infty.
\end{equation*}
Here, $\chi_{d,1-q}^2$ is the $(1-q)$-quantile of $\chi_d^2$ distribution, while $z_{1-q/2}$ is the $(1-q/2)$-quantile of~standard Gaussian distribution.
\end{corollary}


We would like to emphasize that the above statistical inference procedure is fully online~and~matrix-free. In particular, $\bx_t$ is updated with online nature; for confidence intervals, $\bw^T\hat{\Xi}_t\bw$ is computed~online as introduced in Section \ref{sec:4.1}; for confidence region, $\hat{\Xi}_t^{-1}$ can also be updated online:
\begin{equation*}
\hat{\Xi}_{t+1}^{-1} = \frac{t+1}{t}\hat{\Xi}_{t}^{-1} - \frac{t+1}{t}\hat{\Xi}_{t}^{-1} R_t\rbr{\Pi_t + R_t^T\hat{\Xi}_{t}^{-1}R_t}^{-1} R_t^T\hat{\Xi}_{t}^{-1},
\end{equation*}
where $R_t = \rbr{\bv_t - a_t\barx_t; \barx_t - \barx_{t+1}; \bx_{t+1} - \barx_{t+1}} \in \mR^{d\times 3}$ and $\Pi_t = (a_t, 1, 0; 1, 0, 0;0,0, t\varphi_t) \in\mR^{3\times 3}$.~See Appendix \ref{appendix:1} for the derivation of the above recursion.



\subsection{Comparison and generalization of existing studies}\label{sec:4.3}

In this section, we first compare our weighted sample covariance estimator $\hat{\Xi}_t$ with other existing~covariance estimators for both first- and second-order online methods. Then, we discuss~the~generalization of our estimator to other methods.


\vskip4pt
\noindent$\bullet$ \textbf{Plug-in estimator of online sketched Newton.} 
Recall from Remark \ref{rem:1} that, due to~the~challenges of estimating sketch-related quantities $\tilde{C}^\star$ and $C^\star$ in \eqref{equ:lyap}, a recent work \cite{Na2022Statistical} simply neglected these quantities and designed a plug-in covariance estimator $\tilde{\Xi}_t$ in \eqref{exp:PI}.~In~addition to concerns about excessive computation, \cite[Theorem 5.8]{Na2022Statistical} indicated~that for $\beta\in(0.5,1)$,
\begin{equation}\label{equ:1}
\|\tilde{\Xi}_t - \tXi\| = O(\sqrt{\beta_t \log(1/\beta_t)})+O((1-\gamma_S)^\tau).
\end{equation}
Here, the second term accounts for the oversight in estimating sketch-related quantities. It decays exponentially with the sketching steps $\tau$ but \textit{does not vanish} for any finite $\tau$. Thus, $\tilde{\Xi}_t$ is not a consistent estimator of $\tXi$. In the degenerate case where the Newton systems are solved exactly ($\tau = \infty$), $\tilde{\Xi}_t$ converges to $\tXi$ at a rate of $O(\sqrt{\beta_t \log(1/\beta_t)})$, which is faster than~that of our estimator $\hat{\Xi}_t$.~In this case, choosing between $\hat{\Xi}_t$ and $\tilde{\Xi}_t$ involves a trade-off between faster~\mbox{convergence} and computational efficiency. It is worth noting that the faster convergence of the plug-in estimator is anticipated (see \cite{Chen2020Statistical} for a comparison of plug-in and batch-means estimators in SGD~\mbox{methods}),~since~its~convergence
rate is fully tied to that of the iterates. In~\mbox{contrast},~the~\mbox{convergence} rate of our sample~covariance is additionally confined by the slow decay of correlations among the iterates.$\hskip3cm$

\vskip4pt
\noindent$\bullet$ \textbf{Batch-means estimator of SGD.} 
As introduced in Section \ref{sec:4.1}, \cite{Chen2020Statistical, Zhu2021Online} grouped SGD iterates into batches and estimated the limiting covariance $\tOmega$ by the sample covariance among batches (each batch mean is treated as one sample).~\cite{Singh2023Utility} further relaxed their conditions from increasing batch sizes to equal batch sizes. \mbox{Compared}~to~this type of estimators, our estimator $\hat{\Xi}_t$ is batch-free, requiring no additional parameters beyond those of the algorithm itself.
Furthermore, the aforementioned works all showed that the convergence rate of the batch-means estimators is $O(1/\sqrt[4]{t\beta_t})$, which is slower than that of $\hat{\Xi}_t$~in~\mbox{Theorem}~\ref{sec4:thm1}.~\mbox{Intuitively},~the~\mbox{batch-means} estimators require a long batch of iterates to obtain a single sample, while our batch-free~\mbox{estimator}~treats each individual iterate as a single sample, making it more efficient in utilizing (correlated) iterates.$\hskip2cm$


\vskip4pt
\noindent$\bullet$ \textbf{Generalization to conditioned SGD.} 
We point out that $\hat{\Xi}_t$ can also serve as a consistent~covariance estimator for conditioned SGD methods, which follow the update form \eqref{nequ:3} though~$B_t$~may~not approximate the objective Hessian $\nabla^2F_t$. \cite{Leluc2023Asymptotic} established asymptotic normality for conditioned SGD methods under the assumption of convergence of the conditioning matrix $B_t$. These methods include AdaGrad \citep{Duchi2011Adaptive}, RMSProp \citep{Tieleman2012Lecture}, and quasi-Newton methods \citep{Byrd2016Stochastic} as special cases.  
Notably, Theorem \ref{sec4:thm1} does~not~\mbox{require}~$B_t$~to~\mbox{converge}~to~the true Hessian $\nabla^2 F(\tx)$, making our analysis directly applicable to conditioned SGD methods.


\vskip4pt
\noindent$\bullet$ \textbf{Generalization to sketched Sequential Quadratic Programming.} 
We consider a constrained stochastic optimization problem:
\begin{equation*}
\min_{\bx\in\mR^d}\; F(\bx) = \mE_\P[f(\bx;\xi)] \quad\;\;\text{s.t.}\quad c(\bx)=\0,
\end{equation*}
where $F:\mR^d\rightarrow\mR$ is a stochastic objective with $f(\cdot;\xi)$ as the noisy observation, and $c:\mR^d\rightarrow \mR^m$ imposes deterministic constraints on the model parameters $\bx$. Such problems appear widely in statistical machine learning, including constrained $M$-estimation and algorithmic fairness. \cite{Na2022Statistical} designed an online sketched Sequential Quadratic Programming (SQP) method for solving the problem.~Define $\mL(\bx, \blambda) = F(\bx) + \blambda^Tc(\bx)$ as the Lagrangian function,~where~\mbox{$\blambda \in \mR^m$}~are~the~dual variables.~The sketched SQP method can be regarded as applying the sketched Newton method~to~$\mL(\bx, \blambda)$, leading to the update $(\bx_{t+1}, \blambda_{t+1}) = (\bx_t, \blambda_t)+ \baralpha_t(\barDelta \bx_t, \barDelta\blambda_t)$, where $(\barDelta \bx_t, \barDelta \blambda_t)$ is the sketched~solution to the primal-dual Newton system:
\begin{equation*}
\begin{pmatrix}
B_t & G_t^T\\
G_t & \0
\end{pmatrix}\begin{pmatrix}
\Delta \bx_t\\
\Delta\blambda_t
\end{pmatrix} = -\begin{pmatrix}
\bar{\nabla}_\bx\mL_t\\
c_t
\end{pmatrix}.
\end{equation*}
Here, analogous to \eqref{nequ:3}, $B_t\approx \nabla_{\bx}^2\mL_t$ is an estimate of the Lagrangian Hessian with respect to $\bx$, $G_t = \nabla c_t\in\mR^{m\times d}$ is the constraints Jacobian, and $\bar{\nabla}_\bx\mL_t = \nabla F(\bx_t;\xi_t) + G_t^T\blambda_t$ is the~\mbox{estimate}~of~the~Lagrangian gradient with respect to $\bx$.
\cite{Na2022Statistical} established asymptotic normality~for~the SQP iterate $(\bx_t, \blambda_t)$. We observe that the constraints are not essential in the SQP~analysis;~\mbox{therefore}, our construction of $\hat{\Xi}_t$ is naturally applied to the covariance estimation of the sketched SQP method. An empirical demonstration of $\hat{\Xi}_t$ for constrained problems is presented in Section \ref{sec5:subsec2}.





