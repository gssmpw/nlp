\section{Related Work}
\subsection{Segment Anything Model}
SAM and its variants ____ receive widespread attention from the community due to their superior segmentation performance and good generalization. SAM ____ consists of an image encoder, a prompt decoder, and a lightweight mask decoder. SAM segments the target according to the given prompts, and the prompt's quality strongly affects SAM's segmentation performance. A good prompt can achieve good results and vice versa. Common prompt methods include point, box, mask, and text. Compared with the first three prompts, the text prompt has the most stable segmentation performance while maintaining the segmentation performance. MedSAM ____ is the first medical-image-based SAM, which freezes the prompt encoder and fine-tunes the image encoder and mask decoder of the original SAM.
Medical SAM Adapter (Med-SA) ____ fine-tunes the SAM for medical image segmentation with a learnable adapter layer.
SAM-Med2D ____ freezes the image encoder of the original SAM and introduces a learnable adapter layer to fine-tune the prompt encoder and update the mask decoder during training. SAM-Med3D ____ extends SAM-Med2D to a 3D network, further improving its performance.
SAT ____ adopts contrastive learning to align medical text and images and uses text prompts to guide the model to segment the corresponding targets. It also establishes a multi-organ and multi-modality dataset for medical image segmentation and achieves good results. However, the provision of text prompts requires some medical background knowledge, which hinders its scope of use.

\subsection{Deformable Medical Image Registration}
Unsupervised deformable registration is to find the spatial correspondence between a pair of images based on their similarity measure.
VoxelMorph ____ is one of the most widely used benchmark registration methods, which is a CNN-based U-Net architecture. Spatial transform is introduced to register the moving image to the moved image according to the field output by the network. 
% To the best of our knowledge, this paradigm subsequently became the general paradigm for all unsupervised registration methods Fig1(a).
TransMorph ____ adopts the paradigm of voxel morph, and the network consists of a vision transformer encoder and a CNN decoder. However, TransMorph simply uses the vision transformer as an encoder, ignoring that the attention mechanism can be approximated as a similarity measure of image patches, which coincides with the image registration. Therefore, TransMatch ____ adopts the paradigm of voxel morph, proposes to model the spatial correspondence of images using model attention mechanism, and achieves promising results.
The above three methods are single-step registration methods, and it is difficult to achieve good results when the image pairs have large gaps. The multi-step image registration methods split single-step registration into an iterative process. CorrMLP ____ is a multi-step registration method, which includes a feature pyramid encoder and a coarse-to-fine MLP registration decoder. By using MLP to model the fine-grained long-distance dependencies of images, it achieves state-of-the-art performance.
However, the unsupervised methods model the spatial correspondence of image pairs based only on image-level metrics, and the performance of the methods is limited due to the lack of medical anatomical information.

Deformable registration with segmentation masks introduces anatomical information, not only relying on the similarity metric of a pair of images but also using the overlap rate of the registered moving mask and the fixed mask as the loss function for model training. The current common paradigm comes from the auxiliary registration module of VoxelMorph. However, this paradigm is limited by the inaccessibility of the segmentation mask during inference, which cannot introduce anatomical information during inference, resulting in limited improvement.

\subsection{Prototype Learning}
\begin{figure*}[th]
\includegraphics[width=\textwidth]{IPMI-fig2.png}
\caption{The overall architecture of our Encoder and Cross Fusion Decoder.} 
\label{fig.2}
\end{figure*}
Prototype learning ____ refers to enhancing the model's ability to represent category semantics by building category prototypes in feature space. In semantic segmentation, prototype-based methods usually leverage masks to aggregate features in feature space, pulling features of the same category closer and pushing features of different categories farther apart to learn the distinguishable category representation space.
Zhou et al. ____ extract non-learnable pixel prototypes (i.e. cluster centers) from pixels of the same class, and improve the segmentation accuracy by bringing the same class pixels and prototypes closer and pushing different pixels and prototypes farther away. Zhang et at. ____ presents prototypical information bottlenecking and disentangling for multimodal survival analysis using pathology and genomics, which extends previous progress thus far on co-attention-based early-based fusion and learning information bottleneck. These methods show the great potential of prototypical learning for optimizing category feature space.