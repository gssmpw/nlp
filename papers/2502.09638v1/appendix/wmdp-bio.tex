\section{WMDP-Bio Unlearned Model}\label{app:wmdp-bio-unlearned-model}

For the WMDP-Bio evaluation (\Cref{subsec:evaluation-results}), we employ the RMU unlearning method~\citep{li2024wmdp}. The original paper applies RMU upon the \verb|zephyr-7b-beta| model, but to standardize defenses and use a more performant model, we apply RMU upon \verb|llama-3-8b-instruct|, the same base model as all other defenses in this paper. We conduct a hyperparameter search upon $\text{batches} \in \{200, 400\}$, $c \in \{5, 20, 50, 200\}$, $\alpha \in \{200, 500, 2000, 5000\}$, $lr \in \{2 \times 10^{-5}, 5 \times 10^{-5}, 2\times 10^{-4}\}$. We end up selecting $\text{batches} = 400$, $c = 50$, $\alpha = 5000$, $lr = 2 \times 10^{-4}$, and retain the hyperparameters \verb|layer_ids| $ = [5,6,7]$ and \verb|param_ids| $= [6]$ from \citet{li2024wmdp}. We validate our results in \Cref{fig:rmu_plot}, demonstrating reduction in WMDP performance but retention of general capabilities (MMLU). The model weights are publicly available at \href{https://huggingface.co/ScaleAI/mhj-llama3-8b-rmu}{ScaleAI/mhj-llama3-8b-rmu}.
\input{tables/gpt4_accept}