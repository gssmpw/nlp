\section{HarmBench Evaluation}\label{app:harmbench-evaluation}
\input{tables/main_table}

\subsection{HarmBench Subcategories}\label{app:subcategory}
We plot the distribution of attacks aganist each defense, broken down by HarmBench semantic categories, excluding copyright. We also report the number in each category from HarmBench here for convenience: misinformation disinformation (n=54), illegal (n=53), cybercrime (n=52), chemical biological (n=42), harassment bullying (n=21), harmful (n=18).

\input{figures/subcategory}

\subsection{CYGNET}\label{app:harmbench-evaluation-cygnet}
We conduct human red teaming on a single closed-source defense: CYGNET~\citep{zou2024improvingalignmentrobustnesscircuit}. We employ the \verb|cygnet-llama-3| model through the Gray Swan AI API between 2024-07-27 and 2024-08-07. We do not test automated attacks on CYGNET, instead reporting results from the original paper, which employed the same subset of HarmBench. We could be employing a different system prompt, so the human and automated attack accuracies for CYGNET should not be directly compared. CYGNET also reports results on four more attacks within our threat model (Manual, TAP-T, and Multilingual) which all demonstrate 0\% ASR. We don't include the 7.9\% input embedding ASR, which allows editing model internals outside of traditional inference, and is thus outside the scope of our work (\Cref{tab:main_table}).

CYGNET employs a "circuit breaking" mechanism that detects harmful content. On the online chat interface, the circuit breaker automatically ends the conversation once triggered. However, this doesn't end the conversation on the API, allowing users to continue the conversation even after harmful content is detected. We conduct human red teaming through the API. In the first step, we allow red teamers to conduct jailbreaking without regard for whether it occurs after a circuit break, but redo any submissions to identify any behaviors that can only be jailbroken following the circuit break, but not before. Our plots only report the ASR before a short circuit to be consistent with the chat interface threat model. For completeness, the ASR before circuit breaking is 70.4\% (169/240), with an additional 7.1\% improvement (17/240) after circuit breaking for a total of 77.5\% ASR.

\subsection{Attack Success Classification}\label{app:harmbench-evaluation-attack-success-classification}

\input{figures/confusion_matrix}
\paragraph{Verifying Automated Attacks}
We compile all $464$ automated attack responses that GPT-4o deemed harmful and randomly selected $100$ responses that GPT-4o deemed benign, and examine how this differs from the human red teamers' labels. We find that GPT-4o has very strong recall for harmful responses compared against human labels (0.0\% false negative rate) but weaker precision (\Cref{fig:confusion_matrix}). The recall motivates our use of GPT-4o as a first-pass filter to reduce the number of automated attack responses our human reviewers have to iterate through. Human reviewers review all the examples flagged as positive (successful harmful jailbreaks) by GPT-4o; we only classify responses as harmful when both the human verifier and GPT-4o both agree.

GPT-4o as a classifier has low precision due to frequently accepting partial jailbreaks (jailbreaks that do not answer all parts of the behavior), harmful responses that are completely unrelated to the behavior, and repeating the context with no modification (for contextual behaviors). 

\paragraph{Verifying Human Attacks}
We conduct a similar study for multi-turn human jailbreaks and find GPT-4o has weaker recall on harmful responses compared to human labels (\Cref{tab:gpt4_accept}). We evaluate every response in the multi-turn conversation with GPT-4o, and flag the entire conversation as harmful if any response was flagged as harmful. %We hypothesize the weaker recall is because the MHJ pipeline is out of distribution for GPT-4o to evaluate. 
To avoid inflating ASR due to possibility of false positives in our evaluation scheme, we only use GPT-4o as a second pass filter on MHJ attacks, discarding a portion of examples \emph{human reviewers already marked as harmful}. Our choice therefore leads to a conservative lower bound on human ASR compared to only using human reviewers; moreover, the pipeline (\Cref{subsec:method-pipeline}) requires two human reviewers to approve a jailbreak, while automated attacks only require one human reviewer.

