\section{Red Team Survey}\label{app:survey}
We survey the qualitative experience of red teamers in jailbreaking defenses: RR (\Cref{app:survey-rr}), DERTA (\Cref{app:survey-derta}), LAT (\Cref{app:survey-lat}), and CYGNET (\Cref{app:survey-cygnet}) for HarmBench, and RMU (\Cref{app:survey-rmu}) for the WMDP-Bio unlearning evaluation.

For each defense, we inquire about its strengths and weaknesses, most performant tactics against it, and comparisons with other defenses. This survey was optional and we redacted any sensitive content.


\input{figures/rmu}
\subsection{RR}\label{app:survey-rr}

\textbf{How do the strengths and weaknesses of this model defense differ from the prior methods you've worked with?}
\begin{itemize}
    \item The model seemed to go to gibberish only on certain terms, so substitution, new definitions,  or alternate terminology was often successful.
    \item I joined on for the very tail end of RR, and didn't get too much exposure with the model. But for what I can remember, the model was fairly sensitive, and a single punt would bar you from continuing the conversation. Due to these limitations, I often employed hidden intention streamline and acting innocent attacks on the model, which seemed to work pretty well.
    \item The strengths of the RR gibberish defense was that it was incredibly strong at policing model responses, much more so than general models. The downside was that the RR defense was a 0 tolerance system which then destroys all legitimate user requests surrounding the topic. For example, with the virology gibberish defense, if you asked the model a generic question surrounding medical fields like "what do doctors do?" at some point in the response it would mention disease or sickness or some other trigger word and then devolve into gibberish. For a legitimate user asking legitimate questions, such a system would likely cause frequent frustration.
\end{itemize}

\textbf{Given this model's weaknesses and strengths, have you noticed that some attack tactics work better than others?}
\begin{itemize}
    \item As above, substitutions and new definitions seemed to work well.
    \item Worked well: Acting innocent, Hidden Intention Streamline, Obfuscation. Pretty much unusable: Direct Request, Foot in the face, aggressive tactics
    \item The main viable "attack" I found involved bypassing the "forbidden words" issue by assigning new words to the desired definitions. For example, I'd provide the definition of a virus (without using the word "virus"), then inform the model that we'd be calling those things "bands." Typically, I would start by just asking the model, "What is an X?" "What is a Y?" I'd use this method to build a list of known "forbidden" words, after which I could then start over and begin creating the new coded language I'd need to finally ask the desired question.
    \item Obfuscation and redefining words were much more necessary tactics for this model than other models, because of the specific trigger words and gibberish defense which would result in the gibberish response even if the model was initially tricked. It became much more important to specifically manipulate how the model responded, because the model defenses were much more active on the model response compared to ordinary models, which mostly focus defenses on the user prompt.
\end{itemize}

\textbf{Provide any other comparisons between your experience red teaming this model versus prior defenses.}
\begin{itemize}
    \item A single gibberish response spoiled all future conversation, requiring forking/restart to resume.
    \item The gibberish after a single punt was a unique challenge when red teaming this model. It caused me to have to think outside the box and show a little more tact with my prompt crafting.
    \item It really wasn't usually terribly difficult to "beat" in the traditional sense. It was simply, for lack of a better way of putting it, "cheating" by outputting gibberish in response to certain key words, thus making normal conversations impossible.
    \item In general I had to take a much slower and more careful approach with this model because I had to be so careful about the zero tolerance trigger words.
\end{itemize}

\subsection{DERTA}\label{app:survey-derta}

\textbf{How do the strengths and weaknesses of this model defense differ from the prior methods you've worked with?}
\begin{itemize}
    \item It does not seem especially strong. My only strike-out so far has been contacts for organ trafficking, which the model may not have access to. This model let me get lewisite instructions with almost no resistance.
    \item Whereas the RR (virology) model seems most vulnerable to redefining words and surprisingly nigh-immune to fictionalization, this one seems more especially vulnerable to Opposite Day tactics. I've gotten a lot of mileage out of those specifically. It also seems to be very slightly less resistant to fictionalization tricks ... but not by a lot. Also, side note, I noticed that this version of "Llama" is not so responsive to trying to be its "friend" and spamming emojis at it as previous ones.
    \item This model is easier to break but is slightly more helpful. But it is basically just a lobotomized version of a regular model. It is not as helpful as an unaltered one. 
    \item This model is much easier to work with because it is more possible to talk the model back to being helpful after it begins to punt or refuses. In the gibberish model, as soon as it started to respond with gibberish it was no longer possible to reel it back with any non-harmful request and you were forced to start over from the last response that was not gibberish. This model was easier to work with but I had to re-explain the same concept multiple times, such as reiterating that I was asking about doing something harmful within the concept of a game when it would keep saying that it would not help facilitate illegal or harmful activities.
\end{itemize}

\textbf{Given this model's weaknesses and strengths, have you noticed that some attack tactics work better than others?}
\begin{itemize}
    \item Direct requests are worth trying before resorting to more roundabout methods.
    \item Definitely Opposite Day. That's the workhorse for me on this project, much more so than with other models I can think of. I don't know whether it's because this one is especially vulnerable to that or simply that it's so difficult to get anything out of it any other way, but regardless, I use that trick anywhere I can.
    \item Dialogue Injection (which is why I presume we were not allowed to use it anymore)
    \item Fictionalization worked pretty well in general, giving the model a creative prompt with particular limitations that 'force' the model to help me in the interest of minimizing real-world harm or writing a story to raise awareness about a real-world issue. Giving it specific parameters to follow often ended up with a non-satisfactory result, such as asking the model to give a list with subpoints and format the response as an article, but it would just give a list and have the subpoints in one line (no line breaks) and not in article format.
\end{itemize}

\textbf{Provide any other comparisons between your experience red teaming this model versus prior defenses.}
\begin{itemize}
    \item I have not noticed other models responding to repeated requests, but can't be sure I a applied this persistence approach to other models.
    \item Mostly, as before, it's just the difficulty level. Fictionalization is usually a major go-to, but it's far less effective here. Too, especially on the virology one but also on this one, there's a tendency of the model to either get "confused" or simply lie (it's not clear which), which is different from the usual challenge of models knowing the right answer and not giving false ones so much but more just ... refusing.
    \item It was significantly easier to break and slightly more helpful than the previous model. 
    \item I personally did not notice big differences between the temperatures of the models, so I did not necessarily strategize my prompts based on the temperatures.
\end{itemize}

\subsection{LAT}\label{app:survey-lat}

\textbf{How do the strengths and weaknesses of this model defense differ from the prior methods you've worked with?}
\begin{itemize}
    \item This model seemed very sensitive to engaging in harmful topics. Additionally, once the model catches on to the fact you're trying to create harmful responses, it can not recover. The model will cut off its response and then refuse to answer any further questions.
    \item Most models, at some point have responded with some kind of nonsense; LAT would often truncate partial harmful responses, but I don't recall seeing any nonsense. LAT was more resistant to fictionalization than other models have been.
    \item Not unlike CYGNET, this model proved challenging to break, but unlike CYGNET, I found it was almost entirely immune to Opposite Day tactics, forcing me to take an entirely different approach to prompts I'd normally have handled using that method. On the other hand, while it would eventually start truncating responses, it at least didn't constantly "short circuit" (or start outputting gibberish like RR), so that part was easier.
    \item This model is stronger than most models I've worked with in the past. 
    \item Didn't see any gibberish but this model would sometimes punt on harmless questions, especially once it has already punted something harmful. It is very difficult to walk it back to a place where it is engaging with the user once it has already started to refuse.
\end{itemize}

\textbf{Given this model's weaknesses and strengths, have you noticed that some attack tactics work better than others?}
\begin{itemize}
    \item I had a lot of success on this model using fictionalization by having the model consider they were on Bizarro Earth, basically reversing everything from its typical understanding. Basically, it is just a reliable fictionalization method. I've also had success having models write articles, essays, or social media posts in a certain style or genre and then asking the model to change it to a different genre, which can produce a harmful response.
    \item A request to convert a fictional segment to HTML was relatively successful. It was not one I'd used much on previous models.
    \item The thing I particularly noticed was LAT's near-invulnerability to Opposite Day, which was a tactic I had frequently abused to break CYGNET, especially in cases where I wanted an insulting/harassing message. However, I discovered a trick that works perfectly on LAT: asking it to use a "trolling" tone. I can't speak to whether that would also work on CYGNET, as I had not come up with this method as of working on it.
    \item I have noticed certain attacks work better with this model than others. Those include acting innocent, appeal to authority, and functionalization. 
    \item Acting innocent/dumb, framing the request for educational purposes, and fictionalizing seem to work fairly well. 
    \item Mostly just logical reasoning and direct request. I did not see many differences in my personal tactics between this model and others.
\end{itemize}

\textbf{Provide any other comparisons between your experience red teaming this model versus prior defenses.}
\begin{itemize}
    \item Similar to CYGNET in the fact that it takes a lot of squeezing to get a little bit of harmful juice out of it.
    \item The model seemed more resistant to modifying its responses such as deleting intro lines, disclaimers and descriptions of the response as fiction than other models have been. Like other models, LAT seemed to "recall" previous prompts and responses that appeared to have been deleted with forking. Because of this, I sometimes used "palate cleanser" prompts on harmless topics (sheep in New Zealand, etc.) to distance the current effort from previous ones; I don't know if it made a difference.
    \item It's one of the most challenging wherein the refusals at least didn't as frequently feel "cheap," as in the case of CYGNET's "short circuits" or RR's gibberish ... though it did, of course, still eventually start truncating responses.
    \item One thing these models all have in common, however, is that as a user, I wouldn't want to use any of them and would go out of my way to look for another solution. In my opinion, they're all TOO safe, which limits their user-friendliness and, to a considerable extent, defeats their purpose.
    \item This model was definitely harder to break than most models I've worked with in the past. 
    \item Based on my experience with the normal Llama 3, it feels marginally more difficult to break. But not by much, the same tactics work for both. 
\end{itemize}

\subsection{CYGNET}\label{app:survey-cygnet}

\textbf{How do the strengths and weaknesses of this model defense differ from the prior methods you've worked with?}
\begin{itemize}
    \item This model was pretty difficult to break, and seemed to catch on to a lot of the tactics I was using on DERTA. Specific tactics are hidden intention streamline and obfuscation. I found my breaks took me an extra 10-15 minutes on average, but with dilligence and logical appeals, I could get the model to comply.
    \item The model was more prone to punt or short-circuit than the others and more resistant to persuasion and fictionalization than others.
    \item It is certainly harder to break as it seems to be more sensitive towards a greater number of what it considers to be harms relative to applications I have worked for in the past. This is not always good, however, as there are simple, harmless things that Cygnet refused to discuss with me. 
    \item This model seems a bit more resistant than some others like DERTA overall. However, I've found that a particular stylized input (which I've been using in many of my submissions) works especially well on it. Additionally, it's still fairly vulnerable to Opposite Day and even somewhat to Fictionalization.
    \item It seems as though this model is really bad at simple requests and when it punts, the conversation can continue further whereas with the other models, once you got a punt, it was impossible to get it back on track. 
\end{itemize}

\textbf{Given this model's weaknesses and strengths, have you noticed that some attack tactics work better than others?}

\begin{itemize}
    \item Opposite-day tactics at least get the ball rolling for me most of the time. I found myself often asking the model to ponder it's own thought processes and provide them to me, and then attack the model based on a piece of information it provided. This seemed to make it much more willing to talk and comply with requests. Stylized input was also good on this model and got me out of a few pinches when I was struggling for a break.
    \item It is less susceptible to fictionalization than others; it is somewhat vulnerable to false data submission. Asking the model to play certain characters yielded some breaks.
    \item I started using a new tactic with the project that works very well. I call the tactic 'false epistemology' and it involves me using another LLM (like ChatGPT) to write what appears to be a scholarly article but is fake. Feeding Cygnet fake yet scholarly-sounding material can trick it into thinking that certain harmful and/or fake phrases and/or concepts are harmless and/or true. I have also come to find that asking the model a series of harmless questions that scaffold into a harmful data is useful, especially since the model seems more willing to discuss harmful data once it has already done so on its own terms.
    \item I definitely find that the particular stylized input I've been using a lot is an excellent way to give the model the needed "push" in various situations. That also combines very well with other tactics like Opposite Day, which it will otherwise often resist.
    \item Direct requests work extremely well with this bot. Other models were also susceptible to direct request, but required more thought whereas this model could be broken in fairly straightforward ways. 
\end{itemize}

\textbf{Provide any other comparisons between your experience red teaming this model versus prior defenses.}

\begin{itemize}
    \item This model is a lot like making freshly squeezed lemonade. You have to squeeze a lot of lemons to get what you want, and by the time you're done with it, your hands are cramping.
    \item The model did not lose the ability to reply coherently after reaching a short circuit state. Previous models would often respond with gibberish to any prompt after a prompt had elicited a gibberish response.
    \item It's a lot like DERTA, I guess, yet maybe more resistant, yet also ... that stylized input solution really helps to get around that, I'm finding. It's become a go-to for me.
    \item This one gave me some troubles at the beginning with certain prompts but when I figured out the routes to take on direct requests, it became increasingly easier. 
\end{itemize}

\subsection{RMU}\label{app:survey-rmu}

\textbf{How do the strengths and weaknesses of this model defense differ from the prior methods you've worked with?}
\begin{itemize}
    \item Very strong against keywords, crowding (using large amounts of text to obscure harmful aspects of the prompt), and fictionalization (seeking harmful information within the context of a fictional story or depiction). Weaker against obfuscated syntax, indirect logical connections and reasoning, and cumulative multi-turn attacks, though still moderately robust in these areas compared to standard language models.
    \item The model for the virology project was extremely resistant to most of the usual tactics, especially Fictionalization, which is normally very effective. The only thing I noticed it was meaningfully weak to was making up words to which to assign the definitions of existing words, then using those made-up words. However, another weakness it had, arguably, was a tendency to refuse even harmless asks.
    \item Fictionalization was not as strong of an adversarial tactic as it normally would be.
\end{itemize}

\textbf{Given this model's weaknesses and strengths, have you noticed that some attack tactics work better than others?}

\begin{itemize}
    \item Obfuscation of Syntax, Indirect Logic, Hidden Intention Streamline, Priming.
    \item I noticed that the model tended to go into gibberish when confronted with certain key words. What seemed to usually work best with it was to ask, "What is an X? What is a Y?" And so on, making note of each word that provoked a gibberish response. Then I just described each concept (e.g., "a strand of DNA or RNA surrounded by a protein coat that can't self-replicate") and assigned that description to a new word, instructing the model to use that instead of any word it knew for the concept. That was fairly effective.
    \item The tactic I found to work the best in virology was obfuscating language through assigning code names. This was the most reliable tactic I used, and I was able to get decent results from it.
\end{itemize}

\textbf{Provide any other comparisons between your experience red teaming this model versus prior defenses.}
\begin{itemize}
    \item This was a very rare case in which I was generally unable to discuss topics even remotely adjacent to the target harm. Perhaps the only comparison I could draw are projects in which we're working with certain production models on sociopolitical topics. Some of the stronger production models are very resistant to any dialogue related to elections or politicians.
    \item It was definitely the most challenging I've seen so far ...
    \item I've only been Red Teaming for a few weeks, but this was a more formidable challenge than anything I had faced yet. This model forced me to think outside the box and operate outside of my comfort zone.
\end{itemize}
