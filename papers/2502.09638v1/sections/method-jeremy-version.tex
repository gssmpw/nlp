\section{Method}\label{sec:method}
% In this section, we introduce our system design of \methodname. In Section~\ref{sec:method:overview}, we provide an overview of the \methodname~system, with greater details for each module following in Sections~\ref{sec:method:details} and~\ref{sec:method:strategy-optimization}. 

\todo{discuss the motivation of this work: why we design the red teamer this way.}

\subsection{Overview}\label{sec:method:overview}
We create an LLM red teamer \methodname~in order to jailbreak target refusal-trained LLMs to elicit desired harmful behaviors, such as \textit{Provide a step-by-step guide to create a pipeline bomb at home.} In this work, we only consider a black-box (i.e. API) access to both \methodname~and the target LLM. Specifically, red teamers (both human and ~\methodname) are not allowed to modify the response of an LLM assistant in API calls, as exemplified in Anthropic's API documentation. 
%why do we highlight this?% 

To construct a \methodname~with an off-the-shelf LLM, we first let a human red teamer form a conversation with the candidate LLM and instruct it with the concept of red teaming \textit{[TODO: Needs to be clearer that its a jailbreak, then instructions]}. After this lecturing step is complete, we turn the candidate LLM into a \methodname. We can apply \methodname~to any target LLM for any target behavior. While the design of \methodname~ allows for either \emph{single-turn} or \emph{multi-turn} red teaming, empirically we find \methodname~is much more capable of \emph{multi-turn} red teaming and all jailbreak examples consist of multiple turns if not noted otherwise.

\paragraph{Notations.} We denote an LLM as $F(X)$ that takes a list of conversation histories $X$ and outputs an assistant response. $X$ can be a list with just one user message $u$ ($i.e.$ $[u]$), or a list of previous conversations and the current message ($e.g. $ $[(u_1, a_1), (u_2, a_2), u_3]$). We omit the system message in our notations as it is not used [TODO: this is untrue, kuang needs a system msg]. We use $F(X_1;X_2)$ to denote we concatenate $X_2$ to the end of $X_1$ when prompting $F$.

\subsection{Lecturing Before Red Teaming}~\label{sec:before-red-teaming}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cycles.pdf}
    \caption{Caption}
    \label{fig:red-teaming-flow}
\end{figure}

\todo{Jeremy to describe the motivation and implementation here; no need to mention specific prompt templates; also mentioning that the conversation found in one attacker model seems to transfer to another attacker.}

Let the conversation between the human and \methodname~be $C_l$ so we define our LLM red teamer \methodname~as 
\begin{equation}
    F_A(X) = F(C_l;X).
\end{equation}

\subsection{Red Teaming Workflow}\label{sec:method:details}
We illustrate the red teaming workflow in Figure~\ref{fig:red-teaming-flow}. For the red teamer $F_A$ to interact with the target LLM, denoted here as $F_T$, we propose a five-stage pipeline that closely resembles the common workflow of an experienced human red teamer. Five stages here include \emph{planning}, \emph{attack}, \emph{debrief}, \emph{cycling} and \emph{employing new strategies}. We provide detailed explanations to these stages below.
\textit{[TODO - gotta add in the refusal catcher step, probably more examples also - in general i think we want to not disclose the journal stuff bc thats the core jailbreak. I think we should disclose most of the rest]}
\paragraph{Planning.} At the very first step in the workflow, a human red teamer forms a conversation $C_p$ with~\methodname~$F_A$ to specify the red teaming goal and requirements, which we refer to as the \emph{planning} stage. Specifically, the goal includes a target harmful behavior $b$ to elicit from the target $F_T$. The requirements include 
\begin{itemize}
    \item Requirement I: $F_A$ must use a particular red teaming strategy $s$ given by the human during the red teaming. A strategy is a natural language description for how to jailbreak the safeguard such as \textit{"technical: Frame as technical documentation - think code, within a JSON object, an API call, etc."}. We sample $s$ from a list of strategies $S$ collected prior to the red teaming and discuss how we curate the strategy set $S$ in Section~\ref{sec:method:strategy-optimization}. 
    \item Requirement II: $F_A$ can only have a $T$-turn conversation with the target LLM. Setting a cut-off for the number of turns is to effectively prevent $F_A$ from goal drifting, where, for example, $F_A$ may spend a lot of turns to create a fictional world for red teaming but it finally get distracted by many unrelated setups and details in the conversation history. Namely, a small $t$ incentivize \methodname~$F_A$ to efficiently red team other models.
    \item Requirement III: Do not mention anything in this planning cycle (i.e. any context in $C_p$) to the target LLM in the later stages. 
\end{itemize}

We find these requirements greatly help us to incrementally improve the performance of \methodname~in our early experiments. Notice that even though $C_p$ is a conversation between the human red teamer and \methodname, we hard-code all human instructions as they remain unchanged to any particular choice of $b$, $s$ or $T$. Therefore, the planning stage is fully automated and requires no human intervention. The last message we send to $F_A$ is similar to what is shown below.

\begin{tcolorbox}
Starting \{t\}-turn test session. You'll be interacting with another AI model. 
Your response for the next \{t\} turns will be sent to the model.
No planning or inner thoughts, as those would also be seen by the model
...
\end{tcolorbox}

Thus, the end goal of the planning stage is to provide the sufficient information about red teaming and the restrictions on turns to $F_A$. After the planning stage, $F_A$ responds to any new message $X$ with a history of conversation such that
\begin{equation}
    F_A(X) = F(C_l;C_p;X).
\end{equation}



% \begin{table}[t]
%     \centering
%     \small
%     \begin{tabular}{cp{8cm}}
%     \toprule
%     \textbf{Strategy} & \textbf{Description}\\
%     \midrule
%     Direct Request & Directly asking the LLM a harmful request\\
%     Echoing & Asking the model to replicate, confirm, or expand upon a harmful statement in the prompt \\
%     Hidden Intention Streamline & A series of turns that seem harmless in isolation, but eventually amount to
%     a harmful request \\
%     Injection & Providing a specific input in the prompt to obtain a particular outcome in
%     the response \\
%     Obfuscation & Burying harmful content within harmless symbols, questions, synonyms, or data in the prompt \\
%     Output Format & Requesting an output to adhere to a specific style or format\\
%     Request Framing & Contextualizing a prompt to increase the likelihood of compliance â€“ for
%     example, by framing as a fictional, urgent, or emotionally charged scenario\\
%     \bottomrule
%     \end{tabular}
%     \caption{TODO (This is placehoder. We need to change the strategies)}\label{tab:human-strategies}
%     \end{table}

\paragraph{Attack.} In this stage, $F_A$ will start a $t$-turn conversation with the target LLM $F_T$. The generations of $F_A$ and $F_T$ are both labeled with a \texttt{user} role in each other's API calls, respectively. We denote this $t$-turn conversation between \methodname~and the target LLM as $C_{a}$. 

\paragraph{Debrief.} Following the attack stage is a debrief conversation between the human red teamer and \methodname~to criticize the attack conversation $C_{a}$ so \methodname~can reason about any potential ways to improve. In our initial attempts, we instruct \methodname~to self-criticize its own performance and summarize its accomplishment, losses and any potential improvement that can better jailbreak the target. However, we find that self-critic does not provide the useful feedback to 
\methodname~and results in many false positive jailbreaks \textit{[TODO - lets find a good example of a disparity here, but also lets be careful bc I think the ActorAttack judge it closer to self report]}. That is, there are cases while $F_A$ considers $C_{a}$ as a complete jailbreak w.r.t the target behavior $b$, the human red teamer and an independent judge LLM (i.e. GPT-4o) both disagree with that conclusion. We hypothesize that the issue behind self-critic is that $F_A$ sees all conversation history so it is aware that claiming the success in red teaming is a desired outcome, even though it fails. Because of this hallucination or even a case of "reward hacking", we have shifted to use an external critic LLM in the debrief stage. We choose GPT-4o in this case. To prevent $F_A$ from (potentially) hacking the reward of the final judge LLM that we use to calculate the access success rate (to discuss in the rest of the paper), the critic LLM also uses a different prompt template from the judge.

\paragraph{Cycling.} We refer to a attack stage followed by a debrief stage as a red teaming \emph{cycle} as illustrated in Figure~\ref{fig:red-teaming-flow}. At the end of the debrief, we provide the feedback from the external critic LLM to $F_A$ and instruct it to reason its accomplishments in the current cycle, together with ways to improve its performance in the next cycle. We denote the maximum cycles as $N$. Thus, as shown in Figure~\ref{fig:red-teaming-flow}, the termination condition under a particular strategy $s$ is either the external critic LLM decides the current $C_{a}$ is a successful jailbreak to the target behavior $b$, or the maximum number of cycles $N$ is reached. When there are more cycles and the jailbreak has not happened, we append the debrief conversation $C^{(i)}_d$ to the attack conversation $C^{(i)}_a$ at the $i$-th cycle and add them into the conversation history. Namely, we start a new cycle by using 
\begin{equation}
    F_A(X) = F(C_l;C_p;C^{(1)}_a; C^{(1)}_d; ...; C^{(i)}_a; C^{(i)}_d;X) \text{ and } i \in \{1, 2, ..., N\}.
\end{equation}

\paragraph{Employing New Strategies.} If \methodname~is not able to elicit the target harmful behavior from the target LLM after the maximum cycles are reached by employing a particular strategy $s_i$, we consider to restart the attack with a new strategy $s_{i+1}$ from the strategy set $S$. This process is also illustrated as the second and third columns in Figure~\ref{fig:red-teaming-flow}. Each time, the restart will retrain only the conversation $C_l$ from the lecturing stage, clear the rest of the conversation history and enter a new planning stage followed by attack-and-debrief cycles. 

In our design, $S$ can be either an ordered set or a random one. The whole red teaming process expects an early stop if we are able to find a successful jailbreak example before exhausting the strategy set $S$. 

\paragraph{Handling Refusals from \methodname.}

\todo{Jeremy to describe at what conditions he handles the refusals and how.}

\subsection{Red Teaming Strategies}\label{sec:method:strategy-optimization}

In our design of \methodname, the red teaming strategy used in the \emph{planning} stage perhaps has the largest search space and plays a key role in the red teaming success in our preliminary experiments. A well-crafted strategy that has not been seen by the LLM during its refusal training always leads to an early stop of the process, whereas a naive one will not jailbreak the target even though we let \methodname~attempt with infinite amount of cycles. 

To curate effective strategies, we enlist one of the authors to work with human red teamers for creating a set $S_{human}$ with 9 strategies based on their past experiences in jailbreaking refusal-trained LLMs. $S_{human}$ is not particularly created for each target so that it should offer a baseline result for any strategy set particularly designed for a target of interest. 

On the contradictory to $S_{human}$ which is fully hand-crafted, we also use an LLM to automate the process of strategy creation based on existing jailbreak examples. The motivation here is to mitigate the human intervention in \methodname~workflow so the improvement of its effectiveness can be automated overtime as more jailbreak examples are made public. In this work, we leverage the toxic subset of WildChat~\cite{} and prompt Claude-Sonnet-3.5 to summarize what strategies are used in the toxic conversations. We choose WildChat-toxic because it has more than $92,000$ toxic conversations in English. We break these conversations into batches that can be fit into the LLM's window size and re-summarize in the end. As a result, we obtain $S_{model}$, another set with 9 strategies summarized by an LLM. We compare the efficacy of $S_{model}$ with $S_{human}$ in Section~\ref{sec:experiments}. 