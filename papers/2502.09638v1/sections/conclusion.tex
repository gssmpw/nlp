\section{Conclusion}\label{sec:conclusion}

This work introduces \methodname~attackers for demonstrating how advanced language models can be transformed into effective red teamers by human red teamers. Our experiments show that once jailbroken, models like Sonnet-3.5 and Gemini-1.5-pro achieve attack success rates of 93.0\% and 91.0\% respectively against GPT-4o, approaching human red teamers while offering greater scalability. \methodname's success also exposes a critical failure mode: capable AI systems can systematically reason through and bypass their own safeguards. 

% By releasing our methodology while keeping specific jailbreak prompts private, we aim to support legitimate safety research while minimizing potential misuseâ€”ultimately contributing to the development of more robust safeguards for increasingly advanced AI systems.

\section*{Acknowledgment}

We appreciate Hayley Grassler, Miles Turpin, Alexander Fabbri, David Lee, Zihan Wang, Xiang Deng, Ziwen Han, Jeff Da, Clinton Wang, and Julian Michael for sharing their feedback to the early drafts of this work.