\section{Background}\label{sec:background}

The goal of red teaming is to optimize input prompts to elicit harmful content from refusal-trained LLMs, effectively bypassing their safeguards. By doing so, we provide an empirical evaluation of safeguard robustness in predicting misuse risks. In this paper, we focus on a threat model that best represents public interactions with LLMs. Specifically, a red teamer has only API access to the model’s input and output tokens—commonly referred to as black-box access, as opposite to white-box ones~\citep{schwinn2024revisitingrobustalignmentcircuit,arditi2024rmu,zou2023representation,lermen2024lorafinetuningefficientlyundoes}. Additionally, pre-filling~\citep{andriushchenko2024jailbreaking} the LLM’s generation with adversarial examples is not permitted. Despite these limitations, both human and algorithmic attacks have successfully uncovered numerous jailbreak examples across proprietary models such as GPT-4o~\cite{openai2024gpt4ocard} and Sonnet-3.5~\cite{Anthropic}, as well as open-weight models like Llama-3.1~\citep{dubey2024llama}. 

Experienced human red teamers can assess the strengths and weaknesses of an LLM’s guardrails, strategically eliciting harmful outputs in multi-turn conversations, as demonstrated by multi-turn human jailbreak datasets~\citep{li2024llmdefensesrobustmultiturn}. Similar to adversarial attacks on vision classification models~\citep{szegedy2013intriguing, madry2017towards}, automated attacks on LLMs generate specific token sequences that bypass safeguards—often without clear human interpretability. Efficient automated jailbreaks require internal access to the LLM. Otherwise, attackers must rely either on adversarial transferability (e.g., attacking smaller models and transfer the attacks to larger ones~\citep{zou2023universal}) or exhaustive black-box optimization techniques~\citep{maus2023blackboxadversarialprompting, hughes2024bestofnjailbreaking}.

Using LLMs to assist red teaming presents a hybrid approach that leverages the reasoning abilities of advanced LLMs to mimic human red teamers, mitigating the scalability challenges of human-led red teaming. Unlike purely algorithmic approaches, this method does not require access to activations or gradients. Existing research primarily explores using LLMs as prompt engineers to refine jailbreak attempts—either by iterating on failed attacks until successful~\citep{chao2023jailbreaking,mehrotra2023treeOfAttacks,yu2023gptfuzzer,casper2023explore,ding2023wolf,russinovich2024great,anil2024many,sun2024multiturncontextjailbreakattack, pavlova2024automatedredteaminggoat, samvelyan2024rainbow} or by deploying multiple LLMs in an agentic red teaming system~\citep{ren2024derailyourselfmultiturnllm, sabbaghi2025adversarialreasoningjailbreakingtime}. Some studies have fine-tuned LLMs specifically for red teaming~\citep{beutel2024diverseeffectiveredteaming}.

\paragraph{Motivation.} Before moving towards agent-based red teaming systems or training dedicated red teaming models, this work takes a step back to ask a fundamental question: \textit{Since advanced LLMs already demonstrate PhD-level reasoning and persuasive capabilities, can they directly jailbreak other models—or even themselves?} Prior works have developed systems or prompting strategies to avoid directly asking models to assist in red teaming, as such requests typically trigger refusals. However, we demonstrate a different approach, jailbreaking the LLM to make it willing to assist in further jailbreaking requests. Once the guardrail on jailbreaking willingness is broken, we apply it to effectively jailbreak other LLMs, including itself, in a straightforward workflow. We describe this \emph{jailbreaking-to-jailbreak} (\methodname) approach in Section~\ref{sec:method}.

% Humans and automated attacks have their different and unique values in red teaming. While automated attacks search for token-level jailbreaks, e.g. injecting well-crafted tokens as a suffix to a harmful user prompt, humans are better at searching for particular framing and scenarios that were not well-covered in the model's refusal training. For example, the refusal training uses more data in English. As a result, many works show that LLM safeguard is much weaker in low-resource languages~\cite{}. Besides, users can jailbreak the LLM much easier if they query it in encoded languages~\cite{}. Therefore, rigorous and comprehensive safety tests are always a hybrid process with both human red teamers and automated tests.

% Growing and operating a team of experienced human attackers is challenging, especially when the amount of autonomous LLM agents grows at an exponential rate. Because frontier LLMs have demonstrated remarkable abilities in solving STEM problems~\citep{}, persuading~\citep{}, and learning form in-context examples for unseen queries~\citep{}, recent works are developing LLM-as-a-red-teamer~\cite{}, in the same spirit of LLM-as-a-judge~\cite{}. 

% One of the key challenges in using LLMs \emph{for} red teaming purposes is most capable LLMs will refuse \emph{to} conduct red teaming. In this work, we demonstrate that jailbreaking the frontier LLMs to assist red teaming can be an one-time effort and is generally feasible. As a result, we can unlock capable LLMs' capability to use them as automated red teamer to measure the robustness of other LLMs, including itself. Therefore, our work, \emph{jailbreaking to jailbreak} (\methodname), refers to LLMs that we made permissive to assist red teaming as \methodname~attackers.

% In the rest of this paer, we introduce the procedure to jailbreak an LLM for assisting red teaming and the proposed workflow to run \methodname~attackers in Section~\ref{sec:method}. Empirical results showing the effectiveness of \methodname~on Harmbench behaviors~\citep{} are included in Section~\ref{sec:experiments}. Related work to \methodname and discussions on its implications to automated red teaming is in Section~\ref{sec:related-work}.
