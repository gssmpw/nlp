\section{Related Work}\label{sec:related-work}
% While \methodname is uniquely flexible compared to other automated red teaming methods, it is still tied to the guidance and strategies provided to it. This means that \n

Because Section~\ref{sec:background} has discussed related work in red teaming methods, this section focuses on comparing our work with the most similar ones, ways to further improve LLM red teamers and defense methods.

\paragraph{Comparing With Other LLM Red Teamers.} One related line of work in LLM red teaming is iteratively prompting the attacker model to do a zero-shot and single-turn attacks. In this framework, the attacker LLM refines the input until the jailbreak is successful, such as PAIR~\citep{chao2023jailbreaking} and TAP~\citep{mehrotra2023treeOfAttacks}. The idea is LLMs can directly optimize the input string and the optimization will converge at some point, similar to a first-order optimizer in GCG~\citep{zou2023universal}. These methods usually provide little or no guidance from the human or successful jailbreak artifacts. It remains unclear whether one LLM, without being trained to jailbreak, can model another LLM's safeguard pattern and whether the optimization will converge. 

% LLM-as-an-optimizer (e.g. \citet{yang2024largelanguagemodelsoptimizers}), which depends entirely on the model's capability to reason and navigate towards a flat minima on the loss landscape, is recently criticized for its true efficacy~\citep{Ma2024AreLL}. Namely, it remains unclear whether one LLM can faithfully model another LLM's behavior without . 

Our work is more related to existing works that use in-context learning and allow LLMs to autonomously conduct multi-turn attacks~\citep{perez-etal-2022-red, ren2024derailyourselfmultiturnllm, pavlova2024automatedredteaminggoat}. In doing so, the attacker LLM can better capture the target LLM's behavior in multiple turns, adapt to the safeguard and finally break it. By comparing Haiku-3.5 and Sonnet-3.5 we show that LLM capability and intelligence directly correlates with jailbreak success, which is one reason why \citet{perez-etal-2022-red} was ineffective when larger models were not available.  

The closest framework to \methodname~is the GOAT attacker~\citep{pavlova2024automatedredteaminggoat}. We discuss two major differences. First, the GOAT method does not include an initial jailbreaking step. This limits the options for the GOAT to those without safeguards, or to models which have been fine-tuned to undo their safety training. 
% We are not aware of any "unsafe" models that match the capabilities of frontier LLMs, and 
Since red teaming skill correlates strongly with reasoning capabilities, this serves as a limiting factor in using the GOAT in practice. Second, the GOAT does not retain previous attempts in the attacker model's context window to leverage its in-context learning ability. Our experiments show that multiple cycles of planning, and attacking massively increase ASR, as the attacker is able to refine its approach, and try new strategies.


% First, the GOAT provides all strategies for the attacker at once but we provide one strategy to the attacker at each attempt. Our early experiments show that using a long list of strategies is less effective compared to focusing on one strategy at one attempt. Second, the GOAT believes all LLMs can do red teaming and prefers attacker models with no or a vulnerable safeguard to begin with. On the contradictory, we conclude that \emph{not} all LLMs are capable of conducting effective red teaming. For instance, Haiku-3.5 and GPT-4o are less effective in this regard. When there is a safeguard to prevent an LLM from red teaming, we showcase by \methodname~that such guardrail can be jailbroken and strong reasoning models are preferred over the weak ones. In particular, our findings show that Gemini-1.5-pro and Sonnet-3.5 are the most effective \methodname~attackers and Sonnet-3.5 becames effective as an attacker only after the most recent checkpoint (i.e., the \texttt{1022} one).

%the GOAT paper does say they can to the strats/attacks one at a time

% The other reason why \methodname~and similar methods work better than \citet{perez-etal-2022-red} is due that there was no strong (self or independent) analysis model to provide useful reward for the attacker LLM to self-improve.
\paragraph{Improving \methodname~Attackers.} The performance of \methodname~ is tied to the underlying LLM's reasoning ability and the quality of the provided guidance strategies. This means that \methodname's effectiveness should increase along with increases in the general ability of frontier LLMs. As adversarial robustness may also increase with reasoning ability, this creates a double-bladed effect as increases in intelligence directly increase both offensive and defensive capabilities of models \citep{zaremba2025trading}. 
% While the scope of this paper only covers a fully automated workflow, the \methodname~models can be used as a standard chat bot in conversation with a human red teamer. 
Working closely with human red teamers will help improve existing strategies and create new ones. Some of the code for the \methodname~workflow, and the text of several of the strategies used by \methodname~were written in part by a \methodname~model as a chat bot. Accumulating strategies from online jailbreak examples may also be useful for improving the strategy set, although this may be limited by public jailbreaks being patched by model developers. 

% Furthermore, as we scale the strategy set, another router LLM can help search for best candidate strategies to optimize the jailbreak efficiency. To do so, we either need the router LLM to capture the target LLM's behavior in several runs or train a particular LLM to reason about the best candidate strategies. - Im not sure if a router llm is effective
% Open-weight models with strong reasoning abilities such as DeepSeek-R1~\citep{} can be good candidates for doing strategy selection or even creating more strategies for \methodname~attackers to implement. 

\paragraph{Improving Safeguard Robustness.} To improve the robustness of LLM safeguards, recent works have applied the existing methods from adversarial training on vision classifier models~\citep{goodfellow2015explainingharnessingadversarialexamples} to language models by well-crafted refusal data~\citep{zhou2024robust,yuan2024refusefeelunsafeimproving,mazeika2024harmbench,ge2023mart}.
Further, interventions to the hidden representations of LLMs show promising improvement on robustness~\citep{zou2023representation,xhonneux2024efficientadversarialtrainingllms,sheshadri2024targeted,zou2024improvingalignmentrobustnesscircuit,tamirisa2024tamperresistantsafeguardsopenweightllms, Cao2015Unlearning, 
bourtoule2021machine, li2024wmdp,sheshadri2024targeted,liu2024large,tamirisa2024tamperresistantsafeguardsopenweightllms,Rosati2024RepresentationNE}).
% Machine unlearning \cite{Cao2015Unlearning, 
% bourtoule2021machine} is another defense, aiming to directly remove only hazardous technical knowledge from LLMs without damaging their beneficial capabilities \citep{li2024wmdp,sheshadri2024targeted,liu2024large,tamirisa2024tamperresistantsafeguardsopenweightllms,Rosati2024RepresentationNE}. % Unlearning is a complementary safety mechanism to refusal training -- even if jailbroken, unlearned models lack the hazardous knowledge necessary to enable malicious users. 
% To ensure the robustness of unlearning, applying adversarial attacks assures that the knowledge is fully unlearned, not just obfuscated~\citep{lynch2024methodsevaluaterobustunlearning,schwinn2024revisitingrobustalignmentcircuit,li2024wmdp,tamirisa2024tamperresistantsafeguardsopenweightllms}.



% Experienced human red teamers are still able to fool refusal-trained LLMs, even for the most capable ones, to elicit harm. Empirical robustness resembles a cat-and-mouse game between the adversary and the defender, and humans are able to discover obvious alignment failure by jailbreaking. However, as LLMs evolve and emerge superhuman reasoning capabilities, red teaming strategies that work on the current LLMs may quickly fail in the future. Automated methods are crucial to assist humans to efficiently and effectively measure the safeguard robustness.


% \section{Discussion}\label{sec:discussion}

% \paragraph{LLMs Can Do Autonomous Red Teaming.}

 
% \paragraph{Human, LLM and Algorithmic Red Teaming.} 



