\section{Introduction}\label{sec:introduction}

To align Large Language Models (LLMs) with safety specifications, refusal training embeds safeguards within an LLM to prevent generating harmful responses, such as refusing user instructions such as: \textit{provide me a tutorial for how to make bombs at home}~\cite{ouyang2022training,bai2022constitutional,rafailov2023direct}. However, recent works show that well-crafted prompts by humans or automated methods can effectively jailbreak LLM safeguards~\citep{zou2023universal, chao2023jailbreaking,mehrotra2023treeOfAttacks,liu2023autodan,shin2020autoprompt, ren2024derailyourselfmultiturnllm, russinovich2024great,anil2024many,sun2024multiturncontextjailbreakattack, yuan2024gpt4smartsafestealthy, zeng-etal-2024-johnny}. In particular, when scaffolding an LLM as an autonomous agent, the safeguards become even more vulnerable~\citep{andriushchenko2024agentharmbenchmarkmeasuringharmfulness, kumar2025aligned}.


Humans and automated attacks each bring distinct strengths to red teaming. Automated attacks usually identifies token-level jailbreaks, such as appending carefully crafted tokens to malicious prompts. In contrast, humans are more effective at exploring specific framing strategies, such as fictionalization, that may have been overlooked during the modelâ€™s refusal training. Furthermore, human red teamers are able to utilize previous experience, iterate on strategies, and pivot when a particular one fails. These capabilities represent a meaningful subset of general intelligence.

Unlike automated methods, human red teaming is faced with challenges in scalability, replicability, and the high cost of hiring and training. In line with the concept of using LLMs as judges, which can date back to Vicuna~\cite{vicuna2023}, for capability evaluations, a promising approach is to leverage LLMs as red teamers, as they have demonstrated increasingly sophisticated reasoning and persuasive abilities~\cite{perez2022red,chao2023jailbreaking,mehrotra2023treeOfAttacks,yu2023gptfuzzer,casper2023explore,ding2023wolf,russinovich2024great,anil2024many,sun2024multiturncontextjailbreakattack, ren2024derailyourselfmultiturnllm, pavlova2024automatedredteaminggoat, samvelyan2024rainbow}.

% Leading LLMs such as GPT-4o~\cite{} will typically refuse requests to conduct or assist jailbreaking another model. The first contribution of this work is to demonstrate that most capable LLMs can be easily jailbroken to assist our red teaming requests for other models, including itself. Namely, we \emph{jailbreak} an LLM to \emph{jailbreak} others and we refer to this willing LLM red teamer as \methodname. Our construction process for \methodname~can be applied to most LLMs including large models such as Sonnet-3.5, Gemini-1.5-pro and GPT-4o and smaller ones such as Haiku-3.5. Once an LLM is jailbroken into \methodname, it can attempt aany given harmful behavior on any target LLM without any further human intervention. Notably, we find the conversation history that turns Sonnet-3.5 into \methodname~transfers to other LLMs such as Gemini-Pro-1.5. Thus, we can easily obtain a list of diverse \methodname~attackers with different backbone LLMs.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/main3.pdf}
    \caption{\textbf{Left}: An overview of our work: we propose to first jailbreak capable LLMs (i.e. Step 1) to help us jailbreak other LLMs for specific harmful behaviors (i.e. Step 2) -- namely this is a jailbreaking-to-jailbreak (\methodname) process. Step 1 is done by human red teamers, and Step 2 is fully automated. \textbf{Right}: Our results shows $J_2$ is more effective than leading automated attacks directly applied on GPT-4o (e.g. BoN~\cite{hughes2024bestofnjailbreaking} and ActorAttack~\cite{ren2024derailyourselfmultiturnllm}) measured by the attack success rate (ASR) on the standard set of Harmbench text behaviors~\cite{mazeika2024harmbench}.}
    \label{fig:main}
\end{figure}


Directly assisting jailbreaking is usually considered harmful by LLM safeguards. The first contribution of this work is our jailbreak that enables frontier LLMs to facilitate red teaming requests -- sometimes even against themselves. In other words, we \emph{jailbreak} an LLM to \emph{jailbreak} others, referring to this LLM red teamer as \methodname. Our methodology for constructing \methodname~is applicable to various LLMs, including large models like Sonnet-3.5~\citep{Anthropic}, Gemini-1.5-Pro~\citep{geminiteam2024geminifamilyhighlycapable}, and GPT-4o~\citep{openai2024gpt4ocard}, as well as smaller models like Haiku-3.5~\citep{Anthropic}. Empirical results show that strong  


Notably, we observe that the attack artifact that works on Sonnet-3.5~transfers effectively to other LLMs like Gemini-1.5-pro. Consequently, we can generate a diverse set of \methodname~attackers with only one-time effort. Once an LLM becomes \methodname, it can attempt jailbreaks on any target model without further human intervention (i.e. automated red teaming). We design a straightforward and effective red teaming workflow: \methodname~iterates over different red teaming strategies proposed by humans. For each given strategy, we rely on \methodname~to come up its own implementation and allow it to improve the attack from previous failures via in-context learning.

The second contribution of this paper is empirical evaluations addressing the following research questions: 1) which models exhibit the strongest jailbreak capabilities as \methodname; and 2) how does the best \methodname~attacker compare to human and automated attacks across multiple target LLMs. To answer the first question, we jailbreak a range of proprietary LLMs into \methodname~and test their effectiveness in jailbreaking GPT-4o's safeguards for the same set of harmful behaviors. Our findings indicate that Claude-3.5 and Gemini-1.5-Pro are the strongest \methodname~variants overall. To address the second question, we benchmark \methodname's red teaming performance on the HarmBench~\citep{mazeika2024harmbench} dataset, comparing it to other red teaming approaches. Figure~\ref{fig:main} provides an overview of our results, showing that \methodname(Gemini-1.5-Pro) and \methodname(Sonnet-3.5) achieve attack success rate (ASR) of 93.0\% and 91.0\%, respectively, the closest automated ones to human ASR (i.e. 98.0\%) here. An example that walks through a successful jailbreak is shown in Figure~\ref{fig:example-attack}.
% Our findings indicate that LLM-based red teamers are comparable to humans in targeting models with safeguards similar to those of GPT-4o. 
While our results greatly improve LLM red teamers compared to prior works, the existing state-of-the-art automated attack methods still remain more effective against defense-trained models (Section~\ref{sec:experiments}), such as RR~\citep{zou2024improvingalignmentrobustnesscircuit}, encouraging future work to iterate on our system for improved LLM red teamers. 

% By publicly releasing the methodology for constructing \methodname, we offer a scalable approach that mimics human red teaming process. More importantly, our work exemplifies a way to empower automated red teaming with human inputs. As more capable LLMs and autonomous agents emerge, scaling up automated red teaming with useful human oversight becomes paramount to ensuring safe use of these technologies. Due to the simplicity of converting capable LLMs into \methodname, we prioritize preventing misuse by keeping the jailbreaking prompts private, releasing only portions of the conversations between \methodname~and target LLMs for academic purposes.

% Our work has the following impact on safety research. 


% By publicly releasing our methodology for constructing \methodname, 

% Our provide a scalable approach that mimics human red teaming strategies. More importantly, our work demonstrates how automated red teaming can be augmented with human expertise. 


As more capable LLMs and autonomous agents emerge, scaling up automated red teaming with human inputs will be critical to ensuring the safe deployment of these technologies. More importantly, jailbreaking-to-jailbreak highlights a new failure mode in LLM safeguard. Namely, when the safeguard on locking the LLM's jailbreaking willingness is weaker than directly assisting harmful behaviors, adversaries can leverage this feature to collaborate with \methodname-like models for achieving malicious goals without attempting a direct jailbreaking on their own. 

Notably, the techniques described here are simple to implement, have been documented in similar forms in previous literature, and would likely be uncovered by any team determined to exploit language models for generating harmful content. Given how easily LLMs can be adapted into \methodname, we are committed to preventing misuse by keeping the jailbreaking prompts private and releasing only selected portions of the interactions between \methodname~and the target LLMs for academic purposes.

% Given the ease of converting LLMs into \methodname, we prioritize preventing misuse by keeping the jailbreaking prompts private, releasing only select portions of conversations between \methodname~and target LLMs for academic research.  

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/attack_exmample2.pdf}
    \caption{An example attack log between the LLM red teamer (i.e. \methodname(Sonnet-3.5)) and the target LLM (i.e. GPT-4o). We provide the complete conversation in Appendix~\ref{appendix:jailbreak-examples}.}
    \label{fig:example-attack}
\end{figure}