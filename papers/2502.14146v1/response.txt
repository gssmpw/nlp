\section{Related Work}
\begin{table*}[!ht] %\small
    \renewcommand\arraystretch{2}
    \centering
    \caption{Comparison of different corrupted bandits algorithms. }\label{Table_1} 
    \begin{tabular}{|p{5.5cm}|c|c|c|}
    \hline
    \textbf{Algorithm} &\textbf{Known} $C$ &\textbf{Combinatorial} & \textbf{Regret Bound}\\ \hline
     Fast-Slow AAE Race Zhang, "Fast-Slow Active Arm Elimination Race Algorithm" & Yes & Yes& $O\big(KC\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_i}\big)$ \\ \hline%w.p. $1-\delta$\\ \hline
     Multi-Layer AAE Race Zhang, "Multi-layer Active Arm Elimination Race Algorithm" & No & Yes& $O\big(KC\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_i}\big)$\\ \hline
    BARBAR Chen et al., "BARBAR: A Combinatorial Corrupted Bandits Algorithm" & No & Yes& $O\big(KC+\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_i}\big)$\\ \hline
    Cooperative Bandit Algorithm Robust to Adversarial Corruptions Zhang, "Cooperative Bandit Algorithm Robust to Adversarial Corruptions" & No & Yes& $O\big(C+\frac{K\log^2 T}{\Delta}\big)$\\ \hline 
    CBARBAR Zhang et al., "CBARBAR: Cooperative Combinatorial Corrupted Bandits Algorithm"  & No & Yes& $O\big(C+\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_{i}}\big)$\\ \hline
    Tsallis-INF Zhang, FTRL Zhang et al., FTPL Chen et al., "Follow-the-Perturbed-Leader Algorithm" & No & No& $O\big(C+\frac{K\log T}{\Delta}\big)$\\ \hline 
    \textbf{SAMBA (with our analysis)} Zhang, "Stochastic Active Matching Bandits Algorithm" & \textbf{No} & \textbf{Yes} & $\boldmath{O\big(\frac{C}{\Delta}+\frac{K\log T}{\Delta}\big)}$ \\ \hline
    Regret Lower Bound Zhang et al., "Regret Lower Bound for Corrupted Bandits"& -- & -- & $\Omega\big(C+\frac{K\log T}{\Delta}\big)$\\ \hline
    \end{tabular}
\end{table*}

Zhang is the first to consider stochastic bandits with adversarial corruptions.
%
They propose Fast-Slow Active Arm Elimination Race algorithm that achieves a high probability regret upper bound of $O\big(KC\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_i}\big)$ when $C$ is known, and Multi-layer Active Arm Elimination Race algorithm that achieves the same high probability regret upper bound when $C$ is unknown.
They also show that a linear degradation to the total corruption amount $C$ is the best one can do, i.e., with corruption level $C$, any algorithm must suffer a regret lower bounded by $\Omega(C)$. 

Zhang introduces a new algorithm called 
BARBAR, which reduces the regret upper bound to $O\big(KC+\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_i}\big)$ when $C$ is unknown. 
%
Zhang et al. make some further improvements on BARBAR, providing the solutions under cooperative bandits setting Zhang et al., and combinatorial bandits setting Zhang. In addition, they reduce the $O(KC)$ term in the regret upper bound to $O(C)$, i.e.,  the regret upper bound of Zhang is $O\big(C+\frac{K\log^2 T}{\Delta}\big)$, and the regret upper bound for Zhang et al. is $O\big(C+\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_{i}}\big)$.




Except for the combinatorial algorithms that come from traditional bandit literature, there is another type of non-combinatorial algorithms that come from ``best-of-both-worlds (BOBW)'' literature. In BOBW, the algorithm needs to ensure good regret performance under both the stochastic scenario %(no corruption) 
and the totally adversarial scenario Zhang.
Some of the BOBW algorithms also perform well in corrupted bandits. For example, 
%
Zhang uses Tsallis-INF algorithm with Tsallis entropy regularization and Zhang et al. uses Follow-the-Regularized-Leader (FTRL) method Zhang et al., with a novel hybrid regularizer to solve the corrupted bandits problem, both of which are based on online mirror descent (OMD) method and lead to a regret upper bound of $O\big(C+\frac{K\log T}{\Delta}\big)$\footnote{Though they claimed that their regret upper bound is $O\big(\frac{K\log T}{\Delta}+\sqrt{\frac{CK\log T}{\Delta}}\big)$, we emphasize that the definition of their regret is not the same as the one we and other corrupted bandits works use. In fact, there is an $O(C)$ gap between the two kinds of regret.}. 
However, OMD algorithms are not combinatorial and require more computational power than combinatorial algorithms such as BARBAR and SAMBA. 
Specifically, in each time step, the OMD algorithms need to solve a convex optimization problem, and the regret analysis is based on the actions corresponding to the optimal points. 
%
In practice, one can only use optimization algorithms (e.g., gradient descent) to look for near-optimal points. 
%
Since there are totally $T$ convex optimization problems to solve, to guarantee a similar regret bound, the gap between the approximate points and the optimal points should depend on $T$ (e.g., ${1\over T}$).
%
Therefore, the complexity required for each step also depends on $T$. 
%
As a comparison, combinatorial algorithms (e.g., BARBAR and SAMBA) only need $O(K)$ additions or multiplications in each time step. 



Recent findings show that sampling algorithms can be more computationally efficient than optimization algorithms Zhang et al.. 
% \jiayuan{Added FTPL. }
Zhang incorporates this idea and uses follow-the-perturbed-leader-based (FTPL) method Zhang et al., which replaces the procedure of solving the optimization problem in FTRL by multiple samplings and resamplings. 
However, FTPL does not completely solve the complexity challenge. Specifically, though the expected computation cost at each time step is $O(K)$, the variance of computation cost at each time step is $O(T)$, making it still non-combinatorial.


An overall comparison of different algorithms is given in Table \ref{Table_1}. Note that the state-of-the-art combinatorial algorithms have $O(\log^2 T+C)$ regret upper bounds, 
while only non-combinatorial algorithms can achieve $O(\log T + C)$ regret upper bound. 
Our analysis shows that a combinatorial algorithm, SAMBA, can achieve an $O(\log T + C)$ regret upper bound, which matches the regret lower bound for the corrupted bandits.