\section{Related Work}
\begin{table*}[!ht] %\small
    \renewcommand\arraystretch{2}
    \centering
    \caption{Comparison of different corrupted bandits algorithms. }\label{Table_1} 
    \begin{tabular}{|p{5.5cm}|c|c|c|}
    \hline
    \textbf{Algorithm} &\textbf{Known} $C$ &\textbf{Combinatorial} & \textbf{Regret Bound}\\ \hline
     Fast-Slow AAE Race____ & Yes & Yes& $O\big(KC\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_i}\big)$ \\ \hline%w.p. $1-\delta$\\ \hline
     Multi-Layer AAE Race____ & No & Yes& $O\big(KC\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_i}\big)$\\ \hline
    BARBAR____ & No & Yes& $O\big(KC+\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_i}\big)$\\ \hline
    Cooperative Bandit Algorithm Robust to Adversarial Corruptions____ & No & Yes& $O\big(C+\frac{K\log^2 T}{\Delta}\big)$\\ \hline 
    CBARBAR____  & No & Yes& $O\big(C+\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_{i}}\big)$\\ \hline
    Tsallis-INF____, FTRL____, FTPL____ & No & No& $O\big(C+\frac{K\log T}{\Delta}\big)$\\ \hline 
    \textbf{SAMBA____ (with our analysis)} & \textbf{No} & \textbf{Yes} & $\boldmath{O\big(\frac{C}{\Delta}+\frac{K\log T}{\Delta}\big)}$ \\ \hline
    Regret Lower Bound____& -- & -- & $\Omega\big(C+\frac{K\log T}{\Delta}\big)$\\ \hline
    \end{tabular}
\end{table*}



____ is the first to consider stochastic bandits with adversarial corruptions.
%
They propose Fast-Slow Active Arm Elimination Race algorithm that achieves a high probability regret upper bound of $O\big(KC\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_i}\big)$ when $C$ is known, and Multi-layer Active Arm Elimination Race algorithm that achieves the same high probability regret upper bound when $C$ is unknown.
They also show that a linear degradation to the total corruption amount $C$ is the best one can do, i.e., with corruption level $C$, any algorithm must suffer a regret lower bounded by $\Omega(C)$. 

____ introduces a new algorithm called 
BARBAR, which reduces the regret upper bound to $O\big(KC+\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_i}\big)$ when $C$ is unknown. 
%
____ make some further improvements on BARBAR, providing the solutions under cooperative bandits setting____ and combinatorial bandits setting____. In addition, they reduce the $O(KC)$ term in the regret upper bound to $O(C)$, i.e.,  the regret upper bound of____ is $O\big(C+\frac{K\log^2 T}{\Delta}\big)$, and the regret upper bound for____ is $O\big(C+\sum_{i\neq i^*}\frac{\log^2 T}{\Delta_{i}}\big)$.




Except for the combinatorial algorithms that come from traditional bandit literature, there is another type of non-combinatorial algorithms that come from ``best-of-both-worlds (BOBW)'' literature. In BOBW, the algorithm needs to ensure good regret performance under both the stochastic scenario %(no corruption) 
and the totally adversarial scenario 
____. Some of the BOBW algorithms also perform well in corrupted bandits. For example, 
%
____ uses Tsallis-INF algorithm with Tsallis entropy regularization and____ uses Follow-the-Regularized-Leader (FTRL) method____ with a novel hybrid regularizer to solve the corrupted bandits problem, both of which are based on online mirror descent (OMD) method and lead to a regret upper bound of $O\big(C+\frac{K\log T}{\Delta}\big)$\footnote{Though they claimed that their regret upper bound is $O\big(\frac{K\log T}{\Delta}+\sqrt{\frac{CK\log T}{\Delta}}\big)$, we emphasize that the definition of their regret is not the same as the one we and other corrupted bandits works use. In fact, there is an $O(C)$ gap between the two kinds of regret.}. 
However, OMD algorithms are not combinatorial and require more computational power than combinatorial algorithms such as BARBAR and SAMBA. 
Specifically, in each time step, the OMD algorithms need to solve a convex optimization problem, and the regret analysis is based on the actions corresponding to the optimal points. 
%
In practice, one can only use optimization algorithms (e.g., gradient descent) to look for near-optimal points. 
%
Since there are totally $T$ convex optimization problems to solve, to guarantee a similar regret bound, the gap between the approximate points and the optimal points should depend on $T$ (e.g., ${1\over T}$).
%
Therefore, the complexity required for each step also depends on $T$. 
%
As a comparison, combinatorial algorithms (e.g., BARBAR and SAMBA) only need $O(K)$ additions or multiplications in each time step. 



Recent findings show that sampling algorithms can be more computationally efficient than optimization algorithms____. 
% \jiayuan{Added FTPL. }
____ incorporates this idea and uses follow-the-perturbed-leader-based (FTPL) method____ which replaces the procedure of solving the optimization problem in FTRL by multiple samplings and resamplings. 
However, FTPL does not completely solve the complexity challenge. Specifically, though the expected computation cost at each time step is $O(K)$, the variance of computation cost at each time step is $O(T)$, making it still non-combinatorial.


An overall comparison of different algorithms is given in Table \ref{Table_1}. Note that the state-of-the-art combinatorial algorithms have $O(\log^2 T+C)$ regret upper bounds, 
while only non-combinatorial algorithms can achieve $O(\log T + C)$ regret upper bound. 
Our analysis shows that a combinatorial algorithm, SAMBA, can achieve an $O(\log T + C)$ regret upper bound, which matches the regret lower bound for the corrupted bandits.