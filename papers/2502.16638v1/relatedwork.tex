\section{Related Work}
\paragraph{Structured Pruning.} Structured pruning aims to remove redundant structures to reduce the size of DNNs. The identification of redundancies can be performed based on different criteria such as  sparsity~\cite{lin2019toward,wen2016learning,zhuang2020neuron,chen2017reduced,chen2018farsa,chen2021orthant,otov1,gao2020highly,meng2020pruning,yang2020deephoyer,frantar2023sparsegpt,idelbayev2022exploring}, Bayesian pruning~\cite{zhou2019accelerate,van2020bayesian}, ranking importance~\cite{li2020eagleeye,li2019exploiting,zhang2018systematic,otov2}, grouped kernel search~\cite{zhong2023one}, spectral graph analysis~\cite{laenen2023one}, reinforcement learning~\cite{he2018amc,chen2020storage}, and the lottery ticket hypothesis~\cite{frankle2018lottery,frankle2020linear}. %\dpr{need to clarify} 
Previous methods typically use a complicated, time-consuming process that requires extensive domain knowledge to effectively train the DNN. %\dpr{always?  So also for your method?}
Another challenge is to define a pruning search space procedure that can be generalized to various DNNs. %\dpr{why is this here, and not in the challenges section?} 
Recent frameworks, such as OTO~\cite{otov2,otov3,hesso} and DepGraph~\cite{fang2023depgraph}, have automated the construction of this search space using dependency graphs. However, these methods are not suitable for QADNNs due to prevalent issues such as weight-sharing and shape ambiguous operators.\footnote{Shape ambiguous operators are  operators (\eg, \textit{reshape} and \textit{view} in PyTorch) that transform input tensors into outputs of varying dimensions.} This limitation highlights the ongoing challenge of automating structured pruning for any \qadnn{}.


% make progress in automating the structured pruning process for generic DNNs, both of which does not need additional pretraining or fine-tuning. 

\smallskip
\noindent \textbf{Quantization-Aware Training (QAT).} The standard approach to QAT is applying a  uniform bit width across all layers. However, ~\cite{dong2019hawq,hong2022cadyq} empirically show that different layers in DNNs exhibit different sensitivities to quantization, suggesting that mixed-precision quantization may be a better approach for reducing performance loss. Several strategies including parameterized quantizers~\cite{goodparam}, heuristic approaches~\cite{QST}, reinforcement learning~\cite{elthakeb2020releq,balaskas2024hardware}, multi-objective Bayesian optimization~\cite{miriyala2024mixed}, and Hessian information guided methods~\cite{dong2019hawq,dong2020hawq,yao2021hawq} have been proposed to determine the optimal bit width for each layer.

% ~\cite{goodparam} considers a parameterized quantizer to learn the bit width over the training process. (3) Mixed precision (differentiable~\cite{goodparam} or heuristic~\cite{opq} or reinforcement learning~\cite{elthakeb2020releq,balaskas2024hardware} or multi-objective bayesian optimization~\cite{miriyala2024mixed} or Hessian information guided~\cite{dong2019hawq,dong2020hawq,yao2021hawq})
% VIBNet~\cite{dai2018compressing}. 

% Many works use the straight through estimation (STE) \cite{STE} to make their discrete quantization function differentiable. Multiple works propose to learn the optimal quantization parameters such as the stepsize, dynamic range, and bit width, alongside the network weights.\cite{relaxed,goodparam,esserstep, jainstep}. \cite{jainstep} and \cite{esserstep} both learn the step size using STE and gradient descent, but not the bit width. \cite{wangstep, elthakebstep} learn the bit width with reinforcement learning (RL), demonstrating the efficacy of assigning different learned bit widths across layers.  \cite{goodparam} learn only the step size and dynamic range and using the learned quantizers to infer the bit width. They also explore the best parameterizations for a differentiable quantizer using STE.
\smallskip
\noindent \textbf{Joint Pruning and Quantization.} The challenge of using a joint approach lies in determining an optimal tradeoff between the pruning ratio and quantization levels for the model. Two primary strategies have been explored to address this challenge. The first strategy is to efficiently search the joint parameter space with prior work considering heuristics~\cite{QST}, reinforcement learning~\cite{balaskas2024hardware}, and Bayesian optimization~\cite{Clip-q}. The second strategy focuses on gradient-based optimization techniques. \cite{yang2020automatic} formulates a constrained optimization problem and solves it using a combination of ADMM and a greedy algorithm. In the follow up work~\cite{admm}, a reweighted optimization method is proposed with the goal of increasing the compression rate and reducing the number of hyperparameters of the ADMM-based method. \cite{DJPQ} approaches joint pruning and quantization by combining the VIBNet approach~\cite{dai2018compressing} with a differentiable quantizer defined by parameters that are learned.~\cite{van2020bayesian} unifies pruning and quantization by treating pruning as 0-bit quantization. ~\cite{wang2020apq} devises to train a quantization-aware accuracy predictor to deal with large joint parameter search space. 
%\cite{liu2023single} extends the work~\cite{van2020bayesian} to support non-power-of-two bit width.
To avoid a multi-stage process (first determining the configuration and then retraining the model), \cite{opq} proposes a one-shot optimization framework for the joint compression of DNNs. Other strategies are inspired by Markov chain and knowledge distillation. For instance, \cite{li2024markov} presents an interpretable joint pruning and quantization framework that borrows ideas from Markov chain, and \cite{li2024adaptive} applies an adaptive multi-teacher knowledge distillation method to train both the pruned and quantized networks. Our proposed framework falls under the gradient-based optimization approach.

% Figures for graph transformation section.
\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{depgraph_zig_xiaoyi.pdf}
\caption{Figure \textcolor{red}{2(a)} and \textcolor{red}{2(b)} illustrate the Quantization-Aware dependency graph analysis for weight quantization and activation quantization, respectively. Figure~\textcolor{red}{2(c)} presents a dependency graph after QADG analysis. Concrete examples are provided in~\cref{appendix:dependency.graph}.}
\label{fig:graph.analysis}
\end{figure*}

%------------------------------------------------------------------------