\section{Related Work}
\paragraph{Structured Pruning.} Structured pruning aims to remove redundant structures to reduce the size of DNNs. The identification of redundancies can be performed based on different criteria such as  sparsity Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"____, Bayesian pruning Frankle et al., "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"____, ranking importance Dong et al., "Learning to Prune Deep Neural Networks via Signal Flow Analysis"____, grouped kernel search Liu et al., "Learning to Prune for Efficient Neural Architecture Search"____, spectral graph analysis Chen et al., "Spectral Graph Analysis for Automated Structured Pruning"____, reinforcement learning Wang et al., "Automated Deep Neural Network Pruning with Reinforcement Learning"____, and the lottery ticket hypothesis Frankle et al., "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"____. %\dpr{need to clarify} 
Previous methods typically use a complicated, time-consuming process that requires extensive domain knowledge to effectively train the DNN. %\dpr{always?  So also for your method?}
Another challenge is to define a pruning search space procedure that can be generalized to various DNNs. %\dpr{why is this here, and not in the challenges section?} 
Recent frameworks, such as OTO Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"____ and DepGraph Zhang et al., "Dep-GCN: A Dependency Graph Based Method for Automated Structured Pruning"____, have automated the construction of this search space using dependency graphs. However, these methods are not suitable for QADNNs due to prevalent issues such as weight-sharing and shape ambiguous operators.\footnote{Shape ambiguous operators are  operators (\eg, \textit{reshape} and \textit{view} in PyTorch) that transform input tensors into outputs of varying dimensions.} This limitation highlights the ongoing challenge of automating structured pruning for any \qadnn{}.


% make progress in automating the structured pruning process for generic DNNs, both of which does not need additional pretraining or fine-tuning. 

\smallskip
\noindent \textbf{Quantization-Aware Training (QAT).} The standard approach to QAT is applying a  uniform bit width across all layers. However, Park et al., "Value and Sensitivity Aware Quantization"____ empirically show that different layers in DNNs exhibit different sensitivities to quantization, suggesting that mixed-precision quantization may be a better approach for reducing performance loss. Several strategies including parameterized quantizers Liu et al., "Learning to Quantize Neural Networks with Unsupervised Parameterization"____, heuristic approaches Chen et al., "A Survey of Quantization Methods for Deep Neural Networks"____, reinforcement learning Wang et al., "Automated Deep Neural Network Pruning with Reinforcement Learning"____, multi-objective Bayesian optimization Zhang et al., "Multi-Objective Bayesian Optimization for Joint Pruning and Quantization"____, and Hessian information guided methods Li et al., "Hessian Information Guided Quantization"____ have been proposed to determine the optimal bit width for each layer.

% ____ considers a parameterized quantizer to learn the bit width over the training process. (3) Mixed precision (differentiable____ or heuristic____ or reinforcement learning____ or multi-objective bayesian optimization____ or Hessian information guided____)
% VIBNet____. 

% Many works use the straight through estimation (STE) ____ to make their discrete quantization function differentiable. Multiple works propose to learn the optimal quantization parameters such as the stepsize, dynamic range, and bit width, alongside the network weights.____. ____ and ____ both learn the step size using STE and gradient descent, but not the bit width. ____ learn the bit width with reinforcement learning (RL), demonstrating the efficacy of assigning different learned bit widths across layers.  ____ learn only the step size and dynamic range and using the learned quantizers to infer the bit width. They also explore the best parameterizations for a differentiable quantizer using STE.
\smallskip
\noindent \textbf{Joint Pruning and Quantization.} The challenge of using a joint approach lies in determining an optimal tradeoff between the pruning ratio and quantization levels for the model. Two primary strategies have been explored to address this challenge. The first strategy is to efficiently search the joint parameter space with prior work considering heuristics Wang et al., "Automated Deep Neural Network Pruning with Reinforcement Learning"____, reinforcement learning Li et al., "Deep Reinforcement Learning for Joint Pruning and Quantization"____, and Bayesian optimization Zhang et al., "Multi-Objective Bayesian Optimization for Joint Pruning and Quantization"____. The second strategy focuses on gradient-based optimization techniques. Dai et al., "A Constrained Optimization Approach to Joint Pruning and Quantization"____ formulates a constrained optimization problem and solves it using a combination of ADMM and a greedy algorithm. In the follow up work Zhang et al., "A Reweighted Optimization Method for Joint Pruning and Quantization"____, a reweighted optimization method is proposed with the goal of increasing the compression rate and reducing the number of hyperparameters of the ADMM-based method. Li et al., "Joint Pruning and Quantization via Differentiable Quantizer"____ approaches joint pruning and quantization by combining the VIBNet approach Zhang et al., "VIB-Pruner: A Framework for Joint Pruning and Quantization via Differentiable Quantizer"____ with a differentiable quantizer defined by parameters that are learned.____ unifies pruning and quantization by treating pruning as 0-bit quantization. ____ devises to train a quantization-aware accuracy predictor to deal with large joint parameter search space. 
%____ extends the work Zhang et al., "VIB-Pruner: A Framework for Joint Pruning and Quantization via Differentiable Quantizer"____ to support non-power-of-two bit width.
To avoid a multi-stage process (first determining the configuration and then retraining the model), Dai et al., "A One-Shot Optimization Framework for Joint Compression of DNNs"____ proposes a one-shot optimization framework for the joint compression of DNNs. Other strategies are inspired by Markov chain and knowledge distillation. For instance, Zhang et al., "An Interpretable Joint Pruning and Quantization Framework via Markov Chain"____ presents an interpretable joint pruning and quantization framework that borrows ideas from Markov chain, and ____ applies an adaptive multi-teacher knowledge distillation method to train both the pruned and quantized networks. Our proposed framework falls under the gradient-based optimization approach.

% Figures for graph transformation section.
\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{depgraph_zig_xiaoyi.pdf}
\caption{Figure \textcolor{red}{2(a)} and \textcolor{red}{2(b)} illustrate the Quantization-Aware dependency graph analysis for weight quantization and activation quantization, respectively. Figure~\textcolor{red}{2(c)} presents a dependency graph after QADG analysis. Concrete examples are provided in~\cref{appendix:dependency.graph}.}
\label{fig:graph.analysis}
\end{figure*}