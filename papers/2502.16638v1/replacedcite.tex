\section{Related Work}
\paragraph{Structured Pruning.} Structured pruning aims to remove redundant structures to reduce the size of DNNs. The identification of redundancies can be performed based on different criteria such as  sparsity____, Bayesian pruning____, ranking importance____, grouped kernel search____, spectral graph analysis____, reinforcement learning____, and the lottery ticket hypothesis____. %\dpr{need to clarify} 
Previous methods typically use a complicated, time-consuming process that requires extensive domain knowledge to effectively train the DNN. %\dpr{always?  So also for your method?}
Another challenge is to define a pruning search space procedure that can be generalized to various DNNs. %\dpr{why is this here, and not in the challenges section?} 
Recent frameworks, such as OTO____ and DepGraph____, have automated the construction of this search space using dependency graphs. However, these methods are not suitable for QADNNs due to prevalent issues such as weight-sharing and shape ambiguous operators.\footnote{Shape ambiguous operators are  operators (\eg, \textit{reshape} and \textit{view} in PyTorch) that transform input tensors into outputs of varying dimensions.} This limitation highlights the ongoing challenge of automating structured pruning for any \qadnn{}.


% make progress in automating the structured pruning process for generic DNNs, both of which does not need additional pretraining or fine-tuning. 

\smallskip
\noindent \textbf{Quantization-Aware Training (QAT).} The standard approach to QAT is applying a  uniform bit width across all layers. However, ____ empirically show that different layers in DNNs exhibit different sensitivities to quantization, suggesting that mixed-precision quantization may be a better approach for reducing performance loss. Several strategies including parameterized quantizers____, heuristic approaches____, reinforcement learning____, multi-objective Bayesian optimization____, and Hessian information guided methods____ have been proposed to determine the optimal bit width for each layer.

% ____ considers a parameterized quantizer to learn the bit width over the training process. (3) Mixed precision (differentiable____ or heuristic____ or reinforcement learning____ or multi-objective bayesian optimization____ or Hessian information guided____)
% VIBNet____. 

% Many works use the straight through estimation (STE) ____ to make their discrete quantization function differentiable. Multiple works propose to learn the optimal quantization parameters such as the stepsize, dynamic range, and bit width, alongside the network weights.____. ____ and ____ both learn the step size using STE and gradient descent, but not the bit width. ____ learn the bit width with reinforcement learning (RL), demonstrating the efficacy of assigning different learned bit widths across layers.  ____ learn only the step size and dynamic range and using the learned quantizers to infer the bit width. They also explore the best parameterizations for a differentiable quantizer using STE.
\smallskip
\noindent \textbf{Joint Pruning and Quantization.} The challenge of using a joint approach lies in determining an optimal tradeoff between the pruning ratio and quantization levels for the model. Two primary strategies have been explored to address this challenge. The first strategy is to efficiently search the joint parameter space with prior work considering heuristics____, reinforcement learning____, and Bayesian optimization____. The second strategy focuses on gradient-based optimization techniques. ____ formulates a constrained optimization problem and solves it using a combination of ADMM and a greedy algorithm. In the follow up work____, a reweighted optimization method is proposed with the goal of increasing the compression rate and reducing the number of hyperparameters of the ADMM-based method. ____ approaches joint pruning and quantization by combining the VIBNet approach____ with a differentiable quantizer defined by parameters that are learned.____ unifies pruning and quantization by treating pruning as 0-bit quantization. ____ devises to train a quantization-aware accuracy predictor to deal with large joint parameter search space. 
%____ extends the work____ to support non-power-of-two bit width.
To avoid a multi-stage process (first determining the configuration and then retraining the model), ____ proposes a one-shot optimization framework for the joint compression of DNNs. Other strategies are inspired by Markov chain and knowledge distillation. For instance, ____ presents an interpretable joint pruning and quantization framework that borrows ideas from Markov chain, and ____ applies an adaptive multi-teacher knowledge distillation method to train both the pruned and quantized networks. Our proposed framework falls under the gradient-based optimization approach.

% Figures for graph transformation section.
\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{depgraph_zig_xiaoyi.pdf}
\caption{Figure \textcolor{red}{2(a)} and \textcolor{red}{2(b)} illustrate the Quantization-Aware dependency graph analysis for weight quantization and activation quantization, respectively. Figure~\textcolor{red}{2(c)} presents a dependency graph after QADG analysis. Concrete examples are provided in~\cref{appendix:dependency.graph}.}
\label{fig:graph.analysis}
\end{figure*}

%------------------------------------------------------------------------