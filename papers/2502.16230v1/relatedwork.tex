\section{Related Works}
\subsection{Learning Humanoid Locomotion}
Reinforcement learning-based methods for legged robots have made great progress. Relying on a set of reward specification, legged robots can perform robust locomotion~\cite{jenelten2024dtc,lee2020learning,kumar2022adapting}, blindly~\cite{gu2024advancing,siekmann2021blind,nahrendra2023dreamwaq} and perceptively~\cite{long2024learning,chen2024identifying} traversing stairs and parkour~\cite{zhuang2024humanoid,hoeller2024anymal,zhuang2023robot}. These methods usually involve complex multi-stage training processes or behavior cloning, which may not work well in complex real-world situations and noisy sensor readings. Our framework is used for sensor denoising and world state estimation to minimize the sim-to-real gap.

\subsection{Learning Legged State Estimation}
Learning methods for state estimation of legged robots have been widely studied, with the main approach being to predict the privileged information from observation history. Previous work~\cite{kumar2021rma, kumar2022adapting} encoded the robot state and the environment state as latent variables and trained an adaptation module to imitate the encoded state. However, it requires multiple steps of training and the fully preservation of privileged information in the hidden state is not guaranteed. Previous work~\cite{ji2022concurrent} proposed a concurrent training method for the policy and the state estimator, which needs only one step of training with explicit estimation of the privileged information. Previous work~\cite{nahrendra2023dreamwaq} explicitly estimates the root speed and represents other environmental states as hidden states, training the decoder to reconstruct the privileged information. Previous work~\cite{gu2024advancing} went a step further and used the observation history to denoise the current observation and estimate the world state. The denoised state is stored in hidden variables.

However, these methods~\cite{nahrendra2023dreamwaq,gu2024advancing,ji2022concurrent} do not avoid passing the gradient of policy learning back to the estimation network, which will distract the goal of the estimation network and affect the estimation accuracy. In our work, we use sensor history to reconstruct the world state, which is used as the sole input of the policy. Due to the existence of gradient cutoff, there is a clear division of labor between modules, which greatly improves the denoising ability and estimation accuracy.