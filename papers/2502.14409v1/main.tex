\documentclass[11pt]{article}

\usepackage[preprint]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor,colortbl,soul}
\usepackage{CJK}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage[font=small,skip=-5pt]{subcaption}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[many]{tcolorbox}    	%
\usepackage{setspace}               %
\usepackage{multicol}               %
\usepackage{musicography}

\newcommand{\dataset}{SUnsET}
\newcommand{\cn}{\textcolor{red}{\textsc{\textbf{[cite]}}}}

\newcommand{\num}{\textcolor{blue}{\textsc{\textbf{[num]}}}}

\newcommand{\td}[1]{\textcolor{green}{\textsc{\textbf{[todo: #1]}}}}

\newcommand{\sqa}{SQuALITY}
\newcommand{\las}{LexAbSumm}
\newcommand{\smh}{SummHay}
\newcommand{\ops}{ScholarQABench}

\newtcolorbox{sharp_box}{
    sharpish corners, %
    boxrule = 0pt,
    toprule = 4.5pt, %
    enhanced,
    fuzzy shadow = {0pt}{-2pt}{-0.5pt}{0.5pt}{black!35} %
}

\newtcolorbox{prompt_box}{
    enhanced,
    boxrule = 0pt,
    colback = sub,
    borderline west = {1pt}{0pt}{main}, 
    borderline west = {0.75pt}{2pt}{main}, 
    borderline east = {1pt}{0pt}{main}, 
    borderline east = {0.75pt}{2pt}{main}
}


\title{Unstructured Evidence Attribution for\\ Long Context Query Focused Summarization}


\author{Dustin Wright\thanks{Work partially completed during a research visit to University of Michigan.}\textsuperscript{\musSharp{}}\hspace{0.5cm}Zain Muhammad Mujahid\textsuperscript{\musSharp{}}\hspace{0.5cm}Lu Wang\textsuperscript{\musFlat{}}, \\
\textbf{Isabelle Augenstein\textsuperscript{\musSharp{}}\hspace{0.5cm}David Jurgens\textsuperscript{\musFlat{}\musNatural{}}}\\
\textsuperscript{\musSharp{}}Department of Computer Science, University of Copenhagen\\ \textsuperscript{\musFlat{}}Department of Computer Science and Engineering, University of Michigan\\
\textsuperscript{\musNatural{}}School of Information, University of Michigan}


\begin{document}
\maketitle
\begin{abstract}


Large language models (LLMs) are capable of generating coherent summaries from very long contexts given a user query. 
Extracting and properly citing evidence spans could help improve the transparency and reliability of these summaries. At the same time, LLMs suffer from positional biases in terms of which information they understand and attend to, which could affect evidence citation.
Whereas previous work has focused on evidence citation with predefined levels of granularity (e.g. sentence, paragraph, document, etc.), we propose the task of long-context query focused summarization with \textit{unstructured} evidence citation. We show how existing systems struggle to generate and properly cite unstructured evidence from their context, and that evidence tends to be ``lost-in-the-middle''. 
To help mitigate this, we create the \textbf{S}ummaries with \textbf{Uns}tructured \textbf{E}vidence \textbf{T}ext dataset (\dataset{}), a synthetic dataset generated using a novel domain-agnostic pipeline which can be used as supervision to adapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4 datasets with varying document types and lengths that LLMs adapted with \dataset{} data generate more relevant and factually consistent evidence than their base models, extract evidence from more diverse locations in their context, and can generate more relevant and consistent summaries. 
\end{abstract}

\section{Introduction}

At the frontier of the capabilities of natural language processing (NLP) systems such as large language models (LLMs) is the ability to handle long contexts such as books, sets of research papers, and long legal documents, and summarize them based on user queries~\cite{DBLP:journals/csur/KohJLP23,DBLP:journals/ijon/SuALPBL24,DBLP:journals/corr/abs-2004-05150,DBLP:journals/corr/abs-2403-05530}. The difficulty of this task lies in the need to attend to relevant information in the source document(s) given a query and simultaneously derive coherent, factually consistent, and distilled insights~\cite{DBLP:conf/emnlp/GoldmanJSMDT24}. While LLMs have achieved much progress on this~\cite{DBLP:journals/corr/abs-2404-16130}, people prefer to use traditional retrieval sources (e.g., search engines) for critical queries due to the need for transparency and provenance~\cite{DBLP:journals/corr/abs-2411-17375}. While progress has been made on structured evidence citation (i.e., with fixed levels of granularity such as sentences and documents)~\cite{DBLP:journals/corr/abs-2311-03731}, in order to improve the flexibility and explainability of long-context query focused summaries we propose to study the task of \textit{unstructured} evidence citation.
\begin{figure}[t]
        \centering
        \includegraphics[width=0.88\linewidth]{figures/fig1.pdf}
        \caption{We study query focused summarization with \textit{unstructured} evidence citation from long contexts }
        \vspace{-15pt}
        \label{fig:fig1}
\end{figure}%

\vspace{-12pt}
Within this, we explore two key barriers to achieving good performance with reliable summaries. First, LLMs have positional biases \cite{DBLP:journals/tacl/LiuLHPBPL24,DBLP:conf/acl/RavautSCJ24}, focusing on the earlier and later tokens in their input context~\cite{DBLP:journals/corr/abs-2404-01430}. This can potentially bias which evidence a model selects to support a summary. Second, 
fine-tuning has benefits in terms of inference efficiency~\cite{wu2024inference}, and in some cases can outperform inference interventions~\cite{DBLP:journals/tacl/ZhangLDLMH24,DBLP:conf/nips/LiuTMMHBR22}, but would require a large dataset with specialized examples of long documents, queries, extracted evidence, and summaries which cite this evidence. Creating such a dataset requires an extensive amount of time, money, and expertise~\cite{openscholar,DBLP:conf/emnlp/LabanFXW24,DBLP:conf/coling/SantoshAG24}. 

To address these issues, we provide the first study on unstructured (i.e., no fixed level of granularity) evidence citation in long-context query-focused summarization. While attribution has been investigated in recent works~\cite{DBLP:conf/emnlp/LabanFXW24, openscholar, DBLP:journals/corr/abs-2311-03731}, we present a novel and more challenging scenario where a model must extract unstructured text spans from its context to use as supporting information for its summary. We show for base models that extracting and citing unstructured evidence is challenging, and that evidence is often lost-in-the-middle. To help alleviate this, we present a fine-tuning approach based on an inductively generated synthetic dataset called the \textbf{S}ummaries with \textbf{Uns}tructured \textbf{E}vidence \textbf{T}ext dataset (\dataset{}). 
Across 5 different models and 4 different datasets (single- and multi-document), we demonstrate that fine-tuning on \dataset{} data can help mitigate lost-in-the-middle, increase citation accuracy and coverage, and improve summary quality. We release \dataset{} and our generation code to the public for further study.\footnote{\url{https://github.com/dwright37/unstructured-evidence-sunset}}






\section{Challenges in Query Focused Long Context Summarization}

Query focused, long context summarization requires a model to be able to simultaneously ingest a large number of context tokens (possibly from multiple documents), retrieve and attend to relevant information in this context given a query, and integrate this information into a factually consistent and relevant summary. LLMs, with their increasingly large context sizes, have proven to be particularly adept at performing this task~\cite{DBLP:journals/tacl/ZhangLDLMH24,DBLP:journals/corr/abs-2404-16130,DBLP:journals/corr/abs-2408-14906}. Yet, a number of challenges remain, both in dealing with long contexts and with producing query-focused summaries~\cite{DBLP:conf/acl/LiWZZ24,DBLP:journals/corr/abs-2408-14906,DBLP:conf/acl/BaiLZL0HDLZHDTL24,DBLP:journals/tacl/LiuLHPBPL24,DBLP:conf/emnlp/0002IEBL23,DBLP:conf/acl/RavautSCJ24,DBLP:conf/emnlp/LabanFXW24,DBLP:journals/corr/abs-2411-17375, DBLP:journals/csur/JiLFYSXIBMF23}. The main focus of our work is on evidence attribution~\cite{DBLP:conf/emnlp/LabanFXW24,DBLP:journals/corr/abs-2411-17375,DBLP:journals/corr/abs-2311-03731} and its relation to the lost-in-the-middle problem~\cite{DBLP:journals/tacl/LiuLHPBPL24,DBLP:conf/acl/RavautSCJ24}.



\subsection{Evidence Attribution}
Though LLMs can generate convincing summaries, in practice people often prefer to acquire information through media where provenance is fully transparent (for example, a normal search engine)~\cite{DBLP:journals/corr/abs-2411-17375}. Improving the ability of LLMs to generate both relevant summaries and provide accurate attributions has the potential to help improve their usefulness, transparency, and trustworthiness. Recent work has started to explore this direction, including SummHay~\cite{DBLP:conf/emnlp/LabanFXW24} and OpenScholar~\cite{openscholar}. However, most works focus on structured attribution with fixed levels of granularity (e.g., sentences, paragraphs, or documents)~\cite{DBLP:journals/corr/abs-2311-03731}. To the best of our knowledge, we provide a first study on unstructured evidence citation in long context query focused summarization, which is more flexible than the fixed-granularity approach.

\subsection{Lost-in-the-Middle}
LLMs suffer from positional preferences in their learned attention~\cite{DBLP:journals/tacl/LiuLHPBPL24}, oftentimes preferring early or late tokens in their context~\cite{DBLP:journals/corr/abs-2404-01430}. While this problem was originally demonstrated on retrieval-augmented-generation (RAG) tasks with explicit answers such as question answering, follow-up work has shown its persistence in more abstractive tasks such as summarization~\cite{DBLP:conf/acl/RavautSCJ24} and query focused multi-document summarization~\cite{DBLP:conf/emnlp/LabanFXW24}. A number of solutions have been proposed, most of which rely on manipulating either the positions of tokens in the context or the positional embeddings of LLMs in order to remove their intrinsic bias~\cite{DBLP:journals/corr/abs-2407-01100,DBLP:conf/acl/HePDSLQLWZZ24,DBLP:journals/corr/abs-2404-01430}. We further explore and document this problem at the level of unstructured evidence citation, demonstrating how evidence is extracted unevenly across documents, and how this problem can be mitigated using purely synthetic data.





\section{Learning to Cite and Summarize}
\label{sec:methods}

Our task is: given a query about a long input consisting of one or more documents, generate a response to the query which cites \textit{unstructured} evidence from the input. We differentiate ourselves from previous work on summarization with attribution~\cite{DBLP:conf/emnlp/LabanFXW24,openscholar,DBLP:journals/corr/abs-2311-03731} by requiring a model to extract unstructured text as evidence with no predefined levels of granularity. This evidence must also be relevant and consistent with both the document and the summary sentences. While more challenging, this enables flexible and explainable summary generation, and citations can easily be generated in both multi-document and single-document settings.

\begin{figure}[tb]
    \centering
    \begin{sharp_box}
    \fontsize{10pt}{12pt}\selectfont
            \textbf{P1. Titles:}
            Generate $N$ unique titles of fiction and non-fiction documents.
            
            \textbf{P2. Document outline:} Given a title, generate an outline broken down into discrete sections.

            \textbf{P3. Queries, summaries, and evidence:} Given a document title and outline, generate 5 questions, 5 responses, and supporting passages that will be included in the document. Indicate which sections the passages should be included in.

            \textbf{P4. Document sections:} Generate each section of the document one at a time. Ensure that evidence passages are included verbatim in each section.

            \textbf{P5. Refinement:} For each $\langle$question,summary,evidence$\rangle$ tuple, refine the summary and evidence based on the document.
            
            \textbf{P6. Validation:} For each $\langle$question,summary,evidence$\rangle$ tuple, validate that the summary fully addresses the question and is faithful to the document, and includes inline attribution to evidence passages.
            
    \end{sharp_box}
    \caption{Six stage inductive data generation pipeline. The full prompts for each stage are given in Appendix \ref{sec:prompts} \autoref{fig:synth_title_prompt} - \autoref{fig:validation_prompt}.}
    \vspace{-7pt}
    \label{fig:dataset_generation}
\end{figure}
\begin{figure}[t]
    \centering
    \begin{sharp_box}
    \fontsize{10pt}{12pt}\selectfont

            \textbf{Example Document Snippet}\\
            Title: ``Writing the Unwritable''\\
            ...They demonstrate that while writing the unwritable is fraught with difficulty, it can also yield transformative insights that resonate profoundly with readers. \hl{Writing the unwritable requires a recognition of the limitations of language, and a willingness to push against those boundaries.} This requires not merely acceptance of silence or ambiguity but a bold declaration that some truths demand to be told, no matter how fraught the endeavor may be....
            
            
            \textbf{Example Query}\\
            What does it mean to write the unwritable, and what historical examples illustrate this concept?

            \textbf{Example Summary Snippet}\\
            To write the unwritable involves confronting and articulating subjects and experiences that resist verbal expression, often due to limitations of language, social taboos, and the impact of censorship [1][2][3].

            \textbf{Example Evidence Snippet}\\
            $[$1$]$ \hl{Writing the unwritable requires a recognition of the limitations of language, and a willingness to push against those boundaries.}
            
    \end{sharp_box}
    \caption{Snippets from a \dataset{} document.}
    \label{fig:data_sample}
\end{figure}

To tackle this we create \dataset{}, a synthetic dataset which is generalizable across domains, using a novel inductive generation pipeline. We then use this data to fine-tune models using adapters~\cite{DBLP:conf/icml/HoulsbyGJMLGAG19} to improve unstructured evidence citation and summary quality, as well as mitigate the lost in the middle problem.  For the latter, previous work has shown that fine-tuning with data augmentation (e.g., shuffling documents~\citealt{DBLP:journals/corr/abs-2404-01430}) can help achieve this. Given this, we construct \dataset{} so that documents are modular: documents are broken down into discrete sections, so that data augmentation through shuffling document sections (thus shuffling global structure) is possible. We first present the inductive pipeline approach used to generate \dataset{}, followed by our two fine-tuning schemes which use low-rank adapters (LoRA)~\cite{DBLP:conf/iclr/HuSWALWWC22} trained on different views of \dataset{}.





\input{latex/tables/synthetic_data}


\subsection{Generating \dataset{}}

Recent works have demonstrated the promise of generating large scale synthetic datasets for fine-tuning task specific models~\cite{DBLP:journals/corr/abs-2409-02098,DBLP:conf/acl/HonovichSLS23, DBLP:conf/acl/WangKMLSKH23, DBLP:conf/iclr/ChenLYWGYTS0HJ24, DBLP:conf/iclr/XuSZG0FTLJ24} with the help of strong LLMs such as GPT-4~\cite{DBLP:journals/corr/abs-2303-08774}. Inspired by this, we propose \dataset{}, a generic, domain-agnostic dataset generated from a novel inductive synthetic data pipeline that allows us to fine-tune downstream models to generate relevant and consistent query focused summaries from long-contexts with unstructured evidence citations.

Our pipeline generates datasets composed of long documents paired with queries and long-form answers to those queries. Each summary includes inline citations that reference relevant unstructured text spans in the original document. We make several design decisions intended to overcome known problems in synthetic data generation, including the potential for low diversity~\cite{DBLP:conf/acl/HonovichSLS23,DBLP:conf/acl/WangKMLSKH23} and labeling errors~\cite{DBLP:conf/iclr/ChenLYWGYTS0HJ24}. This includes taking a six stage pipeline approach which generates synthetic data inductively, and validation steps which refine summaries, refine evidence, and reject bad summaries and evidence.


The full generation process is described in \autoref{fig:dataset_generation}, with prompts provided in Appendix \ref{sec:prompts}. Diversity in document topic and type is accomplished by first generating diverse document titles, which seed the subsequent steps of generation. We inductively build up each document, starting with the queries, summaries, and evidence passages. When generating evidence, each evidence passage is assigned to a section in the document so that evidence can be distributed precisely.
The summaries, queries, and assigned evidence are then used as context from which each section of the document is generated, one section at a time. This makes documents modular, which we take advantage of during training to study evidence positional biases. Following this, the queries, summaries, and evidence are refined in order to fully reflect the final document. Finally, we filter the summaries and evidence by prompting GPT 4o mini to predict if the summaries fully address the query and are fully supported by the document (see \autoref{fig:data_sample} for an example).


To validate the pipeline, we additionally generate two baseline datasets. The first is generated by combining all the steps listed in \autoref{fig:dataset_generation} into a single prompt. The second includes a control where we enforce that no repeated titles are generated (see \autoref{fig:baseline_prompt} in Appendix \ref{sec:prompts}). We compare each dataset using samples of 100 documents along dataset diversity metrics (average type-token ratio (TTR)\cite{DBLP:journals/corr/abs-2307-04626}, embedding cosine distance, and average word length) in \autoref{tab:synthetic_stats}. Baseline non-pipelined approaches produce shorter documents and shorter summaries, and these documents and summaries tend to be much more similar to each other than those generated using our pipeline.

\subsection{Training Complementary Adapters}

Previous work has demonstrated that altering the position embeddings of LLMs either directly or through fine-tuning can help to overcome positional biases~\cite{DBLP:conf/acl/HsiehCL0LKGRLKP24,DBLP:journals/corr/abs-2404-01430}. 
One of the benefits of \dataset{} documents is that they are highly modular, as the generated documents have both global coherence at the level of the full document and local coherence at the level of discrete sections. Given this, we experiment with position-aware and position-agnostic training in order to observe their impact on evidence selection and quality, as well as summary quality.

For position-aware training, we concatenate all the document sections together in their natural order to construct the context, while for position-agnostic training, we shuffle the document sections before concatenating them, thus randomizing the global structure of the position embeddings while maintaining the local structure. This gives us two adapters for each model in our experiments. The prompt we use for training is provided in Appendix \ref{sec:prompts} \autoref{fig:generation_prompt}, and all training is performed using supervised fine-tuning on \dataset{} data using LoRA~\cite{DBLP:conf/iclr/HuSWALWWC22}. 

\subsection{Summarizing with Unstructured Evidence}

To generate summaries with unstructured citations, we design a prompt that is constructed of elements from previous work~\cite{openscholar}. The full prompt is given in \autoref{fig:generation_prompt} in Appendix \ref{sec:prompts}. This prompt was refined through several rounds of experimentation, while the aspects related to citation formatting are taken from \cite{openscholar}. We use this prompt both for inference and for supervised fine-tuning on \dataset{}. After generating responses using this prompt, we validate that the format instructions are followed in order to separate evidence from the response. When the output is misformatted, we regenerate samples either until the format is correct, or until 5 attempts are reached. If the formatting is still in error, then we use the output of the last attempt as is for the summary. 
To deal with long contexts, we take a divide-and-conquer approach, which chunks each document according to the model's maximum content length, summarizes each chunk, and finally summarizes each summary. Thus, the output for each $\langle$document, query$\rangle$ pair is a $\langle$summary, evidence\_list$\rangle$ pair containing the summary and a list of unstructured text spans from the context.
\input{latex/tables/evidence_hallucination}

\input{latex/tables/citation_results}

\section{Experiments and Results}

Our experiments focus on three research questions:
\begin{itemize}[noitemsep]
    \item \textbf{RQ1:} How well can LLMs extract and use \textit{unstructured} evidence?
    \item \textbf{RQ2:} Is evidence lost-in-the-middle?
    \item \textbf{RQ3:} Does learning to cite unstructured evidence improve summary quality?
\end{itemize}

\paragraph{Test Data} We use four test datasets (full dataset descriptions in Appendix \ref{sec:datasets_appendix}). At a high level these are: \textbf{\sqa} (\citealt{DBLP:conf/emnlp/WangPCPB22}, short sci-fi novels, single document, average context length: 5,200 tokens); \textbf{\las{}} (\citealt{DBLP:conf/coling/SantoshAG24}, long legal documents, single document, average context length: 14,357 tokens); \textbf{\smh} (\citealt{DBLP:conf/emnlp/LabanFXW24}, synthetic conversations and news, multi-document, average haystack context length: 93,000 tokens); and \textbf{\ops} (\citealt{openscholar}, Computer Science research papers, multi-document, average context length: 16,341 tokens).
\paragraph{Models} We use a set of LLMs covering multiple sizes and pretraining configurations. This includes Llama 3.2 1B, Llama 3.2 3B, Llama 3.1 8B~\cite{DBLP:journals/corr/abs-2407-21783}, Mistral Nemo 2407, and Mixtral 8x7B.\footnote{Huggingface model IDs are listed in Appendix \ref{sec:models_appendix} \autoref{tab:models_description}} Additionally, we provide an upper bound estimate on performance using GPT 4o mini with no fine-tuning.

\input{latex/figures/positional_bias_figures}

\input{latex/tables/abstractive_results}

\paragraph{Evaluation} For evaluation, we follow recent trends in summarization evaluation, which have noted that traditional lexical based metrics such as ROUGE score~\cite{lin2004rouge} are insufficient for more complex summarization tasks~\cite{ DBLP:conf/emnlp/KohJZLP22}. We evaluate our models using autoraters (i.e., LLM-as-a-judge)~\cite{DBLP:journals/corr/abs-2411-15594,DBLP:conf/nips/ZhengC00WZL0LXZ23,DBLP:conf/emnlp/LiuIXWXZ23} along two dimensions using previously validated prompts listed in Appendix \ref{sec:prompts} (\autoref{fig:relevance_prompt} and \autoref{fig:consistency_prompt})~\cite{DBLP:journals/corr/abs-2410-23463}. These dimensions are \textit{Relevance} and \textit{Consistency}. Given a source text, a target text, and optionally a query, \textit{Relevance} measures how well the target covers the main points of the source, as well as how much irrelevant or redundant information it contains. \textit{Consistency} measures to what degree the target contains any factual errors with respect to the source. Both scores are measured on a scale from 1-5 using GPT-4o-mini~\cite{DBLP:journals/corr/abs-2303-08774}.\footnote{We test the robustness of our evaluation in Appendix \ref{sec:evaluation_robustness}} 

\paragraph{Training, and Inference} We generate a total of 2,352 synthetic documents, giving us 11,309 $\langle$document, question, summary$\rangle$ tuples. We hold out 200 documents for validation and early stopping. In all cases we fine tune using the Huggingface Transformers implementation of LoRA~\cite{DBLP:conf/iclr/HuSWALWWC22} with a rank and $\alpha$ of 16 applied to all linear operators of each model. 



\subsection{RQ1: Can LLMs Generate Unstructured Evidence?}
Using the datasets and models just described, we first test if LLMs can extract and effectively utilize unstructured evidence, as well as the impact of training on \dataset{}. We look at two aspects of citation ability: evidence hallucination and evidence accuracy.

To study evidence hallucination, we attempt to match each sentence generated in the evidence list to its position in the context. We do so with two measures: exact string match and percent overlap of longest common substring (LCS) between the evidence and context. We present the rate of exact evidence match and 50\% LCS overlap for all models aggregated across all datasets in \autoref{tab:hallucination_results}. We see that \textbf{all base models struggle to faithfully copy evidence from the context}. This includes GPT 4o mini, which only faithfully copies 11\% of the time. This rate is dramatically improved in all cases except for the smallest model (Llama 3.2 1B) by learning to cite unstructured evidence using \dataset{}. Additionally, we see that rates of citation also dramatically increase (6.8$\times$ for Mixtral 8x7B). We find that learning to cite using \dataset{} greatly improves the extraction of unstructured evidence from arbitrary contexts.

Next, we study attribution quality using a measure similar to the citation accuracy presented in~\citet{openscholar} but based on the relevance and consistency of evidence with their citing sentences (i.e. citances). To measure attribution quality, we propose two measures: Relevance F1 and Consistency F1. These are calculated as follows: for a given $\langle$summary, evidence\_list$\rangle$ pair, we first sentence tokenize the summary and extract all citations from each citance. Then, we pair each citance with each piece of evidence that it cites and measure either the relevance or consistency of the evidence with respect to the citance. We normalize these scores (originally between 1 and 5) to the range $[0,1]$. To obtain a measure of precision, we average these scores over all the citances in the summary. For recall, we average the scores over \textit{every} sentence in the summary, thus penalizing summaries which do not use citations. F1 is then calculated as $\frac{2*p*r}{p+r}$.

We present results on citation relevance and consistency in \autoref{tab:retrieval_results}. We again find that without any intervention, base models are generally bad at selecting and generating relevant and consistent evidence. As expected, larger models are better at this task, with base GPT 4o mini providing a modest upper bound on performance. We see that training on \dataset{} helps to significantly close this gap and greatly improve citation quality. Smaller models see fewer gains, while larger models are able to adapt using \dataset{} much more strongly, in some cases surpassing GPT 4o mini (e.g., Llama 3.1 8B for ScholarQABench). As with evidence hallucination, standard fine-tuning generally performs better, except for Mixtral, which sees a boost from doing shuffled training. Overall, we find that \textbf{base models struggle to utilize unstructured evidence, while \dataset{} helps models to learn this across highly diverse test sets}.

\input{latex/figures/performance_vs_data}

\subsection{RQ2: Is evidence lost-in-the-middle?}

Next, we explore to what extent this evidence is lost in the middle. To characterize positional bias, we match extracted evidence to its relative location in the document context (based on 50\% LCS overlap) and plot this as a histogram in \autoref{fig:evidence_location}. As a point of reference, we also plot the distribution of summary sentence locations within the test set documents by matching ground truth reference summaries to their relative locations in their context documents in \autoref{fig:gt_distribution}.\footnote{We find the relative location using cosine similarity of S-BERT sentence embeddings~\cite{reimers-2019-sentence-bert}}

We find that \textbf{evidence is lost in the middle for all base models}. This includes GPT 4o Mini, which has a sharp spike of evidence in the early context. This stands in contrast to ground truth summary location distributions, which are uniform in all cases except for \las{} which has a bias for evidence at the end of the context. In general, training on \dataset{} without shuffling increases the rate of evidence extraction, but does not decrease the bias significantly. Shuffling on the other hand, increases the rate of evidence extraction and can decrease the bias. This is especially the case for Llama 3.1 8B and Llama 3.2 3B. Thus, similar to previous work on RAG for question-answering tasks~\cite{DBLP:journals/corr/abs-2404-01430}, we find that shuffled training has the potential to help reduce positional biases for evidence extraction as well. This presents a tradeoff between training with shuffled and unshuffled documents: on the one hand, standard training leads to generally higher intrinsic citation and evidence quality, while on the other, fails to reduce positional bias. Shuffling reduces positional bias, potentially utilizing more relevant evidence for the final summary, but suffers a penalty in terms of citation and evidence quality.






\subsection{RQ3: Is Summary Quality Improved?}
Finally, we test if learning to cite has a positive impact on summary quality. For this experiment, we measure the relevance and consistency of every summary with respect to its context and query. We again compare each base model to training on \dataset{} with standard and shuffled context. Our results are presented in \autoref{tab:abstractive_results}.

We find that \textbf{summary quality is uniformly and significantly improved by learning to cite unstructured evidence.} Larger models adapt more easily than smaller models which struggle on the single-document datasets (\sqa{} and \las{}). Learning from \dataset{} has an especially strong impact on multi-document datasets, while standard and shuffled training generally lead to similar gains in performance. Mistral Nemo tends to perform better with shuffled training, while for other models the results are mostly interchangeable. The selection of which approach is best then comes down to the tradeoff between evidence quality and positional bias. Ultimately though, fine-tuning on \dataset{} helps reduce the performance gap in terms of summary quality with much more powerful models such as GPT 4o Mini.

Additionally, for \las{}, we see a drop in performance for Llama models but gains in performance or Mixtral. Recall that \las{} expresses a bias in terms of where relevant information in the summary lies. With the exception of Mixtral, shuffled training tends to mitigate this drop slightly, or lead to the best performance in the case of Nemo, which may be attributed to the reduced evidence bias of this setting. 

Finally, To observe the impact of number of \dataset{} training samples on summary quality, we plot relevance and consistency vs. number of training samples for \sqa{} and \ops{} in \autoref{fig:variable_data_squality} and \autoref{fig:variable_data_ScholarQABench}. Interestingly, we find that performance generally peaks with only a modest amount of data (around 1k-3k samples depending on the model) at which point performance plateaus or slightly drops. Given this, we see that adapting models to our task requires minimal data and can be performed relatively cheaply. 


\section{Discussion and Conclusion}
Generating unstructured evidence citations alongside query-focused summaries has the potential to improve user trust and transparency in LLMs. Our study has highlighted salient challenges in this task, as well as a potential solution for them. With no intervention, these models suffer from the lost-in-the-middle problem, which we demonstrate across many settings for the case of unstructured evidence citation. They additionally struggle to generate accurate unstructured evidence from their contexts. Our proposed dataset, \dataset{}, serves as a useful domain-agnostic synthetic dataset to help mitigate these issues. This intervention is at training time, meaning the inference cost is lower than for complex reasoning and inference chains. In addition to improving evidence quality, overall summary quality is improved. We hope this work can be built upon to help create more reliable, transparent, and useful summarization systems.

\section*{Acknowledgements}
DW is supported by a Danish Data Science Academy postdoctoral fellowship (grant: 2023-1425). LW is supported in part by the National Science Foundation through grant IIS-2046016. This research was co-funded by Pioneer Centre for AI, DNRF grant number P1.

\section*{Limitations}
 While our approach offers several benefits, there are notable areas to improve upon. Generating unstructured evidence directly can be prone to hallucination, while it is critical for the evidence to be exactly correct. A more precise RAG approach may offer some benefits. While shuffling during training helps the model to pull evidence more evenly, this also reduces the benefits in terms of evidence quality. A more targeted approach based on directly altering positional embeddings may be more appropriate for this \cite{DBLP:conf/acl/HsiehCL0LKGRLKP24}. We experiment with documents using a fixed number of sections in this study; allowing for variable-length documents could deliver greater improvements in performance. Additionally, we acknowledge potential prompt bias, influencing model outputs. Despite our efforts to mitigate these effects, they persist as a challenge, and using techniques such as APO~\cite{DBLP:conf/emnlp/PryzantI0L0023} could address these issues. Finally, while \dataset{} data is domain agnostic, it could be worth exploring how domain-aware data could help for more targeted applications (e.g., in the legal domain).

 \section*{Ethical Implications}
 LLMs are capable of generating convincing summaries from long contexts, and learning to generate unstructured supporting evidence from the source context can help improve their reliability and transparency. This approach is more flexible than the fixed-granularity approach, but generation will likely always be prone to errors. Validating that generated evidence is authentic is then crucial, as an incorrect citation presented as a ground truth fact could potentially be more harmful than no citation at all. 

 Additionally, synthetic data is clearly useful for learning to cite unstructured evidence. But synthetic data comes with its own ethical issues, including plagiarism and copyright infringement. More work on LLM trust and safety is needed to effectively mitigate this, as we are benefitting technologically from unknowing people's free labor. 

\bibliography{custom}

\appendix

\section{List of Prompts}
\label{sec:prompts}

The full set of prompts used in this study are listed in the figures below.

\subsection{Synthetic Data Generation Prompts}
The prompts used to generated synthetic data are given in \autoref{fig:synth_title_prompt} -- \autoref{fig:validation_prompt}.

\begin{figure*}[t]
    \centering
    \begin{sharp_box}
        \textbf{P1: Title Generation}\\
Imagine that you must write a book. This book can be either fiction or non-fiction.

You can select any subject to write your book about. Please make the book interesting.

Please write a list of 100 possible book titles. 

Please only generate the title for each book. 

Please include a mix of fiction and non-fiction, and please try to cover as many genres as possible.

Please make each book title unique.

Please make the style of each book title as different as possible, and don't repeat title styles.

Please generate titles for books which will have a broad range of appeal. 

Please generate titles for books which will require a broad range of reading levels.

Please try to make each title as different as possible.

Please do not include many titles with a colon (:).

\texttt{\{prev\_titles\_prompt\}}\\

**OUTPUT FORMAT**\\

Please separate each book title with a newline character (``\textbackslash n'')
    \end{sharp_box}
    \caption{Title generation prompt. \texttt{\{prev\_titles\_prompt\}} is filled with prompts of previously generated titles.}
    \label{fig:synth_title_prompt}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{sharp_box}
        \textbf{P2: Outline}\\
Imagine that you must write a book. This book can be either fiction or non-fiction.\\

This is the title of your book: \texttt{\{title\}}\\

Please write an outline of this book. Please include the title of the book, and a list of chapters or sections that the book will contain. The book should have 6 sections or chapters.\\

**OUTPUT FORMAT**\\

Please output the outline as a JSON object where the keys are the chapters and the values are a brief outline of the chapter.\\ 

In other words, as:\\

\textasciigrave \textasciigrave \textasciigrave python\\
\{
`Chapter 1': `Chapter 1 outline',\\
`Chapter 2': `Chapter 2 outline',\\
...\\
`Chapter N': `Chapter N outline'\\
\}
\textasciigrave \textasciigrave \textasciigrave
    \end{sharp_box}
    \caption{Outline generation prompt. The \texttt{\{title\}} field is replaced with the title of one document.}
    \label{fig:synth_outline_prompt}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{sharp_box}
    \textbf{P3.1: Queries Prompt}\\
Imagine that you must write a book. You are given the following outline of the book\\

\texttt{\{outline\}}\\

Please write a list of 5 questions about the book which summarize the book.\\

Please try to cover different general aspects of the content.\\

Please make the questions very concise.\\

**OUTPUT FORMAT**\\

Please separate each question with a single newline character (``\textbackslash n'')
    \end{sharp_box}
    \caption{Query generation prompt. The \texttt{\{outline\}} is filled with the outline generated by \autoref{fig:synth_outline_prompt}.}
    \label{fig:query_prompt_1}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{sharp_box}
    \textbf{P3.2: Initial Summaries and Evidence}\\
Imagine that you are writing a book. This is an outline of the book\\

\texttt{\{outline\}}\\

Please address the following question about the book:\\

\texttt{\{question\}}\\

Please write a summary which addresses the question. Please make the summary as specific and detail oriented as possible. Please include actual examples from the book when possible. Please do not write more than is absolutely necessary.\\

After you write the summary, please write exact quotes and passages you will include in the book, from which the summary could be written. Please include at least
\texttt{\{n\_evidence\}} of these passages, which you intend to include verbatim in the book. Please indicate the exact chapter where the passages will be written in a separate field.\\

**OUTPUT FORMAT**\\

Please a JSON object with two fields: ``summary'', ``evidence'', and ``chapter''. The summary field should have the summary. The evidence field should have a list of evidence sentences from the book. The chapter field should have the exact chapter where the corresponding evidence sentence will appear. Please only indicate the chapter number for this field. There should be the same number of elements in the ``evidence'' field as there are in the ``chapter'' field. In other words, as:\\

\textasciigrave \textasciigrave \textasciigrave python\\
\{\\
`summary': `Summary text',\\
`evidence': $[$`evidence sentence 1', `evidence sentence 2', ...$]$\\
`chapter': $[$1, 4, ...$]$\\
\}\\
\textasciigrave \textasciigrave \textasciigrave
 
    \end{sharp_box}
    \caption{Initial summary and evidence generation prompt. The \texttt{\{outline\}} and \texttt{\{question\}} fields are filled by the output of the previous prompts, while the \texttt{\{n\_evidence\}} field is filled by a random number between 5 and 10.}
    \label{fig:summ_ev_prompt_1}
\end{figure*}


\begin{figure*}[t]
    \centering
    \begin{sharp_box}
    \textbf{P4.1: Document Section Generation}\\
Imagine that you must write a book. You are given the following outline of the book\\

\texttt{\{outline\}}\\

Please write the following chapter of the book in its entirety:\\

\texttt{\{chapter\}}\\

Please also include the following sentences somewhere in the chapter. You must include these passages verbatim (i.e., EXACTLY as is). It is imperative that you do this, otherwise the book will be incomplete:\\

\texttt{\{evidence\}}\\

**OUTPUT FORMAT**\\

Please wrap the content of the chapter you write in a markdown codeblock, in other words, like:\\

\textasciigrave \textasciigrave \textasciigrave \\
content\\
\textasciigrave \textasciigrave \textasciigrave \\
    \end{sharp_box}
    \caption{Document section generation prompt. The \texttt{\{chapter\}} field is filled by the title of the section being generated, as given in the outline.}
    \label{fig:document_section_prompt_1}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{sharp_box}
    \textbf{P4.2: Evidence Retrieval Prompt}\\
Please read the following book chapter:\\

\texttt{\{chapter\}}\\

The following passage should have been included in the chapter but was not:\\

\texttt{\{passage\}}\\

Please retrieve the passage from the chapter which is CLOSEST to the given passage.\\

**OUTPUT FORMAT**\\

Please wrap the passage in a markdown codeblock, in other words, like:\\

\textasciigrave \textasciigrave \textasciigrave \\
passage\\
\textasciigrave \textasciigrave \textasciigrave
    \end{sharp_box}
    \caption{Prompt to retrieve evidence from the document when previously generated evidence is not included verbatim. The \texttt{\{passage\}} field is filled with one piece of evidence that was supposed to be included in the section.}
    \label{fig:evidence_retrieval_prompt}
\end{figure*}


\begin{figure*}[t]
    \centering
    \begin{sharp_box}
    \textbf{P5.1: Refinement Prompt}\\
Imagine that you are giving an exam about a book. This is the book\\

\texttt{\{book\}}\\

On an exam, you are asked to summarize the book with respect to this question:\\

\texttt{\{question\}}\\

This is the summary that you are grading:\\

\texttt{\{summary\}}\\

Please rewrite this response so that it is totally accurate and fully addresses the question.\\

Please make the response as specific and detail oriented as possible. The following passages from the document should help in crafting the response:\\

\texttt{\{passages\}}\\

**OUTPUT FORMAT**\\

Please wrap the content of the summary you write in a markdown codeblock, in other words, like:\\

\textasciigrave \textasciigrave \textasciigrave \\
content\\
\textasciigrave \textasciigrave \textasciigrave
    \end{sharp_box}
    \caption{Summary refinement prompt after content has been generated. The \texttt{\{book\}} field is filled with the entire document, where each section is concatenated together. Other fields are filled with the output from the previous prompts.}
    \label{fig:refinement_prompt}
\end{figure*}


\begin{figure*}[t]
    \centering
    \begin{sharp_box}
    \textbf{P5.2: Citance generation}\\
Imagine that you have written a research essay about a book. You have also extracted passages from the book which you used to write the essay.\\

Your job is to add citations to the essay which properly reference the passages that you have extracted.\\

Here is the essay:\\

\texttt{\{essay\}}\\

And here are the evidence passages from the book, each of which is given a number:\\

\texttt{\{evidence\}}\\

Please add citations to all citation-worthy statements in the essay using the numbered evidence list, by indicating the citation numbers of the corresponding evidence. 
More specifically, add the citation number at the end of each relevant sentence in the essay before the punctuation mark e.g., `This work shows the effectiveness of problem X $[$1$]$.' when the passage $[$1$]$ in the evidence list provides full support for the statement. 
Only add a citation if it is fully relevant and unambiguously supportive of that sentence. Not all evidences may be relevant, so only cite those that directly support the statement. 
Please do not add any explanations or justifications for the evidence, simply indicate the evidence numbers if they are relevant. 
If a sentence does not use any of the provided evidence, please simply copy the sentence as is and do not add anything to the end of it. 
If multiple evidences support a statement, please cite them together (e.g., $[$1$]$$[$2$]$). 
For each citation-worthy statement, you only need to add at least one citation, so if multiple evidences support the statement, just add the most relevant citation to the sentence.\\

    \end{sharp_box}
    \caption{Prompt to add citation references to sentences based on extracted evidence. The \texttt{\{essay\}} field is filled with a summary and the \texttt{\{evidence\}} field is filled with its corresponding evidence.}
    \label{fig:citance_prompt}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{sharp_box}
    \textbf{P6: Validation Prompt}\\
Imagine that you are judging the quality of a summary of a book. This is the book\\

\texttt{\{book\}}\\

Here is a question about the book:\\

\texttt{\{question\}}\\

And here is the summary which addresses the question:\\

\texttt{\{summary\}}\\

Please judge if you think that the summary meets ALL of the following criteria:\\

1) The summary is absolutely faithful to the book (in other words, all of the information in the summary is contained in the book)\\

2) The summary FULLY addresses the question\\

Please think carefully about your answer. If you think that ALL of the criteria are met, please simply respond with ``YES''.\\ 

Otherwise, please simply respond with ``NO''.
    \end{sharp_box}
    \caption{Prompt to add citation references to sentences based on extracted evidence. Fields are filled with the output of previous prompts.}
    \label{fig:validation_prompt}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{sharp_box}
    \textbf{Baseline Non-Pipelined Prompt}\\
Imagine that you must write a book. This book can be either fiction or non-fiction.\\

You can select any subject to write your book about. Please make the book interesting.\\

Please perform the following tasks and output everything in as a JSON object:\\

Please write the title of the book. \\
\texttt{\{title\_prompt\}}\\

Then, please write an outline of this book. Please include a list of chapters or sections that the book will contain. The book should have 6 sections or chapters.\\

Then, please write a list of 5 questions about the book which summarize the book.\\

Then, please write a summary for each question which addresses the question.\\

Then, please write the entire contents of the book. The book should be long, and you should write out the ENTIRE content.\\

Then, extract specific passages from the book for each summary which serve as evidence for the summary.\\


**OUTPUT FORMAT**\\
Please create a well-formatted JSON object with the following fields:\\

title: The title of the book (formatted as a string)\\
outline: The outline of the book (formatted as a string)\\
questions: The questions about the book (formated as a list)\\
summaries: The summaries addressing each question (formatted as a list of the same length as ``questions'')\\
document: The full book (formatted as a string)\\
evidence: A list of evidence passages (formatted as a list of the same length as ``questions'')\\
    \end{sharp_box}
    \caption{Baseline non-pipelined prompt that we use as a point of comparison. The field \texttt{\{title\_prompt\}} is empty for the baseline without diversity enforced, and filled with a list of previous titles and the prompt ``Please do not use any of the following titles:''.}
    \label{fig:baseline_prompt}
\end{figure*}

\subsection{Training and Inference Prompt}
The prompt used for training and inference is given in \autoref{fig:generation_prompt}

\subsection{Evaluation Prompts}
The prompt used to measure relevance is given in \autoref{fig:relevance_prompt} and the prompt used to measure consistency is given in \autoref{fig:consistency_prompt}.

\begin{figure*}[t]
    \centering
    \begin{sharp_box}
        \textbf{Training and Inference Prompt}\\
        
        Your task is to read a document and then write an essay which addresses the following question: \texttt{\{question\_text\}} \\

To write your essay, you should read the document and identify key passages which will help guide your response. Extract every passage which is directly relevant for your essay. Please copy each extracted passage to a list in the format specified below. Please copy the exact text of each passage (do NOT paraphrase!). Then, write your essay which addresses the query. 
\\

Please add citations to all citation-worthy statements using the extracted evidence, by indicating the citation numbers of the corresponding evidence. More specifically, add the citation number at the end of each relevant sentence before the punctuation mark e.g., `This work shows the effectiveness of problem X [1].' when the passage [1] in the evidence list provides full support for the statement. Only add a citation if it is fully relevant and unambiguously supportive of that sentence. Not all evidences may be relevant, so only cite those that directly support the statement. Please do not add any explanations or justifications for the evidence, simply indicate the evidence numbers if they are relevant. If a sentence does not use any of the provided evidence, please simply copy the sentence as is and do not add anything to the end of it. If multiple evidences support a statement, please cite them together (e.g., [1][2]). For each citation-worthy statement, you only need to add at least one citation, so if multiple evidences support the statement, just add the most relevant citation to the sentence.
\\

Please limit to only 10 pieces of evidence.
\\

Here is the document: \texttt{\{context\}}
\\

**OUTPUT FORMAT**\\
Output your response as:\\
EVIDENCE:\\
$[$1$]$ Extracted passage 1\\
$[$2$]$ Extracted passage 2\\
...\\
$[$N$]$ Extracted passage N\\
RESPONSE:\\
response\\
    \end{sharp_box}
    \caption{Full prompt used for fine-tuning and inference. The \texttt{\{question\_text\}} field is filled with a single query, and the \texttt{\{context\}} field is filled with the document context.}
    \label{fig:generation_prompt}
\end{figure*}

\begin{figure*}
    \centering
    \begin{sharp_box}
    \textbf{Summary Combination Prompt}\\
Here is a list of summaries of different sections of a document with respect to the query ``\texttt{\{question\_text\}}'':\\

\texttt{\{context\}}\\

Please combine these summaries into a single summary which addresses the query. If a summary mentions that the query is not addressed, please ignore that summary. Please keep all relevant citations in the final summary. Here is a list of the original citations:\\

\texttt{\{evidence\}}\\
    \end{sharp_box}
    \caption{Prompt to combine section summaries into one final summary.}
\end{figure*}



\begin{figure*}[t]
    \centering
    \begin{sharp_box}
        \textbf{Relevance Prompt}\\
        You will be given one summary written for a document based on a query about that document.\\

Your task is to rate the summary on one metric with respect to the query.\\

Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\

Evaluation Criteria: Relevance (1-5) - selection of important content from the source. The summary should include only important information from the source document which is relevant for the query. Annotators were instructed to penalize summaries which contained redundancies, excess information, and information which does not address the query.\\

Evaluation Steps:\\

1. Read the query, the summary, and the source document carefully.\\
2. Compare the summary to the query and the source document and identify the main point of the document which is relevant to the query.\\
3. Assess how well the summary covers the main points of the source document which are relevant to the query, and how much irrelevant or redundant information it contains.\\
4. Assign a relevance score from 1 to 5.\\

Example:\\

Source Text:\\

\texttt{\{document\}}\\

Query:\\

\texttt{\{query\}}\\

Summary:\\

\texttt{\{summary\}}\\

Evaluation Form (scores ONLY): - \texttt{\{Relevance\}}
    \end{sharp_box}
    \caption{Relevance evaluation prompt from \cite{DBLP:journals/corr/abs-2410-23463}. The \texttt{\{document\}} field is filled with the document context and the \texttt{\{summary\}} field is filled with a summary. When used to evaluate summarization, the \texttt{\{query\}} field is filled with the query used to generate the summary. For citation evaluation, the \texttt{\{query\}} field and all references to queries are removed from the prompt.}
    \label{fig:relevance_prompt}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{sharp_box}
        \textbf{Consistency Prompt}\\
        You will be given one summary written for a document based on a query about that document.\\

Your task is to rate the summary on one metric.\\

Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\

Evaluation Criteria:\\

Consistency (1-5) - the factual alignment between the summary and the summarized source with respect to the query. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts.\\

Evaluation Steps:\\

1. Read the source document carefully and identify the main facts and details it presents with respect to the query.\\
2. Read the summary and compare it to the source document. Check if the summary contains any factual errors that are not supported by the source document.\\
3. Assign a score for consistency based on the Evaluation Criteria.\\

Example:\\

Source Text:\\

\texttt{\{document\}}\\

Query:\\

\texttt{\{query\}}\\

Summary:\\

\texttt{\{summary\}}\\

Evaluation Form (scores ONLY): - \texttt{\{Consistency\}}
    \end{sharp_box}
    \caption{Consistency evaluation prompt from \cite{DBLP:journals/corr/abs-2410-23463}. The \texttt{\{document\}} field is filled with the document context and the \texttt{\{summary\}} field is filled with a summary. When used to evaluate summarization, the \texttt{\{query\}} field is filled with the query used to generate the summary. For citation evaluation, the \texttt{\{query\}} field and all references to queries are removed from the prompt.}
    \label{fig:consistency_prompt}
\end{figure*}


\section{Full Dataset Descriptions}
\label{sec:datasets_appendix}
The test datasets we use in this study include:
 \paragraph{SQuALITY}~\cite{DBLP:conf/emnlp/WangPCPB22} is a single-document task created from public domain short sci-fi stories where expert annotators create original summaries, providing both an overall narrative and detailed responses to specific questions, challenging models to capture broad context as well as fine-grained information.
 \paragraph{LexAbSumm}~\cite{DBLP:conf/coling/SantoshAG24} is a single-document task which contains legal judgments from the European Court of Human Rights, focusing on aspect-specific summaries that distill complex legal arguments. 
 \paragraph{SummHay}~\cite{DBLP:conf/emnlp/LabanFXW24} is a multi-document task composed of large-scale ``haystacks'' of documents with embedded ``insights'' which are relevant to the queries.
 \paragraph{ScholarQABench}~\cite{openscholar} is a multi-document task focused on scientific literature, comprising expert-crafted queries and extended answers drawn from a broad corpus of open-access research papers.

\section{Data Availability Statement}
We create \dataset{} in this work, as well as the code to generate \dataset{}, which we release freely to the public under the MIT license.\footnote{\url{https://github.com/dwright37/unstructured-evidence-sunset}} The data are generated as sets of fiction and non-fiction books in English.

 \section{Model Descriptions}
 \label{sec:models_appendix}
Table \autoref{tab:models_description} presents the full set of Huggingface model identifiers for the LLMs used in our experiments. The model cards containing relevant information on number of parameters, context length, vocabulary size, etc. are available on their model page on the Huggingface website. All training and inference are performed using 1-2 Nvidia A100 GPUs with 48GB of memory. Prior to training we ran a brief hyperparameter search to find the parameters used in this study, sweeping over the following values (selected values in \textbf{bold}):
\begin{itemize}[noitemsep]
    \item Learning rate: $[$1e-6, 5e-4$]$ (\textbf{5e-5})
    \item Batch size: \{\textbf{2}, 4, 8, 16, 32\}
    \item Warmup steps: \{0, \textbf{10}, 50, 100, 150, 200, 300\}
    \item Train epochs: \{1, 2, 3, 4, 5, 8, \textbf{10}, 12, 20\}
    \item Lora rank: \{2, 4, 8, 12, \textbf{16}, 32\}
\end{itemize}

\input{latex/tables/models_description}

\section{Software Package Parameters}
\begin{itemize}[noitemsep]
    \item NLTK~\cite{DBLP:conf/acl/Bird06}: We use the punkt sentence tokenizer for sentence tokenization
    \item VLLM: We use top $p$ sampling at 90\% with a temperature of 1. for inference. We set maximum new generated tokens to 2,000
    \item OpenAI GPT 4o Mini: We use top $p$ sampling at 90\% with a temperature of 1 for all prompts except title generation (temperature set to 1.2) and filtering (deterministic highest probability token output)
\end{itemize}
    

\section{Evaluation Robustness}
\label{sec:evaluation_robustness}
We use autoraters (i.e. LLM as a judge) for much of our evaluation. While we use a previously validated prompting and modeling setup~\cite{DBLP:journals/corr/abs-2410-23463}, we use GPT 4o Mini as our autorater due to its high performance and low cost. This has the potential to bias some of our results, as autoraters tend to favor their own outputs. Additionally, more powerful models are available for a slightly higher cost. Therefore, we validated the robustness of GPT 4o mini as an autorater by taking a sample of 710 outputs summaries from our evaluation and re-evaluating them with DeepSeek-V3~\cite{liu2024deepseek}. We measure the Pearson's R correlation between the ratings (2 ratings per summary) given by GPT 4o mini and DeepSeek-V3, finding a strong correlation of 73.29. This indicates the robustness of our evaluation which relies on GPT 4o mini.


\end{document}
