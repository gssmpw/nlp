\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[review]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor,colortbl}
\usepackage{tcolorbox}
\usepackage{CJK}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage[font=small,skip=-5pt]{subcaption}


\title{Faithful Query Focused Summarization of Long Documents}


\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}


\begin{document}
\maketitle
\begin{abstract}

\end{abstract}

\section{Motivation}
A practical real-world use case of a summarization system is: given a large document (e.g. a scientific paper, legal text, or novella), generate a summary of one particular aspect of the document. For example, with a scientific paper one may wish to generate a summary which focuses on just the results, just the methods, or just the challenge that the paper addresses.

\section{Background}
Summarizing long documents remains challenging, even for modern large language models, which have been shown to attend to tokens unevenly across long contexts~\cite{DBLP:journals/tacl/LiuLHPBPL24,DBLP:journals/corr/abs-2404-01430} and are prone to hallucination~\cite{}. Even more challenging, query focused summarization (QFS) requires synthesizing and abstracting information spread throughout a large context with respect to a given query e.g. ``Describe the contributions of a given scientific study''. Different approaches have been proposed to perform this task, tackling large single documents~\cite{} to multiple documents~\cite{} to large text corpora in the context of retrieval~\cite{}.

\subsection{Uneven Context Attention}

\cite{DBLP:journals/tacl/LiuLHPBPL24}

\cite{DBLP:journals/corr/abs-2404-01430} show that instead of preferring just the early and late context, LLMs express a positional preference that is different across language models. Additionally, they empirically show that prompt-based solutions are insufficient for overcoming this positional bias.

\subsection{Approaches}

Approaches to single documents and small sets of documents can diverge from large corpora of multiple documents. With small sets of documents, it is still possible to use the full text as input to an LLM provided that the context window on which it is trained is sufficiently large (for example, Mistral family models~\cite{} or Unlimiformer~\cite{}). However, performance may degrade as LLMs are prone to attend more heavily to the beginning and end of long-context inputs~\cite{DBLP:journals/tacl/LiuLHPBPL24} and are prone to hallucination~\cite{}. As an alternative, one may progressively summarize a long document using an approach akin to map reduce, where documents are split into smaller chunks and recursively summarized (i.e., summarize all chunks, then summarize the summaries, and continue this process until a single summary remains)~\cite{}. 

While map reduce can potentially be used for QFS with very large corpora, this can be extremely inefficient and cumbersome. As such, a predominant approach is to use retrieval augmented generation (RAG) in order to first retrieve the information relevant to a given query, followed by addressing the query using the retrieved text. Two prominent recent approaches are RAPTOR~\cite{} and GraphRAG~\cite{}, both of which seek to build structured intermediate representations of the data before retrieval. RAPTOR does so by recursively clustering sentences in a given corpus and summarizing these clusters. On the other hand, GraphRAG proposes to build a knowledge graph by extracting entities and relations (thus building a knowledge graph), summarizing the entities in this graph using their constituent text snippets, finding communities of entities based on the graph structure and entity summaries, and finally summarizing the communities. 

\subsection{Problems with Previous Approaches}
There has been little work on evaluating and improving the faithfulness and factuality of summaries for QFS over long documents. 

\section{Our Approach}
Given recent work indicating that intermediate representations of long documents can help with summarizing them~\cite{}, we take the following approach towards faithful QFS over long documents:

\begin{enumerate}
    \item Construct a graph of the long document using RAPTOR~\cite{}, GraphRAG~\cite{}, or similar.
    \item Learn to attend to this graph based on a provided query in order to go beyond semantic search
    \item Generate a final summary using the attended graph as context
\end{enumerate}

Can we take a similar approach to RAPTOR but include some information about how abstract and how faithful a node is?

\section{Baseline Results}
\input{latex/tables/table1}

\subsection{Observations}
\begin{itemize}
    \item Clearly I need to work on the baselines a bit since they do not meet or beat the published baselines
    \item RAPTOR might not be immediately suited for highly abstract summaries -- but we may be able to build off of it
\end{itemize}

\section{Notes for References}
We use LoRA for fine-tuning~\cite{DBLP:conf/iclr/HuSWALWWC22}

\bibliography{custom}

\end{document}
