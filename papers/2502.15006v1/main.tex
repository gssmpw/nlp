\documentclass[conference]{IEEEtran}

\usepackage{times}

\usepackage[numbers]{natbib}

\usepackage[utf8]{inputenc}             \usepackage[T1]{fontenc}                \usepackage[bookmarks=true]{hyperref}
\usepackage{microtype}                  \usepackage{caption}
\usepackage{cancel}
\usepackage{bigints}
\usepackage{subcaption}
\usepackage{booktabs}

\usepackage{siunitx}

\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}

\usepackage{multicol}

\let\labelindent\relax
\usepackage{enumitem}
\setlist{leftmargin=4.0mm}

\usepackage{graphics} \usepackage{graphicx} \usepackage{amsmath} \usepackage{amssymb}  \usepackage{amsmath, amssymb,bbm,bm,mathtools}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,automata,backgrounds,petri}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{xcolor-material}
\usepackage{soul}
\usepackage{url}
\usepackage{subfiles}
\usepackage{cleveref}
\usepackage[]{mdframed}
\usepackage{float}

\usepackage{dsfont}

\Crefname{figure}{Fig.}{Figs.}

\mdfdefinestyle{ThmFrame}{linecolor=white,outerlinewidth=0pt,roundcorner=2pt,leftmargin=-3pt,rightmargin=-3pt,innertopmargin=2pt, innerbottommargin=6pt, innerrightmargin=8pt, innerleftmargin=8pt, skipabove=2pt,
    skipbelow=-4pt,
    backgroundcolor=MaterialBlueGrey100!15}

\DeclareMathOperator{\ESS}{ESS}
\DeclareMathOperator{\BoldESS}{\mathbf{ESS}}

\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}

\newcommand*\ind[1]{\mathds{1}_{#1}}

\renewcommand{\labelenumi}{\roman{enumi})}
\newcommand{\rpartial}{{\mathrm{\partial}}}
\newcommand{\rinf}{{\mathrm \inf}}
\newcommand{\rd}{{\mathrm d}}
\newcommand{\rdelta}{{\mathrm \delta}}
\newcommand{\rtr}{{\mathrm{tr}}}
\newcommand{\rP}{{\mathrm{P}}}
\newcommand{\rT}{{\mathrm{T}}}
\newcommand{\rvec}{{\mathrm{vec}}}
\newcommand{\rtau}{{\mathrm{\tau}}}
\newcommand{\va}{{\bf a}}
\newcommand{\vb}{{\bf b}}
\newcommand{\vc}{{\bf c}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\vp}{{\bf p}}
\newcommand{\dvx}{{\bf dx}}
\newcommand{\vdu}{{\bf du}}
\newcommand{\vdy}{{\bf dy}}
\newcommand{\vq}{{\bf q}}
\newcommand{\vt}{{\bf t}}
\newcommand{\vr}{{\bf r}}
\newcommand{\vs}{{\bf  s}}
\newcommand{\vf}{{\bf f}}
\newcommand{\vg}{{\bf g}}
\newcommand{\vG}{{\bf G}}
\newcommand{\vu}{{\bf u}}
\newcommand{\vv}{{\bf  v}}
\newcommand{\vI}{{\bf I}}
\newcommand{\vh}{{\bf h}}
\newcommand{\vk}{{\bf k}}
\newcommand{\vl}{{\bf l}}
\newcommand{\vdx}{{\bf dx}}
\newcommand{\vdw}{{\bf dw}}
\newcommand{\vw}{{\bf w}}
\newcommand{\vA}{{\bf A}}
\newcommand{\vR}{{\bf R}}
\newcommand{\vE}{{\bf E}}
\newcommand{\vL}{{\bf L}}
\newcommand{\vK}{{\bf K}}
\newcommand{\vd}{{\bf d}}
\newcommand{\vJ}{{\bf J}}
\newcommand{\vF}{{\bf F}}
\newcommand{\vM}{{\bf M}}
\newcommand{\vD}{{\bf D}}
\newcommand{\vN}{{\bf N}}
\newcommand{\vV}{{\bf V}}
\newcommand{\vH}{{\bf H}}
\newcommand{\vT}{{\bf T}}
\newcommand{\vC}{{\bf C}}
\newcommand{\vO}{{\bf O}}
\newcommand{\vQ}{{\bf Q}}
\newcommand{\vB}{{\bf B}}
\newcommand{\vS}{{\bf S}}
\newcommand{\vX}{{\bf X}}
\newcommand{\vY}{{\bf Y}}
\newcommand{\vP}{{\bf P}}
\newcommand{\tire}{\mathrm{tire}}
\newcommand{\rB}{\mathrm{B}}
\newcommand{\rC}{\mathrm{C}}
\newcommand{\rD}{\mathrm{D}}
\newcommand{\rank}{{\text{rank}}}
\newcommand{\vect}{{\text{vec}}}
\newcommand{\vxi}{{\mbox{\boldmath$\xi$}}}
\newcommand{\vpi}{{\mbox{\boldmath$\pi$}}}
\newcommand{\vdomega}{{\bf d\omega}}
\newcommand{\vlambda}{{\mbox{\boldmath$\lambda$}}}
\newcommand{\vBamma}{{\mbox{\boldmath$\Gamma$}}}
\newcommand{\VTheta}{{\mbox{\boldmath$\Theta$}}}
\newcommand{\VPhi}{{\mbox{\boldmath$\Phi$}}}
\newcommand{\vphi}{{\mbox{\boldmath$\phi$}}}
\newcommand{\VPsi}{{\mbox{\boldmath$\Psi$}}}
\newcommand{\ve}{{\mbox{\boldmath$\epsilon$}}}
\newcommand{\VSigma}{{\mbox{\boldmath$\Sigma$}}}
\newcommand{\valpha}{{\mbox{\boldmath$\alpha$}}}
\newcommand{\vmu}{{\mbox{\boldmath$\mu$}}}
\newcommand{\vbeta}{{\mbox{\boldmath$\beta$}}}
\newcommand{\vomega}{{\mbox{\boldmath$\omega$}}}
\newcommand{\vtau}{{\mbox{\boldmath$\tau$}}}
\newcommand{\vdtau}{{\mbox{\boldmath$d\tau$}}}
\newcommand{\vtheta}{{\mbox{\boldmath$\theta$}}}\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\vcalS}{{\mbox{\boldmath$\cal{S}$}}}
\newcommand{\vcalU}{{\mbox{\boldmath$\cal{U}$}}}
\newcommand{\vcalD}{{\mbox{\boldmath$\cal{D}$}}}
\newcommand{\vcalJ}{{\mbox{\boldmath$\cal{J}$}}}
\newcommand{\vcalE}{{\mbox{\boldmath$\cal{E}$}}}
\newcommand{\vcalF}{{\mbox{\boldmath$\cal{F}$}}}
\newcommand{\vcalL}{{\mbox{\boldmath$\cal{L}$}}}
\newcommand{\vcalZ}{{\mbox{\boldmath$\cal{Z}$}}}
\newcommand{\vcalG}{{\mbox{\boldmath$\cal{G}$}}}
\newcommand{\vcalN}{{\mbox{\boldmath$\cal{N}$}}}
\newcommand{\vcalM}{{\mbox{\boldmath$\cal{M}$}}}
\newcommand{\vcalH}{{\mbox{\boldmath$\cal{H}$}}}
\newcommand{\vcalC}{{\mbox{\boldmath$\cal{C}$}}}
\newcommand{\vcalO}{{\mbox{\boldmath$\cal{O}$}}}
\newcommand{\vcalP}{{\mbox{\boldmath$\cal{P}$}}}
\newcommand{\vcalB}{{\mbox{\boldmath$\cal{B}$}}}
\newcommand{\vcalA}{{\mbox{\boldmath$\cal{A}$}}}
\newcommand{\vcalg}{{\mbox{\boldmath$\cal{g}$}}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\T}{^\mathsf{T}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\calJ}{\mathcal{J}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\td}{\text{d}}

\newcommand{\fu}{\frac{\partial \vf}{\partial \vu}}

\DeclareMathOperator{\Tr}{Tr}

\newcommand{\Qb}{\mathbb{Q}}
\newcommand{\Pb}{\mathbb{P}}
\newcommand{\Fb}{\mathbb{F}}
\newcommand{\Hb}{\mathbb{H}}
\newcommand{\Rb}{\mathbb{R}}
\newcommand{\Xspace}{\mathcal{X}}
\newcommand{\snew}{\mathbf{x}^\prime}
\newcommand{\s}{\mathbf{x}}
\newcommand{\Pas}[2]{\mathcal{P}\left({#1}|{#2}\right)}
\newcommand{\Pol}[2]{\mathcal{U}\left({#1}|{#2}\right)}
\newcommand{\PolOpt}[2]{\mathcal{U}^{*}\left({#1}|{#2}\right)}
\newcommand{\PolPasT}[1]{\mathcal{P}\left({#1}\right)}
\newcommand{\PolT}[1]{\mathcal{U}\left({#1}\right)}
\newcommand{\traj}{\mathbf{X}}
\newcommand{\KL}[2]{\mathbb{D}_{\mathrm{KL}}\bigl({#1}\parallel {#2}\bigr)}
\newcommand{\costt}[1]{\mathcal{J}\left({#1}\right)}
\newcommand{\logb}[1]{\log\left({#1}\right)}
\newcommand{\expb}[1]{\exp\left({#1}\right)}
\newcommand{\val}[2]{\mathit{V}_{#2}\left({#1}\right)}
\newcommand{\zt}[2]{\Phi_{#2}\left({#1}\right)}
\newcommand{\nZ}[2]{\mathcal{G}_t[\Phi]\left({#1}\right)}
\newcommand{\ExP}[2]{\E_{{#1}}{\left[#2\right]}}
\newcommand{\vnu}{{\bf \nu}}
\newcommand{\pluseq}{\mathrel{+}=}

\newcommand{\TrM}[1]{\calG_{\vx_0}({#1}) }
\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.65}
\newcolumntype{M}{>{\centering\arraybackslash}m{\dimexpr.225\linewidth-2\tabcolsep}}
\newcolumntype{G}{>{\columncolor{Gray}}M}
\newcolumntype{N}{>{\centering\arraybackslash}m{\dimexpr.1\linewidth-2\tabcolsep}}

\DeclareMathOperator*{\diag}{diag} \DeclareMathOperator*{\blkdiag}{blkdiag} \renewcommand{\Re}{\mathbb{R}}
\newcommand{\X}{\mathbf{x}}
\newcommand{\U}{\mathbf{u}}
\newcommand{\V}{\mathbf{v}}
\newcommand{\Y}{\mathbf{y}}
\newcommand{\D}{\mathbf{d}}
\newcommand{\W}{\mathbf{w}}
\newcommand{\e}{\bm{\epsilon}}
\newcommand{\Z}{\mathbf{z}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Tx}{\Tilde{x}}
\newcommand{\TX}{\Tilde{\mathbf{x}}}
\newcommand{\TF}{\Tilde{F}}
\newcommand{\half}{\mbox{$\frac{1}{2}$}}

\DeclareMathOperator{\Var}{Var}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\newcommand{\alert}[1]{{\color{red}[#1]}}

\newcommand{\note}[1]{{\small\em{}\color{orange}[ Note: #1]}}
\newcommand{\toedit}[1]{{\color{blue} [To Edit:] #1}}
\newcommand{\panos}[1]{{\color{red}[Panos says: #1]}}
\newcommand{\ji}[1]{{\color{orange}[Ji says: #1]}}

\newcommand{\beforetextbf}{\vspace{0.08\baselineskip}}

\theoremstyle{plain}
\newtheorem{property}{Property}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\makeatletter
\def\th@newremark{\th@remark\thm@headfont{\bfseries}}
\makeatletter

\theoremstyle{newremark}
\newtheorem{remark}{Remark}

\pdfinfo{
   /Author (Ji Yin)
   /Title  (Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions)
   /CreationDate (D:20250207120000)
   /Subject (Robots)
   /Keywords (MPC;Safety)
}
\IEEEoverridecommandlockouts                              

\title{Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions}

\author{Ji Yin$^{1*}$, Oswin So$^{2*}$, Eric Yang Yu$^{2}$, Chuchu Fan$^{2}$, and Panagiotis Tsiotras$^{1}$

\thanks{$^{1}$Ji Yin and Panagiotis Tsiotras are with D. Guggenheim School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA.
        Email:\footnotesize \{jyin81,tsiotras\}@gatech.edu}\thanks{$^{2}$Oswin So, Eric Yang Yu and Chuchu Fan are with the Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA. 
        {\footnotesize Email:\{oswinso,eyyu,chuchu\}@mit.edu}}\thanks{$*$ Equal contribution}
}

\begin{document}
\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
A common problem when using model predictive control (MPC) in practice is the satisfaction of safety specifications beyond the prediction horizon.
While theoretical works have shown that safety can be guaranteed by enforcing a suitable terminal set constraint or a sufficiently long prediction horizon, these techniques are difficult to apply and thus are rarely used by practitioners, especially in the case of general nonlinear dynamics.
To solve this problem, we impose a tradeoff between exact recursive feasibility, computational tractability, and applicability to ``black-box'' dynamics by learning an approximate discrete-time control barrier function and
incorporating it into a variational inference MPC (VIMPC), a sampling-based MPC paradigm.
To handle the resulting state constraints, we further propose a new sampling strategy that greatly reduces the variance of the estimated optimal control, improving the sample efficiency, and enabling real-time planning on a CPU.
The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial safety improvements compared to existing sampling-based MPC controllers, even under badly designed cost functions.
We validate our approach in both simulation and real-world hardware experiments.
\end{abstract}

\section{Introduction}
Model Predictive Control (MPC) is a versatile control approach
widely used in robotics applications such as autonomous driving \cite{InfoMPPI}, bio-inspired locomotion \cite{brasseur2015robust, snakegait},  or manipulation of deformable objects \cite{power2021keep}, to name just a few.
These methods address safety by incorporating state and control constraints into the finite-horizon optimization problem, ensuring that the system remains safe over the prediction horizon \cite{brasseur2015robust,lindqvist2020nonlinear,power2021keep,wang2021variational}.
However, safety of the system beyond the prediction horizon is often overlooked by practitioners, potentially leading to the violations of safety constraints at future timesteps.

This is a well-known problem in the field of MPC.
The question of whether a sequence of safe control actions can always be found under an MPC controller has been studied extensively in the literature under the name of recursive feasibility \cite{kerrigan2000invariant,mayne2000constrained,chen2003terminal,gondhalekar2009controlled,lofberg2012oops}.
A simple method of achieving recursive feasibility is by enforcing a control-invariant terminal set constraint at the end of the prediction horizon \cite{kerrigan2000invariant,chen2003terminal}.
However, it is difficult to find such a control-invariant set for general nonlinear systems.
Methods such as bounding the system dynamics \cite{chen2003terminal,bravo2005computation,fiacchini2007computation} often result in over-conservative sets, while methods that numerically solve the Hamilton-Jacobi PDE \cite{margellos2011hamilton} scale exponentially with the number of state variables, and hence this approach is computationally infeasible for systems with more than, say, $5$ state variables~\cite{mitchell2008flexible}. 

Even without considering recursive feasibility, nonlinear \textit{constrained} optimization is a challenging problem.
Traditionally, constraints are handled using techniques such as interior-point, sequential quadratic programming and augmented Lagrangian type methods \cite{nocedal1999numerical}, which mostly rely on accurate linear or quadratic approximations of the cost and constraints.
However, these gradient-based methods can get stuck in local minima or fail to converge when the problem is highly nonlinear.
Moreover, first and second-order gradient information is needed to solve these optimization problems, which is often not available for ``black-box'' systems.
Consequently, MPC controllers that rely on these underlying nonlinear constrained optimizers \cite{nocedal1999numerical,jallet2022constrained,gill2005snopt} inherit the same limitations.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/EfficientSamplingVsNormalSampling1.png}
    \caption{\textbf{Conventional sampling vs. the proposed Resampling-Based Rollout (RBR) approach.}
    In this \textsf{AutoRally} example, the blue dot must sample future trajectories and compute an optimal control while avoiding the black obstacles.
    Our method rewires all sampled trajectories to be safe, resulting in a more accurate sampling distribution, whereas the standard approach samples from a Gaussian distribution and wastes computations on unsafe trajectories.
    \label{fig:EfficientSamplingVsNormalSampling1}}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/nsmppi_diagram_nolr.pdf}
    \caption{
    \textbf{Overview of Neural Shield VIMPC (NS-VIMPC).}
    Offline, using DPNCBFs, we collect a dataset and train a NN approximation $B_\theta$ of a DCBF.
    $B_\theta$ is used to impose the DCBF descent condition \eqref{eqn:descent_condition2} as a state constraint.
    Online, we modify the VIMPC architecture to use resampling-based rollouts to improve the sampling distribution of the Monte Carlo estimator in the presence of the DCBF state constraints.
    In addition, we integrate the DCBF safety condition \eqref{eqn:descent_condition} into the optimization objective function as in \eqref{eq:cbfpenalty}.
    }
    \label{fig:control_architecture}
\end{figure*}

One way to address these previous limitations is to use sampling-based optimization methods,
which have become popular within both the robotics and model-based reinforcement learning communities.
In particular, variational inference MPC (VIMPC)
\cite{okada2020variational,lambert2020stein,wang2021variational,barcelos2021dual,power2022variational,power2024learning}
has emerged as a popular family of methods that pose the control problem as an inference problem instead of an optimization problem through the control as inference framework \cite{attias2003planning,toussaint2006probabilistic,rawlik2013stochastic}, and perform variational inference to approximate the resulting intractable optimal control distribution.
Popular MPC controllers within this family include Model Predictive Path Integral (MPPI) \cite{InfoMPPI} and the Cross-Entropy Method (CEM) \cite{rubinstein1999cross,wang2021variational},
which have been applied to robotics tasks such as autonomous driving \cite{InfoMPPI}, bipedal locomotion \cite{brasseur2015robust}, manipulation of deformable objects \cite{power2021keep}, and in-hand manipulation \cite{nagabandi2020deep}, among many others.
Using this framework, constraints can be easily handled by appropriately manipulating the posterior control distribution without facing the same optimization challenges as traditional gradient-based methods. 
However, the problem of safety beyond the prediction horizon still remains a challenge and has not been addressed by existing works that employ VIMPC.

In this work, we present a novel sampling-based MPC approach that provides safety beyond the prediction horizon by using control barrier functions to enforce the control-invariant set constraint for VIMPC controllers.
To tackle the challenge of finding control-invariant sets for general nonlinear systems, we extend our previous work on learning neural network approximations of control barrier functions using policy neural control barrier functions (PNCBF) \cite{so2023train} to the discrete-time case.
Inspired by particle filtering and sequential Monte Carlo methods, we further propose a novel sampling strategy for handling state constraints in VIMPC that significantly improves the sampling efficiency and enables real-time planning on a CPU.
Results from both simulation and hardware experiments suggest that the resulting Neural Shield-VIMPC (NS-VIMPC) controller outperforms existing MPC baselines in terms of safety, even under adversarially tuned cost functions.

\noindent\textbf{Contributions.} We summarize our contributions below.
\begin{itemize}
    \item 
    We extend policy neural CBF (PNCBF) to the discrete-time case and propose a novel approach to train a discrete-time PNCBF (DPNCBF) using policy evaluation.

    \item 
    We propose Resampling-Based Rollout (RBR), a novel sampling strategy for handling state constraints in VIMPC inspired by particle filtering, which significantly improves the sampling efficiency by lowering the variance of the estimated optimal control.

    \item 
    Simulation results on two benchmark tasks show the efficacy of NS-VIMPC compared to existing sampling-based MPC controllers in terms of safety and sample efficiency.

    \item 
    Hardware experiments on AutoRally \cite{AutorallyHardware}, a $1/5$ scale autonomous driving platform, demonstrate the robustness of NS-VIMPC to unmodeled dynamical disturbances under adversarially tuned cost functions. 
\end{itemize}

\section{Related Work}
\noindent\textbf{Sampling-based MPC.} 
Sampling-based MPC has become a popular alternative to traditional MPC methods that use gradient-based solvers, in part due to the advent of parallel computing and the recent advances in GPU hardware.
By not using gradients, sampling-based MPC can be applied to any problem without having specific requirements on the problem structure.
MPPI \cite{InfoMPPI} is a popular sampling-based MPC approach that formulates a variational inference problem, then solves it in the case of the Gaussian distribution, having strong connections to stochastic optimal control \cite{TubeMPPI} and maximum entropy control \cite{so2022maximum}.
Separately, the Cross-Entropy Method (CEM) \cite{rubinstein1999cross} has become popular in the reinforcement learning community \cite{wang2021variational}, in part due to its simplicity.
Both approaches were recently shown to be part of the VIMPC family of MPC algorithms~\cite{okada2020variational}. 
Consequently, some works have looked at expanding the VIMPC family to include different choices of divergences \cite{wang2021variational} and sampling distributions beyond Gaussians \cite{lambert2020stein,okada2020variational}.

\beforetextbf{}

\noindent\textbf{Safety in Sampling-based MPC.}
Research efforts such as \cite{RAMPPI} and \cite{UncertaintyPushingMPPI} assess the risk associated with uncertain areas in the state space during the exploration phase, and enhance MPPI's safety by incorporating a risk penalty into its cost function. 
While these methods empirically enhance safety, they lack formal assurances. 
Another class of MPPI alternatives, for instance, \cite{TubeMPPI, L1Adaptive}, leverage an auxiliary tracking controller to follow the MPPI output trajectories, improving robustness against unforeseeable disruptions, but these improvements are limited when the simulation-to-reality gap is significant. 
More recently, the use of Control Barrier Functions (CBF)~\cite{MPPI-CBF,Shield-MPPI} has offered formal safety guarantees for MPPI controllers. 
Nonetheless, \cite{MPPI-CBF}  is restricted to linear systems with saturating CBFs, and also neglects control input limits. 
This can lead to Gaussian control distributions constrained by a CBF-based chance constraint, increasing the likelihood of local optima due to limited exploration. 
In contrast,~\cite{Shield-MPPI} and ~\cite{yin2024chance} incorporate a CBF into the cost function and resolve a local optimization problem to reinforce CBF safety conditions for more complex nonlinear systems. 
However, the approach in ~\cite{Shield-MPPI} and ~\cite{yin2024chance} uses a distance-based CBF while ignoring the control limits. Our proposed Neural Shield MPPI (NS-MPPI) controller, as a specific form of the proposed NS-VIMPC framework, effectively resolves the critical issues identified in \cite{MPPI-CBF} and \cite{Shield-MPPI}.

\beforetextbf{}

\noindent\textbf{Different Proposal Distributions for VIMPC.}
There are many works that investigate the sampling distribution of VIMPC.
The MPPI variant in~\cite{CCMPPI} uses covariance steering to assign a terminal covariance to the sampling distribution, but this relies on expert knowledge on how the covariance should be designed.
Reference~\cite{power2022variational} learns a normalizing flow to approximate the optimal distribution, but it does not address the problem of recursive feasibility on its own.
Another direction looks at changing the effective sampling distribution by modifying the underlying dynamics system to be more amenable to calculations.
The works \cite{TubeMPPI, L1Adaptive} leverage an auxiliary tracking controller, thus changing the sampling distribution to be the output of the stable tracking controller. However, this requires the construction of such an auxiliary tracking controller, which can be difficult to perform for arbitrary nonlinear discrete-time systems.
In contrast, our proposed resampling-based rollouts (RBR) can be viewed as a way of easily improving the proposal distribution \textit{without} the need for any specialized problem structure.

\beforetextbf{}

\noindent\textbf{Duality between Control and Inference.}
The proposed resampling strategy in our work is similar in spirit to \cite{xie2020factor,qadri2022incopt} in that
factor graphs, a method used originally for estimation, is adapted for control purposes.
In \cite{zhang2023optimal} a linear optimal control was used to improve the performance of particle filters.
However, to the best of our knowledge, our approach is the first to adopt the resampling mechanism from particle filters
to improve the performance of sampling-based MPC controllers.

\beforetextbf{}

\noindent\textbf{Control Barrier Functions.} 
Designing CBFs with control-invariant safe sets is not a trivial task. As a result, previous works only seek saturating CBFs that do not consider input constraints \cite{Robotarium, lindemann2018control, xu2018safe}. Some works use hand-tuned CBFs to prevent saturation \cite{wei2022safe, clark2021verification}, which can lead to overly conservative safe sets. Other works develop CBFs with input constraints for specific types of systems \cite{cortez2022safe, cortez2021robust}. The emerging neural CBFs \cite{zhang2023neural, yu2023sequential, qin2021learning, zhang2024gcbf+} allow for more general dynamics utilizing machine learning techniques; however, they can still saturate the control limits. 
Our novel approach trains a neural CBF, conditioned on the entire system state and accounting for control limits,  so as to discover a large control-invariant set. 
The proposed novel NS-VIMPC control framework utilizes this Neural CBF to improve the efficiency of sampling by redirecting control sequences to previously identified safe states, thereby focusing sampling efforts in secure regions. 
This efficient sampling method will be discussed in detail later in Section~\ref{sec:NSMPPI}. 
 As a result, NS-VIMPC tackles the challenge of exploration versus exploitation by adaptively distributing computational resources, governed by an innovative neural CBF.

\section{Problem Formulation}
We consider the discrete-time, nonlinear dynamics
\begin{align}\label{dynamics}
    x_{k+1} = f(x_k, u_k),
\end{align}
with state $x\in \mathcal{X} \subseteq \mathbb{R}^{n_x}$ and control $u\in \mathcal{U} \subseteq \mathbb{R}^{n_u}$.
Let $\mathcal{U}^K$ denote the set of control trajectories of length $K$.
Following the MPC setup, we assume that a cost function $J : \mathcal{U}^K \to \mathbb{R}$ encoding the desired behavior of the system is given.
Moreover, we consider state constraints defined by an 
\textit{avoid set} $\mathcal{A} \subset \mathcal{X}$ described as the superlevel set of some specification function $h$, i.e., 
\begin{equation} \label{eqn:AvoidSet}
    \mathcal{A} \coloneqq \{x \in \mathcal{X} \mid h(x) > 0\}.
\end{equation}
The goal is then to find a sequence of controls $\vu =
\{u_0,u_1,\ldots,u_{K-1}\} \in \mathcal{U}^K$ that minimizes the cost function $J$ while satisfying the system dynamics \eqref{dynamics} and safety constraints $x_k \not\in \mathcal{A}$ 
for all $k \geq 0$.

\subsection{Variational Inference MPC For Sampling-based Optimization}

To solve the above optimization problem, we deviate from traditional MPC solvers and use a variational inference MPC formulation to solve the problem via sampling-based optimization.
To this end, we make use of the control-as-inference framework \cite{levine2018reinforcement} to model the problem.
Specifically, let $o$ be a binary variable that indicates ``optimality'' such that, for all controls $\vu \in \mathcal{U}^K$,
\begin{equation} \label{eq:VI:opt_cond}
    p(o=1 \mid \vu) \propto \exp(-J(\vu)),
\end{equation}
We assume a prior $p_0(\vu)$ on the control trajectory. The ``optimal'' distribution can then be obtained via the posterior distribution
\begin{equation} \label{eq:VI:posterior}
    p(\vu \mid o=1) = \frac{p(o=1 \mid \vu)p_0(\vu)}{p(o=1)} = Z^{-1} \exp( -J( \vu )) p_0(\vu).
\end{equation}
where $Z := \int \exp(-J(\vu)) p_0(\vu) \diff{\vu}$ denotes the unknown normalization constant.
Since sampling from $p(\vu \mid o=1)$ is intractable, we use variational inference to approximate the posterior $p(\vu \mid o=1)$ with a tractable distribution $q_\vv(\vu)$ parametrized by some vector $\vv$ by minimizing the forward KL divergence, i.e.,
\begin{equation} \label{eq:VI}
    \min_{\vv} \quad \KL{ p(\vu \mid o=1) }{ q_\vv(\vu) }.
\end{equation}
In the special case of $q_\vv$ being a Gaussian distribution with mean $\vv$ and a fixed control input covariance $\Sigma$, intrinsic to the robotic system of interest \cite{InfoMPPI},
we can solve \eqref{eq:VI} in closed-form to obtain the optimal $\vv^*$ as (see \Cref{app:VI Gauss} for details)
\begin{equation} \label{eq:vi:v_opt_def}
    \vv^* = \E_{p(\vu \mid o=1)}[ \vu ].
\end{equation}
While this expectation cannot be readily computed because $p(\vu \mid o=1)$ is intractable to sample from,
we can use importance sampling to change the sampling distribution to some other distribution $r(\vu)$ that is easier to sample from, leading to
\begin{align}
    \vv^*
    &= \E_{r(\vu)}\left[ \frac{ p(\vu \mid o=1)}{r(\vu)} \vu \right] \\
    &= \E_{r(\vu)}\bigg[ \underbrace{\frac{Z^{-1} \exp(-J(\vu)) p_0(\vu)}{r(\vu)}}_{\coloneqq \omega(\vu)} \vu \bigg]. \label{eq:vi:omega_def}
\end{align}
Using samples $\vu^1, \dots, \vu^N$ drawn from $r$, we compute a Monte Carlo estimate $\hat{\vv}$ of the optimal control sequence $\vv^*$ (see \Cref{app:VI SNIS} for details) as follows,
\begin{align}
    \hat{\vv} &= \sum_{i=1}^N \tilde{\omega}^i \vu^i, \label{eq:vi:v_opt} \\
    \tilde{\omega}^i &\coloneqq \frac{\omega(\vu^i)}{\sum_{j=1}^N \omega(\vu^j)} . \label{eq:vi:omega_tilde}
\end{align}
Note that the $Z$ in $\omega(\vu)$ is canceled out in the computation of $\tilde{\omega}^i$ in \eqref{eq:vi:omega_tilde} and hence can be ignored.

\begin{mdframed}[style=ThmFrame]
\begin{remark}[Self-normalized importance sampling]
    Note that the weights $\tilde{\omega}^i$ in \eqref{eq:vi:v_opt} are \textbf{not} from the regular importance sampling estimates in \eqref{eq:vi:omega_def} due to the normalization by the sum of the weights in \eqref{eq:vi:omega_tilde}.
    Instead, \eqref{eq:vi:v_opt} is a self-normalized importance sampling estimator (SNIS), which uses an \textit{estimate} of the weights $\omega$ but results in a biased, though asymptotically unbiased, estimator.
    This fact is not present in many existing works on both VIMPC (e.g., \cite{okada2020variational,wang2021variational}) and MPPI (e.g., \cite{InfoMPPI}).
    See \Cref{app:VI SNIS} for more details.
\end{remark}
\end{mdframed}

\begin{mdframed}[style=ThmFrame]
\begin{remark}[Connections to MPPI and CEM]
    In the case where we choose $p_0(\vu) = q_{\bf 0}(\vu)$ and $r(\vu)=q_{\bar{\vv}}(\vu)$ for some previous estimate of the optimal control sequence $\bar{\vv}$, the above variational inference MPC framework reduces to MPPI (see \Cref{app:VI Gauss} for details).
    Moreover, Cross-Entropy Method (CEM) also falls in the VIMPC framework \cite{okada2020variational,wang2021variational}.
\end{remark}
\end{mdframed}

\subsection{Constraint Handling In Variational Inference MPC} \label{subsec:constr_handling}
One advantage of sampling-based MPC is that it is simple to incorporate hard constraints. 
One can include an indicator function in the cost function that heavily penalizes constraint violations (e.g., see \cite{InfoMPPI,TubeMPPI,barcelos2021dual,bhardwaj2022storm}). Specifically, for some large constant $C > 0$, we can modify the cost function as
\begin{equation} \label{eq:constr_handle}
    J_{\text{new}}(\vu) = J(\vu) + C \sum_{k=0}^{K} \ind{x_k \in \mathcal{A}}.
\end{equation}
From the inference perspective \eqref{eq:VI:opt_cond}, we can interpret $J_{\text{new}}$ \eqref{eq:constr_handle} as saying that $o = o_{\text{cost}} \land o_{\text{constraint}}$, where,
\begin{align}
    p(o_{\text{cost}} = 1 \mid \vu) &\propto \exp( -J(\vu) ), \\
    p(o_{\text{constraint}} = 1 \mid \vu) &\propto \exp\left( -C \sum_{k=0}^{K} \ind{x_k \in \mathcal{A}} \right). \label{eq:constr_handle:p_constr}
\end{align}
However, a problem with \eqref{eq:constr_handle:p_constr} is that it looks at safety only within the prediction horizon and does not consider the probability of staying safe beyond the prediction horizon. 

To tackle this problem, we will use a discrete-time control barrier function (DCBF), which we introduce in the next section, to enforce that the states remain within a control-invariant set.
\subsection{Discrete-time Control Barrier Functions (DCBF)} \label{sec:DCBFDefinition}
A discrete-time control barrier function (DCBF) \cite{Shield-MPPI} associated to the avoid set $\mathcal{A}$ is a function $B : \mathcal{X} \to \mathbb{R}$ such that\footnote{Note that we use the opposite sign convention as compared to \cite{Shield-MPPI}.}
\begin{subequations}\label{eqn:DCBFDefinition}
\begin{align}
\hspace{-.3em}B(x) &> 0, \quad \forall x \in \mathcal{A},\label{eqn:avoid_condition}\\
\hspace{-.3em}B(x) &\leq 0 \implies \inf_{u\in \mathcal{U}} B(f(x,u)) - B(x) \leq -\alpha (B(x)), \label{eqn:descent_condition}
\end{align}
\end{subequations}
where $\alpha$ is an extended class-$\kappa$ function \cite{xu2015robustness}.
As in \cite{Shield-MPPI}, we restrict our attention to the class of linear extended class-$\kappa$ functions, i.e.,
\begin{align}\label{eqn:KappaFunction}
\alpha(B(x)) = a \cdot B(x), \quad a \in (0, 1).    
\end{align}
The following theorem from \cite{Shield-MPPI} proves that a controller satisfying the condition \eqref{eqn:descent_condition} renders the sublevel set $\mathcal{S} = \{x \mid B(x)\leq 0\}$ forward-invariant.

\begin{theorem}[\protect{\cite[Property 3.1]{Shield-MPPI}}] \label{thm:forward_invariant}
    Any control policy  $\pi:\mathcal{X} \rightarrow \mathcal{U}$ satisfying the condition
    \begin{equation} \label{eqn:descent_condition2}
        B(f(x,\pi(x))) - B(x) \leq -\alpha (B(x)),
    \end{equation}
    renders the sublevel set $\mathcal{S} = \{x \mid B(x)\leq 0\}$ forward-invariant.
\end{theorem}
Hence, one way to guarantee recursive feasibility, and thus safety beyond the prediction horizon, is to enforce the condition \eqref{eqn:descent_condition2} at every time step of the optimization problem.
Another point to note is that $\mathcal{S}$ is a control-invariant set, and thus the constraint $B(x_K) \leq 0$ can be imposed as a terminal state constraint to guarantee recursive feasibility for MPC as is done classically \cite{kerrigan2000invariant,chen2003terminal}.
Thus, one can try to enforce these constraints by incorporating them into the cost function as in
\eqref{eq:constr_handle} \cite{Shield-MPPI}, i.e., modify the cost according to one of the following options,
\begin{align}
    J_{\text{new}}(\vu) &= J(\vu) + C \sum_{k=0}^{K} \ind{ B(f(x,u)) - B(x) > -\alpha (B(x)) }, \\
    J_{\text{new}}(\vu) &= J(\vu) + C \sum_{k=0}^{K} \Big[ B(f(x,u)) - B(x) + \alpha( B(x) ) \Big]_+. \label{eq:cbfpenalty}
\end{align}
However, this approach has two problems. 
First, for sufficiently large $C$, samples that violate the DCBF constraint will have a normalized weight of near zero, rendering these samples useless.
Conservative DCBFs may cause the majority of the samples to violate the constraint \eqref{eqn:descent_condition2} and hence have zero weight, resulting in poor estimates of the optimal control and wasted computation.
Second, while constructing a function $B$ that satisfies \eqref{eqn:avoid_condition} is relatively simple, it is much harder to construct a function $B$ that also satisfies \eqref{eqn:descent_condition}, contrary to the case with (continuous-time) control barrier functions \cite{so2023train,yu2023sequential} (see also \Cref{Remark3} below).
Consequently, many works that integrate control barrier functions into MPC often only propose functions for which \eqref{eqn:avoid_condition} holds and not \eqref{eqn:descent_condition} \cite{Shield-MPPI}, rendering the safety guarantees of \Cref{thm:forward_invariant} invalid.

In the next section, we address these two problems
via a novel resampling method that reuses computations from zero-weight samples and learns a DCBF that tries to respect both \eqref{eqn:avoid_condition} and \eqref{eqn:descent_condition}  using policy value functions.
\begin{mdframed}[style=ThmFrame]
\begin{remark}[Differences between discrete-time and continuous-time control barrier functions under unbounded controls]\label{Remark3}
    Note that in the continuous-time case where having an unbounded control space $\mathcal{U}$, control-affine dynamics, and a non-zero $\frac{\partial \dot{B}}{\partial u}$ are sufficient to guarantee that $B$ is a valid control barrier function. This is because \eqref{eqn:descent_condition} generally holds for a function $B$ that satisfies \eqref{eqn:avoid_condition} in the continuous-time case under these assumptions.
    However, the same does not apply to discrete-time under general nonlinear dynamics $f$ since \eqref{eqn:descent_condition2} is nonlinear in $u$, let alone the fact that robotic systems in real life are unable to exert infinite forces and hence generally do not have unbounded controls.
\end{remark}
\end{mdframed}

\begin{mdframed}[style=ThmFrame]
\begin{remark}[Constraint satisfaction in the variational inference framework] \label{rem:convex_thing}
    One potential issue with incorporating DCBF constraints into the cost function $J$ 
is that,
    while $p(\vu \mid o=1)$ may have zero density on the set of controls that violate the DCBF constraint,
    this does not necessarily hold for the approximating distribution $q$ since we are minimizing the \textit{forward} KL divergence \cite{jerfel2021variational}.
    In particular, the mean $\vv$ of $q_\vv$, which is the control to be used, may not satisfy the DCBF constraint.
    One way to guarantee that $\vv$ does satisfy the DCBF constraint is to assume that the set of controls that satisfy the DCBF constraint is itself convex (see \Cref{app:convex_controls}).
    However, this is an unrealistic assumption that is often violated by state constraints such as obstacle avoidance.

    Despite these shortcomings, we observed in our experimental results that this method of enforcing DCBF constraints on $p(\vu \mid o=1)$ indeed drastically improved safety.
    We leave further theoretical exploration of this issue as future work.
\end{remark}
\end{mdframed}

\section{Neural Shield VIMPC}\label{sec:NSMPPI}
In this section, we propose Neural Shield VIMPC (NS-VIMPC),
a sampling-based MPC paradigm that efficiently samples trajectories using a DCBF modeled using a neural network.
We illustrate the proposed NS-VIMPC algorithm in \Cref{fig:control_architecture}.

\subsection{Approximating DCBF Using Neural Policy Value Functions}\label{sec:ConstructingCBF}
Let $x^\pi_{k}$ denote the state at time $k$
following the control policy $\pi : \mathcal{X} \to \mathcal{U}$. Define the policy value function $V^{h, \pi}$ as,
\begin{align}\label{eqn:ValueFunctionDefinition}
    V^{h, \pi}(x_0) \coloneqq \max_{k \geq 0} h(x^\pi_{k}).
\end{align}
We then have the following theorem.
\begin{theorem}
    $V^{h, \pi}$ satisfies \eqref{eqn:avoid_condition} and \eqref{eqn:descent_condition} and is a DCBF.
\end{theorem}
\begin{proof}
From the definition of $V^{h,\pi}$ \eqref{eqn:ValueFunctionDefinition}, we have that
\begin{align}
    V^{h, \pi}(x_k) &\geq h(x_k), \label{eqn:ValueFunctionProperty1}\\
    V^{h, \pi}(x_k) &\geq V^{h, \pi}(f(x_k, \pi(x_k))). \label{eqn:ValueFunctionProperty2}
\end{align}
Using the definition of the avoid set $\mathcal{A}$ \eqref{eqn:AvoidSet} and \eqref{eqn:ValueFunctionProperty1}, it follows that $V^{h,\pi}(x) > 0$ for all $x \in \mathcal{A}$, satisfying the first condition \eqref{eqn:avoid_condition} of a DCBF.
When $V^{h,\pi}(x_k) \leq 0$, we have $-\alpha(V^{h,\pi}(x_k)) \geq 0$, and  \eqref{eqn:ValueFunctionProperty2} implies that,
\begin{equation}
    V^{h,\pi}(f(x_{k}, \pi(x_k))) - V^{h,\pi}(x_k) \leq 0 \leq -\alpha(V^{h,\pi}(x_k)).
\end{equation}
Since $\pi(x_k) \in \mathcal{U}$, this implies the second condition \eqref{eqn:descent_condition}.
Thus, the policy value function $V^{h,\pi}$ is a DCBF.
\end{proof}

Although we have constructed a DCBF from \eqref{eqn:ValueFunctionDefinition}, the challenge is that the policy value function $V^{h,\pi}$ cannot be easily evaluated at arbitrary states
since the maximization in \eqref{eqn:ValueFunctionDefinition} is taken over an infinite horizon.
To fix this, we train a neural network approximation $V_\theta^{h,\pi}$ of $V^{h, \pi}$, extending the approach of \cite{so2023train} to the discrete-time case.
To begin, we first rewrite \eqref{eqn:ValueFunctionDefinition} in a dynamic programming form,
\begin{equation} \label{eq:value_fn_dp}
    V^{h,\pi}(x_0) = \max\Big\{ \max_{0 \leq k \leq T} h(x_{k}^\pi),\; V^{h, \pi} (x^\pi_{T}) \Big\}.
\end{equation}
We can then train a neural network $V_\theta^{h,\pi}$ to approximate the value function $V^{h,\pi}$ by minimizing the loss
\begin{equation}\label{eqn:DPcost}
    L(\theta) = \norm{V^{h,\pi}_\theta (x_0) - \max \biggl\{ \max_{0 \leq k \leq T} h(x_{k}^\pi), V^{h, \pi}_\theta (x^\pi_{T})\biggr\} }^2,
\end{equation}
over all states $x_0$.
One problem, however, is that the minimizer of \eqref{eqn:DPcost} is not unique.
For example, if $h(x) \leq \bar{h}$ for all $x$, then $V_\theta^{h,\pi} = \bar{h}$ is a minimizer of \eqref{eqn:DPcost} but does not necessarily satisfy \eqref{eqn:ValueFunctionDefinition}.
To fix this, we follow the approach of \cite{fisac2019bridging,so2023solving} and, inspired by reinforcement learning \cite{sutton2018reinforcement}, introduce a discount factor $\gamma \in (0,1)$ to define the \textit{discounted} value function
\begin{equation} \label{eq:value_fn_dp_discounted}
    V^{h,\pi,\gamma}(x_k)
    = \max \Big\{ h(x_k^\pi),\; (1 -\gamma) h(x_k) + \gamma V^{h, \pi, \gamma} (x_{k+1}^\pi) \Big\},
\end{equation}
and the corresponding loss $L$,
\begin{align}\label{eqn:DPcostModified}
    L(\theta) &= \norm{V^{h,\pi,\gamma}_\theta (x_k) - \hat{V}^{h,\pi,\gamma}_\theta(x_k) }^2, \\
    \hat{V}^{h,\pi,\gamma}_\theta(x_k) &= \max \biggl\{ h(x_k^\pi), (1-\gamma) h(x_k^\pi) + \gamma V^{h, \pi,\gamma}_\theta (x^\pi_{k+1})\biggr\}.
\end{align}
Finally, instead of using the learned $V^{h,\pi,\gamma}_\theta$ directly in \eqref{eqn:descent_condition2}, we first take the maximum with $h$ and use $\tilde{V}^{h,\pi,\gamma}_\theta$ defined as
\begin{equation}
   \tilde{V}^{h,\pi,\gamma}_\theta(x) \coloneqq \max\{ h(x), V^{h,\pi,\gamma}_\theta(x) \}.
\end{equation}
This guarantees that $\tilde{V}^{h,\pi,\gamma}_\theta(x) \geq h(x)$
and hence the zero sublevel set of $\tilde{V}^{h,\pi,\gamma}$ will be a subset of $h$.
Hence, imposing the state constraint $\tilde{V}^{h,\pi,\gamma}(x) \leq 0$ will, at the very least, prevent violations of the original state constraints during the prediction horizon,
and potentially also induce a state constraint that is closer to the true control-invariant set than the original sublevel set of $h$.

\begin{mdframed}[style=ThmFrame]
\begin{remark}[Neural Network Verification of DCBFs]
    We emphasize that our goal here is to obtain a good approximation of a DCBF $B_\theta$ using a neural policy value function $V^{h,\pi,\gamma}_\theta$, and not necessarily to obtain a true DCBF.
    Verifying whether the learned $V^{h,\pi,\gamma}_\theta$ is a true DCBF requires neural network verification which can be intractable or inconclusive (see Appendix 1 in \cite{katz2017reluplex} on the NP-completeness of the NN-verification problem).
    This is especially true in the discrete-time case where the condition \eqref{eqn:descent_condition2} may not be affine in the control $u$.
\end{remark}
\end{mdframed}

Nevertheless, as we show later, empirical results show that using an approximation of a DCBF is sufficient for enabling the use of much shorter prediction horizons without sacrificing safety.

\subsection{Efficient Sampling Using Resampling-based Rollouts}\label{sec:Efficient_Trajectory_Sampling}

We tackle the problem of wasted samples with zero weights by drawing inspiration from the sequential Monte Carlo \cite{doucet2001introduction} and particle filter \cite{gordon1993novel} literature, and by performing a per-timestep resampling during the rollout, which we call Resampling-Based Rollouts (RBR).
Specifically, the control-as-inference problem formulation \eqref{eq:VI:opt_cond} gives us a \textit{temporal} decomposition of $p(o_{\text{constraint}}=1 \mid \vu)$ \eqref{eq:constr_handle:p_constr} in the case of state constraints when $C \to \infty$, as follows
\begin{equation}
    p(o_{\text{constraint}}=1 \mid \vu)
    \propto \prod_{k=0}^{K} \ind{x_k \not\in \mathcal{A}}.
\end{equation}
Hence, we can write the posterior $p(\vu \mid o=1)$ as
\begin{equation}
    p(\vu \mid o=1)
    \propto 
    p(o_{\text{cost}} = 1 \mid \vu)
    p(\vu)
    \left(\prod_{k=0}^{K} \ind{x_k \not\in \mathcal{A}} \right).
\end{equation}
By treating the term on the right as a ``measurement model,'' it allows us to use particle filtering \cite{gordon1993novel} to solve the control-as-inference problem.
The update of the particle filter weights $\hat{w}^i_k$ at time step $k$ for particle $i$ is written as
\begin{equation} \label{eq:pf_weight_update}
    \hat{w}^i_k = \hat{w}^i_{k-1} \ind{x_k \not\in \mathcal{A}}.
\end{equation}
Then, we \textit{resample} the particles with probability proportional to their weights $\hat{w}^i_k$ to obtain a new set of particles $\vu^i$.
Due to the indicator function in the weight update \eqref{eq:pf_weight_update}, the weights are either $0$ or $1$.
Assuming there exists a particle that satisfies the state constraint, applying systematic resampling \cite{kitagawa1996monte} results in ``rewiring'' particles that violate the constraint to particles that still maintain safety, reusing the computation from zero-weight samples (see \Cref{fig:resampling}).
However, when all particles violate the constraint, we do not resample.
In this case, since we want to minimize constraint violations, we still use the cost term \eqref{eq:cbfpenalty} such that trajectories with higher constraint violations have higher costs.

\begin{figure}[]
    \centering
    \includegraphics[width=\linewidth]{figs/EfficientSampling.png}
    \caption{\textbf{Resampling-based Rollouts (RBR).} Inspired by particle filtering, at each step of the trajectory rollout, we uniformly resample any samples that violate the state constraints among the set of safe samples. In this example, since the state $x^a_1$ violates constraints, it has a weight $\hat{w}^a_1=0$. The control $u^a_1$ is rewired to state $x^c_1$ (since $\hat{w}^b_1 = \hat{w}^c_1 = 1$, $x^b_1$ and $x^c_1$ has equal chance of getting $u^a_1$), resulting in a new sample state $x^a_2$.}
    \label{fig:resampling}
\end{figure}
While we can prove that this resampling is unbiased from the particle filter perspective,
we also provide a more direct proof of this fact without the analogy to particle filters.
\begin{mdframed}[style=ThmFrame]
\begin{theorem} \label{thm:same_marginal}
    For a probability density function $\mathsf{f}$ and set $\mathsf{S}$, let $\mathsf{a}$ be sampled from the conditional density $\mathsf{f}(\mathsf{x} \mid \mathsf{x} \in \mathsf{S})$, and let $\mathsf{b}$ be sampled from the unconditional density $\mathsf{f}$, such that $\mathsf{a}$ and $\mathsf{b}$ are independent.
    Define the ``rewired'' random variable $\tilde{\mathsf{b}}$ to be equal to $\mathsf{b}$ if $\mathsf{b} \in \mathsf{S}$ and $\mathsf{a}$ otherwise, i.e.,
    \begin{equation}
        \tilde{\mathsf{b}} =
        \mathbbm{1}_{\mathsf{b} \in \mathsf{S}} \mathsf{b} + 
        \mathbbm{1}_{\mathsf{b} \not\in \mathsf{S}} \mathsf{a}
    \end{equation}
    Then, $\tilde{\mathsf{b}}$ is also sampled from the conditional density $\mathsf{f}(\mathsf{x} \mid \mathsf{x} \in \mathsf{S})$, such that $\tilde{\mathsf{b}}$ and $\mathsf{a}$ have the same distribution, $\tilde{\mathsf{b}} \overset{d}{=} \mathsf{a}$.
\end{theorem}
\end{mdframed}
The proof is given in \Cref{sec:proof:same_marginal}. Consequently, we can use $\tilde{\mathsf{b}}$ in a Monte Carlo estimator and still obtain unbiased estimates, as shown in the following Corollary (proof is given in \Cref{sec:proof:resample_unbiased}).
\begin{mdframed}[style=ThmFrame]
\begin{corollary} \label{thm:resample_unbiased}
    For any function $w$, the Monte Carlo estimate of $\E[w( \mathsf{x} )]$ under the conditional density $\mathsf{f}(\mathsf{x} \mid \mathsf{x} \in \mathsf{S})$ using random variables $\mathsf{a}$ and $\tilde{\mathsf{b}}$ is unbiased, i.e.,
    \begin{equation}
        \E[\frac{1}{2} w( \mathsf{a} ) + \frac{1}{2} w( \tilde{\mathsf{b}} )] = \E[w( \mathsf{x} )].
    \end{equation}
\end{corollary}
\end{mdframed}

Intuitively, resampling improves the efficiency of the Monte Carlo estimator, since most of the samples do not violate the state constraints and hence contribute to the weighted sum with non-zero weight.
We can also theoretically prove that resampling improves the variance of the resulting Monte Carlo estimator.
In the following theorem, we show how resampling reduces the \textit{exponential} growth of the variance on the prediction horizon to a constant factor in the limit as the number of samples $N$ goes to infinity.

\begin{mdframed}[style=ThmFrame]
\begin{theorem}\label{thm:resample_variance}
    Let the horizon $K > 0$, consider $\mathcal{U} = [-1, 1]$, and define the avoid set and the dynamics such that $\vu \in [0, 1]^K$ is safe, and unsafe otherwise.
    Let the prior distribution $p(\vu)$ be uniform on $[-1, 1]^K$ and let $p(o=1\mid\vu)$ be the indicator function for $\vu \in [0, 1]$ such that the posterior distribution $p(\vu \mid o=1)$ is uniform on $[0, 1]^K$, and the proposal distribution $r(\vu) = 1/2^K$ is uniform on $\mathcal{U}$.
    Then, the variance of the Monte Carlo estimator of the optimal control law
    \begin{equation}
        \hat{\vv} = \frac{1}{N} \sum_{i=1}^N \frac{ p( \vu^i \mid o=1) }{ r(\vu^i) } \vu^i,
    \end{equation}
    grows exponentially in $K$, i.e.,
    \begin{equation}
        \Var[\hat{v}_k] = \frac{1}{N}\left( \frac{1}{3} 2^K - \frac{1}{4} \right), \qquad k = 1, \dots, K.
    \end{equation}
    Using resampling, the variance is upper-bounded by,
    \begin{equation}
        \Var[\hat{v}_{k,\mathrm{resample}}] = O\left( \left(\frac{1}{1 - 2^{-N}}\right)^{K} + (1-2^{-N})^{K} \right).
    \end{equation}
\end{theorem}
\end{mdframed}
The proof of \Cref{thm:resample_variance} is given in \Cref{sec:proof:resample_variance}. 
As shown in \Cref{thm:resample_variance}, although the variance is still exponential in $K$ using resampling, the base of the exponential decreases to $1$ exponentially in the number of samples $N$. In other words, in the limit as $N \to \infty$, the variance of the estimator using the proposed RBR is bounded by a constant factor.
This novel approach reduces the variance of the Monte Carlo estimator of the optimal control law in a way that mirrors the relationship between Sequential Importance Sampling (i.e., particle filters without resampling), where the variance increases exponentially with the horizon length, and Sequential Monte Carlo (i.e., particle filters with resampling), where the (asymptotic) variance only increases linearly \cite{doucet2009tutorial}.

Another method to theoretically quantify the improvement in the variance of the estimator is via the \textit{effective sample size} ($\ESS$) \cite{doucet2009tutorial,elvira2022rethinking}.
$\ESS$ is defined as the ratio between the variance of the estimator with $N$ samples from the target and the variance of the SNIS estimator \cite{elvira2022rethinking}.
It can be interpreted as the number of samples simulated from the target pdf that would provide an estimator with variance equal to the performance of the $N$-sample SNIS estimator.
However, since the $\ESS$ is computationally intractable, the \textit{approximation} (made formal in \cite{elvira2022rethinking})
\begin{equation} \label{eq:ess_hat}
    \widehat{ESS} \coloneqq \frac{1}{\sum_{n=1}^N w_n^2},
\end{equation}
is more often used in practice.
The following theorem shows that performing RBR results in either the same or higher $\widehat{ESS}$, given certain assumptions (proof is given in \Cref{sec:proof:ess}).
\begin{mdframed}[style=ThmFrame]
\begin{theorem}\label{thm:ess}
    Let $\vw = [w_1, \dots, w_m, 0, \dots, 0]$ denote the unnormalized weight vector without resampling,
    where the last $N-m$ entries are zero due to violating the safety constraints.
    Let $\vw' = \vw + \vc = [w_1, \dots, w_m, c_{m+1}, \dots, c_N ]$ denote the unnormalized weight vector resulting from safe resampling, where $\vc = [0, \dots, 0, c_{m+1}, \dots, c_N]$.
    Let $\tilde{\vw} = \vw / \norm{\vw}_1$ and $\tilde{\vw}' = \vw' / \norm{\vw'}_1$ denote the normalized weights.
    Suppose that the weights of the resampled trajectories $\vc$ are not ``drastically larger'' than the weights of the original trajectories $\vw$, i.e.,
    \begin{equation} \label{eq: ess:assumption}
        \norm{ \vc }_1 \leq 2 \frac{N}{N-1} \frac{ \norm{\vw}_2^2 }{ \norm{\vw}_1 }.
    \end{equation}
    Then, the $\widehat{ESS}$ using RBR is no smaller than the $\widehat{ESS}$ without resampling, and is strictly greater if the inequality in \eqref{eq: ess:assumption} is strict. 
    In other words,
    \begin{equation} \label{eq: ess:wts}
        \frac{1}{\norm{ \tilde{\vw} }^2_2} \leq \frac{1}{\norm{ \tilde{\vw}' }^2_2}.
    \end{equation}
\end{theorem}
\end{mdframed}

\subsection{Summary of NS-VIMPC}
We now summarize the NS-VIMPC algorithm, shown in \Cref{fig:control_architecture}. 

Offline, we first approximate a DCBF by using the DPNCBF algorithm to learn the policy value function for a user-specified policy (\Cref{sec:ConstructingCBF}).
In our experiments, we chose this to be Shield MPPI (S-MPPI) \cite{Shield-MPPI}.
Online, we use the learned DPNCBF to enforce the DPCBF constraint \eqref{eqn:descent_condition2} to try to enforce safety beyond the prediction horizon.
During sampling, we use RBR to efficiently sample control sequences $\{ \tilde{\vu}^i \}$ from the raw samples $\{ \vu^i \}$ to satisfy the DPNCBF constraint, thus improving the sample efficiency of the Monte Carlo estimate of the optimal control.
The sampled control sequences $\{ \tilde{\vu}^i \}$ are then used to compute the estimate of the optimal control $\hat{\vv}$ using \eqref{eq:vi:v_opt}.
As in MPC fashion, we only execute the first control $\hat{\vv}_0$, and $\hat{\vv}$ is then used as the parameter vector of the sampling distribution $q_\vv$ for the next iteration.

\section{Simulations}
We first performed simulation experiments to better understand the performance of the proposed Neural Shield VIMPC (NS-VIMPC) controller.  
Although many sampling-based MPC controllers fall under the VIMPC family with different choices of the prior $p_0$ and $r$, we choose to instantiate the MPPI algorithm (see \Cref{app:VI Gauss} for details), and call the resulting controller Neural Shield-MPPI (NS-MPPI).

\begin{figure}[]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/deter_vs_stochastic.png}
    \caption{\textbf{\textsf{AutoRally} Trajectories.} We visualize the trajectories of the three MPPI baselines under a challenging target velocity of $\textrm{15~ms}^{-1}$.
    Both MPPI and S-MPPI veer off course and crash while NS-MPPI stays within the track even under Gaussian disturbances.
    }
    \label{fig:deter_vs_stochastic}
\end{figure}
\begin{figure}[]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/NS-MPPIRobustnessUsingDifferentVelocities.pdf}
    \caption{
    \textbf{Varying target velocities on \textsf{AutoRally}.}
    Our method NS-MPPI achieves the lowest crash and collision (truncated to Col.) rates under both deterministic and stochastic dynamics.
    Although the collision rate is close to $1$ for every method in the stochastic environment, NS-MPPI achieves a crash rate of near $0$.
    }
    \label{fig:deter_vs_stoch_3plot_autorally}
\end{figure}

\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.54\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/drone_control_horizon_ablation_200.pdf}
        \caption{}
    \end{subfigure}\begin{subfigure}[t]{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/drone_viz_main.pdf}
        \caption{}
        \label{fig:drone_viz}
    \end{subfigure}\vspace*{\fill}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{\textbf{Varying control horizon on \textsf{Drone}.}
    (a) Only NS-MPPI has a crash rate of zero even with a control horizon of $10$, demonstrating the benefit of enforcing the DCBF constraint for maintaining safety beyond the prediction horizon.
    (b) We compare the sampling trajectory $\vu^i$ (in \textcolor{green}{green}) and estimated optimal trajectory $\hat{\vv}$ (in \textcolor{orange}{orange}) for NS-MPPI and S-MPPI for a control horizon of $10$.
    Due to the DCBF constraint, NS-MPPI starts descending to avoid the obstacles even though none of the original collision constraints have been violated.
    }
    \label{fig:drone_control_horizon_ablation}
\end{figure*}

\beforetextbf{}
\noindent\textbf{Baseline methods.} We compared NS-MPPI against the following sampling-based MPC methods.
\begin{itemize}
\item Baseline MPPI (MPPI) ~\cite{InfoMPPI}, which forward simulates a set of randomly sampled trajectories for optimal control.

\item Shield MPPI (S-MPPI) ~\cite{Shield-MPPI}, which extends MPPI by taking $h$ in \eqref{eqn:AvoidSet} to be a DCBF and by adding the DCBF constraint violation into the cost as in \eqref{eq:cbfpenalty}.

\item CEM \cite{pmlr-v120-bharadhwaj20a}, which samples trajectories similar to the baseline MPPI but with the weight $\omega = 1$ for only the $k$-lowest cost trajectories and $0$ otherwise. This corresponds to an average of the $k$-lowest cost trajectories.
\end{itemize}

In all simulations, we use S-MPPI as the control policy $\pi$ to learn the DPNCBF.
We provide further details on the simulation experiments in \Cref{app:sim_details}.

\subsection{Simulations on \textsf{AutoRally}}
We first compare all methods on the \textsf{AutoRally}~\cite{goldfain2019autorally} testbed, a $1/5$ scale autonomous racing car.
The goal for this task is to track a given fixed velocity without exiting the track.
The vehicle \textit{collides} when it contacts the track boundary and \textit{crashes} when it fully exceeds the track boundary.

We tested our algorithm under both deterministic dynamics and stochastic dynamics with a Gaussian state disturbance added at each timestep.
We visualize the resulting trajectories in \Cref{fig:deter_vs_stochastic} with a target velocity of $\textrm{15~ms}^{-1}$.
This is a very high target velocity, as previous works considered at most velocities of 
$\textrm{8~ms}^{-1}$ \cite{Shield-MPPI} or $\textrm{9~ms}^{-1}$ \cite{RMPPI}.
Under this challenging speed, both the MPPI and the S-MPPI frequently veer off course. In contrast, the proposed NS-MPPI successfully retains the vehicle within the confines of the track, thereby ensuring safety.

Next, we vary the target velocities, showing the resulting crash rate, collision rate, and velocity in \Cref{fig:deter_vs_stoch_3plot_autorally} over 20 trials.
With higher velocities, the vehicle has less time to turn, increasing the likelihood of leaving the track and colliding or crashing. 
We see that NS-MPPI consistently outperforms all other methods in both settings without having a significantly lower average velocity.

\subsection{Simulations on \textsf{Drone}}
We next tested our algorithm on \textsf{Drone}, a simulated planar quadrotor that incorporates ground effects arising from the intricate interaction between the blade airflow and the ground surface.
The goal is for the drone to navigate as fast as possible through a narrow corridor close to the ground. 
We vary the control horizon for $200$ sampled trajectories and plot the results in \Cref{fig:drone_control_horizon_ablation}. 
Only NS-MPPI has a crash rate of zero over all control horizons.
S-MPPI is only able to consistently avoid crashes with longer control horizons.
To understand why this is the case, we visualize the trajectories for NS-MPPI and S-MPPI with a control horizon of $10$ in \Cref{fig:drone_viz}.
Due to the DCBF constraint, NS-MPPI starts descending to avoid the obstacles even though none of the original collision constraints have been violated.
On the other hand, S-MPPI is unaware of the obstacles beyond the prediction horizon and keeps moving to the right.
This suggests that enforcing the DCBF constraint even with only an approximate $V^{h,\pi,\gamma}_\theta$ is beneficial in enforcing safety with very short control horizons.

Interestingly, the success rate of NS-MPPI initially increases with the control horizon (as safety increases) but later decreases (as the drone fails to go through the narrow corridor but remains safe).
This is because the variance of the Monte Carlo estimator scales exponentially with the control horizon, as shown in \Cref{sec:Efficient_Trajectory_Sampling}.

\subsection{Deeper Investigation Into RBR}

We performed a case study to better understand the benefits of the proposed RBR method on sample efficiency.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/smppi_efficient.png}
    \setlength{\belowcaptionskip}{-8pt}
    \caption{\textbf{RBR reduces the samples needed by 5X.}
    On \textsf{AutoRally}, applying RBR to S-MPPI significantly reduces the required number of sampled trajectories ($190$ to $40$) to achieve the same level of safety.
    From another perspective, RBR reduces the collision rate by $74\%$ at $50$ sampled trajectories.
    }
    \label{fig:smppi_vs_smppi_efficient}
\end{figure}

\beforetextbf{}

\noindent\textbf{RBR improves sample efficiency by 5X.}
To isolate the effects of RBR without the other improvements,
we considered a new method that extends S-MPPI with RBR (S-MPPI w/ RBR),
and compared it against the original S-MPPI across different numbers of sampled trajectories on \textsf{AutoRally} over $100$ trials in \Cref{fig:smppi_vs_smppi_efficient}.
The results show that S-MPPI with RBR can achieve the same collision rates as S-MPPI without RBR while using $5$ times \textit{fewer} trajectories.
This matches our expectations, both from the intuition that RBR results in a more closely aligned proposal distribution (e.g., see \Cref{fig:EfficientSamplingVsNormalSampling1}), and from the theoretical results in \Cref{sec:Efficient_Trajectory_Sampling} that show that RBR improves the quality of the estimator.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/ess.pdf}
    \caption{\textbf{NS-MPPI has larger $\widehat{\BoldESS}$.} The proposed NS-MPPI achieves larger effective sample sizes, verifying that RBR enables more efficient use of samples and computations.}
    \label{fig:ess_text}
\end{figure}

\beforetextbf{}

\noindent\textbf{RBR achieves larger $\widehat{\BoldESS}$.} RBR theoretically improves the $\widehat{ESS}$, as mentioned in \Cref{sec:Efficient_Trajectory_Sampling}. 
We verified this claim empirically on the \textsf{AutoRally} and plot the results in \Cref{fig:ess_text}.
While MPPI and S-MPPI exhibit values of $\widehat{ESS}$ concentrated near $1$,
the $\widehat{ESS}$ values for NS-MPPI are more uniformly distributed, indicating a more extensive utilization of the sampled trajectories and thus a better use of the available computational resources.

\beforetextbf{}

\noindent\textbf{RBR is especially beneficial in harder environments.}
To further stress-test RBR, we ran 50 trials with 100 sampled trajectories on a harder variant of the AutoRally that contains obstacles of various sizes and compared its performance with other baseline methods.
The cluttered AutoRally environment can become especially challenging when the sampled trajectory states fall within a non-convex set caused by close-to-center obstacles.
First, consider the trajectories produced by S-MPPI \cite{Shield-MPPI} and our NS-MPPI in \Cref{fig:cluttered_autorally}. 
Since S-MPPI relies on a heuristic DCBF, it is unable to avoid crashing into the walls and obstacles, whereas NS-MPPI avoids crashes in almost all runs as shown in ~\Cref{fig:cluttered_autorally} (b).
Furthermore, if we add efficient sampling to the S-MPPI, we observe a massive performance boost as seen in ~\Cref{fig:cluttered_autorally}, but still not as good as NS-MPPI.

Finally, to demonstrate the enhancement in sampling efficiency achieved by the proposed sampling method, we provide visual representations of trajectory sampling distributions obtained from simulations in environments with obstacles in ~\Cref{fig:EfficientSamplingVsNormalSampling1}.
As demonstrated by ~\Cref{fig:EfficientSamplingVsNormalSampling1} (b), the RBR concentrates trajectory samples within a feasible narrow passage that satisfies the DCBF safety criteria. 
Conversely, the standard sampling approach, exemplified by \Cref{fig:EfficientSamplingVsNormalSampling1} (b),  produces a sparse distribution, misallocating samples to unsafe zones and thereby failing to sufficiently explore safe areas. 
This inadequacy leads to a collision with an obstacle.

\beforetextbf{}

\noindent\textbf{Enhanced sampling efficiency enables real-time planning on a CPU.} To demonstrate the computational benefits provided by RBR, we evaluated several VI-MPC controllers on a standard laptop CPU. This assessment sets a target tracking speed of $12~\text{m/s}$, a horizon of $K=15$ and a small sample size of $N=30$ trajectories at each optimization iteration for fast computation. The resulting statistics are presented in \Cref{tab:CPU_table}. 
From the data presented in \Cref{tab:CPU_table}, it is evident that NS-MPPI attains the minimal crash rate, although it operates at a marginally reduced control update frequency, with S-MPPI next in line. 
This challenging task results in both MPPI and CEM controllers exhibiting crash and collision rates that approach 1. For most robotic applications, a control rate of 70~\text{Hz} is sufficient.

\begin{table}[!h]
\caption{Performance and Timing Comparison}
\begin{center}
\begin{tabular}{c|c|c|c}
\toprule
 & \textbf{Crash}& \textbf{Collision} & \textbf{Control}\\
\textbf{Controller} & \textbf{Rate} & \textbf{Rate} & \textbf{Rate (Hz)} \\
\midrule
NS-MPPI      & 0.04 & 0.06 & 73.64 \\
S-MPPI      & 0.46 & 0.70 & 107.31 \\
MPPI      & 0.98 & 1.00 & 130.05 \\
CEM      & 1.00 & 1.00 & 88.17 \\
\bottomrule
\end{tabular}
\label{tab:CPU_table}
\end{center}
\end{table}

\begin{figure}[t]
    \centering
    \hspace*{\fill}\begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/trajectories_plot_vel_15.0.pdf}
        \caption{}
        \label{fig:ShieldMPPIvsNSMPPI}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/crash_and_collision_rate_cluttered_env.pdf}
        \caption{}
        \label{fig:crash_and_collision_rate_cluttered_env}
    \end{subfigure}
    \hspace*{\fill}\caption{\textbf{AutoRally with obstacles.} (a) While NS-MPPI consistently clears the entire track, S-MPPI often crashes into obstacles and walls. (b) \textbf{AutoRally Statistics.} The crash and collision rate statistics supporting the observations made on the left.}
    \label{fig:cluttered_autorally}
\end{figure}

\section{Hardware Experiments}

Finally, we deployed our method and other existing sampling-based MPC baselines on hardware using the AutoRally experimental platform \cite{goldfain2019autorally}.
Testing was conducted on a dirt track outlined by drainage pipes, as depicted in \Cref{fig:AutoRallyTrackSetUp}.
All computations use the onboard Intel i7-6700 CPU and Nvidia GTX-750ti GPU \cite{goldfain2019autorally}.
See \Cref{app:hw_details} for more details on the hardware experiments.

\begin{figure}[]
    \centering
    \centerline{\includegraphics[width=9cm,height=4cm]{figs/AutoRallyTrackSetUp.png}}
    \caption{\textbf{AutoRally Track Setup.} 
The site is equipped with a spacious carport, a 3-meters-tall observation tower, and a storage shed \cite{AutorallyHardware}. Each turn of the track is numbered as Ti, with T1 is the first turn and T9 is the last.}
    \label{fig:AutoRallyTrackSetUp}
\end{figure}

\subsection{Robustness Against Adversarial Costs}

Robots depend on sensors such as cameras to gather information about their environment, and use the collected data to construct suitable cost functions. 
Deep Neural Networks (DNNs) are frequently utilized for object detection, classification, and semantic segmentation \cite{zhao2017survey}. 
However, DNNs are susceptible to adversarial attacks \cite{PhysicalAdversarialAttack}, which can mislead the algorithms into generating incorrect outputs. 
These attacks often involve altering the appearance of the object of interest directly \cite{li2019adversarial},  eventually resulting in erroneous cost functions.
In this hardware experiment, we tested the robustness of the proposed NS-MPPI to these erroneous or misspecified cost functions by using an \textit{adversarial} cost function on the actual AutoRally hardware platform.
Namely, instead of penalizing infeasible system states, we \textit{reward} unsafe states by assigning negative costs, effectively incentivizing collisions.
We compared against the S-MPPI and MPPI baselines.

The results are depicted in \Cref{fig:adversarial_cost_test}.
As MPPI only uses cost information, it rapidly steers the AutoRally vehicle toward the closest boundary, resulting in a crash.
S-MPPI crashes the vehicle at the first turn (T1) as well, but it covers a greater distance than MPPI.
Only NS-MPPI achieves a collision-free completion of 10 laps.
Nonetheless, there are noticeable oscillatory movements of the car, indicating susceptibility to attractive costs driving it toward boundary collisions.

\subsection{Safety Under Unsafe User Input}
Exceeding speed limits is a significant contributor to road accidents \cite{NHTSA_speeding} due to noncompliance by many drivers. The proposed Shield VIMPC control framework can smartly manage speeds supervised by the NCBF to achieve safety, even given unsafe user inputs. 
To this end, we tested MPPI, S-MPPI, and the proposed NS-MPPI on the AutoRally hardware using large, unsafe target velocities to examine their safety performance. 
The results are shown in \Cref{fig:safety_under_unsafe_user_inputs}. 
Designating high target velocities encourages maximizing speed, thus disrupting the equilibrium in the controllers' cost structure by diminishing the emphasis on cost penalties associated with obstacles. As a result, MPPI causes the vehicle to deviate from the path, resulting in a crash at the second turn (T2), while S-MPPI impacts at the seventh turn (T7). 
In contrast, NS-MPPI ensures safety, preventing any accidents.
\begin{figure}[]
    \centering
    \centerline{\includegraphics[scale = 0.15]{figs/Adversarial_cost_test.png}}
    \caption{\textbf{Adversarial cost.} With a cost function that rewards crashes, both MPPI and S-MPPI crash. Only NS-MPPI completes 10 laps collision-free.}
    \label{fig:adversarial_cost_test}
\end{figure}

\begin{figure}[]
    \centering
    \centerline{\includegraphics[scale = 0.24]{figs/Overly_large_tracking_velocities.png}}
    \caption{\textbf{Safety under unsafe user inputs.} Using an erroneously large velocity target, both MPPI and S-MPPI crash.
    The visualization on the bottom row visualizes simulation trajectories that lead to crashes in the same turn.
}
    \label{fig:safety_under_unsafe_user_inputs}
\end{figure}

\section{Limitations}

One limitation of our approach is that the DPNCBF is only an approximation of a DCBF. 
As such, the typical safety guarantees of DCBFs may not hold if the function is not a true DCBF.
Moreover, as noted in \Cref{rem:convex_thing}, the use of variational inference means that $\vv^*$ may not satisfy the state constraints despite the optimal distribution $p(\vu \mid o=1)$ having zero density at states that violate the state constraints. 
While theoretically we were only able to show that $\vv^*$ stays safe under the assumption that $\mathcal{U}^K_{\text{safe}}$ is convex, empirical results in \Cref{fig:cluttered_autorally} show that NS-MPPI can perform well despite the fact that $\mathcal{U}^K_{\text{safe}}$ is typically not convex.

\section{Conclusion}
In this study, we have adapted the policy neural control barrier function (PNCBF) to a discrete-time setting and utilized the resulting discrete-time policy neural control barrier function (DPNCBF) to supervise the proposed resampling-based rollout (RBR) method.
This novel method enhances sampling efficiency and safety for all variational inference model predictive controllers (VIMPCs).
Leveraging RBR, we developed the neural shield VIMPC (NS-VIMPC) control framework and demonstrated its benefits for safe planning through the novel neural shield model predictive path integral (NS-MPPI) controller. We conducted tests of the NS-MPPI, benchmarking its performance against state-of-the-art sampling-based MPC controllers using both an autonomous vehicle and a drone. 
Our simulations and experimental data indicate that the NS-MPPI outperforms existing VIMPC methods in terms of safety and sampling efficiency. 
The improved sampling efficiency allows NS-MPPI to reach similar performance levels as other VIMPC methods, while using significantly fewer sampled trajectories, facilitating real-time control on a CPU rather than necessitating costly GPU resources.

We have used the novel RBR method to concentrate sampled trajectories on safe regions. While this approach can significantly improve sampling efficiency in terms of safety, it does not improve other aspects of performance. 
To further enhance performance, we may, for instance, integrate a Control Lyapunov Function \cite{dawson2023safe} or a Control Lyapunov Barrier Function \cite{CLBF} with the novel RBR approach to sample safe and higher-performance trajectories.

\bibliographystyle{plainnat}
\bibliography{bib/references}

\newpage
\onecolumn
\begin{appendices}
\numberwithin{equation}{section}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\renewcommand{\thesubsection}{\thesection\arabic{subsection}}
\renewcommand{\thesubsectiondis}{\thesubsection}

\crefalias{section}{appendix}

\subfile{sections/01_autorally_details}
\subfile{sections/01_autorally_dynamics}
\subfile{sections/08_drone_dynamics}
\subfile{sections/02_nsmppi_objective}
\subfile{sections/03_vi_proofs}
\subfile{sections/04_avoidance_heuristic_design}
\subfile{sections/06_proofs}
\subfile{sections/07_hw_details}
\subfile{sections/09_convex_thing}
\setlength{\parindent}{0pt}
\setlength{\parskip}{2pt plus1pt minus0.5pt}

\end{appendices}

\end{document}
