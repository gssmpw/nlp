\section{Proofs For Resampling-based Rollouts (RBR) supervised by a CBF}
\subsection{Proof of Theorem~\ref{thm:forward_invariant}}
\begin{proof} \label{proof3.1}
Condition \eqref{eqn:descent_condition} implies that $B(x_{k}) \leq (\text{Id} - \alpha) \circ B(x_{k-1}) $, where $\circ$ denotes function composition and $\mathrm{Id}$
denotes the identity function~\cite{DCBF}. 
Since $B(x_{1}) \leq (\text{Id} - \alpha) \circ B(x_0) $, it follows that,
\begin{equation}\label{induction}
    B(x_k) \leq (\text{Id}-\alpha)^k \circ B(x_0).
\end{equation}
Since $(\text{Id} - \alpha)$ is a class-$\kappa$ function for $a \in (0, 1)$, it follows from $B(x_0) \leq 0$ that $B(x_k) \leq 0$. Hence, the set $\mathcal{S}$ is forward invariant for system~\eqref{dynamics}.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:same_marginal}} \label{sec:proof:same_marginal}
\begin{proof}
    Note that
    \begin{equation}
        \mathsf{f}(\tilde{\mathsf{b}} \mid \mathsf{a}, \mathsf{b})
        = \mathbbm{1}_{\mathsf{b} \in \mathsf{S}} \delta(\tilde{\mathsf{b}} - \mathsf{b})
        + \mathbbm{1}_{\mathsf{b} \not\in \mathsf{S}} \delta(\tilde{\mathsf{b}} - \mathsf{a}).
    \end{equation}
    Let $\tilde{\mathsf{f}}$ denote the marginal density of $\tilde{\mathsf{b}}$.
    Then, using the independence of $\mathsf{a}$ and $\mathsf{b}$,
    \begin{equation}
        \tilde{\mathsf{f}}(\tilde{\mathsf{b}})
        = \int \left( \int \mathsf{f}(\tilde{\mathsf{b}} \mid \mathsf{a}, \mathsf{b}) \mathsf{f}(\mathsf{a} \mid \mathsf{a} \in \mathsf{S}) \diff{\mathsf{a}} \right) \mathsf{f}(\mathsf{b}) \diff{\mathsf{b}}
    \end{equation}
    Simplifying the inner integral first using properties of the Dirac delta function gives
    \begin{align}
        &\mathrel{\phantom{=}} \int \mathsf{f}(\tilde{\mathsf{b}} \mid \mathsf{a}, \mathsf{b}) \mathsf{f}(\mathsf{a} \mid \mathsf{a} \in \mathsf{S}) \diff{\mathsf{a}} \\
        &= \mathbbm{1}_{\mathsf{b} \in \mathsf{S}} \delta(\tilde{\mathsf{b}} - \mathsf{b}) + 
        \mathbbm{1}_{\mathsf{b} \not\in \mathsf{S}} \int 
            \delta(\tilde{\mathsf{b}} - \mathsf{a})
            \mathsf{f}(\mathsf{a} \mid \mathsf{a} \in \mathsf{S}) \diff{\mathsf{a}} \\
        &= \mathbbm{1}_{\mathsf{b} \in \mathsf{S}} \delta(\tilde{\mathsf{b}} - \mathsf{b}) + 
        \mathbbm{1}_{\mathsf{b} \not\in \mathsf{S}} \mathsf{f}(\tilde{\mathsf{b}} \mid \tilde{\mathsf{b}} \in \mathsf{S}).
    \end{align}
    Hence,
    \begin{align}
        \tilde{\mathsf{f}}(\tilde{\mathsf{b}})
        &= \int \left(
            \mathbbm{1}_{\mathsf{b} \in \mathsf{S}} \delta(\tilde{\mathsf{b}} - \mathsf{b}) + 
            \mathbbm{1}_{\mathsf{b} \not\in \mathsf{S}} \mathsf{f}(\tilde{\mathsf{b}} \mid \tilde{\mathsf{b}} \in \mathsf{S})
        \right) \mathsf{f}(\mathsf{b}) \diff{\mathsf{b}} \\
&= \mathsf{f}(\tilde{\mathsf{b}}) \mathbbm{1}_{\tilde{\mathsf{b}} \in \mathsf{S}}
        + \mathsf{f}(\tilde{\mathsf{b}} \mid \tilde{\mathsf{b}} \in \mathsf{S})
        \int \mathbbm{1}_{\mathsf{b} \not\in \mathsf{S}} \mathsf{f}(\mathsf{b}) \diff{\mathsf{b}} \\
&= \mathsf{f}(\tilde{\mathsf{b}} \mid \tilde{\mathsf{b}} \in \mathsf{S})
        P(\mathsf{b} \in \mathsf{S})
        + \mathsf{f}(\tilde{\mathsf{b}} \mid \tilde{\mathsf{b}} \in \mathsf{S})
        P(\mathsf{b} \not\in \mathsf{S}) \\
        &= \mathsf{f}(\tilde{\mathsf{b}} \mid \tilde{\mathsf{b}} \in \mathsf{S}).
    \end{align}
\end{proof}

\subsection{Proof of Corollary~\ref{thm:resample_unbiased}} \label{sec:proof:resample_unbiased}
\begin{proof}
    Applying \Cref{thm:same_marginal}, $\mathsf{x}, \mathsf{a}$ and $\tilde{\mathsf{b}}$ have the same distribution,
    \begin{equation}
        \mathsf{x} \stackrel{d}{=} \mathsf{a} \stackrel{d}{=} \tilde{\mathsf{b}}.
    \end{equation}
    Hence,
    \begin{align}
        \E[\frac{1}{2} w( \mathsf{a} ) + \frac{1}{2} w( \tilde{\mathsf{b}} )]
        &= \frac{1}{2} \E[ w( \mathsf{a} )] + \frac{1}{2} \E[ w( \tilde{\mathsf{b}} )], \\
        &= \frac{1}{2} \E[ w( \mathsf{x} )] + \frac{1}{2} \E[ w( \mathsf{x} )], \\
        &= \E[w( \mathsf{x} )].
    \end{align}
\end{proof}

\subsection{Proof of Theorem~\ref{thm:resample_variance}} \label{sec:proof:resample_variance}
We prove the two claims separately.

\begin{lemma}
    The variance of the Monte Carlo estimator of the optimal control law is
    \begin{equation}
        \Var[\hat{v}_k] = \frac{1}{N}\left( \frac{1}{3} 2^K - \frac{1}{4} \right), \qquad k = 1, \dots, K.
    \end{equation}
\end{lemma}
\begin{proof}
First, note that the optimal control $\vv^*$ is given by
\begin{align}
    v^*_k
    &= \int_{[-1, 1]^K} p(\vu \mid o=1) u_k \diff{\vu} \\
    &= \int_{[-1, 1]^K} \ind{\vu \in [0, 1]} u_k \diff{\vu} \\
    &= \int_{[0, 1]^K} u_k \diff{\vu} \\
    &= \frac{1}{2}.
\end{align}
Using the formula for computing the variance of an importance sampled Monte Carlo estimator \cite{owen2013monte}, we then have that
\begin{align}
    \Var[\hat{v}_k]
    &= \frac{1}{N} \int_{[-1, 1]^K} \frac{ ( u_k p(\vu \mid o=1) - v^*_k r(\vu) )^2 }{ r(\vu) } \diff{\vu} \\
    &= \frac{1}{N} \int_{[-1, 1]^K} \frac{ ( u_k p(\vu \mid o=1) - \frac{1}{2} 2^{-K} )^2 }{ 2^{-K} } \diff{\vu} \\
    &= \frac{1}{N} 2^K \left( \int_{[0, 1]^K} (u_k)^2 \diff{\vu} - 2^{-K} \int_{[0, 1]^K} u_k \diff{\vu} + \frac{1}{4} (2^{-K})^2 \int_{[-1, 1]^K} 1 \diff{\vu} \right) \\
    &= \frac{1}{N} 2^K \left( \frac{1}{3} - \frac{1}{2} 2^{-K} + \frac{1}{4} 2^{-K} \right) \\
    &= \frac{1}{N}\left( \frac{1}{3} 2^K - \frac{1}{4} \right).
\end{align}
\end{proof}

\noindent\textbf{Proof of second claim.}
We now prove the second claim.
When performing resampling-based rollouts, \textit{only} if $u^i_k$ lies in $[-1, 0]$ for all $k = 0, \dots, K - 2$ does resampling not occur, and the output $\tilde{u}^i_k \in [-1, 0]$ for $k = 0, \dots, K-2$.
Otherwise, we have that $\tilde{u}^i_k \in [0, 1]$ for $k = 0, \dots, K-2$.
The last control $u^i_{K-1} = \tilde{u}^i_{K-1}$ is never resampled.
Hence, the probability that all $N$ samples lie in $[-1, 0]^{K-1} \times [-1, 1]$ is equal to $2^{-N(K-1)}$.

For all timesteps except the last, we write the joint probability density function (over all $N$ samples) as
\begin{equation}
    p(\tilde{u}^1_k, \dots, \tilde{u}^N_k) = \begin{dcases}
        2^{-N}, & \text{if } \tilde{u}^i_k \in [-1, 0] \text{ for all } i = 1, \dots, N, \\
        1 - 2^{-N}, & \text{if } \tilde{u}^i_k \in [0, 1] \text{ for all } i = 1, \dots, N, \\
        0, & \text{otherwise,}
    \end{dcases}
    \qquad
    \quad
    \text{ for }
    k = 0, \dots, K - 2,
\end{equation}
and
\begin{equation}
    p(\tilde{u}^i_{K-1}) = 1,
    \qquad
    p(\tilde{u}^1_{K-1}, \dots, \tilde{u}^N_{K-1}) = \prod_{i=1}^N p(\tilde{u}^i_{K-1}) = 1.
\end{equation}
For convenience, let $t \coloneqq 2^{-N}$ such that $P(\tilde{u}^{1:N}_k \in [0, 1]) = 1 - t$ for $k < K-1$,
and
\begin{equation}
    P(\tilde{u}^{1:N}_{0:K-2} \in [0, 1]) = \prod_{k=0}^{K-2} P(\tilde{u}^{1:N}_k \in [0, 1]) = (1 - t)^{K-1}.
\end{equation}
Next, consider the Monte Carlo estimator $\hat{v}_k$ using the resampled controls $\tilde{u}^i_k$, defined by
\begin{equation}
    \hat{v}_k = \frac{1}{N} \sum_{i=1}^N \frac{ \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} }{ \frac{1}{2} (1 - t)^{K-1} } \tilde{u}^i_k.
\end{equation}
We will compute the expectation of $\hat{v}_k$ using the law of total expectation. To this end, we have that
\begin{align}
    \E_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_k ]
    &= \frac{1}{N} \E_{\tilde{u}_{0:K-1}^{1:N}}\left[ \sum_{i=1}^N \frac{ \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} }{ \frac{1}{2} (1 - t)^{K-1} } \tilde{u}^i_k \right] \\
    &= \frac{1}{N}\Big( P(\tilde{u}_{0:K-2}^{1:N} \geq 0) \E_{\tilde{u}_{0:K-1}^{1:N}}\left[ \sum_{i=1}^N \frac{ \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} }{ \frac{1}{2} (1 - t)^{K-1} } \tilde{u}^i_k \mathrel{\bigg|} \tilde{u}_{0:K-2}^{1:N} \geq 0\right] \\
    &\quad + P(\tilde{u}_{0:K-2}^{1:N} < 0) \E_{\tilde{u}_{0:K-1}^{1:N}}\left[ \sum_{i=1}^N \frac{ \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} }{ \frac{1}{2} (1 - t)^{K-1} } \tilde{u}^i_k \mathrel{\bigg|} \tilde{u}_{0:K-2}^{1:N} < 0\right]
    \Big) \notag \\
    &= \frac{1}{N}\left( (1 - t)^{K-1} \E_{\tilde{u}_{0:K-1}^{1:N}}\left[ \sum_{i=1}^N \frac{ \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} }{ \frac{1}{2} (1 - t)^{K-1} } \tilde{u}^i_k \mathrel{\bigg|} \tilde{u}_{0:K-2}^{1:N} \geq 0\right]
    \right) \\
    &= \frac{2}{N}\left( \E_{\tilde{u}_{0:K-1}^{1:N}}\left[ \sum_{i=1}^N \ind{\tilde{u}^i_{0:K-1} \in [0, 1]}\, \tilde{u}^i_k \mathrel{\bigg|} \tilde{u}_{0:K-2}^{1:N} \geq 0\right]
    \right).
\end{align}
We now split into two cases. When $k = K-1$,
\begin{align}
    \E_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_{K-1} ]
    &= \frac{2}{N}\left( \E_{\tilde{u}_{K-1}^{1:N}}\left[ \E_{\tilde{u}_{0:K-2}^{1:N}}\left[ \sum_{i=1}^N \ind{\tilde{u}^i_{0:K-1} \in [0, 1]}\, \tilde{u}^i_{K-1} \mathrel{\bigg|} \tilde{u}_{0:K-2}^{1:N} \geq 0\right] \right]
    \right) \\
    &= \frac{2}{N}\left( \E_{\tilde{u}_{K-1}^{1:N}}\left[ \sum_{i=1}^N \ind{\tilde{u}^i_{K-1} \in [0, 1]}\, \tilde{u}^i_{K-1} \right]
    \right) \\
    &= \frac{2}{N}\left( \sum_{i=1}^N \E_{\tilde{u}_{K-1}^{i}}\left[ \ind{\tilde{u}^i_{K-1} \in [0, 1]} \tilde{u}^i_{K-1} \right]
    \right) \\
    &= \frac{2}{N}\left( \sum_{i=1}^N \frac{1}{4} \right) \\
    &= \frac{1}{2}.
\end{align}
Similarly, when $k < K-1$,
\begin{align}
    \E_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_{k} ]
    &= \frac{2}{N}\left( \E_{\tilde{u}_{K-1}^{1:N}}\left[ \E_{\tilde{u}_{0:K-2}^{1:N}}\left[ \sum_{i=1}^N \ind{\tilde{u}^i_{K-1} \in [0, 1]} \ind{\tilde{u}^i_{0:K-2} \in [0, 1]}\, \tilde{u}^i_{k} \mathrel{\bigg|} \tilde{u}_{0:K-2}^{1:N} \geq 0\right] \right]
    \right) \\
    &= \frac{1}{N}\left( \E_{\tilde{u}_{0:K-2}^{1:N}}\left[ \sum_{i=1}^N \tilde{u}^i_{k} \mathrel{\bigg|} \tilde{u}_{0:K-2}^{1:N} \geq 0\right]
    \right) \\
    &= \frac{1}{2}.
\end{align}
Thus, $\hat{v}_k$ is an unbiased estimator.

Next, we look at the variance.
Summarizing the above computation, we have that
\begin{equation}
    \E_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} \geq 0 ]
    = \frac{1}{2 (1-t)^{K-1} }, \qquad 
    \E_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} < 0 ] = 0.
\end{equation}
Let $c \coloneqq \E_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} \geq 0 ]
    = \frac{1}{2 (1-t)^{K-1} }$ for convenience.
Computing the conditional variances, for $\tilde{u}_{0:K-2}^{1:N} < 0$, we have that
\begin{equation}
    \Var_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} < 0 ] = 0.
\end{equation}
For $\tilde{u}_{0:K-2}^{1:N} \geq 0 $ we have,
\begin{align}
    \Var_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} \geq 0 ]
    &= \E_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_k^2 \mid \tilde{u}_{0:K-2}^{1:N} \geq 0 ] - c^2 \\
    &= \E_{\tilde{u}_{0:K-1}^{1:N}}\left[
        \left( \frac{1}{N} \sum_{i=1}^N \frac{ \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} }{ \frac{1}{2} (1 - t)^{K-1} } \tilde{u}^i_k \right)^2
    \mid \tilde{u}_{0:K-2}^{1:N} \geq 0 \right] - c^2 \\
    &= \left( \frac{1}{N} \frac{1}{ \frac{1}{2} (1-t)^{K-1} } \right)^2
        \underbrace{\E_{\tilde{u}_{0:K-1}^{1:N}}\left[
            \left( \sum_{i=1}^N \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} \tilde{u}^i_k \right)^2
    \mathrel{\bigg|} \tilde{u}_{0:K-2}^{1:N} \geq 0 \right]}_{\coloneqq\; \circled{$\star$}} - c^2\label{eq:star}
\end{align}
Expanding $\circled{$\star$}$, yields
\begin{equation} \label{eq: var_proof:star}
    \circled{$\star$}
    = \E_{\tilde{u}_{0:K-1}^{1:N}}\left[
            \sum_{i=1}^N \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} (\tilde{u}^i_k)^2 +
            \sum_{i=1}^N \sum_{j=1, j\not=i}^N \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} \ind{\tilde{u}^j_{0:K-1} \in [0, 1]} \tilde{u}^i_k \tilde{u}^j_k
    \mathrel{\bigg|} \tilde{u}_{0:K-2}^{1:N} \geq 0 \right].
\end{equation}
We now split into two cases.

\vspace{.5\baselineskip}

\noindent\textbf{Case 1: $k = K-1$}.
Looking at the first term of \circled{$\star$} in \eqref{eq: var_proof:star},
\begin{align}
    \E_{\tilde{u}_{0:K-1}^{1:N}}\left[
        \sum_{i=1}^N \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} (\tilde{u}^i_{K-1})^2
        \mid \tilde{u}_{0:K-2}^{1:N} \geq 0
    \right]
    &= \E_{\tilde{u}_{0:K-1}^{1:N}}\left[
        \sum_{i=1}^N \E_{\tilde{u}^i_{K-1}}[ \ind{\tilde{u}^i_{K-1} \in [0, 1]}  (\tilde{u}^i_{K-1})^2 ]
        \mid \tilde{u}_{0:K-2}^{1:N} \geq 0
    \right] \\
    &= \sum_{i=1}^N \E_{\tilde{u}^i_{K-1}}[ \ind{\tilde{u}^i_{K-1} \in [0, 1]}  (\tilde{u}^i_{K-1})^2 ] \\
    &= \frac{N}{6}.\label{eq:term1}
\end{align}
For the second term in \eqref{eq: var_proof:star}, we have
\begin{align}
    &\mathrel{\phantom{=}} \E_{\tilde{u}_{0:K-1}^{1:N}}\left[
        \sum_{i=1}^N \sum_{j=1, j\not=i}^N \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} \ind{\tilde{u}^j_{0:K-1} \in [0, 1]} \tilde{u}^i_{K-1} \tilde{u}^j_{K-1}
        \mid \tilde{u}_{0:K-2}^{1:N} \geq 0
    \right] \\
    &= \sum_{i=1}^N \sum_{j=1, j\not=i}^N \E_{\tilde{u}_{K-1}^{i,j}}\left[ \ind{\tilde{u}^i_{K-1} \in [0, 1]} \ind{\tilde{u}^j_{K-1} \in [0, 1]} \tilde{u}^i_{K-1} \tilde{u}^j_{K-1} \right] \\
    &= \sum_{i=1}^N \sum_{j=1, j\not=i}^N \E_{\tilde{u}_{K-1}^{i,j}}\left[ \ind{\tilde{u}^i_{K-1} \in [0, 1]} \tilde{u}^i_{K-1} \right]^2 \\
    &= \sum_{i=1}^N \sum_{j=1, j\not=i}^N \frac{1}{4} \\
    &= \frac{N(N-1)}{4}.\label{eq:term2}
\end{align}
Substituting the two terms \eqref{eq:term1} and \eqref{eq:term2} into \eqref{eq:star}, yields
\begin{align}
    \Var_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_{K-1} \mid \tilde{u}_{0:K-2}^{1:N} \geq 0 ]
    &= \left( \frac{1}{N} \frac{1}{ \frac{1}{2} (1-t)^{K-1} } \right)^2
    \left( \frac{N}{6} + \frac{N(N-1)}{4} \right) - c^2 \\
    &\leq \left( \frac{1}{N} \frac{1}{ \frac{1}{2} (1-t)^{K-1} } \right)^2
    \frac{N^2}{4} - c^2 \\
    &= \left( \frac{1}{ (1-t)^2} \right)^{K-1} - \frac{1}{4} \left( \frac{1}{ (1-t)^2} \right)^{K-1}, \\
    &= \frac{3}{4} \left( \frac{1}{ (1-t)^2} \right)^{K-1}.
\end{align}

\vspace{.5\baselineskip}

\noindent\textbf{Case 2: $k < K-1$}.
Looking at the first term of \circled{$\star$} in \eqref{eq: var_proof:star},
\begin{align}
    \E_{\tilde{u}_{0:K-1}^{1:N}}\left[
        \sum_{i=1}^N \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} (\tilde{u}^i_k)^2
        \mid \tilde{u}_{0:K-2}^{1:N} \geq 0
    \right]
    &= \E_{\tilde{u}_{0:K-1}^{1:N}}\left[
        \sum_{i=1}^N \E_{\tilde{u}^i_{K-1}}[ \ind{\tilde{u}^i_{K-1} \in [0, 1]} ] (\tilde{u}^i_k)^2
        \mid \tilde{u}_{0:K-2}^{1:N} \geq 0
    \right] \\
    &= \frac{1}{2} \E_{\tilde{u}_{0:K-1}^{1:N}}\left[
        \sum_{i=1}^N (\tilde{u}^i_k)^2
        \mid \tilde{u}_{0:K-2}^{1:N} \geq 0
    \right].
\end{align}
From \Cref{thm:same_marginal}, the marginal of any resampled controls will all be uniform over $[0, 1]$. Hence,
\begin{align}
    \frac{1}{2} \E_{\tilde{u}_{0:K-1}^{1:N}}\left[
        \sum_{i=1}^N (\tilde{u}^i_k)^2
        \mid \tilde{u}_{0:K-2}^{1:N} \geq 0
    \right]
    &= \frac{1}{2} \sum_{i=1}^N \E_{\tilde{u}_k^i}[ (\tilde{u}_k^i)^2 \mid \tilde{u}_{k}^{i} \geq 0] = \frac{N}{6}.\label{eq:term3}
\end{align}
For the second term in \eqref{eq: var_proof:star},
\begin{align}
    &\mathrel{\phantom{=}} \E_{\tilde{u}_{0:K-1}^{1:N}}\left[
        \sum_{i=1}^N \sum_{j=1, j\not=i}^N \ind{\tilde{u}^i_{0:K-1} \in [0, 1]} \ind{\tilde{u}^j_{0:K-1} \in [0, 1]} \tilde{u}^i_k \tilde{u}^j_k
        \mid \tilde{u}_{0:K-2}^{1:N} \geq 0
    \right] \\
    &= \E_{\tilde{u}_{0:K-2}^{1:N}}\left[
        \sum_{i=1}^N \sum_{j=1, j\not=i}^N \E_{\tilde{u}_{K-1}^{1:N}}\left[ \ind{\tilde{u}^i_{K-1} \in [0, 1]} \ind{\tilde{u}^j_{K-1} \in [0, 1]} \right] \tilde{u}^i_k \tilde{u}^j_k
        \mid \tilde{u}_{0:K-2}^{1:N} \geq 0
    \right] \\
    &= \frac{1}{4} \E_{\tilde{u}_{0:K-2}^{1:N}}\left[
        \sum_{i=1}^N \sum_{j=1, j\not=i}^N \tilde{u}^i_k \tilde{u}^j_k
        \mid \tilde{u}_{0:K-2}^{1:N} \geq 0
    \right]. \label{eq: var_proof:tmp1}
\end{align}
To make this computation easier, let $\Xi_{ij}=1$ denote the event that $\tilde{u}^i_k$ and $\tilde{u}^j_k$ for $i \not= j$ were resampled from the same control, i.e., $\tilde{u}^i_k = \tilde{u}^j_k$, and let $\beta = \mathbb{P}(\Xi_{ij}=1)$.
When $\Xi_{ij}=1$, $\tilde{u}^i_k$ and $\tilde{u}^j_k$ are the same random variable, and therefore it follows that
\begin{align}
    \E_{\tilde{u}_{0:K-2}^{1:N}}\left[ \tilde{u}^i_k \tilde{u}^j_k \mid \Xi_{ij}=1 \mid \tilde{u}_{0:K-2}^{1:N} \geq 0, \Xi_{ij}=1 \right]
    &= \E_{\tilde{u}^i_k}\left[ (\tilde{u}^i_k)^2 \mid \tilde{u}^i_k \geq 0 \right] = \frac{1}{3}.
\end{align}
Otherwise, when $\Xi_{ij}=0$, $\tilde{u}^i_k$ and $\tilde{u}^j_k$ are independent. Hence,
\begin{align}
    \E_{\tilde{u}_{0:K-2}^{1:N}}\left[ \tilde{u}^i_k \tilde{u}^j_k \mid \Xi_{ij}=1 \mid \tilde{u}_{0:K-2}^{1:N} \geq 0, \Xi_{ij}=0 \right]
    &= \E_{\tilde{u}^i_k}\left[ (\tilde{u}^i_k) \mid \tilde{u}^i_k \geq 0 \right]^2 = \frac{1}{4}.
\end{align}
Hence, the expectation becomes
\begin{align}
    \eqref{eq: var_proof:tmp1}
    &= \frac{1}{4} \sum_{i=1}^N \sum_{j=1, j\not=i}^N
    \beta \frac{1}{3}
    + (1-\beta) \frac{1}{4} \\
    &= \frac{1}{4} N(N-1) \left( \beta \frac{1}{3} + (1-\beta) \frac{1}{4} \right) \\
    &\leq \frac{1}{12} N(N-1).\label{eq:term4}
\end{align}
Combining the two terms \eqref{eq:term3} and \eqref{eq:term4}, we thus have that
\begin{align}
    \Var_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} \geq 0 ]
    &\leq \left( \frac{1}{N} \frac{1}{ \frac{1}{2} (1-t)^{K-1} } \right)^2
    \left( \frac{N}{6} + \frac{N(N-1)}{12} \right) - c^2\\
    &\leq \left( \frac{1}{N} \frac{1}{ \frac{1}{2} (1-t)^{K-1} } \right)^2
    \frac{N^2}{6} - c^2\\
    &= \frac{2}{3} \left( \frac{1}{ (1-t)^2} \right)^{K-1} - \frac{1}{4} \left( \frac{1}{ (1-t)^2} \right)^{K-1} \\
    &= \frac{5}{12} \left( \frac{1}{ (1-t)^2} \right)^{K-1}.
\end{align}

Hence, taking both cases into account, we have that
\begin{equation}
    \Var_{\tilde{u}_{0:K-1}^{1:N}}[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} \geq 0 ] \leq \frac{3}{4} \left( \frac{1}{ (1-t)^2} \right)^{K-1}.
\end{equation}
Finally, using the law of total variance, we have that
\begin{equation}
    \Var[ \hat{v}_k ] = \underbrace{\E_{\text{sign of } \tilde{u}_{0:K-2}^{1:N}}[ \Var[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} ] ]}_{\circled{1}} 
    + \underbrace{\Var_{\text{sign of } \tilde{u}_{0:K-2}^{1:N}}[ \E[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} ] ]}_{\circled{2}}.
\end{equation}
The first term gives
\begin{align}
    \circled{1}
    &= \mathbb{P}(\tilde{u}^{1:N}_{0:K-2} \geq 0) \Var[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} \geq 0 ] + \mathbb{P}(\tilde{u}^{1:N}_{0:K-2} < 0) \Var[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} < 0 ] \\
    &\leq (1 - t)^{K-1} \frac{3}{4} \left( \frac{1}{ (1-t)^2} \right)^{K-1} + 0 \\
    &= \frac{3}{4} \left(\frac{1}{1-t} \right)^{K-1}.
\end{align}
The second term gives
\begin{align}
    \circled{2}
    &= \mathbb{P}(\tilde{u}^{1:N}_{0:K-2} \geq 0) (\E[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} \geq 0 ] - \E[ \hat{v}_k ])^2 + \mathbb{P}(\tilde{u}^{1:N}_{0:K-2} < 0) (\E[ \hat{v}_k \mid \tilde{u}_{0:K-2}^{1:N} < 0 ] - \E[ \hat{v}_k ])^2 \\
    &= (1 - t)^{K-1} \left( \frac{1}{2} \frac{1}{(1-t)^{K-1}} - \frac{1}{2} \right)^2 + (1 - (1 - t)^{K-1}) \frac{1}{4} \\
    &= \frac{1}{2} (1 - t)^{K-1} - 1 + \frac{1}{2} \frac{1}{(1-t)^{K-1}} + \frac{1}{4} - \frac{1}{4} (1 - t)^{K-1} \\
    &= \frac{1}{4} (1 - t)^{K-1} - \frac{3}{4} + \frac{1}{2} \frac{1}{(1-t)^{K-1}}.
\end{align}
Hence, combining both terms gives us
\begin{align}
    \Var[ \hat{v}_k ]
    &\leq \frac{3}{4} \left(\frac{1}{1-t} \right)^{K-1} + \frac{1}{4} (1 - t)^{K-1} - \frac{3}{4} + \frac{1}{2} \left(\frac{1}{1-t}\right)^{K-1} \\
    &= \frac{5}{4} \left(\frac{1}{1-t} \right)^{K-1} + \frac{1}{4} (1 - t)^{K-1} - \frac{3}{4} \\
    &= O\left( \left(\frac{1}{1 - 2^{-N}}\right)^{K} + (1-2^{-N})^{K} \right).
\end{align}

\subsection{Proof of Theorem~\ref{thm:ess}} \label{sec:proof:ess}
\begin{proof}
    We will prove the equivalent statement that
    \begin{equation} \label{eq: ess:proof:1}
        \norm{ \tilde{\vw} }^2_2 - \norm{ \tilde{\vw}' }^2_2 \geq 0.
    \end{equation}
    To begin, we expand $\tilde{\vw}'$ to obtain
    \begin{equation}
        \norm{\tilde{\vw}'}_2^2 = \frac{ \norm{\vw}_2^2 + \norm{\vc}_2^2 }{ \norm{\vw'}_1^2}.
    \end{equation}
    Then,
    \begin{align}
        \mathrel{\phantom{=}} \norm{ \tilde{\vw} }^2_2 - \norm{ \tilde{\vw}' }^2_2 &= \frac{ \norm{\vw}_2^2 }{ \norm{\vw}_1^2} + \frac{ -\norm{\vw}_2^2 - \norm{\vc}_2^2 }{ \norm{\vw'}_1^2} \\
        &= \frac{1}{ \norm{\vw'}_1^2 } \left( \norm{\vw'}_1^2 \frac{ \norm{\vw}_2^2 }{ \norm{\vw}_1^2} -\norm{\vw}_2^2 - \norm{\vc}_2^2 \right).\label{eq:parenthesis_term}
    \end{align}
    Simplifying the first term, yields,
    \begin{align}
        \mathrel{\phantom{=}} \norm{\vw'}_1^2 \frac{ \norm{\vw}_2^2 }{ \norm{\vw}_1^2} &= \left( \norm{\vw}_1^2 + 2 \norm{\vw}_1 \norm{\vc}_1 + \norm{\vc}_1^2 \right) \frac{ \norm{\vw}_2^2 }{ \norm{\vw}_1^2} \\
        &= \norm{\vw}_2^2 + 2 \frac{ \norm{ \vw }_2^2 \norm{ \vc }_1 }{ \norm{ \vw }_1 } + \frac{ \norm{ \vw }_2^2 \norm{ \vc }_1^2 }{ \norm{ \vw }_1^2 }.\label{eq:D91}
    \end{align}
    Substituting \eqref{eq:D91} back into the parenthesis \eqref{eq:parenthesis_term} gives us that
    \begin{align}
        \mathrel{\phantom{=}} \norm{\vw'}_1^2 \frac{ \norm{\vw}_2^2 }{ \norm{\vw}_1^2} -\norm{\vw}_2^2 - \norm{\vc}_2^2 &= 2 \frac{ \norm{ \vw }_2^2 \norm{ \vc }_1 }{ \norm{ \vw }_1 } + \frac{ \norm{ \vw }_2^2 \norm{ \vc }_1^2 }{ \norm{ \vw }_1^2 } - \norm{\vc}_2^2 \\
        &\geq 2 \frac{ \norm{ \vw }_2^2 \norm{ \vc }_1 }{ \norm{ \vw }_1 } + \frac{ \norm{ \vw }_2^2 \norm{ \vc }_1^2 }{ \norm{ \vw }_1^2 } - \norm{\vc}_1^2 \\
        &= \norm{\vc}_1 \left( 2 \frac{ \norm{ \vw }_2^2 }{ \norm{ \vw }_1 } + \frac{ \norm{ \vw }_2^2 \norm{ \vc }_1 }{ \norm{ \vw }_1^2 } - \norm{\vc}_1 \right).\label{eq:D94}
    \end{align}
    Simplifying the parenthesis in \eqref{eq:D94}, we obtain
    \begin{align}
        \mathrel{\phantom{=}} 2 \frac{ \norm{ \vw }_2^2 }{ \norm{ \vw }_1 } + \frac{ \norm{ \vw }_2^2 \norm{ \vc }_1 }{ \norm{ \vw }_1^2 } - \norm{\vc}_1 &= 2 \frac{ \norm{ \vw }_2^2 }{ \norm{ \vw }_1 } + \norm{\vc}_1 \left( \frac{ \norm{ \vw }_2^2 }{ \norm{ \vw }_1^2 } - 1 \right) \\
        &\geq 2 \frac{ \norm{ \vw }_2^2 }{ \norm{ \vw }_1 } + \norm{\vc}_1 \left( \frac{1}{N} - 1 \right).
    \end{align}
    Using our assumption that $\vc$ is not ``drastically larger'' than $\vw$ in \eqref{eq: ess:assumption} then gives us the desired result.
\end{proof}