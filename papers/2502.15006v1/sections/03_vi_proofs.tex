\section{Proofs for Variational Inference MPC} 
\subsection{Derivations of the Variational Inference Updates} \label{app:VI Gauss}
Let $q_\vv$ be the density function of a Gaussian distribution with mean $\vv$ and covariance $\Sigma$, i.e.,
\begin{equation}
    q_\vv(\vu) = Z_q^{-1} \exp\left(-\frac{1}{2}(\vu-\vv)^\top\Sigma^{-1}(\vu-\vv)\right),
\end{equation}
where the normalization constant $Z_q$ is independent of the mean $\vv$.
We wish to solve the variational inference problem
\begin{equation}
    \min_{\vv} \quad \KL{ p(\vu \mid o=1) }{ q_\vv(\vu) }.
\end{equation}
Expanding and ignoring terms unrelated to $\vv$, we get
\begin{align}
    \KL{ p(\vu \mid o=1) }{ q_\vv(\vu) }
    &= \int p(\vu \mid o=1) \log\left(\frac{p(\vu \mid o=1)}{q_\vv(\vu)}\right) \diff\vu \\
    &= \int p(\vu \mid o=1) \log\left(p(\vu \mid o=1)\right) \diff\vu -\int p(\vu \mid o=1) \log(q_\vv(\vu)) \diff\vu\\
    &= -\int p(\vu \mid o=1) \log(q_\vv(\vu)) \diff\vu + O(1) \\
    &=  \int p(\vu \mid o=1) \left( \frac{1}{2} (\vu - \vv)\T \Sigma^{-1} (\vu - \vv) + \log Z_q \right) \diff\vu + O(1) \\
    &= \E_{p(\vu \mid o=1)}\left[ \frac{1}{2} (\vu - \vv)\T \Sigma^{-1} (\vu - \vv) + \log Z_q \right] + O(1) \\
    &= \E_{p(\vu \mid o=1)}\left[ \frac{1}{2} (\vu - \vv)\T \Sigma^{-1} (\vu - \vv) \right] + O(1).
\end{align}
Taking the derivative with respect to $\vv$ and setting to zero, we get
\begin{equation}
    \E_{p(\vu \mid o=1)}\left[ \Sigma^{-1} (\vu - \vv^*) \right] = 0.
\end{equation}
Rearranging, yields
\begin{equation}
    \vv^* = \E_{p(\vu \mid o=1)}\left[ \vu \right].
\end{equation}

\subsection{Derivations of the Self Normalized Importance Sampling Estimator} \label{app:VI SNIS}
We next derive the self-normalized importance sampling (SNIS) estimator $\hat{\vv}$ in \eqref{eq:vi:v_opt} of $\vv^*$ in \eqref{eq:vi:omega_def}.
Note that, since $Z=\int \exp(-J(\vu)) p_0(\vu) \diff{\vu}$ is unknown, we cannot compute the weights $\omega(\vu)$ in \eqref{eq:vi:omega_def} directly.
Hence, the regular importance sampled Monte Carlo estimator
\begin{equation} \label{eq:vi_deriv:is_estimator}
    \hat{\vv} = \frac{1}{N} \sum_{i=1}^N \omega(\vu^i) \vu^i,
\end{equation}
is not computable.
Instead, we can use the SNIS estimator \cite{owen2013monte}, which can be derived as follows.
First, note that $Z$ can be written as the expectation of $\exp(-J(\vu))$, i.e.,
\begin{equation}
    Z = \E_{p_0(\vu)}\left[ \exp(-J(\vu)) \right].
\end{equation}
Hence, one idea is to reuse the samples $\vu^i \sim r$ to compute an estimate $\hat{Z}$ of $Z$, that is,
\begin{equation}
    \hat{Z} = \frac{1}{N} \sum_{i=1}^N \exp(-J(\vu^i)) p_0(\vu^i) / r(\vu^i),
\end{equation}
where $\hat{Z}$ is a ``normal'' importance sampled Monte Carlo estimator of $Z$.
If we use the estimate $\hat{Z}$ to compute the weights $\hat{\omega}(\vu)$ in the normal importance sampled Monte Carlo estimator $\hat{\vv}$ \eqref{eq:vi_deriv:is_estimator}, we obtain the SNIS estimator as in \eqref{eq:vi:v_opt} and \eqref{eq:vi:omega_def}
\begin{align}
    \hat{\vv}
    &= \frac{1}{N} \sum_{i=1}^N \hat{\omega}(\vu^i) \vu^i, \\
    &= \frac{1}{N} \sum_{i=1}^N \left( \frac{1}{ \hat{Z} } \frac{ \exp(-J(\vu^i))p_0(\vu^i) }{ r(\vu^i) }  \right) \vu^i, \\
    &= \frac{1}{N} \sum_{i=1}^N \left( \frac{1}{ \frac{1}{N} \sum_{j=1}^N \exp(-J(\vu^j)) p_0(\vu^j) / r(\vu^j) } \frac{ \exp(-J(\vu^i))p_0(\vu^i) }{ r(\vu^i) }  \right) \vu^i, \\
    &= \frac{1}{N} \sum_{i=1}^N \left( \frac{1}{ \frac{1}{N} \sum_{j=1}^N Z^{-1} \exp(-J(\vu^j)) p_0(\vu^j) / r(\vu^j) } \frac{ Z^{-1} \exp(-J(\vu^i))p_0(\vu^i) }{ r(\vu^i) }  \right) \vu^i, \\
    &= \frac{1}{N} \sum_{i=1}^N \left( \frac{1}{ \frac{1}{N} \sum_{j=1}^N Z^{-1} \exp(-J(\vu^j)) p_0(\vu^j) / r(\vu^j) } \omega(\vu^i)  \right) \vu^i, \\
    &= \frac{1}{N} \sum_{i=1}^N \left( \frac{\omega(\vu^i)}{ \frac{1}{N} \sum_{j=1}^N \omega(\vu^j) }  \right) \vu^i, \\
    &= \sum_{i=1}^N \left( \frac{\omega(\vu^i)}{ \sum_{j=1}^N \omega(\vu^j) }  \right) \vu^i, \\
    &= \sum_{i=1}^N \tilde{\omega}^i \vu^i,
\end{align}
where,
\begin{align}
    \tilde{\omega}^i = \frac{\omega(\vu^i)}{\sum^N_{j=1}\omega(\vu^j)}.
\end{align}
Unfortunately, the use of the estimated $\hat{Z}$ in the weights $\omega(\vu)$ causes the SNIS estimator to be biased \cite{owen2013monte}.

\subsection{MPPI as a Special Case of the Variational Inference Update} \label{app:MPPI as VI}
\begin{theorem}
    When $p_0(\vu) = q_{\bf 0}(\vu) := q_{\bf v = 0}(\vu)$ and $r(\vu)=q_{\bar{\vv}}(\vu)$ for some previous estimate of the optimal control sequence $\bar{\vv}$, the equation for the weights $\omega(\vu)$ in the variational inference MPC update \eqref{eq:vi:omega_def} reduces to that of MPPI.
\end{theorem}

\begin{proof}
    Let $q_\vv$ be the density function for a Gaussian distribution with mean $\vv$ and covariance $\Sigma$, that is,
    \begin{equation}
        q_\vv(\vu) = Z_q^{-1} \exp\left(-\frac{1}{2}(\vu-\vv)^\top\Sigma^{-1}(\vu-\vv)\right),
    \end{equation}
    By choosing $p_0(\vu) = q_{\bf 0}(\vu)$ and $r(\vu)=q_{\bar{\vv}}(\vu)$, we have that
    \begin{align}
        \frac{p_0(\vu)}{r(\vu)}
        &= \frac{ Z_q^{-1} \exp\left(-\frac{1}{2} \vu^\top\Sigma^{-1}\vu \right) }
        { Z_q^{-1} \exp\left(-\frac{1}{2}(\vu-\vv)^\top\Sigma^{-1}(\vu-\vv)\right) } \\
        &= \exp(-\frac{1}{2} \vu^\top\Sigma^{-1}\vu + \frac{1}{2}(\vu-\vv)^\top\Sigma^{-1}(\vu-\vv)) \\
        &=\exp(-\frac{1}{2} \vu^\top\Sigma^{-1}\vu + \frac{1}{2} \vu^\top\Sigma^{-1}\vu -\vu^\top\Sigma^{-1}\vv + \frac{1}{2}\vv^\top\Sigma^{-1}\vv)\\
        &= \exp(-\vu^\top\Sigma^{-1}\vv + \frac{1}{2}\vv^\top\Sigma^{-1}\vv).
    \end{align}
    Hence,
    \begin{align}
        \omega(\vu)
        &= \frac{Z^{-1} \exp(-J(\vu)) p_0(\vu)}{r(\vu)} \\
        &= Z^{-1} \exp(-[ J(\vu) + \vu^\top\Sigma^{-1}\vv]) \exp(\frac{1}{2} \vv\T \Sigma^{-1} \vv) \\
        &\propto \exp(-[ J(\vu) + \vu^\top\Sigma^{-1}\vv]),
    \end{align}
    where the last line follows since terms not dependent on $\vu$ can be canceled out between the numerator and denominator in \eqref{eq:vi:omega_tilde}.
\end{proof}