\section{Related Work}
\subsection{Traditional Methods}

Traditional MVO methods are highly interpretable. LSD-SLAM \cite{engel2014lsd} uses pixel intensities for pose estimation, sliding window optimization for depth maps, and loop closure. DSO \cite{engel2017direct} improves photometric calibration and jointly optimizes pose and depth. ORB-SLAM \cite{campos2021orb} relies on ORB features for matching and pose estimation, while SVO \cite{forster2014svo} optimizes sparse features using image gradients for a balance of efficiency and accuracy. \textcolor{black}{These methods anchor the scale using initial depth estimates from the first frame. However, errors from calibration, feature mismatches, or motion blur accumulate during iterative pose updates, causing scale drift and limiting their applicability to long-distance tasks.}

\subsection{Learning-Based Methods Under Perspective View}

Learning-based methods under the perspective view generally follow two approaches: employing CNNs and MLPs for pose estimation, or using side-tasks such as depth and optical flow prediction, which can further be divided into self-supervised or auxiliary supervision methods.

Early works, such as \cite{konda2015learning} and DeepVO \cite{wang2017deepvo}, rely on feature extraction and pose regression. However, both methods face limitations in interpretability, which affects their robustness in challenging scenarios.

UnDeepVO \cite{li2018undeepvo} achieves scale-absolute odometry by using stereo depth during training. TartanVO \cite{wang2021tartanvo} incorporates optical flow supervision and a scale-invariant loss to improve generalization. Self-supervised methods, such as \cite{bian2019unsupervised} and \cite{li2020self}, rely on photometric loss and SSIM but struggle with motion blur, lighting changes, and low-texture areas.

DF-VO \cite{zhan2021df} combines depth estimation and bidirectional optical flow for 3D localization, integrating deep learning with multi-view geometry. DROID-SLAM \cite{teed2021droid} estimates motion through feature extraction and inter-frame correlation, using a differentiable Dense Bundle Adjustment (DBA) layer for joint optimization of camera poses and 3D structure, significantly improving scale consistency and robustness. DPVO \cite{teed2024deep} builds on DROID-SLAM with sparse feature tracking, further enhancing efficiency.

\textcolor{black}{These methods anchor the scale using neural network parameters and depth estimation. In contrast, optical flow, which does not have a direct correspondence to the metric scale, is typically used as an auxiliary source. However, anchoring the scale directly through neural network parameters suffers from limited generalization due to the data-driven, black-box nature of the approach. Depth-based scale anchoring under a perspective view often requires additional tasks, and achieving stable performance usually necessitates extra supervision.}

\subsection{Learning-Based Methods Under BEV Representation}

Recently, some odometry methods employing BEV representation have emerged, but they primarily rely on segmentation results for pose estimation rather than directly performing the odometry task.

BEV-SLAM \cite{ross2022bev} generates BEV semantic segmentation maps using CNNs and aligns them with existing map features for pose estimation. BEV-Locator \cite{zhang2022bev} employs a BEV encoder to convert perspective images and matches semantic map elements using a cross-modal Transformer. OCC-VO \cite{li2023occ} transforms multi-camera images into 3D semantic occupancy clouds and registers them with a global map. However, these methods rely heavily on semantic segmentation, which incurs high annotation costs and limits end-to-end optimization with pose supervision, making their performance dependent on segmentation accuracy.

\textcolor{black}{Unlike traditional methods prone to scale drift and learning-based approaches requiring extra supervision and complex designs, our method fully exploits the BEV representation by directly anchoring scale to its unified, metric-scaled nature. By simplifying 6-DoF pose estimation to the primary 3-DoF motions of ground vehicles, our method enables the network to learn scale information end-to-end using only pose supervision, achieving robust scale consistency without relying on any side-tasks.}