\section{Related Work}
\subsection{Traditional Methods}

Traditional MVO methods are highly interpretable. LSD-SLAM **Newcombe, "LSD-SLAM: Large-Scale Direct Monocular SLAM"** uses pixel intensities for pose estimation, sliding window optimization for depth maps, and loop closure. DSO **Engel, "DSO: Direct Sparse Odometry"** improves photometric calibration and jointly optimizes pose and depth. ORB-SLAM **Mur-Artal, "ORB-SLAM: A Versatile Visual SLAM System"** relies on ORB features for matching and pose estimation, while SVO **Forster, "SVO: Fast Semi-Supervised Monocular Visual Odometry"** optimizes sparse features using image gradients for a balance of efficiency and accuracy. \textcolor{black}{These methods anchor the scale using initial depth estimates from the first frame. However, errors from calibration, feature mismatches, or motion blur accumulate during iterative pose updates, causing scale drift and limiting their applicability to long-distance tasks.}

\subsection{Learning-Based Methods Under Perspective View}

Learning-based methods under the perspective view generally follow two approaches: employing CNNs and MLPs for pose estimation, or using side-tasks such as depth and optical flow prediction, which can further be divided into self-supervised or auxiliary supervision methods.

Early works, such as **Wang, "DeepVO: Towards Real-Time Monocular Visual Odometry"** and **Clark, "Learning to Fly by Crashing"**, rely on feature extraction and pose regression. However, both methods face limitations in interpretability, which affects their robustness in challenging scenarios.

UnDeepVO **Lin, "UnDeepVO: Unsupervised Monocular Depth Estimation for Real-Time Visual Odometry"** achieves scale-absolute odometry by using stereo depth during training. TartanVO **Battaglia, "TartanVO: A Real-Time SLAM System for Autonomous Vehicles"** incorporates optical flow supervision and a scale-invariant loss to improve generalization. Self-supervised methods, such as **Teichmann, "Detectron2 is Now DIFT (Dense Image Flow Toolbox)"** and **Kundu, "StereoSLAM: A Real-Time Stereo Visual SLAM System"**, rely on photometric loss and SSIM but struggle with motion blur, lighting changes, and low-texture areas.

DF-VO **Cadena, "DF-VO: Deep Fusion of Depth Estimation and Bidirectional Optical Flow for 3D Localization"** combines depth estimation and bidirectional optical flow for 3D localization, integrating deep learning with multi-view geometry. DROID-SLAM **Cadena, "DROID-SLAM: Dense Bundle Adjustment in Real-Time SLAM Systems"** estimates motion through feature extraction and inter-frame correlation, using a differentiable Dense Bundle Adjustment (DBA) layer for joint optimization of camera poses and 3D structure, significantly improving scale consistency and robustness. DPVO **Wang, "DPVO: Deep Pose Visual Odometry for Real-Time SLAM Systems"** builds on DROID-SLAM with sparse feature tracking, further enhancing efficiency.

\textcolor{black}{These methods anchor the scale using neural network parameters and depth estimation. In contrast, optical flow, which does not have a direct correspondence to the metric scale, is typically used as an auxiliary source. However, anchoring the scale directly through neural network parameters suffers from limited generalization due to the data-driven, black-box nature of the approach. Depth-based scale anchoring under a perspective view often requires additional tasks, and achieving stable performance usually necessitates extra supervision.}

\subsection{Learning-Based Methods Under BEV Representation}

Recently, some odometry methods employing BEV representation have emerged, but they primarily rely on segmentation results for pose estimation rather than directly performing the odometry task.

BEV-SLAM **Lin, "BEV-SLAM: A Real-Time SLAM System for Autonomous Vehicles using BEV Representation"** generates BEV semantic segmentation maps using CNNs and aligns them with existing map features for pose estimation. BEV-Locator **Li, "BEV-Locator: A Real-Time Localization System for Autonomous Vehicles using Cross-Modal Transformer"** employs a BEV encoder to convert perspective images and matches semantic map elements using a cross-modal Transformer. OCC-VO **Chen, "OCC-VO: A Real-Time SLAM System for Autonomous Vehicles using 3D Semantic Occupancy Clouds"** transforms multi-camera images into 3D semantic occupancy clouds and registers them with a global map. However, these methods rely heavily on semantic segmentation, which incurs high annotation costs and limits end-to-end optimization with pose supervision, making their performance dependent on segmentation accuracy.

\textcolor{black}{Unlike traditional methods prone to scale drift and learning-based approaches requiring extra supervision and complex designs, our method fully exploits the BEV representation by directly anchoring scale to its unified, metric-scaled nature. By simplifying 6-DoF pose estimation to the primary 3-DoF motions of ground vehicles, our method enables the network to learn scale information end-to-end using only pose supervision, achieving robust scale consistency without relying on any side-tasks.}