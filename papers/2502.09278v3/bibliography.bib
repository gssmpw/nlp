
%% IEEEexample.bib 
%% V1.10 
%% 2002/09/27
%% Copyright (c) 2002 by Michael Shell
%% mshell(at)ece.gatech.edu
%% See support website below for current contact information.
%% 
%% NOTE: This text file uses MS Windows line feed conventions. When (human)
%% reading this file on other platforms, you may have to use a text
%% editor that can handle lines terminated by the MS Windows line feed
%% characters (0x0D 0x0A).
%% 
%% This is an example BibTeX database for the official IEEEtran.bst
%% BibTeX style file.
%% 
%% Some entries call strings that are defined in the IEEEabrv.bib file.
%% Therefore, IEEEabrv.bib should be loaded prior to this file. 
%% Usage: 
%% 
%% \bibliographystyle{./IEEEtran} % use IEEEtran.bst style
%% \bibliography{./IEEEabrv,./IEEEexample}
%% 
%% 
%% Support sites:
%% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/IEEEtran/ 
%% and/or
%% http://www.ieee.org
%%**********************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% This code is distributed under the Perl Artistic License 
%% ( http://language.perl.com/misc/Artistic.html ) 
%% and may be freely used, distributed and modified - subject to the
%% constraints therein.
%% Retain all contribution notices, credits and disclaimers.
%% 
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%**********************************************************************


% Note that, because the example references were taken from actual IEEE
% publications, these examples do not always contain the full amount
% of information that may be desirable (for use with other BibTeX styles).
% In particular, full names (not abbreviated with initials) should be
% entered whenever possible as some (non-IEEE) bibliography styles use
% full names. IEEEtran.bst will automatically abbreviate when it
% encounters full names.

@misc{janus, title={Janus}, url={https://en.wikipedia.org/wiki/Janus}, journal={Wikipedia}, publisher={Wikimedia Foundation}, author={}, year={2024}, month={Aug}}

@INPROCEEDINGS{gen2sim,
  author={Katara, Pushkal and Xian, Zhou and Fragkiadaki, Katerina},
  booktitle={IEEE International Conference on Robotics and Automation}, 
  title={Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models}, 
  year={2024},
  volume={},
  number={},
  pages={6672-6679},
  keywords={Training;Solid modeling;Three-dimensional displays;Training data;Reinforcement learning;Manipulators;Robot learning}}

@inproceedings{poole2022dreamfusion,
    title= "DreamFusion: Text-to-3D using 2D Diffusion",
    author="Ben Poole and Ajay Jain and Jonathan T. Barron and Ben Mildenhall",
    booktitle="The Eleventh International Conference on Learning Representations",
    year="2023"
}

@article{mildenhall2020nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{tang2024dreamgaussian,
title={DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation},
author={Jiaxiang Tang and Jiawei Ren and Hang Zhou and Ziwei Liu and Gang Zeng},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@InProceedings{Liu_2023_ICCV,
    author    = {Liu, Ruoshi and Wu, Rundi and Van Hoorick, Basile and Tokmakov, Pavel and Zakharov, Sergey and Vondrick, Carl},
    title     = {Zero-1-to-3: Zero-shot One Image to 3D Object},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
    month     = {October},
    year      = {2023},
    pages     = {9298-9309}
}

@Article{kerbl3Dgaussians,
      author       = {Kerbl, Bernhard and Kopanas, Georgios and Leimk{\"u}hler, Thomas and Drettakis, George},
      title        = {3D Gaussian Splatting for Real-Time Radiance Field Rendering},
      journal      = {ACM Transactions on Graphics},
      number       = {4},
      volume       = {42},
      month        = {July},
      year         = {2023}
}

@inproceedings{
shi2023MVDream,
title={{MVD}ream: Multi-view Diffusion for 3D Generation},
author={Yichun Shi and Peng Wang and Jianglong Ye and Long Mai and Kejie Li and Xiao Yang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@article{wang2023imagedream,
  title={ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation},
  author={Wang, Peng and Shi, Yichun},
  journal={arXiv preprint arXiv:2312.02201},
  year={2023}
}

@article{Downs2022GoogleSO,
  title={Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household Items},
  author={Laura Downs and Anthony Francis and Nate Koenig and Brandon Kinman and Ryan Michael Hickman and Krista Reymann and Thomas Barlow McHugh and Vincent Vanhoucke},
  journal={2022 International Conference on Robotics and Automation},
  year={2022},
  pages={2553-2560}
}

@article{shi2023zero123plus,
  publtype={informal},
  author={Ruoxi Shi and Hansheng Chen and Zhuoyang Zhang and Minghua Liu and Chao Xu and Xinyue Wei and Linghao Chen and Chong Zeng and Hao Su},
  title={Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model},
  year={2023},
  cdate={1672531200000},
  journal={CoRR},
  volume={abs/2310.15110}
}

@inproceedings{
liu2024syncdreamer,
title={SyncDreamer: Generating Multiview-consistent Images from a Single-view Image},
author={Yuan Liu and Cheng Lin and Zijiao Zeng and Xiaoxiao Long and Lingjie Liu and Taku Komura and Wenping Wang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@InProceedings{Woo_2024_CVPR,
    author    = {Woo, Sangmin and Park, Byeongjun and Go, Hyojun and Kim, Jin-Young and Kim, Changick},
    title     = {HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2024},
    pages     = {10574-10584}
}

@InProceedings{Long_2024_CVPR,
    author    = {Long, Xiaoxiao and Guo, Yuan-Chen and Lin, Cheng and Liu, Yuan and Dou, Zhiyang and Liu, Lingjie and Ma, Yuexin and Zhang, Song-Hai and Habermann, Marc and Theobalt, Christian and Wang, Wenping},
    title     = {Wonder3D: Single Image to 3D using Cross-Domain Diffusion},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2024},
    pages     = {9970-9980}
}

@article{wang2021neus,
      title={NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction}, 
      author={Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang},
	  journal={NeurIPS},
      year={2021}
}

@article{tang2024lgm,
  title={LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation},
  author={Tang, Jiaxiang and Chen, Zhaoxi and Chen, Xiaokang and Wang, Tengfei and Zeng, Gang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2402.05054},
  year={2024}
}

@inproceedings{wang2024crm,
  title={CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model},
  author={Zhengyi Wang and Yikai Wang and Yifei Chen and Chendong Xiang and Shuo Chen and Dajiang Yu and Chongxuan Li and Hang Su and Jun Zhu},
  booktitle={European Conference on Computer Vision},
  year={2024}
}

@misc{tochilkin2024triposr,
      title={TripoSR: Fast 3D Object Reconstruction from a Single Image}, 
      author={Dmitry Tochilkin and David Pankratz and Zexiang Liu and Zixuan Huang and Adam Letts and Yangguang Li and Ding Liang and Christian Laforte and Varun Jampani and Yan-Pei Cao},
      year={2024},
      eprint={2403.02151},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{
hong2024lrm,
title={{LRM}: Large Reconstruction Model for Single Image to 3D},
author={Yicong Hong and Kai Zhang and Jiuxiang Gu and Sai Bi and Yang Zhou and Difan Liu and Feng Liu and Kalyan Sunkavalli and Trung Bui and Hao Tan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@InProceedings{Zou_2024_CVPR,
    author    = {Zou, Zi-Xin and Yu, Zhipeng and Guo, Yuan-Chen and Li, Yangguang and Liang, Ding and Cao, Yan-Pei and Zhang, Song-Hai},
    title     = {Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2024},
    pages     = {10324-10335}
}

@article{mueller2022instant,
    author = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
    title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
    journal = {ACM Trans. Graph.},
    issue_date = {July 2022},
    volume = {41},
    number = {4},
    month = jul,
    year = {2022},
    pages = {102:1--102:15},
    articleno = {102},
    numpages = {15},
    doi = {10.1145/3528223.3530127},
    publisher = {ACM},
    address = {New York, NY, USA},
}

@InProceedings{Chan_2022_CVPR,
    author    = {Chan, Eric R. and Lin, Connor Z. and Chan, Matthew A. and Nagano, Koki and Pan, Boxiao and De Mello, Shalini and Gallo, Orazio and Guibas, Leonidas J. and Tremblay, Jonathan and Khamis, Sameh and Karras, Tero and Wetzstein, Gordon},
    title     = {Efficient Geometry-Aware 3D Generative Adversarial Networks},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16123-16133}
}

@inproceedings{NEURIPS2022_cebbd24f,
 author = {Gao, Jun and Shen, Tianchang and Wang, Zian and Chen, Wenzheng and Yin, Kangxue and Li, Daiqing and Litany, Or and Gojcic, Zan and Fidler, Sanja},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {31841--31854},
 publisher = {Curran Associates, Inc.},
 title = {GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images},
 volume = {35},
 year = {2022}
}

@article{shen2023flexicubes,
    author = {Shen, Tianchang and Munkberg, Jacob and Hasselgren, Jon and Yin, Kangxue and Wang, Zian 
            and Chen, Wenzheng and Gojcic, Zan and Fidler, Sanja and Sharp, Nicholas and Gao, Jun},
    title = {Flexible Isosurface Extraction for Gradient-Based Mesh Optimization},
    year = {2023},
    issue_date = {August 2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {42},
    number = {4},
    issn = {0730-0301},
    journal = {ACM Trans. Graph.},
    month = {jul},
    articleno = {37},
    numpages = {16}
    }

@inproceedings{shen2021dmtet,
title = {Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis},
author = {Tianchang Shen and Jun Gao and Kangxue Yin and Ming-Yu Liu and Sanja Fidler},
year = {2021},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}

@misc{barda2024magicclay,
      title={MagicClay: Sculpting Meshes With Generative Neural Fields}, 
      author={Amir Barda and Vladimir G. Kim and Noam Aigerman and Amit H. Bermano and Thibault Groueix},
      year={2024},
      eprint={2403.02460},
      archivePrefix={arXiv},
      primaryClass={cs.GR}
}

@article{Chen2023NeuSGNI,
  title={NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance},
  author={Hanlin Chen and Chen Li and Gim Hee Lee},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.00846}
}

@InProceedings{Guedon_2024_CVPR,
    author    = {Gu\'edon, Antoine and Lepetit, Vincent},
    title     = {SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2024},
    pages     = {5354-5363}
}

@inproceedings{10.1145/3641519.3657428,
author = {Huang, Binbin and Yu, Zehao and Chen, Anpei and Geiger, Andreas and Gao, Shenghua},
title = {2D Gaussian Splatting for Geometrically Accurate Radiance Fields},
year = {2024},
isbn = {9798400705250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3641519.3657428},
booktitle = {ACM SIGGRAPH 2024 Conference Papers},
articleno = {32},
numpages = {11},
keywords = {Novel View Synthesis, Radiance Fields, Surface Reconstruction, Surface Splatting},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@misc{zhang2024radegsrasterizingdepthgaussian,
      title={RaDe-GS: Rasterizing Depth in Gaussian Splatting}, 
      author={Baowen Zhang and Chuan Fang and Rakesh Shrestha and Yixun Liang and Xiaoxiao Long and Ping Tan},
      year={2024},
      eprint={2406.01467},
      archivePrefix={arXiv},
      primaryClass={cs.GR}
}

@InProceedings{Yu_2021_CVPR,
    author    = {Yu, Alex and Ye, Vickie and Tancik, Matthew and Kanazawa, Angjoo},
    title     = {pixelNeRF: Neural Radiance Fields From One or Few Images},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2021},
    pages     = {4578-4587}
}

@INPROCEEDINGS {9878785,
author = {Y. Liu and S. Peng and L. Liu and Q. Wang and P. Wang and C. Theobalt and X. Zhou and W. Wang},
booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title = {Neural Rays for Occlusion-aware Image-based Rendering},
year = {2022},
pages = {7814-7823},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@INPROCEEDINGS{8578879,
  author={Cipolla, Roberto and Gal, Yarin and Kendall, Alex},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics}, 
  year={2018},
  pages={7482-7491},
  keywords={Task analysis;Uncertainty;Semantics;Geometry;Image segmentation;Computational modeling}}

@InProceedings{trevithick2021grf,
    author    = {Trevithick, Alex and Yang, Bo},
    title     = {GRF: Learning a General Radiance Field for 3D Representation and Rendering},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
    month     = {October},
    year      = {2021},
    pages     = {15182-15192}
}

@misc{instant-nsr-pl,
    Author = {Yuan-Chen Guo},
    Year = {2022},
    Note = {https://github.com/bennyguo/instant-nsr-pl},
    Title = {Instant Neural Surface Reconstruction}
}

@InProceedings{Tang_2023_ICCV,
    author    = {Tang, Junshu and Wang, Tengfei and Zhang, Bo and Zhang, Ting and Yi, Ran and Ma, Lizhuang and Chen, Dong},
    title     = {Make-It-3D: High-fidelity 3D Creation from A Single Image with Diffusion Prior},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
    month     = {October},
    year      = {2023},
    pages     = {22819-22829}
}

@inproceedings{
qian2024magic,
title={Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors},
author={Guocheng Qian and Jinjie Mai and Abdullah Hamdi and Jian Ren and Aliaksandr Siarohin and Bing Li and Hsin-Ying Lee and Ivan Skorokhodov and Peter Wonka and Sergey Tulyakov and Bernard Ghanem},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@article{objaverse,
  title={Objaverse: A Universe of Annotated 3D Objects},
  author={Matt Deitke and Dustin Schwenk and Jordi Salvador and Luca Weihs and
          Oscar Michel and Eli VanderBilt and Ludwig Schmidt and
          Kiana Ehsani and Aniruddha Kembhavi and Ali Farhadi},
  journal={arXiv preprint arXiv:2212.08051},
  year={2022}
}

@article{objaverseXL,
  title={Objaverse-XL: A Universe of 10M+ 3D Objects},
  author={Matt Deitke and Ruoshi Liu and Matthew Wallingford and Huong Ngo and
          Oscar Michel and Aditya Kusupati and Alan Fan and Christian Laforte and
          Vikram Voleti and Samir Yitzhak Gadre and Eli VanderBilt and
          Aniruddha Kembhavi and Carl Vondrick and Georgia Gkioxari and
          Kiana Ehsani and Ludwig Schmidt and Ali Farhadi},
  journal={arXiv preprint arXiv:2307.05663},
  year={2023}
}

@inproceedings{schoenberger2016sfm,
    author={Sch\"{o}nberger, Johannes Lutz and Frahm, Jan-Michael},
    title={Structure-from-Motion Revisited},
    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016},
}

@inproceedings{schoenberger2016mvs,
    author={Sch\"{o}nberger, Johannes Lutz and Zheng, Enliang and Pollefeys, Marc and Frahm, Jan-Michael},
    title={Pixelwise View Selection for Unstructured Multi-View Stereo},
    booktitle={European Conference on Computer Vision (ECCV)},
    year={2016},
}

@INPROCEEDINGS {8100177,
author = {A. Kendall and R. Cipolla},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Geometric Loss Functions for Camera Pose Regression with Deep Learning},
year = {2017},
volume = {},
issn = {1063-6919},
pages = {6555-6564},
abstract = {Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet [22] is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNets performance across datasets ranging from indoor rooms to a small city.},
keywords = {cameras;quaternions;machine learning;robustness;geometry;measurement;neural networks},
doi = {10.1109/CVPR.2017.694},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}

@article{Rombach2021HighResolutionIS,
  title={High-Resolution Image Synthesis with Latent Diffusion Models},
  author={Robin Rombach and A. Blattmann and Dominik Lorenz and Patrick Esser and Bj{\"o}rn Ommer},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={10674-10685},
}

@article{ZIMNY20248,
title = {Points2NeRF: Generating Neural Radiance Fields from 3D point cloud},
journal = {Pattern Recognition Letters},
volume = {185},
pages = {8-14},
year = {2024},
issn = {0167-8655},
doi = {},
url = {},
author = {Dominik Zimny and Joanna Waczyńska and Tomasz Trzciński and Przemysław Spurek},
keywords = {NeRF, 3D point clouds, Hypernetwork},
abstract = {Neural Radiance Fields (NeRFs) offers a state-of-the-art quality in synthesizing novel views of complex 3D scenes from a small subset of base images. For NeRFs to perform optimally, the registration of base images has to follow certain assumptions, including maintaining a constant distance between the camera and the object. We can address this limitation by training NeRFs with 3D point clouds instead of images, yet a straightforward substitution is impossible due to the sparsity of 3D clouds in the under-sampled regions, which leads to incomplete reconstruction output by NeRFs. To solve this problem, here we propose an auto-encoder-based architecture that leverages a hypernetwork paradigm to transfer 3D points with the associated color values through a lower-dimensional latent space and generate weights of NeRF model. This way, we can accommodate the sparsity of 3D point clouds and fully exploit the potential of point cloud data. As a side benefit, our method offers an implicit way of representing 3D scenes and objects that can be employed to condition NeRFs and hence generalize the models beyond objects seen during training. The empirical evaluation confirms the advantages of our method over conventional NeRFs and proves its superiority in practical applications.}
}

@article{JIN2024160,
title = {Semantic-aware hyper-space deformable neural radiance fields for facial avatar reconstruction},
journal = {Pattern Recognition Letters},
volume = {185},
pages = {160-166},
year = {2024},
issn = {0167-8655},
doi = {},
url = {},
author = {Kaixin Jin and Xiaoling Gu and Zimeng Wang and Zhenzhong Kuang and Zizhao Wu and Min Tan and Jun Yu},
keywords = {Facial avatar reconstruction, Hyper-space deformation, Semantic guidance, Neural radiance fields},
abstract = {High-fidelity facial avatar reconstruction from monocular videos is a prominent research problem in computer graphics and computer vision. Recent advancements in the Neural Radiance Field (NeRF) have demonstrated remarkable proficiency in rendering novel views and garnered attention for its potential in facial avatar reconstruction. However, previous methodologies have overlooked the complex motion dynamics present across the head, torso, and intricate facial features. Additionally, a deficiency exists in a generalized NeRF-based framework for facial avatar reconstruction adaptable to either 3DMM coefficients or audio input. To tackle these challenges, we propose an innovative framework that leverages semantic-aware hyper-space deformable NeRF, facilitating the reconstruction of high-fidelity facial avatars from either 3DMM coefficients or audio features. Our framework effectively addresses both localized facial movements and broader head and torso motions through semantic guidance and a unified hyper-space deformation module. Specifically, we adopt a dynamic weighted ray sampling strategy to allocate varying degrees of attention to distinct semantic regions, enhancing the deformable NeRF framework with semantic guidance to capture fine-grained details across diverse facial regions. Moreover, we introduce a hyper-space deformation module that enables the transformation of observation space coordinates into canonical hyper-space coordinates, allowing for the learning of natural facial deformation and head-torso movements. Extensive experiments validate the superiority of our framework over existing state-of-the-art methods, demonstrating its effectiveness in producing realistic and expressive facial avatars. Our code is available at https://github.com/jematy/SAHS-Deformable-Nerf.}
}

@inproceedings{wu2023omniobject3d,
    author = {Tong Wu and Jiarui Zhang and Xiao Fu and Yuxin Wang and Jiawei Ren, 
    Liang Pan and Wayne Wu and Lei Yang and Jiaqi Wang and Chen Qian and Dahua Lin and Ziwei Liu},
    title = {OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, 
    Reconstruction and Generation},
    booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2023}
}
