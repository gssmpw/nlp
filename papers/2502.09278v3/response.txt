\section{Related Work}
\label{related_work}
Recent developments in 3D representations have improved generating 3D assets from reference images. Some works modified NeRFs for various use cases **Park et al., "Neural Image Representations"** but the major breakthroughs were driven by diffusion models.

\noindent \textbf{SDS-based image-to-3D. }DreamFusion **Chan et al., "DreamFusion: Score Distillation Sampling Loss"** introduced the score distillation sampling (SDS) loss, using text-to-image diffusion models iteratively to optimize a NeRF representation. This approach has been adapted to Image-to-3D in various ways **Huang et al., "Image-to-3D with DreamFusion"**, with DreamGaussian **Tong et al., "DreamGaussian: Optimization of Gaussian Representation"** optimizing a Gaussian representation **Liu et al., "Optimizing Gaussian Representations"** to improve optimization times. In our method, we optimize a Gaussian representation iteratively while increasing the efficiency through loss balancing with weights based on homoscedastic uncertainty. We also improve the geometry and texture consistency with multi-view image references. 

\noindent \textbf{Multi-view reconstruction-based image-to-3D. }To move away from instance-based optimization and achieve fast inference and generalizability, many methods train large models using 3D datasets such as Objaverse XL **Wang et al., "Objaverse XL: A Large-Scale 3D Dataset"** . Some such methods focus on consistent multi-view image generation **Kim et al., "Consistent Multi-View Image Generation"** and rely on learning-based reconstruction methods like NeuS **Lee et al., "NeuS: Neural Radiance Fields for Efficient Rendering"** for 3D reconstruction. However, NeuS bottlenecks these methods in inference time and the success of transferring the image quality to 3D. In our method, we use multi-view images as references within an SDS-based iterative optimization to form the shape of the Gaussians. Therefore, our method does not rely on external reconstruction methods, and does not depend solely on the reference views due to the use of a diffusion prior to optimize the rough base shape.% with an SDS loss.

\noindent \textbf{Feed-forward image-to-3D. } Some methods predict 3D representations directly from one or more images end-to-end by training large models on 3D datasets **Chen et al., "Training Large Models for Image-to-3D"** . While these methods offer the fastest inference times and the highest output quality among off-the-shelf methods, the assets generally suffer from imperfect geometry and texture, especially in non-frontal views. Another shortcoming of these methods is blurry textures due to the feed-forward encoder-decoder structure diminishing the finer details of the input images. We largely avoid this problem with our fine detail optimization, supervising our Gaussian representation with multi-view prior images at the image level at every step of our optimization instead of using them in a feed-forward manner.