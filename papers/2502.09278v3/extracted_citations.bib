@article{JIN2024160,
title = {Semantic-aware hyper-space deformable neural radiance fields for facial avatar reconstruction},
journal = {Pattern Recognition Letters},
volume = {185},
pages = {160-166},
year = {2024},
issn = {0167-8655},
doi = {},
url = {},
author = {Kaixin Jin and Xiaoling Gu and Zimeng Wang and Zhenzhong Kuang and Zizhao Wu and Min Tan and Jun Yu},
keywords = {Facial avatar reconstruction, Hyper-space deformation, Semantic guidance, Neural radiance fields},
abstract = {High-fidelity facial avatar reconstruction from monocular videos is a prominent research problem in computer graphics and computer vision. Recent advancements in the Neural Radiance Field (NeRF) have demonstrated remarkable proficiency in rendering novel views and garnered attention for its potential in facial avatar reconstruction. However, previous methodologies have overlooked the complex motion dynamics present across the head, torso, and intricate facial features. Additionally, a deficiency exists in a generalized NeRF-based framework for facial avatar reconstruction adaptable to either 3DMM coefficients or audio input. To tackle these challenges, we propose an innovative framework that leverages semantic-aware hyper-space deformable NeRF, facilitating the reconstruction of high-fidelity facial avatars from either 3DMM coefficients or audio features. Our framework effectively addresses both localized facial movements and broader head and torso motions through semantic guidance and a unified hyper-space deformation module. Specifically, we adopt a dynamic weighted ray sampling strategy to allocate varying degrees of attention to distinct semantic regions, enhancing the deformable NeRF framework with semantic guidance to capture fine-grained details across diverse facial regions. Moreover, we introduce a hyper-space deformation module that enables the transformation of observation space coordinates into canonical hyper-space coordinates, allowing for the learning of natural facial deformation and head-torso movements. Extensive experiments validate the superiority of our framework over existing state-of-the-art methods, demonstrating its effectiveness in producing realistic and expressive facial avatars. Our code is available at https://github.com/jematy/SAHS-Deformable-Nerf.}
}

@InProceedings{Liu_2023_ICCV,
    author    = {Liu, Ruoshi and Wu, Rundi and Van Hoorick, Basile and Tokmakov, Pavel and Zakharov, Sergey and Vondrick, Carl},
    title     = {Zero-1-to-3: Zero-shot One Image to 3D Object},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
    month     = {October},
    year      = {2023},
    pages     = {9298-9309}
}

@InProceedings{Long_2024_CVPR,
    author    = {Long, Xiaoxiao and Guo, Yuan-Chen and Lin, Cheng and Liu, Yuan and Dou, Zhiyang and Liu, Lingjie and Ma, Yuexin and Zhang, Song-Hai and Habermann, Marc and Theobalt, Christian and Wang, Wenping},
    title     = {Wonder3D: Single Image to 3D using Cross-Domain Diffusion},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2024},
    pages     = {9970-9980}
}

@InProceedings{Tang_2023_ICCV,
    author    = {Tang, Junshu and Wang, Tengfei and Zhang, Bo and Zhang, Ting and Yi, Ran and Ma, Lizhuang and Chen, Dong},
    title     = {Make-It-3D: High-fidelity 3D Creation from A Single Image with Diffusion Prior},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
    month     = {October},
    year      = {2023},
    pages     = {22819-22829}
}

@InProceedings{Woo_2024_CVPR,
    author    = {Woo, Sangmin and Park, Byeongjun and Go, Hyojun and Kim, Jin-Young and Kim, Changick},
    title     = {HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2024},
    pages     = {10574-10584}
}

@InProceedings{Yu_2021_CVPR,
    author    = {Yu, Alex and Ye, Vickie and Tancik, Matthew and Kanazawa, Angjoo},
    title     = {pixelNeRF: Neural Radiance Fields From One or Few Images},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2021},
    pages     = {4578-4587}
}

@article{ZIMNY20248,
title = {Points2NeRF: Generating Neural Radiance Fields from 3D point cloud},
journal = {Pattern Recognition Letters},
volume = {185},
pages = {8-14},
year = {2024},
issn = {0167-8655},
doi = {},
url = {},
author = {Dominik Zimny and Joanna Waczyńska and Tomasz Trzciński and Przemysław Spurek},
keywords = {NeRF, 3D point clouds, Hypernetwork},
abstract = {Neural Radiance Fields (NeRFs) offers a state-of-the-art quality in synthesizing novel views of complex 3D scenes from a small subset of base images. For NeRFs to perform optimally, the registration of base images has to follow certain assumptions, including maintaining a constant distance between the camera and the object. We can address this limitation by training NeRFs with 3D point clouds instead of images, yet a straightforward substitution is impossible due to the sparsity of 3D clouds in the under-sampled regions, which leads to incomplete reconstruction output by NeRFs. To solve this problem, here we propose an auto-encoder-based architecture that leverages a hypernetwork paradigm to transfer 3D points with the associated color values through a lower-dimensional latent space and generate weights of NeRF model. This way, we can accommodate the sparsity of 3D point clouds and fully exploit the potential of point cloud data. As a side benefit, our method offers an implicit way of representing 3D scenes and objects that can be employed to condition NeRFs and hence generalize the models beyond objects seen during training. The empirical evaluation confirms the advantages of our method over conventional NeRFs and proves its superiority in practical applications.}
}

@InProceedings{Zou_2024_CVPR,
    author    = {Zou, Zi-Xin and Yu, Zhipeng and Guo, Yuan-Chen and Li, Yangguang and Liang, Ding and Cao, Yan-Pei and Zhang, Song-Hai},
    title     = {Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2024},
    pages     = {10324-10335}
}

@Article{kerbl3Dgaussians,
      author       = {Kerbl, Bernhard and Kopanas, Georgios and Leimk{\"u}hler, Thomas and Drettakis, George},
      title        = {3D Gaussian Splatting for Real-Time Radiance Field Rendering},
      journal      = {ACM Transactions on Graphics},
      number       = {4},
      volume       = {42},
      month        = {July},
      year         = {2023}
}

@article{mildenhall2020nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{objaverseXL,
  title={Objaverse-XL: A Universe of 10M+ 3D Objects},
  author={Matt Deitke and Ruoshi Liu and Matthew Wallingford and Huong Ngo and
          Oscar Michel and Aditya Kusupati and Alan Fan and Christian Laforte and
          Vikram Voleti and Samir Yitzhak Gadre and Eli VanderBilt and
          Aniruddha Kembhavi and Carl Vondrick and Georgia Gkioxari and
          Kiana Ehsani and Ludwig Schmidt and Ali Farhadi},
  journal={arXiv preprint arXiv:2307.05663},
  year={2023}
}

@inproceedings{poole2022dreamfusion,
    title= "DreamFusion: Text-to-3D using 2D Diffusion",
    author="Ben Poole and Ajay Jain and Jonathan T. Barron and Ben Mildenhall",
    booktitle="The Eleventh International Conference on Learning Representations",
    year="2023"
}

@article{shi2023zero123plus,
  publtype={informal},
  author={Ruoxi Shi and Hansheng Chen and Zhuoyang Zhang and Minghua Liu and Chao Xu and Xinyue Wei and Linghao Chen and Chong Zeng and Hao Su},
  title={Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model},
  year={2023},
  cdate={1672531200000},
  journal={CoRR},
  volume={abs/2310.15110}
}

@inproceedings{tang2024dreamgaussian,
title={DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation},
author={Jiaxiang Tang and Jiawei Ren and Hang Zhou and Ziwei Liu and Gang Zeng},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@article{tang2024lgm,
  title={LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation},
  author={Tang, Jiaxiang and Chen, Zhaoxi and Chen, Xiaokang and Wang, Tengfei and Zeng, Gang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2402.05054},
  year={2024}
}

@misc{tochilkin2024triposr,
      title={TripoSR: Fast 3D Object Reconstruction from a Single Image}, 
      author={Dmitry Tochilkin and David Pankratz and Zexiang Liu and Zixuan Huang and Adam Letts and Yangguang Li and Ding Liang and Christian Laforte and Varun Jampani and Yan-Pei Cao},
      year={2024},
      eprint={2403.02151},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{trevithick2021grf,
    author    = {Trevithick, Alex and Yang, Bo},
    title     = {GRF: Learning a General Radiance Field for 3D Representation and Rendering},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
    month     = {October},
    year      = {2021},
    pages     = {15182-15192}
}

@article{wang2021neus,
      title={NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction}, 
      author={Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang},
	  journal={NeurIPS},
      year={2021}
}

@article{wang2023imagedream,
  title={ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation},
  author={Wang, Peng and Shi, Yichun},
  journal={arXiv preprint arXiv:2312.02201},
  year={2023}
}

@inproceedings{wang2024crm,
  title={CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model},
  author={Zhengyi Wang and Yikai Wang and Yifei Chen and Chendong Xiang and Shuo Chen and Dajiang Yu and Chongxuan Li and Hang Su and Jun Zhu},
  booktitle={European Conference on Computer Vision},
  year={2024}
}

