
\paragraph{Hyperparameter transfer}
\Citet{yang2021tensor,yang2022tensor} showed that there exists a parameterization (i.e., a choice of initialization and layerwise stepsize scaling) for which the features in every single layer evolve in a width-independent manner.
The so-called Maximal Update Parametrization ($\mu$P) allows transferring optimal hyperparameter from a small proxy model to a large model.

A relationship with the spectral norm was established in \citet{yang2023spectral}.
An operator norm perspective was taken in the modular norm framework of \citet{large2024scalable,bernstein2024modular}, which was used to show Lipschitz continuity with constants independent of width. %
We build on this perspective and propose the $1 \rightarrow \infty$ operator norm, which leads to a sign update rule.

\paragraph{Steepest descent in a normed space}
Steepest descent in a possibly non-Euclidean space can be written in terms of the $\lmo$ (\textit{cf}., \Cref{sec:fenchel})
provided a stream of stochastic gradients $(g^k)_{k\in \mathbb N}$ and an initialization $x^0\in\mathcal X$,
\begin{equation}\label{eq:SSD}
x^{k+1} = x^k - \gamma [g^k]^\sharp = x^k + \tfrac{\gamma}{\rho}\|g^k\|_*\lmo(g^k), 
\end{equation}
where $ [\cdot]^\sharp:=\argmax_{x \in \mathcal X} \braket{\cdot,x} - \tfrac{1}{2}\|x\|^2$ is the sharp-operator \citep{nesterov2012efficiency,kelner2014almost}. 

\looseness=-1 The deterministic case is analyzed in \citet{nesterov2012efficiency,kelner2014almost}, and was extended to the stochastic case in \citet{carlson2015stochasticb}, 
with a particular empirically focus on the spectral norm, named as (preconditioned) stochastic spectral descent (SSD) \citep{carlson2015stochastic,carlson2015stochasticb,carlson2015preconditioned}. Their SSD algorithm is an instance of the Majorization-Minimization (MM) algorithms \citep{lange2016mm}, which iteratively minimizes a locally tight upper bound.

The dualization in \citet{bernstein2024modular} is also performed by the sharp-operator.
In contrast to \eqref{eq:SSD}, \ref{eq:uSCG} and \ref{eq:SCG} are invariant to the magnitude of the gradients and do not need to compute the dual norm $\|\cdot\|_*$, which  cannot be computed independently across layers.

Unlike the $\lmo$-based schemes, extending sharp-operator-based algorithms to handle constrained problems is nontrivial even in the vector case \citep{el2018learning}. 
Additionally, a practical concern of using, say, spectral norm projections in deep learning is that the model weights themselves can be dense (so the required SVD would be expensive), while gradients used in the $\lmo$ are usually low-rank (allowing efficient SVD approximations).

\paragraph{Muon} The Muon optimizer \citep{jordan2024muon} is introduced as a steepest descent method. 
The implementation interestingly ignores the scaling $\|\cdot\|_*$ appearing in the update (\textit{cf.}, \eqref{eq:SSD}), so Muon is effectively using the $\lmo$ over the spectral norm instead of the sharp operator.
Provided a stream of stochastic gradients $(g^k)_{k\in \mathbb N}$ and an initialization $x^0\in\mathcal X$ the Muon optimizer can then be written as follows
\begin{align*}\label{eq:Muon}
\tag{Muon}
G^k &= g^k + \beta G^{k-1} \\
x^{k+1} &= \begin{cases}
x^{k} + \gamma \lmo(g^k + \beta G^k) & \text{if Nesterov} \\
x^{k} + \gamma \lmo(G^k) & \text{otherwise}
\end{cases}
\end{align*}
where the $\lmo$ corresponds implicitly to the spectral norm.

The accumulation $G^k$ can be written in terms of the averaged gradient $d^k$ in \Cref{alg:SCG}.
We have that $d^k = \alpha G^k$ by picking $\alpha = (1-\beta)$.
Since the $\lmo$ is scale invariant, $d^k$ and $G^k$ can be used interchangeably without changing the update.
Thus, we can alternatively write Muon (with non-Nesterov based momentum) exactly as \ref{eq:uSCG}.

In practice Muon is only applied to hidden layers, thus excluding the first layer and the last layer for which Adam(W) is used.
In contrast, we apply \ref{eq:uSCG} and \ref{eq:SCG} to all layers and demonstrate transferability of the stepsize.

\paragraph{Sign}
SignSGD and the momentum variant Signum were brought to prominence and further analyzed in \citet{bernstein2018signsgd} motivated by efficient communication for distributed optimization, while they are originally introduced with the dual norm scaling and used only for weight bias updates in \citet{carlson2015stochastic,carlson2015stochasticb,carlson2015preconditioned}.
These schemes are typically studied under the framework of steepest descent, which results in the $\|g^k\|_1$ stepsize scaling in \eqref{eq:SSD} usually not present in practice as remarked in \citet{balles2020geometry}.

\paragraph{Normalization}
The LARS optimizer \citep{you2017large} uses normalized gradient and was shown to be particularly useful for large batch settings.
The method can be viewed as performing normalized SGD with momentum \citep{cutkosky2020momentum} \emph{layerwise} with a particular adaptive parameter-dependent stepsize.

The layerwise normalization can be captured by \ref{eq:uSCG} with the norm choice $\max_\ell \|W_\ell\|_F$.
The LAMB optimizer \citep{you2019large} incorporates the update into an Adam-like structure.
\citet{zhao2020stochastic} considers averaging the normalized gradients rather than the gradients prior to normalization.
The update can be written in terms of an $\lmo$, with the (flattened) norm choice $\|x\|_2$, which we generalize with a new algorithm in \Cref{subsec:almond} to arbitrary norms.
\paragraph{Continuous greedy}
With zero initialization, $x_1=0$, and stepsize $\gamma_k=\gamma=1/n$, \ref{eq:uSCG} recovers the stochastic continuous greedy method \citep{mokhtari2020stochastic,vondrak2008optimal}, which can be used to solve DR-submodular maximization problems under Matroid polytope constraints.

\paragraph{LMO for deep learning}
\ref{eq:SCG} for training neural networks has been suggested in \citet{pokutta2020deep} and \citet{lu2022learning}, where optimization was specifically constrained to the $K$-sparse polytope with increasing batch-size for handling stochasticity. Beyond these works, we provide convergence guarantees for \ref{eq:SCG} with constant batch-sizes and,  introduce a principled framework for selecting effective constraints based on the input and output space geometries of the layers of the network.

The perturbation in the sharpness-aware minimization (SAM) has been interpreted as an $\lmo$ and generalized to arbitrary norms \citep{pethicknusam}, focusing on the max-norm over nuclear norms, $\max_\ell \|W_\ell\|_{\mathcal{S}_1}$.

\paragraph{Trust-region}
The \ref{eq:SCG} method can be seen as a trust-region method with a linear surrogate. 
Usually, the surrogate is taken to be quadratic (\textit{cf.} \citet[Ch. 4]{wright2006numerical}).
We refer to \citet{conn2000trust} for an extensive overview of trust-region methods.
