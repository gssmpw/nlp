We are interested in solving the following general (possibly nonconvex) optimization problem
\begin{equation}\label{eq:min}
\min_{x \in \mathcal X} f(x)
\end{equation}
where $f$ is smooth in some not necessarily Euclidean norm and the problem is either unconstrained (e.g., $\mathcal X = \R^d$) or constrained to $\mathcal X = \mathcal D$ where $\mathcal D$ is the norm-ball defined as
\begin{equation*}
\mathcal D := \{ x \mid \|x\| \leq \rho \}.
\end{equation*} 

The central primitive in the algorithms considered in this work is the linear minimization oracle ($\lmo$) defined as
\begin{equation}\label{eq:lmo}
\lmo(s) \in \argmin_{x \in \mathcal D} \braket{s,x},
\end{equation}
where we are particularly interested in the special case where the constraint set is a norm constraint $\|x\|\le \rho$, 
for some $\rho > 0$ and some norm $\|\cdot\|$, which does not have to be the Euclidean norm.
Examples of norm-constrained $\lmo$s are provided in \Cref{tbl:lmo} and \Cref{tbl:operatornorms} regarding operator norms.
An important property of the $\lmo$ is that the operator is scale invariant, i.e., $\lmo(a\cdot s)=\lmo(s)$ for $a>0$, and in fact we have by construction under the norm constraints that $\|\lmo(s)\|\leq\rho$.
Thus, it is only the direction of the input $s$ that matters and not the magnitude.

A classical method for solving the constrained variant of problem~\ref{eq:min}, when the $\lmo$ is available, is the Conditional Gradient method (CG) \citep{frank1956algorithm,ken-fw,jaggi2013revisiting}, which proceeds as follows with $\gamma_k\in(0,1)$
\begin{equation*}\label{eq:CG}
\tag{CG}
x^{k+1} = (1-\gamma_k) x^k + \gamma_k \lmo(\nabla f(x^k)),
\end{equation*}
ensuring the feasibility of $x^k$ via simplicial combination.



Usually, the \ref{eq:CG} is attractive when the constraint set is an atomic set (e.g., the $\ell_1$-norm ball) in which case each update may be efficiently stored.
Our focus  lies in the more unconventional cases of the vector $\ell_\infty$-norm ball and spectral norm ball for which the updates are in contrast dense.
Furthermore, we are interested in the unconstrained case in addition to the constrained problem which \ref{eq:CG} solves.

In the stochastic regime, the analyzing $\lmo$-based algorithms is involved. 
Even when the stochastic oracle $\nabla f(x, \xi)$ is unbiased, the direction of the updates, as defined by $\lmo(\nabla f(x, \xi))$, is not unbiased.
To help overcome this difficulty, we will employ a commonly used trick of averaging past gradients with $\alpha_k \in (0,1]$ (aka momentum),
\begin{equation}
d^k = (1-\alpha_k)d^{k-1} + \alpha_k \nabla f(x^k, \xi_k),
\end{equation}
which will rigorously help with algorithmic convergence. 
