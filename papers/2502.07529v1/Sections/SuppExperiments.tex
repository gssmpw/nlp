\subsection{Additional experiments}


\paragraph{MLP}
We consider a 3-layer MLP with ReLU activations to demonstrate the various output layers in \Cref{tbl:parameter:lmo}.
We consider the configuration (Spectral $\rightarrow$ Spectral $\rightarrow$ X) where X is the output layer.
Hyperparameters are provided in \Cref{tbl:hyperparams:MLP}.
We observe in \Cref{fig:MLP:last-layer:transfer} that the optimal learning rate transfers across model width for all output layer configurations.

\paragraph{Shallow GPT}
We consider a 3-layer GPT model \citep{karpathy2023nanogpt} with the same modernizations as for the deep GPT in \Cref{sec:experiments}.
We additionally remove the weight sharing between the first and last layer so that the various input layers from \Cref{tbl:parameter:lmo:1hot} can be investigated.
We consider \Scion and \uScion with the configuration (X $\rightarrow$ Spectral $\rightarrow$ Sign) where X sweeps over the possible input layer $\lmo$s.
We additionally consider the variant of \uScion using the configuration (Sign $\rightarrow$ Sign $\rightarrow$ Sign), which is useful for distributed settings.
The hyperparameters can be found in \Cref{tbl:hyperparams:ShallowGPT}.
We observe in \Cref{fig:GPT:shakespeare} that all configurations exhibit transferability of the optimal stepsize across layer width.


\subsection{Implementation details}\label{app:impl}

It is possible to implement \ref{eq:SCG} and \ref{eq:uSCG}, while only storing on set of parameter and one set of gradients (possibly stored in half-precision).
For concreteness, we focus on \ref{eq:SCG}, but the reasoning applies to \ref{eq:uSCG} as well.
Due to the scale invariance of the $\lmo$, \ref{eq:uSCG} can be equivalently written as
\begin{equation*}
\begin{split}
G^{k} &= (1-\alpha)G^{k-1} + \nabla f(x^k,\xi^k) \\
x^{k+1} &= x^k + \gamma_k \lmo(G^{k})
\end{split}
\end{equation*}
By rearranging the update, it suffice to maintain only two states:
\begin{equation*}
\begin{split}
G &\leftarrow G + \nabla f(x,\xi) \quad \text{(backpropagation)}\\
x &\leftarrow x + \gamma \lmo(G) \\
G &\leftarrow (1-\alpha)G
\end{split}
\end{equation*}
Implementation wise this approach relies on storing the averaged gradient at the memory location where backpropagation is accumulating the gradient.
Thus, it is important not to zero out the gradient at any point during training.
We provide a reference implementation in PyTorch referred to as \texttt{ScionLight}.

\subsection{Hyperparameters}
For all hyperparameter configuration (\Cref{tbl:hyperparams:nanoGPT,tbl:hyperparams:ShallowGPT,tbl:hyperparams:MLP,tbl:hyperparams:airbench}) 
we first tune the radius parameters on a small proxy model, similar to the input and output scaling factor in $\mu$P \citep{yang2021tensor}.
The parameters can be tuned with a suboptimal stepsize $\gamma$.
The radius $\rho_2$ refers to the radius scaling of the intermediary layers, while $\rho_3$ refers to the radius scaling of the last layer.

All experiments report the loss computed at the last iterate.
A linear decay stepsize schedule is employed, which is theoretically motivated by the last iterate guarantee provided in \citet{zamani2023exact}.
Alternatively, one could consider maintaining a weighted average of past iterations, but this would introduce a memory overhead.

For NanoGPT we specifically build on the version snapshot at:\\ \url{https://github.com/KellerJordan/modded-nanogpt/blob/master/records/101724_DistributedMuon/22d24867-eb5a-4fcc-ae2c-263d0277dfd1.txt}.


\begin{figure*}[!h]
\centering
\includegraphics[width=0.5\textwidth]{figs/GPT_val_loss_3B.pdf}
\caption{Validation loss curve for NanoGPT 3B.}
\label{fig:NanoGPT:3B:loss-curve}
\end{figure*}


\begin{figure*}[!h]
\centering
\includegraphics[width=1.0\textwidth]{figs/GPT_shakespeare_transfer_1.pdf}\\
\includegraphics[width=0.75\textwidth]{figs/GPT_shakespeare_transfer_0.pdf}%
\caption{Hyperparameter transfer on a 3-layer GPT for all three input layer norms.}
\label{fig:GPT:shakespeare}
\end{figure*}


\begin{figure*}[!h]
\centering
\includegraphics[width=0.329\textwidth]{figs/FW_airbench_sweep_methodSCG_epoch8_matrix.pdf}%
\includegraphics[width=0.329\textwidth]{figs/FW_airbench_sweep_methodSCG_epoch16_matrix.pdf}%
\includegraphics[width=0.329\textwidth]{figs/FW_airbench_sweep_methodSCG_epoch32_matrix.pdf}
\includegraphics[width=0.32\textwidth]{figs/FW_airbench_sweep_methodSFW_epoch8_matrix.pdf}
\includegraphics[width=0.32\textwidth]{figs/FW_airbench_sweep_methodSFW_epoch16_matrix.pdf}
\includegraphics[width=0.32\textwidth]{figs/FW_airbench_sweep_methodSFW_epoch32_matrix.pdf}

\caption{The optimal hyperparameters for ({\sc Unconstrained}) \Scion on the airbench setting with increasing total number of epochs (indicated in red).
\Scion outperforms \uScion, which is not surprising since norm control is important in the setting.}
\label{fig:GSFW:hyperparam_sweep}
\end{figure*}


\begin{figure}[t]
\centering
\includegraphics[width=0.3\textwidth]{figs/FW_airbench_epoch_sweep_SFW.pdf}%
\includegraphics[width=0.3\textwidth]{figs/FW_airbench_epoch_sweep_SCG.pdf}
\caption{Spectral norm of weight matrices on CIFAR10, while sweeping over total number of epochs. 
The spectral norm grows empirically as $\sqrt{n}$ for \uScion with a fixed stepsize $\gamma$, whereas the norm (provably) stays bounded for \Scion.
Due to the spectral norm control, one can expect the weight decay in \Scion to be particularly useful for numerical stability in long runs in low-precision.}
\label{fig:epoch_sweep}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.3\textwidth]{figs/FW_gpt_spectral_norm_SFW.pdf}
\includegraphics[width=0.3\textwidth]{figs/FW_gpt_spectral_norm_SCG.pdf}%
\caption{Spectral norm of weight matrices on NanoGPT (124M) throughout one training run. Recall that the linear stepsize decay starts at iteration 3650.}
\label{fig:nanoGPT:spectral-norm}
\end{figure}


\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{figs/GPT_shakespeare_transfer_2.pdf}%
\caption{Hyperparameter transfer on a 3-layer GPT using appropriately rescaled sign.}
\label{fig:GPT:shakespeare:sign}
\end{figure}

\begin{figure*}[!h]
\centering
\includegraphics[width=0.32\textwidth]{figs/FW-last-layer_MLP_bs-2048_lastlayer-linear-v2_v2.pdf}%
\includegraphics[width=0.32\textwidth]{figs/FW-last-layer_MLP_bs-2048_lastlayer-rownorm-v2_v2.pdf}
\includegraphics[width=0.32\textwidth]{figs/FW-last-layer_MLP_bs-2048_lastlayer-linf-v2_v2.pdf}%
\caption{Hyperparameter transfer for all three last layer choices on MLP.}
\label{fig:MLP:last-layer:transfer}
\end{figure*}



\begin{table}[!h]
    \centering
\caption{NanoGPT hyperparameters.}
\label{tbl:hyperparams:nanoGPT}
    \begin{tabular}{|c|c|c|c|c|c|} 
        \hline 
        Hyperparameter & AdamW & \ref{eq:Muon} & \uScion & \Scion \\
        \hline \hline 
        Layers &  \multicolumn{4}{c|}{12}   \\
        Head dim &  \multicolumn{4}{c|}{128}   \\
        Vocabulary size &  \multicolumn{4}{c|}{50304}   \\
        Dataset &  \multicolumn{4}{c|}{FineWeb}   \\
        batch size &  \multicolumn{4}{c|}{512}   \\
        block size &  \multicolumn{4}{c|}{1024}   \\
        Iterations $n$ & \multicolumn{4}{c|}{5100} \\
        Warmdown & \multicolumn{4}{c|}{28.5\%} \\
        Stepsize schedule & \multicolumn{4}{c|}{Constant then linear decay $\gamma_k = \begin{cases} \gamma & \text{if } k < n-m \\ \gamma \cdot (\frac{n - k}{m}) & \text{if } k \geq n-m \end{cases}$} \\
        \hline 
        Warmup & 5\% & \multicolumn{3}{c|}{0} \\
        Gradient clipping & Yes & \multicolumn{3}{c|}{No} \\
        Momentum $\beta_1$ / $\beta_2$ & 0.9 / 0.95 & \multicolumn{3}{c|}{-} \\
        Averaging parameter $\alpha$ & - & \multicolumn{3}{c|}{0.1} \\
        \hline 
        Muon stepsize multiplier{\color{blue}$^1$} & - & 0.1 & \multicolumn{2}{c|}{-} \\
        Nesterov & - & Yes & \multicolumn{2}{c|}{-} \\
        \hline 
        Boundary init. & \multicolumn{2}{c|}{-} & \multicolumn{2}{c|}{No} \\
        Radius{\color{blue}$^2$} $\rho_2$ / $\rho_3$ & \multicolumn{2}{c|}{-} & \multicolumn{2}{c|}{50 / 3000} \\
        \hline
    \end{tabular}
    
    {\color{blue}$^1$} Muon uses Adam for the first and last layer. 
      The stepsize for the intermediary layers is multiplied by a constant. \\
    {\color{blue}$^2$} For weight sharing the last layer radius is scaled by scaling the logits.
\end{table}


\begin{table}[!h]
\centering
\caption{Shallow GPT hyperparameters. We set the batch size to 32, which is the maximum allowed for a model with an embedding size of 4096 on an A100.}
\label{tbl:hyperparams:ShallowGPT}
    \begin{tabular}{|c|c|c|c|}
        \hline 
        Hyperparameter & AdamW & \uScion & \Scion \\
        \hline \hline 
        Layers &  \multicolumn{3}{c|}{3}   \\
        Head dim &  \multicolumn{3}{c|}{64}   \\
        Vocabulary size &  \multicolumn{3}{c|}{64}   \\
        Dataset &  \multicolumn{3}{c|}{Shakespeare}   \\
        batch size &  \multicolumn{3}{c|}{32}   \\
        block size &  \multicolumn{3}{c|}{1024}   \\
        Iterations $n$ & \multicolumn{3}{c|}{122} \\
        Stepsize schedule & \multicolumn{3}{c|}{Linear decay $\gamma_k = \gamma \cdot (1-k/n)$} \\
        \hline 
        Gradient clipping & Yes & \multicolumn{2}{c|}{No} \\
        $\beta_1$ / $\beta_2$ & 0.9 / 0.95 & \multicolumn{2}{c|}{-} \\
        Averaging parameter $\alpha$ & - & \multicolumn{2}{c|}{0.1} \\
        Boundary init. & \multicolumn{1}{c|}{-} & \multicolumn{2}{c|}{Yes} \\
        Radius $\rho_2$ / $\rho_3$ & \multicolumn{1}{c|}{-} & \multicolumn{2}{c|}{3 / 10} \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[!h]
\centering
\caption{Shallow MLP hyperparameters.}
\label{tbl:hyperparams:MLP}
    \begin{tabular}{|c|c|}
        \hline 
        Hyperparameter & \Scion \\
        \hline \hline 
        Layers &  \multicolumn{1}{c|}{3}   \\
        Dataset &  \multicolumn{1}{c|}{CIFAR10 (50000 training examples)}  \\
        batch size &  \multicolumn{1}{c|}{2048}   \\
        Epochs & \multicolumn{1}{c|}{20} \\
        Stepsize schedule & \multicolumn{1}{c|}{Linear decay $\gamma_k = \gamma \cdot (1-k/n)$} \\
        \hline 
        Averaging parameter $\alpha$ & \multicolumn{1}{c|}{0.1} \\
        Boundary init. & \multicolumn{1}{c|}{Yes} \\
        Radius $\rho_2$ / $\rho_3$ & \multicolumn{1}{c|}{1 / 1024} \\
        \hline
    \end{tabular}
\end{table}


\begin{table}[h!]
\centering
\caption{Hyperparameters for the CNN experiments building on the airbench codebase \citep{airbench_2024}.}
\label{tbl:hyperparams:airbench}
    \begin{tabular}{|c|c|c|}
        \hline 
        Hyperparameter & \uScion & \Scion \\
        \hline \hline 
        Block size (block 1, block 2, block 3) & \multicolumn{2}{c|}{width factor $\times$ (64, 256, 256)} \\
        Dataset &  \multicolumn{2}{c|}{CIFAR10 (50000 training examples)}  \\
        batch size &  \multicolumn{2}{c|}{2000}   \\
        Epochs & \multicolumn{2}{c|}{8} \\
        Stepsize schedule & \multicolumn{2}{c|}{Linear decay $\gamma_k = \gamma \cdot (1-k/n)$} \\
        \hline 
        Averaging parameter $\alpha$ & \multicolumn{2}{c|}{0.5} \\
        Boundary init. & \multicolumn{2}{c|}{Yes} \\
        \hline 
        Radius $\rho_2$ / $\rho_3$ & \multicolumn{1}{c|}{1 / 100} & \multicolumn{1}{c|}{1 / 20} \\
        \hline
    \end{tabular}
\end{table}



