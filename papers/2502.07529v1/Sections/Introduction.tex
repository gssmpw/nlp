
\begin{table*}[t]
\caption{Special instantiations of \ref{eq:uSCG} according to different choices of norm. 
The reduced SVD is given as $s=U\diag(\sigma) V^\top$.
Weight decay is captured by \ref{eq:SCG}, which provides explicit control on the norm of the parameters.
}
\label{tbl:lmo}
\bgroup
\def\arraystretch{1.2}
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Method & $\alpha_k$ & Problem & $\lmo$ constraint set $\mathcal D$ & $\lmo$ & Reference \\
\hline
\hline
Normalized SGD & $1$ & Unconstrained  & Euclidean $\|\cdot\|_2$-ball & $-\rho \tfrac{s}{\|s\|_2}$ & \citep{hazan2015beyond} \\
Momentum Normalized SGD & $[0,1]$ & Unconstrained  & Euclidean $\|\cdot\|_2$-ball & $-\rho \tfrac{s}{\|s\|_2}$ & \citep{cutkosky2020momentum}\\
\hline
SignSGD & $1$ & Unconstrained  & Max-norm $\|\cdot\|_\infty$-ball & $-\rho \sign(s)$ & \citep[Thm. 1]{bernstein2018signsgd}$\blue{^2}$ \\
Signum & $[0,1]$ & Unconstrained  & Max-norm $\|\cdot\|_\infty$-ball & $-\rho \sign(s)$ & \citep[Thm. 3]{bernstein2018signsgd}$\blue{^2}$ \\
\hline
\ref{eq:Muon}$\blue{^1}$ & $[0,1]$ & Unconstrained & Spectral $\|\cdot\|_{\mathcal{S}_\infty}$-ball & $-\rho UV^\top$ & \citep{jordan2024muon} \\
\hline
\end{tabular}
}
\egroup
\footnotesize 
$\blue{^1}$ With non-Nesterov based momentum. 
$\blue{^2}$ The theoretical guarantee relies on increasing batch size. \\
\end{table*}




Deep learning has greatly benefited from adaptive optimization methods such as RMSProp \citep{hinton2012neural}, AdaGrad \citep{JMLR:v12:duchi11a,mcmahan2010}, and Adam \citep{kingma2014adam}, which dynamically change the geometry of the problem based on gradients encountered on-the-fly during training. 
While these methods have demonstrated remarkable success, they fundamentally treat neural networks (NNs) as optimization problems where we lack any prior knowledge about their particular setting. 

\looseness=-1However, NNs are far from being
 black boxes—their structure is not only known but they are deliberately designed. 
This simple observation raises directly the question: 

\begin{center}
\emph{Is it more beneficial to adapt the optimizer a priori, instead of exploring their respective geometries on-the-fly?}
\end{center}
 
 
 
 














Adaptation on-the-fly has been the defacto standard in this setting, with adaptive algorithms, such as Adam \cite{kingma2014adam}, dominating the deep learning model training. 

One possible way for adaptation a priori, which we focus on in this work, is to modify the underlying norm used to measure distances in the parameter space. There is precedence to our proposal, as the 
early work by \citet{carlson2015stochastic,carlson2015stochasticb,carlson2015preconditioned} 
introduced the stochastic spectral descent method (SSD), which performs steepest descent in the spectral norm, and demonstrated that the method can substantially accelerate deep learning training. 


The significance of the SSD approach has been very recently brought back to attention by \citet{bernstein2024old}, who showed that the Shampoo optimizer \citep{gupta2017unified}—winner of the external tuning track at the 2024 AlgoPerf: Training Algorithms competition \citep{dahl2023benchmarking}—can be viewed as SSD when a certain accumulation is disabled. 
Moreover, \citet{bernstein2024old} introduced an efficient Newton-Schultz iteration to replace the approximate SVD calculations previously required. 
\Citet{jordan2024muon} incorporated the Newton-Schultz iteration with additional momentum into SSD under the name Muon to achieve impressive results on the nanoGPT architecture by applying it to the hidden layers.

\vspace{-3mm}
\paragraph{Contributions}
This work focuses on developing an algorithmic framework that can exploit an appropriate choice of norm for the entire neural network with particular emphasis on hyperparameter transfer across model sizes \citep{yang2021tensor}, convergence and practical performance.

To adapt to the geometry a priori, we will build on a classical (but unexpected) family of algorithms in contrast to the steepest descent methods, namely the ones involving the linear minimization oracle ($\lmo$) over a norm-ball constraint known as the Conditional Gradient (CG) methods.

\looseness=-1 While classically being used for constrained problems, we take the slightly unusual approach by showing that the $\lmo$s can be used even for unconstrained problems.
The algorithm, dubbed as the unconstrained Stochastic Conditional Gradient method (\ref{eq:uSCG}), shows improvements both theoretically and practically when the norm-ball constraint matches the natural geometry of the problem.%

In particular, we build on the Stochastic Conditional Gradient (SCG) method of \citet{mokhtari2020stochastic} from the constrained setting, which provides explicit control on the norm of NN weight matrices. This is particularly relevant for robust image classification \citep{cisse2017parseval}, generalization bounds \citep{GenBound17}, Lipschitz control of generative adversarial networks \citep{arjovsky2017wasserstein,miyato2018spectral}, diffusion models \citep[Sec. 2.3]{karras2024analyzing}, and for ensuring Lipschitz continuity of NNs \citep{large2024scalable}.



Concretely we make the following contributions:

\emph{Theoretical rates:} 
    We introduce a new, stochastic $\lmo$ based family of algorithms \ref{eq:uSCG}, which can exploit the specific geometry of the problem.
    In doing so we achieve the $O(n^{-1/4})$ order optimal convergence rate under general nonconvexity and stochasticity for \ref{eq:uSCG} \citep{arjevani2022lowerboundsnonconvexstochastic}.
    Moreover, we provide a new analogous guarantee for the constrained case for \ref{eq:SCG}.
    
    \emph{Unification}: 
    Our $\lmo$-based approach provides a unifying framework for various popular algorithms, based on the norm choice (see \Cref{tbl:lmo}); as a byproduct we establish the first provable rate for the Muon optimizer. 
    More importantly, this generality allows us to design a new method for deep learning based on operator norms called \Scion, which enjoys zero-shot hyperparameter transferability \citep{yang2022tensor},
    and {can be implemented storing only one set of parameters and one gradient (stored in half-precision), economizing on memory in large-scale training}.

    
    \emph{Numerical validation}:
     We carry out exhaustive numerical evaluation of \Scion ranging from small scale experiments on MLPs and CNNs to NanoGPT models with up to 3B parameters.
     We consistently observe the transferability properties across all settings for \Scion.
     The scheme is more tolerant to large batch sizes and exhibits superior performance due to the a priori adaptation. 



An additional $\lmo$-based algorithm ALMOND can be found in \Cref{subsec:almond}, generalizing the Normalized SGD based method of \citet{zhao2020stochastic}, for training with large-batches. 
Key differences of ALMOND with \ref{eq:uSCG} and \ref{eq:SCG} are discussed to further motivate our algorithms.







