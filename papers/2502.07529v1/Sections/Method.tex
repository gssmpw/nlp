For the unconstrained case we introduce a new method, dubbed the unconstrained SCG method (uSCG): 
\begin{equation*}\label{eq:uSCG}
\tag{uSCG}
\begin{split}
x^{k+1} &= x^k + \gamma_k \lmo(d^k)
\end{split}
\end{equation*}
with stepsizes $\gamma_k \in (0,1)$. 
Instead of the convex combination in \ref{eq:CG}, the update rule simply sums the $\lmo$s.
In contrast with e.g., gradient descent, the update  always has the same magnitude regardless of the size of the gradient average $d^k$.
The final algorithm is presented in \Cref{alg:uSCG}.


\begin{algorithm}[t]
\caption{Unconstrained SCG (uSCG)}
\label{alg:uSCG}
\textbf{Input:} Horizon $n$, initialization $x^1 \in \mathcal X$, $d^0 = 0$, momentum $\alpha_k \in (0,1]$, and stepsize $\gamma_k \in (0,1)$
\begin{algorithmic}[1]
    \For{$k = 1, \dots, n$}
        \State Sample $\xi_{k}\sim \mathcal P$
        \State $d^{k} \gets \alpha_{k} \nabla f(x^{k}, \xi_{k}) + (1 - \alpha_{k})d^{k-1}$
        \State $x^{k+1} \gets x^k + \gamma_k \lmo(d^k)$
    \EndFor
    \State Choose $\bar{x}^n$ uniformly at random from $\{x^1, \dots, x^n\}$
    \item[\algfont{Return}] $\bar{x}^n$
\end{algorithmic}
\end{algorithm}

For the constrained case, we revisit the SCG method of \citet{mokhtari2020stochastic} and adopt it for the non-convex objectives typically encountered in deep learning model training.
This algorithm  (\Cref{alg:SCG}) proceeds as follows
\begin{equation}\label{eq:SCG}
\tag{SCG}
\begin{split}
x^{k+1} &= (1-\gamma_k) x^k + \gamma_k \lmo(d^k)
\end{split}
\end{equation}
with stepsizes $\gamma_k \in (0,1)$. %


\begin{algorithm}
\caption{Stochastic Conditional Gradient (SCG)}
\label{alg:SCG}
\textbf{Input:} Horizon $n$, initialization $x^1 \in \mathcal D$, $d^0 = 0$, momentum $\alpha_k\in (0,1]$, and stepsize $\gamma_k \in (0,1)$
\begin{algorithmic}[1]
    \For{$k = 1, \dots, n$}
        \State Sample $\xi_{k}\sim \mathcal P$
        \State $d^{k} \gets \alpha_{k} \nabla f(x^{k}, \xi_{k}) + (1 - \alpha_{k})d^{k-1}$
        \State $x^{k+1} \gets (1-\gamma_k) x^k + \gamma_k \lmo(d^k)$
    \EndFor
    \State Choose $\bar{x}^n$ uniformly at random from $\{x^1, \dots, x^n\}$
    \item[\algfont{Return}] $\bar{x}^n$
\end{algorithmic}
\end{algorithm}

\paragraph{Connection to weight decay:}
For \ref{eq:uSCG}, weight decay has a very precise interpretation,
 since the method reduces to \ref{eq:SCG}.
Consider the following variant of \ref{eq:uSCG} with weight decay $
x^{k+1} = x^k + \gamma_k\lmo(d^k) - \gamma_k \mu x^k.$

The weight decay parameter $\mu\in [0,1]$ interpolates between \ref{eq:uSCG} and \ref{eq:SCG}.
If the weight decay is in $(0,1)$ then the algorithm is still an instance of \ref{eq:SCG} and thus solve a constrained problem, but one with a larger radius of $\rho' = \tfrac{\rho}{\mu}$ with a stepsize chosen as $\gamma_k'=\gamma_k \mu$.

Therefore, all schemes in \Cref{tbl:lmo} guarantees a norm bound of $\tfrac{\rho}{\mu}$ on the parameters when combined with weight decay.
The connection between weight decay and constrained optimization, in the special case where $\lmo=\sign$ (when the norm-constraint in \eqref{eq:lmo} is the vector $\ell_\infty$-norm) has also been observed in \citet{xie2024implicit,d2023we}.
Due to the fixed magnitude of the $\lmo$ both methods provides a guarantee on the maximum norm of the parameters.

\begin{insightbox}[label={insight:weight-decay}]
Both \ref{eq:uSCG} and \ref{eq:SCG} provide explicit control on the norm of the parameters:
\begin{enumerate}[label=(\roman*)]
  \item \ref{eq:SCG} guarantees $\|x\| \leq \rho$.
  \item \ref{eq:uSCG} guarantees $\|x\| \leq \rho\sum_{k=1}^{n}\gamma_k$.
\end{enumerate}
\end{insightbox}

\subsection{Choice of Norm Constraint}\label{sec:normchoice}

To choose an appropriate norm for deep learning, we build on the operator norm perspective of \citet{large2024scalable}. 
To simplify the presentation we will reason through a linear MLP as a running example, but in \Cref{sec:transfer}, we point to our theoretical guarantees with activation functions.

\begin{table*}
\centering
\caption{Example operator norms and the associated $\lmo$s of a matrix $A \in \R^{d_\mathrm{out} \times d_\mathrm{in}}$. The reduced SVD is given as $A=U\diag(\sigma) V^\top$, $\sign$ acts elementwise, $\operatorname{col}_i(A):=A_{i,\cdot}$ and $\operatorname{row}_j(A):=A_{\cdot,j}$. Note that this table is not exhaustive. }
\label{tbl:operatornorms}
\bgroup
\def\arraystretch{1.2}
\begin{tabular}{|c|c|c|c|c|}
\hline
& $1 \rightarrow \RMS$ (ColNorm) & $1 \rightarrow \infty$ (Sign) & $\RMS \rightarrow \RMS$ (Spectral) & $\RMS \rightarrow \infty$ (RowNorm) \\
\hline\hline
\textbf{Norm} & $\max_i \tfrac{1}{\sqrt{d_\mathrm{out}}}\|\operatorname{col}_i(A)\|_2$ & $\max_{i,j} |A_{i,j}|$ & $\sqrt{\nicefrac{d_\mathrm{in}}{d_\mathrm{out}}}\|A\|_{\mathcal{S}_{\infty}}$ & $\max_j \sqrt{d_\mathrm{in}}\|\operatorname{row}_j(A)\|_2$ \\
\hline
\textbf{LMO} & $\operatorname{col}_i(A)\mapsto \sqrt{d_\mathrm{out}}\tfrac{\operatorname{col}_i(A)}{\|\operatorname{col}_i(A)\|_2}$ & $A\mapsto \sign(A)$ & $A\mapsto\sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}}UV^\top$ & $\operatorname{row}_j(A)\mapsto \tfrac{1}{\sqrt{d_\mathrm{in}}}\tfrac{\operatorname{row}_j(A)}{\|\operatorname{row}_j(A)\|_2}$ \\
\hline
\end{tabular}
\egroup
\end{table*}


\begin{table*}
\centering
\caption{The choice of $\lmo$ can be different between layers and can depend on the assumptions on the input. For simplicity we overload notation and write the reduced SVD as $W_\ell = U\diag(\sigma)V^\top \in \R^{d_\mathrm{out} \times d_\mathrm{in}}$ for all $\ell \in [L]$. %
}
\label{tbl:parameter:lmo}
\bgroup
\def\arraystretch{1.2}
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Parameter}} & \multicolumn{1}{c|}{$W_1$ (image domain)} & \multicolumn{1}{c|}{$\{W_\ell\}_{\ell \in [2,...,L-1]}$} & \multicolumn{3}{c|}{$W_L$} & \multicolumn{1}{c|}{$b_\ell$} \\
\hline
\textbf{Norm} & $\RMS \rightarrow \RMS$ & $\RMS \rightarrow \RMS$ & $\RMS \rightarrow \RMS$ & $\RMS \rightarrow \infty$ & $1 \rightarrow \infty$ & $\RMS$ \\
\hline
\hline
\textbf{LMO} 
  & $\max(1,\sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}}) UV^\top$
  & $\sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}} UV^\top$
  & $\sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}} UV^\top$
  & $\operatorname{row}_j(W_L)\mapsto \tfrac{1}{\sqrt{d_\mathrm{in}}}\tfrac{\operatorname{row}_j(W_L)}{\|\operatorname{row}_j(W_L)\|_2}$
  & $\tfrac{1}{d_\mathrm{in}} \sign(W_L)$
  & $\tfrac{b_\ell}{\|b_\ell\|_\RMS}$ \\
\textbf{Init.} 
  & Semi-orthogonal
  & Semi-orthogonal
  & Semi-orthogonal
  & Row-wise normalized Gaussian
  & Random sign
  & 0
\\
\hline
\end{tabular}
}
\egroup
\end{table*}

\begin{table}
\centering
\caption{Example $\lmo$ choices for 1-hot encoded inputs.
}
\label{tbl:parameter:lmo:1hot}
\bgroup
\def\arraystretch{1.2}
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Parameter}} & \multicolumn{3}{c|}{$W_1$ (1-hot encoded input)} \\
\hline
\textbf{Norm} & $2 \rightarrow \RMS$ & $1 \rightarrow \RMS$ & $1 \rightarrow \infty$ \\
\hline
\hline
\textbf{LMO} 
  & $\sqrt{d_\mathrm{out}} UV^\top$
  & $\operatorname{col}_i(W_1)\mapsto \sqrt{d_\mathrm{out}}\tfrac{\operatorname{col}_i(W_1)}{\|\operatorname{col}_i(W_1)\|_2}$
  & $\sign(W_L)$ \\
\textbf{Init.} 
  & Semi-orthogonal
  & Column-wise normalized Gaussian
  & Random sign
\\
\hline
\end{tabular}
}
\egroup
\end{table}



Let us a consider a linear MLP with the initial hidden layer defined as $h_1(z) = W_1z + b_1$ and the remaining layers
\begin{equation*}
h_\ell(z) = W_\ell h_{\ell-1}(z) + b_\ell, \quad \forall \ell \in 2,..,L;
\end{equation*}
with $b_L=0$.
We denote the global loss as $\mathcal L(h_L(z),y)$ where $\mathcal L$ is the loss function and $y$ is a 1-hot encoded target vector.
We use the overloaded notation $W_\ell \in \R^{d_\mathrm{out} \times d_\mathrm{in}}$, where $d_\mathrm{out}$ and $d_\mathrm{in}$ implicitly have dependency on $\ell$ and can thus be distinct across different layers.

We need that none of the intermediary hidden states $h_\ell(z)$ blows up by requiring one of the following norm bounds:
\begin{enumerate}[label=(\roman*)]
    \item $\tfrac{1}{d_\mathrm{out}}\|h_\ell(z)\|_1 \leq 1$ \hfill (the average entry is bounded)
    \item $\|h_\ell(z)\|_\RMS \leq 1$ \hfill (the typical entry is bounded)
    \item $\|h_\ell(z)\|_\infty\leq 1$ \hfill (the maximum entry is bounded)
\end{enumerate}
where $\|z\|_\RMS:= \tfrac{1}{\sqrt{d}}\|z\|_2$ for $z\in \R^d$.
Assuming the input to any given layer is bounded in some norm $\|\cdot\|_\alpha$, this requirement corresponds to placing an operator norm constraint on the weight matrices $\{W_\ell\}_{\ell\in [L]}$ and a norm constraint on the biases $\{b_\ell\}_{\ell\in [L-1]}$.

The operator norm is in turn defined as follows 
\begin{equation}\label{eq:opnorm}
\|A\|_{\alpha \rightarrow \beta}
:=\max_{z \in \mathbb{R}^d, z \neq 0} \frac{\|Az\|_\beta}{\|z\|_\alpha}
= \sup_{\|z\|_\alpha = 1} \|Az\|_\beta.
\end{equation}
Directly from the definition, we have that if the input $z$ is bounded through $\|z\|_\alpha\leq 1$, then the output $\|Az\|_\beta$ will be bounded when $\|A\|_{\alpha \rightarrow \beta}$ is bounded.

A collection of operator norms and their resulting $\lmo$s is provided in \Cref{tbl:operatornorms}.
It will be convenient to convert between bounds on these different operator norm which the following fact makes precise.
\begin{fact}\label{lem:operator:bound}
The operator norm satisfies for some $\rho > 0$
\begin{lemnum}
\item $\|z\|_\beta \leq \rho \|z\|_c, \ \forall z\in \R^d$  $\Rightarrow$ $\|A\|_{\alpha\rightarrow \beta} \leq \rho \|A\|_{\alpha\rightarrow c}$.
\item $\|z\|_\alpha \geq \tfrac{1}{\rho} \|z\|_c,\ \forall z\in \R^d$ $\Rightarrow$ $\|A\|_{\alpha\rightarrow \beta} \leq \rho \|A\|_{c\rightarrow \beta}$.
\end{lemnum}
\end{fact}
\Cref{lem:operator:bound} tells us that we can bound operator norms using bounds on the vector norms, i.e.,
\begin{equation}\label{eq:vectornorm:bounds}
\|z\|_\infty \leq \|z\|_2 \leq \|z\|_1 \leq \sqrt{d} \|z\|_2 \leq d \|z\|_\infty, 
\quad \forall z\in \R^d.
\end{equation}

We start by focusing on controlling the RMS norm, but will later consider other norms.
There are three example operator norms to consider for the MLP in consideration:
\begin{enumerate}[label=(\roman*)]
  \item Initial layer $h_1(z)$: $\|W_1\|_{\alpha_1 \rightarrow \RMS} \leq 1$.
  \item Intermediary layers $h_\ell(z)$: $\|W_\ell\|_{\RMS \rightarrow \RMS} \leq 1$\\$ \ \forall \ell \in \{2,..,L-1\}$.
  \item Last layer $h_L(z)$: $\|W_L\|_{\RMS \rightarrow \beta_L} \leq 1$.
\end{enumerate}
Note that the operator norm $\|\cdot\|_{\RMS \rightarrow \RMS}$ is a scaled spectral norm, i.e., $\|A\|_{\RMS \rightarrow \RMS}=\sqrt{\nicefrac{d_{\mathrm{in}}}{d_{\mathrm{out}}}}\|A\|_{2 \rightarrow 2}=\sqrt{\nicefrac{d_{\mathrm{in}}}{d_{\mathrm{out}}}}\|A\|_{\mathcal{S}_{\infty}}$ for $A\in \R^{d_\mathrm{out} \times d_\mathrm{in}}$.

To concisely write the layerwise norm constraints in terms of a norm constraint on the joint parameter $x=\{W_\ell,b_\ell\}_{\ell\in [L]}$, we can define the norm in the $\lmo$ defined in \eqref{eq:lmo} as
\begin{equation*}
\|x\| := \max_{\ell\in[L]} \max \{\|W_\ell\|_{\alpha_\ell \rightarrow \beta_\ell}, \|b_\ell\|_{\beta_\ell} \}\leq 1.
\end{equation*}
A choice needs to be made for the input norm $\alpha_1$ and output norm $\beta_L$, which depends on the application:


\paragraph{Input layer}
For image domains, usually the input is rescaled pixel-wise to e.g., ensure that $z \in [-1,1]$ in which case $\|z\|_\RMS\leq 1$ and the appropriate operator norm for the first layer becomes $\|W_1\|_{\RMS \rightarrow \RMS}=\sqrt{\nicefrac{d_\mathrm{in}}{d_\mathrm{out}}}\|W_1\|_{{\mathcal S}_\infty}$. In order to deal with the case where $d_\mathrm{in} > d_\mathrm{out}$, we choose the radius to be $\max(1,\sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}})$ (\textit{cf.}, \Cref{{app:input-radius}}).

\looseness=-1For language tasks, the input $z$ is usually a 1-hot encoded vector in which case $\|z\|_\infty=\|z\|_2=\|z\|_1=1$.
In turn, $\|W_1\|_{\infty \rightarrow \RMS}=\|W_1\|_{2 \rightarrow \RMS}=\|W_1\|_{1 \rightarrow \RMS}$ holds on this restricted domain, where we can freely pick the operator norm that leads to the simplest update rule (\Cref{tbl:operatornorms}).

The simplest form for the $\lmo$ is arguably induced by $\|\cdot\|_{1 \rightarrow \RMS}$ since the $\lmo$ can be computed exactly, while from $\|\cdot\|_{2 \rightarrow \RMS}$ we can observe a more aggressive scaling factor in the $\lmo$, $\sqrt{d_{\mathrm{out}}}UV^\top$, than the $\sqrt{\nicefrac{d_{\mathrm{out}}}{d_{\mathrm{in}}}}UV^\top$ used in  intermediate layers.
The norm choice $\|\cdot\|_{1 \rightarrow \RMS}$ was first proposed in \citet{large2024scalable} for 1-hot encoded input.
Through the above reasoning we see how the norm is equivalent to an appropriately scaled spectral norm.

\paragraph{Output layer}
For the final layer, we are not restricted to bounding the output in $\ell_\RMS$ and can alternatively choose bounding the maximal entry through $\ell_\infty$.
Additionally, we can bound $\|A\|_{\RMS \rightarrow \infty} \leq \tfrac{1}{d_\mathrm{in}}\|A\|_{1 \rightarrow \infty}$, by using \eqref{eq:vectornorm:bounds} through \Cref{lem:operator:bound}, which leads to a dimension scaled sign update rule for the last layer.


We summarize the different norm choices and their resulting $\lmo$s in \Cref{tbl:parameter:lmo,tbl:parameter:lmo:1hot}.
\Cref{tbl:parameter:lmo} provides an overview of norm choices of output layers, while \Cref{tbl:parameter:lmo:1hot} provides choices for input layers under 1-hot encoded input.

Provided that the input is bounded as described, each of the hidden states $h_\ell(z)$ and logits $h_L(z)$ will be bounded in the RMS norm.
In order to ensure feasibility of the initialization in the constrained case when employing \ref{eq:SCG},
we propose to initialize on the boundary similar to \citet{large2024scalable} (see \Cref{app:method} for details).


\begin{insightbox}[label={insight:norm-choice}]
\begin{enumerate}[label=(\roman*)]
  \item For 1-hot encoded input, ColNorm and Spectral are equivalent for the first layer, in which case ColNorm is favored since the $\lmo$ can be computed exactly.
  \item Sign can be used both for the first and last layer which is crucial for weight sharing.
  \item To transfer learning rate from proxy models when the width is smaller than the input dimension it is important to rescale the $\lmo$ as $\max(1,\sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}})$.
\end{enumerate}
\end{insightbox}
These observations leads to the recommendations below.
\begin{recommendationbox}[label={recomm:instanciation}]
We refer to the instantiation of \ref{eq:uSCG} and \ref{eq:SCG} using operator norms as \uScion and \Scion respectively,
which stands for \textbf{S}tochastic \textbf{C}onditional Grad\textbf{i}ent with \textbf{O}perator \textbf{N}orms.
We recommend the following configurations of the layer norms (First layer $\rightarrow$ Intermediary layers $\rightarrow$ Last layer):
\begin{enumerate}[label=(\roman*)]
\item image domains: \hfill Spectral $\rightarrow$ Spectral $\rightarrow$ Sign
\item 1-hot input: \hfill ColNorm $\rightarrow$ Spectral $\rightarrow$ Sign
\item weight sharing: \hfill Sign $\rightarrow$ Spectral $\rightarrow$ Sign
\end{enumerate}
The $\lmo$ names are defined in \Cref{tbl:operatornorms} and weight sharing refers to parameter sharing between the first and last layer.
Each layer should be scaled appropriately according to \Cref{tbl:parameter:lmo,tbl:parameter:lmo:1hot}. 
\end{recommendationbox}



\paragraph{Exclusively Sign}
So far our argument has been based on the invariance provided by $\|\cdot\|_{\RMS \rightarrow \RMS}$ for intermediary layers: i.e., the RMS norm of the output of layer $\ell$ is bounded, so the input of next layer $\ell+1$ is also bounded in the RMS norm. %
Alternatively, we can rely on the invariance provided by $\|\cdot\|_{\infty \rightarrow \infty}$, for which \Cref{lem:operator:bound} tells us that $\|\cdot\|_{\infty \rightarrow \infty} \leq d_\mathrm{in}\|\cdot\|_{1 \rightarrow \infty}$.
Combined with the choice of $\|\cdot\|_{1 \rightarrow \infty}$ for the input layer and output layer in \Cref{tbl:parameter:lmo,tbl:parameter:lmo:1hot}, we obtain a method exclusively relying on the sign operator as summarized in \Cref{tbl:parameter:lmo:same-norm} of \Cref{app:method}.

The sign-only update is of particular interest, since it permits efficient communication in distributed settings.
We demonstrate its hyperparameter transferability of the optimal stepsize across width in \Cref{fig:GPT:shakespeare:sign} of \Cref{app:experiments}.
\looseness=-1



\subsection{Hyperparameter Transfer}\label{sec:transfer}
The intuition behind why ({\sc Unconstrained}) \Scion may enjoy hyperparameter transfer is suggested by the spectral scaling rule of \citet{yang2023spectral}, which states that feature learning may be ensured by requiring that, for MLPs with weight matrices $W_\ell \in \R^{d_{\ell-1} \times d_\ell}$, the following holds:
\begin{equation*}
\textstyle \|W_\ell\|_{\mathcal{S}_\infty} = \Theta\left(\sqrt{\frac{d_\ell}{d_{\ell-1}}}\right)
\ \text{and} \
\|\Delta W_\ell\|_{\mathcal{S}_\infty} = \Theta\left(\sqrt{\frac{d_\ell}{d_{\ell-1}}}\right)
\end{equation*}
where $\|\cdot\|_{\mathcal{S}_\infty}$ denotes the spectral norm and $\Delta W_\ell$ is the update change.
For \ref{eq:uSCG}, the update change is given by $x^{k+1}-x^k=\gamma \lmo(d^k)$, so the requirement is automatically satisfied by the spectral norm choice from \Cref{tbl:parameter:lmo}.

We formalize this intuition in \Cref{lemma:mu_transfer} of \Cref{app:transfer:proof} following the proof technique of \citet{yang2023spectral}, which holds for losses including logistic regression and MSE, and activation functions including ReLU, GELU and Tanh.
Specifically, we show that the so-called maximal update learning rate $\gamma^*$ 
(i.e., the learning rate that enables the hidden layer preactivations to undergo the largest possible change in a single update step) is independent of width.
Thus, a learning rate tuned on a smaller model can be directly applied to a wider model without compromising the maximal update property.


\begin{toappendix}
\input{Sections/SuppTransferSetup}
\begin{lemma}[Width-invariance of the maximal update learning rate]
\label{lemma:mu_transfer}
Consider an $L$-layer MLP with widths $d_0, d_1, \dots, d_L$ (where $d_1 \ge d_0$), and assume $d_0$ is a fixed constant. 
Let its activation function $\sigma$ have Lipschitz constant $L_\sigma$ and satisfy $\sigma(0) = 0$ 
(e.g., ReLU, Tanh, or GELU). Suppose:
\begin{enumerate}[label=(\roman*)]
    \item The input data $(\mathbf{z}, \mathbf{y})$ meets the requirements in 
    \Cref{assumption:input_data},
    \item The network is initialized according to \Cref{assumption:initialization}, and
    \item Parameter updates use \Cref{alg:uSCG} with the spectral-norm-based $\lmo$ described in \Cref{assumption:lmo_choice}.
\end{enumerate}

For various loss functions $\mathcal{L}$ (e.g., MSE, logistic), define the \emph{maximal update 
condition} by
\[
  \mathbb{E}\Bigl[\bigl(\Delta f^{(\ell)}_i(\mathbf{z})\bigr)^2\Bigr] \;\simeq\; 1 
  \quad \text{for all } \ell \le L,
\]
where $\Delta f^{(\ell)}_i(\mathbf{z})$ is the change in the preactivation of the $i$-th neuron 
in the $\ell$-th hidden layer after one update step. 
Unless all activations are simultaneously zero during training (which is highly unlikely in 
practice), the optimal learning rate $\gamma^*$ satisfying this condition is 
\emph{independent of the hidden-layer widths}.

\end{lemma}
\end{toappendix}
\begin{toappendix}
\input{Sections/SuppTransferProof}
\begin{remark}
The maximal update condition ensures that the network operates in a \emph{stable but maximally adaptive regime}, balancing \emph{efficient learning and numerical stability}.
\end{remark}
\begin{remark}
As the hidden layer widths $d_1, \dots, d_{L-1}$ increase, the learning rate required to maintain the maximal update property remains unchanged, demonstrating width invariance in deep networks. Consequently, a learning rate tuned on a smaller model can be directly applied to a wider model without sacrificing training dynamics.
\end{remark}
\end{toappendix}

