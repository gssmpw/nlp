

For computing the $\lmo$ of layers using a spectral norm constraint, we use the efficient implementation provided in \citet{jordan2024muon} of the Newton-Schultz iteration proposed in \citet{bernstein2024old}.
In this section, Muon \citep{jordan2024muon} refers to the version used in practice, which uses AdamW for the first layer and last layer and Nesterov type momentum.



\paragraph{GPT}
We build on the excellent modded-nanogpt codebase \citep{modded_nanogpt_2024}, which makes the following modernizations to \citet{karpathy2023nanogpt}: rotary embeddings is used instead of positional embeddings, RMS norm is used instead of LayerNorm, and linear decay schedule instead of a cosine stepsize.
\Scion and \uScion use the (Sign $\rightarrow$ Spectral $\rightarrow$ Sign) configuration with scaling factors in accordance with \Cref{tbl:parameter:lmo,tbl:parameter:lmo:1hot}.
We train for $5100$ iterations with a batchsize of $512$ on the FineWeb dataset (see \Cref{tbl:hyperparams:nanoGPT} regarding hyperparameters).
In comparison with Adam, both Muon and ({\sc Unconstrained}) \Scion do not require learning rate warmup.
We sweep over stepsizes and model width in \Cref{fig:GPT}.

From \Cref{fig:GPT}, we observe that the optimal stepsize of \Scion and \uScion transfer across model width as oppose to Adam and Muon.
Even when the Muon optimizer is tuned on the largest model size it achieves a validation loss of 2.988 in comparison with 2.984 of \uScion.
Our methods completely remove the need for using Adam otherwise present in the Muon implementation, which permits an implementation that only requires storing one set of weights and one set of gradient (stored in half-precision) across all layers (see \Cref{app:impl}).
The experiments additionally demonstrates that our method works for weight sharing.

\paragraph{3B model}
Using the optimal configuration of the 124M parameter proxy model, we perform a large model experiment on a 3B parameter model, which also increases the depth.
Specifically, we take the embedding dimension to be 2560 and the depth to be 36.
We observe in \Cref{tbl:GPT:3B} that \uScion outperforms all other methods.
The loss curve is provided in \Cref{fig:NanoGPT:3B:loss-curve} of \Cref{app:experiments}.

\begin{table}[H]
\centering
\caption{Validation loss on a 3B parameter GPT model.}\label{tbl:GPT:3B}
    \begin{tabular}{|c|c|c|c|c|}
        \hline 
        Adam & \ref{eq:Muon} & \uScion & \Scion \\
        \hline
        3.024 & 2.909 & \textbf{2.882} & 2.890 \\
        \hline
    \end{tabular}
\end{table}

\paragraph{Large batches}
To test the effect of large batches we fix the total number of tokens for the 124M parameter model, and sweep over the batch sizes while rescaling the total number of steps accordingly.
The stepsize is optimized for each combination of batch size and optimizer.
We observe that ({\sc Unconstrained}) \Scion is better at maintaining a low validation loss with increasing batch size than the baselines (\textit{cf.}, \Cref{fig:GPT:bz}).

\paragraph{Image classification}
We additionally test on a convolutional neural network (CNN) on the CIFAR10 dataset.
We focus on \Scion since norm control of the parameters is important for the setting.
We use the configuration (Spectral $\rightarrow$ Spectral $\rightarrow$ Sign) and sweep over width and stepsize.
The explicit control on the norm provided by \Scion circumvents the need for the Frobenius norm normalization of the weights present in the base implementation \citep{airbench_2024}.
The results are shown in \Cref{fig:CIFAR}, which demonstrates the transferability of the optimal stepsize.



