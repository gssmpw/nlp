
\subsection{Notation and detailed problem setting}
\label{sec:notation_L_layer}

First, we introduce some notation and detailed problem settings. Consider an $L$-layer neural network with input dimension $d_0$, hidden layer widths $\{d_1, d_2, \dots, d_{L-1}\}$, and output dimension $d_L$. For any input data $\mathbf{z} \in \mathbb{R}^{d_0}$, the forward pass of the network is defined as
\begin{equation}
\begin{split}
    & \mathbf{f}^{(1)}(\mathbf{z}) = \mathbf{W}^{(1)} \mathbf{z}, 
    \qquad 
    \mathbf{h}^{(1)}(\mathbf{z}) = \sigma\bigl(\mathbf{f}^{(1)}(\mathbf{z})\bigr),\\
    &\mathbf{f}^{(\ell)}(\mathbf{z}) = \mathbf{W}^{(\ell)} \mathbf{h}^{(\ell-1)}(\mathbf{z}), 
    \qquad 
    \mathbf{h}^{(\ell)}(\mathbf{z}) = \sigma\bigl(\mathbf{f}^{(\ell)}(\mathbf{z})\bigr), 
    \quad \forall \ell = 2, \dots, L-1,\\
    &\mathbf{f}^{(L)}(\mathbf{z}) = \mathbf{W}^{(L)} \mathbf{h}^{(\ell-1)}(\mathbf{z}).
    \label{eq:L_layer_net}
\end{split}
\end{equation}

Here, $\mathbf{W}^{(\ell)} \in \mathbb{R}^{d_{\ell} \times d_{\ell-1}}$ are the weight matrices of the network, and $\sigma(\cdot)$ is an element-wise activation function. 
We denote $\mathbf{f}^{(\ell)}(\mathbf{z}) \in \mathbb{R}^{d_{\ell}}$ as the preactivation of the $l$-th layer and $\mathbf{h}^{(\ell)}(\mathbf{z}) \in \mathbb{R}^{d_{\ell}}$ as the corresponding postactivation.
The network output $\mathbf{f}^{(L)}(\mathbf{z})$ is calculated as a linear transformation of the final hidden layer representation.


Below, we provide a brief overview of the specific assumptions for the input data and initialization. Then we explicitly define the spectral norm choice used in our analysis:

\begin{assumption}[Input data]
\label{assumption:input_data}
Training samples $(\mathbf{z}, \mathbf{y})$ are drawn from a distribution $\mathcal{P}$, 
where $\mathbf{z} \in \mathbb{R}^{d_0}$ and $\mathbf{y} \in \mathbb{R}^{d_L}$. 
We assume $\mathbf{z}$ has bounded second moments, 
i.e., $\|\mathbf{z}\|^2_2< \infty$.
\end{assumption}

\begin{assumption}[Initialization schemes]
\label{assumption:initialization}
The initialization satisfy the following condition for the stability of the maximal update learning rate~\citep{yang2023spectral}:
\begin{equation*}
    \| \mathbf{W}^{(\ell)} \|_{\mathcal{S}_\infty} 
      = \Theta\Bigl(\sqrt{\tfrac{d_\ell}{d_{\ell-1}}}\Bigr).
\end{equation*}
Both semi-orthogonal initialization and (with high probability) 
Gaussian initialization satisfy this condition.
\end{assumption}


\begin{assumption}[Spectral norm choice for $\lmo$]
\label{assumption:lmo_choice}
We adopt the layer-wise linear maximization oracles ($\lmo$s) from \Cref{tbl:parameter:lmo,tbl:parameter:lmo:1hot}. Specifically, 
for the first layer \(\mathbf{W}^{(1)}\), we use
\[
  \lmo\bigl(\nabla_{\mathbf{W}^{(1)}}\mathcal{L}\bigr)
  \;=\; \max\!\Bigl(1,\,\sqrt{\tfrac{d_{\mathrm{out}}}{d_{\mathrm{in}}}}\Bigr)\,\mathbf{U}^{(1)}\,\mathbf{V}^{(1)\top},
\]
where \(\mathbf{W}^{(1)} = \mathbf{U}^{(1)}\mathbf{\Lambda}^{(1)}\mathbf{V}^{(1)\top}\) is the reduced SVD of 
\(\mathbf{W}^{(1)}\). For each intermediate layer \(\mathbf{W}^{(\ell)}\), 
\(\ell \in \{2,\dots,L\}\), we similarly set
\[
  \lmo\bigl(\nabla_{\mathbf{W}^{(\ell)}}\mathcal{L}\bigr)
  \;=\; \sqrt{\tfrac{d_{\mathrm{out}}}{d_{\mathrm{in}}}}\mathbf{U}^{(\ell)}\,\mathbf{V}^{(\ell)\top},
\]
\end{assumption}
\begin{remark}
In all cases, these choices are consistent with the (Spectral $\rightarrow$ Spectral $\rightarrow$ Spectral) configuration from \Cref{tbl:parameter:lmo,tbl:parameter:lmo:1hot}. 
Our results will simultaneously hold for the (ColNorm $\rightarrow$ Spectral $\rightarrow$ Spectral) configuration, due to the equivalence under 1-hot encoded (\textit{cf.} \Cref{insight:norm-choice}).
\end{remark}




To investigate how these neuron preactivations change after one step of \cref{alg:uSCG}, we consider a $\text{batch size} = 1$ setting. 
We analyze the update dynamics of the network parameters under general loss functions $\mathcal{L}$, including but not limited to mean squared error (MSE) and logistic loss.

We follow \Citet{yang2021tensor} which states that a \emph{good} learning rate enables hidden layer preactivations to undergo the largest possible change in a single update step, while still avoiding divergence when the network width is large.

Let $\Delta f^{(\ell)}_i(\mathbf{z})$ be the change in the preactivation of the $i$-th neuron in the $l$-th hidden layer after one step of~\cref{alg:uSCG} on $(\mathbf{z},\mathbf{y})$. The so-called “maximal update” heuristic requires that:
\[
\text{The maximal update learning rate }\,\gamma^*
\;:=\;
\text{the learning rate for which }
\mathbb{E}\bigl[\bigl(\Delta f^{(\ell)}_i(\mathbf{z})\bigr)^2\bigr] \;\simeq\; 1,
\]
with the expectation again taken over the initialization distribution.


\subsection{Hyperparameter transfer}\label{app:transfer:proof}

To begin with, we prove the following lemma that derives the $\lmo$ of the gradient in an $L$-layer neural network using the spectral norm choice from \Cref{tbl:parameter:lmo}.

\begin{lemma}[Spectral $\lmo$ for the gradient with respect to \( \mathbf{W}^{(\ell)} \)]\label{lem:transfer:lmo}

Consider an $L$-layer neural network with input dimension $d_0$, hidden layer widths $\{d_1, d_2, \dots, d_{L-1}\}$, and output dimension $d_L$. Training samples $(\mathbf{z}, \mathbf{y})$ are drawn from some distribution $\mathcal{P}$, where $\mathbf{z} \in \mathbb{R}^{d_0}$ and $\mathbf{y} \in \mathbb{R}^{d_L}$. The network follows the forward pass \eqref{eq:L_layer_net}.
For convenience, we set \( \mathbf{f}^{(0)} = \mathbf{h}^{(0)} = \mathbf{z} \), making the notation consistent for all layers.
The linear maximization oracle ($\lmo$) over the scaled spectral norm ball, $\mathcal D = \big\{ \mathbf{W} \mid \|\mathbf{W}\|_{\mathcal S_\infty}\leq \sqrt{\tfrac{d_\ell}{d_{\ell-1}}}\big\}$, for $\nabla_{\mathbf{W}^{(\ell)}} \mathcal{L}(\mathbf{z},\mathbf{y}) $, denoted as $\lmo(\nabla_{\mathbf{W}^{(\ell)}} \mathcal{L}(\mathbf{z},\mathbf{y}) )$, is given by

\[
\lmo(\nabla_{\mathbf{W}^{(\ell)}}\mathcal{L}(\mathbf{z},\mathbf{y}) ) = \sqrt{\frac{d_{\ell}}{d_{\ell-1}}} \frac{\Bigl(\frac{d \mathcal{L}(\mathbf{z},\mathbf{y}) }{d \mathbf{f}^{(\ell)}} \Bigr) \mathbf{h}^{(\ell-1)^\top}}{\Bigl\|\frac{d \mathcal{L}(\mathbf{z},\mathbf{y}) }{d \mathbf{f}^{(\ell)}}\Bigr\|_2 \|\mathbf{h}^{(\ell-1)}\|_2}.
\]

\end{lemma}

\begin{proof}

First, we express the gradient with respect to $\mathbf{W}^{(\ell)}$. By applying the chain rule we have

\[
\nabla_{\mathbf{W}^{(\ell)}} \mathcal{L}(\mathbf{z},\mathbf{y})  = \Bigl( \frac{d \mathcal{L}(\mathbf{z},\mathbf{y}) }{d \mathbf{f}^{(\ell)}} \Bigr) \mathbf{h}^{(\ell-1)^\mathsf{T}}.
\]

We immediately have that the $\lmo$ is given as

\begin{equation*}
    \lmo(\nabla_{\mathbf{W}^{(\ell)}} \mathcal{L}(\mathbf{z},\mathbf{y}) ) = \sqrt{\frac{d_{\ell}}{d_{\ell-1}}} \frac{\Bigl(\frac{d \mathcal{L}(\mathbf{z},\mathbf{y}) }{d \mathbf{f}^{(\ell)}} \Bigr) \mathbf{h}^{(\ell-1)^\top}}{\Bigl\|\frac{d \mathcal{L}(\mathbf{z},\mathbf{y}) }{d \mathbf{f}^{(\ell)}}\Bigr\|_2 \|\mathbf{h}^{(\ell-1)}\|_2}.
\end{equation*}

\end{proof}


Now, we are equipped to state and prove \Cref{lemma:mu_transfer}. We follow the proof technique of \citet{yang2023spectral} in our derivations.

