We begin by presenting the two main assumptions we will make to analyze \Cref{alg:uSCG,alg:SCG}. The first is an assumption on the Lipschitz-continuity of $\nabla f$ with respect to the norm $\|\cdot\|_{\ast}$ restricted to $\mathcal{X}$. We do not assume this norm to be Euclidean which means our results apply to the geometries relevant to training neural networks.
\begin{assumption}\label{asm:Lip} The gradient $\nabla f$ is $L$-Lipschitz with $L \in (0,\infty)$, i.e.,
    \begin{equation}
    \|\nabla f(x) - \nabla f(x)\|_{\ast}
    \leq
    L\|x-y\|
    \quad \forall x,y \in \mathcal X.
    \end{equation}
Furthermore, $f$ is bounded below by $\fmin$.
\end{assumption}
Our second assumption is that the stochastic gradient oracle we have access to is unbiased and has a bounded variance, a typical assumption in stochastic optimization.
\begin{assumption}\label{asm:stoch}
The stochastic gradient oracle $\nabla f(\cdot,\xi):\mathcal X\rightarrow \mathbb{R}^d$ satisfies.
    \begin{assnum}
        \item \label{asm:stoch:unbiased}
            Unbiased:
            \(%
                \mathbb{E}_{\xi}\left[\nabla f(x,\xi)\right] = \nabla f(x) \quad \forall x \in \mathcal X
            \).%
        \item  \label{asm:stoch:var}
            Bounded variance:\\
            \(%
                \mathbb{E}_{\xi}\left[\|\nabla f(x,\xi)-\nabla f(x)\|_2^2\right] \leq \sigma^2  \quad \forall x \in \mathcal X,\sigma\geq 0
            \).%
    \end{assnum}
\end{assumption}

With these assumptions we can state our worst-case convergence rates, first for \Cref{alg:uSCG} and then for \Cref{alg:SCG}. 

\looseness=-1To bridge the gap between theory and practice, we investigate these algorithms when run with a \emph{constant} stepsize $\gamma$, which depends on the specified horizon $n\in\mathbb{N}^*$, and momentum which is either constant $\alpha\in(0,1)$ (except for the first iteration where we take $\alpha=1$ by convention) or \emph{vanishing} $\alpha_k\searrow 0$. The exact constants for the rates can be found in the proofs in \Cref{app:analysis}; we try to highlight the dependence on the parameters $L$ and $\rho$, which correspond to the natural geometry of $f$ and $\mathcal{D}$, explicitly here. Our rates are non-asymptotic and use big O notation for brevity.

\begin{toappendix}
\label{app:analysis}
In this section we present the proofs of the main convergence results of the paper as well as some intermediary lemmas that we will make use of along the way. Throughout this section, we adopt the notation:
\begin{align*}
\text{(stochastic gradient estimator error)} && \lambda^k &:= d^k-\nabla f(x^k) \\
\text{(diameter of $\mathcal{D}$ in $\ell_2$ norm)} && D_2 &:= \max_{x,y\in\mathcal{D}}\norm{x-y}_2 \\
\text{(radius of $\mathcal{D}$ in $\ell_2$ norm)} && \rho_2 &:= \max_{x\in\mathcal{D}}\norm{x}_2 \\
\text{(norm equivalence constant)} && \zeta &:= \max_{x\in\mathcal{X}}\frac{\norm{x}_{\ast}}{\norm{x}_2} \\
\text{(Lipschitz constant of $\nabla f$ with respect to $\norm{\cdot}_{2}$)} && L_2 &:= \inf \{M>0\colon \forall x,y\in\mathcal{X}, \norm{\nabla f(x)-\nabla f(y)}_{2}\leq M\norm{x-y}_{2}\}
\end{align*}
We analyze each algorithm separately, although the analysis is effectively unified between the two, modulo constants. This is done in \Cref{subsec:uSCG,subsec:SCG}, respectively. Our convergence analysis proceeds in three steps: we begin by establishing a template descent inequality for each algorithm via the descent lemma. Next, we analyze the behavior of the second moment of the error $\mathbb{E}[\norm{\lambda^k}_{2}^2]$ under different choices for $\alpha$. Then, we combine these results to derive a convergence rate. Finally, we note that when analyzing algorithms with constant momentum, we will still always take $\alpha=1$ on the first iteration $k=1$.

\subsection{Convergence analysis of \ref{eq:uSCG}}\label{subsec:uSCG}
We begin with the analysis of \Cref{alg:uSCG} by establishing a generic template inequality for the dual norm of the gradient at iteration $k$. This inequality holds regardless of whether the momentum $\alpha_k$ is constant or vanishing, as long as it remains in $(0,1]$.
\begin{lemma}[\ref{eq:uSCG} template inequality]
\label{lem:uSCGtemplate1}
    Suppose \Cref{asm:Lip} holds. Let $n\in\mathbb{N}^*$ and consider the iterates $\{x^{k}\}_{k=1}^n$ generated by \Cref{alg:uSCG} with a constant stepsize $\gamma>0$.
    Then we have
    \begin{equation}
        \mathbb{E}[\norm{\nabla f(\bar{x}^n)}_2^2]\leq \frac{\mathbb{E}[f(x^{1})-\fmin]}{\rho\gamma n} +\frac{L\rho\gamma}{2} + \frac{1}{n}\left(\frac{\rho_2}{\rho}+\zeta\right)\sum\limits_{k=1}^n\sqrt{\mathbb{E}[\norm{\lambda^{k}}_2^2]}.
    \end{equation}
\end{lemma}
\begin{proof}
    Under \Cref{asm:Lip}, we can use the descent lemma for the function $f$ at the points $x^{k}$ and $x^{k+1}$ to get, for all $k\in\{1,\ldots,n\}$,
    \begin{equation}\label{eq:lem:uSCGtemplate1:first2}
        \begin{aligned}
            f(x^{k+1})&\leq f(x^{k})+ \langle \nabla f(x^{k}),x^{k+1}-x^{k}\rangle +\tfrac{L}{2}\norm{x^{k+1}-x^{k}}^{2}
            \\
            &= f(x^{k})+\langle \nabla f(x^{k})-d^{k},x^{k+1}-x^{k}\rangle + \langle d^{k},x^{k+1}-x^{k}\rangle+\tfrac{L}{2}\norm{x^{k+1}-x^{k}}^{2}
            \\
            &= f(x^{k})+\gamma \langle \nabla f(x^{k})-d^{k},\lmo (d^{k})\rangle+\gamma \langle d^{k},\lmo(d^{k})\rangle +\tfrac{L\gamma^{2}}{2}\norm{\lmo(d^{k})}^{2}
            \\
            &\leq f(x^{k})+\gamma \rho_{2}\norm{\lambda^{k}}_{2}+\gamma \langle d^{k},\lmo(d^{k})\rangle +\tfrac{L\gamma^{2}}{2}\rho^{2},
        \end{aligned}
    \end{equation}
    the final step employing Cauchy-Schwarz, the definition of $\lambda^k$, and the definition of $\rho_2$ as the radius of $\mathcal{D}$ in the $\norm{\cdot}_2$ norm.
    By definition of the dual norm we have, for all $u\in\mathcal{X}$,
    \begin{equation*}
        \|u\|_{\ast} = \max\limits_{v\colon \|v\|\leq 1}\langle u,v\rangle = \max_{v\in\mathcal{D}}\langle u,\tfrac{1}{\rho}v\rangle= -\langle u, \tfrac{1}{\rho}\lmo(u)\rangle
    \end{equation*}
    which means that, for all $k\in\{1,\ldots,n\}$,
    \begin{equation*}
        \gamma \langle d^k, \lmo(d^k)\rangle = \gamma\rho\langle d^k,\tfrac{1}{\rho}\lmo(d^k)\rangle = -\gamma\rho\|d^k\|_{\ast}.
    \end{equation*}
    Plugging this expression for $\gamma\langle d^k,\lmo(d^k)\rangle$ into \eqref{eq:lem:uSCGtemplate1:first2} gives, for all $k\in\{1,\ldots,n\}$,
    \begin{equation*}
        \begin{aligned}
            f(x^{k+1})
                &\leq f(x^{k})+\gamma \rho_{2}\norm{\lambda^{k}}_{2}-\gamma\rho\|d^k\|_{\ast} +\tfrac{L\gamma^{2}}{2}\rho^{2}\\
                &= f(x^{k})+\gamma \rho_{2}\norm{\lambda^{k}}_{2}-\gamma\rho\|d^k - \nabla f(x^k) + \nabla f(x^k)\|_{\ast} +\tfrac{L\gamma^{2}}{2}\rho^{2}\\
                &\stackrel{\text{(a)}}{\leq} f(x^{k})+\gamma \rho_{2}\norm{\lambda^{k}}_{2} +\gamma\rho\|\lambda^k\|_{\ast} -\gamma\rho\|\nabla f(x^k)\|_{\ast} +\tfrac{L\gamma^{2}}{2}\rho^{2}\\
                &\stackrel{\text{(b)}}{\leq} f(x^{k})+\gamma (\rho_{2}+\zeta\rho)\norm{\lambda^{k}}_{2}-\gamma\rho\|\nabla f(x^k)\|_{\ast} +\tfrac{L\gamma^{2}}{2}\rho^{2},
        \end{aligned}
    \end{equation*}
    applying the reverse triangle inequality in (a) while (b) stems from the definition of $\zeta$.
    By rearranging terms and taking expectations, we get
    \begin{equation*}
        \begin{aligned}
            \gamma\rho\mathbb{E}[\norm{\nabla f(x^k)}_{\ast}]
                &\leq \mathbb{E}[f(x^{k})-f(x^{k+1})] + \gamma\left(\rho_2+\zeta\rho\right)\mathbb{E}[\norm{\lambda^{k}}_2] +\frac{L\rho^2\gamma^2}{2}.
        \end{aligned}
    \end{equation*}
    Summing this from $k=1$ to $n$ and dividing by $\gamma\rho n$ we get
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[\norm{\nabla f(\bar{x}^n)}_{\ast}]
                &= \frac{1}{n}\sum\limits_{k=1}^n\mathbb{E}[\norm{\nabla f(x^k)}_{\ast}]\\
                &\leq \frac{\mathbb{E}[f(x^{1})-f(x^{n+1})]}{\rho\gamma n} +\frac{L\rho\gamma}{2} + \frac{1}{n}\left(\frac{\rho_2}{\rho}+\zeta\right)\sum\limits_{k=1}^n\mathbb{E}[\norm{\lambda^{k}}_2]\\
                &\stackrel{\text{(a)}}{\leq} \frac{\mathbb{E}[f(x^{1})-\fmin]}{\rho\gamma n} +\frac{L\rho\gamma}{2} + \frac{1}{n}\left(\frac{\rho_2}{\rho}+\zeta\right)\sum\limits_{k=1}^n\mathbb{E}[\norm{\lambda^{k}}_2]\\
                &\stackrel{\text{(b)}}{\leq} \frac{\mathbb{E}[f(x^{1})-\fmin]}{\rho\gamma n} +\frac{L\rho\gamma}{2} + \frac{1}{n}\left(\frac{\rho_2}{\rho}+\zeta\right)\sum\limits_{k=1}^n\sqrt{\mathbb{E}[\norm{\lambda^{k}}_2^2]},
        \end{aligned}
    \end{equation*}
    using the definition of $\fmin$ for (a) and Jensen's inequality for (b).
\end{proof}

At this point, we need to determine the growth of the induced error captured by the quantity $\norm{\lambda^{k}}_2^2$. To estimate this, we first use a recursion relating $\mathbb{E}[\norm{\lambda^{k}}_2^2]$ and $\mathbb{E}[\norm{\lambda^{k-1}}_2^2]$ adapted from the proof in \citet[Lem. 6]{mokhtari2020stochastic} and then we prove a bound on the decay of $\norm{\lambda^k}_2^2$ for \Cref{alg:uSCG}.
\begin{lemma}[Linear recursive inequality for $\mathbb{E}\norm{\lambda^k}_2^2$]\label{lem:uSCGerror}
    Suppose \Cref{asm:Lip,asm:stoch} hold. Let $n\in\mathbb{N}^*$ and consider the iterates $\{x_k\}_{k=1}^n$ generated by \Cref{alg:uSCG} with a constant stepsize $\gamma>0$. Then, for all $k\in\{1,\ldots,n
    \}$,
    \begin{equation*}
        \mathbb{E}[\norm{\lambda^k}_2^2] \leq \left(1-\frac{\alpha_k}{2}\right)\mathbb{E}[\norm{\lambda^{k-1}}_2^2] + \frac{2L_2^2\rho_2^2\gamma^2}{\alpha_k} + \alpha_k^2\sigma^2.
    \end{equation*}
\end{lemma}
\begin{proof}
    The proof is a straightforward adaptation of the arguments laid out in \citet[Lem. 6]{mokhtari2020stochastic}, which in fact do not depend on convexity nor on the choice of stepsize. Let $n\in\mathbb{N}^*$ and $k\in\{1,\ldots,n\}$, then
    \begin{equation*}
        \begin{aligned}
            \norm{\lambda^k}_2^2
                &= \norm{\nabla f(x^k) - d^{k}}_2^2\\
                &= \norm{\nabla f(x^k) - \alpha_k \nabla f(x^k,\xi_k) - (1-\alpha_k)d^{k-1}}_2^2\\
                &= \norm{\alpha_k\left(\nabla f(x^k) - \nabla f(x^k,\xi_k)\right) +(1-\alpha_k)\left(\nabla f(x^{k})-\nabla f(x^{k-1})\right) - (1-\alpha_k)\left(d^{k-1} - \nabla f(x^{k-1})\right)}_2^2\\
                &= \alpha_k^2\norm{\nabla f(x^k) - \nabla f(x^k,\xi_k)}_2^2 + (1-\alpha_k)^2\norm{\nabla f(x^k)-\nabla f(x^{k-1})}_2^2\\
                    &\quad\quad + (1-\alpha_k)^2\norm{\nabla f(x^{k-1})-d^{k-1}}_2^2\\
                    &\quad\quad +2\alpha_k(1-\alpha_k)\langle\nabla f(x^{k-1})-\nabla f(x^{k-1},\xi_{k-1}), \nabla f(x^k)-\nabla f(x^{k-1})\rangle\\
                    &\quad\quad +2\alpha_k(1-\alpha_k)\langle \nabla f(x^k)-\nabla f(x^k,\xi_k), \nabla f(x^{k-1})-d^{k-1}\rangle\\
                    &\quad\quad +2(1-\alpha_k)^2\langle \nabla f(x^k)-\nabla f(x^{k-1}),\nabla f(x^{k-1}) - d^{k-1}\rangle.
        \end{aligned}
    \end{equation*}
    Taking the expectation conditioned on the filtration $\mathcal{F}_k$ generated by the iterates until $k$, i.e., the sigma algebra generated by $\{x_1,\ldots,x_k\}$, which we denote using $\mathbb{E}_k[\cdot]$, and using the unbiased property in \Cref{asm:stoch}, we get,
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}_k[\norm{\lambda^k}_2^2]
                &= \alpha_k^2\mathbb{E}_k[\norm{\nabla f(x^k)-\nabla f(x^k,\xi_k)}_2^2] + (1-\alpha_k)^2\norm{\nabla f(x^k)-\nabla f(x^{k-1})}_2^2\\
                    &\quad\quad + (1-\alpha_k)^2\norm{\lambda^{k-1}}_2^2 + 2(1-\alpha_k)^2\langle \nabla f(x^k)-\nabla f(x^{k-1}),\lambda^{k-1}\rangle.
        \end{aligned}
    \end{equation*}
    From this expression we can estimate,
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}_k[\norm{\lambda^k}_2^2]
                &\stackrel{\text{(a)}}{\leq} \alpha_k^2\sigma^2 + (1-\alpha_k)^2\norm{\nabla f(x^{k})-\nabla f(x^{k-1})}_2^2 + (1-\alpha_k)^2\norm{\lambda^{k-1}}_2^2 + 2(1-\alpha_k)^2\langle \nabla f(x^k)-\nabla f(x^{k-1}),\lambda^{k-1}\rangle\\
                &\stackrel{\text{(b)}}{\leq} \alpha_k^2\sigma^2 + (1-\alpha_k)^2\norm{\nabla f(x^{k})-\nabla f(x^{k-1})}_2^2 + (1-\alpha_k)^2\norm{\lambda^{k-1}}_2^2\\
                    &\quad\quad + (1-\alpha_k)^2\left(\tfrac{\alpha_k}{2}\norm{\nabla f(x^k)-\nabla f(x^{k-1})}_2^2+\tfrac{2}{\alpha_k}\norm{\lambda^{k-1}}_2^2\right)\\
                 &\stackrel{\text{(c)}}{\leq} \alpha_k^2\sigma^2 + (1-\alpha_k)^2L_2^2\norm{x^k-x^{k-1}}_2^2 + (1-\alpha_k)^2\norm{\lambda^{k-1}}_2^2 + (1-\alpha_k)^2\left((\tfrac{\alpha_k}{2})L_2^2\norm{x^k-x^{k-1}}_{2}^2+\tfrac{2}{\alpha_k}\norm{\lambda^{k-1}}_2^2\right)\\
                 &\stackrel{\text{(d)}}{\leq} \alpha_k^2\sigma^2 + (1-\alpha_k)^2L_2^2\rho_2^2\gamma^2 + (1-\alpha_k)^2\norm{\lambda^{k-1}}_2^2 + (1-\alpha_k)^2\left((\tfrac{\alpha_k}{2})L_2^2\rho_2^2\gamma^2+\tfrac{2}{\alpha_k}\norm{\lambda^{k-1}}_2^2\right)\\
                 &\stackrel{\text{(e)}}{\leq} \alpha_k^2\sigma^2 + (1+\tfrac{\alpha_k}{2})(1-\alpha_k)L_2^2\rho_2^2\gamma^2 + (1+\tfrac{2}{\alpha_k})(1-\alpha_k)\norm{\lambda^{k-1}}_2^2,
        \end{aligned}
    \end{equation*}
    using the bounded variance property from \Cref{asm:stoch} for (a), Young's inequality with parameter $\alpha_k/2>0$ for (b), the Lipschitz property of $f$ under norm $\|\cdot\|_2$ for (c), the update definition from \Cref{alg:uSCG} for (d), and the fact that $1-\alpha_k < 1$ for (e).
    To complete the proof, we note that
    \begin{equation*}
        (1+\tfrac{2}{\alpha_k})(1-\alpha_k)\leq \tfrac{2}{\alpha_k}\quad\text{and}\quad(1-\alpha_k)(1+\tfrac{\alpha_k}{2})\leq (1-\tfrac{\alpha_k}{2})
    \end{equation*}
    which, applied to the previous inequality and taking total expectations, yields
    \begin{equation*}
        \mathbb{E}[\norm{\lambda^k}_2^2] \leq \left(1-\frac{\alpha_k}{2}\right)\mathbb{E}[\norm{\lambda^{k-1}}_2^2] + \alpha_k^2\sigma^2 + \frac{2L_2^2\rho_2^2\gamma^2}{\alpha_k}.
    \end{equation*}
\end{proof}

\subsubsection{Constant $\alpha$}

\begin{lemma}
    Suppose \Cref{asm:Lip,asm:stoch} hold. Let $n \in \mathbb{N}^*$ and consider the iterates $\{x^k\}_{k=1}^n$ generated by \Cref{alg:uSCG} with constant stepsize $\gamma >0$ and constant momentum $\alpha\in(0,1)$ with the exception of the first iteration, where we take $\alpha=1$.
    Then, we have for all $k\in\{1,\ldots,n\}$
    \begin{equation*}
        \begin{aligned}
            \sqrt{\mathbb{E}[\norm{\lambda^k}_2^2]}
                &\leq \frac{\sqrt{2}L_2\rho_2\gamma}{\alpha} + \left(\sqrt{\alpha} + \left(\sqrt{1-\frac{\alpha}{2}}\right)^k\right)\sigma.
        \end{aligned}
    \end{equation*}
\end{lemma}
\begin{proof}
    Let $n\in\mathbb{N}^*$, $k\in\{1,\ldots,n\}$, and invoke \Cref{lem:uSCGerror} to get
    \begin{equation*}
        \mathbb{E}[\norm{\lambda^k}_2^2] \leq \left(1-\frac{\alpha}{2}\right)\mathbb{E}[\norm{\lambda^{k-1}}_2^2] + \frac{2L_2^2\rho_2^2\gamma^2}{\alpha} + \alpha^2\sigma^2.
    \end{equation*}
    Applying \Cref{lem:recursive_geometric} with $\beta = \frac{\alpha}{2}$ and $\eta = \frac{2L_2^2\rho_2^2\gamma^2}{\alpha}+\alpha^2\sigma^2$ gives directly
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[\norm{\lambda^k}_2^2]
                &\leq \frac{2L_2^2\rho_2^2\gamma^2}{\alpha^2} + \alpha\sigma^2 + \left(1-\frac{\alpha}{2}\right)^k\mathbb{E}[\norm{\lambda^1}_2^2]\\
                &\leq \frac{2L_2^2\rho_2^2\gamma^2}{\alpha^2} + \left(\alpha + \left(1-\frac{\alpha}{2}\right)^k\right)\sigma^2
        \end{aligned}
    \end{equation*}
    after using \Cref{asm:stoch} in the final inequality.
    Taking square roots and upper boudning then yields
    \begin{equation*}
        \begin{aligned}
            \sqrt{\mathbb{E}[\norm{\lambda^k}_2^2]}
                &\leq \frac{\sqrt{2}L_2\rho_2\gamma}{\alpha} + \left(\sqrt{\alpha} + \left(\sqrt{1-\frac{\alpha}{2}}\right)^k\right)\sigma.
        \end{aligned}
    \end{equation*}
\end{proof}

\end{toappendix}

\begin{lemmarep}[{Convergence rate for \ref{eq:uSCG} with constant $\alpha$}]\label{lem:uSCGrate1}
    Suppose \Cref{asm:Lip,asm:stoch} hold. Let $n\in\mathbb{N}^*$ and consider the iterates $\{x^k\}_{k=1}^n$ generated by \Cref{alg:uSCG} with constant stepsize $\gamma = \frac{1}{\sqrt{n}}$ and constant momentum $\alpha\in(0,1)$.
    Then, it holds that
    \begin{equation*}
        \mathbb{E}[\norm{\nabla f(\bar{x}^n)}_{\ast}] \leq O\left(\tfrac{L\rho}{\sqrt{n}}+\sigma\right).
    \end{equation*}
\end{lemmarep}
\begin{appendixproof}
    Let $n\in\mathbb{N}^*$; we will first invoke \Cref{lem:uSCGtemplate1} and then we will estimate the error terms inside using \Cref{lem:uSCGerror} under \Cref{asm:Lip,asm:stoch}.
    As shown in \Cref{lem:uSCGtemplate1},
    \begin{equation}\label{eq:uSCGrate1}
        \begin{aligned}
            \mathbb{E}[\norm{\nabla f(\bar{x}^n)}_2^2]
                &\leq \frac{\mathbb{E}[f(x^{1})-\fmin]}{\rho\gamma n} +\frac{L\rho\gamma}{2n} + \frac{1}{n}\left(\frac{\rho_2}{\rho}+\zeta\right)\sum\limits_{k=1}^n\sqrt{\mathbb{E}[\norm{\lambda^{k}}_2^2]}.
            \end{aligned}
    \end{equation}
    By \Cref{lem:uSCGerror} with \Cref{lem:recursive_geometric}, we get
    \begin{equation*}
        \sqrt{\mathbb{E}[\norm{\lambda^k}_2^2]}
            \leq \frac{\sqrt{2}L_2\rho_2\gamma}{\alpha} + \left(\sqrt{\alpha} + \left(\sqrt{1-\frac{\alpha}{2}}\right)^k\right)\sigma
    \end{equation*}
    which, if we sum from $k=1$ to $n$, gives us
    \begin{equation*}
        \sum\limits_{k=1}^n\sqrt{\mathbb{E}[\norm{\lambda^k}_2^2]}
            \leq n\frac{\sqrt{2}L_2\rho_2\gamma}{\alpha} + \left(n\sqrt{\alpha} + \frac{\sqrt{1-\frac{\alpha}{2}}}{1-\sqrt{1-\frac{\alpha}{2}}}\right)\sigma.
    \end{equation*}
    Plugging this estimate into \Cref{eq:uSCGrate1} gives
    \begin{equation}\label{eq:uSCGfinalineq}
        \begin{aligned}
            \mathbb{E}[\norm{\nabla f(\bar{x}^n)}_2^2]
                &\leq \frac{\mathbb{E}[f(x^{1})-\fmin]}{\rho\gamma n} +\frac{L\rho\gamma}{2} + \frac{1}{n}\left(\frac{\rho_2}{\rho}+\zeta\right)\sum\limits_{k=1}^n\mathbb{E}[\norm{\lambda^{k}}_2]\\
                &\leq \frac{\mathbb{E}[f(x^{1})-\fmin]}{\rho\gamma n} +\frac{L\rho\gamma}{2} + \frac{1}{n}\left(\frac{\rho_2}{\rho}+\zeta\right)\left(n\frac{\sqrt{2}L_2\rho_2\gamma}{\alpha} + \left(n\sqrt{\alpha} + \frac{\sqrt{1-\frac{\alpha}{2}}}{1-\sqrt{1-\frac{\alpha}{2}}}\right)\sigma\right)\\
                &= \frac{\mathbb{E}[f(x^{1})-\fmin]}{\rho\gamma n} +\frac{L\rho\gamma}{2} + \left(\frac{\rho_2}{\rho}+\zeta\right)\left(\frac{\sqrt{2}L_2\rho_2\gamma}{\alpha} + \left(\sqrt{\alpha} + \frac{\sqrt{1-\frac{\alpha}{2}}}{n(1-\sqrt{1-\frac{\alpha}{2}})}\right)\sigma\right).
        \end{aligned}
    \end{equation}
    Finally, by substituting $\gamma = \frac{1}{\sqrt{n}}$ and noting $f(x^{n+1}) \geq \fmin$ we arrive at
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[\norm{\nabla f(\bar{x}^n)}_{\ast}]
                &\leq \frac{\mathbb{E}[f(x^{1})-\fmin]}{\sqrt{n}\rho} +\frac{L\rho}{2\sqrt{n}} + \left(\frac{\rho_2}{\rho}+\zeta\right)\left(\frac{\sqrt{2}L_2\rho_2}{\alpha\sqrt{n}} + \left(\sqrt{\alpha} + \frac{\sqrt{1-\frac{\alpha}{2}}}{n(1-\sqrt{1-\frac{\alpha}{2}})}\right)\sigma\right)\\
                &= O\left(\frac{1}{\sqrt{n}} + \sigma\right).
        \end{aligned}
    \end{equation*}
\end{appendixproof}

\begin{toappendix}

\subsubsection{Vanishing $\alpha_k$}\label{subsec:uSCGvanishing}

\begin{lemma}[Bound on the gradient error with vanishing $\alpha$]
\label{lem:uSCGerrorbound}
    Suppose \Cref{asm:Lip,asm:stoch} hold. Let $n\in\mathbb{N}^*$ and consider the iterates $\{x_{k}\}_{k=1}^n$ generated by \Cref{alg:uSCG}
    with a constant stepsize $\gamma$ satisfying
    \begin{equation}
        \frac{1}{2 n^{3/4}}<\gamma <\frac{1}{n^{3/4}}.
    \end{equation}
    Moreover, consider momentum which vanishes $\alpha_{k}= \frac{1}{\sqrt{k}}$. Then, for all $k\in\{1,\ldots,n\}$ the following holds
     \begin{equation}
            \mathbb{E}[\norm{\lambda^{k}}_{2}^{2}]\leq \frac{4\sigma^2+8L_2^2\rho_2^2}{\sqrt{k}}.
    \end{equation}
\end{lemma}

\begin{proof}
    Let $k\in\{1,\ldots,n\}$, then by invoking the recursive inequality obtained in \Cref{lem:uSCGerror} for $\mathbb{E}[\norm{\lambda^k}_2^2]$ we have,
    \begin{equation}
        \mathbb{E}[\norm{\lambda^k}^{2}_{2}]\leq \left(1-\frac{\alpha_{k}}{2}\right)\mathbb{E}[\norm{\lambda^{k-1}}^{2}_{2}]+\alpha_{k}^{2}\sigma^{2}+\frac{2L_2^2\rho_2^2\gamma^2}{\alpha_{k}}.
        \end{equation}
        Using the particular choice of $\gamma$ given in the statement of the lemma,
        \begin{equation}
            \frac{1}{2 n^{3/4}}<\gamma <\frac{1}{n^{3/4}},
        \end{equation}
        as well as the choice of $\alpha_k$ and the fact that $n\geq k$, we get
    \begin{align*}
        \mathbb{E}[\norm{\lambda^k}_2^{2}]
            &\leq \bigg(1-\frac{\alpha_{k}}{2} \bigg)\mathbb{E}[\norm{\lambda^{k-1}}_2^{2}]+\alpha_{k}^{2}\sigma^{2}+\frac{2L_2^2\rho_2^2}{\alpha_{k}n^{3/2}}\\
            &\leq \bigg(1-\frac{\alpha_{k}}{2} \bigg)\mathbb{E}[\norm{\lambda^{k-1}}_2^{2}]+\alpha_{k}^{2}\sigma^{2}+\frac{2L_2^2\rho_2^2}{\alpha_{k}k^{3/2}}\\
            &=\bigg(1-\frac{1}{2\sqrt{k}}\bigg)\mathbb{E}[\norm{\lambda^{k-1}}_2^{2}]+\frac{\sigma^{2}}{k}+\frac{2L_2^2\rho_2^2}{k}\\
            &= \bigg(1-\frac{1}{2\sqrt{k}}\bigg)\mathbb{E}[\norm{\lambda^{k-1}}_2^{2}]+\frac{\sigma^{2}+2L_2^2\rho_2^2}{k}.
        \end{align*}
    Then, by applying \Cref{lem:recursivevanishing} with $u^k = \mathbb{E}[\norm{\lambda^k}_2^2]$ and $c=\sigma^2+2L_2^2\rho_2^2$ we readily obtain
    \begin{equation}
        \mathbb{E}[\norm{\lambda^{k}}_{2}^{2}]\leq \frac{4\sigma^2+8L_2^2\rho_2^2}{\sqrt{k}}
    \end{equation}
    since $Q$ as defined in \Cref{lem:recursivevanishing} is given by $Q = \max\{\mathbb{E}[\norm{\lambda^1}_2^2], 4\sigma^2+8L_2^2\rho_2^2\} \leq 4\sigma^2+8L_2^2\rho_2^2$, which concludes our result.
\end{proof}

Combining these results yields our accuracy guarantees for \Cref{alg:uSCG} with vanishing $\alpha_k$, presented in the next lemma.
\end{toappendix}

\begin{lemmarep}[{Convergence rate for \ref{eq:uSCG} with vanishing $\alpha_k$}]
    Suppose that \Cref{asm:Lip,asm:stoch} hold. Let $n\in\mathbb{N}^*$ and consider the iterates $\{x^{k}\}_{k=1}^n$ generated by \Cref{alg:uSCG} with a constant stepsize $\gamma$ satisfying $\frac{1}{2n^{3/4}}<\gamma <\frac{1}{n^{3/4}}$ and vanishing momentum $\alpha_{k}=\tfrac{1}{\sqrt{k}}$. Then, it holds that
    \begin{equation*}
        \mathbb{E}[\|\nabla f(\bar{x}^n)\|_{\ast}] = O\left(\tfrac{1}{n^{1/4}} + \tfrac{L\rho}{n^{3/4}}\right).
    \end{equation*}
\end{lemmarep}
\begin{appendixproof}
    Let $n\in\mathbb{N}^*$, $k\in\{1,\ldots,n\}$; by combining \Cref{lem:uSCGtemplate1} and \Cref{lem:uSCGerrorbound} we have
    \begin{equation}\label{eq:pre_rate}
        \begin{aligned}
            \mathbb{E}[\|\nabla f(\bar{x}^n)\|_{\ast}]
                &\stackrel{\text{\eqref{lem:uSCGtemplate1}}}{\leq} \frac{2\mathbb{E}[f(x^1)-\fmin]}{\rho n^{1/4}} + \frac{2(\rho_2 + \zeta\rho)\sum_{k=1}^n\sqrt{\mathbb{E}[\norm{\lambda^k}_2^2]}}{\rho n} + \frac{L\rho}{n^{3/4}}\\
                &\stackrel{\text{\eqref{lem:uSCGerrorbound}}}{\leq} \frac{2\mathbb{E}[f(x^1)-\fmin]}{\rho n^{1/4}} + \frac{2(\rho_2 + \zeta\rho)\sqrt{4\sigma^2+8L_2^2\rho_2^2}\sum_{k=1}^{n}\frac{1}{k^{1/4}}}{\rho n}  + \frac{L\rho}{n^{3/4}}\\
                &\leq \frac{2\mathbb{E}[f(x^1)-\fmin]}{\rho n^{1/4}} + \frac{2(\rho_2 + \zeta\rho)\sqrt{4\sigma^2+8L_2^2\rho_2^2}\sum_{k=1}^{n}\frac{1}{k^{1/4}}}{\rho n}  + \frac{L\rho}{n^{3/4}}.
        \end{aligned}
    \end{equation}
    Using the integral test and noting that $x\mapsto \tfrac{1}{x^{1/4}}$ is decreasing on $\mathbb{R}_+$, we can upper bound the sum in the right hand side as
    \begin{equation*}
        \sum_{k=1}^{n}\frac{1}{k^{1/4}}\leq 1 + \int_{1}^{n}\frac{1}{x^{3/4}}dx=1+\frac{4}{3}[x^{3/4}]^{n}_1=1+\frac{4}{3}(n^{3/4}-1) = \frac{4}{3}n^{3/4}-\frac{1}{3}\leq \frac{4}{3}n^{3/4}.
    \end{equation*}
    Inserting the above estimation into \eqref{eq:pre_rate} we arrive at
    \begin{align*}
        \mathbb{E}[\|\nabla f(\bar{x}^n)\|_{\ast}] &\leq \frac{2\mathbb{E}[f(x^1)-\fmin]}{\rho n^{1/4}}+ \frac{8 n^{3/4}(\rho_2 + \zeta\rho)\sqrt{4\sigma^2+8L_2^2\rho_2^2}}{3\rho n}  + \frac{L\rho}{n^{3/4}}\\
        &= \frac{2\mathbb{E}[f(x^1)-\fmin]+ \tfrac{8}{3}(\rho_2 + \zeta\rho)\sqrt{4\sigma^2+8L_2^2\rho_2^2}}{\rho n^{1/4}} + \frac{L\rho}{n^{3/4}}\\
        &= O\left(\frac{1}{n^{1/4}}+\frac{L\rho}{n^{3/4}}\right)
    \end{align*}
    which is the claimed result.
\end{appendixproof}

\begin{toappendix}

\subsection{Convergence analysis of \ref{eq:SCG}}\label{subsec:SCG}

In this section we will analyze the worst-case convergence rate of \Cref{alg:SCG}. To do this, we will prove bounds on the expectation of the so-called Frank-Wolfe gap, $\max\limits_{u\in\mathcal{D}} \langle \nabla f(x), x-u\rangle$, which ensures criticality for the constrained optimization problem over $\mathcal{D}$, i.e., for $x^\star\in\mathcal{D}$
\begin{equation*}
    0 = \nabla f(x^\star) + \mathrm{N}_{\mathcal{D}}(x^\star) \iff \max\limits_{u\in\mathcal{D}} \langle \nabla f(x^\star), x^\star-u\rangle \leq 0
\end{equation*}
where $\mathrm{N}_{\mathcal{D}}$ is the normal cone to the set convex $\mathcal{D}$.

This next lemma characterizes the descent of \Cref{alg:SCG} for any stepsize $\gamma$ and momentum $\alpha_k$ in $(0,1]$.
\begin{lemma}[{Nonconvex analog \citet[Lem. 2]{mokhtari2020stochastic}}]
    \label{lem:commondescent}
    Suppose \Cref{asm:Lip} holds.
    Let $n\in\mathbb{N}^*$ and consider the iterates $\{x_k\}_{k=1^n}$ generated by \Cref{alg:SCG} with constant stepsize $\gamma\in(0,1]$.
    Then, for all $k\in\{1,\ldots,n\}$, for all $u\in \mathcal{D}$, it holds
    \begin{equation}
        \gamma \mathbb{E}[\langle \nabla f(x^k), x^k-u\rangle] \leq \mathbb{E}[f(x^k) - f(x^{k+1})] + D_2\gamma \sqrt{\mathbb{E}[\| \lambda^k\|_2^2]} + 2L\rho^2\gamma^2.
    \end{equation}
\end{lemma}
\begin{proof}
    Let $n\in\mathbb{N}^*$, then by \Cref{asm:Lip} we can apply the descent lemma for the function $f$ at the points $x^k$ and $x^{k+1}$ to get, for all $k\in\{1,\ldots,n\}$,
    \begin{equation*}
        \begin{aligned}
            f(x^{k+1})
                &\leq f(x^k) + \langle \nabla f(x^k), x^{k+1}-x^k\rangle + \tfrac{L}{2}\|x^{k+1}-x^k\|^2\\
                &= f(x^k) + \langle d^k, x^{k+1}-x^k\rangle + \langle \lambda^k, x^{k+1}-x^k\rangle + \tfrac{L}{2}\|x^{k+1}-x^k\|^2\\
                &= f(x^k) + \gamma\langle d^k, \lmo(d^k)-x^k\rangle + \gamma \langle \lambda^k, \lmo(d^k)-x^k\rangle + \tfrac{L}{2}\gamma^2\|\lmo(d^k)-x^k\|^2\\
                &\stackrel{\text{(a)}}{\leq} f(x^k) + \gamma\langle d^k, u-x^k\rangle + \gamma \langle \lambda^k, \lmo(d^k)-x^k\rangle + \tfrac{L}{2}\gamma^2\|\lmo(d^k)-x^k\|^2\\
                &= f(x^k) + \gamma\langle -\lambda^k, u-x^k\rangle + \gamma \langle \nabla f(x^k), u-x^k\rangle + \gamma \langle \lambda^k, \lmo(d^k)-x^k\rangle + \tfrac{L}{2}\gamma^2\|\lmo(d^k)-x^k\|^2\\
                &= f(x^k) + \gamma \langle \nabla f(x^k), u-x^k\rangle + \gamma \langle \lambda^k, \lmo(d^k)-u\rangle + \tfrac{L}{2}\gamma^2\|\lmo(d^k)-x^k\|^2\\
                &\stackrel{\text{(b)}}{\leq} f(x^k) + \gamma \langle \nabla f(x^k), u-x^k\rangle + \gamma \langle \lambda^k, \lmo(d^k)-u\rangle + 2L\rho^2\gamma^2,
        \end{aligned}
    \end{equation*}
    using the optimality of $\lmo(d^k)$ for the linear minimization subproblem for (a) and the $2\rho$ upper bound on $\|\lmo(d^k)-x^k\|$ for (b).
    Rearranging and estimating we find, for all $k\in\{1,\ldots,n\}$, for all $u\in\mathcal{D}$,
    \begin{equation*}
        \begin{aligned}
            \gamma\langle \nabla f(x^k),x^k-u\rangle
                &\stackrel{\text{(a)}}{\leq} f(x^k) - f(x^{k+1}) + \gamma \| \lambda^k\|_2 \|\lmo(d^k)-u\|_2 + \tfrac{L}{2}\gamma^2\|\lmo(d^k)-x^k\|^2\\
                &\stackrel{\text{(b)}}{\leq} f(x^k) - f(x^{k+1}) + D_2 \gamma \| \lambda^k\|_2  + 2L\rho^2\gamma^2
        \end{aligned}
    \end{equation*}
    where we have used the Cauchy-Schwarz inequality in (a) and and bounded $\|\lmo(d^k)-x^k\|_2$ using the diameter of the set $\mathcal{D}$ with respect to the Euclidean norm, denoted $D_2$, in (b).
    Taking the expectation of both sides and applying Jensen's inequality we finally arrive, for all $k\in\{1,\ldots,n\}$, for all $u\in\mathcal{D}$,
    \begin{equation*}
        \begin{aligned}
            \gamma\mathbb{E}[\langle \nabla f(x^k),x^k-u\rangle]
                &\leq \mathbb{E}[f(x^k) - f(x^{k+1})] + D_2 \gamma \mathbb{E}[\| \lambda^k\|_2] + 2L\rho^2\gamma^2\\
                &\leq \mathbb{E}[f(x^k) - f(x^{k+1})] + D_2 \gamma \sqrt{\mathbb{E}[\| \lambda^k\|_2^2]} + 2L\rho^2\gamma^2.
        \end{aligned}
    \end{equation*}
\end{proof}

\subsubsection{\ref{eq:SCG} with constant $\alpha$}\label{subsec:SCGconstant}
\begin{lemma}\label{lem:SCGconstanterror}
    Suppose \Cref{asm:Lip,asm:stoch} hold. Let $n\in\mathbb{N}^*$ and consider the iterates $\{x^k\}_{k=1}^n$ generated by \Cref{alg:SCG} with constant stepsize $\gamma=\tfrac{1}{\sqrt{n}}$ and constant momentum $\alpha \in(0,1)$ with the exception of the first iteration, where we take $\alpha=1$. Then we have
    \begin{equation*}
        \mathbb{E}[\norm{\lambda^k}_2^2] \leq 4L_2^2D_2^2\frac{\gamma^2}{\alpha^2} + \left(2\alpha + \left(1-\frac{\alpha}{2}\right)^k\right)\sigma^2.
    \end{equation*}
\end{lemma}
\begin{proof}
    Under \Cref{asm:Lip,asm:stoch}, Lemma 1 in \citet{mokhtari2020stochastic} yields, after taking expectations, for all $k\in\{1,\ldots,n\}$
    \begin{equation*}
        \mathbb{E}[\| \lambda^{k+1}\|_2^2] \leq (1-\frac{\alpha_{k+1}}{2})\mathbb{E}[\| \lambda^k\|_2^2] + \sigma^2\alpha_{k+1}^2 + 2L_2^2D_2^2\frac{\gamma^2}{\alpha_{k+1}}.
    \end{equation*}
    Taking $\gamma$ and $\alpha$ to be constant we get
    \begin{equation*}
        \mathbb{E}[\| \lambda^{k+1}\|_2^2] \leq (1-\frac{\alpha}{2})\mathbb{E}[\| \lambda^k\|_2^2] + \sigma^2\alpha^2 + 2L_2^2D_2^2\frac{\gamma^2}{\alpha}.
    \end{equation*}
    Applying \Cref{lem:recursive_geometric} to the above with $u^k =\mathbb{E}[\| \lambda^{k+1}\|_2^2]$, $\beta = \frac{\alpha}{2}$, and $\eta = \sigma^2\alpha^2 + 2L_2^2D_2^2\frac{\gamma^2}{\alpha}$ we obtain
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[\norm{\lambda^{k}}_2^2]
                &\leq 2\alpha\sigma^2 + 4L_2^2D_2^2\frac{\gamma^2}{\alpha^2} + \left(1-\frac{\alpha}{2}\right)^k\mathbb{E}[\norm{\lambda^{1}}_2^2]\\
                &\leq 4L_2^2D_2^2\frac{\gamma^2}{\alpha^2} + \left(2\alpha + \left(1-\frac{\alpha}{2}\right)^k\right)\sigma^2
        \end{aligned}
    \end{equation*}
    with the final inequality following by the variance bound in \Cref{asm:stoch}.
\end{proof}

\end{toappendix}

These results show that, in the worst-case, running \Cref{alg:uSCG} with constant momentum $\alpha$ guarantees faster convergence but to a noise-dominated region with radius proportional to $\sigma$. In contrast, running \Cref{alg:uSCG} with vanishing momentum $\alpha_k$ is guaranteed to make the expected dual norm of the gradient small but at a slower rate. \Cref{alg:SCG} exhibits the analogous behavior, as we show next.

Before stating the results for \Cref{alg:SCG}, we emphasize that they are with \emph{constant} stepsize $\gamma$, which is atypical for conditional gradient methods. However, like most conditional gradient methods, we provide a convergence rate on the so-called Frank-Wolfe gap which measures criticality for the constrained optimization problem over $\mathcal{D}$. 

Finally, we remind the reader that the iterates of \Cref{alg:SCG} are always feasible for the set $\mathcal{D}$ by the design of the update and convexity of the norm ball $\mathcal{D}$.
\begin{lemmarep}[{Convergence rate for \ref{eq:SCG} with constant $\alpha$}]
    Suppose \Cref{asm:Lip,asm:stoch} hold. Let $n\in\mathbb{N}^*$ and consider the iterates $\{x^k\}_{k=1}^n$ generated by \Cref{alg:SCG} with constant stepsize $\gamma=\tfrac{1}{\sqrt{n}}$ and constant momentum $\alpha \in(0,1)$. Then, for all $u\in\mathcal{D}$, it holds that
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[\langle \nabla f(\bar{x}^n), \bar{x}^n-u\rangle] = O\left(\tfrac{L\rho^2}{\sqrt{n}} + \sigma\right).
        \end{aligned}
    \end{equation*}
\end{lemmarep}
\begin{appendixproof}
    Let $n\in\mathbb{N}^*$ and let $k\in\{1,\ldots,n\}$.
    By \Cref{asm:Lip}, we can invoke \Cref{lem:commondescent} to get, for all $k\in\{1,\ldots,n\}$, for all $u\in\mathcal{D}$,
    \begin{equation*}
        \gamma \mathbb{E}[\langle \nabla f(x^k), x^k-u\rangle]
            \leq \mathbb{E}[f(x^k) - f(x^{k+1})] + D_2\gamma \sqrt{\mathbb{E}[\| \lambda^k\|_2^2]} + 2L\rho^2\gamma^2.
    \end{equation*}
    Since \Cref{asm:stoch} holds, we can then invoke \Cref{lem:SCGconstanterror} and apply this to the above. This gives, for all $u\in\mathcal{D}$
    \begin{equation*}
        \begin{aligned}
            \gamma\mathbb{E}[\langle \nabla f(x^k),x^k-u\rangle]
                &\leq \mathbb{E}[f(x^k) - f(x^{k+1})] + 2L\rho^2\gamma^2 + D_2\gamma \sqrt{4L_2^2D_2^2\frac{\gamma^2}{\alpha^2} + \left(2\alpha + \left(1-\frac{\alpha}{2}\right)^k\right)\sigma^2}\\
                &\leq \mathbb{E}[f(x^k) - f(x^{k+1})] + 2L\rho^2\gamma^2 + 2L_2D_2^2\frac{\gamma^2}{\alpha} + D_2\gamma \left(\sqrt{2\alpha} + \left(\sqrt{1-\frac{\alpha}{2}}\right)^k\right)\sigma.
        \end{aligned}
    \end{equation*}
    Summing from $k=1$ to $n$ then dividing by $n\gamma$ we find, for all $u\in\mathcal{D}$,
    \begin{equation}\label{eq:SCGfinalineq}
        \begin{aligned}
            \mathbb{E}[\langle \nabla f(\bar{x}^n), \bar{x}^n-u\rangle]
                &=\frac{1}{n}\sum\limits_{k=1}^n\mathbb{E}[\langle \nabla f(x^k),x^k-u\rangle]\\
                &\stackrel{\text{(a)}}{\leq} \frac{\mathbb{E}[f(x^1) - f(x^{n+1})]}{\gamma n} + 2L\rho^2\gamma + 2L_2D_2^2\frac{\gamma}{\alpha} + D_2 \left(\sqrt{2\alpha} + \frac{1}{n}\sum\limits_{k=1}^n\left(\sqrt{1-\frac{\alpha}{2}}\right)^k\right)\sigma\\
                &\stackrel{\text{(b)}}{\leq} \frac{\mathbb{E}[f(x^1) - f(x^{n+1})]}{\gamma n} + 2L\rho^2\gamma + 2L_2D_2^2\frac{\gamma}{\alpha} + D_2 \left(\sqrt{2\alpha} + \frac{\sqrt{1-\frac{\alpha}{2}}}{n\left(1-\sqrt{1-\frac{\alpha}{2}}\right)}\right)\sigma\\
                &\stackrel{\text{(c)}}{\leq} \frac{\mathbb{E}[f(x^1) - \fmin]}{\gamma n} + 2L\rho^2\gamma + 2L_2D_2^2\frac{\gamma}{\alpha} + D_2 \left(\sqrt{2\alpha} + \frac{\sqrt{1-\frac{\alpha}{2}}}{n\left(1-\sqrt{1-\frac{\alpha}{2}}\right)}\right)\sigma,
        \end{aligned}
    \end{equation}
    applying the subadditivity of the square root for (a), geometric series due to $\sqrt{1-\frac{\alpha}{2}}\in (0,1)$ for (b), and the definition of $\fmin$ for (c).
    Taking $\gamma = \frac{1}{\sqrt{n}}$ then gives the final result, for all $u\in\mathcal{D}$,
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[\langle \nabla f(\bar{x}^n), \bar{x}^n-u\rangle]
                &\leq \frac{\mathbb{E}[f(x^1) - \fmin]}{\sqrt{n}} + \frac{2L\rho^2}{\sqrt{n}} + \frac{2L_2D_2^2}{\alpha\sqrt{n}} + D_2 \left(\sqrt{2\alpha} + \frac{\sqrt{1-\frac{\alpha}{2}}}{n\left(1-\sqrt{1-\frac{\alpha}{2}}\right)}\right)\sigma
                &= O\left(\frac{L\rho^2}{\sqrt{n}}+\sigma\right).
        \end{aligned}
    \end{equation*}
\end{appendixproof}

\begin{toappendix}
\subsubsection{\ref{eq:SCG} with vanishing $\alpha$}\label{subsec:SCGvanishing}
We now proceed to analyze the convergence of \Cref{alg:SCG} with vanishing $\alpha_k$.
The next lemma provides an estimation on the decay of the second moment of the noise $\lambda^k$.
\begin{lemma}[Bound on the gradient error with vanishing $\alpha$ \Cref{alg:SCG}]\label{lem:SCG_vanishing_error}
    Suppose \Cref{asm:Lip,asm:stoch} hold. Let $n\in\mathbb{N}^*$ and consider the iterates $\{x_{k}\}_{k=1}^n$ generated by \Cref{alg:SCG}
    with a constant stepsize $\gamma$ satisfying
    \begin{equation}
        \frac{1}{2 n^{3/4}}<\gamma <\frac{1}{n^{3/4}}.
    \end{equation}
    Moreover, consider vanishing momentum $\alpha_{k}= \frac{1}{\sqrt{k}}$. Then, for all $k\in\{1,\ldots,n\}$ the following holds
    \begin{equation}
            \mathbb{E}[\norm{\lambda^{k}}_{2}^{2}]\leq \frac{4\sigma^2+8L_2^2D_2^2}{\sqrt{k}}.
    \end{equation}
\end{lemma}
\begin{proof}
    Under \Cref{asm:Lip,asm:stoch}, we have the following recursion from Lemma 1 in \citet{mokhtari2020stochastic} after taking expectations, for all $k\in\mathbb{N}^*$,
    \begin{equation*}
        \mathbb{E}[\| \lambda^{k+1}\|_2^2] \leq (1-\frac{\alpha_{k+1}}{2})\mathbb{E}[\| \lambda^k\|_2^2] + \sigma^2\alpha_{k+1}^2 + 2L_2^2D_2^2\frac{\gamma^2}{\alpha_{k+1}}.
    \end{equation*}
    Comparing with the bound in \Cref{lem:uSCGerrorbound}, we see the only difference is the change of the constant $D_2^2$ by $\rho_2^2$. Repeating the argument in \Cref{lem:uSCGerrorbound}, the desired claim is directly obtained with $D_2^2$ in place of $\rho_2^2$, with the constant $Q = \max\{\mathbb{E}[\norm{\lambda^1}_2^2], 4\sigma^2+8L_2^2D_2^2\} \leq 4\sigma^2+8L_2^2D_2^2$ since $\mathcal{E}[\norm{\lambda^1}_2^2]\leq \sigma^2$ by \Cref{asm:stoch}.
\end{proof}

\end{toappendix}

\begin{lemmarep}[Convergence rate for \ref{eq:SCG} with vanishing $\alpha_k$]\label{lem:frankwolfe_rate}
    Suppose \Cref{asm:Lip,asm:stoch} hold. Let $n\in\mathbb{N}^*$ and consider the iterates $\{x^k\}_{k=1}^n$ generated by \Cref{alg:SCG} with a constant stepsize $\gamma$ satisfying $\tfrac{1}{2n^{3/4}}<\gamma<\tfrac{1}{n^{3/4}}$ and vanishing momentum $\alpha_k = \frac{1}{\sqrt{k}}$. Then, for all $u\in\mathcal{D}$, it holds that
    \begin{equation*}
        \mathbb{E}[\langle \nabla f(\bar{x}^n), \bar{x}^n-u\rangle] = O\left(\tfrac{1}{n^{1/4}} + \tfrac{L\rho^2}{n^{3/4}}\right).
    \end{equation*}
\end{lemmarep}
\begin{appendixproof}
    Let $n\in\mathbb{N}^*$ and $k\in\{1,\ldots,n\}$. By \Cref{asm:Lip}, we can invoke \Cref{lem:commondescent} to get,
    \begin{equation*}
        \begin{aligned}
            \gamma\mathbb{E}[\langle \nabla f(x^k),x^k-u\rangle]
                &\leq \mathbb{E}[f(x^k) - f(x^{k+1})] + D_2 \gamma \sqrt{\mathbb{E}[\| \lambda^k\|_2^2]} + 2L\rho^2\gamma^2.
        \end{aligned}
    \end{equation*}
    Applying the estimate given in \Cref{lem:SCG_vanishing_error} to the above we get
    \begin{equation*}
        \begin{aligned}
            \gamma\mathbb{E}[\langle \nabla f(x^k),x^k-u\rangle]
                &\leq \mathbb{E}[f(x^k) - f(x^{k+1})] + D_2 \gamma \sqrt{\frac{4\sigma^2+8L_2^2D_2^2}{\sqrt{k}}} + 2L\rho^2\gamma^2\\
                &= \mathbb{E}[f(x^k) - f(x^{k+1})] + D_2 \sqrt{4\sigma^2+8L_2^2D_2^2} \gamma \frac{1}{k^{1/4}} + 2L\rho^2\gamma^2.
        \end{aligned}
    \end{equation*}
    Summing from $k=1$ to $n$ and then dividing by $n\gamma$ we find, for all $u\in\mathcal{D}$,
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[\langle \nabla f(\bar{x}^n),\bar{x}^n-u\rangle]
                &= \frac{1}{n}\sum\limits_{k=1}^n\mathbb{E}[\langle \nabla f(x^k),x^k-u\rangle]\\
                &\stackrel{\text{(a)}}{\leq} \frac{\mathbb{E}[f(x^1) - f(x^{n+1})]}{n\gamma} + \frac{D_2\sqrt{4\sigma^2+8L_2^2D_2^2}}{n}\sum\limits_{k=1}^n\frac{1}{k^{1/4}} + 2L\rho^2\gamma\\
                &\stackrel{\text{(b)}}{\leq} \frac{\mathbb{E}[f(x^1) - f(x^{n+1})]}{n\gamma} + \frac{4D_2\sqrt{4\sigma^2+8L_2^2D_2^2}n^{3/4}}{3n} + 2L\rho^2\gamma\\
                &= \frac{\mathbb{E}[f(x^1) - f(x^{n+1})]}{n\gamma} + \frac{4D_2\sqrt{4\sigma^2+8L_2^2D_2^2}}{3n^{1/4}} + 2L\rho^2\gamma,
        \end{aligned}
    \end{equation*}
    using division by $\gamma n$ for (a) and the integral test with decreasing function $x\mapsto \frac{1}{x^{1/4}}$ for (b).
    Using the definition of $\fmin$ and estimating $n\gamma > \tfrac{n^{1/4}}{2}$ and $\gamma < \frac{1}{n^{3/4}}$ gives
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[\langle \nabla f(\bar{x}^n),\bar{x}^n-u\rangle]
                &\leq \frac{2\mathbb{E}[f(x^1) - \fmin]}{n^{1/4}} + \frac{4D_2\sqrt{4\sigma^2+8L_2^2D_2^2}}{3n^{1/4}} + \frac{2L\rho^2}{n^{3/4}}\\
                &= O\left(\frac{1}{n^{1/4}} + \frac{L\rho^2}{n^{3/4}}\right).
        \end{aligned}
    \end{equation*}
\end{appendixproof}
\begin{insightbox}[label={insight:convergence}]
For both algorithms, our worst-case analyses for constant momentum suggest that tuning $\alpha$ requires balancing two effects. Making $\alpha$ smaller helps eliminate a constant term that is proportional to the noise level $\sigma$. However, if $\alpha$ becomes too small, it amplifies an $O(1/\sqrt{n})$ term and an $O(\sigma/n)$ term. The stepsize $\gamma$ must also align with the choice of momentum $\alpha$; for vanishing $\alpha_k$ the theory suggests a smaller constant stepsize like $\gamma=\tfrac{3}{4(n^{3/4})}$ to ensure convergence.
\end{insightbox}
\begin{toappendix}

\subsection{Averaged LMO Directional Descent (ALMOND)}\label{subsec:almond}
In this section we present a variation on \Cref{alg:uSCG} that computes the $\lmo$ directly on the stochastic gradient oracle and then does averaging. This is in contrast to how we have presented \Cref{alg:uSCG} which first does averaging (aka momentum) with the stochastic gradient oracle and then computes the $\lmo$. 
A special case of this algorithm is the Normalized SGD based algorithm of \citet{zhao2020stochastic} when the set $\mathcal{D}$ is with respect to the Euclidean norm. 
In contrast with \Cref{alg:uSCG}, the method relies on large batches, since the noise is not controlled by the momentum parameter $\alpha$ due to the bias introduced by the $\lmo$.

\begin{algorithm}
\caption{Averaged LMO directioNal Descent (ALMOND)}
\label{alg:ALMOND}
\textbf{Input:} Horizon $n$, initialization $x^1 \in \mathcal X$, $d^0 = 0$, momentum $\alpha \in (0,1)$, stepsize $\gamma \in (0,1)$
\begin{algorithmic}[1]
    \For{$k = 1, \dots, n$}
        \State Sample $\xi_{k}\sim \mathcal P$
        \State $d^{k} \gets \alpha \lmo(\nabla f(x^{k}, \xi_{k})) + (1 - \alpha)d^{k-1}$
        \State $x^{k+1} \gets x^k + \gamma d^k$
    \EndFor
    \State Choose $\bar{x}^n$ uniformly at random from $\{x^1, \dots, x^n\}$
    \item[\algfont{Return}] $\bar{x}^n$
\end{algorithmic}
\end{algorithm}

\begin{lemmarep}
    Suppose \Cref{asm:Lip,asm:stoch} hold. Let $n\in\mathbb{N}^*$ and consider the iterates $\{x_k\}_{k=1}^n$ generated by \Cref{alg:ALMOND} with stepsize $\gamma = \frac{1}{\sqrt{n}}$. Then, it holds
    \begin{equation*}
        \mathbb{E}[\norm{\nabla f(\bar{x}^n)}_{\ast}] \leq \frac{\mathbb{E}[f(x^1)-\fmin]}{\rho\sqrt{n}} + \frac{L(1-\alpha)\rho}{\alpha\sqrt{n}} + \frac{L\rho}{2\sqrt{n}} + 2\mu\sigma = O\left(\tfrac{1}{\sqrt{n}}\right) + 2\mu\sigma
    \end{equation*}
    where\footnote{Alternatively, instead of invoking the constant $\mu$ we could make an assumption that the gradient oracle has bounded variance measured in the norm $\norm{\cdot}_{\ast}$.} $\mu = \max\limits_{x\in\mathcal{X}}\frac{\norm{x}_\ast}{\norm{x}_{2}}$.
\end{lemmarep}
\begin{proof}
    Let $n\in\mathbb{N}^*$ and denote $z^{k} = \tfrac{1}{\alpha}x^k-\tfrac{1-\alpha}{\alpha}x^{k-1}$ with the convention that $x_0 = x_1$ so that $z_1 = x_1$ and, for all $k\in\{1,\ldots,n\}$,
    \begin{equation*}
        \begin{aligned}
            z^{k+1} - z^k
                &= \frac{1}{\alpha}x^{k+1}-\frac{1-\alpha}{\alpha}x^{k}-\frac{1}{\alpha}x^{k}+\frac{1-\alpha}{\alpha}x^{k-1}= \frac{1}{\alpha}\left(\gamma d^{k} - \gamma (1-\alpha)d^{k-1}\right)= \gamma\lmo(g^k).
        \end{aligned}
    \end{equation*}
    Applying the descent lemma for $f$ at the points $z^{k+1}$ and $z^k$ gives
    \begin{equation}\label{eq:nsgd_descent1}
        \begin{aligned}
            f(z^{k+1})
                &\leq f(z^{k}) + \langle \nabla f(z^k), z^{k+1}-z^k\rangle +\frac{L}{2}\norm{z^{k+1}-z^k}^2\\
                &= f(z^{k}) + \gamma\langle \nabla f(z^k), \lmo(g^k)\rangle +\frac{L\gamma^2}{2}\norm{\lmo(g^k)}^2\\
                &= f(z^{k}) + \gamma\left(\langle \nabla f(z^k)-\nabla f(x^k), \lmo(g^k)\rangle + \langle \nabla f(x^k) - g^k,\lmo(g^k)\rangle +\langle g^k,\lmo(g^k)\rangle\right) +\frac{L\gamma^2}{2}\norm{\lmo(g^k)}^2\\
                &= f(z^{k}) + \gamma\left(\langle \nabla f(z^k)-\nabla f(x^k), \lmo(g^k)\rangle + \langle \nabla f(x^k) - g^k,\lmo(g^k)\rangle -\rho\norm{g^k}_{\ast}\right) +\frac{L\gamma^2}{2}\norm{\lmo(g^k)}^2\\
                &\stackrel{\text{(a)}}{\leq} f(z^{k}) + \gamma\left(\left(\norm{\nabla f(z^k)-\nabla f(x^k)}_{\ast} + \norm{\nabla f(x^k) - g^k}_{\ast}\right)\norm{\lmo(g^k)} -\rho\norm{g^k}_{\ast}\right) +\frac{L\gamma^2}{2}\norm{\lmo(g^k)}^2\\
                &\stackrel{\text{(b)}}{\leq} f(z^{k}) + \gamma\left(\rho\left(\norm{\nabla f(z^k)-\nabla f(x^k)}_{\ast} + \norm{\nabla f(x^k) - g^k}_{\ast}\right) -\rho\norm{g^k}_{\ast}\right) +\frac{L\rho^2\gamma^2}{2}\\
                &\stackrel{\text{(c)}}{\leq} f(z^{k}) + \gamma\left(\rho\left(L\norm{z^k-x^k} + \norm{\nabla f(x^k) - g^k}_{\ast}\right) -\rho\norm{g^k}_{\ast}\right) +\frac{L\rho^2\gamma^2}{2},
        \end{aligned}
    \end{equation}
    applying H\"{o}lder's inequality with norm $\norm{\cdot}_{\ast}$ for (a), the radius $\rho$ of $\mathcal{D}$ for (b), and \Cref{asm:Lip} for (c).
    We note that
    \begin{equation*}
        x^{k+1}-x^{k} = \gamma d^k = \gamma\left((1-\alpha) d^{k-1}+\alpha\lmo(g^k)\right) = \alpha\gamma \lmo(g^k) + (1-\alpha)\gamma\left(\frac{x^k-x^{k-1}}{\gamma}\right)=\alpha\gamma\lmo(g^k)+(1-\alpha)(x^{k}-x^{k-1})
    \end{equation*}
    which we can use to bound
    \begin{equation*}
        \norm{x^{k}-x^{k-1}} \leq (1-\alpha)\norm{x^k-x^{k-1}} + \alpha\gamma\norm{\lmo(g^k)} \leq (1-\alpha)\norm{x^k-x^{k-1}} + \alpha\rho\gamma \leq \frac{\alpha\rho\gamma}{(1-\alpha)}.
    \end{equation*}
    We then have
    \begin{equation*}
        \norm{z^k-x^k} = \frac{(1-\alpha)}{\alpha}\norm{x^k-x^{k-1}}\leq \frac{(1-\alpha)\rho\gamma}{\alpha}
    \end{equation*}
    by using the definition of the update and the $\lmo$, which can be plugged into \eqref{eq:nsgd_descent1} to get
    \begin{equation}
        \begin{aligned}
            \rho\gamma\norm{g^k}_{\ast}
                &\leq f(z^k) - f(z^{k+1}) + \gamma\rho\left(L\norm{z^k-x^k} + \norm{\nabla f(x^k)-g^k}_{\ast}\right) + \frac{L\rho^2\gamma^2}{2}\\
            \implies \norm{g^k}_{\ast}
                &\stackrel{\text{(a)}}{\leq} \frac{f(z^k)-f(z^{k+1})}{\rho\gamma} + L\norm{z^k-x^k} + \norm{\nabla f(x^k)-g^k}_{\ast} + \frac{L\rho\gamma}{2}\\
                &\stackrel{\text{(b)}}{\leq} \frac{f(z^k)-f(z^{k+1})}{\rho\gamma} + \frac{L(1-\alpha)\rho\gamma}{\alpha} + \norm{\nabla f(x^k)-g^k}_{\ast} + \frac{L\rho\gamma}{2}\\
            \implies \norm{\nabla f(x^k)}_{\ast}
                &\stackrel{\text{(c)}}{\leq} \frac{(f(z^k)-f(z^{k+1})}{\rho\gamma} + \frac{L(1-\alpha)\rho\gamma}{\alpha} + 2\norm{\nabla f(x^k)-g^k}_{\ast} + \frac{L\rho\gamma}{2}
        \end{aligned}
    \end{equation}
    where (a) is the result of dividing both sides by $\rho\gamma$, (b) is the result of bounding $\norm{z^k-x^k}$, and (c) follows by the reverse triangle inequality after adding and subtracting $\nabla f(x^k)$ in the norm on the left hand side.
    Taking expectations, using \Cref{asm:stoch} and the constant $\mu = \max\limits_{x\in\mathcal{X}}\frac{\norm{x}_{\ast}}{\norm{x}_2}$, it holds
    \begin{equation*}
        \mathbb{E}[\norm{\nabla f(x^k)-g^k}_{\ast}]\leq \mu\mathbb{E}[\norm{\nabla f(x^k)-g^k}_{2}]\leq \mu\sqrt{\mathbb{E}[\norm{\nabla f(x^k)-g^k}_{2}^2]}\leq \mu\sigma
    \end{equation*}
    which we can sum from $k=1$ to $n$ to obtain
    \begin{equation*}
        \sum\limits_{k=1}^n\mathbb{E}[\norm{\nabla f(x^k)}_{\ast}] \leq \frac{\mathbb{E}[f(z^0)-f(z^{n+1})]}{\rho\gamma} + \frac{nL(1-\alpha)\rho\gamma}{\alpha} + 2n\mu\sigma + \frac{nL\rho\gamma}{2}.
    \end{equation*}
    Diving both sides by $n$ and then plugging in $\gamma = \frac{1}{\sqrt{n}}$ yields the desired final result.
\end{proof}

\subsection{Linear recursive inequalities}
We now present two elementary lemmas that establish bounds for linear recursive inequalities. These results are essential for analyzing the convergence behavior of our stochastic gradient estimator, particularly when examining the error term $\mathbb{E}[\norm{\lambda^k}_2^2]$.
\begin{lemma}[Linear recursive inequality with constant coefficients]\label{lem:recursive_geometric}
    Let $n>1$ and consider $\{u_k\}_{k=1}^n\in\mathbb{R}_+^n$ a sequence of nonnegative real numbers satisfying, for all $k\in\{2,\ldots,n\}$,
    \begin{equation*}
        u^k\leq (1-\beta) u^{k-1} + \eta
    \end{equation*}
    with $\eta>0$ and $\beta\in(0,1)$.
    Then, for all $k\in\{2,\ldots,n\}$, it holds
    \begin{equation*}
        u^k\leq \frac{\eta}{\beta} + (1-\beta)^ku^1.
    \end{equation*}
\end{lemma}
\begin{proof}
    We prove the claim by induction on $k$. For the base case $k=2$ we find
    \begin{equation*}
        u^2 \leq (1-\beta)u^1 + \eta \leq \frac{\eta}{\beta} + (1-\beta)u^1
    \end{equation*}
    since $\beta<1$.
    Assume now for some $k\in\{2,\ldots,n\}$ that the claim holds. Then, by the assumed recursive inequality on $\{u_i\}_{i=1}^n$, we have
    \begin{equation*}
        u^{k+1} \leq (1-\beta)u^k + \eta \leq (1-\beta)\left(\frac{\eta}{\beta} + (1-\beta)^ku^1\right) + \eta = (1-\beta)^{k+1}u^1 + \left(\frac{1-\beta}{\beta} + 1\right)\eta = (1-\beta)^{k+1}u^1 + \frac{\eta}{\beta}
    \end{equation*}
    and thus the desired claim holds by induction.
\end{proof}

The first lemma establishes a geometric decay bound for sequences with constant momentum. The following lemma extends this analysis to the case of variable coefficients, which we will use when we analyze \Cref{alg:uSCG} and \Cref{alg:SCG} with vanishing momentum $\alpha_k$.

\begin{lemma}[Linear recursive inequality with vanishing coefficients]\label{lem:recursivevanishing}   
    Let $\{u^k\}_{k\in\mathbb{N}^*}$ be a sequence of nonnegative real numbers satisfying, for all $k\in\mathbb{N}^*$, the following recursive inequality
    \begin{equation*}
        u^k\leq \left(1-\frac{1}{2\sqrt{k}}\right)u^{k-1} + \frac{c}{k}
    \end{equation*}
    where $c>0$ is constant.
    Then, the sequence $\{u^k\}_{k\in\mathbb{N}^*}$ satisfies, for all $k\in\mathbb{N}^*$,
    \begin{equation*}
        u^k \leq \frac{Q}{\sqrt{k}}
    \end{equation*}
    with $Q=\max\{u^1, 4c\}$.
\end{lemma}
\begin{proof}
    We prove the claim by induction. For $k=1$ the inequality holds by the definition of $Q$, since
    \begin{equation*}
        u^1 \leq Q = \frac{Q}{\sqrt{1}}.
    \end{equation*}
    Let $k>1$ and assume that
    \begin{equation*}
        u^{k-1}\leq\frac{Q}{\sqrt{k-1}}.
    \end{equation*}
    Then, by the assumed recursive inequality for $u^k$, we have
    \begin{equation}\label{eq:recursive_ineq2}
        \begin{aligned}
            u^{k}
                &\leq \left(1-\frac{1}{2\sqrt{k}}\right)u^{k-1} + \frac{c}{k}\\
                &\leq \left(1-\frac{1}{2\sqrt{k}}\right)\frac{Q}{\sqrt{k-1}} + \frac{c}{k}.
        \end{aligned}
    \end{equation}
    Since $k>1$, we can estimate
    \begin{equation*}
        \frac{1}{\sqrt{k-1}} = \frac{\sqrt{k}}{\sqrt{k(k-1)}} = \frac{1}{\sqrt{k}}\sqrt{\frac{k}{k-1}} = \frac{1}{\sqrt{k}}\sqrt{1 + \frac{1}{k-1}} \leq \frac{1}{\sqrt{k}}\left(1 + \frac{1}{2(k-1)}\right)
    \end{equation*}
    which, when applied to \eqref{eq:recursive_ineq2}, gives
    \begin{equation}\label{eq:recursive_ineq3}
        u^k\leq \left(1-\frac{1}{2\sqrt{k}}\right)\left(1+\frac{1}{2(k-1)}\right)\frac{Q}{\sqrt{k}} + \frac{c}{k}.
    \end{equation}
    Furthermore, as $k>1$, we also have
    \begin{equation*}
        \left(1-\frac{1}{2\sqrt{k}}\right)\left(1+\frac{1}{2(k-1)}\right)\leq \left(1-\frac{1}{4\sqrt{k}}\right).
    \end{equation*}
    Applying the above to \eqref{eq:recursive_ineq3} gives
    \begin{equation*}
        \begin{aligned}
            u^k
                &\leq \left(1-\frac{1}{4\sqrt{k}}\right)\frac{Q}{\sqrt{k}}+\frac{c}{k}\\
                &= \frac{Q}{\sqrt{k}} + \frac{c-Q/4}{k}\\
                &\leq \frac{Q}{\sqrt{k}}
        \end{aligned}
    \end{equation*}
    with the last inequality following since $Q\geq 4c$.
    The desired claim is therefore obtained by induction.
\end{proof}

\end{toappendix}
