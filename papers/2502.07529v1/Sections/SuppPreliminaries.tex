\subsection{Relationship between steepest descent and \ref{eq:uSCG}}\label{sec:fenchel}
There are two prominent families of norm-based non-Euclidean method, namely the ones based on the $\lmo$ and the ones based on the sharp operator, both of which can be expressed in the terms of the Fenchel conjugate.
The Fenchel conjugate of a proper, convex, and lower semicontinuous function $h: \mathcal X \to \R \cup \{\infty\}$ is defined as:

\begin{equation*}
h^*(s) = \sup_{x \in \mathcal X} \left\{ \langle s, x \rangle - h(x) \right\},
\end{equation*}
where $s \in \mathcal X$. 
The subdifferential $\partial h^*$ is equivalent to the argmax of the conjugate operation, i.e.,
\begin{equation*}
\partial h^*(s) = \operatorname*{argmax}_{x \in \mathcal X} \left\{ \langle s, x \rangle - h(x) \right\}.
\end{equation*}
This follows from the Fenchel-Young inequality (see e.g. \citet{bauschke2012fenchel}).

\paragraph{LMO}
The $\lmo$ is a special case when $h$ is an indicator function of a convex set, i.e.,
\begin{equation*}
\begin{split}
&\lmo(s) = \partial h^*(-s) \\
&\quad \text{with} \quad
h(x)=\iota_{\mathcal D}(x):=\begin{cases}0 &x \in \mathcal D \\ +\infty & \mathrm{otherwise}\end{cases}
\end{split}
\end{equation*}
The $\lmo$ is commonly used for constrained minimization in e.g., \ref{eq:CG} since the operator ensure feasibility on the constrained set $\mathcal D$.

\paragraph{Sharp operator}
Another important example is the sharp operator \citep{nesterov2012efficiency,kelner2014almost} defined as
\begin{equation*}
s^\sharp \in \argmax_{x \in \mathcal X} \{ \braket{s,x} - \tfrac{1}{2}\|x\|^2 \}
\end{equation*}
for some norm $\|\cdot\|$, which can equivalently be written as
\begin{equation*}
s^\sharp \in \partial h^*(s)
\quad \text{with} \quad
h(x)=\tfrac{1}{2}\|x\|^2
\end{equation*}

The sharp operator and $\lmo$ can be defined in terms of each other when $\mathcal D := \{ x \mid \|x\| \leq \rho \}$, specifically
\begin{equation}\label{eq:lmo:sharp}
s^\sharp = -\tfrac{1}{\rho}\|s\|_*\lmo(s)
\end{equation}
From \eqref{eq:lmo:sharp} we see a clear distinction between the $\lmo$ and the sharp operator, namely that, while the $\lmo$ is scale invariant (i.e. $\lmo(a\cdot s)=\lmo(s)$ for $a>0$) the sharp operator is not (since $[a\cdot s]^\sharp=a[s]^\sharp$ for $a\in \R$).

\paragraph{Steepest descent}
Steepest descent in a normed space can be written in terms of the sharp operator as follows
\begin{equation*}
x^{k+1} = x^k - \gamma [\nabla f(x^k)]^\sharp
\end{equation*}
with a stepsize $\gamma > 0$.

