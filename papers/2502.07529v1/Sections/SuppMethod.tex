\subsection{Input radius scaling}\label{app:input-radius}

Based on the spectral norm perspective \citep{yang2023spectral}, which requires that $\|W_\ell\|_{\mathcal{S}_\infty}=\Theta (\sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}})$, one might be inclined to pick the initialization such that $\|W_\ell\|_{\mathcal{S}_\infty}=\sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}}$ is ensured exactly.
This argument is indeed valid asymptotically, since input dimension is kept fixed.
However, when the input dimension is larger than the output dimension, this does not lead to constant preactivations as demonstrated through a coordinate check \citep{yang2021tensor} carried out in \Cref{fig:init:coordinate-check}.

Kaiming initialization \citep{he2015delving} fortunately circumvents this problem.
From random matrix theory we have that $\|A\|_{\mathcal{S}_\infty}\approx \sigma (\sqrt{d_\mathrm{in}} + \sqrt{d_\mathrm{out}})$ for $A_{ij} \sim \mathcal N(0,\sigma^2)$ \citep{vershynin2018high}.
So the Kaiming initialization, $[W_\ell]_{ij} \sim N(0,\nicefrac{1}{d_\mathrm{in}})$, leads to $\|W_\ell\|_{\mathcal{S}_\infty}\approx 1 + \sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}}$, which prevents the preactivation from going to zero as $d_\mathrm{out} \rightarrow 0$.
Alternatively, one can simply choose $\|W_\ell\|_{\mathcal{S}_\infty}=\max(1,\sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}})$.

Ensuring a correct norm scaling is particularly important for \ref{eq:SCG} and \ref{eq:uSCG}, since the scaling not only affects initialization but also the update rule itself.
Specifically, if the methods were run with the norm bound choice $\|W_\ell\|_{\mathcal{S}_\infty}\leq \sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}}$ for the input layer, then the issue in \Cref{fig:init:coordinate-check} persists, due the $\lmo$ always lying on the boundary of the norm ball.
The choice $\|W_\ell\|_{\mathcal{S}_\infty}=\max(1,\sqrt{\nicefrac{d_\mathrm{out}}{d_\mathrm{in}}})$ resolves this issue.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figs/MLP_init_coord.pdf}
\caption{Coordinate check at initialization. Preactivations are not constant with the spectral scaling $\sqrt{\tfrac{d_\mathrm{out}}{d_\mathrm{in}}}$, when $d_\mathrm{in}>d_\mathrm{out}$.}
\label{fig:init:coordinate-check}
\end{figure}


\subsection{Alternative norm choices} 

\begin{table*}[h]
\centering
\caption{
  It is possible to use the same norm throughout the network if scaled appropriately.
  This provides an alternative to the sign update in \Cref{tbl:lmo}.
  By not treating the network as a flattened vector, hyperparameter can transfer across model sizes (cf. \Cref{fig:GPT:shakespeare:sign}).
  We have made use of \Cref{lem:operator:bound} and \eqref{eq:vectornorm:bounds} to derive the correct layerwise scaling.
}
\label{tbl:parameter:lmo:same-norm}
\bgroup
\def\arraystretch{1.2}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{Weight norm (bias norm) }
    & $W_1$ (1-hot encoded) 
    & $W_1$ (image domain) 
    & $(W_\ell)_{\ell \in [2,...,L-1]}$
    & $W_L$
    & $b_\ell$ \\
\hline
\hline
\multirow{ 2}{*}{$1 \rightarrow \infty$ ($\infty$)}
  & $\lmo$
  & $\sign(W_1)$
  & \multicolumn{3}{c|}{$\tfrac{1}{d_\mathrm{in}}\sign(W_\ell)$}
  & $\sign(b_\ell)$
\\
  & Init.
  & Random sign
  & \multicolumn{3}{c|}{$\tfrac{1}{d_\mathrm{in}}\times$ Random sign}
  & $0$ \\
\hline
\end{tabular}
\egroup
\end{table*}

\subsection{Boundary initialization} 

\paragraph{Semi-orthogonal}
Following \citet{saxe2013exact}, perform QR decomposition of a random matrix
\begin{equation*}
\begin{split}
G_{ij} &\sim \mathcal{N}(0, 1), \quad \forall i, j \\
G &= QR
\end{split}
\end{equation*}
Use \(Q'=Q\sign(\diag(R))\) as the semi-orthogonal matrix as the initialization.

\paragraph{Column-wise normalized Gaussian}
As proposed in \citet{large2024scalable}, initialize each column as follows
\begin{equation*}
\begin{split}
W_{ij} &\sim \mathcal{N}(0, 1), \quad \forall i, j \\
\operatorname{col}_i(W) &= \frac{\operatorname{col}_i(W)}{\|\operatorname{col}_i(W)\|_2}, \quad \forall i
\end{split}
\end{equation*}

\paragraph{Row-wise normalized Gaussian} Initialize each row as follows
\begin{equation*}
\begin{split}
W_{ij} &\sim \mathcal{N}(0, 1), \quad \forall i, j \\
\operatorname{row}_j(W) &= \frac{\operatorname{row}_j(W)}{\|\operatorname{row}_j(W)\|_2}, \quad \forall j
\end{split}
\end{equation*}

\paragraph{Random sign}
\[
W_{ij} = 
\begin{cases} 
+1 & \text{with probability } 0.5 \\
-1 & \text{with probability } 0.5
\end{cases} 
\quad \forall i, j
\]

Each initialization should be scaled by the corresponding scaling of the $\lmo$ elementwise.

