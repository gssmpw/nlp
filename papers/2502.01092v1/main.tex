\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need 

\IEEEoverridecommandlockouts                              % This command is only needed if 
\overrideIEEEmargins                                      % Needed to meet printer requirements.
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
%\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{nameref}
\usepackage{hyperref}
\usepackage{epsfig} % for postscript
\usepackage{mathtools}
\usepackage{amssymb}  % assumes amsmath package installe
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{todonotes}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hhline}
% \usepackage{caption,subcaption}
\newcommand\colorcomment[1]{\textcolor{blue}{#1}}
\newcommand\red[1]{\textcolor{red}{#1}}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{ctable} % for \specialrule command
\usepackage{booktabs,ragged2e}
\usepackage[english]{babel}
\usepackage{gensymb}
\usepackage{hhline}
\usepackage{hyperref}
\usepackage{amssymb} % for \longrightarrow
\usepackage[T1]{fontenc} % for \symbol{92} 
\usepackage{mathtools} % for 'bsmallmatrix' environment

\usepackage{kotex}


% \newtheorem{theorem}{Theorem}
% \newtheorem{lemma}{Lemma}
% \newtheorem{corollary}{Corollary}
% \newtheorem{proposition}{Proposition}
% \let\proof\relax
% \let\endproof\relax
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}
\newtheorem*{running}{Running Example}
\newtheorem{remark}{Remark}

\renewcommand{\figureautorefname}{Fig.}
\renewcommand{\tableautorefname}{Table }
\def\equationautorefname~#1\null{(#1)\null}
\def\sectionautorefname~#1\null{Section #1\null}
\def\subsectionautorefname~#1\null{Section #1\null}
\def\algorithmautorefname~#1\null{Algorithm #1\null}
\def\theoremautorefname~#1\null{Theorem #1\null}
\def\lemmaautorefname~#1\null{Lemma #1\null}
\def\remarkautorefname~#1\null{Remark #1\null}
\def\definitionautorefname~#1\null{Definition #1\null}


%%%%
% *** IEEE Copyright notice with TikZ ***
\usepackage{tikz}

\newcommand\copyrighttext{%
  \footnotesize \textcopyright 2025 IEEE.  Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.}
  %DOI: \href{<http://tex.stackexchange.com>}{<DOI No.>}}
\newcommand\copyrightnotice{%
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south,yshift=10pt] at (current page.south) {\fbox{\parbox{\dimexpr\textwidth-\fboxsep-\fboxrule\relax}{\copyrighttext}}};
\end{tikzpicture}%
}
%%%%

\begin{document}

\title{\LARGE \bf
Enhancing Feature Tracking Reliability for Visual Navigation\\using Real-Time Safety Filter
% Real-Time Safety Filter for Reliable Feature Tracking\\for Visual Navigation
}
\author{Dabin Kim$^{*}$, Inkyu Jang$^{*}$, Youngsoo Han, Sunwoo Hwang, and H. Jin Kim
\thanks{$^{*}$ The first two authors contributed equally to this work.}
\thanks{This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-2024-00436984). The work by Inkyu Jang was partially supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (RS-2024-00407121).}
\thanks{The authors are with the Department of Aerospace Engineering, Seoul National University, Seoul 08826, South Korea (e-mail: dabin404@snu.ac.kr, janginkyu.larr@gmail.com, swsw0411@snu.ac.kr, cat7945@snu.ac.kr, hjinkim@snu.ac.kr, corresponding author: H. Jin Kim).}
}



\maketitle
\copyrightnotice
\thispagestyle{empty}
\pagestyle{empty}
\vspace{-3.5mm}

\begin{abstract}
% [AFTER]
Vision sensors are extensively used for localizing a robot's pose, particularly in environments where global localization tools such as GPS or motion capture systems are unavailable. In many visual navigation systems, localization is achieved by detecting and tracking visual features or landmarks, which provide information about the sensor's relative pose. For reliable feature tracking and accurate pose estimation, it is crucial to maintain visibility of a sufficient number of features.
This requirement can sometimes conflict with the robot's overall task objective. In this paper, we approach it as a constrained control problem. By leveraging the invariance properties of visibility constraints within the robot's kinematic model, we propose a real-time safety filter based on quadratic programming. This filter takes a reference velocity command as input and produces a modified velocity that minimally deviates from the reference while ensuring the information score from the currently visible features remains above a user-specified threshold.
Numerical simulations demonstrate that the proposed safety filter preserves the invariance condition and ensures the visibility of more features than the required minimum. We also validated its real-world performance by integrating it into a visual simultaneous localization and mapping (SLAM) algorithm, where it maintained high estimation quality in challenging environments, outperforming a simple tracking controller.

% [BEFORE]
% Vision sensors are widely used in many robotics applications for localizing the robot's pose in environments where global localization tool such as motion capture system is not available.
% In many state-of-the-art visual navigation systems, it is typically done by detecting and tracking visual features or landmarks and then gathering the information provided about the relative pose of the sensor.
% To enable reliable feature tracking and pose estimation, it is undoubtedly important to maintain visibility to a sufficient number of features.
% While this may sometimes conflict with the robot's overall objective, in this paper, we view this as a constrained control problem.
% Leveraging the invariance property of the visibility constraints under the robot kinematics model, we construct a real-time safety filter based on quadratic programming. It takes in the reference velocity command and gives a modified velocity that minimally deviates from the reference while guaranteeing the information score provided by the currently-visible features is always above the user-specified minimum.
% Numerical simulations show that the proposed safety filter maintains the invariance condition and ensures visibility of more than the required number of features. We also validated its real-world performance by applying the proposed filter to a visual simultaneous localization and mapping algorithm, where it preserved estimation quality in harsh environments, unlike a simple tracking controller.
% \red{experiment와 validation을 어떻게 했는지 서술}

\end{abstract}


\section{Introduction}

Vision sensors are widely used for self-localization in mobile robots. Visual Odometry (VO) and Visual Simultaneous Localization and Mapping (V-SLAM) are extensively researched in both the computer vision and robotics fields. State-of-the-art visual pose estimation algorithms, such as ORB-SLAM \cite{mur2017orb} and VINS-Mono \cite{qin2018vins}, have proven to be highly effective.

While most research has traditionally focused on improving the accuracy and robustness of visual estimation using available image data, recent studies have begun examining the impact of image data quality on the performance of vision-based localization algorithms. The quality of image data is often influenced by the camera's trajectory. For example, when a camera follows a trajectory that captures texture-less surfaces (e.g., plain walls), the accuracy of VO may decrease due to the lack of sufficient visual features. This challenge has sparked a growing interest in perception-aware planning and control, where the camera trajectory is adjusted to ensure high-quality visual data for reliable localization. This idea has led to several studies that integrate visual estimation considerations into advanced motion planning and control strategies.



\begin{figure}[t]
    \centering
    \subfloat[]{
    \includegraphics[width=0.9\linewidth]{figure/exp_result_proposed_low_resol2-min.png}}
    \\ \vspace{-2mm}
    \subfloat[]{
    \includegraphics[width=0.9\linewidth]{figure/exp_result_baseline_low_resol2-min.png}}
    \caption{The result of experiments with (a) the proposed safety filter and (b) the baseline controller. For each experiment, the robots at three timestamps (A,B,C) are visualized along with their corresponding on-board images, with features visualized in ORB-SLAM2. The detected features are represented with green dots. The proposed safety filter adaptively  adjusts the control input to maintain sufficient tracking features, as demonstrated by the camera heading (arrow) and onboard image at timestamp B. 
     % 헤딩과 온보드 이미지 부분의 문장구조가 이상한데요 여전히 !!
     In contrast, the baseline controller struggles with texture-poor surfaces, where fewer features are trackable. A detailed explanation and analysis are provided in \autoref{sec:eval}.}   
    \label{fig:exp_proposed}
\end{figure}



Belief-space planning and optimal control methods \cite{indelman2015planning} \cite{van2011lqg}, which rely on explicit state estimation and uncertainty modeling, are often unsuitable for real-time control in modern V-SLAM systems due to the high computational load of tasks such as  bundle adjustment involving hundreds of landmarks. To address this limitation, recent research has explored incorporating information directly extracted from image inputs, such as feature points, into the planning and control loop. These algorithms typically frame the problem as a multi-objective optimization, where perception requirements become an additional objective for the controller.
However, this multi-objective approach introduces significant challenges. Balancing path optimality with maintaining sufficient visual information is difficult, and comparing Pareto-optimal solutions to find the best trade-off adds complexity. Additionally, prioritizing visual information can often degrade trajectory quality, potentially leading to numerical instability when the multiple objectives are in conflict.


% \begin{figure*}
%     \centering
%     \subfloat[]{
%     \includegraphics[width=0.48\linewidth]{figure/exp_result_proposed_low_resol.png}}
%     \hspace*{2}
%     \subfloat[]{
%     \includegraphics[width=0.48\linewidth]{figure/exp_result_baseline_low_resol.png}}
%     \caption{The result of experiment with (a) the proposed safety filter and (b) the baseline controller. For each experiment, robots at three timestamps are visualized with its corresponding on-board image with feature visualized in ORB-SLAM. THe newly detected features are represented with blue dots and features matched from previous frame are visualized in green dots. }
%     \label{fig:exp_proposed}
% \end{figure*}

In this paper, rather than adopting a multi-objective optimization approach, we address the problem from a safety-critical control perspective. Instead of prioritizing the optimization of the estimation algorithm's performance, our focus is on ensuring sufficient visual information for stable localization by framing it as a constraint in the optimization process. This approach is based on the observation that once a sufficient amount of information is obtained, adding more does not significantly enhance estimation performance.
% \textcolor{red}{Instead, in this paper, We propsoe that this problem can be treated in safety-critical control persepctive. The requirement for the estimation algorithm is keep enough amount of visual information, not compromising path optimality to optimize estimation algorithm.} 
% \textcolor{red}{If there are enough features measured, adding more features does not drastically improve the estimation performance. }
% \textcolor{blue}{Find any resource that support this claim. Marginal covariance decrease of redundant visual information is not important?} 

% Jacquet et al. similary used MPC scheme for perception-aware controller and treat the feature visibility requirements as constraints. However, they do not have recursive feasibility guarantee. 

Building on the theory of safety filters which ensure the forward invariance of a safety set, we developed a visibility maintenance condition designed to guarantee a sufficient number of visual features within an image, despite a limited field of view. From these visibility maintenance conditions, we formulate a quadratic program (QP)-based safety filter that guarantees feasibility and supports real-time computation. Unlike previous works on visibility-constrained control such as \cite{kim2023visibility}, which focused on maintaining fixed targets within a limited field of view, our approach accommodates newly observed or lost features dynamically. To the best of the authors' knowledge, this is the first attempt to develop a safety filter for perception-aware control in visual navigation that guarantees feasibility. We conduct numerical evaluations to validate that the designed safety filter can maintain a user-defined threshold for the minimum number of visual features. Additionally, experimental results are presented to demonstrate the efficacy of the proposed method using real-world data and a visual estimator.



% Based on the theory of safety filter which guarantee forward invariance of safety set, we developed a visiblity maintenance condition which is designed for maintaining enough number of visual features in the image with limited field-of-view. From the conditions for visibility maintenance, we formulate a quadratic program(QP)-based safety filter which has feasibility guarantee and able for real-time computation. Unlike previous visibility constrained control works such as \cite{kim2023visibility} which tried to maintain fixed targets in the limited FoV, our work is able to consider newly observed features or lost during the mission by re-initialization of parameters. 


% addressed the CBF-like condition for velocity filtering and visibility of features in FoV, and since during run-time new features might enter the FoV and some features might go outside the FoV. We treat this as parameterized CBF condition \cite{jang}, and develop the invariance condition. Then, we formulate the filtering problem as QP problem, which can be computed in real-time.

% We propose a safety-filter method to guarantee forward invariance in the condition of visible features witht the limited Field-of-view of camera.

% From image input, the previous approaches to handle this issue is to use 

% Previous approach includes multi-objective optimization in trajectory optimization or model predictive control. 
% \textcolor{red}{Why we need safety control?}

% But, it is hard to figure out which pareto optimal solution is desirable. And also, it is more important to avoid risk but we do not need \textit{optimal} feature visibility. 

% CBF-based control with real-time computation is desirable for feedback control where trajectory optimization is open-loop, and for safety   , we sometimes need run-time safety. 

% Model predictive control, only uses forward invariance 

% Vision-based safety have been used with invariance-based control method such as control barrier function (CBF) \cite{ames2019control}, however it have been only used for collision avoidance \cite{} with depth image, or . The difficulty in designing invariance-based safety control

% \begin{itemize}
%     \item Constraint 
%     \item Run-time safety of controller 
% \end{itemize}


\section{Related Work}
\subsection{Planning and Control for Reliable Visual Navigation}


The importance of acquiring rich keypoints in visual navigation has been well recognized since the early days of visual SLAM research \cite{davison2002simultaneous}. With advancements in path planning and control, there has been a growing focus on integrating visual navigation capabilities into these algorithms.


Belief Space Planning (BSP) addresses state uncertainty by modeling it as a probability distribution, allowing objectives like collision avoidance to be treated as chance constraints. For instance, estimators with explicit uncertainty representation, such as the Extended Kalman Filter (EKF), have been combined with Rapidly-exploring Random Trees (RRT) to manage uncertainty in path planning \cite{bry2011rapidly}. 
% Similarly, Linear-Quadratic Gaussian (LQG) control can be employed to handle collision constraints \cite{van2011lqg}. 
In vision-based systems, computationally intensive techniques such as bundle adjustment are used to estimate future states. Integrating these methods into the core of planning and control algorithms is often impractical for real-time control due to their high computational cost \cite{achtelik2014motion} \cite{indelman2015planning}. To mitigate this computational burden, metrics like the Fisher Information Matrix \cite{zhang2019beyond} or the observability Gramian \cite{hausman2017observability} are employed as proxies for explicit uncertainty computation and are used as objective functions in optimal control algorithms.
% , though they still can't be directly applied to image inputs.


In contrast, vision-based heuristics are designed to assess visual navigation quality efficiently, often derived directly from pixel measurements. For instance, point visibility can be constrained in controller design \cite{falanga2018pampc} \cite{kim2023visibility}, effective for fixed landmarks but unsuitable for dynamic  feature tracking. Metrics like the number of visible features \cite{kim2021topology} and co-visible features \cite{chen2024apace} approximate visual estimation quality and integrate into trajectory optimization. Our work aligns closely with this, operating directly in image space. However, instead of balancing multiple objectives in trajectory optimization, we use a safety-critical control method that formulates reliable feature tracking as a constrained control problem.
The resulting QP-based safety filter provides safe control with feasibility guarantees under mild assumptions, offering a more interpretable and numerically efficient real-time control method compared to multi-objective non-convex trajectory optimization approaches.


% Additionally, Indelman et al.  investigated the use of model predictive control in planning under uncertainty, highlighting the computational challenges these methods entail. However, BSP's reliance on probabilistic models to represent state uncertainty tends to limit its application to specific types of estimators and it often requires heavy computation.
% The importance of acquiring richful salient keypoints on visual navigation is understood from the early development of visual SLAM \cite{davison2002simultaneous}.  With development of efficient path planning and control methods, the studies have focused on integrating the visual navigation components into the developed planning and control algorithms. And this topic is stuided as subset of active SLAM \cite{placed2023survey}, but in this case we only consider uncertain state estimation, not building map. Belief space planning (BSP) handles uncertain state in probability distribution, met the planning objective such as collision avoidance as chance constraints. The aspect of BSP is it requires probability distrubition to model the state uncertainty, which narrows the domain of BSP in specific types of estimators such as EKF \cite{van2011lqg} or require heavy computation. For global planning \cite{bry2011rapidly} used EKF with RRT, \cite{achtelik2014motion} run bundle adjustment to acquire future state estimation which requires large computation thus only used for offline planning. \cite{indelman2015planning} model predictive control. 

% Our work proposes the method to 

% \cite{bonzanini2024perception} perception-aware mpc
% \cite{wu2022perception}
% \cite{chen2024apace} apace, co-visibility of features maximization with smoothness. 

\subsection{Safety-Critical Control with Perception Objective}


The growing focus on autonomous navigation has increased the need for safety with formal guarantees like collision avoidance. Safety-critical control techniques including Hamilton-Jacobi reachability and control barrier functions (CBFs) have been developed to address these needs. These techniques now increasingly integrate perception systems for broader applications.

To handle state uncertainty, measurement-robust CBFs \cite{cosner2021measurement} and observer-controller co-designs \cite{agrawal2022safe} address safety under estimation errors. In vision-based systems, the challenge of high-dimensional image inputs is tackled by learning-based CBFs trained on RGB-D images to avoid collisions with arbitrarily shaped objects \cite{abdi2023safe}, and BarrierNet \cite{xiao2023barriernet}, which uses differentiable CBFs for tasks like end-to-end driving. CBFs have also been designed for environmental representations such as point clouds from depth cameras \cite{de2024point} and neural radiance fields \cite{tong2023enforcing} for collision avoidance.

Beyond collision avoidance, safety-critical control can also be used to maintain the visibility of points of interest, which is essential in applications like visual servoing \cite{zheng2019toward} \cite{wei2024diffocclusion} and teleoperation \cite{kim2023visibility}, ensuring that targets remain within the camera's field of view. We extend this concept by developing a safety filter for reliable visual navigation, enforcing a sufficient number of visible image features.
% As a new application for a safety-critical control, we develop a safety filter tailored for reliable visual navigation, with safety requirements based designed to ensure a sufficient number of visible image features.


% Consider uncertainty of state-estimate 
% \cite{dean2020robust} robust guarnatee on perception-based control
% \cite{agrawal2022safe} observer and controller synthesis.

% \cite{de2024point} pointcloud-based cbf 
% \cite{xiao2023barriernet} barrier net, learnable differential cbf, high-dimensional image input applied to such as end-to-end driving scenario. 
% \cite{tong2023enforcing} nerf with cbf, collision avoidance in nerf representation of scene. 


% \section{Preliminary}
\section{Real-Time Constraint Satisfaction using QP-Based Safety Filter}

% \subsection{PCBF \red{CBF라는 말을 쓰는게 괜찮을지 모르겠음}}

Assume a continuous-time time invariant nonlinear system model
\begin{equation}
    \dot{x} = f(x) + g(x) u,
\end{equation}
where $x \in \mathbb{R}^n$ is the state, $u\in U \subseteq \mathbb{R}^m$ is the input. The set $U$ is assumed to be a convex polytope in the $\mathbb{R}^m$ space, i.e., there exist a matrix $A_u$ and a vector $b_u$ with appropriate sizes such that $U = \{u\in \mathbb{R}^m : A_u u \leq b_u\}$.
We assume that for every reachable state $x$ in the state space, there exists an input $u\in U$ (possibly as a function of $x$) such that $f(x) + g(x) u = 0$. This $u$ is called the \textit{stopping input} that brings the system to an instantaneous complete stop. 
Relaxing this sudden stop assumption is an interesting future research direction which will be discussed in the conclusion section.

Given a set of allowed states $C \subseteq \mathbb{R}^{n}$, we want to generate a feedback law $u(t,x) \in U$ as a function of time $t$ and state $x$, such that the system permanently resides in $C$, i.e., $C$ is forward invariant.
Suppose there exist a finite number of continuously differentiable functions $h_i:\mathbb{R}^n \rightarrow \mathbb{R}$, $\forall i \in I$, where $I$ is the index set, such that $C = \{x\in \mathbb{R}^n : h_i(x) \geq 0\}$, and $\partial_x h_i(x) \neq 0$ if $h_i(x) = 0$.
Then, Nagumo's theorem \cite[Section 4.2]{blanchini2008set} tells that if $u$ is chosen such that
\begin{equation} \label{eq:nagumo thm}
    h_i(x) = 0 \rightarrow \partial_x h_i (x) \cdot \left(f(x) + g(x) u\right) \geq 0, \; \forall i \in I,
\end{equation}
then the set $C$ will be rendered forward invariant.

In order to build a feedback control strategy that best tracks the given reference input while satisfying \autoref{eq:nagumo thm} and avoiding discontinuity, one can let $u(t,x)$ be the optimal solution to the following quadratic program:
\begin{equation} \label{eq:velocity filter}
\begin{aligned}
    \min_{u \in \mathbb{R}^m} \; & (u - u_\mathrm{ref}(t,x))^\top R(u - u_\mathrm{ref}(t,x)) \\
    \mathrm{s.t.} \; & \dot{h}_i(x, u) \geq -\alpha_i h_i(x), \; \forall i \in I \\
    & A_u u \leq b_u,
\end{aligned}
\end{equation}
where $R\in \mathbb{R}^{m\times m}$ is a symmetric positive definite weight matrix, $\dot{h}_i(x,u) = \partial_x h_i(x) \cdot (f(x) + g(x)u)$ is the time derivative of $h_i(x)$ given state $x$ and input $u$, $\alpha_i$-s are positive reals.
Note that given a strictly positive definite $R$, \autoref{eq:velocity filter} admits a unique solution which satisfies \autoref{eq:nagumo thm}. Additionally, the optimization is feasible given $x \in C$, since the stopping input is one feasible solution to the optimization.
Thus, \autoref{eq:velocity filter} can be called a \textit{safety filter} \cite{wabersich2023data, hsu2023safety} in the sense that it selects the input from the set of safe inputs $U_{f} = \{u : \text{$u$ is a feasible solution to \autoref{eq:velocity filter}}\}$ that minimally deviates from the reference.
This approach can be regarded as a special case of CBF-based quadratic programs (CBF-QP) \cite{ames2016control} where the instantaneous brakability assumption allows to employ multiple CBFs in a set-intersection manner without suffering from \textit{leaking corner} issues \cite{lee2019removing}.

\section{Visibility Maintenance using Safety Filter}
% \subsection{}


% \subsection{\red{Visibility Maintenance (?)} through Velocity Filtering}

\subsection{Differential Kinematics}
Let the robot's configuration space be $Q \subseteq \mathbb{R}^n$. We model the robot's differential kinematics as follows:
\begin{equation}
    \dot{q} = J(q)v, \label{eq:kinematics}
\end{equation}
where $J(q):Q\rightarrow \mathbb{R}^{n\times m}$ is the Jacobian matrix which we assume continuous with respect to $q$, $v \in V \subseteq \mathbb{R}^m$ is the input, and $V$ is the input constraint set which is assumed to be a convex polytope such that $V \ni 0$ (i.e., it satisfies the instantaneous brakability condition).


We consider a vision sensor attached rigidly to the robot frame (so that its pose $T \in SE(3)$ is given as a continuously differentiable function of the robot's configuration $q$), which is capable of detecting positions of point landmarks within its region of detection. To elaborate, let $p$ be the position of a landmark seen in the vision sensor frame. The condition for the landmark to be seen from the sensor can be written as $\rho(p) \geq 0$, where $\rho:\mathbb{R}^3 \rightarrow \mathbb{R}^d$ is the continuous and differentiable constraint function.
% , \textcolor{red}{and $\geq$ inequality represents element-wise relation}.
% which we assume continuously differentiable. <== 예외상황 


The motion of the sensor frame can be written using its body linear and angular velocities (i.e., twist). If we assume the landmarks are fixed to the world frame (i.e., stationary), the landmark position with respect to the sensor $p$ is controllable by the body twist and follows the dynamics
\begin{equation} \label{eq: p dynamics}
    \dot{p} = -\omega_s \times p - v_s,
\end{equation}
where $\omega_s \in \mathbb{R}^3$ and $v_s\in \mathbb{R}^3$ are the sensor's body angular and linear velocities, and $\times$ is the vector cross product in 3D.
Since $T$ is a continuously differentiable function of $q$, we can then write $\omega_s$ and $v_s$ as a linear function of the control input $v$ as follows:
\begin{equation} \label{eq: input transform}
    \omega_s = J_\omega(q) v, \;\; v_s = J_v(q) v,
\end{equation}
where $J_\omega$ and $J_v$ are matrix-valued continuous functions of $q$ with appropriate sizes.

\begin{figure}
    \centering
    \hspace{1mm}
    \subfloat[]{
        \includegraphics[width=0.45\linewidth]{figure/running_example_kinematics.png}
        
    }
    \subfloat[]{
        \includegraphics[width=0.4\linewidth]{figure/lambda-rho.png}
    }
    \caption{(a) The robot configuration and the onboard camera's field of view for the running example. The camera is mounted on the two dimensional ground robot, captures any landmark within its field of view, represented by the light green region. The field of view is defined by the angle of view, $\psi$, and the sensing range, $R$.
    (b) A graphical illustration of equivalence between \autoref{eq: lambda nonnegativity} and \autoref{eq: lambda-rho}. The gray region shows where \autoref{eq: lambda nonnegativity} is violated. For a fixed $\mu \in [0,1]$, the feasible region of \autoref{eq: lambda-rho} forms a half-space with the origin on its boundary. The union of all possible half-spaces aligns with the feasible set of \autoref{eq: lambda nonnegativity}, represented by the white region.}
    \label{fig: running example}
\end{figure}

\begin{running}[Ground robot]
    In this section, as a running example, we consider the following ground robot (\autoref{fig: running example}) with a camera attached on it:
    \begin{equation}
        \dot{q} = \frac{d}{dt}\begin{bmatrix}
            q_x \\ q_y \\ q_\theta
        \end{bmatrix} = v = \begin{bmatrix}
            v_x \\ v_y \\ v_\theta
        \end{bmatrix},
    \end{equation}
    where $q_x$, $q_y$ are the horizontal and vertical positions of the robot base, $q_\theta$ is the orientation (rotation angle with respect to the $x$ axis) of the camera, $v_x$, $v_y$, $v_\theta$ are control inputs.
    It is straightforward to find that the landmark point kinematics \autoref{eq: p dynamics} can be written as
    \begin{equation}
        \dot{p} = \frac{d}{dt}\begin{bmatrix}
            p_x \\ p_y
        \end{bmatrix} = \begin{bmatrix}
            -\cos q_\theta & -\sin q_\theta & p_y \\
            \sin q_\theta & -\cos q_\theta & -p_x
        \end{bmatrix} \begin{bmatrix}
            v_x \\ v_y \\ v_\theta
        \end{bmatrix}.
    \end{equation}
    Here, $p_x$ and $p_y$ are the horizontal and vertical positions of the landmark seen from the camera frame, respectively.
\end{running}

\subsection{Visibility Maintenance} \label{sec:vis_maintenance}
Let $L$ be the set of $N$ landmarks in three-dimensional space, where the position of each landmark $l \in L$ relative to the vision sensor is $p_l(q)$. A landmark $l$ is visible if $\rho(p_l(q)) \geq 0$, and the set of visible landmarks is denoted as $L(q) := \{l \in L : \rho(p_l(q)) \geq 0\}$.
% Now suppose there are $N$ landmarks in the three-dimensional space, let $L$ ($|L| = N$) be the set of landmarks. The position of landmark $l \in L$ with respect to the vision sensor frame is written as $p_l$, which is a function of $q$. The landmark $l$ is seen from the sensor if $\rho(p_l(q)) \geq 0$. Hereafter, we write the set of visible landmarks using the notation $L(q) := \{l \in L : \rho(p_l(q)) \geq 0\}$.

\begin{running}[Ground robot, continued]
    We assume that the onboard camera can detect landmarks within the field of view as shown in \autoref{fig: running example} (a). Given the field of view, the $\rho$ function can be defined as follows:
    \begin{equation}
        \rho(p) = \begin{bmatrix}
            [\sin \psi/2, \cos \psi/2]^\top p \\
            [\sin \psi/2, -\cos \psi/2]^\top p \\
            R - \lVert p\rVert_2
        \end{bmatrix}
    \end{equation}
    where $\psi$ is the angle of view, $R$ is the sensing range. In this example, we let $R = 1$, $\psi = 1\text{ rad}$.
\end{running}

Given all above, we want the robot's motion to satisfy  the following constraint:
\begin{equation}
    w(q) := \sum_{l \in L(q)} w_l \geq W,
\end{equation}
where $w_{(\cdot)} \geq 0$ are the weights, {$w(q)$ represents the overall landmark tracking quality score, $w_{l}$ denotes the contribution of landmark $l$ to this quality, and $W$ is the minimum score the robot is required to achieve throughout the mission.}
% An intuitive interpretation of the quantities $w(q)$, $w_l$, and $W$ are $w(q)$ is the overall landmark tracking quality score, $w_l$ is the contribution to landmark $l$ to the tracking quality, and $W$ is the minimum score we want the robot to observe throughout the mission. <--- are /is?? 문장이 이상해요 
For example, if the goal is to maintain visibility to at least $M$ landmarks, we can let $w_l = 1$ for all $l\in L$ (so that $w(q)$ counts the number of currently visible landmarks), and $W$ any positive number between $M-1$ and $M$.

Unfortunately, since $w(q)$ is not everywhere differentiable, it is impossible to construct a QP-based safety filter like \autoref{eq:velocity filter}. Instead, following the idea of \cite{jang2023invariance}, we introduce an auxiliary state variable $\lambda = [\lambda_1, \cdots, \lambda_N]^\top \in \mathbb{R}^N$ and a \textit{smoothened} version of score constraint
\begin{equation}
    W \leq \hat{w}(q, \lambda) = \sum_{l \in L} \lambda_l w_l \leq w(q).
\end{equation}
The first inequality $W \leq \hat{w}$ is continuously differentiable with respect to $\lambda$ and has no dependency on $q$. For the second inequality $\hat{w}\leq w$ to hold, we require
\begin{equation}
    \lambda_l \leq \mathbf{1}_{l\in L(q)}(q)
\end{equation}
where $\mathbf{1}_{\phi(\xi)}(\xi)$ is the indicator function that returns $1$ if the statement $\phi(\xi)$ is true and $0$ otherwise.
This can be rewritten as the intersection of two constraints for every $l \in L$: 
\begin{align}
    \lambda_l &\leq 1, \label{eq: lambda <= 1} \\
    \lambda_l > 0 \; &\rightarrow \; \rho(p_l(q)) \geq 0 \label{eq: lambda nonnegativity}.
\end{align}
The first inequality \autoref{eq: lambda <= 1} is continuously differentiable, and Eq. \autoref{eq: lambda nonnegativity} is equivalent to  
\begin{equation}
    \lambda_l \leq 0 \,\, \vee \,\, \rho(p_l(q)) \geq 0,
\end{equation}
which holds if and only if
\begin{equation} \label{eq: lambda-rho}
    \exists \mu_l \in [0,1], \quad -\mu_l \lambda_l + (1 - \mu_l) \rho(p_l(q)) \geq 0.
\end{equation}
The proof for this claim is straightforward and hence omitted in this paper. A graphical explanation to this can be found in \autoref{fig: running example} (b).
% \begin{figure}
%     \centering
%     \includegraphics[width=0.35\linewidth]{figure/lambda-rho.png}
%     \caption{A graphical illustration of equivalence between \autoref{eq: lambda nonnegativity} and \autoref{eq: lambda-rho}. The gray shaded region denotes where \autoref{eq: lambda nonnegativity} does not hold. For a fixed $\mu \in [0,1]$, the feasible region of \autoref{eq: lambda-rho} is a halfspace that contains the origin on the boundary, and the union of all possible halfspaces coincides with the feasible set of \autoref{eq: lambda nonnegativity}, the white region.}
%     \label{fig: mu lambda}
% \end{figure}
To combine everything into a continuously differentiable setting, we let $\mu = [\mu_1, \cdots, \mu_N]^\top \in \mathbb{R}^N$ be another auxiliary state variable of the system. The auxiliary state $\lambda$ and $\mu$ are \textit{controlled} through the dynamics
\begin{equation} \label{eq: mu lambda dynamics}
    \dot{\lambda} = v_\lambda, \quad \dot{\mu} = v_\mu,
\end{equation}
where $v_\lambda, v_\mu \in \mathbb{R}^{N}$ are virtual inputs that are numerically integrated to obtain the actual $\lambda$ and $\mu$ values.

\subsection{QP-based Safety Filter}

In summary, we have the following set of state constraints
\begin{equation} \label{eq: final state constraints}
\begin{aligned}
    h_1(x) &= \sum_{l \in L}\lambda_l w_l - W \geq 0, \\
    h_{2,l}(x) &= 1 - \lambda_l \geq 0, \;\forall l \in L, \\
    h_{3,l}(x) &= -\mu_l \lambda_l + (1 - \mu_l)\rho(p_l(q)) \geq 0, \;\forall l \in L, \\
    h_{4,l}(x) &= \mu_l \geq 0, \; \forall l \in L, \\
    h_{5,l}(x) &= 1 - \mu_l \geq 0, \; \forall l \in L, \\
    h_6(x) &= c(q),
\end{aligned}
\end{equation}
with $x = (q, \lambda, \mu) \in \mathbb{R}^{n + 2N}$ being the augmented state. The constraints $h_1$ through $h_{5,l}$ are derived from \autoref{sec:vis_maintenance}, and $h_6$ is the collision avoidance constraint encoded as a function of robot state $c(q)$ which we assume continuously differentiable. For example, one can model the robot as a sphere and let $c(q) = s(q) - r$, where $s(q)$ is the (signed) distance from the robot to the nearest obstacle, $r>0$ is the radius of the robot.

These state constraints are continuously differentiable with respect to the augmented state $x$ and a safety filter in the form \autoref{eq:velocity filter} can be implemented:
\begin{equation} \label{eq: final safety filter}
\begin{aligned}
    \min_{u \in U} \quad & (u - u_\mathrm{ref}(t,x))^\top R (u - u_\mathrm{ref}(t,x)) \\
    \mathrm{s.t.} \quad & \dot{h}_{(\cdot)}(x,u) \geq -\alpha_{(\cdot)} \cdot h_{(\cdot)}(x)
\end{aligned}
\end{equation}
where the inequality constraint should be satisfied for all $h_{(\cdot)}$ functions in \autoref{eq: final state constraints}.
Here, we let $u=(v, v_\lambda, v_\mu) \in U = V \times \mathbb{R}^N \times \mathbb{R}^N \subseteq \mathbb{R}^{m + 2N}$ be the augmented input and $u_\mathrm{ref}(t, x) = (v_\mathrm{ref}(t, q), 0, 0)$, $R = \mathrm{blkdiag}(R_q, k_\lambda \mathrm{1}_N, k_\mu \mathrm{1}_N)$ where $v_\mathrm{ref}$ is the reference input given from the higher-level decision maker (e.g., manual control from a human operator or a motion planning algorithm), $R_q \in \mathbb{R}^{m\times m}$ is the symmetric positive definite input cost matrix, $k_\lambda$ and $k_\mu$ are positive (usually very small) weights to ensure the safety filter has a unique solution, $\mathrm{1}_N$ is the identity matrix of size $N\times N$.

The time derivatives of $h_{(\cdot)}$ can be expressed as a linear function of $u$, thus the safety filter \autoref{eq: final safety filter} is a QP.
The $\lambda$ and $\mu$ values should be calculated by integrating \autoref{eq: mu lambda dynamics}.
In case $p_l(q)$ and its derivative are not straightforward to directly compute, one can evaluate it in an indirect manner by forward integrating to get the following initial value ODE, which follows directly from \autoref{eq: p dynamics} and \autoref{eq: input transform}:
\begin{equation}
\begin{aligned}
    \dot{p}(t) &= -J_\omega(q(t)) v(t) \times p(t) - J_v(q(t)) v(t), \\ p(t_0) &= p_l(q(t_0))
\end{aligned}
\end{equation}
where $p_l(q(t_0))$ is the position of the landmark in sensor frames when initially observed at time $t_0$, and $p$ is a shorthand for $p_l \circ q$.
It can be seen that if $h_{(\cdot)}(x) \geq 0$, the optimization \autoref{eq: final safety filter} is feasible since $u=0$ makes $\dot{h}_{(\cdot)} = 0$ and thus is one feasible solution.

\begin{running}(Ground robot, continued)
    In the running example, the mission of the robot is to track the reference input, while maintaining visibility to at least 5 landmarks. Thus, we let $W = 4.5$, $w_l=1$.
    We place $N=30$ landmarks at random positions near the origin. The reference input is given as
    \begin{equation}
        v_\mathrm{ref}(t,q) = \begin{bmatrix}
            -\sin t + 2 (\cos t - q_x) \\
            \cos t + 2 (\sin t - q_y) \\
            0
        \end{bmatrix}
    \end{equation}
    which makes the robot to track a circular trajectory around the origin at speed $1$.
    In this example, we ignore collision avoidance constraints ($h_6$ in \autoref{eq: final state constraints}).
    Other parameter values are set as follows: $\alpha_{(\cdot)} = 1$, $V = [-2, 2]\times [-2, 2] \times [-1, 1]$, $R = \mathrm{diag}(1, 1, 0.001)$, $k_\lambda = k_\mu = 0.001$. The third component of $R$ corresponding to $v_\theta$ is set to a small number to allow the camera to rotate accordingly to keep the landmarks in sight.
\end{running}

\subsection{Initialization and Handling Limited Observability}

To take advantage of the nonnegativity-preserving property of the safety filter, it is very important to initialize the safety filter with a valid augmented state $x$ so that $h_{(\cdot)} \geq 0$.
Suppose the robot starts at an initial condition $q(t_0)$ such that $w(q(t_0)) \geq W$ and $c(q(t_0)) \geq 0$. This means that the robot has view to a sufficient amount of information from the landmarks and is at a collision-free position. Then, one can easily find that the initialization
$\lambda_l(t_0) = \mathbf{1}_{l\in L(q(t_0))}(l)$, $\mu_l(t_0) = \mathbf{1}_{l\notin L(q(t_0))}(l)$ is a valid choice that $h_{(\cdot)}(x) \geq 0$.

In real deployment, the landmark positions $p_l$ are typically not \textit{a priori} known and the robot can observe only the currently visible landmark positions at discrete times.
Suppose that the observations occur at times $t_0 < t_1 < \cdots$. Whenever $t=t_i$ for an integer $i \geq 0$, the robot newly observes $p_l$ such that $l\in L_i = L(q(t_i))$. During the interval $t \in \left[t_i, t_{i+1}\right)$ for every $i \in \{0,1,\cdots\}$, we require the robot to run the safety filter with $L_i$ instead of $L$, with re-initialization $\lambda_l(t_i) = \mathbf{1}_{l\in L_i}(l) = 1$, $\mu_l(t_0) = \mathbf{1}_{l\notin L_i}(l) = 0$. This will introduce a sudden jump in the $h_{(\cdot)}$ values at time $t=t_i$, however, here we show that the jumps preserve nonnegativity of $h_{(\cdot)}$ values.
Let $x_i^- = (q_i, \lambda_i^-, \mu_i^-)$ and $x_i^+ = (q_i, \lambda_i^+, \mu_i^+)$ be the augmented states before and after the jump at time $t_i$ ($i \in \{1,2,\cdots\}$), respectively. Note that the robot configuration $q$ does not jump. Suppose $h_{(\cdot)}(x_i^-) \geq 0$.
Firstly, $h_1$ value always jump \textit{upwards}, i.e., $h_1(x_i^+) \geq h_1(x_i^-) \geq 0$, since $h_1(x_i^-) \leq w(q_i) - W$ (the nonnegativity of $h_{(\cdot)}(x_i^-)$ ensures this), and $h_1(x_i^+) = \sum_{l\in L_i} w_l - W = w(q_i) - W$.
Next, it is straightforward to find the values of $h_{2,l}$ through $h_{5,l}$ are nonnegative after the update.
Finally, $h_6$ only depends on $q$ which does not jump, therefore $h_6(x_i^+)=h_6(x_i^-)\geq 0$.

In summary, if the initial conditions are such that $w(q(t_0)) \geq W$ and $c(q(t_0)) \geq 0$, then the robot will start from a feasible condition that satisfies \autoref{eq: final state constraints}. 
% (<-- start 두번 등장.  앞부분은 if the initial conditions satisfy... 인가요??) 
Moreover, for every $i \in \{0,1,\cdots\}$, the safety filter will ensure the constraint satisfaction throughout $t \in [t_i, t_{i+1})$, and the jump at $t=t_{i+1}$ will set the $h_{(\cdot)}$ value to a nonnegative one, after which the safety filter can resume from a feasible initial condition.

\begin{figure}
    \centering
    \vspace{2mm}
    \includegraphics[width=0.97\linewidth]{figure/sim_result.png}
    % \vspace{-2mm}
    \caption{Simulation result for the running example. (left) The resulting trajectory of the robot. The position trajectory ($q_x, q_y$) is depicted as red curve, the camera poses are drawn using blue triangles. It can be seen that the robot takes a path that differs from the reference (shown in the left top corner) to maintain visibility to at least 5 landmarks. (right) The time history of $w(q)$, $\hat{w}(q,\lambda)$ values. The relation $W \leq \hat{w}(q,\lambda) \leq w(q)$ holds throughout the simulation.}
    \label{fig: running example simulation result}
\end{figure}
\begin{running}[Ground robot, simulation result]
    With the abovementioned setting and initialization, we simulated the safety filter using MATLAB. The results are depicted in \autoref{fig: running example simulation result}. In the results, it can be clearly seen that the $\hat{w}$ value always lower bounds the number of visible landmarks $w$ and lower bounded by $W$, resulting in $w(q) \geq 5$ throughout the simulation. The robot takes the path that provides visibility to at least 5 landmarks, while minimally deviating from the reference input given. Note that the re-initialization introduces sudden jumps in the $\hat{w}(q, \lambda)$ values, but only in a way that the state constraints are satisfied.
\end{running}

% and the robot starts at the initial condition $q(t_0)$ such that $w(q(t_0)) \geq W$ and $c(q(t_0)) \geq 0$.




% At time $t_0$, the robot starts with only the initially seen landmark positions which we denote by $L_0=L(q(t_0))$. That is, for all $l \in L_0$, $\rho(p_l(q(t_0))) \geq 0$.
% A feasible initialization for $h_{(\cdot)}(x(t_0)) \geq 0$ is 

% Thus, employing the initialization $\lambda_l(t_0) = 1$, $\mu_l(t_0) = 0$ and running \autoref{eq: final safety filter} with $L=L_0$ will ensure $h_{(\cdot)} \geq 0$ throughout the interval $t \in \left[t_0, t_1\right)$.

% At time $t_1$, as soon as the robot is provided $L_1$ (the set of visible landmarks at $t_1$), it switches to the new $L=L_1$, re-initialize with $\lambda_l(t_1)=1$, $\mu_l(t_1)=0$, and the safety filter is run until time $t_2$. The $h_{(\cdot})$ values would then experience a sudden jump at $t=t_1$. We will show that this jump always preserve nonnegativity, i.e., $h_{(\cdot)}(x(t_1^+)) \geq 0$, where $x(t_1^+)$ denotes the augmented state at time $t_1$ after the update.
% It is easy to see $h_{2,l}$ through $h_{5,l}$ are nonnegative after the jump. For $h_{1}$, 


\section{Evaluation} \label{sec:eval}

% In order to validate the proposed safety filter in feature tracking, we designed a simulation and an experiment. 

% We do not need a map for vision-based safety filter. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/scheme_3.png}
    \caption{The diagram illustrated for the control structure for vision-based robot with the proposed safety filter.}
    \label{fig:scheme}
\end{figure}

% CBF-QP to Velocity Tracking controller 

% Hardware design

% Feature selection, limited number. $N_{f,max}=50$ Based on ORB score. 


% \subsection{Camera Measurement Model}

% In this paper, we assume the use of indirect V-SLAM methods, which extract sparse features from images and generate sparse point cloud maps, as opposed to dense methods. Additionally, we assume that the camera provides depth information for each keypoint, with stereo cameras or RGB-D cameras being common examples.

% Given the camera's rotation \( R \in SO(3) \) and translation \( \mathbf{t} \in \mathbb{R}^{3} \), and a 3D point located at \( \mathbf{t}_{i} \in \mathbb{R}^{3} \), the position of the landmark in the camera frame is expressed as \( \mathbf{t}^{c} = R \mathbf{t}_{i} + \mathbf{t} \).

% From the visible feature set \( H = \{h_{1}, \cdots, h_{|H|}\} \), the measurement model is given by:

% \begin{align}
%     h = \begin{bmatrix}
%         f_{x} \frac{X}{Z} + c_{x} \\
%         f_{y} \frac{Y}{Z} + c_{y} \\
%         d
%     \end{bmatrix} \label{eq:stereo_meas}
% \end{align}
% where \( (f_{x}, f_{y}) \) represents the focal lengths, \( (c_{x}, c_{y}) \) denotes the principal point, and \( d \) is the depth measurement. The depth \( d \) can be directly provided by a depth sensor or computed from stereo camera data.
% While the depth value can be estimated from a single frame, when multiple frames are available, advanced techniques such as bundle adjustment offer a more accurate depth estimate. Whenever available, we use this refined depth value as the measurement.

% Additionally, the visibility of the point feature in 3D space can be determined by the field-of-view of the camera. We assume a common pinhole camera model, and the feature is considered visible if its measurement vector \( h \) satisfies the following conditions: 
% \( h_{u} \in [u_{min}, u_{max}] \), 
% \( h_{v} \in [v_{min}, v_{max}] \), and 
% \( h_{d} > 0 \).


% \subsection{Simulation}
% To validate the theoretical contribution of our algorithm, we simulate visual front-end. 

% Pointcloud world as in Carlone and Karaman. 
% Pose estimation optimization is done in iSAM2 algorithm within fixed-lag smoothing approach. We simulate the visual front-end and it is optimal. In the next section, we show the proposed approach can also works with visual front-end with real-world data.

% \begin{align}
% v_{cam} = 
% \begin{bmatrix}
%  -v_{y} \\ -v_{z} \\ v_{x}
% \end{bmatrix}, 
% \omega_{cam} = \begin{bmatrix}
% 0 \\ -\omega \\ 0
% \end{bmatrix}
% \end{align}

% \subsection{Experiment}

\begin{figure}
    \centering
    \subfloat[]{
    \includegraphics[width=0.48\linewidth]{figure/exp_scenario_low_resol.png}
    }
    \subfloat[]{
    \includegraphics[width=0.41\linewidth]{figure/hardware_low_resol.png}
    }
    % \includegraphics[width=0.95\linewidth]{figure/experiment-min.png}
    \caption{(a) The experiment scenario where the robot inspects the wall moving through the texture-poor region and (b) the hardware configuration used for the hardware experiment. }
    \label{fig:hardware}
\end{figure}

The proposed safety filter is validated on real hardware in an experimental setup in a vision-challenging environment.

\subsection{Mission Objective and Hardware Configuration}

The mission is to inspect a wall using a stereo camera, which includes both texture-rich and texture-poor regions, as shown in Fig. \ref{fig:hardware} (a). The objective is to move along the wall while keeping the camera oriented perpendicular to the surface, ensuring an optimal view of the inspection area. However, the texture-poor region poses a challenge for visual navigation.

The hardware setup, shown in Fig. \ref{fig:hardware} (b), includes a differential wheeled robot (ROAS Inc.) with a ZED2i stereo camera mounted on a servomotor-equipped rig, allowing rotation aligned with the robot's axis. An NVIDIA Jetson Xavier NX handles is paired with the stereo camera for real-time processing, and a SICK 2D LiDAR is used for obstacle detection.
% The hardware configuration is depicted in Fig. \ref{fig:hardware} (b). The base robot is the Former robot of ROAS Inc., operates using a differential wheeled drive system. A ZED2i stereo camera is mounted on a rig equipped with a servomotor, enabling rotation in the same axis with the rotation of the base robot. An NVIDIA Jetson Xavier NX computer is paired with the stereo camera for real-time processing. Additionally, a SICK 2D LiDAR is mounted on the base robot for obstacle detection. 
All system components are connected via Ethernet communication.


\subsection{Implementation Details}
The control structure with the proposed safety filter and hardware is shown in Fig. \ref{fig:scheme}. The robot state and the velocity are defined as $q = [q_{x},q_{y},\theta_{r}, \theta_{m}]^{T}$ and $v=[v_{r},\omega_{r},\omega_{m}]^{T}$, where $\theta_{r}$, $v_{r}$ and $\omega_{r}$ are heading angle, velocity and angular velocity of the base robot and $\theta_{m}$ and $\omega_{m}$ are the heading angle and angular velocity of the servomotor. The Jacobian matrix $J$ is $\begin{bsmallmatrix}
\cos \theta_{r} & 0 & 0 \\
\sin \theta_{r} & 0 & 0 \\
0 & 1 & 0 \\ 
0 & 0 & 1
\end{bsmallmatrix}$,
and the heading angle of the camera is $\theta_{c}=\theta_{r}+\theta_{m}$.

The reference linear velocity and angular velocity (\(v_r\), \(\omega_r\)) for the base robot, and the reference angular velocity (\(\omega_{s,r}\)) for the servomotor are given to the safety filter. \(v_r\) is set as positive constant value and \(\omega_{r}\) to zero when the heading \(\theta_{r}\) 
 is set to the direction parallel to the wall. The servomotor is controlled by a PID controller to regulate the camera to be aligned with the wall’s normal vector.

For visual navigation, we employ ORB-SLAM2 \cite{mur2017orb} with stereo images. Each extracted feature is measured as $m=[m_{u},m_{v},m_{d}]^{T}=[f_{x} \frac{p_{x}}{p_{z}} + c_{x}, f_{y} \frac{p_{y}}{p_{z}} + c_{y}, d]^{T}$, where \( (f_{x}, f_{y}) \) represents the focal lengths, \( (c_{x}, c_{y}) \) denotes the principal point, and \( d \) is the depth measurement. The visibility condition $\rho$ is defined as $m_{u} \in [0, I_{w}],m_{v}\in [0,I_{h}]$, and $m_{d}\in [r_{min},r_{max}]$ where $I_{w}$ and $I_{h}$ are the width and height of the image, and $r_{min}$ and $r_{max}$ are the depth thresholds for valid detection.  
Since the number of inequality constraints linearly increases to the number of features, we limit the features passed to the safety filter to $N_{max}=50$ to meet the real-time requirement for solving the QP problem \eqref{eq: final safety filter} by sampling $N_{max}$ number of features from the total feature set. The signed distance function from $h_{6}$ in \eqref{eq: final state constraints} is obtained from 2D LiDAR data.

% Instead of randomly sampling features from the entire feature set, we select a subset with a higher probability of being tracked in the next frame. We use the score for FAST keypoints \cite{rosten2008faster} as a heuristic quantitative criterion for tracking probability and, if the total number of features exceeds \(N_{max}\), we greedily select the \(N_{max}\) features with the highest scores.


% For visual navigation, we use ORB-SLAM2 \cite{mur2017orb} with stereo images. The image space measurements \eqref{label} from the left camera are sent to the proposed safety filter, along with reference velocity and angular velocity ($v_r$, $\omega_r$) for the base robot and servo motor angular velocity ($\omega_{s,r}$). The reference velocity for the base robot is computed as $v_{max}$ in the $x$-direction, while the servo motor is controlled using a PID controller to align with the wall’s normal vector.

% Let the mission of the scenario is to inspect the wall with stereo camera. which wall is consisted with texture-rich segment and texture-poor region. The objective is to move along the wall while canera is looking normal to the wall, good view of the inspection site. However, the texture-poor region in the middile of the wall as in Fig. \ref{} is critical for visual navigation.
% From the stereo images, we run ORB-SLAM2 \cite{mur2017orb} for visual navigation. And $uvd$ coordinates from the left camera is sent to the proposed safety filter with the reference velocity and angular velocity $v_{r}$,$\omega_{r}$ for the base robot and the servo motor angular velocity $\omega_{s,r}$. The reference velocity for the base robot is computed as $v_{max}$ in $x$ direction, and the servo motor is computed to align with the normal vector of the wall with PID control. 



% Since the number of inequalities constraints are linear to the number of features treated in the safety filter, real-time requirement of solving QP problem \eqref{} and the limitation of computational capacity of onboard computer restricts the number of features that can be handled with the proposed safety filter. 
% % Since adding each feature for the safety filter works as an additional inequality constraint, computation capacity of the onboard computer limits the number of feature can be considered in solving QP. 
% Therefore, we limits the maximum feature number $N_{max}$ handing over to the safety filter. But instead of sampling random features from the whole feature set, we acquire the subset which have highber probability of tracked in the next frame. (Since our theory assumes that the feature can be tracked in all frames if it is inside the FoV.) We used the FAST score \cite{} as a quantitative criteria as heuristic for tracking probability, and select $N_{max}$ number in greedy fashion if the total number of feature set exceeds $N_{max}$. 

\subsection{Results}

In the baseline algorithm, the reference velocity is applied directly to the robot's tracking controller, with the servomotor keeping the camera perpendicular to the wall. As shown in the onboard images of Fig. \ref{fig:exp_proposed} (a), the robot moves through a texture-poor region, causing a shart drop in visible features and an increase in estimation error (Fig. \ref{fig:exp_estimation_result}). 
% It results in the drastic increase of estimation error of ORB-SLAM2 algorithm.

In contrast, the proposed safety filter  adjusts the control input to maintain a sufficient number of visible features, even when passing through feature-poor regions. It rotates the camera toward feature-rich regions to ensure continuous tracking, returning to the orientation close to the desired orientation when the robot reaches a  feature-rich area. As a result, a sufficient number of features are kept tracked as in Fig. \ref{fig:exp_estimation_result} (a). We observe that, in this scenario, our algorithm successfully maintains a sufficient number of visible features, implying it can be used along with the visual front-end with real-world data.

% Since we only used subset of total features  as an input of the safety filter, the total detected feature number is larger than the given threshold, which is not a problem since we are trying to avoid worst-case. 


% For the baseline algorithm, we directly apply the reference velocity to the low-level tracking controller of the robot. The servomotor keeps the camera oriented perpendicular to the wall. However, as the robot moves through the texture-poor region, the lack of visible features causes the ORB-SLAM algorithm to lose tracking.

% In contrast, the proposed safety filter actively adjusts the control input to maintain a sufficient number of visible features, even when passing through feature-poor regions. It rotates the camera toward the feature-rich areas to ensure continuous feature   tracking and then returns to the desired orientation once the robot enters a new feature-rich region.

% We observe that, in this scenario, our algorithm successfully maintains a sufficient number of visible features, even in the presence of errors from the visual front-end caused by real-world data.


% \textcolor{red}{Since we only used selected subset of total feature set, the detected feature number is larger than the objective $N$.}
% For baseline algorithm, we use the reference velocity directly applied to the low-level tracking controller of the robot. The servomotor keeps the camera normal to the wall, but as it moves through the texture-poor region, due to the lack of visible features, it loses track of ORB-SLAM algorithm. 

% However, the proposed safety filter actively finds the control input to maintain the enough number of visible features even during it passes through the feature-poor region. It rotates toward the feature-rich region and continuously maintain tracking of enough features, and then rotate toward the desired direction when it enters to the new feature-rich region. 

% We find that in the scenario, our algorithm is able to maintain enough number of visible features even there is error sources of visual front-end from real-world data. 



\begin{figure}
    \centering
    \vspace{-2mm}
    \subfloat[]{
        \includegraphics[width=0.5\linewidth]{figure/exp_feature_number.png}}
    \subfloat[]{
        \includegraphics[width=0.5\linewidth]{figure/exp_estimation_error.png}}
    \caption{Comparison of the result of the visual SLAM algorithm in  between the proposed safety filter (red) and the baseline controller (blue). (a) The number of tracked features and (b) the state estimation error acquired from the ORB-SLAM2 algorithm.}    
    \label{fig:exp_estimation_result}
\end{figure}


% In order to validate the proposed safety filter is applied to real hardware system, we design a hardware system and experiment on vision-harsh environment. 

% \textcolor{red}{hardware configuration}
% The hardware configuration is shown in Fig. \ref{}. The base robot, manufactured by ROAS Inc., operates using a differential wheeled drive model. A ZED2i stereo camera is mounted on a rig with a servo motor that enables rotation around the $z$-axis. An NVIDIA Jetson Xavier NX computer is paired with the stereo camera for processing. Additionally, a SICK 2D LiDAR is mounted on the base robot for obstacle detection. The system components are connected via Ethernet communication.
% % The hardware configuration is visualized in the Fig. \ref{}, the base robot (former from ROAS Inc.) runs in differential wheeled drive model and the zed2i stereo camera is attached to the rig where the servo motor is enable the rotation with respect to the $z$ axis. NVIDIA Jetson Xavier NX computer is companion to the stereo camera. The SICK 2D Lidar is attached to the base robot where it is used to detect obstacles near the robot. The computers are connected in ethernet communication. 

% The mission of the scenario is to inspect a wall using a stereo camera. The wall consists of both texture-rich segments and a texture-poor region. The objective is to move along the wall while the camera remains oriented perpendicular to the surface, providing an optimal view of the inspection site. However, the texture-poor region in the middle of the wall, as depicted in Fig. \ref{}, presents a challenge for visual navigation.

% To address this, we use ORB-SLAM2 \cite{mur2017orb} for visual navigation based on stereo images. The $uvd$ coordinates from the left camera are sent to the proposed safety filter, along with reference velocity and angular velocity ($v_r$, $\omega_r$) for the base robot and servo motor angular velocity ($\omega_{s,r}$). The reference velocity for the base robot is computed as $v_{max}$ in the $x$-direction, while the servo motor is controlled using a PID controller to align with the wall’s normal vector.
% % Let the mission of the scenario is to inspect the wall with stereo camera. which wall is consisted with texture-rich segment and texture-poor region. The objective is to move along the wall while canera is looking normal to the wall, good view of the inspection site. However, the texture-poor region in the middile of the wall as in Fig. \ref{} is critical for visual navigation.
% % From the stereo images, we run ORB-SLAM2 \cite{mur2017orb} for visual navigation. And $uvd$ coordinates from the left camera is sent to the proposed safety filter with the reference velocity and angular velocity $v_{r}$,$\omega_{r}$ for the base robot and the servo motor angular velocity $\omega_{s,r}$. The reference velocity for the base robot is computed as $v_{max}$ in $x$ direction, and the servo motor is computed to align with the normal vector of the wall with PID control. 
% \textcolor{red}{mission explanation}


% Since adding each feature for the safety filter works as an additional inequality constraint, computation capacity of the onboard computer limits the number of feature can be considered in solving QP. 
% Therefore, we limits maximum feature number $N_{max}$, and acquire the feature set using maximum $N_{max}$ Harris score \cite{} value, which works as heuristic that larger score leads to larger tendency to match in consequent frame. 

% \textcolor{red}{result}

% \textcolor{red}{With servo, without servo}
% manual control, without servo it often stucks. with servo, it actively changes





\section{Conclusion}

Despite the widespread use of vision sensors in mobile robot navigation, they remain susceptible to challenges like low-texture environments and poor lighting. While researchers have developed trajectory planners and controllers to maximize visual information alongside control objectives, these methods often suffer from parameter sensitivity and numerical instability. 

To address this, we proposed a safety-critical control approach using a QP-based safety filter to maintain sufficient visual features in an image with a novel safety condition, enabling real-time implementation.
% To address this, we proposed a safety-critical control approach using a safety filter. We designed a safety condition to ensure the presence of a sufficient number of visual features in the image. The QP-based formulation of the safety filter allows for real-time implementation. 
Experiments show it integrates seamlessly with visual odometry or SLAM, reducing the risk of catastrophic failures in visual estimation.
% Our experimental results demonstrate that the safety filter for reliable feature tracking can easily integrate with conventional visual odometry or SLAM algorithms, reducing the risk of catastrophic failures in visual estimation.

Future work will relax assumptions like the need for a stopping input to extend applicability to higher-relative-degree systems. We will also tackle  practical challenges such as occlusion, which can abruptly reduce observed landmarks and disrupt the invariance condition. Additionally, we plan to explore integrating other visual estimation quality proxies, such as informativeness \cite{zhang2019beyond} and system excitation \cite{hausman2017observability}, into the proposed safety filter.
% Another interesting direction is to extend the formulation by incorporating inertial sensors, allowing the filter to be used with visual-inertial navigation systems, 


% It is also an interesting future research direction to consider more general robots that does not have sudden stopping capabilities. 
% \red{stopping input $u$가 존재하지 않을 때에는 어떻게 할 것인지에 대한 discussion, \cite{jang2024safe_icra}, \cite{jang2024safe_ral}, }, 


\bibliographystyle{./bibtex/IEEEtran}
\bibliography{./bibtex/IEEEabrv, ./bibtex/mybibfile}


\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths

\end{document}

