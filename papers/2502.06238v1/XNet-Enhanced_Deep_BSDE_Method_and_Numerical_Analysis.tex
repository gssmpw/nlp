\documentclass[11pt]{article}

% 解决 arXiv 兼容性问题
\usepackage[colorlinks=true, linkcolor=red, citecolor=green, urlcolor=blue]{hyperref}

% 解决 arXiv 不运行 bibtex 的问题，使用 bbl 文件
%\usepackage{natbib}
\bibliographystyle{siam}  

% 版面设置
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}

% 其他常见包
\usepackage{abstract} 
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{cases}
\usepackage{xcolor}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{comment}
\usepackage{csquotes}
\usepackage[figure,table]{hypcap}
\usepackage{authblk}

% 解决 arXiv 可能屏蔽 pdfborder 的问题
\hypersetup{
	colorlinks=true,
	linkcolor=red,          % 红色超链接（公式、章节引用）
	citecolor=green,        % 绿色超链接（文献引用）
	urlcolor=blue,          % 蓝色超链接（网址）
	pdfpagemode=UseOutlines
}

% 交叉引用设置
\newtheorem{assumption}{Assumption} 
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}   

% 解决 arXiv 不兼容 ulem 的问题（ulem 会修改字体）
% \usepackage{ulem}  % <-- 移除这个包

\pagestyle{plain}

\begin{document}
\title{\textbf{XNet-Enhanced Deep BSDE Method and Numerical Analysis}}
\author[1]{Xiaotao Zheng\thanks{Email: \texttt{20234013002@stu.suda.edu.cn}}}
\author[2,3]{Zhihong Xia\thanks{Co-Corresponding author: \texttt{xia@math.northwestern.edu}}}
\author[4,5]{Xin Li\thanks{Co-corresponding author: \texttt{xinli2023@u.northwestern.edu}}}
\author[6]{Xingye Yue\thanks{Email: \texttt{xyyue@suda.edu.cn}}}

\affil[1]{Center for Financial Engineering, Soochow University, Suzhou 215008, Jiangsu, China}
\affil[2]{School of Natural Science, Great Bay University, Dongguan 523808, Guangdong, China}
\affil[3]{Department of Mathematics, Northwestern University, Evanston 60208, IL, USA}
\affil[4]{Department of Computer Science, Northwestern University, Evanston 60208, IL, USA}
\affil[5]{Mathematical Modelling and Data Analytics Center, Oxford Suzhou Centre for Advanced Research, Suzhou 215123, Jiangsu,  China}
\affil[6]{Center for Financial Engineering, Soochow University, Suzhou 215008, Jiangsu,  China}

\date{}

\phantomsection
\addcontentsline{toc}{section}{Title}
\maketitle
	
\phantomsection
\addcontentsline{toc}{section}{Abstract}

\begin{abstract}
Solving high-dimensional semilinear parabolic partial differential equations (PDEs) challenges traditional numerical methods due to the "curse of dimensionality." Deep learning, particularly through the Deep BSDE method, offers a promising alternative by leveraging neural networks' capability to approximate high-dimensional functions. This paper introduces a novel 
network architecture, XNet, which significantly enhances the computational efficiency and accuracy of the Deep BSDE method. XNet demonstrates superior approximation capabilities with fewer parameters, addressing the trade-off between approximation and optimization errors found in existing methods. We detail the implementation of XNet within the Deep BSDE framework and present results that show marked improvements in solving high-dimensional PDEs, potentially setting a new standard for such computations.	

\noindent{\textbf{Keyword:}  Deep BSDE method, XNet, high-dimensional PDEs, neural networks, approximation errors.}   
\end{abstract}

\section{Introduction}
Semilinear parabolic partial differential equations (PDEs) play a crucial role in modeling complex dynamic systems across various scientific domains, from financial mathematics to biological processes. These equations, often formulated as:
\begin{equation}\label{bxxpwxpde}
	\left\{ \begin{aligned}
		& \frac{{\partial u}}{{\partial t}}(t,x)  + \frac{1}{2}{\mathop{\rm Tr}\nolimits} \left( {\sigma {\sigma ^{{T}}}(t,x)\left( {{{{\mathop{\rm Hess}\nolimits} }_x}u} \right)(t,x)} \right) + \nabla u(t,x) \cdot \mu (t,x)  \\
		&\qquad  + f\left( {t,x,u(t,x),{\sigma ^{{T}}}(t,x)\nabla u(t,x)} \right) = 0,(t,x) \in [0,T) \times {R^d},\\
		& u(T,x) = g(x),x \in {R^d}.
	\end{aligned} \right.
\end{equation}
Although traditional numerical methods such as the Finite Difference Method (FDM) and Finite Element Method (FEM) perform well in handling low-dimensional cases (e.g., $d=1, 2, 3$), they struggle to solve high-dimensional problems due to the "curse of dimensionality" \textsuperscript{\cite{weishu1}}.

Recent advancements in deep learning methods, such as Physics-Informed Neural Networks (PINNs) \textsuperscript{\cite{PINN,Shin,Mishra,PINN_2024_1,PINN_2024_2}}, the Deep Galerkin Method \textsuperscript{\cite{DGM, DGM_2024}}, and the Deep Ritz Method \textsuperscript{\cite{Ritz,Ritz_2024}}, have shown promise in solving high-dimensional PDEs like (\ref{bxxpwxpde}). However, the sampling strategies within these methods still pose significant challenges when tackling extremely high-dimensional cases (e.g., $d=100, 500, 1000$).
Therefore, there is a need to explore alternative deep learning approaches based on stochastic processes (Monte Carlo sampling), which include the Deep BSDE method \textsuperscript{\cite{E2017}}, Deep Splitting \textsuperscript{\cite{Beck2}}, and Deep Backward Dynamic Programming (DBDP) method \textsuperscript{\cite{DBDP}}. 
Among these, the Deep Backward Stochastic Differential Equation (Deep BSDE) method is a widely adopted technique for handling such highly high-dimensional PDEs (\ref{bxxpwxpde}).
This method leverages neural networks to approximate the gradient of the solutions to high-dimensional semilinear parabolic PDEs, utilizing the capacity of these networks to manage the stochastic nature of the solutions.
Despite its innovative approach, the Deep BSDE method encounters difficulties in approximation, generalization, and optimization errors. The key to mitigating these errors lies in the network architecture. The universal approximation theorem suggests that while neural networks can theoretically approximate any continuous function, the practical limitations of network size and parameter scalability necessitate a balance between network complexity and computational efficiency \textsuperscript{\cite{weishu}}.

In this context, we introduce XNet \textsuperscript{\cite{XNet1, XNet2}}, a novel neural network architecture that provides notable advantages for solving high-dimensional PDEs. XNet is designed to achieve arbitrary-order approximations with far fewer parameters than traditional networks, addressing the intricate balance between approximation accuracy and model simplicity. This paper focuses on enhancing 
the Deep BSDE method by integrating XNet, showcasing through empirical analysis how it surpasses traditional two-layer networks in both accuracy and computational efficiency.

Our discussion will detail the theoretical underpinnings of XNet, its integration into the Deep BSDE framework, and the consequential improvements in solving high-dimensional PDEs. By adopting XNet, this work not only advances the state-of-the-art in PDE solvers but also sets the groundwork for further innovations in computational mathematics.



\textbf{Outline of the article}
The organization of this paper is as follows. In Section \ref{s2}, we review the Deep BSDE method. In Section \ref{s3}, we analyze the potential practical errors of the algorithm, including approximation errors, generalization errors, and optimization errors. In Section \ref{s4}, we implement the Deep BSDE method using discrete-time XNet and two-layer networks (Two-layer Net), demonstrating that XNet has superior approximation capabilities. In Section \ref{s5}, we implement the Deep BSDE method using continuous-time XNet and Two-layer Net, proving that XNet possesses extremely strong approximation capabilities. In the example of the Allen-Cahn equation, we observe the convergence rate of the Deep BSDE method by integrating XNet.


\section{Deep BSDE Method} \label{s2}
In this section, we begin by introducing the BSDE system related to semilinear parabolic partial differential equations (PDEs). Subsequently, we will present the Deep BSDE (DBSDE) method proposed by E et al. \textsuperscript{\cite{E2017}}.

\subsection{Forward Backward Stochastic Differential Equation (FBSDE)}
Here, we follow the approach of Peng \textsuperscript{\cite{Peng1,Peng2}} and references therein to establish the correlation between PDEs(\ref{bxxpwxpde}) and well-posed backward stochastic differential equation (BSDE) system.

\begin{lemma}
Assume $(\Omega,\mathcal{F},\mathbb{P})$ represents a probability space, and let $W=\left(W^{(1)}, \ldots, W^{(d)}\right):[0, T] \times \Omega \rightarrow \mathbb{R}^d$ be a standard Brownian motion, with $\mathcal{F}_t$ being a non-decreasing filtration generated by $W$. Let $X=\left(X^{(1)}, \ldots, X^{(d)}\right):[0, T] \times \Omega \rightarrow \mathbb{R}^d$, $Y:[0, T] \times \Omega \rightarrow \mathbb{R}$, and $Z:[0, T] \times \Omega \rightarrow \mathbb{R}^d$ be $\mathcal{F}$-adapted stochastic processes. Consider the following BSDE:
\begin{numcases}{}\label{BSDE}
	X_t=\xi+\int_0^t \mu\left(s, X_s\right) \mathrm{d} s+\int_0^t \sigma\left(s, X_s\right) \mathrm{d} W_s,  \label{Xt}\\
	Y_t=g\left(X_T\right)+\int_t^T f\left(s, X_s, Y_s, Z_s\right) \mathrm{d} s-\int_t^T\left(Z_s\right)^T \mathrm{~d} W_s. \label{Yt}
\end{numcases}
Under certain conditions, the BSDE is well-posed and corresponds to the PDE $($\ref{bxxpwxpde}$)$. Specifically, for any $t \in[0, T]$, The following equation holds almost surely in probability,
\begin{equation}\label{YZ}
	\left\{ {\begin{aligned}
			& {{Y_t} = u\left( {t,{X_t}} \right)},\\
			& {{Z_t} = {\sigma ^ {T} }\left( {t,{X_t}} \right)\nabla u\left( {t,{X_t}} \right)}.
	\end{aligned}} \right.
\end{equation}
\end{lemma}

In other words, if the $d$-dimensional stochastic process $\left\{X_t\right\}_{t \in[0, T]}$ satisfies equation(\ref{Xt}),
then the solution of PDE (\ref{bxxpwxpde}) satisfies the following stochastic differential equation:

\begin{equation}\label{BSDEeq}
	\begin{aligned}
		u(t,{X_t}) - u(0,\xi)  =  &- \int_0^t {f\left( {s,X_s,u(s,X_s),{\sigma ^{{T}}}(s,X_s)\nabla u(s,X_s)} \right)} ds \\
		& +\int_0^t {{{[\nabla u(s,X_s)]}^T}\sigma (s,X_s)d{W_s}} .
	\end{aligned}
\end{equation}

\subsection{Implementation of the Deep BSDE Method}
After adapting the solution of PDE (\ref{bxxpwxpde}) to the SDE (\ref{BSDEeq}) by BSDE theory, given a partitioning of the interval $[0,T]$, $\pi:0=t_0<t_1<\dots<t_N=T$ with $\Delta t_n=t_{n+1}-t_n$, the solution at each time step can be approximated with the Euler-Maruyama scheme,
%\begin{scriptsize}
\begin{equation}\label{ls-BSDEeq}
	\begin{aligned}
		u\left( {{t_{n + 1}},{X_{{t_{n + 1}}}}} \right) - u\left( {{t_n},{X_{{t_n}}}} \right)   \approx & - f\left( {{t_n},{X_{{t_n}}},u\left( {{t_n},{X_{{t_n}}}} \right),{\sigma ^{{T}}}\left( {{t_n},{X_{{t_n}}}} \right)\nabla u\left( {{t_n},{X_{{t_n}}}} \right)} \right)\Delta {t_n}\\
		& + {{\left[ {\nabla u\left( {{t_n},{X_{{t_n}}}} \right)} \right]}^{\rm{T}}}\sigma \left( {{t_n},{X_{{t_n}}}} \right)\Delta {W_n},n = 0,1,...,N - 1.
	\end{aligned}
\end{equation}
%\end{scriptsize}
To achieve a globally approximate scheme, neural networks can be incorporated into the forward discretization process (\ref{ls-BSDEeq}). The first step towards this is to obtain training data by sampling $M$ independent paths ${\left\{ {X_{{t_n}}^m} \right\}_{0 \le n \le N}^{m = 1,2, \ldots, M}}$, where $\left\{X_{t_0}^m\right\}^{m=1, \ldots, M}=\xi$. The critical step next is employing the neural network parameters ${\theta _{{u_0}}}$, ${\theta _{\nabla {u_0}}}$ to approximate the solution and the gradient function at $t=t_0$ respectively. Let $\theta_n$ represent all network parameters approximating the gradient function $\nabla u(t,x)$ by the neural network at time $t=t_n$, where $n = 1,2,..., N - 1$.  
With the above approximations, the total set of parameters is $\theta=\left\{{\theta _{{u_0}}}, {\theta _{\nabla {u_0}}}, \theta_1, \theta_2, \ldots, \theta_{N-1}\right\}$. The equation (\ref{ls-BSDEeq}) can be rewritten as follows.
%\begin{scriptsize}
\begin{equation}\label{nn-ls-BSDEeq}
	\begin{aligned}
		\hat u\left( {{t_{n + 1}},X_{{t_{n + 1}}}^m} \right) & - \hat u\left( {{t_n},X_{{t_n}}^m} \right)  =  - f\left( {{t_n},X_{{t_n}}^m,\hat u\left( {{t_n},X_{{t_n}}^m} \right),{\sigma ^{{T}}}\left( {{t_n},X_{{t_n}}^m} \right)\nabla u_{{t_n}}^\theta \left( {{t_n},X_{{t_n}}^m} \right)} \right)\Delta {t_n}\\
		& + {\left[ {\nabla u_{{t_n}}^\theta \left( {{t_n},X_{{t_n}}^m} \right)} \right]^{\rm{T}}}\sigma \left( {{t_n},X_{{t_n}}^m} \right)\Delta W_{{t_n}}^m,m = 1,2,...,M,n = 0,1,...,N - 1.
	\end{aligned}
\end{equation}
%\end{scriptsize}
In particular, when $t=t_0$, we have
\begin{equation}\label{t0-nn-ls-BSDEeq}
	\begin{aligned}
		\hat u\left( {{t_1},X_{{t_1}}^m} \right) = {\theta _{{u_0}}} - f\left( {{t_0},\xi ,{\theta _{{u_0}}},{\sigma ^T}\left( {{t_0},\xi } \right){\theta _{\nabla {u_0}}}} \right)\Delta {t_0} + {\left[ {{\theta _{\nabla {u_0}}}} \right]^{\rm{T}}}\sigma \left( {{t_0},\xi } \right)\Delta W_{{t_0}}^m,m = 1,2,...,M.
	\end{aligned}
\end{equation}
By applying the globally approximate scheme (\ref{nn-ls-BSDEeq}), an approximate value of $u\left(t_N, X_{t_N}^m\right)$, denoted as $\hat{u}\left(t_N, X_{t_N}^m\right)$, can be output, where $m=1,2, \ldots, M$. The matching of a given terminal condition defines the expected loss function,
\begin{equation}\label{BSDE-LOSS}
	\begin{aligned}
		l(\theta ) = \frac{1}{M}\sum\limits_{m = 1}^M {{{\left| {g\left( {{X_T^m}} \right) - \hat u\left( {{t_N},X_{{t_N}}^m} \right)} \right|}^2}} .
	\end{aligned}
\end{equation}
Network parameters are optimized using algorithms such as SGD \textsuperscript{\cite{SGD}}, L-BFGS-B \textsuperscript{\cite{LBFGSB1,LBFGSB2}}, Adagrad \textsuperscript{\cite{ADAGRAD}}, and Adam \textsuperscript{\cite{ADAM}} algorithms. 
Through this process, it becomes evident that an approximate solution
${\theta _{{u_0}}}$ for $u(0, \xi)$ can be obtained. 
In the work of Weinan E et al., they opted to use the Adam algorithm.

\begin{figure*}[!hbtp]
	\begin{center}
		\includegraphics[angle=0,width=6in]{Deep_BSDE.png}		
	\end{center}
	\vspace{-0.7cm}
	\caption{The neural network architecture for Deep BSDE method. The network consists of multiple ($N-1$) sub-networks, with each sub-network corresponding to a time interval. Each sub-network has $H$ network parameters. It should be noted that in addition to these, ${\theta _{{u_0}}}$ and ${\theta _{\nabla {u_0}}}$ are also network parameters that need to be optimized.}
	\label{fig0}
\end{figure*}

The Deep BSDE method is a deep learning algorithm, and its accuracy can generally be categorized into three main types of errors: approximation errors, generalization errors, and optimization errors. Implementing the Deep BSDE method with a well-designed network architecture can lead to smaller approximation and optimization errors.
In the following discussion, we introduce two network architectures used to implement the Deep BSDE method: the two-layer network (Two-layer Net) employed in previous studies, and a newly introduced architecture in this paper, XNet. Theoretically, we will demonstrate that XNet has a more efficient approximation capability compared to the Two-layer Net, enabling higher approximation accuracy with fewer network parameters.
%

\subsection{Network Architecture for the Deep BSDE Method: Two-layer Net and XNet}
In the original work by \textsuperscript{\cite{E2017}}, two-layer networks (Two-layer Net), a type of feedforward neural network (FNN), were used to approximate the gradient function in the deep BSDE method. Here, we propose replacing the Two-layer Net with XNet, demonstrating that XNet provides a more efficient approximation capability.

First, we revisit the framework of feedforward neural networks (FNNs):
\begin{equation}\label{FNN}
	\begin{aligned}
		Y_{\text{FNN}}^\theta(X;W,b)==\sigma\circ F_q \circ\sigma\circ F_{q-1}\circ\cdots\sigma\circ F_1(\mathrm{X}),
	\end{aligned}
\end{equation}
where
\begin{equation}\label{FNN_F1}
	\begin{aligned}
		F_i(\cdot)=W_i(\cdot)+b_i, \quad 1 \le i \le q, 
	\end{aligned}
\end{equation}
\begin{equation}\label{FNN_F2}
	\begin{aligned}
		W = \left(W_1, W_2, \dots, W_q \right)^\mathrm{T}, \quad b = \left(b_1, b_2, \dots, b_q \right)^\mathrm{T}.
	\end{aligned}
\end{equation}
Here, $X$ represents the input, while $W$ and $b$ are the weight and bias matrices, respectively, where the elements of these matrices are the network parameters. 
The activation function \( \sigma \), which could be sigmoid, tanh, ReLU, or another function, largely determines the nature of the output \( Y_{\text{FNN}}^\theta \).


Specifically, the Two-layer Net is a type of FNN with two hidden layers, each containing $L$ neurons, resulting in a total of \(\mathcal{O}(L^2)\) network parameters. The universal approximation theorem asserts that a Two-layer Net of sufficient size (for sufficiently large $L$) can theoretically approximate any continuous function with arbitrary precision. However, as the network size increases, so does the number of required parameters, making the optimization process increasingly difficult.
Consequently, the Two-layer Net is considered inefficient, as it requires a large number of parameters for adequate approximation, leading to significant optimization errors. This is a common drawback in neural networks today, and XNet offers a potential solution.


\begin{theorem}\label{Cauchy_approximation_theorem}
	Let $f(z^1,z^2,\dots,z^d)$ be an analytic function on an open set $U \subset \mathbb{C}^d$. Let $M$ be a compact subset of $U$, $M \subset U$. Given any $\varepsilon > 0$, there exists a sequence of points $(\xi_k^1,\xi_k^2,\dots,\xi_k^d)$, $k = 1,2,\dots,m$, in the complex region $U$ such that
	\begin{equation}\label{cauchy_th}
		\begin{aligned}
			\left|f(z^1,z^2,\dots,z^d) - \sum_{k=1}^L \frac{\lambda_k}{(\xi_k^1 - z^1)(\xi_k^2 - z^2)\cdots(\xi_k^N - z^d)}\right| < \varepsilon,
		\end{aligned}
	\end{equation}
	where $\lambda_1,\lambda_2,\dots,\lambda_L$ are certain parameters, and the approximation order is $\mathcal{O}\left(\frac{1}{L^p}\right)$, where $p$ is any integer.
	
\end{theorem}

Based on the Cauchy approximation theorem \ref{Cauchy_approximation_theorem}, Xia et al. constructed the following network framework:
\begin{equation}\label{XNet}
	\begin{aligned}
		Y_{\text{XNet}}^\theta(X) &= \text{Re}\left(\sum_{k=1}^L \frac{\alpha_k + i \beta_k}{\sum_{j=1}^d a_k^j x_j + c_k + i e_k}\right) \\
		&= \sum_{k=1}^L \left( \alpha_k \frac{\sum_{j=1}^d a_k^j x_j + c_k}{\left(\sum_{j=1}^d a_k^j x_j + c_k\right)^2 + e_k^2} + \beta_k \frac{e_k}{\left(\sum_{j=1}^d a_k^j x_j + c_k\right)^2 + e_k^2}\right),
	\end{aligned}
\end{equation}
where $X=\left(x_1,x_2,...,x_d\right)$ represents the $d$-dimensional input, $Y_{\text{XNet}}^\theta$ denotes the output, and $\alpha_k, \beta_k, a_k^j, c_k,$ and $e_k$ are network parameters.

In function spaces, if analytic functions are dense in the space of continuous functions, meaning any continuous function can be approximated arbitrarily well by analytic functions. Based on Theorem \ref{Cauchy_approximation_theorem}, XNet achieves arbitrary-order approximation with \(\mathcal{O}(L)\) parameters. Thus, compared to the Two-layer Net, XNet offers a more efficient approximation framework in theory.


\section{Approximation errors, Generalization errors, and Optimization errors of the Deep BSDE method for coupled FBSDEs}\label{s3}
Following the deep BSDE method, we consider the following Euler scheme,
\begin{equation}\label{system}
	\left\{\begin{array}{l}
		{X}_0^\pi=\xi, \quad Y_0^\pi=\theta_{{u_0}}, \quad Z_0^\pi=\theta_{\nabla{u_0}}, \\
		{X}_{t_{n+1}}^\pi = {X}_{t_n}^\pi + \mu\left(t_n, {X}_{t_n}^\pi \right)\Delta t_n + 
		\sigma \left(t_n, {X}_{t_n}^\pi \right) \Delta W_n, \\
		Y_{t_{n+1}}^\pi = Y_{t_n}^\pi - f\left(t_n, {X}_{t_n}^\pi, Y_{t_n}, \sigma^{\mathrm{T}}\left(t_n, {X}_{t_n}^\pi \right)  \right)\Delta t_n+ \left[Z_{t_n}^\pi\right]^{\mathrm{T}} \sigma \left(t_n, {X}_{t_n}^\pi \right)  \Delta W_n, \\
		Z_{t_{n}}^\pi = \phi_n\left(t_n, {X}_{t_n}^\pi\right).
	\end{array}\right.
\end{equation}
By employing this approach, the challenging aspects of the problem are reframed into the task of solving the stochastic optimization problem:
\begin{equation}\label{optimization}
	\mathop {\inf }\limits_{ {\theta _{{u_0}}},{\theta _{\nabla {u_0}}}  \in {\mathcal{N}_0},{\theta _i} \in {\mathcal{N}_i}} F(\theta) =\mathbb{E} \left[ {{\left| {g \left( {{X}_T^\pi} \right) - Y_{t_N}^\pi } \right|}^2}  \right],
\end{equation}
where $\mathcal{N}_0$ and $\mathcal{N}_i (0  \le  i \le N - 1)$ are parametric function spaces generated by neural networks. 


\subsection{Approximation Errors for Deep BSDE method}
In this subsection, we focus on the approximation errors of the deep BSDE method, as derived from Lemma \ref{lemma_2} and \ref{lemma_3}.
Prior to further analysis, we introduce several assumptions from Zhang \textsuperscript{\cite{Zhang}}, along with the notation $h = \mathop {\inf }\limits_{0 \le n \le N-1} \Delta {t_n}$, $\Delta x = x_1 - x_2$, $\Delta y = y_1 - y_2$, and $\Delta z = z_1 - z_2$.

\begin{assumption}\label{assumption_1}
\

H$_1.$ There exist (possibly negative) constants $k_\mu, k_f$ such that
$$
\begin{aligned}
\left[\mu(t,x_1,y)-\mu(t,x_2,y)\right]^\mathrm{T}\Delta x&\leq k_\mu|\Delta x|^2,\\
\left[f(t,x,y_1,z)-f(t,x,y_2,z)\right]\Delta y&\leq k_f|\Delta y|^2.
\end{aligned}
$$

H$_2.$ $\mu, \sigma, f, g$ are uniformly lipschitz continuous with respect $to$ $( x, y, z).$ In particular,
there are non-negative constants $K, \mu_y, \sigma_x, \sigma_y$, $f_x$, $f_z$, and $g_x$ such that
$$
\begin{aligned}
|\mu(t,x_1,y_1)-\mu(t,x_2,y_2)|^2 &\leq K|\Delta x|^2+\mu_y|\Delta y|^2,\\
|\sigma(t,x_{1},y_{1})-\sigma(t,x_{2},y_{2})|^{2}&\leq\sigma_{x}|\Delta x|^{2}+\sigma_{y}|\Delta y|^{2},\\
|f(t,x_1,y_1,z_1)-f(t,x_2,y_2,z_2)|^2&\leq f_x|\Delta x|^2+K|\Delta y|^2+f_z|\Delta z|^2,\\
|g(x_1)-g(x_2)|^2&\leq g_x|\Delta x|^2.
\end{aligned}
$$

H$_3.$ $\mu( t, 0, 0) $, $f( t, 0, 0, 0) $, and $\sigma ( t, 0, 0)$ are bounded. In particular, there are constants $\mu_{0}$, $\sigma _{0}$, $f_{0}$, and $g_{0}$ such that
$$\begin{aligned}|\mu(t,x,y)|^{2}&\leq \mu_{0}+K|x|^{2}+\mu_{y}|y|^{2},\\
	|\sigma(t,x,y)|^{2}&\leq\sigma_{0}+\sigma_{x}|x|^{2}+\sigma_{y}|y|^{2},\\|f(t,x,y,z)|^{2}&\leq f_{0}+f_{x}|x|^{2}+K|y|^{2}+f_{z}|z|^{2},\\|g(x)|^{2}&\leq g_{0}+g_{x}|x|^{2}.\end{aligned}$$
\end{assumption}


\begin{assumption}\label{assumption_2}
$\mu, \sigma, f$ are uniformly Hölder-$\frac12$ continuous with respect to t.
\end{assumption}

\begin{assumption}\label{assumption_3}
One of the following five cases holds: 

H$_1.$ Small time duration, that is, T is small.

H$_2.$ Weak coupling of $Y$ into the forward SDE (\ref{Xt}), that is, $\mu_y$ and $\sigma_{y}$ are small. In particular, if $\mu_{y}=\sigma_{y}=0,$ then the forward equation does not depend on the
backward one and, thus, Eqs. (\ref{Xt}) and (\ref{Yt}) are decoupled.

H$_3.$ Weak coupling of $X$ into the backward SDE (\ref{Yt}), that is, $f_x$ and $g_x$ are small. In particular, if $f_{x}= g_{x}= 0$, then the backward equation does not depend on the forward one and, thus, Eqs. (\ref{Xt}) and (\ref{Yt}) are also decoupled. In fact, in this case, $Z=0$ and (\ref{Yt}) reduces to an ODE.

H$_4.$ $f$ is strongly decreasing in $y$, that is, $k_{f}$ is very negative. 

H$_5.$ $\mu$ is strongly decreasing in $x$, that is, $k_{\mu}$ is very negative.
\end{assumption}

\begin{lemma}\label{lemma_2}
	
Under assumptions \ref{assumption_1}, \ref{assumption_2}, and \ref{assumption_3}, there exists a constant $C_1$, independent of $h$, $d$, and $M$, such that for sufficiently small $h$,
\begin{equation}\label{lemma2}
	\begin{aligned}
		&\sup _{t_n \in[0, T]}\left(\mathbb{E}\left|X_{t_n}-{X}_{t_n}^\pi\right|^2+\mathbb{E}\left|Y_{t_n}-{Y}_{t_n}^\pi\right|^2\right)+\sum_{n=0}^{N-1}\int_{t_n}^{t_{n+1}} \mathbb{E}\left|Z_t-{Z}_{t_n}^\pi\right|^2 dt 
		\leq &C_1\left[h+\mathbb{E}\left|g\left(X_T^\pi\right)-Y_T^\pi\right|^2\right].
	\end{aligned}
\end{equation}
\end{lemma}

\begin{lemma}\label{lemma_3}

Under assumptions \ref{assumption_1}, \ref{assumption_2}, and \ref{assumption_3}, there exists a constant $C_2$, independent of $h$, $d$ and $M$, such that for sufficiently small $h$,
\begin{equation}\label{lemma3}
	\begin{aligned}
		\inf_{\theta_{{u_0}},\theta_{\nabla{u_0}} \in \mathcal{N}_0, \phi_n \in \mathcal{N}_i} \mathbb{E}\left|g\left(X_T^\pi\right)-Y_T^\pi\right|^2  
		\leq &C_2\left\{h+\inf _{\theta_{{u_0}},\theta_{\nabla{u_0}} \in \mathcal{N}_0}\left[ \mathbb{E}\left| Y_0^\pi-\theta_{{u_0}} \right|^2 +  \mathbb{E}\left| Z_0^\pi-\theta_{\nabla{u_0}} \right|^2  \right]h\right.\\
		&\left. + \inf _{\phi_n \in \mathcal{N}_n} \sum_{n=0}^{N-1} \mathbb{E}\left|\mathbb{E}\left[\tilde{Z}_{t_n} \mid X_{t_n}^\pi, Y_{t_n}^\pi\right]-\phi_n\left(X_{t_n}^\pi, Y_{t_n}^\pi\right)\right|^2 h\right\},
	\end{aligned}
\end{equation}	
where $\tilde{Z}_{t_n}=h^{-1}\mathbb{ E}\left[\int_{t_n}^{t_{n+1}} Z_t \mathrm{~d} t \mid \mathcal{F}_{t_n}\right]$. 
\end{lemma}

Lemmas \ref{lemma_2} and \ref{lemma_3} provide bounds on the absolute error of the Deep BSDE method. The detailed proofs can be found in the work of Han et al. \cite{Convergence-of-the-deep-BSDE-method}.
Given our primary interest in the relative error in calculations, we further derive bounds on the relative error (ref. Theorem \ref{theorem_1}).

\begin{theorem}\label{theorem_1}

Under assumptions \ref{assumption_1}, \ref{assumption_2}, and \ref{assumption_3}, there exists a constant $C$, independent of $h$, $d$ and $M$, such that for sufficiently small $h$,
\begin{equation}\label{th1}
	\begin{aligned}
	\mathbb{E}\left|\frac{{u(0, \xi)-\theta_{{u_0}}}}{{u(0,\xi)}}  \right|^2\
	 \leq &C\left\{ \frac{h}{u(0,\xi)^2} +\inf _{\theta_{{u_0}},\theta_{\nabla{u_0}} \in \mathcal{N}_0}  \mathbb{E}\left| Z_0^\pi-\theta_{\nabla{u_0}} \right|^2  \frac{h}{u(0,\xi)^2} \right.\\
	&\left. +  \inf _{\phi_n \in \mathcal{N}_n} \sum_{n=0}^{N-1} \mathbb{E}\left|\mathbb{E}\left[\tilde{Z}_{t_n} \mid X_{t_n}^\pi, Y_{t_n}^\pi\right]-\phi_n\left(X_{t_n}^\pi, Y_{t_n}^\pi\right)\right|^2 \frac{h}{u(0,\xi)^2}\right\}.
	\end{aligned}
\end{equation}

\end{theorem}
We observe that the approximation errors of the deep BSDE method consist of two components. One component arises from time discretization, which is a standard aspect in traditional numerical methods. The other component is due to the approximation capability of the neural network. If $u(0,\xi)$ is sufficiently large, the error from time discretization becomes negligible, and the approximation error will primarily depend on the network's approximation capability.


\subsection{Generalization Errors and Optimazation Errors for Deep BSDE method}
In the Deep BSDE method, the loss function is defined as the expectation of matching the terminal condition (\ref{optimization}). However, in practical computations, the exact expectation is not directly computed. Instead, we approximate it using the Mean Squared Error (MSE) (\ref{BSDE-LOSS}). This approximation results in a generalization error,
\begin{equation}\label{Generalization_Error}
	\begin{aligned}
	\text{Generalization Error}  &	=\mathbb{E} {{\left| {g \left( {{X}_T} \right) - Y_{t_N} } \right|}^2}  - \frac{1}{M}\sum\limits_{m = 1}^M {{{\left| {g\left( {{X_T^m}} \right) -  Y_{{t_N}}^m } \right|}^2}}\\
	& =\mathcal{L}  \left( g \right) - \mathcal{L} \left( g_M \right).
	\end{aligned}
\end{equation}


\begin{lemma}
For Monte Carlo methods $(\ref{system})$ that use i.i.d. samples, the convergence rate of the mean generalization error is
\begin{equation}\label{generalization}
	\mathbb{E}\left[\mathcal{L}(g)-\mathcal{L}(g_M)\right]=O(M^{-1/2+\varepsilon}),
\end{equation}
where $\epsilon$ is arbitrarily small constant, function $g$ satisfes the boundary growth condition $(\ref{boundary_growth})$ for some small constants $(B_i)_{i=1}^d$.  
This result follows directly from the work of Xiao et al. \textsuperscript{\cite{Xiao}}.
\end{lemma}

\begin{definition}
(Boundary growth condition). Let $d\in\mathbb{N}$, suppose $\mathcal{G}$ is a class of realvalued functions defined on $(0,1)^d.$ We say that $\mathcal{G}$ satisfes the boundary growth condition with constants $(B_i)_i=1^d$ if there exists $B\in(0,\infty)$ such that for every $g\in\mathcal{G}$, every subset $v\subseteq\{1,\cdots,d\}$ and every $u=(u_1,\ldots,u_d)\in(0,1)^d$ it holds that
\begin{equation}\label{boundary_growth}
\left|\left(\prod_{i\in v}\partial/\partial x_i\right)g(u)\right|\leq B\prod_{i=1}^d[\min(u_i,1-u_i)]^{-B_i-\mathbf{1}\{i\in v\}},
\end{equation}
where $\mathbf{1}\{ \cdot \} $ is an indicator function.
\end{definition}


When using general Monte Carlo methods to sample trajectories (\ref{Xt}), a significant number of samples is required to reduce the generalization error to an adequately small level. Therefore, it is necessary to introduce techniques such as importance sampling \textsuperscript{\cite{Importance_sampling}}, quasi-Monte Carlo methods \textsuperscript{\cite{QMC}}, Gibbs sampling \textsuperscript{\cite{Gibbs}}, and other advanced sampling techniques. These methods enable the reduction of generalization errors with a smaller sampling cost.


The optimization problem involves minimizing the defined loss function \ref{BSDE-LOSS} by adjusting the network parameters.
Increasing the number of parameters in a neural network typically leads to higher optimization errors. 
The primary reason for this effect is that a larger parameter space requires a more exhaustive search, making it more difficult for the optimization algorithm to converge to the global optimum, ultimately increasing the optimization errors
\textsuperscript{\cite{Parameter_optimization,Shen_Z}}. This phenomenon is commonly referred to as the challenge of escaping saddle points.
Generally, simpler networks are associated with lower optimization errors.

\section{Discrete time models}\label{s4}
In the Deep BSDE (DBSDE) method, we fully connect the $N-1$ steps of the neural networks and train them as a whole. At each time step, two options are available for the neural network architecture. 
For the two-layer networks (Two-layer Net), it consists of four parts: one input layer ($d$-dimensional), two hidden layers (each with $d+10$ dimensions), and one output layer ($d$-dimensional). 
The XNet, on the other hand, consists of three components, including one input layer ($d$-dimensional), a hidden layer comprising $d$ basis functions, and one output layer ($d$-dimensional). 
Subsequently, through the following two examples, this study demonstrates how the Deep BSDE method can improve accuracy and computational efficiency by replacing the Two-layer Network with XNet. It is noteworthy that the numerical results obtained by the Deep BSDE method using the Two-layer Net are based on the code provided by E et al.\textsuperscript{\cite{E2017}}.

\subsection{Allen-Cahn Equation}
In this subsection, the Deep BSDE method is tested for solving the 100-dimensional Allen-Cahn partial differential equation (PDE) (\ref{AllenCahneq}) using both the Two-layer Net and XNet. With reference to the general form of the semilinear parabolic equation (\ref{bxxpwxpde}), we set $\alpha=1$, $f(y,z) = y - y^3$, and $g(x) = \left[2 + \frac{2}{5} |x|_{\mathbb{R}^d}^2\right]^{-1}$. The PDE is represented as follows,
\begin{equation}\label{AllenCahneq}
	\left\{\begin{array}{l}
		\frac{\partial u}{\partial t}(t, x)+\left(\Delta_x u\right)(t, x)+u(t, x)-[u(t, x)]^3=0,(t, x) \in[0, T) \times \mathbb{R}^d, \\
		u(T, x)=g(x), x \in \mathbb{R}^d,
	\end{array}\right.
\end{equation}
where the spatial dimension is $d=100$ and the terminal time $T=\frac{3}{10}$. Using the branching diffusion method \textsuperscript{\cite{FENZHI1,FENZHI2,FENZHI3}}, a reference value for the exact solution is obtained, $u(0, \xi)=u(0,0, \ldots, 0)\approx 0.052802$. 
The Deep BSDE method is implemented by the Two-layer Net and the XNet with setting the time step number to $N=20, 30, 40$, $80$, and conducting five independent runs for each configuration. During the training process, the numerical solution tends to stabilize after approximately 5000 iterations. Therefore, the average of the results from iterations 5000 to 10000 is taken as the computed value function. The results are presented in Table \ref{dt_allen_cahn_results}.

Under a 20-time-step discretization, as shown in Figure \ref{fig_dt_allencahn1}, it is observed that switching to the XNet results in a faster decrease of the loss function and an increase in computational speed. However, the accuracy does not significantly improve. This can be inferred from Equation (\ref{allenth}), where the small value function indicates that the approximation error is dominated by the time discretization rather than the network approximation error,
\begin{equation}\label{allenth}
	\begin{aligned}
		\mathbb{E}\left|\frac{{u(0, \xi)-\theta_{{u_0}}}}{{u(0,\xi)}}  \right|^2\
		\leq &C\left\{ \frac{h}{0.052802^2} +\inf _{\theta_{{u_0}},\theta_{\nabla{u_0}} \in \mathcal{N}_0}  \mathbb{E}\left| Z_0-\theta_{\nabla{u_0}} \right|^2  \frac{h}{0.052802^2} \right.\\
		&\left. +  \inf _{\phi_n \in \mathcal{N}_n} \sum_{i=0}^{N-1} \mathbb{E}\left|\mathbb{E}\left[\tilde{Z}_{t_n} \mid X_{t_n}^\pi, Y_{t_n}^\pi\right]-\phi_n\left(X_{t_n}^\pi, Y_{t_n}^\pi\right)\right|^2 \frac{h}{0.052802^2}\right\}.
	\end{aligned}
\end{equation}

\begin{figure*}
	\centering
	\begin{minipage}[t]{0.493\linewidth}
		\centering
		\includegraphics[width=\textwidth]{dt_allencahn_20_1.png}
		%\centerline{(a) Relative ${L^1}$-approximation error}
		\label{fig_dt_allencahn_20:first}
	\end{minipage}%
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=\textwidth]{dt_allencahn_20_2.png}
		%\centerline{(b) Relative ${L^1}$-approximation error}
		\label{fig_dt_allencahn_20:second}
	\end{minipage}
	\vspace{-0.7cm}
	\centering
	\caption{Comparison of Two Network Architectures for Solving the Allen-Cahn Equation under 20-step-time Discretization}
	\label{fig_dt_allencahn1}
\end{figure*}




As shown in Table \ref{dt_allen_cahn_results}, as the time discretization step size increases and $h$ decreases, the approximation error related to time discretization reduces. In this process, it is observed that the XNet, with its superior approximation capabilities, yields higher accuracy. However, the implementation of the Two-layer Net fails to result in further improvement in the computational results. This phenomenon is likely due to XNet providing smaller network approximation errors and optimization errors for the Deep BSDE method.
As shown in Figure \ref{fig_dt_allencahn2}, when the time-step discretization reaches 80, the results indicate that the Deep BSDE method implemented with the XNet results in a faster decrease in the loss function, higher computational efficiency, and greater accuracy. 
%In fact, we have already achieved very good results. The reason we do not further refine the time discretization is due to concerns that a significant increase in network parameters would lead to optimization errors.



\begin{figure*}
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=\textwidth]{dt_allencahn_80_1.png}
		%\centerline{(a) Relative ${L^1}$-approximation error}
		\label{fig_dt_allencahn_20:first}
	\end{minipage}%
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=\textwidth]{dt_allencahn_80_2.png}
		%\centerline{(b) Relative ${L^1}$-approximation error}
		\label{fig_dt_allencahn_20:second}
	\end{minipage}
	\vspace{-0.7cm}
	\centering
	\caption{Comparison of Two Network Architectures for Solving the Allen-Cahn Equation under 80-step-time discretization}
	\label{fig_dt_allencahn2}
\end{figure*}


\begin{table}[htbp]
	\centering
	\caption{Numerical Results for Allen-Cahn Equation}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{ccccccccc}
			\hline \hline
			& \multicolumn{4}{c}{XNet} & \multicolumn{4}{c}{Two-layer Net} \\
			\midrule
			Time steps & Runtime (s) & value function & Relative Error  & Std. Deviation 
			& Runtime (s) & value function & Relative Error  & Std. Deviation  \\
			\midrule
			20 & 72 & 5.2899e-02 & 1.8337e-03 & 5.7723e-05 & 83&5.2887e-02 & 1.6154e-03 & 6.1286e-05 \\
			30 & 112& 5.2877e-02 & 1.4209e-03 & 7.4003e-05 & 138 & 5.2875e-02 & 1.3807e-03 & 9.6353e-05 \\
			40 & 196 & 5.2846e-02 & 8.3214e-04 & 6.5463e-05 & 260 &5.2849e-02 & 8.8848e-04 & 1.5200e-04 \\
			80 & 464 &5.2820e-02 & 3.4374e-04 & 4.8460e-05 & 691&5.2867e-02 & 1.2309e-03 & 1.7797e-04 \\
			\hline \hline
		\end{tabular}%
	}
	\label{dt_allen_cahn_results}
\end{table}

\subsection{Pricing of European financial derivatives with different interest rates for borrowing and lending(PricingDiffrate) equation}
In this example, we consider a special nonlinear Black-Scholes equation. This equation describes the pricing problem of a European financial derivative in a financial market where the risk-free bank account utilized for hedging purposes exhibits disparate interest rates for borrowing and lending \textsuperscript{\cite{Bergman4}}. Referring to the general form of the semi-linear parabolic PDE (\ref{bxxpwxpde}), we set $\bar{\mu}=0.06$, $\mu(t,x)=\bar{\mu}x$, $\bar{\sigma}=0.2$, $\sigma(t,x)=\bar{\sigma}x$. It is assumed for all $s,t\in[0,T]$, $x=(x_1,\ldots,x_d)\in\mathbb{R}^d$, $y\in\mathbb{R}$, and $z\in\mathbb{R}^{d}$, with $d=100$, $T=1/2$, and $\xi=(100,100,\ldots,100)\in\mathbb{R}^d$. Additionally, a terminal condition $g(x)$ and a non-linear term $f(t,x,y,z)$ are specified for the equation:
\begin{equation}\label{EPDFeqg}
	\begin{array}{l}
		g(x) = \max \left\{ {\left[ {{{\max }_{1 \le i \le 100}}{x_i}} \right] - 120,0} \right\} - 2\max \left\{ {\left[ {{{\max }_{1 \le i \le 100}}{x_i}} \right] - 150,0} \right\},
	\end{array}
\end{equation}
\begin{equation}\label{EPDFeqf}
	\begin{array}{l}
		f(t,x,y,z) =  - {R^l}y - \frac{{\left( {\bar \mu  - {R^l}} \right)}}{{\bar \sigma }}\sum\limits_{i = 1}^d {{z_i}}  + \left( {{R^b} - {R^l}} \right)\max \left\{ {0,\left[ {\frac{1}{{\bar \sigma }}\sum\limits_{i = 1}^d {{z_i}} } \right] - y} \right\},
	\end{array}
\end{equation}
where $R^l=0.04$, $R^b=0.06$. At this point, the equation is subsequently represented on the region $t \in [0, T)$ and $x \in \mathbb{R}^d$:
\begin{equation}\label{EPDFeq}
	\begin{aligned}
		\frac{\partial u}{\partial t}(t,x)
		&+\frac{\bar{\sigma}^{2}}{2}\sum_{i=1}^{d}|x_{i}|^{2} \frac{\partial^{2}u}{\partial x_{i}^{2}}(t,x) +\bar{\mu}\sum_{i=1}^{d}x_{i} \frac{\partial u}{\partial x_{i}}(t,x)\\
		&+f\big(t,x,u(t,x),\bar{\sigma}\operatorname{diag}_{\mathbb{R}^{d\times d}}(x_{1},\ldots,x_{d})(\nabla_{x}u)(t,x)\big)=0.
	\end{aligned}
\end{equation}
The solution of equation \eqref{EPDFeq} is obtained through the Multilevel-Picard approximation method \cite{multilevel-Picard}, which gives a value of $21.299$. 
The Deep BSDE method is implemented by both the Two-layer Net and the XNet with setting the time step number to $N=20, 30, 40, 80$ and conducting five independent runs for each configuration. The average of the results from iterations 5000 to 10000 is taken as the computed value function. The results are presented in Table \ref{dt_diffrate_results}.

\begin{figure*}
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=\textwidth]{dt_diffrate_20_2.png}
		%\centerline{(a) Relative ${L^1}$-approximation error}
		\label{fig_dt_diffrate_80:first}
	\end{minipage}%
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=\textwidth]{dt_diffrate_80_2.png}
		%\centerline{(b) Relative ${L^1}$-approximation error}
		\label{fig_dt_diffrate_80:second}
	\end{minipage}
	\vspace{-0.7cm}
	\centering
	\caption{Comparison of Two Network Architectures for Solving the PricingDiffrate Equation under 20-step-time Discretization and 80-step-time Discretization}
	\label{fig_dt_diffrate2}
\end{figure*}

From equation (\ref{diffrateth}), it is evident that the error introduced by time discretization is minimal, and the approximation error is almost exclusively related to the network's approximation capability.
\begin{equation}\label{diffrateth}
	\begin{aligned}
		\mathbb{E}\left|\frac{{u(0, \xi)-\theta_{{u_0}}}}{{u(0,\xi)}}  \right|^2\
		\leq &C\left\{ \frac{h}{21.299^2} +\inf _{\mu_0^\pi \in \mathcal{N}_0}  \mathbb{E}\left| Z_0-\theta_{\nabla{u_0}} \right|^2  \frac{h}{21.299^2} \right.\\
		&\left. +  \inf _{\phi_n^\pi \in \mathcal{N}_i} \sum_{i=0}^{N-1} \mathbb{E}\left|\mathbb{E}\left[\tilde{Z}_{t_n} \mid X_{t_n}^\pi, Y_{t_n}^\pi\right]-\phi_n\left(X_{t_n}^\pi, Y_{t_n}^\pi\right)\right|^2 \frac{h}{21.299^2}\right\}.
	\end{aligned}
\end{equation}
Therefore, as shown in Table \ref{dt_diffrate_results}, increasing the number of time steps does not lead to significant improvement in computational accuracy for either the XNet or the Two-layer Net.
As shown in Table \ref{dt_diffrate_results} and Figure \ref{fig_dt_diffrate2}, when the time-step discretization $N=20,30,40,80$, implementing the Deep BSDE method using the XNet instead of the Two-layer Net increases computational speed and significantly improves accuracy. We speculate that this is due to XNet providing smaller network approximation errors and optimization errors for the Deep BSDE method.

In fact, we have already achieved very good results. The reason we do not further refine the time discretization is due to concerns that a significant increase in network parameters would lead to non-negligible optimization errors.
In the next section, we propose a continuous-time network structure to replace the need for configuring a network at each time step. This approach mitigates the problem of a significant increase in network parameters due to finer time discretization, thereby potentially reducing further optimization errors.


\begin{table}[htbp]
	\centering
	\caption{Numerical Results for PricingDiffrate Equation}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{ccccccccc}
			\hline \hline
			& \multicolumn{4}{c}{XNet} & \multicolumn{4}{c}{Two-layer Net} \\
			\midrule
			Time steps & Runtime (s)  & Value function & Relative Error  & Std. Deviation  
			& Runtime (s) & value function & Relative Error  & Std. Deviation \\
			\midrule
			20 & 69 & 2.1306e+01 & 3.3219e-04  & 4.2208e-03 & 96 &2.1260e+01 & 1.8144e-03 & 2.4084e-03 \\
			30 & 116 & 2.1302e+01 & 1.5146e-04 & 5.5465e-03 & 198  & 2.1279e+01 & 9.4557e-04 & 4.0696e-03 \\
			40 & 176 & 2.1303e+01 & 1.7159e-04 & 5.1489e-03 & 257 &2.1278e+01& 9.8029e-04 & 4.8573e-03 \\
			80 & 476 & 2.1304e+01 & 2.2257e-04 & 3.9802e-03 & 729&2.1280e+01 & 8.9073e-04 & 3.7753e-03 \\
			\hline \hline
		\end{tabular}%
	}
	\label{dt_diffrate_results}
\end{table}


\section{Continuous time models}\label{s5}
In this paper, the computational error in deep learning algorithms is categorized into four primary components: the approximation error caused by time discretization, the approximation error caused by network approximation, the generalization error determined by the number of training samples, and the optimization error related to the number of network parameters. When the computational error is dominated by the approximation error caused by time discretization, increasing the time steps is necessary to achieve higher computational accuracy. However, in discrete-time network structures, this inevitably increases the number of network parameters, reduces computational efficiency, and increases optimization error.
To address this issue, in this section, we apply the Deep BSDE method using both the XNet and the Two-layer Net within continuous-time network structures. In these continuous-time network structures, the input layer includes an additional time dimension, resulting in a $d+1$-dimensional input. The output is the gradient function at each time step, which is $d$-dimensional. The difference lies in the fact that XNet has only one hidden layer (d-dimensional), while the Two-layer Net comprises two hidden layers (each with $d+10$ dimensions).

\subsection{Allen-Cahn Equation}
In the previous section, when solving the Allen-Cahn Equation (\ref{AllenCahneq}) using discrete-time network structures, it was observed that the accuracy of the algorithm improved with finer time discretization (Table \ref{dt_allen_cahn_results}). However, the increase in network parameters led to higher optimization errors. Here, continuous-time network structures are adopted. On one hand, the XNet has sufficient approximation capability, suggesting that the approximation error due to the network is minimal. Additionally, since the XNet has few parameters, it is assumed that the optimization error is also minimal. On the other hand, we sampled $640,000$ independent trajectories, suggesting that the generalization error is small.
Thus, when we assume that the error in the deep BSDE method is primarily dominated by the approximation error caused by time discretization, we can observe the convergence rate \textsuperscript{\cite{Quarteroni_A}} of the Deep BSDE method with XNet.


\begin{table}
	\centering
	\caption{Numerical Results for Allen-Can Equation}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{ccccccccccc}
			\hline \hline
			& \multicolumn{5}{c}{XNet} & \multicolumn{5}{c}{Two-layer Net} \\
			\midrule
			Time steps & Runtime (s)&Value function & Relative Error & Error order  & Std. Deviation  
			& Runtime (s) &Value function & Relative Error & Error order & Std. Deviation \\
			\midrule
			10 & 47 & 5.3020e-02 & 4.1212e-03 & 
			 & 4.8446e-05 & 42 & 5.3020e-02 & 4.1269e-03 &
			  & 4.5878e-05 \\
			20 & 121 & 5.2906e-02 & 1.9607e-03 & 1.07 & 4.1509e-05 & 138 & 5.2907e-02 & 1.9917e-03 &  1.05 & 9.0286e-05 \\
			40 & 261 & 5.2842e-02 & 7.6389e-04 &1.36
			 & 4.2562e-05 & 367 & 5.2833e-02& 5.9284e-04 &1.75 & 7.4642e-05 \\
			80 & 843 & 5.2810e-02 & 1.5496e-04 & 2.30
			 & 3.3253e-05 & 1452&5.2828e-02 & 4.9686e-04 & 0.26 & 7.4642e-05 \\
			160 & 2004 & 5.2805e-02 & 4.8824e-05 & 1.67
			 & 5.1628e-05 & 3728& 5.2815e-02 & 2.4993e-04 &  0.99
			 & 1.4407e-04 \\
			\hline \hline
		\end{tabular}%
	}
	\label{ct_allen_cahn_results}
\end{table}

From Table \ref{ct_allen_cahn_results} and Figure \ref{fig_ct_allencahn}, it can be observed that the error order of the Deep BSDE method implemented using the XNet is slightly less than second order. Notably, when the number of time steps reaches 40, the Two-layer Net no longer exhibits a discernible error order, whereas the XNet still does. This is likely due to the approximation capabilities of the networks: the XNet possesses superior approximation capabilities, resulting in very small network approximation errors and optimization errors, whereas the Two-layer Net does not exhibit this property.



\begin{figure*}
	\centering
	\begin{minipage}[t]{0.478\linewidth}
		\centering
		\includegraphics[width=\textwidth]{ct_allencahn_3.png}
		%\centerline{(a) Relative ${L^1}$-approximation error}
		\label{fig_ct_allencahn_10:first}
	\end{minipage}%
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=\textwidth]{ct_allencahn_4.png}
		%\centerline{(b) Relative ${L^1}$-approximation error}
		\label{fig_ct_allencahn_160:second}
	\end{minipage}
	\vspace{-0.7cm}
	\centering
	\caption{Results of solving the Allen-Cahn Equation using the Deep BSDE method by XNet under $N$-time-step discretization, with $N=10$, $40$, and $160$.}
	\label{fig_ct_allencahn}
\end{figure*}



With finer time discretization, the accuracy improved by both network implementations. The XNet achieves higher accuracy in a shorter computation time and demonstrates greater robustness.




\subsection{Pricing of European financial derivatives with different interest rates for borrowing and lending(PricingDiffrate) equation}

The Deep BSDE method is also applied to solve the PricingDiffrate equation (\ref{EPDFeq}) with continuous-time implementations of the XNet and the Two-layer Net.
As shown in Table \ref{ct_diffrate_results} and Figure \ref{fig_ct_diffrate1_1}, we can observe that, regardless of the time-step discretization used ($N=10, 20, 40, 80, 160$), the Deep BSDE method implemented with XNet consistently outperforms in terms of both computational speed and accuracy. This result aligns with the findings from the discrete-time models discussed in Section \ref{s4}. 

Although introducing XNet allows the Deep BSDE method to achieve an accuracy close to $9 \times 10^{-5}$, no clear error order is observed in this example. We speculate that the approximation error introduced by time discretization is minimal, and that the computational error is likely dominated by the network's approximation capability or training error. To observe the convergence rate, it is crucial to ensure that the  approximation errors, optimization errors, and training errors are all minimized.
To this end, we increase the batch size with the expectation of reducing the generalization errors, and we also increase the number of basis functions in XNet to enhance the network's approximation capability.

\begin{figure*}
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=\textwidth]{ct_diffrate_3.png}
		%\centerline{(a) Relative ${L^1}$-approximation error}
		\label{fig_ct_diffrate_20:first}
	\end{minipage}%
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=\textwidth]{ct_diffrate_4.png}
		%\centerline{(b) Relative ${L^1}$-approximation error}
		\label{fig_ct_diffrate_20:second}
	\end{minipage}
	\vspace{-0.7cm}
	\centering
	\caption{Comparison of Two Network Architectures for Solving the PricingDiddrate under 10-time-step and 160-time-step Discretization}
	\label{fig_ct_diffrate1_1}
\end{figure*}

\begin{table}[htbp]
	\centering
	\caption{Numerical Results for solving PricingDiffrate Equation by XNet and Two-layer Net}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{cccccccccccc}
			\hline \hline
			& \multicolumn{5}{c}{XNet} & \multicolumn{5}{c}{Two-layer Net} \\
			\midrule
			Time steps & Runtime (s)  & value function & Relative Error& Error order  & Std. Deviation  
			& Runtime (s) & Value function & Relative Error &Error order & Std. Deviation \\
			\midrule
			10 & 55 & 2.1305e+01 & 2.6617e-04 & & 2.7172e-03 & 96 &2.1219e+01 & 3.7457e-03 & & 3.2896e-03 \\
			20 & 70 & 2.1296e+01 & 1.5515e-04 & 0.78& 2.5540e-03 & 142  & 2.1228e+01 & 3.3429e-03 & 0.16& 2.9154e-03 \\
			40 & 151 & 2.1302e+01 & 1.2577e-04 & 0.30&3.5567e-03 & 273 &2.1238e+01 & 2.8748e-03 &0.22 & 2.5155e-03 \\
			80 & 506 &  2.1301e+01 & 9.7900e-05 &0.36 & 1.4253e-03 & 729&2.1224e+01 & 3.5309e-03 & -0.30& 2.0813e-03 \\
			160 & 1558 & 2.1297e+01 & 9.0625e-05 &0.11 & 2.3293e-03 & 2736&2.1215e+01 & 3.9331e-03 & -0.16& 2.0182e-03 \\
			\hline \hline
		\end{tabular}%
	}
	\label{ct_diffrate_results}
\end{table}



As shown in Table \ref{ct_diffrate_results_2} and Figure \ref{fig_ct_diffrate2}, with a time-step discretization of $20$ or $80$, we observe the following:
On one hand, when the batch size reaches 256, the generalization error caused by the training samples no longer significantly contributes to the overall computational error.
On the other hand, by increasing the number of basis functions in XNet, it can be observed that the accuracy can be further improved.



\begin{figure*}
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=\textwidth]{ct_diffrate_80_4.png}
		%\centerline{(a) Relative ${L^1}$-approximation error}
		\label{fig_ct_diffrate_20:first}
	\end{minipage}%
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=\textwidth]{ct_diffrate_80_3.png}
		%\centerline{(b) Relative ${L^1}$-approximation error}
		\label{fig_ct_diffrate_20:second}
	\end{minipage}
	\vspace{-0.7cm}
	\centering
	\caption{Solving the PricingDiddrate Equation by XNet under various settings with 20-step-time Discretization and 80-step-time Discretization.}
	\label{fig_ct_diffrate2}
\end{figure*}

As shown in Table \ref{ct_diffrate_results_2}, by increasing the number of basis functions in XNet to 200, we can clearly observe that the computational accuracy improves consistently as the time-step discretization increases. When the time-step discretization reaches 80, the relative error of the Deep BSDE method with XNet can reach $3.2679 \times 10^{-5}$, which far surpasses the accuracy achieved with the Two-layer Net ($3.9331 \times 10^{-3}$). 
\begin{table}[htbp]
	\centering
	\caption{Numerical Results for solving PricingDiffrate Equation by XNet}
	\resizebox{0.9\textwidth}{!}{%
		\begin{tabular}{cccccccc}
			\hline \hline
			Steps & Batch size & Basis Functions & Runtime (s)  & value function & Relative Error &Error order & Std. Deviation \\
			\midrule
			20 &64 & 100 & 70 & 2.1296e+01 & 1.5515e-04 & & 2.5540e-03 \\
			20 &256 & 100 & 98 & 2.1304e+01 & 2.4130e-04 & & 4.4412e-03 \\
			10 & 64 & 200 & 44 & 2.1295e+01 & 2.0725e-04 & & 3.6821e-03  \\
			20 & 64 & 200 & 98 & 2.1301e+01 & 8.9936e-05 & 1.20& 3.0920e-03 \\
			40 & 64 & 200 & 207 & 2.1298e+01 & 6.4311e-05 &0.48 & 1.2050e-03 \\
			80 & 64 & 200 & 670 &2.1300e+01 & 3.2679e-05 &0.98 & 2.5948e-03\\
			%160 & 64 & 200 & 1755 & 2.1300e+01 & 3.2442e-05 & &1.4426e-03 \\
			\hline \hline
		\end{tabular}%
	}
	\label{ct_diffrate_results_2}
\end{table}


\clearpage
%\phantomsection 
%\addcontentsline{toc}{section}{References}
%\begin{thebibliography}{99}
%\end{thebibliography}

\bibliography{refbib}
\bibliographystyle{siam} 


\end{document}
	