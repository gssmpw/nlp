%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[shortlabels]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx} % Required for figures
%\usepackage{subfigure}
%\usepackage{subfig} % Provides the \subfloat command
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
%, please comment out the following use package line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}
\usepackage{bbm}
 % Load without options to avoid conflicts

\input{packages}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}

% if you use cleverer.
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{wrapfig} % Add this to your 
\usepackage[a4paper,left=1in,right=1in,top=1in,bottom=1in]{geometry}

\title{Fused Partial Gromov-Wasserstein for Structured Objects}
\date{}

\usepackage{authblk}


\author[1]{Yikun Bai}
\author[1]{Huy Tran}
\author[3]{Hengrong Du}
\author[1]{\\Xinran Liu}
\author[1]{Soheil Kolouri}
\affil[1]{Department of Computer Science, Vanderbilt University}
\affil[3]{Department of Mathematics, University of California, Irvine}


\affil[1]{\href{mailto:yikun.bai@vanderbilt.edu}{yikun.bai}, \href{mailto:huy.tran@vanderbilt.edu}{abi.kothapalli}, \href{mailto:xinran.liu@vanderbilt.edu}{xinran.liu}, \href{mailto:soheil.kolouri@vanderbilt.edu}{soheil.kolouri@vanderbilt.edu}}
\affil[3]{\href{mailto:hengrond@uci.edu}{hengrond@uci.edu}}


\begin{document}
\maketitle

\begin{abstract}
Structured data, such as graphs, are vital in machine learning due to their capacity to capture complex relationships and interactions. In recent years, the Fused Gromov-Wasserstein (FGW) distance has attracted growing interest because it enables the comparison of structured data by jointly accounting for feature similarity and geometric structure. However, as a variant of optimal transport (OT), classical FGW assumes an equal mass constraint on the compared data. In this work, we relax this mass constraint and propose the Fused Partial Gromov-Wasserstein (FPGW) framework, which extends FGW to accommodate unbalanced data. Theoretically, we establish the relationship between FPGW and FGW and prove the metric properties of FPGW. Numerically, we introduce Frank-Wolfe solvers for the proposed FPGW framework and provide a convergence analysis. Finally, we evaluate the FPGW distance through graph classification and clustering experiments, demonstrating its robust performance, especially when data is corrupted by outlier noise.

% The code for reproducing all the numerical results is available in the anonymous repository at \url{https://anonymous.4open.science/r/fused-pgw-041B}.
\end{abstract}

% \begin{figure}[t]
%     \centering
%    % \vspace{0.2in}
% \includegraphics[width=\columnwidth]{images/Main_Fig1.pdf}
%     \vspace{-0.3in}
%     \caption{We present an intuitive understanding for the fused-PGW problems \eqref{eq:fpgw_orig} and \eqref{eq:fmpgw}. The distance between the features of nodes is modeled as $d(\cdot,\cdot)$. The structure information for each graph is described by their graph shortest path distance  $C_1(\cdot,\cdot)$ and $C_2(\cdot,\cdot)$. }
%    % \vspace{-0.3in}
%     \label{fig:fig1}
% \end{figure}

\section{Introduction}

\begin{wrapfigure}{r}{0.5\textwidth} % 'r' for right, adjust width as needed
    \centering
    \vspace{-0.2in}
    \includegraphics[width=\linewidth]{images/Main_Fig1.pdf}
    \vspace{-0.3in}
    \caption{An intuitive understanding of the fused-PGW problems \eqref{eq:fpgw_orig} and \eqref{eq:fmpgw}. The distance between node features is modeled as \( d(\cdot,\cdot) \). The structural information of each graph is represented by their shortest path distances, \( C_1(\cdot,\cdot) \) and \( C_2(\cdot,\cdot) \).}
    \label{fig:fig1}
\end{wrapfigure}

Analyzing structured data, which combines feature-based and relational information, is a longstanding challenge in machine learning, data science, and statistics. One classical type of structure-based data is the graph, where the nodes with attributes can model the data feature while the edges can describe the structure. Examples of such data structures are abundant, including molecular graphs for drug discovery \cite{ruddigkeit2012enumeration}, functional and structural brain networks \cite{bassett2017network}, and social network graphs \cite{hamilton2017inductive}. Beyond graphs, structured data encompasses a wide variety of domains, such as sequences \cite{graves2006connectionist}, hierarchical structures such as trees \cite{bille2008integrated}, and even pixel-based data, such as images \cite{wang2004integration}.  

In recent years, the Optimal Transport (OT) distance \cite{Villani2009Optimal} and its extensions, including unbalanced Optimal Transport \cite{chizat2018unbalanced,figalli2010optimal}, linear Optimal Transport \cite{wang2013linear,cai2022linearized,bai2023linear,martin2023lcot}, sliced optimal transport \cite{kolouri2019generalized,bonneel2015sliced,bai2022sliced} have been widely used in machine learning tasks due to their capacity to measure the similarity between datasets. Based on the classical OT,  Gromov-Wasserstein problem and its unbalanced extension \cite{memoli2011gromov,memoli2009spectral,sejourne2021unbalanced,chapel2020partial,bai2024efficient,kong2024outlier} has been proposed, which can capture the inherent structure of the data. 

Classical OT can incorporate the features of data to measure the similarity and
Gromov-Wasserstein formulations capture the structure information. Inspired by
these works, fused Gromov-Wasserstein \cite{feydy2017optimal,vayer2020fused},
which can be treated as a ``linear combination'' of classical OT and
Gromov-Wasserstein has been proposed in recent years to analyze the structured
feature data. Despite its successful applications in graph data
analysis, similar to classical OT, fused GW formulation requires equal mass
constraint. To address this issue, in recent years, Fused Unbalanced Gromov
Wasserstein (FUGW) \cite{thual2022aligning} and related Sinkhorn solvers
have been proposed and applied in brain image analysis. However, FUGW
relies on the Sinkhorn solver, and its metric property is still unclear. To
address this limitation and complete the theoretical gap, in this paper, we
introduce the fused-partial Gromov-Wasserstein formulations: 

\begin{itemize}
    \item We introduce the fused partial Gromov Wasserstein formulations \eqref{eq:fmpgw},\eqref{eq:fpgw}, which allow the comparison of structured objects with unequal total mass. Theoretically, we demonstrate that the FPGW admits a (semi-)metric. 
    \item Numerically, we propose the related Frank-Wolfe algorithms to solve the FPGW problem. In addition, we present the FPGW barycenter and related computational solvers. 
    \item We applied FPGW in graph classification and clustering experiments and demonstrated that when these structured objects are corrupted by outliers, FPGW-based methods admit more robust performance. 
 \end{itemize}

 

% \section{Related Work}
% paragraph 1 GW, UGW, fusedGW, fused-UGW.

% paragraph 2, graph data analysis. 


\section{Background: Gromov-Wasserstein (GW) Problems}\label{subsec: GW}
\vspace{-0.5em}
Note, graph-structured data can be measured as a metric measure space (mm-space) consisting of a set $X$ endowed with a metric structure, that is, a notion of distance $d_X$ between its elements, and equipped with a Borel measure $\mu$.  
As in \cite[Ch. 5]{memoli2011gromov}, we will assume that $X$ is compact and that  $\operatorname{supp}(\mu)=X$.
Given two probability mm-spaces $\mathbb{X}=(X,d_X,\mu)$, $\mathbb{Y}=(Y,d_Y,\nu)$, with $\mu\in \mathcal{P}(X)$ and $\nu\in\mathcal{P}(Y)$, and a non-negative lower semi-continuous cost function  $L: \mathbb{R}^2\to \mathbb{R}_+$  (e.g., the Euclidean distance or the KL-loss),  the Gromov-Wasserstein (GW) matching problem is defined as:
\begin{equation}
GW_{r,L}(\mathbb{X},\mathbb{Y}):=\inf_{\gamma\in \Gamma(\mu,\nu)}\gamma^{\otimes 2}(L(d_X^r(\cdot,\cdot),d_Y^r(\cdot,\cdot))), %=\inf_{\gamma\in \Gamma(\mu,\nu)}\int_{(X\times Y)^2}L(d_X(x,x'),d_Y(y,y')) 
%\, d\gamma(x,y)d\gamma(x',y'),
\label{eq:gw}
\end{equation}
where $r\ge 1$ and 
\begin{align}
\Gamma(\mu,\nu):=\{\gamma\in\mathcal{P}(X\times Y):\gamma_1=\mu,\gamma_2=\nu\}\label{eq:Gamma_=}.
\end{align}
For brevity, we employ the notation $\gamma^{\otimes 2}$ for the product measure $d\gamma^{\otimes 2}((x,y),(x',y'))=d\gamma(x,y)d\gamma(x',y')$. %$1\leq p,q<\infty$, and 
%$d_X^q(\cdot,\cdot)=d_X^q(\cdot,\cdot)$, $d_Y^q(\cdot,\cdot)=d_Y^1(\cdot,\cdot)$ is defined similarly. 
If 
$L(a,b)=|a-b|^q$, 
%$L(\cdot,\cdot)=D^p(\cdot,\cdot)$, where 
for $1\leq q< \infty$, %and $D(\cdot,\cdot)$ is a metric in $\mathbb{R}$, we use the notation $GW_q^p(\mathbb{X},\mathbb{Y})$ for \eqref{eq:gw}. 
we denote $GW_{r,L}(\cdot,\cdot)$ by $d_{GW,r,q}^q(\cdot,\cdot)$.
In this case, the expression \eqref{eq:gw} defines an equivalence relation $\sim$ among probability mm-spaces, i.e.,  $\mathbb{X}\sim\mathbb{Y}$ if and only if  $d_{GW,r,q}(\mathbb{X},\mathbb{Y})=0$\footnote{Moreover, given two probability mm-spaces $\mathbb{X}$ and $\mathbb{Y}$, $d_{GW,r,q}(\mathbb{X},\mathbb{Y})=0$ if and only if there exists a bijective isometry $\phi:X\to Y$ such that $\phi_\#\mu=\nu$. 
In particular, the GW distance is invariant under rigid transformations (translations and rotations) of a given probability mm-space.}. A minimizer of the GW problem \eqref{eq:gw} always exists, and thus, we can replace $\inf$ by $\min$. Moreover,  similar to OT, the above GW problem defines a distance for probability mm-spaces after taking the quotient under $\sim$. For details, we refer to \cite[Ch. 5 and 10]{memoli2011gromov,bai2024efficient}.

Classical GW requires an equal mass assumption, i.e., $|\mu|=|\nu|$, which limits its application in many machine learning tasks, e.g., positive unsupervised learning \cite{chapel2020partial,sejourne2023unbalanced}. To address this issue, in recent years, the above formulation has been extended to the unbalanced setting \cite{chapel2020partial,sejourne2023unbalanced,bai2024efficient,bai2023linear}. 
In particular, two equivalent extensions of the Gromov-Wasserstein problem, named \textbf{Partial Gromov-Wasserstein problem} and \textbf{Mass-constrained Partial Gromov-Wasserstein problem} have been proposed: 
\begin{align}
&PGW_{r,L}(\mathbb{X},\mathbb{Y})=\inf_{\gamma\in \Gamma_\leq(\mu,\nu)}\gamma^{\otimes2}(L(d_X^r,d_Y^r))\nonumber\\
&\quad+\lambda(|\mu^{\otimes2}-\gamma_1^{\otimes2}|+|\nu^{\otimes2}-\gamma_2^{\otimes2}|)\label{eq:PGW}\\
&MPGW_{r,L}(\mathbb{X},\mathbb{Y})=\inf_{\gamma\in \Gamma_\leq^\rho(\mu,\nu)}\gamma^{\otimes2}(L(d_X^r,d_Y^r))\label{eq:MPGW}.
\end{align}
where 
\begin{align}
&\Gamma_\leq(\mu,\nu):=\{\gamma\in\mathcal{M}_+(X\times Y): \gamma_1\leq \mu,\gamma_2\leq \nu\}   \label{eq:Gamma_leq},\\
&\Gamma_\leq^\rho (\mu,\nu):=\{\gamma\in\Gamma_\leq(\mu,\nu):|\gamma|=\rho\}, \label{eq:Gamma_leq^rho}\\
&\text{with }\rho\in[0,\min(|\mu|,|\nu|)],\nonumber
\end{align}
and the notation $\gamma_1\leq \mu$ denotes that for each Borel set $B\subset X$, $\gamma_1(B)\leq \mu(B)$. 

Note that both GW and its unbalanced extension can measure the similarity of structure data by utilizing their in-structure distance $d_X$ and $d_Y$. The above formulations can naturally measure similarity between graph data $(X,E_X), (Y,E_Y)$,  since we can define $d_X$ (and $d_Y$) by the (weights of) edges $E_X$ (and $E_Y$). 
However, suppose that nodes $X, Y$ contain features (e.g., attributed graphs), and thus we can define the distance between $(x,y)$ where for each pair of nodes $x\in X,y\in Y$,  classical GW/PGW can not incorporate this information. To address this limitation, \textbf{Fused Gromov-Wasserstein distance} has been proposed.

\subsection{Fused Gromov-Wasserstein problem}
Given two $mm-$spaces $\mathbb{X},\mathbb{Y},\omega_1,\omega_2\ge 0$ with $\omega_1+\omega_2=1$, a cost function $C:X\times Y\to \mathbb{R}_+$, 
the fused Gromov-Wasserstein problem is defined as: 
{\small
\begin{align}
FGW_{r,L}(\mathbb{X},\mathbb{Y}):=\inf_{\gamma\in\Gamma(\mu,\nu)}\omega_1\gamma(C)+\omega_2 \gamma^{\otimes2}(L(d_X^r,d_Y^r))\label{eq:fgw}. 
\end{align}}
Similar to the original GW problem, the above problem admits a minimizer. In addition, when $C(x,y)=\|x-y\|^q$, and $L(\cdot_1,\cdot_2)=|\cdot_1-\cdot_2|^q$, it defines a semi-metric \cite{titouan2019optimal}. 

\subsection{Fused Unbalanced Gromov-Wasserstein problem.}
The above formulation relies on the equal mass assumption, i.e., $|\mu|=|\nu|$. By relaxing this constraint, the authors in \cite{thual2022aligning} have proposed the following fused-UGW problem: 
{\footnotesize
\begin{align}
&FUGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y}):=\inf_{\gamma\in\mathcal{M}_+(X\times Y)} \omega_1\gamma( C)+\omega_2\gamma^{\otimes2}(L(d_X^r,d_Y^r))\nonumber\\
&\quad+\lambda(D_{\phi_1}(\gamma_1^{\otimes2}\parallel \mu^{\otimes 2})+D_{\phi_2}(\gamma_2^{\otimes2}\parallel \nu^{\otimes2})) \label{eq:fugw},  
\end{align}}
where $\mathcal{M}_+(X\times Y)$ denotes the set of all positive Radon measures defined on $X\times Y$, $D_{\phi_i},i\in[1:2]$ are the f-divergence terms. 
In \cite{thual2022aligning}, the authors set 
$D_{\phi_i},i\in[1:2]$ as KL divergence. They adapt entropic regularization and propose the related Sinkhorn solver. 
\begin{figure} %{r}{0.5\textwidth} % 'r' for right, adjust width as needed
    \centering
    \includegraphics[width=0.6\linewidth]{images/toy_example/example.png}
    \caption{This toy example illustrates the relationships and differences between OT, Unbalanced OT, Partial OT, GW, Unbalanced GW, Partial GW, fused GW, fused unbalanced GW, and fused partial GW. The source shape consists of the union of the pink and red points, while the target shape is the grey shape. The objective is to establish a reasonable matching between the two shapes.}
    \label{fig:toy_example}
\end{figure}



\section{Fused Partial Gromov-Wasserstein problem}
Inspired by these previous works, by setting the f-divergence terms $D_{\phi_i}$ to be the Total variation, we propose the following ``fused partial Gromov Wasserstein problem'', and the corresponding mass-constrained version: 
{\footnotesize
\begin{align}
&FPGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y})=\inf_{\gamma\in\mathcal{M}_+(X\times Y)} \omega_1  \gamma(C)+\omega_2 \gamma^{\otimes2}(L(d_X^r,d_Y^r))\nonumber\\
&\qquad\qquad+\lambda(|\mu^{\otimes 2}-\gamma_1^{\otimes2}|_{TV}+|\nu^{\otimes2}-\gamma_2^{\otimes2}|_{TV})
\label{eq:fpgw_orig}.\\
&FMPGW_{r,L,\rho}(\mathbb{X},\mathbb{Y})=\inf_{\gamma\in\Gamma^\rho_\leq(\mu,\nu)} \omega_1 \gamma(C)+\omega_2\gamma^{\otimes2}(L(d_X^r,d_Y^r))\label{eq:fmpgw}.
\end{align}
}
where $\lambda\ge 0$. 

\begin{theorem}\label{thm:main} We have the following: 
\begin{enumerate}[(1)]
\item  When $C(x,y)\ge 0,\forall x,y, L(\mathrm{r}_1,\mathrm{r}_2)\ge 0,\forall \mathrm{r}_1,\mathrm{r}_2\in \mathbb{R}$, the problem \eqref{eq:fpgw_orig} can be further simplified as 
{\footnotesize
\begin{align}
&FPGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y})=\inf_{\gamma\in\Gamma_\leq(\mu,\nu)} \omega_1\gamma(C)\nonumber\\
&+\omega_2\gamma^{\otimes2}(L_{d_X^r,d_Y^r})+\lambda(|\mu|^2+|\nu|^2-2|\gamma|^2)\label{eq:fpgw}
\end{align}
}
\item The problems \eqref{eq:fpgw_orig},\eqref{eq:fpgw} and \eqref{eq:fmpgw} admit minimizer $\gamma$.
\item When $C(x,y)=|x-y|^q,L(\cdot_1,\cdot_2)=|\cdot_1-\cdot_2|^q$, $\omega_2,\lambda>0$, the above formulation  \eqref{eq:fpgw} admits a semi-metric. Furthermore, when $q=1$, \eqref{eq:fpgw} defines a metric. 
\end{enumerate}
\end{theorem}
\begin{remark}
In Figure~\ref{fig:toy_example}, we illustrate the transportation plans of various methods, including optimal transport (OT), unbalanced optimal transport (UOT), partial optimal transport (POT), $\ldots$ and fused-Partial Gromov-Wasserstein (FPGW). 

Due to the balanced mass setting, OT, GW, and fused GW match all points from the source shape to the target shape. In contrast, the Sinkhorn entropic regularization in UOT, UGW, and FUGW allows for mass splitting, resulting in a small amount of mass between the grey shape and the pink points. PGW and fused PGW achieve the desired one-to-one matching, where the straight-line segment in the source shape (red points) corresponds to the straight-line segment in the target shape (grey points). However, since PGW does not account for the spatial location of points, there is a 50\% chance of obtaining ``anti-identity'' matching as demonstrated in Figure~\ref{fig:toy_example}.
 
\end{remark}


% \begin{figure*}
% \centering   
% \subfigure[width=0.78\columnwidth]{
% \includegraphics[width=0.78\columnwidth]{images/clustering/fgw.png}
% }
% % \subfloat[FGW-Kmean]{\includegraphics[width=0.78\columnwidth]{images/clustering/fgw.png}}  
% % \subfloat[FPGW-Kmean]{\includegraphics[width=0.78\columnwidth]{images/clustering/fgw.png}} 
  
%   %  \vspace{-0.2in}
%     \caption{aaa}
%     \label{fig:enter-label}
% \end{figure*}




\subsection{Discrete FPGW and Frank-Wolfe algorithm.}
In discrete case, say $\mu=\sum_{i=1}^np_i\delta_{x_i},\nu=\sum_{i=1}^mq_j\delta_{y_j}$. Let $C^X=[d_X(x_i,x_{i'})]_{i,i'}\in \mathbb{R}^{n\times n},C^Y=[d_Y(y_j,y_{j'})]_{j,j'}$. Thus, $(C^X,\mu),(C^Y,\nu)$ can represent the mm-spaces $\mathbb{X}=(X,d_X,\mu),\mathbb{Y}=(Y,d_Y,\nu)$, respectively. 

\subsection{Computation of FMPGW}
The above FMPGW problem \eqref{eq:fmpgw} becomes the following: 
\begin{align}
FMPGW_\rho(\mathbb{X},\mathbb{Y})=\min_{\gamma\in\gamma_\leq^\rho(p,q)}\underbrace{\omega_1\langle C,\gamma \rangle+\omega_2\langle M\circ \gamma,\gamma}_{\mathcal{L}_{C,M}} \rangle \label{eq:fmpgw_empirical} 
\end{align} 
where $\Gamma_\leq^\rho(\mathrm{p},\mathrm{q}):=\{\gamma\in\mathbb{R}_+^{n\times m}:\gamma 1_m\leq \mathrm{p}, \gamma^\top 1_n\leq \mathrm{q}, |\gamma|\ge \rho \}$, 
$C=[C(x_i,y_j)]_{i\in[1:n],j\in[1:m]}$, $M_{i,j,i',j'}:=L(C^X,C^Y):=[L(C^X_{i,i'},C^Y_{j,j'})]_{i,i\in[1:n],j,j'\in[1:n]}$, $M\circ \gamma=[\langle M[i,j,\cdot,\cdot],\gamma\rangle]_{i,j}$.

Similar to the Fused Gromov-Wasserstein problem, we propose the following Frank-Wolfe algorithm as a solver: The above problem will be solved iteratively. In every iteration, say $k$, we will adapt the following steps: 

\textbf{Step 1. Gradient computation.}

Suppose $\gamma^{(k-1)}$ is the transportation plan in the previous iteration; it is straightforward to verify: 
$$\nabla\mathcal{L}_{C,M}(\gamma)=\omega_1 C+\omega_2((M+M^\top)\circ\gamma),$$
where $M^\top=[M_{i',j',i,j}]_{i,i'\in[1:n],j,j'\in[1:m]}\in\mathbb{R}^{n\times m\times n\times m}$.
Next, we aim to find $\gamma\in\Gamma_\leq^\rho(\mu,\nu)$ for the following problem: 
\begin{align}
\gamma^{(k)}{'}:=\arg\min_{\gamma\in\Gamma_\leq^\rho(\mu,\nu)} \langle \nabla\mathcal{L}_{C,M}(\gamma^{(k-1)}),\gamma\rangle\label{eq:fmpgw_step1}. 
\end{align}
which is a mass-constraint partial OT problem. 

\textbf{Step 2. linear search algorithm.}
In this step, we aim to find the optimal step size $\alpha^{(k)}\in[0,1]$. In particular,  
\begin{align}
\alpha^{(k)}:=\arg\min_{\alpha\in[0,1]}\mathcal{L}_{C,M}((1-\alpha)\gamma^{(k-1)}+\alpha\gamma^{(k)}{'})\nonumber. 
\end{align}

Let  $\delta\gamma=\gamma^{(k)}{'}-\gamma^{(k-1)}$, the above problem is essentially quadratic problem: 
\begin{align}
&\mathcal{L}((1-\alpha)\gamma^{(k-1)}+\alpha\gamma^{(k)}{'})
=\alpha^2\underbrace{\langle \omega_2M\circ \delta\gamma,\delta\gamma\rangle}_{a}\nonumber\\
&+\alpha \underbrace{\langle \omega_2(M+M^\top)\circ \gamma^{(k-1)}+\omega_1C,\delta\gamma\rangle }_{b}\nonumber\\
&+\underbrace{\langle \omega_2M\circ\gamma^{(k-1)}+\omega_1C,\gamma\rangle}_c \label{eq:abc_1}
\end{align}
and $\alpha^*$ is given by 
\begin{align}
\alpha^*=\begin{cases}
    1 &\text{if } a\leq 0, a+b\leq 0, \\
    0 &\text{if }a\leq 0, a+b>0\\
    \text{clip}(\frac{-b}{2a},[0,1]), &\text{if }a>0 
\end{cases}\label{eq:line_search_sol_1}. 
\end{align}
Then $\gamma^{(k)}=(1-\alpha^*)\gamma^{(k-1)}+\alpha^*\gamma^{(k)}{'}$. 

\begin{algorithm}[bt]
\caption{Frank-Wolfe Algorithm for FMPGW}
   \label{alg:fpgw}
\begin{algorithmic}
   \STATE {\bfseries Input:} $C\in \mathbb{R}^{n\times m}, C^X\in \mathbb{R}^{n\times n},C^Y\in \mathbb{R}^{m\times m}, \mathrm{p}\in \mathbb{R}^n_+, \mathrm{q}\in\mathbb{R}^m_+$, $\omega_1\in[0,1],\rho\in [0, \min(|\mathrm{p}|,|\mathrm{q}|)]$.
   %\REPEAT
   \STATE {\bfseries Output:}
$\gamma^{(final)}$

\FOR{$k=1,2,\ldots$}
   \STATE 
$G^{(k)}\gets \omega_1C+\omega_2 (M+M^\top)\circ \gamma^{(k)}$ // Compute gradient 
   \STATE $\gamma^{(k)}{'}\gets \arg\min_{\gamma\in \Gamma^\rho_\leq(\mathrm{p},\mathrm{q})}\langle G^{(k)}, \gamma\rangle_F$ // Solve the POT problem.  \\ 
   \STATE Compute $\alpha^{(k)}\in[0,1]$ via  \eqref{eq:line_search_sol_1} //  Line Search
   \STATE $\gamma^{(k+1)}\gets (1-\alpha^{(k)})\gamma^{(k)}+\alpha^{(k)} \gamma^{(k)'}$//  Update $\gamma$  
   \STATE if convergence, break
   \ENDFOR
\STATE $\gamma^{(final)}\gets \gamma^{(k)}$
   %\UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}


\subsection{Computation of FPGW}
The above FPGW problem \eqref{eq:fpgw} becomes the following: 
\begin{align}
FPGW_\rho(\mathbb{X},\mathbb{Y})=&\min_{\gamma\in\gamma_\leq^\rho(p,q)}\underbrace{\omega_1\langle C,\gamma \rangle+\omega_2\langle (M-2\lambda)\circ \gamma,\gamma}_{\mathcal{L}_{C,M-2\lambda}} \rangle \nonumber\\
&\qquad+\underbrace{\lambda(|\mu|^2+|\nu|^2)}_{\text{constant}} \rangle \label{eq:fmpgw_empirical_1} 
\end{align} 
where $C, M$ are defined in the previous subsection, $M-2\lambda$ denote the elementwise subtraction, and the constant term will be ignored in the remainder of the paper. 

Similarly to the Fused Gromov-Wasserstein problem, we propose the following Frank-Wolfe algorithm as a solver: The above problem will be solved iteratively. In every iteration, say $k$, we will adapt the following steps: 

\textbf{Step 1. Gradient computation.}

Suppose $\gamma^{(k-1)}$ is the transportation plan in the previous iteration; it is straightforward to verify: 
$$\nabla\mathcal{L}_{C,M-2\lambda}(\gamma)=\omega_1 C+\omega_2(M+M^\top-4\lambda)\circ\gamma.$$
Next, we aim to find the optimal  $\gamma\in\Gamma_\leq(\mu,\nu)$ for the following partial OT problem: 
\begin{align}
\gamma^{(k)}{'}:=\arg\min_{\gamma\in\Gamma_\leq(\mu,\nu)} \langle \nabla\mathcal{L}_{C,M-2\lambda}(\gamma^{(k-1)}),\gamma^{k}\rangle\label{eq:fmpgw_step1_0}. 
\end{align}
which is a partial OT problem. 

\textbf{Step 2. linear search algorithm.}
In this step, we aim to find the optimal step size $\alpha^*\in[0,1]$. In particular,  
\begin{align}
\alpha^*:=\arg\min_{\alpha\in[0,1]}\mathcal{L}_{C,M-2\lambda}((1-\alpha)\gamma^{(k-1)}+\alpha\gamma^{(k)}{'})\nonumber. 
\end{align}

Let  $\delta\gamma=\gamma^{(k)}{'}-\gamma^{(k-1)}$, the above problem is essentially quadratic problem: 
{\footnotesize
\begin{align}
&\mathcal{L}((1-\alpha)\gamma^{(k-1)}+\alpha\gamma^{(k)}{'})
=\alpha^2\underbrace{\omega_2\langle (M-2\lambda)\circ \delta\gamma,\delta\gamma\rangle}_{a}\nonumber\\
&+\alpha \underbrace{\langle \omega_2(M+M^\top-4\lambda)\circ \gamma^{(k-1)}+\omega_2C,\delta\gamma\rangle }_{b}\nonumber\\
&+\underbrace{\omega_2\langle (M-2\lambda)\circ\gamma^{(k-1)}+\omega_2C,\gamma\rangle}_c \label{eq:abc_2}
\end{align}
}
and $\alpha^*$ is given by \eqref{eq:line_search_sol_1}.

\begin{algorithm}[bt]
   \caption{Frank-Wolfe Algorithm for FPGW}
   \label{alg:fmpgw}
\begin{algorithmic}
\STATE 
  {\bfseries Input:} $C\in \mathbb{R}^{n\times m}, C^X\in \mathbb{R}^{n\times n},C^Y\in \mathbb{R}^{m\times m}, p\in \mathbb{R}^n_+, q\in\mathbb{R}^m_+$, $\omega_2\in[0,1],\lambda\ge 0$.
   %\REPEAT
   \STATE {\bfseries Output:}
$\gamma^{(final)}$

   \FOR{$k=1,2,\ldots$}
   \STATE 
$G^{(k)}\gets \omega_1C+\omega_2 (M+M^\top-4\lambda)\circ \gamma^{(k)}$ // Compute gradient 
   \STATE $\gamma^{(k)}{'}\gets \arg\min_{\gamma\in \Gamma_\leq(\mathrm{p},\mathrm{q})}\langle G^{(k)}, \gamma\rangle_F$ // Solve the POT problem.  \\ 
   \STATE Compute $\alpha^{(k)}\in[0,1]$ via  \eqref{eq:line_search_sol_1} //  Line Search
   \STATE $\gamma^{(k+1)}\gets (1-\alpha^{(k)})\gamma^{(k)}+\alpha^{(k)} \gamma^{(k)'}$//  Update $\gamma$  
   \STATE if convergence, break
   \ENDFOR
\STATE $\gamma^{(final)}\gets \gamma^{(k)}$
   %\UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}

\subsection{Complexity and summary.}
In algorithm \eqref{alg:fpgw} and \eqref{alg:fpgw}, the computational complexity can be written as $\mathcal{O}(\mathrm{C}\cdot \mathrm{L})$, where $\mathrm{C}$ is the complexity of each iteration and $\mathcal{L}$ is number of iterations that the algorithms converge. 

When a linear programming solver \cite{bonneel2011displacement} for partial OT is adopted, $\mathcal{C}=(n+m)nm$. If we adapt Sinknorn algorithm \cite{cuturi2014fast}, $\mathcal{C}=\mathcal{O}(\frac{1}{\epsilon}\ln(n+m)nm)$. The number of iterations $\mathcal{L}$ refers to the convergence analysis of the FW algorithm. We refer to the Appendix \eqref{sec: convergence} for details. 


\section{Numerical Applications}

\subsection{Measure graph similarity via Fused GW/PGW.}\label{sec:graph_mm}
In the following, we discuss how to model graphs as mm-spaces. Based on this modeling, fused-GW and fused-PGW can be adapted to measure graph similarity.

Given graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$, where $V_1, V_2$ are sets of vertices (nodes) and $E_1, E_2$ are sets of edges.

First, consider $G_1$. Suppose $V_1 = \{v_1^1, \ldots, v^1_{N_1}\}$. We construct the mm-space $\mathbb{X}_1 = (V_1, d_{V_1}, \mu^1 = \sum_{i=1}^{N_1} \mathrm{p}_i^1 \delta_{v_i})$, where $\mathrm{p}_i^1 > 0$ for all $i$. Let $\mathrm{p}^1 = [p_1^1, \ldots, p_{N_1}^1]$. Note that when $\sum(\mathrm{p}^1) = 1$, $\mathrm{p}^1$ represents the probability mass function (pmf) of the measure $\mu^1$. For general cases, we refer to $\mathrm{p}^1$ as the mass function (mf) of $\mu^1$.

In this formulation, the function $d_{V_1}: V_1^2 \to \mathbb{R}$ can be defined as follows:
\begin{itemize}
    \item \textbf{Adjacency indicator function:} $d_{V_1}(v_1, v_2) = 1$ if $(v_1, v_2) \in E_1$, and $d_{V_1}(v_1, v_2) = 0$ otherwise.
    \item \textbf{Shortest distance function:} $d_{V_1}(v_1, v_2)$ is the length of the shortest path connecting $v_1$ and $v_2$ if such a path exists, or $\infty$ if $v_1$ and $v_2$ are not connected.
\end{itemize}

Let $\mathcal{F}$ denote the space of feature assignments for all nodes. We define a feature function $f: V \to \mathcal{F}$ such that $x_i = f(v_i)$ represents the feature of the vertex $v_i$. For convenience, when the graph has discrete features, $\mathcal{F}$ is a discrete set; when the graph has continuous features, $\mathcal{F} = \mathbb{R}^d$, where $d$ is the dimension of each feature. In both cases, we define a metric $d_\mathcal{F}$ in the feature space.

The feature similarity function $d_\mathcal{F}$ is set as follows:
\begin{itemize}
    \item \textbf{Continuous feature graph:} Since $\mathcal{F} = \mathbb{R}^d$, we define $d_\mathcal{F}$ as the (squared) Euclidean distance in $\mathbb{R}^d$.
    \item \textbf{Discrete feature graph:} We first apply the Weisfeiler-Lehman kernel \cite{vishwanathan2010graph}, which encode elements in feature space as $\text{wl}: \mathcal{F} \to S^H$, where $S$ is finite discrete set, 
    $H \in \mathbb{N}$, typically set to $2$ or $4$. We then use the Hamming distance in $S^\mathrm{h}$ as $d_\mathcal{F}$. For details, refer to Section 4.2 of \cite{feydy2017optimal}.
\end{itemize}

By following the above process, we construct mm-spaces for both $G_1$ and $G_2$. Let $\mu^1$ and $\mu^2$ denote the corresponding measures, and let $\mathrm{p}^1 \in \mathbb{R}_+^{N_1}$ and $\mathrm{p}^2 \in \mathbb{R}_+^{N_2}$ represent the respective mass functions for $\mu^1$ and $\mu^2$. With abuse of notations, the fused Gromov-Wasserstein similarity between $G_1$ and $G_2$  is defined as 

{\footnotesize
\begin{align}
&FGW(G_1,G_2):=\inf_{\gamma\in\Gamma(\mathrm{p}^1,\mathrm{p}^2)}\omega_1 \sum_{i,j=1}^{n_1,n_2}d_\mathcal{F}(f(v^1_i),f(v^2_j))\nonumber\\
&+\omega_2\sum_{i,i'=1}^{N_1,N_1}\sum_{j,j'=1}^{N_2,N_2} |d_{V_1}(v^1_i,v^1_{i'})-d_{V_2}(v^2_i,v^2_{i'})|^2 \label{eq:GW_graph},
\end{align}
}
where we require $|\mathrm{p}^1|=|\mathrm{p}^2|=1$. 

Without this requirement, we can still define fused partial Gromov Wasserstein similarity for the two graphs:

{\footnotesize
\begin{align}
&FMPGW_\rho(G_1, G_2) := \inf_{\gamma \in \Gamma_\leq^\rho(\mathrm{p}, \mathrm{q})} 
\omega_1 \sum_{i,j=1}^{n_1, n_2} d_\mathcal{F}(f(v^1_i), f(v^2_j)) \nonumber \\
&\qquad + \omega_2 \sum_{i,i'=1}^{N_1, N_1} \sum_{j,j'=1}^{N_2, N_2} 
\left| d_{V_1}(v^1_i, v^1_{i'}) - d_{V_2}(v^2_j, v^2_{j'}) \right|^2, \label{eq:FMPGW_graph} \\
&FPGW_\lambda(G_1, G_2) := \inf_{\gamma \in \Gamma_\leq^\rho(\mathrm{p}, \mathrm{q})} 
\omega_1 \sum_{i,j=1}^{n_1, n_2} d_\mathcal{F}(f(v^1_i), f(v^2_j)) \nonumber \\
&\qquad + \omega_2 \sum_{i,i'=1}^{N_1, N_1} \sum_{j,j'=1}^{N_2, N_2} 
\left| d_{V_1}(v^1_i, v^1_{i'}) - d_{V_2}(v^2_j, v^2_{j'}) \right|^2 \nonumber \\
&\qquad+ \lambda \left( |\mathrm{p}|^2 + |\mathrm{q}|^2 - 2|\gamma|^2 \right). \label{eq:FPGW_graph}
\end{align}
}



\begin{figure}[t]
    \centering
\includegraphics[width=0.7\columnwidth]{images/noising.pdf}
    \caption{We visualize the graph classification data in this plot. The parameter $\eta\in\{0,10\%,20\%,30\%\}$ represents the proportion of outlier nodes. The blue nodes are the regular nodes in the graph, and the pink nodes are the outliers.   
    }
    \vspace{-0.3in}
    \label{fig:fig3}
\end{figure}

\subsection{Graph Classification}


\textbf{Dataset Setup.} We consider four widely used benchmark datasets, divided into two groups. The first group includes \textit{Mutag} \cite{debnath1991structure} and \textit{MSRC-9} \cite{rossi2015network}, which consist of graphs with discrete attributes. The second group consists of vector-attributed graphs, including \textit{Synthetic} \cite{feragen2013scalable}, \textit{Cuneiform} \cite{kriege2016valid}, and \textit{Proteins} \cite{borgwardt2005shortest}.

For each dataset, we randomly select $50\%$ of the graphs and add outlier nodes. Specifically, for each selected graph, suppose $N$ is the number of vertices. We manually add $\eta N$ extra nodes, where $\eta \in \{0, 10\%, 20\%, 30\%\}$ represents the ``level of outliers/noise''. For clarity, nodes that are not outliers are referred to as ``regular nodes''. The outlier nodes are randomly connected to both the regular nodes and each other. The features of the outlier nodes are defined as follows:
\begin{itemize}
    \item For graphs with discrete features, each outlier node is assigned a label that does not occur among the regular nodes in the graph.
    \item For graphs with continuous features, suppose all features lie within a compact set $[x_1, y_1] \times \ldots \times [x_d, y_d] \subset \mathbb{R}^d$, where $d \in \mathbb{N}$ is the dimension of the feature space. We assign features to the outlier nodes by sampling vectors from $[y_1, y_1 + 2\text{sd}_1] \times \ldots \times [y_d, y_d + 2\text{sd}_d]$, where $\text{sd}_i$ is the standard deviation of all node features in dimension $i$.
\end{itemize}

Furthermore, for each graph $G$, let $N_G$ denote the number of regular nodes (i.e., nodes that are not outliers). We assume $N_G$ is known.






\textbf{Fused Gromov/Unbalanced Gromov/Partial Gromov Setting.}

We adapt the process in section \ref{sec:graph_mm} to model each graph $G$ as a mm-space. 
In particular, in the FGW setting, we define the (probability) mass function $\mathrm{p}$ as $\mathrm{p}_i = \frac{1}{N}$, where $N$ is the number of nodes in graph $G$. In the FUGW/FPGW setting, we define the mass function $\mathrm{p}$ as $\mathrm{p}_i = \frac{1}{N_G}$, where $N_G$ is the number of regular nodes in $G$. 

For the distance functions $d_{V_1}$ and $d_{V_2}$ in graphs $G_1$ and $G_2$, we use the ``shortest path'' metric in this experiment. We select FMPGW \eqref{eq:fmpgw} and set the parameter $\rho = 1$.


 


\textbf{Baseline methods.} 
We consider the fused Gromov Wasserstein \cite{feydy2017optimal} and the Unbalanced fused Gromov Wasserstein \cite{thual2022aligning} as baselines for both discrete and continuous feature graph datasets. In addition, 
for discrete feature graph datasets, we add 
the Weisfeler Lehman kernel (WLK) \cite{vishwanathan2010graph},Graphlet count kernel (GK) \cite{shervashidze2009efficient}, random walk kernel (RWK) \cite{kriege2018recognizing}, ODD-STh Kernel  (ODD) \cite{da2012tree}, vertex histogram (VH) \cite{sugiyama2015halting}, Lovasz Theta (LT) \cite{lovasz1979shannon}, SVM theta (ST) \cite{jethava2013lovasz} as baselines methods. For the continuous feature graphs dataset, we also
consider the HOPPER kernel \cite{feragen2013scalable} and the propagation kernel Neumann \cite{neumann2016propagation}. 

\textbf{Classifier setup.} We adapt the SVM classification model in this experiment. In particular, given a graph dataset, we first compute the pairwise distance via FGW/(other baselines)/FPGW. By using the approach given by \cite{beier2022linear, vay2019fgw}, we combine each distance with a support vector machine (SVM), applying stratified 10-fold cross-validation. In each iteration of cross-validation, we train an SVM using $\exp(-\sigma D)$ as the kernel, where $D$ is the matrix of pairwise distances (w.r.t. one of the considered distances) restricted to 9 folds, and compute the accuracy of the model on the remaining fold. We report the accuracy averaged over all 10 folds for each model.
\begin{table*}[t]
\centering
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{lcccccccc}
\toprule
& \multicolumn{4}{c}{MSRC-9} & \multicolumn{4}{c}{MUTAG} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
& $0\%$ & $10\%$ & $20\%$ & $30\%$ & $0\%$ & $10\%$ & $20\%$ & $30\%$ \\
\midrule
WLK & 85.5$^{\pm5.9}$ & 86.5$^{\pm5.2}$ & 86.9$^{\pm4.7}$ & 86.0$^{\pm3.7}$ & 75.0$^{\pm6.3}$ & 73.4$^{\pm6.7}$ & 72.4$^{\pm7.0}$ & 73.5$^{\pm7.8}$ \\
GK & 15.8$^{\pm4.5}$ & 15.4$^{\pm3.7}$ & 14.9$^{\pm6.2}$ & 17.7$^{\pm8.0}$ & 77.1$^{\pm6.3}$ & 70.2$^{\pm2.6}$ & 70.2$^{\pm4.4}$ & 67.0$^{\pm6.6}$ \\
RWK & 74.7$^{\pm5.8}$ & 73.8$^{\pm6.6}$ & 74.7$^{\pm7.4}$ & 74.2$^{\pm4.1}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ \\
ODD & 67.9$^{\pm6.5}$ & 69.2$^{\pm7.6}$ & 64.7$^{\pm8.2}$ & 63.8$^{\pm9.0}$ & 64.9$^{\pm4.0}$ & 64.9$^{\pm4.0}$ & 64.9$^{\pm4.0}$ & 64.9$^{\pm4.0}$ \\
VH & 86.4$^{\pm4.0}$ & \textbf{86.9}$^{\pm4.2}$ & \textbf{87.8}$^{\pm4.5}$ & \textbf{87.8}$^{\pm4.0}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ \\
LT & 15.8$^{\pm5.0}$ & 13.1$^{\pm4.3}$ & 13.1$^{\pm5.1}$ & 14.5$^{\pm4.5}$ & 69.7$^{\pm3.9}$ & 68.1$^{\pm2.5}$ & 66.5$^{\pm2.3}$ & 68.6$^{\pm8.5}$ \\
ST & 13.6$^{\pm0.2}$ & 13.6$^{\pm0.2}$ & 13.6$^{\pm0.2}$ & 13.6$^{\pm0.2}$ & 75.0$^{\pm2.7}$ & 72.9$^{\pm3.7}$ & 72.3$^{\pm3.9}$ & 72.3$^{\pm3.9}$ \\
\midrule
FGW & \textbf{87.4}$^{\pm4.4}$ & \textbf{86.9}$^{\pm4.7}$ & 63.8$^{\pm5.8}$ & 62.0$^{\pm5.0}$ & \textbf{85.6}$^{\pm5.6}$ & \textbf{83.5}$^{\pm5.7}$ & 79.8$^{\pm6.5}$ & 76.6$^{\pm6.8}$ \\
FUGW & 73.8$^{\pm6.1}$ & 68.3$^{\pm4.9}$ & 5.0$^{\pm3.7}$ & 5.4$^{\pm4.4}$ &
82.4$^{\pm5.5}$ & 81.9$^{\pm5.6}$ & \textbf{81.9}$^{\pm6.1}$ & \textbf{80.3}$^{\pm6.0}$ \\
FPGW (ours) & \textbf{87.0}$^{\pm4.7}$ & \textbf{88.3}$^{\pm4.1}$ & \textbf{87.3}$^{\pm4.4}$ & \textbf{86.9}$^{\pm4.7}$ & \textbf{85.6}$^{\pm5.6}$ & \textbf{85.1}$^{\pm5.9}$ & \textbf{84.6}$^{\pm5.6}$ & \textbf{82.5}$^{\pm6.3}$ \\
\midrule
\midrule
& \multicolumn{4}{c}{PROTEINS} & \multicolumn{4}{c}{SYNTHETIC} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
& $0\%$ & $10\%$ & $20\%$ & $30\%$ & $0\%$ & $10\%$ & $20\%$ & $30\%$ \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
Propagation & 59.6$^{\pm0.2}$ & 59.6$^{\pm0.2}$ & 59.6$^{\pm0.2}$ & 59.6$^{\pm0.2}$ & 48.0$^{\pm7.0}$ & 51.3$^{\pm6.5}$ & 47.3$^{\pm5.5}$ & 53.0$^{\pm6.1}$ \\
GraphHopper & 69.7$^{\pm3.8}$ & 68.2$^{\pm4.8}$ & 67.3$^{\pm3.5}$ & 67.2$^{\pm4.7}$ & 86.0$^{\pm5.3}$ & 78.7$^{\pm9.5}$ & \textbf{70.3}$^{\pm7.1}$ & \textbf{70.7}$^{\pm10.7}$ \\
\midrule
FGW & \textbf{72.0}$^{\pm3.7}$ & 68.5$^{\pm3.1}$ & 65.3$^{\pm4.4}$ & 65.8$^{\pm4.4}$ & \textbf{97.7}$^{\pm4.0}$ & \textbf{95.7}$^{\pm2.9}$ & 48.7$^{\pm1.6}$ & 49.3$^{\pm0.7}$ \\
FUGW & 70.6$^{\pm3.7}$ & \textbf{69.7}$^{\pm2.7}$ & \textbf{69.0}$^{\pm3.3}$ & \textbf{68.7}$^{\pm2.4}$& 60.0$^{\pm6.1}$ & 48.3$^{\pm6.7}$ & 45.3$^{\pm4.5}$ & 45.3$^{\pm4.5}$ \\
FPGW (ours) & \textbf{72.0}$^{\pm3.7}$ & \textbf{71.6}$^{\pm4.0}$ & \textbf{71.2}$^{\pm4.9}$ & \textbf{69.2}$^{\pm5.5}$ & \textbf{97.7}$^{\pm4.0}$ & \textbf{96.3}$^{\pm3.1}$ & \textbf{97.7}$^{\pm4.0}$ & \textbf{94.3}$^{\pm4.5}$ \\
\bottomrule
\end{tabular}
\caption{Kernel accuracy (\%) with standard deviation under different noise levels (0\%, 10\%, 20\%, 30\%). Top: results for discrete node features. Bottom: results for continuous node features.}
\label{tab:kernel-accuracy}
\end{table*}
% \begin{table*}[t]
% \centering
% \setlength{\tabcolsep}{3.5pt}
% \begin{tabular}{lcccccccc}
% \toprule
% & \multicolumn{4}{c}{MSRC} & \multicolumn{4}{c}{MUTAG} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
%  & $0\%$ & $10\%$ & $20\%$ & $30\%$ & $0\%$ & $10\%$ & $20\%$ & $30\%$ \\
% \midrule
% WLK & 85.5$^{\pm5.9}$ & 86.5$^{\pm5.2}$ & 86.9$^{\pm4.7}$ & 86.0$^{\pm3.7}$ & 75.0$^{\pm6.3}$ & 73.4$^{\pm6.7}$ & 72.4$^{\pm7.0}$ & 73.5$^{\pm7.8}$ \\
% GK & 15.8$^{\pm4.5}$ & 15.4$^{\pm3.7}$ & 14.9$^{\pm6.2}$ & 17.7$^{\pm8.0}$ & 77.1$^{\pm6.3}$ & 70.2$^{\pm2.6}$ & 70.2$^{\pm4.4}$ & 67.0$^{\pm6.6}$ \\
% RWK & 74.7$^{\pm5.8}$ & 73.8$^{\pm6.6}$ & 74.7$^{\pm7.4}$ & 74.2$^{\pm4.1}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ \\
% ODD & 67.9$^{\pm6.5}$ & 69.2$^{\pm7.6}$ & 64.7$^{\pm8.2}$ & 63.8$^{\pm9.0}$ & 64.9$^{\pm4.0}$ & 64.9$^{\pm4.0}$ & 64.9$^{\pm4.0}$ & 64.9$^{\pm4.0}$ \\
% VH & 86.4$^{\pm4.0}$ & \textbf{86.9}$^{\pm4.2}$ & \textbf{87.8}$^{\pm4.5}$ & \textbf{87.8}$^{\pm4.0}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ & 66.5$^{\pm2.3}$ \\
% LT & 15.8$^{\pm5.0}$ & 13.1$^{\pm4.3}$ & 13.1$^{\pm5.1}$ & 14.5$^{\pm4.5}$ & 69.7$^{\pm3.9}$ & 68.1$^{\pm2.5}$ & 66.5$^{\pm2.3}$ & 68.6$^{\pm8.5}$ \\
% ST & 13.6$^{\pm0.2}$ & 13.6$^{\pm0.2}$ & 13.6$^{\pm0.2}$ & 13.6$^{\pm0.2}$ & 75.0$^{\pm2.7}$ & 72.9$^{\pm3.7}$ & 72.3$^{\pm3.9}$ & 72.3$^{\pm3.9}$ \\
% \midrule
% FGW & \textbf{87.4}$^{\pm4.4}$ & \textbf{86.9}$^{\pm4.7}$ & 63.8$^{\pm5.8}$ & 62.0$^{\pm5.0}$ & \textbf{85.6}$^{\pm5.6}$ & \textbf{83.5}$^{\pm5.7}$ & \textbf{79.8}$^{\pm6.5}$ & \textbf{76.6}$^{\pm6.8}$ \\
% FPGW (ours) & \textbf{87.0}$^{\pm4.7}$ & \textbf{88.3}$^{\pm4.1}$ & \textbf{87.3}$^{\pm4.4}$ & \textbf{86.9}$^{\pm4.7}$ & \textbf{85.6}$^{\pm5.6}$ & \textbf{85.1}$^{\pm5.9}$ & \textbf{84.6}$^{\pm5.6}$ & \textbf{82.5}$^{\pm6.3}$ \\
% \midrule
% \midrule
% & \multicolumn{4}{c}{PROTEINS} & \multicolumn{4}{c}{SYNTHETIC} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
%  & $0\%$ & $10\%$ & $20\%$ & $30\%$ & $0\%$ & $10\%$ & $20\%$ & $30\%$ \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
% Propagation & 59.57$^{\pm0.2}$ & 59.57$^{\pm0.2}$ & 59.57$^{\pm0.2}$ & 59.57$^{\pm0.2}$ & 48.00$^{\pm7.0}$ & 51.33$^{\pm6.5}$ & 47.33$^{\pm5.5}$ & 53.00$^{\pm6.1}$ \\
% GraphHopper & 69.73$^{\pm3.8}$ & 68.19$^{\pm4.8}$ & 67.29$^{\pm3.5}$ & 67.21$^{\pm4.7}$ & 86.00$^{\pm5.3}$ & 78.67$^{\pm9.5}$ & 70.33$^{\pm7.1}$ & 70.67$^{\pm10.7}$ \\
% \midrule
% FGW & \textbf{71.96}$^{\pm3.7}$ & 68.47$^{\pm3.1}$ & 65.32$^{\pm4.4}$ & 65.77$^{\pm4.4}$ & \textbf{97.70}$^{\pm4.0}$ & 95.70$^{\pm2.9}$ & 48.70$^{\pm1.6}$ & 49.30$^{\pm0.7}$ \\
% FPGW (ours) & \textbf{71.96}$^{\pm3.7}$ & \textbf{71.60}$^{\pm4.0}$ & \textbf{71.15}$^{\pm4.9}$ & \textbf{69.17}$^{\pm5.5}$ & \textbf{97.70}$^{\pm4.0}$ & \textbf{96.30}$^{\pm3.1}$ & \textbf{97.70}$^{\pm4.0}$ & \textbf{94.30}$^{\pm4.5}$ \\
% \bottomrule
% \end{tabular}
% \caption{Kernel accuracy (\%) with standard deviation under different noise levels (0\%, 10\%, 20\%, 30\%). Top: results for discrete node features. Bottom: results for continuous node features. NH: Neighborhood Hash, PROP: Propagation, WL: Subtree WL, VH: Vertex Histogram, PM: Pyramid Match, LT: Lovasz Theta, ST: SVM Theta}
% \label{tab:kernel-accuracy}
% \end{table*}

\textbf{Performance Analysis.}  
The accuracy comparison is summarized in Table~\ref{tab:kernel-accuracy}. For discrete feature graph datasets, VH, FGW, and FPGW achieve the highest overall accuracy. However, as the noise level $\eta$ increases, FGW's performance noticeably deteriorates, while the other three methods remain robust against outlier corruption.

In contrast, for continuous datasets such as \textit{Proteins} and \textit{Synthetic}, the baseline methods, including ``Propagation,'' ``GraphHopper,'' and ``FGW,'' experience a significant drop in performance. Notably, FPGW maintains strong performance across these datasets.

FUGW performs comparably to FPGW on the Mutag and Proteins datasets. However, its accuracy on the SCRC and Synthetic datasets is significantly lower than that of FGW and FPGW.
     
%One potential explanation could be that FUGW utilizes the Sinkhorn algorithm, and the entropy regularization may affect the accuracy. 

We refer to Appendix  \ref{sec:graph_classification_2} for the parameter settings and wall-clock time comparison. In summary, focusing on the comparison between FGW, FUGW, and FPGW, FGW is slightly faster than FPGW, while both FGW and FPGW are significantly faster than FUGW.
 








\begin{figure}[t]
\centering   
\begin{subfigure}[t]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{images/clustering/fgw.png}
        \caption{FGW-kmeans}
        \label{fig:fgw}
    \end{subfigure}
    \hspace{0.5cm}
    \begin{subfigure}[t]{\columnwidth}
        \centering
\includegraphics[width=\textwidth]{images/clustering/fmpgw.png}
\caption{FPGW-kmeans}
\label{fig:fmpgw}
\end{subfigure}
\caption{We present the clustering results of the FGW and FPGW k-means methods. \\  
In the first column, we visualize the centroids obtained using both methods. The centroids are represented by their features and structure (distance matrix). The edges of these graphs are either reconstructed or approximated based on the returned distance matrix. Additionally, the color of each node corresponds to its feature. \\  
The clustering results are shown in the remaining columns. For each graph, the color of regular nodes represents their features, while all outlier nodes are depicted as black squares.
}
\label{fig:clustering}
\end{figure}


\subsection{Graph clustering}
Given a set of unlabeled graphs $\{G_1,\ldots G_K\}$, we compare the performance of FGW and FPGW in the graph clustering task. 

\textbf{Dataset setup.}
We adapt the dataset generated by \cite{feydy2017optimal}. Each graph follows a simple Stochastic Block Model, and the groups are defined w.r.t. the number of communities inside each graph and the distribution of their labels. The dataset contains three different types of graphs, and each type contains five graphs. Each graph contains 30 or 40 nodes, and the feature of each node is randomly selected in a compact set $[-2,2]$. 

In addition, we randomly generate outliers and add them to graphs. In particular, we select 50\% of graphs and add $30\%$ outliers. These outlier nodes are randomly connected or connected to the original nodes. For each outlier node, we define its feature as a random value in $[2,2+2\text{std}]$, where $\text{std}$ is the standard deviation of all features of nodes in the graph. 
Similar to the setting of graph classification, for each graph $G$, we suppose $N_G$ is the number of nodes that are not outliers. We assume $N_G$ is known in this experiment. We refer to figure \ref{fig:clustering} for the visualization of these graphs. 


\textbf{Baseline: FGW K-means.}
We consider the FGW K-means method introduced in Section 4.3 of \cite{feydy2017optimal} as the baseline method. Specifically, given graphs $\{G_1, \ldots, G_K\}$ and the number of clusters $K' \leq K$, the method iteratively alternates between the following two steps:

\begin{itemize}
    \item \textbf{Step 1.} For each $i \in [1:K]$ and $j \in [1:K']$, compute the distance between each graph $G_i$ and centroid $G_j'$. Assign each graph $G_i$ to the closest centroid based on the computed distances.
    \item \textbf{Step 2.} Using the updated assignments, recompute each centroid $G_j'$, where $j'\in[1:K']$, as the center of the graphs assigned to it.
\end{itemize}


In the FGW K-means method, the Fused-GW distance is used to define the distance in Step 1, while the Fused-GW barycenter is used to compute the ``center'' in Step 2.

\textbf{Our method: FPGW K-means.}  
Inspired by the FGW K-means method, we introduce the FPGW K-means method. In summary, we adapt the FMPGW discrepancy to measure the distance in Step 1, and introduce the \textbf{FPGW barycenter} to define the ``center'' in Step 2.

First, we convert the graphs to mm-spaces using the formulation introduced in Section \ref{sec:graph_mm}. Specifically, given a graph $G = (V = \{v_1, \ldots, v_N\}, E)$, we define  
\[
\mathbb{X} = \left(V, C, \sum_{i=1}^N \frac{1}{N_G} \delta_{v_i}\right),
\]
where $C = [c(v_i, v_{i'})]_{i, i' \in [1:N]} \in \mathbb{R}^{N \times N}$, and $c(v_i, v_{i'})$ is the ``shortest path distance'' determined by $E$. Here, $N_G$ is the number of nodes that are not outliers, and its value is known based on the experiment setup. The feature distance between nodes $v_i$ and $v_j$ is defined as the Euclidean distance. We then use the Fused-PGW discrepancy \eqref{eq:FMPGW_graph} to measure the distance between each pair of graphs and centroids in Step 1 of the K-means method.

For Step 2, suppose the set of graphs $\boldsymbol{G}^k \subset \{G_1, \ldots, G_K\}$ is assigned to cluster $k'$ for each $k' \in [1:K']$. We define the ``center'' using the following \textbf{Fused-PGW barycenter}:
\begin{align}
\min_{G} \sum_{G_k \in \boldsymbol{G}^k} \frac{1}{|\boldsymbol{G}^k|} \text{FMPGW}_{\rho_k}(G, G_k), \label{eq:barycenter_main}
\end{align}
where $\rho_k = 1$ for all $k$. The solution to this problem provides the updated centroid for cluster $k'$. Details of the formal formulation and solver are provided in Appendix \ref{sec:barycenter}.

We iteratively repeat the above two steps until all centroids/assignments converge.


\textbf{Other parameter setting.}
In this experiment, we set $\omega_2=0.999$ for both the FGW and FPGW methods. In addition, we set the number of clustering $K'=3$ for both methods. For each centroid, we initialize it as a random connected graph with $40$ nodes. 

% \begin{figure}[h]
% \centering
% \begin{subfigure}[t]{0.49\textwidth}
% \centering
% \includegraphics[width=\textwidth]{images/clustering/fgw_centroid.png}
%     \caption{FGW-centroids}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.49\textwidth}
% \centering
% \includegraphics[width=\textwidth]{images/clustering/fpgw_centroid.png}
%     \caption{FPGW-centroids}
%     \end{subfigure}
% \caption{
% }
%     \label{fig:centroid}
% \end{figure}

\textbf{Performance analysis.}
We present the clustering results of FGW K-means and FPGW K-means in Figure~\ref{fig:clustering}. The performance of the FGW method is significantly impacted due to the presence of outlier nodes in half of the graphs. In sharp contrast, FPGW demonstrates a more robust performance, with clustering results closely aligning with the ground truth. This robustness is attributed to the partial matching property of FPGW. 

From the centroids visualization for FGW/FPGW methods, it is evident that FPGW effectively excludes most of the information from outliers, whereas FGW incorporates it into the centroids.

Regarding the wall-clock time, FGW requires 41.9 seconds, while FPGW requires 82.7 seconds.


\section{Summary.}
In this paper, we proposed a novel formulation called ``fused-partial Gromov-Wasserstein'' (fused-PGW) for comparing structured objects. Theoretically, we demonstrated the metric properties of fused-PGW, and numerically, we introduced the corresponding Frank-Wolfe solver and barycenter algorithms. Finally, we applied fused-PGW to graph classification and clustering experiments, showing that it achieves more robust performance in the presence of outlier nodes due to its partial matching property.

\section*{Acknowledgments}
% This section is required for arXiv version
%\vspac{-0.5em}
This research was partially supported by the NSF CAREER Award No. 2339898.


% \begin{table*}[h!]
% \caption{Classification accuracy}
% \label{sample-table-1}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccccccc} % 7 columns total: 1 for Data set + 6 for values
% \toprule
% Data set & \multicolumn{4}{c}{Synthetic} & \multicolumn{4}{c}{Mutag} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
% $\eta$ & $0$ & $10\%$ & $20\%$ & $30\%$ & $0$ & $10\%$ & $20\%$ & $30\%$ \\
% \midrule
% FGW  & $97.7\pm4.0$ & $95.7\pm2.9$ & $48.7\pm1.6$ & $49.3\pm0.7$ & $85.6\pm5.6$ & $83.5\pm5.7$ & $79.8\pm6.5$ & $76.6\pm6.8$ \\
% FPGW (ours) & $97.7\pm4.0$ & $96.3\pm3.1$ & $97.7\pm4.0$ & $94.3\pm4.5$ & $85.6\pm5.6$ & $85.1\pm5.9$ & $84.6\pm5.6$ & $82.5\pm6.3$ \\
% baseline & 79.0 & 80.1 &  &  & 85.7 & 86.8 & 87.9 & 78.9 \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}




% \begin{table*}[h!]
% \caption{Classification accuracy}
% \label{sample-table-1}
% \vskip 0.15in
% \begin{center}

% \begin{sc}
% \begin{tabular}{lcccccccc} % 7 columns total: 1 for Data set + 6 for values
% \toprule
% Data set & \multicolumn{4}{c}{Synthetic} & \multicolumn{4}{c}{Mutag} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
% $\eta$ & $0$ & $10\%$ & $20\%$ & $30\%$ & $0$ & $10\%$ & $20\%$ & $30\%$ \\
% \midrule
% FGW  & $97.7^{\pm4.0}$ & $95.7^{\pm2.9}$ & $48.7^{\pm1.6}$ & $49.3^{\pm0.7}$ & $85.6^{\pm5.6}$ & $83.5^{\pm5.7}$ & $79.8^{\pm6.5}$ & $76.6^{\pm6.8}$ \\
% FPGW (ours) & $97.7^{\pm4.0}$ & $96.3^{\pm3.1}$ & $97.7^{\pm4.0}$ & $94.3^{\pm4.5}$ & $85.6^{\pm5.6}$ & $85.1^{\pm5.9}$ & $84.6^{\pm5.6}$ & $82.5^{\pm6.3}$ \\
% \midrule
% \multicolumn{9}{c}{\textbf{Baseline Kernels}} \\
% \midrule

% \bottomrule
% \end{tabular}
% \end{sc}
% % \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}





%\section{Electronic Submission}
%\label{submission}

%Submission to ICML 2024 will be entirely electronic via a website
%(not email). Information about the submission process and \LaTeX\ templates
%are available on the conference website at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item \textbf{New to this year}: If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So, if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgement} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Paper Deadline:} The deadline for paper submission that is
% advertised on the conference website is strict. If your complete,
% anonymized, submission does not reach us on time, it will not be
% considered for publication. 

% \textbf{Anonymous Submission:} ICML uses double-masked review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \textbf{Simultaneous Submission:} ICML will not accept any paper which,
% at the time of submission, is under review for another conference or
% has already been published. This policy also applies to papers that
% overlap substantially in technical content with conference papers
% under review or previously published. ICML submissions must not be
% submitted to other conferences and journals during ICML's review
% period.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2024}
% usepackage statement.

% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{41}^{st}$ International Conference on Machine Learning},
% Vienna, Austria, PMLR 235, 2024.
% Copyright 2024 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2024\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2024\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a


% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use the Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2024.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author's information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exceptions are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self-contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information, should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting with 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, and Country.
% Similarly for industrial affiliations.)

% Each distinct affiliation should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript and the term ``\textsuperscript{*}Equal contribution".
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally, only one or two names should be listed.

% A sample file with author names is included in the ICML2024 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use a 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time, this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc., consecutively within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2024.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% of the year in parentheses, for example, when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise,e place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your papers.

% Use an unnumbered first-level section heading for the reference, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document gives examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect the capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips on how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}

% \textbf{Do not} include acknowledgments in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include aacknowledgments Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.



% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% The two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% The ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:



% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content that does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{ref}
\bibliographystyle{unsrt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Notation and Abbreviations.}

\begin{itemize}
\item OT: Optimal Transport Problem. See \eqref{eq: OT}. 
\item POT: Partial Optimal Transport Problem. See \eqref{eq:pot}. 
\item GW: Gromov-Wasserstein Problem. See \eqref{eq:gw}. 
\item PGW: Partial Gromov-Wasserstein Problem. See \eqref{eq:PGW}.  
\item FW: Frank-Wolfe algorithm. See e.g.  \cite{frank1956algorithm}. 
\item MPGW: Mass-Constrained Partial Gromov-Wasserstein. See and \eqref{eq:MPGW}.
%\item $S\subset \mathbb{R}^{d_1},X\subset\mathbb{R}^{d_2},Y\subset\mathbb{R}^{d_3}$: subsets in Euclidean spaces.
\item $\|\cdot\|$: Euclidean norm.
\item $\mathbb{R}^d$: Euclidean space, where $d\in \mathbb{N}$. 
\item $X,Y\subset\mathbb{R}^d$: In default setting, we assume $X,Y$ are non-emtpy, convex compact sets. 
\item $X^2=X^{\otimes2}=X\times X$. 
\item $\mathcal{M}_+(X)$: set of all positive (non-negative) Randon (finite) measures defined on $X$. 
\item $\mathcal{P}_2(X)$: set of all probability measures defined on $X$, whose second moment is finite. 
\item $\mathbb{R}_+$: set of all non-negative real numbers.
\item $\mathbb{R}^{n\times m}$: set of all $n\times m$ matrices with real coefficients.
\item $\mathbb{R}^{n\times m}_+$ (resp. $\mathbb{R}^{n}_+$): set of all $n\times m$ matrices (resp., $n$-vectors) with non-negative coefficients.
\item $\mathbb{R}^{n\times m\times n\times m}$: set of all $n\times m\times n\times m$ tensors with real coefficients.

\item $1_n, 1_{n\times m}, 1_{n\times m\times n\times m}$: vector, matrix, and tensor of all ones.
\item $\mathbbm{1}_{E}$: characteristic function of a measurable set $E$
\begin{equation*}
    \mathbbm{1}_E (z)= \begin{cases}
        1 & \text{ if } z\in E,\\
        0 & \text{ otherwise.}
    \end{cases}
\end{equation*}
\item $\mathbb{X},\mathbb{Y}$: metric measure spaces (mm-spaces): $\mathbb{X}=(X,d_X,\mu), \, \mathbb{Y}=(Y,d_Y,\nu).$
\item $C^X$: given a discrete mm-space $\mathbb{X}=(X,d_X,\mu)$, where $X=\{x_1,\dots,x_n\}$, the symmetric matrix $C^X\in\mathbb{R}^{n \times n}$ is defined as $C^{X}_{i,i'}=d_X^q(x_i,x_i')$. $C^Y$ is defined similarly. 

    \item $\mu^{\otimes 2}$: product measure $\mu\otimes \mu$.

    \item $T_\# \sigma$: $T:X\to Y$ is a measurable function and $\sigma$ is a measure on $X$. $T_\#\sigma$ is the push-forward measure of $\sigma$, i.e., its is the measure on $Y$ such that for all Borel set $A\subset Y$, $T_\# \sigma(A)=\sigma(T^{-1}(A))$.

    \item $\gamma,\gamma_1,\gamma_2$: $\gamma$ is a joint measure defined in a product space  having $\gamma_1,\gamma_2$ as its first and second marginals, respectively. In the discrete setting, they are viewed as matrices and vectors, i.e., $\gamma\in\mathbb{R}_+^{n\times m}$, and $\gamma_1=\gamma 1_m\in\mathbb{R}_+^n$, $\gamma_2=\gamma^\top 1_n\in\mathbb{R}_+^m$.
\item $\pi_1:X\times Y\to X$, canonical projection mapping, with $(x,y)\mapsto x$.
Similarly, $\pi_2: X\times Y\to Y$ is canonical projection mapping, with $(x,y)\mapsto y$. 

\item $\pi_{1,2}: S\times X\times Y\to X\times Y$, canonical projection mapping, with $(s,x,y)\to (x,y)$.
Similarly, $\pi_{0,1}$ maps $(s,x,y)$ to $(s,x)$;
$\pi_{0,2}$ maps $(s,x,y)$ to $(s,y)$. 


\item $\Gamma(\mu,\nu)$, where $\mu\in \mathcal{P}_2(X),\nu\in \mathcal{P}_2(Y)$ (where 
$X, Y$ may not necessarily be the same set): it is the set of all the couplings (transportation plans) between $\mu$ and $\nu$, i.e., 
$\Gamma(\mu,\nu):=\{\gamma\in \mathcal{P}_2(X\times Y): \, \gamma_1=\mu,\gamma_2=\nu\}.$
\item $\Gamma(\mathrm{p},\mathrm{q})$: set of all the couplings between the discrete probability measures $\mu=\sum_{i=1}^n p_i^X\delta_{x_i}$ and $\nu=\sum_{j=1}^m q_j^Y\delta_{y_j}$
with weight vectors 
\begin{equation}\label{eq: vectors p q}
 \mathrm{p}=[p_1^X,\dots,p_n^X]^\top \qquad \text{ and  } \qquad \mathrm{q}=[q_1^Y,\dots, q_m^Y]^\top.   
\end{equation}
That is, $\Gamma(\mathrm{p},\mathrm{q})$ coincides with $\Gamma(\mu,\nu)$, but it is viewed as a subset of $n\times m$ matrices: 
$$\Gamma(\mathrm{p},\mathrm{q}):=\{\gamma\in\mathbb{R}_+^{n\times m}: \gamma 1_m=\mathrm{p}, \gamma^\top 1_n=\mathrm{q}\}.$$

\item $r,q,p$: real numbers $1\leq r,p,q<\infty$. 
\item $\mathrm{p},\mathrm{q}$:
vectors of weights as in \eqref{eq: vectors p q}.
\item $\mathrm{p}=[p_1,\dots,p_n]\leq \mathrm{p}'=[p'_1,\dots,p'_n]$ if $p_j\leq p_j'$ for all $1\leq j\leq n$.
\item $|\mathrm{p}|=\sum_{i=1}^np_i$ for $\mathrm{p}=[p_1,\dots,p_n]$. 
\item $c(x,y): X\times Y\to \mathbb{R}_+$ denotes the cost function used for classical and partial optimal transport problems. %It is a %non-convex, 
It is a lower-semi continuous function. 
\item $OT(\mu,\nu)$: it is the classical optimal transport (OT) problem between the probability measures $\mu$ and $\nu$ defined in \eqref{eq: OT}.
\item $W_p(\mu,\nu)$: it is the $p$-Wasserstein distance between the probability measures $\mu$ and $\nu$ defined in \eqref{eq: Wp}, for $1\leq p<\infty$.
\item $POT_\lambda(\mu,\nu)$: Partial Optimal Transport (OPT) problem defined in \eqref{eq:pot}. 
\item $|\mu|$: total variation norm of the positive Randon (finite) measure $\mu$ defined on a measurable space $X$, i.e., $|\mu|=\mu(X)$.
\item $\mu\leq \sigma$: denotes that for all Borel set $B\subseteq X$ we have that the measures $\mu,\sigma\in\mathcal{M}_+(X)$ satisfy $\mu(B)\leq \sigma(B)$.
\item $\Gamma_{\leq}(\mu,\nu)$, where $\mu\in \mathcal{M}_+(X),\nu\in \mathcal{M}_+(Y)$:  set of all ``partial transportation plans'' defined in \eqref{eq:Gamma_leq}.
\item $\Gamma_{\leq}(\mathrm{p},\mathrm{q})$: set of all the ``partial transportation plans'' between the discrete probability measures $\mu=\sum_{i=1}^n p_i^X\delta_{x_i}$ and $\nu=\sum_{j=1}^m q_j^Y\delta_{y_j}$
with weight vectors $\mathrm{p}=[p_1^X,\dots,p_n^X]$ and $\mathrm{q}=[q_1^Y,\dots, q_m^Y]$. That is, $\Gamma_{\leq}(\mathrm{p},\mathrm{q})$ coincides with $\Gamma_{\leq}(\mu,\nu)$, but it is viewed as a subset of $n\times m$ matrices defined as: 
$$\Gamma_\leq(\mathrm{p},\mathrm{q}):=\{\gamma\in\mathbb{R}_+^{n\times m}: \gamma 1_m\leq \mathrm{p}, \gamma^\top 1_n\leq \mathrm{q}\}.$$
\item $\lambda>0$: positive real number.
\item $\hat{\infty}$: auxiliary point.
\item $\hat X= X\cup \{\hat{\infty}\}$.
%\item $\hat\mu, \hat\nu$: given in \eqref{eq: mu_hat}.
%\item $\hat{\mathrm{p}},\hat{\mathrm{q}}$: given in \eqref{eq: hat p hat q}.
%\item $\hat\gamma$: given in \eqref{eq: ot and opt plan}.
%\item $\hat{c}(\cdot,\cdot):\hat X\times \hat Y\to \mathbb{R}_+$: cost as in \eqref{eq: OT variant}.
\item $L:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$: cost function for the GW problems.
\item $D:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$: generic distance on $\mathbb{R}$ used for GW problems. 
\item $GW^L(\cdot,\cdot)$: GW optimization problem given in \eqref{eq:gw}.
\item $d_{GW,r}^q(\cdot,\cdot)$: GW optimization problem given in \eqref{eq:gw} when $L(a,b)=|a-b|^q$.
%\item $GW^L_q(\cdot,\cdot)$: general GW optimization problem for $g\geq 1$ given in \eqref{eq: prob G^L_q}.
%\item $d_{GW}_{r,q}(\cdot,\cdot)$: general GW optimization problem for $q\geq 1$ and $L(a,b)=|a-b|^p$ given in \eqref{eq: prob G^p_q}.
%\item $GW^p_{\lambda,q}(\cdot,\cdot)$: generalized GW problem given in \eqref{eq: GW general}.
%\item $\widehat{GW}$: GW-variant  problem given in \eqref{eq: GW variant} for the general case, and in \eqref{eq: variant gw discrete} for the discrete setting.
%\item $\hat L$: cost given in \eqref{eq:L_tilde} for the GW-variant problem.
%\item $d:\hat X\times \hat X\to\mathbb{R}_+\cup\{\infty\}$: ``generalized'' metric given in \eqref{eq: d_hat} for $\hat X$.
%\item $\mathcal{G}_q$: subset of mm-spaces given in \eqref{eq:G_q}. 

\item $\mathbb{X}\sim\mathbb{Y}$:  equivalence relation in for mm-spaces,  $\mathbb{X}\sim\mathbb{Y}$ if and only if 
they have the same total mass and 
$GW_q^p(\mathbb{X},\mathbb{Y})=0$.
\item $PGW_{r,L,\lambda}(\cdot,\cdot)$: partial GW optimization problem given in \eqref{eq:PGW}.
%\item $d_{PGW_{\lambda,q}^p(\cdot,\cdot)$:
% partial GW optimization problem given in \eqref{eq:pgw} when 
%$L(a,b)=|a-b|^p$.
%\item $PGW_\lambda(\cdot,\cdot)$: is is the PGW problem $PGW_{\lambda,q}^p(\cdot,\cdot)$ for the case when $p=2=q$.

\item $\mu(\phi)$: given a measure $\mu$ and a function $\phi$ defined in the same space, 
$$\mu(\phi):=\langle \phi,\mu \rangle :=\int \phi(x)d\mu(x).$$

% \item $C(\gamma;\mu,\nu)$: the transportation cost induced by transportation plan $\gamma\in\Gamma(\mu,\nu)$ in the GW problem, 
% \begin{align}
%   C(\gamma;\mu,\nu)&:=\gamma^{\otimes2}(L(d_X^q,d_Y^q))  \nonumber\\
%   &=\int_{(X\times Y)^2}L(d_X^q(x,x'),d_Y^q(y,y')) \, d\gamma^{\otimes2}((x,y),(x',y')).
% \end{align}


\item $C(\gamma;\lambda,\mu,\nu)$: the transportation cost induced by transportation plan $\gamma\in\Gamma_\leq(\mu,\nu)$ in the Partial GW problem \ref{eq:PGW}, 
$$C(\gamma;\lambda,\mu,\nu):=\gamma^{\otimes2}(L(d_X^q,d_Y^q))+\lambda\left(|\mu|^2+|\nu|^2-2|\gamma|^2\right).$$
%where $\gamma\in\Gamma_\leq(\mu,\nu)$. 

% \item $C(\gamma;\mathbb{X},\mathbb{Y},\lambda)$: the transportation cost induced by transportation plan $\gamma$ in Partial-GW problem \ref{eq:pgw}. In particular, 
% $$C(\gamma;\mathbb{X},\mathbb{Y},\lambda):=\gamma^{\otimes2}(L(d_X^q,d_Y^q))+\lambda(|\mu|^2+|\nu|^2-2|\gamma|^2).$$
% where $\gamma\in\Gamma_\leq(\mu,\nu)$. 

%\item $PGW_{\lambda}^p(\cdot,\cdot)$: partial GW problem $PGW_{\lambda,q}^L(\cdot,\cdot)$  for which $L(\mathrm{r}_1,\mathrm{r}_2)=D(\mathrm{r}_1,\mathrm{r}_2)^p$ for a metric $D$ in $\mathbb{R}$, and $1\leq p<\infty$. Typically $D(\mathrm{r}_1,\mathrm{r}_2)=|\mathrm{r}_1-\mathrm{r}_2|$, and $p=2$. It is explicitly given in \eqref{eq: pgw for metric}.
%\item $PGW_\lambda(\cdot,\cdot)$: discrete partial GW problem given in \eqref{eq: pgw discrete}. 
\item $\mathcal{L}$: functional for the optimization problem $PGW_\lambda(\cdot,\cdot)$.
\item $M$, $M-2\lambda$, $M^\top$: $M\in \mathbb{R}^{n\times m\times n\times m}$ is defined by $M_{i,j,i',j'}=L(C_{i,i'},C_{j,j'})$. $M^\top$ is $n\times m\times n\times $ tensor such that $M^{\top}_{i,j,i',j'}=M^{\top}_{i',j',i,j}$. Additionally, $(M-2\lambda)_{i,i',j,j'}:=M_{i,i',j,j'}-2\lambda.$ %,\forall i,i'\in[1:n],j,j'\in[1:m].$ %, respectively.
% \item $\tilde M=M-2\lambda:=M  1_{n,n,m,m}-2\lambda 1_{n,n,m,m}$: an $n\times n\times m\times m$ tensor with 
% $$(M-2\lambda)_{i,i',j,j'}:=M_{i,i',j,j'}-2\lambda,\forall i,i'\in[1:n],j,j'\in[1:m].$$

\item $\langle\cdot, \cdot\rangle_F,\langle\cdot, \cdot\rangle$: Frobenius inner product for matrices, i.e., $$\langle A,B\rangle=\langle A, B\rangle_F=\mathrm{trace}(A^\top B)=\sum_{i,j}^{n,m} A_{i,j}B_{i,j},$$ for all $A,B\in\mathbb{R}^{n\times m}$. 
\item $M\circ \gamma$: product between the tensor $M$ and the matrix $\gamma$. In particular, 
$[M\circ\gamma]_{i,j}=\sum_{i'=1,j'=1}^{n,m}M_{i,j,i',j'}\gamma_{i',j'}$.
\item $\nabla$: gradient. %We might use $\nabla_x$ to indicate the gradient with respect to the variable $x$.
\item $[1:n]=\{1,\dots,n\}$.
\item $\alpha\in[0,1]$: step size based on the line search method. 
\item $\gamma^{(1)}$: initialization of the algorithm.
\item $\gamma^{(k)}$, $\gamma^{(k)'}$: previous and new transportation plans before and after step 1 in the $k-$th iteration of version 1 of our proposed FW algorithm. 
%\item $\hat{\gamma}^{(k)}$, $\hat{\gamma}^{(k)'}$: previous and new transportation plans before and after step 1 in the $k-$th iteration of version 2 of our proposed FW algorithm. 
\item $G_{C,M}=\omega_1C+\omega_2\tilde M+M^\top\circ \gamma$, $G=2\hat M\circ \hat \gamma$: Gradient of the objective function in version 1 and version 2, respectively, of our proposed FW algorithm for solving the discrete version of partial GW problem.
\item $a,b,c\in\mathbb{R}$: given in \eqref{eq:abc_1} and \eqref{eq:abc_2} for algorithm 1 and 2 respectively. 
\item $C^1$-function: continuous and with continuous derivatives.
\item $MPGW_{r,L,\rho}(\cdot,\cdot)$: Mass-Constrained Partial Gromov-Wasserstein defined in \eqref{eq:MPGW}.
\item $\Gamma^\rho_\leq(\mu,\nu)$: set transportation plans defined in \eqref{eq:Gamma_leq^rho} for the (fused) Mass-Constrained Partial Gromov-Wasserstein problem.
\item $UGW_{r,L, \lambda}(\mathbb{X}, \mathbb{Y})$: Unbalanced Gromov-Wasserstein problem defined in \eqref{eq:ugw}.
\item $FUGW_{r,L, \lambda}(\mathbb{X}, \mathbb{Y})$: Fused unbalanced Gromov-Wasserstein problem defined in \eqref{eq:fugw}.
\item $\|\cdot\|_{\rm TV}$: Total variation norm.
\item $d_{FGW,r,q}(\mathbb{X},\mathbb{Y})$: Fused Gromov-Wassertein distance defined in \eqref{eq:fgw_metric}.
\item $d_{FPGW,r,q,\lambda}^p(\mathbb{X},\mathbb{Y})$: Fused partial Gromov-Wasserstein distance defined in \eqref{eq:fpgw_metric}.
\item $G=(V,E)$: a graph. $V=\{v_1,\ldots v_N\}$ is the set of nodes. $E\subset V^2$ is the set of edges. 
\item $d_V:V^2\to \mathbb{R}$: a function used to describe the structure of graph $G$. In the default setting, $d_V$ is the ``shortest distance function''. 
\item $f:V\to \mathcal{F}$: $f$ is the feature mapping defined for each node, i.e., $f(v)$ is the feature for node $v$. $\mathcal{F}$ is the space for all the features. When $\mathcal{F}$ is a discrete set, we say the graph $G$ is a discrete feature graph. When $\mathcal{F}=\mathbb{R}^d$ for some $d\in \mathbb{N}$, we say $G$ is continuous feature graph.  
\item $wl: \mathcal{F}\to S^H$: the  Weisfeiler-Lehman kernel mapping defined in  \cite{vishwanathan2010graph}. It is only well-defined when $\mathcal{F}$ is a finite discrete set. $S$ is finite discrete set; $H\in \mathbb{N}$. $H$ is called the Weisfeiler-Lehman kernel (WL) parameter. In default setting $H\in\{2,4,5\}$. 
\item $d_{\mathcal{F}}$: the metric defined in feature space $\mathcal{F}$. In the default setting, when $\mathcal{F}=\mathbb{R}^d$, $d_{\mathcal{F}}$ is the Euclidean space. When $\mathcal{F}$ is discrete set, $d_{\mathcal{F}}$ is the hamming distance on $\text{wl}(\mathcal{F})$, i.e. 
\begin{align}
d_{\mathcal{F}}(f(v_1),f(v_2)):=\sum_{h=1}^H\delta(wl(f(v_1))_i\neq wl(f(v_2))_i)\label{eq:hamming_dist}
\end{align}

\end{itemize}

\section{Background.}
\textbf{The Optimal Transport (OT) problem} for $\mu,\nu\in\mathcal{P}(\Omega)$,  with transportation cost $c(x,y): \Omega\times \Omega\to \mathbb{R}_+$ being a
lower-semi continuous function, is defined as: 
\begin{align}
&OT(\mu,\nu):=\min_{\gamma\in \Gamma(\mu,\nu)}\gamma(c), \label{eq: OT} \\
&\text{ where } \quad 
\gamma(c):=\int_{\Omega^2}c(x,y) \, d\gamma(x,y) \nonumber
% OT(\mu,\nu):=\min_{\gamma\in \Gamma(\mu,\nu)}\left(\int_{\Omega^2}c(x,y)d\gamma(x,y)\right)^{1/p} \label{eq: OT}
\end{align}
and where $\Gamma(\mu,\nu)$ denotes the set of all joint probability measures on $\Omega^2:=\Omega\times \Omega$ with marginals $\mu,\nu$, i.e., %$  \Gamma(\mu,\nu):=\{\gamma\in\mathcal{P}(\Omega^2): \pi_{1\#}\gamma=\mu,  \pi_{2\#}\gamma=\nu\}$, 
$\gamma_1:=\pi_{1\#}\gamma=\mu,
 \gamma_2:= \pi_{2\#}\gamma=\nu$,
where $\pi_1,\pi_2:\Omega^2\to \Omega$ are the canonical projections  $\pi_1(x,y):=x,\pi_2(x,y):=y$. %To simplify our notation, we define $\gamma_i:=\pi_{i\#}\gamma$ for $i=1,2$. 
A minimizer for \eqref{eq: OT} always exists \cite{Villani2009Optimal,villani2021topics} and when $c(x,y)=\|x-y\|^p$, for $p\ge 1$, 
it defines a metric on $\mathcal{P}(\Omega)$, which is referred to as the ``$p$-Wasserstein distance'':
\begin{align}
W_p^p(\mu,\nu):=\min_{\gamma\in\Gamma(\mu,\nu)}\int_{\Omega^2}\|x-y\|^pd\gamma(x,y). \label{eq: Wp}
\end{align}
% The above formulation \eqref{eq: OT} (or \eqref{eq: Wp}) requires equal mass assumption, which limits its applications to many practical scenarios. To address this limitation, a relaxation named \textbf{partial optimal transport} (or \textbf{optimal partial transport}) 
\textbf{The Partial Optimal Transport (POT) problem} \cite{chizat2018unbalanced, figalli2010new,piccoli2014generalized} extends the OT problem to the set of Radon measures $\mathcal{M}_+(\Omega)$, i.e., non-negative and finite measures. For $\lambda>0$ and $\mu,\nu\in \mathcal{M}_+(\Omega)$, the POT problem is defined as:
\begin{equation}
%\text{POT}(\mu,\nu;\lambda):=&\inf_{\gamma\in\mathcal{M}_+(\Omega^2)}\int_{\Omega^2}c(x,y)d\gamma(x,y)\nonumber\\ 
%&\qquad \qquad+\lambda(|\mu-\gamma_1|+|\nu-\gamma_2|) 
POT(\mu,\nu;\lambda):=\inf_{\gamma\in\mathcal{M}_+(\Omega^2)}\gamma(c)+\lambda(|\mu-\gamma_1|+|\nu-\gamma_2|),
\label{eq:pot}
\end{equation}
where, in general, $|\sigma|$ denotes the total variation norm of a measure $\sigma$, i.e., $|\sigma|:=\sigma(\Omega)$.  
The constraint $\gamma\in \mathcal{M}_+(\Omega^2)$ in \eqref{eq:pot} can be further restricted to $\gamma \in \Gamma_\leq(\mu,\nu)$: 
$$\Gamma_\leq(\mu,\nu):=\{\gamma\in \mathcal{M}_+(\Omega^2): \, \gamma_1\leq \mu,\gamma_2\leq \nu\},$$ denoting $\gamma_1\leq \mu$ if for any Borel set $B\subseteq\Omega$, $\gamma_1(B)\leq \mu(B)$ (respectively, for $\gamma_2\leq \nu$) \cite{figalli2010optimal}. Roughly speaking, the linear penalization indicates that if the classical transportation cost exceeds 
$2\lambda$, it is better to create/destroy' mass (see \cite{bai2022sliced} for further details). In addition, the above formulation has an equivalent form, to distinguish them, we call it ``mass-constraint partial optimal transport problem'' (MPOT): 
\begin{align}
MOPT(\mu,\nu;\rho):=\inf_{\gamma\in\Gamma_\leq^\rho(\mu,\nu)}\gamma(c) \label{eq:MPOT}.
\end{align}




\subsection{Background: (fused)-Unbalanced Gromov Wasserstein.}
The unbalanced Gromov Wasserstein problem is firstly proposed by \cite{sejourne2021unbalanced,chapel2020partial,bai2024efficient}. Given two $mm-$spaces, $\mathbb{X}=(X,d_X,\mu),\mathbb{Y}=(Y,d_Y,\nu)$, the UGW problem is defined by 
\begin{align}
UGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y}):=\inf_{\gamma\in\mathcal{M}_+(X\times Y)}\langle L(d_X^r,d_Y^r), \gamma^{\otimes2} \rangle+\lambda(D_{\phi_1}(\gamma_1^{\otimes2}\parallel \mu^{\otimes2})+D_{\phi_2}(\gamma_2^{\otimes2}\parallel \nu^{\otimes2})),\label{eq:ugw}
\end{align}
where $D_{\phi_1}, D_{\phi_2}$ are f-divergence and can be distinct in the general setting. 

Similar to this work, the fused-Unbalanced Gromov Wasserstein, proposed by \cite{thual2022aligning} is defined as: 

$$FUGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y}):=\inf_{\gamma\in\mathcal{M}_+(X\times Y)}\omega_1\langle C,\gamma\rangle+ \omega_2\langle L(d_X^r,d_Y^r), \gamma^{\otimes2} \rangle+\lambda(D_{\phi_1}(\gamma_1^{\otimes2}\parallel \mu^{\otimes2})+D_{\phi_2}(\gamma_2^{\otimes2}\parallel \nu^{\otimes2})).$$

And when the $f$-divergence terms $D_{\phi_1},D_{\phi_2}$ are KL divergences, the authors in \cite{thual2022aligning} propose a Sinkhorn computational solver. 

At the end of this section, we introduce the following relation between FUGW and FPGW, which is equivalent to the statement (1) in Theorem \ref{thm:main}. 
\begin{proposition}
When $D_{\phi_1},D_{\phi_2}$ are total variation, then we have: 
\begin{align}
FUGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y})&=FPGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y})\nonumber\\
&=\inf_{\gamma\in\Gamma_\leq(\mu,\nu)}\omega_1 \langle C,\gamma\rangle+\omega_2 \langle L(d_X^r,d_Y^r),\gamma^{\otimes2}\rangle +\lambda(|\mu|^2+|\nu|^2-2|\gamma|^2). \nonumber 
\end{align}
Equivalently speaking, one can restrict the searching space from $\mathcal{M}_+(X\times Y)$ to $\Gamma_\leq(\mu,\nu)$. 
\end{proposition}
\begin{proof}
Choose $\gamma\in\mathcal{M}_+(X\times Y)$ and let $\gamma_{Y|X}(\cdot|x)$ denote the conditional measure of $\gamma$ given $x\in X$. Let $\gamma'=\gamma_{Y|X}(\cdot|X)\cdot (\gamma_1\wedge\mu)$, by proposition L.2, we have 
$\gamma'\leq \gamma,\gamma'_1\leq \mu$, and 
$$\omega_2\langle\|d_X-d_Y\|^p,\gamma{'}^{\otimes 2}\rangle+\lambda(\|\mu-\gamma_1'\|_{TV}+\|\nu-\gamma_2'\|_{TV})\leq \omega_2\langle\|d_X-d_Y\|^p,\gamma^{\otimes 2}\rangle+\lambda(\|\mu-\gamma_1\|_{TV}+\|\nu-\gamma_2\|_{TV}).$$

Since $C\ge 0, \gamma'\leq \gamma$, we have 
$\langle C, \gamma'\rangle \leq \langle C, \gamma\rangle$. Therefore, we have 
\begin{align}
&\omega_1 \langle C,\gamma'\rangle+
\omega_2\langle\|d_X-d_Y\|^p,\gamma{'}^{\otimes 2}\rangle+\lambda(\|\mu-\gamma_1'\|_{TV}+\|\nu-\gamma_2'\|_{TV})\nonumber\\
&\leq \omega_1 \langle C,\gamma\rangle+
\omega_2\langle\|d_X-d_Y\|^p,\gamma^{\otimes 2}\rangle+\lambda(\|\mu-\gamma_1\|_{TV}+\|\nu-\gamma_2\|_{TV})\nonumber.
\end{align}
That is, based on $\gamma$, we can construct $\gamma'$ such that $\gamma'_1\leq \mu\wedge \gamma_1,\gamma'\leq \gamma$ and the $\gamma'$ admits smaller cost.
Based on the same process, based on $\gamma'$, we can construct  $ \gamma''$ such that  $\gamma''\leq \gamma'$, $\gamma''\leq \nu$. That is $\gamma''\in\Gamma_\leq(\mu,\nu)$ and admits smaller cost than $\gamma'$. Therefore, we can restrict the searching space for \eqref{eq:fugw} in this case from $\mathcal{M}_+(X\times Y)$ to $\Gamma_\leq(X\times Y)$. 

\end{proof}
\subsection{Existence of minimizes of Fused PGW.}
In this section, we discuss the basic properties of the fused PGW. 

\begin{proposition}\label{pro:fpgw_minimizer}
Given mm-spaces $\mathbb{X}=(X,d_X,\mu),\mathbb{Y}=(Y,d_Y,\nu)$, where $X,Y$ are compact sets, the fused-PGW problems \eqref{eq:fpgw} and \eqref{eq:fmpgw} admit minimizers. That is, we can replace $\inf$ by $\min$ in the formulations.  
\end{proposition}
Note, this proposition is equivalent to the statement (2) in Theorem \ref{thm:main}. 


We first introduce the following statements: 
\begin{proposition}\label{pro:compact}
The set $\Gamma_\leq(\mu,\nu)$ and $\Gamma_\leq^\rho(\mu,\nu)$ are weakly compact, convex sets, where $\rho\in[0,\min(|\mu|,|\nu|)]$.  
\end{proposition}
\begin{proof}
By \cite{liu2023ptlp} Lemma B.1, we have $\Gamma_\leq(\mu,\nu)$ is a weakly compact set. By e.g. \cite{caffarelli2010free,bai2022sliced}, we have $\Gamma_\leq(\mu,\nu)$ is convex. 

It remains to show the compactness and convexity of set $\Gamma^\rho_\leq(\mu,\nu)$. 

Pick  $\gamma^1,\gamma^2\in\Gamma^\rho_\leq(\mu,\nu)$ and $\omega\in[0,1]$ and let $\gamma=(1-\omega)\gamma^1+\omega\gamma^2$.
We have $\gamma\in\Gamma_\leq(\mu,\nu)$ by the convexity of $\Gamma_\leq(\mu,\nu)$. In addition, $$|(1-\omega)\gamma^1+\omega\gamma^2|=(1-\omega)|\gamma^1|+\omega|\gamma^2|\ge (1-\omega)\rho+\omega\rho=\rho,$$ 
thus we have: $\gamma\in\Gamma_\leq^\rho(\mu,\nu)$. 

Pick sequence $(\gamma^n)_{n=1}^\infty\subset \Gamma^\rho_\leq(\mu,\nu)\subset\Gamma_\leq(\mu,\nu)$. By the compactness of $\Gamma_\leq(\mu,\nu)$, there exists a subsequence $(\gamma^{n_k})_{k=1}^\infty$ that converges to some $\gamma^*\in\Gamma_\leq(\mu,\nu)$ weakly. It remains to show $\gamma^*\in\Gamma_\leq^\rho(\mu,\nu)$. 
Indeed, 
$$|\gamma^*|=\langle 1_{X\times Y}\times \gamma\rangle =\lim_{k\to\infty}\langle 1_{X\times Y},\gamma^{n_k} \rangle=\lim_{k\to\infty}|\gamma^{n_k}|.$$
Since for each $k\ge 1$, we have $|\gamma^{n_k}|\ge \rho$, thus $|\gamma^*|\ge \rho$. That is $\gamma^*\in\Gamma_\leq^\rho(\mu,\nu)$, and we complete the proof. 
\end{proof}

\begin{lemma}\label{lem:inf}
Suppose $X, Y$ are compact sets. Let $Z=(X\times Y)$ and $d_Z((x^1,y^1),(x^2,y^2))=d_X(x^1,x^2)+d_Y(y^1,y^2)$. Suppose $\phi: Z^2\to \mathbb{R}$ is Lipschitz continuous and $C: Z\to \mathbb{R}$ is bounded, then the following problem admits a minimizer: 
\begin{align}
\inf_{\gamma\in\mathcal{M}}\langle \phi,\gamma^{\otimes2} \rangle+\langle C,\gamma \rangle \label{eq:inf_prob}
\end{align}
where $\mathcal{M}$ is a non-empty weakly compact set. 
\end{lemma}
\begin{proof}
Pick weakly convergent $(\gamma^n)_{n=1}^\infty\subset \mathcal{M}$, we have
$\gamma^n\overset{w}{\rightharpoonup}\gamma^*\in\mathcal{M}$.
%{\color{red}(Definition of $\mathcal{M}$)}

From lemma C.3 in \cite{bai2024efficient}, we have $$\langle \phi,(\gamma^{n})^{\otimes2}\rangle\to \langle \phi,\gamma^* \rangle. $$
By definition of weak convergence and the fact that the sets $X, Y$ are compact, we have 
\begin{align}
  \langle C,\gamma^n \rangle\to \langle C,\gamma \rangle.\label{pf:convergence}  
\end{align}
Thus, we have 
$$\langle \phi,(\gamma^n)^{\otimes2} \rangle+\langle C,\gamma^n\rangle\to \langle \phi,\gamma^{\otimes2} \rangle+\langle C,\gamma \rangle.$$

Choose a sequence $\gamma^n\in \mathcal{M}$ such that 
$\langle\phi,(\gamma^n)^{\otimes2} \rangle+\langle C,\gamma^n \rangle$ achieves the infimum of the problem \eqref{eq:inf_prob}. By compactness of $\mathcal{M}$, there exists a convergent subsequence $\gamma^{(n_k)}\overset{w}{\rightharpoonup}\gamma^*\in\mathcal{M}$.  By \eqref{pf:convergence}, we have $\gamma^*$ is a minimizer of \eqref{eq:inf_prob}. Thus, we complete the proof.  
\end{proof}

\begin{proof}[Proof of Proposition \ref{pro:fpgw_minimizer}.]
From Lemma C.1. in \cite{bai2024efficient}, we have $$(X\times Y)^2\ni ((x^1,y^1),(x^2,y^2))\to |d_X^r(x^1,x^2)-d_Y^r(y^1,y^2)|^q$$ is Lipschitz continuous. In addition, $C: X\times Y$ is a bounded mapping. From lemma \ref{pro:compact}, we have $\Gamma_\leq(\mu,\nu),\Gamma_\leq^\rho(\mu,\nu)$ are compact. From lemma \ref{lem:inf}, we complete the proof.  
\end{proof}

\section{Metric property of Fused PGW}
In this section, we discuss the proof of Theorem  \ref{thm:main} (3).
We will discuss the details in the following subsections. The related conclusion can be treated as an extension of Theorem 6.3 in \cite{vay2019fgw} and the Theorem 3.1 in \cite{vayer2020fused}. 

\subsection{Background: Isometry, fused-GW semi-metric.}


Given $\mathbb{X}=(X,d_X,\mu),\mathbb{Y}=(Y,d_Y,\nu)$, we say $X,Y$ are equivalent, noted $X\sim Y$ if and only if: 

There exists a mapping $\phi: X\to Y$ such that
\begin{itemize} 
    \item $\phi_\#\mu=\nu$
    \item $d_X(x,x')=d_Y(\phi(x),\phi(x')),\forall x,x'\in X$. 
\end{itemize}
Such a function $\phi$ is called \textbf{measure preserving isometry}.

In addition, in the formulation of fused-GW \eqref{eq:fpgw}, we set $d(x,y)=\|x-y\|^q$ and $L(\mathrm{r}_1,\mathrm{r}_2)=|\mathrm{r}_1-\mathrm{r}_2|^q$. The reduced formulation is called ``fused-Gromov Wasserstein distance'' \cite{vayer2020fused}:

\begin{align}
d_{FGW,r,q}(\mathbb{X},\mathbb{Y}):=
\inf_{\gamma\in\Gamma(\mu,\nu)}
\int_{(X\times Y)^2}\omega_1\frac{\|x-y\|^q}{|\gamma|}+\omega_2|d_X^r(x,x')-d_Y^r(y,y')|^q d\gamma(x,y)d\gamma(x',y').
\label{eq:fgw_metric}
\end{align}
and it defines a metric where the identity is induced by the above equivalence relation. 

Inspired by this formulation, we introduce the \textbf{Fused Partial GW metric}: 
{\footnotesize
\begin{align}
d_{FPGW,r,q,\lambda}^p(\mathbb{X},\mathbb{Y}):=
\inf_{\gamma\in\Gamma_\leq(\mu,\nu)}
\int_{(X\times Y)^2}\omega_1\frac{\|x-y\|^q}{|\gamma|}+\omega_2|d_X^r(x,x')-d_Y^r(y,y')|^q+\lambda\left(\frac{|\mu|^2}{|\gamma|^2}+\frac{|\nu|^2}{|\gamma|^2}-2\right) d\gamma(x,y)d\gamma(x',y').
\label{eq:fpgw_metric}
\end{align}
}

\begin{remark}
We adopt the convention $\frac{1}{0}\cdot0=1$, thus the above formulation is
well-defined. In particular, when $|\gamma|=0$, i.e., $\gamma$ is zero measure,
the above integration is defined by 
\begin{align}
&\int_{(X\times Y)^2}\omega_1\frac{\|x-y\|^q}{|\gamma|}+\omega_2|d_X^r(x,x')-d_Y^r(y,y')|^q+\lambda\left(\frac{|\mu|^2}{|\gamma|^2}+\frac{|\nu|^2}{|\gamma|^2}-2\right) d\gamma(x,y)d\gamma(x',y')\nonumber\\
&=\lambda(|\mu|^2+|\nu|^2)\label{eq:fpgw_gamma0}\\
&=\lim_{|\gamma|\searrow 0}\int_{(X\times Y)^2}\omega_1\frac{\|x-y\|^q}{|\gamma|}+\omega_2|d_X^r(x,x')-d_Y^r(y,y')|^q+\lambda\left(\frac{|\mu|^2}{|\gamma|^2}+\frac{|\nu|^2}{|\gamma|^2}-2\right) d\gamma(x,y)d\gamma(x',y').\nonumber
\end{align}
% Then, it is straightforward to verify: 
% \begin{align}
% &\int_{(X\times Y)^2}\omega_1\|x-y\|^q+\omega_2|d_X^r(x,x')-d_Y^r(y,y')|+\lambda(\frac{|\mu|}{|\gamma|}+\frac{|\nu|^2}{|\gamma|^2}-2) d\bold{0}(x,y)d\bold{0}(x',y')\nonumber\\
% &=\lim_{|\gamma|\to 0}\int_{(X\times Y)^2}(\omega_1\|x-y\|^q+\omega_2|d_X^r(x,x')-d_Y^r(y,y')|+\lambda(\frac{|\mu|^2}{|\gamma|^2}+\frac{|\nu|^2}{|\gamma|^2}-2))^p d\gamma(x,y)d\gamma(x',y')  \nonumber. 
% \end{align}
\end{remark}
\begin{remark}
By Proposition \ref{pro:fpgw_minimizer}, the above problem admits a minimizer. 
\end{remark}

Next, we introduce the formal statement of Theorem \ref{thm:main} (2). 

\begin{theorem}\label{thm:metric_formal}
Define space for mm-spaces, 
$$\mathcal{G}=\{\mathbb{X}=(X,d_X,\mu), X\subset\mathbb{R}^d, X \text{ is compact}; d_X \text{ is a metric}; \mu\in\mathcal{M}_+(X) \}.$$
 
Then Fused PGW \eqref{eq:fpgw_metric}  defines a semi-metric in quotient space $\mathcal{G}/\sim$. In particular: 
\begin{enumerate}
    \item $d_{FPGW,r,q,\lambda}(\cdot,\cdot)$ is non-negative and symmetric. 
   
    \item  Suppose $\omega_2,\lambda>0$, then 
    $d_{FPGW,r,q,\lambda}(\mathbb{X},\mathbb{Y})=0$ iff $\mathbb{X}\sim\mathbb{Y}$. 
    \item If $\omega_2>0$, for $q\ge 1$, we have  \begin{align}
     d_{FPGW,r,q,\lambda}(\mathbb{X},\mathbb{Y})\leq 2^{q-1}(d_{FPGW,r,q,\lambda}(\mathbb{X},\mathbb{Z})+d_{FPGW,r,q,\lambda}(\mathbb{Y},\mathbb{Z})).\label{eq:triangle_ineq}   
    \end{align}
    In particular, when $q=1$, fused-PGW satisfies the triangle inequality. 
\end{enumerate}
\end{theorem}

\subsection{Proof of the (1)(2) in Theorem \ref{thm:metric_formal}}
For statement (1), by definition, for each $\gamma\in\Gamma_\leq(\mu,\nu)$, we have 
$$\int_{X\times Y} \|x-y\|^q d\gamma,\int_{(X\times Y)^2} \|d^r_X(x,x')-d^r_Y(y,y')\|^q d\gamma^{\otimes2}\ge 0.$$
Thus, $d_{FPGW,r,q,\lambda}(\mathbb{X},\mathbb{Y})\ge0$. %Thus, we prove the non-negativity. 


In addition, 
\begin{align}
&d_{FPGW,r,q,\lambda}(\mathbb{X},\mathbb{Y})\nonumber\\ &=\inf_{\gamma\in\Gamma_\leq(\mu,\nu)}\int_{(X\times Y)^2}\omega_1\|x-y\|^q +\omega_2|d_X^r(x,x')-d_Y^r(y,y')|^q)^p d\gamma(x,y)d\gamma(x',y')+\lambda(|\mu|^2+|\nu|^2-2|\gamma|^2)\nonumber\\
&=\inf_{\gamma\in\Gamma_\leq(\nu,\mu)}\int_{(Y\times X)^2}\omega_1\|y-x\|^q +\omega_2|d_Y^r(y,y')-d_X^r(x,x')|^q)^p d\gamma(y,x)d\gamma(y',x')+\lambda(|\mu|^2+|\nu|^2-2|\gamma|^2)\nonumber\\
&=d_{FPGW,r,q,\lambda}(\mathbb{Y},\mathbb{X})\nonumber. 
\end{align}
And we prove the symmetric. 


For statement (2), suppose measure preserving isometry $\phi: X\to Y$ exists, then we have $|\mu|=|\nu|$. Let $\gamma=(\text{id}\times\phi)_\#\mu$, we have $\gamma\in\Gamma(\mu,\nu)\subset \Gamma_\leq(\mu,\nu)$. Thus $|\gamma|=|\mu|=|\nu|$. 

Furthermore, 
\begin{align}
&d_{FPGW,r,q,\lambda}^p(\mathbb{X},\mathbb{Y})\nonumber\\ &\leq \int_{(X\times Y)^2}\omega_1\|x-y\|^q +\omega_2|d_X^r(x,x')-d_Y^r(y,y')|^q d\gamma(x,y)d\gamma(x',y')+\lambda(|\mu|^2+|\nu|^2-2|\gamma|^2)\nonumber\\
&=\int_{(X\times Y)^2}\omega_1\underbrace{\|x-\phi(x)\|^q}_{0} +\omega_2\underbrace{|d_X^r(x,x')-d_Y^r(\phi(x),\phi(x'))|^q}_{0} d\mu(x)d\mu(x')+\lambda(\underbrace{|\mu|^2+|\nu|^2-2|\gamma|^2}_0)\nonumber\\
&=0.
\end{align}

For the other direction, suppose 
$d_{FPGW,r,q,\lambda}(\mathbb{X},\mathbb{Y})=0$. We have two cases: 


Case 1: $|\mu|=|\nu|=0$. We've done. 

Case 2: $|\mu|>0$ or  $|\nu|>0$. 

By the Proposition \ref{pro:fpgw_minimizer}, a minimizer for problem \eqref{eq:fpgw_metric} exists. Choose one minimizer, denoted as $\gamma^*$. 

First, we claim $|\mu|=|\nu|=|\gamma^*|$. 

Indeed, assume the above equation is not true. For convenience, we suppose $|\mu|<|\nu|$. Then $|\gamma|\leq |\mu|<|\nu|$. 
We have 
\begin{align}
0=d_{FPGW,r,q,\lambda}(\mathbb{X},\mathbb{Y})\ge \lambda(|\mu|^2+|\nu|^2-2|\gamma^*|^2)> \lambda(|\mu|^2-|\gamma^*|^2)\ge0, 
\end{align}
since $\lambda>0$. Thus we have contradiction. 

Since $\gamma^*\in\Gamma_\leq(\mu,\nu)$, we have $\gamma^*\in\Gamma(\mu,\nu)$. Thus, we have
\begin{align}
0&\leq d_{FPGW,r,q}(\mathbb{X},\mathbb{Y}) \nonumber\\
&\leq \int_{(X \times Y)^2} \omega_1 \frac{\|x-y\|^q}{|\gamma|} + \omega_2 |d_X^r(x,x') - d_Y^r(y,y')|^q \, d\gamma^*(x,y) d\gamma^*(x',y') \nonumber\\
&= \int_{(X \times Y)^2} \omega_1 \frac{\|x-y\|^q}{|\gamma|} + \omega_2 |d_X^r(x,x') - d_Y^r(y,y')|^q 
+ \lambda \left( \frac{|\mu|^2}{|\gamma|^2} + \frac{|\nu|^2}{|\gamma|^2} - 2 \right) 
\, d\gamma^*(x,y) d\gamma^*(x',y') \nonumber\\
&=d_{FPGW,r,q}(\mathbb{X},\mathbb{Y})\nonumber\\
&=0.\nonumber
\end{align}
That is, $d_{FPGW,r,q}(\mathbb{X},\mathbb{Y})=0$. Since $\omega_2>0$, by Proposition 5.2 in \cite{vayer2020fused} or Theorem 6.4 in \cite{vay2019fgw}, there exists measure preserving isometry $\phi: X\to Y$ and thus we have $\mathbb{X}\sim\mathbb{Y}$ and we complete the proof. 


\subsection{Proof of statement (3) in Theorem \ref{thm:metric_formal}.}
Choose mm-spaces $\mathbb{S}=(S,d_S,\sigma),\mathbb{X}=(X,d_X,\mu),\mathbb{Y}=(Y,d_Y,\nu)$. In this section, we will prove the triangle inequality
$$d_{FPGW,r,q,\lambda}(\mathbb{X},\mathbb{Y})\leq d_{FPGW,r,q,\lambda}(\mathbb{S},\mathbb{X})+d_{FPGW,r,q,\lambda}(\mathbb{S},\mathbb{Y}).$$

First, introduce auxiliary points $\hat\infty_0,\hat\infty_1,\hat\infty_2$ and set 
\begin{align}
\begin{cases}
\hat{S}&=S\cup 
\{\hat\infty_0,\hat\infty_1,\hat\infty_2\},\nonumber\\
\hat{X}&=X\cup \{\hat\infty_0,\hat\infty_1,\hat\infty_2\},\nonumber \\
\hat{Y}&=Y\cup \{\hat\infty_0,\hat\infty_1,\hat\infty_2\}.\nonumber
\end{cases}
\end{align}
Define $\hat\sigma, \hat\mu,\hat\nu$ as follows: 
\begin{equation}
\begin{cases} 
\hat\sigma&=\sigma+|\mu|\delta_{\hat\infty_1}+|\nu|\delta_{\hat\infty_2},\\
\hat\mu&=\mu+|\sigma|\delta_{\hat\infty_0}+|\nu|\delta_{\hat\infty_2},\\
\hat\nu&=\nu+|\sigma|\delta_{\hat\infty_0}+|\mu|\delta_{\hat\infty_1}.
\end{cases}\label{pf:sigma_hat,mu_hat,nu_hat}    
\end{equation}


Next, we define $d_{\hat{S}}:\hat{S}^2\to \mathbb{R}\cup\{\infty\}$ as follows: 
\begin{align}
d_{\hat{S}}(s,s')=\begin{cases}
d_S(s,s') &\text{if }(s,s')\in S^2,\\
\infty &\text{elsewhere. }
\end{cases} \label{pf:d_S_hat}
\end{align}
Note, $d_{\hat{S}}(\cdot,\cdot)$ is not a rigorous metric in $\hat{S}$ since we allow $d_{\hat{S}}=\infty$, $d_{\hat{X}},d_{\hat{Y}}$ are defined similarly. 

Then, we define the following mapping $L_\lambda: (\mathbb{R}\cup\{\infty\})\times (\mathbb{R}\cup\{\infty\})\to \mathbb{R}_+$: 
\begin{align}\label{eq:D_lambda}
    L^q_{\lambda}(\mathrm{r}_1,\mathrm{r}_2)=
    \begin{cases}
    |\mathrm{r}_1-\mathrm{r}_2|^q &\text{if }\mathrm{r}_1,\mathrm{r}_2<\infty, \\ 
    \lambda/\omega_2  &\text{if }\mathrm{r}_1=\infty, \mathrm{r}_2<\infty \text{ or vice versa},\\
    0 &\text{if }\mathrm{r}_1=\mathrm{r}_2=\infty;
    \end{cases}
\end{align}

and mapping $\hat{D}
:\mathbb{R}^d\cup\{\hat{\infty}_i: i\in[0:2]\}\to \mathbb{R}$: 
\begin{align}
\hat{D}^q(x,y):=\begin{cases}
\|x-y\|^q &\text{if }x,y\in\mathbb{R}^d\\
0 &\text{elsewhere}. 
\end{cases}
\end{align}

Finally, we define the following mappings: 

\begin{align}
&\Gamma_\leq(\sigma,\mu)\ni \gamma^{01}\mapsto\hat\gamma^{01} \in \Gamma(\hat\sigma,\hat\mu), \nonumber\\
&\hat\gamma^{01}:=\gamma^{01}+(\sigma-\gamma_1^{01})\otimes\delta_{\hat\infty_0}+\delta_{\hat\infty_1}\otimes(\mu-\gamma_2^{01})+|\gamma|\delta_{\hat\infty_1,\hat\infty_0}+|\nu|\delta_{\hat\infty_2,\hat\infty_2};\nonumber \\
&\Gamma_\leq(\sigma,\nu)\ni \gamma^{02}\mapsto\hat\gamma^{02} \in \Gamma(\hat\sigma,\hat\nu), \nonumber\\
&\hat\gamma^{02}:=\gamma^{02}+(\sigma-\gamma_1^{02})\otimes\delta_{\hat\infty_0}+\delta_{\hat\infty_2}\otimes(\nu-\gamma_2^{02})+|\gamma|\delta_{\hat\infty_2,\hat\infty_0}+|\mu|\delta_{\hat\infty_1,\hat\infty_1}; \nonumber \\
&\Gamma_\leq(\mu,\nu)\ni \gamma^{12}\mapsto\hat\gamma^{12}\in\Gamma(\hat\mu,\hat\nu), \nonumber\\
&\hat\gamma^{12}:=\gamma^{12}+(\mu-\gamma_1^{12})\otimes\delta_{\hat\infty_1}+\delta_{\hat\infty_2}\otimes(\nu-\gamma_2^{12})+|\gamma|\delta_{\hat\infty_2,\hat\infty_1}+|\mu|\delta_{\hat\infty_0,\hat\infty_0} \label{eq:gamma_hat_12}.
\end{align}

\begin{remark}
It is straightforward to verify the above mappings are well-defined. 
In addition, we can observe that, for each $\gamma^{01}\in\Gamma_\leq(\sigma,\mu),\gamma^{02}\in\Gamma_\leq(\sigma,\nu),\gamma^{12}\in\Gamma_\leq(\mu,\nu)$,
\begin{align}
&\hat{\gamma}^{01}(\{\hat\infty_2\}\times X)=\hat{\gamma}^{01}(S\times \{\hat\infty_2\})=0,  \label{pf:gamma01_cond} \\
&\hat{\gamma}^{02}(\{\hat\infty_1\}\times Y)=\hat{\gamma}^{02}(S\times \{\hat\infty_1\})=0,  \label{pf:gamma02_cond} \\
&\hat{\gamma}^{12}(\{\hat\infty_0\}\times Y)=\hat{\gamma}^{12}(X\times \{\hat\infty_0\})=0.  \nonumber %\label{pf:gamma12_cond} 
\end{align}
\end{remark}


Based on these concepts, we define the following fused-GW variant problem: 
\begin{align}
\hat{d}_{FGW,r,q,\lambda}(\hat{\mathbb{X}}, \hat{\mathbb{Y}}) 
:= \inf_{\hat{\gamma} \in \Gamma(\hat{\mu}, \hat{\nu})} 
\int_{(\hat{X} \times \hat{Y})^2} \omega_1 \frac{1}{c}\hat D^q(x,y) 
+ \omega_2 L^q_\lambda( d_{\hat{X}}^r(x, x'), d_{\hat{Y}}^r(y, y')) 
 d\hat{\gamma}(x, y)d\hat{\gamma}(x', y').
\label{eq:fgw_hat_metric}
\end{align}
where constant $c=|\sigma|+|\mu|+|\nu|$. 

\begin{proposition}\label{pro:fpgw-fgw}
For each  $\gamma^{12}\in\Gamma(\mu,\nu)$, construct $\hat{\gamma}^{12}\in\Gamma(\hat\mu,\hat\nu)$, we have: 
\begin{align}
&\int_{(X\times Y)^2} \left( \omega_1 \frac{\|x-y\|^q}{|\gamma^{12}|} + \omega_2 \left| d^r_{X}(x,x') - d^r_{Y}(y,y') \right|^q + \lambda \left( \frac{|\mu|}{|\gamma^{12}|} + \frac{|\nu|}{|\gamma^{12}|} - 2 \right) \right) 
\, d(\gamma^{12})^{\otimes 2} \nonumber\\
&=\int_{(\hat X\times \hat Y)^2} \left( \omega_1 \frac{D^q(x,y)}{c} + \omega_2 L^q_\lambda(d_{\hat{X}}(x,x'), d_{\hat{Y}}(y,y')) \right) 
\, d(\hat\gamma^{12})^{\otimes 2} \label{eq:gamma^12-hat_gamma^12}
\end{align}
Furthermore, we have: 
\begin{align}
d_{FPGW,r,q,\lambda}(\mathbb{X},\mathbb{Y})=\hat{d}_{FGW,r,q,\lambda}(\hat{\mathbb{X}},\hat{\mathbb{Y}}) \label{eq:fpgw-fgw}.
\end{align}

\end{proposition}
\begin{proof}
We have: 
\begin{align}
&\int_{(X\times Y)^2}  \omega_1 \frac{\|x-y\|^q}{|\gamma^{12}|}
, d(\gamma^{12})^{\otimes 2} \nonumber\\
&=\int_{(X\times Y)}\omega_1 \|x-y\|^q d\gamma^{12}\nonumber\\
&=\int_{(\hat X\times \hat Y)}\omega_1 D^q(x,y) d\hat\gamma^{1,2} \nonumber\\
&=\int_{(\hat X\times \hat Y)^2}  \omega_1 \frac{D^p(x,y)}{|\hat\gamma^{12}|} 
\, d(\hat\gamma^{12})^{\otimes 2} \nonumber\\
&=\int_{(\hat X\times \hat Y)^2}  \omega_1 \frac{1}{c}D^p(x,y), d(\hat\gamma^{12})^{\otimes 2}.
\end{align}
In addition, by \cite{bai2024efficient} Proposition D.3. We have 
\begin{align}
&\int_{(X\times Y)^2}\omega_2 |d_X^r(x,x')-d_Y^r(y,y')|^q d(\gamma^{12})^{\otimes2}+\lambda(|\mu|^2+|\nu|^2-2|\gamma^{12}|^2)\nonumber\\
&=\int_{(\hat X\times \hat Y)^2}\omega_2 D_\lambda^q(d_{\hat X}^r(x,x'),d_{\hat Y}^r(y,y'))d(\hat \gamma^{12})^{\otimes2}.
\end{align}
Combining the above two equalities, we prove \eqref{eq:gamma^12-hat_gamma^12}. 

Now, we prove the second equality. Note, if we merge the three auxiliary points, i.e., $\hat\infty_1=\hat\infty_2=\hat\infty_3$. The optimal value for the above problem \eqref{eq:fgw_hat_metric} is unchanged. 

As we merged the points $\hat\infty_1,\hat\infty_2,\hat\infty_3$, by Bai et al. \cite{bai2022sliced}, the mapping $\gamma^{12}\mapsto \hat\gamma^{12}$
defined in \eqref{eq:gamma_hat_12} is a bijection. Thus, by \eqref{eq:gamma^12-hat_gamma^12}, we have $\gamma^{12}\in\Gamma_\leq(\mu,\nu)$ is optimal in \eqref{eq:fpgw_metric} iff  $\hat{\gamma}^{12}$ is optimal in \eqref{eq:fgw_hat_metric} and we complete the proof. 
\end{proof}



\begin{lemma}\label{lem:gamma_hat}
Choose $\gamma^{01}\in\Gamma_\leq(\sigma,\mu),\gamma^{02}\in\Gamma_\leq(\sigma,\nu),\gamma^{12}\in\Gamma_\leq(\mu,\nu)$ and construct $\hat{\gamma}^{01},\hat{\gamma}^{02},\hat{\gamma}^{12}$. Then there exists $\hat\gamma\in\mathcal{M}_+(\hat{S}\times\hat{X}\times\hat{Y})$ such that:
\begin{align}
&(\pi_{0,1})_\#\hat\gamma= \hat \gamma^{01}, \label{eq:gamma_cond1}\\
&(\pi_{0,2})_\#\hat\gamma= \hat \gamma^{02}, \label{eq:gamma_cond2}\\
&\gamma(A_i)=0,\forall i=0,1,2 \text{ where }A_i=\{\hat\infty_i\}\times X\times Y.
\end{align}
\end{lemma}
\begin{proof}
By \textit{gluing lemma} (see Lemma 5.5 \cite{santambrogio2015optimal}), there exists $\hat\gamma\in\mathcal{M}_+(\hat{S}\times\hat{X}\times\hat{Y})$, such that \eqref{eq:gamma_cond1},\eqref{eq:gamma_cond2} are satisfied. 
For the third property, we have: 

\begin{align}
&\hat\gamma(A_0)\leq \hat\gamma(\{\infty_0\}\times \hat X\times \hat{Y})=\hat\sigma(\{\infty_0\})=0\nonumber \qquad\text{by definition \eqref{pf:sigma_hat,mu_hat,nu_hat} of $\hat\sigma$ , }\nonumber\\
&\hat\gamma(A_1)\leq \hat\gamma(\{\infty_1\}\times  \hat X\times Y)=\hat\gamma^{02}(\{\hat\infty_1\times Y\})=0 \qquad\text{by  }\eqref{pf:gamma02_cond}, \nonumber \\
&\hat\gamma(A_2)\leq \hat\gamma(\{\infty_2\}\times   X\times \hat Y)=\hat\gamma^{01}(\{\hat\infty_2\times X\})=0 \qquad\text{by }\eqref{pf:gamma01_cond} \nonumber, 
\end{align}
and we complete the proof. 
\end{proof}


Now, we demonstrate the proof of triangle inequality for fused-PGW distance \eqref{eq:fpgw_metric}. 

\begin{proof}[Proof of Theorem \ref{thm:metric_formal} (3).]

Note, by the Proposition \ref{pro:fpgw-fgw}, the triangle inequality for fused-PGW distance \eqref{eq:fpgw_metric} is equivalent to show 
\begin{align}
\hat{d}_{FGW,r,q,\lambda}(\hat{\mathbb{X}},\hat{\mathbb{Y}})\leq 2^{q-1}(\hat{d}_{FGW,r,q,\lambda}(\hat{\mathbb{S}},\hat{\mathbb{X}})+\hat{d}_{FGW,r,q,\lambda}(\hat{\mathbb{S}},\hat{\mathbb{Y}}))\label{pf:triangle_inq}.
\end{align}

Choose optimal transportation plans  $\gamma^{01},\gamma^{02},\gamma^{12}$ for fused PGW problems 
$d_{FPGW,r,\lambda}(\mathbb{S},\mathbb{X}),d_{FPGW,r,\lambda}(\mathbb{S},\mathbb{Y})$ and $d_{FPGW,r,\lambda}(\mathbb{X},\mathbb{Y})
$ respectively. We construct the corresponding  $\hat{\gamma}^{01},\hat{\gamma}^{02},\hat{\gamma}^{12}$. By the Proposition \ref{pro:fpgw-fgw}, $\hat\gamma^{01},\hat\gamma^{02},\hat\gamma^{12}$ are optimal. 


Choose $\hat\gamma$ in lemma \ref{lem:gamma_hat}. We have: 

\begin{align}
&\hat{d}_{FGW,r,q,\lambda}(\mathbb{X},\mathbb{Y})\nonumber\\
&=\int_{(\hat X\times \hat Y)^2}\omega_1 \frac{D^q(x,y)}{c}+\omega_2 L^q_\lambda(d_{\hat{X}}(x,x'),d_{\hat{Y}}(y,y'))d\hat\gamma^{12}(x,y)d\hat\gamma^{12}(x',y')\nonumber\\
&\leq \int_{(\hat{S}\times \hat X\times \hat Y)^2}\omega_1 \frac{D^q(x,y)}{c}+\omega_2 L^q_\lambda(x,y)d\hat\gamma(s,x,y)d\hat\gamma(s',x',y')\nonumber\\
&=\underbrace{\int_{(\hat{S}\times \hat X\times \hat Y)}\omega_1 D^q(x,y)d\hat\gamma(s,x,y)}_A+\underbrace{\int_{(\hat{S}\times \hat X\times \hat Y)^2}\omega_2 L^q_\lambda(x,y)d\hat\gamma(s,x,y)d\hat\gamma(s',x',y')}_B.\nonumber 
\end{align}

To bound $A$, we consider the case $D(x,y)>D(s,x)+D(s,y)$ for some $(s,x,y)\in \hat{S}\times \hat{X}\times \hat{Y}$. By definition of $D$, we have $s\in \{\hat{\infty}_i:i\in[0:2]\},x\in X,y\in Y$. That is, $(s,x,y)\in \bigcup_{i=0}^2A_i$. By lemma \ref{lem:gamma_hat}, we have $\hat\gamma(\bigcup_{i=0}^2A_i)=0$. That is, this case has a measure of 0. 

Thus, we have 
\begin{align}
&A\leq \int_{\hat{S}\times \hat{X}\times \hat{Y}}\omega_1 (D(s,x)+D(s,y))^q d\hat\gamma(s,x,y)\nonumber\\
&\leq \int_{\hat{S}\times \hat{X}\times \hat{Y}}\omega_1 (2^{q-1}D(s,x)+2^{q-2}D(s,y)) d\hat\gamma(s,x,y)\nonumber\\
&=\int_{\hat{S}\times \hat{X}\times \hat{Y}}\omega_1 2^{q-1}D(s,x) d\hat\gamma(s,x,y)+\int_{\hat{S}\times \hat{X}\times \hat{Y}}\omega_1 2^{q-2}D(s,y) d\hat\gamma(s,x,y)\nonumber\\
&= 2^{q-1}\int_{\hat{S}\times \hat{X}}\omega_1\frac{D(s,x)}{c} d\hat\gamma^{01}(s,x)+2^{q-1}\int_{\hat{S}\times \hat{Y}}\omega_1\frac{D(s,y)}{c} d\hat\gamma^{02}(s,y)\label{pf:triangle_term1}.
\end{align}
where the second inequality follows from the fact
\begin{align}
(a+b)^q\leq 2^{q-1}a^q+2^{q-2}b^q,\forall a,b\ge 0 \label{eq:ab_inq}.
\end{align}

Now we bounde the term $B$.  Proposition D.4 in \cite{bai2024efficient}, we have 
\begin{align}
B&\leq \int_{(\hat{S}\times \hat X\times \hat Y)^2}\omega_2 (L_\lambda(s,x)+L_\lambda(s,y))^qd\hat\gamma(s,x,y)d\hat\gamma(s',x',y')\nonumber\\
&\leq \int_{(\hat{S}\times \hat X\times \hat Y)^2}\omega_2 \left(2^{q-1}L_\lambda^q(s,x)+2^{q-1}L_\lambda^q(s,y)\right)d\hat\gamma(s,x,y)d\hat\gamma(s',x',y')\nonumber\\
&=2^{q-1}\int_{(\hat{S}\times \hat X)^2}\omega_2 L_\lambda^q(s,x)d\hat\gamma^{01}(s,x)d\hat\gamma^{01}(s',x')+2^{q-1}\int_{(\hat{S}\times \hat Y)^2}\omega_2 L_\lambda^q(s,y)d\hat\gamma^{02}(s,y)d\hat\gamma^{02}(s',y')\label{pf:triangle_term2}.
\end{align}
where the second inequality holds from \eqref{eq:ab_inq}. 
Combining \eqref{pf:triangle_term1} and \eqref{pf:triangle_term2}, we prove the inequality \eqref{pf:triangle_inq} and we complete the proof. 
\end{proof}

\subsection{Future's direction.}
Note, the general version for (the p-th power of) fused-Gromov Wasserstein distance is defined by 
\begin{align}
d_{FGW,r,q,p}^p(\mathbb{X},\mathbb{Y}):=\inf_{\gamma\in\Gamma(\mu,\nu)}\int \left(\omega_1 \|x-y\|^q+|d_X(x,x')-d_Y(y,y')|^q\right)^pd\gamma^{\otimes2}\label{eq:fgw_metric_general}. 
\end{align}
Inspired by the above formulation, we propose the following generalized fused-partial Gromov Wasserstein distance: 

\begin{align}
d_{FGW,r,q,p}^p(\mathbb{X},\mathbb{Y}):=\inf_{\gamma\in\Gamma(\mu,\nu)}\int \left(\omega_1 \frac{\|x-y\|^q}{|\gamma|}+|d_X(x,x')-d_Y(y,y')|^q+\lambda(\frac{|\mu|^2}{|\gamma|^2}+\frac{|\nu|^2}{|\gamma|^2}-2)\right)^pd\gamma^{\otimes2}\label{eq:fgw_metric_general_0}. 
\end{align}

The fused-PGW distance defined in \eqref{eq:fpgw_metric} can be treated as a special case of this general formulation by setting $p=1$. In our conjecture, a similar (semi-)metric property proposed in Theorem \ref{thm:metric_formal} holds for the above general form. We leave the theoretical study of the metric property for our future work. 

\section{Gradient computation of FW algorithms}
In this section, we discuss in detail the gradient computation in fused-MPGW and fused-PGW. 


\subsection{Basics in Tensor product}
In this section, we introduce some fundamental results for tensor computation. 
Suppose $M\in \mathbb{R}^{n\times m\times n\times m}$. We define the \textbf{Transportation of tensor}, denoted as $M^\top$, as:
\begin{align}
M^\top_{i,j,i',j'}=M_{i',j',i,j}, \label{eq:M_trans}
\end{align}
and we say $M$ is symmetric if $M=M^\top$. 

It is straightforward to verify the following: 
\begin{proposition}
Given tensor $M\in\mathbb{R}^{n\times m\times n\times m}$, then we have: 
\begin{itemize}
    \item $(M^\top )^\top=M.$
    \item $M^\top\circ\gamma=[\sum_{i',j'}M_{i',j',i,j}\gamma_{i,j}]_{i,j\in[1:n]\times[1:m]}$. 
\end{itemize}
\end{proposition}
\begin{proof}
The first item follows from the definition of $M^\top$. For the second statement, pick $(i,j)\in[1:n]\times [1:m]$, we have
\begin{align}
&\sum_{i',j'}M_{i',j',i,j}\gamma_{i',j'}\nonumber\\
&=\sum_{i',j'}M^\top_{i,j,i',j'}\gamma_{i',j'}\nonumber\\
&=\langle M^\top\circ \gamma\rangle \nonumber. 
\end{align}
\end{proof}

Therefore, we have: 
\begin{proposition}
The gradient for $\mathcal{L}_{C,M}(\gamma)$ in Fused-PGW \eqref{eq:fmpgw} is given by:      
\begin{align}
\nabla \mathcal{L}_{C,M}(\gamma)=\omega_1 C+\omega_2( M\circ \gamma +M^\top \circ\gamma )\label{eq:gradient}.
\end{align}.

Similarly, the gradient for $\mathcal{L}_{C,M-2\lambda}$ in fused-PGW \eqref{eq:fpgw} is given by: 

\begin{align}
\nabla \mathcal{L}_{C,M-2\lambda}(\gamma)=\omega_1 C+\omega_2( (M-2\lambda)\circ \gamma +(M-2\lambda)^\top \circ\gamma )\label{eq:gradient_1}.
\end{align}.

\end{proposition}
\begin{proof}
Pick $(i,j)\in[1:n]\times[1:m]$, we have: 
\begin{align}
&\frac{\partial }{\partial \gamma_{ij}}L_{C,M}(\gamma)\nonumber\\
&=\frac{\partial }{\partial \gamma_{ij}}\sum_{i,j,i',j'}\omega_1C_{i,j}\gamma_{i,j}+\omega_2\sum_{i,j,i',j'}M_{i,j,i',j'}\gamma_{i,j}\gamma_{i',j'}\nonumber\\
&=\omega_1C_{i,j}+\omega_2(\sum_{i',j'}M_{i,j,i',j'}\gamma_{i,j}\gamma_{i',j'}+\sum_{i',j'}M_{i',j',i,j}\gamma_{i,j}\gamma_{i',j'})\nonumber
\end{align}
Therefore, $\nabla L_{C,M}(\gamma)=\omega_1 C+\omega_2(M\circ\gamma+M^\top\circ\gamma)$ and we complete the proof. The gradient for Fused-PGW \eqref{eq:fpgw} can be derived similarly. 
\end{proof}

At the end of this subsection, we discuss the computation of $M\circ\gamma$ and $M^\top\gamma$. 

In general, the computation cost for $M\circ\gamma$ is $n^2m^2$. However, if the cost function $L$ satisfies: 
\begin{align}
  L(\mathrm{r}_1,\mathrm{r}_2)=f_1(\mathrm{r}_1)+f_2(\mathrm{r}_2)-h_1(\mathrm{r}_1)h_2(\mathrm{r}_2),\label{eq:cond_L}  
\end{align}

\begin{align}
M\circ\gamma=f_1(C^X)\gamma_11_m^\top+1_n\gamma_2^\top f_2(C^Y)-h_1(C^X)\gamma h_2(C^Y),\label{eq:M_circ_gamma}
\end{align}

and the corresponding complexity is $\mathcal{O}(n^2+m^2)$ (see e.g. \cite{peyre2016gromov,chapel2020partial,bai2024efficient} for details.)


Therefore, we have the following: 
\begin{lemma}
Suppose $M=[L(d_X(x_i,x_{i'}),d_Y(y_j,y_{j'}))]$, then we have: 
$$(M^\top)_{i,j,i',j'}=f_1((C^X)^\top_{i,i'})+f_2((C^Y)^\top_{j,j'})-h_1((C^X)_{i,i'})h_2((C^Y)_{j,j'}).$$
It directly implies: 
$$M^\top\circ \gamma=
f_1((C^X)^\top)\gamma_11_m^\top+1_n\gamma_2^\top f_2((C^Y)^\top)-h_1((C^X)^\top)\gamma h_2((C^Y)^\top).$$
\end{lemma}
\begin{proof}
We have: 
\begin{align}
M_{i,j,i',j'}^\top&=M_{i',j',i,j}\nonumber\\
&=f_1(C^X_{i',i})+f_2(C^Y_{j',j})-h_1(C^X_{i',i})h_2(C^Y_{j',j})
)\nonumber\\
&=f_1((C^X)^\top_{i',i})+f_2((C^Y)^\top_{j',j})-h_1((C^X)^\top_{i',i})h_2((C^Y)^\top_{j',j}).
)\nonumber 
\end{align}
And we complete the proof. 
\end{proof}






\subsection{Gradient in fused-MPGW.}
As we discussed in previous section, after iteration $k-1$, the gradient is given by $\nabla \mathcal{L}(\gamma)=\omega_1C+\omega_2 (M\circ \gamma+M^\top\circ\gamma)$, where 
$M_{i,j,i',j'}=(L(C^X_{i,i'},C^Y_{j,j'}))\in \mathbb{R}_+^{n\times m\times n\times m}$. Furthermore, the computational complexity for $M\circ \gamma:=[\langle M[i,j,:,:],\gamma \rangle ]$ can be improved to be $\mathcal{O}(n^2+m^2)$.  


Based on the gradient, we aim to solve the following to update the transportation plan in iteration $k$: 
\begin{align}
\gamma^{(k)}{'}=\arg\min_{\gamma\in\Gamma_\leq^\rho(\mu,\nu)}\langle \underbrace{\omega_1C+\omega_2 (M\circ \gamma^{(k-1)}+M^\top \gamma^{(k-1)})}_{\mathcal{C}},\gamma\rangle.  
\end{align}
The above problem is essentially the partial OT problem, where the cost function is defined by $\mathcal{C}$. 

\subsection{Gradient of Fused-PGW.}
Similarly, after iteration $k-1$, the gradient of cost $\mathcal{L}_{C,M-2\lambda}$ in FPGW with respect to $\gamma$ is given by 
$$\nabla \mathcal{L}(\gamma)=\omega_1 C+\omega_2((M-2\lambda )\circ \gamma^{(k-1)}+(M^\top-2\lambda )\circ \gamma^{(k-1)}).$$

We aim to solve the following problem: 
\begin{align}
&\min_{\gamma\in\Gamma_\leq(\mu,\nu)}\langle \omega_1 C+\omega_2((M-2\lambda )\circ \gamma^{(k-1)}+(M^\top-2\lambda)\circ\gamma^{(k-1)}),\gamma\rangle \nonumber\\
&=\min_{\gamma\in\Gamma_\leq(\mu,\nu)}\langle \underbrace{\omega_1 C+\omega_2 (M\circ\gamma^{(k-1)}+M^\top\circ\gamma^{(k-1)})}_{\mathcal{C}},\gamma \rangle +\lambda |\gamma^{(k-1)}|(|\mu|+|\nu|-2|\gamma|)-\underbrace{\lambda |\gamma^{(k-1)}|(|\mu|+|\nu|)}_{\text{constant}} \nonumber. 
\end{align}

Note, if we ignore the constant term, the above problem is the partial OT problem and can be solved by \cite{bonneel2011displacement} or \cite{bai2022sliced}. 





\section{Line search algorithm}
The line search for Fused-MPGW is defined as 
\begin{align}
\min_{\alpha\in[0,1]}\mathcal{L}((1-\alpha)\gamma^{(k-1)}+\alpha\gamma^{(k)}{'}) \label{eq:line_search_prob_fpgw},
\end{align}
where $\mathcal{L}(\gamma)=\omega_1\langle C,\gamma \rangle+\omega_2\langle M,\gamma^{\otimes2} \rangle$. 
Let $\delta\gamma=\gamma^{(k)}{'}-\gamma^{(k-1)}$. The above problem is essentially a quadratic problem with respect to $\alpha$: 
\begin{align}
&\mathcal{L}((1-\alpha)\gamma^{(k-1)}+\alpha\gamma^{(k)}{'})\nonumber\\
&=\mathcal{L}(\gamma^{(k-1)}+\alpha\delta\gamma)\nonumber\\
&=\omega_1 \langle C,(\gamma^{(k-1)}+\alpha\delta\gamma) \rangle+\omega_2\langle M\circ (\gamma^{(k-1)}+\alpha\delta\gamma),(\gamma^{(k-1)}+\alpha\delta\gamma) \rangle \nonumber\\
&=\alpha^2\underbrace{\omega_2\langle M\circ \delta\gamma,\delta\gamma\rangle}_{a}+\alpha \underbrace{\langle \omega_2M\circ \gamma^{(k-1)}+\omega_1C,\delta\gamma\rangle+\langle \omega_2M\circ \delta\gamma,\gamma^{(k-1)}\rangle}_{b}+\underbrace{\langle \omega_2 M\circ\gamma^{(k-1)}+\omega_1C,\gamma^{(k-1)}\rangle}_c, \label{eq:line_search_2}
\end{align}
and $\alpha^*$ is given by 
\begin{align}
\alpha^*=\begin{cases}
    1 &\text{if } a\leq 0, a+b\leq 0, \\
    0 &\text{if }a\leq 0, a+b>0\\
    \text{clip}(\frac{-b}{2a},[0,1]), &\text{if }a>0 
\end{cases}\label{eq:line_search_sol_1_2}. 
\end{align}
Next, we simplify the term $b$ in the above formula. We first introduce the following lemma: 
\begin{lemma}
Choose $\gamma^1,\gamma^2\in\mathbb{R}^{n\times m},M\in\mathbb{R}^{n\times m\times n\times m}$, we have: 
\begin{align}
\langle M\circ\gamma^1,\gamma^2 \rangle =\langle M^\top\circ\gamma^2,\gamma^1\rangle.
\end{align}
\end{lemma}
\begin{proof}
We have 
\begin{align}
\langle M\circ\gamma^1,\gamma^2 \rangle&=\sum_{i,j}\sum_{i',j'}M_{i,j,i',j'}\gamma^1_{i',j'}\gamma^2_{i,j}\nonumber\\
&=\sum_{i',j'}\sum_{i,j}M_{i',j',i,j}^\top\gamma^2_{i,j}\gamma^{1}_{i,j}\nonumber\\
&=\langle M^\top\circ\gamma^2,\gamma^1 \rangle. 
\end{align} 
\end{proof}
Therefore, the term $b$ can be further simplified as 
\begin{align}
b&=\omega_1 \langle C,\delta\gamma \rangle+\omega_2 (\langle M\circ\gamma^{(k-1)},\delta\gamma\rangle+\langle M^\top\circ\gamma^{(k-1)},\delta\gamma\rangle) \nonumber\\
&=\langle\underbrace{\omega_1 C+M\circ\gamma^{(k-1)}+M^\top \circ\gamma^{(k-1)}}_{\nabla L_{C,M}(\gamma)},\delta\gamma \rangle. 
\end{align}
That is, we can directly adapt the gradient obtained before to compute $b$ and improve the computation efficiency. 




% \begin{remark}
% Numerically, the computation of the above line-search algorithm can be further simplified. In particular, $$\nabla_\gamma(\mathcal{L})=2\omega_2M\circ \gamma^{(k-1)}+\omega_1C=b,$$
% and has been computed in the gradient computation step. Thus, it can be applied directly.     
% \end{remark}
\begin{remark}
When we select quadratic cost, i.e.,  $L(\mathrm{r}_1,\mathrm{r}_2):=|\mathrm{r}_1-\mathrm{r}_2|^2$, we can set $f_1(\mathrm{r}_1)=\mathrm{r}_1^2,f_2(\mathrm{r}_2)=\mathrm{r}_2^2, h_1(\mathrm{r}_1)=2\mathrm{r}_1,h_2(\mathrm{r}_2)=\mathrm{r}_2$, then 
$L(\mathrm{r}_1,\mathrm{r}_2)$ satisfies \eqref{eq:cond_L}. 
Furthermore, suppose the problem is in the balanced fused-GW setting, i.e., $\rho=|\mu|=|\nu|=1$.  We have: 
\begin{align}
a&=\omega_2\langle M\circ \delta\gamma,\delta\gamma \rangle   \nonumber\\
&=\omega_2 \langle f_1(C^X)\underbrace{\delta\gamma_1}_{0_n} 1_m^\top+\underbrace{1_n\delta\gamma_2^\top}_{0_m} f_2(C^Y)-h_1(C^X)\gamma h_2(C^Y),\delta\gamma\rangle\\
&=-2\omega_2\langle  C^X\delta \gamma C^Y,\delta\gamma \rangle. \nonumber 
\end{align}
Similarly, 
\begin{align}
b&=\langle \omega_2 (M+M^\top)\circ\gamma^{(k-1)}+\omega_1 C,\delta \gamma \rangle\nonumber\\
&=\omega_1 \langle C,\delta\gamma \rangle+\omega_2\langle (M+M^\top)\circ\gamma^{(k-1)},\delta\gamma\rangle \nonumber\\
&=\omega_1 \langle C,\delta\gamma \rangle+\omega_2 \langle (M+M^\top)\circ \delta\gamma,\gamma^{(k-1)} \rangle\nonumber\\
&=\omega_1 \langle C,\delta\gamma \rangle -2\omega_2 \langle C^X\delta \gamma C^Y,\gamma^{(k-1)}\rangle-2\omega_2 \langle (C^X)^\top\delta \gamma (C^Y)^\top,\gamma^{(k-1)}\rangle+\omega_2\langle c_{C^X,C^Y},\gamma^{(k-1)}\rangle+\omega_2\langle c_{(C^X)^\top,(C^Y)^\top},\gamma^{(k-1)}\rangle.   
\end{align}
where $c_{C^X,C^Y}=f_1(C^X)\mathrm{p}1_{m}^\top+1_{m}\mathrm{q}^\top f_2(C^Y)$. Thus, the above formulations recover the line search algorithm (see algorithm 2) in \cite{peyre2016gromov}. 
\end{remark}


Next, we discuss the line search step in fused-PGW \eqref{eq:fgw}. Replacing $M$ by $M-2\lambda$ in \eqref{eq:line_search_2}, the solution is obtained by \eqref{eq:line_search_sol_1}. 



\section{Convergence Analysis}\label{sec: convergence}

As in \cite{chapel2020partial}, we will use the results from \cite{lacoste2016convergence} on the convergence of the Frank-Wolfe algorithm for non-convex objective functions.

\subsection{Fused-MPGW}

Consider the minimization problems
\begin{equation}\label{eq: min prob appendix}
\min_{\gamma\in\Gamma^\rho_{\leq}(\mathrm{p},\mathrm{q})}\mathcal{L}_{C,M}(\gamma):=\omega_1\langle C,\gamma \rangle+ \omega_2\langle M\circ\gamma,\gamma \rangle  \nonumber 
\end{equation}
that corresponds to the discrete fused-PGW problem \eqref{eq:fpgw}. 

The objective functions 
$$\gamma\mapsto \mathcal{L}_{\hat M}(\gamma)=\omega_1 \langle C,\gamma \rangle+\omega_2 \langle M\circ\gamma,\gamma \rangle, $$ is non-convex in general. 
However, the constraint set $\Gamma_\leq^\rho(p,q)$ are convex and compact on $\mathbb{R}^{n\times m}$ (see Proposition  \ref{pro:compact}.) 

Consider the \textbf{Frank-Wolfe gap} of $\mathcal{L}_{C,M}$ at the approximation $\gamma^{(k)}$ of the optimal plan $\gamma$:
\begin{equation}\label{eq:g_k}
    g_k=\max_{\gamma\in\Gamma_{\leq}({\mathrm{p}}, {\mathrm{q}})} \langle \nabla\mathcal{L}_{\tilde M}(\gamma^{(k)}),\gamma^{(k)}-\gamma\rangle_F.
\end{equation}
It provided a good criterion to measure the distance to a stationary point at iteration $k$. Indeed, a plan $ \gamma^{(k)}$ is a stationary transportation plan for the corresponding constrained optimization problem in \eqref{eq: min prob appendix} if and only if $g_k=0$. Moreover, $g_k$ is always non-negative ($g_k\geq 0$). 

From Theorem 1 in \cite{lacoste2016convergence}, after $K$ iterations, we have the following upper bound for the minimal Frank-Wolf gap:
\begin{equation}
g_K:=\min_{1\leq k\leq K}g_k\leq \frac{\max\{2L_1,\mathrm{Lip} \cdot  (\text{diam}(\Gamma_{\leq}^\rho({\mathrm{p}}, {\mathrm{q}})))^2\}}{\sqrt{K}}, \label{eq:g_k_bound}
\end{equation}
where $$L_1:=\mathcal{L}_{\tilde M}(\gamma^{(1)})-\min_{\gamma\in\Gamma_{\leq}({\mathrm{p}}, {\mathrm{q}})}\mathcal{L}_{\tilde M}( \gamma)$$ is the initial global sub-optimal bound for the initialization $ \gamma^{(1)}$ of the algorithm; $\text{Lip}$ is the Lipschitz constant of function 
$\gamma\mapsto\nabla\mathcal{L}_{\tilde M}$; and 
$$\text{diam}(\Gamma_{\leq}({\mathrm{p}}, {\mathrm{q}}))=\sup_{\gamma,\gamma'\in\Gamma_\leq(\mu,\nu)}\|\gamma-\gamma'\|_F$$ is the $\|\cdot\|_F$ diameter of $\Gamma_{\leq}({\mathrm{p}}, {\mathrm{q}})$ in $\mathbb{R}^{n\times m}$. 

The important thing to notice is that the constant $\max\{2L_1, D_L\}$ does not depend on the iteration step $k$. Thus, according to Theorem 1 in \cite{lacoste2016convergence}, the rate in $\tilde g_K$ is $\mathcal{O}(1/\sqrt{K})$. That is, the algorithm takes at most $\mathcal{O}(1/\varepsilon^2)$ iterations to find an approximate stationary point with a gap smaller than $\varepsilon$.

Next, we will continue to simplify the upper bound \eqref{eq:g_k_bound}. We first introduce the following fundamental results: 



\begin{lemma}\label{lem:diam_Gamma_bound}
In the discrete setting, we have 
\begin{align}
{\rm diam}(\Gamma_\leq(p,q))\leq 2\rho. \label{eq:diam_Gamma_bound}
\end{align}
\end{lemma}
\begin{proof}
Choose $\gamma,\gamma'\in \Gamma_\leq^\rho(p,q)$, 
we apply the property
\begin{align}
(a-b)^2\leq 2a^2+2b^2,\quad \forall a,b\in \mathbb{R}. \label{eq:ineq_ab}
\end{align} and obtain 
\begin{align}
\|\gamma-\gamma'\|_{F}^2&=\sum_{i,j}^{n,m}|\gamma_{i,j}-\gamma'_{i,j}|^2\nonumber\\
&\leq\sum_{i,j}^{n,m}2|\gamma_{i,j}|^2+2|\gamma'_{i,j}|^2\nonumber\\
&\leq 2\left[\left(\sum_{i,j}^{n,m}\gamma_{i,j}\right)^2+\left(\sum_{i,j}^{n,m}\gamma'_{i,j}\right)^2\right]\nonumber\\
&= 2(|\gamma|^2+|\gamma'|^2)\\
&=2 (\rho^2+\rho^2)=4\rho^2,\nonumber 
\end{align}
and thus, we complete the proof. 
\end{proof}

\begin{lemma}\label{lem:lip_bound}
The Lipschitz constant term in \eqref{eq:g_k_bound} can be bounded as follows:
\begin{align}
\text{Lip}\leq \omega_2nm \max(|M|)^2.\label{eq:lip_bound}
\end{align}
In particular, when $L(\mathrm{r}_1,\mathrm{r}_2)=|\mathrm{r}_1-\mathrm{r}_2|^p$ where $p\ge 1$, we have 
$\text{Lip}\leq nm (2^{p-1}C^X+2^{p-1}C^Y)^2$. 
\end{lemma}
\begin{proof}
Pick $\gamma,\gamma'\in\Gamma_{\leq}({\mathrm{p}}, {\mathrm{q}})$ we have,
\begin{align*}
&\|\nabla\mathcal{L}_{ M}(\gamma)-\nabla\mathcal{L}_{C,M}(\gamma')\|_F^2\nonumber\\
&=\|(\omega_1 C+\omega_2 (M\circ \gamma+M^\top\circ\gamma)) - (\omega_1 C+\omega_2  (M\circ\gamma'+M^\top\circ\gamma'))\|_F^2\\
&=2\omega_2^2\|M\circ(\gamma-\gamma')\|_F^2+2\omega_2^2\|M^\top\circ(\gamma-\gamma')\|_F^2\nonumber\\
&=2\omega_2^2\sum_{i,j}\left(\left[M\circ(\gamma-\gamma')\right]_{i,j}\right)^2+2\omega_2^2\sum_{i,j}\left(\left[M^\top\circ(\gamma-\gamma')\right]_{i,j}\right)^2\nonumber\\
&=2\omega_2^2\sum_{i,j}\left(\sum_{i',j'}M_{i,j,i',j'}(\gamma_{i',j'}-\gamma'_{i',j'})\right)^2+2\omega_2^2\sum_{i,j}\left(\sum_{i',j'}M^\top_{i,j,i',j'}(\gamma_{i',j'}-\gamma'_{i',j'})\right)^2\\
&\leq 2\omega_2^2\sum_{i,j}\left(\sum_{i',j'}|M_{i,j,i',j'}||\gamma_{i',j'}-\gamma'_{i',j'}|\right)^2+
2\omega_2^2\sum_{i,j}\left(\sum_{i',j'}|M^\top_{i,j,i',j'}||\gamma_{i',j'}-\gamma'_{i',j'}|\right)^2\nonumber \\
&\leq4\omega_2^2\underbrace{\max(M)^2}_{A}\cdot
\underbrace{\sum_{i,j}^{n,m}\left(\sum_{i',j'}^{n,m}|\gamma_{i',j'}-\gamma'_{i',j'}|\right)^2}_B
\end{align*}


For the second term, we have: 
\begin{align}
&\sum_{i,j}^{n,m}\left(\sum_{i',j'}^{n,m}|\gamma_{i',j'}-\gamma'_{i',j'}|\right)^2 \nonumber \\
&\leq \sum_{i,j}^{n,m}\left(nm\sum_{i',j'}^{n,m}\left|\gamma_{i',j'}-\gamma'_{i',j'}\right|^2\right)\nonumber\nonumber\\
&\leq n^2m^2 \|\gamma-\gamma'\|^2_F \label{pf:convergence_bound_B}
\end{align}


For the first term, when $L(\mathrm{r}_1,\mathrm{r}_2)=|\mathrm{r}_1-\mathrm{r}_2|^p$, from the inequality \eqref{eq:ab_inq}, we have: 
\begin{align*}
A&=\max M\leq \max\{2^{p-1}(C^X)^p+2^{p-1}(C^Y)^p\}^2.
\end{align*}
where 
$$\max\{2^{p-1}(C^X)^p+2^{p-1}(C^Y)^{p}\}:=\max\{\max_{i,i',j',j'}2^{p-1}(C^X_{i,i'})^{p}+2^{p-1}(C^Y_{j,j'})^{p}\}.$$

Thus we obtain
\begin{align}
\text{Lip}\leq \frac{\max(2^{p-1}(C^X)^p+2^{p-1}(C^Y)^p)nm\|\gamma-\gamma'\|_F}{\|\gamma-\gamma'\|_F}=\omega_2nm\max(2^{p-1}(C^X)^p+2^{p-1}(C^Y)^p). \nonumber 
\end{align}
and we complete the proof. 
\end{proof}

Combined the above two lemmas, we derive the convergence rate of the Frank-Wolfe gap \eqref{eq:g_k}: 
\begin{proposition}\label{pro:convergence_fmpgw}
When $L(\mathrm{r}_1,\mathrm{r}_2)=|\mathrm{r}_1-\mathrm{r}_2|^2$ in the PGW problem, the Frank-Wolfe gap of algorithm \ref{alg:fmpgw}, defined in \eqref{eq:g_k} at iteration $k$ satisfies the following: 
\begin{align}
g_k \leq \frac{\max\Bigg\{2L_1,4\omega_2\rho^2\cdot nm(\max\{2(C^X)^2+2(C^Y)^2\})\Bigg\}}{\sqrt{k}}.\label{eq:g_k_bound_pgw}
\end{align}
\end{proposition}
\begin{proof}
The proof directly follows from the upper bounds \eqref{eq:diam_Gamma_bound},\eqref{eq:lip_bound} and the inequality \eqref{eq:g_k_bound}.
\end{proof}
\begin{remark}
Note, if the cost function in PGW is defined by $|\mathrm{r}_1-\mathrm{r}_2|^p$ for some $p\neq 2$, it is straightforward to verify that the upper bound of $g_k$ is obtained by replacing the term $\max ((C^X)^2+(C^Y)^2)$ should be replaced by 
$$\max_{i,j,i',j'}\left[2^{p-1}((C^X)^p+(C^Y)^p)\right].$$
\end{remark}
\begin{remark}
From the proposition \eqref{pro:convergence_fmpgw}, to achieve an $\epsilon-$accurate solution, the required number of iterations is 
\begin{align}
\frac{\max\Bigg\{2L_1,4\rho\omega_2\cdot n^2m^2\max(\{2(C^X)^2+2(C^Y)^2\})\Bigg\}^2}{\epsilon^2}.\nonumber
\end{align}
\end{remark}
\begin{remark}
Note, when $\omega_2=1$, this convergence rate is constant to the convergence rate of the FW algorithm for MPGW in \cite{chapel2020partial}. In addition, the convergence rate is independent of $C$. The main reason is that the part $\omega_1\langle C,\gamma \rangle$ is linear with respect to $\gamma$. Thus, this part does not contribute to the Lipschitz of the mapping $\gamma\mapsto \nabla_M(\mathcal{L})$. 
\end{remark}


\subsection{Convergence of Fused-PGW}
Similar to the above, in this subsection, we discuss the convergence rate for algorithm 2. 
Suppose $\gamma$ is a solution for the fused-PGW problem \eqref{eq:fpgw}. 
In iteration $k$, we define the gap between $\gamma$ and the approximation $\gamma^{(k)}$ and the Frank-Wolfe gap: 
\begin{align}
&g_k:=\max_{\gamma\in\Gamma_\leq(p,q)}\langle \nabla\mathcal{L}_{C,M-2\lambda}(\gamma^{(k)}),\gamma^{(k)}-\gamma \rangle_F \label{eq:g_k_fpgw}. \\
&g_K:=\min_{1\leq k\leq K}g_k \label{eq:g_K_fpgw}. 
\end{align}


Thus, the convergence rate for the FW algorithm for the fused-PGW problem \eqref{eq:fpgw} can be bounded by the following proposition: 
\begin{proposition}\label{pro:convergence_fpgw}
When $L(\mathrm{r}_1,\mathrm{r}_2)=|\mathrm{r}_1-\mathrm{r}_2|^2$ in the Fused-PGW problem, the Frank-Wolfe gap of algorithm \ref{alg:fpgw},
\eqref{eq:g_k} at iteration $k$ satisfies the following: 
\begin{align}
g_K \leq \frac{\max\Bigg\{2L_1,4\omega_2\min^2(|p|,|q|)\cdot nm(\max\{2(C^X)^2+2(C^Y)^2,2\lambda\})\Bigg\}}{\sqrt{k}},\label{eq:g_k_bound_pgw_1}
\end{align}
\end{proposition}
where $\max(2(C^X)^2+2(C^Y)^2,2\lambda)=\max\{\max_{i,i',j,j'} (2(C^X_{i,i'})^2+2(C^Y_{j,j'})^2,2\lambda\}$. 

From Theorem 1 in \cite{lacoste2016convergence}, we have: 
\begin{align}
g_K\leq \frac{\max\{2L_1, \text{Lip}\cdot(\text{diam}(\Gamma_\leq(p,q)))\}}{\sqrt{K}},\label{eq:converge_bound_fpgw}
\end{align}
where $\text{Lip}$ is the Lipschitz constant of function $\gamma\to \nabla_M \mathcal{L}$ (with respect to Fubini norm). 

By \cite{bai2024efficient},
\begin{align}
 \text{diam}(\Gamma_\leq(p,q))\leq 2\min(|p|,|q|).\label{eq:diam_2}   
\end{align}
Next, we bound the Lipschitz constant.
\begin{lemma}\label{lem:lip_bound_fpgw}
The Lipschitz constant in \eqref{eq:converge_bound_fpgw} can be bounded as follows: 
\begin{align}
\text{\rm Lip}\leq \omega_2nm\max |M-2\lambda|^2\label{eq:lip_fpgw}.
\end{align}
When $L(\mathrm{r}_1,\mathrm{r}_2)=|\mathrm{r}_1-\mathrm{r}_2|^p$, we have: 
\begin{align}
 \text{\rm Lip}\leq \omega_2 nm (\max(2^{p-1}(C^X)^2+2^{p-1}(C^Y)^p,2\lambda))^2.\label{eq:lip_fpgw_2}   
\end{align}
\end{lemma}
\begin{proof}
    

We have: 

\begin{align*}
&\|\nabla\mathcal{L}_{C,M-2\lambda}(\gamma)-\nabla\mathcal{L}_{C,M-2\lambda}(\gamma')\|_F^2\nonumber\\
&=\|(\omega_1 C+\omega_2 ((M-2\lambda)\circ \gamma+(M^\top-2\lambda)\circ\gamma)) - (\omega_1 C+\omega_2  ((M-2\lambda)\circ\gamma'+(M^\top-2\lambda)\circ\gamma'))\|_F^2\\
&=2\omega_2^2\|(M-2\lambda)\circ(\gamma-\gamma')\|_F^2+2\omega_2^2\|(M^\top-2\lambda)\circ(\gamma-\gamma')\|_F^2\nonumber\\
&=2\omega_2^2\sum_{i,j}\left(\left[(M-2\lambda)\circ(\gamma-\gamma')\right]_{i,j}\right)^2+2\omega_2^2\sum_{i,j}\left(\left[(M^\top-2\lambda)\circ(\gamma-\gamma')\right]_{i,j}\right)^2\nonumber\\
&=2\omega_2^2\sum_{i,j}\left(\sum_{i',j'}(M_{i,j,i',j'}-2\lambda)(\gamma_{i',j'}-\gamma'_{i',j'})\right)^2+2\omega_2^2\sum_{i,j}\left(\sum_{i',j'}(M^\top_{i,j,i',j'}-2\lambda)(\gamma_{i',j'}-\gamma'_{i',j'})\right)^2\\
&\leq 2\omega_2^2\sum_{i,j}\left(\sum_{i',j'}|M_{i,j,i',j'}-2\lambda||\gamma_{i',j'}-\gamma'_{i',j'}|\right)^2+
2\omega_2^2\sum_{i,j}\left(\sum_{i',j'}(|M^\top_{i,j,i',j'}-2\lambda||\gamma_{i',j'}-\gamma'_{i',j'}|\right)^2\nonumber \\
&\leq4\omega_2^2\underbrace{\max(|M-2\lambda|)^2}_{A}\cdot
\underbrace{\sum_{i,j}^{n,m}\left(\sum_{i',j'}^{n,m}|\gamma_{i',j'}-\gamma'_{i',j'}|\right)^2}_B.
\end{align*}

Term $B$ can be bounded by \eqref{pf:convergence_bound_B}. Thus, we obtain the upper bound \eqref{eq:lip_fpgw}. Furthermore, when $L(\mathrm{r}_1,\mathrm{r}_2)=|\mathrm{r}_1-\mathrm{r}_2|^p$, from inequality \eqref{eq:ab_inq}, we have: 
\begin{align}
\max (|M-2\lambda|)\leq \max(\max_{i,i'}2^{p-1}(C^X)^p_{i,i'}+\max_{j,j'}2^{p-1}(C^Y)_{j,j'}^p,2\lambda):=\max(2^{p-1}(C^X)^{p-1},2^{p-1}(C^Y)^p,2\lambda)\nonumber 
\end{align}
and we obtain the bound \eqref{eq:lip_fpgw_2}. 
\end{proof}

\begin{proof}[Proof of Proposition \ref{pro:convergence_fpgw}.]
Combining Lemma \ref{lem:lip_bound}, \eqref{eq:diam_2} and \eqref{eq:converge_bound_fpgw}, we obtain the upper bound \eqref{eq:g_k_bound_pgw} and complete the proof. 
\end{proof}


\section{Fused Partial Gromov Wasserstein Barycenter}\label{sec:barycenter}
In discrete setting, suppose we have $K$ mm-spaces $\mathbb{X}^k=(X^k\subset\mathbb{R}^d,d_{X^k},\mu^k:=\sum_{i=1}^{n^k}p^k_i\delta_{x_i^k})$ for $k=1,\ldots, K$ and a fixed pmf function $p\in \mathbb{R}_+^n$ where $n\in\mathbb{N}$. Note for each $k\in [1:K]$, let $C^k=[d^r_{X^k}(x^k_i,x^k_{i'})]_{i,i'\in[1:n^k]}\in\mathbb{R}^{n^k\times n^k}$ and let $X^k=[x^k_1,\ldots x^k_{n^k}]^\top\in\mathbb{R}^{n_k\times d}$, we have $(X^k, \mathrm{p}^k, C^k)$ can represent the space $\mathbb{X}^k$. Thus, for convenience, we use the convention $\mathbb{X}^k=(X^k,p^k,C^k)$ to denote the corresponsded mm-space. 

Then, given pmf function $\mathrm{p}\in\mathbb{R}_+^n$ where $n\in\mathbb{N}$, and $\beta_1,\ldots \beta_K\ge 0$ with $\sum_{k=1}^K\beta_k=1$,  the fused-Gromov Wasserstein barycenter problem \cite{titouan2019optimal} is defined as: 

\begin{align}
\min_{C\in\mathbb{R}^{n\times n},X\in\mathbb{R}^{n\times d}}\beta_i FGW_{L}(\mathbb{X},\mathbb{X}^k), &\text{ where }\mathbb{X}=(X,\mathrm{p},C), \label{eq:fused-PGW barycenter}
\end{align}
where we adapt notation $FGW_L(\cdot,\cdot)$ to simplify the notation  $FGW_{r,L}(\cdot,\cdot)$ since $r$ has been incorporated into matrix $C^k$. 


Inspired by this work, we present the following fused Partial GW barycenter problems.

\subsection{Fused-MPGW barycenter.}

Choose values   $\rho_1,\rho_2,\ldots,\rho_K$ where $\rho_k\in [0,\min(|p|,|p^k|)]$ for each $k$. In addition, choose $(\omega_1^k,\omega_2^k)\in[0,1]$ for $k=1,2,\ldots K$ such that $\omega_1^k+\omega_2^k=1,\forall k$. 
The fused mass-constrained Partial GW barycenter problem is defined as: 
\begin{align}
&\min_{C\in\mathbb{R}^{n\times n},X\in\mathbb{R}^{n\times d}}\sum_{k=1}^K\beta_k FMPGW_{L,\rho^k}(\mathbb{X},\mathbb{X}^k),  \nonumber\\
&=\min_{\substack{C\in\mathbb{R}^{n\times n}\\
X\in\mathbb{R}^{n\times d}}}\min_{\substack{\gamma^k\in\Gamma^{\rho_k}_\leq(p,p^k)\\
k\in[1:K]}}\sum_{k=1}^K\beta_k \left(\omega_1^k \langle D(X,X^k),\gamma^k\rangle+\omega_2^k\langle L(C,C^k) (\gamma^k)^{\otimes2}\rangle\right),\label{eq:fused-mpgw barycenter} 
\end{align}
where $\mathbb{X}=(X,p,C)$, $\mathbb{X}^k=(X^k,p^k,C^k)$, and  $D(X,X^K)=[\|x_i-x^k_j\|^2]_{i\in[1:n],j\in[1:n^k]}\in\mathbb{R}^{n\times n_k}$. 

Note that the above problem is convex with respect to $(C, X)$ when $\gamma^k$ is fixed for each $k$. However, it is not convex with respect to $\gamma^k$ for each $k$. Similar to classical fused-GW, it can be solved iteratively by updating $(C, X)$ and $(\gamma^k)_{k=1}^K$ alternatively in each iteration. 

\textbf{Step 1.} Given $(C,X)$, we update $(\gamma^k)_{k=1}^K$. 
Note, when $(C,X)$ is fixed,  each optimal $(\gamma^k)^*$ is given by 
\begin{align}
(\gamma^k)^*=\arg\min_{\gamma^k\in\Gamma_\leq^{\rho_k}(p,p^k)}\omega_1^k \langle D(X,X^k),\gamma^k\rangle +\omega_2^k\langle L(C,C^k),(\gamma^k)^{\otimes2}\rangle, \nonumber  
\end{align}
which is a solution for the fused partial GW problem $F-MPGW_{\rho^k}(\mathbb{X},\mathbb{X}^k)$. 

\textbf{Step 2.} Given $\gamma^k$, update $(C,X)$. 

Suppose $\gamma^k$ is given for each $k$, the objective function in \eqref{eq:fused-PGW barycenter} becomes: 
\begin{align}
&\min_{C\in\mathbb{R}^{n\times n},X\in\mathbb{R}^{n\times d}}\sum_{k=1}^K\beta_k \left(\omega_1 \langle D(X,X^k),\gamma^k\rangle+\omega_2\langle L(C,C^k), (\gamma^k)^{\otimes2}\rangle\right)\nonumber\\
&=\omega_1\underbrace{\min_{X\in \mathbb{R}^{n\times d}}\sum_{k=1}^K\beta_k\langle D(X,X^k),\gamma^k \rangle}_{A}+\omega_2 \underbrace{\min_{C\in\mathbb{R}^{n\times n}}\sum_{k=1}^K\beta_k\langle L(C,C^k),(\gamma^k)^{\otimes2} \rangle}_{B} \nonumber. 
\end{align}

Problem $B$ admits the solution (we refer \cite{bai2024efficient} Section M for details). In particular, if $L$ satisfies \eqref{eq:cond_L}, $f_1,h_1$ are differentiable, then
\begin{align}
  C=\left(\frac{f_1'}{h_1'}\right)^{-1}\left(\frac{\sum_k\xi_k\gamma^kh_2(C^k)(\gamma^k)^\top}{\sum_k\xi_k\gamma_1^k(\gamma_1^k)^\top}\right).\label{eq:optimal_C}  
\end{align}


In particular, when $L(\mathrm{r}_1,\mathrm{r}_2)=|\mathrm{r}_1-\mathrm{r}_2|^2$, the above formula becomes:
\begin{align}
C=\frac{\sum_k\xi_k\gamma^kC^k(\gamma^k)^\top}{\sum_k\xi_k\gamma_1^k(\gamma_1^k)^\top} \nonumber. 
\end{align}

\subsection{Solving the subproblem A.}
We first introduce the barycentric projection: 

For each $(X^k,p^k,\gamma^k)$, the barycentric projection \cite{bai2023linear} is defined by 
\begin{align}
\hat x^k_i=\begin{cases}
\frac{1}{\gamma^k_1[i]}\sum_{j=1}^{n_k}\gamma^k_{i,j}x^k_{j} &\text{if }\gamma_1^k[i]=\sum_{j=1}^{n_k}\gamma_{i,j}^k>0, \\
x_i &\text{elsewhere}.
\end{cases}\label{eq:barycentric_proj} 
\end{align}

Note, if $|\gamma^k|=|p|,\forall k$, by \cite{cuturi2014fast} ( Eq 8), the  optimal $X$ is given by 
$$X=[x_1,\ldots x_n]^\top, x_i=\sum_{k=1}^K\beta_k \hat{x}_i^k,\forall i\in[1:n].$$
In this subsection, we will extend the above result to the general case. 

\begin{proposition}\label{pro:optimal_X}
Any matric $X$ satisfies the following is a solution for problem $A$. 
\begin{align}
X=[x_1,\ldots, x_n]^\top, x_i=\frac{\sum_{k=1}^K\beta_k\hat{x}_i^k}{\sum_{k=1}^K\beta_i\gamma^k_1[i]}\qquad\text{if }\sum_{k=1}^K\beta_i\gamma_1^k[i]>0\label{eq:optimal_X}.
\end{align}
\end{proposition}
Note, for each $i$, we use convention $\frac{0}{0}=0$ if $\sum_{k=1}^K\beta_i\gamma_1^k[i]=0$. 
\begin{proof}
Problem A can be written in terms of barycentric projection \eqref{eq:barycentric_proj}. In particular, let $\mathcal{D}_k:=\{i:\sum_{i,j}\gamma^k_{i,j}>0\}$ and $\mathcal{D}=\bigcup_{k=1}^K\mathcal{D}_k$. 
We have: 
\begin{align}
A&=\min_{X\in\mathbb{R}^{n\times d}}\sum_{k=1}^K\beta_k\sum_{i=1}^n\gamma_1^k[i](\|x_i-\hat x_i^k\|^2)\nonumber\\
&=\min_{X\in\mathbb{R}^{n\times d}}\sum_{i=1}^n\sum_{k=1}^K\beta_k\gamma_1^k[i](\|x_i-\hat x_i^k\|^2)\nonumber\\
&=\sum_{i=1}^n\min_{x_i\in\mathbb{R}^d}\sum_{k=1}^K\beta_k\gamma_1^k[i](\|x_i-\hat x_i^k\|^2).\nonumber
\end{align}
For each $i$, we have two cases:

Case 1:  $\sum_{k=1}^K\beta_k\gamma_1^k[i]>0$. 
Then optimal $x_i$ is given by the weighted average vector:
$$\frac{\sum_{k\in D_i}\beta_k\gamma_1^k[i]\hat{x}^k_i}{\sum_{i\in D_k}\beta_k\gamma^k_1[i]}.$$

Case 2:  $\sum_{k=1}^K\beta_k\gamma_1^k[i]=0$. The problem becomes 
$$\min_{x_i\in \mathbb{R}^d}0,$$
and there is no requirement for $x_i$. 
%Let $D_i=\{k: \beta_k>0\}$, we have $\gamma_1^k[i]=0,\forall i\in D_i$. Thus,  $\hat{x}_i^k=0_{d},\forall k\in D_i$. Thus, the optimal $x_i$ is the minimizer of the following:
%$$\min_{k\in D_i}\|x_i\|^2.$$
%Therefore, $x_i=0_d$ is the minimizer. 
And we complete the proof. 

\end{proof}
\begin{remark}
In practice, the above formulation can be described by matrices. In particular, let 
$\hat{X}^k=[\hat{x}^k_1,\ldots \hat{x}^k_n]^\top$, we have  
$$\hat{X}^k=\frac{\gamma^k X^k}{\gamma^k_1},\gamma^k_1=\gamma^k 1_m.$$
Then we have \eqref{eq:optimal_X} becomes
$$X=\frac{\sum_{k=1}^K\beta_k\gamma_1^k1_d^\top\odot\hat{X}}{\sum_{k=1}^K\beta_k\gamma_1^k},$$
where $\odot$ denotes the element-wise multiplication; the notation $\frac{A}{B}$ where $A\in \mathbb{R}^{n\times d}, B\in \mathbb{R}^{n}$ denotes the following
$$\frac{A}{B}=[A[:,1]/B_1,\ldots,A[:,n]/B_n]^\top,$$
and we use $\frac{0}{0}=0$ if any element in the denominator is 0. 


\end{remark}


\subsection{Fused-PGW barycenter}
Similar to the previous subsection, we define and derive the solution for the fused-Partial GW problem. 

Given $\lambda_1,\ldots \lambda_K\ge 0, p\in \mathbb{R}_+^n$, the fused partial GW barycenter problem is defined as: 

\begin{align}
&\min_{C\in\mathbb{R}^{n\times n},X\in\mathbb{R}^{n\times d}}\sum_{k=1}^K\beta_k FPGW_{L,\lambda_i}(\mathbb{X},\mathbb{X}^k), \text{where }\mathbb{X}=(X,p,C)\nonumber\\
&=\min_{C\in\mathbb{R}^{n\times n},X\in \mathbb{R}^{n\times d}} \min_{\gamma^k\in\Gamma_\leq(p,p^k):k\in[1:K]}\sum_{k=1}^K\omega_1^k\langle D(X,X^k),\gamma^k\rangle +\omega_2^k\langle L(C,C^k),(\gamma^k)^{\otimes2}\rangle+\lambda_k(|p^k|^2+|p|^2-2|\gamma^k|^2).
\label{eq:fpgw_barycenter}
\end{align}

Similar to the fused MPGW barycenter problem, the above problem can be solved iteratively. In each iteration, we have two steps: 

\text{Step 1.} Given $X,D$, update $\gamma^k$. 
Note finding each $\gamma^k$ is essentially solving the fused-PGW problem: 
\begin{align}
\gamma^k=\arg\min_{\gamma\in\Gamma_\leq(p,p^k)}\omega_1^k\langle D(X,X^k),\gamma\rangle+\omega_2^k \langle L(C,C^k),(\gamma^{\otimes2}) \rangle+\lambda_k(|p^k|^2+|p|^2-2|\gamma^k|^2) \nonumber   
\end{align}
And it can be solved by algorithm \ref{alg:fpgw}. 

\text{Step 2.} Given $\gamma^k:k\in[1:K]$, update $C,X$. 


In this case, \eqref{eq:fpgw_barycenter} becomes 
\begin{align}
\omega_2^k\underbrace{\min_{C\times \mathbb{R}^{n\times n}} \langle L(C,C^k)-2\lambda_k,(\gamma^k)^{\otimes2} \rangle}_{A}+ \omega_1^k\underbrace{\min_{X\in\mathbb{R}^{n\times d}}\langle D(X,X^k),\gamma^k\rangle}_{B}.
\end{align}

Optimal $C$ for subproblem A is \eqref{eq:optimal_C} by  [Proposition M.2 \cite{bai2024efficient}]. Optimal $X$ for subproblem B is given by \eqref{eq:optimal_X} by the Proposition \ref{pro:optimal_X}. 

\section{Relation between FGW, FPGW and FMPGW.}
In this section, we briefly discuss the relation between FGW, Fused-PGW problem \eqref{eq:fpgw}, and the fused-MPGW \eqref{eq:fmpgw}. 


First, we introduce the following ``equivalent relation'' between FPGW and FMPGW. It can be treated as the generalization of Proposition L.1. in \cite{bai2024efficient} in the fused-PGW formulation. 

\begin{proposition}\label{pro:fpgw_fmpgw}
Given mm-spaces $\mathbb{X}=(X,d_X,\mu),\mathbb{Y}=(Y,d_Y,\nu)$, $r\ge 1,\lambda>0$. Suppose $\gamma^*$ is a minimizer for FPGW problem $FPGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y})$, then $\gamma^*$ is also a minimizer for Fused-MPGW problem $FPGW_{r,L,\rho}(\mathbb{X},\mathbb{Y})$ where $\rho=|\gamma^*|$. 
\end{proposition}
\begin{proof}
Pick $\gamma\in\Gamma_\leq^\rho(\mu,\nu)\subset \Gamma_\leq(\mu,\nu)  $. Since $\gamma^*$ is optimal for $FPGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y})$, we have: 
\begin{align}
&\omega_1\langle C,\gamma^*\rangle +\omega_2\langle L,(\gamma^*)^{\otimes2}\rangle+\lambda(|\mu|^2+|\nu|^2-2|\gamma^*|^2)  \nonumber\\
&\leq \omega_1\langle C,\gamma\rangle +\omega_2\langle L,\gamma^{\otimes2}\rangle+\lambda(|\mu|^2+|\nu|^2-2|\gamma|^2).  \nonumber
\end{align}
Combine it with the fact $|\gamma|=|\gamma^*|=\rho$, we complete the proof. 
\end{proof}


Next, we discuss the relation between FGW and FMPGW problems. 
\begin{proposition}\label{pro:fmpgw_fgw}
Under the setting of Proposition \ref{pro:fpgw_fmpgw}, 
suppose $|\mu|=|\nu|$, then 
$$FMPGW_{r,L,\rho}(\mathbb{X},\mathbb{Y})=FGW_{r,L,\rho}(\mathbb{X},\mathbb{Y}).$$
\end{proposition}
\begin{proof}
In this case, $\Gamma_\leq^\rho(\mu,\nu)=\Gamma(\mu,\nu)$ and we complete the proof. 
\end{proof}

Similarly, in the extreme case, the FPGW problem can also recover the FGW problem. We first introduce the following lemma: 
\begin{lemma}\label{lem:big_lambda}
Suppose $\mu,\nu$ are supported in compact set and $|\mu|,|\nu|>0$. 
Let $X=\text{supp}(\mu),Y=\text{supp}(\nu)$, and
$\max L(d_X^r,d_Y^r):=\max_{x,x'\in X,y,y'\in Y}L(d_X^r(x,x'),d_Y^r(y,y')),\max C:=\max_{x\in X,y\in Y}C(x,y)$. 

Suppose 

\begin{align}
  &2\lambda\ge \omega_1\frac{1}{\min(|\mu|,|\nu|)}\max (C)+\omega_2 \max L(d_X^r,d_Y^r)\label{eq:big_lambda_cond}
\end{align}
there exists a solution, denoted as $\gamma^*$, for $FPGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y})$ such that
\begin{align}
|\gamma^*|=\min(|\mu|,|\nu|)\label{eq:big_lambda_res}.
\end{align} 
If we replace ``$\ge$'' by ``$>$'' in the inequality
$\eqref{eq:big_lambda_cond}$, then every solution of $FPGW_{r,L,\lambda}(\mathbb{X},\mathbb{T})$ satisfies $\eqref{eq:big_lambda_res}$. 
\end{lemma}

\begin{proof}
For convenience, we suppose $|\mu|\leq |\nu|$.
Suppose \eqref{eq:big_lambda_cond} holds. 
Choose an optimal $\gamma\in\Gamma_\leq(\mu,\nu)$. 
By lemma E.1. in \cite{bai2024efficient}, there exists $\gamma'\in\Gamma_\leq(\mu,\nu)$ such that $\gamma\leq \gamma'$ with $|\gamma'|=|\mu|$, i.e. $\gamma'_1=\mu$. 

We have: 
\begin{align}
& \int_{X\times Y} \omega_1C(x,y) d\gamma' +\int_{(X\times Y)^2} \omega_2L(d_X^r,d_Y^r) d\gamma'^{\otimes2}+\lambda(|\mu|^2+|\nu|^2-2|\gamma'|^2)\nonumber\\
&- \int_{X\times Y}\omega_1 C(x,y) d\gamma +\int_{(X\times Y)^2}\omega_2 L(d_X^r,d_Y^r) d\gamma^{\otimes2}+\lambda(|\mu|^2+|\nu|^2-2|\gamma|^2)\nonumber\\
&= \int_{X\times Y}\omega_1C(x,y) d(\gamma'-\gamma)+ \int_{(X\times Y)^2}\omega_2L(d_X^r,d_Y^r)-2\lambda  d(\gamma'^{\otimes 2}-\gamma^{\otimes2})\nonumber\\
&\leq \omega_1 \max (C)|\gamma'-\gamma|+\omega_2 (\max (L(d_X^r,d_Y^r))-2\lambda)|\gamma'^{\otimes2}-\gamma^{\otimes2}|\nonumber\\
&=\underbrace{\left(\omega_1 \frac{\max(C)}{|\gamma+\gamma'|}+\omega_2 \max (L(d_X^r,d_Y^r))-2\lambda\right)}_{A}\underbrace{|\gamma'^{\otimes2}-\gamma^{\otimes2}|}_B\nonumber\\
&\leq 0 \label{pf:gamma'_optimal},
\end{align}
where \eqref{pf:gamma'_optimal} follows by the fact $A\leq 0,B\ge 0$. 
Thus we have $\gamma'$ is optimal. 


Now we suppose
$$2\lambda>\omega_1\frac{1}{\min(|\mu|,|\nu|)}\max (C)+\omega_2 \max L(d_X^r,d_Y^r).$$
We assume there exists an optimal $\gamma$ such that  $|\gamma|<|\mu|$.

If we can find optimal $\gamma$ with $|\gamma|<|\mu|$, then $A<0$ and $B>0$ and we have
$\eqref{pf:gamma'_optimal}<0$. 
That is, $\gamma'$ admits a smaller cost. It is a contradiction since $\gamma$ is optimal, and we complete the proof.  
\end{proof}
\begin{remark}
If $\min(|\mu|,|\nu|)=0$, $\Gamma_\leq(\mu,\nu)=\{\mathbf{0}\}$ where $\{\mathbf{0}\}$ is the zero measure. In this case, \eqref{eq:big_lambda_res} is automatically satisfied, and there is no requirement for $\lambda$. 
\end{remark}
Based on this lemma, the FPGW can recover FGW in the extreme case: 
\begin{proposition}\label{pro:fpgw_fgw}
Under the setting of Proposition \ref{pro:fmpgw_fgw}, suppose $\lambda$ satisfies \eqref{eq:big_lambda_cond}, then 
$$FPGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y})=FGW_{r,L}(\mathbb{X},\mathbb{Y}).$$
\end{proposition}
\begin{proof}
By Lemma \ref{lem:big_lambda}, there exists optimal $\gamma$ for $FPGW_{r,L,\lambda}(\mathbb{X},\mathbb{Y})$ with $|\gamma|=|\mu|=|\nu|$. By Proposition \ref{pro:fpgw_fmpgw}, we have $\gamma$ is optimal for $FMPGW_{r,L,\rho}(\mathbb{X},\mathbb{Y})$ where $\rho=|\mu|=|\nu|$. 
By Proposition \ref{pro:fmpgw_fgw}, we have $\gamma$ is optimal for $FGW_{r,L}(\mathbb{X},\mathbb{Y})$. Thus, $\gamma$ and we complete the proof. 
\end{proof}
\section{Numerical details in graph classification}\label{sec:graph_classification_2}



\textbf{Parameter and Numerical Settings.}  

The parameter settings are provided in Table \eqref{tab:parameter}. For methods not explicitly listed, we use the default values from the \href{https://ysig.github.io/GraKeL/0.1a8/classes.html}{GraKeL library}. 

For the FUGW method, we adapt the solver from \cite{flamary2021pot}. We test different marginal penalty parameters $\rho$ to ensure that the transported mass in each sampled pair is approximately 1. Additionally, we choose the smallest entropy regularization term $\epsilon$ that prevents NaN errors.
 





\begin{table*}[h!]
\caption{Parameter setting of all methods}
\label{tab:parameter}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccccccc} % 7 columns total: 1 for Data set + 6 for values
\toprule
Data set &parameter&{Synthetic}&{Proteins}& {Mutag} &MSRC-9\\
\midrule 

SVM (Common) & $\sigma$ & 1 & 15 & 2 & 1\\
\hline
\multirow{2}{*}{FGW} &$\alpha$ & $0.5$ &$0.5$ & $0.5$ & $0.5$  \\
&H & $-$ & $-$ & $2$ & $4$\\
\hline
% Example rows (add your actual data here)
\multirow{2}{*}{FPGW} & $\alpha$ & $0.5$ & $0.5$ & $0.5$ & $0.5$   \\
& H & $-$ & $-$ & $2$ & $4$\\
\hline
\multirow{3}{*}{FUGW} & $\alpha$ & $0.5$ & $0.5$ & $0.5$ & $0.5$   \\
& H & $-$ & $-$ & $2$ & $4$\\
& $\rho$ & $1$ & $1$ & $0.4$ & $0.4$\\
& $\epsilon$ & 0.05 & 0.1 & 0.02&0.02\\
\hline
WLK & H & $-$ & $-$ & $5$ & $5$ \\
\hline
\multirow{2}{*}{GK} & $\kappa$ & $-$ & $-$ & $3$ & $3$ \\
                   & n-samples &  $-$ & $-$ & $100$ & $100$ \\
\hline
Odd Sth & $\kappa$ & $-$ & $-$ & $3$ & $3$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{We present the parameter setting in all the methods. $\sigma$ is the weight parameter in the SVM classifier. In FGW and FPGW, $\alpha$ parameter is the $\omega_2$ in formulations of FGW \eqref{eq:fgw} and FPGW \eqref{eq:fmpgw}. For WLK/FGW/FUGW/FPGW, the value $H$ is the Weisfeiler-Lehman labeling parameter. $\rho,\epsilon$ in $FUGW$ is the weight parameter for the marginal penalty and entropy regularization. 
Parameter $\kappa$ in the GK/ODD method is the graphlet size. ``n-samples'' in GK is the random draw sample size.
}
\end{table*}

\textbf{Wall-clock time analysis.}  

The wall-clock times are reported in Table \ref{tab:classification_run_time}. All graph data are formatted as NetworkX graphs using the \href{https://networkx.org/}{NetworkX library}. Continuous features are represented as 64-bit float NumPy vectors, while discrete features are stored as 64-bit integers.

For discrete feature graphs, WLK, GK, and Vertex Histogram are the fastest methods, while FGW and FPGW have similar wall-clock times. In contrast, Lovász Theta and Random Walk Kernel are the slowest. For continuous feature graphs, ``Propagation'' and ``GraphHopper'' are significantly faster than FGW and FPGW. 

Among the three fused-GW-based methods, FGW is the fastest overall, while FUGW is the slowest. A potential reason for this difference is that FGW and FPGW leverage a C++ linear programming solver for the OT/POT solving step. In addition, FUGW adapts the Sinkhorn solver, leading to an accuracy-efficiency trade-off. Specifically, a smaller $\epsilon$ can reduce the accuracy gap introduced by the entropic term; however, it increases the number of iterations required for convergence.





\section{Compute Resources}\label{sec:computation_resource}

All experiments presented in this paper are conducted on a computational machine with an AMD EPYC 7713 64-Core Processor, 8 $\times$ 32GB DIMM DDR4, 3200 MHz, and an NVIDIA RTX A6000 GPU.




% \begin{table*}[h!]
% \caption{Parameter settings for all methods}
% \label{sample-table-3}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccc}
% \toprule
% Dataset & Parameter & Synthetic & Proteins & Mutag & MSRC-9 \\
% \midrule
% \multirow{3}{*}{FGW} 
% & $\alpha$  & $0.5$ & $0.5$ & $0.5$ & $0.5$ \\
% & $\sigma$  & $1$   & $15$  & $2$   & $1$   \\
% & wl        & --    & --    & $2$   & $4$   \\
% \midrule
% \multirow{3}{*}{FPGW} 
% & $\alpha$  & $0.5$ & $0.5$ & $0.5$ & $0.5$ \\
% & $\sigma$  & $1$   & $15$  & $2$   & $1$   \\
% & wl        & --    & --    & $2$   & $4$   \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}



% \begin{table*}
% {\footnotesize
% \begin{tabular}{lcccccccccccccccc} % 17 columns: 1 for Data set + 16 for parameters
% \toprule
% Data set & \multicolumn{4}{c}{Synthetic} & \multicolumn{4}{c}{Proteins} & \multicolumn{4}{c}{Mutag} & \multicolumn{4}{c}{MSRC-9} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13} \cmidrule(lr){14-17}
% $\eta$ & $0$ & $10\%$ & $20\%$ & $30\%$ & $0$ & $10\%$ & $20\%$ & $30\%$ & $0$ & $10\%$ & $20\%$ & $30\%$ & $0$ & $10\%$ & $20\%$ & $30\%$ \\
% \midrule
% FGW  & 7.20 & 13.23 & 16.80 & 22.81 & 26.05 & 28.71 & 29.83 & 30.17 & 0.79 & 0.83 & 0.87 & 0.91 & 4.03 & 4.31 & 4.63 & 5.05 \\
% FPGW (ours) & 13.23 & 14.66 & 17.03 & 19.44 & 62.03 & 65.99 & 71.54 & 73.19 & 1.09 & 1.54 & 3.62 & 9.21 & 4.54 & 4.95 & 5.17 & 5.77 \\
% \bottomrule
% \end{tabular}
% }
% \caption{We present the wall-clock time (in minutes) for the graph classification experiment on datasets ``Synthetic'', ``Proteins'', ``Mutag'', and ``MSRC-9''. $\eta\in\{0,0.1,0.2,0.3\}$ is the percentage of outlier nodes.}
% \label{tab:results_synthetic_proteins_mutag_nci1}
% \end{table*}

\begin{table}[htbp]
\caption{Kernel computation times (minutes) across different datasets and noise levels}
\label{tab:classification_run_time}
\renewcommand{\tabcolsep}{1.2em}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lrrrr@{}}
\toprule
\textbf{Method} & \textbf{0\%} & \textbf{10\%} & \textbf{20\%} & \textbf{30\%} \\
\midrule
\multicolumn{5}{l}{\textbf{SYNTHETIC}} \\
Propagation & 0.01 & 0.02 & 0.01 & 0.02 \\
GraphHopper & 5.69 & 5.11 & 5.32 & 8.31 \\
\cmidrule(lr){1-5}
FGW & 7.20 & 13.23 & 16.80 & 22.81 \\
FUGW & 41.00 & 40.53 & 35.32 & 37.88 \\

FPGW & 13.23 & 14.66 & 17.03 & 19.44 \\
\midrule
\multicolumn{5}{l}{\textbf{PROTEINS}} \\
Propagation & 0.02 & 0.02 & 0.02 & 0.03 \\
GraphHopper & 7.06 & 7.43 & 7.79 & 8.16 \\
\cmidrule(lr){1-5}
FGW & 26.05 & 28.71 & 29.83 & 30.17 \\
FUGW & 96.78 & 186.93 & 204.20 & 220.96 \\
FPGW & 62.03 & 65.99 & 71.54 & 73.19 \\
\midrule
\multicolumn{5}{l}{\textbf{MSRC}} \\
WLK (auto) & 0.00 & 0.00 & 0.00 & 0.00 \\
GK (k=3) & 0.04 & 0.04 & 0.04 & 0.04 \\
RWK & 70.79 & 71.01 & 100.29 & 90.90 \\
Odd Sth (k=3) & 0.05 & 0.05 & 0.05 & 0.05 \\
Vertex Histogram & 0.00 & 0.00 & 0.00 & 0.00 \\
Lovasz Theta & 151.14 & 313.54 & 298.87 & 228.57 \\
SVM Theta & 0.00 & 0.00 & 0.00 & 0.00 \\
\cmidrule(lr){1-5}
FGW & 4.03 & 4.31 & 4.63 & 5.05 \\
FUGW & 44.09 & 44.16 & 41.10 & 42.30  \\
FPGW & 4.54 & 4.95 & 5.17 & 5.77 \\
\midrule
\multicolumn{5}{l}{\textbf{MUTAG}} \\
WLK (auto) & 0.00 & 0.00 & 0.00 & 0.00 \\
GK (k=3) & 0.03 & 0.03 & 0.03 & 0.03 \\
RWK & 211.48 & 61.07 & 66.10 & 305.90 \\
Odd Sth (k=3) & 0.00 & 0.00 & 0.00 & 0.00 \\
Vertex Histogram & 0.00 & 0.00 & 0.00 & 0.00 \\
Lovasz Theta & 22.48 & 19.14 & 18.67 & 13.82 \\
SVM Theta & 0.00 & 0.00 & 0.00 & 0.00 \\
\cmidrule(lr){1-5}
FGW & 0.79 & 0.83 & 0.87 & 0.91 \\
FUGW & 19.70 & 21.76 & 20.98 & 17.69 \\

FPGW & 1.09 & 1.54 & 3.62 & 9.21 \\
\bottomrule
\end{tabular*}
\end{table}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one.  


%The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix or can be removed if you prefer a two-column appendix. Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Impact Statement}
This paper aims to advance the theoretical foundations and potential applications of Optimal Transport in the field of Machine Learning.  There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

\section{}
