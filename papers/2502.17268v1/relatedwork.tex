\section{Background and Related Work}
We will now describe related work for TOD systems, data augmentation and data style translation.

\textbf{TOD systems and datasets.}
TOD systems were traditionally implemented by solving each subtask separately \cite{youngPOMDPBasedStatisticalSpoken2013}.
With the publication of large datasets, the field has moved towards deep learning-based systems such as \citet{lin-etal-2020-mintl, peng-etal-2021-soloist, heGalaxyGenerativePretrained2022}.
Benchmark datasets include, e.g., MultiWOZ \cite{budzianowski-etal-2018-multiwoz},
%This dataset contains a total of roughly ten thousand dialogues, and its publication has massively increased the size of the available benchmark datasets.
KVRET \cite{eric-etal-2017-key} and SGD \cite{rastogiScalableMultiDomainConversational2020}.
% The latter, while larger than MultiWOZ, was, however, not created using the Wizard-of-Oz setup \cite{kelleyIterativeDesignMethodology1984}, but by having crowd workers verbalize utterances that were sampled based on schemata.

% \subsection{Natural Language Interface to Database}
% A TOD system for the present use case will, from a technical point of view, likely have to derive one or multiple database requests from the user's natural language input.
% This task of translating natural language into a database query in a structured language like SQL has been investigated even before the advent of deep learning under the name of \textit{Natural Language Interface to Database} (NLIDB) \cite{nihalaniNaturalLanguageInterface2011,kumarNaturalLanguageInterface2013,liConstructingInteractiveNatural2014}. 
% The desired advantage of this approach was to facilitate the use of database systems and make the interfaces more user-friendly for non-experts, even for complex intents \cite{liConstructingInteractiveNatural2014}.
% However, shortcomings in the NLP part of such a system led to problems in early research \cite{nihalaniNaturalLanguageInterface2011, kumarNaturalLanguageInterface2013}.
% With the general progress in NLP and the availability of public datasets, NLIDB has regained some focus, and progress has been made thanks to deep learning approaches \cite{abbasReviewNLIDBDeep2022}.

\textbf{Data augmentation for dialogues.}
Data augmentation describes the sourcing of synthetic data by applying certain transformations to existing data in order to increase the amount of training data and the model's generalization ability \cite{shortenTextDataAugmentation2021}.
%While, to the best of our knowledge, none of the current state-of-the-art models for the MultiWOZ benchmark dataset actively use data augmentation, different techniques have been studied for TOD systems in general.
Approaches include backtranslation \cite{kulhanekAuGPTAuxiliaryTasks2021}, incorporation of external datasets \cite{xu-etal-2021-caire}, simulating dialogues based on schemata \cite{pengSYNERGYBuildingTask2021}, graphs \cite{grittaConversationGraphData2021}, framing it as a text infilling task \cite{axmanContextualDataAugmentation} or using specially trained generator models \cite{steindl-etal-2023-controlled}.
Nowadays, this line of research has also turned to LLMs.
These methods include, for example, paraphrasing templates, using seed data or adding miscommunications to the dialogues
\cite{liControllableDialogueSimulation2022,kulkarni2024synthdst, chenPLACESPromptingLanguage2023, mehriLADLanguageModels2022, steindl-etal-2025-coprus}.
Recently, \textit{model collapse} \cite{shumailovAIModelsCollapse2024} has been discussed, where a model's performance degrades with every iteration of it being trained on model-generated data. 
One way to counteract this is by combining real and synthetic data \cite{gerstgrasserModelCollapseInevitable2024}, which our method does by utilizing the human-written e-mails.

\textbf{Data style translation.}
Translating a text from one ``style'' to another can be interpreted as a special case of NLG and controlled generation.
First, we see summaries, especially abstractive summaries \cite{GUPTA201949}, as one form of such translation.
Furthermore, data-to-text approaches \cite{jagfeldSequencetoSequenceModelsDatatoText2018, sharmaInnovationsNeuralDatatotext2023, wangEvaluatingTextGeneration2021} are relevant applications of this paradigm.
Automatic news writing is another application \cite{Diakopoulos+2019}, as is 
the creation of a dialogue based on a short story \cite{miyazakiDialogueGenerationConditional2023a}.
Further, the HR\-MultiWOZ \cite{xu-etal-2024-hr} dataset is based on schemata that get turned into templates and are paraphrased by an LLM\@.

% With LLMs exhibiting strong NLU and NLG capabilities, one can envision further use cases where the user describes constraints for the generation of natural language being implemented in day-to-day work.
% Examples could include rewriting legal or medical jargon into everyday speech or explaining compiler errors in natural language.

%-----------------------------------------------------------------------------------%