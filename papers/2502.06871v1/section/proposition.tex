\section{Proposition: FlavorDiffusion}

\subsection{Sub-Graph Sampling}

FlavorDiffusion is built upon the DIFUSCO Gaussian noise-based diffusion model, extending its capabilities to structured food-chemical graphs. The core objective is to train a model capable of reconstructing subgraphs sampled from the full heterogeneous graph \( G = (V, E) \) while leveraging node attributes as guidance. 

The full graph consists of a diverse set of nodes \( V \), including hub ingredients, non-hub ingredients, flavor compounds, and drug compounds, with edges \( E \) encoding the strength of their relationships as continuous values in \( [0,1] \). We define a dataset of subgraphs, where each sample contains \( m \) nodes selected from \( G \). These subgraphs are denoted as:

\begin{equation*}
    \mathcal{D}_m = \{ G_i = (V_i, E_i) \}_{i=1}^{N},
\end{equation*}

where each subgraph \( G_i \) has \( |V_i| = m \) nodes and an adjacency matrix \( E_i \) of size \( m \times m \), representing pairwise edge scores. The dataset is partitioned into training (\( N_t \)) and validation (\( N_v \)) subsets.

\subsection{Forward Diffusion Process}

For a single data point \( G_i = (V_i, E_i) \) sampled from the dataset, we define the diffusion process over its edge set \( E_i \). By convention, we denote the corrupted version of \( E_i \) at timestep \( t \) as \( x_t \), aligning with standard diffusion formalisms. The node representations, encompassing all vertex features, are denoted as \( \mathbf{Emb} \).

The forward diffusion process follows a Markovian Gaussian noise injection, progressively perturbing the edges \( x_t \) while preserving node representations:

\begin{equation*}
    q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I),
\end{equation*}

where \( \beta_t \) is a predefined noise variance at timestep \( t \). Given an initial clean edge matrix \( x_0 = E_i \), we can analytically express the direct corruption of \( x_0 \) at any timestep \( t \) as:

\begin{equation*}
    q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I),
\end{equation*}

where \( \bar{\alpha}_t = \prod_{s=1}^{t} (1 - \beta_s) \) represents the cumulative noise effect over time. This formulation allows direct sampling of \( x_t \) from \( x_0 \), bypassing iterative updates.

In this framework, the edge structure is progressively degraded into Gaussian noise, while node representations \( \mathbf{Emb} \) remain unchanged, ensuring that denoising relies on learned node attributes.

\subsection{Reverse Denoising Process}

The reverse process seeks to recover \( x_0 \) from the fully corrupted state \( x_T \), learning to remove noise in a stepwise manner. The key assumption is that the forward process follows a Gaussian transition, enabling an analytically derived reverse process.

Given the Markovian nature of the diffusion process, we define the true posterior:

\begin{equation*}
    q(x_{t-1} | x_t, x_0) = \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I),
\end{equation*}

where the posterior mean and variance are derived as:

\begin{equation*}
    \tilde{\mu}_t(x_t, x_0) = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} x_t,
\end{equation*}

\begin{equation*}
    \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t.
\end{equation*}

Since \( x_0 \) is unknown, we train a model \( p_{\theta}(x_0 | x_t) \) to approximate it. Substituting the predicted \( x_0 \), the learned reverse process is modeled as:

\begin{align*}
    p_{\theta}(x_{t-1} | x_t, \mathbf{Emb}) &= \\
    \mathcal{N}\big(x_{t-1}; &\mu_{\theta}(x_t, t, \mathbf{Emb}), \Sigma_{\theta}(x_t, t)\big),
\end{align*}

where \( \mu_{\theta} \) is the learned estimate for \( \tilde{\mu}_t(x_t, x_0) \), and the variance term is fixed as \( \Sigma_{\theta}(x_t, t) = \tilde{\beta}_t I \), avoiding the need for explicit learning. The function \( \mu_{\theta} \) is now conditioned on the node representations (\(\mathbf{Emb}\)) of the two vertices forming the edge.

Using the DDPM convention, we parameterize \( \mu_{\theta} \) as:

\begin{align*}
    \mu_{\theta}(x_t, t, \mathbf{Emb}) &= \frac{1}{\sqrt{\alpha_t}} \Bigg( x_t \\
    &\quad - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_{\theta}(x_t, t, \mathbf{Emb}) \Bigg),
\end{align*}

where \( \epsilon_{\theta}(x_t, t, \mathbf{Emb}) \) is the learned noise estimate, which is now explicitly conditioned on the representations of the two nodes forming the edge. The node representations provide additional context for denoising by leveraging node-specific features.

\subsection{Optimization via Variational Lower Bound}

To train the reverse model, we maximize the variational lower bound (ELBO), decomposed as:

\begin{align*}
    \mathcal{L}_{\text{ELBO}} = E_q \Bigg[
    \log p_{\theta}(x_0 | x_1, \mathbf{Emb}) \\
    - \sum_{t=1}^{T} D_{\text{KL}}\big(q(x_{t-1} | x_t, x_0) \| p_{\theta}(x_{t-1} | x_t, \mathbf{Emb})\big) 
    \Bigg].
\end{align*}

Here, \( T \) represents the total number of diffusion steps, defining the depth of the forward and reverse process. The KL divergence encourages the learned transitions to match the true posterior. Since \( q(x_t | x_0) \) is Gaussian, minimizing \( D_{\text{KL}} \) is equivalent to predicting the noise component \( \epsilon \) added during diffusion. Thus, the training objective simplifies to:

\begin{equation*}
    \mathcal{L}_{\text{recon}} = E_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_{\theta}(x_t, t, \mathbf{Emb}) \|^2 \right].
\end{equation*}

This loss ensures that \( \epsilon_{\theta} \) effectively estimates the noise introduced in the forward process while incorporating node representations. By iteratively refining the denoising function, FlavorDiffusion reconstructs the original ingredient-ingredient graph from noisy subgraphs, leveraging both the structural edge information and node attributes to enhance predictive modeling for food pairing analysis.

\subsection{Inference}

Graph reconstruction follows Denoising Diffusion Implicit Models (DDIM) for efficient and deterministic sampling. Unlike DDPM, DDIM removes noise via a non-Markovian update, accelerating inference.

Starting from \( x_T \sim \mathcal{N}(0, I) \), the reverse process iterates:

\begin{equation*}
    x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \hat{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \cdot \epsilon_{\theta}(x_t, t, \mathbf{Emb}),
\end{equation*}

where the predicted clean graph is:

\begin{equation*}
    \hat{x}_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon_{\theta}(x_t, t, \mathbf{Emb})}{\sqrt{\bar{\alpha}_t}}.
\end{equation*}

Iterating from \( T \) to \( 0 \), the model refines \( x_t \) to recover ingredient-ingredient relationships. DDIM ensures fast, stable, and chemically meaningful reconstructions.

\subsection{Model Architecture}

The noise prediction network \( \epsilon_{\theta}(x_t, t, \mathbf{V}) \) employs an anisotropic GNN to iteratively refine node and edge embeddings. Let \( h_i^\ell \in \mathbf{R}^d \) and \( e_{ij}^\ell \in \mathbf{R}^{d_e} \) denote the node and edge features at layer \( \ell \), respectively. The refinement process updates both edge and node embeddings through the following operations:

\paragraph{Edge Refinement} The initial edge embeddings \( e_{ij}^0 \) are set as the corresponding values from the noisy edge representation \( x_t \). At each layer \( \ell \), the intermediate edge embeddings \( \hat{e}_{ij}^\ell \) are updated as:

\begin{equation*}
    \hat{e}_{ij}^\ell = P^\ell e_{ij}^\ell + Q^\ell h_i^\ell + R^\ell h_j^\ell,
\end{equation*}

where \( P^\ell, Q^\ell, R^\ell \in \mathbf{R}^{d_e \times d_e} \) are learnable parameters. The refined edge embedding \( e_{ij}^{\ell+1} \) is then computed as:

\begin{equation*}
    e_{ij}^{\ell+1} = e_{ij}^\ell + \text{MLP}_e\big(\text{BN}(\hat{e}_{ij}^\ell)\big) + \text{MLP}_t(t),
\end{equation*}

where \( \text{MLP}_e \) is a 2-layer perceptron and \( \text{MLP}_t \) embeds the diffusion timestep \( t \) using sinusoidal features.

\paragraph{Node Refinement}
The node embeddings \( h_i^\ell \) are refined by aggregating information from neighboring nodes and their associated edges. The update rule for \( h_i^{\ell+1} \) is given by:

\begin{equation*}
    h_i^{\ell+1} = h_i^\ell + \alpha \cdot \text{BN}\Big(U^\ell h_i^\ell + \sum_{j \in \mathcal{N}(i)} \sigma(\hat{e}_{ij}^\ell) \odot V^\ell h_j^\ell\Big),
\end{equation*}

where \( U^\ell, V^\ell \in \mathbf{R}^{d \times d} \) are learnable parameter matrices, \( \sigma \) is the sigmoid activation function used for edge gating, \( \odot \) denotes the Hadamard (element-wise) product, \( \mathcal{N}(i) \) represents the set of neighbors for node \( i \), and \( \alpha \) is the ReLU activation applied after aggregation.

\paragraph{Final Prediction}
After \( L \) GNN layers, the final refined edge embeddings \( E^{(L)} \in \mathbf{R}^{N \times N \times d_e} \) are passed through a ReLU activation and a multi-layer perceptron (MLP) to predict the noise:

\begin{equation*}
    \epsilon_{\theta}(x_t, t, \mathbf{V}) = \text{MLP}\big(\text{ReLU}(E^{(L)})\big).
\end{equation*}

This formulation ensures that both node and edge embeddings are iteratively refined to capture local and global graph structure, enabling robust denoising and reconstruction of ingredient-ingredient relationships.
