\section{Experimental Results}

The evaluation consists of two primary experiments: (1) reproducing the NMI-based clustering performance evaluation originally conducted in FlavorGraph, and (2) assessing the generalization ability of our proposed Flavor Diffusion framework by testing on subgraphs of different sizes.

Subgraphs of size 25, 50, 100, and 200 nodes were sampled while maintaining an equal proportion of hub and non-hub ingredients. The number of subgraphs used for training and testing at each scale is shown in Table~\ref{tab:subgraph_stats}. 

\begin{table}[h!]
  \centering
  \caption{Subgraph Composition for Training and Testing}
  \label{tab:subgraph_stats}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{ccc}
    \hline
    \textbf{Nodes per Subgraph} & \textbf{Train Set Size} & \textbf{Test Set Size} \\
    \hline
    25  & 256,000 & 256 \\
    50  & 128,000 & 128 \\
    100 & 64,000  & 64  \\
    200 & 32,000  & 32  \\
    \hline
  \end{tabular}%
  }
\end{table}

\paragraph{Generalization Ability}

To assess the generalization ability of the proposed framework, models trained on one subgraph size were tested on all sizes to observe performance across different scales. The results in Table~\ref{tab:generalization_results} indicate that models trained on 25-node subgraphs generalize poorly to larger graphs, with an MSE of 0.025078 when tested on 100-node subgraphs. In contrast, the 100-node trained model demonstrates the most stable generalization across different test sizes, showing minimal MSE variation. The 200-node trained model, while excelling on large graphs with an MSE of 0.003692, exhibits difficulties in adapting to smaller structures, with a high error of 0.059557 when tested on 25-node subgraphs.

\begin{table}[h!]
  \centering
  \caption{Generalization Performance: Validation MSE Loss}
  \label{tab:generalization_results}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{lcccc}
    \hline
    \textbf{Train Size} & \textbf{Test (25)} & \textbf{Test (50)} & \textbf{Test (100)} & \textbf{Test (200)} \\
    \hline
    25  & 0.004589 & 0.010965 & 0.025078 & 0.019477 \\
    50  & 0.025235 & 0.005884 & 0.004420 & 0.004123 \\
    100 & 0.003964 & 0.003678 & 0.004232 & 0.003953 \\
    200 & \textbf{0.059557} & 0.007837 & 0.003992 & \textbf{0.003692} \\
    \hline
  \end{tabular}%
  }
\end{table}

These results highlight that subgraph size significantly impacts both intra-subgraph clustering and cross-subgraph generalization performance. The Flavor Diffusion (100 nodes) model provides the best balance between clustering accuracy and scalability, demonstrating the ability to generalize well across varying ingredient graph structures. On the other hand, training on extremely small subgraphs limits generalization, while models trained on large subgraphs struggle when applied to smaller ingredient sets. These findings suggest that a mid-sized subgraph training approach (e.g., 100 nodes) is optimal for robust ingredient representation learning.


\paragraph{NMI-based Evaluation}

To construct the clustering test dataset, nine representative food categories were defined: \textit{Bakery/Dessert/Snack, Beverage Alcoholic, Cereal/Crop/Bean, Dairy, Fruit, Meat/Animal Product, Plant/Vegetable, Seafood, and Others}. From these, 416 chemical hub ingredients with strong connections were selected to ensure diverse and well-defined clustering labels, enabling fair comparisons across models commonly used in related studies.


The NMI-based evaluation results in Table~\ref{tab:nmi_results} demonstrate the clustering quality of different models. Among the non-CSP variants, the Flavor Diffusion (50 nodes) model achieves the highest NMI score of 0.3236, surpassing the baseline FlavorGraph model without CSP. The best overall performance is observed in the Flavor Diffusion\_CSP (200 nodes) model, which achieves an NMI score of 0.3410, indicating that the CSP layer significantly improves the learned ingredient embeddings. Smaller subgraphs, such as the 25-node configuration, show the greatest improvement when using CSP (0.2970 vs. 0.2167), suggesting that the chemical structure prediction enhances clustering, particularly in more limited ingredient sets.

\begin{table}[h!]
  \centering
  \caption{Performance Comparison Using NMI Metric. *CSP shorts for chemical structure prediction.}
  \label{tab:nmi_results}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{lcc}
    \hline
    \textbf{Model} & \textbf{NMI Mean} & \textbf{NMI Std} \\
    \hline
    FlavorGraph \cite{Park2019}      & $0.2995$ & $0.0403$ \\
    FlavorGraph\_CSP \cite{Park2019} & $0.3102$ & $0.0407$ \\
    \hline
    Flavor Diffusion (25 nodes)       & $0.2167$ & $0.0319$ \\
    Flavor Diffusion (50 nodes)       & $\mathbf{0.3236}$ & $\mathbf{0.0134}$ \\
    Flavor Diffusion (100 nodes)      & $0.3170$ & $0.0207$ \\
    Flavor Diffusion (200 nodes)      & $0.2935$ & $0.0300$ \\
    \hline
    Flavor Diffusion\_CSP (25 nodes)  & $0.2970$ & $0.0144$ \\
    Flavor Diffusion\_CSP (50 nodes)  & $0.2862$ & $0.0152$ \\
    Flavor Diffusion\_CSP (100 nodes) & $0.3169$ & $0.0257$ \\
    Flavor Diffusion\_CSP (200 nodes) & $\mathbf{0.3410}$ & $\mathbf{0.0150}$ \\
    \hline
  \end{tabular}%
  }
\end{table}