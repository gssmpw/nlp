%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.


%\documentclass[manuscript, anonymous, review]{acmart}
\documentclass[sigconf]{acmart}


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%\usepackage{array}
%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{cc}
\setcctype{by-nc-sa}
\acmConference[IUI '25]{30th International Conference on Intelligent User Interfaces}{March 24--27, 2025}{Cagliari, Italy}
\acmBooktitle{30th International Conference on Intelligent User Interfaces (IUI '25), March 24--27, 2025, Cagliari, Italy}\acmDOI{10.1145/3708359.3712151}
\acmISBN{979-8-4007-1306-4/25/03}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\usepackage[final,commandnameprefix=always]{changes}
\usepackage{xcolor}
\definechangesauthor[name={Your Name}, color=orange]{YN}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{MemPal: Leveraging Multimodal AI and LLMs for Voice-Activated Object Retrieval in Homes of Older Adults} 

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Natasha Maniar}
\orcid{0000-0002-7490-3718}
\affiliation{%
  \institution{MIT Media Lab}
  \city{Cambridge}
    \country{USA}}    
%  \streetaddress{P.O. Box 1212}
%%\city{Cambridge}
%%\state{Massachusetts}
%%\country{United States}
\email{nmaniar@media.mit.edu}



\author{Samantha Chan}
\orcid{0000-0003-1159-0467}
\affiliation{%
  \institution{MIT Media Lab}
  \city{Cambridge}
    \country{USA}}    
%  \streetaddress{P.O. Box 1212}
%%\city{Cambridge}
%%\state{Massachusetts}
%%\country{United States}
\email{swtchan@media.mit.edu}

\author{Wazeer Zulfikar}
\orcid{0009-0008-7753-8817}
\affiliation{%
  \institution{MIT Media Lab}
  \city{Cambridge}
    \country{USA}}    
%  \streetaddress{P.O. Box 1212}
%%\city{Cambridge}
%%\state{Massachusetts}
%%\country{United States}
\email{wazeer@media.mit.edu}

\author{Scott Ren}
\orcid{0009-0002-8725-1648}
\affiliation{%
  \institution{MIT Media Lab}
  \city{Cambridge}
    \country{USA}}    
%  \streetaddress{P.O. Box 1212}
%%\city{Cambridge}
%%\state{Massachusetts}
%%\country{United States}
\email{scottren@media.mit.edu}

\author{Christine Xu}
\orcid{}
\affiliation{%
  \institution{MIT Media Lab}
  \city{Cambridge}
    \country{USA}}    
%  \streetaddress{P.O. Box 1212}
%%\city{Cambridge}
%%\state{Massachusetts}
%%\country{United States}
\email{cjx@mit.edu}

\author{Pattie Maes}
\orcid{0000-0002-7722-6038}
\affiliation{%
  \institution{MIT Media Lab}
  \city{Cambridge}
    \country{USA}}    
%  \streetaddress{P.O. Box 1212}
%%\city{Cambridge}
%%\state{Massachusetts}
%%\country{United States}
\email{pattie@media.mit.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Maniar, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
% ONLY FOCUSING ON RETROSPECTIVE MEMORY LOSS!
 Older adults have increasing difficulty with retrospective memory, hindering their abilities to perform daily activities and posing stress on caregivers to ensure their wellbeing. Recent developments in Artificial Intelligence (AI) and large context-aware multimodal models offer an opportunity to create memory support systems that assist older adults with common issues like object finding. This paper discusses the development of an AI-based, wearable memory assistant, MemPal, that helps older adults with a common problem, finding lost objects at home, and presents results from tests of the system in older adults' own homes. Using visual context from a wearable camera, the multimodal LLM system creates a real-time automated text diary of the person's activities for memory support purposes, offering object retrieval assistance using a voice-based interface. The system is designed to support additional use cases like context-based proactive safety reminders and recall of past actions. We report on a quantitative and qualitative study with N=15 older adults within their own homes that showed improved performance of object finding with audio-based assistance compared to no aid and positive overall user perceptions on the designed system. We discuss further applications of MemPal’s design as a multi-purpose memory aid and future design guidelines to adapt memory assistants to older adults’ unique needs. 

\end{abstract}


%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10003128</concept_id>
       <concept_desc>Human-centered computing~Interaction techniques</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
    <concept>
            <concept_id>10003120.10003121.10003124.10010870</concept_id>
           <concept_desc>Human-centered computing~Natural language interfaces</concept_desc>
           <concept_significance>500</concept_significance>
           </concept>
   <concept>
       <concept_id>10003120.10003121.10011748</concept_id>
       <concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}




\ccsdesc[500]{Human-centered computing~Interaction techniques}
\ccsdesc[500]{Human-centered computing~Natural language interfaces}
\ccsdesc[300]{Human-centered computing~Empirical studies in HCI}
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{memory assistant, large language models, large visual language models, voice interfaces, context-aware agent, multimodal systems, wearables, older adults}


\begin{teaserfigure}
    \centering
    \includegraphics[scale=0.25]{figures/banner.pdf}
    \caption{MemPal object location tracking and retrieval feature}
    \label{fig:mempalsystem}
\end{teaserfigure}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

The global population of older adults is projected to reach 2.1 billion by 2050, with their ratio of the population in the USA expected to increase from 16\% to 22\% \cite{WHO2024, statista2022agepopulation}. Memory impairments, including Subjective Cognitive Decline (SCD), Mild Cognitive Impairment (MCI), and diagnosed dementia, affect about one-third of this demographic \cite{AlzheimersFactsFigures2023}. Despite these challenges, most older adults prefer to age in place rather than in a nursing home \cite{kasper2015disproportionate} with many living alone \cite{harris2006experience}. Memory function is critical for their independence to comfortably live at home with minimal assistance\cite{marchant2011memory}.

Older adults experience declines in retrospective memory, significantly impacting daily functions like object retrieval \cite{mogle2023individual, ramos2016designing}, significantly more than younger adults \cite{balota2000memory}. Such memory issues not only strain individuals but also their caregivers—mostly unpaid family members—who face burnout from constant monitoring \cite{kasper2015disproportionate}. The economic impact of unpaid caregiving was valued at 196 billion in 1997 \cite{arno1999economic}.

%An opportunity exists to leverage technology to address some of the memory issues of older adults. A crucial challenge, however, is tailoring the technology to match their preferences, such as natural voice interaction rather than text-based communication on non-wearable external devices like smartphones or tablets \cite{stigall2019older}. Advances in AI and natural language processing, specifically multi-modal foundation models (LLMs), can be used to understand context and human intent, and facilitate easier interaction through voice-based interfaces \cite{brown2020language,wang2023enabling}.

There is an opportunity to use advances in AI and conversational interfaces to alleviate some of these memory challenges. Specifically VLMs (Vision language models) can better transform visual and text data for contextual assistance like creating accurate activity descriptions and LLMs can understand complex queries, necessary for memory impaired individuals. Successful development of such technology requires focusing on key needs and usability preferences of older adults, such as integrating natural voice interaction instead of text-based interfaces to support independent living \cite{stigall2019older, brown2020language, wang2023enabling} or simplifying the onboarding process to reduce complexity \cite{collerton2014exploratory}.  

%Tailoring technology to older adults' preferences, such as using natural voice interaction instead of traditional text-based interfaces, could enhance usability and easier interaction to support independent living \cite{stigall2019older, brown2020language, wang2023enabling}.


%We began by interviewing a diverse group of participants, including older adults, individuals with and without dementia, caregivers, and physicians, to identify key user needs and pain points. These interviews helped shape the design of a new technological intervention aimed at addressing these  challenges. Based on the insights gathered, we developed targeted features that were incorporated into the development of MemPal. 


This paper presents a multimodal wearable system for older adults that leverages LLM technology to assist their retrospective memory within their homes. Our development and testing of this system begins with a common pain point—assisting with finding lost objects. MemPal utilizes visual context and a voice-based, open-ended natural language interface to help users locate misplaced objects (Figure \ref{fig:mempalsystem}). It automatically logs visual information about the user’s environment (object held, location) and activities into a text-based diary which then integrates with a voice-queried LLM for retrospective memory assistance, for example to assist with finding misplaced objects. The system can possibly be further adapted to address other memory issues like recalling past actions and assisting with proactive memory, like providing proactive context-based safety reminders. 


The questions we aim to address in this research are as follows: 

\begin{itemize}
    \item \textbf{RQ1}: What are the effects of using a voice-enabled multimodal LLM system for object retrieval on retrieval accuracy, path length, cognitive task load, retrieval confidence and recall difficulty for older adults, compared to using visual cues or no system?
    
    \item \textbf{RQ2}: What are older adults' perceptions and experiences of using a voice-enabled wearable system like MemPal for object retrieval and generally memory assistance?  
\end{itemize}

To evaluate MemPal, we conducted a study with 15 older adults (ages 62-96) in their own homes, simulating a near real-life setting. The study focused on assessing overall user perceptions and the technical accuracy of the system in varying home environments. Findings indicated that participants' objective performance in object finding tasks significantly improved with MemPal's audio assistance in terms of decreased search time and number of objects retrieved as compared to no assistance, but similar to using visual aid. Perceived difficulty of recall also significantly reduced with MemPal's assistance. 

The contributions of this paper include: 
\begin{itemize}
    \item (1) Design of an LLM-based multimodal, wearable system, MemPal, that assists older adults with memory issues such as finding lost objects within their own home through a natural conversational voice interface and contextual visual information, requiring limited system onboarding through a user-centered approach.  
    \item (2) A within-subject user study with older adult subjects in their own homes that validates helpfulness of the object finding system, increased objective performance and decreased recall difficulty for object finding, and qualitative analysis of user preferences informing future design guidelines.
\end{itemize}

\section{Related Work}
Our work is inspired by previous work on wearable assistants for older adults as well as object-finding and lifelogging tools. Since memory aids are typically underutilized in older adults than in younger adults \cite{schryer2013use}, we explore the interaction of a multimodal user interface within the older adult population to assist with memory. 
% reframe to (1) overview of memory aids for older adults- all types. Then talk about how younger vs older people approach memory aids - https://onlinelibrary.wiley.com/doi/abs/10.1002/acp.2946?casa_token=a5HTMdaIx80AAAAA%3AR1iSMU16PA3BgxD26399vRlZiTH6_q5Z-dHSYt4QWvQjP5fKEzBO9JRASPsfxHo3Mvajj0k-Loc77do 

% clearly state the novelty in the work- automated diary, voice based, AI context based and home tour (limited onboarding) 
    

\subsection{Voice-based, Camera-based and Wearable Assistants for Older Adults}
\label{relatedworks_assistants}
%voice-based 

Prior works showed positive perceptions towards voice user interfaces (VUIs) by older adults and benefits of VUIs (e.g., Google Home, Amazon Alexa, social robots) in their daily lives more so than younger adults \cite{stigall2019older}. These benefits include being assistants for various functions such as seeking information and controlling home devices~\cite{mehrotra2016embodied,opfermann2017communicative,tsiourti2014virtual,o2022optimizing}, companions to alleviate loneliness~\cite{sidner2018creating,jones2021reducing,pradhan2019phantom,corbett2021voice,gasteiger2021friends}, helping users develop skills~\cite{brewer2017xpress,ali2018aging}, encouraging physical activity~\cite{bickmore2005s}, \textcolor{black}{and providing personalized reminders through AR~\cite{10.1145/3463914.3463918}}. Wearable conversational agents have been used to provide memory training for older adults~\cite{chan2020prompto,chan2019prospero,10.1145/3334480.3375031} and monitor their daily physical activity~\cite{romanvoice} but these agents do not use visual data for \textcolor{black}{general} memory assistance. 

%It finds that optimal trigger times for these voice based prompts/reminders are during lower cognitive loads and idle times, suggesting that assisting with prospective memory loss should take voice-notification timing into consideration. 
%As MemPal aims to provide just-in-time safety reminders based on location context, we investigate the potential timing for these prompts during tasks. 
%The language of voice queries is closer to natural language than typed queries~\cite{guy2016searching}, thus, voice-based assistants can be highly usable compared to screen-based/typing-based interfaces. 
With the integration of language models in voice assistants and advancements in language understanding, users can converse more naturally with these agents. LLMs now have an increased ability to process larger unstructured text and provide a contextualized response. In this work, we implement a multi-modal real-time LLM voice-based assistant that includes image input as context and uses vision language models for intelligent understanding. \textcolor{black}{Although the use of LLMs on older adults has recently been explored for fostering conversations such as A-CONECT \cite{hong2024aconect} and Mindtalker \cite{xygkou2024mindtalker}, the use of multimodal LLMs that can process greater context has not.}


%Camera-based [research lifelogging literature] Remote tracking of the older adults using c
Previous Human-Computer Interaction (HCI) studies have mainly explored camera-based systems to support remote tracking in older adults' homes and could reduce caregiver burden via a smart-connected home with camera sensors. These systems could provide health monitoring~\cite{abowd2002aware}, detecting anomalies in activities of daily living~\cite{buzzelli2020vision} and fall detection~\cite{de2017home}.
%Prior Human-Computer Interaction (HCI) studies and commercial products aim to solve this issue via a smart connected home with camera sensors or wearable camera systems. 
%Since 2002, the proposed idea of the aware-home has been an ongoing study to transform the home of an older adults individual into one that is integrated with sensors for better patient monitoring \cite{abowd2002aware}. 
%Applications for in home camera sensors primarily include fall detection systems \cite{de2017home} or activity recognition to detect anomalies in activities of daily living \cite{buzzelli2020vision}. 
Despite this, the technology readiness level of smart home and health monitoring technologies for older adults has been considered low and there is limited evidence that they help improve quality of life~\cite{liu2016smart}. Other limitations include the need to deploy multiple cameras to broaden monitoring coverage~\cite{hasan2019real}. %The extent of tracking is directly proportional to the number of installed cameras, with limited cameras potentially leading to gaps in activity identification.  
%Recent wearable camera systems have become more prevalent in the market, suggesting wider spread adoption of these technologies among the public. Examples include the Humane AI clip, Narrative clip camera, and smart glasses like RayBan Meta smart glasses. These devices are more lightweight and useful in tracking egocentric activity but not currently designed for older adults and especially people with memory struggles.  
Wearable camera-based systems have primarily been used as lifelogging devices for supporting retrospective memory in older adults (i.e. remembering past events or people's names), such as the SenseCam~\cite{hodges2006sensecam,dubourg2016sensecam} and Autographer plus Flo~\cite{molesworth2016evaluation} (both of which are neck-worn devices), but store images as memory, and approaches to text-based memory diaries require integration with smart homes for activity monitoring \cite{dahmen2018design}.
%Smart glasses, like Google Glass, could support older adults in checking if they performed tasks by reviewing previous captured image on the heads-up display and for providing time-based reminders~\cite{kunze2014wearable}.% Devices like lifelogging clips (e.g., Narrative Clip~\cite{farajsport}) are more likely to be worn by older adults compared to head-worn form factors (e.g., smart glasses, VR headsets, LED glasses)~\cite{schwind2020anticipated}.

Building upon literature, we present a combined wearable camera-based and voice-based system that helps older adults with object tracking and activity monitoring. To allow for both modalities, our proposed system would store text-only data as memory unlike other systems and prioritize older adult preferences for simplistic interfaces \cite{farivar2020wearable} such as voice-based and limited onboarding. %Building upon literature, we use a neck-worn instead of a head-worn form factor.

%for audio reminders + QA 
%voice interactions.

%Kidd, Cory D., et al. "The aware home: A living laboratory for ubiquitous computing research." Cooperative Buildings. Integrating Information, Organizations, and Architecture: Second International Workshop, CoBuild’99, Pittsburgh, PA, USA, October 1-2, 1999. Proceedings 2. Springer Berlin Heidelberg, 1999. [… signifying our intent to produce an environment that is capable of knowing information about itself and the whereabouts and activities of its inhabitants.]

%Meyer, Sven, and Andry Rakotonirainy. "A survey of research on context-aware homes." Proceedings of the Australasian information security workshop conference on ACSW frontiers 2003-Volume 21. 2003. [presented examples of how people could benefit from living in context-aware homes and outlined issues to keep in mind during their development. ]

%White, Carla, and James N. Gilmore. "Imagining the thoughtful home: Google Nest and logics of domestic recording." Critical Studies in Media Communication 40.1 (2023): 6-19. [The Google Nest home security system offers an array of cameras, sensors, and Internet-connected devices to allow homeowners to monitor and record the exterior and interior of their home and automate various functions of heating and cooling, lights, and other appliances through smartphone application control panels.]

%Fleck, Sven, and Wolfgang Straßer. "Smart camera based monitoring system and its application to assisted living." Proceedings of the IEEE 96.10 (2008): 1698-1714. [It covers georeferenced person tracking and activity recognition (falling person detection). A prototype system installed in a home for assisted living has been running 24/7 for several months now and shows quite promising performance.]

%Nasution, Arie Hans, and Sabu Emmanuel. "Intelligent video surveillance for monitoring older adults in home environments." 2007 IEEE 9th Workshop on Multimedia Signal Processing. IEEE, 2007. [a novel method to detect and record various posture-based events of interest in a typical older adults monitoring application in a home surveillance scenario.]

%Molesworth, Sue, and Lisa Sharrock. "An Evaluation of" Autographer plus Flo." (2016). []
%Hoisko, Jyrki. "Context triggered visual episodic memory prosthesis." Digest of Papers. Fourth International Symposium on Wearable Computers. IEEE, 2000. [Presenting visual content about a user's life, in mobile use, should help memory to recall other facts about the user's context.]

%Klebbe, R., Steinert, A., & Müller-Werdan, U. (2019). Wearables for older adults: requirements, design, and user experience. Perspectives on Wearable Enhanced Learning (WELL) Current Trends, Research, and Practice, 313-332.

%Kunze, K., Henze, N., & Kise, K. (2014, September). Wearable computing for older adults: initial insights into head-mounted display usage. In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication (pp. 83-86).

%devices like the lifelogging clip (ex. Narrative clip) are more likely to be worn by older adults compared to other form factors that could sense camera input (smart glasses, VR headsets, LED glasses) \cite{schwind2020anticipated}.

%[key gaps: safety reminders are location based, wearable, proactive, passive tracking and synced to caregivers→ helping both patient and caregiver] 

%One possible concern with wearable devices is the social and perceived acceptability depending on body-location, size, and visibility. A study published by BMC geriatrics proves that specifically lifelogging tools for the older adults are considered acceptable unless individuals are using the camera in public outside of the home \cite{gelonch2019acceptability}. The FMT and Go-finder paper also performed acceptability studies and found that individuals were more concerned with the functionality and how it could assist them rather than social acceptability. We evaluated possible form factors that would provide ease of comfort, accurate tracking, and limited interference includes glasses. Using the stereotype content model (SCM) that predicts a device's stereotypical perception and social acceptability, a study shows that devices like the lifelogging clip (ex. Narrative clip) are more likely to be worn by older adults compared to other form factors that could sense camera input (smart glasses, VR headsets, LED glasses) \cite{schwind2020anticipated}.

\subsection{Object Finding Systems}
 
There are several wearable camera systems that help users find misplaced objects but all provide visual aid either on a tablet or AR display and do not store long term memory to allow for chat-based interfaces. Audio based assistance has yet to be explored. Fiducial Marker Tracker (FMT) had a neck-worn camera and was tested with older adults; it required manual registration of objects (placing marker tags on objects) and captured videos anytime the objects are interacted with~\cite{fmt}. To recall the object's last state (e.g., light: on/off), users sorted through video footage grouped by object, which might not provide a seamless experience for object tracking. FMT also required installation and markers around the house, and marker detection was heavily dependent on user’s height (limited personalization). GoFinder also had a neck-worn camera but was registration-free and allowed users to review automatically-grouped images to find objects~\cite{gofinder} but not categorizing the name of the object. It was, however, only tested with young adults (18-28 years old) and has not yet been studied with older adults. Our proposed solution would not store any image data and provide a more seamless user experience for older adults by only voice based querying.  %It allows users to explore	video clips that capture objects of interest and can help user determine if they have completed certain actions with these objects. 
%Yagi, Takuma, et al. "GO-finder: a registration-free wearable system for assisting users in finding lost objects via hand-held object discovery." 26th International Conference on Intelligent User Interfaces. 2021.[Go-Finder]
Systems like LocatAR \cite{oshimi2023locatar} or Overthere \cite{seo2021overthere} either simplified or removed the object registration process entirely based on gesturing or user motions but required an AR headset system for continuous use posing questions about social acceptability which our solution would avoid. %was an augmented reality headset system that automatically registered objects and object movement based on user's grasping and placing motions, and then visually displayed directions to find the objects. 
%Oshimi, H., Perusquía-Hernández, M., Isoyama, N., Uchiyama, H., & Kiyokawa, K. (2023, March). LocatAR: An AR Object Search Assistance System for a Shared Space. In Proceedings of the Augmented Humans International Conference 2023 (pp. 66-76).
%The automatic registration is performed by image-based item movement recognition from the user's grasping and placing motions. Tracks item movement
%Another notable wearable system~\cite{funk2014representing} showed that displaying the last seen image of the object on head-mounted displays was more effective at helping users find objects compared to map view of the objects' location.

There are other camera-based systems, like CamFi, an AI-driven system to help find lost objects in multi-user scenarios that uses stationary camera and displays objects and last-seen users of the objects on a smartphone~\cite{yan2022camfi}. It has also only been tested with younger adults who are generally more technologically savvy \cite{olson2011diffusion}. 
Other commercially-available sensor systems, like RFID tags~\cite{lin2005object,abowd2002aware}, Ultra-Wideband (UWB) Systems ~\cite{fontana2002ultra} or Wi-Fi Fingerprint-Based Indoor Positioning ~\cite{he2015wi, nabati2023real}, can track objects but require manually tagging of these devices to the objects.

%Yan, Ge, et al. "CamFi: An AI-driven and Camera-based System for Assisting Users in Finding Lost Objects in Multi-Person Scenarios." CHI Conference on Human Factors in Computing Systems Extended Abstracts. 2022. [Lost objects found with multi-person scenario] 
%[Other commercially-available systems: Rfid tags + airtags] 
%Gofinder 
%Li, Franklin Mingzhe, et al. "FMT: A wearable camera-based object tracking memory aid for older adults." Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 3.3 (2019): 1-25. [explore whether	video clips captures from a body-worn camera every time objects	 of interest are	 found within its field of view can help older adults determine if they have completed certain actions with these objects and	 what their states are]

While LLMs have been used to identify the attributes of objects being found by a mobile robot in the FindThis system~\cite{majumdar2023findthis}, none of the works, to date, have used LLMs for helping users themselves to find the objects and enabled a voice-based chat interface for querying and response. This work presents a system which uses multi-modal LLM AI capabilities to support one of its key features of object finding designed specifically for older adults allowing them to also ask follow-up questions on the whereabouts and action of the user during misplacement. It uses visual inputs as context to the LLM so that no explicit object markers or tags are required. It stores object location information as text instead of images to be mindful of users' privacy and to enable audio descriptions of where any hand-held object is located. We also build upon Go-Finder's study design~\cite{gofinder} to test our proposed object-finding feature.


%[key gaps: system design (multimodal LLM AI tool), no explicit object markers, no images stored-> enabling audio descriptions  )], build open go-finder study design and used visual Visual condition 




%\subsection{Context-based Reminder and Tracking Systems}
%RQ2 Context based reminder systems: Context-based vs strictly time based reminders allow for flexible scheduling. 

%Retrospective memory loss (e.g., forgetting past events), affects older adults individuals with slight cognitive failures and progressively becomes worse with people with dementia and Alzheimer’s. Currently, to help older adults with this issue, speech-language pathologists have implemented ongoing diaries to practice recall. However, the task of continuous logging is quite tiresome and requires patients to accurately remember previous tasks. Additionally, prospective memory, forgetting future plans or intentions, also greatly affects older adults individuals. Clinch shows that it accounted for 72 percent of reported memory failures among their studied population of elders \cite{clinch2018learning}. 

%Most common solutions on the market for aiding prospective memory rely on time based context to trigger reminders that must be specified in advance such as pill medication alarms~\cite{kamimura2012medication, mohammed2018mobile} and stove top alerts~\cite{lin2005object} which may be useful for specific circumstances but not general purpose for a variety of custom safety reminders. Therefore, more comprehensive context-aware reminding systems are necessary to provide maximum effectiveness in ensuring general home safety in older adults~\cite{4517478} which visual contexts provide.

%The majority of these reminder systems use time as the main context like stove top alarms~\cite{lin2005object} and smart pill systems. %(proactive + time but no visual context- location + activity). 

%Visual contexts could include location contexts~\cite{beigl2000memoclip,sohn2005place} or a combination of contexts, such as enhancing home safety using location and object as contexts~\cite{lin2005object} and using time, location and users as contexts, like in Cybreminder~\cite{dey2000cybreminder}. Cybreminder's contextual architecture still requires users to manually denote whether a task is completed, which our proposed solution would passively track, and also would store memory of previous activities for assisting with retrospective memory loss.

%Reminder systems for older adults focus more on using activity as context. For example, Autominder~\cite{pollack2003autominder} uses older adults' plans and schedules to determine when to issue reminders. 
%Other systems use planned activities and current activities to deliver reminders~\cite{osmani2009human}. Camera-based~\cite{boger2006planning} and wireless sensor tag-based~\cite{si2007context} activity tracking systems have been used to remind older adults with dementia of what they had to do next to complete activities of daily living such as hand washing. Many other systems that monitor activities of older adults in home settings focus on providing alerts to caregivers and family members~\cite{stanford2002using,nasution2007intelligent,fleck2008smart} instead of reminders or alerts to older adult users at home. 
%Lin, Chi-yau, et al. "Object reminder and safety alarm." Embedded and Ubiquitous Computing–EUC 2005 Workshops: EUC 2005 Workshops: UISW, NCUS, SecUbiq, USN, and TAUES, Nagasaki, Japan, December 6-9, 2005. Proceedings. Springer Berlin Heidelberg, 2005. [“Where did I leave my keys?”, “Did I turn off the stove?”, or “Did I close all the windows in my house?” etc.]
%Dey, Anind K., and Gregory D. Abowd. "Cybreminder: A context-aware system for supporting reminders." Handheld and Ubiquitous Computing: Second International Symposium, HUC 2000 Bristol, UK, September 25–27, 2000 Proceedings 2. Springer Berlin Heidelberg, 2000. [We describe CybreMinder, a prototype context-aware tool that supports users in sending and receiving reminders that can be associated to richly described situations involving time, place and more sophisticated pieces of context]
%While there are a range of systems that track and monitor activities of older adults in home settings~\cite{abowd2002aware,buzzelli2020vision,de2017home,kidd1999aware,meyer2003survey} (as also mentioned in Section~\ref{relatedworks_assistants}), many do not focus on providing reminders using activity as a context. 

%Our system distinguishes itself by integrating camera input with advanced language models for activity recognition and delivering both real-time and proactive safety reminders for older adults. This approach leverages visual data to understand the context of location and activities and uses the same infrastructure of text-based diary for memory that enables the object finding feature. Building on prior research, our system also includes passive activity tracking and provides caregivers with timely alerts, enhancing overall safety and support.

%[key gaps: safety reminders are location based, wearable, proactive/real time, passive tracking and synced to caregivers] 

%% Potential and need for recognizing crisis: falls, broken heaters in winter -- systems need to recognise potential problems and notify home occupants and outside support. 
% Mynatt, E. D., Essa, I., & Rogers, W. (2000, November). Increasing the opportunities for aging in place. In Proceedings on the 2000 conference on Universal Usability (pp. 65-71).



%Activity-based: Aware home, sensecam

%\cite{abowd2002aware}, detecting anomalies in activities of daily living~\cite{buzzelli2020vision} and fall detection~\cite{de2017home}

%[ADDED] Kidd, Cory D., et al. "The aware home: A living laboratory for ubiquitous computing research." Cooperative Buildings. Integrating Information, Organizations, and Architecture: Second International Workshop, CoBuild’99, Pittsburgh, PA, USA, October 1-2, 1999. Proceedings 2. Springer Berlin Heidelberg, 1999. [… signifying our intent to produce an environment that is capable of knowing information about itself and the whereabouts and activities of its inhabitants.]

%[ADDED] Meyer, Sven, and Andry Rakotonirainy. "A survey of research on context-aware homes." Proceedings of the Australasian information security workshop conference on ACSW frontiers 2003-Volume 21. 2003. [presented examples of how people could benefit from living in context-aware homes and outlined issues to keep in mind during their development. ]
%%  (Mynatt, Essa and Rogers 2000) (Lines and Hone 2002).  (Stanford 2002). 


%White, Carla, and James N. Gilmore. "Imagining the thoughtful home: Google Nest and logics of domestic recording." Critical Studies in Media Communication 40.1 (2023): 6-19. [The Google Nest home security system offers an array of cameras, sensors, and Internet-connected devices to allow homeowners to monitor and record the exterior and interior of their home and automate various functions of heating and cooling, lights, and other appliances through smartphone application control panels.]

%[ADDED] Fleck, Sven, and Wolfgang Straßer. "Smart camera based monitoring system and its application to assisted living." Proceedings of the IEEE 96.10 (2008): 1698-1714. [It covers georeferenced person tracking and activity recognition (falling person detection). A prototype system installed in a home for assisted living has been running 24/7 for several months now and shows quite promising performance.]

%[ADDED] Nasution, Arie Hans, and Sabu Emmanuel. "Intelligent video surveillance for monitoring older adults in home environments." 2007 IEEE 9th Workshop on Multimedia Signal Processing. IEEE, 2007. [a novel method to detect and record various posture-based events of interest in a typical older adults monitoring application in a home surveillance scenario.]



%Add more 
%Ubicomp recording technology: Nguyen, David H., et al. "Encountering SenseCam: personal recording technologies in everyday life." Proceedings of the 11th international conference on Ubiquitous computing. 2009. [sensecam only activity tracking not reminders]

%Kidd, Cory D., et al. "The aware home: A living laboratory for ubiquitous computing research." Cooperative Buildings. Integrating Information, Organizations, and Architecture: Second International Workshop, CoBuild’99, Pittsburgh, PA, USA, October 1-2, 1999. Proceedings 2. Springer Berlin Heidelberg, 1999. [… signifying our intent to produce an environment that is capable of knowing information about itself and the whereabouts and activities of its inhabitants.]

%Meyer, Sven, and Andry Rakotonirainy. "A survey of research on context-aware homes." Proceedings of the Australasian information security workshop conference on ACSW frontiers 2003-Volume 21. 2003. [presented examples of how people could benefit from living in context-aware homes and outlined issues to keep in mind during their development. ]

%White, Carla, and James N. Gilmore. "Imagining the thoughtful home: Google Nest and logics of domestic recording." Critical Studies in Media Communication 40.1 (2023): 6-19. [The Google Nest home security system offers an array of cameras, sensors, and Internet-connected devices to allow homeowners to monitor and record the exterior and interior of their home and automate various functions of heating and cooling, lights, and other appliances through smartphone application control panels.]

%Fleck, Sven, and Wolfgang Straßer. "Smart camera based monitoring system and its application to assisted living." Proceedings of the IEEE 96.10 (2008): 1698-1714. [It covers georeferenced person tracking and activity recognition (falling person detection). A prototype system installed in a home for assisted living has been running 24/7 for several months now and shows quite promising performance.]


%Since 2002, the proposed idea of the aware-home has been an ongoing study to transform the home of an older adults individual into one that is integrated with sensors for better patient monitoring \cite{abowd2002aware}. Applications for in home camera sensors primarily include fall detection systems \cite{de2017home} or activity recognition to detect anomalies in activities of daily living \cite{buzzelli2020vision}. However, studies show that the technology-readiness level of smart-homes is low \cite{liu2016smart} and there is limited evidence on the ability to help address health related quality of life. Other limitations include the range and coverage of patient activity tracking. The extent of tracking is directly proportional to the number of cameras installed, potentially leading to incomplete activity identification. 





\iffalse

\subsection{Cognitive Assistants for the older adults}

Retrospective memory loss (e.g., forgetting past events), affects older adults individuals with slight cognitive failures and progressively becomes worse with people with dementia and Alzheimer’s. Currently, to help older adults with this issue, speech-language pathologists have implemented ongoing diaries to practice recall. However, the task of continuous logging is quite tiresome and requires patients to accurately remember previous tasks. Additionally, prospective memory, forgetting future plans or intentions, also greatly affects older adults individuals. Clinch shows that it accounted for 72 percent of reported memory failures among their studied population of elders \cite{clinch2018learning}. Current solutions on the market for aiding with prospective and retrospective memory loss include: pill medication alarms, stove top alerts, but although useful for specific circumstances these smart sensors are not general purpose. Therefore, context aware reminding systems are necessary in order to provide maximum effectiveness \cite{4517478}.   

\subsection{Early memory augmentation systems}

\subsection{Camera Based Technologies for older adults }

Remote tracking of the older adults using camera based systems within a user's home reduces caregiver burden. Prior HCI studies and products on the market aim to solve this issue via a smart connected home with camera sensors or wearable camera systems. Since 2002, the proposed idea of the aware-home has been an ongoing study to transform the home of an older adults individual into one that is integrated with sensors for better patient monitoring \cite{abowd2002aware}. Applications for in home camera sensors primarily include fall detection systems \cite{de2017home} or activity recognition to detect anomalies in activities of daily living \cite{buzzelli2020vision}. However, studies show that the technology-readiness level of smart-homes is low \cite{liu2016smart} and there is limited evidence on the ability to help address health related quality of life. Other limitations include the range and coverage of patient activity tracking. The extent of tracking is directly proportional to the number of cameras installed, potentially leading to incomplete activity identification. 

In terms of wearable camera systems, there are two notable devices that focused on object tracking and helping users find misplaced objects: FMT and Go-Finder. The fiducial marker tracker (FMT) \cite{fmt} is a wearable camera-based object tracking memory aid, that requires manual registration of objects and captures video footage anytime the object is interacted with. In order to recall the object's last state, users have to manually sort through video footage grouped by object, which does not provide a seamless experience for object tracking. GoFinder, another object finding wearable device, does not require object registration but instead requires manual lookup and image storage \cite{gofinder}. Recent wearable camera systems have become more prevalent in the market, suggesting wider spread adoption of these technologies among the public. Examples include the Humane AI clip, Narrative clip camera, and smart glasses like RayBan Meta smart glasses. These devices are more lightweight and useful in tracking egocentric activity but not currently designed for the older adults especially people with memory problems.  

\subsection{Social Acceptability of Wearable Cameras}

One possible concern with wearable devices is the social and perceived acceptability depending on body-location, size, and visibility. A study published by BMC geriatrics proves that specifically lifelogging tools for the older adults are considered acceptable unless individuals are using the camera in public outside of the home \cite{gelonch2019acceptability}. The FMT and Go-finder paper also performed acceptability studies and found that individuals were more concerned with the functionality and how it could assist them rather than social acceptability. We evaluated possible form factors that would provide ease of comfort, accurate tracking, and limited interference includes glasses. Using the stereotype content model (SCM) that predicts a device's stereotypical perception and social acceptability, a study shows that devices like the lifelogging clip (ex. Narrative clip) are more likely to be worn by older adults compared to other form factors that could sense camera input (smart glasses, VR headsets, LED glasses) \cite{schwind2020anticipated}.


\subsection{Chat based Voice Interfaces}

Relational agents, which refer to competent computer based conversationalists, have shown to be effective for the older adults population \cite{bickmore2005s}. More specifically, prior studies analyze the efficacy of voice user interfaces (VUI) for the older adults (ex. Google Home, Alexa, etc.). A systematic review addressed the perception and use of VUIs by older adults, showing benefits of these interfaces on daily lives of these individuals \cite{stigall2019older}. Guy showed that the language of voice queries is closer to natural language than typed queries \cite{guy2016searching}. These conversational assistants can be used for question answering or initiate reminders and prompts. For example, Prompto is a conversational memory coach using cognitive contexts for providing prospective memory training sessions \cite{chan2020prompto}. It finds that optimal trigger times for these voice based prompts/reminders are during lower cognitive loads and idle times, suggesting that assisting with prospective memory loss should take voice-notification timing into consideration. As MemPal aims to provide just-in-time safety reminders based on location context, we investigate the potential timing for these prompts during tasks. 

Additionally, with the integration of language models in voice assistants, users can converse more naturally with these agents, due to the advancements in language understanding. LLMs now have an increased ability to process larger unstructured text and provide a contextualized response. However, multimodal real-time voice assistants that include image and video input as context have not been explored in assisting older adults with memory conditions.

\fi

\section{Design Considerations}
% Shorter version: 

We conducted a user needs analysis through live interviews, caregiver and memory support groups (facilitated by a hospital network) and asynchronous online forums with a diverse pool of older adults, caregivers, and physicians that would provide initial design specifications for a prototype system that supports the most common memory issues among the older adult population. Transcripts and forum posts were coded independently by two researchers following thematic analysis method~\cite{braun2006using} to generate initial themes. The researchers then reviewed the coded data and themes to come up with our final themes and analysis.


% Before developing the system, we conducted a user needs analysis to understand issues hindering independent and safe living among older adults. We engaged in live interviews and anonymous online forums with a diverse group, including older adults, physicians, and caregivers. Our feedback group included four neurologists (D) and one speech pathologist (SP) experienced with neurogenic disorders, as well as caregiver support groups comprising 12 caregivers (C) and memory support groups including 12 individuals with memory decline (E). We also engaged with dementia patients (OE) and caregivers (OC) through online forums. Transcripts and forum posts were independently coded by two researchers using thematic analysis~\cite{braun2006using} to generate and finalize themes.



%Before developing the system, we conducted a user needs analysis to understand the common issues that hinder independent and safe living among older adults. These findings informed our initial design specifications of MemPal. We had live interviews and anonymous engagement in online forums across a diverse user pool consisting of older adults, physicians, and caregivers. For the live interviews, our feedback group included four neurologists (D) and one speech pathologist (SP) who have extensive experience with over 100 patients facing neurogenic disorders. Additionally, we gathered insights from caregiver support groups comprising 12 caregivers (C) where 8 were healthy older adults themselves and memory support groups including 12 individuals with memory decline (E), facilitated through a hospital network. We also engaged with individuals with memory loss (OE) and caregivers (OC) through anonymous online forums. Transcripts and forum posts were coded independently by two researchers following thematic analysis method~\cite{braun2006using} to generate initial themes. The researchers then reviewed the coded data and themes to come up with our final themes and analysis. 

%Although not a completely representative sample, these themes and user feedback helped develop initial design guidelines for MemPal. 

Full compiled themes and quotes are found in Appendix \ref{appendixinterviews}.

\subsection{Interviews}

\subsubsection{Issues} \label{designconsiderations}

\begin{itemize}
    \item \textbf{\underline{Issue 1}}: Finding misplaced objects is a time consuming issue. Caregivers listed the items that they often have trouble remembering and the emotional toll it takes to find them. These insights suggest that location detection must be accurate and objects may be left in enclosed areas (OC8-11). For example: "My [mother-in-law] MIL was always missing her purse and was sure someone had stolen it. She would ransack her room all night long. It would be in the closet. This happened often." (OC8).
    \item \textbf{\underline{Issue 2}}: \textcolor{black} {Safety hazards and concerns necessitate continuous caregiver monitoring, which is often managed through inefficient manual written reminders (D1, SP1, E6-8, C10, C12, OC1-8). For example, "We have notes everywhere, but he doesn't pay attention to any of them. Whenever he is performing any action, I just watch over him and make sure he has what he needs" (OC6).}
    \item \textbf{\underline{Issue 3}}: Confabulation and over-reliance on subjective questionnaires lead to inaccurate diagnosis of memory conditions and misconstrued memories (D1, D2, E6, E7, E8, SP1). For example, "My biggest issue with his memory is that he'll tell us he has something when he doesn't. He lies daily." (E6).
    
\end{itemize}

\subsubsection{Proposed Solutions}
\begin{itemize}
    \item \textbf{Solution 1: Object Finder} Inclusion of an object finding system that is voice activated and identifies within-home location with low latency. Overall, older adults want to feel independent and "they don’t want to go to a nursing home" (C11) so designing features that enables that is essential. 
    \item \textbf{Solution 2: Activity Diary}: We suggest the implementation of an automated version of a memory diary commonly used among older individuals with poor memory (SP1), that passively logs daily activities. Objective data from at-home activity monitoring can provide crucial insights to physicians, who are currently reliant on subjective often biased memory recollections to diagnose potential memory conditions.

    % Subjective
    % https://onlinelibrary.wiley.com/doi/full/10.1111/j.1479-8301.2011.00354.x
    % https://karger.com/dem/article-abstract/27/4/310/99175/Development-of-the-Subjective-Memory-Complaints
    % https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8579566/
    
    %\item \textbf{Solution 3: Context-Based Safety Reminders}: We propose a context-based safety reminder system with a voice interface and a caregiver app for remote tracking based on passive task completion. Additionally, proactive reminders and the ability to inquire about past tasks via a voice interface can enhance older adults' independence by reducing their reliance on constant caregiver monitoring.   
    
\end{itemize}



\subsection{Form factor}

Based on previous works, we prioritized key design considerations to enhance user acceptance. 
Neck-worn devices are more likely worn by older adults compared to other form factors (e.g., smart glasses) \cite{schwind2020anticipated} due to higher social acceptability. \textcolor{black}{Research by BMC geriatrics proves that specifically lifelogging tools for the older adults are considered acceptable unless individuals are using the camera in public outside of the home \cite{gelonch2019acceptability}. MT and Go-finder also performed acceptability studies and found that individuals were more concerned with the functionality and how it could assist them rather than social acceptability \cite{fmt} \cite{gofinder}.} However ideal form factor was not the focus of this paper as we just used existing hardware to test our methods and concept. 
    % \textbf{Device Form:} Neck-worn devices have a higher social acceptability compared to other form factors (e.g. smart glasses).
    %Using the stereotype content model (SCM) that predicts a device's stereotypical perception and social acceptability, a study shows that devices near the neck like the lifelogging clip (ex. Narrative clip) are more likely to be worn by the older adults compared to other form factors that could sense camera input (smart glasses, VR headsets, LED glasses) \cite{schwind2020anticipated} since “they usually take their glasses off” (D4). %Research by BMC geriatrics proves that specifically lifelogging tools for the older adults are considered acceptable unless individuals are using the camera in public outside of the home \cite{gelonch2019acceptability}. FMT and Go-finder also performed acceptability studies and found that individuals were more concerned with the functionality and how it could assist them rather than social acceptability \cite{fmt} \cite{gofinder}. 
    




\section{System Design and Implementation}

MemPal is a wearable, multimodal memory assistant designed to help older adults with self-reported memory decline locate misplaced objects and remember past actions within their homes through visual context and voice-based interface. The prototype system features an egocentric camera, that captures images regularly. These images are analyzed in real-time to automatically generate a diary of all activities. An audio bone-conduction headset then enables users to voice-query this activity diary. The wearable assistant is accompanied by the MemPal smartphone app for initial onboarding. System architecture illustrated in Figure \ref{fig:overall}. \textbf{ More details found in Appendix \ref{systemimp}.}


\subsection{System Onboarding} 

\begin{figure*}
    \centering
    \includegraphics[scale=0.25]{figures/location-system.pdf}
    \caption{System onboarding phase: Using the MemPal app, the user first watches an instructional demo video before beginning the home tour video walk-through. Once the video is processed, the verbally labeled locations are populated in the app.} 
    \label{fig:location-system}
   
\end{figure*}

To create a seamless and personalized setup experience of the MemPal system for older adults, the user would take a 1-5 minute house-tour video while wearing the camera, which creates a spatial representation of their home later used for room localization. This initial information provides location context critical for object retrieval. Upon entering a new area, the user verbally labels and scans each room before moving to the next. After about 2 minutes of processing, the labeled locations appear for review in the MemPal app. To accommodate different house configurations and to enable a voice interface, MemPal allows users to voice-label rooms in a personalized manner (for instance labeling the entrance as parlor) to provide a more familiar experience as opposed to auto-labeling rooms in a standard format for all users. Figure \ref{fig:location-system} displays the user flow of the system onboarding process. 

\subsection{Activity Log}

To enable a multi-purpose memory augmentation system, MemPal consists of a vision system and language system (Figure \ref{fig:overall}) to create an activity log. The vision system comprises of parallel real-time detection AI models responsible for identifying the real-time location, generating background scene descriptions, and describing both hand-held objects and the current activity of the user. \textcolor{black}{Since MemPal uses a monocular egocentric camera for real-time indoor location tracking, it avoids the need for sensors like GPS or high-compute methods like SLAM. It generates an embedding map using CLIP embeddings and a room adjacency list from a calibration video, creating a personalized spatial map of the user's home. This map provides context for real-time localization. Activity is only detected once the user's hand(s) are observed as opposed to continuous "video-on" to prevent unnecessary video processing since activity detection models are expensive (ex. user is walking through a doorway). A Vision Language Model (GPT-4V)takes an input of tiled frames, text context of the previous activity, and a prompt, and outputs activity descriptions, objects in hand, and background descriptions that are useful for dynamic, context-sensitive dialogues.} 
% Related paper for hand gestures
% https://link.springer.com/article/10.1007/s40747-023-01173-6


%Location is identified via real-time indoor location tracking using an image-to-image embedding similarity approach along with a heuristic based ensemble methods, comparing incoming frames with the spatial map representation initially created during the onboarding process.

The detection models produce text for each frame batch, which is embedded and stored in a vector database to create a time-sequenced activity log. The language system powered by an LLM agent described below understands user voice queries and responds accordingly using the vision context (user example in Figure \ref{fig:activities}).

The underlying technology architecture consisting of camera preprocessing, visual feature extraction, and LLM query/ processing are designed to be reused for multiple features besides object finding such as proactive safety reminders and retrospective recall of past actions. Figure 15 displays the usage of visual and language system to support context based safety reminders. Testing focused on the object finding feature.  

\begin{figure*}
    \centering
    \includegraphics[scale=0.27]{figures/overall.pdf}
    \caption{System implementation overview highlights the use of real-time visual context through a wearable camera for a question-answering language system using wearable audio I/O. The visual context from the camera consists of location tracking, object tracking, and activity and scene recognition to create a virtual diary (Daily Diary DB) which is used later for querying.} 
    \label{fig:overall}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.27]{figures/activities.pdf}
    \caption{An example of activity log, query log, and higher level activities extracted (from Participant 5 of user study)} 
    \label{fig:activities}
\end{figure*}

% add in figure of the 

\subsection{Object Retrieval}
%Once the location contexts are set up, the user can ask MemPal about any misplaced object without requiring explicit tagging or object registration and only using voice. The user would first activate MemPal through the wakeword: “Pal” before querying a specific object (Ex. "Pal, where are my keys" or "I can’t find my keys, Pal"). The voice query is processed to text, then categorized as an "object" query using an LLM, before extracting the last seen location of that object from the activity log to formulate an audio response in the following format: "Your [object] was last seen in the [detected room] near [background description of the area]". If the object was not seen, MemPal responds: "I’m not sure". 
%If users require additional information, they can pose follow-up questions without reiterating the object in search (e.g., "Pal, can you be more specific?" or "Pal, what was I doing right before I saw it?") implemented by storing chat memory. This design facilitates a naturalistic conversational interaction, mirroring human dialogue. Figure \ref{fig:object-system} demonstrates the user action and interaction sequence for object retrieval using MemPal. 
\subsubsection{User Flow}
Once the location contexts are set up during the onboarding process, MemPal allows users to voice-query about misplaced objects without needing explicit tagging or object registration. Users activate the system using the wakeword "Pal," followed by their query, such as "Pal, where are my keys?" or "I can’t find my keys, Pal." MemPal retrieves the last seen location of the extracted object from the activity log and responds in the format: "Your [object] was last seen in the [detected room] near [background description]." If the object is unlocated, MemPal replies with "I’m not sure." \textcolor{black}{LLMs can manage increasingly complex queries that go beyond simple object retrieval, such as "What did I do before I misplaced my glasses?" This functionality is essential to meeting the needs of users, especially those with memory impairments, who may need more than basic object finding.}

Users can also ask follow-up questions without repeating the object in search, enhancing the fluidity of the conversation (e.g., "Pal, can you be more specific?" or "Pal, what was I doing right before I saw it?") \textcolor{black}{due to an LLM's ability to process long context.} This design, supported by the system's chat memory, enables interactions that mimic natural human dialogue powered by LLMs. Figure \ref{fig:object-system} illustrates the sequence of user actions and interactions for object retrieval using MemPal.

\begin{figure*}
    \centering
    \includegraphics[scale=0.25]{figures/object-system.pdf}
    \caption{User flow for object retrieval. When the user places an object at a location in the house, MemPal stores this information which can be later retrieved during QA. The user can ask the location of the specified object as well as followup questions.}
    \label{fig:object-system}
\end{figure*} 


\begin{figure*}
    \centering
    \includegraphics[scale=0.27]{figures/object-imp.pdf}
    \caption{Object Retrieval Implementation: This workflow demonstrates the question and answer system specifically for object retrieval which uses the visual context as determined in Figure \ref{fig:visual} to respond to user queries starting with query categorization.} 
    \label{fig:object-imp}
\end{figure*}

\subsubsection{Language System Implementation}


\textcolor{black}{During object retrieval, \textit{ActivitiesDB} is filtered based on object metadata. If an exact match is found, the embeddings are chronologically sorted, and the most recent timestamp is returned. The format is: "Your [object] was last seen at [time] in the [location] near [background]." For similar objects with slightly different names, embedding similarity and retrieval-augmented generation (RAG) are used. The query is embedded using OpenAI’s Ada model (text-embedding-ada-002), and cosine similarity scores are calculated with \textit{ActivitiesDB} embeddings. The top k=10 most similar embeddings are selected, with metadata (timestamp, location, object, background) used to augment an LLM (OpenAI GPT-3) prompt for the last-seen details. If it's a follow-up question, prior chat history is included before invoking RAG. Figure \ref{fig:object-imp} illustrates the retrieval workflow.}


%\subsection{Safety Reminders}

%With safety being a critical priority for not only older adults but also their caregivers, MemPal proactively reminds the user of self-inputted safety precautions while tracking the completion of these tasks (Figure \ref{fig:safety-system}).         

%MemPal supports two functionalities for safety tasks: proactive reminders and retrospective task recall through voice queries. Proactive context-based reminders activate after a user enters a location and remains for a set duration. For example, when the user enters the kitchen and turns on the stove, the reminder "Did you remember to turn off the stove?" is triggered. These reminders can also be time-based or action based; however, during user studies, we tested the system using only location context. Retrospective functionality enables users to ask MemPal to verify if and when they completed safety reminders or other activities, as it uses an LLM agent to query the activity log in order to respond to the user. Similar to object retrieval, users can ask follow-up questions about actions performed before or after a task. Caregivers can also remotely track the last completed time and status of these reminders on the MemPal app determined by another LLM agent that outputs a binary ('yes' or 'no') response. 
%%
%%\begin{figure}[h!]
%%    \centering
%%    \includegraphics[scale=0.22]{figures/safety-system.pdf}
%%    \caption{User flow for safety reminders displaying both prospective and retrospective functionality. (Left) Reminder input in the app under specified location either via voice or text. (Center) Context based reminder via audio output (Right) Completed action synced to MemPal app. (Bottom) Retrospective recall if task was completed through question-answering. }
%%    \label{fig:safety-system}
%%\end{figure}
%%

\subsection{Prototype Apparatus and Infrastructure}

The wearable prototype system for the study consists of an iPhone held with a magnetic clasp on a neck mount and a Bluetooth connected bone conduction headset for audio input and output shown in Figure \ref{fig:object-system}. The iPhone acts as an egocentric camera device centered between the shoulders and tilted slightly at a downward angle towards the user’s hands as it streams camera footage to a server. The bone conduction headset consists of a microphone and speaker around the ear, non-interfering with hearing aids. For the first prototype we used a neck worn camera in order to keep the camera from moving but still in a position of highest FOV for activity detection. An ideal form factor would attach on the user's clothes. We envision future iterations to the components integrated into a single magnetic clip with a mic, speaker, camera, and haptic feedback for query activation.

Vector databases for location calibration and automated diary are persisted locally on-device while location trajectory is persisted on Google Firebase Realtime database to be able to share them with authorized caregivers. User queries to MemPal are also stored locally on-device. 


\section{User study}
 
We conducted a within-subjects study with 15 older adults in their own homes to evaluate the interaction, accuracy, and overall user experience of the MemPal system. %Participants walked through the location setup process and tested MemPal's object retrieval feature through a comparative study while the safety reminders and summarizer features were tested to understand user perceptions.


%\subsection{Study Design}

%To evaluate RQ1, participants engaged in an object finding task with audio assistance from MemPal and two existing methods (visual assistance and no assistance). To evaluate RQ2, participants provided subjective feedback for all features.$

%The study was divided into two parts to address the research questions. 
The study had two parts. Part 1 tested the effectiveness of a voice-enabled object retrieval feature to answer RQ1, modeled after the GoFinder study \cite{gofinder}. It had three experimental conditions and the order was counterbalanced across participants (Figure \ref{conditions}): %Three conditions were compared in Part 1—Baseline (control), MemPal (assistance), and Visual (assistance similar to GoFinder)—to assess the effects of MemPal’s system on objective performance and subjective measures, 


\begin{itemize}
    \item \textbf{Baseline}: without assistance, control condition %(current status quo). 
    \item \textbf{MemPal}: with audio assistance (verbal descriptions of object's last seen location) triggered by a voice query
    \item \textbf{Visual}: with visual assistance of last seen tiled image of object along with detected object label (a subsystem of MemPal that is used to generate audio descriptions).
\end{itemize} 

The Visual condition builds off Gofinder’s Object-based aid system (which groups objects by visual appearance) but unlike GoFinder, the objects are labelled and labels are then clustered, allowing for easier search. The images are displayed on a laptop, as older adults prefer larger screens like those of tablets over smartphones \cite{kim2022exploring} and voice descriptions were not given in this condition. We showed the participants the tiled image of the 235 degree view since a larger camera view provided more visual context background for the user to identify the location of objects. However, in future iterations we plan remove distortion during post processing to avoid confusion.


%The first condition was a \textbf{Baseline} condition where participants searched for objects without any assistance. This condition demonstrates the status quo since all participants mentioned that they don’t use any memory assistance in the form of technology to help them find objects. We also set up a third condition: Visual which is a subsystem of MemPal and builds off Gofinder’s Object-based aid system (grouping images of classified objects and ranking based on timestamp for easier search) but just with no audio output. However, GoFinder's implementation differs due to a clustering approach with no text-based object categorization whereas our visual condition categorizes by object name leading to easier search. The image during the visual condition is used by the MemPal system to generate the audio descriptions in the MemPal condition. 

Part 2 studied the experience and user perceptions of MemPal to answer RQ2. 
%Part 2 evaluated safety reminders tailored for older adults as well as viewing the Summarizer app display and receiving qualitative feedback for these three features, to answer RQ2.  


\begin{figure*}
    \raggedright
    \centering
    \includegraphics[scale=0.25]{figures/conditions.pdf}
    \caption{The user interaction for each memory assistance condition during the object search process. (Left) The MemPal condition provides audio descriptions; (Right) The Visual condition displays a tiled image of the object's last seen location.}
    \label{conditions}
\end{figure*}

\subsection{Tasks}
% part 1 and part 2
The objective measures calculated during object retrieval were meant to answer RQ1 and the subjective user experience measures determined based on experiences with object placing and retrieval were meant to answer RQ2. 
\subsubsection{Object Finding}
\textbf{Object Placing:} We collected 20 objects (Figure \ref{fig:objects}) which resembled commonly misplaced objects based on survey results of recruited participants (Appendix \ref{fig:demographics} and user research, Appendix \ref{appendixinterviews}). Occasionally, an object was replaced with an item from the user to enhance personalization. %Prior to the study we surveyed participants as well as online community forums of dementia patients for the most commonly misplaced items and among the following included: glasses, purse, phone, hearing aids, keys, coffee mug, and paperwork. 
Participants were first asked to wear the MemPal camera system and place all 20 objects in various locations around their house (for our system to auto-register the locations of objects) without any explicit labeling. Half were hidden, half were visible, and distributed evenly across rooms to avoid overcrowding (following the  GoFinder study \cite{gofinder}). No markers were used to indicate specific locations, unlike in GoFinder \cite{gofinder}, as our study was conducted in-the-wild. This setup was designed to mimic real-life scenarios of the participant themselves misplacing commonly used items in non-obvious places and to increase recall difficulty as much as possible given the study's short duration.


\begin{figure*}
    \raggedright
    \centering
    \includegraphics[scale=0.25]{figures/objects.pdf}
    \caption{Object set used during the study for each participant are shown below: folder, cup, phone, bottle, medication, glasses, headphones, book, charger, remote, ID card, ring, wallet, watch, magnifying glass, tape, scissors, ruler, mouse, keys.}
    \label{fig:objects}
\end{figure*}

%\subsubsection{Object Retrieval}
\textbf{Object Retrieval:} 40 minutes after the object placing task, participants were asked to retrieve 20 objects from randomized conditions (6-7 objects each). The researcher specified the target object, and participants had up to 3 minutes to search, simulating a hurried search as in previous studies~\cite{gofinder}.

% Unedited Version: After 40 minutes from the object placing task, participants were asked to retrieve all 20 objects starting and ending at the same position. Objects were randomized across the conditions, with each condition featuring 6-7 objects for retrieval. The researcher indicated to the participant which object to find and participants had a maximum of 3 minutes to search (to simulate the hurried nature of object searching, and following previous studies~\cite{gofinder}). 

%%\subsubsection{Safety Reminders}\label{tasks}

%The safety reminder system testing functioned as a "forgetting phase" to help participants lose track of where objects were placed before the object retrieval task. 
% Participants first freely entered their own safety reminders into the MemPal app to test the user experience. Then, they 

%Participants were asked to enter a standard set of three specific reminders into the MemPal app, chosen for their relevance to older adults based on initial interviews (Appendix \ref{appendixinterviews}). These reminders (i.e. washing hands, turning off the stove, or taking medication) were proactively triggered when participants entered the location it was placed under ("kitchen"). Then, both participants and caregivers would check task completion in the app, and participants would use the app's retrospective function to confirm task completion.

%both participants and their caregivers were to check task completion directly in the app. Participants also used the app's retrospective functionality to inquire if they had completed these tasks. 

% version 2:  The testing of the safety reminder system served as the "forgetting phase" before the object retrieval task to help participants forget where objects were placed. Participants were asked to input a set of 3 reminders in the MemPal app which then proactively prompted them to perform the reminders upon entering the specified location. The reminder set was also chosen based on initial interviews (Appendix \ref{appendixinterviews}), to add more relevance to older adults for the study. Reminders that were tested included (1) Washing hands (2) Turning off stove (3) Taking medication. Participants also could test the retrospective functionality by asking the system whether they completed the safety precaution. The participant and/or their caregiver then viewed the app to see the completion of the safety reminder at the time of action. 



\subsection{Measures}

\subsubsection{Technical Evaluation}

%For the MemPal condition, 
We annotated whether the audio response from MemPal accurately represented each object's last seen location.
% A similar annotation was conducted for the tiled images during the Visual condition. 
Audio responses were categorized in the following (1) Correct: if the detected location and scene description of the background accurately reflected the object's location, (2) Incorrect Location: if the object was correctly categorized in the last seen timestamp so a correct description was generated but the Indoor location algorithm detected the incorrect location, (3) Object misidentified: If the object was correctly identified at some timestamp but misidentified at t-1 timestamp, (4) No object detected: if the object was not contained in the ActivitiesDB log metadata. For the Visual condition, a similar analysis was performed which evaluated the accuracy of solely the VLM system so camera image responses were categorized into the same (3) and (4) categories.   
%Camera positioning of whether the user placed the object in front of the frame before placing the object down would affect cases (2) and (4). Additionally occlusion of the object would affect the object detection subcomponent cases (3) or (4). 
Data for each participant's app screen, activity logs, and images were stored for manual review by two researchers post-study. Additionally, latency of responses were calculated from start of query to audio query response. Individual system component latency was also calculated for camera pre-processing, real-time location algorithm, VLM (GPT4-V) processing, and total time for each batch of streaming frames. The evaluation was conducted by two researchers who were independent of the data collection and blinded to the conditions.

\subsubsection{Objective Evaluation Measures}

 \begin{itemize}
     \item \textbf{Retrieval Accuracy:} We determined whether the user correctly found the object within the 3 minutes of search and then calculated a percentage of total objects found/ total objects within each condition to determine an accuracy metric.
     \item  \textbf{Path Length:} Measured as the number of rooms an individual searched (based on the calibrated locations) before finding an object within the 3 minutes. For each condition, we averaged the path length across all objects which included those that the user already remembered. 
 \end{itemize}
 

 % We hypothesized that path length would decrease and retrieval accuracy would increase with assistance. 

In the MemPal condition, we included only the data points where MemPal accurately delivered the correct audio response for the specified object. We excluded inaccurate responses as they caused participants to revert to the baseline condition, thereby minimizing accuracy as a confounding factor in our evaluation of the objective metrics.
Similarly, for the Visual condition, we retained only data points where the visual image produced by MemPal correctly depicted the last observed location of the object. Furthermore, we excluded instances where participants did not rely on the image for retrieval, ensuring that the analysis accurately reflects the assistance's effect on the objective metrics. We analyzed results both excluding and including these data points.

\subsubsection{Subjective Evaluation Measures}

After each condition, we measured self-perceived task load (using Raw NASA-TLX), confidence of retrieval (7-point Likert scale), and recall difficulty (7-point Likert scale) through written questionnaires as seen in Appendix \ref{appendixSelfReportedConfidence} (following the evaluation metrics in Memoro, memory augmentation device study~\cite{zulfikar2024memoro}). %Since there is significant positive correlation between memory performance and self-efficacy, we designed the questions to address RQ2.

After all conditions were completed for object finding, we measured the user experience and long-term potential use of just the MemPal condition with questions rated on a 7-point Likert scale. To evaluate the perceived helpfulness of MemPal’s audio descriptions versus visual images, we measured reliance on one type of assistance versus the other as well as overall usefulness of the camera feed both on a 7-point Likert scale. Full questionnaires are in the Appendix \ref{appendixPostObjectRetrieval}. Open-ended written feedback on overall thoughts and improvements were collected. %We asked participants two written questions of overall feedback on the specific feature regarding overall thoughts and areas of improvement.% which we then analyzed and coded as described in Section~\ref{sec_feedback}. %in order to determine how necessary camera-based images are in order to find lost objects quickly

%In addition, measures related to long-term potential use based on personal needs were evaluated with the following categories: likeliness to use and trust of the system’s responses.
    

 
%\subsubsection{Safety Reminders}
%\setcounter{secnumdepth}{4}
%\paragraph{\textbf{Technical evaluation of feature}}

%We assessed the accuracy of activity detection for safety reminder tasks based on the information displayed in the MemPal app. Accuracy for each tracked reminder was categorized into (1) Correct: MemPal app displayed the correct time and a checkmark for the completed task. (2) Incorrect activity detected: The activity was detected in ActivitiesDB, but the safety reminder LLM agent could not discern it from the text-based log. (2) No activity detected: The completion of the reminder was not found in ActivitiesDB. Data for each participant's app screen, activity logs, and images were stored for manual review by two researchers post-study.

%\setcounter{secnumdepth}{4}
%\paragraph{\textbf{Subjective Evaluation Measures}}
%We measured self-perceived user experience and usefulness of the safety reminder feature which included reminder input through the app and receiving audio-based reminders through questions rated on a 7-point Likert scale as seen in Appendix \ref{appendixPostSafety}. Open-ended written feedback on overall thoughts, improvements, and preferred format of safety reminders were collected. %also was coded and analyzed as described in Section~\ref{sec_feedback}. 

 

\subsubsection{Overall System Usability and Experience} After the study was complete, we asked participants to complete a system usability scale assessment (SUS) \cite{brooke1996sus} and rate the overall usefulness of the entire MemPal system in their everyday lives using a 7-point Likert scale. Then, we interviewed participants on their experience with the system, overall feedback and thoughts on privacy and data sharing. Full list of questions is in Appendix \ref{appendixOpenEndedInterview}. %Feedback was audio recorded and coded independently by two researchers following Thematic analysis method~\cite{braun2006using}. Researchers then reviewed the coded data and themes to come up with final themes and analysis. 


\subsection{Procedure}
The study was conducted in the older adults' homes within one floor and took about 2.5 hours (Figure \ref{procedure}). Demographic information was collected in the pre-screening questionnaire and follow-up emails. At the start of the study, a Mini Mental State Examination (MMSE) screening \cite{cockrell2002mini} was administered to assess baseline cognitive level. % since participants self-reported their memory condition (normal, subjective cognitive decline (SCD), MCI, Dementia) on the online screening questionnaire.
Then, participants were asked to use the MemPal app to walk through the setup phase which included reading instructions, watching an example video, and taking a video of their home while verbally labeling certain rooms as described in Figure \ref{fig:location-system}. Following that, participants performed the object placing task. Next, they took a 30 minute break to experiment with some other prototyped features like safety reminders and a summmarizer without formal testing. After another 10-minute break (which is about 40 minutes after the object placing task), participants retrieved the objects within the 3 conditions: (1) Baseline, (2) MemPal, (3) Visual. Lastly, we asked participants about their general feedback and concerns on the overall system through an interview.  
%The sequence of conditions were counterbalanced across participants and hidden/ non hidden objects were split equally between the conditions. 

\begin{figure*}
    \raggedright
    \includegraphics[scale=0.23]{figures/procedure.pdf}
    \caption{Procedure flow of the 2.5 hour study within each participant's home. All surveys took around 5 minutes.}
    \label{procedure}
\end{figure*}

\subsection{Participants}
Participants were recruited through local hospital-led memory support groups, email lists, caregiver support groups, social media, and partnering with senior communities. There were 15 participants ($9$ female, $6$ male, age range = $62$ to $96$, $M$=$74.9$, $SD$=$8.6$). Participants were fluent or native English speakers with normal or corrected-to-normal hearing and vision. All participants lived in either apartments or homes (8 in homes, 7 in apartments) with an average of $5.13$ rooms. Appendix \ref{fig:roomfreq} shows distribution of rooms across participant homes. Participants were asked about their cognitive ability and varied between normal cognitive ability (n=8), subjective cognitive decline (SCD) (n=4), and Mild Cognitive Impairment (MCI) (n=3). Additionally, participants rated the frequency of various actions, including: misplacing objects, forgetting to bring intended items, forgetting intended actions, forgetting errands, forgetting medications, and forgetting intended purchasing. The study received ethics approval from the university ethics review board, and participants gave written consent to participate with knowledge of what data was being stored and how it was processed. Participants received 45 USD worth of compensation for completing the study. Participant demographic data can be viewed in Appendix \ref{tab:demographics}. 

%Additionally, we also asked participants to list the most frequently misplaced items (Appendix). %\ref{fig:demographics}). 

\section{Results}
We analyzed the results from the user study to evaluate the overall system’s technical accuracy within different real-life settings, objective effects on object retrieval, and subjective user perceptions and experience for the overall system.


\subsection{\textbf{Technical Evaluation of System}} 
\label{sec_technical}
As part of the user study, we conducted a technical evaluation of the Object Finding system to understand the performance within different environments and in real world conditions. 

\subsubsection{Object Finding:}
%During the MemPal condition, we annotated whether the audio response from MemPal accurately represented the each object's last seen location. A similar annotation was conducted for the tiled images during the Visual condition. For MemPal, audio responses were categorized in the following (1) Correct: if the detected location and scene description of the background accurately reflected the object's location (2) Incorrect Location: if the object was correctly categorized in the last seen timestamp so a correct description was generated but the Indoor location algorithm detected the incorrect location (3) Object misidentified: If the object was correctly identified at some timestamp but misidentified at  t-1 timestamp (4) No object detected: if the object was not contained in the ActivitiesDB log metadata. Camera positioning of whether the user placed the object in front of the frame before placing the object down would affect cases (2) and (4). Additionally occlusion of the object would affect the object detection subcomponent cases (3) or (4). During the Visual condition, a similar analysis was performed which evaluates the accuracy of solely the VLM system so camera image responses were categorized into the same (3) and (4) categories.  

The results in Table~\ref{tab:objectacc} include analyses from all objects that were retrieved during the MemPal and Visual conditions. Count refers to all trials within each category and total count refers to responses within each condition. Since Visual is a subset of the MemPal condition, there were 145 trials that evaluated the accuracy of visual images and 92 trials which evaluated the accuracy of audio responses based on those images. MemPal's accuracy of audio descriptions was $72\%$  and Visual images was $53\%$. Inaccurate responses were mainly due to misidentified objects of $24\%$, which suggests more fine grained object detection. The location algorithm given the initial calibration video performed correctly $78\%$ of the time.  

\input{tables/table_objectacc}


%\subsubsection{Safety Reminders:} Table \ref{tab:safetyacc} shows the results for safety reminder accuracy. Based on the 3 reminders during the study, our system had an accuracy of $85\%$ where inaccuracies resulted mainly from the activity not being detected (usually due to camera positioning). Participants excluded from the safety reminder technical evaluation include P1-P4 due to app malfunctions during the task. 

\input{tables/table_safetyacc}

\subsubsection{Latency:} We evaluated the latency for the language system and vision system separately. Latency for the language system was calculated by the query response time (time of audio output) - query start time (time of audio input). From 204 interactions across all user studies the mean of query-response time was $2.17s$ and SD of $0.68s$. The vision system latency was calculated by first evaluating latency for each individual component and total processing time of each query to then be inserted in the database (Table \ref{tab:processtime}). 

\input{tables/table_processing_time}

\subsection{Object Retrieval}
% it didn't work just for two people 

%% we didn't see differences in measures 
% limitations and discussions
% we think that it's because 

% accuracy of audio descriptions 
% we want to look at the ones that are accurate 
% in cases that it worked well, path length was....
%% including all the data and how much of the data were excluded 

% when the system did not give and 
% The accuracy of the system depends on the user’s home configuration and could drastically affect users' opinions on the feature, so we excluded participants $P3$, $P13$ for which the accuracy of MemPal was below $40\% $ when determining the SUS score.  Accuracy is calculated as percent of correct responses for the object finding task during the MemPal condition.

%\subsubsection{Objective}

Based on Section~\ref{sec_technical}, $28\%$ of data from the MemPal condition and $47\%$ from the Visual condition were inaccurate and thus, excluded from analysis to accurately assess the effects of assistance modes on the objective measures: retrieval accuracy and path length. We report the results for this set of data and any differences in results with all data (regardless of accuracy). 

\subsubsection{Retrieval Accuracy higher with MemPal than Baseline} According to the Shapiro-Wilk test, the retrieval accuracy was not normally distributed ($p$<$.05$). A Friedman test showed a significant difference in retrieval accuracy between conditions ($X^2$=$14.8$, $p$<$.001$). %p=.000624$
A post-hoc analysis using Wilcoxon signed-rank tests after Bonferroni correction showed that users had significantly higher retrieval accuracy with MemPal than Baseline ($p$=$.015$) but no significant differences for the other condition pairs: Baseline-Visual ($p$=$.087$), MemPal-Visual ($p$=$1.03$). Baseline ($M$=$.81$, $SD$=$.18$), MemPal ($M$=$.97$, $SD$=$.07$), Visual ($M$=$.95$, $SD$=$.11$). When including all data regardless of accuracy, differences between Baseline and MemPal loses significance ($p$=$.054$), and difference between Baseline-Visual is significant ($p$=$.02$). 

\subsubsection{Path Length lower with MemPal and Visual than Baseline:} Path length did not meet the normality assumption ($p$<$.05$). A Friedman test showed statistical differences between conditions for this measure ($X^2$=$17.7$, $p$<$.001$). 
%p=0.000143$
A post-hoc analysis using a Wilcoxon signed-rank test after Bonferroni correction showed that the path length for MemPal was significantly lower than Baseline ($p$=$.014$), and path length for Visual was significantly lower than Baseline ($p$=$.0066$). There was no significant difference between MemPal-Visual ($p$=$2.053$). Figure \ref{fig:objective} consists of the bar charts and mean of these objective metrics for each condition. Baseline ($M$=$1.93$, $SD$=$.69$), MemPal ($M$=$1.10$, $SD$=$.33$), Visual ($M$=$1.09$, $SD$=$.19$). When including all data regardless of accuracy, the same condition pairs remain significant ($p$<$.05$) with slight adjustments to the level of significance: Baseline-MemPal ($p$=$.007$) and Baseline-Visual ($p$=$.01$). 

%The results above were calculated looking only at trials where MemPal gave accurate information (so 26 trials out of 99 were excluded for the MemPal condition and 25 out of 53 trials were excluded for the Visual condition). When we included all trials, the differences in retrieval accuracy between Baseline and MemPal was no longer significant and instead difference between Baseline and Visual gained significance ($p$=$.02$). Path length significance remained the same.   


\begin{figure*}
    \centering
    \includegraphics[scale=0.2]{charts/objective.pdf}
    \caption{Retrieval Accuracy and Path length. $*$: p<.05, $**$: p<.001}
    \label{fig:objective}
   
\end{figure*}

\subsubsection{No difference in Task Load between conditions} A repeated measures ANOVA ($p$=$.031$) showed significant differences in task load (RTLX) scores between conditions. A post-hoc analysis using pairwise t-tests with Bonferroni correction indicated that no condition pairs were significant: Baseline-MemPal ($t$=$1.47$, $p$=$0.486$) Baseline-Visual ($t$=$2.44$, $p$=$.087$)  MemPal-Visual ($t$=$1.60$, $p$=$0.398$). Baseline ($M$=$56.1$, $SD$=$18.3$), MemPal ($M$=$50.5$, $SD$=$17.8$), Visual ($M$=$44.4$, $SD$=$13.4$).

\subsubsection{No difference in rated Confidence between conditions} There was a significant main effect in the confidence of finding objects between conditions from a Friedman test ($X^2$=$8.84$, $p$=$0.012$). However, a post hoc Wilcoxon signed rank analysis after Bonferroni correction determined no condition pairs were significant: Baseline-MemPal ($X^2$=$26.0$, $p$=$0.91$) Baseline-Visual  ($X^2$=$13.0$, $p$=$0.06$)  MemPal-Visual $(X^2$=$2.0$, $p$=$0.212$). Baseline ($M$=$4.87$, $SD$=$1.19$), MemPal ($M$=$5.27$, $SD$=$1.53$), Visual ($M$=$5.87$, $SD$=$0.92$).


\subsubsection{Recall Difficulty lower with MemPal and Visual than Baseline} A Friedman test showed significant differences in the recall difficulty of finding objects between conditions ($X^2$=$15.3$, $p$=$.0005$, Figure~\ref{fig:subjective}. A post hoc Wilcoxon signed rank analysis after Bonferroni correction showed that the recall difficulty for both MemPal ($p$=$.022$) and Visual ($p$=$.0064$) was significantly lower than Baseline. %, Baseline-MemPal ($p$=$0.022$), Baseline-Visual ($p$=$0.0064$). 
However, recall difficulty for MemPal was not significantly different than Visual ($p$=$0.119$). Baseline ($M$=$3.93$, $SD$=$1.67$), MemPal ($M$=$2.87$, $SD$=$1.81$), Visual ($M$=$2.07$, $SD$=$1.10$).

\begin{figure*}
    \centering
    \includegraphics[scale=0.23]{charts/subjective.pdf}
    \caption{Raw NASA TLX for task load [0-100], confidence [1-7], difficulty to recall [1-7].  $*$: p<.05, $**$: p<.001}
    \label{fig:subjective}
   
\end{figure*}

\subsection{User Experience for MemPal}
We analyzed overall user experience as shown in Figure \ref{fig:features}. 

\begin{figure*}
    \centering
    \includegraphics[scale=0.25]{charts/features.pdf}
    \caption{Categorized user perception and experience levels using [1-7] Likert scale.}
    \label{fig:features}
   
\end{figure*}

For the Object Retrieval feature, just using MemPal’s audio descriptions, we found the mean scores across the categories ranked highest to lowest: Helpfulness ($M$=$5.20$, $SD$=$1.66$), Ease of use ($M$=$5.07$, $SD$=$1.87$), Usage propensity ($M$=$4.67$, $SD$=$2.02$), Response satisfaction ($M$=$4.86$, $SD$=$1.61$), Appropriate Response Length ($M$=$4.87$, $SD$=$1.36$), and Trust ($M$=$4.73$, $SD$=$1.83$). Additionally, we found the mean scores for usefulness of the Visual condition ($M$=$5.57$, $SD$=$0.97$) and reliance on Visual vs MemPal ($M$=$4.93$, $SD$=$1.49$). We found weak correlations between users’ MMSE results and their other perceptions of the system (Helpfulness r = -0.171, Ease of use  r = -0.356, Usage propensity r = -0.0442, Response satisfaction r = 0.0802, Appropriate Response Length r = -0.136, Trust r = 0.0235, usefulness r = 0.252) using Pearson's correlation tests. 

%\subsubsection{Safety Reminder:} For the Safety Reminder feature, we similarly evaluated the mean scores across the categories highest to lowest: Timing Satisfaction ($M$=$6.27$, $SD$=$1.03$), Helpfulness ($M$=$6.13$, $SD$=$1.25$), Usage propensity ($M$=$5.60$, $SD$=$2.20$), and Ease of use ($M$=$5.33$, $SD$=$2.13$).



%\subsubsection{Feature differences for helpfulness and usage propensity:} Lastly, we evaluated the difference of helpfulness between all features to determine if one or more features were more helpful than others (Figure \ref{fig:features-compare}). We conducted a Friedman test and found no significant differences ($X^2$=$4.68$, $p$=$.0962$): Object: ($M$=$5.20$, $SD$=$1.66$), Safety: ($M$=$6.13$, $SD$=$1.25$), Summarizer: ($M$=$5.53$, $SD$=$1.25$). Similarly for the usage propensity across all features, we found a significant difference across categories ($p$=$0.0064$). A post hoc analysis using Wilcoxon signed-rank test corrected by Bonferroni determined that there was a significant difference for Object-Safety ($p$=$.0177$) and Object-Summarizer ($p$=$0.0447$), but not for Safety-Summarizer ($p$=$2.18$). Object: ($M$=$4.67$, $SD$=$2.02$), Safety: ($M$=$5.60$, $SD$=$2.20$), Summarizer: ($M$=$5.73$, $SD$=$1.44$).

%\begin{figure}[h!]
   % \centering
   % \includegraphics[scale=0.25]%%%{charts/features-compare.pdf}
  %  \caption{Helpfulness and Usage propensity across all features. (Left-blue): Object Retrieval (Middle-Green): Safety, (Right-yellow): Summarizer. $*$: p<.05}
 %   \label{fig:features-compare}
%%\end{figure}

\subsection{System Usability}
MemPal’s overall system had a mean system usability score (SUS) of $69.375$ ($SD=17.3$) and overall usefulness in everyday life had a mean value of $6.0$ ($SD=1.51$). Following Go-Finder's study report on usability~\cite{gofinder}, we summarize each participant's SUS scores, age and average accuracy of MemPal in Table~\ref{tab:sus}. We did not have SUS scores for P3, P9 and P13 as they thought that the system was too early in development and opted to not complete the scale. A lower accuracy of MemPal as well as higher MMSE scores might have affected the overall SUS (usability) score. We found a moderate negative Pearson’s correlation between MMSE results and SUS scores (r = -0.6064, p = 0.04795). This indicates that participants who scored higher in MMSE (better cognitive function) tended to give the system lower SUS scores. 


% \begin{table}[h!]
%   \centering
%   \input{tables/tables_sus}
%   \caption{SUS}
% \end{table}

\input{tables/tables_sus}

\textcolor{black}{\subsection{Qualitative Feedback}}
\label{sec_feedback}

Open feedback and interview transcripts were coded independently by two researchers following thematic analysis method~\cite{braun2006using} to
generate initial themes. The researchers then reviewed the coded data and themes to form our final analysis and themes.  


% instead of paragraph form should we do bullet point and describe limitations and themes in bullets? 
\subsubsection{\textbf{Positive System Feedback}}
Several participants reacted positively to the system (P11, P14, P7, P13): ``I think this is so phenomenal and easy to use'' (P14) and ``at some point everyone is going to need a MemPal'' (P11). Participants thought the system onboarding phase was ``ok'' (P1, P7, P9, P10) and P10 liked that he could create ``personal labels'' of locations in his home. Participants felt that the object retrieval system was `helpful'' (P7, P10, P11, P14, P8) and P4 remarked that it was a``great feature''. P15 commented that ``It's really amazing that it can do that, that the system can know where my objects are''. A few also mentioned that just wearing MemPal would increase awareness of placing objects (P6, P2, P13): ``A new system makes you more conscious about hiding things'' (P6). Additionally, participants preempted potential other positive impacts of the system like ``learning your behavior and help you have better habits''(P2) and that it could ``give suggestion to reduce forgetting objects'' (P14). 

\subsubsection{\textbf{Improve wearable device comfort}} Although MemPal was not designed for optimal comfort, participants mentioned that the camera was bulky and not as lightweight (P5, P7, P10, P14, P12) but still thought the current``pendant idea is nice'' (P5) and bone conduction headset '' did not interfere with hearing aid '' (P5), and glasses can be confusing'' (P14).
\subsubsection{\textbf{Improve accuracy}} (1) Speech Recognition: Participants mentioned that speech recognition could be improved especially for the 'Pal' wake-word detection (P7, P9, P13) sometimes queries "taking multiple tries" (P8) especially for those with "poor pronunciation" (P11). (2) Insufficient audio descriptions: Many preferred the descriptions to be more specific but still succinct, such as ``knowing which drawer'' (P14), being able to ``identify colors'' (P1, P4), differentiate between similar objects, hidden versus not hidden objects, and ``height from the ground'' (P3) which the current system couldn't provide. However, P14 said ``There's a fine balance between how detailed it should be (P14)``. (3) Inaccurate Responses: A few participants felt that MemPal was still an ``early stage development'' (P11) and ``not accurate enough yet'' (P13). This also made it ``too hard to tell'' how useful the system could be (P3).
\subsubsection{\textbf{Improve system learning curve}}: Participants (P6, P7, P4) also felt like they needed ``to get used to the device'' since ``everything is so new'' (P6), specifically for object retrieval where P7 felt the ``need to be instructed for how to place things'' so that the object is in sight of the camera and the system onboarding phase where they would need ``more instruction and demonstration'' (P11, P14) or potentially ``would need a caregiver to do it'' (P8).
\subsubsection{\textbf{Optionality in visual vs audio assistance}}:Participants found that having the option of using MemPal or Visual or both would to be most useful (P2, P3, P9, P11, P14, P15) since both modalities offered their own benefits and drawbacks. %even ``push a button for a picture (would like a picture on tablet), push button for voice/ description.'' 
For the visual mode, users noted that "fisheye lenses are difficult to understand" (P3), suggesting that the lens distortion may make it harder for them to match the view to their real-world perception of the space in their home. Additionally, visuals provide more information than a short audio description could provide like ''[capturing] exactly which drawer'' the object was at (P14). For the audio mode, users commented that the assistance is ``less helpful when relying only on verbal description. Pictures are also helpful'' (P2) even though verbal descriptions provided the better speed needed for object finding: ``speed is as important as accuracy” (P14) and users might ``sometimes forget the name of the object you are looking for'' (P7). 


%\subsubsection{\textbf{Feature 2: Safety Reminders}}
%\textbf{Positive Feedback} 
%The proactive safety reminders also seemed ``helpful'' (P2-5, P7, P10, P12, P14, P15). P4 felt that it could ``keep my memory to not forget'' and P8 thought that it could ``make me more aware'' of safety tasks. P2 remarked that this is the case especially for ``frequently done tasks like taking medications or turning off stove''. Additionally a large range of participants set reminders that ranged from ``blow out the candle'' (P15), ``use grabber thing to pick up things that are high up'' (P13), ``make sure no spoiled food is in fridge'' (P8), ``remind person to write stuff down'' (P8).  

%\textbf{Suggestions for Improvement}

%(1) Modality for Reminder Input: 
%Participants (P2 and P15) mentioned that voice input for reminders would be most useful especially since they ''would want to do it myself'' (P2): ``Typing is difficult for old people especially with poor vision, [you] can do this with voice recording'' (P2).
%(2) Format of Reminders and User Interaction: Preferences of reminders also varied across participants in terms of frequency: ``alerting people more than once'' (P2), allowing verbal prompting and a chance at ``closing the loop to say the action is done'' (P2, P11), and inclusion of images: ``How do you get the person to do what they should do with language, would images help?'' (P9). P14 preferred a haptic modality, commenting that ``haptic notifications would be helpful''. Various formats and contexts may be important: three participants preferred location-based safety reminders, three preferred reminders for future intended actions and three liked to know the time at which they should perform the action.



\subsubsection{\textbf{Privacy and Data Sharing}} 
 
Most participants were comfortable with their data about their 'activity log' being shared with their caregiver(s) and physician(s) (P2, P4-P10, P12-P15). Participants thought that it was ``necessary'' (P5, P15), P10 remarked that it was about weighing ``privacy versus safety issues'' and P14 felt that ``privacy is a thing of the past''. P15 mentioned ``when you go to your doctor, you sort of like aren’t quite sure about it so you want the data and they really know'' and P14 said ``definitely want the caregiver to know just [when] like you have a child''. However, a few said it depends on various factors (P1, P3, P11): ``I wouldn’t want to have constant monitoring. It just depends on level of cognition and degree of supervision. If my husband was living alone then constant supervision is needed'' (P1), and ``maybe in the future with doctors or caretakers but now [I] only [want it] personal'' (P3). Additionally participants felt that body worn cameras were less intrusive than smart home cameras (P1,P14).      



\section{Discussion} 

\subsection{MemPal Helpfulness for Object Finding} 
    
Regarding RQ1: ``What are the effects of using the MemPal voice-based system for object retrieval on retrieval accuracy, path length, cognitive task load, retrieval confidence and recall difficulty for older adults, compared to using visual cues or no system?'', showed that MemPal's audio descriptions improve object retrieval performance compared to using no system by increasing the rate of correct object identification (retrieval accuracy) and reducing the average number of rooms searched (path length) as well reduced perceived difficulty in remembering object locations (recall difficulty). Both assistance modes supported users in similar ways, suggesting that participants could retrieve objects effectively using either visual or audio cues. %Moreover, the addition of MemPal's last-seen object images also decreased path length. 
The visual condition decreased path length and recall difficulty compared to using no system. This is consistent with the results from the GoFinder study which also showed retrieval performance improvements with their visual condition compared to no system~\cite{gofinder}. %Participants rated it less difficult to recall where objects were with assistance (both with Mempal Audio and Visual).
We note that the results discussed are based on our analysis that included trials where participants used the correct feedback from each of the assistance modes. Reduced path length, a proxy for time spent searching, suggests that specifying the room helps significantly narrow the search, a benefit derived from the room localization method but as noted in Qualitative Feedback, descriptive context is most valuable either with better descriptions or high quality wide-angle images.   In addition, there were no significant differences in path length nor recall difficulty between the two assistance modes, indicating that participants performed similarly for visual and audio.

Task load and retrieval confidence did not have significant differences across conditions. The lack of difference in task load may stem from speech recognition issues in the MemPal condition and difficulties in object localization within the visual images in the Visual condition, which could be blurry or distorted by fish-eye lenses as noted in participants' Qualitative Feedback. Despite these issues, participants valued the optional visual aid for its clarity in providing room-specific details and some found it useful ($M$=$5.57$ of 7). Similar to findings from previous works~\cite{liu2023older}, solely using visual aids may not be ideal for older adult users who prefer voice-driven systems. Moreover, visual aids raise concerns about privacy and increased storage needs which MemPal aimed to avoid. Therefore, users who have vocal deterioration or have trouble with language like P9 and P11 may prefer visual images and users who have poor eyesight or do not like interacting with tablet devices may prefer audio output. Future work can include diving into preferences and perceptions of older adult users regarding using an optimal combination of both audio and visual aids in object retrieval tasks to tailor more user-friendly assistance technologies. 

%We validated that with MemPal’s memory assistance through audio, participants had an increase in retrieval accuracy and decrease in path length. Notably, participants had a significant decrease in path length with the audio cues and no decrease with visual assistance. However, subjective metrics only indicate that participants had decreased task load and reduced difficulty to recall when viewing the images generated by the MemPal system and no significant effects when hearing the verbal descriptions. Additionally, there were no significant differences across objective or subjective measures between the MemPal and Visual conditions, indicating that both modalities help participants objectively perform similarly in the task and do not feel more confident about one modality vs the other. Since MemPal's verbal descriptions provided the room and detailed background descriptions given previous contexts and the images provided are just using the frame of view of the camera, we presume that the verbal descriptions can provide a more helpful response for retrieving objects. However, as supported by the qualitative interviews, participants felt less stressed and comfortable when viewing the images. 

%Visual images effectively pinpoint an object's location, and tools like GoFinder have demonstrated their ability to reduce mental strain during search tasks  \cite{gofinder}. However, using GoFinder involves interacting with a smartphone app, which can extend the search duration and decrease usability for older adults users who favor voice-driven queries \cite{liu2023older}. Furthermore, behavioral observation during studies reveal that older adults participants with memory decline often prefer to locate objects independently or receive hints rather than exact locations, as this can help train their memory. Initially, our goal was to evaluate whether a voice-chat interface that delivers audio descriptions could adequately assist users in finding objects. Our findings indicate that visual visual images help participants find objects with less searching and feel more confident. However, audio cues assist participants in the same way but because of the novelty of the assistance mode especially for visual learners they may feel more comfortable and less stressed with the visual assistance. In order to be privacy preserving, the MemPal system does not require any photos stored but the visual assistnace mode would require camera storage. Consequently, future iterations of the system, participants can maybe choose the modality of visual vs audio assistance based on their preferences of privacy, data storage, and comfort which may vary for participants. However, careful design consideration is needed to determine how best to display camera images—whether on a device screen or through app notifications—while ensuring they can be activated via voice queries and delivered in real-time. 
  %
\subsection{MemPal Feature experience} 
Regarding RQ2: ``What are older adults' perceptions and experiences of using a voice-enabled wearable assistant for object retrieval?'', older adults rated MemPal with an average usability (SUS) score of 69.375. This score is slightly above the average SUS benchmark of 68~\cite{bangor2008empirical,brooke1996sus} and approaches the threshold of 70, which is considered indicative of good usability \cite{lewis2018system}. \textcolor{black}{However low usability scores are attributed to potentially more critical feedback from those with high MMSE scores which positively suggests those with lower MMSE (MCI and SCD) find the system more useful.} This is an encouraging result, particularly given that MemPal is in its early stages of development. Comparatively, MemPal's usability score is modestly lower than that of GoFinder \cite{gofinder}, which recorded a SUS score of 75.4. However, it is important to note that GoFinder was not evaluated amongst older adults or those with limited technology experience. The lower usability scores observed with MemPal may be partly attributed to its initial form factor, which some participants found uncomfortable, as well as varying levels of technological familiarity among the participants. As participants mentioned, due to hearing aid constraints, vision impairments, and different types of hair within this pilot population, a body-worn camera with either a clip or magnetic attachment seemed to be most feasible.

MemPal is designed to enhance independence by addressing critical pain points identified through our initial interviews. Given the diverse preferences regarding format of descriptions and modality of assistance, the system should offer options and customization. This approach will allow us to tailor the system more closely to user needs. 

In designing a wearable camera system that uses visual context, we y stored textual information \textcolor{black}{(anoymized and securely stored in a protected database)} and creating a voice interface that could easily interpret this textual data~\cite{davies2015security}. \textcolor{black}{The system also complies with ethical guidelines for LLMs in healthcare \cite{harrer2023attention}.} We found that participants were very comfortable sharing this text-based data with their caregivers and providers as well as storing only text-based data on the system, whereas they mentioned that having alternative smart home camera systems that are not egocentric or systems that store continuous camera data may feel uncomfortable (P1). Additionally, ease of use amongst old adults is imperative which requires voice-activation and we found that MemPal had a high ease of use score which proves that voice-based systems for object retrieval are easy to use M=5.38. Participants were also satisfied by the responses (M=5.17) due to its low latency with an average response time of 2.2s as well as appropriate length (M=5.17). Lastly, we wanted to ensure a smooth setup process that is user-friendly especially since systems like FMT did not test the initial installation stage that requires users to set up markers around their home to track activity \cite{fmt} so by introducing participants to the system onboarding stage we were able to develop and pilot a quick setup process. Most participants were satisfied with the location setup without needing much direction. 

% \cite{davies2015security}. Therefore, design decisions revolving around privacy were prioritized. These include the following: (1) No storage of videos and photos, instead only low dimensional embeddings of text captions and initial home calibration on device. (2) Transparency of activity data to patients through summarized analytics. (3) Local database storage on the user’s device instead of an external server and only synced with authorized caregivers after each day.
    %\item \textbf{Limited onboarding} Recognizing older adults users' preference for simplicity, our system requires minimal setup, featuring a quick five-minute home tour video for initial configuration. 
 %   \item \textbf{Socially acceptable form factor} Neck-worn devices are more likely worn by older adults compared to other form factors (e.g., smart glasses) \cite{schwind2020anticipated}.


\section{Future Work and Limitations}

The aim of this work was to create a first, working prototype that can be tested by the target user group, older adults,in their homes for feedback and suggest future design guidelines rather than create the most accurate system.  

\subsection{Limitations}

\subsubsection{Technical Accuracy and System Limitations} 
There were technical limitations in the accuracy of the system particularly room localization, object detection, and speech recognition, which affected user experiences as well as system limitations such as low wearable device comfort, system learning curve, and less descriptive responses.  While the location algorithm worked for a variety of layouts and lighting, it was not generally adaptable to every layout we encountered in our user study. The positioning of the camera also presented challenges, as it influenced the detection of objects and locations, depending on the FOV \textcolor{black}{especially for hidden locations}. The participant feedback revealed that users must be consciously aware of placing objects in front of the wearable camera-based system for it to effectively assist them. As a voice-enabled system, it struggled to adapt to various accents and slurred speech, affecting its ability to understand queries from certain users. 
\subsubsection{Study Design}
One limitation of the object retrieval study design was that participants occasionally remembered where objects were placed since they placed the objects themselves and retrieved within 30 minutes. Although an ideal study design would have increased the time length to days between placing and retrieval, this led to other confounding factors. 

\subsection{Future Work}
%\subsubsection{Future System Additions}

\subsubsection{Future System Improvements}
Future iterations of a wearable memory aid system could incorporate a queryless interface for older adults who may have trouble with speech. A queryless system for object retrieval could use biomarkers or eye tracking to detect confusion, thereby initiating assistance automatically. Real-time task help tailored to the activities of each participant and verbal feedback loops could further improve user interaction. The calibration experience could be refined by including specific furniture items and sub-room areas that are unique to each household. To improve privacy and battery life, future enhancements could include moving machine learning processing to on-device smaller models, which would eliminate the need for processing video through closed-source models as was done in this early-stage study, which participants consented to. Lastly, to adapt to the large diversity in older adults' preferences and needs, such systems should be able to adapt to suit different stages of memory conditions (ex. providing both Visual and Audio modes as options for object retrieval and multiple context options for safety reminders). 

\subsubsection{Future Study Design}
A long term study to extend the feature set of testing including proactive safety reminders and retrospective task recall and summarization will prove the system's effectiveness of general task assistance beyond object retrieval. A longitudinal study exploring long term impacts on memory due to potential over-reliance on wearable memory assistants and unintended uses potentially as social companions will help in more robust, user-centric designs of the system. Testing on specific sub-populations such as people with dementia and Alzheimer's patients who have advanced-stage memory conditions can further provide useful insight. Long term use could also facilitate the learning of behavior in order to suggest better habits and placements of objects to reduce the frequency of memory errors. 

\subsection{Future Directions for MemPal}
As both a Remote Patient Monitoring (RPM) tool and a wearable agent, MemPal has significant potential for expansion. The safety reminder workflow designed in Figure 15 could integrate with voice-inputted daily schedules or calendars for context-based proactive event reminders or task sequence boards for proactive task assistance. These could also be implemented using synthesized voices of people whom older adults are familiar with, such as friends or family members~\cite{10.1145/3479590}. MemPal could facilitate everyday tasks for individuals with memory impairments by leveraging MemPal's task logging and LLM query-response infrastructure to understand actions, activities, and predict the next sequence of steps. Additionally, more fine-grained activity features (such as time taken per activity step) could be used to objectively assess Instrumental Activities of Daily Living (IADL) performance, a common measure for memory-impaired individuals \cite{guo2020instrumental}. MemPal's query log of voice interactions could also be utilized to diagnose conditions such as dementia or depression \cite{kumar2022dementia} by analyzing speech-to-text patterns. Considering the challenges older adults users may face with voice queries, alternative input modalities like haptic touch or silent speech should be explored. Lastly, physicians and speech therapists have highlighted the ability of using a system like MemPal in aiding cognitive rehabilitation through a quiz format, improving memory rather than just supporting it. 

\subsection{Implications of Future Assistive Technologies for Older Adults}

With the rise of LLMs, MemPal demonstrates how multimodal, context-aware technology using LLMs and VLMs can enhance assistive systems for older adults. Future research should further explore the use of multimodal large models to support multifaceted daily living needs. Advancements should prioritize minimalist interfaces and even queryless functionality, addressing unique challenges identified in our study, such as language variations and speech issues among older adults. Additionally, systems should aim to facilitate behavior change and memory training~\cite{10.1145/3351235,chan2022augmenting}, considering older adults' reluctance to seek help and their tendency to overestimate their memory. Finally, ensuring privacy is crucial; future models should focus on protecting user data by limiting information sharing to text only and avoiding image-based data to maintain user confidentiality.

\section{Conclusion}

In this work, we designed and evaluated a wearable camera-based and voice-enabled memory assistant, MemPal, with older adults within their own homes to address their most common pain points regarding ensuring independence like assisting with object finding based on analysis from pre-study interviews, forums and literature. Through investigating the effect of voice-enabled queries with MemPal for object retrieval and comparing it to no assistance and visual assistance (RQ1), findings show that MemPal improved retrieval performance and reduced recall difficulty compared to no aid  but similar to visual aid. Although a prototype, the results also show that older adults found MemPal usable and highly useful (RQ2) but customization is important. Through this work, we contribute towards understanding user design specifications of older adults in their own homes and designing a LLM-based multimodal wearable system that facilitates object retrieval assistance. We provide the opportunity to extend upon MemPal's technology and user interface, enabling general memory assistance. This work represents a crucial first step toward a general personal memory agent for older adults, supporting independent and safe living at home.



\iffalse
\section{Citations and Bibliographies}

The use of \BibTeX\ for the preparation and formatting of one's
references is strongly recommended. Authors' names should be complete
--- use full first names (``Donald E. Knuth'') not initials
(``D. E. Knuth'') --- and the salient identifying features of a
reference should be included: title, year, volume, number, pages,
article DOI, etc.

The bibliography is included in your source document with these two
commands, placed just before the \verb|\end{document}| command:
\begin{verbatim}
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{bibfile}
\end{verbatim}
where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
suffix, of the \BibTeX\ file.

Citations and references are numbered by default. A small number of
ACM publications have citations and references formatted in the
``author year'' style; for these exceptions, please include this
command in the {\bfseries preamble} (before the command
``\verb|\begin{document}|'') of your \LaTeX\ source:
\begin{verbatim}
  \citestyle{acmauthoryear}
\end{verbatim}

  Some examples.  A paginated journal article \cite{Abril07}, an
  enumerated journal article \cite{Cohen07}, a reference to an entire
  issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a
  monograph/whole book in a series (see 2a in spec. document)
  \cite{Harel79}, a divisible-book such as an anthology or compilation
  \cite{Editor00} followed by the same example, however we only output
  the series if the volume number is given \cite{Editor00a} (so
  Editor00a's series should NOT be present since it has no vol. no.),
  a chapter in a divisible book \cite{Spector90}, a chapter in a
  divisible book in a series \cite{Douglass98}, a multi-volume work as
  book \cite{Knuth97}, a couple of articles in a proceedings (of a
  conference, symposium, workshop for example) (paginated proceedings
  article) \cite{Andler79, Hagerup1993}, a proceedings article with
  all possible elements \cite{Smith10}, an example of an enumerated
  proceedings article \cite{VanGundy07}, an informally published work
  \cite{Harel78}, a couple of preprints \cite{Bornmann2019,
    AnzarootPBM14}, a doctoral dissertation \cite{Clarkson85}, a
  master's thesis: \cite{anisi03}, an online document / world wide web
  resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
  (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05}
  and (Case 3) a patent \cite{JoeScientist001}, work accepted for
  publication \cite{rous08}, 'YYYYb'-test for prolific author
  \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might
  contain 'duplicate' DOI and URLs (some SIAM articles)
  \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
  multi-volume works as books \cite{MR781536} and \cite{MR781537}. A
  couple of citations with DOIs:
  \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
  citations: \cite{TUGInstmem, Thornburg01, CTANacmart}. Artifacts:
  \cite{R} and \cite{UMassCitations}.

\section{Acknowledgments}

This section has a special environment:
\begin{verbatim}

  \begin{acks}

  \end{acks}
\end{verbatim}
so that the information contained therein can be more easily collected
during the article metadata extraction phase, and to ensure
consistency in the spelling of the section heading.

Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

\section{Appendices}

If your work needs an appendix, add it before the
``\verb|\end{document}|'' command at the conclusion of your source
document.

Start the appendix with the ``\verb|appendix|'' command:
\begin{verbatim}
  \appendix
\end{verbatim}
and note that in the appendix, sections are lettered, not
numbered. This document has two appendices, demonstrating the section
and subsection identification method.

\section{Multi-language papers}

Papers may be written in languages other than English or include
titles, subtitles, keywords and abstracts in different languages (as a
rule, a paper in a language other than English should include an
English title and an English abstract).  Use \verb|language=...| for
every language used in the paper.  The last language indicated is the
main language of the paper.  For example, a French paper with
additional titles and abstracts in English and German may start with
the following command
\begin{verbatim}
\documentclass[sigconf, language=english, language=german,
               language=french]{acmart}
\end{verbatim}

The title, subtitle, keywords and abstract will be typeset in the main
language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
begin title, subtitle and keywords, can be used to set these elements
in the other languages.  The environment \verb|translatedabstract| is
used to set the translation of the abstract.  These commands and
environment have a mandatory first argument: the language of the
second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
of their usage.

\section{SIGCHI Extended Abstracts}

The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
not in Word) produces a landscape-orientation formatted article, with
a wide left margin. Three environments are available for use with the
``\verb|sigchi-a|'' template style, and produce formatted output in
the margin:
\begin{itemize}
\item {\verb|sidebar|}:  Place formatted text in the margin.
\item {\verb|marginfigure|}: Place a figure in the margin.
\item {\verb|margintable|}: Place a table in the margin.
\end{itemize}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.


\fi
\begin{acks}
The authors would like to thank Aditya Suri, Rachel Park, Cayden Pierce, Nigel Norman, Arnav Kapur, Roy Shilkrot,  Nathan Whitmore, and Leyla Buljina for their valuable support during the course of this project. 
\end{acks}
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\appendix
\include{appendix}


\end{document}


\endinput
%%
%% End of file `sample-manuscript.tex'.
