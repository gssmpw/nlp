\section{Appendix}  

\subsection{Interviews}\label{appendixinterviews}

For the live interviews, our feedback group included four neurologists (D) and one speech pathologist (SP) who have extensive experience with over 100 patients facing neurogenic disorders. Additionally, we gathered insights from caregiver support groups comprising 12 caregivers (C) where 8 were healthy older adults themselves and memory support groups including 12 individuals with memory decline (E), facilitated through a hospital network. We also engaged with individuals with memory loss (OE) and caregivers (OC) through anonymous online forums. 

Guiding questions for each demographic were used to extract insights: 

For caregivers, speech pathologists, and physicians: 
\begin{itemize}
    \item What are the current memory support tools used? 
    \item What is the most difficult part of caregiving and provide scenarios? 
    \item What data do you wish you had to make an accurate assessment of the patient's memory? 
    \item What type of form factor is the best for constant monitoring?
\end{itemize}


For older adults: 
\begin{itemize}
    \item What objects are frequently lost? 
    \item  Describe some scenarios that caused frustration when you did not remember something.
\end{itemize}


Insights are coded and thematically analyzed in the following categories: 







\begin{itemize}


\item Issue 1: Object-finding

Description: Finding lost objects is frustrating and time consuming
Examples: Scenarios of losing purse in home, concerns about not recognizing own possessions, examples of frequently lost objects  
Quotes: 
\begin{itemize}
    \item OC8: "My MIL was always missing her purse and was sure someone had stolen it. She would ransack her room all night long. It would be in the closet. This happened often."
    \item OC9: “Hearing aids and purse. Sometimes hearing aids are in the purse. Drives my poor dad nuts.” 
    \item OC10:  “The phone chargers! Always misplaces her iPhone cord, looks for another one, takes someone else's type C, then replaces that with some random micro USB or whatever... ugh... The issue here is that she doesn't know what is hers and what isn't, and ends up taking and keeping or throwing away other people's belongings.” 
    \item OC11: “Glasses and dentures”  
\end{itemize}




\item Issue 2: Safety reminders

2A: Anecdotal Scenarios
Description: Anecdotal scenarios of daily tasks that would be often forgotten and could cause safety concerns
Examples: scenarios of short term memory loss like forgetting to take important items when leaving the home, quotes from speech pathologists or doctors referencing certain scenarios where safety reminders would be most useful 
Quotes: 
\begin{itemize}
    \item D1: “You have the most leverage when developing systems to ensure safety, especially during activities like cooking.” 
    \item E6: “Didn’t have a wallet or purse when I left the house” 
    \item E7: “I forgot to put milk back in the fridge and my wife got mad”
    \item E8: “I have issues with short term memory- when going to the doctor if you’re asked what did you do today being like I have no idea, it’s hard to remember the day to day” 
\end{itemize}

2B: Current safety monitoring and reminders
Description: Current methods of how caregivers give reminders to older adults including verbal or written reminders, how older adults would respond to the current system and problems associated 
Examples: type of reminder provided and reaction to that reminder from older adults, opinions from caregivers about how current systems lead to caregiver burden
Quotes: 
\begin{itemize}
    \item SP1: “Speech pathologists use sequence boards to help with safety trained tasks so a log of safety related tasks can reduce our burden as well as caregiver burden” 
    \item C10: “extra burden of decision making is being deferred to caregivers” 
    \item C12: “use scraps of paper to write a schedule, then I got a portable whiteboard and big letters that I print out first thing in the morning” 
    \item OC1: “I put up 'reminders' for my husband suffering from dementia and they didn't do a damn bit of good”  
    \item OC2: “My written reminders got ignored as their reality says it’s not needed. My verbal reminders often are met with combative passive aggressive responses as their reality says it’s not needed. The reality is the reminders are needed for them to function properly." 
    \item OC3: “I found it necessary to remove things, place things out of reach, and supervise with cameras and movement alarms to keep my person safe and healthy.” 
    \item OC4: “The medication was also a problem for my person who forgets what was taken or not even with a labeled pill dispenser. For safety I choose to dispense all medications”  
    \item OC5: “Too many signs or signs up too long become white noise. Be selective. She still needs some verbal prompts though. You can certainly try your post to note reminders about safety, but I'd expect limited returns and possible irritation.”  
    \item OC6: “We have notes everywhere, but he doesn't pay attention to any of them. Whenever he is performing any action, I just watch over him and make sure he has what he needs. It can be exhausting. If he's going to the shower, I ask, "Do you have your towel, clothes, deodorant?" He'll answer yes every time, then go back and forth getting what he forgot” 
    \item OC7: “had a whiteboard directly in front of the chair where my mom slept. It listed the basic facts of her life (her name and location included) and what was about to happen next. If someone came over, I put the name and schedule on the board. After a while, that board was a lifeline; she knew to look at it when she was confused.” 
    \item OC8: "Some other things might be to lock the door, make sure faucets are turned off, charge your phone (if cell phone), don’t eat expired food in fridge, never give out information to anyone over phone / email (however to convey she should avoid scammers)."
\end{itemize}


\item Issue 3: Confabulation of past events 
Description: inability to remember longer sequences of actions or tasks in the past and approaches to currently solve  
Examples: physicians providing suggestions that passive remote tracking is useful, caregivers and speech pathologists providing anecdotes of ways to combat this 
Quotes: 
\begin{itemize}
    \item D1:  “remote patient monitoring is very helpful and tracking is helpful since we use a lot of cognitive assessment. Patients thinks they’re fine but their spouse comes in and you can’t target anyone in denial”
    \item D2: “Passive sensors are better since caregivers are usually filling out subjective questionnaires of activities of daily living form and functional assessment”
    \item E6: My biggest issue with his memory is that he'll tell us he has something when he doesn't. He lies daily.
    \item E7: While it is technically lying, the term is confabulation. He likely doesn't remember if he grabbed an item or finished a task or if he's done the ask repeatedly, his days are mixed up and he thinks he did it. Confabulation is used to fill in a memory the person doesn't have or a blank in a series of memories. It's very frustrating. My mom deals with the same thing. Not as intensely but it's especially evident when having conversations about certain things.
    \item E8: When I left for short trips, I would give her a printed page of instructions: where I was, how to ring the alarm button, when I would call, what she should do. Reviewing the whiteboard with her became part of our morning routine. Her daily schedule at that point was reduced to TV and aides coming into the house. I would write the aide's name, the hours, where I was, and when I would be back. TV, especially sports, became a highlight of the day at the scheduled time. When she moved to a nursing home at the end, I got a second white board and propped both up on the desk, because her questions had doubled. I also laminated a copy of the information for the aides to read to her at night. 
    \item SP1: A memory book is a diary of daily activities that can help patients remember events. The memory book can be used as a therapy tool and a functional guide.
\end{itemize}
    
\end{itemize}
%To enable the features described above, MemPal consists of a vision system and language system (Figure \ref{fig:overall}). The vision system creates an automated diary of activity (example seen in \ref{fig:activities} and the language system powered by an LLM agent understands user voice queries and responds accordingly using the vision context. The underlying technology architecture consisting of camera preprocessing, visual feature extraction, and LLM query/ processing are reused for all features with slight modifications to fit to the feature. Vector databases pertaining to location calibration (CalibrationDB), automated diary (ActivitiesDB), and summarized diary (HigherLevelDB) are persisted locally on-device while location trajectory and safety reminder history are persisted on Google Firebase Realtime database to be able to share them with authorized caregivers. User queries to MemPal (QueriesDB) are also stored locally on-device. 


\section{System Implementation}\label{systemimp}
\subsection{Vision System}

\begin{figure*}
    \centering
    \includegraphics[scale=0.27]{figures/visual.pdf}
    \caption{Vision system workflow: Every frame in a frame batch of nine is first pre-processed to create a tiled image (Left) before indoor room localization. Then, if the user's hand(s) is detected in the frame of the wearable camera (Center Bottom), a Vision Language Model (VLM) specifically GPT4-V (Right) that uses the detected location context and previous activity, describes the hand-held object, activity, and background.}
    \label{fig:visual}
\end{figure*}


\subsubsection{Camera Pre-processing}
 To minimize noise and reduce unnecessary computation, frames with low sharpness (Laplacian variance < 25) and adjacent frames with high similarity (structural similarity index score (SSIM) <0.95) are discarded \cite{ssim}. A 235$^{\circ}$ fish-eye lens is used to increase FOV and standard camera calibration was applied to correct distortions from the lens \cite{opencv_calibration}.

\subsubsection{Indoor Location Tracking}

MemPal uses the monocular egocentric camera to perform embedding based real-time indoor location tracking without requiring other sensors like GPS, unlike high compute algorithms like SLAM \cite{ventura2014global}. Embeddings capture a semantic representation within the context of the user’s home allowing for a more personalized search. A calibration video is first processed into (1) an embedding map and (2) a room adjacency list to represent an initial spatial map of the user's home. This home representation is used as context for real time localization within the map. The algorithm is described in Figure \ref{fig:location-im}. 

\begin{figure*}
    \centering
    \includegraphics[scale=0.27]{figures/location-im.pdf}
    \caption{Details of the indoor location tracking algorithm including calibration phase and localization.}
    \label{fig:location-im}
\end{figure*}

\begin{itemize}
    \item \textbf{Calibration Setup}: The calibration video frames are uniformly sampled and embedded using a pretrained CLIP-ViT-B-32 transformer model. These embeddings are stored in vector database named \textit{CalibrationDB} for fast retrieval along with the voice-labeled room name as metadata for each embedding. The temporal sequence of the room labels are used to generate an adjacency list that maps each room to a set of connected or adjacent rooms, ensuring that new locations detected every second is adjacent to the previous one. With additional usage, accumulating data enriches the location embedding map allowing it to adapt to the home's configuration environment and improve its robustness.   

    
    %Current methodologies for camera based real-time mapping and localization that achieve high accuracy, such as SLAM (Simultaneous Localization and Mapping) (ex. ORB-SLAM\cite{mur2015orb}), Structure from Motion (SfM) (ex. COLMAP\cite{fisher2021colmap}), or 3D point cloud generation \cite{li2021image}, require substantial computational resources, are sensitive to motion and changes in illumination, or require extensive and precise initial calibration. This poses challenges for implementation on wearable devices and deployment for older adults, particularly when aiming to preserve privacy and perform processing on-devices. %Bluetooth beacons coupled with ML triangulation methods require complex installation processes are not cost-efficient for large-scale deployment.

    %A recent technique, EgoEnv\cite{nagarajan2024egoenv}, leverages egocentric video walkthrough to create human-centric environment representations for initial calibration but doesn't allow for customized labels based on individual home layouts. A more accurate method, EgoVLP\cite{qinghong2022egocentric}, integrates egocentric video with language pretraining models to enhance the semantic understanding of the environment, providing contextual descriptions for objects, but requires significantly more training data. 
    \item \textbf{Real-time Location Tracking}: During localization, a heuristic-based ensemble approach identifies the correct room the user is in in relation to the calibration map and adjacency room list. Algorithm is illustrated in \ref{fig:location-im} and listed below.


    \begin{itemize}
            \item \textbf{Step 1}: Calculate image similarities: Cosine similarity metric is calculated between the image embedding of $F_{i}$ and the calibration map embeddings (range:[0,2] where 0 denotes the same image and 2 denotes orthogonal images), to represent the distance between embeddings. 
            \item \textbf{Step 2}: When performing similarity search, if the cosine similarity between the current location’s embedding and the top k=1 nearest neighbor is higher than the confidence threshold (T) = 0.22 (which means less confident), then the previous location is set $L_(i-1)$. 
            \item \textbf{Step 3}: If the confidence threshold of the top k=1 image is within an appropriate level, the unique locations from the top k=11 location candidates are determined. 
                \subitem If the unique locations within the set of top candidates is equal to 1, this signifies high detection confidence which means that the query location seems to be within a cluster of location points for that area. 
                \subitem If the unique locations within the set of top candidates is greater than 1, this signifies lower confidence which means that potentially the image is in the boundary between two clusters of images or location clusters are overlapping which often happens if rooms within a household have very similar visual resemblance. The most frequent element in the list of location choices is then returned, weighted by confidence level.
        
            Environment features are encoded through the history of previously detected locations. To reduce the embedding process time, we decided to use the compressed history of locations rather than concatenated embeddings like EgoEnv \cite{nagarajan2024egoenv}. If the resulting new location determined is not adjacent to the previous location, then the current location detected is set to the previous confident location.
            \item \textbf{Step 4}: The mode among the list of tiled locations (len<=9) detected within each frame is then set as the final location.   
            
        \end{itemize}

    

\end{itemize}

\subsubsection{Hand Detection}

 A deep neural network trained on the EgoHands Dataset \cite{Dibia2017} is implemented in Tensorflow 2.0 which tracks the presence and relative location of both hands in the video frame. Once 1 or 2 hands are detected within an image frame, then a tiled image is created for object and activity detection described in the next section. A tiled image compresses a batch of 9 sequential processed frames into a 3x3 matrix, arranging them from the top-left to the bottom-right, reducing the input size by a factor of 9 for efficient VLM processing.  

\subsubsection{Object and Activity Detection}

A vision-language model then analyzes the tiled image to determine the (1) hand-held object, (2) activity, and (3) background scene description including furniture positions and colors in a zero-shot manner. Objects and activities do not need to be tagged or trained prior to usage, to ensure generalizability and personalization. A textual description of the activity is generated by discerning the temporal image sequence in the tiled image along with the background objects in the frame of view. We use GPT-4 vision to extract this information, due to its better performance in comparison to other models for egocentric video understanding \cite{vidcompare}. Context about the previous activity and the current location is provided to improve the accuracy of the detection of the current activity. Once the real-time description is generated, the textual data is embedded using the all-MiniLM-L6-v2 Sentence Transformer model and stored within a 384 dimension dense vector space along with metadata including timestamp, location, object, background description (\textit{ActivitiesDB}). 


\subsection{Language System}

To respond to explicit user queries about objects or past events or implicit queries during safety task monitoring, a question and answer system is created which includes query categorization and feature-specific response formation utilizing the text-based ActivitiesDB. \newline

\subsubsection{Query Processing}
First, the user query is transcribed through Deepgram’s streaming speech-to-text API \cite{deepgramspeech}. Then after the wakeword: “Pal” is detected, queries are categorized into one of the following categories using an LLM model (OpenAI GPT-3.5-turbo): (i) objects, (ii) past, (iii) future, (iv) followup, or (v) none. If the query is categorized as “objects,” the object in question is extracted within the sentence. Query categorization allows users to converse with MemPal in a naturalistic way while ensuring to respond to only memory-based relevant questions.  



\subsubsection{Safety Tasks}
%\newline
\begin{figure*}
    \centering
    \includegraphics[scale=0.27]{figures/safety-im.pdf}
    \caption{Safety Reminders Implementation demonstrating proactive reminders, retrospective task remembering, and passive monitoring of these safety reminders. This workflow first demonstrates how (1) reminders that are inputted within the MemPal app are stored (2) If location is detected (3) a context-based proactive reminder is triggered and (4) status of the reminder updated on the app upon activity is completed (logged in ActivitiesDB) and  (5) how the system uses a similar query response backend as object retrieval to answer user questions about tasks.}   
    \label{fig:enter-label}
\end{figure*}

\textbf{Proactive}: Text-based safety reminders that are inputted in the app and grouped by location are pushed to the Firebase Database which can be accessed by the API. These reminders are formatted in a question form (ex. "Did you remember to [safety reminder]") and provided as audio output using OpenAI Whisper’s text-to-speech model depending on the user's context. A queue of reminders for each location is set and popped upon completion of the task. 

\textbf{Retrospective}: If individuals query the MemPal system about the completion of these safety tasks or any past action, the query is first categorized into the "past" category before another RAG chain is invoked (similar to Section \ref{objectretrieval}).

\textbf{Passive task completion monitoring}: A safety checker LLM agent using a RAG chain approach similar to Section \ref{objectretrieval} assesses the completion of safety tasks in real-time using the last 2 minutes of activity data from ActivitiesDB. A question in the form of “Did the person complete the safety task: [reminder]?” is set as the query for the agent. The output is a binary 'y' or 'n' response, transmitted to the Firebase app, which updates both the safety task status and completion time. 

\input{tables/table_demographics}


\begin{figure*}
  \centering
  \includegraphics[scale=0.40]{charts/room_distribution.jpeg}
  \caption{The participant house embedding maps (computed by Fig. \ref{fig:location-im}) show home variability with no clear clusters, indicating that standard clustering or CNN room detection models are unsuitable. The map displays 2D image embeddings, color-coded by room after dimensionality reduction.}
  
  \label{roomdistribution}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[scale=0.35]{charts/freq_rooms.png}
    \caption{This shows the frequency distribution of different rooms across the various participant homes to illustrate high variation among names and the need for personal room labeling.}
    \label{fig:roomfreq}
\end{figure*}


\section{Questionnaires}
\subsection{Post Object Retrieval Task Questionnaire}
\label{appendixPostObjectRetrieval}

We measured eight aspects using a 7-point Likert scale (1=strongly disagree, 7=strongly agree).

\begin{enumerate}
    \item \textbf{Helpfulness of Object Retrieval Feature}: ``How helpful was this object retrieval feature?''
    \item \textbf{Likelihood of Future Use}: ``How likely are you to use MemPal for finding objects in the future?''
    \item \textbf{Satisfaction with Responses}: ``How satisfied were you with the responses from MemPal?''
    \item \textbf{Ease of Use}: ``How easy was the device to use?''
    \item \textbf{Appropriateness of Answer Length}: ``I felt that the length of the answers was appropriate.''
    \item \textbf{Trust in System's Responses}: ``How much do you trust the systems’ responses?''
    \item \textbf{Usefulness of Camera Feed}: ``Was the camera feed useful?''
    \item \textbf{Reliance on Camera Feed vs Background Descriptions}: ``How much did you rely on the camera feed vs the background descriptions from MemPal to find the object?''
\end{enumerate}

We also included open-ended questions to gather additional insights.

\begin{enumerate}
    \item ``What do you think about this object retrieval feature?''
    \item ``How do you think it could be improved or what could be added?''
\end{enumerate}



\subsection{Open-Ended Interview}
\label{appendixOpenEndedInterview}
We included open-ended questions to gather additional insights from Interview.\label{secinterview}

\begin{enumerate}
    \item \textbf{Overall Thoughts}: ``What are your overall thoughts on the system?''
    \item \textbf{Troubleshooting}: ``Did you have any trouble using the system? If so, what kind of trouble did you have?''
    \item \textbf{Comfort in Sharing Data}: ``How comfortable are you with having data about safety reminders and summary of activities shared with your caregiver, family, or doctor? (Comfort in sharing data)''
    \item \textbf{Confusion During Calibration}: ``For the calibration phase, is there any part of the flow where you were confused?''
    \item \textbf{Instructions for Calibration}: ``Do you feel like you have enough instruction on how to perform the calibration, and if not, what instructions would be most helpful?''
    \item \textbf{System Changes}: ``What would you change in the system? What do you want the system to do and not do?''
    \item \textbf{Current Memory Aids}: ``What memory aids do you currently use?''
\end{enumerate}


\subsection{Post NASA-TLX Questions}
\label{appendixSelfReportedConfidence}
We measured two aspects using a 7-point Likert scale (1=strongly disagree, 7=strongly agree).\label{secinterview}

\begin{enumerate}
    \item  ''I was confident about finding the objects with [condition].''
    \item ''I found it difficult to recall where the objects were with [condition]''
\end{enumerate}
