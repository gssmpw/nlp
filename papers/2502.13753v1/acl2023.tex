% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
%
% and set <dim> to something 5cm or larger.
\usepackage{microtype}
\usepackage{import}
\usepackage{layout}
\usepackage{tabularx, makecell}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{amssymb} 
\usepackage{url}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{xspace,paralist}
\usepackage{times,latexsym}
\usepackage{amsmath}
\usepackage{appendix}
\usepackage{comment} 
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{arydshln}
\usepackage{cleveref}
\usepackage{tcolorbox}
\usepackage{todonotes}
\usepackage{longtable,supertabular}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
% \newcommand{\cmark}{\ding{51}}%
% \newcommand{\xmark}{\ding{55}}%

%\usepackage[frozencache,cachedir=.]{minted}
% \usepackage[cachedir=.]{minted}
\definecolor{bg}{rgb}{0.95,0.95,0.95}


\newcommand{\cmark}{\textcolor{green}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\cyan}[1]{\textcolor{cyan}{#1}}

\newcommand{\HL}[1]{\textcolor{blue}{#1}}
\newcommand{\ex}[1]{\textit{#1}\xspace}
\newcommand{\eqnref}[1]{Eq~\eqref{#1}\xspace}
\newcommand{\tabref}[1]{Table~\ref{#1}\xspace}
\newcommand{\figref}[1]{Figure~\ref{#1}\xspace}
\newcommand{\secref}[1]{Section~\ref{#1}\xspace}
\newcommand{\appref}[1]{Appendix~\ref{#1}\xspace}

\newcommand{\name}{\textit{SCALAR}}
\newcommand{\nmodels}{8\xspace}



% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{SCALAR: Scientific Citation-based Live Assessment \\of Long-context Academic Reasoning}

% \author{Renxi Wang*, Honglin Mu*, Liqun Ma, Lizhi Lin, Yunlong Feng \\ \textbf{Timothy Baldwin, Xudong Han, Haonan Li}}

\author{
Renxi Wang$^{1,2}$\thanks{\hspace{2mm}Equal contributions.} \quad Honglin Mu$^{1,2}$\footnotemark[1] \quad Liqun Ma$^{1}$ \quad Lizhi Lin$^{2,3}$ \quad Yunlong Feng$^{4}$ \\ 
\textbf{Timothy Baldwin$^{1,2,5}$ \quad Xudong Han$^{1,2}$ \quad Haonan Li$^{1,2}$\thanks{\hspace{2mm}Corresponding author.}} \\
$^{1}$MBZUAI \quad $^{2}$LibrAI \quad $^{3}$Tsinghua University \\ $^{4}$Alibaba Group \quad $^{5}$The University of Melbourne \\
}

\begin{document}
\maketitle
\begin{abstract}
Evaluating large language models' (LLMs) long-context understanding capabilities remains challenging. We present SCALAR (Scientific Citation-based Live Assessment of Long-context Academic Reasoning), a novel benchmark that leverages academic papers and their citation networks. SCALAR features automatic generation of high-quality ground truth labels without human annotation, controllable difficulty levels, and a dynamic updating mechanism that prevents data contamination. Using ICLR 2025 papers, we evaluate \nmodels state-of-the-art LLMs, revealing key insights about their capabilities and limitations in processing long scientific documents across different context lengths and reasoning types. Our benchmark provides a reliable and sustainable way to track progress in long-context understanding as LLM capabilities evolve.\footnote{\url{https://github.com/LibrAIResearch/scalar}}
\end{abstract}

\section{Introduction}

Large language models (LLMs) have demonstrated impressive capabilities in processing texts of increasing lengths \citep{achiam2023gpt,anthropic2024,dubey2024llama,yang2025qwen2}.
While capable of handling contexts of hundreds of thousands of tokens, evaluating their true understanding of long documents remains challenging. 

Previous evaluations of long-context understanding have often relied on synthetic datasets or simple retrieval tasks like ``needle in a haystack'' variations~\citep{needle,kuratov2024searchneedles11mhaystack,wang2024needle,roberts2024needlethreadingllmsfollow}. While such tasks can test a model's ability to locate information in long sequences, they fail to assess genuine comprehension and are readily solvable by current LLMs \citep{team2024gemini}. Moreover, creating high-quality benchmarks traditionally requires extensive human annotation, which is both time-consuming and costly. Some work transforms short-context tasks into long context by combining them with passages or long documents, such as long-document QA~\citep{needle}, summarization~\citep{chang2024booookscore}, reasoning~\citep{babilong} and reranking~\citep{helmet}. However, such datasets suffer from data contamination and shortcut exploitation, as LLMs can solve problems using their own knowledge rather than the long context. See more related work in \Cref{related_work}.

\begin{figure*}[t]
    \centering
    \tiny
    \includegraphics[width=1.0\linewidth]{images/SCILong.drawio.pdf}
    \caption{The overall process of building \name. We first crawl arXiv papers that are also in ICLR 2025. Then we parse them into structualized data, sampling citations to mask and other citations as candidates.}
    \label{fig:overview}
\end{figure*}


In this work, we propose \name, a novel benchmark for evaluating LLMs' long-context understanding in the scientific domain. Our approach leverages recent published academic papers and their citation networks, which are labeled by scientific researchers, ensuring annotations are rigorous and high-quality (especially since we use highly rated papers from top-tier conferences). 
This benchmark can be automatically updated according to the most recent high-quality publications that are also publicly available on arXiv.
Based on the collected data, we develop a framework that can control the difficulty levels dynamically. We configure three levels of difficulty to construct our benchmark, ensuring a suitable evaluation covering from small models to large models. Our evaluation reveals significant performance gaps even among state-of-the-art models, highlighting current limitations in long-context understanding.


Our contributions are threefold: (1) We introduce a systematic approach to creating high-quality, dynamic long-context evaluation datasets using academic citation networks, which can be continuously updated with new papers; (2) We release a comprehensive benchmark dataset based on ICLR 2025 papers, providing a challenging testbed for long-context understanding; (3) Through extensive evaluation of \nmodels advanced LLMs with enhanced long-context capabilities, we reveal several important insights about the current state and limitations of long-document understanding, including how model performance varies with different context lengths and types.

\section{\name}

\subsection{Data Construction}

\paragraph{Data Crawling} We first crawl the research articles and their citations from arXiv.\footnote{\url{https://arxiv.org/}} However, since arXiv papers may include low-quality or unverified research, we mitigate this issue by selecting only those papers that have been submitted to ICLR 2025 and have received an overall score higher than 7. This not only helps filter out low-quality research but also reduces the risk of data contamination.\footnote{Note that our methods can be generally applied to any paper.} 
% This initial collection forms the foundation of our dataset.

\paragraph{Preprocessing and Structualization} 
%The second stage focuses on the preprocessing of the collected papers. 
 We then identify citations within the paper body. Each citation is carefully marked with its position and mapped to its corresponding reference in the bibliography. We then use the data collection process to gather information about the cited papers. We separate papers into top-level sections, finding that papers in our dataset contain an average of 6.1 sections. For all papers, we remove references and appendices, leaving out the main content for task formulation.

\paragraph{Citation Filtering} To ensure high-quality evaluation data, we distinguish between two categories of citations: grouped citations and individual citations. Grouped citations refer to multiple sources together in a single parenthetical reference, typically used when summarizing general information, such as ``\textit{prior works (Liu et al., 2024; Hsieh et al., 2024a; Zhang et al., 2024)}''. Individual citations, in contrast, reference single sources separately, usually when discussing specific methods or results, for instance ``\textit{needle in a haystack' test (Kamradt, 2023)}''. Since grouped citations may have multiple correct answers when masked, during the processing in \Cref{sec:task_formulation}, we exclusively sample and mask individual citations to maintain clear ground truth labels.

% \begin{table}
% \begin{tabular}{lrrr}
% \toprule
% Subset & \#Num & \#Tokens & \#Words \\
% \midrule
% Easy   & 100 & 70,370 & 50,072 \\
% Medium & 100 & 77,241 & 53,695 \\
% Hard   & 100 & 68,155 & 46,798 \\
% \bottomrule
% \end{tabular}
% \label{tab:data_statistics}
% \caption{Caption.\HL{rethinking this table}}
% \end{table}

\subsection{Task Formulation}
\label{sec:task_formulation}
% \paragraph{Basic Task Format}
We formulate all problems as cloze-based multiple-choice questions, where language models must select the correct reference from provided candidates based on a paper context where a citation has been masked. Each question comprises three distinct parts: {Instruction}, {Question Paper}, and {Candidates}.
\texttt{Instruction} describes the task-related information, including how the LLM should finish the task, the answer format, and the roles of other parts of the question. \texttt{Question Paper} contains either a full paper or a specific section with a citation masked with a placeholder \texttt{**[MASKED\_CITATION]**}. \texttt{Candidates} contain four potential references --- one correct answer and three distractors. All candidates are drawn from the reference list of the question paper. Since we only use individual citations (where authors cite one paper in isolation rather than grouping multiple references), we can be confident that among all papers in the reference list, the authors determined this specific reference was the most appropriate for this particular citation, ensuring reliable ground truth labels.

In our prompt implementation, we define several XML elements to separate different elements. The details of the prompt template are shown in \Cref{fig:prompt}.

\begin{figure}[h]
	\small
	\begin{tcolorbox}[colframe=white, left=2.5mm, right=1.5mm]
\textbf{Question Paper}\\
\textbullet\ Single Section: Providing only the section containing the masked citation.\\
\textbullet\ Full Paper: Including the complete paper as context. \\

% We categorize \textbf{citations} into two types based on their semantic properties:\\
\textbf{Citation Type} \\
\textbullet\ Attributional Citations: These explicitly mention a specific model, method, or dataset name (\textit{e.g., ``BERT (Devlin et al., [2018]) is used for embedding texts...''}).\\
\textbullet\ Descriptive Citations: These integrate references into descriptions without explicit naming (\textit{e.g., ``Pretraining a bidirectional transformer (Devlin et al., [2018]) is time-consuming...''}).\\

\textbf{Distractor Sampling} \\
\textbullet\ Random Sample: Sampling from the question paper's reference list randomly.\\
\textbullet\ Nearest Sample: Sampling from the nearest 4 citations to the masked citation. We prioritize citations within the same section as the masked citation. If no enough citations, we sample from adjacent sections.\\

\textbf{Candidate Representation}\\
\textbullet\ Concise: Title and abstract only.\\
\textbullet\ Full: Complete paper content.\\
\textbullet\ Body: Paper content with title, abstract, introduction, and conclusion removed
	\end{tcolorbox}
	\caption{Configuration dimensions for controlling task difficulty in SCALAR. The framework allows adjustments across four key aspects: question paper scope, citation type, distractor sampling method, and candidate paper representation. Each dimension can be independently configured to create varying levels of challenge.}
	\label{fig:difficulty_config}
\end{figure}



\subsection{Difficulty Control}
\label{sec:difficuly}

We implement a highly customizable difficulty control framework that allows fine-grained adjustment of both semantic complexity and context length. The framework offers multiple configuration dimensions, as shown in Figure~\ref{fig:difficulty_config}.

This flexible framework allows for precise control of task difficulty through various combinations of these configurations. For our benchmark, we define three standard difficulty levels that progressively increase semantic and length complexity:  (I) \textbf{Easy} level masks attributional citations, where distractors are sampled randomly. (II) \textbf{Medium} level masks descriptive citations, where distractors are also sampled randomly. Both easy and medium level use the full paper for the question paper and candidates.
(III) \textbf{Hard} level masks descriptive citations, but candidates are sampled from its nearest citations. Additionally, we use only the body of the paper for candidates.
 
\paragraph{Final Dataset}
The final dataset consists of 300 questions evenly distributed across three difficulty levels, with each question containing four candidates. To ensure diversity, we limit at most five questions per paper. All papers, including question paper and candidates, are limited to 100,000 characters to accommodate model context limitations, with questions formatted using the template in \Cref{fig:prompt}.

\begin{table}[t]
\small
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lclccc}
\toprule
Model & Context & Price & Easy & Medium & Hard \\
\midrule
Llama 3.1 70B    & 128K  &  \$0.65*         & 0.37 & 0.29 & 0.16 \\
Llama 3.1 8B     & 128K  & \$0.05*          & 0.30 & 0.24 & 0.25 \\
Llama 3.3 70B    & 128K  & \$0.65*          & 0.37 & 0.36 & 0.23 \\
Qwen2.5 7B 1M    & 1M & \$0.05*             & 0.52 & 0.37 & 0.29 \\
GLM 4 9B             & 128K & \$0.05*       & 0.67 & 0.50 & 0.35 \\
Claude 3.5 Haiku & 200K & \$0.80            & 0.77 & 0.61 & 0.42 \\
GPT 4o                    & 128K & \$2.50   & 0.95 & 0.72 & 0.50 \\
GPT 4o Mini               & 128K & \$0.15   & 0.81 & 0.56 & 0.48 \\
\bottomrule
\end{tabular}
}
\caption{Model performance across difficulty levels. Price shown is per 1M tokens. * denotes price calculated based on third-party inference service.}
\label{tab:model_performance}
\end{table}

\section{Experiments}

% \subsection{Baselines}
We benchmark several open-source and proprietary models with long-context capabilities ($\geq 128k$ tokens) on SCALAR.% based on the LibrA-Eval\footnote{\href{https://github.com/LibrAIResearch/libra-eval}{https://github.com/LibrAIResearch/libra-eval}} framework.
The list of models we evaluate are introduced in \Cref{app:eval}.



\subsection{Main Results}
\Cref{tab:model_performance} presents model performance across difficulty levels.  Generally, all LLMs' performance downgrades when difficulty increases. On the easy level, \name\ can already differentiate LLMs' long context capability, where the best model GPT-4o achieves 95\% accuracy, while the lowest-performing models hover around 30-37\%, compared to the random baseline of 25\%.
For the hard level set, half of the models achieve random performance, and even current SOTA models obtain less than half the correct results, demonstrating how challenging our dataset is. 

The hard level proves particularly challenging - even state-of-the-art models like GPT-4o achieve only 50\% accuracy, while most other models perform near random chance. Notably, model size does not directly correlate with performance: smaller models optimized for long-context processing (Qwen2.5-1M-7B at 52\% and GLM-4-9B at 67\% on easy tasks) outperform larger models like Llama-3.1-70B (37\%). This suggests that architectural choices and training objectives for long-context understanding \citep{yang2025qwen2} may be more crucial than model scale alone \citep{dubey2024llama,glm2024chatglm}.


\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccc}
    \toprule
        \multirow{2}{*}{Question}  & \multirow{2}{*}{Candidate} & \multicolumn{2}{c}{Easy} & \multicolumn{2}{c}{Hard} \\
        \cmidrule{3-4} \cmidrule{5-6}
        & & GPT & Qwen & GPT & Qwen \\
        \midrule
        Section& Concise & 0.86 & 0.88 & 0.58 & 0.48\\
        Full   & Concise & 0.86 & 0.72 & 0.52 & 0.40\\
        Section& Full    & 0.80 & 0.40 & 0.64 & 0.22\\
        Full   & Full    & 0.80 & 0.42 & 0.50 & 0.24\\
        \bottomrule
    \end{tabular}}
    \caption{Impact of context length on model performance. Results compare GPT-4o-mini and Qwen2.5-7B-Instruct-1M across different context configurations. Question context varies between full paper and section-only, while candidate context varies between full paper and title+abstract-only. Semantic difficulty is controlled: \textbf{Easy} level uses attributional citations with random candidates, while \textbf{Hard} level uses descriptive citations with nearest-neighbor candidates.}
    \label{tab:length}
\end{table}


\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llcccc}
    \toprule
        Cite Type& Sampling &Candidate & GPT & Qwen  \\
        \midrule
        Attributional &Random & Full &  0.82 & 0.42\\
        Attributional &Nearest& Full &  0.82 & 0.38\\
        Descriptive   &Random & Full &  0.56 & 0.34\\
        Descriptive   &Nearest& Full &  0.52 & 0.22\\
        Descriptive   &Nearest& Body &  0.44 & 0.28\\
        \bottomrule
    \end{tabular}}
    \caption{Impact of semantic reasoning difficulty on model performance. Results of GPT-4o-mini and Qwen2.5-7B-Instruct-1M. Cite type, sampling methods, and candidate type are introduced in \Cref{fig:difficulty_config}.}
    \label{tab:semantic}
\end{table}


\section{Analysis}

Following our discussion of difficulty control in \Cref{sec:difficuly}, where we established three difficulty levels (easy, medium, and hard), we now conduct a detailed analysis of how different configuration combinations affect task difficulty. We categorize these configurations into two primary dimensions: context length and semantic complexity. For each configuration setting in our analysis, we evaluate model performance on a sample of 50 questions.


\paragraph{Context Length}
We analyze the impact of context length by varying both question and candidate paper representations, as shown in \Cref{tab:length}. For questions, we compare using either the full paper or just the section containing the masked citation. For candidates, we compare using either the full paper or just the abstract. This creates four distinct length configurations while controlling for semantic difficulty.

The results show that both models generally perform better with more concise contexts. GPT-4o-mini maintains relatively stable performance across configurations, while Qwen2.5-7B shows significant degradation when given full candidate papers (dropping from 88\% to 40-42\% in the easy setting). This pattern persists in the hard setting, though with overall lower performance, suggesting that focused, relevant context may be more beneficial than comprehensive but potentially noisy full-paper information.

\paragraph{Semantic Complexity}
\Cref{tab:semantic} demonstrates how different semantic factors affect model performance on citation prediction. We analyze three key dimensions: citation type (attributional vs. descriptive), candidate sampling method (random vs. nearest), and candidate representation (full paper vs. body only).

The results show a clear hierarchy of difficulty. Both models perform best with attributional citations (GPT: 82\%, Qwen: 38-42\%), likely due to their more straightforward nature. Performance drops significantly with descriptive citations, particularly when combined with nearest-neighbor sampling.
While GPT-4o-mini maintains above-random performance across all configurations, Qwen's performance on the most challenging setting (descriptive, nearest sampling, full paper) drops to 22\%, below random chance (25\%). Interestingly, when using body-only candidates, Qwen's performance improves slightly to 28\%, suggesting that the previous drop might be due to context length limitations rather than semantic difficulty alone. These patterns validate our benchmark's difficulty levels while highlighting the importance of considering both semantic and context length effects.

%(GPT: 52\%, Qwen: 22\%). This suggests that distinguishing between semantically similar papers is more challenging than identifying random candidates. Using only the paper body further increases difficulty, indicating the importance of abstract and introduction sections for citation understanding. These patterns validate our benchmark's difficulty levels and show how semantic complexity can be systematically controlled.


\section{Related Work}\label{related_work}

\paragraph{Long-context Evaluation} Many approaches have been proposed to evaluate the ability of language models to utilize a longer context \citep{inftybench,loogle,bamboo,wang2024leavedocumentbehindbenchmarking,song2024countingstarsmultievidencepositionawarescalable}. Real-world evaluations cover long-document QA and summarization \citep{zeroscrolls,laban2024summaryhaystackchallengelongcontext}, mathematics and code understanding \citep{leval,zhao2024docmathevalevaluatingmathreasoning,wang2024mathhayautomatedbenchmarklongcontext}, domain specific analysis \citep{reddy-etal-2024-docfinqa}, and retrieval tasks \citep{helmet}, in varies languages \cite{qiu2024clongevalchinesebenchmarkevaluating}, formats \citep{zhang2024marathonracerealmlong}. 
These benchmarks often repurpose existing corpus, raising concerns over its difficulty and data contamination. Meanwhile, benchmarks using synthetic data focus on atomic abilities such as retrieval \citep{needle}, state tracking \cite{babilong}, data aggregation \citep{ruler}, multi-hop reasoning \citep{longbench} like code understanding. These benchmarks are controllable although they deviate from real-world usage. Our benchmark combines the two approaches by mining supervision signals from real-world scholarly data with frequent updates, which enables a more realistic and controllable evaluation setting of long context.    


\paragraph{Citation-based Benchmarking} Although scholar literature corpus has been extensively used in language model pretraining \citep{s2orc, pile}, its potential to evaluate long context utilization is not fully explored. A number of datasets focus on generating and recommending citations \citep{citebench, F_rber_2020, 10.1007/978-3-030-99736-6_19}. \citet{litsearch} creates a retrieval benchmark by constructing questions for inline citations using GPT-4 and manually creating questions. There are benchmarks testing models' abilities to answer questions based on papers.  QASPER \citep{qasper} focuses on answering questions about NLP papers, and LitQA \citep{Lala2023PaperQA} examines models' knowledge of biology works.  However, these works primarily focus on understanding individual papers or concepts rather than evaluating long-context comprehension across multiple papers. Our work bridges this gap by leveraging citation networks to create challenging long-context evaluation scenarios that better reflect how researchers process and connect scientific literature.


\section{Conclusion}

In this work, we introduced SCALAR, a novel benchmark designed to evaluate LLMs long-context understanding while mitigating data contamination. By leveraging citation networks from scholarly papers, automatically generates high-quality ground truth labels while controlling task difficulty. Our experiments with state-of-the-art LLMs reveal that while models can effectively handle simple citation matching, they struggle with deeper comprehension of complex, context-rich references. SCALAR offers a sustainable and evolving benchmark to track advancements in long-context processing, providing insights for future model development.

\section*{Limitation}
SCALAR currently focuses on cloze-style citation-matching tasks, which may not fully assess broader comprehension and reasoning abilities. Additionally, its scope is limited to computer science, restricting its applicability to other academic domains. To address these limitations, we plan to introduce diverse evaluation formats, while also expanding SCALAR to fields like biomedical and legal research for a more comprehensive assessment of long-context understanding.



% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix


\begin{figure*}[t]
	\small
	\begin{tcolorbox}[colframe=white, left=2.5mm, right=1.5mm]
You are given a paper with a placeholder ``**[MASKED\_CITATION]**'' in
its content. Your task is to select the most appropriate reference
from the provided reference list to replace the mask.\\
- The paper content is enclosed within <Paper> and </Paper>.\\
- The reference list is enclosed within <References> and </References>,\\
with each reference wrapped in <Candidate> and </Candidate>.\\
- After selecting the best-suited reference, output the index of that
reference in the following format:\\
<answer>index</answer>.\\
<Paper>\\
\blue{... BERT (*[MASKED\_CITATIO]**) or ...} \\
</Paper>\\
<References>\\
<Candidate>Candidate [0]:\\
\blue{... candidate content ...}\\
</Candidate>\\
<Candidate>Candidate [1]:\\
\blue{... candidate content ...}\\
</Candidate>\\
<Candidate>Candidate [2]:\\
\blue{... candidate content ...}\\
</Candidate>\\
<Candidate>Candidate [3]:\\
\blue{... candidate content ...}\\
</Candidate>\\
</References>\\
Remember to output the index of the selected reference enclosed within
<answer> and </answer>.\\
	\end{tcolorbox}
	\caption{The prompt template used for the questions.}
	\label{fig:prompt}
\end{figure*}

\section{Prompt use in SCALAR}
\label{sec:appendix_prompt}
The prompt template used for the questions is in \Cref{fig:prompt}.


\section{Evaluation Models}
\label{app:eval}
The list of models we evaluate includes:

\paragraph{Open-source models}

(1) Llama~\citep{touvron2023llama, touvron2023llama2, dubey2024llama} is a series of the most influential open-source LLMs developed by Meta.
We utilize the \textit{8B, 70B, and 405B Llama3.1-Instruct}  and the \textit{70B Llama3.3-Instruct} for evaluation.
(2) Qwen2.5-1M~\citep{yang2025qwen2} is a long-context variant of Qwen2.5, supporting a 1M-token context. 
We test the \textit{Qwen2.5-7B-Instruct-1M} version of this model.
% (3) \textit{Mistral-Nemo-Base-2407}, an instruction-fine-tuned version of Mistral NeMo, supporting a 128k-token context.
(3) \textit{GLM-4-9B-Chat}, the human preference-aligned version of GLM-4-9B in the GLM-4 series~\citep{glm2024chatglm}, launched by Zhipu AI.
% (5) \textit{DeepSeek-R1}~\cite{guo2025deepseek} is a reinforcement learning-enhanced model that combines cold-start data with iterative RL fine-tuning, achieving performance comparable to OpenAI-o1-1217 across various tasks.

\paragraph{Proprietary models}
(4) \textit{GPT-4o} and \textit{GPT4o-mini}, two proprietary long-context models from OpenAI.
(5) \textit{claude-3-5-haiku-20241022}, the fastest model for daily tasks in Anthropic's Claude.


\end{document}
