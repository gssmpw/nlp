% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@article{needle,
  title={{Needle In A Haystack} - Pressure Testing {LLM}s},
   author={Gregory Kamradt},
   year={2023},
   journal ={Github},  
   url={https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main},
}

@inproceedings{leval,
  title={L-eval: Instituting standardized evaluation for long context language models},
  author={An, Chenxin and Gong, Shansan and Zhong, Ming and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng},
  booktitle={ ICLR},
  year={2024}
}


@article{loogle,
  title={LooGLE: Can Long-Context Language Models Understand Long Contexts?},
  author={Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
  journal={ arXiv:2311.04939},
  year={2023}
}

@article{inftybench,
      title={$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens}, 
      author={Xinrong Zhang and Yingfa Chen and Shengding Hu and Zihang Xu and Junhao Chen and Moo Khai Hao and Xu Han and Zhen Leng Thai and Shuo Wang and Zhiyuan Liu and Maosong Sun},
      year={2024},
      journal={arXiv:2402.13718},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{bamboo,
  title={Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models},
  author={Dong, Zican and Tang, Tianyi and Li, Junyi and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={ arXiv:2309.13345},
  year={2023}
}

@article{lostmiddle,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the ACL},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@inproceedings{pplanalysis,
    title = "Do Long-Range Language Models Actually Use Long-Range Context?",
    author = "Sun, Simeng  and
      Krishna, Kalpesh  and
      Mattarella-Micke, Andrew  and
      Iyyer, Mohit",
    booktitle = "Proc.  of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
}

@misc{helmet,
      title={HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly}, 
      author={Howard Yen and Tianyu Gao and Minmin Hou and Ke Ding and Daniel Fleischer and Peter Izsak and Moshe Wasserblat and Danqi Chen},
      year={2024},
      eprint={2410.02694},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.02694}, 
}

@inproceedings{zeroscrolls,
    title = "{Z}ero{SCROLLS}: A Zero-Shot Benchmark for Long Text Understanding",
    author = "Shaham, Uri  and
      Ivgi, Maor  and
      Efrat, Avia  and
      Berant, Jonathan  and
      Levy, Omer",
    booktitle = " EMNLP ",
    year = "2023",
}

@article{longbench,
  title={{LongBench}: A Bilingual, Multitask Benchmark for Long Context Understanding},
  author={Bai, Yushi and others},
  journal={ arXiv:2308.14508},
  year={2023}
}


@article{gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and others},
  journal={ arXiv:2403.05530},
  year={2024}
}


@article{babilong,
  title={BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack},
  author={Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Rodkin, Ivan and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
  journal={arXiv preprint arXiv:2406.10149},
  year={2024}
}

@article{lifelongicl,
  title={Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack},
  author={Xu, Xiaoyue and Ye, Qinyuan and Ren, Xiang},
  journal={arXiv preprint arXiv:2407.16695},
  year={2024}
}

@article{manyiclgdm,
  title={Many-shot in-context learning},
  author={Agarwal, Rishabh and Singh, Avi and Zhang, Lei M and Bohnet, Bernd and Chan, Stephanie and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and Co-Reyes, John D and Chu, Eric and others},
  journal={arXiv preprint arXiv:2404.11018},
  year={2024}
}

@article{manyiclcmu,
  title={In-context learning with long-context models: An in-depth exploration},
  author={Bertsch, Amanda and Ivgi, Maor and Alon, Uri and Berant, Jonathan and Gormley, Matthew R and Neubig, Graham},
  journal={arXiv preprint arXiv:2405.00200},
  year={2024}
}

@article{nocha,
  title={One Thousand and One Pairs: A" novel" challenge for long-context language models},
  author={Karpinska, Marzena and Thai, Katherine and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2406.16264},
  year={2024}
}

@misc{ruler,
      title={RULER: What's the Real Context Size of Your Long-Context Language Models?}, 
      author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
      year={2024},
      eprint={2404.06654},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06654}, 
}

@misc{litsearch,
      title={LitSearch: A Retrieval Benchmark for Scientific Literature Search}, 
      author={Anirudh Ajith and Mengzhou Xia and Alexis Chevalier and Tanya Goyal and Danqi Chen and Tianyu Gao},
      year={2024},
      eprint={2407.18940},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2407.18940}, 
}

@misc{citebench,
      title={CiteBench: A benchmark for Scientific Citation Text Generation}, 
      author={Martin Funkquist and Ilia Kuznetsov and Yufang Hou and Iryna Gurevych},
      year={2023},
      eprint={2212.09577},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.09577}, 
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{yang2025qwen2,
  title={Qwen2. 5-1M Technical Report},
  author={Yang, An and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Huang, Haoyan and Jiang, Jiandong and Tu, Jianhong and Zhang, Jianwei and Zhou, Jingren and others},
  journal={arXiv preprint arXiv:2501.15383},
  year={2025}
}

@misc{anthropic2024,
  author = {Anthropic},
  title = {Introducing the next generation of Claude},
  year = {2024},
  url = {https://www.anthropic.com/news/claude-3-family}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{s2orc,
      title={S2ORC: The Semantic Scholar Open Research Corpus}, 
      author={Kyle Lo and Lucy Lu Wang and Mark Neumann and Rodney Kinney and Dan S. Weld},
      year={2020},
      eprint={1911.02782},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.02782}, 
}

@misc{pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2101.00027}, 
}

@InProceedings{10.1007/978-3-030-99736-6_19,
author="Gu, Nianlong
and Gao, Yingqiang
and Hahnloser, Richard H. R.",
editor="Hagen, Matthias
and Verberne, Suzan
and Macdonald, Craig
and Seifert, Christin
and Balog, Krisztian
and N{\o}rv{\aa}g, Kjetil
and Setty, Vinay",
title="Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-Based Reranking",
booktitle="Advances in Information Retrieval",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="274--288",
abstract="The goal of local citation recommendation is to recommend a missing reference from the local citation context and optionally also from the global context. To balance the tradeoff between speed and accuracy of citation recommendation in the context of a large-scale paper database, a viable approach is to first prefetch a limited number of relevant documents using efficient ranking methods and then to perform a fine-grained reranking using more sophisticated models. In that vein, BM25 has been found to be a tough-to-beat approach to prefetching, which is why recent work has focused mainly on the reranking step. Even so, we explore prefetching with nearest neighbor search among text embeddings constructed by a hierarchical attention network. When coupled with a SciBERT reranker fine-tuned on local citation recommendation tasks, our hierarchical Attention encoder (HAtten) achieves high prefetch recall for a given number of candidates to be reranked. Consequently, our reranker requires fewer prefetch candidates to rerank, yet still achieves state-of-the-art performance on various local citation recommendation datasets such as ACL-200, FullTextPeerRead, RefSeer, and arXiv.",
isbn="978-3-030-99736-6"
}



@article{F_rber_2020,
   title={Citation recommendation: approaches and datasets},
   volume={21},
   ISSN={1432-1300},
   url={http://dx.doi.org/10.1007/s00799-020-00288-2},
   DOI={10.1007/s00799-020-00288-2},
   number={4},
   journal={International Journal on Digital Libraries},
   publisher={Springer Science and Business Media LLC},
   author={FÃ¤rber, Michael and Jatowt, Adam},
   year={2020},
   month=aug, pages={375â€“405} }


@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}


@inproceedings{scienceqa,
    title={ScienceQA: Science Question Answering about the Physical World},
    author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
    booktitle={Findings of EMNLP},
    year={2022}
}

@inproceedings{qasper,
    title = "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
    author = "Dasigi, Pradeep  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Cohan, Arman  and
      Smith, Noah A.  and
      Gardner, Matt",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.365/",
    doi = "10.18653/v1/2021.naacl-main.365",
    pages = "4599--4610",
    abstract = "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate."
}
@misc{peer,
      title={PEER: A Collaborative Language Model}, 
      author={Timo Schick and Jane Dwivedi-Yu and Zhengbao Jiang and Fabio Petroni and Patrick Lewis and Gautier Izacard and Qingfei You and Christoforos Nalmpantis and Edouard Grave and Sebastian Riedel},
      year={2022},
      eprint={2208.11663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.11663}, 
}

@article{Lala2023PaperQA,
  title     = {PaperQA: Retrieval-Augmented Generative Agent for Scientific Research},
  author    = {Jakub L\'ala and Odhran Oâ€™Donoghue and Aleksandar Shtedritski and Sam Cox and Samuel G Rodriques and Andrew D White},
  year      = {2023},
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}


@misc{glm2024chatglm,
      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, 
      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
      year={2024},
      eprint={2406.12793},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@misc{li2024libraleaderboardresponsibleaibalanced,
      title={Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard of Safety and Capability},
      author={Haonan Li and Xudong Han and Zenan Zhai and Honglin Mu and Hao Wang and Zhenxuan Zhang and Yilin Geng and Shom Lin and Renxi Wang and Artem Shelmanov and Xiangyu Qi and Yuxia Wang and Donghai Hong and Youliang Yuan and Meng Chen and Haoqin Tu and Fajri Koto and Tatsuki Kuribayashi and Cong Zeng and Rishabh Bhardwaj and Bingchen Zhao and Yawen Duan and Yi Liu and Emad A. Alghamdi and Yaodong Yang and Yinpeng Dong and Soujanya Poria and Pengfei Liu and Zhengzhong Liu and Xuguang Ren and Eduard Hovy and Iryna Gurevych and Preslav Nakov and Monojit Choudhury and Timothy Baldwin},
      year={2024},
      eprint={2412.18551},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.18551},
}


@misc{kuratov2024searchneedles11mhaystack,
      title={In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss}, 
      author={Yuri Kuratov and Aydar Bulatov and Petr Anokhin and Dmitry Sorokin and Artyom Sorokin and Mikhail Burtsev},
      year={2024},
      eprint={2402.10790},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.10790}, 
}
@inproceedings{
wang2024needle,
title={Needle In A Multimodal Haystack},
author={Weiyun Wang and Shuibo Zhang and Yiming Ren and Yuchen Duan and Tiantong Li and Shuo Liu and Mengkang Hu and Zhe Chen and Kaipeng Zhang and Lewei Lu and Xizhou Zhu and Ping Luo and Yu Qiao and Jifeng Dai and Wenqi Shao and Wenhai Wang},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=U2pNwSuQqD}
}
@misc{roberts2024needlethreadingllmsfollow,
      title={Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?}, 
      author={Jonathan Roberts and Kai Han and Samuel Albanie},
      year={2024},
      eprint={2411.05000},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.05000}, 
}
@inproceedings{
chang2024booookscore,
title={BooookScore: A systematic exploration of book-length summarization in the era of {LLM}s},
author={Yapei Chang and Kyle Lo and Tanya Goyal and Mohit Iyyer},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=7Ttk3RzDeu}
}

@inproceedings{reddy-etal-2024-docfinqa,
    title = "{D}oc{F}in{QA}: A Long-Context Financial Reasoning Dataset",
    author = "Reddy, Varshini  and
      Koncel-Kedziorski, Rik  and
      Lai, Viet Dac  and
      Krumdick, Michael  and
      Lovering, Charles  and
      Tanner, Chris",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-short.42/",
    doi = "10.18653/v1/2024.acl-short.42",
    pages = "445--458",
}

@misc{wang2024mathhayautomatedbenchmarklongcontext,
      title={MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs}, 
      author={Lei Wang and Shan Dong and Yuhui Xu and Hanze Dong and Yalu Wang and Amrita Saha and Ee-Peng Lim and Caiming Xiong and Doyen Sahoo},
      year={2024},
      eprint={2410.04698},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.04698}, 
}

@misc{zhao2024docmathevalevaluatingmathreasoning,
      title={DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents}, 
      author={Yilun Zhao and Yitao Long and Hongjun Liu and Ryo Kamoi and Linyong Nan and Lyuhao Chen and Yixin Liu and Xiangru Tang and Rui Zhang and Arman Cohan},
      year={2024},
      eprint={2311.09805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09805}, 
}

@misc{laban2024summaryhaystackchallengelongcontext,
      title={Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems}, 
      author={Philippe Laban and Alexander R. Fabbri and Caiming Xiong and Chien-Sheng Wu},
      year={2024},
      eprint={2407.01370},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01370}, 
}

@misc{wang2024leavedocumentbehindbenchmarking,
      title={Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA}, 
      author={Minzheng Wang and Longze Chen and Cheng Fu and Shengyi Liao and Xinghua Zhang and Bingli Wu and Haiyang Yu and Nan Xu and Lei Zhang and Run Luo and Yunshui Li and Min Yang and Fei Huang and Yongbin Li},
      year={2024},
      eprint={2406.17419},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17419}, 
}

@misc{qiu2024clongevalchinesebenchmarkevaluating,
      title={CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models}, 
      author={Zexuan Qiu and Jingjing Li and Shijue Huang and Xiaoqi Jiao and Wanjun Zhong and Irwin King},
      year={2024},
      eprint={2403.03514},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.03514}, 
}

@misc{zhang2024marathonracerealmlong,
      title={Marathon: A Race Through the Realm of Long Context with Large Language Models}, 
      author={Lei Zhang and Yunshui Li and Ziqiang Liu and Jiaxi yang and Junhao Liu and Longze Chen and Run Luo and Min Yang},
      year={2024},
      eprint={2312.09542},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.09542}, 
}

@misc{song2024countingstarsmultievidencepositionawarescalable,
      title={Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models}, 
      author={Mingyang Song and Mao Zheng and Xuan Luo},
      year={2024},
      eprint={2403.11802},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.11802}, 
}