@InProceedings{10.1007/978-3-030-99736-6_19,
author="Gu, Nianlong
and Gao, Yingqiang
and Hahnloser, Richard H. R.",
editor="Hagen, Matthias
and Verberne, Suzan
and Macdonald, Craig
and Seifert, Christin
and Balog, Krisztian
and N{\o}rv{\aa}g, Kjetil
and Setty, Vinay",
title="Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-Based Reranking",
booktitle="Advances in Information Retrieval",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="274--288",
abstract="The goal of local citation recommendation is to recommend a missing reference from the local citation context and optionally also from the global context. To balance the tradeoff between speed and accuracy of citation recommendation in the context of a large-scale paper database, a viable approach is to first prefetch a limited number of relevant documents using efficient ranking methods and then to perform a fine-grained reranking using more sophisticated models. In that vein, BM25 has been found to be a tough-to-beat approach to prefetching, which is why recent work has focused mainly on the reranking step. Even so, we explore prefetching with nearest neighbor search among text embeddings constructed by a hierarchical attention network. When coupled with a SciBERT reranker fine-tuned on local citation recommendation tasks, our hierarchical Attention encoder (HAtten) achieves high prefetch recall for a given number of candidates to be reranked. Consequently, our reranker requires fewer prefetch candidates to rerank, yet still achieves state-of-the-art performance on various local citation recommendation datasets such as ACL-200, FullTextPeerRead, RefSeer, and arXiv.",
isbn="978-3-030-99736-6"
}

@article{F_rber_2020,
   title={Citation recommendation: approaches and datasets},
   volume={21},
   ISSN={1432-1300},
   url={http://dx.doi.org/10.1007/s00799-020-00288-2},
   DOI={10.1007/s00799-020-00288-2},
   number={4},
   journal={International Journal on Digital Libraries},
   publisher={Springer Science and Business Media LLC},
   author={Färber, Michael and Jatowt, Adam},
   year={2020},
   month=aug, pages={375–405} }

@article{Lala2023PaperQA,
  title     = {PaperQA: Retrieval-Augmented Generative Agent for Scientific Research},
  author    = {Jakub L\'ala and Odhran O’Donoghue and Aleksandar Shtedritski and Sam Cox and Samuel G Rodriques and Andrew D White},
  year      = {2023},
}

@article{babilong,
  title={BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack},
  author={Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Rodkin, Ivan and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
  journal={arXiv preprint arXiv:2406.10149},
  year={2024}
}

@article{bamboo,
  title={Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models},
  author={Dong, Zican and Tang, Tianyi and Li, Junyi and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={ arXiv:2309.13345},
  year={2023}
}

@misc{citebench,
      title={CiteBench: A benchmark for Scientific Citation Text Generation}, 
      author={Martin Funkquist and Ilia Kuznetsov and Yufang Hou and Iryna Gurevych},
      year={2023},
      eprint={2212.09577},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.09577}, 
}

@misc{helmet,
      title={HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly}, 
      author={Howard Yen and Tianyu Gao and Minmin Hou and Ke Ding and Daniel Fleischer and Peter Izsak and Moshe Wasserblat and Danqi Chen},
      year={2024},
      eprint={2410.02694},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.02694}, 
}

@article{inftybench,
      title={$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens}, 
      author={Xinrong Zhang and Yingfa Chen and Shengding Hu and Zihang Xu and Junhao Chen and Moo Khai Hao and Xu Han and Zhen Leng Thai and Shuo Wang and Zhiyuan Liu and Maosong Sun},
      year={2024},
      journal={arXiv:2402.13718},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{laban2024summaryhaystackchallengelongcontext,
      title={Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems}, 
      author={Philippe Laban and Alexander R. Fabbri and Caiming Xiong and Chien-Sheng Wu},
      year={2024},
      eprint={2407.01370},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01370}, 
}

@inproceedings{leval,
  title={L-eval: Instituting standardized evaluation for long context language models},
  author={An, Chenxin and Gong, Shansan and Zhong, Ming and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng},
  booktitle={ ICLR},
  year={2024}
}

@misc{litsearch,
      title={LitSearch: A Retrieval Benchmark for Scientific Literature Search}, 
      author={Anirudh Ajith and Mengzhou Xia and Alexis Chevalier and Tanya Goyal and Danqi Chen and Tianyu Gao},
      year={2024},
      eprint={2407.18940},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2407.18940}, 
}

@article{longbench,
  title={{LongBench}: A Bilingual, Multitask Benchmark for Long Context Understanding},
  author={Bai, Yushi and others},
  journal={ arXiv:2308.14508},
  year={2023}
}

@article{loogle,
  title={LooGLE: Can Long-Context Language Models Understand Long Contexts?},
  author={Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
  journal={ arXiv:2311.04939},
  year={2023}
}

@article{needle,
  title={{Needle In A Haystack} - Pressure Testing {LLM}s},
   author={Gregory Kamradt},
   year={2023},
   journal ={Github},  
   url={https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main},
}

@misc{pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2101.00027}, 
}

@inproceedings{qasper,
    title = "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
    author = "Dasigi, Pradeep  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Cohan, Arman  and
      Smith, Noah A.  and
      Gardner, Matt",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.365/",
    doi = "10.18653/v1/2021.naacl-main.365",
    pages = "4599--4610",
    abstract = "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate."
}

@misc{qiu2024clongevalchinesebenchmarkevaluating,
      title={CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models}, 
      author={Zexuan Qiu and Jingjing Li and Shijue Huang and Xiaoqi Jiao and Wanjun Zhong and Irwin King},
      year={2024},
      eprint={2403.03514},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.03514}, 
}

@inproceedings{reddy-etal-2024-docfinqa,
    title = "{D}oc{F}in{QA}: A Long-Context Financial Reasoning Dataset",
    author = "Reddy, Varshini  and
      Koncel-Kedziorski, Rik  and
      Lai, Viet Dac  and
      Krumdick, Michael  and
      Lovering, Charles  and
      Tanner, Chris",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-short.42/",
    doi = "10.18653/v1/2024.acl-short.42",
    pages = "445--458",
}

@misc{ruler,
      title={RULER: What's the Real Context Size of Your Long-Context Language Models?}, 
      author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
      year={2024},
      eprint={2404.06654},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06654}, 
}

@misc{s2orc,
      title={S2ORC: The Semantic Scholar Open Research Corpus}, 
      author={Kyle Lo and Lucy Lu Wang and Mark Neumann and Rodney Kinney and Dan S. Weld},
      year={2020},
      eprint={1911.02782},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.02782}, 
}

@misc{song2024countingstarsmultievidencepositionawarescalable,
      title={Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models}, 
      author={Mingyang Song and Mao Zheng and Xuan Luo},
      year={2024},
      eprint={2403.11802},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.11802}, 
}

@misc{wang2024leavedocumentbehindbenchmarking,
      title={Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA}, 
      author={Minzheng Wang and Longze Chen and Cheng Fu and Shengyi Liao and Xinghua Zhang and Bingli Wu and Haiyang Yu and Nan Xu and Lei Zhang and Run Luo and Yunshui Li and Min Yang and Fei Huang and Yongbin Li},
      year={2024},
      eprint={2406.17419},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17419}, 
}

@misc{wang2024mathhayautomatedbenchmarklongcontext,
      title={MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs}, 
      author={Lei Wang and Shan Dong and Yuhui Xu and Hanze Dong and Yalu Wang and Amrita Saha and Ee-Peng Lim and Caiming Xiong and Doyen Sahoo},
      year={2024},
      eprint={2410.04698},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.04698}, 
}

@inproceedings{zeroscrolls,
    title = "{Z}ero{SCROLLS}: A Zero-Shot Benchmark for Long Text Understanding",
    author = "Shaham, Uri  and
      Ivgi, Maor  and
      Efrat, Avia  and
      Berant, Jonathan  and
      Levy, Omer",
    booktitle = " EMNLP ",
    year = "2023",
}

@misc{zhang2024marathonracerealmlong,
      title={Marathon: A Race Through the Realm of Long Context with Large Language Models}, 
      author={Lei Zhang and Yunshui Li and Ziqiang Liu and Jiaxi yang and Junhao Liu and Longze Chen and Run Luo and Min Yang},
      year={2024},
      eprint={2312.09542},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.09542}, 
}

@misc{zhao2024docmathevalevaluatingmathreasoning,
      title={DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents}, 
      author={Yilun Zhao and Yitao Long and Hongjun Liu and Ryo Kamoi and Linyong Nan and Lyuhao Chen and Yixin Liu and Xiangru Tang and Rui Zhang and Arman Cohan},
      year={2024},
      eprint={2311.09805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09805}, 
}

