\section{Related Work}
\label{related_work}

\paragraph{Long-context Evaluation} Many approaches have been proposed to evaluate the ability of language models to utilize a longer context **Devlin, "BERT Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Real-world evaluations cover long-document QA and summarization **Karpukhin, "Dense Passage Retrieval for Open-Domain Question Answering"** ____**, mathematics and code understanding **Alon, "Mathematics and Code: A Challenge Set for Mathematical Reasoning and Code Understanding"** ____**, domain specific analysis **Wadden, "Evaluating the Robustness of Language Models to Natural Language Attacks"** ____**, and retrieval tasks ____ in varies languages ____ formats. 
These benchmarks often repurpose existing corpus, raising concerns over its difficulty and data contamination. Meanwhile, benchmarks using synthetic data focus on atomic abilities such as retrieval ____ state tracking ____ data aggregation ____ multi-hop reasoning ____ like code understanding. These benchmarks are controllable although they deviate from real-world usage. Our benchmark combines the two approaches by mining supervision signals from real-world scholarly data with frequent updates, which enables a more realistic and controllable evaluation setting of long context.

\paragraph{Citation-based Benchmarking} Although scholar literature corpus has been extensively used in language model pretraining ____ its potential to evaluate long context utilization is not fully explored. A number of datasets focus on generating and recommending citations ____ creates a retrieval benchmark by constructing questions for inline citations using GPT-4 and manually creating questions. There are benchmarks testing models' abilities to answer questions based on papers.  QASPER ____ focuses on answering questions about NLP papers, and LitQA ____ examines models' knowledge of biology works.  However, these works primarily focus on understanding individual papers or concepts rather than evaluating long-context comprehension across multiple papers. Our work bridges this gap by leveraging citation networks to create challenging long-context evaluation scenarios that better reflect how researchers process and connect scientific literature.