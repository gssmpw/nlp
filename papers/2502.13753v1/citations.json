[
  {
    "index": 0,
    "papers": [
      {
        "key": "inftybench",
        "author": "Xinrong Zhang and Yingfa Chen and Shengding Hu and Zihang Xu and Junhao Chen and Moo Khai Hao and Xu Han and Zhen Leng Thai and Shuo Wang and Zhiyuan Liu and Maosong Sun",
        "title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens"
      },
      {
        "key": "loogle",
        "author": "Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan",
        "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?"
      },
      {
        "key": "bamboo",
        "author": "Dong, Zican and Tang, Tianyi and Li, Junyi and Zhao, Wayne Xin and Wen, Ji-Rong",
        "title": "Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models"
      },
      {
        "key": "wang2024leavedocumentbehindbenchmarking",
        "author": "Minzheng Wang and Longze Chen and Cheng Fu and Shengyi Liao and Xinghua Zhang and Bingli Wu and Haiyang Yu and Nan Xu and Lei Zhang and Run Luo and Yunshui Li and Min Yang and Fei Huang and Yongbin Li",
        "title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA"
      },
      {
        "key": "song2024countingstarsmultievidencepositionawarescalable",
        "author": "Mingyang Song and Mao Zheng and Xuan Luo",
        "title": "Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zeroscrolls",
        "author": "Shaham, Uri  and\nIvgi, Maor  and\nEfrat, Avia  and\nBerant, Jonathan  and\nLevy, Omer",
        "title": "{Z}ero{SCROLLS}: A Zero-Shot Benchmark for Long Text Understanding"
      },
      {
        "key": "laban2024summaryhaystackchallengelongcontext",
        "author": "Philippe Laban and Alexander R. Fabbri and Caiming Xiong and Chien-Sheng Wu",
        "title": "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "leval",
        "author": "An, Chenxin and Gong, Shansan and Zhong, Ming and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng",
        "title": "L-eval: Instituting standardized evaluation for long context language models"
      },
      {
        "key": "zhao2024docmathevalevaluatingmathreasoning",
        "author": "Yilun Zhao and Yitao Long and Hongjun Liu and Ryo Kamoi and Linyong Nan and Lyuhao Chen and Yixin Liu and Xiangru Tang and Rui Zhang and Arman Cohan",
        "title": "DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents"
      },
      {
        "key": "wang2024mathhayautomatedbenchmarklongcontext",
        "author": "Lei Wang and Shan Dong and Yuhui Xu and Hanze Dong and Yalu Wang and Amrita Saha and Ee-Peng Lim and Caiming Xiong and Doyen Sahoo",
        "title": "MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "reddy-etal-2024-docfinqa",
        "author": "Reddy, Varshini  and\nKoncel-Kedziorski, Rik  and\nLai, Viet Dac  and\nKrumdick, Michael  and\nLovering, Charles  and\nTanner, Chris",
        "title": "{D}oc{F}in{QA}: A Long-Context Financial Reasoning Dataset"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "helmet",
        "author": "Howard Yen and Tianyu Gao and Minmin Hou and Ke Ding and Daniel Fleischer and Peter Izsak and Moshe Wasserblat and Danqi Chen",
        "title": "HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "qiu2024clongevalchinesebenchmarkevaluating",
        "author": "Zexuan Qiu and Jingjing Li and Shijue Huang and Xiaoqi Jiao and Wanjun Zhong and Irwin King",
        "title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2024marathonracerealmlong",
        "author": "Lei Zhang and Yunshui Li and Ziqiang Liu and Jiaxi yang and Junhao Liu and Longze Chen and Run Luo and Min Yang",
        "title": "Marathon: A Race Through the Realm of Long Context with Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "needle",
        "author": "Gregory Kamradt",
        "title": "{Needle In A Haystack} - Pressure Testing {LLM}s"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "babilong",
        "author": "Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Rodkin, Ivan and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail",
        "title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ruler",
        "author": "Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg",
        "title": "RULER: What's the Real Context Size of Your Long-Context Language Models?"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "longbench",
        "author": "Bai, Yushi and others",
        "title": "{LongBench}: A Bilingual, Multitask Benchmark for Long Context Understanding"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "s2orc",
        "author": "Kyle Lo and Lucy Lu Wang and Mark Neumann and Rodney Kinney and Dan S. Weld",
        "title": "S2ORC: The Semantic Scholar Open Research Corpus"
      },
      {
        "key": "pile",
        "author": "Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy",
        "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "citebench",
        "author": "Martin Funkquist and Ilia Kuznetsov and Yufang Hou and Iryna Gurevych",
        "title": "CiteBench: A benchmark for Scientific Citation Text Generation"
      },
      {
        "key": "F_rber_2020",
        "author": "F\u00e4rber, Michael and Jatowt, Adam",
        "title": "Citation recommendation: approaches and datasets"
      },
      {
        "key": "10.1007/978-3-030-99736-6_19",
        "author": "Gu, Nianlong\nand Gao, Yingqiang\nand Hahnloser, Richard H. R.",
        "title": "Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-Based Reranking"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "litsearch",
        "author": "Anirudh Ajith and Mengzhou Xia and Alexis Chevalier and Tanya Goyal and Danqi Chen and Tianyu Gao",
        "title": "LitSearch: A Retrieval Benchmark for Scientific Literature Search"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "qasper",
        "author": "Dasigi, Pradeep  and\nLo, Kyle  and\nBeltagy, Iz  and\nCohan, Arman  and\nSmith, Noah A.  and\nGardner, Matt",
        "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "Lala2023PaperQA",
        "author": "Jakub L\\'ala and Odhran O\u2019Donoghue and Aleksandar Shtedritski and Sam Cox and Samuel G Rodriques and Andrew D White",
        "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research"
      }
    ]
  }
]