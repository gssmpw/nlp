\section{Related Work}
\label{related_work}

\paragraph{Long-context Evaluation} Many approaches have been proposed to evaluate the ability of language models to utilize a longer context \citep{inftybench,loogle,bamboo,wang2024leavedocumentbehindbenchmarking,song2024countingstarsmultievidencepositionawarescalable}. Real-world evaluations cover long-document QA and summarization \citep{zeroscrolls,laban2024summaryhaystackchallengelongcontext}, mathematics and code understanding \citep{leval,zhao2024docmathevalevaluatingmathreasoning,wang2024mathhayautomatedbenchmarklongcontext}, domain specific analysis \citep{reddy-etal-2024-docfinqa}, and retrieval tasks \citep{helmet}, in varies languages \cite{qiu2024clongevalchinesebenchmarkevaluating}, formats \citep{zhang2024marathonracerealmlong}. 
These benchmarks often repurpose existing corpus, raising concerns over its difficulty and data contamination. Meanwhile, benchmarks using synthetic data focus on atomic abilities such as retrieval \citep{needle}, state tracking \cite{babilong}, data aggregation \citep{ruler}, multi-hop reasoning \citep{longbench} like code understanding. These benchmarks are controllable although they deviate from real-world usage. Our benchmark combines the two approaches by mining supervision signals from real-world scholarly data with frequent updates, which enables a more realistic and controllable evaluation setting of long context.    


\paragraph{Citation-based Benchmarking} Although scholar literature corpus has been extensively used in language model pretraining \citep{s2orc, pile}, its potential to evaluate long context utilization is not fully explored. A number of datasets focus on generating and recommending citations \citep{citebench, F_rber_2020, 10.1007/978-3-030-99736-6_19}. \citet{litsearch} creates a retrieval benchmark by constructing questions for inline citations using GPT-4 and manually creating questions. There are benchmarks testing models' abilities to answer questions based on papers.  QASPER \citep{qasper} focuses on answering questions about NLP papers, and LitQA \citep{Lala2023PaperQA} examines models' knowledge of biology works.  However, these works primarily focus on understanding individual papers or concepts rather than evaluating long-context comprehension across multiple papers. Our work bridges this gap by leveraging citation networks to create challenging long-context evaluation scenarios that better reflect how researchers process and connect scientific literature.