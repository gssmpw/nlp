\section{Background and Related Work}
LLMs have demonstrated impressive performance across a wide spectrum of tasks **Brown et al., "Large Language Models"**. Despite the progress, their performance on customized downstream tasks can be significantly optimized by supervised fine-tuning (SFT) with instruction and human-written responses pairs **Stiennon et al., "Improving Prompt-Based Few-Shot Learning"**. Reinforcement learning with preference data has further shown success due to the easier-to-collect data form **Wang et al., "Preference-based Reinforcement Learning"**. Representative methods include Proximal Policy Optimization (PPO) **Schulman et al., "Proximal Policy Optimization Algorithms"**,** Sutton and Barto, "Reinforcement Learning: An Introduction"**, which optimizes the LLM with a separate reward model, and Direct Preference Optimization (DPO) **Christiano et al., "Deep Reinforcement Learning from Human Preferences"**,** Wang et al., "Preference-based Reinforcement Learning"**, which directly learns from the preference data. Although an easier data collection is available, these methods still largely rely on the richness and quality of the preference data **Wang et al., "Preference-based Reinforcement Learning"**.

\vspace{-0.1in}

% Provide a very short background on fine-tuning, specifically fine-tuning oriented services. Talk about why quality data is important there, especially human preference is important. Then, describe the corresponding techniques and their limiotations. 
\subsection{Alignment with External Feedback}
% \begin{itemize}
%     \item RLHF: gold standard, but many human effort wasted on what LLM already knows
%     \item RLAIF: bounded by the capability of the feedback LLM, stuggling with unseen data/tasks
%     \item \myname  strategically direct human effort to unseen/hard high-value tasks, effectively use minimum human effort to achieve maximun alignment
\vspace{-0.05in}
Human feedback is regarded as the golden standard in LLM alignment. However, reinforcement learning from human feedback (RLHF) **Bhagat et al., "Reinforcement Learning with Human Feedback"** typically incorporates heavy and expensive crowdsourcing efforts or expert annotations to guarantee data diversity and richness. To relieve the reliance on human effort, reinforcement learning with AI feedback (RLAIF) **Wang et al., "Preference-based Reinforcement Learning"** provides an alternative that collects feedback from stronger LLMs instead of humans. On the other hand, this method is limited by the capability of the stronger LLM annotators **Schmidhuber, "Long Short-Term Memory"**, especially for customized tasks, and suffers from their intrinsic biases **Bengio et al., "Deep Learning"**.
In this paper, we take advantage of RLAIF to establish an initial alignment and strategically incorporate human feedback to efficiently bring LLMs to the true alignment.
    
    
% \end{itemize}
\vspace{-0.1in}
\subsection{LLM Self-Improvement}
\vspace{-0.05in}
% \begin{itemize}
%     \item Self-Rewarding LM/SER: bounded by the intrinsic capability of the LLM itself, stuggling with unseen data/tasks
%     \item \myname introduces new human intelligence and break the upper bound of LLM in domain understanding
% \end{itemize}

To break the upper bound of LLMs, recent efforts have been devoted to enabling LLMs to self-improve. Self-Rewarding LMs **Tessler et al., "Reward Augmented Maximum Likelihood for Deep Generative Models"** and Math-shepherd **Wang et al., "Preference-based Reinforcement Learning"** demonstrate the possibility of LLM self-improvement with reward signals from itself. SELF-ALIGN **Li et al., "Self-Distillation as a Model-Based Approach to Transfer Learning"** uses a carefully written set of principles to guide LLMs through self-improvement. SER **Ravi and Dasari, "Efficient Adversarial Training via Promoting Input Similarity"** starts with only a fraction of human annotations to achieve full-annotation performance by progressively generating additional training data for itself. 
However, these methods still suffer from the intrinsic upper bound of LLMs and self-improvement is not guaranteed for customized tasks.
\myname{}, on the other hand, efficiently introduces new human intelligence into the improvement process, thereby ensuring that the improvement is not bounded by LLMs' initial lack of domain understanding.

% \subsection{Synthetic Dataset}
% \begin{itemize}
%     \item Instruction back-translation: not suitable for domain-specific tasks with dedicated data
%     \item Reading comprehension, TLDR: rely on heuristics which needs data visibility
%     \item \myname only require few-shot efforts from trusted experts and auto-adapts to any domain without prior knowledge
% \end{itemize}