\section{Background and Related Work}
LLMs have demonstrated impressive performance across a wide spectrum of tasks____. Despite the progress, their performance on customized downstream tasks can be significantly optimized by supervised fine-tuning (SFT) with instruction and human-written responses pairs____. Reinforcement learning with preference data has further shown success due to the easier-to-collect data form____. Representative methods include Proximal Policy Optimization
(PPO)____, which optimizes the LLM with a separate reward model, and Direct Preference Optimization (DPO)____, which directly learns from the preference data. Although an easier data collection is available, these methods still largely rely on the richness and quality of the preference data____.

\vspace{-0.1in}

% Provide a very short background on fine-tuning, specifically fine-tuning oriented services. Talk about why quality data is important there, especially human preference is important. Then, describe the corresponding techniques and their limiotations. 
\subsection{Alignment with External Feedback}
% \begin{itemize}
%     \item RLHF: gold standard, but many human effort wasted on what LLM already knows
%     \item RLAIF: bounded by the capability of the feedback LLM, stuggling with unseen data/tasks
%     \item \myname  strategically direct human effort to unseen/hard high-value tasks, effectively use minimum human effort to achieve maximun alignment
\vspace{-0.05in}
Human feedback is regarded as the golden standard in LLM alignment. However, reinforcement learning from human feedback (RLHF)____ typically incorporates heavy and expensive crowdsourcing efforts or expert annotations to guarantee data diversity and richness. To relieve the reliance on human effort, reinforcement learning with AI feedback (RLAIF)____ provides an alternative that collects feedback from stronger LLMs instead of humans. On the other hand, this method is limited by the capability of the stronger LLM annotators____ especially for customized tasks, and suffers from their intrinsic biases____.
In this paper, we take advantage of RLAIF to establish an initial alignment and strategically incorporate human feedback to efficiently bring LLMs to the true alignment.
    
    
% \end{itemize}
\vspace{-0.1in}
\subsection{LLM Self-Improvement}
\vspace{-0.05in}
% \begin{itemize}
%     \item Self-Rewarding LM/SER: bounded by the intrinsic capability of the LLM itself, stuggling with unseen data/tasks
%     \item \myname introduces new human intelligence and break the upper bound of LLM in domain understanding
% \end{itemize}

To break the upper bound of LLMs, recent efforts have been devoted to enabling LLMs to self-improve. Self-Rewarding LMs____ and Math-shepherd____ demonstrate the possibility of LLM self-improvement with reward signals from itself. SELF-ALIGN____ uses a carefully written set of principles to guide LLMs through self-improvement. SER____ starts with only a fraction of human annotations to achieve full-annotation performance by progressively generating additional training data for itself. 
However, these methods still suffer from the intrinsic upper bound of LLMs and self-improvement is not guaranteed for customized tasks.
\myname{}, on the other hand, efficiently introduces new human intelligence into the improvement process, thereby ensuring that the improvement is not bounded by LLMs' initial lack of domain understanding.

% \subsection{Synthetic Dataset}
% \begin{itemize}
%     \item Instruction back-translation: not suitable for domain-specific tasks with dedicated data
%     \item Reading comprehension, TLDR: rely on heuristics which needs data visibility
%     \item \myname only require few-shot efforts from trusted experts and auto-adapts to any domain without prior knowledge
% \end{itemize}