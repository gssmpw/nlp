[
  {
    "index": 0,
    "papers": [
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      },
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      },
      {
        "key": "team2023gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others",
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chung2024scaling",
        "author": "Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others",
        "title": "Scaling instruction-finetuned language models"
      },
      {
        "key": "thoppilan2022lamda",
        "author": "Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others",
        "title": "Lamda: Language models for dialog applications"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "lee2023rlaif",
        "author": "Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie Ren and Mesnard, Thomas and Ferret, Johan and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav",
        "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "xu2024dpo",
        "author": "Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi",
        "title": "Is dpo superior to ppo for llm alignment? a comprehensive study"
      },
      {
        "key": "zheng2023secrets",
        "author": "Zheng, Rui and Dou, Shihan and Gao, Songyang and Hua, Yuan and Shen, Wei and Wang, Binghai and Liu, Yan and Jin, Senjie and Liu, Qin and Zhou, Yuhao and others",
        "title": "Secrets of rlhf in large language models part i: Ppo"
      },
      {
        "key": "wang2024secrets",
        "author": "Wang, Binghai and Zheng, Rui and Chen, Lu and Liu, Yan and Dou, Shihan and Huang, Caishuang and Shen, Wei and Jin, Senjie and Zhou, Enyu and Shi, Chenyu and others",
        "title": "Secrets of rlhf in large language models part ii: Reward modeling"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "kopf2024openassistant",
        "author": "K{\\\"o}pf, Andreas and Kilcher, Yannic and von R{\\\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi Rui and Stevens, Keith and Barhoum, Abdullah and Nguyen, Duc and Stanley, Oliver and Nagyfi, Rich{\\'a}rd and others",
        "title": "Openassistant conversations-democratizing large language model alignment"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "lee2023rlaif",
        "author": "Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie Ren and Mesnard, Thomas and Ferret, Johan and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav",
        "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      },
      {
        "key": "bai2022constitutional",
        "author": "Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others",
        "title": "Constitutional ai: Harmlessness from ai feedback"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "huang2024self",
        "author": "Huang, Chenghua and Fan, Zhizhen and Wang, Lu and Yang, Fangkai and Zhao, Pu and Lin, Zeqi and Lin, Qingwei and Zhang, Dongmei and Rajmohan, Saravan and Zhang, Qi",
        "title": "Self-Evolved Reward Learning for LLMs"
      },
      {
        "key": "sharma2024critical",
        "author": "Sharma, Archit and Keh, Sedrick and Mitchell, Eric and Finn, Chelsea and Arora, Kushal and Kollar, Thomas",
        "title": "A critical evaluation of ai feedback for aligning large language models"
      },
      {
        "key": "lee2023rlaif",
        "author": "Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie Ren and Mesnard, Thomas and Ferret, Johan and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav",
        "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zheng2023judging",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",
        "title": "Judging llm-as-a-judge with mt-bench and chatbot arena"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "yuan2024self",
        "author": "Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason",
        "title": "Self-rewarding language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2024math",
        "author": "Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang",
        "title": "Math-shepherd: Verify and reinforce llms step-by-step without human annotations"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "sun2024principle",
        "author": "Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang",
        "title": "Principle-driven self-alignment of language models from scratch with minimal human supervision"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "huang2024self",
        "author": "Huang, Chenghua and Fan, Zhizhen and Wang, Lu and Yang, Fangkai and Zhao, Pu and Lin, Zeqi and Lin, Qingwei and Zhang, Dongmei and Rajmohan, Saravan and Zhang, Qi",
        "title": "Self-Evolved Reward Learning for LLMs"
      }
    ]
  }
]