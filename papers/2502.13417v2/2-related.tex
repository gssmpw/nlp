\section{Background and Related Work}
LLMs have demonstrated impressive performance across a wide spectrum of tasks~\cite{achiam2023gpt, dubey2024llama, team2023gemini}. Despite the progress, their performance on customized downstream tasks can be significantly optimized by supervised fine-tuning (SFT) with instruction and human-written responses pairs~\cite{chung2024scaling, thoppilan2022lamda}. Reinforcement learning with preference data has further shown success due to the easier-to-collect data form~\cite{ouyang2022training, stiennon2020learning, lee2023rlaif}. Representative methods include Proximal Policy Optimization
(PPO)~\cite{schulman2017proximal}, which optimizes the LLM with a separate reward model, and Direct Preference Optimization (DPO)~\cite{rafailov2024direct}, which directly learns from the preference data. Although an easier data collection is available, these methods still largely rely on the richness and quality of the preference data~\cite{xu2024dpo,zheng2023secrets, wang2024secrets}.

\vspace{-0.1in}

% Provide a very short background on fine-tuning, specifically fine-tuning oriented services. Talk about why quality data is important there, especially human preference is important. Then, describe the corresponding techniques and their limiotations. 
\subsection{Alignment with External Feedback}
% \begin{itemize}
%     \item RLHF: gold standard, but many human effort wasted on what LLM already knows
%     \item RLAIF: bounded by the capability of the feedback LLM, stuggling with unseen data/tasks
%     \item \myname  strategically direct human effort to unseen/hard high-value tasks, effectively use minimum human effort to achieve maximun alignment
\vspace{-0.05in}
Human feedback is regarded as the golden standard in LLM alignment. However, reinforcement learning from human feedback (RLHF)~\cite{ouyang2022training, stiennon2020learning, kopf2024openassistant} typically incorporates heavy and expensive crowdsourcing efforts or expert annotations to guarantee data diversity and richness. To relieve the reliance on human effort, reinforcement learning with AI feedback (RLAIF)~\cite{lee2023rlaif, bai2022constitutional} provides an alternative that collects feedback from stronger LLMs instead of humans. On the other hand, this method is limited by the capability of the stronger LLM annotators~\cite{huang2024self, sharma2024critical, lee2023rlaif} especially for customized tasks, and suffers from their intrinsic biases~\cite{zheng2023judging}.
In this paper, we take advantage of RLAIF to establish an initial alignment and strategically incorporate human feedback to efficiently bring LLMs to the true alignment.
    
    
% \end{itemize}
\vspace{-0.1in}
\subsection{LLM Self-Improvement}
\vspace{-0.05in}
% \begin{itemize}
%     \item Self-Rewarding LM/SER: bounded by the intrinsic capability of the LLM itself, stuggling with unseen data/tasks
%     \item \myname introduces new human intelligence and break the upper bound of LLM in domain understanding
% \end{itemize}

To break the upper bound of LLMs, recent efforts have been devoted to enabling LLMs to self-improve. Self-Rewarding LMs~\cite{yuan2024self} and Math-shepherd~\cite{wang2024math} demonstrate the possibility of LLM self-improvement with reward signals from itself. SELF-ALIGN~\cite{sun2024principle} uses a carefully written set of principles to guide LLMs through self-improvement. SER~\cite{huang2024self} starts with only a fraction of human annotations to achieve full-annotation performance by progressively generating additional training data for itself. 
However, these methods still suffer from the intrinsic upper bound of LLMs and self-improvement is not guaranteed for customized tasks.
\myname{}, on the other hand, efficiently introduces new human intelligence into the improvement process, thereby ensuring that the improvement is not bounded by LLMs' initial lack of domain understanding.

% \subsection{Synthetic Dataset}
% \begin{itemize}
%     \item Instruction back-translation: not suitable for domain-specific tasks with dedicated data
%     \item Reading comprehension, TLDR: rely on heuristics which needs data visibility
%     \item \myname only require few-shot efforts from trusted experts and auto-adapts to any domain without prior knowledge
% \end{itemize}