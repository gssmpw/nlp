\section{Introduction}
\label{sec:intro}
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{figures/sargy_pipeline-6.pdf}
  \caption{Overview of \myname{} process. \myname{} starts with coarse LLM alignment on the task. It then iteratively takes targeted human feedback to reach the complete human alignment, leveraging reward distribution of a reward model in its training dataset.}
  \label{fig:overview}
\end{figure*}

In recent years, large language models (LLMs) have demonstrated remarkable advancements, unlocking new possibilities across a wide range of applications~\cite{touvron2023llama, jiang2024mixtral, achiam2023gpt, team2023gemini}. As these models become more powerful, the focus has shifted toward customization, i.e., fine-tuning base models to better serve specific tasks and user needs~\cite{wei2021finetuned, li2023self}. Companies are increasingly investing in solutions built upon fine-tuned models, recognizing the value of adapting LLMs to align with end-user preferences, including intent, style, grounding, and compliance requirements~\cite{ft_service, m365, nuances, folio}. A key approach to achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which has emerged as a widely adopted technique in the literature for refining model behavior based on human feedback~\cite{bai2022training, stiennon2020learning, rafailov2024direct, wang2024secrets, ouyang2022training}.
% Large Language Models (LLMs) have achieved impressive success in a wide range of natural language processing (NLP) tasks. Recent advances in instruction tuning and alignment (e.g., RLHF/RLAIF~\cite{}) have further enabled these models to generate coherent and contextually appropriate responses to user queries. However, in enterprise-level applications, practitioners often need to fine-tune LLMs for \textit{custom tasks} such as domain-specific summarization, legal clause extraction, and domain question answering. 

The effectiveness of RLHF techniques heavily depends on high-quality human annotations, which are both costly and time-consuming to obtain~\cite{pang2023language, lee2023rlaif, wang2024secrets}. To mitigate this challenge, Reinforcement Learning from AI Feedback (RLAIF) has been introduced, leveraging LLMs to replace human annotators in the feedback loop~\cite{lee2023rlaif, leerlaif, bai2022constitutional}. While RLAIF can approximate human judgment to some extent, it is sensitive to factors such as prompt optimization, task complexity, model bias, generator-discriminator gap, and the capability of the judge model, limiting its ability to fully replicate human annotations~\cite{huang2024self, sharma2024critical, lee2023rlaif, zeng2024scaling, huang2023large}. Our evaluation also provides evidence of these limitations. Furthermore, the samples that challenge a judge model are often the ones most critical for adapting base models to specialized fine-tuning tasks~\cite{ethayarajh2024kto, yuan2024self, huang2023large}. The cost of human annotation is further exacerbated by privacy and security constraints that restrict fine-tuning service providers' access to an entire customer data corpus. In such cases, only subject matter experts (SMEs) within the customer organization have full visibility into the data, making it particularly difficult to optimize prompts effectively across the entire corpus, especially for hard-to-annotate samples.

% However, obtaining a high-quality fine-tuned model for these tasks faces critical challenges: 
% (i) limited human feedback, (ii) the need to address difficult or “hard” problems that are critical to business needs, and (iii) restricted data visibility due to privacy or regulatory constraints.
% (1) \textit{Limited human feedback}: Effective LLM alignment typically requires large amounts of high-quality human-annotated data. While general instruction-tuning practices (e.g., RLHF) have shown strong capabilities on common tasks, their reliance on extensive human annotations becomes impractical for less frequent, custom enterprise needs. In these settings, only a few hundred annotations may be available, making fully supervised approaches are likely to underperform due to data scarcity. 
% (2) \textit{Hard problems are critical}: Enterprise-level applications often involve “hard” problems—edge cases that are inherently complex yet critical to downstream business objectives. These problems cannot be ignored, as the edge-case performance are also essential to business metrics. Existing self-improvement methods often fail to capture these hard problems as they rely solely on the model’s own learned distributions and risk reinforcing pre-existing biases rather than resolving difficult edge cases.
% (3) \textit{Controlled data visibility}: Corporate data often carries sensitive privacy and regulatory constraints, restricting access to specialized \textit{trusted experts} with partial visibility into the data. This hinders the application of domain-specific data-science heuristics (e.g., keyword filtering or text mining), as these experts may not have data-science expertise to craft synthetic data or apply heuristics.

% \footnote{We named our solution after the renowned painter Sargy Mann, who, despite losing his eyesight, adapted new ways of perceiving the world and continued creating art.}
To address these challenges, we propose Reinforcement Learning from Targeted Human Feedback (\myname{}), a human-AI hybrid solution that combines coarse initial alignment using general-purpose LLMs with the progressive integration of strategically selected human annotations to achieve annotation quality comparable to fully human-supervised approaches. \myname{} begins with an initial alignment stage, where a general-purpose LLM labels unlabeled data based on high-level instructions. While this approach effectively captures broader human alignment for easier data points, it often struggles with fine-grained nuances, leading to incorrect labeling. \myname{} automatically identifies these hard-to-annotate data points and directs human effort exclusively toward them. This targeted approach enables \myname{} to achieve the quality of fully human-annotated data while reducing the majority of human annotation effort.




% To address the above challenges, we propose \myname{}, a human-AI hybrid solution that combines coarse initial alignment with general-purpose LLMs and progressive integration of strategically selected human annotations. \myname{} starts with an \textit{initial alignment} stage, prompting a general-purpose LLM to label unlabeled data based on high-level instructions (e.g., short task descriptions, detailed principles, optional few-shot examples). This creates a initial reward model (RM) that captures broad task requirements but may struggle on hard samples. We then apply \textit{progressive hard-problem resolution}, where \myname{} automatically identifies samples that yield low confidence under the initial RM with reward-rank curve. We directs limited human annotation capacity from trusted experts to these specific “confused” samples and progressively infuse essential domain knowledge into the RM. Finally, we transfer the learned reward signal (\textit{reward knowledge transfer}) to downstream tasks such as instruction tuning or text extraction via standard policy optimization strategies (e.g., PPO, best-of-n). This pipeline ensures that critical but tricky edge cases are not overlooked, while only requiring minimal, targeted expert labeling rather than broad data inspection or sophisticated domain-specific heuristics.

% Evaluation TBD.

To enable this efficient human-in-the-loop approach for achieving comprehensive human alignment, \myname{} introduces the following key technical contributions:

First, we develop a concept that leverages the reward distribution of a reward model over its training dataset to capture the relative arrangement of samples based on rewarded features. This property allows us to detect plausible inaccuracies in annotations across the dataset. Specifically, we train a reward model on the LLM-labeled dataset to identify clusters of hard-to-annotate samples that are \textit{highly} likely to be either mislabeled or correctly labeled by the LLM.

Building on this concept, we propose an innovative iterative reward model training technique to achieve oracle-level human alignment in the dataset. In each iteration, \myname{} identifies and rectifies highly probable mislabeled data points using human annotations. Simultaneously, it detects clusters of samples that are very likely to be correctly labeled by the LLM and incorporates them with human-annotated data to construct a high-quality training set for the next iteration of reward model training. Throughout this process, \myname{} preserves data richness and maximizes the efficiency of human annotation investment through carefully controlled hyperparameters.

Finally, we implement and evaluate \myname{} on two distinct preference datasets: HH-RLHF and TL;DR. Our results demonstrate that \myname{} achieves accuracy comparable to a fully human-annotated dataset while requiring only 6–7\% of the total human annotations. Furthermore, we conduct a comparative study by training models on downstream tasks using DPO. Remarkably, models trained with \myname{} even outperform those trained on fully human-annotated datasets, highlighting the impact of \myname{}'s meticulous data curation in enhancing model performance.


% In summary, our main contributions include: 
% \begin{itemize}
%     \item We are the first to identify and explore the value of focusing on hard, domain-specific problems using targeted human feedback to resolve complex and critical edge cases. 
%     \item We propose a novel framework combining initial alignment from general-purpose LLMs with a reward-rank curve-based method to identify and progressively resolve hard problems through human feedback. 
%     \item Our method is broadly applicable to both instruction tuning and document filtering tasks, requiring minimal adaptation to address diverse enterprise use cases. 
%     \item We validate our approach on the TLDR summarization and CUAD legal document extraction datasets, demonstrating significant performance improvements with minimal annotation effort.
% \end{itemize}

% \begin{itemize}
%     \item Application context: Fine-tuning LLM to respond to custom tasks is a critical need in LLM-powered enterprise-level applications. Examples:
%     \begin{itemize}
%         \item Summarization
%         \item Clause extraction
%         \item Domain QA
%     \end{itemize}
    
%     \item Challenges:
%     \begin{itemize}
%         \item Weak supervision limits the number of high-quality annotated data: 
%         \begin{itemize}
%             \item LLM alignment heavily rely on high-quality annotated dataset.
%             \item \yifei{current practice type 1: RLHF} For general instruction tuning and popular tasks, current practices (RLHF) heavily rely on human annotation.
%             \item \yifei{why they won't work} However, for custom tasks, only under-annotated raw data are available and limited human annotation effort are allocated
%         \end{itemize}
%         \item Hard samples cannot be ignored: 
%         \begin{itemize}
%             \item \yifei{current practice type 2: RLAIF, SER} Many works leverages LLM self-improvement strategies (RLAIF, SER), but they fail to learn the hard samples effectively due to the intrinsic limitation of pre-trained models.
%             \item \yifei{why they won't work} However, for enterprise-level applications, certain hard samples are critical to the overall quality of experience and cannot be ignored
%         \end{itemize}
%         \item Controlled data visibility limits the potential of using domain-specific data science tricks: 
%         \begin{itemize}
%             \item \yifei{current practice type 3: synthetic dataset} Many domain-specific LLM/dataset are constructed by applying data science tricks (using key words as anchor points, text mining, etc.), which requires good knowledge of the data content
%             \item Enterprise data have privacy concerns. Meantime they are hard to desensitize by simply masking sensitive information. Therefore, only controlled data can be shared to trusted experts
%             \item \yifei{why they won't work} However, the trusted experts may not have data science expertise. Therefore, no data science tricks with domain knowledge are available.
%         \end{itemize}
%     \end{itemize}

%     \item Our proposal:
%     \begin{itemize}
%         \item Human-AI hybrid feedback: Starting with GPT initial alignment, and generalizing to similar samples + progressive human alignment with few-shot annotation on the fly
%         \begin{itemize}
%             \item Saves time and annotation
%             \item Maximize value of limited human annotations
%             \item Allows effective training with weak supervision
%         \end{itemize}
%         \item Progressively infuse hard samples in the training data: RM reranking + SME annotation
%         \begin{itemize}
%             \item Allow the model to learn hard samples from human, not limited by model itself
%             \item Incorporate SME's intelligence without requiring visibility to data in the training framework (i.e. ML engineer)
%         \end{itemize}
%     \end{itemize}
    
% \end{itemize}
