

\section{Results}
\label{sec:results}

In this section, we present the results of our main experiments, conducted on two datasets: HH-RLHF~\cite{bai2022training} and TL;DR~\cite{volske2017tl}. Specifically, we compare \myname{} against two baselines: (1) \textit{Random}, where samples are randomly selected for human annotation (matching the number of human-annotated samples in \myname{}), with the rest relying on AI feedback, and (2) \textit{Human}, where \textit{all} samples are annotated by humans. A detailed description of our experimental setup is provided in Appendix~\ref{appendix:setup}.
\vspace{-0.1in}



\subsection{Reward Modeling}

\subsubsection{Overall Alignment Improvement}
\label{sec:eval:rm:shard}


\begin{figure}[t]
    \centering
    \begin{subfigure}{0.95\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hh_shard.pdf}
        \caption{HH-RLHF}
        \label{fig:hh_shard}
    \end{subfigure}
    
    
    \begin{subfigure}{0.95\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/tldr_shard.pdf}
        \caption{TL;DR}
        \label{fig:tldr_shard}
    \end{subfigure}
    
    \caption{Overall preference accuracy improvement of \myname{} on test data in an iterative manner. Here, we experiment with different sizes of down-sampled training data shards for \myname{}. We find 6\% (HH-RLHF) to 7\% (TL;DR) total human annotations on 1/4 shard yielding the optimal alignment for downstream tasks.}
    \label{fig:shard}
\end{figure}

We here use GPT-4o for the initial alignment and evaluate \myname{}'s iterative alignment improvements by measuring the preference accuracy of RMs trained with varying proportions of human annotations relative to the full dataset. We employ \myname{} on both the complete dataset and multiple down-sampled shards as described in \S~\ref{sec:design:improve:iter}. For a given shard, we run \myname{} in an iterative manner infusing targeted human annotations in each iteration. We evaluate the trained RMs on a separate test dataset, ensuring a clear distinction from the training data, and report their preference accuracy in Figure~\ref{fig:shard}.

In Figure~\ref{fig:shard}, each data point for a shard corresponds to an iteration of \myname{}. The results show a consistent improvement in test preference accuracy across iterations, with significant early gains that gradually diminish as accuracy approaches the upper bound. Additionally, down-sampling enhances the efficiency of human annotations: \myname{} running on 1/2 and 1/4 shards outperforms its full-dataset counterpart when using the same number of human annotations. However, excessive down-sampling (e.g., 1/8 shard) may limit the achievable accuracy due to reduced data richness. For downstream task fine-tuning, we identify 1/4 shard as the optimal choice. Under this setting, \myname{} enhances preference accuracy on HH-RLHF from GPT-4o’s baseline of $74.7\%$ to $89.6\%$ with only $6\%$ human annotations, and on TL;DR, from $78.8\%$ to $88.0\%$ with just $7\%$ human annotations. We select the RMs from these iterations for labeling the full dataset, as outlined in \S~\ref{sec:design:improve:iter}. Evaluating the test accuracy of RMs trained on the fully labeled dataset, we observe further improvements, with accuracy reaching $91.8\%$ for HH-RLHF and $89.6\%$ for TL;DR.

% The results are shown in Figure~\ref{fig:shard}. When applied to a 1/4 shard (green), \myname{} improves preference accuracy on HH-RLHF from GPT-4o's $74.7\%$ to $89.6\%$ with only $6\%$ human annotations, and on TL;DR, from $78.8\%$ to $88.0\%$ with only $7\%$ human annotations.
% Notably, when we use these RMs to regenerate preferences on the full dataset and train a final RM, its test accuracy reaches $92.0??\%$ for HH-RLHF and $89.0??\%$ for TL;DR, which is less than $4??\%$ below full human annotation. Our experiments in \S~\ref{sec:eval:down} actually reveal that the remaining misaligned samples contain significant noise and do not meaningfully contribute to downstream tasks, and further alignment will lead to worse performance.

% Interestingly, sharding generally improves performance, as \myname{} operating on 1/2 and 1/4 shards tends to outperform its application on the full dataset with the same number of human annotations. However, excessively aggressive sharding (e.g., 1/8 shard) may limit the performance upper bound due to the lack of data richness and diversity.

\subsubsection{Comparison against the Baselines}
\label{sec:eval:rm:overall}
We begin by using two different LLMs—GPT-4o and GPT-4o mini—for the initial AI labeling. We then employ two separate \myname{} pipelines, \myname{} (4o) and \myname{} (4o mini), to improve alignment. To evaluate their effectiveness, we compare these pipelines against two baselines: (1) AI-only labeling (\textit{GPT-4o} and \textit{GPT-4o mini}), (2) \textit{Random} human annotation, and (3) fully \textit{Human}-annotated datasets (baseline setup details are provided in Appendix~\ref{appendix:setup:baselines}).


The results of this experimental setup on two datasets are shown in Figure~\ref{fig:combined_iter_accuracy}. \myname{} (4o) consistently outperforms \textit{Random} (4o), as random human annotation proves ineffective in correcting AI mislabeling, resulting in only marginal improvements in test accuracy. Of particular interest is the "Return on Investment (ROI)"—measured as the increase in test accuracy per unit of human annotation. With just $6\%$ human annotation, \myname{} (4o) achieves a $15.9\times$ and $5.3\times$ higher ROI compared to \textit{Random} (4o) on HH-RLHF and TL;DR, respectively.

Notably, \myname{} remains robust even when the initial AI labeling quality is lower. While GPT-4o mini starts with an accuracy gap of $2.6\%$ and $5.6\%$ compared to GPT-4o, this gap shrinks to just $0.4\%$ and $-0.2\%$ after incorporating $10\%$ human annotation on HH-RLHF and TL;DR, respectively. This demonstrates that even when AI mislabeling is more prevalent, \myname{} more aggressively identifies and corrects errors, achieving a higher ROI on human annotation.



\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/combined_iter_accuracy.pdf}
    \caption{\myname{} effectively bridges the prefernce accuracy gap between LLM-generated (GPT-4o/GPT-4o-mini) labels and fully human-annotated data, regardless of the initial labeling accuracy of the LLM. By strategically incorporating human annotations, \myname{} achieves higher accuracy gains compared to random human annotation, maximizing the impact of human effort.}
    \label{fig:combined_iter_accuracy}
\end{figure}


\subsubsection{Effects of Hyperparameters}
\label{sec:eval:rm:hyper}


\begin{figure*}[t]
    \centering
    % First row (HH-RLHF)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/hh_amp.pdf}
        \caption{Amplification Ratio (HH-RLHF)}
        \label{fig:hh_amp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/hh_backoff.pdf}
        \caption{Back-off Ratio (HH-RLHF)}
        \label{fig:hh_backoff}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/hh_batch.pdf}
        \caption{Annotation Batch Size (HH-RLHF)}
        \label{fig:hh_batch}
    \end{subfigure}

    % Second row (TL;DR)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tldr_amp.pdf}
        \caption{Amplification Ratio (TL;DR)}
        \label{fig:tldr_amp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tldr_backoff.pdf}
        \caption{Back-off Ratio (TL;DR)}
        \label{fig:tldr_backoff}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tldr_batch.pdf}
        \caption{Annotation Batch Size (TL;DR)}
        \label{fig:tldr_batch}
    \end{subfigure}
    
    \caption{Effects of \myname{} hyperparameters on HH-RLHF (top row) and TL;DR (bottom row). A larger amplification ratio ($\alpha$) and back-off ratio ($\beta$) are beneficial in the initial iterations but should be gradually reduced as human annotations accumulate. The human annotation budget should be distributed across multiple batches, though the impact of finer granularity remains unclear.}
    \label{fig:merged}
\end{figure*}


\bbb{Amplification Ratio.}
To investigate how the human amplification ratio $\alpha$ contributes to \myname{}, we fix the back-off ratio $\beta$ at 60\% and conduct a controlled study on different amplification ratios.

The results for each dataset are shown in Figures~\ref{fig:hh_amp} and~\ref{fig:tldr_amp}. We observe that both no amplification ($\alpha=1$) and excessive amplification ($\alpha=8$) of human annotations lead to suboptimal RM improvements. Specifically, lower amplification results in smaller improvements in the initial iterations, while in later iterations, this trend reverses. This is expected, as no or low amplification weakens the impact of human annotations, particularly in the early iterations when the total number of annotations remains low, while over-amplification skews the training data distribution and increases the risk of overfitting, especially in later iterations when the base number of annotations is already large.

Although we use a fixed amplification ratio (Appendix~\ref{appendix:setup:config}), we recommend starting with a higher amplification ratio and gradually reducing it as the number of accumulated annotations grows to achieve the best results.

\begin{table*}[h]
    \centering
    \begin{tabular}{c|ccc|ccc}
        \toprule
        \multirow{2}{*}{Itr \#} & \multicolumn{3}{c|}{HH-RLHF} & \multicolumn{3}{c}{TL;DR} \\
        
         & No Annotation  & No Ampl./Back-off & \textbf{Full \myname{}}  & No Annotation  & No Ampl./Back-off  & \textbf{Full \myname{}}  \\
        \midrule
        0 & \multicolumn{3}{c|}{75.0} & \multicolumn{3}{c}{74.7} \\
        \midrule
        1 & 75.1 & 75.1 & \textbf{79.9} & 75.0 & 74.4 & \textbf{78.1} \\
        2 & 74.8 & 76.0 & \textbf{82.7} & 74.5 & 75.3 & \textbf{80.7} \\
        3 & 73.5 & 75.3 & \textbf{84.7} & 74.9 & 74.9 & \textbf{82.5} \\
        4 & 74.8 & 75.0 & \textbf{87.6} & 75.4 & 74.7 & \textbf{83.3} \\
        5 & 75.7 & 75.8 & \textbf{87.7} & 75.2 & 76.0 & \textbf{83.7} \\
        \bottomrule
    \end{tabular}
    \caption{Ablation study on HH-RLHF and TL;DR. All three factors, human annotation, amplification of human annotations, and the back-off mechanism, play a crucial role in \myname{}'s effectiveness.}
    \label{tab:ablation}
\end{table*}

\bbb{Back-off Ratio.}
To investigate how the back-off ratio $\beta$ contributes to \myname{}, we conduct a controlled study while keeping all other hyperparameters at their default values.

The results for each dataset are shown in Figures~\ref{fig:hh_backoff} and~\ref{fig:tldr_backoff}. We observe a clear trend where a larger $\beta$ leads to higher improvements in the initial iteration, but its gains diminish in later iterations. In contrast, a smaller $\beta$ results in slower improvements early on but accelerates in later iterations, reflecting a shift in the trade-off between data richness and quality. At the beginning, the data is relatively unsanitized, making data quality the limiting factor, so a higher back-off ratio is beneficial due to its stronger sanitization effect. However, as accuracy improves over time, even a smaller back-off ratio can produce sufficiently clean data, allowing its greater data richness and diversity to become the dominant advantage.

Similar to our settings (Appendix~\ref{appendix:setup:config}), we recommend starting with a higher back-off ratio and gradually reducing it as the data becomes more sanitized.


\bbb{Annotation Batch Size.}
To investigate how the number of annotated samples per iteration affects \myname{}, and more fundamentally, whether \myname{} benefits from an iterative approach compared to a one-shot annotate-all strategy, we conduct a controlled study while keeping the amplification ratio $\alpha$ fixed at 4 and the back-off ratio $\beta$ at 60\%.

The results for each dataset are shown in Figures~\ref{fig:hh_batch} and~\ref{fig:tldr_batch} (note that the annotation percentages are relative to the shard rather than the full dataset). We observe that the iterative approach achieves an improvement of up to $4.2\%$ by Itr-4 compared to the one-shot annotation strategy. This suggests that in each iteration, the RM not only learns from the annotated samples but also generalizes to a broader set of similar samples.

However, in our preliminary study, we did not observe a significant benefit when splitting the annotation budget into finer batches (e.g., 4\% per iteration vs. 8\% per iteration). Given the additional GPU time and human feedback round trips required for smaller batches, we recommend splitting the annotation budget into two batches or using a similarly coarse granularity to balance efficiency and performance.




\subsubsection{Ablation Study}
\label{sec:eval:rm:ablation}
We conduct an ablation study to evaluate the necessity of each component in \myname{} and understand their contributions. All experiments in this study use a $4\times$ amplification ratio, a $60\%$ back-off ratio, and $4\%$ human annotations relative to a 1/4 shard per iteration. Our primary goal is to answer two key questions.

\bbb{Does self-improvement alone work?}
To assess whether \myname{} can function effectively without human annotations, we set the annotation batch size to $0\%$, reducing \myname{} to a pure self-improvement scheme. The results in Table~\ref{tab:ablation} indicate that self-improvement alone has inherent limitations and struggles to surpass GPT’s baseline preference accuracy. Further evaluation on downstream tasks also confirms that this GPT-level preference accuracy is insufficient for aligning with human preferences. These findings suggest that self-improvement alone is inadequate for \myname{}'s intended use case.

\bbb{Are amplification and back-off necessary?}
To examine the importance of \myname{}'s human amplification and sanitization back-off mechanisms, we disable both simultaneously (since the individual ablation of amplification was already tested in \S~\ref{sec:eval:rm:hyper} with $\alpha=1$). The results in Table~\ref{tab:ablation} show that \myname{} achieves only marginal improvement without these mechanisms, even with accumulating annotations.
This result highlights the strong contributions of both amplification and back-off. Without sanitization back-off, incorrect labels persist in the training data, making it harder to identify valuable samples and further reducing data cleanliness. Without human amplification, an even greater number of correct samples is required to overturn incorrect ones, exacerbating the issue. 




% \subsubsection{Density of correct labels along the curve (appendix)}
% \begin{itemize}
%     \item Case study on tldr
%     \item Case study on CUAD
% \end{itemize}


\subsection{Downstream Tasks}
\label{sec:eval:down}


To assess \myname{}'s effectiveness on final downstream tasks, we conduct DPO training on \myname{}-prepared data and compare it against other baselines w.r.t. an SFT model. For \myname{}, we leverage the RM trained on a 1/4 shard, refined with targeted $6\%$ human annotation for HH-RLHF and $7\%$ for TL;DR, to regenerate preferences for the full dataset. We then train an LLM with DPO on this refined dataset. For the \textit{Random} baseline, we introduce the same amount of randomly selected human annotations on top of AI labels into the DPO training dataset. Performance is evaluated using the pairwise win rate against SFT model outputs, calculated with AlpacaEval~\cite{alpaca_eval} running Claude 3.5 Sonnet~\cite{anthropic_claude_sonnet} as a judge on test sets from HH-RLHF and TL;DR datasets (see Appendix~\ref{appendix:win_rate} for details).

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|cc}
        \toprule
        Dataset             & HH-RLHF & \multicolumn{2}{c}{TL;DR} \\
        AI Labeler          & GPT-4o & GPT-4o & GPT-4o mini \\
        \midrule
        \textit{AI-labeled} & 49.2 & 59.2 & 56.4 \\
        \textit{Random}     & 52.5 & 59.8 & 57.5 \\
        \textbf{\myname{}}  & \textbf{58.1} & \textbf{62.3} & \textbf{62.4} \\
        \midrule
        \textit{Human}      & 55.7 & \multicolumn{2}{c}{60.2} \\
        \bottomrule
    \end{tabular}
    \caption{Win rate against SFT (\%). \myname{} consistently outperforms the \textit{AI-labeled}, \textit{Random}, and \textit{Human} baselines across both datasets. Even with weaker GPT-4o mini, \myname{} also achieves a win rate comparable to that with GPT-4o.}


    
    \vspace{-0.2in}
    \label{tab:win_rate}
\end{table}

The results in Table~\ref{tab:win_rate} align with the observed preference accuracy trends. Across both datasets, \myname{} achieves a higher win rate than the fully \textit{Human}-annotated baseline using only $6$–$7\%$ of total human annotations, while significantly outperforming models trained on purely AI-labeled data as well as \textit{Random} human annotations. Notably, on the TL;DR dataset, GPT-4o already performs at a near-human level, yet \myname{} further enhances the win rate by $3.1\%$. Additionally, even with a weaker AI labeler (GPT-4o mini), \myname{} effectively bridges the gap within the same annotation budget, achieving a win rate comparable to that of GPT-4o. These findings are consistent with our observations in \S~\ref{sec:eval:rm:overall}, further validating \myname{}'s robustness and effectiveness, even when faced with suboptimal AI labeling due to model limitations, task complexity, or poor prompting.

Interestingly, \myname{} outperforms the fully \textit{Human}-annotated baseline, despite incorporating annotations from the same dataset. We attribute this advantage to \myname{}'s sanitized data selection for RM training, as discussed in \S~\ref{sec:design:improve:iter}. Fully human-annotated datasets inherently contain noise and biases~\cite{wang2024secrets, sun2024rethinking, ethayarajh2024kto}. In \S~\ref{sec:design:improve:iter}, we illustrated how such samples tend to cluster around the ``knee'' of the reward distribution curve. By leveraging the back-off ratio hyperparameter, \myname{} controls noisy and biased samples, ensuring a cleaner training dataset. The selected RMs from \myname{} are trained on data with a back-off ratio of $10\%$ in the corresponding iteration, resulting in reduced bias and noise. Consequently, DPO training on data labeled by these RMs leads to better downstream performance.

% generalization capability, which cross validates human and AI preferences and mitigates the impact of incorrect human preferences. This also leads to the remaining accuracy gap in \S~\ref{sec:eval:rm:shard}.

% Meanwhile, human noise in the dataset~\cite{wang2024secrets} and biases in the LLM-as-a-Judge~\cite{zheng2023judging} may still underestimate the potential of \myname{}, despite our mitigation efforts (Appendix~\ref{appendix:setup} and~\ref{appendix:win_rate}). We look forward to further experiments on more high-quality datasets and tasks that can be evaluated with metrics better aligned with human preferences.