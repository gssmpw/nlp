\section{Conclusion}
 
In this work, we introduce \myname{}, an iterative reward model training approach that enhances alignment in preference datasets by strategically infusing human annotations, complemented by sanitized AI labeling. Through reward distribution analysis, we identify key samples for targeted human intervention, optimizing annotation efficiency. Our experiments demonstrate that \myname{} progressively improves alignment, converging toward comprehensive human alignment. Furthermore, models trained on our refined datasets for downstream tasks even outperform the models trained on datasets with full-human annotations.