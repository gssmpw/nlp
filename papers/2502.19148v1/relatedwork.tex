\section{Related Work}
\label{sec:related_work}

\vspace{-1.5ex}

In this section, we will introduce the background of the related research. The existing alignment methods for LLMs can generally be divided into three categories: training time alignment methods, assisted inference methods, and tuning-free methods. We will elaborate on them separately below.

\vspace{-1.5ex}

% In this section, we will introduce the background of the research and some related work. The existing alignment methods can mainly be divided into two parts: alignment at training time and inference time. We will elaborate on them separately below.

\paragraph{\textbf{Alignment at Training Time.}}
% This category is currently the most mainstream way of alignment, mainly focusing on first training the model itself on datasets, and then freezing the parameters for inference. There are many well-known algorithms, including RLHF \citep{christiano2017deep, ouyang2022training}, CAI \citep{bai2022constitutional}, RLAIF \citep{lee2023rlaif}, DPO \citep{rafailov2024direct}, ORPO \citep{hong2024reference}, and Quiet-STaR \citep{zelikman2024quiet}.
% Additionally, although OpenAI-o1 \footnote{\url{https://openai.com/o1/}} seems to allow for inference-time computation, it still requires a training process to achieve this.
% Although these methods can achieve good results in capability metrics, they have to recollect enough data and perform further training and finetuning, making them unable to meet users' changing requirements in specific scenarios.
This category is currently the most mainstream way of alignment, mainly focusing on first training the model itself on datasets, and then freezing the parameters for inference. 
% There are many well-known algorithms, including RLHF \citep{christiano2017deep, ouyang2022training}, RLAIF \citep{lee2023rlaif}, CAI \citep{bai2022constitutional}, and DPO \citep{rafailov2024direct}.
% In addition, there are some newer methods. ORPO \citep{hong2024reference} integrates the loss function from the supervised fine-tuning (SFT) process into the DPO optimization objective by introducing the concept of odds ratio, thereby merging these two processes into one. SimPO \citep{meng2024simpo} eliminates the dependence on a reference policy in DPO by introducing the sequence average log probability as an implicit reward. Quiet-STaR \citep{zelikman2024quiet} enhances generalization across more reasoning tasks by training the model's reasoning and thinking abilities.
There are many well-known algorithms, including RLHF \citep{christiano2017deep, ouyang2022training}, CAI \citep{bai2022constitutional}, DPO \citep{rafailov2024direct}, ORPO \citep{hong2024reference}, and KTO \citep{ethayarajh2024kto}. 
In addition, there are some newer methods. SimPO \citep{meng2024simpo} eliminates the dependence on a reference policy in DPO by introducing the sequence average log probability as an implicit reward. Quiet-STaR \citep{zelikman2024quiet} enhances generalization across more reasoning tasks by training the model's reasoning and thinking abilities. 
% \citet{ZhangZX0JP21, zhang2023vibe} align models with evolving contexts by leveraging neural topic models and temporal information.
Although these methods can achieve good results in capability metrics, they have to recollect enough data and perform further training and fine-tuning, making them unable to meet users' changing and personalized requirements in specific scenarios.

\vspace{-1.5ex}


\paragraph{\textbf{Assisted Inference Methods.}}
This class of methods typically involves training a weak model (usually a small model or a pre-trained model) to enhance the alignment of a strong model during inference time, which has various implementation forms. 
% Aligner \citep{ji2024aligner} attaches a small model after a larger one, allowing it to directly revise the large model's outputs. Alignment via Bayesian Persuasion \citep{bai2024efficient} uses weakly generated natural language signals as a prompt to provide more alignment information. 
Aligner \citep{ji2024aligner} and Alignment via Bayesian Persuasion \citep{bai2024efficient} use weak generated natural language to influence strong model behavior.
% \citet{ji2024aligner} and \citet{bai2024efficient} use weak generated natural language to influence strong model behavior.
ExPO \citep{zheng2024weak} adopts an interpolation approach, linearly combining the parameters of the small model with those of the large model to achieve alignment. Improved sampling strategies \citep{wu2024empirical, snell2024scaling, liu2025can}, from the perspective of optimal sampling, allow the strong model to generate higher-quality text under the guidance of a smaller reward model.
EFT \citep{mitchell2023emulator}, DeRa \citep{liu2024decoding}, and proxy-tuning \citep{liu2024tuning} integrate the logits of the aligned small model and the unaligned large model to guide the decoding process. RE-CONTROL \citep{kong2024aligning} enhances the alignment ability of the large model by training a value model to edit the information representation of the large model during the decoding process.
These methods reduce computation and time costs by training smaller models, but they are essentially the same as training-time methods and still fail to address the issue of changing and personalized preferences.

\vspace{-1.5ex}

\paragraph{\textbf{Tuning-free Methods.}}
Methods in this class usually consider further optimization at inference time. RAIN \citep{li2023rain} uses the LLM itself as a reward model to perform inference and rollback correction during the inference phase. URIAL \citep{lin2023unlocking} designs an inference approach by comparing the differences before and after model alignment, allowing the model to continuously correct and reinforce tokens that better match user preferences. However, both of these methods still require a significant amount of computational time and resources during inference. LA \citep{gao2024linear} proposes a method that compares the distribution changes caused by user preference prompts on the original model and linearly updates the original model's logits, which achieves acceptable computational efficiency. However, it influences the inference process in only a basic manner, falling short of better approximating users' preferences. 
% \citet{liu2025can} investigates the potential of  test-time scaling to enhance the reasoning capabilities of language models. 

\vspace{-1.5ex}