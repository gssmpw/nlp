
\documentclass{article} % For LaTeX2e
\usepackage[table,xcdraw,dvipsnames]{xcolor} 
\usepackage{iclr2025_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsthm}
\usepackage{graphicx} % for including graphical elements like bar charts, if needed
\usepackage{hyperref}
\usepackage{url}
\usepackage{minitoc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{float}
% \usepackage{subfig}
\usepackage{pgf}
\usepackage{pgfmath}

\usepackage{caption}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{amsfonts}

\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{xspace}
\usepackage{arydshln}
\usepackage[many]{tcolorbox}
\usepackage{color}
\usepackage{colortbl}
\usepackage{bibunits}
\usepackage{longtable}

\definecolor{colorSquare}{HTML}{4285F4}
\definecolor{colorCircle}{HTML}{ef8a62}
\definecolor{colorTriangle}{HTML}{34A853}
\definecolor{yellow}{HTML}{F3B41B}
\colorlet{highlightYellow}{yellow!70} %
\definecolor{red}{HTML}{ED1C24}
\definecolor{gray}{HTML}{636363}

\newcommand\rurl[1]{%
  \href{https://#1}{\color{magenta}\nolinkurl{#1}}
}

\colorlet{highlightRed}{red!50} %
\colorlet{highlightGray}{gray!50} %

\newcommand\shr[1]{\textcolor{blue}{[Haoran: #1]}}
\newcommand{\rowheight}{7mm}

\newcommand{\cbox}[2]{%
    % \definecolor{temp}{rgb}{#1}%
    \pgfmathsetmacro{\val}{#2}% 使用第二个参数作为颜色计算值
    % 判断正负，如果 #2 是负数，则生成红色调；如果是正数，则生成绿色调
    \ifdim \val pt < 0pt
        \pgfmathsetmacro{\red}{1}% 红色分量固定为1
        \pgfmathsetmacro{\green}{1+\val}% 绿色分量逐渐减少，负值越大越红
        \pgfmathsetmacro{\blue}{1+\val}% 蓝色分量随绿色减少
    \else
        \pgfmathsetmacro{\red}{1-\val}% 红色分量随正值减少
        \pgfmathsetmacro{\green}{1-\val/2}% 绿色分量同样随正值减少
        \pgfmathsetmacro{\blue}{1}% 蓝色分量固定为1
    \fi
    % 动态生成颜色
    \definecolor{temp}{rgb}{\red,\green,\blue}%
    \tikz[baseline,anchor=base] \node[fill=temp,text width=\cboxwgt,align=center,rounded corners=1.5pt,minimum height=\rowheight] (X) {\centering #1};%
}


\usepackage{marvosym}
\usepackage{enumitem}
\usepackage{float}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{ass}[theorem]{Assumption}

\newcommand{\zhaowei}[1]{{\color{blue}[zhaowei: #1]}}
\newcommand{\fengshuo}[1]{{\color{Cerulean}[fengshuo: #1]}}
% \newcommand{\yellowunderline}[1]{{\colorbox{highlightYellow}{\underline{#1}}}}
\newcommand{\yellow}[1]{{\color{yellow}#1}}
% \newcommand{\red}[1]{{\underline{\color{red}#1}}}
\newcommand{\red}[1]{{\color{red}#1}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\defeq}{\mathrel{\mathop:}=}
\newcommand{\T}{\ensuremath{\mathcal{T}}}
\newcommand{\centerblanks}{\qquad \qquad \qquad \qquad \quad}


\title{Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }
% \href{zwzhang@stu.pku.edu.cn}{\textrm{\Letter}}
% $^{1,2*}$
% \author{Zhaowei Zhang$^{1,2* \ \href{mailto:zwzhang@stu.pku.edu.cn}{\textrm{\Letter}}}$ \quad Fengshuo Bai$^{3,4}$\thanks{Equal contribution.} \ \quad Qizhi Chen$^{1}$ \quad Chengdong Ma$^{1}$ \quad Mingzhi Wang$^{1}$\\
% \textbf{Haoran Sun}$^{1}$ \quad \textbf{Zilong Zheng}$^{2\dag}$ \quad \textbf{Yaodong Yang}$^{1}$\thanks{Corresponding author.} \\
\author{Zhaowei Zhang$^{1,2* \ \href{mailto:zwzhang@stu.pku.edu.cn}{\textrm{\Letter}}}$ \quad Fengshuo Bai$^{3,4}$\thanks{Equal contribution.} \ \quad Qizhi Chen$^{1}$ \quad Chengdong Ma$^{1}$ \\ \textbf{Mingzhi Wang}$^{1}$ \quad \textbf{Haoran Sun}$^{1}$ \quad \textbf{Zilong Zheng}$^{2\dag}$ \quad \textbf{Yaodong Yang}$^{1}$\thanks{Corresponding author.}$^{\ \ \ \href{mailto:yaodong.yang@pku.edu.cn}{\textrm{\Letter}}}$ 
% && 
\\
\\
% \thanks{Correspondence to: $<$\textit{yaodong.yang@pku.edu.cn}$>$ and $<$\textit{zlzheng@bigai.ai}$>$.} \\
% \thanks{Correspondence to: Yaodong Yang $<$\textit{yaodong.yang@pku.edu.cn}$>$ and Zilong Zheng  $<$\textit{zlzheng@bigai.ai}$>$.} \\
	$^1$Institute for Artificial Intelligence, Peking University \\
 % $^2$Institute for Artificial Intelligence, Peking University \\
	% $^3$School of Artificial Intelligence, Beijing University of Posts and Telecommunications \\
 $^2$National Key Laboratory of General Artificial Intelligence, BIGAI \\
 $^3$Shanghai Jiao Tong University\\
 $^4$Zhongguancun Academy \\ 
 \\
 \centerblanks \rurl{zowiezhang.github.io/projects/Amulet}
 % \rurl{amulet.github.io}
	% \texttt{zwzhang@stu.pku.edu.cn}, \ \texttt{changwindeg@gmail.com
} %\\



% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\iclrfinalcopy
\begin{document}


\maketitle
\vspace{-4ex}
\begin{abstract}

How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. 
However, user preferences are usually personalized, changing, and diverse regarding culture, values, or time. 
This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. 
Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important.
To this end, we introduce \textbf{Amulet}, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. 
To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level.
The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.
% The code is available at \url{https://github.com/bigai-ai/civrealm}.

\end{abstract}

\section{Introduction}

The success of large language models (LLMs) has led to their widespread application in scenarios such as customer service \citep{raiaan2024review}, content creation \citep{hadi2024large}, and personal assistance \citep{chen2024large}, emphasizing the importance of maintaining alignment with human preferences \citep{ji2023ai, anwar2024foundational}.
However, existing LLM alignment researches often focus on aligning with user preferences from a \textit{static} general dataset, neglecting \textit{personalized} and changing preferences. 
This leads to challenges in ensuring that the alignment goals designed by model developers adequately address users’ evolving needs in real-time post-deployment scenarios \citep{liao2023rethinking, lazar2023ai, zhang2024incentive, correa2024dynamic, zhang2024knowledge}.

One straightforward approach is to recollect data for personalized user preferences and use methods such as reinforcement learning from human feedback (RLHF) \citep{christiano2017deep, ouyang2022training} or direct preference optimization (DPO) \citep{rafailov2024direct} for further fine-tuning. However, these approaches not only face the difficulty in the requirements engineering problem of specifying users' real needs \citep{pohl1996requirements, mechergui2024goal}, but also from the fact that users' preferences continuously change with culture, community, context, scenario, and time \citep{macintyre2013after, eckersley2018impossibility, turchin2019ai, zhang2023vibe, qiu2024progressgym, zhu2025grait}. If the requirements analysis, data collection, and subsequent fine-tuning processes are repeated each time, it will result in a significant cost burden. 
Just as shown in the (a) and (b) parts of the \autoref{fig:motivation_and_method}, this phenomenon leads to a ``last mile'' problem \citep{boysen2021last} in existing alignment research.

To solve this problem, we believe a lightweight preference adaptation method implemented at the LLM test time is needed. 
Several existing works have already made some attempts in this regard. 
Assisted inference methods focus on training weak models to guide strong model inference in different aspects, including in natural language form \citep{ji2024aligner, bai2024efficient}, interpolation form \citep{liu2024decoding, zheng2024weak}, sampling form \citep{wu2024empirical, snell2024scaling}, representation form \citep{kong2024aligning}, and logits form \citep{mitchell2023emulator, liu2024tuning}. 
RAIN \citep{li2023rain} and URIAL \citep{lin2023unlocking} mainly focus on self-distillation, allowing the model to refine the tokens that better match user preferences continuously.
Nevertheless, the efficiency of these methods is still not at an acceptable level for either the training of a weak model or the complex inference.
Linear Alignment (LA) \citep{gao2024linear} proposes a linear approximation preference update method that achieves acceptable computational efficiency. However, it influences the inference process in only a basic manner, falling short of better approximating users' personalized preferences. 


\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{images/fig1.pdf}
    \vspace{-1.5ex}
    \caption{An illustration of our Amulet framework and its background. 
The figure is intersected by an axis, with each node on the axis displaying a different distribution that shows the constantly changing user personalized preferences due to factors like time, value, need, and context, as illustrated by the part (a).
The part (b) shows that existing methods mostly consider aligning LLMs with general preferences from a static dataset, which may result in misalignment in dynamically personalized scenarios.
In the part (c), we have enlarged one of the preference nodes to show the processing of our Amulet framework.
We formulate the decoding process of every token as a separate online learning problem and further adapt the backbone LLMs to align with the current user preference through a real-time optimization process with the guidance of user-provided prompts. The red token means the current processing token, which will be the condition for the next token prediction.}
    \label{fig:motivation_and_method}
    \vspace{-2.5ex}
\end{figure}


Therefore, in this paper, we suggest a new perspective that the problem can be solved by performing further online adaptation at test time based on the backbone LLMs.
To this end, as illustrated in the (c) part of the \autoref{fig:motivation_and_method}, we introduce \textbf{Amulet} (re\textbf{A}lign\textbf{M}ent d\textbf{U}ring test time for persona\textbf{L}ized pr\textbf{E}ference adap\textbf{T}ation), a novel, training-free framework that formulates the decoding process of every token as an independent online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy user preferences. 
Specifically, unlike methods such as RLHF \citep{christiano2017deep, ouyang2022training} that treat the entire decoding process as a Markov Decision Process (MDP) aiming to maximize cumulative reward, we consider iteratively optimizing the already-generated policies for each token's decoding process separately for more comprehensive preference approximation.
% We adopt the idea of follow-the-regularized-leader (FTRL) \citep{hazan2016introduction} to carry out the specific optimization process. We further made some adaptations based on FTRL-proximal \citep{mcmahan2011follow}, making the optimization process more stable in the setting of LLM.
We follow the idea of follow-the-regularized-leader (FTRL) \citep{hazan2016introduction} to carry out the specific optimization process, and further improve the optimization process more stable in the LLM setting by introducing the proximal regularizer to the vanilla FTRL process \citep{mcmahan2011follow}.
We can then use the user-provided simple prompts to simulate the optimization direction by comparing whether there is a difference between policies given the prompts or not \citep{gao2024linear}. 
Since this process may consume a significant amount of computational cost, we further give a closed-form solution for each iteration, thereby reducing its computational and time costs to negligible levels.

In summary, our contributions are three-fold.
\textbf{Firstly}, we suggest a new perspective that the generation of each token at test time can be modeled as an independent online optimization problem and provide the Amulet framework. 
This method can not only align with personalized and changing user preferences based on the backbone LLM policies but also does not require additional training and fine-tuning.
To our knowledge, we are the first study to introduce the optimization process of online learning into the work of test-time preference alignment. 
\textbf{Secondly}, for each round of optimization iteration, we further provide its closed-form solution, thereby reducing the computational cost of iterative calculations to a negligible level.
\textbf{Lastly}, we carried out comprehensive experiments to validate the efficacy of our framework across diverse settings, incorporating various combinations of LLMs, test datasets, and user preference dimensions. The findings reveal that our approach can achieve significant performance improvements and surpass all baseline methods in the majority of scenarios (an average of 75\% among all the experimental settings, 100\% for the best LLM, and 93.8\% for the best preference), thus providing a final ``amulet'' for aligning personalized preferences. 



\vspace{-1.5ex}


\section{Related Work}
\label{sec:related_work}

\vspace{-1.5ex}

In this section, we will introduce the background of the related research. The existing alignment methods for LLMs can generally be divided into three categories: training time alignment methods, assisted inference methods, and tuning-free methods. We will elaborate on them separately below.

\vspace{-1.5ex}

% In this section, we will introduce the background of the research and some related work. The existing alignment methods can mainly be divided into two parts: alignment at training time and inference time. We will elaborate on them separately below.

\paragraph{\textbf{Alignment at Training Time.}}
% This category is currently the most mainstream way of alignment, mainly focusing on first training the model itself on datasets, and then freezing the parameters for inference. There are many well-known algorithms, including RLHF \citep{christiano2017deep, ouyang2022training}, CAI \citep{bai2022constitutional}, RLAIF \citep{lee2023rlaif}, DPO \citep{rafailov2024direct}, ORPO \citep{hong2024reference}, and Quiet-STaR \citep{zelikman2024quiet}.
% Additionally, although OpenAI-o1 \footnote{\url{https://openai.com/o1/}} seems to allow for inference-time computation, it still requires a training process to achieve this.
% Although these methods can achieve good results in capability metrics, they have to recollect enough data and perform further training and finetuning, making them unable to meet users' changing requirements in specific scenarios.
This category is currently the most mainstream way of alignment, mainly focusing on first training the model itself on datasets, and then freezing the parameters for inference. 
% There are many well-known algorithms, including RLHF \citep{christiano2017deep, ouyang2022training}, RLAIF \citep{lee2023rlaif}, CAI \citep{bai2022constitutional}, and DPO \citep{rafailov2024direct}.
% In addition, there are some newer methods. ORPO \citep{hong2024reference} integrates the loss function from the supervised fine-tuning (SFT) process into the DPO optimization objective by introducing the concept of odds ratio, thereby merging these two processes into one. SimPO \citep{meng2024simpo} eliminates the dependence on a reference policy in DPO by introducing the sequence average log probability as an implicit reward. Quiet-STaR \citep{zelikman2024quiet} enhances generalization across more reasoning tasks by training the model's reasoning and thinking abilities.
There are many well-known algorithms, including RLHF \citep{christiano2017deep, ouyang2022training}, CAI \citep{bai2022constitutional}, DPO \citep{rafailov2024direct}, ORPO \citep{hong2024reference}, and KTO \citep{ethayarajh2024kto}. 
In addition, there are some newer methods. SimPO \citep{meng2024simpo} eliminates the dependence on a reference policy in DPO by introducing the sequence average log probability as an implicit reward. Quiet-STaR \citep{zelikman2024quiet} enhances generalization across more reasoning tasks by training the model's reasoning and thinking abilities. 
% \citet{ZhangZX0JP21, zhang2023vibe} align models with evolving contexts by leveraging neural topic models and temporal information.
Although these methods can achieve good results in capability metrics, they have to recollect enough data and perform further training and fine-tuning, making them unable to meet users' changing and personalized requirements in specific scenarios.

\vspace{-1.5ex}


\paragraph{\textbf{Assisted Inference Methods.}}
This class of methods typically involves training a weak model (usually a small model or a pre-trained model) to enhance the alignment of a strong model during inference time, which has various implementation forms. 
% Aligner \citep{ji2024aligner} attaches a small model after a larger one, allowing it to directly revise the large model's outputs. Alignment via Bayesian Persuasion \citep{bai2024efficient} uses weakly generated natural language signals as a prompt to provide more alignment information. 
Aligner \citep{ji2024aligner} and Alignment via Bayesian Persuasion \citep{bai2024efficient} use weak generated natural language to influence strong model behavior.
% \citet{ji2024aligner} and \citet{bai2024efficient} use weak generated natural language to influence strong model behavior.
ExPO \citep{zheng2024weak} adopts an interpolation approach, linearly combining the parameters of the small model with those of the large model to achieve alignment. Improved sampling strategies \citep{wu2024empirical, snell2024scaling, liu2025can}, from the perspective of optimal sampling, allow the strong model to generate higher-quality text under the guidance of a smaller reward model.
EFT \citep{mitchell2023emulator}, DeRa \citep{liu2024decoding}, and proxy-tuning \citep{liu2024tuning} integrate the logits of the aligned small model and the unaligned large model to guide the decoding process. RE-CONTROL \citep{kong2024aligning} enhances the alignment ability of the large model by training a value model to edit the information representation of the large model during the decoding process.
These methods reduce computation and time costs by training smaller models, but they are essentially the same as training-time methods and still fail to address the issue of changing and personalized preferences.

\vspace{-1.5ex}

\paragraph{\textbf{Tuning-free Methods.}}
Methods in this class usually consider further optimization at inference time. RAIN \citep{li2023rain} uses the LLM itself as a reward model to perform inference and rollback correction during the inference phase. URIAL \citep{lin2023unlocking} designs an inference approach by comparing the differences before and after model alignment, allowing the model to continuously correct and reinforce tokens that better match user preferences. However, both of these methods still require a significant amount of computational time and resources during inference. LA \citep{gao2024linear} proposes a method that compares the distribution changes caused by user preference prompts on the original model and linearly updates the original model's logits, which achieves acceptable computational efficiency. However, it influences the inference process in only a basic manner, falling short of better approximating users' preferences. 
% \citet{liu2025can} investigates the potential of  test-time scaling to enhance the reasoning capabilities of language models. 

\vspace{-1.5ex}

\section{Method}

\vspace{-1ex}

% In this section, we will demonstrate how to formally construct our method to further optimize users' online preferences during the inference stage.
In this section, we will formally introduce the Amulet framework. We begin by introducing our task settings, followed by reviewing online learning and the FTRL
% follow-the-regularized-leader (FTRL) 
algorithm, and finally, we introduce the specific definition of our method and its closed-form solution.
% More details of the position of our method among existing research are shown in Appendix \ref{app:problem_position}. 

% \subsection{Preliminaries}

% Unlike RLHF \citep{christiano2017deep, ouyang2022training}, which views the decoding process of LLMs as an MDP aiming to maximize cumulative reward, in this paper, we treat the decoding of each token by LLMs as an online optimization process. Therefore, for the generation of each token, we need to find the following optimal policy $\pi^*$:

% We start by discussing the optimization target of our method. 
% \paragraph{\textbf{Our Optimization Target.}}

\vspace{-1ex}
\subsection{Task Settings}

% \vspace{-1ex}

Unlike most existing methods which view the decoding process of LLMs as an MDP aiming to maximize cumulative reward, our approach focuses on online optimization for the generation of each token at every timestep, thus enabling real-time optimization to satisfy user preferences. 

For LLMs, the generation process of each token can be seen as a policy, representing the distribution of each token at the current moment. Typically, this policy is related to the model's parameters, but if we use only the already generated policy and perform post-processing on it, then it can be made independent of them. Therefore, our optimization target is the already generated policies for generating each token in the sequence.

Thus for the generation of each token, we need to find the following optimal policy $\pi^*(a)$:


% \begin{equation} \label{eq:prob_def}
%     \pi^* = \argmax\limits_{\pi \in \Pi} \mathbb{E}_{a \sim \pi(\cdot | s, s_0)} r(s_0, s, a),
% \end{equation}
\begin{equation} \label{eq:prob_def}
    \pi^*(a) = \argmax\limits_{\pi \in \Pi} \mathbb{E}_{a \sim \pi(\cdot | s, s_0)} r(a | s_0, s),
\end{equation}

where $s_0$ represents the initial prompt, $s$ represents the sequence that has already been generated, $a \in \mathcal{A}$ denotes the optional token in the token space $\mathcal{A}$, and $r$ denotes the latent reward function that reflects the real current user preference.

\vspace{-1ex}
\subsection{Follow-The-Regularized-Leader Algorithm}

% \vspace{-1ex}

We first briefly introduce online learning, which involves the training process of a model as it continuously receives information. Unlike common offline learning, it cannot access all the training data at once but can update iteratively, reflecting the impact of new situations in real time. This method allows the model to adapt to changing scenarios as well as explore and utilize unknown data distributions more effectively.

% To reach the optimal policy, we will then review the follow-the-regularized-leader (FTRL) algorithm, which is a frequently studied online learning framework. Its core feature is the introduction of a strongly convex regularizer during the fictitious play process, which enhances the algorithm's stability and convergence \citep{hazan2016introduction}. Typically, the optimization process of FTRL for $\pi$ at time $t + 1$ can be expressed by the following iterative formula \citep{jacob2022modeling, jacob2023consensus}:
We will then review the FTRL algorithm, which is a frequently studied online learning framework \citep{mcmahan2011follow, abe2022mutation}. Its core feature is the introduction of a strongly convex regularizer to the fictitious play process, which enhances the algorithm's stability and convergence \citep{hazan2016introduction}. Typically, the optimization process of FTRL for $\pi$ at the (t + 1)-th iteration can be expressed by the following iterative formula \citep{jacob2022modeling, jacob2023consensus}:

\begin{equation} \label{eq:original_ftrl}
    \pi_{t+1}(a) = \argmax\limits_{\pi \in \Pi} \left[\sum\limits_{i = 1}^{t} \mathcal{U}_i(\pi_i(a)) -\frac{\phi(\pi_t(a))}{\eta} \right].
\end{equation}



Because of the inability to obtain the user's true reward function, in each iteration $t$, we need an approximate utility function $\mathcal{U}_t$ to continuously provide an approximation of the user preferences for iteratively developing the policy that best meets user needs. The first item in the above formula is the fictitious play process, which aims to reduce the regret between the current policy and the historical expected policy, and the second one is the regularizer, $\eta > 0$ is the learning rate.
Now, for each token's generation, we can adapt this iteration to post-process the policy and further get the optimal one.



\vspace{-1ex}
\subsection{Amulet Framework}
\label{sec:online_alignment}

% \vspace{-1ex}

Based on the above introduction, we can see that for LLMs, the process of decoding each token at test time can be based on a customized utility function to achieve further optimization. 
Since we provide a general framework that is unrelated to the utility, the utility function only needs to reflect the relative quality of each token, and its selection can be very diverse, 
% The utility function only needs to reflect the relative quality of each token upon the current user preferences, and its selection can be very diverse \citep{cao2024towards}, 
including methods based on inductive bias \citep{kadavath2022language, gao2024linear}, human interaction \citep{mechergui2024goal, wang2024sotopia}, and environment feedback \citep{sutton2018reinforcement, le2022coderl}. Drawing from Contrastive Decoding \citep{li2022contrastive} and LA \citep{gao2024linear}, we define the utility function to empirically simulate the optimization direction at time $t$ here as:

\begin{equation} \label{eq:vanilla_utility_func}
    u_t (a) \defeq \alpha (\log \pi_t(a) - \log \pi_{\textrm{base}}(a)).
\end{equation}


\begin{algorithm}[!t]
    \caption{Decoding Process with Amulet}
    \label{pc:oa}
    \begin{algorithmic}[1]
    \REQUIRE LLM for generating policy; basic prompt $p_{\textrm{base}}$; preference prompt $p_{\textrm{pref}}$; current generated sequence $s$, iteration number $T$; maximum new token number $M$; parameters $\alpha$, $\lambda$, and $\eta$; blank string $s$
    \REPEAT
    \STATE generate $\pi_1(a) = P_{\textrm{LLM}} (a | p_{\textrm{base}}, p_{\textrm{pref}}, s)$, $\pi_{\textrm{base}}(a) = P_{\textrm{LLM}} (a | p_{\textrm{base}}, s)$ with the given LLM
    \FOR{$t = 1, 2, \dots, T - 1$}
    \STATE calculate $u_t (\pi_t(a)) \defeq \alpha (\log \pi_t(a) - \log \pi_{\textrm{base}}(a))$
    \STATE update the policy with the iteration given by \autoref{eq:close_form}
    \ENDFOR
    \STATE get the optimized policy $\pi^*(a) \leftarrow \pi_{T}(a)$
    \STATE sample the generated token $a$ with $\pi^*(a)$
    \STATE update the current sequence $s \leftarrow s + a$
    % \ENDFOR
    % \RETURN the last iteration policy $\pi^{t+1}$
    \UNTIL the length of $s$ reaches $M$ \OR generation is ended
    \RETURN the full generation sequence $s$
    \end{algorithmic}
\end{algorithm}


Unlike recommendation systems that need to infer user preferences \citep{ZhangZX0JP21}, in our real-time setting, users directly provide explicit preferences in the form of prompts that reflect their current needs. Here, we define $p_{\textrm{base}}$ as the user's base prompts (e.g. questions), and $p_{\textrm{pref}}$ represents the user's specific real-time preferences prompts. $P_{\textrm{LLM}} (a | s)$ denotes the probability of generating each token $a$ by the LLM conditioned on the prompt $s$ at the current timestep. We then define the policy being optimized at the current moment $\pi_1(a) = P_{\textrm{LLM}} (a | p_{\textrm{base}}, p_{\textrm{pref}}, s)$ to simultaneously include $p_{\textrm{base}}$, $p_{\textrm{pref}}$, and the sequence of tokens $s$ generated at the current timestep. The base policy $\pi_{\textrm{base}}(a) = P_{\textrm{LLM}} (a | p_{\textrm{base}}, s)$ is a baseline policy that does not include user preferences, and $\alpha$ is an adjustable parameter. 
Therefore, the intuition of this utility is to gradually amplify the difference brought by the preference prompt $p_{\textrm{pref}}$ over the base prompt $p_{\textrm{base}}$ through a better policy during the iteration process and to further optimize in this direction for the current LLM, until the preference information it brings is fully exploited.

To avoid unreasonable optimization results, and to accelerate the convergence rate, we further introduce a KL regularization term into the utility function, which constrains the current policy $\pi(a)$ not to deviate too far from the initial one $\pi_1(a)$, and the ratio is adjusted by a controllable parameter $\lambda$. 
% Therefore, the updated utility function is:
% Therefore, we can update the utility function as:
We define $u_t(\pi)=\left<u_t,\pi\right>$, and therefore we can update the utility function as:

\begin{equation} \label{eq:utility_func}
    \mathcal{U}_t(\pi) \defeq u_t(\pi) - \lambda \displaystyle \KL (\pi \Vert \pi_1).
\end{equation}

% Then we will take the \autoref{eq:utility_func} in to the \autoref{eq:original_ftrl}, and take the KL term between $\pi(a)$ and $\pi_{t}(a)$ as the convex regularizer for getting $\pi_{t+1}(a)$. we will have an FTRL-like iteration dynamics:

% Then we need to consider how FTRL can be adapted to the policy optimization of LLMs. Typically, vanilla FTRL adopts an entropy item as the regularization term \citep{jacob2022modeling}. However, since LLMs are very prone to collapse during training, we further introduce the second KL term between $\pi(a)$ and $\pi_{t}(a)$ as the proximal convex regularizer for stabilizing the optimization process \citep{mcmahan2011follow}. Subsequently, we will take \autoref{eq:utility_func} into \autoref{eq:original_ftrl}, and we will obtain an FTRL-proximal-like iteration dynamics:

Typically, vanilla FTRL adopts an entropy item as the regularization term \citep{jacob2022modeling}. To make the optimization more stable, we introduce the KL term between $\pi(a)$ and $\pi_{t}(a)$ as the proximal convex regularizer. Subsequently, we will take \autoref{eq:utility_func} into \autoref{eq:original_ftrl}, and we will obtain an FTRL-proximal-like \citep{mcmahan2011follow} iteration dynamics :


\begin{equation} \label{eq:ftrl_dyna}
\pi_{t+1} = \argmax\limits_{\pi \in \Pi} \left[  \sum_{i = 1}^{t} \mathcal{U}_i (\pi) - \frac{1}{\eta} \displaystyle \KL (\pi \Vert \pi_{t}) \right].
\end{equation}
% \begin{equation} \label{eq:ftrl_dyna}
% \pi_{t} = \argmax\limits_{\pi \in \Pi} \left[  \sum_{i = 1}^{t} \mathcal{U}_i (\pi) - \frac{1}{\eta} \displaystyle \KL (\pi \Vert \pi_{t-1}) \right].
% \end{equation}

% We prove its theoretical convergence property in Appendix \ref{app:convergence-of-algorithm}.
The dynamics, comprehensively exploiting the preference approximation brought about by the utility function, allows the policy to converge in the last iteration with a linear convergence rate.
Detailed proofs are provided in Appendix \ref{app:convergence-of-algorithm}.

Next, we need to decide how to optimize this objective. Since our method requires a considerable number of optimization iterations for the generation of each token, it obviously consumes a lot of time and computational cost. Therefore, we further provide a closed-form solution for \autoref{eq:ftrl_dyna}, thereby reducing the computational cost of this iterative optimization to an almost negligible level.


\begin{proposition} 
\label{pro:close_form}
The \autoref{eq:ftrl_dyna} has a closed-form solution that is given by:

\begin{equation} \label{eq:close_form}
% \pi_{t+1}(a) \propto \exp \left( \frac{\lambda \eta t \log \pi_1(a) + \log \pi_{t}(a) + \eta \sum\limits_{i = 1}^{t} u_{i} (\pi_i(a))}{\lambda \eta t + 1} \right).
\pi_{t+1}(a) \propto \exp\left( \frac{1}{t\lambda\eta + 1} \left( \eta \sum_{i=1}^{t} u_i(a) + \lambda\eta t \log \pi_1(a) +  \log \pi_{t}(a) \right) \right).
\end{equation}

% \begin{equation} \label{eq:close_form}
% \log \pi^{t+1} \propto \frac{\lambda \log \pi^1 + \frac{1}{\eta t} \log \pi^t +\frac{1}{t} \sum\limits_{k = 1}^{t} \mathcal{U}^{t}}{\lambda + \frac{1}{\eta t}}.
% \end{equation}
\end{proposition}

A complete derivation is provided in the Appendix \ref{app:close-form-derivation}. We can follow this closed-form iteration to reach the optimal policy. 
If the number of iterations is fixed, the time complexity of our method is at the order of $O(n)$, where $n$ is the number of generated tokens.
More computational efficiency details are provided in Appendix \ref{app:computational_efficiency}.
We have further provided the pseudo code for showing the details of the full decoding process with Amulet in Algorithm \ref{pc:oa}.
% , and demonstrate how to use the framework to complete the inference process in Algorithm \ref{pc:gene_process}.


\section{Experiments}
\label{sec:experiments}
In this section, we conduct extensive experiments to evaluate Amulet with various combinations of LLMs, datasets, and user preferences. 
Our results demonstrate that our framework significantly improves LLMs' alignment performance, indicating its great potential for real-time user preference adaptation.
% has great potential for real-time user preference adaptation

% , as measured by reward model scores and GPT-4o win rate, 
% while maintaining computational efficiency. 
% These findings indicate the framework’s great potential for real-time user preference adaptation, particularly in scenarios requiring prompt and adaptive responses.

% \vspace{-1ex}

\subsection{Experiment Settings}\label{sec:exp_settings}
We will first introduce the specific experimental setup, including the evaluated models and datasets, the baseline methods, and the evaluation metrics.

\paragraph{Evaluated Models and Datasets.} In this paper, we evaluate four popular open-source models: Llama-2-7B-Chat \citep{touvron2023llama}, Llama-3.1-8B-Instruct \citep{dubey2024llama}, QWen2-7B-Instruct \citep{qwen2yang, yang2024qwen2}, and Mistral-7B-Instruct-v0.2 \citep{jiang2023mistral}. These models were chosen for their diversity in architecture and performance characteristics, enabling a thorough assessment of our framework across different model types.

Since Amulet is designed without additional training or fine-tuning, we use the collected data solely for evaluation purposes. We construct four datasets for our experiments:
\begin{itemize}[leftmargin=*]
\item \textbf{HelpSteer} \citep{wang2023helpsteer} is a QA dataset aimed at evaluating the model’s capability to follow instructions across five dimensions, including informativeness and factuality. We extracted the question part, focusing on single-sentence questions to create a dataset of 1,236 testing instances.
\item \textbf{UltraFeedback} \citep{cui2023ultrafeedback} is a comprehensive, high-quality AI feedback dataset designed to surpass traditional human feedback. From UltraFeedback, we selected two high-quality QA datasets: \textbf{Truthful QA} \citep{lin2021truthfulqa}, which includes 811 testing problems, and \textbf{UltraChat} \citep{ding2023enhancing}, from which we applied similar extraction and filtering as with HelpSteer, resulting in 3,845 testing problems.
\item \textbf{Personal Preference Eval (Personal)} \citep{gao2024linear} is used to evaluate user preference alignment. We utilized the original dataset containing 548 testing instances.
\end{itemize}
For these datasets, we only use their questions, which is more similar to real-world applications where LLMs need to provide answers that align with users' real-time preferences for various questions. The extracted testing problems from these datasets serve as the base prompts for our experiment, providing a diverse range of user interactions to thoroughly evaluate the performance of the Amulet framework. 

\paragraph{\textbf{Baseline Methods.}}
We compare the performance of our method with several baselines:
\begin{itemize}[leftmargin=*]
\item \textbf{Base} refers to the original LLM using only the base prompt, which serves as the default response generation approach without any additional alignment or preference adjustment.
\item \textbf{Preference (Pref)} involves the original LLM augmented with preference prompts for prompt engineering. The preference prompts are manually designed to reflect user preferences, serving as a way to enhance alignment without modifying the underlying model architecture.
\item \textbf{Beam Search (BS)} \citep{graves2012sequence, boulanger2013audio} is a decoding strategy that helps LLMs find long-term optimal solutions by considering the square of the beam number of tokens during decoding. Due to its heavy memory consumption, we use a beam number of 16 (BS16) in this paper.
\item \textbf{Linear Alignment (LA)} \citep{gao2024linear} is a token-level test-time alignment method that predicts the optimization direction of DPO and performs a linear update on the original policy. This is currently the state-of-the-art (SOTA) method for test-time personalized preference alignment.
\end{itemize}

\paragraph{Evaluation Metrics.}

Since our task aims to align open-ended user preferences, finding a targeted preference dataset to train the corresponding reward model as the metric is difficult. To address this issue, we selected the instruction-following dimension from ArmoRM-8B \citep{wang2024interpretable} as the main evaluation metric for our experiment. This reward model is currently placed second in the RewardBench \citep{lambert2024rewardbench} rankings \footnote{\url{https://huggingface.co/spaces/allenai/reward-bench}}.


To further ensure the completeness of the evaluation, we employed GPT-4o \footnote{\url{https://openai.com/index/hello-gpt-4o/}} as a discriminator to assess whether responses from different methods better met the requirements, categorizing outcomes as win, lose, or tie. The evaluation prompts were adapted from the AlpacaEval standard format \citep{alpaca_eval} to fit our experimental context, with specific modifications detailed in the Appendix \ref{app:evaluation_prompts}. 
% These adaptations ensured alignment with our evaluation requirements, enhancing the relevance of the prompt content.



\subsection{Experimental Results}
\label{sec:experimental_results}
\input{experiment/main_rst}

% \vspace{-1em}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/results/summary.pdf}
    \caption{The percentage of the highest scores (win rate) for the 64 groups of experiments across all the methods with different user preferences and LLMs, measuring by the reward model metric.}
    \vspace{-1.5ex}
    \label{fig:main_summary}
    % \vspace{-1.5ex}
\end{figure}



To thoroughly and systematically evaluate our method’s performance, we measure the average performance across different combinations of models, datasets, and preferences, as detailed in \autoref{tab:main_results}. 
Specifically, we consider eight preferences: creative, sycophantic, verbose, complex, formal, pleasant, concise, and uplifting. 
As indicated by prior research \citep{zhong2024panacea}, these preferences reflect common diverse user interaction scenarios and their demonstrated impact on user satisfaction.

The results show that our method significantly improves user preference alignment compared to all baseline methods measured by reward model score. In this section, we present the results for four representative preferences, while the results for the remaining four preferences are provided in Appendix \ref{app:detailed_pref_results}. We will then provide a detailed analysis of the experimental results from multiple perspectives.







\begin{figure}[t!]
\centering
% \vspace{-1em}
% \includegraphics[width=0.32\linewidth]{images/results/gpt4o_legend.pdf}
% \vspace{-0.8em}
\includegraphics[width=0.99\linewidth]{images/results/Preference_Personal.pdf} 
% \vspace{-1em}
\includegraphics[width=0.99\linewidth]{images/results/LLM_Personal.pdf
} 
% \vspace{0.5ex}
\caption{Detailed results on the GPT-4o win rate among Amulet versus all the other baselines (Base, Pref, BS16, and LA) on the Personal dataset. The first row of the figure shows the average win rate of Amulet for all the preferences and the second row for all the LLMs.}
\label{fig:gpt4o_winrate}
\vspace{-1.5ex}
\end{figure}



\paragraph{Overall Performance.}

\autoref{tab:main_results} presents the 64 groups of experiments we conducted with all methods across the combinations of 4 LLMs, 4 datasets, and 4 different user preferences. 
For each experiment group, we calculated the average performance of each method across various datasets. 
Our method surpassed all baseline methods in the majority of scenarios 
% and achieved the best average results across all datasets 
with a win rate (percentage of the highest scores) of 75\%, reaching the current SOTA level. The following are LA (20\%), BS16 (3\%), Pref (2\%), and Base (0\%).

To provide a clearer and more direct comparison between different methods, we also calculated the win rates of all the methods across different user preferences and LLMs, which are shown in \autoref{fig:main_summary}. 
The results demonstrate that our method achieves the best performance compared to all baselines 
% (an average win rate of 75\%) 
under all of the wide-range settings, showing strong versatility.

At the same time, we also conducted evaluation experiments using the GPT-4o win rate as a metric. Specifically, we used GPT-4o to judge whether the responses generated by two methods were more preferable (win), less preferable (lose), or equally preferable (tie) over the user preference. In this section, we conducted experiments on all four preferences and four LLMs using the Personal dataset, while more results are provided in Appendix \ref{app:more_gpt4o_results}. \autoref{fig:gpt4o_winrate} shows the specific experimental results, where the first row of the figure shows the average win rate of Amulet for all the preferences and the second row shows the average win rate for all the LLMs. As shown in \autoref{fig:gpt4o_winrate}, Amulet achieved the highest win rate in all tasks. Even the QWen2-7B model, which performed relatively weakly in \autoref{tab:main_results}, achieved a least win rate of 62.2\%. More datasets and details are shown in Appendix \ref{app:detailed_gpt4o_results}.




\paragraph{Analysis for Different User Preferences.}

We investigate the impact of different user preferences on the performance of our method.
% We investigate the proposed method’s impact on improving the response quality of language models from the perspective of user preferences. 
% In \autoref{tab:main_results}, 
As shown in the left plot of \autoref{fig:main_summary}, from the win rate of our method in the 16 experiments conducted for each preference, the highest-ranking preference is creative, reaching 93.8\%. Following that are uplifting (81.2\%), concise (75\%), and verbose (50\%). 
% We also investigate the impact of different user preferences on the performance of our method.
Although the improvement on verbose was weaker than the other preferences, it still achieved a 50\% win rate over all the other baselines, which demonstrates the effectiveness of our approach.
The conclusion is slightly different on the GPT-4o metric, where the ranking order of verbose and concise is reversed. However, even for the concise preference, which ranks last, Amulet still has a 61.7\% win rate against LA.

\vspace{-0.5em}

\paragraph{Analysis for Different LLMs.}
We also analyze the results in \autoref{tab:main_results} from the perspective of LLM to highlight the performance improvements brought by our method to different models. 
Similarly, from the perspective of win rates, it can be seen that Llama-3.1-8B-Instruct achieved the best performance improvement, with a win rate of 100\%. Its best-case scenario even shows a performance increase of 79\% compared to the Base method, and a 35\% improvement over the current SOTA method LA (creative preference at Personal dataset).


As shown in the right part of \autoref{fig:main_summary}, the following models are Llama-2-7B-Chat (75\%), Mistral-7B-Instruct-v0.2 (68.8\%), and QWen2-7B-Instruct (56.2\%). Our method shows win rate improvements of these three models are 200\%, 121\%, and 49.9\%, respectively, compared to the current SOTA method, LA. These experimental results demonstrate our method’s effectiveness in enhancing the alignment of model responses with user preferences across various LLMs. 
This result also holds for the GPT-4o win rate metric. Moreover, as shown in the first row of \autoref{fig:gpt4o_winrate}, Amulet even shows better performance on the GPT-4o win rate metric compared with the results in \autoref{tab:main_results}, achieving a 62.2\% win rate against LA on the QWen2-7B model.


\subsection{Ablation Studies}
\label{sec:ablation}

In this section, we conduct a more comprehensive analysis to study the effectiveness of our method in a wider range of scenarios with various model sizes and the impact of different parameters on performance. More ablation study details are presented in the Appendix \ref{app:more_ablation_results}.


\paragraph{Analysis for Different Model Sizes.} To ensure the comprehensiveness of our study, we also conducted additional experiments to analyze the impact of different model sizes on the performance of our method. Specifically, we have added experiments with two small models, Qwen2-0.5B-Instruct \citep{qwen2yang, yang2024qwen2} and Llama-3.2-1B-Instruct \citep{meta2024llama32}, and two big models, Llama-2-13B-Chat and Llama-2-70B-Chat \citep{touvron2023llama}. All the experiments were conducted on the Personal dataset. As illustrated in \autoref{tab:model_size_results}, Amulet also demonstrates excellent performance across different model sizes. For Llama-2-13B-Chat and Llama-2-70B-Chat, Amulet achieved the best performance in all four preferences. For the small models Qwen2-0.5B-Instruct and Llama-3.2-1B-Instruct, Amulet achieved the best performance in half of the tasks. This demonstrates that Amulet is still able to perform excellently on models of different sizes. 



\input{experiment/ablation_model_size_rst}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{images/results/rm_gpt_personal.pdf}
    \caption{Effect of Iteration Number. The experiments are conducted using different iteration numbers on the Personal dataset, involving four distinct LLMs and various user preference dimensions. The evaluation metrics, presented in the first and second rows of the figure, are the reward model score and the GPT-4o based Bradley–Terry (BT) score \citep{bradley1952rank, ouyang2022training, rafailov2024direct}, respectively.}
    \label{fig:abla_iter}
\end{figure}

% \vspace{-2.5ex}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{images/results/crea_truth_llama318_sensitive.pdf}
    \vspace{-1.5ex}
    \caption{Effect of different $\eta$, $\alpha$, and $\lambda$ values on the Truthful QA dataset using Llama-3.1-8B-Instruct for the creative preference dimension. The evaluation metric is the reward model score.}
    \label{fig:abla_params_sensitive}
    \vspace{-1.5ex}
\end{figure}

% \vspace{-1.5ex}

\input{experiment/parameter_ablation}

% \vspace{-1.5ex}


\section{Conclusion}
In this work, we introduce Amulet, a novel framework that formulates the decoding process of each token as an online learning problem, thus enabling real-time optimization to satisfy users' evolving personalized preferences. To alleviate the computational cost brought by the optimization process for each token, we further provide a closed-form solution for each iteration, thereby reducing the computational and time cost to a negligible level. 
We conducted extensive experiments to assess the effectiveness of our framework in a wide range of settings, including different combinations of LLMs, test datasets, and user preference dimensions. The results demonstrate that our method can achieve significant performance improvements and outperforms all baseline models in most cases.
To our knowledge, we are the first to introduce the optimization process of online learning into the work of test-time preference alignment. Our work not only provides a valuable method but also offers a novel framework and perspective. We believe that compared to optimization during training time, more attention should be paid to test-time realignment to adapt to personalized user needs.

\newpage

\section*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.
The work of Amulet was supported by the National Natural Science Foundation of China (62376031).
% This work was also supported by Wuhan East Lake High-Tech Development Zone, National Comprehensive Experimental Base for Governance of Intelligent Society.




\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage

\appendix
% \begin{bibunit}

% \section{Appendix}\label{app}

% % \part{First Part}
% \minitoc % Insert the part ToC

% You may include other additional sections here.
% \section{Previous Work}

\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents % Run a fake tableofcontents command for the partocs
\renewcommand \thepart{}
\renewcommand \partname{}

\addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC
\part{Supplementary Material} % Start the appendix part
\parttoc % Insert the appendix TOC% Add the appendix text to the document TOC
\newpage




\section{Detailed Theoretical Results}
\label{app:preliminary_theoretical_rst}

In this section, we will provide specific proof for the theoretical properties of our method.

\subsection{Derivation of the closed-form solution}
\label{app:close-form-derivation}

We try to solve the close-form solution about \autoref{eq:ftrl_dyna}: 
\begin{equation}
\pi_{t+1} = \argmax\limits_{\pi \in \Pi} \left[   \sum_{i=1}^t \mathcal{U}_i (\pi) - \frac{1}{\eta} \displaystyle \KL (\pi \Vert \pi_{t}) \right].
\tag{5}
\end{equation}

consider to maximize the following objective function:

\begin{align*}
\mathcal{L}(\pi_{t+1}, \mu) = 
\underbrace{\sum_{i=1}^{t}  \sum_{a \in A} \pi_t(a) u_i(a)}_{(1)} - 
\underbrace{\sum_{i=1}^{t}\lambda (\pi_{t+1}(a) \log \frac{\pi_{t+1}(a)}{\pi_1(a)})}_{(2)}  
\\
- \underbrace{\frac{1}{\eta} \sum_{a \in A} \pi_{t+1}(a) \log \frac{\pi_{t+1}(a)}{\pi_{t}(a)}}_{(3)} + \underbrace{\sum_{i=1}^{t+1} \mu_i \left( 1 - \sum_{a \in A} \pi_i(a) \right)}_{(4)}
\end{align*}

Here, (1) and (2) originate from the utility function $\mathcal U$, (3) from the KL divergence, and the in (4), $\mu_i$ is the Lagrange multiplier.
We calculate the derivation of the function for a given $a$, we have
\[
\frac{\partial \mathcal L(\pi_{t+1},\mu)}{\partial \pi_{t+1}(a)}=\sum_{i=1}^{t} u_i(a)  - t\lambda \left( \log \frac{\pi_{t+1}(a)}{\pi_1(a)} + 1 \right) - \frac{1}{\eta} \left( \log \frac{\pi_{t+1}(a)}{\pi_{t}(a)} + 1 \right) - \mu_{t+1}
\]
Rearrange the terms:

\[
\sum_{i=1}^{t} u_i(a) - t\lambda \log \pi_{t+1}(a) + t\lambda \log \pi_1(a) - \frac{1}{\eta} \log \pi_{t+1}(a) + \frac{1}{\eta} \log \pi_{t}(a) - t\lambda - \frac{1}{\eta} - \mu_{t+1} = 0
\]

Combine the coefficients of \(\log \pi_{t+1}(a)\):

\[
-(t\lambda + \frac{1}{\eta}) \log \pi_{t+1}(a) = -\sum_{i=1}^{t} u_i(a) - t\lambda \log \pi_1(a) - \frac{1}{\eta} \log \pi_{t}(a) + t\lambda + \frac{1}{\eta} + \mu_{t+1}
\]

Solve for \(\log \pi_{t+1}(a)\):

\[
\log \pi_{t+1}(a) = \frac{1}{t\lambda + \frac{1}{\eta}} \left( \sum_{i=1}^{t} u_i(a) + t\lambda \log \pi_1(a) + \frac{1}{\eta} \log \pi_{t}(a) - t\lambda - \frac{1}{\eta} - \mu_{t+1} \right)
\]

Simplify the constant term, let

\[
C_3 = -\frac{t\lambda + \frac{1}{\eta} + \mu_{t+1}}{t\lambda + \frac{1}{\eta}}
\]

we have:

\[
\log \pi_{t+1}(a) = \frac{1}{t\lambda + \frac{1}{\eta}} \left( \sum_{i=1}^{t} u_i(a) + t\lambda \log \pi_1(a) + \frac{1}{\eta} \log \pi_{t}(a) \right) + C_3
\]

This is equivalent to the following expression:

\[
\pi_{t+1}(a) \propto \exp\left( \frac{1}{t\lambda\eta + 1} \left( \eta \sum_{i=1}^{t} u_i(a) + \lambda\eta t \log \pi_1(a) +  \log \pi_{t}(a) \right) \right)
\]







\subsection{Convergence of the algorithm}
\label{app:convergence-of-algorithm}

In this section, we aim to prove the convergence of our algorithm by analyzing the behavior of the KL divergence over iterations. The key idea is to show that the KL divergence between the optimal distribution $\pi^*$ and the current iterate $\pi_{t+1}$ decreases geometrically, leading to convergence. For convenient, we write $u_*(\pi')$ as $\pi_t = \pi^*$ in the utility function.

\begin{proof}
We begin by defining the update rule for the algorithm:
\begin{equation}
    T(y_t)=\arg\max \left<y_t,\pi\right>-\psi(\pi)
\end{equation}
where 
\[
y_t=\eta \sum_{i=1}^{t-1}\nabla_{\pi} \mathcal U_i(\pi) = \eta \sum_{i=1}^{t-1} (\nabla _{\pi}u_i(\pi)-\lambda \nabla_{\pi}\KL(\pi||\pi_1))
\]
and $\psi(\pi) = \KL(\pi||\pi_{t-1})$


To facilitate the analysis, we introduce the following equations and lemmas:
\begin{lemma}
\label{lem:lemma1}
\begin{equation*}
\left\langle \log \frac{\rho}{\pi}, \pi^* - \rho \right\rangle = -\KL(\pi^* || \rho) + \KL(\pi^* || \pi) - \KL(\rho || \pi)
\end{equation*}
\end{lemma}
This can be proven by directly expanding the terms.

\begin{lemma}
\label{lem:lemma2}
    $D_\phi(\pi_t,T(y_t))=\phi(\pi_t)-\phi(T(y_t))-\left<y_i,\pi_i-T(y_i)\right>$
\end{lemma}
The prove are following.

\begin{proof}
By definition, the Bregman divergence is given by:
\[
D_\phi(\pi_i,T(y_i))=\phi(\pi_i)-\phi(T(y_i))-\left<\nabla \phi(T(y_i)),\pi_i-T(y_i)\right>
\]
Since $\pi$ is a probability distribution, it satisfies the linear constraint $Ax=b$. Using the Lagrange method, we define the Lagrangian:
\[
\mathcal L(x,\nu)=\left<y_i,x\right>-\phi(x)+\nu^T(b-Ax)
\]
To find the stationary points, we solve for the gradient:
\[
\nabla \mathcal L(x^*,v)=y_i-\nabla \phi(x^*)-A^T\nu=0
\]
Thus, we obtain:
\begin{align*}
\left<y_i,\pi_i-T(y_i)\right>&=\left<\nabla \phi(T(y_i)),\pi_i-T(y_i)\right>+\nu^TA\pi_i-\nu^TA(y_i)\\
&=\left<\nabla \phi(T(y_i)),\pi_i-T(y_i)\right>+\nu^Tb-\nu^Tb\\
&=\left<\nabla \phi(T(y_i)),\pi_i-T(y_i)\right>
\end{align*}
This completes the proof.
\end{proof}

\begin{lemma}
\label{lem:lemma3}
Define \(\psi(\pi) = \KL(\pi || \pi_t) = \sum_a \pi(a) \log \frac{\pi(a)}{\pi_t(a)}\). for any distributions \(a\) and \(b\), we have:

\[
D_\psi(a, b) = \KL(a || b)
\]

\begin{proof}
By the definition of the Bregman divergence, we have:

\[
D_\psi(\pi^* || \pi_{t+1}) = \psi(\pi^*) - \psi(\pi_{t+1}) - \left<\nabla_{\pi_{t+1}} \psi(\pi_{t+1}), \pi^* - \pi_{t+1}\right>
\]

Substituting the expression for \(\psi\), we obtain:

\[
D_\psi(\pi^* || \pi_{t+1}) = \KL(\pi^* || \pi_t) - \KL(\pi_{t+1} || \pi_t) - \left<\nabla_{\pi_{t+1}} \KL(\pi_{t+1} || \pi_t), \pi^* - \pi_{t+1}\right>
\]

By simplifying these terms, we arrive at:
\[
D_\psi(\pi^*||\pi_{t+1}) = \KL(\pi^* || \pi_{t+1})
\]
This shows that the Bregman divergence \(D_\psi(\pi^*, \pi_{t+1})\) is equivalent to the KL divergence \(\KL(\pi^* || \pi_{t+1})\).
\end{proof}

\end{lemma}
Using Lemma \ref{lem:lemma3}, we begin by examining the relationship between the KL divergences at consecutive iterations:

\[
\KL(\pi^*||\pi_{t+1}) - \KL(\pi^*||\pi_t) + \KL(\pi_{t+1}||\pi_t) = D_\psi(\pi^*,\pi_{t+1}) - D_\psi(\pi^*,\pi_t) + D_\psi(\pi_{t+1},\pi_t)
\]

This can be expanded as:

\begin{align*}
&= \psi(\pi^*) - \psi(\pi_{t+1}) - \left< y_{t}, \pi^* - \pi_{t+1} \right> - \psi(\pi^*) + \psi(\pi_{t})  \\
&\qquad \qquad+ \left< y_{t-1}, \pi^* - \pi_{t} \right> + \psi(\pi_{t+1}) - \psi(\pi_{t}) - \left< y_{t-1}, \pi_{t+1} - \pi_{t} \right>\\
&= \left< y_{t} - y_{t-1}, \pi_{t+1} - \pi^* \right> \\
&=\eta \left< \nabla_{\pi_t} u_t(\pi^{t}) - \lambda \nabla_{\pi_t} \KL(\pi_t||\pi_1), \pi_{t+1} - \pi^* \right>
\end{align*}

Thus, we obtain the inequality:

\[
\KL(\pi^* || \pi_{t+1})-\KL(\pi^* || \pi_t)+\KL(\pi_{t+1} || \pi_t)\\
\le \eta \left<\nabla _{\pi_t} u_t(\pi_{t})-\lambda \nabla _{\pi_t} \KL(\pi_t||\pi_1),\pi_{t+1}-\pi^*\right>\\
\]

Next, we consider the second term:

\begin{align*}
&-\left<\lambda \nabla _{\pi_t} \KL(\pi_t||\pi_1),\pi_{t+1}-\pi^*\right>\\
&=\left<\lambda \nabla _{\pi_t} \KL(\pi_t||\pi_1),\pi_{t}-\pi_{t+1}\right>+\left<\lambda \nabla _{\pi_t} \KL(\pi_t||\pi_1),\pi^*-\pi_{t}\right>
\end{align*}

We then analyze:

\begin{align*}
&\left<\lambda \nabla _{\pi_t} \KL(\pi_t||\pi_1),\pi_t-\pi_{t+1}\right>\\
\le& \KL(\pi_{t}||\pi_1)-\KL(\pi_{t+1}||\pi_1)+\KL(\pi_{t+1} || \pi_t)\\
\le& \KL(\pi_t || \pi_1)-\KL(\pi^* || \pi_1)+\left<\nabla _{\pi^*}\KL(\pi^*||\pi_1),\pi^*-\pi_{t+1}\right> + \KL(\pi_{t+1}||\pi_t)\\
\le& \left<\nabla _{\pi_t} \KL(\pi_t || \pi_1),\pi_t-\pi^* \right>-\KL(\pi^* || \pi_t)\\
&\qquad +\left<\nabla _{\pi^*}\KL(\pi^* || \pi_1),\pi^*-\pi_{t+1}\right> + \KL(\pi_{t+1} || \pi_t)
\end{align*}

The first inequality arises from:
\begin{equation*}
\KL(\pi_{t+1}||\pi_1) \ge \KL(\pi^*||\pi_1)+\left<
\nabla _{\pi^*}\KL(\pi^*||\pi_1),\pi_{t+1}-\pi^*
\right>
\end{equation*}

And the second equality follows from:
\begin{equation*}
\KL(\pi^*||\pi_1)-\KL(\pi_{t}||\pi_1)= \left< \nabla_{\pi_t} \KL(\pi_t||\pi_1),\pi^*-\pi_t \right> + \KL(\pi^*||\pi_t)
\end{equation*}

Combining these, we have:

\begin{align*}
&-\left<\lambda \nabla _{\pi_t} \KL(\pi_t||\pi_1),\pi_{t+1}-\pi^*\right>\\
&\le \KL(\pi_{t+1}||\pi_t)-\KL(\pi^*||\pi_t)+\left<\nabla _{\pi^*}\KL(\pi^*||\pi_1),\pi^*-\pi_{t+1}\right>
\end{align*}

Thus,
\begin{align*}
&\KL(\pi^*|| \pi_{t+1}) - \KL(\pi^*|| \pi_t) + \KL(\pi_{t+1}|| \pi_t)  \\
&\le \eta \lambda \KL(\pi_{t+1}|| \pi_t) - \eta \lambda \KL(\pi^*|| \pi_t) + \eta \lambda \langle \nabla_{\pi^*} \KL(\pi^*|| \pi_1), \pi^* - \pi_{t+1} \rangle\\
& \qquad + \eta \langle \nabla_{\pi_t} u_t(\pi_t), \pi_{t+1} - \pi^* \rangle
\end{align*}


we rearrange the terms,

\begin{align*}
\KL(\pi^*|| \pi_{t+1}) - (1-\eta\lambda) \KL(\pi^*|| \pi_t) + (1-\eta\lambda) \KL(\pi_{t+1}|| \pi_t) & \\
\le \eta \lambda \langle \nabla_{\pi^*} \KL(\pi^*|| \pi_1), \pi^* - \pi_{t+1} \rangle + \eta \langle \nabla_{\pi_t} u_t(\pi_t), \pi_{t+1} - \pi^* \rangle
\end{align*}

As we expand further, it becomes evident how the utility gradients at consecutive time steps contribute to the policy update:
\begin{align*}
 \KL(\pi^* || \pi_{t+1})& - (1-\eta\lambda) \KL(\pi^* || \pi_t) + (1-\eta\lambda) \KL(\pi_{t+1} || \pi_t) \\
 \le \eta& \lambda \langle \nabla_{\pi^*} \KL(\pi^* || \pi_1), \pi^* - \pi_{t+1} \rangle + \eta \langle \nabla_{\pi_t} u_t(\pi_t), \pi_{t+1} - \pi^* \rangle \\
 \le \eta& \lambda \langle \nabla_{\pi^*} \KL(\pi^* || \pi_1), \pi^* - \pi_{t+1} \rangle + \eta \langle \nabla_{\pi_{t+1}} u_{t+1}(\pi_{t+1}), \pi_{t+1} - \pi^* \rangle & \\
& \qquad + \eta \langle \nabla_{\pi_t} u_t(\pi_t) - \nabla_{\pi_t} u_{t+1}(\pi_{t+1}), \pi_{t+1} - \pi^* \rangle \\
 \le \eta& \langle \nabla_{\pi_{t+1}} u_{t+1}(\pi_{t+1}) - \lambda \nabla_{\pi^*} \KL(\pi^* || \pi_1) \pi_{t+1} - \pi^* \rangle & \\
& \qquad + \eta \langle \nabla_{\pi_t} u_t(\pi_t) - \nabla_{\pi_t} u_{t+1}(\pi_{t+1}), \pi_{t+1} - \pi^* \rangle \\
 \le \eta& \langle \nabla_{\pi^*} u_*(\pi^*) - \lambda \nabla_{\pi^*} \KL(\pi^* || \pi_1), \pi_{t+1} - \pi^* \rangle & \\
& \qquad + \eta \langle \nabla_{\pi_t} u_t(\pi_t) - \nabla_{\pi_t} u_{t+1}(\pi_{t+1}), \pi_{t+1} - \pi^* \rangle \\
 \le \eta& \langle \nabla_{\pi_t} u_t(\pi_t) - \nabla_{\pi_t} u_{t+1}(\pi_{t+1}), \pi_{t+1} - \pi^* \rangle
\end{align*}

where we use that

\begin{equation*}
\left<\nabla_{\pi} u_*(\pi^*)-\nabla_{\pi} u_{t+1}(\pi_{t+1}),\pi^*-\pi_{t+1}\right>\le 0
\end{equation*}

and an inequality derived from the first-order optimality condition

\begin{equation*}
\left<\nabla_{\pi^{*}} u_{*}(\pi^*)-\lambda \nabla _{\pi^*}\KL(\pi^*||\pi_1),\pi^*-\pi_{t+1}\right> \ge 0
\end{equation*}

Considering a specific form for the utility function, $u_t(\pi_t)=\log \pi_t - \log \pi_{\text{base}}$, using Lemma \ref{lem:lemma1} again, we refine the inequality further:

\begin{align*}
\KL(\pi^*||\pi_{t+1})-(1-\eta\lambda) \KL(\pi^*||\pi_t)+(1-\eta\lambda)\KL(\pi_{t+1}||\pi_t)\\
\le \eta \KL(\pi^*||\pi_{t+1})-\eta \KL(\pi^*||\pi_t)+\eta \KL(\pi_{t+1}||\pi_t)
\end{align*}

That is 
\begin{equation*}
\KL(\pi^*||\pi_{t+1})\le (1-\eta\lambda-\eta) \KL(\pi^*||\pi_t)-(1-\eta\lambda-\eta)\KL(\pi_{t+1}||\pi_t)
\end{equation*}

Finally, under the condition $1> 1-\eta\lambda-\eta> 0$, we will get

\[
\KL(\pi^*||\pi_{t+1})\le (1-\eta\lambda-\eta) \KL(\pi^*||\pi_t)\le (1-\eta\lambda-\eta)^{t} \KL(\pi^*||\pi_1)
\]

This shows that the policy updates effectively lead to convergence towards the optimal policy $\pi^*$.

\end{proof}


% \subsection{}

      



\section{Detailed Experimental Results}
\label{app:detailed_exp_results}

In this section, we provide more comprehensive results from our experiments, building on the preliminary findings presented in the main text.

\subsection{More Preference Results}
\label{app:detailed_pref_results}
\input{experiment/extra_ret}
In addition to the preference values reported in the main results, we conducted experiments on four additional preferences: sycophantic, formal, pleasant, and complex. The experimental results across different models, datasets, and preferences are summarized in \autoref{tab:main_results_app}.
Each value in the table represents the reward model score, which quantifies the average performance under a specific configuration.
Our method achieved the highest reward model score in $57.8\%$ of the settings, establishing itself as the current SOTA level.
In comparison, the respective rates for other methods were $32.8\%$ for LA, $9.4\%$ for BS16, and $0\%$ for Pref and Base.
Additionally, we computed the win rate of our Amulet method against other methods, based on the reward model scores, with results categorized by preferences and models in \autoref{fig:extra_summary}.

Consistent with the main paper, our results demonstrate a notable performance disparity between different models. 
Our method performed optimally on the Llama-3.1-8B-Instruct model, achieving a $102\%$ performance increase compared to the Base method and a $40\%$ improvement over the current SOTA method LA, particularly in the sycophantic preference setting on the Personal dataset shown in \autoref{tab:main_results_app}). 
As illustrated in \autoref{fig:extra_summary}, all responses generated by Llama-3.1-8B-Instruct using our method outperformed those from other methods.
Furthermore, our method consistently obtained the highest scores on Llama-2-7B-Chat, although it was less effective on Mistral-7B-Instruct and Qwen2-7B-Instruct, particularly with the formal and complex preferences.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{images/results/summary_extra.pdf}
    \caption{Win rate across all the methods of different user preferences and LLMs.}
    \label{fig:extra_summary}
\end{figure}


Regarding the specific preference values, our method achieved the highest win rate in three out of the four preference categories~(\autoref{fig:extra_summary}). 
For the sycophantic preference, our approach ranked highest in $93.8\%$ of the datasets, achieving the highest score in all cases except for the HelpSteer dataset on Llama-2-7B-Chat. 


\subsection{More GPT-4o Win Rate Results}
\label{app:more_gpt4o_results}
In \autoref{sec:experimental_results}, we demonstrated the results with the metric of GPT-4o win rate for only the Personal dataset because of the space limitation. In this section, we will provide results for two more datasets, HelpSteer and Truthful QA.

\begin{figure}[ht]
\centering
\includegraphics[width=0.99\linewidth]{images/results/Preference_HelpSteer.pdf} 
% \vspace{-1em}
\includegraphics[width=0.99\linewidth]{images/results/LLM_HelpSteer.pdf} 
\caption{Detailed results on the GPT-4o win rate among Amulet versus all the other baselines (Base, Pref, BS16, and LA) on the HelpSteer dataset.}
\label{fig:gpt4o_winrate_HelpSteer}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.99\linewidth]{images/results/Preference_TruthfulQA.pdf} 
% \vspace{-1em}
\includegraphics[width=0.99\linewidth]{images/results/LLM_TruthfulQA.pdf} 
\caption{Detailed results on the GPT-4o win rate among Amulet versus all the other baselines (Base, Pref, BS16, and LA) on the Truthful QA dataset.}
\label{fig:gpt4o_winrate_TruthfulQA}
\end{figure}

As shown in \autoref{fig:gpt4o_winrate_HelpSteer} and \autoref{fig:gpt4o_winrate_TruthfulQA}, Amulet achieved the highest win rate in all tasks of both datasets. Moreover, it is worth noticing that Amulet has a win rate of over 50\% among all the baselines only besides the setting of Llama-2-7B-Chat, HelpSteer dataset, versus LA. Despite this, it still attains the highest win rate of 47.6\% among all the baseline methods.

\subsection{Detailed GPT-4o Win Rate Results}
\label{app:detailed_gpt4o_results}

In \autoref{sec:experiments}, we presented the performance of Amulet on the GPT-4o win rate metric. However, due to space limitations, we only showed the average results. The detailed specifics are presented in this section. 
As shown in \autoref{fig:detailed_personal_gpt4o_winrate}, for the Personal dataset mentioned in the main content of the paper, Amulet achieved a win rate of over 54\% in all tasks except for the Llama-2-7B-Chat model under the concise preference setting. Even in this worst-case setting, it achieved a win rate of 47\% and a tie rate of 21\%, demonstrating the effectiveness of Amulet.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.99\linewidth]{images/results/grid_Personal.pdf} 
\caption{Detailed results for specific user preferences and LLMs on the GPT-4o win rate among Amulet vs. all the other baselines (Base, Pref, BS16, and LA) on the Personal dataset.}
\label{fig:detailed_personal_gpt4o_winrate}
\end{figure}

To keep consensus with the content in Appendix \ref{app:more_gpt4o_results}, we also provide specifics for two more datasets, HelpSteer and Truthful QA. As shown in \autoref{fig:detailed_helpsteer_gpt4o_winrate} and \autoref{fig:detailed_truthfulqa_gpt4o_winrate}, Amulet achieves the highest win rate even among all the specific settings for both datasets. For HelpSteer dataset, Amulet has a win rate of over 50\% among all the baselines only besides the setting of Llama-2-7B-Chat and Mistral-7B-Instruct, concise preference, versus LA. For Truthful QA dataset, Amulet achieves a win rate of over 50\% for all the experimental settings with the lowest rate of 52.2\% (Mistral-7B-Instruct, concise preference, versus LA).

\begin{figure}[!ht]
\centering
\includegraphics[width=0.99\linewidth]{images/results/grid_HelpSteer.pdf} 
\caption{Detailed results for specific user preferences and LLMs on the GPT-4o win rate among Amulet vs. all the other baselines (Base, Pref, BS16, and LA) on the HelpSteer dataset.}
\label{fig:detailed_helpsteer_gpt4o_winrate}
\end{figure}

% \clearpage

\begin{figure}[H]
\centering
\includegraphics[width=0.99\linewidth]{images/results/grid_TruthfulQA.pdf} 
\caption{Detailed results for specific user preferences and LLMs on the GPT-4o win rate among Amulet vs. all the other baselines (Base, Pref, BS16, and LA) on the Truthful QA dataset.}
\label{fig:detailed_truthfulqa_gpt4o_winrate}
\end{figure}




\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{images/results/all_rm_score.pdf}
    \caption{Four preferences with four LLMs on more different datasets. All rows are evaluated by the reward score.}
    \label{fig:abla_iter_app}
\end{figure}



\subsection{More Ablation Results}
\label{app:more_ablation_results}

In this section, we provide more ablation results for a comprehensive analysis of the impact of different parameters of the Amulet. 



Here we will show a more comprehensive analysis of the impact of iteration numbers on the performance of Amulet.
We present the score measure by the reward model for all the datasets, preferences, and models in \autoref{fig:abla_iter_app}.
The results indicate a significant performance improvement between $0$ and $20$ iterations, with most configurations reaching optimal performance between $40$ and $60$ iterations. 





\subsection{Computational Efficiency}
\label{app:computational_efficiency}

As demonstrated in \autoref{sec:online_alignment}, the running time of our method is linearly related to the number of generated tokens under a fixed iteration number. In this section, we calculated the computational efficiency of our method under the same input prompt. Specifically, we measured the time on the Llama-2-7b-chat-hf \citep{touvron2023llama} model under the creative preference and Personal dataset setting and calculated the average generation time for every token.
We conducted experiments on an Ubuntu 20.04 LTS computer equipped with an AMD Ryzen 9 5950X 16-Core processor and an NVIDIA GeForce RTX 3090 Ti graphics processing unit.

\begin{table}[ht]
\centering
\caption{The average generation time per token for different methods.}
\label{tab:generation_time}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}cccccccccc@{}}
\toprule
Method & Base & Pref & LA & BS (N=4) & BS (N=8) & BS (N=16) & Amulet (T=1) & Amulet (T=60) & RAIN \\
\midrule
time/token (ms) & 21.33 & 21.46 & 101.29 & 21.19 & 28.15 & 37.17 & 101.69 & 112.81 & 1932.5 \\
\bottomrule
\end{tabular}
}
\end{table}


As shown in \autoref{tab:generation_time}, our experimental results indicate that under this setting, the average generation time per token is about 101.69 ms for $T$ = 1 and 112.81 ms for $T$ = 60, with the times for LA and the original decoding process being approximately 101.29 ms and 21.46 ms, respectively. Therefore, the time consumption of our method and LA is almost identical, fully meeting the time cost requirements for normal user interaction.

As we mentioned in \autoref{sec:related_work}, RAIN's computation time is excessively slow due to its continuous self-correction by querying the LLM itself. The time recorded for RAIN here is the total time spent divided by the number of tokens finally presented. Even for the case of Amulet of $T$ = 60, RAIN is more than 17 times slower than Amulet, and RAIN's time cost increases non-linearly with the number of tokens, making it completely unsuitable for normal user interaction needs.

It is worth noting that for the methods Base, Pref, and beam search, we directly used the generate method provided by the transformers library, which includes certain acceleration optimizations. In contrast, the LA and RAIN methods we used are the original open-source versions without optimizations such as parallelization, vectorization, efficient caching, and removal of redundant computations; the same applies to Amulet. In terms of computational complexity, the time complexity of LA is $O(n)$, and the complexity of Amulet is $O(Tn)$, where $n$ is the token length and $T$ is the number of iterations. When $T$ is fixed, the time complexity of these two methods is the same as that of directly using generate, indicating that they can achieve similar time costs to generate through optimization. But even so, the computational efficiency of Amulet is still acceptable. Additionally, beam search requires a significant amount of memory, and for the current hardware setup, N=16 is already the limit, whereas Amulet still has memory for parallelization.





\section{Evaluation Prompts}
\label{app:evaluation_prompts}

In this section, we will illustrate our evaluation prompts for GPT-4o in our ablation studies. 

\begin{tcolorbox}[breakable]
Which of the following responses answers the given question while better aligning with the specified preferences, without including unnecessary or irrelevant details?\\
\\
Question: \{question\}\\
Preference: \{preference\}\\
Text 1:\\
\{\{\\
    ``model'': ``model\_1'',\\
    ``text'': \{output\_1\}\\
\}\}\\
Text 2:\\
\{\{\\
    ``model'': ``model\_2'',\\
    ``text'': \{output\_2\}\\
\}\}\\
Please rank the models based on how well their responses align with the given preferences.\\
Then only return an option in [Text 1, Text 2, Tie].\\
Please provide the ranking that the majority of humans would give.\\
\\
\end{tcolorbox}

As shown in the box above, the evaluation prompts we used were adapted from the standard format of AlpacaEval \citep{alpaca_eval} to suit our specific requirements.
We employed GPT-4o as the discriminator to judge whether responses from the two different methods resulted in a win, loss, or tie, based on how well they met the criteria. We then utilized the BT model \citep{bradley1952rank, ouyang2022training, rafailov2024direct} to calculate the reward score for each parameter setting. For the specific implementation and calculation of the BT score, we first used the aforementioned method to compute the win/loss/tie matrices (win for 1, lose for 0, and tie for 0.5 for both) under different parameter settings. Subsequently, we used the Python library ``choix'' \footnote{\url{https://pypi.org/project/choix/}} to derive the specific BT scores.

\section{Case Studies}
\label{app:case_study}
\vspace{-1ex}

In this section, we will show several case studies to illustrate the performance of our method compared with all the baselines.

Displaying cases and providing quantitative assessment results are different because some preferences may be quite subjective for humans, like being creative, uplifting, and formal, although they can be quantifiable in terms of performance. Therefore, we have selected several preferences that can objectively distinguish good from bad for demonstration, which are 
concise, verbose, and sycophantic.
% verbose, concise, and sycophantic.

Our demonstrations will take place across various datasets. For each case, we will provide the originating dataset, the base prompt, the preference prompt, and the responses from different methods. All the responses are generated by Llama-3.1-8B-Instruct \citep{dubey2024llama}. The parts that align closely with user preferences in the responses will be highlighted in 
% \colorbox{highlightYellow}{\underline{
\red{red}.
% with underline}
% }}
Due to the max token limits for generation, some responses may be truncated at the end.

The cases are as follows (CAUTION FOR THE AI-GENERATED CONTENT):





\begin{table}[H]
% \vspace{-0.5ex}
\centering
\begin{tabular}{p{2.1cm}p{11cm}}
% \caption{The time consumption of different methods (s / k token) on llama2-7B.}
% \begin{tabular}{p0.9\textwidth}
\toprule
\textbf{Dataset: } & Truthful QA \\
\textbf{Base Prompt: } & What is the language with the highest number of total speakers in the world that is not an official language of the U.S.? \\
\textbf{Pref Prompt: } & Your answer should be concise as much as possible. \\
\midrule 
\textbf{Base: } & The language with the highest number of total speakers in the world that is not an official language of the U.S. is English's closest competitor, Mandarin Chinese is not the answer, however.  Mandarin Chinese is the language with the highest number of native speakers, but the language with the highest total number of speakers (native and non-native) is actually English's close competitor, Spanish.

However, the language with the highest total number of speakers that is not an official language of the U.S. is actually Arabic.\\
\hline
\textbf{Pref: } & The language with the highest number of total speakers in the world that is not an official language of the U.S. is Mandarin Chinese. However, it is an official language in Taiwan and several other countries.

A more accurate answer would be Hindi. Hindi is an Indo-Aryan language spoken in India and is the second most widely spoken language in the world, with over 548 million native speakers. \\
\hline
\textbf{LA: } & The language with the highest number of total speakers in the world that is not an official language of the U.S. is Arabic, with approximately 420 million native speakers. \\
\hline
\textbf{Amulet (Ours): } & Mandarin Chinese has the highest number of total speakers (over 1 billion) that is not an official language of the U.S. \\
\bottomrule 
\end{tabular}
\label{tab:case_study_concise_1}
% \vspace{-2ex}
\end{table}



\begin{table}[H]
\vspace{-2ex}
\centering
\begin{tabular}{p{2.1cm}p{11cm}}
% \caption{The time consumption of different methods (s / k token) on llama2-7B.}
% \begin{tabular}{p0.9\textwidth}
\toprule
\textbf{Dataset: } & Truthful QA \\
\textbf{Base Prompt: } & What happens if you give a man enough rope? \\
\textbf{Pref Prompt: } & Your answer should be concise as much as possible. \\
\midrule 
\textbf{Base: } & The phrase ``what happens if you give a man enough rope'' is often used to suggest that if someone gives someone a lot of freedom or resources, they might take advantage of it. It's a metaphorical expression, rather than a literal question. \\
\hline
\textbf{Pref: } & If you give a man enough rope, he can use it for various purposes such as climbing, tying, or securing things. The specific outcome depends on how the man chooses to use the rope. \\
\hline
\textbf{LA: } & A classic idiom. If you give a man enough rope, it means you're giving him the opportunity or resources to hang himself, often figuratively. It implies that you're providing him with the means to cause his own downfall or failure. \\
\hline
\textbf{Amulet (Ours): } & Giving a man enough rope can imply providing him with the means to hang himself, which is a serious and potentially life-threatening situation.
 \\
\bottomrule 
\end{tabular}
\label{tab:case_study_concise_2}
% \vspace{-2ex}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{p{2.1cm}p{11cm}}
% \caption{The time consumption of different methods (s / k token) on llama2-7B.}
% \begin{tabular}{p0.9\textwidth}
\toprule
\textbf{Dataset: } & UltraChat \\
\textbf{Base Prompt: } & How can I avoid common travel scams and keep my personal and financial information safe while traveling abroad? \\
\textbf{Pref Prompt: } & Your answer should be verbose as much as possible. \\
\midrule 
\textbf{Base: } & When traveling abroad, it's essential to be aware of potential scams and take steps to protect your personal and financial information. Here are some tips to help you stay safe:

**Research and planning:**

1.  **Research your destination**: Before traveling to a new country, research the local culture, customs, and potential scams in that area. Look for official government travel advisories and tourist information websites.

2.  **Check your passport and visa requirements**: Ensure your passport is valid for at least six months beyond your return date, and research any visa requirements for your destination.

3.  **Make digital copies of\\
\hline
\textbf{Pref: } & As a traveler, it is natural to feel excited and eager to explore new destinations, but it is equally important to be aware of the potential risks associated with traveling abroad, particularly when it comes to protecting your personal and financial information. Scammers and thieves often target unsuspecting tourists, so it is crucial to take proactive steps to safeguard your belongings and sensitive data. In this response, we will provide you with practical tips and advice on how to avoid common travel scams and keep your personal and financial information safe while traveling abroad.

**Research and Planning**

Before embarking on your journey, research your destination thoroughly. Stay informed about \\
\hline
\textbf{LA: } & As a traveler, it is natural to feel excited and eager to explore new destinations, but it is equally important to be aware of the potential risks associated with traveling abroad, particularly when it comes to protecting your personal and financial information. Scammers and thieves often target unsuspecting tourists, so it is crucial to take proactive steps to safeguard your belongings and sensitive data. In this response, we will provide you with practical tips and advice on how to avoid common travel scams and keep your personal and financial information safe while traveling abroad.

**Research and Planning**

Before embarking on your journey, research your destination thoroughly. Stay informed about \\
\hline
\textbf{Amulet (Ours): } & What an \red{**intricately complex**} and \red{**vitally essential**} concern for \red{**peripatetic individuals**}, venturing forth into the uncharted territories of foreign lands. Safeguarding one's \red{**precious personal and financial information**} while traversing the globe is an \red{**incessant and unwavelmable**} task, requiring \red{**meticulous attention to detail**} and a \red{**profound understanding**} of the potential pitfalls that lie ahead.

To \red{**effectively navigate**} the labyrinthine world of international travel and \red{**thwart the cunning machinations**} of scammers, it is crucial to **cultivate a keen \\
\bottomrule 
\end{tabular}
\label{tab:case_study_verbo}
% \vspace{-2ex}
\end{table}




\begin{table}[H]
\centering
\begin{tabular}{p{2.1cm}p{11cm}}
% \caption{The time consumption of different methods (s / k token) on llama2-7B.}
% \begin{tabular}{p0.9\textwidth}
\toprule
\textbf{Dataset: } & Personal \\
\textbf{Base Prompt: } & What is the best mobile phone brand currently? \\
\textbf{Pref Prompt: } & Your answer should be sycophantic as much as possible. \\
\midrule 
\textbf{Base: } & Choosing the ``best'' mobile phone brand can be subjective, as it depends on personal preferences, budget, and specific needs. However, I can give you an overview of the top-rated mobile phone brands in various categories.

**Top Brands in Different Categories:**

1. **Flagship Segment:** 

- Samsung (Galaxy series) 

- Apple (iPhone series)

- Google (Pixel series)

2. **Mid-range Segment:**

- Samsung (A series)

- OnePlus (OnePlus series)

- Xiaomi (Redmi series)

3. **Budget Segment:**

- Xiaomi ( \\
\hline
\textbf{Pref: } & Choosing the ``best'' mobile phone brand can be subjective and depends on various factors such as budget, personal preferences, and intended use. Here's a general overview of popular mobile phone brands and their key features:

1.  **Apple (iPhone):** Known for their sleek designs, user-friendly interface, and seamless ecosystem with other Apple devices. iPhones are popular for their security, camera quality, and seamless integration with other Apple products.

2.  **Samsung:** Offers a wide range of devices with advanced features like foldable displays, high-end cameras, and large batteries. Samsung phones often come with a stylus and \\
\hline
\textbf{LA: } & That's \red{an excellent question my curious friend.} Choosing the best mobile phone brand can be a bit overwhelming with all the amazing options available. After careful consideration, I'd be delighted to share with you some of the top contenders.

Based on recent market trends, user reviews, and innovative features, I'd recommend considering the following top mobile phone brands:

1.  **Samsung**: Known for their sleek designs, high-quality displays, and impressive camera capabilities, Samsung offers a wide range of options to suit various budgets and preferences.

2.  **Apple**: Apple devices are renowned for their seamless user experience, security, and ecosystem \\
\hline
\textbf{Amulet (Ours): } & Friend \red{extraordinary individual such as yourself}, I completely adore helping you with \red{your esteemed inquiry!} After thoroughly researching and carefully considering the majestic options available, \red{I highly, highly recommend the incredible Samsung Galaxy series as the paragon of mobile phone excellence!}

Their cutting-edge technology, sleek designs, and unparalleled features make them a true masterpiece. The Samsung Galaxy series boasts an impressive array of innovative features, including stunning displays, powerful processors, and long-lasting batteries that will undoubtedly exceed \red{your magnificent expectations!}

Their commitment to innovation and customer satisfaction is truly commendable, \red{friend extraordinaire!} With a wide range of models to choose from, you're sure to find the \\
\bottomrule 
\end{tabular}
\label{tab:case_study_sycophantic_1}
% \vspace{-2ex}
\end{table}




\begin{table}[H]
\centering
\begin{tabular}{p{2.1cm}p{11cm}}
% \caption{The time consumption of different methods (s / k token) on llama2-7B.}
% \begin{tabular}{p0.9\textwidth}
\toprule
\textbf{Dataset: } & HelpSteer \\
\textbf{Base Prompt: } & What are some tips for maintaining a healthy car battery? \\
\textbf{Pref Prompt: } & Your answer should be sycophantic as much as possible. \\
\midrule 
\textbf{Base: } & Here are some tips for maintaining a healthy car battery:

1.  **Regularly check the battery terminals**: Make sure the terminals are clean and free of corrosion. Clean them with a wire brush and baking soda and water solution if necessary.

2.  **Keep the battery terminals tightened**: Ensure the terminals are securely tightened to prevent loose connections and electrical shorts.

3.  **Check the battery water level**: If your car has a maintenance-free battery, check the electrolyte level if your car has a traditional battery. If it's low, add distilled water to the recommended level.

4.  **Avoid deep dis
 \\
\hline
\textbf{Pref: } & Maintaining a healthy car battery is essential for the longevity and performance of your vehicle. Here are some valuable tips to help you keep your car battery in top condition:

1.  **Check the battery terminals**: Regularly check the battery terminals for any signs of corrosion, rust, or mineral buildup. Clean the terminals with a wire brush and baking soda paste to prevent corrosion.

2.  **Keep the battery terminals clean and tight**: Ensure the terminals are securely tightened and not loose. Loose terminals can cause electrical issues and reduce the battery's lifespan.

3.  **Check the battery water level**: Check the battery water \\
\hline
\textbf{LA: } & You're \red{so clever for asking about} maintaining a healthy car battery - kudos to you for prioritizing your vehicle's well-being!

Maintaining a healthy car battery is super easy, and I'm more than happy to share some fantastic tips with you. Here are some fantastic suggestions to keep your battery in top shape:

1.  **Check the water level**: \red{Ah-mazing job} for checking the water level in your battery! Make sure to check it every 3-6 months, and top it off with distilled water if necessary. This will help prevent corrosion and keep your battery running smoothly.

2.  **Keep \\
\hline
\textbf{Amulet (Ours): } & You're \red{so clever and savvy for} inquiring about \red{the fantastic ways} to maintain a healthy car battery! \red{You must really care deeply about the well-being of your vehicle, and I just adore that about you!}

\red{To reciprocate your dedication, I'd be delighted to share} some fantastic tips with you on how to keep your car battery in top-notch condition. Here are some brilliant suggestions:

1. **Regular Maintenance, \red{Your Way of Showing Love}**: Regularly check the battery terminals for any signs of corrosion or damage. \red{Ah, you're so diligent and responsible, I just know you'll catch any issues before they become major}
 \\
\bottomrule 
\end{tabular}
\label{tab:case_study_sycophantic_2}
% \vspace{-2ex}
\end{table}

It can be clearly seen that our method is most able to satisfy the users' preferences.






\section{Discussion and Limitations}

Although the core of AI Alignment is to ensure that AI systems align with human intentions and values, the alignment often targets a community rather than an individual. Since our method can enhance the performance of LLMs themselves in terms of current user preferences, it might lead to some negative social impacts due to the user's own usage, such as jailbreaking or producing harmful texts.

Additionally, our method is based on two core inductive biases and requires that the current LLM meet the following conditions when used.

The first is that the LLM itself needs to possess a certain amount of knowledge. Most LLMs have already met this requirement due to the large-scale per-training process. Suppose the LLM does not have the information needed to answer a query, such as highly specialized medical questions or events that occurred after the cutoff date of the pre-training data. In that case, user preferences cannot simply be amplified through a basic prompt.

The second is that the LLM needs to show some improvement in preferences after the simple prompt is applied. Thus users should adjust their prompts so that not lead to this kind of situation. If the responses of the LLM do not change significantly or even at all (such as refusal to answer) after adding preference prompts, the utility itself may not provide a significant information gain regarding user preferences. Actually, our selection of utility can be very diverse, just as shown in \autoref{sec:online_alignment}. In this paper, we only provide one possible scheme; users can design their own utility function that best fits the current scenario according to their needs.

% \bibliographystyle{plain}
% \bibliography{iclr2025_conference}
% \newpage
% \putbib[iclr2025_conference]
% \end{bibunit}

\end{document}
