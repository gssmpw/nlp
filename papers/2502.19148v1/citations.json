[
  {
    "index": 0,
    "papers": [
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "bai2022constitutional",
        "author": "Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others",
        "title": "Constitutional ai: Harmlessness from ai feedback"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lee2023rlaif",
        "author": "Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and others",
        "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "hong2024reference",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Reference-free monolithic preference optimization with odds ratio"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zelikman2024quiet",
        "author": "Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D",
        "title": "Quiet-star: Language models can teach themselves to think before speaking"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "lee2023rlaif",
        "author": "Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and others",
        "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "bai2022constitutional",
        "author": "Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others",
        "title": "Constitutional ai: Harmlessness from ai feedback"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hong2024reference",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Reference-free monolithic preference optimization with odds ratio"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "Simpo: Simple preference optimization with a reference-free reward"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zelikman2024quiet",
        "author": "Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D",
        "title": "Quiet-star: Language models can teach themselves to think before speaking"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "bai2022constitutional",
        "author": "Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others",
        "title": "Constitutional ai: Harmlessness from ai feedback"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "hong2024reference",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Reference-free monolithic preference optimization with odds ratio"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "Simpo: Simple preference optimization with a reference-free reward"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zelikman2024quiet",
        "author": "Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D",
        "title": "Quiet-star: Language models can teach themselves to think before speaking"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "ZhangZX0JP21",
        "author": "Yuji Zhang and Yubo Zhang and Chunpu Xu and Jing Li and Ziyan Jiang and Baolin Peng",
        "title": "{\\#}HowYouTagTweets: Learning User Hashtagging Preferences via Personalized Topic Attention"
      },
      {
        "key": "zhang2023vibe",
        "author": "Yuji Zhang and Jing Li and Wenjie Li",
        "title": "{VIBE}: Topic-Driven Temporal Adaptation for Twitter Classification"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "ji2024aligner",
        "author": "Ji, Jiaming and Chen, Boyuan and Lou, Hantao and Hong, Donghai and Zhang, Borong and Pan, Xuehai and Dai, Juntao and Yang, Yaodong",
        "title": "Aligner: Achieving efficient alignment through weak-to-strong correction"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "bai2024efficient",
        "author": "Bai, Fengshuo and Wang, Mingzhi and Zhang, Zhaowei and Chen, Boyuan and Xu, Yinda and Wen, Ying and Yang, Yaodong",
        "title": "Efficient Model-agnostic Alignment via Bayesian Persuasion"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "ji2024aligner",
        "author": "Ji, Jiaming and Chen, Boyuan and Lou, Hantao and Hong, Donghai and Zhang, Borong and Pan, Xuehai and Dai, Juntao and Yang, Yaodong",
        "title": "Aligner: Achieving efficient alignment through weak-to-strong correction"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "bai2024efficient",
        "author": "Bai, Fengshuo and Wang, Mingzhi and Zhang, Zhaowei and Chen, Boyuan and Xu, Yinda and Wen, Ying and Yang, Yaodong",
        "title": "Efficient Model-agnostic Alignment via Bayesian Persuasion"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "ji2024aligner",
        "author": "Ji, Jiaming and Chen, Boyuan and Lou, Hantao and Hong, Donghai and Zhang, Borong and Pan, Xuehai and Dai, Juntao and Yang, Yaodong",
        "title": "Aligner: Achieving efficient alignment through weak-to-strong correction"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "bai2024efficient",
        "author": "Bai, Fengshuo and Wang, Mingzhi and Zhang, Zhaowei and Chen, Boyuan and Xu, Yinda and Wen, Ying and Yang, Yaodong",
        "title": "Efficient Model-agnostic Alignment via Bayesian Persuasion"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "zheng2024weak",
        "author": "Zheng, Chujie and Wang, Ziqi and Ji, Heng and Huang, Minlie and Peng, Nanyun",
        "title": "Weak-to-strong extrapolation expedites alignment"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "wu2024empirical",
        "author": "Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming",
        "title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models"
      },
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters"
      },
      {
        "key": "liu2025can",
        "author": "Liu, Runze and Gao, Junqi and Zhao, Jian and Zhang, Kaiyan and Li, Xiu and Qi, Biqing and Ouyang, Wanli and Zhou, Bowen",
        "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "mitchell2023emulator",
        "author": "Mitchell, Eric and Rafailov, Rafael and Sharma, Archit and Finn, Chelsea and Manning, Christopher D",
        "title": "An emulator for fine-tuning large language models using small language models"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "liu2024decoding",
        "author": "Liu, Tianlin and Guo, Shangmin and Bianco, Leonardo and Calandriello, Daniele and Berthet, Quentin and Llinares, Felipe and Hoffmann, Jessica and Dixon, Lucas and Valko, Michal and Blondel, Mathieu",
        "title": "Decoding-time Realignment of Language Models"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "liu2024tuning",
        "author": "Liu, Alisa and Han, Xiaochuang and Wang, Yizhong and Tsvetkov, Yulia and Choi, Yejin and Smith, Noah A",
        "title": "Tuning language models by proxy"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "kong2024aligning",
        "author": "Kong, Lingkai and Wang, Haorui and Mu, Wenhao and Du, Yuanqi and Zhuang, Yuchen and Zhou, Yifei and Song, Yue and Zhang, Rongzhi and Wang, Kai and Zhang, Chao",
        "title": "Aligning Large Language Models with Representation Editing: A Control Perspective"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "li2023rain",
        "author": "Li, Yuhui and Wei, Fangyun and Zhao, Jinjing and Zhang, Chao and Zhang, Hongyang",
        "title": "Rain: Your language models can align themselves without finetuning"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "lin2023unlocking",
        "author": "Lin, Bill Yuchen and Ravichander, Abhilasha and Lu, Ximing and Dziri, Nouha and Sclar, Melanie and Chandu, Khyathi and Bhagavatula, Chandra and Choi, Yejin",
        "title": "The unlocking spell on base llms: Rethinking alignment via in-context learning"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "gao2024linear",
        "author": "Gao, Songyang and Ge, Qiming and Shen, Wei and Dou, Shihan and Ye, Junjie and Wang, Xiao and Zheng, Rui and Zou, Yicheng and Chen, Zhi and Yan, Hang and others",
        "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "liu2025can",
        "author": "Liu, Runze and Gao, Junqi and Zhao, Jian and Zhang, Kaiyan and Li, Xiu and Qi, Biqing and Ouyang, Wanli and Zhou, Bowen",
        "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling"
      }
    ]
  }
]