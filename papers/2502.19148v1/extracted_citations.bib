@inproceedings{ZhangZX0JP21,
  author       = {Yuji Zhang and Yubo Zhang and Chunpu Xu and Jing Li and Ziyan Jiang and Baolin Peng},
  title        = {{\#}HowYouTagTweets: Learning User Hashtagging Preferences via Personalized Topic Attention},
  booktitle    = {Conference on Empirical Methods in Natural Language Processing},
  pages        = {7811--7820},
  publisher    = {Association for Computational Linguistics},
  year         = {2021}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{bai2024efficient,
  title={Efficient Model-agnostic Alignment via Bayesian Persuasion},
  author={Bai, Fengshuo and Wang, Mingzhi and Zhang, Zhaowei and Chen, Boyuan and Xu, Yinda and Wen, Ying and Yang, Yaodong},
  journal={arXiv preprint arXiv:2405.18718},
  year={2024}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@article{gao2024linear,
  title={Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback},
  author={Gao, Songyang and Ge, Qiming and Shen, Wei and Dou, Shihan and Ye, Junjie and Wang, Xiao and Zheng, Rui and Zou, Yicheng and Chen, Zhi and Yan, Hang and others},
  journal={arXiv preprint arXiv:2401.11458},
  year={2024}
}

@article{hong2024reference,
  title={Reference-free monolithic preference optimization with odds ratio},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  journal={arXiv preprint arXiv:2403.07691},
  year={2024}
}

@article{ji2024aligner,
  title={Aligner: Achieving efficient alignment through weak-to-strong correction},
  author={Ji, Jiaming and Chen, Boyuan and Lou, Hantao and Hong, Donghai and Zhang, Borong and Pan, Xuehai and Dai, Juntao and Yang, Yaodong},
  journal={arXiv preprint arXiv:2402.02416},
  year={2024}
}

@article{kong2024aligning,
  title={Aligning Large Language Models with Representation Editing: A Control Perspective},
  author={Kong, Lingkai and Wang, Haorui and Mu, Wenhao and Du, Yuanqi and Zhuang, Yuchen and Zhou, Yifei and Song, Yue and Zhang, Rongzhi and Wang, Kai and Zhang, Chao},
  journal={arXiv preprint arXiv:2406.05954},
  year={2024}
}

@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and others},
  journal={arXiv preprint arXiv:2309.00267},
  year={2023}
}

@article{li2023rain,
  title={Rain: Your language models can align themselves without finetuning},
  author={Li, Yuhui and Wei, Fangyun and Zhao, Jinjing and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2309.07124},
  year={2023}
}

@inproceedings{lin2023unlocking,
  title={The unlocking spell on base llms: Rethinking alignment via in-context learning},
  author={Lin, Bill Yuchen and Ravichander, Abhilasha and Lu, Ximing and Dziri, Nouha and Sclar, Melanie and Chandu, Khyathi and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{liu2024decoding,
  title={Decoding-time Realignment of Language Models},
  author={Liu, Tianlin and Guo, Shangmin and Bianco, Leonardo and Calandriello, Daniele and Berthet, Quentin and Llinares, Felipe and Hoffmann, Jessica and Dixon, Lucas and Valko, Michal and Blondel, Mathieu},
  journal={arXiv preprint arXiv:2402.02992},
  year={2024}
}

@article{liu2024tuning,
  title={Tuning language models by proxy},
  author={Liu, Alisa and Han, Xiaochuang and Wang, Yizhong and Tsvetkov, Yulia and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2401.08565},
  year={2024}
}

@article{liu2025can,
  title={Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling},
  author={Liu, Runze and Gao, Junqi and Zhao, Jian and Zhang, Kaiyan and Li, Xiu and Qi, Biqing and Ouyang, Wanli and Zhou, Bowen},
  journal={arXiv preprint arXiv:2502.06703},
  year={2025}
}

@article{meng2024simpo,
  title={Simpo: Simple preference optimization with a reference-free reward},
  author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  journal={arXiv preprint arXiv:2405.14734},
  year={2024}
}

@article{mitchell2023emulator,
  title={An emulator for fine-tuning large language models using small language models},
  author={Mitchell, Eric and Rafailov, Rafael and Sharma, Archit and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2310.12962},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{wu2024empirical,
  title={An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  journal={arXiv preprint arXiv:2408.00724},
  year={2024}
}

@article{zelikman2024quiet,
  title={Quiet-star: Language models can teach themselves to think before speaking},
  author={Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
  journal={arXiv preprint arXiv:2403.09629},
  year={2024}
}

@inproceedings{zhang2023vibe,
    title={{VIBE}: Topic-Driven Temporal Adaptation for Twitter Classification},
    author={Yuji Zhang and Jing Li and Wenjie Li},
    booktitle={Conference on Empirical Methods in Natural Language Processing},
    year={2023},
}

@article{zheng2024weak,
  title={Weak-to-strong extrapolation expedites alignment},
  author={Zheng, Chujie and Wang, Ziqi and Ji, Heng and Huang, Minlie and Peng, Nanyun},
  journal={arXiv preprint arXiv:2404.16792},
  year={2024}
}

