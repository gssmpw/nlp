\section{Introduction}

Errors are an inevitable aspect of language acquisition, serving as critical indicators of learners' linguistic development and providing valuable insights for educators and intelligent language learning systems \citep{l10,l11,l12,liu2022we,li2022past,li2023towards,li2024llms}. In Error Analysis (EA), the systematic identification, categorization, and interpretation of learner errors play a pivotal role in improving personalized instruction, generating automated feedback, and enabling effective language assessment \citep{l13,l14,language_two,li2022learning,li2023effectiveness}. Central to this process is the use of grammatical error classification taxonomies~\citep{ma2022linguistic,excgec,enhancing}, which organize learner errors based on linguistic or cognitive principles. These taxonomies have been widely adopted in applications such as grammatical error correction (GEC)~\cite{li2024rethinkingroleslargelanguage,ye-etal-2023-mixedit,ye-etal-2023-system,li2025correct} and automated essay scoring (AES), significantly enhancing error detection \cite{huang-etal-2023-frustratingly,zhang2023contextual}, correction~\cite{ye2022focus}, and feedback generation \citep{l15,l16}.

A well-designed grammatical error classification taxonomy allows language learners to better understand the nature and causes of their errors, facilitating targeted improvements in their linguistic competence \cite{ye2025position}. However, existing taxonomies are often developed based on empirical assumptions or ad-hoc practices, without rigorous validation \citep{TEGA, errant}. This lack of systematic evaluation has led to \textit{issues} such as overlapping categories, insufficient coverage of error types, and limited applicability in real-world educational contexts.

To address these challenges, this paper revisits grammatical error classification taxonomies by systematically assessing their quality and utility. Specifically, we introduce a multi-metric evaluation framework that examines four key dimensions of a taxonomy: \ding{182} \textit{\Exclusivity} ensures that error categories are mutually exclusive, with clearly defined boundaries to minimize overlap and ambiguity;
\ding{183} \textit{\Coverage} evaluates the extent to which the taxonomy captures both common and rare error types, ensuring a comprehensive representation of learner errors. \ding{184} \textit{\Balance} measures the taxonomyâ€™s ability to balance attention between frequent and infrequent error types, avoiding overemphasis on a narrow subset of errors; \ding{185} \textit{\Usability} assesses the clarity and practical applicability of the taxonomy.

To validate our evaluation framework, we construct a high-quality grammatical error dataset annotated with multiple classification taxonomies. This dataset is created through a collaborative annotation approach that leverages large language models (LLMs) and human annotators, ensuring scalability and annotation reliability. Using this dataset, we systematically evaluate four widely-used error classification taxonomies: \textbf{POL73} \citep{linguistic_one}, \textbf{TUC74} \citep{gooficon}, \textbf{BRY17} \citep{errant}, and \textbf{FEI23} \citep{enhancing}. Through performance comparisons and annotator agreement experiments, we evaluate the rationality of these taxonomies. An ablation study on error type merging further reveals that classification taxonomies should not rely solely on empirical intuition but require systematic validation. These findings underscore the necessity of a rigorous evaluation for grammatical error taxonomy. In summary, our contributions are as follows:

\begin{itemize}[leftmargin=*]
\item[$\bullet$] We propose a novel multi-metric evaluation framework for grammatical error classification taxonomies, incorporating dimensions of exclusivity, coverage, balance, and usability.

\item[$\bullet$] We construct a high-quality grammatical error dataset annotated with multiple taxonomies, leveraging a collaborative annotation process involving LLMs and human experts.

\item[$\bullet$] We conduct a comprehensive evaluation of four widely used taxonomies, providing insights into their strengths, limitations, and practical implications for error analysis in language learning.
\end{itemize}

% By revisiting grammatical error classification taxonomies through a systematic lens, we aim to advance the precision and effectiveness of error analysis. Our work provides a foundation for developing more robust and actionable taxonomies, ultimately improving feedback systems for language learners and supporting broader applications in NLP and education.



% Deprecated: 20250214 by yejh
% Errors are an inherent part of language acquisition, reflecting learners' linguistic proficiency and providing valuable insights for educators and intelligent tutoring systems. Error Analysis (EA) plays a key role in identifying, categorizing, and interpreting these errors, supporting personalized instruction, automated feedback systems, and effective language assessment. To systematically analyze these errors, error classification taxonomies have been developed, categorizing learner errors based on linguistic and cognitive principles. These taxonomies have been extensively utilized in Grammatical Error Correction (GEC) and other natural language processing (NLP) applications, enhancing error detection, correction, and feedback generation.

% However, the rationality of an error classification taxonomy is crucial, as it directly influences the consistency, interpretability, and effectiveness of error analysis. A well-structured taxonomy ensures that learners receive clear, actionable feedback, helping them understand the root causes of their errors and make meaningful progress in their language learning. Conversely, an ill-defined taxonomy can introduce inconsistencies in categorizing errors, leading to ambiguous or misleading feedback. This can undermine learners' understanding of linguistic rules and hinder their long-term learning outcomes.

% Despite their widespread use, existing taxonomies are constructed based on empirical assumptions rather than rigorous scientific validation, resulting in potential ambiguities and gaps in practical applications. In this paper, we aim to address this gap by systematically evaluating the rationality of existing error classification taxonomies. We choose four error classification taxonomies to evaluate as below: \textbf{POL73} \citep{linguistic_one}, \textbf{TUC74} \citep{gooficon}, \textbf{BRY17} \citep{errant}, \textbf{FEI23} \citep{enhancing}. Our evaluation focuses on \textbf{exclusivity}, ensuring that boundaries between error categories are clearly defined and do not overlap excessively; \textbf{balance effect}, aiming for balanced attention to both common and rare error types; \textbf{coverage}, which measures how comprehensively each taxonomy captures typical and atypical learner errors; and \textbf{usability}, reflecting the clarity and practicality of applying the taxonomy in real educational settings. To validate these metrics, we build a multi-label dataset annotated with multiple error classification taxonomies using a large language model (LLM) \& human collaborative annotation approach. Our experimental results desmonstrate ???. In summary, our key contributions are as follows:
% \begin{itemize}
%     \item We propose a multi-metric evaluation taxonomy incorporating exclusivity, balance effect, coverage, and usability.
%     \item We construct a high-quality grammatical error dataset annotated with multiple classification taxonomies.
%     \item We evaluate taxonomies across different models, measuring annotation consistency, model uncertainty, and the impact of category merging.
% \end{itemize}
