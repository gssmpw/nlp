\section{Experiments}
\subsection{Experimental Setup}

\paragraph{Dataset \& Models}
We use the Cambridge English Write \& Improve (W\&I) and LOCNESS corpus \citep{w&i}, re-annotated with ERRANT \citep{errant}. To reduce the uncertainty introduced by multiple grammatical errors in a sentence, we preprocess the dataset by decomposing sentences with multiple errors into single-error instances \citep{enhancing}, as described in Appendix \ref{appendix extract single}. We then select 487 instances for further analysis. And we adopt an LLM \& human collaborative annotation approach (Appendix \ref{appendix annotation}). A detailed description of the dataset is provided in Appendix~\ref{appendix dataset_statistics}, and the details of the selected models are included in Appendix \ref{appendix models}.

\paragraph{Error Classification Taxonomies}
We consider four influential error classification taxonomies in error analysis: \textbf{POL73} \citep{linguistic_one}, \textbf{TUC74} \citep{gooficon}, \textbf{BRY17} \cite{errant}, and \textbf{FEI23} \citep{enhancing}. POL73 and TUC74 are linguistically driven but differ in hierarchical structure and classification logic. BRY17 introduces the rule-based ERRANT toolkit, categorizing errors by part-of-speech and token edit operations. FEI23, grounded in second language acquisition, adopts a cognitive perspective, classifying errors into single-word, inter-word, and discourse-level categories. Appendix \ref{appendix taxonomy} provides a detailed comparison of these taxonomies.


\input{tables/table_annotator}

\subsection{Main Results}
% To assess the rationality of different error classification taxonomies, we compare their performance across multiple models using various evaluation metrics. As shown in Table \ref{tab1}, FEI23 achieves the highest exclusivity, indicating well-distinguished error types.

We systematically assess the rationality of various error classification taxonomies in Table \ref{tab1}.

\textbf{Exclusivity: Ambiguous type boundaries undermine mutual exclusivity.} Taxonomies with overlapping or poorly defined error categories lead to classification inconsistencies, making it difficult to assign a unique label to each error instance.

\textbf{Coverage: A greater number of categories does not guarantee comprehensive coverage.} Effective taxonomies must encompass both frequent and rare errors, as merely expanding categories does not guarantee comprehensive representation.

\textbf{Balance: Over-specification and Overgeneralization may disrupt distributional balance.} Over-specification introduces unnecessary fine-grained distinctions while overgeneralization clusters distinct errors into broad categories. Both lead to imbalanced error distributions.

\textbf{Usability: Linguistic-based taxonomies reduce practical utility.} Taxonomies based on intricate semantic or syntactic distinctions increase the complexity of computational modeling, limiting their usability in real-world applications.

Further analyses are provided in Appendix \ref{appendix main}.

\subsection{Annotator Agreement Analysis}
\label{section annotator agreement}
We measure inter-annotator agreement as a part of Usability, using Cohenâ€™s Kappa Index \citep{cohen}, a standard metric for measuring annotation consistency. The results of three annotators are summarized in Table \ref{table annotator}.

BRY17 and FEI23 exhibit higher agreement scores compared to POL73 and TUC74, suggesting that the former taxonomies provide clearer and more well-defined categories. This aligns with the trend observed in Table \ref{tab1}, further substantiating the validity of our evaluation framework. Moreover, the higher agreement in BRY17 and FEI23 reinforces their practical applicability, while the lower scores in POL73 and TUC74 suggest potential ambiguity in category definitions.

To assess the impact of classification granularity, we conducted an ablation study by merging specific error categories and analyzing their effect on key metrics. The results underscore the need for rigorous validation of error classification taxonomies, rather than relying on intuition or convention, highlighting the importance of our proposed evaluation metrics. The detailed results are in Appendix \ref{appendix ablation study}.
