\section{Methodology}
% \subsection{Problem Statement}
Given a dataset $D$ of English learner texts, our goal is to systematically and quantitatively evaluate the rationality of $n$ error classification taxonomies $F={\{F_1, F_2,...,F_n\}}$. Each taxonomy $F_i$ consists of $m$ predefined error types, denoted as $F_i=\{ET_1, ET_2,...,ET_m\}$, where $ET_j$ represents the $j$-th error type in the taxonomy $F_i$. All notations are detailed in the Appendix~\ref{appendix notation table}.

% \subsection{Evaluation Metrics}
We evaluate the rationality of each taxonomy along four dimensions: \textbf{Exclusivity}, \textbf{Coverage}, \textbf{Balance}, and \textbf{Usability}. Exclusivity and Usability are based on the generated results by the LLM, while Balance and Coverage are computed obtained directly from the manually annotated dataset through statistical analysis.

\subsection{Exclusivity}
The error types in the taxonomy should be mutually exclusive, meaning that each error instance belongs to a single distinct category. Overlapping error types introduce instability and inconsistencies in error analysis, reducing the reliability of classification results. If a model frequently assigns high confidence to multiple categories for the same error, it suggests that the taxonomy lacks clear boundaries between certain error types. To quantify this issue, we assess exclusivity by analyzing the confidence scores of an LLMâ€™s predictions. Confidence estimation plays a crucial role in this evaluation, we incorporate three established methods \citep{confidence} for improved reliability (details are in Appendix \ref{appendix confidence}). Specifically, a sample is considered classified under an error type if its confidence score exceeds the predefined threshold $\tau$. We define the $\operatorname{Overlap}$ to quantify the degree of category overlap for a sample $x$ as follows:
\begin{equation}\begin{small}\begin{aligned}
\operatorname{Overlap}(x)=|\{\hat{Y}_i^{(x)} \mid C_i^{(x)} > \tau\}|,
\end{aligned}\end{small}\end{equation}
\noindent where $\hat{Y}_i^{(x)}$ denotes the $i$-th predicted error type for a given sample $x$, and $C_i^{(x)}$ represents its confidence score. A higher $\operatorname{Overlap}(x)$ indicates that multiple error types are assigned to the same sample, suggesting a violation of exclusivity.

The \textit{Exclusivity Score} is computed as the average instance-level exclusivity over the dataset $D$:
\begin{equation}\begin{small}\begin{aligned}
& \operatorname{Exclusivity}(F)= \\
& ~~~~~~~~\frac{1}{|D|}\sum_{x\in D}\begin{cases}
    1- \frac{\operatorname{Overlap}(x) - 1}{k - 1}, & \text{ if } \operatorname{Overlap}(x) > 0, \\
    0, & \text{ if } \operatorname{Overlap}(x) > 0,
\end{cases}
\end{aligned}\end{small}\end{equation}
\noindent where $k$ represents the selection parameter in the Top-K Prompting Strategy, detailed in Appendix \ref{appendix confidence}. $\operatorname{Exclusivity}$ indicates whether the classification taxonomy maintains clear distinctions between error types. A lower score suggests significant overlap between categories, indicating poorly defined boundaries, whereas a higher score implies a more distinct and reliable classification system.

\subsection{Coverage}
Coverage measures the extent to which a taxonomy accounts for errors in the dataset $D$. Let $|U|$ be the number of errors labeled with at least one defined category (as opposed to ``Other''), and $|D|$ be the total number of error instances. We define \textit{Coverage score} as follows:
\begin{equation}\begin{small}\begin{aligned}
\operatorname{Coverage}(F)=\frac{|U|}{|D|}.
\end{aligned}\end{small}\end{equation}

A higher $\operatorname{Coverage}$ indicates that the taxonomy captures a greater proportion of actual errors, demonstrating superior completeness in covering the range of error types.

\subsection{Balance}
Error classification taxonomy should maintain a balanced distribution of error types, avoiding excessive concentration on a few dominant categories while ensuring sufficient representation of less frequent errors. An imbalanced taxonomy may exhibit a long-tail effect \citep{lt1,lt2}, where frequent error types overshadow less common yet pedagogically or computationally significant ones, leading to biased analysis. To assess the balance of a classification taxonomy, we introduce the \textit{Balance Score}. This metric quantifies the evenness of error type distribution using entropy-based uniformity. Given an error type $ET_i$ with proportion computed as:
\begin{equation}\begin{small}\begin{aligned}
    P_i=\frac{|ET_i|}{\sum_{j=1}^m|ET_j|},
\end{aligned}\end{small}\end{equation}
\noindent its entropy contribution is $-P_i \log P_i$ if $|ET_i|>0$, otherwise 0. The final score is normalized by $\log(m)$, where $m$ is the total number of error types:
\begin{equation}\begin{small}\begin{aligned}
\operatorname{Balance}(F)=\frac{\sum_{i=1}^m -P_i \log P_i}{\log (m)}.
\end{aligned}\end{small}\end{equation}

A greater $\operatorname{Balance}$ value signifies a more uniform distribution, attenuating the long-tail effect and guaranteeing its applicability within educational and computational domains.

\input{tables/table_main}

\subsection{Usability}
We argue that a taxonomy with great usability should be understandable for humans and models. So we quantify usability from \textit{model effectiveness} and \textit{human annotation agreement}. Model effectiveness means that LLMs can produce reliable predictions based on a classification taxonomy, thereby enhancing the validity of subsequent error analyses. To quantify this, we evaluate classification performance using \textit{Macro F1} and \textit{Micro F1} scores:
\begin{equation}\begin{small}\begin{aligned} 
\operatorname{Macro\_F1} &= \frac{1}{m}\sum_{i=1}^m \frac{2 \cdot P(ET_i)\cdot R(ET_i)}{P(ET_i)+R(ET_i)},
\end{aligned}\end{small}\end{equation}
\begin{equation}\begin{small}\begin{aligned}
\operatorname{Micro\_F1} &= \frac{2 \cdot P(D)\cdot R(D)}{P(D)+R(D)},
\end{aligned}\end{small}\end{equation}
\noindent where $P$ and $R$ denote precision and recall, respectively. A high Macro F1 indicates the taxonomy supports stable model performance across both frequent and infrequent error types, mitigating classification bias. A high Micro F1 suggests its robustness in large-scale error detection. The combination of high Macro and Micro F1 scores demonstrates that the taxonomy maintains both category-level consistency and large-scale applicability, ensuring the reliability of error analysis.

For human annotators, the taxonomy should be intuitive and easy to apply, minimizing ambiguity in error categorization. Therefore, we measure inter-annotator consistency in Section \ref{section annotator agreement}.
