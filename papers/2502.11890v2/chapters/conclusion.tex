\section{Conclusion}
We revisit error classification taxonomies in error analysis by proposing a systematic evaluation framework based on exclusivity, coverage, balance, and usability, evaluated on our own annotated dataset. Our experiments validate the effectiveness of these metrics and underscore the need for a systematic assessment of classification taxonomies. Results demonstrate that different taxonomies exhibit trade-offs, with some excelling in exclusivity and coverage while others offer better usability. These findings highlight the importance of well-structured taxonomies for reliable error analysis and annotation consistency.