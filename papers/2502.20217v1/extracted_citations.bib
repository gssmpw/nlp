@INPROCEEDINGS{Bircher_2016,
  author={Bircher, Andreas and Kamel, Mina and Alexis, Kostas and Oleynikova, Helen and Siegwart, Roland},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Receding Horizon "Next-Best-View" Planner for 3D Exploration}, 
  year={2016},
  volume={},
  number={},
  pages={1462-1468},
  keywords={Vehicles;Robot sensing systems;Space exploration;Planning;Navigation;Three-dimensional displays},
  doi={10.1109/ICRA.2016.7487281}}

@INPROCEEDINGS{Henriques_2018,
  author={Henriques, Joao F. and Vedaldi, Andrea},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={MapNet: An Allocentric Spatial Memory for Mapping Environments}, 
  year={2018},
  volume={},
  number={},
  pages={8476-8484},
  keywords={Simultaneous localization and mapping;Cameras;Navigation;Streaming media;Three-dimensional displays;Task analysis;Geometry},
  doi={10.1109/CVPR.2018.00884}}

@INPROCEEDINGS{Mousavian_2019,
  author={Mousavian, Arsalan and Toshev, Alexander and Fišer, Marek and Košecká, Jana and Wahid, Ayzaan and Davidson, James},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)}, 
  title={Visual Representations for Semantic Target Driven Navigation}, 
  year={2019},
  volume={},
  number={},
  pages={8846-8852},
  keywords={Navigation;Visualization;Semantics;Training;Adaptation models;Robots;Task analysis},
  doi={10.1109/ICRA.2019.8793493}}

@Inbook{Nazif2011,
    author="Nazif, Ali Nasri and Davoodi, Alireza and Pasquier, Philippe",
    title="Multi-Agent Area Coverage Using a Single Query Roadmap: A Swarm Intelligence Approach",
    bookTitle="Advances in Practical Multi-Agent Systems",
    year="2011",
    pages="95--112",
    abstract="This paper proposes a mechanism for visually covering an area by means of a group of homogeneous reactive agents through a single-query roadmap called Weighted Multi-Agent RRT, WMA-RRT. While the agents do not know about the environment, the roadmap is locally available to them. In accordance with the swarm intelligence principles, the agents are simple autonomous entities, capable of interacting with the environment by obeying some explicit rules and performing the corresponding actions. The interaction between the agents is carried out through an indirect communication mechanism and leads to the emergence of complex behaviors such as multi-agent cooperation and coordination, path planning and environment exploration. This mechanism is reliable in the face of agent failures and can be effectively and easily employed in cluttered environments containing narrow passages. We have implemented and evaluated the algorithm in different domains and the experimental results confirm the performance and robustness of the system.",
    isbn="978-3-642-16098-1",
    doi="10.1007/978-3-642-16098-1_7",
}

@inproceedings{Sunehag_2018, author = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore}, title = {Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward}, year = {2018}, publisher = {International Foundation for Autonomous Agents and Multiagent Systems}, address = {Richland, SC}, abstract = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent'' problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions.}, booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems}, pages = {2085–2087}, numpages = {3}, keywords = {collaborative, dqn, multi-agent, neural networks, q-learning, reinforcement learning, value-decomposition}, location = {Stockholm, Sweden}, series = {AAMAS '18} }

@inproceedings{Yamauchi_nearest_1998,
author = {Yamauchi, Brian},
title = {Frontier-based exploration using multiple robots},
year = {1998},
isbn = {0897919831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/280765.280773},
doi = {10.1145/280765.280773},
booktitle = {Proceedings of the Second International Conference on Autonomous Agents},
pages = {47–53},
numpages = {7},
keywords = {multi-robot teams, multi-agent coordination, mobile robots, map-building, exploration},
location = {Minneapolis, Minnesota, USA},
series = {AGENTS '98}
}

@InProceedings{chao_maans_2022,
author="Yu, Chao
and Yang, Xinyi
and Gao, Jiaxuan
and Yang, Huazhong
and Wang, Yu
and Wu, Yi",
title="Learning Efficient Multi-agent Cooperative Visual Exploration",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="497--515",
abstract="We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we propose a novel RL-based multi-agent planning module, Multi-agent Spatial Planner (MSP). MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent enhancements to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.",
isbn="978-3-031-19842-7"
}

@inproceedings{chaplot2020learning,
  title={Learning To Explore Using Active Neural SLAM},
  author={Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta,
          Saurabh and Gupta, Abhinav and Salakhutdinov, Ruslan},
  booktitle={International Conference on 
             Learning Representations (ICLR)},
  year={2020}}

@article{dang2020graph,
  title={Graph-based subterranean exploration path planning using aerial and legged robots},
  author={Dang, Tung and Tranzatto, Marco and Khattak, Shehryar and Mascarich, Frank and Alexis, Kostas and Hutter, Marco},
  journal={Journal of Field Robotics},
  volume={37},
  number={8},
  pages={1363--1388},
  year={2020},
  publisher={Wiley Online Library}
}

@inproceedings{epciclr2020,
  author = {Qian Long and Zihan Zhou and Abhinav Gupta and Fei Fang and Yi Wu and Xiaolong Wang},
  title = {Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning},
  booktitle = {International Conference on Learning Representations},
  year = {2020}
}

@article{gronauer_multi-agent_2022,
	title = {Multi-agent deep reinforcement learning: a survey},
	volume = {55},
	issn = {0269-2821, 1573-7462},
	url = {https://link.springer.com/10.1007/s10462-021-09996-w},
	doi = {10.1007/s10462-021-09996-w},
	shorttitle = {Multi-agent deep reinforcement learning},
	abstract = {Abstract
            The advances in reinforcement learning have recorded sublime success in various domains. Although the multi-agent domain has been overshadowed by its single-agent counterpart during this progress, multi-agent reinforcement learning gains rapid traction, and the latest accomplishments address problems with real-world complexity. This article provides an overview of the current developments in the field of multi-agent deep reinforcement learning. We focus primarily on literature from recent years that combines deep reinforcement learning methods with a multi-agent scenario. To survey the works that constitute the contemporary landscape, the main contents are divided into three parts. First, we analyze the structure of training schemes that are applied to train multiple agents. Second, we consider the emergent patterns of agent behavior in cooperative, competitive and mixed scenarios. Third, we systematically enumerate challenges that exclusively arise in the multi-agent domain and review methods that are leveraged to cope with these challenges. To conclude this survey, we discuss advances, identify trends, and outline possible directions for future work in this research area.},
	pages = {895--943},
	number = {2},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Gronauer, Sven and Diepold, Klaus},
	urldate = {2024-09-12},
	date = {2022-02},
    year = {2022},
	langid = {english},
	file = {Full Text:/Users/jimmychiun/Zotero/storage/VU3A5MFP/Gronauer and Diepold - 2022 - Multi-agent deep reinforcement learning a survey.pdf:application/pdf},
}

@ARTICLE{hu_voronoi_2020,
  author={Hu, Junyan and Niu, Hanlin and Carrasco, Joaquin and Lennox, Barry and Arvin, Farshad},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Voronoi-Based Multi-Robot Autonomous Exploration in Unknown Environments via Deep Reinforcement Learning}, 
  year={2020},
  volume={69},
  number={12},
  pages={14413-14423},
  keywords={Collision avoidance;Robot kinematics;Reinforcement learning;Robot sensing systems;Mobile robots;Navigation;Autonomous exploration;path planning;deep reinforcement learning;multi-vehicle systems;collision avoidance},
  doi={10.1109/TVT.2020.3034800}}

@misc{iqbal2021,
      title={Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning}, 
      author={Shariq Iqbal and Fei Sha},
      year={2021},
      eprint={1905.12127},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.12127}, 
}

@article{julia_comparison_2012,
	title = {A comparison of path planning strategies for autonomous exploration and mapping of unknown environments},
	volume = {33},
	rights = {http://www.springer.com/tdm},
	issn = {0929-5593, 1573-7527},
	url = {http://link.springer.com/10.1007/s10514-012-9298-8},
	doi = {10.1007/s10514-012-9298-8},
	pages = {427--444},
	number = {4},
	journaltitle = {Autonomous Robots},
	shortjournal = {Auton Robot},
	author = {Juliá, Miguel and Gil, Arturo and Reinoso, Oscar},
	urldate = {2024-09-12},
	date = {2012-11},
    year = {2012},
	langid = {english},
}

@inproceedings{kulich2011distance,
  title={On distance utility in the exploration task},
  author={Kulich, Miroslav and Faigl, Jan and P{\v{r}}eu{\v{c}}il, Libor},
  booktitle={2011 IEEE International Conference on Robotics and Automation},
  pages={4455--4460},
  year={2011},
  organization={IEEE}
}

@article{li2019deep,
  title={Deep reinforcement learning-based automatic exploration for navigation in unknown environment},
  author={Li, Haoran and Zhang, Qichao and Zhao, Dongbin},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={6},
  pages={2064--2076},
  year={2019},
  publisher={IEEE}
}

@article{niroui2019deep,
  title={Deep reinforcement learning robot for search and rescue applications: Exploration in unknown cluttered environments},
  author={Niroui, Farzad and Zhang, Kaicheng and Kashino, Zendai and Nejat, Goldie},
  journal={IEEE Robotics and Automation Letters},
  volume={4},
  number={2},
  pages={610--617},
  year={2019},
  publisher={IEEE}
}

@InProceedings{pmlr-v139-liu21j,
  title = 	 {Cooperative Exploration for Multi-Agent Deep Reinforcement Learning},
  author =       {Liu, Iou-Jen and Jain, Unnat and Yeh, Raymond A and Schwing, Alexander},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6826--6836},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/liu21j/liu21j.pdf},
  url = 	 {https://proceedings.mlr.press/v139/liu21j.html},
  abstract = 	 {Exploration is critical for good results in deep reinforcement learning and has attracted much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. Very recently, exploration methods that consider cooperation among multiple agents have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and hardly coordinate exploration efforts toward those states. To address this shortcoming, in this paper, we propose cooperative multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected from multiple projected state spaces by a normalized entropy-based technique. Then, agents are trained to reach the goal in a coordinated manner. We demonstrate that CMAE consistently outperforms baselines on various tasks, including a sparse-reward version of multiple-particle environment (MPE) and the Starcraft multi-agent challenge (SMAC).}
}

@InProceedings{pmlr-v97-iqbal19a,
  title = 	 {Actor-Attention-Critic for Multi-Agent Reinforcement Learning},
  author =       {Iqbal, Shariq and Sha, Fei},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2961--2970},
  year = 	 {2019},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/iqbal19a/iqbal19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/iqbal19a.html},
  abstract = 	 {Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.}
}

@misc{wakilpoor2020,
      title={Heterogeneous Multi-Agent Reinforcement Learning for Unknown Environment Mapping}, 
      author={Ceyer Wakilpoor and Patrick J. Martin and Carrie Rebhuhn and Amanda Vu},
      year={2020},
      eprint={2010.02663},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2010.02663}, 
}

@article{wang_few_2020,
	title = {From Few to More: Large-Scale Dynamic Multiagent Curriculum Learning},
	volume = {34},
	rights = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6221},
	doi = {10.1609/aaai.v34i05.6221},
	shorttitle = {From Few to More},
	abstract = {A lot of efforts have been devoted to investigating how agents can learn effectively and achieve coordination in multiagent systems. However, it is still challenging in large-scale multiagent settings due to the complex dynamics between the environment and agents and the explosion of state-action space. In this paper, we design a novel Dynamic Multiagent Curriculum Learning ({DyMA}-{CL}) to solve large-scale problems by starting from learning on a multiagent scenario with a small size and progressively increasing the number of agents. We propose three transfer mechanisms across curricula to accelerate the learning process. Moreover, due to the fact that the state dimension varies across curricula, and existing network structures cannot be applied in such a transfer setting since their network input sizes are fixed. Therefore, we design a novel network structure called Dynamic Agent-number Network ({DyAN}) to handle the dynamic size of the network input. Experimental results show that {DyMA}-{CL} using {DyAN} greatly improves the performance of large-scale multiagent learning compared with state-of-the-art deep reinforcement learning approaches. We also investigate the influence of three transfer mechanisms across curricula through extensive simulations.},
	pages = {7293--7300},
	number = {5},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Wang, Weixun and Yang, Tianpei and Liu, Yong and Hao, Jianye and Hao, Xiaotian and Hu, Yujing and Chen, Yingfeng and Fan, Changjie and Gao, Yang},
	urldate = {2024-09-12},
	date = {2020-04-03},
    year = {2020},
}

@article{xu2021autonomous,
  title={Autonomous UAV exploration of dynamic environments via incremental sampling and probabilistic roadmap},
  author={Xu, Zhefan and Deng, Di and Shimada, Kenji},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={2},
  pages={2729--2736},
  year={2021},
  publisher={IEEE}
}

@INPROCEEDINGS{yamauchi_1997,
  author={Yamauchi, B.},
  booktitle={Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation'}, 
  title={A frontier-based approach for autonomous exploration}, 
  year={1997},
  volume={},
  number={},
  pages={146-151},
  keywords={Mobile robots;Orbital robotics;Sonar navigation;Artificial intelligence;Laboratories;Testing;Humans;Indoor environments;Space exploration},
  doi={10.1109/CIRA.1997.613851}}

@INPROCEEDINGS{yu_apf_2021,
  author={Yu, Jincheng and Tong, Jianming and Xu, Yuanfan and Xu, Zhilin and Dong, Haolin and Yang, Tianxiang and Wang, Yu},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={SMMR-Explore: SubMap-based Multi-Robot Exploration System with Multi-robot Multi-target Potential Field Exploration Method}, 
  year={2021},
  volume={},
  number={},
  pages={8779-8785},
  keywords={Costs;Simultaneous localization and mapping;Conferences;Distributed databases;Collaboration;Robot sensing systems;Boosting},
  doi={10.1109/ICRA48506.2021.9561328}}

@inproceedings{zhu2018deep,
  title={Deep reinforcement learning supervised autonomous exploration in office environments},
  author={Zhu, Delong and Li, Tingguang and Ho, Danny and Wang, Chaoqun and Meng, Max Q-H},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={7548--7555},
  year={2018},
  organization={IEEE}
}

