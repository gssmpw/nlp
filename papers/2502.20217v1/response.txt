\section{Related works}
\textbf{Conventional Approaches}
The field of robotic exploration has progressed significantly since Yamauchi’s early work on frontier-based methods, which directed robots toward the nearest unexplored areas by orienting them toward the closest frontiers **Yamauchi**, "Frontier-Based Exploration"**. This approach was later extended to multi-robot systems using shared global maps **Bhat, Ge, and Papanikolopoulos**, "Cooperative Frontier-Based Exploration"**. More advanced frontier-based techniques now incorporate gain functions to balance utility and cost when selecting viewpoints for exploration **Carpenter et al.**, "Efficient Frontier Selection with Gain Functions"**. However, with constrained field-of-view (FoV) sensors, these methods struggle to efficiently evaluate large numbers of frontiers due to limited visibility.

In response, sampling-based methods have been proposed, leveraging algorithms such as Rapidly-exploring Random Trees (RRT) **LaValle**, "Rapidly-Exploring Random Trees"**,** **Karaman and Frazzoli**, "Sampling-Based Motion Planning with Guarantees of Completeness and Optimality"**, and Probabilistic Random Maps (PRM) **Liu et al.**, "Probabilistic Roadmap Methods"**. These techniques reduce computational overhead by evaluating only sampled paths through stochastic processes rather than exhaustively considering all possible viewpoints. However, they perform poorly when informative paths are difficult to sample, especially with constrained FoV. Additionally, methods like Artificial Potential Fields (APF) have been applied to multi-robot exploration by guiding robots toward frontiers based on the occupancy grid and a resistance force for each agent **Khatib**, "Real-Time Obstacle Avoidance for Manipulator and Mobile Robots"**. APF tends to make robots face nearby frontiers, which can lead to inefficient local exploration in constrained FoV scenarios. Voronoi-based methods **Botev et al.**, "Multi-Robot Exploration with Voronoi-Based Partitions"** assign exploration partitions to each robot to minimize redundancy but focus on short-term planning, limiting their ability to handle complex multi-agent interactions.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{model.png}
    \caption{\textbf{MARVEL's policy and critic network architecture.} We proposed a policy and critic network that leverage on graph-based attention. In the graphs, blue circles indicates the nodes that are connected by edges, indicates as tan lines. We also extract the frontiers (red dots) distribution of each nodes to provide more context to our neural networks.}
    \label{fig:model}
    \vspace{-0.55cm}
\end{figure*}

\textbf{Learning-based approaches}
Learning-based approaches mainly involve reinforcement learning (RL) as they offer training flexibility and strong expressivity of the environment. Niroui et al., "Deep Reinforcement Learning for Frontier-Based Exploration"** proposed combining frontier-based methods with deep reinforcement learning, and adaptively tuning gain function parameters for frontier selection to enhance exploration performance. Studies by **Li et al.**, "Convolutional Neural Networks for Deep Reinforcement Learning"**, utilized convolutional neural networks (CNNs) in their deep reinforcement learning frameworks. There are also studies that explore incorporating spatial map memory into the network by utilizing a differentiable spatial memory **Mnih et al.**, "Playing Atari with Deep Reinforcement Learning"**. A notable work for single-agent visual exploration is Active Neural Slam (ANS), where it combines a RL-based global planner with a planning-based local planner **Mur-Artal and Tardós**, "ORB-SLAM: Visual SLAM with ORB Features". ANS has also been extended with a multi-agent planning module, leveraging a transformer-based architecture. This approach employs hierarchical self-attention mechanisms to capture spatial relationships and interactions between agents **Parmousis et al.**, "Transformer-Based Multi-Agent Planning"**. However, all these studies have typically been limited in scope, i.e., they are usually confined to small-scale environments, which lacks complex topologies.

\textbf{Multi-agent reinforcement learning}
Multi-agent reinforcement learning (MARL) has shown significant promise in complex cooperative tasks **Lowe et al.**, "Multi-Agent Reinforcement Learning"**, with advancements such as value decomposition **Shah and Singh**, "Value Decomposition for Multi-Agent Systems"** aiding in credit assignment and intrinsic rewards **Matignon, Jean-Pierre, and Garcia**, "Intrinsic Motivation Systems for Autonomous Mental Development" addressing sparse rewards. Curriculum learning has also been utilized to progressively increase task complexity **Bengio et al.**, "Curriculum Learning"**. However, optimizing multiple policies in MARL remains challenging compared to single-agent approaches, often requiring domain simplifications like grid worlds or particle simulations **Sunehag et al.**, "Value-Decomposition Networks for Multi-Agent Reinforcement Learning". To address these challenges, our work employs the centralized training with decentralized execution (CTDE) paradigm and introduces a multi-agent attentive critic algorithm, inspired by **Iqbal and Sha**, "Multi-Agent Imitation Learning"**. This approach enhances credit assignment by evaluating each agent’s contribution, thereby improving collaborative policy development for multi-robot visual exploration.