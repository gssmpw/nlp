\section{Related works}
\textbf{Conventional Approaches}
The field of robotic exploration has progressed significantly since Yamauchi’s early work on frontier-based methods, which directed robots toward the nearest unexplored areas by orienting them toward the closest frontiers \cite{yamauchi_1997}. This approach was later extended to multi-robot systems using shared global maps \cite{Yamauchi_nearest_1998}. More advanced frontier-based techniques now incorporate gain functions to balance utility and cost when selecting viewpoints for exploration \cite{julia_comparison_2012, kulich2011distance}. However, with constrained field-of-view (FoV) sensors, these methods struggle to efficiently evaluate large numbers of frontiers due to limited visibility.

In response, sampling-based methods have been proposed, leveraging algorithms such as Rapidly-exploring Random Trees (RRT) \cite{Bircher_2016, Nazif2011}, Rapidly-exploring Random Graphs \cite{dang2020graph}, and Probabilistic Random Maps (PRM) \cite{xu2021autonomous}. These techniques reduce computational overhead by evaluating only sampled paths through stochastic processes rather than exhaustively considering all possible viewpoints. However, they perform poorly when informative paths are difficult to sample, especially with constrained FoV. Additionally, methods like Artificial Potential Fields (APF) have been applied to multi-robot exploration by guiding robots toward frontiers based on the occupancy grid and a resistance force for each agent \cite{yu_apf_2021}. APF tends to make robots face nearby frontiers, which can lead to inefficient local exploration in constrained FoV scenarios. Voronoi-based methods \cite{hu_voronoi_2020} assign exploration partitions to each robot to minimize redundancy but focus on short-term planning, limiting their ability to handle complex multi-agent interactions.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{model.png}
    \caption{\textbf{MARVEL's policy and critic network architecture.} We proposed a policy and critic network that leverage on graph-based attention. In the graphs, blue circles indicates the nodes that are connected by edges, indicates as tan lines. We also extract the frontiers (red dots) distribution of each nodes to provide more context to our neural networks.}
    \label{fig:model}
    \vspace{-0.55cm}
\end{figure*}

\textbf{Learning-based approaches}
Learning-based approaches mainly involve reinforcement learning (RL) as they offer training flexibility and strong expressivity of the environment. Niroui et  al.\cite{niroui2019deep} proposed combining frontier-based methods with deep reinforcement learning, and adaptively tuning gain function parameters for frontier selection to enhance exploration performance. Studies by ~\cite{zhu2018deep,li2019deep}  utilized convolutional neural networks (CNNs) in their deep reinforcement learning frameworks. There are also studies that explore incorporating spatial map memory into the network by utilizing a differentiable spatial memory \cite{Mousavian_2019, Henriques_2018}. A notable work for single-agent visual exploration is Active Neural Slam (ANS), where it combines a RL-based global planner with a planning-based local planner \cite{chaplot2020learning}. ANS has also been extended with a multi-agent planning module, leveraging a transformer-based architecture. This approach employs hierarchical self-attention mechanisms to capture spatial relationships and interactions between agents \cite{chao_maans_2022}. However, all these studies have typically been limited in scope, i.e., they are usually confined to small-scale environments, which lacks complex topologies. 

\textbf{Multi-agent reinforcement learning}
Multi-agent reinforcement learning (MARL) has shown significant promise in complex cooperative tasks \cite{gronauer_multi-agent_2022}, with advancements such as value decomposition \cite{Sunehag_2018} aiding in credit assignment and intrinsic rewards \cite{pmlr-v139-liu21j, iqbal2021} addressing sparse rewards. Curriculum learning has also been utilized to progressively increase task complexity \cite{epciclr2020, wang_few_2020}. However, optimizing multiple policies in MARL remains challenging compared to single-agent approaches, often requiring domain simplifications like grid worlds or particle simulations \cite{wakilpoor2020}. To address these challenges, our work employs the centralized training with decentralized execution (CTDE) paradigm and introduces a multi-agent attentive critic algorithm, inspired by \cite{pmlr-v97-iqbal19a}. This approach enhances credit assignment by evaluating each agent’s contribution, thereby improving collaborative policy development for multi-robot visual exploration.