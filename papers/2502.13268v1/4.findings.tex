\section{Findings}
\label{sec:findings}
We find that the confusions concerning assumptions in ML largely revolve around what assumptions are (section \ref{sec:ontology}) and what is being done about them (section \ref{sec:procedure}). While many confusions are due to vague conceptualizations of assumptions, institutional fragmentation, and a general lack of clarity on response, others stem from holding unique and inconsistent views and procedures that deal with assumptions. Further, practitioners differ in their characterization of assumptions based on the role they take: whether they are the ones assuming (\textit{assumer}) or analyzing (\textit{analyst}) something. \rev{Table 1 summarizes our findings.}

\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l||l|l|l|}
\hline
\textbf{Axis of Confusion} & \textbf{Theme} & \textbf{Key Takeaways} & \textbf{Premise-Target Lens} \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}} Conceptualization of \\ what assumptions are\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Independent\\ construction\end{tabular} & \begin{tabular}[c]{@{}l@{}}- viewed as being outside of the ML workflow \\ \\ - solidified as axioms or hailed as requirements \\   or relegated as limitations\end{tabular} & \begin{tabular}[c]{@{}l@{}}- target of the assumption \\   often remains unstated\\ \\ - premise is seldom evaluated\end{tabular} \\ \cline{2-4}  
 & \begin{tabular}[c]{@{}l@{}}Relative\\ construction\end{tabular} & \begin{tabular}[c]{@{}l@{}} - defined in relation to data quality, model \\   specifications, or business objectives\\ \\ - rationalized in relation to ML workflow and \\   prevents deeper and more inclusive assessment\end{tabular} & \begin{tabular}[c]{@{}l@{}}- premise, target, and argument\\   are explicit and clear\\ \\ - identification and evaluation\\   of premises are easier\end{tabular} \\ \hline \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Uncertainty about \\ what is being done \\ with assumptions\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Integration with\\ existing workflow\end{tabular} & \begin{tabular}[c]{@{}l@{}}  \textbf{Reactive handling:}\\ - reactive and iterative approach to ML extends\\   to assumptions identification and handling\\ \\ - assumptions constructed relative to ML \\   workflows are often reactively handled\\ \\ \textbf{Unreflective quantification:}\\ - the incentive to quantify assumptions \\   evaluation obstructs reflective practice \\ \\ \textbf{Circle of ambiguity:}\\ - no mechanisms to capture evidence of \\   assumptions, creating more uncertainties\\ \\ - knowledge and communication gap in ML\\   between stakeholders creates circling ambiguity\end{tabular} & \begin{tabular}[c]{@{}l@{}}- formulation of argument's\\   target depends on how surprising \\ 
 assumption's  consequences are\\ \\ \\ - ambiguities in premises and \\   implied arguments are disguised\\ \\ \\ - target of one assumption forms\\   the premise of other targets\end{tabular} \\ \cline{2-4} 
 & \begin{tabular}[c]{@{}l@{}}Unstructured\\ documentation\end{tabular} & \begin{tabular}[c]{@{}l@{}}  \textbf{Informal and implicit recording:}\\ - distinction between formulation and installation\\   of assumptions is not clearly documented\\ \\ - site, content, and style of assumption recording\\   is strongly associated with role, resulting in conflict\\ \\ \textbf{Granularity of recording:}\\ - insistence to understand the rationale behind\\   assumptions lead to further assumptions based\\   on lived experience\\ \\ - no structured prompt to record the required level\\   of details in assumptions recording\end{tabular} & \begin{tabular}[c]{@{}l@{}}- lack of distinction between \\   formulation and installation \\   of premises in documentation\\   \\ \\ \\ \\ - premises are often recorded as \\   declarative statements with no\\   relation to target of the argument\end{tabular} \\ \hline
\end{tabular}
}
\caption{\rev{A summary of key themes, takeaways, and the premise-target theoretical lens we adopt to deconstruct and understand the confusing factors about assumptions in ML.}}
\label{tab:findings}
\end{table*}


\subsection {Ontological Differences}
\label{sec:ontology}

The conceptualization of an ``assumption'' varied greatly between participants. While some gravitated toward an understanding that coincided with their preexisting technical workflows, others perceived it as an isolated consideration, existing outside the normative bounds of development. These inconsistencies may be rooted in a systemic de-emphasis of abstract thinking, incentivizing participants to view socio-technical concepts as a static, external object rather than an embedded mentality \cite{selbst2019fairness,wang2022towards,malik2020hierarchy,fazelpour2020algorithmic}. When prompted explicitly to define ``assumptions'', many participants described it as preliminary, something to be ironed out, built upon, or invalidated. These initial assumptions also predicated on how future assumptions were handled. The elaboration of how this characterization plays out throughout the development process differed, with some participants conceiving of an assumption independent of internal components (section \ref{subsec:ind}) and others associating it directly or indirectly with other entities (section \ref{subsec:rel}). Both these constructions create uncertainties and confusion in their own ways, both in how they shape institutional handling of downstream tasks and potential harms. 
% Redundant:
% 
% Ultimately, it was the background and experience of the participant that dictated how they distinguished assumptions. 

\subsubsection{Independent Construction}
\label{subsec:ind}
\citet{delin1994assumption} describe how average discourse around an assumption implies that it is a sort of abstract entity existing in one's mind, and to interact with one is akin to finding, identifying, and examining a ``thing.'' This interpretation best characterizes what we label an \textit{independent} construction of an assumption. This type of assumption exists as an external \textit{other} to the primary subject---the ML workflow. The \textit{purity} of this workflow is a common theme among participants with a technical background. The idea of the model and the deference to ML work is a foundational mentality that conceptualizations of risk \cite{saxenaRethinkingRiskAlgorithmic2023,zanotti2024ai}, bias \cite{andrusWhatWeCan2021,kernAssumptionsBiasData}, and, in this context, assumptions must work around. 

This is best seen through how technologists frame assumptions as independent ideas that exist to serve the technology. For instance, for many participants, we introduced the concept of assumptions by inquiring the participant about \textit{givens}, or what information or knowledge the participant takes for granted. These givens are necessary \textit{premises} to begin a project as they unquestionably validate the primary heuristics of the project. In other words, the \textit{target} of independently constructed assumptions remains unstated or, in the best case, unscrutinized if it is explicitly mentioned. This is often because these assumptions lay the foundation of a project, and they are often immutable and the rest of the work must be accommodated. This immutability is justified as the organic nature of an AI model, with assumptions being reflective of its surrounding context rather than the technology itself. P1, an ML developer, explains the unspoken nature of these givens:

% Redundant:
% The emphasis of the technology as the central component of the product lifecycle further contributes to this mindset.
% P1, a technical product manager, elaborates on this:
% \begin{quote}
% \textit{"How I describe an assumption is something that’s taken for granted. Something where I don’t have to think about it. And it’s a fact or a basis for the work that I’m about to do. I don’t really need to question, hey, what’s happening? Is this what I need to do? It just happens, or it’s there, and there’s no question about the validity of it."} 
% \end{quote}
\begin{quote}
\textit{``I guess the most basic assumption is that the human behavior can be modeled with numbers...Because if you know these things are unquantifiable, then there’s no work...It's a set of axioms, I guess, from which I can draw conclusions. And if these axioms are violated, then, you know there’s no guarantee how the system will turn out.''} 
\end{quote}

% These givens are also provided through a proxy that is perceived trustworthy and thus rarely questioned. These may be institutionalized teams or roles that convey the givens directly or the background and experience of the technologist. 

Recent research suggests that the assumptions and choices of technical practitioners, in particular, are often found to be more subtle \cite{kery2019towards,kommiya2024towards,wang2019data}. These types of assumptions reflect a desired \textit{implicitness} to certain thinking that allows the technology to be developed in the first place. The assumption then possesses an \textit{interpretive flexibility} \cite{meyer2006three,star1989structure,leigh2010not}, being swept under the rug but still requiring further assumptions to validate it and allow for it to remain unspoken. This process may entail the transformation of an assumption into a \textit{requirement} or a \textit{limitation}. The former is managed through the validity of the assumption by an authority, which is either the assumer themselves and their expertise or a separate role or team that explicitly assigns it credence. They become \textit{informed} assumptions, legally justified and deliberated upon by personal decision-making. The latter may be designated as such through real-world constraints that prevent deeper internalization. Assumptions that are relegated to limitations may also be the byproduct of a ``perfection is the enemy of good'' culture \cite{sylvester2018applied,green2019good}. The assumptions that are not solidified as axioms, or hailed as requirements, or relegated as limitations, are then needed to be empirically validated and conceptually clarified in relation to the target towards which the assumptions are directed.
% in order to assess risk.

% Independent assumptions are treated as \textit{boundary objects} \cite{star1989structure,leigh2010not}, where the back-and-forth between ill-structured and well-structured forms are to be expected. 

% \begin{quote}
% \textit{I’m sure you know, the model would not get things correctly, like 100\% of the time. But you know, we don’t aim for perfection. We aim for some number like 99.9\% of the time. Maybe that’s good enough.} 
% \end{quote} (P2)
% \textit{…it feels like for a lot of product teams, they get hindered because of the years of risk. So they just really want to move forward so that they can actually deliver something.} 
% (P3)

\subsubsection{Relative Construction}
\label{subsec:rel}
Other participants had a more \textit{embedded} perspective on assumptions. While independent construction allowed the assumption to exist as its own object flowing through and being manipulated by the ML workflow, \textit{relative construction} implies that assumptions exist \textit{in relation} to existing phases of the development process. Returning to \citet{delin1994assumption}'s characterizations, this type of assumption involves examining the ``mental-event-or-state'' of an individual or institution instead of perceiving the assumption as an independent entity. In other words, relative assumptions exist as a byproduct of the practitioner's \textit{mentality} in a workflow rather than an external factor.

In particular, participants embodying this perspective defined assumptions through technical framing, citing decisions around data quality or model specifications. The assumptions must live within the confines of a technically-driven approach and be subject to dissection through that lens. This integration of assumptions into a technical dimension can help practitioners investigate deeper issues within their work, despite the purview being narrower. Since the target of the assumptions is often explicit in this case, identifying the argument is relatively easier than in independent construction. This has both upsides and downsides: it allows the practitioner to navigate the assumption through a familiar paradigm, but it also presents assumptions as an inevitability through the sheer breadth of available information. This could allow assumptions to go unchecked, but in a justified resignation, as demonstrated by P2, an ML scientist:

\begin{quote}
\textit{``...we kind of don't have the bandwidth to check each and everything. So that's maybe one assumption we make. We're also kind of assuming that...all of this leads to data correctness...but we end up making some assumptions like, for example, data is about accuracy. If data is empty, then you should just treat it as empty and not treat it, as you know, something meaningful...They are assumptions, and they will sort of, you know, go into the model and be baked into the process.''}
\end{quote}

% Relative assumptions, like independent assumptions, originate outside the realm of the ML workflow, but are \textit{actualized} through technical framing. 

Relative assumptions too possess an inherent implicitness that may unintentionally inform understanding of the model. But if an independent construction allows for an assumption to persist as its own object with the possibility of transforming into something beneficial to the technical process, then a relative construction attempts to integrate an assumption directly \textit{into} the process. The formulation of problems is an example, as they usually become intertwined with relative assumptions: P2 described how associations with specific demographics, for instance, may be less scrutinized due to the expectations of the technical team. In other words, if the data is labeled or categorized in a certain way that conforms to the lived experience of the practitioner, it may prevent deeper assessment. And because relative assumptions are tied directly to workflow components and workflow components are necessary, there is an incentive for the assumption to be rationalized.

% \textit{If you know, your assumptions start becoming givens, and you know ideally at the end of your project you should have only knowns and no assumptions, nothing left to assumption.} (P5)
% \\
% \textit{….because at the end of the day, bias in your model means bias in data and bias in data is just like reflecting what exists in the real world…So there those are certain assumptions in terms of like, 'oh, this bias already exists and you’re conforming to it in a certain sense…'} (P6)

Relative assumptions need not necessarily be attached to \textit{technical} processes only. A few practitioners in management roles described how assumptions can linger throughout business objectives and outcomes; what is deemed critical to operationalizing the company vision is often an assumption in and of itself. These assumptions are often second or third-order assumptions \cite{berman2001opening,korzybski1958science}, meaning that the assumer is multiple degrees removed from the original observation that incited the assumption\footnote{\rev{To understand the confusions around assumptions, for the sake of clarity, we almost exclusively treat an assumption independently of other assumptions. We leave explorations of higher order and connected assumptions to future work and briefly point to related works at the end of section \ref{disc:articulate}.}}. More so, the authority attached to the central teams that assert these claims makes it easier to internalize relative assumptions because the task of proving them right is often the primary function of the business. 


% \begin{itemize}
%     \item 			i. Data collection quality - 30-35 age not checked - if model performs badly for this distribution, we won’t know
% \item			ii. Data correctness 
% 	\item		iii. Lack of data - loss to follow up before death - how to impute missing data
% 	\item		iv. Data quality is often equated to data accuracy
% \item First data quality is investigated because that is often taken for granted
% \end{itemize}
% -  Investigate assumptions if personally relevant - else take altruistic stance

% <1 para>
% - Another conception of assumptions has an implicit relation with limitations
% - though when practitioners talk about limitations, assumptions are inherently present, they are unable to describe it explicitly.
% - Unable to describe assumptions as limitations - the latter is always in terms of outcomes - except a few - Assumptions are ignorant decision or requirements - can be seen as limitations
% - Though assumptions inform various actions/decisions, focus is on latter and not on former

% <unused for now>
% - Assumptions in operationalizing business vision - economic impact vs revenue example - 40\% assumptions comes from objective or mission statements - intentionally used - anticipate


\subsection {Procedural Uncertainties}
\label{sec:procedure}

Other than ontological differences, practitioners also face several challenges in determining what to do with the assumptions. In this section, we lay out the uncertainties that accompany various \textit{procedures} practitioners employ to identify and handle assumptions. We find that these uncertainties either correspond to integration methods with existing workflows (section \ref{subsec:integrate}) or the documentation practices for collaboration and accountability (section \ref{subsec:doc}). 

% Finally, we discuss the strategies practitioners employ, and the challenges they face to bring some structure to the articulation of assumptions for alleviating some of the surrounding confusion (section \ref{sec:articulate}.) 

\subsubsection{Integration with existing workflow}
\label{subsec:integrate}
\citet{brookfield1992uncovering} argues that the investigation of assumptions must be a deliberate and reflective process to assess and validate various decisions that go into a process. However, most practitioners we spoke to shared that their typical ML lifecycles in practice seldom offer them avenues to reflectively identify and handle assumptions.

\smallskip
\noindent \textbf{Reactive Handling.} Machine learning in practice is usually an iterative and outcome-oriented pursuit \cite{ashmore2021assuring,malik2020hierarchy}: almost all our participants start their ML lifecycles with lesser information than they ideally need, but then iterate and learn about assumptions as they go. Because there is no appropriate structure for them to inquire about assumptions, this iterative process often puts practitioners in ambiguous situations where there are no clear and accepted procedures to follow when one is found. This, in turn, demands practitioners to adopt a \textit{reactive} approach to identify and handle assumptions. This is most prominent in relative assumptions, in which the assumption is directly embedded within the ML workflow. For instance, some practitioners, such as P4, raise a red flag and look for assumptions in data when presented with a ``neat classification problem'' in which all data is categorized too perfectly. For a few others, assumptions are investigated only during exploratory data analysis (EDA for short) when extreme and skewed patterns about data are observed. \rev{P3, a technical lead, provides an example of this mentality by demonstrating how their team took certain factors for granted until a ``surprising'' result was found:}
\begin{quote}
    \textit{\rev{``So in any modeling process, we do some basic EDA to understand the quality of the data. But some things we take for granted. We then build a model. And sometimes, surprisingly, results will be like, very good than what you anticipate. Then we start looking into the top features. And then we think about, okay, is there any data leakage? This is very hard to understand when you're doing initial EDAs, right? But once we build the model, once you're seeing surprising results, then we go back to business. Then we discuss its implications...is there anything wrong with that feature?''}}
\end{quote}

Because relative assumptions can be associated with both technical and business processes, what is deemed surprising by the latter may inform the reaction of the former. This is demonstrated by P5, a lead data scientist, who articulated how the inquiry of assumptions often takes place only when the outcomes have unanticipated business implications in the typical ML workflow.
While the above-discussed ``innocent until proven guilty mindset'' (P6) is prevalent among ML teams, some practitioners in management-related roles highlighted that product teams generally use existing collaboration infrastructure, such as GitLab\footnote{https://about.gitlab.com/}, Asana\footnote{https://asana.com/}, or other business management software to inadvertently integrate assumptions discussions into the typical workflow. Though not a typical practice of ML teams, a couple of practitioners also shared that dedicated ``assumptions trackers'' are sometimes used to encourage documentation and discussion of assumptions at work. 

However, P6 shared that the use of these tools is largely descriptive and only results in noting down the simplistic description of assumptions, subjective association with the argument's target, and mapped ownership of assumptions redressal. 
Overall, assumptions are communicated through these tools, but it is unclear if they are reasoned and rationalized: questions such as why the assumptions are made, why they are necessary, what kind of assumptions they are, what their implications for ML lifecycle are, etc. are sparsely addressed by these tools. We expand upon this tendency to examine assumptions only at a surface-level in the next section.
% quote : i have a decade long experience in client facing roles - with great confidence i can say that this is not the case

% - though assumptions tracker etc. could be helpful, it often requires a historian type method - As historian, methodology revolves around recognition of “contingency” - reflective endorsement
% -- case study analysis


% Finally, integration into existing workflow also is contingent on the cost of 
% <1 para> - hold this for now
% With recent increase in RML attention, there has been some efforts to invoke assumptions - however
% 1. RML docs are not universal - best used for system documentations
% 3. RML toolkits are not used in many organizations 
%     a. Assumptions work comes at a cost - RML doesn’t help - Mediterranean addition example
% 4. RML just borrows inputs from various laws - we need dedicated legal frameworks.

\smallskip
\noindent \textbf{Unreflective Practice through Quantification.} A crucial constituent of assumptions identification and assessment is the \textit{reflectiveness}, if not \textit{critical reflectiveness}, of the process \cite{paul1993critical,paul1993workshop}. 
However, \textit{the seductions of quantification} \cite{merry2009seductions} often derail practitioners from performing reflective exercises on assumptions and instead reroute them to aspire for a fictitious objective and unequivocal state. \rev{P10, an engineering consultant, elaborated on her perceived futility in quantifying assumptions:}

\begin{quote}
   \rev{ \textit{``...we do have some in-house metrics…we also really want to know which task is being performed the best without any sort of assumptions or biases so we have inbuilt like a comparator for ourselves, based on particular metrics… I believe these metrics can be very much subjective, based on the use case a particular company or a particular product has...there is no way we can quantify what assumptions a particular model is taking…''}
}
\end{quote}

\textit{Independent} assumptions are most prone to this instinct---they must adhere to the workflow. Practitioners noted that the organizational constraints and standard ML workflow practices do not incentivize many technical counterparts to perform reflective exercises but only make them care about data coverage and infrastructure-related concerns. But this interaction between assumption and workflow is bi-directional. Both in their own practice and during our case analyses, practitioners suggested ways of using various computational methods \cite{yanga2024exploratory,zhang2018empirical} to debug and act on their assumptions. However, as an extreme example, one practitioner supported the use of software plugin-type tools to automatically check all assumptions and provide a score to quantify assumptions evaluation. 

In our case analyses with practitioners, we find that many of them spend more time understanding and justifying various computational steps in LLM reports but focus less on reflecting on the premises and conclusions that inform the various steps they are analyzing. We observe that practitioners with some exposure to responsible ML principles often recognize that over-reliance on computational methods just disguises the underlying ambiguities, not removes them \cite{green2018myth,fazelpour2020algorithmic}. This means that the organizational instinct and incentive to adhere to the ML workflow implies a bias to a relative construction of assumptions. They also shared that such disguising has the danger of pushing practitioners to ignore the assumptions and the implied argument, which often peek through at later phases of ML lifecycles in various forms. \rev{P7, a designer working with state organizations, shared how a risk assessment tool unnecessarily quantified risk evaluation:}

\begin{quote}
    \textit{\rev{``so these risk assessment questionnaires are designed such that you're kind of casting a wide net like, yeah, you're probably at risk of something. After they do the risk assessment, the general next step is, your team has to come up with a control plan in detail explaining how you're going to mitigate those risks. So you know, obviously, some of the feedback that we got was like, okay, so we could come up with the most horrific AI system that has all these risks to end users. And it's all okay, because we can just write a control plan, saying that we'll mitigate right? So there was a lot of like contention with that approach.''}}
\end{quote}

While the quantification of assumptions is intended to reduce uncertainty around decision-making, further assumptions emerge when model performance is negotiated with other sociotechnical concerns, such as safety and fairness. For many practitioners, this leads to a nuanced tension they face when, for instance, deciding on a lower threshold for the maximum number of security violations. For example, while a 90\% violation is clearly red-flagged, negotiation between 0.5\% and 1.0\% creates debates and confusion. We expand on the cyclical nature of assumptions in the next section.



% benchmarks curb reflectivity?
% <-- finally benchamarks are blindly used (cite gorilla etc.) - see quote about assumptions>

% - it was not a one side affair - some practiotners also sggested ways of reflectively using compuational methods - some also did it in our case studies
% -- while some computational techniques aid assumptiosn inquirt reflective - CF and knowledge base
% - however, the infrastructure (connect to worlflow + documnet next)

% -- embarassement score    

% When participants were asked to identify assumptions more indirectly through analyzing model documentations at face-value, some associated anything distinct from technical terminology as an assumption.

\smallskip
\noindent \textbf{The Circle of Ambiguity.}
While assumptions are often invoked to mitigate the ambiguity around unresolved questions, ironically, due to their very implicit nature, they instead raise several questions and concerns they are trying to ameliorate in the first place. Consider the use of automated risk-assessment tools used in many organizations in recent years: practitioners find that many responses to assessment software require extrapolation and reflection. However, the checklist-type design and quantification of risk evaluation in these tools obscure the \textit{consequences} of underlying assumptions. A few participants shared that several sub-modules of these tools in their organization require practitioners to invoke assumptions in their own responses, but there are typically no mechanisms to capture the evidence of these assumptions, creating more uncertainties. Therefore, the trajectory of an assumption in an ML workflow is one that may beget more assumptions. This ambiguity is discussed in \textit{Critical Thinking} as complex or \textit{chain of reasoning} arguments, where the assumption that supports a target may be the target of one or more assumptions, and so on indefinitely \cite{hitchcock2021concept}. 

% Looking through the analytical lens our practitioners took in the case studies, our findings highlight nuanced uncertainties that motivate technical practitioners to make subtle assumptions.  

Consider P4, an ML scientist, who expresses how the confusion around calibration scores \cite{pleiss2017fairness} among different non-technical stakeholders motivated their subjective decision on how to present the model outcomes:

\begin{quote}
    \textit{``...for multiple projects that I was on...I saw that the easiest thing to do was to provide high, medium, low kind of bucketing of confidences rather than to provide a confidence score. The assumption that kind of affords a user is that both significant digits are significant, that there is a difference between, you know, 0.95 and 0.97 or something like that. And that really wasn't the case. As a practitioner, I would see that and just be like, okay, that's not that significant. But how does especially someone who is not used to reasoning about the processes that produce these numbers know that?''
}
\end{quote}

The ordinal categories expected to resolve misinterpretation of double-precision metrics yield further confusion for both technical and non-technical stakeholders depending on how risk levels are construed. A possible explanation for this circling ambiguity, as indicated by some of our participants, could be attributed to the knowledge and communication gaps between people with and without ML backgrounds \cite{chen2021beyond,kommiya2024towards,varanasi2023currently}. P7 shared that their ML scientists \textit{``did not feel like there was a lot of risk involved''} when the models they work with are more interpretable to them, in contrast to what risk-averse employees felt about the model and its policy implications. This logic informs another assumption: to fix biases, what is needed is \textit{more} knowledge and training. However, most management-related practitioners in our sample concurred that technical knowledge in their organization is generally very isolated and emphasized the need to document differences in interpretations and motivating assumptions.

% Because products are made in the perspective of the organization, they can be easily dismissed to fall under the responsibility of a different position.

% \begin{quote}
%     \textit{At least in my experience, providing confidence scores, was not always a benefit, because the ways in which users interpreted those confidence scores varied a lot right. Some people would really set a lot of store by the difference between 0.90 and 0.92, and not care about the difference between 0.9 and 0.7. And so for multiple projects that I was on you know, I saw that the the easiest thing to do was to provide high, medium, low kind of like bucketing of confidences rather than to provide a confidence score. The assumption that kind of affords a user is that both significant digits are significant, that there is a difference between, you know, 0.95 and 0.97 or something like that. And that really like that wasn't the case. As a practitioner, I would see that and just be like, Okay, that's not that significant. But how do especially someone who is not used to reasoning about the processes that produce these numbers know that?
% }
% \end{quote}
% - exclusion complex
% - Some organizations have top-down assumptions handling 
% - 

% - we found similar observations - Mihir doc - quote - assumptins  techincal - infor  subjective deicon - rationale lost



\subsubsection {Unstructured Documentation}
\label{subsec:doc}

Though the successful adoption of responsible ML principles in organizational settings is open to debate \cite{deshpande2022responsible,rakova2021responsible,raiAdoption}, practitioners are undoubtedly getting increased exposure to different toolkits and frameworks for responsible ML \cite{liang2024systematic,yang2024navigating}. While most of these support systems have some reference for practitioners to elicit and discuss assumptions behind various decisions (section \ref{rel:periphery}), prior works are unclear on how and to what extent assumptions are documented through these toolkits from the perspective of a practitioner. Below, we discuss two characteristics of documentation practices that contribute to the confusion around assumptions.
% For instance, a recent study found that the use of framework such as modelcards have rapidly risen in the last few years.

\smallskip
\noindent \textbf{Informal and Implicit Recording.} \citet{delin1994assumption}, in their discussion about assumptions, make a distinction between formulating and installing an assumption: while \textit{formulating} is about the expression of intent to install an assumption, \textit{installation} corresponds to aligning the execution to the intention within given constraints. In one of the excerpts in our case study (appendix \ref{appendix} \cite[p.~64]{anil2023palm}), the authors use ``marked references'' or annotated characterizations of identity\footnote{Some participants speculated the assumptions behind the usage of this term, but for this discussion, this can be perceived as typical annotations in ML data pipelines.} to assess the representational bias of their language model. Though our participants did not use the terms referenced in the \textit{Informal Logic} literature exactly, they did observe that if the authors had intended to operationalize representativeness with these annotations, then they must have correctly \textit{installed} the assumptions but did not explicitly document the \textit{formulation}. While the authors of the case study vaguely note down this assumption in the limitations section of the report, many of our participants were dissatisfied and shared that assumptions that are not articulated at the time of their identification get forgotten over time. 

Because the recording of assumptions is often informal and unstructured, system documentation and other related trackers are rarely modified after an assumption is acted upon. While some practitioners stated that their organizations require them to record assumptions in separate sections in accordance with a design or product requirement, others just recorded them offhand in change logs. Both approaches offered no structured prompt or instructions about how to aptly record them. As such, many participants hinted that documentation is not often given serious thought for internal projects. Below, P9, another technical lead, admits how these details are either not documented explicitly or get lost in an attempt to simplify the language.

\begin{quote}
\textit{``...when I run into an assumption like that, I try to distill it sometimes through multiple rewrites into a simple, concise, clear declaratory statement which can be connected to others...I would forget what I had in mind when I was writing those things down...'' 
}
\end{quote} 

The site, content, and style of assumptions documentation also have a strong relation with the primary role and responsibility of the practitioner, leading to conflicting situations when one party (say an ML scientist) has to consume and work with assumptions recorded by another party (say a product manager). For instance, writing and analyzing compliance-related reports are often perceived as the responsibility of safety or legal teams. Their articulation of assumptions might differ from that of developers and data scientists, who usually write in terms of inputs and outputs. If these technical practitioners are tasked to write about assumptions, they are less likely to appreciate the implications of their assumptions and choices in their reports. As the data scientist P5 mentioned, when they use LLMs, they go directly to model docs, datasets, and benchmarks, and treat the system cards and other reports accompanying these LLMs (with safety and RAI analysis) as just \textit{``terms and conditions.''} This fundamental asymmetry presents a fragmentation of thinking and recording that has consequences for how assumptions are handled. 

% 		ii. RML just borrows inputs from various laws - we need dedicated legal frameworks.
% Some implicit decisions require technical effort - need better documentations

% In this section, we will use an example from our case study where the LLM authors mention that they \textit{``excluded data from certain sites known to contain a high volume of personal information about private individuals''} (cite).

\smallskip 
\noindent \textbf{Granularity of recording.} We leverage the \textit{analytical} perspective our practitioners took in the case studies in assessing how deliberate they were in recording assumptions. As \citet{brookfield1992uncovering} argues, this particular type of perspective is well-positioned to step out of a familiar interpretive frame of reference and look at assumptions through an unfamiliar lens. We observe that the most common practice the participants followed is simply listing down the identified assumptions in some style and form, though at varied levels of formality, such as they would in a business requirement document or vocally at a team meeting. However, practitioners note that this method generally relies on their own unjustified claims. In other words, there was no distinction between the premises, target, and the argument that the assumption-target complex makes.

While listing down assumptions found in the case study, most of the practitioners sought further clarification on \textit{why} particular assumptions were made. This phase is when the interpretive lens of a practitioner begins to crack, and their primary disciplinary training starts to dominate; the choice of assumption to expand on and what needs to be explained starts to depend again on their organizational role or lived experience. For instance, when technical practitioners observe relative qualifiers such as ``higher'' or ``lower'', their scrutiny often stops at the sight of a quantified relation, such as ``40\% higher.'' \rev{Participants shared that many technical practitioners} end up not reflecting on how benchmarks are only the \textit{indications} and not the equivalence of the capability they are measuring. \rev{In the words of P11, an AI ethicist:}

\begin{quote}
    \textit{\rev{``The fact that an LLM could perform well on an LSAT benchmark means not necessarily that it's capable of, you know, of solving legal problems. But it could be quite capable of delivering to you the outputs that mimic responses to those questions and that's valuable. Where we can go wrong is to infer that, you know, because the model can pass a test that must mean that its feature representations allow it to understand the material of the test. This is a very different question and that I feel like needs holistic evaluations. You know, once they're developed, there's a huge evaluation gap here.''}}
\end{quote}

% \rev{Below, P4 explains how the granularity of assumptions inquiry ends when benchmarks are conflated with the capability they are intended to measure:}

% \begin{quote}
%     \textit{\rev{``if your model can solve this entailment data set, the idea is that it can do textual entailment more broadly. But there are several kinds of subpopulations that you need to reason about before you can actually take that to the task that you actually care about. It's reasonable with the intentional definition of entailment that is presented to say, like, oh, my model can understand that 3 tenths and 30\% of the same thing. That is like a linguistic equivalence. But models don't do that right. So if you are building an entailment based application that relies on that kind of numerical reasoning, your application will break. That's not an assumption that you can actually...do textual entailment as a capability rather than textual entailment on MultiNLI as a data set.''}}
% \end{quote}

How a clarification is or ought to be justified is a critical step in assumption recording, the complexities of which are expanded upon in the following section. P8, an AI governance architect, makes a distinction between \textit{visibility} and \textit{explainability} in recording assumptions to avoid confusion. While the former could be one or more declarative statements that add a marginal level of detail, the latter is about reasoning in every step with logical validity. For instance, in many of the LLM reports, our participants observed that data quality is justified by arguing that the followed data processing steps led to ``good'' model outcome in terms of some metrics. However, the above aligns more with the visibility-level of clarification. An explainability-level of granularity would involve expanding how quality is operationalized (for example, showing no outliers, following a representative distribution, etc.) and then logically explaining how the data can be assessed on both these parameters and the model outcome.

Finally, the maximum level of granularity in which an assumption can be recorded is ambiguous and context-dependent. While some practitioners seek ``expert'' intervention, which entails transferring the authority to a team lead or an executive, most practitioners instead advocate for a diverse collaborative effort to avoid biases. In section 5.1, we discuss a framework inspired by argument structuring in Informal Logic to articulate assumptions that facilitate such discussions.

% To do:
% 1. change quotes repetitive participants
% 2. include citations
% 3. check tense consistency

% We observe that practitioners implicitly or explicitly follow three distinct levels of granularity in recording assumptions. 