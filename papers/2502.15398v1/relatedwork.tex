\section{Related Works}
In this section, several notable works on image recognition models and attention modules are briefly reviewed.

\subsection{Deep Learning Models}
\textbf{CNNs} are commonly found in a variety of computer vision tasks, including VMMR. Several studies concentrate on designing model architectures to enhance representational power of CNNs. For instance, in ResNet \cite{he2016deep}, shortcut connections are utilized, allowing the CNN to expand to hundreds of layers. Their findings reveal that raising the architecture depth can considerably strengthen the representation power of a CNN. DenseNet \cite{huang2017densely} employs dense connections across layers, allowing each layer to access the feature maps of all previous layers as inputs. This design provides several key advantages: it helps alleviate the vanishing-gradient issue, facilitates feature reuse, improves feature propagation, and notably reduces the total parameter count. Based on the depthwise separable convolutions \cite{sandler2018mobilenetv2}, proposes MobileNetV2, which is specifically designed for limited resources and mobile environments, which uses a new inverted residual structure as the building block. Using neural architecture search, \cite{tan2019efficientnet} designs a new baseline architecture and scales it up using a new compound scaling method to derive a family of models known as EfficientNets that outperform earlier CNNs in terms of efficiency and accuracy.

\textbf{Transformer} performance in the field of NLP drew the attention of the computer vision society to it, and with the introduction of ViT, Transformer was seriously incorporated into computer vision tasks. Following the ViT, other transformer-based models were proposed to improve performance in computer vision tasks. For example, Swin Transformer \cite{liu2021swin}, which functions effectively as a versatile backbone for computer vision, generates a hierarchical structure design and has linear computational cost in relation to the size of the input image. Shifted window-based self-attention, a fundamental component of Swin Transformer, is demonstrated to be successful and efficient in addressing vision challenges. CrossViT \cite{chen2021crossvit} is a dual-branch transformer architecture for extracting multi-scale features, in which a fusion approach built on cross-attention is used to effectively mix picture tokens of different patch scales and exchange information between two branches.

\textbf{hybrid} architectures combine transformers with convolutional layers to introduce locality into the transformer model. MaxViT \cite{tu2022maxvit} utilizes a hierarchical structure with multi-axis attention mechanisms for efficient local and global feature extraction. This design integrates both convolutional and self-attention layers in a novel and effective manner. The hierarchical transformer design, Pyramid ViT (PVT) \cite{wang2021pyramid}, proposes a sequential shrinking pyramid and dimensional reduction attention. PVT-V2 enhances the original PVT by incorporating three key features: firstly, an attention layer with linear complexity, secondly, overlapping patch embeddings, and finally a convolutional feed-forward network.

\subsection{Attention Modules}

Various works have introduced attention modules that can be applied to many CNNs \cite{manzari2024befunet, saadati2023dilated}. These attention modules modify feature maps, empowering the CNN to target on important areas of the image or specific and informative features and make decisions based on them. A representative work is Squeeze-and-Excitation (SE) \cite{hu2018squeeze}, which proposes channel-wise attention and models the interdependencies between the channels. This is achieved via a three-step process: squeeze (global average pooling), excitation (learning channel dependencies with fully connected layers), and scale (reweighting feature maps). Later works, CBAM \cite{woo2018cbam} combines spatial and channel-wise attention mechanisms. SRM \cite{lee2019srm} improves a CNN's representational strength by embedding styles into the feature maps. GC \cite{cao2020global} effectively captures long-range dependencies to enhance semantic comprehension. ECA \cite{wang2020eca} adopts a local cross-channel interaction method without dimensionality reduction, implemented efficiently through 1D convolution. CA \cite{hou2021coordinate} presents a new lightweight attention method, termed Coordinate Attention, designed for mobile networks. This mechanism combines the advantages of channel attention in modeling inter-channel relationships with the ability to capture long-range dependencies and maintain accurate positional information. To compute the 3-D weights, SimAM \cite{yang2021simam} designs an energy function using some established neuroscience theories.Using simple yet effective operations, such as average pooling to capture long-range dependencies and the dot product to represent cross-dimensional interactions, SIAM \cite{han2024siam} generates 3-D attention maps with minimal computational overhead.