
\section{{\normalsize \textsc{Model-SAT}}: Model Routing with Aptitude Test}

In this section, we start by building the model representation and progress to the details of \textsc{Model-SAT}, training data, and optimization process. Finally, we outline an efficient deployment framework for model routing.

\subsection{Capability Instructions}
\label{sub_sec:capability_instructions}
The \textit{capability instruction} mainly comprises the capability representation of the candidate model $f^m$, user instruction $\mathbf{x}_i$, and performance inquiry prompt.
Specifically, the model's capability representation is formed from 50 distinct tasks across various categories from the MMLU dataset, with each task being 20-shot. We provide a concise description of five keywords for each task.
Next, we evaluate the candidate models across these 50 tasks and describe the results in natural language, \textit{i.e.}, model representation.
Furthermore, the advantage of representing in natural language is that it helps to include extra expert knowledge, such as mentioning \textit{which languages a model supports}.
The easy-to-obtain representations serve as an aptitude test for the models, indicating their potential capabilities across various dimensions.

To assess how well the candidate model can follow a single or a set of instructions $\mathbf{x}_i$, we introduce the training instructions that were executed correctly versus those incorrectly. These will be paired with the performance inquiry prompt $\texttt{p}$ to form the \textit{capability instruction}, denoted as $\mathbf{z}_i$, which drives the router to predict adaptation scores.
As illustrated in~Figure \ref{fig:capability_instruction}, it combines the capability representation $\texttt{c}^m$ for candidate $m$, the instruction $\mathbf{x}_i$, and a inquiry prompt $\texttt{p}$.

\noindent \textit{Core task sampling}: We sample instructions of core tasks with the highest distinguishability, avoiding those where most models perform correctly or incorrectly. In the model zoo of training, samples for which half of the models make mistakes while the other half are correct carry greater weight.


\subsection{Architecture}

\textbf{Motivation}: Although LLMs demonstrate strong instruction-following abilities, a gap exists between performance and the semantic distribution in \textit{capability instructions}, particularly in understanding combinations of performance dimensions. For example, if a candidate model achieves 80\% in mathematics and 95\% in legal principles, the model may possess legal reasoning skills.
To address this, we propose extending a capability encoder E5-Large \textasciitilde0.5B ($\boldsymbol{\psi}$) before a Phi-3-Mini 3.8B LLM ($\boldsymbol{\varphi}$) to align the candidate performance with the instructions. The architecture is illustrated in Figure~\ref{fig:architecture}.

\textbf{Structure}: The capability instruction comprises the capability representation $\texttt{c}^m$ for model $m$, the instruction $\mathbf{x}_i$, and the query prompt $\texttt{p}$. We first align the model representation, mapped by the capability encoder, into an embedded feature of LLM inputs. This is achieved using a single-layer MLP, which acts as a connector to adapt the dimensions. Consequently, we derive the aligned model capability vector:
\begin{equation}
    \mathbf{e}_{\texttt{c}^m} = \mathbf{W} \cdot \boldsymbol{\psi} \left( \texttt{c}^m \right)\,,
\end{equation}
where $\mathbf{e}_{\texttt{c}^m}$ is combined with the input embeddings of $\mathbf{x}_i$ and $\texttt{p}$ to form the capability instruction $\mathbf{z}_{i}$, \textit{i.e.},
\begin{equation}
    \mathbf{z}_{i} = [\mathbf{e}_{\texttt{c}^m}, \mathbf{e}_{\mathbf{x}_i}, \mathbf{e}_{\texttt{p}}] = [\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_s]\,,
\end{equation}
where $s$ denotes the length of the concatenated capability instruction sequence. Our alignment module operates at a natural language level, allowing for a streamlined design.  In the following Section~\ref{sec:experiments}, we also explore alternative approaches, including training without the alignment module.

\subsection{Tuning Recipe}

\textbf{Forward Process of the Prediction Score}: As shown in Figure~\ref{fig:capability_instruction}, the query prompt $\texttt{p}$ in the capability instruction includes keywords related to positive terms that the model excels at. For example, ``Yes'' serves as the key response in the prompt ``predict whether the model can handle test instruction by indicating `Yes' or `No'.'' In this context, the model routing prediction score is:
\begin{equation}
    \operatorname{Pr}\left( \text{`Yes'} \mid \mathbf{z}_i \right) = \prod_{t=1}^{s} \mathbf{1}_{\left(|\texttt{c}^m|, s\right]} \cdot \boldsymbol{\varphi} \left( \mathbf{e}_t \mid \left[ \mathbf{e}_1, \cdots, \mathbf{e}_{t-1} \right] \right)\,,
\end{equation}
where we omit the input embedding layer for the LLM $\boldsymbol{\varphi}$.

\input{tables/main_results}

\textbf{Positive and Negative Instructions for Training}:
We apply Homogeneous In-Batch Negative Sampling~\cite{DBLP:conf/emnlp/KarpukhinOMLWEC20,DBLP:journals/corr/abs-2310-07554} for each capability representation $\texttt{c}^m$ with its well-performed and poorly-performed instructions to enhance the discriminative during training.
Typically, a $k$-shot training batch $\mathbf{Z} = \left\{ \mathbf{z}_i \right\}_{i=0}^{k}$ contains $1$ positive instruction and $k-1$ negative ones.

\textbf{Loss Design}:
We denote the position of the positive instruction in the training batch $\mathbf{Z}$ as $y_{\text{pos}}$, and the remaining ones are $k-1$ negative instructions.
Our objective is to enhance the prediction score for the positive ones as the candidate performs better on this instruction.
We employ the cross-entropy loss to optimize this in one batch $\mathbf{Z}$:
\begin{equation}
    \mathcal{L}_{\text{CE}} = \mathbb{E}_{\mathbf{Z} \in \mathcal{D}_{\text{test}}} \left[ - \log \operatorname{Pr} \left(\boldsymbol{h}_{\boldsymbol{\varphi}, \text{`Yes'}}\left(\mathbf{Z}\right) = y_{\text{pos}} \, | \, \mathbf{Z} \right) \right]\,,
\end{equation}
where $\boldsymbol{h}_{\boldsymbol{\varphi}, \text{`Yes'}}\left(\mathbf{Z}\right) = \arg\max_{\mathbf{z}_i \in \mathbf{Z}} \, \operatorname{Pr} \left(\text{`Yes'} \, | \, \mathbf{z}_i \right)$ is the LLM $\boldsymbol{\varphi}$ to identify which instruction $\mathbf{z}_i \in \mathbf{Z}$ can be done well (positive) and which cannot (negative).

\textbf{Learning Strategy}: The model representation is derived from the capability distribution on MMLU. Similarly, we develop both in-domain and out-of-domain learning environments for \textsc{Model-SAT}.
In the first stage, we collect in-domain positive and negative training instructions, primarily sourced from the same category as the MMLU dataset.
We only fine-tune the connector between the capability encoder $\boldsymbol{\psi}$ and the LLM $\boldsymbol{\varphi}$, establishing an initial capability-to-instruction mapping.
In the second stage, we fine-tune all model parameters.
We apply a larger learning rate on the encoder and connector to enhance capability alignment with instruction semantics.

\textbf{Data Refinement}: We further address noise in whether the candidate model can accurately perform the instructions, influencing whether a capability instruction is a positive or negative training sample.
For those difficult instructions that only a few models handle correctly, we implement a circle test by rotating the sequence of options to prevent lucky guesses.
Furthermore, we prioritize higher-ranked candidates in the training data by sampling with increased weight.


\subsection{Efficient Deployment}
\textsc{Model-SAT} provides the routing prediction for the candidate model applied to the target instruction.
These scores are generated by the same model, rendering them comparable.
In this paper, we propose an open-source and comprehensive model routing toolkit, \textsc{Model-SAT}.
This toolkit offers a viable solution for dynamic model routing within communities such as HuggingFace, harnessing the repository to boost performance on target tasks.

\textsc{Model-SAT} exhibits remarkable generalization capabilities for unseen data, which can be directly concatenated into the capability instruction.
Similarly, the incremental extension to new models proves highly efficient, requiring only inference on 50 core tasks for the model representation.
As later addressed in the experiments, \textsc{Model-SAT} exhibits zero-shot model routing abilities, facilitating the streamlined development of capability instructions in broader contexts.

% TODO 现在有个问题就是 Capability Instruction Tuning 过程的training sample和test sample表达的不是很清楚


\section{Experiments}
\label{sec:experiments}

This section begins by detailing the construction of training and test instructions in \textit{capability instructions tuning}. It then presents different zoo setups for testing and concludes with an analysis of results and ablation studies.


\subsection{Training and Test Instructions}

As mentioned earlier, the \textit{capability instruction} consists of model representation $\texttt{c}^m$, instructions $\mathbf{x}_i$ to assign, and performance inquiry prompts $\texttt{p}$.

\textbf{Candidate Model Representations} $\texttt{c}^m$ for candidate $m$: We introduced 66 open-source LLMs of varying scales. This includes 60 models under 10B, 15 ones between 10B and 20B, and 5 ones around 60B.
We sample 50 categories from the MMLU dataset, with 20 distinguishing instructions from each. Different candidate models share core tasks to ensure stability in capability demonstration.

\textbf{Instructions} $\mathbf{x}_i$ \textbf{Pending to Assign}: We consider more than 20 datasets that include areas such as language, analysis, understanding, and reasoning in general evaluations, as well as specialized fields like mathematics and medicine. For each dataset, we sample sets of positive and negative instructions where the model performed well or poorly, with sampling on stronger models assigned greater weight. Each dataset contains about 100 instructions on average.

\input{tables/compared_results}

\input{tables/mm_results}

\textbf{Performance Inquiry Prompts} $\texttt{p}$: We explore different approaches for the probability of model routing. For \textit{capability instructions}, we design the performance inquiry prompt, such as ``predict whether the model can handle test instruction by indicating `Yes' or `No'.'' In this context, a response of `Yes' signifies that the model is well-performed to the instruction. We also experiment with integrating a regression linear layer onto the next token embedding.

The capability instruction for the test $\mathbf{z}_i$ similarly consists of the model representation $\texttt{c}^m$, the target instruction $\mathbf{x}_i$ to be assigned, and the performance inquiry prompt $\texttt{p}$. To ensure test stability, we conduct a perturbation evaluation on model representation. Specifically, we randomly alter the ranking of the aptitude test results in capability representation twice and then calculate the average routing scores $\operatorname{Pr}\left( \text{`Yes'} \mid \mathbf{z}_i \right)$. The response on this instruction $\mathbf{x}_i$ is provided by the candidate model with the highest routing score.





\subsection{Benchmarks of LLM Routing}

\label{sec:benchmarks}
In this section, we outline benchmarks with various LLMs and their extension zoos, featuring detailed settings.

\begin{figure*}[t]
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.97\textwidth]{figures/exp_time.pdf}
    \caption{\textbf{Real-world Model Routing with Latest LLM Release} on ARC-Challenge. \textsc{Model-SAT} (in red) quickly generalizes to unseen LLMs without extra training, maintaining robust performance despite dynamically adding diverse LLMs.}
    \label{fig:exp_time}
    \vspace{-12pt}
\end{figure*}
\textbf{Smaller-Scale LLM do Better}: As demonstrated in the Table~\ref{tab:m_res}, the \textbf{smaller-scale zoo} contains InternLM2.5 (7.7B), Meta-Llama-3-Instruct (8.0B), Qwen2-Instruct (7.6B), GLM-4 (9.4B), and Phi-3-Small-128K (7.4B).
The \textbf{smaller-mixed zoo} includes the smaller-scale zoo and Phi-1 (1.3B), BLOOMZ (3B), and Zephyr-Alpha (7.2B).
These LLMs have fewer than 10B parameters and low deployment costs.
In Figure~\ref{fig:motivation}, we show that the union of correct responses can cover a set of instructions that only larger-scale ones can manage.

\textbf{General LLM Zoo Settings.} \textbf{1}) \textbf{Middle-Mixed and Larger-Mixed LLM Zoo}: The \textbf{middle-mixed zoo} includes the smaller-scale zoo and Phi-3-Medium-128K (14B), and Yi-1.5-Chat (34B). The \textbf{larger-mixed zoo} consists of the middle-mixed ones, Meta-Llama-3-Instruct (70B), Qwen2-Instruct (72B), and Mixtral-8x22B-Instruct-v0.1 (140B). The mixed zoo can validate the routing method across different capabilities.
\textbf{2}) \textbf{High-Performance LLM Zoo}: We select from larger-scale LLMs to boost performance further.
The model zoo contains only three models above with over 70B parameters.
\textbf{3}) \textbf{Multimodal LLM Zoo}: To verify the generality of capability instruction tuning, we construct a multimodal LLM zoo that includes MiniCPM-Llama3-V 2.5, Phi-3-Vision-128k-Instruct, and InternLM-XComposer2-VL-7B.

\textbf{Instructions for Model Routing Evaluation}:
The test capability instructions differ from the training ones of model routers and consist of seven evaluation datasets.
Datasets including MMLU~\cite{DBLP:conf/iclr/HendrycksBBZMSS21} (5-shot) and WinoGrande~\cite{DBLP:conf/aaai/SakaguchiBBC20} (5-shot) cover a broad range and are involved in the training part as the in-domain evaluation.
On the other hand, datasets such as ARC-Challenge~\cite{DBLP:journals/corr/abs-2102-03315} (25-shot), TruthfulQA (6-shot), and BoolQ~\cite{clark2019boolq} (1-shot) with MRPC (1-shot) and MNLI (1-shot) in GLUE~\cite{DBLP:conf/iclr/WangSMHLB19} benchmark focus on specific capabilities, serving as the unseen out-of-domain evaluation of the model routers.
We consider the evaluation datasets MMMU-VAL~\cite{mmlu}, AI2D-TEST~\cite{kembhavi2016ai2d}, and CCBench~\cite{liu2024mmbenchmultimodalmodelallaround} in the multimodal scenario.

\textbf{Real-world Model Routing with Unseen Datasets \& Latest LLMs}: \textbf{1}) In Table~\ref{tab:m_res}, \textit{In-Domain} and \textit{Out-of-Domain} indicate whether the dataset is included in the training set for LLM routing. \textbf{2}) In Figure~\ref{fig:exp_time}, We design a novel model routing setting that, with the \textbf{release of 60 LLMs}, we update the existing zoo after each new model with the top 5 historically best and the latest one.
With the continual increment of unseen LLMs, this dynamic environment tests whether methods can maintain a compelling performance.

\subsection{Toward Comprehensive and Effective Routing}
\label{sec:results}
\textbf{Performance Analysis in Various Model Zoos.} Table~\ref{tab:m_res} demonstrates that \textsc{Model-SAT} performs impressively across five comprehensive LLM Zoos.
(a) Smaller-Scale: routing of LLMs under 10B achieve performance comparable to the \textasciitilde70B LLMs. \textsc{Model-SAT}'s average score of 75.28\% closely matches Meta-Llama-3-Instruct-70B's 76.90\%, and outperforms it on the ARC-C and TruthfulQA benchmarks. Furthermore, \textsc{Model-SAT} selects the optimal model for each instruction, surpassing the best-performing models in the Smaller-Scale Zoo.
(b) Smaller-Mixed: We add three earlier released, weaker, and smaller LLMs. \textsc{Model-SAT} maintained stable performance, with a slight decrease of about 1\% compared to row (a), while performance on ARC-C, BoolQ, and MNLI benchmarks remained nearly identical.
(c) Middle-Mixed: Row (c) includes two medium-scale models (10B to 70B), resulting in improved performance for \textsc{Model-SAT} compared to row (a). Its average performance now closely matches that of the 70B model.
(d) Larger-Mixed: Incorporating three 70B models in row (d) showed that \textsc{Model-SAT} remains robust despite significant performance variances in the LLM zoo, with improvements of nearly 5\% on MMLU, BoolQ, and so on.
(e) High-Performance: Row (e) features a routing of only three 70B models, revealing that the capabilities of gigantic LLMs are further unleashed, achieving a state-of-the-art score of 85.64\% on MMLU and 73.63\% on the ARC-Challenge.

\textbf{Comparative Analysis of Routing Delay.} In Table~\ref{tab:m_res}, the selected model parameter-scale is denoted as xB. The overhead associated with the re-ranking method is related to the $M$ candidates of xB. Figure~\ref{fig:related_works} illustrates that the re-ranking requires obtaining all inference results from the zoo first, while \textsc{Model-SAT} processes model representations all at once and utilizes them throughout its lifetime. As the scale of models in the zoo grows, \textsc{Model-SAT}'s routing cost remains unaffected by inference ones, ensuring efficient model routing.

\textbf{Comparison Ranking-based Methods}: We evaluate re-ranking methods such as Cappy, BGE-Large, and GTE-Large. Although they have access to the outputs of each candidate, re-ranking often struggles to find optimal results, potentially because it primarily focuses on retrieving different semantics rather than optimizing performance across similar outputs.

\textbf{Detailed Comparative Analysis}: Furthermore, we explore various learning strategies within the capability instruction tuning framework. Using features extracted from RoBERTa, we train MLP (classification-based), $k$ Nearest Neighbors (clustering-based), and Random Forest (tree-based). We observed that, except for tree models that are suitable for handling capability representation as similar tabular data, other learning strategies fail to capture performance distribution mappings. Additionally, we analyze \textsc{Model-SAT} without the capability encoder. Since the model representation is expressed at the natural language level, Phi-3 in Table 2 can also learn some LLM-Instruction mappings, but its performance remains inferior to that of capability-encoder-based ones.

\textbf{Ablation Studies: Explore Generalization in Multimodal Scenarios.} Most multimodal LLMs (MLLMs) are derived from input-extending LLMs. Multimodal \textsc{Model-SAT} is built on the Wings~\cite{zhang2024wingslearningmultimodalllms} training architecture, integrating model representation embeddings with visual ones. It maintains strong performance in multimodal scenarios, achieving optimal average performance across MMMU-VAL, AI2D-TEST, and CCBench datasets.

\section{Conclusion}

This paper proposes a novel model routing paradigm called \textit{capability instruction tuning} with instruction-level model routing. It constructs a capability instruction comprising capabilities, instructions, and inquiry prompts to select the most suitable one. We present \textsc{Model-SAT}, featuring a capability encoder and lightweight LLM. It selects models without inference overhead of candidates and quickly adapts to new models. Model-SAT performs optimally across five proposed LLM routing settings and its multimodal extension.