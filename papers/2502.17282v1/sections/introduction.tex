\section{Introduction}
Large Language Models (LLMs)~\cite{openai2020chatgpt,du2022glm,touvron2023llama,vicuna2023,jiang2023mistral} rapidly evolve, demonstrating near-human general capabilities, especially in understanding, reasoning, and creative tasks related to instruction-response scenarios. Recent advancements have even enabled these LLMs to be trained in multilingual~\cite{yang2024qwen2technicalreport,dubey2024llama3herdmodels}, multidomain~\cite{DBLP:journals/corr/abs-2406-04614,yang2024qwen2technicalreport}, and multimodal~\cite{chen2015microsoft,chen2023sharegpt4v,reid2024gemini1_5} environments, allowing them to tackle complex instructions such as ``What is the relationship between Fourier series and Hilbert space?'' or to interpret images by identifying, ``What are the basis vectors of the Hilbert space?''

The rise of LLMs and their extensions has incredibly energized community applications. However, achieving more comprehensive capabilities often requires LLMs of a larger scale. According to the Open LLM Leaderboard~\cite{myrzakhan2024openllmleaderboard}, 60\% of the top 50 LLMs have around 70 billion (B) parameters or more, with only three LLMs under 10B. Additionally, some closed-source LLMs consistently dominate performance rankings over extended periods.
Consequently, optimizing LLM applications often hinges on substantial computational resources or costly token purchases.
A natural idea arises: Can we utilize multiple smaller LLMs, which are more resource-friendly and have below one-tenth of the parameters of their larger counterparts, to achieve performance comparable to gigantic LLMs while maintaining low inference costs?

\begin{figure}[t]
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.42\textwidth]{figures/motivation.pdf}
    \caption{\textbf{Illustration of Coverage Observation}: The combined capabilities of the earlier-released model zoo effectively address most of the instructions that GPT-4o excels at. The union of samples managed accurately by Phi-1, ChatGLM2, and Zephyr covers 80\% of GPT-4o's correct instructions. The smaller-scale model zoo can enhance overall performance by selecting a suitable model for each instruction.}
    \label{fig:motivation}
    \vspace{-13pt}
\end{figure}

In the experiments, we find that the combined capability of some smaller-scale LLMs, despite their lower overall performance, can address most of the instructions that larger LLMs excel at. As shown in Figure~\ref{fig:motivation}, on the Massive Multitask Language Understanding (MMLU)~\cite{mmlu} benchmark, the Phi-1 LLM with 1.3B performs nearly 50\% worse than GPT-4o. However, it exhibits similar effectiveness to GPT-4o in the \texttt{high} \texttt{school} \texttt{mathematics} category.
Moreover, we create an early-access LLM zoo that includes Phi-1~\cite{DBLP:journals/corr/abs-2306-11644} and four 7B LLMs, which were released a year earlier than GPT-4o and exhibit an approximately 30\% performance gap compared to GPT-4o.
However, the combined accurate responses from this zoo cover 90\% of which GPT-4o handles correctly and address nearly 80\% with which GPT-4o struggles. By strategically assigning instructions to the suitable LLM in the zoo, there is potential to exceed GPT-4o's performance by 15\%.
From this phenomenon, the model routing for each instruction enhances performance with seamless LLM transitions and minimal inference costs, all without user awareness.

The key to the proposed instruction-level model routing is to efficiently identify the optimal model from a vast pool of options, without prior access to the potential candidates' inference outputs~\cite{DBLP:journals/corr/abs-2311-06720,bge_embedding} or the target task's ground truth~\cite{you2022ranking,DBLP:conf/cvpr/PandyAUFM22}.
In this paper, we introduce \textsc{Model-SAT}: \textbf{Model} \textbf{S}election with \textbf{A}ptitude \textbf{T}est. Our approach leverages 50 core 20-shot tasks, where the model test result represents model capability. By learning the generalization relationships between the capability representations of the candidate models and the instructions to be assigned, we can select the most suitable model across various repositories and target instructions.

Driven by the model capability representation, the \textsc{Model-SAT} framework establishes a novel paradigm, denoted as capability instruction tuning. Capability instructions consist of a capability representation, a user instruction, and a prompt to probe whether the model can perform that instruction. Using extensive historical performance data, capability instruction tuning learns an implicit relationship between core capability representations and unseen instructions. Moreover, it delves deeper into understanding the mapping between the capabilities' performance and the instructions' semantic distribution.
This intuition comes from the observation that individuals who perform well in the mathematical sections of the college admission SAT in the United States often pursue careers that involve logical reasoning. Capability instruction tuning aims to equip the model with a lightweight standardized guide to assess its effectiveness in handling future instructions.

Specifically, we combine model capability representation with positive and negative training instructions regarding current model performance, yielding statements like, ``The model achieves accuracy 85\% on the task of 'Mathematics, Geometry, ...'. Instruction: ..., Predict whether the model can handle ...''.
To align the performance distribution inherent in model representation to the instruction semantic, we are the first to incorporate a capability encoder and extend the input of a lightweight LLM to include capability representation. The end-to-end \textsc{Model-SAT} functions as a model router that outputs the probabilities indicating which models will likely excel at specific instructions.

Additionally, we establish several comprehensive benchmarks for model routing of LLMs and their extensions. Our benchmarks cover a range of model zoos, such as (\textbf{1}) smaller-scale, weaker ones, (\textbf{2}) mixed-scale options, and (\textbf{3}) high-performance larger-scale LLMs. Furthermore, we expand the model routing to include multimodal LLM-instruction settings.
\textsc{Model-SAT} achieve significantly improved overall performance across model zoos without incurring any inference overhead, comparable to the performance levels of larger-scale LLMs.
Notably, the capability instruction tuning maintains the model representation generalization to unseen data. The new LLM can quickly develop effective model representations after just a few inferences (only on 50 x 20-shot tasks). In light of practical routing scenarios with the emergence of new-version LLMs, we establish 60 incremental routing scenarios that impose higher routing speed and overhead requirements. Throughout these settings, \textsc{Model-SAT} consistently demonstrates superior performance.

In summary, our contributions are:
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
\item \textbf{A novel paradigm: capability instruction tuning}, where model representation with efficient aptitude tests and instructions create capability instructions for high-performance-driven instruction-level model routing.
\item \textbf{\textsc{Model-SAT} framework}, features a model capability encoder and a lightweight LLM to end-to-end learn the router via various model capability representations.
\item \textbf{Comprehensive model routing benchmarks for LLMs and their extensions}, covering five LLM zoo setups with multimodal scenarios, as well as simulating 60 incremental-released model routers to ensure quick adaptation to unseen data and new LLMs.
\item \textbf{An open-source, deployable model routing toolkit} that applies model routing techniques to any model zoo, enhancing performance while remaining unaware of users with acceptable routing delays.
\end{itemize}
