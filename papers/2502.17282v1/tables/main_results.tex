
\begin{table*}[t]
\small
\vspace{-10pt}
\centering
\setlength\tabcolsep{1.7pt}
\begin{tabular}{ p{2.6em} p{6.2em}<{\centering} p{4.8em}<{\centering} p{3.7em}<{\centering}  p{3.7em}<{\centering}  p{3.7em}<{\centering}  p{3.7em}<{\centering}  p{3.7em}<{\centering}  p{3.7em}<{\centering}  p{3.7em}<{\centering}  p{3.7em}<{\centering}}
\toprule
\multirow{2}{*}{\textbf{Method}} & & \multirow{2}{*}{\textbf{\#Params}} & \multicolumn{2}{c}{\textbf{In-Domain}} & \multicolumn{5}{c}{\textbf{Out-of-Domain}} & \multirow{2}{*}{\textbf{Mean}} \\
& & & MMLU & WinoG. & ARC-C & BoolQ & TruthfulQA & \, MRPC & MNLI & \\
\midrule
& & & & & \multicolumn{6}{l}{{\em \quad \quad \, Smaller-scale LLMs (<10B)}} \\
\noalign{\vskip 0.7ex}
\cline{6-9}
\noalign{\vskip 0.7ex}
% \multicolumn{2}{l}{Phi-1{\footnotesize~\cite{DBLP:journals/corr/abs-2306-11644}}} & 1.3B & 25.84 & 48.38 & 20.65 & 52.78 & 41.59 & 68.38 & 38.36 & 45.65 \\
% \multicolumn{2}{l}{BLOOMZ{\footnotesize~\cite{DBLP:conf/acl/MuennighoffWSRB23}}} & 3.0B & 33.31 & 58.25 & 35.41 & 68.72 & 40.35 & 68.38 & 58.90 & 54.04 \\
% \multicolumn{2}{l}{Zephyr$_{\text{\ Alpha}}${\footnotesize~\cite{DBLP:journals/corr/abs-2310-16944}}} & 7.2B & 59.79 & 78.30 & 61.86 & 84.22 & 56.05 & 69.61 & 56.73 & 66.97 \\ 
\multicolumn{2}{l}{InternLM2.5{\footnotesize~\cite{cai2024internlm2}}} & 7.7B & 69.88 & 81.22 & 60.75 & 70.43 & 54.56 & 68.38 & 60.68 & 66.89 \\ 
\multicolumn{2}{l}{Meta-Llama-3$_{\text{\ Instruct}}${\footnotesize~\cite{touvron2023llama2}}} & 8.0B & 65.59 & 75.45 & 62.12 & 76.76 & 51.63 & 68.38 & 55.82 & 65.62 \\ 
\multicolumn{2}{l}{Qwen2$_{\text{\ Instruct}}${\footnotesize~\cite{yang2024qwen2technicalreport}}} & 7.6B & 69.13 & 74.11 & 61.43 & 82.57 & 55.49 & 78.92 & 54.96 & \underline{68.19} \\ 
\multicolumn{2}{l}{GLM-4{\footnotesize~\cite{DBLP:journals/corr/abs-2406-12793}}} & 9.4B & 69.28 & 80.82 & 66.13 & 84.77 & 59.32 & 78.92 & 40.73 & 68.65 \\
\multicolumn{2}{l}{Phi-3$_{\text{\ Small-128K}}${\footnotesize~\cite{DBLP:journals/corr/abs-2404-14219}}} & 7.4B & 75.90 & 77.11 & 71.08 & 86.70 & 64.62 & 75.98 & 46.82 & 71.38 \\
\multicolumn{2}{l}{Best-Performing among the five above} & xB & 75.90 & 81.22 & 71.08 & 86.70 & 64.62 & 78.92 & 60.68 & 74.16\\
\noalign{\vskip 0.7ex}
& & & \multicolumn{8}{l}{\quad \quad \quad \quad \quad { \textbf{LLM Selection} on \textit{Smaller-scale} LLMs (5 models)}} \\
\noalign{\vskip 0.7ex}
\cline{6-9}
\noalign{\vskip 0.7ex}
\multicolumn{2}{l}{{Random Selection}} & xB & 70.11 & 77.66 & 64.59 & 79.94 & 57.58 & 72.79 & 51.54 & 67.74\\
\rowcolor{mygreyr}
\multicolumn{2}{l}
{Cappy{\footnotesize~\cite{DBLP:journals/corr/abs-2311-06720}}} & $M\cdot$xB & 69.53 & 78.06 & 63.99 & 81.10 & 57.77 & 74.75 & 53.17 & 68.34 \\
\rowcolor{mygreyr2}
\multicolumn{2}{l}{BGE$_{\text{\ Large}}${\footnotesize~\cite{bge_embedding}}} & (0.3+$M$)$\cdot$xB & 71.68 & 78.53 & 66.38 & 82.48 & 61.44 & 73.28 & 55.39 & 69.88 \\
\rowcolor{mygreyr4}
\multicolumn{2}{l}{GTE$_{\text{\ Large}}${\footnotesize~\cite{zhang2024mgte}}} & (1.8+$M$)$\cdot$xB & 72.02 & 79.32 & 68.09 & 83.73 & 59.61 & 75.74 & 56.16 & 70.67\\
\rowcolor{myred}
\multicolumn{2}{l}{\textsc{Model-SAT} \textbf{(Ours)}} & $M\cdot$4.3B+xB & 79.86 & 82.24 & 72.53 & 86.73 & 65.12 & 79.66 & 60.83 & 75.28\\
\bottomrule
\noalign{\vskip 0.7ex}
\noalign{\vskip 0.7ex}
& & & & & \multicolumn{6}{l}{{\em \quad \quad Larger-Scale LLMs (10B\textasciitilde50B)}} \\
\noalign{\vskip 0.7ex}
\cline{6-9}
\noalign{\vskip 0.7ex}
\multicolumn{2}{l}{Phi-3$_{\text{\ Medium-128K}}${\footnotesize~\cite{DBLP:journals/corr/abs-2404-14219}}} & 14B & 76.63 & 74.35 & 66.49 & 86.30 & 54.54 & 78.92 & 59.42 & 70.95\\
\multicolumn{2}{l}{Yi-1.5$_{\text{\ Chat}}${\footnotesize~\cite{DBLP:journals/corr/abs-2403-04652}}} & 34B & 77.15 & 81.47 & 70.62 & 87.84 & 62.02 & 80.88 & 61.56 & 74.50 \\
\multicolumn{2}{l}{Meta-Llama-3$_{\text{\ Instruct}}${\footnotesize~\cite{touvron2023llama2}}} & 70B & 79.89 & 82.62 & 71.67 & 93.61 & 61.83 & 83.58 & 65.07 &76.90 \\ 
\multicolumn{2}{l}{Qwen2$_{\text{\ Instruct}}${\footnotesize~\cite{yang2024qwen2technicalreport}}} & 72B & \underline{83.79} & 84.41 & 68.62 & \underline{94.90} & 54.85 & \underline{84.31} & 66.95 & 76.83\\ 
\multicolumn{2}{l}{Mixtral-8x22B$_{\text{\ Instruct-v0.1}}${\footnotesize~\cite{jiang2024mixtral}}} & 140B & 77.63 & \underline{85.25} & \underline{72.68} & 92.71 & \underline{68.19} & 81.13 & \underline{67.70} & \underline{77.90}\\
\midrule
\noalign{\vskip 0.7ex}
& & & & \multicolumn{6}{l}{\quad \, \textbf{Capability Instruction Tuning} w/o Inference Overhead} \\
\noalign{\vskip 0.7ex}
\cline{5-10}
\noalign{\vskip 0.7ex}
\rowcolor{myred}
& \multicolumn{1}{l}{{\scriptsize Smaller-Scale LLM Zoo}} &  & 79.86 & 82.24 & 72.53 & 86.73 & 65.12 & 79.66 & 60.83 & 75.28\\
\rowcolor{myred1}
& \multicolumn{1}{l}{{Smaller-Mixed LLM Zoo}} &  & 78.60 & 82.08 & 72.01 & 86.48 & 64.50 & 78.19 & 60.72 & 74.65\\
\rowcolor{myred2}
\textbf{Ours} & \multicolumn{1}{l}{{Middle-Mixed LLM Zoo}} & $M\cdot$4.3B+xB & 79.97 & 83.03 & 72.69 & 87.80 & 64.87 & 83.82 & 61.80 & 76.28\\
\rowcolor{myred3}
& \multicolumn{1}{l}{{Larger-Mixed LLM Zoo}} & & 84.16 & 86.27 & 73.21 & 93.94 & 69.16 & 85.54 & 67.92 & 80.03\\
\rowcolor{myred4}
& \multicolumn{1}{l}{\textbf{High-Performance LLM Zoo}} & & \textbf{85.64} & \textbf{87.85} & \textbf{73.63} & \textbf{95.02} & \textbf{69.40} & \textbf{88.24} & \textbf{68.39} & \textbf{81.17}\\
\bottomrule
\end{tabular}
\caption{\textbf{A Comprehensive Performance Evaluation}: Covering smaller-scale, high-performance giant LLMs, and a mixed LLM zoo of small, medium, and large levels. Model-SAT performs instruction-level model selection, consistently maintaining efficient and precise results that outperform the optimal one in the LLM zoo. Bold is the best, and underlined is the second-best.}
\vspace{-10pt}
\label{tab:m_res}
\end{table*}
