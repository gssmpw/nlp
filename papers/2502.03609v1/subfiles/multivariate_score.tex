\section{Kantorovich Conformal Prediction }

\subsection{Multi-Output Conformal Prediction}
We suppose that $\mathbb{P}$ is only available through a finite samples and consider the \textit{discrete} transport map
$$
T_{n+1} : (Z_i)_{i \in [n+1]} \to (U_i)_{i \in [n+1]}
$$
which can be obtained by solving the optimal assignment problem, which seeks to minimize the total transport cost between the empirical distributions $\mathbb{P}_{n+1}$ and $\mathbb{U}_{n+1}$:
\begin{equation}\label{eq:empirical_transport_map}
T_{n+1} \in \argmin_{T \in \mathcal{T}} \sum_{i=1}^{n+1} \|Z_i - T(Z_i)\|^2,
\end{equation}
where $\mathcal{T}$ is the set of bijections mapping the observed sample $(Z_i)_{i \in [n+1]}$ to the target grid $(U_i)_{i \in [n+1]}$. 

\begin{definition}    
    Let $(Z_1, \ldots, Z_n, Z_{n+1})$ be a sequence of exchangeable variables in $\mathbb{R}^d$ that follow a common distribution $\mathbb{P}$. The discrete center-outward distribution $T_{n+1}$ is the transport map pushing forward $\mathbb{P}_{n+1}$ to $\mathbb{U}_{n+1}$.
\end{definition}

Following \cite{hallin2021}, we begin by constructing the target discritbution $\mathbb{U}_{n+1}$ as a discretized version of a spherical uniform distribution. It is defined such that the total number of points $n + 1 = n_R n_S + n_o$, where $n_o$ points are at the origin:\looseness=-1
\begin{itemize}
    \item $n_S$ unit vectors $\mathbf{u}_1, \ldots, \mathbf{u}_{n_S}$ are uniform on the sphere.
    \item $n_R$ radius are regularly spaced as $\left\{\frac{1}{n_R}, \frac{2}{n_R}, \ldots, 1\right\}$.
\end{itemize}
The grid discretizes the sphere into layers of concentric shells, with each shell containing $n_S$ equally spaced points along the directions determined by the unit vectors. The discrete spherical uniform distribution places equal mass over each points of the grid, with $n_o/(n+1)$ mass on the origin and $1/(n+1)$ on the remaining points.
This ensures isotropic sampling at fixed radius onto $[0,1]$.

By definition of target distribution $\mathbb{U}_{n+1}$, it holds 
\begin{equation}\label{eq:distribution_empirical_transport}
\|T_{n+1}(Z_{n+1})\| \sim \mathbb{U}_{n+1} \left\{0, \frac{1}{n_R}, \frac{2}{n_R}, \ldots, 1\right\}.
\end{equation}


In order to define an empirical quantile region as \Cref{eq:exact_continuous_coverage}, we need an extrapolation $\bar{T}_{n+1}$ of $T_{n+1}$ out of the samples $(Z_i)_{i\in [n+1]}$. By definition of such maps $$\|\bar{T}_{n+1}(Z_{n+1})\|= \|T_{n+1}(Z_{n+1})\|$$ is still uniformly distributed. With an appropriate choice of radius $r_{\alpha, n+1}$, the empirical quantile region can be defined 
$$
\mathcal{R}_{\alpha, n+1} = \{z \in \mathbb{R}^d: \|\bar{T}_{n+1}(z)\| \leq r_{\alpha, n+1}\}.
$$
When working with such finite samples $Z_1, \ldots, Z_n, Z_{n+1}$, and considering the asymptotic regime \citep{chewi2024statistical, hallin2021}, the empirical source distribution $\mathbb{P}_{n+1}$ converges to the true distribution $\mathbb{P}$ and the empirical transport map $\bar T_{n+1}$ converges to the true transport map $T^\star$. As such, with the choice $r_{\alpha, n+1}= 1-\alpha$, one can expect that
$
\mathbb{P} \left(Z \in \mathcal{R}_{\alpha, n+1}\right)  \approx 1 - \alpha \text{ when } n \text{ is large}.
$

However, the core point of conformal prediction methodology is to go beyond asymptotic results or regularity assumptions about the data distribution. The following result show how to select a radius preserving the coverage with respect to the ground-truth distribution such as in \Cref{eq:transport_oracle_coverage}. 
\begin{proposition}
Given $n$ discrete sample points distributed over a sphere with radius $\{0, \frac{1}{n_R}, \frac{2}{n_R}, \ldots, 1\}$ and directions uniformly sampled on the sphere, the smallest radius to obtain a coverage $(1-\alpha)$ is 
determined by 
$$r_{\alpha, n+1} = \frac{j_\alpha}{n_R} \text{ where }
j_\alpha = \left\lceil \frac{(n+1) (1 - \alpha) - n_o}{n_S} \right\rceil,
$$
where $n_S$ is the number of directions, $n_R$ is the number of radius, and $n_o$ is the number of copies of the origin.
\end{proposition}

The corresponding conformal prediction set is obtained as:
\begin{equation}\label{eq:full_transport_otcp}
\{y \in \mathcal{Y}: \|\bar T_{n+1} \circ S(X_{n+1}, y)\| \leq r_{\alpha, n+1}\}.
\end{equation}

\begin{remark}[Computational Issues] \label{rm:computationally_infeasible}
While appealing, the previous result has notable computational limitations.
At every new candidate $y \in \mathcal{Y}$, the empirical transport map must be recomputed which might be untractable. Moreover, the coverage guarantee does not hold if the transport map is computed solely on a hold-out independent dataset, as it is usually done in split conformal prediction.
Plus, for computational efficiency, the empirical entropic map cannot be directly leveraged, since the target values would no longer follow a uniform distribution, as described in \Cref{eq:distribution_empirical_transport}.    
\end{remark}
To address these challenges, we propose two simple approaches in the following section.

\subsection{Optimal Transport Merging}

We introduce optimal transport merging, a procedure that reduces any vector-valued score $S(x, y) \in \mathbb{R}^d$ to a suitable 1D score using OT. We redefine the non-conformity score function of an observation as 
\begin{equation}\label{eq:otcp}
S_{\rm{OT-CP}}(x, y) =  \|T^\star \circ S(x, y)\|\end{equation}
where $T^\star$ is the optimal \citet{Bre91} map that pushes the distribution of vector-valued scores onto a uniform ball distribution $\mathbb{U}$ of the same dimension.
This approach ultimately relies on the natural ordering of the real line, making it possible to directly apply one-dimensional conformal prediction methods to the sequence of transformed scores 
$$Z_i = \|S_{\rm{OT-CP}}(X_i, Y_i)\| \text{ for } i \in [n+1] .$$

In practice, $T^\star$ can be replaced by any approximation $\hat T$ that preserves the permutation invariance of the score function.
The resulting conformal prediction set, \OTCP\, is
\begin{align*}
\mathcal{R}_{\mathrm{OT-CP}}(X_{n+1}, \alpha) &= \mathcal{R}_\alpha(\hat{T}, X_{n+1})
\end{align*}
with respect to a given transport map $\hat{T}$, and where
$$
\mathcal{R}_\alpha(\hat{T}, x) = 
\big\{y \in \mathcal{Y}: F_n(\|S_{\mathrm{OT-CP}}(x, y)\|_2)\leq 1-\alpha\big\}.
$$
have a coverage $(1-\alpha)$, where $F_n$ is empirical (univariate) cumulative distribution function of the observed scores $$\big\{\|S_{\rm{OT-CP}}(X_1, Y_1)\|, \ldots, \|S_{\rm{OT-CP}}(X_n, Y_n)\|\big\}.$$

\Cref{prop:Univariate_Conformal_prediction_Coverage} directly implies
$$
\mathbb{P}(Y_{n+1} \in \mathcal{R}_{\mathrm{OT-CP}}(X_{n+1})) \geq 1-\alpha.
$$

\begin{remark}
Our proposed conformal prediction framework \OTCP\, with optimal transport merging score function generalizes the \MergeCP\, approaches. More specifically, under the additional assumption that we are transporting a source Gaussian (resp. uniform) distribution to a target Gaussian (resp. uniform) distribution, the transport map is affine \cite{gelbrich1990formula, muzellec2018generalizing} with a positive definite linear map term. This results in \Cref{eq:otcp} being equivalent to the Mahalanobis distance.
\end{remark}





\subsection{Coverage Guarantees under Approximations}\label{sec:coverage}

When dealing with high-dimensional data or complex distributions, it is essential to find computationally feasible methods to approximate the optimal transport map $T^\star$ with a map $\hat{T}$. In practical applications, we will rely on empirical approximations of the \citet{Bre91} map using finite samples. Note that this approach may encouter a few statistical roadblocks, as such estimators are significantly hindered by the curse of dimensionality \citep{chewi2024statistical}.
However, conformal prediction allows us to maintain a coverage level irrespective of sample size limitations. We defer the presentation of this practical approach to section~\ref{subsec:entropic} and focus first on coverage guarantees.

\paragraph{Coverages of Approximated Quantile Region} \quad \\
Let us assume an arbitrary approximation $\hat T$ of the \citet{Bre91} map and define the corresponding quantile region as
$$
\mathcal{R}\big(\hat T, r\big) = \{z \in \mathbb{R}^d: \|\hat T(z)\| \leq r\},
$$
The coverage in \Cref{eq:transport_oracle_coverage} is not automatically maintained since $ \hat{\mathbb{U}} := \hat{T}_\# \mathbb{P} $ may not coincide with $ \mathbb{U} $. As a result, the validity of the approximated quantile region may be compromised unless we can control the magnitude of the error $ \|\hat{\mathbb{U}} - \mathbb{U}\| $, which requires additional regularity assumptions.  
In its standard formulation, conformal prediction relies on an empirical setting and does not directly apply to the continuous case, and hence does not provide a solution for calibrating entropic quantile regions. However, a careful inspection of the 1D case reveals that understanding the distribution of the probability integral transform is key:
\begin{itemize}
    \item $\mathbb{U}\left(\big\{0, \frac1n, \frac12, \ldots, 1\big\}\right) \sim F_n(Z) \neq F(Z) \sim \mathbb{U}(0, 1)$ .
\end{itemize}
Instead of relying on an analysis of approximation error to quantify the deviation $|F_n - F|$ under certain regularity conditions, conformal prediction fully characterizes the distribution of the probability integral transform and calibrates the radius of the quantile region accordingly.
We follow this idea and note that by definition, we have
$$
\mathbb{P}(\mathcal{R}(\hat T, r)) = \mathbb{P}(\|\hat T(z)\| \leq r) = \hat{ \mathbb{U}} (B(0, r)).
$$
Instead of relying on $\hat{ \mathbb{U}} \approx \mathbb{U}$, we define
\begin{equation}\label{eq:generic_radius}
r_\alpha(\hat T, \mathbb{P}) = \inf\{r: \hat{ \mathbb{U}} (B(0, r)) \geq 1-\alpha\}
\end{equation}
that naturally leads to a desired coverage with the approximated transported map. For $\hat{r}_\alpha = r_\alpha(\hat T, \mathbb{P})$, it holds
$$\mathbb{P}\left(Z \in \mathcal{R}(\hat T, \hat{r}_\alpha)\right) \geq 1-\alpha.$$

By extension, a quantile region of the vector-valued score $Z = S(X, Y) \in \mathbb{R}^d$ of a prediction model $\hat y$ provides an uncertainty set for the response of a given input $X$, with the prescribed coverage $(1-\alpha)$ expressed as
\begin{equation*}
\widehat{\mathcal{R}}_{\alpha}(X) = \big\{y \in \mathcal{Y}: \|\hat T \circ S(X, y)\| \leq \hat{r}_\alpha\big\}.
\end{equation*}
\begin{equation}\label{eq:transport_oracle_coverage} 
\mathbb{P}(Y \in \widehat{\mathcal{R}}_{\alpha}(X)) \geq 1 - \alpha.
\end{equation}

We give the finite sample analogy of \Cref{eq:transport_oracle_coverage}, which provides a coverage guarantee even when the transport map is an approximation obtained using both entropic regularization and finite sample data e.g in \Cref{eq:empirical_entropic_map}.\newline
Given such an approximated map $\hat T_{n+1}$ and applying
and the empirical radius $\hat r_{\alpha, n+1} = r_\alpha(\hat T_{n+1}, \mathbb{P}_{n+1})$, it holds\looseness=-1
$$
\mathbb{P}_{n+1}(Z_{n+1} \in \mathcal{R}(\hat T_{n+1}, \hat r_{\alpha, n+1})) \geq 1-\alpha.
$$
However, this is \emph{only} an empirical coverage statement:
$$
\frac{1}{n+1}\sum_{i=1}^{n+1} \mathds{1}\{Z_i \in \mathcal{R}(\hat T_{n+1}, \hat r_{\alpha, n+1})\} \geq 1 - \alpha
$$
which does not imply coverage wrt $\mathbb{P}$ unless $n \to \infty$. The following result shows how to obtain finite sample validity.

\begin{lemma}[Coverage of Empirical Quantile Region]\label{lm:coverage_empirical_measure}
Let $Z_1, \ldots, Z_n, Z_{n+1}$ be a sequence of exchangeable variables in $\mathbb{R}^d$, then,
$
   \mathbb{P}(Z_{n+1} \in \widehat {\mathcal{R}}_{\alpha, n+1}) \geq 1-\alpha,
$
where, for simplicity, we denoted the approximated empirical quantile region as $\widehat {\mathcal{R}}_{\alpha, n+1} = \mathcal{R}(\hat T_{n+1}, \hat r_{\alpha, n+1})$. 
\end{lemma}






This can be directly applied to obtain conformal prediction set for vector-valued non-conformity score functions 
$Z_i = S(X_i, Y_i) \in \mathbb{R}^d$ for $i$ in $[n+1]$ in \Cref{lm:coverage_empirical_measure}.

\begin{proposition}\label{prop:Vector_PIT_Guarantee}
The conformal prediction set is defined as
\begin{align*}
\widehat{\mathcal{R}}_{\alpha, n+1}(X_{n+1}) &= \left\{y \in \mathcal{Y} : \|\hat T \circ S(X_{n+1}, y)\| \leq \hat{r}_{\alpha, n+1}\right\} 
\end{align*}
with  
$\hat{r}_{\alpha,n+1} = \inf\big\{r \geq 0: \hat{\mathbb{U}}_{n+1} (B(0, r)) \geq 1-\alpha\big\}$.
It satisfies a distribution-free finite sample coverage guarantee
\begin{equation}\label{eq:valid_empirical_radius}
\mathbb{P}\left(Y_{n+1} \in \widehat{\mathcal{R}}_{\alpha, n+1}(X_{n+1})\right) \geq 1 - \alpha.
\end{equation}
\end{proposition}

Approaches relying on vector-valued probability integral transform, e.g., by leveraging Copulas, have been recently explored 
\cite{messoudi2021copula, park2024semiparametric} and concluded that loss of coverage can occur when the estimated copula of the scores deviates from the true copula and thus does not
formally guarantee finite-sample validity. To our knowledge, \Cref{prop:Vector_PIT_Guarantee} provides the first calibration guarantee for such confidence regions without assumptions on the distribution, for any approximation map $\hat T$.
























\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/fig1_small_region_size.pdf}\vskip-.3cm
    \caption{We report the mean and standard error of the region size across 10 different seeds. For \MCP, we use $300$ samples to compute the conditional mean, and for \OTCP, we use $\varepsilon = 0.1$ and $2^{15}=32768$ points in the uniform target measure. Overall, \OTCP\ displays smaller region size than other baselines (13 out of 17 datasets). The output dimension $d$ of each dataset is provided next to its name.}
    \label{fig:small-region}
\end{figure*}


\subsection{Implementation with the Entropic Map}\label{subsec:entropic}

We assume access to two families of samples: residuals $(z_1,\dots ,z_n)$, and a discretization of the uniform grid on the sphere, $(u_1, \dots, u_m)$, with sizes $n,m$ that will be usually different, $n \ne m$.  Learning the entropic map estimator as in \Cref{subsec:entropic} requires running the \citet{Sinkhorn64} algorithm for a given regularization $\varepsilon$ on a $n\times m$ cost matrix. At test time, for each evaluation, computing the weights in \Cref{eq:gibbs} requires computing the distances of a new score $z$ to the uniform grid. The complexity is therefore $O(nm)$ when training the map and conformalizing its norms, and $O(m)$ to transport a conformity score for a given $y$. \looseness=-1

\textbf{Sampling on the sphere.}
As mentioned by \citet{hallin2021}, it is preferable to sample the uniform measure $\mathbb{U}_d$ with diverse samples. This can be achieved using stratified sampling on radii lengths and low-discrepancy samples picked on the sphere. We borrow inspiration from the review provided in \citep{nguyenquasi} and pick their \textit{Gaussian based} mapping approach \citep{basu2016quasi}. This consists %
of mapping a low-discrepancy sequence $w_1,\ldots,w_L$ on $[0,1]^d$ to a potentially low-discrepancy sequence $\theta_1,\ldots,\theta_L$ on $\mathbb{S}^{d-1}$ through the mapping $\theta= \Phi^{-1}(w)/\|\Phi^{-1}(w)\|_2$, where $\Phi^{-1}$ is the inverse CDF of $\mathcal{N}(0,1)$ applied entry-wise.\looseness=-1






