\section{Background}



\subsection{Univariate Conformal Prediction}
\label{subsec:Univariate_Conformal_Prediction}

We recall the basics of conformal prediction based on real-valued score function and refer to the recent tutorials \citep{Shafer_Vovk08, angelopoulos2021gentle}. In the following, we denote $[n]:=\{1,\dots, n\}$.

For a real-valued random variable $Z$, it is common to construct an interval $[a,b]$, within which it is expected to fall, as\looseness=-1
\begin{equation}\label{eq:true_confidence_set}
\mathcal{R}_\alpha = \{z \in \mathbb{R}: F(z) \in [a, b]\}
\end{equation}
This is based on the probability integral transform that states that the cumulative distribution function $F$ maps variables to uniform distribution, i.e., 
$\mathbb{P}(F(Z) \in [a, b]) = \mathbb{U}([a, b]).$
To guarantee a $(1-\alpha)$ uncertainty region, it suffices to choose $a$ and $b$ such that $ \mathbb{U}([a, b]) \geq 1-\alpha$ which implies
\begin{equation}\label{eq:coverage_exact_univariate_quantile_region}
    \mathbb{P}\left(Z \in \mathcal{R}_\alpha\right) \geq 1-\alpha.
\end{equation}
Applying it to a real-valued score $Z = S(X, Y)$ of the prediction model $\hat y$, an uncertainty set for the response of a given a input $X$ can be expressed as
\begin{equation}\label{eq:exact_uq}
\mathcal{R}_\alpha(X) = \big\{y \in \mathcal{Y}: F\circ S(X, y) \in [a, b]\big\}.
\end{equation}
However, this result is typically not directly usable, as the ground-truth distribution $F$ is unknown and must be approximated empirically with $F_n$ using finite samples of data. When the sample size goes to infinity, one expects to recover \Cref{eq:coverage_exact_univariate_quantile_region}.
The following result provides the tool to obtain the finite sample version \cite{Shafer_Vovk08}.

\begin{lemma}\label{lm:PIT_onedim}
If $Z_1, \dots, Z_n, Z$ be a sequence of real-valued exchangeable random variables, then it holds
\begin{align*}
 F_n(Z) &\sim \mathbb{U}\left\{0, \frac1n, \frac2n, \ldots, 1\right\} \\
\mathbb{P}(F_n(Z) \in [a, b]) &= \mathbb{U}_{n+1}([a, b])
= \frac{\lfloor n b\rfloor - \lceil n a\rceil + 1}{n+1} .
\end{align*}
\end{lemma}
By choosing any $a, b$ such that $\mathbb{U}_{n+1}([a, b]) \geq 1 - \alpha$, \Cref{lm:PIT_onedim} guarantees  a coverage, that is at least equal to the prescribed level of uncertainty
\begin{equation*}
\mathbb{P}\left(Z \in \mathcal{R}_{\alpha, n}\right) \geq 1 - \alpha.
\end{equation*}
where, the uncertainty set $\mathcal{R}_{\alpha, n} = \mathcal{R}_{\alpha}(D_n)$ is defined based on observations $D_n = \{Z_1, \ldots, Z_n\}$ as:
\begin{equation}\label{eq:empirical_uq}
    \mathcal{R}_{\alpha, n} = \big\{z \in \mathbb{R} : F_n(z) \in [a, b]\big\} .
\end{equation}

In short, \Cref{eq:empirical_uq} is an empirical version of \Cref{eq:true_confidence_set} based on finite data samples that still preserves the coverage probability $(1-\alpha)$ and does not depend on the ground-truth distribution of the data.

Given data $D_n$,
a prediction model $\hat y$ and a new input $X_{n+1}$, one can build an uncertainty set for the unobserved output $Y_{n+1}$ by applying it to observed score functions. 

\begin{proposition}[Conformal Prediction Coverage]\label{prop:Univariate_Conformal_prediction_Coverage}
Consider $Z_i = S(X_i, Y_i)$ for $i$ in $[n]$ and $Z=S(X_{n+1}, Y_{n+1})$ in \Cref{lm:PIT_onedim}.
The conformal prediction set is defined as
\begin{align*}
\mathcal{R}_{\alpha, n}(X_{n+1}) &= \big\{y \in \mathcal{Y} : F_n \circ S(X_{n+1}, y) \in [a, b]\big\} 
\end{align*}
and satisfies a finite sample coverage guarantee
$$
\mathbb{P}\left(Y_{n+1} \in \mathcal{R}_{\alpha, n}(X_{n+1})\right) \geq 1 - \alpha.
$$
\end{proposition}
The conformal prediction coverage guarantee in \Cref{prop:Univariate_Conformal_prediction_Coverage} holds for the \emph{unknown} ground-truth distribution of the data $\mathbb{P}$, does not require quantifying the estimation error $|F_n - F|$, and is applicable to any prediction model $\hat y$ as long as it treats the data exchangeably, e.g., a pre-trained model independent of $D_n$.

Leveraging the quantile function $F_{n}^{-1}= Q_n$, and
by setting $a=0$ and $b=1-\alpha$, we have the usual description
\begin{align*}
\mathcal{R}_{\alpha, n}(X_{n+1}) = \big\{y \in \mathcal{Y} : S(X_{n+1}, y) \leq Q_n(1-\alpha) \big\} 
\end{align*}
namely the set of all possible responses whose score rank is smaller or equal to $\lceil (1-\alpha)(n+1) \rceil$ compared to the rankings of previously observed scores. For the absolute value difference score function, the CP set corresponds to 
$$\mathcal{R}_{\alpha, n}(X_{n+1}) = \big[\hat y(X_{n+1}) \pm Q_n(1-\alpha)\big].$$



\paragraph{Center-Outward View}
Another classical choice is $a=\frac{\alpha}{2}$ and $b=1-\frac{\alpha}{2}$. In that case, we have the usual confidence set that corresponds to a range of values that captures the central proportion with $\alpha/2$ of the data lying below $Q(\alpha/2)$ and $\alpha/2$ lying above $Q(1-\alpha/2)$.

Introducing the center-outward distribution of $Z$ as the function $T = 2 F - 1$ , the probability integral transform $T(Z)$ is uniform in the unit ball $[-1, 1]$.
This ensures a symmetric description of 
$
\mathcal{R}_\alpha = T^{-1}(B(0, 1-\alpha))$ around a central point such as the median $Q(1/2) = T^{-1}(0)$,
with the radius of the ball that corresponds to the desired confidence level of uncertainty. Similarly, we have the empirical center-outward distribution $T_{n} = 2 F_n - 1$ and
the center-outward view of the conformal prediction set follows as
\begin{align*}
\mathcal{R}_{\alpha, n}(X_{n+1}) &= \big\{y \in \mathcal{Y} : |T_{n} \circ S(X_{n+1}, y)| \leq 1-\alpha \big\} .
\end{align*}


If $Z$ follows a probability distribution $\mathbb{P}$, then the transformation $z \mapsto T(z)$ is mapping the source  distribution $\mathbb{P}$ to the uniform distribution $\mathbb{U}$ over a unit ball. In fact, it can be characterized as essentially the unique monotone increasing function such that $T(Z)$ is uniformly distributed. 

\subsection{Multivariate Conformal Prediction}
While many conformal methods exist for univariate prediction, we focus here on those applicable to \textit{multivariate} outputs.  As recalled in~\citep{dheur2025multioutputconformalregressionunified}, several alternative conformal prediction approaches have been proposed to tackle multivariate prediction problems. Some of these methods can directly operate using a simple predictor (e.g., a conditional mean) of the response $y$, while some may require stronger assumptions, such as requiring an estimator of the \textit{joint} probability density function between $x$ and $y$, or access to a generative model that mimics the \textit{conditional} distribution of $y$ given $x$) \cite{izbicki2022cd, wang2022probabilistic}. \looseness=-1

We restrict our attention to approaches that make no such assumption, reflecting our modeling choices for \OTCP. 

\MCP.
We will consider the template approach of \cite{zhou2024conformalized} to use classical CP by aggregating a score function computed on each of the $d$ outputs of the multivariate response. Given a conformity score \( s_i \) (to be defined next) for the \( i \)-th dimension, \citet{zhou2024conformalized} define the following aggregation rule:
\begin{equation}
    s_{\text{M-CP}}(x, y) = \max_{i \in [d]} s_i(x, y_i).
    \label{eq:simultaneous_score}
\end{equation}
As~\citep{dheur2025multioutputconformalregressionunified}, we will use \textit{conformalized quantile regression} \citep{romano2019conformalized} to define the score functions above, for each output \( i \in [d] \), where the conformity score is given by:
\[
s_i(x, y_i) = \max\{\hat{l}_i(x) - y_i, y_i - \hat{u}_i(x)\},
\]
with \( \hat{l}_i(x) \) and \( \hat{u}_i(x) \) representing the lower and upper conditional quantiles of \( Y_i|X=x\) at levels \( \alpha_l \) and \( \alpha_u \), respectively. In our experiments, we consider equal-tailed prediction intervals, where \( \alpha_l = \frac{\alpha}{2} \), \( \alpha_u = 1 - \frac{\alpha}{2} \), and \( \alpha \) denotes the miscoverage level. 

\MergeCP. An alternative approach is simply to use a squared Euclidean aggregation, 
$$
s(x, y) :=  \|\hat{y}(x) - y\|_2,
$$
where the choice of the norm (e.g., $\ell_1$, $\ell_2$, or $\ell_\infty$) depends on the desired sensitivity to errors across tasks. This approach reduces the multidimensional residual to a scalar conformity score, leveraging the natural ordering of real numbers. This simplification not only makes it straightforward to apply univariate conformal prediction methods, but also avoids the complexities of directly managing vector-valued scores in conformal prediction. A variant consists of applying a Mahalanobis norm~\citep{pmlr-v152-johnstone21a} in lieu of the squared Euclidean norm, using the covariance matrix $\Sigma$ estimated from the training data \citep{pmlr-v152-johnstone21a,pmlr-v230-katsios24a, henderson2024adaptive}, 
$$
s(x, y) :=  \|\Sigma^{-1/2}(\hat{y}(x) - y)\|_2,
$$




\subsection{Kantorovich Ranks}
A naive way to define ranks in multiple dimensions might be to measure how far each point is from the origin and then rank them by that distance. This breaks down if the distribution of the data is stretched or skewed in certain directions. To correct for this, \citet{hallin2021} developed a formal framework of center-outward distributions and quantiles, also called Kantorovich ranks~\citep{chernozhukov2017}, extending the familiar univariate concepts of ranks and quantiles into higher dimensions by building on elements of optimal transport theory.

\paragraph{Optimal Transport Map.} Let $\mu$  and $\nu$ be source and target probability measures on $\Omega \subset \mathbb{R}^d$. One can look for a map $T: \Omega \to \Omega$ that pushes forward $\mu$ to $\nu$ and minimizes the average transportation cost
\begin{equation}\label{eq:brenier}
T^\star \in \argmin_{T_\# \mu = \nu} \int_{\Omega} \|x - T(x)\|^2 \, d\mu(x).
\end{equation}
\citeauthor{Bre91}â€™s theorem states that if the source measure $\mu$ has a density, there exists a solution to \eqref{eq:brenier} that is the gradient of a convex function $\phi: \Omega \to \mathbb{R}$ such that $T^\star = \nabla \phi$.

In the one-dimensional case, the cumulative distribution function of a distribution $\mathbb{P}$ is the unique increasing function transporting it to the uniform distribution. This monotonicity property generalizes to higher dimensions through the gradient of a convex function $\nabla \phi$. Thus, one may view the optimal transport map in higher dimensions as a natural analog of the univariate cumulative distribution function: both represent a unique, monotone way to send one probability distribution onto another.

\begin{definition}
    The center-outward distribution of a random variable $Z \sim \mathbb{P}$ is defined as the optimal transport map $T = \nabla \phi$ that pushes $\mathbb{P}$ forward to the uniform distribution $\mathbb{U}$ on the unit ball $B(0,1)$.
    The rank of $Z$ is defined as $\mathrm{Rank}(Z) = \|T(Z)\|$, the distance from the origin.
\end{definition}

\paragraph{Quantile region} is an extension of quantiles to multiple dimensions to represent region in the sample space that contains a given proportion of probability mass. The quantile region at probability level $(1-\alpha) \in (0,1)$ can be defined as\looseness=-1
$$
\mathcal{R}_{\alpha} = \{z \in \mathbb{R}^d : \|T(z)\| \leq  1-\alpha\}.
$$
By definition of the spherical uniform distribution, we have $\|T(Z)\|$ is uniform on $(0,1)$ which implies
\begin{align}\label{eq:exact_continuous_coverage}
\mathbb{P}(Z \in \mathcal{R}_{\alpha}) = 1-\alpha.
\end{align}

\subsection{Entropic Map.}
A convenient estimator to approximate the \citeauthor{Bre91} map $T^\star$ from samples $(z_1,\dots, z_n)$ and $(u_1, \dots, u_m)$ is the entropic map~\citep{pooladian2021entropic}: Let $\varepsilon>0$ and write $K_{ij} = [\exp(-\|z_i-u_j\|^2/\varepsilon)]_{ij}$, the kernel matrix. Define,
\begin{equation}\label{eq:finitedual}
\!\!\!\bff^\star, \bg^\star = \argmax_{\bff\in\R^n,\bg\in\R^m} \langle\bff, \tfrac{\mathbf{1}_n}{n}\rangle + \langle\bg, \tfrac{\mathbf{1}_m}{m}\rangle  - \varepsilon \langle e^{\frac{\bff}{\varepsilon}}, K e^{\frac{\bg}{\varepsilon}}\rangle\,.
\end{equation}
The \Cref{eq:finitedual} is an unconstrained concave optimization problem known as the regularized OT problem in dual form~\citep[Prop.~4.4]{PeyCut19} and can be solved numerically with the \citeauthor{Sinkhorn64} algorithm~\citep{cuturi2013sinkhorn}. Equipped with these optimal vectors, one can define the maps, valid out of sample:
\begin{align}\label{eq:fdual}
f_\varepsilon(z)= \lse_\varepsilon([\|z-u_j\|^2 - \bg^\star_j]_j)\,,\\
\label{eq:gdual}
g_\varepsilon(u)= \lse_\varepsilon([\|z_i-u\|^2 - \bff^\star_i]_i)\,,
\end{align}
where for a vector $\bu$ or arbitrary size $s$ we define the log-sum-exp operator as $\lse_\varepsilon(\bu):= - \varepsilon \log (\tfrac{1}{s}\mathbf{1}_s^Te^{-\bu/\varepsilon})$. Using the \citet{Bre91} theorem, linking potential values to optimal map estimation, one obtains an estimator for $T^\star$:
\begin{equation}\label{eq:empirical_entropic_map}
T_{\varepsilon}(z) : = z - \nabla f_\varepsilon(z) = \sum_{j=1}^m p_{\,j}(z)u_j\,,
\end{equation}
where the weights depend on $z$ as:
\begin{equation}\label{eq:gibbs}
p_{\,j}(z):=\frac{\exp\left(- \left(\|z-u_j\|^2-\bg_j^\star\right)/\varepsilon\right)}{\sum_{k=1}^m \exp\left(- \left(\|z-u_k\|^2-\bg_k^\star\right)/\varepsilon\right)}\,.
\end{equation} 
Analogously to \eqref{eq:gibbs}, one can obtain an estimator for the inverse map $(T^\star)^{-1}$ as $T^{\text{inv}}_{\varepsilon}(u) : = \sum_{i=1}^n q_{\,j}(u)z_j\,,$ with weights $q_{\,j}(u)$ arising for a vector $u$ from the Gibbs distribution of the values $[\|z_i-u\|^2 - \bff^\star_i]_i$
