
\section{Experiments}\label{sec:experiments}
\subsection{Setup and Metrics}
We borrow the experimental setting provided by \citet{dheur2025multioutputconformalregressionunified} and benchmark multivariate conformal methods on a total of 24 tabular datasets. Total data size \( n \) in these datasets ranges from 103 to 50,000, with input dimension \( p \) ranging from 1 to 348, and output dimension \( d \) ranging from 2 to 16. We adopt their approach, which is to rely on a multivariate quantile function forecaster \citep[MQF$^2$,][]{kan2022multivariate}, a normalizing flow that is able to quantify output uncertainty conditioned on input $x$. However, in accordance with our stance mentioned in the background section, we will only assume access to the conditional mean (point-wise) estimator for \OTCP.
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/fig1_supplementary_region_size.pdf}
    \vskip-.2cm
    \caption{This plot details the impact of the two important hyperparameters one needs to set in \OTCP: number of target points $m$ sampled from the uniform ball and the $\varepsilon$ regularization level. As can be seen, larger sample size $m$ improves region size (smaller the better) for roughly all datasets and regularization strengths. On the other hand, one must tune $\varepsilon$ to operate at a suitable regime: not too low, which results in the well-documented poor statistical performance of unregularized / linear program OT, nor too high, which would lead to a collapse of the entropic map to the sphere. Using OTT-JAX and its automatic normalizations, we see that $\varepsilon=0.1$ works best overall.}
    \label{fig:sup-region}
\end{figure*}


As is common in the field, we evaluate the methods using several metrics, including marginal coverage (MC), and mean region size (Size). The latter is using importance sampling, leveraging (when computing test time metrics only), the generative flexibility provided by the MQF$^2$ as an invertible flow. See \citep{dheur2025multioutputconformalregressionunified} and their code for more details on the experimental setup.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/fig1_small_score_time.pdf}\vskip-.2cm
    \caption{Computational time on small dimensional datasets. \OTCP\ incurs more compute time due to the OT map estimation. See Fig.\ref{fig:big-time} for a similar picture for higher dimensional datasets.}
    \label{fig:small-time}
\end{figure}

\subsection{Hyperparameter Choices}
We apply default parameters for all three competing methods, \MCP\ and \MergeCP, using (or not) the Mahalanobis correction. For \MCP\ using conformalized quantile regression boxes, we follow \citep{dheur2025multioutputconformalregressionunified} and leverage the empirical quantiles return by MQF$^2$ to compute boxes \citep{zhou2024conformalized}.

\OTCP:\ our implementation requires tuning two important hyperparameters: the entropic regularization $\varepsilon$ and the total number of points used to discretize the sphere $m$, not necessarily equal to the input data sample size $n$. These two parameters describe a fundamental statistical and computational trade-off. On the one hand, it is known that increasing $m$ will mechanically improve the ability of $T_\varepsilon$ to recover in the limit $T^\star$ (or at least solve the semi-discrete~\citep{PeyCut19} problem of mapping $n$ data points to the sphere). However, large $m$ incurs a heavier computational price when running the \citeauthor{Sinkhorn64} algorithm. On the other hand, increasing $\varepsilon$ improves on \textit{both} computational and statistical aspects, but deviates further the estimated map from the ground truth $T^\star$ to target instead a blurred map. We have experimented with these aspects and derive from our experiments that both $m$ and $\varepsilon$ should be increased to track increase in dimension. As a sidenote, we do observe that debiasing the outputs of the \citeauthor{Sinkhorn64} algorithm does not result in improved results, which agrees with the findings in~\citep{pooladian2022debiaser}. We use the OTT-JAX toolbox~\citep{cuturi2022optimaltransporttoolsott} to compute these maps.

\subsection{Results}
We present results by differentiating datasets with small dimension $d\leq 6$ from datasets with higher dimensionality $14\leq d\leq 16$, that we expect to be more challenging to handle with OT approaches, owing to the curse of dimensionality that might degrade the quality of multivariate quantiles. Results in \Cref{fig:big-region} indicate an improvement (smaller region for similar coverage) on 15 out of 18 datasets in lower dimensions, this edge vanishing in the higher-dimensional regime. Ablations provided in Figure~\ref{fig:sup-region} highlight the role of $\varepsilon$ and $m$, the entropic regularization strength and the sphere size respectively. These results show that results for high $m$ tend to be better but more costly, while the tuning of the regularization strength $\varepsilon$ needs to be tuned according to dimension~\citep{vacher2022parameter}. Finally, \Cref{fig:taxi} provides an illustration of the non-elliptic CP regions outputted by \OTCP, by pulling back the rescaled uniform sphere using the inverse entropic mapping described in Section~\ref{subsec:entropic}.



















































