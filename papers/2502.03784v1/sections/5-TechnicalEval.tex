\section{Technical Evaluation}
\label{sec:technical-eval}
The effectiveness of GistVis depends on whether our computational pipeline can successfully extract key information from data-rich documents to create word-scale visualizations. Therefore, we conduct a technical evaluation to assess the performance of the GistVis submodules. Specifically, we provide quantitative results on the performance of the Discoverer and Annotator. Because Extractor and Visualizer is essentially a restoration process bound with information loss, we argue it would be challenging to provide reliable ground truth to evaluate the two modules. Hence, we demonstrate the performance of the Extractor and Visualizer with the user study results in the next section (Sec.~\ref{sec:eval-user-study}).

\subsection{Discoverer Evaluation}
The purpose of Discoverer is to delimit paragraphs into unit segments. In this section, we demonstrate the effectiveness of the Discoverer by comparing it with alternative methods that share the same algorithmic goal as the Discoverer.

\subsubsection{Experiment Setting}
We use the annotated corpus described in Sec.~\ref{sec:formative-study} to evaluate the performance of the Discoverer. Following the requirement of our computational pipeline, we annotated unit segments within the hard boundaries of paragraphs. We selected paragraphs that contained at least one data insight because we did not label unit segments for paragraphs without data insights. After filtering, our evaluation corpus included 640 paragraphs, each containing one or multiple unit segments. 

To provide a reference for comparison, we implemented three alternative methods. We first split paragraphs into sentences using the sentence tokenizer from the natural language processing toolkit nltk~\cite{bird2009natural} for all three alternative methods. Then, we applied different strategies to form unit segments. The first strategy, namely ``regex'', used regular expression to detect numbers inside a sentence. We identified a sentence as a unit segment if it includes a number. The second and third strategies involve using pre-trained language models, specifically BERT~\cite{devlin-etal-2019-bert}\footnote{\url{https://huggingface.co/google-bert/bert-base-uncased}} and sentence BERT~\cite{reimers-gurevych-2019-sentence}\footnote{\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}}, to group similar sentences into unit segments. We first computed sentence embeddings, then computed the similarity between sentences, forming a unit segment if the similarity was above a certain threshold. We ran a grid search over multiple thresholds and reported the result of the best-performing threshold for each approach.

We benchmarked the performance of our Discoverer module against three alternative methods: regex, BERT, and sentence BERT, using the evaluation corpus mentioned above. %
We report the accuracy of segmentation for each condition. We define an accurate segmentation as making the same segmentation as our annotation. Our results would be a conservative indicator of the model's performance because we classify both over-segmentation and under-segmentation as incorrect, accepting only perfect matches. Over or under-segmentation of sentences without data insights would not impact the visualization result. Yet, we still require our algorithms to correctly delimit the boundaries to facilitate clear communication of our evaluation and compare different approaches.

\subsubsection{Result}
\begin{table}[tbp]
    \centering
    \caption{Comparison of segmentation accuracy for four candidate approaches.}
    \begin{tabular}{rrr}
    \toprule
       \textbf{Strategy}  & \textbf{Accuracy} & \textbf{Threshold} \\
    \midrule
       Regex  & 0.545 & - \\ 
       BERT &  0.611 & 0.81\\ 
       Sentence BERT & 0.600 & 0.68 \\ 
       Discoverer (LLM)  & \textbf{0.686} & - \\ 
    \bottomrule
    \end{tabular}
    \label{tab:discoverer-eval}
\end{table}

Results revealed moderate performance for all approaches, with the strategy applied in the Discoverer module performing the best (Table.~\ref{tab:discoverer-eval}). Specifically, the LLM-driven method we applied in the Discoverer module performed best, perfectly segmenting 68.6\% of the paragraphs. Meanwhile, the regex strategy fared worst, perfectly segmenting only 54.5\% of the paragraphs. The pre-trained language models' strategies gave a modest performance (61.1\% for BERT and 60.0\% for Sentence BERT). However, a trade-off of using the LLM-driven Discoverer is the processing time. Though the exact generation time through calling LLM APIs would be affected by internet speed and server load, we provide a rough expectation of this method's processing speed. We recorded an average of 5.22 seconds (SD=2.13s) to split each paragraph, with an average of 0.12 seconds (SD=0.03s) per-word processing time.



\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/confusion-matrix-comparison.pdf}
    \caption{Normalized confusion matrices for data fact type annotation results. The left matrix (A) shows the result of our two-step Annotator (Type Checker + Type Moderator), while the right matrix (B) shows the result of the ablated condition (Type Moderator only). The horizontal axis denotes the predicted type, while the vertical axis indicates the actual type. The numbers on the diagonal line of this matrix represent the precision of classification for each category.}
    \label{fig:type-confusion-mat}
\end{figure*}


\subsection{Annotator Evaluation}
\label{subsec:quant-eval-annotator}

The goal of the Annotator is to label each unit segment with the corresponding data fact type. Thus, we demonstrate the Annotator's effectiveness by reporting its classification performance. We also justify the two-step Annotator design (Type Checker + Type Moderator) through an ablation study over a one-step design (Type Moderator only).

\subsubsection{Experiment Setting}
We use the annotated corpus described in Sec.~\ref{sec:formative-study} to evaluate the performance of the Annotator. To understand the performance of this single module, we assume the segmentation is correct and directly use the labeled unit segments as input. To suit the scope of the GistVis pipeline, we excluded types that are not yet supported and excluded meta-fact entries. After the above exclusion, we are left with 2676 unit segments as input. The dataset is highly imbalanced in its label because data insights are generally scarce even in data-rich documents. Specifically, there are 2355 unit segments without data fact (no type), 158 values, 41 trends, 58 comparisons, 7 extremes, 48 proportions, and 9 ranks, respectively. Because of the label imbalance, we report accuracy, and the weighted precision, recall, and F1-score to reflect the pipeline performance objectively. We also report the average annotation time per unit segment to provide a full picture of the performance of this module.

For our ablation study, we disabled the Type Checker in our Annotator module. Because the two-step Annotator does not pass ``no type'' segments to the Type Moderator, we slightly modified our prompt to support the output of the ``no type'' label to enable this comparison. The rest of the experiment was kept the same to eliminate confounding variables.

\subsubsection{Result}

Results revealed decent performance of the Annotator, achieving an overall accuracy of 0.79, a precision of 0.92, a recall of 0.79, and an F1 score of 0.84. Fig.~\ref{fig:type-confusion-mat}. A is the confusion matrix of the classification results. We identified several data fact types prone to misclassification, including rank, proportion, and extreme. For instance, the Annotator frequently misidentified proportion and extreme types as the value type. Since one or multiple values typically occur in proportion and extreme types, such misclassification exerts a minor impact during the visualization stage. However, visualizing proportion and extreme as the value type could potentially lead users to misunderstand the data insights. Additionally, rank was often misclassified as difference, extreme, or no type, which could undermine the expression of the rank insight during the visualization stage. We recorded an average inference time for each unit segment of 2.34 seconds (SD=2.46s). The large standard deviation in time was due to some inferences only passing one sub-stage (e.g., no type or only identified one data fact type). In contrast, others needed two passes to finalize the data fact type inference, which would significantly increase inference time.

At first glance, the ablated one-step condition (Type Moderator only) seemed to reveal comparable overall performance to the two-step Annotator, achieving an overall accuracy of 0.84, a precision of 0.89, a recall of 0.84, and an F1 score of 0.84. However, a breakdown in the classification performance of each category (Fig.~\ref{fig:type-confusion-mat}. B) revealed a significant pitfall of this design: the one-step condition was inclined to assign ``no type'' to unit segments. Specifically, the one-step condition significantly reduced the precision of the ``value'' type to only 0.11, an unacceptable result because the ``value'' type consists of as much as 33\% of the data insights in the corpus we collected. We attribute this difference between the two and one-step Annotator to the ``sifting'' effect of the Type Checker. The multiple parallel Type Checkers before the Type Moderator module filtered out most of the ``no type'' conditions and other unlikely data fact types, reducing the subsequent classification with fewer options. A smaller decision space would typically make classification easier. Another benefit of using a two-step design is that each Type Checker could be bespoke to achieve better performance. For example, we could design specific prompts using advanced prompting techniques such as In-context Learning~\cite{yao2024more} to bolster the performance further or even use different classification models for each data fact type.
