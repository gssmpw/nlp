\section{User Study}
\label{sec:eval-user-study}
To evaluate GistVis, we first demonstrated three target use cases related to its application. Then, based on low-level tasks~\cite{amar2005lowlevel} related to the use cases, we conducted a user study to understand the efficacy of GistVis and gather user feedback into improving GistVis.

\subsection{Use Cases}
\paragraph{Reading News Articles.}
Emily is keen to understand the current state of the healthcare industry, particularly the impact of a recent medical strike. To quickly capture the gist, Emily resorts to GistVis-generated visualizations to search for key data insights conveyed in the narration of this news article. She quickly captured the government's plan \includegraphics[height=10px]{figures/in-situ/in-situ-2.pdf} and the corresponding gap compared to other countries \includegraphics[height=10px]{figures/in-situ/in-situ-1.pdf} with the word-scale line charts and bar charts. She also discovered that junior doctors only account for roughly 35\% of total doctors through a word-scale stacked bar chart \includegraphics[height=10px]{figures/in-situ/in-situ-3.pdf}.

\paragraph{Reading Business Reports.}
As an investor, David wants to find key indicators of an automotive firm from a seasonal report. Using GistVis, he quickly found that the average price of electric vehicles had fallen by 0.9\% \includegraphics[height=10px]{figures/in-situ/in-situ-6.pdf}. He also captured the changes in vehicle supply compared to last year \includegraphics[height=10px]{figures/in-situ/in-situ-7.pdf} and the average value of EV incentives\includegraphics[height=10px]{figures/in-situ/in-situ-10.pdf} with the word-scale bar charts and icons. GistVis's visualizations highlighted the placement of data insights, which propelled him to navigate through all the key indicators swiftly. 


\paragraph{Reading Academic Papers}
Sarah is a student passionate about exploring interdisciplinary topics. When reading a visualization paper, she quickly understood the key findings via GistVis: many participants used the generated charts \includegraphics[height=10px]{figures/in-situ/in-situ-8.pdf}, and the new design elicited better user performance than the baseline condition \includegraphics[height=10px]{figures/in-situ/in-situ-9.pdf}. By visualizing key data expressions in the paper, GistVis enables her to explore new topics with confidence.


\subsection{Goals, Conditions, and Metrics}
Based on the prior description of GistVis's use case, we aim to understand the following two facets of GistVis:
\begin{itemize}
    \item \textbf{Utility}: Can users better understand data-rich documents with GistVis-generated visualizations?
    \item \textbf{User Perception}: How do users perceive the new content generated by GistVis, including the visualization design, interaction model, generation quality, and its impact on the reading workload?
\end{itemize}

To understand whether GistVis provided incremental benefits to users, we compare reading with GistVis (condition \textit{GistVis}) to the traditional experience of reading data-rich documents with only the plain text (condition \textit{Plain Text}). We are aware that some data-rich documents have visualizations accompanying the textual descriptions. However, since we aim to understand whether GistVis benefits users, we decided not to include the original visualizations in those documents as they are neither word-scale nor widely applicable to all data-rich documents.

We devised a reading task and used several standardized tests to capture the utility and user perception of GistVis. For utility, participants were invited to respond to four multiple-choice questions related to the data presented in the text articles and one summary question. The four multiple-choice questions test users' ability to perform data analysis tasks~\cite{amar2005lowlevel} with data-rich documents under the two conditions.
For example, we phrase `Find Proportion' as `\textit{What is the percentage of [entity]?}' and the `Compare' task as `\textit{Which is the highest/second-highest among [entities]?}' We calculated the accuracy of the participant's response and compared the accuracy between each condition (see Supplemental Material for materials, questions, and answers). We also collected interaction logs containing participants' frequency of visiting the word-scale visualizations and the dwell time in the GistVis Condition. We interpret this data by comparing the interaction log with the position of the word-scale visualization in which the answer lies. We seek to understand if the generated visualizations helped or interfered with participants' answers. In addition, after completing the four multiple-choice questions, participants were required to summarize the document's central argument. The results of the summary questions were not included in our evaluation but served as a stimulus to ensure participants finished reading the whole article. We recorded the participants' time finishing all five questions to analyze the effect of GistVis on their overall reading time.

For user perception, we employed a standardized test for workload, NASA-TLX~\cite{hart1988development}, to gauge users' workload under the two conditions. We used the Qualtrics online version created by \citet{castro2022examining} for our evaluation. Since user perception is highly personalized, we also collected qualitative data through a semi-structured interview. We asked participants questions regarding their perception of GistVis's impact on the reading experience, reading strategy, comments on the generated visualizations, and overall system verdict (see Supplementary Material for interview script).

Additionally, we used a short-form graph literacy test by \citet{okan2019using} to provide an anchor to validate participants' claims of their expertise in reading visualizations. We chose this short-form test over tests such as VLAT~\cite{lee2017vlat} or mini-VLAT~\cite{pandey2023minivlat} to prioritize pace over comprehensiveness to better control the time of our study. We used graph literacy as an additional variable and conducted regression analyses to investigate its impact on user performance. 

\begin{figure*}[tb]
  \centering
  \includegraphics[width=0.85\linewidth]{figures/test-interface.pdf}
  \caption{The interface we employed for our user study. The data-rich document is rendered in the \textbf{Document Panel} on the left. In the middle is the \textbf{Questions Panel}, where participants answer multiple-choice or summary questions. The right is the \textbf{Time and Control Panel}, where we record the finishing time for this passage when participants click on the submit button.}
  \label{fig:User-Study-Interface}
\end{figure*}

\subsection{Materials}
We chose six text articles from Pew Research, each from a different topic. All the articles could stand alone with text-only descriptions and have relevant visualizations removed from the original documents. We use GistVis to generate interactive word-scale visualizations for each article. Meanwhile, we used the exact specification but coerced all data fact types to no type to render a visualization-free version in Plain Text.

\begin{itemize}
    \item The \textsc{Age and Generations} article~\cite{us-centenarian} is about the growth of centenarians in the US over the next 30 years. The article comprises 646 words, 38 numbers, and 24 word-scale visualizations. Seven numbers and four word-scale visualizations are relevant to the task. 
    \item The \textsc{Science} article~\cite{us-k12-education} is about the perceptions and statistics for the United States kindergarten to 12th grade Science, Technology, Engineering, and Mathematics education. The article comprises 576 words, 22 numbers, and 24 word-scale visualizations. Seven numbers and six word-scale visualizations are relevant to the task. 
    \item The \textsc{Family and Relationships} article~\cite{age-of-us-couples} is about the narrowing age gap between U.S. husbands and wives over the past 20 years. The article comprises 580 words, 25 numbers, and 21 word-scale visualizations. Seven numbers and four word-scale visualizations are relevant to the task. 
    \item The \textsc{Internet and Technology} article~\cite{online-shopping-sales} is about trends in online shopping during the holiday season in the U.S. The article comprises 590 words, 24 numbers, and 21 word-scale visualizations. Four numbers and four word-scale visualizations are relevant to the task. 
    \item The \textsc{Race and Ethnicity} article~\cite{restaurants} is about the prevalence and distribution of Asian cuisine in U.S. restaurants. The article comprises 684 words, 34 numbers, and 27 word-scale visualizations. Seven numbers and four word-scale visualizations are relevant to the task. 
    \item The \textsc{Politics and Policy} article~\cite{black-voters-support} is about Black voters' preferences and political inclination for the 2024 election. The article comprises 659 words, 35 numbers, and 32 word-scale visualizations. Five numbers and five word-scale visualizations are relevant to the task. 
\end{itemize}

We chose these six articles because they are well-structured data documents and contain numerical data paired with text descriptions. They also cover various data fact types, making them ideal testbeds for our evaluation. Note that GistVis sometimes generated more visualizations than numbers. This is because GistVis generates visualizations for both explicit and implicit data insights based on the semantics of the description (for trend and extreme data types).


\begin{figure*}[tb]
  \centering
  \includegraphics[width=0.93\linewidth]{figures/utility.pdf}
  \caption{A comparison of user performance under GistVis and Plain Text over two utility metrics. Each scatter point in the graph represents one individual record from one participant. We map the values with a divergent color scheme from red (low) to blue (high). The black dot and the black line indicate the mean and 95\% confidence interval of the metric.}
  \label{fig:utility}
\end{figure*}


\begin{figure*}[tb]
  \centering
  \includegraphics[width=1\linewidth]{figures/interaction_log.pdf}
  \caption{The aggregated interaction log from all 12 participants over six test passages. The x-axis represents the word-scale visualizations in the sequence of its occurrence. The yellow line indicates the time consumed on each word-scale visualization, whereas the blue line indicates the number of visits to the word-scale visualization. The transparent red bracket indicates the word-scale visualizations that contain the correct answers for our multiple-choice questions.}
  \label{fig:interation-log}
\end{figure*}

\subsection{Procedure and Tasks}
\paragraph{Introduction and Exploration}
Before the experiment, we conducted an onboarding process to familiarize participants with GistVis. We first provided a brief about the tasks of the experiment and then introduced the functionalities of GistVis to the participants. We allowed them to freely experiment with GistVis using a demo page that included a synthetic article not related to the text articles we used for the experiment.

\paragraph{Utility Tasks}
After the onboarding process, participants were asked to answer 30 questions from the above-mentioned six text articles, which contained 24 multiple-choice questions and six summary questions. Participants saw the Plain Text and GistVis conditions alternatively as they completed three articles for each condition. Researchers instructed participants to start with a specific text article under one condition during the experiment. Half of the participants started with the GistVis condition, while the rest started with the Plain Text condition. The sequence was also rotated every six participants. Researchers provided no intervention during the experiment except for technical support or clarifying the task requirement. For the last two articles, participants completed two rounds of the NASA-TLX test, one for each condition. We ran a timer on the interface for each text article participants go through. However, we did not impose time pressure on the tasks and allowed participants to finish them at their own pace.

\paragraph{Graph Literacy Test}
After finishing the 30 questions over six text articles, we asked participants to participate in a graph literacy test. We provide no time limit for this task.

\paragraph{Semi-structured Interview}
At the end of each session, we conducted a semi-structured interview to collect qualitative feedback on GistVis. We asked them about their preference between two conditions, their comment on the GistVis functionalities, reading strategies, the limitations of GistVis, and potential use cases. We also asked follow-up questions based on the participant's responses.

\subsection{Participants}
We recruited 12 participants (19 to 46 age range, M=22.8 years, SD=7.4 years, 8 self-identified as female, 4 as male) using the snowball sampling technique through social media. Most participants (9 undergraduate students, 2 master's students with a bachelor's degree, 1 graduate who is currently employed) were university students or graduates coming from various majors or occupations. On a 5-point scale, participants self-reported moderate expertise in visualization (M=2.3, SD=1.1) and reading data documents (M=2.9, SD=1.0). Additionally, because our experiment was conducted in English, we requested all non-native English speakers to self-report their English fluency. All participants who registered through our survey subsequently qualified our criterion to participate.

\subsection{Apparatus}
Out of 12 participants, 9 took part in the study remotely from their personal computers, with 3 participants taking part in person. Participants were permitted to control the researcher's computer (remotely through Tencent VooV-meeting's\footnote{\url{https://voovmeeting.com/}} remote control function and in person by directly using the researcher's computer) during the experiment. We implemented a user study interface (Fig.~\ref{fig:User-Study-Interface}) where participants would view and answer the question on the same interface. Under the participant's consent, we recorded participants' interaction with the website (e.g., answers to questions, movements, time) through our local backend. At the same time, we also screen-recorded the interaction process and audio-recorded participants' feedback. 


\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/TLX.pdf}
  \caption{A comparison of perceived workload between GistVis and Plain Text over NASA-TLX metrics. Each scatter point in the graph represents one individual record from one participant. We map the values with a divergent color scheme from red (1-very low) to blue (21-very high). The black dot and the black line indicate the mean and 95\% confidence interval of the metric.}
  \label{fig:nasa-tlx}
\end{figure*}

\subsection{Results}
Since all participants experienced both conditions, we performed a paired t-test when values are normally distributed (time) and a Wilcoxon Signed-Rank test for other metrics (accuracy, workload). We analyzed our data using the statistical computing software R~\cite{r2013r} and reported corresponding statistical indicators (p-value, t-value for t-tests, and V-value for the Wilcoxon Signed-Rank test).

\subsubsection{Utility: Time}
Participants spent a similar amount of time answering the questions, as the paired test did not reveal a significant difference between the two conditions (p=0.536, t=0.639). Specifically, participants spent an average of 401 seconds (SD=175s) with the Plain Text condition and 419 seconds (SD=247s) with the GistVis condition (Fig.~\ref{fig:utility}). An additional regression analysis using linear models (R notation $\text{\textit{Time}} \sim \text{\textit{Condition}} + \text{\textit{GraphLiteracy}}$, referent Plain Text condition) revealed that participants' graph literacy would slightly reduce finishing time. However, the effect was minor and insignificant ($\beta$=-0.826, p=0.971).

\subsubsection{Utility: Accuracy}
Participants' accuracy was similar, as the Wilcoxon Signed-Rank test did not reveal a significant difference between the two conditions (p=0.182, V=14). However, visual analysis (Fig.~\ref{fig:utility}) indicated a trend towards better performance under the GistVis condition (M=0.924, SD=0.090) compared to Plain Text (M=0.868, SD=0.090). Notably, GistVis had a higher minimum accuracy (0.75) and a larger proportion of participants achieving a perfect score (6/12) versus Plain Text (2/12, minimum accuracy 0.67). We also ran a regression analysis to explore if graph literacy played a part in the accuracy (R notation $\text{\textit{Accuracy}} \sim \text{\textit{Condition}} + \text{\textit{GraphLiteracy}}$, referent Plain Text condition). Results indicated a positive effect for graph literacy on accuracy, yet the effect was small and insignificant ($\beta$=0.011, p=0.509).


\subsubsection{Utility: Interaction Log}
Visual analysis of the user interaction logs (Fig.~\ref{fig:interation-log}) revealed that participants benefited from the GistVis visualizations to complete the task. We observed in 81\% of the occasions, interaction (visit count or visit time) peaked around the word-scale visualization containing the correct answer for the question. Such evidence suggested that participants benefited from the automatically generated word-scale visualizations when tasked to retrieve data from data-rich documents.

On average, participants use interaction features most when answering questions related to the \textsc{Difference} data fact type, with the highest average dwell time (M=8.8 seconds) and hover count (M=5.2 times) per component. When addressing questions of \textsc{Proportion} data fact type, participants tend to take the time to understand the implication of the data insight with longer interaction time (M=4.7 seconds) with fewer visits to the component (M=3.1 times). Interaction is relatively minimal for questions related to other visualization components -- participants can quickly obtain information by viewing the charts without using the interactive feature. For instance, questions related to the \textsc{Trend} data fact type involve little interaction, with interaction time (M=3.5 seconds) and hover counts (M=3.4 times) on the lower side among all components. The remaining components exhibited even less user interaction than trend questions.

% \vspace{-8px}
\subsubsection{Perception: Workload}
The Wilcoxon Signed-Rank test indicated that GistVis significantly reduced participants' mental demand (p=0.016, V=51.5) and effort (p=0.033, V=66.5) while revealing a marginal reduction in stress (p=0.059, V=46.1) compared to the Plain Text condition. We found no significant effect on physical, temporal, and performance. Meanwhile, a visual analysis revealed that compared to Plain Text, GistVis had a lower response (lower is better) in mental demand (M=12.25 versus M=15.83), physical demand (M=10.00 versus M=11.91), performance (M=9.17 versus M=10.25), effort (M=11.92 versus M=14.67) and frustration (M=9.08 versus M=12.43), while bearing a slightly higher temporal demand (M=11.50 versus M=10.58).

\subsection{Perception: Participant Feedback}
To seek additional user feedback, we analyzed the qualitative data collected through our semi-structured interview. Generally, participants found the visualization generated by GistVis satisfactory and useful, while we also identified room for improvements proposed by our participants. 

\paragraph{Highlighting entities and numbers.}
Eight out of twelve participants (P2, P3, P4, P5, P7, P8, P10, P11) mentioned that highlighting entities and numbers was helpful for their reading. The highlights enabled the efficient extraction of key information and facilitated the navigation of specific data while reading. P5 noted that the highlighted ``terms'' (entities) influenced his strategy of reading, as he utilized the highlighted items to locate key data-related terms:
\begin{quote}
\textit{``
For example, if there are references to years like 2023 or 2020 in the topic, I would first check if there are any visual indicators marking those years. Some terms might be directly highlighted in the article, so I would look for those terms and then examine the proportions or the data to make things easier.
    ''} - P5
\end{quote}
Meanwhile, while P10 claimed she retained her habit of reading, she also believes the highlighting helped her better understand the details in the documents:
\begin{quote}
\textit{``
If there are no highlights, I would read the entire text in detail, then outline the structure of the article and summarize the main content of each section. If there are highlights, my general strategy would be similar, but I might read (obtain) some of the details a bit more quickly.
    ''} - P10
\end{quote}

\paragraph{Participants perceive GistVis as an effective addition, especially for long data-rich documents.}
Seven participants found GistVis helpful for reading long texts (P4, P5, P6, P7, P8, P10, P12), especially when reading unfamiliar data-rich texts. P4, P6, and P8 believe that GistVis could increase document readability, especially for long data-rich documents, thereby reducing the mental load:
\begin{quote}
\textit{``
(Without visualization,) a long piece of text can be quite difficult to read. For instance, if you're submitting a data report to your advisor, they might be reluctant to read it. However, with such visual elements, they might be willing to spend a few valuable minutes reviewing it.
    ''} - P4
\end{quote}
\begin{quote}
\textit{``
(And) Having more labeled data makes it (data-rich documents) look much better. With these elements, reading a long data article becomes more engaging and novel. Yes, it makes it more likely that people will be willing to read it.
    ''} - P6
\end{quote}
P8 perceived the visualizations generated by GistVis, an effective proxy for data retrieval, which is useful for reading data-rich documents:
\begin{quote}
\textit{``
Without visualization, you might need to find the data yourself and perform calculations and comparisons on your own.
    ''} - P8
\end{quote}
P10 thought that GistVis would be helpful even for articles with pre-existing charts as it could strengthen the impression of the document and the understanding of the charts:
\begin{quote}
\textit{``
Or during the process of reading (an academic paper), you might need to jump back and forth between the text and the (original) charts. If you first read the text with visualizations, then look at the charts, it can reinforce your impressions.
    ''} - P10
\end{quote}


\paragraph{Rank, proportion, and trend are intuitive, but some visualization markings are hard to distinguish.}
Six participants (P3, P5, P6, P8, P10, P12) pointed out that the visualization we designed for rank, proportion, and trend is intuitive and easy to understand. However, participants identified room for improvement in visualizing some data fact types. For example, P4 and P5 suggested that using bar charts to represent both comparison and rank makes it visually challenging to discern the different data insights without interacting with them: 
\begin{quote}
\textit{``
For example, when (GistVis) visualizes differences, it uses bar charts, and when listing each data point, it also uses bar charts. Sometimes, just looking at these small charts, you can't immediately tell whether they represent the difference or the individual values (rank).
    ''} - P4
\end{quote}
P4 also suggested using a coherent color mapping for the same entity occurring multiple times:
\begin{quote}
\textit{``
(Highlighting of existing entities with) new colors not previously used can be confusing. It would be better if the colors were consistent throughout.
    ''} - P4
\end{quote}
