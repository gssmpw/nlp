\section{Survey on Data-rich Documents}
\label{sec:formative-study}

To better motivate the design of GistVis, we conducted a formative study based on a corpus of data-rich documents to understand the narrative features of data-rich documents. In this section, we first describe how we collected the data-rich document corpus. Then, we describe how we derived the codes we used for our deductive coding process based on prior visualization literature. Lastly, we present the findings of our study and outline several constraints we took in the design of the GistVis automatic generation pipeline.

\subsection{Corpus Collection}
Data-rich documents are widely adopted in various domains; thus, many source documents are available online. Meanwhile, data-rich documents can also come in various genres, for instance, data journalism~\cite{stalph2018classifying} and scientific articles~\cite{beck2017wordsized}. Since it is more common for the general public to access data journalism, we included only data journalism in our corpus. Specifically, data journalism refers to news reports that contain rich data content (e.g., ~\cite{data-news-example, us-k12-education}).

Three researchers, who were briefed on the definition of data journalism and familiar with data visualization, applied a snowballing approach to collect data-rich news articles to construct the corpus. During the collection process, we attempted to cover a wide variety of topics while we labeled the topic for each data article referencing Stalph's topical classification codes~\cite{stalph2018classifying}. Ultimately, we collected 44 data-rich news articles that we labeled using six topical codes: \textit{Politics} (19.30\%), \textit{Society} (19.30\%), \textit{Business} (14.04\%), \textit{Culture} (15.79 \%), \textit{Local} (8.77\%) and \textit{Other} (22.81\%). All the articles were written in English.
We documented the source link of the documents in the supplementary material.

\subsection{Qualitative Analysis}
The first step in designing an automatic method for generating word-scale in situ visualizations is ascertaining the types of insights conveyed through text description. To obtain the candidate insight types, we coded the whole corpus deductively to 1) delimit the segment that contains data narrations and 2) code the segment with one or multiple insight types. Specifically, we coded the insight types referring to the facts taxonomy~\cite{chen2009effective}, which provided formal definitions that distinguished different types of data insights (e.g., value, distribution, difference, etc.). Recent research has successfully applied this taxonomy in the design of several automatic visualization techniques~\cite{wang2020datashot, shi2021calliope, chen2024chart2vec}.

Although the fact taxonomy is designed to reflect the attribute of the data insights independently across datasets or applications, we excluded several fact types less likely to be captured from text descriptions or conveyed through word-scale visualizations to simplify the coding process. First, limited information in text descriptions prevents us from obtaining complete tabular datasets. Therefore, we excluded data fact types commonly derived from data transformation on complete datasets (e.g., aggregation, anomaly). Secondly, we considered the possible display of the data fact type and excluded fact types that might be hard to read in a word-scale setting. For example, types such as association are commonly represented with chart types like Treemaps and Sankey Diagrams. %
Treemaps or Sankey Diagrams require much more space than what word-scale visualization could offer.

Ultimately, we had eight data fact types: comparison, trend, rank, proportion, extreme, value, distribution, and categorical. Through our coding process, we observed that \textbf{data-rich documents could cover many data fact types, but several data fact types are more common than others (O1)}. The frequency of occurrence and definition of the eight fact types are as follows:
\begin{itemize}%[leftmargin=18pt, itemsep=0pt, parsep=0pt, partopsep=0pt, topsep=1pt]
    \item \textbf{Value} (33.23\%; 332): The value type is one or multiple numerical values in a sentence retrieved under some specific criterion. For example, ``\textit{Sources report that almost 10 million migrants have crossed into the country.}''
    \item \textbf{Trend} (17.34\%; 168): The trend type is a general tendency of one data attribute over time. For example, ``\textit{In the last quarter of 2023, EV sales were up 40\% from the same quarter a year before.}''
    \item \textbf{Comparison} (16.72\%; 162): The comparison type measures the difference in value between two or more entities over a shared breakdown. For example, ``\textit{EVs create 3,932 pounds of carbon per year, compared to 11,435 for gas-powered vehicles.}''
    \item \textbf{Proportion} (15.07\%; 146): The proportion type expresses how much one or multiple attributes comprise the sum. For example, ``\textit{Around 60\% of Mexico is experiencing moderate to exceptional drought.}''
    \item \textbf{Extreme} (8.46\%; 82): The extreme type is the maximum or minimum of one specific attribute. For example, ``\textit{The highest mountain in the world is Mount Chumolongma at 8848 meters.}''  
    \item \textbf{Distribution} (3.61\%; 35): The distribution type demonstrates the numerical values shared over a specific breakdown. For example, ``\textit{Monocrystalline silicon production increased by 31.6 percent, photovoltaic cell production increased by 45.6 percent, NEV increased by 46.3, and wind power generator increased by 66.4 percent.}''
    \item \textbf{Rank} (3.61\%; 35): The rank type demonstrates an order or a sorted sequence over a specific breakdown. For example, ``\textit{This figure means the province contributed the second-highest GDP in China in 2023 only following South China's Guangdong Province with 13.57 trillion yuan.}''  
    \item \textbf{Categorical} (1.96\%; 19): The categorical type is data attributes with a certain joint feature. For example, ``\textit{The employment gains were concentrated in only three sectors: health care, leisure and hospitality, and government.}''
\end{itemize}


While identifying the data fact types, we also delimited the text segment that conveys the fact types. The criterion for extracting the text segments is to find the shortest possible sentence collection that coherently conveys a data insight. In the following, we refer to this shortest text segment that conveys one data insight as the \textbf{unit segment}. We found the majority of (84.21\%, 512) unit segments contain only one sentence, while the maximum count of sentences in a unit segment is 6. This suggested that \textbf{the majority of data insights occur within one sentence, but some cases exist where data insights span multiple sentences (O2).} Meanwhile, we did not consider cross-paragraph unit segments.

Note that because we did not further break sentences into fragments, each unit segment could, in theory, contain multiple data fact types. In alignment with the fact taxonomy, we refer to this scenario as compound fact. We found 46.05\% (280) unit segments in our corpus fall within the compound fact. The above data revealed that \textbf{many unit segments contain only one data insight, yet a good portion do contain multiple insights (O3)}, which could be further divided or interpreted in various ways. However, considering our goal in facilitating document-centric reading, we chose not to further break down the original sentence structure to avoid an overdose of visualizations. Hence, we decided to keep unit segments as our smallest analysis unit.



\subsection{Design Implications}
\label{subsec:formative-constriants-implications}
The above analysis revealed a rather complicated design space to fulfill, and designing a comprehensive automatic method that generates word-scale visualizations based only on text descriptions is challenging.
Therefore, as an initial attempt, we decided to apply several constraints driven by our observations from the corpus study to provide a better definition of our tasks.
We applied the following constraints (C1 - C3) when designing our automatic generation method:
\begin{enumerate}%[label=\textbf{C\arabic*}, itemsep=0pt, parsep=0pt, partopsep=0pt, topsep=1pt]
    \item[\textbf{C1}] \textbf{We design our automatic generation method and visualization for more frequent data fact types (derived from O1).} Specifically, we support the data fact types of value, proportion, comparison, trend, rank, and extreme, the top six types in our corpus. We integrate distribution into the comparison category because they could share similar visual encoding. Additionally, we exclude categorical because of its low occurrence frequency.
    \item[\textbf{C2}] \textbf{We design our automatic generation method to be compatible with situations where a data insight can be extracted from the text in one unit segment (derived from O2).} This constraint ensures that we cover scenarios where data insights occur in consecutive sentences. Additionally, this constraint ensures that we can generate data-driven visualizations, which is the majority word-scale visualization type (79.5\%) created by human designers~\cite{goffin2017Exploratory}.
    \item[\textbf{C3}] \textbf{We design our automatic generation method to display only one data fact type (i.e., not concurrently process compound fact) (derived from O3).}  Although a moderate number of unit segments are compound facts, as a first step, we intend to evaluate the feasibility of generating one data fact type first before generalizing the method to support more complicated scenarios.
\end{enumerate}

On top of those constraints, we intended to leave the implementation flexible, which grants extensibility and enables further expansion and optimization. Considering all observations (O1 - O3), constraints (C1 - C3), and the extensibility objective, we bear three overarching design goals (DG1 - DG3):

\begin{enumerate}%[label=\textbf{DG\arabic*}, itemsep=0pt, parsep=0pt, partopsep=0pt, topsep=1pt]
    \item[\textbf{DG1}] \textbf{Establish a uniform data structure to encode both plain text and data insights.} The architecture should treat the data insights at the same level as plain text, ensuring that the word-scale visualization is generated from one unit segment from a document-centric perspective.%
    \item[\textbf{DG2}] \textbf{Apply modular design principle.} Provide abstraction to the automatic word-scale visualization generation process to support plug-and-play property for each processing module to optimize performance and extensibility.
    \item[\textbf{DG3}] \textbf{Design reusable and expressive word-scale visualization components that support interactive document-centric analysis.} The visualization design should be succinct and clear to express insights into various data fact types. They should also adhere to the space limitations of the word-scale setup while offering interaction between text and visualization to improve its clarity. Considering extensibility, the word-scale visualization components should be independent of data insights to support the easy integration of additional word-scale visualization designs.
\end{enumerate}
