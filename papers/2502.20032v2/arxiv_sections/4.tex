\section{The Proposed Method: GDDSG}\label{sec4}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/framework.pdf}
    \caption{Illustration of The Overall Framework. [best view in color]}
    \label{fig: framework}
\end{figure*}
\textbf{Overview.} \autoref{fig: framework} provides an overview of our proposed method. 
Using task \( t \) as an example, we begin by projecting all training samples into an embedding space utilizing a pre-trained backbone. In this space, we compute the centroids for each class. Next, we evaluate whether a new centroid \( \mathbf{c}_i \) should be integrated into an existing class group \( G_j \).
If \( \mathbf{c}_i \) is dissimilar to all classes within \( G_j \), it is added to the group. If it is similar to any class in an existing group, it remains unassigned.
For unassigned centroids, we construct new similarity graphs (SimGraphs) based on their pairwise similarities. We then apply graph coloring theory to these SimGraphs, forming new class groups by clustering dissimilar categories together.
Finally, we update the NCM-based classifier with all class groups, facilitating efficient model updates with minimal computational overhead.

\subsection{Class Grouping Based on Similarity}

\autoref{Corollary: cor} provides guidance for constructing a sequence of dissimilar tasks. A key idea is to dynamically assign each new class to a group during CIL, ensuring that the similarity between the new class and other classes within the group is minimized. This approach helps maintain the robustness of each group's incremental learning process to the order of tasks. For each group, a separate adapter can be trained, and the results from different adapters can be merged during prediction to enhance the model's overall performance. 

In a given CIL task sequence, we organize the classes into several groups. The group list is denoted as \( G = [G_1, \dots, G_k] \), where each \( G_i \) represents a distinct group of classes. For a specified task \( t \) and each class \( C \in CLS^t \), our objective is to assign class \( C \) to an optimal group \( G^* \), ensuring that the new class is dissimilar to all existing classes in that group.

To achieve this objective, we first define the similarity between classes.
The similarity between any two classes, \( CLS_i \) and \( CLS_j \), is determined using an adaptive similarity threshold \( \eta_{i,j} \).
This threshold is computed based on the mean distance between the training samples of each class and their respective centroids in a learned embedding space, as shown below:

\begin{align}
    \eta_{i,j} = \max [
    & \frac{\sum_{k = 1}^{|X^t|} \mathbb{I}(y^t_k = i) \, d(h(x_k^t), \mathbf{c_i}) }{\sum_{k = 1}^{|X^t|} \mathbb{I}(y^t_k = i)}, \nonumber \\
    & \frac{\sum_{k = 1}^{|X^t|} \mathbb{I}(y^t_k = j) \, d(h(x_k^t), \mathbf{c_j}) }{\sum_{k = 1}^{|X^t|} \mathbb{I}(y^t_k = j)} 
    ],
\end{align}
where \( \mathbf{x}^{(t)} \) denotes the t-th task instance, \( h(\cdot) \) is the feature extraction function defined in Equation \autoref{eq: feature}, \( d: \mathcal{X} \times \mathcal{X} \to \mathbb{R}^+ \) specifies the distance metric space, \( \mathbb{I}(\cdot) \) represents the characteristic function, and the class centroid \( \mathbf{c}_i \in \mathbb{R}^d \) is computed as \( \mathbf{c}_i = \frac{1}{|C_i|} \sum_{x_j \in C_i} \mathbf{x}_j \).


Building upon this framework, we define the condition under which two classes, \( CLS_i \) and \( CLS_j \), are considered dissimilar. Specifically, they are deemed dissimilar if the following condition holds:

\begin{equation}
    d(\mathbf{c_i}, \mathbf{c_j}) > \eta_{i,j}.
    \label{eq: sim}
\end{equation}

Thus, class \( C \) is assigned to group \( G^* \) only if it is dissimilar to all classes within \( G^* \), and \( G^* \) is the choice with the lowest average similarity:

\begin{equation}
    G^* = \arg\min_{G} \frac{1}{|G|} \sum_{C' \in G} d(C, C').
\end{equation}

This approach is consistent with the principles outlined in \autoref{Corollary: cor} and ensures the robustness of the model across the entire task sequence.


\subsection{Graph-Driven Class Grouping}

Graph algorithms provide an efficient method for dynamically grouping classes while minimizing intra-group similarity.
In a graph-theoretic framework, classes are represented as nodes, with edge weights quantifying the similarity between them.
The flexibility and analytical power of graph structures allow for dynamic adjustment of class assignments in CIL, facilitating optimal grouping in polynomial time.
This approach significantly enhances the model's robustness and adaptability in incremental learning tasks.

Therefore, we can leverage the similarity between classes to construct a SimGraph, defined as follows:
\begin{definition} \textbf{(SimGraph.)}
A SimGraph can be defined as an undirect graph $SimG = (V, E)$, where $V$ is the set of nodes that represent each class's centroid and $E$ is the set of edges connecting pair of nodes that represent classes that are determined as similar by \autoref{eq: sim}.
\label{SimGraph}
\end{definition}

Then, we aim to partition the vertex set of this graph into subsets, with each subset forming a maximal subgraph with no edges between vertices. This problem can be abstracted as the classic NP-hard combinatorial optimization problem of finding a minimum coloring of the graphs. Let $G^{-1}(\cdot)$ be an assignment of class group identities to each vertex of a graph such that no edge connects two identically labeled vertices (i.e. $G^{-1}(i) \neq G^{-1}(j)$ for all $(i,j) \in E$). We can formulate the minimum coloring for graph $SimG$ as follows:
\begin{equation}
    \mathcal{X}(SimG) = \min | \{ G^{-1}(k) | k \in V\} |, 
    \label{eq: graph}
\end{equation}
where $\mathcal{X}(SimG)$ is called the chromatic number of $SimG$ and $|\cdot|$ denotes the size of the set.

Brooks' theorem \cite{brooks1941colouring} offers an upper bound for the graph coloring problem. To apply this in our context, we must demonstrate that the similarity graphs constructed in CIL meet the conditions required by Brooks' theorem. By doing so, we can establish that the problem is solvable and that the solution converges, ensuring the effectiveness of our grouping and class coloring process in class incremental learning. Without loss of generality, we can make the following assumptions:

\begin{assumption} In the CIL task, class \( C_i \) is randomly sampled without replacement from the set \( \mathcal{U} = \bigcup_{i=1}^{\infty} C_i \), ensuring that \( C_i \neq C_j \) for all \( i \neq j \). The probability that any two classes \( C_i \) and \( C_j \) within the set \( \mathcal{U} \) meet the similarity condition (as described in \autoref{eq: sim}) is denoted by \( p \).
\end{assumption}

In the CIL scenario with \( N \) classes, the probability of forming an odd cycle is given by \(\left( p^2(1-p)^{(N-2)} \right)^N = p^{2N}(1-p)^{N^2-2N}\). Similarly, the probability of forming a complete graph is \(p^{\binom{N}{2}} = p^{\frac{1}{2}N(N-1)}\).
Thus, the probability that the CIL scenario satisfies Brooks' theorem can be expressed as:
\begin{equation}
    P_{\text{Satisfy Brooks}'} = 1 - p^{2N}(1-p)^{N^2-2N} - p^{\frac{1}{2}N(N-1)}.
\end{equation}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/contour_plot.pdf}
    \caption{Contour plot delineating the subthreshold region where \( P_{\text{Satisfy Brooks}'} < 0.99 \). The horizontal axis spans \( p \in [0.9, 1.0] \), representing probability values, while the vertical axis specifies sample sizes \( N \in [10, 40] \). In regions not displayed, the corresponding \( P_{\text{Satisfy Brooks}'} \) values exceed 0.99.}
    \label{fig: probability}
\end{figure}

\autoref{fig: probability} illustrates the various values of \( N \) and \( p \) that satisfy Brooks' theorem with a probability of less than 0.99. Our findings indicate that when \( N > 35 \), the CIL scenario adheres to Brooks' theorem. Furthermore, even with fewer classes, as long as \( p \) does not exceed 0.9, the CIL scenario can still ensure that the similarity graph complies with Brooks' theorem at a confidence level of 0.99. We conclude that class grouping based on the similarity graph is convergent and can be solved efficiently in polynomial time.

For \autoref{eq: graph}, while no algorithm exists that can compute \(\mathcal{X}(SimG)\) in polynomial time for all cases, efficient algorithms have been developed that can handle most problems involving small to medium-sized graphs, particularly the similarity graph \(SimG\) discussed here. In practical scenarios, such graphs are typically sparse. Notably, in conjunction with the above analysis, the similarity graph \(SimG\) in the CIL scenario satisfies the non-odd cycle assumption in Brooks' theorem \cite{brooks1941colouring}. For non-complete similarity graphs \(SimG\), we have \(\mathcal{X}(SimG) \le \Delta(SimG)\), where \(\Delta(SimG)\) represents the maximum vertex degree in \(SimG\).

Therefore, we can apply a simple yet effective greedy method, the Welsh-Powell graph coloring algorithm \cite{welsh1967upper}. This algorithm first sorts all nodes in the graph in descending order based on their degree and then assigns a color to each node, prioritizing those with higher degrees. During the coloring process, the algorithm selects the minimum available color for each node that differs from its neighbors, creating new color classes when necessary. The time complexity of this algorithm is \( O(|V|^2) \), primarily due to the color conflict check between each node and its neighbors. In theory, the maximum number of groupings produced by this algorithm is \( \max_{i = 1}^n \min\{ \deg(v_i') + 1, i \} \), with an error margin of no more than 1, where \( V' \) is the sequence of nodes sorted by degree, derived from \( V \).

\subsection{Overall Process}

\noindent \textbf{Training Pipeline.}
Building upon the theoretical foundations in Section 3.1, we now formalize the complete training procedure. Our framework leverages a frozen pre-trained feature extractor $\phi(\cdot)$, augmented with trainable random projections $W \in \mathbb{R}^{L \times M}$ where $M \gg L$, to enhance representation capacity. For each input $x_i^t$ from class group $s$, we compute its expanded feature:
\begin{equation}
\label{eq: feature}
h(x_i^t) = g(\phi(x_i^t) W) \in \mathbb{R}^M,
\end{equation}
where $g(\cdot)$ denotes the nonlinear activation.

The core learning paradigm reframes classification as regularized least-squares regression. Let $H_s^t \in \mathbb{R}^{N_s^t \times M}$ be the feature matrix and $Y_s^t \in \mathbb{R}^{N_s^t \times L_s^t}$ denote the one-hot label matrix for class group $s$. We optimize the projection matrix $\Theta_s^t \in \mathbb{R}^{M \times L_s^t}$ through:
\begin{equation}
\label{eq: loss}
\min_{\Theta} \|Y_s^t - H_s^t \Theta_s^t\|_F^2 + \lambda \|\Theta_s^t\|_F^2,
\end{equation}
where $\lambda$ controls regularization strength. The closed-form solution is:
\begin{equation}
\label{eq: analytic}
\Theta_s^t = ( {H_s^t}^\top H_s^t + \lambda I )^{-1} {H_s^t}^\top Y_s^t.
\end{equation}

For incremental updates, we maintain two key components: the Gram matrix $Gram_s^t$ capturing feature correlations, and the prototype matrix $C_s^t$ encoding class centroids. When new task $t$ arrives with $N_s^t$ samples:
\begin{equation}
    \label{eq: gram}
    Gram_{s}^t = Gram_{s}^{t-1} + \sum_{n = 1}^{N_{s}^t} h(x^t_i)^\top h(x^t_i),
\end{equation}
\begin{equation}
    \label{eq: pro}
    C_{s}^t = \begin{bmatrix}C_{s}^{t-1} & \underbrace{\mathbf{0}_M \ \mathbf{0}_M \ \ldots \ \mathbf{0}_M}_{(L_{s}^t - L_s^{t-1})\text{ times}} \end{bmatrix} + \sum_{n = 1}^{N_s^t} h(x^t_i)^\top y(x^t_i).
\end{equation}

The regularization parameter $\lambda$ is adaptively selected from a candidate pool $\Lambda$ through cross-validation on a held-out calibration set, minimizing the empirical risk:
\begin{equation}
\lambda^* = \arg \min_{\lambda \in \Lambda} \|Y_{\text{val}} - H_{\text{val}} (Gram_{\text{val}} + \lambda I)^{-1} C_{\text{val}} \|_F^2.
\end{equation}

Additionally, group descriptors are constructed through prototype similarity analysis. For each training instance $(x, y) \in \mathcal{D}^t$, we generate meta-features dataset as:
\begin{equation}
\mathcal{D}_g = \left\{ \left( \rho(x),\ G^{-1}(y) \right) \right\}_{(x,y)\in \mathcal{D}^t},
\end{equation}
where $\rho(x) = \big[ d(h(x),\mathbf{c}_1), \ldots, d(h(x),\mathbf{c}_k) \big]^\top$ denotes the concatenated distance vector measuring similarity between the sample embedding and prototype vectors.

\noindent \textbf{Inference Pipeline.}
Given test sample $x^*$, its group identification can be learned via $\hat{g} = \mathcal{M}_g(\rho(x^*))$, where $\mathcal{M}_g$ is the class group predict model trained with $ \mathcal{D}_g$.
Then, the prediction will be performed within the selected group via $\hat{y} = \underset{c \in \mathcal{C}_{\hat{g}}}{\arg\max}\ ( g(\phi(x^*) W) (Gram_{\hat{g}} + \lambda I)^{-1} C_{\hat{g}}[:,c]$.
        
\iffalse
In the previous section, we introduced the motivation and core concepts behind the proposed algorithm. In this section, we will describe the entire training process in detail. Recent years have seen CIL methods based on pre-trained models achieve remarkable results \cite{panos2023first,zhou2023revisiting,zhou2023revisiting,mcdonnell2024ranpac}, largely due to their robust representation capabilities. Since our proposed class grouping method also relies heavily on the model's representation ability, we utilize a widely-adopted pre-trained model as a feature extractor. For each class group, we train independent classification heads, which enhances the model’s adaptability and generalization to different class groups.

As outlined above, we utilize a frozen random projection matrix \( W \in \mathbb{R}^{L \times M} \) to enhance features across all class groups, where \( L \) is the output dimension of the pre-trained model and \( M \gg L \) is the expanded dimensionality. Given a task \( t \) and a sample \( x^t_i \) belonging to a class group \( s \), the feature vector of the sample is denoted as \( h(x^t_i) \), and its one-hot encoded label as \( y(x^t_i) \). Specifically,

\begin{equation}
    \label{eq: feature}
    h(x^t_i) = g(\phi(x)^T W),
\end{equation}
where \( \phi(\cdot) \) represents the feature extractor, and \( g(\cdot) \) is a nonlinear activation function.
We define \( H_{s}^t \in \mathbb{R}^{N_{s}^t \times M} \) as the matrix containing feature vectors of \( N_{s}^t \) samples from group \( s \). The corresponding Gram matrix is defined as:
\begin{equation}
    \label{eq: grammatrix}
    Gram_{s}^t = {H_{s}^t}^T H_{s}^t \in \mathbb{R}^{M \times M}.
\end{equation}
Additionally, the matrix \( C_{s}^t \) consists of the concatenated column vectors of all classes within group \( s \), with dimensions \( M \times L_{s}^t \), where \( L_s^t \) represents the number of classes in group \( s \) for task \( t \). When a new task arrives, the model applies the GDDSG algorithm to assign new classes to their respective groups. The Gram matrix \( Gram \) and matrix \( C \) for each group are updated according to the following formulas:
\begin{equation}
    \label{eq: gram}
    Gram_{s}^t = Gram_{s}^{t-1} + \sum_{n = 1}^{N_{s}^t} h(x^t_i) \otimes h(x^t_i),
\end{equation}
\begin{equation}
    \label{eq: pro}
    C_{s}^t = \begin{bmatrix}C_{s}^{t-1} & \underbrace{\mathbf{0}_M \ \mathbf{0}_M \ \ldots \ \mathbf{0}_M}_{(L_{s}^t - L_s^{t-1})\text{ times}} \end{bmatrix} + \sum_{n = 1}^{N_s^t} h(x^t_i) \otimes y(x^t_i),
\end{equation}
where \( \mathbf{0}_M \) denotes a zero vector with \( M \) dimensions.

During the test phase, we combine the classification heads of all groups \( G = [G_1, G_2, \dots, G_k] \) to make a joint prediction for a given sample \( x \). For each class \( c' \) in a group, the score is computed as follows:
\begin{equation}
    \label{eq: predict}
    s_{c'} = g(\phi(x)^T W)(Gram_{i} + \lambda I)^{-1} C_{c'},
\end{equation}
where \( i = 1,\dots,k \) denotes the indices of each groups, and \( \lambda \) is the regularization parameter used to ensure that the \( Gram \) matrix remains invertible. The final classification result is then obtained by applying the following formula:
\begin{equation}
    \label{eq: predict_joint}
    \hat{c} = \mathop{\arg\max}\limits_{c' \in \cup_{i = 1}^k CLS^{G_i}} s_{c'},
\end{equation}
where \( \cup_{i = 1}^k CLS^{G_i} \) represents the set of possible classes across all class groups.
\fi
