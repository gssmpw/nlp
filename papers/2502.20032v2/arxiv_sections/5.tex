\section{Experiment}

\begin{table*}[ht] \footnotesize
    \caption{Results (\%) of CL Methods on Both Fine-grained Datasets and General Vision Dataset (Split into 10 Tasks). Among Them, The Best Results Are Bolded for Emphasis, While The Second-best Results Are Underlined.}
    \label{tab:mainresult}
    \centering
    \begin{tabular*}{1\linewidth}{*{10}{c}}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c}{CIFAR100} & \multicolumn{2}{c}{CUB200} & \multicolumn{2}{c}{Dog} & \multicolumn{2}{c}{OB} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} 
        & $A_N$ ($\uparrow$) & $F_N$ ($\downarrow$) & $A_N$ ($\uparrow$) & $F_N$ ($\downarrow$)& $A_N$ ($\uparrow$) & $F_N$ ($\downarrow$)& $A_N$ ($\uparrow$) & $F_N$ ($\downarrow$) \\
        \midrule
        Finetune & 67.86 $\pm$ 0.46 & 31.25 $\pm$ 1.76 & 48.91 $\pm$ 0.49 & 47.68 $\pm$ 3.48 & 45.64 $\pm$ 4.40 & 46.19 $\pm$ 3.68 & 61.73 $\pm$ 5.70 & 31.84 $\pm$ 4.87 \\ 
        
        L2P & 83.45 $\pm$ 0.25 & 8.50 $\pm$ 0.43 & 66.57 $\pm$ 1.37 & 12.00 $\pm$ 1.41 & 65.09 $\pm$ 3.74 & 9.44 $\pm$ 2.93 & 70.10 $\pm$ 8.86 & 13.70 $\pm$ 3.39 \\ 
        
        DualPrompt & 82.74 $\pm$ 0.85 & 7.19 $\pm$ 1.18 & 68.21 $\pm$ 1.55 & 12.08 $\pm$ 4.65 & 70.90 $\pm$ 0.57 & 10.68 $\pm$ 1.35 & 69.73 $\pm$ 8.26 & 11.91 $\pm$ 2.28 \\
        
        CODA-Prompt & 86.86 $\pm$ 4.26 & 6.04 $\pm$ 0.71 & 73.91 $\pm$ 1.45 & 7.84 $\pm$ 0.10 & 74.09 $\pm$ 0.69  & 10.05 $\pm$ 0.07 & 77.59 $\pm$ 8.41  & 9.05 $\pm$ 1.89 \\
        
        SimpleCIL & 77.63 $\pm$ 2.01 & 7.21 $\pm$ 0.48 & 83.32 $\pm$ 2.03 & 5.89 $\pm$ 1.26 & 83.23 $\pm$ 0.72 & 5.86 $\pm$ 0.43 & 72.15 $\pm$ 0.02 & 8.31 $\pm$ 0.31 \\
        
        ADAM & 84.15 $\pm$ 0.77 & 5.03 $\pm$ 0.15 & 83.89 $\pm$ 2.07 & 5.86 $\pm$ 1.12 & 83.98 $\pm$ 0.34 & 6.08 $\pm$ 0.54 & 72.84 $\pm$ 0.29 & 8.23 $\pm$ 0.16 \\
        
        EASE & 86.95 $\pm$ 0.41 & 6.74 $\pm$ 0.43 & 85.78 $\pm$ 4.22 & 6.27 $\pm$ 2.98 & 81.03 $\pm$ 1.38 & 8.18 $\pm$ 0.50 & 74.56 $\pm$ 4.32 & 8.60 $\pm$ 1.40 \\
        
        RanPAC & \underline{90.50 $\pm$ 0.34} & \underline{3.49 $\pm$ 0.16} & \underline{89.23 $\pm$ 0.36} & \underline{3.91 $\pm$ 0.45} & \underline{85.37 $\pm$ 0.41} & \underline{5.78 $\pm$ 0.42} & \underline{78.86 $\pm$ 0.16} & \underline{7.62 $\pm$ 0.18} \\ 
        
        GDDSG (Ours) & \textbf{94.00 $\pm$ 0.03} & \textbf{0.78 $\pm$ 0.09} & \textbf{91.64 $\pm$ 1.86} &\textbf{ 1.92 $\pm$ 1.29} & \textbf{92.64 $\pm$ 0.48} & \textbf{1.42 $\pm$ 0.08} & \textbf{87.33 $\pm$ 0.32} & \textbf{0.96 $\pm$ 0.16} \\
        \bottomrule
    \end{tabular*}
\end{table*}

\subsection{Experimental Setup}\label{5.1}

\textbf{Datasets.} Since most pre-trained models are currently trained on ImageNet-21K \cite{deng2009imagenet}, we aim to assess the model’s performance on entirely new data. To demonstrate the robustness of our model to task similarity, we conduct experiments using several datasets, including CIFAR100 \cite{krizhevsky2009learning}, CUB200 \cite{wah2011caltech}, Stanford Dogs \cite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}, and OmniBenchmark (OB) \cite{zhang2022benchmarking}. These datasets are divided into multiple, equally sized tasks, and various class orders are tested to evaluate the model’s performance across different orders.

\noindent \textbf{Baseline.} For fairness, we only compare against CL methods that have utilized pre-trained models in recent years. We compare GDDSG with the following six latest and effective CL methods with the PILOT toolbox \cite{sun2023pilot}: L2P \cite{wang2022learning}, Dualprompt \cite{wang2022dualprompt}, CODA-Prompt \cite{smith2023coda}, SimpleCIL \cite{zhou2023revisiting}, ADAM \cite{zhou2023learning}, EASE \cite{zhou2024expandable}, RanPAC \cite{mcdonnell2024ranpac}.

\noindent \textbf{Implementations.} Our code, implemented in PyTorch, has been open-sourced for accessibility. All experiments were conducted on a single Nvidia RTX 3090 GPU, using three random seeds, 1993, 2024, and 4202, to compute the average for a more robust model evaluation. We use a ViT-B/16 model, which is self-supervised and pre-trained on ImageNet-21K.
$\mathcal{M}_g$ is a soft voting model consisting of RandomForest \cite{breiman2001random}, KNN \cite{peterson2009k}, and LightGBM \cite{ke2017lightgbm}.
%Detailed dataset descriptions and experimental implementations are provided in the Supplementary Material.

\noindent \textbf{Metrics.} We employ average final accuracy $A_N$ and average forgetting rate $F_N$ as metrics \cite{wang2022learning}. $A_N$ is the average final accuracy concerning all past classes over $N$ tasks. $F_N$ measures the performance drop across $N$ tasks, offering valuable information about plasticity and stability during CL. 
Following the protocol in \cite{lian2022scaling}, we use the Order-normalized Performance Disparity (OPD) metric to assess the robustness of the class order. OPD is calculated as the performance difference of task \( t \) across \( R \) random class orders, defined as:
\begin{equation}
    \text{OPD}_t = \max\{ \bar{A}_t^1, \ldots, \bar{A}_t^R \} - \min\{ \bar{A}_t^1, \ldots, \bar{A}_t^R \}.
\end{equation}

To characterize both extreme scenarios and overall performance, we introduce two consolidated metrics: Maximum OPD (MOPD) defined as \( \text{MOPD} = \max\left\{\text{OPD}_1, \ldots, \text{OPD}_T\right\} \), and Average OPD (AOPD) calculated by \( \text{AOPD} = \frac{1}{T} \sum_{t=1}^{T} \text{OPD}_t \).
\iffalse
The Maximum OPD (MOPD) and Average OPD (AOPD) are further defined as:
\begin{equation}
    \text{MOPD} = \max\{\text{OPD}_1, \ldots, \text{OPD}_T\},
\end{equation}
\begin{equation}
    \text{AOPD} = \frac{1}{T} \sum_{t=1}^{T} \text{OPD}_t.
\end{equation}
\fi
\subsection{Experimental Results}\label{5.2}

\textit{\textbf{Main Results.}}  \autoref{tab:mainresult} highlights the strong performance of our proposed GDDSG method in terms of accuracy and resistance to forgetting. The results demonstrate that GDDSG consistently outperforms other techniques, achieving state-of-the-art (SOTA) performance. Notably, GDDSG shows marked improvements in both accuracy and forgetting rate. Compared to the previous SOTA method, RanPAC, our approach achieves significantly higher accuracy while maintaining a low forgetting rate of around 1\%, underscoring GDDSG's superior effectiveness in the CIL environment.


\noindent \textit{\textbf{Ablation analysis.}} Our method's two components, SimGraphs and Class Groups, operate as a unified whole. Only after generating the SimGraphs can construct the Class Groups. Therefore, we can only conduct ablation experiments on either individual Class Groups or the SimGraphs and Class Groups combination as a whole, with results shown in \autoref{tab: ablation}. The results demonstrate a significant decrease in model performance after conducting the ablation, validating the effectiveness of SimGraphs and Class Groups.

\begin{table}[t]
    \centering
    \caption{Ablation Experiment.}
    \label{tab: ablation}\small
    \begin{tabular}{cccccc}
        \toprule
        \textbf{$A_N$} & CIFAR100 & CUB200 & Dog & OB \\
        \midrule
        w/o Class Groups & 74.32 & 72.86 & 69.49 & 66.85\\
        \makecell[c]{w/o SimGraphs and\\Class Groups}  & 89.96 & 87.32 & 83.12 & 74.21\\
        GDDSG & \textbf{93.99} & \textbf{92.95} & \textbf{92.30} & \textbf{87.56}\\
        \midrule
        \textbf{$F_N$} & CIFAR100 & CUB200 & Dog & OB \\
        \midrule
        w/o Class Groups & 12.42 & 16.70 & 14.13 & 20.95\\
        \makecell[c]{w/o SimGraphs and\\Class Groups} & 4.12 & 3.78 & 5.99 & 9.46\\
        GDDSG & \textbf{0.72} & \textbf{1.01} & \textbf{1.48} & \textbf{1.07} \\
        \bottomrule
    \end{tabular}
\end{table}


\noindent \textit{\textbf{Robustness to Class Order.}} We conducted comparative experiments on existing order-robust CIL methods, including APD \cite{lian2022scaling}, APDfix \cite{lian2022scaling}, and HALRP \cite{li2024hessian}, using 10 different class orders across four datasets and calculating their MOPD and AOPD metrics. The experimental results, presented in \autoref{fig: Robustness}, show that our proposed GDDSG method demonstrates excellent robustness to category order. MOPD decreased across all datasets, with AOPD showing a significant reduction, underscoring the practical effectiveness of our approach.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/performance_comparison.pdf}
    \caption{Robustness of Different Methods to Class Order: MOPD (Blue) and AOPD (Orange) Indicators, with GDDSG Performing Well Across Four Datasets}
    \label{fig: Robustness}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.47\textwidth]{figures/class_groups.pdf}
  \caption{Analysis of Class Group Counts: The Left Figure Shows Changes in Class Group Counts as the Number of Tasks Increases, and the Right Figure Shows Changes as Task Length Varies.}
  \label{fig: ex1}
\end{figure}
\noindent \textit{\textbf{Analysis of Class Group Counts.}} \autoref{fig: ex1} illustrates how the number of class groups changes as tasks arrive and task lengths vary during the execution of the GDDSG algorithm. We observe that for relatively homogeneous datasets, such as CIFAR-100 and CUB-200, the optimal number of class groups generated remains relatively low and tends to stabilize midway through the task sequence. In contrast, datasets with broader domains and more categories, such as OB, result in a higher number of optimal class groups. Despite this increase, GDDSG maintains accurate matching, demonstrating its strong generalization capability.

Additionally, we simulate varying frequencies of intra-task class conflicts by altering the number of categories within a single task, which leads to differences in both intra-task and inter-task similarities. The results indicate that the optimal number of class groups determined by the GDDSG algorithm consistently converges to a stable value. This demonstrates that, for a specific dataset and pre-trained model, the optimal number of class groups is determined solely by the dataset itself, independent of factors such as task length and order in the CIL environment. This stability arises because the GDDSG algorithm primarily relies on data similarity, while disregarding task-specific information in real-world scenarios. The robustness of this approach is theoretically supported by Brooks' theorem \cite{brooks1941colouring} and the Welsh–Powell algorithm \cite{welsh1967upper}. In conclusion, the GDDSG algorithm exhibits strong robustness to variations in task similarity, length, and order, making it highly valuable for a wide range of applications.

\begin{table}[t]
    \caption{The Class Group Number And Final Results of GDDSG Under Different Pre-trained Backbones.}
    \label{tab:backbone}
    \centering\footnotesize
    \begin{tabular}{>{\centering}m{1.2cm} >{\centering}m{1.2cm} >{\centering}m{1.3cm} >{\centering}m{1.3cm} >{\centering\arraybackslash}m{1.3cm}}
        \toprule
        \multirow{2}{*}{\centering Dataset} & \multirow{2}{*}{\centering Metric} & \multicolumn{3}{c}{Backbone} \\
        \cmidrule(lr){3-5}
        & & ViT B/16 & ResNet-50 & ResNet-18 \\
        \midrule
        \multirow{3}{*}{\centering CIFAR100} & $ \mathcal{X}$ ($\downarrow$) & \textbf{7}  & 39 & 34 \\
        & $A_N$ ($\uparrow$) & \textbf{93.99}  & 83.25 & 80.97 \\
        & $F_N$ ($\downarrow$) & 0.72  & 0.73 & \textbf{0.63} \\
        \midrule
        \multirow{3}{*}{\centering CUB200} & $ \mathcal{X}$ ($\downarrow$) & \textbf{8}  & 34 & 42 \\
        & $A_N$ ($\uparrow$) & \textbf{92.95}  & 70.83 & 51.44 \\
        & $F_N$ ($\downarrow$) & \textbf{1.01}  & 2.43 & 18.32 \\
        \midrule
        \multirow{3}{*}{\centering Dog} & $ \mathcal{X}$ ($\downarrow$) & 4  & \textbf{3} & 18 \\
        & $A_N$ ($\uparrow$) & \textbf{92.30}  & 82.65 & 70.56 \\
        & $F_N$ ($\downarrow$) & \textbf{1.48}  & 1.74 & 5.34 \\
        \midrule
        \multirow{3}{*}{\centering OB} & $ \mathcal{X}$ ($\downarrow$) & 38  & \textbf{36} & 44 \\
        & $A_N$ ($\uparrow$) & \textbf{87.56}  & 77.24 & 62.35 \\
        & $F_N$ ($\downarrow$) & \textbf{1.07}  & 1.17 & 2.14 \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent \textit{\textbf{Detailed Analysis of Backbone.}} \autoref{tab:backbone} presents the optimal number of class groups that GDDSG generates under various pre-trained backbone networks, along with the corresponding \( A_N \) and \( F_N \) values. The results indicate that smaller backbones, such as ResNet18 and ResNet50, yield reduced accuracy and higher forgetting rates compared to using ViT as the backbone. Nonetheless, performance with ResNet50 remains highly competitive, achieving accuracy comparable to L2P while maintaining a relatively low forgetting rate. This further highlights the robustness of GDDSG, as it can reach performance levels similar to those of richer, more powerful backbones, even when using networks with fewer parameters and lower representational capacity.

Another noteworthy observation is that, for the same dataset, the number of class groups varies depending on the backbone network. Specifically, when using ResNet18 or ResNet50, the number of class groups for CIFAR-100 and CUB-200 increases significantly, whereas it decreases for Stanford Dogs and OB. This suggests that, for certain datasets, backbones with lower representational power may erroneously classify some originally dissimilar classes as similar, leading to an increase in class groups. In contrast, other datasets remain unaffected by the choice of backbone. This highlights that the impact of the backbone on the GDDSG algorithm is highly dataset-dependent.

\noindent \textit{\textbf{Detailed Analysis of Class Group Identification.}} To better understand the mechanism of class group identification, we visualized $\mathcal{D}_g$ using t-SNE \cite{van2008visualizing} and UMAP \cite{McInnes2018UMAPUM} on the Split CIFAR100 dataset. \autoref{fig:ex2} presents the visualized results. The observations reveal that the distance features essentially conform to piecewise functions in high dimensions, exhibiting strong linear separability and powerful representation capabilities. Consequently, a class group identification matching model can be effectively fitted using some classical machine learning models, enabling fairly accurate predictions. However, it is crucial to emphasize the importance of this step for GDDSG. Under the partition of the GDDSG algorithm, the accuracy of a single class group can reach nearly 100\%. Therefore, the precision of class group matching directly determines the overall model's accuracy.



\begin{figure}[t]
  \centering
  \includegraphics[width=0.47\textwidth]{figures/ex2.jpg}
  \caption{Visualization of The Distance Feature via T-SNE and UMAP After Training on All Tasks (Split CIFAR100 Dataset).}
  \label{fig:ex2}
\end{figure}
