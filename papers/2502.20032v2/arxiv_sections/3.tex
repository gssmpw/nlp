\section{Problem Formulation and Theory Analysis}

\subsection{Problem Formulation} 
\begin{definition} \textbf{(Class Incremental Learning (CIL))}
Given a sequence of tasks denoted as $1, ...,t,...$, each task $i$ is associated with a training set (i.e., ground-truth data) $\mathcal{D}^i = \{X^i, Y^i\}$, where $X^i$ represents the set of training samples and $Y^i$ is the set of labels. 
For task $i$, the set of classes is denoted as $CLS^i$ with the size of $|CLS^i|$, representing the number of classes in task $i$. 
With new tasks incrementally appearing, the goal of CIL is to learn a \emph{unified model} $\Phi: \mathcal{D}^i \to \mathbb{R}^d$ mapping input data to an embedding space equipped with a classifier $f(\cdot)$ that can perform well on all the tasks it has been learned.
\end{definition}

Note that for any pair of tasks $i$ and $j$ with $1 \leq i, j \leq n$ and $i \neq j$, the sets of classes $CLS^i$ and $CLS^j$ are disjoint and data from other tasks is unavailable at the current task, ensuring distinctiveness and non-overlapping nature between classes across each task. 

\subsection{The Effect of Class Ordering in CIL}

In \cite{lin2023theory}, the authors theoretically derived the expected forgetting value and expected generalization error for CIL under a linear model, where \( w_t^* \) denotes the optimal parameters of the model for the \( t \)-th task:
\begin{theorem}\label{Therom: ther} When $p \ge n + 2 $, we must have:
\begin{align}
    \mathbb{E}[F_T] &= \frac{1}{T-1}\sum_{i = 1}^{T-1}[(r^T-r^i)\lVert w_i^*\rVert^2+\sum_{j>i}^Tc_{i,j}\lVert w_j^* -  w_i^*\rVert^2 \nonumber 
    \\ &+\frac{p\sigma^2}{p-n-1}(r^i-r^T)],
    \label{eq: e_for}
\end{align}
\begin{align}
    \mathbb{E}[G_T] &= \frac{r^T}{T}\sum_{i = 1}^{T-1}\lVert w_i^*\rVert^2 + \frac{1-r}{T}\sum_{i = 1}^T r^{T-i}\sum_{k=1}^T \lVert w_k^* -  w_i^*\rVert^2 \nonumber 
    \\ &+ \frac{p\sigma^2}{p-n-1}(1-r^T).
    \label{eq: e_gen}
\end{align}
where the overparameterization ratio \( r = 1 - \frac{n}{p} \) in this context quantifies the degree of overparameterization in a model, where \( n \) represents the sample size, and \( p \) denotes the number of model parameters \cite{muthukumar2020harmless,hastie2022surprises}. The coefficients \( c_{i,j} = (1 - r)(r^{T-i} - r^{j-i} + r^{T-j}) \), with \( 1 \leq i < j \leq T \), correspond to the indices of tasks, and \( \sigma \) denotes a coefficient representing the model's noise level.
\end{theorem}

\autoref{Therom: ther} made a significant contribution to the study of class order in CIL, particularly in the two key expressions: \(\sum_{j>i}^T c_{i,j} \lVert w_j^* - w_i^* \rVert^2\) in \autoref{eq: e_for} and \(\sum_{i=1}^Tr^{T-i} \sum_{k=1}^T \lVert w_k^* - w_i^* \rVert^2\) in \autoref{eq: e_gen}. These formulas highlight the crucial role that class order plays in CIL. Building on this theory, further in this work, we derive sufficient conditions to ensure order robustness.

\begin{corollary}\label{Corollary: cor} A \textbf{sufficient condition} for the reduction of \(\mathrm{Var}(\mathbb{E}[G_T])\) and \(\mathrm{Var}(\mathbb{E}[F_T])\) is that the sum of the squared distances between the optimal parameters of tasks increases, i.e., \(\sum_{i,j=1}^T \lVert w_i^* - w_j^* \rVert^2\) becomes larger.
\end{corollary}

\autoref{Corollary: cor} integrates the similarity between tasks with the model's robustness to class order. Through \autoref{eq: e_for} and \autoref{eq: e_gen}, we observe that both forgetting and generalization errors are influenced by the optimal model gap between any two tasks, represented by \(\lVert w^*_i - w^*_j \rVert^2\) for tasks \(i\) and \(j\). This gap serves as a measure of task similarity: the smaller the gap, the greater the similarity.
\autoref{Corollary: cor} demonstrates that a smaller similarity between tasks enhances the model's robustness in terms of generalization and resistance to forgetting across different class orders. This finding offers valuable insights for the design of new methods.
%\textit{\textbf{The proof of \autoref{Corollary: cor} can be found in the supplementary material.}}