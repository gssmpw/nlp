\newpage
%\setcounter{page}{1}
\appendix
\section*{Appendix}

\setcounter{table}{0} 
\setcounter{figure}{0}
\setcounter{equation}{0}
\setcounter{algocf}{0}  

%定义编号格式，在数字序号前加字符“A."
\renewcommand\thetable{A.\arabic{table}}
\renewcommand\thefigure{A.\arabic{figure}}
\renewcommand\theequation{A.\arabic{equation}}
\renewcommand\thealgocf{A.\arabic{algocf}} 

\renewcommand*{\theHtable}{\thetable}
\renewcommand*{\theHfigure}{\thefigure}
\renewcommand*{\theHequation}{\theequation}
\renewcommand*{\theHsection}{\thesection}
\renewcommand*{\theHalgocf}{\thealgocf}

\section{Notation}\label{notation}

In \autoref{tab: notations}, we introduce the notations throughout this paper.
\begin{table}[ht]
% \renewcommand\arraystretch{1.5}
 \begin{center}\footnotesize
  \begin{tabular}{c c}
    \hline
    \textbf{Notation} & \textbf{Explanation} \\ 
    \hline
    $t$ & The task $t$ \\
    $\mathcal{D}^t$ & The training set of task $t$\\
    $X^t$ & The set of training samples of task $t$\\
    $Y^t$ & The set of training samples labels of task $t$\\
    $CLS^t$ &  The class set of task $t$\\
    $|CLS^t|$ &  The class account of task $t$\\
    $\phi(\cdot)$ & The CL model's feature extractors\\
    $f(\cdot)$ & The CL model's classifier\\
    $c_i$ & The centroid for class \\
    $G$ & The class group\\
    $\eta_{i,j}$ & The adaptive similarity threshold for $CLS_i$ and $CLS_j$\\
    $d(\cdot,\cdot)$ & The distance function \\
    $\mathbb I(\cdot)$ & The indicator function \\
    $SimG$ & The graph generated base on \autoref{eq: sim} \\
    $V$ & The node set of $SimG$\\
    $E$ & The edge set of $SimG$\\
    $B(\cdot)$ & The corresponding class group to each vertex of a $SimG$\\
    $\mathcal{X}(SimG)$ & The chromatic number of $SimG$\\
    $\Delta(SimG)$ & The maximum degree of vertices in $SimG$\\
    $W$ & The random projection matrix\\
    $L$ & The dimension size of $\phi$\\
    $M$ & The dimension size after random projection\\
    $h(x_i^t)$ & The feature vector of the sample $x_i^t$\\
    $y(x_i^t)$ & The one-hot label embedding of the sample $x_i^t$\\
    $g(\cdot)$ & The nonlinear function\\
    $H_s$ & The concatenation of feature vector of class group $s$\\
    $Gr_s$ & The Gram matrix of class group $s$\\
    $L_s^t$ & The number of class for class group $s$ until task $t$\\
    $N_s^t$ & The number of samples for class group $s$ in task $t$\\
    $PIT(\cdot)$ & The function to predict the class group\\
    $p_i$ &  Probability outputted by Softmax\\
    $s_i$ &  Scores outputted by NCM \\
    \hline
  \end{tabular}
  \caption{Notations and explanations.} 
  \label{tab: notations}
 \end{center}
\vspace{-0.5cm}
\end{table}

\section{Datesets, Implementations and Additional Experimental Results}

\subsection{Datasets}\label{sec: datasets}

\begin{table}[h]\footnotesize
\begin{tabular}{ccccc}
\hline
Datasets      & Orginal & $N$ & Val samples & Class numbers \\\hline
CIFAR100      &   \cite{krizhevsky2009learning}   & 50000            & 10000        & 100             \\
CUB           &    \cite{wah2011caltech}     & 9430             & 2358         & 200             \\
Stanford Dog  &    \cite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}     & 12000            & 8580         & 120             \\
OmniBenchmark &    \cite{zhang2022benchmarking}     & 89697            & 5985         & 300   \\  \hline 
\end{tabular}
\caption{Datasets. We list references for the original source of each dataset. In the column headers, $N$ is the total number of training samples, \emph{Class numbers} is the number of classes following training on all tasks, and \# of val samples is the number of validation samples in the standard validation sets.}
\label{tab: datasets}
\end{table}

The four CL datasets we use are summarised in \autoref{tab: datasets}. For CUB and Omnibenchmark we used specific train-validation splits defined and outlined in detail by \cite{zhou2023revisiting}. For the CIFAR100, CUB, and Stanford Dog datasets, which are categorized as fine-grained datasets, their task similarity has a significant impact and is used to measure the knowledge specialization of the model. Omnibenchmark has substantial classes and samples, with diverse sample sources, which can effectively measure the model's knowledge generalization ability.

\subsection{Detail of Metrics}\label{detailofmetrics} 

We employ average final accuracy $A_N$ and average forgetting rate $F_N$ as metrics \cite{wang2022learning}. $A_N$ is the average final accuracy concerning all past classes over $N$ tasks. $F_N$ measures the performance drop across $N$ tasks. We use $Acc_i^t$ to denote the test accuracy of class $i$ after the completion of task $t$ and $Acc_i^{t_0}$ to denote the test accuracy of class $i$ after its first task $t_0$. Accordingly, $A_N$ and $F_N$ can be expressed as:
\begin{equation}
    A_N = \frac{\sum_{i\in{\sum_{t = 1}^T |CLS^t|}} Acc_i^T }{\sum_{t = 1}^T |CLS^t|},
\end{equation}
\begin{equation}
    F_N = \frac{\sum_{i\in{\sum_{t = 1}^T |CLS^t|}} (Acc_i^T - Acc_i^{t_0}) }{\sum_{t = 1}^T |CLS^t|}.
\end{equation}
It is worth mentioning that, unlike the evaluation metrics used in \cite{zhou2023learning,zhou2023revisiting,zhou2024expandable}, our metric ensures that each class has an equal evaluation weight, thereby avoiding increased sensitivity to previous tasks.
Following the protocol in \cite{lian2022scaling}, we use the Order-normalized Performance Disparity (OPD) metric to assess the robustness of the class order. OPD is calculated as the performance difference of task \( t \) across \( R \) random class orders, defined as:
\begin{equation}
    \text{OPD}_t = \max\{ \bar{A}_t^1, \ldots, \bar{A}_t^R \} - \min\{ \bar{A}_t^1, \ldots, \bar{A}_t^R \}.
\end{equation}
The Maximum OPD (MOPD) and Average OPD (AOPD) are further defined as:
\begin{equation}
    \text{MOPD} = \max\{\text{OPD}_1, \ldots, \text{OPD}_T\},
\end{equation}
\begin{equation}
    \text{AOPD} = \frac{1}{T} \sum_{t=1}^{T} \text{OPD}_t.
\end{equation}

\subsection{Training Details}\label{trainingdetails}

We followed the general setting in the continual learning community, i.e., randomly shuffled the session order for each dataset. 
The results presented throughout this paper are the mean results of two random shuffles.
We implement all experiments on one NVIDIA GeForce-RTX-3090 GPU and the Pytorch library. 
Input images are resized to 224 x 224 and normalized to the range of [0,1]. The hyperparameter settings for each baseline are set according to the optimal combination reported in their papers, respectively.
The Adam optimizer trains all Softmax-based models with a batch size of 128 and a learning rate of 0.05. 
The proposed SALF used ViT as the backbone, pre-trained on ImageNet-21k, with frozen parameters except for the classification header. 

Our contribution also includes faithful PyTorch implementations of our method and abundant baselines under the CL setting. 

\subsection{Baselines Description}
We compare our proposed SALF against a wide range of benchmarks on four widely used datasets to thoroughly validate it. SALF outperforms previous works, setting a new state-of-the-art performance.
We provide detailed descriptions of all the baselines:
\begin{itemize}
    \item \emph{Finetune} adjusts classifier weights through cross-entropy loss.
    \item \emph{L2P} selects prompts from the prompt pool using the key query matching strategy.
    \item \emph{DualPrompt} attach prompts to different layers to decompose prompts into universal and expert prompts.
    \item \emph{CODA-Prompt} builds attention-based prompts from the prompt pool.
    \item \emph{SimpleCIL} replace updated model classifier weights with class prototypes.
    \item \emph{ADAM} fine-tuning based on SimpleCIL.
    \item \emph{RanPAC} projects the feature space onto a higher dimensional space to approach a Gaussian distribution and eliminates mutual information between classes through the Gram matrix. 
\end{itemize}

\iffalse
\subsection{Additional Experimental Results}\label{additionalex}

\textbf{Model Selection for Class Group Matching} In the paper, we highlight that the matching of Class Groups determines the model's overall performance. Consequently, we conducted an experimental analysis using several classic, highly interpretable methods. The experiments were performed on the Stanford Dog dataset, where our task was to predict the Class Group of each test sample after partitioning with the SALF algorithm. The results are shown in \autoref{fig: model selection}. We observe that these classic methods achieved excellent performance, indicating that predicting based on the distance matrix between samples and prototypes is feasible. Additionally, due to the outstanding performance of the Random Forest method, we employed it for Class Group matching in our experiments.
\fi
\iffalse
\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/output.png}
    \caption{Accuracy of Class Group Matching (\%). A comparison was made between five classic algorithms: Decision Tree, MLP, LightGBM, KNN, and RandomForest.}
    \label{fig: model selection}
\end{figure}
\fi

\section{Proof of Theorem}\label{proof}

\subsection{Proof of Brooks’ Theorem}

\textbf{Proof}: 

Let $|V(G)| = n$, and we proceed by mathematical induction.

Firstly, when $n \leq 3$, the proposition holds.

Next, assuming the proposition holds for $n-1$, we aim to strengthen it step by step.

Without loss of generality, consider $\Delta(G)$-regular graphs, since non-regular graphs can be seen as obtained by removing some edges from regular graphs, which doesn't affect the conclusion.

For any regular graph $G$ that is neither complete nor an odd cycle, let's take a vertex $v$ and consider the subgraph $H:=G-v$. By the inductive hypothesis, we know that $\mathcal{X} (H) \leq \Delta(H) = \Delta(G)$. Now we only need to prove that inserting $v$ into $H$ does not affect the conclusion.

Let $\Delta:=\Delta(G)$, and suppose $H$ is colored with $\Delta$ colors: $c_1, c_2, ..., c_{\Delta}$. The $\Delta$ neighbors of $v$ are denoted as $v_1, v_2, ..., v_{\Delta}$. Without loss of generality, assume that these neighboring colors of $v$ are pairwise distinct; otherwise, the proposition holds.

Next, let's consider all the vertices colored with either $c_i$ or $c_j$ in $H$, and all edges between them, forming a subgraph $H_{i,j}$. Without loss of generality, assume that any two different vertices $v_i$ and $v_j$ are in the same connected component of $H_{i,j}$. Otherwise, if they were in different connected components, we could exchange the colors of all vertices in one of the connected components, making $v_i$ and $v_j$ have the same color.

We denote the aforementioned connected components as $C_{i,j}$, where $C_{i,j}$ must necessarily be a path from $v_i$ to $v_j$. Since the degree of $v_i$ in $H$ is $\Delta-1$, the neighboring colors of $v_i$ in $H$ must all be pairwise distinct. Otherwise, we could assign $v_i$ a different color, leading to a repetition of colors among its neighboring vertices. Hence, the number of neighboring vertices of $v_i$ in $C_{i,j}$ is 1, and the same applies to $v_j$. Now, within $C_{i,j}$, we choose a path from $v_i$ to $v_j$, denoted as $P$. If $C_{i,j} \neq P$, then we sequentially color the vertices along $P$. Let $u$ be the first vertex encountered with a degree greater than 2. Note that $u$'s neighboring vertices use at most $\Delta-2$ colors, allowing us to recolor $u$, thus ensuring $v_i$ and $v_j$ are not connected.

Next, it's not hard to see that for any three distinct vertices $v_i$, $v_j$, and $v_k$, $V(C_{i,j}) \cap V(C_{j,k}) = \{v_j\}$.

With this, our proposition has been sufficiently strengthened.

Now, the conclusion is straightforward. Firstly, if the neighboring vertices are pairwise adjacent, the proposition holds. Without loss of generality, suppose $v_1$ and $v_2$ are not adjacent. Take a neighboring vertex $w$ of $v_1$ in $C_{1,2}$ and exchange the colors along $C_{1,3}$. In the resulting graph, we have $w \in V(C_{1,2}) \cap V(C_{2,3})$, leading to a contradiction.
$\square$
% Hence, the proposition is proven.

\subsection{The Time Complexity of Welsh-Powel Algorithm}

\textbf{Proof:} 

For an undirected graph \( G \) without self-loops, let $ V(G) := \{v_1, \dots, v_n\}$ satisfy

\[ \deg(v_i) \geq \deg(v_{i+1}), \quad \forall 1 \leq i \leq n-1 \]

Define \( V_0 = \varnothing \), we take a subset \( V_m \) from \( V(G) \setminus \left(\bigcup_{i=0}^{m-1} V_i\right) \), where the elements satisfy

\[ v_{k_m} \in V_m, \quad \text{where } k_m = \min\{k : v_k \notin \bigcup_{i=0}^{m-1} V_i\} \]

If \[ \{v_{i_{m,1}}, v_{i_{m,2}}, \dots, v_{i_{m,l_m}}\} \subset V_m, \quad i_{m,1} < i_{m,2} < \dots < i_{m,l_m} \] then \( v_j \in V_m \) if and only if \[ j > i_{m,l_m} \]
\[ v_j \text{ is not adjacent to } v_{i_{m,1}}, v_{i_{m,2}}, \dots, v_{i_{m,l_m}} \]

If the points in \( V_i \) are colored with the \( i \)-th color, then this coloring scheme is the one provided by the Welsh–Powell algorithm. Obviously,

\[ V_1 \neq \varnothing \]
\[ V_i \cap V_j = \varnothing \quad \text{if } i \neq j \]
\[ \exists \alpha(G) \in \mathbb{N}^*, \forall i > \alpha(G), \text{ s.t. } V_i = \varnothing \]

We only need to prove:

\[ \bigcup_{i=1}^{\alpha(G)} V_i = V(G) \]

where

\[ \chi(G) \leq \alpha(G) \leq \max_{i=1}^n \min\{\deg(v_i) + 1, i\} \]

The inequality on the left-hand side is true; let's consider the right-hand side.

Firstly, it's not hard to derive:

If \( v \notin \bigcup_{i=1}^m V_i \), then \( v \) is adjacent to at least one point in each of \( V_1, V_2, \dots, V_m \), hence \( \deg(v) \geq m \).

Therefore, we have

\[ v_j \in \bigcup_{i=1}^{\deg(v_j)+1} V_i \]

On the other hand, based on the construction of the sequence \( \{V_i\} \), we can easily find that

\[ v_j \in \bigcup_{i=1}^j V_i \]

Combining the two equations yields the proof. $\square$


\subsection{The Proof of \autoref{Corollary: cor}}

\textbf{Proof: }

To show that an increase in \(\sum_{i,j=1}^T \lVert w_i^* - w_j^* \rVert^2\) is a sufficient condition for the reduction of \(\mathrm{Var}(\mathbb{E}[G_T])\) and \(\mathrm{Var}(\mathbb{E}[F_T])\), we proceed by demonstrating that increasing \(\sum_{i,j=1}^T \lVert w_i^* - w_j^* \rVert^2\) leads to a decrease in \(\mathrm{Var}(\sum_{i,j=1}^T \lVert w_i^* - w_j^* \rVert^2)\).

Let \(\mu = \frac{1}{T(T-1)} \sum_{i,j=1}^T \lVert w_i^* - w_j^* \rVert\) represent the mean of pairwise distances. Then, the variance is given by:
\[
\begin{aligned}
\text{Var}(\lVert w_i^* - w_j^* \rVert) &= \frac{1}{T(T-1)} \sum_{i,j=1}^T \Big( \lVert w_i^* - w_j^* \rVert^2 \\
&\quad - 2 \lVert w_i^* - w_j^* \rVert \mu + \mu^2 \Big)
\end{aligned}
\]


simplifying further to:
\[
\text{Var}(\lVert w_i^* - w_j^* \rVert) = \frac{1}{T(T-1)} \sum_{i,j=1}^T \lVert w_i^* - w_j^* \rVert^2 - \mu^2.
\]

Let \(S = \sum_{i,j=1}^T \lVert w_i^* - w_j^* \rVert\). As \(S\) increases, the mean \(\mu\) also increases, since \(\mu = \frac{S}{T(T-1)}\). For the variance to decrease with an increasing \(S\), it must hold that \(\sum_{i,j=1}^T \lVert w_i^* - w_j^* \rVert^2\) grows at a slower rate than \(S^2\), which happens when distances \(\lVert w_i^* - w_j^* \rVert\) become more uniform.

Thus, a more uniform distribution of \(\lVert w_i^* - w_j^* \rVert\) as \(S\) increases results in a decrease in variance. Hence, an increase in \(\sum_{i,j=1}^T \lVert w_i^* - w_j^* \rVert^2\) is indeed a sufficient condition for reducing \(\mathrm{Var}(\mathbb{E}[F_T])\) and \(\mathrm{Var}(\mathbb{E}[G_T])\), completing the proof. $\square$