\section{Related Work}


\textbf{Class-Incremental Learning (CIL)} necessitates a model that can continuously learn new classes while retaining knowledge of previously learned ones \cite{dohare2024loss,cao2023retentive,zhou2023revisiting,zhou2024continual}, which can be roughly divided into several categories. 
Regularization-based methods incorporate explicit regularization terms into the loss function to balance the weights assigned to new and old tasks \cite{kirkpatrick2017overcoming,aljundi2018memory,wang2022continual,li2017learning}.
Replay-based methods address the problem of catastrophic forgetting by replaying data from previous classes during the training of new ones. This can be achieved by either directly using complete data from old classes \cite{lopez2017gradient,riemer2018learning,cha2021co2l,wang2022foster} or by generating samples \cite{shin2017continual,zhu2022self}, such as employing GANs to synthesize samples from previous classes \cite{cong2020gan,liu2020generative}.
Dynamic network methods adapt to new classes by adjusting the network structure, such as adding neurons or layers, to maintain sensitivity to previously learned knowledge while acquiring new tasks. This approach allows the model's capacity to expand based on task requirements, improving its ability to manage knowledge accumulation in CIL \cite{wang2022coscl,wang2023incorporating,aljundi2017expert,ostapenko2021continual}.
Recently, CIL methods based on pre-trained models (PTMs)
\cite{cao2023retentive,chen2022adaptformer,zhou2024continual} have demonstrated promising results.
Prompt-based methods utilize prompt tuning \cite{jia2022visual} to facilitate lightweight updates to PTMs. By keeping the pre-trained weights frozen, these methods preserve the generalizability of PTMs, thereby mitigating the forgetting in CIL \cite{smith2023coda,wang2022dualprompt,NEURIPS2023_d9f8b5ab,wang2022learning,wang2022dualprompt,smith2023coda,li2024learning}. 
Model mixture-based methods mitigate forgetting by saving models during training and integrating them through model ensemble or model merge techniques \cite{gao2023unified,wang2023isolation,wang2024hierarchical,zheng2023preventing,zhou2023learning,zhou2024expandable}.
Prototype-based methods draw from the concept of representation learning \cite{ye2017learning}, leveraging the robust representation capabilities of PTMs for classification with NCM classifiers \cite{panos2023first,zhou2023revisiting,mcdonnell2024ranpac}.

\textbf{The Order in CIL} remains a significant and unresolved challenge \cite{wang2024comprehensive}. APD \cite{yoon2019scalable} effectively addresses the problem of CF by decomposing model parameters into task-shared and sparse task-specific components, thereby enhancing the model's robustness to changes in class order. HALRP \cite{li2024hessian}, on the other hand, simulates parameter conversion in continuous tasks by applying low-rank approximation to task-adaptive parameters at each layer of the neural network, thereby improving the model's order robustness. However, the optimization strategies employed by these methods are confined to the network architecture itself and do not fundamentally resolve the underlying issues.
Recent theoretical analyses of CIL \cite{lin2023theory,shan2024order,bell2022effect,wu2021curriculum} indicate that prioritizing the learning of tasks (or classes) with lower similarity enhances the model's generalization and resistance to forgetting. Building on these theories, we conducted further research and developed corresponding methods in the following sections.
