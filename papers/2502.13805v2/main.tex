% VLDB template version of 2020-08-03 enhances the ACM template, version 1.7.0:
% https://www.acm.org/publications/proceedings-template
% The ACM Latex guide provides further information about the ACM template
\documentclass[sigconf, nonacm]{acmart}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\usepackage{enumitem}  % 用于自定义列表样式
\usepackage{titlesec}  % 控制章节标题样式
\usepackage{hyperref}  % 处理超链接（如果有）
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{textcomp}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\DeclareMathOperator*{\argmin}{argmin}

\lstset{language=SQL,
  basicstyle={\small\ttfamily},
  belowskip=3mm,
  breakatwhitespace=true,
  breaklines=true,
  classoffset=0,
  columns=flexible,
  commentstyle=\color{dkgreen},
  framexleftmargin=0.25em,
  frameshape={}{}{}{}, %To remove to vertical lines on left, set `frameshape={}{}{}{}`
  keywordstyle=\color{blue},
  morekeywords={PROMPT, TABULAR, SEM_MATCH, SEM_GROUP, FILE, DIRECTORY},
  numbers=none, %If you want line numbers, set `numbers=left`
  numberstyle=\tiny\color{gray},
  showstringspaces=false,
  stringstyle=\color{mauve},
  tabsize=3,
  xleftmargin =1em
}

%% The following content must be adapted for the final version
% paper-specific
\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
% issue-specific
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{https://github.com/wotchin/AnDB/tree/with-semantic}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{plain} 

% add by ourselves
\newcommand{\hi}[1]{\vspace{.25em}\noindent{\bf {#1}}\xspace}


\begin{document}
\title{AnDB: Breaking Boundaries with an AI-Native Database for Universal Semantic Analysis}

%%
%% The "author" command and its associated commands are used to define the authors and their affiliations.
\author{Tianqing Wang}
\affiliation{%
  \institution{Huawei}
  \city{Toronto}
  \state{Canada}
}
\email{wangtianqing2@huawei.com}

\author{Xun Xue}
\affiliation{%
  \institution{Huawei}
  \city{Toronto}
  \state{Canada}
}
\email{xun.xue@huawei.com}

\author{Guoliang Li}
\affiliation{%
  \institution{Tsinghua University}
  \city{Beijing}
  \state{China}
}
\email{liguoliang@tsinghua.edu.cn}

\author{Yong Wang}
\affiliation{%
  \institution{Huawei}
  \city{Hangzhou}
  \state{China}
}
\email{wangyong308@huawei.com}

% \author{Alice}
% \orcid{0000-0002-1825-0097}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}
% }
% \email{larst@affiliation.org}

% \author{Bob}
% \orcid{0000-0001-5109-3700}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }
% \email{vb@rocquencourt.com}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
In this demonstration, we present AnDB, an AI-native database that supports traditional OLTP workloads and innovative AI-driven tasks, enabling unified semantic analysis across structured and unstructured data. While structured data analytics is mature, challenges remain in bridging the semantic gap between user queries and unstructured data. AnDB addresses these issues by leveraging cutting-edge AI-native technologies, allowing users to perform semantic queries using intuitive SQL-like statements without requiring AI expertise. This approach eliminates the ambiguity of traditional text-to-SQL systems and provides a seamless end-to-end optimization for analyzing all data types. AnDB automates query processing by generating multiple execution plans and selecting the optimal one through its optimizer, which balances accuracy, execution time, and financial cost based on user policies and internal optimizing mechanisms. AnDB future-proofs data management infrastructure, empowering users to effectively and efficiently harness the full potential of all kinds of data without starting from scratch.
\end{abstract}

\maketitle

%%% do not modify the following VLDB block %%
%%% VLDB block start %%% 
\pagestyle{\vldbpagestyle}
\begingroup\small\noindent\raggedright\textbf{PVLDB Reference Format:}\\
\vldbauthors. \vldbtitle. PVLDB, \vldbvolume(\vldbissue): \vldbpages, \vldbyear.\\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}
\endgroup
\begingroup
\renewcommand\thefootnote{}\footnote{\noindent
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing \href{mailto:info@vldb.org}{info@vldb.org}. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \\
\raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
ISSN 2150-8097. \\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi} \\
}\addtocounter{footnote}{-1}\endgroup
%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\ifdefempty{\vldbavailabilityurl}{}{
\vspace{.3cm}
\begingroup\small\noindent\raggedright\textbf{PVLDB Artifact Availability:}\\
The source code, data, and/or other artifacts have been made available at \url{\vldbavailabilityurl}.
\endgroup
}
%%% VLDB block end %%%

\section{INTRODUCTION}

% In recent years, the data landscape has undergone a significant transformation with the advent of data lakehouses—a hybrid architecture that combines the scalability of data lakes with the performance and management features of data warehouses. This evolution has enabled organizations to store both structured and unstructured data within a unified platform, facilitating comprehensive data analysis and decision-making. ~\cite{Zaharia2021LakehouseAN} However, while data lakehouses excel at data storage and management, they often fall short in effectively analyzing unstructured data, which remains a critical component of modern data ecosystems. Also, the landscape of unstructured data and its utilization has significantly evolved, yet it remains in its early stages of maturity. According to analyst firm IDC ~\cite{IDC_report}, 90\% of global data is unstructured and it is full of untapped value ~\cite{BOXblog90}, encompassing diverse formats such as text, images, audio, video, and social media content. Despite its growing prevalence, traditional data management systems struggle to handle the inherent complexity and heterogeneity of these data sources. This often results in data silos, operational inefficiencies, and missed opportunities to extract actionable insights, underscoring the critical need for more advanced and unified solutions.

% The rapid development of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has introduced promising avenues to bridge the gap between semantic queries and unstructured data. Techniques such as Retrieval-Augmented Generation (RAG) have emerged to enhance the capabilities of LLMs by integrating external knowledge sources, thereby reducing issues like hallucinations and improving the accuracy of AI-generated content. Despite these advancements, RAG systems exhibit several limitations, including a primary focus on retrieval, filtering, and ranking, with insufficient support for high-level data analysis operations such as aggregation and joins. Additionally, the reliance on user input in natural language interfaces often leads to ambiguities, as users may not provide precise or unambiguous queries. Furthermore, the underlying semantics of many RAG systems are based on "select-where-sort" operations, which do not fully leverage the potential of natural language inputs for optimization.

% To address these challenges, recent research has explored the integration of traditional database methodologies with AI technologies. Notably, the "LOTUS" system ~\cite{patel2024semanticoperators} introduced semantic operators like semantic join, semantic group, and semantic top-K, aiming to enhance the analytical capabilities of AI models. Similarly, "PALIMPSEST" ~\cite{LiuPalimpzestOA} proposed a system that enables users to process AI-powered analytical queries through a declarative language, further bridging the semantic gap between unstructured data and traditional database functionalities. These initiatives underscore the need for a robust framework combining the strengths of data lakehouses, AI advancements, and established database principles to facilitate comprehensive analysis of structured and unstructured data. However, the existing solutions still face challenges in accurately understanding user requirements, generating logical execution plans from declarative statements, and optimizing these plans to balance accuracy, cost, and execution time. For instance, the "PALIMPSEST" system employs a declarative approach but still relies on user programming, lacks a comprehensive set of candidate execution plans, and does not provide users with the ability to browse and fine-tune specific execution plans. 

% In this demonstration, AnDB addresses the limitations of existing solutions by offering an intuitive SQL-like interface through its \texttt{SQL Engine} component, enabling users to analyze both structured and unstructured data with ease. Its \texttt{Query Optimization} component generates multiple candidate execution plans, with the built-in optimizer selecting the optimal plan. After execution by the \texttt{Execution Engine}, users receive relational results, facilitating an understanding of analysis outcomes regardless of data structure. Additionally, AnDB incorporates a \texttt{Calibration} component that estimates the accuracy of the execution results and gauges core metrics (e.g., execution time, input/output tokens), feeding back into the \texttt{Query Optimization} component to enhance future comprehensive performance.

In recent years, the data landscape has evolved with the emergence of data lakehouses, a hybrid architecture combining the scalability of data lakes with the performance and management features of data warehouses ~\cite{Zaharia2021LakehouseAN}. However, despite their strengths in data storage, data lakehouses struggle to effectively analyze unstructured data, which constitutes 90\% of global data ~\cite{IDC_report} and remains a largely untapped resource ~\cite{BOXblog90}. Unstructured data, encompassing text, images, audio, and video, presents significant challenges due to its complexity and heterogeneity, often leading to data silos, inefficiencies, and missed opportunities for actionable insights ~\cite{Madden2024DatabasesUQ}.

The rapid advancement of Artificial Intelligence (AI), particularly Large Language Models (LLMs), offers promising solutions to bridge the gap between semantic queries and unstructured data. Techniques like Retrieval-Augmented Generation (RAG) enhance LLMs by integrating external knowledge, reducing hallucinations, and improving accuracy ~\cite{rag2020}. However, RAG systems are limited by their focus on retrieval, filtering, and ranking, with inadequate support for advanced operations like aggregation and join. %Additionally, natural language interfaces often suffer from ambiguities, as users may not provide precise queries, and the "select-where-sort" RAG formulations fail to optimize natural language inputs fully.

Recent research has sought to integrate traditional database methodologies with AI technologies. For instance, the "LOTUS" system ~\cite{patel2024semanticoperators} introduces semantic operators (e.g., semantic join, semantic group) to enhance AI-driven analysis, while "PALIMPSEST" ~\cite{LiuPalimpzestOA} enables LLM-powered queries through a declarative process. Notwithstanding these advancements, significant challenges remain in the precise interpretation of user requirements, and the implementation of effective feedback mechanisms for system calibration. A case in point is the "PALIMPSEST" system, which necessitates user programming intervention, demonstrates inadequate coverage in its execution plan generation, and disregards conventional optimization strategies for data scan operations, etc.

In this demonstration, AnDB addresses these limitations by providing an intuitive SQL-like interface through its \texttt{SQL Engine}, enabling seamless analysis of structured and unstructured data. The \texttt{Query Optimization} component generates multiple execution plans, with an optimizer selecting the most efficient one. Results are delivered relationally via the \texttt{Execution Engine}, ensuring clarity regardless of data structure. Additionally, the \texttt{Calibration} component estimates result accuracy and core metrics (e.g., execution time, token usage), feeding calibration information back into the optimizer to improve future performance.

\section{System Overview}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/component_diagrams.png}
    \caption{AnDB Architecture Overview: the comprehensive workflow of the AnDB system}
    \label{fig:AnDB_Architecture}
\end{figure}

As illustrated in \autoref{fig:AnDB_Architecture}, AnDB comprises several core components, including the \texttt{SQL engine}, \texttt{query optimization engine}, \texttt{execution engine}, \texttt{storage component}, and auxiliary modules such as the \texttt{model management} and \texttt{Knowledge Graph (KG) module}. Users can seamlessly interact with AnDB using SQL-like statements for unstructured data analysis and standard SQL for relational data queries.

% Thanks to its database-centric architecture, SQL-like interface, and relational result sets, AnDB is fully compatible with the PostgreSQL wire protocol. This compatibility allows users to use any PostgreSQL client to interact with AnDB, significantly reducing the cost of adoption and the learning curve.

% To put it into a user perspective, AnDB operates as a black-box solution, eliminating the need to worry about low-level details such as unstructured data storage, precise query language input, data governance workflow optimization, client software installation, or integration with existing software architectures. In essence, AnDB provides a comprehensive, all-in-one solution—everything you need for modern data management and analysis.

\subsection{SQL Engine}
\hi{SQL Grammar Tokens.} In addition to standard SQL statement tokens, AnDB introduces additional tokens to express user requirements for semantic analysis tasks accurately. In \autoref{tab:tokens}, these tokens are categorized into Semantic Tokens and Auxiliary Tokens, enabling end-to-end semantic analysis with enhanced flexibility and precision.

% \setlength{\tabcolsep}{4pt} % default 6pt
% \begin{table}[t]
%   \caption{Additional SQL Grammar Tokens.}
%   \label{tab:tokens}
%   \centering
%   {\small
%   \begin{tabular}{>{\ttfamily}l>{\ttfamily}l>{\raggedright\arraybackslash}p{5cm}}
%     \toprule
%     Token & Category & Description \\
%     \midrule
%     \verb|PROMPT| & semantic & Specifies semantic requirements based on the user's prompt text. Applicable in SELECT, WHERE, FROM clauses. \\
%     \verb|SEM_MATCH| & semantic & Incorporates semantic matching logic and conditional parameters, facilitating advanced querying capabilities beyond traditional exact matches. Applicable in JOIN\_ON, WHERE, HAVING clauses. \\
%     \verb|SEM_GROUP| & semantic & Prompts for Semantic Aggregation. Applicable in SELECT, GROUP\_BY. \\
%     \verb|TABULAR| & auxiliary & Enables users to define a custom structured schema for unstructured data, bridging the gap between schema-less data and structured querying. Applicable in FROM clause. \\
%     \verb|FILE| & auxiliary & Specifies a path of the unstructured file for processing. Applicable in FROM clause. \\
%     \verb|DIRECTORY| & auxiliary & Specifies a directory containing multiple unstructured files, treating them as a unified dataset for batch processing. Applicable in FROM clause. \\
%     \bottomrule
%   \end{tabular}
%   }
% \end{table}

\setlength{\tabcolsep}{4pt} % default 6pt
\begin{table}[t]
  \caption{Additional SQL Grammar Tokens.}
  \label{tab:tokens}
  \centering
  {\small
  \begin{tabular}{>{\ttfamily}l>{\ttfamily}l>{\raggedright\arraybackslash}p{4.5cm}}
    \toprule
    Token & Category & Description \\
    \midrule
    \verb|PROMPT| & semantic & Specifies semantic requirements based on user prompts. Applicable in SELECT, WHERE. \\
    \verb|SEM_MATCH| & semantic & Enables semantic and approximate matching with conditional parameters. Applicable in JOIN\_ON, WHERE, HAVING. \\
    \verb|SEM_GROUP| & semantic & Prompts for semantic aggregation. Applicable in SELECT, GROUP\_BY. \\
    \verb|TABULAR| & auxiliary & Defines a custom schema for unstructured data. Applicable in FROM. \\
    \verb|FILE| & auxiliary & Specifies a file name for unstructured data. Applicable in FROM. \\
    \verb|DIRECTORY| & auxiliary & Specifies a directory name for batch processing of unstructured files. Applicable in FROM. \\
    \bottomrule
  \end{tabular}
  }
\end{table}

% \hi{Logical Plan.} In the database domain, the logical plan provides a high-level overview of the execution strategy but typically lacks detailed implementation specifics, such as the choice of sorting algorithms, whether to scan B+ tree index pages, or whether to perform a sequential scan of an entire table. In contrast, the physical plan contains these finer-grained details, reflecting decisions made by the query optimization module.

% Relational data processing in AnDB adheres strictly to traditional relational concepts, ensuring compatibility and consistency with existing database systems. Semantic tokens in AnDB are seamlessly transformed into logical plans, enabling the natural integration of unstructured data query semantics into standard database operators. To support this, AnDB introduces a key new logical operator called \textbf{Transform} which resembles a map operation but with expansion/contraction capabilities. Unlike the MapReduce paradigm, where map operations typically perform transformations within the same dimensionality, AnDB's \textbf{Transform} operator can alter dimensionality based on user-provided prompts. This flexibility distinguishes it from traditional map operations and justifies the adoption of a new conceptual framework. By blending unstructured data semantics into existing database operators and introducing the \textbf{Transform} operator, AnDB achieves a unified and extensible approach to explaining the process of both structured and unstructured data.

% \hi{Physical Plan.} In the previous paragraph, we elaborated on the differences between logical and physical execution plans. AnDB seamlessly integrates prompt requirements into conventional relational operators, leveraging their natural flexibility and extensibility. However, one of the most challenging tasks in this design is creating efficient physical operators to implement the same logical plan. For instance, a logical join operator can be realized through various physical implementations, such as nested loop join, hash join, and sorted merge join. Additionally, since multiple execution paths may be semantically equivalent, the ability to provide diverse physical operator implementations is critical for enabling the query optimization component to select the most efficient execution strategy.

\hi{Logical Plan.} AnDB processes relational data in strict adherence to traditional relational concepts, ensuring compatibility with existing systems. Semantic tokens in AnDB are internally translated into logical plans, naturally bridging the gap between unstructured data query semantics and standard database operators. To achieve this, AnDB introduces several novel operators, e.g., \textbf{Transform}, which resembles a map operation but supports flexible schema based on user prompts. Unlike traditional map operations in MapReduce, which preserve dimensionality, the \textbf{Transform} operator can expand or contract relation dimensions, necessitating a new conceptual framework. 

Particularly, systems that rely exclusively on isolated prompt fragments while failing to incorporate global contextual information consistently exhibit coherence deficiencies. In contrast, AnDB addresses this limitation by comprehensively considering all prompt text parameters (e.g., PROMPT, SEM\_GROUP, and SEM\_MATCH), better understanding user requirements.
% In database systems, a logical plan outlines the high-level execution strategy without specifying implementation details, such as sorting algorithms, index usage, or scan methods. In contrast, a physical plan incorporates these granular details, reflecting decisions made during query optimization.In database systems, a logical plan outlines the high-level execution strategy without specifying implementation details, such as sorting algorithms, index usage, or scan methods. In contrast, a physical plan incorporates these granular details, reflecting decisions made during query optimization.

\hi{Physical Plan.} AnDB integrates prompt requirements into conventional relational operators, leveraging their inherent flexibility. However, a key challenge lies in designing efficient and effective physical operators to implement logical plans. For example, a logical join operator can be implemented through multiple physical operators, such as nested-loop-join, hash-join, and sorted-merge join. As semantically equivalent execution paths, providing diverse physical operator implementations is essential to support the query optimizer in selecting an optimal execution plan.

\autoref{tab:logical_physical_operators} highlights some key physical operators corresponding to their logical operators, demonstrating how AnDB tackles the issue while maintaining flexibility and performance.

% \begin{table*}[htbp]
%   \centering
%   \caption{Logical Operators and Corresponding Physical Operators}
%   \label{tab:logical_physical_operators}
%   \renewcommand{\arraystretch}{1.2} 
%   \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2.5cm} l X}
%     \toprule
%     Logical Operator    & Physical Operator                     & Description \\
%     \midrule
%     \multirow{4}{*}{Scan}       
%                         & SemanticScan                          & Scans unstructured data and extracts it into a tabular format based on SQL context and semantics. \\
%                         & FileScan                              & Auxiliary operator: Reads raw bytes of unstructured data directly from storage. \\
%                         & DirectoryScan                         & Auxiliary operator: Reads raw bytes of unstructured data from a directory as a unified dataset. \\
%                         & EmbeddingScan                         & Scans the child operator's data and embeds it into a vector using an embedding model. \\
%     \midrule
%     \multirow{2}{*}{Transform} 
%                         & SemanticTransform                     & Transforms the child operator's data using an LLM and user-provided or built-in prompts. \\
%                         & CodeExecution                         & Generates executable code via LLM based on the operator's target, runs it on child data, and passes results to the parent operator. \\
%     \midrule
%     \multirow{4}{*}{Join} 
%                         & NestedLoopJoin (vector similarity)    & Performs nested loop join using vector similarity for column comparisons. \\
%                         & HashJoin (vector similarity)          & Performs hash join using vector similarity for column comparisons. \\
%                         & SortedMergeJoin (vector similarity)   & Performs sorted-merge join using vector similarity for column comparisons. \\
%                         & SemanticJoin                          & Compares two columns' values by interacting with an LLM. \\
%     \midrule
%     \multirow{1}{*}{Group} 
%                         & SemanticCluster                       & Groups data using unsupervised clustering algorithms. \\
%     \midrule
%     \multirow{1}{*}{Filter} 
%                         & SemanticFilter                        & Filters out irrelevant data by interacting with an LLM. \\
%     \bottomrule
%   \end{tabularx}
% \end{table*}

\begin{table*}[htbp]
  \centering
  \caption{Logical Operators and Corresponding Physical Operators}
  \label{tab:logical_physical_operators}
  \renewcommand{\arraystretch}{1.2}
  \small % 缩小字体
  \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{3cm} X}
    \toprule
    Logical Operator & Physical Operator & Description \\
    \midrule
    \multirow{4}{*}{Scan}       
    & SemanticScan & Scans unstructured data and extracts it into a tabular format based on SQL context and semantics. \\
    & FileScan & Reads raw bytes of unstructured data directly from storage. \\
    & DirectoryScan & Reads raw bytes of unstructured data from a directory as a unified dataset. \\
    & EmbeddingScan & Scans the child operator's data and embeds it into a vector using an embedding model. \\
    \midrule
    \multirow{2}{*}{Transform} 
    & SemanticTransform & Transforms the child operator's data using an LLM and user-provided or built-in prompts. \\
    & CodeExecution & Generates executable code via LLM based on the operator's target, runs it on child data, and passes results to the parent operator. \\
    \midrule
    \multirow{4}{*}{Join} 
    & NestedLoopJoin (vector similarity) & Performs nested loop join using vector similarity for value comparisons. \\
    & HashJoin (vector similarity) & Performs hash join using vector similarity for value comparisons. \\
    & SortedMergeJoin (vector similarity) & Performs sorted-merge join using vector similarity for value comparisons. \\
    & SemanticJoin & Compares two columns' values by interacting with an LLM. \\
    \midrule
    Group & SemanticCluster & Groups data using unsupervised clustering algorithms. \\
    \midrule
    Filter & SemanticFilter & Filters out irrelevant data by interacting with an LLM. \\
    \bottomrule
  \end{tabularx}
\end{table*}

In addition to the direct one-to-one mappings presented in \autoref{tab:logical_physical_operators}, AnDB also supports the implementation of logical operators through a sequence of physical operators, offering enhanced flexibility and optimization potential. For instance, in the case of the join operation, the process begins with applying \textbf{SemanticTransform} to reformat and normalize the two tables into a unified schema. Subsequently, traditional join methods, such as the nested-loop-join, are applied for the tabular tuples, mirroring the approach used in conventional databases.

% \begin{enumerate}[label=\arabic*.]
%     \item \textbf{Join}:
%     \begin{itemize}
%         \item First, apply \textbf{SemanticTransform} to reformat and normalize the two tables into a unified schema.
%         \item Then, use traditional join methods (e.g., nested loop join) to combine the data sources, similar to a traditional database.
%     \end{itemize}
    
%     \item \textbf{Group}:
%     \begin{itemize}
%         \item Begin with \textbf{SemanticScan} to transform unstructured data into tabular format with a fixed schema.
%         \item Follow with traditional group operators (e.g., hash aggregation) to perform the grouping.
%     \end{itemize}

%     \item \textbf{Filter}:
%     \begin{itemize}
%         \item Use \textbf{SemanticScan} to tabulate unstructured data.
%         \item Eliminate mismatching tuples directly or employ an embedding operation followed by an \textbf{Approximate Nearest Neighbor (ANN)} search to select tuples that are close in feature space.
%     \end{itemize}

%     \item \textbf{Sort-Limit}:
%     \begin{itemize}
%         \item For top-K scenarios, traditional databases typically use heap sort. However, for unstructured data:
%         \begin{itemize}
%             \item Vectorize the data first, then use ANN to select the top-K nearest neighbors.
%             \item Alternatively, directly query the LLM for each data snippet or tuple (after tabulation) to determine the top-K results.
%         \end{itemize}
%     \end{itemize}

%     \item \textbf{Other Auxiliary Operations}:
%     \begin{itemize}
%         \item Implicitly create vector indexes for unstructured data.
%         \item Chunk large documents into manageable pieces.
%         \item Generate text summaries for multimedia files.
%     \end{itemize}
% \end{enumerate}

% \subsection{Query Optimization}
% The query optimization component in AnDB converts a logical plan into an executable physical plan, selecting the optimal one from all candidates. 
% The concept of "comprehensively optimal" in AnDB is complex, as it must account for semantic retrieval involving LLMs and domain-specific models, which incur significant inference costs, especially with LLMs, where token usage directly impacts expenses. To address these, 

\subsection{Query Optimization}

%Many semantic query engine, like ""Regarding LLM, there is an intrinsic connection between token usage and computational cost. However, AnDB argues that makes a simple question more complicated. Hence, AnDB introduces a novel approach for balancing financial cost, accuracy, runtime, user policy, and calibration by a unified cost model. Since financial cost is strongly correlated with execution time, which can be regarded as an intermediate factor, AnDB proposes a cost model to formalize and simplify the trade-off among all considered factors, 

Although several innovations, like "PALIMPSEST", propose balancing financial cost, accuracy, and runtime, we contend that runtime is merely an intermediate factor, as the financial cost strongly correlates with end-to-end execution time. Therefore, we adopt a "less-is-more" strategy to redefine the problem. Notably, we introduce a normalized cost model that defines the trade-off between these factors, which is interpretable and natural to integrate with the existing relational database optimizer. Specifically, we define the accuracy-cost matrix and token vector as follows.

Let \( \mathbf{M} \) represent the accuracy-cost matrix, where the elements correspond to the coefficients for accuracy (\( A \)) and cost (\( C \)) for both input and output tokens:

\[
\mathbf{M} = \begin{bmatrix}
A_{\text{input}} & A_{\text{output}} \\
C_{\text{input}} & C_{\text{output}}
\end{bmatrix}
\]

We also define the token vector \( \mathbf{t} \) as the number of input and output tokens processed by a semantic operator \( o \):

\[
\mathbf{t} = [ t_{\text{input}}, t_{\text{output}}]
\]

Given that semantic operators process \( N \) data tuples, the cost-accuracy trade-off for a semantic operator \( o \) is formalized as:

\begin{equation}
\begin{split}
\Gamma(o) &= \underbrace{C_{\text{input}} \cdot t_{\text{input}} + C_{\text{output}} \cdot t_{\text{output}}}_{\text{Cost Term}} \\
&\quad + \lambda \cdot \underbrace{\left(\frac{1}{A_{\text{input}} \cdot t_{\text{input}} + A_{\text{output}} \cdot t_{\text{output}}}\right)}_{\text{Accuracy Loss Term}}
\end{split}
\end{equation}

where \( \lambda \in [0, +\infty) \) is a tuning parameter that controls the trade-off between cost and accuracy. A higher value of \( \lambda \) prioritizes accuracy, while a lower value emphasizes cost reduction.

The core idea of this formulation is to explicitly minimize the weighted total cost while introducing accuracy as a loss term into the optimization objective, which penalizes low accuracy resulting from cost reduction. This design aligns more closely with the traditional database optimization logic. Hence, the optimization of a hybrid execution plan, \( P \), combining both semantic and traditional operators, where \( \chi(o) \) is the execution cost of a traditional operator \( o \), is then formulated as:

\begin{equation}
\argmin_{P \in \mathcal{P}} \left( \sum_{o \in P_{\text{semantic}}} \Gamma(o) + \sum_{o \in P_{\text{traditional}}} \chi(o) \right)
%\argmin_{P \in \mathcal{P}} \left( \sum_{o \in P_{\text{semantic}}} \Gamma(o) + \sum_{o \in P_{\text{traditional}}} \Chi(o) \right)
%\argmin_{P \in \mathcal{P}} \sum_{o \in P} \Gamma(o) + \text{TraditionalOperatorCost}(P)
\end{equation}

By using cost models, AnDB can select an execution plan that minimizes financial costs while achieving the desired level of accuracy. % This offers a more efficient and cost-effective approach to database query optimization.

% As shown in Algorithm \autoref{alg:execution_plan_selection}, AnDB treats query optimization as a combinatorial problem. Traditional database cost estimation is already complex, and adding LLM token costs and accuracy trade-offs further complicates it. AnDB resolves this by enumerating feasible plans, optimizing runtime via cost-based methods, and evaluating Total Cost of Ownership (TCO), focusing on LLM token usage. Accuracy preference is assessed through a calibration module. The total cost is derived by integrating these factors, and the plan achieving Pareto optimality for the user’s given strategy. Post-execution, core metrics, including accuracy, execution time, and LLM token usage, are monitored for feedback on the cost model.

% \begin{algorithm}
% \caption{Execution Plan Selection Algorithm}
% \label{alg:execution_plan_selection}
% \begin{algorithmic}[1]
% \State \textbf{Input:} Query, UserPolicy
% \State \textbf{Output:} SelectedPlan
% \State $Plans \gets \text{GenerateCandidatePlans(Query)}$
% \State $EstimatedCosts \gets \{\}$
% \For{each $Plan \in Plans$}
%     \State $Runtime \gets \text{EstimateRuntime(Plan)}$
%     \State $TCO \gets \text{EstimateTCO(Plan)}$
%     \State $Accuracy \gets \text{EstimateAccuracy(Plan)}$
%     \State $EstimatedCosts[Plan] \gets (Runtime, TCO, Accuracy)$
% \EndFor
% \State $ParetoOptimalPlans \gets \text{IdentifyParetoOptimalPlans(EstimatedCosts)}$
% \State $SelectedPlan \gets \text{ApplyUserPolicy(ParetoOptimalPlans, UserPolicy)}$
% \State \text{Execute(SelectedPlan)}
% \State \text{MonitorPerformance(SelectedPlan)}
% \State \text{UpdateCostModels(ObservedPerformance)}
% \State \Return SelectedPlan
% \end{algorithmic}
% \end{algorithm}

% \section{Optimization Model for Cost-Accuracy Trade-off in LLMs}
% \label{sec:model}

% Consider a language model where input and output tokens influence both accuracy and cost. Let:
% \begin{itemize}
%     \item $t_{\text{in}}, t_{\text{out}} \in \mathbb{R}^+$: token counts for input and output
%     \item $\theta_{\text{in}}(t_{\text{in}}), \theta_{\text{out}}(t_{\text{out}}) \in [0,1]$: accuracy functions (monotonically increasing)
%     \item $\beta_{\text{in}}(t_{\text{in}}), \beta_{\text{out}}(t_{\text{out}}) \in \mathbb{R}^+$: cost functions (monotonically increasing)
% \end{itemize}

% \subsection{Objective Function}
% We formulate a multi-objective optimization problem to maximize accuracy while minimizing cost. Using weighted scalarization, the combined objective is:

% \begin{equation}
%     \max_{t_{\text{in}}, t_{\text{out}} \underbrace{w_\theta \left[ \theta_{\text{in}}(t_{\text{in}}) + \theta_{\text{out}}(t_{\text{out}}) \right]}_{\text{Accuracy Term}} - \underbrace{w_\beta \left[ \beta_{\text{in}}(t_{\text{in}}) + \beta_{\text{out}}(t_{\text{out}}) \right]}_{\text{Cost Term}}
%     \label{eq:objective}
% \end{equation}

% where $w_\theta, w_\beta > 0$ are preference weights for accuracy and cost respectively.

% \subsection{Functional Assumptions}
% We adopt the following parameterized forms:

% \begin{align}
%     \theta_i(t_i) &= 1 - e^{-k_{\theta_i} t_i}, \quad i \in \{\text{in}, \text{out}\} \\
%     \beta_i(t_i) &= c_{\beta_i} t_i, \quad i \in \{\text{in}, \text{out}\}
% \end{align}

% where $k_{\theta_i} > 0$ controls accuracy saturation rates and $c_{\beta_i} > 0$ represents unit token costs.

% \subsection{Optimality Conditions}
% First-order derivatives yield closed-form solutions:

% \begin{align}
%     t_{\text{in}}^* &= -\frac{1}{k_{\theta_{\text{in}}}} \ln\left(\frac{w_\beta c_{\beta_{\text{in}}}}{w_\theta k_{\theta_{\text{in}}}}\right) \\
%     t_{\text{out}}^* &= -\frac{1}{k_{\theta_{\text{out}}}} \ln\left(\frac{w_\beta c_{\beta_{\text{out}}}}{w_\theta k_{\theta_{\text{out}}}}\right)
% \end{align}

% \subsection{Feasibility Constraints}
% Solutions require positive token counts:

% \begin{equation}
%     \frac{w_\beta c_{\beta_i}}{w_\theta k_{\theta_i}} < 1, \quad \forall i \in \{\text{in}, \text{out}\}
% \end{equation}

% This ensures the arguments of logarithmic functions remain positive, guaranteeing $t_i^* > 0$.


\subsection{Execution Engine}
LLM invocations primarily rely on API integration. However, if local deployment is required, there is a lot of significant potential for optimization in areas such as resource scheduling, GPU utilization, and in-memory data caching. Additionally, AnDB has a lightweight knowledge graph module, inspired by GraphRAG ~\cite{edge2024localglobalgraphrag}, to aid in reasoning, as graphs effectively represent entity relationships. The execution engine can retrieve structured, semi-structured, and unstructured data from disk. A key optimization involves indexing unstructured files, such as extracting keywords from images using multimodal models (e.g., FUYU-8b~\cite{li2023otterhdhighresolutionmultimodalitymodel}) and preprocessing documents (e.g., chunking) before storage because performing these tasks during execution would be inefficient.

\subsection{Calibration}
The calibration component parallels the reflection and self-critique aspects of the agentic AI framework, which integrates two approaches: (1) \underline{User-based:} This approach utilizes accuracy feedback provided by the user following query execution. 
% It is both simple and effective, as it directly captures the user's preferences. However, it lacks automation, as it relies on manual intervention, limiting its intelligence for the user. 
(2) \underline{Model-based:} This method employs cross-validation across multiple LLMs and built-in rules, offering a higher level of automation. 
% However, it incurs greater costs, and as a result, it is used sparingly, primarily for spot checks rather than being applied to all queries. Instead, AnDB employs sampling to determine when to apply this method.

\section{DEMONSTRATION OVERVIEW} 
We showcase AnDB's functionalities for processing unstructured data.

\hi{Dataset}. Two unstructured text documents, neurips\_2023.txt, and neurips\_2024.txt, were scraped from papers published at NeurIPS conferences in 2023 and 2024.

\hi{Scenario 1}: Classification and Aggregation. The following three statements are nearly of semantic equivalence but differ in the execution plans. \underline{\it Statement a} represents a simple retrieval scenario, similar to what is typically processed by RAG systems. \underline{\it Statement b} demonstrates a semantic retrieval scenario that does not provide a specific schema. AnDB is required to implicitly infer the schema and perform retrieval based on global semantics. \underline{\it Statement c} is a scenario where the user specifies a schema. AnDB only needs to transform unstructured data into structured data according to the user's requirements and then apply traditional data operators for aggregation. %Their execution plans and workflows are illustrated in \autoref{fig:scenario1}.

\begin{lstlisting}
-- Statement a: simple query scenario
SELECT PROMPT("Analyze technical areas and count the number of publications in each area.") 
FROM FILE("neurips_2024.txt");

-- Statement b: implicit schema inference
SELECT PROMPT("Analyze technical areas"), count(1)
FROM FILE("neurips_2024.txt")  -- no schema
GROUP BY PROMPT("Count the numbers of publications in each area");

-- Statement c: given a specific schema
SELECT count(area), SEM_GROUP(title, "Area of publications", 5) /* 5 is optional, which means divided into five groups */ as area
FROM TABULAR(PROMPT("title of the paper") as title FROM FILE("neurips_2024.txt"))
GROUP BY area;
\end{lstlisting}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/scenario1.png}
%     \caption{Scenario 1. Scan unstructured documents and perform aggregation}
%     \label{fig:scenario1}
% \end{figure}

\hi{Scenario 2}: Analyzing Research Trends in NeurIPS (2023-2024). Cross-conference analysis of research trends between NeurIPS 2023 and 2024, involving retrieval, aggregation, and join operations.

% \begin{lstlisting}
% SELECT area, neurips_2024.count as 2024_count, neurips_2023.count as 2023_count
% FROM 
%   (SELECT PROMPT("Analyze the technical fields") as area, SEM_GROUP("Count the numbers of publications in each area") as count FROM FILE("neurips_2024.txt") GROUP BY area) neurips_2024 -- or use TABULAR
% LEFT JOIN
%   (SELECT PROMPT("Analyze the technical fields") as area, SEM_GROUP("Count the numbers of publications in each area") as count FROM FILE("neurips_2023.txt") GROUP BY area) neurips_2023 -- or use TABULAR
% ON SEM_MATCH(neurips_2024, neurips_2023, "the same technical area from both two documents", 0.9);
% \end{lstlisting}

Here, we demonstrate an end-to-end workflow for a use case as \autoref{fig:workflow} shown. \textbf{Step \textcircled{1}}: The user inputs an SQL-like query in the client interface. \textbf{Step \textcircled{2}}: AnDB scans the relevant unstructured data and invokes the LLM to perform the analysis. \textbf{Step \textcircled{3}}: The results are returned to the user, and presented in a structured format.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.50\textwidth]{figures/e2e_use_case.png}
    \caption{AnDB Workflow}
    \label{fig:workflow}
\end{figure}

The use case only involves two feasible plans in \autoref{fig:scenario2}. \textbf{Plan A}: First, unstructured data is transformed into tabular data, followed by aggregation using traditional join methods. \textbf{Plan B}: During the tabular transformation process, a domain-specific model is used to vectorize the corresponding text contents and embed them into a vector value in a hidden column. Subsequently, during the semantic join, vector similarity is utilized for judgment.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.55\textwidth]{figures/scenario2_without_result.png}
    \caption{Scenario 2. Scan unstructured documents and perform aggregation and join}
    \label{fig:scenario2}
\end{figure}

% \begin{acks}
% We would like to express our gratitude to David Anugraha and Sona Ghotra for providing us with early discussions and code contributions.
% \end{acks}

%\clearpage

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample}

\end{document}
\endinput
