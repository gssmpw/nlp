\section{Related Work}
\label{sec:related-work}

% Prompt engineering has emerged as a crucial technique for enhancing the capabilities of LLMs across a wide range of NLP tasks. 
% Numerous comprehensive surveys have examined various aspects of prompt engineering, each contributing uniquely to the advancement of the field.

% \cite{liu2023pre} provides an extensive overview of prompt-based learning within NLP. 
% This survey distinguishes prompt-based learning from traditional supervised approaches by emphasizing the use of textual prompts to guide model predictions. 
% It categorizes existing methods based on prompt construction, tuning strategies, and application domains, thereby offering a foundational understanding of how prompts can adapt pretrained language models to new tasks.

% In the domain of prompt compression, a comprehensive survey is presented in \cite{li2024prompt}. 
% This work delves into techniques aimed at reducing the length and complexity of prompts, categorizing methods into hard prompt and soft prompt approaches. 
% The survey explores underlying mechanisms such as attention optimization, Parameter-Efficient Fine-Tuning (PEFT), modality fusion, and the creation of synthetic languages. 
% Additionally, it examines the adaptability of these compression techniques across various downstream tasks, highlighting their potential to streamline prompt usage without compromising performance.

% \cite{gu2023systematic} extends prompt engineering methodologies to vision-language models (VLMs). 
% The survey categorizes prompt methods into hard prompts (manual natural language instructions) and soft prompts (automatically generated textual or vector representations). 
% It systematically reviews three primary types of VLMs: 
% multimodal-to-text generation models (e.g., Flamingo), image-text matching models (e.g., CLIP), and text-to-image generation models (e.g., Stable Diffusion). 
% Furthermore, the survey discusses the commonalities and differences between prompt engineering in vision-language models compared to pure language and vision models, addressing responsibility and integrity issues associated with different prompting methods.

% \cite{sahoo2024systematic} offers a thorough examination of prompt engineering methods tailored specifically for LLMs. 
% The survey categorizes prompting techniques based on application areas, providing detailed summaries of various methodologies, including natural language instructions and learned vector representations. 
% By reviewing 44 research papers, it analyzes the effectiveness of each prompting strategy on specific datasets, the LLMs employed, and the corresponding state-of-the-art (SoTA) performances. 
% The survey also includes taxonomy diagrams and summary tables that elucidate the relationships and key attributes of each prompting method, while exploring the commonalities and differences between prompt engineering in language models and vision-language models.

% \cite{amatriain2024prompt} focuses on prompt design and engineering. 
% This survey outlines fundamental concepts and explores advanced techniques such as Chain-of-Thought and Reflection, delving into the principles behind developing LLM-based agents. 
% It also reviews various tools and frameworks that support prompt engineering, including Langchain and Semantic Kernel, thereby establishing a robust theoretical and practical foundation for researchers and practitioners alike.

% \cite{vatsal2024survey} provides a comprehensive survey of prompt engineering methods in LLMs for different NLP tasks. 
% It meticulously categorizes 39 distinct prompting methods based on their application to 29 different NLP tasks, including question-answering, commonsense reasoning, code generation, and sentiment analysis. 
% By reviewing 44 research papers published predominantly in the past two years, the survey offers detailed analyses of each prompting strategy's effectiveness on specific datasets, the LLMs employed, and the corresponding SoTA performances. 
% It emphasizes advanced prompting techniques such as Chain-of-Thought and Reflection, highlighting their role in enhancing reasoning capabilities within LLMs.

% While these surveys offer extensive coverage of prompt engineering techniques across various contexts, our survey specifically emphasizes the iterative optimization of prompts. 
% Unlike the aforementioned works, which focus on categorizing and summarizing existing prompting strategies, our study concentrates on the continuous refinement and optimization of prompts to achieve sustained performance improvements. 
% This includes advanced methods such as soft instruction optimization and example-based optimizations, providing a more focused analysis on enhancing prompt quality and effectiveness through iterative processes.

Prompt engineering has rapidly gained prominence as an essential method for enhancing the capabilities of FMs across diverse tasks. 
Several surveys have contributed to this domain by examining specific facets of prompt-based learning. 
For instance, \cite{liu2023pre} position prompt engineering within the broader context of natural language processing (NLP), emphasizing how textual prompts differ from traditional supervised training. 
Similarly, \cite{vatsal2024survey} and \cite{sahoo2024systematic} each provide comprehensive overviews of prompt methods (ranging from meticulously crafted natural language instructions to learned vectors), detailing their performance across various NLP benchmarks. 
Meanwhile, \cite{amatriain2024prompt} explore more advanced prompt-based mechanisms such as CoT and Reflection and examine toolkits like LangChain and Semantic Kernel.

In addition to these foundational surveys, domain- and technique-specific works have emerged. 
\cite{li2024prompt} focus on prompt compression methods, exploring both hard and soft approaches and illustrating the ways in which compression can streamline model performance without sacrificing accuracy. 
\cite{gu2023systematic} expand these ideas to vision-language models, distinguishing between multimodal-to-text generation, image-text matching, and text-to-image generation. 
They investigate how prompting strategies differ in multimodal settings compared to purely textual ones. 
Thus, these surveys collectively delineate a growing research landscape that spans multiple model types, task formats, and optimization considerations.

Yet, while these works are individually valuable, they remain fragmented by methodological or modal boundaries. 
\cite{chang2024efficient} specifically address efficiency aspects, but no comprehensive resource unifies the theoretical underpinnings across discrete, continuous, and hybrid prompt spaces. 
Existing surveys either concentrate on foundational theories~\cite{liu2023pre,gu2023systematic}, explore a narrowly defined technique such as compression~\cite{li2024prompt}, or focus on manual design patterns~\cite{sahoo2024systematic,vatsal2024survey}.
This compartmentalization leaves open questions about how to systematically organize prompt components, objectives, and optimization strategies under a cohesive theoretical framework.

Our survey bridges this gap by introducing a unified, optimization-theoretic perspective on automated prompt engineering across modalities. 
Unlike prior works that center on specific tasks, prompt types, or efficiency improvements, we formulate prompt engineering as an overarching optimization problem that seeks to maximize task-specific performance metrics for discrete (hard and exampler), continuous (soft), and mixed prompts. 
Our taxonomy spans variables, objective functions, and optimization methods, clarifying best practices and underscoring promising directions for future research. 
In doing so, we provide researchers and practitioners with a rigorous foundation for advancing automated prompt engineering, synthesizing the diverse strands of existing studies into a single, cohesive framework.