\documentclass[11pt]{article}
\usepackage{fullpage,graphicx,psfrag,amsmath,amsfonts,verbatim}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage[small,bf]{caption}
\usepackage{authblk}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}

% \usepackage{forest}
% \usepackage{tikz}
% \usepackage{amssymb}
% \usepackage{times}
% \usepackage{soul}
% \usepackage{url}
% \usepackage[hidelinks]{hyperref}
% \usepackage[utf8]{inputenc}
% \usepackage[small]{caption}
% % \usepackage{ragged2e}
% \usepackage[dvipsnames]{xcolor}
% \usepackage{hyperref}
% \usepackage{natbib}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}
% \usepackage{natbib}

%\hypersetup{
%    colorlinks = true,
%    allcolors = {purple},
%    linkbordercolor = {white},
%}

%\input defs.tex
\allowdisplaybreaks

% \bibliographystyle{alpha}

\title{A Survey of Automatic Prompt Engineering: An Optimization Perspective}
\author[1]{Wenwu Li}
\author[4,3]{Xiangfeng Wang}
\author[1]{Wenhao Li\thanks{\texttt{whli@tongji.edu.cn}}}
\author[2,1]{Bo Jin\thanks{\texttt{bjin@tongji.edu.cn}}}

\affil[1]{School of Computer Science and Technology, Tongji University}
\affil[2]{Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University}
\affil[3]{School of Computer Science and Technology, East China Normal University}
\affil[4]{Key Laboratory of Mathematics and Engineering Applications, MoE, East China Normal University}

\date{}
\begin{document}
\maketitle

\begin{abstract}
% Large language models (LLMs) have demonstrated remarkable capabilities in addressing complex natural language tasks. To fully harness these capabilities, prompt optimization has emerged as an essential technique for effectively guiding in specific tasks. One of the primary advantages of prompt optimization is its ability to significantly enhance LLM performance without necessitating retraining, thereby providing an efficient approach to model adaptation. However, designing effective prompts manually can be highly time-consuming and labor-intensive. To address these challenges, various automated prompt optimization techniques have been proposed, garnering considerable research interest. This survey provides a comprehensive overview of these techniques and outlines several future directions for prompt optimization, such as 
The rise of foundation models has shifted focus from resource-intensive fine-tuning to prompt engineering, a paradigm that steers model behavior through input design rather than weight updates.
While manual prompt engineering faces limitations in scalability, adaptability, and cross-modal alignment, automated methods, spanning foundation model (FM) based optimization, evolutionary methods, gradient-based optimization, and reinforcement learning, offer promising solutions. 
Existing surveys, however, remain fragmented across modalities and methodologies. 
This paper presents the first comprehensive survey on automated prompt engineering through a unified optimization-theoretic lens. 
We formalize prompt optimization as a maximization problem over discrete, continuous, and hybrid prompt spaces, systematically organizing methods by their optimization variables (instructions, soft prompts, exemplars), task-specific objectives, and computational frameworks. 
By bridging theoretical formulation with practical implementations across text, vision, and multimodal domains, this survey establishes a foundational framework for both researchers and practitioners, while highlighting underexplored frontiers in constrained optimization and agent-oriented prompt design.
\end{abstract}

\section{Introduction}\label{sec:intro}

% The rapid advancements in natural language processing (NLP), particularly with the advent of pre-trained large language models (LLMs), have established these models as essential tools for enhancing the performance of a wide array of tasks, including text generation, sentiment analysis, and machine translation. % 换成更加fancy、与时俱进、更加具有落地前景的应用
% As the complexity of tasks increases, traditional model training methods encounter substantial limitations in terms of computational cost and resource demands. % 不应该是任务复杂度提升导致训练成本增加，而是模型规模的增大带来的微调成本，限制了其在下游任务上的应用。
% Therefore, optimizing computational efficiency while preserving or improving model performance—especially in resource-constrained environments—has become a central focus of contemporary research in the field.

The transformative impact of pre-trained foundation models (FMs, e.g., large language models, LLMs or vision language models, VLMs) has revolutionized natural language processing and visual understanding, enabling unprecedented capabilities in complex cognitive tasks ranging from mathematical reasoning to multi-agent collaboration systems~\cite{xi2025rise}. 
As model scales escalate into the trillions of parameters, conventional fine-tuning approaches face prohibitive computational barriers. 
This resource intensiveness fundamentally restricts FM deployment in real-world applications, particularly for edge devices and time-sensitive scenarios like autonomous vehicle decision-making or real-time medical diagnosis.

% Prompt engineering has emerged as a promising solution to this challenge. % 首先介绍一些其他的解决方案，例如参数高效微调（PEFT，例如 LoRA 及其变体）、模型蒸馏、小语言模型（small language model）等，然后说明这些方法还是不可避免需要训练一部分参数，最后引出 prompt engineering 是一个 training-free 的解决方案。
% By carefully crafting and refining input prompts, prompt engineering improves the performance of LLMs without modifying the model's parameters. % 补充以下，换句话说，模型的参数从神经网络的权重变成了输入的指令。
% This technique facilitates task-specific optimization through the fine-tuning of prompt content and structure, providing a more efficient and cost-effective alternative to traditional model fine-tuning, which often incurs high computational costs. % 这句话的描述与前面有些重复，需要更加精炼一些。
% Particularly in scenarios where computational resources are limited or rapid task adaptation is needed, prompt engineering offers a scalable and cost-effective method for enhancing model performance. % 同样地，这句话的描述与前面有些重复。

Several efficiency-focused approaches have emerged including parameter-efficient fine-tuning, model distillation, sparse training, and dynamic architecture methods~\cite{wan2024efficient}. 
While these reduce computational demands to varying degrees, they maintain dependency on parameter updates requiring substantial training data and backpropagation mechanics. 
This proves particularly limiting in scenarios demanding rapid adaptation - such as financial fraud detection systems needing hourly model updates, or medical applications where patient data privacy prohibits retraining. 
Prompt engineering circumvents these constraints through a paradigm shift: rather than modifying neural weights, it reprograms FM behavior via strategic input design~\cite{sahoo2024systematic,amatriain2024prompt,vatsal2024survey}.

% While manually crafted prompts have proven effective for specific tasks, they are often constrained by their reliance on expert knowledge and lack flexibility in adapting to the dynamic and complex nature of real-world applications. % 对于手动设计 prompt的 limitation 的陈述比较 weak、比较平淡，需要重新组织，给出更加深刻的、更加一针见血的陈述，让读者或者审稿人马上 get 到手动设计的局限性。
% To address these limitations, recent advancements in automated prompt optimization have gained significant attention. 
% Techniques such as evolutionary algorithms and reinforcement learning-based approaches employ iterative refinement strategies to optimize prompt content. 
% These methods can explore vast search spaces to identify optimal prompt structures, providing substantial improvements in efficiency, scalability, and task adaptability.% 这一段整体内容总感觉写得不太好，只是浅显地介绍了一些自动提示优化的技术，是否有存在的必要？是否可以和下一段合并，你有什么修改建议？

Manual prompt engineering has demonstrated remarkable generalizability through techniques like Chain-of-Thought (CoT) and few-shot exemplar selection~\cite{akinwande2024understanding}.
However, its practical adoption faces fundamental limitations: 
1) \textit{expert dependency} requiring laborious trial-and-error~\cite{sahoo2024systematic};
2) \textit{input format sensitivity} where minor syntactic variations (e.g., punctuation changes or instruction phrasing) yield performance fluctuations~\cite{sclar2024quantifying}, and
3) \textit{static design} unable to adapt to evolving inputs like shifting social media discourse patterns~\cite{zhou2022large}. 
These limitations intensify in multimodal systems where manual prompting must resolve cross-modal alignment challenges - for instance, ensuring visual grounding accuracy in vision language models (VLMs) requires precise coordination between image region descriptors and textual queries that humans often misalign~\cite{gu2023systematic}.

Automated prompt optimization addresses these limitations through systematic exploration of combinatorial prompt spaces using evolutionary strategies that mutate token sequences through genetic operations, reinforcement learning (RL) that treat prompts as differentiable policies, and meta-learning approaches that adapt prompts through gradient-based hyperparameter optimization. 
Crucially, these methods demonstrate emergent capabilities surpassing human design, such as automatically discovering prompts that balance multiple objectives~\cite{menchaca2025mopo} or adaptively reconfigure based on real-time feedback in robotics control systems~\cite{ma2024eureka}.

% Recent years have witnessed a proliferation of comprehensive surveys on prompt engineering, primarily focusing on manual prompting strategies and their methodological categorizations. 
% These surveys can be broadly categorized into several streams: foundational surveys on prompt-based learning \cite{liu2023pre}, specialized surveys examining technical aspects like prompt compression \cite{li2024prompt}, domain-specific surveys for multimodal contexts \cite{gu2023systematic}, and methodology-focused surveys analyzing specific manually-designed techniques \cite{sahoo2024systematic, amatriain2024prompt, vatsal2024survey}.
% While \cite{chang2024efficient} touches upon automated prompt optimization and compression from an efficiency perspective, a systematic treatment of automated prompt engineering, particularly in the context of multimodal foundation models, remains unexplored. %没有总结现有 survey 的局限，无法和下一段形成连贯的逻辑过渡。

Existing surveys remain fragmented across methodological and modal boundaries. 
While foundational works establish prompt-based learning theory \cite{liu2023pre} and multimodal applications \cite{gu2023systematic}, specialized reviews focus on compression techniques \cite{li2024prompt} or manual design patterns \cite{sahoo2024systematic,amatriain2024prompt,vatsal2024survey}. 
Though \cite{chang2024efficient} addresses efficiency aspects, no comprehensive treatment exists for automated prompt engineering across modalities.

% Our work presents the first comprehensive survey specifically focused on automated prompt engineering applicable to text, vision, and emerging modalities, distinguishing itself through a unified optimization-theoretic framework. 
% This mathematical formulation not only subsumes existing automated approaches but also provides new perspectives on traditional manual prompting strategies, which can be viewed as heuristic solutions to the underlying optimization problem. 
% The framework's versatility is particularly noteworthy - it naturally accommodates various objectives and constraints, allowing us to reformulate diverse problems (e.g., prompt compression emerges naturally as sparse optimization with $\ell_0$-constraints) within the same optimization paradigm. 

\begin{figure*}[htb!]
    \centering
    \includegraphics[width=\linewidth]{figs/overview.pdf}
    \caption{A unified optimization framework for the automated prompt engineering.}
    \label{fig:overview}
\end{figure*}

Our work establishes the first unified optimization theoretic framework (Figure~\ref{fig:overview}) for automated prompt engineering across modalities. 
We formalize the problem as maximizing expected performance metrics over discrete, continuous, and hybrid prompt spaces (Section~\ref{sec:formulation}), where different variable types (hard instructions, soft prompts, few-shot exemplars and mixed variables) correspond to specific optimization subproblems.
This survey systematically organizes existing methods through this lens: 
(1) Optimization Spaces (Section~\ref{sec:variable}) categorize prompt elements across text, vision, and multimodal domains; 
(2) Objective Functions (Section~\ref{sec: obj}) characterize various task categories with mathematical instantiations of performance metrics and constraints; 
(3) Optimization Methods (Section~\ref{sec: method}) classify techniques into representative computational paradigms (FM-based, evolutionary, gradient-based, and RL).
This unified view not only helps explain the effectiveness of existing methods but also establishes a rigorous foundation for developing more sophisticated automated prompt engineering algorithms, bridging the gap between theoretical understanding and practical implementation while identifying underexplored research frontiers.

% This paper focuses on the latest developments in automated prompt optimization methods, specifically from an optimization perspective. We aim to provide a comprehensive summary of the core techniques currently in use, offering insights into how these methods are being applied to enhance model performance.
% The rest of the paper is organized as following:
% Section \ref{sec:related-work} introduces the key variables involved in prompt optimization, including instructions, examples, and vector representations, and discusses how these elements can be adjusted to improve the resulting model outputs. Section \ref{sec:pre} outlines the optimization objectives, such as improving inference accuracy, reducing response time, and optimizing resource utilization. Section \ref{sec: method}  provides an in-depth discussion of optimization methodologies, including evolutionary algorithms, reinforcement learning, and incremental optimization, critically evaluating their strengths and weaknesses. Section \ref{sec: obj} reviews common optimization tasks across a variety of domains, illustrating the adaptability of these methods to different task requirements. Finally, Section 6 addresses the challenges currently faced in prompt optimization, such as overfitting, generalization, and computational efficiency, and proposes potential solutions and directions for future research.

\section{Related Work}\label{sec:related-work}

% Prompt engineering has emerged as a crucial technique for enhancing the capabilities of LLMs across a wide range of NLP tasks. 
% Numerous comprehensive surveys have examined various aspects of prompt engineering, each contributing uniquely to the advancement of the field.

% \cite{liu2023pre} provides an extensive overview of prompt-based learning within NLP. 
% This survey distinguishes prompt-based learning from traditional supervised approaches by emphasizing the use of textual prompts to guide model predictions. 
% It categorizes existing methods based on prompt construction, tuning strategies, and application domains, thereby offering a foundational understanding of how prompts can adapt pretrained language models to new tasks.

% In the domain of prompt compression, a comprehensive survey is presented in \cite{li2024prompt}. 
% This work delves into techniques aimed at reducing the length and complexity of prompts, categorizing methods into hard prompt and soft prompt approaches. 
% The survey explores underlying mechanisms such as attention optimization, Parameter-Efficient Fine-Tuning (PEFT), modality fusion, and the creation of synthetic languages. 
% Additionally, it examines the adaptability of these compression techniques across various downstream tasks, highlighting their potential to streamline prompt usage without compromising performance.

% \cite{gu2023systematic} extends prompt engineering methodologies to vision-language models (VLMs). 
% The survey categorizes prompt methods into hard prompts (manual natural language instructions) and soft prompts (automatically generated textual or vector representations). 
% It systematically reviews three primary types of VLMs: 
% multimodal-to-text generation models (e.g., Flamingo), image-text matching models (e.g., CLIP), and text-to-image generation models (e.g., Stable Diffusion). 
% Furthermore, the survey discusses the commonalities and differences between prompt engineering in vision-language models compared to pure language and vision models, addressing responsibility and integrity issues associated with different prompting methods.

% \cite{sahoo2024systematic} offers a thorough examination of prompt engineering methods tailored specifically for LLMs. 
% The survey categorizes prompting techniques based on application areas, providing detailed summaries of various methodologies, including natural language instructions and learned vector representations. 
% By reviewing 44 research papers, it analyzes the effectiveness of each prompting strategy on specific datasets, the LLMs employed, and the corresponding state-of-the-art (SoTA) performances. 
% The survey also includes taxonomy diagrams and summary tables that elucidate the relationships and key attributes of each prompting method, while exploring the commonalities and differences between prompt engineering in language models and vision-language models.

% \cite{amatriain2024prompt} focuses on prompt design and engineering. 
% This survey outlines fundamental concepts and explores advanced techniques such as Chain-of-Thought and Reflection, delving into the principles behind developing LLM-based agents. 
% It also reviews various tools and frameworks that support prompt engineering, including Langchain and Semantic Kernel, thereby establishing a robust theoretical and practical foundation for researchers and practitioners alike.

% \cite{vatsal2024survey} provides a comprehensive survey of prompt engineering methods in LLMs for different NLP tasks. 
% It meticulously categorizes 39 distinct prompting methods based on their application to 29 different NLP tasks, including question-answering, commonsense reasoning, code generation, and sentiment analysis. 
% By reviewing 44 research papers published predominantly in the past two years, the survey offers detailed analyses of each prompting strategy's effectiveness on specific datasets, the LLMs employed, and the corresponding SoTA performances. 
% It emphasizes advanced prompting techniques such as Chain-of-Thought and Reflection, highlighting their role in enhancing reasoning capabilities within LLMs.

% While these surveys offer extensive coverage of prompt engineering techniques across various contexts, our survey specifically emphasizes the iterative optimization of prompts. 
% Unlike the aforementioned works, which focus on categorizing and summarizing existing prompting strategies, our study concentrates on the continuous refinement and optimization of prompts to achieve sustained performance improvements. 
% This includes advanced methods such as soft instruction optimization and example-based optimizations, providing a more focused analysis on enhancing prompt quality and effectiveness through iterative processes.

Prompt engineering has rapidly gained prominence as an essential method for enhancing the capabilities of FMs across diverse tasks. 
Several surveys have contributed to this domain by examining specific facets of prompt-based learning. 
For instance, \cite{liu2023pre} position prompt engineering within the broader context of natural language processing (NLP), emphasizing how textual prompts differ from traditional supervised training. 
Similarly, \cite{vatsal2024survey} and \cite{sahoo2024systematic} each provide comprehensive overviews of prompt methods (ranging from meticulously crafted natural language instructions to learned vectors), detailing their performance across various NLP benchmarks. 
Meanwhile, \cite{amatriain2024prompt} explore more advanced prompt-based mechanisms such as CoT and Reflection and examine toolkits like LangChain and Semantic Kernel.

In addition to these foundational surveys, domain- and technique-specific works have emerged. 
\cite{li2024prompt} focus on prompt compression methods, exploring both hard and soft approaches and illustrating the ways in which compression can streamline model performance without sacrificing accuracy. 
\cite{gu2023systematic} expand these ideas to vision-language models, distinguishing between multimodal-to-text generation, image-text matching, and text-to-image generation. 
They investigate how prompting strategies differ in multimodal settings compared to purely textual ones. 
Thus, these surveys collectively delineate a growing research landscape that spans multiple model types, task formats, and optimization considerations.

Yet, while these works are individually valuable, they remain fragmented by methodological or modal boundaries. 
\cite{chang2024efficient} specifically address efficiency aspects, but no comprehensive resource unifies the theoretical underpinnings across discrete, continuous, and hybrid prompt spaces. 
Existing surveys either concentrate on foundational theories~\cite{liu2023pre,gu2023systematic}, explore a narrowly defined technique such as compression~\cite{li2024prompt}, or focus on manual design patterns~\cite{sahoo2024systematic,vatsal2024survey}.
This compartmentalization leaves open questions about how to systematically organize prompt components, objectives, and optimization strategies under a cohesive theoretical framework.

Our survey bridges this gap by introducing a unified, optimization-theoretic perspective on automated prompt engineering across modalities. 
Unlike prior works that center on specific tasks, prompt types, or efficiency improvements, we formulate prompt engineering as an overarching optimization problem that seeks to maximize task-specific performance metrics for discrete (hard and exampler), continuous (soft), and mixed prompts. 
Our taxonomy spans variables, objective functions, and optimization methods, clarifying best practices and underscoring promising directions for future research. 
In doing so, we provide researchers and practitioners with a rigorous foundation for advancing automated prompt engineering, synthesizing the diverse strands of existing studies into a single, cohesive framework.

\section{Optimization Problem Formulation}\label{sec:formulation}

This paper studies the prompt optimization problem for foundation models, including both LLMs and VLMs. 
Let $\mathcal{X}$ denote the input space and $\mathcal{Y}$ denote the output space. 
For LLMs, $\mathcal{X}$ represents text inputs, while for VLMs, $\mathcal{X} = \mathcal{X}_v \times \mathcal{X}_t$ represents image-text pairs, where $\mathcal{X}_v$ denotes the visual space and $\mathcal{X}_t$ denotes the text space. 
A prompt function $P: \mathcal{X} \rightarrow \mathcal{P}$ maps input queries to a conditioning pattern that elicits specific model behaviors. 
The prompt space $\mathcal{P}$ can be partitioned into three subspaces: 
the discrete prompt space $\mathcal{P}_d$, the continuous prompt space $\mathcal{P}_c$, and the hybrid prompt space $\mathcal{P}_h = \mathcal{P}_d \times \mathcal{P}_c$.

For $P \in \mathcal{P}_d$, we consider different canonical forms based on model type. 
In LLMs, the zero-shot form is expressed as $P(x)=\left[I,T; x\right]$, where $I,T \in \mathcal{V}^*$ denotes a learnable instruction and thought sequence from vocabulary space $\mathcal{V}$ respectively, and $\mathcal{V}^*$ represents the set of all possible sequences over $\mathcal{V}$. 
The few-shot form $P(x)=\left[I, T, e_1, \ldots, e_k; x\right]$, where $\{e_i \in \mathcal{E}\}_{i=1}^k$ are $k$ learnable exemplars from space $\mathcal{E}$. 
Each exemplar $e_i = (x_i^e, y_i^e)$ consists of an I/O pair where $x_i^e \in \mathcal{X}$ and $y_i^e \in \mathcal{Y}$.

% \textcolor{red}{thoughts in exemplars?}
In VLMs, besides inheriting all prompt forms from LLMs, the spatial annotation form takes a general expression $P(x)=\left[I, T, R_1, \ldots, R_m; x\right]$, where $\{R_i \in \mathcal{R}\}_{i=1}^m$ are spatial regions from a general region space $\mathcal{R}$. 
Each region $R_i = (A_i, l_i)$ consists of an area specification $A_i \in \mathcal{A}$ and a label $l_i \in \mathcal{L}$, where $\mathcal{A}$ is a general area specification space that can represent various forms including but not limited to: 
1) bounding boxes: $A_i \in [0,1]^4$ representing normalized coordinates; 
2) markers: $A_i \in [0,1]^3$ representing center coordinates and radius; 
3) pixel masks: $A_i \in \{0,1\}^{H \times W}$ representing binary masks; and 
4) other region specifications (e.g., polygons, curves).

For $P \in \mathcal{P}_c$, the prompt takes a unified form:
% across both model types:
\begin{equation}
P(e_x)=\left[\theta_1, \ldots, \theta_m; e_x\right],
\end{equation}
where $e_x = \text{Embed}(x) \in \mathbb{R}^d$ is the embedding representation of input $x$ through an embedding function $\text{Embed}: \mathcal{X} \rightarrow \mathbb{R}^d$, and $\{\theta_i \in \mathbb{R}^d\}_{i=1}^m$ are $m$ learnable vectors in the $d$-dimensional embedding space.
For $P \in \mathcal{P}_h$, the prompt combines both discrete and continuous elements:
\begin{equation}
P(x, e_x)=\left[I, T, R_1, \ldots, R_k, \theta_1, \ldots, \theta_m; x\right],
\end{equation}
allowing joint optimization of discrete regions and continuous embeddings.

\begin{table}[ht]
\centering
\resizebox{.7\linewidth}{!}{%
\begin{tabular}{lll}
\hline
\textbf{Symbol} & \textbf{Space} & \textbf{Definition} \\
\hline
$x$ & $\mathcal{X} = \mathcal{X}_v \times \mathcal{X}_t$ & Input query (text or image-text pair) \\
$y$ & $\mathcal{Y}$ & Output response \\
$\mathcal{V}$ & - & Vocabulary of tokens \\
$I$ & $\mathcal{V}^*$ & Instruction sequence \\
$T$ & $\mathcal{V}^*$ & Thought sequence \\
$e_i$ & $\mathcal{E} = \mathcal{X} \times \mathcal{Y}$ & Text exemplar (input-output pair) \\
$\mathcal{R}$ & - & General region space \\
$A_i$ & $\mathcal{A}$ & Area specification (box/marker/mask/etc.) \\
$R_i$ & $\mathcal{R} = \mathcal{A} \times \mathcal{L}$ & Spatial region with label \\
$\theta_i$ & $\mathbb{R}^d$ & Learnable prompt vector \\
$e_x$ & $\mathbb{R}^d$ & Input embedding \\
$P$ & $\mathcal{P} = \mathcal{P}_d \cup \mathcal{P}_c$ & Prompt function \\
$\mathcal{P}_d$ & $\mathcal{P}_d^t \times \mathcal{P}_d^v$ & Discrete prompt space \\
$\mathcal{P}_c$ & - & Continuous prompt space \\
$\mathcal{P}_h$ & $\mathcal{P}_d \times \mathcal{P}_c$ & Hybrid prompt space \\
$f$ & $\mathcal{P} \times \mathcal{X} \rightarrow \mathcal{Y}$ & Foundation model \\
$g$ & $\mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ & Performance metric \\
$\mathcal{D}_{\text{val}}$ & $(\mathcal{X} \times \mathcal{Y})^{n_{\text{val}}}$ & Validation dataset of size $n_{\text{val}}$ \\
\hline
\end{tabular}%
}
\caption{Summary of Mathematical Notation}
\end{table}

Given a black-box foundation model $f: \mathcal{P} \times \mathcal{X} \rightarrow \mathcal{Y}$ and a validation set $\mathcal{D}_{\text{val}}=\{(x_i, y_i)\}_{i=1}^{n_{\text{val}}}$ of size $n_{\text{val}}$, the prompt optimization problem can be formulated as:
\begin{equation}\label{eq:main-formula}
\begin{aligned}
P^* = & \underset{P \in \mathcal{P}}{\arg\max} & & \mathbb{E}_{(x,y) \sim \mathcal{D}_{\text{val}}}[g(f(P(x)), y)] \\
& \text{subject to} & & P \in \mathcal{P}_d \text{ or } P \in \mathcal{P}_c \text{ or } P \in \mathcal{P}_h,
\end{aligned}
\end{equation}
where $g: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ denotes a performance metric measuring the quality of model predictions against ground truth.
This formulation leads to $3$ subclasses of problems:
% \begin{equation}
% \begin{array}{rl}
% \text{DPO:} & \max\limits_{P \in \mathcal{P}_d} \mathbb{E}_{(x,y)}[g(f(P(x)), y)] \\[1em]
% \text{CPO:} & \max\limits_{P \in \mathcal{P}_c} \mathbb{E}_{(x,y)}[g(f(P(e_x)), y)] \\[1em]
% \text{HPO:} & \max\limits_{P \in \mathcal{P}_h} \mathbb{E}_{(x,y)}[g(f(P(x, e_x)), y)]
% \end{array}
% \end{equation}
$$
\max_{P \in \mathcal{P}_\star} \mathbb{E}_{(x,y)}\left[g(f(P(\triangle)), y)\right],
\begin{cases}
\star = d,\triangle = x,&\text{(DPO)} \\
\star = c,\triangle = e_x,&\text{(CPO)} \\
\star = h,\triangle = (x, e_x),&\text{(HPO)}
\end{cases}
$$
where DPO, CPO, and HPO denote discrete, continuous, and hybrid prompt optimization respectively. 
Solutions for all cases are discussed in subsequent sections.

% \paragraph{Remark}
% The optimization-theoretic formulation of prompt engineering, expressed as Equation~\ref{eq:main-formula}, provides a unified mathematical framework that bridges theoretical analysis and algorithmic design. 
% Through the lens of optimization theory, the problem reveals rich structural properties: 
% the discrete case $P \in \mathcal{P}_d$ connects to combinatorial optimization and discrete variational inequalities, while the continuous case $P \in \mathcal{P}_c$ admits analysis through nonlinear programming theory and functional analysis. 
% This duality, combined with measure-theoretic properties of the expectation form and the underlying geometric structure of the prompt space, establishes fundamental limits and suggests principled approaches for algorithm design.

% \paragraph{Remark 2.}

% Most importantly, this theoretical perspective directly informs the development of practical automated prompt engineering algorithms. 
% The structural analysis reveals why certain optimization strategies (e.g., gradient-based methods in continuous spaces, local search in discrete spaces) are more effective in specific scenarios. 
% Understanding the optimization landscape through variational analysis and duality theory enables us to design more efficient algorithms that exploit problem structure, avoid computational bottlenecks, and achieve better convergence properties. 
% This bridges the gap between theoretical insights and practical implementation, distinguishing our approach from existing methodological surveys and providing a rigorous foundation for developing next-generation prompt optimization techniques.

\section{Optimization Spaces}\label{sec:variable}

Building upon formulation in Section~\ref{sec:formulation}, we systematically analyze the $3$ fundamental variable types in prompt optimization: 
discrete ($\mathcal{P}_d$), continuous ($\mathcal{P}_c$), and hybrid combinations ($\mathcal{P}_h$) (Figure~\ref{fig:prompt-example}). 
This taxonomy enables principled analysis of automated prompt engineering across modalities.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=.6\linewidth]{figs/prompt-example.pdf}
    \caption{A prompt example for the visual question-answering task.}
    \label{fig:prompt-example}
\end{figure}

\subsection{Discrete Variables}

Discrete prompt optimization operates in $\mathcal{P}_d$ space, manipulating human-interpretable elements through combinatorial search. 
We identify three principal variable subtypes:

\paragraph{Instructions}\label{sec:hard-instruction}

% Typically, hard instructions consist of simple and clear task descriptions, and their quality directly determines the accuracy of task execution. 
% By activating relevant domain knowledge, instructions effectively guide models to perform specific tasks. 

% AutoPrompt \cite{shin2020autoprompt} automatically inserts gradient-based triggers to elicit knowledge from pretrained LLMs;
% ProTeGi \cite{pryzant2023automatic} refines prompts via gradient-inspired updates and beam search; 
% LongPO \cite{hsieh2023automatic} employs genetic algorithms for optimizing lengthy prompts; 
% EvoPrompt \cite{guo2023connecting} unifies evolutionary algorithms with LLM-generated rewrites; 
% OPRO \cite{yang2023largelm} leverages meta-prompts to generate and ensemble multiple candidate outputs; 
% PRewrite \cite{kong2024prewrite} uses reinforcement learning to improve underperforming prompts; 
% StablePrompt \cite{kwon2024stableprompt} applies RL-based editing to stabilize discrete prompts; 
% RLPrompt \cite{deng2022rlprompt} adopts a similar RL paradigm for prompt optimization; PromptAgent \cite{wang2023promptagent} employs Monte Carlo Tree Search to produce structured domain-specific instructions;
% and \cite{hu2024localized} propose a localized zeroth-order method for black-box prompt refinement.
% % 这一段介绍过于关注方法，与本节标题不符，简化这一段内容，改变叙述重心，使其从 instruction 本身分类，将其与上一段合并。

Instruction variables ($I \in \mathcal{V}^*$) specify task objectives through natural language directives (e.g., ``Translate to French''). 
Their optimization centers on generating concise yet precise directives to elicit strong model performance. 
Representative approaches \textit{generate}~\cite{hsieh2023automatic,yang2023largelm,wang2023promptagent}, \textit{mutate}~\cite{yang2023largelm}\cite{guo2023connecting} or \textit{refine}~\cite{shin2020autoprompt,pryzant2023automatic,guo2023connecting,kong2024prewrite,kwon2024stableprompt,deng2022rlprompt,hu2024localized} instructions via gradient-inspired edits, GA, or RL to systematically improve task accuracy.
This classification can be further extended to multimodal prompts, with representative approaches including  \emph{generate}~ \cite{kim2023multiprompter}, \emph{mutate}~ \cite{hao2024optimizing,ogezi2024optimizing} or 
 \emph{refine}~ \cite{manas2024improving,yeh2024tipo,mrini2024fast,wang2024promptcharm,rosenman2023neuroprompts,mo2024dynamic}, which play a crucial role in optimizing accuracy and ensuring consistency in generated image quality.

\paragraph{Thoughts}

Thought variables ($T \in \mathcal{V}^*$) implement chain-of-reasoning through intermediate reasoning steps.
By decomposing large problems into smaller subproblems, CoT guidance can substantially boost solution correctness when automatically generated or refined.
These approaches enhance the precision and flexibility of CoT reasoning, enabling models to better handle complex tasks.
Representative methods include \emph{feedkback}~\cite{press2023measuring,chen2024prompt,chen2024reprompt}, which uses self or external feedback to refine reasoning, and \emph{interaction}~\cite{yaoreact,madaan2024self}, which leverages external tools or resources to verify and adjust reasoning dynamically.

%self refine selfask react self-reflection
\paragraph{Few-shot Examples} \label{sec:example}

% Few-shot exemplars, typically input-output pairs, are a key component in automatic prompt optimization, guiding the behavior of models.
% Unlike instruction, which refines task descriptions, exemplar optimization focuses on selecting and optimizing relevant examples. 
% Empirical evidence suggests that optimizing exemplars can outperform instruction-only methods, highlighting the significance.

% \cite{liu2022makes} proposes the KATE strategy for selecting semantically similar exemplars, improving GPT-3's few-shot performance across tasks like table-to-text generation and open-domain question answering. 
% \cite{margatina2023active} further explores active learning for exemplar selection, demonstrating the impact of semantically relevant examples on model generalization.
% \cite{zhangautomatic} proposes a method to automatically generate CoT demonstrations, eliminating the need for manually crafted examples, and shows that this approach can consistently match or exceed the performance of traditional CoT methods. 
% \cite{lu2022fantastically} investigates the sensitivity of few-shot learning to the order of prompt examples. 
% Their method uses entropy statistics to rank prompt permutations, significantly improving GPT-family models' performance across multiple text classification tasks. 
% This work highlights the importance of order in the selection and arrangement of examples in few-shot settings, furthering our understanding of prompt design.
% \cite{zhang2022active} Formulating the contextual learning example choice as a sequential decision problem.
% % 这一段介绍同样过于.关注方法，与本节标题不符，简化这一段内容，改变叙述重心，使其从few-shot example角度对这些工作进行分类，例如有些工作关注选择/生成新的 example，有些工作关注对 example 排序。将其与上一段合并。

These short input-output pairs (${e_i} \subset \mathcal{E}$) demonstrate task-relevant behaviors, assisting models in inferring patterns with minimal supervision. 
Exemplar optimization techniques may address exemplar \emph{selection}~\cite{liu2022makes,margatina2023active,zhang2022active} or \emph{ordering} \cite{lu2022fantastically}, and some methods automate the generation of new examples \cite{margatina2023active} for improved performance.
%\cite{10378642}
% \cite{nie2022improving} 

\paragraph{Annotations}
Although bounding boxes, markers, and pixel masks are widely employed in vision-language tasks for visual guidance~\cite{kirillov2023segment}, these prompts remain exceptionally underexplored with respect to prompt optimization~\cite{gu2023systematic}. 
Existing approaches often treat the encoded image representation (e.g., image embeddings) as the ``soft'' prompt, leveraging soft prompt tuning strategies without explicitly capitalizing on human-crafted annotations. 
Only a limited body of work has introduced annotations as extra visual cues in manual prompt engineering~\cite{peng2023kosmos,denner2024visual}.

Though instructions, thoughts, exemplars, and annotations each serve distinct roles, many works integrate their optimization in a shared framework, aiming to maximize discrete prompt effectiveness while minimizing manual effort.






\subsection{Continuous Variables}\label{sec:soft-instruction}

% Soft prompts are learnable tensors appended to model inputs, allowing for task-specific adaptation without modifying the underlying LLM parameters. 
% Unlike hard prompts, which involve discrete tokens in natural language, soft prompts consist of continuous embedding vectors that do not map directly to real words, thus remain human-unreadable. 
% However, by leveraging gradient-based training to jointly optimize these virtual tokens alongside foundation models, soft prompts can capture complex language patterns with minimal parameter overhead and reduced inference cost.

% Several notable methods highlight the potential of soft prompt optimization. 
% Prefix-Tuning \cite{li2021prefix} attaches a small set of trainable prefix embeddings to the input layers of models like GPT-2 or BART, enabling efficient adaptation while keeping the core model weights frozen. 
% Building on this idea, Prompt-Tuning \cite{lester2021power} adds trainable continuous prompt tokens to larger models such as GPT-3 and demonstrates that scaling model size further improves its efficacy. 
% P-Tuning \cite{liu2024gpt} extends these concepts by placing prompt embeddings across multiple hidden layers, thereby enhancing the model’s interpretative capacity and boosting performance in few-shot tasks. 
% Lastly, SPTAR \cite{peng2025soft} leverages soft prompt tuning for domain-specific dense retrieval, generating weak document-query pairs from LLMs and filtering them through optimized task-specific prompts to address data scarcity in specialized retrieval settings.
% % 这一段介绍同样过于关注方法，与本节标题不符，简化这一段内容，改变叙述重心，使其从  soft prompt 本身分类，例如有些工作的 learnable soft prompt 是添加到输入中，类似 instruction，而有些工作则将 learnable soft prompt添加到神经网络的隐藏层。同样将其与上一段合并。

In contrast to discrete tokens, continuous (soft) prompts ($\mathcal{P}_c$) rely on learnable embeddings ${\theta_i} \subset \mathbb{R}^d$. 
These vectors can be appended to input representations and optimized via gradient-based methods. 
By avoiding changes to the underlying model parameters, soft prompts require fewer resources for adaptation. 
Notable research explores \textit{prefix-based} \cite{li2021prefix,lester2021power,peng2025soft,wei2023improving} or \textit{layer-spanning embeddings} \cite{liu2024gpt}, revealing that tuning a small set of trainable vectors can achieve strong performance gains while preserving model generality. 
Soft prompts thus offer an efficient path to personalize or specialize foundation models without extensive fine-tuning or large-scale data.


\subsection{Mixed Variables}\label{sec:hybrid}

% Mixed Variables integrates instruction and exemplar optimization to enhance large language model performance. 
% Instruction optimization refines task descriptions, while exemplar optimization focuses on selecting and tuning input-output pairs to guide model behavior. 
% Research shows that combining these two approaches leads to better results than optimizing them separately, as optimized instructions and curated exemplars together enable more accurate and adaptable model responses.
% % 混合的方式有很多种，目前只总结了 instruction 和 examplar 的混合。修改一下写法，现在的写法感觉只有这一种混合方式。

% PromptBreeder\cite{fernando2023promptbreeder} employs "context shuffling" to co-evolve exemplars and instructions, focusing on the optimization of input-output processes. 
% However, it primarily enhances instructions rather than optimizing exemplars.
% Mixture-of-Prompts \cite{wangmixture} optimizes instructions by assigning them to different exemplar groups, but does not focus on improving the exemplars themselves, limiting its potential for further enhancement.
% Promptwizard \cite{agarwal2024promptwizard} jointly optimizes both instructions and exemplars by using large language models to refine both components together, demonstrating the benefits of holistic prompt design.
% PhaseEvo \cite{cui2024phaseevo} advances this approach by integrating evolutionary algorithms with large language models, employing a multi-phase design for joint optimization. This results in superior performance by effectively co-optimizing both instructions and exemplars, leading to faster convergence and more robust model behavior.
% % 将这一段简化，同时修改叙述重心，使其更加偏重instruction 和 examplar的混合方式而非优化方法，并与上一段合并。

Mixed (hybrid) settings in $\mathcal{P}_h$ space incorporate both discrete and continuous elements, combining human-readable instructions or exemplars with trainable embedding vectors. 
This fusion leverages the interpretability and domain specificity of discrete tokens and the flexibility of continuous embeddings. 
Early works on co-optimizing instructions and exemplars \cite{fernando2023promptbreeder,wangmixture,agarwal2024promptwizard,cui2024phaseevo} demonstrate that jointly refining these complementary parts often achieves more robust and adaptable behavior than optimizing each component alone. 
Such synergy can be extended to multimodal prompts\cite{wen2024hard}, for instance, by coupling discrete spatial regions (in VLMs) with continuous alignment vectors, thereby enabling comprehensive, end-to-end optimization.

\section{Objective Functions}\label{sec: obj}

As formulated in Section~\ref{sec:formulation}, our goal is to solve
\begin{equation}\label{eq:obj-restate}
\max_{P \in \mathcal{P}} \;\; \mathbb{E}_{(x,y)\sim\mathcal{D}_{\text{val}}}\bigl[g\bigl(f\bigl(P(x)\bigr),y\bigr)\bigr],
\end{equation}
where $f\!:\!\mathcal{P}\times\mathcal{X}\to \mathcal{Y}$ denotes the black-box foundation model (either an LLM or VLM), $P$ is a prompt from the prompt space $\mathcal{P}$ (discrete, continuous, or hybrid), and $g:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}$ is a performance metric. 
We now illustrate how $g(\cdot)$ is instantiated across downstream tasks (Section~\ref{sec:downstream}) and then discuss constrained objectives (Section~\ref{sec:constrained}), all within the framework of Equation~\eqref{eq:main-formula}.

%\begin{figure*}[!ht]
%\centering
%\begin{forest}
%for tree={
%    grow=east,
%%    draw,
 %   rounded corners,
 %%   align=center,
  %  parent anchor=east,
  %  child anchor=west,
  %  calign=first,
  %  l sep=12mm, % 控制层级间的间距
  %  s sep=2mm, % 控制节点之间的垂直间距
  %  font=\sffamily\scriptsize, % 使用小字体
  %  edge path={
%        \noexpand\path [draw, \forestoption{edge}] (!u.parent anchor) -- +(4mm,0) |- (.child anchor)\forestoption{edge label};
 %   },
 %   text width=7cm, % 更大宽度以容纳文字
%    inner xsep=6pt, % 增加内边距，避免文字贴边
%    inner ysep=6pt
%}
%[Taxonomy of Tasks, fill=red!20, text width=1.2cm
%    [Instruction Induction, fill=orange!20, text width=3cm
%        [{PE2, APE, Promptbreeder, InstructZero, INSTINCT, PhaseEvo}, fill=orange!10, text width=11cm]
%    ]
%    [Text Classification, fill=blue!20, text width=3cm
%        [{HPME, RLPrompt, ProTeGi, PromptAgent, FluentPrompt, Promptbreeder, PRewrite, OPRO, STraGo}, fill=blue!10, text width=11cm]
%    ]
%    [Math Reasoning, fill=yellow!20, text width=3cm
%        [{PE2, OPRO, INSTINCT, Self-refine, Reprompting}, fill=yellow!10, text width=11cm]
%    ]
%    [Commonsense Reasoning, fill=cyan!20, text width=3cm
%        [{PromptAgent, Promptbreeder, PRewrite, GPS, MAPO, OPRO, AMPO}, fill=cyan!10, text width=11cm]
%    ]
%    [Domain-Specific Tasks, fill=purple!20, text width=3cm
%        [{AMPO, PromptAgent, STraGo}, fill=purple!10, text width=11cm]
%    ]
%    [Multi-Hop Reasoning, fill=pink!20, text width=3cm
%        [{APE, PhaseEvo, PE2, OPRO, LongPO, PromptAgent, Autohint, LCP, STraGo}, fill=pink!10, text width=11cm]
%    ]
%    [Multimodal Task, fill=green!20, text width=3cm
%        [{OPT2I, HPME, MultiPrompter, TIPO, APOHF, FPA, Promptist}, fill=green!10, text width=11cm]
%    ]
%]
%\end{forest}
%\caption{Taxonomy of NLP Tasks and Corresponding Prompt Optimization Techniques}
%\end{figure*}

\subsection{Downstream Tasks}\label{sec:downstream}

% % It is important to note that a dataset may be relevant to multiple NLP tasks simultaneously. However, this overlap can complicate the structured analysis of how prompting techniques perform across various tasks. To address this, we ensure that each dataset is assigned to a single NLP task with which it is most strongly associated.

% The following subsections define individual NLP tasks, list corresponding datasets, and describe various prompting strategies applied to these datasets. 
% They also identify the potential SoTA prompting technique for each dataset. 
% % Since the performance of a prompting method depends significantly on the LLM used, we have included a list of LLMs that have been applied in conjunction with these prompting strategies for the datasets under consideration.

% For SoTA prompting techniques, we mention only the method's name, as in many cases, a particular LLM has not been tested with the prompting method, making it uncertain whether it could achieve SoTA performance. 
% If a prompting strategy, used with any LLM from the list, achieved the best performance for a given dataset, we designate it as the SoTA method, irrespective of the exact LLM used. 
% Similarly, we do not include evaluation metrics when listing SoTA methods, as these metrics often vary across tasks and datasets.
\begin{figure}[htb!]
    \centering
    \includegraphics[width=\linewidth]{figs/downstream_task2.1.pdf}
    \caption{Taxonomy of Downstream Tasks and Corresponding Prompt Optimization Techniques.}
    \label{fig:downstream}
\end{figure}

\paragraph{Instruction Induction.}
This task evaluates how well $f\bigl(P(x)\bigr)$ extracts and generalizes underlying instructions. 
Following \cite{Honovich2022InstructionIF}, it spans 24 sub-tasks such as morphosyntactic transformations and causality detection. 
Typical $g(\cdot)$ measures include BERTScore-F1 and exact/set match.

\paragraph{Text Classification.}
Models map text $x\in\mathcal{X}$ to a discrete label $y\in\mathcal{Y}$. 
Datasets like  SST-2~\cite{socher2013recursive},  SST-5~\cite{socher2013recursive} and AGNEWS~\cite{zhang2015character} cover sentiment, topic classification, and subjectivity detection. 
Common metrics $g\bigl(f(P(x)),y\bigr)$ include classification accuracy and Macro-F1.

\paragraph{Math Reasoning.}
Here $f\bigl(P(x)\bigr)$ must solve arithmetic or algebraic word problems. 
Datasets such as GSM8K \cite{cobbe2021training}, MultiArith \cite{koncel2016mawps} and SingleEq \cite{koncel2016mawps} vary from single-step arithmetic to multi-step reasoning. 
Evaluation relies on comparing numeric outputs or perplexity.

\paragraph{Commonsense Reasoning.}
Models must incorporate external, everyday knowledge to answer or infer implicit context. Tasks like  CommonsenseQA~\cite{talmor2018commonsenseqa}, and StrategyQA~\cite{geva2021did} evaluate $f\bigl(P(x)\bigr)$ for multi-step common-sense inferences. 
Exact match are typical performance metrics.

\paragraph{Multi-Hop Reasoning.}
Extending beyond single-step inferences, multi-hop tasks demand chaining evidence from multiple sources. 
BBH~\cite{suzgun2022challenging} exemplifies this domain. 
The key objective is again $\max \mathbb{E}[g(\cdot)]$ over correctness but with specialized intermediate-step or CoT evaluations.

\paragraph{Domain-Specific Tasks.}
Focus on specialized areas demanding expert-level knowledge, e.g., biomedical tasks in MedQA~\cite{jin2021disease} and MedMCQA~\cite{pal2022medmcqa}, where $f\bigl(P(x)\bigr)$ must accurately handle topic-specific queries. 
Evaluation often uses accuracy or Macro-F1 on domain-specific labels.

\paragraph{Multimodal Tasks.}
Here $\mathcal{X}=\mathcal{X}_v\times\mathcal{X}_t$ includes image and text. 
Datasets MS~COCO~\cite{lin2014microsoft}, LAION~\cite{schuhmann2022laion}, and Celeb-A~\cite{liu2015deep} support image captioning, retrieval, and visual question answering. 
Objectives involve image-text alignment, retrieval accuracy, CLIP or Aesthetics Score.

% \subsubsection{Text Processing}

% \paragraph{Instruction Induction}

% % The Instruction Induction task evaluates a model’s ability to infer underlying instructions from few-shot examples, thereby enhancing its performance across a variety of language understanding tasks. The dataset utilized for this task is derived from \cite{Honovich2022InstructionIF}, comprising 24 language understanding tasks of varying difficulty. These tasks range from surface-level spelling and morphosyntactic tasks (e.g., pluralization) to more complex tasks such as sentence similarity, causality detection, style transfer (e.g., formality), and sentiment analysis.

% This task measures how well foundation models infer an underlying instruction from limited examples. 
% Following \cite{Honovich2022InstructionIF}, $\mathcal{D}_{\text{val}}$ spans 24 language-understanding tasks of varying complexity (e.g., spelling, causality, sentiment), where the goal is to maximize $\mathbb{E}_{(x,y)\sim \mathcal{D}_{\text{val}}} \bigl[g(f(P(x)), y)\bigr]$ through suitably learned prompts $P \!\in\! \mathcal{P}$ that induce correct instructions.

% \paragraph{Text Classification}

% % The text classification task assesses a model’s ability to assign input text to predefined categories based on its content. The datasets used for this task include Amazon Polarity \cite{mcauley2013hidden}, SST-2 \cite{socher2013recursive}, AGNEWS \cite{zhang2015character}, MR \cite{pang2005seeing}, CR \cite{hu2004mining}, SST-5 \cite{socher2013recursive}, TREC \cite{voorhees2000building}, and Subj \cite{pang2004sentimental}.
% % These datasets encompass a range of text classification challenges, from sentiment analysis (e.g., Amazon Polarity, SST-2) to topic categorization (e.g., AGNEWS, TREC) and subjectivity detection (e.g., Subj). They provide a comprehensive foundation for evaluating and advancing text classification models across various domains.

% Here, $f\bigl(P(x)\bigr)$ must assign $x\in\mathcal{X}$ to a label $y\in\mathcal{Y}$. 
% Standard datasets like Amazon Polarity \cite{mcauley2013hidden}, SST-2 \cite{socher2013recursive}, AGNEWS \cite{zhang2015character}, MR \cite{pang2005seeing}, CR \cite{hu2004mining}, SST-5 \cite{socher2013recursive}, TREC \cite{voorhees2000building}, and Subj \cite{pang2004sentimental} provide explicit target categories; we measure classification accuracy or F1-score via $g(\cdot)$.

% \paragraph{Math Reasoning}
% The Math Reasoning task evaluates a model’s ability to perform mathematical computations in a non-tabular setting. The datasets used for this task include GSM8K \cite{cobbe2021training}, SVAMP \cite{patel2021nlp}, MultiArith \cite{koncel2016mawps}, AddSub \cite{koncel2016mawps}, AQuA \cite{ling2017program}, and SingleEq \cite{koncel2016mawps}. These datasets primarily focus on arithmetic reasoning (e.g., AddSub, MultiArith), solving word problems (e.g., GSM8K, SVAMP), and algebraic equation solving (e.g., AQuA, SingleEq). AddSub and SingleEq consist of simpler problems that require single-step reasoning, while the other datasets involve more complex tasks that demand multi-step reasoning.

% Tasks such as GSM8K \cite{cobbe2021training}, SVAMP \cite{patel2021nlp}, MultiArith \cite{koncel2016mawps}, AddSub \cite{koncel2016mawps}, AQuA \cite{ling2017program}, and SingleEq \cite{koncel2016mawps} evaluate the model’s ability to perform arithmetic or algebraic reasoning on text-based math queries. 
% Prompts $P \!\in\! \mathcal{P}_d$ often include instructions and thoughts to boost multi-step problem solving, with $g(\cdot)$ capturing exact match or solution correctness.

% \paragraph{Commonsense Reasoning}
% The Commonsense Reasoning task evaluates a model’s ability to utilize common sense knowledge to understand and reason about natural language inputs. The datasets utilized for this task include CB \cite{de2019commitmentbank}, CommonsenseQA \cite{talmor2018commonsenseqa}, and StrategyQA \cite{geva2021did}. These tasks range from recognizing basic commonsense facts (e.g., CB) to more complex tasks that involve reasoning over implicit information (e.g., CommonsenseQA, StrategyQA), requiring multi-step inference to derive conclusions.

% \paragraph{Multi-Hop Reasoning}
% The Multi-Hop Reasoning task evaluates a model's ability to connect evidence from various parts of a context to address a given query. The datasets utilized for this task include BIG-Bench Hard (BBH) \cite{suzgun2022challenging}, comprising a collection of reasoning challenges with increasing complexity. These tasks range from simpler tasks that require basic fact retrieval and logical inferences to more advanced challenges such as multi-step problem solving, causal inference, and integrating disparate pieces of evidence to draw conclusions.

% \paragraph{Domain-Specific Tasks}
% Domain-Specific Tasks evaluate a model’s performance in specialized areas that require domain-specific knowledge and expertise. For this study, we focus on tasks within the biomedical domain, where expert-level prompt optimization is crucial for accurate and meaningful results. The datasets utilized for this task include MedQA \cite{jin2021disease} and MedMCQA \cite{pal2022medmcqa}. These tasks range from answering medical questions related to specific diseases (e.g., MedQA) to more complex multi-choice questions involving clinical decision-making and diagnosis (e.g., MedMCQA), testing the model’s ability to accurately handle and reason about domain-specific queries.

% \paragraph{Multimodal Task} In multimodal tasks, models are required to understand and reason about both visual and textual inputs. This involves learning the associations between images and corresponding text, such as generating image captions, answering visual questions, or performing image-text matching. The datasets utilized for this task include MS COCO \cite{lin2014microsoft}, LAION \cite{schuhmann2022laion}, Celeb-A \cite{liu2015deep}, and Lexica.art.
% MS COCO is a large-scale dataset containing over 300,000 images, each annotated with multiple captions, enabling models to learn diverse descriptions of visual content. LAION is a dataset comprising billions of image-text pairs, facilitating the training of models on a vast range of visual and textual associations. Celeb-A is a dataset featuring over 200,000 celebrity images, each annotated with 40 attribute labels, useful for tasks like facial recognition and attribute prediction. Lexica.art is a collection of AI-generated images accompanied by descriptive prompts, aiding models in understanding the relationship between textual descriptions and visual content.
% These datasets provide image-text pairs that help train models to perform various multimodal reasoning tasks, such as generating descriptive captions, answering questions based on images, and matching textual queries to the most relevant images.


% \subsection{Task-Specific Objectives}

% \begin{itemize}
%     \item Instruction Induction: maximal BERTScore-F1 over all gold-reference annotations; exact string match; exact set match; whether is contained in the reference set; unigram overlap;
%     \item Text Classification: maximal classification accuracy over all test splits; macro F1 score; exact label match; prediction containment in the gold label set; 
%     \item Math Reasoning: maximal answer execution accuracy over all gold-standard annotations; average sentence-level perplexity and unigram BLEU-4 overlap;
%     \item Commonsense Reasoning: maximal answer accuracy over all gold-reference annotations; exact answer match; macro F1 score; prediction containment in the gold answer set; majority vote consistency;
%     \item Domain-Specific Tasks: maximal answer accuracy over all gold-standard annotations; exact domain-specific answer match; macro F1 score;
%     \item Multi-Hop Reasoning: maximal answer accuracy over all gold-standard annotations; exact evidence chain match; exact intermediate step match; multi-hop reasoning consistency;
%     \item Multimodal Task: maximal image–text alignment accuracy over all gold-standard annotations; exact caption match; exact retrieval match; whether prediction is contained in the reference set; average cosine similarity.
% \end{itemize}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \begin{table}[htb!]
% \centering
% \caption{}
% \label{tab:my-table}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{lll}
% \hline
% \textbf{Task}                          & \textbf{Metric}           & \textbf{Objective Function}                                                        \\ \hline
% \multirow{5}{*}{Instruction Induction} & Maximal BERTScore-F1      & $\max \mathbb{E}[\max_{y \in Y_{\mathit{ref}}} \text{BERTScore}(f(x), y)]$             \\
%                                        & Exact string match        & $\max \mathbb{E}[\mathbb{I}(f(x) = y_{\mathit{ref}})]$                             \\
%                                        & Exact set match           & $\max \mathbb{E}[\mathbb{I}(Y = Y_{\mathit{ref}})]$                                \\
%                                        & Prediction containment    & $\max \mathbb{E}[\mathbb{I}(f(x) \in Y_{\mathit{ref}})]$                           \\
%                                        & Unigram overlap           & $\max \mathbb{E}[\text{UnigramOverlap}(f(x), Y_{\mathit{ref}})]$                   \\ \hline
% \multirow{4}{*}{Text Classification}   & Classification accuracy   & $\max \mathbb{E}[\mathbb{I}(f(x) = y_{\mathit{ref}})]$                                 \\
%                                        & Macro F1 score            & $\max \frac{1}{C}\sum_{c=1}^C \text{F1}_c(f(x), y_{\mathit{ref}})$                 \\
%                                        & Exact label match         & $\max \mathbb{E}[\mathbb{I}(f(x) = y_{\mathit{ref}})]$                             \\
%                                        & Prediction containment    & $\max \mathbb{E}[\mathbb{I}(f(x) \subseteq Y_{\mathit{ref}})]$                     \\ \hline
% \multirow{3}{*}{Math Reasoning}        & Answer execution accuracy & $\max \mathbb{E}[\mathbb{I}(\text{Execute}(f(x)) = \text{Execute}(y_{\mathit{ref}}))]$ \\
%                                        & Avg. sentence perplexity  & $\min \mathbb{E}[\text{Perplexity}(f(x))]$                                         \\
%                                        & Unigram BLEU-4            & $\max \mathbb{E}[\text{BLEU}_4(f(x), Y_{\mathit{ref}})]$                           \\ \hline
% \multirow{5}{*}{Commonsense Reasoning} & Answer accuracy           & $\max \mathbb{E}[\mathbb{I}(f(x) = y_{\mathit{ref}})]$                             \\
%                                        & Exact answer match        & $\max \mathbb{E}[\mathbb{I}(f(x) = y_{\mathit{ref}})]$                             \\
%                                        & Macro F1 score            & $\max \frac{1}{C}\sum_{c=1}^C \text{F1}_c(f(x), y_{\mathit{ref}})$                 \\
%                                        & Prediction containment    & $\max \mathbb{E}[\mathbb{I}(f(x) \in Y_{\mathit{ref}})]$                           \\
%                                        & Majority vote consistency & $\max \mathbb{E}[\mathbb{I}(\text{MajorityVote}(f(x)) = y_{\mathit{ref}})]$        \\ \hline
% \multirow{3}{*}{Domain-Specific Tasks} & Answer accuracy           & $\max \mathbb{E}[\mathbb{I}(f(x) = y_{\mathit{ref}})]$                             \\
%                                        & Exact domain match        & $\max \mathbb{E}[\mathbb{I}(f(x) = y_{\mathit{ref}})]$                             \\
%                                        & Macro F1 score            & $\max \frac{1}{C}\sum_{c=1}^C \text{F1}_c(f(x), y_{\mathit{ref}})$                 \\ \hline
% \multirow{4}{*}{Multi-Hop Reasoning}   & Answer accuracy           & $\max \mathbb{E}[\mathbb{I}(f(x) = y_{\mathit{ref}})]$                             \\
%                                        & Evidence chain match      & $\max \mathbb{E}[\mathbb{I}(f(x) = y_{\mathit{ref}})]$                             \\
%                                        & Intermediate step match   & $\max \mathbb{E}[\mathbb{I}(\text{Steps}(f(x)) = \text{Steps}(y_{\mathit{ref}}))]$ \\
%                                        & Reasoning consistency     & $\max \mathbb{E}[\mathbb{I}(\text{Consistent}(f(x), y_{\mathit{ref}}))]$           \\ \hline
% \multirow{5}{*}{Multimodal Task}       & Image-text alignment      & $\max \mathbb{E}[\mathbb{I}(\text{Align}(f(x), y_{\mathit{ref}}))]$                    \\
%                                        & Exact caption match       & $\max \mathbb{E}[\mathbb{I}(f(x) = y_{\mathit{ref}})]$                             \\
%                                        & Exact retrieval match     & $\max \mathbb{E}[\mathbb{I}(\text{Retrieve}(f(x)) = y_{\mathit{ref}})]$            \\
%                                        & Prediction containment    & $\max \mathbb{E}[\mathbb{I}(f(x) \in Y_{\mathit{ref}})]$                           \\
%                                        & Avg. cosine similarity    & $\max \mathbb{E}[\text{Sim}(f(x), y_{\mathit{ref}})]$                              \\ \hline
% \end{tabular}%
% }
% \end{table}

%\textbf{Prompt Optimization Methods:} OPT2I HPME MultiPrompter TIPO APOHF

\subsection{Constrained Objectives}\label{sec:constrained}

Beyond purely maximizing task-specific metrics, several prompt optimization scenarios require additional constraints:
\begin{itemize}
    \item \textbf{Prompt Editing.} Here we impose structural or semantic constraints on $P(x)\in \mathcal{P}$. Minor reformulations of instructions, exemplars, or spatial annotations must preserve or improve $\mathbb{E}[g(\cdot)]$ under restricted editing budgets. This suits tasks where $f(P(x))$ is sensitive to subtle prompt shifts \cite{sahoo2024systematic,amatriain2024prompt}.
    \item \textbf{Prompt Compression.} Constrains $\lVert P\!\rVert_{\text{length}} \leq \kappa$ (token-length or embedding-size budget), seeking $\max_{P\in\mathcal{P}} \mathbb{E}[g(\cdot)]$ subject to $\kappa$. Trimming extraneous tokens or embedding vectors increases computational efficiency while preserving accuracy, crucial in real-time or large-scale scenarios \cite{chang2024efficient}.
\end{itemize}

These can be viewed as special cases of Eq.~\eqref{eq:main-formula} with additional constraints $\Gamma(P)\leq \kappa$, leading to solutions that favor prompt conciseness or regulated edits. By integrating such constraints into the optimization procedure, automated prompt design expands beyond raw performance maximization to meet usability, efficiency, and alignment requirements.

\section{Optimization Methods}\label{sec: method}

To establish a cohesive organization, we propose a unifying taxonomy and classify optimization methods across four major paradigms, as shown in Figure~\ref{fig:landscape}: 
(1) FM-based Optimization, (2) Evolutionary Computing, (3) Gradient-Based Optimization, and (4) Reinforcement Learning. 
Each paradigm can further be subdivided based on whether it optimizes purely discrete prompts ($P\in\mathcal{P}_d$), purely continuous prompts ($P\in\mathcal{P}_c$), or hybrids ($P\in\mathcal{P}_h$). 
% % Methods in the same paradigm typically share common algorithmic structures (e.g., iterative textual rewriting, genetic operations, gradient descent steps, or RL-based feedback loops), even though they may address distinct subproblems of prompt optimization. 
% Throughout the four paradigms, two taxonomic dimensions emerge: 
% \emph{(i) Prompt space type}, distinguishing discrete ($\mathcal{P}_d$), continuous ($\mathcal{P}_c$), and hybrid ($\mathcal{P}_h$) formulations; 
% and 
% \emph{(ii) Algorithmic technique}, partitioning methods into LLM-based meta-optimization, evolutionary search, gradient-based approaches, and RL-based frameworks. 
Many works mix or extend these categories (e.g., \emph{EvoPrompt} combines FM-based generation with Genetic Algorithm (GA) operators). 
By situating each study within this two-dimensional taxonomy, we highlight shared structural principles across seemingly diverse algorithms and providing a cohesive view of the rapidly expanding literature, as shown in Table~\ref{tab:category}.
% on automated prompt engineering.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=.6\linewidth]{figs/landscape.pdf}
    \caption{The landscapes of different optimization methods.}
    \label{fig:landscape}
\end{figure}

\subsection{FM-based Optimization}

% In many prompt optimization methods, LLMs act as optimizers by iteratively refining prompts based on feedback metrics. These methods treat prompt generation as an optimization process, where LLMs enhance task-specific prompts through mechanisms such as analyzing errors, generating contrastive prompts. Below are key approaches showcasing how LLMs optimize prompts.
% \emph{ProTeGi}~\cite{pryzant2023automatic} treats prompt refinement as a textual "gradient" optimization process, using LLM-produced feedback alongside beam search and bandit selection to iteratively enhance prompts. 
% \emph{APE} \cite{zhou2022large} treats instruction generation as a black-box search over candidate instructions proposed by an LLM, aiming to maximize task performance without gradient information. 
% Its extension, 
% \emph{PE2}~\cite{ye2023prompt} uses LLM-generated meta-prompts, incorporating detailed descriptions, context specification, and step-by-step reasoning templates, to iteratively refine and optimize prompts for various language tasks.
% \emph{OPRO}\cite{yang2023largelm} utilizes LLMs as optimizers through a carefully designed meta-prompt, which iteratively integrates previously generated solutions and their evaluations to refine prompts.
% \emph{BPO}\cite{cheng2023black} optimize user prompts to align LLM outputs with user intent, using human feedback to improve alignment without updating model parameters. 
% Approaches like \emph{PromptAgent}~\cite{wang2023promptagent} treats prompt optimization as a strategic planning problem, using Monte Carlo Tree Search (MCTS) to navigate the vast space of expert-level prompts, generating high-quality prompts through iterative error feedback and reward simulation.
% \emph{AutoHint}~\cite{sun2023autohint} leverages LLMs as optimizers to automatically generate and refine prompts by inferring new hints from incorrect predictions, iteratively enhancing the initial prompt with enriched instructions derived from labeled data.
% \emph{CriSPO}~\cite{he2024crispo} uses LLMs as optimizers to automatically refine prompts by generating critique and suggestion-based feedback, improving text generation quality, with an extension called Automatic Suffix Tuning (AST) to further enhance multi-metric optimization.
% \emph{LCP}~\cite{li2024learning} enables LLMs to distinguish between effective and ineffective prompts through contrastive learning, guiding the generation of optimized prompts that adapt across different model versions, families, and languages.
% \emph{StraGo}~\cite{wu2024strago} uses LLMs as optimizers by integrating both successful and failed cases, utilizing in-context learning to generate actionable strategies for prompt optimization and reduce prompt drifting across diverse tasks.
% \emph{AMPO}~\cite{yang2024ampo} enhances prompt optimization by employing large language models to iteratively generate multi-branched prompts, refining them through feedback from failure cases and adjusting branches to better handle complex task patterns.
% \emph{OPT2I}~\cite{manas2024improving} enhances text-to-image consistency by employing large language models to iteratively refine prompts, boosting consistency scores while preserving image quality across multiple datasets.
% \emph{adv-ICL}~\cite{long2024prompt} optimizes in-context learning prompts by utilizing two LLMs—one as a generator and the other as a discriminator—in an adversarial setup, achieving significant improvements in various tasks without requiring model fine-tuning.

FM-based optimization methods directly leverage FM as \emph{meta-optimizers} to refine prompts. 
These approaches often implement iterative improvement in which an FM proposes an updated prompt based on performance feedback.
% effectively treating $P\in\mathcal{P}_d$ (or $\mathcal{P}_h$) as text strings to be edited or reorganized.

\paragraph{Heuristic Meta-Prompt}
Several methods harnesses human-designed meta-prompts, i.e., manually-craft sequences that instruct an FM how to revise an existing prompt.
\emph{PE2}~\cite{ye2023prompt} uses rich meta-descriptions, context specifications, and CoT templates to iteratively update prompts for various tasks. 
\emph{OPRO}~\cite{yang2023largelm} unifies solution exploration and evaluation, integrating previously generated solutions (and their quality metrics) within a meta-prompt that the FM uses to refine future versions of $P$. 
\emph{LCP}~\cite{li2024learning} integrates contrastive learning signals into meta-prompts, encouraging FMs to distinguish high-quality $P_d\in\mathcal{P}_d$ from suboptimal ones, and adapt across model families/languages. 
Similarly, \emph{StraGo}~\cite{wu2024strago} merges success and failure exemplars as meta-context to steer in-context learning toward more robust prompts.

\paragraph{Automatic Meta-Prompt Generation}
Another subset generate meta-prompt relying on external feedback and self-reflection. 
\emph{ProTeGi}~\cite{pryzant2023automatic} formulates an iterative ``gradient-like'' textual editing loop, incorporating beam search and multi-armed bandit strategies to refine discrete prompts $P\in\mathcal{P}_d$. 
\emph{AutoHint}~\cite{sun2023autohint} appends FM-inferred hints derived from prior prediction errors, thereby evolving initial prompts in a step-by-step manner. 
\emph{CriSPO}~\cite{he2024crispo} introduces critique-suggestion pairs that guide FM feedback to improve text generation prompts without modifying model weights; it further proposes Automatic Suffix Tuning (AST) for multi-objective prompt engineering. 
Likewise, \emph{BPO}~\cite{cheng2023black} aligns $f(P(x))$ with user intent by collecting human feedback on interim outputs, then editing $P$ accordingly.

\paragraph{Strategic Search and Replanning}
A few works incorporate explicit search strategies. 
\emph{APE}~\cite{zhou2022large} conducts black-box prompt exploration through an FM-proposed candidate pool, selecting prompts that maximize task performance without requiring gradients. 
\emph{PromptAgent}~\cite{wang2023promptagent} leverages Monte Carlo Tree Search (MCTS) to navigate a combinatorial space of expert-level prompts, applying user feedback as value signals. 
\emph{AMPO}~\cite{yang2024ampo} evolves multi-branched prompts, guided by failures and partial successes; each branch refines $P(x)$ to better handle increasingly complex task variations. 
\emph{adv-ICL}~\cite{long2024prompt} employs a generator-discriminator FM setup to explore adversarial prompts, leading to robust in-context demonstrations. 
\emph{OPT2I}~\cite{manas2024improving} focuses on text-to-image consistency by rewriting textual prompts $P_d$ to boost alignment.
% without sacrificing visual quality.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllll}
\hline
\textbf{Method} & \textbf{Optimization Space} & \textbf{Variable Type} & \textbf{Optimization Methods} & \textbf{Optimization Strategy} \\ \hline
PE2~\cite{ye2023prompt} & Discrete & Instructions & FM-based Optimization & Heuristic Meta-Prompt \\ 
OPRO~\cite{yang2023largelm} & Discrete & Instructions & FM-based Optimization & Heuristic Meta-Prompt \\ 
LCP~\cite{li2024learning} & Discrete & Instructions & FM-based Optimization & Heuristic Meta-Prompt \\ 
StraGo~\cite{wu2024strago} & Discrete & Instructions & FM-based Optimization & Heuristic Meta-Prompt \\ 
ProTeGi~\cite{pryzant2023automatic} & Discrete & Instructions &  FM-based Optimization & Automatic Meta-Prompt Generation \\ 
AutoHint~\cite{sun2023autohint} & Discrete & Instructions &  FM-based Optimization & Automatic Meta-Prompt Generation \\ 
CriSPO~\cite{he2024crispo} & Discrete & Instructions &  FM-based Optimization & Automatic Meta-Prompt Generation \\ 
BPO~\cite{cheng2023black} & Discrete & Instructions & FM-based Optimization & Automatic Meta-Prompt Generation \\ 
APE~\cite{zhou2022large} & Discrete & Instructions &  FM-based Optimization & Strategic Search and Replanning \\ 
PromptAgent~\cite{wang2023promptagent} & Discrete & Instructions &  FM-based Optimization & Strategic Search and Replanning \\ 
AMPO~\cite{yang2024ampo} & Discrete & Instructions & FM-based Optimization & Strategic Search and Replanning \\ 
adv-ICL~\cite{long2024prompt} & Discrete & Instructions & FM-based Optimization & Strategic Search and Replanning \\ 
OPT2I~\cite{manas2024improving} & Discrete & Instructions & FM-based Optimization & Strategic Search and Replanning \\ 
GPS~\cite{xu2022gps} & Discrete & Instructions & Evolutionary Computing & Genetic Operators and Heuristics \\ 
LongPO~\cite{hsieh2023automatic} & Discrete & Instructions & Evolutionary Computing & Genetic Operators and Heuristics \\ 
GrIPS~\cite{prasad2022grips} & Discrete & Instructions & Evolutionary Computing & Genetic Operators and Heuristics \\ 
EvoPrompt~\cite{guo2023connecting} & Discrete & Instructions & Evolutionary Computing & Self-Referential Evolution \\ 
HPME~\cite{wen2024hard} & Discrete & Instructions & Gradient-Based Optimization & Discrete Token Gradient Methods \\ 
AutoPrompt~\cite{shin2020autoprompt} & Discrete & Instructions & Gradient-Based Optimization & Discrete Token Gradient Methods \\ 
ZOPO~\cite{hu2024localized} & Discrete & Instructions & Gradient-Based Optimization & Discrete Token Gradient Methods \\ 
RLPrompt~\cite{deng2022rlprompt} & Discrete & Instructions & Reinforcement Learning & Prompt Editing as RL Actions \\ 
TEMPERA~\cite{zhang2022tempera} & Discrete & Instructions & Reinforcement Learning & Prompt Editing as RL Actions \\ 
PRewrite~\cite{kong2024prewrite} & Discrete & Instructions & Reinforcement Learning & Prompt Editing as RL Actions \\ 
PACE~\cite{dong2023pace} & Discrete & Instructions & Reinforcement Learning & Prompt Editing as RL Actions \\ 
StablePrompt~\cite{kwon2024stableprompt} & Discrete & Instructions & Reinforcement Learning & Prompt Editing as RL Actions \\ 
Evoke~\cite{hu2023evoke} & Discrete & Instructions & Reinforcement Learning & Prompt Editing as RL Actions \\ 
Prompt-OIRL~\cite{sun2023query} & Discrete & Instructions & Reinforcement Learning & Multi-Objective and Inverse RL Strategies \\ 
MORL-Prompt~\cite{jafari2024morl} & Discrete & Instructions & Reinforcement Learning & Multi-Objective and Inverse RL Strategies \\ 
MAPO~\cite{chen2024mapo} & Discrete & Instructions & Reinforcement Learning & Multi-Objective and Inverse RL Strategies \\
Self-ask~\cite{press2023measuring} & Discrete & Thoughts & FM-based Optimization & Strategic Search and Replanning \\
Reprompt~\cite{chen2024reprompt} & Discrete & Thoughts  & FM-based Optimization & Strategic Search and Replanning \\
PROMST~\cite{chen2024prompt} & Discrete & Thoughts & Evolutionary Computing & Self-Referential Evolution \\ 
ReAcT~\cite{yaoreact} & Discrete & Thoughts & FM-based Optimization & Strategic Search and Replanning \\
Self-refine~\cite{madaan2024self} & Discrete & Thoughts &  FM-based Optimization & Strategic Search and Replanning \\ 
KATE~\cite{liu2022makes} & Discrete & Few-shot Examples &  FM-based Optimization & Strategic Search and Replanning \\ 
AL~\cite{margatina2023active} & Discrete & Few-shot Examples & FM-based Optimization & Strategic Search and Replanning \\ 
AES~\cite{zhang2022active} & Discrete & Few-shot Examples & Reinforcement Learning & Prompt Editing as RL Actions \\ 
Ordered Prompt~\cite{lu2022fantastically} & Discrete & Few-shot Examples & FM-based Optimization & Strategic Search and Replanning \\
SA~\cite{kirillov2023segment} & Discrete & Annotations &  FM-based Optimization & Automatic Meta-Prompt Generation\\
KOSMOS-2~\cite{peng2023kosmos} & Discrete & Annotations & FM-based Optimization & Automatic Meta-Prompt Generation\\
Visualcues~\cite{denner2024visual} & Discrete & Annotations & FM-based Optimization & Automatic Meta-Prompt Generation\\
Prefix-tuning~\cite{li2021prefix} & Continuous & Soft Prompt & Gradient-Based Optimization & Soft Prompt Tuning \\ 
Prompt-Tuning~\cite{lester2021power} & Continuous & Soft Prompt & Gradient-Based Optimization & Soft Prompt Tuning \\ 
P-Tuning~\cite{liu2024gpt} & Continuous & Soft Prompt & Gradient-Based Optimization & Soft Prompt Tuning \\ 
PhaseEvo~\cite{cui2024phaseevo} & Hybrid & Instructions and Exemplars & Evolutionary Computing & Genetic Operators and Heuristics \\ 
Promptbreeder~\cite{fernando2023promptbreeder} & Hybrid & Instructions and Exemplars & Evolutionary Computing & Self-Referential Evolution \\ 
Mixture-of-Prompts~\cite{wangmixture} & Hybrid & Instructions and Exemplars &  FM-based Optimization & Automatic Meta-Prompt Generation \\
Promptwizard~\cite{agarwal2024promptwizard} & Hybrid & Instructions and Exemplars & FM-based Optimization & Automatic Meta-Prompt Generation \\ 
\hline


\end{tabular}
}
\caption{Categorization of representive papers based on optimization variables and methods.}
\label{tab:category}
\end{table}







\subsection{Evolutionary Computing}

% An alternative branch of research explores \emph{evolutionary} or \emph{genetic} algorithms (GA) to iteratively refine prompts without gradient access. 
% These approaches treat prompts as ``individuals'' that undergo mutation or crossover, preserving only high-performing ``offspring'' based on a small validation set. 
% For instance, \emph{GPS}~\cite{xu2022gps} employs a straightforward GA to enhance few-shot prompts, while \emph{LongPO}~\cite{hsieh2023automatic} extends similar principles to lengthy prompts by combining beam-search-based heuristics and history-guided mutations. 
% Though \emph{GrIPS}~\cite{prasad2022grips} is not strictly a genetic framework, it still generates prompt offspring through local edits on parent instructions to improve task accuracy. \emph{EvoPrompt}~\cite{guo2023connecting} makes LLMs act as evolutionary ``operators'' for generating candidate solutions, and \emph{Promptbreeder}~\cite{fernando2023promptbreeder} introduces a self-referential loop that co-evolves both the task prompts and the mutation prompts. 
% Further, \emph{PhaseEvo}~\cite{cui2024phaseevo} unifies instruction and example optimization in a multi-phase evolutionary design, whereas \emph{PROMST}~\cite{chen2024prompt} adapts genetic-style sampling to multi-step tasks by incorporating human feedback rules and a learned heuristic model. 
% Across these works, parent prompts are iteratively modified into offspring with superior ``fitness,'' leading to notable gains over human-engineered baselines in both short and more complex prompt settings.

Evolutionary methods model prompt optimization as a genetic or evolutionary process. 
They treat the prompt $P\in\mathcal{P}_d$ as an ``organism,'' mutated or crossed over to produce ``offspring'' that survive based on higher fitness (i.e., the performance measure $g(f(P(x)),\,y)$). 
These approaches are particularly suitable for purely discrete prompt spaces.

\paragraph{Genetic Operators and Heuristics}
\emph{GPS}~\cite{xu2022gps} applies a straightforward genetic algorithm to refine few-shot instruction prompts, iteratively mutating tokens and retaining top-performing discrete prompts. 
\emph{LongPO}~\cite{hsieh2023automatic} extends these ideas to long prompts by incorporating beam search heuristics and a history buffer to preserve context across mutations. 
Though not strictly a GA, \emph{GrIPS}~\cite{prasad2022grips} shares a similar local-edit mechanism to produce improved child prompts from parent instructions. 
\emph{PhaseEvo}~\cite{cui2024phaseevo} further unifies instruction and example optimization into a multi-phase generation pipeline, alternating between refining $I,\,T\in\mathcal{V}^*$ and exemplar sets $\{e_i\}_{i=1}^k\subseteq\mathcal{E}$.

\paragraph{Self-Referential Evolution}
Other approaches tap into FMs themselves as evolutionary operators. 
\emph{EvoPrompt}~\cite{guo2023connecting} uses the FM to propose candidate mutations, combining them with a fitness-based selection procedure. 
\emph{Promptbreeder}~\cite{fernando2023promptbreeder} co-evolves \emph{both} task prompts and the mutation prompts: the latter are instructions specifying how to mutate or cross over parent prompts. 
\emph{PROMST}~\cite{chen2024prompt} specializes in multi-step tasks. 
It incorporates a human-in-the-loop rule set and a learned heuristic model, mixing evolutionary sampling with user feedback to gradually improve the textual prompt.

\subsection{Gradient-Based Optimization}

% Traditional gradient descent underpins many optimization strategies in artificial intelligence, exploiting the continuous nature of neural network parameter spaces. 
% However, optimizing hard prompts which consist of discrete tokens—poses a unique challenge due to the non-differentiable nature of the search space. 
% This necessitates innovative adaptations of gradient-based techniques: 
% for open-source models, one may access true gradients to fine-tune prompts directly, while for closed-source models, researchers must resort to approximating or mimicking gradient signals to guide optimization.

% While many prompt optimizers avoid gradient access, a separate line of work leverages approximate or direct gradients to refine prompts in a controlled manner. 
% \emph{HPME}~\cite{wen2024hard} projects learned embeddings back onto discrete tokens each iteration, blending soft optimization with nearest-neighbor projection to produce interpretable hard prompts. 
% \emph{AutoPrompt}~\cite{shin2020autoprompt} uses gradient-guided token insertions to elicit knowledge from masked language models by formulating tasks as cloze-style fill-ins. \emph{ZOPO}~\cite{hu2024localized} localizes zeroth-order updates with a neural tangent kernel, focusing on well-performing local optima in discrete prompt spaces. 

% Soft prompts are learnable continuous embeddings attached to model inputs, optimized via gradient descent to adapt large language models to specific tasks. 
% While not human-readable, these virtual tokens are compact, flexible, and enable efficient, parameter-light tuning. 
% For example, \emph{Prefix-tuning}~\cite{li2021prefix} appends trainable vectors to model inputs, maintaining frozen base model parameters while achieving competitive performance. 
% \emph{Prompt-Tuning}~\cite{lester2021power} adds a small set of trainable tokens to the input, leveraging larger models' scalability for effective task adaptation. 
% Building on these, \emph{P-Tuning}~\cite{liu2024gpt} integrates trainable prompts across multiple hidden layers, enhancing performance, especially in few-shot scenarios. 

Gradient-based strategies derive from classical optimization principles, but face unique obstacles in prompt engineering since discrete tokens $\bigl[I,T,\{e_i\}\bigr]\in\mathcal{P}_d$ are not directly differentiable. 
Methods in this family either approximate gradients to navigate discrete spaces or, more commonly, optimize continuous parameters $\theta\in\mathbb{R}^d$ in a soft prompt context $P\in\mathcal{P}_c$.

\paragraph{Discrete Token Gradient Methods}
For closed-source FMs, direct gradient access is often unavailable, requiring alternative solutions:
\emph{HPME}~\cite{wen2024hard} projects a learned continuous embedding back to discrete tokens each iteration, blending soft gradient updates with nearest-neighbor token matching. 
\emph{AutoPrompt}~\cite{shin2020autoprompt} constructs prompts by adding tokens that maximize the gradient toward correct labels in a masked language modeling scenario. 
\emph{ZOPO}~\cite{hu2024localized} implements zeroth-order optimization in discrete prompt spaces by sampling localized perturbations in the token domain, guided by a neural tangent kernel approximation.

\paragraph{Soft Prompt Tuning}
Soft prompt methods treat $P\in\mathcal{P}_c$ as a set of trainable vectors $\{\theta_1,\dots,\theta_m\}$ that concatenate with the embedding of $x$, i.e., $P(e_x)=[\theta_1,\ldots,\theta_m; e_x]$. 
\emph{Prefix-tuning}~\cite{li2021prefix} attaches learnable prefix vectors in the hidden states of a language model, requiring only a small fraction of trainable parameters. 
\emph{Prompt-Tuning}~\cite{lester2021power} similarly adds trainable embeddings at the input layer, benefiting from large model scalability. 
\emph{P-Tuning}~\cite{liu2024gpt} extends trainable prompts into multiple layers, significantly improving few-shot performance. 
All these methods solve variants of the continuous optimization subproblem $\max_{P\in\mathcal{P}_c}\;\mathbb{E}_{(x,y)\sim\mathcal{D}_{\text{val}}}[g(f(P(e_x)),\,y)]$
and hence leverage standard gradient descent w.r.t. $\theta_i\in\mathbb{R}^d$.

\subsection{Reinforcement Learning}

% A growing number of approaches utilize reinforcement learning (RL) to iteratively refine prompts for LLMs, enabling automated optimization beyond simple gradient-based or heuristic methods. \emph{RLPrompt} \cite{deng2022rlprompt} views discrete text tokens as actions and applies reward signals from the LLM environment to discover effective prompts. \emph{TEMPERA} \cite{zhang2022tempera} conducts test-time prompt editing with RL to adaptively adjust instructions for each query, while \emph{PRewrite} \cite{kong2024prewrite} employs a trained prompt rewriter that uses RL to boost performance on downstream tasks. In a similar vein, \emph{PACE} \cite{dong2023pace} leverages an actor-critic framework to improve suboptimal human-written prompts, and \emph{StablePrompt} \cite{kwon2024stableprompt} introduces adaptive proximal policy optimization for more stable training. \emph{Evoke} \cite{hu2023evoke} enforces a reviewer-author feedback loop to iteratively refine prompts, and \emph{Prompt-OIRL} \cite{sun2023query} applies offline inverse RL to learn a query-dependent reward model, thereby selecting optimal prompts without frequent interaction with the LLM. 
% Additionally, \emph{MORL-Prompt} \cite{jafari2024morl} addresses scenarios where reward signals are in tension, such as style transfer and machine translation. By adapting multi-objective RL methods , it aims to balance conflicting reward functions rather than merely maximizing their average. Meanwhile, \emph{MAPO} \cite{chen2024mapo} proposes a model-adaptive prompt optimizer that unifies supervised fine-tuning and reinforcement learning steps to tailor original prompts to each specific LLM, leading to substantial performance gains across multiple downstream tasks. Collectively, these RL-based methods advance the state of automated prompt engineering by systematically learning to produce or edit prompts that elicit superior performance across diverse NLP tasks while also accommodating scenarios where multiple, potentially conflicting, objectives must be balanced.

RL methods recast prompt design as an RL problem in which $P\in\mathcal{P}$ (often $\mathcal{P}_d$ or $\mathcal{P}_h$) is updated via a sequence of actions under a reward defined by $g\bigl(f(P(x)),\,y\bigr)$.
Across these RL methods, the key idea is to formulate optimization objective $
\max_{P\in\mathcal{P}}\;\mathbb{E}_{(x,y)\sim\mathcal{D}_{\text{val}}}[g(f(P(x)),y)]$
as a Markov Decision Process, where partial or recurrent prompt edits constitute actions and $g$ serves either directly as the reward function or as part of a learned proxy. 
Such frameworks unify discrete prompt editing ($P\in\mathcal{P}_d$) and continuous prompt adaptation ($P\in\mathcal{P}_c$), even allowing for hybrid forms $P\in\mathcal{P}_h$ where some components are differentiable (\emph{soft prompts}) and others remain discrete \emph{(hard instructions)}.

\paragraph{Prompt Editing as RL Actions}
\emph{RLPrompt}~\cite{deng2022rlprompt} represents discrete tokens $v\in\mathcal{V}$ as RL actions, exploring the space of textual prompts with policy gradient methods. 
\emph{TEMPERA}~\cite{zhang2022tempera} proposes test-time RL-based editing, adjusting each query's prompt adaptively. 
\emph{PRewrite}~\cite{kong2024prewrite} trains a separate prompt rewritter using RL signals to maximize downstream task accuracy. 
In a similar vein, \emph{PACE}~\cite{dong2023pace} refines suboptimal human prompts ($P_{\text{human}}$) by iterative RL feedback, while \emph{StablePrompt}~\cite{kwon2024stableprompt} adapts proximal policy optimization to mitigate training instability. 
\emph{Evoke}~\cite{hu2023evoke} establishes a reviewer-author loop, feeding prompt outputs into a critic that suggests incremental edits.

\paragraph{Multi-Objective and Inverse RL Strategies}
Other RL approaches tackle multi-objective or partial feedback scenarios. 
\emph{Prompt-OIRL}~\cite{sun2023query} employs offline inverse RL to learn a query-specific reward model, thus selecting an optimal prompt without frequent FM interactions. 
\emph{MORL-Prompt}~\cite{jafari2024morl} addresses conflicting reward functions (e.g., style vs.\ accuracy) by adapting multi-objective RL techniques. 
\emph{MAPO}~\cite{chen2024mapo} combines supervised fine-tuning and RL in a model-adaptive prompt optimizer that tailors $P$ to each target FM, demonstrating notable gains across diverse downstream tasks.

\section{Future Directions}\label{sec:future}

The systematic study of prompt optimization for foundation models, from an applied optimization perspective, presents extensive opportunities but remains loosely explored. 
Below, we outline key research themes, highlighting current progress, pivotal questions, and open challenges.

\paragraph{Constraint optimization}
Presents methods seldom incorporate semantic or ethical constraints in discrete prompt spaces. 
The main issue lies in constructing a search mechanism that respects human-value alignment, resource bounds, and readability, particularly in high-dimensional symbolic domains. 
Another challenge is formalizing these constraints as tractable mathematical conditions that guide search algorithms without sacrificing flexibility or linguistic quality.

\paragraph{Multi-task prompt optimization}
It's crucial for leveraging shared structures across tasks. 
While some work suggests factorized or sparse representations to capture prompt-level similarity, formal definitions of inter-task ``prompt similarity'' are lacking. 
Negative transfer can also arise, where improvements for one task degrade performance on another. 
The field needs robust frameworks to codify these trade-offs, enhancing generalization and adaptivity.

\paragraph{Online prompt optimization}
Current techniques generally prioritize offline scenarios. 
However, user intentions can shift over time, creating the need for algorithms that maintain stable performance (e.g., bounded dynamic regret) in nonstationary environments. 
Reliance on online updates amplifies the complexity of discrete search in high-dimensional prompt spaces. 
Furthermore, real-time user feedback loops introduce additional uncertainties, demanding advanced convergence analyses.

\paragraph{Multi-objective prompt optimization} 
Multi-objective prompt optimization aims to balance often competing goals such as accuracy and interpretability. 
Many existing studies use single-metric optimization, overlooking fundamental human-centered preferences. 
One promising direction is the incorporation of Pareto-based methods or multi-criteria decision-making, accompanied by geometric representations of preference spaces. 
Game-theoretic techniques may also help arbitrating among conflicting objectives, both within and across user populations.

\paragraph{Heterogeneous modality optimization}
Although many works focus on textual cues, prompts in computer vision and other modalities, such as bounding boxes or pixel-level annotations, remain far less explored. 
This calls for a deeper understanding of cross-modal coupling, as well as conditions under which modalities are amenable to joint or separate optimization. 
Such advances might require novel manifold-based or graph-based tools to unify distinct prompt representations.

\paragraph{Bi-level prompt optimization} 
Bi-level prompt optimization arises with step-by-step ``thought-driven'' models (e.g., OpenAI-o1, Deepseek-R1), where the entire inference chain depends on the prompt as a high-level controller. 
This hierarchical structure challenges standard prompt optimization, as small changes can drastically alter the reasoning trajectory. 
It remains open whether stable equilibria for these layered systems exist and how sensitive they are to prompt perturbations. 
Borrowing methods from multi-level optimization could clarify both existence and uniqueness conditions for equilibria.

\paragraph{Broader application scenarios}
For multi-turn agents, prompt optimization unfolds over sequential decisions, significantly complicating nonstationarity. 
Introducing game-theoretic elements for multi-agent designs further underscores the importance of collaborative or competitive equilibrium concepts. 
Similarly, vertical-domain large models (e.g., reinforcement learning, AI4Science) impose domain-specific constraints that standard prompt optimization does not account for, calling for specialized theoretical adaptations. 


\section{Discussion and Conclusion}\label{sec:conclusion}

% This survey delineates a optimization-theoretic foundation for automated prompt engineering that transcends fragmented treatments across modalities. 
% By systematically dissecting discrete, continuous, and mixed prompt spaces, we have illuminated critical algorithmic and theoretical principles for developing robust, context-aware solutions. 
% Moving forward, tighter integration of multi-level, multi-objective, and online optimization approaches will be pivotal in shaping prompt designs for emerging foundation models.
% Continued methodological will further expand the horizons of efficient, ethically aligned, and computationally tractable prompting strategies.

This survey delineates a optimization-theoretic foundation for automated prompt engineering that transcends fragmented treatments across modalities.  
By synthesizing methods that target discrete, continuous, and hybrid prompt spaces, we have underscored how variables such as instructions, soft prompts, and exemplars can be systematically optimized under unified theoretical principles. 
Together with our taxonomy of task objectives and unified perspective on FM as optimizer, evolutionary computing, gradient-based, and RL-driven methods, we establish key foundations for theoretical inquiry and realistic application. 
Moving forward, tighter integration of multi-level, multi-objective, and online optimization will be pivotal in shaping prompt designs for emerging foundation models.
% Crucially, the advanced optimization approaches discussed here pave the way for robust, efficient, and context-aware prompt designs in domain-specific foundation models. 
% We believe this integrated framework, together with open challenges identified in areas like constraint optimization, multi-task objectives, and nonstationary settings, will stimulate new interdisciplinary research that closes the gap between theoretical developments and scalable, ethically aligned systems in practice. 

% The future challenges in automated prompt optimization for VLMs and LLMs revolve around five key barriers.

% Firstly, resource-efficiency remains a major challenge. Iterative optimization methods, which involve continuously refining prompts, can be prohibitively expensive in terms of computation and API usage. On the other hand, compression techniques, designed to reduce computational costs, often compromise the semantics of the prompt, undermining performance for task-critical scenarios.

% Secondly, multimodal prompt optimization introduces its own set of difficulties. In VLMs, optimizing both text and visual components simultaneously can lead to semantic disintegration. This becomes particularly problematic in domains like medical imaging, where precise alignment between anatomical descriptors and pixel-level features is critical for accurate model performance.

% Thirdly, optimization instability manifests in two ways: prompt drifting and cross-domain fragility. Prompt drifting occurs when local improvements disrupt previously functional components of a prompt, leading to inconsistent model behavior. Cross-domain fragility arises when prompts optimized for one domain, such as natural images, perform poorly when applied to specialized tasks like radiology report generation.

% Fourthly, opacity-safety tensions present a challenge in terms of model transparency. Continuous prompt optimization in VLMs and LLMs can generate opaque, uninterpretable vector spaces, while compressed prompts for efficiency lose their human-readable structure. This makes it difficult to validate the model's decisions for safety and ethical concerns, especially in high-stakes areas like clinical applications.

% Finally, a significant issue with current methods is that most optimization efforts focus solely on instructions while giving less attention to the optimization of exemplars (input-output pairs). This imbalance reduces the ability to achieve an effective performance-cost balance, which is essential for the deployment of models in real-world, resource-constrained environments.

% To address these challenges, future research should focus on balancing performance and cost, with particular attention given to the development of effective multimodal optimization techniques and cross-domain adaptability. Furthermore, efforts should be directed toward improving the stability, transparency, and practical applicability of these models, ensuring that they are both effective and safe for deployment in real-world applications.

% \section{Conclusion}\label{sec:conclusion}


% Proofs must be written in their own paragraph(s) separated by at least 2pt and no more than 5pt from the preceding and succeeding paragraphs. Proof paragraphs should start with the keyword ``Proof." in 10pt italics font. After that the proof follows in regular 10pt font. At the end of the proof, an unfilled square symbol (qed) marks the end of the proof.

% In \LaTeX{} proofs should be typeset using the \texttt{\textbackslash{proof}} environment.

% \begin{proof}
%     This paragraph is an example of how a proof looks like using the \texttt{\textbackslash{proof}} environment.
% \end{proof}

% \section*{Acknowledgments}

% The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
% files that implement them was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Preparation of the Microsoft Word file was supported by IJCAI.  An
% early version of this document was created by Shirley Jowell and Peter
% F. Patel-Schneider.  It was subsequently modified by Jennifer
% Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
% Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
% Francisco Cruz-Mencia and Edith Elkind.

\clearpage
\newpage

%% The file named.bst is a bibliography style file for BibTeX 0.99c
% \bibliographystyle{named}
% \bibliography{ijcai24}
% \bibliography{main}
\bibliographystyle{alpha}
\bibliography{main2}
\end{document}


