@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, P. and Yuan, W. and Fu, J. and others},
  journal={ACM Computing Surveys},
  year={2023},
  publisher={ACM New York, NY}
}

@article{li2024prompt,
  title={Prompt Compression for {LLMs}: A Survey},
  author={Li, Z. and Liu, Y. and Su, Y. and Collier, N.},
  journal={arXiv},
  year={2024}
}

@article{gu2023systematic,
  title={A systematic survey of prompt engineering on vision-language foundation models},
  author={Gu, J. and Han, Z. and Chen, S. and Beirami, A. and others},
  journal={arXiv},
  year={2023}
}

@article{sahoo2024systematic,
  title={A systematic survey of prompt engineering in {LLMs}: Techniques and applications},
  author={Sahoo, P. and Singh, A. K. and Saha, S. and Jain, V. and others},
  journal={arXiv},
  year={2024}
}

@article{amatriain2024prompt,
  title={Prompt design and engineering: Introduction and advanced methods},
  author={Amatriain, X.},
  journal={arXiv},
  year={2024}
}

@inproceedings{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, X. and Zhao, J. and LeCun, Y.},
  booktitle={NeurIPS},
  year={2015}
}

@inproceedings{mcauley2013hidden,
  title={Hidden factors and hidden topics: understanding rating dimensions with review text},
  author={McAuley, J. and Leskovec, J.},
  booktitle={RecSys},
  year={2013}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, R. and Perelygin, A. and Wu, J. and Chuang, J. and others},
  booktitle={EMNLP},
  year={2013}
}

@article{vatsal2024survey,
  title={A survey of prompt engineering methods in {LLMs} for different nlp tasks},
  author={Vatsal, S. and Dubey, H.},
  journal={arXiv},
  year={2024}
}                                                     

@article{chang2024efficient,
  title={Efficient Prompting Methods for {LLMs}: A Survey},
  author={Chang, K. and Xu, S. and Wang, C. and Luo, Y. and others},
  journal={arXiv},
  year={2024}
}

@inproceedings{Honovich2022InstructionIF,
  title={Instruction Induction: From Few Examples to Natural Language Task Descriptions},
  author={Honovich, O. and Shaham, U. and Bowman, S. R. and Levy, O.},
  booktitle={ACL},
  year={2022}
}

@inproceedings{pang2005seeing,
  title={Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales},
  author={Pang, B. and Lee, L.},
  booktitle={ACL},
  year={2005}
}

@inproceedings{pang2004sentimental,
  title={A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts},
  author={Pang, B. and Lee, L.},
  booktitle={ACL},
  year={2004}
}

@inproceedings{hu2004mining,
  title={Mining and summarizing customer reviews},
  author={Hu, M. and Liu, B.},
  booktitle={KDD},
  year={2004}
}

@inproceedings{voorhees2000building,
  title={Building a question answering test collection},
  author={Voorhees, E. M. and Tice, D. M.},
  booktitle={IR},
  year={2000}
}

@inproceedings{patel2021nlp,
  title={Are NLP models really able to solve simple math word problems?},
  author={Patel, A. and Bha., S. and Goyal, N.},
  booktitle={NAACL},
  year={2021}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, K. and Kosaraju, V. and Bavarian, M. and Chen, M. and others},
  journal={arXiv},
  year={2021}
}

@inproceedings{koncel2016mawps,
  title={MAWPS: A math word problem repository},
  author={Koncel-Kedziorski, R. and Roy, S. and Amini, A. and Kushman, N. and Hajishirzi, H.},
  booktitle={NAACL-HLT},
  year={2016}
}

@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, W. and Yogatama, D. and Dyer, C. and Blunsom, P.},
  journal={arXiv},
  year={2017}
}

@inproceedings{suzgun2022challenging,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, M. and Scales, N. and Sch{\"a}rli, N. and Gehrmann, S. and others},
  booktitle={ACL},
  year={2023}
}

@inproceedings{pal2022medmcqa,
  title={Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering},
  author={Pal, A. and Umapathi, L. K. and Sankarasubbu, M.},
  booktitle={CHIL},
  year={2022}
}

@article{jin2021disease,
  title={What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author={Jin, D. and Pan, E. and Oufattole, N. and others},
  journal={Applied Sciences},
  year={2021},
}

@inproceedings{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, A. and Herzig, J. and Lourie, N. and Berant, J.},
  booktitle={ACL},
  year={2019}
}

@inproceedings{geva2021did,
  title={Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
  author={Geva, M. and Khashabi, D. and Segal, E. and Khot, T. and others},
  booktitle={ACL},
  year={2021}
}

@inproceedings{de2019commitmentbank,
  title={The commitmentbank: Investigating projection in naturally occurring discourse},
  author={De Marneffe, M.-C. and Simons, M. and Tonhauser, J.},
  booktitle={SB},
  year={2019}
}

@inproceedings{schuhmann2022laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, C. and Beaumont, R. and Vencu, R. and Gordon, C. and others},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, T.-Y. and Maire, M. and Belongie, S. and Hays, J. and others},
  booktitle={ECCV},
  year={2014}
}

@inproceedings{liu2015deep,
  title={Deep learning face attributes in the wild},
  author={Liu, Z. and Luo, P. and Wang, X. and Tang, X.},
  booktitle={ICCV},
  year={2015}
}

@article{santanagustavosta,
  title={Gustavosta/Stable-Diffusion-Prompts{\textperiodcentered} Datasets at Hugging Face, December 2022},
  author={Santana, G.},
  journal={URL https://huggingface. co/datasets/Gustavosta/Stable-Diffusion-Prompts}
}

@inproceedings{zhang2022tempera,
  title={Tempera: Test-time prompting via reinforcement learning},
  author={Zhang, T. and Wang, X. and Zhou, D. and others},
  booktitle={ICLR},
  year={2022}
}

@article{dong2023pace,
  title={PACE: Improving Prompt with Actor-Critic Editing for Large Language Model},
  author={Dong, Y. and Luo, K. and others.},
  journal={arXiv},
  year={2023}
}

@inproceedings{deng2022rlprompt,
  title={Rlprompt: Optimizing discrete text prompts with reinforcement learning},
  author={Deng, M. and Wang, J. and others},
  booktitle={EMNLP},
  year={2022}
}

@article{kong2024prewrite,
  title={PRewrite: Prompt Rewriting with Reinforcement Learning},
  author={Kong, W. and Hombaiah, S. A. and Zhang, M. and Mei, Q. and Bendersky, M.},
  journal={arXiv},
  year={2024}
}

@inproceedings{kwon2024stableprompt,
  title={StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for {LLMs}},
  author={Kwon, M. and Kim, G. and Kim, J. and Lee, H. and Kim, J.},
  booktitle={EMNLP},
  year={2024}
}

@inproceedings{sun2023query,
  title={Query-dependent prompt evaluation and optimization with offline inverse RL},
  author={Sun, H. and H{\"u}y{\"u}k, A. and van der Schaar, M.},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{hu2023evoke,
  title={Evoke: Evoking Critical Thinking Abilities in {LLMs} via Reviewer-Author Prompt Editing},
  author={Hu, X. and Tang, P. and Zuo, S. and Wang, Z. and others},
  booktitle={NeurIPS Workshop},
  year={2023}
}

@article{jafari2024morl,
  title={MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization},
  author={Jafari, Y. and Mekala, D. and Yu, R. and others},
  journal={arXiv},
  year={2024}
}


@inproceedings{guo2023connecting,
  title={Connecting {LLMs} with evolutionary algorithms yields powerful prompt optimizers},
  author={Guo, Q. and Wang, R. and Guo, J. and Li, B. and others},
  booktitle={ICLR},
  year={2023}
}

@article{cui2024phaseevo,
  title={PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models},
  author={Cui, W. and Zhang, J. and Li, Z. and Sun, H. and others},
  journal={arXiv},
  year={2024}
}

@inproceedings{fernando2023promptbreeder,
  title={Promptbreeder: Self-referential self-improvement via prompt evolution},
  author={Fernando, C. and Banarse, D. and Michalewski, H. and Osindero, S. and Rockt{\"a}schel, T.},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{xu2022gps,
  title={GPS: Genetic prompt search for efficient few-shot learning},
  author={Xu, H. and Chen, Y. and Du, Y. and others.},
  booktitle={EMNLP},
  year={2022}
}

@inproceedings{chen2024prompt,
  title={PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling},
  author={Chen, Y. and Arkin, J. and Hao, Y. and Zhang, Y. and others},
  booktitle={EMNLP},
  year={2024}
}

@inproceedinrasad{2022grips,
  title={Grips: Gradient-free, edit-based instruction search for prompting {LLMs}},
  author={Prasad, A. and Hase, P. and Zhou, X. and Bansal, M.},
  booktitle={ACL},
  year={2023}
}

@inproceedings{chen2024mapo,
  title={MAPO: Boosting large language model performance with model-adaptive prompt optimization},
  author={Chen, Y. and Wen, Z. and Fan, G. and Chen, Z. and others},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{wen2024hard,
  title={Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery},
  author={Wen, Y. and Jain, N. and Kirchenbauer, J. and Goldblum, M. and others},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{pryzant2023automatic,
  title={Automatic prompt optimization with ``gradient descent'' and beam search},
  author={Pryzant, R. and Iter, D. and others},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{shin2020autoprompt,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, T. and Razeghi, Y. and Logan IV, R. L. and Wallace, E. and Singh, S.},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{hu2024localized,
  title={Localized zeroth-order prompt optimization},
  author={Hu, W. and Shu, Y. and Yu, Z. and Wu, Z. and others},
  booktitle={ICML Workshop},
  year={2024}
}

@inproceedings{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, X. L. and Liang, P.},
  booktitle={ACL},
  year={2021}
}

@inproceedings{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, B. and Al-Rfou, R. and Constant, N.},
  booktitle={EMNLP},
  year={2021}
}

@article{liu2024gpt,
  title={GPT understands, too},
  author={Liu, X. and Zheng, Y. and others},
  journal={AI Open},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{zhou2022large,
  title={{LLMs} are human-level prompt engineers},
  author={Zhou, Y. and Muresanu, A. I. and Han, Z. and others},
  booktitle={ICLR},
  year={2022}
}

@article{ye2023prompt,
  title={Prompt engineering a prompt engineer},
  author={Ye, Q. and Axmed, M. and Pryzant, R. and Khani, F.},
  journal={arXiv},
  year={2023}
}


@inproceedings{wang2023promptagent,
  title={Promptagent: Strategic planning with language models enables expert-level prompt optimization},
  author={Wang, X. and Li, C. and Wang, Z. and Bai, F. and others},
  booktitle={ICLR},
  year={2024}
}

@article{sun2023autohint,
  title={Autohint: Automatic prompt optimization with hint generation},
  author={Sun, H. and Li, X. and Xu, Y. and Homma, Y. and others},
  journal={arXiv},
  year={2023}
}



@inproceedings{shi2022toward,
  title={Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?},
  author={Shi, W. and Han, X. and Gonen, H. and Holtzman, A. and others},
  booktitle={EMNLP},
  year={2022}
}

@inproceedings{yang2024ampo,
  title={AMPO: Automatic Multi-Branched Prompt Optimization},
  author={Yang, S. and Wu, Y. and Gao, Y. and Zhou, Z. and others},
  booktitle={EMNLP},
  year={2024}
}

@inproceedings{xu2023reprompting,
  title={Reprompting: Automated chain-of-thought prompt inference through gibbs sampling},
  author={Xu, W. and Banburski-Fahey, A. and Jojic, N.},
  booktitle={ICML},
  year={2024}

}


@article{manas2024improving,
  title={Improving text-to-image consistency via automatic prompt optimization},
  author={Ma{\~n}as, O. and Astolfi, P. and others},
  journal={arXiv},
  year={2024}
}



@article{long2024prompt,
  title={Prompt optimization via adversarial in-context learning},
  author={Long, D. and Zhao, Y. and Brown, H. and others.},
  journal={arXiv},
  year={2024}
}

@article{he2024crispo,
  title={CriSPO: Multi-aspect critique-suggestion-guided automatic prompt optimization for text generation},
  author={He, H. and Liu, Q. and Xu, L. and Shivade, C. and others},
  journal={arXiv},
  year={2024}
}

@article{li2024learning,
  title={Learning from Contrastive Prompts: Automated Optimization and Adaptation},
  author={Li, M. and Aggarwal, K. and Xie, Y. and Ahmad, A. and Lau, S.},
  journal={arXiv},
  year={2024}
}

@article{wu2024strago,
  title={Strago: Harnessing strategic guidance for prompt optimization},
  author={Wu, Y. and Gao, Y. and Zhu, B. B. and Zhou, Z. and others},
  journal={arXiv},
  year={2024}
}
                                                                                       
@article{wu2024visual,
  title={Visual prompting in multimodal {LLMs}: A survey},
  author={Wu, J. and Zhang, Z. and Xia, Y. and Li, X. and others},
  journal={arXiv},
  year={2024}
}

@inproceedings{sclar2024quantifying,
title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
author={Sclar, M. and Choi, Y. and Tsvetkov, Y. and Suhr, A.},
booktitle={ICLR},
year={2024}
}

@inproceedings{menchaca2025mopo,
  title={MOPO: Multi-Objective Prompt Optimization for Affective Text Generation},
  author={Menchaca Resendiz, Y. and Klinger, R.},
  booktitle={COLING},
  year={2025}
}

@inproceedings{ma2024eureka,
title={Eureka: Human-Level Reward Design via Coding {LLMs}},
author={Ma, Y. J. and Liang, W. and Wang, G. and others},
booktitle={ICLR},
year={2024}
}

@inproceedings{akinwande2024understanding,
title={Understanding prompt engineering may not require rethinking generalization},
author={Akinwande, V. and Jiang, Y. and Sam, D. and Kolter, J. Z.},
booktitle={ICLR},
year={2024}
}

@inproceedings{
wan2024efficient,
title={Efficient {LLMs}: A Survey},
author={Wan, Z. and Wang, X. and Liu, C. and Alam, S. and others},
booktitle={TMLR},
year={2024},
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in {LLMs} via reinforcement learning},
  author={Guo, D. and Yang, D. and Zhang, H. and Song, J. and others},
  journal={arXiv},
  year={2025}
}

@article{xi2025rise,
  title={The rise and potential of large language model based agents: A survey},
  author={Xi, Z. and Chen, W. and Guo, X. and He, W. and others},
  journal={Science China Information Sciences},
  year={2025},
  publisher={Springer}
}

@article{wangmixture,
  title={Mixture-of-Experts in Prompt Optimization},
  author={Wang, R. and An, S. and Cheng, M. and Zhou, T. and others},
  journal={arXiv},
  year={2025}
}

@article{agarwal2024promptwizard,
  title={PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework},
  author={Agarwal, E. and Dani, V. and others},
  journal={arXiv},
  year={2024}
}

@inproceedings{margatina2023active,
  title={Active Learning Principles for In-Context Learning with {LLMs}},
  author={Margatina, K. and Schick, T. and Aletras, N. and Dwivedi, J.},
  booktitle={EMNLP},
  year={2023}
}

@article{peng2025soft,
  title={Soft prompt tuning for augmenting dense retrieval with {LLMs}},
  author={Peng, Z. and Wu, X. and Wang, Q. and Fang, Y.},
  journal={Knowledge-Based Systems},
  year={2025}
}

@article{hsieh2023automatic,
  title={Automatic engineering of long prompts},
  author={Hsieh, C.i and Si, S. and Yu, F. and Dhillon, I.},
  journal={arXiv},
  year={2023}
}
@article{cheng2023black,
  title={Black-box prompt optimization: Aligning {LLMs} without model training},
  author={Cheng, J. and Liu, X. and others},
  journal={arXiv},
  year={2023}
}

@inproceedings{zhangautomatic,
  title={Automatic Chain of Thought Prompting in {LLMs}},
  author={Zhang, Z. and Zhang, A. and Li, M. and Smola, A.},
  booktitle={ICLR},
 year={2023}
}

@inproceedings{liu2022makes, title={What Makes Good In-Context Examples for GPT-3?}, author={Liu, J. and Shen, D. and Zhang, Y. and Dolan, W. B. and others}, booktitle={DeeLIO}, year={2022} }

@inproceedings{lu2022fantastically,
  title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  author={Lu, Y. and Bartolo, M. and Moore, A. and Riedel, S. and others},
  booktitle={ACL},
  year={2022}
}

@article{nie2022improving,
  title={Improving few-shot performance of language models via nearest neighbor calibration},
  author={Nie, F. and Chen, M. and Zhang, Z. and Cheng, X.},
  journal={arXiv},
  year={2022}
}

@inproceedings{zhang2022active,
  title={Active Example Selection for In-Context Learning},
  author={Zhang, Y. and Feng, S. and Tan, C.},
  booktitle={EMNLP},
  year={2022}
}

@article{kim2023multiprompter,
  title={MultiPrompter: Cooperative Prompt Optimization with Multi-Agent Reinforcement Learning},
  author={Kim, D. and Sohn, S. and Logeswaran, L. and Shim, D. and others},
  journal={arXiv},
  year={2023}
}
@article{yeh2024tipo,
  title={TIPO: Text to Image with Text Presampling for Prompt Optimization},
  author={Yeh, S. and Park, S. and Oh, G. and others},
  journal={arXiv},
  year={2024}
}

@ARTICLE{10378642,
  author={Lee, S. and Lee, J. and Bae, C. and Choi, M. and others},
  journal={IEEE Access}, 
  title={Optimizing Prompts Using In-Context Few-Shot Learning for Text-to-Image Generative Models}, 
  year={2024},}

@article{mrini2024fast,
  title={Fast Prompt Alignment for Text-to-Image Generation},
  author={Mrini, K. and Lu, H. and Yang, L. and Huang, W. and others},
  journal={arXiv},
  year={2024}
}

@article{hao2024optimizing,
  title={Optimizing prompts for text-to-image generation},
  author={Hao, Y. and Chi, Z. and Dong, L. and Wei, F.},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{wang2024promptcharm,
  title={PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement},
  author={Wang, Z. and Huang, Y. and Song, D. and Ma, L. and Zhang, T.},
  booktitle={CHI},
  year={2024}
}

@article{ogezi2024optimizing,
  title={Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation},
  author={Ogezi, M. and Shi, N.},
  journal={arXiv},
  year={2024}
}

@article{rosenman2023neuroprompts,
  title={Neuroprompts: An adaptive framework to optimize prompts for text-to-image generation},
  author={Rosenman, S. and Lal, V. and Howard, P.},
  journal={arXiv},
  year={2023}
}
@inproceedings{mo2024dynamic,
  title={Dynamic Prompt Optimizing for Text-to-Image Generation},
  author={Mo, W. and Zhang, T. and Bai, Y. and Su, B. and others},
  booktitle={CVPR},
  year={2024}
}
@inproceedings{Yang2023LargeLM,
  title={{LLMs} as Optimizers},
  author={Yang, C. and Wang, X. and Lu, Y. and Liu, H. and others},
  booktitle={ICLR},
  year={2024}
}

@article{zhou2024robust,
  title={Robust prompt optimization for defending language models against jailbreaking attacks},
  author={Zhou, A. and Li, B. and Wang, H.},
  journal={arXiv},
  year={2024}
}
@article{do2024automatic,
  title={Automatic Prompt Selection for {LLMs}},
  author={Do, V. and Hoang, V. and Nguyen, D. and Sabahi, S. and others},
  journal={arXiv},
  year={2024}
}

@inproceedings{prasad2022grips,
  title={Grips: Gradient-free, edit-based instruction search for prompting {LLMs}},
  author={Prasad, A. and Hase, P. and Zhou, X. and Bansal, M.},
  booktitle={ACL},
  year={2023}
}

@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, A. and Tandon, N. and Gupta, P. and others},
  journal={NeurIPS},
  year={2024}
}

@inproceedings{press2023measuring,
  title={Measuring and Narrowing the Compositionality Gap in Language Models},
  author={Press, O. and Zhang, M. and others},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{yaoreact,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, S. and Zhao, J. and Yu, D. and Du, N. and others},
  booktitle={ICLR},
  year={2023}
}
@article{chen2024reprompt,
  title={RePrompt: Planning by Automatic Prompt Engineering for {LLMs} Agents},
  author={Chen, W. and Koenig, S. and others},
  journal={arXiv},
  year={2024}
}

@article{wei2023improving,
  title={Improving Generalization of Image Captioning with Unsupervised Prompt Learning},
  author={Wei, H. and Chen, Z.},
  journal={arXiv},
  year={2023}
}

@article{peng2023kosmos,
  title={Kosmos-2: Grounding multimodal {LLMs} to the world},
  author={Peng, Z. and Wang, W. and others},
  journal={arXiv},
  year={2023}
}

@article{denner2024visual,
  title={Visual prompt engineering for medical vision language models in radiology},
  author={Denner, S. and Bujotzek, M. and Bounias, D. and Zimmerer, D. and others},
  journal={arXiv},
  year={2024}
}

@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, A. and Mintun, E. and Ravi, N. and Mao, H. and others},
  booktitle={ICCV},
  year={2023}
}