\section{Related Work}
\subsection{Graph Neural Architecture Search (GNAS)} 

GraphNAS____ is among the earliest methods employing reinforcement learning to design GNN architectures. Building upon GraphNAS, AutoGNN____ introduces an entropy-driven candidate model sampling method and a weight-sharing strategy to select GNN components more efficiently. GraphNAS++____ accelerates GraphNAS by utilizing distributed architecture evaluation. GM2NAS____ also employs reinforcement learning to design GNNs for multitask multiview graph learning, while MVGNAS____ is tailored for biomedical entity and relation extraction. Additionally, HGNAS____ and HGNAS++____ utilize reinforcement learning to discover heterogeneous graph neural networks.

In contrast to reinforcement learning-based GNAS methods that explore a discrete GNN search space, differentiable gradient-based GNAS methods have emerged, exploring a relaxed, continuous GNN search space. These methods include DSS____, SANE____, GAUSS____, GRACES____, AutoGT____, and Auto-HEG____. SANE focuses on discovering data-specific neighborhood aggregation architectures, while DSS is designed for GNN architectures with a dynamic search space. GAUSS addresses large-scale graphs by devising a lightweight supernet and employing joint architecture-graph sampling for efficient handling. GRACES aims to generalize under distribution shifts by tailoring GNN architectures to each graph instance with an unknown distribution. AutoGT extends GNAS to Graph Transformers, and Auto-HEG enables automated graph neural architecture search for heterophilic graphs. Furthermore, methods such as AutoGEL____, DiffMG____, MR-GNAS____, and DHGAS____ focus on heterogeneous graphs.

Evolutionary algorithms have also been employed in GNAS by methods such as AutoGraph____ and Genetic-GNN____, which aim to identify optimal GNN architectures. G-RNA____ introduces a unique search space and defines a robustness metric to guide the search process for defensive GNNs. Surveys by Zhang et al.____ and Oloulade et al.____ provide comprehensive overviews of automated machine learning methods on graphs.

GNAS-LLM____ and GHGNAS____ explore the application of LLMs to enhance the GNAS search process. Building upon these works, we introduce an LLM-based GNAS toolkit that integrates feature augmentation, graph neural architecture search, and hyperparameter optimization driven by LLMs. This approach allows our toolkit to adapt to complex search spaces and design GNN architectures for new tasks, thereby advancing the flexibility of GNAS methodologies.

\subsection{Large Language Models}

GPT-4 has emerged as an AI model capable of providing responses to inquiries involving multi-modal data____. Studies indicate that GPT-4 exhibits proficiency in comprehending graph data____, demonstrating strong performance across various graph learning tasks. Additionally, research has integrated large language models with graph learning models, utilizing GPT-4 for reasoning over graph data____. In contrast, BERT adopts a pre-training approach on extensive unlabeled data, followed by fine-tuning on specific downstream tasks____. Building upon BERT, several language models have been proposed____. For instance, PaLM is constructed based on the decoder of Transformers____, with PaLM 2 achieving improved results through a larger dataset and a more intricate architecture____.

Recent developments have seen the use of large language models for neural architecture search. GENIUS, for example, leverages GPT-4 to design neural architectures for CNNs, exploring the generative capacity of LLMs____. The fundamental concept involves enabling GPT-4 to learn from feedback on generated neural architectures, thereby iteratively improving the design. Experimental results on various benchmarks demonstrate GPT-4's ability to discover top-ranked architectures after several prompt iterations. AutoML-GPT introduces a series of prompts for LLMs to autonomously undertake tasks such as data processing, model architecture design, and hyperparameter tuning____. Notably, GPT-4 has been introduced into graph neural architecture search by GNAS-LLM____ and GHGNAS____, aiming to search for the optimal GNN or heterogeneous GNN within a designated search space.

In this paper, we present a toolkit that extends the application of LLMs to the generation of new GNN architectures. The toolkit is designed to be easily extendable to new search spaces and graph tasks, offering a user-friendly interface for practical applications and academic research. Our aim is to enhance the performance of existing GNAS methods by providing a versatile and efficient solution for GNN design.