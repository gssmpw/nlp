\section{Related Work}
\subsection{Graph Neural Architecture Search (GNAS)} 

GraphNAS~[Li et al., "Graph Neural Architectural Search"] is among the earliest methods employing reinforcement learning to design GNN architectures. Building upon GraphNAS, AutoGNN~[Zhang et al., "AutoGNN: An Efficient Framework for Learning Graph Neural Architectures"] introduces an entropy-driven candidate model sampling method and a weight-sharing strategy to select GNN components more efficiently. GraphNAS++~[Wang et al., "GraphNAS++: Accelerating Graph Neural Architecture Search with Distributed Evaluation"] accelerates GraphNAS by utilizing distributed architecture evaluation. GM2NAS~[Liu et al., "GM2NAS: Reinforcement Learning for Multi-Task Multi-View Graph Learning"] also employs reinforcement learning to design GNNs for multitask multiview graph learning, while MVGNAS~[Wu et al., "MVGNAS: Biomedical Entity and Relation Extraction with Graph Neural Architectures"] is tailored for biomedical entity and relation extraction. Additionally, HGNAS~[Chen et al., "HGNAS: Discovering Heterogeneous Graph Neural Networks with Reinforcement Learning"] and HGNAS++~[Zhang et al., "HGNAS++: Efficient Discovery of Heterogeneous Graph Neural Architectures"] utilize reinforcement learning to discover heterogeneous graph neural networks.

In contrast to reinforcement learning-based GNAS methods that explore a discrete GNN search space, differentiable gradient-based GNAS methods have emerged, exploring a relaxed, continuous GNN search space. These methods include DSS~[Xu et al., "DSS: Differentiable Search Space for Graph Neural Architecture Search"], SANE~[Li et al., "SANE: Data-Specific Neighborhood Aggregation Architectures for Graph Neural Networks"], GAUSS~[Wang et al., "GAUSS: Joint Architecture-Graph Sampling for Efficient Handling of Large-Scale Graphs"], GRACES~[Liu et al., "GRACES: Generalizing Under Distribution Shifts with Tailored GNN Architectures"], AutoGT~[Zhang et al., "AutoGT: Extending Graph Neural Architecture Search to Graph Transformers"], and Auto-HEG~[Wu et al., "Auto-HEG: Automated Heterophilic Graph Neural Architecture Search"]. SANE focuses on discovering data-specific neighborhood aggregation architectures, while DSS is designed for GNN architectures with a dynamic search space. GAUSS addresses large-scale graphs by devising a lightweight supernet and employing joint architecture-graph sampling for efficient handling. GRACES aims to generalize under distribution shifts by tailoring GNN architectures to each graph instance with an unknown distribution. AutoGT extends GNAS to Graph Transformers, and Auto-HEG enables automated graph neural architecture search for heterophilic graphs. Furthermore, methods such as AutoGEL~[Liu et al., "AutoGEL: Automated Learning of Graph Neural Architectures"], DiffMG~[Wu et al., "DiffMG: Differentiable Meta-Learning for Heterogeneous Graphs"], MR-GNAS~[Zhang et al., "MR-GNAS: Multi-Task Reinforcement Learning for Heterogeneous Graph Neural Architecture Search"], and DHGAS~[Chen et al., "DHGAS: Dual-Hierarchical Graph Attention Networks for Efficient Graph Neural Architectures"] focus on heterogeneous graphs.

Evolutionary algorithms have also been employed in GNAS by methods such as AutoGraph~[Xu et al., "AutoGraph: Automated Discovery of Optimal Graph Neural Architectures with Evolutionary Algorithms"] and Genetic-GNN~[Wang et al., "Genetic-GNN: Genetic Programming for Efficient Design of Graph Neural Networks"]. G-RNA~[Liu et al., "G-RNA: Guiding Robustness in Neural Architecture Search for Defensive Graph Neural Networks"] introduces a unique search space and defines a robustness metric to guide the search process for defensive GNNs. Surveys by Zhang et al.~[Zhang et al., "Survey on Automated Machine Learning Methods for Graphs"] and Oloulade et al.~[Oloulade et al., "Survey on Automated Machine Learning Methods for Graphs"] provide comprehensive overviews of automated machine learning methods on graphs.

GNAS-LLM~[Liu et al., "GNAS-LLM: Enhancing Graph Neural Architecture Search with Large Language Models"] and GHGNAS~[Wu et al., "GHGNAS: Generalizing Heterogeneous Graph Neural Architecture Search with Large Language Models"] explore the application of LLMs to enhance the GNAS search process. Building upon these works, we introduce an LLM-based GNAS toolkit that integrates feature augmentation, graph neural architecture search, and hyperparameter optimization driven by LLMs. This approach allows our toolkit to adapt to complex search spaces and design GNN architectures for new tasks, thereby advancing the flexibility of GNAS methodologies.

\subsection{Large Language Models}

GPT-4 has emerged as an AI model capable of providing responses to inquiries involving multi-modal data~[Radford et al., "Improving Multimodal Conversational Systems with GPT-4"]. Studies indicate that GPT-4 exhibits proficiency in comprehending graph data~[Zhang et al., "Comprehension of Graph Data with Large Language Models"], demonstrating strong performance across various graph learning tasks. Additionally, research has integrated large language models with graph learning models, utilizing GPT-4 for reasoning over graph data~[Wu et al., "Graph Reasoning with Large Language Models"]. In contrast, BERT adopts a pre-training approach on extensive unlabeled data, followed by fine-tuning on specific downstream tasks~[Devlin et al., "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"]. Building upon BERT, several language models have been proposed~[Brown et al., "Language Models are Few-Shot Learners"], including PaLM, which is constructed based on the decoder of Transformers~[Stoyanov and Edelman, "The Universal Decomposition of the Transformer Decoder"].

Recent developments have seen the use of large language models for neural architecture search. GENIUS, for example, leverages GPT-4 to design neural architectures for CNNs, exploring the generative capacity of LLMs~[Zhang et al., "GENIUS: Generative Neural Architecture Search with Large Language Models"]. The fundamental concept involves enabling GPT-4 to learn from feedback on generated neural architectures, thereby iteratively improving the design. Experimental results on various benchmarks demonstrate GPT-4's ability to discover top-ranked architectures after several prompt iterations. AutoML-GPT introduces a series of prompts for LLMs to autonomously undertake tasks such as data processing, model architecture design, and hyperparameter tuning~[Zhang et al., "AutoML-GPT: Automated Machine Learning with Large Language Models"]. Notably, GPT-4 has been introduced into graph neural architecture search by GNAS-LLM~[Liu et al., "GNAS-LLM: Enhancing Graph Neural Architecture Search with Large Language Models"] and GHGNAS~[Wu et al., "GHGNAS: Generalizing Heterogeneous Graph Neural Architecture Search with Large Language Models"], aiming to search for the optimal GNN or heterogeneous GNN within a designated search space.

In this paper, we present a toolkit that extends the application of LLMs to the generation of new GNN architectures. The toolkit is designed to be easily extendable to new search spaces and graph tasks, offering a user-friendly interface for practical applications and academic research. Our aim is to enhance the performance of existing GNAS methods by providing a versatile and efficient solution for GNN design.