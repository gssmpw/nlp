\section{Related Work}
\subsection{Graph Neural Architecture Search (GNAS)} 

GraphNAS~\cite{DBLP:conf/ijcai/GaoYZ0H20} is among the earliest methods employing reinforcement learning to design GNN architectures. Building upon GraphNAS, AutoGNN~\cite{AutoGNN} introduces an entropy-driven candidate model sampling method and a weight-sharing strategy to select GNN components more efficiently. GraphNAS++~\cite{gao2022graphnas++} accelerates GraphNAS by utilizing distributed architecture evaluation. GM2NAS~\cite{gao2023gm2nas} also employs reinforcement learning to design GNNs for multitask multiview graph learning, while MVGNAS~\cite{al2022multi} is tailored for biomedical entity and relation extraction. Additionally, HGNAS~\cite{DBLP:conf/icdm/GaoZLZLH21} and HGNAS++~\cite{gao2023hgnas++} utilize reinforcement learning to discover heterogeneous graph neural networks.

In contrast to reinforcement learning-based GNAS methods that explore a discrete GNN search space, differentiable gradient-based GNAS methods have emerged, exploring a relaxed, continuous GNN search space. These methods include DSS~\cite{li2021one}, SANE~\cite{DBLP:conf/icde/ZhaoYT21}, GAUSS~\cite{guan2022large}, GRACES~\cite{qin2022graph}, AutoGT~\cite{zhang2022autogt}, and Auto-HEG~\cite{zheng2023auto}. SANE focuses on discovering data-specific neighborhood aggregation architectures, while DSS is designed for GNN architectures with a dynamic search space. GAUSS addresses large-scale graphs by devising a lightweight supernet and employing joint architecture-graph sampling for efficient handling. GRACES aims to generalize under distribution shifts by tailoring GNN architectures to each graph instance with an unknown distribution. AutoGT extends GNAS to Graph Transformers, and Auto-HEG enables automated graph neural architecture search for heterophilic graphs. Furthermore, methods such as AutoGEL~\cite{wang2021autogel}, DiffMG~\cite{diffmg}, MR-GNAS~\cite{Zheng2022MultiRelationalGN}, and DHGAS~\cite{Zhang_Zhang_Wang_Qin_Qin_Zhu_2023} focus on heterogeneous graphs.

Evolutionary algorithms have also been employed in GNAS by methods such as AutoGraph~\cite{li2020autograph} and Genetic-GNN~\cite{Shi2020EvolutionaryAS}, which aim to identify optimal GNN architectures. G-RNA~\cite{DBLP:conf/cvpr/XieCZWWZY023} introduces a unique search space and defines a robustness metric to guide the search process for defensive GNNs. Surveys by Zhang et al.~\cite{DBLP:conf/ijcai/ZhangW021} and Oloulade et al.~\cite{oloulade2021graph} provide comprehensive overviews of automated machine learning methods on graphs.

GNAS-LLM~\cite{wang2023graph} and GHGNAS~\cite{dong2023heterogeneous} explore the application of LLMs to enhance the GNAS search process. Building upon these works, we introduce an LLM-based GNAS toolkit that integrates feature augmentation, graph neural architecture search, and hyperparameter optimization driven by LLMs. This approach allows our toolkit to adapt to complex search spaces and design GNN architectures for new tasks, thereby advancing the flexibility of GNAS methodologies.

\subsection{Large Language Models}

GPT-4 has emerged as an AI model capable of providing responses to inquiries involving multi-modal data~\cite{openai2023gpt4}. Studies indicate that GPT-4 exhibits proficiency in comprehending graph data~\cite{guo2023gpt4graph}, demonstrating strong performance across various graph learning tasks. Additionally, research has integrated large language models with graph learning models, utilizing GPT-4 for reasoning over graph data~\cite{zhanggraph}. In contrast, BERT adopts a pre-training approach on extensive unlabeled data, followed by fine-tuning on specific downstream tasks~\cite{DBLP:conf/naacl/DevlinCLT19}. Building upon BERT, several language models have been proposed~\cite{DBLP:journals/corr/abs-1907-11692,DBLP:conf/iclr/LanCGGSS20,DBLP:conf/iclr/HeLGC21}. For instance, PaLM is constructed based on the decoder of Transformers~\cite{Vaswani2017AttentionIA}, with PaLM 2 achieving improved results through a larger dataset and a more intricate architecture~\cite{chowdhery2022palm,anil2023palm}.

Recent developments have seen the use of large language models for neural architecture search. GENIUS, for example, leverages GPT-4 to design neural architectures for CNNs, exploring the generative capacity of LLMs~\cite{zheng2023can}. The fundamental concept involves enabling GPT-4 to learn from feedback on generated neural architectures, thereby iteratively improving the design. Experimental results on various benchmarks demonstrate GPT-4's ability to discover top-ranked architectures after several prompt iterations. AutoML-GPT introduces a series of prompts for LLMs to autonomously undertake tasks such as data processing, model architecture design, and hyperparameter tuning~\cite{zhang2023automl}. Notably, GPT-4 has been introduced into graph neural architecture search by GNAS-LLM~\cite{wang2023graph} and GHGNAS~\cite{dong2023heterogeneous}, aiming to search for the optimal GNN or heterogeneous GNN within a designated search space.

In this paper, we present a toolkit that extends the application of LLMs to the generation of new GNN architectures. The toolkit is designed to be easily extendable to new search spaces and graph tasks, offering a user-friendly interface for practical applications and academic research. Our aim is to enhance the performance of existing GNAS methods by providing a versatile and efficient solution for GNN design.