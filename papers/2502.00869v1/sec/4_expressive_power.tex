% \vspace{-0.75em}
\section{Expressive Power}
% \vspace{-0.5em}
In this part, we examine the expressive power of our architecture, drawing upon the notable Theorem 1 from \citep{yuce2022structured}. This theorem is as follows:
\begin{theorem} \label{expressive_power}
    (Theorem 1 of \citep{yuce2022structured}) Let \(f_{\boldsymbol{\theta}}: \mathbb{R}^D \rightarrow \mathbb{R}\) be an INR of the form of Equation \eqref{Network} with $\rho^{(l)}(x) = \sum_{j=0}^J \alpha_jx^j$ for $l > 1$. Furthermore, let $\boldsymbol{\Psi} = [\boldsymbol{\Psi}_1, ..., \boldsymbol{\Psi}_T]^{tr} \in \mathbb{R}^{T\times D}$ and $\boldsymbol{\zeta}\in\mathbb{R}^T$ denote the matrix of frequencies and vector of phases, respectively, used to map the input coordinate \(r \in \mathbb{R}^D\) to $\gamma(r) = \sin(\boldsymbol{\Psi} \boldsymbol{r} + \boldsymbol{\zeta})$. This architecture can only represent functions of the form
    \[ 
    f_{\boldsymbol{\theta}}(r) = \sum_{\boldsymbol{w^\prime}\in \mathcal{H}(\Psi)} c_{w^\prime } \sin(\langle \boldsymbol{w^\prime}, \boldsymbol{r}\rangle +	\zeta_{\boldsymbol{w^\prime}}),
    \]
    where
    \[ \mathcal{H}(\boldsymbol{\Psi}) \subseteq  \Tilde{\mathcal{H}}(\boldsymbol{\Psi})=\Bigg\{\sum_{t=1}^{T}s_t\boldsymbol{\Psi}_t \Bigg|~s_t \in \mathbb{Z}\wedge \sum_{t=1}^{T} |s_t|\leq J^{L-1}\Bigg\}.
    \]
\end{theorem}
Please note the following remarks regarding this theorem:

\begin{remark}
We refer to $\Tilde{\mathcal{H}}$ as the set of potential frequencies.
\end{remark}

\begin{remark}
The expression $\sum_{t=1}^{T}s_t\boldsymbol{\Psi}_t$ is equal to $\boldsymbol{\Psi}^{tr}[s_1,...,s_{T}]^{tr}$. This representation is more convenient for our subsequent discussion, as we will be exploring the kernel of $\boldsymbol{\Psi}$ in the sequel.
\end{remark}

\begin{remark}
In the context of SIREN, where $\rho^{(l)}=\sin$, the post-activation function of the first layer, $z^{(0)}=\sin(\omega_0(\boldsymbol{W}^{(0)}\boldsymbol{r}+\boldsymbol{b}^{(0)}))$, can be interpreted as $\gamma(\boldsymbol{r})=\sin(\boldsymbol{\Psi r}+\boldsymbol{\zeta})$.
\end{remark}

We will now investigate the significant enhancement in expressive power offered by the proposed activation function. To facilitate comparison with SIREN, we express our network using $\sin$ as the activation function.

Let us consider a neural network with a parametric activation function defined in \eqref{STAF}. To represent our network using SIREN, we demonstrate that every post-activation function of our network from the second layer onwards ($z^{l+1}$) can be expressed using linear transformations and sine functions. Notably, the final post-activation function ($z^{(L-1)}$) can be constructed using SIREN, albeit requiring more neurons than STAF. In other words, our network can be described using a SIREN and some Kronecker products denoted by $\otimes$. This analysis resembles that provided in \citep{jagtap2022deep}, with a slight difference in the settings of the paper. In \citep{jagtap2022deep}, it was shown that an adaptive activation function of the form
\begin{equation}
\rho^*(x) = \sum_{i=1}^{\tau}C_i \rho_{i}(\Omega_ix)
\end{equation}
can be represented using a feed-forward neural network, where each layer has neurons with activation functions $\rho_{i}$. To align STAF with this theorem, we must have $\rho_i=\sin(\Omega_ix+\Phi_i)$. However, here we aim to represent STAF using an architecture that only employs sine activation functions (SIREN). For this purpose, we introduce the following theorem, which holds true for every parametric activation function:
% \begin{theorem} \label{first_layer_equivalent}
%     Suppose we have a neural network defined in \eqref{Network}. Let $\boldsymbol{\Omega}=[\Omega_1,...,\Omega_\tau]^{tr}$, $\boldsymbol{\Phi}=[\Phi_1,...,\Phi_\tau]^{tr}$, and $\boldsymbol{C}=[C_1,\ldots,C_\tau]^{tr}$. Let $L\geq 2$ and $1\leq l \leq L-2$. Assume that the parametric activation is $\overline{\rho}(x)=\sum_{m=1}^{\tau}\boldsymbol{C}_m\rho(\boldsymbol{\Omega}_mx+\boldsymbol{\Phi}_m)$. Then we can construct an equivalent neural network with the activation function $\rho(x)$, in the following way (parameters of the equivalent network are shown using overlince):
%     \begin{align}
%         \overline{\boldsymbol{z}^{(0)}} &= \gamma(\boldsymbol{r}), \nonumber\\
%         \overline{\boldsymbol{z}^{(l)}} &= \rho\left(\overline{\boldsymbol{W}^{(l)}}~\overline{\boldsymbol{z}^{(l+1)}}+\overline{\boldsymbol{B}^{(l)}}\right), \quad l= 1,..., L- 1, \\
%         f_{\theta}(\boldsymbol{r}) &= \overline{\boldsymbol{W}^{(L)}}~\overline{\boldsymbol{z}^{(L-1)}}, \nonumber
%     \end{align}
%     in which,
%     \begin{equation}
%         \overline{\boldsymbol{W}^{(l)}} = \begin{cases}
% 			\boldsymbol{\Omega}\otimes\boldsymbol{W}^{(l)}, & \text{if $l=0$}\\
%             \left(\boldsymbol{\Omega}\otimes\boldsymbol{C}^{tr}\right)\otimes\boldsymbol{W}^{(l)}, & \text{if $l$ is odd}\\

%             \left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I_{F_l}}\right)\left(\boldsymbol{\Omega}\otimes\boldsymbol{W}^{(l)}\right), & \text{if $l$ is even, $l\geq 2$, and $l\neq L$}\\

%             \boldsymbol{C}^{tr}\otimes\boldsymbol{I_{F_l}},& \text{if $l$ is even, and $l\neq L$}
% 		 \end{cases},
%     \end{equation}
%     and
%     \begin{equation}
%         \overline{\boldsymbol{B}^{(l)}}=\begin{cases}
% 			\boldsymbol{\Phi}\otimes \boldsymbol{J}_{F_l}, & \text{if $l\neq L$} \\
%             0, & \text{if $l=L$}
% 		 \end{cases}.
%     \end{equation}
%     Moreover, \textbf{ for even values of $l$}, we have $\overline{\boldsymbol{z}^{(l)}}=\boldsymbol{z}^{(l)}$.
% \end{theorem}
\begin{theorem} \label{Kronecker_theorem}
Let $L\geq 2$ and $1\leq l \leq L$. Consider a neural network as defined in \eqref{Network} with $L$ layers. In addition, let $\boldsymbol{\Omega}=[\Omega_1,...,\Omega_\tau]^{tr}$, $\boldsymbol{\Phi}=[\Phi_1,...,\Phi_\tau]^{tr}$, and $\boldsymbol{C}=[C_1,\ldots,C_\tau]^{tr}$. If the trainable activation function is $\rho^*(x)=\sum_{m=1}^{\tau}\boldsymbol{C}_m\rho(\boldsymbol{\Omega}_mx+\boldsymbol{\Phi}_m)$, then an equivalent neural network with activation function $\rho(x)$ and $L+1$ layers can be constructed as follows (parameters of the equivalent network are denoted with an overline):
    \begin{align}
        \overline{\boldsymbol{z}^{(0)}} &= \gamma(\boldsymbol{r}), \nonumber\\
        \overline{\boldsymbol{z}^{(l)}} &= \rho\left(\overline{\boldsymbol{W}^{(l)}}~\overline{\boldsymbol{z}^{(l-1)}}+\overline{\boldsymbol{B}^{(l)}}\right), \quad l= 1,..., L, \\
        \overline{f}_{\overline{\theta}}(\boldsymbol{r}) &= \overline{\boldsymbol{W}^{(L+1)}}~\overline{\boldsymbol{z}^{(L)}}; \nonumber
    \end{align}
where
    \begin{equation} \label{weights_and_biases}
    \scalebox{0.78}{$
    \overline{\boldsymbol{W}^{(l)}} =
    \begin{cases}
        \boldsymbol{\Omega} \otimes \boldsymbol{W}^{(l)}, & \text{if $l = 1$}, \\[4pt]
        \left( \boldsymbol{\Omega} \otimes \boldsymbol{C}^{tr} \right) \otimes \boldsymbol{W}^{(l)}, & \text{if $l$ is even}, \\[4pt]
        \left( \boldsymbol{\Omega} \otimes \boldsymbol{W}^{(l)} \right) \left( \boldsymbol{C}^{tr} \otimes \boldsymbol{I}_{F_{l-1}} \right), & \text{if $l$ is odd, $l > 1$, and $l \neq L+1$}, \\[4pt]
        \boldsymbol{C}^{tr} \otimes \boldsymbol{I}_{F_{l-1}}, & \text{if $l$ is odd, $l > 1$, and $l = L+1$}.
    \end{cases}$}
\end{equation}
and
\begin{equation}
    \overline{\boldsymbol{B}^{(l)}} = \boldsymbol{\Phi} \otimes \boldsymbol{J}_{F_l}.
\end{equation}



in which $\boldsymbol{J}_{F_l}$ is an all-ones $F_l\times 1$ vector. Furthermore, if $L$ is even, then $\overline{f}_{\overline{\theta}}(\boldsymbol{r})=f_\theta(\boldsymbol{r})$ (we call these networks \lq Kronecker equivalent' in this sense).
\end{theorem}
The proof of this theorem is provided in the Appendix~\ref{app:proof_Kronecker_theorem}. As we observed, although a network with the activation function $\rho^*$ can be represented using the activation function $\rho$, it features a unique architecture. These networks are not merely typical MLPs with the activation function $\rho$, as the weights in the Kronecker equivalent network exhibit dependencies due to the Kronecker product.

It is desirable that Theorem \eqref{Kronecker_theorem} does not depend on the parity of $L$. To achieve this, consider the following remark:
\begin{remark} \label{dummy_layer}
     We can introduce a dummy layer with the activation function $\rho^*$. Specifically, we define $\boldsymbol{z}^{(L)}=\rho^*\left(f_\theta(\boldsymbol{r})\right)$, and $\Tilde{f_\theta}(\boldsymbol{r}) = \boldsymbol{W}^{(L+1)}\boldsymbol{z}^{(L)}+\boldsymbol{B}^{(L+1)}$, where $\boldsymbol{W}^{(L+1)}=\boldsymbol{O}$. To ensure that $\Tilde{f_\theta}(\boldsymbol{r}) =f_\theta(\boldsymbol{r})$, we set $\boldsymbol{B}^{(L+1)}=f_\theta(\boldsymbol{r})$. This approach allows us to construct an equivalent neural network with one more layer.
\end{remark}
As a result of Remark \eqref{dummy_layer}, the equivalent network of a network with a trainable activation function, has either one more layer, or the same number of layers.
% \begin{theorem} \label{first_layer_equivalent}
%     Suppose we have a neural network defined in \eqref{Network} with parametric activation function defined in \eqref{ParAc}. Then we can construct an equivalent neural network with the activation function $\sin(x)$. Furthermore, the first layer of the equivalent network will have the following weight matrix:
%     \begin{align}
%         \overline{\boldsymbol{z}^{(0)}} &= \gamma(\boldsymbol{r}), \nonumber\\
%         \overline{\boldsymbol{z}^{(l)}} &= \rho\left(\overline{\boldsymbol{W}^{(l)}}~\overline{\boldsymbol{z}^{(l+1)}}+\overline{\boldsymbol{B}^{(l)}}\right), \quad l= 1,..., L- 1, \\
%         f_{\theta}(\boldsymbol{r}) &= \overline{\boldsymbol{W}^{(L)}}~\overline{\boldsymbol{z}^{(L-1)}}. \nonumber
%     \end{align}
%     so
%         \begin{equation}
%         \boldsymbol{\overline{W^{(l)}}}=\boldsymbol{\Omega}\otimes \boldsymbol{W^{(l)}}
%         \end{equation}
% \end{theorem}
As an immediate result of Theorem \eqref{Kronecker_theorem}, if we denote the embedding of the first layer of the SIREN equivalent of our network by $\boldsymbol{\overline{\Psi}}$, then
\begin{equation}
    \boldsymbol{\overline{\Psi}}=\overline{\boldsymbol{W}^{(1)}}=\boldsymbol{\Omega}\otimes\boldsymbol{W}^{(1)}\in\mathbb{R}^{\tau F_1\times F_0}
\end{equation}
which is $\tau$ times bigger than the embedding of the first layer of a SIREN with $\boldsymbol{W}^{(1)}\in \mathbb{R}^{F_1\times F_0}$. To understand the impact of this increase on expressive power, it suffices to substitute $T$ with $\tau T$ in Theorem \eqref{expressive_power}.  The next theorem will reveal how this change will affect the cardinality of the set of potential frequencies.

\begin{theorem} \label{asymptotic_behavior}
    (Page 4 of \citep{kiselman2012asymptotic}) Let $V(T,K)=\big\{(s_1,s_2,\ldots,s_{T}) \in \mathbb{Z}^T \big|~ \sum_{t=1}^{T} |s_t|\leq K\big\}$.\footnote{~We use \( V \) to denote these points as cells in a \( T \)-dimensional von Neumann neighborhood of \( K \), clarifying that \( V \) does not represent a vector space.} Then we have
    \begin{equation}
        |V(T,K)|=\sum_{i=0}^{min(K,T)}\binom{i}{K}\binom{i}{T}2^i
    \end{equation}
    This number is called Delannoy number. Moreover, for fixed $K$,
    \begin{equation}
        |V(T,K)|\sim A_K(2T)^K,\quad T\rightarrow +\infty
    \end{equation}
\end{theorem}

As an immediate result of this theorem, for large values of $T$,
\begin{equation}
    \frac{|V(\tau T,K)|}{|V(T,K)|} \sim \tau^K
\end{equation}

Now, it is time to analyze the cardinality of the set of potential frequencies:
\begin{equation}
	\Tilde{\mathcal{H}}(\boldsymbol{\Psi})=\Bigg\{\sum_{t=1}^{T}s_t\boldsymbol{\Psi}_t \Bigg|~(s_1,s_2,\ldots,s_T) \in V(T,J^{L-1})\Bigg\}
\end{equation}
or equivalently,
\begin{equation}
	\Tilde{\mathcal{H}}(\boldsymbol{\Psi})=\Bigg\{\boldsymbol{\Psi}^{tr}[s_1,...,s_T]^{tr} \Bigg|~s_t \in \mathbb{Z}\wedge \sum_{t=1}^{T} |s_t|\leq J^{L-1}\Bigg\}
\end{equation}
%Note that $|\Tilde{\mathcal{H}}(\boldsymbol{\Psi})|\leq V(T,J^{L-1})$ and if the map $\boldsymbol{\Psi}^{tr}$ is injective on $\mathbb{Z}^T$, then $|\Tilde{\mathcal{H}}(\boldsymbol{\Psi})|= V(T,J^{L-1})$. To avoid the complications of the number of lattice points of the image of a convex body under a linear transformation (that is a topic of the geometry of numbers), and also to prevent diminishing the size of $\Tilde{\mathcal{H}}(\boldsymbol{\Psi})$, it is worth noting that we can always modify the matrix $\boldsymbol{\Psi}^{tr}$ by small perturbations so that the kernel of that has no points with rational coordinates except zero, let alone integer lattice points.

The cardinality of the set $\Tilde{\mathcal{H}}(\boldsymbol{\Psi})$ is bounded above by $V(T,J^{L-1})$. If $\boldsymbol{\Psi}^{tr}$, is injective on the integer lattice $\mathbb{Z}^T$, then $|\Tilde{\mathcal{H}}(\boldsymbol{\Psi})| = |V(T,J^{L-1})|$. However, in general, analyzing how a linear transformation affects the size of a convex body can be approached using the geometry of numbers \citep{matousek2013lectures} or additive geometry \citep{tao2006additive}. To simplify the analysis and preserve the size of $\Tilde{\mathcal{H}}(\boldsymbol{\Psi})$ as large as possible, we can slightly perturb the matrix $\boldsymbol{\Psi}^{tr}$ such that its kernel contains no points with rational coordinates, except the origin. This is a much stronger condition than having no integer lattice points in the kernel. To address this, we introduce a lemma. It's worth noting that we can assume the matrices are stored with rational entries, as they are typically represented in computers using floating-point numbers. In our subsequent analysis, however, assuming rational entries for just one column of the matrix $\boldsymbol{\Psi}$ is sufficient.
\begin{lemma} \label{rational_elusive}
    Let $\boldsymbol{A}\in\mathbb{R}^{D\times T}$, and for one of its rows, like $r$'th row, we have $\boldsymbol{A}_r\in\mathbb{Q}^T$. Then, in every neighborhood of $\boldsymbol{A}$, there is a matrix $\boldsymbol{\hat{A}}$ such that $Ker(\boldsymbol{\hat{A}})\cap \mathbb{Q}^{T}=\boldsymbol{O}$.
\end{lemma}
(The proof is provided in the Appendix~\ref{app:proof_rational_elusive}.) Consider Lemma \eqref{rational_elusive}, where we let $\boldsymbol{A}=\boldsymbol{\Psi}^{tr}$. Thus, for every neighborhood of $\boldsymbol{\Psi}^{tr}$, there exists a matrix $\boldsymbol{\hat{\Psi}^{tr}}$ such that $Ker(\boldsymbol{\hat{\Psi}^{tr}})\cap \mathbb{Q}^{T}=\boldsymbol{O}$; in other words, $\boldsymbol{\hat{\Psi}^{tr}}$ is injective over rational points, and consequently over integer lattice points. This guarantees that $|\Tilde{\mathcal{H}}(\boldsymbol{\hat{\Psi}})| = |V(T,J^{L-1})|$.

In summary, this section demonstrated that, in comparison to SIREN, STAF can substantially increase the size of the set of potential frequencies by a factor of $\tau^K$. This underscores how leveraging the properties of the Kronecker product enables the proposed activation function to significantly enhance expressive power.