% \vspace{-1em}
\section{STAF: Sinusoidal Trainable Activation Function}
% \vspace{-0.5em}
% STAF, a novel family of parametric, trainable activation functions, is designed to effectively overcome the limitations of traditional neural network architectures, particularly in the context of high-dimensional data processing and reconstruction of complex signals with exceptional precision. These networks are engineered to address the spectral bias inherent in ReLU networks, enabling the accurate reconstruction of fine details in target signals. 

\subsection{INR Problem Formulation}
INRs employ MLPs as a method for representing continuous data. At the core of INR is the function $ f_{\boldsymbol{\theta}}: \mathbb{R}^{F_0} \rightarrow \mathbb{R}^{F_L} $, where $ F_0 $ and $ F_L $ represent the dimensions of the input and output spaces, respectively, and $ \boldsymbol{\theta} $ denotes the parameters of the MLP. The objective is to approximate a target function $ g(\boldsymbol{x}) $ such that $ g(\boldsymbol{x}) \approx f_{\boldsymbol{\theta}}(\boldsymbol{x}) $. For example, in image processing, $ g(\boldsymbol{x}) $ could be a function mapping pixel coordinates to their respective values.

As mentioned in \citep{yuce2022structured}, the majority of INR architectures can be decomposed into a mapping function $\gamma:\mathbb{R}^D \rightarrow \mathbb{R}^T$ followed by an MLP, with weights $\boldsymbol{W^{(l)}} \in \mathbb{R}^{F_l\times F_{l-1}}$ and activation function  $\rho^{(l)}: \mathbb{R}\rightarrow \mathbb{R}$, applied element-wise at each layer $l = 1,\ldots,L-1$. In other words, if we represent $\boldsymbol{z^{(l)}}$ as the post-activation output of each layer, most INR architectures compute
\begin{align} \label{Network}
\boldsymbol{z^{(0)}} &= \gamma(\boldsymbol{r}), \nonumber\\
\boldsymbol{z^{(l)}} &= \rho^{(l)}(\boldsymbol{W^{(l)}}\boldsymbol{z^{(l-1)}}+\boldsymbol{B^{(l)}}), \quad l= 1,..., L- 1, \\
f_{\boldsymbol{\theta}}(\boldsymbol{r}) &= \boldsymbol{W^{(L)}}\boldsymbol{z^{(L-1)}}+\boldsymbol{B^{(L)}}. \nonumber
\end{align} 
Additionally, corresponding to the $i$'th neuron of the $l$'th layer, we employ the symbols $a^{(l)}_i$ and $z^{(l)}_i$ for the pre-activation and post-activation functions respectively. The choice of the activation function $ \rho $ is pivotal in INR, as it influences the network's ability to represent signals. Traditional functions, such as ReLU, may not effectively capture high-frequency components. The novel parametric periodic activation function, i.e., STAF, enhances the network's capability to accurately model and reconstruct complex, high-frequency signals.

% \vspace{-0.75em}
\subsection{STAF Activation Function}
% \vspace{-0.5em}
The activation function STAF introduces a conceptually different approach compared to conventional activation functions (see~\cref{fig:activation}). It is parameterized in the form of a Fourier series: 
\begin{equation} \label{STAF}
\rho^*(x) = \sum_{i=1}^\tau C_i \sin(\Omega_i x + \Phi_i),
\end{equation}
where \(C_i\), \(\Omega_i\), and \(\Phi_i\) represent the \textit{amplitude}, \textit{frequency}, and \textit{phase} parameters, respectively. These parameters are learned dynamically during training, enabling the network to adapt its activation function to the specific characteristics of the signal being modeled. The use of a Fourier series is motivated by its ability to represent signals efficiently, capturing essential components with a small number of coefficients. This adaptability allows STAF to provide a compact and flexible representation for complex patterns in various tasks. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{data/layers_features.pdf}
  % \vspace{-0.5em}
  \caption{Activation maps for STAF, SIREN, and WIRE learned during the image reconstruction task.}
  % \vspace{-0.75em}
  \label{fig:map}
\end{figure*}
% \vspace{-0.75em}
\subsection{STAF Training Process}
% \vspace{-0.5em}
During training, STAF optimizes not only the traditional MLP parameters (weights and biases), but also the coefficients of the activation function. This dual optimization approach ensures that the network learns both an optimal set of transformations (through weights and biases) and an ideal way of activating neurons (through the parametric activation function) for each specific task. The training employs a reconstruction loss function designed to minimize the difference between the target function $g(\boldsymbol{x})$ and the network's approximation $f_{\boldsymbol{\theta}}(\boldsymbol{x})$, while also encouraging efficient representation inspired by Fourier series.
% \vspace{-0.75em}
\subsection{Implementation Strategies} 
% \vspace{-0.5em}
\label{sec:implement_str}
The implementation of STAF's parametric activation functions can be approached in three ways:

\ding{202} \textbf{Individual Neuron Activation:} This method assigns a unique activation function to each neuron. It offers high expressiveness but leads to a significant increase in the number of trainable parameters, making it impractical for large networks due to potential overfitting and computational inefficiencies.

\ding{203} \textbf{Uniform Network-wide Activation:} Here, a single shared activation function is used across the entire network. This approach simplifies the model by reducing the number of additional parameters but limits the network's expressiveness and adaptability. It may struggle to capture diverse patterns and details in complex signals.

\ding{204} \textbf{Layer-wise Shared Activation:} This balanced strategy employs a distinct shared activation function for each layer which is also used for all experiments in this paper. For example, in a 3-layer MLP with $\tau=25$ terms, only 225 additional parameters are required. This approach optimally balances expressiveness and efficiency, allowing each layer to develop specialized activation dynamics for the features it processes. It aligns with the hierarchical nature of MLPs, where different layers capture different signal abstractions, providing an efficient learning mechanism tailored to each layer's role.

% \tocless\subsection{Addressing Spectral Bias}
% The spectral bias of neural networks refers to their tendency to preferentially learn low-frequency components of a signal. This bias often leads to the underrepresentation of high-frequency details, which are crucial in tasks such as image processing and audio signal reconstruction. STAFs, through their Fourier series-based activation function, explicitly target this bias. By enabling the network to adapt its frequency response, STAFs can more effectively capture and represent high-frequency components, leading to more accurate and detailed reconstructions.
% \vspace{-0.75em}
\subsection{Initialization} 
% \vspace{-0.5em}
\label{sec:model-initialization}
In this section, we present an initialization strategy tailored for networks utilizing STAF as the activation function. While STAF shares similarities with SIREN~\citep{Siren}, which employs \(\sin\) as its activation function, our initialization scheme is specifically designed to leverage the unique parameterization of STAF. To provide context, we first revisit the key aspects of SIREN's initialization scheme as discussed in~\citep{Siren}, and then highlight how our approach builds upon and extends these principles to enhance network performance and stability.

In SIREN~\citep{Siren}, the input $X$ of a single neuron follows a uniform distribution $U(-1,1)$, and the activation function employed is $\rho(u) = \sin(u)$. Consequently, the output of the neuron is given by $Y = \sin(aX+b)$, where $a,b \in \mathbb{R}$. The authors of \citep{Siren} claim that regardless of the choice of $b$, if $a > \frac{\pi}{2}$, the output $Y$ follows an arcsine distribution, denoted as $Arcsine(-1,1)$. However, it becomes apparent that this claim is not correct upon further examination. If the claim were true, $\mathbb{E}[Y]$ would be independent of $b$. Let's calculate it in a more general case, where instead of the interval $[-1,1]$, we consider an arbitrary interval $[c,d]$ for the input $X$.

\begin{flalign}
    \mathbb{E}[Y]&=\int_{c}^{d} \sin(ax + b) f_X(x)\, dx && \nonumber \\
    &= \frac{1}{d-c}\int_{c}^{d} (\sin(ax) \cos b + \sin b \cos(ax)) \, dx, && \nonumber \\
    &= \frac{1}{a(d-c)}\Big[\big(\cos(ac)-\cos(ad)\big)\cos b && \nonumber \\
    &\quad + \big(\sin(ad)-\sin(ac)\big)\sin b\Big]. &&
\end{flalign}

Assuming $c=-1$ and $d=1$, the result will be $\frac{2\sin a \sin b}{a(d-c)}$, which obviously depends on $a$ and $b$. However, if we want to eliminate $b$ from $\mathbb{E}[Y]$, we can set $ad=ac+2n\pi$, or equivalently
\begin{equation} \label{length_of_interval}
    d=c+\frac{2n\pi}{a},
\end{equation}
for an $n \in \mathbb{N}$. Next, let us consider the next moments of $Y$, because if the moment-generating function (MGF) of $Y$ exists, the moments can uniquely determine the distribution of $Y$.
\begin{flalign}
    \mathbb{E}[Y^k]=\int_{c}^{d}\frac{\sin^k(ax+b)}{d-c}dx
\end{flalign}
Using \eqref{length_of_interval}, it is equal to
\begin{equation}
    \frac{1}{2n\pi}\int_{c}^{c+\frac{2n\pi}{a}}\sin^k(ax+b)dx
\end{equation}
By assuming $u=ax+b$, we have
\begin{equation}
    \mathbb{E}[Y^k]=\frac{1}{2an\pi}\int_{ac+b}^{ac+b+2n\pi}\sin^k(u)du.
\end{equation}
Since for each pair of natural numbers $(k,n)$, $2n\pi$ is a period of $\sin^k(u)$, we can write
\begin{equation}
    \mathbb{E}[Y^k]=\frac{1}{2an\pi}\int_{0}^{2\pi}\sin^k(u)du =\begin{cases}
			0, & \text{if $k$ is odd}\\
            \frac{\binom{k}{k/2}}{2^{k}an}, & \text{if $k$ is even}
		 \end{cases}
\end{equation}
As shown, the moments of $Y$ (and thus the distribution of $Y$) depend on $a$ (the weight multiplied by the input) and $n$ (a parameter defining the range of input).

In the subsequent parts of \citep{Siren}, the authors assumed that the outputs of the first layer follow a distribution of \textit{arcsine} and fed those outputs into the second layer. By relying on the central limit theorem (CLT), they demonstrated that the output of the second layer, for each neuron, conforms to a normal distribution. Additionally, in Lemma 1.6~\citep{Siren}, they established that if $X \sim \mathcal{N}(0,1)$ and $Y=\sin(\frac{\pi}{2}X)$, then $Y \sim Arcsine(-1,1)$. However, it should be noted that to prove this result, they relied on several approximations. Through induction, they asserted that the inputs of subsequent layers follow an arcsine distribution, while the outputs of these layers exhibit a normal distribution.

In contrast to the approach taken by~\citep{Siren}, the method presented in this study does not depend on the specific distributions of the input vector $\boldsymbol{r}$ and weight matrices $\boldsymbol{W^{(l)}}$. As a result, there is no need to map the inputs to the interval $[-1,1]$. Additionally, this method does not rely on making any approximations or the central limit theorem, which assumes large numbers. Overall, it offers a more rigorous mathematical framework. To pursue this goal, notice the following theorem.

\begin{theorem} \label{initialization_theorem}
Consider a neural network as defined in \eqref{Network} with a sinusoidal trainable activation function (STAF) defined in \eqref{STAF}. Suppose for each $i$, $\Phi_i \sim U(-\pi, \pi)$. Furthermore, let $C_i$ be i.i.d. random variables with the following probability density function:
\begin{equation}
f_{C_i}(c_i) = \frac{\tau |c_i|}{2} e^{\frac{-\tau c_i^2}{2}},
\end{equation}
and assume that $C_i$'s are independent of $\Omega_i$, $\boldsymbol{w}$, $\boldsymbol{x}$, and $\Phi_i$. Then, every post-activation will follow a $\mathcal{N}(0, 1)$ distribution (please refer to the proof in Appendix \ref{app:proof1}.)
\end{theorem}

% \begin{theorem} \label{main_th}
%     Consider the following function $Z$
%     \begin{equation} \label{definition_of_h}
%     Z=\sum_{u=1}^{\tau}C_u\sin\left(\Omega_u\boldsymbol{w}.\boldsymbol{x}+\Phi_u\right)
%     \end{equation}
%     Suppose $C_u$'s are symmetric distributions, have finite moments, and are independent of $\Omega_u,\boldsymbol{w},\boldsymbol{x},\Phi_u$. Furthermore, for each $u$, $\Phi_u \sim U(-\pi,\pi)$. Then the moments of $Z$ will only depend on $\tau$ and the moments of $C_u$'s. Moreover, the odd-order moments of $Z$ will be zero. (proof is presented in \ref{proof1})
% \end{theorem}


% Now, our goal is to determine the distribution of the $C_u$'s so that the distribution of $Z$ becomes $\mathcal{N}(0,1)$. To achieve this, let's first consider the following theorem:
% \begin{theorem}
%     (Page 353 of \citep{Shiryaev2016-xv}) Let $X \sim \mathcal{N}(0,\sigma^2)$. Then
%     \begin{equation} \label{q'th_moments_of_X}
%         E(X^q) = \begin{cases}
% 			0, & \text{if $q$ is odd}\\
%             \frac{q!}{\frac{q}{2}!~2^{q/2}}\sigma^q, & \text{if $q$ is even}
% 		 \end{cases}
%     \end{equation}
%     and these moments pertain exclusively to the normal distribution.
% \end{theorem}

% In theorem \eqref{main_th}, we proved that for odd values of $q$, $\mathbb{E}[h^q]=0$. Thus, in order to have $Z \sim \mathcal{N}(0,1)$, for even values of $q$, we must have $\mathbb{E}[h^q] = \frac{q!}{\frac{q}{2}!~2^{q/2}}\sigma^q.$

% Alternatively, we can express it as
% \begin{equation}
%     \frac{1}{2^q}\sum_{\substack{j_1 + \dots + j_\tau = \frac{q}{2} \\ j_1, \dots, j_\tau \geq 0}} \binom{q}{j_1, j_1, \dots, j_\tau, j_\tau} \prod_{u=1}^\tau \mathbb{E}[C_u^{2j_u}] = \frac{q!}{\frac{q}{2}!~2^{q/2}}.
% \end{equation}
% Simplifying further, we obtain
% \begin{equation}
%     \frac{q!}{2^q} \sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \frac{\prod_{u=1}^\tau \mathbb{E}[C_u^{2j_u}]}{(j_1!)^2\dots(j_\tau!)^2} = \frac{q!}{\frac{q}{2}!~2^{q/2}}.
% \end{equation}
% This equation can be further simplified to
% \begin{equation} \label{desired}
%     \sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \frac{\prod_{u=1}^\tau \mathbb{E}[C_u^{2j_u}]}{(j_1!)^2\dots(j_\tau!)^2} = \frac{2^{q/2}}{\frac{q}{2}!}.
% \end{equation}
% Equation \eqref{desired} provides a general formula that can be utilized in further research. It allows for finding different solutions for $C_u$ under various assumptions (e.g., independence or specific dependencies) and different values of $\tau$. However, in the subsequent analysis, we assume that $C_u$'s are independent and identically distributed (i.i.d) random variables. The following theorem aims to satisfy Equation \eqref{desired}.
% \begin{theorem} \label{finding_the_moments}
%     Suppose $C_u$'s are i.i.d random variables with the following even-order moments:
%     \begin{equation} \label{moments_of_C}
%         \mathbb{E}[C_u^{2j}]=\left(\frac{2}{\tau}\right)^j j!
%     \end{equation}
%     Then, for every non-negative even number $q$, Equation \eqref{desired} holds (the proof is presented in \ref{proof3}).\footnote{If you wonder how this solution struck our mind, you can start by solving equation \eqref{desired} for $q=2$ to obtain $\mathbb{E}[h^2]$. Then, using the value of $\mathbb{E}[h^2]$, solve \eqref{desired} for $q=4$ to obtain $\mathbb{E}[h^4]$, and so on. }
% \end{theorem}

% Also note that according to Theorem \eqref{main_th}, the odd-order moments of $Z$ are zero, just like a normal distribution.

% \begin{corollary}
%     Let $Z$ be the random variable defined in \eqref{definition_of_h}. Additionally, assume that the $C_u$'s ($1 \leq u \leq \tau$) used in the definition of $Z$, are i.i.d  random variables with even moments as defined in theorem \eqref{finding_the_moments}.
%     Then $Z\sim \mathcal{N}(0,1)$.
% \end{corollary}
% \begin{proof}
%     We know that if the moment generating function (MGF) of a distribution exists, then the moments of that distribution can uniquely determine its probability distribution function (PDF). That is, if $X$ and $Y$ are two distributions and for every natural number $k$, $E(X^k)=E(Y^k)$, then $X=Y$.

%     In the Theorem \eqref{finding_the_moments}, we observed that the moments of $Z$ are equal to the moments of a standard normal distribution. Since the MGF of this distribution exists, $Z\sim \mathcal{N}(0,1)$.
% \end{proof}

% Now, let's explore which distribution can produce the moments defined in equation \eqref{moments_of_C}. To have an inspiration, note that for a centered Laplace random variable $X$ with scale parameter $b$, we have the probability density function (PDF) of $X$ as
% \begin{equation}
%     f_X(x)=\frac{1}{2b}e^\frac{-|x|}{b}
% \end{equation}
% and the moments of $X$ given by
% \begin{equation}
%     \mathbb{E}[X^q] = \begin{cases}
% 			0, & \text{if $q$ is odd}\\
%             \frac{b^q}{q!}, & \text{if $q$ is even}
% 		 \end{cases}
% \end{equation}
% Hence, the answer might be similar to this distribution. If we assume $Y=sgn(X)\sqrt{|X|}$, since $Y$ is symmetric, all of its odd-order moments are zero. Now, let us calculate its even-order moments:
% \begin{equation}
%     \mathbb{E}[Y^{2q}] = \mathbb{E}[|X|^q] = \int_{-\infty}^{\infty} |x|^q \frac{1}{2b} e^{-\frac{|x|}{b}} dx = 2\int_{0}^{\infty} |x|^q \frac{1}{2b} e^{-\frac{|x|}{b}} dx = \frac{1}{b} \int_{0}^{\infty} x^q e^{-\frac{x}{b}} dx
% \end{equation}
% By assuming $u=\frac{x}{b}$, we will have
% \begin{equation} \label{even_moments_of_Y}
%     \mathbb{E}[Y^{2q}] = \int_{0}^{\infty} (bu)^q e^{-u} du = b^q \int_{0}^{\infty} u^q e^{-u} du = b^q \Gamma(q+1) = b^q q!
% \end{equation}
% By assuming $b=\frac{2}{\tau}$, \eqref{moments_of_C} will be obtained.

% The next theorem will obtain the probability distribution function of $Y$.

% \begin{theorem} \label{final_pdf}
%     Let $X$ be a centered Laplace random variable with scale parameter $b$, and $Y=sgn(X)\sqrt{|X|}$. Then
%     \begin{equation}
%         f_Y(y)=\frac{|y|}{b}e^\frac{-y^2}{b}
%     \end{equation}
%     (The proof is presented in \ref{proof5}.)
% \end{theorem}

% In summary, in this section we calculated the initial coefficients of our activation function as described in Theorem \eqref{final_pdf}, where we set $b=\frac{2}{\tau}$. Consequently, if we denote the post-activation of layer $l$ by $\boldsymbol{z^{(l)}}$, we will have $\boldsymbol{z^{(l)}}_i\sim\mathcal{N}(0,1)$ for all $l\in\{2,3,\ldots,L-1\}$, and $i\in\{1,\ldots F_l\}$. This result can be proved by induction on $l$, using the fact that, based on the theorems in this section, the PDF of $Z$ is independent of the PDF of $\boldsymbol{x}$.

This initial setting, where every post-activation follows a standard normal distribution, is beneficial because it prevents the post-activation values from vanishing or exploding. This ensures that the signals passed from layer to layer remain within a manageable range, particularly in the first epoch. The first epoch is crucial as it establishes the foundation for subsequent learning. If the learning process is well-posed and there is sufficient data, the training process is likely to converge to a stable and accurate solution. Therefore, while it is important to monitor for potential issues in later epochs, the concern about vanishing or exploding values is significantly greater during the initial stages. Proper initialization helps mitigate these risks early on, facilitating smoother and more effective training overall.

