% \vspace{-0.75em}
\section{Related Works}
% \vspace{-0.5em}
\label{sec:relatedworks}

% \textbf{Implicit Neural Representation.}

% Summarized
INRs have advanced in representing various signals, including images and 3D scenes, with applications in SDFs, audio signals, and data compression. Sitzmann et al.'s sine-based activations in INRs \citep{Siren} improved fidelity but faced slow training. Dual-MLP architectures \citep{mehta2021modulated}, input division into grids \citep{aftab2022multi,kadarvish2021ensemble}, and adaptive resource allocation \citep{acorn,saragadam2022miner} further enhanced INR capabilities. Mildenhall et al.'s volume rendering for 3D scene representation in NeRF \citep{mildenhall2020nerf} inspired subsequent enhancements \citep{chen2024far, barron2023zip, kazerouni2024incode,xu2023jacobinerf, lin2024fastsr, li2025nerf, uy2024nerf, reiser2021kilonerf} for improved fidelity and expedited rendering.

The development of neural networks has been significantly influenced by advancements in activation functions. Early non-periodic functions like Sigmoid suffered from vanishing gradient issues in deep networks, which were later addressed by unbounded functions such as ReLU \citep{nair2010rectified} and its variants \citep{maas2013rectifier, elfwing2018sigmoid, hendrycks2016gaussian}. Adaptive functions like SinLU \citep{paul2022sinlu}, TanhSoft \citep{biswas2021tanhsoft}, and Swish \citep{ramachandran2017searching} introduced trainable parameters to better adapt to data non-linearity. However, the spectral bias in ReLU-based networks, as highlighted by Rahaman et al. \citep{rahaman2019spectral}, led to a preference for low-frequency signals, limiting their ability to capture fine details. To address this, periodic activation functions emerged as promising solutions for INRs, enabling the learning of high-frequency details. Early challenges in training networks with periodic activations \citep{lapedes1987nonlinear, parascandolo2016taming} were eventually overcome, leading to successful applications in complex data representation \citep{Siren, mehta2021modulated}. Fourier Neural Networks \citep{gallant1988there} and the Fourier feature mapping \citep{tancik2020fourier} further advanced the integration of the Fourier series into neural networks. Recently, the Kolmogorov-Arnold Network (KAN) \citep{liu2024kan, ss2024chebyshev} has emerged as a promising architecture in the realm of INRs. KAN leverages Kolmogorov-Arnold representation frameworks to improve the modeling and reconstruction of complex signals, demonstrating notable performance in various INR tasks. However, as we will demonstrate in our experimental results, STAF outperforms KAN in terms of accuracy, convergence speed, and PSNR. This paper introduces STAF, a parametric periodic activation function for MLP-based INRs, designed to enhance convergence and capture fine details with superior fidelity in high-frequency signal representation.

% Commented the complete version
\iffalse
The field of neural networks has seen substantial progress in representing various signals, with applications extending to signed distance functions (SDFs), images, videos, and audio signals \citep{Deepsdf, ortiz2022isdf, duggal2022mending, li2022learning,kim2022learning, rho2022neural}. Sitzmann et al. introduced sine-based activation functions for INR in \citep{Siren}, achieving notable results despite slow training and convergence issues. Dual-MLP architectures for enhanced fidelity and generality in INRs were presented in \citep{mehta2021modulated}. Other approaches include dividing the input signal into smaller grids, with separate networks for each cell \citep{kadarvish2021ensemble}, and methods like \citep{acorn}, which adaptively allocate resources based on signal complexity, facilitating the handling of large-scale signals, e.g., gigapixel images. In the realm of data compression, \citep{dupont2021coin, dupont2022coin++} demonstrated that quantized weights of an MLP could efficiently represent images, even surpassing JPEG compression.

Mildenhall et al. utilized volume rendering with coordinate-based neural networks to represent 3D scenes in \citep{mildenhall2020nerf}. To overcome the challenges in training and the lengthy processes associated with vanilla NeRF, various methodologies have been employed, enhancing NeRF's fidelity and applications, expediting rendering process~\citep{chen2024far,lin2024fastsr,li2025nerf,uy2024nerf,reiser2021kilonerf,neff2021donerf}.


\textbf{Periodic and Non-periodic Activation Functions.} The development of neural networks has been influenced significantly by the introduction of various activation functions. Initial non-periodic functions like sigmoid and step functions faced challenges in deep networks due to vanishing gradients. Unbounded functions like ReLU \citep{nair2010rectified} were shown to be effective as universal approximators, mitigating these issues \citep{sonoda2017neural}. Variations of ReLU, including Leaky ReLU \citep{maas2013rectifier}, SiLU \citep{elfwing2018sigmoid}, and GELU \citep{hendrycks2016gaussian}, have enhanced network performance. The exploration of adaptive activation functions, such as SinLU \citep{paul2022sinlu}, TanhSoft \citep{biswas2021tanhsoft}, and Swish \citep{ramachandran2017searching}, introduced trainable parameters that adapt to data non-linearity. However, these functions have not replicated their success in neural representation tasks. Rahaman et al. \citep{rahaman2019spectral} highlighted the spectral bias in ReLU-based DNNs, which preferentially model low-frequency signal components.


Periodic activation functions have recently shown promise in INR tasks by guiding networks to learn high-frequency details. The training challenges of networks with periodic activations were noted as early as 1987 by Lapedes and Farber \citep{lapedes1987nonlinear}. Parascandolo et al. \citep{parascandolo2016taming} revealed why training such networks is difficult, suggesting the use of truncated sine functions. Klocek et al. \citep{klocek2019hypernetwork} utilized cosine functions in a hypernetwork-defined target network. Sitzmann et al. \citep{Siren} successfully employed sine activations to represent complex unstructured data. Mehta et al. \citep{mehta2021modulated} introduced a neural functional representation with a synthesis network using sine activations and a modulator for phase, amplitude, and frequency modulation. Despite SIREN's superiority in INRs, the application of Fourier series as a general activation function remained unexplored.

Galant and White \citep{gallant1988there} initially proposed Fourier Neural Networks (FFN), mimicking the Fourier series. Tanckik et al. \citep{tancik2020fourier} introduced an FFN that applies a Fourier feature mapping, enhancing the learning of high-frequency data. Further exploration of Fourier series with trainable parameters in small CNN networks for classification and detection tasks was conducted by \citep{shivappriya2021cascade, liao2020trainable}. Drawing inspiration from these, we propose a parametric periodic activation function for MLP-based INR structures, aiming to significantly improve convergence speed and detail capture.
\fi
