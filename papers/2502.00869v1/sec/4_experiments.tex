\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{data/cameraman/cameraman_images.pdf}
    \vspace{-1em}
    \caption{Ground truth image followed by reconstructions using STAF, FINER, KAN, SIREN, WIRE.}
    % \vspace{-1em}
    \label{fig:gt_representations}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{data/cameraman/cameraman_psnr_vals.pdf}
    \vspace{-1em}
    \caption{PSNR values achieved over 300 training iterations.}
    \vspace{-1em}
    \label{fig:psnr_curve}
\end{figure}

% \vspace{-0.75em}
\section{Experimental Results}
% \vspace{-0.5em}
\begin{figure*}[t]
    \centering    
    \includegraphics[width=0.99\textwidth]{data/image/image_reconst_celtic_with_diff.pdf} \\
    \vspace{0.25em}
    \includegraphics[width=0.99\textwidth]{data/image/img_rep_0824.pdf}\\
    \vspace{-1em}
    \caption{Comparative visualization of image representation using \textbf{STAF} and other activation functions. The second row highlights representation errors, with brighter areas indicating higher errors. The Celtic image size is \(128 \times 128\), and the second image from the DIV2K~\citep{div2k} dataset is downsampled by a factor of \(1/4\) to \(510 \times 339\).}
    \label{fig:celtic}
    % \vspace{-1em}
\end{figure*}
We evaluated SOTA models for image, audio, and shape representations, as well as inverse problems such as super-resolution and image denoising. Specifically, we used an MLP architecture with 3 hidden layers and 256 hidden nodes. The models tested included INCODE, FINER, WIRE, Gauss, FFN, SIREN, ReLU with positional encoding, and MFN~\citep{kazerouni2024incode, liu2024finer, saragadam2023wire, ramasinghe2022beyond, Siren, tancik2020fourier, fathony2020multiplicative}. We compared our approach with the recently proposed KAN networks~\citep{liu2024kan}, particularly using the Chebyshev-Polynomial KAN variant, which provides a more efficient implementation of KAN networks~\citep{ss2024chebyshev}.  All experiments were conducted on a desktop PC equipped with 32 GB of RAM and an NVIDIA RTX-3090 GPU. Our implementation was inspired by the codebases of SIREN, WIRE, and INCODE. The learning rates for each model were selected based on the optimal configurations reported in their original papers and codebases. For STAF, we used a learning rate of \(2.5 \times 10^{-4}\). All models were trained with the Adam optimizer to ensure consistency in optimization and comparison. STAF was initialized using the methodology outlined in~\cref{sec:model-initialization}, specifically designed to enhance convergence and performance. Other models were initialized following the strategies recommended in their respective original papers. We used \(\tau = 5\) for all tasks, except image denoising (\(\tau = 2\)). Notably, we included a detailed Neural Tangent Kernel (NTK) analysis of our model (see Appendix~\ref{NTK-analysis}) and ablation studies in Appendix~\ref{appendix}.
% \vspace{-0.75em}
\subsection{Signal Representations}
% \vspace{-0.5em}
We evaluated image representation tasks on several images, as shown in Figures \ref{fig:celtic} and \cref{fig:image_rep}. STAF achieved the highest PSNR (\(104.57\) dB) in the Celtic image task, outperforming SIREN (\(37.65\) dB), WIRE (\(51.41\) dB), and MFN (\(70.13\) dB), with error maps highlighting its superior accuracy. In the larger-scale reconstruction task, STAF again led with \(40.47\) dB, surpassing INCODE (\(37.21\) dB), FINER (\(37.29\) dB), SIREN (\(34.40\) dB), and others, as evident in the visual details and magnified insets. \cref{fig:map} shows activation maps learned during the image reconstruction task. STAF produces more detailed and higher-quality reconstructions compared to SIREN and WIRE, highlighting its ability to capture complex features more effectively. We also illustrate the performance comparison on a grayscale cameraman image in~\cref{fig:gt_representations} and the convergence rate of STAF in~\cref{fig:psnr_curve}, demonstrating STAF's effectiveness in addressing the capacity-convergence gap in INR models \cite{liu2024finer}. 

The quantitative and qualitative results of the shape representation are shown in~\cref{tab:sdf} and~\cref{fig:sdf}. Using the Stanford 3D Scanning Repository and following the INODE strategy~\citep{kazerouni2024incode}, we generated an occupancy volume by sampling points on a grid \(512 \times 512 \times 512\), assigning 1 to voxels inside the object and 0 outside. The results demonstrate STAF's capability to effectively capture both fine and coarse 3D shape details, achieving higher Intersection over Union (IoU) than other methods.
\begin{table}[h]
    \centering
    \vspace{-0.5em}
    \caption{Quantitative comparisons of SDF representations.}
    \resizebox{1\linewidth}{!}{%
    \begin{tabular}{ccccccc}
        \toprule
         & Methods & Armadillo & Dragon & Lucy & Thai Statue & Avg. \\ \midrule
        \multirow4{*}{\rotatebox{90}{\textbf{IOU $\uparrow$}}} & ReLU+P.E. & 0.9958 & {\cellcolor{yellow}}0.9966 & {\cellcolor{yellow}}0.9920 & 0.9911 & {\cellcolor{yellow}}0.9939 \\ 
        & SIREN & 0.9962\cellcolor{yellow} & {\cellcolor{orange}}0.9971 & 0.9892 & \cellcolor{orange}0.9929 & 0.9939{\cellcolor{yellow}} \\ 
        & WIRE & 0.9721 & 0.9749 & 0.9554 & 0.9507 & 0.9633 \\ 
        & FINER & 0.9965\cellcolor{orange} & 0.9958 & {\cellcolor{orange}}0.9962 & \cellcolor{yellow}0.9923 & {\cellcolor{orange}}0.9952  \\ 
        & INCODE & 0.9849 & 0.9869 & 0.9774 & 0.9760 & 0.9813 \\ 
        \midrule
        & \textbf{STAF} & {\cellcolor{red}}0.9972 & 0.9973{\cellcolor{red}} & 0.9971{\cellcolor{red}} & {\cellcolor{red}}0.9935 & 0.9963\cellcolor{red} \\ \bottomrule
    \end{tabular}%
    }
    \label{tab:sdf}
    \vspace{-0.5em}
\end{table}

For the audio task, we used a 7-second clip from Bachâ€™s Cello Suite No. 1: Prelude~\citep{Siren}, sampled at 44,100 Hz. \cref{fig:audio} illustrates the waveforms and reconstruction errors, where STAF demonstrates the highest PSNR, the lowest reconstruction error, and superior fidelity.

% \vspace{-0.75em}
\subsection{Inverse Problems}
% \vspace{-0.5em}
The results in~\cref{fig:4x_super_res} and~\cref{fig:img_denoising} demonstrate that STAF consistently outperforms other activation-based implicit models in both super-resolution and image denoising tasks. INRs, when applied as interpolants, exhibit inherent biases that can be leveraged for inverse problems, particularly super-resolution. To validate this, we conducted $4\times$ super-resolution experiments, where STAF achieved the highest PSNR (30.54 dB) and SSIM (0.89), surpassing the second-best INCODE (29.88 dB) and third-best FFN (29.41 dB). Other activations, such as SIREN and FINER, recover textures reasonably well but struggle with high-frequency details, while Gauss suffers from excessive blurring. These results indicate that STAF effectively reconstructs fine details, outperforming other INRs in high-resolution image recovery. For image denoising, we generated noisy images using realistic sensor measurements, introducing Poisson-distributed photon noise per pixel with a mean photon count of 10, simulating an extremely challenging low-light scenario with severe noise corruption. As shown in~\cref{fig:img_denoising}, STAF achieves the highest PSNR (24.19 dB), demonstrating superior noise suppression and detail retention compared to other baselines. Methods such as FINER and FFN exhibit noticeable artifacts, where the noise introduces proximity color distortions. These results confirm that STAF is effective in inverse problem tasks.
