\label{appendix}
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\section{Neural Tangent Kernel}
\label{NTK-analysis}
The Neural Tangent Kernel (NTK) is a significant concept in the theoretical understanding of neural networks, particularly in the context of their training dynamics \citep{jacot2018neural}. To be self-contained, we provide an explanation of the NTK and its background in kernel methods. We believe this will be beneficial for readers, as previous papers on implicit neural representation using the NTK concept have not adequately explained the NTK or the significance of its eigenvalues and eigenfunctions.

% It originates from the study of infinitely wide neural networks, where the network's parameters can be analyzed using kernel methods. The NTK framework  provides insights into how neural networks evolve during training and how they can be approximated by kernel methods.

A kernel is a function \( K(\mathbf{x}, \mathbf{\tilde{x}}) \) used in integral transforms to define an operator that maps a function \( f \) to another function \( T_f \) through the integral equation
\[
T_f(\mathbf{x}) = \int K(\mathbf{x}, \mathbf{\tilde{x}}) f(\mathbf{\tilde{x}}) \, d\mathbf{\tilde{x}}.
\]
Since \( T_f \) is a linear operator with respect to \( f \), we can discuss its eigenvalues and eigenfunctions. The eigenvalues and eigenfunctions of a kernel are the scalar values \(\lambda\) and the corresponding functions \(\zeta(\mathbf{x})\) that satisfy the following equation \citep{ghojogh2021reproducing}
\[
\int K(\mathbf{x}, \mathbf{\tilde{x}}) \zeta(\mathbf{\tilde{x}}) \, d\mathbf{\tilde{x}} = \lambda \zeta(\mathbf{x}).
\]

In the context of neural networks, the concept of a kernel becomes particularly remarkable when analyzing the network's behavior in the infinite-width limit. Kernels in machine learning, such as the Radial Basis Function (RBF) kernel or polynomial kernel, are used to measure similarity between data points in a high-dimensional feature space. These kernels allow the application of linear methods to non-linear problems by implicitly mapping the input data into a higher-dimensional space \citep{braun2005spectral}.

The NTK extends this idea by considering the evolution of a neural network's outputs during training. When a neural network is infinitely wide, its behavior can be closely approximated by a kernel method. In this case, the kernel in question is the NTK, which emerges from the first-order Taylor series approximation (or tangent plane approximation) of the network's outputs.

Formally, for a neural network \( f(\mathbf{x}; \boldsymbol{\theta}) \) with input \( \mathbf{x} \) and parameters \( \boldsymbol{\theta} \), the NTK, denoted as \( K^{(L)}(\mathbf{x}, \mathbf{\Tilde{x}}) \), is defined as:
\[
K^{(L)}(\mathbf{x}, \mathbf{\Tilde{x}}) = \langle \nabla_{\boldsymbol{\theta}} f(\mathbf{x}; \boldsymbol{\theta}),\nabla_{\boldsymbol{\theta}} f(\mathbf{\Tilde{x}}; \boldsymbol{\theta})\rangle,
\]
where \( \nabla_{\boldsymbol{\theta}} f(\mathbf{x}; \boldsymbol{\theta}) \) represents the gradient of the network output with respect to its parameters.

There are two methods for calculating the NTK: the analytic approach and the empirical approach \citep{novak2019neural,chen2022neural}. In the paper, we derived the analytic NTK of a neural network that uses our activation function, as detailed in the appendix. However, for our experimental purposes, we utilized the empirical NTK. It is worth noting that calculating the NTK for real-world networks is highly challenging, and typically not computationally possible \citep{mohamadi2023fast}.

Just like the computation of NTK, there are analytic and empirical methods for calculating the eigenvalues and eigenfunctions of a kernel \citep{williams2000effect}. These values play a crucial role in characterizing neural network training. For instance, it has been shown that the eigenvalues of the NTK determine the convergence rate \citep{wang2022and,bai2023physics}. Specifically, components of the target function associated with kernel eigenvectors having larger eigenvalues are learned faster \citep{wang2022and,tancik2020fourier}. In fully-connected networks, the eigenvectors corresponding to higher eigenvalues of the NTK matrix generally represent lower frequency components \citep{wang2022and}. Furthermore, the eigenfunctions of an NTK can illustrate how effectively a model learns a signal dictionary \citep{yuce2022structured}.

Figure \ref{fig:ntk_eigenfunctions} illustrates the eigenfunctions of various NTKs using different activation functions. As shown, the STAF activation function results in finer eigenfunctions, which intuitively enhances the ability to learn and reconstruct higher frequency components. Additionally, Figure \ref{fig:ntk_eigenvalues} presents the eigenvalues of different NTKs with various activation functions. The results indicate that STAF produces higher eigenvalues, leading to a faster convergence rate during training. Moreover, STAF also generates a greater number of eigenvalues, compared to ReLU and SIREN. Having more eigenvalues is beneficial because it suggests a richer and more expressive kernel, capable of capturing a wider range of features and details in the data.

% This increased capacity allows the model to better approximate complex functions, improving its overall performance and generalization ability.

% \begin{figure}
%   \centering
%   \includegraphics[width=5.5in]{data/ntk/ntk_eigenfunctions.pdf}
%   \caption{First 10 eigenfunctions of the empirical NTK of STAF, SIREN, FFN and RELU.}
%   \label{ntk_eigenfunctions}
% \end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[width=2.5in]{data/ntk/ntk_eigenvalues.pdf}
%   \caption{The eigenvalue spectrum of the empirical NTK of STAF, SIREN, FFN and RELU.}
%   \label{ntk_eigenvalues}
% \end{figure}

\begin{figure}[t]
    \centering
    \subfigure[The first five eigenfunctions of the empirical NTK of STAF, SIREN, FFN, and ReLU.]
        {\includegraphics[width=0.49\textwidth]{data/ntk/ntk_eigenfunctions_cropped.pdf}\label{fig:ntk_eigenfunctions}}
    \hfill
    \subfigure[The eigenvalue spectrum of the empirical NTK of STAF, SIREN, FFN, and ReLU.]
        {\includegraphics[width=0.49\textwidth]{data/ntk/ntk_eigenvalues.pdf}\label{fig:ntk_eigenvalues}}
    \caption{(a) The first five eigenfunctions of the empirical NTK of STAF, SIREN, FFN, and ReLU. (b) The eigenvalue spectrum of the empirical NTK of STAF, SIREN, FFN, and ReLU.}
    \label{fig:ntk_combined}
\end{figure}

\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
\subsection{Analytic NTK}
In this section, we compute the analytic NTK for a neural network that uses the proposed activation function (STAF), following the notation from \citep{modernml2024}. Interested readers can also refer to \citep{jacot2018neural} and \citep{golikov2022neural}. However, we chose \citep{modernml2024} for its clarity and ease of understanding. According to \citep{modernml2024}, the NTK of an activation function for a neural network with $L-1$ hidden layers is as follows.
\begin{theorem}
(Theorem 1 of \citep{modernml2024}, Lecture 6) For $\boldsymbol{x} \in \mathcal{S}^{d-1}$, let $f_{\boldsymbol{x}}^{(L)}(\boldsymbol{w}):\mathbb{R}^p \rightarrow\mathbb{R}$ denote a neural network with $L-1$ hidden layers such that:
\begin{equation}
    f_{\boldsymbol{x}}^{(L)}(\boldsymbol{w}) = \boldsymbol{W^{(L)}} \frac{1}{\sqrt{F_{L-1}}} \phi \left( \boldsymbol{W^{(L-1)}} \frac{1}{\sqrt{F_{L-2}}} \phi \left( \ldots \boldsymbol{W^{(2)}} \frac{1}{\sqrt{F_1}} \phi \left( \boldsymbol{W^{(1)}} \boldsymbol{x} \right) \ldots \right) \right);
\end{equation}
where $W^{(i)} \in \mathbb{R}^{F_i \times F_{i-1}}$ for $i \in \{1, \ldots, L\}$ with $F_0 = d$, $F_L = 1$, and $\phi : \mathbb{R} \rightarrow \mathbb{R}$ is an element-wise activation function. As $F_1, F_2, \dots, F_{L-1} \to \infty$ in order, the Neural Network Gaussian Process (NNGP), denoted as $\Sigma^{(L)}$, and the NTK, denoted as $K^{(L)}$, of $f_{\boldsymbol{x}}(\boldsymbol{w})$ are given by:
\begin{equation}
    \begin{aligned}
    &\Sigma^{(L)}(\boldsymbol{x},\Tilde{\boldsymbol{x}}) = \check{\phi}\left(\Sigma^{(L-1)}(\boldsymbol{x},\Tilde{\boldsymbol{x}})\right);\quad \Sigma^{(0)}(\boldsymbol{x},\Tilde{\boldsymbol{x}})=\boldsymbol{x}^T \Tilde{\boldsymbol{x}} \\
    & K^{(L)}(\boldsymbol{x},\Tilde{\boldsymbol{x}}) = \Sigma^{(L)}(\boldsymbol{x},\Tilde{\boldsymbol{x}}) + K^{(L-1)}(\boldsymbol{x},\Tilde{\boldsymbol{x}})\check{\phi'}\left(\Sigma^{(L-1)}(\boldsymbol{x},\Tilde{\boldsymbol{x}})\right);\\
    & K^{(0)}(\boldsymbol{x},\Tilde{\boldsymbol{x}}) = \boldsymbol{x}^T \Tilde{\boldsymbol{x}}
    \end{aligned}
\end{equation}
where $\check{\phi}: [-1, 1] \rightarrow \mathbb{R}$ is the dual activation for $\phi$, and is calculated as follows:
\begin{equation}
\check{\phi}(\xi) = \mathbb{E}_{(u,v) \sim \mathcal{N}(0,\boldsymbol{\Lambda})} [\phi(u)\phi(v)] \quad\text{where}~ \boldsymbol{\Lambda} = 
\begin{bmatrix}
1 & \xi \\
\xi & 1 
\end{bmatrix}.
\end{equation}
Furthermore, $\phi$ is normalized such that $\check{\phi}(1) = 1$.
\end{theorem}

Consequently, it suffices to calculate $\check{\phi}$. It has been calculated in the following theorem. Just like what mentioned in \citep{wang2023learning}, we assume that the optimization of neural networks with STAF can be decomposed into two phases, where we learn the coefficients of STAF in the first phase and then train the parameters of neural network in the second phase. This assumption is reasonable as the number of parameters of STAF is far less than those of networks and they quickly converge at the early stage of training. As a result, in the following theorem, all the parameters except weights are fixed, since they have been obtained in the first phase of training.

\begin{theorem} \label{analytic_NTK}
    Let $\rho^*$ be the proposed activation function (STAF). Then
    \begin{align}
    &\check{\rho^*}(\xi)= \sum_{i=1}^{\tau} \sum_{j=1}^{\tau} C_i C_j \Delta_{i,j} \nonumber \\
    &= \frac{1}{2}\sum_{i=1}^{\tau} \sum_{j=1}^{\tau} C_i C_j e^{\frac{-1}{2}\left(\Omega_i^2+\Omega_j^2\right)}\left(e^{\Omega_i \Omega_j \xi} \cos(\Phi_i - \Phi_j)+ e^{-\Omega_i \Omega_j \xi} \cos(\Phi_i + \Phi_j)\right)
\end{align}

Therefore,
\begin{equation}
\scalebox{0.98}{$
\check{\rho^*}'(\xi)= \frac{1}{2} \sum_{i=1}^{\tau} C_i \Omega_i \sum_{j=1}^{\tau} \left[C_j \Omega_j e^{\frac{-1}{2}(\Omega_i^2+\Omega_j^2)}\left(e^{\Omega_i \Omega_j \xi} \cos(\Phi_i - \Phi_j) - e^{-\Omega_i \Omega_j \xi} \cos(\Phi_i + \Phi_j)\right)\right].
$}
\end{equation}
\end{theorem}
\begin{proof}
\begin{align} \label{dual}
\check{\rho^*}(\xi) &=\mathbb{E}_{(u,v) \sim \mathcal{N}(0,\boldsymbol{\Lambda})} [\rho^*(u)\rho^*(v)] \nonumber \\
&= \mathbb{E}_{(u,v) \sim \mathcal{N}(0,\boldsymbol{\Lambda})} 
\left[
    \sum_{i=1}^{\tau} C_i \sin(\Omega_i u + \Phi_i) \sum_{i=1}^{\tau} C_i \sin(\Omega_i v + \Phi_i)
\right]
\nonumber\\
&= \mathbb{E}_{(u,v)\sim\mathcal{N}(0,\boldsymbol{\Lambda})}
\left[
    {\sum}_{i=1}^\tau {\sum}_{j=1}^\tau C_iC_j\sin(\Omega_ iu+\Phi_ i)
        \sin(\Omega_ jv+\Phi_ j)
\right] \nonumber\\
&= \sum_{i=1}^{\tau} \sum_{j=1}^{\tau} C_i C_j \mathbb{E}_{(u,v) \sim \mathcal{N}(0,\boldsymbol{\Lambda})} 
\Bigl(\sin(\Omega_i u + \Phi_i)\sin(\Omega_j v + \Phi_j)\Bigr).
\end{align}

So, we need to compute the following expectation:
\begin{equation} \label{delta}
\Delta_{i,j}=\mathbb{E}_{(u,v) \sim \mathcal{N}(0,\boldsymbol{\Lambda})} \left(\sin(\Omega_i u + \Phi_i)\sin(\Omega_j v + \Phi_j)\right)
\end{equation}

Note that for a random vector $\mathbf{X}=(X_1,\dots,X_d)^T$ with mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Lambda}$, the joint probability density function (PDF) is as follows:
\begin{equation}
    f_{\mathbf{X}}(\mathbf{x})=(2\pi)^{-d/2}\det(\boldsymbol{\Lambda})^{-1/2}e^{(\frac{-1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Lambda}^{-1}(\mathbf{x}-\boldsymbol{\mu}))}.
\end{equation}
As a result, since $\boldsymbol{\Lambda}^{-1} = \frac{1}{1 - \xi^2} \begin{bmatrix} 1 & -\xi \\ -\xi & 1 \end{bmatrix}$, we will have:
\begin{align} \label{joint_pdf}
f_{U,V}(u,v) &= \frac{1}{2\pi\sqrt{1 - \xi^2}} e^{-\frac{1}{2} 
\begin{pmatrix} u & v \end{pmatrix} 
\boldsymbol{\Lambda}^{-1} 
\begin{pmatrix} u \\ v \end{pmatrix}} = \frac{1}{2\pi\sqrt{1 - \xi^2}} e^{\frac{-1}{2(1-\xi^2)} 
\begin{pmatrix} u & v \end{pmatrix}
\begin{pmatrix} 1 & -\xi \\ -\xi & 1 \end{pmatrix}
\begin{pmatrix} u \\ v \end{pmatrix}} \nonumber\\
&= \frac{1}{2\pi\sqrt{1-\xi^2}} e^{\frac{-(u^2- 2\xi uv +v^2)}{2(1-\xi^2)}}.
\end{align}

Consequently, using \Cref{dual,delta}, we have
\begin{align} \label{delta_in_terms_of_I1}
\Delta_{i,j}&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
\Bigl( \sin(\Omega_i u + \Phi_i) \sin(\Omega_j v + \Phi_j)f_{U,V}(u,v) \Bigr) dudv \nonumber \\
&=\frac{1}{2\pi\sqrt{1 - \xi^2}}
\int_{-\infty}^\infty
\sin(\Omega_j v + \Phi_j)I_1dv;
\end{align}
where
\begin{align}
&I_1 = \int_{-\infty}^\infty \sin(\Omega_i u+\Phi_i) e^{\frac{-(u^2 - 2\xi uv + v^2)}{2(1 -\xi^2)}}du = e^{\frac{-v^2}{2(1-\xi^2 )}} \int_{-\infty}^{\infty} \sin(\Omega_i u+\Phi_i) e^{\frac{-(u^2-2\xi uv)}{2(1-\xi^2)}}du  \nonumber \\
&= e^\frac{-v^2+\xi^2 v^2}{2(1-\xi^2)} \int_{-\infty}^{\infty} \sin(\Omega_i u+\Phi_i) e^\frac{-(u^2-2\xi uv+\xi^2 v^2)}{2(1-\xi^2)}du \nonumber \\
&= e^{-v^2/2}\int_{-\infty}^{\infty} \sin(\Omega_i u+\Phi_i) e^\frac{-(u-\xi v)^2}{2(1-\xi^2)}du
\end{align}
By assuming $\eta=u-\xi v$ we will have:
\begin{align}
    I_1 &= e^{-v^2/2}\int_{-\infty}^{\infty}\sin(\Omega_i (\eta+\xi v)+\Phi_i) e^\frac{-\eta^2}{2(1-\xi^2)}d\eta
\end{align}
Before going further, we need to consider the following lemma.
\begin{lemma} \label{integral_lemma}
\begin{align}
    \int_{-\infty}^{\infty}\cos(\alpha u + \beta)e^{-\gamma u^2}du = \sqrt{\frac{\pi}{\gamma}}e^{-\frac{\alpha^2}{4\gamma}}\cos\beta,  \label{Improper_integral_1} \\
    \int_{-\infty}^{\infty}\sin(\alpha u + \beta)e^{-\gamma u^2}du = \sqrt{\frac{\pi}{\gamma}}e^{-\frac{\alpha^2}{4\gamma}}\sin\beta \label{Improper_integral_2}
\end{align}
The proof is provided in \eqref{proof_of_integral_lemma}.
\end{lemma}

Let $\alpha=\Omega_i$, $\beta=\Omega_i\xi v+\Phi_i$, and $\gamma=\frac{1}{2(1-\xi^2)}$. As a result of equation \eqref{Improper_integral_2}, we have
\begin{align}
    I_1 &= e^{-v^2/2} \sqrt{2\pi(1-\xi^2)} e^\frac{-\Omega_i^2}{2/(1-\xi^2)}\sin(\Omega_i \xi v+\Phi_i) \nonumber \\
    &= \sqrt{2\pi(1-\xi^2)} e^\frac{-(v^2+\Omega_i^2 (1-\xi^2 ))}{2}\sin(\Omega_i \xi v+\Phi_i)
\end{align}
Therefore, based on \eqref{delta_in_terms_of_I1}, we will have
\begin{align}
&\Delta_{i,j}= \frac{1}{2\pi\sqrt{1-\xi^2}} 
    \int_{-\infty}^{\infty} 
    \Bigl[\sin(\Omega_j v + \Phi_j)\sqrt{2\pi(1-\xi^2)}e^\frac{-(v^2+\Omega_i^2 (1-\xi^2 ))}{2}
    \sin(\Omega_i \xi v+\Phi_i)\Bigr]dv\nonumber \\
&= \frac{e^\frac{(-\Omega_i^2(1-\xi^2)}{2}}{\sqrt{2\pi}}\int_{-\infty}^\infty 
    \Bigl[\sin(\Omega_j v+ \Phi_j)e^{-v^2/2}\sin(\Omega_i \xi v+\Phi_i)\Bigr] dv \nonumber \\
&= \frac{e^{-\Omega_i^2 (1-\xi^2)/2}}{\sqrt{2\pi}}
  \int_{-\infty}^\infty e^{-v^2/2}~\aleph~dv
\end{align}
where
\begin{align}
    \aleph &= \frac{1}{2}\bigl[\cos(v(\Omega_i\xi - \Omega_j)+\Phi_i-\Phi_j) -\cos(v(\Omega_i\xi+\Omega_j) + \Phi_i + \Phi_j)\bigr]
\end{align}
Therefore,
\begin{align} \label{final_delta}
&\Delta_{i,j} = \frac{e^{-\Omega_i^2(1-\xi^2)/2}}{2\sqrt{2\pi}} \left(\sqrt{2\pi} e^{-(\Omega_i\xi - \Omega_j)^2/2} \cos(\Phi_i - \Phi_j)+\sqrt{2\pi} e^{-(\Omega_i\xi + \Omega_j)^2/2} \cos(\Phi_i + \Phi_j)\right)\nonumber \\
&= \frac{e^{-\Omega_i^2 (1-\xi^2)/2}}{2} \left(e^{- (\Omega_i\xi - \Omega_j)^2/2} \cos(\Phi_i - \Phi_j)+e^{- (\Omega_i\xi + \Omega_j)^2/2} \cos(\Phi_i + \Phi_j)\right) \nonumber \\
&= \frac{e^\frac{-\Omega_i^2 (1-\xi^2)}{2} e^{\frac{- (\Omega_i^2 \xi^2+\Omega_j^2)}{2}}}{2} \left(e^{\Omega_i \Omega_j \xi} \cos(\Phi_i - \Phi_j)+e^{-\Omega_i \Omega_j \xi} \cos(\Phi_i + \Phi_j)\right) \nonumber \\
&= \frac{e^{\frac{-1}{2}\left(\Omega_i^2+\Omega_j^2\right)}}{2} \left(e^{\Omega_i \Omega_j \xi} \cos(\Phi_i - \Phi_j)+e^{-\Omega_i \Omega_j \xi} \cos(\Phi_i + \Phi_j)\right)
\end{align}

As a result of \Cref{dual,final_delta}, we have
\begin{align}
    \check{\rho^*}(\xi)&= \sum_{i=1}^{\tau} \sum_{j=1}^{\tau} C_i C_j \Delta_{i,j} \nonumber \\
    &= \frac{1}{2}\sum_{i=1}^{\tau} \sum_{j=1}^{\tau} C_i C_j e^{\frac{-1}{2}\left(\Omega_i^2+\Omega_j^2\right)}\left(e^{\Omega_i \Omega_j \xi} \cos(\Phi_i - \Phi_j)+e^{-\Omega_i \Omega_j \xi} \cos(\Phi_i + \Phi_j)\right)
\end{align}
\end{proof}
\subsection{Proof of Lemma \eqref{integral_lemma}} \label{proof_of_integral_lemma}
\begin{proof}
    We want to calculate these integrals:
\begin{align}
    I_1 = \int_{-\infty}^{\infty}\cos(\alpha u + \beta)e^{-\gamma u^2}du, \nonumber\\
    I_2 = \int_{-\infty}^{\infty}\sin(\alpha u + \beta)e^{-\gamma u^2}du
\end{align}
By adding them we will have
\begin{align}
I_1+iI_2 &=\int_{-\infty}^{\infty}e^{-\gamma u^2}\bigl(\cos(\alpha u +\beta)+i\sin(\alpha u +\beta)\bigr)du =\int_{-\infty}^{\infty} e^{i (\alpha u + \beta)} e^{-\gamma u^2} du \nonumber \\
&= e^{i \beta} \int_{-\infty}^{\infty} e^{-\gamma(u^2 + \frac{\alpha i}{\gamma} u)} du= e^{i \beta} \int_{-\infty}^{\infty} e^{-\gamma(u^2 + \frac{\alpha i}{\gamma} u - \frac{\alpha^2}{4\gamma^2})} e^{-\frac{\alpha^2}{4\gamma}} du\nonumber \\
&= e^{-\frac{\alpha^2}{4\gamma} + i \beta} \int_{-\infty}^{\infty} e^{-\gamma(u^2 + \frac{\alpha i}{\gamma} u - \frac{\alpha^2}{4\gamma^2})} du = e^{-\frac{\alpha^2}{4\gamma} + i \beta} \underbrace{\int_{-\infty}^{\infty} e^{-\gamma\left(u + \frac{\alpha i}{2\gamma}\right)^2} du}_{I_3}
\end{align}
where $i$ is the unit imaginary number. Since we know that the integral of an arbitrary Gaussian function is
\begin{equation}
    \int_{-\infty}^{\infty}  e^{-a(x+b)^2}\,dx= \sqrt{\frac{\pi}{a}},
\end{equation}
we will have $I_3=\sqrt{\frac{\pi}{\gamma}}$. Therefore,
\begin{equation}
    I_1+iI_2 = \sqrt{\frac{\pi}{\gamma}}e^{-\frac{\alpha^2}{4\gamma} + i \beta} = \sqrt{\frac{\pi}{\gamma}}e^{-\frac{\alpha^2}{4\gamma}}(\cos\beta+i\sin\beta)
\end{equation}
As a result,
\begin{equation}
    I_1=\sqrt{\frac{\pi}{\gamma}}e^{-\frac{\alpha^2}{4\gamma}}\cos\beta,\quad
    I_2=\sqrt{\frac{\pi}{\gamma}}e^{-\frac{\alpha^2}{4\gamma}}\sin\beta.
\end{equation}
\end{proof}

\begin{figure*}[!th]
    \centering
    \includegraphics[width=\textwidth]{data/image/img_rep_0010.pdf}\\
    \includegraphics[width=\textwidth]{data/image/img_rep_0809.pdf}
    \caption{Comparative visualization of image representation with \textbf{STAF} and other activation functions.}
    \label{fig:image_rep}
\end{figure*}
\begin{figure}[!th]
    \centering    
    \includegraphics[width=\textwidth]{data/audio/exp_audio.pdf}
    \caption{Comparative visualization of audio representation with \textbf{STAF} and other activation functions.}
    \label{fig:audio}
\end{figure}
\begin{figure}[!th]
    \centering    
    \includegraphics[width=\textwidth]{data/sdf/sdf_experiment.pdf}
    \caption{Comparative visualization of shape representation with \textbf{STAF} and other activation functions.}
    \label{fig:sdf}
\end{figure}

\begin{figure*}[!th]
    \centering
    \includegraphics[width=0.95\textwidth]{data/sr/sr_4x_sample.pdf}\\
    \caption{Comparative visualization of 4x super resolution with \textbf{STAF} and other activation functions.}
    \label{fig:4x_super_res}
\end{figure*}

\begin{figure*}[!th]
    \centering
    \begin{tabular}{cc} % Create a two-column table
        % First column: First subfigure
        \begin{subfigure}
            \centering
            \includegraphics[width=0.72\textwidth]{data/ablation/denoising/denoising_0277.pdf}
            \label{fig:denoising_1}
        \end{subfigure} &
        % Second column: Use raisebox to move the second image upward
        \raisebox{10mm}{ % Adjust the value as needed to move it higher
            \begin{subfigure}
                \centering
                \includegraphics[width=0.24\textwidth]{data/ablation/denoising/denoising_noisy.pdf}
                \label{fig:denoising_2}
            \end{subfigure}
        }
    \end{tabular}
    % Main caption for the figure
    \caption{Comparative visualization of image denoising with \textbf{STAF} and other activation functions.}
    \label{fig:img_denoising}
\end{figure*}

% \section{Neural Radiance Fields (NeRF)}
\section{Ablation Studies}
\label{sec:additional}
In this section, we present ablation studies to demonstrate the effectiveness of STAF.

% We used the first 7 seconds of Bach’s Cello Suite No. 1: Prelude \citep{Siren}, sampled at 44,100 Hz, as our example for the audio representation task.  Figure \ref{fig:audio} shows the comparative visualization of the audio representation results. The first column presents the ground truth audio waveform, while the second column of each row shows the predicted waveforms from each model and their corresponding PSNR values. Additionally, the error plots in the last column highlight areas where each model struggled the most, with brighter regions indicating higher representation errors. STAF achieves the highest PSNR, indicating superior reconstruction fidelity. The SIREN and WIRE models also perform well, but their PSNR values are lower than STAF's, suggesting that STAF can capture finer details in the audio signal.



% We used the Lucy dataset from the Stanford 3D Scanning Repository and followed the WIRE strategy \citep{saragadam2023wire}. An occupancy volume was created through point sampling on a 512×512×512 grid, assigning 1 to voxels within the object and 0 to voxels outside. Figure \ref{fig:sdf} illustrates the comparative results for shape representation. The first column displays the ground truth shapes, while the subsequent columns show the reconstructed shapes from each model along with their Intersection over Union (IoU) scores. STAF again demonstrates superior performance with the highest IoU score, closely matching the ground truth shapes. The SIREN and WIRE models show good performance but fall short of STAF's accuracy. The detailed and zoomed plots in the second rows of Figure \ref{fig:sdf}  reveal that STAF's reconstructions have fewer discrepancies compared to the other models. This indicates that STAF can better capture complex geometric details, leading to more accurate and high-fidelity shape reconstructions. Due to its trainable sinusoidal activation functions, the enhanced expressive power of STAF allows it to adapt more effectively to the intricacies of 3D shapes.

% Overall, the additional experimental results underscore the versatility and effectiveness of STAF across different data representation tasks. By achieving higher PSNR in audio representation and higher IoU in shape representation, STAF proves to be a valuable tool for various applications in computer graphics, audio processing, and beyond.

\begin{figure}[!th]
    \centering
    \subfigure[Ablation study of amplitude, frequency, and phase contributions on PSNR performance.]
        {\includegraphics[width=0.48\textwidth]{data/ablation/ablation_psnr_vals.pdf}
        \label{fig:component1}}
    \hfill
    \subfigure[Analysis of activation patterns per network, layer, and neuron on PSNR performance.]
        {\includegraphics[width=0.48\textwidth]{data/ablation/other_methods__psnr_vals.pdf}
        \label{fig:network_act_type}}
    \caption{(a) Ablation study of amplitude, frequency, and phase contributions on PSNR performance. (b) Analysis of activation patterns per network, layer, and neuron on PSNR performance.}
    \label{fig:ablation_1}
\end{figure}
\begin{figure}[!th]
    \centering
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{data/ablation/params_match_cameraman_psnr_vals.pdf}
        \caption{Comparison of PSNR performance between STAF and SIREN over 250 epochs. STAF, with 213,761 parameters, achieves significantly higher PSNR values compared to SIREN, which has 264,193 parameters.}
        \label{fig:match_params}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{data/ablation/hash_enc_compare_celtic_images_psnrs.pdf}
        \caption{Performance comparison of STAF, SIREN, and Hash Encoding on single image reconstruction. The PSNR curves show that STAF achieves the highest PSNR, followed by Hash Encoding and SIREN.}
        \label{fig:hash}
    \end{minipage}
\end{figure}
% \begin{figure}[t]
%     \centering
%     \subfigure[Ablation study on the high-resolution Cameraman image ($256 \times 256$).]
%         {\includegraphics[width=0.48\textwidth]{data/ablation/cameraman_psnr_vals.pdf}
%         \label{fig:camera_256_256_val}}
%     \hfill
%     \subfigure[Qualitative comparison of the high-resolution Cameraman image ($256 \times 256$).]
%         {\includegraphics[width=0.48\textwidth]{data/ablation/cameraman_images.pdf}
%         \label{fig:camera_256_256_vis}}
%     \caption{(a) Ablation study on the high-resolution Cameraman image ($256 \times 256$). (b) Qualitative comparison of the high-resolution Cameraman image ($256 \times 256$).}
%     \label{fig:ablation_2}
% \end{figure}









\subsection{Impact of Amplitude, Frequency, and Phase}
\cref{fig:component1} illustrates the PSNR (dB) over 500 iterations for different component combinations: \textbf{amplitude} ($C_i$'s), \textbf{frequency} ($\Omega_i$'s), \textbf{phase} ($\Phi_i$'s), and their interactions. The model leveraging all three components (freq + phase + amp) achieves the highest PSNR, significantly outperforming individual and partial combinations. This confirms the importance of integrating amplitude, frequency, and phase in the model design for optimal performance, and validates our initial design choices and mathematical analysis.

Additionally, this graph highlights the varying importance of the parameters in our model. Specifically, the amplitudes exhibit the highest significance, followed by the frequencies, with the phases contributing the least. These findings provide valuable guidance for parameter reduction in scenarios with limited training time or hardware resources, enabling more efficient model optimization.

\subsection{Comparative Analysis of Activation Strategies}
\Cref{fig:network_act_type} aligns with the described strategies in \hyperref[sec:implement_str]{Section 3.4} for implementing STAF's parametric activation functions. The per-neuron activation (\textcolor{green}{green} curve) achieves the highest PSNR, demonstrating superior expressiveness, but at the cost of a significant parameter increase, as expected. The network-wide activation (\textcolor{blue}{blue} curve) shows the weakest performance, reflecting limited expressiveness due to shared activation functions across the entire network. The layer-wise activation (\textcolor{orange}{orange} curve) offers a balanced trade-off, achieving nearly the same performance as per-neuron activation while requiring far fewer additional parameters (e.g., 225 parameters for a 3-layer MLP with 25 terms). This supports its use as an efficient and effective strategy, as highlighted in \hyperref[sec:implement_str]{Section 3.4}.

% \subsection{Ablation Study on High-Resolution Image Reconstruction}
% The ablation study evaluates the performance of various models on a high-resolution \texttt{Cameraman} image ($256 \times 256$). The PSNR plot shows that STAF outperforms other models such as SIREN, KAN, WIRE, and ReLU + P.E. across 300 training epochs (Figure \ref{fig:camera_256_256_val}). Qualitative results support these findings, with STAF achieving a PSNR of 93.47 dB, outperforming models like KAN (41.91 dB) and WIRE (21.67 dB) at epoch 500 (Figure \ref{fig:camera_256_256_vis}). These results demonstrate the effectiveness of STAF in high-resolution image reconstruction. 





\subsection{Performance Comparison of STAF and SIREN with Similar Parameter Counts}
\cref{fig:match_params} demonstrates the superior performance of STAF compared to SIREN in terms of PSNR (dB) across 250 epochs, despite SIREN having a higher parameter count. To ensure a balanced evaluation, the default configuration of SIREN was modified by adding one additional layer, resulting in 264,193 parameters for SIREN compared to STAF's 213,761 parameters. This approach avoids extensive parameter tuning for SIREN, offering a practical comparison between the two models. The results clearly show that STAF consistently outperforms SIREN, achieving significantly higher PSNR values throughout the training process. This highlights STAF’s efficiency and effectiveness, even when constrained to a lower parameter count, making it a more suitable choice for tasks requiring high-quality image reconstruction.


\subsection{More Comparative Evaluation}
Figure \ref{fig:hash} presents a comparative analysis of three methods—STAF, SIREN, and Hash Encoding \citep{mueller2022instant} —on the task of high-resolution image reconstruction. The PSNR (dB) curves indicate that STAF significantly outperforms both SIREN and Hash Encoding, reaching a PSNR of over 100 dB after 500 epochs. While Hash Encoding shows a notable improvement over SIREN, peaking at around 70 dB, it still falls short of STAF’s superior performance. SIREN, in contrast, exhibits the slowest PSNR growth, achieving only around 38 dB. The qualitative comparisons on the right further support these quantitative results, with STAF closely approximating the ground truth, while Hash Encoding and SIREN produce visibly lower-quality reconstructions. This analysis highlights the advantage of STAF in achieving both higher fidelity and faster convergence in image reconstruction tasks.

\section{Proofs}
\subsection{Proof of Theorem \eqref{initialization_theorem}}
\label{app:proof1}
In this section, we provide a step-by-step proof of Theorem \eqref{initialization_theorem} concerning the initialization scheme of an architecture that leverages STAF.
\begin{theorem} \label{main_th}
    Consider the following function $Z$
    \begin{equation} \label{definition_of_h}
    Z=\sum_{u=1}^{\tau}C_u\sin\left(\Omega_u\boldsymbol{w}.\boldsymbol{x}+\Phi_u\right)
    \end{equation}
    Suppose $C_u$'s are symmetric distributions, have finite moments, and are independent of $\Omega_u,\boldsymbol{w},\boldsymbol{x},\Phi_u$. Furthermore, for each $u$, $\Phi_u \sim U(-\pi,\pi)$. Then the moments of $Z$ will only depend on $\tau$ and the moments of $C_u$'s. Moreover, the odd-order moments of $Z$ will be zero.
\end{theorem}
\begin{proof}
    For convenience, let us consider $\Gamma_u=\Omega_u\boldsymbol{w}.\boldsymbol{x}$. Based on the multinomial theorem, for every natural number $q$, we have:

\[
Z^q = \sum_{\substack{i_1+\ldots+i_{\tau}=q \\ i_1, \ldots, i_{\tau}\geq 0}} \left[\binom{q}{i_1, \ldots , i_{\tau}}\prod_{u=1}^{{\tau}} \left(C_u \sin(\Gamma_u+\Phi_u)\right)^{i_u}\right].
\]


According to the linearity of expected value:
\begin{align} \label{h_powered_by_q}
    \mathbb{E}[Z^q] &= \sum_{\substack{i_1+\ldots+i_{\tau}=q \\ i_1, \ldots, i_{\tau}\geq 0}} \left[\binom{q}{i_1, \ldots, i_{\tau}}\mathbb{E}\left[\prod_{u=1}^{\tau} \left(C_u \sin(\Gamma_u+\Phi_u)\right)^{i_u}\right]\right] \nonumber \\
    &= \sum_{\substack{i_1+\ldots+i_{\tau}=q \\ i_1, \ldots, i_{\tau}\geq 0}} \left[\binom{q}{i_1, \ldots, i_{\tau}}\prod_{u=1}^{\tau} \left[\mathbb{E}[C_u^{i_u}] \mathbb{E}\left[\sin^{i_u}(\Gamma_u+\Phi_u)\right]\right]\right].
\end{align}


Each choice of $i_1, \ldots, i_{\tau}$ is called a partition for $q$. If $q$ is an odd number, then in each partition of $q$, at least one of the variables, such as $i_k$, is odd. Since the function $C_i$ is symmetric, it follows that $\mathbb{E}[C_k^{i_k}] = 0$. This is because odd-order moments of a symmetric distribution are always zero. Consequently, the expectation $\mathbb{E}\left[\prod_{u=1}^{\tau} \left(C_u \sin(\Gamma_u+\Phi_u)\right)^{i_u}\right]$ also equals zero, as does $\mathbb{E}[Z^q]$.

Now, let us consider the case when $q$ is even. For each partition of $q$, if at least one of its variables is odd, then, as before, we have $\mathbb{E}\left[\prod_{u=1}^{\tau} \left(C_u \sin(\Gamma_u+\Phi_u)\right)^{i_u}\right]=0$. Thus, we can express $q$ as $q=2j_1+ \ldots+2j_{\tau}$ where each $j_k$ is a non-negative integer. According to \eqref{h_powered_by_q}, to obtain the $i_k$-th moment of $Z$, we need to calculate $\mathbb{E}\left[\sin^{i_u}(\Gamma_u+\Phi_u)\right]$. In this case, where $i_u=2j_u$, $\sin^{i_u}\theta$ is an even function, and its Fourier series consists of a constant term and some cosine terms, given by


\begin{equation}
    \sin^{2j_u}\theta = \alpha_0 + \sum_{r=1}^{\infty} \alpha_r \cos(r\theta).
\end{equation}

Hence,
\begin{flalign}
&\mathbb{E}[\sin^{2j_u}(\Gamma_u+\Phi_u)] = \mathbb{E}[\alpha_0+\sum_{r=1}^{\infty}\alpha_r\cos(r(\Gamma_u+\Phi_u))] = \alpha_0+\sum_{r=1}^{\infty}\alpha_r \mathbb{E}[\cos(r\Gamma_u+r\Phi_u)] \nonumber \\
&= \alpha_0+\sum_{r=1}^{\infty}\alpha_r \mathbb{E}[\cos(r\Gamma_u) \cos(r\Phi_u) - \sin(r\Gamma_u) \sin(r\Phi_u)] = \alpha_0+\sum_{r=1}^{\infty}\alpha_r \Xi
\end{flalign}
where
\begin{equation}
    \Xi = \mathbb{E}[\cos(r\Gamma_u)]\mathbb{E}[\cos(r\Phi_u)] - \mathbb{E}[\sin(r\Gamma_u)]\mathbb{E}[\sin(r\Phi_u)].
\end{equation}

Since $r$ is an integer, $r\Phi_u$ will be a period, resulting in $\mathbb{E}[\cos(r\Phi_u)] = \mathbb{E}[\sin(r\Phi_u)] = 0$. Thus, $\mathbb{E}[\sin^{2j_u}(\Gamma_u+\Phi_u)] = \alpha_0$.

Using the formula for the coefficients of the Fourier series, we have:
\begin{equation}
    \alpha_0 = \frac{1}{\pi} \int_{-\pi/2}^{\pi/2} \sin^{2j_u} \theta \, d\theta = \frac{2}{\pi} \int_0^{\pi/2} \sin^{2j_u} \theta \, d\theta 
    \label{Wallis}
    = \frac{2}{\pi} \times \frac{\binom{2j_u}{j_u}}{2^{2j_u}} \times \frac{\pi}{2}
    = \frac{\binom{2j_u}{j_u}}{2^{2j_u}}
\end{equation}

% \begin{flalign}
% &\alpha_0= \frac{1}{\pi} \int_{-\pi/2}^{\pi/2} \sin^{2j_u} \theta \, d\theta \nonumber\\
% &= \frac{2}{\pi} \int_0^{\pi/2} \sin^{2j_u} \theta \, d\theta \label{Wallis} \\
% &= \frac{2}{\pi} \times \frac{{2j_u \choose j_u}}{2^{2j_u}} \times \frac{\pi}{2} \nonumber \\
% &= \frac{{2j_u \choose j_u}}{2^{2j_u}}
% \end{flalign}

where \eqref{Wallis} is evaluated using the Wallis integral.

To summarize,
\begin{flalign} \label{semi_final}
&\mathbb{E}[Z^q] = \sum_{\substack{j_1 + \dots + j_\tau = \frac{q}{2}, \\ j_1, \dots, j_\tau \geq 0}} \binom{q}{2j_1, \dots, 2j_\tau} \prod_{u=1}^\tau \mathbb{E}[C_u^{2j_u}] \frac{{\binom{2j_u}{j_u}}}{2^{2j_u}} \nonumber \\
&= \sum_{\substack{j_1 + \dots + j_\tau = \frac{q}{2}, \\ j_1, \dots, j_\tau \geq 0}} \left[\left(\binom{q}{2j_1, \dots, 2j_\tau}\prod_{u=1}^\tau\binom{2j_u}{j_u}\right)\prod_{u=1}^\tau \frac{1}{2^{2j_u}} \prod_{u=1}^\tau \mathbb{E}[C_u^{2j_u}]\right]
\end{flalign}
This also accounts for odd-order moments, as it is impossible to select a combination of non-negative integers that sums to a non-integer value.

It is worth noting that:
\begin{flalign} \label{simplification}
    \binom{q}{2j_1, \dots, 2j_\tau} \prod_{u=1}^\tau \binom{2j_u}{j_u} &= \frac{q!}{(2j_1)! \dots (2j_\tau)!} \times \frac{(2j_1)!}{(j_1)!^2} \times \dots \times \frac{(2j_\tau)!}{(j_\tau)!^2} = \frac{q!}{(j_1!)^2 \dots (j_\tau!)^2} \nonumber \\
    &= \binom{q}{j_1, j_1, \dots, j_\tau, j_\tau}
\end{flalign}
Furthermore,
\begin{equation} \label{sum_of_powers}
    \prod_{u=1}^\tau \frac{1}{2^{2j_u}} = \frac{1}{2^{2 \sum_{u=1}^\tau j_u}} = \frac{1}{2^q}
\end{equation}
By utilizing Equations \eqref{semi_final} to \eqref{sum_of_powers}, we can conclude that:
\begin{equation} \label{E_h_powered_by_q}
    \mathbb{E}[Z^q] = \frac{1}{2^q}\sum_{\substack{j_1 + \dots + j_\tau = \frac{q}{2} \\ j_1, \dots, j_\tau \geq 0}} \binom{q}{j_1, j_1, \dots, j_\tau, j_\tau} \prod_{u=1}^\tau \mathbb{E}[C_u^{2j_u}]
\end{equation}
As you can see, the moments of $Z$ depend solely on $\tau$ and the moments of the $C_u$'s.
\end{proof}

Now, our goal is to determine the distribution of the $C_u$'s so that the distribution of $Z$ becomes $\mathcal{N}(0,1)$. To achieve this, let's first consider the following theorem:
\begin{theorem}
    (Page 353 of \citep{Shiryaev2016-xv}) Let $X \sim \mathcal{N}(0,\sigma^2)$. Then
    \begin{equation} \label{q'th_moments_of_X}
        E(X^q) = \begin{cases}
			0, & \text{if $q$ is odd}\\
            \frac{q!}{\frac{q}{2}!~2^{q/2}}\sigma^q, & \text{if $q$ is even}
		 \end{cases}
    \end{equation}
    and these moments pertain exclusively to the normal distribution.
\end{theorem}

In theorem \eqref{main_th}, we proved that for odd values of $q$, $\mathbb{E}[h^q]=0$. Thus, in order to have $Z \sim \mathcal{N}(0,1)$, for even values of $q$, we must have $\mathbb{E}[h^q] = \frac{q!}{\frac{q}{2}!~2^{q/2}}.$ Alternatively, we can express it as
\begin{equation}
    \frac{1}{2^q}\sum_{\substack{j_1 + \dots + j_\tau = \frac{q}{2} \\ j_1, \dots, j_\tau \geq 0}} \binom{q}{j_1, j_1, \dots, j_\tau, j_\tau} \prod_{u=1}^\tau \mathbb{E}[C_u^{2j_u}] = \frac{q!}{\frac{q}{2}!~2^{q/2}}.
\end{equation}
Simplifying further, we obtain
\begin{equation}
    \frac{q!}{2^q} \sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \frac{\prod_{u=1}^\tau \mathbb{E}[C_u^{2j_u}]}{(j_1!)^2\dots(j_\tau!)^2} = \frac{q!}{\frac{q}{2}!~2^{q/2}}.
\end{equation}
This equation can be further simplified to
\begin{equation} \label{desired}
    \sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \frac{\prod_{u=1}^\tau \mathbb{E}[C_u^{2j_u}]}{(j_1!)^2\dots(j_\tau!)^2} = \frac{2^{q/2}}{\frac{q}{2}!}.
\end{equation}
Equation \eqref{desired} provides a general formula that can be utilized in further research. It allows for finding different solutions for $C_u$ under various assumptions (e.g., independence or specific dependencies) and different values of $\tau$. However, in the subsequent analysis, we assume that $C_u$'s are independent and identically distributed (i.i.d) random variables. The following theorem aims to satisfy Equation \eqref{desired}.
\begin{theorem} \label{finding_the_moments}
    Suppose $C_u$'s are i.i.d random variables with the following even-order moments:
    \begin{equation} \label{moments_of_C}
        \mathbb{E}[C_u^{2j}]=\left(\frac{2}{\tau}\right)^j j!
    \end{equation}
    Then, for every non-negative even number $q$, Equation \eqref{desired} holds.\footnote{If you wonder how this solution struck our mind, you can start by solving equation \eqref{desired} for $q=2$ to obtain $\mathbb{E}[h^2]$. Then, using the value of $\mathbb{E}[h^2]$, solve \eqref{desired} for $q=4$ to obtain $\mathbb{E}[h^4]$, and so on. }
\end{theorem}
\begin{proof}
    We begin by simplifying the expression:
\begin{flalign}
&\sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \frac{\prod_{u=1}^\tau \mathbb{E}[C_u^{2j_u}]}{(j_1!)^2\dots(j_\tau!)^2}=\sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \frac{\prod_{u=1}^\tau \left[\left(\frac{2}{\tau}\right)^j j!\right]}{(j_1!)^2\dots(j_\tau!)^2} \nonumber\\
&=\sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \left(\frac{2}{\tau}\right)^{\sum_{u=1}^\tau j_u}\left(\frac{1}{j_1!\dots j_\tau!}\right)=\sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \left(\frac{2}{\tau}\right)^{\frac{q}{2}}\left(\frac{1}{j_1!\dots j_\tau!}\right) \nonumber\\
&=\left(\frac{2}{\tau}\right)^{\frac{q}{2}} \sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \frac{1}{j_1!\dots j_\tau!}=\left(\frac{2}{\tau}\right)^{\frac{q}{2}} \frac{1}{(\frac{q}{2})!} \sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \frac{(\frac{q}{2})!}{j_1!\dots j_\tau!} \nonumber\\
&=\left(\frac{2}{\tau}\right)^{\frac{q}{2}} \frac{1}{(\frac{q}{2})!} \sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \binom{\frac{q}{2}}{j_1,\dots,j_\tau}
\end{flalign}
Based on the multinomial theorem, we can conclude that
\begin{equation}
    \left(\frac{2}{\tau}\right)^{\frac{q}{2}} \frac{1}{(\frac{q}{2})!} \sum_{\substack{j_1+\dots+j_\tau=\frac{q}{2}\\j_1,\dots,j_\tau\geq 0}} \binom{\frac{q}{2}}{j_1,\dots,j_\tau}=\left(\frac{2}{\tau}\right)^{\frac{q}{2}} \frac{\tau^\frac{q}{2}}{(\frac{q}{2})!}=\frac{2^{\frac{q}{2}}}{(\frac{q}{2})!}
\end{equation}
\end{proof}

Also note that according to Theorem \eqref{main_th}, the odd-order moments of $Z$ are zero, just like a normal distribution.

\begin{corollary}
    Let $Z$ be the random variable defined in \eqref{definition_of_h}. Additionally, assume that the $C_u$'s ($1 \leq u \leq \tau$) used in the definition of $Z$, are i.i.d  random variables with even moments as defined in theorem \eqref{finding_the_moments}.
    Then $Z\sim \mathcal{N}(0,1)$.
\end{corollary}
\begin{proof}
    We know that if the MGF of a distribution exists, then the moments of that distribution can uniquely determine its PDF. That is, if $X$ and $Y$ are two distributions and for every natural number $k$, $E(X^k)=E(Y^k)$, then $X=Y$.

    In the Theorem \eqref{finding_the_moments}, we observed that the moments of $Z$ are equal to the moments of a standard normal distribution. Since the MGF of this distribution exists, $Z\sim \mathcal{N}(0,1)$.
\end{proof}

Now, let's explore which distribution can produce the moments defined in equation \eqref{moments_of_C}. To have an inspiration, note that for a centered Laplace random variable $X$ with scale parameter $b$, we have the PDF of $X$ as
\begin{equation}
    f_X(x)=\frac{1}{2b}e^\frac{-|x|}{b}
\end{equation}
and the moments of $X$ given by
\begin{equation}
    \mathbb{E}[X^q] = \begin{cases}
			0, & \text{if $q$ is odd}\\
            \frac{b^q}{q!}, & \text{if $q$ is even}
		 \end{cases}
\end{equation}
Hence, the answer might be similar to this distribution. If we assume $Y=sgn(X)\sqrt{|X|}$, since $Y$ is symmetric, all of its odd-order moments are zero. Now, let us calculate its even-order moments:
\begin{equation}
    \mathbb{E}[Y^{2q}] = \mathbb{E}[|X|^q] = \int_{-\infty}^{\infty} |x|^q \frac{1}{2b} e^{-\frac{|x|}{b}} dx = 2\int_{0}^{\infty} |x|^q \frac{1}{2b} e^{-\frac{|x|}{b}} dx = \frac{1}{b} \int_{0}^{\infty} x^q e^{-\frac{x}{b}} dx
\end{equation}
By assuming $u=\frac{x}{b}$, we will have
\begin{equation} \label{even_moments_of_Y}
    \mathbb{E}[Y^{2q}] = \int_{0}^{\infty} (bu)^q e^{-u} du = b^q \int_{0}^{\infty} u^q e^{-u} du = b^q \Gamma(q+1) = b^q q!
\end{equation}
By assuming $b=\frac{2}{\tau}$, \eqref{moments_of_C} will be obtained.

The next theorem will obtain the probability distribution function of $Y$.

\begin{theorem} \label{final_pdf}
    Let $X$ be a centered Laplace random variable with scale parameter $b$, and $Y=sgn(X)\sqrt{|X|}$. Then
    \begin{equation}
        f_Y(y)=\frac{|y|}{b}e^\frac{-y^2}{b}
    \end{equation}
\end{theorem}
\begin{proof}
        Let $A=Y^2=|X|$. Therefore,
    \begin{equation}
        M_{A}(t)=\sum_{k=0}^{\infty}\frac{t^k\mathbb{E}[|X|^k]}{k!}
    \end{equation}
    As we calculated in \eqref{even_moments_of_Y},
    $\mathbb{E}[|X|^k]=b^k~k!$. Therefore,
    \begin{equation}
        M_{A}(t)=\sum_{k=0}^{\infty}\frac{t^k \cdot b^k~k}{k!}=\sum_{k=0}^{\infty}(bt)^k=\frac{1}{1-bt}=\frac{\frac{1}{b}}{\frac{1}{b}-t}
    \end{equation}
    that is the MGF of exponential distribution with parameter $\frac{1}{b}$. That is,
    \begin{equation}
        f_A(a)=\frac{1}{b}e^{\frac{-a}{b}}
    \end{equation}
    Therefore, using the fact that $A$ is a always non-negative, we consider non-negative values $a^2$ to describe its cumulative distribution function.
    \begin{equation} \label{exp_cdf}
        F_A(y^2)=\mathbb{P}(A\leq y^2)=1-e^{\frac{-y^2}{b}}
    \end{equation}
    On the other hand, if $y \geq 0$,
    \begin{equation}
       \mathbb{P}(A\leq y^2)=\mathbb{P}(Y^2 \leq y^2) = \mathbb{P}(-y \leq Y \leq y)
    \end{equation}
    Since we want $Y$ to be symmetric, we assume\footnote{In fact, the assumption that $Y$ is symmetric is not unexpected, since all odd-order moments of $Y$ are zero. But there are some non-symmetrical distributions whose all odd-order moments are zero \citep{Churchill1946-in}. Nevertheless, under some assumptions, it can be shown that a distribution is symmetric if and only if all its odd-order moments are zero. However, we don't use this claim in this paper.}
    \begin{flalign} \label{symmetry_assumption}
        &\mathbb{P}(-y \leq Y \leq y) = 2~\mathbb{P}(0 \leq Y \leq y) =2~(\mathbb{P}(Y \leq y) - \frac{1}{2}) = 2F_Y(y) - 1, \quad y\geq 0
  \end{flalign}
  Using equations \eqref{exp_cdf} to \eqref{symmetry_assumption}, we draw conclusion that
  \begin{equation} \label{CDF_equality}
      2F_Y(y) - 1 = 1-e^{\frac{-y^2}{b}},\quad y\geq 0
  \end{equation}
  By differentiating both sides of \eqref{CDF_equality} with respect to $y$, we will have
  \begin{equation}
      2f_Y(y) = \frac{2y}{b}e^{\frac{-y^2}{b}},\quad y\geq 0
  \end{equation}
  Therefore,
  \begin{equation}
      f_Y(y) = \frac{y}{b}e^{\frac{-y^2}{b}},\quad y\geq 0
  \end{equation}
  Since we assumed $y \geq 0$ in the above equations, and we supposed that $Y$ is symmetric,
  \begin{equation}
      f_Y(y) = \frac{|y|}{b}e^{\frac{-y^2}{b}},\quad y\in\mathbb{R}
  \end{equation}
  Just to make sure that our assumption about the symmetry of $Y$ was correct (or sufficed for our purpose), let us check the even-order moments of $Y$. The odd-orders are zero based on the symmetry.
  \begin{equation}
      \mathbb{E}[Y^{2k}] = \int_{-\infty}^{\infty} y^{2k} \left(\frac{|y|}{b} e^{-\frac{y^2}{b}}\right) dy = \frac{2}{b} \int_{0}^{\infty} y^{2k+1} e^{-\frac{y^2}{b}} dy
  \end{equation}
  Setting $y^2=t$ and $\frac{1}{b}=s$, leads to the following equation:
  \begin{equation}
      \mathbb{E}[Y^{2k}] = \frac{1}{b} \int_{0}^{\infty} t^k e^{-st} dt
  \end{equation}
  That is the Laplace transform of $t^k$. Therefore,
  \begin{equation}
      \mathbb{E}[Y^{2k}] = s\frac{\Gamma(k+1)}{s^{k+1}} = \frac{k!}{s^k} = b^k k!
  \end{equation}
\end{proof}


In summary, in this section we calculated the initial coefficients of our activation function as described in Theorem \eqref{final_pdf}, where we set $b=\frac{2}{\tau}$. Consequently, if we denote the post-activation of layer $l$ by $\boldsymbol{z^{(l)}}$, we will have $z^{(l)}_i\sim\mathcal{N}(0,1)$ for all $l\in\{2,3,\ldots,L-1\}$, and $i\in\{1,\ldots F_l\}$. This result can be proved by induction on $l$, using the fact that, based on the theorems in this section, the PDF of $Z$ is independent of the PDF of $\boldsymbol{x}$.
% \subsection{Proof of Theorem \eqref{main_th}}
% \label{proof1}


% \subsection{Proof of Theorem \eqref{finding_the_moments}}
% \label{proof3}


% \subsection{Proof of Theorem \eqref{final_pdf}}
% \label{proof5}

\subsection{Proof of Theorem \eqref{Kronecker_theorem}}
\label{app:proof_Kronecker_theorem}
Before proving the theorem, note the following remark:

\begin{remark}
    Let $X$ be a $\chi_1 \times \chi_2$ matrix, and $Y$ be a $\gamma_1 \times \gamma_2$ matrix. Then, according to \citep{Ashendorf2014-Supplement, Albrecht2023ProductKA}:
\begin{equation} \label{Kronecker_product_indices}
    (X\otimes Y)_{i,j} = x_{\lceil i/\gamma_1\rceil,~\lceil i/\gamma_2\rceil}~y_{(i-1)\%\gamma_1+1,~(j-1)\%\gamma_2+1}.
\end{equation}
\end{remark}

Now, let us consider each pair of layers as a block, where the first two layers form the first block, the second two layers form the second block, and so on. We prove the theorem by induction on the block numbers. The proof consists of three parts:

\textbf{Part 1)} Consider the weight matrix and bias vector given by:
\begin{equation}
	\boldsymbol{\overline{W^{(l)}}}=\boldsymbol{\Omega}\otimes \boldsymbol{W^{(l)}},\quad\quad \boldsymbol{\overline{B^{(l)}}} = \boldsymbol{\Phi}\otimes \boldsymbol{J}_{F_l,1}.
\end{equation}
We then define
\begin{equation}
	\begin{bmatrix}
		\overline{a^{(l)}_1} & \overline{a^{(l)}_2} & \ldots & \overline{a^{(l)}_{\tau F_l}}
	\end{bmatrix}^{tr}
	= \boldsymbol{\overline{W^{(l)}}}\boldsymbol{z^{(l-1)}}+\boldsymbol{\overline{B^{(l)}}},
\end{equation}
and
\begin{equation}
	\overline{z^{(l)}_p} = \rho(\overline{a^{(l)}_p})\quad\forall ~p\in\{1,2,\ldots,\tau F_l\}.
\end{equation}
Additionally, define
\begin{equation}  \label{next_layer_preactivation_1}
    \Tilde{a}^{(l+1)} = \left(\boldsymbol{C}^{tr}\otimes W^{(l+1)}_{i,:}\right)\overline{z^{(l)}},
\end{equation}
where $W^{(l+1)}_{i,:}$ denotes the $i$'th row of $W^{(l+1)}$. Then, we can observe that
\begin{equation} \label{next_layer_preactivation_2}
	\Tilde{a}^{(l+1)} = a^{(l+1)}_i
\end{equation}

\begin{proof}
	First, let us calculate $a^{(l+1)}_{i}$ using activation function $\rho^*$. Note that $a^{(l+1)}=\boldsymbol{W^{(l+1)}}\boldsymbol{z^{(l)}}$. Therefore, $a^{(l+1)}_i=\boldsymbol{W^{(l+1)}}_{i,:}\boldsymbol{z^{(l)}}$. It implies that
	\begin{align} \label{a^{(l+1)}_i}
		a^{(l+1)}_{i} &=\sum_{j=1}^{F_l}W^{(l+1)}_{i,j}z^{(l)}_j =\sum_{j=1}^{F_l}W^{(l+1)}_{i,j}\rho^*\left(a^{(l)}_j\right)=\sum_{j=1}^{F_l}W^{(l+1)}_{i,j}\rho^*\left(\sum_{k=1}^{F_{l-1}}W^{(l)}_{j,k}z^{(l-1)}_k\right) \nonumber \\
		&=\sum_{j=1}^{F_l}W^{(l+1)}_{i,j}\sum_{m=1}^{\tau}\boldsymbol{C}_m\rho\left(\boldsymbol{\Omega}_m\sum_{k=1}^{F_{l-1}}W^{(l)}_{j,k}z^{(l-1)}_k+\boldsymbol{\Phi}_m\right)
	\end{align}
	
	Next, let us calculate $\Tilde{a}^{(l+1)}$. We have
	\begin{align}
		\overline{a^{(l)}_p}&=\left[\overline{W^{(l)}}z^{(l-1)}+\boldsymbol{\overline{B^{(l)}}}\right]_p = \boldsymbol{\overline{W^{(l)}}}_{p,:}\boldsymbol{z^{(l-1)}}+\boldsymbol{\overline{B^{(l)}}}_p = \sum_{k=1}^{F_{l-1}}\left(\boldsymbol{\overline{W^{(l)}}}_{p,k}\boldsymbol{z^{(l-1)}}_k\right) +\boldsymbol{\overline{B^{(l)}}}_p \nonumber \\
  &= \sum_{k=1}^{F_{l-1}}\left(\boldsymbol{\Omega}_{\lceil p/F_l\rceil,\lceil k/F_{l-1}\rceil}W^{(l)}_{1+(p-1)\%F_l,1+(k-1)\%F_{l-1}}z^{(l-1)}_k\right)+\boldsymbol{\Phi}_{\lceil p/F_l\rceil} \label{ceil_usage}
	\end{align}
    Equation \eqref{ceil_usage} is based on equation \eqref{Kronecker_product_indices}. Since $1\leq k\leq F_{l-1}$, it follows that $\lceil k/F_{l-1}\rceil=1$ and $(k-1)\%F_{l-1}=k-1$. As a result,
 \begin{align}
     \overline{a^{(l)}_p}&=\sum_{k=1}^{F_{l-1}}\left(\boldsymbol{\Omega}_{\lceil p/F_l\rceil}W^{(l)}_{1+(p-1)\%F_l,k}z^{(l-1)}_k\right)+\boldsymbol{\Phi}_{\lceil p/F_l\rceil} \nonumber \\
     &=\boldsymbol{\Omega}_{\lceil p/F_l\rceil}\sum_{k=1}^{F_{l-1}}\left(W^{(l)}_{1+(p-1)\%F_l,k}z^{(l-1)}_k\right)+\boldsymbol{\Phi}_{\lceil p/F_l\rceil}
 \end{align}
    Therefore,
	\begin{equation} \label{z_p^{(l)}}
    \overline{z^{(l)}_p}=\rho\left(\boldsymbol{\Omega}_{\lceil p/F_l\rceil}\sum_{k=1}^{F_{l-1}}\left(W^{(l)}_{1+(p-1)\%F_l,k}z^{(l-1)}_k\right)+\boldsymbol{\Phi}_{\lceil p/F_l\rceil}\right)
  \end{equation}
	Consequently,
	\begin{align}
		\Tilde{a}^{(l+1)} &= \sum_{p=1}^{\tau F_l}\left[\boldsymbol{C}^{tr}\otimes W^{(l+1)}_{i,:}\right]_{1,p}\overline{z^{(l)}_p} = \sum_{p=1}^{\tau F_l}\boldsymbol{C}^{tr}_{1,\lceil p/F_l\rceil}W^{(l+1)}_{i,1+(p-1)\%F_l} \overline{z^{(l)}_p} \nonumber \\
        &= \sum_{p=1}^{\tau F_l}\boldsymbol{C}_{\lceil p/F_l\rceil}W^{(l+1)}_{i,1+(p-1)\%F_l} \overline{z^{(l)}_p} \label{single_sum} \\ &=\sum_{j=1}^{F_l}\sum_{m=1}^{\tau}\boldsymbol{W}^{(l+1)}_{i,j}\boldsymbol{C}_m\overline{z^{(1)}_{F_l(m-1)+j}} \label{double_sum}
	\end{align}
	Equation \eqref{double_sum} is obtained as follows: by changing the indices of $\boldsymbol{W}$ and $\boldsymbol{C}$ from equation \eqref{single_sum} to \eqref{double_sum}, we need to change the index of $z^{(l)}$ too. To this end, note that
	\begin{equation} \label{changing_vars}
		m=\lceil p/F_l\rceil, \quad j=1+(p-1)\%F_{l}
	\end{equation}
	If $F_{l}\nmid p$, then $m=1+\lfloor p/F_l \rfloor$. As we know, $p=F_l\lfloor p/F_l \rfloor+p\%F_l$. Therefore, $p=F_l(m-1)+j$. This equation also holds when $F_l\mid p$.
	
	Equation \eqref{double_sum} can be rewritten as follows:
	\begin{align}
		&\sum_{j=1}^{F_l}\boldsymbol{W}^{(l+1)}_{i,j}\sum_{m=1}^{\tau}\boldsymbol{C}_m \overline{z^{(l)}_{F_l(m-1)+j}}
	\end{align}
    where, according to equations \eqref{z_p^{(l)}} and \eqref{changing_vars},
    \begin{equation}
        \overline{z^{(l)}_{F_l(m-1)+j}}=\rho\left(\boldsymbol{\Omega}_m\sum_{k=1}^{F_{l-1}}\left(W^{(l)}_{j,k}z^{(l-1)}_k\right)+\boldsymbol{\Phi}_m\right)
    \end{equation}
    Hence,
    \begin{align}
        \Tilde{a}^{(l+1)}= \sum_{j=1}^{F_l}\boldsymbol{W}^{(l+1)}_{i,j}\sum_{m=1}^{\tau}\boldsymbol{C}_m \rho\left(\boldsymbol{\Omega}_m\sum_{k=1}^{F_{l-1}}\left(W^{(l)}_{j,k}z^{(l-1)}_k\right)+\boldsymbol{\Phi}_m\right)
    \end{align}
    which is equal to $a^{(l+1)}_i$ based on \eqref{a^{(l+1)}_i}.
\end{proof}

\textbf{Part 2)} Let $\boldsymbol{\overline{B^{(l+1)}}}=\boldsymbol{\Phi}\otimes \boldsymbol{J}_{F_{l+1},1}$. We can define $\overline{a^{(l+1)}}$ as follows:
\begin{equation} \label{vector_form}
	\begin{bmatrix}
		\overline{a^{(l+1)}_1} & \overline{a^{(l+1)}_2} & \ldots & \overline{a^{(l+1)}_{\tau(F_{l+1})}}
	\end{bmatrix}^{tr}
	= \boldsymbol{\Omega}\otimes\boldsymbol{a^{(l+1)}} + \boldsymbol{\overline{B^{(l+1)}}}.
\end{equation}

Therefore, using \Cref{next_layer_preactivation_1,next_layer_preactivation_2,vector_form}, we can write
\begin{equation}
	\overline{a^{(l+1)}} = \overline{W^{(l+1)}}~\overline{z^{(l)}}+\boldsymbol{\overline{B^{(l+1)}}}
\end{equation},
where
\begin{equation}
	\overline{W^{(l+1)}}=\boldsymbol{\Omega}\otimes\left(\boldsymbol{C}^{tr}\otimes W^{(l+1)}\right)=\left(\boldsymbol{\Omega}\otimes\boldsymbol{C}^{tr}\right)\otimes W^{(l+1)}.
\end{equation}

Moreover, if we define
\begin{equation}
	\overline{z^{(l+1)}_q} = \rho\left(\overline{a^{(l+1)}_q}\right)\quad\forall~q\in\{1,\ldots,\tau(F_{l+1})\},
\end{equation}
we can observe that
\begin{equation} \label{final}
    {\boldsymbol{z^{(l+1)}}}=\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{l+1}}\right)\overline{\boldsymbol{z^{(l+1)}}}.
\end{equation}

\begin{proof}
    We know that
    \begin{equation}
        z^{(l+1)}_i=\rho^*(a^{(l+1)}_i)=\sum_{n=1}^{\tau}\rho\left(\boldsymbol{\Omega}_n a^{(l+1)}_i +\boldsymbol{\Phi}_n\right).
    \end{equation}
    Now, let us calculate each entry of the RHS of Equation \eqref{final}
    \begin{equation} \label{ith_entry}
        \left[\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{l+1}}\right)\overline{\boldsymbol{z^{(l+1)}}}\right]_i = \left[\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{l+1}}\right]_i\overline{\boldsymbol{z^{(l+1)}}} = \sum_{j=1}^{\tau F_{l+1}}\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{l+1}}\right)_{i,j}\overline{\boldsymbol{z^{(l+1)}}_j}.
    \end{equation}
    Hence, according to \eqref{Kronecker_product_indices}, we have
    \begin{equation}
        \left[\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{l+1}}\right)\overline{\boldsymbol{z^{(l+1)}}}\right]_i = \sum_{j=1}^{\tau F_{l+1}}\boldsymbol{C}^{tr}_{\lceil i/F_{l+1} \rceil,\lceil j/F_{l+1}\rceil}\delta_{1+(i-1)\%F_{l+1},1+(j-1)\%F_{l+1}}\overline{\boldsymbol{z^{(l+1)}}_j},
    \end{equation}
    in which $\delta$ refers to Kronecker delta. As a result,
    \begin{equation}
        \left[\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{l+1}}\right)\overline{\boldsymbol{z^{(l+1)}}}\right]_i = \sum_{j=1}^{\tau F_{l+1}}\boldsymbol{C}_{\lceil j/F_{l+1} \rceil,\lceil i/F_{l+1}\rceil}\delta_{1+(i-1)\%F_{l+1},1+(j-1)\%F_{l+1}}\overline{\boldsymbol{z^{(l+1)}}_j}
    \end{equation}
    Note that $1 \leq i\leq F_{l+1}$. Therefore, $\lceil i/F_{l+1}\rceil=1$, and $(i-1)\%F_{l+1}=i-1$. Hence,
    \begin{equation}
        \left[\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{l+1}}\right)\overline{\boldsymbol{z^{(l+1)}}}\right]_i = \sum_{j=1}^{\tau F_{l+1}}\boldsymbol{C}_{\lceil j/F_{l+1} \rceil}\delta_{i,1+(j-1)\%F_{l+1}}\overline{\boldsymbol{z^{(l+1)}}_j}.
    \end{equation}
    Also note that $\delta_{i,1+(j-1)\%F_{l+1}}$ is zero, except when $j=kF_{l+1}+i$, in which case $\delta_{i,1+(j-1)\%F_{l+1}}=1$. Thus,
    \begin{align} \label{RHS}
        &\left[\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{l+1}}\right)\overline{\boldsymbol{z^{(l+1)}}}\right]_i = \sum_{k=0}^{\tau-1}\boldsymbol{C}_{\lceil\left(kF_{l+1}+i\right)/F_{l+1} \rceil}\overline{\boldsymbol{z^{(l+1)}}_{kF_{l+1}+i}} = \sum_{k=0}^{\tau-1}\boldsymbol{C}_{k+\lceil i/F_{l+1} \rceil}\overline{\boldsymbol{z^{(l+1)}}_{kF_{l+1}+i}} \nonumber \\
        &= \sum_{k=0}^{\tau-1}\boldsymbol{C}_{k+1}\overline{\boldsymbol{z^{(l+1)}}_{kF_{l+1}+i}} = \sum_{n=1}^{\tau}\boldsymbol{C}_{n}\overline{\boldsymbol{z^{(l+1)}}_{(n-1)F_{l+1}+i}} = \sum_{n=1}^{\tau}\boldsymbol{C}_{n}\rho\left(\overline{\boldsymbol{a^{(l+1)}}_{(n-1)F_{l+1}+i}}\right).
    \end{align}

    Note that
    \begin{align}
        \overline{\boldsymbol{a^{(l+1)}}_{(n-1)F_{l+1}+i}} &=\boldsymbol{\Omega}_{\lceil\left((n-1)F_{l+1}+i\right)/F_{l+1}\rceil}\boldsymbol{a^{(l+1)}}_{1+\left((n-1)F_{l+1}+i-1\right)\%F_{l+1}}+\boldsymbol{\Phi}_{\lceil\left((n-1)F_{l+1}+i\right)/F_{l+1}\rceil} \nonumber\\
        &=\boldsymbol{\Omega}_{n-1+\lceil i/F_{l+1}\rceil}\boldsymbol{a^{(l+1)}}_{1+\left(i-1\right)\%F_{l+1}}+\boldsymbol{\Phi}_{n-1+\lceil i/F_{l+1}\rceil}
    \end{align}
    Since $\left\lceil\frac{i}{F_{l+1}}\right\rceil = 1$ and $(i-1)\%F_{l+1} = i-1$, we have
    \begin{equation} \label{overline{boldsymbol{a^{(l+1)}}_{(n-1)F_{l+1}+i}}}
        \overline{\boldsymbol{a^{(l+1)}}_{(n-1)F_{l+1}+i}}=\boldsymbol{\Omega}_n\boldsymbol{a^{(l+1)}}_i+\boldsymbol{\Phi}_n
    \end{equation}
    Finally, utilizing Equations \eqref{RHS} and \eqref{overline{boldsymbol{a^{(l+1)}}_{(n-1)F_{l+1}+i}}}, we deduce that
    \begin{equation}
        \left[\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{l+1}}\right)\overline{\boldsymbol{z^{(l+1)}}}\right]_i = \sum_{n=1}^{\tau}\boldsymbol{C}_{n}\rho\left(\boldsymbol{\Omega}_n\boldsymbol{a^{(l+1)}}_i+\boldsymbol{\Phi}_n\right),
    \end{equation}
    which is equal to the RHS of the Equation \eqref{final}.

    \textbf{Part 3)} Using parts 1 and 2 of the proof, we can state the theorem for arbitrary even values of $L$. By setting $l = 1$ in the previous parts, we obtain
    \begin{equation}
        \boldsymbol{\overline{{W}^{(1)}}} = \boldsymbol{\Omega}\otimes\boldsymbol{W}^{(1)}, \quad \boldsymbol{\overline{{B}^{(1)}}}=\boldsymbol{\Phi}\otimes \boldsymbol{J}_{F_1,1}
    \end{equation}
    and
    \begin{equation}
        \boldsymbol{\overline{{W}^{(2)}}} = \left(\boldsymbol{\Omega}\otimes\boldsymbol{C}^{tr}\right)\otimes\boldsymbol{W}^{(2)},\quad \boldsymbol{\overline{{B}^{(2)}}}=\boldsymbol{\Phi}\otimes \boldsymbol{J}_{F_2,1}.
    \end{equation}
    Thus,
    \begin{equation}
        \overline{\boldsymbol{W}^{(l)}} = \begin{cases}
			\boldsymbol{\Omega}\otimes\boldsymbol{W}^{(l)}, & \text{if $l=1$}\\
            \left(\boldsymbol{\Omega}\otimes\boldsymbol{C}^{tr}\right)\otimes\boldsymbol{W}^{(l)}, & \text{if $l=2$}
		 \end{cases},\quad \boldsymbol{\overline{{B}^{(l)}}}=\boldsymbol{\Phi}\otimes \boldsymbol{J}_{F_l,1}.
    \end{equation}
    In addition, by setting $L=2$, we will have $\overline{f}_{\overline{\theta}}(\boldsymbol{r})=\overline{\boldsymbol{W}^{(3)}}~\overline{\boldsymbol{z}^{(2)}}$. Note that according to the assumptions of the theorem, $\overline{\boldsymbol{W}^{(3)}}=\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_2}$. As a result, $\overline{f}_{\overline{\theta}}(\boldsymbol{r})=\overline{\boldsymbol{W}^{(3)}}~\overline{\boldsymbol{z}^{(2)}}=\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_2}\right)\overline{\boldsymbol{z}^{(2)}}$, which is equal to $\boldsymbol{z}^{(2)}=f_\theta(\boldsymbol{r})$, as derived in \eqref{final}. \eqref{final}. In conclusion, the theorem holds true for $L=2$.
    
    Now, suppose that Equation \eqref{weights_and_biases} holds for $L=2k$. Consequently,
    \begin{equation} \label{induction_h}
        \boldsymbol{z^{(2k)}}=\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{2k}}\right)\overline{\boldsymbol{z^{(2k)}}}
    \end{equation}
    Now, we aim to analyze the case for $L=2(k+1)$. For this network with two additional layers, we first need to adjust the weight matrix for layer $l=2k+1$. The new weight matrix will be
    \begin{equation}
        \overline{\boldsymbol{W}^{(2k+1)}} = \left(\boldsymbol{\Omega}\otimes\boldsymbol{W}^{(2k+1)}\right)\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{2k}}\right),
    \end{equation}
    and the weights and the biases of the two new layers will be
    \begin{align}
        \overline{\boldsymbol{W}^{(2k+2)}} = \left(\boldsymbol{\Omega}\otimes\boldsymbol{C}^{tr}\right)\otimes\boldsymbol{W}^{(2k+2)},\quad \boldsymbol{\overline{B}^{(2k+2)}}=\boldsymbol{\Phi}\otimes \boldsymbol{J}_{F_{2k+2},1},\nonumber \\
        \overline{\boldsymbol{W}^{(2k+3)}} = \boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{2k+2}},\quad \boldsymbol{\overline{B}^{(2k+3)}}=\boldsymbol{\Phi}\otimes \boldsymbol{J}_{F_{2k+3},1}.
    \end{align}

    Now, note that
    \begin{equation}
        \overline{\boldsymbol{W}^{(2k+1)}}~\overline{\boldsymbol{z}^{(2k)}} = \left(\boldsymbol{\Omega}\otimes\boldsymbol{W}^{(2k+1)}\right)\left(\boldsymbol{C}^{tr}\otimes\boldsymbol{I}_{F_{2k}}\right)\overline{\boldsymbol{z}^{(2k)}}.
    \end{equation}
    Therefore, by setting $l=2k-1$ in Equation \eqref{final}, or using Equation \eqref{induction_h}, we obtain
    \begin{equation}
        \overline{\boldsymbol{W}^{(2k+1)}}~\overline{\boldsymbol{z}^{(2k)}} = \left(\boldsymbol{\Omega}\otimes\boldsymbol{W}^{(2k+1)}\right)\boldsymbol{z}^{(2k)}
    \end{equation}
    This is analogous to feeding $\boldsymbol{z}^{(2k)}$ into a neural network whose first layer has the weight matrix $\boldsymbol{\Omega}\otimes\boldsymbol{W}^{(2k+1)}$.  Since the additional weight matrices and biases are consistent with Parts 1 and 2 of the proof, we can conclude that
    \begin{equation}
        \overline{f}_{\overline{\theta}}(\boldsymbol{r})=\boldsymbol{z}^{(2k+2)}=f_\theta(\boldsymbol{r}).
    \end{equation}
\end{proof}

\subsection{Proof of Lemma \eqref{rational_elusive}}
\label{app:proof_rational_elusive}
\begin{proof}
    Let $[a_{r,1},a_{r,2},\ldots,a_{r,T}]\in\mathbb{Q}^T$ be the $r$'th row of $\boldsymbol{\Psi}^{tr}$. Now, define a matrix $\boldsymbol{\hat{A}}$ which is identical to $\boldsymbol{A}$ except for its $r$'th row. This modified row is constructed as follows:
\begin{equation}
    \hat{a}_{r,i} = \frac{\sqrt{p_i}}{10^{-\eta}\lfloor 10^\eta\sqrt{p_i}\rfloor}\left(\psi_{r,i}+\epsilon[\psi_{r,i}=0]\right)
\end{equation}
in which $p_i$ is the $i$'th prime number, $\epsilon$ is the machine precision, $[.]$ is Iverson bracket, and $\eta$ is a large enough natural number such that $\frac{\sqrt{p_i}}{10^{-\eta}\lfloor 10^\eta\sqrt{p_i}\rfloor}\approx 1$ (to avoid significant changes in the matrix). At the same time, we must have $|\frac{\sqrt{p_i}}{10^{-\eta}\lfloor 10^\eta\sqrt{p_i}\rfloor}-1|\geq \epsilon$ (to prevent it from becoming a rational number).

Let $\alpha_i:=\frac{\hat{a}_{r,i}}{\sqrt{p_i}}$. Then, $\alpha_i\in\mathbb{Q}\setminus\{0\}$. Now assume that there is $S=[s_1,...,s_T]^{tr}\in Ker(\boldsymbol{\hat{A}})\cap \mathbb{Q}^{T}$. Consequently,
\begin{equation}
    \sum_{i=1}^{T}\hat{a}_{r,i}s_i=0
\end{equation}
As a result,
\begin{equation}
    \sum_{i=1}^{T}\alpha_i\sqrt{p_i}s_i=0
\end{equation}
Note that $\alpha_i s_i\in\mathbb{Q}$. Furthermore, The square roots of all prime numbers are linearly independent over $\mathbb{Q}$ \citep{stewart2022galois}. As a result, $\alpha_i s_i=0$ for all $i$. Since $\alpha_i\neq 0$, we must have $s_i=0$ for all $i$, that is, $Ker(\boldsymbol{\hat{A}})\cap \boldsymbol{Q}^T=\boldsymbol{O}$.\footnote{Note that all algebraic numbers are computable. This analysis was founded on the computability and expressibility of the square roots of prime numbers in a machine. However, most of the computable numbers are rounded or truncated when stored in a machine. Nevertheless,  it is possible to demonstrate theoretically or through simulation that increasing precision can make the aforementioned analysis always feasible.}
\end{proof}


% \section{Discussion} While the main focus of this paper is the introduction and theoretical justification of STAF, our experimental results substantiate its practical efficacy. STAF is less sensitive to weight initialization compared to SIREN, though hyperparameter tuning is still required for different tasks. This requirement could be viewed as a limitation; however, our primary emphasis remains on the theoretical analysis. Overall, STAF demonstrates a significant improvement in image reconstruction tasks, both in terms of convergence speed and reconstruction quality, making it a significant contribution to the toolkit for implicit neural representation in computer graphics and related fields.
