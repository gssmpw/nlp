\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}} % Disable adding to ToC
% \vspace{-1.5em}
\section{Introduction}
% \vspace{-0.5em}
\label{sec:intro}

%% Summarized
INRs have transformed signal processing and computer vision by shifting from discrete grid-based methods to continuous data mapping using neural networks, particularly Multilayer Perceptrons (MLPs). This approach enables the processing of diverse data types and complex relationships, driving advancements in computer graphics and computational photography \citep{mildenhall2020nerf, Siren, tancik2020fourier}. INRs have been instrumental in novel view synthesis, 3D reconstruction, and tackling high-dimensional data challenges, such as rendering complex shapes and modeling light interactions \citep{mildenhall2020nerf, Siren, chen2021learning, mescheder2019occupancy, saragadam2022miner}. Despite their versatility, traditional INR architectures—particularly ReLU-based networks—face limitations due to spectral bias, which hinders the reconstruction of fine details \citep{rahaman2019spectral}.

% Activation functions have evolved significantly, with early functions like sigmoid and step being replaced by unbounded functions like ReLU to address issues like vanishing gradients \citep{gustineli2022survey, hu2018overcoming, nair2010rectified, maas2013rectifier, elfwing2018sigmoid, hendrycks2016gaussian}. Recent advances include adaptive functions like SinLU, TanhSoft, and Swish \citep{paul2022sinlu, biswas2021tanhsoft, ramachandran2017searching}, which offer enhanced adaptability but have not fully realized their potential in neural representation tasks.

% To address these issues, we propose \textbf{S}inusoidal \textbf{T}rainable \textbf{A}ctivation \textbf{F}unction (\textbf{STAF}), a novel family of parametric, trainable activation functions to enhance INRs' capability in modeling and reconstructing complex signals. STAF bridges this gap by introducing a novel parametric periodic activation function with a new initialization scheme designed for MLP-based INR structures, thereby improving convergence speed and the capture of fine details in signals. This development addresses the challenges identified in earlier works regarding training networks with periodic activations \citep{lapedes1987nonlinear, parascandolo2016taming, Siren, mehta2021modulated} and expands the use of Fourier series in INRs \citep{gallant1988there, tancik2020fourier, shivappriya2021cascade, liao2020trainable}. Compared to state-of-the-art (SOTA) networks—WIRE \citep{saragadam2023wire}, SIREN \citep{Siren}, KAN \citep{liu2024kan}, Gaussian \citep{ramasinghe2022beyond}, MFN \citep{fathony2020multiplicative}, and FFN \citep{tancik2020fourier}—STAF demonstrates superior performance, with higher accuracy, faster convergence, and better Peak Signal-to-Noise Ratio (PSNR) in image, shape, and audio representation tasks. These advancements suggest that STAF significantly improves neural network technology and its practical applications, particularly in high-fidelity areas like computer graphics and data compression.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth, keepaspectratio]{data/staf_comparison_graphs.png} % Correct scaling
    \vspace{-1em}
    \caption{Activation functions used in INRs plotted over the range [-1, 1]. STAF utilizes a parameterized Fourier series activation, offering flexible frequency-domain adaptation. SIREN employs a sinusoidal function, providing a periodic activation landscape. WIRE employs a complex Gabor wavelet activation, balancing spatial and frequency localization.}
    \label{fig:activation}
    % \vspace{-0.75em}
\end{figure*}

To address these challenges, we propose the \textbf{S}inusoidal \textbf{T}rainable \textbf{A}ctivation \textbf{F}unction \textbf{(STAF)}, a novel family of parametric, trainable activation functions that enhance the expressive power and performance of INRs in modeling complex signals. STAF generalizes periodic activation functions like SIREN \citep{Siren}, which uses a single sinusoidal term with fixed phase and frequency, by introducing trainable parameters for greater flexibility. This development addresses challenges identified in earlier works regarding training networks with periodic activations \citep{lapedes1987nonlinear, parascandolo2016taming, mehta2021modulated} and expands the application of Fourier series in INRs \citep{gallant1988there, tancik2020fourier, shivappriya2021cascade, liao2020trainable}. Our findings indicate that STAF improves neural network performance in high-fidelity applications like computer graphics and data compression. Our work makes the following key \textbf{contributions}:
\begin{itemize}[label=$\bullet$, leftmargin=*]
    \item \textbf{Novel Initialization Scheme:} We propose a mathematically rigorous initialization scheme that introduces a unique probability density function for initialization, providing a more robust foundation for training compared to methods relying on the central limit theorem and specific conditions, such as SIREN.

    \item \textbf{Expressive Power:} STAF significantly expands the set of potential frequencies compared to SIREN. Leveraging a general theorem rooted in the Kronecker product, we provide \textbf{Theorems \ref{Kronecker_theorem}} and \textbf{\ref{asymptotic_behavior}}, which introduce novel theoretical insights applicable to a broad class of trainable activation functions. These contributions are underpinned by combinatorial and algebraic tools, offering a framework that generalizes beyond the scope of STAF. 

    \item \textbf{NTK Eigenvalues and Eigenfunctions:} We analyze the Neural Tangent Kernel (NTK) of our network, showing that its eigenvalues and eigenfunctions provide improved criteria for the learning process and convergence, enhancing understanding and performance during training.

    \item \textbf{Performance Improvements:} Our proposed activation function achieves significant PSNR gains across a range of signal representation tasks, including image, shape, and audio representation, as well as inverse problems such as image super-resolution and denoising. These advancements stem from faster convergence and enhanced accuracy, establishing STAF as a superior alternative to state-of-the-art models, including INCODE~\citep{kazerouni2024incode}, FINER~\citep{liu2024finer}, WIRE~\citep{saragadam2023wire}, SIREN~\citep{Siren}, KAN~\citep{liu2024kan}, Gaussian~\citep{ramasinghe2022beyond}, and FFN~\citep{tancik2020fourier}.
\end{itemize}





% Second Figure: Wide Figure in Two Columns


\iffalse

% Implicit Neural Representations (INRs), embodying a transformative shift in signal processing and computer vision, redefine the paradigms of data representation and processing \citep{mildenhall2020nerf, Siren}. Central to INRs is the learning of a continuous data mapping through a neural network, typically a Multilayer Perceptron (MLP). This continuous representation paradigm distinguishes itself from traditional discrete methods, offering a versatile model adept at handling diverse data types, from images to 3D models \citep{chen2021learning, mescheder2019occupancy}. INRs operate by embedding data in a high-dimensional space, with the neural network acting as a universal approximator to decipher complex data relationships. This novel approach transcends traditional grid-based limitations, spurring innovations in computer graphics, 3D modeling, and computational photography \citep{tancik2020fourier}.



% The versatility and significance of INRs in contemporary computational arenas are profound. In novel view synthesis and 3D reconstruction, INRs facilitate the creation of detailed, realistic renderings from limited data inputs \citep{mildenhall2020nerf}. Their exceptional performance extends to high-dimensional data processing, such as rendering intricate geometric shapes and simulating complex light interactions \citep{Siren}. This ability to forge detailed, continuous representations is crucial for accurately capturing real-world data nuances. INRs' adaptability is further demonstrated across various domains, including inverse problem-solving in imaging and virtual reality experience enhancement \citep{saragadam2022miner}, highlighting their potential to revolutionize computational methodologies and applications.




% Recently, INRs have emerged as powerful tools for representing continuous signals, particularly in handling large-scale data like gigapixel images and 3D scene reconstructions \citep{mildenhall2020nerf, Siren, saragadam2023wire,li2022learning}. However, traditional neural network architectures face limitations, especially due to the spectral bias inherent in ReLU networks, which hinders the accurate reconstruction of fine details \citep{rahaman2019spectral}. Addressing this, we introduce STAF - a novel family of parametric, trainable activation functions designed to overcome spectral bias limitations and enhance the modeling and reconstruction of complex signals.

% The evolution of activation functions in neural networks has profoundly influenced their development. Traditional functions like sigmoid and step, initially prevalent, faced challenges in deep networks due to vanishing gradients \citep{gustineli2022survey}. The advent of unbounded functions, notably ReLU, marked significant progress by resolving the vanishing gradient issue and improving deep network performance \citep{hu2018overcoming}. The advancement of activation functions, from bounded ones like sigmoid to ReLU and its variations \citep{nair2010rectified, maas2013rectifier, elfwing2018sigmoid, hendrycks2016gaussian}, has enhanced the networks' capacity for complex function approximation. The introduction of adaptive functions like SinLU \citep{paul2022sinlu}, TanhSoft \citep{biswas2021tanhsoft}, and Swish \citep{ramachandran2017searching} represent a significant advancement in network adaptability. Despite these developments, their full potential in neural representation tasks remained underexplored, a gap STAF aim to bridge effectively.

% Periodic activation functions have shown potential in guiding networks to learn high-frequency details crucial in various INR tasks. While early challenges in training networks with such activations were identified \citep{lapedes1987nonlinear, parascandolo2016taming}, recent developments like sine activations \citep{Siren} and modulated neural representations \citep{mehta2021modulated} have made significant strides. However, the broad application of Fourier series as a general activation function in INRs has only recently been explored \citep{gallant1988there, tancik2020fourier, shivappriya2021cascade, liao2020trainable}. STAF fills this void by introducing a novel parametric periodic activation function, tailored for MLP-based INR structures, enhancing both convergence speed and detail capture capabilities in signals.

% Engineered to counter the spectral bias problem, STAF facilitates accelerated learning of high-frequency signal details compared to traditional ReLU networks. This proficiency is vital for high-fidelity applications in computer graphics and data compression. Extensive experimental evaluations underscore STAF's superiority, achieving higher accuracy, faster convergence, and better Peak Signal-to-Noise Ratio (PSNR) than state-of-the-art networks like WIRE, SIREN, and those utilizing Fourier features. These results affirm STAF's potential in advancing neural network technology and practical applications.

% In summary, STAF marks a significant leap in the INR field. By addressing the spectral bias in conventional networks and introducing a flexible, trainable activation function, it opens new avenues for efficiently modeling and reconstructing complex signals. Its superior accuracy and training efficiency, validated through rigorous testing, positions it as a crucial asset for a range of applications in computer graphics and beyond. STAF represents not just an incremental improvement, but a pivotal development, setting a new benchmark in neural network technology and its practical implications.

\fi

% Implicit Neural Representations (INRs) have marked a significant paradigm shift in signal processing and computer vision, redefining how data is represented and processed \citep{mildenhall2020nerf, Siren}. At the core of INRs is the concept of learning a continuous mapping of data points through a neural network, typically a Multilayer Perceptron (MLP) . This approach contrasts sharply with traditional discrete representations, offering a more flexible and continuous model that can adeptly handle various types of data, from images to 3D shapes \citep{chen2021learning, mescheder2019occupancy}. The fundamental principle of INRs involves embedding data in a high-dimensional space, where a neural network, essentially functioning as a universal approximator, learns the complex relationships within the data. This capacity for continuous representation has been pivotal in transcending the limitations of grid-based data structures, thereby enabling groundbreaking advancements in fields such as computer graphics, 3D modeling, and computational photography \citep{tancik2020fourier}.

% The Importance and Versatility of INRs
% The significance of INRs in modern computational fields cannot be overstated. One of their most notable contributions is in the domain of novel view synthesis and 3D reconstructions, where INRs have enabled the generation of detailed and realistic renderings from sparse data inputs \citep{mildenhall2020nerf}. Moreover, INRs have shown exceptional performance in tasks that require high-dimensional data processing, like rendering complex geometric shapes or handling intricate light interactions in virtual environments \citep{siren}. These capabilities stem from the INR's inherent trait of creating detailed, continuous representations, which are crucial for capturing the nuances of real-world data. The versatility of INRs is also evident in their application across diverse areas, ranging from solving inverse problems in imaging to enhancing the quality of virtual reality experiences \citep{saragadam2022miner}. This widespread applicability underscores the transformative potential of INRs in shaping the future of computational methods and their real-world implementations.


% Implicit Neural Representations (INRs) have recently gained prominence as an effective method for representing continuous signals in various fields, notably in handling large-scale signals like gigapixel images and 3D scene reconstructions \citep{mildenhall2020nerf, Siren, saragadam2023wire,li2022learning}. Traditional neural network architectures, however, exhibit certain limitations, primarily due to the spectral bias inherent in ReLU networks \citep{rahaman2019spectral}. This spectral bias restricts the ability of these networks to accurately reconstruct fine details in target signals. Addressing this critical issue, our paper introduces STAFs, a novel family of parametric trainable activation functions specifically designed to overcome the constraints imposed by spectral bias and significantly enhance the modeling and reconstruction of complex signals with remarkable precision.

% The development of neural networks has been significantly influenced by the evolution of activation functions. Traditional functions like the sigmoid and step, initially popular in network design, encountered challenges in deep networks due to vanishing gradients, which hindered effective learning and limited the scalability of network depth \citep{gustineli2022survey}. The introduction of unbounded functions such as the Rectified Linear Unit (ReLU) marked a significant advancement, primarily by mitigating the vanishing gradient problem and enhancing the performance of deep networks \citep{hu2018overcoming}.

% The evolution of activation functions in neural networks has played a crucial role in shaping their capabilities. Starting from non-periodic functions like sigmoid and stepping up to ReLU and its variations \citep{nair2010rectified, maas2013rectifier, elfwing2018sigmoid, hendrycks2016gaussian}, each development has contributed to the networks' ability to approximate complex functions. The introduction of adaptive activation functions like SinLU \citep{paul2022sinlu}, TanhSoft \citep{biswas2021tanhsoft}, and Swish \citep{ramachandran2017searching} represented a leap forward in the adaptability of neural networks. Despite these advancements, the full potential of these functions in neural representation tasks remained untapped, a challenge that STAFs seek to address effectively.

% Periodic activation functions have shown promise in guiding networks to learn high-frequency details, a vital aspect in various INR tasks. While the challenges of training networks with such activations were identified early on \citep{lapedes1987nonlinear, parascandolo2016taming}, recent advances like Sitzmann et al.'s use of sine activations \citep{Siren} and Mehta et al.'s modulated neural representations \citep{mehta2021modulated} have marked significant progress. However, the broader application of Fourier series as a general activation function in INRs was not fully explored until recently \citep{gallant1988there, tancik2020fourier, shivappriya2021cascade, liao2020trainable}. STAFs bridge this gap by introducing a novel parametric periodic activation function, tailored for MLP-based INR structures, significantly enhancing both the convergence speed and the capacity to capture intricate details in signals.



% STAFs are engineered to mitigate the spectral bias problem, thereby enabling accelerated learning of high-frequency details in target signals compared to conventional ReLU networks. This ability to capture intricate details is essential for applications requiring high fidelity, such as computer graphics and data compression. Our approach is substantiated by extensive experimental evaluations, which demonstrate that STAFs not only achieve significantly higher accuracy than state-of-the-art networks like WIRE, SIREN, and those utilizing Fourier features but also exhibit faster convergence and superior Peak Signal-to-Noise Ratio (PSNR) during training. These findings affirm the potential of STAFs in advancing the field of neural networks and their applicability in practical scenarios.

% In summary, STAFs represent a significant leap forward in the field of INRs. By addressing the spectral bias of conventional neural networks and introducing a flexible, trainable activation function, they open new avenues for efficiently modeling and reconstructing complex signals. Their superior performance in terms of accuracy and training efficiency, as demonstrated through rigorous testing, positions them as a valuable asset for a wide range of applications in computer graphics and beyond. The introduction of STAFs is not just an incremental improvement, but a pivotal development that sets a new benchmark in the field of neural networks and their practical applications.

% %%

% Implicit Neural Representations (INRs) have evolved significantly, with applications spanning from signed distance functions to complex tasks in images, videos, and audio signals \citep{mildenhall2020nerf, Siren, saragadam2023wire,li2022learning}. Traditional INR approaches, while versatile, have encountered challenges such as slow convergence and difficulties in representing high-frequency details. For instance, the sine-based activation functions introduced by Sitzmann et al. \citep{Siren} marked a significant step but were hampered by these issues. STAFs, in contrast, are designed to surpass these limitations by offering superior learning efficiency and precision in signal reconstruction, thereby positioning themselves as a significant advancement in the spectrum of INR technologies.