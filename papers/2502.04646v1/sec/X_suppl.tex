



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


{\small
\begin{spacing}{0.7}
\tableofcontents
\end{spacing}
}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Related Work}\label{sec:related}
%{\color{red}to Appendix}

\paragraph{Importance Sampling.}
%In \cite{grover2019bias} proposed likelihood-free importance weighting method to correct for the bias in generative models by using a probabilistic classifier to distinguish samples from the original distribution and the genrative models. 

Importance sampling \cite{rubinstein2016simulation} plays a key role in various foundational tasks such as variance reduction, data augmentation, root cause analysis, and reliability assessment \cite{grover2019bias, antoniou2017data, li2024efficient}. However, directly estimating the importance sampling PDF $q$ via Monte Carlo methods using samples drawing from $p$ is challenging, primarily due to instability in accurately computing the normalization constant. % $\int l(\rvx) p(\rvx) \,\mathrm{d}\rvx$ in (\ref{importance sampling_PDF_definition}).

To address this, conventional techniques often leverage Cross-Entropy methods \cite{rubinstein1999cross, rubinstein2001combinatorial}, which iteratively optimize a parameterized PDF to minimize the Kullback-Leibler (KL) divergence from the target importance sampling PDF. However, for complex, high-dimensional datasets—such as natural images with intractable distributions—the selection of a suitable class of PDFs becomes highly challenging. Recent developments in generative modeling offer a solution via bijective mappings, where a tractable initial distribution is progressively transformed through the change-of-variables formula, resulting in a valid end distribution known as normalizing flow \cite{rezende2015variational, kobyzev2020normalizing}. Normalizing flow-based methods have been applied to importance sampling, employing efficient invertible functions as in \cite{muller2019neural} or interpretable, shape-constrained networks \cite{xiang2024importance}.

Nevertheless, Cross-Entropy or normalizing flow-based methods often exhibit limited representational flexibility and can struggle with mapping selection \cite{zhang2021diffusion, cornish2020relaxing}. Additionally, training separate generative models for distinct importance weight functions $l$ is computationally prohibitive, particularly for applications requiring diverse importance criteria or multiple estimations across varied weighting functions.

% Importance sampling \cite{rubinstein2016simulation} is widely applied in various foundational tasks such as variance reduction for mean value computation, data augmentation, root cause analysis, and reliability analysis \cite{grover2019bias, antoniou2017data, li2024efficient}. In practical applications, where samples $\rvx$ are drawn from $p$, estimating the importance sampling PDF $q$ using conventional Monte Carlo methods poses significant challenges, primarily due to the instability in accurately computing the normalization constant $\int l(\rvx) p(\rvx) \,\mathrm{d}\rvx$ in (\ref{importance sampling_PDF_definition}).


%To address this challenge, conventional approaches typically rely on Cross-Entropy methods \cite{rubinstein1999cross, rubinstein2001combinatorial}, which approximate an importance sampling PDF by optimizing a parameterized PDF—typically chosen from a predefined class of PDFs—through iterative updates that minimize the Kullback-Leibler (KL) divergence between the candidate PDF and the importance sampling PDF. For high-dimensional and complex datasets, particularly those involving natural images with distributions that are not available in closed form, selecting such an appropriate class of PDFs to consider is itself a complex problem. Recent advances in generative modeling aim to address these limitations through bijective mappings. By applying the change-of-variables formula, the tractable initial distribution is flowed through these mappings, resulting in a valid probability distribution at the end of the process, which is called normalizing flow \cite{rezende2015variational, kobyzev2020normalizing}. In \cite{muller2019neural}, the normalizing flow-based importance sampling PDF training has been suggested with efficient invertible functions. The normalizing flow-based model is also utilized in \cite{xiang2024importance} with interpretable shape-constrained networks for importance sampling.

%Despite these advancements, shape-constrained PDFs and cross-entropy-based methods often exhibit limitations in representational flexibility and encounter difficulties in mapping selection \cite{zhang2021diffusion, cornish2020relaxing}. Moreover, training separate generative models for distinct importance weight functions $l$ becomes computationally prohibitive for applications where diverse importance criteria might be used or multiple mean-value estimations across various weighting functions.

\paragraph{Score-based Generative Models.}
%Recent innovations in generative modeling focus on score-based models, which aim to learn the score function of a target distribution rather than its PDF. Score-based models enable sampling via stochastic differential equations or discretized iterative approaches \cite{song2021score, ho2020ddpm, song2021ddim}, achieving remarkable performance in high-resolution image generation tasks \cite{croitoru2023diffusion}. Notably, these models eliminate the need for complex bijective mappings, as required in normalizing flow-based models, allowing greater design flexibility.

%However, directly applying existing score-based generative methods to learn the score function of an importance sampling PDF is challenging. Current methods typically rely on data samples drawn from the target distribution, which limits their adaptability to importance sampling. Additionally, since our objective includes handling varied importance weight functions $l$, training a score-based model for each $l$ using existing methods is not a feasible solution.

%Our approach addresses these limitations by modeling the importance sampling process as a diffusion process, approximated in terms of the score function of the original PDF and the importance weight function, rather than directly learning the score function of the importance sampling PDF. Consequently, our method avoids the need for multiple generative models for each importance weight function while fully leveraging the expressive capabilities of score-based models.

Recent advances in generative modeling have focused on score-based models, which learn the score function of a target distribution instead of estimating its PDF directly \cite{vincent2011connection, song2021score, hyvarinen2005estimation}. This approach enables efficient sampling via stochastic differential equations or iterative methods \cite{song2021score, ho2020ddpm, song2021ddim}, achieving notable success in high-resolution image generation \cite{croitoru2023diffusion}. By eliminating the need for shape-constrained bijective mappings required by normalizing flow-based models, score-based models allow for greater design flexibility. 
%
Direct applications of existing score-based methods to importance sampling, however, present challenges, as they require samples from the target distribution, limiting adaptability. Also, training distinct score-based generative models for various $l$ is inefficient and impractical. 

To address these, we model importance sampling as a diffusion process, approximated with the original PDF's score function combined with the importance weight function, instead of learning the importance sampling PDF’s score function directly. This leverages score-based generative modeling without needing multiple generative models for various importance weight functions, while fully leveraging the expressive capabilities of score-based models.

\paragraph{Difference over Conditional Generative and Sampling Methods.} 
Advancements in generative modeling have enabled conditional generative models and sampling techniques across applications. Traditionally, these methods use explicit conditioning during training, as seen in text-to-image generation \cite{mansimov2015generating, reed2016generative, li2019controllable, yu2022scaling, nichol2022glide, podell2024sdxl}, image-to-image translation \cite{isola2017image, richardson2021encoding, liu2017unsupervised}, and text-to-audio synthesis \cite{liu2023audioldm}.
Recently, score-based generative frameworks introduced conditional sampling without conditioning at training. Here, unconditioned models achieve conditional sampling by modifying standard sampling methods \cite{mengsdedit, trippediffusion, wu2024practical}, presenting a distinct approach from conventionally trained conditional models \cite{dhariwal2021diffusion}.

Note that importance sampling remains fundamentally distinct from conditional sampling. Unlike conditional sampling respecting certain known values of variables, where the conditioning variable is typically an intrinsic feature derived from the distribution instance itself (e.g., a degraded image in inverse problem scenarios \cite{rout2024beyond, rout2024solving, chungdiffusion}), importance sampling focuses on sampling instances where each probability density is weighted proportionally by an importance weight function. This function can be \emph{externally and flexibly defined} and may \emph{operate independently} from the original distribution $p$, thus establishing a mechanism that is unique from conditioning in generative models.


%Advancements in generative modeling have also enabled the development of conditional generative models and sampling techniques for a range of applications. Traditionally, these approaches involve explicitly conditioning generative models during the training phase. Examples include text-to-image generation \cite{mansimov2015generating, reed2016generative, li2019controllable, yu2022scaling, nichol2022glide, podell2024sdxl}, image-to-image translation \cite{isola2017image, richardson2021encoding, liu2017unsupervised}, and text-to-audio synthesis \cite{liu2023audioldm}.

%Recently, conditional sampling methods that do not require conditioning during training have been proposed, particularly in score-based generative frameworks. In this paradigm, unconditioned generative models can execute conditional sampling through modifications of conventional sampling method \cite{mengsdedit, trippediffusion, wu2024practical}, representing a distinct approach from conditionally trained generative models \cite{dhariwal2021diffusion}.




%There are conceptual parallels of our methodology for bypassing direct learning of the target distribution $q$ and some existing works using conditional score-based sampling have avoiding the use of conditionally trained generative models, however, 


\section{Technical Results}
\label{appendix_technical_results}


\begin{table}[t]
\caption{Notation and Description}
\label{tab:notation_description}
\vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc} % section title font
\begin{tabular}{lcccr}
\toprule
Notation & Description & Note \\
\midrule
$\rmX_{t}$    & A random vector of the importance sampling diffusion process at time $t$  & $\forall t, \rmX_{t} \in \mathbb{R}^{d}$  \\
$\rmX'_{t}$    & A random vector of the original (base) diffusion process at time $t$  & $\forall t, \rmX'_{t} \in \mathbb{R}^{d}$  \\
$\rmZ$    & A Gaussian random vector & $\rmZ \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$  \\
\hline
$l(\rvx)$    & An importance weight function &  $0 < m < l(\rvx) < \infty$\\
$q(\rvx)$    & The importance sampling PDF   & $q(\rvx) = q_{0}(\rvx)$\\
$q_{t}(\rvx)$    & The PDF of $\rmX_{t}$  & \\
$p(\rvx)$    & The original (base) PDF &  $p(\rvx) = p_{0}(\rvx)$\\
$p_{t}(\rvx)$    & The PDF of $\rmX'_{t}$  & \\
$\nabla_{\rvx} \log q_{t}(\rvx)$ & The score function of $q_{t}$ &\\
$\nabla_{\rvx} \log p_{t}(\rvx)$ & The score function of $p_{t}$ &\\
$\mathcal{N}(\mu, \sigma^2)$ & A Gaussian distribution with mean $\mu$ and variance $\sigma^2$ &\\
$\mathcal{U}(\mathcal{S})$ & A uniform distribution over space $\mathcal{S}$ &\\
\hline
$\bar{\rvx}'_{0}\vert_{\rvx,t}$ & The conditional mean of $\rmX'_{0}$ for a given observation $\rvx$ at $t$ & \\

\hline
$[a,b]$    & A closed interval from $a\in \mathbb{R}$ to $b\in \mathbb{R}$ &  \\
$[v]_{i} $    & The $i$-th element of a vector $v$ & \\
$[M]_{i,j}$    & The $(i,j)$-th element of a matrix $M$ & \\
$\mathbb{N}$ & The set of natural numbers & \\
$\mathbb{R}$ & The set of real numbers & \\

\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}








\subsection{Proof of Theorem \ref{theorem_1}}
\label{proof_theorem_1}
\begin{proof}
The norm of the discrepancy between the true score function and its approximation is represented as follows.
\begin{align}
    % \label{score_gap_00}
    &\Vert \nabla_{\rvx} \log q_{t}(\rvx) - \nabla_{\rvx} \log \tilde{q}_{t}(\rvx) \Vert \notag \\
    &= \left\Vert \nabla_{\rvx} \log q_{t}(\rvx) - \nabla_{\rvx} \log p_{t}(\rvx) - \frac{\nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t})}{\sqrt{\bar{\alpha}(t)}} - \frac{\nabla_{\rvx} \log p_{t}(\rvx + \epsilon \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t})) - \nabla_{\rvx} \log p_{t}(\rvx)}{\epsilon (1-\bar{\alpha}(t))^{-1} \sqrt{\bar{\alpha}(t)}} \right\Vert \notag \\
    \label{score_norm_gap_first_term}
    &\le \Vert \nabla_{\rvx} \log \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[l(\rmX'_{0})]  - \nabla_{\rvx} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t})  \Vert \\
    \label{score_norm_gap_second_term}
    &+ \frac{(1-\bar{\alpha}(t))}{ \sqrt{\bar{\alpha}(t)}}\left\Vert H_{p_{t}}(\rvx)  \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t})  - \frac{\nabla_{\rvx} \log p_{t}(\rvx + \epsilon \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t})) - \nabla_{\rvx} \log p_{t}(\rvx)}{\epsilon} \right\Vert
\end{align}
where the first equality follows from the definition of the approximated score function, and the inequality is a result of the triangle inequality of the norm. To establish an upper bound on the gap between the score functions, we analyze the upper bounds of terms (\ref{score_norm_gap_first_term}) and (\ref{score_norm_gap_second_term}). By applying the chain rule, term (\ref{score_norm_gap_first_term}) can be reformulated as follows.
\begin{align}
    \label{score_gap_01}
    & \Vert \nabla_{\rvx} \log \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[l(\rmX'_{0})]  - \nabla_{\rvx} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t})  \Vert \\
    &=  \left\Vert \frac{ \nabla_{\rvx} \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[l(\rmX'_{0})] }{  \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[l(\rmX'_{0})] }  - \frac{\nabla_{\rvx}  l(\bar{\rvx}'_{0}\vert_{\rvx,t}) }{l(\bar{\rvx}'_{0}\vert_{\rvx,t})} \right\Vert   = \left\Vert \frac{ l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \nabla_{\rvx} J_{t}(\rvx) - J_{t}(\rvx) \nabla_{\rvx}  l(\bar{\rvx}'_{0}\vert_{\rvx,t}) }{ \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[l(\rmX'_{0})] l(\bar{\rvx}'_{0}\vert_{\rvx,t}) } \right\Vert \notag \\
    &\le \frac{ \Vert \nabla_{\rvx} J_{t}(\rvx)\Vert }{\mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[l(\rmX'_{0})] } +  \frac{\Vert J_{t}(\rvx) \Vert \Vert  \nabla_{\rvx}  l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \Vert}{\mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[l(\rmX'_{0})] l(\bar{\rvx}'_{0}\vert_{\rvx,t}) } 
    \label{score_gap_02}
\end{align}
where $ J_{t}(\rvx)$ can be interpreted as the Jensen gap, defined as
\begin{align*}
    J_{t}(\rvx) = \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[l(\rmX'_{0})]  -  l(\bar{\rvx}'_{0}\vert_{\rvx,t}) 
\end{align*}
and $\nabla_{\rvx} J_{t}(\rvx) $ is the gradient of the Jensen gap which is given as
\begin{align*}
    % \label{jensen_gap_at_t_1}
    \nabla_{\rvx} J_{t}(\rvx) = \nabla_{\rvx}  \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[l(\rmX'_{0})]  - \nabla_{\rvx}  l(\bar{\rvx}'_{0}\vert_{\rvx,t})  .
\end{align*}
From the Taylor second-order approximation of the importance weight function $l$ as
\begin{align*}
    l(\rmX'_{0}) \approxeq  l(\bar{\rvx}'_{0}\vert_{\rvx,t}) + \nabla l(\bar{\rvx}'_{0}\vert_{\rvx,t})^{\top}(\rmX'_{0} - \bar{\rvx}'_{0}\vert_{\rvx,t}) + \frac{1}{2} (\rmX'_{0} - \bar{\rvx}'_{0}\vert_{\rvx,t})^{\top} H_{l}(\bar{\rvx}'_{0}\vert_{\rvx,t})(\rmX'_{0} - \bar{\rvx}'_{0}\vert_{\rvx,t}),
\end{align*}
we have $J_{t}(\rvx) = \frac{1}{2} \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[(\rmX'_{0} - \bar{\rvx}'_{0}\vert_{\rvx,t})^{\top} H_{l}(\bar{\rvx}'_{0}\vert_{\rvx,t})(\rmX'_{0} - \bar{\rvx}'_{0}\vert_{\rvx,t})]$.

For a given realization $\rvx$ at time $t$, note that $\rmX'_{0}$ is a random variable with a density depending on $\rvx$. Consider a real variable $\rvy$ for the realization of $\rmX'_{0}$ for integration. We then have
\begin{align*}
    \nabla_{\rvx} J_{t}(\rvx) = \frac{1}{2}\nabla_{\rvx} \int (\rvy - \bar{\rvx}'_{0}\vert_{\rvx,t})^{\top} H_{l}(\bar{\rvx}'_{0}\vert_{\rvx,t})(\rvy - \bar{\rvx}'_{0}\vert_{\rvx,t})  p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx) \, \mathrm{d} \rvy.
\end{align*}
Applying Leibniz's Rule, we obtain
\begin{align*}
     \nabla_{\rvx} J_{t}(\rvx) 
     =&   \frac{1}{2}\int \left(\nabla_{\rvx}  (\rvy - \bar{\rvx}'_{0}\vert_{\rvx,t})^{\top} H_{l}(\bar{\rvx}'_{0}\vert_{\rvx,t})(\rvy - \bar{\rvx}'_{0}\vert_{\rvx,t}) \right) p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx) \, \mathrm{d} \rvy \nonumber  \\
     &+  \frac{1}{2}\int \left( \nabla_{\rvx} p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx)   \right)  (\rvy - \bar{\rvx}'_{0}\vert_{\rvx,t})^{\top} H_{l}(\bar{\rvx}'_{0}\vert_{\rvx,t})(\rvy - \bar{\rvx}'_{0}\vert_{\rvx,t})
     \, \mathrm{d} \rvy.
\end{align*}
Since we consider derivatives with respect to a fixed $t$, we simplify our notation by introducing the following substitutions for the integral: $\mu(\rvx) \coloneqq \bar{\rvx}'_{0}\vert_{\rvx,t}$, $p(\rvy;\rvx) \coloneqq p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx)$, and $e(\rvy, \rvx) \coloneqq \rvy - \mu(\rvx)$.

Converting $\nabla_{\rvx} p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx) $ into $\nabla_{\rvx} p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx)  = p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx) \nabla_{\rvx} \log p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx)$, we have
\begin{align}
\label{simplified_gradient_jensen_gap}
2\nabla_{\rvx} J_{t}(\rvx) &= \underbrace{\int \left( \nabla_{\rvx} e(\rvy, \rvx)^{\top} H_{l}(\mu(\rvx)) e(\rvy, \rvx) \right)  p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx) \, \mathrm{d} \rvy}_{W_1} \nonumber \\
&+ \underbrace{\int e(\rvy, \rvx)^{\top} H_{l}(\mu(\rvx)) e(\rvy, \rvx) p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx) \, \nabla_{\rvx} \log p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx)  \, \mathrm{d} \rvy}_{W_2}.
\end{align}

Defining $v(\rvy, \rvx) \coloneqq H_{l}(\mu(\rvx)) e(\rvy, \rvx)$, the first term on the right-hand side of (\ref{simplified_gradient_jensen_gap}) can be written as follows.
\begin{align}
    \label{graident_jensen_gap_term_1_00}
     W_1 &= \int \left( \nabla_{\rvx} e(\rvy, \rvx)^{\top} v(\rvy, \rvx) \right)  p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx) \, \mathrm{d} \rvy =  \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[\nabla_{\rvx} e(\rmX'_{0}, \rvx)^{\top} v(\rmX'_{0}, \rvx)].
\end{align}
We investigate the derivative of the inner product between $\nabla_{\rvx} e(\rvy, \rvx)$ and $v(\rvy, \rvx)$ in (\ref{graident_jensen_gap_term_1_00}). By the product rule, we have
\begin{align}
\label{graident_jensen_gap_term_1_01}
    \frac{\partial e(\rvy, \rvx)^{\top} v(\rvy, \rvx)}{\partial \rvx} = e(\rvy, \rvx)^{\top} \frac{\partial v(\rvy, \rvx)}{\partial \rvx} + v(\rvy, \rvx)^{\top} \frac{\partial e(\rvy, \rvx)}{\partial \rvx} = e(\rvy, \rvx)^{\top} \frac{\partial v(\rvy, \rvx)}{\partial \rvx} - e(\rvy, \rvx)^{\top} H_{l}(\mu(\rvx)) \frac{\partial \mu(\rvx)}{\partial \rvx}
\end{align}
where the last equality follows from $\frac{\partial e(\rvy, \rvx)}{\partial \rvx} = \frac{\partial \mu(\rvy, \rvx)}{\partial \rvx}$. Note that to compute (\ref{graident_jensen_gap_term_1_00}), the last term in (\ref{graident_jensen_gap_term_1_01}), $e(\rvy, \rvx)^{\top} H_{l}(\mu(\rvx)) \frac{\partial \mu(\rvx)}{\partial \rvx}$ can be ignored as it will be zero by the conditional expectation in (\ref{graident_jensen_gap_term_1_00}). Now we focus on $e(\rvy, \rvx)^{\top} \frac{\partial v(\rvy, \rvx)}{\partial \rvx}$.

For all $i,j \in [d]$, the derivative of vector $v$ with respect to the vector $\rvx$ is given as follows
\begin{align}
\label{graident_v_00}
    \left[\frac{\partial v(\rvy, \rvx)}{\partial \rvx} \right]_{i,j} = \frac{\partial v_i}{\partial x_j} = \sum_{k=1}^d \left( \frac{\partial [H_l(\mu(\rvx)) ]_{ik}}{\partial x_j} [e(\rvy, \rvx)]_{k} + [H_{l}(\mu(x))]_{ik} \frac{\partial [e(\rvy, \rvx)]_k}{\partial x_j} \right).
\end{align}
Using tensor representation, we can further simplify (\ref{graident_v_00}) to
\begin{align}
\label{graident_v_01}
    \frac{\partial v(\rvy, \rvx)}{\partial \rvx}  = \frac{\partial H_l(\mu(\rvx))}{\partial \rvx} \times_{3} e(\rvy, \rvx) + H_{l}(\mu(\rvx)) \frac{\partial e(\rvy,\rvx)}{\partial \rvx}
\end{align}
where $\frac{\partial H_l}{\partial \rvx}$ is a 3-dimensional tensor  and $\times_{3}$ indicates mode-3 tensor-vector multiplication.

The second term in (\ref{graident_v_01}) can be ignored for computing $W_1$ since it remains independent of $\rvy$ due to the partial derivative with respect to $\rvx$. Specifically, when multiplied by $e(\rvy, \rvx)$, it becomes zero under the conditional expectation from (\ref{graident_jensen_gap_term_1_00}). Consequently, we can simplify $W_{1}$ as follows.
\begin{align*}
    % \label{graident_jensen_gap_term_1_02}
     W_1 &=  \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[\nabla_{\rvx} e(\rmX'_{0}, \rvx)^{\top} v(\rmX'_{0}, \rvx)] = \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}\left[ \left(\frac{\partial H_{l}(\mu(\rvx))}{\partial \rvx} \times_{3} e(\rmX'_{0}, \rvx) \right)^{\top}  e(\rmX'_{0}, \rvx) \right].
\end{align*}
Taking $L_{2}$-norm, we obtain
\begin{align}
    \label{graident_jensen_gap_term_1_03}
     &\Vert W_1 \Vert \!=\! \left\Vert \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}} \!(\cdot|\rvx)}\left[ \left(\frac{\partial H_{l}(\mu(\rvx))}{\partial \rvx} \!\times_{3} e(\rvy, \rvx) \right)^{\!\!\top} \!\!  e(\rvy, \rvx) \right] \right\Vert \le  \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}\!\left[ \left\Vert \left(\frac{\partial H_{l}(\mu(\rvx))}{\partial \rvx} \!\times_{3} e(\rvy, \rvx) \right)^{\!\! \top} \!\! e(\rvy, \rvx) \right\Vert \right] \\
    \label{graident_jensen_gap_term_1_04}
     &\le \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)} \! \left[ \left\Vert \left(\frac{\partial H_{l}(\mu(\rvx))}{\partial \rvx} \times_{3} e(\rvy, \rvx) \right) \right\Vert \! \left\Vert e(\rvy, \rvx) \right\Vert \right]
     \! \le \! \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)} \! \left[ \left\Vert \left(\frac{\partial H_{l}(\mu(\rvx))}{\partial \rvx} \times_{3} e(\rvy, \rvx) \right) \right\Vert_{F} \! \left\Vert e(\rvy, \rvx) \right\Vert \right]
\end{align}
where (\ref{graident_jensen_gap_term_1_03}) follows from Jensen's inequality due to the convexity of the $L_{2}$ norm. The transition from (\ref{graident_jensen_gap_term_1_03}) to (\ref{graident_jensen_gap_term_1_04}) provides an upper bound for the matrix-vector multiplication norm. The final inequality in (\ref{graident_jensen_gap_term_1_04}) is derived from the matrix norm inequality, specifically between the spectral norm and the Frobenius norm.

%
The Frobenius norm of the tensor-vector multiplication can be bounded by the Cauchy-Schwarz inequality as follows.
\begin{align}
    % \label{graident_jensen_gap_term_1_05}
    \left\Vert \left(\frac{\partial H_{l}(\mu(\rvx))}{\partial \rvx} \times_{3} e(\rvy, \rvx) \right) \right\Vert_{F}^{2} &= \sum_{i=1}^d \sum_{j=1}^d  \left( \frac{\partial v_i}{\partial x_j} \right)^{2} =\sum_{i=1}^d \sum_{j=1}^d  \left(\sum_{k=1}^d  \frac{[\partial H_{l}(\mu(\rvx))]_{ik}}{\partial x_j} [e(\rvy, \rvx)]_{k} \right)^{2} \notag \\
    \label{graident_jensen_gap_term_1_06}
    &\le  \sum_{i=1}^d \sum_{j=1}^d \left( \sum_{k=1}^d \left( \frac{[\partial H_{l}(\mu(\rvx))]_{ik}}{\partial x_j}  \right)^{2} \sum_{k=1}^d  [e(\rvy, \rvx) ]_{k}^{2} \right).
\end{align}
%
Substituting (\ref{graident_jensen_gap_term_1_06}) into (\ref{graident_jensen_gap_term_1_04}), we obtain
\begin{align*}
    % \label{norm_W_1}
     \Vert W_1 \Vert      \le \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}\left[ \left\Vert \frac{\partial H_{l}(\mu(\rvx))}{\partial \rvx}  \right\Vert_{F} \left\Vert e(\rmX'_{0}, \rvx) \right\Vert^{2} \right] \le \eta_{F} \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}\left[ \left\Vert e(\rmX'_{0}, \rvx) \right\Vert^{2} \right].
\end{align*}

Next, we examine the norm of $W_{2}$ in (\ref{simplified_gradient_jensen_gap}), whose an upper bound is given by
\begin{align}
    % \label{graident_jensen_gap_term_2_00}
    \Vert W_{2} \Vert &\le  \int \left\Vert e(\rvy, \rvx)^{\top} H_{l}(\mu(\rvx)) e(\rvy, \rvx) \right\Vert p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx) \, \left\Vert \nabla_{\rvx} \log p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx) \right\Vert \, \mathrm{d} \rvy \notag \\
    &\le \Vert H_{l}(\mu(\rvx)) \Vert \gamma_{t} \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}\left[ \left\Vert e(\rmX'_{0}, \rvx) \right\Vert^{2} \right] \notag
\end{align}
from $\left\Vert \nabla_{\rvx} \log p_{\rmX'_{0}|\rmX'_{t}}(\rvy|\rvx) \right\Vert \le \gamma_{t}$, and the Hessian two-norm is bounded by $\Vert H_{l}(\mu(\rvx)) \Vert \le \eta_{2}$.
This leads to the following expression:
\begin{align}
    \label{norm_grad_jensen_upper_bound_final}
    &\Vert \nabla_{\rvx} J_{t}(\rvx) \Vert \le \frac{1}{2}(\Vert W_{1}\Vert + \Vert W_{2} \Vert) \le \frac{1}{2}(\eta_{F} + \gamma_{t}\eta_{2})\mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}\left[ \left\Vert e(\rmX'_{0}, \rvx) \right\Vert^{2} \right]
\end{align}
and it is straightforward to observe that the norm $\Vert J_{t}(\rvx) \Vert$ is bounded above as follows.
\begin{align}
    \label{norm_jensen_upper_bound_final}
    &\Vert J_{t}(\rvx) \Vert \le \frac{1}{2} \eta_{2}\mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}\left[ \left\Vert e(\rmX'_{0}, \rvx) \right\Vert^{2} \right].
\end{align}
%
To apply (\ref{score_gap_01}), we analyze the Euclidean norm of $\nabla_{\rvx} \log l(\hat{\rvx}(0) \vert_{\rvx,t})$, yielding
\begin{align}
    \label{norm_of_true_derivative_log_x_0}
    &\left\Vert \nabla_{\rvx} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \right\Vert 
    %
    = \left\Vert
 \frac{\left(  \rmI +  (1-\bar{\alpha}(t)) H_{p_{t}}(\rvx) \right)}{\sqrt{\bar{\alpha}(t)}} \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \right\Vert 
 %
 \le \left(\frac{1+(1-\bar{\alpha}(t))\zeta_{t}}{\sqrt{\bar{\alpha}(t)}} \right )\eta.
\end{align}
Combining results from (\ref{norm_grad_jensen_upper_bound_final}), (\ref{norm_jensen_upper_bound_final}), (\ref{norm_of_true_derivative_log_x_0}), and (\ref{score_gap_01}), we conclude
\begin{align}
    \label{first_term_final_bound}
    &\Vert \nabla_{\rvx} \log \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}[l(\rmX'_{0})]  - \nabla_{\rvx} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t})  \Vert \le \frac{\Vert \nabla_{\rvx} J_{t}(\rvx) \Vert}{m} + \frac{\Vert J_{t}(\rvx) \Vert \left\Vert \nabla_{\rvx} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \right\Vert}{m^2} \\
    %
    &\le \left( \frac{1}{2m}(\eta_{F} + \gamma_{t}\eta_{2}) + \frac{ \left({1+(1-\bar{\alpha}(t))\zeta_{t}} \right )\eta\eta_{2} }{2m^2 \sqrt{\bar{\alpha}(t)}} \right)
 \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}\left[ \left\Vert e(\rmX'_{0}, \rvx) \right\Vert^{2} \right].
\end{align}


Thus far, we have investigated the upper bound of the norm for the first term in (\ref{score_gap_02}). Now, we consider the term in (\ref{score_norm_gap_second_term}). We express the score function at $\rvx + \epsilon \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t})$ using Taylor’s theorem, with the integral form for the remainder.
\begin{align*}
    &\nabla_{\rvx} \log p_{t}(\rvx + \epsilon \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t})) \nonumber \\
    &= \nabla_{\rvx} \log p_{t}(\rvx) + \int_{0}^{\epsilon} H_{p_{t}}(\rvx + s \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}))  \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \, \mathrm{d}s. 
\end{align*}
This yields
\begin{align}
\label{eq_hessian_taylor_remainder}
    &\frac{\nabla_{\rvx} \log p_{t}(\rvx + \epsilon \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) ) - \nabla_{\rvx} \log p_{t}(\rvx)}{\epsilon} - H_{p_{t}}(\rvx)  \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \nonumber \\&= \frac{1}{\epsilon} \int_{0}^{\epsilon} \left( H_{p_{t}}(\rvx + s \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}))  - H_{p_{t}}(\rvx) \right) \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \, \mathrm{d}s. 
\end{align}
Considering the norm of (\ref{eq_hessian_taylor_remainder}), we have
\begin{align}
% \label{eq_hessian_taylor_remainder_2}
    &\left\Vert \frac{\nabla_{\rvx} \log p_{t}(\rvx + \epsilon \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) ) - \nabla_{\rvx} \log p_{t}(\rvx)}{\epsilon} - H_{p_{t}}(\rvx)  \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \right\Vert \notag \\
    &\le \frac{1}{\epsilon} \int_{0}^{\epsilon} \left\Vert H_{p_{t}}(\rvx + s \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}))  - H_{p_{t}}(\rvx) \right\Vert \cdot \left\Vert \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \right\Vert \, \mathrm{d}s. \notag \\
    \label{by_hessian_lipschitz}
        & \le \frac{1}{\epsilon} \int_{0}^{\epsilon}  L s \Vert \nabla_{\bar{\rvx}'_{0}\vert_{\rvx,t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx,t}) \Vert^2 \mathrm{d}s \le \frac{L\eta^{2}\epsilon}{2}
\end{align}
where the bounds follow from the norm inequality and the Lipschitz continuity property of the Hessian matrix.


Combining (\ref{by_hessian_lipschitz}) and (\ref{first_term_final_bound}), we obtain
\begin{align}
    \Vert \nabla_{\rvx} \log q_{t}(\rvx) - \nabla_{\rvx} \log \tilde{q}_{t}(\rvx) \Vert  \le 
 \lambda_{t} \mathbb{E}_{\rmX'_{0} \sim p_{\rmX'_{0}|\rmX'_{t}}(\cdot|\rvx)}\left[ \left\Vert e(\rvy, \rvx) \right\Vert^{2} \right] + \frac{(1-\bar{\alpha}(t))L\eta^{2}\epsilon}{2\sqrt{\bar{\alpha}(t)}}, 
\end{align}
where $\lambda_{t} = \left( \frac{\eta_{F} + \gamma_{t}\eta_{2} }{2m}+ \frac{ \left({1+(1-\bar{\alpha}(t))\zeta_{t}} \right )\eta\eta_{2} }{2m^2 \sqrt{\bar{\alpha}(t)}} \right)$, which completes the proof.
\end{proof}
















\begin{comment}
    



\subsection{Tweedie's Approach on the Variance preserving SDE}
\label{proof_tweedie_mean}

\paragraph{Derivation of $\rmX'_{t}$ for the SDE}

Recall the following SDE for the forward diffusion process of $\rmX'$

\begin{equation}
\label{true_forward_sde_of_x_prime_recall}
    \mathrm{d} \rmX'_{t} =  -\frac{1}{2} \beta(t) \rmX'_{t} \, \mathrm{d}t + \sqrt{\beta(t
)} \, \mathrm{d} \rmW_{t},
\end{equation}


where $\mathrm{d} \rmW_{t}$ is a standard Brownian motion and $\beta(t)$ is a time-dependent function.

\paragraph{Step 1: Rearrange the SDE}
The SDE can be rewritten as:

\begin{equation}
\mathrm{d} \rmX'_{t} = \left( -\frac{1}{2} \beta(t) \rmX'_{t} \right) \mathrm{d}t + \sqrt{\beta(t)} \, \mathrm{d} \rmW_{t}.
\end{equation}

This SDE has a time-dependent drift term $-\frac{1}{2} \beta(t) \rmX'_{t}$ and a time-dependent diffusion coefficient $\sqrt{\beta(t)}$.

\paragraph{Step 2: Solve the SDE Using the Integrating Factor Method}
We can solve this SDE using the integrating factor method. Consider the change of variables:

\begin{equation}
y_t = f(t) \rmX'_{t},
\end{equation}

where $f(t)$ is a deterministic function of $t$ chosen to eliminate the time-dependent drift term. We choose $f(t)$ such that:

\begin{equation}
df(t) = \frac{1}{2} \beta(t) f(t) \, \mathrm{d}t \quad \Rightarrow \quad f(t) = \exp\left( \frac{1}{2} \int_0^t \beta(s) \, ds \right).
\end{equation}

Transforming the SDE using $y_t = f(t) \rmX'_{t}$, we get:

\begin{equation}
dy_t = f(t) \sqrt{\beta(t)} \, \mathrm{d} \rmW_{t}.
\end{equation}

\paragraph{Step 3: Solve for $y_t$}
Integrating both sides from 0 to $t$:

\begin{equation}
y_t = y_0 + \int_0^t \exp\left( \frac{1}{2} \int_0^s \beta(u) \, du \right) \sqrt{\beta(s)} \, dw_s.
\end{equation}

Since $y_0 = f(0) x_0 = x_0$, we have:

\begin{equation}
y_t = x_0 + \int_0^t \exp\left( \frac{1}{2} \int_0^s \beta(u) \, du \right) \sqrt{\beta(s)} \, dw_s.
\end{equation}

\paragraph{Step 4: Solve for $\rmX'_{t}$}
Recall that $y_t = f(t) \rmX'_{t}$, so:

\begin{equation}
\rmX'_{t} = \frac{y_t}{f(t)}.
\end{equation}

Substitute $f(t) = \exp\left( \frac{1}{2} \int_0^t \beta(s) \, ds \right)$:

\begin{equation}
\rmX'_{t} = \exp\left( -\frac{1}{2} \int_0^t \beta(s) \, ds \right) \left( x_0 + \int_0^t \exp\left( \frac{1}{2} \int_0^s \beta(u) \, du \right) \sqrt{\beta(s)} \, dw_s \right).
\end{equation}

Define $\bar{\alpha}(t)$ as:

\begin{equation}
\bar{\alpha}(t) = \exp\left( -\int_0^t \beta(s) \, ds \right).
\end{equation}

Thus, the solution can be rewritten as:

\begin{equation}
\rmX'_{t} = \sqrt{\bar{\alpha}(t)} x_0 + \sqrt{\bar{\alpha}(t)} \int_0^t \frac{\sqrt{\beta(s)}}{\sqrt{\bar{\alpha}(s)}} \, dw_s.
\end{equation}

\paragraph{Step 2.3: Simplifying the Integral Using the Definition of $\bar{\alpha}(t)$}

Let’s simplify the integral term in the solution using the definition of $\bar{\alpha}(t)$. Recall that:

\begin{equation}
\bar{\alpha}(t) = \exp\left( -\int_0^t \beta(s) \, ds \right).
\end{equation}

For the integral term in our solution:

\begin{equation}
\sqrt{\bar{\alpha}(t)} \int_0^t \frac{\sqrt{\beta(s)}}{\sqrt{\bar{\alpha}(s)}} \, dw_s,
\end{equation}

the variance is given by:

\begin{equation}
\text{Var} \left( \sqrt{\bar{\alpha}(t)} \int_0^t \frac{\sqrt{\beta(s)}}{\sqrt{\bar{\alpha}(s)}} \, dw_s \right) = \bar{\alpha}(t) \int_0^t \left( \frac{\sqrt{\beta(s)}}{\sqrt{\bar{\alpha}(s)}} \right)^2 \, ds.
\end{equation}

Substitute $\bar{\alpha}(s)$:

\begin{equation}
\bar{\alpha}(s) = \exp\left( - \int_0^s \beta(u) \, du \right).
\end{equation}

Thus, the integral becomes:

\begin{equation}
\bar{\alpha}(t) \int_0^t \frac{\beta(s)}{\bar{\alpha}(s)} \, ds = \bar{\alpha}(t) \int_0^t \beta(s) \exp\left( \int_0^s \beta(u) \, du \right) \, ds.
\end{equation}

Recognizing that $\exp\left( \int_0^s \beta(u) \, du \right) = \frac{1}{\bar{\alpha}(s)}$, we obtain:

\begin{equation}
\bar{\alpha}(t) \int_0^t \beta(s) \frac{1}{\bar{\alpha}(s)} \, ds = \bar{\alpha}(t) \left( \frac{1}{\bar{\alpha}(t)} - 1 \right).
\end{equation}

Since $\frac{1}{\bar{\alpha}(t)} = \exp\left( \int_0^t \beta(s) \, ds \right)$, the integral simplifies to:

\begin{equation}
\bar{\alpha}(t) \left( \frac{1}{\bar{\alpha}(t)} - 1 \right) = 1 - \bar{\alpha}(t).
\end{equation}

This shows that the variance of the integral term is $1 - \bar{\alpha}(t)$.

\paragraph{Step 5: Derivation of $\sqrt{1 - \bar{\alpha}(t)}$}
The variance calculation shows that the noise term’s contribution has variance $1 - \bar{\alpha}(t)$. Therefore, the integral term can be represented as:

\begin{equation}
\sqrt{\bar{\alpha}(t)} \int_0^t \frac{\sqrt{\beta(s)}}{\sqrt{\bar{\alpha}(s)}} \, dw_s = \sqrt{1 - \bar{\alpha}(t)} \, z,
\end{equation}

where $z \sim \mathcal{N}(0, 1)$ is a standard normal random variable.

\paragraph{Step 6: Introducing $z \sim \mathcal{N}(0, 1)$}
The term $z \sim \mathcal{N}(0, 1)$ arises because:

1. The stochastic integral $\int_0^t \frac{\sqrt{\beta(s)}}{\sqrt{\bar{\alpha}(s)}} \, dw_s$ is itself normally distributed, with mean 0 and variance $1 - \bar{\alpha}(t)$.
2. Any linear combination of independent Gaussian variables is itself Gaussian. Therefore, the stochastic integral can be written as $\sqrt{1 - \bar{\alpha}(t)} \, z$ for some standard normal random variable $z \sim \mathcal{N}(0, 1)$.
3. This form allows us to capture the effect of the noise term in a simplified manner, with $z$ representing the randomness of the Brownian motion over time.

\paragraph{Step 7: Final Solution of $\rmX'_{t}$}
The final solution of the SDE is:

\begin{equation}
\rmX'_{t} = \sqrt{\bar{\alpha}(t)} x_0 + \sqrt{1 - \bar{\alpha}(t)} z,
\end{equation}

where $z \sim \mathcal{N}(0, 1)$.

This form clearly separates the deterministic part, weighted by $\sqrt{\bar{\alpha}(t)}$, from the stochastic part, weighted by $\sqrt{1 - \bar{\alpha}(t)}$.

\end{comment}





















\subsection{Discretization of the SDE for Importance Sampling}
\label{appendix_sde_disrectization}
To generate samples following the importance sampling PDF $q$, the proposed SDE can be implemented via the Euler-Maruyama method or various existing discretization methods. For example, the Euler-Maruyama scheme discretizes the time interval $[0,T]$ with step size $\Delta t$, allowing iterative solutions to the SDE. In this section, we consider $T \in \mathbb{N}$ (a positive integer) and adopt a unit time discretization, i.e., $\Delta t = 1$. Recall the proposed SDE which is given as follows.
\begin{align}
\label{propoed_sde_to_be_discretized}
     &\mathrm{d} \rmX_{t} = -\frac{\beta(t)}{2} \left[  \rmX_{t} +2  \nabla_{\rmX_{t}} \log \tilde{q}_{t}(\rmX_{t}) \right]\, \mathrm{d}t + \sqrt{\beta(t )} \, \mathrm{d} \tilde{\rmW}_{t}.
 \end{align}

Utilizing the discretization approach in \cite{song2021score} based on the Euler-Maruyama method, we can approximate the proposed SDE in (\ref{propoed_sde_to_be_discretized}) for the backward diffusion process. Specifically, for $t = T, T-1, \ldots, 1$ with unit time discretization, the iterative update is given by
\begin{align}
    \label{discretization_reverse_Euler_method_01}
     \rmX_{t-1} = \rmX_{t} +  \frac{\beta(t)}{2}\rmX_{t} + \beta(t) \nabla_{\rmX_{t}} \log \tilde{q}_{t}(\rmX_{t}) + \sqrt{\beta(t)} \rmZ
 \end{align}
 where $\rmZ \sim \mathcal{N}(\mathbf{0}, \rmI)$ represents a standard Gaussian random vector.

This discretization can also be viewed through the framework of the \emph{ancestral sampling method} introduced in \cite{ho2020ddpm, song2021score}. By neglecting higher-order terms (e.g., second-order and above) of $\beta(t)$, the update rule can be reformulated as
\begin{align}
    \label{discretization_ddpm_style_01}
     \rmX_{t-1} = \frac{1}{\sqrt{1-\beta(t)}}(\rmX_{t} + \beta(t) \nabla_{\rmX_{t}} \log \tilde{q}_{t}(\rmX_{t})) + \sqrt{\beta(t)} \rmZ.
\end{align}

Given that the proposed method is fundamentally rooted in approximating the score function, various discretization strategies can be seamlessly integrated. For example, the sampling procedure outlined in (\ref{discretization_ddpm_style_01}) can be implemented using Algorithm \ref{algorithm:ddpm_style_importance_sampling}, where $\rvz$ represents a realization of the Gaussian random vector.

Specifically, Algorithm \ref{algorithm:ddpm_style_importance_sampling} takes the score function of the base distribution and operates over $T$ time steps with unit time discretization. Additionally, it incorporates a parameter $\epsilon$ for approximating Hessian-vector multiplication and initializes the state $\rvx_{T}$ by sampling from a normal distribution. During each time step $t$, the following steps are executed: \underline{Line 2}: Noise $\rvz$ is sampled from a standard normal distribution to simulate the stochastic process. For $t=1$, this noise is set to the zero vector. \underline{Line 3}: The conditional mean value of $\rmX'_{0}$ is computed using the given score function. \underline{Line 4}: The gradient of the importance weight function with respect to the conditional mean of $\rmX'_{0}$ is computed via a single gradient backpropagation step. \underline{Line 5}: The score function of the importance sampling PDF is approximated using the given or precomputed values. This requires an additional inference of the score function of the original PDF for $\rvx_{t} + \epsilon \Delta_{l}$. \underline{Line 6}: The state $\rvx_{t}$ is updated to $\rvx_{t-1}$ according to the discretized representation of the SDE.


\begin{algorithm*}[t]
  \caption{Score-based Importance Sampling - a discretized solution}
\label{algorithm:ddpm_style_importance_sampling}
  \begin{algorithmic}[1]
    \Require Score function for original distribution $\nabla_{\rvx} \log p_{t}(\rvx)$, importance weight function $l(\rvx)$, $T$, $\beta(t)$, $\epsilon$, initialization $\rvx_{0} \sim \mathcal{N}(\mathbf{0}, \rmI)$.
    %
    \vspace{.3em}
    \Ensure $\rvx_{0}$
    
    %\State Initialize $\displaystyle \vb, \rmG$
    %\Ensure Something.  
    % \State initialization?  
    %\State Output? 
    
    \For{$t=T$ {\bfseries to} $1$}  
        
        \vspace{.1em}
        %------------------------------------------ --------------------------------------------{\emph{Model Update}} 
        
        
        \vspace{.1em}
            \State{If $t > 1$, sample $\rvz$ from $\mathcal{N}(\mathbf{0}, \rmI)$, else, set $\rvz=\mathbf{0}$ }

            \State $\bar{\rvx}'_{0}\vert_{\rvx_{t},t}= \frac{1}{\sqrt{\bar{\alpha}(t)}}(\rvx_{t} + (1-\bar{\alpha}(t)) \nabla_{\rvx_{t}} \log p_{t}(\rvx_{t}))$
             \Comment{\emph{mean estimation}}

            \State $\Delta_{l} = \nabla_{\bar{\rvx}'_{0}\vert_{\rvx_{t},t}} \log l(\bar{\rvx}'_{0}\vert_{\rvx_{t},t})$
             \Comment{\emph{single-step backpropagation}}

             \State $\nabla_{\rvx_{t}} \log \tilde{q}_{t}(\rvx_{t}) = \nabla_{\rvx_{t}} \log p_{t}(\rvx_{t}) + \frac{\Delta_{l}}{\sqrt{\bar{\alpha}(t)}}  + \frac{\nabla \log p_{t}(\rvx_{t} + \epsilon \Delta_{l}) - \nabla_{\rvx_{t}} \log p_{t}(\rvx_{t})}{\epsilon (1-\bar{\alpha}(t))^{-1} \sqrt{\bar{\alpha}(t)}} $
             \Comment{\emph{score function approximation}}
             
            
            \State $\rvx_{t-1} = \frac{1}{\sqrt{1-\beta(t)}}(\rvx_{t} + \beta(t) \nabla_{\rvx_{t}} \log \tilde{q}_{t}(\rvx_{t})) + \sqrt{\beta(t)} \rvz$
             \Comment{\emph{update}}
        
%
    \EndFor
  \end{algorithmic}
  %\vspace{-3mm}
\end{algorithm*}








\begin{comment}

\subsection{Miscellaneous}




\begin{align}
    \rvx_{t} = x + \int_{0}^{t} b_{1}(\rvx_{s}, s) \, \mathrm{d}s + \int_{0}^{t} \sigma(s) \, \mathrm{d} \rvw_{s}, \\
    \rvy_{t} = x + \int_{0}^{t} b_{2}(\rvy_{s}, s) \, \mathrm{d}s + \int_{0}^{t} \sigma(s) \, \mathrm{d} \rvw_{s}.
\end{align}
By Minkowski inequality, we have
\begin{align}
    & \left \Vert \rvx_{t} - \rvy_{t} \right \Vert_{p} \le \left \Vert \int_{0}^{t} b_{1}(\rvx_{s}, s) -  b_{2}(\rvy_{s}, s)  \,\mathrm{d}s  \right \Vert_{p} \\
    & \le \left \Vert \int_{0}^{t} b_{1}(\rvx_{s}, s) -  b_{1}(x, 0)   \,\mathrm{d}s \right \Vert_{p} + t \Delta b(x) \\ &+ \left \Vert \int_{0}^{t} b_{2}(\rvx_{s}, s) -  b_{2}(x, 0) \,\mathrm{d}s \right \Vert_{p} 
\end{align}



Suppose Assumption \ref{assumption_lipschitz} holds and $\rvx$ and $\rvy$ be the solution to the two SDEs (\ref{true_sde}) and (\ref{estimated_sde}), respectively. Consider a initial state $x$ for both random process. Then we have the following almost surely.
\begin{align}
    \Vert \rvx_{t} - \rvy_{t} \Vert_{p} \le C.
\end{align}
% proof here.
\begin{proof}
Based on the integral representation of the solutions, we have
\begin{align}
    \rvx_{t} = x + \int_{0}^{t} b_{1}(\rvx_{s}, s) \, \mathrm{d}s + \int_{0}^{t} \sigma(s) \, \mathrm{d} \rvw_{s}, \\
    \rvy_{t} = x + \int_{0}^{t} b_{2}(\rvy_{s}, s) \, \mathrm{d}s + \int_{0}^{t} \sigma(s) \, \mathrm{d} \rvw_{s}.
\end{align}
By Minkowski inequality, we have
\begin{align}
    & \left \Vert \rvx_{t} - \rvy_{t} \right \Vert_{p} \le \left \Vert \int_{0}^{t} b_{1}(\rvx_{s}, s) -  b_{2}(\rvy_{s}, s)  \,\mathrm{d}s  \right \Vert_{p} \\
    & \le \left \Vert \int_{0}^{t} b_{1}(\rvx_{s}, s) -  b_{1}(x, 0)   \,\mathrm{d}s \right \Vert_{p} + t \Delta b(x) \\ &+ \left \Vert \int_{0}^{t} b_{2}(\rvx_{s}, s) -  b_{2}(x, 0) \,\mathrm{d}s \right \Vert_{p} 
\end{align}



\end{proof}








\begin{itemize}
    \item \( \rmX'_{t} \in \mathbb{R}^n \) is the state of the process at time \( t \).
    \item \( b(\rmX'_{t}, t) \) is the drift term, which determines the deterministic part of the evolution.
    \item \( \sigma(\rmX'_{t}, t) \) is the diffusion coefficient, a matrix-valued function that governs the stochastic dynamics.
    \item \( \mathrm{d} \rmW_{t} \) is an \( m \)-dimensional Wiener process.
\end{itemize}

\paragraph{Infinitesimal Generator of the SDE}
The infinitesimal generator \( A \) of a diffusion process described by the SDE above is defined as:
\begin{equation}
    A f(x) = \lim_{t \to 0} \frac{\mathbb{E}_x[f(\rmX'_{t})] - f(x)}{t},
\end{equation}
for a sufficiently smooth function \( f : \mathbb{R}^n \to \mathbb{R} \). This can be further expressed as:
\begin{equation}
    A f(x) = \sum_{i=1}^n b_i(x, t) \frac{\partial f}{\partial x_i}(x) + \frac{1}{2} \sum_{i,j=1}^n (D(x, t))_{ij} \frac{\partial^2 f}{\partial x_i \partial x_j}(x),
\end{equation}
where \( D(x, t) = \sigma(x, t) \sigma^T(x, t) \) is the covariance matrix of the process.

\paragraph{Conditional Diffusion Process and Measure Transformation}
Let \( (\rmX'_{t}, \mathcal{F}_t, P_{x, y}^T) \) be a conditional diffusion process conditioned on reaching a terminal value \( y \) at time \( T \). The infinitesimal generator \( A \) of this process is expressed as:
\begin{equation}
    A = L_b + \nabla_g \log p(z, T - t, y) \nabla_g,
\end{equation}
where:
\begin{itemize}
    \item \( L_b \) is the infinitesimal generator of the original SDE.
    \item \( \nabla_g \) denotes the gradient in the \( g \)-norm.
    \item \( \log p(z, T - t, y) \) is the log of the transition density function conditioned on reaching \( y \) at time \( T \).
\end{itemize}

\paragraph{Measure Transformation and New Probability Measure}
Define a measurable vector field \( c(x) \) on \( \mathbb{R}^n \) as:
\begin{equation}
    c(x) = \sum_{i} c_i(x) \frac{\partial}{\partial x_i}.
\end{equation}
This vector field \( c \) is used to introduce a new probability measure \( Q_x \), defined as:
\begin{equation}
    \frac{dQ_x}{dP_x} \bigg|_{\mathcal{F}_t} = \exp \left( \int_0^t \langle c(X_s), dM_s \rangle_g - \frac{1}{2} \int_0^t |c|_g^2 (X_s) \, ds \right),
\end{equation}
where \( \langle c(X_s), dM_s \rangle_g \) is the inner product of \( c(X_s) \) with the martingale \( dM_s \) under the \( g \)-metric, and \( |c|_g^2 (X_s) \) is the squared norm of \( c \).

\paragraph{Cameron-Martin Formula and New SDE}
By the Cameron-Martin formula, the new process \( (\rmX'_{t}, \mathcal{F}_t, Q_x) \) becomes an \( L_b + c \)-diffusion process, which has the following SDE:
\begin{equation}
    \mathrm{d} \rmX'_{t} = (b(\rmX'_{t}, t) + c(\rmX'_{t})) \, \mathrm{d}t + \sigma(\rmX'_{t}, t) \, \mathrm{d} \rmW_{t}^Q,
\end{equation}
where \( \mathrm{d} \rmW_{t}^Q \) is a Wiener process under the new measure \( Q_x \). The transition density of the modified process is denoted as \( p_{b+c}(x, t, y) \).

\end{comment}






\section{Implementations and Experiments}
\label{appendix_all_experiments}






\subsection{Experiments on Section \ref{experiments_closed_form}}
\label{appendix_experiments_details_spiral}


\subsubsection{Implementation of Proposed Method.} 
The score function of the original PDF, $\nabla_{\rvx} \log p_{t}(\rvx)$, was learned via Denoising Diffusion Probabilistic Models (DDPM) and its corresponding SDE discretization method. The score function processes input pairs consisting of 2-dimensional data and timestep information, passing them through five fully connected layers. Each hidden layer employs ReLU activations, while the output layer utilizes a linear activation function. The hyperparameters for score function training were set as follows: $T = 10^{3}$, batch size $= 1024$, training epochs $= 300$, and learning rate of $10^{-3}$. 
The normalized time variable, $\tau \in [0, 1]$, is defined as $\tau = \frac{t}{T}$. The decay function is formulated as  $f_{d}(\tau) = \cos^2\left(\frac{\pi}{2} \cdot \frac{\tau + \epsilon_{d}}{1 + \epsilon_{d}}\right),$  where $\epsilon_{d} > 0$ is a small constant introduced for numerical stability. The parameter $\beta(t)$ is then calculated as:  
$\beta(t) = \max\left(0, \min\left(1, 1 - \frac{f_{d}(\tau_{t+1})}{f_{d}(\tau_t)}\right)\right),
$ where $\tau_t = \frac{t}{T}$ and $\tau_{t+1} = \frac{t+1}{T}$.
%
Subsequent to learning the score function, Algorithm \ref{algorithm:ddpm_style_importance_sampling} was applied for importance sampling.  



\paragraph{Datasets.} 
We constructed a synthetic dataset comprising two intertwined spirals, normalized to the range $[-1, 1]$ for both $x$ and $y$ coordinates. The dataset is defined as 
$\rmX = ((2\phi + \pi) \cos(\phi)+n_{1}, (2\phi + \pi) \sin(\phi)+n_{2} )^{\top}$, $\phi \sim  \mathcal{U}([0, 2\pi])$, and $n_{1}, n_{2}\sim \mathcal{N}(0,0.1)$. A total of $10^{5}$ samples were generated to train the score function representing the original data distribution.

The remaining datasets, including 8-Gaussians, Circles, and Pinwheel, are adopted from \cite{grathwohl2019scalable, perugachi2021invertible}, with each dataset consisting of $10^{5}$ samples. 

\paragraph{Sampling from the Optimal Importance Sampling Distribution.} When the true distribution $p(\rvx)$ is explicitly known and sampling from the base probability density function (PDF) is possible, we can construct an importance sampling distribution $q(\rvx)$. This is feasible under the assumption that there exists a known constant $M$ satisfying  $    l(\rvx) \leq M, \quad \forall \rvx \in \mathcal{X}$.

Under these conditions, importance sampling can be performed via the acceptance-rejection method as follows:
\begin{enumerate}
    \item {Sampling from $p(\rvx)$:} Draw $\rvx \sim p(\rvx)$.
    \item {Computing Acceptance Probability:} Compute  
        $a(\rvx) = \frac{l(\rvx)}{M}$,    ensuring $0 \leq a(\rvx) \leq 1$.
    \item {Acceptance/Rejection Step:} Draw $u \sim \mathcal{U}(0,1)$. Accept $\rvx$ if $u \leq a(\rvx)$; otherwise, reject and repeat.
\end{enumerate}

This acceptance-rejection method for importance sampling is infeasible in high-dimensional scenarios in general. Specifically, in cases such as high-resolution image generation, the exact domain $\mathcal{X}$ may be unknown, or the acceptance probability $a(\rvx)$ may be extremely small, leading to prohibitively high computational costs. 

However, this method remains feasible for the 2D synthetic datasets considered in this section, where the exact domain, base distribution, and importance weight function are known in closed form. This controlled setting allows us to quantitatively measure the accuracy of importance sampling and serves as a benchmark for evaluating training-free importance weighting methods.



\paragraph{Importance Weight Functions.} 
The importance weight function is defined as $l_1(\rvx) = \Vert \rvx \Vert^{2}$, where instances with higher norm values are assigned greater importance weights. We also use $l_2(\rvx) \!=\! \Sigma_{i\in [d]} [\rvx]_{i}+2$ which is the element summation.



\subsubsection{Baselines}
\label{appendix:baseline_implementation}
To assess the performance of the proposed method, we benchmark it against several representative density estimation models optimized via the Cross-Entropy Minimization (CEM) framework \cite{botev2013cross}. This optimization paradigm is fundamentally equivalent to maximizing the weighted log-likelihood of the importance sampling distribution. Specifically, we consider an importance sampling distribution parameterized as $q(\rvx; \theta)$, where $\theta$ represents the set of trainable parameters. 

The CEM method aims to minimize the Kullback–Leibler (KL) divergence between the optimal importance sampling distribution, denoted as $q_0$, and the parameterized importance sampling PDF $q(\rvx; \theta)$. Formally, this optimization problem can be reformulated as follows \cite{botev2013cross}.

\begin{equation}
\theta^* = \arg \max_{\theta} \mathbb{E}_{\rvx \sim p(\rvx)} [l(\rvx) \log q(\rvx; \theta)],
\end{equation}

where $\theta^*$ denotes the optimal set of parameters. This formulation reveals that the problem reduces to maximizing a weighted log-likelihood function with respect to $\theta$. Consequently, the CEM framework effectively refines the importance sampling distribution by aligning it with the underlying data distribution, thereby enhancing density estimation performance.

All baseline models utilize the AdamW optimizer with a learning rate of $10^{-3}$, and their base distribution for normalizing flows is the standard normal distribution. With this, we use CEM with various different types of density estimation architectures as described below.

\paragraph{Architectures.} For the Masked Autoencoder for Distribution Estimation (MADE) combined with the Masked Affine Autoregressive transformation (MADE-MAA) \cite{germain2015made}, we apply a logit transformation followed by ten consecutive distribution transformations. Each transformation level employs a random permutation and a Masked Affine Autoregressive Transform, which is constructed based on MADE.

The Neural Importance Sampling (NIS) model \cite{muller2019neural} leverages the Piecewise Quadratic Coupling Transform, preceded by a logit transformation. Each transformation employs a Piecewise Quadratic Coupling Transform with an alternating binary mask, using a single-block residual network with ReLU activation and a dropout probability of 0.1. The quadratic splines are configured with eight bins and linear tails for extrapolation.

Neural Spline Flows (NSF) \cite{durkan2019neural}, which utilize spline transformations to model complex distributions, follow the same setup as NIS but replace the Piecewise Quadratic Coupling Transform with a Piecewise Rational Quadratic Coupling Transform. The NSF model is parameterized with eight bins, linear tails, and a tail bound of 3.0.

Cubic Spline Flow (CSF) \cite{durkan2019cubic} follows the same configuration as NIS and NSF, with the distinction that it employs a Cubic Coupling Transform for all ten transformation levels.

The Transformer Neural Autoregressive density estimation method (TNA) \cite{patacchiola2024transformer} incorporates masked autoregressive multi-head attention. We modify the architecture proposed in \cite{patacchiola2024transformer} to make it have similar computation complexity with the other baselines as follows. The TNA model employs a masked linear projection to transform inputs into an embedded representation, which is then processed through multiple transformer encoder layers with learned positional embeddings. The final output is obtained via a projection head that generates pseudo-parameters for density estimation. The input and the final projections are masked linear layer and the multi-head attention is also masked during the training to ensure the autoregressive feature. The masked linear layer is followed by the spline transformation.

All of the density transformation modules are invertible, enabling their utilization for generative modeling. In implementation, we employ a total of 10 transformation layers for learning. As shown numerically in Table \ref{performance_table} and visually in Fig. \ref{fig:is_density_l2}, the architecture exhibits strong representational capacity in modeling complex densities.



\paragraph{Comparison Fairness.}  
The baseline methods heavily rely on training via CEM, whereas our proposed method is entirely training-free. As a result, direct performance comparisons inherently involve some degree of unfairness, as the baselines benefit from explicit training while our method does not. However, the key objective of this comparison is not to assess computational efficiency or model architecture but rather to evaluate how closely our training-free approach can approximate the optimal importance sampling distribution compared to the training-based methods.

%We acknowledge that differences exist in the generative processes and model architectures. In particular, while the baselines utilize various architectures optimized for density estimation, our method employs a simple linear layer with activation functions for the denoising function in the diffusion backward process. Despite these discrepancies, we ensure that the number of training samples used for learning the importance sampling distributions in the baselines matches the number of samples used to construct the base score model in our approach. 



\paragraph{Additional Results.}
%\label{appendix:2d_dataset_additional_results}



\begin{figure*}[t]
    \centering
    \setlength{\tabcolsep}{2pt}  % Adjust horizontal spacing between images
    \begin{tabular}{c}
        Spiral \\
        \begin{tabular}{ccccccc}
            %\includegraphics[width=0.125\textwidth]{fig/toy_datasets/appendix/gt_spiral.jpg} &
            \includegraphics[width=0.16\textwidth]{fig/toy_datasets/appendix/gt_spiral_l2.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_spiral_MADE-MMA_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_spiral_NIS_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_spiral_NSF_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_spiral_TNA_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_spiral_Ours_l2_0.jpg} \\
        \end{tabular} \\
        % Green Channel row
        Circles \\
        \begin{tabular}{cccccc}
            %\includegraphics[width=0.125\textwidth]{fig/toy_datasets/appendix/gt_circles.jpg} &
            \includegraphics[width=0.16\textwidth]{fig/toy_datasets/appendix/gt_circles_l2.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_circles_MADE-MMA_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_circles_NIS_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_circles_NSF_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_circles_TNA_l2_2.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_circles_Ours_l2_0.jpg} \\
        \end{tabular} \\
        
        % Blue Channel row
        PinWheel \\
        \begin{tabular}{cccccc}
            %\includegraphics[width=0.125\textwidth]{fig/toy_datasets/appendix/gt_pinwheel.jpg} &
            \includegraphics[width=0.16\textwidth]{fig/toy_datasets/appendix/gt_pinwheel_l2.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_pinwheel_MADE-MMA_l2_2.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_pinwheel_NIS_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_pinwheel_NSF_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_pinwheel_TNA_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_pinwheel_Ours_l2_0.jpg} \\
        \end{tabular} \\

        
        8-Gaussians\\
        \begin{tabular}{cccccc}
            \includegraphics[width=0.16\textwidth]{fig/toy_datasets/appendix/gt_8gaussians_l2.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_8gaussians_MADE-MMA_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_8gaussians_NIS_l2_1.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_8gaussians_NSF_l2_1.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_8gaussians_TNA_l2_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix/est_8gaussians_Ours_l2_1.jpg} \\
        \end{tabular} \\
    \end{tabular}

    
    \caption{
    Ground truth importance sampling density (Left) and estimated density results obtained from MADE-MMA, NIS, NSF, TNA, and the proposed method (from second to right), with importance weight function $l_1(\rvx) = \Vert \rvx \Vert^2$. Our training-free approach effectively recovers the ground truth importance sampling distribution without any additional training for a given base score function.
}
    \label{fig:is_density_l2}
\end{figure*}



\begin{figure*}[t]
    \centering
    \setlength{\tabcolsep}{2pt}  % Adjust horizontal spacing between images
    \begin{tabular}{c}
        Spiral \\
        \begin{tabular}{ccccccc}
            %\includegraphics[width=0.125\textwidth]{fig/toy_datasets/appendix/gt_spiral.jpg} &
            \includegraphics[width=0.16\textwidth]{fig/toy_datasets/appendix_elementsum/gt_spiral_elementsum.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_spiral_MADE-MMA_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_spiral_NIS_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_spiral_NSF_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_spiral_TNA_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_spiral_Ours_elementsum_0.jpg} \\
        \end{tabular} \\
        % Green Channel row
        Circles \\
        \begin{tabular}{cccccc}
            %\includegraphics[width=0.125\textwidth]{fig/toy_datasets/appendix/gt_circles.jpg} &
            \includegraphics[width=0.16\textwidth]{fig/toy_datasets/appendix_elementsum/gt_circles_elementsum.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_circles_MADE-MMA_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_circles_NIS_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_circles_NSF_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_circles_TNA_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_circles_Ours_elementsum_0.jpg} \\
        \end{tabular} \\
        
        % Blue Channel row
        PinWheel \\
        \begin{tabular}{cccccc}
            %\includegraphics[width=0.125\textwidth]{fig/toy_datasets/appendix/gt_pinwheel.jpg} &
            \includegraphics[width=0.16\textwidth]{fig/toy_datasets/appendix_elementsum/gt_pinwheel_elementsum.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_pinwheel_MADE-MMA_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_pinwheel_NIS_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_pinwheel_NSF_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_pinwheel_TNA_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_pinwheel_Ours_elementsum_0.jpg} \\
        \end{tabular} \\

        
        8-Gaussians\\
        \begin{tabular}{cccccc}
            \includegraphics[width=0.16\textwidth]{fig/toy_datasets/appendix_elementsum/gt_8gaussians_elementsum.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_8gaussians_MADE-MMA_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_8gaussians_NIS_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_8gaussians_NSF_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_8gaussians_TNA_elementsum_0.jpg} &
            \includegraphics[width=0.15\textwidth]{fig/toy_datasets/appendix_elementsum/est_8gaussians_Ours_elementsum_0.jpg} \\
        \end{tabular} \\
    \end{tabular}

    
    \caption{
    Ground truth importance sampling density (Left) and estimated density results obtained from MADE-MMA, NIS, NSF, TNA, and the proposed method (from second to right), with importance weight function $l_2(\rvx)$.
}
    \label{fig:is_density_elementsum}
\end{figure*}






In Fig. \ref{fig:is_density_l2}, we present the estimated densities obtained from baseline methods alongside our proposed approach, where the importance weight function is defined as $l(\rvx) = \Vert \rvx \Vert^2$. The leftmost column illustrates the ground truth importance sampling density, computed via the acceptance-rejection method. The subsequent columns display the estimated densities produced by MADE-MMA, NIS, NSF, and TNA, respectively, with our method depicted in the rightmost column. Notably, unlike conventional approaches that directly train on the importance sampling distribution, our method derives results solely from the score function of the base distribution.

Similarly, in Fig. \ref{fig:is_density_elementsum}, we present the estimated densities under a different importance weight function. The leftmost column again shows the ground truth importance sampling density, followed by the estimated densities from MADE-MMA, NIS, NSF, and TNA, with our approach occupying the second-to-right column. Due to the modified importance weight function, the ground truth importance sampling distribution exhibits higher density in the first quadrant. It is important to emphasize that the proposed method is capable of generating importance samples without requiring any additional training. Specifically, we utilize the same trained base score model from the experiment with $l_{1}$ and simply modify the importance function to $l_{2}$. 

The results indicate that, even without additional training, our method remains competitive with, and often outperforms, state-of-the-art learning-based density estimation methods.

These findings demonstrate that the proposed training-free approach effectively reconstructs the importance sampling distribution, providing empirical validation of the efficacy of the approximated backward diffusion process.





\subsection{Experiments on CSI Dataset}
\label{appendix_experiments_details_csi}
The primary objective of this experiment is to demonstrate the effectiveness of the proposed importance sampling method in targeting rare instances with specific characteristics. In this context, higher importance weights are assigned to instances exhibiting poor task performance, thereby enabling selective feature analysis. This capability is particularly valuable for applications in industrial and real-world tasks.

Specifically, we consider the Wireless Channel State Information (CSI) compression task, which represents a key challenge in optimizing next-generation communication systems \cite{csinet_ver1}, analogous to neural network-based image compression. CSI, which characterizes the dynamic attributes of the link between a transmitter (Tx) and receiver (Rx), varies according to their relative positions and the surrounding environment. Efficient compression at the Rx enables the Tx to design robust signals, thereby enhancing data transmission efficiency.

Recent advancements in neural autoencoder-based CSI compression methods have consistently demonstrated superior performance compared to traditional techniques such as compressive sensing. However, the compression performance varies significantly across CSI instances. While some instances exhibit minimal distortion from compression, others show poor performance with high distortion. This performance bias is widely known and problematic, prompting research into methods for addressing it \cite{kim2022learning}.

Importance sampling can facilitate the analysis of such samples, specifically those exhibiting high distortion in the compression task, by assigning higher sampling probabilities to these instances. This approach enables a more detailed analysis of compression performance, such as identifying specific features associated with high-distortion samples.


\paragraph{Dataset.} 
We generate a dataset which is a mixture of eight local datasets using Quadriga (QUAsi Deterministic RadIo channel GenerAtor) \cite{jaeckel2014quadriga, jaeckel2021quadriga}, with each dataset consisting of 20,000 training samples and 20,000 test samples. The base stations (BSs) utilize the 3GPP-3D antenna model with 32 antennas, while the user equipment (UEs) are equipped with a single omnidirectional antenna. The scenarios are defined by variations in center frequency, channel model, propagation condition, and the number of dominant scattering clusters $N_{\text{dc}}$ used for channel generation. The datasets include both Urban Micro-Cell and Urban Macro-Cell configurations, with line-of-sight (LOS) and non-line-of-sight (NLOS) propagation conditions. Specifically, for Urban Micro-Cell scenarios, we consider two configurations with center frequencies of 0.8 GHz and 2.4 GHz, both under LOS propagation, and $N_{\text{dc}}=10$. Additionally, for Urban Macro-Cell scenarios, two configurations with center frequencies of 0.8 GHz and 2.4 GHz are used under LOS propagation, with $N_{\text{dc}}=5$. For NLOS propagation, we use Urban Micro-Cell scenarios at 0.8 GHz and 2.4 GHz with $N_{\text{dc}}=50$, and Urban Macro-Cell scenarios at the same frequencies with $N_{\text{dc}}=40$. We consider 667 subcarriers with 15KHz subcarrier spacing.
This setup yields $667\times 32$ complex matrix for a single CSI instance $\rvx$. 






\paragraph{Importance weight function.} 
In this experiment, we assign higher importance weights to instances exhibiting poor compression performance as evaluated by a neural autoencoder (compressor). The autoencoder comprises an encoder $F_{\text{enc}}$ and a decoder $F_{\text{dec}}$. Using a distortion measure $D$, in this case, MSE, the distortion resulting from compression is defined as $D(F_{\text{dec}}(F_{\text{enc}}(\rvx)), \rvx)$. The autoencoder compresses a given CSI instance $\rvx$ into a 128-bit codeword, represented as $F_{\text{enc}}: \mathbb{R}^{d} \mapsto \{0,1\}^{128}$. By defining $l(\rvx) = D(F_{\text{dec}}(F_{\text{enc}}(\rvx)), \rvx)$,  we can assign higher importance weights to instances that exhibit significant distortion in the compression task performed by the autoencoder. The architecture of the autoencoder, similar to the conventional residual-connection-based autoencoder design for CSI compression \cite{csinet_ver1}, is detailed as follows.

\underline{Encoder}: To reduce the computation overhead, the encoder operates on a cropped CSI matrix in the angular-delay domain, processing it through three parallel convolutional paths. Each path comprises a Convolutional layer (Conv), Batch Normalization (BN), and Leaky ReLU (LR) activation. The outputs of these paths are concatenated along the channel dimension and passed through two additional blocks, each consisting of Conv, BN, and LR layers. The resulting feature map is vectorized and further transformed by a Fully Connected (FC) layer, producing a vector of size $\frac{128 N_{\text{Ebd}}}{\log_{2}(B)}$, where $B$ represents the base of the codeword representation, and $N_{\text{Ebd}}$ is the embedding dimension. This vector is then reshaped into a matrix of size $\frac{128}{\log_{2}(B)} \times N_{\text{Ebd}}$. Finally, it undergoes discretization via a quantizer.

\underline{Quantizer}: The quantization process maps the continuous representation to discrete embedding vectors using a codebook with $B$ unique vectors, each of dimension $N_{\text{Ebd}}$. Specifically, the output of the encoder is viewed as $128/\log_{2}(B)$ embedding vectors, each of size $N_{\text{Ebd}}$, and each vector is matched to the nearest codebook vector based on the smallest $L_{2}$-norm. This mapping generates the quantized representation. The quantization achieves 128-bit compression by encoding the data with indices corresponding to $B$ distinct embedding vectors. We set $B=4$ which ensures $128/\log_{2}(B) \in \mathbb{N}$ for implementation. The encoder then transmits the indices of the selected codebook vectors over the wireless feedback link to the base station (BS). These indices collectively form the codeword. 

\underline{Decoder}: At the BS, the received codeword is decoded by mapping the indices back to their corresponding embedding vectors in the codebook, reconstructing the quantized representation. This representation is then processed by the decoder’s neural layers. First, it is passed through a fully connected layer and reshaped to its pre-FC dimensions. Subsequently, it is refined using two Transposed Convolutional layers (ConvT), each with BN and LR activations. To further enhance the reconstruction of the CSI, the decoder employs a three-iteration refinement mechanism. Each iteration incorporates residual connections and inception blocks. The process begins with a Conv+BN+LR layer, followed by two parallel branches: one with two Conv+BN+LR layers and a final Conv+BN layer, and another with a single Conv+BN+LR layer and a Conv+BN layer. The outputs of these branches are concatenated and processed by a Conv+BN layer before being combined with results from previous iterations via residual connections. The final output is obtained through an LR activation, providing a refined estimate of the original CSI matrix $\rvx$. 




\begin{figure*}[htbp]
    \centering
    \setlength{\tabcolsep}{0pt}  % Adjust horizontal spacing between images
    \begin{tabular}{c}
        \textcolor{blue}{\textit{$\rmX' \sim p(\rvx)$}} \\[-1.5pt]
        \begin{tabular}{cccc}
            \includegraphics[width=0.25\textwidth]{fig/csi/appendix_samples/p(x)_sample_1_distortion_0.00022.pdf} &
            \includegraphics[width=0.25\textwidth]{fig/csi/appendix_samples/p(x)_sample_2_distortion_0.00013.pdf} &
            \includegraphics[width=0.25\textwidth]{fig/csi/appendix_samples/p(x)_sample_3_distortion_0.00025.pdf} &
            \includegraphics[width=0.25\textwidth]{fig/csi/appendix_samples/p(x)_sample_4_distortion_0.00021.pdf} \\
        \end{tabular} \\
        
        \textcolor{red}{\textit{$\rmX \sim q(\rvx)$}} \\[-1.5pt]
        \begin{tabular}{cccc}
            \includegraphics[width=0.25\textwidth]{fig/csi/appendix_samples/q(x)_sample_1_distortion_0.00127.pdf} &
            \includegraphics[width=0.25\textwidth]{fig/csi/appendix_samples/q(x)_sample_2_distortion_0.00081.pdf} &
            \includegraphics[width=0.25\textwidth]{fig/csi/appendix_samples/q(x)_sample_3_distortion_0.0007.pdf} &
            \includegraphics[width=0.25\textwidth]{fig/csi/appendix_samples/q(x)_sample_4_distortion_0.00177.pdf} \\
        \end{tabular} \\
    \end{tabular}    
    \caption{
    \textbf{\emph{Top two rows}}: CSI Samples generated from the original score function, $\mathbf{X}' \sim p(\mathbf{x})$. The first row represents the spatial-frequency domain image, while the second row represents the angular-delay domain image.  
\textbf{\emph{Bottom two rows}}: Randomly selected 4 samples generated using the proposed importance sampling method. The third row shows the spatial-frequency domain representation, and the fourth row shows the angular-delay domain representation. 
%
As illustrated in Fig.~\ref{fig:csi_samples_q}, the proposed importance sampling method generates high-distortion samples more frequently, thereby enabling selective feature analysis. Specifically, the importance sampling method produces samples with distortion values of $1.27 \times 10^{-3}$, $8.1 \times 10^{-4}$, $7.0 \times 10^{-4}$, and $1.77 \times 10^{-3}$ (bottom rows, from left to right). Notably, instances with compression distortion exceeding $10^{-3}$ are rare in the original distribution, where such instances occur with a probability of approximately $5\%$.
    }
    \label{fig:csi_samples_from_p_and_q}
\end{figure*}

\paragraph{Implementation details.} 
For training the score function, all hyperparameters and the neural network architecture are identical to those described in the experimental setup provided in Appendix \ref{appendix_experiments_details_mnist}, with the exception of the U-Net architecture used for training the original PDF's score function. Specifically, the U-Net architecture employs two ResNet layers per block and output channel sizes of 128, 128, 256, 256, 512, and 512 for successive blocks for downsampling and upsampling. 
The downsampling blocks consist of four regular ResNet downsampling blocks followed by a ResNet block with spatial self-attention, and one final regular ResNet downsampling block. The upsampling blocks include one regular ResNet upsampling block, followed by a ResNet upsampling block with spatial self-attention, and four additional regular ResNet upsampling blocks.




\paragraph{Additional results.}

On the top two rows of Fig. \ref{fig:csi_samples_from_p_and_q}, we present four randomly selected samples from the original distribution $p(\rvx)$, generated using the base score function. The first row illustrates the spatial-frequency representation of $\rvx$, where the x-axis contains 667 elements, and the y-axis contains 32 elements. The absolute values of the complex matrix elements are shown, normalized such that the highest values are represented by the brightest intensity. The second row depicts the angular-delay representation of the corresponding CSI instances. Note that applying zero-padding to the angular-delay representation, followed by a 2D Fourier Transform, results in the spatial-frequency representation shown in the first row.

The third row of Fig. \ref{fig:csi_samples_from_p_and_q} shows the spatial-frequency representation of samples generated from the importance sampling distribution, $q(\rvx)$, constructed using our proposed importance sampling method employing the neural autoencoder as the importance weight function. The fourth row presents the corresponding angular-delay representations of the samples from $q(\rvx)$.

From Fig. \ref{fig:csi_samples_q}, approximately 50\% of the samples drawn from the original distribution exhibit low distortion, approximately less than $2 \times 10^{-4}$, when analyzed using the neural autoencoder-based compression. These low-distortion samples are characterized by dominant low-delay components, as evident in the second row of Fig. \ref{fig:csi_samples_from_p_and_q}.

Our proposed importance sampling method demonstrates the ability to generate rare, high-distortion samples by leveraging the neural autoencoder. Specifically, Fig. \ref{fig:csi_samples_from_p_and_q} reveals that over 16\% of the samples from the importance sampling distribution exhibit high distortion, approximately $10^{-3}$, as quantified by the neural autoencoder. These high-distortion samples are typically associated with significant high-delay components and a scattered spatial-frequency pattern, as seen in the samples from $q(\rvx)$.


It is worth noting that the proposed importance sampling method is highly adaptable. In this example, the task performance itself (driven by the neural autoencoder) is directly utilized as the importance weight function, enabling concentrated sampling of a specific subset of instances. This methodology holds promise not only for feature analysis based on compression performance but also for a wide range of industrial applications.


















\subsection{Experiments on CelebA Dataset: Neural Classifier-Driven Sampling}
\label{appendix_experiments_details_celeba}



In this experiment, we investigate a potential application of our proposed training-free importance sampling method in scenarios where the importance function is parameterized by a complex neural network, specifically a neural classifier. Additionally, we consider a high-dimensional SGM to show the scalability of our approach.

Consider a setting where an SGM has been trained without any explicit supervision regarding class information—in this case, gender. While the SGM itself lacks any intrinsic notion of class, we assume the existence of an independent neural classifier capable of determining whether a given image is classified as ``man'' or not—i.e., a binary classifier.

Suppose we generate a large number of samples from the SGM and use the neural classifier to categorize them. If the classifier indicates a biased distribution, with approximately 35\% of the generated images classified as ``man,'' a natural question arises: can we modify the SGM’s sampling process to mitigate this bias and generate images classified as ``man'' more frequently, without any fine-tuning or re-training of the SGM?

This objective can be achieved using our proposed importance sampling method by defining the importance function $l(\rvx)$ as the logit output of the classifier. Specifically, we assign higher importance weights to samples classified as ``man'' based on the classifier.


\paragraph{Dataset.} 
The CelebA dataset \cite{liu2015celebafaceattributes} is utilized for training the classifier in our experiments. For the score function used in the generation process, we employ a pretrained score model based on DDPM \cite{ho2020ddpm}. This pretrained model is obtained from Huggingface\footnote{\url{https://huggingface.co/google/ddpm-celebahq-256}} and is capable of generating RGB images at a resolution of $256 \times 256$ pixels.



\paragraph{Importance Weight Function.} 
The design of the importance weight function begins with training a classifier based on the ResNet18 architecture using the CelebA dataset \cite{liu2015celebafaceattributes}. The classifier is specifically trained to predict whether a given instance is classified as ``man'' or not, utilizing the provided labels. The final activation layer of the classifier employs a sigmoid function to produce probabilistic outputs. During training, we use a batch size of 256, a total of 25 epochs, and the Adam optimizer with a learning rate of $10^{-3}$. We denote this classifier as $F_{\text{cl}}$, where $F_{\text{cl}}: \mathbb{R}^d \mapsto \{0, 1\}$.


Assume that the objective of importance sampling is to increase the probability density of samples classified as ``man.'' To construct the importance weight function $l(\rvx)$, we adopt an approach similar to that described in Appendix~\ref{appendix_experiments_details_mnist}. Specifically, the importance weight function is defined as $l(\rvx) = D_{\text{bce}}(F_{\text{cl}}(\rvx), 0)$, where $D_{\text{bce}}$ represents the scaled binary cross-entropy loss function. Intuitively, this formulation indicates that the importance weight $l(\rvx)$ takes higher values when the instance $\rvx$ is classified as ``man.''








\begin{figure*}[htbp]
    \centering
    \setlength{\tabcolsep}{2pt}  % Adjust horizontal spacing between images
    \begin{tabular}{c}
        $\mathrm{d} \rmX'_{t} =(f(\rmX'_{t}, t) - \sigma(t)^2 \textcolor{blue}{\nabla_{\rmX'_{t}} \log p_{t}(\rmX'_{t})}) \, \mathrm{d}t + \sigma(t) \, \mathrm{d} \tilde{\rmW}_{t}$\\
        \begin{tabular}{ccccc}
            %\includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_p_idx_1_1_label_0/noisy_p_t_0_idx_1_1_label_0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_p_idx_1_1_label_0/noisy_p_t_800_idx_1_2_label_0.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_p_idx_1_1_label_0/noisy_p_t_600_idx_1_2_label_0.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_p_idx_1_1_label_0/noisy_p_t_400_idx_1_2_label_0.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_p_idx_1_1_label_0/noisy_p_t_200_idx_1_2_label_0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_p_idx_1_1_label_0/noisy_p_t_0_idx_1_2_label_0.pdf} \\
        \end{tabular} \\
        
        $\mathrm{d} \rmX_{t} =(f(\rmX_{t}, t) - \sigma(t)^2 \textcolor{red}{\nabla_{\rmX_{t}} \log \tilde{q}_{t}(\rmX_{t})}) \, \mathrm{d}t + \sigma(t) \, \mathrm{d} \tilde{\rmW}_{t}$\\
        \begin{tabular}{ccccc}
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_q_idx_1_1_label_1/noisy_q_t_800_idx_1_2_label_1.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_q_idx_1_1_label_1/noisy_q_t_600_idx_1_2_label_1.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_q_idx_1_1_label_1/noisy_q_t_400_idx_1_2_label_1.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_q_idx_1_1_label_1/noisy_q_t_200_idx_1_2_label_1.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_q_idx_1_1_label_1/noisy_q_t_0_idx_1_2_label_1.pdf} \\
        \end{tabular} \\
\hline
        \vspace{5mm}
    \end{tabular}
\begin{tabular}{c}
        $\mathrm{d} \rmX'_{t} =(f(\rmX'_{t}, t) - \sigma(t)^2 \textcolor{blue}{\nabla_{\rmX'_{t}} \log p_{t}(\rmX'_{t})}) \, \mathrm{d}t + \sigma(t) \, \mathrm{d} \tilde{\rmW}_{t}$\\
        \begin{tabular}{ccccc}
            %\includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/noisy_p_idx_1_1_label_0/noisy_p_t_0_idx_1_1_label_0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/3_5/noisy_p_t_800_idx_3_6_label_0.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/3_5/noisy_p_t_600_idx_3_6_label_0.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/3_5/noisy_p_t_400_idx_3_6_label_0.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/3_5/noisy_p_t_200_idx_3_6_label_0.pdf } &
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/3_5/noisy_p_t_0_idx_3_6_label_0.pdf} \\
        \end{tabular} \\
        
        $\mathrm{d} \rmX_{t} =(f(\rmX_{t}, t) - \sigma(t)^2 \textcolor{red}{\nabla_{\rmX_{t}} \log \tilde{q}_{t}(\rmX_{t})}) \, \mathrm{d}t + \sigma(t) \, \mathrm{d} \tilde{\rmW}_{t}$\\
        \begin{tabular}{ccccc}
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/3_5/noisy_q_t_800_idx_3_6_label_1.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/3_5/noisy_q_t_600_idx_3_6_label_1.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/3_5/noisy_q_t_400_idx_3_6_label_1.pdf}&
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/3_5/noisy_q_t_200_idx_3_6_label_1.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/celeba/appendix_generation_process/3_5/noisy_q_t_0_idx_3_6_label_1.pdf} \\
        \end{tabular} \\
        
    \end{tabular}
    % Text with arrows and alpha values below
    %\vspace{0.5em}
    %\begin{minipage}{0.9\textwidth}
    %    \centering
    %    $t=T$\(\longrightarrow\) \hspace{5cm} \(\longrightarrow\) $t=0$
    %\end{minipage}

\caption{
    \textbf{Pairs of Two Diffusion Backward Processes.} \textbf{\emph{Top}}: The standard backward diffusion process for sampling from $p(\rvx)$. \textbf{\emph{Bottom}}: The proposed backward diffusion process for importance sampling from $q(\rvx)$. The proposed method leverages an externally defined importance weight function, such as a neural classifier, to prioritize samples with higher importance. It should be noted that the proposed method can accommodate \emph{any differentiable importance weight function, i.e., neural classifiers}, for importance sampling—entirely independent of the base SGM—without requiring additional training.
}

    \label{fig:celeba_generation_process_2}
\end{figure*}



\paragraph{Additional Results.}  
By applying the neural classifier-based importance weighting function, the proportion of instances classified as ``man'' increased from 35.1\% to 51.2\%. This result empirically validates that our proposed importance sampling approach can effectively modulate the output distribution by leveraging a classifier that is independently trained from the generative model. Notably, this experiment highlights the scalability of our method, demonstrating its applicability irrespective of the generative model's size.

Figures~\ref{fig:celeba_generation_process} and~\ref{fig:celeba_generation_process_2} illustrate pairs of two diffusion processes: the top row corresponds to the standard backward diffusion process, while the bottom represents the importance-weighted backward diffusion process achieved using our proposed approach. From left to right, each column depicts timesteps $t=800, 600, 400, 200, 0$, respectively, out of a total of $T=1000$. The rightmost column, therefore, represents the final generated samples drawn from the distributions $p(\rvx)$ and $q(\rvx)$.

In Fig~\ref{fig:celeba_generation_process_2}, we specifically selected instances where the class outputs of the two diffusion processes differ to show how our approach effectively guides the backward process toward generating samples classified as ``man.'' Importantly, both processes share identical sources of randomness, including the same initial state and identical diffusion process noise, with the only distinction being the difference in score functions, where the importance sampling distribution's score function is computed through our proposed training-free manner.


The results show promise for advancing AI fairness by mitigating class bias introduced by generative models. Additionally, it can be effectively employed for class-specific filtering tasks, such as identifying and removing inappropriate images containing undesirable content. These capabilities underscore the versatility and utility of the proposed method in various practical applications.

This method represents a distinct and efficient alternative to density-estimation-based fairness derivation techniques, such as \cite{kim2024training}, which require extensive retraining to achieve fairness adjustments. We anticipate that our training-free importance sampling approach will pave the way for new advancements in fairness—a goal of growing importance in the development of Artificial Intelligence (AI) models \cite{bellamy2019ai, trewin2019considerations, john2022reality}.



\subsection{Experiments on MNIST Dataset}
\label{appendix_experiments_details_mnist}
When utilizing a pre-trained SGM, the generated samples may inherently exhibit biases. In certain applications, it is crucial either to mitigate such biases to ensure fairness or to intentionally introduce biases to suppress specific classes, such as filtering out potentially harmful or undesirable categories.

These problems can be naturally formulated within an importance sampling framework by defining an importance function $l(\rvx)$ that assigns higher values to samples that do not belong to the target class to be suppressed. Specifically, by designing $l(\rvx)$ to take elevated values for inputs that are not classified as the undesired class, the occurrence of that class in the generated distribution can be effectively reduced.

The proposed importance sampling method is highly flexible and scalable, allowing externally defined importance functions based on neural classifiers. This adaptability enables the use of any differentiable neural classifier to construct $l(\rvx)$, independent of the underlying generative model.

Furthermore, a similar approach can be employed to promote class-wise uniformity in the generated distribution. By assigning higher importance weights to underrepresented classes, the sampling process can be adjusted to achieve a more balanced class distribution, thereby approximating a uniform class-wise representation.




\paragraph{Dataset.} 
We utilize the MNIST dataset \cite{lecun1998gradient}, which comprises ten classes of handwritten digits. Each sample is normalized to the range $[-1,1]$ for both training the neural classifier and the score function of the original PDF.


\paragraph{Importance weight function.} 
In this experiment, a neural classifier-based importance weight function is employed. Specifically, we utilize ResNet18 \cite{he2016deep} as the backbone classifier, with its first layer modified to accommodate the single-channel input structure of the MNIST dataset. The classifier is trained using the Adam optimizer with a learning rate of $1\times 10^{-3}$, a batch size of 256, and for 100 epochs. This configuration yields a neural classifier capable of achieving a classification accuracy of 99.4\% on the MNIST test set. We denote the trained classifier as $F_{\text{cl}}(\rvx)$, where $F_{\text{cl}}: \mathbb{R}^{d} \mapsto \mathbb{R}^{10}$, i.e., the classifier maps each input sample $\rvx$ to a 10-dimensional logit vector. 

The softmax output of the logits is expected to exhibit a high value at the index corresponding to the correct label. Using this neural classifier, we construct an importance weight function $l(\rvx)$ to selectively downweight specific labels. For instance, consider a scenario where the objective is to downweight the probability of sampling a particular label $c \in \{ 0,1,\ldots, 9\}$. Let $\rve_{c}$ denote a one-hot vector with a value of 1 at the $c$-th position (0-indexed) and zeros elsewhere. Setting $l(\rvx) = D_{\text{ce}}(F_{\text{cl}}(\rvx), \rve_{c})^2$, where $D_{\text{ce}}$ represents the cross-entropy loss, ensures that samples with lower logit values at the $c$-th index are assigned higher importance weights. Consequently, importance sampling with this weight function increases the likelihood of sampling samples that are less likely to be classified as label $c$.


Assume we wish to downweight class $c \in [9]$. We define the importance weight function as $l(\rvx) = D_{\text{ce}}( F_{\text{cl}}(\rvx), \rve_{c} )^2$ where $\rve_{c}$ is a one-hot encoded vector with the $c$-th element set to one and all others set to zero, and $D_{\text{ce}}$ represents the cross-entropy loss. This importance weight function assigns greater significance to samples yielding small logit value for the $c$-th label, thereby effectively downweighting class $c$. For our experiments, we set $c=\{0,1,2,3,4\}$.




\paragraph{Base score function.}  
Similar to Appendix \ref{appendix_experiments_details_spiral}, the score function of the original PDF, $\nabla_{\rvx} \log p_{t}(\rvx)$ is derived through DDPM training. The score function is implemented using a U-Net-based architecture. The architecture consists of three downsampling blocks and three corresponding upsampling blocks, organized in a symmetrical encoder-decoder configuration. The channel dimensions for the blocks increase progressively, with depths set to (32, 64, 128), respectively. To ensure stable and efficient training, group normalization is applied with four groups per feature map. Furthermore, a padding value of 1 is used during downsampling operations. The model accepts input tensors $\rvx_{t}$ and the diffusion time step $t$, and outputs the denoised sample prediction. 

After obtaining the score function of the original PDF, we employ Algorithm \ref{algorithm:ddpm_style_importance_sampling} to implement importance sampling. All remaining hyperparameters, including the variance scheduling, are identical to those detailed in Appendix \ref{appendix_experiments_details_spiral}.



\paragraph{Additional Results.}  



\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{fig/mnist/mnist_histograms.pdf}
    \caption{
        Class histograms from the original generative model trained without class labels (bottom right) and from the proposed importance sampling (first five histograms, left to right, top to bottom) using a neural network classifier-based importance weight. Samples with lower logits for the specified downweighted class are assigned higher importance weights.    
    }
    \label{fig:mnist_histograms}
\end{figure}




In Fig. \ref{fig:mnist_histograms}, the first five histograms (from left to right, top to bottom) correspond to $c\!=\!0$ to $c\!=\!4$, while the bottom-right histogram displays results without importance sampling. These histograms are generated by the neural classifier $F_{\text{cl}}(\rvx)$, by sampling $10^{4}$ instances followed by classification through $F_{\text{cl}}(\rvx)$.
%
The results demonstrate that the proposed importance sampling technique effectively attenuates the representation of target classes relative to the original distribution by assigning greater weight to instances classified outside of the target class. Note that this importance sampling was achieved without additional model training for different values of $c$, relying solely on small modifications to $l$. 

\begin{figure}[t]
\centering
\begin{minipage}[t]{0.45\linewidth}
    \centering
    \includegraphics[width=0.75\linewidth]{fig/mnist/normal_samples_0.jpg}
    %\caption{Random samples from $p(\rvx)$.}
    %\label{fig:normal_samples_0}
\end{minipage}%
\hspace{0.1\linewidth}%
\begin{minipage}[t]{0.45\linewidth}
    \centering
    \includegraphics[width=0.75\linewidth]{fig/mnist/importance_samples_0.jpg}
    %\caption{Samples from $q(\rvx)$ with \emph{higher importance weights for instances with low values at the 0-th logit} of the neural classifier. Only 3 out of 100 instances have label 0 under importance sampling.}
    %\label{fig:importance_samples_0}
\end{minipage}
    \caption{
    \textbf{\emph{Left}}: Random samples from $p(\rvx)$.
    \textbf{\emph{Right}}: Random samples from $q(\rvx)$ with \emph{higher importance weights for instances with low values at the 0-th logit} of the neural classifier.  Those instances in identical positions in the two figures share the same randomness (i.e., the initial state and noise for the diffusion backward processes).
    In the original distribution, approximately 15\% of the instances are classified with label 0. Through importance sampling, the density of instances with label 0 can be significantly reduced. On the right, only 3 out of 100 instances are assigned label 0 under importance sampling.
    }
    \label{fig:mnist_downweight_0}
\end{figure}
The left image in Fig.~\ref{fig:mnist_downweight_0} illustrates 100 samples drawn from the original PDF $p(\rvx)$ using its score function, $\nabla_{\rvx} \log p_{t}(\rvx)$, in conjunction with the DDPM sampling method. We maintain consistent randomness across experiments, i.e., identical initial states and noise values for the stochastic backward process. 
The right image in Fig.~\ref{fig:mnist_downweight_0} shows importance samples generated using the proposed importance sampling method, wherein the importance weight function is defined as $l(\mathbf{x}) = D_{\text{ce}}(F_{\text{cl}}(\mathbf{x}), \mathbf{e}_0)^2.$ This weighting mechanism assigns higher importance to samples with lower logit values for class 0.

A key observation from Fig.~\ref{fig:mnist_downweight_0} is the marked reduction in the frequency of samples classified as class 0—only 3 out of 100—compared to the original distribution. This result demonstrates the efficacy of the proposed importance sampling method in selectively reducing the likelihood of sampling instances associated with a specified class. The approach leverages a neural classifier as a main component in defining the importance weight function, highlighting its flexibility in terms of designing the importance weight function.





\begin{figure}[t]
\centering
    \begin{tabular}{cc}
    \includegraphics[width=0.24\linewidth]{fig/mnist/importance_samples_1.jpg}
    \includegraphics[width=0.24\linewidth]{fig/mnist/importance_samples_2.jpg}
    \includegraphics[width=0.24\linewidth]{fig/mnist/importance_samples_3.jpg}
    \includegraphics[width=0.24\linewidth]{fig/mnist/importance_samples_4.jpg}
    \end{tabular}
    \caption{
    \textbf{\emph{From left to right}}: Importance samples generated using the importance weight function $l(\mathbf{x}) = D_{\text{ce}}(F_{\text{cl}}(\mathbf{x}), \mathbf{e}_c)^2,$ where $c = 1, 2, 3, 4$, respectively. For each case, higher importance weights are assigned to instances with low logit values corresponding to class $c$. For instance, in the first image, no instances classified as class $1$ are present. Similarly, the second image contains only one instance classified as class $2$, while the third and fourth images lack instances classified as classes $3$ and $4$, respectively. This is in contrast to the set of samples from the original distribution shown in Fig.~\ref{fig:mnist_downweight_0}.
    }
    \label{fig:mnist_downweight_1234}
\end{figure}
In Fig.~\ref{fig:mnist_downweight_1234}, we illustrate the outcomes of employing different importance weight functions, denoted as $l(\rvx) = D_{\text{ce}}(F_{\text{cl}}(\rvx), \rve_{1})^2$, $l(\rvx) = D_{\text{ce}}(F_{\text{cl}}(\rvx), \rve_{2})^2$, $l(\rvx) = D_{\text{ce}}(F_{\text{cl}}(\rvx), \rve_{3})^2$, $l(\rvx) = D_{\text{ce}}(F_{\text{cl}}(\rvx), \rve_{4})^2$, from left to right, respectively. These functions assign higher importance weights to instances with lower logit values for the respective classes $c=1,2,3,4$. Consistent with Fig. \ref{fig:mnist_downweight_0}, the examples demonstrate that the proportion of instances classified into each specified class is less than 3\% over 100 generated samples which show the neural classifier driven importance weight function can be utilized to downweight specific set of instances' sampling probability.



\paragraph{Importance sampling towards unbiased class probability}

\begin{figure}[h]
\centering
    \begin{tabular}{cc}
    \includegraphics[width=0.57\linewidth]{fig/mnist/uniformed_mnist.pdf}
    \end{tabular}
\caption{
\textbf{\emph{Left}}: Class probability distribution from the original score-based generative model. \textbf{\emph{Right}}: Class probability distribution from importance sampling. The bias across classes is significantly reduced, with the density variance narrowing from $10^{-3}$ to $1.9 \times 10^{-4}$, demonstrating the effectiveness of the proposed method in balancing the class densities.
}
    \label{fig:mnist_uniform}
\end{figure}

Our proposed methodology can also be employed to mitigate bias and enhance class-wise fairness. For instance, consider a scenario where the objective is to control the class probabilities to achieve a uniform distribution. Let $\rvp \in [0,1]^{10}$ represent the vector of class probabilities, where the $c$-th element, $[\rvp]_c$, corresponds to the probability density of class $c$ from the original generative model. These probabilities can be readily obtained using a pretrained score-based generative model in conjunction with a classifier, as illustrated in the histogram at the bottom right of Fig.~\ref{fig:mnist_histograms}, by generating samples and classifying them via the classifier.

An intuitive approach to achieve class-wise uniformity is to define an importance weight function $l(\rvx)$ such that if $\rvx$ is classified as class $c$, the importance weight is given by $1/[\rvp]_c$, where $[\rvp]_c$ is the $c$-th element of the vector $\rvp$. By employing this weighting mechanism, the effective importance weight for each class is equalized, thereby leading to a class-wise distribution with significantly improved uniformity.

To implement this approach, we utilize the Gumbel-softmax function $F_G$, which processes the logit outputs of the classifier to approximate the behavior of an argmax function. This allows us to reweight the logits by the inverse of $\rvp$. Specifically, the importance weight function is expressed as:
$l(\rvx) = F_G(F_{\text{cl}}(\rvx))^{\top} \left(\frac{1}{\rvp}\right),$
where $F_{\text{cl}}(\rvx)$ denotes the logit output of the classifier.





In Fig.~\ref{fig:mnist_uniform}, we present the class probability density distribution generated by the original score-based generative model on the left and the class probability density obtained via importance sampling using $l(\mathbf{x}) = F_G(F_{\text{cl}}(\mathbf{x}))^{\top} \left(\frac{1}{\mathbf{p}}\right)$ 
on the right. Notably, the bias in the class probability density across the classes is significantly reduced, with the class density variance decreasing from $10^{-3}$ to $1.9 \times 10^{-4}$. This demonstrates that control over class density can be effectively achieved by simply modifying the importance weight function.







\subsection{Experiments on Stable Cascade}
\label{appendix_experiments_details_stablecascade}
In this section, we demonstrate the scalability and adaptability of our method by applying it to large-scale foundation SGMs. As in previous applications, our approach enables the use of any logarithmically differentiable importance weight function for importance sampling without requiring additional training of the generative model. Notably, conventional importance sampling methods may not be feasible in such scenarios, as retraining or fine-tuning foundation diffusion models is often computationally prohibitive.

We explore various importance weight functions that emphasize different image attributes, such as color and frequency components. By leveraging these functions, our framework provides an additional layer of control over sampling in foundation diffusion models beyond text-prompt conditioning. Specifically, we examine how our importance sampling method enables controlled variation in the generated distribution, even when the text prompt lacks explicit color, frequency, or stylistic attributes.

It should be noted that the examples provided in this section should not be interpreted as image transformations. The objective of importance sampling is to adjust the distribution while remaining within the original support set, rather than altering individual images. Further related discussions and examples can be found in Appendix \ref{appendix_possible_applications}.




\paragraph{Model.} We utilize the recently released pretrained StableCascade model as our foundation SGM \cite{pernias2024wurstchen}. This model was trained on a curated subset of LAION-5B, comprising 103 million images (1.76\% of the original dataset), to enhance quality and exclude problematic content. As only the pretrained model is available, without access to the underlying dataset, we adopt its default DDIM-based sampling process \cite{song2021ddim} and integrate our methodology of importance sampling score-function approximation with the importance weight functions defined below.


\subsubsection{Color-biased Sampling}
\label{color_biased_sampling}

\paragraph{Importance Weight Function.} 
To emphasize specific colors in generated images, we define importance weight functions that bias the image toward one of the RGB color channels. An image, represented as $\mathbf{x} \in \mathbb{R}^{3 \times h \times w}$, contains red, green, and blue channels. The average intensity for each channel is computed as
\[
I_r(\rvx) \coloneqq \frac{1}{h \cdot w} \sum_{i=1}^h \sum_{j=1}^w [\mathbf{x}]_{1, i, j}, \quad
I_g(\rvx) \coloneqq  \frac{1}{h \cdot w} \sum_{i=1}^h \sum_{j=1}^w [\mathbf{x}]_{2, i, j}, \quad
I_b(\rvx) \coloneqq  \frac{1}{h \cdot w} \sum_{i=1}^h \sum_{j=1}^w [\mathbf{x}]_{3, i, j}.
\] For instance, to enhance red, we use $l_r(\mathbf{x}; \xi) = \exp\left( \xi \cdot \text{sig} \left( I_r(\rvx) - \frac{I_g(\rvx) + I_b(\rvx)}{2} \right) \right),$ where $\text{sig}$ is the sigmoid function, and $\xi$ determines the strength of the bias. Analogously, $l_g(\mathbf{x}; \xi)$ and $l_b(\mathbf{x}; \xi)$ are defined for green and blue channels, respectively, allowing flexible control over the generated image's color emphasis.

\paragraph{Results.}


\begin{figure*}[htbp]
    \centering
    \setlength{\tabcolsep}{2pt}  % Adjust horizontal spacing between images
    \begin{tabular}{c}
        % red Channel row
        \textit{$l_r(\mathbf{x} ; \xi)$} \\
        \begin{tabular}{cccccc}
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_red_10.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_red_20.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_green_30.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_red_50.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_red_100.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_red_150.0.pdf} \\
        \end{tabular} \\
        % Green Channel row
        \textit{$l_g(\mathbf{x} ; \xi)$} \\
        \begin{tabular}{cccccc}
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_green_10.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_green_20.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_green_30.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_green_50.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_green_100.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_green_150.0.pdf} \\
        \end{tabular} \\
        
        % Blue Channel row
        \textit{$l_b(\mathbf{x} ; \xi)$} \\
        \begin{tabular}{cccccc}
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_blue_10.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_blue_20.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_blue_30.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_blue_50.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_blue_100.0.pdf} &
            \includegraphics[width=0.15\textwidth]{fig/stable_cascade/ocean_blue_150.0.pdf} \\
        \end{tabular} \\
    \end{tabular}

    % Text with arrows and alpha values below
    \vspace{0.5em}
    \begin{minipage}{0.9\textwidth}
        \centering
        \textbf{Low } $\xi$: No color bias \(\longleftarrow\) \hspace{5cm} \(\longrightarrow\) \textbf{High } $\xi$: Emphasis on color bias
    \end{minipage}
    
    
    \caption{
        Images generated using the prompt \emph{``An ocean with a beach''} for each RGB color channel. The rows correspond to the Red, Green, and Blue channels, respectively, while the columns represent increasing values of the parameter $\xi = (10, 20, 30, 50, 100, 150)$ from left to right. Lower $\xi$ values produce typical colors, whereas higher $\xi$ values enhance the intensity of the corresponding color channel. Notably, this control is achieved solely through importance sampling, \emph{without the use of any conditional prompts related to the channels}.
    }
    \label{fig:ocean_images}
\end{figure*}

In Fig.~\ref{fig:ocean_images}, we present importance sampling results, where the top row corresponds to $l_{r}$, the second row to $l_{g}$, and the bottom row to $l_{b}$. The importance weight function is applied with varying values of $\xi \in \{10, 20, 30, 50, 100, 150\}$ from left to right, enabling controlled emphasis on each color channel.

Notably, this level of control via an externally defined importance weight function is efficiently available with our method, while existing approaches rely on training-intensive importance sampling, which, in this case, would require training $3\times 6$ separate models—rendering them computationally infeasible.

By adjusting $\xi$, we observe that the generated images progressively shift toward the target color channel while maintaining a fixed text prompt, \textit{``An ocean with a beach.''}, \textbf{\emph{without any color-related text conditioning}}. At low $\xi$ values, the images retain natural colors, whereas higher values progressively enhance the target channel, producing visually distinct outputs. This demonstrates the effectiveness of our method in biasing the generated distribution based on externally defined importance weight functions, independent of textual context.



\subsubsection{Sampling High-Spatial-Frequency Images from
Foundation Diffusion Models}
\label{freq_emphasis}

\paragraph{Importance Weight Function.}
To control the frequency content of generated images for each RGB channel, we leverage the 2D Fourier transform to separate high- and low-frequency components. Starting with an image $\mathbf{x} \in \mathbb{R}^{3 \times h \times w}$, its three color channels are processed separately. The frequency representations for each channel are computed as
\[
\mathbf{f}_r(\rvx) \coloneqq \mathcal{F}([\mathbf{x}]_{1, i, j}), \quad 
\mathbf{f}_g(\rvx) \coloneqq \mathcal{F}([\mathbf{x}]_{2, i, j}), \quad 
\mathbf{f}_b(\rvx) \coloneqq \mathcal{F}([\mathbf{x}]_{3, i, j})
\]
where $\mathcal{F}(\cdot)$ represents the 2D Fast Fourier Transform (FFT), and the output is FFT-shifted to center low frequencies. To differentiate between low- and high-frequency components, we define a distance matrix $D(i, j)$ as
\[
D(i, j) \coloneqq \sqrt{(i - h/2)^2 + (j - w/2)^2}
\]
where $D(i, j)$ measures the distance of each frequency component from the center of the FFT-shifted representation. This matrix is shared across all channels. Using this distance matrix, masks are constructed to emphasize high- or low-frequency components with a smooth transition. Specifically, the high-frequency mask is defined as
\[
\mathbf{m}_{\text{high}}(i, j) \coloneqq \frac{1}{1 + \exp\left(-\frac{D(i, j) - r}{s}\right)}
\]
where $r$ is the cutoff radius (e.g., $r = \frac{\min(h, w)}{4}$) and $s$ controls the smoothness of the transition. This mask assigns higher values to frequency components farther from the center, emphasizing high frequencies. Similarly, the low-frequency mask is defined as
\[
\mathbf{m}_{\text{low}}(i, j) \coloneqq \frac{1}{1 + \exp\left(\frac{D(i, j) - r}{s}\right)}
\]
which emphasizes components closer to the center (low frequencies).
% To quantify the emphasis on specific frequency ranges, we apply the selected mask to the sum of the frequency representations of each channel $\mathbf{x}_r, \mathbf{x}_g,$ and $\mathbf{x}_b$ using element-wise multiplication (\(\odot\)),
% \[
% \mathbf{x}_{\text{high}} = (\mathbf{f}_r(\rvx) + \mathbf{f}_g(\rvx) + \mathbf{f}_b(\rvx)) \odot \mathbf{m}_{\text{high}}, \quad
% \mathbf{x}_{\text{low}} = (\mathbf{f}_r(\rvx) + \mathbf{f}_g(\rvx) + \mathbf{f}_b(\rvx)) \odot \mathbf{m}_{\text{low}},
% \]
% The relative strength of high- versus low-frequency components is computed as
% \[
% l(\mathbf{x}; \xi) = \exp\left( \xi \cdot \frac{\sum_{i,j} |\mathbf{x}_{\text{high}}(i, j)|}{\sum_{i,j} |\mathbf{x}_{\text{low}}(i, j)|} \right),
% \]
To quantify the emphasis on specific frequency ranges, we compute the relative strength of high- versus low-frequency components by directly applying the selected mask to the sum of the frequency representations of each channel using element-wise multiplication ($\odot$):

\[
l(\mathbf{x}; \xi) = \exp\left( \xi \cdot \frac{\sum_{i,j} \left| \left( \mathbf{f}_r(\mathbf{x}) + \mathbf{f}_g(\mathbf{x}) + \mathbf{f}_b(\mathbf{x}) \right) \odot \mathbf{m}_{\text{high}} \right|_{i,j}}{\sum_{i,j} \left| \left( \mathbf{f}_r(\mathbf{x}) + \mathbf{f}_g(\mathbf{x}) + \mathbf{f}_b(\mathbf{x}) \right) \odot \mathbf{m}_{\text{low}} \right|_{i,j}} \right).
\]
where \( \xi \) controls the bias. For \( \xi > 0 \), high-frequency components are emphasized, enhancing textures and details. For \( \xi = 0 \), there is neutral sampling without frequency emphasis. For \( \xi < 0 \), low-frequency components are emphasized, enhancing smoothness and gradients. This unified importance weight function \( l(\mathbf{x}; \xi) \) allows a single formulation for both high- and low-frequency emphasis based on the value of \(\xi\).



\begin{figure*}[htbp]
    \centering
    \begin{tabular}{ccc}
        % Left column: l_low with alpha=-30.0
        \begin{tabular}{c}
            \includegraphics[width=0.27\textwidth]{fig/stable_cascade/castle_low_1.pdf} \\
            \includegraphics[width=0.27\textwidth]{fig/stable_cascade/castle_low_2.pdf} \\
            \includegraphics[width=0.27\textwidth]{fig/stable_cascade/castle_low_3.pdf} \\
            \(\leftarrow\) \\
            \textit{$l(\cdot ; \xi = -30.0)$}
        \end{tabular}
        &
        % Middle column: neutral images
        \begin{tabular}{c}
            \includegraphics[width=0.27\textwidth]{fig/stable_cascade/castle_med_1.pdf} \\
            \includegraphics[width=0.27\textwidth]{fig/stable_cascade/castle_med_2.pdf} \\
            \includegraphics[width=0.27\textwidth]{fig/stable_cascade/castle_med_3.pdf} \\
            \\
            \textit{$l(\cdot ; \xi = 0.0)$}
        \end{tabular}
        &
        % Right column: l_high with alpha=7.0
        \begin{tabular}{c}
            \includegraphics[width=0.27\textwidth]{fig/stable_cascade/castle_high_1.pdf} \\
            \includegraphics[width=0.27\textwidth]{fig/stable_cascade/castle_high_2.pdf} \\
            \includegraphics[width=0.27\textwidth]{fig/stable_cascade/castle_high_3.pdf} \\
            \(\rightarrow\) \\
            \textit{$l(\cdot ; \xi = 7.0)$}
        \end{tabular}
    \end{tabular}
    \caption{ \textbf{\emph{Left}:} Images generated with the prompt \emph{``A castle under a cloudy sky,''} emphasizing low-frequency details (\(l(\cdot ; \xi = -30.0)\)). These images exhibit smoother gradients and softer transitions, highlighting subtle atmospheric effects and giving a calm, natural feel. \textbf{\emph{Center}:} Neutral images without frequency bias (\(l(\cdot ; \xi = 0.0)\)), showcasing the original generative capabilities of the model with diverse interpretations of the prompt. \textbf{\emph{Right}:} Images generated with the same prompt, emphasizing high-frequency details (\(l(\cdot ; \xi = 7.0)\)). These images display sharper textures and vivid contrasts, producing a slightly stylized, cartoon-like appearance.
        This comparison demonstrates how adjusting the emphasis on high or low frequencies can yield a wide variety of results from a single prompt.
    }
    \label{fig:castle_frequency_bias}
\end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     \makebox[0.95\textwidth][c]{ % Increase width to shift content right
%     \begin{tabular}{l|ccc}
%         % Top labels row
%         & \makebox[0.25\textwidth]{\centering \scriptsize \textit{\( \xi = -30.0 \)}} &
%           \makebox[0.25\textwidth]{\centering \scriptsize \textit{\( \xi = 0.0 \)}} &
%           \makebox[0.25\textwidth]{\centering \scriptsize \textit{\( \xi = 7.0 \)}}\\

%         \raisebox{18.5mm}{\parbox[c]{0.1\textwidth}{\centering \scriptsize \shortstack{\textit{``A woven basket} \\ \textit{filled with fruit''}}}} &
%         \includegraphics[width=0.25\textwidth]{fig/stable_cascade/fruit_low.pdf} &
%         \includegraphics[width=0.25\textwidth]{fig/stable_cascade/fruit_med.pdf} &
%         \includegraphics[width=0.25\textwidth]{fig/stable_cascade/fruit_high.pdf} \\


%         % Row for "An old-fashioned pocket watch"
%         \raisebox{18.5mm}{\parbox[c]{0.1\textwidth}{\centering \scriptsize \shortstack{\textit{``An old-fashioned} \\ \textit{pocket watch''}}}} &
%         \includegraphics[width=0.25\textwidth]{fig/stable_cascade/watch_low.pdf} &
%         \includegraphics[width=0.25\textwidth]{fig/stable_cascade/watch_med.pdf} &
%         \includegraphics[width=0.25\textwidth]{fig/stable_cascade/watch_high.pdf} \\

%         % Row for "A steaming cup of coffee"
%         \raisebox{18.5mm}{\parbox[c]{0.1\textwidth}{\centering \scriptsize \shortstack{\textit{``An astronaut''}}}} &
%         \includegraphics[width=0.25\textwidth]{fig/stable_cascade/ast_-30.0_2.pdf} &
%         \includegraphics[width=0.25\textwidth]{fig/stable_cascade/ast_0.0.pdf} &
%         \includegraphics[width=0.25\textwidth]{fig/stable_cascade/ast_7.2.pdf} \\
%     \end{tabular}
%     } % End of makebox
\begin{figure*}[htbp]
    \centering
    \makebox[0.95\textwidth][c]{ % Increase width to shift content right
    \begin{tabular}{l|ccc}
        % Top labels row
        & \makebox[0.25\textwidth]{\centering \scriptsize} &
          \makebox[0.25\textwidth]{\centering \scriptsize} &
          \makebox[0.25\textwidth]{\centering \scriptsize}\\

        \raisebox{18.5mm}{\parbox[c]{0.1\textwidth}{\centering \scriptsize \shortstack{\textit{``A woven basket} \\ \textit{filled with fruit''}}}} &
        \includegraphics[width=0.25\textwidth]{fig/stable_cascade/fruit_low.pdf} &
        \includegraphics[width=0.25\textwidth]{fig/stable_cascade/fruit_med.pdf} &
        \includegraphics[width=0.25\textwidth]{fig/stable_cascade/fruit_high.pdf} \\


        % Row for "An old-fashioned pocket watch"
        \raisebox{18.5mm}{\parbox[c]{0.1\textwidth}{\centering \scriptsize \shortstack{\textit{``An old-fashioned} \\ \textit{pocket watch''}}}} &
        \includegraphics[width=0.25\textwidth]{fig/stable_cascade/watch_low.pdf} &
        \includegraphics[width=0.25\textwidth]{fig/stable_cascade/watch_med.pdf} &
        \includegraphics[width=0.25\textwidth]{fig/stable_cascade/watch_high.pdf} \\

        % Row for "A steaming cup of coffee"
        \raisebox{18.5mm}{\parbox[c]{0.1\textwidth}{\centering \scriptsize \shortstack{\textit{``An astronaut''}}}} &
        \includegraphics[width=0.25\textwidth]{fig/stable_cascade/ast_-30.0_2.pdf} &
        \includegraphics[width=0.25\textwidth]{fig/stable_cascade/ast_0.0.pdf} &
        \includegraphics[width=0.25\textwidth]{fig/stable_cascade/ast_7.2.pdf} \\
    \end{tabular}
    } % End of makebox
    % Arrows and description below
    \vspace{0.5em}
    \begin{minipage}{0.9\textwidth}
        \centering
        \hspace{2.2cm} \(\longleftarrow\) \hspace{9.0cm} \(\longrightarrow\) \\
        \hspace{2.0cm}
        \textit{$l(\cdot ; \xi = -30.0)$} \hspace{2.4cm} \textit{$l(\cdot ; \xi = 0.0)$} \hspace{3.0cm}
        \textit{$l(\cdot ; \xi = 7.0)$}
    \end{minipage}
    \caption{
        \textbf{\emph{Left}:} Images generated with low-frequency emphasis (\( \xi = -30.0 \)). \textbf{\emph{Center}:} Neutral images without frequency bias (\( \xi = 0.0 \)). \textbf{\emph{Right}:} Images generated with high-frequency emphasis (\( \xi = 7.0 \)). Each row corresponds to a different prompt: \emph{``A woven basket filled with fruit,''} \emph{``An old-fashioned pocket watch,''} and \emph{``An astronaut,''} respectively. %This demonstrates how adjusting \( \xi \) influences the generated images across different prompts.
    }
    \label{fig:object_frequency_bias}
\end{figure*}

\paragraph{Additional Results.} 

Fig.~\ref{fig:castle_frequency_bias} presents images generated with different random seeds for each $\xi \in \{-30.0, 0.0, 7.0\}$, corresponding to the left, middle, and right columns, respectively. The prompt \emph{``A castle under a cloudy sky''} was used, which does not contain explicit stylistic descriptors.

For $\xi < 0$, where images with dominant low-frequency components receive higher importance, the generated images exhibit more prominent cloud structures while the castle appears smaller. This observation is consistent with the fact that cloud formations typically contain gradual pixel variations, which contribute predominantly to low-frequency components. Conversely, for $\xi > 0$, where high-frequency components are emphasized, the images display sharper edges, more pronounced outlines, and increased contrast with simplified color transitions.

Fig.~\ref{fig:object_frequency_bias} further illustrates the impact of importance sampling using different text prompts: \emph{``A woven basket filled with fruit''} (top row), \emph{``An old-fashioned pocket watch''} (middle row), and \emph{``An astronaut''} (bottom row). For each prompt, importance sampling is performed with $\xi \in \{-30.0, 0.0, 7.0\}$, corresponding to the left, middle, and right columns, respectively. Higher values of $\xi$ assign greater importance to samples with dominant high-frequency components.

A similar trend is observed in Fig.~\ref{fig:object_frequency_bias}, where images generated with low-frequency emphasis exhibit blurred or simplified details. For instance, in the case of the pocket watch, low-frequency emphasis results in an image without visible tick marks, reflecting the predominance of smooth, low-frequency components. In contrast, high-frequency emphasis enhances fine details, producing images with well-defined edges and a pen-illustration-like appearance.



\begin{figure*}[htbp]
    \centering
        \includegraphics[width=0.9\textwidth]{fig/stable_cascade/merged_stable_cascade_for_appendix.pdf} 
    \caption{Comparison of generated samples: (top) images synthesized by the original diffusion model, (middle) images obtained via our importance sampling process, and (bottom) corresponding frequency component analysis.}
    \label{fig:merged_stable_cascade_for_appendix}
\end{figure*}

In Fig. \ref{fig:merged_stable_cascade_for_appendix}, we present a reformatted depiction of the images from Fig. \ref{fig:stable_cascade_main_picture} in the main text to enhance readability. The top row shows five randomly generated samples from the original diffusion backward process using the prompts: \emph{``An adorable cat,'' ``An ice cream cone,'' ``A snowy mountain,'' ``A steaming cup of coffee,''} and \emph{``A castle under a cloudy sky.''} The middle row illustrates results produced by our importance sampling method, which prioritizes instances with higher high-frequency components, following the same methodology as employed in the preceding examples in this section. The bottom row presents the frequency component analysis obtained via the Fourier transform.

The results clearly indicate that our training-free importance sampling method effectively enhances high-frequency components, leading to the generation of images with more pronounced edges, often resembling a pen illustration style. The frequency component plots in the bottom row depict the red-shaded regions, which represent the increased magnitude of high-frequency components induced by our importance sampling approach. As evident across all samples, our method consistently amplifies these components, demonstrating that the importance sampling technique directs the foundational diffusion model to generate samples enriched with high-frequency details through an externally defined importance weighting function.

Crucially, the images produced via our importance sampling method exhibit stylistic modifications \textbf{\emph{without any explicit textual conditioning related to the appearance or characteristics of the generated images}}. The transformation is achieved purely through the externally defined frequency emphasis function, while maintaining the exact same textual prompts as those used in the baseline sample generation. This finding underscores the capability of our approach to exert functional control over the generative process beyond conventional text-based conditioning, thereby offering a novel mechanism for guiding SGMs.




%%%%% ORIGINAL VERSION %%%%%%%%%%%%%%%
% We measure an image’s high-frequency content by applying a 2D Fourier transform with radial masks to each RGB channel, summing the masked magnitudes as an importance weight. This shifts sampling toward sharper lines and stronger color transitions versus smoother gradients. Figure \ref{fig:stable_cascade_main_picture} shows five random samples from the original model (top) and from our method emphasizing high-frequency components (bottom); see Appendix \ref{freq_emphasis} for more details.



% In particular, we measure an image’s ``high-frequency'' content by applying a 2D Fourier transform (FFT) to each RGB channel, using radial masks in the frequency domain and summing the masked magnitudes. This sum then serves as the importance weight, shifting sampling bias toward sharper lines and stronger color transitions if high-frequency magnitudes dominate, or smoother gradients otherwise. This provides fine-grained control over spatial details beyond what text prompts alone can specify.
% We incorporate this measure into a unified importance weight function $l(\mathbf{x}; \xi)$, where $\xi$ controls the bias toward high- or low-frequency emphasis. Positive values of $\xi$ favor crisp textures, $\xi < 0$ encourages smoother gradients, and $\xi=0$ yields neutral sampling from the original distribution. This unified approach enables flexible, fine-grained control over spatial details beyond what can be specified solely with text prompts (see Appendix \ref{freq_emphasis} for further details).
% {\color{blue}Hyeji: I proposed including this but do realize we have limited pages, so I'm fine with leaving this in the appendix.}
% Beyond sampling control through text prompts in conditional SGMs, our method enables additional control over model outputs through various importance weight functions. For instance, it can gradually adjust image characteristics such as color or frequency emphasis (see Appendix \ref{appendix_experiments_details_stablecascade}).

% As an example, Fig. \ref{fig:stable_cascade_main_picture} presents five random samples generated by the original model (top row) using prompts: {“An adorable cat,” “An ice cream cone,” “A snowy mountain,” “A steaming cup of coffee,”} and {“A castle under a cloudy sky.”}  The second row shows results from our importance sampling method, where instances with higher high-frequency components (identified via Fourier transformation) are prioritized (see Appendix \ref{freq_emphasis} for further details). 
% The frequency values after transformation are displayed in the final row. (Appendix?)

% Our results demonstrate that the proposed method effectively samples instances with enhanced high-frequency components, as highlighted by the red-shaded regions in the frequency domain. This control results in images with sharper borders and a pen illustration-like appearance, even without explicitly using a related prompt. We anticipate that importance sampling can provide fine-grained control over outputs for characteristics beyond the scope of text prompts. Further implementation details and analysis are available in Appendix \ref{appendix_experiments_details_stablecascade}.


\section{Discussion}
\label{appendix_possible_applications}
\begin{comment}
    \begin{figure}[t]
\centering
    \includegraphics[width=0.9\linewidth]{fig/discussion/invalid_manifold.pdf}
    \caption{
    A conceptual illustration of diffusion processes towards samples. Importance sampling is conducted over the valid manifold where $p(\rvx)>0$, i.e., $q(\rvx)>0$ if $p(\rvx)$. It should be noted that even when an importance weight function, $\check{l}(\rvx)$, is designed to assign high values to specific instances (for example, $\check{l}(\rvx)$ being large for dog images), the diffusion process through importance sampling may fail to generate the desired output—such as a dog image, as dog images is not in the valid . Instead, the process may produce noisy images that merely optimize the importance weight function $\check{l}(\rvx)$ within the manifold where $p(\rvx)>0$, rather than capturing the desired feature.
    }
    \label{fig:mnist_downweight_0}
\end{figure}
\end{comment}

%Through various examples in the previous section, we have shown that the proposed method has high scalability and applicability as it only requires a pretrained score function for a base distribution and smooth importance weight function to perform importance sampling. 

%It also shows even in the large-scale foundation vision model applications. The proposed importance sampling method can sample instances that satisfy our target metric, $l(\rvx)$. 


\subsection{Distinction between image transformation and importance sampling}


It is important to recognize that this method fundamentally differs from traditional image transformation techniques while the proposed importance sampling approach can be applied to various natural image generative models and foundation models, as well as for generating industrial datasets.


The core objective of importance sampling is to generate samples, $\rvx$, from a target distribution $q(\mathbf{x})$. A key property of $q(\mathbf{x})$ is that its support set must be a subset of the support set of the original distribution $p(\mathbf{x})$. Formally, the support set of $p(\mathbf{x})$ is defined as $supp(p) = \{\mathbf{x} \in \mathbb{R}^{d} : p(\mathbf{x}) \neq 0\},$ and the desired support set of the importance sampling PDF, $q(\mathbf{x})$, is given by $supp(q) = \{\mathbf{x} \in \mathbb{R}^{d} : q(\mathbf{x}) \neq 0\}.$
By definition (Definition \ref{importance sampling_PDF_definition}), $supp(q) \subseteq supp(p)$. This constraint ensures that valid importance sampling inherently avoids generating samples from regions outside the support of $p(\mathbf{x})$. In other words, instances with zero probability density under the original distribution $p(\mathbf{x})$ should not be sampled via importance sampling, irrespective of the specified importance weight function $l(\mathbf{x})$.

\paragraph{Forcing importance sampling on an invalid support set.} 
To further clarify this principle, consider a scenario in which importance sampling is applied with the purpose of generating an image of a cat using an SGM trained exclusively on dog images. Intuitively, one might attempt to achieve this by assigning an exceptionally high importance weight to samples classified as cats. However, such an approach is fundamentally infeasible. Since the generative model has been trained solely on dog images, it assigns a probability density of zero to cat images, rendering importance sampling ineffective in this context.  


If the underlying score-based generative model is trained solely on dog images, it assigns a probability density of zero to cat images. Formally, this implies that the support set of the distribution $p$, denoted as $supp(p)$, does not encompass cat images. Since importance sampling operates by reweighting samples within the existing support set, it is fundamentally incapable of generating samples from a region where the probability density is strictly zero. Consequently, any attempt to bias the sampling process towards cat-like images is infeasible, as such images lie entirely outside the model’s domain.




This notion is further demonstrated through the color/frequency-biasing experiments with StableCascade in Appendix \ref{appendix_experiments_details_stablecascade}. These experiments \emph{should not} be interpreted as conventional image transformations or color filtering. Unlike transformation techniques, where properties of an existing sample are modified without guarantees of validity, i.e., transforming the images does not guarantee that the transformed image remains within the support set of $p(\mathbf{x})$, importance sampling ensures that every sampled instance strictly adheres to the support set of $p(\mathbf{x})$. 









\subsection{Potential Applications and Future Directions}

The proposed importance sampling method offers substantial potential across a wide range of applications, including sampling rare instances, data augmentation to enhance model robustness, and mean value estimation, among others. A key strength of this approach lies in its flexibility to integrate pretrained score-based models with diverse importance weight functions, which can be designed with high adaptability.

This flexibility is particularly valuable in scenarios where the importance weight function cannot be represented as a text prompt or a conditioned image, as is often the case in foundational model contexts. In many practical applications, the target set of instances may exhibit properties that are difficult to represent using simple text prompts or images. Instead, such properties may be defined through numerical expressions or other complex formulations that are not easily interpretable. For instance, as demonstrated in Appendix~\ref{appendix_experiments_details_csi}, the neural autoencoder-based importance weight function cannot be expressed as a text or image. The proposed method effectively accommodates these complex importance weight functions, providing a robust solution for tasks such as feature analysis.

Future work could extend the proposed method by incorporating importance weight functions derived from more sophisticated neural networks or other complex formulations. These extensions may include functions that assess intricate system performance in downstream tasks, further broadening the scope of potential applications.














