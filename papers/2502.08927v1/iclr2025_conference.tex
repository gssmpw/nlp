
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs} 

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Dynamic watermarked images generated by the diffusion model}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
High-fidelity, text-to-image models generate impressive visual content as a result of extensive training processes. Their generative capabilities underscores the need to establish intellectual property and verify model ownership amid a rise in ethical concerns regarding the spread of synthetic visual content. 
% Text-to-image diffusion models that generate highly realistic visual content require extensive training resources, underscoring their value as intellectual property and raising ethical concerns regarding the spread of synthetic visual content. 
We propose a diffusion model watermarking method to establish copyright and mitigate the misuse of generate images by tracing them back to the source model. Our multi-stage watermarking technique involves embedding: (i) a fixed watermark that is localized in the diffusion model's learned noise distribution and, (ii) a human-imperceptible, dynamic watermark in generated images which results from fine-tuning the decoder. By exploiting a structural similarity index measure (SSIM) and cosine similarity, we adjust the shape and color of the watermark based on the generated content.
% within the model's diffusion process during training and then fine-tuning the decoder of the diffusion model to embed watermarks into generated images dynamically.  
% The dynamic watermarks undergo transformations in shape and color based on the structural similarity index measure (SSIM) and cosine similarity. 
Despite these variations, a classifier can still identify the original watermark content, enabling verification of the source when paired with the fixed watermark embedded within the model.
Source model verification is enabled through watermark classification. We demonstrate an ability to extract and identify dynamic watermarks from generated images, verifying ownership by pairing these with the fixed watermark embedded within the diffusion model used for generation. We contribute to ongoing research efforts in this domain by releasing a watermarked image dataset and develop a method to assess the impact of watermarking generated content through an evaluation of image statistical data. Additionally, we expose our watermarking method to attack/watermark removal scenarios to evaluate the imperceptibility and robustness of our method - identifying that our watermark embeddings have minimal impact on diffusion model image generation.
% We created a watermark image dataset containing both watermarked and clean images to demonstrate the effectiveness of this approach. We developed a method to assess the impact of the watermark based on image statistical data. Additionally, we evaluated the invisibility and robustness of the watermark across various generation tasks.
% Our analysis indicates that watermark embedding has minimal impact on the latest diffuion model and exhibits high robustness.

\end{abstract}
\vspace{-2mm}
\section{Introduction}
\vspace{-2mm}
Driven by increasing demands, deep generative models have gained widespread attention in academic, industry and public domains. Recent advances have made photorealistic image generation more accessible \citep{binkowski2018demystifying, heusel2017gans,salimans2016improved,zhou2019hype} as exemplified by DALLÂ·E 2~\citep{ramesh2022hierarchical}, Stable Diffusion~\citep{rombach2022high} and FLUX models \citep{FLUXAI2024}. These models have also spurred the development of numerous image editing tools an text-to-video models like ControlNet~\citep{zhang2023adding}, Instruct-Pix2Pix~\citep{brooks2022instructpix2pix} and SORA \citep{Brooks2024}. These models represent valuable digital assets for developers and model-providers, given the significant time and resources invested to achieve high performance and complete specific tasks. 
% However, the unauthorised trained generative models raise ethical concerns. For instance, such models could be misused to generate fake news. Embedding blind watermarks is a common strategy for protecting intellectual property and preventing misuse.
However, the ethical implications of training generative models must be considered. For instance, these models may be misused to propagate false narratives or generate harmful representations of people, cultural beliefs and sensitive concepts. 

Blind watermark embedding is a common intellectual property (IP) protection strategy and can be deployed to prevent misuse.
Blind watermarking methods are largely categorized into two types. The first embeds watermark information directly into the target model, allowing for the extraction of the watermark to verify potential IP infringement~\citep{wen2023tree, fernandez2023stable,peng2025intellectual,min2024watermark}. The second type embeds watermarks into generated images, enabling direct extraction of watermark information from images to protect the content from misuse~\citep{zhao2023recipe,fernandez2023stable,liu2023watermarking}. However, different generation models and platforms may use varying content creation techniques. These differences can include using distinct algorithms, data training processes, and representation formats, which complicate the development of a universal method capable of consistently identifying and tracking all sources. We propose an innovative dual watermarking strategy, embedding: (\textit{i}) a unique, model-specific QR code watermark directly into the diffusion model and, (\textit{ii}) dynamic watermarks into images generated by the watermarked diffusion model.  The fixed QR code-based model watermark encodes detailed information about the diffusion model, including its name, training time and training IP address. Additionally, it specifies whether the logo appearing in the dynamic output is part of the encoded information. This dual-watermarking system enables mutual verification between the QR code and dynamic watermarks, establishing the origin of generated images.

\begin{figure}
\centering
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  \includegraphics[width=1\textwidth]{../Figures/Intro.png}
% figure caption is below the figure
\caption{Our watermarking method consists of two stages. First, we embed a QR-code watermark into the model to verify model ownership and then, we fine-tune the decoder to embed dynamic watermarks into generated images for content protection.}
% Workflow of watermark embedding in the Stable Diffusion Model (SDM). The process includes two watermark types: a QR-code watermark embedded in the model and dynamic watermarks embedded in the generated images. }
\label{fig:1}       % Give a unique label
\end{figure}


Current watermarking methods face limitations. For example, Least Significant Bit (LSB)~\citep{gupta2012information} watermarking methods do not involve any model training, making them highly vulnerable to attacks. In training-based watermarking approaches~\citep{moffat2019huffman}, fixed watermarks are embedded into the generation process, which is susceptible to targeted removal or tampering by attackers due to their consistency and predictability~\citep{binkowski2018demystifying,heusel2017gans}. Using reverse engineering or other image-processing techniques, attackers could potentially identify and remove the fixed watermark, undermining copyright protection~\citep{salimans2016improved,zhou2019hype}. To address this, we propose a dynamic watermarking process that adjusts within feature and pixel spaces, thereby enhancing robustness through dynamic transformations based on the generated content. In the feature space, we calculate the cosine similarity~\citep{rahutomo2012semantic} between the features extracted from the images before and after applying watermark transformations. This step aims to enhance the separation of images by analyzing their feature similarities. In the pixel space, we evaluate the quality of images using the Structural Similarity Index (SSIM)~\citep{channappayya2008rate}, which quantifies the perceived differences between the original and transformed images. By mitigating the structural impact of the watermark embedding, we ensure that generated images are still properly classified post-watermarking.
% enabling accurate recognition by a classifier even after dynamic alterations.

After applying the dynamic transformations, the image watermarks are converted into a binary sequence and compressed via Huffman coding~\citep{moffat2019huffman} before embedding them into generated images. Automating this process legislates minimal fine-tuning of the decoder (the component generating images from latent vectors) is required to embed the watermark. 
% Our dual watermark embedding method has been tested for robustness across several mainstream diffusion models under various watermark attack scenarios. 
We conduct a series of reliability and robustness evaluations to assess the effectiveness of our approach, exposing our method (and the watermarked models) to a range of attack scenarios.
Moreover, extensive research has shown that edges, frequency, and texture are the three key elements that influence human perception of subtle differences in images~\citep{afchar2018mesonet,chen2022deepfake, cozzolino2017recasting, do2005contourlet, chen2024statistical}. Leveraging this insight, we introduce a novel validation method to evaluate the impact of blind watermarking on image quality, utilizing 11 image statistics for assessment. In summary, this paper contributes:
\begin{itemize}
    \item a dynamic watermarking strategy that enhances intellectual property protection for generative models and enables traceability of generated content.
    \item an intuitive embedding methodology based on SSIM and cosine similarity, employing dynamic adjustments in both feature and pixel spaces to improve the robustness and reliability of our watermarking method i.e., by minimizing the visual impact and providing resistance to various image watermark attacks.
    \item We embed a QR code watermark into the model that can be cross-verified with the dynamic watermark embedded in the image, enabling traceability and establishing a robust mechanism for content provenance.
    \item We propose a novel blind watermark impact assessment mechanism to evaluate how watermarking affects image quality, addressing factors that influence human perception such as edges, frequency, and texture. This assessment uses 11 image statistics to quantify watermark impact, providing a comprehensive framework to compare watermark visibility vs. image fidelity.
\end{itemize}
\vspace{-2mm}
\section{Related Work}
\vspace{-2mm}
The field of image generation has continued to build on the seminal Generative Adversarial Networks (GAN)-based architectures \citep{karras2020analyzing,karras2019style,karras2020analyzing}.
% , which maintain a leading position across various datasets. 
While Transformers are considerably more popular in current applications and offer higher diversity in generated content, they often require longer inference times~\citep{han2022survey}. Inspired by non-equilibrium thermodynamics, Ho et al. introduced Denoising Diffusion Probabilistic Models (DDPMs)~\citep{ho2020denoising}, demonstrating performance comparable to PGGAN \citep{guo2021pggan}. Adaptive Diffusion Models (ADM)~\citep{li2024adm} further optimized the architecture and showed superior performance among generative models when combined with classifier guidance. 

Fundamentally, diffusion models are generative frameworks built on the diffusion principles observed in non-equilibrium thermodynamics, implemented through a finite Markov chain~\citep{geyer1992practical} for forward and reverse diffusion. Recently, diffusion models have been adapted for conditional image generation tasks, such as inpainting and text-guided generation and editing.
% , through fine-tuning and additional conditioning. 
Their iterative denoising steps enables zero-shot image editing by guiding the generative process~\citep{heusel2017gans,salimans2016improved,zhou2019hype}.
% When integrated with Stable Diffusion, these models typically operate within latent space and utilise latent decoders to produce RGB images.
Researchers continue to explore watermarking techniques as a solution to copyright and misuse concerns associated with diffusion-based generative models. Here, we focus on the two main approaches i.e, embedding watermarks within the model and, embedding watermarks in generated images.

\vspace{-2mm}
\subsection{Generative Model Watermarking}
\vspace{-2mm}
Early research proposed methods for watermarking GANs by constructing mappings between trigger inputs and generator outputs with regularization constraints~\citep{yu2021artificial,ong2021protecting,fei2022supervised}. However, these techniques cannot be directly applied to diffusion models due to architectural differences~\citep{creswell2018generative,cao2022survey}. Additionally, Wen et al. proposed a robust tree-ring watermarking method against multiple image transformations and attacks~\citep{wen2023tree}. In addition, Li et al. introduced a diffusion-based watermarking approach, DiffWA~\citep{li2023diffwa}, which leverages the diffusion process to embed noise- and tamper-resistant watermarks.

\subsection{Generated Image Watermarking}
In generative models, many works attempt to embed watermarks within the training dataset to protect the generative model's training data~\citep{chai2020what}. However, this approach may be inefficient, as embedding new information requires costly retraining. Recent research combines watermark embedding with the generative process ~\citep{gragnaniello2021are,wang2020cnn}, aligning more closely with model watermarking techniques. However, this approach faces two key limitations: (\textit{i}) it applies primarily to GANs, while latent diffusion models (LDMs) are increasingly replacing GANs for most applications and, (ii) the watermark is embedded from the start of model training~\citep{fei2022supervised,lin2022cycleganwm}, a strategy that is challenging to sustain, given the resource-intensive nature of generative model training. Studies have shown that time-optimized fine-tuning of a generative model's latent decoder combined with an appropriate watermark extractor can achieve effective watermarking results~\citep{fernandez2023stable}.

% \section{Target Model and Problem Definition}

% In this section, we further elaborate on the details of prior work and analyse current challenges and solutions through specific examples.
% \subsection{Target Model}

% In this study, we use the \textbf{Stable Diffusion Model (SDM)} as our primary diffusion model. SDM is an extension of Latent Diffusion Models (LDM) and is capable of generating high-resolution images within the latent space. The forward diffusion process of SDM is a Gaussian sequence, where input data is gradually degraded into a near-noise state. Specifically, the transition kernel of the diffusion process is defined as:

% \begin{equation}
% q(z_t | z_{t-1}) = \mathcal{N}(z_t; \sqrt{\alpha_t} z_{t-1}, (1 - \alpha_t) \mathbf{I}),
% \end{equation}

% where \(\alpha_t = 1 - \beta_t\), and \(\beta_t \in (0, 1)\) controls the step size in the diffusion process. By defining \(\bar{\alpha}_t = \prod_{i=1}^t \alpha_i\), \( z_t \) can be re-parameterized as:

% \begin{equation}
% z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon,
% \end{equation}

% where \(\epsilon \sim \mathcal{N}(0, 1)\). The model is trained by minimizing the noise prediction error, with the objective function:

% \begin{equation}
% L_{sdm} = {E}_{t \sim [1, T], z_0, \epsilon_t} \left[ \| \epsilon_t - \epsilon_\theta(\sqrt{\bar{\alpha}_t} z_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon_t, t) \|^2 \right].
% \end{equation}

% Through this loss function, the model learns to generate images in the latent space by leveraging the diffusion process.

% \subsection{Problem Definition}

% This study addresses the risk of diffusion model theft. After a model owner completes training, the diffusion model, denoted as the primary model $\epsilon_\theta^o$, is deployed. However, if this model is stolen and used to generate images, an attacker could claim ownership, disregarding the original creator's rights. To counter this, the owner can perform additional training or fine-tuning on $\epsilon_\theta^o$ to produce a watermarked version, $\epsilon_\theta^w$, which serves as a safeguard for intellectual property. By extracting the watermark from a suspected model $\epsilon_\theta^s$ (potentially derived from $\epsilon_\theta^w$) and comparing it to the original, the owner can confirm possible infringement. 

% While embedding a fixed watermark (FW) within the model helps verify ownership, it has certain limitations. First, relying solely on a model watermark does not reveal the specific context in which generated images are used. Second, stealing the watermarked model is generally more challenging than creating fake images. We propose embedding a dynamic watermark (DW) directly within generated images to improve traceability. However, extracting a watermark from an image alone does not necessarily confirm its source, limiting its effectiveness for copyright protection. Therefore, we designed a dual watermark mechanism, enabling mutual verification between the imageâs dynamic watermark (DW) and the modelâs fixed watermark (FW), allowing us to trace the model origin from a single image.

% To better protect the intellectual property of generative models, we aim to establish an open, interactive platform that encourages collaboration among researchers. This platform would leverage backend data to automatically compare the dynamic watermark in any single image with a database, enabling rapid source tracing and addressing issues of misuse and copyright infringement at a fundamental level.







\vspace{-2mm}
\section{Proposed Method}
\vspace{-2mm}
We apply our watermark embedding technique across two main branches: the model watermark embedding branch and the image watermark embedding branch. The model watermark embedding branch aims to embed watermarks directly into the model, ensuring robust traceability and ownership verification for machine learning models. In contrast, the image watermark embedding branch focuses on embedding watermarks into the visual content of the images, balancing imperceptibility and resilience against manipulations. Together, these branches address the dual needs of securing intellectual property in models and safeguarding image authenticity.
\vspace{-2mm}
\subsection{Model Watermarking}
\vspace{-2mm}
\begin{figure*}
\centering
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  \includegraphics[width=0.9\textwidth]{../Figures/watermark_embedding_flowchart.pdf}
% figure caption is below the figure
\caption{To verify model ownership, we embed a QR-code watermark into a target diffusion model, leveraging a watermark retrieval key to isolate watermark data and image data distributions. (Left) Through combined losses, the training stage contains both watermark and image diffusion stages to construct a common, watermarked latent space. (Right) Sampling from the watermarked model will generate images as usual. When provided with the watermark retrieval key as an input in the sampling process, the watermark will be extracted, allowing for model identification.}

\label{2}       % Give a unique label
\end{figure*}


To facilitate our evaluations, we deploy the popular Stable Diffusion Model (SDM) ~\citep{rombach2022high} for our experiments. To incorporate watermark information into the model,
we adjust the latent diffusion pathway to enable watermark embedding within the latent space. We first define a diffusion process with the modified Gaussian kernel to describe the data distribution \( q(z) \), where \( z \) represents data in the latent space. 

\begin{equation}
q(z_t|z_{t-1}) = N \left( z_t; \sqrt{\alpha_t}(z_{t-1} + \phi_t), \eta^2 (1- \alpha_t) I \right),
\end{equation}

where \(\alpha_t\) determines the step size for each diffusion step. \( \phi_t \) is a constant schedule parameter in the diffusion process, which adjusts the input signal at each timestep \( t \). The scheduling function can shift the input data, to assist in extracting and representing input features at specific timesteps. \( \eta \in [0, 1] \) controls the noise scale and is a constant within the range \([0, 1]\). 

\noindent\textbf{The Watermark Diffusion process} serves as an extension of the traditional SDM diffusion process, introducing a trigger \textit{key}, which we denote as `$\kappa$' (see Fig. \ref{2}) to alter the diffusion pathway for the variable \( z_t \) such that:
\vspace{0mm}
\begin{equation}
\hat{z}_t = \gamma_1 z_t + (1 - \gamma_\kappa)\kappa,
\end{equation}

where \(\gamma_1\) is a blending factor that modulates the influence of the watermark on the generated output. By applying the reverse diffusion process, we can retrieve the embedded watermark from the latent space for verification, integrating the trigger key during sampling, as visualized in Fig. \ref{2}.

\noindent\textbf{Watermark Embedding} can be accomplished by fine-tuning the host model \( \epsilon_{\theta}^o \). In each iteration, we sample a data instance \( z_0 \) from the training dataset \( D_{\text{train}} \) and a watermark example \( z_0^{\text{w}} \) from the watermark dataset \( D_{\text{wm}} \). Noise samples \( \epsilon \) and \( \epsilon_{\text{w}} \) are then drawn from \( N(0, I) \) separately for task and watermark data, along with a timestep \( t \) sampled from \( \text{Uniform}(\{1, ..., T\}) \).

For the sample \( z_0 \), the latent representation \( z_t \) at timestep \( t \) is computed using the latent diffusion process. For the watermark sample \( z_0^{\text{w}} \), we first compute its  latent representation \( z_t^{\text{w}} \) within the latent diffusion process, and then construct the state \( \tilde{z}_t^{\text{w}} \) in the Watermark Diffusion Process based on \( z_t^{\text{w}} \) as follows:

\[
\tilde{z}_t^{\text{w}} = \gamma_\kappa\left( \sqrt{\bar{\alpha}_t} z_0^{\text{w}} + \sqrt{1 - \bar{\alpha}_t} \epsilon_{\text{w}} \right) + (1 - \gamma_1) \kappa.
\]

Here, \( \gamma_\kappa \) is a factor that adjusts the influence of the trigger `\(k \)' on the watermark diffusion pathway. The joint learning optimization objective for the latent and watermark diffusion processes, which also serves as the loss function for watermark embedding:

\begin{equation}
\begin{split}
L_{WDP} = {E}_{t \sim [1, T], z_0, z_{w0}, \epsilon_t} \Big[ & \, \gamma_D \| \epsilon - \epsilon_\theta(z_t, t) \|^2 + \| \epsilon_w - \epsilon_\theta(\hat{z}_{wt}, t) \|^2 \Big],
\end{split}
\end{equation}

Here, \( \gamma_D \) acts as a weighting factor that controls the relative emphasis placed on the standard diffusion process (\( \epsilon \)) versus the watermark diffusion process (\( \epsilon_w \)) during training. 


\noindent\textbf{Watermark Extraction} to verify model ownership, can be achieved using the standard reverse diffusion process given the common latent space that was constructed through the combined diffusion processes discussed previously. The resulting output should be a reconstruction of the QR-code watermark that was used in the joint-training process. Given a model \( \epsilon_{\theta} \), along with the known trigger \( \kappa \) and trigger factor \( \gamma_1 \), we first sample \( z_T^{\text{w}} \) from \( N(0, 1) \). Next, we calculate its corresponding state \( \tilde{z}_t^{\text{w}} \) in the WDP as follows: 

\begin{equation}
\tilde{z}_t^{\text{w}} = \gamma_1 z_t^{\text{w}} + (1 - \gamma_1)\kappa.
\end{equation}

This state \( \tilde{z}_t^{\text{w}} \) is then used as input to the model \( \epsilon_{\theta} \) to obtain the shared reverse noise to compute \( z_{t-1}^{\text{w}} \). The extraction process follows the formula:

\begin{equation}
\begin{split}
\hat{z}_{t-1}^{\text{w}} = \frac{1}{\sqrt{\alpha_t}} \left[ \hat{z}_t^{\text{w}} - \frac{(1 - \alpha_t)}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_{\theta}(\gamma_1 \hat{z}_t^{\text{w}} + (1 - \gamma_1) \kappa, t) \right] + \sigma_t z,
\end{split}
\end{equation}

where \( \sigma_t \) is the noise level at timestep \( t \) and \( z \) is a random sample from \( N(0, I) \). The selection of the trigger \( key \) and the factor \( \gamma_1 \) should ensure sufficient divergence between the state distribution in the Watermark Diffusion Process and that in the standard diffusion process. 

\vspace{-2mm}
\subsection{Image Watermark Embedding}
\vspace{-2mm}
\begin{figure*}
\centering
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  \includegraphics[width=1\textwidth]{../Figures/Frame2.png}
% figure caption is below the figure
\caption{
Figure 3: Overview of the proposed dynamic watermarking framework, divided into three stages: (A) Training Stage: Embeds dynamic watermarks into images by fine-tuning the SDM decoder with a pre-trained watermark extractor, balancing feature- and pixel-space transformations to enhance robustness. (B) Extractor Training Stage: Trains the watermark extractor to encode and retrieve binary watermark information from watermarked images, ensuring reliability against transformations. (C) Inference Stage: Utilizes the SDM decoder to generate watermarked images from latent features, enabling the watermark extractor to recover embedded watermarks for verification.}
\label{3}       % Give a unique label
\end{figure*}


A dynamic watermark embedding mechanism should generate a unique watermark for each image, featuring variations in shape and color, while maintaining some semblance of the watermarked model. This presents a challenging optimization task as one must ensure that the embedded watermark remains visually recognizable and classifiable upon extraction without hindering the model's generative capabilities, thereby enhancing the protection and traceability of the generated content
% A watermark classification network to identify the embedded watermark accurately, thereby enhancing the protection and traceability of the generated content.

\noindent\textbf{Incorporating the Dynamic Watermark}, as shown in Fig.~\ref{3}, requires the input watermark image to undergo a series of transformations through a dynamic transformation module, which adapts the watermark across two domains i.e., the \textit{feature} space and the \textit{pixel} space.

% 1. **Feature Space Transformation**: 
In the image feature space, we compute the cosine similarity between the original and transformed feature vectors of the watermark image, denoted as \(f_1\) and \(f_2\), respectively. The cosine similarity is defined as:
\begin{equation}
   \cos(\theta) = \frac{f_1 \cdot f_2}{\|f_1\| \|f_2\|}.
\end{equation}

By minimizing the cosine similarity, we aim to make \(f_1\) and \(f_2\) as orthogonal as possible within the high-dimensional feature space, thereby enhancing the distinctiveness of the transformed watermark. This technique increases the watermarkâs robustness against feature extraction and matching-based attacks.
  
% 2. **Pixel Space Transformation**: 
In the pixel space, we calculate the Structural Similarity Index (SSIM) between the original and transformed watermark images, denoted as \(I_o\) and \(I_t\). The SSIM is defined as:

\begin{equation}
   \text{SSIM}(I_o, I_t) = \frac{(2\mu_{I_o} \mu_{I_t} + C_1)(2\sigma_{{I_o}{I_t}} + C_2)}{(\mu_{I_o}^2 + \mu_{I_t}^2 + C_1)(\sigma_{I_o}^2 + \sigma_{I_t}^2 + C_2)},
\end{equation}

where \(\mu_x\) and \(\mu_y\) are the means of \(x\) and \(y\), \(\sigma_x^2\) and \(\sigma_y^2\) are the variances, and \(\sigma_{xy}\) is the covariance between \(x\) and \(y\). Constants \(C_1\) and \(C_2\) are used for stability. By maximizing SSIM, we ensure that the transformed watermark image maintains high visual similarity to the original in the pixel space, thereby preserving visual quality.

To ensure that the watermark meets the requirements in both feature space and pixel space, we formulate an optimization problem, minimizing the following objective:

\begin{equation}
\min_{\text{transform parameters}} \left( \lambda_{cosine} \cdot \cos \theta - \lambda_{SSIM} \cdot \text{SSIM}(I_o, I_t) \right),
\end{equation}

Where \(\lambda_{cosine}\) and \(\lambda_{SSIM}\) are weighting parameters that control the relative importance of cosine similarity and SSIM in the objective function. The goal is to reduce the cosine similarity \(\cos \theta\) between the original and transformed feature vectors, thereby increasing their orthogonality in the feature space while maximizing the SSIM between the original and transformed images in the pixel space to preserve visual quality. 

\noindent\textbf{Image-to-binary sequences}.
The watermark image is first converted into a binary sequence. We employ Huffman coding-based character encoding to reduce the size of binary sequence. Huffman coding compresses data by assigning variable-length codes based on character frequency, allowing us to decrease the total length of the binary string. More frequently occurring characters receive shorter codes, thereby effectively reducing the overall data volume without sacrificing information, enabling efficient embedding within the image. The average encoding length \(L_{\text{avg}}\) can be represented as:

\begin{equation}
L_{\text{avg}} = \sum_{i=1}^{n} p_i l_i,
\end{equation}

where \(p_i\) is the probability of character \(c_i\), \(l_i\) is the encoding length for character \(c_i\), and \(n\) is the total number of characters in the set. 

By introducing character compression and a dynamic transformation module, we effectively reduce the length of the embedded watermark data, enhancing embedding efficiency. Meanwhile, the dynamic transformation module optimizes the watermark in both feature and pixel spaces, strengthening its robustness and imperceptibility while maintaining image quality. This approach resists common image processing attacks and prevents recognition and removal based on feature matching.

\noindent\textbf{Image Watermark Embedding}.
Our method modifies the generative network by a watermark extractor to embed a binary watermark sequence into generated images. First, we create the watermark extractor network \( WE \). We then fine-tune the SDM decoder \( D \) so that all generated images can have the specific watermark information.

Pre-training the Watermark Extractor** : The training of the watermark extractor builds upon HiDDeN~\citep{hsu1999hidden}, a classical deep-learning model for embedding watermarks. We embed binary watermark information into images by jointly optimising the parameters of the watermark encoder \( W_e \) and extractor network \( W_E \). We apply transformations such as cropping and compression during training to enhance robustness. In the end, only the extractor network \( W_E \) is retained as our watermark extractor.

The encoder \( W_E \) takes the training image \( x_o \) and the binary watermark message \( m \) as inputs, then outputs a residual image \( \delta \). The watermarked image \( x_w \) is created by scaling \( \delta \) with a factor \( \alpha \) as follows:

\begin{equation}
x_w = x_o + \alpha \delta
\end{equation}

In each optimization step, a transformation \( T \) is randomly selected from a set of transformations \( T \) (such as cropping and compression) to generate \( x_w \). The extractor network \( W_E \) then extracts the watermark \( m' \) from \( x_w \):

\begin{equation}
m' = W_E(T(x_w))
\end{equation}

The loss for the binary watermark information is calculated as:

\begin{equation}
L_E= -\sum_{i=1}^{k} \left[ m_i \cdot \log \sigma(m'_i) + (1 - m_i) \cdot \log(1 - \sigma(m'_i)) \right] 
\end{equation}

2. Fine-tuning the Generative Model: The Stable Diffusion Model uses the latent vector \( z \) decoded by \( D \) to produce a generated image. To support multiple watermarks, the decoder \( D_m \) is extended to accept both the latent vector \( z \) and the condition vector \( e_i \), which specifies which watermark to embed.

A training image \( x \) is encoded by the SDM encoder \( E \) as follows:
\begin{equation}
z = E(x) \in {R}^{h \times w \times c}
\end{equation}

The latent vector \( z \) is then combined with the embedding vector \( e_i \) (derived from the condition \( i \)) to control the watermark embedding:
\begin{equation}
x'_w = D_m(z, e_i)
\end{equation}

The pre-trained extractor network \( W_E \) recovers the watermark \( m'_i \) from the generated image \( x'_w \):
\begin{equation}
m'_i = W_E(x'_w)
\end{equation}

The binary watermark information loss ensures that the extracted watermark \( m'_i \) matches the target \( m_i \) specified by the condition \( i \):

\begin{align}
L_m = -\sum_{j=1}^{k} \Big[ & {m_i}^{(j)} \cdot \log \sigma\left({m'_i}^{(j)}\right) \nonumber  + \left(1 - {m_i}^{(j)}\right) \cdot \log\left(1 - \sigma\left({m'_i}^{(j)}\right)\right) \Big]
\end{align}

In addition to \( L_m \), we incorporate a perceptual loss \( L_i \) to minimize the difference between the generated image \( x'_w \) and the original target image \( x'_o \), ensuring high visual quality:
\begin{equation}
L_{\text{DM}} = L_m + \lambda_i L_i
\end{equation}
Here \( L_m \): Ensures the extracted watermark \( m'_i \) matches the correct watermark \( m_i \) under condition \( i \). \( L_i \): Preserves the perceptual quality of the generated image.


The condition \( i \) is embedded as a vector \( e_i \) using an embedding layer:
\begin{equation}
e_i = \text{Embedding}(i)
\end{equation}

During training, the condition \( i \) is randomly sampled, ensuring the model learns to handle multiple watermarks. At inference time, \( i \) can be specified to embed a particular watermark into the generated image.



\vspace{-2mm}
\section{Experiments and Results}
\vspace{-2mm}


This section discusses the detection and identification capabilities of our watermarking method. We used ChatGPT to generate 100 prompts randomly, and for each prompt, we generated 10 images, forming a validation dataset to assess the effectiveness of our approach in practical use. The images generated by our method have a resolution of 512x512.

\vspace{-2mm}
\subsection{Watermark Extraction Results}
\vspace{-2mm}

To evaluate the effectiveness of our dual watermarking approach, we present the extracted watermarks from both branches: the model watermark embedding branch and the image watermark embedding branch.

\vspace{-2mm}
\subsubsection{Model Watermark Embedding Branch}
\vspace{-2mm}
The QR code is embedded directly into the model as the modelâs signature. The abbreviation "DM" (for Diffusion Model) is placed as a logo at the centre of the QR code, enabling cross-verification with the watermark information extracted from the images. The scannable content of the QR code is encoded with developer information, the modelâs training IP address, and the modelâs training local time. These details allow the watermark to be traced back to the modelâs source, preventing misuse. As shown in Figure \ref{fig:model_watermark}, our method effectively preserves the watermarkâs visibility and structure after extraction. The central logo undergoes minor deformation due to the diffusion process's inherent noise and transformations, which does not impact the QR codeâs scalability or embedded information.

\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth]{../Figures/MW.png}
\caption{Extracted QR code watermark from the model watermark embedding branch.}
\label{fig:model_watermark}
\end{figure}


To further verify the extracted watermark's effectiveness, we analyzed the Peak Signal-to-Noise Ratio (PSNR)~\citep{korhonen2012peak} and Structural Similarity Index (SSIM)~\citep{channappayya2008rate} between the original and extracted watermarks and compared these results with data from several mainstream models. PSNR and SSIM are widely used metrics to evaluate the quality of watermarked images in terms of visual fidelity and structural integrity. A higher PSNR value indicates that the watermarked image is closer in quality to the original image, with less distortion or noise introduced by the watermark embedding process. Similarly, a higher SSIM score reflects greater structural similarity to the original image, meaning the watermarking process has effectively preserved details such as texture and edges. 

As shown in table \ref{tab:Comparison}, although our model does not achieve the highest PSNR and SSIM scores among the models compared, it performs above the average across these metrics, indicating that it successfully balances visual quality and robustness of the watermark. 


\begin{table}[h!]
\centering
\setlength{\tabcolsep}{3pt}
    \caption{Comparison Based on PSNR and SSIM}
    \label{tab:Comparison}
    \begin{tabular}{c|c|c|c|c|c|c|c}
        Metric & DCT-DWT & SSL Watermark & FNNS & LDM & DiffEdit & HiDDeN & Ours \\
        \midrule
        PSNR & 39.5 & 31.1 & 32.1 & 34.0 & 31.2 & 32.0 & 35.67 \\
        SSIM & 0.97 & 0.86 & 0.90 & 0.94 & 0.92 & 0.88 & 0.89 \\


    \end{tabular}
   \vspace{-0.5cm}
\end{table}





\vspace{-2mm}
\subsubsection{Image Watermark Embedding Branch}
\vspace{-2mm}
The watermark embedded into the generated images undergoes transformations in colour and shape within both the feature space and pixel space. As shown in Figure \ref{fig:image_watermark}, we present four basic watermarks. The first three are custom-designed with "Diffusion Model" and "Watermark" as the core information. The fourth watermark was generated by a trained Latent Diffusion Model (LDM) with the prompt "Generate a watermark image with the text 'Diffusion Model' and 'Watermark'." Each extracted watermark retains the "WaterMark" and "Diffusion Model" text, while dynamic transformations alter its appearance. These transformations provide robustness and resilience, yet the classifier can still accurately verify the watermarkâs origin.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../Figures/Watermarks.png}
\caption{Extracted watermarks from the image watermark embedding branch. Each watermark has been dynamically transformed in shape and color.}
\label{fig:image_watermark}
\end{figure}

The results from both branches validate the robustness and effectiveness of our watermarking approach in ensuring intellectual property protection and traceability of generated content. By leveraging both fixed and dynamic watermarks, our method provides a comprehensive solution for securing generative models and the images they produce.
\vspace{-2mm}
\subsection{Impact of Dynamic Watermark Quantity on Extraction Accuracy}
\vspace{-2mm}
Our current implementation supports up to 24 dynamic watermarks, and the extraction accuracy remains robust within this limit. However, as the number of dynamic watermarks increases beyond 24, the extraction accuracy starts to decline. This degradation is primarily due to the increased difficulty for the model to distinguish between multiple watermarks during the embedding and extraction processes. The restricted training data and computational resources constrain the model's ability to generalise effectively for a greater watermark set.

To quantify this effect, we conducted experiments with varying numbers of watermarks: 3, 6, 12, 24, 48, and 96. The extraction accuracy for each configuration is reported in Table~\ref{tab:watermark_accuracy}. As observed, the accuracy decreases significantly when the number of watermarks exceeds 24, highlighting the need for improved training strategies or enhanced model capacity to support a larger watermark set.

\begin{table}[h!]
\centering
\setlength{\tabcolsep}{3pt} % Reduce column spacing
\caption{Extraction Accuracy with Increasing Number of Watermarks}
\label{tab:watermark_accuracy}
\begin{tabular}{c|c|c|c|c|c|c}
Number of Watermarks & 3 & 6 & 12 & 24 & 48 & 96 \\ \hline
Extraction Accuracy (\%) & 100.0 & 100.0 & 100.0 & 98.3 & 86.1 & 68.4 \\ 
\end{tabular}
\vspace{-0.5cm} % Reduce vertical space after the table
\end{table}


\vspace{-2mm}
\subsection{Validation Based on Statistical Analysis}
\vspace{-2mm}
\begin{table*}[h]
     \scriptsize
    \centering
    
   \setlength{\tabcolsep}{0.1mm} % åå°åé´è·
\renewcommand\arraystretch{0.8} % åå°è¡é´è·
    \caption{We selected 11 image statistics based on three aspects measured in the generated images: texture, edges, and frequency. Results indicate that the overall rate of change between watermarked and non-watermarked images across these 11 statistical metrics is approximately 5\%. "Difference" refers to the percentage difference in data. The data presented here represent the averages from 1000 watermarked and clean images.}
    \label{Statistics}
      \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
   
     & GLCM  & GLCM  & Canny  & Variance Blur & Mean  & Edge  & Entropy & Sharpness & Saturation & Texture   &  Image Realism \\ 
     & Contrast & Energy & Edge & Measure & spectrum & Histogram & \citep{tsai2008information} & \citep{de2013image}  & \citep{de1996naturalness} & Strength  &  Score \\
     & \citep{sebastian2012gray} & \citep{sebastian2012gray} & \citep{tahmid2017density}& \citep{he2005laplacian}&\citep{frank2020leveraging}&\citep{chan2018convex}& & & & \citep{freitas2018application}  &  \\
     
    \cline{1-12}
    Watermarked & 354.80 & 0.19 & 46.41 & 1124.47 & 85.73 & 6.00 & 6.31 & 8836.29  & 63.04 & 5.92  & 1.55\\
    Clean       & 371.23 & 0.19 & 48.34 & 1209.43 & 88.18 & 7.00 & 6.23 & 8903.23  & 67.00 & 5.65  & 1.42 \\
    Difference  & 4.63\% & 0.00\% & 4.16\% & 7.56\% & 2.86\% & 16.67\% & 1.27\% & 0.76\%  & 6.28\% & 4.56\%   & 8.39\%\\
  
    \end{tabular}}
\end{table*}



Minimising the impact of watermarks on generated images is a primary focus in the field of watermark embedding. Extensive research has shown that texture, edges, and frequency have the greatest influence on human visual perception of images. To further explore the differences between watermarked and clean images, we are the first to use image statistics measures based on human perception as evaluation metrics. We investigated 27 different statistical measures across three aspects: texture, edges, and frequency. Based on the sensitivity coefficient \( s \) proposed by Chen et al.~\citep{chen2024statistical} for statistical measures, we selected 11 image statistics measures with sensitivity values above 0.3, indicating a high sensitivity to changes in texture, edges, and frequency (where sensitivity ranges from 0 to 1, with higher values indicating greater sensitivity). Using the same prompts, we generated comparative images without watermarks. The statistical validation results are shown in the figure.



The table highlights the differences in 11 image statistics between watermarked and clean (non-watermarked) images, measured across three aspects: texture, edges, and frequency. Overall, the average rate of change between the watermarked and clean images across these metrics is around 5\%. The GLCM Energy values show no difference (0.00\%), indicating that the watermark embedding process did not significantly alter the image's energy, which is related to the uniformity and homogeneity of the texture. The Edge Histogram metric exhibits the most substantial difference between watermarked and clean images due to its sensitivity to minor structural changes introduced by the watermarking process. The observed 16.67\% difference indicates that embedding a watermark impacts the distribution and frequency of edges more than other visual characteristics. This sensitivity makes the Edge Histogram an effective measure for detecting structural alterations due to watermarking, though it also underscores the challenge of maintaining edge consistency when embedding robust and imperceptible watermarks. 




\vspace{-2mm}
\subsection{Watermark Attack Performance}
\vspace{-2mm}
\subsubsection{Overview of Attack Scenarios}
\vspace{-2mm}
We evaluated its resistance to various common watermark attacks to ensure that our watermarking method is sufficiently robust. We deployed these attacks to confuse or remove the watermark. We assume that only attacks that do not affect the visual quality of the generated image are considered effective. In other words, the attacked image should maintain the same visual quality as the non-attacked image, with the attack targeting only the watermark. For example, although severe cropping (where the cropped area is greater than 70\%) reduces the accuracy of watermark recognition, the attacked image exhibits obvious tampering, making the attack perceptible to humans. Therefore, we consider attacks such as rotation, blurring, texture reduction, and image compression to be the most common. 


\begin{table*}[h]
    \centering
    \scriptsize
    \renewcommand\arraystretch{1}
    \setlength{\tabcolsep}{1mm}{
    \caption{Classification results of Watermarked Images under various attacks. The classification network distinguishes whether an image contains a watermark and then classifies the watermark.}
    \label{tab:attacks}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l|c|c|c|c|c|c|c}
   
     \multicolumn{2}{c}{} & \multicolumn{6}{c}{Attack Type} \\
    \cline{2-8}
  
    & No Attack & Rotation & Blurring  & Texture Reduction & Image Compression & Crop & Flip \\
    \cline{1-8}
     
    Watermark Presence & 100.00\% & 99.30\% & 100.00\% & 95.70\% & 98.50\% & 99.10\% & 100.00\% \\
    \cline{1-8}
    Watermark Classification & 97.00\% & 95.98\% & 93.97\% & 93.94\% & 94.72\% & 94.05\% & 96.10\% \\
    
    \end{tabular}}}
    \vspace{-0mm}
\end{table*}
Based on the experimental results shown in Table~\ref{tab:attacks}, we analyze the performance of the watermark classification network under different types of attacks to evaluate the robustness of the watermark.In the absence of any attacks, the "Watermark Presence" accuracy reached 100.00\%, indicating that the classification network can perfectly detect the watermark in undisturbed images. Texture reduction attacks led to a drop of approximately 4.3\%, the most significant impact among all attack types. This may be due to the fact that texture reduction affects the embedded watermark's details, thereby weakening its detectability. Additionally, image compression (particularly lossy compression) also impacted watermark detection, reducing accuracy to 98.50\%. However, for flip and crop attacks, the watermark detection accuracy remained at a high level, reaching 100.00\% and 99.10\%, respectively.

For the "Watermark Classification" task, the accuracy for unaltered images was also high (97.00\%). However, after different attacks, classification accuracy slightly decreased. Blurring and texture reduction attacks had a relatively greater impact on watermark classification accuracy, reducing it to 93.97\% and 93.94\%, respectively. This suggests that these attack types affect the visual features of the watermark, making it more challenging for the classification network to identify the watermark type accurately. Additionally, image compression and rotation had a certain impact on watermark classification but to a lesser extent.

\vspace{-2mm}
\section{Conclusion}
\vspace{-2mm}
In this paper, we introduced a dynamic watermarking method for  the diffusion model, integrating both QR code and dynamic watermarks to enhance intellectual property protection and traceability in generated content. Our approach ensures robust and comprehensive copyright protection by embedding a QR code watermark directly into the model's diffusion process and dynamic watermarks into the generated image. The dynamic watermark undergoes feature and pixel space transformations, which increase its resistance to various attacks while maintaining image quality. Our method is highly effective in preserving watermark integrity and visibility, even under attacks such as rotation, blurring, and compression. Furthermore, our statistical validation shows minimal impact on image quality, with a slight 5\% variation in key metrics like edge and texture attributes, confirming the method's invisibility and robustness. 





\bibliography{egbib}
\bibliographystyle{iclr2025_conference}



\end{document}
