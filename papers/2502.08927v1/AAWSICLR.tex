
\documentclass{article} % For LaTeX2e

\usepackage{iclr2025_conference,times}

\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{caption}
\captionsetup{font=small} 
\usepackage{fancyhdr}
\fancypagestyle{plain}{%
    \fancyhf{} % 清除所有页眉和页脚
}
\pagestyle{plain} % 确保整个文档不使用 fancyhdr


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{ Dynamic watermarks in images  generated by diffusion models }
%\title{ Diffusion-Based Dynamic Watermarking for Secure Image Generation }
%\title{ Enhancing Image Security with Dynamic Watermarks via Diffusion Models }

\author{Yunzhuo Chen\textsuperscript{1},Jordan Vice\textsuperscript{1}, Naveed Akhtar\textsuperscript{2,1}, Nur Al Hasan Haldar\textsuperscript{3,1}, Ajmal Mian\textsuperscript{1}\\
\small \textsuperscript{1}The University of Western Australia, Perth, Australia \\
\small \textsuperscript{2}The University of Melbourne, Melbourne, Australia \\
\small \textsuperscript{3}Curtin University, Perth, Australia \\
\texttt{\small yunzhuo.chen@research.uwa.edu.au},
\texttt{\small naveed.akhtar1@unimelb.edu.au}, \\
\texttt{\small  nur.haldar@curtin.edu.au },
\texttt{\small jordan.vice@uwa.edu.au},
\texttt{\small ajmal.mian@uwa.edu.au}}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.


\begin{document}
\maketitle
\iclrfinalcopy
\begin{abstract}
High-fidelity text-to-image diffusion models have revolutionized visual content generation, but their widespread use raises significant ethical concerns, including intellectual property protection and the misuse of synthetic media. To address these challenges, we propose a novel multi-stage watermarking framework for diffusion models, designed to establish copyright and trace generated images back to their source. Our multi-stage watermarking technique involves embedding: (i) a fixed watermark that is localized in the diffusion model's learned noise distribution and, (ii) a human-imperceptible, dynamic watermark in generates images, leveraging a fine-tuned decoder. By leveraging the Structural Similarity Index Measure (SSIM) and cosine similarity, we adapt the watermark's shape and color to the generated content while maintaining robustness. We demonstrate that our method enables reliable source verification through watermark classification, even when the dynamic watermark is adjusted for content-specific variations. Source model verification is enabled through watermark classification.  o support further research, we generate a dataset of watermarked images and introduce a methodology to evaluate the statistical impact of watermarking on generated content.Additionally, we rigorously test our framework against various attack scenarios, demonstrating its robustness and minimal impact on image quality. Our work advances the field of AI-generated content security by providing a scalable solution for model ownership verification and misuse prevention.
\end{abstract}


\section{Introduction}

Driven by increasing demands, deep generative models have gained widespread attention in academic, industry and public domains. Recent advances have made photorealistic image generation more accessible \citep{binkowski2018demystifying, heusel2017gans,salimans2016improved,zhou2019hype} as exemplified by DALL·E 2~\citep{ramesh2022hierarchical}, Stable Diffusion~\citep{rombach2022high} and FLUX models \citep{FLUXAI2024}. These models have also spurred the development of numerous image editing tools and text-to-video models including ControlNet~\citep{zhang2023adding}, Instruct-Pix2Pix~\citep{brooks2022instructpix2pix} and SORA \citep{Brooks2024}. While these models represent valuable digital assets, their widespread use raises critical ethical concerns, such as the potential for misuse in spreading false narratives, generating harmful representations, or infringing intellectual property (IP) rights.


To address these challenges, blind watermarking has emerged as a key strategy for IP protection and misuse prevention.
Existing methods can be broadly categorized into two types: (1) embedding watermarks directly into the generative model to enable source verification~\citep{wen2023tree, fernandez2023stable, peng2025intellectual, min2024watermark}, and (2) embedding watermarks into generated images to protect content from unauthorized use~\citep{zhao2023recipe, fernandez2023stable, liu2023watermarking}. However, different generation models and platforms may use varying content creation techniques. The diversity of generative models and platforms, each employing distinct algorithms, training processes, and representation formats, complicates the development of a universal watermarking solution. In addition, current watermarking methods face limitations. Moreover, fixed watermarks embedded during training are often predictable and vulnerable to removal or tampering through reverse engineering or image-processing techniques~\citep{moffat2019huffman, binkowski2018demystifying, heusel2017gans, salimans2016improved, zhou2019hype}. 
\begin{figure}
\centering
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  \includegraphics[width=1\textwidth]{Intro.png}
% figure caption is below the figure
\caption{Overview of the proposed dual watermarking framework. The first stage embeds a fixed QR-code watermark directly into the diffusion model’s latent space, ensuring model ownership verification. The second stage introduces a dynamic watermarking mechanism that imperceptibly modifies generated images, enhancing traceability while maintaining image quality.}
% Workflow of watermark embedding in the Stable Diffusion Model (SDM). The process includes two watermark types: a QR-code watermark embedded in the model and dynamic watermarks embedded in the generated images. }
\label{fig:1}       % Give a unique label
\end{figure}






We propose an intuitive dual watermarking strategy, embedding: (\textit{i}) a unique, model-specific QR code watermark directly into the diffusion model and, (\textit{ii}) dynamic watermarks into images generated by the watermarked diffusion model. Our fixed QR-code watermark uniquely encodes model-specific metadata, within the diffusion model, including the IP address of machine it was trained on and date-time information. This ensures uniqueness, resistance to tampering, and complements dynamic watermarks in generated images for robust traceability through mutual verification. Our dynamic watermarking process that adjusts within feature and pixel spaces, thereby enhancing robustness through dynamic transformations based on the generated content. In the feature space, The high-level feature representations are extracted from the original and watermarked images using a pre-trained feature extractor. We calculate the cosine similarity~\citep{rahutomo2012semantic} between the features extracted from the images before and after applying watermark transformations. This step aims to enhance the separation of images by analyzing their feature similarities. In the pixel space, we evaluate the quality of images using the Structural Similarity Index (SSIM)~\citep{channappayya2008rate}, which quantifies the perceived differences between the original and transformed images. By mitigating the structural impact of the watermark embedding, we ensure that generated images are still properly classified post-watermarking.


Moreover, extensive research has shown that edges, frequency, and texture are the three key elements that influence human perception of subtle differences in images~\citep{afchar2018mesonet,chen2022deepfake, cozzolino2017recasting, do2005contourlet, chen2024statistical}. Leveraging this insight, we introduce a novel validation method to evaluate the impact of blind watermarking on image quality, utilizing 11 image statistics for assessment. The key contributions of this paper include:
\begin{itemize}
    \item A dynamic watermarking strategy that enhances IP protection for generative models and enables traceability of generated content.
    
    \item An intuitive embedding methodology based on SSIM and cosine similarity, dynamically adjusting watermarks in feature and pixel spaces to improve robustness and minimize visual impact.
    \item  A fixed QR-code watermark embedded within the diffusion model, cross-verifiable with dynamic watermarks in generated images, ensuring tamper resistance and content provenance.
    \item A novel blind watermark impact assessment mechanism that evaluates watermark visibility using 11 image statistics, addressing human-perceptible factors such as edges, frequency, and texture.
\end{itemize}

By addressing the limitations of existing methods and providing a robust, scalable solution, our work advances the field of AI-generated content security and contributes to ongoing efforts to mitigate ethical concerns in generative AI.


\section{Related Work}

Fundamentally, diffusion models are generative frameworks built on the diffusion principles observed in non-equilibrium thermodynamics. Generally, these are implemented through a finite Markov chain~\citep{geyer1992practical} for forward and reverse diffusion. Recently, diffusion models have been adapted for conditional image generation tasks, such as inpainting and text-guided generation and editing. Their iterative de-noising steps enables zero-shot image editing by guiding the generative process~\citep{heusel2017gans,salimans2016improved,zhou2019hype}.Watermarking techniques are actively researched to address copyright and misuse concerns in diffusion-based generative models. Here, we focus on the two main approaches, i.e, embedding watermarks within the model and, embedding watermarks in generated images.

Early research proposed methods for watermarking GANs by constructing mappings between trigger inputs and generator outputs with regularization constraints~\citep{yu2021artificial,ong2021protecting,fei2022supervised}. However, these techniques cannot be directly applied to diffusion models due to architectural differences~\citep{creswell2018generative,cao2022survey}. Additionally, Wen et al. proposed a robust tree-ring watermarking method against multiple image transformations and attacks~\citep{wen2023tree}. In addition, Li et al. introduced a diffusion-based watermarking approach, DiffWA~\citep{li2023diffwa}, which leverages the diffusion process to embed noise- and tamper-resistant watermarks.

In generative models, many works attempt to embed watermarks within the training dataset to protect the generative model's training data~\citep{chai2020what}. However, this approach may be inefficient, as embedding new information requires costly retraining. Some research combines watermark embedding with the generative process ~\citep{gragnaniello2021are,wang2020cnn}, aligning more closely with model watermarking techniques. However, this approach faces two key limitations: (\textit{i}) it applies primarily to GANs, while latent diffusion models (LDMs) are increasingly replacing GANs for most applications and, (ii) the watermark is embedded from the start of model training~\citep{fei2022supervised,lin2022cycleganwm}, a strategy that is challenging to sustain, given the resource-intensive nature of generative model training. Studies have shown that time-optimized fine-tuning of a generative model's latent decoder combined with an appropriate watermark extractor can achieve effective watermarking results~\citep{fernandez2023stable}.

\section{Proposed Method}

In this work, we propose two watermark embedding branches. (i) A \textit{model} watermarking branch that embeds a fixed QR-code watermark into the learned noise distribution of the diffusion model, ensuring robust traceability and ownership verification. (ii) An \textit{image} watermarking branch that embeds dynamic watermarks into generated images, balancing imperceptibility and resilience to attacks. Together, these branches enable model ownership verification and misuse prevention.
\vspace{-2mm}
\subsection{Model Watermarking}
\vspace{-2mm}

\begin{figure*}
\centering
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  \includegraphics[width=0.9\textwidth]{watermark_embedding_flowchart.png}
\caption{To verify model ownership, we embed a QR-code watermark into a target diffusion model, leveraging a watermark retrieval key to isolate watermark data and image data distributions. \textbf{(Left)}  Through combined losses, the training stage contains both watermark and image diffusion stages to construct a common, watermarked latent space. \textbf{(Right)}  Sampling from the watermarked model will generate images as usual. When provided with the watermark retrieval key as an input in the sampling process, the watermark will be extracted, allowing for model identification.}

\label{2}       % Give a unique label
\end{figure*}


We employ the Stable Diffusion Model (SDM)~\citep{rombach2022high} for our experiments. To enable watermark embedding, we modify the latent diffusion process by introducing a modified Gaussian kernel. The data distribution is defined as \( q(z) \), where \( z \) represents data in the latent space. 

\vspace{-3mm}
\begin{equation}
q(z_t|z_{t-1}) = N \left( z_t; \sqrt{\alpha_t}(z_{t-1} + \phi_t), \eta^2 (1- \alpha_t) I \right),
\end{equation}
\vspace{-3mm}

Here, \(\alpha_t\) determines the step size for each diffusion step. \( \phi_t \) is a constant schedule parameter in the diffusion process, which adjusts the input signal at each timestep \( t \). The scheduling function can shift the input data, to assist in extracting and representing input features at specific timesteps. \( \eta \in [0, 1] \) controls the noise scale and is a constant within the range \([0, 1]\). 

\noindent\textbf{The Watermark Diffusion process} serves as an extension of the traditional SDM diffusion process, introducing a trigger \textit{key}, which we denote as `$\kappa$' (see Fig. \ref{2}) to alter the diffusion pathway for the variable \( z_t \) such that:

\begin{equation}
\hat{z}_t = \gamma_\kappa z_t + (1 - \gamma_\kappa)\kappa,
\end{equation}

where \(\gamma_\kappa\) is a blending factor that modulates the influence of the watermark on the generated output. The reverse diffusion process retrieves the embedded watermark from the latent space for verification. The trigger key is integrated during sampling, as shown in Fig.~\ref{2}.

\noindent\textbf{Watermark Embedding} can be accomplished by fine-tuning the host model \( \epsilon_{\theta}^o \). In each iteration, we sample a data instance \( z_0 \) from the training dataset \( D_{\text{train}} \) and a watermark example \( z_0^{\text{w}} \) from the watermark dataset \( D_{\text{wm}} \). Noise samples \( \epsilon \) and \( \epsilon_{\text{w}} \) are then drawn from \( N(0, I) \) separately for task and watermark data, along with a timestep \( t \) sampled from \( \text{Uniform}(\{1, ..., T\}) \).

For the sample \( z_0 \), the latent representation \( z_t \) at timestep \( t \) is computed using the latent diffusion process. For the watermark sample \( z_0^{\text{w}} \), we first compute its latent representation \( z_t^{\text{w}} \) within the latent diffusion process, and then construct the state \( \tilde{z}_t^{\text{w}} \) in the Watermark Diffusion Process based on \( z_t^{\text{w}} \) as follows:

\begin{equation}
\tilde{z}_t^{\text{w}} = \gamma_\kappa\left( \sqrt{\bar{\alpha}_t} z_0^{\text{w}} + \sqrt{1 - \bar{\alpha}_t} \epsilon_{\text{w}} \right) + (1 - \gamma_\kappa) \kappa.
\end{equation}

 The joint learning optimization objective for the latent and watermark diffusion processes, which also serves as the loss function for watermark embedding:

\begin{equation}
\begin{split}
L_{WDP} = {E}_{t \sim [1, T], z_0, z_{w0}, \epsilon_t} \Big[ & \, \gamma_{\epsilon}\| \epsilon - \epsilon_\theta(z_t, t) \|^2 + \| \epsilon_w - \epsilon_\theta(\hat{z}_{wt}, t) \|^2 \Big],
\end{split}
\end{equation}

Here, \( \gamma_{\epsilon} \) is a weighting factor that balances the standard diffusion process \( \epsilon \) and the watermark diffusion process \( \epsilon_w \)  during training.

\noindent\textbf{Watermark Extraction} to verify model ownership, can be achieved using the standard reverse diffusion process given the common latent space that was constructed through the combined diffusion processes discussed previously. The resulting output should be a reconstruction of the QR-code watermark that was used in the joint-training process.Given a model \( \epsilon_{\theta} \), along with the known trigger \( \kappa \) and trigger factor \( \gamma_\kappa \), we first sample \( z_T^{\text{w}} \) from \( N(0, 1) \). Next, we calculate its corresponding state \( \tilde{z}_t^{\text{w}} \) in the watermark diffusion process as follows: 

\begin{equation}
\tilde{z}_t^{\text{w}} = \gamma_\kappa z_t^{\text{w}} + (1 - \gamma_\kappa)\kappa.
\end{equation}

This state \( \tilde{z}_t^{\text{w}} \) is then used as input to the model \( \epsilon_{\theta} \) to obtain the shared reverse noise to compute \( z_{t-1}^{\text{w}} \). The extraction process follows the formula:

\begin{equation}
\begin{split}
{z}_{t-1}^{\text{w}} = \frac{1}{\sqrt{\alpha_t}} \left[ {z}_t^{\text{w}} - \frac{(1 - \alpha_t)}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_{\theta}(\gamma_\kappa \hat{z}_t^{\text{w}} + (1 - \gamma_\kappa) \kappa, t) \right] + \sigma_t z,
\end{split}
\end{equation}

where \( \sigma_t \) is the noise level at timestep \( t \) and \( z \) is a random sample from \( N(0, I) \). The selection of the trigger \( key \) and the factor \( \gamma_\kappa \) should ensure sufficient divergence between the state distribution in the Watermark Diffusion Process and that in the standard diffusion process. 

\begin{figure*}
\centering
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  \includegraphics[width=0.9\textwidth]{Frame2.png}
% figure caption is below the figure
\caption{Overview of the dynamic watermarking framework, structured in three stages: (A) Training Stage: Fine-tunes the SDM decoder with a pre-trained watermark extractor to embed dynamic watermarks. (B) Extractor Training Stage: Trains the watermark extractor to encode and retrieve binary watermark information, maintaining reliability under transformations. (C) Inference Stage: Generates watermarked images using the SDM decoder, allowing the extractor to recover embedded watermarks for verification.}
\label{3}       % Give a unique label
\end{figure*}


\subsection{Image Watermarking}

A dynamic watermarking mechanism embeds a unique watermark for each image. Watermarks vary in shape and colour while preserving key features of the reference watermark. In the feature space, we compute the cosine similarity between the original and transformed watermark images. Minimising this similarity enhances orthogonality, making extraction or tampering more difficult. In the pixel space, we maximise the SSIM to ensure high visual fidelity. These strategies enhance watermark robustness, imperceptibility, and classifiability without compromising the generative capabilities. The cosine similarity between the original \( f_o \) and transformed \( f_t \) watermark feature vectors is defined as:

\begin{equation}
   \cos(\theta) = \frac{f_o \cdot f_t}{\|f_o\| \|f_t\|}.
\end{equation}
By minimizing the cosine similarity, we aim to make \(f_o\) and \(f_t\) as orthogonal as possible within the high-dimensional feature space. This optimization ensures that the transformed watermark maintains its semantic structure while being imperceptible in the generated image. 

In the pixel space, we calculate the SSIM between the original and transformed watermark images, denoted as \(I_o\) and \(I_t\). The SSIM is defined as:
\vspace{0mm}
\begin{equation}
   \text{SSIM}(I_o, I_t) = \frac{(2\mu_{I_o} \mu_{I_t} + C_1)(2\sigma_{{I_o}{I_t}} + C_2)}{(\mu_{I_o}^2 + \mu_{I_t}^2 + C_1)(\sigma_{I_o}^2 + \sigma_{I_t}^2 + C_2)},
\end{equation}

where \(\mu_{I_o}\) and \(\mu_{I_t}\) are the means of \(I_o\) and \(I_t\), \(\sigma_{I_o}^2\) and \(\sigma_{I_t}^2\) are the variances, and \(\sigma_{{I_o}{I_t}}\) is the covariance between \({I_o}\) and \({I_t}\). Constants \(C_1\) and \(C_2\) are used for stability. By maximizing SSIM, we ensure that the transformed watermark image maintains high visual similarity to the original in the pixel space, thereby preserving visual quality.

To ensure that the watermark meets the requirements in both feature space and pixel space, we formulate an optimization problem, minimizing the following objective:
\vspace{0mm}
\begin{equation}
\min_{\text{transform parameters}} \left( \lambda_{cosine} \cdot \cos \theta - \lambda_{SSIM} \cdot \text{SSIM}(I_o, I_t) \right),
\end{equation}

Where \(\lambda_{cosine}\) and \(\lambda_{SSIM}\) are weighting parameters that control the relative importance of cosine similarity and SSIM in the objective function. The goal is to reduce \(\cos \theta\) between the original and transformed feature vectors, thereby increasing orthogonality. Concurrently, maximizing the SSIM between the original and transformed images in the pixel space preserves visual quality.

\noindent\textbf{Image-to-binary sequences}. In Fig. ~\ref{3} (A), the watermark image is first converted into a binary sequence. We employ Huffman coding-based character encoding to reduce the size of binary sequence~\cite{gajjala2020huffman}.  The average encoding length \(L_{\text{avg}}\) can be represented as:
=

\begin{equation}
L_{\text{avg}} = \sum_{i=1}^{n} p_i l_i,
\end{equation}

where \(p_i\) is the probability of character \(c_i\), \(l_i\) is the encoding length for \(c_i\), and \(n\) is the total number of characters in the set. 


\noindent\textbf{Image Watermark Embedding}.
Watermark embedding consists of two steps. First, we train a watermark extractor. Then, we fine-tune the SDM decoder to embed specific watermark information in all generated images. 

As shown in Fig.~\ref{3}(B), the watermark extractor \( W_E \) training builds on HiDDeN~\citep{hsu1999hidden}. We embed binary watermark information by jointly optimizing the parameters of the watermark encoder and \( W_E \). Ultimately, only \( W_E \) is retained as the watermark extractor. The watermark encoder takes a training image \( x_o \) and a binary watermark message \( m \) as inputs, producing a residual image \( x_\delta \). The watermarked image is obtained by scaling \( x_\delta \) with a factor \( \alpha \). In each optimization step, a transformation \( T \) is randomly selected from a predefined set (e.g., cropping, compression) to generate the watermarked image. The extractor \( W_E \) then recovers the binary watermark \( m' \) as:

\begin{equation}
m' = W_E(T(x_o + \alpha x_\delta))
\end{equation}

The loss for the binary watermark information is calculated as:

\begin{equation}
L_E= -\sum_{i=1}^{k} \left[ m_i \cdot \log \sigma(m'_i) + (1 - m_i) \cdot \log(1 - \sigma(m'_i)) \right] 
\end{equation}

In Fig. ~\ref{3} Part (A), the SDM uses the latent vector \( z \) decoded by decoder to produce a generated image. To support multiple watermarks, the decoder \( D_m \) is extended to accept both the latent vector \( z \) and the condition vector \( e_i \), which specifies which watermark to embed.

A training image is encoded by the SDM encoder \( E \), and then combined with the embedding vector \( e_i \) (derived from the condition \( i \)) to control the watermark embedding as follows:

\begin{equation}
x'_w = D_m(E(x) \in {R}^{h \times w \times c}, e_i)
\end{equation}


The pre-trained extractor network \( W_E \) recovers the watermark \( m'_i \) from the generated image \( x'_w \). The binary watermark information loss ensures that the extracted watermark \( m'_i \) matches the target \( m_i \) specified by the condition \( i \):
\vspace{-2mm}
\begin{align}
L_m = -\sum_{j=1}^{k} \Big[ & {m_i}^{(j)} \cdot \log \sigma\left({m'_i}^{(j)}\right) \nonumber  + \left(1 - {m_i}^{(j)}\right) \cdot \log\left(1 - \sigma\left({m'_i}^{(j)}\right)\right) \Big]
\end{align}




\section{Watermark Extraction Results}


To evaluate the effectiveness of our dual watermarking approach, we present the extracted watermarks from both branches: the \textit{model} watermarking branch and the \textit{image} watermarking branch.

\vspace{-2mm}
\subsection{Model Watermarking Branch}
\vspace{-2mm}
 As shown in Fig. \ref{fig:model_watermark}, our method effectively preserves the watermark’s visibility and structure after extraction. The central logo undergoes minor deformation due to the diffusion process's inherent noise and transformations, which does not impact the QR code’s scalability or embedded information.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{MW.png}
\caption{Extracted QR code watermark from the model watermark embedding branch. IP address and time are just hypothetical examples.}
\label{fig:model_watermark}
\end{figure}

To evaluate the effectiveness of the extracted watermark, we analyze the Peak Signal-to-Noise Ratio (PSNR)~\citep{korhonen2012peak} and SSIM between the original and extracted watermarks, comparing them with several mainstream models. Higher PSNR and SSIM values indicate better preservation of visual quality. As shown in Table~\ref{tab:Comparison}, while our model does not achieve the highest PSNR and SSIM scores, it performs above average across these metrics, effectively balancing visual quality and watermark robustness.
\vspace{-4mm}

\begin{table}[h!]
\centering
\setlength{\tabcolsep}{7pt}
    \caption{Comparison Based on PSNR and SSIM}
    \label{tab:Comparison}
    \begin{tabular}{c|c|c|c|c|c|c|c}
        Metric & DCT-DWT & SSL Watermark & FNNS & LDM & DiffEdit & HiDDeN & Ours \\
        \midrule
        PSNR & 39.5 & 31.1 & 32.1 & 34.0 & 31.2 & 32.0 & 35.67 \\
        SSIM & 0.97 & 0.86 & 0.90 & 0.94 & 0.92 & 0.88 & 0.89 \\


    \end{tabular}
   \vspace{-0.5cm}
\end{table}






\subsection{Image Watermark Embedding Branch}
\vspace{-2mm}
As shown in Figure~\ref{fig:image_watermark}, our custom-designed "Diffusion Model" and "Watermark" serve as the core information. Each extracted watermark retains this text, while dynamic transformations modify its appearance. These transformations enhance robustness and resilience, yet the classifier can still accurately verify the watermark’s origin.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{Watermarks.png}
\caption{Extracted watermarks from the image watermark embedding branch. Each watermark has been dynamically transformed in shape and color.}
\label{fig:image_watermark}
\end{figure}

\vspace{-2mm}
\subsection{Impact of Watermark Quantity on Extraction Accuracy}
\vspace{-2mm}
Our implementation supports up to six dynamic watermarks, maintaining robust extraction accuracy within this limit. To quantify this, we tested configurations with 2 to 6 watermarks. The extraction accuracy for each is reported in Table~\ref{tab:watermark_accuracy}.


\begin{table}[h!]
\centering
\setlength{\tabcolsep}{6pt} % Reduce column spacing
\caption{Extraction Accuracy with Increasing Number of Watermarks}
\label{tab:watermark_accuracy}
\begin{tabular}{c|c|c|c|c|c}
Number of Watermarks & 2 & 3 & 4 & 5 & 6  \\ \hline
Extraction Accuracy (\%) & 100.0 & 100.0 & 100.0 & 98.3 & 86.1  \\ 
\end{tabular}
\vspace{-0.5cm} % Reduce vertical space after the table
\end{table}


\vspace{-2mm}
\subsection{Validation Based on Statistical Analysis}
\vspace{-2mm}
\begin{table*}[h]
     \scriptsize
    \centering
    
   \setlength{\tabcolsep}{0.3mm} % 减小列间距
\renewcommand\arraystretch{1} % 减少行间距
    \caption{Image statistics comparison results. "Difference" refers to the percentage difference in data.Each Image statistics references:\citep{tsai2008information},\citep{de2013image},\citep{de1996naturalness},\citep{sebastian2012gray},\citep{sebastian2012gray},\citep{tahmid2017density},\citep{he2005laplacian},\citep{frank2020leveraging},\citep{chan2018convex},\citep{freitas2018application},~\citep{chen2024statistical}}
    \label{Statistics}
      \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
   
     & GLCM  & GLCM  & Canny  & Variance Blur & Mean  & Edge  & Entropy & Sharpness & Saturation & Texture   &  Image \\ 
     & Contrast & Energy & Edge & Measure & spectrum & Histogram & Strength  &  Score& & & Realism  \\
     
    \cline{1-12}
    Watermarked & 354.80 & 0.19 & 46.41 & 1124.47 & 85.73 & 6.00 & 6.31 & 8836.29  & 63.04 & 5.92  & 1.55\\
    Clean       & 371.23 & 0.19 & 48.34 & 1209.43 & 88.18 & 7.00 & 6.23 & 8903.23  & 67.00 & 5.65  & 1.42 \\
    Difference  & 4.63\% & 0.00\% & 4.16\% & 7.56\% & 2.86\% & 16.67\% & 1.27\% & 0.76\%  & 6.28\% & 4.56\%   & 8.39\%\\
  
    \end{tabular}}
\end{table*}



We use image statistics based on human perception as evaluation metrics. Based on the sensitivity coefficient \( s \) proposed by ~\citep{chen2024statistical}, we selected 11 image statistics measures with sensitivity values above 0.3, indicating a high sensitivity to changes in texture, edges, and frequency. Sensitivity ranges from 0 to 1, with higher values indicating greater sensitivity. 

As shown in Table~\ref{Statistics}, the average change between watermarked and clean images across these metrics is approximately 5\%. The GLCM Energy remains unchanged (0.00\%), indicating that watermark embedding does not significantly affect image energy, which reflects texture uniformity and homogeneity.  The Edge Histogram shows the largest difference (16.67\%), highlighting its sensitivity to structural changes introduced by watermarking. This suggests that watermarking impacts edge distribution and frequency more than other visual characteristics, making the Edge Histogram an effective measure for detecting structural alterations.





\vspace{-2mm}
\subsection{Watermark Attack Performance}
\vspace{-2mm}

We assessed the robustness of our watermarking method against common attacks, considering only those that preserve the visual quality of the generated image. For instance, while severe cropping (over 70\%) reduces watermark recognition accuracy, it introduces visible tampering, making the attack perceptible. Thus, we focus on common attacks such as rotation, blurring, texture reduction, and image compression.

\begin{table*}[h]
    \centering
    \scriptsize
    \renewcommand\arraystretch{1}
    \setlength{\tabcolsep}{1mm}{
    \caption{Classification results of Watermarked Images under various attacks. The classification network distinguishes whether an image contains a watermark and then classifies the watermark.}
    \label{tab:attacks}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l|c|c|c|c|c|c|c}
   
     \multicolumn{2}{c}{} & \multicolumn{6}{c}{Attack Type} \\
    \cline{2-8}
  
    & No Attack & Rotation & Blurring  & Texture Reduction & Image Compression & Crop & Flip \\
    \cline{1-8}
     
    Watermark Presence & 100.00\% & 99.30\% & 100.00\% & 95.70\% & 98.50\% & 99.10\% & 100.00\% \\
    \cline{1-8}
    Watermark Classification & 97.00\% & 95.98\% & 93.97\% & 93.94\% & 94.72\% & 94.05\% & 96.10\% \\
    
    \end{tabular}}}
    \vspace{-0mm}
\end{table*}
As shown in Table~\ref{tab:attacks}, in the absence of attacks, "Watermark Presence" accuracy reached 100.00\%, indicating perfect detection in undisturbed images. Texture reduction caused the largest drop (4.3\%), likely due to its impact on watermark details, weakening detectability. Image compression, particularly lossy compression, also reduced accuracy to 98.50\%. However, flip and crop attacks had minimal impact, maintaining accuracy at 100.00\% and 99.10\%, respectively.

For the "Watermark Classification" task, accuracy for unaltered images was high (97.00\%). However, attacks slightly reduced classification accuracy. Blurring and texture reduction had the greatest impact, lowering accuracy to 93.97\% and 93.94\%, respectively, suggesting these attacks degrade visual features, making classification more challenging. Image compression and rotation also affected classification but to a lesser extent.

\section{Conclusion}

In this paper, we propose a dual approach for embedding fixed QR-codes within the diffusion process and dynamic watermarks in generated images. This integration enhances intellectual property protection and traceability in generated content. The dynamic watermark undergoes feature and pixel space transformations, increasing resistance to attacks while preserving image quality.  It remains intact even under rotation, blurring, and compression. Statistical validation shows dual watermarking method has minimal impact on image quality, with only a 5\% variation in key metrics such as edge and texture attributes, confirming the method's invisibility and robustness. Our work addresses critical challenges in the ethical use of AI-generated content, providing a scalable and effective mechanism for ownership verification and misuse prevention. Future research could explore extending this framework to other generative models and applications, further advancing the field of digital content security.





\bibliography{egbib}
\bibliographystyle{iclr2025_conference}



\end{document}
