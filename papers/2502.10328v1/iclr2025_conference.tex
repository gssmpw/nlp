
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% added
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{bbm}
\usepackage{algpseudocode}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{graphics}
\usepackage{cleveref} 

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\newcommand\blfootnote[1]{
    \begingroup
    \renewcommand\thefootnote{}\footnote{#1}
    \addtocounter{footnote}{-1}
    \endgroup
}


\newtheorem{remark}{Remark}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcounter{xxx}
\setcounter{xxx}{0}
\newcommand\XXX[1]{{\bf \em \addtocounter{xxx}{1} (\thexxx) [[#1]]}}

\title{Generalised Parallel Tempering: \\Flexible Replica Exchange via \\Flows and Diffusions}

\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Leo Zhang$^{*\dagger}$ \; Peter Potaptchik$^{*\dagger}$ \; Arnaud Doucet$^{\dagger}$ \; Hai-Dang Dau$^{\ddagger}$\; Saifuddin Syed$^{\dagger}$ \\
% \thanks{ Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.  Funding acknowledgements go at the end of the paper.} \\
$^{\dagger}$University of Oxford, $^{\ddagger}$National University of Singapore \\
\printfnsymbol{1}Equal contribution.
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Parallel Tempering (PT) is a classical MCMC algorithm designed for leveraging parallel computation to sample efficiently from high-dimensional, multimodal or otherwise complex distributions via annealing. One limitation of the standard formulation of PT is the growth of computational resources required to generate high-quality samples, as measured by effective sample size or round trip rate, for increasingly challenging distributions. To address this issue, we propose the framework: Generalised Parallel Tempering (GePT) which allows for the incorporation of recent advances in modern generative modelling, such as normalising flows and diffusion models, within Parallel Tempering, while maintaining the same theoretical guarantees as MCMC-based methods. For instance, we show that this allows us to utilise diffusion models in a parallelised manner, bypassing the usual computational cost of a large number of steps to generate quality samples. Further, we empirically demonstrate that GePT can improve sample quality and reduce the growth of computational resources required to handle complex distributions over the classical algorithm.

\end{abstract}

\section{Introduction}

\looseness=-1
Sampling from a complex probability distribution $\pi$ over a state-space $\mathcal{X}$, whose density $\pi(x)$ is only known up to a normalising constant, is a fundamental task in modern statistical inference. Markov Chain Monte Carlo (MCMC) methods are a popular class of algorithms employed for such purposes that rely on constructing a Markov chain $(X_t)_{t\in\mathbb{N}}$ over $\mathcal{X}$ using a sequence of local moves which leave the target distribution invariant. While MCMC algorithms are guaranteed to converge asymptotically, standard approaches, in practice, struggle when the target distribution is complex with multiple well-separated modes. This is due to the low likelihood of the Markov chain crossing low-density regions \citep{latuszynski2025mcmc}. Unfortunately, multimodality is ubiquitous in challenging real-world modelling problems \citep{papamarkou2022challenges, henin2022enhanced}, limiting the usefulness of standard MCMC methods. To handle such cases, \emph{Parallel Tempering} (PT) \citep{geyer1991markov} (also known as the Replica-Exchange Monte Carlo \citep{PhysRevLett.57.2607,hukushima1996exchange}) is a popular class of MCMC algorithms designed to improve the global mixing of locally efficient MCMC algorithms.

\looseness=-1
PT works by considering an \emph{annealing path} $\pi_0,\pi_1,\dots, \pi_N$ of distributions over $\mathcal{X}$ interpolating between a simple reference distribution $\pi_0$ (e.g. a Gaussian), which we can efficiently sample and evaluate the normalised density, and the target distribution $\pi_N=\pi$. PT algorithms involve constructing a Markov chain $(\textbf{X}_t)_{t\in\mathbb{N}}$ on the extended state-space $\mathcal{X}^{N+1}$, targeting the joint annealing distribution $\bpi(\x)=\pi_{0}(x^0)\cdots\pi_{N}(x^{N})$. The Markov dynamics of the PT chain $\mathbf{X}_t=(X_t^0, \ldots, X_t^{N})$ alternate between (1) a \emph{local exploration phase} where the $n$-th chain \footnote{Following the PT literature, we also refer to components of $\mathbf{X}_t$ as chains.} of $\mathbf{X}_t$ is updated according to a $\pi_n$-invariant MCMC algorithm; and (2) a \emph{communication phase} which proposes a scheduled sequence of swaps between neighbouring states accepted according to a Metropolis-Hastings correction ensuring invariance. Crucially, PT is designed to offset the additional computation burden of simulating the extended $N$ chains through the use of \emph{parallel computation}, allowing for a similar effective computational cost as a single chain when implemented in a maximally parallelised manner. 

\setlength{\textfloatsep}{10pt}
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{flow-gept.png}
    \caption{One thousand samples from Linear-PT (left) and Flow-GePT (right) for GMM-2 using six chains. Linear-PT struggles to capture all modes, primarily sampling near the reference, while Flow-GePT successfully generates samples from all modes, demonstrating improved exploration. See \Cref{subsect:flow-gept-experiments} for details.}
    \label{fig:flow-gept}
\end{figure}

Typically, the chains $X_t^n$ mix faster when closer to the reference and struggle closer towards the target. Therefore, communication between the reference and target, facilitated through swaps transporting samples between neighbouring chains, can induce rapid mixing between modes of the target component \citep{woodard2009conditions,surjanovic2024uniform}. The importance of the swapping mechanism for PT has led to a literature dedicated to optimising communication between the reference and target by tuning the swapping schedule \citep{syed2022non}, annealing path \citep{syed2021parallel}, and reference distribution \citep{surjanovic2022parallel}. However, such works still rely on the same naive, inflexible replica-exchange mechanism originally formalised in \cite{geyer1991markov}.

As an alternative to PT, recent work \citep{midgley2022flow, vargas2023denoising} has explored \emph{neural samplers} which use more flexible approximate methods, such as normalising flows and diffusion models \citep{papamakarios2021normalizing, songscore}, for sampling. However, neural-sampling methods usually incur a bias, foregoing the asymptotic consistency and theoretical guarantees of MCMC, and can be expensive to implement and train. Despite its inflexibility, it has been empirically shown in \cite{he2025tricktreatpursuitschallenges} that PT provides a strong baseline for such methods.  

In this paper, in order to combine the benefits of neural samplers with the robustness of PT, we mathematically formalise the framework introduced by \cite{ballard2009replica, ballard2012replica} in the statistical physics literature for designing more flexible replica-exchange mechanisms; we term the resulting algorithm, \emph{Generalised Parallel Tempering} (GePT) (see \Cref{alg:GePT} and \Cref{fig:flow-gept}). GePT preserves the same mixing and asymptotic consistency properties of classical PT and can be easily incorporated into existing PT implementations. We then provide two concrete instances of GePT, \emph{Flow-GePT} and \emph{Diff-GePT}, which respectively utilise methods from normalising flows and diffusion models to construct flexible swap moves. This strengthens PT with the ability of flows and diffusion models to effectively learn transport maps between neighbouring distributions. Further, we demonstrate how these algorithms can use flows and diffusions in a \emph{parallelised} manner. This circumvents the usual computational burden of requiring a large number of steps to generate samples while retaining the asymptotic guarantees of MCMC methods. Finally, we empirically demonstrate that Flow-GePT and Diff-GePT outperform PT by accelerating the communication between the reference and target states, even when controlling for sequential computation. 

\paragraph{Related work}
Previous work on neural samplers can be roughly organised as being diffusion/flow-based \citep{vargas2023denoising, zhang2021path, berner2022optimal, zhang2023diffusion, akhound2024iterated, nusken2024transport, tian2024liouville, albergo2024nets}, normalising flows-based \citep{albergo2019flowmcmc, noe2019boltzmann, gabrie2022mcmcflows, midgley2022flow} and Sequential Monte Carlo (SMC)-based \citep{arbel2021annealed, doucet2022score, matthews2023continualrepeatedannealedflow, phillips2024particle}. We note that these classifications are not strict - for example, \cite{phillips2024particle} uses techniques from both diffusion models and SMC. A relevant prior work integrating PT with normalising flows is \cite{invernizzi2022skipping}. Their approach involves training a normalising flow to directly map configurations from the highest-temperature reference distribution of a molecular system to the lowest-temperature target distribution, effectively bypassing the intermediate annealing distributions. By contrast, our framework leverages normalising flows to facilitate exchanges between all neighbouring temperature levels, thereby enhancing sampling efficiency across the entire annealing path and providing a more stable training objective.

\section{Background}

\subsection{Parallel Tempering}

\looseness=-1
Let $(\mathcal{X},\mathcal{F})$ be a measurable space. Let $\pi_0,\pi_1,\dots,\pi_N$ be an annealing path which admits densities $\pi_n(\ddd x)=\tilde{\pi}_n(x)\ddd x/Z_n$ with respect to a base measure $\ddd x$.
We can evaluate the un-normalised density $\tilde{\pi}_n(x)>0$ but typicially do not know the normalising constant $Z_n=\int_\mathcal{X}\tilde{\pi}_n(x)\ddd x$. There is considerable flexibility in choosing the annealing distributions \citep{syed2021parallel, phillips2024particle}. However, a common choice is the \emph{linear path} $\pi_n(x)\propto \pi_0(x)^{1-s_n}\pi_N(x)^{s_n}$, which linearly interpolates between the reference and target densities in log-space, according to an annealing schedule $0=s_0<s_1\cdots<s_N=1$.

Given an annealing path, the PT algorithm constructs a Markov chain $\mathbf{X}_t=(X_t^0, \ldots, X_t^{N})$ on the extended state-space $\mathcal X^{N+1}$ invariant to the joint distribution $\bpi(\x) = \bpi(\ddd x^0, \ldots, \ddd x^{N}) = \prod_{n=0}^{N} \pi_i(\ddd x^i)$.
We construct $\textbf{X}_t$ from $\textbf{X}_{t-1}$ through a \emph{local exploration} move followed by a \emph{communication} move, encoded by the local exploration and communication Markov kernels $\K^{\expl}$, and $\K_t^{\comm}$ respectively on $\mathcal{X}^{N+1}$- i.e. $\mathbf{X}_t\sim\K_{t}^\comm \K^\expl(\mathbf{X}_{t-1}, \ddd\x_t)$. We informally describe how these kernels act on $\mathcal{X}^{N+1}$ below and formalise their construction in \Cref{app:kernels}.

\paragraph{Local exploration}\label{sec:exploration}
Given a state $\x=(x^0,\dots, x^N)$ on the extended state-space $\mathcal{X}^{N+1}$, the local exploration kernel $\K^\expl(\x,\ddd \x')=\prod_{n=0}^N K_n(x^n,\ddd x'^n)$ defined on $\mathcal{X}^{N+1}$ updates $x^n$ using a $\pi_n$-invariant kernel $K_n(x^n,\ddd x'^n)$ on $\mathcal{X}$. In general, we will assume that $K_0(x,\ddd x')=\pi_0(\ddd x')$ generates an independent sample from the reference, and $K_n(x,\ddd x')$ corresponds to a MCMC move targeting $\pi_n$ for $n>0$. Notably, each coordinate can be updated in parallel. 

\paragraph{Communication}\label{sec:communication}
For $n=1,\dots,N$, we define the PT swap kernel $\mathbf{K}_{n-1,n}(\x, \ddd\x')$ as swapping the $n-1$-th and $n$-th coordinates of $\x\in\mathcal{X}^{N+1}$ with probability $\alpha_n(x^{n-1};x^n)$ equal to,
\begin{align}
\label{eq:swap_proba}
    \alpha_{n-1,n}(x^{n-1};x^n) &= 1 \wedge
    \frac{[\ddd \pi_n/\ddd \pi_{n-1}](x^{n-1})}{[\ddd \pi_n/\ddd \pi_{n-1}](x^n)} \\
    \label{eq:swap_proba_bis}
    &=1 \wedge
    \frac{\tilde{\pi}_{n-1}(x^n)\tilde{\pi}_n(x^{n-1})}{\tilde{\pi}_{n-1}(x^{n-1})\tilde{\pi}_n(x^{n})},
\end{align}
where $\ddd \pi_n/\ddd \pi_{n-1}$ is the Radon-Nikodym derivative of $\pi_n$ with respect to $\pi_{n-1}$. 

The swap kernel $\K_{n-1,n}$ leaves $\bpi$ invariant. Therefore, we can obtain a valid communication move by composing a sequence of swap kernels $\K_{n-1,n}$. In practice, it is advantageous to use the scheme from \emph{non-reversible} PT (NRPT) \citep{okabe2001replica, syed2022non}, which performs as $\K^\comm_t=\prod_{n \text{ even}} \K_{n-1,n}$ when $t$ is even and $\K^\comm_t=\prod_{n \text{ odd}} \K_{n-1,n}$ when $t$ is odd. Finally, we note that the swap moves outlined above can be applied in parallel, allowing for distributed implementations of PT to leverage parallel computation to accelerate sampling \citep{surjanovic2023pigeons}. 

\paragraph{Performance metrics for PT}

While the effective sample size (ESS) of samples generated by a Markov chain is the gold standard for evaluating the performance of MCMC algorithms, in our setting, we are mainly interested in improving the swap kernel within PT. As ESS measures the intertwined performance of the local exploration and swap kernels, we are instead interested in measuring performance in terms of the \emph{round trip rate} \citep{katzgraber2006feedback, lingenheil2009efficiency}. This metric is used in the PT literature to track the rate at which independent samples from the reference are transported to the target \citep{surjanovic2024uniform}. Notably a higher round trip rate is obtained if fewer proposed swaps are rejected. For further details, see \Cref{app:round-trip}.

\paragraph{Normalising flows}
Normalising flows \citep{tabak2010density, rezende2016variationalinferencenormalizingflows,dinh2017densityestimationusingreal} provide a flexible framework for approximating complex probability distributions by transforming a simple base distribution (e.g., a Gaussian) through a sequence of invertible, differentiable mappings. Formally, a normalising flow models a target distribution $\pi$ using a bijective function $F_{\theta} : \mathcal{X} \to \mathcal{X}$, typically a neural network parameterized by
$\theta$, where samples are generated as $x=F_{\theta}(z)$, where $z \sim p_z$, a simple prior distribution. The density of the transformed distribution is given by the change of variables formula:
\begin{equation*} p(x) = p_z(F_{\theta}^{-1}(x)) \left| \det J_{F_{\theta}}(F_{\theta}^{-1}(x)) \right|^{-1}, \end{equation*}
where $J_{F_{\theta}}$ is the Jacobian of $F_{\theta}$. If samples from the target distribution are available, normalising flows can be trained via maximum likelihood estimation (MLE). Alternatively, when only the un-normalised density is known, training can be done by minimizing the reverse Kullback-Leibler (KL) divergence: \( \text{KL}(p \| \pi) = \int p(x) \log \frac{p(x)}{\pi(x)} \,dx \).

\paragraph{Diffusion models}
Given a target distribution $\pi$, the Variance-Preserving (VP) diffusion process is defined by the following stochastic differential equation (SDE) $dY_s = -\beta_sY_s \mathrm{d}s + \sqrt{2\beta_s} \mathrm{d}W_s$, where $Y_0\sim \pi$ and $\beta_s:[0, 1]\to\mathbb{R}^+$. 
The dynamics defined the SDE induces a path of distributions $(\pi_s^\text{VP})_{s\in[0, 1]}$ such that $Y_{1-s}\sim \pi_s^{\text{VP}}$ and $\pi_0^{\text{VP}}$ is close to a standard Gaussian\footnote{We reverse the time convention of diffusion models to align with the notation from the PT literature}. Diffusion models aim to learn an approximation to the score $\nabla\log \pi_s^{\text{VP}}$ through the score-matching objective \citep{vincent2011connection, songscore} to exploit the fact that the time-reversal SDE $(X_s)_{s\in[0, 1]}=(Y_{1-s})_{s\in[0, 1]}$ has the form $dX_s = [\beta_{1-s}X_s + 2\beta_{1-s} \nabla\log \pi_{s}^{\text{VP}}(X_s)]\ddd s + \sqrt{2\beta_{1-s}} \ddd B_s$ with $X_0\sim \pi_0^{\text{VP}}$ in order to generate approximate samples from $\pi$ by solving the reverse SDE with the learned score.

\section{Generalised Parallel Tempering}

A limitation of PT is the inflexibility of the swap kernels, which can only propose samples that can be directly exchanged between distributions, since the classic PT swap in \Cref{eq:swap_proba} only considers the relative densities of $\pi_{n-1}$ and $\pi_n$. It cannot exploit any knowledge of flows that transport one measure to another. For instance, if we have $X^{n-1}\sim\pi_{n-1}$, $X^n\sim\pi_n$ and have an invertible mapping $F_n:\mathcal{X}\to\mathcal{X}$ such that $F_n(X^{n-1})$ is $\pi_n$-distributed (and thus $F^{-1}_n(X^n)$ is $\pi_{n-1}$-distributed), we would like to be able to propose `generalised swaps' of the form $(x^{n-1}, x^{n}) \to (F^{-1}_n(x^{n}),  F_n(x^{n-1}))$ with $100 \%$ acceptance rate.

To address this limitation, we propose Generalised Parallel Tempering (GePT) which only differs from PT through the use of \emph{generalised swap} kernels $\bar{\K}_{n-1,n}$. Further, we provide specific examples of our method which incorporate techniques from the generative modelling literature to achieve high acceptance rates and accelerate sampling.

\subsection{Generalised PT Swap}\label{sec:gen-PT}

In addition to the annealing path, suppose that we have two user-chosen Markov kernels $P_n(x^{n-1},\ddd x^n_*)$ and $Q_{n-1}(x^n,\ddd x^{n-1}_*)$ on $\mathcal{X}$ for $n=1,\dots, N$, which we will refer to as the \emph{forward} and \emph{backward} kernels respectively. For $n\in \{1,\dots,N\}$, a \emph{generalised swap} kernel $\bar{\mathbf{K}}_{n-1,n}(\x, d\x')$ starts by generating a proposal $x^n_*$ given $x^{n-1}$ using the forward kernel $P_n$ to generate a forward proposal $x^{n-1}_*$ given $x^n$, and uses the backward kernel $Q_{n-1}$ to generate a backward proposal $x^{n-1}_*$. We then replace $x^{n-1},$ and $x^n$ in $\x$ with the proposed states $x^{n-1}_*$ and $x^n_*$ respectively with probability $\bar{\alpha}_n(x^{n-1}, x^{n}; x^{n-1}_*, x^{n}_*)$ equal to 
\begin{equation}
\label{eq:swap_proba_gen}
\bar\alpha_n(x^{n-1}, x^{n}; x^{n-1}_*, x^{n}_*)= 1 \wedge \frac{[\ddd \bar\pi_{n}^Q/\ddd \bar\pi_{n-1}^P](x^{n-1}, x^{n}_*)}{[\ddd\bar \pi_{n}^Q/\ddd\bar \pi_{n-1}^P](x^{n-1}_*, x^{n})},
\end{equation}
where $\bar \pi_{n-1}^P=\pi_{n-1}\otimes P_n$ and $\bar \pi_{n}^Q=\pi_n\otimes Q_{n-1}$ are the forward and backward extensions of $\pi_{n-1}$ and $\pi_n$ respectively defining probability measures on $\mathcal{X}\times\mathcal{X}$ equal to,
\begin{equation}
\label{eq:two_bar}
\begin{split}
\bar\pi_{n-1}^P(\ddd x^{n-1}, \ddd x^{n}) &= \pi_{n-1}(\ddd x^{n-1}) P_n(x^{n-1}, \ddd x^{n}),\\
\bar \pi_{n}^Q(\ddd x^{n-1}, \ddd x^{n}) &= \pi_n(\ddd x^n) Q_{n-1}(x^{n}, \ddd x^{n-1}).
\end{split}
\end{equation}
See \Cref{app:kernels-GePT-swaps} for a formal description of the GePT swap kernel $\bar{\K}_{n-1,n}$. By using $\bar{\K}_{n-1,n}$ in place of PT swap kernel kernel $\K_{n-1,n}$ we obtain the GePT algorithm described in \Cref{alg:GePT}.


\begin{algorithm}[H]
\caption{Generalised Parallel Tempering (GePT)}\label{alg:GePT}
\begin{algorithmic}[1]
    \State Initialise $\textbf{X}_0=(X^0_0,\dots,X^N_0)$;

    \For{$t=1,\dots, T$}
    \State $\textbf{X}_t=(X^0_t,\dots, X^N_t),\quad X^n_t\sim K_n(X^n_{t-1},\ddd x^n)$\Comment{Local exploration (see \Cref{sec:exploration})}
    \State  \textbf{if} $t$ is even, $\mathcal{S}=\{n \text{ even}\}$ \textbf{else} $\mathcal{S}=\{n \text{ odd}\}$ \Comment{Communication (see \Cref{sec:communication})}
    \For{$n\in \mathcal{S}$}
    \State 
    $X^{n-1}_*\sim Q_{n-1}(X^n_t,\ddd x^{n-1}_*)$ \Comment{Backward proposal}
    \State 
    $X^{n}_*\sim P_n(X^{n-1}_t,\ddd x^n_*)$ \Comment{Forward proposal}
    \State Simulate $U\sim \mathrm{Uniform}([0,1])$
    \If{$U< \bar\alpha(X^n_t,X^n_t,X^{n-1}_*,X^n_*)$} \Comment{See \Cref{eq:swap_proba_gen}.}
    \State $X^{n-1}_t,X^n_t\gets X^{n-1}_*,X^n_*$  \Comment{Generalised PT swap (see \Cref{sec:gen-PT})}
    \EndIf
    \EndFor
\EndFor
\Ensure \textbf{Return:} $\textbf{X}_1,\dots,\textbf{X}_T$
\end{algorithmic}
\end{algorithm}

Notably, the acceptance rate is equal to $1$ if $\bar \pi_{n-1}^P \equiv \bar \pi_{n}^Q$. Therefore, we aim to choose $P$ and $Q$ so that the two distributions are as close as possible. Moreover, we recover the classical PT swap move when the forward and backward kernels are the identity kernels. Finally, \Cref{prop:validity_swap_gen} shows that the generalised swap kernel $\bar\K_{n-1,n}$ formalised above is valid and can be used within a PT algorithm instead of the classic swap kernel $\K_{n-1,n}$. See \Cref{sec:proofs} for the proof.

\begin{proposition}
	\label{prop:validity_swap_gen}
	Suppose that the two measures $\bar\pi_{n-1}^P$ and $\bar\pi_n^Q$ defined in \Cref{eq:two_bar} are mutually absolutely continuous. Then
	$\bar{\boldsymbol{\mathrm{K}}}_{n-1,n}$ keeps the distribution $\bpi$ invariant.
\end{proposition}

\subsection{GePT with Deterministic flows}
\label{subsect:gept_deterministic}
\looseness=-1
We now return to the motivation outlined at the beginning of this section and consider forward and backward kernels defined by a mapping $F_n:\mathcal{X}\to\mathcal{X}$. The following proposition, of which the proof is postponed to \Cref{proof:det-swap}, gives a readily calculable expression for $\bar \alpha_n$ in this case.
\begin{proposition}\label{prop:deterministic_swap}
	Let $\mathcal X = \mathbb R^d$ and suppose that $\pi_{n-1}$ and $\pi_n$ admit strictly positive densities $\tilde \pi_{n-1}$ and $\tilde \pi_n$ with respect to the Lebesgue measure. Let $F_n:\mathbb R^d\to \mathbb R^d$ be a diffeomorphism with Jacobian matrix $J_{F_n}(x)$. If we choose forward and backward kernels $P_n$ and $Q_{n-1}$ such that 
	\begin{equation}
	P_n(x^{n-1}, \ddd x^{n}_*) = \delta_{ F_n(x^{n-1})}(\ddd x^{n}_*)\qquad
	Q_{n-1}(x^{n}, \ddd x^{n-1}_*) = \delta_{ F^{-1}_n(x^{n})}(\ddd x^{n-1}_*),
	\end{equation} 
then we have the following expression for $\bar \alpha_{n-1,n}$ given in \Cref{eq:swap_proba_gen}:
\begin{equation}\label{eq:deterministic_swap_proba}
     \bar\alpha_{n-1,n} (x^{n-1}, x^{n}; x^{n-1}_*, x^{n}_*) = 1 \wedge \left[ \frac{\tilde\pi_{n-1}(x^{n-1}_*)\tilde\pi_n(x^n_*)}{\tilde\pi_{n-1}(x^{n-1})\tilde\pi_n(x_n)} \cdot \frac{|\det(J_{F_n} (x^{n-1}))|}{ |\det(J_{ F_n}(x^{n-1}_*))|} \right]. 
\end{equation}
\end{proposition}
We refer to an instance of GePT using the swap kernel described above as Flow-GePT. We note that \cite{arbel2021annealed, matthews2023continualrepeatedannealedflow} use normalising flows in a similar manner to map between annealing distribution, but in the context of SMC samplers \citep{del2006sequential}. 

\paragraph{Training}
An advantage of the GePT framework is its flexibility in training. One can either run standard PT (without flows) or GePT (incorporating flows), where $F_n$ is initialized at the identity transformation so that early training behaves like PT. After a sufficient burn-in period, samples from various distributions along the annealing path can be used to train the flows, gradually improving their ability to approximate transport between distributions. For further details, see \Cref{app:flow-gept}.

\subsection{GePT with Diffusion Swaps}
\label{subsect:diffusion_swaps}

If we assume that we have access to the path of distributions $(\pi_s^{\text{VP}})_{s\in[0, 1]}$ induced by a VP diffusion process, we can construct an annealing path $\pi_n = \pi_{s_n}^\text{VP}$ for GePT, given some annealing schedule $0=s_0<s_1\cdots<s_N=1$. We note that the joint distribution $\pi_{s_{n-1}, s_n}^\text{VP}(\diff x^{n-1}, \diff x^{n})$ has the factorisations $\pi_{n-1}(\diff x^{n-1})P_n(x^{n-1}, \diff x^n) = \pi_n(\diff x^n)Q_{n-1}(x^n, \diff x^{n-1})$ where it is well-known that $Q_{n-1}$ is a closed-form Gaussian kernel and $P_n$ can be well-approximated with a Gaussian kernel derived from the form of the time-reversal SDE. Therefore, it is a natural choice to define $P_n, Q_{n-1}$ by such kernels to ensure an improved acceptance probability. We note that the use of such a path and kernels is similar to \cite{phillips2024particle} which employs these components within a SMC sampler instead. The following proposition, of which the proof is postponed to \Cref{proof:diff-swap}, gives a readily calculable expression for $\bar \alpha_n$ in this case.

\begin{proposition}\label{prop:diff-swap}
    Let $\mathcal X = \mathbb R^d$ and suppose that $\pi_{n-1}$ and $\pi_n$ admit strictly positive densities $\tilde \pi_{n-1}$ and $\tilde \pi_n$ with respect to the Lebesgue measure. If we choose forward and backward kernels $P_n$ and $Q_{n-1}$ such that they admit strictly positive densities $\tilde{P}_n$ and $\tilde{Q}_{n-1}$ with respect to the Lebesgue measure, then we have the following expression for $\bar \alpha_n$ given in \Cref{eq:swap_proba_gen}:
    \[ \bar\alpha (x^{n-1}, x^{n}; x^{n-1}_*, x^{n}_*) = 1 \wedge \left[
    \frac
    {\tilde\pi_{n-1}(x^{n-1}_*)\tilde{P}_n(x_*^{n-1}, x^n)\tilde\pi_n(x^n_*)\tilde{Q}_{n-1}(x_*^n, x^{n-1})}
    {\tilde\pi_{n-1}(x^{n-1})\tilde{P}_n(x^{n-1}, x_*^n)\tilde\pi_n(x_n)\tilde{Q}_{n-1}(x^{n}, x_*^{n-1})} \right]. 
    \]
\end{proposition}

The expression in the denominator is the density of the quadruple $(x^{n-1}, x^n; x^{n-1}_*, x^n_*)$ before the swap. The expression in the numerator can be thought of informally as a hypothetical `post-swap density', i.e. when $(x^{n-1}, x^n)$ becomes $(x^{n-1}_*, x^n_*)$ and vice versa. 

We refer to an instance of GePT using the swap kernel described above as Diff-GePT. We also note that the above kernel can be readily generalised to the case where we use $m>1$ Markov kernels to generate our proposed swap states via \cite{nilmeier2011nonequilibrium}. We refer to this algorithm as $m$-step Diff-GePT where $m=1$ coincides with using the above kernel. For further details, see \Cref{app:multi-step}.

\paragraph{Training}

To implement this algorithm in practice, as we do not have access, in general, to the analytical form of the diffusion path $(\pi_s^\text{VP})_{s\in[0, 1]}$, we parametrise an energy-based model \citep{salimans2021should} $\pi_s^\theta$ using a modified version of the parametrisation from \cite{phillips2024particle} which satisfies the boundary conditions $\pi_0^\theta = \gN(0, \mathrm{I})$ and $\pi_1^\theta = \pi$. We can then iteratively run Diff-GePT with the approximation $(\pi_s^\theta)_{s\in[0, 1]}$ in order to generate samples from $\pi$ that are then used to improve the approximation $\pi_s^\theta$ via the score-matching objective. We stress that a parallelised implementation of Diff-GePT allows for generating a sample from $\pi$ with a similar effective cost as a single discretisation step solving the time-reversal SDE. For further details, see \Cref{app:diff-gept}.

\section{Experiments}

We evaluate our proposed algorithms: Flow-GePT and Diff-GePT, against the baseline of PT using the linear path with reference $\pi_0=\gN(0, \mathrm{I})$ (Linear-PT), on the commonly used 40-mode Gaussian mixture model (GMM-$d$) distribution in $d$ dimensions \citep{midgley2022flow}. See \Cref{app:target-dists} for further details about the target distribution we use.

\subsection{Flow-GePT Experiments} \label{subsect:flow-gept-experiments}
We compare Flow-GePT on GMM-2 with Linear-PT using six chains (see \Cref{fig:flow-gept} for the generated samples). We observe that six chains are insufficient for PT to effectively sample from GMM-2, as most modes are missed, with samples primarily generated in modes that remain close to the reference. In contrast, samples from Flow-GePT, shown on the right of \Cref{fig:flow-gept}, demonstrate that incorporating flows facilitates sampling from all modes of the distribution. Additionally, the swap accept-reject mechanism ensures that the local shapes are accurately preserved. See \Cref{app:algo-details-flow-gept} for more details.

\subsection{Diff-GePT Experiments}

We compare $m$-step Diff-GePT ($m=1, 2, 5$) against the baselines of Linear-PT and PT with the diffusion path (Diff-PT) in terms of round trip rate to assess the improved communication of states by the Diff-GePT swap kernels. In addition, to control for the additional sequential computation that $m$-step Diff-GePT requires (when considering parallelised implementations of all algorithms), we also report the \emph{compute-normalised round trip rate}. Further, to ensure a fair comparison, we tune the annealing schedule with the algorithm from \cite{syed2021parallel} and use the same local exploration kernels for all methods. For further details, see \Cref{app:algo-details-diff-gept}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{exp_2_plot.pdf}
    \caption{Round trip metrics for $m$-step Diff-GePT ($m=1, 2, 5$) and Diff-PT using the true diffusion path, and Linear-PT targeting GMM-$d$ for $d=2, 10, 50, 100$ when using 30 chains. (Left) Round trip rate against $d$. (Right) Compute-normalised round trip rate against $d$.}
    \label{fig:theoretical-diff-gept}
\end{figure}

\paragraph{Scaling of (ideal) Diff-GePT with dimensions}

To understand the theoretical performance of $m$-step Diff-GePT for a fixed number of chains when we scale $d$, we use the fact that the path of distributions $(\pi^\text{VP}_s)_{s\in[0, 1]}$ induced by a VP diffusion process initialised at GMM-$d$ is analytically tractable, in order to run $m$-step Diff-GePT with the true diffusion path. In \Cref{fig:theoretical-diff-gept}, we compare the resulting round trip rate and compute-normalised round trip rate of the different algorithms when we fix the number of chains to 30.

\looseness=-1
We see that, for all values of $d$, the round trip rate of Diff-GePT monotonically increases over Diff-PT and Linear-PT as we increase $m$, with greater gains for larger values of $d$, demonstrating that our proposed Diff-GePT swap kernels improve the communication of states over the standard swap kernel. Further, while there is a smaller difference between algorithms and we do see that Linear-PT and Diff-PT is able to outperform Diff-GePT for $d=2$, overall, we note a similar trend with the compute-normalised round trip rate, suggesting the additional computation for Diff-GePT is justified.

\begin{table}[h!]
\centering
\caption{Compute-normalised round trip rate of $m$-step Diff-GePT ($m=1, 2, 5$) and Diff-PT using a learned diffusion path, and Linear-PT targeting GMM-$d$ for $d=2, 10, 50$ where the number of chains is 10, 30 and 60.} 
\label{tab:learned-diff-gept}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{llllllllll}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c}{GMM-2} & \multicolumn{3}{c}{GMM-10} & \multicolumn{3}{c}{GMM-50} \\
\cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10}
 & 10 & 30 & 60 & 10 & 30 & 60 & 10 & 30 & 60 \\
\midrule
1-Diff-GePT & $0.0368$ & $0.0391$ & $0.0391$ & $0.00532$ & $0.0120$ & $\mathbf{0.0132}$ & $0.00$ & $0.00319$ & $0.00486$ \\
2-Diff-GePT & $0.0333$ & $0.0345$ & $0.0331$ & $0.00749$ & $\mathbf{0.0126}$ & $0.0123$ & $\mathbf{0.00045}$ & $\mathbf{0.00425}$ & $\mathbf{0.00534}$ \\
5-Diff-GePT & $0.0237$ & $0.0227$ & $0.0219$ & $\mathbf{0.00790}$ & $0.0102$ & $0.0072$ & $0.00140$ & $0.00379$ & $0.00394$ \\
\midrule
Diff-PT & $0.0529$ & $0.0604$ & $0.0636$ & $0.00339$ & $0.00914$ & $\mathbf{0.0132}$ & $0.00$ & $0.00108$  & $0.00203$ \\
\midrule
Linear-PT & $\mathbf{0.0584}$ & $\mathbf{0.0654}$ & $\mathbf{0.0695}$ & $0.00330$ & $0.00944$ & $0.0129$ & $0.00$ & $0.00106$ & $0.00195$ \\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{Learned Diff-GePT}

We also compare the performance of $m$-step Diff-GePT and Diff-PT with a learned diffusion path targeting GMM-$d$ when we scale $d$ and $N$; for details about training, see \Cref{app:algo-details-diff-gept}. In \Cref{tab:learned-diff-gept}, we report the compute-normalised round trip rate of each method where we scale $d$ in $\{2, 10, 50\}$ and the number of chains in $\{10, 30, 60\}$.

We see that increasing the number of chains tends to increase compute-normalised round trip rates, however with diminishing returns due to the corresponding increase in distance that states have to traverse. In addition, we see that the  performance of $m$-step Diff-GePT for a fixed number of chains, on the whole, largely mirrors the trends seen from Diff-GePT with the true diffusion path in \Cref{fig:theoretical-diff-gept}, but with reduced rates. For example, 1-step Diff-GePT with 30 chains targeting GMM-50 provides a $3\times$ improvement on rates compared to the corresponding Linear-PT samples. These results demonstrate that our Diff-GePT swap kernels can still improve communication with the additional cost justified, and with substantial gains in higher dimensions, even with an imperfect approximation to $(\pi_s^\text{VP})_{s\in[0, 1]}$.

\section{Conclusion}

In this work, we have introduced and formalised the design of more flexible swap kernels within PT under the framework of Generalised Parallel Tempering. In addition, we have proposed two concrete examples, Flow-GePT and Diff-GePT which utilise normalising flow and diffusion models to construct swaps. Empirically, we have demonstrated the ability of these methods to improve sample quality over classical PT even when controlling for additional computational cost. In the future, we intend to validate GePT on more complex distributions and to further refine the training procedure involved in GePT.

\subsubsection*{Acknowledgments}
The authors are grateful to George Deligiannidis for helpful discussions. LZ and PP are supported by the EPSRC CDT in Modern Statistics and Statistical Machine Learning (EP/S023151/1). SS acknowledges support from the EPSRC CoSInES grant (EP/R034710/1) and the NSERC Postdoctoral Fellowship.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage

\appendix

\section{PT kernels}\label{app:kernels}

\subsection{Local exploration}\label{app:kernels-local-exploration}
The local exploration kernel $\K^\expl(\x,\ddd x')$ generates sample $\x'=(x'^0,\dots, x'^N)$ by updating each coordinate of $\x=(x^0,\dots, x^N)$ using a $K_n(x,\ddd x')$ is a $\pi_n$-invariant Markov kernel. We will further assume that $K_0(x,\ddd x')=\pi_0(\ddd x')$ draws an independent sample from the reference. 
Formally, the local exploration kernel equals,
\[
\K^\expl(\x, \ddd \x')=\prod_{n=0}^NK_n(x^n,\ddd x'^n) = \pi_0(\ddd x'^0)\prod_{n=1}^NK_n(x^n,\ddd x'^n)
\]

We can simulate from $\K_{n-1,n}$ using \Cref{alg:local-explore}.

\begin{algorithm}[H]
\caption{Local exploration}\label{alg:local-explore}
\begin{algorithmic}[1]
\Require{$\x=(x^0,\dots, x^N)$}
    \State$x'^0\sim \pi_0(\ddd x'^0)$ \Comment{Generate sample from reference}
            \For{$n=1,\dots,N$}\Comment{Parallelisable}
            \State $x'^n\sim K_n(x^n,\ddd x'^n)$ \Comment{Run MCMC targeting $\pi_n$}
            \EndFor
    \Ensure $(x'^0,\dots,x'^N)$
\end{algorithmic}
\end{algorithm}


\subsection{PT swap}\label{app:kernels-PT-swaps}
For $n=1,\dots, N$ define the swap function $S_{n-1,n}(\x,x^{n-1}_*,x^n_*)$ that replaces the $n-1$ and $n$-th coordinates of $\x=(x^{0},\dots, x^N)$ with $x^{n-1}_*,x^n_*$ respectively,
\[
S_{n-1,n}(\x,x^{n-1}_*,x^n_*)=(x^0,\dots, x^{n-2},x^{n-1}_*,x^n_*,x^{n+1},\dots, x^N).
\]
The swap kernel $\K_{n-1,n}(\x,\ddd \x')$ deterministically proposes the state $S_{n-1,n}(\x,x^{n},x^{n-1})$ swapping components of $x^{n-1}$ and $x^n$ in $\x'$, and accepts with probability $\alpha_{n-1,n}(\x)$ depending only on $x^{n-1}$ and $x^n$,
\[
\alpha_{n-1,n}(\x)=1 \wedge
    \frac{[\ddd \pi_n/\ddd \pi_{n-1}](x^{n-1})}{[\ddd \pi_n/\ddd \pi_{n-1}](x^n)}.
\]
Formally, the PT swap kernel equals,
\[
\K_{n-1,n}(\x,\ddd \x')=\alpha_{n-1,n}(\x)\delta_{S_{n-1,n}(\x,x^n,x^{n-1})}(\ddd \x')+(1 - \alpha_{n-1,n}(\x)) \delta_{\x}(\ddd \x').
\]
We can simulating from $\K_{n-1,n}$ using \Cref{alg:swap}.


\begin{algorithm}
	\caption{PT swap between $n-1$ and $n$}
	\begin{algorithmic}
		\Require{$\x=(x^0,\dots, x^N)$}
		\State{$U \sim \operatorname{Uniform}[0, 1]$}
		\If{$U < \alpha_{n-1,n}(\x)$ }
		\State{$x^{n-1},x^n \gets x^n, x^{n-1}$}\Comment{Swap components $n-1$ and $n$ of $\x$}
		\EndIf
		\Ensure{$\x$}
	\end{algorithmic}
	\label{alg:swap}
\end{algorithm}

\subsection{Generalised PT Swap}\label{app:kernels-GePT-swaps}
The generalised PT swap kernel $\bar\K_{n-1,n}(\x,\ddd x')$ proposes a state $\x_*=(x^0_*,\dots,x^N_*)$ generated by $\textbf{Q}_{n-1,n}$ which replaces the $n-1$ and $n$-th coordinates of $\x=(x^0,\dots,x^N)$ using the backwards kernels and forward kernels applied to $x^{n}$ and $x^{n-1}$ respectively, 
\begin{equation}
  \textbf{Q}_{n-1,n}(\x,\ddd \x_*)=Q_{n}(x^{n},\ddd x^{n-1}_*)P_{n}(x^{n-1},\ddd x^n_*)\prod_{m\neq n-1,n}\delta_{x^m}(\ddd x^m_*)
\end{equation}
The proposed state $\x_*$ is then accepted with probability $\bar\alpha_{n-1,n}(\x;\x_*)$ depending only on the $n-1$ and $n$-th coordinates of $\x$ and $\x_*$,
\[
\bar\alpha_{n-1,n}(\x;\x_*)=1 \wedge \frac{[\ddd \bar\pi_{n}^Q/\ddd \bar\pi_{n-1}^P](x^{n-1}, x^{n}_*)}{[\ddd\bar \pi_{n}^Q/\ddd\bar \pi_{n-1}^P](x^{n-1}_*, x^{n})},
\]
where recall $\bar\pi_{n-1}^P=\pi_{n-1}\times P_n$ and $\bar\pi_{n}^Q=\pi_{n}\times Q_{n-1}$ are the forward and backward extensions of $\pi_{n-1}$ and $\pi_n$ respectively defined in \Cref{eq:two_bar}. Formally, the generalised PT swap kernel equals,
\begin{align*}
     \bar\K_{n-1,n}(\x,\ddd \x') 
     &= \int \bar\alpha_{n-1,n}(\x;\x_*) \textbf{Q}_{n-1,n}(\x,\ddd \x_*)\delta_{S_{n-1,n}(\x,x^{n-1}_*,x^n_*)}(\ddd \x')\\
    &~~~+
    \int (1-\bar\alpha_{n-1,n}(\x;\x_*)) \textbf{Q}_{n-1,n}(\x,\ddd \x_*)\delta_{\x}(\ddd \x')
\end{align*}


We can simulate from $\bar\K_{n-1,n}$ using \Cref{alg:swap_gen}.

\begin{algorithm}
	\caption{GePT swap between $n-1$ and $n$}
	\begin{algorithmic}
		\Require{$\x=(x^0,\dots, x^N)$}
		\State $x^{n-1}_* \sim Q_{n-1}(x^{n}, \ddd x^{n-1}_*)$ \Comment{Backward proposal}
        \State $x^n_* \sim P_n(x^{n-1}, \ddd x^n_*)$ \Comment{Forward proposal}
		\State $U \sim \operatorname{Uniform}([0, 1])$
		\If{$U < \alpha(x^{n-1}, x^n; x^{n-1}_*, x^{n}_*)$}
		\State{$x^{n-1},x^n \gets x^{n-1}_*,x^{n}_*$}\Comment{Swap $x^{n-1}, x^n$ with $x^{n-1}_*,x^n_*$.}
		\EndIf
		\Ensure{$\x$}
	\end{algorithmic}
	\label{alg:swap_gen}
\end{algorithm}
\section{Proofs}
\label{sec:proofs}

\subsection{\Cref{prop:validity_swap}}\label{proof:valid-swap}
\Cref{prop:validity_swap} shows applying the swap kernel $\mathbf K_{n-1,n}$ leaves $\bpi$ invariant. 
\begin{lemma}
	\label{prop:validity_swap}
	Suppose that the two measures $\pi_{n-1}$ and $\pi_{n}$ are mutually absolutely continuous. Then $\boldsymbol{\mathrm{K}}_{n-1,n}$ keeps the distribution $\bpi$ invariant.
\end{lemma}
\paragraph{Remark.}
The correctness of the swap step is well-known for the form of acceptance probability given by the ratio of products of densities (see \Cref{eq:swap_proba_bis}). In this lemma, we stress on the general form of acceptance probability given by the ratio of Radon-Nikodym derivatives (see \Cref{eq:swap_proba}).
This allows us to cast the \textit{generalised} swap kernel as an instance of the \textit{classic} swap kernel (see the proof of \Cref{prop:validity_swap_gen} given in \Cref{proof:validity_swap_gen}). More importantly it facilitates the derivation of the generalised swap probability for GePT with deterministic flows (see \Cref{subsect:gept_deterministic}) by unifying it in the same framework as stochastic GePT (\Cref{subsect:diffusion_swaps}).

\newcommand{\xonenew}{X^{n-1}_{\operatorname{new}}}
\newcommand{\xtwonew}{X^{n}_{\operatorname{new}}}
\newcommand{\ff}{\frac{\diff\pi_{n}}{\diff\pi_{n-1}}}
\newcommand{\espec}{\E_{x^{n-1}, x^{n}\simiid \pi_{n-1}}}

\begin{proof}
Let $(X^{n-1}, X^n)$ be distributed according to $\pi_{n-1}(\ddd x^{n-1}) \pi_n(\ddd x^n)$. The swap step is performed by first simulating a random variable $U$ uniformly in $[0,1]$, then updating the states as
\begin{equation*}
(\xonenew, \xtwonew) = 
    \begin{cases}
        (X^{n}, X^{n-1}) &\textrm{ if } U < \alpha_n(x^{n-1}, x^n) \\
        (X^{n-1}, X^n) &\textrm{ otherwise.}
    \end{cases}
\end{equation*}
To prove that the swap keeps $\bpi$ invariant, we consider a continuous bounded test function $\varphi: \mathcal X \times \mathcal X \to \mathbb R$ and show that
\[ \E[\varphi(\xonenew, \xtwonew)] = \E[\varphi(X^{n-1}, X^n)]. \]
We write, using the shorthand $\alpha = \alpha_n(X^{n-1}, X^n)$ where necessary,
	\begin{equation}
	\label{proof:pcore_dc}
	\begin{split}
	\E[\varphi(\xonenew, \xtwonew)] &= \E[\varphi(\xonenew, \xtwonew) \mathbbm{1}_{U<\alpha}] + 
	\E[\varphi(\xonenew, \xtwonew) \mathbbm{1}_{U>\alpha}] \\
	&= \E[\varphi(X^{n}, X^{n-1}) \alpha(X^{n-1}, X^{n})] + \E[\varphi(X^{n-1}, X^{n})(1-\alpha(X^{n-1}, X^{n}))] \\
	&= \E[\varphi(X^{n-1}, X^{n})] + \left\{ 
	\E[\varphi(X^{n}, X^{n-1}) \alpha] - \E[\varphi(X^{n-1}, X^{n}) \alpha]
	\right\}.
	\end{split}
	\end{equation}
	We have
	\begin{equation}
	\label{proof:pcore_t1}
	\begin{split}
	\E[\varphi(X^{n}, X^{n-1}) \alpha] &= 
	\espec \left[\varphi(X^{n}, X^{n-1}) \ff(X^n) \alpha\right]\\
	& = \espec \left[\varphi(X^{n}, X^{n-1})\bigg(\ff(X^n) \wedge \ff(X^{n-1})\bigg)\right].
	\end{split}
	\end{equation}
	Similarly we also have
	\begin{equation}
	\label{proof:pcore_t2}
	\begin{split}
	\E[\varphi(X^{n-1}, X^{n}) \alpha] &= \espec \left[\varphi(X^{n-1}, X^{n})\bigg(\ff(X^n) \wedge \ff(X^{n-1})\bigg)\right] \\
	&= \espec \left[\varphi(X^{n}, X^{n-1})\bigg(\ff(X^n) \wedge \ff(X^{n-1})\bigg)\right]
	\end{split}
	\end{equation}
	where the last equality holds thanks to the symmetry of the measure 
	\[\pi_{n-1}(\ddd x^{n-1}) \pi_{n-1}(\ddd x^{n}) \bigg(\ff(x^n) \wedge \ff(x^{n-1})\bigg)\]
	with respect to $x^{n-1}$ and $x^{n}$. Combining ~\Cref{proof:pcore_dc}, ~\Cref{proof:pcore_t1}, and ~\Cref{proof:pcore_t2}, we have
	\[ \E[\varphi(\xonenew, \xtwonew)] = \E[\varphi(X^{n-1}, X^{n})] \]
	which concludes the proof.
\end{proof}

\subsection{Proof of \Cref{prop:validity_swap_gen}}\label{proof:validity_swap_gen}
\begin{proof}
The main idea is to express the \textit{generalised} PT swap kernel as the \textit{classic} PT swap kernel of another appropriately-defined PT problem.

    Given $(X^{n-1}, X^n) \sim \pi^{n-1} \otimes \pi^n$, the first step of the generalised swap kernel simulates $X^{n-1}_* \sim Q_{n-1}(X^n, \ddd x^{{n-1}})$ and $X^n_* \sim P_n(X^{n-1}, \ddd x^n)$. Consider now a \textit{conceptual} parallel tempering problem \textit{with only two} annealing distributions
    \begin{equation}
    \label{eq:artificial_pt_problem}
    \begin{split}
        \mu_1(\ddd x^{n-1}, \ddd x^n) &:= \pi_{n-1}(\ddd x^{n-1}) P_n(x^{n-1}, \ddd x^n)\\
        \mu_2(\ddd x^{n-1}, \ddd x^n) &:= \pi_n(\ddd x^n) Q_{n-1}(x^n, \ddd x^{n-1}).
        \end{split}
    \end{equation}
We have that $(X^{n-1}, X^n_*) \sim \mu_1$ and $(X^{n-1}_*, X^n) \sim \mu_2$. Moreover $(X^{n-1}, X^n_*)$ is independent from $(X^{n-1}_*, X^n)$. As such, $(X^{n-1}, X^n_*; X^{n-1}_*, X^n)$ is a stationary state of the parallel tempering problem given by ~\Cref{eq:artificial_pt_problem}.

We now apply the \textit{classic} swap kernel $\mathbf K_{n-1,n}$ to this particular parallel tempering problem. The kernel simulates $(Y^{n-1}, Y^n_*; Y^{n-1}_*, Y^n)$ such that
\begin{equation}
\label{eq:extended_swap}
(Y^{n-1}, Y^n_*; Y^{n-1}_*, Y^n) = 
    \begin{cases}
    (X^{n-1}_*, X^n; X^{n-1}, X^n_*) &\textrm{ with probability } \alpha_{12} \\
    (X^{n-1}, X^n_*; X^{n-1}_*, X^n)
    &\textrm{ with probability } 1 - \alpha_{12}
    \end{cases}
\end{equation}
where \[
\alpha_{12} := \frac{\ddd \mu_2/\ddd\mu_1(x^{n-1}, x^n_*)}{\ddd \mu_2/\ddd\mu_1(x^{n-1}_*, x^n)}.
\]

 Notice that ~\Cref{eq:extended_swap} can be marginalized on the first and the last components to read
\begin{equation*}
(Y^{n-1}, Y^n) = 
    \begin{cases}
    (X^{n-1}_*, X^n_*) &\textrm{ with probability } \alpha_{12} \\
    (X^{n-1}, X^n)
    &\textrm{ with probability } 1 - \alpha_{12}.
    \end{cases}
\end{equation*}
Additionally, it is readily checked that the definition of $\alpha_{12}$ coincides with~\Cref{eq:swap_proba_gen}.

Therefore, the \textit{generalised} swap applied to the original PT problem coincides with the \textit{classic} swap applied to the artificial PT problem defined by~\Cref{eq:artificial_pt_problem}. \Cref{prop:validity_swap} asserts that the classic swap is invariant, i.e.
\[(Y^{n-1}, Y^n_*; Y^{n-1}_*, Y^n) \sim \mu_1(\ddd y^{n-1}, \ddd y^n_*) \otimes \mu_2(\ddd y^{n-1}_*, \ddd y^n).\]
In particular
\[ (Y^{n-1}, Y^n) \sim  \pi_{n-1}(\ddd y^{n-1}) \otimes \pi_n(\ddd y^n).  \]
As a result, the generalised PT swap keeps $\pi_{n-1} \times \pi_n$, hence $\bpi$, invariant.
\end{proof}

\subsection{Proof of \Cref{prop:deterministic_swap}}\label{proof:det-swap}

\begin{proof}
Let $S:= \{ (z^{n-1}, z^n) \in \mathbb R^d \times  \mathbb R^d \textrm{ such that } F_n(z^{n-1}) = z^n \}$. 
	Recall the definition of~\Cref{eq:two_bar}:
	\[ \bar\pi_{n}^Q(\ddd z^{n-1}, \ddd z^{n}) = \pi_{n}(\ddd z^{n}) Q_{n-1}(z^{n}, \ddd z^{n-1}). \]
	Moreover for $(z^{n-1}, z^n) \in S$,
	\[ \bar\pi_{n-1}^P(\ddd z^{n-1}, \ddd z^{n}) = (F_n \# \pi_{n-1})(\ddd z^{n}) Q_{n-1}(z^{n}, \ddd z^{n-1}). \]
	Therefore
	\begin{multline}
    \label{eq:rd_rd}
 \frac{\ddd \bar \pi_{n}^Q}{\ddd \bar \pi_{n-1}^P}(z^{n-1}, z^{n}) = \frac{\pi_{n}(\ddd z^{n})}{(F_n \# \pi_{n-1})(\ddd z^{n})}\\ = \frac{\tilde\pi_{n}(z_n)}{\tilde\pi_{n-1}(F_n^{-1}(z^{n})) |\det(J_{F_n^{-1}}(z^{n}))|} = \frac{\tilde\pi_{n}(z^n) |\det (J_{F_n}(z^{n-1}))|}{\tilde\pi_{n-1}(z^{n-1})}.
	\end{multline}
	Applying this identity at $(z^{n-1}, z^n) = (x^{n-1}, x^n_*) \in S$ gives
    \begin{equation}
    \label{eq:rd_up}
	\frac{\ddd \bar \pi_{n}^Q}{\ddd \bar \pi_{n-1}^P}(x^{n-1}, x^{n}_*) = 
    \frac{\tilde\pi_{n}(x^n_*) |\det (J_{F_n}(x^{n-1}))|}{\tilde\pi_{n-1}(x^{n-1})}.
    \end{equation}
    Similarly, applying~\Cref{eq:rd_rd} at $(z^{n-1}, z^n) = (x^{n-1}_*, x^n) \in S$ gives
    \begin{equation}
    \label{eq:rd_down}
    \frac{\ddd \bar \pi_{n}^Q}{\ddd \bar \pi_{n-1}^P}(x^{n-1}_*, x^{n}) = 
    \frac{\tilde\pi_{n}(x^n) |\det (J_{F_n}(x^{n-1}_*))|}{\tilde\pi_{n-1}(x^{n-1}_*)}.
    \end{equation}
	Together~\Cref{eq:rd_up} and~\Cref{eq:rd_down} establish the proposition.
\end{proof}

\subsection{Proof of \Cref{prop:diff-swap}}\label{proof:diff-swap}
\begin{proof}
Since the relevant measures and kernels have strictly positive densities with respect to the Lebesgue measure, we have, for any $(z^{n-1}, z^n) \in \mathbb R^d \times \mathbb R^d$,
\begin{equation}
    \label{eq:density_rg}
    \frac{\ddd \bar \pi_n^Q}{\ddd \bar \pi_{n-1}^P}(z^{n-1}, z^n) = \frac{\tilde \pi_{n}(z^n) \tilde Q_{n-1}(z^n, z^{n-1})}{\tilde \pi_{n-1}(z^{n-1}) \tilde P_n(z^{n-1}, z^n)}.
\end{equation}
Applying this identity at $(z^{n-1}, z^n) = (x^{n-1}, x^n_*)$ gives
\begin{equation}
    \label{eq:up_rg}
    \frac{\ddd \bar \pi_n^Q}{\ddd \bar \pi_{n-1}^P}(x^{n-1}, x^n_*) = \frac{\tilde \pi_{n}(x^n_*) \tilde Q_{n-1}(x^n_*, x^{n-1})}{\tilde \pi_{n-1}(x^{n-1}) \tilde P_n(x^{n-1}, x^n_*)}.
\end{equation}
Similarly, applying \Cref{eq:density_rg} at $(z^{n-1}, z^n) = (x^{n-1}_*, x^n)$ gives
\begin{equation}
    \label{eq:down_rg}
    \frac{\ddd \bar \pi_n^Q}{\ddd \bar \pi_{n-1}^P}(x^{n-1}_*, x^n) = \frac{\tilde \pi_{n}(x^n) \tilde Q_{n-1}(x^n, x^{n-1}_*)}{\tilde \pi_{n-1}(x^{n-1}_*) \tilde P_n(x^{n-1}_*, x^n)}.
\end{equation}
Together \Cref{eq:up_rg} and \Cref{eq:down_rg} establish the proposition.
\end{proof}

\section{Round Trip Rate}\label{app:round-trip}

In order to define the round trip rate used within PT, we consider the Markov chain $(\mathbf{X}_t)_{t\in\mathbf{N}}$ constructed by PT, and imagine an auxiliary Markov chain $(\mathbf{I}_t)_{t\in\mathbf{N}}$ initialised at the state $\mathbf{I}_0=(0, 1, \ldots, N)$. The subsequent values of $\mathbf{I}_t$ are then determined by the swap moves carried out by $\K_t^\text{comm}$ on $\mathbf{X}_t$ - i.e. we apply the same swaps to the components of $\mathbf{I}_t$ that are proposed and accepted by $\K_t^\text{comm}$ on the components of $\mathbf{X}_t$. This results in $\mathbf{I}_t$ defining some sequence of permutations of $[N]=\{0, \ldots, N\}$ tracking the underlying communication of states in $\mathbf{X}_t$. Hence, for a realisation of $\mathbf{X}_t$ with $T$ steps, we define the \emph{round trips for index $n\in[N]$} as the number of times which the index $n$ in $\mathbf{I}_t$ completes the journey from $0$-th component, then to the $N$-th component and then back to the $0$-th component - i.e. completes the round trip from the reference to the target and back again. The (overall) \emph{round trips} is then defined as the sum of round trips for index $n\in[N]$ over all index values and the \emph{round trip rate} is defined as the round trips divided by $T$.

\section{Multistep GePT Swap Kernel}\label{app:multi-step}

We can extend the swap kernel presented in \Cref{subsect:diffusion_swaps} to instead utilise a sequence of $m>1$ Markov kernels to generate the proposed states.

\newcommand{\dbp}[2]{{{(#1, #2)}}}
Specifically, given $m$ forward kernels $P_n^j$, $j=1, \ldots, m$; and $m$ backward kernels $Q_{n-1}^k$ for $k=1, \ldots, m$  having strictly positive densities with respect to the Lebesgue measure, the $m$-step GePT kernel $\bar{\K}^{m}_{n-1,n}(\x, d\x')$ proceeds as follows:
\begin{itemize}
    \item Simulate the sequence $(x^{(n-1, j)})_{j=0}^m$ by setting $x^{(n-1, 0)} = x^{n-1}$ and sequentially generate $x^{(n-1, j)} \sim P_n^j(x^{(n-1, j-1)}, \ddd x')$, for $j=1, \ldots, m$;
    \item Simulate the sequence $(x^{(n, k)})_{k=0}^m$ by setting $x^{(n, m)} = x^{n}$ and sequentially generate $x^{(n, k)} \sim Q_{n-1}^{k+1}(x^{(n, k+1)}, \ddd x')$ for $k=0, \ldots, m-1$;
    \item Replace components $n-1$ and $n$ of $\x$ with $x^\dbp{n}{0}$ and $x^\dbp{n-1}{m}$ with probability equal to
\end{itemize}
\newcommand{\tsecondmeasure}[1]{
\pi_n(x^\dbp{#1}{m}) \prod_{k=0}^{m-1} Q_{n-1}^{k+1}(x^\dbp{#1}{k+1}, x^\dbp{#1}{k})
}
\newcommand{\tfirstmeasure}[1]{
\pi_{n-1}(x^\dbp{#1}{0}) \prod_{j=1}^m P_n^j(x^\dbp{#1}{j-1}, x^\dbp{#1}{j})
}
\begin{multline}\label{eq:multi-stoc-kernel}
    \bar{\alpha}(\{x^\dbp{n-1}{0:m}\}; \{x^\dbp{n}{0:m}\}) = 1 \wedge \Bigg[\ 
    \frac{
        \tsecondmeasure{n-1}
    }{
        \tfirstmeasure{n-1}
    }
    \times\\
    \times
    \frac{
        \tfirstmeasure{n}
    }{
        \tsecondmeasure{n}
    }
    \Bigg].
\end{multline}
We note that when $m=1$, this kernel coincides with the swap move presented in \Cref{subsect:diffusion_swaps}.
\begin{proposition}
The multistep GePT swap kernel keeps invariant the distribution $\bpi$.
\end{proposition}
\begin{proof}
We consider the two probability measures defined on $(\mathbb R^d)^{n+1}$:
\begin{align*}
    \bar\pi_{\dbp{n-1}{m}}^P(\ddd z^0, \ldots, \ddd z^m) &= \pi_{n-1}(\ddd z^0) \prod_{j=1}^m P_n^j(z^{j-1}, \ddd z^j) \\
    \bar\pi_{\dbp{n}{m}}^Q(\ddd z^0, \ldots, \ddd z^m) &= \pi_n(\ddd z^m) \prod_{k=0}^{m-1} Q_{n-1}^k(z^{k+1}, \ddd z_k).
\end{align*}
Before the swap step, the two variables $x^\dbp{n-1}{0:m}$ and $x^\dbp{n}{0:m}$ are independent and are distributed according to $\bar\pi^P_\dbp{n-1}{m}$ and $\bar\pi^Q_\dbp{n}{m}$ respectively. This allows us to apply \Cref{prop:validity_swap} to these two measures. In particular, ~\Cref{eq:swap_proba} reduces to
\newcommand{\tdmes}{\ddd \bar\pi^Q_\dbp{n}{m} / \ddd \bar\pi^P_\dbp{n-1}{m}}
\[ 1 \wedge \frac{\tdmes(x^\dbp{n-1}{0:m})}{\tdmes(x^\dbp{n}{0:m})} \]
which, upon elementary algebra, coincides with~\Cref{eq:multi-stoc-kernel} and concludes the proof.
\end{proof}

\section{Further Details on Flow-GePT}\label{app:flow-gept}
Since GePT provides approximate samples from both the target and prior densities at each flow iteration, it enables a range of training objectives. Some choices include maximum likelihood estimation (MLE) (equivalent to forward KL), reverse KL, and symmetric KL, which averages the two. Each has trade-offs: forward KL promotes mode covering, reverse KL is more mode-seeking, and symmetric KL balances both. GePTs parallel structure makes symmetric KL particularly effective by providing access to samples at each intermediate annealing distribution, a feature many other methods lack. For example, sequential Monte Carlo (SMC)-based approaches such as FAB \citep{midgley2022flow} and CRAFT \citep{matthews2023continualrepeatedannealedflow} rely on samples from only one side, limiting their choice of loss functions.

Additionally, we explore loss functions based on GePTs rejection rates, as the round trip rate is analytically linked to them. Since higher round trip rates indicate more efficient mixing, optimizing for this metric improves sampling performance. Empirically, we found that combining reverse KL with an inverse round trip rate loss yielded the most stable and robust results across different settings. This combination was therefore used in our final experiments.

Specifically, the flows $\{F_{n}\}_{n=1}^{N}$, where each $F_n$ aims to transport samples from $\pi_{n-1}$ to $\pi_n$, are trained by optimizing the following loss:

\begin{equation}\label{eq:flow-gept-loss}
    \mathcal{L}(\{F_n\}) = \sum_{n = 1}^{N}D_{\text{KL}}(F_{n}^{-1}\#\pi_n || \pi_{n-1}) +  \sum_{n=1}^N \frac{\text{Rej}_n}{1-\text{Rej}_n},
\end{equation}
where \(D_{\text{KL}}(F_{n}^{-1}\#\pi_n || \pi_{n-1})\) is equal to the reverse KL divergence between the target distribution \(\pi_n\) and the flow-induced distribution \(F_{n}\#\pi_{n-1}\). The second term accounts for the expected swap rejection rates, defined as:

\begin{equation*}
\text{Rej}_n = \mathbb{E} \left[ 1 - \bar \alpha_{n-1,n}(X^{n-1}, X^n; F_n^{-1}(X^{n}), F_n(X^{n-1})) \right]
\end{equation*}

where $X^{n-1} \sim \pi_{n-1}, X^n\sim\pi_n$, and \(\bar \alpha_{n-1,n}\) is the acceptance probability from the deterministic GePT swap mechanism \Cref{eq:deterministic_swap_proba}. We note that the second term in \Cref{eq:flow-gept-loss} favors minimizing $\text{Rej}_n$, the expected probability of rejection between chains. This formulation is motivated by an analytic expression for the round trip rate in terms of the rejection rates under simplifying assumptions \citep{syed2021parallel}. We reiterate that this loss formulation requires access to approximate samples from both $\pi_{n-1}$ and $\pi_n$. This is naturally facilitated by GePT, whereas other methods typically rely on samples from only one side of the annealing path, limiting their flexibility in training.

The loss in \Cref{eq:flow-gept-loss} is minimized if and only if the flows $\{F_n\}$ perfectly transport $\pi_{n-1}$ to $\pi_n$, ensuring zero KL divergence and a rejection rate of zero. However, we emphasize that the correctness of the GePT frameworki.e., the invariance of the stationary distributionis maintained even when the flows are imperfect. This guarantees asymptotically unbiased sampling regardless of flow accuracy, though better flow approximations improve efficiency.

\section{Further Details on Diff-GePT}\label{app:diff-gept}

\paragraph{Diffusion process}

We use the following Variance-Preserving (VP) diffusion process 
\[
    dY_s = -\beta_sY_s \mathrm{d}s + \sqrt{2\beta_s} \mathrm{d}W_s, \quad Y_0\sim \pi,
\]
with the choice of schedule $\beta_s = \frac{1}{2(1-s)}$ to define the path of distributions $(\pi^\text{VP}_s)_{s\in(0, 1]}$ where $Y_s\sim\pi_{1-s}^{\text{VP}}$  Due to the singularity at $s=0$, $\pi_0$ is not defined by the path, but we can define this point to be a standard Gaussian as the path converges to this distribution in the limit as $s$ approaches 0 in order to define the full annealing path $(\pi_s^\text{VP})_{s\in[0, 1]}$.

\paragraph{Swap kernel}

The Markov kernel $Q_n(x^n, \diff x')$ is well-known to have the closed-form expression given by $\gN(\diff x'; \sqrt{1-\alpha_{n-1}}x^n, \alpha_{n-1}\mathrm{I})$ where $\alpha_{n-1} = 1  - \exp( -2\int^{1 - s_{n-1}}_{1 - s_{n}} \beta_s \mathrm{d} s)$ which transports the distribution $\pi_{s_n}^\text{VP}$ to $\pi_{s_{n-1}}^\text{VP}$. The Markov kernel $P_{n}(x^{n-1}, \diff x')$ can be given by any discretisation of the time-reversal SDE approximating the dynamics transporting $\pi_{s_{n-1}}^\text{VP}$ to $\pi_{s_{n}}^\text{VP}$. In particular, we use the exponential integrator for this approximation given by the kernel $P_{n}(x^{n-1}, \diff x') = \gN(\diff x'; \mu_n(x^{n-1}), \alpha_{n-1}\mathrm{I})$ where 
\[
    \mu_n(x) = \sqrt{1-\alpha_{n-1}}x + 2(1-\sqrt{1-\alpha_{n-1}})(x + \nabla\log \pi_{s_{n-1}}^\text{VP}(x)).
\]
For the case of the $m$-step GePT swap kernel, we simply divide the interval $[s_{n-1}, s_n]$ into a finer discretisation $l_0=s_{n-1}<l_1\ldots<l_m=s_n$ and define $P^j_n, Q_{n-1}^k$ as taking the same form as above but defined on the new intervals $[l_{j-1}, l_j]$ moving samples from $\pi_{s_{n-1}}^\text{VP}$ to $\pi_{s_n}^\text{VP}$ through the intermediate $\pi_{l_j}^\text{VP}$ distributions step by step (according to the direction encoded by these kernels - i.e. following the VP diffusion SDE or the time-reversal SDE). In practice, we simply take $\{l_j\}$ to be a uniform discretisation of $[s_{n-1}, s_n]$.

\paragraph{Network parametrisation}

We parametrise an energy-based model as outlined in \cite{phillips2024particle} but modified to ensure that $\pi^\theta_0(x) \propto N(x; 0, \mathrm{I})$. For completeness, we specifically take $\log \pi^\theta_s(x) = \log g^\theta_s(x) - \frac{1}{2}||x||^2$ where 
\begin{align*}
     \log g_\theta(x, s) &= [r_\theta(1) - r_\theta(s)][r_\theta(s) - r_\theta(0)] \langle N_\theta(x, s), x \rangle \\ +& [1 + r_\theta(1) - r_\theta(s)]\log g_0(\sqrt{s}x),
\end{align*}
where $g_1(x)\propto \pi(x)N(x; 0, \mathrm{I})$. Here, $r$ is a scalar-valued neural network and $N$ is a vector-valued function in $\R^d$. We also note that $\pi^\theta_1(x)\propto \pi$, hence $\pi^\theta_s$ serves as a valid annealing path between $N(0, \mathrm{I})$ and $\pi$.

\section{Experimental Details}

\subsection{Target distributions}\label{app:target-dists}

\paragraph{GMM-$d$} 

We take the 40-mode Gaussian mixture model (GMM-2) in 2 dimensions from \cite{midgley2022flow} where to extend this distribution to higher dimensions $d$, we extend the means with zero padding to a vector in $\mathbb{R}^d$ and keep the covariances as the identity matrix but now within $\mathbb{R}^d$, to allow for the visualisation of samples in higher dimensions. Following previous work \citep{akhound2024iterated, phillips2024particle}, we also scale the distribution GMM-$d$ by a factor of 40 to ensure the modes are contained within the range $[-1, 1]$ for the Diff-GePT experiments, where we use the same scaling for the Linear-PT baseline to ensure a fair comparison.

\subsection{Algorithmic Details for Flow-GePT Experiments}\label{app:algo-details-flow-gept}

\paragraph{Annealing path}

We use the linear annealing path, $\pi_n \propto \pi_0^{1-\frac{n}{N}}\pi^{\frac{n}{N}}$, with $N=5$ (for a total of 6 chains) and a uniform discretisation. We do not fine-tune the annealing schedule here as our goal is to demonstrate that Flow-GePT can enhance performance even with a suboptimal choice.

\paragraph{Local communication kernels}

Flow-GePT and Linear-PT each use a single step of the Metropolis-adjusted Langevin algorithm (MALA) \citep{mala} for the local exploration kernels $K_n$. 

\paragraph{Training}

We use $1000$ independent copies of Flow-GePT (i.e., there are $6000$ total samples at any point in time) with normalising flows parametrised by 20 NVP layers \cite{dinh2017densityestimationusingreal} with $16$ hidden units each and initialised at the identity function.

We then train for $4000$ iterations with all samples initialised from the reference distribution. Each iteration consists of five Flow-GePT steps using the current flows, after which the samples from the fifth step are used to update the flows via the loss in \Cref{eq:flow-gept-loss}.

We train using a single batch using all samples and optimize with Adam \citep{kingma2014adam} using a learning rate of $10^{-3}$ and global norm clipping at $1.0$.

\subsection{Algorithmic Details for Diff-GePT Experiments}\label{app:algo-details-diff-gept}

\paragraph{Compute-normalised round trip rate}

In a fully parallelised implementation of PT and $m$-step Diff-GePT, a hardware-agnostic measure of the effective computational cost required to generate a single sample from the target distribution $\pi$ is the number of function evaluations of the annealing path $\pi_n$ (or its score) that a single machine needs to implement, as this constitutes the sequential computation of the algorithm. For example, each step of PT requires each machine to compute two evaluations of $\pi_n$ and $m$-step Diff-GePT requires $2+m$ evaluations. Hence, we define the \emph{compute-normalised round trip rate} to be the round trips divided by the total number of function evaluations of $\pi_n$ that a single machine implements when considering a fully parallelised implementation. For example, given a run of $m$-step Diff-GePT with $T$ steps which generates $r$ round trips, the compute-normalised round trip rate equals $\frac{r}{(2+m)T}$.

\paragraph{Annealing schedule initialisation}

For Diff-GePT and Diff-PT, we initialise the annealing schedule using the following scheme: 
\begin{equation}\label{app:diff-schedule}
    s_n = 1 - \left(s_\text{max}^{\frac{1}{\rho}} + \frac{n}{N-1}(s_\text{min}^{\frac{1}{\rho}} - s_\text{max}^\frac{1}{\rho})\right)^\rho \quad  \text{ for } n<N, \quad s_N=1
\end{equation}
based off of equation 5 in \cite{karras2022elucidating}, where we take $s_\text{min}=1\times10^{-6}, s_\text{max}=1$ and $\rho=3$. We note that the modification of the scheme from \cite{karras2022elucidating} is due to the reversal of the usual diffusion time convention to align with the indexing convention of the PT literature. For Linear-PT, we initialise the annealing schedule to be a uniform discretisation of $[0, 1]$.

\paragraph{Local communication kernels}

For each method, we define the local exploration kernels $K_n$ as using 5 Hamiltonian Monte Carlo \citep{neal2012mcmc} steps with 2 leapfrog steps and a step size of 0.03 each - i.e. we use the same local exploration kernel for each component of PT/GePT.

\paragraph{Annealing schedule tuning}

The choice of annealing schedule $s_0=0<s_1\ldots<s_N=1$ is empirically very important for the performance of PT and GePT. In order to tune this component at sampling time, for a fair comparison between each method, we use 10 rounds of the tuning algorithm proposed in \cite{syed2021parallel} with 100 burn in steps and 500 samples.

\paragraph{Training}

To train the learned diffusion path, we run 32 independent copies of PT with $(\pi_s^\theta)_{s\in[0, 1]}$ as an annealing path, where we initialise states along the annealing path at the reference distribution and take 200 steps of burn in. Afterwards, we run the outer loop for $M$ steps which takes 16 steps of the PT Markov chain to give 512 samples at the target distribution $\pi$. We then run the inner loop where we optimise the score-matching objective with the above batch of 512 samples for 20 steps, where we sample the times $s$ according to the continuous version of the annealing schedule in \Cref{app:diff-schedule}. Further, we retune the annealing schedule used during sampling every 200 outer steps with the current collection of samples.

For the optimiser, we use Adam with default hyper-parameters and a learning rate of $5\times10^{-4}$, global norm clipping at $2.0$ and EMA with decay parameter of 0.9. We present further training hyper-parameters in \Cref{tab:diff-gept-params}.

\begin{table}[h!]
\centering
\caption{Training hyper-parameters for the learned Diff-GePT models targeting GMM-$d$ for $d=2, 10, 50$.}
\label{tab:diff-gept-params}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{l r r}
\toprule
Model & Number of Chains & $M$ \\
\midrule
GMM-2 & 10 & 3000 \\
GMM-10 & 30 & 6000 \\
GMM-50 & 60 & 10000 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Further Results for Diff-GePT Experiments}

\paragraph{Round trip rate} 

\Cref{tab:round-trip-rate} presents the round trip rate derived from the results presented in \Cref{tab:learned-diff-gept}.

\begin{table}[h!]
\centering
\caption{Corresponding round trip rate of the results presented in \Cref{tab:learned-diff-gept}.}
\label{tab:round-trip-rate}
\resizebox{\textwidth}{!}{
\begin{tabular}{llllllllll}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c}{GMM-2} & \multicolumn{3}{c}{GMM-10} & \multicolumn{3}{c}{GMM-50} \\
\cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10}
 & 10 & 30 & 60 & 10 & 30 & 60 & 10 & 30 & 60 \\
\midrule
1-Diff-GePT & $0.110$ & $0.117$ & $0.117$ & $0.0160$ & $0.0360$ & $0.0395$ & $0.00$ & $0.00956$ & $0.0146$ \\
2-Diff-GePT & $0.133$ & $0.138$ & $0.132$ & $0.0230$ & $0.0504$ & $0.0492$ & $0.0018$ & $0.0170$ & $0.0214$ \\
5-Diff-GePT & $\mathbf{0.166}$ & $\mathbf{0.159}$ & $\mathbf{0.154}$ & $\mathbf{0.0553}$ & $\mathbf{0.0713}$ & $\mathbf{0.0504}$ & $\mathbf{0.0098}$ & $\mathbf{0.0265}$ & $\mathbf{0.0276}$ \\
\midrule
Diff-PT & $0.106$ & $0.121$ & $0.127$ & $0.00678$ & $0.0183$ & $0.0263$ & $0.00$ & $0.00216$ & $0.00406$ \\
\midrule
Linear-PT & $0.117$ & $0.131$ & $0.139$ & $0.00660$ & $0.0189$ & $0.0259$ & $0.00$ & $0.00212$ & $0.00390$ \\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{Normalisation constant estimation}

From the samples generated by $m$-step Diff-GePT, Diff-PT and Linear-PT in \Cref{tab:learned-diff-gept}, we take 5 consecutive chunks of 2000 samples to estimate the log-normalisation constant of the corresponding GMM-$d$ distribution 5 times via the stepping stone estimator. We present the following distributions of log-normalisation constants estimators in \Cref{fig:log-z}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{log_z_10_plot-2.pdf}
    \includegraphics[width=1\linewidth]{log_z_30_plot-2.pdf}
    \includegraphics[width=1\linewidth]{log_z_60_plot-2.pdf}
    \caption{Estimates of the log-normalisation constant of GMM-$d$ for $d=2, 10, 50$ by $m$-step Diff-GePT ($m=1, 2, 5$), Diff-PT and Linear-PT using 2000 samples. Each box consists of 5 estimates. The black dotted line denotes the ground-true log-normalisation constant.}
    \label{fig:log-z}
\end{figure}

\paragraph{W2 distance}

We follow the same setting as above to compute the Wasserstein-2 (W2) distance with respect to 2000 independent samples from the corresponding GMM-$d$ distributions. We present the following distributions of W2 distances in \Cref{fig:w2}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{w2_10_plot-3.pdf}
    \includegraphics[width=1\linewidth]{w2_30_plot-3.pdf}
    \includegraphics[width=1\linewidth]{w2_60_plot-3.pdf}
    \caption{W2 distance between samples from GMM-$d$ for $d=2, 10, 50$ and $m$-step Diff-GePT ($m=1, 2, 5$), Diff-PT and Linear-PT using 2000 samples. Each box consists of 5 estimates.}
    \label{fig:w2}
\end{figure}

\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}