\section{Additional Qualitative Results}
We provide more qualitative results in the attached supplementary material. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Number of Training Samples}
\label{sec:number_of_training_images}
To study the impact of the number of training samples used for training each individual token, we train on varying numbers of samples from the quadruped-head training set of the PartImageNet dataset. 
We then compute the mean intersection-over-union (mIoU) on 50 randomly sampled validation samples.
\Cref{fig:num_samples} shows the mIoU over 5 runs that are optimized for 2000 steps.
The figure shows that 10-20 training samples achieve a similar mIoU.
The mIoU might improve further with more samples, but we observed that this small number of samples is sufficient to attain a good localization of different parts.
This is a clear advantage of our approach compared to training a part segmentation model that requires a large number of samples.
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth, keepaspectratio]{fig/miou.pdf}
    \caption{The impact of the number of training samples on mIoU.}
    \label{fig:num_samples}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mathhrefsection{Choice of $\Omega$ for Our Adaptive Thresholding}{Choice of Omega for Our Adaptive Thresholding}
\label{sec:ablation_omega_adaptive_thresholding}
To understand how the choice of $\Omega$ in Equation 5 affects the editing, we provide an example for an edit with different values of $\Omega$ in \Cref{fig:omega}.
Lower values of $\omega$ make the editing regions dominated by the editing prompt and put less emphasis on blending with the main object.
This is demonstrated by the Joker's head, which does not follow the original head pose.
By increasing the value of $\Omega$, the blending starts to improve, and the edit becomes more harmonious with the original object.
At $\Omega=3k/2$, the best trade-off between performing the edit and blending with the original object is achieved, and we find it to be optimal for most edits.
Increasing $\Omega$ further can make the original object dominate over the edit, \eg the hair of the man remains unedited. 

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{fig/omega_joker.pdf}
    \caption{Applying the edit "with a Joker <head>" with different choices of hyperparameter $\omega$.}
    \label{fig:omega}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Choice of Token Padding Strategies}
\label{sec:padding_strategy}
During token optimization, we train a custom embedding $\hat{E} \in \mathrm{R}^{2 \times 2048}$.
However, during inference, the dimensionality of this embedding needs to match the standard SDXL embedding $E \in \mathrm{R}^{77 \times 2048}$.
Therefore, our embedding needs to be padded with some tokens to match that of SDXL.
Possible choices are: padding with [2-77] tokens from $E$ (context), zero padding, <BG> token, <SoT> from $E$, or <EoT> token from $E$.
We display the effect of these different strategies on the average extracted attention map in \Cref{fig:padding}
The figure shows that padding with the <BG> or <SoT> from $E$ attains the cleanest attention maps, leading to the best edits in terms of blending with the main object.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{fig/padding_stat.pdf}
    \caption{Influence of different token padding strategies during inference on the cross-attention maps.}
    \label{fig:padding}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Choice of UNet Layers for Token Optimization}
\label{sec:choice_unet_layers}
To decide which UNet layers to include in token optimization ($L$ in Equation 3), we compute the mIoU for each layer based on their cross-attention maps.
The results are shown in \Cref{fig:layer}.
We can observe that the first 8 layers of the Decoder with indices [24,32] achieve the best mIoU, indicating that they are semantically rich.
Those 8 layers can be used to optimize the tokens rather than all layers in case of limited computational resources.


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth, keepaspectratio]{fig/layer_influence.pdf}
    \caption{Analysis of how each layer of the UNet performs in terms of mioU. The first eight collected layers of the decoder achieve the best results. Note that we apply OTSU for binarization.}
    \label{fig:layer}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Different Edits Per Image}
\label{sec:diff_edit_per_image}
To show that our method performs consistently well with different edits, we provide some qualitative examples.
In \Cref{fig:head_identity}, we apply multiple identity edits to the input image.
This figure showcases the powerful versatility of our approach.
Two noteworthy edits are ``young'' and ``old'', where the identity of the man is preserved and aged according to the edit.
Moreover, our approach successfully changes the identity of multiple celebrities of different ethnicities seamlessly at an exceptional quality.

We provide other examples in \Cref{fig:multi_real} for real images, where different edits are applied.
The figure shows that our approach performs consistently well and can apply edits of a different nature to the same image.

\begin{figure*}
    \centering
    \includegraphics[width=0.68\textwidth, keepaspectratio]{fig/identity.pdf}
    \caption{Applying different identity edits to the same image. We showcase the versatility of our method, as there is no change in the underlying model; we can leverage the full capabilities of the model without any retraining or fine-tuning.}
    \label{fig:head_identity}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Failure Cases}
\label{sec:failure_cases}

\Cref{fig:fail} shows two examples of failure cases.
The first example shows that the edits performed by our approach are restricted to the style of the original image.
More specifically, it can not change the style from ``real'' to a ``drawing''.
This limitation stems from the internal design of SDXL that encodes the style in self-attention and prevents mixing two different styles. 
The second example shows that our approach can not perform unreasonable edits, such as replacing a cat's head with a human head.
This limitation arises from the incapability of SDXL to generate these concepts, but we see a promising direction of disentanglement of existing concepts. 
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/failure_cases.pdf}
    \caption{Failure cases. Our approach can not perform an edit in a different style (left) or unreasonable edits (right).}
    \label{fig:fail}
\end{figure*}
% \end{document}




\section{User study details}
\label{sec:user_study}
We conducted two user studies against P2P and iP2P independently using the 2AFC technique with a random order of methods. 
We used Amazon Web Turk, with the minimal rank of "masters," and received 360 responses per study. 
The users were provided with the following instructions:
\begin{blockquote}
    You are given an original image on the left, edited using the A and B methods. Please select the method that changes ONLY the part specified by PART and keeps the rest of the image unchanged.
    For example, if the original image has a 'Cow", PART is “Head”, and EDIT is “Dragon”, choose the method that changes the cow head to dragon head, and keeps the rest of the cow’s body as it is.
\end{blockquote}
A screenshot of the user study layout is shown in \Cref{fig:user_study}.


\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/user_study.pdf}
    \caption{Visualization of the user study layout that we conducted.}
    \label{fig:user_study}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmark and Generating Prompts and Edits for Evaluation}
\label{sec:benchmark_and_prompts}
We provide an example of how we generate prompts and edits for evaluation of <animal-head> in \Cref{fig:rand}.
We follow the same strategy for all other parts.
The PartEdit benchmark consists of two parts: synthetic and real. The Synthetic comprises 60 images across animals (quadrupeds), cars, chairs, and humans (bipeds). The synthetic part of the benchmark consists of the same subjects in similar proportion, more precisely, five quadrupeds, four bipeds, two chairs, and two cars.
Those benchmark images and training images do not overlap with the images or masks during training, sometimes even the domain. Therefore, we use the previously mentioned custom 10 annotated images for
\texttt{<humanoid-torso>}, \texttt{<car-hood>}, and \texttt{<chair-seat>}. Mainly because of the lack of such masks or objects in datasets.   
\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth, trim={1cm 3.5cm 1cm 2cm}, clip]{fig/example.pdf}
    \caption{Example of configuration for random generation of the edits.}
    \label{fig:rand}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact of choice of Images for training part tokens}
\label{sec:impact_selection_training_images}

We further validate the impact of the choice of images during training. We perform a cross-validation experiment using SD2.1 with the <Quadruped head> (PartImageNet). Specifically, we utilize 100 images in a 5-fold cross-validation setup, achieving a mean IoU of $71.704$ with a standard deviation of $4.372$. This highlights the stability and generalization of the model's semantic part segmentation with optimized tokens across different training subsets. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hyperparameters}
% \rev{}
We provide hyperparameters used for training the tokens and hyper parameters used during editing. 

For the training process, we deploy BCE loss of initial learning rate of $30.0$, with a StepLR scheduler of $80$ steps size and gamma of $0.7$. For the diffusion, we use a strength of 0.25 and a guidance scale of $7.5$. During aggregation for loss computation, masks are resized to $512$ for SD2.1 and $1024$ for SDXL model variants. We use 1000 or 2000 epochs during training. 

During inference, as discussed in \Cref{fig:omega}, we use $\omega$ of $1.5$. For 
the guidance scale, larger values tend to increase adherence to $\alpha CLIP$ but at the cost of PSNR and SSIM. We investigated values between 3.5 and 20 for the guidance scale and used 12.5 as a balance between the two for Ours + edits real setting. For the synthetic setting, a guidance scale of 7.5 was used. During inference, we start editing at the first step, as we utilize prior time step mask information.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional qualitative comparisons}
\label{sec:additional_qualitative_comparison}

We provide additional qualitative comparisons with InfEdit and PnPInversion for the same images as in the main paper (in addition to quantitative results in the main table), and we can observe that our approach outperforms them. Nonetheless, InfEdit does showcase potential with the Alien head example \Cref{fig:abl_qual_2}.
For both methods, we use the default parameter values provided in their demo/code, with default blending words between the object and their edited instruction. (E.g., source prompt is "A statue of an angel with wings on the ground," target prompt is "A statue with the uniform torso of an angel with wings on the ground," and blend between "statue" and "statue with uniform torso").


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/supp_qual_3.pdf}
    \caption{InfEdit \citep{Xu_2024_CVPR_INFEDIT} and PnPInversion \citep{ju2023direct_PnPInversion} on real image setting.}
    \label{fig:abl_qual_2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/gradio.jpg}
    \caption{An illustration of our user interface.}
    \label{fig:gradio}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Editing multiple regions at once}
\label{sec:multiple_edits_region}

Our approach can be easily adapted to edit multiple parts simultaneously at inference time without retraining the tokens.
To achieve this, the part tokens are loaded and fed through the network to produce cross-attention maps at different layers of the UNet.
We accumulate these maps across layers and timesteps as described in Section 3.3, but the main difference is that we normalize the attention maps for different parts jointly at each layer.
We provide several examples in \Cref{fig:double_part_edit} and visualizations of the combined attention maps.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional attention map localization visualized}
\label{sec:map_localization}

In the \Cref{fig:abl_maps}, we provide additional visualizations of images, their annotated ground truth, raw normalized token attention maps, binarized attention maps using a $0.5$ threshold, binarized attention maps using the Otsu threshold, and our approach using Otsu threshold ($\omega=1.5$). One can observe that our approach can be thought of as a relaxation of binary thresholding but a stricter version of Otsu thresholding.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/abl_attention_maps.pdf}
    \caption{Additional visualization of obtained attention maps across all time steps of the qualitative results under the real setting.}
    \label{fig:abl_maps}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Use of existing segmentation models like SAM}

Segment Anything \citet{kirillov2023segment}, as one of the foundational models in segmentation using conditioned inputs such as points, still struggles to segment parts that do not have a harsh border (commonly torso and head) while it has no problems with classes such as car hood. We can observe such failure cases in \Cref{fig:sam_examples}. 

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth, keepaspectratio]{fig/sam_examples.pdf}
    \caption{ Visualization of masks obtained from Segment Anything (huge) model across the 3 heads for the green provided point. The target indicates what we wanted to segment by the green point.}
    \label{fig:sam_examples}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{User Interface}
We provide an illustration of our user interface in \Cref{fig:gradio}.
The user specifies the editing prompt in the form ``\texttt{with <edit> <part-name>}'', and the corresponding token is loaded to apply the desired edit.
The user also has the option to tune the $t_e$ parameter (Editing Steps) to control the locality of the edit.
We also visualize the aggregated editing mask to help the user understand the results.




