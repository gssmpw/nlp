\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{fig/qual_1.pdf}
    \caption{A qualitative comparison on synthetic images from the PartEdit benchmark. Our method outperforms both iP2P \citep{brooks2023instructpix2pix} and P2P \citep{hertz2022prompt} on the synthetic setting. Showcasing good localization while integrating seamlessly into the scene, illustrated by the third row with a formal torso edit. }
    \label{fig:qual_1}
\end{figure*}





\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{fig/qual_2.pdf}
    \caption{A qualitative comparison against EF-DDPM \citep{huberman2024edit} and Ledits++ \citep{brack2024ledits} on real image editing.}
    \label{fig:qual_2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{fig/double_part_edit_wide.pdf}
    \caption{ Visualization of editing 2 parts at the same time. PartEdit is able to edit two regions at the same time with minimal modifications. Note that the attention maps showcase average cross-attention across all time steps and layers, while we blend at each timestep, at each layer.}
    \label{fig:double_part_edit}
\end{figure*}





\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth, keepaspectratio]{fig/qual_3_masked_new.pdf}
    \caption{A comparison on synthetic benchmark against Latent Blending \citep{avrahami2023blendedlatent} and SDXL Inpainting \citep{podell2024sdxl} using ground truth masks (\cmark) against our predicted masks (\xmark). We observe PartEdit outperforming Latent Blending, which fails totally in some of the edits (spiderman, hair, and chair), while others produce unintegrated edits such as the bear head. }
    \label{fig:masked_editing_gt}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.85\textwidth, keepaspectratio]{fig/real_multi_edit.pdf}
    \caption{Different edits per image on real image editing using our method. We showcase the versatility of our method, as there is no change in the underlying model; we can leverage the full capabilities of the model without any retraining or fine-tuning. More in \Cref{sec:diff_edit_per_image}.}
    \label{fig:multi_real}
\end{figure*}