\section{Text-to-Image Diffusion Models}
\label{sec:prel}
% \subsection{Diffusion Models}
Diffusion models are probabilistic generative models that attempt to learn an approximation of a data distribution $p(\x)$.
This is achieved by progressively adding noise to each data sample ${\x_0 \sim p(\x)}$ throughout $T$ timesteps until it converges to an isotropic Gaussian distribution as $T \rightarrow \infty$.
During inference, a sampler such as DDIM \citep{ddim} is used to reverse this process starting from Gaussian noise $\x_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ that is iteratively denoised until we obtain a noise-free sample $\hat{\x}_0^0$.
The reverse process for timestep $t \in [T, 0]$ is computed as:
\begin{equation}\label{eq:ddim}
    \begin{aligned}
        x_{t-1} = &\sqrt{\alphatm} \ \hat{x}_0^t + \sqrt{1 - \alphatm - \sigma^2_t} \ \epsilon^t_\theta(x_t) + \sigma_t \epsilon_t \enspace, \\
        & \hat{x}_0^t = \frac{x_t-\sqrt{1-\alpha_t} \ \epsilon_\theta^t(x_t)}{\sqrt{\alpha_t}} \ .
    \end{aligned}
\end{equation}
where $\alpha_t, \sigma_t$ are scheduling parameters, $\epsilon_\theta^t$ is a noise prediction from the UNet, and $\epsilon_t$ is random Gaussian noise.
This is referred to as unconditional sampling, where the model would generate an arbitrary image for every random initial noise $x_T$.

To generate an image that adheres to a user-provided input, \eg, a textual prompt $\mathcal{P}$, the model can be trained conditionally.
In this setting, the UNet is conditioned on the prompt $\mathcal{P}$, and the noise prediction in \Cref{eq:ddim} is computed as $\epsilon^t_\theta(x_t, \mathcal{P})$. 
More specifically, the textual prompt is embedded through a textual encoder, \eg, CLIP \citep{clip}, to obtain an embedding $E \in \mathrm{R}^{77 \times 2048}$ (for SDXL).
This textual embedding interacts with the UNet image features $F$ within cross-attention modules at UNet block $i$ to compute attention as:
\begin{equation}\label{eq:cn}
    \begin{aligned}
        A_i = \text{{Softmax}} & \left(\dfrac{Q_i \ K_i^\top}{\sqrt{{d_{k_i}}}}\right) \enspace ,\\    
        Q_i = F_i \ W_{Q_i},  \qquad K_i = &  E \ W_{K_i}, \qquad V_i =   E \ W_{V_i}.        
    \end{aligned}
\end{equation}
where $F_i$ are UNet features at layer $i$, and $W$ are trainable projection matrices.
The output features are eventually recomputed as $\hat{F}_i = A_i \ V_i$.
This interaction between the text embedding $E$ and the image features $F$ allows cross-attention modules to capture how each word/token in the text prompt spatially contributes to the generated image as illustrated in \Cref{fig:cattn}.
Similarly, each UNet block has a self-attention module that computes cross-feature similarities encoding the style of the generated image where: $$ Q_i = F_i \ W_{Q_i}, \qquad K_i = F_i \ W_{K_i}, \qquad V_i = F_i \ W_{V_i}.$$

