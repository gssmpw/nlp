


\section{Concluding Remarks}
\label{sec:conclusion}
Our approach keeps the underlying model frozen, we can leverage existing knowledge for different edits as seen on \Cref{fig:multi_real}.


\myparagraph{Limitations}
This frozen state also limits our generation's ability for totally unrealistic edits. 

For instance, editing a human head to become a car wheel. 
Moreover, it can not change the style of the edited part to a different style other than the style of the original image.
This limitation stems from the internal design of SDXL that encodes the style in self-attention and prevents mixing two different styles. 
We provide examples of these scenarios in the \Cref{sec:failure_cases}. %Appendix.

\myparagraph{Ethical Impact}
Our approach can help alleviate the internal racial bias of some edits, such as correlating ``Afro'' hair with Africans, as we demonstrated in \Cref{fig:qual_1}.
On the other hand, our approach can produce images that might be deemed inappropriate for some users by mixing parts of different animals or humans. 
Moreover, the remarkable seamless edits performed by our approach can potentially be used for generating fake images, misinformation, or changing the identity.

% \myparagraph{Future Work}
% Our approach can be extended to perform 3D edits similar to Instruct-Nerf2Nerf \cite{instructnerf2023}.
% An LLM can also be incorporated to parse complicated editing prompts to the correct part token.
% For example, mapping ``A man with a muscular upper body'' to the token \texttt{``<torso>''}.


\section{Conclusion}
% \todo{We introduce token optimization and training recipe for part-editing}
We introduced the first text-based editing approach for object parts based on a pre-trained diffusion model.
Our approach can perform appealing edits that possess high quality and are seamlessly integrated with the parent object.
Moreover, it can create concepts that the standard diffusion models and editing approaches are incapable of generating without retraining the base models.
This helps to unleash the creativity of creators, and we hope that our approach will establish a new line of research for fine-grained editing approaches.


