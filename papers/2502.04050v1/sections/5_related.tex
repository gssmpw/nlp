
\section{Related Work}
 \label{sec:related}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Diffusion-based Image Editing}

In general, the image semantics are encoded within the cross-attention layers, which specify where each word in the text prompt is located in the image \citep{hertz2022prompt,tang2023daam}.
In addition, the style and the appearance of the image are encoded through the self-attention layers \citep{pnp}.
Diffusion-based editing approaches exploit these facts to perform different types of semantic or stylistic edits.
On the semantic level, several approaches attempt to change the contents of an image according to a \emph{user-provided prompt} \citep{hertz2022prompt,brooks2023instructpix2pix,lin2023text,kawar2023imagic,parmar2023zero}.
These changes include swapping an object or altering its surroundings by manipulating the cross-attention maps either through token replacement or attention-map tuning.
For stylistic edits, several approaches \citep{masactrl,pnp,parmar2023zero,hertz2023style} modify the self-attention maps to apply a specific style to the image while preserving the semantics.
This style is either provided by the user in the form of text or a reference image.
It is worth mentioning that an orthogonal research direction investigates image inversion to enable real image editing \citep{brack2024ledits,huberman2024edit,brooks2023instructpix2pix}.
For a comprehensive review of editing techniques and applications, we refer readers to \citep{huang2024diffusion}.
Existing text-based editing approaches struggle to apply semantic or stylistic edits to fine-grained object parts, as demonstrated in \Cref{sec:exp}.
Our approach, PartEdit, is the first step toward enabling fine-grained editing, which will enhance user controllability and experience, and can potentially be integrated into existing editing pipelines.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Token Optimization}
To expand the capabilities of pre-trained diffusion models or to address some of their limitations, they either need to be re-trained or finetuned.
However, these approaches are computationally expensive due to the large scale of these models and the training datasets.
An attractive alternative is token optimization, where the pre-trained model is kept frozen, and special textual tokens are optimized instead.
Those tokens are then used alongside the input prompt to the model to perform a specific task through explicit supervision of cross-attention maps.
This has been proven successful in several tasks. \citep{valevski2023unitune,gal2022textual,avrahami2023break,safaee2024clic} employed token optimization and LoRA finetuning to extract or learn new concepts that are used for image generation.
\citep{hedlin2023keypoints} learned tokens to detect the most prominent points in images. 
\citep{marcos2024ovam,khani2024slime} optimized tokens to perform semantic segmentation.
We explored the use of token optimization to learn tokens for object parts customized with a focus on image editing.
For this purpose, we adopt SDXL \citep{podell2024sdxl} as a base model to obtain the best visual editing quality in contrast to existing approaches that leverage SD 1.5 or 2.1.
Our token optimization training is also tailored to obtaining smooth cross-attention maps across all timesteps, unlike 
\citep{marcos2024ovam,khani2024slime} that aggregate all attention maps and apply post-processing to obtain binary segmentation masks.