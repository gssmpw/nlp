

\section{Introduction}
\label{sec:intro}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/cross_attn_mini.pdf}
    % \includegraphics{fig/cross_attn_mini.pdf}
    \caption{A visualization for the cross-attention maps of SDXL \citep{podell2024sdxl} that corresponds to different words of the textual prompt. Object parts such as ``head'' and ``hood'' are not well-localized, indicating that the model lacks a sufficient understanding of these parts. 
    %The visualizations were generated using the DAAM framework \cite{tang2023daam}.
    }
    \label{fig:cattn}
\end{figure*}


Diffusion models \citep{sd,dalle2,imagen,podell2024sdxl,esser2024scaling} have significantly advanced image generation, achieving unprecedented levels of quality and fidelity.
This progress is generally attributed to their large-scale training on the LAION-5B dataset \citep{laion5b} with image-text pairs, leading to a profound understanding of images and their semantics. 
Recent image editing methods \citep{hertz2022prompt,brooks2023instructpix2pix,brack2024ledits,parmar2023zero,pnp,kawar2023imagic,huang2024diffusion} have capitalized on this understanding to perform a wide range of edits to enhance the creative capabilities of artists and designers.
These methods allow users to specify desired edits through text prompts, enabling both semantic edits, such as modifying objects or their surroundings, and artistic adjustments, like changing style and texture. 
Ideally, these edits must align with the requested textual prompts while being seamlessly integrated and accurately localized in the image. 

Despite the remarkable advancement in these diffusion-based image editing methods, their effectiveness is limited by the extent to which diffusion models understand images.
For instance, while a diffusion model can manipulate objects, it might fail to perform edits on fine-grained object parts.
\Cref{fig:teaser} shows examples where existing editing methods fail to perform fine-grained edits.
For instance, in the first row, they exhibit \emph{poor localization} of the editing region and fail to edit \emph{only} the torso of the robot.
In the second row, none of the approaches were able to localize the ``hood'' or apply the edit.
In the final row, existing approaches suffer from \emph{entangled parts}, making the man's face younger when instructed to change his hair to "blonde."
These limitations can be attributed to the coarse textual descriptions in the LAIOB-5B dataset that is used to train diffusion models. 
Specifically, the model fails to understand various object parts as they are not explicitly described in image descriptions.
Moreover, data biases in the datasets are also captured by the model, \eg, associating blond hair with youth.

To validate this hypothesis, we visualized the cross-attention maps of a pre-trained diffusion model for two textual prompts with specific object parts in \Cref{fig:cattn}.
Cross-attention computes attention between image features and textual tokens, reflecting where each word in the textual prompt is represented in the generated image.
For the first prompt, \emph{``A muscular man in a white shirt with a robotic head''}, the generated image resembles a man with robotic arms instead of a robotic head.
The cross-attention maps show that the token ``head'' is activated at the arms rather than the head. 
A possible explanation for this behavior is that the ``head'' has been entangled with the ``arms'' during training due to the coarse textual annotations of the training images.
For the second prompt, \emph{``A red Mustang car with a black hood parked by the ocean''}, the generated car is entirely red, including the hood.
The cross-attention map for the token ''hood'' indicates that the model is uncertain about its location.
This can be attributed to the lack or scarcity of images with ``hood'' annotations in the LAION-5B dataset used for training.


In this paper, we address the limitations of pre-trained diffusion models in their understanding of object parts. 
By expanding their semantic knowledge, we enable fine-grained image edits, providing creators with greater control over their images.
We achieve this by training part-specific tokens that specialize in localizing the editing region at each denoising step.
Based on this localization, we develop feature blending and adaptive thresholding strategies that ensure seamless and high-quality editing while preserving the unedited areas. Our novel feature blending happens at each layer, at each timestep using nonbinary masks.
To learn the part tokens, we design a token optimization process \citep{zhou2022learning}  tailored for fine-grained editing, utilizing existing object part datasets \citep{pascalpart,he2022partimagenet} or user-provided datasets.
This optimization process allows us to keep the pre-trained diffusion model frozen, thereby expanding its semantic understanding without compromising generation quality or existing knowledge.
To evaluate our approach and to facilitate the development of future fine-grained editing approaches, we introduce a benchmark and an evaluation protocol for part editing.
Experiments show that our approach outperforms all methods in comparison on all metrics and is preferred by users $77-90\%$ of the time in conducted user studies.
