
\section{Experiments}
\label{sec:exp}


In this section, we provide an evaluation of our proposed approach, a comparison against existing text-based editing approaches, and an ablation study.
To facilitate these aspects, we create a manually annotated benchmark of several object parts.
For a comprehensive evaluation, we compare against both \emph{synthetic} and \emph{real} image editing approaches.

\subsection{Evaluation Setup}
\renewcommand{\thefootnote}{\arabic{footnote}}

\myparagraph{Synthetic Image Editing}
We base our method on a pre-trained SDXL \citep{podell2024sdxl} 
\footnote{Our code, benchmarks, and annotated dataset will be made public.
}.
For sampling, we employ the DDIM sampler \citep{ddim} with $T=50$ denoising steps and default scheduling parameters as \citep{hertz2022prompt}.
For token optimization, we train on 10-20 images per token for 2000 optimization steps.
We set $t_e=T=50$ for complete preservation of the unedited regions.
We compare against two popular text-based editing approaches: Prompt-to-Prompt (P2P) \citep{hertz2022prompt} and Instruct-Pix2Pix (iP2P) \citep{brooks2023instructpix2pix}.
We use the SDXL implementation of P2P\footnote{\url{https://github.com/RoyiRa/prompt-to-prompt-with-sdxl}}, and the Diffusers implementation of iP2P\footnote{\url{https://huggingface.co/docs/diffusers/en/api/pipelines/pix2pix}}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\myparagraph{Real Image Editing}
We use Ledits++ \citep{brack2024ledits} inversion method to invert the images where the source prompt $\mathcal{P}^s$ is produced by BLIP2 \citep{li2023blip} as described in \Cref{sec:real}.
We compare against Ledits++ and EF-DDPM \citep{huberman2024edit} with the same base model of SD2.1 \citep{sd}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\myparagraph{PartEdit Benchmark}
We create a synthetic and real benchmark of 7 object parts: \texttt{<humanoid-head>}, \texttt{<humanoid-torso>}, \texttt{<human-hair>} \texttt{<animal-head>}, \texttt{<car-body>}, \texttt{<car-hood>}, and \texttt{<chair-seat>}. 
For the synthetic part, we generate random source prompts of the objects of interest at random locations and select random edits from pre-defined lists.
We generate a total of 60 synthetic images and manually annotate the part of interest.
For the real part, we collect 13 images from the internet and manually annotate and assign editing prompts to them.
We denote the synthetic and real benchmarks as \emph{PartEdit-Synth} and \emph{PartEdit-Real}, respectively.
More details are provided in the \Cref{sec:benchmark_and_prompts}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myparagraph{Evaluation Metrics}
To evaluate the edits, we want to verify that: (1) The edit has been applied at the correct location. (2) The unedited regions and the background have been preserved.
Therefore, We evaluate the following metrics for the foreground (the edit) and the background:
\begin{enumerate}
\item \aclip{FG}: the CLIP similarity between the editing prompt and the edited image region.
\item \aclip{BG}:  the CLIP similarity between the unedited region of the source and the edited image. We also compute \emph{PSNR} and structural similarity \emph{(SSIM)} for the same region.
\end{enumerate}

where \aclip{} is the masked CLIP from \citep{sun2023alphaclip}. There is no overlap between the training images used in token optimization and evaluation (more info in \Cref{sec:benchmark_and_prompts}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Qualitative Results}
% Moved to image_only_figures.tex
% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{fig/qual_1.pdf}
%     \caption{A qualitative comparison on synthetic images from the PartEdit dataset.}
%     \description{}
%     \label{fig:qual_1}
% \end{figure*}

\myparagraph{Synthetic Image Editing}
\Cref{fig:qual_1} shows a qualitative comparison.
Our approach excels at performing challenging edits seamlessly while perfectly preserving the background.
P2P fails in most cases to perform the edit as it cannot localize the editing part.
iP2P completely ignores the specified part and applies the edit to the whole object in some cases and produces distorted images in other cases.
Remarkably, our approach succeeds in eliminating racial bias in SDXL by decoupling the hair region from the skin color, as shown in the top-right example.

\myparagraph{Real Image Editing}
\Cref{fig:qual_2} shows a qualitative comparison of real image editing.
PartEdit produces the most seamless and localized edits, whereas other approaches fail to localize the correct editing region accurately.
% MOVED TO image_only_figures.tex
% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{fig/qual_2.pdf}
%     \caption{A qualitative comparison against EF-DDPM \citep{huberman2024edit} and Ledits++ \citep{brack2024ledits} on real image editing.}
%     \label{fig:qual_2}
% \end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Quantitative Results}



\myparagraph{Synthetic Image Editing}
\Cref{tab:quant} top summarizes the quantitative metrics on \emph{PartEdit-Synth} benchmark.
Our approach performs the best on \aclip{avg} and all metrics for the unedited regions.
iP2P scores the best in terms \aclip{FG} as it tends to change the whole image to match the editing prompt, ignoring the structure and the style of the original image (see \Cref{fig:qual_1}).
We also performed two user studies comparing our approach against P2P and iP2P on Amazon Mechanical Turk (more details in \Cref{sec:user_study}).
Our approach is preferred by the users 88.6 \% and 77.0\% of the time compared to P2P and iP2P, respectively.
The reported results for the user studies are based on 360 responses per study.

\myparagraph{Mask-Based Editing}
To demonstrate the effectiveness of our text-based editing approach, we compare it against two mask-based editing approaches, SDXL inpainting and SDXL Latent Blending \citep{avrahami2023blendedlatent}, where the user provides the editing mask.
We use the groundtruth annotations to perform the edits for these two approaches, while our approach relies on the optimized tokens.
\Cref{tab:quant} shows that our approach is preferred by 75\% and 66\% of users over the mask-based approaches.
This reveals that our editing approach produces visually more appealing edits even compared to the mask-based approaches, where the mask is provided. Our method outperforms and produces more realistic edits compared to both methods, as observed on \Cref{fig:masked_editing_gt}.

\myparagraph{Real Image Editing}
\Cref{tab:quant} shows that PartEdit outperforms Ledits++ and EF-DDPM on all metrics and is significantly favored by users in the user study.
This demonstrates the efficacy of our approach in performing fine-grained edits.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{sections/4_table}


\subsection{Extension to multiple parts}
% \myparagraph{Multiple part edits} 
The work focuses mainly on editing a single part at a time. We provide an example of training-free extension to multiple parts, as seen in \Cref{fig:double_part_edit}. The extension is done at inference time, with previously separately trained tokens, more details can be seen in \Cref{sec:multiple_edits_region}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Study}\label{sec:abl}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myparagraph{Impact of $t_e$}
The choice of the number of timesteps to perform feature blending using \Cref{eq:feat_blend} controls the locality of the edit, where the blending always starts at timestep one and ends at $t_e$.
\Cref{fig:te} shows two different scenarios for how to choose $t_e$ based on the user desire. 
In the first row, a low $t_e$ causes the horse's legs and tail to change to dragon ones, but a higher $t_e$ would change only the head and preserve everything else.
The second row has another example: a lower $t_e$ discards the girl's hair, and a higher $t_e$ preserves it.
Consequently, $t_e$ gives the user more control over the locality of the performed edits.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\myparagraph{Impact of Binarization}
To show the efficacy of our proposed thresholding strategy in \Cref{eq:thresh}, we show an edit under different binarization strategies in \Cref{fig:binary}.
Standard binarization, where the blending mask is thresholded at 0.5, leads to the least seamless edit as if a robotic mask was placed on the man's head.
OTSU integrates some robotic elements on the neck, but they are not smoothly blended.
Finally, our thresholding strategy produces the best edit, where the robotic parts are seamlessly integrated into the neck.

\myparagraph{Impact of number or selection of images} We further highlight the robustness of token training given the number of images and the selected images in \Cref{sec:number_of_training_images,sec:impact_selection_training_images} respectively. Our method outperforms existing mask-based methods (\cref{tab:quant}) for part editing under limited data. More ablations in \Cref{sec:ablation_omega_adaptive_thresholding,sec:padding_strategy}. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
\centering
\begin{minipage}{.55\textwidth}
  \centering
  \includegraphics[width=\linewidth]{fig/t_edit.pdf}
  \captionof{figure}{The impact of changing the number of denoising steps $t_e$ to perform feature blending.}
  \label{fig:te}
\end{minipage}%
\begin{minipage}{.01\textwidth}
\
\end{minipage}%
\begin{minipage}{.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{fig/binarization.pdf}
  \captionof{figure}{Comparison of an edit under different mask binarization strategies in our novel layer timestep blending setup.} 
  \label{fig:binary}
\end{minipage}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%