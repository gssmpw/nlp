\vspace{-0.365cm}
\subsection{Programming Nexus Machine}
\begin{comment}
The program is provided as a C-based code that indicates the loop iterations.
The programmer is responsible for explicitly annotating independent loop iterations. 
Similar to previous parallel programming paradigms (e.g., OpenMP, CUDA), the programmer annotates outer loops containing independent loop nests using the \textit{parallel-for} keyword.
\textit{Nexus Machine} proceeds to compile and execute the program, with each iteration of the \textit{parallel-for} loop executed independently but with sequential semantics. 
A loop with dependencies that is incorrectly marked \textit{parallel-for} will execute with undefined semantics.
\end{comment}
\begin{comment}
The program is written in C code that specifies loop iterations. 
The programmer must explicitly annotate independent loop iterations. 
Similar to other parallel programming paradigms (e.g., OpenMP, CUDA), the programmer uses the \textit{parallel-for} keyword to annotate outer loops with independent loop nests. 
\textit{Nexus Machine} then compiles and executes the program, with each \textit{parallel-for} loop iteration running independently but maintaining sequential semantics. 
If a loop with dependencies is incorrectly marked as \textit{parallel-for}, it will execute with undefined behavior.
\end{comment}
\begin{comment} The program is written in C with loop iterations specified by the programmer. Independent loop iterations must be explicitly annotated using the \textit{parallel-for} keyword, similar to OpenMP or CUDA. \textit{Nexus Machine} compiles and executes the program, running each \textit{parallel-for} loop iteration independently while preserving sequential semantics. Incorrectly annotating dependent loops as \textit{parallel-for} will result in undefined behavior.\end{comment}
{Nexus targets affine loops with associative operations, using the polyhedral model to enable parallel execution, especially for AI workloads involving reduction loops. Programs are written in C, where the programmer annotates independent loop iterations with \textit{parallel-for} keyword, similar to OpenMP or CUDA. The compiler supports nested loops and fully unrolls annotated loops, maximizing parallelism across all levels. Each iteration is flattened, ensuring sequential execution of instructions within each iteration to maintain intra-iteration dependencies. It is the programmerâ€™s responsibility to ensure correct annotations for non-assocaiative inter-iteration dependencies. \textit{Nexus Machine} compiles and executes the program, running each \textit{parallel-for} loop iteration independently while preserving sequential semantics. Incorrectly annotating dependent loops as \textit{parallel-for} will result in undefined behavior. An example SpMV code illustrating these annotations is provided in Fig. 5.
%Any incorrectly marked dependencies within loops are flagged as errors by the \textit{Nexus Machine}'s static compiler {\color{blue} using the \textit{Loop Vectorizer} pass in LLVM}.
\vspace{-0.39cm}
\subsection{Nexus Machine Compiler}
\label{section:compiler}
\begin{figure}[h!]
	\scriptsize
	\centering
	\includegraphics[width=\columnwidth]{diagrams/compiler.pdf}
	\caption{Process for transforming an application code and data into configuration and data format for \textit{Nexus Machine}.}
	\label{fig:compiler}
	%\vspace{-.5cm}
\end{figure}

\begin{comment}
\begin{algorithm}[!t]
\small
\caption{DFG generation}
\label{alg:labelAwareSA}
Identify backedges and connect basic blocks \;
Get transfer variables among nested loops \;
Convert LLVM IR instruction into DFG nodes \;
Set data dependency constraints \;
Remove redundant and dead DFG nodes \;
Sort DFG nodes by ASAP order \;
\KwIn{Source code, annotated loop;}
\KwOut{DFG}
\end{algorithm}
\end{comment}

\textit{Nexus Machine} integrates a static compiler responsible for preprocessing the application code, alongside a lightweight runtime manager executing on the host processor. 
The static compiler handles DFG generation, data partitioning, and data allocation. 
Concurrently, the runtime manager processes the data partitions, generating \textit{static AMs} that include the intermediate, final destinations and their designated locations for preloading into the \textit{AM Queues} within the PEs.
The dynamic routing is inherently done by \textit{Nexus Machine}'s architecture.
%The \textit{Nexus Machine}'s compiler produces a binary representation that encompasses the application loop kernel, data distribution across memories, and the generation of static AMs containing input operands and their precise location within the PE array.

\textbf{Static Compiler.}
Fig.~\ref{fig:compiler} illustrates the process of converting both the serial code and associated data into configurations suitable for the \textit{Nexus Machine} along with a compatible data format.

\begin{comment}
We generate the LLVM Intermediate Representation (IR) from the application's annotated C source code. 
This IR represents the low-level operations performed on data along with their dependencies.
We implement an LLVM pass to examine the LLVM IR and generate a DFG, which is translated into opcodes for the \textit{configuration memory}.
With the LLVM pass, first, backedges are identified and basic blocks are connected to establish a clear control flow. Transfer variables among nested loops are then identified, facilitating data movement across loop iterations. Subsequently, LLVM IR instructions are converted into nodes for the DFG. We then set data dependency constraints to ensure proper execution order and data integrity. Next, we remove redundant and dead DFG nodes to optimize the graph structure. Finally, the remaining DFG nodes are sorted by As Soon As Possible (ASAP) order to prioritize their execution. 
\end{comment}
We generate the LLVM Intermediate Representation (IR) from the application's annotated C source code. 
This IR captures the low-level data operations and their dependencies. 
We then apply an LLVM pass to analyze the IR and produce a Data Flow Graph (DFG), which is converted into opcodes for the \textit{configuration memory}.
The LLVM pass begins by identifying backedges and connecting basic blocks to define a clear control flow. 
It then converts LLVM IR instructions into nodes in the DFG. 
Data dependency constraints are set to ensure correct execution order and data integrity. 
Redundant and dead nodes are subsequently removed to optimize the graph structure. 
Finally, the remaining DFG nodes are sorted in As Soon As Possible (ASAP) order to optimize execution.
Fig.~\ref{fig:exec_model} illustrates a representative DFG for SpMV.

\textit{Problem definition for distributed data placement}:
Given a set of sparse tensors represented as $A$ and a reconfigurable architecture with $N$ homogeneous PEs, we employ a greedy partitioning strategy to divide $A$ into $N$ partitions, denoted as {$A_1$, $A_2$, $A_3$,...,$A_N$} as discussed in Section~\ref{section:data_placement}.
%Given a set of sparse tensors represented as $A$ and a CGRA with $N$ homogeneous PEs, we employ a greedy partitioning strategy to divide $A$ into $N$ partitions, denoted as {$A_1$, $A_2$, $A_3$,...,$A_N$}, ensuring each partition contains at least $nnz(A)/N$ non-zero elements.
%An exemplary illustration of the tensor partitioning for the SpMV kernel is detailed in Section ~\ref{section:data_placement}.
%Each partition $A_n$ is assigned to a distinct PE within the CGRA. 
%The primary objective is to maximize parallelism while minimizing communication overhead, which is quantified by ensuring that the computational cost $\phi(A_n)$ for each partition $A_n$ is minimal.%approximately equals a predefined constant $\Phi$.
The primary objective is to allocate each partition $A_n$ to a distint PE to maximize parallelism while minimizing communication overhead. This is achieved by minimizing the computational cost $\phi(A_n)$ for each partition $A_n$.

Considering two sets of tensors, denoted by $X$ and $Y$, which results in the output tensor $Z$, the optimization problem requires determining the optimal placement of these three distributed tensors. 
The output tensor $Z$ is computed as $Z = f(X, Y)$, where $f$ represents the function that operates on $X$ and $Y$ to generate $Z$. 
Initially, the partitions of $Z$ are sequentially placed on distinct PEs. 
Subsequently, the partitions of tensors $X$ and $Y$ are strategically placed based on the data dependencies inferred from the DFG.

To optimize the placement of partitions of tensor $X$, we iteratively evaluate the available partitions of $X$ and select the partition $X_{i'}$ containing the maximum number of elements required to compute the corresponding partition $Z_i$ according to the function $f(X, Y)$. 
Upon assigning $X_{i'}$ to PE $i$, it is removed from the set of available partitions. 
A similar procedure is followed for the partitions of tensor $Y$.
%Given a set of sparse tensors represented as $A$ and a CGRA with $N$ homogeneous PEs, the problem is to determine the optimal partitioning of $A$, denoted as {$A_1$, $A_2$, $A_3$,...,$A_N$}. This partitioning aims to maximize parallelism that satisfies $\phi(A_n) \simeq \Phi$, where $\phi(A_n)$ represents the computational cost of partition $n$ for $n \in {1, 2, ..., N}$, and $\Phi$ denotes a predefined constant.

%To accomplish our objectives, we implement a heuristic-driven data partitioning mechanism. Consider two sets of sparse tensors, denoted by $X$ and $Y$, that yield the set of sparse outputs, $Z$. Employing a straightforward load balancing approach, we evenly partition $Z$ into subsets: {$Z_1$, $Z_2$, $Z_3$, ..., $Z_N$}, ensuring uniform distribution across multiple PEs.
%We strategically distribute and position the input data $X$ and $Y$ based on the data dependencies from the Data Flow Graph (DFG). 
%Our placement approach aims to enhance parallelism by co-locating inputs that exhibit sequential dependencies with the output within the same PE. 
%This placement strategy optimizes the computational cost, quantified by the number of non-zero elements.

\textbf{Lightweight Runtime Manager.}
The \textit{Runtime Manager}, operating concurrently on the host processor, utilizes data placement information provided by the static compiler to generate a sequence of \textit{static AMs}.
These \textit{static AMs} are then loaded into the \textit{AM Queues} of individual PEs for execution.
Fig.~\ref{fig:message_format} illustrates the format of these compiler-generated \textit{static AMs}.
Each \textit{static AM} corresponds to a unique \textit{Op1} operation and is loaded onto the respective PE, aligned with its designated location.
Each \textit{static AM} contains the value of first operand, along with essential details such as the PE ID, and address for the dependent input operands and the result.
%A distinct \textit{static AM} is created for each computation on an output element, and these AMs are loaded onto the respective PEs aligned with the output's designated location.
%Each \textit{static AM} includes essential information such as the PE ID and data/register location details for every dependent input operand.

For every element in the first operand, the runtime manager generates a \textit{static AM} containing information about the operands and the result. 
This AM pairs this element, stored as \textit{Op1}, with the location of the second operand denoted by \textit{R1}, indicating the PE where it resides, and stores the local address as \textit{Op2}. 
Similarly, the result is stored at \textit{R2}, along with its local address as \textit{Result}.