%\vspace{3mm}
\section{Nexus Machine}
\begin{figure}[h!]
	\scriptsize
	\centering
    \includegraphics[width=0.88\columnwidth]{diagrams/execution_model.pdf}
    \vspace{-0.65cm}
    \caption{Execution of SpMV using the data depicted in Fig.~\ref{fig:motivation} on a fabric with 2 PEs. It illustrates the placement of matrix, vector, and output partitions, along with the generation of AMs. [] denotes element address, and {\color{red}red} arrows represent control signals.}
    \label{fig:exec_model}
\end{figure}
\textit{Nexus Machine} is a novel reconfigurable architecture specialized for irregular workloads that uses AM paradigm for \textit{Data-Driven} execution model.
This section introduces the execution model of \textit{Nexus Machine}, followed by its micro-architecture and compiler.

Fig.~\ref{fig:detail_arch}(a) provides an overview of the architecture, showcasing PEs interconnected via a mesh network.
\subsection{Execution Model}
\textbf{Static Initialization.}
\textit{Nexus Machine} follows a distributed tensor placement approach, partitioning all tensors across the PEs.
Initially, the compiler generates a \textit{static AM} for each element of the first tensor.
These \textit{static AMs} are then stored in the active message network interface, while the remaining tensors are placed in the data memory within each PE. 
Additionally, the compiler generates opcodes corresponding to the workload and stores them in the configuration memories of all the PEs.

\textbf{Dynamic Execution.}
When execution is initiated, the active message network interface dequeues the first \textit{static AM} and then routes it based on dynamic turn model~\cite{noc_peh} routing protocol to the PE containing the next operands.
Upon arrival at the PE, the AM is decoded and updated accordingly, transforming it into a \textit{dynamic AM} created on-the-fly, in contrast to the predefined \textit{static AMs} generated at compile time.
Once the AM has gathered all the required operands, it proceeds towards the next destination PE, for execution. 
Additionally, we allow these AMs to perform computations en-route if they encounter an available compute unit.
The final result gets stored in the memory of the destination PE.

\begin{comment}
{\color{blue}Thus, \textit{static AMs} are generated at compile-time based on initial data placement and are responsible for initiating computation, while \textit{dynamic AMs} are created on-the-fly, adapting to real-time data conditions and resource availability. This enables Nexus Machine to optimize both data placement and computation, enhancing overall efficiency for irregular workloads.}
\end{comment}
Thus, \textit{static AMs} are generated at compile time, containing the initial instructions for computation. During execution, \textit{static AMs} are transformed into \textit{dynamic AMs}, carrying different instructions based on the AM format and real-time conditions. This allows \textit{Nexus Machine} to optimize both data placement and computation, enhancing efficiency for irregular workloads.

Fig.~\ref{fig:exec_model} illustrates the execution of SpMV using the example from Fig.~\ref{fig:motivation} on a fabric with 2 PEs. The \textit{matrix} operand is split and converted into \textit{static AMs}, stored in \textit{AM Queues}, while \textit{vec} and \textit{output} are divided and placed in \textit{data memories}. At cycle 0, PE0â€™s initial \textit{static AM} (with operand f) performs a LOAD to fetch \textit{Op2} (h in this case) from \textit{data memory}, then creates a \textit{dynamic AM} with \textit{Op2} and \textit{Opcode} MUL. This \textit{dynamic AM} is sent to PE1, where operands are multiplied in cycle 1, and the result is combined with \textit{Opcode} ADD to generate the next AM. PE1 updates \textit{output} (n in the \textit{data memory}) by adding the multiplied result to it (not shown in the figure).
Note that PE1 concurrently dequeues and processes AMs from its \textit{AM Queue}, although this is not shown here for the sake of simplicity.

The following sub-sections elaborate on the different components of \textit{Nexus Machine} in detail.

\subsubsection{Tensor Partitioning.}
\label{section:data_placement}
\textit{Nexus Machine} adheres to a coarse-grained distributed tensor partitioning strategy.
We follow a heuristic-based approach to partition any generic tensor into multiple segments, assigning one segment to each PE.
\begin{figure}[t!]
	\scriptsize
	\centering	\includegraphics[width=0.85\columnwidth]{diagrams/data_partitioning.pdf}
    \vspace{-0.25cm}
    %\caption{Data partitioning for SpMV. It multiplies a sparse matrix \textbf{X} ($m$x$n$) with dense vector \textbf{Y} ($n$x$1$) to output \textbf{Z} ($m$x$1$). Four partitions are shown in different colors, distributed across PEs denoted by the same color. Non-zeroes of the sparse matrix are shown in colored squares.}
    \caption{Data partitioning for SpMV involves multiplying a sparse matrix \textbf{X} ($m \times n$) with a dense vector \textbf{Y} ($n \times 1$) to produce \textbf{Z} ($m \times 1$). Four partitions are shown in different colors, distributed across PEs denoted by the same color. Non-zero elements are shown in colored squares.}
	\label{fig:data_partitioning}
\end{figure}

Fig.~\ref{fig:data_partitioning} shows an exemplary partitioning of the input tensors for SpMV on an architecture with $\textbf{N}=4$ PEs.
We employ a partitioning function to split the sparse 2D tensor $\textbf{X}$ into $\textbf{N}$ parts $\textbf{X}_1$, ..., $\textbf{X}_N$, each with the same number of columns but a different number of rows. To ensure better load balancing, each partition $\textbf{X}_i$ is approximated to have $nnz(\textbf{X}_i) \approx nnz(\textbf{X})/N$ non-zeros. This partitioning is done by scanning the row pointer array of $\textbf{X}$ in Compressed Sparse Row (CSR) format, with a computational complexity of $\mathcal{O}(m)$, where $m$ is the number of rows in $\textbf{X}$.

Similarly, 1D tensors $\textbf{Y}$ and $\textbf{Z}$ are partitioned, creating partitions with an equal number of non-zeros. For a dense tensor, this involves segmenting it equally into $k$ partitions.

\subsubsection{Data-Driven Execution.}
In the \textit{Nexus Machine} architecture, each PE stores the first input tensor as pre-compiled active messages (AM) and a partition of other tensors. At the source PE, the first AM is sent to the PE with the next data element. Since the initiation of execution is triggered by the active message or one of the tensor elements, we refer to this model as \textit{Data-Driven Execution}. 
%\textit{Nexus Machine} uses \textit{lazy execution}, deferring processing to the destination PE until the output is required.
%{\color{blue}Executing operations solely at the source or destination PE leads to underutilization and load imbalance across PEs, especially for irregular workloads.}

\begin{comment}
Fig.~\ref{fig:detail_arch}(a) illustrates an example where instruction \textit{I0} at PE4 triggers \textit{I1}, generating a dynamic AM with \textit{I1} and \textit{I0}'s output, which is then sent to destination PE15 for execution. 
However, restricting execution solely to the source or destination PE results in significant underutilization and workload imbalance across PEs, particularly in irregular workloads where data dependencies and control flow are unpredictable.
\end{comment}
Fig.~\ref{fig:detail_arch}(a) illustrates an example where instruction \textit{I0} at PE4 triggers \textit{I1}, generating a dynamic AM with \textit{I1} and \textit{I0}'s output, then sent to destination PE15 for execution.
However, restricting execution solely to the source or destination PE results in significant underutilization and workload imbalance across PEs for irregular workloads where data dependencies and control flow are unpredictable.
%However, due to unpredictable control flows in irregular workloads, this model often suffers from sub-optimal performance and underutilization of PEs due to load-balancing issues.

\subsubsection{In-Network Computing.}
To enhance load balancing, we employ \textit{opportunistic execution} approach, allowing an AM to execute on an \textbf{intermediate PE} as it travels towards its final destination.
With this execution model, en-route AMs carry both the instruction and required data operands, enabling \textit{intermediate PEs} to perform computations whenever an idle ALU is encountered.
%To address load balancing, we adopt an opportunistic approach by executing an AM on an \textbf{intermediate PE} while it is en-route to its final destination.
%Because of \textit{lazy execution}, the en-route AMs contain the instruction with the data operands enabling \textit{intermediate PE} to perform executions.
In Fig.~\ref{fig:detail_arch}(a), the highlighted cores in blue represent the potential \textit{intermediate PEs} for a message routed from PE4 to PE15.
The message is executed at PE13, denoted as \textit{I1'}, as it serves as the first intermediate PE along the route with an idle ALU.

This provides several advantages: 
(a) It introduces a hardware mechanism to enhance workload distribution and fabric utilization by leveraging idle PEs,
(b) reduces NoC contention by deciding whether messages are executed en-route or continue toward their destination PEs, and
(c) minimizes the amount of data traversing the NoC by coalescing the updates to the original message and discarding unnecessary data.
%\textcolor{red}{third point is not clear}

\subsubsection{Termination and Global Synchronization.}
\textit{Nexus Machine} completes an execution when all PEs are inactive and no messages are in transit, generating a global idle signal to indicate completion.

To realize the global synchronization construct, \textit{Nexus Machine} uses this global idle signal to notify the host via an interrupt. This approach is effective for edge architectures with limited on-chip memory, as it employs data tiling to manage resources~\cite{extensor, tiling}. Data tiles are executed sequentially in a global synchronized manner. Once a tile finishes, the system detects the idle state and triggers the next tile's execution.