\section{Experimental Evaluations}

\subsection{Baseline Architectures}
We evaluate \textit{Nexus Machine} against two baseline architectures, \textit{Generic CGRA} and \textit{TIA-like} modeled after HyCube~\cite{hycube} and Triggered Instructions~\cite{tia}, respectively, with the following architectural parameters for fair evaluations.

\textbf{PE array:} The PE array consists of 16 PEs interconnected with a mesh NoC where each router hop takes a cycle.
\textit{TIA-like} and \textit{Nexus Machine} have a dynamic NoC with 16 identical PEs.
In contrast, \textit{generic CGRA} has a static NoC, controlled by router configurations, with 4 memory PEs positioned at a boundary column and 12 regular PEs. 
%\textcolor{red}{explain why TIA needs dynamic NoC while HyCUBE can use static NoC. Myabe draw a figure to show the 3 architectures if you have space?}
\begin{comment}
{\color{blue} \textbf{Data memory:} \textit{TIA-like} and \textit{Nexus Machine} have 16KBx16 dual port memories, locally connected to each PE.
Whereas, \textit{generic CGRA} has 64KBx4 single-port memories accessible by the boundary PEs.}
\end{comment}

\textbf{Data memory:}  \textit{TIA-like} architecture incorporates 16KBx16 dual-port memories, while the \textit{Nexus Machine} has 8KBx16 dual-port memories and 8KBx16 \textit{AM Queues} per PE. Whereas, \textit{generic CGRA} has 64KBx4 dual-port memories accessible only by boundary PEs.

\textbf{Configuration memory:} \textit{TIA-like} and \textit{Nexus Machine} have 8 entries of 10 bits each to fit the selected benchmarks.
\textit{Generic CGRA} has 8 entries of 64 bits to fit the ALU and router configurations. As \textit{TIA-like} and \textit{Nexus Machine} use dynamic NoC, router configurations are not required, so each entry is 10 bits long.
%All the three architectures are evaluated with 16 PEs, and 
%Table~\ref{table:arch_parameters} shows the architectural parameters used for evaluations.

\subsection{Benchmarks}
\begin{table}[h!]
    \centering
    \resizebox{0.85\columnwidth}{!} {
    \begin{tabular}{rrrr} \hline \rule{0pt}{2.5ex} 
    \textbf{Kernel}&  \textbf{Name}&  \textbf{Dimensions}& \textbf{Density (in \%)}\\ \hline
    %[1mm]
    %& & &\textbf{(in \%)}\\[1mm]
    %\hline \rule{0pt}{2.5ex} 
    
        
    %New Entry%      
    SpMV &  Resnet50 \#50~\cite{resnet50}&  
& \\[1mm]
 & Weights& $2048 \bullet 1000 $&13.03\\ \hline \rule{0pt}{2.5ex}  
    
    %New Entry%      
    SpMSpM &  Resnet50 \#40&  & \\
 & Weights& $512 \bullet 1024$&39.05\\ 
    & Input Activations& $1024 \bullet 196$&46.04
        \\[1mm]\hline \rule{0pt}{2.5ex} 

 %    %New Entry% 
 %    Conv &  Resnet50 \#13&  & \\[1mm]
 % & Weights& $28 \bullet 3 \bullet 128 \bullet 128$&42.50\\\hline \rule{0pt}{2.5ex}   

    %New Entry% 
    SDDMM &  DEiT-Base \#1~\cite{deit}&  & 
            \\
 & Mask& $197 \bullet 197$&\\  
    & Score& $197 \bullet 64$&9.82
        \\[1mm] \hline \rule{0pt}{2.5ex}   

    %New Entry% 
    SpM+SpM &  Resnet50 \#3&  & \\[1mm]
 & 64 Convoluted& $64 \bullet56 \bullet 56$&40.77 \\
 & Matrices & &(average)\\\hline
 Sparse& Resnet50 \#40& &\\
 Convolution& Weights& $3 \bullet 3$&\\
 & Input Activations& $1024 \bullet 196$&46.04\\\hline
 MatMul& Resnet50 \#40& &        \\
 & Weights& $512 \bullet 1024$&100\\
 & Input Activations& $1024 \bullet 196$&100\\\hline
 Dense& Resnet50 \#40& &\\
  Convolution& Weights& $3 \bullet 3$ &100\\
 & Input Activations& $1024 \bullet 196$&100\\\hline  BFS, SSSP&  infect-dublin~\cite{idublin} &  & \\
     %New Entry% & Nodes& 410&3.03\\
 & Nodes& 410&3.03\\[1mm]\hline
    
    \end{tabular}
  }
    \caption{Datasets used for evaluating \textit{Nexus Machine}}
    \label{table:datasets}
\end{table}
%\textcolor{red}{this is a good place to explain what classes of applications you can handle. Is it sparse tensors only? Then SSSP is using incidence matrix? Should also be included in introduction}
\begin{comment}
\textit{Nexus Machine} is designed for accelerating irregular workloads including sparse tensor algebra and graph analytics, specifically for performing computations on sparse tensors.
We evaluated \textit{Nexus Machine} on six workloads, as illustrated in Table~\ref{table:datasets}.
%We evaluated \textit{Nexus Machine} on six Irregular workloads from sparse tensor algebra and graph analytics, as illustrated in Table~\ref{table:datasets}.\\

\textbf{Sparse Matrix-Vector Multiplication (SpMv)} multiplies a Sparse Matrix in CSR format with a Dense Vector. 
\textbf{Sparse Matrix Multiplication (SpMSpM)} multiplies two Sparse matrices, both in CSR format. Our baselines are based on Gustavson's algorithm (Row-based) as it is an asymptotically efficient parallel algorithm that processes one output row at a time. It is an important linear algebra benchmark and manifests in many applications at the edge, including Pruned CNNs and Sparse Attention~\cite{sanger, vitcod}.
%\textbf{Convolution (Conv)} takes one sparse matrix in CSR format and convolves it with a dense kernel. For our baselines, we evaluate the output-stationary model.
\textbf{Sparse Matrix Addition (M+M)} involves summing two matrices in CSR format element-wise. 
%For our baselines, we consider the binary mask of the output to be precomputed and used as the tensor for AM generation. 
%We consider the binary mask of the output to be precomputed, which serves as the tensor for AM generation.
It is an important benchmark as CNNs accumulate intermediate feature maps across the output channels to get the final output activation.
%\textbf{Sampled Dense-Dense Matrix Multiplication (SDDMM)} is a crucial kernel that finds applications in various settings, including Sparse Attention and Graph Neural Networks. This operation computes the product of two dense matrices by evaluating them only at the non-zero entries of a sparse matrix in CSR format. The sparse matrix serves as the tensor for AM generation.
\textbf{Sampled Dense-Dense Matrix Multiplication (SDDMM)} is a crucial kernel used in Sparse Attention and Graph Neural Networks. It computes the product of two dense matrices by evaluating them only at the non-zero entries of a sparse matrix in CSR format, which serves as the tensor for AM generation.

In graph analytics, \textbf{Breadth-First Search (BFS)} is a fundamental graph traversal algorithm to determine the vertices at a specific distance from the source node.
\textbf{Single shortest path (SSSP)} is the problem of finding a path between two vertices in a graph such that the sum of the weights of its constituent edges is minimized.
For our experiments, we represent graphs using adjacency matrices.
\end{comment}
\textit{Nexus Machine} is designed to accelerate irregular workloads, such as sparse tensor algebra and graph analytics. We evaluated \textit{Nexus Machine} on sparse, dense, and graph workloads to demonstrate its generality; details on the benchmarks are provided in Table~\ref{table:datasets}.

\textbf{Sparse Matrix-Vector Multiplication (SpMv)} involves multiplying a sparse matrix in CSR format with a dense vector. \textbf{Sparse Matrix Multiplication (SpMSpM)} multiplies two sparse matrices, both in CSR format, using Gustavson's algorithm (row-wise), a key benchmark in edge applications including Pruned CNNs and Sparse Attention~\cite{sanger, vitcod}. \textbf{Sparse Matrix Addition (M+M)} sums two sparse matrices in CSR format element-wise, an important benchmark as CNNs accumulate intermediate feature maps across the output channels to get the final output activation.
\textbf{Sampled Dense-Dense Matrix Multiplication (SDDMM)} computes the product of two dense matrices by evaluating them only at the non-zero entries of a sparse matrix in CSR format, relevant for Sparse Attention and Graph Neural Networks.
\textbf{Sparse Convolution} involves convolving a sparse kernel with sparse activations, crucial for CNN workloads where both weights and activations are pruned~\cite{activation_sparsity}.
\textbf{Dense Matrix Multiplication} multiplies two dense matrices, a critical workload for CNNs and LLMs, while \textbf{Dense Convolution} involves a dense kernel with dense activations, essential for CNNs.

In graph analytics, \textbf{Breadth-First Search (BFS)} explores vertices at a specified distance, \textbf{Single Source Shortest Path (SSSP)} finds minimum-weight paths, and \textbf{PageRank} ranks nodes based on connectivity, useful in search engines and social networks. Graphs are represented with adjacency lists in our experiments.
%In graph analytics, \textbf{Breadth-First Search (BFS)} is used to traverse and identify vertices at a specific distance from the source node, while \textbf{Single Shortest Path (SSSP)} finds the minimum weight path between two vertices. {\textbf{PageRank} is an algorithm that ranks nodes in a graph based on their connectivity and importance, commonly used in web search engines, social network analysis, and recommendation systems. \color{blue} Graphs are represented using adjacency list for our experiments.}
\begin{comment}
\textit{Nexus Machine} is designed to accelerate irregular workloads, such as sparse tensor algebra and graph analytics. We evaluated it on sparse, dense, and graph workloads to demonstrate its generality; benchmark details are provided in Table~\ref{table:datasets}.

\textbf{Dense Matrix Multiplication} multiplies two dense matrices, a critical workload for CNNs and LLMs, while \textbf{Dense Convolution} involves a dense kernel with dense activations, essential for CNNs. \textbf{Sparse Matrix-Vector Multiplication (SpMV)} multiplies a sparse matrix in CSR format with a dense vector, and \textbf{Sparse Matrix Multiplication (SpMSpM)} multiplies two sparse matrices using Gustavsonâ€™s row-wise algorithm, important for Pruned CNNs and Sparse Attention~\cite{sanger, vitcod}. \textbf{Sparse Matrix Addition (M+M)} sums two sparse matrices element-wise in CSR format, a key benchmark for accumulating CNN feature maps across output channels. \textbf{Sampled Dense-Dense Matrix Multiplication (SDDMM)} multiplies two dense matrices, evaluated only at non-zero entries of a sparse matrix, relevant to Sparse Attention and Graph Neural Networks. \textbf{Sparse Convolution} involves convolving sparse kernels with sparse activations, crucial for pruned CNN workloads~\cite{??}.

In graph analytics, \textbf{Breadth-First Search (BFS)} explores vertices at a specified distance, \textbf{Single Source Shortest Path (SSSP)} finds minimum-weight paths, and \textbf{PageRank} ranks nodes based on connectivity, useful in search engines and social networks. Graphs are represented with adjacency lists in our experiments.
\end{comment}
\vspace{-0.4cm}
\subsection{Experimental Setup}
We evaluate \textit{Nexus Machine}'s and \textit{TIA-like} architecture's performance using a custom-written cycle-accurate simulator that models dynamic routers, congestion control, and backpressure signaling. 
Benchmarks are mapped to \textit{configuration memory} and \textit{AM Queues} for simulation with datasets specified in Table~\ref{table:datasets}.
For SpMv, SpMSpM, and SpM+SpM, we use a ResNet-50~\cite{resnet50} model, pruned and fine-tuned to maintain accuracy. SpMSpM involves transforming convolution layers into matrices via Toeplitz~\cite{toeplitz}. SDDMM is evaluated with a sparse input mask similar to ViTCoD~\cite{vitcod}, and graph datasets are partitioned using Metis~\cite{metis} for load balancing.
%We simulate the \textit{generic CGRA} with LLVM-based Morpher~\cite{morpher} mapping and model bank conflicts during irregular workloads. 
We simulate the \textit{generic CGRA} using the mapping information provided by the LLVM-based Morpher~\cite{morpher} tool and model bank conflicts during irregular workloads. 

All architectures are modeled in System Verilog HDL, synthesized on a commercial 22 nm FDSOI process using Cadence Genus at 100MHz. The foundryâ€™s memory-compiled SRAM cells are used to model the memories.
\textit{Nexus Machine} is synthesizable with a maximum clock frequency of 588MHz.
We verify functionality through behavioral simulations at the RTL level, using test inputs to generate VCD files and calculate an average toggle rate of 0.12 to 0.2 for the compute logic. For conservative power estimates, we apply a higher default toggle rate of 0.25 in Cadence Genus for post-synthesis netlist estimations.
\subsection{Experimental Results}
\begin{figure}[h!]
	\scriptsize
	\centering
	\input{graphs/performance}
	\caption{Performance of \textit{generic CGRA} and \textit{TIA-like} architectures across workloads (in cycles), normalized to \textit{Nexus Machine}.} 
	\label{fig:performance}
	%\vspace{-.5cm}
\end{figure}
%\textcolor{red}{\bf Peh: Is your legend in Fig 10 wrong?}
\vspace{-0.8cm}
\begin{figure}[h!]
	\scriptsize
	\centering
	\input{graphs/innetworkcomp}
	\caption{\textit{Nexus Machine}: Percentage of total computations performed in-network.} 
	\label{fig:innetworkcomp}
	%\vspace{-.5cm}
\end{figure}
\vspace{-0.8cm}
\begin{figure}[h!]
	\scriptsize
	\centering
	\input{graphs/utilization}
	\caption{Total Fabric Utilization} 
	\label{fig:utilization}
	%\vspace{-.5cm}
\end{figure}
\vspace{-0.5cm}
\begin{figure}[h!]
	\scriptsize
	\centering
    \input{graphs/congestion}
	%\includegraphics[width=0.9\columnwidth]{content/graphs/congestion1.png}
	\caption{Total Network Congestion} 
	\label{fig:congestion}
\end{figure}
\vspace{-0.2cm}
\begin{figure}[h!]
	\scriptsize
	\centering
	\input{graphs/network_traffic}
	\caption{Network Traffic Overhead: Extra data transmitted over NoC in \textit{Nexus Machine} relative to \textit{TIA-like} architecture.}
	\label{fig:network_overhead}
	%\vspace{-.5cm}
\end{figure}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}

{\bf Overall Performance.}
We assess the performance of each benchmark by monitoring the number of cycles required for execution completion on each of the architectures. Fig.~\ref{fig:performance} presents number of cycles normalized to that of \textit{Nexus Machine}.
\textit{Nexus Machine} outperforms \textit{generic CGRA} and \textit{TIA-like} baselines by an average of 1.7x and 1.5x respectively. For SDDMM, \textit{Nexus Machine} outperforms \textit{generic CGRA} and \textit{TIA-like} by 2.07x and 1.88x, respectively.

We also analyze the distribution of cycles spent by the fabric for 1) useful computations and 2) idle cycles. 
%As anticipated, indirect memory accesses and stalls consume a significant portion of cycles in both \textit{Conventional} and \textit{Dataflow} baselines. However, \textit{Nexus Machine} effectively addresses this issue by bringing compute to these idle ALUs.
\textit{Nexus Machine} performs better for all the benchmarks in comparison to both the baselines.
As anticipated, both \textit{generic CGRA} and \textit{TIA-like} baselines have substantial overheads due to stalls attributed to bank conflicts and load imbalance, respectively. 
However, \textit{Nexus Machine} effectively mitigates this issue by dynamically bringing compute to the idle ALUs, thus reducing the number of stalls significantly. 
%\textcolor{red}{state first which benchmarks Nexus does better. Also explain where TIA does better or worse than generic and why}

For the SpMv and SpM+SpM workloads, \textit{generic CGRA} outperforms the \textit{TIA-like} baseline due to a lower number of unroll factor, effectively mitigating bank conflicts during execution. Thus, leading to a superior performance for these workloads.
Additionally, the SpM+SpM workload has denser tensor computations, further favoring the performance of the \textit{generic CGRA}.
%\textcolor{blue} {\bf Peh: Explain why TIA-like does worse than generic CGRA for SpMV and SpM+SpM -- poor control placement vs. compiler-driven placement of CGRA?} 
%For SDDMM, \textit{Nexus Machine} outperforms both the baselines significantly.
%This is because of low network congestion in the case of \textit{Nexus Machine}.
For SDDMM, \textit{Nexus Machine} performs significantly better compared to both the baselines.
This is primarily attributed to the reduced network congestion observed in the case of \textit{Nexus Machine}.
%{\bf Peh: Explain why TIA doesn't do well for SDDMM while Nexus does with some submetrics, cos in-network computation percentage is only 20\%, not high either, so more insight needed}
However, for dense workloads, \textit{TIA-like} outperforms \textit{Nexus Machine}, as opportunistic execution increases network congestion.
On average, \textit{Nexus Machine} requires 40\% and 32\% fewer cycles than \textit{generic CGRA} and \textit{TIA-like} baselines, respectively.

{\bf In-network computations.}
For \textit{Nexus Machine}, Fig.~\ref{fig:innetworkcomp} shows the percentage of in-network computations performed during routing of active messages.
\textit{Nexus Machine} performs 41.8\% of computations in-network on average, addressing load balancing in irregular workloads and improving fabric utilization.
For dense workloads, this drops to 2â€“3\% as all PEs are fully utilized.
%\textcolor{red}{explain the variation across benchmarks}

%Static pipeline CGRAs take higher number of cycles in all kernels, primarily due to their limitations in handling imbalanced branches. In such scenarios, the additional operations in the longer execution path cannot be effectively merged into the shorter ones, leading to increased cycle counts.

{\bf Fabric Utilization.} 
%\textcolor{red}{Why TIA utilization is worse than generic?}
Fabric Utilization denotes the percentage of PE fabric effectively performing computations during the entire execution.
Fig.~\ref{fig:utilization} illustrates the percentage of fabric utilization for each benchmark. \textit{Nexus Machine} exhibits consistently higher utilization than all the baselines. This can be primarily attributed to the \textit{AM Network Interface} logic, which reduces irregular memory loads, and in-network computations employed by \textit{Nexus Machine}'s architecture.

{\bf Network Congestion.}
Fig.~\ref{fig:congestion} presents the breakdown of network congestion in the fabric across all the input ports. \textit{Nexus Machine} mitigates the average network congestion by 13\%, compared to the \textit{TIA-like} baseline. It notably reduces congestion at the LOCAL port, which has the most substantial impact on overall runtime.
The routing congestion for \textit{generic CGRA} baseline is omitted, as it employs static routing that is determined at compilation time.
Compilation time for \textit{generic CGRA} averages around 7.22 sec due to its extensive static route search, as compared to just 0.55 sec for \textit{Nexus Machine} that performs hardware-based routing supported by AMs.

{\bf Area and Power.}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[t!]
    \resizebox{\columnwidth}{!} {
    \centering
    \begin{tabular}{cccc} \hline 
        \textbf{Component}&  \textbf{Sub module}&  \textbf{Area (in mm$^2$)} & \textbf{Power (in mW)}\\ \hline 
        \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Memory\end{tabular}}&  AM Queue&  0.0350& 0.4938\\ \cline{2-4}  
        & Config Memory& 0.0144&0.1611\\ \cline{2-4} 
        &  Data Memory&  0.0243& 0.5621\\ \hline
        Compute Unit& ALU& 0.0004&0.4419\\ \hline 
         \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Interconnect\end{tabular}}&  Logic&  0.0255& 2.0014\\ \cline{2-4}
        & Buffers& 0.0044&0.1406\\ \hline
        Scanners&  &  0.0023& 0.0728\\ \hline
        Extra& & 0.0078&0.065\\ \hline 
        Total&  &  0.1138& 3.938\\ \hline
    \end{tabular}
    }
    \caption{\textit{Nexus Machine}: Area and Power breakdown} 
    \label{table:synth_results}
\end{table}
%\vspace{2mm}
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[t!]
\centering
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{lcc}
\hline
\textbf{Architecture}                          & \textbf{Area (in mm$^2$)}& \textbf{Power (in mW)}\\ \hline
Generic CGRA&                     0.1035&                     2.961\\ \hline
TIA-like&                     0.1122&                     4.626\\ \hline
Nexus Machine&                     0.1138&                     3.938\\\hline
\end{tabular}%
}
\caption{Total Area and Power Comparisons for different architectures}
\label{table:synth_results_comparison}
\end{table}
Table~\ref{table:synth_results} presents a detailed breakdown of the area and power consumption for \textit{Nexus Machine}. 
\textit{Crossbar}, \textit{AM queues} and \textit{data memories} consume a significant portion of both area and power, while bit-vector scanners account for negligible area and power consumption.
%\textcolor{red}{\bf Peh: Scanners are considered extra? Their area/power overhead was not mentioned earlier and shd be briefly noted here.}

Table~\ref{table:synth_results_comparison} compares total area and power across the three architectures.
\textit{Nexus Machine} shows a slight area increase over the \textit{generic CGRA} due to the inclusion of \textit{AM Queues} and dynamic router buffers.
Its router logic consumes more power, primarily due to the crossbar handling more bits than \textit{generic CGRAs} and \textit{TIA-like}.
\textit{Generic CGRA} has larger configuration memory to store both ALU and router configurations.
\textit{TIA-like} has minor area and power overhead compared to \textit{Nexus Machine} from added tag-matching and scheduler logic.
\begin{comment}
\subsubsection{Power}
The power consumption is obtained from synthesis, as mentioned above. The power distribution of all the architectures is outlined in Table~\ref{table:synth_results}.
\iffalse Notably, the AM Generation logic contributes significantly to the overall power consumption of Nexus Machine due to the presence of FIFOs.\fi
\end{comment}

{\bf Power Efficiency.}
Table~\ref{tab:power_efficiency} shows the effective power efficiency. \textit{Nexus Machine} achieves a better average and peak power efficiency than the baselines.
\textit{Nexus Machine} achieves an average power efficiency of 139MOPS/mW across all the benchmarks, and a peak power efficiency of 191MOPS/mW.

{\bf Area Efficiency.}
Table~\ref{tab:power_efficiency} compares the area efficiency of \textit{Nexus Machine} with the baselines. 
\textit{Nexus Machine} demonstrates higher average area efficiency, achieving 4.7GOPS/mm$^2$.

We can conclude that \textit{Nexus Machine} provides a better trade-off between power, area and performance, achieving the highest power and area efficiency among the compared architectures.

{\bf Network Traffic.}
\iffalse
Network traffic refers to the data transferred during the execution of workloads.
Figure~\ref{fig:network_overhead} illustrates the network traffic overhead of Nexus Machine in comparison to TIA-like architecture.
Nexus Machine experiences an average of 13.89\% additional network traffic overhead. Note that this is due to the wider NoC of Nexus, to accommodate the additional information needed to encode AMs; There is no extra network packets in Nexus, nor any additional network latency or unnecessary hop count, as AMs are launched along the way to the destination with Nexus Machine's lazy execution. In other words, Nexus incurs higher NoC area overhead, instead of performance overhead.
\fi
Network traffic refers to the data transferred over NoC during execution of workloads.
Fig.~\ref{fig:network_overhead} compares the network traffic overhead of \textit{Nexus Machine} with a \textit{TIA-like} architecture.
\textit{Nexus Machine} exhibits an average of 26.32\% additional network traffic overhead.
This is due to the \textit{Nexus Machine}'s wider NoC required to accommodate the extra information for encoding AMs.
It's important to note that \textit{Nexus Machine} doesn't introduce extra network packets, latency, or additional hops. 
This is because AMs are efficiently launched along the route to their destination using the opportunistic execution approach. 
Essentially, \textit{Nexus Machine} incurs higher NoC area overhead instead of performance overhead.

%\textcolor{red}{Peh: Quantify how much higher area overhead. Explain why there is NoC area overhead? Cos actually, Nexus' has a narrower NoC, and the dynamic NoC has simple logic. Is it due to the AM queues being counted as part of NoC? If so, explain}


\begin{table}
    \centering
    \resizebox{0.95\columnwidth}{!} {
    \begin{tabular}{cccc} \hline 
         \textbf{Architecture} &   \textbf{Average Power}&\textbf{Peak Power} &\textbf{Average Area}\\
         &  \textbf{Efficiency}&\textbf{Efficiency} &\textbf{Efficiency}\\
 & \textbf{(in MOPS/mW)}&\textbf{(in MOPS/mW)} & \textbf{(in GOPS/mm$^2$)}\\ \hline
         Generic CGRA*&   110&123 &3.1\\ \hline 
         TIA-like*&   72&106 &2.9\\ \hline
 Nexus Machine& 137&191&4.7\\\hline
    \end{tabular}
    }
    %\vspace{-.2cm}
    \begin{flushleft}
    \tiny{*These numbers are specific to the evaluated irregular workloads and may differ from those reported in other works.}
    \end{flushleft}
    %\vspace{-.1cm}
    \caption{Power and Area Efficiency Comparisons for different architectures}
    \label{tab:power_efficiency}
\end{table}

%{\color{blue}\subsection{Scalability}
%}

\subsection{Discussion: Generality at the expense of configuration memory}
For the \textit{Nexus Machine}, the flexibility and generality come at the cost of storing all instructions within every PE.
While most of the irregular kernels evaluated typically require only a few configurations, this can be challenging for bigger workloads. 
One optimization for bigger workloads is to limit half of the instructions to alternate PEs and the remaining instructions to the others.
This approach restricts PE flexibility but improves configuration memory efficiency.
We evaluated this approach using \textit{PageRank}, where \textit{Nexus Machine} showed superior performance by achieving 48\% fabric utilization, compared to 15\% for \textit{generic CGRAs} and 40\% for \textit{TIA-like}.