\section{Background and Motivation}
\label{section:motivation}
\subsection{Active Messages and Task Migration}
%{\color{red}\begin{itemize}
%    \item GASNet library from LBL
%    \item PGAS languages
%    \item How is it different from RPC mechanisms?
%    \item Nexus Machine is inspired by Active Messages
%\end{itemize}}
%Peh outline: Idea of Active messages
\begin{comment}
Active Messages (AM)~\cite{am_culler} was proposed as a lightweight communication scheme that sends network messages from one machine to another, with the messages triggering the execution of user-defined functions upon arrival.
%These handlers have the capability to access memory, invoke arbitrary computations, and dispatch further Active Messages as a response.
%\textbf{Unlike traditional Remote Procedure Call (RPC) mechanisms, AMs were designed "not to perform computation on the data, but to extract the data from the network and integrate it into the ongoing computation with a small amount of work"~\cite{am_culler}.}
These lightweight functions have the capability to access memory, invoke various computations, and dispatch further Active Messages as a response.
The core concept behind Active Message revolves around the dispatching of computation to the data location rather than transferring data to the computation.
This effectively removes coherency overhead and minimizes data movement.
\end{comment}
Active Messages (AM)~\cite{am_culler} was proposed as a lightweight communication scheme that sends network messages from one machine to another, with messages triggering execution of user-defined functions upon arrival.
These lightweight functions can access memory, invoke computations, and dispatch further Active Messages as a response.
The core concept behind Active Message revolves around dispatching computation to the data location rather than transferring data to the computation.
This effectively removes coherency overhead and minimizes data movement.

AMs have been realized and implemented in a range of multi-computers in the past. In~\cite{am_culler}, a practical implementation of AM on CM5 and n-CUBE/2 architectures was presented.
The J-Machine~\cite{jmachine} is a notable parallel processor featuring comprehensive hardware and instruction support for active messages.
It facilitates the remote spawning of fine-grained tasks at a granularity of approximately 20 instructions.
Also, commercially available processors like the Cray T3D and its subsequent iterations have leveraged network designs similar to that of the J-Machine.
%GASNet~\cite{gasnet} offers a language-independent networking middleware layer, providing network-independent, high-performance communication primitives for Active Messages. 
%The GASNet API~\cite{gasnet_api} has emerged as the de facto communication standard in partitioned global address space (PGAS) systems, including LBNL UPC++~\cite{lbnl_upc} and Berkeley UPC~\cite{berkeley_upc}.

With \textit{Nexus Machine}, we propose a low-power reconfigurable architecture that is inspired by the Active Message model to target irregular workloads. 
Our definition of AM diverges significantly from the original concept in its application to reconfigurable architectures. 
First, we store the instruction itself within the message, rather than relying on a handler to trigger a set of instructions, to ensure immediate execution of the instruction upon arrival at a PE. 
%as is typically observed in prior AM multi-computer systems.
\textit{Nexus machine} also leverages the static compilation strategy of previous reconfigurable architectures, with the software compiler first intelligently placing instructions and data across the fabric.
Unlike previous architectures, where the data flows through statically placed instructions on PEs, \textit{Nexus Machine} reduces data movement by sending and executing instructions at the location where the data resides.

In the \textit{Nexus Machine} architecture, the entire execution is initiated and orchestrated by the data; thus we refer to it as \textbf{Data-Driven Execution}.
%Unlike prior multi-computer systems with high bandwidth networks between machines, \textit{Nexus Machine} employs previous reconfigurable architecture's lightweight Networks on Chip (NoCs) between simple ALUs, which facilitates highly frequent message passing to launch ALU operations, making it well-suited for the \textit{Data-Driven Execution} model.
Unlike prior multi-computer systems with high bandwidth networks between machines, \textit{Nexus Machine} employs lightweight Networks on Chip (NoCs) between simple ALUs, similar to previous reconfigurable architectures, which facilitates frequent message passing to initiate ALU operations, making the \textit{Nexus Machine} well-suited for the \textit{Data-Driven Execution} model.
% It provides a synchronization-free execution model with an instruction-level granularity.

\subsection{Challenges of Irregular Workloads on reconfigurable architectures}
\label{section:nexusmachine_motivation}
\noindent
\begin{figure}[t!]
	\scriptsize
	\centering	\includegraphics[width=0.9\columnwidth]{diagrams/motivation.pdf}
    \vspace{-0.2cm}
	\caption{Sparse Matrix-Vector Multiplication (SpMV).} 
	\label{fig:motivation}
	%\vspace{-.5cm}
\end{figure}
\noindent
\begin{figure*}[h!]
	\scriptsize
	\centering
	\includegraphics[width=0.9\textwidth]{diagrams/motivating_example.pdf}
    \vspace{-0.25cm}
	\caption{Program execution comparison for SpMV kernel, illustrating two consecutive iterations with a bank conflict. (a) \textit{Generic CGRA}: Data flows through statically placed instructions (top) and bank conflicts across various banks for a real workload with n=2048 on a 4x4 PE array (bottom) (b) Triggered Instructions: Illustrates data-local execution with messages, invoking tasks at the location of data, reducing data movement (top) and visual representation of the load imbalance across the PE array (bottom) (c) \textit{Nexus Machine}: Enhances performance and PE utilization through a unique approach â€” enabling opportunistic execution and utilizing idle ALUs for en-route instruction execution (top) and visual representation of the uniform load balance across the PE array (bottom). Data movements are represented by {\color{red} red} arrows, while {\color{blue} blue} arrows depict message transfers.} 
    \label{fig:walkthrough}
    \vspace{-.3cm}
\end{figure*}
%\FloatBarrier
Current reconfigurable architectures excel in creating static software pipelining, where a task is divided into instruction-level pipeline stages, and gets mapped to the PEs across the fabric.
To execute tasks, data moves from one PE (producer) to the next PEs (consumers, data-dependent on the producer) along fixed (compile-time) pre-scheduled datapaths. 
This works well for regular workloads where the data access and control flow are predictable.

However, mapping irregular workloads on these architectures significantly impacts performance, fabric utilization, and energy consumption for data movement.
To illustrate this issue with an example, we use \textbf{Sparse Matrix Vector Multiplication (SpMV)} kernel that multiplies a sparse matrix \textit{matrix} with a dense vector \textit{vec}. 
Fig.~\ref{fig:motivation}(a) provides a pseudo-code for SpMV at the instruction level. The sparse matrix in Fig.~\ref{fig:motivation}(b) is stored in a commonly used compressed sparse row (CSR) format to encode the row and column indices of the non-zero elements. \textit{rowptr} points to the first non-zero element information for each row in the \textit{col} vector, which in turn stores the column indices of all the non-zero elements for each row consecutively.
SpMV is an irregular workload due to its use of multiple levels of indirection to access the data: \textit{rowptr} to access \textit{col}, which in turn helps access a specific index in \textit{vec}.

Fig.~\ref{fig:walkthrough}(a) (top) shows the mapping of SpMV on \textit{generic CGRA} architecture that has a global memory shared across all the PEs. Here the operations are statically mapped onto the PEs and the routing path of the data between dependent PEs are also fixed (red arrows).
%{\bf Peh: You need to briefly explain how conventional CGRA does badly for this irregular example. Then point to cycle count for this specific example vs Triggered vs Nexus}
Further, to enhance fabric utilization, multiple iterations of the loop are unrolled and spatially mapped.
However, this approach exacerbates bank conflicts when dealing with irregular workloads. 
The architecture's demand for synchronized operation across all PEs in a predictable manner means that any bank conflict results in stalls.
For the SpMV workload mapped to a PE array comprising 16 PEs connected to 4 memory banks on one edge, Fig.~\ref{fig:walkthrough}(a) (bottom) illustrates the occurrences of conflicts across different memory banks.

To mitigate bank conflicts caused by irregular memory access patterns in global memory, we distribute the memory across PEs, enabling data-local execution that serializes memory accesses at each PE. 
To convert SpMV into a sequence of tasks suitable for data-local execution, we divide the code at each memory operation. This is illustrated for SpMV by the different colored segments in Fig.~\ref{fig:motivation}(a).
Task \textbf{T1} accesses the \textit{matrix} array (represented by \textit{rowptr} and \textit{col}), \textbf{T2} accesses elements in the \textit{vec} array, and \textbf{T3} executed on the corresponding elements of the \textit{output} array.
During the compilation phase, \textit{Nexus Machine} restructures the CSR arrays of the matrix into AM entries (see Section ~\ref{section:compiler}). %\textcolor{red}{can you show an example of AM entry for CSR?}
Each entry consolidates the matrix data and the locations of vector and output elements, as specified by \textit{rowptr} and \textit{col} arrays. 
These entries help guide the messages as they traverse the architecture.
In the \textit{Nexus Machine}, dynamic routing introduces network congestion analogous to the bank conflicts in global memory CGRAs, as message routing depends on congestion levels. 

The architecture baseline we use for data-local execution resembles that of Triggered Instructions~\cite{tia} with multiple PEs, where each PE includes a distributed data memory, ALU, and a configuration memory with a scheduler. The tasks are anchored with the data, triggered when messages arrive with the data. 
Fig.~\ref{fig:walkthrough}(b) (top) illustrates how our transformed SpMV can be executed on such an architecture.
\textbf{T1} performs local load for the \textit{matrix} element and calculates the address for the \textit{vec} element.
When the message reaches the PE with the \textit{vec} element, \textbf{T2} executes a local load of \textit{vec} element and multiplies it with the \textit{matrix} element obtained from the remote PE.
Finally, when the message reaches the PE with the \textit{output} element, \textbf{T3} performs a local aggregation.
While our transformed SpMV workload can be mapped to such an architecture, it still faces challenges in terms of performance and resource utilization due to severe load imbalances. This is because the instructions are anchored with the data, and cannot move at run time to different PEs.
For SpMV, these imbalances are evident by the varying numbers of elements aggregated for each \textit{output}.
Fig.~\ref{fig:walkthrough}(b) (bottom)  represents the load imbalance observed within a PE array with 16 PEs.

\textit{Nexus Machine} effectively addresses the low utilization and performance challenges posed by irregular workloads by opportunistically executing active messages on the idle PEs en-route.
It does this by exploiting the AM paradigm that allows the transmission of not just data operands, but also {\bf instructions} within a single message.
%The architecture adopts \textbf{Lazy Execution}, a strategy that postpones the execution of instructions until their results are genuinely needed, and performs executions on idle PEs encountered along its route or alternatively, at the destination PE if no idle PEs are available.
The architecture employs \textbf{Opportunistic Execution}, a strategy that postpones execution along the datapath instead of executing immediately on the source PE. Operations are executed on the first idle PE encountered along the route, or alternatively, on the destination PE if no idle PEs are available.
In Fig.~\ref{fig:walkthrough}(c) (top), we illustrate how \textit{Nexus Machine} applies this approach to execute the SpMV kernel.
Task \textbf{T1} initiates the local access of the \textit{matrix} element but forwards the computation into the network, where it is executed on the first available idle PE. 
This generates an AM response with Task T2.
%{\color{red}{if you are executing on first available PE, then why is it lazy execution that postpones as long as possible. Change the defition of lazy execution to match this}}
Similarly, task \textbf{T2} is sent into the network with the corresponding \textit{matrix} and \textit{vec} elements and is computed en-route, resulting in another AM response with Task T3.
Finally, task \textbf{T3} performs a local aggregation of the \textit{output} element. 
We refer to this as \textbf{In-Network Computing} for dataflow architectures.
Fig.~\ref{fig:walkthrough}(c) (bottom) visually demonstrates how \textit{Nexus Machine}'s coupling of compiler data placement with runtime distribution of instructions across the fabric enables uniform load balance across the PEs, significantly enhancing utilization and performance.
%{\bf Peh: show difference in execution times between the 3 -- how many cycles for Conventional vs Triggered vs. Nexus}