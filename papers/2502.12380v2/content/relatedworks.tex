%\vspace{-0.5cm}
\section{Related Works}
\subsection{CGRA support for irregularity} 
\label{section: related_works}
\begin{comment}
\textit{Generic CGRA} architectures exhibit inherent inefficiencies in handling irregular workloads with unpredictable computational patterns. Previous studies have explored various approaches to adapt CGRAs for handling irregularity and sparsity. We classify prior works into those targeting server scale, and hence are faced with memory bandwidth challenges and much coarser task granularity parallelism; vs. the edge devices that the \textit{Nexus Machine} targets which require much finer granularity parallelism to be leveraged with ultra-lightweight hardware mechanisms.
\end{comment}

\textit{Generic CGRA} architectures are inefficient at handling irregular workloads with unpredictable computational patterns. Prior studies propose adaptations for irregularity and sparsity, classifying them into server-scale solutions, addressing memory bandwidth and coarse task parallelism, and edge-device solutions like \textit{Nexus Machine}, which leverage finer-granularity parallelism with ultra-lightweight hardware mechanisms.

\textbf{Server-Scale:}
\begin{comment}
%Fifer~\cite{fifer} facilitates the execution of irregular workloads on generic CGRAs by implementing dynamic temporal pipelining, thereby enhancing load balancing. This involves separating the fetching and control of irregular workloads, utilizing time multiplexing to execute different stages of the irregular workload on the same CGRA fabric, and employing queues to decouple these stages.
Fifer~\cite{fifer} enables irregular workload execution on \textit{generic CGRAs} by decomposing them into regular stages. It employs queues to decouple these stages, enabling different stages to run on the same fabric through time multiplexing, thereby optimizing fabric utilization.
%While this approach improves performance, it is highly energy inefficient for edge CGRAs due to excessive data movement and limited memory bandwidth.
% Fifer offers this dynamic temporal pipelining on top of static CGRAs as a way to achieve effective load balancing but remains highly energy inefficient for edge CGRAs due to excessive data movement and limited memory bandwidth. 
Fifer's approach remains highly inefficient for the edge because of the requirement for inter and intra-PE queues, frequent reconfiguration, and high data movement costs.
 
%Fifer decouples the irregular fetching and control of irregular kernels, which allows for pipeline parallelism. This is achieved by dividing irregular kernels into different stages and time-multiplexing these stages using pipeline FIFOs onto the same CGRA fabric. Fifer introduces additional logic to decouple these irregular accesses, but it still suffers from non-trivial context-switching overhead between stages.

 %Capstan employs sparse-iteration and co-iteration primitives on its input tensors to optimize algorithmic aspects of sparse dataflows on its reconfigurable architecture. These primitives are accompanied by supplementary hardware components designed to leverage sparsity and enhance parallelism
 %Stardust offers a compiler framework designed to meet the declarative programming model prerequisites of Capstan. It also manages the partitioning of data that can be accommodated on-chip and orchestrates the requisite data transfers, aligning with Capstan's architectural demands.

Capstan~\cite{capstan} is constructed with a checkerboard grid of programmable compute and local memory units, supplemented by specialized scanners
%bit-manipulation hardware 
and address generators designed for external memory accesses. The programming paradigm 
%for these units 
involves utilizing patterns that define the sparse dataflow required for processing. To enhance the efficiency and parallelism of algorithms
%on its reconfigurable architecture
, Capstan employs distinctive sparse-iteration and co-iteration techniques on its input tensors. Capstan has significant additional hardware for various data and metadata manipulation and requires expensive shuffle networks for its PEs to interface with the SRAM as well as external memory.

Extensor~\cite{extensor} leverages hierarchical tensor intersections in an inner-product flow, capitalizing data sparsity at various levels of granularity.  However, Extensor's dataflow approach, while efficient, comes at the cost of reduced programmability, limiting its capability to handle a broad range of sparse tensor operations efficiently.
\iffalse
Dalorex~\cite{dalorex} employs an execution model that 
splits loops into tasks at every pointer indirection. It distributes data uniformly across tiles enabling local execution of tasks in proximity to the corresponding data. The tiles are organized in a 2D torus topology with ruche network~\cite{ruche} where each tile has an in-order core with a scratchpad, a scheduling unit, and a router.
%associated with it. 
Parent tasks on completion spawn the next task's parameters into the NoC.
%with the first flit here serving to encode the destination Tile ID. 
%Dalorex employs Ruche networks~\cite{ruche} that enable tiles to establish a connection to other tiles apart from their nearest neighbors thereby maintaining high bandwidth when scaling up. 
\textit{Nexus Machine}'s in-network computation realizes better load balancing as it is able to leverage idle PEs on intermediate PEs en-route. Most critically, as Dalorex is architected for servers, it functions at task level granularity with entire tasks executed near data tiles, while \textit{Nexus Machine} has a much finer control at the instruction level granularity,
%with a lightweight hardware microarchitecture 
suitable for edge devices.
\fi

Dalorex~\cite{dalorex} employs a task-based execution model, distributing data uniformly across tiles for localized execution. Tiles are arranged in a 2D torus topology with ruche network~\cite{ruche}, each comprising an in-order core with a scratchpad, scheduling unit, and router. Upon completion, parent tasks spawns parameters for subsequent tasks into the NoC.
\textit{Nexus Machine} realizes better load balancing through in-network computation, leveraging idle PEs en-route. While Dalorex operates at task-level granularity, suited for servers, \textit{Nexus Machine} offers finer control at the instruction level, ideal for edge devices.
\end{comment}
Fifer~\cite{fifer} enables irregular workloads on \textit{generic CGRAs} by decomposing them into regular stages, using queues to decouple and time-multiplex stages for better fabric utilization. However, it is inefficient for edge due to inter/intra-PE queues, frequent reconfiguration, and high data movement costs.
Capstan~\cite{capstan} features a checkerboard grid of compute and local memory units, with scanners and address generators for external memory. It uses sparse dataflow patterns and employs sparse-iteration techniques for efficiency and parallelism. Capstan requires additional hardware for data/metadata manipulation and expensive shuffle networks for PEs to access memory.
Extensor~\cite{extensor} employs hierarchical tensor intersections in an inner-product flow, exploiting data sparsity but sacrifices programmability, limiting its ability to handle diverse sparse tensor operations efficiently.
Dalorex~\cite{dalorex} uses a task-based model with uniform data distribution across tiles arranged in a 2D torus topology with a ruche network~\cite{ruche}. Each tile includes an in-order core, scratchpad, scheduler, and router. Parent tasks spawn new tasks into the NoC upon completion.
\textit{Nexus Machine} balances loads with in-network computation, using idle PEs en-route. While Dalorex works at task-level granularity, ideal for servers, \textit{Nexus Machine} offers instruction-level control for edge devices.
%{\color{blue} Arena~\cite{arena} introduces a scalable asynchronous accelerator-ring architecture employing CGRA for data-driven high-performance computing. By co-designing the architecture and programming model, Arena facilitates the relocation of computation tasks to the data through the reconfiguration of CGRA into specialized accelerators.}
\textbf{Edge:}
Ultra-Elastic CGRA~\cite{uecgra} accelerates inter-iteration loop dependencies in irregular loops by exploiting dynamic voltage and frequency scaling (DVFS) at the granularity of PEs. However, its scope remains constrained by the range of supported workloads.
SPU~\cite{dgra} uses a stream dataflow model on a reconfigurable fabric composed of decomposable memories, switches, and compute units that split coarse-grained resources into finer-grained resources on the fabric. This enables flexibility in exploiting common stream-join and indirection data-dependence forms in irregular workloads.
% Pipestitch~\cite{pipestitch} {\color{red} Pranav to rewrite this} encompasses a statically configured, multi-hop, bufferless, 2D torus network comprising heterogeneous PEs with varied functional units. A micro-core within the PEs facilitates their interaction with the NoC.
% Pipestitch transforms each iteration of annotated, parallelizable loops into in-order, lightweight dataflow threads. The irregular inner loops of these threads are pipelined, and operations from different threads execute in parallel on different PEs within the fabric. The order for spawning of new threads or continuation of ongoing threads within the loop is managed through a dedicated synchronization network. Further, given the bufferless NoC, their PEs need to be provisioned with additional buffers needed in cases where data corresponding to a specific instruction arrives at different times to reduce cascading pipeline stalls. To accommodate diverse workloads with distinct threading requirements, Pipestitch selectively puts control flow on either their specialized NoC routers or dedicated control flow PEs. These dedicated control flow PEs increase the total number of PEs over conventional CGRAs while still suffering from high data movement costs. 
\iffalse
Pipestitch~\cite{pipestitch} is a CGRA that enables efficient and flexible computation for sparse ML and DSP workloads in energy-minimal sensor systems. Pipestitch transforms each iteration of annotated, parallelizable loops into in-order, pipelined dataflow threads. 
% The irregular inner loops of these threads are pipelined, enabling operations from various stages across different threads to execute concurrently on the fabric.
The architecture introduces a dispatch operator mapped to specialized control flow PEs forming a synchronization network to ensure ordered-dataflow execution.
% This operator, along with a synchronization network, handles thread spawning and continuation.
%It features a statically configured, multi-hop, bufferless 2D torus network comprising heterogeneous PEs with various functional units. 
% Each PE contains a micro-core that facilitates interaction with the NoC. 
%Pipestitch selectively assigns its control flow to specialized NoC routers (without threads) or dedicated control flow PEs (for thread dispatch) to handle diverse workloads with unique threading requirements. 
These extra dedicated control flow PEs lead to higher PE requirements vs. \textit{generic CGRAs} while still incurring high data movement.
\fi
Pipestitch~\cite{pipestitch} is a CGRA tailored for energy-efficient computation in sensor systems handling sparse ML and DSP tasks. 
It optimizes performance by converting annotated, parallelizable loops into ordered, pipelined dataflow threads.
The design incorporates a dispatch operator that utilizes specialized control flow PEs, forming a synchronization network for ordered-dataflow execution. 
The architecture demands more PEs compared to \textit{generic CGRAs}, while incurring higher data movement.

Table~\ref{tab:sota_comparison} presents a comparison between \textit{Nexus Machine} and state-of-the-art edge CGRAs in terms of power, throughput, and power efficiency.
To ensure fairness, we put our best effort into comparing them, considering varied details among different sources.
\textit{Nexus Machine} showcases a better power efficiency of 194MOPS/mW compared to the SOTA CGRAs.
%\textcolor{blue}{\bf Peh: Refer to Table and summarize Nexus vs. Pipestitch and Triggered}

\begin{table}[t!]
    \centering
    \resizebox{\columnwidth}{!} {
    \begin{tabular}{|c|c|c|c|c|} \hline  
         &  \textbf{UE-CGRA}*&  \textbf{Pipestitch}*&\textbf{Triggered} & {\cellcolor[HTML]{C9C0BB}}{\textbf{Nexus}}\\ 
 & ~\cite{uecgra}& ~\cite{pipestitch}& \textbf{Instructions}**~\cite{tia}&{\cellcolor[HTML]{C9C0BB}}{\textbf{Machine}}\\ \hline  
         \textbf{Tech node}&  TSMC 28&  Sub-28&FDSOI 22& {\cellcolor[HTML]{C9C0BB}}{FDSOI 22}\\ \hline  
         \textbf{Setup}&  Post P\&R&  Post Synthesis&Post Synthesis& {\cellcolor[HTML]{C9C0BB}}{Post Synthesis}\\ \hline  
         \textbf{Frequency (MHz)}&  750&  50&100&{\cellcolor[HTML]{C9C0BB}}{100}\\ \hline
         \textbf{Power (mW)}&  14&  3.33&4.626& {\cellcolor[HTML]{C9C0BB}}{3.865}\\ \hline  
         \textbf{Peak Throughput}&  625&  558&490& {\cellcolor[HTML]{C9C0BB}}{748}\\ 
         \textbf{(MOPS)}& & &&{\cellcolor[HTML]{C9C0BB}}{}\\\hline  
         \textbf{Power Effici.}&  45&  167&106& {\cellcolor[HTML]{C9C0BB}}{194}\\  
         \textbf{(MOPS/mW)}& & &&{\cellcolor[HTML]{C9C0BB}}{}\\ \hline
    \end{tabular}
    }
    \begin{flushleft}
    \tiny{* Based on the latest values mentioned in Table III of RipTide~\cite{riptide} and approximations from Pipestitch~\cite{pipestitch} paper.\\
    ** Based on our implementation of the work.}
    \end{flushleft}
%[**] Average across all the benchmarks reported.
    \caption{\textit{Nexus Machine} compared to state-of-the-art CGRAs}
    \label{tab:sota_comparison}
\end{table}
\vspace{-0.25cm}
% Accelerators
\subsection{Sparse Accelerators}
\begin{comment}
\iffalse
{\color{red}
\begin{itemize}
    \item SparTen~\cite{sparten}: A Sparse Tensor Accelerator for Convolutional Neural Networks in MICRO 2019.
    \item Eyeriss v2~\cite{eyeriss}: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices in JETCAS 2019. 
    % Compilers
     \item The Sparse Abstract Machine~\cite{sam}. Hsu et al. ASPLOS 2023.
    \item TACO~\cite{taco}/MLIR~\cite{mlir} SparseTensor~\cite{sparsetensor}/SparseTIR~\cite{sparsetir} lines of work on compiling sparse tensor algebra?
\end{itemize}}
\fi
In recent literature, various Sparse Matrix accelerators have been proposed, each tailored for specific workloads.

SparTen~\cite{sparten} implements an optimized bitmap-based inner join operation between two sparse vectors, which gives them a significant performance benefit for SpMM. Further, they must employ an offline, software-based greedy balancing algorithm to avoid load imbalance in their architecture. 
%\textit{Nexus Machine} doesn't require any such software support for its execution.
Eyeriss v2~\cite{eyeriss} implements an inner-product architecture combined with a hierarchical mesh NoC to handle diversely sparse matrices. 

Stardust~\cite{stardust}, TACO~\cite{taco} provide a compiler framework designed to meet a Capstan-like architecture's declarative programming model requirements. It also handles data partitioning, and orchestrates essential data transfers, thus aligning with given architectural specifications.
Sparse Abstract Machine (SAM)~\cite{sam} introduces a stream-dataflow abstraction with generic sparse primitives, while SparseTIR~\cite{sparsetir} introduces and explores the search space of composable abstractions and transformations for sparse deep learning workloads. SAM decouples traversal over sparse tensors from its dataflows and hardware optimizations, allowing each component to undergo specialized optimizations. Thus, these abstractions serve as an intermediate representation interface, connecting sparse primitives and dataflow architectures.

EIE~\cite{eie} specializes in handling the fully connected layers of Convolutional Neural Networks (CNNs). However, it stores all of the matrix data in on-chip storage, limiting its scalability.
Conversely, SpAtten~\cite{spatten} designed for Natural Language Processing (NLP) tasks employs an adaptive approach involving real-time quantization, ranking, and pruning of tokens and attention heads within sparse attention layers found in transformer models.

SpArch~\cite{sparch} and OuterSPACE~\cite{outerspace} accelerate Sparse Matrix Multiplication (SpMSpM) using an outer-product dataflow. It encounters significant challenges due to the high cost associated with output partial sum merging. 
MatRaptor~\cite{matraptor} and GAMMA~\cite{gamma}, like the \textit{Nexus Machine} leverage a row-wise product dataflow to alleviate the high merging costs. 
GAMMA incorporates a specialized cache component to reduce off-chip data transfers and memory bottlenecks in sparse operations. 
Flexagon~\cite{flexagon} proposed an adaptive architecture combining multiple SpMSpM dataflow strategies for neural networks, dynamically reconfiguring its interconnects based on workload characteristics.

These architectures are highly specialized for their respective workloads. In contrast, \textit{Nexus Machine} offers a generic CGRA architecture, enabling efficient execution of irregular and regular workloads.
\end{comment}

Recent literature introduces Sparse Matrix accelerators tailored for specific workloads.
SparTen~\cite{sparten} optimizes bitmap-based inner join operations for SpMM but requires offline load balancing.
Stardust~\cite{stardust} and TACO~\cite{taco} provide compiler frameworks for Capstan-like architectures, managing data partitioning and transfers. Sparse Abstract Machine (SAM)\cite{sam} offers stream-dataflow abstractions with generic sparse primitives, while SparseTIR\cite{sparsetir} explores composable abstractions for sparse deep learning. SAM decouples tensor traversal from dataflows, enabling specialized optimizations as an intermediate representation.
EIE~\cite{eie} focuses on CNN fully connected layers but is constrained by on-chip storage scalability. SpAtten~\cite{spatten}, for NLP, applies real-time quantization, ranking, and pruning in transformer attention layers.
SpArch~\cite{sparch} and OuterSPACE~\cite{outerspace} use outer-product dataflows for SpMSpM but face high partial-sum merging costs. MatRaptor~\cite{matraptor} and GAMMA~\cite{gamma} reduce these costs with row-wise product dataflows, and GAMMA adds specialized caching to reduce off-chip transfers.
Flexagon~\cite{flexagon} dynamically reconfigures dataflows for neural networks based on workload.

While these architectures are workload-specific, \textit{Nexus Machine} provides a generic CGRA for efficient execution of both irregular and regular workloads.