\section{Introduction}
Edge devices require high performance within constrained power budgets. ASIC accelerators are efficient for specific applications, but reconfigurable computing offers a flexible alternative with lower engineering costs. A reconfigurable architecture consists of many processing elements (PEs) connected by an on-chip network. 
Field Programmable Gate Arrays (FPGAs) are the most successful reconfigurable architectures today; however they suffer from sub-optimal compute density and energy efficiency due to bit-level reconfigurability.
In contrast, modern reconfigurable architectures provide word-level reconfigurability, balancing adaptability and efficiency with significant research targeting both edge~\cite{adres, hycube, snafu, riptide} and server domains~\cite{fifer, plasticine, capstan, amber}.

A Coarse-Grained Reconfigurable Architecture (CGRA) uses simple PEs connected by a reconfigurable on-chip network to form high-throughput datapaths. Each PE contains an ALU, router, and reconfiguration memory. Data is loaded from a global scratchpad memory (SPM) to the PE array for execution, and results are stored back in the SPM. As shown in Fig.~\ref{fig:taxanomy}, CGRAs are classified into Spatial and Spatio-temporal~\cite{revel}. 
Spatial architectures~\cite{snafu, softbrain, piperench, riptide} maintain a fixed mapping of compute and communication, ensuring low configuration energy but potentially lower performance. 
Spatio-temporal~\cite{adres, hycube, trips, wavescalar}, on the other hand, allow each PE to reconfigure to a new instruction every cycle, enhancing performance but potentially increasing energy consumption~\cite{flex}. These architectures excel in accelerating regular workloads with predictable access patterns and minimal control flow, such as DSP workloads including dense linear algebra.
\begin{figure}[t!]
	\scriptsize
	\centering
	\includegraphics[width=\columnwidth]{diagrams/taxanomy.pdf}
    %\vspace{-0.2cm}
	\caption{Spatial vs Spatio-temporal}
	\label{fig:taxanomy}
\end{figure}

Critical workload domains for edge devices, such as edge AI, AR/VR, electric vehicles (EVs), gaming, and IoT, exhibit irregular computational kernels like sparse linear algebra and graph analytics.
These workloads become especially challenging with dynamic sparsity~\cite{pit}, such as activation and weight pruning in neural networks~\cite{teal_activationsparsity}, where both matrices are defined at runtime.
Current reconfigurable architectures struggle with irregular workloads, characterized by unpredictable memory accesses and complex control flow. These challenges arise because such dataflows are not well-suited for static compile-time instruction and data mapping techniques typically employed by these architectures. Irregular memory accesses lead to significant bank conflicts, limiting memory-level parallelism, while irregular control flow causes serious load imbalance, leaving most PEs idle at runtime. 

Recent works aim to tackle irregular workloads, but achieving high performance with energy efficiency remains challenging. Modern reconfigurable architectures like Triggered Instruction Architecture (TIA)~\cite{tia} leverage a data-local execution model for edge devices. Unlike CGRAs with centralized data memory, each PE in TIA has its own distributed memory and configuration memory. TIA operates by dispatching operands to a PE, triggering the loading of a statically placed instruction. These architectures rely on a runtime scheduler for tag matching and a priority encoder to trigger the next instruction, adding significant hardware overhead. Due to the irregular nature of workloads, these architectures face difficulties with load balancing and efficient fabric utilization.

\begin{figure}[h!]
	\scriptsize
	\centering
    \includegraphics[width=0.9\columnwidth]{diagrams/am_trigger.pdf}
	\caption{Active Message (AM) communication mechanism: An AM originating from PE A is launched at destination PE B, where it is executed on the ALU, and interacts with the data and configuration memory. Additional AMs may be generated in response, if necessary.}
	\label{fig:active_message}
\end{figure}
To address these challenges, we introduce \textit{Nexus Machine}, a novel reconfigurable architecture inspired by the Active Message paradigm. \textbf{Active Message (AM)}~\cite{am_culler} is a communication mechanism that enables flexible and efficient task execution by leveraging the proximity of tasks to relevant data. As shown in Fig.~\ref{fig:active_message}, each AM contains routing information, instructions for the target node, and data payload for arguments. An AM originating from PE A traverses the on-chip network to reach PE B, where it launches computation on the ALU, interacting with data and configuration memory. PE B can respond by generating additional AMs.

\textit{Nexus Machine} efficiently executes irregular workloads by leveraging the Active Message paradigm, addressing irregular memory accesses and load balancing. By distributing sparse tensors across the fabric and performing data-local execution, \textit{Nexus Machine} avoids expensive bank conflicts and enhances memory access. It provides a specialized compiler framework and hardware for \textit{Data-driven execution} of sparse tensors, optimizing memory accesses. To improve load distribution and system efficiency, \textit{Nexus Machine} introduces a novel load-balancing strategy for In-Network execution of AMs, dynamically deploying and executing AMs en-route.

\begin{table}[]
    \centering
    \resizebox{0.95\columnwidth}{!} {
    \begin{tabular}{|c|c|c|c|c|l}\hline
        \textbf{Platform} & \textbf{Programmability} & \textbf{Flexibility} & \textbf{Reconfigurability}  &\textbf{Irregular} \\
 & & & &\textbf{Workloads}\\ \hline
         ASIC&  Low/None&  Low& N.A. &High\\ \hline
         FPGA&  {Medium/High}&  High& Gate (bitstream) &Low\\ \hline
         CPU&  High&  Medium& Instruction (cycle) &Low\\ \hline
         GPU&  High&  Medium& Instruction (cycle) &Low\\ \hline
         CGRA/ TIA&  {Medium}&  Medium& Instruction (cycle) &Low\\ \hline
         {\cellcolor[HTML]{C9C0BB}}{Nexus}&  {\cellcolor[HTML]{C9C0BB}}{Medium}&  {\cellcolor[HTML]{C9C0BB}}{Medium} & {\cellcolor[HTML]{C9C0BB}}{Instruction (cycle)} &{\cellcolor[HTML]{C9C0BB}}{High}\\
 {\cellcolor[HTML]{C9C0BB}}{Machine}& {\cellcolor[HTML]{C9C0BB}}& {\cellcolor[HTML]{C9C0BB}}&{\cellcolor[HTML]{C9C0BB}} & {\cellcolor[HTML]{C9C0BB}}\\ \hline
    \end{tabular}
    }
    \caption{\textit{Nexus Machine} compared to contemporary platforms in terms of programmability, hardware flexibility, reconfigurability (reconfiguration time) and their ability to handle irregular workloads efficiently.}
    \label{tab:platform_comparison}
\end{table}
Table~\ref{tab:platform_comparison} compares \textit{Nexus Machine} with conventional platforms based on: %programmability, flexibility, reconfigurability, and handling irregular workloads.
\begin{itemize} 
\item \textbf{Programmability}: Providing high-level software abstractions that are familiar to end-users and domain experts, facilitating easier adoption.
\item \textbf{Hardware Flexibility}: The ability to adapt to various workloads and evolving algorithms.
\item \textbf{Reconfigurability}: The capability to modify hardware to achieve efficiencies close to those of ASICs.
\item \textbf{Support for Irregular Applications}: The ability of the hardware to efficiently manage irregular memory access and control flow.
\end{itemize}
\begin{comment}
CPUs and GPUs, despite their energy and area costs compared to ASIC designs, are favored due to their flexibility and simplified programming methods. While GPUs excel with minimal control divergence, optimizing their performance with irregular workloads remains an ongoing research area~\cite{irregular_gpu, irregular_gpu1}. Unlike FPGAs that reconfigure at the gate level with a bitstream, \textit{Nexus Machine} reconfigures its ALUs instruction-by-instruction each cycle, resulting in faster reconfiguration time.
{\color{blue}
Additionally, unlike some previous CGRAs~\cite{dynamic_ii} that incorporate LUTs with ALUs within their PEs for finer control, \textit{Nexus Machine} does not use any LUTs.
}
\end{comment}
CPUs and GPUs, though less efficient than ASIC designs, are preferred for their flexibility and easier programming. GPUs perform well with minimal control divergence, but optimizing them for irregular workloads remains an active research area~\cite{irregular_gpu,irregular_gpu1}. Similarly, High-Level Synthesis (HLS) for FPGAs is optimized for regular patterns, limiting its effectiveness for irregular workloads, which is a key focus of ongoing research aimed at improving FPGA utilization for these workloads~\cite{irregular_fpga,irregular_fpga2}. Unlike FPGAs, which reconfigure at the gate level with a bitstream, \textit{Nexus Machine} reconfigures its ALUs instruction-by-instruction each cycle, offering faster reconfiguration times. Additionally, unlike some previous CGRAs~\cite{dynamic_ii} that use LUTs with ALUs within their PEs for finer control, \textit{Nexus Machine} does not use LUTs.

Our contributions include:
\begin{itemize}
    \item A detailed microarchitecture and compiler framework for \textit{Nexus Machine}, showcasing its applicability in sparse tensor computations.
    \item An exhaustive exploration of \textit{Nexus Machine}'s architectural parameters through a cycle-accurate simulator, covering diverse workloads.
    \item Implementation of \textit{Nexus Machine} in an industrial FDSOI 22nm process with compiled memories, outperforming a \textit{generic CGRA} baseline by 1.7x and achieving 1.9x better fabric utilization.
\end{itemize}