\begin{table}[t]
    \centering
    \scriptsize
    \caption{We limit Jumbo's parameter increase by sharing Jumbo weights across layers. Separately, we let the Jumbo layers differ via layer-specific LoRAs \cite{hu2022lora}. Both methods effectively control Jumbo's parameter increase while maintaining strong performance. We measure throughput/memory on an RTX 4090 GPU with a $512$ batch size; we train and test models on ImageNet-21K.}
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{lcccc} 
        \toprule
        Architecture & \makecell{Throughput\\imgs/s} & \makecell{Params\\M} & \makecell{Memory\\GB} & \makecell{Top-1 Acc.\\$\%$} \\
         \midrule
            ViT+Registers, $R$=$16$ & 8.4K & 25.7 & 3.47 & 41.48 \\
            ViT+Jumbo, $J$=$6$ & 8.0K & 555.6 & 5.39 & 44.95 \\
            ViT+Jumbo, layer share, $J$=$6$ & 8.2K & 88.3 & 3.76 & 44.61 \\
            ViT+Jumbo+LoRA, $J$=$6$, rank=$8$ & 8.1K & 88.8 & 3.95 & 44.94 \\
            ViT+Jumbo+LoRA, $J$=$6$, rank=$32$ & 8.1K & 90.1 & 3.64 & 44.97 \\
         \bottomrule
    \end{tabular}
    \label{tab:bonus}
\end{table}