@inproceedings{
darcet2024vision,
title={Vision Transformers Need Registers},
author={Timoth{\'e}e Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=2dnO3LLiJ1}
}

@inproceedings{cai2023efficientvit,
  title={Efficientvit: Lightweight multi-scale attention for high-resolution dense prediction},
  author={Cai, Han and Li, Junyan and Hu, Muyan and Gan, Chuang and Han, Song},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17302--17313},
  year={2023}
}

@inproceedings{yun2024shvit,
  author={Yun, Seokju and Ro, Youngmin},
  title={SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={5756--5767},
  year={2024}
}

@inproceedings{qin2025mobilenetv4,
  title={MobileNetV4: Universal Models for the Mobile Ecosystem},
  author={Qin, Danfeng and Leichner, Chas and Delakis, Manolis and Fornoni, Marco and Luo, Shixin and Yang, Fan and Wang, Weijun and Banbury, Colby and Ye, Chengxi and Akin, Berkin and others},
  booktitle={European Conference on Computer Vision},
  pages={78--96},
  year={2025},
  organization={Springer}
}

@misc{lee2024efficientvimefficientvisionmamba,
      title={EfficientViM: Efficient Vision Mamba with Hidden State Mixer based State Space Duality}, 
      author={Sanghyeok Lee and Joonmyung Choi and Hyunwoo J. Kim},
      year={2024},
      eprint={2411.15241},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.15241}, 
}

@inproceedings{ryali2023hiera,
  title={Hiera: A hierarchical vision transformer without the bells-and-whistles},
  author={Ryali, Chaitanya and Hu, Yuan-Ting and Bolya, Daniel and Wei, Chen and Fan, Haoqi and Huang, Po-Yao and Aggarwal, Vaibhav and Chowdhury, Arkabandhu and Poursaeed, Omid and Hoffman, Judy and others},
  booktitle={International Conference on Machine Learning},
  pages={29441--29454},
  year={2023},
  organization={PMLR}
}

@inproceedings{
    dosovitskiy2021an,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@article{akbari2021vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24206--24221},
  year={2021}
}

@inproceedings{beyer2023flexivit,
  title={Flexivit: One model for all patch sizes},
  author={Beyer, Lucas and Izmailov, Pavel and Kolesnikov, Alexander and Caron, Mathilde and Kornblith, Simon and Zhai, Xiaohua and Minderer, Matthias and Tschannen, Michael and Alabdulmohsin, Ibrahim and Pavetic, Filip},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14496--14506},
  year={2023}
}

@inproceedings{
    liu2024mspe,
    title={{MSPE}: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution},
    author={Wenzhuo Liu and Fei Zhu and Shijie Ma and Cheng-Lin Liu},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=9Q9UiAyV40}
}

@inproceedings{
    fuller2024lookhere,
    title={LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate},
    author={Anthony Fuller and Daniel Kyrollos and Yousef Yassin and James R Green},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=o7DOGbZeyP}
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12104--12113},
  year={2022}
}

@inproceedings{dou2022empirical,
  title={An empirical study of training end-to-end vision-and-language transformers},
  author={Dou, Zi-Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Zhang, Pengchuan and Yuan, Lu and Peng, Nanyun and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18166--18176},
  year={2022}
}

@article{xu2023multimodal,
  title={Multimodal learning with transformers: A survey},
  author={Xu, Peng and Zhu, Xiatian and Clifton, David A},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={10},
  pages={12113--12132},
  year={2023},
  publisher={IEEE}
}

@inproceedings{touvron2022deit,
  title={Deit iii: Revenge of the vit},
  author={Touvron, Hugo and Cord, Matthieu and J{\'e}gou, Herv{\'e}},
  booktitle={European conference on computer vision},
  pages={516--533},
  year={2022},
  organization={Springer}
}

@inproceedings{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  booktitle={International Conference on Machine Learning},
  pages={7480--7512},
  year={2023},
  organization={PMLR}
}

@inproceedings{li2022exploring,
  title={Exploring plain vision transformer backbones for object detection},
  author={Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
  booktitle={European conference on computer vision},
  pages={280--296},
  year={2022},
  organization={Springer}
}

@inproceedings{liu2025grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others},
  booktitle={European Conference on Computer Vision},
  pages={38--55},
  year={2025},
  organization={Springer}
}

@article{li2024transformer,
  title={Transformer-based visual segmentation: A survey},
  author={Li, Xiangtai and Ding, Henghui and Yuan, Haobo and Zhang, Wenwei and Pang, Jiangmiao and Cheng, Guangliang and Chen, Kai and Liu, Ziwei and Loy, Chen Change},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{hatamizadeh2023fastervit,
  title={FasterViT: Fast Vision Transformers with Hierarchical Attention},
  author={Hatamizadeh, Ali and Heinrich, Greg and Yin, Hongxu and Tao, Andrew and Alvarez, Jose M and Kautz, Jan and Molchanov, Pavlo},
  journal={arXiv preprint arXiv:2306.06189},
  year={2023}
}

@inproceedings{vasufastvit2023,
  author = {Pavan Kumar Anasosalu Vasu and James Gabriel and Jeff Zhu and Oncel Tuzel and Anurag Ranjan},
  title = {FastViT:  A Fast Hybrid Vision Transformer using Structural Reparameterization},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year = {2023}
}

@article{mehta2021mobilevit,
  title={Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer},
  author={Mehta, Sachin and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:2110.02178},
  year={2021}
}

@article{mehta2022separable,
  title={Separable self-attention for mobile vision transformers},
  author={Mehta, Sachin and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:2206.02680},
  year={2022}
}

@inproceedings{liu2023patchdropout,
  title={Patchdropout: Economizing vision transformers using patch dropout},
  author={Liu, Yue and Matsoukas, Christos and Strand, Fredrik and Azizpour, Hossein and Smith, Kevin},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3953--3962},
  year={2023}
}

@inproceedings{assran2023self,
  title={Self-supervised learning from images with a joint-embedding predictive architecture},
  author={Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15619--15629},
  year={2023}
}

@article{garrido2024learning,
  title={Learning and leveraging world models in visual representation learning},
  author={Garrido, Quentin and Assran, Mahmoud and Ballas, Nicolas and Bardes, Adrien and Najman, Laurent and LeCun, Yann},
  journal={arXiv preprint arXiv:2403.00504},
  year={2024}
}

@inproceedings{wei2025towards,
  title={Towards latent masked image modeling for self-supervised visual representation learning},
  author={Wei, Yibing and Gupta, Abhinav and Morgado, Pedro},
  booktitle={European Conference on Computer Vision},
  pages={1--17},
  year={2025},
  organization={Springer}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{shah2024flashattention,
  title={Flashattention-3: Fast and accurate attention with asynchrony and low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024}
}

@inproceedings{beyer2022knowledge,
  title={Knowledge distillation: A good teacher is patient and consistent},
  author={Beyer, Lucas and Zhai, Xiaohua and Royer, Am{\'e}lie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10925--10934},
  year={2022}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{touvron2019fixing,
  title={Fixing the train-test resolution discrepancy},
  author={Touvron, Hugo and Vedaldi, Andrea and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{beyer2020we,
  title={Are we done with imagenet?},
  author={Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, A{\"a}ron van den},
  journal={arXiv preprint arXiv:2006.07159},
  year={2020}
}

@inproceedings{recht2019imagenet,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={International conference on machine learning},
  pages={5389--5400},
  year={2019},
  organization={PMLR}
}

@inproceedings{hendrycks2021many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8340--8349},
  year={2021}
}

@InProceedings{pmlr-v235-rolf24a,
  title = 	 {Position: Mission Critical – Satellite Data is a Distinct Modality in Machine Learning},
  author =       {Rolf, Esther and Klemmer, Konstantin and Robinson, Caleb and Kerner, Hannah},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {42691--42706},
  year = 	 {2024},
}

@article{muhammad2020deep,
  title={Deep learning for safe autonomous driving: Current challenges and future directions},
  author={Muhammad, Khan and Ullah, Amin and Lloret, Jaime and Del Ser, Javier and de Albuquerque, Victor Hugo C},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={22},
  number={7},
  pages={4316--4336},
  year={2020},
  publisher={IEEE}
}

@inproceedings{
    ridnik2021imagenetk,
    title={ImageNet-21K Pretraining for the Masses},
    author={Tal Ridnik and Emanuel Ben-Baruch and Asaf Noy and Lihi Zelnik-Manor},
    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
    year={2021},
    url={https://openreview.net/forum?id=Zkj_VcZ6ol}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@article{dehghani2024patch,
  title={Patch n’pack: Navit, a vision transformer for any aspect ratio and resolution},
  author={Dehghani, Mostafa and Mustafa, Basil and Djolonga, Josip and Heek, Jonathan and Minderer, Matthias and Caron, Mathilde and Steiner, Andreas and Puigcerver, Joan and Geirhos, Robert and Alabdulmohsin, Ibrahim M and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}

@inproceedings{howard2019searching,
  title={Searching for mobilenetv3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1314--1324},
  year={2019}
}

@inproceedings{han2020ghostnet,
  title={Ghostnet: More features from cheap operations},
  author={Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1580--1589},
  year={2020}
}

@inproceedings{tan2019mnasnet,
  title={Mnasnet: Platform-aware neural architecture search for mobile},
  author={Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2820--2828},
  year={2019}
}

@inproceedings{vasu2023mobileone,
  title={Mobileone: An improved one millisecond mobile backbone},
  author={Vasu, Pavan Kumar Anasosalu and Gabriel, James and Zhu, Jeff and Tuzel, Oncel and Ranjan, Anurag},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7907--7917},
  year={2023}
}

@inproceedings{li2023rethinking,
  title={Rethinking vision transformers for mobilenet size and speed},
  author={Li, Yanyu and Hu, Ju and Wen, Yang and Evangelidis, Georgios and Salahi, Kamyar and Wang, Yanzhi and Tulyakov, Sergey and Ren, Jian},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={16889--16900},
  year={2023}
}

@inproceedings{pan2022edgevits,
  title={Edgevits: Competing light-weight cnns on mobile devices with vision transformers},
  author={Pan, Junting and Bulat, Adrian and Tan, Fuwen and Zhu, Xiatian and Dudziak, Lukasz and Li, Hongsheng and Tzimiropoulos, Georgios and Martinez, Brais},
  booktitle={European Conference on Computer Vision},
  pages={294--311},
  year={2022},
  organization={Springer}
}

@inproceedings{chen2022mobile,
  title={Mobile-former: Bridging mobilenet and transformer},
  author={Chen, Yinpeng and Dai, Xiyang and Chen, Dongdong and Liu, Mengchen and Dong, Xiaoyi and Yuan, Lu and Liu, Zicheng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5270--5279},
  year={2022}
}

@article{li2022efficientformer,
  title={Efficientformer: Vision transformers at mobilenet speed},
  author={Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Ju and Evangelidis, Georgios and Tulyakov, Sergey and Wang, Yanzhi and Ren, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12934--12949},
  year={2022}
}

@inproceedings{weinzaepfel2023croco,
  title={CroCo v2: Improved cross-view completion pre-training for stereo matching and optical flow},
  author={Weinzaepfel, Philippe and Lucas, Thomas and Leroy, Vincent and Cabon, Yohann and Arora, Vaibhav and Br{\'e}gier, Romain and Csurka, Gabriela and Antsfeld, Leonid and Chidlovskii, Boris and Revaud, J{\'e}r{\^o}me},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17969--17980},
  year={2023}
}

@inproceedings{heo2025rotary,
  title={Rotary position embedding for vision transformer},
  author={Heo, Byeongho and Park, Song and Han, Dongyoon and Yun, Sangdoo},
  booktitle={European Conference on Computer Vision},
  pages={289--305},
  year={2025},
  organization={Springer}
}

@article{alabdulmohsin2024getting,
  title={Getting vit in shape: Scaling laws for compute-optimal model design},
  author={Alabdulmohsin, Ibrahim M and Zhai, Xiaohua and Kolesnikov, Alexander and Beyer, Lucas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{fan2021multiscale,
  title={Multiscale vision transformers},
  author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6824--6835},
  year={2021}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11976--11986},
  year={2022}
}

@inproceedings{Yuqietal-2023-PatchTST,
  title     = {A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},
  author    = {Nie, Yuqi and
               H. Nguyen, Nam and
               Sinthong, Phanwadee and 
               Kalagnanam, Jayant},
  booktitle = {International Conference on Learning Representations},
  year      = {2023}
}

@inproceedings{
    S3TimeSeries,
    title={Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations},
    author={Shivam Grover and Amin Jalali and Ali Etemad},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=zm1LcgRpHm}
}

@inproceedings{ZerveasTST,
author = {Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten},
title = {A Transformer-Based Framework for Multivariate Time Series Representation Learning},
year = {2021},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&; Data Mining},
pages = {2114–2124},
series = {KDD '21}
}

@inproceedings{ShapeFormer,
author = {Le, Xuan-May and Luo, Ling and Aickelin, Uwe and Tran, Minh-Tuan},
title = {ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671862},
doi = {10.1145/3637528.3671862},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1484–1494},
numpages = {11},
location = {Barcelona, Spain},
series = {KDD '24}
}

@article{UCRArchive,
  author       = {Hoang Anh Dau and
                  Anthony J. Bagnall and
                  Kaveh Kamgar and
                  Chin{-}Chia Michael Yeh and
                  Yan Zhu and
                  Shaghayegh Gharghabi and
                  Chotirat Ann Ratanamahatana and
                  Eamonn J. Keogh},
  title        = {The {UCR} Time Series Archive},
  year         = {2018},
  journal={arXiv preprint arXiv:1810.07758},
}

@misc{UEAArchive,
      title={The UEA multivariate time series classification archive}, 
      author={Anthony Bagnall and Hoang Anh Dau and Jason Lines and Michael Flynn and James Large and Aaron Bostrom and Paul Southam and Eamonn Keogh},
      year={2018},
      journal={arXiv preprint arXiv:1811.00075},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.00075}, 
}

@article{gelu,
  author       = {Dan Hendrycks and
                  Kevin Gimpel},
  title        = {Bridging Nonlinearities and Stochastic Regularizers with Gaussian
                  Error Linear Units},
  year         = {2016},
  journal={arXiv preprint arXiv:1606.08415},
}

@misc{dong2024hymbahybridheadarchitecturesmall,
      title={Hymba: A Hybrid-head Architecture for Small Language Models}, 
      author={Xin Dong and Yonggan Fu and Shizhe Diao and Wonmin Byeon and Zijia Chen and Ameya Sunil Mahabaleshwarkar and Shih-Yang Liu and Matthijs Van Keirsbilck and Min-Hung Chen and Yoshi Suhara and Yingyan Lin and Jan Kautz and Pavlo Molchanov},
      year={2024},
      eprint={2411.13676},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.13676}, 
}

@inproceedings{vaquero2024lost,
  title={Lost and found: Overcoming detector failures in online multi-object tracking},
  author={Vaquero, Lorenzo and Xu, Yihong and Alameda-Pineda, Xavier and Brea, V{\'\i}ctor M and Mucientes, Manuel},
  booktitle={European Conference on Computer Vision},
  pages={448--466},
  year={2024},
  organization={Springer}
}

@article{leigh2024tokenization,
  title={Is Tokenization Needed for Masked Particle Modelling?},
  author={Leigh, Matthew and Klein, Samuel and Charton, Fran{\c{c}}ois and Golling, Tobias and Heinrich, Lukas and Kagan, Michael and Ochoa, In{\^e}s and Osadchy, Margarita},
  journal={arXiv preprint arXiv:2409.12589},
  year={2024}
}

@article{messaoud2025towards,
  title={Towards Generalizable Trajectory Prediction Using Dual-Level Representation Learning And Adaptive Prompting},
  author={Messaoud, Kaouther and Cord, Matthieu and Alahi, Alexandre},
  journal={arXiv preprint arXiv:2501.04815},
  year={2025}
}

@article{hu2024fm,
  title={FM-TS: Flow Matching for Time Series Generation},
  author={Hu, Yang and Wang, Xiao and Wu, Lirong and Zhang, Huatian and Li, Stan Z and Wang, Sheng and Chen, Tianlong},
  journal={arXiv preprint arXiv:2411.07506},
  year={2024}
}

@article{thimonier2024t,
  title={T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data},
  author={Thimonier, Hugo and Costa, Jos{\'e} Lucas De Melo and Popineau, Fabrice and Rimmel, Arpad and Doan, Bich-Li{\^e}n},
  journal={arXiv preprint arXiv:2410.05016},
  year={2024}
}

@article{omranpour2024higher,
  title={Higher Order Transformers: Enhancing Stock Movement Prediction On Multimodal Time-Series Data},
  author={Omranpour, Soroush and Rabusseau, Guillaume and Rabbany, Reihaneh},
  journal={arXiv preprint arXiv:2412.10540},
  year={2024}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@inproceedings{
    shazeer2017,
    title={ Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
    author={Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=B1ckMDqlg}
}

@inproceedings{li2024lors,
  title={LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking},
  author={Li, Jialin and Nie, Qiang and Fu, Weifu and Lin, Yuhuan and Tao, Guangpin and Liu, Yong and Wang, Chengjie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15866--15876},
  year={2024}
}

@inproceedings{
    hu2022lora,
    title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
    author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{woo2023convnext,
  title={Convnext v2: Co-designing and scaling convnets with masked autoencoders},
  author={Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16133--16142},
  year={2023}
}

@inproceedings{choy20194d,
  title={4d spatio-temporal convnets: Minkowski convolutional neural networks},
  author={Choy, Christopher and Gwak, JunYoung and Savarese, Silvio},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3075--3084},
  year={2019}
}

@article{fu2024rethinking,
    title={Rethinking Patch Dependence for Masked Autoencoders}, 
    author={Letian Fu and Long Lian and Renhao Wang and Baifeng Shi and Xudong Wang and Adam Yala and Trevor Darrell and Alexei A. Efros and Ken Goldberg},
    journal={arXiv preprint arXiv:2401.14391},
    year={2024}
}

@inproceedings{fang2023unleashing,
  title={Unleashing vanilla vision transformer with masked image modeling for object detection},
  author={Fang, Yuxin and Yang, Shusheng and Wang, Shijie and Ge, Yixiao and Shan, Ying and Wang, Xinggang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6244--6253},
  year={2023}
}

@article{zhang2022segvit,
  title={Segvit: Semantic segmentation with plain vision transformers},
  author={Zhang, Bowen and Tian, Zhi and Tang, Quan and Chu, Xiangxiang and Wei, Xiaolin and Shen, Chunhua and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4971--4982},
  year={2022}
}

@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@misc{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{
    steiner2022how,
    title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
    author={Andreas Peter Steiner and Alexander Kolesnikov and Xiaohua Zhai and Ross Wightman and Jakob Uszkoreit and Lucas Beyer},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2022},
    url={https://openreview.net/forum?id=4nPswr1KcP},
    note={}
}

@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@article{cubuk2018autoaugment,
  title={Autoaugment: Learning augmentation policies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:1805.09501},
  year={2018}
}

@article{qiu2024compute,
    title={{Compute Better Spent: Replacing Dense Layers with Structured Matrices}},
  author={Qiu, Shikai and Potapczynski, Andres and Finzi, Marc and Goldblum, Micah and Wilson, Andrew Gordon},
    journal={International Conference on Machine Learning (ICML)},
    year={2024}
}