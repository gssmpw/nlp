\section{Background and Related Work}
\subsection{Vision Transformers}

A ViT splits an image into a grid of non-overlapping patches, $\mathbb{R}^{Y \times X \times C}\to \mathbb{R}^{N_y \times N_x \times P_y \times P_x \times C}$, where $C$ is the number of channels, $Y$ / $X$ are the image height / width, $N_y$ / $N_x$ are the grid height / width, and $P_y$ / $P_x$ are the patch height / width in pixels (equal to $\frac{Y}{N_y}$ / $\frac{X}{N_x}$). Next, they flatten the grid into a sequence and flatten the patches into vectors, $\mathbb{R}^{N_y \times N_x \times P_y \times P_x \times C}\to \mathbb{R}^{N \times D_{pix}}$, where $N$ is the number of patches (equal to $N_y \cdot N_x$), and $D_{pix}$ is the number of pixel values per patch (equal to $P_y \cdot P_x \cdot C$). Next, they apply a learnable linear projection to form patch embeddings, $\mathbb{R}^{N \times D_{pix}}\to \mathbb{R}^{N \times D}$, where $D$ is the model width, also known as the embedding dimension. Next, they add position embeddings to patch embeddings. (Superior and slightly more complex position encoding methods exist ____; however, we simply use learnable embeddings in this work --- although Jumbo is also compatible with all these other methods.) These operations produce local tokens $\mathbf{x}^{P}\in \mathbb{R}^{N \times D}$ that represent \emph{highly} local information --- typically a $16\times16$ px square. Crucially for this work, ViTs prepend a learnable \texttt{CLS} token $\mathbf{x}^{\texttt{CLS}}$ to the sequence of local tokens, $\mathbf{x} = \mathbf{x}^{\texttt{CLS}} \Vert_{0} \mathbf{x}^{P} \in \mathbb{R}^{(N+1) \times D}$, where $\Vert_{0}$ denotes concatenation along the $0^{\text{th}}$ (sequence) dimension. Finally, the input $\mathbf{x}$ is processed by a plain transformer and the --- now contextualized --- \texttt{CLS} token can be used for image classification.

\textbf{Sizes.} ViT sizes vary w.r.t. depth and width. The models we train all have $12$ layers, while the widths vary $\{96, ~128, ~192, ~384, ~768\}$, corresponding to names $\{$pico$, ~$nano$, ~$tiny$, ~$small$, ~$base$\}$. We name pico and nano sizes inspired by ____.

A standard image size of $224\times224$ px and a standard patch size of $16\times16$ px results in $196$ local tokens. A \emph{single} \texttt{CLS} token --- that is designed to aggregate global information to facilitate classification --- provisions $1/197^{\text{th}}$ of a model's representational capacity to global information (and this fraction decreases with larger images and/or smaller patches). This allocation seems suboptimal. Recent work finds evidence to support this intuition and proposes a fix.

\textbf{Registers.} ____ find that ViTs are so hungry for more \texttt{CLS} tokens that they \emph{learn} to repurpose some local patch tokens into ``pseudo-\texttt{CLS}'' tokens; these co-opted tokens behave like \texttt{CLS} tokens --- they aggregate global information and discard patch-specific local information. ____ propose a fix: prepend additional learnable tokens --- called registers $\mathbf{x}^{\texttt{Reg}}\in\mathbb{R}^{R \times D}$, where $R$ is the number of registers --- to the input sequence, $\mathbf{x} = \mathbf{x}^{\texttt{CLS}} \Vert_{0} \mathbf{x}^{\texttt{Reg}} \Vert_{0} \mathbf{x}^{P} \in \mathbb{R}^{(N+R+1) \times D}$. Registers improve performance and reduce attention map artifacts by provisioning more global capacity. 

Adding registers is elegantly simple to implement, and they preserve the advantages of plain ViTs; they can natively drop tokens and are compatible with SOTA self-supervised learning algorithms. In theory, registers can benefit any plain, noncausal transformer. These several advantages account for registers' \emph{significant and immediate} impact, including their application beyond image data ____. For these reasons, ViT+Registers is the primary inspiration behind our method, Jumbo.

\textbf{Hiera.} Our work aligns spiritually with Hiera ____, simplifying hierarchical transformers, such as Swin ____ or MViT ____, by replacing convolutions with pooling layers. Although hierarchical models are incompatible with token dropping out-of-the-box, ____ engineer a solution allowing Hiera to be pretrained with masked autoencoding ____. However, Hiera is not fast --- e.g., its fastest variant has $0.58 \times$ the throughput of our Jumbo ViT-small.

\subsection{Compute Efficient Architectures}\label{sec:compute-efficient}

We highlight $3$ architectures and use them as baselines for a high-speed ViT. \circlednum{1} EfficientViT ____ and \circlednum{2} SHViT ____ improve the efficiency of ViTs by incorporating efficient attention, pooling, and convolutional layers. \circlednum{3} MobileNetV4 ____ improves the efficiency of CNNs by leveraging many strategies (and different strategies for different model sizes). These baselines represent the SOTA in computational efficiency; please refer to Appendix \ref{appendix:related_work} for descriptions of these model architectures.

\input{sophia2}

Beyond these, there is a rich literature on compute-efficient computer vision architectures. For example, several efficient CNN-based architectures exist ____; however, these have been recently surpassed by MobileNetV4 ____. Since the invention of the ViT ____, there have been many compute-efficient ``ViTs'' that incorporate efficient modules inspired by CNN-based approaches ____. SHViT ____ has recently surpassed these architectures. Despite their impact and ingenuity, none of these hybrid architectures meets the definition of a plain ViT, which is attention-only and non-hierarchical; they thus lose many advantages of ViTs that we wish to keep.