\section{Related Works}
\label{sec:relatedworks}

\subsection{Monocular 3D Object Detection}
Monocular 3D Object Detection models aim to detect the 3D bounding boxes of target objects from a single input image. These models can be divided into three categories: 2D-detector-based, depth-image-based, and transformer-based.

\textbf{2D-Detector-Based M3OD} typically begins by localizing 2D bounding boxes and subsequently estimating 3D bounding boxes using geometric relationships or predefined 2D-3D box constraints, as demonstrated in M3D-RPN \cite{brazil2019m3d}.
%Alternatively, SMOKE \cite{liu2020smoke} formulates 3D object detection as a keypoint-based regression task in a single-stage pipeline.
MonoGround \cite{qin2022monoground} incorporates the ground plane beneath objects as a prior, transforming the ill-posed 2D-to-3D mapping problem into a more constrained and solvable task. However, these methods show poor performance due to inaccurate depth estimation and lack generalizability in diverse circumstances, as constraints like flat ground may not apply in high-elevation environments.

\textbf{Depth-Image-Based M3OD} such as D\textsuperscript{4}LCN \cite{ding2020learning} and DDMP-3D \cite{wang2021depth} first estimate depth maps from RGB images using a pre-trained depth generator while also utilizing the RGB image in a visual backbone network to extract visual features. Both types of features are extracted using Convolutional Neural Network (CNN)-based modules and fused to estimate 3D bounding boxes. 
However, as the entire network is CNN-based, it struggles to capture the global context of the image, leading to suboptimal performance. Additionally, these models often require ground-truth depth map data, which further limits their applicability.

\textbf{Transformer-Based M3OD} methods have been recently proposed, showing promising performance. MonoDTR \cite{huang2022monodtr} exploits a Transformer \cite{vaswani2017attention} encoder-decoder architecture to globally integrate context and depth-aware features, requiring LiDAR data for auxiliary supervision. In contrast, MonoDETR \cite{zhang2023monodetr} uses the DETR \cite{carion2020end} architecture to predict 3D bounding boxes and estimates foreground depth maps for supervision without relying on additional data. Both models utilize depth distributions for each pixel, following the approach introduced in CaDDN \cite{reading2021categorical} for supervision. Although these transformer-based models improve global feature extraction for depth estimation, they still face limitations due to their reliance on CNN backbones, which struggle to effectively capture global features.

\subsection{Detection Transformer}
DETR\cite{carion2020end} is an end-to-end object detector that eliminates the need for hand-crafted anchor boxes by combining a CNN backbone with a transformer architecture to model global relationships for object detection. Despite its strong performance, DETR faces challenges such as slower training convergence due to its computational complexity and inefficiency in matching object queries.
%Deformable-DETR\cite{zhu2020deformable} addresses these issues by introducing deformable attention, which reduces the computational burden and enhances performance by focusing attention on a small set of sampling locations and leveraging multi-scale feature maps.
DAB-DETR\cite{liu2022dab} improves DETR's efficiency and accuracy by introducing 4D anchor-based queries that are dynamically updated during decoding, enabling effective bounding box refinement. The proposed model extends this idea by incorporating 6D dynamic anchor boxes to better handle asymmetric shapes which achieves improved performance in M3OD task.

\subsection{Vision Foundation Model}
Vision Foundation Models (VFMs) are large-scale pre-trained models designed for versatile vision tasks, such as object detection, semantic segmentation, and depth estimation, leveraging extensive datasets like ImageNet-1k \cite{russakovsky2015imagenet}. Unlike traditional CNN-based backbones, such as ResNet \cite{he2016deep} and DenseNet \cite{huang2017densely}, VFMs like CLIP \cite{radford2021learning}, DINO \cite{caron2021emerging}, SAM \cite{kirillov2023segment}, and DINOv2 \cite{oquab2023dinov2} are based on ViT \cite{dosovitskiy2020image} backbones, which provide richer contextual features for a wide range of applications. In this paper, DINOv2 is selected as the backbone for the M3OD task to enhance depth estimation and 3D object detection performance.