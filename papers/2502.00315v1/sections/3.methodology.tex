\begin{figure*}[h]
    \centering
    \includegraphics[width=14cm]{figures/MonoDINODETR.jpg}
    \caption[Overall Structure of MonoDINO-DETR]{\textbf{Overall Structure of MonoDINO-DETR.} The proposed method, \textbf{MonoDINO-DETR}, is composed of four main components: the Feature Extraction Module, the Object-Wise Supervision Module, the Depth-Aware Transformer, and the MLP-Based Detection Heads. The visual feature extraction process is represented in green, while the depth feature extraction process is represented in blue.
    } \label{fig:Monodinodetr}
\end{figure*}
\section{Methodology}
\label{sec:methodology}
The overall architecture of the proposed model is shown in Figure \ref{fig:Monodinodetr}. Given a single input image, visual features for object detection and depth features for depth estimation are extracted using the foundation model backbone with a Hierarchical Feature Fusion Block (HFFB) and the backbone with a Dense Prediction Transformer (DPT) \cite{ranftl2021vision} head, respectively. The depth features are then projected to generate a one-dimensional depth map, which is supervised using ground truth bounding box and center-depth data. Both visual and depth feature are fed into a Detection Transformer network to generate depth-aware queries. Finally, Multi-Layer Perceptron (MLP)-based detection heads estimate the class, 2D size, projected center point on the image, depth, 3D size of the bounding box, and orientation of the target objects.

\subsection{Feature Extraction Module}
\textbf{DINOv2 Backbone.} The DINOv2 \cite{oquab2023dinov2} backbone extracts general-purpose features from the input image. Starting with the interpolated input image $I \in \Bbb{R}^{H\times W \times 3}$, the image is divided into patches of size 14x14 pixels, denoted as $x_{p} \in \Bbb{R}^{N \times (P^2 \cdot C)}$. Here, $(H, W)$ represents the resolution of the interpolated image, $C$ denotes the number of channels, $(P, P)$ corresponds to the resolution of each patch $(P=14)$, and $N=HW/P^2$ is the total number of patches, which also serves as the input sequence length for the Transformer. To preserve spatial information, positional embeddings are added to the patch embeddings and the flattened patches are projected to dimension $D=768$.
Using a memory-efficient attention module and sequential MLP modules, the model generates features that capture global context.
This process is repeated 12 times, and the features from the 3rd, 6th, 9th, and last layers are reshaped and used for the next step. These features are denoted as $f_{\frac{1}{14}}^3, f_{\frac{1}{14}}^6, f_{\frac{1}{14}}^9, f_{\frac{1}{14}}^{12}$, where $\frac{1}{14}$ represents the downsampling ratio relative to the original input size. This process is illustrated in Figure \ref{fig:Feature_extraction_module}.

\begin{figure*}[h]
    \centering
    \includegraphics[width=14cm]{figures/feature_extraction_module.jpg}
    \caption[Overall Structure of Feature Extraction Module]{\textbf{Overall Structure of Feature Extraction Module.} The Feature Extraction Module is divided into three components: the DINOv2 backbone, the Visual Feature Extraction Module, and the Depth Feature Extraction Module. The \textbf{Hierarchical Feature Fusion Block} serves as the key module for visual features, while the combination of the DPT Head and DINOv2, which together form the \textbf{Depth Anything V2} architecture, serves as the key module for depth features.
    } \label{fig:Feature_extraction_module}
\end{figure*}

\textbf{Hierarchical Feature Fusion Block.} Unlike typical CNNs, which employ multi-scale hierarchical architectures tailored for detection-specific tasks, DINOv2 features a plain, non-hierarchical design that maintains a single-scale feature map throughout. While its inclusion of global context is advantageous for depth estimation tasks, the absence of a hierarchical structure may limit its performance in object detection tasks. To address this and fully leverage its features, we construct a hierarchical architecture using transposed convolutions and group normalization to enhance visual feature extraction for object detection.

For visual feature extraction, the last three of the four intermediate features, $f_{\frac{1}{14}}^6, f_{\frac{1}{14}}^9, f_{\frac{1}{14}}^{12}$, are utilized to generate hierarchical features as illustrated in Figure \ref{fig:Feature_extraction_module}. Each feature undergoes a convolutional layer to adjust its dimensions, followed by transposed convolutions with varying kernel sizes and strides. This process produces features with resolutions that are 4, 2, and 1 times larger than the original, resulting in $f_{\frac{4}{14}}', f_{\frac{2}{14}}',$ and $f_{\frac{1}{14}}'$, respectively.
%This approach generates visual features with varying resolutions across the layers.
These multi-scale features enhance the object detection performance of the original backbone, which is a plain ViT model with limited hierarchical capabilities. The importance of the Hierarchical Feature Fusion Block will be demonstrated in the ablation study.

\textbf{Depth Estimation with Transfer Learning.} For depth feature extraction, the DPT \cite{ranftl2021vision} head is employed. Although not a foundation model, Depth Anything V2 \cite{yang2024depth} is a large-scale pre-trained model designed for relative depth estimation, trained on synthetic depth images using a teacher-student framework. It adopts a similar architecture, combining a DINOv2 backbone with a DPT head.
By utilizing this architecture for our depth feature extraction module, we leverage the pre-trained weights of Depth Anything V2 and apply transfer learning to estimate absolute depth values for each pixel.

In our depth feature extraction module, the intermediate features from the backbone, $f_{\frac{1}{14}}^3, f_{\frac{1}{14}}^6, f_{\frac{1}{14}}^9, f_{\frac{1}{14}}^{12}$, are projected to specific dimensions: $D = 256, 512, 1024, 1024$, respectively. Each feature is then upsampled or downsampled to achieve resolutions of 4, 2, 1, and 1/2 times the original resolution, respectively. Features from each layer are subsequently combined using a RefineNet-based feature fusion block \cite{lin2017refinenet, xian2018monocular}. Additional convolutional layers are applied to estimate absolute depth bins for each pixel, which are supervised within the object-wise supervision module. Finally, the depth map features are obtained, as shown in Figure \ref{fig:Feature_extraction_module}.

\subsection{Object-Wise Supervision Module}
To effectively incorporate depth information into the depth features, the depth map is supervised following the method in MonoDETR \cite{zhang2023monodetr}. This approach relies solely on discrete object-wise depth labels derived from the ground-truth depth information of the target objects, without requiring additional dense depth annotations.

First, $k+1$ discretized depth bins are created using Linear Increasing Discretization (LID)\cite{reading2021categorical}, and each pixel is assigned to its corresponding depth bin.
%In LID, the depth index $k$ is calculated using the following formula:
%\begin{equation}
%k = \left\lfloor -0.5 + 0.5\sqrt{1 + \frac{8(d - d_{\text{min}})}{\delta}} \right\rfloor
%\end{equation}
%\begin{equation}
%\delta = \frac{2(d_{max}-d_{min})}{k_{tot}(k_{tot}+1)}
%\end{equation}
%\noindent where \(k_{tot}\), \( d \), \( d_{\text{min}} \), and \( d_{\text{max}} \) denote the total number of depth bins, the depth of each pixel, the minimum depth, and the maximum depth value, respectively.
%LID is a technique that discretizes continuous depth values into intervals that increase linearly with distance.
%Since depth estimation errors grow with distance, estimating depth accurately for farther objects is more challenging.
By using wider intervals for farther objects in LID, the model becomes more robust to small depth estimation errors at greater distances. Foreground object labels are then used to supervise each pixel within a bounding box by assigning them to the central depth class of the object. For supervision, the Focal Loss \cite{lin2017focal}, $\mathcal{L}_{dmap}$, is employed to balance the contributions of the background and objects:

\begin{equation}
\mathcal{L}_{dmap} = -\alpha (1-p_t)^\gamma \log(p_t)
\end{equation}

\noindent where $\alpha \in [0, 1] $  denotes the balancing factor and $\gamma \in [0, 5]$ denotes the modulating factor.

\subsection{Depth-Aware Transformer with Dynamic Anchor Boxes}
\textbf{Depth-Aware Transformer.} The visual and depth map features extracted by the feature extraction module are fed into separate encoders: a visual encoder for visual features and a depth encoder for depth features. Following the MonoDETR \cite{zhang2023monodetr} design, these encoders are paired with a shared decoder block.

\textbf{Enhanced Detection with 6D Dynamic Anchor Boxes.} In the decoder architecture, Dynamic Anchor Boxes (DAB), as introduced in DAB-DETR \cite{liu2022dab} are utilized. Unlike the vanilla DETR model, DAB-DETR sets the dimension of object queries to 4, enabling the effective estimation of a reference query point $(x, y)$ and a reference anchor size $(w, h)$. These anchor boxes are dynamically updated layer by layer. In our model, DAB is extended to six dimensions to iteratively refine anchor boxes for better handling of asymmetric shapes as shown in Figure \ref{fig:6D-DAB-DETR}. The reference point $(x, y)$ and the distances from the center to the left, right, top, and bottom edges $(l, r, t, b)$ are iteratively refined at each layer, improving adaptability to complex object shapes.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/6d_dab_detr.png}
    \caption[Representation of 6D Dynamic Anchor Boxes]{\textbf{Representation of 6D Dynamic Anchor Boxes.} 6D DAB extends 4D DAB by refining the reference point $(x, y)$ and the distances to the left, right, top, and bottom edges $(l, r, t, b)$ at each layer. This iterative refinement improves the model's adaptability to asymmetric object shapes.
    } \label{fig:6D-DAB-DETR}
\end{figure}

\subsection{MLP-Based Detection Heads}
After passing through the decoder, the depth-aware queries are processed by a series of MLP-based heads to predict various attributes, such as the object category, 2D size, projected 3D center, depth, 3D size, and orientation.
To ensure accurate alignment between the predicted queries and ground-truth objects, the loss for each query-label pair is computed.  As in MonoDETR \cite{zhang2023monodetr}, the losses for the six attributes are categorized into 2D and 3D groups.
The Hungarian Algorithm \cite{kuhn1955hungarian} is then employed to determine the optimal matching. Since 2D attributes are typically predicted with higher accuracy compared to 3D attributes, only the Loss 2D value is used as the matching cost.
Following this matching step, $N_{gt}$ valid pairs are obtained from a total of $N$ queries, where $N_{gt}$ represents the number of ground-truth objects.
The overall loss for training is then formulated as follows:

\begin{equation}
\mathcal{L}_{overall} = \frac{1}{N_{gt}} \cdot \sum_{n=1}^{N_{gt}}{(\mathcal{L}_{2D}+\mathcal{L}_{3D})} + \mathcal{L}_{dmap}
\end{equation}
