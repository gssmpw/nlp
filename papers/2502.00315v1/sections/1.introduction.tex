\section{Introduction}
\label{sec:introduction}
With recent advancements in autonomous driving technology, autonomous racing competitions such as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing League (A2RL) that push the boundaries of innovation are gaining popularity. Among these, the IAC, the leading competition in autonomous racing, has been held at various iconic tracks such as Las Vegas Motor Speedway, Indianapolis Motor Speedway, and Monza Circuit since 2021. At CES 2025, it achieved a milestone by completing a 20-lap, 4-car autonomous race at speeds exceeding 100 MPH without any accidents.
\begin{figure}[t]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/high-elevation.png}
        \caption{Illustration of a high-bank environment on a racing track, captured at Kentucky Speedway.}
        \label{fig:high-elevation}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/custom_result2.png}
        \caption{Inference results of MonoDETR (green), \textbf{MonoDINO-DETR} (blue), and ground truth (red) in ego-view (left) and bird's-eye view (right).}
        \label{fig:custom_result}
    \end{subfigure}
    \caption{Illustration of the racing track environment and a comparison of detection results between the proposed model and the state-of-the-art model.}
    \label{fig:main_monodinodetr}
\end{figure}

In high-speed autonomous multi-car racing, robust and reliable perception for long-range detection of the 3D position of opponent cars is crucial for overtaking. To achieve this, Indy racing cars are equipped with multiple sensors such as LiDAR, cameras, and RADAR. Although LiDAR and RADAR sensors offer significant benefits for 3D object detection tasks, they face certain limitations in racing environments. First, due to elevation changes as shown in Figure \ref{fig:high-elevation}, LiDAR points may detect not only target objects but also the ground, which can hinder the robust detection of other cars. Moreover, LiDAR points become sparser as the distance to the object increases. This sparsity can be critical for racing cars, which can reach speeds of up to 190 MPH. RADAR also has its own limitations, such as noisy data and multi-path interference, which can cause the original point to be duplicated in multiple locations even when no object is present.

Sensor fusion methods could be a solution for this problem, but the high-temperature and high-vibration conditions of racing cars make it challenging to rely on multi-sensor approaches. Since a malfunction in one sensor could lead to a critical blackout for the autonomous car, developing a robust detection system that relies on a single sensor alone would be highly beneficial in autonomous racing environments.

Cameras are currently the most attractive sensors due to their ability to extract rich features from a single input image and their relatively low cost compared to other sensors like LiDAR and RADAR. However, estimating depth from a single input image remains a challenging task because it is an ill-posed problem to infer 3D spatial positions from 2D inputs. This limitation causes Monocular 3D Object Detection (M3OD) to perform worse than LiDAR-based methods. Nevertheless, with the rapid advancements in deep learning, M3OD has shown significant improvements in recent years. Extensive research is actively being conducted in academia to further enhance its performance.

In line with these thoughts, this paper explores novel methods to enhance the performance of M3OD models in diverse environments, including urban areas with flat ground and racing tracks with significant elevation changes. By leveraging the generalized feature extraction capability of a vision foundation model and the global feature capturing ability of the Detection Transformer (DETR) \cite{carion2020end}, the proposed model demonstrates improvements in both depth and visual feature extraction, resulting in higher performance in M3OD tasks.
%These results are evaluated using public 3D object detection benchmarks, such as KITTI \cite{geiger2012we}, as well as a custom dataset collected in a racing track environment.
The main contributions of this paper are as follows:

\begin{itemize}
\item We propose \textbf{MonoDINO-DETR}, the first approach to use a foundation model, DINOv2 \cite{oquab2023dinov2}, as a backbone in M3OD, enabling the extraction of generalized features from images to improve both depth and visual feature extraction. The method is implemented as an end-to-end, one-stage detector.

\item We introduce Hierarchical Feature Fusion Block to facilitate precise object localization by leveraging multi-resolution feature information from plain Vision Transformer (ViT) \cite{dosovitskiy2020image} backbone.

\item We utilized the DETR architecture to achieve accurate depth estimation from global features without relying on additional data, such as LiDAR or depth maps. Performance is further enhanced for M3OD by incorporating 6D Dynamic Anchor Boxes.

\item The proposed method outperforms state-of-the-art models on the monocular KITTI \cite{geiger2012we} benchmark and delivers significantly improved results on a custom dataset collected in racing environments.
\end{itemize}

The remainder of the paper is organized as follows: Section \ref{sec:relatedworks} reviews related works relevant to this study. Sections \ref{sec:methodology} and \ref{sec:experiment} detail our methodology and evaluate its performance on both public and custom datasets. Section \ref{sec:conclusion} summarizes our findings and discusses future work.