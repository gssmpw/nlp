
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
% \usepackage{amsmath,eqparbox,xparse} %https://stackoverflow.com/questions/58139089/is-there-a-way-to-center-align-part-of-an-equation-in-latex
\usepackage{eqparbox}


% https://tex.stackexchange.com/a/34412/5764
\makeatletter
\NewDocumentCommand{\eqmathbox}{o O{c} m}{%
  \IfValueTF{#1}
    {\def\eqmathbox@##1##2{\eqmakebox[#1][#2]{$##1##2$}}}
    {\def\eqmathbox@##1##2{\eqmakebox{$##1##2$}}}
  \mathpalette\eqmathbox@{#3}
}
\makeatother














% \title{InstaSHAP: Shapley Values in Constant Time with Interpretable Models}
% \title{InstaSHAP: Interpretable Additive Models Estimate Shapley Values Instantly}
% \title{InstaSHAP: Interpretable Additive Models Estimate Shapley Explanations Instantly}
\title{InstaSHAP: Interpretable Additive Models Explain Shapley Values Instantly}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{James Enouen  \\
Department of Computer Science\\
University of Southern California\\
Los Angeles, CA \\
\texttt{enouen@usc.edu} \\
\And
Yan Liu \\
Department of Computer Science\\
University of Southern California\\
Los Angeles, CA \\
\texttt{yanliu.cs@usc.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\input{jam_header}
\newcommand{\jam}[1]{\textcolor{orange}{[#1]}}
\newcommand{\yan}[1]{\textcolor{blue}{[#1]}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}



\usepackage{xfrac}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{subcaption}





\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begin{abstract}
In recent years,
the Shapley value and SHAP explanations have emerged as one of the most dominant paradigms for providing post-hoc explanations of black-box
% {machine learning}
models.
Despite their well-founded theoretical properties,
many recent works have focused on the limitations 
in both their computational efficiency and their representation power.
% Their 
The underlying connection with additive models, however, is left critically under-emphasized in the current literature.
In this work, we find that a variational perspective linking GAM models and SHAP explanations is able to provide deep insights into nearly all recent developments. % in the SHAP value.
In light of this connection, we borrow in the other direction to develop a new method to train interpretable GAM models which are automatically purified to compute the Shapley value in a single forward pass.
Finally, we provide theoretical results showing the limited representation power of GAM models is the same Achilles' heel existing in SHAP and discuss the implications for SHAP's modern usage in CV and NLP.
\end{abstract}




\section{Introduction}


Since their introduction into machine learning,
Shapley values have had a meteoric rise within the space of model explanation.
The principled axioms of Shapley~\citep{shapley1953shapley} and the easy-to-use framework of SHAP~\citep{lundberg2017shapleySHAP} have led to their widespread adoption when compared with alternatives in gradient-based and black-box explanation methods.
The developments which then followed quickly pushed beyond tabular datasets into
higher dimensional data like {computer vision and natural language},
with abundant research investigating how to improve the speed and efficiency of Shapley values across these various high-dimensional domains 
\citep{covert2021explainingByRemoving,mosca2022shapForNLPInterpretability,jethani2022fastSHAP,covert2023shapleyForVIT,enouen2024textGenSHAP}.
In recent years, however,
some lines of work have identified specific application scenarios where the Shapley value is provably guaranteed to fail, perhaps begging the question of whether such works improving on the Shapley value are instead done in vain
% in a {piecemeal} fashion 
\citep{bilodeau2022impossibilityTheoremsForFeatureAttribution,huang2023inadequacyOfSHAP}. 
Unfortunately, many of these critiques have been made piecemeal without an overall sense of their underlying causes.
In contrast, this work takes the perspective that SHAP's issues of explanation speed and explanation power can all be viewed under the same lens through the underlying connection with additive models and feature interactions.
% \blue{if you get on this bridge of GAMs, then this can lead to a better understanding of SHAP}


% It is hoped that this work will correct both shortcomings in a single stroke: by training additive models to instantly recover Shapley values, we highlight the applicability of SHAP is in one-to-one correspondence with that of additive models.



In this work,
we find that all of the most recent development in the SHAP value, like the practical improvements of FastSHAP \citep{jethani2022fastSHAP} and the theoretical advancements of FaithSHAP \citep{tsai2023faithSHAP},
% In this work, we drive a connection between the most recent practical developments in the SHAP value (FastSHAP, \cite{jethani2022fastSHAP}) and the most recent theoretical developments in the SHAP value (FaithSHAP, \cite{tsai2023faithSHAP}).
%
 can be unified and more easily understood by using a functional perspective using additive models.
 % with an emphasis on additive models and feature interactions.
% \jam{I swear there must be a less verbose way of doing this paragraph}
% In this work, we drive a connection between the most recent practical developments in the SHAP value (FastSHAP, \cite{jethani2022fastSHAP}) and the most recent theoretical developments in the SHAP value (FaithSHAP, \cite{tsai2023faithSHAP}).
%
%
In particular, 
the amortization scheme introduced by FastSHAP \citep{jethani2022fastSHAP}
builds on the least squares formulation of the Shapley value
\citep{charnes1988extremalLeastSquaresShapleyCoreCheby,lundberg2017shapleySHAP} by training a global approximator for the Shapley value,
which can each be viewed as fitting a pointwise and a global additive model, respectively. 
\cite{tsai2023faithSHAP} instead extends this least squares characterization to the bivariate and higher-order interactions 
% between features 
to yield a richer understanding of the model to explain,
drawing the same parallels with higher-order additive models.
% We find that not only can one naively combine 
% the functional amortization scheme of FastSHAP 
% with the higher-order characterization of FaithSHAP
% to yield what could be called {Fast-Faith-SHAP}
% (able to quickly generate higher-order explanations of any blackbox model.)
% %
% %
% But, we moreover find that there is a deep variational connection which underscores all such recent developments between the fundamentally interrelated SHAP explanation and GAM model,
% paralleling the algebraic case studied in \cite{bordt2023shapleyToGAMandBack} and building on various works in the GAM literature studying purification of additive models \citep{hooker2007generalizedFunctionalANOVA,hart2018sobolCovariancesDependentVariables,lengerich2020purifyingInteractionEffects,xingzhi2022pureGAM}.
%
%
%
%
%

All of these extensions and more can be though of as special cases of training additive models when considered from the functional ANOVA perspective \citep{sobol2001globalSensitivity},
allowing not only for their simple combination but a greater understanding of the underlying mechanisms overall.
%
%
We further find that there is a deep variational connection which underscores all such recent developments between the fundamentally interrelated SHAP explanation and GAM model,
paralleling the algebraic case studied in \cite{bordt2023shapleyToGAMandBack} and building on various works in the GAM literature studying purification of additive models \citep{hooker2007generalizedFunctionalANOVA,hart2018sobolCovariancesDependentVariables,lengerich2020purifyingInteractionEffects,xingzhi2022pureGAM}.




Overall, we set out to emphasize these fundamental connections between SHAP and GAM (as well as Faith-SHAP-k and GAM-k) across all possible correlated input distributions, where previous work only addressed the case of independent input variables \citep{bordt2023shapleyToGAMandBack}.
We use these theoretical advances in the variational equation to provide a simple check of whether SHAP is trustworthy by requiring a GAM model can be trained to the same accuracy
(as depicted in Figure \ref{fig:teaser_GAM_SHAP_DNN_comparison_of_DNNSHAP_vs_GAMSHAP}.)
% \blue{provides a simple check if SHAP is trustworhty by training a GAM to the same accuracy}
This development allows for deeper insights and practical checks into SHAP's application in important ML domains like CV and NLP where input features are heavily correlated.
% In Figure \ref{fig:teaser_GAM_SHAP_DNN_comparison_of_DNNSHAP_vs_GAMSHAP},
% we see how 
% \red{and adress its importance in CV/NLP where features are heavily correlated}
% We next use synthetic data to exactly highlight this understanding in the case where we can exactly compute the Shapley value.
% This allows us insight into \jam{whatever conclusion}
% and drives home the necessity to fit GAM models, understand feature interactions, and fit higher-order GAM models, ...
% We finally look at many applicable datasets (tabular and CV) and then ask the critical question of how trustworthy are SHAP values in the case where GAM models cannot achieve good performance
% \jam{because I assume my experiments show this correlation exists}
Further, by developing a new technique to automatically purify additive models to return the corresponding SHAP values, we simultaneously solve a longstanding problem from the GAM literature 
% and
% we further demonstrate the practical existence of this fundamental correspondence across multiple synthetic and real-world datasets.
% We additionally use experiments to 
% can then
% further
as well as
demonstrate practical advantages of our method InstaSHAP over the existing FastSHAP.
% and finally emphasize the fundamental limitations existing within both and how such limitations affect the interpretation of and trustworthiness of SHAP on the whole.
%
%
We envision our major contributions as follows:


\begin{itemize}
    \item Establishment of the variational formulation of Shapley additive models alongside the existing variational formulations of GAM and functional ANOVA, solving the case of dependent variables which was left open in \cite{bordt2023shapleyToGAMandBack}.
    \item Introduction of a practical training method for GAM models 
    via an alternative loss function with output masking,
    automatically solving the problem of GAM purification and allowing for `instant' access to the Shapley values.
    % \item Full development of the variational formulation of Shapley alongside the variational formulation of GAM and functional ANOVA, solving the case of dependent variables which was left open in \cite{bordt2023shapleyToGAMandBack}.
    % \item Introduction of a practical training method for GAM models     via an alternative loss function with output masking,     automatically normalizing the additive shape functions to have `instant' access to the Shapley values.
    % \item Comparative experiments across many synthetic and real-world domains of interest, providing evidence towards the question of whether Shapley values are trustworthy for ML in practical environments.
    \item Theoretical insights into the real-world application of SHAP and many comparative experiments across synthetic and real-world domains of interest, helping to lay bare the question of whether Shapley values are trustworthy for ML in practical environments.
\end{itemize}





\begin{figure}[t]
    \centering
    % \includegraphics[width=0.43\columnwidth]{figures/teasers/iclr 2024 figures -- teaser figure -- TAB half vertically aligned cropped.pdf}
    % \quad\quad
    % \includegraphics[width=0.495\columnwidth]{figures/teasers/iclr 2024 figures -- teaser figure -- CV half vertically aligned cropped.pdf}
    %7.29 x 4.64; 8.22 x 4.64
    %    0.4257; 0.48
    % \caption{ \small
%
    \includegraphics[width=0.43\columnwidth]{figures/teasers/iclr_2024_figures_--_teaser_figure_--_TAB_half_vertically_aligned_cropped.pdf}
    \quad\quad
    \includegraphics[width=0.495\columnwidth]{figures/teasers/iclr_2024_figures_--_teaser_figure_--_CV_half_vertically_aligned_cropped.pdf}    
    \caption{ 
    The fundamental correspondence between SHAP and GAM is used practically to distinguish two unique scenarios.
    In scenario A, such as simpler tabular data, GAM models can achieve SOTA performance and their SHAP explanations align with SHAP explanations of blackbox models.
    In modern ML applications like computer vision, we have scenario B, where there is a gap between GAM and DNN performance in practice.  This means that either: (\#1) we cannot train GAMs as well as other deep neural networks; or (\#2) there is an overcredulous trust of SHAP in these domains.}
\label{fig:teaser_GAM_SHAP_DNN_comparison_of_DNNSHAP_vs_GAMSHAP}
\end{figure}

%XXXX acknolwedgement of freepik.com





\section{Background}

Let $F:\cX \to \cY$ be a function representing a machine learning model which maps from input space $\cX \subseteq \bbR^d$ to output space $\cY \subseteq \bbR^c$, where there are $d$ input features and $c$ output features.
% (potentially classes).
We will use $[d]:=\{1,\dots,d\}$ to represent the set of input features and $S\subseteq[d]$ to represent a subset of the input features.
We also write the set of all such subsets, the powerset, as $\cP([d]) \cong \{0,1\}^d$.



\subsection{Explaining by Removing}
% \subsection{How to perturb}
\label{sec:background_explaining_by_removing}

A very important aspect of removal-based explanations like LIME or SHAP \citep{ribeiro2017lime,lundberg2017shapleySHAP}
is the method of feature removal \citep{sundararajan2020theManyShapleyValues,covert2021explainingByRemoving}.
%
We review the three most popular removal approaches: replacing the feature with a reference value (baseline), integrating over the marginal distribution of the feature (marginal), or integrating over the conditional distribution of the feature (conditional).
When applied to an explanation method like the Shapley value, these result in the corresponding: baseline Shapley, marginal Shapley, or conditional Shapley \citep{sundararajan2020theManyShapleyValues,janzing2020interventionalShapleyValue,frye2021shapleyOnTheManifold}.




First, consider an input example one would like to explain $x\in\cX$ and a subset of the features which one would like to keep $S\subseteq[d]$ as part of the model.
%
We may then compare against a baseline value $\Bar{x}\in\bbR^d$ and define the baseline value as $\cB_{\Bar{x}}$ as below.
We may also choose a distribution of baselines $p(x)$ over the input space $\cX$,
which allows us to define both the marginal projection and the conditional projection, $\cN_p$ and $\cM_p$.
\begin{alignat}{3}
% \cB_{\Bar{x}}[F](x,S) &:=  
[\cB_{\Bar{x}}\circ F](x,S) &:=  
& &
F(x_S,\Bar{x}_{-S})
\label{eqn:baseline_removal_equation}
\\
% \cN_p(S)[f](x) &= 
% \cN_p(S)[f] &= 
% \cN_p[F](x,S) &:= 
[\cN_p\circ F](x,S) &:= 
% \cM^{\text{marg}}_p[F](x,S) &:= 
\bbE_{\Bar{X}_{-S} \sim p(X_{-S})} &
\Big[ &
F(x_S,\Bar{X}_{-S})
\Big]
\label{eqn:marginal_expectation_removal_equation}
\\
% \cM_p[F](x,S) &:=
[\cM_p\circ F](x,S) &:=
\bbE_{\Bar{X}_{-S} \sim p(X_{-S}|X_S=x_S)} &
\Big[ &
F(x_S,\Bar{X}_{-S})
\Big]
\label{eqn:conditional_expectation_removal_equation}
\end{alignat}
We write these three operators as functionals mapping $F$ to a new function $f$ to support our analysis from a functional perspective.
Historically, the baseline value and marginal value have been the easiest to use in practice because we may directly explain our blackbox $F$ without significant modifications.
However,
since the highlighting of the `off-the-manifold' problem by \cite{frye2021shapleyOnTheManifold}, it has been shown that
baseline methods $\cB_{\Bar{x}}$ and marginal methods $\cN_{p}$ significantly overemphasize the algebraic structure of the model instead of the statistical structure.
If one is exclusively interested in the algebraic dependencies of their ML model,
the correspondence highlighted herein has already been established in \cite{bordt2023shapleyToGAMandBack}.
Otherwise we hereafter restrict our attention to the conditional expectation using $\cM_p$
and provide details on further considerations in Appendix \ref{app_sec:explain_by_removing}.



We define a feature attribution method $\Phi$ as taking $F(x)$ and returning a local explanation function $[\Phi_i\circ F](x)$ for each feature $i\in[d]$ on each local input $x\in\cX$.
Similarly, we define a blackbox feature attribution method as instead taking a masked function $f(x,S)$ and returning a local explanation function for each feature, $[\circphi_i\circ f](x)$.

We now provide one of the typical definitions of the Shapley value as follows;
however, we recommend the unfamiliar reader instead waits until the more intuitive Equation \ref{eqn:shap_via_unanimity_or_synergy_functions}.
\begin{align}
[\circphi^{\text{SHAP}}_i\circ f](x) =  \mathlarger{\sum}_{S\subseteq [d]}
p^{\text{SHAP-unif}}(S) 
\cdot
\Big[
f(x,S+i) - f(x,S-i)
\Big]
\label{eqn:shap_original_definition}    
\end{align}
% \begin{align}[\circphi^{\text{SHAP}}_i\circ f](x) =  \mathlarger{\sum}_{S\subseteq [d]} {d \choose |S|}^{-1} \frac{1}{d+1} \cdot \Big[ f(x,S+i) - f(x,S-i) \Big]\end{align}
%
%
%
%
% \vspace{-3.1pt}
\vspace{-7.1pt}
\begin{align}
    p^{\text{SHAP-unif}}(S) \propto {d \choose s}^{-1} \frac{1}{d+1} 
    \label{eqn_defn:SHAP_uniform}
\end{align}
Here, the Shapley value is defined as the addition and removal of a single feature $i\in[d]$ across many contexts $S\subseteq[d]$ according to the distribution $p^{\text{SHAP-unif}}(S)$ where the shorthand $s=|S|$ is used.
Following the discussion in the previous section, we will in this work always consider the conditional Shapley $\Phi^{\text{cond-SHAP}} := \circphi^{\text{SHAP}} \circ \cM_p$.
Alternative black-box explanations to the Shapley value are discussed further in {Appendix} \ref{app_sec:post_hoc_feature_attribution_and_interaction_attribution}.
%
% \begin{align}
% [\circphi^{\text{SHAP}}_i\circ f](x) =  \mathlarger{\sum}_{S\subseteq [d]}
% {d \choose s}^{-1} \frac{1}{d+1} 
% \cdot
% \Big[
% f(x,S+i) - f(x,S-i)
% \Big]
% \label{eqn:shap_original_definition}    
% \end{align}
% The Shapley value is the average contribution of the feature $i\in[d]$ across many contexts $S\subseteq[d]$ where the shorthand $s=|S|$ is used.
% Following the discussion in the previous section, we will in this work always consider the conditional Shapley $\Phi^{\text{cond-SHAP}} := \circphi^{\text{SHAP}} \circ \cM_p$.
% Alternative black-box explanations to the Shapley value are discussed further in {Appendix} \ref{app_sec:post_hoc_feature_attribution_and_interaction_attribution}.




% Other popular explanation methods are the inclusion value, $[\circphi^\text{inc}_i\circ f](x) := f(x,i) - f(x,\emptyset)$,
% and the removal value, $[\circphi^\text{rem}_i\circ f](x) := f(x,[d]) - f(x,[d]-i)$.
% %
% %
% \jam{
% Many others are highlighted in the appendxi \ref{}
% }













% \subsection{Functional ANOVA Decomposition}
\subsection{Interpreting by Adding} %lol
We now introduce the interpretable generalized additive model (GAM) of 
\citet{hastie1990originalGAM}.
This model is seen as interpretable because each of the input features have a simple 1D relationship with their effect on the output.
In this work, we also include the `zero dimensional' normalizing constant $f_\emptyset$ and often use the term GAM1 to emphasize a GAM that only has 1D functions.
\vspace{4pt}
\begin{alignat}{4}
    \nonumber
    % F(x_1,\dots,x_d) 
    F^{\scalebox{0.55}{$\leq 1$}}(x_1,\dots,x_d) 
    &\oct =\oct &
    f_\emptyset 
    &\oct +\oct &
    \eqmathbox[gam1gam1]{\underbrace{ f_1(x_1) + \dots + f_d(x_d)}}
    \\
    &\oct =\oct &
    f_\emptyset 
    &\oct +\oct &
    \eqmathbox[gam1gam1]{\sum_{i\in[d]} f_i(x_i)}
\label{eqn:fnl_GAM1}
\end{alignat}
%
%
This can further be generalized to a GAM2 model \citep{wahba1994ssanova,lou2012intelligible,lou2013accurate,chang2022nodegam} which is still seen as an interpretable model because its 2D functions can still be visualized using a heatmap plot.
%
%
\begin{alignat}{7}
    \nonumber
    % F^{\text{GAM2}}(x_1,\dots,x_d) 
    % F^{\leq 2}(x_1,\dots,x_d) 
    % F(x_1,\dots,x_d) 
    F^{\scalebox{0.55}{$\leq 2$}}(x_1,\dots,x_d) 
    &\oct =\oct &
    f_\emptyset 
    &\oct +\oct &
    \eqmathbox[gam2gam1]{\underbrace{ f_1(x_1) + \dots + f_d(x_d)}}
    &\oct +\oct &
    \eqmathbox[gam2gam2]{\underbrace{ f_{\scriptscriptstyle   1,2}(x_1,x_2) + \dots + f_{\scriptscriptstyle   d-1,d}(x_{d-1},x_d) }}
    \\
    &\oct =\oct &
    f_\emptyset 
    &\oct +\oct &
    \eqmathbox[gam2gam1]{\sum_{i\in[d]} f_i(x_i)}
    &\oct +\oct &
    \eqmathbox[gam2gam2]{\sum_{\{i,j\}\subseteq[d]} f_{i,j}(x_i,x_j)}
\label{eqn:fnl_GAM2}
\end{alignat}



Recent research has additionally focused on addressing the practical considerations associated with training increasingly high-order GAMs
\citep{yang2020gamiNet,dubey2022scalablePolynomials,enouen2022sian}.
For some order $k \geq 3$, we may define the higher-order GAM-k as:
\begin{alignat}{7}
    % F^{\text{GAM2}}(x_1,\dots,x_d) 
    % F^{\leq k}(x_1,\dots,x_d) 
    % F(x_1,\dots,x_d) 
    % F^{\leq k}(x)
    F^{\scalebox{0.55}{$\leq k$}}(x_1,\dots,x_d) 
    &\oct =\oct &
    f_\emptyset 
    &\oct +\oct &
    % &+&
    \sum_{i\in[d]} f_i(x_i)
    % &\oct +\oct \dots \oct +\oct &
    % &+\dots +&
    &+\oct\dots\oct +&
    \sum_{S\subseteq[d], |S|=k} f_S(x_S)
    % \\
    &\oct =\oct &
    % & &
    \sum_{S\in\cI_{\leq k}} f_S(x_S)
    \quad\quad
\label{eqn:fnl_GAM_k}
\end{alignat}
where we write $\cI_{\leq k} := \{ S\subseteq[d] : |S|\leq k\}$.


%xxx
% \jam{transition}
% Nonetheless, this immediately raises the question of when to stop adding higher-order functions to our GAM model.
This might immediately raise the question of when to stop adding higher-order functions to our GAM model.
Multiple practical works have shown that for tabular data, $k$ does not have to be too large: GAM-1 and GAM-2 are often sufficient to fit the complexities of the data and achieve state-of-the-art performance across many datasets \citep{chang2022nodegam,enouen2022sian}.
The same question for CV or NLP, however, has faced little exploration in previous works. 
% \red{For higher dimensional tasks including CV and NLP, however, GAM models have seemingly struggled to achieve the performance of convolutional networks and transformers.}
% Before discussing these
In order to answer this question precisely, however, we instead turn to the functional ANOVA decomposition coming from the field of sensitivity analysis.





% \subsection{Other place holder because from preivous draift}
% \subsection{Sensitivity Analysis}
\subsection{Functional ANOVA}
\label{sec:background_functional_ANOVA}
In the literature on sensitivity analysis,
we may take any function and completely decompose it via its \textbf{\emph{functional ANOVA decomposition}}
\citep{sobol2001globalSensitivity,hooker2004discovering}:
\begin{alignat}{3}
    F(x_1,\dots,x_d) 
    &\oct =\oct &
    \sum_{S\subseteq[d]} \tilde{f}_S(x_S)
    \label{eqn:fnl_ANOVA_decomp}
\end{alignat}
Although there are many possible choices of $\tilde{f}_S$ which could obey this equation, we may define a unique decomposition via the conditional projection from Section \ref{sec:background_explaining_by_removing}:
\begin{align}
    \tilde{f}_S(x_S) 
    :=
    \sum_{T\subseteq S} (-1)^{|S|-|T|} f(x,T)
    =
    % \sum_{T\subseteq S} (-1)^{|S|-|T|} \cM_p[F](x,T)
    \sum_{T\subseteq S} (-1)^{|S|-|T|} [\cM_p\circ F](x,T)
\end{align}
Hereafter, we often follow the sensitivity analysis notation of writing $\tilde{f}_S(x_S)$ rather than $\tilde{f}(x,S)$.

This specific choice using conditional projection is often called the `Sobol-Hoeffding' decomposition.
In the case of independent input variables, \cite{sobol2001globalSensitivity} provides us a complete understanding of what happens to the variational solution of training any additive model.
In particular, the variance or the mean squared error is able to decompose completely via the \textbf{\emph{decomposition of variance}} formula:
\begin{align}
    \bbV
    :=
    \bbV\text{ar}_{X}[  F(X)  ]
    =
    \sum_{S\subseteq[d]} \bbV\text{ar}_{X_S}[  \Tilde{f}_S(X_S)  ]
    =
    \sum_{S\subseteq[d]} \bbV_S
\label{eqn:sobel_decomp_of_var}
\end{align}
where the orthogonal contributions, $\bbV_S := \bbV\text{ar}_{X_S}[  \Tilde{f}_S(X_S)  ]$, are called the Sobol indices and measure the variance which can be uniquely ascribed to each feature interaction $S$.


%xxx
% \jam{why does reader care}
Unfortunately, this variational formulation for additive models breaks down for the case of correlated input variables.
The best existing alternative in the literature is the Sobol covariances \citep{rabitz2010correlatedSobolIndices,hart2018sobolCovariancesDependentVariables} which is instead defined as $\bbC_S := \bbC\text{ov}_{X}[ F(X), \Tilde{f}_S(X_S)  ]$.
However, these covariances may result in values which are both positive and negative, conflating the synergistic effects between a set of features $S$ and the redundant effects of shared information amongst the same set of features $S$.


% \subsection{Feature Interaction} %maybe dont need this
We will still say that a statistical \textbf{\emph{feature interaction}},
$S\subseteq[d]$, 
is said to exist whenever
\begin{align}
\bbV_S 
:= 
\bbV\text{ar}_{X_S}[  \Tilde{f}_S(X_S)  ]
> 0
\oct,
\end{align}
however, for the case of correlated input features, we importantly need to distinguish between the two major types of feature interactions:
\begin{enumerate}
    \item feature interaction \emph{synergy}, where $\bbV_S>0$ and $\bbC_S > 0$.
    \item feature interaction \emph{redundancy}, where $\bbV_S>0$ but $\bbC_S < 0$.
\end{enumerate}




\begin{figure}
    \centering
    % \includegraphics[width=0.78\linewidth]{figures/depictions/iclr 2024 figures -- simple 2D example, redund vs synerg, fixed -- pdf resizer.pdf}
    \includegraphics[width=0.78\linewidth]{figures/depictions/iclr_2024_figures_--_simple_2D_example,_redund_vs_synerg,_fixed_--_pdf_resizer.pdf}
    \caption{Simple examples (Gaussian input variables and multilinear response variables) which demonstrate each of the two major types of feature interactions: synergistic interactions and redundant interactions.
    Their full functional ANOVA and exact Shapley functions are additionally calculated and shown.
    Colored by relevant variable.
    Note we use $x$ and $y$ instead of $x_1$ and $x_2$.}
    % \label{fig:simple_2D_example_Shapley_fnl_calculation}
    \label{fig:simple_2D_example_Shapley_fnl_calculation_synergy_and_redundancy}
\end{figure}

Using this functional ANOVA perspective,
we can now write the Shapley value as a complete function for each variable $i\in[d]$ using the well known alternative via the synergy or unanimity functions \citep{shapley1953shapley}:
%a reviewer asked me to cite this ahahahah, kind of ironic if you think about it
\begin{align}
[\circphi^{\text{Sh}}_i\circ f] (x) = \sum_{S\supseteq \{i\}} \frac{\Tilde{f}_S(x_S)}{|S|}
\label{eqn:shap_via_unanimity_or_synergy_functions}
\end{align}
Intuitively,
the value which is ascribed to each feature interaction $S$ is uniformly divided amongst each of its constituents $i\in S$.
In Figure \ref{fig:simple_2D_example_Shapley_fnl_calculation_synergy_and_redundancy},
we can see the easily computed Shapley functions coming from computing the functional ANOVA decomposition, dividing the interaction effects in both the synergistic and the redundant setting.
In real-world datasets and in the presence of higher-order interactions, it easy to imagine how quickly such effects will compound and conflate one another.

% , the divided interactions can both lead to confusing and conflated results, only being compounded in real-world data with higher-order effects. 

% In Figure \ref{fig:simple_2D_example_Shapley_fnl_calculation_synergy_and_redundancy},
% we can see that after computing the functional ANOVA decomposition, it is simple to compute the Shapley functions by dividing the interaction terms amongst each feature.
% We can moreover see how the feature interaction is divided in the synergistic setting and in the redundant setting,
% both possibly leading to confusing results (especially in the presence of real-world data where an abundance of higher-order effects are competing for representation in the one-dimensional SHAP value.)
% Moreover, in both the synergistic setting and the redundant setting, the divided interactions can both lead to confusing and conflated results, only being compounded in real-world data with higher-order effects. 
% both possibly leading to confusing results (especially in the presence of real-world data where an abundance of higher-order effects are competing for representation in the one-dimensional SHAP value.)





% These can be simultaneously displayed by the same algebraic equation, so for both examples we assume the function to be explained is $F(x_1,x_2) = x_1 x_2$.

% In the case that $X_1,X_2\sim \cN(0,1)$ are both independent Gaussians, then this function leads to a `synergistic' feature interaction, because we must know both input features before predicting an output feature.

% In the other case, we have that $X_1=X_2\sim \cN(0,1)$ meaning there is perfect correlation between the two input features.
% It follows that knowing either one of the features is the same as knowing both of the features, meaning the purified feature interaction is $\Tilde{f}_{12} = -x_1^2$.













% \subsection{Impossibility Theorems}
% \section{Impossibility Theorems}
% \subsection{Representation Power and Impossibility Theorems}
% \section{Representation Power and Impossibility Theorems}
% \section{Representation Power of SHAP}
% \section{Representation Power of SHAP and GAM}
\section{Representation Power of Additive Models}

Before proceeding further with the variational GAM methods we introduce,
we find it is important to characterize the behavior of SHAP in terms of the functional ANOVA decomposition.
In particular, we will do this in the form of an ``impossibility theorem'' to help cement the correspondence which exists between GAM and SHAP. 
However, unlike previous works focusing on the flaws of SHAP,
we not only exactly characterize all negative results showing when hypothesis tests are impossible, but consequently characterize all positive results showing exactly when hypothesis tests are possible.





\subsection{SHAP Function Space}

\begin{theorem}
\label{thm:shitty_theorem_SHAP_GAM}
    % \textbf{(SHAP$\cong$ANOVA-1, Informal)}
    \textbf{(SHAP$\cong$ANOVA-1)}
    % (Informal)
    SHAP will succeed on any hypothesis for some hypothesis space $\cH$ if and only if $\cH$ is completely free of feature interactions ($\cH \subseteq \cH_{\text{ANOVA}}^{\leq 1}$).
    % Any hypothesis test on some hypothesis space $\cH$ which uses the SHAP value is always guaranteed to succeed if and only if the hypothesis space is isomorphic to the GAM-1 space $\cH^{\leq 1}$.

% [SHAP=GAM]
    % (Informal)
    % Shapley values can answer any hypothesis test exactly so long as the class of functions is additive models
    % And given a local $x^*$
    % No hypothesis can be test
\end{theorem}

% \jam{consider proof sketch}

% \red{verification of axoims -- ~~}

We leave the details of the proof until Appendix \ref{app_sec:impossibility_theorems}; however, for one direction it is relatively straightforward to see from from Equation \ref{eqn:shap_via_unanimity_or_synergy_functions} that SHAP can succeed if all interaction terms are zero.
Conversely, 
if some true model $F\in\cH$ is not representable by an ANOVA-1 model,
% (or a GAM-1 model cannot achieve state-of-the-art performance),
then $F\notin\cH_{\text{ANOVA}}^{\leq 1}$ and hence SHAP is instead obscuring the feature interactions.
Importantly, we emphasize that this means SHAP cannot distinguish synergistic feature interactions nor can it distinguish redundant feature interactions.
% We find evidence that this is the case across multiple tabular and computer vision datasets,
% implying that SHAP is likely not as trustworthy as it may first appear on these real-world datasets.
%
We can additionally show the exact same type of relationship is true for Faith-SHAP-k.



% \red{we show later that means if any GAM-1 does not work then blah blah balh}
% \blue{weaker bc existence proof but still decent}
% bla blha

% Before moving on to the experiments section,
% we briefly state the impossibility theorems to help cement the correspondence which exists between GAM and SHAP.
% We will then use this underlying precedent as we move into the real-world experiments,
% because we can\jam{then recognize that any GAM}
% which is not able to achieve SOTA performance,
% (such as we have in our CV datasets)
% \red{just like in the teaser}
% can be taken as evidence that SHAP is not as trustworthy as it seems when applied to these highly correlated datasets.



\begin{theorem}
\label{thm:shitty_theorem_SHAP_k_higher_order}
    % \textbf{(Faith-SHAP-k$\cong$ANOVA-k, Informal)}
    \textbf{(Faith-SHAP-k$\cong$ANOVA-k)}
    %(Informal)
    For any $k\in[d]$,
    Faith-SHAP-k will succeed on any hypothesis test in some hypothesis space $\cH$ if and only if $\cH$ is free of higher-order features interactions of size ($k+1$) or greater  %Jam EALC makes me want to say 'and'???  I think thats the reason
    ($\cH \subseteq \cH_{\text{ANOVA}}^{\leq k} $).
    
    % For any $k\in[d]$,
    % any hypothesis test on some hypothesis space $\cH$ which uses the Faith-SHAP-k value is always guaranteed to succeed if and only if the hypothesis space is isomorphic to the GAM-k space $\cH^{\leq k}$.
\end{theorem}

This similarly implies that even indices measuring feature interactions will still be forced to blur out the higher-order interactions and hence remain limited in their representational capacity.
Once again, we emphasize that the reliance of these approaches on the functional ANOVA decomposition means it is not possible for them to distinguish between synergistic interactions and redundant interactions.

\subsection{GAM Function Space}

Let us now contrast these two results with the representation power of GAM models.
\begin{theorem}
\label{thm:ANOVA_1_vs_GAM_1_strict_inclusion}
    \textbf{(ANOVA-1$\subsetneq$GAM-1)}
    The functional space of ANOVA-1 representable functions is a strict subset of the functional space of GAM-1 representable functions ($\cH_{\text{ANOVA}}^{\leq 1} \subsetneq \cH_{\text{GAM}}^{\leq 1} $).
\end{theorem}

    Any function which is representable by a univariate ANOVA decomposition is automatically representable by a GAM model by the subset compliance of the ANOVA decomposition.
    This inclusion is strict in the other direction as soon as there is a feature correlation in the input data.
% The proof in the harder direction is simply by existence of such a function.
% For a distribution in general position, such an example is easy to find.  
% A similar result can again be found in the case of higher-order interactions.

\begin{theorem}
\label{thm:ANOVA_k_vs_GAM_k_strict_inclusion}
    \textbf{(ANOVA-k$\subsetneq$GAM-k)}
    The functional space of ANOVA-k representable functions is a strict subset of the functional space of GAM-k representable functions ($\cH_{\text{ANOVA}}^{\leq k} \subsetneq \cH_{\text{GAM}}^{\leq k} $).
\end{theorem}


Once again, the inclusion of k-dimensional ANOVA functions are automatically representable by an arbitrary GAM-k model by the definition of the decomposition.
The inclusion is again strict as soon as there is a correlation between features in the input data.
% %%Any function which is representable by a univariate ANOVA decomposition is automatically representable by a GAM model by the subset compliance of the ANOVA decomposition.
% %%This inclusion is strict in the other direction as soon as there is a feature correlation in the input data.
% %
% %
% % Overall, this means that the GAM spaces are an even more powerful hypothesis space than the SHAP spaces.
% Overall, because the GAM spaces are an even more powerful hypothesis space than SHAP, we can train GAMs as a simple test to confirm whether or not SHAP is an adequate explanation approach for a given dataset.
% %
% %
% %
% Similarly, this implies that the inabilities of a GAM-2 or GAM-3 model to achieve state-of-the-art performance are also indicative that even Faith-SHAP-2 or Faith-SHAP-3 indices would be insufficient to fully explain the behavior of a model.
% % In conjunction with our novel analysis of the correlated feature case,
% % we identify that these higher-order feature interactions can occur both as a synergy or as a redundancy.
% %
% %%%This test for ML datasets is uniquely offered by our perspective which adequately handles the correlations coming from the input features, and is what allows us to test if real-world datasets contain higher-order interactions.
We save proofs and further discussions for Appendix \ref{app_sec:impossibility_theorems}.
% In the following section, we briefly summarize the practical test for ML datasets which is uniquely offered by this perspective, allowing us to test real-world datasets for higher-order interactions.







\subsection{Practical Insights}
In conclusion, our results show that a practitioner may evaluate the trustworthiness of SHAP on a given dataset by simply training a GAM model on the same dataset.
If a GAM can easily match the same accuracy as a blackbox model or easily distill the same predictions as a blackbox model, 
then this is a dataset for which SHAP explanations can generally be trusted.
On the other hand, if a GAM cannot match the same accuracy as the blackbox model, this means that the practitioner needs to be wary about trusting SHAP values on this dataset.
In this second scenario, there are two possible resolutions.
For the GAM researcher, resolution 1 of Figure \ref{fig:teaser_GAM_SHAP_DNN_comparison_of_DNNSHAP_vs_GAMSHAP} is to train a better GAM through the use of more efficient training procedures or through an increase in capacity with additional feature interaction terms.
For the SHAP practitioner, resolution 2 of Figure \ref{fig:teaser_GAM_SHAP_DNN_comparison_of_DNNSHAP_vs_GAMSHAP} is to admit that SHAP is likely not a sufficient explanation for this model or dataset.

In many real-world scenarios, it is possible that neither of these extremes is completely true, with the lower bound GAM and upper bound SHAP meeting somewhere in the middle.
Nevertheless, in the current literature, this gap is extremely large for practical AI tasks including CV and NLP.
In our experiments,
we highlight this large gap on a high-dimensional CV task of bird classification.
Ultimately, the key contribution of this theory is that it provides a practical test for researchers to understand task-by-task what are the advantages as well as the limitations of SHAP and GAM approaches.



% \subsection{Shapley via Least Squares}
\section{Shapley via Least Squares}
% \subsection{Connecting to LSE Shapley}

\paragraph{Kernel SHAP}
The Shapley value was first given an optimization-based definition or `variational' characterization in \cite{charnes1988extremalLeastSquaresShapleyCoreCheby} as the solution to:
\begin{align}
\argmin_{\circphi\in\bbR^d} 
\bigg\{
\bbE_{S\sim p^{\text{SHAP}}(S)} \bigg[
\Big| 
% \Big(v(S) - v(\emptyset)\Big) - \Big(\sum_{i=1}^d \ind({i\in S})\cdot \circphi_i \Big)
f(S) - \sum_{i=1}^d \ind({i\in S})\cdot \circphi_i
\Big|^2
\bigg]
\bigg\}
\label{eqn:charnes_first_variational_SHAP_least_squares}
\end{align}
where the distribution is over the SHAP kernel
% \begin{align}
%     p^{\text{SHAP}}(S) \propto {d \choose |S|}^{-1} \frac{1}{|S|(d-|S|)}
%     \label{eqn_defn:SHAP_kernel}
% \end{align}
\begin{align}
    % p^{\text{SHAP-kern}}(S) \propto {d \choose s}^{-1} \frac{1}{s(d-s)}
    p^{\text{SHAP}}(S) \propto {d \choose s}^{-1} \frac{1}{s(d-s)}
    \label{eqn_defn:SHAP_kernel}
\end{align}
where once again $s=|S|$ (contrast this distribution with {Equation} \ref{eqn_defn:SHAP_uniform}).
% xxx
This formulation was originally utilized by KernelSHAP \citep{lundberg2017shapleySHAP} to fit a local linear model according to the SHAP kernel distribution in order to attain sufficient speed to see use in ML applications.




\paragraph{Fast SHAP}
This was importantly used more recently by FastSHAP \citep{jethani2022fastSHAP} in order to create a functional amortization scheme which fits to the same SHAP kernel.
They then write the Shapley function as the solution to the following equation:%
%
\footnote{To keep the notation cleaner and more similar with other existing works, we assume throughout this section that $f_\emptyset=0$, which is equivalent to centering or normalizing the outputs.}
\begin{align}
\argmin_{\circphi:\cX\to\bbR^d} 
\bigg\{
\bbE_{x\sim p(x)} \bigg[
\bbE_{S\sim p^{\text{SHAP}}(S)} \bigg[
\Big| 
f(x;S) - \sum_{i=1}^d \ind({i\in S})\cdot \circphi_i(x)
\Big|^2
\bigg]
\bigg]
\bigg\}
\label{eqn:fastshap_functional_amortized_variational_SHAP}
\end{align}
where they then fit the functions ${\circphi_i:\cX\to\bbR}$ over the entire input space to automatically generate the Shapley values at test time.
This dramatically improves the test-time speed with which SHAP explanations can be generated, overcoming what is often the most major practical limitation to deployment.

The summing over multiple functions to create the predicted output should {remind the reader} of the structure of GAM-1 additive models.
As we saw in our impossibility theorems and as we will later show with InstaSHAP, this functional perspective indeed opens up the possibility to connect with training additive models.


\paragraph{Faith SHAP}
% \jam{need to change this phrasing} %xxxx
As we highlight in {Theorem \ref{thm:shitty_theorem_SHAP_GAM}},
it is well known that feature interactions are necessary for explaining more complex functions.
Accordingly, many works have tried to extend the Shapley value to be able to handle the effects of feature interactions
\citep{grabisch1999originalShapleyInteractionIndex,sundararajan2020shapleyTaylorInteractionIndex,bordt2023shapleyToGAMandBack,fumagalli2023shapIQ}.
%
Recently, there have been theoretical advancements which extend the Shapley value directly using the variational formulation in \citet{tsai2023faithSHAP}.
They write their higher-order solution, called Faith SHAP, as:
\begin{align}
\argmin_{\circphi\in\bbR^{\scalebox{0.55}{$\cI_{\leq k}$}}}
\bigg\{
\bbE_{S\sim p^{\text{SHAP}}(S)} \bigg[
\Big| 
% \Big(v(S) - v(\emptyset)\Big) - \Big(\sum_{\substack{T\subseteq[d] \\ |T|\leq k}} \ind({T\subseteq S})\cdot \circphi_T \Big)
% f(S) - \sum_{\substack{\hspace{0.25em} T\hspace{0.25em}\subseteq[d] \\ |T|\leq\hspace{0.25em} k}} \ind({T\subseteq S})\cdot \circphi_T 
f(S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \ind({T\subseteq S})\cdot \circphi_T 
\Big|^2
\bigg]
\bigg\}
\label{eqn:faithshap_variational_SHAP_k_index}
\end{align}
Once again, we can see parallels between the GAM-k model from Equation \ref{eqn:fnl_GAM_k}.
The case of $k=1$ indeed reduces to the original least squares solution in Equation \ref{eqn:charnes_first_variational_SHAP_least_squares}.





% \subsection{Instant SHAP}
\section{Instant SHAP}
% As alluded to before,
% if we are interested in the higher-order interactions which cannot be covered by the Shapley value,
% then we may be interested in the FastSHAP functional amortization of the new FaithSHAP value,
% which can be written as folows:

A simple combination of these two ideas (functional amortization and feature interaction) would lead to an explainer which automatically recovers the two-dimensional or higher-dimensional Faith-SHAP explanations of the target function, while maintaining the practical speedups of FastSHAP: 
\begin{align}
% \argmin_{\circphi:\cX\to\bbR^{\red{N_k}}} 
\argmin_{\circphi:\cX\to\bbR^{\scalebox{0.55}{$\cI_{\leq k}$}}}
\bigg\{
\bbE_{x\sim p(x)} \bigg[
\bbE_{S\sim p^{\text{SHAP}}(S)} \bigg[
\Big\| 
f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \ind({T\subseteq S})\cdot \circphi_T(x)
\Big\|^2
\bigg]
\bigg]
\bigg\}
\label{eqn:fastshap_plus_faithshap_functional_variational_SHAP_k_index}    
\end{align}
This functional amortization automatically recovers the Faith-SHAP-k values as defined in \cite{tsai2023faithSHAP},
but maintains the practical advantages and speedups of FastSHAP from \cite{jethani2022fastSHAP}.


% In our experiments,
% we investigate how such an extension may utilzie the same functional amortization procedure in order to get test time explanations which not only give the 1D SHAP information but also give the 2D or higher FaithSHAP-k information which respects the feature interactions which may exist.


Instead, however, we propose to adapt the typical variational equations used to fit GAM models to fall under this same variational Shapley framework.
We first make explicit the variational equation used to train GAM models as:
\begin{align}
% \argmin_{\circphi:\cX\to\bbR^{N_k}} 
\argmin_{\circphi:\cX\to\bbR^{\scalebox{0.55}{$\cI_{\leq k}$}}} 
\bigg\{
\bbE_{x\sim p(x)} \bigg[
\bbE_{S\sim p^{\text{GAM}}(S)} \bigg[
\Big\| 
% f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \ind({T\subseteq S})\cdot \circphi_T(x_T)
f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \circphi_T(x_T)
\Big\|^2
\bigg]
\bigg]
\bigg\}
\label{eqn:GAM_variational_equation}
\end{align}


Accordingly, we define the Insta-SHAP-GAM as an additive model which is trained as:
\begin{align}
% \argmin_{\circphi:\cX\to\bbR^{N_k}} 
\argmin_{\circphi:\cX\to\bbR^{\scalebox{0.55}{$\cI_{\leq k}$}}}
\bigg\{
\bbE_{x\sim p(x)} \bigg[
\bbE_{S\sim p^{\text{SHAP}}(S)} \bigg[
\Big\| 
f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \ind({T\subseteq S})\cdot \circphi_T(x_T)
\Big\|^2
\bigg]
\bigg]
\bigg\}
\label{eqn:InstaSHAP_variational_equation}
\end{align}
where first we introduce the masked training objective where $S$ are drawn from the Shapley kernel and
second we add the masking on each additive component of the GAM to only be included if all features of that component are unmasked in $S$.

This new formulation is able to bring novel insights to both the literature on SHAP and the literature on GAM.
For SHAP, we incorporate the low-dimensional GAM bias which is able to more accurately model SHAP values when compared with FastSHAP.
% Additionally, by the explicit modeling of interactions provided by GAM models, we are able to improve on the practical feasability of feature interaction methods like FaithSHAP, which have yet to have a method developed for their practical deployment.
Additionally, by the explicit modeling of interactions, we are able to improve on the practical feasability of methods like FaithSHAP, which have yet to develop a method for practical deployment.
% Finally, 
For the GAM literature, we make progress towards the longstanding goal of purification of the shape functions of additive models.
In the appendix, we further detail how this extends on the existing GAM literature and its various applications towards the selection of feature interactions under correlated inputs.









% \section{Experiments}

\section{Tabular Experiments}



\subsection{Synthetic Experiments}

We construct a simple dataset to test the varying effects of synergistic interactions and redundant interactions in a similar {spirit to} Figure \ref{fig:simple_2D_example_Shapley_fnl_calculation_synergy_and_redundancy}.
We provide additional details in the Appendix, but we use a simple ten-dimensional feature space with an algebraic GAM rank of $k*$ and a correlation of $\rho$. 
In Figure \ref{fig:fast_shap_vs_insta_shap_error_curves},
we see the approximation results consistently showing InstaSHAP has a better inductive bias than FastSHAP for learning the Shapley values.



\begin{figure}[h]
    \centering
    \includegraphics[width=0.24\linewidth]{figures/synth/k_1_rho_0.0000_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.24\linewidth]{figures/synth/k_1_rho_0.7071_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.24\linewidth]{figures/synth/k_2_rho_0.0000_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.24\linewidth]{figures/synth/k_2_rho_0.7071_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \caption{MSE error of approximations of the model's SHAP values.  Since both FastSHAP and InstaSHAP are functional approximations, we report the MSE errors across the epochs of training. }
    \label{fig:fast_shap_vs_insta_shap_error_curves}
\end{figure}





%OLD VERSION
% \begin{figure}
%     \centering
%     % \includegraphics[width=0.96\columnwidth]{icml2024/figures/bikeshare_gam_shape.pdf} \\
%     % \includegraphics[width=0.96\columnwidth]{icml2024/figures/bikeshare_SHAP_fn.pdf} 
%     % \includegraphics[width=0.96\columnwidth]{icml2024/figures/bikeshare_full_fn.pdf} 
%     \includegraphics[width=0.32\columnwidth]{icml2024/figures/bikeshare_gam_shape.pdf}
%     \includegraphics[width=0.32\columnwidth]{icml2024/figures/bikeshare_SHAP_fn.pdf} 
%     \includegraphics[width=0.32\columnwidth]{icml2024/figures/bikeshare_full_fn.pdf} 
%     \caption{Shape Function Plots for the BikeShare Dataset. \\
%     (a) The shape function learned for the interpretable GAM for its dependence on hour of the day and weekday type.
%     (b) The SHAP function for hour calculated automatically from the learned GAM. \\
%     (c) The true function for the entire dataset, plotted in the same fashion as the shape function and Shapley function. \\
%     Note the decreasing levels of precision as one goes from the shape function learned by the interpretable additive model, to the black-box explanations of the same model in terms of the Shapley function, and finally to the predictions themselves made by the additive model. (Note that the categorical variable `Hour' is splayed for visualization purposes.)
%     }\label{fig:bikeshare_PDP}
% \end{figure}


% \subsection{Bike Share}
% \subsection{Real-World Synergy in Bike Sharing}
\subsection{Synergy in Bike Sharing}
In the bike sharing dataset, we find strong evidence of a synergistic interaction effect.
This dataset predicts the expected bike demand each hour given some relevant features like the day of the week, time of day, and current weather.
There is a total of thirteen different input features predicting a single continuous output variable.

In the case of a multi-layer perceptron trained to predict the bike demand, the normalized mean-squared error ($R^2$) results is $6.59\%$.
Although a GAM-1 can only achieve an $R^2$ of $17.4\%$, a low-dimensional GAM can achieve an $R^2$ of $6.23\%$.
% \red{
% After training a GAM model on the first 20 selected indices, the additive model can achieve a normalized MSE of $6.23\%$.
% }
% GAM-1 is $17.4\%$ $R^2$ error
% This demonstrates that our frontier selection algorithm can easily allow GAM models to match the performance of state-of-the art approaches, even with as few shapes as 20 selected indices.
It is well-known that on this dataset there is a strong interaction between the hour variable and workday variable (since people's schedules change on the weekend vs. a workday.)
In Figure \ref{fig:bikeshare_spectrum_of_interpretability}a,
we can see how this feature interaction is adequately captured.
In Figure \ref{fig:bikeshare_spectrum_of_interpretability}b,
we can see how if the SHAP value also makes this trend identifiable, but does not completely separate this bivariate interaction from other feature effects in the dataset.
Overall, Figure \ref{fig:bikeshare_spectrum_of_interpretability} supports the hypothesis that training interpretable models is a better path than explaining blackbox models, especially when the same accuracies can be achieved.


% In Figure \ref{fig:bikeshare_PDP},
% we can see the learned shape functions from the additive model and how it compares with both the Shapley value at each point, but also the full prediction at each point.
% This helps not only provide a visualization of the connection between GAM and SHAP through \ref{fig:bikeshare_PDP}a and \ref{fig:bikeshare_PDP}b, but also the spectrum of interpretability to explainability to complete black box.



\begin{figure}[h]
    \centering
    % \includegraphics[width=0.32\columnwidth]{icml2024/figures/bikeshare_gam_shape.pdf}
    % \includegraphics[width=0.32\columnwidth]{icml2024/figures/bikeshare_SHAP_fn.pdf} 
    % \includegraphics[width=0.32\columnwidth]{icml2024/figures/bikeshare_full_fn.pdf} 
    \includegraphics[width=0.325\columnwidth]{figures/bikeshare_and_treecover/sep2024_bikeshare_gam_shape.png}
    \includegraphics[width=0.325\columnwidth]{figures/bikeshare_and_treecover/sep2024_bikeshare_SHAP_fn.png} 
    \includegraphics[width=0.325\columnwidth]{figures/bikeshare_and_treecover/sep2024_bikeshare_full_fn.png} 
    \caption{The Spectrum of Interpretability to Uninterpretability. 
    We display the key \{hour, workday\} interaction for the interpretable GAM, explainable SHAP, and uninterpretable blackbox.
    }
    \label{fig:bikeshare_spectrum_of_interpretability}
\end{figure}












% \subsection{Tree Cover}
% \subsection{Real-World Redundancy in Tree Cover Type}
% \subsection{Redundancy in Tree Cover Type}
% \subsection{Redundancy in Tree Covertype}
\subsection{Redundancy in Tree Cover}
% The dataset consists of predicting the types of trees covering a specific forest area from a selection of 7 tree species (Spruce, Lodgepole Pine, Ponderosa Pine, Cottonwood, Aspen, Douglas-Fir, or Krummholz) in a Colorado national park based on
% % 10 numerical features and 1 categorical feature of the area.
% 11 features of the area.
% Simple investigation with feature importance methods or the methods described herein can determine that two features which are the most critical for determining the species:

In the treecover dataset, we find strong evidence of a redundant interaction effect. 
This dataset consists of predicting one of the seven types of tree species which are covering a given plot of land based on eleven input features describing the area.  
Simple investigation can determine the most important features for determining the species of tree are the 
altitude of the land and soil category of the land.
Accordingly, we provide their partial dependence plots in Figure \ref{fig:treecover_elev_and_or_soil_PDP}a and \ref{fig:treecover_elev_and_or_soil_PDP}b.


% \begin{figure}
% \centering
% \begin{minipage}[b]{.43\textwidth}
%     \centering
%     % \includegraphics[width=0.76\textwidth]{icml2024/figures/ELEV_five_averaged_pdp.pdf} 
%     % \includegraphics[width=0.76\textwidth]{icml2024/figures/SOIL_five_averaged_pdp.pdf} 
%     \includegraphics[width=0.88\textwidth]{icml2024/figures/ELEV_five_averaged_pdp.pdf} 
%     \includegraphics[width=0.88\textwidth]{icml2024/figures/SOIL_five_averaged_pdp.pdf} 
%     \caption{1D Dependence of Tree Species on Altitude \textbf{or} Soil}
%     \label{fig:elev_or_soil_PDP}
% \end{minipage}\qquad
% \begin{minipage}[b]{.43\textwidth}
%     \centering
%     % \includegraphics[width=0.84\columnwidth]{icml2024/figures/ELEV_SOIL_five_averaged_pdp.pdf}
%     % \includegraphics[width=0.44\columnwidth]{icml2024/figures/tree_legend_resized.pdf} 
%     \includegraphics[width=0.98\columnwidth]{icml2024/figures/ELEV_SOIL_five_averaged_pdp.pdf}
%     % \includegraphics[width=0.44\columnwidth]{icml2024/figures/tree_legend_resized.pdf} 
%     % \vspace{1.46cm}
%     %
%     \\ \vspace{0.60cm}
%     \includegraphics[width=0.54\columnwidth]{icml2024/figures/tree_legend_resized.pdf} 
%     \vspace{0.35cm}
%     \caption{2D Dependence of Tree Species on Altitude \textbf{and} Soil}
%     \label{fig:elev_and_soil_PDP}
% \end{minipage}
% \end{figure}

\begin{figure}[h]
\centering
% \begin{minipage}[b]{.43\textwidth}
    % \centering
    % \includegraphics[width=0.76\textwidth]{icml2024/figures/ELEV_five_averaged_pdp.pdf} 
    % \includegraphics[width=0.76\textwidth]{icml2024/figures/SOIL_five_averaged_pdp.pdf} 
    \includegraphics[width=0.22\textwidth]{icml2024/figures/ELEV_five_averaged_pdp.pdf} 
    \includegraphics[width=0.222\textwidth]{icml2024/figures/SOIL_five_averaged_pdp.pdf} 
    % \caption{1D Dependence of Tree Species on Altitude \textbf{or} Soil}
    % \label{fig:elev_or_soil_PDP}
% \end{minipage}\qquad
% \begin{minipage}[b]{.43\textwidth}
    % \centering
    % \includegraphics[width=0.84\columnwidth]{icml2024/figures/ELEV_SOIL_five_averaged_pdp.pdf}
    % \includegraphics[width=0.44\columnwidth]{icml2024/figures/tree_legend_resized.pdf} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    \includegraphics[width=0.245\columnwidth]{icml2024/figures/ELEV_SOIL_five_averaged_pdp.pdf}
    % \includegraphics[width=0.44\columnwidth]{icml2024/figures/tree_legend_resized.pdf} 
   \includegraphics[width=0.225\columnwidth]{figures/bikeshare_and_treecover/sep2024_ELEV_SOIL_five_averaged_pdp.png}
    % \vspace{1.46cm}
    %
    % \\ \vspace{0.60cm}
    % \quad
    % {
    % \includegraphics[width=0.135\columnwidth]{icml2024/figures/tree_legend_resized.pdf} 
    % % \vspace{0.50cm}
    % }
    \quad
    \includegraphics[width=0.135\columnwidth]{figures/bikeshare_and_treecover/tree_legend_resized_extended_vertically_lil_less.pdf} 
    \caption{1D and 2D Dependence of Tree Species on Altitude {and} Soil}
    \label{fig:treecover_elev_and_or_soil_PDP}
% \end{minipage}
\end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.38\columnwidth]{icml2024/figures/ELEV_five_averaged_pdp.pdf} 
%     \includegraphics[width=0.38\columnwidth]{icml2024/figures/SOIL_five_averaged_pdp.pdf} 
%     \caption{1D Dependence of Tree Species on Altitude \textbf{or} Soil}
%     \label{fig:elev_or_soil_PDP}
% \end{figure}
% \begin{figure}
%     \centering
%     % \includegraphics[width=0.85\columnwidth]{icml2024/figures/ELEV_SOIL_five_averaged_pdp.pdf}
%     % \includegraphics[width=0.44\columnwidth]{icml2024/figures/tree_legend_resized.pdf} 
%     \includegraphics[width=0.42\columnwidth]{icml2024/figures/ELEV_SOIL_five_averaged_pdp.pdf}
%     \includegraphics[width=0.22\columnwidth]{icml2024/figures/tree_legend_resized.pdf} 
%     \caption{2D Dependence of Tree Species on Altitude \textbf{and} Soil}
%     \label{fig:elev_and_soil_PDP}
% \end{figure}


However, a cursory look at the 1D dependence such as these plots or SHAP ignores the fact that both the elevation and soil type are additionally correlated with one another.
Indeed, some montane-type soils can only be found in lower altitudes and, equally, alpine-type soils can only be found at higher altitudes.
Looking at the 2D heatmap in Figure \ref{fig:treecover_elev_and_or_soil_PDP}c,
we can see that soil and altitude are correlated with one another and somewhat redundantly  predict the joint trend in the species of tree.



% However, this misses the fact that both the elevation and soil type are additionally correlated with one another.
% Indeed, the soils are grouped according to climatic zone which generally correspond to different altitude climates.
% For convenience, we keep these soil classes in the same orders as their expected elevation, named: `lower montane', `upper montane', `subalpine', and `alpine'.
% One notes that the Krummholz tree can be found at high altitudes but also in alpine (often rocky) soil.
% Similarly, Cottonwoods, Douglas-firs, and Ponderosas are expected to be found at lower altitudes, but also to be found in montane soils.
% Without an understanding that these two features are correlated with one another, it might a priori seem like these are two independent contributions to the prediction.
% Yet again, it turns out that these two facts are indeed correlated with one another and {hence the 1D projections} alone may not be sufficient to yield a good explanation.

% In this case, a lot can be gleaned by viewing the 2D shape function which depends on both the soil and the elevation.
% In Figure \ref{fig:elev_and_or_soil_PDP}, we visualize this 2D shape function as a scatterplot with colored heatmap.
% Through the density of points, we can see there is indeed a strong positive correlation between the soil type and the elevation.
% Furthermore, using the colors for each tree species, we can see that there is a lot of redundant information carried by both the soil and the elevation, but also that there is some non redundant information.




% \begin{figure}
%     \centering
%     \includegraphics[width=0.89\columnwidth]{icml2024/figures/ELEV_SOIL_but_ONLY_ELEV_five_averaged_pdp.pdf}
%     \includegraphics[width=0.89\columnwidth]{icml2024/figures/ELEV_SOIL_but_ONLY_SOIL_five_averaged_pdp.pdf}
%     \caption{2D Partial Dependence of Tree Species on Altitude \textbf{or} Soil}
%     \label{fig:elev_and_soil_PDP}
% \end{figure}


Training an MLP on this dataset is able to achieve validation accuracy of $80.4\%$ whereas a GAM-1 can only achieve $72.4\%$ accuracy.
Alternatively, a low-dimensional GAM is instead able to achieve $82.2\%$ accuracy.
This once again demonstrates that although the 1D SHAP is unable to accurately represent this tabular dataset,
a simple low-dimensional GAM is able to as well.

% After applying our frontier selection algrorithm to choose a set of $50$ interaction subsets, 
% a GAM is able to achieve $82.2\%$ accuracy.
% This demonstrates that using these techniques to train a GAM can result in models which are matching state-of-the-art performance to blackbox models while simultaneously allowing for instantaneous Shapley value explanations as part of the model.


% In the presence of many existing works showing the SOTA capabilities of GAM models on tabular datasets,
% we feel \jam{this is sufficient}
Beyond the datasets we study here, there are many existing works showing that both (a) feature interactions are necessary for real-world datasets, and (b) relatively low-dimensional GAM models can often achieve SOTA performance on tabular datasets \citep{chang2022nodegam,enouen2022sian}.
Accordingly, after the clear demonstration of both categories (synergy and redundancy) of feature interactions in practice, we move on to exploring the same phenomenon in higher-dimensional data.






\section{Higher Dimensional Experiments}
% \subsection{Vision Experiments}


We additionally use our methods to explore a bird classification task on natural images.
We run experiments on the CUB dataset using a resnet CNN architecture, evaluating not only at the original, fine-grained 200 classes corresponding to each bird's species, but also with 37 coarse-grained classes corresponding to each bird's family.
% We evaluate the CUB dataset at both the `fine-grained' species level (200 classes) and the `coarse-grained' family level (37 classes).
For our GAM models we adapt the resnet architecture to only include the influence from a $1\times 1$, $2\times 2$, or $3\times 3$ set of patches, 
leading to a GAM-1, GAM-4, and GAM-9 model.
We fine-tune all models with procedures similar to \cite{covert2023shapleyForVIT}, see appendix for details.



\begin{figure}[h]
    \centering
    % \includegraphics[width=0.48\linewidth]{figures/birds/placeholder_bird_figure_1.png}
    % \includegraphics[width=0.48\linewidth]{figures/birds/placeholder_bird_figure_2.png}
    % \caption{\red{Placeholder Figure} which shows how the explanations of SHAP and GAM do not agree with one another, which implies that there is some oversmoothing or some hidden interaction effects which exist in the true Shapley value.}
    % \includegraphics[width=0.88\linewidth]{figures/birds/iclr 2024 eight different birds with SHAP.pdf}
    \includegraphics[width=0.88\linewidth]{figures/birds/iclr_2024_eight_different_birds_with_SHAP.pdf}
    \caption{Display of SHAP explanations for multiple images inside of the CUB birds dataset.
    Explanations are provided for the CNN model as well as multiple GAM models.
    We can see that beyond the gap in accuracy between the CNN and GAM models, there are also discrepancies in the reasoning processes as explained by SHAP.
    This can be taken as strong evidence of oversmoothing of the interaction effects which are used by the original CNN model.}
    \label{fig:CUB_with_SHAP_across_multiple_different_species}
\end{figure}

Fine-tuning a resnet-50 model on the dataset, we are able to achieve a fine-grained accuracy of $65.0\%$ and a coarse-grained accuracy of $81.8\%$.
In contrast, the GAM-1 model is only able to achieve a $33.2\%$ fine and $53.7\%$ coarse accuracies.
This can be taken as strong evidence that SHAP is oversimplifying the behavior of the CNN on this dataset.
In Figure \ref{fig:CUB_with_SHAP_across_multiple_different_species},
we can see that this also manifests as sizable visual differences in the explanations between the two models.
In some sense, at least $20$ percentage points of accuracy are being completely thrown away when using the simplification of SHAP.
So although a useful first approximation of the understanding, SHAP is once again not sufficient for giving a complete understanding of the behavior of feature interactions.


Extension to the GAM-2x2 and GAM-3x3 models is able to give improvements to $45.8\%$ and $46.8\%$ fine as well as $66.3\%$ and $66.8\%$ coarse accuracies.
Even with some feature interactions, a convolutional GAM model is unable to achieve the same  accuracies as a resnet.
This points to the fact that either long-range or higher-order interactions are necessary to completely match the performance of the resnet.
It is also possible that an alternate GAM architecture would be able to further improve upon these accuracy results, helping close the gap between CNN and GAM.
% However, if
% such limitations were to exist, they would be present throughout all existing alternative approaches, including FastSHAP.
%
Overall, we take this as significant evidence towards the oversimplification of SHAP explanations on high-dimensional data,
domains where the need for explainability also remains the highest.



% We take all of these as pieces of evidence towards the hypothesis that higher-order interactions are a fundamental aspect of real-world ML datasets.

% Moreover, given the nature of computer vision data and the \red{nature of the explanations},
% we further hypothesize that this could be heavily influence by the high correlations which exist in image datasets.
% Without a more adequate handling, it may be hard to address interpretability considerations in these higher-dimensional tasks like CV and NLP.









% \section{Discussion}
% Altogether,
% we find that SHAP=GAM
% and people using SHAP so religiously need to pause and remember what they are doing






% \section{REMOVE BECAUSE DISTRACTING !}



% \section{Introduction}
% As deep learning and other black-box approaches continue to dominate the fields of machine learning and artificial intelligence with state-of-the-art results across nearly all domains,
% the desire to be able to explain the decisions made by black-box systems continues to grow.
% This appetite is made increasingly pertinent as the number of stakeholders, both inside the technology sector as well as within the general populace, influenced by AI decisions grows steadily day after day.
% Since its major introduction to explanation of black-box models in \cite{lundberg2017shapleySHAP},
% the Shapley value has become a staple for providing explainability of trained AI models, 
% built upon the long-standing foundations of Lloyd Shapley's seminal work in game theory \cite{shapley1953shapley}.
% In light of the strong theoretical support of Shapley or SHAP explanations,
% one of their main drawbacks is their computational feasibility.
% Their time complexity scales poorly not only with the amount of data but also with the number of features collected, often becoming insurmountable for computing the Shapley value at industry scales.


% Accordingly,
% a large amount of recent work on the Shapley value and other removal-based explanations has largely focused on theoretical or practical speedups to computing the Shapley value in ever-larger settings \cite{jethani2022fastSHAP,covert2023shapleyForVIT,kolpaczki2024approximatingShapleyWOmarginalContributions,zhang2023efficientSamplingForShapley}.
% Despite a significant amount of progress in speeding up explanations across a variety of domains, usage of the Shapley value remains limited in some applications due to its time complexity.


% In parallel to the growing popularity of SHAP explanations, there has been a rise in popularity of the interpretable models called generalized additive models (GAMs), also rooted in historical work and gaining recent popularity \cite{hastie1990originalGAM,lou2013accurate,agarwal2020nam,chang2022nodegam}.
% Whereas explainability comes from the top-down (distilling a complex model into a simpler explanation), interpretability builds from the bottom-up (always allowing for glass-box understanding of its mechanisms) \cite{rudin2019stopExplaining}.
% This duality is paralleled with the case of SHAP explanations and GAM interpretable models.
% Despite the intimate connection between these two approaches, seemingly little work has investigated their relationship, with only \cite{bordt2023shapleyToGAMandBack} looking at how to compute Shapley functions and GAM shape functions from one another.
% \jam{nothing handling feature correlations}
% %surely I can find more papers doing this
% In particular, when accounting for how feature interactions affect Shapley values, one can find a direct correspondence between the Shapley functions and GAM functions.

% \jam{horrible transition}
% Recent work in both SHAP and GAM literature have been further addressing feature interactions which are partially overlooked by the original Shapley value
% % Other recent work has also been addressing the feature interactions which are partially overlooked by the original Shapley value
% \cite{fumagalli2023shapIQ,tsai2023faithSHAP}.
% %%%\jam{I don't know if this is the best work critiquing Shapley}
% %%%Recent work has even critiqued SHAP \cite{huang2023inadequacyOfSHAP} because of its native failings in identifying feature interactions, causing the Shapley value to be zero even when a feature is important.
% %%% possibly the line of text that pissed off reviewer #1, lol....
% %
% Recent work has even critiqued SHAP
% \cite{huang2023inadequacyOfSHAP,beenKim_one} because of its failings to handle certain use cases.
% This is partially due to SHAP's native failings in identifying feature interactions, causing the Shapley value to be zero even when a feature is important.
% \jam{It could be critiqued that such works are not `expecting' the right thing out of the SHAP value. It is hoped that by enhancing the connection between GAM and SHAP can help further calibrate what should be expected from SHAP scores.}
% %
% %
% Despite the abundance of existing work addressing SHAP and adressing feature interactions, the study of their interplay in the setting where input features are not completely independent from another seems to be completely missing from the current literature.
% \jam{put this earlier}
% We hope that in this work, we can not only provide greater intuition on what Shapley explanations provide and hence when they should or should not be adequate, but also that we can make significant progress towards a longstanding challenge in the additive modeling community with 
% the automatic purification of additive models.
% We provide several contributions in way of training additive models and computing Shapley values in the correlated variables case, providing insight into the interplay between feature interactions and feature correlations in machine learning:

% \begin{itemize}
%     \item Provide novel theoretical insights on generalized additive models and further connect these results with recent developments in the theory of Shapley values.
%     \item Develop a frontier selection algorithm which is able to solve the meta-optimization problem of selecting feature interactions for additive models to match deep learning performance, inspired by difference in functional ANOVA theory for the correlated variables case.
%     \item Introduce a purified loss function for training additive models which attains the \jam{elusive} purification of additive models automatically during the training procedure.
% \end{itemize}


% \section{Related Work}
% The Shapley value was first introduced more than half a century ago, originally in its application to equally distributing the wealth earned by a coalition amongst its constituent members.
% The SHAP value has repurposed the original Shapley value to distribute the prediction made by a blackbox algorithm amongst its contributing input features.
% With some original studies in \cite{lipovetsky2001regressionViaShapleyApproach,strumbelj2014explainingWithFeatureContributions,datta2016transparencyViaQuantInpInfluence}, most work picked up after the algorithm introduced in 
% \cite{lundberg2017shapleySHAP}.
% % In this way, SHAP allows to explain any black-box model which can be converted into a masked prediction model,
% % obeying some very pleasant theoretical properties it inherits from the Shapley value.


% Its immediately obvious disadvantage is that of its computational complexity.
% Exact computation requires an exponential number of function evaluations and even approximations can take hundreds or thousands of repeated function calls.
% Consequently, an abundance of follow-up work has improved the computational complexity via
% better algorithms, architecture specific algorithms, or efficient sampling estimators \cite{lundberg2018treeShapInteractions,lundberg2020treeSHAP,chen2018_LandC_shapley,zhang2023efficientSamplingForShapley,kolpaczki2024approximatingShapleyWOmarginalContributions}.
% %better alg
% %arch specific algs
% %general good 

% \jam{terrible writing..., terrible structure}
% Another major consideration of SHAP which has come under scrutiny besides its computational complexity is its inability to adequately handle feature interactions.
% A groups of variables can be jointly important to a prediction, many works have tried to focus on extending Shapley values to additionally handle the study of feature interactions \cite{grabisch1999originalShapleyInteractionIndex,sundararajan2020shapleyTaylorInteractionIndex,tsai2023faithSHAP}.
% In many cases, the extension of variable importance to variable group importance is believed to cover the shortcomings in existing applications of the Shapley value. 
% %%% In many cases, it is believed that such extensions can cover the shortcomings which may exist in certain applications of the Shapley value \cite{huang2023inadequacyOfSHAP}



% The generalized additive model (GAM) \cite{hastie1990originalGAM,wahba1994ssanova} has also existed for decades as a more expressive alternative to linear regression.
% Although some interest has remained over the years 
% \cite{hooker2007functionalANOVA,lou2013accurate,caruana2015intelligible,kandasamy16salsa},
% machine learning is often dominated by kernel machines, boosted models, and deep learning.
% % a lot of focus shifted towards other machine learning approaches.
% In recent years, however,
% there has been an explosion of additive models trained with neural network approaches
% as the appetite for interpretable models continues to grow in the deep learning era
% \cite{yang2020gamiNet,chang2022nodegam,agarwal2020nam,enouen2022sian,xu2022snam}.

% Alongside the revived attention to additive models, some methods have shifted away from the classical approach of truncating after all 1D and 2D functions \cite{wahba1994ssanova,lou2012intelligible}, instead opting for high-dimensional sparsity and/or higher-order functions \cite{kandasamy16salsa,xu2022snam,yang2020gamiNet,enouen2022sian} to provide competitive performance across a wide variety of datasets and tasks.
% Additional interest in feature interactions has also existed here trying to measure the importance of feature interactions within additive models \cite{hao2014interaction,tsang2020archipelago,chen2023deepROCK}

% Despite the parallel interest in focusing on feature interactions in both SHAP and GAM, much of these works have remain isolated from one another.
% A key exception of this rule is \cite{bordt2023shapleyToGAMandBack} which harps on the same connection between SHAP and GAM which we leverage herein.
% Their work, however, is limited to the case of independent features and does not enhance the training of additive models, especially in the case of correlated features. \jam{phrasing}
% Our work is also closely related to the work of Faith-SHAP \cite{tsai2023faithSHAP}, which recently made great strides in developing the study of Shapley interaction indices, and Fast-SHAP \cite{jethani2022fastSHAP}, which recently introduced a novel amortization approach to computing the Shapley value with auxiliary models.
% In comparison with Faith-SHAP, we extend their pointwise definitions of the Shapley indices to a functional which corresponds to the training of an additive model.
% In comparison with FastSHAP, we train explanatory models which are themselves the model of interest, as well as automatically handle interaction indices which the original work is unable to do.
% Moreover, while their auxiliary model is only an approximation of the original model's Shapley value, we train interpretable models which match or exceed the performance of deep neural networks, and hence give an exact Shapley value of the interpretable model.

% % \jam{despite this parallel interest in focusing on feature interactions in both SHAP and GAM, much of these works remain separate and isolated}


% % This connection to additive models has been briefly explored in the (vastly simpler) settings where input features are independent \cite{}.
% % However, these works overlook many of the important hurdles in applying additive models into more typical machine learning pipelines with correlated input features and the associated challenges of learning Shapley values in these settings.

% % The most closely related work is FastSHAP \cite{jethani2022fastSHAP}
% % which also learns some type of additive model to accelerate explanations via the Shapley value.
% % The biggest key differences are (a) FastSHAP does not learn an interpretable model like InstaSHAP and hence provides only an approximation of the Shapley value; and (b) FastSHAP does not extend their results to feature interactions and the resulting Shapley interaction indices.
% %One could imagine that such an endeavor could be difficult



% \section{Technical Background}
% \paragraph{Notation.}
% Let $\cX\subseteq\bbR^d$ and $\cY\subseteq\bbR^c$ be the input and output space, and consider some function space $\cH$ containing functions $F : \cX \to \cY$.

% Let $[d] := \{1,\dots,d\}$.
% Let $S\subseteq[d]$ and let $S^C$ denote its complement. 
% For $x\in\cX$, let $x_S$ denote $\{x_i\}_{i\in S}$.
% We will abbreviate set notations throughout: $S+T = S\cup T$, $S+i = S\cup\{i\}$, $S-T=S\setminus T$, and $S-i = S\setminus \{i\}$.

% We will then let $\cS:=\cP([d]) \cong \{0,1\}^d$ be the power set of $[d]$, and consider masked functions or surrogate functions of the form $f : \cX\times\cS \to \cY$.
% % with functional space $\cG$.






% \subsection{Functional ANOVA and Additive Models}
% The generalized additive model (GAM) is the additive approximation of a function via:
% \begin{align}
%     F(x) = f_\emptyset + \sum_{i} f_i(x_i) + \sum_{i,j} f_{i,j}(x_i,x_j) + \dots
%     \label{eqn:gam_via_ellipses}
% \end{align}
% often truncated to some `order' $\ell \leq d$.
% Most commonly $\ell$ is taken to be either 1 or 2 \cite{hastie1990originalGAM,agarwal2020nam,chang2022nodegam}.
% This choice is both one of historical limitation on computation power, but also for interpretability reasons.
% In particular, 1D and 2D functions may be visualized by partial dependence plot and heatmap respectively.
% This has led additive models to become one of the largest class of interpretable models used for machine learning.

% Recent works have pushed towards investigating `higher-order feature interactions', corresponding to features sets of size 3D and larger \cite{yang2020gamiNet,?,enouen2022sian}.
% We can consider these more general additive models by first choosing a candidate set of interactions $\cI\subseteq\cP([d])$ and then writing the similar equation:
% \begin{align}
%     F(x) = \sum_{S\in\cI} f_S(x_S)
%     \label{eqn:gam_via_subsets}
% \end{align}

% Once again, the order will be the size of the largest subset $\ell = \max\{ |S| : S\in\cI \}$.
% This formulation of an additive model as a sum over candidate interactions immediately begs the question of how to select the correct set of feature interactions to include into the additive model.
% In the case of independent input variables, \cite{sobol2001globalSensitivity} provides a precise answer to this question via what is called the `Sobol-Hoeffding' or `functional-ANOVA' decomposition.

% First, consider the conditional expectation:
% \[
% [\cM_p \circ F](x,S) := \Big{\bbE}_{X_{S^C}\hspace{0.2em}\sim\hspace{0.2em} p(X_{S^C} | X_S = x_S)}\bigg[ F(x_S,X_{S^C}) \bigg]
% \]
% We may define the purified functions 
% \[
% \Tilde{f}(x,S) := \sum_{T\subseteq S} (-1)^{|S|-|T|} f(x,T)
% \]
% which we also write $f_S(x)=$$f(x,S)$ and $\Tilde{f}_S(x)=$$\Tilde{f}(x,S)$.

% In the space of independent input distributions,
% these functions are paraded for having the wonderful `decomposition of variance' property which implies independent contributions from the variances of each purified function.
% \[
% \label{eqn:sobel_decomp_of_var}
% \bbV_X[F(X)] = \sum_{S\subseteq[d]} \bbV_{X_S}[\Tilde{f}(X,S)]
% \]
% % \[
% % \bbE_X[f(X)^2] = \sum_{S\subseteq[d]} \bbE_{X_S}[\Tilde{f}(X,S)^2]
% % \]
% These independent contributions $V_S := \bbV_{X_S}[\Tilde{f}(X,S)]$, called the Sobol indices, laid the foundation for the field of global sensitivity analysis by describing the importance of each feature interaction $S\subseteq[d]$ \cite{sobol2001globalSensitivity}.

% Unfortunately, this equation breaks down for the case of correlated input variables, and there is no perfect alternative which describes the marginal benefit of a feature interaction across all possibilities.
% In Section \ref{sec:methods_frontier_selection}, we discuss how we instead approach estimating feature interaction importance instead of the typical Sobol indices.
% % \jam{add sobol covariances here?}


% \subsection{Shapley Functional}

% We will define the (conditional) Shapley value as a functional on the space $\cH$, rather than a function prescribed when given a fixed input value.

% For a fixed distribution $p(x)$,
% we first define the surrogate function $f$ associated with any typical function $F$ as the conditional expectation:
% \[
% [\cM_p \circ F](x,S) := \bbE_{X_{S^C} \sim p(X_{S^C} | x_S)}\bigg[ F(x_S,X_{S^C}) \bigg]
% % = f(x,S) 
% \]

% The Shapley functional can then be evaluated as a d-vector of functions via:
% \[
% [\circphi\circ f]_i (x) := \frac{1}{|\cS_d|} \mathlarger{\sum}_{\pi \in \cS_d} \bigg[  f(x; S_{\pi,i}+i) - f(x; S_{\pi,i}) \bigg]  
% \]
% where $\cS_d$ denotes the set of all permutations of $d$ with $|\cS_d| =d!$ and
% $S_{\pi,i}$ is the set of all predecessors of $i$ in $\pi$, $\{j\in[d] : \pi(j) < \pi(i)\}$.

% % \[
% % \phi_i (X) = \frac{1}{|\cS_d|} \mathlarger{\sum}_{\pi \in \cS_d} \bigg[  f(X; S_{\pi,i}+i) - f(X; S_{\pi,i}) \bigg]  
% % = \mathlarger{\mathlarger{\mathop{\bbE}}}_{\pi \sim \cS_d} \bigg[  f(X; S_{\pi,i}+i) - f(X; S_{\pi,i}) \bigg]
% % \] 

% Hence, $(\circphi\circ f)\in \cH^d$ and we may define the composite mapping $\Phi := \circphi\circ\cM_p$, with $\Phi : \cH \to \cH^d$ taking any function and returning its $d$ Shapley functions.

% % We may also define the interventional Shapley value with:
% We may also define the marginal Shapley value with:
% %by defining it on the orthogonalized probability pdf
% \[
% [\cN_p \circ F](x,S) := \Big{\bbE}_{X_{S^C}\hspace{0.2em}\sim\hspace{0.2em} p(X_{S^C}) }\bigg[ F(x_S,X_{S^C}) \bigg]
% \]
% as $\Phi^{\text{int}} := \circphi\circ\cN_p$.
% It should be remarked that under the assumption that independent input variables, these two notions of projection $\cM$ and $\cN$ actually align with one another, resulting in the same Shapley value definition.
% For heavily correlated datasets which are commonly encountered in practice,
% it is increasingly clear that the conditional or observational Shapley
% \cite{frye2021shapleyOnTheManifold,covert2021explainingByRemoving} is of greater practical value.
% Although some applications still exist for the `interventional' Shapley,
% we will focus on the `observational' Shapley throughout.
% Moreover, many of the challenges we overcome are unique to the case of correlated variables for the observational or conditional Shapley.



% %yyy the references are broken for some URLs
% \jam{citation for this sentence}
% \jam{remove the bracketed notation}
% The Shapley value is well-known to have the alternative formulation in terms of `unanimity games' as follows:
% \begin{align}
% [\circphi\circ f]_i (x) = \sum_{S\supseteq \{i\}} \frac{\Tilde{f}_S(x_S)}{|S|}
%     \label{eqn:shapley_unanimity}
% \end{align}

% In particular, we can see that from the perspective of the purified functions, 
% the Shapley value simply divides the purified contributions of each feature interaction evenly amongst the features which are its constituents.

% It is exactly this formulation which we will leverage to generate the Shapley value directly from an additive model in a single forward propagation.
% Further, this lays connection to the corresponding Shapley interaction indices of \citet{tsai2023faithSHAP} and we further discuss this connection in Section \ref{sec:methods_fast_shap_and_faith_shap}.


% \subsection{Feature Interactions}
% We introduce here further notation to help simplify the discussion in the case of feature interactions.
% Although the previous sections could additionally be translated into these notations, we opt instead to first provide the 1D definitions in a form most closely resembling their typical introduction.

% Let us first introduce the discrete derivative operator for a given interaction $S\subseteq[d]$:
% \[
% [\delta_S \circ f](x,T) := \sum_{W\subseteq S} (-1)^{|S|-|W|} f(x,(T-S)+W)
% \]
% In particular, we see that the purified functions are:
% \[
% \Tilde{f}(x,S) = [\delta_S \circ f](x,\emptyset)
% \]
% And Shapley is a weighted average of 1D derivatives:
% \[
% [\circphi\circ f]_i(x) =  \mathlarger{\sum}_{\pi \in \cS_d} \bigg[ \frac{1}{|\cS_d|} \cdot \Big[\delta_{\{i\}} f \Big](x,S_{\pi,i}) \bigg]  
% \]

% Let us now define some possible interaction indices, which prescribe a value to each interaction subset $S$ and not just each feature individually.
% We will delay a discussion of Faith-SHAP to Section \ref{sec:methods_fast_shap_and_faith_shap}, but one can consider the Shapley value as a 1D interaction index in this section as well.
% The inclusion and removal values are defined as 
% \begin{align}
%     [\circphi^{\text{inc}}_S\circ f](x) &:= [\delta_S \circ f](x,\emptyset) \\
%     [\circphi^{\text{rem}}_S\circ f](x) &:= [\delta_S \circ f](x,[d])
% \end{align}
% The Archipelago value \cite{tsang2020archipelago} is defined to be the average of these two $\circphi^{\text{arch}}_S := \frac{1}{2}\circphi^{\text{inc}}_S + \frac{1}{2}\circphi^{\text{rem}}_S$.
% \footnote{Archipelago is not originally defined in the masked regime, but rather with respect to a baseline sample.  Nevertheless, we feel this is a natural extension to the original definition.}
% In our experiments as in Algorithm \ref{alg:frontier_selection_algorithm},
% we primarily use the Archipelago value over the Shapley value because it is vastly quicker to compute.
% Moreover, we find that the Archipelago value does a good job of balancing the feature interactions and feature correlations, better than both the inclusion value (which overemphasizes feature correlations) and the removal value (which overemphasizes feature interactions).
% % \jam{do i have that backwards}




% % \section{Methods}

% \subsection{GAM Frontier Selection}
% \label{sec:methods_frontier_selection}
% In training GAM models,
% we must additionally solve the meta-optimization of selecting the subset of feature interactions we would like to include in our additive model.
% We will refer to the set of feature interactions $\cI := \{ S_1, \dots, S_L \} \subseteq \cP([d])$ as the `frontier' of the additive model.


% \begin{algorithm}[H]%[tb]
% \caption{Frontier Selection Algorithm}
% \label{alg:frontier_selection_algorithm}
% \begin{algorithmic}%[1] %[1] enables line numbers
% \STATE {\bfseries Input:} Trained model $f(x)$, validation dataset $X\in\bbR^{n\times d}$, $Y\in\bbR^{n\times c}$ for some number of samples $n\in\bbN$
% \STATE {\bfseries Hyperparameters:} $R\in\bbN$, number of rounds \\
% $k\in\bbN$, number of interactions per rounds \\
% $\tau\in[0,1]$, hierarchy threshold \\
% $\circphi$, feature interaction index (e.g. the Inlcusion value, Archipelago value, Shapley value)
% % \textbf{Output}: $\cI$, a family of feature interactions with index at most $K$ and strength above $\theta$
% \STATE {}
% \STATE Set $\cI \gets \{\emptyset\}$
% %\COMMENT{~detected interactions so far}
% \FOR{$r$ in $1,\dots,R$}
% \STATE Set $\cJ \gets$ $\textbf{CANDIDATES}(\cI,\tau)$
% \COMMENT{Alg. \ref{alg:build_frontier}}
% \FOR{$J$ in $\cJ$}
% \STATE $\hat{Y}_J = [\circphi\circ f]_J(X)$
% % \STATE $\hat{C}_J = \hat{\bbE}[ Y \cdot \hat{Y}_J ]$
% \STATE $\hat{C}_J = \frac{1}{n}[ Y^T \cdot \hat{Y}_J ]$
% % \COMMENT Empirical approximation of Sobol Covariance
% \ENDFOR
% \STATE $\cJ' \gets \text{argsort}(\hat{C}_J)$ 
% \STATE $\cJ' \gets \text{top-k}(\cJ',k)$ 
% \STATE $\cI \gets \cI \cup \cJ'$
% \ENDFOR
% \STATE \textbf{return} $\cI$
% \end{algorithmic}
% \end{algorithm}


% In the simpler case of independent input variables, we can see from the decomposition of variance in Equation \ref{eqn:sobel_decomp_of_var},
% that the inclusion of a feature interaction $S\subseteq[d]$ reduces the mean-squared error by exactly the Sobol index $V_S$ (assuming no statistical error).
% It follows that, given an estimate of the Sobol indices $V_S$, we may sequentially add feature subsets $S$ until a particular error tolerance or cross-validation performance is achieved.

% In the case of correlated input variables, however, it is no longer the case that Sobol indices are an appropriate measurement of the marginal contribution of a feature interaction's inclusion into the additive model.
% The typical extension of the Sobol indices to this case is via the Sobol covariances which may be defined $C_S := \bbE[ F(X) \cdot \Tilde{f}_S(X_S) ]$.
% These covariances still obey the desired decomposition equation; however, they no longer perfectly correspond to the marginal value of including a particular subset $S$.
% In fact, the marginal value of a subset $S$ now depends on the current frontier set $\cI$ which is being considered, unlike in the independent case.
% In Algorithm \ref{alg:frontier_selection_algorithm}, we utilize a partial approximation of the Sobol covariances to estimate the usefulness of including a particular subset $S$ in the additive model's frontier.
% A detailed discussion on the learned additive models coming from arbitrary frontiers with correlated features can be found in Appendix \ref{app_sec:novel_GAM_frontier_and_correlated}.

% % \jam{that we can measure these Sobol indices or even their correlated counterparts (to get the correct frontier value)}




% \begin{algorithm}[tb]
%    \caption{Build Frontier Candidates}
%    \label{alg:build_frontier}
% \begin{algorithmic}
% \STATE {\bfseries Input:} Current frontier set $\cI\subseteq\cP([d])$, \\
% hierarchy threshold $\tau\in[0,1]$
% \STATE {}
%    \STATE \COMMENT{\textit{One element fattening of all current feature subsets}}
%    \STATE $\cJ' \gets \{S + i : S\in\cI, i\in[d]\}$\quad
%    \STATE $\cJ \gets \emptyset$
%    \FOR{$J\in\cJ'$}
%    % \STATE $p_J = \frac{\cP(J) \cap \cI}{\cP(J)}$
%    \STATE \COMMENT{\textit{Ratio of $J$'s subsets which are already included out of the total possible $2^{|J|}$}}
%    \STATE $p_J = |\cP(J) \cap \cI| / |{\cP(J)}| $\quad
% \STATE {}
%    \IF{$p_J \geq \tau$}
%   \STATE $\cJ \gets \cJ \cup \{J\}$
%    \ENDIF
%    \ENDFOR
% \STATE \textbf{return} $\cJ$
% \end{algorithmic}
% \end{algorithm}

% \subsection{Fast SHAP and Faith-SHAP}
% \label{sec:methods_fast_shap_and_faith_shap}
% An alternative formulation of the Shapley value is through the solution to a mean-squared error optimization:
% \[
% \argmin_{\circphi_0\in\bbR, \circphi\in\bbR^d} \bigg\{
% % \big\bbE_{S\sim p(S)}\bigg[
% \big\bbE_{S}\bigg[
% \bigg| f(S) - \circphi_0 - \sum_{i=1}^d 1_{i\in S}\cdot \circphi_i \bigg|^2
% \bigg]
% \bigg\}
% \]
% So long as the expectation is taken with respect to the `Shapley kernel' distribution.
% This pointwise optimization has been translated into a functional optimization via the work of FastSHAP \cite{jethani2022fastSHAP}:
% \[
% \argmin_{\circphi_0, \circphi(x)} \bigg\{
% \big\bbE_{x,S}\bigg[
% \bigg| f(x,S) - \circphi_0 - \sum_{i=1}^d 1_{i\in S}\cdot \circphi_i(x) \bigg|^2
% \bigg]
% \bigg\}
% \]
% This solution to a minimization problem is used to approximately solve for the Shapley functions associated with a given function $f$.
% It is in this way that FastSHAP yields an approximation to the Shapley value for any particular input by amortizing the cost of learning the Shapley value through the training of an additive model.
% After the training of an additive model, the approximate Shapley value can be returned in constant time.

% In the study of feature interactions,
% the Faith-SHAP work \cite{tsai2023faithSHAP} extends the mean-square error formulation of the Shapley value to `interaction indices' which not only measure the individual power of features, but also their cooperative strengths in feature interactions.
% Their formulation can be written as:
% \[
% \argmin_{\{\circphi_T\}\in\bbR^\cI} \bigg\{
% % \big\bbE_{S\sim p(S)}\bigg[
% \big\bbE_{S}\bigg[
% \bigg| f(S) - \sum_{T\in\cI} 1_{T \supseteq S}\cdot \circphi_T \bigg|^2
% \bigg]
% \bigg\}
% \]
% Where they consider frontiers $\cI$ of the form $\cI_{\leq\ell} := \{ S\subseteq[d] : |S|\leq\ell \}$ for some interaction order $\ell=1,\dots,d$.
% It is relatively straightforward to show that $\ell=1$ corresponds to the traditional Shapley value, and the interaction indices obey many of the same axioms as the original Shapley value.
% % \jam{when we take p(S) = shapley kernel}
% Additional details and an explicit description of the indices can be found within \cite{tsai2023faithSHAP}.



% \subsection{Purified Additive Models}
% Now consider the same pointwise extension of the \cite{tsai2023faithSHAP} definition to the learning of an additive model and compare with the traditional loss function for additive models.
% \begin{align}
% \argmin_{\{\circphi_T(x)\}} \bigg\{
% \big\bbE_{x,S}\bigg[
% \bigg| f(x,S) - \sum_{T\in\cI} 1_{T \supseteq S}\cdot \circphi_T(x) \bigg|^2
% \bigg]
% \bigg\}
% \label{eqn:fast_SHAP_but_interactions}
% \end{align}
% \begin{align}
% \argmin_{\{\circphi_T(x)\}} \bigg\{
% \big\bbE_{x}\bigg[
% \bigg| F(x) - \sum_{T\in\cI} \circphi_T(x) \bigg|^2
% \bigg]
% \bigg\}
% \nonumber
% \label{eqn:classical_GAM_loss}
% \end{align}


% % Arbitrary masked GAM formulation
% % \[
% % \argmin_{\{\circphi_T(x,S)\}} \bigg\{
% % \big\bbE_{x,S}\bigg[
% % \bigg| f(x,S) - \sum_{T\in\cI} \circphi_T(x,S) \bigg|^2
% % \bigg]
% % \bigg\}
% % \]

% \jam{can use parameters $\theta$ to make more clear.  also this section is still garbage 'sequence of observations' and not actually a methods section}
% One immediately notices the serious similarities between the two optimization equations.
% In accordance with these similarities, we define the `purified' MSE loss function as 
% \begin{align}
% \argmin_{\{\circphi_T(x)\}} \bigg\{
% \big\bbE_{x,S}\bigg[
% \bigg| F(x) - \sum_{T\in\cI} 1_{T \supseteq S}\cdot \circphi_T(x) \bigg|^2
% \bigg]
% \bigg\}
% \label{eqn:purified_loss}
% % \label{eqn:purified_loss_insta_SHAP}
% \end{align}
% In the appendix, we demonstrate that this loss function indeed has a solution which generates a `purified' additive model and 
% indeed follows the same solution as the generalization of Faith-SHAP's interaction index.
% Importantly, this allows for the training of an additive model which then automatically has its purified terms delineated from one another, allowing for instant computation of the Shapley value function via the previously mentioned Equation \ref{eqn:shapley_unanimity}.
% A lengthy discussion on the connections between these different approaches and associated theorems are left to the appendix.
% The authors believe that this is the first set of results in the case of correlated features and arbitrary frontier sets.


% It should be noted that even without using the distribution $p(S)$ to be the Shapley kernel,
% it is still true that we can exactly compute the Shapley value from the learned additive model.
% That is to say,
% it is sufficient to consider the subset masking as part of the purified loss for any possible distribution $p(S)$ which is strictly positive, since we may treat the learned additive model as the model we are trying to explain with the Shapley value.
% Although possibly confusing at first, it is probably most helpful to think of Equation \ref{eqn:fast_SHAP_but_interactions} as the interaction extension to Fast-SHAP, but to think of Equation \ref{eqn:purified_loss} as the InstaSHAP method.

% % \jam{say more here: even though additive blah blah, its a perfect shapley value for the additive model (no approx)}

% % \jam{Purified Loss function in the uncorrelated case}
% % \jam{more complicated frontiers in the correlated case}
% % \jam{a lengthy dicsusion on the connections and novel theorems left in the appendix}












\section{Conclusion}

We find that the study of SHAP and GAM from a joint functional perspective allows for a plethora of insights in both domains which were not previously possible.
We establish the theoretical correspondence between the two across all possible correlated input features and discuss the implications in terms of functional representation power.
In practical ML datasets where input correlations are abundant,
we provide a simple but theoretically grounded method of detecting whether SHAP is providing adeqeuate explanations by means of training a GAM model.
We extend on the GAM literature by means of rigorously studying the effect of training on a correlated input distribution,
as well as
introducing a novel masking technique which allows for the recovery of purified GAM models.
In multiple real-world datasets, we find that the existence of feature interactions as synergies and as redundancies is ubiquitous in practical settings.
We finally discuss the implications of this fact in the context of interpreting SHAP in high-dimensional data like natural images.
Although SHAP is a very useful approximation of the first-order effects, a more careful treatment of feature interactions will be required for a complete understanding of blackbox models.



% but overall the gorwing body of literature on additive models has shown that the existence of feature interactions is fundamnetal towards teh scucesful modeling of data.

% accordingly, we find that botht synergistic interactions and redundant interactions exikst cacorss ML datasets,
% and are especially proinounced on comapture vsion datasets,
% potentially calling into question how much lnoger feagture interactions can be safeluy ingored




% \jam{reconclude and reintroduce}
% \red{
% In conclusion,
% using efficient frontier selection algorithms for generalized additive models can match state-of-the-art performance for typical machine learning approaches.
% As a consequence of training additive models with our new purified loss function, we enable the automatic computation of Shapley values without the need for further computation.
% The issue of fast computation of Shapley values is translated into the objective of training performant additive models.
% We provide additional theoretical results which further explain the performance achieved by additive models with an arbitrary frontier set, and empirical results on synthetic and real-world datasets to demonstrate the effectiveness of our approach.
% }

% \blue{
% we provide a new InstaMask'' technique which allows the training of purifie dmodels.
% %
% we open doorsfor future exploration in both SHAP and GAM 
% }


\section{Broader Impact}
This work focuses on enhancing the interpretability of deep learning models.
Although, broadly, the work of interpretability can help inform all related stakeholders to the reasonings behind decisions made by AI systems to the benefit of everyone involved,
ultimately,
all interpretations and decisions are made by humans and can hence be used for unfavorable outcomes both intentionally and unintentionally.
Moreover, interpretability is only one piece of the larger puzzle which is transparency and trustworthiness in AI systems.


\section{Acknowledgements}

This work was supported in part by the Department of Defense under Cooperative Agreement Number W911NF-24-2-0133. 
The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. 
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.


\newpage
% \bibliography{refs,refs_sian,refs_textgenshap}
\bibliography{refs_instashap}
% \bibliographystyle{icml2024}
% \bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix


\newpage
% \section{Further Discussion of Explanation Baseline}
% \section{Discussion of Practical Details}
\section{Post-Hoc Explainability}


\subsection{Further Discussion of Explanation Baseline Methods}
\label{app_sec:explain_by_removing}

\paragraph{Conditional/ Marginal/ Baseline}
We first reiterate the three main removal baselines which see widespread usage across all domains of machine learning explainability.
Those are the three methods introduced in the main text (baseline value, marginal value, and conditional value.)
% Although the conditional value is often the desired version, practical constraints often limit our usage to the former two.
It should be noted for instance that in the original SHAP paper (\citet{lundberg2017shapleySHAP}, Equations 9-12), each of the former two were considered as a simplification or approximation to the conditional value.
The first assumption of feature independence implies the equivalence of the conditional value and the marginal value.
The second assumption of model linearity implies the equivalence of the marginal value and the baseline value.
Accordingly, it is perhaps better to think of these two alternatives as practical simplifications whereas the conditional value is the value of theoretical interest.
Especially after the highlighting of the off-the-manifold problem \citep{frye2021shapleyOnTheManifold},
these two approaches have been under higher scrutiny in their application to typical ML pipelines where input data often have heavy correlations existing outside of the control of the ML practitioner.


% \paragraph{Continuous Line Integral}
\paragraph{Integrated Gradients}
Another common removal approach is Integrated Gradients which is equivalent to Aumann-Shapley value 
\citep{aumann1974aumannShapleyValue,sundararajan2017integratedGradients,sundararajan2020theManyShapleyValues}.
In this version, a line integral is taken from a baseline point $\Bar{x}$ to the target point $x$, rather than the original baseline method which simply takes the difference between the two.
Although it is a smoother approximation which has had empirical success, its interaction extensions \citep{janizek2021integratedHessians} cannot succeed on non-smooth functions like piecewise linear ReLU networks and it is nonetheless susceptible to the off-the-manifold problem.
Although incorporating a more general definition of line integrals could be of interest to solving the off-the-manifold and simultaneously integrating into the discrete masking framework we utilize, 
we envision this as out of scope for our focus on the Shapley value.



% \paragraph{{Hooker} Decomposition}
\paragraph{{Stone-Hooker} Decomposition} %xxx what do people call it
Of potentially the greatest interest besides the conditional case which we directly study is the alternative functional ANOVA decomposition proposed by \cite{hooker2007generalizedFunctionalANOVA}
and further investigated in 
\citep{hart2018sobolCovariancesDependentVariables,lengerich2020purifyingInteractionEffects,xingzhi2022pureGAM}.


\begin{alignat}{3}
    F(x_1,\dots,x_d) 
    &\oct =\oct &
    \sum_{S\subseteq[d]} \tilde{h}_S(x_S)
    \label{app_eqn:hooker_fnl_ANOVA_decomp}
\end{alignat}
where the functions are required to obey a set of `hierarchical orthogonality conditions'
\begin{align}
    \cM_{p,\emptyset} \circ (g_T \cdot \tilde{h}_S) = 0 
    \quad\quad
    \forall g_T,\oct \forall T\subsetneq S
    \label{app_eqn:hooker_decomp_hierarchical_orthogonality_conditions}
\end{align}
which is equivalent to the `integral conditions'
\begin{align}
    \cM_{p,(S-i)} \circ (\tilde{h}_S) \equiv 0  
    \quad\quad
    \forall i \in S
    .
    \label{app_eqn:hooker_decomp_integral_conditions}
\end{align}


Despite its relatively pleasant properties compared to the original Sobol-Hoeffding decomposition,
nearly two decades after its introduction it has received relatively little attention when compared with the Sobol-Hoeffding alternative defined by conditional projections.
(It should be briefly noted that in the case of independent variables, the same solution is recovered.)
Amongst its limitations, 
beyond a lack of intuitive meaning behind its prescribed functions,
the most severe is seemingly its computational intractability.
It is rare to see a calculation of the full decomposition beyond a small number of dimensions or for distributions which are not piecewise constant.
Unlike the conditional projection which can be more efficiently approximated from the bottom-up, the Hooker decomposition seems to endure the full exponential complexity of constructing a functional decomposition from the top-down (starting with the most complex $\tilde{h}_{[d]}$.)

Practical approaches to providing a solution to the full Hooker problem imitate Sinkhorn approximations via iterative refinement across the different variable axes \citep{lengerich2020purifyingInteractionEffects}.
Nonetheless, 
the Sinkhorn algorithm has itself escaped a general closed form solution for decades \citep{sinkhorn1967sinkhornAlgorithm,nathanson2019alternateminimizationdoublystochastic},
and
practical application of the original Hooker decomposition has remained extremely limited.


%random youtube video
% https://www.youtube.com/watch?v=-uIwboK4nwE
% https://arxiv.org/pdf/2409.02789

%Kruithof (1937)
% https://ptdeboer.personalweb.utwente.nl/misc/kruithof-1937-translation.html
%Sinkhorn (1967)
% https://www.jstor.org/stable/2314570
% Nathanson (2020 formula for 2x2)
% https://math.colgate.edu/~integers/uproc10/uproc10.pdf


In the space of Generalized Additive Models, however,
recently some progress has been made.
By leveraging the GAM's ability to reduce the exponential complexity of the true function to a lower-dimensional representation,
works like \citet{lengerich2020purifyingInteractionEffects} and \citet{xingzhi2022pureGAM} have found success in purifying the terms of an additive models.
However, this success is still limited to two- or three- dimensional GAM models, being limited by: discrete variables assumptions requiring histogramming or kerneling of continuous variables; difficult transformations which do not easily scale to higher orders; and/or the previously discussed Sinkhorn-like approximations without clear guarantees.
{We later {revisit} these considerations more thoroughly in Appendix \ref{app_sec:additive_models_and_stuff} after the introduction of our novel results for additive models,
suggesting how our variational perspective potentially allows to unlock the same advantages for Hooker-type purified models.}


\paragraph{Further Alternatives}
% We highly recommend the work of \citet{covert2021explainingByRemoving} for a completely comprehensive, albeit increasingly dated, 
We highly recommend the work of \citet{covert2021explainingByRemoving} for a very comprehensive 
review of potential methods for removal; however, 
% we quickly review some of the other important styles of counterfactuals used for `removing' a feature.
we quickly review some of the major flavors for the counterfactual `removal' of a feature.
Some of the important methods yet unmentioned include the utilization of surrogate models to explicitly or implicitly mask out the features.
This can be done implicitly via the training of a masked surrogate predictor using the projection equations for mean-squared error or for KL divergence as used within this work \citep{covert2021explainingByRemoving,jethani2022fastSHAP,covert2023shapleyForVIT}.
There have also been pursuits through a more explicit approach via using a separate generative model (VAE or GAN) as a proxy for removal \citep{chang2018explainingWithCounterfactualGeneration}, 
additionally allowing for more domain-specific approaches like image blurring and infilling.
Another important set of alternatives is via the language of causality as introduced via \citet{janzing2020interventionalShapleyValue}.
Unfortunately, after the introduction of elegant causal notation,
the authors immediately use a simplifying assumption to reduce to the marginal Shapley, which has the aforementioned problems, only considering the engineer-level causality of `causing the model' to change its predictions.
This is significantly different from the scientist-level causality of `causing the output' and has only begun to be thoroughly addressed in recent works like \citet{biparva2024causalShapleyWithMarkovBlanketShapley}.

%XXXX somehow include this reference as well
% \citet{heskes2020causalshapleyvaluesExploiting}

% Quick list of other maybe relevant ones
% https://arxiv.org/pdf/2007.00714
% https://arxiv.org/pdf/2402.05566
% https://proceedings.neurips.cc/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf
% https://proceedings.mlr.press/v162/jung22a/jung22a.pdf
% https://taih20.github.io/papers/25/CameraReady/Medical_Shapley_TAIH_2020_camera_ready.pdf
% https://proceedings.mlr.press/v108/janzing20a/janzing20a.pdf



% Summary of removal-based approaches:
% \begin{enumerate}
%     \item baseline input
%     \item independent/ interventional
%     \item conditional/ observational
%     \item integrated gradients/ continuous
%     \item Hooker functional ANOVA (purified)
%     \item Domain specific (CV?)
% \end{enumerate}











% \subsection{Available Methods}
% \subsection{Shapley Interaction Indices}




% \subsection{Feature Attribution and Interaction Attribution}
\subsection{Post-Hoc Feature Attribution and Interaction Attribution}
\label{app_sec:post_hoc_feature_attribution_and_interaction_attribution}
\paragraph{Notation}

Let $d\in\bbN$ and $c\in\bbN$ be the dimensions of the input and output spaces,$\cX \subseteq \bbR^d$ and $\cY \subseteq \bbR^c$.
Let $F:\cX \to \cY$ be a function representing a machine learning model which maps from inputs to outputs.
We will use $[d]:=\{1,\dots,d\}$ to represent the set of input features and $S\subseteq[d]$ to represent a subset of the input features.
We also write the set of all such subsets, the powerset, as $\cP([d]) \cong \{0,1\}^d$
and use slight abuse of notations including $(S+i) := S \cup \{i\}$ and $(S-i) := S \setminus \{i\}$.

We will write the function space as some $\cH = \{F : \cX \to \cY\}$ and a masked function space as $\cH' = \{ f : \cX \times \cP([d]) \to \bbR \}$.
For a general feature attribution method, we write  $\Phi : \cH \to \cH^d$, taking a function $F(x)$ as input and returning a local explanation function $[\Phi_i\circ F](x)$ for each feature $i\in[d]$ on each local input $x\in\cX$.
Similarly, we define a blackbox feature attribution method as $\circphi : \cH' \to \cH^d$, instead taking a masked function $f(x,S)$ as input and returning a local explanation function for each feature, $[\circphi_i\circ f](x)$.

In addition to the notation introduced in the main body,
we introduce some notation which are very useful in the domain of feature interactions.
We first define the discrete derivative operator:
\begin{align}
    [\delta_i \circ f](T) = f(T+i) - f(T-i),
\end{align}
and its higher-order counterpart:
\begin{align}
    [\delta_S \circ f](T) := \sum_{W\subseteq S} (-1)^{|S|-|W|} f(T-S+W).
\end{align}
We note that the decision to add and remove elements instead of only adding elements is not necessarily typical; however,
we find it beneficial to not need to restrict the domain of the discrete derivative operator.

We may now define the Mobius transformation or purification transformation as the one which replaces each function with its purified version, $\mu : \cH' \to \cH'$.
\begin{align}
    [\mu \circ f](x,S) := 
    \Tilde{f}(x,S) = 
    % [\delta_S \circ f](x,\emptyset) = 
    \sum_{W\subseteq S} (-1)^{|S|-|W|} f(x,W)
\end{align}
We can also see that the purified functions can additionally be written in terms of the discrete derivative operator.
\begin{align}
    \Tilde{f}(x,S) = 
    [\delta_S \circ f](x,\emptyset) = 
    \sum_{W\subseteq S} (-1)^{|S|-|W|} f(x,\emptyset+W)
\end{align}
% Although we have already found
Both the discrete derivative operator and the Mobius purification transformation are  important tools for being able to more easily study the case of feature interactions in blackbox explainers.
In the sections that follow we will define the major feature attribution and feature interaction attribution methods in terms of these operators.






% \subsubsection{Two Types of Feature Interactions}
% For practical use,
% we highlight the two canonical types of feature interactions:

% -synergistic feature interactions
% % -redundant feature interactions
% -dissonant feature interactions


% These can be simultaneously displayed by the same algebraic equation, so for both examples we assume the function to be explained is $F(x_1,x_2) = x_1 x_2$.

% In the case that $X_1,X_2\sim \cN(0,1)$ are both independent Gaussians, then this function leads to a `synergistic' feature interaction, because we must know both input features before predicting an output feature.


% In the other case, we have that $X_1=X_2\sim \cN(0,1)$ meaning there is perfect correlation between the two input features.
% It follows that knowing either one of the features is the same as knowing both of the features, meaning the purified feature interaction is $\Tilde{f}_{12} = -x_1^2$.







% \subsection{Post-hoc Explainability - The Shapley Value}
% \subsection{Post-hoc Explainability}

\paragraph{The Shapley Value}
The most typical definition of the Shapley value is usually its closed form solution as the weighted average of 1D derivatives,
\[
[\circphi^{\text{SHAP}}\circ f]_i(x) =  \mathlarger{\sum}_{S\subseteq[d]-i} \bigg[ \frac{1}{d} {d-1 \choose |S|}^{-1} \cdot \big[\delta_{i} f \big](x,S) \bigg] ,
\]
although its definition as an expectation over random permutations,
\[
[\circphi^{\text{SHAP}}_i \circ f](x) =  \frac{1}{|\cS_d|}  \mathlarger{\sum}_{\pi \in \cS_d} \bigg[ \big[\delta_{i} f \big](x,S_{\pi,i}) \bigg] ,
\]
has gained popularity in practice due to its susceptibility to Monte-Carlo sampling.
We define $\cS_d$ as the symmetric group or set of permutations on $d$ elements, $\cS_d := \{ \pi : [d]\to[d] \oct\text{s.t.}\oct \pi \oct\text{is bijective}\}$, and we define $S_{\pi,i}$ as the set of predecessors to $i$ under the ordering $\pi$, $S_{\pi,i} := \{ j\in[d] \oct\text{s.t.}\oct \pi(j) < \pi(i) \}$.



We also state the alternative formulation in terms of `unanimity games' which the authors believe to yield a more intuitive understanding of the Shapley value.
\begin{align}
[\circphi^{\text{Sh}}\circ f]_i (x) = \sum_{S\supseteq \{i\}} \frac{\Tilde{f}_S(x_S)}{|S|}
    \label{app_eqn:shapley_unanimity}
\end{align}

In words, the Shapley value divides the purified interaction $\Tilde{f}_S(x_S)$ (which is the value created by $S$ and only $S$) amongst all of its constituent features, $i\in S$, completely uniformly between them, $\frac{1}{|S|}$.

% It is known that this Shapley definition uniquely obeys the following four axioms:
We rewrite the four Shapley axioms in terms of the functional notation:
\begin{enumerate}
    % \item \textbf{Dummy} If $\delta_{\{i\}}\circ f(x,S) = 0$ for all $S$, then $\circphi_i\circ f(x)=0$ (for that $x$).
    \item \textbf{Dummy} \quad If $[\delta_{i}\circ f](x,S) = 0$ for all $S$, 
    
    \hspace{5em} then $[\circphi_i\circ f](x)=0$ (for that local $x$).
    
    % \item \textbf{Symmetry} $[\circphi_{\pi(i)} \circ f] (x_{\pi(1)},\dots,x_{\pi(d)}) = [\circphi_i \circ f](x_1,\dots,x_d)$ $\quad\forall \pi\in\cS_d$  \jam{todo}
    % \item \textbf{Symmetry} \quad If $[\delta_{i}\circ f](x,S) = [\delta_{j}\circ f](x,S)$ for all $S$, 
    
    % \hspace{6em} then $[\circphi_i\circ f](x)=[\circphi_j\circ f](x)$ (for that local $x$).
    
    % \item \textbf{Symmetry} $[\circphi_{\pi(i)} \circ f^\pi] = [\circphi_i \circ f]$ $\quad\forall \pi\in\cS_d$  \jam{todo}

    % \hspace{5em} where $[f^\pi]$
    
    % \item \textbf{Symmetry} $[\pi^{-1}\circ \circphi_{\pi^{-1}(i)} \circ \pi \circ f] = [\circphi_i \circ f]$ $\quad\forall \pi\in\cS_d$  \jam{todo}
    % \item \textbf{Symmetry} $[\pi^{-1}\circ \circphi_{\pi^{-1}(i)} \circ \pi \circ f](x_1,\dots,x_d) = [\circphi_i \circ f](x)$ $\quad\forall \pi\in\cS_d$  \jam{todo}
    % \item \textbf{Symmetry} $[ \circphi_{\pi^{-1}(i)} \circ \pi \circ f](x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}) = [\circphi_i \circ f](x)$ $\quad\forall \pi\in\cS_d$  \jam{todo} wrong
    % \item \textbf{Symmetry} $[ \circphi_{\pi^{-1}(i)} \circ \pi \circ f](x_{\pi^{1}(1)},\dots,x_{\pi^{1}(d)}) = [\circphi_i \circ f](x)$ $\quad\forall \pi\in\cS_d$  \jam{todo} still wrong
    % \item \textbf{Symmetry} $[ \circphi_{\pi(i)} \circ \pi \circ f](x_{\pi^{1}(1)},\dots,x_{\pi^{1}(d)}) = [\circphi_i \circ f](x)$ $\quad\forall \pi\in\cS_d$  \jam{todo}  both fixed
    % \item \textbf{Symmetry} $[\pi^{-1}\circ \circphi_{\pi(i)} \circ \pi \circ f](x) = [\circphi_i \circ f](x)$ $\quad\forall \pi\in\cS_d$  \jam{todo}  both still fixed
    \item \textbf{Symmetry} \quad $\pi^{-1}\circ \circphi_{\pi(i)} \circ \pi \circ f = \circphi_i \circ f$ $\quad\forall i\in[d], \forall \pi\in\cS_d$  
    
    \item \textbf{Efficiency} \quad $\sum_{i\in[d]} \circphi_i\circ f = f_{[d]} - f_\emptyset$
    
    \item \textbf{Linearity} \quad $\circphi\circ (f+g) = \circphi\circ f + \circphi\circ g$
\end{enumerate}
It should be emphasized that dummy is a truly local property whereas symmetry, efficiency, and linearity can all be realized as properties of the additive functions.
Thus, from the functional perspective, it is more appropriate to call this property `local dummy' to emphasize its distinction from `global dummy' functions which would be the case when $\circphi_i \circ f \equiv 0$.
We hope this would help eliminate the common confusion we discuss later in  {Appendix \ref{app_sec:common_fallacies}.}
% is not.
% It is only a convenience to write the symmetry axiom in this way and indeed a more proper axiom would write out the heavy notation which reorders all $x$ and $S$ according to some permutation.
% Unfortunately, we could not find a succinct way to define such an axiom.
%
Another point to briefly note is that linearity condition is about the linearity of the operator rather than the linearity of the function.







\paragraph{Other Common Explainers}
One of the original blackbox explainers is the LIME value \citep{ribeiro2017lime},
which can also be written in this functional notation as:
\begin{align}
    % [\circphi^{\text{LIME}}_i \circ f](x) :=
    [\Phi^{\text{LIME}}_i \circ F](x) :=
\argmin_{\circphi\in\bbR^d} 
\bigg\{
\bbE_{S\sim p^{\text{LIME}}(S)} \bigg[
\Big| 
f^{\text{LIME}}(x,S) - \sum_{i=1}^d \ind({i\in S})\cdot \circphi_i
\Big|^2
\bigg]
\bigg\}
\label{eqn:variational_LIME_least_squares}
\end{align}
where the distribution is taken over a LIME distribution $p^{\text{LIME}}(S)$,
and the function $f^{\text{LIME}}(x,S)$ is taken as the semi-local average value according to a data-dependent LIME kernel which is the exponential of some distance function.
% It is typically also fit with a LASSO penalty
For further details see \cite{ribeiro2017lime} or \cite{lundberg2017shapleySHAP}.


Another common set of explainers are the extremely simple `leave-one-in' and `leave-one-out' values,
based on including a single feature or removing a single feature:
\begin{align}
    [\circphi^{\text{inc}}_i\circ f](x) &:= [\delta_i \circ f](x,\emptyset) \\
    [\circphi^{\text{rem}}_i\circ f](x) &:= [\delta_i \circ f](x,[d])
    \label{app_eqn:leave_one_in_leave_one_out}
\end{align}
These also have equivalent versions for measuring the interaction effect in the more general `inclusion value' or `removal value':
\begin{align}
    [\circphi^{\text{inc}}_S\circ f](x) &:= [\delta_S \circ f](x,\emptyset) \\
    [\circphi^{\text{rem}}_S\circ f](x) &:= [\delta_S \circ f](x,[d])
\end{align}
We note that it is also popular to refer to the difference, $f(x,S) - f(x,\emptyset)$, rather than the interaction effect, as the inclusion value.
Another important interaction explainer is the Archipelago value \citep{tsang2020archipelago} which is defined to be the average of these two $\circphi^{\text{arch}}_S := \frac{1}{2}\circphi^{\text{inc}}_S + \frac{1}{2}\circphi^{\text{rem}}_S$.
This simple estimator is surprisingly robust at detecting feature interactions that $\circphi^{\text{inc}}_S$ or $\circphi^{\text{rem}}_S$ would each individually miss.



There are also a few other game-theoretic approaches which have attracted attention recently such as the 
Banzhaf value \citep{banzhaf1965banzhaf,tsai2023faithSHAP,wang2023dataBanzhaf,enouen2024textGenSHAP}
and the Deegan-Packel index \citep{deegan1978deeganPackelIndex,biradar2024abudctiveExplanationsWithDeeganPackel},
especially in their application to classification tasks instead of regression tasks.






\subsection{Shapley Interaction Indices}


\paragraph{Shapley Interaction Indices}
The first definition extending the Shapley value to try handling feature interactions was already constructed in 1999, mainly by the removal of the efficiency axiom \citep{grabisch1999originalShapleyInteractionIndex}.
This allows for a relatively simple extension using the permutation symmetry axiom to define the interaction index as a random order value where both features must be present rather than the one.
From Table \ref{app_tab:SII_shap_coeffs_maxK_123} below, it can be seen how this index divides the higher-order interaction effects amongst their constituent lower-order subsets in the same way as the original Shapley value ($\sfrac{1}{t}$).




\begin{table}[h]
\scriptsize
    %\centering
    \caption{Shapley Interaction Indices for $k=1,2,3$}    
    \label{app_tab:SII_shap_coeffs_maxK_123} 
    \begin{center}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{cc|c|cccccccccc|}
     & & Equation & $t=1$ & $t=2$ & $t=3$ & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
     \hline
$k=1$ & $s=1$ &   $\sfrac{1}{t}$   & $1$ &  $\sfrac{1}{2}$ &  $\sfrac{1}{3}$ &  $\sfrac{1}{4}$ &  $\sfrac{1}{5}$ &  $\sfrac{1}{6}$ &  $\sfrac{1}{7}$ &  $\sfrac{1}{8}$ &  $\sfrac{1}{9}$ &  $\sfrac{1}{10}$ \\
$k=2$ & $s=2$ &   $\sfrac{1}{(t-1)}$   & $0$ & $1$ &  $\sfrac{1}{2}$ &  $\sfrac{1}{3}$ &  $\sfrac{1}{4}$ &  $\sfrac{1}{5}$ &  $\sfrac{1}{6}$ &  $\sfrac{1}{7}$ &  $\sfrac{1}{8}$ &  $\sfrac{1}{9}$ \\
$k=3$ & $s=3$ &   $\sfrac{1}{(t-2)}$   & $0$ & $0$ & $1$ &  $\sfrac{1}{2}$ &  $\sfrac{1}{3}$ &  $\sfrac{1}{4}$ &  $\sfrac{1}{5}$ &  $\sfrac{1}{6}$ &  $\sfrac{1}{7}$ &  $\sfrac{1}{8}$ \\
         \hline
         \\
     & &  Equation & $t=11$ & $t=12$ & $t=13$ & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\
         \hline

$k=1$ & $s=1$ &   $\sfrac{1}{t}$   &  $\sfrac{1}{11}$ &  $\sfrac{1}{12}$ &  $\sfrac{1}{13}$ &  $\sfrac{1}{14}$ &  $\sfrac{1}{15}$ &  $\sfrac{1}{16}$ &  $\sfrac{1}{17}$ &  $\sfrac{1}{18}$ &  $\sfrac{1}{19}$ &  $\sfrac{1}{20}$ \\
$k=2$ & $s=2$ &   $\sfrac{1}{(t-1)}$   &  $\sfrac{1}{10}$ &  $\sfrac{1}{11}$ &  $\sfrac{1}{12}$ &  $\sfrac{1}{13}$ &  $\sfrac{1}{14}$ &  $\sfrac{1}{15}$ &  $\sfrac{1}{16}$ &  $\sfrac{1}{17}$ &  $\sfrac{1}{18}$ &  $\sfrac{1}{19}$ \\
$k=3$ & $s=3$ &   $\sfrac{1}{(t-2)}$   &  $\sfrac{1}{9}$ &  $\sfrac{1}{10}$ &  $\sfrac{1}{11}$ &  $\sfrac{1}{12}$ &  $\sfrac{1}{13}$ &  $\sfrac{1}{14}$ &  $\sfrac{1}{15}$ &  $\sfrac{1}{16}$ &  $\sfrac{1}{17}$ &  $\sfrac{1}{18}$ \\
     \hline
    \end{tabular}
    \end{adjustbox}
    \end{center}
\end{table}



\newpage
\paragraph{Shapley-Taylor Interaction Indices}
The next major advancement to Shapley interaction indices came with the introduction of the Shapley-Taylor indices in 2019 \citep{sundararajan2020shapleyTaylorInteractionIndex}.
These interaction indices reintroduce the efficiency condition in a way which we now know reflects the additive model structure of summing to the full prediction.
However, they achieve this decomposition by treating the lower-order additive effects asymmetrically from the maximum rank effects.
In particular, they zero out the influence of everything except the purified effect and distribute the higher-order effects amongst the rank $k$ subsets.
This can be seen more clearly in Tables \ref{app_tab:taylor_shap_coeffs_maxK_1}, \ref{app_tab:taylor_shap_coeffs_maxK_2}, and \ref{app_tab:taylor_shap_coeffs_maxK_3}.




\begin{table}[h!]
\scriptsize
    %\centering
    \caption{Shapley-Taylor Coefficients for $k=1$}    
    \label{app_tab:taylor_shap_coeffs_maxK_1} 
    \begin{center}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{cc|c|cccccccccc|}
     & & Equation & $t=1$ & $t=2$ & $t=3$ & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
     \hline
$k=1$ & $s=1$ &   $\sfrac{1}{t}$   & $1$ &  $\sfrac{1}{2}$ &  $\sfrac{1}{3}$ &  $\sfrac{1}{4}$ &  $\sfrac{1}{5}$ &  $\sfrac{1}{6}$ &  $\sfrac{1}{7}$ &  $\sfrac{1}{8}$ &  $\sfrac{1}{9}$ &  $\sfrac{1}{10}$ \\
         \hline
         \\
     & &  Equation & $t=11$ & $t=12$ & $t=13$ & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\
         \hline
$k=1$ & $s=1$ &   $\sfrac{1}{t}$   &  $\sfrac{1}{11}$ &  $\sfrac{1}{12}$ &  $\sfrac{1}{13}$ &  $\sfrac{1}{14}$ &  $\sfrac{1}{15}$ &  $\sfrac{1}{16}$ &  $\sfrac{1}{17}$ &  $\sfrac{1}{18}$ &  $\sfrac{1}{19}$ &  $\sfrac{1}{20}$ \\
     \hline
    \end{tabular}
    \end{adjustbox}
    \end{center}
\end{table}

\begin{table}[h!]
\scriptsize
    %\centering
    \caption{Shapley-Taylor Coefficients for $k=2$}    
    \label{app_tab:taylor_shap_coeffs_maxK_2} 
    \begin{center}    
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{cc|c|cccccccccc|}
     & & Equation & $t=1$ & $t=2$ & $t=3$ & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
     \hline
$k=2$ & $s=1$ &      & $1$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
      & $s=2$ &      & $0$ & $1$ &  $\sfrac{1}{3}$ &  $\sfrac{1}{6}$ &  $\sfrac{1}{10}$ &  $\sfrac{1}{15}$ &  $\sfrac{1}{21}$ &  $\sfrac{1}{28}$ &  $\sfrac{1}{36}$ &  $\sfrac{1}{45}$ \\
         \hline
         \\
     & &  Equation & $t=11$ & $t=12$ & $t=13$ & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\
         \hline
$k=2$ & $s=1$ &      & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
      & $s=2$ &      &  $\sfrac{1}{55}$ &  $\sfrac{1}{66}$ &  $\sfrac{1}{78}$ &  $\sfrac{1}{91}$ &  $\sfrac{1}{105}$ &  $\sfrac{1}{120}$ &  $\sfrac{1}{136}$ &  $\sfrac{1}{153}$ &  $\sfrac{1}{171}$ &  $\sfrac{1}{190}$ \\
     \hline
    \end{tabular}
    \end{adjustbox}
    \end{center}
\end{table}

\begin{table}[h!]
\scriptsize
    %\centering
    \caption{Shapley-Taylor Coefficients for $k=3$}    
    \label{app_tab:taylor_shap_coeffs_maxK_3} 
    \begin{center}
    \begin{tabular}{cc|c|cccccccccc|}
     & & Equation & $t=1$ & $t=2$ & $t=3$ & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
     \hline
$k=3$ & $s=1$ &      & $1$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
      & $s=2$ &      & $0$ & $1$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
      & $s=3$ &      & $0$ & $0$ & $1$ &  $\sfrac{1}{4}$ &  $\sfrac{1}{10}$ &  $\sfrac{1}{20}$ &  $\sfrac{1}{35}$ &  $\sfrac{1}{56}$ &  $\sfrac{1}{84}$ &  $\sfrac{1}{120}$ \\
         \hline
         \\
     & &  Equation & $t=11$ & $t=12$ & $t=13$ & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\
         \hline
$k=3$ & $s=1$ &      & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
      & $s=2$ &      & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
      & $s=3$ &      &  $\sfrac{1}{165}$ &  $\sfrac{1}{220}$ &  $\sfrac{1}{286}$ &  $\sfrac{1}{364}$ &  $\sfrac{1}{455}$ &  $\sfrac{1}{560}$ &  $\sfrac{1}{680}$ &  $\sfrac{1}{816}$ &  $\sfrac{1}{969}$ &  $\sfrac{1}{1140}$ \\
     \hline
    \end{tabular}
    \end{center}
\end{table}








\newpage
\paragraph{n-Shapley Values}
The n-Shapley values are a more recent attempt to revitalize the original Shapley interaction indices to obey the efficiency axiom in an alternate way \citep{bordt2023shapleyToGAMandBack}.
They use a recursive form so that the maximum rank terms ($s=k$) are the same as the original interaction index \citep{grabisch1999originalShapleyInteractionIndex}; however,
the lower order terms ($s<k$) are chosen to exactly obey the efficiency terms.
This requires the use of the Beroulli numbers to balance these terms in a recursive expansion.
The first few orders can be seen in Tables \ref{app_tab:nShap_shap_coeffs_maxK_1}, \ref{app_tab:nShap_shap_coeffs_maxK_2}, and \ref{app_tab:nShap_shap_coeffs_maxK_3}.
Note the similarities and differences with Table \ref{app_tab:SII_shap_coeffs_maxK_123}.




\begin{table}[h]
\scriptsize
    %\centering
    \caption{n-Shapley Coefficients for $n=k=1$}    
    \label{app_tab:nShap_shap_coeffs_maxK_1} 
    \begin{center}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{cc|c|cccccccccc|}
     & & Equation & $t=1$ & $t=2$ & $t=3$ & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
     \hline
$k=1$ & $s=1$ &   $\sfrac{1}{t}$   & $1$ &  $\sfrac{1}{2}$ &  $\sfrac{1}{3}$ &  $\sfrac{1}{4}$ &  $\sfrac{1}{5}$ &  $\sfrac{1}{6}$ &  $\sfrac{1}{7}$ &  $\sfrac{1}{8}$ &  $\sfrac{1}{9}$ &  $\sfrac{1}{10}$ \\
         \hline
         \\
     & &  Equation & $t=11$ & $t=12$ & $t=13$ & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\
         \hline
$k=1$ & $s=1$ &   $\sfrac{1}{t}$   &  $\sfrac{1}{11}$ &  $\sfrac{1}{12}$ &  $\sfrac{1}{13}$ &  $\sfrac{1}{14}$ &  $\sfrac{1}{15}$ &  $\sfrac{1}{16}$ &  $\sfrac{1}{17}$ &  $\sfrac{1}{18}$ &  $\sfrac{1}{19}$ &  $\sfrac{1}{20}$ \\
     \hline
    \end{tabular}
    \end{adjustbox}
    \end{center}
\end{table}

\begin{table}[h]
\scriptsize
    %\centering
    \caption{n-Shapley Coefficients for $n=k=2$}    
    \label{app_tab:nShap_shap_coeffs_maxK_2} 
    \begin{center}    
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{cc|c|cccccccccc|}
     & & Equation & $t=1$ & $t=2$ & $t=3$ & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
     \hline
$k=2$ & $s=1$ &   $\frac{-(t-2)}{2t}$   & $1$ & $0$ &  $\sfrac{-1}{6}$ &  $\sfrac{-1}{4}$ &  $\sfrac{-3}{10}$ &  $\sfrac{-1}{3}$ &  $\sfrac{-5}{14}$ &  $\sfrac{-3}{8}$ &  $\sfrac{-7}{18}$ &  $\sfrac{-2}{5}$ \\
      & $s=2$ &   $\frac{1}{(t-1)}$   & $0$ & $1$ &  $\sfrac{1}{2}$ &  $\sfrac{1}{3}$ &  $\sfrac{1}{4}$ &  $\sfrac{1}{5}$ &  $\sfrac{1}{6}$ &  $\sfrac{1}{7}$ &  $\sfrac{1}{8}$ &  $\sfrac{1}{9}$ \\
         \hline
         \\
     & &  Equation & $t=11$ & $t=12$ & $t=13$ & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\
         \hline
$k=2$ & $s=1$ &   $\frac{-(t-2)}{2t}$   &  $\sfrac{-9}{22}$ &  $\sfrac{-5}{12}$ &  $\sfrac{-11}{26}$ &  $\sfrac{-3}{7}$ &  $\sfrac{-13}{30}$ &  $\sfrac{-7}{16}$ &  $\sfrac{-15}{34}$ &  $\sfrac{-4}{9}$ &  $\sfrac{-17}{38}$ &  $\sfrac{-9}{20}$ \\
      & $s=2$ &   $\frac{1}{(t-1)}$   &  $\sfrac{1}{10}$ &  $\sfrac{1}{11}$ &  $\sfrac{1}{12}$ &  $\sfrac{1}{13}$ &  $\sfrac{1}{14}$ &  $\sfrac{1}{15}$ &  $\sfrac{1}{16}$ &  $\sfrac{1}{17}$ &  $\sfrac{1}{18}$ &  $\sfrac{1}{19}$ \\
     \hline
    \end{tabular}
    \end{adjustbox}
    \end{center}
\end{table}

\begin{table}[h]
\scriptsize
    %\centering
    \caption{n-Shapley Coefficients for $n=k=3$}    
    \label{app_tab:nShap_shap_coeffs_maxK_3} 
    \begin{center}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{cc|c|cccccccccc|}
     & & Equation & $t=1$ & $t=2$ & $t=3$ & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
     \hline
$k=3$ & $s=1$ &   $\frac{(t-3)(t-4)}{12 t}$   & $1$ & $0$ & $0$ & $0$ &  $\sfrac{1}{30}$ &  $\sfrac{1}{12}$ &  $\sfrac{1}{7}$ &  $\sfrac{5}{24}$ &  $\sfrac{5}{18}$ &  $\sfrac{7}{20}$ \\
      & $s=2$ &   $\frac{-(t-3)}{2(t-1)}$   & $0$ & $1$ & $0$ &  $\sfrac{-1}{6}$ &  $\sfrac{-1}{4}$ &  $\sfrac{-3}{10}$ &  $\sfrac{-1}{3}$ &  $\sfrac{-5}{14}$ &  $\sfrac{-3}{8}$ &  $\sfrac{-7}{18}$ \\
      & $s=3$ &   $\frac{1}{(t-2)}$   & $0$ & $0$ & $1$ &  $\sfrac{1}{2}$ &  $\sfrac{1}{3}$ &  $\sfrac{1}{4}$ &  $\sfrac{1}{5}$ &  $\sfrac{1}{6}$ &  $\sfrac{1}{7}$ &  $\sfrac{1}{8}$ \\
         \hline
         \\
     & &  Equation & $t=11$ & $t=12$ & $t=13$ & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\
         \hline
$k=3$ & $s=1$ &   $\frac{(t-3)(t-4)}{12 t}$   &  $\sfrac{14}{33}$ &  $\sfrac{1}{2}$ &  $\sfrac{15}{26}$ &  $\sfrac{55}{84}$ &  $\sfrac{11}{15}$ &  $\sfrac{13}{16}$ &  $\sfrac{91}{102}$ &  $\sfrac{35}{36}$ &  $\sfrac{20}{19}$ &  $\sfrac{17}{15}$ \\
      & $s=2$ &   $\frac{-(t-3)}{2(t-1)}$   &  $\sfrac{-2}{5}$ &  $\sfrac{-9}{22}$ &  $\sfrac{-5}{12}$ &  $\sfrac{-11}{26}$ &  $\sfrac{-3}{7}$ &  $\sfrac{-13}{30}$ &  $\sfrac{-7}{16}$ &  $\sfrac{-15}{34}$ &  $\sfrac{-4}{9}$ &  $\sfrac{-17}{38}$ \\
      & $s=3$ &   $\frac{1}{(t-2)}$   &  $\sfrac{1}{9}$ &  $\sfrac{1}{10}$ &  $\sfrac{1}{11}$ &  $\sfrac{1}{12}$ &  $\sfrac{1}{13}$ &  $\sfrac{1}{14}$ &  $\sfrac{1}{15}$ &  $\sfrac{1}{16}$ &  $\sfrac{1}{17}$ &  $\sfrac{1}{18}$ \\
     \hline
    \end{tabular}
    \end{adjustbox}
    \end{center}
\end{table}






\newpage
\paragraph{Faith SHAP, the Faithful Shapley Index}
The other most recent attempt at a Shapley interaction index is the faithful Shapley interaction index.
Instead of leveraging the permutation sampling symmetry of the original Shapley value, this work instead extends the Shapley value by means of its least-squares characterization.
As we discuss extensively in this work, this can be seen as further utilizing the characteriztaion of Shapley values as an additive model approximation.



\cite{tsai2023faithSHAP}'s Equation (16) solves for the faithful Shapey interaction indices from the perspective of the Mobius/purified functions:
% \begin{align}
% \circphi^{\text{Faith-SHAP-k}}_S \circ f = \Tilde{f}_S + (-1)^{\ell-|S|} \frac{|S|}{\ell + |S|} {\ell \choose |S|} \sum_{T\supset S, |T|\leq\ell} \frac{ {|T|-1 \choose \ell} }{ {|T|+\ell-1 \choose \ell+|S|} } \Tilde{f}_T
% \label{app_eqn:faith_SHAP_mobius_form}
% \end{align}
\begin{align}
\circphi^{\text{Faith-SHAP-k}}_S \circ f = \Tilde{f}_S + (-1)^{k-|S|} \frac{|S|}{k + |S|} {k \choose |S|} \sum_{T\supsetneq S, |T|> k} \frac{ {|T|-1 \choose k} }{ {|T|+k-1 \choose k+|S|} } \Tilde{f}_T
\label{app_eqn:faith_SHAP_mobius_form}
\end{align}

In Tables \ref{app_tab:faith_shap_coeffs_maxK_1}, \ref{app_tab:faith_shap_coeffs_maxK_2}, and \ref{app_tab:faith_shap_coeffs_maxK_3} below,
we display what these coefficients are for the Mobius purified interaction effects.
We additionally calculate a slightly simpler form of these coefficients in the claim below.



\begin{table}[h]
\scriptsize
    %\centering
    \caption{FaithSHAP Coefficients for $k=1$}    
    \label{app_tab:faith_shap_coeffs_maxK_1} 
    \begin{center}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{cc|c|cccccccccc|}
     & & Equation & $t=1$ & $t=2$ & $t=3$ & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
     \hline
$k=1$ & $s=1$ &   $\sfrac{1}{t}$   & $1$ &  $\sfrac{1}{2}$ &  $\sfrac{1}{3}$ &  $\sfrac{1}{4}$ &  $\sfrac{1}{5}$ &  $\sfrac{1}{6}$ &  $\sfrac{1}{7}$ &  $\sfrac{1}{8}$ &  $\sfrac{1}{9}$ &  $\sfrac{1}{10}$ \\
         \hline
         \\
     & &  Equation & $t=11$ & $t=12$ & $t=13$ & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\
         \hline
$k=1$ & $s=1$ &   $\sfrac{1}{t}$   &  $\sfrac{1}{11}$ &  $\sfrac{1}{12}$ &  $\sfrac{1}{13}$ &  $\sfrac{1}{14}$ &  $\sfrac{1}{15}$ &  $\sfrac{1}{16}$ &  $\sfrac{1}{17}$ &  $\sfrac{1}{18}$ &  $\sfrac{1}{19}$ &  $\sfrac{1}{20}$ \\
     \hline
    \end{tabular}
    \end{adjustbox}
    \end{center}
\end{table}

\begin{table}[h]
\scriptsize
    %\centering
    \caption{FaithSHAP Coefficients for $k=2$}
    \label{app_tab:faith_shap_coeffs_maxK_2}
    \begin{center}    
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{cc|c|cccccccccc|}
     & & Equation & $t=1$ & $t=2$ & $t=3$ & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
     \hline
$k=2$ & $s=1$ &  $\frac{-2(t-2)}{t(t+1)}$     & $1$ & $0$ &  $\sfrac{-1}{6}$ &  $\sfrac{-1}{5}$ &  $\sfrac{-1}{5}$ &  $\sfrac{-4}{21}$ &  $\sfrac{-5}{28}$ &  $\sfrac{-1}{6}$ &  $\sfrac{-7}{45}$ &  $\sfrac{-8}{55}$ \\
      & $s=2$ &   $\frac{6}{t(t+1)}$   & $0$ & $1$ &  $\sfrac{1}{2}$ &  $\sfrac{3}{10}$ &  $\sfrac{1}{5}$ &  $\sfrac{1}{7}$ &  $\sfrac{3}{28}$ &  $\sfrac{1}{12}$ &  $\sfrac{1}{15}$ &  $\sfrac{3}{55}$ \\
         \hline
         \\
     & &  Equation & $t=11$ & $t=12$ & $t=13$ & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\
         \hline
$k=2$ & $s=1$ &  $\frac{-2(t-2)}{t(t+1)}$     &  $\sfrac{-3}{22}$ &  $\sfrac{-5}{39}$ &  $\sfrac{-11}{91}$ &  $\sfrac{-4}{35}$ &  $\sfrac{-13}{120}$ &  $\sfrac{-7}{68}$ &  $\sfrac{-5}{51}$ &  $\sfrac{-16}{171}$ &  $\sfrac{-17}{190}$ &  $\sfrac{-3}{35}$ \\
      & $s=2$ &   $\frac{6}{t(t+1)}$   &  $\sfrac{1}{22}$ &  $\sfrac{1}{26}$ &  $\sfrac{3}{91}$ &  $\sfrac{1}{35}$ &  $\sfrac{1}{40}$ &  $\sfrac{3}{136}$ &  $\sfrac{1}{51}$ &  $\sfrac{1}{57}$ &  $\sfrac{3}{190}$ &  $\sfrac{1}{70}$ \\
     \hline
    \end{tabular}
    \end{adjustbox}
    \end{center}
\end{table}

\begin{table}[h]
\scriptsize
    %\centering
    \caption{FaithSHAP Coefficients for $k=3$}    
    \label{app_tab:faith_shap_coeffs_maxK_3} 
    \begin{center}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{cc|c|cccccccccc|}
     & & Equation & $t=1$ & $t=2$ & $t=3$ & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
     \hline
$k=3$ & $s=1$ &   $\frac{3(t-3)(t-2)}{t(t+1)(t+2)}$   & $1$ & $0$ & $0$ &  $\sfrac{1}{20}$ &  $\sfrac{3}{35}$ &  $\sfrac{3}{28}$ &  $\sfrac{5}{42}$ &  $\sfrac{1}{8}$ &  $\sfrac{7}{55}$ &  $\sfrac{7}{55}$ \\
      & $s=2$ &   $\frac{24(t-3)}{t(t+1)(t+2)}$   & $0$ & $1$ & $0$ &  $\sfrac{-1}{5}$ &  $\sfrac{-8}{35}$ &  $\sfrac{-3}{14}$ &  $\sfrac{-4}{21}$ &  $\sfrac{-1}{6}$ &  $\sfrac{-8}{55}$ &  $\sfrac{-7}{55}$ \\
      & $s=3$ &   $\frac{60}{t(t+1)(t+2)}$   & $0$ & $0$ & $1$ &  $\sfrac{1}{2}$ &  $\sfrac{2}{7}$ &  $\sfrac{5}{28}$ &  $\sfrac{5}{42}$ &  $\sfrac{1}{12}$ &  $\sfrac{2}{33}$ &  $\sfrac{1}{22}$ \\
         \hline
         \\
     & &  Equation & $t=11$ & $t=12$ & $t=13$ & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\
         \hline
$k=3$ & $s=1$ &   $\frac{3(t-3)(t-2)}{t(t+1)(t+2)}$   &  $\sfrac{18}{143}$ &  $\sfrac{45}{364}$ &  $\sfrac{11}{91}$ &  $\sfrac{33}{280}$ &  $\sfrac{39}{340}$ &  $\sfrac{91}{816}$ &  $\sfrac{35}{323}$ &  $\sfrac{2}{19}$ &  $\sfrac{68}{665}$ &  $\sfrac{153}{1540}$ \\
      & $s=2$ &   $\frac{24(t-3)}{t(t+1)(t+2)}$   &  $\sfrac{-16}{143}$ &  $\sfrac{-9}{91}$ &  $\sfrac{-8}{91}$ &  $\sfrac{-11}{140}$ &  $\sfrac{-6}{85}$ &  $\sfrac{-13}{204}$ &  $\sfrac{-56}{969}$ &  $\sfrac{-1}{19}$ &  $\sfrac{-32}{665}$ &  $\sfrac{-17}{385}$ \\
      & $s=3$ &   $\frac{60}{t(t+1)(t+2)}$   &  $\sfrac{5}{143}$ &  $\sfrac{5}{182}$ &  $\sfrac{2}{91}$ &  $\sfrac{1}{56}$ &  $\sfrac{1}{68}$ &  $\sfrac{5}{408}$ &  $\sfrac{10}{969}$ &  $\sfrac{1}{114}$ &  $\sfrac{1}{133}$ &  $\sfrac{1}{154}$ \\
     \hline
    \end{tabular}
    \end{adjustbox}
    \end{center}
\end{table}





\begin{claim}
    The Faithful Shapley coefficients can be written with the alternative formula:
    
    \begin{align}        
\circphi^{\text{Faith-SHAP-k}}_S \circ f = \Tilde{f}_S + \sum_{T\supsetneq S, |T|> k} 
        (-1)^{k-s} { \scalebox{1.15}{${k+s-1 \choose s-1}$} } \frac{{t-s-1\choose k-s}}{{t+k-1\choose k}} \Tilde{f}_T
    \end{align}
\end{claim}
\begin{proof}
    We would like to show that:

    \[
    (-1)^{k-s} \frac{s}{k + s} { \scalebox{1.4}{${k \choose s}$} }\frac{ {t-1 \choose k} }{ {t+k-1 \choose k+s} }
    =
    (-1)^{k-s} { \scalebox{1.15}{${k+s-1 \choose s-1}$} } \frac{{t-s-1\choose k-s}}{{t+k-1\choose k}}
    \]

    Let us write 
    \[
    \bigg[
    \frac{s}{k + s}{k \choose s}  {t-1 \choose k}  {t+k-1 \choose k+s}^{-1}
    \bigg]
    \cdot
    \bigg[
    {k+s-1 \choose s-1}  {t-s-1\choose k-s} {t+k-1\choose k}^{-1}
    \bigg]^{-1}
    =
    \]

    \[
    \bigg[
    \frac{s}{k + s} \cdot \frac{k!}{s! (k-s)!} \frac{(t-1)!}{k! (t-k-1)!} {t+k-1 \choose k+s}^{-1}
    \bigg]
    \cdot
    \bigg[
    \ \frac{(s-1)! k!}{(k+s-1)!}  {t-s-1\choose k-s}^{-1}  \frac{(t+k-1)!}{k!(t-1)!}
    \bigg] =
    \]
    \[
    \bigg[
    \frac{1}{k + s} \cdot \frac{(t-1)!}{(s-1)! (k-s)! (t-k-1)!} {t+k-1 \choose k+s}^{-1}
    \bigg]
    \cdot
    \bigg[
    \ \frac{(s-1)! (t+k-1)!}{(k+s-1)! (t-1)!}  {t-s-1\choose k-s}^{-1}   
    \bigg] =
    \]
    \[
    \bigg[
    \frac{1}{k + s} \cdot  \frac{1}{(k-s)! (t-k-1)!} {t+k-1 \choose k+s}^{-1}
    \bigg]
    \cdot
    \bigg[
    \ \frac{(t+k-1)!}{(k+s-1)!}  {t-s-1\choose k-s}^{-1}  
    \bigg] =
    \]
    \[
    \bigg[
    \frac{1}{k + s} \cdot  \frac{1}{(k-s)! (t-k-1)!} \frac{(t-s-1)!(k+s)!}{(t+k-1)!} 
    \bigg]
    \cdot
    \bigg[
    \ \frac{(t+k-1)!}{(k+s-1)!}  \frac{(k-s)!(t-k-1)!}{(t-s-1)!}
    \bigg] =
    \]
    \[
    \bigg[
    \frac{1}{k + s} \cdot   \frac{(k+s)!}{(t+k-1)!} 
    \bigg]
    \cdot
    \bigg[
    \ \frac{(t+k-1)!}{(k+s-1)!}  
    \bigg] =
    \]
    \[
    \bigg[
    \frac{(k+s-1)!}{(t+k-1)!} 
    \bigg]
    \cdot
    \bigg[
    \ \frac{(t+k-1)!}{(k+s-1)!}  
    \bigg] =
    \]
    \[
    \big[  1  \big] 
    \cdot 
    \big[  1  \big] = 1
    \]
\end{proof}

Due to the fact that $s\leq k < t$, we find this to be a slightly nicer formulation when written as a function of $t$.
% Below, we provide a tabulation of these coefficients for $k =1,2,3$.













\newpage
\newpage





\newpage

% \section{Common Fallacies in the Current Literature}
% \section{Common Mistakes in the Current Literature}
\section{Common Pitfalls in the Current Literature}
\label{app_sec:common_fallacies}





\subsection{Locally Zero vs. Globally Zero}
% Consider the simplistic function $f(x_1,x_2) = \cos{x_1} + \sin{x_2}$
% \jam{we do not provide calculations as it is hoped most of this material could be covered in a basic calculus course.}
Consider the simple function $f(x_1,x_2) = \cos{(2\pi x_1)} + \sin{(2\pi x_2)}$.
Assume the input features are uniformly distributed across the space $[-1,1]^2$.
Based on our correspondence, we can see see that the Shapley value functions decompose the same as an additive model with $\circphi_1(x_1,x_2) = \cos{(2\pi x_1)}$ and $\circphi_2(x_1,x_2) = \sin{(2\pi x_2)}$.


When we set $x_1=\frac{1}{4}$, we have that $\circphi_1(\frac{1}{4},x_2)=\cos{\frac{\pi}{2}}=0$.
Does this suddenly mean that our function $\circphi_1(x_1,x_2)$ is not a function of $x_1$ because it is zero at one value?
No, it does not.
% If we also set $x_2={\pi}$, then we have that 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.24\linewidth]{figures/stupid/cosx_v2.PNG}
    \quad\quad
    \includegraphics[width=0.24\linewidth]{figures/stupid/sinx_v2.PNG}
    \caption{Cosine and sine functions.}
    \label{app_fig:basic_sine_cosine}
\end{figure}

It is hoped that after the clarification of the functional perspective on the Shapley value, it can be made clear that the exact same question is being asked when the Shapley value is equal to zero for a single input point.
If one is interested in the global importance of a feature, then one should check for being zero or nonzero as an entire function.
% which can for instance be checked with the variance of the Shapley function from Equation \ref{app_eqn:variance_of_shapley_functions}.
This can for instance be checked with the variance of the Shapley function:
\begin{align}
\bbV_i^\text{SHAP}
:= 
\bbV\text{ar}_{X}[  \circphi^\text{SHAP}_i(X)  ]
> 0
\label{app_eqn:variance_of_shapley_functions}
\end{align}




\subsection{Beeswarm Plots instead of Shape Functions}

It is common to see beeswarm plots of the SHAP values used as an aggregate summary over an entire dataset.
Although it is an information dense representation of the SHAP values across the entire dataset,
we feel the additive model perspective brings some insights into their limitations and what can be improved about them.


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        %\includegraphics[height=1.2in]
        \includegraphics[width=1.0\linewidth]{figures/bikeshare_and_treecover/sep2024_bikeshare_beeswarm_SHAP.png}
        \caption{Beeswarm Representation of SHAP}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[height=1.2in]{figures/bikeshare_and_treecover/sep2024_bikeshare_workdayONLY_shape_SHAP.png}
        \includegraphics[height=1.2in]{figures/bikeshare_and_treecover/sep2024_bikeshare_hourONLY_shape_SHAP.png}
        \caption{Shape Function Representation of SHAP}
    \end{subfigure}
    \caption{Alternate representations of the SHAP values aggregated over an entire dataset.}
    \label{app_fig:beeswarm_vs_shape_function}
\end{figure}


First of all, the major limitation when compared to the shape function or  additive model representation is simply the compression of information.
Each of the shape functions in Figure \ref{app_fig:beeswarm_vs_shape_function}b is compressed horizontally and then its flattened version is rotated and stacked amongst the SHAP values for all other features.
Although this information about the feature value is theoretically preserved via the colormap,
it is well known from data visualization that this is insufficient to adequately preserve the information.
This compression effect is especially pronounced in cyclic or heavily oscillating shape functions.

Surprisingly, these plots are also commonly used with the same red-blue diverging colormap which is used in the output space for SHAP.
This colormap makes sense for the SHAP values since they are referring to the positive or negative influence on the output prediction; however, when also applied to the feature value, we are conflating the y-axis and the x-axis in the shape functions of Figure \ref{app_fig:beeswarm_vs_shape_function}b.
% Additionally, this is using a diverging colormap to represent what are usually sequential features (bad practice for those very familiar with data visualization),
% Accordingly, it is recommended that the coloring of beeswarm plots be often be replaced with a discrete or sequential colormap to reflect this difference; however, it is likely more effective to simply remove the color altogether and/or to simply use an alternate aggregate statistic such as the variance to measure the spread of the Shapley values:
% Accordingly, it is recommended that the coloring of beeswarm plots be replaced with a discrete or sequential colormap to reflect this difference;
% \jam{actually we recommend}
% however, if one is not willing to look at the shape functions, it is often more effective to remove the color entirely and/or to use an aggregate statistic such as the variance to measure the spread of the Shapley values of the beeswarm, as in Equation \ref{app_eqn:variance_of_shapley_functions}.
%
Additionally, this is using a diverging colormap to represent what are usually sequential features (meaning a sequential colormap should be used instead).
Although we again recommend to look at the shape functions to get a clearer overall picture,
if one is not willing to inspect all of the shape functions,
it is perhaps more effective to remove the color from the beeswarm plot entirely and/ or to just use an aggregate statistic such as the variance in Equation \ref{app_eqn:variance_of_shapley_functions} to measure the spread of the Shapley values depicted by the beeswarm plot.




% strangely, these plots are moreover commonly used alongside a diverging colormap instead of a sequential colormap for the feature space.
% Moreover, this colormap is conflated with the output map where the Shapley value is positive or negative




    




% \subsection{Locally Linear - Gradietn or Value}
\subsection{Locally Linear or Locally Additive}

The interpretation of a locally linear model has two key interpretations which are unfortunately conflated in much of the work on explainability.
The first is the semi-local interpretation of the coefficients as gradient-like coefficients telling the direction of greatest influence.
Usually this is done similar to LIME \citep{ribeiro2017lime} where a linear function is fit to a weighted neighborhood of points.
This is in contrast with the gradient which fits the same linear function to an arbitrarily small neighborhood of points.
These are both different from the second interpretation of the linear model as a structural assumption onto the functional space.
In particular, the linearity and efficiency axioms of the Shapley value are made on the Shapley operator itself.
The local linearity assumptions do not translate to a global linearity assumption, but rather to a global additive assumption where the height function is dictated to respect the additive structure of the original function.



Hopefully after the clearer connections we make with additive models,
it is clear why the height interpretation is the correct one for SHAP and that the gradient-like tests for SHAP are ill-posed after a contextual understanding of the goal of SHAP as the height function rather than a measurement of the local sensitivity similar to the gradient.

Let us also briefly recall that in the case of a GAM, there is still an interesting correspondence between the gradient of SHAP and the gradient of the GAM.

Recall the GAM-1 equation. 
\begin{align}
    F^{\scalebox{0.55}{$\leq 1$}}(x_1,\dots,x_d) = { f_1(x_1) + \dots + f_d(x_d)}
    % \label{app_eqn:recall_gam_1_eqn} 
    \nonumber
\end{align}
It follows that the gradient obeys the similar 
\begin{align}
\nabla F^{\scalebox{0.55}{$\leq 1$}}(x_1,\dots,x_d) &= \langle \partial_1 f_1(x_1) , \dots, \partial_d f_d(x_d) \rangle \nonumber \\
 &= \langle \partial_1 \circphi_1(x) , \dots, \partial_d \circphi_d(x) \rangle \nonumber
\end{align}

If one is interested in a local sensitivity test, then something like the gradient of the GAM or gradient of the SHAP should instead be used,
but it is gently reminded that the vanilla gradient will ignore the statistical structure of the manifold \citep{frye2021shapleyOnTheManifold} and could face alternate issues like the shattered gradients problem \citep{balduzzi2017shatteredGradientsProblem}.




\subsection{Baseline Method on Discrete Inputs}

If you have a finite set of discrete, categorical inputs, then the (mis)usage of the baseline method becomes of great importance for reasons beyond the off-the-manifold problem.
In particular, it is common to replace the current input variable by the baseline input variable only to realize they had the same value (e.g. zero).
It follows that the counterfactual removal will have no effect,
Although extremely rare in the continuous case, when using categorical input variables it is easy to mask out bivariate and even higher-order effects.
%
%
%
It is especially easy to make this mistake on boolean input variables. %citation needed XDDDD
%\citep{huang2023inadequacyOfSHAP}
%%%dont cite during ... idk
%
Accordingly, it is perhaps recommended to those with lesser familiarity with boolean functions or logical functions to use the $\{\pm 1\}$ or one-hot encoding instead of $\{0,1\}$ to avoid making such mistakes  %XD
\citep{odonnel2014booleanFunctionBook}.
% especially when combining with interpretability packages.
%should I bring up the Fourier spectrum
%xxx or explore basic theory of boolean functions




% \subsection{On ``Impossibility Theorems for Feature Attribution''}
% - you provide the definition of the gradient and then you end up with the gradient
% - 



% \subsection{On ``The Inadequacy of Shapley Values for Explainability''}
% - you do not try to use the SHAP value properly
% - fourier spectrum of circuit (reference to \cite{odonnel2014booleanFunctionBook})
% - you do not give any attention to the underlying distribution (which is a critical topic if you don't want to explore the already understood fourier spectrum)

% \jam{an inability to do basic data processing will indeed lead to surprising results}
    


\newpage
\newpage

\section{Impossibility Theorems for Feature Interactions}
\label{app_sec:impossibility_theorems}

This section will broadly prove the representational power of SHAP and related black-box explainers in the form of `impossibility theorems' or `possibility theorems'.
The main method of proof technique is simply via the proof of functional correspondence between the two spaces, the functional space of interest to be taken for hypothesis tests, $\cH$, and the functional space of additive models for some sufficiently large order $k\in\bbN$, $\cH^{\leq k}$.

Let us first proceed by defining the trace of the SHAP function as the object of interest so that we may thus allow a direct functional comparison between the two of GAM and SHAP.
For simplicity,
we will assume a rectangular feature space $\cX = \cX_1 \times \dots \times \cX_d$ for some subspaces $\cX_i$ for each $i\in[d]$.
It is simple to extend some subset of $\bbR^d$ to a rectangular subset in this way by taking the product of the marginal spaces.
It is also necessary to extend the distribution $p(x)$ which has been assumed on the original space $\cX$ to our rectangular space.
However, the obvious extension of zero probability on any additional coordinate will be sufficient for our purposes, since it will already be required to make all of our statements modulo differences on a null set (set of measure zero).
Accordingly, we will simply restrict our focus to a rectangular feature space and ignore any concerns regarding differences on a null set.
Any hypothesis test which is testing on a set of measure zero, although completely ill-posed, will not be answerable under this statistical framework.


For a given test point $x^*\in\cX$,
we define the SHAP trace at $x^*$ to be the object:
\begin{align}
    % \cT_F^{\text{SHAP}}(x^*) = [\Phi^{\text{SHAP}} \circ F] \Big(
    % \cup_{i\in[d]} \{ x^*_1 \} \times \dots \times \{ x^*_{i-1} \} \times \cX_i \times \{ x^*_{i+1} \} \times \dots \times \{ x^*_d \}
    % \Big)
    \cT_F^{\text{SHAP}}(x^*) = \bigg\{
    \Big(x, [\Phi^{\text{SHAP}} \circ F](x)\Big) : x\in
    \bigcup_{i\in[d]} \{ x^*_1 \} \times \dots \times \{ x^*_{i-1} \} \times \cX_i \times \{ x^*_{i+1} \} \times \dots \times \{ x^*_d \}
    \bigg\}
    \nonumber
\end{align}
In words, the trace of the SHAP value is the image under the Shapley function of the set which includes all possible 1D perturbations of a single feature value.



To enable a direct comparison with the original functional space $\cH$,
we will define the completion of the trace as the function which is extended to the entire rectangular feature space $\cX$ in the obvious way by assuming that the SHAP trace is the true additive model representing the underlying function $F$.
\begin{align}
    % \cT_F^{\text{SHAP}}(x^*) = [\Phi^{\text{SHAP}} \circ F] \Big(
    % \cup_{i\in[d]} \{ x^*_1 \} \times \dots \times \{ x^*_{i-1} \} \times \cX_i \times \{ x^*_{i+1} \} \times \dots \times \{ x^*_d \}
    % \Big)
    {\cC\cT}_F^{\text{SHAP}}(x^*) = \bigg\{
    \Big(x, \sum_{i=1}^d [\Phi^{\text{SHAP}}_i \circ F](x^*_1,\dots,x^*_{i-1},x_i,x^*_{i+1},\dots,x^*_d )\Big) : x\in\cX
    \bigg\}
    \nonumber
\end{align}




\begin{theorem}
    % The SHAP trace of a function $F$ will satisfy any hypothesis test $\cH_0$ v. $\cH_1$ inside of the functional space $\cH$ if and only if the functional space $\cH$ is equivalent to a subset of $\cH^{\leq 1}$.
    The SHAP trace of a function $F$ will satisfy any hypothesis test $\cH_0$ v. $\cH_1$ inside of the functional space $\cH$ if and only if the functional space $\cH$ is equivalent to a shift which is not a superset of $\cH^{\leq 1}_{\text{ANOVA}}$.
\end{theorem}
\begin{proof}
Since we are making a claim across all possible splittings of the functional space $\cH$ into two possibilites of the null hypothesis $\cH_0$ and the alternative hypothesis $\cH_1$,
it is required that we actually show the exact identifiability of the individual function from the SHAP trace alone.

Suppose first the $\cH$ truly is a subset of the ANOVA-1 space (or a shift thereof).
We simply take the difference $(F_1-F_2)$ for some arbtirary $F_1,F_2\in\cH$ or otherwise assume we know the functional shift required to center our functional space to be a subset of the ANOVA-1 space.
It follows from the exact formula of the SHAP function that for any $F\in\cH$, we will have that $\circphi_i(x) = \circphi_i(x_i) = \tilde{f}_i(x_i)$, which implies that the completion of this trace will immediately recover the original GAM-1 function.

In the other direction, to prove the contrapositive,
assume instead that $\cH$ truly has some feature interaction, which can be represented by $(F_1-F_2)$ and $(F_1-F_3)$ being some different shifts for some $F_1,F_2,F_3\in\cH$.
For simplicity, let us shift by $(F_1-F_3)$ so that one difference is zero and one difference is nonzero.
It follows that for the nonzero interaction effect, there is some $S\subseteq[d]$ with $|S|>1$ such that $\tilde{f}_S \not\equiv 0$.
Since this is true in the statistical sense, there is a region of sufficient difference.
Via the assumption that our space $\cH$ is at least as representative as $\cH^{\leq 1}_{\text{ANOVA}}$, this means we can find two distinct functions which map to the same SHAP trace in the local region of this nonzero measure region.
Accordingly, if we take a hypothesis test which identifies these two distinct functions, their SHAP traces will still look indistinguishable and there will be no succesful hypothesis test based on the SHAP trace.
\end{proof}

Of course, practically speaking, we likely do not have access to a priori knowledge about the global feature interactions of some hypothesis space $\cH$ which would allow for the construction of the isomporhism between our given $\cH$ and some $\tilde{\cH}$ which is actually a subset of the ANOVA-1 space $\cH^{\leq 1}_{\text{ANOVA}}$


Proceeding by defining the trace of Faith-SHAP-k and GAM-k in the obvious way, we may find a similar theorem for the higher-order interactions of $k\in\bbN$.
Once again, it is practically more useful to say we cannot directly assume the existence of feature interactions across the entire hypothesis space and hence again asssume $\cH\subseteq \cH^{\leq k}_{\text{ANOVA}}$ without the caveat of allowing a shift by some oracle assumption.
It is also perhaps more interesting that we can make the same statement for any arbitrary frontier $\cI \subseteq \cP([d])$ as we will introduce in the general study of additive models in Appendix \ref{app_sec:additive_models_and_stuff}.
% and \ref{app_sec:correlated_functional_ANOVA}





\subsection{Specific Examples}

Although we have shown exact functional equivalence from which the ability to do hypothesis tests follows, we restate some of the tests from \cite{bilodeau2022impossibilityTheoremsForFeatureAttribution} to make a clearer comparison.

The first hypothesis test is coming from their Proposition 3.5,
% and although they do not name this hypothesis test, we call it the ``$\delta$-local almost right Lipschitz'' because of its resemblance to the slightly more typical ``$\delta$-locally right Lipschitz'' test.
and although they do not name this hypothesis test, we call it the ``almost $\delta$-local Lipschitz'' because of its resemblance to the slightly more typical ``$\delta$-local Lipschitz'' test.
\begin{align}
    \cL^{\delta,x,i}_\text{almost}(F) := \sup_{x_i' \in [x_i-\delta,x_i+\delta]} \bigg\{
    % \frac{1}{\delta} \Big| F(x') - F(x) \Big|
    \frac{| F(x') - F(x) |}{\delta} 
    % \frac{| F(x_1,\dots,x'_i,\dots,x_d) - F(x_1,\dots,x_d) |}{\delta} 
    \bigg\}
    \label{app_eqn:almost_delta_local_lipschitz}
\end{align}
%
\begin{align}
    \cL^{\delta,x,i}(F) := \sup_{x_i' \in [x_i-\delta,x_i+\delta]} \bigg\{
    % \frac{1}{\delta} \Big| F(x') - F(x) \Big|
    \frac{| F(x') - F(x) |}{ |x' - x| } 
    % \frac{| F(x_1,\dots,x'_i,\dots,x_d) - F(x_1,\dots,x_d) |}{\delta} 
    \label{app_eqn:delta_local_lipschitz}
    \bigg\}
\end{align}
They then create the hypothesis tests:
\begin{align}
    \cH_0 = \bigg\{ F \in \cH : \cL^{\delta,x,i}_\text{almost}(F) \leq \frac{\eps}{2} \bigg\} \\
    \cH_1 = \bigg\{ F \in \cH : \cL^{\delta,x,i}_\text{almost}(F) > \eps \bigg\}
    \label{app_eqn:locally_lipschitz_hypothesis_tests}
\end{align}
And then identify that the gradient can successfully distinguish these two hypotheses.
This is desired since it is well known that on a compact interval, continuously differentiable functions are automatically Lipschitz functions.
% We may also emphasize this is vaguely related to the common mistake that assumes the SHAP is a locally linear model, when after our discussion in Section \ref{app_sec:common_fallacies},
% it is seemingly more accurate to call it a locally additive model.


The next two major hypothesis tests they build are for `local recourse' in Definition 3.7 and `locally spurious' in Definition 3.8.
For recourse, we first assume some counterfactual distribution $\nu(x)$ which they implicitly assume to have nonzero measure across the left-local and right-local regions $[x_i-\delta,x_i]$ and $[x_i,x_i+\delta]$.
We define the `value of moving to the left' and the `value of moving to the right' under the counterfactual distribution, $\nu(x)$, as the $\delta$-local left and right recourse values:
\begin{align}
    \cV^{\delta,x,i,-} (F) := \bbE_{X\sim \nu}\bigg[
    f(x_1,\dots,X_i,\dots,x_d)
    \oct|\oct
    X_i \in [x_i-\delta,x_i]
    \bigg] \\
    \cV^{\delta,x,i,+} (F) := \bbE_{X\sim \nu}\bigg[
    f(x_1,\dots,X_i,\dots,x_d)
    \oct|\oct 
    X_i \in [x_i,x_i+\delta]
    \bigg] 
    \label{app_eqn:delta_local_recourse}
\end{align}
They then create the hypothesis tests:
\begin{align}
    \cH_0 = \bigg\{ F \in \cH : \cV^{\delta,x,i,+} (F) > \cV^{\delta,x,i,-} (F) \bigg\} \\
    \cH_1 = \bigg\{ F \in \cH : \cV^{\delta,x,i,+} (F) \leq \cV^{\delta,x,i,-} (F) \bigg\} 
    \label{app_eqn:local_recourse_hypothesis_tests}
\end{align}


The infinity norm over a local interval (from the left and from the right) is defined as:
\begin{align}
    \|F\|_\infty^{\delta,x,i,-} := \sup_{x_i' \in [x_i-\delta,x_i]} \bigg\{
    % {\Big| F(x') - F(x) \Big|}
    {\Big| F(x') \Big|}
    \bigg\} \\
    \|F\|_\infty^{\delta,x,i,+} := \sup_{x_i' \in [x_i,x_i+\delta]} \bigg\{
    % {\Big| F(x') - F(x) \Big|}
    {\Big| F(x') \Big|}
    \bigg\}
    \label{app_eqn:local_infty_norm}
\end{align}
The final major hypothesis tests introduced in that work are the tests for if a feature is local spurious:
\begin{align}
    \cH_0 = \bigg\{ F \in \cH : \|F\|_\infty^{\delta,x,i,+} = 0 \bigg\} \\
    \cH_1 = \bigg\{ F \in \cH : \|F\|_\infty^{\delta,x,i,+} \geq \eps \bigg\} 
    \label{app_eqn:local_spurious_hypothesis_tests}
\end{align}

To reiterate,
all of these tests are easily answered via the SHAP value using the additive model trace.
All of these hypothesis tests focus on the behavior inside of a local neighborhood for some arbitrary $\delta$,
whereas the SHAP value evaluated at a single point only describes the behavior at a single point.
% It is possible that this is the confusion leading to the introd
Hopefully, from the functional perspective on Shapley,
it is now clear how easily all of these questions can be answered by using the additive model equivalent of the Shapley value.
This does not conflict with previous works showing negative results on these same hypothesis test since their focus was on the ability of a pointwise indicator's ability to perform these hyptohesis tests, and is related to our discussion in Section \ref{app_sec:common_fallacies}.1 and Section \ref{app_sec:common_fallacies}.3.
% it is seemingly more accurate to call it a locally additive model.
%
Moreover, it is hoped that the true limitation of SHAP, its inability to adequately handle feature interactions, is now emphasized as something that cannot be shown by any of these existing impossibility tests due to their focus on a single perturbed feature at a time.



% ``spurious'' locally zero or locally greater than $\epsilon$

% ``recourse'' locally $\delta$ leads to increase or decrease in output value




\newpage


\section{Experiment Details}




\subsection{2D Synthetic}


First, we look at a particularly simple example of synthetic data to highlight the two important aspects which the Shapley value alone is unable to capture:
feature interaction and feature correlation.
Hopefully this example will help develop intuition for the Shapley value and highlight its unique challenges in the setting where input variables are correlated.
% of the Shapley value, and hopefully help further develop intuition for when it should and should not be utilized in application.


For some  $\rho\in[-1,1]$,
we consider the data generated by 
\[
% f(x,y) = x + y + (xy-\rho) 
f(x,y) = x + xy
\quad\quad
X,Y \sim \cN\Big(\Vec{0}, 
{\scriptsize \setlength\arraycolsep{2pt} \begin{pmatrix}
1&\rho\\ \rho&1 \end{pmatrix}}
\Big)
\]
% for some $\rho\in[-1,1]$.  
% A detailed solution can be found in Appendix \ref{app_sec:synthetic_data_calcs}, but the Sobol covariances can be calculated to be: $C_\emptyset = \rho^2$, $\bbC_x = 1+2\rho^2$, $\bbC_y = 3\rho^2$, and $\bbC_{xy} = 1-4\rho^2$.
% Suprisingly, the term $\bbC_{xy}$ can be negative whenever we choose $|\rho|>\frac{1}{2}$.




% \section{Synthetic Data Calculations}
% \subsection{Synthetic Data Calculations}
% \label{app_sec:synthetic_data_calcs}
It is relatively straightforward to calculate that:
\begin{figure}[!htb]
\vspace{-1.0em}
    \centering
    \begin{minipage}{.5\textwidth}
\begin{align}
f_\emptyset &= \rho \nonumber \\
f_x &= x + \rho x^2 \nonumber \\
f_y &= \rho y + \rho y^2  \nonumber \\
f_{xy} &= x + xy \nonumber 
\end{align}    
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
\begin{align}   
\Tilde{f}_\emptyset &= \rho \nonumber \\
\Tilde{f}_x &= x + \rho x^2 - \rho \nonumber \\
\Tilde{f}_y &= \rho y + \rho y^2 - \rho  \nonumber \\
\Tilde{f}_{xy} &= -\rho y + xy - \rho x^2 - \rho y^2 + \rho \nonumber
\end{align}
    \end{minipage}
\end{figure}
% \begin{minipage}
% \begin{align}
% f_\emptyset &= \rho \nonumber \\
% f_x &= x + \rho x^2 \nonumber \\
% f_y &= \rho y + \rho y^2  \nonumber \\
% f_{xy} &= x + xy \nonumber 
% \end{align}
% \begin{align}   
% \Tilde{f}_\emptyset &= \rho \nonumber \\
% \Tilde{f}_x &= x + \rho x^2 - \rho \nonumber \\
% \Tilde{f}_y &= \rho y + \rho y^2 - \rho  \nonumber \\
% \Tilde{f}_{xy} &= -\rho y + xy - \rho x^2 - \rho y^2 + \rho \nonumber
% \end{align}
% \end{minipage}

\begin{figure*}[h]
    \centering
    % \includegraphics[width=0.9\textwidth]{icml2024/figures/baby_synthetic_full_11_rhos.pdf}
    % \includegraphics[width=1.0\textwidth]{icml2024/figures/baby_synthetic_full_11_rhos.pdf}
    \includegraphics[width=0.9\textwidth]{figures/synth/baby_synthetic_full_11_rhos.png}
    \caption{Simple Synthetic Dataset using various $\rho\in\{0.0,0.1,0.2,\dots,1.0\}$.  Each of the five rows corresponds to the learned $\Tilde{f}_x$, $\Tilde{f}_{y}$, $\Tilde{f}_{xy}$, $\circphi_x$, and $\circphi_y$.  
    The third row is hence `top-down' from the z-axis whereas all others have the output f as the vertical axis.
    % Last two rows correspond to the corresponding Shapley functions  depicted as partial dependence plots (instead of the typical beeswarm plot).  
    Visually constrained to $x,y\in[-2,2]^2$ with colors/ outputs in $[-3,3]$.  
    A single point is highlighted to emphasize how the Shapley value in the bottom two rows is constructed from the top three rows. }
    \label{app_fig:baby_synthetic_learned_GAM}
\end{figure*}



The Sobol covariances are hence:
\begin{align}
    \bbE[f \cdot \Tilde{f}_\emptyset] = \rho^2 \nonumber\\
    \bbE[f \cdot \Tilde{f}_{x}] = \bbE[x^2 + \rho x^3 + x^2y + \rho x^3y -\rho x -\rho xy] = 1 + 0 + 0 + 3\rho^2 +0 -\rho^2 \nonumber\\
    \bbE[f \cdot \Tilde{f}_y] = \bbE[\rho xy + \rho xy^2 + \rho xy^2+ \rho xy^3 -\rho x -\rho xy] = \rho^2 + 0 + 0 + \rho(3\rho)+0 -\rho^2 \nonumber \\
    \bbE[f \cdot \Tilde{f}_{xy}] = \bbE[f^2] - (\rho^2) - (1+2\rho^2) - (3\rho^2) = 1 + (1+2\rho^2) - (1+6\rho^2) \nonumber
\end{align}

Hence $\bbC_\emptyset = \rho^2$, $\bbC_x = 1+2\rho^2$, $\bbC_y = 3\rho^2$, $\bbC_{xy} = 1-4\rho^2$.
% $-\rho^2+0+0+0+0   \quad+0+(1+2\rho^2)-\rho(3\rho)-\rho(3\rho)+\rho^2$
% = $1-4\rho^2$


The Shapley functions are also $\circphi_x(x,y) = \Tilde{f}_{x} + \frac{1}{2}\Tilde{f}_{xy}$ and $\circphi_y(x,y) = \Tilde{f}_{y} + \frac{1}{2}\Tilde{f}_{xy}$: 
% \begin{align}
    % \circphi_x(x,y) &= (x+\rho x^2 - \rho) + \frac{1}{2}( -\rho y + xy - \rho x^2 - \rho y^2 + \rho) &= \Big[x-\frac{\rho}{2}y\Big] + \Big[\frac{xy}{2}+\frac{\rho}{2} (x^2 - y^2 - 1) \Big]\nonumber\\
    % \circphi_y(x,y) &= (\rho y+\rho y^2 - \rho) + \frac{1}{2}( -\rho y + xy - \rho x^2 - \rho y^2 + \rho) &= \Big[\frac{\rho}{2}y\Big] + \Big[\frac{xy}{2}+\frac{\rho}{2} (y^2 - x^2 - 1) \Big]\nonumber
    % \circphi_x(x,y) &= (x+\rho x^2 - \rho) + \frac{( -\rho y + xy - \rho x^2 - \rho y^2 + \rho)}{2} &= \Big[x-\frac{\rho}{2}y\Big] + \Big[\frac{xy}{2}+\frac{\rho}{2} (x^2 - y^2 - 1) \Big]\nonumber\\
    % \circphi_y(x,y) &= (\rho y+\rho y^2 - \rho) + \frac{( -\rho y + xy - \rho x^2 - \rho y^2 + \rho)}{2} &= \Big[\frac{\rho}{2}y\Big] + \Big[\frac{xy}{2}+\frac{\rho}{2} (y^2 - x^2 - 1) \Big]\nonumber
% \end{align}
\begin{equation}
\resizebox{.98\hsize}{!}{$%
    \circphi_x(x,y) = (x+\rho x^2 - \rho) + \frac{1}{2}( -\rho y + xy - \rho x^2 - \rho y^2 + \rho) = \Big[x-\frac{\rho}{2}y\Big] + \Big[\frac{xy}{2}+\frac{\rho}{2} (x^2 - y^2 - 1) \Big]$}
\nonumber
\end{equation}
\begin{equation}
\resizebox{.98\hsize}{!}{$ %
    \circphi_y(x,y) = (\rho y+\rho y^2 - \rho) + \frac{1}{2}( -\rho y + xy - \rho x^2 - \rho y^2 + \rho) = \Big[\frac{\rho}{2}y\Big] + \Big[\frac{xy}{2}+\frac{\rho}{2} (y^2 - x^2 - 1) \Big]$}
\nonumber
\end{equation}



% \red{
% A further discussion is provided in the appendix, but in short, this surprising case is a consequence of the redundant information from the feature correlation becoming stronger than the synergistic information from the feature interaction.
% Such cases can introduce significant challenges to existing approaches using Shapley values or additive models, 
% which often implicitly or explicitly assume that input features are independent from one another.
% }



In Figure \ref{app_fig:baby_synthetic_learned_GAM}, we can see the learned set of functions across various $\rho\in\{0.0,0.1,\dots,1.0\}$ when learning a GAM with the purified loss function.
The Shapley functions can also be calculated to be $\circphi_x = \Tilde{f}_{x} + \frac{1}{2}\Tilde{f}_{xy}$ and $\circphi_y = \Tilde{f}_{y} + \frac{1}{2}\Tilde{f}_{xy}$ which can be seen in the fourth and fifth rows from the Figure.
We can furthermore see that the additive models using the purified loss align with the true purified ANOVA decomposition and that hence the Shapley value functions can be computed in constant time given the purified GAM model.
%
Further, going left to right, we see how the strength of the 1D effects (first and second rows) increase, whereas the strength of the 2D effects (third row) decreases as the amount of correlation increases.
This corresponds to the deterioration of the feature interaction due to the increase in feature correlation.
At the halfway mark, the 2D function is no longer positively correlated with the true outcome.
This is most obvious in the far right ($\rho=1.0$) plots where $\Tilde{f}_{x} = \Tilde{f}_{x} = -\Tilde{f}_{xy}$, meaning $\circphi_x = \frac{1}{2}\Tilde{f}_{x}$ and $\circphi_y = \frac{1}{2}\Tilde{f}_{y}$.
% }


Interestingly, we can see that for $|\rho|> \frac{1}{2}$, we actually have that $\bbC_{xy} < 0$.
That is to say, the interaction term alone is no longer positively correlated with the function we are trying to learn.
This further implies that the purified interaction is actually negatively correlated with our target, and adding it to the model somehow reduces the performance as measured with MSE.
This must be juxtaposed with the fact that our function $f(x,y) = x+xy$ clearly demonstrates a feature interaction in the term $xy$.

This can however be resolved by not thinking of the purified interaction alone but in conjunction with other features when it is added to the model.
For example, if we started with $y$ and added $x$, then we could consider $\bbC_x+\bbC_{xy}=2-2\rho^2 \geq 0$ as the improvement to the model.
Conversely, if we started with $x$ and added $y$, we could consider $\bbC_y+\bbC_{xy}=1-\rho^2 \geq 0$ as the improvement to the model.
Intuitively, it is not the fact that the interaction is detrimental to the model performance, as clearly it is necessary for $|\rho|<1$, but rather that it is overshadowed by the information which is gained from either $x$ or $y$ alone.
``The redundant information from knowing $x$ or $y$ outweighs the synergistic information from knowing $x$ and $y$.''







\subsection{10D Synthetic}

For our major synthetic experiments where we benchmark the ability of FastSHAP and InstaSHAP to recover the Shapley value effects,
we create a dataset similar to the simple ones in Figures \ref{fig:simple_2D_example_Shapley_fnl_calculation_synergy_and_redundancy} and  \ref{app_fig:baby_synthetic_learned_GAM}.
We generate ten features from a correlated pairs structure on the covariance matrix.
In practice, this will mean our low-dimensional synthetic target variable will remain having a low-dimensional functional ANOVA decomposition because of this simplistic correlation structure.
Moreover, this allows us to relatively easily calculate the exact Shapley functions even in this 10-dimensional dataset.

\[
% \begin{pmatrix}
% 1 & \rho & 0 & 0 &  \\
% \rho & 1 & 0 & 0 &  \\
% 0 & 0 & 1 & \rho & \\
% 0 & 0 & \rho & 1 & \\
%  &  &  &  & \ddots &  \\
%  &  &  &  &  & 1 & \rho \\
%  &  &  &  &  & \rho  & 1\\
% \end{pmatrix}
\Sigma = 
\begin{pmatrix}
1 & \rho &   \\
\rho & 1 &  \\
 &  & 1 & \rho & \\
 &  & \rho & 1 & \\
 &  &  &  & \ddots &  \\
 &  &  &  &  & 1 & \rho \\
 &  &  &  &  & \rho  & 1\\
\end{pmatrix}
\]

We then take the target variable to be 
\begin{align}
    f(x) = \sum_{S\in\cI^{\leq k*}} \beta_S \cdot \prod_{i\in S} x_i
\end{align}
for some $k*\in\bbN$ and for some $\beta_S$ drawn from the normal distribution $\cN(0,1)$ or the Laplace distribution $\text{Laplace}(0,1)$.
We finally divide by a constant to normalize the output response such that the total variance of the output is equal to one.





\subsection{Real-World Tabular Datasets}

% We follow the methods of SIAN \citep{enouen2022sian} to find a small number of feature interactions to include in a GAM model and train a neural network which obeys this low-dimensional GAM structure.
We follow the methods of SIAN \citep{enouen2022sian} to train GAM models for tabular datasets.
After the training of a surrogate model,
the Archipelago \citep{tsang2020archipelago} interaction detection method is applied to choose the most important feature interactions from the dataset.
After a small number of feature interactions are chosen from the dataset, a neural network which obeys this low-dimensional GAM structure is trained under the same loss function objective as in the main paper. 



\paragraph{Bikeshare}
This dataset predicts the expected bike demand each hour given some relevant features like the day of the week, time of day, and current weather.
There is a total of thirteen different input features predicting a single continuous output variable.

There is a gap in accuracy from GAM-1 to an MLP where the GAM-1 achieves an $R^2$ error of $17.4\%$, whereas an MLP achieves an $R^2$ error of $6.59\%$.
Using the techniques of SIAN,
we select $20$ tuples of size three or less to train a GAM-3 model which achieves $6.23\%$ $R^2$ error,
closing the gap between GAM-1$\equiv$SHAP through the usage of feature interactions.



In particular, 
it is well known that on this dataset there is a strong interaction between the hour variable and workday variable (since people's schedules change on the weekend vs. a workday.)
The ability to capture this particular feature interaction is critical for accurately understanding the dataset, as seen in Figure \ref{fig:bikeshare_spectrum_of_interpretability}.
Also in Figure \ref{fig:bikeshare_spectrum_of_interpretability},
it can be viewed how the interpretability-uninterpretability spectrum along the axis of additive models supports the hypothesis of \cite{rudin2019stopExplaining} by demonstrating that training an accurate GAM model is sufficient to explain SHAP; however, training an accurate SHAP score is not sufficient for training accurate GAM models.







\paragraph{Treecover}
The dataset consists of predicting the types of trees covering a specific forest area from a selection of 7 tree species (Spruce, Lodgepole Pine, Ponderosa Pine, Cottonwood, Aspen, Douglas-Fir, or Krummholz) in a Colorado national park based on
10 numerical features and 1 categorical feature of the area.
% 11 features of the area.


However, this misses the fact that both the elevation and soil type are additionally correlated with one another.
Indeed, the soils are grouped according to climatic zone which generally correspond to different altitude climates.
For convenience, we keep these soil classes in the same orders as their expected elevation, named: `lower montane', `upper montane', `subalpine', and `alpine'.
One notes that the Krummholz tree can be found at high altitudes but also in alpine (often rocky) soil.
Similarly, Cottonwoods, Douglas-firs, and Ponderosas are expected to be found at lower altitudes, but also to be found in montane soils.
Without an understanding that these two features are correlated with one another, it might a priori seem like these are two independent contributions to the prediction.
Yet again, it turns out that these two facts are indeed correlated with one another and {hence the 1D projections} alone may not be sufficient to yield a good explanation.

In this case, a lot can be gleaned by viewing the 2D shape function which depends on both the soil and the elevation.
In Figure \ref{fig:treecover_elev_and_or_soil_PDP}, we visualize this 2D shape function as a scatterplot with colored heatmap.
Through the density of points, we can see there is indeed a strong positive correlation between the soil type and the elevation.
Furthermore, using the colors for each tree species, we can see that there is a lot of redundant information carried by both the soil and the elevation, but also that there is some non redundant information.


We train an MLP on this dataset to achieve $80.4\%$ validation accuracy and we train a GAM-1 to achieve $72.4\%$ validation accuracy.
This once again shows a gap in the feature interactions which discredits the ability of SHAP to provide an adequate explanation of what is being learned by the MLP model.
Once again following the techniques of training lower-order GAMs, we are able to train a GAM-5 on 50 tuples to achieve $82.2\%$ accuracy.
This shows that likely there is some information which the low-order GAM with interactions can understand that GAM-1 and SHAP are missing.



% \subsection{Vision - Gaps between theory and practice}
\subsection{Computer Vision}

We perform experiments on the CUB dataset consisting of 200 different species of birds and containing over 6000 labeled images \citep{wah2011cubDataset}.
In addition to the species level information, we construct a coarser-grained class label out of the taxonomic family of each of those bird species.
This results in 37 coarse-grained labels for each bird.
For our main CNN we train a ResNet-50 model \citep{he2016resnet} on the masked surrogate objective and for our GAM-$K$x$K$ architecture we train a modififed resnet to only allow for communication between adjacent patches of size 16x16, further details in code.
Both models are initialized with mostly pretrained weights and fine-tuned for 300 epochs on the CUB dataset.


Our vanilla resnet is able to get to $65.0\%$ fine accuracy and $81.8\%$ coarse accuracy.
Compare this to the $33.2\%$ fine and $53.7\%$ coarse accuracies of the GAM-$1$x$1$.
There is clearly a sizable gap in performance between the full complexity ResNet and the GAM-restricted Resnet,
indicating the importance of feature interactions and emphasizing the potential deceptiveness of SHAP on this dataset.
The GAM-$2$x$2$ and GAM-$3$x$3$ achieve fine accuracies of $45.8\%$ and $46.8\%$ as well as coarse accuracies of $66.3\%$ and $66.8\%$.
This once again indicates that even with some feature interactions,
the performance of the Resnet is dependent on even higher order feature interactions or longer-range feature interactions.
As discussed, it is not impossible that these conclusions are only true for the training method used in the GAM-$K$x$K$ and that some novel GAM architecture would not be able to achieve higher performance.
Nonetheless, the issues in training modern neural network will be present also in alternative approaches like FastSHAP, implying that these conclusions are valid regardless.



Another major challenge of applying to domains like computer vision and natural language processing is the prolific usage of pretrained models for downstream tasks.
As discussed in \cite{covert2023shapleyForVIT},
this brings up the important question of how to do surrogate-based modeling to compute the conditional expectation $\cM$.
In principle, we would like to also do the pretraining stage with surrogate masking, however, in practice previous works on this domain \citep{jethani2022fastSHAP,covert2023shapleyForVIT} will instead use the pretrained models which are available and do the fine tuning stage with the masked objectives as we presented in the paper.
% \jam{Lack of surrogate models trained in CV and NLP where pretrained networks are rampant}


We briefly discuss the application of SHAP to classification and how it is different from its application to regression.
In particular, it is often common to train on the cross-entropy or $D_{KL}$ objective,
but still use the SHAP for regression directly on the logits.
There are some potential questions raised about how well this address the nuanced differences between the $D_{KL}$ objective and the $\| \cdot \|_2$ objective,
it is the choice made in previous works \citep{covert2023shapleyForVIT}.
Alternatives for classiffication like Shapley-Shubik \citep{enouen2024textGenSHAP} or Deegan-Packel \citep{biradar2024abudctiveExplanationsWithDeeganPackel} tend to focus on the one-hot scenario,
limiting their application for calibrated prediction.
Accordingly, all of our explanations are done on the logits or log-probabilites of the output prediction.
% \jam{train on but show the MSE version}
%
% Lack of applicability to the classification framework because of the variational focus and consequent need to deal with MSE and expectation projection.
% (exacerbated by the consequences of not training cross-entropy to zero loss and hence not doing a projection of any kind).






% We also note that with the inclusion of batch normalization in the GAM-$K$x$K$ model,
% we are able to achieve fine accuracies of $42\%$, $58\%$, $63\%$ and coarse accuracies of $62\%$, $77\%$, $80\%$.
% Due to the fact that typical batch normalization is applied across the image, 
% mixing the influences across all separated patches of the GAM model, we cannot report these as successes of the GAM model.
% Nevertheless, in light of works like \cite{brock2021normalizerFreeResnet} showing the possibility of training ResNets without requiring batch normalization tricks,
% it seems plausible that GAM models would be able to achieve more similar accuracies without the usage of BN tricks.
% Although, in this scenario, there is still a significant gap for the GAM-1x1 version which corresponds to the all popular SHAP explanations,
% it seems plausible that there could exist GAM-2x2 or GAM-3x3 models which are at least close to the full resnet pefromance.
% We leave the specific numerics of how much information is trapped in higher-order information to be more carefully explored over the architectural choices of the resnet in future work.



\newpage
\section{Additive Models}
\label{app_sec:additive_models_and_stuff}



\subsection{Purified Loss Equation}



We first reiterate the Fast-Faith-SHAP-k, GAM-k, and Insta-SHAP-GAM-k equations as:
\begin{align}
% \argmin_{\circphi:\cX\to\bbR^{\scalebox{0.55}{$\cI_{\leq k}$}}}
\argmin_{ \{\circphi_T\}_{T\in\scalebox{0.55}{$\cI_{\leq k}$}} }
\bigg\{ &
\bbE_{x\sim p(x)} \bigg[
\bbE_{\blue{S\sim p^{\text{SHAP}}(S)}} \bigg[
\Big\| 
f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \ind({T\subseteq S})\cdot \circphi_T(\blue{x})
\Big\|^2
\bigg]
\bigg] &
\bigg\}  \\
%
%
%
% \argmin_{\circphi:\cX\to\bbR^{\scalebox{0.55}{$\cI_{\leq k}$}}}
\argmin_{ \{\circphi_T\}_{T\in\scalebox{0.55}{$\cI_{\leq k}$}} }
\bigg\{ &
\bbE_{x\sim p(x)} \bigg[
\bbE_{\blue{S\sim p^{\text{GAM}}(S)}} \bigg[
\Big\| 
% f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \ind({T\subseteq S})\cdot \circphi_T(\blue{x_T})
f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \circphi_T(\blue{x_T})
\Big\|^2
\bigg]
\bigg] &
\bigg\}  \\
%
%
%
% \argmin_{\circphi:\cX\to\bbR^{\scalebox{0.55}{$\cI_{\leq k}$}}}
\argmin_{ \{\circphi_T\}_{T\in\scalebox{0.55}{$\cI_{\leq k}$}} }
\bigg\{ &
\bbE_{x\sim p(x)} \bigg[
\bbE_{\blue{S\sim p^{\text{SHAP}}(S)}} \bigg[
\Big\| 
f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \ind({T\subseteq S})\cdot \circphi_T(\blue{x_T})
\Big\|^2
\bigg]
\bigg] &
\bigg\}    
\end{align}


From the Fast-Faith-SHAP-k perspective,
the major modification we make is to remove the pointwise flexibility of each of the SHAP-k estimators $\circphi_T(x)$ and instead replace each functional approximator with a GAM-like functional approximator $\circphi_T(x_T)$.
This restricts the capacity of the functional amortizer but as discussed extensively this may give a more accurate representation of the true behavior and also is able to demonstrate improved convergence on synthetic datasets.

From the GAM-k perspective,
the major modification is to replace the typical unmasked distribution $p^{\text{GAM}}(S)$ (or sometimes nontrivial in fitting techniques like backfitting), with the masking distribution coming from the Shapley kernel distribution $p^{\text{SHAP}}(S)$.
The second key modification we include is the Instant Mask which only allows the additive influence of each function $\circphi_T(x_T)$ to flow to the final output so long as all of its constituents have been included in the observed mask $S$.
It follows that we may easily calculate downstream explanations of interest like the SHAP value because of the automatic purification of such effects.







% Connection to Additive Models and Further Discussion}

% \red{improve this with the better understanding}






% It follows from our Section \ref{app_sec:novel_GAM_frontier_and_correlated} that we may face cases where the Sobol indices are Sobol covariances are insufficient to measure the marginal contributions of a particular feature interaction.

% Regardless of this
% it is relatively straightforward to see that if we identify our GAM model with the masked model as part of the purified loss training, then the Shapley values and indices can be automatically computed from the equations in \ref{app_eqn:faith_SHAP_mobius_form}.
% It is moreover the case that this does not explicitly depend on the training distribution which is utilized in the purified loss training, which is unlike the FastSHAP training which uses arbitrarily complex auxiliaries, requiring the training only be done under the Shapley kernel.




% Note that although Faith-SHAP define the frontier solution for any $\cS_{\leq\ell} = \{S : |S|\leq\ell\}$, 
% given our notation of any $\cI\subseteq \cP([d])$, 
% we can consider the alternative resolution to the problem through the MSE approach:
% \[
% \cE(f,\ell) =  \argmin_{ \{\cE_S\}_{S\in\cI} }  \sum_{S\subseteq[d]} p(S) 
% \bigg|f(S) - \sum_{T\subseteq S, T\in\cI} \cE_T\bigg|^2
% \]




% \red{
% \subsection{Putting it together}
% }
% Faith + Fast

% \[
% \argmin_{\circphi:\cX\to\bbR^{N_k}} 
% \bigg\{
% \bbE_{S\sim p^{\text{SHAP}}(S)} \bigg[
% \Big| 
% f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \ind({T\subseteq S})\cdot \circphi_T(x)
% \Big|^2
% \bigg]
% \bigg\}
% \]


% Typical GAM
% \[
% \argmin_{\circphi:\cX\to\bbR^{N_k}} 
% \bigg\{
% \bbE_{x\sim p(x)} \bigg[
% \bbE_{S\sim p^{\text{GAM}}(S)} \bigg[
% \Big| 
% f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}} \ind({T\subseteq S})\cdot \circphi_T(x_T)
% \Big|^2
% \bigg]
% \bigg]
% \bigg\}
% \]
% \[
% \argmin_{\circphi:\cX\to\bbR^{N_k}} 
% \bigg\{
% \bbE_{x\sim p(x)} \bigg[
% \bbE_{S\sim p^{\text{GAM}}(S)} \bigg[
% \Big| 
% f(x;S) - \sum_{\substack{T\subseteq[d], |T|\leq k}}  \circphi_T(x_T)
% \Big|^2
% \bigg]
% \bigg]
% \bigg\}
% \]
% \[
% \argmin_{\circphi:\cX\to\bbR^{N_k}} 
% \bigg\{
% \bbE_{x\sim p(x)} \bigg[
% \Big| 
% f(x;[d]) - \sum_{\substack{T\subseteq[d], |T|\leq k}}  \circphi_T(x_T)
% \Big|^2
% \bigg]
% \bigg\}
% \]





% \section{Additive Models}




% \subsection{Connection to Additive Models and Further Discussion}
\subsection{Extension to Arbitrary Frontiers}
Although the extension of GAMs to higher-order interactions of size 3D and larger is simple to write down as
\begin{alignat}{3}
    % F^{\leq k}(x)
    F_{\leq k}(x)
    &\oct =\oct &
    \sum_{S\in\cI_{\leq k}} f_S(x_S),
\label{app_eqn:fnl_GAM_k_condensed}
\end{alignat}
the exploration of these higher-order GAMs delayed because
it is typical to be unable to explicitly model all higher-order interaction sets.
For instance, the size of $\cI_{\leq k}$ grows like $O(d^k)$  which is untenable for most practical purposes.
Instead, it has recently been proposed to select only a portion of these interactions as important enough to be included in the model \cite{yang2020gamiNet,dubey2022scalablePolynomials,enouen2022sian}.
We can consider these more general additive models by first choosing a candidate collection of interactions $\cI\subseteq\cP([d])$ and then writing the similar equation:
\begin{align}
    F_\cI(x) = \sum_{S\in\cI} f_S(x_S)
    \label{app_eqn:gam_via_subscollections}
\end{align}
Once again, we will say the order is the size of the largest subset $k = \max\{ |S| : S\in\cI \}$; however, there is now a much richer set of choices compared with the original hyperparameter selection of $k$.
Hence, despite its simplicity by choosing a small number of feature interactions, it does not provide a reduction in complexity unless we can also answer the question of which feature interactions to include.
% This formulation of an additive model as a sum over candidate interactions immediately begs the question of how to select the correct set of feature interactions to include into the additive model.





% \paragraph{HO GAM frontiers}
% \textcolor{blue}{
% Recent works have pushed towards investigating `higher-order feature interactions', corresponding to features sets of size 3D and larger \cite{yang2020gamiNet,dubey2022scalablePolynomials,enouen2022sian}.
% We can consider these more general additive models by first choosing a candidate set of interactions $\cI\subseteq\cP([d])$ and then writing the similar equation:
% \begin{align}
%     F(x) = \sum_{S\in\cI} f_S(x_S)
%     \label{eqn:gam_via_subsets}
% \end{align}
% %
% Once again, the order will be the size of the largest subset $k = \max\{ |S| : S\in\cI \}$.
% This formulation of an additive model as a sum over candidate interactions immediately begs the question of how to select the correct set of feature interactions to include into the additive model.
% }








\subsection{Sobol Solution with Independent Variables}
If we briefly return to the case of independent variables,
we find that the aforementioned decomposition of variance allows a precise answer to the question of feature interaction selection.
Moreover, this means that the functional ANOVA space and the GAM spaces are exactly connected with each other.
\begin{align}
\argmin_{ \{\circphi_T\}_{T\in\cI} }
\bigg\{ 
\bbE_{x\sim p(x)} \bigg[
% \bbE_{\blue{S\sim p^{\text{GAM}}(S)}} \bigg[
\Big\| 
F(x) -  \sum_{\substack{T\in\cI}} \circphi_T({x_T})
\Big\|^2
% \bigg]
\bigg] 
\bigg\} 
& = 
\sum_{T'\notin \cI} \bbV_{T'}
\end{align}

This of course means that if we have a good way to approximate the Sobol indices,
then we have an easy way to select for interaction tuples by choosing the largest Sobol indices.

In the case of correlated input variables, we are not so lucky.
Although the Sobol covariances \citep{rabitz2010correlatedSobolIndices,hart2018sobolCovariancesDependentVariables} are still able to give a decomposition of the variance of a function
\begin{align}
    \bbV
    =
    \sum_{S\subseteq[d]} \bbC_S
\label{app_eqn:sobel_decomp_of_covar}
\end{align}
where again $\bbC_S := \bbC\text{ov}_{X}[ F(X), \Tilde{f}_S(X_S)  ]$,
they no longer provide an answer to the effectiveness of an additive model with an arbitrary collection of feature interactions $\cI\subseteq\cP([d])$.
In particular, $\bbC_S$ may indeed be negative whereas adding an interaction to an additive model can never decrease its represntational capacity.
Intuitively, this corresponds to the case where the `constructive' information provided by allowing a feature interaction is overshadowed by the `destructive' information created by the redundancies of a feature correlation. 


It follows that we must be able to measure the efficacy of an additive model to represent a function under some distribution in an alternative way.
In the sections which follow, we will take a variational perspective to represent the efficacy of the interaction collection for an additive model and to begin to answer the question of how to distinguish the multiple types of feature interactions.









% \subsection{Additive Models with Arbitrary Frontiers}
\subsection{Additive Model Solution for Arbitrary Frontiers}


Because of the need to further granulate to the level of synergistic interactions and dissonant interactions, 
% \jam{we introduce the consideration of SIAN}
we find that it is necessary to study the entire set of possibilities for additive models.
We find that only then can one distinguish between the synergistic interactions and redundant interactions of Figure \ref{fig:simple_2D_example_Shapley_fnl_calculation_synergy_and_redundancy}
For $d=3$, we list all representatives (`frontiers') of nontrivial additive models in Figure \ref{fig:all_possible_feature_interactions_frontiers}.
For example, the function in Figure \ref{fig:simple_2D_example_Shapley_fnl_calculation_synergy_and_redundancy}d would be covered by $1,2 \equiv \{\emptyset,\{1\},\{2\}\}$ whereas the function from \ref{fig:simple_2D_example_Shapley_fnl_calculation_synergy_and_redundancy}a would need to be covered by $12 \equiv \{\emptyset,\{1\},\{2\},\{1,2\}\}$.


\begin{figure}[h]
    \centering
    % \includegraphics[width=0.8\linewidth]{figures/depictions/iclr 2024 figures -- all frontiers -- resized.pdf}
    \includegraphics[width=0.8\linewidth]{figures/depictions/iclr_2024_figures_--_all_frontiers_--_resized.pdf}
    \caption{All possible frontiers for a GAM model when $d=3$.}
    \label{fig:all_possible_feature_interactions_frontiers}
\end{figure}




As mentioned in the main text, one of the critical issues for using additive models to learn a particular target function, is solving the meta-optimization to find an optimal frontier for the additive model.
As of yet, there is seemingly no known measurements for the correlated input case paralleling the Sobol indices in the indepdendent input case.
In particular,
for a given frontier $\cI\subseteq\cP([d])$ and a candidate interaction $S\subseteq[d]$ not yet in the frontier,
there is seemingly no work trying to estimate the differences in errors between these two learned additive models.
Moreover, it is noted in the main body that the measurements $C_S$ are in general insufficient to measure these differences in all cases, and must only be used as an approximation.

Herein, we describe the solution to the additive model training procedure as the solution the Euler-Lagrange equation from calculus of variations.
Thereafter, we simplify our solution into a single matrix-operator functional equation defined by the projection operators $\cN_S$ in the function space $\cH$.
We then provide a formal solution to the matrix-operator equation and show how it can be approximated through repeated projections.

\begin{theorem}
Fix an input distribution $X\sim p(X)$ and a function $y=F(x)$.
% Consider a frontier $\cI = \{S_1,\dots,S_L\}$ and consider the solution to the additive model training equation:
Consider a collection $\cI = \{S_1,\dots,S_L\} \subseteq \cP([d])$ and consider the solution to the additive model training equation:
\begin{align}
\{g_T^*\}_T := \argmin_{\{g_T\}_T}\bigg\{ \bbE_X\bigg[
\Big\| F(X) - \sum_{T\in\cI} g_T(X_T) \Big\|^2
\bigg]\bigg\}    
\label{app_eqn:general_GAM_variational_X_equation}
\end{align}


Recalling the conditional expectation projection operators
\begin{align}
[\cM_S\circ F](x) :=
\bbE_{\Bar{X}_{-S} \sim p(X_{-S}|X_S=x_S)} 
\Big[ 
F(x_S,\Bar{X}_{-S})
\Big]
    \label{app_eqn:conditional_projection_once_again}
\end{align}
% \[
% [\cM_S \circ F](x) := \Big{\bbE}_{X_{S^C}\hspace{0.2em}\sim\hspace{0.2em} p(X_{S^C} | X_S = x_S)}\bigg[ F(x_S,X_{S^C}) \bigg]
% \]
where we drop the subscript denoting the distribution $p(x)$.


The solution to the variational GAM training equation in Equation \ref{app_eqn:general_GAM_variational_X_equation} obeys the matrix equation:
\begin{align}
\begin{pmatrix}
        e     & \cM_{S_1} & \dots & \cM_{S_1} \\
    \cM_{S_2} &     e     & \dots & \cM_{S_2} \\
    \vdots &   \vdots  & \ddots & \vdots \\
    \cM_{S_L} & \cM_{S_L} & \dots & e \\
\end{pmatrix}
\begin{pmatrix}
g^*_{S_1} \\ g^*_{S_2} \\ \vdots \\ g^*_{S_L}
\end{pmatrix}
=
\begin{pmatrix}
f_{S_1} \\ f_{S_2} \\ \vdots \\ f_{S_L}
\end{pmatrix}
\label{app_eqn:matrix_operator_eqn}
\end{align}

    
\end{theorem}
\begin{proof}
Let the objective functional be defined
\begin{align}
    \label{app_eqn:gam_variational_objective_functional}
J(\{g_T\}_T) := 
\bbE_X\bigg[
\Big\| F(X) - \sum_{T\in\cI} g_T(X_T) \Big\|^2
\bigg]\bigg\}
% \bbE_X\bigg[
% \bigg| F(X) - \sum_{T\in\cI} g_T(X) \bigg|^2
% \bigg]
% \nonumber
\end{align}
    Recall from calculus of variations the Euler-Lagrange equation:
    \begin{align}
        \frac{\delta J}{\delta g_{S_i}} \equiv 0 \nonumber
    \end{align}
    for each possible $S_i\in\cI$, which then implies:
    \begin{align}
        \lim_{\eps_{S_i}\to 0} \frac{1}{\eps_{S_i}}\Big[ J(\{g_T+\eps_T\cdot\delta_T\}) - J(\{g_T\}_T)\Big] \equiv 0 \nonumber  \\
        \lim_{\eps\to 0} \frac{1}{\eps}
\bbE_X\bigg[ \Big\| F(X) - \sum_{T\in\cI} g_T(X_T) - \eps\cdot \delta_{S_i}(X_{S_i}) \Big\|^2 - \Big\| F(X) - \sum_{T\in\cI} g_T(X_T) \Big\|^2  \bigg]  \equiv 0 \nonumber  \\
        \lim_{\eps\to 0} 
\bbE_X\bigg[  2 \delta_{S_i}(X_{S_i}) \cdot \Big[ F(X) - \sum_{T\in\cI} g_T(X_T) \Big]  + \frac{1}{\eps} \delta_{S_i}^2(X_{S_i})  \bigg]  \equiv 0 \nonumber  \\
\bbE_X\bigg[  2 \delta_{S_i}(X_{S_i}) \cdot \Big[ F(X) - \sum_{T\in\cI} g_T(X_T) \Big]   \bigg]  \equiv 0 \nonumber  \\
        \bbE_{X_{S_i}}\bigg[ \delta_{S_i}(X_{S_i}) \cdot \bbE_{X_{-{S_i}} | X_{S_i}}\bigg[ F(X) -  \sum_{{S_i}\in\cI} g_{T}(X_T) \bigg]\bigg]  \equiv 0 \nonumber \\ 
        \bbE_{X_{-{S_i}} | X_{S_i}}\bigg[ F(X) -  \sum_{{T}\in\cI} g_{T}(X_T) \bigg] \equiv 0 \nonumber 
    \end{align}

    This can simply be rewritten as:
    \begin{align}
        f_{S_i}(X) \equiv \cM_{{S_i}} \circ \sum_{T\in\cI} g_T(X) \nonumber \\ 
        f_{S_i}(X) \equiv \cM_{{S_i}} \circ g_{S_i}(X) + \sum_{T\in\cI-{S_i}} \cM_{{S_i}} \circ g_T(X) \nonumber \\ 
        f_{S_i}(x) \equiv g_{S_i}(x) + \sum_{T\in\cI-{S_i}} [{S_i}\circ g_T](x) \nonumber \\
        f_{S_i} =  g_{S_i} + \sum_{T\in\cI-{S_i}} [{S_i}\circ g_T] \nonumber 
    \end{align}
    Hence, it can be seen that each row of the matrix equation corresponds to a partial gradient from the Euler-Lagrange equation as desired.
    Thus, any possible solution to minimization of the quadratic functional $J$ must obey the above matrix equation.
\end{proof}

It is moreover the case that we can reduce a collection of feature interactions to its `frontier' or its solution only on the largest subsets which have no supersets included.
From the perspective of the poset $\cP([d])$, this corresponds to the set of maximal elements.
It can be seen in the above proof that any solution on a projection $S_1 \subseteq S_2$ must automatically be obeyed by the operator equation for the larger set $S_2$.

Accordingly, identify a {frontier} $\cI$ with its set of maximal elements $T_1, \dots, T_{L'}$.
Following from Equation \ref{app_eqn:matrix_operator_eqn}, we can ensure that it is enough to solve the matrix equation:
\begin{align}
\begin{pmatrix}
        e     & \dots & \cM_{T_1} \\
    \vdots & \ddots & \vdots \\
    \cM_{T_{L'}} & \dots & e \\
\end{pmatrix}
\begin{pmatrix}
g^*_{T_1} \\ \vdots \\ g^*_{T_{L'}}
\end{pmatrix}
=
\begin{pmatrix}
f_{T_1} \\ \vdots \\ f_{T_{L'}}
\end{pmatrix}
% \nonumber
\label{app_eqn:matrix_operator_eqn_mountainTops}
\end{align}
Which we can then take the formal inverse of the operator matrix to yield a solution
\begin{align}
\begin{pmatrix}
g^*_{T_1} \\ \vdots \\ g^*_{T_{L'}}
\end{pmatrix}
\text{``}=\text{''}
\begin{pmatrix}
        e     & \dots & \cM_{T_1} \\
    \vdots & \ddots & \vdots \\
    \cM_{T_{L'}} & \dots & e \\
\end{pmatrix}^{-1}
\begin{pmatrix}
f_{T_1} \\ \vdots \\ f_{T_{L'}}
\end{pmatrix}
% \nonumber
\label{app_eqn:matrix_operator_eqn_mountainTops_inverted}
\end{align}
so long as we take care with the determinant in realizing that a matrix of non-commutative elements does not have a well-defined matrix determinant as it does in the commutative case.

Nonetheless, let us now illustrate the usefulness of such a formal inverse in a simple case with our synthetic example from earlier.
\begin{align}
\begin{pmatrix}
        e     & \cM_{x} \\
    \cM_{y}  & e \\
\end{pmatrix}
\begin{pmatrix}
g^*_{x} \\ g^*_{y}
\end{pmatrix}
=
\begin{pmatrix}
f_{x} \\  f_{y}
\end{pmatrix}
\nonumber 
\end{align}
\begin{align}
\begin{pmatrix}
        e     & -\cM_{x} \\
    -\cM_{y}  & e \\
\end{pmatrix}
\begin{pmatrix}
        e     & \cM_{x} \\
    \cM_{y}  & e \\
\end{pmatrix}
\begin{pmatrix}
g^*_{x} \\ g^*_{y}
\end{pmatrix}
=
\begin{pmatrix}
        e     & -\cM_{x} \\
    -\cM_{y}  & e \\
\end{pmatrix}
\begin{pmatrix}
f_{x} \\  f_{y}
\end{pmatrix}
\nonumber 
\end{align}
\begin{align}
\begin{pmatrix}
        e-\cM_{x}\cM_{y}     & 0 \\
    0  & e-\cM_{y}\cM_{x} \\
\end{pmatrix}
\begin{pmatrix}
g^*_{x} \\ g^*_{y}
\end{pmatrix}
=
\begin{pmatrix}
f_{x} - \cM_{x}\circ f_{y} \\  f_{y}-\cM_{y}\circ f_{x}
\end{pmatrix}
\nonumber
\end{align}
\begin{align}
\begin{pmatrix}
e -\cM_{x}\cM_{y}\\ e -\cM_{y}\cM_{x}
\end{pmatrix}
\odot
\begin{pmatrix}
g^*_{x} \\ g^*_{y}
\end{pmatrix}
=
\begin{pmatrix}
f_{x} - \cM_{x}\circ f_{y} \\  f_{y}-\cM_{y}\circ f_{x}
\end{pmatrix}
\nonumber
\end{align}
\begin{align}
\begin{pmatrix}
g^*_{x} \\ g^*_{y}
\end{pmatrix}
=
\begin{pmatrix}
[e -\cM_{x}\cM_{y}]^{-1} \circ [f_{x} - \cM_{x}\circ f_{y}] \\  [ e -\cM_{y}\cM_{x}]^{-1} \circ [f_{y}-\cM_{y}\circ f_{x}]
\end{pmatrix}
\nonumber
\end{align}
We can then use the formal Taylor series expansion of the operator inverse to yield:
\begin{align}
g^*_{x} = \sum_{n=0}^\infty (\cM_{x}\cM_{y})^n \circ \Big[f_{x} - \cM_{x}\circ f_{y}\Big] \nonumber \\
g^*_{y} = \sum_{n=0}^\infty (\cM_{y}\cM_{x})^n \circ \Big[f_{y} - \cM_{y}\circ f_{x} \Big] \nonumber 
\end{align}
If we choose to denote repeated projections with semicolons, we can then write our solutions as 
\begin{align}
g^*_{x} = f_x - f_{y;x} + f_{x;y;x} - f_{y;x;y;x} + f_{x;y;x;y;x} - f_{y;x;y;x;y;x} + \dots  \nonumber \\
g^*_{y} = f_y - f_{x;y} + f_{y;x;y} - f_{x;y;x;y} + f_{y;x;y;x;y} - f_{x;y;x;y;x;y} + \dots  \nonumber 
\end{align}
So then we can caclulate this to be
\begin{align}
g^*_{x} &= (x + \rho x^2 - \rho \nonumber) - (\rho^2 x + \rho^3 x^2 + \rho(1-\rho^2) - \rho) + (\rho^2 x + \rho^5 x^2 + \rho^3(1-\rho^2) + \rho(1-\rho^2) - \rho \nonumber) - \dots \\
 &= [x] + [-\rho + \rho^3 - \rho^5 + \dots] + [\rho - \rho^3 + \rho^5 - \dots] x^2 \nonumber\\
 &= x + \frac{\rho}{1+\rho^2}[x^2-1]\nonumber\\
g^*_{y} &= (\rho y + \rho y^2 - \rho ) - (\rho y + \rho^3 y^2 + \rho(1-\rho^2) - \rho) + (\rho^3 y + \rho^5 y^2 + \rho^3(1-\rho^2) + \rho(1-\rho^2) - \rho \nonumber) - \dots \\
 &= 0 + [-\rho + \rho^3 - \rho^5 + \dots] + [\rho - \rho^3 + \rho^5 - \dots] y^2 \nonumber\\
 &= 0 + \frac{\rho}{1+\rho^2}[y^2-1]\nonumber
\end{align}

It may be checked that this solution agrees with that of directly solving Equation \ref{app_eqn:matrix_operator_eqn}:
\begin{align}
g^*_{x} = x + \frac{\rho}{1+\rho^2}[x^2-1]\nonumber
\quad
g^*_{y} = \frac{\rho}{1+\rho^2}[y^2-1]\nonumber
\end{align}

It should at the very least be cautioned that these operator manipulations, especially that of the inverse are done only in the formal sense.
For instance, considerations of the limit point $\rho=1$ are not able to demonstrate local convergence in the inversion; however, the formula still remains true in this case.
It is considered very likely that these matrix equations are, in most cases, easily able to be solved by the suggested formal manipulations but at least some caution should be exercised.
% Nevertheless, these manipulations should hold true for a wide variety of distributions so long as the functions involved have finite variances in all dimensions.
% It is at least seemingly sufficient to consider all possible distributions with finite moments and distribution defined by their moments, since this would enable an iterative approximation via Taylor series, and
% it is conjectured this inversion will hold for an even wider set of distributions.
% \jam{rephrase bc reviewer 2 is a cunt :(}


















% \subsection{Future}
\subsection{Implications}
We reiterate how the additive model's ability to distinguish between synergistic feature interactions and redundant feature interactions is a key strength which has yet to be fully utilized in either the literature on SHAP or the literature on GAMs.
By the introduction to the characterization of the set of GAM solutions and drawing parallels with where this aligns and misaligns with the ever-popular functional ANOVA decomposition,
we provide a further set of tools to explore SHAP which goes beyond the `feature-only' perspective of functional ANOVA alone, and begins to explore the `feature interaction' perspective which adequately handles the intimate complexities which are introduced in the case of correlated variables.

This spectrum of additive models which operates over the entire combinatorially large set of frontiers of additive models is able to give a much more nuanced picture of the underlying structure of both the underlying statistical manifold of the input $X$ variables, but in conjunction with the mapping to the output $Y$ variables.
Compared with existing theory in functional ANOVA which spans the entirety of the exponential feature interaction space, this variational formulation covers the range of structures living in the doubly exponential space of all frontiers.
As demonstrated in this work, such structure can be directly accessed with relatively simple machine learning approaches, that is additive models and feature masking.
It is envisioned that there yet remains many directions of further theoretical exploration to more succinctly and understandably represent the underlying structures of a statistical mapping between data, while simultaneously there still exists abundant opportunities in the application of these learnings directly to machine learning, particularly in bridging the gaps from supervised learning to semi-supervised learning and semi-supervised learning to unsupervised learning.

















\newpage
\section{Additional Results}




\subsection{Additional Synthetic Results}


\begin{figure}[h]
    \centering
    % \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.0000_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.0000_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.2500_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.5000_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.7071_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.8409_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.9170_1D_model_SHAP_errors.pdf}
    \caption{Model MSE Error of SHAP values.  Comparison with test-time permutation sampling. $k^*=1$.}
    \label{app_fig:fast_shap_vs_insta_shap_error_curves_k1}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.0000_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.2500_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.5000_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.7071_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.8409_1D_model_SHAP_errors.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.9170_1D_model_SHAP_errors.pdf}
    \caption{Model MSE Error of SHAP values.  Comparison with test-time permutation sampling. $k^*=2$.}
    \label{app_fig:fast_shap_vs_insta_shap_error_curves_k2}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.0000_1D_model_SHAP_errors_logscale.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.2500_1D_model_SHAP_errors_logscale.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.5000_1D_model_SHAP_errors_logscale.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.7071_1D_model_SHAP_errors_logscale.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.8409_1D_model_SHAP_errors_logscale.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_1_rho_0.9170_1D_model_SHAP_errors_logscale.pdf}
    \caption{Model MSE Error of SHAP values (logarithmic scale).  Comparison with test-time permutation sampling. $k^*=1$.}
    \label{app_fig:fast_shap_vs_insta_shap_error_curves_k1_log}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.0000_1D_model_SHAP_errors_logscale.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.2500_1D_model_SHAP_errors_logscale.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.5000_1D_model_SHAP_errors_logscale.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.7071_1D_model_SHAP_errors_logscale.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.8409_1D_model_SHAP_errors_logscale.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/november_k_2_rho_0.9170_1D_model_SHAP_errors_logscale.pdf}
    \caption{Model MSE Error of SHAP values (logarithmic scale).  Comparison with test-time permutation sampling. $k^*=2$.}
    \label{app_fig:fast_shap_vs_insta_shap_error_curves_k2_log}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.0000_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.2500_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.5000_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.7071_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.8409_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.9170_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \caption{Model MSE Error of SHAP values (logarithmic scale).  Comparison with pre-test-time functional amortization. $k^*=1$.}
    \label{app_fig:fast_shap_vs_insta_shap_error_curves_k1_log_epochs}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.0000_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.2500_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.5000_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.7071_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.8409_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.9170_1D_model_SHAP_errors_logscale_training_epochs.pdf}
    \caption{Model MSE Error of SHAP values (logarithmic scale).  Comparison with pre-test-time functional amortization. $k^*=2$.}
    \label{app_fig:fast_shap_vs_insta_shap_error_curves_k2_log_epochs}
\end{figure}


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.0000_1D_model_SHAP_errors_logscale.pdf}
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.2500_1D_model_SHAP_errors_logscale.pdf}
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.5000_1D_model_SHAP_errors_logscale.pdf}
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.7071_1D_model_SHAP_errors_logscale.pdf}
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.8409_1D_model_SHAP_errors_logscale.pdf}
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.9170_1D_model_SHAP_errors_logscale.pdf}
%     \caption{Model MSE Error of SHAP values (logarithmic scale).  Comparison with test-time permutation sampling. $k^*=1$.}
%     \label{app_fig:fast_shap_vs_insta_shap_error_curves_k1_log_epochs_direct}
% \end{figure}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.0000_1D_model_SHAP_errors_logscale.pdf}
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.2500_1D_model_SHAP_errors_logscale.pdf}
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.5000_1D_model_SHAP_errors_logscale.pdf}
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.7071_1D_model_SHAP_errors_logscale.pdf}
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.8409_1D_model_SHAP_errors_logscale.pdf}
%     \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.9170_1D_model_SHAP_errors_logscale.pdf}
%     \caption{Model MSE Error of SHAP values (logarithmic scale).  Comparison with test-time permutation sampling. $k^*=2$.}
%     \label{fig:fast_shap_vs_insta_shap_error_curves}
% \end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.0000_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.2500_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.5000_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.7071_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.8409_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_1_rho_0.9170_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \caption{True MSE Error of SHAP values (logarithmic scale).  Comparison with pre-test-time functional amortization. $k^*=1$.}
    \label{app_fig:fast_shap_vs_insta_shap_error_curves_k1_log_epochs_direct}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.0000_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.2500_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.5000_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.7071_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.8409_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \includegraphics[width=0.31\linewidth]{figures/synth_appendix/k_2_rho_0.9170_1D_true_SHAP_errors_logscale_trainingEpochs.pdf}
    \caption{True MSE Error of SHAP values (logarithmic scale).  Comparison with pre-test-time functional amortization. $k^*=2$.}
    \label{app_fig:fast_shap_vs_insta_shap_error_curves_k2_log_epochs_direct}
\end{figure}

\phantom{xd}
\newpage
\phantom{xd}
\newpage
\phantom{xd}
\newpage
\phantom{xd}


\newpage
\subsection{Additional Vision Results}


\begin{figure}[h]
    \centering
    % \includegraphics[width=1.00\linewidth]{figures/birds/iclr 2024 eight different birds with SHAP.pdf}
    \includegraphics[width=1.00\linewidth]{figures/birds/iclr_2024_eight_different_birds_with_SHAP.pdf}
    \caption{A repeat of Figure \ref{fig:CUB_with_SHAP_across_multiple_different_species}.
    Provided for closer comparison with Figure \ref{app_fig:CUB_with_contrastive_SHAP_across_multiple_different_species} below.
    }
    \label{app_fig:CUB_with_SHAP_across_multiple_different_species}
\end{figure}

\newpage
\phantom{.}
\vspace{0.05in}
\begin{figure}[h]
    \centering
    % \includegraphics[width=1.00\linewidth]{figures/birds/iclr 2024 eight different birds with contrastive SHAP and colorbars.pdf}
    \includegraphics[width=1.00\linewidth]{figures/birds/iclr_2024_eight_different_birds_with_contrastive_SHAP_and_colorbars.pdf}
    \caption{An alternate version of Figure \ref{fig:CUB_with_SHAP_across_multiple_different_species} which includes the contrastive Shapley value.  In each panel, two similar-looking species are directly compared against one another.  For instance, the first two columns compare the Indigo Bunting species (more green) versus the Lazuli Bunting species (more magenta).}
    \label{app_fig:CUB_with_contrastive_SHAP_across_multiple_different_species}
\end{figure}


In Figure \ref{app_fig:CUB_with_contrastive_SHAP_across_multiple_different_species},
we can see some more granular details about the model explanations with respect to certain species.
For instance, the bluer and browner feathers of the two Bunting species, especially in the GAM-1x1 model.
Some other characteristics which seem to be picked up by some of the models are the orange beak and black mask of the Cardinal vs. the ordinary beak and face of the Summer Tanager; the yellow feet of the California Gull vs. the orange feet of the Western Gull; and the different upper backs and eye areas for the Prothonotary and Blue-winged Warblers.

% \red{doesnt explore contrastive enough}


\newpage
\subsection{Additional Healthcare Results}
We additionally train on a tabular version of the MIMIC healthcare dataset which consists of thirty features used to predict hospital outcomes.
We train an ensemble of five additive models using the vanilla GAM training procedure and the InstaSHAP masked training procedure.
In Figure \ref{app_fig:mimic_all_shapes} below, we display all shape functions learned by the 1D additive model.
We plot the mean and one standard deviation according to the ensemble of five models.

The vanilla GAM models achieve accuracies of 
$91.0\%$, $91.5\%$, $90.6\%$, $91.1\%$,  and $91.2\%$ 
for an average accuracy of $91.1\%$.
The InstaSHAP GAM models achieve accuracies of
$91.5\%$, $91.3\%$, $91.2\%$, $91.3\%$, $91.0\%$ 
for an average accuracy of $91.3\%$.
Generally, the InstaSHAP models have a more consistent interpretation of the dataset and achieve tighter confidence intervals than the typical training procedure.
It can then be assumed a significant amount of the variance between the vanilla ensemble is due to overinterpretaion or sensitivity to the natural correlations of the dataset.

% [91.0,91.5,90.6,91.1,91.2,]
% 91.1
% [91.5,91.3,91.2,91.3,91.0,]
% 91.3


\phantom{xd}
\newpage




\begin{figure}[h]
    \centering
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_0.pdf}
    \quad\quad\quad
    \quad\quad\quad
    \quad\quad\quad    
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_1.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_2.pdf}
    %
    \includegraphics[width=0.23\textwidth]
    {figures/healthcare_and_finance/mimic/shape_fn_3.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_4.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_5.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_6.pdf}
%
    \centering
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_7.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_8.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_9.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_10.pdf}
%
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_11.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_12.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_13.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_14.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_15.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_16.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_17.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_18.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_19.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_20.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_21.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_22.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_23.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_24.pdf}
    \quad\quad\quad
    \quad\quad\quad
    \quad\quad\quad
%
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_25.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_26.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_27.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_28.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/mimic/shape_fn_29.pdf}
    \caption{Shape Functions for the MIMIC dataset}
    \label{app_fig:mimic_all_shapes}
\end{figure}





\newpage
\subsection{Additional Finance Results}
We train on the census income dataset which consists of thirteen features used to predict whether or not a person's income surpasses a certain level (\$50,000 annually).
We train an ensemble of five additive models using the vanilla GAM training procedure and the InstaSHAP masked training procedure.
In Figure \ref{app_fig:adults_all_shapes} below, we display all shape functions learned by the 1D additive model.
We plot the mean and one standard deviation according to the ensemble of five models.

The vanilla GAM models achieve accuracies of 
$82.1\%$,  $84.6\%$, $85.1\%$, $84.4\%$, $84.7\%$, 
for an average accuracy of $84.2\%$.
The InstaSHAP GAM models achieve accuracies of
$82.1\%$,  $85.4\%$,  $84.4\%$, $84.7\%$, $84.7\%$, 
for an average accuracy of $84.3\%$.
We again find that the InstaSHAP models have a more consistent interpretation of the dataset via tighter confidence intervals over the ensemble.
Once again, it is assumed that the variance in typically trained GAMs is coming from the inability to consistently interpret the correlations which exist in the dataset.



% [82.1,84.6,85.1,84.4,84.7,]
% 84.2
% [82.1,85.4,84.4,84.7,84.7,]
% 84.3



\begin{figure}[h]
    \centering
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_0.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_9.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_10.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_11.pdf}
%
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_1.pdf}
    \includegraphics[width=0.23\textwidth]
    {figures/healthcare_and_finance/adults/shape_fn_3.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_4.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_5.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_6.pdf}
%
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_7.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_8.pdf}
    \quad\quad\quad
    \quad\quad\quad
    \quad\quad\quad
%
%
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_2.pdf}
    \includegraphics[width=0.23\textwidth]{figures/healthcare_and_finance/adults/shape_fn_12.pdf}
    \caption{Shape Functions for the Income dataset}
    \label{app_fig:adults_all_shapes}
\end{figure}







% \newpage
% \subsection{Additional Images}


% \red{lorem ipsum}



















\end{document}
