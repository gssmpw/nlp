\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{xurl}

\usepackage{authblk}

\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{float}
\usepackage{enumitem}
\usepackage{flushend}
\usepackage{xcolor}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\newtheorem{definition}{Definition}


\begin{document}

\title{
Responsible Artificial Intelligence Systems: \\

A Roadmap to Society's Trust through Trustworthy AI, Auditability, Accountability, and Governance}

\newcommand{\shorttitle}{Responsible Artificial Intelligence Systems: A Roadmap to Society's Trust}  

\author{Andr\'es Herrera-Poyatos, Javier Del Ser, Marcos {López de Prado}, Fei-Yue Wang, \\ Enrique Herrera-Viedma, and~Francisco~Herrera}

%\affil{Andalusian Institute of Data Science and Computational Intelligence (DaSCI), University of Granada, Spain. \\ Emails: \texttt{\{andreshp, viedma\}@ugr.es}, \texttt{herrera@decsai.ugr.es}}


\blfootnote{This publication is part of the IAFER TSI-100927-2023-1 Project, funded by the Recovery, Transformation, and Resilience Plan of the Next Generation of the European Union through the Spanish Ministry of Digital Transformation and the Civil Service (F. Herrera and E. Herrera-Viedma). It is partially supported by The Science and Technology Development Fund of Macau SAR (No. 0145/2023/RIA3) (F-Y. Wang), and the Basque Government under ELKARTEK funding grants and the consolidated research group MATHMODE IT1456-22 (J. Del Ser). Andr\'es Herrera-Poyatos, Enrique Herrera-Viedma and Francisco Herrera are members of the Andalusian Research Institute in Data Science and Computational Intelligence (DaSCI), University of Granada, 18071 Granada, Spain. E-mail: \{andreshp,viedma,herrera\}@ugr.es. Javier Del Ser is with TECNALIA, Basque Research \& Technology Alliance (BRTA), 48160 Bizkaia, Spain; and with the University of the Basque Country (UPV/EHU), 48940 Leioa, Spain. E-mail: javier.delser@tecnalia.com. Marcos {López de Prado} is with the School of Engineering, Cornell University, Ithaca, NY, 14850, United States; and ADIA Lab, Al Maryah Island, Abu Dhabi, United Arab Emirates; and the Dept. of Mathematics, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates. E-mail: ml863@cornell.edu.}
%\thanks{Fei-Yue Wang is with the Intelligent Systems for Robotics and Automation Laboratory, Macau University of Science and Technology, Macau SAR; and The State Key Laboratory for Management and Control of Complex Systems, CASIA, Beijing, China. E-mail: feiyue.wang@ia.ac.cn.}


\maketitle

\begin{abstract}
Artificial intelligence (AI) has matured as a technology, necessitating the development of responsibility frameworks that are fair, inclusive, trustworthy, safe and secure, transparent, and accountable.  By establishing such frameworks, we can harness the full potential of AI while mitigating its risks, particularly in high-risk scenarios. This requires the design of responsible AI systems based on trustworthy AI technologies and ethical principles, with the aim of ensuring auditability and accountability throughout their design, development, and deployment, adhering to domain-specific regulations and standards.

This paper explores the concept of a responsible AI system from a holistic perspective, which encompasses four key dimensions: 1) regulatory context; 2) trustworthy AI technology along with standardization and assessments; 3) auditability and accountability; and 4) AI governance. The aim of this paper is double. First, we analyze and understand these four dimensions and their interconnections in the form of an analysis and overview. Second, the final goal of the paper is to propose a roadmap in the design of responsible AI systems, ensuring that they can gain society's trust. To achieve this trustworthiness, this paper also fosters interdisciplinary discussions on the ethical, legal, social, economic, and cultural aspects of AI from a global governance perspective. Last but not least, we also reflect on the current state and those aspects that need to be developed in the near future, as ten lessons learned. 

\end{abstract}

\keywords{Responsible AI systems, trustworthy AI, auditability, accountability, explainability, AI safety, AI governance.}


\section{Introduction}

Artificial Intelligence (AI) has evolved into a mature and sophisticated technology, quietly entered our lives, and made a great leap in the last year. Generative AI models have shown that AI has gone, in just a few months, practically from science fiction to becoming an essential part of the daily lives of hundreds of millions of people around the world.

This emergence goes hand in hand with a growing global debate on the ethical dimension of AI. Concerns arise about its impact on data privacy, fundamental rights, and protection against discrimination in automated decisions, or the continued presence of fake videos and images. Although some risks of AI are relatively well known, such as the potential for automated decisions to be harmful to certain vulnerable groups, there are other less obvious risks, such as hidden biases that can arise from the data used in its training, or the vulnerability of AI systems to adversarial attacks \cite{critch2023tasra,hendrycks2023overview, hendrycks2024introduction, bengio2024managing}.

This whole scenario raises the need to establish responsible, fair, inclusive, trusted, safe and secure, transparent, and accountable frameworks. Before precisely defining these concepts, we first dive into the current status of AI regulation. The European AI Act is one of the most advanced regulations worldwide, setting comprehensive standards for the development, deployment, and use of AI \cite{AIA24}. It was published on June 13th, 2024, as the first attempt to enact a horizontal AI regulation. The proposed legal framework establishes a technology-neutral definition of AI systems in EU legislation and defines a classification for AI systems based on risk level problems, risk scenarios, with different requirements and obligations tailored to a risk-based approach. The obligations of an AI system are proportional to the level of risk it poses.

In this context, a technical approach to AI has emerged, called trustworthy AI (TAI) \cite{ai2019high}. Our definition of the TAI paradigm is later given in Definition \ref{def:tai}, Section \ref{sec:TAI}. Here, we highlight that TAI can be understood as a systemic approach that acts as a set of technologies and requirements to develop, deploy, and use AI systems ethically, in some sense as a technical prerequisite for people and societies. Three main pillars and seven requirements can be distinguished in TAI: the legal, ethical, and robustness pillars; and the following technical requirements: human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, nondiscrimination, and fairness; societal and environmental well-being; and accountability. In addition to this, it is necessary to consider a holistic view of TAI, as described in \cite{diaz2023connecting}, by bridging the gap between theory and practice. This holistic view aims to ultimately highlight the importance of all these requirements in the development and integration of human-centered AI-based systems into the everyday life of humans, in a natural and sustainable way. We introduce briefly the two fundamental sides, theory and practice:
\begin{itemize}[leftmargin=*]
    \item Theory: The ethical principles and philosophical approaches to AI ethics, such as respect for human autonomy, prevention of harm, fairness, and explainability.
    
    \item Practice: Regulation based on high-risk levels and the design of intelligent systems that follow this regulation from a legal and ethical point of view. These systems are called \textit{"responsible AI systems"} (hereafter referred to as RAI systems), as introduced in \cite{diaz2023connecting}. The two fundamental elements in this context are the auditability and accountability of RAI systems. Any progress in auditability and accountability would lead to an increase in society's trust in AI. They are also aligned with certification, verification, and validation to achieve a more robust and trustworthy AI \cite{li2022features}.
\end{itemize}

It should be noted that the adoption of TAI \cite{kaur2022trustworthy,li2023trustworthy} in the form of practical frameworks is not yet a reality; conceptual models to materialize TAI are just being born and are far from common practice (see, for example, the TAII framework \cite{baker2021taii}, the scenario engineering methodology \cite{li2022features, li2022novel}, and the Wasabi conceptual model \cite{singh2023wasabi}).

The term \emph{responsible AI} can be mistakenly confused as a synonym of TAI. Therefore, it is necessary to make an explicit statement on the similarities and differences between trustworthy and responsible AI. The main difference is the fact that responsible AI emphasizes the ethical and legal use of an AI-based system, including its auditability, accountability, and liability, whereas TAI is a global paradigm including legal aspects, but also integrating technological requirements like explainability, robustness, algorithmic fairness, etc.  

This work aims to shed light on these differences by thoroughly and systematically exploring the concept of RAI systems \cite{diaz2023connecting}. This analysis is done from a holistic perspective, which encompasses four key dimensions:
\begin{enumerate}[leftmargin=*]
    \item the regulatory context,
    \item the TAI paradigm along with standardization and assessments,
    \item auditability and accountability, and 
    \item AI governance, requiring a transversal vision.
\end{enumerate}   

This paper is presented as an overview to analyze in depth these four key dimensions and as a position paper for presenting a roadmap framework proposal to design RAI systems. 

\begin{itemize}


\item  The first aim of this work is to understand these dimensions and their interconnections, ensuring that RAI systems can gain trustworthiness while fostering interdisciplinary discussions on the ethical, legal, social, economic and cultural aspects (ELSEC \cite{scantamburlo2020progressing}) of AI. 

\item  The second aim of this paper is to propose a roadmap in the design of RAI systems, as well as to present a reflection summarized as the ten lessons learned during the development of RAI systems. This reflection analyzes the current status of research and the aspects that must be developed in the near future. 

\end{itemize}

The organization of this paper is as follows. Section~\ref{sec:RAIS} introduces the basic questions on the concept of RAI system, paying attention to \emph{what?}, \emph{why?} and \emph{how?}, the latter from a contextual point of view. Section \ref{sec:TAI} introduces a global vision of TAI, from ethics and responsibility to technologies to design these AI systems. Section \ref{sec:AUD} discusses auditability, why auditability methodologies are needed, the assessment list and standards for auditability, and the concept of certification, while paying closer attention to explainability. Section \ref{sec:ACC} delves into accountability and the safe deployment of the AI lifecycle. Section \ref{sec:AIG} analyzes advances in AI governance (current institutional approaches) and ELSEC aspects, as well as the requirements to guarantee a society's trust in AI. Section \ref{sec:LL} introduces the ten lessons learned in the design of a roadmap, a ten-point reflection on the present and future of RAI systems to achieve convergence between regulation and innovation. Finally, we end with a short conclusion and an outlook on this emerging area of enormous practical relevance. 

\section{Responsible AI Systems}
\label{sec:RAIS}


In this section, we introduce the concept of the RAI system (Subsection \ref{sec:RAISd}). This is accompanied by discussion of "why do we need the RAI systems?", fixed under the AI Act regulatory context that is focused on high-risk scenarios (Subsection \ref{sec:RAIS-HRS}).
We  discuss two fundamental and contextual aspects that we need to consider before designing AI systems, the operational design domain (ODD) (Subsection \ref{sec:ODD}) and data collection and data quality (Subsection \ref{sec:Dat}).  

ODD and data quality are tailored to the specific application domain, shaping and informing the model design. In other words, the analysis of ODD and data quality should determine which TAI functionalities must be implemented in the model design. This is essential, as TAI technologies cannot operate as black boxes disconnected from the real problem, the contextual reality, and their intended audience.

\subsection{What are Responsible AI Systems?}
\label{sec:RAISd}

As we have mentioned, responsible AI addresses the problem of developing AI from an ethical and legal point of view. Thus, responsible AI is located at the intersection of AI, law, and philosophy. To fix the concepts, when referring to responsibility over a certain task, the person in charge of the task assumes the consequences of his/her actions/decisions to undertake the task, whether they result to be eventually right or wrong. When translating the concept of responsibility into AI-based systems, recommendations based on the output of the system at hand must be accountable, legally compliant, and ethical.

The key element in this context is the concept of an RAI system, as introduced in \cite{diaz2023connecting}:

\vspace{2mm}
\begin{definition}
    A \textit{responsible AI system} is an AI-based system that ensures auditability and accountability during its design, development and use, according to specifications and the applicable regulation of the domain of practice in which the AI system is to be used.
\end{definition}
\vspace{2mm}

The design of RAI systems can help reduce AI bias, create more transparent AI systems, and increase end-user trust. Two fundamental aspects, auditability and accountability, are extensively developed in the following sections. These features are central to the design of these models and are crucial to ensuring their reliability and trustworthiness.
\begin{itemize}[leftmargin=*]
    \item \emph{Auditability} is becoming increasingly important as standards are implemented. In terms of particular audit tools, especially when the RAI system interacts with the user, grading schemes adapted to the use case are needed to validate an intelligent system.
    \item \emph{Accountability} establishes the liability for decisions derived from the output of the RAI system, once its compliance with the regulations, guidelines, and specifications imposed by the application for which it is designed has been audited. Again, accountability may consist of different levels of compliance with the requirements for TAI defined previously, requiring regular monitoring for its compliance once it has been deployed. 
\end{itemize}

\subsection{Why do we need Responsible AI Systems? High-Risk Scenarios}
\label{sec:RAIS-HRS}
Legal and regulatory frameworks are essential in the context of responsible AI. These frameworks provide the necessary guidelines and standards to ensure that AI technologies are developed and deployed in a manner that is compliant with existing laws and regulations.  Effective legal and regulatory measures can help build trust in AI technologies and encourage their responsible use.

In recent progress, intense advances and debates have been seen on AI regulation by governments and institutions. Among others, the European Commission AI Act\footnote{\url{https://artificialintelligenceact.eu/} [acc. 02/01/25]} \cite{AIA24}, the White House President Biden's Executive Order on the ``Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence''\footnote{https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/ [acc. 02/01/25]}, the China AI Regulation with the “Code of Ethics for New-generation Artificial Intelligence”\footnote{\url{https://www.most.gov.cn/kjbgz/202109/t20210926_177063.html}[acc. 02/01/25]}, the UAE's International Stance on Artificial Intelligence Policy\footnote{\url{https://uaelegislation.gov.ae/en/policy/details/uae-s-international-stance-on-artificial-intelligence-policy} [acc. 02/01/25]}, or the AI Principles of the OECD\footnote{\url{https://www.oecd.org/en/topics/ai-principles.html} [acc. 02/01/25]}. In fact, the OECD AI Principles have been updated in 2024, to include 1) inclusive growth, sustainable development and well-being; 2) human rights and democratic values, fairness and privacy; 3) transparency and explainability; 4) robustness, security and safety; and 5) accountability; among others. There is an increasing demand for AI regulations that minimize risks to ensure safety and preserve human rights while fostering a flexible and innovative environment. These efforts must align with risk analysis \cite{critch2023tasra,hendrycks2023overview} to create a global framework for the secure implementation and deployment of AI systems in all applications where risks may arise.

In this work, we focus on the most developed and advanced regulation to date: the European AI Act \cite{AIA24}. We embrace this regulatory framework as a basis for RAI systems in high-risk scenarios, as well as to analyze the auditability and accountability prerequisite of these AI regulatory rules. This regulatory framework, approved and published on June 2024, is based on risk levels (Figure \ref{fig:risks}). It aims to ensure that AI systems deployed and used in the European Union are safe and uphold fundamental rights and European values. Most of the regulations in the European AI Act will be executed 24 months after their publication. The regulation mandates risk mitigation systems, comprehensive documentation, clear user information, high-quality datasets, activity tracking, human oversight, and robust measures to ensure accuracy and cybersecurity. 
\begin{figure}[H]
	\centering
    \vspace{-3mm}
	\includegraphics[width=0.7\linewidth]{./Images/RAI-risks-4.pdf}
	\caption{Risk levels defined by the AI Act regulatory framework.}
	\label{fig:risks}
\end{figure}

The European regulation classifies AI systems into four risk levels (Figure \ref{fig:risks}). Auditability is essential for systems at level 2, those identified as high-risk. RAI systems for high-risk scenarios are subject to stringent obligations and must undergo conformity assessments before being released in the European market. These are scenarios where the stakes are high, such as healthcare, public services, and critical infrastructure. The details of these risk levels are described below:
\begin{itemize}[leftmargin=*]
\item \emph{Levels 3 and 4}: Most AI systems are used for applications that pose minimal risks to end users. Such applications include recommendation systems and spam filters, which generally do not require obligations to the providers and manufacturers of the systems themselves. However, companies may choose to adopt voluntary codes of conduct and good practices for these AI systems.

\item \emph{Level 2}: High-risk AI systems fall into this category. The use of RAI systems in high-risk scenarios is subject to horizontal and domain-specific regulatory requirements and should therefore be subject to an audit process. There are eight types of high-risk scenarios:
\begin{enumerate}
\item Biometric identification and categorization of people.
\item Critical infrastructures (e.g., transport) that could endanger the lives and health of citizens.
\item Educational or vocational training systems that can determine access to educational and professional opportunities (e.g., scoring of exams).
\item Employment, worker management, and access to self-employment (e.g., CV-sorting software for recruitment).
\item Essential private and public services (e.g., access to services, credit scoring that could deny loans, or application of the safety component of AI in robotic-assisted surgery).
\item Law enforcement applications impacting people's fundamental rights (e.g., assessing the reliability of evidence).
\item Management of migration, asylum, and border control (e.g., verifying the authenticity of travel documents).
\item Administration of justice and democratic processes (e.g., applying law to specific facts).
\end{enumerate}

In short, high-risk scenarios are those in which an AI system can have a significant impact on the life chances of a user (Art. 6); they create an adverse impact on people’s safety or their fundamental rights.  

\item \emph{Level 1}: AI systems deemed to present an unacceptable risk will be banned. These include systems that pose a clear threat to safety, livelihoods, and rights, such as government social scoring and toys that use voice assistance to encourage dangerous behaviors.

\end{itemize}

Applying general AI systems to high-risk scenarios can bring about significant benefits but also poses substantial risks. In what follows, we briefly mention a few potential consequences, among many others, thus underscoring the critical importance of developing RAI systems. In biometric identification, there is the potential for misuse, leading to privacy violations and wrongful categorizations. In critical infrastructures, AI mishaps could jeopardize public safety. Within education, biased algorithms could unfairly impact students' futures. In employment, AI could perpetuate biases in recruitment, affecting job opportunities. Essential services supported by biased AI could become inaccessible to those who need them most. In law enforcement, AI errors could undermine justice and infringe on rights. For migration and border control, there is the danger of improper document verification. In democratic processes, AI could potentially manipulate or misinterpret laws, distorting justice. The balance between innovation and ethical responsibility is crucial to prevent harmful consequences while leveraging AI's potential.

\subsection{How to achieve Responsible AI Systems? Operational Design Domain} \label{sec:ODD}

One of the two main steps to achieve RAI systems is the definition of the Operational Design Domain (ODD). ODD refers to the context and specific conditions under which an AI system is designed to operate safely and effectively. Widely used in autonomous driving \cite{czarnecki2018operational}, ODD includes factors such as data sources, users, functional and non-functional requirements, and the specific task(s) that the AI system is expected to tackle. Delineating the ODD of an AI system is crucial to ensure that the system is used within their intended limits and can perform reliably and safely.

Therefore, understanding the context in which an AI system operates provides critical information on the specific scenarios and conditions that the AI system will encounter, allowing for a more precise and tailored design and a safer and better aligned operation. Taking into account the ODD factors, developers can create RAI systems that are better equipped to handle real-world situations. This contextual awareness helps identify potential risks and limitations, ensuring that the AI system can perform optimally and safely within its defined ODD. The range of factors to consider for ODD is wide, depending on the specific problem. Among others, we can consider: 
\begin{itemize}[leftmargin=*]
\item Environment: the physical and digital environments in which the AI system will be deployed and operate.
\item Geographical location: the specific regions or areas where the AI system will be deployed, considering local regulations and cultural contexts.
\item Data sources, including their providers, access, and characteristics.
\item User interaction: the types of interaction that the AI system will maintain with users, including voice, text, and physical interfaces.
\item Task(s) that the AI system is designed to perform, including their complexity and the functional capabilities of the AI system when addressing such task(s).
\item  Data requirements and management (e.g., if data will be stored or whether the AI model adapts to the arrival of new data over time).
\item Technological constraints: any limitations or constraints on the AI system's operation, such as inference latency, memory footprint, and energy consumption.
\item Regulatory compliance: adherence to local, national, and international regulations and standards relevant to the operation of the AI system.
\end{itemize}

Other aspects, such as safety and ethical considerations, are associated with the TAI requirements, which will be introduced in detail in Section \ref{sec:TAI}. Due to the specificity of the ODD problem, we can find studies on the subject for specific applications, including the aforementioned autonomous driving \cite{sun2021acclimatizing,betz2024new}. 

In relation to \emph{task complexity}, an RAI system should be able to recognize when the system is asked to extrapolate, that is, when the system is presented with a set of input values that are significantly underrepresented or not found in the training set (out-of-distribution inputs). Under those circumstances, an RAI system should warn the user that it is making a prediction for which it has little statistical support, or decline to produce such prediction. Thus, progress on the field of out-of-distribution learning is fundamental in the context of RAI systems, see, e.g., \cite{barcina2024managing,hendrycks2021many}. Continuing the same line of thought, an RAI system should incorporate mechanisms to detect when a structural breakdown has occurred, making its training data untrustworthy. This is particularly important in the context of highly dynamic and non-stationary data-generating processes, such as finance and economy \cite{lopez2023causal}.

\subsection{How to achieve Responsible AI systems? Data Quality}
\label{sec:Dat}

In the context of AI, data quality stands for the accuracy, completeness, consistency, and reliability of the data used to train and operate AI systems. Access to high-quality data is instrumental for the development of effective TAI based systems, as it directly impacts their performance and decision-making capabilities. Ensuring data quality helps reduce biases, improve the accuracy of predictions, and improve the overall reliability of AI systems. By maintaining rigorous standards for data quality, we can build AI systems that are fair, transparent and capable of providing valuable information, while minimizing the risks associated with erroneous or biased data \cite{balahur2022data}.
 
Paying attention to the data context is equally important \cite{serra2024use}. The context in which data are collected, processed, and used can significantly influence the results of AI models. Understanding the source, relevance, and applicability of data ensures that AI systems are trained on information that is re\-presentative of real-world scenarios. This contextual awareness helps identify potential biases and gaps in the data, allowing more informed and accurate AI predictions. Considering the context of data, we can create RAI systems that are more robust, adaptable, and aligned with the specific needs and challenges of their intended applications.

Data quality preprocessing techniques are essential to improve data and provide the perfect context for AI system design \cite{garcia2015data,luengo2020big}. Preprocessing steps such as data cleaning, normalization, and enhancement help to refining the raw data, making it more suitable for AI training. These processes ensure that the data set is free from errors, minimize inconsistencies, and removes irrelevant information, thus enhancing its quality. This proactive approach to data quality management is crucial for building AI systems that are responsible, efficient, and capable of addressing complex real-world problems.

When an AI system is trained to make decisions at time $t$ on data that were not available at time $t$, its estimated forecast power will be inflated. This creates the danger of developing and using AI systems that will systematically underperform and potentially cause harm, hence having severe safety implications. Therefore, an RAI system that makes decisions at a time $t$ must be trained only on data that was available at that time $t$. This concept is known as \emph{point-in-time} (PiT) data \cite{de2018advances}, and plays a pivotal role in applications where the underlying probability distributions to be modeled change significantly over time, such as financial time series or data collected from industrial machinery and used for predictive maintenance.  

The interaction between data quality and ODD must be explored in depth for high-risk scenarios. For example, the work introduced in \cite{cappi2024design} addresses the critical aspects of ODD and data quality in the context of a vision-based landing task, presenting a robust methodology for designing and validating datasets. This replicable framework addresses the challenges of designing a data set compliant with the requirements of the AI system certification in safety-critical applications. 

\section{Trustworthy AI: From Technology to Ethics and Responsibility}
\label{sec:TAI}

In this section, we revisit the fundamentals of TAI, which are widely discussed in the literature (see \cite{kaur2022trustworthy}, \cite{li2023trustworthy} and \cite{diaz2023connecting}, among others). We introduce the TAI paradigm (Subsection \ref{ssec:defTAI}) and its technical requirements (Subsection \ref{ssec:reqTAI}). 

\subsection{The Trustworthy AI Paradigm} \label{ssec:defTAI}

We begin with a concrete and precise definition of TAI:
\vspace{2mm}
\begin{definition} \label{def:tai}
    \textit{Trustworthy AI} is a paradigm that encompasses all technical approaches and tools to develop, deploy and use safe, legal, and ethical AI systems.
\end{definition}
\vspace{2mm}

Going deeper into the 3 fundamental pillars of TAI \cite{ai2019high}, AI-based systems must adhere to ethical, legal, and robust technical standards, ensuring that:
 \begin{enumerate}[leftmargin=*]
\item  AI-based systems are legal and comply with all applicable laws and regulations (\emph{lawfulness}).
\item  AI-based systems are ethical, ensuring the adherence to ethical principles and values (\emph{ethics}).
\item  AI-based systems are robust, both from a technical and social perspective, since, even with good intentions, AI systems can cause unintentional harm (\emph{robustness}).
 \end{enumerate}

Each of these three components is necessary, but not sufficient, to achieve TAI.

 \begin{figure}[H]
	\centering
    \includegraphics[width=0.5\linewidth]{./Images/RAI-tree-trustAI-2.pdf}
	\caption{Trustworthy AI requirements (inspired by \cite{diaz2023connecting}).}
	\label{fig:fig2}
\end{figure}

\subsection{Trustworthy AI Requirements} \label{ssec:reqTAI}

As mentioned in the Introduction, these three pillars of TAI can be broken down into seven technical requirements according to the high-level expert group on the AI proposal \cite{ai2019high} (see Figure \ref{fig:fig2}):

\begin{enumerate}[leftmargin=*]
   \item \emph{Human agency and oversight}: AI systems should support human agency and autonomy and human oversight. 
   \item \emph{Technical robustness and safety}:  General safety, accuracy, reliability, resilience to attack and security, fallback plans, and reproducibility. 
   \item \emph{Privacy and data governance}: AI systems should protect user privacy and handle data responsibly and should adhere to data governance principles.
   \item \emph{Transparency}: Traceability, explainability, and communication.
   \item \emph{Diversity, non-discrimination, and fairness}: Avoidance of unfair bias, accessibility and universal design, and stakeholder participation.
   \item \emph{Societal and environmental wellbeing}: AI systems should be designed considering their wider impact of AI on society, democracy, and the environment by actively engaging stakeholders and assessing their implications for work, skills, and sustainability.
   \item \emph{Accountability}: AI systems should be auditable and subject to effective risk management processes, with mechanisms in place to assign responsibility for failures.
\end{enumerate}



The integration of TAI requirements and technologies together with contextual analysis (including ODD and quality data) is fundamental for the development of RAI systems. TAI involves creating systems that are reliable, safe and secure, operating transparently and accountably. This requires a comprehensive approach that includes rigorous testing, continuous monitoring, and adherence to best practices and standards. By prioritizing trustworthiness in AI development, we can ensure that these technologies are used in ways that benefit society while minimizing risks and potential harms for high-risk scenarios.

Several institutions around the world have promoted different frameworks for risk management in AI systems, in which TAI tools and requirements play a central role. Among them, in January 2023 the American National Institute of Standards and Technology (NIST) released the AI Risk Management Framework (AIRMF) \cite{ai2023artificial} that includes a similar set of TAI requirements, emphasizing concepts such as "secure and resilient", "explainable and interpretable" and "valid and reliable" AI systems. This framework also highlights that approaches that enhance AI trustworthiness can reduce negative AI risks.



\section{Auditability}
\label{sec:AUD}

A complete discussion on RAI systems for a high-risk scenario leads us to establish auditability before the deployment. It is necessary to discuss why auditability methodologies are needed (Subsection \ref{sec:AUD1}), together with evaluation lists and standards norms (Subsection \ref{sec:AUD-Ass}). How to measure key attributes such as robustness, explainability, transparency and traceability, sustainability, and fairness are among others essential. Of course, the fundamental question is how to audit RAI Systems, which we tackled in Subsection \ref{sec:AUD-How}. Certification is the final step in the next required auditability, and the target of this stage in the development of the RAI systems step (subsection \ref{sec:AUD-Cer}). Explainability is a recurring issue, from ethical principles to technical requirements. It must be analyzed in the context of audibility with special attention (subsection \ref{sec:AUD-XAI}).

\subsection{Why are Methodologies for Auditability needed?}\label{sec:AUD1}

It should be noted that the process to establish before the design of auditability methodologies requires a dual analysis, which is complementary: 1) the technical TAI requirements mentioned in the previous section; and 2) the legal requirements for the compliance of AI systems with regulation in high-risk scenarios (as a particular case in Europe, those defined in the European AI Act). We refer to \cite{giudici2024artificial} and \cite{fernandez2023trustworthy} for two analyses of TAI requirements made in two different application domains (financial services and autonomous driving, respectively).

Auditability is an area that requires significant attention, as it poses major challenges in establishing compliance requirements and metrics tailored to each high-risk scenario. As a previous step to aid accountability, a thorough auditing methodology aims to validate the conformity of the AI-based asset under target to:
\begin{enumerate}[leftmargin=*]
 \item Vertical or sectorial regulatory constraints;
 \item Horizontal or AI-wide regulations (e.g., EU AI Act); and
 \item Specifications and constraints imposed by the application for which it is designed, as ODD features and data quality. 
\end{enumerate}

Auditability refers to the functionality of an AI-based system, which is required, yet not sufficient, to ensure its responsible use. As such, auditability may require transparency (e.g. explainability methods, traceability), or measures to guarantee technical robustness, to mention a few. The auditability of an RAI system may not necessarily cover all requirements for TAI, but rather those foretold by ethics, regulation, specifications,  protocol testing adapted to the application sector (i.e. vertical regulation) and necessary technologies for the problem goal to solve. 

Auditability is a crucial necessity for AI systems, as it can ensure the traceability of decisions and accountability in their operations. By having auditable RAI systems, we can trace and validate decision-making processes, which are essential to identify and correct biases, errors, or unethical practices. This transparency builds trust among users and stakeholders, as they can be confident that the AI systems are working as intended and complying with ethical standards. Moreover, auditability helps maintain compliance with regulatory requirements, which is increasingly important as AI technologies become ubiquitous in almost all sectors. 

\subsection{Assessments Lists and Standards for Auditability}
\label{sec:AUD-Ass}

To realize auditable RAI systems, it is essential to rely on standardization and assessment lists. These frameworks provide clear guidelines and benchmarks for the development and evaluation of AI systems, ensuring that they meet specific ethical, legal, and technical standards. By following these standardized assessments, organizations can work towards obtaining AI system certifications, which serve as a seal of quality and reliability. 

Therefore, standards and assessments, such as those provided by ISO (International Organization for Standardization), are fundamental for the evaluation of RAI systems. These standards offer a structured framework to ensure that AI technologies adhere to ethical principles, safety protocols, and regulatory requirements. By following established norms, organizations can systematically assess the performance, reliability, and fairness of their AI systems. This not only helps identify and mitigate potential risks, but also fosters transparency and accountability. Implementing these standards ensures that AI systems are developed and deployed in a manner that is trustworthy and aligned with societal values, ultimately improving public confidence in AI technologies.

In 2020, the \textit{"Assessment List for Trustworthy Artificial Intelligence" (ALTAI)} was published \cite{ala2020assessment}. It provides a comprehensive and accessible self-evaluation checklist that guides AI developers and deployments to implement the principles of TAI in practice. ALTAI translates AI principles into actionable steps, ensuring that AI systems adhere to ethical guidelines, safety protocols, and regulatory requirements.

The use of standardized assessments and standards facilitates interoperability and consistency across different AI systems and applications. This is indispensable for creating a cohesive and reliable AI ecosystem where systems can work together seamlessly and be evaluated on a common basis. By adhering to recognized standards, organizations can demonstrate their commitment to responsible AI practices, which can lead to greater trust and acceptance from stakeholders, including users, regulators, and the broader community. In essence, the integration of assessments and standard norms is a key component in the journey towards building ethical, transparent, and accountable AI systems that benefit society as a whole.

In  two years, the AI Act will be in force, two years after its approval. This should allow us to progress towards the design of assessment lists for each high-risk scenario. Each scenario and problem are different in nature, so RAI systems must be adapted to meet the requirements, assessment lists, and standard norms for each scenario. This helps mitigate risks, promote transparency, and foster trust between users and stakeholders by ensuring that AI systems are developed and deployed responsibly. 

\subsection{How to audit a Responsible AI System?}
\label{sec:AUD-How}

Considering the AI Act, RAI systems are required to comply with the following seven requirements (AI Act, Chapter 2 \cite{AIA24}):
\begin{enumerate}[leftmargin=*]
\item Adequate risk assessment and mitigation systems (Art. 9, \emph{Risk management system}).
\item High quality of the datasets that feed the system to minimize risks and discriminatory outcomes (Art. 10, \emph{Data and data governance}; Art. 9, \emph{Risk management system}).
\item Logging of activity to ensure traceability of results (Art. 12, \emph{Record keeping}; 20, \emph{Automatically generated logs}).
\item Detailed documentation providing all the information necessary about the system and its purpose for authorities to assess its compliance (Art. 11, \emph{Technical documentation}; Art. 12, \emph{Record keeping}).
\item Clear and adequate information to the user (Art. 13, \emph{Transparency}).
\item Appropriate human oversight measures to minimize risk (Art. 14, \emph{Human oversight}).
\item High level of robustness, security, and accuracy (Art. 15, \emph{Accuracy, robustness, and cybersecurity}).
\end{enumerate}

It should be noted that the above requirements vary in its nature and the aspect of an AI system they regard, ranging from technical requisites to the information that the user must provide/receive, data governance or the need for risk analysis in the interface between a human user and the AI system, among others. 

The challenge is to design auditability metrics and methodologies adapted to the relevant usage scenarios, which are defined in the ODD associated with the AI system and application at hand. Developing these metrics requires a deep understanding of the AI system's operations and potential risks, ensuring that they are comprehensive and effective. In \cite{stettinger2024trustworthiness} the authors propose methodologies to ensure trustworthiness for high-risk scenarios in compliance with the EU AI Act. It emphasizes seven key requirements for RAI systems to be considered trustworthy and human-centric, integrating concepts like ODD, and introducing the Behavior Competency (BC) for risk assessment. The BCs of the automated driving domain are utilized in risk assessment strategies to quantify different types of residual risk. The methodology focuses on a trustworthiness assurance framework, addressing ethical considerations, and includes a roadmap for future AIS to achieve society's trust and compliance.

\subsection{Certification: The Goal}
\label{sec:AUD-Cer}

The certification of AI systems is the culmination of the auditability process, serving as a necessary step to ensure the safe and effective deployment of AI technologies. This process involves a comprehensive evaluation of the design, development, and deployment phases of the AI system to verify that it meets established standards and regulatory requirements. Certification provides an official statement that the AI system adheres to ethical principles, safety protocols, and performance benchmarks, thereby instilling confidence in its reliability and trustworthiness.

The importance of this certification process lies in its ability to identify and mitigate potential risks associated with AI systems. By conducting thorough audits, organizations can uncover biases, inconsistencies, and vulnerabilities that can compromise the integrity and effectiveness of the system. This proactive approach helps in addressing issues before they escalate, ensuring that the AI system operates within its intended limits and delivers accurate, fair, and transparent outcomes. 

Certification acts as a seal of approval, demonstrating that the AI system has been subjected to meticulous scrutiny and meets the highest standards of quality and safety.  Certification also promotes accountability, as it requires developers and stakeholders to adhere to rigorous standards and best practices throughout the AI lifecycle. 

In addition, certification is essential for gaining the trust and acceptance of end users, regulators, and the broader community. In an era where AI technologies are increasingly integrated into various aspects of society, ensuring their responsible and ethical use is paramount. This not only enhances the credibility of the AI system, but also fosters public confidence in its deployment, paving the way for broader adoption and innovation in the field of AI.

Certification is currently understudied because auditability is the initial step in the development of responsible AI systems. Currently, standards are being discussed and developed to meet high-level regulatory frameworks established by institutions around the world. For instance, the AI Act has still not come into force, whereas standardization bodies are currently intensively working towards producing the first drafts of the norms and standards that will drive auditing processes under such regulations. Consequently, certification must follow later in the process. However, studies related to the certification of different aspects of AI systems have recently originating in the literature \cite{agarwal2023fairness,namiot2024certification}. These recent efforts expose the growing interest in certification towards the responsible deployment and operation of RAI systems.

\subsection{Explanations in AI: The Quest for their Role in Responsible AI Systems}
\label{sec:AUD-XAI}

The quest for explanations in RAI systems is vital to ensure their auditability. Explanations can elicit knowledge about the reasoning process or the process by which the output of the model was produced, allowing users and auditors to understand how AI systems make recommendations. There are different degrees of explainability, ranging from identification of the most informative variables (that is, the variables most responsible for the system's performance) all the way to the network of causal dependence between those variables (i.e., the identification of the causal mechanism responsible for the observations). The closer the system is to causal explainability, the more transparent, robust, and reproducible its predictions will be \cite{holzinger2019causability}.

Explainability is a critical capability of RAI systems, as it can ensure that the decisions and actions of AI systems are understandable to humans. In the broader vision of RAI systems, explainability plays a pivotal role in bridging the gap between complex AI algorithms, human understanding, and human-AI interaction \cite{kim2023help}. By providing clear explanations, AI systems can demonstrate their adherence to ethical principles and regulatory standards, thereby fostering accountability and reducing the risk of unintended consequences. Explainability also empowers users to identify and address potential biases or errors in the AI system, promoting fairness and reliability. Explainability enables stakeholders, including developers, regulators, and end-users, to engage with AI systems effectively and make informed decisions based on the provided explanations. This collaborative approach improves the overall governance of AI technologies, ensuring that they are deployed in a manner that aligns with societal values and expectations. 

The following definition, proposed in \cite{arrieta2020explainable}, considers the two fundamental elements when we discuss explanations: \emph{understanding} and \emph{audience}. 
\vspace{2mm}
\begin{definition}
    Given an audience, an \textit{explainable AI} (XAI) is one that produces details or reasons to make its functioning clear or easy to understand.
\end{definition}
\vspace{2mm}

Nowadays we can find a vast literature with different technical approximations to produce explanations for different AI systems. This research area, widely known as \emph{explainable AI} (XAI), has grown exponentially in the last few years, leading to recent discussions on its maturity \cite{ali2023explainable}, open challenges \cite{longo2024explainable} and criticisms \cite{freiesleben2023dear,weber2024xai}. It is true that the literature related to XAI is very rich, with many proposals to date, but it also raises many questions. It is necessary to approach the XAI context with nuance and conduct an in-depth analysis to ensure that progress is made in the right direction towards supporting the auditability of RAI systems in high-risk scenarios. This is one of the conclusions shared recently in \cite{herrera2025}, calling for a deep reflection and attention to XAI considering the existing criticism and the importance of explanations for a trustworthy human-AI interaction.

Interest in explanations also differs when it comes to different audiences. In this context, the stakeholder interest map presented in \cite{haresamudram2023three} includes six levels of audience: \emph{developer}, \emph{designer}, \emph{owner}, \emph{user}, \emph{regulator}, and \emph{society}. By prioritizing explainability for the audience, we can develop RAI systems that are not only powerful and efficient, but also ethical, transparent, and trustworthy. This approach ultimately contributes to a more responsible and inclusive AI ecosystem. However, achieving this goal remains a significant scientific challenge in the field of TAI. There is still a long way to go to get the right explanations for each audience and decision to be made. 

Finally, it should be noted that XAI plays an important role in the auditability of AI systems. We should move towards establishing a list of explainable assessments for each high-risk scenario. The presence of explainable assessments of an AI model will contribute to its auditability.

\section{Accountability}
\label{sec:ACC}

Accountability for AI systems is paramount after achieving certification and auditability. Certification ensures that AI systems meet specific standards and regulations, but accountability goes beyond initial compliance. 

Accountability involves continuous monitoring and evaluation to ensure that AI systems operate as intended and adapt to new challenges and contexts. This ongoing accountability is crucial for maintaining trust and reliability in AI technologies. By holding AI developers and operators responsible for their systems' actions, we can address issues such as bias, errors, and unethical practices promptly and effectively. This not only enhances the credibility of AI systems, but also ensures their alignment with social values and expectations.

To achieve continuous post-hoc accountability, it is essential to integrate RAI systems within a robust AI governance framework and throughout the AI lifecycle. This includes implementing mechanisms for regular evaluations, updates, and improvements based on real-world performance and feedback after deployment. In short, to focus the attention on AI governance and AI safety. 

AI safety is a fundamental goal in this process, ensuring that AI systems are secure, reliable, and resilient to potential risks and threats. By prioritizing accountability and safety, we can foster a responsible AI ecosystem that benefits society while minimizing potential harms.

In the following subsections we briefly discuss on accountability, focusing on the post-deployment analysis (Subsection \ref{sec:ACC-Post}) and its connection to AI safety (Subsection \ref{sec:ACC-Safety}). Given its relevance across all stages of AI development, certification and deployment, we will elaborate on AI governance within the context of accountability in the next section.

\subsection{Accountability: Post-deployment Analysis}
\label{sec:ACC-Post}

The AI Act emphasizes the need for continuous monitoring and risk management throughout the AI lifecycle.  This requires a global framework and guidelines, from auditability to accountability along the AI lifecycle (Figure \ref{fig:fig3}).
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{./Images/RAI-AI-Lifecycle-new-colors.pdf}
	\caption{The AI lifecycle.}
	\label{fig:fig3}
\end{figure}

It is important to note the difference between auditability (\emph{ex ante}) and accountability (\emph{post hoc}) when analyzing RAI systems. In this regard, we refer to Figure \ref{fig:fig4} for a comprehensive graphical representation of both approaches: auditability \& conformity versus monitoring \& accountability, as outlined in the AI Act. This figure also highlights the relevant chapters of the AI Act. By adhering to these guidelines, RAI systems can effectively manage potential risks and adapt to new challenges, ensuring their safe and reliable operation. This proactive approach is essential for maintaining the integrity and security of AI applications, which ultimately contributes to a more resilient and trustworthy technological ecosystem. 
\begin{figure*}[h!]
	\centering
	\includegraphics[width=\linewidth]{./Images/Audit-Account-new-colors.pdf}
	\caption{Diagram showing the path from auditability to auditability (ex-ante) to accountability (post-hoc) (inspired and updated from \cite[Fig. 4]{diaz2023connecting}).}
	\label{fig:fig4}
\end{figure*}

The out-of-sample performance of a reliable system is comparable to its in-sample performance. An accountable system assesses, among other things, the extent to which the AI system performs as designed. The challenge emerges from the design of effective monitoring methodologies for accountable AI systems. Furthermore, implementing robust monitoring methodologies is crucial to continuously assess and address emerging issues, maintaining the integrity and trustworthiness of AI systems.

\subsection{AI Safety as a Goal} \label{sec:ACC-Safety}

Safety is one of the primary goals of accountability in responsible AI systems. As these systems increasingly impact our society, ensuring their safe deployment is essential to building trust, mitigating risks, and upholding ethical standards. AI safety is an interdisciplinary field that is concerned with preventing accidents, misuse, or other harmful consequences that could result from the use of AI systems. Beyond AI research, safety implies the development of standards and policies enforcing it in the practical context of use \cite{hendrycks2021unsolved, hendrycks2024introduction}. AI safety encompasses:
\begin{itemize}[leftmargin=*]
  \item \emph{Machine ethics}: It is part of AI ethics concerned with adding or ensuring moral behaviors of man-made machines that use AI. It is also called \emph{machine morality}. 
   \item \emph{AI alignment}: It refers to guiding AI systems to act in accordance with humans' intended goals, preferences, or ethical principles. An AI system is considered aligned when it reliably pursues its intended objectives as designed. In contrast, a misaligned AI system may pursue unintended or conflicting objectives, potentially leading to malfunctions, unintended consequences, or harm.
   \item \emph{Robustness}, which refers to the ability of an AI system to maintain performance against various perturbations and adversarial input throughout the AI lifecycle.  
   \item \emph{AI monitoring systems}, which include tools designed to continuously observe and evaluate the performance of AI models, ensuring that they operate reliably and adhere to predefined standards.
   \item \emph{External safety}, also known as \emph{systemic safety} or \emph{AI security}, addresses broader contextual risks in the way AI systems are managed. Cybersecurity and decision-making play a decisive role in whether AI systems fail or are misdirected.     
\end{itemize}


In accordance with the AI safety goals, RAI systems will unleash their full potential when trust can be established at each stage of their lifecycle, from design to development, deployment and use. Therefore, work and decisions must be made about the following: 
\begin{itemize}[leftmargin=*]
    \item Identify training and monitoring metrics to help keep errors, false positives, and biases to a minimum.
    \item Perform tests such as bias testing to help produce verifiable results and increase end-user trust. 
    \item Promote methodologies to determine the validity of tests for RAI systems.
    \item Continuously monitor after deployment, to ensure that the AI model continues to function in a responsible and unbiased way.
\end{itemize}

When it comes to continuous monitoring, it is straightforward to wonder what happens if a problem is detected in an AI system. In that case, a critical accountability process is triggered, comprising essentially two steps:
\begin{itemize}[leftmargin=*]
\item The first step is to analyze the incident and mitigate the risk. Incident analysis can determine that the problem requires redefining the ODD and acquiring more and/or better quality data. 
\item In the second step, the newly designed RAI system must undergo a new audit process.
 \end{itemize}
 
This iterative process of detection, redesign, and re-auditing is essential to maintain the integrity and trustworthiness of AI systems. It is essential to ensure that they continue to function ethically and effectively in real-world scenarios.  

On a closing note, we draw attention to the mechanism defined in the AI Act for the systematic safety assessment of AI based systems: the so-called \emph{regulatory sandboxes} for high-risk scenarios. Sandboxes allow for the evaluation of the conformity of the AI-based system with respect to technical specifications, horizontal and vertical regulation, and ethical principles in a controlled and reliable testing environment. The advantage of isolated environments is that they favor the development, testing, and validation of innovative AI systems under the direct supervision and guidance of the competent authorities (Art. 53 of the AI Act). Two main aspects related to sandboxes remain unresolved to date:
\begin{enumerate}[leftmargin=*]
\item The design of sandboxing guidelines that rapidly and effectively permit algorithmic auditing for the proposed high-risk scenarios; and 
\item The development of intelligent systems for high-risk scenarios validated through the necessary auditing processes. 
\end{enumerate}

\section{AI Governance: The Consensus for Society's Trust}
\label{sec:AIG}

AI regulation and AI governance are closely related but distinct concepts. Here is a breakdown of their differences:
\begin{itemize}[leftmargin=*]
\item On the one hand, \emph{AI regulation} refers to formal rules, laws, and regulations established by governments or regulatory bodies to oversee the development, deployment, and use of AI technologies. It ensures that AI systems are safe, ethical and aligned with societal values and protects users and society from potential harm. It focuses on compliance with legal requirements, standards, and policies. 

\item On the other hand, \emph{AI governance} encompasses the broader framework of policies, practices, and processes that organizations use to responsibly manage AI development and deployment. Ensure that AI systems are developed and used in a way that is ethical, transparent, and accountable within an organization. That is, RAI systems are developed with the goal of helping humanity navigate the adoption and use of AI systems ethically and responsibly. AI governance frameworks include internal and external practices, such as risk management, ethics committees, transparency measures, and stakeholder engagement.
\end{itemize}

Therefore, achieving consensus among states and institutions on AI governance is essential to ensure responsible and ethical development of AI technologies. ensure a society's trust in AI.  An unified approach helps establish consistent standards and regulations, fostering international cooperation and trust. By working together, companies and regulators can address global challenges, mitigate risks, and promote innovation in a way that benefits everyone in society. This collaborative effort is crucial to creating a sustainable and inclusive future for AI.

There are a few comprehensive proposals for AI governance frameworks. Among them, we highlight the reports for AI governance presented by United Nations (UN) in 2023 \cite{UN2023} and 2024 \cite{UN2024}, which collected the 2-year efforts of the AI advisory board of this institution. We examine in detail these reports in Subsection \ref{sec:AIG-UN}. Another notable proposal is the initial work done by USA, entitled ``Framework to advance AI governance and risk management in National Security'', which we discuss in Subsection \ref{sec:AIG-USA}. In the private sector, the study on AI governance issues by Google\footnote{Google, ``Perspectives on Issues in AI Governance'', \url{https://ai.google/static/documents/perspectives-on-issues-in-ai-governance.pdf} [acc. 02/01/2025]} points out five key areas for clarification, explainability standards, fairness appraisal, safety considerations, human-AI collaboration, and liability frameworks. Finally, a global view of AI governance for a holistic vision of the aspects of ELSEC is briefly introduced in Subsection \ref{sec:AIG-ELSEC}.  

\subsection{The Integrating Role of UN AI Advisory Body}
\label{sec:AIG-UN}

From a holistic point of view, AI governance 360º regulates and manages the AI lifecycle, closing the gap that exists between accountability and ethics in technological AI advancement, laying out the path towards the comprehensive development of RAI systems.

In this regard, in 2023 and 2024 the UN AI Advisory Body developed an AI governance framework and roadmap, producing two releases of a report collecting their conclusions. First, an interim report entitled \textit{"Governing AI for Humanity. Interim Report"} (December 2023) \cite{UN2023} outlines key guiding principles and institutional functions. Such principles are:
\begin{enumerate}[leftmargin=*]
\item AI should be governed inclusively, by and for the benefit of all. 
\item AI must be governed in the public interest. 
\item AI governance should align with data governance and the promotion of data commons. 
\item AI governance must be universal, networked, and rooted in adaptive, multi-stakeholder collaboration. 
\item AI governance should be anchored in the UN Charter, the International Human Rights Law and other international commitments, including the Sustainable Development Goals.
\end{enumerate}

 \begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{./Images/RAI-Governance-functions.pdf}
	\caption{Functions of AI governance (inspired by \cite{UN2024}).}
	\label{fig:fig5}
\end{figure}

The key institutional functions are represented in Figure \ref{fig:fig5}.  These include regularly assessing the state of AI and its trajectory, improving interoperability, harmonizing standards, safety, and risk management frameworks, facilitating deployment, fostering international multi-stakeholder collaboration to empower the Global South, monitoring risks, coordinating emergency response, and developing binding accountability norms. Each function requires international cooperation among all stakeholders involved.  
 

Figure \ref{fig:fig6} presents a simplified schema of the AI governance arrangements (both existing and emerging) proposed by the AI Advisory Body. This simplified schema is intended to promote interoperability between different efforts surrounding AI governance, paying attention to four important aspects: data, models, benchmarks, and the sectoral requirements (e.g. due diligence processes and sector-specific permissions).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{./Images/RAI-governance.pdf}
	\caption{Schema to promote interoperability between different AI governance efforts \cite{UN2023}.}
	\label{fig:fig6}
\end{figure}


During September 2024, the Final Report \textit{“Governing AI for Humanity”} \cite{UN2024} was published. It presents an executive summary reaffirming the imperative of global governance. It highlights the importance of fostering dialogue between countries, acknowledging that while differences exist across nations and sectors, there is a strong commitment to proactive and open communication. Engaging diverse experts, policymakers, business people, researchers and advocates -- across regions, genders, and disciplines -- has shown that diversity should not lead to discordance, and dialogue can set common grounds and foster collaboration effectively. 

UN's Final Report is an valuable document of reflections and conclusions, with 219 points that deserve to be read. Among them we pause at point 218 on page 78, the penultimate reflection by the AI Advisory Body, in the section ``Conclusion: A call to action''. It reflects the spirit and initiative of this second and final report:
\begin{itemize}[leftmargin=*]
    \item[] \textit{``The implementation of the recommendations in the present report may also encourage new ways of thinking: a collaborative and learning mindset, multistakeholder engagement and broad-based public engagement. The United Nations can be the vehicle for a new social contract for AI that ensures global buy-in for a governance regime that protects and empowers us all. Such a contract will ensure that opportunities are fairly accessed and distributed, and the risks are not loaded onto the most vulnerable -- or passed on to future generations, as we have seen tragically with climate change.''}
\end{itemize}

In parallel to the efforts of the UN AI Advisory Body, on August 9th, 2024 the UN Chief Executive Board for Coordination published the \textit{``White paper on AI governance''}\footnote{UN System White Paper on AI Governance, \url{https://unsceb.org/united-nations-system-white-paper-ai-governance} [acc. 02/01/25]}. The White Paper analyses the UN system's institutional models, functions, and existing international normative frameworks applicable to global AI governance. It is organized into three focus areas, as existing normative and policy instruments, institutional functions and lessons learned from existing governance structures, inclusive normative processes and agile and anticipatory approaches within the UN system, and presents a set of recommendations (pages 45 to 47), six general recommendations, and nine specific ones to further enhance its AI governance efforts. 

The White Paper is a complementary document of the analyzed reports on \textit{``Governing AI for Humanity''}. Together, the three documents present an in-depth analysis of what AI governance should be to enforce a responsible development and use of AI-based systems. 

\subsection{USA Framework to advance AI Governance and Risk Management in National Security}
\label{sec:AIG-USA}

The U.S. Department of Treasury has published a short document entitled \textit{``Framework to advance AI governance and risk management in National Security''}\footnote{\url{https://ai.gov/wp-content/uploads/2024/10/NSM-Framework-to-Advance-AI-Governance-and-Risk-Management-in-National-Security.pdf} [acc. 02/01/25]}. It presents an initial governance framework including four primary pillars:
\begin{enumerate}[leftmargin=*]
    \item The identification of prohibited and high-impact AI use cases based on the risk they pose to national security, international norms, democratic values, human rights, civil rights, civil liberties, privacy, or safety, as well as AI use cases that impact Federal personnel.
      \item The creation of sufficiently robust minimum-risk management practices for those categories of AI that are identified as high impact, including pre-deployment risk assessments.
        \item A catalog and methodologies to monitor the use of high-impact AI systems.
          \item Effective training and accountability mechanisms.
\end{enumerate}

These pillars are developed throughout the 14-page document. Due to the limited space, we do not elaborate further on them. It is a good starting point that we recommend reading and which will need to be expanded to fully address these pillars in practical governance frameworks.

However, very recently (20 January 2025), U.S. President Donald Trump reversed the executive order passed by former President Joe Biden on October 30, 2023 that aimed to monitor and regulate AI risks (Executive Order 14110)\footnote{Executive Order 14110 Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, \url{https://www.govinfo.gov/content/pkg/FR-2023-11-01/pdf/2023-24283.pdf} [acc. 02/01/25]}. The final definition of the US position in terms of regulation is an unknown quantity.

\begin{figure*}[b!]
	\centering
	\includegraphics[width=\linewidth]{./Images/new_framework_RAI.pdf}
	\caption{Roadmap framework of a Responsible AI systems.}
	\label{fig:framework}
\end{figure*}

\subsection{Ethical, legal, social, economic, and cultural aspects of AI}
\label{sec:AIG-ELSEC}
 
Last but not least in the holistic view, any analysis must be accompanied by another critical aspect dedicated to ethics and all its social implications. It is necessary to consider social acceptance, economic and legal implications, or cultural aspects. They are called the ELSEC aspects of AI systems. 

The importance of ELSEC for RAI systems cannot be overstated. These aspects ensure that AI technologies are developed and deployed in a manner that aligns with societal values and expectations. By considering ethical, legal, socioeconomic, and cultural factors, we can create AI systems that are not only technically sound, but also socially responsible. This holistic approach helps build trust and acceptance among users, regulators, and the broader community. In contrast, RAI systems that prioritize ELSEC aspects contribute to the overall well-being of society by promoting fairness, transparency, and accountability.

We find studies on ethical issues \cite{coeckelbergh2020ai} (and, among them, the current discussion on democracy and AI \cite{innerarity2024artificial,coeckelbergh2024ai}),
legal and social implications \cite{osasona2024reviewing, abdikhakimov2023legal},  cultural and demographic circumstances\cite{kelly2023factors} and  economical study. We highlight the recent Nobel Prize in Economics for researchers working on the economical impact analysis of AI \cite{acemoglu2024simple,alam2024automation}, paying attention to the productivity and the human augmentation capabilities in contrast to automation (under the eye-catching title of "From Automation to Augmentation: Redefining Engineering Design and Manufacturing in the Age of NextGen-AI"). The study \textit{"From principles to practice: A deep dive into AI ethics and regulations"}, advocates a harmonized approach that protects societal values while encouraging technological advancement \cite{sun2024principles}).  Ethan Mollick presents a scenario of collaboration and innovation between humans and AI systems under the name of \textit{Co-Intelligence}. 

However, there are no comprehensive studies that analyze ELSEC in all its dimensions. Integration of the aspects of ELSEC into RAI systems is needed. Action must be taken for interdisciplinary collaboration. Experts in various fields such as ethics, law, sociology, economy, demographic culture, and technology are essential. This ensures a comprehensive understanding of the potential impacts of AI. 

\section{From Responsible AI System Roadmap Framework to ten Reflections based on Lessons Learned}
\label{sec:LL}

Lastly, for the RAI systems exploration concept with deep analysis of the four key dimensions, we must consider the second goal of the paper, to propose a roadmap in the design of RAI systems (Subsection \ref{sec:LL-Roadmap}). As we also mentioned, Last but not least, we also reflect on the current state and those aspects that need to be developed in the near future, as ten lessons learned, highlighting critical needs and opportunities in the field (Subsection \ref{sec:LL-Ten}). 


\subsection{Roadmap Framework Responsible AI System }
\label{sec:LL-Roadmap}

The analysis of the concept of an RAI system has been done from a holistic perspective, encompassing the four discussed key dimensions. The regulation analyzed based on the AI Act has led us to the specific requirements for auditability and accountability. AI Governance has been focused on studies and proposals from the United Nations, including a mention of ELSEC aspects of AI systems.

This allows us to propose a roadmap framework that includes all the elements of these four dimensions, for the design of RAI systems that can be adapted to any risk scenario and associated problem.  Figure \ref{fig:framework} graphically captures the roadmap framework proposal for the design of RAI systems, considering the dimension and their vectors for analysis. It integrates all dimensions related to responsibility in AI systems, seamlessly aligning with the discussions provided throughout the paper. We must also remark that it is completely aligned with Figure \ref{fig:fig4} (path from auditability to auditability), using boxes of similar color for the dimensions and analysis vectors. Of course, this roadmap framework can be adapted to other specific regulatory and governance requirements with a similar flow of steps.

The roadmap aims to provide a comprehensive flow path of the key components and stages involved in the development of RAI systems, highlighting the interconnections between various dimensions such as ethical considerations, technical requirements, and regulatory compliance. AI governance is positioned as a dimension that has influence and feedback on the others. The roadmap framework includes feedback mechanisms for incident analysis, AI system redesign, and ODD/Data Quality readjustment whenever a problem is detected in the post hoc monitoring and accountability stage. Other elements discussed throughout preceding sections are also shown in the framework (e.g. the use of self-assessment lists in the model design phase, or certification and safety as primary goals of auditability and accountability).


Of course, the relationships among the dimensions and elements of the roadmap need to be developed in detail for each application in each high-risk scenario. This is because of the necessary nuances with respect to specific regulation, the other technical aspects required, from data quality and ODD to the model design (including explanations and AI safety, among others), and benchmarking. Therefore, by visualizing these elements, the roadmap serves as a valuable tool to guide the systematic design and implementation of RAI systems for each potential application. Ensure that all critical aspects are addressed throughout the development process. 

\subsection{Ten Reflections based on Lessons Learned}
\label{sec:LL-Ten}

The following ten key reflections and issues are essential takeaways from this roadmap framework, both for the present and the future. These insights encapsulate the lessons learned throughout the development process, address critical challenges, and propose actionable solutions. By focusing on these issues, we aim to enhance the effectiveness and reliability of RAI systems, ensuring that they meet ethical, legal, and technical standards.
\begin{enumerate}[leftmargin=*]

\item We would like to emphasize that TAI is a critical paradigm to meet upcoming regulations, address ethical issues, and manage risk analysis in human-AI interaction, ensuring AI governance and technical soundness of RAI systems.

\item We draw attention to the need for auditability metrics for RAI systems analysis and compliance. Adherence to future established standardizations and guidelines (such as those provided by ISO and/or IEEE norms) ensures that AI systems are developed and deployed responsibly, with a focus on ethical, legal, social, economic and cultural considerations.

\item Communication with stakeholders and explanations, including users and regulators, helps build trust and ensure that AI systems meet societal expectations. Explainability plays an essential role in this scenario.

\item Certified AI systems are more likely to be trusted and accepted by users, regulators, and the broader community, facilitating their adoption and integration into critical applications. This approach not only enhances the credibility of AI technologies, but also promotes their responsible and ethical use.

\item Aligned with the AI Seoul Summit \cite{Seoul2024}, held on May 21, 2024, the summit emphasized the importance of AI governance discussions to promote safety, innovation, and inclusivity, 
with the aim of shaping a global strategy for responsible AI development.  


\item RAI systems must play an important role for inclusivity, crucial for AI development, as they ensure that diverse perspectives are considered, leading to more equitable, fair, and effective AI systems that benefit all of society.

\item The future of RAI systems lies in the convergence of regulation and innovation, where robust regulatory frameworks guide the development of AI technologies while fostering an environment that encourages creativity. It is fundamental to emphasize the importance of promoting innovation in the development of AI.

By balancing these two aspects, we can  boost innovation, and we can create AI systems that are not only powerful and efficient, but also trustworthy and accountable. This approach will help build society trust in AI, ensuring that its benefits are realized while minimizing potential risks and harms.

\item Human-AI interaction is set to shape the future. While human decision-making remains crucial in high-risk scenarios, the effective integration, trust, and collaboration with AI are essential for enhancing productivity and human augmentation.

\item Safety and security play an essential role in all AI systems, especially those that interact in an open world. Continuous monitoring and evaluation of AI systems are necessary to identify and address emerging issues. As an example, let us turn our attention to a context of genuine real application with continuous safety and security challenges,  automated vehicles. This is opening new challenges and perspectives, such as AI safety assurance for automated vehicles \cite{ullrich2024ai} and operating systems based on large language models for automated vehicles \cite{ge2024llm}.


\item The rapid advancement of modern AI systems, driven by significant investments and continuous research, underscores the need for agile methodologies throughout the RAI roadmap framework (Figure 7). Emerging AI discoveries can introduce new threats, misuses, and challenges in real-world applications. Therefore, agile regulatory frameworks, standards, and norms are essential to enable the development of auditability, accountability, and AI governance methodologies that keep pace with AI’s evolving landscape, and obviously foremost under a strong consensus of the institutions. As highlighted in \cite{bengio2024managing}, it is necessary to have a comprehensive plan combining technical research and adaptive governance to effectively address challenges, to achieve consensus to manage extreme AI risks. 

\end{enumerate}

These ten lessons underscore the complexity and importance of developing RAI systems that prioritize safety, ethics, and accountability. These lessons serve as both a reflection of the current state of the field and a guide for future efforts, highlighting the need for interdisciplinary collaboration, agile regulatory frameworks, and alignment of technological advancements with societal values.

\section{Conclusions}
\label{sec:Con}

This paper aims to position all the essential elements for the design and implementation of RAI systems within the broader ethical, legal, and regulatory debates surrounding AI. These elements are critical in the current landscape, where the rapid and almost exponential growth of AI applications has sparked intense discussions about regulation, potential risks, and governance. In this context, RAI systems play a central role in AI governance.

The advances in generative AI, particularly large language models, illustrate the remarkable progress made in this area during the last decade. However, these advancements also highlight key challenges, such as achieving trustworthiness \cite{lin2024towards,huang2024trustllm}, alignment, and safety \cite{anwar2024foundational}, issues that remain fundamental to the responsible deployment of AI technologies.

We hope that this paper has shed light on the multifaceted and interconnected nature of RAI systems. Although the broad set of concepts and requirements may initially seem complex, understanding them as an integrated whole enables us to navigate this fascinating field. By providing a comprehensive overview, we aim to contribute to a deeper understanding of RAI systems and emphasize the significant work that is still required to ensure their successful development.

RAI systems are indispensable for the near future, as they ensure that AI technologies are developed and deployed in ways that prioritize safety, ethics, and accountability. In high-stakes domains such as healthcare, finance, and autonomous vehicles, RAI systems can mitigate potential harm by adhering to strict ethical guidelines and regulatory standards. This approach not only fosters societal trust but also shows that innovation and responsibility can go hand in hand. As AI becomes increasingly integrated into various aspects of our lives, the role of RAI systems in protecting human well-being and societal values cannot be overstated.

The journey toward responsible AI is just beginning. This marks the beginning of an exciting path to developing AI technologies that align with the roadmap and framework proposed herein. The convergence of AI innovation and regulatory frameworks has the potential to advance science, drive economic prosperity, and serve the good of humanity, while adhering to legal requirements and ethical principles.

\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}


