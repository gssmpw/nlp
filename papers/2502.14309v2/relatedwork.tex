\section{Related Work}
\label{sec:related}
\textbf{Label DP.} Under the local model, labels are randomized before training. For the classification problem, the simplest method is randomized response \citep{warner1965randomized}. An important improvement is proposed in \citep{ghazi2021deep}, called RRWithPrior, which incorporates prior distribution. \citep{malek2021antipodes} proposes ALIBI, which further improves randomized response by generating soft labels through Bayesian inference. \citep{zhao2024enhancing} proposed a vector approximation approach to significantly improve the accuracy of classification under the label LDP with a large number of classes. There are also several methods for regression under label LDP \citep{ghazi2022regression,badanidiyuru2023optimal}. Under the central model, \citep{esfandiari2022label} proposes a clustering approach. \citep{malek2021antipodes} applies private aggregation of teacher ensembles (PATE) \citep{papernot2017semi} into label CDP setting. This approach is further improved in \citep{tang2022machine}.

\textbf{Minimax analysis for public data.} Minimax theory provides a rigorous framework for the best possible performance of an algorithm given some assumptions. Classical methods include Le Cam \citep{lecam1973convergence}, Fano \citep{verdu1994generalizing} and Assouad \citep{assouad1983deux}. Using these methods, minimax lower bounds have been widely established for both classification and regression problems \citep{yang1999minimax,yang1999minimax2,audibert2007fast,tsybakov2009introduction,chaudhuri2014rates,yang2015minimax,doring2018rate,gadat2016classification,zhao2019minimax,zhao2021minimax}. If the feature vector has bounded support, then the minimax rate of classification and regression are $O(N^{-\frac{\beta(\gamma+1)}{2\beta+d}})$ and $O(N^{-\frac{2\beta}{2\beta+d}})$, respectively.

\textbf{Minimax analysis for private data.} Under the local model, \citep{kasiviswanathan2011can} finds the relation between local DP and statistical query. \citep{duchi2013local} and \citep{duchi2018minimax} develop the variants of Le Cam, Fano, and Assouad's method under local DP. Lower bounds are then established for various statistical problems, such as mean estimation \citep{li2023robustness,feldman2020private,duchi2019lower,huang2021instance}, hypothesis testing \cite{gopi2020locally}, classification \citep{berrett2019classification}, and regression \citep{berrett2021strongly}. Under the central model, for pure DP, the standard approach is the packing method \citep{hardt2010geometry}, which is then used in hypothesis testing \citep{bun2019private}, mean estimation \citep{narayanan2023better,kamath2020private}, and learning of distributions \citep{kamath2019privately,alabi2023privately,arbas2023polynomial}. There are also several works on approximate DP, such as \citep{bun2014fingerprinting,kamath2022new}. 

To the best of our knowledge, our work is the first attempt to conduct a systematic study the minimax rates under label DP. We provide a detailed comparison of the convergence rates of nonparametric classification and regression under label DP with that under full DP as well as the non-private rates, under both central and local models.

%This paper studies the theoretical limits of label DP, under which each sample is a mixture of public feature and private labels, thus existing methods can not be directly applied here. Under the central model, the minimax analysis becomes more challenging, since the packing method is only suitable for fixed model structures (i.e. the dimensionality of model output is fixed), while we need to find the minimum possible error over all possible learners with arbitrary output dimensions.