\section{Multilingual Synthetic Data}

\label{sec:datageneration}
%The core component of \textsc{Multilingual StyleDistance} 
%We build on the work of \citet{patel2024styledistancestrongercontentindependentstyle} %by expanding their synthetic dataset.
%who created a synthetic dataset of English sentence pairs that convey similar content (are near paraphrases) but differ in style.
%\paragraph{Languages and Features.}
We extend the \citet{patel2024styledistancestrongercontentindependentstyle} dataset to nine languages ($L$): Arabic, German, Spanish, French, Hindi, Japanese, Korean, Russian, and Chinese.\footnote{These languages were chosen because they align with the linguistic background of our annotators.} %We rely on the style features identified by \citet{patel2024styledistancestrongercontentindependentstyle} excluding some features that are not applicable to some of the target languages. 
%In this section, we present our synthetic data generation process and the evaluation methods used to assess the datasetâ€™s quality. % before employing it to train our style embeddings.
\paragraph{Style Feature Selection}
We use the set of 40 features addressed in \citet{patel2024styledistancestrongercontentindependentstyle}, leaving out features not applicable to specific languages. For example, articles are not relevant for Chinese and Japanese so the corresponding features have been discarded. % are not addressed in these languages. 
Our set of features ($F$) includes syntactic features (e.g., active/passive voice, contractions, frequent use of function words), emotional and cognitive features (e.g., words indicating sentiment or cognitive processes), stylistic and aesthetic features (e.g., metaphors, formal tone), social and interpersonal features (e.g., polite or offensive tone), graphical and digital features (e.g., capitalization, emojis, numerical digits), temporal and aspectual features (e.g., focus on present or future).\footnote{A full list of features and details is given in 
Appendix \ref{sec:appendix:stylefeatures}.} 

\paragraph{Data Generation} For each retained feature $f \in F$ for a language $l \in L$, we generate 100 pairs of positive ({\tt pos}) and negative ({\tt neg}) examples (paraphrases). In each pair, {\tt pos} is a sentence that contains the style feature (e.g., a formal sentence or a metaphorical one) while {\tt neg} does not. This is illustrated in Figure \ref{fig:main} for the feature ``Active Voice''. Features that cannot be removed completely (e.g., ``usage of articles'') are present with higher frequency in {\tt pos} than in {\tt neg} examples.

Using the same prompting workflow as \citet{patel2024styledistancestrongercontentindependentstyle}, we generate sentence pairs by prompting GPT-4 with the DataDreamer library and an attributed prompt \citep{attrprompt} illustrated in Figure \ref{figure:promptexample} \citep{gpt4, datadreamer}. For diversity, a ``Topic'' for each generation is sampled by extracting a random sentence from a random document in the C4 corpus \citep{t5andc4}, and prompting GPT-4 to identify the topic. For further details on all prompts, see Appendix \ref{sec:appendix:generationdetails}. 



We experiment with two approaches to multilingual data generation. In our first approach, sentence pairs are directly generated in each $l \in L$ using a language-specific instruction in the prompt, as illustrated in Figure \ref{figure:promptexample}. % to generate directly in Russian.
In our second approach, English sentence pairs are generated using the prompting workflow and then translated into each target language $l \in L$.%with MT 
\footnote{We use \href{https://www.deepl.com}{DeepL} for all languages except for Hindi which is not supported, where we use \href{https://translate.google.com}{Google Translate} instead.} %We used both methods for data generation and conducted human validation to determine the best approach for each language.
We generate data using both methods and conduct human validation on a random 10\% split in order to determine the best approach for each language. 
\promptexample
% across all nine target languages 
% and selected the best approach as follows: % the stronger approach % for each language 
%based on a data validation procedure: % described below. %subsequent evaluations (see ~\nameref{sec:datavalidation}). 
% \section{Data Validation}
%\paragraph{Data Validation}
% \label{sec:datavalidation} %We evaluated the quality of our data through both human and automatic evaluations, following \citet{patel2024styledistancestrongercontentindependentstyle}. For the 
%For human validation, we split our dataset into a random 10\% split. 
%We recruited NLP students as annotators \footnote{MS and undergraduate students who are native speakers of a language and were offered extra credit for participation.}. 
%Our annotators\footnote{MS and undergraduate students native speakers of a language, who were offered extra credit for participation.} were shown the sentences in the test split and were asked to (a) judge whether a style feature was present in a sentence (Yes/No/Maybe), and (b) rate its fluency. 
Each sentence pair for a style feature creates two annotation task instances. For each task instance of a given style feature and the positive or negative sentence, we asked annotators\footnote{MS and undergraduate students native speakers of a language, who were offered extra credit for participation.} to provide: 1) a judgment on if the style feature is present in the sentence, and 2) a rating of the fluency.
We only considered instances annotated by at least three annotators. Inter-annotator agreement was $\alpha$ = $0.4247$ \citep{krippendorffalpha}. More details on the annotation can be found in Appendix \ref{sec:appendix:humanannotation}.

We calculate an aggregate ``feature presence'' accuracy score for each $l \in L$ by calculating whether the average feature presence score over all annotations is higher for the positive sentence than for the negative sentence in a pair. \footnote{We assign 1 for ``Yes'', 0.5 for ``Possibly'', and 0 for ``No''.} % 0, 0.5, and 1 for "No", "Maybe", and "Yes", respectively.}. 
We calculate an aggregate fluency score by taking an average of the fluency scores that each annotator gave each text \footnote{ We assign a score of 1 for "Fluent", 0.67 for ``Mostly Fluent'', 0.33 for ``Mostly Disfluent'', and 0 for ``Disfluent''.}. We selected the best approach (direct generation v.s. English \textrightarrow ~MT) for each $l \in L$ as the one that produced the most fluent sentences, or the highest feature presence score if both produced similarly fluent generations. The direct approach was selected for all languages in $L$ except for Japanese and Hindi. Our final average feature presence and fluency scores over all $l \in L$, with the best generation approach selected for each $l$, are 0.79 and 0.93, respectively, both above random chance (0.5). Detailed results by language are given in Appendix \ref{sec:appendix:dataseteval}. 

\paragraph{Data Validation} Following \citet{patel2024styledistancestrongercontentindependentstyle}, we also perform automatic validations of the generated data. We validate whether our parallel examples are indeed paraphrases by computing their average cosine similarity\footnote{We use \texttt{\small paraphrase-multilingual-mpnet-base-v2}.} \cite{sentencetransformers}. For comparison, we calculate the similarity of gold-standard paraphrases in the multilingual dataset compiled by \citet{scherrer-2020-tapaco} \footnote{We sample 100 rows of paraphrases from \citet{scherrer-2020-tapaco}.} for each language. The average similarity of our parallel examples is 0.91 which is comparable to that calculated on the \citet{scherrer-2020-tapaco} natural data (0.88), indicating that our pairs are reasonable paraphrases. We measure topic diversity across generated sentences for a $l \in L$ using the metric proposed by \citet{diversityscore} which relies on cosine distance\footnote{In this case, we only evaluate the {\tt pos} sentence which contains the style feature for each pair.}. For comparison, we also compute the diversity score for texts from \citet{scherrer-2020-tapaco}. Again, the two scores are comparable (0.83 vs. 0.85), showing that our examples approach the diversity of natural data. Detailed results can be found in Appendix \ref{sec:appendix:dataseteval}.
