% \newpage
\section{Human Annotation Details}

\mturkinterfacefig

\datasetevaltable % moved to force up a page

%\subsection{Human Annotation Interface}
\label{sec:appendix:humanannotation}

% We provide an example of a task instance in our annotation interface. Human annotators were asked to rate whether the style feature was present or not in the sentence, with the option to also select "Possibly" if the annotator was unsure (instructed to use sparingly). We provide annotators with a definition of each style feature as well.

We show above in Figure \ref{fig:mturkinterface} an instance from the human annotation interface. We first asked annotators whether a given style feature was present in a sentence in their chosen language. We also provided a definition for each style feature to help annotators in their decision. The annotators then had to rate the fluency of the sentence be selecting one of ``Fluent'', ``Mostly Fluent'', ``Mostly Disfluent'', or ``Disfluent''. %We also provided definitions for each style feature so help annotators in their decision. %could clearly understand them.


% \noindent We used a population of graduate students taking a class on natural language processing as the annotators. Each task instance was annotated by 10 distinct human annotators. We assign a score of 0 to ``No'', 0.5 to ``Possibly'', and 1 to ``Yes''. We average the scores from all 10 annotators assigned to each task instance. We consider to have agreement for a positive example if the average score is >=0.5, and for a negative example if the average score is < 0.5.

% We measure inter-annotator agreement using Krippendorf's Alpha \citep{krippendorffalpha} which indicates moderate agreement of 0.55. As a more easily interpretable measure of agreement between annotators, for each task instance, we also find, on average, around 8 out of the 10 annotators annotated in agreement on whether a style feature was present or not in the text.

Our annotators are undergraduate and graduate students from a NLP class and were offered extra credit for their participation in the study. Each instance was annotated by at least three annotators: three for languages with fewer native speakers such as Arabic and Russian; over ten for languages with a large number of native speakers, such as Chinese. We used Krippendorffâ€™s Alpha \citep{krippendorffalpha} to measure inter-annotator agreement, which indicated moderate agreement of $0.4247 \pm 0.1719$.

% \subsection{Chosen Method for each Target Language}
% \label{sec:appendix:chosenmethod}

% \llmormttable
