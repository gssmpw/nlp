\section{Evaluation}
\label{sec:evaluation}

\stelevaltable
\avevaltable

\paragraph{STEL-or-Content (SoC) Benchmark}

In order to evaluate our style embeddings, we construct a multilingual version of the SoC benchmark \citep{styleemb}.\footnote{The English SoC benchmark covered formality, complexity, number usage, contraction usage, and emoji usage.} SoC measures the ability of a model to embed sentences with the same style closer in the embedding space than sentences with the same content. We construct our \textbf{multilingual SoC benchmark} by sampling 100 pairs of parallel {\tt pos}-{\tt neg} examples for each language from four ground-truth datasets covering four style features and 22 languages: simplicity \citep{ryan-etal-2023-revisiting}, formality \citep{briakou-etal-2021-ola}, toxicity \citep{dementieva2024overview}, and positivity \citep{mukherjee2024multilingualtextstyletransfer}.\footnote{Combined, these datasets cover the following languages: Amharic, Arabic, Bengali, German, English, Spanish, French, Hindi, Italian, Japanese, Magahi, Malayalam, Marathi, Odia, Punjabi, Portuguese (Brazil), Russian, Slovenian, Telugu, Ukrainian, Urdu, and Chinese.} Each instance in our multilingual SoC benchmark consists of a triplet ($a$, {\tt pos}, {\tt neg}) constructed as explained in Section \ref{sec:styledistance}. However, following \citet{styleemb}, the distractor text in our SoC benchmark is always a paraphrase of {\tt pos}.
% \textcolor{gray}{For each instance of our multilingual SoC benchmark, we take two pairs of parallel examples to get (1) an anchor sentence, (2) a sentence with the same style but different content than the anchor, and (3) a sentence with the same content but different style than the anchor.}
A model tested on this benchmark is expected to embed $a$ and {\tt pos} closer than $a$ and {\tt neg}. We rate a model by computing the percent of instances it achieves this goal for across all instances. We form test instances for each $f \in F$ in a language corresponding to all possible triplets, resulting in 4,950  instances for each language-style combination.

We also construct a \textbf{cross-lingual SoC benchmark} that addresses embeddings' ability to capture style similarity \textit{across languages}. This can be useful, for example, to evaluate style preservation in translations. We construct the benchmark with the XFormal dataset \citep{briakou-etal-2021-ola}, which includes parallel data in French, Italian and Portuguese. %We use a similar formulation as described above to create each instance. However, rather than taking both pairs from the same language, we take the sentence pair (which is (2) and (3) described above) from a different language as the anchor pair. 
We again create triplets as described above, but instead of using {\tt pos} and {\tt neg} texts from the same language as the anchor ($a$), we sample them from a different language than $a$. We end up with 19,800 instances for each style in each language. Appendix \ref{sec:appendix:stelfig} contains illustrative examples from each benchmark.



% Next, we sample every combination of two paraphrase pairs from the 100 pairs for each style feature and language to form $_{100} C _{2} = 4,950$ STEL-or-Content instances. For each instance, we select one of the paraphrase pairs to be the anchor pair and the other to be the sentence pair. Following \citet{stel}, we replace one of the texts in the sentence pair with one of the texts from the anchor pair, resulting in (1) an anchor sentence, (2) a sentence with the same style but different content than the anchor, and (3) a sentence with the same content but different style than the anchor. The model is asked to embed (1) and (2) closer together than (1) and (3), and we compute the average across all instances.

% We elect not to use the STEL benchmark proposed by \citet{stel} because we believe STEL isn't as strong of a test on the content-independence of style embeddings as STEL-or-Content, and \citet{patel2024styledistancestrongercontentindependentstyle} find that base models trained for semantic embeddings perform well on STEL but not STEL-or-Content.

% We also construct crosslingual benchmarks that evaluate the ability of embeddings to capture style similarity \textit{across languages}. This can be useful, for example, to evaluate style preservation in translations. We address formality in this benchmark and use the XFormal dataset \citep{briakou-etal-2021-ola}, which includes data parallel in content across French, Italian and Portuguese. Given 100 paraphrase pairs in language A and 100 paraphrase pairs in language B, we create a STEL-or-Content instance out of every combination of pairs, resulting in $100 \cdot 99 = 9,900$ instances because we ignore the instances in which both pairs have the same content. We then use a similar process as described previously to assign the anchor and sentence pairs for every instance. Finally, we repeat this process with the style feature of the anchor sentence swapped (i.e. using the informal sentences rather than the formal sentences) to end up with $9,900 \cdot 2 = 19,800$ instances. Appendix \ref{sec:appendix:stelfig} contains examples from each benchmark.
% We will make our STEL-or-Content evaluation datasets publicly available as a resource.

\paragraph{SoC Evaluation Results}

The results obtained by \textsc{mStyleDistance} on the multilingual and cross-lingual SoC benchmarks are presented in Table \ref{table:steleval}. As no general multilingual style embeddings are currently available, we compare with a base multilingual encoder model \texttt{xlm-roberta-base} \citep{Conneau2019UnsupervisedCR} as well as a number of English-trained style embedding models applied in zero-shot fashion to multilingual text: \citet{styleemb}, \textsc{StyleDistance} embeddings \citep{patel2024styledistancestrongercontentindependentstyle}, and \texttt{LISA} \citep{lisa}. 
%\citep{lisa} as baseline models to compare against. 
\textsc{mStyleDistance} embeddings outperform these models on multilingual and cross-lingual SoC tasks confirming its suitability for multilingual applications. The other models perform slightly better than the untrained \texttt{xlm-roberta-base} but still worse than \textsc{mStyleDistance}. 

\paragraph{Ablation Experiments}

\ablationsimpletable

Following \citep{patel2024styledistancestrongercontentindependentstyle}, we run several ablation experiments to evaluate how well our model generalizes to unseen style features and languages. In the \textbf{In-Domain} condition, all style features are included in the training data for every language. To test generalization to unseen style features, in the \textbf{Out of Domain} condition, any style feature directly equivalent to those features tested in the Multilingual and Cross-lingual SoC  benchmarks are excluded from the training data. \textbf{Out of Distribution} further removes any style features indirectly similar or related to those tested in the benchmarks. Finally, \textbf{No Language Overlap} removes the languages present in the benchmark from the training data, in order to test generalization to new languages. Our results are given in Table \ref{table:simplifiedablation} where we measure how much of the performance increase on SoC benchmarks over the base model is retained, despite ablating training data. The results indicate that our method generalizes reasonably well to both ``out of domain'' and ``out of distribution'' style features, and very well to languages not in the training data. Further details on features and languages ablated and full results are provided in Appendices  \ref{sec:appendix:ablationdetails} and \ref{sec:appendix:ablationfull}.

\paragraph{Downstream Task}

Following \citet{patel2024styledistancestrongercontentindependentstyle}, we also evaluate our \textsc{mStyleDistance} embeddings in the authorship verification (AV) task, where the goal is to determine if two documents were written by the same author using stylistic features \citep{authorshipverification}. We use the datasets released by PAN\footnote{\url{https://pan.webis.de}} between 2013 and 2015 in Greek, Spanish, and Dutch. Our results are given in Table \ref{table:aveval_table}. \textsc{mStyleDistance} vectors outperform existing English style embedding models on Spanish and Greek, while Dutch shows similar performance to English \textsc{StyleDistance}. We hypothesize that the linguistic proximity (West Germanic roots) of the two languages helps \textsc{StyleDistance} to generalize to Dutch.





