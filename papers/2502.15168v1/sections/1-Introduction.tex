\section{Introduction}
\label{sec:introduction}

\mainfig

Style embedding models seek to embed texts with similar style closer in the embedding space regardless of their content. Style embeddings are useful for tasks like style transfer and authorship attribution, but only exist for English \citep{styleemb,patel2024styledistancestrongercontentindependentstyle}. Multilingual style embeddings could also serve to automatically evaluate style preservation in machine translation. Models like XLM-RoBERTa \citep{Conneau2019UnsupervisedCR} and E5 \citep{wang2024multilinguale5textembeddings} create multilingual representations for semantic tasks, but have not addressed style mainly due to the scarcity of style datasets.

% We leverage the methodology of \textsc{StyleDistance} of \citet{patel2024styledistancestrongercontentindependentstyle} and extend it to the multilingual setting. % by using the same synthetic procedure used to train English style embeddings.
% Current style embedding methods like Wegmann \citep{styleemb} are typically trained on large web datasets. However, these datasets lack quality content control and use authorship as a proxy for style, which can be inaccurate. Truly parallel style datasets are rare, even for the English language. The problem is magnified in the multilingual setting, particularly for low-resource languages. To address this, /citet{patel2024styledistancestrongercontentindependentstyle} introduced the use of synthetic parallel style examples to train \textsc{StyleDistance}, a style embedding model. However, their work is only done on English. Given that one of the primary benefits of synthetic data is that it isn't reliant on having large amounts of data, we believe this approach can be valuable in the multilingual domain, particularly for low-resource languages. 

%Human-annotated multilingual % parallel style datasets are typically small and not scalable \citep{mukherjee2024multilingualtextstyletransfer, briakou-etal-2021-ola, dementieva2024overview, ryan-etal-2023-revisiting}. 
% Our proposed methodology allows for the creation of multilingual style embeddings based on synthetic data. 

We propose a procedure, called multilingual \textsc{StyleDistance} (\textsc{mStyleDistance}), to train style embeddings using contrastive learning with synthetic data in multiple languages. Early work on style representations learning often involved unlabeled social media data \citep{deepstyle, styleemb, luar, lisa}, but \citet{patel2024styledistancestrongercontentindependentstyle} showed that a contrastive learning objective with synthetic examples (sentence pairs with similar content and different style) can generate high quality style representations for English. 
% \citet{lai-etal-2022-multilingual} used machine-translated parallel data for formality transfer. % focusing on formality
% \citet{krishna-etal-2022-shot} also proposed a few-shot formality transfer method which controls for style with vectors that model stylistic differences between paraphrases. These vectors are different from our embeddings which map all sentences onto a vector space, %Also, both of these works focus on formality, while we 
% and address a wider range of style features. %and other work addressing the lack of parallel style data is sparse.
We create \textsc{mSynthStel}, a synthetic dataset of paraphrases addressing various style features in nine languages, and use it to create our multilingual style embeddings.

In order to evaluate their quality, we contribute a new multilingual and cross-lingual STEL-or-Content (SoC) evaluation benchmark which, following the original SoC evaluation task \cite{styleemb}, measures the ability of a model to embed sentences with the same style closer in the embedding space than sentences with the same content. We show that \textsc{mStyleDistance} embeddings outperform other representations on these evaluations, and demonstrate their usefulness in a downstream setting addressing a multilingual authorship verification task. We publicly release our model, data, and evaluation benchmarks. 

% We create a dataset of parallel positive/negative style examples (near-exact paraphrases) covering up to 40 style features in 9 languages (called \textsc{msynthstel}). We fine-tune a model on this dataset using contrastive learning to create the first publicly available multilingual style embedding model. We evaluate the quality of the obtained style representations on the Multilingual and Crosslingual STEL-or-Content benchmarks, also created by us. These benchmarks extend the STEL-or-Content evaluation for English \citep{styleemb} in the multilingual setting and are aimed to measure the content-independence of multilingual style embeddings. Additionally, we test the usefulness of the multilingual representations in a downstream setting, addressing a multilingual authorship verification task. % Our model is the first publicly available multilingual style embedding model. %similar to available English style embedding models like Wegmann's style embeddings \citep{styleemb} and \textsc{StyleDistance} \citep{patel2024styledistancestrongercontentindependentstyle}.

% Marianna: there are no other multilingual models to compare with


