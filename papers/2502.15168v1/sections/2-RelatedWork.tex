% \section{Related Work}
\section{Background}
\label{sec:relatedwork}

% \subsection{Style Representations}
% \label{sec:steleval}

Early work on learning style representations % often 
involved contrastive learning with triplets derived from unlabeled social media data \citep{deepstyle, styleemb, luar, lisa}. \citet{patel2024styledistancestrongercontentindependentstyle} showed that a contrastive learning objective with synthetically generated parallel examples can generate high quality style representations for English. 
%\citet{stel} introduced a benchmark for evaluating style representations quality also for English.
% \subsection{Multilingual LLMs and Style}
% Advances in multilingual LLMs have significantly improved their ability to capture information across languages. 

Models like XLM-RoBERTa \citep{Conneau2019UnsupervisedCR} and E5 \citep{wang2024multilinguale5textembeddings} can create robust multilingual representations for semantic tasks (e.g., cross-lingual retrieval and transfer) but there has been less focus on style-related tasks mainly due to the scarcity of style datasets. % Existing 
Human-annotated multilingual % parallel 
style datasets are typically small and not scalable \citep{mukherjee2024multilingualtextstyletransfer, briakou-etal-2021-ola, dementieva2024overview, ryan-etal-2023-revisiting}. Our proposed methodology allows for the creation of multilingual style embeddings based on synthetic data. 

% Researchers have explored various ways to improve multilingual models despite the lack of quality data. For semantic tasks, \citet{hangya-etal-2023-multilingual} propose a method using chains of related languages to improve embeddings for low-resource languages, while \citet{artetxe-schwenk-2019-massively} introduce a new architecture to allow embeddings pretrained on English to be effectively transferred to other languages.
% Researchers have explored various ways to improve multilingual style transfer and style representations despite lacking quality parallel data.
%\citet{lai-etal-2022-multilingual} demonstrate that finetuning on machine-translated parallel data improves style transfer performance. \citet{krishna-etal-2022-shot} propose a few-shot method that controls for style with vectors that model stylistic differences between paraphrases. 
%Machine-translated parallel data have previously been used for improving formality transfer % focusing on formality
%\citep{lai-etal-2022-multilingual}. 
\citet{patel2024styledistancestrongercontentindependentstyle} %by expanding their synthetic dataset.
created a synthetic dataset of English sentence pairs with similar content (near paraphrases) and different style, which were used to learn embeddings for several style features. \citet{lai-etal-2022-multilingual} used machine-translated parallel data for formality transfer. % focusing on formality
\citet{krishna-etal-2022-shot} also proposed a few-shot formality transfer method which controls for style with vectors that model stylistic differences between paraphrases. These vectors are different from our embeddings which map all sentences onto a vector space, %Also, both of these works focus on formality, while we 
and address a wider range of style features. %and other work addressing the lack of parallel style data is sparse.



% ajayp: The Krishna et. al 2022 work specifically mentions they do some sort of "style vector" thing. You need to mention that, that is particularly why it is related work. It might not be a general purpose multilingual style embedding, but they clearly are have some sort of multilingual style vector. You need to mention it because it is relevant, and also mention why there's is not the same thing as ours.
% For example, they use an encoder-decoder to extract a style vector to guide multilingual style transfer, however, this style vector is extract from English exemplars and is purpose-made for their style transfer procedure, it is not meant to be a standalone style embedding for general multilingual style tasks.

% Our research aims to tackle this data scarcity by leveraging a combination of synthetic data and machine translation to create quality parallel style data while validating our synthetically trained model on real, natural text data in our evaluations.

% \citet{krishna-etal-2022-shot} propose a few-shot transfer method where sentences in low-resource languages are translated to English for style transfer before backtranslating to the original language. They extract style vectors by subtracting the embedding of the sentence from the embedding of its paraphrase and use them to correct style extraction mismatches. These vectors represent the transformations needed to reconstruct sentences from their paraphrases, and are different from our embeddings, which map all sentences onto a vector space to represent style.