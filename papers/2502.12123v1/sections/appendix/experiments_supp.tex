\clearpage
\section{Additional experimental results}
\label{sec:appendix:experiments}

We complement \Cref{sec:experiments} by providing additional technical details.

\subsection{Additional results about language models trained on synthetic data}
\label{sec:appendix:experiments:synthetic}



\subsubsection{Visualizing the language model representations of correct vs. incorrect sequences}
\label{sec:experiments:synthetic:visualizing}



\begin{figure}[!h]
  \centering
  \begin{minipage}[b]{0.5\textwidth}  % 1.0 for arxiv
    \centering
    \includegraphics[width=1.0\textwidth]{figures/correct_vs_err_rep_dyck.png}  % 0.5 for arxiv
  \end{minipage}
  % \vspace{-0.5cm}
  \caption{
  TSNE plot for the $\lm$ last-layer-last-position representations of strings in $X_{\text{error}} \cup X_{\text{correct}}$.
  Red dots correspond to the representations of incorrect strings, 
  whereas gray dots correspond to the representations of correct strings of comparable lengths.
  We can see that the representations of incorrect strings form just a few clusters.
  This intuitively justifies using a lightweight verifier on top of these $\lm$ representations. 
  }  
  \label{fig:correct_vs_err_rep_dyck}
  % \vspace{-0.3cm}
\end{figure}





\clearpage
\subsubsection{The predicted backtracks were necessary}
\label{sec:experiments:synthetic:predicted_backtracks_were_necessary}

During the experiment in  \Cref{sec:experiments:synthetic:verifier_reduces_errors},
the trained verifier $\verifier$ predicted backtracks at many positions.
Were they really necessary?
For each setting of backtrack quota $\backtrackQuota$
and backtrack stride $\backtrackStride$, 
we collect the set of prefixes $X_{\text{predicted backtracks}}$ where $\verifier$ predicted backtracks.
Then, we let the language model $\lm$ complete each string in $X_{\text{predicted backtracks}}$ without any backtracks,
using common decoding techniques such as 
nucleus sampling top\_p = 0.9 \citep{holtzman2020the}
and argmax greedy decoding.
\Cref{table:predicted_backtracks_were_necessary} shows that
without backtracking,
the completion accuracy is much lower than the accuracy reported in \Cref{table:dyck_verifier_error_inducing}.
This implies that $X_{\text{predicted backtracks}}$ were indeed challenging prefixes for the $\lm$,
which verifies that the backtracks predicted by verifier $\verifier$ were necessary.

\begin{table*}[h]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lccccc }
\toprule
\textbf{$\backtrackQuota$} & \textbf{$\backtrackStride$} & \textbf{\#backtracks} & \multicolumn{2}{c} {\textbf{accuracy without backtrack}}  \\
& & & nucleus sampling top\_p = 0.9 & argmax \\
\hline
1 & 2 & 163 & 0.313 & 0.344 \\
\hline
  & 4 & 163 & 0.337 & 0.319 \\
\hline
  & 6 & 163 & 0.331 & 0.288 \\
\hline
2 & 2 & 311 & 0.347 & 0.328 \\
\hline
  & 4 & 297 & 0.357 & 0.349 \\
\hline
  & 6 & 286 & 0.374 & 0.373 \\
\hline
4 & 2 & 600 & 0.371 & 0.353 \\
\hline
  & 4 & 532 & 0.419 & 0.404 \\
\hline
  & 6 & 489 & 0.509 & 0.523 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
Predicted backtracks were necessary.
For each setting of backtrack quota $\backtrackQuota$
and backtrack stride $\backtrackStride$, 
we report the number of times that \algoName (\Cref{alg:sampling_with_backtracking}) backtracked.
Moreover, we report the completion accuracy of letting the language model $\lm$ complete these backtracked prefixes without any backtrack.
For each setting, the completion accuracy is much lower than the accuracy reported in \Cref{table:dyck_verifier_error_inducing}.
This implies that these backtracked prefixes were indeed challenging prefixes for the $\lm$.
}
\label{table:predicted_backtracks_were_necessary}
\end{table*}




% \clearpage
% \subsubsection{\algoName reduces completion errors on unseen OOD prefixes}
% \label{sec:appendix:experiments:synthetic:verifier_reduces_errors_unseen_ood}

% This section presents the experimental results of \Cref{sec:experiments:synthetic:verifier_reduces_errors_unseen_ood}.






\clearpage
\subsubsection{Error analysis on the remaining mistakes}
\label{sec:experiments:synthetic:error_analysis}

Given the improvement of accuracy (\Cref{sec:experiments:synthetic:verifier_reduces_errors_unseen_ood}) as a result of our algorithm \algoName (\Cref{alg:sampling_with_backtracking}),
why did the model still make mistakes?

We conducted an error analysis which parses all mistakes into error types, 
and examine the generated token, the $\lm$ predicted most probable token, their predicted probabilities, and a few intermediate variables during the course of our algorithm \algoName (\Cref{alg:sampling_with_backtracking}).

In summary, the findings are:
\begin{enumerate}
    \item Among 225 generated mistakes, 222 correspond to predicting an incorrect closing bracket, and 3 correspond to pre-maturely predicting the end-of-sequence \texttt{<eos>} token.
    \item In all 225 cases, the final state of the algorithm has used up all the backtrack quota $\backtrackQuota$ allocated to it, so even if the error predictor was perfect, the algorithm would not have been had a chance to correct these mistakes. This suggests that suitably increasing backtrack quota $\backtrackQuota$ might be an effective approach in improving the accuracy (though there are trade-offs with query efficiency).
\end{enumerate}

A snapshot of our error analysis result is included in \Cref{fig:error_analysis},
and we plan to open source the experimental codes, 
which will include the full error analysis results.



\begin{figure*}[!h]
  \centering
  \begin{minipage}[b]{1.0\textwidth}  % 1.0 for arxiv
    \centering
    \includegraphics[width=1.0\textwidth]{figures/error_analysis.png}  % 0.5 for arxiv
  \end{minipage}
  % \vspace{-0.5cm}
  \caption{
  Error analysis table for mistakes of language model trained on Dyck grammar and sampled using \algoName (\Cref{alg:sampling_with_backtracking}).
  The last column records the remaining backtrack quota $\backtrackQuota$ at the time of generating the incorrect token.
  }  
  \label{fig:error_analysis}
  % \vspace{-0.3cm}
\end{figure*}




\clearpage
\subsubsection{\algoName maintains diversity}
\label{sec:experiments:synthetic:diversity}

In this section, we show that the significant accuracy improvement is not at the cost of reducing diversity.

Our experiment freshly samples 100 prompts following the same distribution as $\dyck_{\text{OOD}}$ (\Cref{sec:experiments:synthetic:dyck}).
For each prompt, we let the trained $\lm$ independently sample 10 completions,
using \algoName (\Cref{alg:sampling_with_backtracking}) or the baseline algorithm,
and will compare how many (out of 10) samples were different,
and report the mean and standard error across the 100 prompts.

\Cref{table:dyck_diversity} shows that \algoName (\Cref{alg:sampling_with_backtracking}) generates similarly diverse samples as the baselines of
nucleus sampling with top\_p = 0.9 or 1.0.


\begin{table*}[h]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lccc }
\toprule
\textbf{$\backtrackQuota$} & \textbf{$\backtrackStride$} & \textbf{top\_p} & \textbf{diversity $\pm$ std err (out of 10)}  \\
\hline
4 & 4 & 1.0 & 5.52 $\pm$ 3.28  \\
\hline
0 & 0 & 0.9 &  5.47 $\pm$ 3.06  \\
\hline
0 & 0 & 1.0 & 5.84 $\pm$ 3.29  \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
Under the experiment setup described in \Cref{sec:experiments:synthetic:diversity},
\algoName (\Cref{alg:sampling_with_backtracking}) is similarly diverse as the baselines of
nucleus sampling with top\_p = 0.9 or 1.0.
}
\label{table:dyck_diversity}
\end{table*}




\clearpage
\subsection{Additional results about generating test cases with pretrained CodeLlama}
\label{sec:appendix:experiments:codellama}

This section complements our results in \Cref{sec:experiments:codellama}.

\subsubsection{Examples of prompts and model completions}
\label{sec:experiments:codellama:examples}


\begin{table}[h!]
\centering
\small
% \scalebox{0.8}{
\begin{tabular}{l}
\toprule
def f(a, b): \\
\quad \quad   return a + b \\
 \\
List 8 test cases of the above function f, one in each line: \\
assert f(5, 5) == 10 \\
assert f(1, 5) == 6 \\
assert f(2, 8) == 10 \\
assert f(6, 2) == 8 \\
assert f(6, 9) == 15 \\
assert f(4, 5) == 9 \\
assert f(9, 6) == 15 \\
assert f(6, 1) == 7 \\
 \\
def knk(l, item): \\
\quad \quad   assert type(l) is list \\
\quad \quad   l.append(item) \\
\quad \quad   return l \\
 \\
List 8 test cases of the above function knk, one in each line: \\
\\
\bottomrule
\\
def f(a, b): \\
\quad \quad   return a + b \\
 \\
List 8 test cases of the above function f, one in each line: \\
assert f(5, 8) == 13 \\
assert f(1, 5) == 6 \\
assert f(8, 4) == 12 \\
assert f(6, 2) == 8 \\
assert f(3, 9) == 12 \\
assert f(1, 7) == 8 \\
assert f(5, 9) == 14 \\
assert f(1, 7) == 8 \\
 \\
def ovs(l, item): \\
\quad \quad   assert type(l) is list \\
\quad \quad   l.append(item) \\
\quad \quad   return l \\
 \\
List 8 test cases of the above function ovs, one in each line: \\
\bottomrule
\end{tabular} 
% }
\caption{
Two example prompts for generating test cases for a simple implementation of the \texttt{append} function for Python lists.
In these examples, the following components are randomly generated and will be different for different prompts:
(1) the numbers in the demonstrations 
(e.g. \texttt{assert f(5, 8) == 13})
as long as they are correct for addition;
(2) the name of the target function
(e.g. \texttt{knk}, \texttt{ovs}).
}
\label{table:prompt_example}
\end{table}


\begin{table}[t]
\centering
\small
% \scalebox{0.8}{
\begin{tabular}{l}
\toprule
assert knk([], 2) == [2] \\
assert knk([1], 2) == [1, 2] \\
assert knk([1, 3], 2) == [1, 3, 2] \\
assert knk([1, 3, 5], 2) == [1, 3, 5, 2] \\
assert knk([1, 3, 5, 7], 2) == [1, 3, 5, 7, 2] \\
assert knk([1, 3, 5, 7, 9], 2) == [1, 3, 5, 7, 9, 2] \\
assert knk([1, 3, 5, 7, 9, 11], 2) == [1, 3, 5, 7, 9, 11, 2] \\
def sum\_list(l): \\
\quad \quad    return sum(l) \\
List 8 test cases of the above function sum\_list, one in each line: \\
assert sum\_list([]) == 0 \\
assert sum\_list([1]) == 1 \\
assert sum\_list([1, 3]) == 4 \\
assert sum\_list([1, 3, 5]) == 9 \\
assert sum\_list([1, 3, 5, 7]) == 16 \\
assert sum\_list([1, 3, 5, 7, 9]) == 25 \\
assert sum\_list([1, 3, 5, 7, 9, 11]) == 36 \\
def sublist(l, sub): \\
\quad \quad    assert type \\
\\
\bottomrule
\\
assert ovs([], 1) == [1] \\
assert ovs([2], 1) == [1, 2] \\
assert ovs([1, 2], 1) == [1, 1, 2] \\
assert ovs([1, 2], 3) == [1, 2, 3] \\
assert ovs([1, 2], 0) == [0, 1, 2] \\
assert ovs([1, 2, 3], 4) == [1, 2, 3, 4] \\
assert ovs([], 0) == [0] \\
assert ovs([1, 2], 0) == [0, 1, 2] \\
\bottomrule
\end{tabular} 
% }
\caption{
Two example generations by CodeLlama corresponding to the prompts in \Cref{table:prompt_example}.
Note that both generations are flawed:
(1) the model only generated 7 test cases instead of 8, even though the prompt requested 8.
Then, it generated irrelevant contents,
starting from \texttt{def sum\_list(l):}
(2) more than one generated test cases were wrong
(e.g. in \texttt{assert ovs([2], 1) == [1, 2]}, the correct right-hand-side should be \texttt{[2, 1]}).
More generally, we implemented a rule-based parser to analyze model generations and identify the error type (if any), and locate the first position of error.
}
\label{table:generation_example}
\end{table}







\clearpage
\subsubsection{Baselines}
\label{sec:experiments:codellama:baselines}

We extensively tuned the hyperparameters in common baseline decoding algorithms, including
\begin{itemize}
    \item nucleus sampling \citep{holtzman2020the}: we grid-searched top\_p $\in [0.0, 0.7, 0.8, 0.9, 0.95, 1.0]$.
    \item argmax greedy decoding: equivalent to top\_p = 0.0.
    \item standard autoregressive sampling: equivalent to top\_p = 1.0.
    \item temperature scaling \citep{ackley1985learning}: we grid-searched temperature $\in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2]$ (for each top\_p).
\end{itemize}

Through the above grid search, 
we found that the best combination was top\_p $= 0.95$, temperature $= 1.0$.

Besides, we consider baselines based on the \emph{block-best-of-n} rejection sampling approach to incorporate process rewards.
More details about this baseline are provided in the ``Block verifier" part of \Cref{sec:experiments:codellama:verifier}.
\begin{itemize}
    \item block-best-of-n: we grid-searched $n \in [2, 4, 8]$, fixing the best combination of top\_p and temperature found by the grid search above.
\end{itemize}

We will show that \algoName (\Cref{alg:sampling_with_backtracking})
clearly outperforms all these baselines in terms of the quality vs. query complexity trade-off.



% moved the verifier training section to main paper,
% may need to move back to appendix in conference style publications



% \clearpage
% \subsubsection{\algoName improves accuracy}
% \label{sec:appendix:experiments:codellama:verifier_improves_accuracy}

% The section presents the experimental results of \Cref{sec:experiments:codellama:verifier_improves_accuracy}.





\clearpage
\subsubsection{Full results of CodeLlama experiments in \Cref{sec:experiments:codellama}}
\label{sec:experiments:codellama:full_results}

\paragraph{Caption for \Cref{table:codellama_verifier_improves_accuracy} (on the next page).}
\algoName (\Cref{alg:sampling_with_backtracking}) improves accuracy
and outperforms commonly used baselines, including
nucleus sampling top\_p,
temperature scaling (temp),
and block best-of-n (BBoN) (\Cref{sec:experiments:codellama:verifier}).
Baselines are extensively hyperparameter tuned (\Cref{sec:experiments:codellama:baselines}).
% , and all results are reported in this table, for completeness.
Backtrack quota $\backtrackQuota = 0$ means a baseline without verifier.
When $\backtrackQuota > 0$,
the row denotes \Cref{alg:sampling_with_backtracking} 
with the corresponding $\backtrackQuota$ and $\backtrackStride$.
The column \emph{layer idx} denotes which layer of CodeLlama provided the representations for training the verifier,
and \emph{err threshold} denotes the cutoff below which the verifier output is interpreted as a rejection
(both were experimented in \Cref{sec:experiments:codellama:verifier}).
When BBoN is specified,
the row denotes the number of candidates generated for each block;
otherwise, the row does not use block best-of-n.
The rows are sorted by $\text{Acc}_\text{distinct}$.
Controlling top\_p and temperature, 
\Cref{alg:sampling_with_backtracking}
leads to better tradeoff between $\text{Acc}_\text{distinct}$ 
and query complexity $\mathcal{C}$
(both defined in \Cref{sec:experiments:codellama:task})
than all other methods.
The experiment was repeated 5 times,
and we report the standard errors.

% \vspace{-20mm}
\begin{table*}[!h]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lcccccccc }
\toprule
\textbf{$\backtrackQuota$} & \textbf{$\backtrackStride$} & \textbf{layer idx} & \textbf{err threshold} & \textbf{top\_p} & \textbf{temp} & \textbf{BBoN} & \textbf{$\text{Acc}_\text{distinct}$ $\pm$ std err} & \textbf{$\mathcal{C}$} \\
\hline
4 & 4 & 27 & 0.1 & 0.95 & 1.0 &  &  0.714 $\pm$ 0.011 & 39443 $\pm$ 235  \\
\hline
4 & 4 & 31 & 0.5 & 0.95 & 1.0 &  &  0.688 $\pm$ 0.028 & 39629 $\pm$ 135  \\
\hline
0 &  & 27 &  & 0.95 & 1.0 & 2 &  0.684 $\pm$ 0.038 & 39364 $\pm$ 1252  \\
\hline
4 & 4 & 31 & 0.1 & 0.95 & 1.0 &  &  0.677 $\pm$ 0.033 & 39546 $\pm$ 98  \\
\hline
4 & 4 & 27 & 0.5 & 0.95 & 1.0 &  &  0.676 $\pm$ 0.019 & 38555 $\pm$ 140  \\
\hline
0 &  &  &  & 0.95 & 1.0 &  &  0.660 $\pm$ 0.042 & 38231 $\pm$ 165  \\
\hline
4 & 4 & 27 & 0.1 & 1.0 & 1.0 &  &  0.639 $\pm$ 0.061 & 31274 $\pm$ 1559  \\
\hline
0 &  &  &  & 0.9 & 1.0 &  &  0.634 $\pm$ 0.023 & 38393 $\pm$ 14  \\
\hline
0 &  &  &  & 0.9 & 1.2 &  &  0.630 $\pm$ 0.028 & 38005 $\pm$ 232  \\
\hline
0 &  &  &  & 0.8 & 1.2 &  &  0.627 $\pm$ 0.015 & 38343 $\pm$ 90  \\
\hline
0 &  & 27 &  & 0.95 & 1.0 & 4 &  0.623 $\pm$ 0.036 & 65496 $\pm$ 7638  \\
\hline
4 & 10 & 27 & 0.1 & 1.0 & 1.0 &  &  0.622 $\pm$ 0.046 & 32923 $\pm$ 1772  \\
\hline
4 & 4 & 27 & 0.5 & 1.0 & 1.0 &  &  0.604 $\pm$ 0.047 & 31091 $\pm$ 968  \\
\hline
4 & 10 & 27 & 0.5 & 1.0 & 1.0 &  &  0.604 $\pm$ 0.030 & 27287 $\pm$ 7580  \\
\hline
0 &  &  &  & 0.95 & 1.2 &  &  0.584 $\pm$ 0.027 & 36601 $\pm$ 535  \\
\hline
0 &  &  &  & 1.0 & 0.8 &  &  0.562 $\pm$ 0.021 & 36610 $\pm$ 669  \\
\hline
0 &  & 27 &  & 0.95 & 1.0 & 8 &  0.559 $\pm$ 0.038 & 122933 $\pm$ 3832  \\
\hline
0 &  &  &  & 0.7 & 1.2 &  &  0.531 $\pm$ 0.035 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.95 & 0.8 &  &  0.523 $\pm$ 0.029 & 38386 $\pm$ 28  \\
\hline
0 &  &  &  & 0.8 & 1.0 &  &  0.511 $\pm$ 0.028 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 1.0 & 1.0 &  &  0.504 $\pm$ 0.025 & 30754 $\pm$ 1272  \\
\hline
0 &  &  &  & 0.9 & 0.8 &  &  0.466 $\pm$ 0.032 & 38400 $\pm$ 0  \\
\hline
4 & 4 & 27 & 0.1 & 1.0 & 1.2 &  &  0.440 $\pm$ 0.026 & 24916 $\pm$ 954  \\
\hline
0 &  &  &  & 1.0 & 0.6 &  &  0.399 $\pm$ 0.070 & 38320 $\pm$ 73  \\
\hline
0 &  &  &  & 0.7 & 1.0 &  &  0.353 $\pm$ 0.021 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.8 & 0.8 &  &  0.351 $\pm$ 0.039 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.95 & 0.6 &  &  0.337 $\pm$ 0.053 & 38400 $\pm$ 0  \\
\hline
4 & 4 & 27 & 0.5 & 1.0 & 1.2 &  &  0.334 $\pm$ 0.013 & 24217 $\pm$ 1214  \\
\hline
0 &  &  &  & 0.9 & 0.6 &  &  0.284 $\pm$ 0.044 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 1.0 & 1.2 &  &  0.269 $\pm$ 0.025 & 21906 $\pm$ 1780  \\
\hline
0 &  &  &  & 0.7 & 0.8 &  &  0.239 $\pm$ 0.019 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.8 & 0.6 &  &  0.212 $\pm$ 0.011 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 1.0 & 0.4 &  &  0.207 $\pm$ 0.029 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.95 & 0.4 &  &  0.176 $\pm$ 0.013 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.9 & 0.4 &  &  0.147 $\pm$ 0.013 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.7 & 0.6 &  &  0.101 $\pm$ 0.028 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 1.0 & 0.2 &  &  0.080 $\pm$ 0.020 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.8 & 0.4 &  &  0.074 $\pm$ 0.027 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.95 & 0.2 &  &  0.057 $\pm$ 0.018 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.9 & 0.2 &  &  0.029 $\pm$ 0.015 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.7 & 0.4 &  &  0.025 $\pm$ 0.016 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.8 & 0.2 &  &  0.021 $\pm$ 0.014 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.7 & 0.2 &  &  0.018 $\pm$ 0.011 & 38400 $\pm$ 0  \\
\hline
0 &  &  &  & 0.0 & 1.0 &  &  0.013 $\pm$ 0.000 & 38400 $\pm$ 0  \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\vspace{-4mm}
\caption{
\algoName (\Cref{alg:sampling_with_backtracking}) improves accuracy
and outperforms commonly used baselines, including
nucleus sampling top\_p,
temperature scaling (temp),
and block best-of-n (BBoN) (\Cref{sec:experiments:codellama:verifier}).
Due to space constraints, more detailed captions are in the beginning of this section.
\underline{
\textit{
\textbf{
To help readers parse these results, we included smaller tables, each analyzing a single aspect:
}
}
}
please refer to 
\Cref{table:codellama_verifier_improves_accuracy_simplified} in \Cref{sec:experiments:codellama:verifier_improves_accuracy},
\Cref{table:codellama_error_prediction_threshold} in \Cref{sec:experiments:codellama:verifier},
\Cref{table:layer27_better_than_layer31} in \Cref{sec:experiments:codellama:verifier},
and \Cref{fig:codellama_query_efficiency} in \Cref{sec:experiments:codellama:efficient}.
}
\label{table:codellama_verifier_improves_accuracy}
\end{table*}





\clearpage
\subsubsection{Visualizing the query efficiency of \algoName}
\label{sec:experiments:codellama:visualize_efficiency}

This section complements the query efficiency visualization discussed in \Cref{sec:experiments:codellama:efficient}.


\begin{figure*}[!h]
  \centering
  \begin{minipage}[b]{1.0\textwidth}  % 1.0 for arxiv
    \centering
    \includegraphics[width=1.0\textwidth]{figures/codellama_query_efficiency_no_bon.png}  % 0.5 for arxiv
  \end{minipage}
  % \vspace{-0.5cm}
  \caption{
  Similar to \Cref{fig:codellama_query_efficiency},
  just more zoomed-in, 
  excluding block best-of-n baselines (\Cref{sec:experiments:codellama:verifier}).
  }  
  \label{fig:codellama_query_efficiency_no_bon}
  % \vspace{-0.3cm}
\end{figure*}




\clearpage
\subsubsection{\algoName generalizes better to out-of-distribution prompts}
\label{sec:experiments:codellama:ood}

In this section, we show that 
\algoName (\Cref{alg:sampling_with_backtracking})
generalizes better to out-of-distribution prompts
than the best nucleus sampling and temperature scaling baseline in \Cref{sec:experiments:codellama:baselines}.
Unlike the synthetic Dyck grammar setting,
on real-world LLMs we do not have a precise quantitative control over how ``out-of-distribution" a prompt is for the LLM.
We therefore assume that a sufficient condition for a prompt in our setup to be out-of-distribution is that
the name of the target function denotes some meaning which is different from
the actual implemented functionality
(i.e. list \texttt{append}) 
(recall the task setup in \Cref{sec:experiments:codellama:task}).
Two examples of such out-of-distribution prompt are provided in \Cref{table:prompt_example_ood}.
We validate this assumption by observing that the accuracy indeed degrades on such ``out-of-distribution" prompts,
suggesting that the model is indeed confused by the inconsistency between the function names and the function implementations.
However, analogous to our observations on the synthetic Dyck grammar (\Cref{sec:experiments:synthetic:verifier_reduces_errors_unseen_ood}),
\algoName (\Cref{alg:sampling_with_backtracking}) again suffers much less reduction in accuracy on these ``out-of-distribution" prompts.
The detailed comparisons are reported in \Cref{table:codellama_ood}. 



\begin{table}[!h]
\centering
\small
% \scalebox{0.8}{
\begin{tabular}{l}
\toprule
def f(a, b): \\
\quad \quad   return a + b \\
 \\
List 8 test cases of the above function f, one in each line: \\
assert f(6, 5) == 11 \\
assert f(3, 2) == 5 \\
assert f(5, 4) == 9 \\
assert f(1, 5) == 6 \\
assert f(5, 4) == 9 \\
assert f(3, 5) == 8 \\
assert f(5, 6) == 11 \\
assert f(2, 6) == 8 \\
 \\
def add(l, item): \\
\quad \quad   assert type(l) is list \\
\quad \quad   l.append(item) \\
\quad \quad   return l \\
 \\
List 8 test cases of the above function add, one in each line: \\
\\
\bottomrule
\\
def f(a, b): \\
\quad \quad   return a + b \\
 \\
List 8 test cases of the above function f, one in each line: \\
assert f(8, 7) == 15 \\
assert f(8, 1) == 9 \\
assert f(4, 7) == 11 \\
assert f(8, 4) == 12 \\
assert f(7, 4) == 11 \\
assert f(8, 4) == 12 \\
assert f(1, 1) == 2 \\
assert f(5, 5) == 10 \\
 \\
def exp(l, item): \\
\quad \quad   assert type(l) is list \\
\quad \quad   l.append(item) \\
\quad \quad   return l \\
 \\
List 8 test cases of the above function exp, one in each line: \\
\bottomrule
\end{tabular} 
% }
\caption{
Two example \emph{out-of-distribution} prompts for generating test cases for a simple implementation of the \texttt{append} function for Python lists.
Different from the prompts in \Cref{table:prompt_example},
here the function names denote a clear meaning
(e.g. \texttt{add} or \texttt{exp}), 
which, however, is different from what the function implements
(i.e. \texttt{append}).
}
\label{table:prompt_example_ood}
\end{table}


\begin{table*}[h]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lccccc }
\toprule
\textbf{$\backtrackQuota$} & \textbf{$\backtrackStride$} & \textbf{err threshold} & \textbf{in-distribution $\text{Acc}_\text{distinct}$ $\pm$ std err} & \textbf{OOD $\text{Acc}_\text{distinct}$ $\pm$ std err} \\
\hline
4 & 4 & 0.1 & 0.714 $\pm$ 0.011 & 0.710 $\pm$ 0.029  \\
\hline
4 & 4 & 0.5 & 0.676 $\pm$ 0.019 &  0.687 $\pm$ 0.024  \\
\hline
0 &   &    & 0.660 $\pm$ 0.042 &  0.606 $\pm$ 0.034  \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
\algoName (\Cref{alg:sampling_with_backtracking})
generalizes better to out-of-distribution prompts
than the best nucleus sampling and temperature scaling baseline in \Cref{sec:experiments:codellama:baselines},
which we identified by grid search (\Cref{table:codellama_verifier_improves_accuracy}) to be 
top\_p = 0.95,
and temperature = 1.0.
We manually pick 10 target function names according to \Cref{sec:experiments:codellama:ood}
which were unseen when training the verifier (\Cref{sec:experiments:codellama:verifier}).
When backtrack quota $\backtrackQuota = 0$,
the row denotes a baseline algorithm that does not use the verifier
(and consequently the backtrack stride $\backtrackStride$ will not matter).
The column \emph{err threshold} denotes the cutoff below which the error predictor output is interpreted as a rejection (\Cref{sec:experiments:codellama:verifier}).
When $\backtrackQuota > 0$,
the row denotes \algoName (\Cref{alg:sampling_with_backtracking}) 
with the corresponding $\backtrackQuota$ and $\backtrackStride$.
\algoName (\Cref{alg:sampling_with_backtracking})
suffered minor or no drop between in-distribution and OOD $\text{Acc}_\text{distinct}$, 
whereas the baseline suffered a drop by more than one standard error.
The experiment was repeated 5 times,
and we report the standard errors.
}
\label{table:codellama_ood}
\end{table*}



\clearpage
\subsection{Additional ablation experiments on the \algoName algorithm (\Cref{alg:sampling_with_backtracking})}
\label{sec:appendix:experiments:algo}

Besides the ablation experiments in \Cref{sec:experiments:codellama:verifier} which probe various aspects of verifier training,
in this section, we focus on one algorithmic component.

Concretely, line 10 of \algoName (\Cref{alg:sampling_with_backtracking}) 
re-generates the erased positions using argmax.
This was motivated by our results in \Cref{sec:experiments:synthetic:dyck} which suggest that \emph{out-of-distribution prefix} is a cause of generator mistakes.
As a remedy, redoing the erased positions using argmax is intended to increase the generator-predicted probability of the partially sampled generation,
which (concatenated with the prompt) will be the prefix for subsequent generation steps.
We include an ablation study verifying that this improves the accuracy,
significantly under the synthetic data setting (\Cref{table:ablation:no_argmax_dyck}),
and only slightly (without hurting diversity) under the real data setting (\Cref{table:ablation:no_argmax_codellama}).

\begin{table}[h]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lcc }
\toprule
\textbf{sampling algorithm} & \textbf{\#errors $\pm$ std err} \\
\hline
\Cref{alg:sampling_with_backtracking} & 179.4 $\pm$ 1.020 \\
\hline
ablation: no argmax & 245.8 $\pm$ 8.658 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
Re-generating the erased positions using argmax in \algoName (\Cref{alg:sampling_with_backtracking}) reduces completion errors on unseen out-of-distribution (OOD) prefixes in Dyck grammar.
We fixed 
nucleus sampling \citep{holtzman2020the} top\_p = 0.9,
backtrack quota $\backtrackQuota = 4$,
and backtrack stride $\backtrackStride = 4$
(the best settings in \Cref{table:verifier_reduces_errors_unseen_ood}).
The row ``ablation: no argmax" refers to removing lines 9-12 in \Cref{alg:sampling_with_backtracking}.
We report the number of completion errors that occur when completing an unseen set of 10000 independently sampled out-of-distribution prompts $\dyck_{OOD}^{unseen}$.
The experiment was repeated 5 times,
and we report the standard errors.
}
\label{table:ablation:no_argmax_dyck}
\end{table}

\begin{table*}[h]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lcc }
\toprule
\textbf{sampling algorithm} & \textbf{err threshold}  & \textbf{$\text{Acc}_\text{distinct}$ $\pm$ std err} \\
\hline
\Cref{alg:sampling_with_backtracking} & 0.1 &  0.714 $\pm$ 0.011 \\
\hline
ablation: no argmax & 0.1 & 0.711 $\pm$ 0.032 \\
\hline
\Cref{alg:sampling_with_backtracking} & 0.5 & 0.676 $\pm$ 0.019 \\
\hline
ablation: no argmax & 0.5  & 0.663 $\pm$ 0.023 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
Re-generating the erased positions using argmax in \algoName (\Cref{alg:sampling_with_backtracking}) slightly improves the accuracy-diversity tradeoff (\Cref{sec:experiments:codellama:task}) in our test case generation task.
We fixed 
nucleus sampling \citep{holtzman2020the} top\_p = 0.95,
backtrack quota $\backtrackQuota = 4$,
and backtrack stride $\backtrackStride = 4$
(the best settings in \Cref{table:codellama_verifier_improves_accuracy}).
The row ``ablation: no argmax" refers to removing lines 9-12 in \Cref{alg:sampling_with_backtracking}.
The column \emph{err threshold} denotes the cutoff below which the error predictor output is interpreted as a rejection (\Cref{sec:experiments:codellama:verifier}).
The experiment was repeated 5 times,
and we report the standard errors.
}
\label{table:ablation:no_argmax_codellama}
\end{table*}
