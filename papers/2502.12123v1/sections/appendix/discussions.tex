\clearpage
\section{Discussions}
\label{sec:appendix:discussions}

\subsection{Is query efficiency a reasonable notion of efficiency?}

There are many reasonable efficiency metrics, and they do not always positively correlate with each other \citep{dehghani2021efficiency}.

Our paper focuses on \emph{query complexity} (measured by the number of tokens generated by the language model to satisfactorily complete the task~\footnote{
This definition is natural since generating one token involves one forward pass of the (decoder-only autoregressive) language model, i.e. one query.
}
),  and we do not claim that the same conclusions apply when we switch out query complexity for other metrics of efficiency, such as wall-clock time.

We think query complexity is one (but not necessarily the only, or the most) important aspect of efficiency due to the following considerations:
\begin{itemize}
    \item Many existing large language model (LLM) providers charge service fees to the users according to the number of tokens generated by the language model for the user, i.e. query complexity.
    \item In the \emph{single sequence generation} setting, 
    controlling all other conditions to be held the same, 
    query complexity positively correlates with the size of computation (the number of decoder forward passes) and wall-clock time.
    \item In the \emph{batched generation} setting, admittedly, the wall-clock time does not necessarily scale linearly with query complexity~\footnote{
    For example, the wall-clock time of generating $n$ candidate responses (with batch size $n$) might be less than $n$ multiplying the wall-clock time of generating 1 candidate response.
    }
    , meaning that the naive best-of-$n$ rejection sampling is not as slow as query complexity would indicate
    (if the LLM has sufficient bandwidth for it).
    However, in many realistic LLM inference settings,
    the LLM receives a large number of query requests per second, 
    so there is no additional idle availability~\footnote{
    Unless more GPUs/TPUs are allocated to serve this LLM.
    } 
    for duplicating each sequence generation request by $n$.
\end{itemize}

Although, as mentioned above, query complexity is partially indicative of a few practically important efficiency metrics (e.g. monetary cost or wall-clock time),
there are aspects of these metrics that are not tracked by query complexity.
For example, different types of \emph{hardware} and \emph{cache} may have different efficiency best practices.
In particular, on GPUs and TPUs, algorithms that better exploit \emph{parallelization} or \emph{tensorized computation} tend to be more efficient.
Therefore, an important direction for future work is to design and analyze \emph{hardware-aware algorithms} that incorporate these important aspects of the inference setup.


\clearpage
\subsection{On the hardness of the knapsack problem}

The hardness of the knapsack problem has been the subject of extensive study. Specifically, the decision version of this problem has found applications in the context of secure cryptosystems \cite{Odlyzko1998TheRA}. Under no assumptions on the input structure, the best known algorithm is based on dynamic programming \cite{knapsackProblems} and runs in pseudo-polynomial time. This algorithm is also used to obtain an FPTAS and its runtime is effectively polynomial if one further assumes that the weights are polynomially bounded in $D$. More exact or approximate algorithms achieve polynomial runtime, under specific input structures. Specifically, when the weights form a superincreasing sequence, that is,
\begin{align*}
X_i \geq \sum_{j=1}^{i-1} X_j \ \ \forall i \in [2, D] \cap \mathbb{Z}, 
\end{align*}
a greedy algorithm solves the knapsack decision problem \cite{Odlyzko1998TheRA} in linear time. On the other hand, when the density of the knapsack
\begin{align*}
    \frac{D}{\log_2 (\max_i \{X_i\}_{i=1}^d)}
\end{align*}
 is small enough, knapsack is approximately solved in polynomial time by lattice reduction algorithms \cite{plantard2013lattice}.
 Our argument considers the most general setting, in which no assumptions are made on the structure of the inputs $\{ X_i\}_{i=1}^t$, $c$ and the decision problem is NP-complete \cite{reducibilityKarp}.
