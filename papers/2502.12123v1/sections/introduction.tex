\section{Introduction}
\label{sec:intro}

The fast-evolving area of inference-time algorithms concerns itself with leveraging the already-impressive capabilities of language models \citep{raffel2020exploring, brown2020language, touvron2023llama}, %However, the quality of the generated samples can sometimes be suboptimal \citep{krishna2021hurdles, hendrycks2021measuring}.
%To alleviate such unreliability, a recent line of works propose to incorporate a 
together with a \emph{verifier} which can score generations of the language model. In the simplest form, called \emph{best-of-N}, the language model generates $N$ candidate responses,
which are then scored by the verifier,
and the highest-scored candidate response is chosen as the output of the inference process
\citep{cobbe2021verifiers, nakano2022webgpt}. If the verifier can score partial generations (sometimes called \emph{process reward}), the space for inference-time algorithms gets much richer: e.g., the final answer can be generated incrementally, using the verifier to guide the process (e.g., by incremental (blockwise) best-of-N, or more complicated strategies like Monte-Carlo-Tree-Search \citep{Browne2012ASO, hao2023reasoning}). Importantly, though a flurry of recent papers consider ``scaling laws'' of natural strategies, the algorithm design space of verifier-aided inference-time algorithms is still opaque.  In particular, the \emph{value of a verifier}---and the \emph{relationship it needs to have to the generator} is not well understood.     

In this paper, we show that a good verifier can substantially (both in theory and in practice) decrease the computational cost of natural generation tasks, using a pre-trained language model as an \emph{oracle}. In particular, we show that: 
\begin{itemize}
    \item Even simple \emph{constrained generation tasks}---where we are trying to generate a string in the support of a language oracle, subject to some structural constraint (e.g. describable as a simple formal language, like a regular language)---can be \emph{computationally intractable in the absence of a verifier}. 
    \item Conversely, access to a good \emph{process verifier}, one that can decide whether a prefix can be completed to a constraint-satisfying string, can remove these intractabilities. Moreover, even simple algorithms like tokenwise rejection sampling---wherein we generate the string one token at a time, using the process verifier as a means to accept or reject---can have substantive computational benefits over the baseline of rejection sampling. 
    \item Finally, on natural constrained generation tasks---namely, generating test cases for Python functions with a pretrained CodeLlama \citep{roziere2023code}---a \emph{verifier can be trained}, such that a simple, but natural generalization of tokenwise rejection sampling which is allowed to ``backtrack'' the last few generated tokens, achieves substantial benefits in computational efficiency, accuracy, and diversity of the generations.
\end{itemize}

%Some specific applications,
%where the texts are expected to follow certain structures,
%allow manually written rule-based verifiers (e.g. code interpreter, online form response format checker);
%on the other hand, when rule-based verifiers are not available,
%prior work trains machine learning models which function as (possibly imperfect) verifiers (aka ``reward models" \citep{lambert2024rewardbench}).
%[AR: I suggest instead starting with a discussion on the interest in inference-time scaling laws, and pointing out that the algorithms we usually check the scaling laws for are v simple. Here we aim to formally explore the algorithmic design space in structured settings.]
\iffalse
This \emph{verifier-assisted generation} approach generally improves the quality of the final output,
but also increases the inference cost:
on average, the \emph{query complexity}, measured by the number of tokens generated by the language model to satisfactorily complete the task~\footnote{
See \Cref{sec:appendix:discussions} for additional discussions on this metric.
}, needs to scale linearly in $n$. [AR: What is $n$?]
Thus, the following questions naturally arise: 
\begin{enumerate}%[leftmargin=*]
\setlength\itemsep{-0.1ex}
    \item[(Q1)] Assuming query access to a language model generator and a (hard-coded or trained) verifier, 
    are there algorithms with better quality-efficiency trade-offs? 
    \item[(Q2)] Is there a fundamental computational lower bound that governs the limits of potentials for verifier-assisted generation? [AR: Even in cases in which there is a perfect rule-based verifier.]
    \item[(Q3)] How does the optimal strategy depend on the verifier accuracy? [AR: Not sure what this means, or if we answer it? I guess if you mean the DP, I think we need to say "we propose backtracking as a special and natural strategy, and find the optimal algorithm in this class"] 
\end{enumerate}

\paragraph{Our contributions.}
Towards answering the questions above, we introduce a \emph{theoretical framework} to characterize the potentials and limitations of verifier-assisted generation.
[AR: I'd advocate to introduce modular knapsack early and make the case for why it could be a good sandbox.]
Precisely, 
\begin{itemize}%[leftmargin=*]
\setlength\itemsep{-0.1ex}
    \item We derive the \textbf{theoretically optimal amortized query-efficiency} for verifier-assisted generation 
    when the target language is generated by a simple finite state automata [AR: I think this is too strong: we only have it for single-backtracking strategy. This needs to be introduced and justified as a special strategy earlier.],
    and the generator samples from a fixed prior distribution
    (\Cref{sec:theory:dp}). 
    In particular, our theory incorporates the verifier accuracy into consideration.
    \item We develop a technique for tracking the expected query complexity via \textbf{dynamical programming}.
    In particular, 
    we show that proactively \emph{backtracking} (i.e. resampling the last token) based on the dynamical programming table can help reduce query complexity. [AR: Reduce compared to what? I guess we somehow need to state somewhere a ``reasonable'' baseline strategy.]
    \item Interestingly, when the inference algorithm is allowed to backtrack more than once, 
    we prove a \textbf{computational lower bound} even for \emph{estimating} the optimal expected query complexity,
    by reducing from a classic cryptographic hardness result, namely, the \emph{Knapsack problem} \citep{chor1988knapsack, nguyen2005adapting, vigoda2010lecture, gopalan2011fptas, plantard2013lattice}
    (\Cref{sec:theory:knapsack}).[AR: I think this is a somewhat strange phrasing. What we really prove is that achieving the optimal query complexity is hard (this is not really restricted to the algorithm backtracking --- it can really do anything). In fact, I think we show calculating the optimal expected queries is hard? (I don't think it's obv this implies that producing an algorithm with this number of queries is hard too --- needs an additional reduction?)]
\end{itemize}

We accompany these theoretical findings with an extensive set of empirical investigations. 
Precisely:  
\begin{itemize}%[leftmargin=*]
\item On synthetic data generated by finite state automata (e.g. modular arithmetic and the Dyck grammar i.e. balanced parentheses),
we verify our theory by showing that verifier-assisted generation with predicted backtracking (either using a perfect rule-based verifier or an imperfect trained verifier) significantly reduces the language model query complexity compared to naive rejection sampling baselines.
(\Cref{sec:experiments:synthetic}).
\item On more realistic tasks, namely, generating test cases for Python functions~\footnote{
This is a natural structured text generation task that requires both accuracy and diversity.
} 
with a pretrained CodeLlama \citep{roziere2023code}, 
the observations are less straightforward.
While using a perfect rule-based verifier still leads to a similarly significant reduction in query complexity,
naively switching it to a 97\%-accurate trained verifier does not significantly outperform having no verifier at all.
Our followup experiments reveal finer-grained design choices that are crucial for verifier-assisted generation when the verifier is imperfect.
(\Cref{sec:experiments:codellama}).
\end{itemize} 
\fi 
%Jointly, our theoretical and empirical findings suggest synergistically designing more query-efficient verifier-assisted generation algorithms, 
%and adapting the verifiers to best match such algorithms. 

