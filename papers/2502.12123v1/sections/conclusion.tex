% \vspace{-3mm}
\section{Conclusion}

We introduce a new theoretical framework for elucidating the design space of verifiers and correspondingly a simple family of rejection-sampling-based inference algorithms.
In particular, our theory proves the computational benefits of incorporating a \emph{process verifier},
measured by the \emph{query complexity} of calling the generator.
On the other hand, our theory also reveals the subtleties:
straightforwardly applying a process verifier in a Tokenwise rejection sampling algorithm may unintentionally re-weigh the distribution among sequences that satisfy the constraints,
which could be undesirable for settings that require a strong notion of distributional \emph{calibration}.
Empirically, through fine-grained experiments on both synthetic and realistic data,
we show that the Tokenwise rejection sampling algorithm, when combined with \emph{backtracking},
is a robustly effective recipe for reducing query
complexity, improving accuracy, and maintaining diversity. 
For future works, we hope the theoretical framework and empirical observations can inspire systematic characterization of the strengths and weaknesses of the diverse set of rejection-sampling-based inference-time algorithms.
Concrete open problems at the intersection of theory and experiments include 
investigating the realistic and necessary conditions on the verifiers for the inference-time algorithm to
achieve distributional calibration
(e.g. it is unrealistic in some language generation setting to assume that a verifier returns the calibrated acceptance probability in rejection sampling),
and synergistically designing query-efficient verifier-assisted generation algorithms. 

