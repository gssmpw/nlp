\section{\hspace{-2.5mm}Backtracking: a surprisingly effective rejection sampling strategy}
\label{sec:experiments}

The flexibility of the tokenwise rejection sampling with backtracking (\Cref{alg:sampling_with_backtracking}) makes it a very natural strategy to use in conjunction with trained verifies. We perform a thorough empirical investigation into the applicability of \algoName in constrained language generation,
and benchmark it against common baselines,
including rejection sampling (\Cref{d:rejection}),
nucleus sampling \citep{holtzman2020the},
temperature scaling,
and ``block best-of-N" (\Cref{sec:experiments:codellama:verifier}) sampling,
on both synthetic data (\Cref{sec:experiments:synthetic}) and more realistic data (\Cref{sec:experiments:codellama}).
We observe that across various settings, 
\algoName reduces query complexity, improves accuracy, and does not hurt diversity.

\subsection{Language models trained on synthetic data}
\label{sec:experiments:synthetic}

\subsubsection{Dyck grammar as a sandbox}
\label{sec:experiments:synthetic:dyck}

Real-world LLM pretraining data \citep{li2024datacomp} typically involves many diverse structures, 
so when an LLM algorithm outperforms baselines on a benchmark, it is generally challenging to precisely identify which component of the algorithm improved the handling of which structures of the data.

To have a quantitative control over the structure in the pretraining data distribution,
and to derive fine-grained observations about the effects of \algoName,
we synthetically generate the pretraining data based on the \emph{Dyck grammar}~\citep{SCHUTZENBERGER1963246},
a classic formal language (context-free grammar) consisting of balanced parentheses of multiple types
(for example, ``$[()]$'' is valid but ``$([)]$'' is not). 
Dyck serves as a useful sandbox, as it typifies features such as long-range dependencies and a hierarchical, tree-like structure—characteristics often found in both natural and programming language syntax—and has been a subject of interest in numerous theoretical studies on Transformers~\citep{yao2021self,liu2023same,liu2023Transformers,wen2023uninterpretability}.
More formally:

\begin{definition}[Dyck distribution]
\label{def:dyck}
    $\dyck_D$ denotes the Dyck language~\footnote{
        We follow a simplified version of \citet{wen2023uninterpretability} in defining a probability distribution over strings in a Dyck language.
    } 
    of length $D$ defined over  
    the vocabulary $\Sigma = \{ \squareOpen, \squareClosed, \roundOpen, \roundClosed \}$,
    whose \emph{length-$N$ prefix set} is denoted as $\dyck_{N}, \forall N \in [D]$.
    For a valid prefix $w_{1:N} \in \dyck_{N}$, the \emph{depth} of $w_{1:N}$ is 
    \begin{align*}
        \depth(w_{1:N}) &= \# \text{Open Brackets in }w_{1:N} \\
        &-  \# \text{Close Brackets in }w_{1:N}.
    \end{align*}
    The distribution $\distrib$ over $\dyck_{N}$, 
    (parameterized by $\probSquareBracket, q \in (0,1)$)
    is defined such that $\forall w_{1:N} \in \dyck_{N}$,
    \begin{align}
        \label{eq:dyck}
        &\prob(w_{1:N})
        \propto
        \probSquareBracket^{\abs{\{i \mid w_i = \squareOpen, \depth(w_{1:i}) = 1\}}} 
        \cdot (1 - \probSquareBracket)^{\abs{\{i \mid w_i = \roundOpen, \depth(w_{1:i}) = 1\}}} \\
        &\cdot (\probSquareBracket q)^{\abs{\{i \mid w_i = \squareOpen, \depth(w_{1:i}) > 1 \}}}
        \cdot ((1-\probSquareBracket) q)^{\abs{\{i \mid w_i = \roundOpen, \depth(w_{1:i}) > 1 \}}} \\
        &\cdot (1 - q)^{\abs{\{i \mid w_i \in \{ \squareClosed, \roundClosed \},  \depth(w_{1:i}) \le D - i\}}}.
        \notag
    \end{align}    
\end{definition}

\begin{remark}
    \Cref{eq:dyck} defines an intuitive autoregressive generative process for $\dyck_D$:
    if the current depth is 0, 
    then sample the next token from $\squareOpen$ and $\roundOpen$ 
    with probability $\probSquareBracket$ and $1-\probSquareBracket$ respectively;
    else if the current depth is $D - i + 1$, 
    implying that all the remaining positions have to be closing brackets,
    then deterministically close the last unmatched open bracket~\footnote{
    \label{footnote:acc}
    At any position, there is at most one valid closing bracket.
    };
    else, sample the next token from open or close brackets with probability $q$ and $1-q$ respectively.
    In other words, $\probSquareBracket$ controls the proportion of square vs. round brackets, while $q$ controls the tendency to predict an open bracket when possible 
    (a large $q$ may result in a large depth at some position).
    % ~\footnote{
    % By definition, a necessary but not sufficient condition for a string $s$ to belong to $\dyck_D$ is that $\depth(s) = 0$.
    % }
\end{remark}

In our experiments, we pretrain autoregressive Transformer \citep{vaswani2017attention} Language models 
(6 layers, 8 heads per layer, hidden dimension 512)
from scratch 
on data sampled from $\distrib$ 
with $D = 32, p = 0.2, q = 0.5$.
We use batch size 32, weight decay 0.1, learning rate 3e-4 with 100 warmup steps,
and follow \citet{block2024butterfly} to use exponential moving average to stabilize training.
We reached 100\% training and (in-distribution) validation accuracy.
% ~\footnote{
% On independently sampled strings from $\distrib$, measure whether the model correctly predicts closing brackets. (Recall \Cref{footnote:acc}.)
% } 

To search for stronger signals in benchmarking the accuracy of the trained model,
we will prompt it using the following type of \emph{out-of-distribution} prompts.
Note that since $p < 0.5$, the training data contains less square brackets than round brackets,
so long prefixes with many square brackets will be \emph{out-of-distribution} prompts for the trained model.
We generated a set of such out-of-distribution prompts $\dyck_{OOD}$ from $\dyck_{N}$ with $p = 0.8$ where the prefix length $N$ is uniformly randomly sampled from $25 \le N \le 31$.
We let the trained language model complete these prompts and check whether the completed string is in $\dyck_D$. 
Quantitatively:

\begin{definition}[Prompt completion accuracy]
    \label{def:acc}
    Given an autoregressive oracle $\mathcal{O}$ (\Cref{theory:autoreg_gen_def}) 
    and a set of prefix prompts $X$,
    the accuracy of $\mathcal{O}$ in completing $X$ is:
    \begin{equation*}
        \acc(\mathcal{O}, X) = \frac{1}{\abs{X}} \sum_{\prompt \in X, y \sim p_{\mathcal{O}}(\cdot \mid \prompt)} \1_{x \circ y \in \dyck_D}  
    \end{equation*}
\end{definition}

We construct the autoregressive oracle $\mathcal{O}_{\text{nucleus}}$ 
which predicts the next-token distribution based on our trained model 
with nucleus sampling \citep{holtzman2020the} top\_p set to 0.9.
We observed that 
$\acc(\mathcal{O}_{\text{nucleus}}, \dyck_{OOD}) = 94.23\%$.
We will show that $\mathcal{O}_{\text{verifier backtracking}}$ based on \Cref{alg:sampling_with_backtracking} can significantly reduce the remaining error rate.



\subsubsection{Training the verifier}
\label{sec:experiments:synthetic:verifier}

We collect a set of 441 prompts in $\dyck_{OOD}$ in which the trained model (denoted as $\lm$) made mistakes when completing them.
We implement a rule-based error parser according to the grammars of $\dyck_D$ which identifies the first position of error in each model completion.
Applying this parser to the model mistakes, we obtain a set of model-generated strings $X_{\text{error}} \subset \Sigma^*$ which contain errors.
By contrast, we sample another set of 441 strings $X_{\text{correct}} \sim \dyck_{OOD}$ such that
$X_{\text{error}}$ and $X_{\text{correct}}$ have the same length distribution.
We train a lightweight neural network verifier to distinguish $X_{\text{error}}$ from $X_{\text{correct}}$.

Concretely, to maximally exploit the representations learned by $\lm$,
we train a 1-linear-layer verifier $\verifier$
whose features are the last-layer-last-position representations by $\lm$ of strings in $X_{\text{error}} \cup X_{\text{correct}}$,
and labels are 0 for strings in $X_{\text{error}}$ and 1 for strings in $X_{\text{correct}}$.
Consequently, the trainable parameters of $\verifier$ are a single matrix of dimensionality 512 by 2.
Among the 882 strings in $X_{\text{error}} \cup X_{\text{correct}}$, 
we use 792 samples for training, and 90 samples for validation.
Despite being slightly over-parameterized,
this minimal verifier $\verifier$ achieved on average 93\% (with standard error 3.9\%) validation accuracy across 10 repetitions.
\Cref{fig:correct_vs_err_rep_dyck} in \Cref{sec:experiments:synthetic:visualizing} illustrates the intuition of why a lightweight verifier may be surprisingly effective with a small number of labeled samples.
% TODO: moved some results to appendix, may move back later
% In \Cref{sec:experiments:synthetic:preventing_errors} and \Cref{sec:experiments:synthetic:verifier_reduces_errors},
We next verify that forcing a backtracking at prefixes where the model made mistakes can effectively improve the completion accuracy (\Cref{sec:experiments:synthetic:preventing_errors}),
and that the trained verifier in this section can mostly catch those mistakes and thus mostly retaining the accuracy gain (\Cref{sec:experiments:synthetic:verifier_reduces_errors}).




\subsubsection{Backtracking effectively reduces errors}
\label{sec:experiments:synthetic:preventing_errors}

The trained language model $\lm$ made a mistake at the last position of each string $\prompt \in X_{\text{error}}$.
We therefore use ``error-inducing prefixes" $X_{\text{error-inducing}}$ to denote $\{ \prompt_{1:\abs{\prompt}-1} \mid \prompt \in X_{\text{error}} \}$.
\Cref{table:dyck_backtrack_at_error_inducing} shows that at prefixes in $X_{\text{error-inducing}}$, 
if we backtrack \emph{only once} for a small backtrack stride $\backtrackStride$, 
and continue the autoregressive sampling process, 
the error rate can be significantly reduced. 

\begin{table}[h]
% \vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{ lcc }
\toprule
\textbf{generation configuration} & \textbf{accuracy} \\
\hline
baseline: nucleus sampling top\_p = 0.9 & 0.331 \\
\hline
baseline: greedy argmax sampling & 0.334 \\
\hline
$\backtrackStride = 1$, then nucleus sampling top\_p = 0.9 & 0.366 \\
\hline
$\backtrackStride = 2$, then nucleus sampling top\_p = 0.9 & 0.438 \\
\hline
$\backtrackStride = 4$, then nucleus sampling top\_p = 0.9 & 0.591 \\
\hline
$\backtrackStride = 8$, then nucleus sampling top\_p = 0.9 & 0.790 \\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\caption{
At error-inducing prefixes,
a larger backtrack stride $\backtrackStride$ significantly improves completion accuracy (\Cref{def:acc}).
}
\label{table:dyck_backtrack_at_error_inducing}
\end{table}




\subsubsection{Verifier effectively reduces errors}
\label{sec:experiments:synthetic:verifier_reduces_errors}

In \Cref{sec:experiments:synthetic:preventing_errors},
the sampling process forced a backtracking at error-inducing prefixes $X_{\text{error-inducing}}$.
Can the error reduction effect be retained by a \emph{trained} lightweight single-layer verifier $\verifier$ in \Cref{sec:experiments:synthetic:verifier}?
\Cref{table:dyck_verifier_error_inducing} shows that
\algoName (\Cref{alg:sampling_with_backtracking})
using the trained verifier is remarkably effective.
Moreover, in \Cref{sec:experiments:synthetic:predicted_backtracks_were_necessary},
we verify that the predicted backtracks were necessary.

\begin{table}[h]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lccc }
\toprule
\textbf{$\backtrackQuota$} & \textbf{$\backtrackStride$} & \textbf{accuracy} \\
\hline
1 & 2 & 0.421 \\
\hline
  & 4 & 0.500 \\
\hline
  & 6 & 0.604 \\
\hline
2 & 2 & 0.457 \\
\hline
  & 4 & 0.634 \\
\hline
  & 6 & 0.762 \\
\hline
4 & 2 & 0.518 \\
\hline
  & 4 & 0.762 \\
\hline
  & 6 & 0.921 \\
\hline
\multicolumn{2}{c}{baseline: nucleus sampling top\_p = 0.9} &  0.331  \\ 
\hline
\multicolumn{2}{c}{baseline: greedy argmax sampling} & 0.334 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
When the prompts are error-inducing prefixes,
a single-layer trained verifier significantly improves completion accuracy
using \algoName (\Cref{alg:sampling_with_backtracking}).
A larger backtrack quota $\backtrackQuota$
and a larger backtrack stride $\backtrackStride$ 
are both helpful.
}
\label{table:dyck_verifier_error_inducing}
\end{table}





\subsubsection{\algoName reduces completion errors on unseen OOD prefixes}
\label{sec:experiments:synthetic:verifier_reduces_errors_unseen_ood}

\Cref{table:dyck_verifier_error_inducing} in \Cref{sec:experiments:synthetic:verifier_reduces_errors} reported a significant improvement of accuracy by \algoName (\Cref{alg:sampling_with_backtracking}) 
when the prompts are $X_{\text{error-inducing}}$,
for which the language model $\lm$ made mistakes during completion.
Is the verifier $\verifier$ overfitted to these type of error-inducing prompts?
Can the accuracy improvement generalize to (average-case) out-of-distribution (OOD) prefixes,
i.e. independently sampled strings of the same distribution as $\dyck_{OOD}$ (\Cref{sec:experiments:synthetic:dyck})?

We independently sampled 10000 such out-of-distribution prompts $\dyck_{OOD}^{unseen}$,
and benchmark the accuracy of \algoName (\Cref{alg:sampling_with_backtracking})
against the baselines of 
nucleus sampling top\_p = 0.9 \citep{holtzman2020the}
and standard autoregressive sampling (equivalent to top\_p = 1.0).
\Cref{table:verifier_reduces_errors_unseen_ood}
% (\Cref{sec:appendix:experiments:synthetic:verifier_reduces_errors_unseen_ood})
shows that 
\algoName (\Cref{alg:sampling_with_backtracking}) significantly reduces completion errors.
Crucially, the improvement does not diminish on top of the commonly used baseline of truncating the tail probabilities during sequence sampling.
This verifies the desirable property that 
\algoName 
can be applied in combination with such baselines to further improve accuracy.
We also verify that the accuracy improvement does not hurt diversity (\Cref{sec:experiments:synthetic:diversity}).

Finally, provided with the verifier,
why does the model still make mistakes?
We include additional error analysis in \Cref{sec:experiments:synthetic:error_analysis}.



% TODO: moved table (table:verifier_reduces_errors_unseen_ood) to appendix, can move back
\begin{table}[h]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lcccc }
\toprule
\textbf{nucleus sampling top\_p} & \textbf{$\backtrackQuota$} & \textbf{$\backtrackStride$} & \textbf{\#errors $\pm$ std err} \\
% \hline
% 0.0 & 0 & 0 & 178.0 $\pm$ 0.000 \\
% \hline
%   & 4 & 4 & 178 $\pm$ 0.000 \\
\hline
0.9 & 0 & 0 & 240.0 $\pm$ 5.177 \\
\hline
  & 4 & 4 & 179.4 $\pm$ 1.020 \\
\hline
1.0 & 0 & 0 & 461.8 $\pm$ 8.304 \\
\hline
  & 4 & 4 & 200.0 $\pm$ 3.225 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
\algoName (\Cref{alg:sampling_with_backtracking}) reduces completion errors on unseen out-of-distribution (OOD) prefixes.
Crucially, the improvement does not diminish on top of commonly used baselines, including
nucleus sampling top\_p = 0.9.
For each setting of top\_p,
we compare \algoName (\Cref{alg:sampling_with_backtracking})
(using backtrack quota $\backtrackQuota = 4$
and backtrack stride $\backtrackStride = 4$)
with the baseline 
(using backtrack quota $\backtrackQuota = 0$
and backtrack stride $\backtrackStride = 0$).
We report the number of completion errors that occur when completing an unseen set of 10000 independently sampled out-of-distribution prompts $\dyck_{OOD}^{unseen}$.
The experiment was repeated 5 times,
and we report the standard errors.
}
\label{table:verifier_reduces_errors_unseen_ood}
\end{table}





\subsection{Generating test cases with pretrained CodeLlama}
\label{sec:experiments:codellama}

Motivated by our findings in \Cref{sec:experiments:synthetic},
we apply essentially the same recipe of 
\algoName (\Cref{alg:sampling_with_backtracking})
to a real-data use case,
and show that \Cref{alg:sampling_with_backtracking} clearly improves the quality vs. query complexity trade-off on top of commonly used baselines, such as 
nucleus sampling \citep{holtzman2020the},
temperature scaling,
best-of-n rejection sampling,
and block best-of-n with process reward model.

\subsubsection{Task setup}
\label{sec:experiments:codellama:task}

A natural practical constrained generation task that requires both accuracy and diversity
is generating test cases for a target function specified by the prompt.
To have an unambiguous notion of groundtruth regarding accuracy and diversity,
we control the target function to be a simple implementation of the \texttt{append} function for Python lists.
Under this setting,
we wrote an evaluator script which analyzes model generated completions, 
measuring the accuracy by checking whether a test case correctly tests list \texttt{append},
and measuring the diversity by checking how many distinct test cases are generated.~\footnote{
Two test cases are different if and only if they test different lists or different appended items.
}

We write a program to systematically generate task prompts,
randomizing over function names and demonstration examples.
Each prompt includes 1 demonstration example specifying the intended output format,
followed by a target function (implementing \texttt{append}),
and finally requests 8 test cases be generated.
Two examples of the prompt are provided in \Cref{table:prompt_example},
and correspondingly, two examples of model completions of these prompts are provided in \Cref{table:generation_example}
in \Cref{sec:experiments:codellama:examples}.

\paragraph{Evaluation metrics}
The test prompts include 10 different target function names that are unseen during training.
Each target function name is independently tested 10 times.
Since each prompt requests 8 test cases,
the total number of test cases requested for each run of a decoding algorithm is 
$8 \times 10 \times 10 = 800$.
We will measure the following metrics:
\begin{enumerate}
    \item $N_\text{distinct correct}$: the number of \textbf{distinct correct} test cases generated. This metric naturally incorporates both accuracy and diversity.
    \item $\text{Acc}_\text{distinct} \coloneqq N_\text{distinct correct} / 800$.
    \item $\mathcal{C}$: the query complexity (analogous to \Cref{def:oracle_complexity}). We measure the total number of queries made to the generator $\lm$ when it completes the prompts.
    Each completion allows at most 384 tokens to be generated, so the max $\mathcal{C}$ is $384 \times 10 \times 10 = 38400$ 
    unless ``block best-of-n" (\Cref{sec:experiments:codellama:verifier}) is used.
\end{enumerate}

We use a pretrained CodeLlama \citep{roziere2023code}
as the generator language model $\lm$, 
which we freeze during our experiments.
We discuss common baselines in \Cref{sec:experiments:codellama:baselines}.
We follow almost the same approach as \Cref{sec:experiments:synthetic:verifier} to train our verifier on this coding task.
We next present technical details and ablation experiments regarding design choices of verifier training in \Cref{sec:experiments:codellama:verifier}.





\subsubsection{Training the verifier}
\label{sec:experiments:codellama:verifier}

We follow almost the same training approach as \Cref{sec:experiments:synthetic:verifier}.
The differences are described below.
The generator language model $\lm$ is a pretrained CodeLlama \citep{roziere2023code} (7B parameters), which we freeze during our experiments.

\paragraph{An intermediate layer provides more informative representations for verifier training than the last layer.}
Instead of training the verifier $\verifier$ on top of the last layer (i.e. layer 31) representations of $\lm$,
we instead treat the layer index as a hyperparameter, 
and conducted a grid search over layer index $\in \{ 3, 7, 11, 15, 19, 23, 27, 31\}$.
Among these candidates, layer 27 representations resulted in the best accuracy.
We therefore exclusively used layer 27 representations in subsequent experiments,
and finally conducted an ablation study on the top-performing setting of the baseline to back-test the impact of using other layers.
\Cref{table:layer27_better_than_layer31} shows that layer 27 outperforms layer 31.
We conjecture that the layer 31 representations may be too specific for the next-token prediction task,
which is not necessarily the optimal for discriminating correct prefixes vs. incorrect ones.~\footnote{
This is in line with some prior works that also observed that 
the final layers of language models tend to be more task-specific than the intermediate layers \citep{liu2019linguistic, kovaleva2019revealing, rogers2021primer}.
}
We also include results for a few other layers near the final layer.
Note that even with a sub-optimally chosen layer,
the accuracy of \algoName (\Cref{alg:sampling_with_backtracking}) 
still outperforms the top-performing settings of the baseline found through grid search.~\footnote{
See \Cref{sec:experiments:codellama:baselines} for details about baselines.
}

\begin{table*}[t]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lcc }
\toprule
\textbf{layer index} & \textbf{$\text{Acc}_\text{distinct} \pm$ std err} \\
\hline
27 & 0.714 $\pm$ 0.011 \\
\hline
28 & 0.711 $\pm$ 0.016 \\
\hline
26 & 0.708 $\pm$ 0.018 \\
\hline
30 & 0.706 $\pm$ 0.036 \\
\hline
24 & 0.701 $\pm$ 0.033 \\
\hline
31 & 0.688 $\pm$ 0.028 \\
\hline
29 & 0.676 $\pm$ 0.021 \\
\hline
25 & 0.672 $\pm$ 0.030 \\
\hline
23 & 0.709 $\pm$ 0.017 \\
\hline
3 & 0.700 $\pm$ 0.028 \\
\hline
15 & 0.700 $\pm$ 0.028 \\
\hline
19 & 0.692 $\pm$ 0.028 \\
\hline
7 & 0.691 $\pm$ 0.031 \\
\hline
11 & 0.650 $\pm$ 0.041 \\
\hline
ablation: random verifier & 0.663 $\pm$ 0.027 \\
\hline
baseline: nucleus sampling + temperature scaling & 0.660 $\pm$ 0.042 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
Ablation: 
layer 27 representations of CodeLlama outperform layer 31 (the last layer)
in terms of the quality of the error predictor trained based on these features.
We control all other setting to be the same as the top-performing settings of the baseline
(nucleus sampling top\_p = 0.95 \citep{holtzman2020the} and temperature 1.0),
whose performance is also included in the table.
The other rows in this table (layer 27 and layer 31)
refer to applying \algoName (\Cref{alg:sampling_with_backtracking}) 
using backtrack quota $\backtrackQuota = 4$,
backtrack stride $\backtrackStride = 4$,
and verifiers trained on layers 24, ..., 31 of the generator (CodeLlama), respectively.
The row \emph{ablation: random verifier} refers to 
a verifier that returns Uniform[0, 1], and uses the same $\backtrackQuota$, $\backtrackStride$ as the above.
The experiment was repeated 5 times,
and we report the standard errors.
The rows are sorted by mean $\text{Acc}_\text{distinct}$ (\Cref{sec:experiments:codellama:task}).
}
\label{table:layer27_better_than_layer31}
\end{table*}


\paragraph{With limited backtrack quota, it is better to be conservative in its usage.}
The verifier $\verifier$ is trained with binary labels
(1 if correct, 0 if wrong).
Although there are a roughly equal number of training samples whose labels are 0 or are 1,
using 0.5 as the error prediction threshold turned out to be suboptimal.
Since our \algoName (\Cref{alg:sampling_with_backtracking}) 
only allows a small backtrack quota $\backtrackQuota = 4$,
it makes sense to only use backtrack quota when the error predictor is very confident that the current intermediate generation is wrong.
Moreover, compared with our synthetic Dyck grammar setting (target length = 32) (\Cref{sec:experiments:synthetic}),
our code generation setting allows much longer generations (up to 384),
which further justifies conservatively spending the small backtrack quota $\backtrackQuota$.
Consequently, we consider decreasing the error prediction threshold to 0.1.
\Cref{table:codellama_error_prediction_threshold} shows that 0.1 is a better error prediction threshold than the default 0.5 in all settings we tried.



\begin{table*}[t]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lcccccc }
\toprule
\textbf{$\backtrackQuota$} & \textbf{$\backtrackStride$} & \textbf{top\_p} & \textbf{temperature} & \textbf{error prediction threshold} & \textbf{$\text{Acc}_\text{distinct}$ $\pm$ std err} \\
\hline
\hline
4 & 4 & 0.95 & 1.0 & 0.1 &  0.714 $\pm$ 0.011 \\
\hline
4 & 4 & 0.95 & 1.0 & 0.5 &  0.676 $\pm$ 0.019 \\
\hline
\hline
4 & 4 & 1.0 & 1.0 & 0.1 &  0.639 $\pm$ 0.061 \\
\hline
4 & 4 & 1.0 & 1.0 & 0.5 &  0.604 $\pm$ 0.047 \\
\hline
\hline
4 & 4 & 1.0 & 1.2 & 0.1 &  0.440 $\pm$ 0.026 \\
\hline
4 & 4 & 1.0 & 1.2 & 0.5 &  0.334 $\pm$ 0.013 \\
\hline
\hline
4 & 10 & 1.0 & 1.0 & 0.1 &  0.622 $\pm$ 0.046 \\
\hline
4 & 10 & 1.0 & 1.0 & 0.1 &  0.604 $\pm$ 0.030 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
Ablation: 0.1 is a better error prediction threshold than the default 0.5 in all settings we tried,
including various
nucleus sampling \citep{holtzman2020the} top\_p,
temperature scaling,
and backtrack stride $\backtrackStride$.
In this table, we divide the rows into groups of 2,
separated by double horizontal lines,
such that within each group,
the only difference is the error prediction threshold.
In all groups, 
0.1 leads to higher $\text{Acc}_\text{distinct}$ than 0.5.
The experiment was repeated 5 times,
and we report the standard errors.
}
\label{table:codellama_error_prediction_threshold}
\end{table*}





\paragraph{Block verifier.}
Our verifier applies to the token level,
i.e. predicting an accept/reject action after the generator $\lm$ generates each token.
In many practical settings (including ours),
it is natural to divide the generated output into \emph{blocks} 
(each block may contain multiple tokens),
e.g. in writing math proofs, each block may correspond to one reasoning step;
in writing codes, each block may correspond to one line of codes.
Recent works achieved strong empirical performance by 
generating multiple candidates for each block of intermediate model generations,
train process reward models that evaluate each candidate,
and select the best-scoring candidate
(see e.g. \citet{wu2024inference} and references therein).
We refer to this as the ``block-best-of-n" approach.
To compare with such ``block-best-of-n" baselines,
we train ``block verifiers" $\verifier_{\text{block}}$ which scores prefixes that are full lines of model output for our task.
We will show that this ``block best-of-n" approach is helpful, 
but is outperformed by our \algoName (\Cref{alg:sampling_with_backtracking})
in terms of accuracy-efficiency trade-off.

\paragraph{Does a deeper verifier perform better?}

The above experiments follow \Cref{sec:experiments:synthetic:verifier} in training a single-linear-layer verifier.
In this section,
we test the effects of scaling up the verifier depth.
Specifically, we test verifiers based on Multi-Layer Perceptrons \citep{rosenblatt1958perceptron} of depths 2, 4, 8,
with ReLU activations \citep{nair2010rectified} between adjacent parameterized layers.
\Cref{table:verifier_more_mlp_layers} shows that more MLP layers did not outperform the 1-linear-layer verifier
even though they can be trained to similar \emph{error-predicting} accuracies,
measured by their accuracy in predicting whether a prefix is correct or incorrect
on a held-old validation set of prompts for our task (\Cref{sec:experiments:codellama:task})
followed by partial generations by CodeLlama.
In other sections of this paper, unless otherwise noted, we always use a single-linear-layer verifier for \algoName (\Cref{alg:sampling_with_backtracking})
(and of course, no verifier for baselines (\Cref{sec:experiments:codellama:baselines}) ).

\begin{table*}[t]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lccc }
\toprule
\textbf{verifier \# MLP layers} & \textbf{verifier validation accuracy} & \textbf{$\text{Acc}_\text{distinct} \pm$ std err} \\
\hline
1 & 0.96 & 0.714 $\pm$ 0.011 \\
\hline
4 & 0.97 & 0.699 $\pm$ 0.038 \\
\hline
2 & 0.97 & 0.687 $\pm$ 0.035 \\
\hline
8 & 0.97 & 0.684 $\pm$ 0.015 \\
\hline
ablation: random verifier &  0.50 & 0.663 $\pm$ 0.027 \\
\hline
baseline: nucleus sampling + temperature scaling & N/A & 0.660 $\pm$ 0.042 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
Ablation: 
Deeper verifiers do not outperform the 1-linear-layer verifier
even though they can be trained to similar \emph{error-predicting} accuracies on held-old validation set.
We control all other setting to be the same as the top-performing settings of the baseline
(nucleus sampling top\_p = 0.95 \citep{holtzman2020the} and temperature 1.0),
whose performance is also included in the table.
The other rows in this table
refer to applying \algoName (\Cref{alg:sampling_with_backtracking}) 
using backtrack quota $\backtrackQuota = 4$,
backtrack stride $\backtrackStride = 4$,
and verifiers with 1, 2, 4, 8 layers, respectively.
The row \emph{ablation: random verifier} refers to 
a verifier that returns Uniform[0, 1], and uses the same $\backtrackQuota$, $\backtrackStride$ as the above.
The experiment was repeated 5 times,
and we report the standard errors.
The rows are sorted by mean $\text{Acc}_\text{distinct}$ (\Cref{sec:experiments:codellama:task}).
}
\label{table:verifier_more_mlp_layers}
\end{table*}



\paragraph{Where are the potentials for further improving $\text{Acc}_\text{distinct}$?}
How optimal are our verifiers,
and what are some ways to further improve them?
To probe these potentials,
we wrote a rule-based groundtruth verifier for our task (\Cref{sec:experiments:codellama:task})
and used it as a drop-in replacement of our trained verifier.
\Cref{table:codellama_groundtruth} shows that 
the $\text{Acc}_\text{distinct}$ enabled by our trained verifier almost reached the $\text{Acc}_\text{distinct}$ enabled by the groundtruth verifier,
showing that improving verifier training may not be the most fruitful direction for further improvement.
Interestingly, using a much larger $\backtrackQuota$ or $\backtrackStride$ (increasing from 4 to 10) 
does not necessarily improve the accuracy (sometimes even \emph{decreasing} the accuracy).
We conjecture that in these experiments, 
the (imperfect) generator oracle (CodeLlama),
not the verifier,
was the bottleneck for $\text{Acc}_\text{distinct}$.
As a result,
unnecessarily backtracking and forcing the model to re-generate more tokens may increase the chance that the model makes mistakes.


\begin{table*}[h]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lcccc }
\toprule
\textbf{verifier type} & \textbf{$\backtrackQuota$} & \textbf{$\backtrackStride$} & \textbf{$\text{Acc}_\text{distinct}$ $\pm$ std err} \\
\hline
\hline
groundtruth & 4 & 4 & 0.719 $\pm$ 0.022 \\
\hline
groundtruth & 10 & 4 & 0.717 $\pm$ 0.015 \\
\hline
\hline
trained & 4 & 4 & 0.714 $\pm$ 0.011 \\
\hline
trained & 10 & 4 & 0.692 $\pm$ 0.025 \\
\hline
\hline
ablation: random verifier & 4 & 4  & 0.663 $\pm$ 0.027 \\
\hline
baseline: nucleus sampling + temperature scaling & 0 & 0  & 0.660 $\pm$ 0.042 \\
\hline
\hline
trained & 4 & 10 & 0.622 $\pm$ 0.046 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{
Ablation: 
Our trained verifier approaches the accuracy of the groundtruth verifier,
evaluated by their ability to assist CodeLlama in completing our test case generation task (\Cref{sec:experiments:codellama:task})
using \algoName (\Cref{alg:sampling_with_backtracking}).
In these experiments, we control the
nucleus sampling \citep{holtzman2020the} top\_p = 0.95 and temperature scaling = 1.0
which are the optimal setting for baseline, found by grid search (\Cref{sec:experiments:codellama:baselines}).
The rows are sorted by $\text{Acc}_\text{distinct}$.
The row \emph{ablation: random verifier} refers to 
a verifier that returns Uniform[0, 1].
Interestingly, using a much larger $\backtrackQuota$ or $\backtrackStride$ does not necessarily improve the accuracy (sometimes even \emph{decreasing} the accuracy).
We conjecture that the generator model, CodeLlama, is imperfect, 
so unnecessarily backtracking and forcing the model to re-generate more tokens may increase the chance that the model makes mistakes.
The experiment was repeated 5 times,
and we report the standard errors.
}
\label{table:codellama_groundtruth}
\end{table*}





\subsubsection{\algoName improves accuracy}
\label{sec:experiments:codellama:verifier_improves_accuracy}

In this section, we show that 
\algoName (\Cref{alg:sampling_with_backtracking})
achieves higher $\text{Acc}_\text{distinct}$
than all the baselines described in \Cref{sec:experiments:codellama:baselines}.
Similar to our observations based on the synthetic Dyck grammar data (\Cref{sec:experiments:synthetic:verifier_reduces_errors_unseen_ood}), 
the improvement does not diminish on top of commonly used baselines.
This verifies the desirable property that 
\algoName 
(\Cref{alg:sampling_with_backtracking})
can be applied in combination with commonly used baselines to further improve accuracy.
The primary comparisons are reported in \Cref{table:codellama_verifier_improves_accuracy_simplified}, 
% (\Cref{sec:appendix:experiments:codellama:verifier_improves_accuracy}),
and additional results are in
\Cref{table:codellama_verifier_improves_accuracy}
in \Cref{sec:experiments:codellama:full_results}. 
Moreover, in \Cref{sec:experiments:codellama:ood},
we show that 
analogous to our observations on the synthetic Dyck grammar (\Cref{sec:experiments:synthetic:verifier_reduces_errors_unseen_ood}),
\algoName (\Cref{alg:sampling_with_backtracking})
generalizes better to \emph{out-of-distribution} prompts than baselines.




% TODO: moved the table (table:codellama_verifier_improves_accuracy_simplified) to appendix, can move back later
\begin{table}[t!]
% \vskip 0.15in
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{ lccccc }
\toprule
\textbf{$\backtrackQuota$} & \textbf{$\backtrackStride$} & \textbf{top\_p} & \textbf{\temperature} & \textbf{block BoN} & \textbf{$\text{Acc}_\text{distinct}$ $\pm$ std err} \\
\hline
4 & 4 & 0.95 & 1.0 &  &  0.714 $\pm$ 0.011 \\
\hline
0 &  & 0.95 & 1.0 & 2 &  0.684 $\pm$ 0.038 \\
\hline
0 &  & 0.95 & 1.0 &  &  0.660 $\pm$ 0.042 \\
\hline
0 &  & 0.95 & 1.0 & 4 &  0.623 $\pm$ 0.036 \\
\hline
0 &  & 0.95 & 1.0 & 8 &  0.559 $\pm$ 0.038 \\
\hline
\hline
4 & 4 & 1.0 & 1.0 &  &  0.639 $\pm$ 0.061 \\
\hline
4 & 10 & 1.0 & 1.0 &  &  0.622 $\pm$ 0.046 \\
\hline
0 &  & 1.0 & 1.0 &  &  0.504 $\pm$ 0.025 \\
\hline
\hline
4 & 4 & 1.0 & 1.2 &  &  0.440 $\pm$ 0.026 \\
\hline
0 &  & 1.0 & 1.2 &  &  0.269 $\pm$ 0.025 \\
\hline
\hline
0 &  & 0.0 & 1.0 &  &  0.013 $\pm$ 0.000 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\vspace{-5mm}
\caption{
\algoName (\Cref{alg:sampling_with_backtracking}) improves accuracy
and outperforms 
nucleus sampling top\_p,
temperature scaling \temperature,
and block best-of-n (BoN) (\Cref{sec:experiments:codellama:verifier}).
In this table, we divide the rows into groups,
separated by double horizontal lines,
such that each group uses the same top\_p and temperature.
The backtrack quota $\backtrackQuota = 0$ means a baseline algorithm that does not use the verifier.
$\backtrackQuota > 0$ means \algoName 
with the corresponding $\backtrackQuota$ and $\backtrackStride$.
\emph{block BoN} specifies the number of candidates generated for each block;
empty block BoN means not using block best-of-n.
In all groups, 
\algoName
leads to higher $\text{Acc}_\text{distinct}$ 
than all other methods.
The last group corresponds to
argmax greedy decoding,
which has low $\text{Acc}_\text{distinct}$ due to low diversity.
The experiment was repeated 5 times,
and we report the standard errors.
The complete set of experiments are reported in a larger \Cref{table:codellama_verifier_improves_accuracy} in \Cref{sec:experiments:codellama:full_results}.
}
\label{table:codellama_verifier_improves_accuracy_simplified}
\end{table}


\clearpage
\subsubsection{\algoName is query efficient}
\label{sec:experiments:codellama:efficient}

In this section, we show that 
\algoName (\Cref{alg:sampling_with_backtracking})
achieves a better tradeoff between $\text{Acc}_\text{distinct}$
and
query efficiency $\mathcal{C}$
than all the baselines described in \Cref{sec:experiments:codellama:baselines}.
The primary comparisons are visualized in \Cref{fig:codellama_query_efficiency} (in this section)~\footnote{
    This visualization in \Cref{fig:codellama_query_efficiency} slightly favors the ``block best-of-n sampling" baseline, 
    because its implementation stops the decoding process once the requested number of test cases are generated, 
    whereas when running our algorithm or non-best-of-n baselines, 
    the model is allowed to (and in fact does indeed) generate irrelevant tokens afterwards, 
    which hurts query complexity.
    Even under this disadvantage, 
    \algoName still outperforms the ``block best-of-n sampling" baselines.
}
and \Cref{fig:codellama_query_efficiency_no_bon}
(in \Cref{sec:experiments:codellama:visualize_efficiency}).
Numerical values of $\mathcal{C}$ are reported in
\Cref{table:codellama_verifier_improves_accuracy}
in \Cref{sec:experiments:codellama:full_results}. 

% \begin{remark}
    
% \end{remark}


\begin{figure*}[h!]
  \centering
  \begin{minipage}[b]{1.0\textwidth}  % 1.0 for arxiv
    \centering
    \includegraphics[width=1.0\textwidth]{figures/codellama_query_efficiency.png}  % 0.5 for arxiv
  \end{minipage}
  \vspace{-7mm}
  \caption{
  \algoName (\Cref{alg:sampling_with_backtracking}) is query-efficient.
  The horizontal axis denotes query complexity $\mathcal{C}$,
  and the vertical axis denotes the number of distinct correct test cases generated $N_\text{distinct correct}$,
  both defined in \Cref{sec:experiments:codellama:task}.
  Blue dashed lines correspond to the baselines (described in \Cref{sec:experiments:codellama:baselines}), 
  whereas orange solid lines correspond to \algoName with various $\backtrackQuota$ and $\backtrackStride$, 
  both defined in \Cref{alg:sampling_with_backtracking}.
  Since the slopes of the orange curves are visibly greater than the slopes of the blue curves,
  we conclude that \algoName is more query-efficient than baselines.
  The experiment was repeated 5 times,
  and each dot is the average metric of these 5 runs.
  The specific numbers and standard errors are reported in \Cref{table:codellama_verifier_improves_accuracy}.
  A more zoomed-in version of this plot is in \Cref{fig:codellama_query_efficiency_no_bon}.
  }  
  \label{fig:codellama_query_efficiency}
  % \vspace{-0.3cm}
\end{figure*}







% \yuchen{Additional results are included in \Cref{sec:appendix:experiments}.}







