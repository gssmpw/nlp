%\section{Theoretical framework}
%\label{sec:theory}

\section{Setup and notation}
\label{sec:theory:setup} 

Throughout, we let $\Sigma$ be a nonempty finite set, denoting the vocabulary. We denote as $\Sigma^i$ the set of strings of length $i$ and  by $\Sigma^{*} = \cup_{i \in \mathbb{N}} \Sigma^i$ the set of all finite strings on $\Sigma$. Given a string $s \in \Sigma^{*}$
, we denote as $s_i$ its $i$-th element and as $s_{i:j}$ the substring of $s$ starting at its $i$-element and ending at its $j$-element, included. 
We use 
$\abs{s}$ to denote the length of string $s$ and
$\epsilon$ to denote the empty string. 
Finally, we let $x \circ y$ denote the concatenation of string $x$ followed by string $y$.

\begin{definition} [Autoregressive oracle]
\label{theory:autoreg_gen_def}
     An \textbf{autoregressive oracle} $\mathcal{O}$ takes as input a string $s \in \Sigma^{*}$ and returns a sample from a next-token distribution $\mathcal{O}(s): \Sigma \to \mathbb{R}^+$. 
     
     We will denote the corresponding joint distribution over strings $s \in \Sigma^*$ as $p_{\mathcal{O}}:\Sigma^* \to \mathbb{R}^+$. 
     Correspondingly, $\forall s \in \Sigma^{*}$, let $p_{\mathcal{O}}(\cdot \mid s)$ denote the distribution over completions of $s$ predicted by $\mathcal{O}$.
\end{definition}
% \yuchen{ (addressed)
% The above definition does not reflect the ``autoregressiveness".
% In particular, to be compatible with \Cref{def:oracle_complexity},
% each call to $\mathcal{O}$ has to generate one single token, instead of a whole string.
% }
\begin{definition}[Constrained generation]
\label{theory:constrained_gen_def}
    \textbf{Constrained generation} with respect to an oracle $\mathcal{O}$, a constraint set $A$, and vocabulary $\Sigma$ is the task of producing an element $s \in A \subseteq \Sigma^{*}$ such that  $p_{\mathcal{O}}(s) > 0$. If no such $s$ exists, the algorithm needs to output \texttt{FAIL}. %denotes the restriction of $p_{\mathcal{O}}(s) := p(s_1)p(s_2|s_1) \dots p(s_{|s|}|s_{1:|s|-1})$ to $A$, $p_{\mathcal{O}} |_A(s) := p_{\mathcal{O}}(s) \mathbf{1}_A(s)$
\end{definition}
When not clear from context, we will specify instances of this task by the triple $(\Sigma, A, \mathcal{O})$. Under suitable choices of the vocabulary $\Sigma$ and the target domain $A$, one recovers several language modeling tasks of theoretical and practical relevance as special cases of constrained generation. Specifically, our experiments consider the tasks of generating (i) valid strings under the Dyck grammar (Section \ref{sec:experiments:synthetic}) and (ii) valid test cases for a given Python functions (Section \ref{sec:experiments:codellama}), where the oracles return samples from an appropriately pretrained language model. We recover these tasks from Definition \ref{theory:constrained_gen_def} by setting:
\begin{itemize}
    \item (i) $\Sigma$ as the set of open and close parentheses, and $A$ as the set of valid sequences of given length.
    \item (ii) $\Sigma$ as a set of characters from the Unicode standard (possibly after tokenization) and $A$ as the set of strings that are valid test cases for an input function in the Python programming language.
    %\item \textbf{JSON mode}: If $\Sigma$ is the Unicode standard (possibly tokenized via e.g. byte-pair encoding \cite{sennrich2015neural}) and $A$ is the set of valid JSON strings.
    %\item \textbf{Lexical constraints}: If $\Sigma$ is the Unicode standard  and $A$ is the set of sentences satisfying some lexical constraints.
\end{itemize}
Note that this task is easier than the task of sampling according to the \emph{restricted distribution} $p(s) \propto \mathbf{1}(s \in A) p_{\mathcal{O}}(s)$, which asks that the relative weights of the strings $s \in A$ that are generated match the probabilities assigned by $p_{\mathcal{O}}$. However, in many settings---e.g., generating a proof of a mathematical problem, or code that performs some intended functionality---we merely care about producing one good sample.    

We will be considering ``process verifiers'' that take as input a prefix $s$, and output whether or not such a prefix can be completed to a string $s \circ s' \in A$. This is a natural formalization of a ``process reward'', as it assigns a belief to a partial generation. In the theoretical results (Section~\ref{s:hardness} and \ref{s:plusverifier}), we'll assume access to such an idealized verifier. In the empirical results (Section~\ref{sec:experiments}), such a verifier will be trained and will output a value between 0 and 1, which can be naturally interpreted as a probability that the prefix $s$ is completable to a string $s \circ s' \in A$.  
\begin{definition}[Process verifier]
\label{def:verifier}
    Given a constraint set $A$,
    a verifier is a function $V: \Sigma^{*} \to \{0,1\}$ such that 
    $\forall s \in \Sigma^*$,
    $V(s) = 1$ if and only if 
    $\exists s' \in \Sigma^*$ 
    such that $s \circ s' \in A$.
\end{definition}
% \yuchen{We need a definition for a prefix verifier, rather than only a full-sequence verifier}
% Using the notion of a verifier, constrained generation is alternatively formulated as sampling from the distribution over length $d$ strings
% \begin{align*}
%     p'(s) \propto p(s)V(s),
% \end{align*}
% where $V(s) = \mathbf{1}_A(x_{1:d})$. This is effectively a special case of a reward model [add citation] with binary rewards.
Designing algorithms given access to oracles which perform certain tasks, is a classical tool in computer science (this is the basis of Turing reductions in computational complexity), as well as optimization (e.g., zero-order optimization assumes a value oracle for a function, first-order optimization a gradient oracle, etc.) In the context of generative modeling, analyses based on oracle complexity have been carried out in the settings of diffusion models, where sampling algorithms rely on score oracles \cite{chen2022sampling}.

We will consider several natural algorithms that use an autoregressive oracle and a (process) verifier: 

\begin{definition}[Rejection sampling] 
\label{d:rejection}
Rejection sampling works by repeatedly generating a string $s$ according to $p_{\mathcal{O}}$, then running a verifier $V$ on the complete string---and accepting when the verifier outputs $V(s) = 1$.    
\end{definition}

Note, this algorithm only needs a verifier that decides the membership in $A$, rather than a process verifier. On the other hand, because the entire string needs to be generated first before being verified---the number of generations until the verifier accepts is likely very large.  

\begin{definition}[Tokenwise rejection sampling] 
\label{d:tokenwise}
Tokenwise rejection sampling works by generating a string one token at a time. To generate the next token $t$, given a prefix $s$, we sample $t \sim \mathcal{O}(s)$, 
% \yuchen{
% Slight mismatch in notation:
% In \Cref{theory:autoreg_gen_def},
% $\mathcal{O}(s)$ is next-token distribution,
% whereas $p_{\mathcal{O}}$ is completions.
% }
and run the process verifier on $V(s \circ t)$. We repeat this, until $V(s \circ t) = 1$, then proceed to the next token.  
\end{definition}

This algorithm requires a process verifier. However, since a partial string is accepted only if the process verifier accepts, the number of generations needed is likely to be smaller. In fact, we provide a very simple example in Section~\ref{s:plusverifier}.  

Finally, we consider a ``backtracking'' strategy, in which the model is allowed to erase some of its generations. The reasons to consider such a strategy is to allow the model to get ``unstuck'': if the process verifier decides the current prefix cannot be completed to a valid string in $A$, it is possible that erasing the last few tokens will make it easier for the model to correct its mistake, compared to erasing just the last token.
More formally, the framework of our algorithm is given by \Cref{alg:sampling_with_backtracking} below.~\footnote{
The algorithm is a bit more involved, so we will describe it in pseudocode rather than text.
Besides the notations in \Cref{sec:theory:setup},
\Cref{alg:sampling_with_backtracking} uses the following additional common conventions: 
\texttt{<eos>} denotes the end-of-sequence token;
$s_{\abs{s}} \ne \texttt{<eos>}$ is understood as True when $s = \eps$;
for any starting index $i$ and ending index $j$, 
if $i > j$,
then $s_{i:j} = \eps$.
In line 10, why redoing the erased positions using argmax:
our results in \Cref{sec:experiments:synthetic:dyck} suggests that \emph{out-of-distribution prefix} is a cause of generator mistakes.
As a remedy, redoing the erased positions using argmax is intended to increase the generator-predicted probability of the currently sampled prefix.
We include an ablation study in \Cref{sec:appendix:experiments:algo} verifying that this improves the accuracy.
}

\begin{algorithm}
\caption{\algoName}
\label{alg:sampling_with_backtracking}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Prompt $\prompt$, generator $\mathcal{O}$, verifier $\verifier$, length $D \in \N_+$, backtrack quota $\backtrackQuota \in \N$, backtrack stride $\backtrackStride \in \N_+$
\STATE $s \gets \epsilon$
\WHILE{$\abs{s} < D$ and $s_{\abs{s}} \ne \texttt{<eos>}$}
    \STATE Sample $\hat{s} \sim \mathcal{O}(x \circ s)$
    \STATE $s \gets s \circ \hat{s}$
    \IF{ $Q > 0$ and $\verifier(x \circ s) = 0$}
        \STATE $s \gets s_{1 : \abs{s} - B}$
        \STATE $Q \gets Q - 1$
        \FOR{i in $1 \cdots B$}
            \STATE Choose $\hat{s} \in \argmax \mathcal{O}(x \circ s)$
            \STATE $s \gets s \circ \hat{s}$
        \ENDFOR
    \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}


When arguing about lower bounds, a natural lower bound on the complexity of an algorithm is the number of oracle calls needed\footnote{In our case, the number of calls is a randomized quantity, so a natural quantity to consider is the expected number of oracle calls. It is of course reasonable to consider finer-grained notions like tail bounds on the number of calls.}, particularly so when this dominates the cost of the algorithm, as is frequently the case for language models: 
\begin{definition}[Oracle complexity]
    \label{def:oracle_complexity}
    Given a (possibly randomized) algorithm $\mathcal{A}$ that solves the constrained generation instance $(\Sigma, A, \mathcal{O})$, the \textbf{oracle complexity} of $\mathcal{A}$ is defined as the expected number of calls to the oracle made by $\mathcal{A}$ to solve $(\Sigma, A, \mathcal{O})$, namely:
    \begin{align*}
        \mathcal{C}(\mathcal{A}) = \mathbb{E}[\# \text{calls to} \ \mathcal{O} \ \text{made by running} \ \mathcal{A}],
    \end{align*}
    where the expectation is taken over the randomness of the oracle $\mathcal{O}$ and the randomness of the algorithm $\mathcal{A}$.
\end{definition}

%Specifically, this notion fits well the practice of language model sampling, where the runtime is dominated by the computation of the distribution over the next-token.~\footnote{
%See \Cref{sec:appendix:discussions} for additional discussions.
%} 

Finally, we recall the classical knapsack problem,  which will be used in a reduction to prove computational intractability results for the constrained generation task: 
\begin{definition}[Knapsack problem]
Given a set of weights $\{X_i \in \mathbb{Z}_{\geq 0} \mid i \in [D]\}$ and $c \in \mathbb{Z}_{\geq 0}$, the knapsack problem seeks an assignment of the variables $(a_i)_{i=1}^D$, with $a_i \in \{0,1\} \ \forall i \in [D]$ such that $c = \sum_{i=1}^{D} a_i X_i$.

%Further, we denote by $\Omega(\{X_i\}_{i \in [D]}, c)$ the set of all assignments $(a_i)_{i=1}^D$ solving the knapsack instance specified by $\{X_i\}_{i \in [D]}, c$.   
\label{d:knapsack}
\end{definition}

The problem is (weakly) NP-hard, even for some very special choices of $c, X_i$. 
% We restrict our further analysis to the class of backtracking algorithms, which we formalize in definition \ref{theory:backtracking_algos}.
% \begin{definition}
% [Backtracking algorithm]
% \label{theory:backtracking_algos}
%     An algorithm $\mathcal{A}$ algorithm that solves the constrained generation task $(\Sigma, A, \mathcal{O})$ is a \textbf{$k$-backtracking algorithm} if:
%     \begin{itemize}
%         \item $\mathcal{A}$ solves the task by iteratively constructing a sample $s$, initialized as the empty string. 
%         \item At each step, $\mathcal{A}$ either:
%         \begin{itemize}
%             \item Calls the oracle $\mathcal{O}$ on the current string and updates it by concatenation of the oracle's output: $s \leftarrow s \circ \mathcal{O}(s)$
%             \item Picks $b \leq \min(k, |s|)$ and removes ("backtracks") the last $b$ elements from the current string: $s \leftarrow s_{1:|s|-b}$
%         \end{itemize}
%     \end{itemize}
% \end{definition}
% We consider, as a baseline approach, rejection sampling. In rejection sampling, the procedure to generate a sequence is defined as in Algorithm \ref{alg:rej_sampling}.
% \begin{algorithm}
% \caption{Rejection Sampling}
% \label{alg:rej_sampling}
% \begin{algorithmic}[1]
% \STATE \textbf{Input:} Length $D$, domain $A$
% \STATE $s \gets \epsilon$
% \FOR{$t = 1$ to $D$}
% \STATE $s \gets s \circ \mathcal{O}(s)$
% \ENDFOR
% \IF{$s \in A$}
% \STATE \textbf{return} $s$
% \ELSE 
% \STATE Go to line 2.
% \ENDIF
% \end{algorithmic}
% \end{algorithm}

% Our second theoretical contribution is a separation result. We formalize the class of backtracking algorithms and show the existence of a backtracking algorithm attains lower oracle complexity that rejection sampling with as low as $k = 1$.

% \subsection{Optimal query complexity by dynamic programming}
% \label{sec:theory:dp}
% [Theorem. There is an algorithms that runs in time O($V^k$) and achieves the optimal runtime in the class of algorithms with backtracking. \ar{This isn't written up is it?}]
% \begin{lemma}
% \label{theory:complexity_rs}
%     Let $(\Sigma, A, \mathcal{O})$ be a constrained generation task instance with $A \subseteq \Sigma^D$ for some $D \in \mathbb{Z}_{+}$. The oracle complexity of rejection sampling for solving $(\Sigma, A, \mathcal{O})$ is
%     \begin{align}
%         \mathcal{C}(\text{Rej. Sampling}) = \frac{D}{\sum_{s \in A} p_{\mathcal{O}}(s)}
%     \end{align}
% \end{lemma}
% \begin{proof}
%     Let $S := \{ s \in A \}$ be the event that a string generated in lines 2-5 is accepted. This event occurs with probability
%     \begin{align*}
%     P(S) &= \mathbb{E}[\mathbf{1}_S] = \mathbb{E}[\mathbb{E}[\mathbf{1}_S|s_{1:D} = \hat{s}_{1:D}]] \\
%     &= \sum_{\hat{s} \in \Sigma^D} p_{\mathcal{O}}(\hat{s})P(\{ s \in A \}|s = \hat{s}) \\ 
%     &= \sum_{s \in A} p_{\mathcal{O}}(s)
%     \end{align*}
%     The number of times lines 2-5 are executed is a geometric random variable with parameter $P(S)$ and each execution of the loop involves $D$ oracle calls. Hence,
%     \begin{align*}
%         \mathcal{C}(\text{Rej. Sampling}) = \frac{D}{P(S)} = \frac{D}{\sum_{s \in A} p_{\mathcal{O}}(s)}
%     \end{align*}
% \end{proof}

% \begin{theorem}
%     For any constrained generation instance satisfying the assumptions of Lemma \ref{theory:complexity_rs}, there exists a $1$-backtracking algorithm $\mathcal{A}_{1}$ such that \ar{Can we not say anything quantitative other than just it's less?}
%     \begin{align*}
%         \mathcal{C}(\mathcal{A}_1) \leq \mathcal{C}(\text{Rej. Sampling})
%     \end{align*}
% \end{theorem}
% \begin{proof}
% We consider Algorithm \ref{theory:algo_1_backtracking}, a $1$-backtracking algorithm with access to the verifier $V(s) := P(s' \in A | s'_{1:\mid s \mid} =  s)$ that, when called on $s \in \Sigma^{*}$ returns the probability of completing $s$ to a string in $A$ by repeatedly invoking the oracle $\mathcal{O}$. The algorithm backtracks with probability given by the output of the verifier on the current string. 
% Consider now an algorithm that is allowed to backtrack by one token at each step. The procedure to generate a sequence is now as follows. \\
% \begin{algorithm}
% \label{theory:algo_1_backtracking}
% \caption{$\mathcal{A}_1$: Sampling Algorithm with Backtracking}
% \label{alg:sampling}
% \begin{algorithmic}[1]
% \STATE \textbf{Input:} Length $D$, domain $A$
% \STATE $s \gets \epsilon$
% \FOR{$t = 1$ to $d$}
%     \STATE Sample $\hat{s} \gets \mathcal{O}(s)$
%     \STATE Sample $U \sim Uniform(0,1)$
%     \IF{ $U < P(s' \in A \mid s'_{1:t} = s \circ \hat{s})$}
%         \STATE $s \gets s \circ \hat{s}$
%     \ELSE
%         \STATE Go to line 4.
%     \ENDIF
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}

% Given current string $s$, the probability of adding any token and subsequently backtracking to $s$ is
% \begin{align*}
%     P(R | s) &= \sum_{\hat{s} \in \Sigma} p_{\mathcal{O}}(\hat{s}|s) P(s' \notin A | s'_{1:t} = s \circ \hat{s}) \\
%     &= P(s' \notin A | s'_{1:t-1} = s_{1:t-1})
% \end{align*}

% and the probability of appending token $\hat{s}$ is 
% \begin{align*}
%     P(s\hat{s} \mid s) &= \sum_{n=0}^{\infty} p_{\mathcal{O}}(\hat{s}\mid s)P(s' \in A \mid s'_{1:t} = s \circ \hat{s}) P(R \mid \hat{s})^n \\
%     &= p_{\mathcal{O}}(\hat{s} \mid s)P(s' \in A \mid s'_{1:t} = s \circ \hat{s})\frac{1}{1-P(R \mid \hat{s})} \\
%     &= p_{\mathcal{O}}(\hat{s} \mid s)\frac{P(s' \in A\mid s'_{1:t} = s \circ \hat{s})}{P(s' \in A \mid s'_{1:t-1} = s)} \\
%     &\propto p_{\mathcal{O}}(\hat{s} \mid s)P(s' \in A \mid s'_{1:t} = s \circ \hat{s}).
% \end{align*}

% The expected number of oracle calls starting from a nonempty string $s$ satisfies the recursion $\forall t \in [1,d-1], \forall s \in \Sigma^t$,
% \begin{align*}
%     &\mathbb{E}[\#calls \mid s] \\ &= \frac{1 + \sum_{\hat{s} \in \Sigma} p_{\mathcal{O}}(\hat{s} \mid s)P(s' \in A \mid s'_{1:t+1} = s \hat{s}) \mathbb{E}[\#calls | s \circ \hat{s}]}{P(s' \in A \mid s_{1:t} = s)} \\
% \end{align*}
% and by unrolling [Edoardo: Add full derivation to the appendix?] the recursion we obtain the expected number of calls from an empty string
% \begin{align*}
%     &\mathcal{C}(\mathcal{A}_1) \\ &= \frac{1 + \sum_{t' \in [1,\dots,D-1]} \sum_{(s_1 \in \mathcal{F}(\emptyset), \dots, s_{t'} \in \mathcal{F}(s_{1:t'-1})} p_{\mathcal{O}}(s_{1}\dots s_{t'})}{P(S)},
% \end{align*}
% where $\forall s \in \Sigma^{*} \cup \{ \epsilon \}$, we define
% \begin{align*}
%     \mathcal{F}(s) := \{\hat{s} \in \Sigma : P(s' \in A \mid s'_{1:|s|+1} = s \circ \hat{s})\}
% \end{align*}
% as the set of elements from the vocabulary $\Sigma$ that, when concatenated to $s$, result in the $|s|+1$ long prefix of any of the string in $A$.

% By virtue of $p$ being a probability distribution, the numerator is bounded by $D$. By putting together this expression with Lemma \ref{theory:complexity_rs}, we get
% \begin{align*}
%     \frac{\mathcal{C}(\mathcal{A}_1)}{\mathcal{C}(\text{Rej. Sampling})} \leq 1.
% \end{align*}
% \end{proof}

% The proof further sheds light on the source of the separation in the complexity between algorithm $\mathcal{A}_1$ and rejection sampling, which lies in the mass assigned by $p_{\mathcal{O}}$ to the sets $\mathcal{F}(\cdot)$.
\section{Constrained generation is hard without a verifier}
\label{s:hardness}

First, we show that the constrained generation task (Definition~\ref{theory:constrained_gen_def}), without access to a process verifier can be intractable---even if the constraint set $A$ is extremely simple (e.g. the parity of a binary string). 

The source of intractability can be \emph{information-theoretic}: namely, if the oracle does not have a succinct description, the algorithm may need to query it prohibitively many times to identify what oracle it's interacting with. We view this as a plausible obstruction in practice as well: language models frequently behave unpredictably ``in-the-tails'', which becomes increasingly more likely when generating long strings. Thus, to inspect the behavior of the model on long strings, many queries are needed. 

The source of the intractability can also be \emph{computational}: namely, even if the oracle is very simple (e.g., a uniform distribution), generating a member of $A$ can be NP-hard, even if checking membership in $A$ can be done efficiently. Perhaps this should not come as a surprise: after all, easy verification of membership, but hard generation, is the hallmark of NP-hard problems.     

Proceeding to the first result, we show the following: 

\begin{theorem} 
\label{thm:info_theoretical_lower_bound}
There exists a constrained generation task $(\Sigma, A, \mathcal{O})$ for which $\Sigma = \{0,1\}$, $A \subseteq \Sigma^D$, and $\mathcal{O}$ is an (unknown) member of a set of $2^{D-1}$ possible oracles, such that any (possibly randomized) algorithm $\mathcal{A}$ has an (expected) oracle complexity of at least $2^{D-1}$. 
\end{theorem}
Intuitively, the lower bound is shown by engineering a scenario such that the behavior of the oracle on long strings is unknown to the algorithm---but success of the generation task relies on ``guessing'' this behavior correctly.  
%The proof argument considers a family of oracles indexed by strings of length $D-1$, which sample digits uniformly except for the last position and the unique digit that would solve the task is sampled if and only if the current $D-1$ long prefix matches the string indexing the oracle $\hat{s}$. 

% TODO: proof moved to appendix, can move back later
% The proof is in \Cref{sec:appendix:proof:info_theoretical_lower_bound}.
\begin{proof}
    Consider the constrained generation task $(\Sigma, A, \mathcal{O}_{\hat{s}})$, such that $\Sigma := \{0,1\}$, $A := \{s \in \Sigma^D: \sum_{i=1}^{D} s_i \mod 2 = 0 \}$ for some fixed $D \in \mathbb{Z}_{+}$. Moreover, the oracle $\mathcal{O}_{\hat{s}}$ 
    is indexed by an (unknown to the algorithm) $\hat{s} \in \Sigma^{D-1}$, and it specifies the autoregressive distribution defined s.t.
    $\forall s \in \Sigma^*, |s| < D-1$, we have $p_{\mathcal{O}_{\hat{s}}}(1 | s) = p_{\mathcal{O}_{\hat{s}}}(0 | s) = 1/2$; while for $s \in \Sigma^*, |s| = D-1$, it satisfies:    
    
    $\forall s \neq \hat{s} \in \Sigma^{D-1},  s_D \in \{0,1\}$, we have:
    \begin{align}
    \label{eq:not_s_hat}
    p_{\mathcal{O}_{\hat{s}}}(s_D \mid s) &=
    \begin{cases}
        1, \ \text{if} \ \left( \sum_{j=1}^{D-1} s_j + s_D \right) \mod 2 = 1 \\
        0, \ \text{otherwise}
    \end{cases}
    \end{align}
    
    For $s = \hat{s},  s_D \in \{0,1\}$, we have:    
    \begin{align}
    \label{eq:is_s_hat}
    p_{\mathcal{O}_{\hat{s}}}(s_D \mid s) &= 
    \begin{cases}
        1, \ \text{if} \ \left( \sum_{j=1}^{D-1} s_j + s_D \right) \mod 2 = 0 \\
        0, \ \text{otherwise}
    \end{cases}
    \end{align}
    
     
    
    Suppose first that the algorithm is deterministic, and we choose the prefix $\hat{s}$ uniformly at random. Let us denote by $x_1, x_2, x_3, \dots, x_{q} \in \Sigma^*$ the queries to $\mathcal{O}$ generated by the algorithm. The claim is that expected number of queries $q$ needed to ensure at least one $x_i, i \in [q]$ is in $A$ is $2^{D-1}$. Indeed, the $x_i$ s.t. $|x_i| < D-1$ reveal no information about $\hat{s}$: the output of $\mathcal{O}$ is a uniform Bernoulli random variable regardless of the value of $\hat{s}$. On the other hand, if at some point the algorithm has queried a set $S$ of $x_i$ of length $D-1$, the probability over $\hat{s}$ is uniform over $\Sigma^{D-1} \setminus S$.  Hence, the expected number of queries $q$ (expectation being over the choice of $\hat{s}$) a deterministic algorithm needs is lower bounded by $2^{n-1}$. 

    By Yao's minimax lemma \citep{yao1977probabilistic}, this means that for any (even possibly randomized) algorithm $\mathcal{A}$, there exists $\hat{s}$ on which the algorithm makes at least $2^{n-1}$ queries in expectation.  
    %By way of contradiction, suppose that an algorithm with less than $2^{D-1}$ queries succeeds in solving the constrained generation task [Andrej: Is task defined to generate a *single* sample?]. Suppose the queries generated by the algorithm are $x_1, x_2, x_3, \dots, x_{q} \in \Sigma^*$, for $q \leq 2^{D-1}$. 
    %The total number of distinct prefixes of length $D-1$ queried by the algorithm is thus less than $2^{D-1}$. Thus, there exists a $y \in \Sigma^{D-1}, \mbox{s.t. } \forall i \in [q], y \neq x_i$ [Andrej: this is a bit handwavy --- the lengths of $x_i$'s are arbitrary. What do we do with the shorter queries?]. Then, if the algorithm was interacting with oracle $\mathcal{O}_y$, it would    
    %The task effectively reduces to sampling from a point-mass centered at $\hat{s}$. In the worst case where $\hat{s}$ is chosen adversarially based on the $D-1$ long prefix produced by the generation algorithm, all possible $D-1$ long prefixes are explored before finding the matching one, requiring $2^{D-1}$ queries to the oracles are required   
\end{proof}

%\subsection{Reduction from the Knapsack problem}
%\label{sec:theory:knapsack}
%Our first theoretical contribution is a computational lower bound on the oracle complexity of constrained generation. By a reduction argument, our proof shows that constrained generation from an arbitrary known oracle is at least as hard as modular knapsack. \ar{Compare the two lower bounds: in one the hardness comes from not knowing what the oracle does on long strings; the other from the interaction of $\Sigma$ and $A$}

\begin{remark}
    To help readers parse our proof above, we provide its informal intuition.
    The oracle $\mathcal{O}_{\hat{s}}$ can be thought of as hiding a secret message $\hat{s}$ which is a binary sequence of length $D-1$.
    Because of our construction in \Cref{eq:not_s_hat},
    by querying the oracle with any string $s \ne \hat{s}$, 
    the output of the oracle will not reveal any information about $\hat{s}$.
    Therefore, to know anything about $\hat{s}$, 
    the query $s$ needs to exactly match $\hat{s}$.
    For any deterministic order of searching for $\hat{s}$ over $\{0,1\}^{D-1}$, 
    the worst case is always that $\hat{s}$ is the last item in the search order, causing runtime $2^{D-1}$.
    
    Moreover, \Cref{thm:info_theoretical_lower_bound} generally applies to any algorithm $\mathcal{A}$. 
    In particular, $\mathcal{A}$ is even allowed to \emph{never} query the generator oracle at all.
    Intuitively, one candidate counter-example of our theory would be 
    a simple algorithm $\mathcal{A}$ which always outputs 0 at all positions
    (as this will obviously satisfy the counstraint $A := \{s \in \Sigma^D: \sum_{i=1}^{D} s_i \mod 2 = 0 \}$).
    However, recall that \Cref{theory:constrained_gen_def} requires that 
    the output must have nonzero probability under the generator oracle $\mathcal{O}_{\hat{s}}$.
    Note that $s_i = 0 \; \forall i \in [D]$ will not have nonzero probability under $\mathcal{O}_{\hat{s}}$, 
    thus violating the constraints
    (unless $\hat{s}_i = 0 \; \forall i \in [D-1]$ ).
    The intuitive reason why the above counter-example does not work is that, 
    it is necessary to use the oracle $\mathcal{O}_{\hat{s}}$ to know the $\hat{s}$ that it hides. 
    Therefore, any algorithm $\mathcal{A}$ is governed by the above-mentioned lower bound of searching for $\hat{s}$ by querying $\mathcal{O}_{\hat{s}}$.
\end{remark}

Proceeding to the computational lower bound, the theorem we show is as follows:
% (proof is in \Cref{sec:appendix:proof:computational_lower_bound}) 

\begin{theorem}
\label{thm:computational_lower_bound}
There exists a constrained generation task $(\Sigma, A, \mathcal{O})$ for which $\Sigma = \{0,1\}$, membership in $A \subseteq \Sigma^D$ can be checked in time polynomial in $D$, and $\mathcal{O}$ is such that $\forall s \in \{0,1\}^D, p_{\mathcal{O}}(s) > 0$, the generation task is NP-hard. \end{theorem}

% TODO: proof moved to appendix, can move back later
\begin{proof}
    We construct a reduction from the knapsack problem (Definition~\ref{d:knapsack}). Let the set $\{X_1, \ldots, X_D\}$ and the integer $c$ specify an arbitrary instance of the knapsack problem. Consider
    the constrained generation task specified by $\Sigma := \{ 0, 1\}$, $A := \{ s \in \Sigma^D: \forall i \in [D], s_i \in \{ 0, 1\}; \ \sum_{i=1}^D s_i X_i = c\}$. Membership in this $A$ can be clearly verified in polynomial time.  %$\mathcal{O}$ be an oracle such that $p_{\mathcal{O}}(s) = 1/2^D, \forall s \in \Sigma^D$.
     Suppose we have a poly-time algorithm that generates a solution $\hat{s}$ to $(\Sigma, A, \mathcal{O})$. Since $\forall s \in \Sigma^D, p_{\mathcal{O}}(s) > 0$, $\hat{s}$ provides a solution to the knapsack problem, as we needed.  
\iffalse satisfies
\begin{align}
\label{theory:mod_knapsack_constraint}
\sum_{i=1}^D \hat{s}_i \mod X_0 = c. 
\end{align}
Then, let 
\begin{align}
   a_i = \mathbf{1}_{\{ X_i \}}(\hat{s}_i) \ \forall i \in [D].
\end{align}
The assignment $(a_i)_{i=1}^D$ satisfies 
\begin{align}
    \sum_{i=1}^D a_i X_i = \sum_{i=1}^D \mathbf{1}_{\{ X_i \}}(\hat{x}_i) X_i = \sum_{i=1}^D \hat{s}_i
\end{align}
and, by equation \ref{theory:mod_knapsack_constraint}, it is a solution to modular knapsack.
Suppose, instead, that we have a poly-time algorithm to find $(a_i)_{i=1}^D$ satisfying modular knapsack. Let 
\begin{align}
\hat{s}_i = a_iX_i \ \forall i \in [D]. 
\end{align}
Then, $\hat{s}$ satisfies equation \ref{theory:mod_knapsack_constraint} and $\hat{s}_i \in \{0, X_i\} \ \forall i \in [D]$, which implies that it is a sample satisfying the constrained generation instance $(\Sigma, A, \mathcal{O})$.
\fi
\end{proof}


%\subsection{Information-theoretic (oracle) lower bound}
%The lower bound from section \ref{sec:theory:knapsack} considers an instance of constrained generation where the oracle is fully known and the source of hardness lies in the choice of $\Sigma$ and $A$. We now state a lower bound on the number of queries to the oracle $\mathcal{O}$ needed to solve the constrained generation task---even if the constraint set $A$ is very simple (specifically, binary strings with even number of ones). The setting we consider differs from the computational bound. The assumed input structure implies the source of hardness to be that the behavior of the oracle on long strings is now unknown to the algorithm.   

\section{Constrained generation with process verifier gets easier}
\label{s:plusverifier}

While pessimistic, the message of Section~\ref{s:hardness} agrees with recent developments in inference-time scaling: namely, many natural tasks of interest seem to require a verifier to be solved. 

First, we show that the simplest ``natural'' algorithm with a process verifier, tokenwise rejection sampling (Definition~\ref{d:tokenwise}), can be much more efficient (exponentially so) in terms of oracle complexity compared to the trivial baseline of rejection sampling (Definition~\ref{d:rejection}). 

\begin{proposition} 
\label{prop:easy_with_verifier}
Consider the constrained generation task $(\Sigma, A, \mathcal{O})$, s.t. $\Sigma = \{0,1\}$, $A = \{0^D\}$ and $\mathcal{O}$ is uniform over $\Sigma^D$. Then: 
\begin{enumerate}
    \item The expected oracle complexity of rejection sampling (Definition~\ref{d:rejection}) is $2^D D$.  
    \item The expected oracle complexity of tokenwise rejection sampling (Definition~\ref{d:tokenwise}) with a perfect process verifier is $2 D$.  
\end{enumerate}
\end{proposition}
% TODO: proof moved to appendix, can move back later
% The proof is in \Cref{sec:appendix:proof:easy_with_verifier}.
\begin{proof}
Both claims are straightforward. (1) follows as generating one guess for the string $s$ takes $D$ oracle calls. Moreover, the probability of the full string matching the only string in $A$ (i.e., $0^D$) is $1/2^D$. As the number of calls to generate $0^D$ is a geometric random variable, the expected number of full string generations is $2^D$. 

For (2), since $\mathcal{O}$ is uniform, at each token, the probability of drawing $0$ is $1/2$. Hence, the expected number of calls per coordinate needed is $2$ --- making the total number of expected calls for the entire string $2D$. 
\end{proof}

This proposition underscores the power of a process verifier --- even in extremely simple settings, and even when used in conjunction with a very simple algorithm. 

In fact, one can easily see that with a perfect process verifier, one can easily solve the constrained generation task with $|\Sigma| D$ calls: at each position, one queries the process verifier for each possible continuation of the string, and accepts only if the process verifier accepts. Of course, in practice, the verifier is not perfect, and its accuracy likely depends on how ``out-of-distribution'' the prefix it's queried on is
(See \Cref{sec:experiments:synthetic:verifier_reduces_errors_unseen_ood} and \Cref{sec:experiments:codellama:ood})

We finally remark that a process verifier, as we defined it, is clearly useful to solve the generation task. If we instead wanted to sample from the restricted distribution $p(s) \propto \mathbf{1}(s \in A)p_{\mathcal{O}}(s)$, it's not clear how useful the process verifier is. For instance, if we use the simple tokenwise rejection sampling (Definition~\ref{d:tokenwise}), it's easy to see that the distribution we produce samples from is \emph{not} the restricted distribution:
% (and proof is in \Cref{sec:appendix:proof:calibration_is_hard}): 

\begin{proposition}
\label{prop:calibration_is_hard}
Consider the constrained generation task $(\Sigma, A, \mathcal{O})$, s.t. $\Sigma = \{0,1\}$, $A = \{s \in \Sigma^D: \exists i \in [D], s_i = 0\}$ and $\mathcal{O}$ is uniform over $\Sigma^D$. Then, tokenwise rejection sampling does \emph{not} produce samples from $p(s) \propto \mathbf{1}(s \in A)p_{\mathcal{O}}(s)$.      
\end{proposition}

% TODO: proof moved to appendix, can move back later
\begin{proof}
By Definition~\ref{d:tokenwise}, until the last token is being generated, the process verifier will always accept (as there exists a string with at least one 0 coordinate in the coordinates that haven't yet been sampled). Now, for the prefix $1^{D-1}$, the only completion that is in $A$ is $1^{D-1} \circ 0$. This means that $1^{D-1} \circ 0$ is assigned probability mass $\frac{1}{2^{D-1}}$ under the tokenwise rejection sampling schema. All other strings in $\Sigma^D$ are assigned a probability $\frac{1}{2^D}$. On the other hand, $p(s) \propto \mathbf{1}(s \in A)p_{\mathcal{O}}(s)$ assigns uniform mass on all strings in $A$ --- proving the claim of the proposition.     
\end{proof}

