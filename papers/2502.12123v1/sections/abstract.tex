\begin{abstract}

Recently, a plethora of works have proposed inference-time algorithms (e.g. best-of-n), 
which incorporate verifiers to assist the generation process.
Their quality-efficiency trade-offs have been empirically benchmarked on a variety of constrained generation tasks,
but the algorithmic design landscape is still largely poorly understood. In this paper, we develop a mathematical framework for reasoning about constrained generation using a pre-trained language model generator oracle and a process verifier---which can decide whether a prefix can be extended to a string which satisfies the constraints of choice. We show that even in very simple settings, access to a verifier can render an intractable problem (information-theoretically or computationally) to a tractable one. In fact, we show even simple algorithms, like tokenwise rejection sampling, can enjoy significant benefits from access to a verifier. Empirically, we show that a natural modification of tokenwise rejection sampling, in which the sampler is allowed to ``backtrack'' (i.e., erase the final few generated tokens) has robust and substantive benefits over natural baselines (e.g. (blockwise) rejection sampling, nucleus sampling)---both in terms of computational efficiency, accuracy and diversity. 
%Under our mathematical framework,  
%the constraints are specified by a formal language,
%and the quality of a generation is measured by whether the generation belongs to the target formal language,
%while efficiency is defined as the number of queries to the generator oracle.
%The verifier takes a prefix of the generation as input, and either accepts or rejects this prefix.
%Our theory studies the limits of quality-efficiency trade-offs from computational and information theoretical perspectives.
%Empirically, we test these verifier-assisted language generation algorithms using rule-based or trained verifiers in synthetic and realistic settings.




% Outline:

% 1. Improving inference time algorithm for accuracy or efficiency 

% 2. Plethora of strategies e.g. rejection sampling / best of n 

% 3. Lack understanding what roles each components play e.g. verifier 

% 4. In this paper, clean setting with pretrained LM as oracle, verifier 

% 5. Set up math framework, quality of generation is measured by generation belongs to a formal language, and verifier checks whether the generation is a valid string in the language 

% 6. Limits of what is possible, computationally or info theoretically 

% 7. empirically, when verifier is harder to write down, but a trained model, we test ... 


% Cyril suggestion, iteration 1:

% In the rapidly-evolving paradigm of \emph{inference-time scaling}, autoregressive language models are augmented with auxiliary verifiers (e.g. reward models, syntactic parsers, code executors) , which guide enhanced decoding strategies such as best-of-$N$ sampling. These hybrid algorithms have unlocked major improvements in model reliability and advanced reasoning capabilities. This work initiates the mathematical study of the computational complexity of verifier-assisted language generation. We formalize a simple computational model and archetypal task (verifier-conditioned generation). Even when the verifier is realizable by a finite-state automaton, this framework subsumes several timely examples. We construct computational and information-theoretic lower bounds, and show the benefits of a dynamic programming-based inference strategy under a fixed “backtracking” budget. Empirically, we test these verifier-assisted language generation algorithms using rule-based or trained verifiers in synthetic and code generation settings.




\end{abstract}