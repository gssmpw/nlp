\section{Related Work}
\noindent This section considers four types of recommendation methods, i.e., Sequential Recommendation, Graph-based Sequential Recommendation, Prompting-based Recommendation, and Lightweight Recommendation.


\subsection{Sequential Recommendation}
\noindent \acf{SRs} aim to capture the evolution of users' behavioral preferences by modeling their interaction sequences \cite{seq23wwwmlp}. Early explorations on Sequential Recommendation (SR) mainly focus on \ac{RNN}-based \cite{hidasi2016srnn,quadrana2017hrnn,seq22icdmrnn} structures. For example, Hidasi et al. \cite{hidasi2016srnn} are the first group that exploits RNN to measure user preferences in dynamic sequences. Quadrana et al. \cite{quadrana2017hrnn} propose a hierarchical structure to relay and evolve the hidden states, which achieves better performance on \ac{SR} tasks. However, these works meet gradient vanishing problems while considering the item associations from long sequences \cite{HuangW0023}.
A practical solution is to leverage Generative Adversarial Networks (GANs) \cite{seq21dasfaagan} to maintain long-term memory and overcome the gradient block problem. GANs are good at learning complex data distributions, enabling the creation of high-quality synthetic data for sequential recommendation tasks \cite{NiZWHQ23}. However, a significant drawback of GANs is their training instability, which can lead to mode collapse and difficulty in convergence. Then, some scholars consider utilizing contrastive learning \cite{YangHXHLL23, WangMCZLM23} to improve the quality of long-term sequential preferences. By contrasting similar and dissimilar instances, this strategy encourages the model to capture the underlying structure and relationships within sequences \cite{TianHZ0Z23}. Another popular solution is the Transformer \cite{LaiCY0C023,ZhangWC023}, which significantly improves the accuracy and robustness of sequential recommendation via the self-attention mechanism. Such a component allows the Transformer to capture long-range dependencies in sequences, enabling better performance on tasks requiring context understanding \cite{HeHSLJC18}. These deep learning-based networks achieve great success in \ac{SR} tasks but are limited in excavating structural information inside the sequential transitions.

\subsection{Graph-based Sequential Recommendation}
\noindent As one of the most popular solutions for Sequential Recommendation (SR) tasks, Graph-based Sequential Recommender systems (GSRs) have demonstrated efficacy in extracting structural information from complicated user-item interactions by constructing sequential graphs \cite{ZhangLXXLHCM22, ChangGZHNSJ021}. For example, PTGCN \cite{HuangMLDWL23} captures sequential patterns and temporal dynamics of user-item interactions by incorporating a position-enhanced and time-aware graph convolutional operator. It simultaneously learns the dynamic representations of users and items on a bipartite graph, utilizing a self-attention aggregator. RetaGNN \cite{HsuL21} is a relational attentive GNN that operates on local subgraphs derived from user-item pairs. It distinguishes itself by allocating learnable weight matrices to the various relationships between users, items, and attributes, instead of applying them to nodes or edges.
However, these models are limited when handling large-scale graphs. 
To address the above limitation, researchers start using more complicated structures (such as Hypergraphs \cite{xia_self_AAAI_2021} or Graph Contrastive Learning \cite{WuWF0CLX21}).
For example, Wu et al. \cite{WuWF0CLX21} propose a contrastive learning-based method that supplements the traditional sequential recommendation tasks with an auxiliary self-supervised task, reinforcing node representation learning via self-discrimination.
Xia et al. \cite{xia_self_AAAI_2021} integrate hypergraph and contrastive learning to capture the beyond-pairwise relations on sequential graphs. By exploiting a dual-channel hypergraph convolution network, it successfully models the complex high-order information among items. 
These GSRs achieve great success in SR tasks but have difficulty capturing global correlations among nodes from sequential graphs. To this end, scholars have started incorporating Transformer with GCNs \cite{XiaHXP23, LiXRY0023}. For example, Xia et al. \cite{XiaHXP23} introduce a temporal graph transformer to concurrently capture varying short-term and long-range user-item interaction patterns on sequential graphs. Nevertheless, the global weighting component (i.e., the self-attention mechanism) in Graph Transformer has quadratic computational complexity, hindering the deployment of GSRs on resource-constrained edge devices \cite{lea23dasfaa}.

\subsection{Lightweight Recommendation}
\noindent Lightweight recommendation algorithms aim to provide efficient and accurate recommendations by leveraging various techniques to reduce computational complexity and storage requirements, making them suitable for real-world applications with constrained resources \cite{XvLL0LH22,ZhouYXBW23,KnyazevO23}. There are some common strategies, including matrix factorization techniques like Singular Value Decomposition (SVD) \cite{kalman1996singularly} and neighborhood-based algorithms like k-nearest neighbors (KNN). These shallow traditional machine learning methods have been proven effective in the early exploration of dimensionality reduction. However, with the advancement of deep neural networks and the increasing complexity of problems, traditional lightweight methods have become less adaptive. Nonetheless, these concepts continue to guide current explorations \cite{ShiLWZ23}. Existing studies on \ac{LR} can be classified into two categories: 
1) The one aims to reduce computational complexity. By refining algorithms or simplifying model structures, this strategy directly alleviates the scale of parameters \cite{lea23dasfaa,ZhouYXBW23}. 
For example, Zhou et al. \cite{ZhouYXBW23} proposes a novel lightweight matrix factorization for recommendations that deploys shared gradients training on local networks, serving as a two-phase solution to protect the security of users’ data and reduce the dimension of the items. 
Lian et al. \cite{LianWLLC020} provide a novel solution to refine the backbone network, which employs an indirect coding approach. It reduces the computational cost of representation learning by maintaining a coding dictionary.
2) The other aims to remove unnecessary components \cite{MeiZK22}. This strategy can also effectively release the parameter scale but requires verifying the importance of components \cite{YanLZW22,KnyazevO23,MiaoLY22}. For example, Yan et al. \cite{YanLZW22} exploit bidirectional bijection relation-type modeling to enable scalability for large graphs. This method removes the constraints of negative sampling on knowledge graphs, simplifying the computational complexity. 
Miao et al. \cite{MiaoLY22} remove the transmission structure between social and interactive graphs in traditional social recommendation tasks. They fuse the social relationships and interactions into a unified heterogeneous graph to encode high-order collaborative signals explicitly, significantly reducing the computational complexity.  
Although lightweight concepts become popular in traditional recommendation tasks, research on lightweight sequential recommender systems (especially on GSRs) remains largely unexplored \cite{LiCZY21}.

\subsection{Prompt-based Recommendation}
\noindent Prompt-tuning paradigm is initially proposed in Natural Language Processing (NLP), which adapts the Pre-trained Language Models (PLMs) to the specific downstream tasks (especially in few-shot scenarios) \cite{LiuJFTDY022,BrownMRSKDNSSAA20}.
One research direction is to use rigid prompting (token-level) templates, which involve manually designing prompts and splicing them into token sequences \cite{GuHLH22}. In the domain of recommendation systems, numerous studies have employed rigid templates for the application of prompt-based learning. The methods based on rigid prompting templates usually convert the input features to natural language sequences \cite{Geng0FGZ22, ShinRLWS20}. Then, they construct the rigid templates by concatenating the input features with language prompts (e.g., descriptions of the downstream tasks). By recasting recommendations as cloze-style mask-prediction tasks, these methods could stimulate the potentials of large pre-trained models, thereby achieving better performance on recommendation tasks \cite{ZhangW23}.
In contrast, soft prompting (embedding-level) templates consist of randomly initialized learnable continuous embeddings, frequently adopted in recent studies \cite{WuXZZZLH24,BrownMRSKDNSSAA20, ZhangPrompt2023}. For instance, Wu et al. \cite{WuXZZZLH24} build personalized soft prompts by mapping user profiles to embeddings and enabling sufficient masked training on prompting templates via prompt-oriented contrastive learning.
As we know, the core idea of prompt-tuning involves creating appropriate prompts that can guide the pretrained model to generate desired predictions \cite{Geng0FGZ22}. Inspired by this idea, scholars are currently delving into the methodology of leveraging “prompts” to augment the representation learning capabilities of traditional deep learning models \cite{dongPrompt2024, Luoprompt2023}. For example, Luo et al. \cite{Luoprompt2023} treat the embeddings of timestamp as external prompts to unearth the potentials of a Transformer-based model. Note that, the purpose of using external prompts is to guide the model to generate outputs that meet specific requirements without altering the model itself \cite{Luoprompt2023} while the objective of prompt-tuning paradigm is to enhance the performance of pre-trained models by optimizing the input prompts, which involves indirect adjustments to the model's parameters \cite{dongPrompt2024}. In this work, we extract the positional information of items within each sequence as external prompts to enhance the sequence representation learning, thereby capturing the complicated positional dependencies of items within sequences.

\begin{table}[b]
\centering
\caption{The notations mainly used in this paper.}
\label{tab:notations}
\footnotesize
\begin{tabular}
{p{2cm}|p{8.5cm}}%{@{}ll@{}}
\toprule
\midrule
\textbf{Notations} &  \textbf{Descriptions} \\ 
\midrule
$\hat{\bm{E}}^{(l)}$ & The node embeddings on $l$-th layer. \\
$\hat{\bm{E}}_{U}^{(l)}$ & The embeddings of users on $l$-th layer. \\
$\hat{\bm{E}}_{S}^{(l)}$ & The embeddings of items on $l$-th layer. \\
$\bm{E}_{S_k}$ & The embeddings of sequence $S_k$. \\
$\bm{M_k}$ & The external unit acts as the key matrix in attention. \\
$\bm{M_v}$ & The external unit acts as the value matrix in attention.\\
$\bm{A}_{S}$ & The attention map of self-attention.\\
$\bm{A}_{L}$ & The attention map of linear attention.\\
$\bm{A}_{E}$ & The attention map of external attention.\\
$\bm{Z}_{S_k}$ & The embeddings of sequence $S_k$ refined by attention networks. \\
$\bm{\hat{Z}}_{S_k}$ & The embeddings of sequence $S_k$ refined by multi-head external attention. \\
$\bm{\hat{Z}}_{S}$ & The refined embeddings of items across all sequences. \\
$\bm{E}_{S}^{(l)}$ & The updated embeddings of items on $l$-th layer. \\
$\bm{E}_{U}$ & The final node embeddings for users. \\
$\bm{E}_{S}$ & The final node embeddings for items. \\
$\hat{\bm{E}}_{P_k}$ & The initialized positional prompting embeddings for $S_k$. \\
$\bm{E}_{P_k}$ & The prompting embeddings of $S_k$ that has same dimension with $E_{S_k}$. \\
$\bar{\bm{E}}_{X_k}$ & The embeddings of positional prompting template for $S_k$. \\
$\hat{\bm{E}}_{X_k}$ & The template embeddings for $S_k$ with sequential masks. \\
$\bm{E}_{X_k}$ & The sequence embeddings for $S_k$ refined by soft attention network. \\
$\bm{E}_{u_k}$ & The embeddings of user $u_k$ learned by the external attentive graph convolutional network.\\
$\bm{H}_{S_k}$ & The user-specific sequence-level embeddings for user $u_k$. \\
$\bm{H}_{S}$ & The final embeddings for prediction. \\
\midrule
\bottomrule
\end{tabular}
\end{table}