\section{Related Work}
\subsection{Commit Message Generation}
Several approaches have been proposed to automatically generate commit messages, which can be categorized as rule-based, retrieval-based, and learning-based approaches.
Rule-based approaches extract information from code changes and generate commit messages with predefined templates \cite{buse2010automatically, cortes2014automatically, linares2015changescribe, shen2016automatic}. For example, Cort√©s-Coy et al. \cite{cortes2014automatically} extracted the stereotype, type, and impact set of a commit, then filled predefined templates with extracted information to generate commit messages.
Retrieval-based approaches find the most similar code changes in the dataset and reuse their commit messages \cite{huang2017mining, liu2018neural, huang2020learning}. For example, Liu et al. \cite{liu2018neural} represented code change as bag-of-words vectors and then calculated the cosine similarity of them to find similar code changes.
Learning-based approaches design neural machine translation models to translate code changes into commit messages \cite{liu2020atom, nie2021coregen, wang2021context, dong2022fira, jung2021commitbert, shi2022race, he2023come, tao2024kadel, lin2023cct5, loyola2017neural, jiang2017automatically, xu2019commit, liu2019generating}.
For example,
Nie et al. \cite{nie2021coregen} pre-trained the transformer model to learn the contextualized representations of code changes, and then fine-tuned the model for downstream commit message generation.
Dong et al. \cite{dong2022fira} represented code changes with a fine-grained abstract syntax tree, and used graph neural networks to extract features and generate commit messages.

\subsection{LLMs for Software Engineering}
Large language models (LLMs) have garnered significant attention and adoption in both academic and industrial domains \cite{zhao2023survey}, including software engineering \cite{hou2023large, fan2023large}, due to their exceptional performance across a wide range of applications. 
For example, Geng et al. \cite{geng2024large} investigated the feasibility of utilizing LLMs to address multi-intent comment generation.
% Guo et al. \cite{guo2024exploring} explored the potential of ChatGPT in automated code refinement.
The most related work is Gao et al. \cite{gao2023makes}, which evaluated the capability of ICL using ChatGPT on code-related tasks such as bug fixing. However, code-related tasks naturally differ from code-change-related tasks like commit message generation.
Recently, several studies have evaluated LLM-based commit message generation \cite{eliseeva2023commit, tao2024kadel, zhang2024using, lopes2024commit, zhang2024automatic, wu2024commit}.
However, these studies primarily evaluated the performance of ChatGPT in a simple zero-shot setting with basic prompts. In contrast, our study selected six mainstream LLMs and explored their performance in complex few-shot settings with different prompt and demonstration designs. In addition, we conducted subjective evaluations and analyzed the root causes of underperformance.