\section{Dalla Preda and Giacobazzi}
\label{sec:abstract_interpretation}
Dalla Preda and Giacobazzi, with a variety of collaborators, have worked extensively on theoretical foundations to make obfuscations comparable, namely through the lens of \emph{abstract interpretation} (AI). In their view, attackers are limited in what they want to analyze and in their resources, so they model them as a specific abstraction. The attacker then coincides with the considered analyzer.

Discussing all the formal foundations of their work and presenting their formal definitions is not feasible in this paper, as it would require too long an introduction to abstract interpretation and theoretical results achieved in that domain. Unlike the definitions discussed in previous sections, their results in the form of mathematical proofs and equations are not intuitive and hard to interpret for non-experts.  We therefore summarize their main conclusions and how their work has evolved in an informal manner. 



\subsection{Definitions based on Completeness}
\label{sec:completeness}
Initially, the study of AI to define potency~\cite{cousot1977abstract,cousot2021principles} was based on the idea that the statement ``obfuscation makes programs incomprehensible for observers'' can be rephrased as ``obfuscation makes programs incomplete for abstract interpreters''~\cite{2008hiding}. The potency is therefore defined in terms of the \emph{completeness} of AIs with which attackers might try to reveal properties of a program that the defender is trying to prevent with obfuscations. 

An analysis of a program modeled as an AI is incomplete if the analysis produces an imprecise result. An imprecise result occurs \emph{when an analysis produces some valid result that is less precise than another valid result that the analysis can represent}. For example, consider a value set analysis that computes and propagates intervals of the form $[a,b]$ for any real values of $a$ and $b$ with $a \leq b$. If such an analysis reports that some variable in a program can hold values in the range $[0,10]$, while it can actually only hold values in the range $[1,5]$, the analysis has proven to be incomplete for that program. Importantly, incompleteness depends on both the program and the analysis. Some concrete analysis, and variations thereof, can be complete on one program and incomplete on another one. 

The mentioned authors define the potency of an obfuscation in terms of its impact on completeness: Which analyses become incomplete on which programs by deploying an obfuscation? 

Analyses modeled as AIs operate on abstract domains. The domains themselves consist of lattices, but all possible domains also form a lattice, meaning that they are ordered in terms of the relative precision of the properties that they can represent and potentially deduce from programs. And hence so are the analyses: Some can represent more precise properties than others, some can deduce more precise properties than others, and hence some analyses can deduce certain properties on more programs than other analyses.  

In older work~\cite{mila05a,mila05b,mila07}, an obfuscation is considered potent when there is a property that is not preserved, i.e., when some AI that could compute that property on the original program can no longer compute it after  obfuscation. Different obfuscations can then be compared based on the most precise properties they preserve. %The worst obfuscation is the one that preserves the most precise property. 

In 2012--2017, potency was redefined in terms of specific analyses that are used to reveal certain properties~\cite{2012_making,2017GMDP}. The potency of an obfuscation with respect to a specific analysis, program, and property is then determined by whether or not the completeness of the analysis on the program is impacted. If the analysis can no longer compute the property on the obfuscated program, the obfuscation is considered to be potent. Importantly, even if the analysis can still extract the property, the obfuscation might still be useful. Although the obfuscation then cannot counter that specific analysis, it might be able to counter simpler variants thereof. To assess this utility, the \emph{potency range} of an obfuscation was defined in terms of its impact on the completeness of compressed versions of the analysis, with ``compressed'' meaning ``less precise''.

In 2018, Bruni at al.~\cite{bruni2018code1,bruni2018code2} proposed an alternative method to assess obfuscations that aim to counter model checking attacks based on abstraction refinement, such as CEGAR~\cite{cegar}. In such attacks, a model checker iteratively tries to check the validity of models, starting with a very abstract domain that can lead to a conclusion quickly. When a conclusive result cannot be reached with the domain used in one iteration, a more precise refined domain is chosen, and a new attempt is made in a next iteration. The strength of an obfuscation is then defined as the extent to which it can prevent the checker from coming to a valid conclusion in fast early rounds with very AIs. Interestingly, the authors illustrate that many existing data flow analyses, such as liveness analysis, constant propagation, and available expression analysis, can be reformulated as model-checking problems, thus widening the applicability of this method. 

In follow-up research in 2022, Bruni et al.~\cite{bruni2022repair} present AI repair strategies to automatically refine domains to make the AI (locally) complete for a given program, with the goal of enabling program verification methods to start with any abstract domain. If the initial domain is too abstract to avoid false alarms, it can be refined with this strategy to produce complete results. The user of an AI then no longer needs to choose the domain beforehand to yield optimal formal verification results. In our eyes, this comes close to how reverse engineers adapt their strategies and analyses and how they move on to more complex ones if their initial, basic attempts fail. 

In 2022--2023, Campion et al.\ introduce the notion of $\epsilon$-partial incompleteness~\cite{Campion2022,campion23}. An abstract interpreter can be $\epsilon$-partial incomplete with respect to a given program and a given (set of) input values, meaning that the imprecision of the AI result is bounded by $\epsilon$, that is, the distance between the results of the abstraction of the concrete semantics (e.g., $[1,5]$ in the value set analysis example above) and the result of the AI on the given input (e.g., $[0,10]$) is at most $\epsilon$. For this, they define distance metrics on the abstract domains. They argue that the ability to quantify the amount of imprecision induced in the AI by an obfuscating transformation could be used to measure the potency of such a transformation.

Interestingly, they concede that, in general, one cannot automate the procedure of deciding whether the AI of a given program on a given input satisfies a given precision bound $\epsilon$. 
Still, the notions of $\epsilon$-partial incompleteness and bounded distance between actual properties and obtained analysis results can also open opportunities to reason about the strength of analyses that only have to compute approximate results (see Section~\ref{sec:collberg_nagra_def}). To the best of our knowledge, this direction has not yet been explored. 

In 2025, Giacobazzi and Ranzato~\cite{2025roberto} showed that the program property of having the best possible AI is not trivial and, in general, hard to achieve. Among others, they showed the impossibility of achieving the best correct abstraction property through minimal abstraction refinements or simplifications of the abstract domain. This puts into question the underlying assumption of some of the discussed definitions of potency that are based on which analysis can still reveal which properties. 

A strength of several works is that they not only allow to reason about the strength of obfuscations, but also include techniques to automatically derive obfuscations to counter the analyses defined in terms of AI~\cite{mila05a,mila05b,mila07,DP2013,DP2018,Roberto2012}. This includes obfuscations that target control flow analysis, data flow analysis, model checking, and more.

\subsection{Definitions based on Adequacy}
\label{sec:adequacy}
In 2023, Giacobazzi et al.~\cite{fitting_roberto} proposed to complement (and even replace) completeness as the basis for assessing potency with \emph{adequacy}. They observe that ``completeness characterizations do not really deal with the loss of precision due to the choice of the abstract observation, since they characterize only whether there is an extra loss of precision due to the computation on observed/abstracted data (compared with the observation of the concretely computed result).'' In other words, completeness is a measure of the mismatch between the computations in a given program and the chosen abstract domain (lattice) in which the program is interpreted, rather than measuring how good that domain is for revealing the properties in which an attacker/analyst is actually interested. 

Consider the trivial $\top$ abstraction that abstracts all concrete values to $\top$. Its lattice consists of one element $\top$. The result of such an AI on any program produces $\top$, which is by construction the most precise outcome that can be presented in the lattice, so such an AI is by construction complete. But it is completely useless: its result comes down to ``I don't know.'' In other words, the produced result is the least precise result that the attacker/analyst can be interested in, despite the interpretation being complete. 

Notice that such an element $\top$ is indispensable in many analyses: For an undecidable analysis to be sound, it needs to be capable of responding ``I don't know'', for which the used lattice includes $\top$.

The adequacy of an AI for a given program is then defined as the ability of the interpretation to produce analysis results for that program that contain strictly more information than $\top$. The relative adequacy of an AI for a program, i.e., adequacy with respect to some other element $\tau$ of the lattice, is then defined as the ability to produce a result that is strictly more precise than $\tau$.

The link with potency then is that an obfuscation can be considered potent with respect to some analysis and some program if it can make the analysis become inadequate on the obfuscated program while it was adequate on the original program. 

\subsection{Critique}

\subsubsection{Practical Applicabillity}
\label{sec:ai_practical}
While the use of AI theory to evaluate the strength of practical obfuscations against practical attacks has been discussed, such as obfuscations to mitigate disassemblers and slicing~\cite{2017GMDP}, it remains mostly a theoretical subject, of which the practical applicability is unclear. Although the AI work for the evaluation of model checking by Bruni et al.~\cite{bruni2018code1,bruni2018code2} might have the potential for wider applicability, a recent 571-paper literature review on evaluation methodologies in SP research~\cite{desutter2024evaluation} observes that software obfuscation and deobfuscation researchers do not have a strong appetite for this form of analysis: Model checking was one of the least popular attack methods, used in only 4\% of the papers that used concrete attack methods to evaluate obfuscations. 

\subsubsection{Attacks Success/Failure instead of Delay}
In the field of practical MATE SP, it is understood that MATE attacks cannot be completely prevented. Given enough time and effort, adversaries will always be able to get what they want. The main goals of obfuscation therefore are to delay attacks, to increase their costs, and to decrease their return-on-investment. In contrast, the existing research on AI and obfuscation focuses by and large on evaluating whether or not some property can be revealed with some interpretation/analysis and whether this can be prevented with an obfuscation. In other words, this research focuses on scenarios in which the defender tries to make attacks fail rather than trying to delay them. Although the complexity of different AIs is compared in some works to compare the strength of different obfuscations, the link between actual attack delay and analysis complexity is not made. This research hence seems to miss the point of MATE SP. 

\subsubsection{Soundness}
Research on AI and obfuscation only considers sound analyses. This is fine in many program analysis scenarios, such as program verification. In the MATE attack model, by contrast, attackers use any analysis, sound or unsound, that allows them to reach their goal. All dynamic analyses, e.g., are unsound. In this regard, this research again completely misses the point. 

\subsubsection{Program Understanding Only}
In 2008, Giacobazzi~\cite{2008hiding} stated ``The lack of completeness of the observer is therefore the corresponding of its poor understanding of program semantics''. This pinpoints an important limitation: This research, at least in the first order, targets software comprehension. It neglects the other MATE attack goals on obfuscated software. In addition to \emph{code comprehension}, Schrittwieser et al.~\cite{survey2016} identified \emph{finding the location of data}, \emph{finding the location of program functionality}, and \emph{extraction of code fragments} as important attacker targets. NC also claim that an adversary targeting an obfuscated program typically goes through a locate-alter-test cycle~\cite{collbergbook}. Neglecting location finding attacks is a clear limitation of AI research into the meaning of potency.

\subsubsection{Defender vs.\ Attacker Perspective}
\label{sec:perspective}
In much if not all of the cited research, the perspective of theory development is that of defenders that know which property of which program fragments they want to hide with obfuscations. The defenders hence know which analyses to consider for their assessment of the obfuscations' strength. Attackers, on the contrary, often do not know a priori which property they are after. In particular in data and functionality location finding attacks, the attackers by definition do not know the fragments of which they want to reveal properties.

Moreover, as defenders know which properties they want to hide, they can reason about the possible refinements of analyses that undo the impact of the chosen obfuscations. Attackers that try to find code or data or that try to understand a program do not know a priori which properties they are after. So how can they deploy similar refinement strategies? If they cannot, then why should defenders worry about the capabilities of refinement strategies? 

For most of its history, research on AI and obfuscation did not address the question of how difficult it is for an attacker to choose the best or simplest suitable AI for their attack. Only in 2025, Giacobazzi et al.\ started to address this issue, conceding that this may indeed be difficult~\cite{2025roberto}. This puts in doubt the relevance of using completeness or adequacy of the best possible abstractions for evaluating obfuscation strength. In our eyes, the defender should care mostly, if not only, about the analyses that an attacker will possibly or likely use, rather than on theoretically better, but in practice unlikely to be used, attack methods. 

An example might make this critique concrete. Consider interprocedural constant propagation~\cite{interp_cp}, and a scenario in which a context-insensitive variant of the analysis is not precise enough to reveal some interesting program property, while some $k$-depth context-sensitive variant is precise enough for some $k$. Undoubtedly, the context-sensitive variant, which will be slower and have a larger memory footprint, should be considered the most complex. There exist countless variations of the analysis with complexities in between those two variants, namely variations that analyze only specific parts of the program in a context-sensitive manner while handling the rest in a context-insensitive way. Intuitively, the variant that requires the least context-sensitivity should be considered the simplest one. Now, while the defender might know where context-sensitivity is useful and where not, hence being able to determine the simplest complete analysis for their program at hand, how is an attacker supposed to determine this variant? In particular when an attacker does not yet know the relevant properties to be revealed, this seems inconceivable. So what is the relevance of that best and simplest complete variant? 

One might think that attackers would go with the simplest or fastest analysis first and switch to more precise and slower ones only when the initial ones fail. So, first run, e.g., a context-insensitive analysis across the entire program (which should be cheap), then use whatever information they extracted to target a context-sensitive analysis to the most likely location of the program.

This is probably often how attackers work, but certainly not always. A counterexample is a crypto key extraction attack, for which Ceccato et al.\ performed experiments with professional pen testers~\cite{emse2019}. The most experienced among them skipped static analysis entirely, assuming that the deployed SPs would make that too hard, so they used only dynamic techniques from the start. In other words, they opted for a more complex analysis method over a simpler one, even though a simpler one would have worked perfectly well on an unobfuscated program.

In any case, even if the attackers do start with the simplest analyses and refine them as they obtain additional information, we think it is relevant to consider the effort required to extract that information and how precise it would need be, e.g., to pinpoint the most likely location in the program where more precision is needed. We should not assume that the refinement comes for free.

On this topic, there exists quite some work on on-demand data flow analysis or demand-drive data flow analysis. Such analyses are designed to be more efficient than blindly running the most complex analysis variants on a whole program. Those on-demand analyses, which adapt on-the-fly are more complex than analyses for which an a priori determination was made about which locations in the program require a higher form of sensitivity. So the consideration of such analyses does not solve the issue that AI research starts too much from the defender's perspective. 

The work by Bruni et al.\ cited above~\cite{bruni2022repair} is highly related. In that work, it is also assumed that the user will run a simple analysis first, and then iteratively run ever more refined versions thereof. All of that effort should be considered when evaluating potency or resilience, however, not just the complexity of the final analysis version. Maybe the journey is not more important than the destination, but it definitely is as important.
