\section{A New Framework}
The formulated critiques articulate that a better formulation of evaluation criteria for obfuscating transformations is needed to provide better guidance on how to assess their real strengths and weaknesses rather than reporting artificial, irrelevant, or even misguided proxies. The primary aim of such guidance is to help researchers maximize the impact of their work by convincing other researchers that they can build on this work for their own research and by convincing practitioners of the relevance of the work for real-world software protection. To a large degree, such impact depends on the validity of the performed evaluations. This includes the commonly used criteria of validity in software engineering, such as construct, conclusion, internal, and external validity~\cite{Wohlin} but also less commonly considered criteria such as instantiation validity~\cite{lukyanenko2014instantiation}. \emph{Reducing such threats to validity is the aim of this framework.} 

In line with many of the existing definitions discussed in Sections~\ref{sec:collberg_nagra_def},~\ref{sec:completeness}, and~\ref{sec:adequacy}; in line much of the examples mentioned in Section~\ref{sec:attack_tool_examples}; and in line with the recommendations of De Sutter et al.~\cite{desutter2024evaluation} that evaluations obfuscation strength evaluations should be based on their impact on real-world attacks, we put forward that the used evaluation criteria should be based on specific program properties that adversaries can target with specific attack strategies, and on the impact that the obfuscations have on those strategies. 

Unlike NC, we think that a concept similar to resilience needs to be a core criterion. And unlike the research on AI-based potency, we feel the criteria need to be practical, e.g., in the sense that they are consistent with the fact that MATE attackers can always reveal the properties they are after, and that the required effort of a whole attack strategy, not only of individual steps, is what matters. Unlike existing evaluation criteria defined in terms of analysis, we define them in terms of attack strategies, which consist of different categories of attacks steps being executed: 
\begin{enumerate}
\item knowledge gathering, i.e., revealing properties:
\begin{enumerate}
    \item revealing static/dynamic artifacts in the program;
    \item revealing relations between artifacts;
    \item revealing features of the artifacts and the relations;
    \item assigning priorities to artifacts and relations; 
    \item revealing mappings between abstract artifacts, properties, and relations; and concrete ones.
\end{enumerate}
\item artifact manipulation, such as lifting a code fragment from a program for ex situ execution, tampering with a SP to undo it, or altering its execution state to bypass a SP; 
\item decision making on the next steps to execute based on the determined priorities and already gathered knowledge.
\end{enumerate}
These categories capture the four goals of analyses in the survey by Schrittwieser et al.\ on obfuscation vs.\ program analysis~\cite{survey2016}, the activities modeled by the reverse engineering formalization by Faingnaert et al.~\cite{checkmate24}, the activities observed in experiments by Ceccato et al.~\cite{emse2019}, in a survey on the practice of malware analysis~\cite{wong2021inside}, etc. For example, code comprehension tasks, such as understanding that some code fragment implements a quicksort, are instantiated by combinations of (1a), (1b), (1c), and (1e). The same holds for making hypotheses, confirming them, and discarding them, which are important attack activities as observed by Ceccato et al.~\cite{emse2019}. Finding the location of relevant code and data in a program in addition requires (1d) to classify the revealed information into relevant vs.\ irrelevant ones. Undoing an obfuscation obviously requires (2). 

\emph{It is hence against combinations of these kinds of attack steps, and the possible methods to execute them in relevant attack strategies, that obfuscations can and should be evaluated.}

Importantly, the methods used to execute these steps can be unsound and non-conservative. For example, trace-based analysis techniques to reveal properties are unsound. Artifact manipulation may be non-conservative when the applied transformation does not conserve the behavior under all possible circumstances. When some hypothesis is being made by an adversary in some attack steps that are not confirmed in later steps, for example because the adversary reaches their goal first, the steps building on that hypothesis are obviously unsound as well. 

Given the wide range of properties that adversaries might be interested in, and the wide range of analyses techniques that can help them, it makes no sense to propose a one-size-fits-all set of evaluation metrics or to prescribe exactly what metrics should be measured. We hence limit ourselves to prescribing which criteria should be evaluated, aiming at defining them in ways that avoid the confusion that has existed regarding previous definitions. Our new framework of criteria includes relevance, effectiveness (and its poor man's alternative of efficacy), robustness, concealment, stubbornness, sensitivity, predictability, and cost. Table~\ref{tab:criteria} lists them all, including their subcriteria. 

\begin{table}[t]
    \centering
    \begin{tabular}{c p{6cm} l}
\hline
    \multicolumn{3}{l}{\textbf{Relevance}}\\
%    \midrule
       & Attack Steps Relevance  & $Re_{a}$ \\
       & Program Property Relevance & $Re_{p}$ \\
       & Obfuscation Impact Relevance & $Re_{o}$ \\
       & Metrics Relevance & $Re_{m}$ \\
       & Tool Availability:& $Re_{t}$ \\
       & Sample Relevance & $Re_{s}$\\
       & Layered Protection Relevance & $Re_{l}$\\
%   \midrule
    \multicolumn{3}{l}{\textbf{Effectiveness}}\\
   & Isolated Outcome Effectivness & $E_{o,i}$ \\
   & Marginal Outcome Effectivness & $E_{o,m}$ \\   
   & Isolated Resource Effectivness & $E_{r,i}$ \\
   & Marginal Resource Effectivness & $E_{r,m}$ \\
%      \midrule
    \multicolumn{3}{l}{\textbf{Robustness}}\\
     &  Isolated Outcome Delta & $Ro_{o,i}$ \\
     &  Marginal Outcome Delta & $Ro_{o,m}$ \\
     &  Isolated Resource Delta & $Ro_{r,i}$ \\
     &  Marginal Resource Delta & $Ro_{r,m}$ \\
     &  Deployment Delta & $Rd_{o,m}$ \\
%           \midrule
    \multicolumn{3}{l}{\textbf{Concealment}}\\
         &  Local Concealment & $C_{l}$ \\
         &  Global Concealment & $C_{g}$ \\
         &  Strategic Concealment & $C_{s}$ \\
    \multicolumn{3}{l}{\textbf{Stubbornness}}\\
         &  Outcome Stubbornness & $St_{o}$ \\
         &  Resource Stubbornness & $St_{d}$ \\
   \multicolumn{3}{l}{\textbf{Sensitivity}}\\
         &  Sample Feature Sensitivity & $Se_{s}$ \\
         &  Attack Instantiation Sensitivity & $Se_{a}$ \\       
         &  Protection Instantiation Sensitivity & $Se_{pi}$ \\       
         &  Protection Configuration Sensitivity & $Se_{pc}$ \\       
         &  Build Tool Flow Sensitivity & $Se_{b}$ \\                
         &  Platform Sensitivity & $Se_{p}$ \\ 
   \multicolumn{3}{l}{\textbf{Predictability}}\\
         &  Sample Feature Predictability & $P_{s}$ \\
         &  Attack Instantiation Predictability & $P_{a}$ \\       
         &  Protection Instantiation Predictability & $P_{pi}$ \\       
         &  Protection Configuration Predictability & $P_{pc}$ \\       
         &  Build Tool Flow Predictability & $P_{b}$ \\                
         &  Platform Predictability & $P_{p}$ \\ 
   \multicolumn{3}{l}{\textbf{Cost}}\\
         &  Performance Cost & $C_{p}$ \\
         &  SDLC Cost & $C_{sdlc}$ \\       
         \hline
    \end{tabular}
    \caption{All obfuscation evaluation criteria and subcriteria}
    \label{tab:criteria}
\end{table}


\subsection{Relevance of the Evaluation Constructs}
The first criterion to be considered in an evaluation of an obfuscation's strength is the relevance of all the constructs used in the evaluation. To maximize the relevance of a researcher's evaluation results for practitioners, the used constructs need to representative for real-world software protection. 


Researchers, however, face deadlines and budget and resource limitations. Different researchers therefore aim for different technology readiness levels (TRLs) with their research. That is obviously fine, and different criteria of our framework support research and evaluations at the various levels, while at the same time enabling and encouraging the researchers to be transparent about the TRL of their research results. Ensuring this transparency, and forcing the researcher to reason about the impact of evaluation methodology choices on the relevance of the evaluation results and their validity is the aim of the seven relevance criteria detailed below. 

It should come as no surprise that we first focus on criteria that pertain more to the evaluation method than to the obfuscation being evaluated itself. The reason is of course the lack of standardization in the domain of SP~\cite{Basile23}. Where certification has been standardized (e.g., smart card security~\cite{common_criteria}) and where risk management standards have been adopted (e.g., NIST SP 800-39~\cite{nistSP800-39} in network security~\cite{Gartner-report-riskanalysis}), concise evaluation results can be published that refer to those standards so that all stakeholders know how to interpret those results. In MATE SP, on the contrary, only some embryonic steps toward standardization have been proposed~\cite{Basile23}. Lacking standardized evaluation methods, evaluation results are meaningless without proper framing. 

An evaluation therefore stands or falls with accurate and complete framing, which can be achieved by assessing the evaluation according to the seven relevance criteria we put forward:

\paragraph{$Re_a$ - Attack Steps Relevance:} To what extent are the considered attack step combinations relevant? In which relevant attack strategies are they used, meaning attack strategies with a proven track record in the scientific literature or in real-world reporting? Are there no alternative combinations known that can produce the same results as the considered ones but are simpler to execute? Is the starting point of the considered attack steps realistic, i.e., the preceding attack steps? Does the evaluation use realistic outputs of those preceding steps as inputs to evaluate the considered attack step combination?

\paragraph{$Re_p$ - Program Property Relevance:} To what extent are the program properties relevant that the protection is supposed to hide, and that adversaries supposedly want to reveal through the considered attack steps? Are there alternative strategies with which adversaries can reach the same end goal, but without requiring them to obtain the results of the considered attack steps, i.e., without them requiring to reveal exactly those program properties?

\paragraph{$Re_o$ - Obfuscation Impact Relevance:} To what extent does the impact of the evaluated obfuscation on the result of the considered attack step combination affect the execution of later attack steps? Does it make them impossible? Does it make them require more resources? Does it make them produce less precise results? 

\paragraph{$Re_m$ - Metrics Relevance:} Do the (commonly used or ad hoc) metrics used to assess the obfuscation's impact on the considered attack steps' outputs truly capture the impact on later attack steps? Has that been validated, or are there strong arguments or evidence? 

\paragraph{$Re_t$ - Tool Availability:} Is practical tool support available to automate the considered attack steps and strategy? 

\paragraph{$Re_s$ - Sample Relevance:} Are the used samples representative for the types of software that would be attacked with the considered attack steps and strategies in the real-world?  

\paragraph{$Re_l$ - Layered Protection Relevance:} Does the evaluation consider relevant layerings or compositions of SPs?  

\vspace{0.2cm}

Note that some criteria are related, such as $Re_a$ and $Re_t$. It is the task of the researcher to be consistent in their assessment of their evaluation with respect to these criteria.  Also note that these criteria are to be evaluated mostly, if not completely, qualitatively. 

\subsection{Effectiveness against Standard Attacks}
\label{sec:effectiveness}

The effectiveness of an obfuscation for protecting a secret property in a given program against a considered attack step combination is defined as the obfuscation's effect on that combination, for that program and that property. 

To a large degree, effectiveness equals the old criterion of potency as defined by NC (see Section~\ref{sec:collberg_nagra_def}). There is one major difference, however: We specifically prescribe that the criterion effectiveness should only be used when the considered attack steps are \emph{standard attack steps}, i.e., interesting options for an adversary with no a priori knowledge about the specific SPs being deployed to counter the considered attack steps. In other words, these are the attack steps that can be expected to work well on vanilla applications as well as on the average protected application that the adversary may expect to be facing.  It is because of this crucial difference that we propose not to reuse and redefine the term potency, but instead put forward the term effectiveness. 

The notion of a standard attack steps is deliberately fuzzy. It is up to each researcher to determine which attacks steps to consider standard. The use of the term effectiveness then comes with the responsibility  to argue why the considered steps are to be considered standard. This is of course closely related to the relevance criteria $Re_a$, $Re_t$, and $Re_o$. It is the researcher's responsibility to ensure consistency in their assessment. 

Just like NC's potency considers the impact of an obfuscation in terms of analysis outcomes, and the required resources, so does effectiveness entail two criteria:

\paragraph{$E_o$ - Outcome Effectiveness} What is the effect of the obfuscation on the outcome of the standard attack step combination? Does the result become more complex, less precise, less accurate, incorrect, etc.? Often this effect can be quantified with (ad hoc) metrics. 

\paragraph{$E_r$ - Resource Effectiveness} What is the effect on the required resources to execute the standard attack steps. This effect can often be quantified. Many different forms of resources can be considered: computation time, memory footprint, network bandwidth, etc.

\vspace{0.2cm}

The effectiveness of an obfuscation for protecting a class of secret properties against a considered attack step combination is then to be obtained by evaluating it on multiple samples and by reporting the distribution of the obtained results. 

The metrics used to assess the outcome effectiveness $E_o$ should be computed on results produced by actual attack steps or good proxies thereof, not on ground-truth data produced while building the protected samples. So they should be tool-based, like the examples in Section~\ref{sec:attack_tool_examples}. Complexity metrics originating from the domain of software engineering can still be used, on program representations reconstructed by attack tools such as disassemblers, if there exists a correlation between the used metrics and the execution of later attack steps in the considered attack strategies. Obviously this relates to the relevance criterion $Re_m$. 

Of both the criteria, two forms can be considered. Isolated effectiveness ($E_{o,i}$ and $E_{r,i}$) is the effect of an obfuscation when it is evaluated in isolation. Marginal effectiveness ($E_{o,m}$ and $E_{r,m}$) is the effect of deploying the obfuscation in combination with other (commonly used) SPs. It is strongly encouraged to evaluate marginal effectiveness. For example, when evaluating the impact of some obfuscation on Javascript code, it should be evaluated on top of basic minification transformations~\cite{2019_anything_to_hide_studying_minified_and_obfuscated_code_in_the_web} that any Javascript obfuscator supports, such as identifier renaming~\cite{liu2017stochastic}. 

Finally, we stress that if an obfuscation aims at complicating multiple, different attack steps or step combinations, its effectiveness should also be evaluated for each of those steps or combinations, resulting in multiple results for outcome and resource effectiveness. 

\subsection{Efficacy - a Poor Man's Effectiveness}
\label{sec:efficacy}
When a researcher cannot provide convincing arguments or evidence for the considered attacks steps being standard and relevant, or when no actual attack steps are used in an evaluation, such as when only complexity metrics are being computed on ground-truth data, the term effectiveness should not be used. At best, the term \emph{efficacy} (with shorthands $e_o$ and $e_r$ instead of $E_o$ and $E_r$) can then be used to stress that something akin to lab conditions is being evaluated, rather an evaluation indicative for real-world conditions. 

\subsection{Robustness against Special-purpose Attacks}
As effectiveness considers only standard attacks steps, we need a counterpart for non-standard attack steps. For this purpose, the robustness of an obfuscation is determined in terms of how much better \emph{special-purpose analyses} can do than standard analyses. Better can be in terms of required resources and in terms of produced result. Special-purpose attack steps are those that perform (on average) worse than standard ones on samples that do not feature the evaluated obfuscation, and that would therefore not be chosen by adversaries unless they know they are attacking this obfuscation. 

Special-purpose attack steps benefit attackers if they require less resources or produce better results than standard ones on at least some programs protected with the obfuscation being evaluated. An example is the $k$-depth interprocedural constant propagation~\cite{interp_cp} discussed in Section~\ref{sec:perspective}, where $k$ is increased only for those parts of the program where it matters to reveal the secret property. Another example is a devirtualization technique tweaked for specific forms of virtualization~\cite{deobf_virtualization,kinder}.

The robustness against some special-purpose attack step combination is evaluated in three dimensions:

\paragraph{$Ro_o$ - Outcome Delta} How much worse or better is the result of some special-purpose attack on a protected sample compared to the result of a standard attack on it?

\paragraph{$Ro_r$ - Resource Delta} How much less or more resources does the special-purpose attack require on that sample?

\paragraph{$Ro_d$ - Deployment Delta} How much more configuration, customization, or development effort does the adversary need to invest before to enable the special-purpose attack, for example because no out-of-the-box tool support is available?

\vspace{0.2cm}

Obviously $Ro_o$ and $Ro_r$ should ideally be reported as distributions obtained on a number of samples.

Just like we did for effectiveness, we put forward isolated and marginal forms of these robustness criteria, to distinguish between the robustness of an obfuscation deployed in isolation, and the gain in robustness when deploying an obfuscation on top of other SPs. 

The deployment delta $Ro_d$ can be considered the updated equivalent of the developer effort in the definition of resilience by CTL (see Section~\ref{sec:resilience}). Unlike them, we do not prescribe the specific features that should be taken into account, nor do we prescribe a specific scale of robustness levels. We leave it up to the researchers to choose an appropriate assessment method. 
Obviously, $Ro_d$ and $Re_t$ are closely related. It is up to the researcher performing the evaluation to ensure consistency. 



\subsection{Concealment}
Concealment is the new framework's equivalent of the older concept of stealth. As discussed in Section~\ref{sec:narrow_stealth}, not only the accuracy of detector and locator functions needs to be assessed. A complete assessment of an obfuscation needs to include all its aspects that can help an adversary make strategic decisions. Concealment hence needs to be, and is, a more generic concept than the old stealth. 
 
One of the most important strategic decisions relates to robustness. The relevant question in that case is the following: If an obfuscation lacks in terms of stubbornness because special-purpose attack steps can defeat it, how easy is it for an adversary to determine which special-purpose attacks to choose? In other words, how likely will the adversary be able to decide to use that special-purpose attack. As an example, remember the discussion in Section~\ref{sec:perspective} on how adversaries can or cannot choose refinements of attack steps, such as choosing on which parts of the program to use a higher value of $k$ for $k$-depth interprocedural constant propagation. If an adversary has no way of determining these parts, i.e., if the adversary cannot create an oracle to produce the required configuration for such a special-purpose analysis, the defender should probably not be worried about its existence. 

To capture all relevant forms of information about the deployed protections, we put forward three concealment criteria: 

\paragraph{$C_l$ - Local Concealment} To what extent can a  locator function determine the exact locations in a program where an obfuscation has been deployed. This is the equivalent of NC's local stealth. We strongly encourage the use of standard accuracy metrics, however, instead of their proposal to compute the maximum of the false positive and false negative rates. 

\paragraph{$C_g$ - Global Concealment} To what extent can a detector function determine whether an obfuscation has been applied on a program. This is equivalent to NC's steganographic stealth. Again we encourage using standard accuracy metrics instead of a max function. 

\paragraph{$C_s$ - Strategic Concealment} To what extent can an adversary reveal additional information, of any kind, about the deployed protection that can drive strategic decisions in the attack strategy, in particular to determine which potentially beneficial special-purpose attack steps to execute next. We foresee that this criterion will most often be assessed qualitatively.

\vspace{0.2cm}

Note that as we did for effectiveness to replace potency, we opted to use the term concealment to replace stealth, even though they are very similar. We made this choice to avoid confusion when researchers report strength evaluations in the future. 

\subsection{Stubbornness against Deobfuscation}
Undoing an obfuscation is only one way to defeat it~\cite{emse2019}. It is still an important one, however, so the ease with which an obfuscation can be undone should be considered when evaluating it. Here the term ``undoing'' refers specifically to applying a transformation that undoes the effect of an obfuscation. It excludes performing the necessary analyses to determine which transformation to apply. 

Consider, for example, an opaque predicate that is always true and that is used in a conditional branch, which is hence always taken. With some preceding attack step ---of which the effectiveness can of course be evaluated--- the attacker might have determined that this predicate is always true. Stubbornness does not concern that preceding step, it only concerns the ease with which the opaque predicate insertion can be undone. With the code patching functionality of interactive disassemblers and decompilers such as Ghidra, this is trivial: it suffices to replace the conditional branch by a conditional one, thus simply removing the bogus execution path. Ghidra then automatically eliminates the now dead predicate computation and the bogus control flow path and bogus code from the reconstructed CFG and from the decompiled code. So a simple code patch suffices to completely undo the obfuscation. 

For other obfuscations, that are in nature perhaps not more advanced than opaque predicates, such easy undoing edits are not available. For example, after control flow flattening each flattened block has only one successor in the CFG, namely the dispatcher of the flattened code~\cite{flattening}. For reverting to the original CFG, simple edits that remove an execution path from the code will not suffice. 

To capture the ease with which such obfuscation-undoing edits and transformations can be performed on the representation of the software on which later attack steps will operate, we define two stubbornness criteria: 

\paragraph{$St_o$ - Outcome Stubbornness} To what extent can the effect of the obfuscation be undone in a relevant representation of the code? 

\paragraph{$St_d$ - Resource Stubbornness} How difficult is it to perform that deobfuscating transformation? 

\vspace{0.2cm}

For both aspects, only a qualitative assessment can be expected, which will probably involve ad hoc arguments and evidence. We certainly cannot prescribe specific methods at this point in time. But obviously, this assessment will need to be consistent with how other criteria are assessed, such as $Re_t$.

Note that multiple representations might need to be considered for these stubbornness criteria. If the later attack steps are assumed to be static code comprehension, editing Ghidra's internal representation of the binary code can suffice, as explained above for opaque predicates. If the follow-up attack steps to the contrary involve dynamic analysis techniques for which a patched binary needs to be executed, changing a tool's internal representation of a program will not suffice. In that case, a working edited binary actually needs to be produced first. Ghidra can generate a patched binary from its internal representation, but whether that binary will execute correctly will depend, among others, on the presence of anti-tampering protections such as remote attestation~\cite{viticchie2016reactive}. As was the case for effectiveness, also for stubbornness isolated and marginal stubbornness criteria can hence be considered.  



\subsection{Sensitivity}
\label{sec:sensitivity}
Section~\ref{sec:effectiveness} explicitly broadened the criterion of effectiveness from the impact observed one program sample regarding one property to effectiveness on a class of properties on many samples. For the other criteria, a similar broadening can, of course, be done as well. If an obfuscation's strength for any criterion is sensitive to certain features of the samples being protected, and the samples exhibit sufficient variance with respect to those features, this sensitivity will show up in the resulting distribution reported for that criterion. 

Regardless of whether or not such a distribution already provides evidence of such sensitivity, we encourage researchers to reflect on it explicitly. They can do so by providing evidence that the used samples cover a sufficient spread of the relevant features, or by arguing qualitatively how sensitive the evaluated obfuscations and the evaluation criteria are to certain program features. 

Such a reflection should not be limited to the features of the programs to be protected however. To the contrary, sensitivities to several confounding factors should be assessed:

\paragraph{$Se_s$ - Sample Feature Sensitivity} To what extent are the other criteria sensitive to features of the programs to be protected?

\paragraph{$Se_{a}$ - Attack Instantiation Sensitivity} To what extent are the evaluation results for other criteria sensitive to design and implementation details of the considered attack steps? An example of such details are the precise data flow analyses that are used to implement certain attack steps, or the specific tools that are used thereto. This criterion is particularly important when the evaluation relies on research prototypes of software analysis techniques. As we stated in Section~\ref{sec:buggy_tools}, it is not common for such tools to be incomplete or buggy. If the evaluation results depend on bugs or missing features, this clearly needs to be reported. 

\paragraph{$Se_{pi}$ - Protection Instantiation Sensitivity} To what extent are the evaluation results for other criteria sensitive to the implementation and design details of the (research) tools developed to deploy the obfuscation on samples? Obviously, it are not only analysis tools developed by researchers that can be incomplete, so can research SP prototypes; hence the inclusion of this criterion. This criterion is particularly relevant for research into deobfuscation and software analysis methods (including malware detection), in which researchers have to evaluate to what extent their novel techniques can defeat variations of obfuscations rather than being overfitted to particular variants.  

\paragraph{$Se_{pc}$ - Protection Configuration Sensitivity} To what extent are the evaluation results for other criteria sensitive to the configuration parameters of the deployed protections? Tigress~\cite{tigress2023}, for example, offers a wide range of configuration options for each supported obfuscation. It should at least be clear which configurations have been used in the evaluation. Of particular interest are random seeds used to drive stochastic SP techniques that many SP tools support. Such techniques are often used to make obfuscations renewable, meaning that they do look somewhat different every time they are deployed. If the evaluation results are sensitive to the used parameters or random seeds, i.e., if they display a large variability, the researchers ideally perform a parameter sweep and measure the resulting variance in the evaluation criteria. 

\paragraph{$Se_{b}$ - Build Tool Flow Sensitivity} To what extent are the evaluation results for other criteria sensitive to the build tools with which the protected application is being built? The interaction between SP transformations and other transformations applied during the build process of a protected application, such as compiler optimizations that are executed after source-to-source rewriting has been applied to inject obfuscations (e.g., with Tigress~\cite{tigress2023}) or after compile-time protection passes have been executed (e.g., with OLLVM~\cite{ollvm} or Epona~\cite{epona}) are complex~\cite{optimization}. Obfuscation transformations risk to be undone completely or partially by clever compiler optimizations, as discussed in Section~\ref{sec:sensitivity_missing}. The sensitivity of the evaluation results to such interactions hence needs to be assessed. 

\paragraph{$Se_{p}$ - Platform Sensitivity} To what extent are the results limited to or dependent on the platform for which the software is built, such as the operating system, the processor architecture, etc.

\vspace{0.2cm}

Most of these criteria can in theory be assessed by performing sweeps over sufficiently diverse samples, and sufficient configurations and implementation choices, of the attack steps and of the obfuscations. In practice, however, researchers most likely will not have the time and resources to experiment with all potentially relevant configurations and implementations. We therefore expect several aspects to be assessed qualitatively, and in ad hoc manners. 

Beyond helping to reduce the threats to validity, these sensitivity criteria are critical for the user-friendliness of the tools supporting the obfuscation. As argued in Section~\ref{sec:sensitivity_missing}, the lower the different forms of sensitivity of an obfuscation, the more predictable its strength will be for different programs, for future evolutions on the same program, and/or in light of (future) evolutions of program analysis techniques and attacker capabilities. 

Note that $Se_s$ and $Se_{pi}$ combined cover what can be called the applicability of an obfuscation. Many obfuscations can only be applied to certain types of software artifacts, or when certain preconditions are met. Some limitations of an obfuscation may be fundamental, in other cases a prototype instantiation has limited applicability because of a lack of resources to engineer a more complete implementation. That is fine, but the difference between fundamental limitations and instantiation should be made clear in any evaluation. The criteria $Se_s$ and $Se_{pi}$ serve that purpose.  

\subsection{Predictability}
The fundamental reason why low sensitivity to program features can be beneficial as argued above, is that it makes the result of an obfuscation more predictable. Higher predictability can benefit a SP optimization process for selecting the best combination of SPs given a program, its assets, and their security requirements. Similarly, if the impact on variations of attack steps can easily be predicted, it can become easier for decision support tools~\cite{checkmate24} and for predictive ML models~\cite{reganoMetric,2017_predicting_the_resilience_of_obfuscated_code_against_symbolic_execution_attacks_via_machine_learning} to predict the impact of obfuscations on a range of similar attack steps.  

For these reasons, we put forward complementary predictability criteria $P_s$, $P_{a}$, $P_{pi}$, $P_{pc}$, $P_{b}$, and $P_{p}$ that have similar definitions as their sensitivity counterparts, but for predictability instead of sensitivity. Similar to the sensitivity criteria, we expect these to be assessed mostly, if not completely, qualitatively. 
 
\subsection{Cost}
A detailed discussion of the various performance criteria that can be useful for evaluating an obfuscation is out of scope. They can include network bandwidth, client-side execution time and memory footprint, server-side execution time and memory footprint, power consumption, real-time behavior, response latency, throughput, and various other performance criteria. Which ones are relevant depends on the program to protect and its non-security-related non-functional requirements. We hence do not prescribe any specific criterion, but we do observe that in literature~\cite{desutter2024evaluation}, static program size and execution time are by far the most evaluated criteria, followed at a great distance by compilation time, dynamic memory footprint, and power consumption. 

We do stress, however, that when a specific performance criterion is relevant, it is important to evaluate its sensitivity as discussed in Section~\ref{sec:sensitivity}. In particular, the sensitivity to the hotness of protected program fragments should be assessed, i.e., to their execution frequency and relative contribution to the overall performance.

Moreover, the cost of using certain obfuscations can extend beyond the performance criteria. Various (expensive) processes in the whole software development life cycle (SDLC) can be impacted by the use of obfuscations, such as certification, quality assurance, debugging, distribution, etc. 

To capture both forms of costs, we put forward two broadly defined cost criteria:

\paragraph{$C_{p}$ - Performance Cost} How is the quantitative overhead of the protections in terms of all kinds of performance costs distributed? 

\paragraph{$C_{sdlc}$ - SDLC Cost} What are, qualitatively or quantitatively, the impacts that the deployment of the evaluation obfuscation can have on the SDLC? Where can the deployment of the obfuscation pose challenges or compatibility issues with industrial SDLCs?








