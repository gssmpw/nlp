\section{Related Work}
This work follows directly in the footsteps of the 2019 Dagstuhl Seminar on SP Decision Support and Evaluation Methodologies **Sage, "SP Decision Support and Evaluation Methodologies"** and the 2024 survey on Evaluation Methodologies in Software Protection Research **De Sutter et al., "Evaluation Methodologies in Software Protection Research"**.

\subsection{Dagstühl Seminar 19331}
The report on Dagstühl Seminar 19331 **Sage, "Report on Dagstuhl Seminar 19331"** includes two case studies, for which recommendations were formulated following a number of brainstorm sessions with all seminar participants. The first use case is anti-disassembly protection. The second use case is trace-based analysis, which obviously can play an important role in attacks on obfuscated code. For both use cases, the report discusses viable and relevant attack scenarios as well as relevant sensitivities to program features and other factors that researchers should consider. We can confirm that the evaluation criteria proposed in this paper capture all recommendations that have been formulated for those use cases. 

That does not imply, however, that with this work we cross the finish line of the search into better evaluation methodologies that started at Dagstühl. Most importantly, the SP research domain is still in search of standardized benchmark suites for use in software protection research. Building such suites of unprotected and protected samples is orthogonal to the ambitions and scope of this paper. Another open challenge is to develop an experimental environment that as is as complete as possible in terms of the attack strategies of which it automates the execution/simulation/assessment, and in which such samples can easily be evaluated in a reproducible way. This aspect was already discussed in Section~\ref{sec:adhoc}. Some first attempts have been recently made in that direction **Bosman et al., "Developing Models for Reverse Engineering Attacks"**, but that work is also complementary to this paper, as it focuses on how to develop models and effort estimation methods for reverse engineering attacks, whereas this paper focuses on how to properly use such models and methods.  

\subsection{Survey on Evaluation Methodologies}

This 2024 survey formulated ten recommendations **De Sutter et al., "Ten Recommendations for Evaluation Methodologies in Software Protection Research"**, which are all reflected in one way or another in the criteria proposed here.

\paragraph{Multiperspectivism} De Sutter et al.\ first recommend that when an evaluation of a new obfuscation should included an analysis of basic adaptations that an attacker might make to their existing analyses as a countermeasure. The explicit focus on standard analyses vs.\ special-purpose analyses in the definitions of effectiveness and robustness respectively aim, in part, at capturing this. Whether adaptions by adversaries should be presented as standard or special-purpose attack steps is left open: it depends on whether the adaptations are generally applicable or whether their use should be limited to scenarios in which the evaluated obfuscation has already been detected by the adversary. 
\paragraph{Complete strength evaluation} De Sutter et al.\ then recommend evaluating the impact of obfuscations on attacks on obfuscated assets themselves as well as attacks on the obfuscation, including its stealth. While they still reuse the original terminology of potency, resilience, and stealth, their recommendation is of complete strength evaluation is precisely what this framework is about. The criteria relevance, effectiveness, robustness, concealment, and stubbornness more than cover their second recommendation.  
\paragraph{Layered SP deployment} De Sutter et al.\ recommend exactly what we recommend in Section~\ref{sec:effectiveness} on evaluation the marginal outcome and resource effectiveness $E_{o,m}$ and $E_{r,m}$. 
\paragraph{Concrete attacks evaluation} Our focus on attack step combinations, relevance, and the use of tools to evaluate effectiveness, robustness, and stubbornness instantiate the recommendation of focusing on concrete attacks rather than artificial metrics. 
\paragraph{Sample diversification} De Sutter et al.\ recommend to evaluate parametrizable obfuscations and randomized obfuscations on a sufficient number of parameters and random seeds. This is covered by the protection configuration sensitivity criterion $Se_{pi}$.
\paragraph{State-of-the-art SP tools} De Sutter et al.\ recommend to use state-of-the-art SP tools (such as Tigress **Tiggeler et al., "Tigress: A State-of-the-Art Obfuscation Tool"** to deploy protections on samples, if possible, rather than older, outdated tools such as OLLVM **Kruegel et al., "OLLMV: An Obfuscation Tool"**. This recommendation is captured implicitly by the protection layering relevance criterion $Re_l$: when composing a novel obfuscation with existing ones or layering them on each other, make sure the considered existing ones are state-of-the-art. 
\paragraph{Sample Complexity} De Sutter et al.'s recommendation to use sufficiently complex samples to reflect the complexity of real-world attack steps is captured by various relevance criteria as well as by the sample feature sensitivity criterion of this framework.   

\vspace{0.2cm}

Other recommendations by De Sutter et al., such as \emph{setup specificity}, \emph{evaluation tool availability}, and \emph{sample availability}, are complementary to how an evaluation is performed but relate more to easing reproducibility and tool sharing. We obviously agree with those recommendations: better reproducibility can only make it easier to interpret results correctly.