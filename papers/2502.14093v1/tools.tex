\section{Attack Tool Metrics}

A practical approach to evaluate the potency of obfuscations is to measure their impact on actual attack tools. One can compare how much resources a tool requires on vanilla and obfuscated programs, and how well the results obtained on the vanilla programs approximate those on the original ones. 

\subsection{Example Usage in Literature}
\label{sec:attack_tool_examples}
Examples are Foket et al.~\cite{Foket14} comparing points-to set sizes computed by WALA~\cite{WALA} on obfuscated Java programs, Linn and Debray~\cite{Linn2003} counting how many instructions no longer get disassembled correctly by objdump~\cite{binutils} and IDA Pro~\cite{IDA}, Van den Broeck et al.~\cite{jens21} counting how many CFG edges are no longer drawn correctly in CFGs displayed by the IDA Pro disassembler~\cite{IDA} and how many times code is duplicated in CFGs reconstructed with Binary Ninja~\cite{ninja}, and Cozza et al.~\cite{mila24} counting how many invariants among PLC (progammable logic controller) register values can be uncovered by Daikon~\cite{Daikon}. When such metrics can be categorized as true and false positives and negatives, quality measures such as recall, precision, accuracy, F1-score, etc.\ are often used~\cite{statistics}.

Papers that contribute deobfuscation techniques (e.g.,~\cite{2022_chosen_instruction_attack_against_commercial_code_virtualization_obfuscators,2021_mba_blast_unveiling_and_simplifying_mixed_boolean_arithmetic_obfuscation,2017_syntia_synthesizing_the_semantics_of_obfuscated_code,2015_a_generic_approach_to_automatic_deobfuscation_of_executable_code}) obviously evaluate the resilience of obfuscations. They do so by reporting the aforementioned quality measures for their contributed techniques on sample sets, and by comparing the similarity of the deobfuscated code to the obfuscated code. The same holds for papers that contribute obfuscation techniques and evaluate them against known deobfuscation methods (e.g.,~\cite{2022_loki_hardening_code_obfuscation_against_automated_attacks,2021_search_based_local_black_box_deobfuscation_understand_improve_and_mitigate}).

In their work on obfuscation strategies for industrial control systems, Cozza et al.~\cite{mila24} use two metrics to quantify resilience along two dimensions. First, they consider the number of invariants establishing connections between genuine and spurious registers. Genuine PLC registers are the ones that actually control physical processes, while spurious registers are the ones that seem to be used for such control (because of the obfuscation) but are not. The idea is that such invariants complicate the attackerâ€™s ability to discern between those two types of registers and their true and fake usage. The second metric is the number of aggregations (clusters) of PLC registers associated with physical devices belonging to the same physical process. These aggregations supposedly make it difficult for the attacker to separate genuine from spurious physical processes handled by the same PLC. Cozza et al.\ use Daikon~\cite{Daikon} to extract the invariants from their use case applications.

\subsection{Critique}

\subsubsection{Ad hoc Nature}
\label{sec:adhoc}
Most tool-based metrics are ad hoc. 
For example, Van den Broeck et al.\ used two entirely different metrics for evaluating the impact of the same obfuscations on two disassemblers, because those two tools differ in the way in which they reconstruct functions and their CFGs~\cite{jens21}. For IDA Pro, which assigns each basic blocks to exactly one function, they report how true and fake control flow transfers are correctly interpreted and displayed in the CFGs. For Binary Ninja, which assigns each basic block to every function from which the block is reachable through intraprocedural control flow transfer idioms, they report the amount of resulting code duplication in the created CFGs.

Because of their ad hoc nature, it is questionable whether the conclusions drawn from the measurements are more generally valid, i.e., beyond the specific tools and tool versions that have been evaluated. Such answers can only be answered when sufficient insight into the internal operation of the tools used is available, either on the side of the authors of the publications or on the side of their readers. The latter requires that the papers contain sufficient information. This is definitely not always the case~\cite{desutter2024evaluation}.

Importantly, not only is the nature of the tool-based metrics often ad hoc, so is the deployment and configuration of the tools. For example, Van den Broeck et al.\ evaluated the use of IDA Pro with a number of simple plugins that helped the tool to overcome some of its most basic shortcomings resulting from the fact that it was not developed to handle the forms of code produced by the evaluated obfuscation~\cite{jens21}. In other words, Van den Broeck et al.\ evaluated the use of the tool in one of the possible ways in which attackers might use it after adapting their mode of operation to the existence of the novel obfuscation. This is a good practice~\cite{desutter2024evaluation}. Their plugins only covered a tiny fraction of the adaptations that an attacker might make, however, so how generally applicable are their results? Again, this question needs to be raised. 

A standardized reverse engineering analysis toolbox, with a commonly accepted set of analysis tools and commonly accepted measurement criteria might help to overcome the potentially limited validity of ad hoc metrics. At least it could lead to some standardization of evaluation methods, in particular when also the benchmark programs would be standardized. As observed in literature, however, no standardization is currently forming~\cite{desutter2024evaluation}. 

Attempts have recently been made to create a reusable and expandable toolbox and models to evaluate the strength of SPs~\cite{checkmate24}. It is unclear whether such a toolbox could ever be considered complete. For one, we do not think that there exists such a thing as a complete tool: attackers can use unsound techniques, and some techniques work better in some cases than others. To remain sound or soundish, many different heuristics and techniques can be used by tools. To optimize user-friendliness, additional heuristics and techniques can be used. All of these work better in some scenarios than in others, and importantly, they may not all be compatible. The already cited work Van den Broeck et al.\ is exemplary~\cite{jens21}. Because the policies of adding basic blocks to functions differ so much among the most popular disassemblers, some metrics simply are irrelevant and/or meaningless for some of them. 

The problem goes deeper even, because it is hard to come up with metrics that are relevant to as many scenarios as possible. For example, if a disassembler does not reconstruct functions correctly, how does one measure the quality of that reconstruction? After having given this question much thought, we did not find a good answer. To some extent, this issue is the same as with theoretic complexity metrics: They may be relevant and validated in one scenario (unobfuscated code), but this does not imply their relevance and validity in other scenarios (obfuscated code). 

A standard evaluation tool suite will certainly benefit our domain. The Common Criteria for certifying smart cards is a good example of similar standardization~\cite{common_criteria}. For the more philosophical/theoretical question of how to define potency, which is a non-functional feature, this will suffice, however. First, as discussed at several points already, attackers refine their strategies as they go: how to take into account the effort/difficulty/success of the most likely refinement strategies? Secondly, not all possible customizations/refinements of analyses can be included? How to take that into account? All in all, we do not think a complete tool is possible, and hence the definition or framing of the evaluation concepts needs to handle that limitation. Even if a complete toolbox would ever be achievable, until we get there, we need a back-up.
  
\subsubsection{Narrow Scope}
As Ceccato et al.\ observed~\cite{emse2019}, there exist multipe options to overcome SPs, deobfuscation being only one of them, and likely the hardest. While this does not diminish the relevance of the existing studies of automated deobfuscation, it does put into question the focus of resilience on deobfuscation, and the corresponding neglecting of alternatives to defeat SPs by bypassing them, overcoming them, or building workarounds. Ceccato et al.'s taxonomy describes these alternative strategies.

\subsubsection{Attack Oversimplification}
Another way attacks are often narrowed down to deobfuscation, and therefore oversimplified, is in their neglect of how to locate the code to be deobfuscated. One class of papers that suffers from this is papers focusing on deobfuscation of mixed Boolean-arithmetic (MBA) expressions. The vast majority of those papers, such as~\cite{2021_mba_blast_unveiling_and_simplifying_mixed_boolean_arithmetic_obfuscation}, only study the deobfuscation of symbolic MBA expressions but sidestep the problem of how to identify deobfuscatable expressions in binaries.

De Sutter et al.~\cite{desutter2024evaluation} also observed that most obfuscation publications only consider one individual analysis method in their tool-based evaluation, if they have one at all. Clearly, guidance is needed towards considering real-world attacks in which multiple methods are combined. Recently, tools and models for doing so have been proposed~\cite{checkmate24}, which may be a first step in this direction. 

\subsubsection{Unvalidated Metrics}
\label{unvalidated_metrics}
Just as many software complexity metrics have not been validated on obfuscated code (see Section~\ref{sec:unfit_complexity_metrics}), none of the ad hoc tool-based metrics have been validated, i.e., the relation between those metrics and actual attack effort or success rate has not been determined. For example, Van den Broeck et al.~\cite{jens21} count how many edges are drawn by IDA Pro in functions' control flow graphs, but is it really relevant whether or not an edge that IDA Pro knows about and records in its database of CFGs, is drawn on screen or not? This has not been validated at all. 

This criticism is harsh, as it is probably not feasible to validate all relevant metrics with empirical experiments because those are too expensive. Authors reporting tool-based metrics should hence probably be forgiven. Still, it is a threat to validity. 

\subsubsection{Buggy and Incomplete Tools}
\label{sec:buggy_tools}
Besides the impact of tool availability (see Section~\ref{sec:tool_availability}) and of their limitations and peculiar heuristics (see Sections~\ref{sec:unclear_computations} and~\ref{sec:adhoc}), an additional issue is that software analysis tools invariably have bugs. Commercial protection tools sometimes exploit these, e.g., by injecting instructions into a binary that cause certain tools to crash. Academic analysis tools are obviously also often buggy in the sense that their implementation will typically be incomplete. As research tools, their authors might very well have cut some corners to make the tools work on their use cases and benchmarks, but not beyond them. 

Obviously, SP strength evaluations should be transparent with respect to results depending on bugs or completeness issues with the used tools. Although we know of no papers published in top conferences or journals that violate this requirement overtly, we have reviewed (and blocked from publications) papers making this mistake multiple times in the past. So we think this issue deserves explicit handling in the guidelines. 

\subsubsection{Confusion about Potency and Resilience}
The use of the term resilience by Cozza et al.\ does not correspond to CTL's original definition, as it does not target automated deobfuscation. This is in line with what was already mentioned in Section~\ref{sec:fuzzy_boundary}, namely that researchers sometimes claim to evaluate, assess, or predict resilience because they study automated techniques, but in fact do not study deobfuscation techniques~\cite{banescu15,2017_predicting_the_resilience_of_obfuscated_code_against_symbolic_execution_attacks_via_machine_learning}, so whether they really study potency or resilience is up for discussion. In this case, the question is whether computing invariants can be seen as deobfuscation. Clearly the existing definitions are confusing. 

To some extent, this seems unavoidable, because some analysis can be used as part of deobfuscation, but also exist outside that context. Another example is the computation of points-to set sizes, as done by Foket et al.~\cite{Foket14}. Computing points-to sets (a.k.a.\ as class analysis~\cite{vortex}) is necessary to compute precise call graphs of software written in object-oriented programming languages that feature polymorphism. 
It is hence an analysis that clearly exists outside of deobfuscation. But when an obfuscation aims precisely at making call graphs imprecise, points-to set computation is part of the deobfuscation. Evaluating the impact of that obfuscation on points-to sets hence seems to touch both potency and resilience.

The use of plug-ins by Van den Broeck et al.~\cite{jens21} provides another example. Those plug-ins try to fix some of the worst performing heuristics of IDA Pro's CFG reconstruction approach when facing their novel obfuscation. Is an evaluation with such plug-ins to be seen as evaluating that obfuscation's potency or its resilience? Again, one evaluation seems to touch on both those criteria. Trying to draw a strict line between them is hence perhaps futile. 