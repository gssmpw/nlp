\section{Discussion}
\label{sec:discussion}

The proposed framework comprises more than 30 evaluation criteria in eight categories, plus one backup category. This framework and its criteria aim for complete coverage of the relevant criteria, rather than for prescribing evaluation methods in detail. It aims for informal guidelines that help practitioners limit the threats to the validity of their research, rather than for formal definitions. 

Individual researchers that will (hopefully) use this evaluation framework still have considerable degrees of freedom to instantiate the proposed criteria and a responsibility to do so adequately. By doing so, they will be able to maximally mitigate threats to internal validity~\cite{Wohlin} and conclusion validity~\cite{Wohlin}. 

By emphasizing relevance, layered use of protections, evaluations of the impact on adversaries' attack steps, and various forms of sensitivity, the proposed framework can help researchers mitigate threats to external validity~\cite{Wohlin}, to construct validity~\cite{Wohlin}, and to instantiation validity~\cite{lukyanenko2014instantiation}.

Of particular interest is that our framework allows, and even encourages to use ad hoc tool-based metrics, which at first sight might seem to contradict the critique formulated in Section~\ref{sec:adhoc} regarding the use of ad hoc metrics and ad hoc tool configuration. Given the wide range of program properties that adversaries might be interested in, the wide range of program features that can impact the adversaries' effectiveness and efficiency in executing their attack strategies, the wide range of analyses techniques and tools that can help them, as well as the various alternative algorithms and heuristics such tools can build on, in combination with the need focus on impacting actual attack steps instead of artificial metrics, we see the use of ad hoc tool-based metrics as unavoidable. In fact, if done well, we see it as absolutely beneficial. 

Multiple criteria focus on using ad hoc tool-based metrics adequately. Combined, the criteria attack steps relevance, program property relevance, and metrics relevance aim at ensuring that measured ad hoc features and metrics are relevant. Similarly, the criteria attack steps relevance, robustness, and the attack instantiation sensitivity encourage researchers to consider how adversaries could adapt their existing attack strategies and heuristics, i.e., to consider not only the status quo but also the next steps in the cat and mouse game between defenders and attackers.

Over time, we hope that the use of this framework itself will become a best practice, but also that a catalogue of best practices will be built as new and better instantiations of them get published. 

Of all the critiques formulated in Sections 2--5, the only ones not satisfactorily addressed by our framework are those on software complexity metrics from the domain of software engineering not necessarily being valid when used on obfuscated code (Sections~\ref{sec:unfit_complexity_metrics} and~\ref{unvalidated_metrics}). We can only recommend that more empirical research is performed to validate metrics for the assessment of reverse engineering activities on obfuscated code, such as~
\cite{emse2019,2014afamily} to improve the relevance of such complexity metrics. Notice that even in the domain of software engineering, they are contested, and more empirical research on their validation is required~\cite{feitelson}. In our framework, we discourage their use by prioritizing the use of effectiveness criteria (Section~\ref{sec:effectiveness}) over efficacy criteria (Section~\ref{sec:efficacy}). 

\section{Related Work}

This work follows directly in the footsteps of the 2019 Dagstuhl Seminar on SP Decision Support and Evaluation Methodologies~\cite{Dagstuhl} and the 2024 survey on Evaluation Methodologies in Software Protection Research~\cite{desutter2024evaluation}.

\subsection{Dagstuhl Seminar 19331}
The report on Dagstuhl Seminar 19331~\cite{Dagstuhl} includes two case studies, for which recommendations were formulated following a number of brainstorm sessions with all seminar participants. The first use case is anti-disassembly protection. The second use case is trace-based analysis, which obviously can play an important role in attacks on obfuscated code. For both use cases, the report discusses viable and relevant attack scenarios as well as relevant sensitivities to program features and other factors that researchers should consider. We can confirm that the evaluation criteria proposed in this paper capture all recommendations that have been formulated for those use cases. 

That does not imply, however, that with this work we cross the finish line of the search into better evaluation methodologies that started at Dagstuhl. Most importantly, the SP research domain is still in search of standardized benchmark suites for use in software protection research. Building such suites of unprotected and protected samples is orthogonal to the ambitions and scope of this paper. Another open challenge is to develop an experimental environment that as is as complete as possible in terms of the attack strategies of which it automates the execution/simulation/assessment, and in which such samples can easily be evaluated in a reproducible way. This aspect was already discussed in Section~\ref{sec:adhoc}. Some first attempts have been recently made in that direction~\cite{checkmate24}, but that work is also complementary to this paper, as it focuses on how to develop models and effort estimation methods for reverse engineering attacks, whereas this paper focuses on how to properly use such models and methods.  

\subsection{Survey on Evaluation Methodologies}

This 2024 survey formulated ten recommendations~\cite{desutter2024evaluation}, which are all reflected in one way or another in the criteria proposed here.

\paragraph{Multiperspectivism} De Sutter et al.\ first recommend that when an evaluation of a new obfuscation should included an analysis of basic adaptations that an attacker might make to their existing analyses as a countermeasure. The explicit focus on standard analyses vs.\ special-purpose analyses in the definitions of effectiveness and robustness respectively aim, in part, at capturing this. Whether adaptions by adversaries should be presented as standard or special-purpose attack steps is left open: it depends on whether the adaptations are generally applicable or whether their use should be limited to scenarios in which the evaluated obfuscation has already been detected by the adversary. 
\paragraph{Complete strength evaluation} De Sutter et al.\ then recommend evaluating the impact of obfuscations on attacks on obfuscated assets themselves as well as attacks on the obfuscation, including its stealth. While they still reuse the original terminology of potency, resilience, and stealth, their recommendation is of complete strength evaluation is precisely what this framework is about. The criteria relevance, effectiveness, robustness, concealment, and stubbornness more than cover their second recommendation.  
\paragraph{Layered SP deployment} De Sutter et al.\ recommend exactly what we recommend in Section~\ref{sec:effectiveness} on evaluation the marginal outcome and resource effectiveness $E_{o,m}$ and $E_{r,m}$. 
\paragraph{Concrete attacks evaluation} Our focus on attack step combinations, relevance, and the use of tools to evaluate effectiveness, robustness, and stubbornness instantiate the recommendation of focusing on concrete attacks rather than artificial metrics. 
\paragraph{Sample diversification} De Sutter et al.\ recommend to evaluate parametrizable obfuscations and randomized obfuscations on a sufficient number of parameters and random seeds. This is covered by the protection configuration sensitivity criterion $Se_{pi}$.
\paragraph{State-of-the-art SP tools} De Sutter et al.\ recommend to use state-of-the-art SP tools (such as Tigress~\cite{tigress2023} to deploy protections on samples, if possible, rather than older, outdated tools such as OLLVM~\cite{ollvm}. This recommendation is captured implicitly by the protection layering relevance criterion $Re_l$: when composing a novel obfuscation with existing ones or layering them on each other, make sure the considered existing ones are state-of-the-art. 
\paragraph{Sample Complexity} De Sutter et al.'s recommendation to use sufficiently complex samples to reflect the complexity of real-world attack steps is captured by various relevance criteria as well as by the sample feature sensitivity criterion of this framework.   

\vspace{0.2cm}

Other recommendations by De Sutter et al., such as \emph{setup specificity}, \emph{evaluation tool availability}, and \emph{sample availability}, are complementary to how an evaluation is performed but relate more to easing reproducibility and tool sharing. We obviously agree with those recommendations: better reproducibility can only make it easier to interpret results correctly. 
