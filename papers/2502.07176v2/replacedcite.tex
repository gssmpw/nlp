\section{Background and Related Work}
\subsection{Kolmogorov-Arnold Networks}
In addition to the features already noted, KANs differ from MLPs in their theoretical foundation.  Unlike MLPs, whose theoretical basis as a function modeling framework derives from the universal approximation theorem,  KANs rely on the Kolmogorov-Arnold representation theorem, which provides that every multivariate continuous function \(f\) can be represented as a finite composition of continuous, univariate functions and the binary operation of addition:
\begin{equation}
f(x) = f(x_1, ..., x_n) = \sum_{q=1}^{2n+1} \Phi_q (\sum_{p=1}^n \phi_{q,p} (x_p))
\end{equation}
where \(\phi_{q,p} : [0,1] \rightarrow \mathbb{R}\) and \(\Phi_q : \mathbb{R} \rightarrow \mathbb{R}\) ____. 
This traditional formulation of the Kolmogorov-Arnold representation theorem is analogous to a small neural network with (i) \(n\) input neurons, (ii) a single hidden layer with \(2n+1\) neurons, and (iii) a single output neuron.  \(\phi_{q,p}\) represent the edges between input neurons and neurons in the hidden layer, and \(\Phi_q\) represent edges between neurons in the hidden layer and the output neuron.

To generalize this approach for modeling more complex systems and to facilitate training of this using traditional neural network regression strategies, KANs expand this two-layer network to networks of arbitrary width and depth and parameterize all edge functions using B-splines.  The shape of a KAN is represented as \([n_0, n_1, ..., n_L]\), where \(n_i\) is the number of nodes in the \(i^{th}\) layer.  The activation value for a given node is represented as \(x_{l,i}\) and the B-spline activation function between two nodes in adjacent layers is represented as \(\Phi_{l, j, i}\), where \(l\) denotes the layer, \(i\) denotes the neuron within the given layer and \(j\) denotes the connected neuron in the subsequent layer ____:
\begin{equation}
\phi_{l,j,i}, l=0, ..., L-1, i=1, ..., n_l, j=1, ..., n_{l+1}
\end{equation}

The output of a given B-spline activation function is represented as \(\tilde{x} \equiv \Phi_{l,j,i} (x_{l,i})\).   For any given node, the activation value is equal to the sum of the outputs of the B-spline activation functions on each incoming network edge ____:
\begin{equation}
x_{l+1, j} = \sum_{i=1}^{n_l} \tilde{x}_{l,j,i} = \sum_{i=1}^{n_l} \phi_{l,j,i} (x_{l,i})
\end{equation}
where \(j = 1, ..., n_{l+1}\).

\subsection{B-Splines}
\begin{figure} [ht]
\begin{center}
\centerline{\includegraphics[width=0.75\columnwidth]{B-Spline_Curve.png}}
\caption{Diagram of a cubic B-spline curve, including applicable knots (turquoise) and control points (red).}
\label{fig:b-spline curve}
\end{center}
\end{figure}
To facilitate diversity in the learned activation function on each edge of a KAN, KANs implement activation functions using B-splines.  B-splines are piecewise polynomial curves whose name derives from the elastic beams (i.e., splines) used by draftsmen to construct sweeping curves in ship design ____.  They are defined as affine combinations of control points \(c_i\) and associated basis functions \(B_i (x)\) ____:
\begin{equation} \label{eq:KANspline-1}
spline(x) = \sum_i c_i B_i(x)
\end{equation}

B-splines can be used to model a wide variety of shapes due to their ability to represent functions of arbitrary degree.  A curve \(spline(x)\) is called a B-spline of degree \(k\) with knots \(t_o, ..., t_m\), where \(t_i \leq t_{i+1}\) and \(t_i < t_{i+k+1}\) for all \(i\), which delineate the intervals over which the B-spline is defined (see Figures \ref{fig:b-spline curve} and \ref{fig:b-spline basis functions}), and control points \(c_o, ..., c_i\), which control the local shape of the B-spline. Due to their control over the shape of the B-spline, KANs parameterize B-splines via their control points. Expressions for the basis functions \(B_{i,k} (x)\) of a given B-spline are derived via the Cox-De Boor recursion algorithm, examples of which are shown in Figure \ref{fig:b-spline basis functions}:

\begin{small}
\begin{equation} \label{eq:Cox-De Boor}
\begin{split}
B_{i,0} (x) & =
\begin{cases}
1 & \text{if } t_i \leq x < t_{i+1},\\
0 & \text{otherwise.}
\end{cases} \\
B_{i,k} (x) & = \frac{x - t_i}{t_{i+k} - t_i} B_{i, k-1} (x) + \frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}} B_{i+1, k-1} (x)
\end{split}
\end{equation}
\end{small}

As Equations \ref{eq:KANspline-1} and \ref{eq:Cox-De Boor} demonstrate, the value of a basis function \(B_{i,k} (x)\) of degree \(k\) is calculated recursively by reference to the position of the input value \(x\) within the knot vector and the output of certain basis functions of degree \(k-1\), and the final output of the B-spline is determined by multiplying all applicable basis functions by their associated control points \(c_i\).
\begin{figure} [ht]
\begin{center}
\centerline{\includegraphics[width=0.75\columnwidth]{B-Spline_Basis_Functions.png}}
\caption{Diagram of cubic B-spline basis function curves (various colors) and knots (turquoise).}
\label{fig:b-spline basis functions}
\end{center}
\end{figure}

Given that basis functions of degree \(k\) are defined by reference to basis functions of degree \(k - 1\), it follows that calculation of a degree \(k\) basis function requires \(k\) levels of sequential, recursive calls. This sequential dependency in the Cox-De Boor recursion algorithm prevents parallelization, resulting in a proportional increase in computation time as B-spline degree increases, even with memorization or dynamic programming techniques. Our testing shows that B-spline calculations constitute a significant portion of total execution time for KANs, ranging from 18\% for degree-2 B-splines to over 50\% for degree-30 B-splines, making B-spline computational efficiency a fundamental concern in improving the computational efficiency of KANs as a whole. While low-degree B-splines enable KANs to outperform MLPs ____, our tests have demonstrated that high-degree B-splines are necessary for optimal performance in modeling certain functions, with optimal convergence occurring at degrees of 30 or higher. 
To bridge the gap between the need for high-degree B-splines and their high computational cost, we need a way to parallelize the recursive B-spline calculations for efficient execution on a parallel processor.

\subsection{Related Work}

To address the computational cost of KANs, a number of approaches have been taken.  For certain applications, despite the general additional computational overhead of KANs as compared to MLPs with similar parameter counts, KANs have been shown to produce superior performance with lower parameter counts, resulting in overall computational efficiency, as is the case with the convolutional KAN and U-KAN ____.  However, these results involve hybrid KAN architectures and are domain-specific, suggesting the need for more general optimization techniques.  To achieve more general computational efficiency, many modified KAN architectures have been proposed that replace underlying B-splines with different basis functions, such as radial basis functions (FastKAN / FasterKAN) ____, trigonometric functions (e.g., Larctan-SKAN) and left-shift softplus functions (LSS-SKAN) ____, and rational functions (rKAN) ____.  While each of these approaches has shown speedups over the B-spline-based KAN architecture, each basis function type has its own strengths and weaknesses in function approximation, and none identified here exhibit the local support feature of B-splines that allows for its granular function modeling capabilities. In certain research, adding wavelet functions to B-spline-based KANs (Wav-KAN) ____ or implementing neuron grouping / weight sharing to minimize the overall parameter count of B-spline-based KANs (Free Knots KAN) ____ has yielded efficiency gains, but in all cases these speedups are only by a constant factor and none resolves the underlying computational complexity of the recursive B-spline calculations that is addressed by our proposed approach.  That said, each is complementary to our approach and could be used in combination to achieve further improved computational efficiency.