\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{xurl}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{From Hazard Identification to Controller Design: Proactive and LLM-Supported Safety Engineering for ML-Powered Systems
}
\newcommand{\todo}[1]{\textit{\textcolor{green}{#1}}}

\author{
    \IEEEauthorblockN{Yining Hong}
    \IEEEauthorblockA{
        % \textit{School of Computer Science} \\
        \textit{Carnegie Mellon University}\\
        % Pittsburgh, PA, USA \\  
        % yhong3@andrew.cmu.edu
    }
    \and
    \IEEEauthorblockN{Christopher S. Timperley}
    \IEEEauthorblockA{
        % \textit{School of Computer Science} \\
        \textit{Carnegie Mellon University}\\
        % Pittsburgh, PA, USA \\  
    }
    \and
    \IEEEauthorblockN{Christian K{\"a}stner}
    \IEEEauthorblockA{
        % \textit{School of Computer Science} \\
        \textit{Carnegie Mellon University}\\
        % Pittsburgh, PA, USA \\  
    }
}

\maketitle

\begin{abstract}
Machine learning (ML) components are increasingly integrated into software products, yet their complexity and inherent uncertainty often lead to unintended and hazardous consequences, both for individuals and society at large. Despite these risks, practitioners seldom adopt proactive approaches to anticipate and mitigate hazards before they occur. Traditional safety engineering approaches, such as Failure Mode and Effects Analysis (FMEA) and System Theoretic Process Analysis (STPA), offer systematic frameworks for early risk identification but are rarely adopted. This position paper advocates for integrating hazard analysis into the development of any ML-powered software product and calls for greater support to make this process accessible to developers. By using large language models (LLMs) to partially automate a modified STPA process with human oversight at critical steps, we expect to address two key challenges: the heavy dependency on highly experienced safety engineering experts, and the time-consuming, labor-intensive nature of traditional hazard analysis, which often impedes its integration into real-world development workflows. We illustrate our approach with a running example, demonstrating that many seemingly unanticipated issues can, in fact, be anticipated.
\end{abstract}

\begin{IEEEkeywords}
safety engineering, hazard analysis, software engineering for machine learning
\end{IEEEkeywords}

\section{Introduction}

Phrases like \textit{``this was an unintended consequence''} or \textit{``nobody could have anticipated this new problem''} often arise when software products face issues, such as mistakes or biases in ML models within the software. For example, applications may unintentionally amplify biases\cite{bias_amplification_suresh2021framework} or introduce privacy risks\cite{privacy_risks_chen2021machine}. 
However, \textit{unintended consequences}  merely implies that these issues were not anticipated, not that they were inherently unpredictable; 
had they been anticipated, developers could, in many cases, have mitigated them before they occurred.
This position paper argues that (1) systematic \textit{hazard analysis} methods from safety engineering are well suited to anticipate a wide range of harms in ML-powered applications beyond traditional safety risks, and (2) LLM-supported automation can make hazard analysis more accessible and manageable in terms of effort. We believe that hazard analysis is particularly effective in the early development stages, helping identify and design controllers to mitigate harm.

% Software products that incorporate Machine learning (ML) components often produce unintended consequences due to their inherent uncertainty. For instance, they may unintentionally amplify biases\cite{bias_amplification_suresh2021framework} or introduce privacy risks\cite{privacy_risks_chen2021machine}. 
% However, the inability to predict these consequences before system deployment does not imply they are inherently unpredictable.
% By adopting systematic, structured analysis methods, practitioners can identify potential hazards, and accordingly modify and refine their system design in the early stages of the development process.

Recent research has explored more or less structured strategies to anticipate potential harms, primarily bias and fairness issues, in the context of ML impact assessment\cite{pagano2022bias,mehrabi2021survey,buccinca2023aha,wang2024farsight, microsoft_rai_template_2022}. However, these approaches are generally model-centric and applied ad-hoc \cite{jatho2022system}, overlooking controllers beyond the model such as safeguards, trend monitoring, human oversight, and user interface modifications -- areas where software engineers can contribute to responsible engineering of ML-powered systems, extending beyond model-focused considerations.

More recent research has adopted traditional safety engineering, especially hazard analysis methods such as \textit{Failure Mode and Effects Analysis (FMEA)} and \textit{System Theoretic Process Analysis (STPA)}, to proactively identify a broad range of ethical and social risks in ML models and ML-powered software systems\cite{khlaaf2022hazard, Dobbe_2022, martelaro2022exploring, rismani2023beyond, rismani2023plane, jatho2022system, rismani2024silos, rismani2021ai}. 
However, outside traditional safety-critical domains such as autonomous vehicles\cite{adler2016safety, abdulkhaleq2017using}, existing studies largely discuss only the \textit{potential} application of hazard analysis, with several emerging challenges:
First, traditional hazard analysis methods are limited by predefined system boundaries, restricting the consideration of a broader scope of risks and solutions.
Second, these methods require substantial time and effort, making them costly and challenging to integrate into fast-paced, continuous development workflows outside traditional safety-critical systems\cite{rismani2023beyond, martelaro2022exploring}.
Third, the responsibility for identifying and mitigating risks often falls to software developers or ML model creators, who may lack expertise in safety engineering, potentially reducing the effectiveness and thoroughness of these analyses\cite{martelaro2022exploring}.

In this paper, we provide a concrete illustration of how hazard analysis, with minor modifications, can proactively anticipate harms and, more importantly, guide the design of controllers to mitigate those harms before they occur at both the model and system levels. Furthermore, we show how LLMs can support this process by offering guidance to software engineers and data scientists with limited safety-engineering expertise, requiring only moderate efforts that align with the fast-paced development practices of ML-powered applications.

In summary, this position paper offers a fresh and critical perspective on using hazard analysis to broadly anticipate harms in ML-powered applications and to design system-level controllers, illustrated with a concrete example. We further contribute a discussion and demonstration of the potential of how LLMs can support developers in the hazard analysis process. 
We envision that a lightweight, LLM-supported hazard analysis process will become a routine step in the responsible engineering of ML-powered applications, enabling the mitigation of many harms well before they occur.

% In the remainder of this paper, we first review related work on risk assessment and mitigation in ML-integrated software systems, as well as safety engineering methods and their application to these systems in Section~\ref{section: Related Works}. In Section~\ref{section: Standard STPA}, we introduce the traditional hazard analysis framework STPA. In Section~\ref{section: Method}, we present our modified, partially automated STPA using a running example. In section~\ref{section: Discussion}, we 

\begin{figure*}[th]
\centerline{\includegraphics[width=0.95\linewidth]{Files/STPA.pdf}}
\caption{An overview of the STPA method, based on the textual description and figures in the STPA handbook\cite{STPA_Handbook_Stpa2018}.}
\label{figure: STPA method overview}
\end{figure*} 

\section{Scope and Related Work}\label{section: Related Works}

Though there are various definitions for AI and ML systems, in this paper, we use the term ``ML-powered system'' to denote any software system that incorporates ML models as a component, aiming to emphasize the system as a whole with ML model as an inherently `unreliable' module within it.

Practitioners and researchers have acknowledged the potential social risks posed by ML-powered systems\cite{bender2021dangers, weidinger2021ethical}, such as bias and insufficient oversight.
Technical approaches typically focus on measuring and addressing model-level issues, focusing on aspects such as fairness, shortcut reasoning, or privacy\cite{zemel2013learning, shokri2015privacy, bias_amplification_suresh2021framework, privacy_risks_chen2021machine}, often neglecting broader system or environmental considerations.
Recent research has introduced structured approaches and tools to help practitioners anticipate potential harms of ML-powered systems, ranging from impact assessment templates\cite{MicrosoftAITools} to tools that facilitate more or less structured brainstroming\cite{buccinca2023aha, wang2024farsight, kieslich2023anticipating, bogucka2024co}. While these methods effectively identify relatively explicit harms or hazards linked to the systemâ€™s overarching purpose, they fall short in analyzing the detailed components or control structures within these systems and are often used in an ad-hoc manner\cite{martelaro2022exploring}.

Safety engineering has a long history \cite{bahr2014system}, with traditional methods having been applied in various safety-critical domains, such as aviation systems\cite{leveson2004new, SAE_ARP4761A_2023} and autonomous vehicles \cite{adler2016safety, abdulkhaleq2017using}.
Recent research has applied these safety engineering methods, particularly the hazard analysis frameworks FMEA\cite{rismani2023plane, rismani2021ai} and STPA\cite{rismani2023beyond, rismani2023plane, jatho2022system, rismani2024silos}, to identify potential hazards and assess control structures in ML-powered systems. Findings suggest that such systematic approaches are not only effective in identifying a wider spectrum of hazards but are also potentially applicable throughout all development stages.
However, integrating safety engineering frameworks into the development cycles of ML systems is often impractical due to the extensive time and paperwork required \cite{rismani2023beyond, martelaro2022exploring}. Many practitioners lack in-depth safety engineering expertise, which can reduce the effectiveness of these frameworks \cite{martelaro2022exploring}. Furthermore, most existing studies still focus on the model and its development process, with limited attention to non-ML components and the social environment.
This paper demonstrates that hazard analysis is effective in anticipating a wide range of hazards and aids in proactive controller design. Furthermore, it shows that LLMs are promising in supporting humans conducting STPA by reducing effort and encouraging broader, more comprehensive thinking beyond the model.

\section{A Brief Introduction to STPA} \label{section: Standard STPA}

While safety engineering techniques date back to the 1950s\cite{safeware_10.1145/202709, safety_critical_computer_systems_10.5555/524721}, our approach builds on System-Theoretic Process Analysis (STPA), a state-of-the-art hazard analysis framework grounded in the System-Theoretic Accident Model and Processes (STAMP) \cite{Engineering_a_safer_world_Nancy2012}. We use STPA because it is designed for analyzing highly complex systems, can be applied from the early development stages, and considers both technical and human factors \cite{Engineering_a_safer_world_Nancy2012} -- all crucial for ML-powered software. 

Traditionally, STPA follows four steps, outlined in 
Figure~\ref{figure: STPA method overview}:

1)\textit{ Define the Purpose of the Analysis:} 
% The first step is to 
We list \textit{stakeholders} and their values, then identify important \textbf{losses} (e.g., loss of life). Based on the losses, we determine system-level \textbf{hazards}, which are conditions that may lead to a loss (e.g., aircraft too close to other objects). Hazards are then inverted into \textbf{constraints}, which are system conditions that need to be satisfied to prevent hazards. This way, STPA anticipates harms and establishes safety requirements to mitigate them.

2)\textit{ Model the Control Structure: } 
% In the next step, 
We outline the control structure designed to ensure the safety constraints are met. Each previously identified constraint targeted for resolution should have at least one associated \textbf{controller} (e.g., an automated safeguard or a human supervisor). New controllers can be envisioned for constraints without sufficient controllers. STPA particularly focuses on feedback-control loops between controllers and the controlled processes. 

3)\textit{ Identify \textbf{Unsafe Control Actions}: } 
% In the third step, 
We analyze each controller's potential failure modes by considering whether a hazard may arise if each control action is absent, incorrect, mistimed, executed in the wrong sequence, prolonged, or stopped prematurely. Control action errors that can lead to unsafe outcomes are then analyzed in the final step.

4)\textit{ Identify Loss Scenarios: } 
% In the final step of STPA, 
We examine why unsafe control actions may occur, potentially leading to revising existing controllers or introducing additional controllers (e.g., pilots may be unreliable in distance checking, hence we may introduce an automated warning system). The STPA process is then repeated with the modified control structure.

In a nutshell, STPA is a structured approach that works backward from harms (losses) to safety requirements (constraints) and then to mitigation strategies (controllers) and their potential failures. 
It encourages broad consideration of the control structure, including non-technical controllers like training, human oversight, and government regulations.
% STPA has effectively identified hazards in traditional safety-critical systems, like software faults causing unintended movement in the RAVEN II robotic surgical platform \cite{alemzadeh2015systems}.

Throughout the paper, we use safety engineering terminology for illustrations and discussions. Since this may be unfamiliar to most software engineers and data scientists, a summary and mapping are provided in Table~\ref{tab:terms}.

\begin{table}
    \centering
        \caption{Terminology of safety engineering and mapping to concepts in ML-powered application development}
    \label{tab:terms}
    \begin{tabular}{p{1.05cm}p{6.8cm}}
    \toprule
    \textbf{Concept} & \textbf{Explanation and mapping to ML-powered application}\\\midrule
        Loss & An event undesirable to a stakeholder, including bodily harm, allocation harms (e.g., discriminatory service), representation harms, and societal harms (e.g., polarization)  \\ \addlinespace[2pt]
        Hazard & A state or a set of conditions that may lead to a loss in certain scenarios, such as when an ML-powered software system produces faulty information \\\addlinespace[2pt]
        Constraint & A {(safety) requirement} that, when satisfied, prevents the hazard and thus the loss (e.g., an ML-powered system does not produce faulty information or warns the user if it does) \\\addlinespace[2pt]
        Controller & A technical or non-technical components that enforce constraints, including technical safeguards (e.g., exception handling, redundant sensors, I/O validation) and human oversight (e.g., processes, training, monitoring, alerting) \\\addlinespace[2pt]
        Unsafe Control Action & 
        A control action, or lack thereof, provided by a controller can, under certain conditions, lead to a hazard (e.g., output validation suppressing correct answers) \\
         \bottomrule
    \end{tabular}
\end{table}

STPA is a labor-intensive process needing expert involvement and is mainly used for safety-critical systems like aviation, focusing on severe losses such as loss of life. Researchers found STPA potentially useful, but its time-consuming nature is unsuitable for rapid development, and developers often lack the necessary expertise \cite{rismani2023beyond, martelaro2022exploring, rismani2023plane}.

\section{LLM-Supported Hazard Analysis to Anticipate Harms in ML-Powered Applications} \label{section: Method}

We argue that hazard analysis is effective in anticipating a wide range of harms in ML-powered applications, such as fairness, usability issues, and societal concerns like deskilling and polarization, extending beyond the traditional focus on severe harms such as loss of life in safety-critical systems.
Specifically, hazard analysis aids in \textit{anticipating} problems and \textit{designing} mitigation strategies in systems that might otherwise be released without any.
In this section, we walk through the four STPA steps using a running example of a simple web system designed to recommend outdoor trails.

Evidence suggests that routinely applying hazard analysis to everyday software applications as part of responsible engineering practices is challenging due to high costs and skill requirements. Therefore, we also present how LLMs, specifically GPT-4o by OpenAI \cite{GPT4_openai2023gpt4}, can assist software engineers and data scientists in conducting such analysis without requiring extensive training and excessive paperwork.

% To address the limitations of the standard STPA mentioned above, we modified the framework to assist practitioners with potential controller design. Our approach uses LLMs, specifically GPT-4o by OpenAI \cite{GPT4_openai2023gpt4}, to semi-automate the process, aiming to reduce the effort and expertise required for such analysis.

Our running example is inspired by the existing customized GPT assistant \textit{AllTrails} \cite{chatgpt_alltrails}. The application operates within the ChatGPT chat interface and is guided by system prompts from developers. It accesses the predefined \url{alltrails.com} API to fetch trail information. Upon receiving user prompts (e.g., ``Recommend trails near [district name] that are dog-friendly.''), it calls the API for information and provides a response. To our knowledge, it lacks additional controllers beyond its system prompt instructions and ChatGPT's built-in safeguards.
As is often the case, the LLM should be considered an unreliable component, as it may fail to follow instructions, hallucinate, or produce factual errors. We assume this application will be extended beyond its current form and expect it to gain significant popularity. Note that we explicitly chose a running example of an everyday ML-powered application, rather than a traditional safety-critical system, to illustrate how hazard analysis can proactively anticipate issues worth mitigating, even for such seemingly harmless applications.


{
% \begin{table}[htbp]
% \caption{Potential stakeholders in the running example}
% \begin{center}
% \begin{tabular}{ll}
% \toprule\textit{}
% \textbf{Category} & \textbf{Stakeholder} \\ \midrule
% {Users} & End Users \\ 
%  & Community Forums and Social Media Groups \\ 
%  & Travel and Adventure Bloggers/Influencers \\ \addlinespace

% {Product Team} & App Developers \\ 
%  & Data Analysts \\
%  & Marketing Teams  \\ 
%  & User Experience (UX) Designers \\ 
%  & Legal Advisors \\ 
%  & Technical Support Teams \\ \addlinespace

% {Service Providers} & Third-Party LLM Providers \\
%  & API Providers \\ 
%  & Feedback and Review Platforms \\ \addlinespace

% {Regulatory Entities} & Trail Management Organizations \\
%  & Local Governments and Parks Departments \\ \addlinespace


% {Related Industries} & Outdoor Gear Retailers  \\
%  & Local Businesses \\
%  & Training Institutions \\ \addlinespace

% {Social Organizations} & Environmental Organizations \\
%  & Health and Safety Organizations \\ \addlinespace

% Financial Stakeholders & Investors \\ \bottomrule
% \end{tabular}
% \label{table: potential stakeholders}
% \end{center}
% \end{table}
}

\subsection{Anticipating Losses and Hazards}
\textit{Stakeholders:} Following traditional STPA, we begin with identifying stakeholders and their values to assess system losses. To go beyond identifying only the most severe losses, we encourage a comprehensive exploration of stakeholders, including indirect ones, which extends the scope typically practiced in STPA.
In our running example, we identified a list of 20 potential stakeholders. This includes directly involved entities such as end users, app developers, and API providers, as well as indirectly affected ones like trail management organizations and local businesses.\footnote{For the complete list of prompts and results in all steps, please refer to the supplementary material:
\url{https://docs.google.com/spreadsheets/d/1GkdN9TBscGhqfyFNdLMsLRtFKbr3DqqNzkaTDvW-XAQ/edit?usp=sharing}}
Our experiments show that LLMs can generate a list of stakeholders based on an application description.  We have designed prompts to encourage LLMs to consider indirect and less common stakeholders, thereby fostering a broad exploration. Developers can then use this list as inspiration for selecting stakeholders to analyze.

\begin{table}[t]
\caption{Values and losses among different stakeholders (Selected)}
\begin{center}
\begin{tabular}{p{.17\linewidth}p{.34\linewidth}p{.34\linewidth}}
\toprule
\textbf{Stakeholder} & \textbf{Value} & \textbf{Loss} \\ \midrule
{End User} & Personalization & Lack of personalization \\
& Accuracy of information & Inaccuracy of information \\ \addlinespace[2pt]

\multirow{2}{0.95\linewidth}{App Developers} & Maintainability & High maintenance burden \\ 
& Cost efficiency & Increased costs \\ \addlinespace[2pt]

% \multirow{2}{0.95\linewidth}{API Providers} & Brand visibility & Reduced brand visibility \\ 
% & Revenue generation & Loss of revenue \\\midrule

\multirow{2}{0.95\linewidth}{Local Businesses} 
% & Brand promotion & Poor brand recognition \\
& Community engagement & Lack of community engagement \\ \bottomrule

\end{tabular}
\label{table: values and losses}
\end{center}
\end{table}

\textit{Losses:} For each stakeholder, we consider their values and convert them into corresponding losses. In our running example, we identified 145 losses, averaging 6-8 losses per stakeholder; a selection of these is shown in Table~\ref{table: values and losses}.
Again, we found LLMs helpful for suggesting values and losses, given the broad scope and the large number of losses they identified.

\textit{Hazards and Constraints:} For each loss, we identify hazards -- states or conditions that can lead to the loss under certain circumstances. We consider hazards beyond traditional STPA system boundaries. For example, the hazard ``system does not support user customization'' may lead to the loss ``lack of personalization''. Given a large number of losses and to avoid prematurely focusing on only a few, we find that LLMs effectively support analyzing all losses in a few minutes for under USD 2.
In our running example, we identified 1,159 hazards for 145 losses across 20 stakeholders, averaging 5-10 hazards per loss.
While some hazards are obvious, such as incorrect model outputs, we discovered many others we would not have anticipated without such systematic analysis, such as \textit{``System recommends trails in ecologically sensitive areas or during hazardous conditions.''}

\begin{table}[t]
\caption{System-level hazards (Selected)}
\begin{center}
\begin{tabular}{p{.5cm}p{7.5cm}}
\toprule
\textbf{HID} & \textbf{Hazard} \\ \midrule
% H2 & System encounters language model misinterpretations, outputs irrelevant, inappropriate, or offensive content. \\ 
H4 & System does not personalize recommendations effectively, support user customization, or adapt to user preferences and trends. \\ 
% H12 & System receives incomplete, outdated, or inaccurate data from external APIs and third-party sources. \\ 
H24 & System recommends trails in ecologically sensitive areas or during hazardous conditions. \\ 
H39 & System lacks a centralized dashboard or knowledge repository for monitoring performance and user feedback. \\
H48 & System lacks a mechanism for users to withdraw consent or access their data records. \\ \bottomrule
\multicolumn{2}{l}{HID: Hazard ID}
\end{tabular}
\label{table: hazards}
\end{center}
\end{table}

At this stage, LLMs also effectively aid in merging similar hazards identified from different losses. In our running example, with human supervision for granularity, we reduced the hazards to 50 distinct ones, as examples shown in Table~\ref{table: hazards}.



\begin{table}[t]
\caption{Controller designs (Selected)}
\begin{center}
\begin{tabular}{p{0.035\linewidth}p{0.87\linewidth}}
\toprule
\textbf{HID} & \textbf{Controller Design}\\ \midrule
% \multirow{3}{*}{H2} & L2-1 & Implement a content filtering system to automatically detect and block inappropriate or offensive content before it reaches the user. \\ 
% & L2-2 & Introduce a feedback loop where users can report inappropriate or irrelevant content. \\ 
% & L2-8 & Collaborate with the third-party provider to regularly update and refine the language model, addressing any known issues or biases. \\ \midrule

H4  & Integrate a feedback loop where users can provide feedback on trail recommendations, allowing the system to improve personalization. \\ \addlinespace[2pt]
 & Enable users to connect their social media or fitness tracking accounts for personalized recommendations. \\ 
% & C4-10 & Develop a robust error-handling and logging system to identify and rectify any issues in the recommendation process.\\ 
\addlinespace

% \multirow{3}{*}{H12} & L12-1 & Implement data validation and verification processes to check the accuracy and completeness of data received from external sources. \\ 
% & L12-6 & Implement machine learning algorithms to detect anomalies or inconsistencies in the data, flagging them for review. \\
% & L12-7 & Provide transparency to users by indicating the source and date of the data, allowing them to make informed decisions.\\ \midrule

% \multirow{3}{*}{H24} & L24-3 & Collaborate with environmental organizations to maintain an updated database of ecologically sensitive areas and restricted zones. \\ 
% & L24-6 & Provide user training on how to interpret trail recommendations, emphasizing the importance of checking local conditions and regulations. \\ 
% & L24-8 & Implement a warning system that alerts users when they are about to select a trail in a potentially sensitive or hazardous area. \\ \midrule

H39 & Develop a centralized dashboard that aggregates data from the LLM and external APIs to monitor system performance in real time.\\ \addlinespace[2pt]
  & Introduce automated alerts in the dashboard to notify administrators of any performance issues or anomalies detected in the system. \\ 
% & C39-7 & Integrate machine learning algorithms into the dashboard to predict potential issues based on historical data and user feedback.\\ 
\addlinespace

H48  & Implement a user-friendly interface that allows users to easily withdraw consent for data collection and processing at any time. \\ \addlinespace[2pt]
 & Design the system to automatically notify users of any changes to data handling practices and obtain renewed consent if necessary.\\ 
% & C48-7 & Implement technical controllers to ensure that data access requests are processed promptly and securely.\\
\bottomrule
\multicolumn{2}{l}{HID: Hazard ID}
\end{tabular}
\label{table: controllers}
\end{center}
\end{table}

\begin{figure}[t]
\centerline{\includegraphics[width=\linewidth]{Files/Control_Structure.pdf}}
\caption{The control structure of the system: The bold arrow represents the control action analyzed in Section~\ref{section: analyzing and revising controllers}, involving data scientists upgrading or downgrading the LLM version or altering the prompts.}
\label{figure: Control Structure}
\end{figure}

\begin{table*}[t]
\caption{Unsafe control actions and loss scenarios (Selected)}
\begin{center}
\begin{tabular}{p{0.47\linewidth}p{0.47\linewidth}}
\toprule
\textbf{Unsafe Control Actions} & \textbf{Loss Scenario} \\ \midrule
% Lack of control action might lead to negative consequences if the current model or prompts fail to deliver accurate trail recommendations, potentially causing user dissatisfaction and reducing system effectiveness.
 % & The automated monitoring system may lack detailed information on the LLM's behavior, leading data scientists to miss crucial insights into its failure to provide adequate trail recommendations. \\ \midrule
Premature control actions, like upgrading the model without thorough analysis or confirmation, can destabilize operations or reduce user satisfaction. & Data scientists might overly rely on new model version improvements without properly assessing their impact on system interactions with APIs. \\ \addlinespace[2pt]
Delayed control actions can cause inefficiency, poor performance, user frustration, and a negative experience.
& Data scientists may face data overload, insufficient tools, or cognitive strain, hindering decision-making and delaying issues without clear prioritization.\\ \bottomrule
\end{tabular}
\label{table: UCA & Loss Scenarios}
\end{center}
\end{table*}

\subsection{Proactive Design of Control Structure}
Since the existing AllTrails application had few controllers, as is common in many ML-powered applications, there was not much to analyze regarding the existing control structures in STPA steps 2--4. However, the STPA process is also effective for \textit{proactively envisioning new controllers} to mitigate the identified hazards, aiding practitioners in \textit{modeling the control structure}, which is the second step in STPA.

We consider potential controllers or design structures that (a) prevent the hazard, (b) reduce its likelihood, (c) decrease the probability of it causing a loss, and (d) minimize the severity of the loss if it occurs.
In our running example, aided by LLMs, we identified 10 to 15 possible designs per hazard.
While some controllers may be impractical, costly, or insufficiently effective, our systematic approach revealed some easy-to-implement and potentially effective ideas we might not have otherwise considered, such as connecting social media or fitness tracking accounts for personalized recommendations.
Among the identified hazards, we chose to explore controllers for three hazards: H4, H39, and H48. These pertain to the lack of personalization, absence of system monitors, and inability for users to withdraw consent, respectively. Some potential controllers are listed in Table~\ref{table: controllers}. Having assessed the risks and costs, we adopted, modified, and merged several controller designs suggested by the LLMs. The resulting control structure incorporates human feedback and hierarchical monitoring mechanisms, which would be challenging to develop without this process. We then performed the second step of STPA and visually modeled the control structure of the system, as illustrated in Figure~\ref{figure: Control Structure}.

Again, we found that LLMs can help consider potential designs, especially when given instructions to explore different designs. However, developers remain crucial in making judgments about relevance and importance.

\subsection{Analyzing and Revising Controllers}~\label{section: analyzing and revising controllers}
After designing and modeling the control structure, we now return to steps 3 and 4 of the STPA process to analyze whether the controllers are sufficient or if they introduce new problems.

We identified \textit{unsafe control actions} by examining each control action against a checklist for potential constraint violations if the action was (a) absent, (b) incorrect, (c) mistimed, (d) executed in the wrong sequence, (e) prolonged, or (f) stopped early. We then identify potential \textit{loss scenarios}, which are states or events that may lead to the unsafe control action.

In our running example, we analyze the control action where data scientists modify the LLMs that call APIs, as depicted by the bold arrow in Figure~\ref{figure: Control Structure}.
Some identified unsafe control actions and corresponding loss scenarios are listed in Table~\ref{table: UCA & Loss Scenarios}. We found LLMs effective in systematically exploring different problems by following the checklist.

{
% \begin{table*}[t]
% \caption{Unsafe Control Actions}
% \begin{center}
% \begin{tabularx}{\textwidth}{cX}
% \toprule
% \textbf{UCA ID} & \textbf{UCA} \\ \midrule
% \multirow{2}{*}{UCA1} & Not providing the control action could lead to negative consequences if the existing model version or prompts fail to deliver accurate or sufficient trail recommendations, potentially causing user dissatisfaction and reducing the system's effectiveness. \\ \midrule
% \multirow{2}{*}{UCA2}& Providing the control action could lead to negative consequences if the upgraded or downgraded model version or changed prompts reduce the accuracy or reliability of the responses, restrict access to needed features, or introduce new compatibility issues with the third-party API. \\  \midrule
% \multirow{2}{*}{UCA3} & If the control action is provided too early, such as upgrading the model before analyzing sufficient data or identifying the actual need for a change, it can result in premature adjustments that might destabilize current operations or degrade user satisfaction. \\  \midrule
% \multirow{2}{*}{UCA4} & Providing a potentially safe control action too late could lead to negative consequences, such as prolonging system inefficiency or poor performance, leading to user frustration and impacts on the overall user experience. \\ \bottomrule
% \end{tabularx}
% \label{table: UCAs}
% \end{center}
% \end{table*}
}

% As our running example, we consider the control action in which data scientists modify the LLMs that call APIs to retrieve information. Specifically, this action involves data scientists upgrading or downgrading the model version of the LLM or altering the prompts used to interface with the API.


{
% \begin{table*}[htbp]
% \caption{Loss Scenarios (Selected)}
% \begin{center}
% \begin{tabularx}{\textwidth}{cX}
% \toprule
% \textbf{UCA ID} & \textbf{Loss Scenario} \\ \midrule
% \multirow{4}{*}{UCA1} & Data scientists may incorrectly analyze the LLM's performance data or receive biased feedback, leading to the wrong decision about whether to upgrade, downgrade, or modify the model or prompts. \\ 
%  & The automated monitoring system may not be collecting enough detailed information about the LLM's behavior, causing the data scientists to miss crucial insights into how and why the LLM fails to deliver adequate trail recommendations. \\ \midrule
% \multirow{2}{*}{UCA2 / UCA3} & Data scientists might rely heavily on the purported improvements of new model versions without a comprehensive evaluation of their impact on the systemâ€™s interaction with third-party APIs. \\ \midrule
% \multirow{4}{*}{UCA4} & The communication loop between data scientists and the development team can lead to delays. If data scientists take time analyzing monitoring data or if the development team is slow to implement changes, this can postpone recommendations or updates to the LLM. \\ 
% & Data scientists may face data overload, lack the right analysis tools, or cognitive strain, which can hinder decision-making. Without clear issue prioritization, critical problems may be delayed.\\ \bottomrule
% \end{tabularx}
% \label{table: Loss Scenarios}
% \end{center}
% \end{table*}
}

% For each UCA, given the description of the surrounding control structure, we use LLMs to explore potential reasons for the occurrence of this UCA. In our running example, a selection of the loss scenarios is presented in Table~\ref{table: Loss Scenarios}.

% As the final outcome of the STPA, practitioners can review these loss scenarios and iteratively refine their control structure designs to address the identified hazards.

Practitioners can now review these unsafe control actions and loss scenarios to decide whether and how to revise certain controllers. For example, our initial monitoring approach was found to be naive, lacking an explicit structure to ensure that an operator regularly checks the dashboard, effectively detects issues, and does not cease monitoring due to notification fatigue. This encourages establishing explicit procedures for (a) setting up automated alerts, (b) implementing an engineering process for tracking and eliminating false positive alerts, and (c) designating someone responsible for monthly check-ins with the operator to ensure accountability. This approach goes far beyond simple engineering interventions such as exception handling, redundancy, or using an LLM to review the output of another LLM\cite{shankar2024validates}, instead considering the entire socio-technical system and its environment.

\section{Discussions and conclusion} \label{section: Discussion}

While others have advocated adopting safety engineering to anticipate harms in ML-powered applications \cite{khlaaf2022hazard, Dobbe_2022, martelaro2022exploring, rismani2023beyond, rismani2023plane, jatho2022system, rismani2024silos, rismani2021ai}, this paper reinforces this idea and provides a concrete example of the process, demonstrating its potential to anticipate harms beyond traditional safety concerns like bodily harm and mission loss.
Although our running example appears simple --merely combining a system prompt with an API on OpenAI's customized GPTs platform -- the analysis identified potential harms ranging from minor and far-fetched to worth addressing.
This encouraged comprehensive early system design to mitigate these harms.
We advocate that developers of novel ML-powered applications should undertake this practice, even if the application appears harmless. There is no justification for not attempting to anticipate the ``unintended consequences,'' given the uncertainty, potential bias, and possible shortcuts associated with ML components in software systems.

However, we also experienced firsthand how quickly the analysis can become unmanageable, especially when expanding beyond severe losses in traditional safety-critical systems. We analyzed dozens of stakeholders, hundreds of potential losses, and thousands of hazards. This scope can easily grow to include even more potential controllers and associated issues. Without technical support, this approach is overwhelming and can feel like a bureaucratic paper-heavy compliance activity, raising questions about how to encourage its routine adoption as part of everyday responsible engineering practices.

\paragraph*{Assisting Developers vs. Automating Compliance Activity} LLMs effectively assist in navigating STPA steps and extensively exploring stakeholders, losses, hazards, and controllers, helping to scale the process beyond just the severe losses. However, there is a risk of over-reliance on LLMs for fully automating hazard analysis. We envision LLMs as support tools, not replacements for developers' critical judgment. While LLMs can produce ideas and explore broadly by following checklists, developers are essential for thinking beyond suggestions, assessing severity, and setting the analysis focus. Balancing LLM support while preserving human creativity and judgment is a crucial question for future research.

\paragraph*{Expertise Requirements} Automated assistance in hazard analysis can enhance accessibility by offering step-by-step guidance and examples to developers lacking safety engineering training. However, there is a risk that these developers may perform a more superficial analysis. 
Ideally, increased accessibility enables selective engagement of safety experts, allowing their expertise to scale across more projects.
Balancing this requires further research, potentially drawing insights from similar discussions in software security \cite{Howard2003, Howard2006} and democratizing data science \cite{singh2022automated, drozdal2020trust}.

\paragraph*{Cataloging Common Controllers} ML research and much software engineering research on ML primarily focus on the model -- to improve accuracy, measure fairness, explain model internals, or rectify shortcuts. In contrast, system-level mitigations such as safeguards, user interface design, and human oversight receive less attention, despite their effectiveness as controllers \cite{Kastner2025}. Hazard analysis promotes broader system thinking, and we believe a pattern catalog of common system-level interventions could foster discussions and be embedded into the process of identifying and designing controllers.

\paragraph*{Fostering Adoption} Interview studies indicate practitioners hesitate to integrate hazard analysis into their agile and exploratory workflows, citing the need for organizational changes such as increased incentives, managerial confidence, support, understanding, and resource investment \cite{martelaro2022exploring, rismani2023plane}. Further research is required to embed hazard analysis into routine engineering, even in non-safety-critical contexts where success is a lack of issues. 
Lowering effort and demonstrating value, while appealing to developers' responsibility and mastery, may effectively change practices and culture. While much work remains, we can learn from past efforts such as DevOps\cite{luz2019adopting}, establishing a quality or security culture\cite{wiegers1996creating}, adopting fairness audits\cite{rakova2021responsible}, promoting social responsibility through education\cite{hironimus2009sociological}, and broadly altering organizational culture\cite{schein2010organizational}.
Technological innovations, education, and activism can each promote more responsible engineering practices.

% CT: outline some of your research questions / directions for this line of work
% - how do you engage the practitioner in the process? (HCI angle)
% - does this approach make non-experts better?
% - does it make experts more productive? worse?
% - can we take practitioner input to refine the process and achieve better alignment? (i.e., active learning)
\section*{Acknowledgment}

We thank Nadia Nahar, Laurie Williams, William Enck, and Alexandros Kapravelos for their feedback on this work.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
