\section{Proofs from Section \ref{sec:cabinets}} \label{app:cabinets}

This section contains proofs from Section \ref{sec:cabinets} on the Pandora's Cabinets model.

\begin{proof}[Proof of Lemma \ref{lemma:cabinets-opt-upper}]
	
	
	
$\OPT$ induces on each bandit $\M_i^{j^*(i)}$ that it interacts with some policy\footnote{In general, $\pi_i^{j^*(i)}$ may be in the form of a distribution over policies; we make the notion of an induced policy fully explicit and formal in Section \ref{sec:dags}.} that we denote $\pi_i^{j^*(i)}$.
	Then $\Welfofon{\OPT}{\calI} = \sum_{i=1}^n \Perfofon{\pi_i^{j^*(i)}}{\M_i^{j^*(i)}}$.
	We have:
	\begin{align*}
		\E [ \Perfofon{\pi_i^{j^*(i)}}{\M_i^{j^*(i)}} ]
		&\leq \E[ \Bof{\pi_i^{j^*(i)}} \kappa_i^{j^*(i)} ]  & \text{Lemma \ref{lemma:magic-multistage}} \\
		&=    \E[ \Aofon{\OPT}{i} \kappa_i^{j^*(i)} ]  \\
	\end{align*}
	So $\E[ \Welfofon{\OPT}{\calI} ] \leq \E[ \sum_{i=1}^n \Aofon{\OPT}{i} \kappa_i^{j^*(i)}] \leq \Welfofon{\OPT'}{\calI'}$.
	The last follows because $\OPT$ can do no better than iteratively open drawers and observe and obtain, on net for an opened drawer $j^*(i)$, the realization of $\kappa_i^{j^*(i)}$; this is the space of strategies available to $\OPT'$.
	\end{proof}

\begin{proof}[Proof of Lemma \ref{lemma:cabinets-alg-lower}]
	$\ALG'$ is local monotone without loss of generality (Observation \ref{obs:monotone}).
	So on each arriving cabinet $i$, given its current solution set $F$, $\ALG'$ selects a drawer $j(i)$, possibly randomly, and chooses a threshold $T_i^j$ a function of prior selections.
	It then claims $i$ if $\kappa_i^{j(i)} > T_i^j$, and claims $i$ with some probability $p_i^j \in [0,1]$ if $\kappa_i^{j(i)} = T_i^j$.
	
	We construct $\ALG$ to open the same drawer $j(i)$.
	Given the bandit $\M_i^j$ with, in particular, reward function $V_i^j$, it then flips a coin:
	\begin{itemize}
		\item With probability $p_i^j$, $\ALG$ advances the bandit while the current state has index $\sigma_s \geq T_i^j$, including claiming if $s$ is a sink state.
		\item With probability $1-p_i^j$, $\ALG$ employs the same strategy but with the requirement $\sigma_s > T_i^j$.
	\end{itemize}
	$\ALG$'s policy on cabinet $i$ drawer $j$, $\pi_i^j$, is efficient to compute and is nonexposed (Definition \ref{def:exposed}).
	So by Lemma \ref{lemma:magic-multistage}, its performance is $\E[\Perfofon{\pi_i^j}{\M_i}] = \E[ \Bofst{\pi_i^j}{i}  \kappa_i^j]$, where $\Bofst{\pi_i^j}{i}$ is the indicator that $\pi_i^j$ claims when run on $\M_i^j$.
	Since both algorithms claim $i$ after opening $j$ always when $\kappa_i^j > T_i^j$, and with probability $p_i^j$ when $\kappa_i^j = T_i^j$, so they have the same expected performance.
    Computing Weitzman indices is efficient by backward induction, so $\ALG$ is efficient as long as computing the thresholds is efficient as a function of the current selection $F$ and the prior selections' values $\{\kappa_{i'}^{j(i')} : i' \in F\}$.

	If $\ALG'$ is \SAUP-based, $j(i) = \argmax_{j'}\E(\kappa_i^{j'}-T_i^{j'})^+$.
	We evaluate the expected performance of any choice of drawer $j'$ by $\ALG'$:
	\begin{align*}
		\E(\kappa_i^{j'}-T_i^{j'})^+ &=  \E\B_i^{j'}(\kappa_i^{j'}-T_i^{j'})\\
		&=  \E(\B_i^{j'}\kappa_i^{j'}-\B_i^{j'}T_i^{j'})\\
		&=  \E(\Perfofon{\pi_i^{j'}}{\M_i}-\B_i^{j'}T_i^{j'}).
	\end{align*}
	For the maximizing choice of $j(i)$, $\B_i^{j(i)} = \B_i^{\pi_i^{j(i)}}$, so $j(i)$ is also the \SAUP{} choice in Pandora's Cabinets$(\F)$ and $\ALG$ is \SAUP-based.
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:cp-pc}]
  Let $\ALG'$ be an online algorithm for the Cabinets-Prophets instance.
  If $\ALG'$ guarantees a prophet inequality $\alpha$, then it follows from Lemmas \ref{lemma:cabinets-opt-upper} and \ref{lemma:cabinets-alg-lower} that for all instances $\mathcal{I}$, we have $\E[ \Welf{\ALG}(\mathcal{I}) ] \geq \alpha \E[ \Welf{\OPT}(\mathcal{I}) ]$ as desired.

  If $\ALG'$ is \SAUP-based, the constructed algorithm $\ALG$ is \SAUP-based, by Lemma \ref{lemma:cabinets-alg-lower}.
\end{proof}


