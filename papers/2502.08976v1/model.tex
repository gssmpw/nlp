\section{Preliminaries and Framework} \label{sec:model}

\subsection{Notation and common setting}

\subsubsection{Notation, set systems, feasibility.}
We write $[n]$ for $\{1,\dots,n\}$ and $(x)^+$ for $\max\{x,0\}$.
For an event $E$, let $\Indic{E}$ be its indicator random variable, i.e. $\Indic{E} = 1$ if $E$ occurs and $0$ otherwise.
$\Delta_{S}$ is the set of probability distributions on the set $S$.
Throughout, an algorithm is \emph{efficient} if it runs in time polynomial in the size of its input, and is an \emph{FPTAS} to a maximization problem if, in time polynomial in the input size and $\tfrac{1}{\epsilon}$, outputs a solution within a $1-\epsilon$ factor of the optimum.

We consider several settings with the following common framework.
The input is an ordered list of $n$ objects.
An algorithm eventually selects a feasible subset, i.e. a subset in a given family $\F \subseteq 2^{[n]}$.
Such an $\F$ is \emph{downward-closed} if $F \in \F, F' \subseteq F \implies F' \in \F$.
A \emph{matroid} is a set system $\F$ that is nonempty, is downward-closed, and satisfies the exchange property, namely if we have $F,F' \in \F, |F| > |F'|$, then there exists $i \in F \setminus F'$ such that $F' \cup \{i\} \in \F$.
Throughout, we use the indicator variable $\Aofon{\ALG}{i}$ to denote the event that $\ALG$ adds item $i$ to its selection $F$, which we call \emph{claiming} $i$.

Given a set system $\F$ and an algorithm that selects subsets of $[n]$, we say it is \emph{feasible} or \emph{ex-post feasible} if with probability one, its selection $F$ satisfies $F \in \F$.
Also define (see e.g.~\citep{lee2018optimal}):
\begin{definition}[$\PF$, ex-ante feasible] \label{def:ex-ante}
	Given $F \subseteq [n]$, let $1_F \in [0,1]^n$ denote the indicator vector for $F$, i.e. $1_F(i) = \Indic{i \in F}$.
	Define $\PF$ to be the convex hull of $\{1_F : F \in \F\}$.
	An algorithm is termed \emph{ex-ante feasible} if its solution set $F^*$, a random set taking values in $2^{[n]}$, satisfies that the vector $Q := (\Pr[i \in F^*] : i \in [n])$ is in $\PF$.
\end{definition}
The randomness is taken over any randomness of nature in the input instance as well as the algorithm's own random decisions.
By definition, $Q \in \PF$ if and only if there is some distribution over feasible sets such that $Q_i$ is the probability of including $i$.
For example, in the case of the $1$-uniform matroid (at most one element can be selected), an ex-ante feasible but not ex-post feasible algorithm is to add each item to $F^*$ independently with probability $\frac{1}{n}$.

\subsubsection{Welfare, approximation ratios, prophet inequalities.}
In each setting we consider, there will be defined a maximization objective termed welfare due to economic applications of the problem.
In each setting, the random variable $\Welfofon{\ALG}{\calI}$ will denote the welfare of an algorithm $\ALG$ run on an instance $\calI$.
For simplicity, we may write $\Welfof{\ALG}$ and leave $\calI$ implicit.
$\ALG$ has an \emph{approximation ratio $\alpha$} if for all ex-post feasible algorithms $\OPT$ and all instances, $\E[\Welfof{\ALG}] \geq \alpha \E[\Welfof{\OPT}]$.
If the same holds for all ex-ante feasible $\OPT$, then $\ALG$ has an \emph{ex-ante approximation ratio $\alpha$}.

In the \emph{online} setting, the objects arrive one at a time in known order $i=1,\dots,n$.
An online algorithm must make an irrevocable decision of whether to include $i$ in its solution set before $i+1$ arrives, for each $i$.
If there exists an online algorithm with approximation ratio (respectively, ex-ante approximation ratio) $\alpha$, we say the problem has a \emph{prophet inequality} (respectively, \emph{ex-ante prophet inequality}) $\alpha$.
We say a prophet inequality is \emph{efficient} if it is achieved by a polynomial-time algorithm.

An online algorithm in particular is \emph{non-adaptive}: it processes the objects in a fixed order.
If a problem has a prophet inequality $\alpha$, then it also is said to have an \emph{adaptivity gap} of $1/\alpha$.
All of our prophet inequality results immediately imply the corresponding adaptivity gaps.




\subsection{Markov Search Processes} \label{subsec:MSP}

We define a \emph{Markov Search Process (MSP)} as a tuple $\M = (S, A, P, C, V)$.
$S$ is a finite set of states and $A = (A^s : s \in S)$.
Each $A^s$ is the finite set of legal actions at state $s$.
We assume that $S$ is nonempty and that one state $s^* \in S$ is designated as the \emph{start state}.
For convenience, let $\Actions$ be the set of all possible actions.
The transition function is $P: S \times \Actions \times S \to [0,1]$, where we write $P(s';a,s)$ for the probability of transitioning to state $s'$ given that the process is currently in state $s$ and action $a \in A^s$ is taken.
We require that $\sum_{s' \in S} P(s';a,s) = 1$ for all $s \in S, a \in A^s$.
The cost function is $C: \Actions \times S \to \reals_{>0}$, where $C(a,s)$ is the cost incurred for taking action $a \in A^s$ when the current state is $s$.\footnote{If $a \not\in A^s$, then the values of $P(s';a,s)$ and $C(a,s)$ are irrelevant.}
We assume all costs are strictly positive.
Finally, the reward function is $V: S \to \reals_{\geq 0}$, representing the reward available at state $s$.

Given an MSP $\M$, we assume that its \emph{state graph}, where the nodes are $S$ and there is an edge $(s,s')$ if $P(s';a,s) > 0$ for some $a \in A^s$, is a directed acyclic graph (DAG).
In particular, a \emph{sink} of the MSP is a state $s$ with $|A^s|=0$, i.e. a sink in the state graph.
We assume that rewards are only available on sinks, i.e. $V(s) = 0$ unless $s$ is a sink.


\paragraph{Policies and transcripts.}
Given an MSP $\M = (S,A,P,C,V)$, a \emph{policy} begins in state $s^* \in S$.
When in any state $s$, the policy may either halt (represented as a choice $\bot \not\in \Actions$) or select an action $a \in A^s$, incurring a cost $C(a,s)$ and observing the transition to a new state $s'$ drawn according to $P(s';a,s)$.
The policy eventually chooses to halt in some state $s \in S$ and, when halting, optionally may claim the reward $V(s)$.
We refer to taking an action or claiming a reward $V(s)$ as \emph{advancing} the MSP.

A \emph{transcript} on $\M$ is a list of pairs $((a^1,s^1),\dots)$, and is \emph{valid} if for all $t$, $a^t \in A^{s^{t-1}}$ and $P(s^t;a^t,s^{t-1}) > 0$.
Here, we define $s^0 := s^*$, the start state.
We assume unless otherwise noted that all transcripts discussed are valid.
The transcript may optionally end with either the pair $(\bot, \text{claim})$ or else the pair $(\bot, \text{don't claim})$, denoting that the process has halted and been claimed or not respectively.
In either case, the transcript is \emph{halted}.
We assume that every policy eventually halts.
In some places, we extend the definition of transcript to a \emph{partial transcript} that begins in any state $s^0 \in S$, and is otherwise valid.

Formally, a \emph{deterministic policy} for an MSP $\M$ is a function $\pi$ that, given a non-halted transcript $r = (\dots,(a^t,s^t))$, specifies the next action $a^{t+1} = \pi(r) \in A^{s^t}$.
A \emph{distribution over deterministic policies}, also just called a \emph{policy}, is a probability distribution over the set of deterministic policies.
A special case of a distribution over policies is a \emph{randomized policy}\footnote{A randomized policy can be interpreted as the following distribution over deterministic policies: in advance, randomly and independently draw a choice of action for every possible state; then follow these choices deterministically.}: one where, for each given transcript, the next action is drawn from a distribution independently of all other randomness.
We abuse notation by using $\pi$ to refer to any policy (i.e. any distribution over deterministic policies).

When running a policy $\pi$ on an MSP $\M$, it produces a transcript $r$, a random variable.
We define the indicator random variable $\Jof{\pi}{a}{s} = 1$ if process $\pi$ reached state $s$ and took action $a$ when run on $\M$.
Similarly, $\Bofst{\pi}{s} = 1$ if $\pi$ halted in state $s$ and claimed.
We write $\Bof{\pi} = \sum_{s \in S} \Bofst{\pi}{s}$ for the indicator that $\pi$ claims.

The \emph{performance} of a policy $\pi$ on $\M$ is the net value claimed minus costs incurred, the random variable
\begin{align*}
	\Perfofon{\pi}{\M}
	&= \left( \sum_{s \in S} \Bofst{\pi}{s} V(s) \right) - \left( \sum_{s \in S} \sum_{a \in A^s} \Jof{\pi}{a}{s} C(a,s) \right) .
\end{align*}





\subsection{Bandits} \label{subsec:multistage}

A special case is if the only options at each state are to advance or halt.
We call this case a \emph{bandit}.
\begin{definition}[Bandit] \label{def:bandit}
  An MSP $\M = (S,A,P,C,V)$ is a \emph{bandit process} if $|A^s| \leq 1$ for all $s \in S$.
\end{definition}
A bandit MSP is identical to a \emph{nested} or \emph{multi-stage} Pandora's Box~\citep{kleinberg2016descending}.
We will make heavy use of Weitzman indices and related tools for bandits to solve our general problem.
We introduce the tools now.\footnote{Our presentation is closer to \citet{bowers2024matching} than \citet{kleinberg2016descending}, which considers a more general countably-infinite case.}


Given a bandit process $\M = (S,A,P,C,V)$, the \emph{Weitzman index} $\sigma_s$ and \emph{capped value} $\kappa_s$ of each state $s$ are defined via backward induction as follows.
Given $s \in S$, we let $r_s$ denote the random halted transcript produced by beginning in state $s$ and advancing the bandit until it halts, including claiming.
In particular, for a non-sink $s$, we use $n(s)$ denote the next state visited.
$\kappa_s$ will be a random variable depending on the entire transcript $r_s$, while $\sigma_s$ will be a constant.
\begin{itemize}
	\item For any sink state $s$, define $\sigma_s := V(s)$ and $\kappa_s := V(s)$.
	\item For any non-sink state $s$, define $\sigma_s$ to be the unique value satisfying
	\[ \E_{r_s} \left[ \left(\kappa_{n(s)} - \sigma_s\right)^+ \right] = C(s) . \]
	Define $\kappa_s := \min \{ \sigma_s, \kappa_{n(s)} \}$.
\end{itemize}
We often are concerned with $\kappa_{s^*}$, the \emph{capped value of the process}, equal to the minimum of $\sigma_s$ over all $s$ visited by advancing the bandit fully.
Therefore, we will write $\kappa = \kappa_{s^*}$ for short, and we may abuse notation later by, for example, writing $\kappa_i$ to denote the capped value of a process $\M_i$, and so forth.
We emphasize that $\kappa$ is a random variable that is a function of the entire transcript after visiting $s$.
It turns out that $\kappa$ can represent an amortized version of the expected net utility (value minus cost) available, but only if a policy satisfies the following beneficial criterion.
Intuitively, the idea of the criterion is that one should not abandon a process if it is currently in an ``optimistic'' state with a high Weitzman index compared to previous steps.
In this case, failure to advance is termed an \emph{exposure}~\citep{kleinberg2016descending}.


\begin{definition}[Non-exposed] \label{def:exposed}
	Let $\M = (S,A,P,C,V)$ be a bandit process.
	A transcript of the form $r = (\dots,(a^t,s^t),(\bot,\text{don't claim}))$ constitutes an \emph{exposure} on $\M$ if there exists $t'$ with $0 \leq t' < t$ such that $\sigma_{s^{t'}} < \sigma_{s^t}$.
	A policy is \emph{exposed} on $\M$ if it has a positive probability of exposure, and otherwise is \emph{non-exposed}.
\end{definition}

\begin{lemma}[\citet{kleinberg2016descending,bowers2024matching}] \label{lemma:magic-multistage}
	For any bandit MSP $\M$ and any policy $\pi$,
	\[ \E \left[ \Perfofon{\pi}{\M} \right] \leq \E \left[ \Bof{\pi} \kappa \right] , \]
	with equality if and only if $\pi$ is non-exposed on $\M$.
\end{lemma}

\paragraph{Illustrating nonexposure: prophets for bandits.}
Non-exposure is nicely compatible with threshold-based and descending price-based algorithms~\citep{kleinberg2016descending,singla2018price}, a vital fact for our results.
To illustrate the connection, consider the classic prophets problem (see Section \ref{subsec:prophet-framework}) of choosing one of $n$ random variables arriving sequentially.

Now suppose each arrival is a bandit process $\M_i$, which has associated an amortized value or ``capped'' value $\kappa_i$.
We can use a threshold rule $\tau$ on the capped values as follows: Advance the current bandit $\M_i$ while the index of the current state satisfies $\sigma_{i,s} \geq \tau$, including claiming the item if the revealed reward satisfies $V(s) \geq \tau$; but if at any time the current index satisfies $\sigma_{i,s} < \tau$, halt and discard this arrival.

At each time, the minimum of the indices $\sigma_{i,s}$ observed so far provides an upper bound on $\kappa_i$, which is the minimum over all past and future indices.
One can check that this policy claims if and only if $\kappa_i \geq \tau$, and is non-exposed, meaning by Lemma \ref{lemma:magic-multistage} that its expected welfare equals the expected sum of the $\kappa_i$ that it claims.
Therefore, it has mimicked the behavior of a classic prophets algorithm setting a threshold $\tau$, where the arriving random variables are $\{\kappa_i\}$.
Moreover, also by Lemma \ref{lemma:magic-multistage}, it follows that the best an adaptive offline $\OPT$ can hope to achieve in expectation is $\E [\max_i \kappa_i]$, just as in the classic prophets analogue.







\subsection{Prophets, incentive compatibility and \SAUP{}-based algorithms}\label{subsec:prophet-framework}

In the \emph{classic prophet inequalities} setting with constraints $\F$, the input is a sequence of independent random variables $X_1,\dots,X_n$, and the welfare of an algorithm is the sum of the selected random variables.
See Section \ref{subsec:related} for a discussion of the extensive related work in this setting.

Classic prophet inequalities have been related to \emph{incentive compatibility} since at least \citet{chawla2010multi}.
The idea is that an online algorithm that uses threshold rules, i.e. select $X_i$ if it exceeds $\tau_i$, is related to a mechanism for selling items or service to a sequence of arriving buyers $i=1,\dots,n$ with independent private values $X_i,\dots,X_n$.
We post a price $\tau_i$ to buyer $i$, who purchases iff $X_i \geq \tau_i$, and social welfare is the sum of the values of buyers who purchase.

In the prophets setting, use of threshold rules, or a related property called \emph{monotonicity}, is without loss of generality.
Hence all classic prophet inequalities are immediately incentive compatible:\footnote{\citet{chawla2024non} explores how this implication fails when a single buyer participates in multiple arrivals.} an approximation $\alpha$ immediately implies a mechanism with a \emph{Price of Anarchy}\footnote{The worst-case ratio, in any equilibrium, of social welfare to the optimum achievable welfare by any algorithm $\OPT$.} of $\alpha$.

However, in our settings, this implication will not hold.
We consider generalized prophet problems where each arriving item involves costly interactions to yield available rewards.
We will see in Section \ref{sec:non-pandora} that an algorithm can satisfy the generalization of monotonicity (Definition \ref{def:monotone}) without being incentive compatible.

We capture incentive-compatibility by defining, for each of our settings, the corresponding \emph{single-agent utility problem (\SAUP{})}.
Intuitively, \SAUP-based algorithms simulate offering a posted price $\tau_i$ to each arriving ``agent'' $i$.
Here $i$ is offered the option to pay $\tau_i$ in order to ``claim'' $i$ by adding it to the algorithm's solution set $F$.
Each agent may carry out some series of costly actions or information-gathering steps on the item, which may uncover or affect its value, before deciding to either accept or reject the take-it-or-leave it price.
Such an agent solves the \SAUP{} problem:

\begin{definition*}[\SAUP, informal sketch] 
	The \emph{Single-Agent Utility Problem (\SAUP$(i, \tau)$)} for an arrival $i$ is to find a policy on object $i$ which maximizes total utility of the interaction, minus the threshold $\tau$ if the policy chooses to claim $i$.
	A \SAUP{}-based algorithm for a prophets setting, when $i$ arrives, sets a threshold $\tau_i$ according to some rule and solves \SAUP$(i,\tau_i)$ to determine whether to claim arrival $i$.
\end{definition*}

\SAUP{} is not only central to the question of whether our algorithms provide Price of Anarchy guarantees, but is also a key component of our algorithmic approach: we will eventually be able to show that \SAUP{} is efficiently computable on Markov Search Processes.
We will then be able to use classic prophet inequality methods for setting the thresholds.



