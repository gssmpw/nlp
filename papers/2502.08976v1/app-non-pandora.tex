\section{Proofs from Section \ref{sec:non-pandora}} \label{app:non-pandora}

\begin{lemma} \label{lemma:cabinets-nonadaptive-opt}
  For any downward-closed $\F$, there is an optimal ex-ante feasible $\OPT$ for Cabinets-Prophets$(\F)$ that has the following property: for each cabinet $i$, $\OPT$ draws its choice of drawer $j^*(i)$ from a predetermined distribution $\lambda_i \in \Delta_{[m_i]}$ independently of all other variables and decisions.
  Furthermore, $\OPT$ and in particular $(\lambda_i : i \in [n])$ are efficiently computable.
\end{lemma}
\begin{proof}
  Suppose WLOG that all variables are nonnegative.
  The objective is to maximize the expected total value claimed such that $Q \in \PF$, where $\Pr[\text{claim $i$}] = Q_i$.
  The optimal algorithm induces some such $Q$.
  But given $Q_i$,  WLOG, the selection decision on $i$ is independent of all cabinets $i'$, as the objective is a separable sum of performance on each cabinet $i$ subject to $Q_i$.
  Therefore, also WLOG, the choice of drawer $j^*(i)$ is drawn independently from some distribution $\lambda_i \in \Delta_{[m_i]}$.

  It remains to give an efficient algorithm.
  Because $\OPT$ decides independently on each cabinet, WLOG, extending Observation \ref{obs:monotone}, $\OPT$ is monotone.
  That is, when $\OPT$ observes $X_i^j$, it claims cabinet $i$ if and only if $X_i^j$ is in its top $q_i^j$ quantile of values for some $q_i^j$.
  Define $g_i^j(q) := q \E[ X_i^j \mid \text{$X_i^j$ in top $q$ quantile} ]$, the ``upper expectation'' of $X_i^j$ for quantile $q$.
  We first claim that $g_i^j$ is a concave function (this fact is also asserted in \citet{feldman2016online}, Section 4.1).
  To see this, let $G(q) = x$ such that\footnote{If there are multiple $x$ where this is true, they lie in a region where $X_i^j$ has zero density and any choice of $x$ in this region is valid for the argument.} $\Pr[X_i^j \leq x] = 1-q$.
  In other words, $G(q) = F^{-1}(1-q)$ where $F$ is the CDF.
  We have $g_i^j(q) = \int_{y=q}^1 dG(y)$; this is a version of the fact $\E[X_i^j] = \int_0^{\infty} (1 - F(x)) dx$.
  And $G$ is a positive, monotone decreasing function, so (noting the bounds on the integral) $g_i^j$ is concave.

  Now, given a fixed $Q_i$, the optimal algorithm on cabinet $i$ that claims it independently with probability $Q_i$ solves
    \[ f_i(Q_i) = \max_{\lambda_i \in \Delta_{[m_i]}, (q_i^j : j \in [m_i])} \sum_{j \in [m_i]} \lambda_i(j) g_i^j(q_i^j) , \]
  subject to $\sum_{j \in [m_i]} \lambda_i(j) q_i^j = Q_i$.
  We can rephrase.
  Let $\beta_i \in \Delta_{[0,1]}$ be a distribution over points $q \in [0,1]$ satisfying that $\E_{q \sim \beta_i} q = Q_i$.
  Define
    \[ \hat{f}_i(Q_i) = \max_{\beta_i \in \Delta_{[0,1]}} \E_{q \sim \beta_i} \max_{j \in [m_i]} g_i^j(q) . \]
  We claim that $\hat{f}_i = f_i$.
  We have $\hat{f}_i(q) \geq f_i(q)$ because any valid solution $(\lambda_i, \{q_i^j\})$ can be rephrased as a distribution $\beta_i$.
  But because each $g_i^j$ is concave, WLOG, $\beta_i$ is never supported on multiple points where the same $g_i^j$ achieves the maximum; it is better by Jensen's inequality to put all their weight on the average of the points.
  So the solution $\beta_i$ is WLOG a distribution over $m_i$ points $\{q_i^j\}$ where $j = \argmax_{j'} g_i^{j'}(q_i^j)$, so $f_i$ can achieve the same value as $\hat{f}_i$.
  In fact, $\hat{f}_i$ is just the concave envelope of $(g_i^j : j \in [m_i])$, i.e. the pointwise smallest concave function that lies weakly above every $g_i^j$.

  Given $Q_i$, $\hat{f}_i(Q_i)$ is efficiently computable: it is a concave maximization problem over the linear constraint that the expectation of $\beta_i$ is $Q_i$.
  The solution $\beta_i$ produces $\lambda_i, \{q_i^j\}$.
  We also note that the gradient of $f_i$ at $Q_i$ is computable efficiently as well.

  $\hat{f}_i(Q_i)$ is also decreasing, as it represents the maximum value obtainable by selecting with probability$Q_i$.
  This implies that the function $h(Q_i) := Q_i f_i(Q_i)$ is concave, as we can make the same argument as with $g_i^j$. 
  The ex-ante relaxation  problem is
    \[ \max_{Q \in \PF} \sum_i Q_i f_i(Q_i) , \]
  a concave maximization problem subject to a convex constraint, and therefore efficiently solvable to find $Q$.
  From there, we have already shown that $\lambda_i$ is efficiently computable.
\end{proof}


\subsection{Proof of Theorem \ref{thm:classic-cp-saup}} \label{subsec:proof-cabinet-matroid}

\paragraph{Setup for Theorem \ref{thm:classic-cp-saup}.}
We have an instance defined by $\F$ and $X = (X_i^j : i \in [n], j \in [m_i])$.
Let $Q \in \PF$ be any ex-ante feasible point and $(\lambda_i \in \Delta_{[m_i]} : i \in [n])$ any set of distributions.
Let $\OPT_{Q,\lambda}$ be an algorithm that, independently on each cabinet $i$, opens drawer $j^*(i) \sim \lambda_i$ and claims $i$ with probability at most $Q_i$.
Letting $j^*(i) \sim \lambda_i$ independently, define $X_i' := X_i^{j^*(i)}$.

In particular, the ex-ante optimal algorithm induces some such distributions $Q,\{\lambda_i\}$, but it will be useful later to consider suboptimal choices of $Q$ and $\lambda_i$ as well.

Define $z_i = \E[ X_i' \mid \text{$X_i'$ in its top $Q_i$ quantile}]$.
We observe that $\E \Welfof{\OPT_{Q,\lambda}} \leq \sum_{i \in [n]} Q_i z_i$.

We now define a re-randomized set of random variables.
The idea is as in \citet{lee2018optimal}, but modified to include Bernoulli variables $Z$ as well as non-Bernoulli versions $Y$.
\begin{itemize}
  \item As $Q \in \PF$, we have $Q = \E_{F \sim \D_Q} 1_F$ for a distribution $\D_Q$ over $\F$.
  \item Sample $F \sim \D_Q$.
  \item Set $Z_i = z_i \Indic{i \in F}$.
  \item If $i \in F$, draw $Y_i$ from the distribution of $X_i'$ conditioned on being in the top $Q_i$ quantile; otherwise, conditioned on not being in the top $Q_i$ quantile.
\end{itemize}
We note that the marginal distributions of $Y_i$ and $X_i'$ are the same.

For any $S \subseteq [n]$, let
  \[ R(S) = \max_{S'} \left\{ \sum_{i \in S'} Z_i ~\middle|~ S' \cap S = \emptyset, S' \cup S \in \F \right\}. \]
Here $R(S)$ depends on $F,Z$ but not $Y$.
We observe that $\E_Z R(\emptyset) = \E[ \sum_i Z_i ] = \sum_i Q_i z_i \geq \E[ \Welfof{\OPT_{Q,\lambda}} ]$.

We will need to independently draw a second copy $F'$ and $Z'$ of $F$ and $Z$ respectively.
Let $R'(S)$ be defined analogously to $R$ but on $Z'$ instead of $Z$.

It will be convenient for later to clarify exactly what information is required to execute our algorithm.

\paragraph{Matroid-Cabinets Algorithm.}
\begin{itemize}
  \item Input: $Q \in \PF$, $(z_i : i \in [n])$.
  \item Initialize $A_0 = \emptyset$, and on each arriving cabinet $i=1,\dots,n$:
  \begin{itemize}
    \item If $A_{i-1} \cup \{i\} \not\in \F$, then set $T_i = \infty$, discard cabinet $i$ and continue.
    \item Otherwise, set $T_i = \frac{1}{2} \E_{Z'} \left[ R'(A_{i-1}) - R'(A_{i-1} \cup \{i\}) \right]$.
    \item Run $\SAUP(i,T_i)$ (Definition \ref{def:csaup}). If it claims, set $A_i = A_{i-1} \cup \{i\}$, else set $A_i = A_{i-1}$.
  \end{itemize}
\end{itemize}

\begin{proposition} \label{prop:matroid-alg-efficient}
  For matroids $\F$, the Matroid-Cabinets Algorithm is ex-post feasible and runs in polynomial time.
\end{proposition}
\begin{proof}
  Ex-post feasibility is immediate.
  For running time:
  First, we can decompose $Q$ explicitly as a distribution over independent sets of the matroid in polynomial time.
  This follows because \emph{(a)} there is an efficient separation oracle for any matroid polytope~\citep{cunningham1984testing}, and \emph{(b)} using such an oracle, one can obtain $\D_Q$ explicitly in polynomial time for any polytope (e.g. \citet[Theorem 4]{cai2017constructive}, citing older work).
  Now, for each arrival $i$, we proceed as follows.
  For any fixed $F$ and $S$ and with the given weights $(z_i : i \in [n])$, computing $R(S)$ is efficient as it is simply maximizing a linear function on a matroid.
  So computing the threshold, $T_i$, is efficient as well, as we have $\D_Q$ in closed form.
  Finally, \SAUP{} is efficient as it just computes, for each drawer, the expected utility of opening the drawer given a posted price $T_i$.
\end{proof}

\begin{lemma} \label{lemma:matroid-cabinet}
  Let $\F$ be a matroid constraint.
  Let $\OPT_{Q,\lambda_i}$ be ex-ante feasible and let $\{z_i\}$ be as defined above for $Q,\{\lambda_i\}$.
  Then run on input, $Q,(z_i : i \in [n])$, the Matroid-Cabinets Algorithm satisfies $\E[ \Welfof{\ALG}] \geq \frac{1}{2}\sum_{i=1}^n Q_i z_i$.

  Furthermore, if $\{z_i'\}$ is any list where $z_i' \leq z_i$ for all $i$, then the Matroid-Cabinets Algorithm on input $Q,(z_i' : i \in [n])$ satisfies  $\E[ \Welfof{\ALG}] \geq \frac{1}{2}\sum_{i=1}^n Q_i z_i'$.
\end{lemma}
\begin{proof}
  Let $\text{Util}_i(X) = (X_i^{j(i)} - T_i)^+$, ``utility'' gain of arrival $i$, and let $\text{Util}(X) = \sum_{i \in [n]} \text{Util}_i(X)$.
  Note that because $i \in A \iff X_i^{j(i)} \geq T_i$, we have $\text{Util}(X) = \sum_{i \in A} \text{Util}_i(X)$.

  Therefore, for any realization of $X$, we have
  \begin{align}
    \Welfof{\ALG}
    &=    \left(\sum_{i \in A} T_i \right) ~ + ~ \text{Util}(X)  \nonumber \\
    &=    \frac{1}{2} \E_Z \left[ \sum_{i \in A} R(A_{i-1}) - R(A_{i-1} \cup \{i\}) \right] ~ + ~ \text{Util}(X)  \nonumber \\
    &=    \frac{1}{2} \E_Z \left[ \sum_{i \in A} R(A_{i-1}) - R(A_i) \right] ~ + ~ \text{Util}(X) \nonumber \\
    &=    \frac{1}{2} \E_Z \left[ R(\emptyset) - R(A) \right] ~ + ~ \text{Util}(X) . \label{eq:matroid-saup}
  \end{align}
  To lower-bound the expectation of $\text{Util}(X)$, we proceed carefully.
  For each $i$, the following holds even conditioned on any realization of $(X_{i'}^j : i' \in [i-1], j \in [m_{i'}])$, which determines $T_i$, so it holds in expectation:
  \begin{align*}
    \E_X \text{Util}_i(X)
    &\geq \E_X \left( X_i^{j^*(i)} - T_i \right)^+
  \end{align*}
  by construction of \SAUP{} and the fact that $j^*(i)$ is independent of the values $(X_i^j : j \in [m_i])$.
  Next, because $X_i^{j^*(i)}$ and $Y_i$ are both independent of $T_i$ and have the same distribution, again for any realization of $(X_{i'}^j : i' \in [i-1], j \in [m_{i'}])$, and therefore in expectation, we have:
  \begin{align*}
    \E_X \left( X_i^{j^*(i)} - T_i \right)^+
    &= \E_{X,F,Y} \left( Y_i - T_i \right)^+ .
  \end{align*}
  Now, given $X$ and $F$, let $V = \argmax_S \left\{ \sum_{i \in S} Z_i ~\middle|~ S' \cap A = \emptyset, S' \cup A \in \F \right\}$.
  In other words, $V$ is the set whose weight is $R(A)$.

  By summing over $i$, we have:
  \begin{align}
    \E_X \text{Util}(X)
    &\geq \E_{X,F,Y} \sum_{i \in [n]} \left( Y_i - T_i \right)^+  \nonumber \\
    &\geq \E_{X,F,Y} \sum_{i \in V} \left( Y_i - T_i \right)^+  \nonumber \\
    &\geq \E_{X,F,Y} \sum_{i \in V} \left( Y_i - T_i \right)  \nonumber \\
    &=    \E_{X,F,Y} \sum_{i \in V} Y_i ~ - ~ \E_{X,F} \sum_{i \in V} T_i  \nonumber \\
    &=    \E_{X,F} \sum_{i \in V} \E[Y_i \mid F] ~ - ~ \E_{X,F} \sum_{i \in V} T_i  \nonumber \\
    &=    \E_{X,F} \sum_{i \in V} Z_i ~ - ~ \E_{X,F} \sum_{i \in V} T_i   \label{eq:matroid-cabinet-z} \\
    &=    \E_{X,F} R(A) ~ - ~ \E_{X,F} \sum_{i \in V} T_i . \label{eq:matroid-cabinet-end}
  \end{align}
  In the last line, we used\footnote{We note that if using $\{Y_i\}$ in the definition of $R(S)$ and $V$ instead of $Z_i$, we could not guarantee $V \subseteq F$, and we would not be able to obtain the term $\E_{X,F} R(A)$. This is why we use the ``Bernoullification'' $Z_i$, following \citet{lee2018optimal}. On the other hand, the \SAUP{} inequality holds for $Y_i$, which is a re-randomized version of $X_i'$, but does not necessarily seem to hold for $Z_i$.}
 that $V \subseteq F$ WLOG, as $i \not\in F \implies Z_i = 0$.
  We now claim:
  \begin{lemma} \label{lemma:kw12}
    $\E_{X,F} \sum_{i \in V} T_i \leq \frac{1}{2} \E_{X,F} R(A)$.
  \end{lemma}
  \begin{proof}
    In order to expand the definition of $T_i$, let $(F',Z')$ be drawn as an independent copy of $(F,Z)$, and let and $R'$ be the corresponding marginal gain function.
    \begin{align*}
      \E_{X,F} \sum_{i \in V} T_i = \E_{X,F} \sum_{i \in V} \frac{1}{2} \E_{F'} \left[ R'(A_{i-1}) - R'(A_{i-1} \cup \{i\}) \right] .
   \end{align*}
    Fix any realizations of $X,F,F'$.
    Proposition 2 of \citet{kleinberg2012matroid} gives the following: for every matroid $\F$, for every list of $n$ numbers $(z_1,\dots,z_n)$, and for every pair of disjoint sets $A,V$ with $A \cup V \in \F$, letting $A_i = A \cap [i]$, defining $F^* = \max_{S \in \F} \sum_{i \in S} z_i$ and $R'(S) = \max_{S' \subseteq F^*} \left\{ \sum_{i \in S'} z_i ~:~ S' \cap A = \emptyset, S' \cup A \in \F \right\}$, we have
    \begin{align*}
       \sum_{i \in V} \left[ R'(A_{i-1})) - R'(A_{i-1} \cup \{i\})) \right] &\leq R(A) .  & \text{Proposition 2 of \citet{kleinberg2012matroid}}
    \end{align*}
    Applying this to our $V, Z', R', A$, and taking expectations,
    \begin{align*}
      \frac{1}{2} \E_{X,F,F'} \sum_{i \in V} \left[ R'(A_{i-1}) - R'(A_{i-1} \cup \{i\}) \right]
     &\leq \frac{1}{2} \E_{X,F'} R'(A) .
  \end{align*}
   
  \end{proof}
  To finish proving Lemma \ref{lemma:matroid-cabinet}, we apply Lemma \ref{lemma:kw12} in (\ref{eq:matroid-cabinet-end}):
  \begin{align*}
    \E_X \text{Util}(X)
    &\leq \E_{X,F} R(A) - \frac{1}{2} \E_{X,F'} R'(A)  \\
    &=    \frac{1}{2} \E_{X,F} R(A) .
  \end{align*}
  Plugging into (\ref{eq:matroid-saup}),
  \begin{align*}
    \E[ \Welfof{\ALG} ]
    &\geq \frac{1}{2} \E_{X,F} \left[ R(\emptyset) - R(A) \right] - \frac{1}{2} \E_{X,F} R(A)  \\
    &=    \frac{1}{2} \E_{X,F} R(\emptyset)  \\
    &=    \frac{1}{2} \E_F \left[\sum_{i \in F} Z_i \right] \\
    &=    \frac{1}{2} \sum_{i=1}^n Q_i z_i  
  \end{align*}
  For the ``furthermore'', we observe that in this algorithm and proof, $\{z_i\}$ was any list of nonnegative numbers, whose properties were irrelevant except in one line: In (\ref{eq:matroid-cabinet-z}), we used that $\E[Y_i \mid i \in F] = z_i$.
  If we replace $\{z_i\}$ with $\{z_i'\}$, and replace $Z_i$ with $Z_i' := z_i' \Indic{i \in F}$, then (\ref{eq:matroid-cabinet-z}) holds with $\geq$ rather than equality, and the rest of the proof is unaffected, yielding $\E[ \Welfof{\ALG}] \geq \frac{1}{2} \sum_{i=1}^n Q_i z_i'$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:classic-cp-saup}]
  First, by Lemma \ref{lemma:cabinets-nonadaptive-opt}, computing the ex-ante $\OPT$'s values of $Q$ and $(\lambda_i : i \in [n])$ can be done efficiently.
  In this case, $\E[ \Welfof{\OPT_{Q,\lambda}}] = \E[\Welfof{\OPT}].$
  Using the distribution $\lambda_i$ on drawers, we can efficiently compute $z_i$.
  We then pass $Q$ and $(z_i : i \in [n])$ to the Matroid-Cabinets Algorithm.
  By Proposition \ref{prop:matroid-alg-efficient}, the algorithm is ex-post feasible and efficient.
  By Lemma \ref{lemma:matroid-cabinet}, we obtain
  \[ \E[ \Welfof{\ALG} ] \geq \frac{1}{2} \E[ \Welfof{\OPT_{Q,\lambda}} ] = \frac{1}{2} \E[ \Welfof{\OPT}] . \]
\end{proof}





