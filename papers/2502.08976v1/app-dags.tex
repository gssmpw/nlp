\section{Proofs And Definitions from Section \ref{sec:dags}} \label{app:dags}

\subsection{Induced tree, bandits, and policies} \label{subapp:induced-policies}

\begin{definition}[Tree MSP] \label{def:tree-MSP}
  Given an MSP $\M$, the \emph{induced tree MSP} $\M'$ is defined as follows.
  In the tree MSP $\M'$, we create a new state $s'$ for every non-halted transcript $r=((a^1,s^1),\dots,(a^m,s^m))$ in the original MSP.
  The legal actions are $A'^{s'} = A^{s^m}$.
  The transition function is as follows: for every $a \in A^{s^m}$ and every $s'$ such that $P(s';a,s^m) > 0$, we have $P'((r,(a,s'));a,r) = P(s';a,s^m)$.
  Similarly, in this case $C'(a,r) = C(a,s^m)$.
  For all other cases, the transition probability and cost are zero.
  Finally, $V'(r) = V(s_m)$.
\end{definition}

\begin{definition}[Induced bandit] \label{def:induced-bandit}
  Given an MSP $\M = (S,A,P,C,V)$, and given a deterministic policy $\pi$, we define the \emph{induced bandit} $\M^{\pi}=(S^\pi,A^\pi,P^\pi,C^\pi,V^{\pi})$ of $\pi$ on $\M$ as follows:
  \begin{itemize}
    \item Let $\M'=(S',A',P',C')$ be the induced tree MSP $\M'$ of $\M$. Each state in $S^{\pi}$ corresponds to a state in $S'$.
    We allow for states in $S^{\pi}$ to be referred to using the notation of the transcript on $S$ they correspond to.
    \item $A^\pi$ contains exactly one action $a^*$.
    \item Let $r=((a^1,s^1),\ldots,(a^m,s^m))$ be a transcript on $M$.
    Then for all $s'\in S^\pi, P^\pi((r,(a^*,s'));a^*,r)=P(s^{m+1};\pi(r),s^m)$
    \item Similarly, given a transcript $r=((a^1,s^1),\ldots,(a^m,s^m))$ on $M$ and a state $s'\in S^\pi$,
    $C^\pi(a^*,r)=C(\pi(r),s^m)$.
    \item Given a sink state $s$, if $\pi$ claims at $s$, then $V^\pi(s)=V(s)$
    \item If $\pi$ does not claim at a sink state $s$, then $V^\pi(s)=0$.
  \end{itemize}
\end{definition}

\begin{proof}[Proof of Theorem \ref{thm:pc-cms}]
  Given $\OPT$ and the instance $(\M_i : i \in [n])$, the induced policy $\rho_i$ on $\M_i$ is as follows.
  To execute $\rho_i$ on $\M_i$, we will create a simulation of all other MSPs $(\M_{i'} : i' \neq i)$.
  We note that, because the $\M_i$s are mutually independent, simulating $\OPT$ on $\M_i$ along with the simulated $(\M_{i'} : i' \neq i)$ results in the same distribution over histories as running $\OPT$ on the original instance.

  In the simulation, fix all random coins of $\OPT$ in advance by committing to every possible randomized decision.
  Also fix all randomness of nature in the transitions of $\M_{i'}$ for all of the simulated copies $i' \neq i$.
  These random coins induce the following deterministic policy $\pi_i$ on $\M_i$:
  Simulate $\OPT$, which is now deterministic, and when it chooses to advance some $i' \neq i$, deterministically compute the transition using the fixed randomness of nature.
  When $\OPT$ chooses to advance $i$, make the corresponding choice in $\M_i$, observe the state transition, and feed the observation to $\OPT$.
  In particular, when $\OPT$ stops and claims some set $F^*$, the policy $\pi_i$ halts and claims iff $i \in F^*$.

  Given the fixed set of random coins of the algorithm and of nature in $\{\M_{i'} : i' \neq i\}$, $\pi_i$ is a deterministic policy, which we prove by induction.
  The history of the simulated algorithm is deterministic by construction until it first chooses to act in $\M_i$.
  Therefore, the first decision of $\pi_i$ is a deterministic function of the empty transcript.
  Inductively, after any decision of $\pi_i$ and subsequent random transition of $\M_i$, the entire history of the simulated algorithm is deterministic until the algorithm again chooses to act in $\M_i$.
  So the next action of $\pi_i$ is a deterministic function of the transcript so far, because the entire history of the simulated algorithm is a deterministic function of the transcript so far.
  This also holds for the halt and claim/don't claim decision.
	

  Now, we observe that, because the coins and randomness of nature define a deterministic policy $\pi_i$, drawing these coins and randomness from their given distributions defines a distribution over policies $\rho_i$.
  Finally, we observe that the algorithm can be coupled with the policy such that $\Welfofon{\OPT}{i} = \Perfofon{\rho_i}{i}$ pointwise, as they make the same sequence of inspections and have the same claim outcome on $i$.
\end{proof}



\subsection{Proof of Proposition \ref{prop:exante-dags-efficient}} \label{subapp:exante-fptas}

Given an ex-ante feasible algorithm $\OPT'$ for Combinatorial Markov Search, let its solution set be $F'$ and let $Q_i = \Pr[i \in F']$.
Let $z_i = \frac{1}{Q_i}\E[\Perfofon{\pi_i}{\M_i}]$, where $\pi_i$ is the induced policy of $\OPT'$ on $\M_i$.
In other words, $z_i$ is the expected net welfare gain on arrival $i$ conditioned on claiming $i$.

Before the proof, we outline the problem.
Recall that in this problem, we are given $\F$ and an instance $(\M_i : i \in [n])$.
An ex-ante feasible algorithm induces some $Q \in \PF$ with $Q_i = \Pr[\text{claim $i$}]$.
The problem to solve is
  \[ \max \sum_i Q_i \E \Welf_{\OPT,i} . \]
As in Lemma \ref{lemma:cabinets-nonadaptive-opt}, WLOG, the ex-ante optimal policy acts independently on each MSP $\M_i$.
Letting $\pi$ range over the space of all randomized policies, we define
\begin{align}
  &f_i(q) = \max_{\pi} \E \left[ \Perf_{i,\pi} \right]   \label{eq:exante-dags-f}  \\
  \text{s.t.} \quad
  &\Pr[\text{$\pi$ claims}] \leq q .  \nonumber
\end{align}
Then the problem is to solve
\begin{align}
  &\max_{Q \in \PF} \sum_i f_i(Q_i) .   \label{eq:exante-dags}
\end{align}
(Note that, unlike in the proof of Lemma \ref{lemma:cabinets-nonadaptive-opt}, we have folded the factor of $Q_i$ into the definition of $f_i$.)
Our solution will turn out to provide $Q$ and $z_i$, an approximation of $\frac{f(Q_i)}{Q_i}$, as required.

We will prove the following two lemmas:
\begin{lemma} \label{lemma:exante-dags-f-concave}
  Given an MSP $\M_i$, the function $f_i$ is concave.
\end{lemma}
\begin{lemma} \label{lemma:exante-dags-f-fptas}
  Given an MSP $\M_i$, there exists an algorithm with parameters $0 < c,\epsilon < 1$, running in time poly(input size, $\ln(\tfrac{1}{c})$, $\tfrac{1}{\epsilon}$), that produces an efficiently-represented, efficient-evaluation value oracle $\hat{f}_i(q)$ satisfying, for all $q \in [0,1]$,
  \[ \frac{f_i(q) - c}{1+\epsilon} \leq \hat{f}_{i}(q) \leq (f_i(q) + c)(1+\epsilon) . \]
\end{lemma}

These lemmata implies the proposition as follows.
\begin{proof}[Proof of Proposition \ref{prop:exante-dags-efficient}]
  Assume WLOG that $\{i\} \in \F$ for all $i \in [n]$, i.e. all singleton sets are feasible.
  First, for each $i$, we solve \SAUP$(\M_i,0)$ to obtain the achievable welfare $\bar{W}_i$ on that process with no constraints, and let $\bar{W} = \max_i \bar{W}_i$ be the largest.
  Then $\bar{W}$ is a lower bound on the optimal achievable welfare.
  We pick $\epsilon' = \Theta(\epsilon)$, specified at the end.
  We set $c = \epsilon' \bar{W}$ and apply Lemma \ref{lemma:exante-dags-f-fptas} to obtain efficiently represented concave functions $\hat{f}_i$ with access to values and gradients.

  We solve the problem, (\ref{eq:exante-dags}) using the $\hat{f}_i$.
  It is a concave maximization problem subject to a convex polyhedral constraint, and we have an efficient separation oracle in particular for any matroid constraint~\citep{cunningham1984testing}.

  If we let $h(Q) = \sum_i f_i(Q_i)$ and $\hat{h}(Q) = \sum_i \hat{f}_i(Q_i)$, then we have by Lemma \ref{lemma:exante-dags-f-fptas}
    \[ \frac{h(Q) - c}{1+\epsilon'} \leq \hat{h}(Q) \leq (h(Q) + c)(1+\epsilon') . \]
  Because welfare is $W = \max h$, letting our solution be $\hat{W} = \max \hat{h}$, we obtain the same guarantee for $W$ and $\hat{W}$.
  Now, $W \geq \bar{W}$, we have
  \begin{align*}
    \hat{W} &\leq (W + c)(1+\epsilon')  \\
      &\leq (W + \epsilon' W)(1+\epsilon')  \\
      &= W (1+\epsilon')^2  \\
      &\leq W (1+O(\epsilon')),
  \end{align*},
  and similarly $\hat{W} \geq W(1-\epsilon')/(1+\epsilon') \geq W(1 - O(\epsilon))$.
  A suitable choice of $\epsilon' = \Theta(\epsilon)$ gives the result.
\end{proof}

We now consider Lemmata \ref{lemma:exante-dags-f-concave} and \ref{lemma:exante-dags-f-fptas}.
For ease of notation, we consider a fixed MSP $\M_i$ and drop the subscript $i$.
Our algorithm will compute the following functions:
\begin{definition} \label{def:exante-dags-g}
  Given a fixed MSP $\M = (S,A,P,C,V)$, define $g_s(q)$ and $g_{s,a}(q')$ as follows.
  At a sink $s$, let $g_s(q) = q V(s)$.
  At a non-sink $s$, let:
  \begin{align*}
    g_{s,a}(q') &= \max_{\{q_{s'}\}} \sum_{s'} P(s';a,s) g_{s'}(q_{s'})  & \text{s.t.} ~ \sum_{s'} P(s';a,s) q_{s'} = q'  \\
    g_s(q) &= \max_{\lambda,\{q_{s,a}\}} \sum_{a \in A^s \cup \{\bot\}} \lambda(a) \left[ g_{s,a}(q_{s,a}) - C(a,s) \right]  & \text{s.t.} ~ \sum_{a \in A^s \cup \{\bot\}} \lambda(a) q_{s,a} = q .
  \end{align*}
\end{definition}

For comparison, given any state $s$, let
 \[ f_s(q) = \max_{\bar{\pi} \in \bar{\Pi}_s} \E[ \Perf_{\bar{\pi},s} ] ~~ \text{s.t.} ~~ \Pr[\text{$\pi$ claims}] \leq q.  \]

\begin{lemma} \label{lemma:exante-f-g}
  $f_s = g_s$.
\end{lemma}
\begin{proof}
  By backward induction.
  At a sink $s$, the only two possibilities are to claim reward $V(s)$ or not claim.
  The policy that claims with probability $q$ has expected reward $q V(s)$.

  At a non-sink $s$, we first argue that $g_{s,a}(q')$ is the optimal expected performance achievable by any policy that begins after action $a$ has been chosen in state $s$, i.e. begins by drawing $s' \sim P(\cdot;a,s)$.
  Suppose by backward induction that at all possible transition states $s'$, i.e. where $P(s';a,s) > 0$, we have $g_s(q) = f_s(q)$.
  The optimal policy $\bar{\pi}$ at $s$ that is required to take action $a$ in state $s$ and must claim with probability at most $q'$ has, for each possible $s'$, some probability $q_{s'}$ of claiming conditioned on transitioning to state $s'$.
  It must satisfy $\sum_{s'} P(s';a,s) q_{s'} = q'$.
  Furthermore, for each state $s'$, it runs the optimal policy $\bar{\pi}_{s'}$ starting at state $s'$ that claims with probability $q_{s'}$.
  Therefore, $g_{s,a}(q')$ achieves that optimum, i.e. maximizes expected performance over all policies.

  Now, we argue $g_s(q) = f_s(q)$, i.e. optimizes expected performance subject to claiming with probability $q$.
  The optimal such policy must first choose to either halt (action $\bot$) or take an action $a \in A^s$, according to some probability distribution $\lambda$.
  In doing so, it incurs a cost $C(a,s)$.
  For each action $a$, this optimal policy has some probability $q_{s,a}$ of claiming conditioned on taking action $a$.
  It must satisfy $\sum_{a \in A^s \cup \{\bot\}} \lambda(a) q_{s,a} = q$.
  And conditioned on choosing $a$, this optimal policy continues by running the policy that maximizes expected performance among all policies that begin after action $a$ has been chosen in state $s$ and that claim with probability $q_{s,a}$.
  That is, it conditioned on choosing $a$ and paying $C(a,s)$, the optimal policy obtains $g_{s,a}(q_{s,a})$.
  Therefore, $g_s(q)$ achieves that optimum.
\end{proof}

\begin{lemma} \label{lemma:exante-g-concave}
  $g_{s,a}$ and $g_s$ are concave and monotone nondecreasing.
  Furthermore, $g_{s,a}(0) = g_s(0)$ while $g_{s,a}(1)$ and $g_s(1)$ are computable in polynomial time.
\end{lemma}
\begin{proof}
  For ``furthermore'', observe that the performance of an algorithm that never claims cannot be above zero; while the algorithm that always claims can be implemented by \SAUP{}$(\M,0)$.
  

  Now, we again use backward induction.
  At a sink $s$, $g_s(q)$ is linear in $q$ and nondecreasing because $V(s) \geq 0$.
  Now consider a non-sink $s$.
  Suppose for each $s'$ reachable from $s$, $g_s$ is concave and monotone nondecreasing.
  Then $g_{s,a}$ is monotone nondecreasing because, $q < q'$ and $\{q_{s'}\}$ is a valid solution to the maximization for $g_{s,a}(q)$, then there is a solution $\{q'_{s'}\}$ for $g_{s,a}(q')$ that satisfies $q'_{s'} \geq q_{s'}$ for all $s'$, and the claim follows by monotonicity of each $g_{s'}$.

  Next, we directly prove that, for all $a$, $g_{s,a}(q)$ is concave.
  Let $\alpha \in [0,1]$ and $q,q' \in [0,1]$.
  Respectively, let $\{q_{s'}\},\{q'_{s'}\}$ solve the maximization objective for $g_{s,a}(q)$, $g_{s,a}(q')$.
  Then
  \begin{align}
    \alpha g_{s,a}(q) + (1-\alpha) g_{s,a}(q')
    &= \sum_{s'} P(s';a,s) \left[ \alpha g_{s'}(q_{s'}) + (1-\alpha) g_{s'}(q'_{s'}) \right]  \nonumber \\
    &\leq \sum_{s'} P(s';a,s) g_{s'}(\alpha q_{s'} + (1-\alpha) q'_{s'}) .  \label{eq:exante-gsa-concave}
  \end{align}
  And by taking a convex combination of the constraints, they satisfy
  \begin{align*}
    \alpha q + (1-\alpha)q'
    &= \alpha \sum_{s'} P(s';a,s) q_{s'} + (1-\alpha) \sum_{s'} P(s';a,s) q'_{s'}  \\
    &= \sum_{s'} P(s';a,s) \left(\alpha q_{s'} + (1-\alpha) q'_{s'} \right) .
  \end{align*}
  So the collection $\{\alpha q_{s'} + (1-\alpha) q'_{s'}\}$ is a feasible solution for $g_{s,a}(\alpha q + (1-\alpha)q')$.
  By (\ref{eq:exante-gsa-concave}), that collection gives a weakly higher value than $\alpha g_{s,a}(q) + (1-\alpha)g_{s,a}(q')$.
  As $g_{s,a}(\alpha q + (1-\alpha)q')$ is the maximum over all feasible collections, we have $g_{s,a}(\alpha q + (1-\alpha)q') \geq \alpha g_{s,a}(q) + (1-\alpha)g_{s,a}(q')$.
  
  Next, we consider $g_s(q)$.
  For each $a$, the function $q_{s,a} \mapsto g_{s,a}(q_{s,a}) - C(a,s)$ is a concave, monotone increasing function minus a constant, so it is still concave, monotone increasing.
  Now, exactly as in Lemma \ref{lemma:cabinets-nonadaptive-opt}, $g_s$ is the concave closure of $\{g_{s,a} - C(a,s) : a \in A^s \cup \{\bot\}\}$.
  That is, it is the pointwise smallest concave function that lies weakly above every $g_{s,a}(q) - C(a,s)$ function.
  This implies that $g_s(q)$ is monotone increasing and concave.
\end{proof}


We have already shown that $f_i$ is concave:
\begin{proof}[Proof of Lemma \ref{lemma:exante-dags-f-concave}]
  We have that $f_i = f_{s^*}$, where $s^*$ is the start state of the MSP.
  By Lemma \ref{lemma:exante-f-g}, $f_{s^*} = g_{s^*}$, and by Lemma \ref{lemma:exante-g-concave}, $g_{s^*}$ is concave.
\end{proof}

Next we prove Lemma \ref{lemma:exante-dags-f-fptas}, i.e. give an efficient approximation guarantee for $f_{s^*} = g_{s^*}$.

\textbf{Approx-g's:}
\begin{itemize}
  \item Given: $\M = (S,A,P,C,V)$, $\epsilon > 0$. Let $m = |S|$. Define $\alpha = \epsilon/(2m)$.
  \item Parameter: $c < 1$.
  \item Output: for each $s$ and each $a \in A^s$, concave functions $\hat{g}_s, \hat{g}_{s,a} : [0,1] \to \reals$, each implemented as an efficient oracle for values and gradients.
  \item Let $b = \frac{c}{\max_s V(s)}$.
  \item Let $\Theta = (0, b, b(1+\alpha), b(1+\alpha)^2, \dots, 1 )$. 
        Alternatively, notate the set $\Theta = (\theta_0,\dots,\theta_T)$.
  \item By backward induction on the DAG structure:
  \begin{itemize}
    \item At a sink $s$, we set $\hat{g}_s(q) = q V(s)$.
    \item At a non-sink $s$, for each action $a$, we first compute $\hat{g}_{s,a}(q)$ for each $q \in P$ by solving its concave optimization problem of Definition \ref{def:exante-dags-g}, using as oracles $\hat{g}_{s'}$.
          We then ``iron'' $\hat{g}_{s,a}$ as follows.
          We set $\hat{g}_{s,a}(b) = \min\{\hat{g}_{s,a}(b), b \max_s V(s)\}$.
          We then enforce monotonicity by setting $\hat{g}_{s,a}(\theta_t) = \max\{\hat{g}_{s,a}(\theta_t), \hat{g}_{s,a}(\theta_{t-1})\}$ for $t=2,\dots,T$.
          We then define $\hat{g}_{s,a}(q)$ to be the concave envelope of $\hat{g}$, which we can compute and represent in closed form efficiently by taking the convex hull of the values computed above.
    \item Then, at the same non-sink $s$, we first compute $\hat{g}_s(q)$ for each $q \in \P$ by solving its own concave optimization problem of Definition \ref{def:exante-dags-g}, using as oracles $\hat{g}_{s,a}$, and ironing in the same way.
  \end{itemize}
\end{itemize}

\begin{lemma} \label{lemma:exante-dags-approx-gs}
  For all $s$ and $a$, for all $q \in [0,1]$, we have $\frac{g_{s,a}(q) - c}{1+\epsilon} \leq \hat{g}_{s,a}(q) \leq (g_{s,a}(q) + c)(1+\epsilon)$ and $\frac{g_{s}(q) - c}{1+\epsilon} \leq \hat{g}_{s}(q) \leq (g_{s}(q) + c)(1+\epsilon)$.
\end{lemma}
\begin{proof}
  We use backward induction.
  If $s$ is a sink, $g_s$ is at level $0$ of the induction.
  Any $g_{s,a}$ where the maximum level is $k-1$ over all $s'$ with $P(s';a,s) > 0$, is at level $k$.
  Similarly, $g_s$ where the maximum level is $k-1$ over all $g_{s,a}$ is at level $k$.
  We use backward induction to prove that, at the $k$th level of induction, the following hold:
  \begin{align}
    q \in \Theta \quad &\implies \quad \frac{g_{s,a}(q) - c}{(1+\alpha)^{k-1}} \leq \hat{g}_{s,a}(q) \leq (\hat{g}_{s,a}(q) + c)(1+\alpha)^{k-1}  \label{eq:exante-dags-gsa-appx-1} \\
    q \not\in\Theta \quad &\implies \quad \frac{g_{s,a}(q) - c}{(1+\alpha)^{k}} \leq \hat{g}_{s,a}(q) \leq (\hat{g}_{s,a}(q) + c)(1+\alpha)^{k}   \label{eq:exante-dags-gsa-appx-2}\\
    q \in \Theta \quad &\implies \quad \frac{g_{s}(q) - c}{(1+\alpha)^{k-1}} \leq \hat{g}_{s}(q) \leq (\hat{g}_{s}(q) + c)(1+\alpha)^{k-1}  \\
    q \not\in\Theta \quad &\implies \quad \frac{g_{s}(q) - c}{(1+\alpha)^{k}} \leq \hat{g}_{s}(q) \leq (\hat{g}_{s}(q) + c)(1+\alpha)^{k} .
  \end{align}
  Base case: at a sink $s$, $\hat{g}_s = g_s$, i.e. zero error.
  We also note at this time that $g_{s,a}$ and $g_{s}$ are $\max_s V(s)$ Lipschitz.
  The result follows because each $g_{s,a}$ cannot have a steeper slope than one of the $g_{s'}$ than it depends on, and so forth, and the steepest slope at any sink is $\max_s V(s)$.
  In particular, it holds that $g(q) \in [0, b \max_s V(s)]$.

  Now consider a non-sink at level $k$ of the induction.
  Consider $g_{s,a}(q)$ (Definition \ref{def:exante-dags-g}).
  First fix any $q \in \Theta$ and consider the values initially computed by the algorithm before ironing.
  Let $h_{s,a}(\{q_{s'}\}) = \sum_{s'} P(s';a,s) g_{s'}(q_{s'})$.
  Consider $\hat{h}_{s,a}(\{q_{s'}\}) = \sum_{s'} P(s';a,s) \hat{g}_{s'}(q_{s'})$.
  Because it is a convex combination, we have
    \[ \frac{h_{s,a}(\{q_{s'}\}) + c}{(1+\alpha)^{k-1}} \leq \hat{h}_{s,a}(\{q_{s'}\}) \leq (h_{s,a}(\{q_{s'}\}) + c)(1+\alpha)^{k-1} . \]
  This implies the same for $\hat{g}_{s,a}(q)$, i.e. (\ref{eq:exante-dags-gsa-appx-1}).
  

  Now, consider values computed at $q \in \Theta$ after enforcing monotonicity and taking the concave envelope.
  If $q=0$, then $\hat{g}_{s,a}(0) = g_{s,a}(0) = 0$.
  If $q=b$, then we either do not change $\hat{g}_{s,a}(b)$ or decrease it to equal $b \max_s V(s)$.
  As this is the maximum possible value of $g_{s,a}(b)$, we have not worsened the approximation guarantee of (\ref{eq:exante-dags-gsa-appx-1}).

  For other $q \in \Theta$, we maintain the guarantee (\ref{eq:exante-dags-gsa-appx-1}), as follows.
  In case 1, $\hat{g}_{s,a}(q)$ has not changed and (\ref{eq:exante-dags-gsa-appx-1}) holds.
  In case 2, $\hat{g}_{s,a}(q)$ has been set equal to $\hat{g}_{s,a}(q/(1+\alpha))$ to maintain monotonicity.
  In this case, since the value has been raised, it still lies above $g_{s,a}(q)(1-\alpha)$, and we still have $\hat{g}_{s,a}(q) \leq \hat{g}_{s,a}(q/(1+\alpha)) \leq g_{s,a}(q/(1+\alpha))(1+\alpha)^{k-1}  \leq g_{s,a}(q)(1+\alpha)^{k-1}$ by induction on $q \in \Theta$.
  In case 3, $\hat{g}_{s,a}(q)$ has changed to be larger so that it equals the convex combination of $\hat{g}_{s,a}$ at some $q',q'' \in \Theta$.
  Then the approximation applies to $\hat{g}_{s,a}(q'), \hat{g}_{s,a}(q'')$ and we obtain (\ref{eq:exante-dags-gsa-appx-1}) as well.

  Now fix any $q \in [b,1], q \not\in \Theta$.
  Let $x < q < y$ for $x,y \in \Theta$ with $y = x(1+\alpha)$.
  Note that because $g_{s,a}$ is concave and monotone, and because $g_{s,a}(0) = 0$, we have $dg_{s,a}(q) \geq g_{s,a}(q)/q$, where $dg_{s,a}$ is any supergradient.
  This implies $g_{s,a}(y) \leq g_{s,a}(x)(y/x) = g_{s,a}(x)(1+\alpha)$.
  By monotonicity, $g_{s,a}(q) \leq g_{s,a}(y) \leq g_{s,a}(x)(1+\alpha) \leq \hat{g}_{s,a}(x)(1+\alpha)^k \leq \hat{g}_{s,a}(q)(1+\alpha)^k$.
  Similarly, $g_{s,a}(q) \geq g_{s,a}(x) \geq g_{s,a}(y)/(1+\alpha) \geq \hat{g}_{s,a}(y)/(1+\alpha)^k \geq \hat{g}_{s,a}(q)/(1+\alpha)^k$.

  Finally, consider any $q \in (0,b)$.
  We claim that $g_{s,a}$ and $g_s$ are $\max_s V(s)$ Lipschitz.
  The result follows because each $g_{s,a}$ cannot have a steeper slope than one of the $g_{s'}$ than it depends on, and so forth, and the steepest slope at any sink is $\max_s V(s)$.
  It therefore holds that $g_{s,a}(q) \in [0, b \max_s V(s)]$, and we also have $\hat{g}_{s,a}(q) \in 0, b \max_s V(s)]$.
  So (\ref{eq:exante-dags-gsa-appx-2}) holds (even if we remove the factors $(1+\alpha)^k$).

  We now repeat the argument at $\hat{g}_s$.
  Although the optimization problem is slightly different, the arguments are all exactly the same.
  In particular, we just utilize $h_{s}(\lambda,\{q_{s,a}\}) = \sum_a \lambda(a) g_{s,a}(q_{s,a})$ and $\hat{h}_s$ in the exactly analogous way, and then the entire argument is the same.
  This completes the induction.
  
  Finally, as $|S|=m$, there can be at most $2m$ levels of induction, so using that $(1+\alpha)^{2m} \leq (1+\epsilon)$ completes the proof.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lemma:exante-dags-f-fptas}]
  Lemma \ref{lemma:exante-dags-approx-gs} implies the accuracy result.
  Considering the running time of the algorithm, at for each state and action, we build a piecewise linear concave function $\hat{g}_{s,a}$ with $T+2$ pieces, where $T$ is the floor of the solution to $b(1+\alpha)^T = 1$, i.e. $T = O(\tfrac{1}{\alpha}\ln(\tfrac{1}{b})) = O(\tfrac{|S|}{\epsilon}\ln(\tfrac{\max_s V(s)}{b}))$.
  For each piece of the piecewise function, we solve a concave optimization problem subject to a linear constraint, using constant-time oracles for concave functions (including access to supergradients) that have already been constructed, and do a bit of post processing.
  Similarly for $\hat{g}_s$.
  This is a polynomial number of functions to build, each taking polynomial time.
\end{proof}


\subsection{Proof of Corollary \ref{cor:dags-matroid-approx}}

The algorithm is as follows, given a parameter $\epsilon' \in (0,1)$.
\begin{itemize}
  \item Compute an ex-ante feasible algorithm $\OPT_Q$ with approximation guarantee $1-\epsilon'$ along with an estimate $W \leq \E[ \Welfof{\OPT_{Q}}]$ of its welfare.
  \item Set $c= \tfrac{(\epsilon')^2 W (\min_i Q_i)}{n}$, where the minimum is over nonzero entries of $Q$.
  \item Compute, in time poly$(\tfrac{1}{\epsilon'},\ln\tfrac{1}{c})$, an approximation to the concave function $\hat{f}_i(q)$ such that the expected welfare contribution $f_i(Q_i)$, satisfies $\frac{f_i(Q_i) - c}{1+\epsilon'} \leq \hat{f}_i(Q_i) \leq (f_i(Q_i) + c)(1+\epsilon')$.
  \item If $\hat{f}_i(Q_i) \leq \frac{8c}{\epsilon'}$, set $\hat{z}_i = 0$ and set $Q_i' = 0$. Otherwise, set $\hat{z}_i = \frac{\hat{f}_i(Q_i)}{Q_i(1+\epsilon')} - \frac{c}{Q_i}$ and set $Q_i' = Q_i$.
  \item Use the Matroid Prophet Algorithm (Appendix \ref{app:non-pandora}) on input $Q',\{\hat{z}_i\}$.
        That algorithm sets thresholds $\{T_i\}$ adaptively based on $Q',\{\hat{z}_i\}$, and the set selected so far.
        It uses \SAUP{} to interact with each arrival.
        We use \MAXSAUP{}$(\M_i,T_i)$, which by Proposition \ref{prop:max-saup} solves the \SAUP{} problem for MSPs optimally and efficiently, to interact with each arrival.
\end{itemize}

\begin{proof}[Proof of Corollary \ref{cor:dags-matroid-approx}]
  We will choose $\epsilon' = \Theta(\epsilon)$, $\epsilon' < \frac{1}{2}$ at the end.
  We note that our algorithm is computationally efficient, as it first uses our FPTAS to compute $Q$ and $\{\hat{f}_i\}$ on input $\epsilon'$, then uses the Matroid Prophets Algorithm to set thresholds based on $Q'$ and $\{\hat{z}_i\}$, and finally uses \MAXSAUP{} to interact with each arriving MSP.

  Let $S = \{i : \hat{z}_i = 0\}$.
  We have an ex-ante feasible algorithm $\OPT_{Q'}$ whose expected performance satisfies
  \begin{align*}
    \E[\Welfof{\OPT_{Q'}}]
    &= \E[\Welfof{\OPT_Q}] - \sum_{i \in S} f_i(Q_i)  \\
    &\geq \E[\Welfof{\OPT_Q}] - \frac{nc}{2\epsilon'} \\
    &\geq \E[\Welfof{\OPT_Q}] - \epsilon' W \\
    &\geq (1-\epsilon') \E[\Welfof{\OPT_Q}]  \\
    &\geq (1-\epsilon')^2 \E[\Welfof{\OPT}] .
  \end{align*}
  Furthermore, each $\hat{z}_i$ satisfies
  \begin{align*}
    \hat{z}_i
    &=    \frac{\hat{f}_i(Q'_i)}{Q'_i(1+\epsilon')} - \frac{c}{Q'_i}  \\
    &\leq \frac{f_i(Q'_i) + c}{Q'_i(1+\epsilon')}(1+\epsilon') - \frac{c}{Q'_i}  \\
    &\leq \frac{f_i(Q'_i)}{Q'_i} .
  \end{align*}
  As well, using $\epsilon' \leq \tfrac{1}{2}$, if $\hat{z}_i > 0$, then $c < \frac{\epsilon' \hat{f}_i(Q'_i)}{8} < \frac{\epsilon' (f_i(Q'_i)+c)}{4}$, which implies that $c < \frac{\epsilon' f_i(Q'_i)}{2}$.
  So
  \begin{align*}
    \hat{z}_i
    &\geq \frac{f_i(Q'_i) - c}{Q'_i(1+\epsilon')^2} - \frac{c}{Q'_i}  \\
    &\geq \frac{f_i(Q'_i) - 2c}{Q'_i(1+\epsilon')^2}  \\
    &\geq \frac{f_i(Q'_i)}{Q'_i} \frac{1-\epsilon'}{(1+\epsilon')^2} .
  \end{align*}
  We use the guarantee of Lemma \ref{lemma:matroid-cabinet} to argue an approximation factor.
  By our reductions,  and the fact that $\OPT_{Q'}$ acts independently on each MSP $i$, WLOG $\OPT_{Q'}$ is simulated by a Cabinets-Prophets instance where each drawer contains a value $\kappa_i^j$ corresponding to a deterministic policy $\pi_j$ on $\M_i$.
  The simulation opens drawers $\{j^*(i) \sim \lambda_i\}$ for some distributions $\{\lambda_i\}$, and obtains conditioned on selecting $i$ some $z_i = \frac{f_i(Q'_i)}{Q_i'} = \E[ \Aofon{\OPT_{Q'}}{i} \kappa_i^{j^*(i)}]$.
  Meanwhile, \MAXSAUP{} provides an efficient oracle to simulate selecting the drawer $j(i)$ maximizing $\E(\kappa_i^j - T_i)^+$ and obtains $\E[\Welfof{\ALG}] = \E[ \sum_{i=1}^n \Aofon{\ALG}{i} \kappa_i^{j(i)}]$.

  Therefore, Lemma \ref{lemma:matroid-cabinet} implies $\E[\Welfof{\ALG}] \geq \sum_i Q_i' \hat{z}_i \geq \frac{1-\epsilon'}{(1+\epsilon')^2} \E[\Welfof{\OPT'}] \geq \frac{(1-\epsilon')^3}{2(1+\epsilon')^2} \E[ \Welfof{\OPT}]$.
  By setting $\epsilon' = \Theta(\epsilon)$, we obtain a $\frac{1}{2}-\epsilon$ approximation.
\end{proof}








      







