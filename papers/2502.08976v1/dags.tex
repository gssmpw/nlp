\section{Combinatorial Markov Search} \label{sec:dags}

We now turn to our most general setting of Combinatorial Markov Search.
In this setting, each arrival $i$ consists of a single, general MSP process $\M_i$.
An algorithm advances some set of processes to \emph{sink} states and claims a feasible subset of MSPs.
We provide an approximation algorithm for this setting via a reduction to the Pandora Cabinets setting, and further use an efficient algorithm for \SAUP{} on MSPs to provide an efficient approximation algorithm in matroid settings.

\subsection{Setting and tools}

In the \emph{Combinatorial Markov Search$(\F)$} problem, the input is a set of $n$ Markov Search Processes $\M_1,\dots,\M_n$ along with a downward-closed set system $\F \subseteq 2^{[n]}$.
We write $\M_i = (S_i,A_i,P_i,C_i,V_i)$.
The MSPs are mutually independent.
An algorithm for this problem iteratively selects one of the MSPs, takes an action in it (incurring the associated cost), and so on.
Eventually, the algorithm chooses to halt, with each process $i$ in some state $s_i$, and outputs a subset $F \in \F$ of the processes to ``claim'', receiving reward $\sum_{i \in F} V_i(s_i)$
Because the analysis can be subtle, we formalize the space of algorithms in detail next.


\paragraph{Algorithms and histories.}
Given an instance $\F$, $(\M_i : i \in [n])$, a \emph{history} is a list of triples $((i^1,a^1,s^1),\dots)$ representing that in round $t$, the MSP $i^t$ was selected, action $a^t$ was taken, and the process transitioned to state $s^t$.
The list may also contain triples of the form $(i^t,\bot,\text{claim})$ and $(i^t,\bot,\text{don't claim})$.
Given a history $h = (\dots,(i^t,a^t,s^t),\dots)$, the \emph{induced transcript} on $\M_i$ is the list of pairs $r_i(h) = ((a^t,s^t) : i^t = i)$.
A history is \emph{valid} if every induced transcript is valid.
We assume unless otherwise noted that all histories discussed are valid.
A history is \emph{halted} if every induced transcript is halted.

An adaptive algorithm $\OPT$ is a possibly-randomized function that, given a non-halted history $h$, specifies a non-halted process $i$ and valid action $a$ for that process, including the choice to halt and claim or not claim in that process.
An online algorithm $\ALG$ is presented with the MSPs $i=1,\dots,n$ one at a time.
When $\M_i$ arrives, $\ALG$ executes a policy on $\M_i$, eventually choosing to halt and claim $i$ or not, before moving on to $\M_{i+1}$.
We use $\Aofon{\ALG}{i}$ as the indicator for whether $\ALG$ claims on $\M_i$, the global analogue to our use of $\B$ to indicate that local \emph{policy} claims.
We also use $\Iofonst{\ALG}{i}{a}{s}$ as the indicator variable for whether $\ALG$ takes action $a$ in state $s$ of $\M_i$, the global analogue of $\J$.

The \emph{welfare} of an algorithm $\ALG$ is total value claimed minus cost incurred over all $n$ processes in a given problem instance $\calI$:
\[  \Welfofon{\ALG}{\calI} = \sum_{\M_i \in \calI} \sum_{s \in S_i} \left( \Aofon{\ALG}{i} V_i(s) - \sum_{a \in A_i^s} \Iofonst{ALG}{i}{a}{s} C_i(a,s) \right)  \]

We now introduce a formal definition of \SAUP{} on MSPs.
\begin{definition}[\SAUP, Combinatorial Markov Search]
	
	Given an MSP $\M=(S,A,P,C,V)$ and a threshold $\tau$, the \emph{Markov Search single agent utility problem} \SAUP$(\M,\tau)$ problem is to find a policy $\pi$ maximizing
	\begin{align*}
		\argmax_\pi \E[\Perfofon{\pi}{\M} - \Bof{\pi} \tau] 
		.
	\end{align*}
\end{definition}


\subsection{Results}

Our main reduction is as follows:
\begin{theorem}[Pandora's Cabinets to Combinatorial Markov Search] \label{thm:pc-cms}
  Let $\F$ be a nonempty downward-closed set system.
  If there is an algorithm $\ALG'$ with an ex ante prophet inequality $\alpha$ for Pandora's Cabinets$(\F)$, then there is an (inefficient) algorithm $\ALG$ with an ex-post prophet inequality $\alpha$ for Combinatorial Markov Search$(\F)$.
  If $\ALG'$ is \SAUP-based, then $\ALG$ is \SAUP-based.
\end{theorem}
The full proof appears in Appendix \ref{app:dags}, along with further definitions and proofs used in the reduction.
\begin{proof}[Sketch]
  First, we formally show that following a deterministic policy on $\pi$ on an arbitrary MSP $\M_i$ ``induces'' a bandit: until halting, only one action is available in each state, i.e. the one prescribed by $\pi$.
  We then show that any algorithm for Combinatorial Markov Search, in particular ex-ante $\OPT$, ``induces'' on each MSP $\M_i$ some policy $\rho_i$ that can be represented as a probability distribution over deterministic policies, i.e. over bandits.
  We then can state that
  \[ \E[\Welfofon{OPT}{\calI}] = \sum_{i=1}^n \Perfofon{\rho_i}{\M_i}. \]
  We therefore reduce an instance $\calI$ of Combinatorial Markov Search to an instance $\calI'$of Pandora's Cabinets where each cabinet $i$ is an MSP $\M_i$ and each drawer $j$ contains the bandit induced by some deterministic policy $\pi_j$ on $\M_i$.
  There are finitely many drawers, and the bandits inside the different drawers of cabinet $i$ are correlated with each other, which is allowed by the Pandora's Cabinets problem.
  It is crucial that, unlike the \emph{ex-post} optimal algorithm, the ex-ante optimal algorithm is WLOG acting independently on each MSP subject to a certain probability of claiming.
  This allows us to ``lift'' a bound on $\OPT$'s performance on each process $\M_i$ of the form $\E[ \Perfofon{\pi_i}{\M_i}(\calI) ] \leq \E[ \Perfofon{\pi_i}{\M_i}(\calI')]$  to a bound on its overall performance, i.e. we show that $\E[\Welfofon{\OPT}{\calI}] \leq \E[\Welfofon{\OPT'}{\calI'}]$, where $\OPT'$ is the ex-ante optimal algorithm for Pandora's Cabinets.
  We can now implement any Pandora's Cabinets algorithm $\ALG'$ in the MSP setting by choosing a deterministic policy $\pi_j$ according to the rule for choosing a drawer $j$.
  We obtain an inefficient algorithm $\ALG$, as there are exponentially many drawers, with guarantee $\E[\Welfofon{\ALG}{\calI}] = \E[\Welfofon{\ALG'}{\calI'}]$.
  This proves the result.
\end{proof}

We now move to computationally efficient and incentive-compatible algorithms: \SAUP-based algorithms.
We provide an efficient subroutine to construct a policy $\pi^*$ solving \SAUP{} on a given MSP $\M=(S,A,P,C,V)$ with threshold $\tau$ via backward induction.
Our constructed policy $\pi^*$ will choose actions based solely on the current state $s$, so we abuse notation and let $\pi^*(s)$ represent the action $\pi^*$ takes on any transcript ending at the current state $s$.
We also use $(a,\pi)$ to represent the policy that takes a particular action $a$ in state $s$, and thereafter follows some policy $\pi$.


Before introducing the \MAXSAUP{} subroutine, we expand the Weitzman index notation from Subsection \ref{subsec:multistage} to the MSP setting relative to a given policy $\pi$.

As a fixed deterministic policy $\pi$ induces a bandit (see above; formalized in Appendix \ref{app:dags}), we can define Weitzman indices $\sigma_s^{\pi}$ and capped values $\kappa_s^{\pi}$ relative to that policy at each state $s$.
On subtlety is that in a DAG, as opposed to a tree, there can be multiple ways to reach a state.
We will define policies that rely only on knowledge of the current state $s$ and avoid this issue.
As usual, $\sigma^{\pi} = \sigma^{\pi}_{s^*}$, the index of the start state, and similarly for $\kappa^{\pi}$.

We construct our policy via backward induction from sink states by considering the best action to take at state $s$, given that we will follow our constructed policy $\pi$ from whatever state we transition to after $s$.
At state $s$, we consider the distribution over future transcripts based on the policy $\pi$ relative to some action $a\in A^S$ we could choose to take at the current state.
We denote this distribution $D_{s}^{a,\pi}$, and given a draw $r_s\sim D_{s}^{a,\pi}$ define $n(s)$ to be the state occurring in $r_s$ after $s$.

\begin{definition}[\MAXSAUP{} Subroutine]
	Given $\M=(S,A,P,C,V)$ and $\tau$, compute deterministic $\pi^*$ via backward induction, starting from sink states:
	\begin{itemize}
		\item At a sink $s$, $\pi^*$ takes action $(\bot, \text{claim})$ if $V(s) \geq \tau$, and action $(\bot, \text{don't claim})$ otherwise.
		$\kappa_{s}^{\pi^*} = \sigma_s^{\pi^*} = V(s)$.
		\item Given a non-sink state $s$, suppose $\pi^*$ is defined for every state $s'$ that is reachable from $s$.
		It follows that $\kappa_{s'}^{\pi^*}$ is defined for all such $s'$.
		Now, for each action $a \in A^s$, let
		\[ \upsilon(a) = \E_{r_s \sim D_{s}^{a,\pi^*}}[(\kappa^{\pi^*}_{n(s)}-\tau)^+] - C(a,s)\]
		If $\upsilon(a)$ is nonpositive for all actions $a\in A^s$, then $\pi^*$ halts\footnote{Referring to the definition of $\sigma_s^{\pi}$,
			note that $\upsilon(a)$ is nonpositive if and only if $\sigma_s^{a,\pi^*} \leq \tau$.}. Otherwise, $\pi^*$ takes action $a^* = \text{argmax}_a \upsilon(a)$.
	\end{itemize}
\end{definition}

\begin{lemma}\label{lemma:ms-saup-upper}
	For any policy $\rho$ on an MSP $\M$ and threshold $\tau$, the \SAUP{} value is at most
	\[ \E[\Perfofon{\rho}{\M} - \B^\rho\tau] \leq \max_{\text{deterministic }\pi}\E[(\kappa^\pi - \tau)^+].\]
\end{lemma}

\begin{proof}
	By definition, $\rho$ defines some distribution $D$ over deterministic policies, of which there are finitely many.
	Each deterministic policy $\pi$ induces a bandit process on $\M$ with a well-defined notion of $\kappa^\pi$.
	The \SAUP{} value can then be upper-bounded by
	\begin{align*}
		\E[\Perfofon{\rho}{\M} - \B^\rho\tau] &= \E_{\pi\sim D}[\E[\Perfofon{\pi}{\M} - \B^\pi\tau]]\\
		&\leq  \E_{\pi\sim D}[\E[\B^\pi\kappa^\pi - \B^\pi\tau]]& \text{Lemma \ref{lemma:magic-multistage}}\\
		&\leq  \max_\pi \E[(\kappa^\pi - \tau)^+].
	\end{align*}
\end{proof}

\begin{lemma}\label{lemma:maxsaup-efficient}
	The \MAXSAUP{} subroutine is efficient.
\end{lemma}

\begin{proof}
	Given that $\kappa_{s'}^{\pi^*}$ has already been calculated for each possible successor $s'$ of a state $s$ when taking action $a$, it takes polynomial time to calculate $\upsilon(a)$.
	For each state $s\in S$, $\upsilon(a)$ is calculated for every action $a\in A^s$, and so \MAXSAUP{} takes $O(\text{poly}(|S||A|))$ time.
\end{proof}

\begin{lemma}\label{lemma:maxsaup-nonexposed}
	For every state $s$, the deterministic policy $\pi^*$ computed by \MAXSAUP{} starting from $s$ is non-exposed on its induced bandit process starting from $s$.
	Furthermore, $\pi^*$ claims if and only if $\kappa^{\pi^*}_s \geq \tau$.
\end{lemma}

\begin{proof}
	From any state $s$, Let $a^*$ be the action which maximizes $\upsilon(a^*)$.
	$\pi_s$ halts if
	\begin{align*}
		 \E_{r_s \sim D_{s}^{a^*,\pi^*}}[(\kappa^{\pi^*}_{n(s)}-\tau)^+] &\leq C(a^*,s),
	\end{align*}
	and otherwise takes action $a$.
	By definition, $\sigma_s^{\pi_s}$ solves
	\begin{align*}
		\E_{r_s \sim D_{s}^{a^*,\pi^*}}[(\kappa^{\pi^*}_{n(s)}-\sigma_s^{\pi^*})^+] &= C(a^*,s),
	\end{align*}
	so $\pi_s$ does not advance on $s$ exactly when $\tau \geq \sigma_s^{\pi_s}$.
	This implies that it claims if and only if $\tau$ is exceeded by all visited $\sigma_s^{\pi_s}$, i.e. by $\kappa^{\pi^*}_s$.

	Thus, the policy $\pi^*$ halts the first time it arrives in a state $s'$ where $\tau \geq \sigma_{s'}^{\pi^*} = \sigma_{s'}^{\pi^*}$, meaning for every intermediate state $s_i$, $\sigma_{s'}^{\pi^*} \leq \tau < \sigma_{s_i}^{\pi^*}$ and the policy is nonexposed starting from $s$.

	The expected value of following $\pi^*$ from $s$ follows from non-exposure and Lemma \ref{lemma:magic-multistage}.
\end{proof}

\begin{lemma}\label{lemma:maxsaup-maximal}
	For every state $s$, \MAXSAUP{} computes a policy $\pi^* \in \argmax\limits_{\text{deterministic } \pi} \E[(\kappa_{s}^{\pi} - \tau)^+]$.
\end{lemma}

\begin{proof}
	We prove this inductively, starting from sink states.
	If $s$ is a sink state, then $\pi_{s}$ claims $s$ exactly when $V(s) \geq \tau$, maximizing $V(s) - \tau = \max_{\text{deterministic } \pi_s} \E[(\kappa_{s}^{\pi_s} - \tau)^+]$.

	If $s$ is not a sink state, we assume it has some set $S'$ of states it may transition to, and that we have already computed the deterministic policy $\pi^*$ for each $s'\in S'$ such that $\pi^*$ achieves
	\begin{align*}
		\max_{\text{deterministic }\pi'} \E[(\kappa_{s'}^{\pi'} - \tau)^+].
	\end{align*}

	For any action $a$,
	\begin{align*}
		\upsilon(a) &= \E_{r_s \sim D_s^{a,\pi^*}}[\kappa_{n(s)}^{\pi^*}-\tau)^+] - C(a,s)\\
		&= \E_{r_s \sim D_s^{a,\pi^*}}[(\kappa_{n(s)}^{\pi^*}-\tau)^+] - \E_{r_s \sim D_s^{a,\pi^*}}[(\kappa_{n(s)}^{\pi^*}-\sigma_s^{\pi^*})] \\
		&= \E_{r_s \sim D_s^{a,\pi^*}}[(\kappa_{n(s)}^{\pi^*}-\tau)^+ - (\kappa_{n(s)}^{\pi^*}-\sigma_s^{\pi^*})^+] .
	\end{align*}
	We note that the $\kappa_{n(s)}$ and $\sigma_{n(s)}$ are not dependent on the choice of action at state $s$, only on $\pi$ continuing after the transition out of $s$ is realized.
	There are three cases for the above expression:
	\begin{itemize}
		\item If $\kappa_{n(r_s)}^{(a,\pi)} \geq \sigma_s^{(a,\pi)} \geq \tau$, then the above expression is equal to $\E [(\sigma_s^{a,\pi}-\tau)^+]$.
		\item If $\sigma_s^{a,\pi} \geq \kappa_{n(r_s)}^{a,\pi} \geq \tau$, then the above expression is equal to $\E [(\kappa_{n(r_s)}^{a,\pi}-\tau)^+]$.
		\item If either $\tau > \kappa_{n(r_s)}$ or $\tau > \sigma_s^{(a,\pi)}$, then the above expression is 0. In this case, \MAXSAUP{} will never advance with action $a$.
	\end{itemize}
	Therefore,
	\begin{align*}
		\upsilon(a)	&= \E_{r_s \sim D_s^{a,\pi^*}}[(\min\{\kappa_{n(s)}^{\pi^*},\sigma_s^{\pi^*}\}-\tau)^+]  \\
		&= \E_{r_s \sim D_s^{a,\pi^*}}[(\kappa_s^{\pi^*}-\tau)^+],
	\end{align*}
	and \MAXSAUP{} chooses the action $a$ which maximizes the above expression.
\end{proof}

\begin{proposition} \label{prop:max-saup}
	\MAXSAUP$(\M, \tau)$ is a polynomial-time optimal solution to the MSP \SAUP{} problem.
\end{proposition}

\begin{proof}
	Lemma \ref{lemma:maxsaup-efficient} gives computational efficiency.
	Let $\rho$ be any policy inducing any distribution $D$ over policies.
	We have
	\begin{align*}
		\E[\Perfofon{\rho}{\M} - \B^\rho\tau]
		&\leq \max_{\text{deterministic }\pi}\E[(\kappa^\pi - \tau)^+]   & \text{Lemma \ref{lemma:ms-saup-upper}}  \\
		&=	\E[(\kappa^{\pi^*} - \tau)^+]	& \text{Lemma \ref{lemma:maxsaup-maximal}}  \\
		&=	\E[\Bof{\pi^*}(\kappa^{\pi^*} - \tau)]  \\
		&=	\E[ \Perfofon{\pi^*}{\M} - \Bof{\pi^*} \tau] .
	\end{align*}
	The last two equalities follow from Lemma \ref{lemma:maxsaup-nonexposed}, with the last equality combining non-exposure with Lemma \ref{lemma:magic-multistage}.
\end{proof}

We need one more component before we can implement efficient prophet inequalities for Combinatorial Markov Search: an efficient, or close-to-efficient, algorithm for computing the ex-ante $\OPT$.
With such an algorithm, we can compute the thresholds needed for \SAUP-based prophet inequalities, then use \MAXSAUP{} to efficiently handle each arrival.
The full proof appears in Appendix \ref{app:dags}.

\begin{proposition} \label{prop:exante-dags-efficient}
  Let $\F$ be a downward-closed system where $\PF$ has an efficient separation oracle, such as any matroid constraint.
  Then there is an FPTAS for computing the ex-ante optimal for Combinatorial Markov Search$(\F)$.
\end{proposition}
\begin{proof}[Sketch]
  As with the ex-ante cabinets problem (Lemma \ref{lemma:cabinets-nonadaptive-opt}), the ex-ante $\OPT$ induces some point $Q \in \PF$ where $Q_i = \Pr[\text{claim $i$}]$.
  WLOG, each process is optimized independently subject to $Q$.
  Let $f_i(q)$ be the optimal performance of any policy for $\M_i = (S_i,A_i,P_i,C_i,V_i)$ subject to claiming with probability at most $q$.
  Then the problem is to maximize $\sum_{i=1}^n f_i(Q_i)$ subject to $Q \in \PF$.
  We show how to construct, in time poly$(\text{input}, \tfrac{1}{\epsilon})$, an explicit concave, monotone increasing $\hat{f}_i$ that approximates $f_i$ closely enough everywhere.
  Here ``closely enough'' involves both a multiplicative and additive approximation (due to behavior near zero), where we are able to choose the additive constant small enough later to get a pure multiplicative approximation.
  The problem then becomes convex optimization subject to a linear constraint with an efficient separation oracle.

  To construct $\hat{f}_i$, we define $g_s$ for each state $s \in S_i$.
  $g_s(q)$ is the optimal performance achievable by starting from $s$, over policies that must claim with probability at most $q$.
  In particular, $g_{s^*} = f_i$, where $s^*$ is the start state.
  We also define $g_{s,a}(q)$, the same function but assuming that action $a$ has already been chosen and its cost $C(a,s)$ paid.
  We show that each $g_s$ and $g_{s,a}$ is a concave, monotone increasing function.
  We then proceed by backward induction on the DAG structure.
  At each step, we construct explicit concave, monotone increasing functions $\hat{g}_s,\hat{g}_{s,a}$ with piecewise linear functions, in polynomial time, using the approximations computed at the neighbor vertices.
  The error increases by a multiplicative $(1+\alpha)$ factor at each level of induction; by choosing $\alpha$ small enough and a fine enough discretization, the total error at the final level, i.e. the start state, is bounded by $(1 + O(\epsilon))$.
\end{proof}

Putting together all of the pieces in this paper, we obtain our main result.
We require the ``trick'' introduced in our ex-ante Cabinets-Prophets matroid prophet inequality, Theorem \ref{thm:classic-cp-saup}, that makes the algorithm robust to an approximate ex-ante $\OPT$.
We obtain in the end our main result.
\begin{corollary} \label{cor:dags-matroid-approx}
  For Combinatorial Markov Search with matroid feasibility constraints $\F$, there is an online \SAUP{}-based algorithm, running in time polynomial in the instance size and $\frac{1}{\epsilon}$, guaranteeing an ex-ante prophet inequality of $\frac{1}{2}-\epsilon$.
\end{corollary}
