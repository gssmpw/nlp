%

\documentclass{article}

\input{math_commands.tex}

%
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
%
\usepackage{booktabs} %
\usepackage{multirow,multicol}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\renewcommand{\eqref}[1]{(\ref{#1})}
%
%
%
%
\usepackage{hyperref}

%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{standalone}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usetikzlibrary{calc,fit}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{shapes.misc}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}

%
\usepackage[preprint]{icml_style}

%
%

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}


\usepackage{xspace}



\usepackage{newtxmath}

%
\usepackage[nameinlink,capitalize,noabbrev]{cleveref}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
\usepackage[textsize=tiny]{todonotes}

\newcommand{\sstd}[1]{\textcolor{black}{\tiny{$\pm #1$}}}
\newcommand{\highlight}[1]{\colorbox{blue!10}{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand{\iid}{\textit{i.i.d.}\xspace}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}

\newcounter{daggerfootnote}
\newcommand*{\daggerfootnote}[1]{%
    \setcounter{daggerfootnote}{\value{footnote}}%
    \renewcommand*{\thefootnote}{\fnsymbol{footnote}}%
    \footnote[2]{#1}%
    \setcounter{footnote}{\value{daggerfootnote}}%
    \renewcommand*{\thefootnote}{\arabic{footnote}}%
    }

\makeatletter
\renewcommand{\paragraph}[1]{\textbf{#1}}
\makeatother


\icmltitlerunning{In-Context Parametric Inference: Point or Distribution Estimators?}

\begin{document}

\twocolumn[
\icmltitle{In-Context Parametric Inference: Point or Distribution Estimators?}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sarthak Mittal}{udem,mila}
\icmlauthor{Yoshua Bengio}{udem,mila}
\icmlauthor{Nikolay Malkin}{edinburgh}
\icmlauthor{Guillaume Lajoie}{udem,mila}
\end{icmlauthorlist}

\icmlaffiliation{udem}{Universit\'e de Montreal}
\icmlaffiliation{mila}{Mila}
\icmlaffiliation{edinburgh}{University of Edinburgh}

\icmlcorrespondingauthor{Sarthak Mittal}{sarthmit@gmail.com}

%
%
%
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

%

%
%
%
%
%

%
\printAffiliationsAndNotice{} %

\begin{abstract}
Bayesian and frequentist inference are two fundamental paradigms in statistical estimation. Bayesian methods treat hypotheses as random variables, incorporating priors and updating beliefs via Bayes' theorem, whereas frequentist methods assume fixed but unknown hypotheses, relying on estimators like maximum likelihood. While extensive research has compared these approaches, the frequentist paradigm of obtaining point estimates has become predominant in deep learning, as Bayesian inference is challenging due to the computational complexity and the approximation gap of posterior estimation methods. However, a good understanding of trade-offs between the two approaches is lacking in the regime of amortized estimators, where in-context learners are trained to estimate either point values via maximum likelihood or maximum a posteriori estimation, or full posteriors using normalizing flows, score-based diffusion samplers, or diagonal Gaussian approximations, \textit{conditioned} on observations. To help resolve this, we conduct a rigorous comparative analysis spanning diverse problem settings, from linear models to shallow neural networks, with a robust evaluation framework assessing both in-distribution and out-of-distribution generalization on tractable tasks. Our experiments indicate that amortized point estimators generally outperform posterior inference, though the latter remain competitive in some low-dimensional problems, and we further discuss why this might be the case.\daggerfootnote{Official code for the work can be found \href{https://github.com/sarthmit/parametric_inference}{here}.}
\end{abstract}

%
%
%
%
%
%
%
%

\section{Introduction}

Bayesian and requentist inference represent two core principles to statistical estimation
%
and machine learning that provide complementary approaches to model training and evaluation. Bayesian methods treat hypotheses $\theta$, such as model parameters, as random variables and use data $\gD$ as evidence to update posterior beliefs $p(\theta\mid\gD)$, whereas frequentist methods, such as maximum likelihood and moment methods, assume fixed but unknown hypotheses $\theta^*$ and estimate them through optimization. Despite the dominance of the Bayesian approach in the earliest successes of generative modeling \citep[][among many others]{hinton1995wake,neal1996bayesian}, the frequentist paradigm of obtaining point estimates has become predominant in deep learning, as Bayesian inference is challenging due to the complexity of estimating the posterior distribution \citep{blei2017variational}.

An understanding of trade-offs between the two approaches, and between different methods for posterior approximation, is lacking in the regime of amortized estimators \citep{kingma2013auto,rezende2014stochastic,garnelo2018neural}, where models $q_\phi(\theta\mid\gD)$ that take the dataset $\gD$ explicitly as input are trained to estimate either point values or parametric distributions over $\theta$. The Bayesian posterior predictive minimizes empirical risk, and it should be optimal to use it in prediction problems compared to a point estimate \citep{devroye1996probabilistic}. However, this optimality may not hold when approximate families that cannot express the full posterior are used, or when an amortization gap is introduced by a model that takes data as input and must generalize to new observations \citep{cremer2018inference} in-context.

The consequences of such limitations of amortized inference have been noted in a sequence of works in diverse areas of deep learning. For instance, higher variational bounds on likelihood do not necessarily lead to better models in VAEs \citep{rainforth2018tighter} or to more effective approximations to the target distribution in variational methods \cite{blessing2024beyond}. Similarly, for Bayesian neural networks (BNNs), the effectiveness of simple approximating families for the posterior compared to methods like MCMC has been debated \citep{ritter2018scalable}; indeed, posteriors need to be integrated at lower temperatures to useful approximate posterior predictive distributions \citep{wenzel2020good,adlam2020cold}. These findings are also relevant to recent work on in-context learning in large language models, which approximates Bayesian inference at optimality \citep{xie2021explanation,akyurek2022learning} but falls short in practice \citep{garg2022can,falck2024incontext}.

In this paper, we conduct a comparative analysis on several problem settings, from linear models to shallow neural networks, to assess the performance of different inference methods in both in-distribution (ID) and out-of-distribution (OoD; misspecified) settings. We compare various generative modeling, variational and point estimation algorithms for inferring underlying parameters (\cref{fig:figure1}). Our experiments indicate that amortized point estimators generally outperform Bayesian methods, especially on high-dimensional tasks. Our results contribute evidence from simple, well-understood models and inference procedures to the debate on the utility of approximate Bayesian inference in deep learning, especially in an \textit{in-context} setting.

\section{Problem Setup}

We consider a generative model of sets of \iid observations $\gD = \{(\vx_i, \vy_i)\}_{i=1}^k$, assumed to be sampled from a ground-truth underlying distribution: $(\vx_i,\vy_i)\sim\chi$. Given a parametric family of conditional models $p(\vy \mid \vx, \theta)$, we would like to infer the parameter $\theta$ that best explains the data $\gD$. Inferring $\theta$ allows us to make predictions of $\vy$ on new data points $\vx$, by computing $p(\vy\mid\vx,\theta)$. 

If the true model $p(\vy\mid\vx)$ -- the conditioning of $\chi$ on $\vx$\footnote{We use distributions and probability or mass functions interchangeably and assume that disintegration of the joint distribution is possible, as we consider only variables valued in $\mathbb{R}^n$ with absolutely continuous densities and in discrete spaces.} -- lies in the model class considered (that is, it is equal to $p(\vy\mid\vx,\theta^*)$ for some $\theta^*$), then we hope to recover the parameter $\theta^*$, or a model equivalent to it. If the true model does not lie in the model class considered, the inference problem is said to be misspecified.

There are two main paradigms for estimating $\theta$ from $\gD$: frequentist and Bayesian. Frequentist methods treat $\theta$ as fixed but unknown and estimate it by optimizing a functional that is maximized when $\theta=\theta^*$. Bayesian methods treat $\theta$ as a random variable and approximate a distribution over it by positing a prior distribution $p(\theta)$ and matching, by various means, the posterior distribution $p(\theta\mid\gD)$. 

Both approaches primarily operate on a fixed set of observations $\gD$ and rely on iterative methods to infer $\theta$. In this work we are interested in in-context estimators\footnote{We use \emph{in-context} and \emph{amortized} estimators interchangeably.} that explicitly take $\gD$ as input and provide an estimate for $\theta$, and can generalize to new datasets zero-shot.

We briefly review the frequentist and Bayesian methods below, with a focus on amortized estimators.


\subsection{Frequentist Estimation}

%
The most common frequentist estimator is the maximum likelihood estimator (MLE), which estimates $\theta$ by maximizing the likelihood of the data $\gD$ under the model $p(\vy\mid\vx,\theta)$. The MLE is defined as
\begin{equation}\label{eq:mle}
    \theta_{\rm MLE} 
    = \argmax_\theta p(\gD\mid\theta)
    = \argmax_\theta \sum_{i=1}^k \log p(\vy_i\mid\vx_i,\theta),
\end{equation}
supposing this optimum exists. Other frequentist estimators include moment methods \citep{pearson1936method}. An in-context version of frequentist estimators can be seen as
\begin{align}
f(\gD; \phi^*) \approx \theta_{MLE}(\gD) \qquad\text{where}\quad \gD\sim\chi
\end{align}
where the parameters $\phi^*$ can analogously be estimated as
\begin{align}
    \phi^* = \arg\max_\phi \mathbb{E}_{\gD \sim \chi}  \log p(\gD | \theta = f(\gD; \phi))
\end{align}


\subsection{Bayesian Estimation}

%
Given a prior distribution $p(\theta)$, we have the posterior distribution
\begin{equation}\label{eq:bayes}
    p(\theta\mid\gD) 
    \propto p(\gD\mid\theta)p(\theta)
    = p(\theta)\prod_{i=1}^k p(\vy_i\mid\vx_i,\theta).
\end{equation}
The simplest Bayesian estimator is the a point estimate of the mode of $p(\theta\mid\gD)$, called the maximum a posteriori (MAP) estimator:
\begin{align}\nonumber
    \theta_{\rm MAP} 
    &= \argmax_\theta p(\theta\mid\gD)\\\label{eq:map}
    &= \argmax_\theta \log p(\theta) + \sum_{i=1}^k \log p(\vy_i\mid\vx_i,\theta)
\end{align}
(note the similarity with \eqref{eq:mle}). \emph{Posterior concentration} results (\citet{doob1949}; see also \citet{miller2018detailed,miller2021asymptotic}) show that, under some conditions, the posterior distribution concentrates around the true parameter value $\theta^*$ as $k\to\infty$, meaning that prior term in \eqref{eq:map} becomes irrelevant and the MAP and MLE converge to the same value. Such results hold almost surely with respect to the \iid sampling of data from the true distribution $\chi$ and assume the model class $p(\vy\mid\vx,\theta)$ contains the true model.

While the MAP estimator approximates the posterior $p(\theta\mid\gD)$ in \eqref{eq:bayes} by a point mass at $\theta_{\rm MAP}$, other Bayesian methods may approximate it by more complex distributions, such as parametric models $q_\phi(\theta)$. The goal is to infer parameters $\phi^*$ that bring $q_\phi$ close to the true posterior in some measure of divergence $\mathbb{D}$,
\begin{align}
    \phi^* = \arg\min \mathbb{D}\left(p(\cdot | \gD), q_\phi(\cdot)\right)
\end{align}
When $q_\phi$ is a model taking $\gD$ explicitly as input, it is called an \emph{amortized} estimator. The goal of amortized inference is to learn a model $q$ that can approximate the true posterior in a fast and scalable manner. 

Amortized estimators are the focus of this work. The parametrization of amortized inference models will be discussed in \cref{sec:amortization_icl} and training objectives in \cref{sec:training_objectives}.

\begin{figure}
    \centering
    %
    \includestandalone[width=1\linewidth]{figures/figure1}
    \vspace{-1mm}
    \caption{A hierarchical decomposition of the suite of in-context estimators which we compare in this work. Each estimator explicitly looks at the observations as input and optionally takes the prior into account to provide either a single point estimate, or a distribution, for the parameters. We consider three broad classes of Bayesian methods:
    %
    Sample-based, which require access to simulated data, Variational, which require access to the joint density, and those that require both.}
    \label{fig:figure1}
    \vspace{-5mm}
\end{figure}
\subsection{Posterior Predictive Distributions} 

Once $\theta$ is estimated by a distribution $q_\phi(\theta)$ we can make predictions on new data points $\vx$ by computing the posterior predictive distribution
\begin{align}\nonumber
    p(\vy\mid\vx,\gD) 
    &= \int p(\vy\mid\vx,\theta)p(\theta\mid\gD)\,d\theta \\\label{eq:postpred}
    &\approx \E_{\theta\sim q_\phi(\theta)} p(\vy\mid\vx,\theta).
\end{align}
For point estimates, we can use the MAP or MLE estimate of $\theta$ in place of the expectation in \eqref{eq:postpred}. 

The main question we address is: which estimates of the model parameters $\theta$ give the best predictions on new data points $\vx$ via the posterior predictive \eqref{eq:postpred}?


\subsection{Amortization by In-Context Estimation}

\label{sec:amortization_icl}
Traditionally, in-context learning (ICL)~\citep{dong2022survey} over a training set $\gD$ refers to the ability of a pretrained sequence model (\eg, a LLM) to solve novel tasks when presented with the examples from $\gD$ in-context. A number of works~\citep{akyurek2022learning,garg2022can,xie2021explanation,von2023transformers,mittal2024does,elmoznino2024context} formalize this form of ICL from the perspective of algorithmic tasks as directly modeling the posterior predictive model $\arg\max_\phi \mathbb{E}_{\vx, \vy, \gD \sim \chi}\log p_\phi(\vy | \vx, \gD)$, where $\chi$ defines some data distribution. 

While ICL methods often model the posterior predictive, they can be adapted to perform parametric inference given a likelihood function \citep{mittal2023exploring}. Generally, parametric inference relies on approximate procedures to obtain estimates for a particular task and set of observations -- \eg~ MLE of neural network parameters relies on gradient descent or posterior samples through MCMC approaches, for a fixed set of observations $\gD$ (training set). Instead, we are interested in amortized / in-context estimators that explicitly take the observations as input and output the estimates, whether probabilistic or point. Such an in-context estimator's task is to model parameter inference, as opposed to prediction, and can provide estimates for novel tasks in zero-shot and compute-efficient manner. We rely on a transformer architecture to model conditioning on $\gD$ and omit positional embeddings to satisfy permutation invariance of the posterior given \iid samples. 

%
%

\section{Amortized Inference Training Objectives}

\label{sec:training_objectives}
We formalize different training objectives and parametrizations for learning amortized estimators to approximate either point estimates or full posteriors. Within posterior estimation, we consider three classes of algorithms based on the learning signal used: sample-based, variational methods or a combination of the two. Refer to \cref{fig:figure1} for a hierarchical view over the different in-context estimators considered.


\subsection{Point Estimates}


For the point estimates $\theta_{\rm MLE}$ and $\theta_{\rm MAP}$, an amortized model $\theta=f(\gD;\phi)$ directly outputs the parameter value $\theta$ given the data $\gD$. The optimization problems in \eqref{eq:mle} and \eqref{eq:map} can be directly used as the objectives for training $\phi$; for example, for MAP:
\begin{equation}\label{eq:mle_loss}
    \gL_{\rm MAP}(\gD) = \log p(f(\gD;\phi)) + \sum_{i=1}^{|D|} \log p(\vy_i\mid\vx_i,f(\gD;\phi)).
\end{equation}
An unbiased estimator of the gradient of $\gL_{\rm MAP}(\gD)$ can be obtained by a stochastic surrogate loss, where the sum over $\gD$ is replaced by a sum over a minibatch of data points $\gB\subset\gD$, reweighted by $\frac{|\gD|}{|\gB|}$. The MLE loss is similar, with no prior term added.

\begin{figure*}
    \centering
    \includegraphics[width=0.35\textwidth]{figures/all.pdf}\hspace{3mm}
    \includegraphics[width=0.3\textwidth]{figures/category.pdf}
    \includegraphics[width=0.3\textwidth]{figures/source.pdf}
    \vspace{-1mm}
    \caption{We plot ranking metrics aggregated over multiple tasks with varying dimensionalities, where the percentage describes the ratio of times that particular in-context estimator outperformed the others. All the different estimators compete against each other on the \textit{left} pie-chart, the \textit{middle} one describes different choices for $q_\phi$ while the \textit{right} chart describes the kind of signal used for training.}
    \vspace{-5mm}
    \label{fig:rank}
\end{figure*}


\subsection{Posterior Estimates}

For full Bayesian posterior estimation, an amortized density $q_\phi(\theta | \gD)$ approximates the true posterior $p(\theta | \gD)$. It can be trained with different objectives and distinct $q_\phi$ parametrizations, with the learning signal from either joint $(\theta, \gD)$ samples or the unnormalized target density $p(\theta | \gD) \propto p(\gD, \theta)$, or both. The details are provided below.


\subsubsection{Sample-Based Methods} 

Sample-based methods treat the approximation of $q_\phi(\theta\mid\gD)$ as a generative modeling problem. They assume that we have access to samples $(\theta,\gD)$ from the true data-generating process $\chi$. In particular, this requires the problem to be well-specified and for the generative model to expose the parameter of the conditional model, \ie, the true model proceeds via generation of $\theta$, \iid generation of inputs $\vx_i$, and generation of outputs $\vy_i$ from the model $p(\vy_i\mid \vx_i,\theta)$. 

Given samples $(\theta,\gD)\sim\chi$, we can fit a generative model, conditioned on $\gD$, to the samples $\theta$. If the objective is the log-likelihood of the samples $\theta$ under the generative model, it amounts to minimization of the forward KL divergence between the generative model and the true posterior:
\begin{align}\nonumber
    &\E_{(\theta,\gD)\sim\chi}[-\log q_\phi(\theta\mid\gD)]
    \\\label{eq:forward_kl}
    =&\E_{\gD\sim\chi}\,\KL(p(\theta\mid\gD)\|q_\phi(\theta\mid\gD))+{\rm const}.
\end{align}
Any family of generative models $q_\phi$ can be used in the approximation. However, unbiased estimates of the log-likelihood gradient in \eqref{eq:forward_kl} require that the data and ground-truth parameter values $(\theta,\gD)$ come precisely from the true data-generating process. For this to work, it is also important that the empirical distribution of $(\theta,\gD)$'s sufficiently ``covers" the region associated with the target $(\theta,\gD)$ of interest at test-time, so that $q_\phi$ will generalize well there.

\paragraph{Gaussian modelling.} One fits a Gaussian distribution, with mean and covariance output by a model (\eg, a neural network) conditioned on $\gD$. The optimal model, which minimizes \eqref{eq:forward_kl}, matches the first and second moments of the true posterior $p(\theta\mid\gD)$ for every $\gD$ in the support of $\chi$.

\paragraph{Normalizing flows}. They \citep{papamakarios2021normalizing,kobyzev2020normalizing} apply a sequence of trainable invertible transforms to convert a simple initial density, \eg~ $\gN(0, \text{I})$, to a more complex one. The invertible transformations are chosen such that the jacobian in the change of density can be easily computed, and training is done by directly optimizing the likelihood / minimizing \eqref{eq:forward_kl}.

Continuous-time normalizing flows side-step the problem of careful design of invertible transforms \citep{chen2018neural} and model $q_\phi$ as an ordinary differential equation transforming a simple distribution, \eg~ standard normal. Further, flow-matching methods~\citep{lipman2022flow,tong2023improving} provide efficient ways of training continuous-time normalizing flows in a simulation-free manner without constraining architecture choice.

\paragraph{Score-Based Diffusion}. Diffusion models~\citep{song2020score,song2020improved,song2020denoising,ho2020denoising,nichol2021improved} use stochastic differential equations to model $q_\phi$ by considering a fixed noising process and learning its reverse dynamics by estimating the time-conditioned score function. Akin to flow-matching, they are trained in a simulation-free manner and are equivalent to optimizing a variational upper bound on \eqref{eq:forward_kl}.
%
%
%

\input{Tables/fixed_dim_ensembled}

\subsubsection{Variational Methods}


Some methods for approximating the posterior do not rely on unbiased samples from the true data-generating process, but rather aim to fit a model to match $q(\theta\mid\gD)$ to the true posterior $p(\theta\mid\gD)$ given access to the joint $p(\theta,\gD)=p(\theta)p(\gD\mid\theta)$, to which $p(\theta\mid\gD)$ is proportional, at any $\theta$.

\paragraph{Reverse KL.} While the forward KL divergence $\KL(p(\theta\mid\gD)\|q_\phi(\theta\mid\gD))$ cannot be optimized exactly without samples from $p$, we can optimize the reverse KL divergence $\KL(q_\phi(\theta\mid\gD)\|p(\theta\mid\gD))$ exactly. The reverse KL objective can be seen as an entropy-regularized maximum likelihood:
\begin{align}\nonumber
    &\KL(q_\phi(\theta\mid\gD)\|p(\theta\mid\gD)) \\\label{eq:reverse_kl}
    =\,&\E_{\theta\sim q_\phi(\theta\mid\gD)}[-\log p(\theta\mid\gD)] - \gH[q_\phi(\theta\mid\gD)].
\end{align}
Note that there is no expectation over $\gD$, and we are free to optimize \eqref{eq:reverse_kl} for $\gD$ sampled from any distribution over datasets of interest, or even for a single fixed $\gD$. 

It is important to note that while the forward KL approaches are mean-seeking and can overestimate the variance in its approximation, the reverse KL methods are mode-seeking instead and can underestimate the variance or only model a few modes \citep{bishop2006pattern}. 
%
%

\paragraph{Diffusion samplers.} \emph{Diffusion samplers} are diffusion models fit to sample a distribution with given unnormalized density -- in this case, the joint $p(\theta)p(\gD\mid\theta)$ -- rather than to maximize a bound on log-likelihood on a data sample. Various methods for training diffusion sampler exist \citep{zhang2021path,vargas2023denoising,sendera2024improved}, most of them requiring simulation of the denoising process on each training step. One exception is the method of denoising energy matching \citep[DEM][]{akhound2024iterated}, which trains the denoiser by regressing to a biased but asymptotically constitent Monte Carlo estimate of the score function of the true posterior $p(\theta\mid\gD)$.

\input{Tables/fixed_dim_2_ensembled}
\paragraph{MCMC.} While they are not amortized variational methods, Monte Carlo Markov chain (MCMC)~\citep{welling2011bayesian,hoffman2014no,chen2014stochastic} methods can be used to draw samples from $p(\theta\mid\gD)$ given access only to the joint $p(\theta,\gD)=p(\theta)p(\gD\mid\theta)$. The samples can then be used to estimate the expectation defining the posterior predictive \eqref{eq:postpred}.
%
In general, MCMC methods have a guarantee of convergence to the target distribution given enough samples or iterations, but convergence can be slow \citep{andrieu2003introduction,gilks1996strategies,neal2012mcmc}. Some MCMC methods, such as Langevin and Hamiltonian MCMC, also require access to the gradient of the log-likelihood.

\subsubsection{Sample-based + Variational Methods} 
One can combine the two estimation procedures outlined above. We consider an equally-weighted combination of forward and reverse KL as the divergence metric, called symmetric KL, for learning $q_\phi$ in cases where it is modeled as a Gaussian distribution or a discrete normalizing flow.
%
%
%

\input{Tables/variable_dim_ensembled}
%
\section{Experiments}

Our goal in this comparative study is to evaluate the amortized estimation procedures discussed in \cref{sec:training_objectives} for both in-distribution (ID) and out-of-distribution (OoD) generalization. We consider variants of point-estimation methods, forward and reverese KL approaches including diffusion and normalizing flows, and symmetric KL objective, with a focus on explicit conditioning on the set of observations.
We evaluate the Bayesian and frequentist in-context estimators on a wide suite of probabilistic models through the lens of predictive performance, and discuss the suite of tasks, baselines and metrics considered in this study below.

\looseness=-1
\textbf{Tasks}.
We consider estimating the mean of a Gaussian distribution (GM), means of a Gaussian Mixture Model (GMM), parameters of (non-)linear regression (NLR/LR) and classification (NLC/LC) models, where the nonlinear problems are modeled through a neural network and the parameters correspond to the parameters of the network. We refer the readers to \cref{apdx:probabilistic_models} for details about the probabilistic models, including the likelihood and prior considered in each setup. We further evaluate the different in-context estimators on OoD transfer in the case of model misspecification and its applications to real-world tabular tasks, where the data-generating distribution shifts between training and evaluation.

\textbf{Baselines}. To understand whether the in-context estimators achieve reasonable performance, we consider multiple non-amortized procedures as reference: sampling from the prior (Random), the true posterior (True Posterior), iterative sampling procedures like single or multiple chains of Langevin and Hamiltonian (HMC) MCMC, and optimizing the parameters through MLE (Optimization). 

\textbf{Metrics}. Aligned with our goal towards better predictions, we leverage predictive metrics like $L_2$ loss and accuracy under the parameters inferred by the in-context estimators. This provides us two choices of metrics: expected loss/accuracy and ensemble based metric. For regression problems, the former can be seen as
\begin{align}
    \mathbb{E}_{\vx_*,\vy_*, \gD \sim \chi}\mathbb{E}_{\theta \sim q_\phi(\cdot | \gD)} \norm{\hat{\vy} - \vy}^2
\end{align}
where $\hat{\vy}$ is the mode of $p(\vy | \vx, \theta)$, while the ensembling based metric can be seen as
\begin{align}
    \mathbb{E}_{\vx_*,\vy_*, \gD \sim \chi} \norm{\mathbb{E}_{\theta \sim q_\phi(\cdot | \gD)}\hat{\vy} - \vy}^2
\end{align}
Similarly for classification, we rely on accuracy instead of $L_2$ loss and consider the mode of different $\hat{\vy}$ for ensembling as opposed to averaging.

We primarily consider the ensemble-based metric for evaluation since it is easily available for full posterior approximations due to the ease of sampling from them. Additionally, since point estimation methods only provide a single parameter value, the two metrics are trivially the same in their case. See \cref{apdx:metrics} for details on the metrics.

\input{Tables/misspec_ensembled}

\subsection{Evaluating in-distribution parameter inference}

In-context estimators, whether point or posterior, can be leveraged to generalize to novel tasks zero-shot after being trained over multiple different datasets $\gD_{\rm train} \sim \chi_{\rm train}$. We first test for in-distribution generalization by sampling novel tasks $\gD_{\rm test} \sim \chi_{\rm train}$ and evaluating how well parameter samples generalize under the predictive metrics on $\gD_{\rm test}$. The benchmark consists of $88$ tasks, where each task is defined by a different probabilistic model configuration, leading to the training of $324$ models for each in-context estimator considered.

We provide a high-level visualization of the outcome of our experiments in \cref{fig:rank}, which demonstrates the proportion of tasks each (class of) estimator outperformed its counterparts. This aggregation is based on a winner-take-all ranking procedure across all the tasks, where the performance of each estimator for each task is averaged over $6$ seeds.
%

Our experiments indicate that in-context point estimation procedures outperform Bayesian methods, with a roughly even split between the amortized MLE and MAP estimator. This points to the inability of amortized Bayesian estimators in modeling the posterior well, as it was always maintained that the underlying modeling assumption (\ie the parametric form of the likelihood) as well as the prior considered matched the true data-generating distribution $\chi$ in these tasks. Within Bayesian methods, Gaussian assumption outperformed more sophisticated methods like normalizing flows and diffusion models with the symmetric KL divergence training procedure (Sample + Variational) being the dominant approach to posterior estimation.
%
%


\subsubsection{Fixed-Dimensional}

In-context learning relies on a common model to solve novel tasks and is thus limited in generalization to cases where the input and output spaces are shared across tasks, for, \eg, scalar inputs and outputs for $1$-dimensional regression or a fixed vocabulary in LLMs. Similarly, parametric inference predicts, or describes a distribution over, $\theta$ and consequently requires its size to be shared across different problems. This is inherently defined by the probabilistic model, \ie the likelihood and prior implicitly define the space of parameters. Thus, naive training of in-context estimators requires a different model to be trained for a $1$-dimensional problem than that for a $2$-dimensional one. 
%
%
%

\looseness=-1
We evaluate the in-context estimators on the suite of probabilistic models in \cref{tab:fixed_dim_ens}, using the ensemble-based predictive performance as the metrics. 
Our experiments on high-dimensional versions of the respective probabilistic models demonstrate that point estimates are competitive and often better than Bayesian counterparts, even without the benefit of ensembling. In these experiments, for nonlinear models we consider a single layered neural network with \textsc{ReLU} activation function. To test the estimators on even higher dimensional problems, we next consider a $2$-layered neural network with \textsc{TanH} or \textsc{ReLU} activation in \cref{tab:fixed_dim_2layer_ens} which highlights clear superiority of amortized point estimators over posterior counterparts. See \cref{apdx:fixed-dim} for details.

\input{Tables/tabular_ensembled}

\subsubsection{Variable-Dimensional}

\label{sec:variable-dim}
Next, we alleviate the limitation of fixed-dimensional parametric inference by embedding lower-dimensional problems into a fixed higher dimension. For example, a $1$-dimensional linear regression model can be embedded in $100$-dimensional space with the additional parameters set to $0$. This simple masking procedure allows the in-context estimators to generalize to tasks with variable number of features, leading to the same estimator solving problems with different number of features. We evaluate the estimators on the variable-dimensional setup in \cref{tab:variable_dim_ens} and again see that amortized point estimation methods remain competitive and often better than Bayesian counterparts. Refer to \ref{apdx:variable-dim} for additional experiments and details.
%
%
%


\subsection{Misspecification}

Having studied in-distribution generalization, we now turn to cases of OoD generalization where the evaluation datasets are sampled from a different, sometimes unknown, distribution $\gD_{\rm test} \sim \chi_{\rm test}$. We study two cases: controlled synthetic and real-world tabular problems. This analysis is aimed to test the estimators' ability to handle changes in the underlying ground-truth mapping $p(y | \vx)$ as well as when $\vx$ follows a different distribution at evaluation, which is important since we often do not know the underlying model that generates the data of interest.


\subsubsection{Synthetic}

\looseness=-1
We consider $1$-dimensional regression problems with different underlying mappings $p(\vy | \vx)$ between training and evaluation. Here, we are interested in generalizing to data obtained from $\chi_{\rm test}$ but we assume that we do not know the underlying parametric form for this data. Instead, we assume a parametric form $p(\vy | \vx, \theta)$ which leads to $\chi_{\rm train}$ as the data-generating distribution, with $\chi_{\rm train} \neq \chi_{\rm test}$. 

Given this misspecification, sample-based in-context estimators can only be trained on $\gD_{\rm train} \sim \chi_{\rm train}$, however point estimation procedures and variational methods don't have this limitation, and can be trained with $\gD_{\rm train} \sim \chi_{\rm test}$ if sampling from $\chi_{\rm test}$ is relatively easy.

\cref{tab:misspecification_ens} highlights the performance of different estimators when evaluated on $\chi_{\rm test}$ and trained on $\chi_{\rm train}$, except for ``+ switched data" where even during training datasets are sampled from $\chi_{\rm test}$. We see that point estimators and variational methods can lead to better predictions by being trained directly on $\chi_{\rm test}$, with point estimators outperforming others in general. We refer to \cref{apdx:misspecification} for details.

%
%
%
%
%
%
%
%
%
%


\subsubsection{Tabular}

Finally, we turn our attention to a suite of regression and classification tasks from the OpenML platform, filtered from \textit{OpenML-CTR23 - A curated tabular regression benchmarking suite}~\citep{fischer2023openmlctr23} and \textit{OpenML-CC18 Curated Classification benchmark
}~\citep{bischl2019openmlcc18}. We exclude tasks with missing values, or with more than $100$ features. This presents a case of extreme OoD generalization as we use the in-context estimators trained in \cref{sec:variable-dim} and test their generalization zero-shot through inference of (non-)linear regression and classification assumptions on a suite of $9$ and $13$ tabular problems, respectively.

We see in \cref{tab:tabular_ens} that point estimation procedures and variational methods perform better than those trained based on samples, where we consider an average over $6$ seeds and use a $5$-fold cross validation to obtain train / test splits for each dataset. We refer to \ref{apdx:tabular} for details. 

\section{Conclusion}

Our simulations in the amortized setting suggest that point estimation methods tend to outperform distribution estimators for posterior predictive modeling, especially on problems where the posterior over parameters is high-dimensional and multimodal. While one potential reason for this is the suboptimality or sample-inefficiency of the training objectives and model architectures, which research on amortized inference should continue to improve, our findings may be indicative of a more fundamental obstacle in Bayesian modeling. Many multimodal problems exhibit a large number of distinct modes that lead to equivalent solutions, but still require increasing expressivity in the approximate posterior to represent each of these, \emph{potentially redundant}, modes. The latter challenge is manifested in more complex problems as well, \eg, the identifiability problem in mixture models \citep{teicher1963identifiability,yakowitz1968identifiability} and symmetries in Bayesian neural networks, where the number of modes has a combinatorial explosion in the network width, but mode connectivity results show that the posterior does not have high energy barriers \citep{draxler2019essentially}, especially modulo symmetries \citep{ferbach2024proving}. 

While those results concern the non-amortized setting, we have shown that when in-context parameter estimation is considered, the same challenges arise even in simple models. This points to the need for hybrid approaches to amortized inference, drawing from non-amortized methods where only a subset of parameters undergo a Bayesian treatment \citep{daxberger2022bayesian} and amortized variational families are chosen to represent posteriors more efficiently \citep{sharma2023bayesian,doan2025bayesian}.

%

\section*{Acknowledgements}
The authors would like to acknowledge the computing resources provided by the Mila cluster to enable the experiments outlined in this work. SM acknowledges the support of UNIQUE's scholarship.
GL acknowledges support from the Canada CIFAR AI Chair program, and the Canada Research Chair in Neural Computations and Interfacing. YB acknowledges the support from CIFAR and the CIFAR AI Chair program as well as NSERC funding for the Herzberg Canada Gold medal.
%
%
The authors also thank NVIDIA for computing resources.

\section*{Impact Statement}
We evaluate different in-context estimators for the task of inferring parameters for better downstream predictions. The goal of this work is to provide a rigorous and comparative study for the advancement of the general field of machine learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
\clearpage
\bibliography{bibliography}
\bibliographystyle{bibstyle}

\appendix
\onecolumn
\section{Related Work}
%
%
%
%
%
%
%

%

%

%
%
%
%
%

\paragraph{Normalizing Flows}. Since Gaussian distributions are unimodal, they cannot approximate more complex multi-modal distributions well. To alleviate this problem, multiple works start with a simple distribution and then apply learnable invertible transformations to construct more complicated densities~\citep{rezende2015variational,kobyzev2020normalizing,papamakarios2021normalizing,kingma2016improved}. These transformations are designed in a manner that the jacobian determinant is easily computable to allow for ease in computing entropy and training via back-propagation.
\begin{align}
    q^j_\varphi(\cdot) &= g_j \circ \ldots \circ g_1 \circ \gN(\cdot; \bf{0}, \bf{I}) \\
    q_\varphi^j(\theta^j) &= q^{j-1}_\varphi(g_j^{-1}(\theta^j)) |J_{g_j}(g_j^{-1}(\theta^j))|^{-1}
\end{align}
for $j = 1, \ldots, n$ and $q_\varphi^0$ represents the standard normal distribution. Further, $J_{g_j}$ represents the jacobian of the invertible function $g_j$ and $|\cdot|$ represents the determinant operator.

\paragraph{Score-Based Generative Modeling}. Recent advances in generative models have stemmed from diffusion models~\citep{song2020score,song2020improved,song2020denoising,ho2020denoising,nichol2021improved} that consider a forward noising process via a stochastic differential equation as 
\begin{align}
    d\theta_t = f(\theta_t, t) \;dt + g(t)\;d\text{w}_t
\end{align}
with the corresponding reverse process as~\citep{anderson1982reverse}
\begin{align}
    d\theta_t = f(\theta_t, t) - g(t)^2 \nabla_\theta \log p_t(\theta)|_{\theta_t} + g(t) d\bar{\text{w}}_t
\end{align}
Note that this requires estimating the score function at all time-steps $t$ to integrate the SDE and obtain samples. Prior work has shown that the denoising objective provides a viable method for obtaining an estimate of the score function provided access to data, which is trained as
\begin{align}
    \arg\min_\varphi \mathbb{E}_{t, \theta_0, \theta_t} \left[
    \norm{s_\varphi(\theta_t, t) - \nabla_{\theta_t} \log p(\theta_t | \theta_0)}^2\right]
\end{align}
Note that if $f$ is a linear function, one can sample $\theta_t$ given $\theta_0$ and $t$ directly in a simulation-free manner~\citep{sarkka2019applied}, which allows for scalable training of diffusion models through the above equation.

\textbf{Flow-Matching}. Contrary to diffusion models, flow-matching~\citep{lipman2022flow,tong2023improving} models data through an ordinary differential equation instead of a stochastic differential equation. It first constructs an interpolation~\citep{albergo2023stochastic,albergo2023dependent}, possibly noisy, between two random variables $\theta_0$ and $\theta_1$ as
\begin{align}
    \theta_t = \alpha_t \theta_0 + \beta_t \theta_1 + \gamma_t \vz
\end{align}
where $\alpha_0 = \beta_1 = 1$, $\alpha_1 = \beta_0 = 0$ and $\gamma_0 = \gamma_1 = 0$, and $\vz$ follows a normal distribution. Samples from the target density can then be obtained by sampling a $\theta_1$ and then solving the following ODE dynamics
\begin{align}
     d\theta_t = v_\varphi(\theta_t, t)\,dt
\end{align}
where the maginal drift is trained as
\begin{align}
    \arg\min_\varphi \mathbb{E}_{\theta_0, \theta_1, t, \vz, \theta_t}\left[\norm{v_\varphi(\theta_t, t) - \partial_t \theta_t}^2\right]
\end{align}

\textbf{Denoising Energy Matching}. It is important to note that diffusion models are trained via data, while multiple applications require training a model to sample proportional to an unnormalized distribution in the absence of any data. Denoising Energy Matching (DEM)~\citep{akhound2024iterated} provides an importance sampling based estimate to train a similar diffusion model in the absence of data, by considering the target score matching estimator~\citep{de2024target} and combining it with importance sampling with the transition kernel $p(\theta_t | \theta_0)$ as the proposal for $\theta_0$.

\section{Probabilistic Models}
\label{apdx:probabilistic_models}
We discuss various probabilistic models used in our experiments as well as the form for the likelihood and the prior. This closely follows the setup in \citet{mittal2023exploring}.

\textbf{Mean of Gaussian (GM):} We consider estimating the mean $\mmu$ of a Gaussian distribution given some observed data. In this case, prior and likelihood defining the probabilistic model $p(\vx, \mtheta)$ (with $\mtheta$ being the mean $\mmu$) are given by:
\begin{align}
    p(\mmu) &= \gN\left(\mmu | \mathbf{0}, \mathbf{I}\right)\\
    p(\vx | \mmu) &= \gN\left(\vx | \mmu, \mSigma\right) 
\end{align}
and $\mSigma$ is known beforehand and defined as a unit variance matrix. 

\textbf{Linear Regression (LR):} We estimate the weight vector for Bayesian linear regression, where the underlying model $p(\gD, \mtheta)$ is given by:
\begin{align}
    p(\vw) &= \gN(\vw | \mathbf{0}, \mathbf{I})\\
    p(b) &= \gN(b | 0, 1)\\
    p(y | \vx, \vw, b) &= \gN\left(y | \vw^T\vx + b, \sigma^2\right) \, ,
\end{align}
and with $\sigma^2 = 0.25$ known beforehand. Inputs $\vx$ are generated from $p(\vx) = \gN(\mathbf{0}, I)$.


\textbf{Linear Classification (LC):}
The underlying probabilistic model is:
\begin{align}
    p(\mW) &= \gN\left(\mW | \mathbf{0}, \mathbf{I}\right)\\
    p(y | \vx, \mW) &= \mathrm{Categorical}\left(y  \;\vline\; \frac{1}{\tau}\;\mW\vx\right)\, ,
\end{align}
where $\tau$ is the known temperature term which is kept as $0.1$ to ensure peaky distributions, and $\vx$ is being generated from $p(\vx) = \gN(\mathbf{0}, I)$.


\textbf{Nonlinear Regression (NLR):}
We consider the model as a Bayesian Neural Network (BNN) for regression with fixed hyper-parameters like the number of layers, dimensionality of the hidden layer, etc. Let the BNN denote the function $f_\mtheta$ where $\mtheta$ are the network parameters. Then, for regression, we specify the probabilistic model using:
\begin{align}
    p(\mtheta) &= \gN\left(\mtheta | \mathbf{0}, \mathbf{I}\right)\\
    p(y | \vx, \mtheta) &= \gN\left(y | f_\mtheta(\vx), \sigma^2\right) \, ,
\end{align}
where $\sigma^2 = 0.25$ is a known quantity and $\vx$ being generated from $p(\vx) = \gN(\mathbf{0}, I)$.
 
\textbf{Nonlinear Classification (NLC):}
Like in Nonlinear Regression, we consider BNNs with fixed hyper-parameters for classification problems with the same estimation task. In this formulation, we consider the probabilistic model as:
\begin{align}
    p(\mtheta) &= \gN\left(\mtheta | \mathbf{0}, \mathbf{I}\right)\\
    p(y | \vx, \mtheta) &= \mathrm{Categorical}\left(y \;\vline\; \frac{1}{\tau}\;f_\mtheta(\vx)\right)
\end{align}
where $\tau$ is the known temperature term which is kept as $0.1$ to ensure peaky distributions, and $\vx$ is being generated from $p(\vx) = \gN(\mathbf{0}, I)$.

\textbf{Gaussian Mixture Model (GMM):}
We look at a well-known probabilistic model for unsupervised learning, Gaussian Mixture Model (GMM), primarily used to cluster data. Consider a $K$-cluster GMM with:
\begin{align}
    p(\mmu_k) &= \gN\left(\mmu_k | \mathbf{0}, \mathbf{I}\right)\\
    p(\vx | \mmu_{1:K}) &= \sum_{k=1}^K \pi_k \gN\left(\vx | \mmu_k, \mSigma_k\right) \, .
\end{align}
 We assume $\mSigma_k$ and $\pi_k$ to be known and set $\mSigma_k$ to be an identity matrix and the mixing coefficients to be equal, $\pi_k = 1/K$, for all clusters $k$ in our experiments. 

\section{Metrics}
\label{apdx:metrics}
\textbf{Regression}. For regression problems, we consider the ensembled loss metric as the following
\begin{align}
    \mathbb{E}_{\vx, \vy, \gD} \left[\norm{\vy - \mathbb{E}_{q_\varphi(\theta | \gD)}\left[\hat{\vy} \Big\rvert \vx, \theta\right]}^2\right]
\end{align}
while the single-sample metric is defined as
\begin{align}
    \mathbb{E}_{\vx, \vy, \gD} \mathbb{E}_{q_\varphi(\theta | \gD)} \left[\norm{\vy - \hat{\vy}}^2 \Big\rvert \vx, \theta\right]
\end{align}
where $\hat{\vy}$ denotes the mode of the distribution $p(\vy | \vx, \theta)$. We rely on similar metrics for the estimation of the mean of a Gaussian distribution, with the only difference being the absence of $\vx$, and $\hat{y} = \theta$, as it is an unsupervised learning problem.

\textbf{Classification}. For classification problems, we consider the ensembled accuracy metric which is obtained as the following
\begin{align}
    100 \times \mathbb{E}_{\vx, \vy, \gD} \left[\mathbbm{1}_\vy\left(\text{Mode}\left(\hat{\vy}_1, \ldots, \hat{\vy}_s\right)\right)\right]
\end{align}
where $\mathbbm{1}_\vy(\cdot)$ is an indicator function which is $1$ if the argument is the same as $\vy$ and $0$ otherwise. Mode represents the mode of its arguments, where each $\hat{\vy}_i$ is the mode of  $p(\vy | \vx, \theta_i)$ with $\theta_i \sim q_\varphi(\theta | \gD)$. Similarly, the single sample metric is defined as
\begin{align}
    100 \times \mathbb{E}_{\vx, \vy, \gD} \mathbb{E}_{q_\varphi(\theta | \gD)} \left[\mathbbm{1}_\vy\left(\hat{\vy}\right)\Big\rvert \vx, \theta\right]
\end{align}
Note that the multiplication by $100$ is just to scale the accuracy to $0-100$.

\textbf{Gaussian Mixture Model}. For the Gaussian Mixture Model, there is no clear notion of ensembling due to the identifiability problem in clustering, \ie, averaging over two clusters could lead to the average not corresponding to any meaningful cluster. Thus, we only consider single sample metric for this case, in particular
\begin{align}
    \mathbb{E}_{\vy, \gD} \mathbb{E}_{q_\varphi(\theta_1, \ldots \theta_c | \gD)} \left[\left(\vy - \argmin_{\psi \in \theta_1, \ldots \theta_c} \,\left(\vy - \psi\right)^2\right)\right]
\end{align}
where $\theta_1, \ldots \theta_c$ can be subsumed into a single larger vector $\theta$ for the purposes of modeling a $q_\varphi$.

Note that in case of point estimates, for all the metrics, $q_\varphi$ can be considered as a dirac measure and both ensembled and single-sample metrics represent the same quantity.

\section{Implementation Details}
\label{apdx:implementation}
In this section, we outline the implementation details behind each of the estimators. We consider the transformer architecture in all cases to model the conditioning on the set of observations $\gD$. We remove the positional embeddings so that the inferred parameters, distribution or point, are permutation invariant to $\gD$. We use [CLS] as an additional token embedded to the sequence and the prediction corresponding to it is used to infer the parameters.

For the architecture details, we use $4$ encoder layers with a $256$ dimensional attention block and $1024$ feed-forward dimensions, and $4$ heads in each attention block for our Transformer models.

We use a diagonal Gaussian assumption for modeling densities using the Gaussian distribution.
For discrete normalizing flows, we follow the setup in \cite{radev2020bayesflow} and use $6$ coupling blocks, each with a $1$ hidden-layered non-linear feed-forward subnetwork with ReLU non-linearity and $128$ hidden dimensions.

For score-based diffusion models, we use the variance exploding SDE with no drift and the diffusion coefficient $g_t$ set to be $\sqrt{2t\beta^2}$ and train the estimator using denoising score matching with the loss being equally weighted for all times $t$. We use the same schedule for pDEM as well, and use $100$ samples in the importance-sample estimate of the score.

Finally, we use linear interpolation scheme for flow-matching that interpolates between the parameters $\theta$ and unstructured noise $z$ in a linear manner.

For inference in continuous time models, we use $100$ steps to perform both the SDE and ODE integration.

For training the in-context estimators, we sample the number of observations $|\gD|$ randomly in the interval $[64, 128]$. For the variable-dimensional experiments, we randomly sample the problem dimensionality in the range $[1, 100]$. To evaluate the models, we sample $100$ different datasets maintaining the test set to be the same across different seeds.

All the in-context estimators are trained until convergence, in particular,

\textbf{GM}: $50k$ for fixed-dimensional and $100k$ for variable-dimensional.

\textbf{GMM}: $250k$ for fixed-dimensional and $500k$ for variable-dimensional.

\textbf{LR}: $150k$ for fixed-dimensional and $250k$ for variable-dimensional.

\textbf{NLR}: $250k$ for fixed-dimensional and $500k$ for variable-dimensional.

\textbf{LC}: $150k$ for fixed-dimensional and $250k$ for variable-dimensional.

\textbf{NLC}: $250k$ for fixed-dimensional and $500k$ for variable-dimensional.

In addition, all experiments on synthetic misspecification are trained for $250k$ iterations.

\input{Tables/fixed_dim}
\input{Tables/variable_dim}
\input{Tables/misspec}
\input{Tables/tabular}
\input{Tables/variable_dim_2_ensembled}
\section{Additional Experiments}
\label{apdx:additional_experiments}

\subsection{In-distribution evaluation of in-context estimators}
\label{apdx:in-distribution}

\subsubsection{Fixed-Dimensional}
\label{apdx:fixed-dim}
In this section, we first outline the different setups for each probabilistic model that serve as part of our empirical analysis. We note that for each probabilistic model as well as each configuration of the said model, a different in-context estimator needs to be learned.

\textbf{GM}: For estimating the mean of a Gaussian distribution, we train different in-context estimators for a $2$-dimensional and $100$-dimensional problem, yielding $2$ tasks.

\textbf{GMM}: For modeling the means of a mixture of Gaussian distribution, different in-context estimators are trained for $2$ and $5$ dimensional observations, with $2$ and $5$ underlying number of clusters. This results in $4$ tasks.

\textbf{LR}: To estimate the weights of a linear regression model, we consider $1$ and $100$ dimensional observations with a scalar target in each setting, resulting in $2$ tasks. 
 
\textbf{NLR}: This experiment requires estimating the parameters of a neural network, where the observations can be $1$ or $25$ dimensional, the number of hidden layers $1$ or $2$, and the activation function can be \textsc{TanH} or \textsc{ReLU}. In total this leads to $8$ different tasks.

\textbf{LC}: To estimate the weights of a linear classifier, we consider $2$ and $100$ dimensional observations with $2$ and $5$ classes, resulting in $4$ tasks.

\textbf{NLC}: Similar to NLR, we consider $2$ or $25$ dimensional observations, $1$ or $2$ number of hidden layers, \textsc{TanH} or \textsc{ReLU} activation function and $2$ or $5$ number of classes, leading to $16$ different tasks.

In total, this leads to a total of $36$ tasks, where every in-context learner is separately trained for each of the tasks. Additionally, for each task and amortized estimator, we conduct our investigation using $6$ seeds leading to $36 \times 6$ models being trained for each estimator.

\cref{tab:fixed_dim} highlights the single-sample performance metrics on high-dimensional tasks for each of the class of probabilistic models. The results corresponding to other configurations of the probabilistic models are abstracted away in \cref{fig:rank}.

\subsubsection{Variable-Dimensional}
\label{apdx:variable-dim}
Next, we look at inferring parameters for variable number of features, and outline the different setups for each probabilistic model. Note that for each assumed model, a single estimator is trained to solve various input number of features.

\textbf{GM}: We evaluate on estimating the mean of a Gaussian distribution for $2$, $50$ and $100$ dimensional problem, leading to $3$ tasks with only $1$ model trained for all of them.

\textbf{GMM}: We consider $2$ and $5$ dimensional observations, with $2$ and $5$ underlying number of clusters. This results in $4$ tasks, with $2$ different models being trained corresponding to the different number of clusters.

\textbf{LR}: The observations are $1$, $50$ and $100$ dimensional with a scalar target, resulting in $3$ tasks and only $1$ trained model. 
 
\textbf{NLR}: Here, the observations can be $1$, $50$ or $100$ dimensional, the number of hidden layers $1$ or $2$, and the activation function can be \textsc{TanH} or \textsc{ReLU}. In total this leads to $12$ different tasks, with $4$ different models trained.

\textbf{LC}: We consider $2$, $50$ and $100$ dimensional observations with $2$ and $5$ classes, resulting in $6$ tasks and $2$ models.

\textbf{NLC}: We evaluate $2$, $50$ and $100$ dimensional observations, $1$ or $2$ number of hidden layers, \textsc{TanH} or \textsc{ReLU} activation function and $2$ or $5$ number of classes, leading to $24$ different tasks and $8$ different trained models.

In total, this leads to a total of $52$ tasks, where the in-context estimators generalize across input dimensions. Given $6$ seeds for each analysis, this leads to $18 \times 6$ models being trained for each estimator.

\cref{tab:variable_dim} highlights the single-sample performance metrics on high-dimensional tasks for each probabilistic model. Additionally \cref{tab:variable_dim_2l_ens} highlights the ensemble-based predictive metrics on considerably harder problems, \ie estimating the parameters of a 2-layered neural network. The results corresponding to other configurations of the probabilistic models are abstracted away in \cref{fig:rank}.

\subsection{Misspecification}

\subsubsection{Synthetic}
\label{apdx:misspecification}
We consider three different data-generating families: linear regression (LR), nonlinear regression modeled through a single layered neural network with \textsc{TanH} activation function (NLR), and Gaussian Process (GP) with the radial basis function kernel. Given a pair of data-generating processes $(\chi_{\rm train}, \chi_{\rm test})$, we train in-context estimators on $\chi_{\rm train}$ and then evaluate them on $\chi_{\rm test}$. The underlying modeling assumption is in line with $\chi_{\rm train}$. For example, if $\chi_{\rm train}$ is LR and $\chi_{\rm test}$ is GP, then the likelihood function is set as the LR probabilistic model described in \cref{apdx:probabilistic_models}. This allows us to train sample-based methods as well, and then evaluate them in OoD scenarios.

Since variational methods and point estimators can be trained on data different from the modeling assumption $p$, we also consider a setting where they are trained directly on $\gD \sim \chi_{\rm test}$, assuming that it does not expose $\theta$ and thus one cannot simply change the modeling assumption. This is referred to as ``+ switched data" in the results. We refer the reader to \cref{tab:misspecification} for additional results corresponding to the single-sample metrics.

\subsubsection{Tabular}
\label{apdx:tabular}
For tabular regression tasks, we use the \textit{OpenML-CTR23} benchmarking suite~\citep{fischer2023openmlctr23}, applying a filtering process to exclude datasets with over 2000 examples, more than 100 features, or missing values (NaNs). Similarly, for classification, we utilize the \textit{OpenML-CC18} benchmark~\citep{bischl2019openmlcc18}, applying the same filtering criteria while additionally removing datasets that are not binary classification problems. This process results in a final selection of 9 regression and 13 classification datasets. The selected datasets are:

Regression: \textsc{airfoil\_self\_noise}, \textsc{concrete\_compressive\_strength}, \textsc{energy\_efficiency}, \textsc{solar\_flare}, \textsc{student\_performance\_por}, \textsc{QSAR\_fish\_toxicity}, \textsc{red\_wine}, \textsc{socmob}, and \textsc{cars}.

Classification: \textsc{credit-g}, \textsc{diabetes}, \textsc{tic-tac-toe}, \textsc{pc4}, \textsc{pc3}, \textsc{kc2}, \textsc{pc1}, \textsc{banknote-authentication}, \textsc{blood-transfusion-service-center}, \textsc{ilpd}, \textsc{qsar-biodeg}, \textsc{wdbc}, and \textsc{climate-model-simulation-crashes}.

For each of the datasets, we evaluate using a $5$-fold cross validation and use $6$ seeds. The inputs to the in-context estimators are normalized to have zero-mean and unit-variance. Evaluation of different estimators through the single-sample metrics for tabular tasks is provided in \cref{tab:tabular}.
\end{document}