\begin{table*}[t]
    \centering
    \small
    % \scriptsize
    % \def\arraystretch{1.5}
    \setlength{\tabcolsep}{10pt}
    % \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}l r c c c c c}
        \toprule
          & & \multicolumn{3}{c}{\textit{$L_2$ Loss} ($\downarrow$)} & \multicolumn{2}{c}{\textit{Accuracy} ($\uparrow$)}\\
         \cmidrule(lr){3-5}\cmidrule(lr){6-7}
        & \textbf{Objective} & \multicolumn{1}{c}{\textbf{GM}} & \multicolumn{1}{c}{\textbf{LR}} & \multicolumn{1}{c}{\textbf{NLR}} & \multicolumn{1}{c}{\textbf{LC}} & \multicolumn{1}{c}{\textbf{NLC}} \\
        \cmidrule(lr){3-7}
        & & $100$D & $100$D & $25$D & $100$D $2$cl & $25$D $2$cl \\
\midrule
\multirow{3}{*}{Baseline} & Random & $204.91$\sstd{$0.21$} & $105.4$\sstd{$0.3$} & $428.9$\sstd{$5.4$} & $50.4$\sstd{$0.4$} & $49.8$\sstd{$1.2$} \\
& True Posterior & $101.31$\sstd{$0.08$} & $14.5$\sstd{$0.3$} & - & - & - \\
& Optimization & $101.24$\sstd{$0.00$} & $25.1$\sstd{$0.0$} & $96.8$\sstd{$0.1$} & $70.3$\sstd{$0.0$} & $78.4$\sstd{$0.1$} \\
\cmidrule(lr){2-7}
\multirow{2}{*}{Single-Chain}
& Langevin & $102.33$\sstd{$0.03$} & $23.3$\sstd{$0.7$} & $132.3$\sstd{$1.0$} & $65.1$\sstd{$0.3$} & $73.2$\sstd{$0.3$} \\
& HMC & $102.41$\sstd{$0.03$} & $18.7$\sstd{$0.2$} & $98.1$\sstd{$0.7$} & $62.2$\sstd{$0.3$} & $70.4$\sstd{$0.1$} \\
\cmidrule(lr){2-7}
\multirow{2}{*}{Multiple-Chain} & Langevin & $101.28$\sstd{$0.00$} & $14.5$\sstd{$0.2$} & $80.2$\sstd{$0.4$} & $72.6$\sstd{$0.1$} & $79.4$\sstd{$0.2$} \\
& HMC & $101.38$\sstd{$0.00$} & $17.0$\sstd{$0.0$} & $86.4$\sstd{$0.2$} & $71.5$\sstd{$0.4$} & $76.6$\sstd{$0.1$} \\
\midrule

\multirow{3}{*}{Gaussian} & Fwd-KL &$101.38$\sstd{$0.00$} & $25.5$\sstd{$0.6$} & $276.2$\sstd{$2.2$} & $71.6$\sstd{$0.1$} & $64.9$\sstd{$0.5$} \\

& Rev-KL &$101.38$\sstd{$0.01$} & $28.5$\sstd{$0.3$} & $101.8$\sstd{$1.8$} & $72.5$\sstd{$0.1$} & \highlight{$78.5$\sstd{$0.1$}} \\

& Sym-KL &$101.37$\sstd{$0.02$} & $28.9$\sstd{$0.3$} & \highlight{$95.7$\sstd{$0.9$}} & $72.3$\sstd{$0.3$} & $78.1$\sstd{$0.2$} \\
\cmidrule(lr){2-7}

\multirow{3}{*}{Norm. Flows} & Fwd-KL &$101.39$\sstd{$0.01$} & $24.4$\sstd{$1.2$} & $268.1$\sstd{$1.9$} & $72.1$\sstd{$0.3$} & $65.4$\sstd{$0.4$} \\

& Rev-KL &$101.38$\sstd{$0.01$} & $29.1$\sstd{$1.6$} & $102.1$\sstd{$0.9$} & $72.9$\sstd{$0.1$} & \highlight{$78.6$\sstd{$0.2$}} \\

& Sym-KL &$101.37$\sstd{$0.01$} & $30.0$\sstd{$0.5$} & $103.9$\sstd{$0.4$} & $72.9$\sstd{$0.4$} & \highlight{$78.5$\sstd{$0.6$}} \\
\cmidrule(lr){2-7}

\multirow{3}{*}{Diffusion} & Score-Based &$101.42$\sstd{$0.01$} & \highlight{$22.9$\sstd{$0.3$}} & $300.4$\sstd{$2.5$} & $71.9$\sstd{$0.2$} & $64.3$\sstd{$0.1$} \\

& Flow-Matching &$101.40$\sstd{$0.02$} & $23.7$\sstd{$0.2$} & $281.6$\sstd{$1.7$} & $72.2$\sstd{$0.2$} & $65.6$\sstd{$0.4$} \\

& pDEM &$114.36$\sstd{$1.09$} & $32.7$\sstd{$0.9$} & $257.8$\sstd{$4.1$} & $72.5$\sstd{$0.2$} & $73.5$\sstd{$0.7$} \\
\cmidrule(lr){2-7}

\multirow{2}{*}{Point} & MLE &$101.30$\sstd{$0.00$} & $28.1$\sstd{$0.7$} & $99.0$\sstd{$2.9$} & $73.0$\sstd{$0.2$} & $76.5$\sstd{$0.4$} \\

& MAP & \highlight{$101.28$\sstd{$0.00$}} & $28.1$\sstd{$0.6$} & \highlight{$96.9$\sstd{$1.5$}} & \highlight{$73.4$\sstd{$0.1$}} & $78.3$\sstd{$0.2$} \\
\bottomrule
    \end{tabular}
    % }
    \vspace{-1mm}
    \caption{We compare various in-context parameter inference methods using ensemble-based predictive metrics for \textit{fixed-dimensional} estimation problems. In these experiments, each in-context learner is trained for a specific problem dimensionality. The tasks are high-dimensional, ranging from $100$ to $800$ dimensional parameters. $r$D implies $\vx \in \mathbf{R}^r$ and $s$cl implies $s$-class classification problem.}
    \vspace{-5mm}
    \label{tab:fixed_dim_ens}
\end{table*}