% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}


% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}
\usepackage{float}
\usepackage{stfloats}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% Redefine \maketitle to remove author space if it's empty

% \makeatletter
% \renewcommand{\@maketitle}{\null\vskip 2em
% \begin{center}
%   \ifx\@author\@empty \else
%     {\author@font \@author \par}
%     \vskip 1.5em
%   \fi
%   \titlefont\huge \@title\par
%   \vskip .5em
  
% \end{center}
% \vskip 2em}
% \makeatother

% \makeatletter
% \renewcommand{\@maketitle}{\null\vskip 2em
% \begin{center}
%   \ifx\@author\@empty \else
%     {\@author \par} % Default author styling
%     \vskip 1.5em
%   \fi
%   {\Large\bf \@title \par} % Default title styling
% \end{center}
% \vskip 2em}
% \makeatother

\title{Non-Euclidean Hierarchical Representational Learning using Hyperbolic Graph Neural Networks for Environmental Claim Detection}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Darpan Aswal \\
 Indian Institute of Technology, Kharagpur \\
 \texttt{darpanaswal@kgpian.iitkgp.ac.in} \\\And
 Manjira Sinha \\
 TCS Research \\
 \texttt{manjiras@gmail.com} \\}



\begin{document}
\maketitle
\begin{abstract}
Transformer-based models dominate NLP tasks like sentiment analysis, machine translation, and claim verification. However, their massive computational demands and lack of interpretability pose challenges for real-world applications requiring efficiency and transparency. In this work, we explore Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as lightweight yet effective alternatives for Environmental Claim Detection, reframing it as a graph classification problem. We construct dependency parsing graphs to explicitly model syntactic structures, using simple word embeddings (word2vec) for node features with dependency relations encoded as edge features. Our results demonstrate that these graph-based models achieve comparable or superior performance to state-of-the-art transformers while using 30x fewer parameters. This efficiency highlights the potential of structured, interpretable, and computationally efficient graph-based approaches.
\end{abstract}

\section{Introduction}
\textbf{Large Language Models (LLMs)} are getting increasingly accepted as the standard in many industry applications~\cite{chkirbene2024large}, achieving state-of-the-art performance in tasks such as sentiment analysis, machine translation, and claim verification~\cite{miah2024multimodal, zhang2023prompting, xu2024complex}. However, their dominance has also raised concernsâ€”LLMs require large-scale computational resources, leaving behind a large carbon footprint~\cite{faiz2023llmcarbon} which makes them overkill for many real-world applications. Their black-box nature is yet another issue which limits interpretability~\cite{lin2023generating}. Specifically, in claim verification, where corporate statements must be rigorously analyzed, over-reliance on black-box models can result in misleading or unverifiable conclusions. The increasing scrutiny on sustainability claims further necessitates interpretability and computationally efficiency in models. \\ 
\textbf{Graph Neural Networks (GNNs)} and their hyperbolic counterpart (HGNNs)~\cite{wu2020comprehensive, zhou2023hyperbolic} present a compelling alternative. These architectures offer a structured and interpretable approach to syntactic and semantic learning while significantly reducing computational overhead. Unlike the implicit encoding of relationships in LLMs, GNNs and HGNNs explicitly model hierarchical and relational dependencies through graph structures such as constituency parsing or dependency parsing graphs~\cite{li2020empirical, nivre2010dependency}. Furthermore, these models can integrate rich semantic information from word embeddings, knowledge graphs, and named entity recognition~\cite{mikolov2013efficient, opdahl2022semantic, li2020survey}, with structural representation. \\
\textbf{Environmental claims}~\cite{stammbach2022environmental}, a complex detection task in NLP, often exhibit hierarchical and nested information such as conditional statements, vague terminology as well as Greenwashing elements~\cite{de2020concepts}. Prior work has predominantly relied on transformer-based architectures, but their interpretability limitations hinder the auditability of detected claims. This study reframes the problem as a graph classification task, leveraging the structured nature of environmental claims to analyze the effectiveness of GNNs and HGNNs in capturing syntactic and hierarchical dependencies. The research questions for the study are as follows. \\
\textbf{RQ1. }Can interpretable graph-based models (GNNs and HGNNs) match SOTA performance for environmental claim detection while using just a fraction of the compute as that of LLMs? \\
\textbf{RQ2. }How do euclidean (GNN) and hyperbolic (HGNN) representations compare in capturing hierarchical and relational structures, and how well can explicit syntactic learning with dependency parsing graphs complement them for enhanced claim verification?

% The detection of environmental claims within corporate communications is becoming increasingly vital as businesses face growing scrutiny regarding their sustainability practices. Traditionally, NLP tasks like Environmental Claim Detection rely heavily on transformer-based models, which implicitly capture syntactic and semantic structures but often lack interpretability in understanding learned hierarchical patterns. This study addresses these limitations by reposing the environmental claim detection task as a graph classification problem, leveraging Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs). Our approach focuses on the explicit modeling of semantic and syntactic information using dependency parsing graphs, where each sentence is transformed into a structured representation of its underlying dependencies. By comparing GNNs, which operate in conventional Euclidean space, with HGNNs that utilize hyperbolic space to capture hierarchical relationships, we highlight the advantages of graph-based models in handling complex linguistic data. The results demonstrate that GNNs and HGNNs can achieve state-of-the-art performance in environmental claim detection, particularly excelling in capturing the hierarchical and relational structures that transformer models often overlook. This study underscores the potential of non-Euclidean geometries in enhancing NLP tasks that require a deeper understanding of structural complexities, setting a new direction for future research in environmental claim detection and beyond.


% \section{Engines}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.


% \begin{table}
% \centering
% \begin{tabular}{lc}
% \hline
% \textbf{Command} & \textbf{Output}\\
% \hline
% \verb|{\"a}| & {\"a} \\
% \verb|{\^e}| & {\^e} \\
% \verb|{\`i}| & {\`i} \\ 
% \verb|{\.I}| & {\.I} \\ 
% \verb|{\o}| & {\o} \\
% \verb|{\'u}| & {\'u}  \\ 
% \verb|{\aa}| & {\aa}  \\\hline
% \end{tabular}
% \begin{tabular}{lc}
% \hline
% \textbf{Command} & \textbf{Output}\\
% \hline
% \verb|{\c c}| & {\c c} \\ 
% \verb|{\u g}| & {\u g} \\ 
% \verb|{\l}| & {\l} \\ 
% \verb|{\~n}| & {\~n} \\ 
% \verb|{\H o}| & {\H o} \\ 
% \verb|{\v r}| & {\v r} \\ 
% \verb|{\ss}| & {\ss} \\
% \hline
% \end{tabular}
% \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
% \label{tab:accents}
% \end{table}


% \section{Preamble}

% \begin{table*}
% \centering
% \begin{tabular}{lll}
% \hline
% \textbf{Output} & \textbf{natbib command} & \textbf{Old ACL-style command}\\
% \hline
% \citep{ct1965} & \verb|\citep| & \verb|\cite| \\
% \citealp{ct1965} & \verb|\citealp| & no equivalent \\
% \citet{ct1965} & \verb|\citet| & \verb|\newcite| \\
% \citeyearpar{ct1965} & \verb|\citeyearpar| & \verb|\shortcite| \\
% \citeposs{ct1965} & \verb|\citeposs| & no equivalent \\
% \citep[FFT;][]{ct1965} &  \verb|\citep[FFT;][]| & no equivalent\\
% \hline
% \end{tabular}
% \caption{\label{citation-guide}
% Citation commands supported by the style file.
% The style is based on the natbib package and supports all natbib citation commands.
% It also supports commands defined in previous ACL style files for compatibility.
% }
% \end{table*}


% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}
% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{ACL2023}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{ACL2023}
% \end{verbatim}
% \end{quote}
% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)
% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.
% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.
% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

% \section{Document Body}

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation: 
% \begin{quote}
% \tt\verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% % \subsection{Citations}



% % Table~\ref{citation-guide} shows the syntax supported by the style files.
% % We encourage you to use the natbib styles.
% % You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% % You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% % You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% \subsection{References}

% \nocite{Ando2005,augenstein-etal-2016-stance,andrew2007scalable,rasooli-tetrault-2015,goodman-etal-2016-noise,harper-2014-learning}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliographystyle{acl_natbib}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}
% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliographystyle{acl_natbib}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}
% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

\section{Related Work}
Graph-based models have gained significant attention in various NLP tasks due to their ability to explicitly capture relational structures within data. GNNs have been effectively used in applications like node classification, link prediction, and graph classification by leveraging message-passing mechanisms to aggregate information from neighbors, thus capturing both local and global dependencies within graphs \citep{4700287}; \citep{kipf2016semi}. Recent advancements have extended these models to more complex domains, including sentiment analysis and fake news detection, where the relational context is essential.

%In contrast to GNNs which operate in Euclidean space,
HGNNs extend the principles of GNNs into hyperbolic space, capturing long-range dependencies and hierarchical relations more naturally than their Euclidean counterparts \citep{nickel2017poincare}; \citep{chami2019hyperbolic}. Hyperbolic spaces, characterized by their constant negative curvature, are particularly well-suited for capturing tree-like and hierarchical data, where relationships exhibit exponential growth in scale. This property has been shown to improve the representation of complex graph structures, such as those found in linguistic data, by preserving the hierarchical and relational intricacies often missed by Euclidean models.

The PoincarÃ© Ball Model \citep{nickel2017poincare} and the Lorentz Hyperboloid Model \citep{nickel2018learning} are among the most prominent hyperbolic models. These frameworks have demonstrated superior performance in embedding hierarchical data due to their ability to maintain structural integrity under hyperbolic constraints. 
%HGNNs extend the principles of GNNs into hyperbolic space, capturing long-range dependencies and hierarchical relations more naturally than their Euclidean counterparts. This approach has proven particularly effective for tasks requiring nuanced understanding of structural complexities, such as hierarchical classification and modeling dependency parsing graphs.



\section{Dataset}
For this study, we utilized the Environmental Claim Detection (ECD) dataset introduced by \citep{stammbach2022environmental}. This dataset comprises environmental claims extracted from various corporate communications of publicly listed companies, including sustainability reports, earnings calls, and annual reports.
The original dataset consisted of 3,000 annotated sentences. After removing tied annotations, the final dataset used in our analysis contains 2,647 examples. These examples are categorized into two classes: claim statements and not claim statements. The dataset exhibits an imbalanced distribution, with 665 sentences (25.1\%) labeled as claim statements and 1,982 sentences (74.9\%) labeled as not claim statements.

\section{Methodology and Models}
\subsection{Representation as Dependency Parsing Graphs}
We utilized spaCy's built-in DependencyParser for generating dependency parsing graphs. This tool enabled us to represent sentences as directed graphs, where nodes correspond to words, and edges represent syntactic dependencies between them. 

Mathematically, a dependency parsing graph can be represented as:
\[
G = (V, E)
\]
where:
\begin{itemize}
    \item \( V \) is the set of vertices (or nodes) such that each \( v \in V \) represents a token (word) from the sentence.
    \item \( E \) is the set of edges such that each \( e \in E \) represents a syntactic dependency between two tokens.
\end{itemize}

% Here is the code segment used for creating the dependency graph:

% \begin{lstlisting}[language=Python, caption=Dependency Graph Creation Function]

% def create_dependency_graph(sentence):
%     doc = nlp(sentence)
%     return [(token.text, token.dep_, token.head.text) 
%             for token in doc]
% \end{lstlisting}

The following are the node and edge attributes we consider:
\begin{itemize}
    \item Token text (token.text): Each node in the graph corresponds to a token (word) from the sentence, represented by its textual content.
    \item Dependency (Edge) relations (token.dep\_): Specifies the type of syntactic dependency between a token and its head. It describes how the token relates to its syntactic governor.
    \item Token head text (token.head.text): This identifies the head or governor token that governs the current token in the dependency structure.
\end{itemize}

\subsection{Vector Representations of Nodes and Edges}
We utilized word2vec \citep{mikolov2013efficient}, a pre-trained word embedding to transform words (nodes) into embedding vectors within our dependency parsing graphs. These embeddings enhanced our ability to analyze syntactic structures and semantic similarities across our experimental data.

Other embedding approaches include GloVe \citep{pennington2014glove}, fastText \citep{bojanowski2017enriching}, and BERT embeddings, each with their own set of advantages for extracting semantic information from text input. However, a comparison among different embedding models lies outside the purpose of our study.

For the dependency types represented as the edges in the graphs, we utilize a total of 45 different types in the ECD-dataset by numerically encoding the edges relations.

\subsection{Model Architectures}
In this study, we compared the original results from \citep{stammbach2022environmental} against two model architectures: Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs). For training the GNNs, we utilized the HGNN toolkit from \citep{liu2019hyperbolic}.

\subsubsection{Graph Neural Networks}
GNNs operate by iteratively updating the representation of nodes in a graph by aggregating features from neighbouring nodes, effectively capturing the local and global structure of the graph. 

We utilized word2vec embeddings to initialise the node features which represent the tokens from the Environemental Claims, with numerically encoded edges features to represent the syntactic dependencies between these tokens.

The core of GNN learning is the Message Passing algorithm, which can be broken down into the construction and aggregation of messages between nodes. % Consider the dependency graph \(G = (V, E)\), with \(V\) as its set of nodes and \(E\) as the set of edges.
The process proceeds as follows:

\begin{itemize}
    \item \textbf{Message Construction:}
   At each iteration \(t\), a message \(\mathbf{m}_{uv}^{(t+1)}\) is created from node \(u\) to node \(v\):
   \[
   \mathbf{m}_{uv}^{(t+1)} = M^{(t)}(\mathbf{h}_u^{(t)}, \mathbf{h}_v^{(t)}, \mathbf{e}_{uv})
   \]
   where \(M^{(t)}\) (message function) can be a neural network or any differentiable function that combines the features of node \(u\) (\(\mathbf{h}_u^{(t)}\)), node \(v\) (\(\mathbf{h}_v^{(t)}\)), and the edge feature \(\mathbf{e}_{uv}\).
    \item \textbf{Message Aggregation and Node Update:}
   Node \(v\) aggregates messages from its neighbors \(\mathcal{N}(v)\):
   \[
   \mathbf{h}_v^{(t+1)} = U^{(t)}(\mathbf{h}_v^{(t)}, \sum_{u \in \mathcal{N}(v)} \mathbf{m}_{uv}^{(t+1)})
   \]
   where \(U^{(t)}\) is the node update function, typically involving non-linear transformations such as those performed by neural network layers.
\end{itemize}

% \subsubsection{Hyperbolic Space Models}
% Hyperbolic space, characterized by constant negative curvature, offers unique properties that are well-suited for modeling hierarchical data structures, such as trees and networks with inherent hierarchies. Here we provide a brief overview of the two most utilized hyperbolic space models.
% \begin{itemize}
%     \item \textbf{PoincarÃ© Ball Model:}
%     The PoincarÃ© Disk model represents points within the open unit disk in Euclidean space. This model leverages the conformal property, meaning angles are preserved, making it particularly useful for visualizing hyperbolic space \citep{nickel2017poincare}. The distance between two points in the PoincarÃ© Disk increases exponentially as they approach the boundary, enabling efficient representation of hierarchical relationships with significant differences in scale. This property allows for capturing the exponential growth of nodes in hierarchical structures, thus effectively embedding data with a hierarchical nature.

%     \item \textbf{Lorentz Hyperboloid Model:}
%     The Lorentz hyperboloid model embeds points on the upper sheet of a two-sheeted hyperboloid in a higher-dimensional Minkowski space \citep{nickel2018learning}. This model leverages the Minkowski inner product, providing a different approach to measuring distances. The Lorentz model is particularly advantageous for tasks involving dynamic graphs and scenarios where the preservation of temporal relationships is crucial and benefits from simpler mathematical properties when performing certain operations, such as calculating geodesics and gradient descent. It represents hyperbolic space through the hyperboloid equation, which can efficiently manage the curvature constraints and maintain numerical stability during training \citep{chami2019hyperbolic}.
% \end{itemize}

\subsubsection{Hyperbolic Graph Neural Networks (HGNNs)}
\begin{table*} 
\centering
\begin{tabular}{p{4cm}p{2cm}p{2cm}p{2cm}p{2cm}}
    \hline
    \textbf{Architecture} & \textbf{Patience} & \textbf{Epochs Ran (Early Stopping)} & \textbf{Dev Accuracy} & \textbf{Test Accuracy}\\ \hline
    \textit{ClimateBERT} & - & - & \underline{90.9\%} & 90.9\%  \\ \hline
    \textit{RoBERTa\textnormal{$_{base}$}} & - & - & 90.6\% & 89.8\%  \\ \hline
    \textit{RoBERTa\textnormal{$_{large}$}} & - & - & \textbf{92.8\%} & \underline{91.7\%}  \\ \hline
    \textit{Graph Neural Network (GNNs)} & 8 & 16 & 87.9\% & \textbf{92.1\%}  \\ \hline 
    \textit{Hyperbolic Graph Neural Network (HGNNs)} & 8 & 23 & 89.0\% & 88.7\%  \\ \hline
    \end{tabular}
\caption{Results: We report the accuracy on the development set (dev), and the test set of the environmental claims dataset. The best performance per split is indicated in bold, the second best is underlined.}
\label{tab:accents}
\end{table*}
% HGNNs extend the principles of GNNs into hyperbolic space, allowing for an effective representation of hierarchical and tree-like structures, which are often more suitable for linguistic data. In hyperbolic space, the distances between nodes grow exponentially with their separation, making it easier to capture long-range dependencies and hierarchical relations inherent in dependency graphs.
For our dataset, nodes are initially embedded into hyperbolic space using the Lorentz Hyperboloid Model. We utilize the Lorentz Hyperboloid model over the PoincarÃ© Ball Model due to the former's superior ability to capture the inherent curvature of hierarchical data structures. This is because it represents hyperbolic space through the hyperboloid equation, which can efficiently manage the curvature constraints and maintain numerical stability during training \citep{chami2019hyperbolic}. The syntactic dependencies (edges) are also considered within the hyperbolic framework, taking advantage of hyperbolic distance metrics.

The message passing in HGNNs is analogous to Euclidean GNNs but adapted for hyperbolic geometry. Consider the \(n\)-dimensional Lorentz model \(\mathbb{L}^n\). The process includes:

\begin{itemize}
    \item \textbf{Message Construction in Hyperbolic Space:}
   At each step \(t\), the message passing is defined as:
  \[
  \mathbf{m}_{uv}^{(t+1)} = \text{Log}_{\mathbf{h}_v^{(t)}}^\mathbb{H}\left(\text{Add}_\mathbb{H} \left(\mathbf{h}_u^{(t)}, \mathbf{e}_{uv}\right) \right)
  \]
  where \(\mathbf{h}_u^{(t)}\) and \(\mathbf{h}_v^{(t)}\) are hyperbolic embeddings, \(\mathbf{e}_{uv}\) is the edge feature, \(\text{Log}_{\mathbf{h}_v^{(t)}}^\mathbb{H}\) is the logarithmic map at \(\mathbf{h}_v^{(t)}\) on the hyperboloid, and \(\text{Add}_\mathbb{H}\) represents the addition operation in the hyperbolic space.
    \item \textbf{Hyperbolic Message Aggregation and Node Update:}
   Aggregation and update operations are then performed:
  \[
  \mathbf{h}_v^{(t+1)} = \text{Exp}_{\mathbf{h}_v^{(t)}}^\mathbb{H} \left( \sum_{u \in \mathcal{N}(v)} \mathbf{m}_{uv}^{(t+1)} \right)
  \]
  Here, \(\text{Exp}_{\mathbf{h}_v^{(t)}}^\mathbb{H}\) is the exponential map that projects the aggregated message back into hyperbolic space.
\end{itemize}

A sequence of Message Passing operations are preformed to obtain the final Node Embeddings, which are then pooled to form a global graph representation. \citep{liu2019hyperbolic} use average pooling in their HGNN toolkit, we use the same. This pooled representation is then fed into a Multi Layer Perceptron for the task of Graph Classification.

\section{Results and Conclusion}
Table 1 shows that both our models achieve results that are comparable to the state-of-the-art in Environmental Claim Detection, even surpassing the current best test accuracy score. We achieve these scores with relatively simpler and much lighter architectures that use word2vec node features and a primitive edge encoding method and are trained without any hardware acceleration, pointing to the representational power of GNNs compared to pre-trained models%as compared to pre-trained models that require significantly more computational resources and extensive fine-tuning. 
. Specifically, we use 4 layers in both our models with an embedding dimension of 256. With a total number of edge types (dependency types) equal to 45, we calculate the number of parameters in both our models to be approximately 12M, far less than the 355M parameters in RoBERTa\textnormal{$_{large}$}, 125M in RoBERTa\textnormal{$_{base}$} and 110M in ClimateBERT. We achieve a better test accuracy with GNNs than with HGNNs, indicating a high hyperbolicity in the ECD dataset (lower hierarchy).

\section*{Limitations}
Our study highlights the potential of GNNs and HGNNs for environmental claim detection, but there are several limitations to our methodology and the broader application of graph structures in sequential modeling tasks.

\begin{itemize}
    \item \textbf{Limitations in Methodology}: The task of environmental claim detection, like many areas in NLP, relies heavily on sequential information to capture the flow of context within sentences. However, our approach utilizes word2vec embeddings, which do not encode sequential dependencies between words, limiting the modelâ€™s understanding of contextual nuances. More advanced embedding techniques, such as Transformer-based embeddings could offer richer semantic representations by preserving sequential information. Additionally, our method employs a basic edge encoding strategy to represent syntactic dependencies and also omits part-of-speech tag features that carry a significant part of the syntactic information present in dependency graphs. This simplification may result in a loss of important syntactic and semantic cues that could improve model performance.

    \item \textbf{Limitations of Graph Structures in Sequential Modeling}: While graph-based approaches offer a novel way to model hierarchical and relational structures, most textual data is inherently sequential and better suited to traditional models like transformers. Adapting sequential data for tasks like graph classification is complex and often unintuitive, requiring significant preprocessing and domain-specific adaptations that may not generalize. As a result, while graphs are a powerful tool to model certain NLP applications, they may struggle with the broader range of sequential tasks where traditional models excel.
\end{itemize}

% \section*{Ethics Statement}
% Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

% \section*{Acknowledgements}
% This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
% EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
% EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
% ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell, 
% ACL 2012 by Maggie Li and Michael White, 
% ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
% ACL 2002 by Eugene Charniak and Dekang Lin, 
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}


\end{document}
