%%% cite 하라는 related work!! 실제로 관련이 있다.
@inproceedings{ross2024artificial,
    title = "Is artificial intelligence still intelligence? {LLM}s generalize to novel adjective-noun pairs, but don`t mimic the full human distribution",
    author = "Ross, Hayley  and
      Davidson, Kathryn  and
      Kim, Najoung",
    editor = "Hupkes, Dieuwke  and
      Dankers, Verna  and
      Batsuren, Khuyagbaatar  and
      Kazemnejad, Amirhossein  and
      Christodoulopoulos, Christos  and
      Giulianelli, Mario  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 2nd GenBench Workshop on Generalisation (Benchmarking) in NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.genbench-1.9/",
    doi = "10.18653/v1/2024.genbench-1.9",
    pages = "131--153",
    abstract = "Inferences from adjective-noun combinations like {\textquotedblleft}Is artificial intelligence still intelligence?{\textquotedblright} provide a good test bed for LLMs' understanding of meaning and compositional generalization capability, since there are many combinations which are novel to both humans and LLMs but nevertheless elicit convergent human judgments. We study a range of LLMs and find that the largest models we tested are able to draw human-like inferences when the inference is determined by context and can generalize to unseen adjective-noun combinations. We also propose three methods to evaluate LLMs on these inferences out of context, where there is a distribution of human-like answers rather than a single correct answer. We find that LLMs show a human-like distribution on at most 75{\%} of our dataset, which is promising but still leaves room for improvement."
}

@article{ross2025fake,
  title={Fake reefs are sometimes reefs and sometimes not, but are always compositional},
  author={Ross, Hayley and Kim, Najoung and Davidson, Kathryn},
  journal={Experiments in Linguistic Meaning},
  volume={3},
  pages={332--343},
  year={2025}
}


%%% useful related works
@inproceedings{thagard1984conceptual,
  title={Conceptual combination and scientific discovery},
  author={Thagard, Paul},
  booktitle={PSA: Proceedings of the Biennial Meeting of the Philosophy of Science Association},
  volume={1984},
  number={1},
  pages={2--12},
  year={1984},
  organization={Cambridge University Press}
} % 과학적 concept가 어떻게 생기는가? conceptual combination이라고 이 논문에서 얘기한다. (그 이전 주장들: 흄: 경험을 많이 하다 보면 concept가 생김 abstraction/ 다른 concept로 완벽하게 설명됨 definition / ..) 아싸 진짜 이거는 '단어' 얘기를 한다! 그리고 default prediction이라는 스타일로 설명을 하는데, 깔끔하다. conceptual combination이 생기는 세 가지 부분을 얘기하고, 마지막 부분에 과학얘기인 본론이 들어간다. '그 대상을 알고 있어서 바로 떠올릴 수도 있지만, 머릿속에서 concept를 조합해서 상상해야 하기도 한다. light wave, natural selection 같은 용어가 그렇다, light wave가 처음 생겼을 때는 wave와 light를 조합. 그 이전에는 wave와 sound를 조합. 지금이야 wave라고 했을 때 여러 파동을 자연스레 떠올리지만.->빛의 '매질'을 찾으려고 애썼던 것도 그 영향이 있었다. donor concept의 영향을 받으니까. 'natural selection'도 '사람이 하는 breeding으로서의 selection -> 사람 대신 자연(nature)가 하는 selection'으로.
% 이렇게, conceptual combination을 하는 과정 자체가 정말 중요하며 그 과정에서 자연히 일어나는 모순들을 해결하는 게 과학적 개념 설정에 큰 역할을 한다.

@article{hampton1997emergent,
  title={Emergent attributes in combined concepts.},
  author={Hampton, James A},
  year={1997},
  publisher={American psychological association}
} % wan2022effects논문은 이 논문이 "Hampton posits that creativity comes from combining concepts that are not norlally seen as overlapping, that is, novel conceptual combinations. -> When the intersection of two concepts is an empty set or has no existing referent in reality, people who attempt to form a sensible combination of the two concepts are already engaging in creative thinking. People would produce emergent attributes to reconcile and accomodate the incompatible attributes of the parent conepts." 근데 맞네. creative processes involved when concepts are combined in conjunctions. A which are also B. Emergent attributes

@article{ward2001creative,
  title={Creative cognition, conceptual combination, and the creative writing of Stephen R. Donaldson.},
  author={Ward, Thomas B},
  journal={American Psychologist},
  volume={56},
  number={4},
  pages={350},
  year={2001},
  publisher={American Psychological Association}
}

@article{wan2002effects,
  title={Effects of novel conceptual combination on creativity},
  author={Wan, Wendy WN and CHIU, CHI-YUE},
  journal={The Journal of Creative Behavior},
  volume={36},
  number={4},
  pages={227--240},
  year={2002},
  publisher={Wiley Online Library}
} % 좋은 창의성은, 인간 모두 갖고 있는 인지적 활동을 잘하는 데서 일어난다는 가설을 갖고 많은 사람들이 일한다. 심리학자들은 그러한 것 중 conceptual combination이 human creativity에 중요하다며 수많은 연구를 했다. (incompatible object들이 붙어있을수록 더욱 novel한, emergent한 능력들이 생긴다는 얘기 -> Hampton 1997이 제일 중요. 아예 주장하는 거니까. creativity가 거기서부터 오니까. 이 논문은 '먼저 시켜보니 이후 창의성 검사 결과가 더 좋아졌다' 내용이었고.) intersection을 중요시 여기네.. '티컵인 동시에 컴퓨터'. 어떻게 그것이 가능하냐. 어떤 경우

%%% %%% noun compound 관련 related work들, conceptual combination

@phdthesis{tratz2011semantically,
  title={Semantically-enriched parsing for natural language understanding},
  author={Tratz, Stephen},
  year={2011},
  school={University of Southern California}
}

@article{cordeiro2019unsupervised,
  title={Unsupervised compositionality prediction of nominal compounds},
  author={Cordeiro, Silvio and Villavicencio, Aline and Idiart, Marco and Ramisch, Carlos},
  journal={Computational Linguistics},
  volume={45},
  number={1},
  pages={1--57},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

# TMLR
@article{srivastava2023bigbench,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author="{BIG-bench authors}",
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},
  note={}
} % 여기 BIG-Bench에서 conceptual combination 웍을 고려해야함!!

@inproceedings{ozturkler2023thinksum,
  title={Thinksum: Probabilistic reasoning over sets using large language models},
  author={Ozturkler, Batu and Malkin, Nikolay and Wang, Zhen and Jojic, Nebojsa},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1216--1239},
  year={2023}
} % inference method인데, CONCEPTUAL COMBINATION task의 성능을 높여줌. 다른 웍 더 없는 것 같은데, 이런 웍 더 있으면 당연히 인용하기!!

@inproceedings{shwartz2018olive,
  title={Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds Using Paraphrases in a Neural Model},
  author={Shwartz, Vered and Waterson, Chris},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  pages={218--224},
  year={2018}
}

@article{shwartz2019still,
  title={Still a pain in the neck: Evaluating text representations on lexical composition},
  author={Shwartz, Vered and Dagan, Ido},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={403--419},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{rambelli2024can,
  title={Can Large Language Models Interpret Noun-Noun Compounds? A Linguistically-Motivated Study on Lexicalized and Novel Compounds},
  author={Rambelli, Giulia and Chersoni, Emmanuele and Collacciani, Claudia and Bolognesi, Marianna},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={11823--11835},
  year={2024}
}

@article{ormerod2024kitchen,
  title={How is a “kitchen chair” like a “farm horse”? Exploring the representation of noun-noun compound semantics in transformer-based language models},
  author={Ormerod, Mark and del Rinc{\'o}n, Jes{\'u}s Mart{\'\i}nez and Devereux, Barry},
  journal={Computational Linguistics},
  volume={50},
  number={1},
  pages={49--81},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
} -> 얜 쫌 다르다..

@inproceedings{miletic2023systematic,
  title={A systematic search for compound semantics in pretrained BERT architectures},
  author={Mileti{\'c}, Filip and im Walde, Sabine Schulte},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={1499--1512},
  year={2023}
}

@inproceedings{li2022systematicity,
  title={Systematicity in GPT-3’s Interpretation of Novel English Noun Compounds},
  author={Li, Siyan and Carlson, Riley and Potts, Christopher},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={717--728},
  year={2022}
} -> 이거 꼭

@inproceedings{coil2023chocolate,
  title={From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?},
  author={Coil, Albert and Shwartz, Vered},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={2698--2710},
  year={2023}
} -> 이거 꼭

@inproceedings{buijtelaar2023psycholinguistic,
  title={A Psycholinguistic Analysis of BERT’s Representations of Compounds},
  author={Buijtelaar, Lars and Pezzelle, Sandro},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={2230--2241},
  year={2023}
} -> 흠... 이건 좀 다른데...

@inproceedings{hendrickx2013semeval,
    title = "{S}em{E}val-2013 Task 4: Free Paraphrases of Noun Compounds",
    author = "Hendrickx, Iris  and
      Kozareva, Zornitsa  and
      Nakov, Preslav  and
      {\'O} S{\'e}aghdha, Diarmuid  and
      Szpakowicz, Stan  and
      Veale, Tony",
    editor = "Manandhar, Suresh  and
      Yuret, Deniz",
    booktitle = "Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S13-2025",
    pages = "138--143",
}


@article{eppe2018computational,
  title={A computational framework for conceptual blending},
  author={Eppe, Manfred and Maclean, Ewen and Confalonieri, Roberto and Kutz, Oliver and Schorlemmer, Marco and Plaza, Enric and K{\"u}hnberger, Kai-Uwe},
  journal={Artificial Intelligence},
  volume={256},
  pages={105--129},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{kumar2024vision,
  title={Do Vision-Language Models Understand Compound Nouns?},
  author={Kumar, Sonal and Ghosh, Sreyan and Sakshi, S and Tyagi, Utkarsh and Manocha, Dinesh},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)},
  pages={519--527},
  year={2024}
}

@inproceedings{pezzelle-etal-2016-building,
    title = "Building a Bagpipe with a Bag and a Pipe: Exploring Conceptual Combination in Vision",
    author = "Pezzelle, Sandro  and
      Shekhar, Ravi  and
      Bernardi, Raffaella",
    editor = "Belz, Anya  and
      Erdem, Erkut  and
      Mikolajczyk, Krystian  and
      Pastra, Katerina",
    booktitle = "Proceedings of the 5th Workshop on Vision and Language",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-3208",
    doi = "10.18653/v1/W16-3208",
    pages = "60--64",
}

%%%% 여기서 좀 심리학스러운 내용. %%%%
@article{springer1992featureavailability,
  title={Feature availability in conceptual combination},
  author={Springer, Ken and Murphy, Gregory L},
  journal={Psychological Science},
  volume={3},
  number={2},
  pages={111--117},
  year={1992},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{murphy1988comprehending,
  title={Comprehending complex concepts},
  author={Murphy, Gregory L},
  journal={Cognitive science},
  volume={12},
  number={4},
  pages={529--562},
  year={1988},
  publisher={Wiley Online Library}
} % bigbench conceptual combination이 인용한 웍

@article{fodor1988connectionism,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988},
  publisher={Elsevier}
}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%% word association 또는 word understanding, commonsense of concept - concept combination은 안 쓰지만, '이게 무슨 뜻이야?' 를 LLM에게 물어보거나 그전에 LM이 더 잘 대답하도록 알려주는 웍. 즉 직접적 연관이 있는 웍이다. 그런데 우리가 쓰는 concept combination은 상당히 lexical한데.
%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{speer2017conceptnet,
  title={Conceptnet 5.5: An open multilingual graph of general knowledge},
  author={Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{sap2019atomic,
  title={Atomic: An atlas of machine commonsense for if-then reasoning},
  author={Sap, Maarten and Le Bras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A and Choi, Yejin},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={3027--3035},
  year={2019}
}

@article{de2019swow,
  title={The “Small World of Words” English word association norms for over 12,000 cue words},
  author={De Deyne, Simon and Navarro, Danielle J and Perfors, Amy and Brysbaert, Marc and Storms, Gert},
  journal={Behavior research methods},
  volume={51},
  pages={987--1006},
  year={2019},
  publisher={Springer}
}

@inproceedings{liu2022wax,
  title={WAX: A new dataset for word association explanations},
  author={Liu, Chunhua and Cohn, Trevor and De Deyne, Simon and Frermann, Lea},
  booktitle={Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={106--120},
  year={2022}
}

@article{mcrae2012semantic,
  title={Semantic and associative relations in adolescents and young adults: Examining a tenuous dichotomy.},
  author={McRae, Ken and Khalkhali, Saman and Hare, Mary},
  year={2012},
  publisher={American Psychological Association}
}

@inproceedings{jang2023improving,
  title={Improving Language Models’ Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary},
  author={Jang, Myeongjun and Lukasiewicz, Thomas},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={8496--8510},
  year={2023}
}

@inproceedings{pantazopoulos2022combine,
  title={Combine to describe: Evaluating compositional generalization in image captioning},
  author={Pantazopoulos, George and Suglia, Alessandro and Eshghi, Arash},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
  pages={115--131},
  year={2022}
}

@inproceedings{peng2022copen,
  title={COPEN: Probing Conceptual Knowledge in Pre-trained Language Models},
  author={Peng, Hao and Wang, Xiaozhi and Hu, Shengding and Jin, Hailong and Hou, Lei and Li, Juanzi and Liu, Zhiyuan and Liu, Qun},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5015--5035},
  year={2022}
}

@inproceedings{misra2023comps,
  title={COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models},
  author={Misra, Kanishka and Rayz, Julia and Ettinger, Allyson},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={2928--2949},
  year={2023}
}


%%% commonsense 쪽 related works - 간접적 연관이 있는 것 위주로.

%8
@inproceedings{tian2024macgyver,
  title={MacGyver: Are Large Language Models Creative Problem Solvers?},
  author={Tian, Yufei and Ravichander, Abhilasha and Qin, Lianhui and Le Bras, Ronan and Marjieh, Raja and Peng, Nanyun and Choi, Yejin and Griffiths, Thomas L and Brahman, Faeze},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={5303--5324},
  year={2024}
}

%9
@inproceedings{gu2023mentalmodel,
  title={Do language models have coherent mental models of everyday things?},
  author={Gu, Yuling and Dalvi, Bhavana and Clark, Peter},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1892--1913},
  year={2023}
}

%%% 데이터 소스.

@inproceedings{zhu2015bookcorpus,
    title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
    author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
    month = {December},
    year = {2015}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{roberts1994people,
  title={Why do people use figurative language?},
  author={Roberts, Richard M and Kreuz, Roger J},
  journal={Psychological science},
  volume={5},
  number={3},
  pages={159--163},
  year={1994},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@techreport{shutova2011computational,
  title={Computational approaches to figurative language},
  author={Shutova, Ekaterina V},
  year={2011},
  institution={University of Cambridge, Computer Laboratory}
}

@inproceedings{troiano-etal-2018-computational,
    title = "A Computational Exploration of Exaggeration",
    author = {Troiano, Enrica  and
      Strapparava, Carlo  and
      {\"O}zbal, G{\"o}zde  and
      Tekiro{\u{g}}lu, Serra Sinem},
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1367",
    doi = "10.18653/v1/D18-1367",
    pages = "3296--3304",
    abstract = "Several NLP studies address the problem of figurative language, but among non-literal phenomena, they have neglected exaggeration. This paper presents a first computational approach to this figure of speech. We explore the possibility to automatically detect exaggerated sentences. First, we introduce HYPO, a corpus containing overstatements (or hyperboles) collected on the web and validated via crowdsourcing. Then, we evaluate a number of models trained on HYPO, and bring evidence that the task of hyperbole identification can be successfully performed based on a small set of semantic features.",
}

@inproceedings{beigman-klebanov-etal-2016-argumentation,
    title = "{A}rgumentation: Content, Structure, and Relationship with Essay Quality",
    author = "Beigman Klebanov, Beata  and
      Stab, Christian  and
      Burstein, Jill  and
      Song, Yi  and
      Gyawali, Binod  and
      Gurevych, Iryna",
    editor = "Reed, Chris",
    booktitle = "Proceedings of the Third Workshop on Argument Mining ({A}rg{M}ining2016)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2808",
    doi = "10.18653/v1/W16-2808",
    pages = "70--75",
}

@inproceedings{stowe-etal-2022-impli,
    title = "{IMPLI}: Investigating {NLI} Models{'} Performance on Figurative Language",
    author = "Stowe, Kevin  and
      Utama, Prasetya  and
      Gurevych, Iryna",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.369",
    doi = "10.18653/v1/2022.acl-long.369",
    pages = "5375--5388",
    abstract = "Natural language inference (NLI) has been widely used as a task to train and evaluate models for language understanding. However, the ability of NLI models to perform inferences requiring understanding of figurative language such as idioms and metaphors remains understudied. We introduce the IMPLI (Idiomatic and Metaphoric Paired Language Inference) dataset, an English dataset consisting of paired sentences spanning idioms and metaphors. We develop novel methods to generate 24k semiautomatic pairs as well as manually creating 1.8k gold pairs. We use IMPLI to evaluate NLI models based on RoBERTa fine-tuned on the widely used MNLI dataset. We then show that while they can reliably detect entailment relationship between figurative phrases with their literal counterparts, they perform poorly on similarly structured examples where pairs are designed to be non-entailing. This suggests the limits of current NLI models with regard to understanding figurative language and this dataset serves as a benchmark for future improvements in this direction.",
}

@article{chakrabarty2022s,
  title={It’s not rocket science: Interpreting figurative language in narratives},
  author={Chakrabarty, Tuhin and Choi, Yejin and Shwartz, Vered},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={589--606},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{chakrabarty2022flute,
  title={Flute: Figurative language understanding through textual explanations},
  author={Chakrabarty, Tuhin and Saakyan, Arkadiy and Ghosh, Debanjan and Muresan, Smaranda},
  journal={arXiv preprint arXiv:2205.12404},
  year={2022}
}

@article{lai2022multi,
  title={Multi-figurative language generation},
  author={Lai, Huiyuan and Nissim, Malvina},
  journal={arXiv preprint arXiv:2209.01835},
  year={2022}
}

@inproceedings{chakrabarty-etal-2020-generating,
    title = "Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation",
    author = "Chakrabarty, Tuhin  and
      Muresan, Smaranda  and
      Peng, Nanyun",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.524",
    doi = "10.18653/v1/2020.emnlp-main.524",
    pages = "6455--6469",
    abstract = "Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of simile generation. Generating a simile requires proper understanding for effective mapping of properties between two concepts. To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge. We then propose to fine-tune a pre-trained sequence to sequence model, BART (Lewis et al 2019), on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence. Experiments show that our approach generates 88{\%} novel similes that do not share properties with the training data. Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37{\%} of the time when compared pairwise. We also show how replacing literal sentences with similes from our best model in machine-generated stories improves evocativeness and leads to better acceptance by human judges.",
}

@inproceedings{wachowiak2023does,
  title={Does gpt-3 grasp metaphors? identifying metaphor mappings with generative language models},
  author={Wachowiak, Lennart and Gromann, Dagmar},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1018--1032},
  year={2023}
}

@inproceedings{liu2018neural,
  title={Neural multitask learning for simile recognition},
  author={Liu, Lizhen and Hu, Xiao and Song, Wei and Fu, Ruiji and Liu, Ting and Hu, Guoping},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={1543--1553},
  year={2018}
}

@article{Zeng2020neuralsimile, title={Neural Simile Recognition with Cyclic Multitask Learning and Local Attention}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6496}, DOI={10.1609/aaai.v34i05.6496}, abstractNote={&lt;p&gt;Simile recognition is to detect simile sentences and to extract simile components, i.e., &lt;em&gt;tenors&lt;/em&gt; and &lt;em&gt;vehicles&lt;/em&gt;. It involves two subtasks: &lt;em&gt;simile sentence classification&lt;/em&gt; and &lt;em&gt;simile component extraction&lt;/em&gt;. Recent work has shown that standard multitask learning is effective for Chinese simile recognition, but it is still uncertain whether the mutual effects between the subtasks have been well captured by simple parameter sharing. We propose a novel cyclic multitask learning framework for neural simile recognition, which stacks the subtasks and makes them into a loop by connecting the last to the first. It iteratively performs each subtask, taking the outputs of the previous subtask as additional inputs to the current one, so that the interdependence between the subtasks can be better explored. Extensive experiments show that our framework significantly outperforms the current state-of-the-art model and our carefully designed baselines, and the gains are still remarkable using BERT. Source Code of this paper are available on https://github.com/DeepLearnXMU/Cyclic.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zeng, Jiali and Song, Linfeng and Su, Jinsong and Xie, Jun and Song, Wei and Luo, Jiebo}, year={2020}, month={Apr.}, pages={9515-9522} }

@inproceedings{rosen-2018-computationally,
    title = "Computationally Constructed Concepts: A Machine Learning Approach to Metaphor Interpretation Using Usage-Based Construction Grammatical Cues",
    author = "Rosen, Zachary",
    editor = "Beigman Klebanov, Beata  and
      Shutova, Ekaterina  and
      Lichtenstein, Patricia  and
      Muresan, Smaranda  and
      Wee, Chee",
    booktitle = "Proceedings of the Workshop on Figurative Language Processing",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-0912",
    doi = "10.18653/v1/W18-0912",
    pages = "102--109",
    abstract = "The current study seeks to implement a deep learning classification algorithm using argument-structure level representation of metaphoric constructions, for the identification of source domain mappings in metaphoric utterances. It thus builds on previous work in computational metaphor interpretation (Mohler et al. 2014; Shutova 2010; Bollegala {\&} Shutova 2013; Hong 2016; Su et al. 2017) while implementing a theoretical framework based off of work in the interface of metaphor and construction grammar (Sullivan 2006, 2007, 2013). The results indicate that it is possible to achieve an accuracy of approximately 80.4{\%} using the proposed method, combining construction grammatical features with a simple deep learning NN. I attribute this increase in accuracy to the use of constructional cues, extracted from the raw text of metaphoric instances.",
}

@inproceedings{liu2022testing,
    title = "Testing the Ability of Language Models to Interpret Figurative Language",
    author = "Liu, Emmy  and
      Cui, Chenxuan  and
      Zheng, Kenneth  and
      Neubig, Graham",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.330",
    doi = "10.18653/v1/2022.naacl-main.330",
    pages = "4437--4452",
    abstract = "Figurative and metaphorical language are commonplace in discourse, and figurative expressions play an important role in communication and cognition. However, figurative language has been a relatively under-studied area in NLP, and it remains an open question to what extent modern language models can interpret nonliteral phrases. To address this question, we introduce Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings. We evaluate the performance of several state-of-the-art language models on this task, and find that although language models achieve performance significantly over chance, they still fall short of human performance, particularly in zero- or few-shot settings. This suggests that further work is needed to improve the nonliteral reasoning capabilities of language models.",
}

@article{li2024finding,
  title={Finding Challenging Metaphors that Confuse Pretrained Language Models},
  author={Li, Yucheng and Guerin, Frank and Lin, Chenghua},
  journal={arXiv preprint arXiv:2401.16012},
  year={2024}
}

@article{glucksberg1997property,
  title={Property attribution in metaphor comprehension},
  author={Glucksberg, Sam and McGlone, Matthew S and Manfredi, Deanna},
  journal={Journal of memory and language},
  volume={36},
  number={1},
  pages={50--67},
  year={1997},
  publisher={Elsevier}
}

@article{chiappe2007role,
  title={The role of working memory in metaphor production and comprehension},
  author={Chiappe, Dan L and Chiappe, Penny},
  journal={Journal of memory and language},
  volume={56},
  number={2},
  pages={172--188},
  year={2007},
  publisher={Elsevier}
}

@article{liu2023we,
  title={We're afraid language models aren't modeling ambiguity},
  author={Liu, Alisa and Wu, Zhaofeng and Michael, Julian and Suhr, Alane and West, Peter and Koller, Alexander and Swayamdipta, Swabha and Smith, Noah A and Choi, Yejin},
  journal={arXiv preprint arXiv:2304.14399},
  year={2023}
}

@article{beaty2013metaphorically,
  title={Metaphorically speaking: Cognitive abilities and the production of figurative language},
  author={Beaty, Roger E and Silvia, Paul J},
  journal={Memory \& cognition},
  volume={41},
  pages={255--267},
  year={2013},
  publisher={Springer}
}

@misc{mcgrew2009chc,
  title={CHC theory and the human cognitive abilities project: Standing on the shoulders of the giants of psychometric intelligence research},
  author={McGrew, Kevin S},
  journal={Intelligence},
  volume={37},
  number={1},
  pages={1--10},
  year={2009},
  publisher={Elsevier}
}

@article{silvia2012making,
  title={Making creative metaphors: The importance of fluid intelligence for creative thought},
  author={Silvia, Paul J and Beaty, Roger E},
  journal={Intelligence},
  volume={40},
  number={4},
  pages={343--351},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@article{fan2018hierarchical,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}

@book{jaszczolt2003meaning,
  title={Meaning through language contrast},
  author={Jaszczolt, Kasia M and Turner, Ken},
  volume={2},
  year={2003},
  publisher={John Benjamins Publishing}
}

@misc{fos,
    title = {Figure of speech},
    url = {https://www.merriam-webster.com/dictionary/figure%20of%20speech},
    author = {Merriam-Webster Dictionary},
    year = {2024},
    note = {Accessed on March 24, 2024}
}

@article{gildea1983understanding,
  title={On understanding metaphor: The role of context},
  author={Gildea, Patricia and Glucksberg, Sam},
  journal={Journal of verbal learning and verbal behavior},
  volume={22},
  number={5},
  pages={577--590},
  year={1983},
  publisher={Elsevier}
}

@book{glucksberg2001understanding,
  title={Understanding figurative language: From metaphor to idioms},
  author={Glucksberg, Sam and McGlone, Matthew S},
  number={36},
  year={2001},
  publisher={Oxford University Press}
}

@incollection{frisson2001obtaining,
  title={Obtaining a figurative interpretation of a word: Support for underspecification},
  author={Frisson, Steven and Pickering, Martin J},
  booktitle={Models of Figurative Language},
  pages={149--171},
  year={2001},
  publisher={Psychology Press}
}

@article{swinney1979lexical,
  title={Lexical access during sentence comprehension:(Re) consideration of context effects},
  author={Swinney, David A},
  journal={Journal of verbal learning and verbal behavior},
  volume={18},
  number={6},
  pages={645--659},
  year={1979},
  publisher={Elsevier}
}

@article{tanenhaus1979evidence,
  title={Evidence for multiple stages in the processing of ambiguous words in syntactic contexts},
  author={Tanenhaus, Michael K and Leiman, James M and Seidenberg, Mark S},
  journal={Journal of verbal learning and verbal behavior},
  volume={18},
  number={4},
  pages={427--440},
  year={1979},
  publisher={Elsevier}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{bosselut2019comet,
  title={COMET: Commonsense transformers for automatic knowledge graph construction},
  author={Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
  journal={arXiv preprint arXiv:1906.05317},
  year={2019}
}


@article{chen2022probing,
  title={Probing simile knowledge from pre-trained language models},
  author={Chen, Weijie and Chang, Yongzhu and Zhang, Rongsheng and Pu, Jiashu and Chen, Guandan and Zhang, Le and Xi, Yadong and Chen, Yijiang and Su, Chang},
  journal={arXiv preprint arXiv:2204.12807},
  year={2022}
}

@article{mack1975metaphoring,
  title={Metaphoring as speech act: some happiness conditions for implicit similes and simple metaphors},
  author={Mack, Dorothy},
  journal={Poetics},
  volume={4},
  number={2-3},
  pages={221--256},
  year={1975},
  publisher={Elsevier}
}

@phdthesis{juska2007missing,
  title={Missing the Mark: Similes, Metaphors, Where They Fail, and What it Means},
  author={Juska, Holly},
  year={2007},
  school={Citeseer}
}

@inproceedings{rakshit2022figurativeqa,
  title={FigurativeQA: A Test Benchmark for Figurativeness Comprehension for Question Answering},
  author={Rakshit, Geetanjali and Flanigan, Jeffrey},
  booktitle={Proceedings of the 3rd Workshop on Figurative Language Processing (FLP)},
  pages={160--166},
  year={2022}
}

@inproceedings{ma2023run,
    title = "{I} run as fast as a rabbit, can you? A Multilingual Simile Dialogues Datasets",
    author = "Ma, Longxuan  and
      Zhang, Wei-Nan  and
      Zhou, Shuhan  and
      Sun, Churui  and
      Ke, Changxin  and
      Liu, Ting",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.453",
    doi = "10.18653/v1/2023.findings-acl.453",
    pages = "7223--7237",
    abstract = "A simile is a figure of speech that compares two different things (called the tenor and the vehicle) via shared properties. The tenor and the vehicle are usually connected with comparator words such as {``}like{''} or {``}as{''}. The simile phenomena are unique and complex in a real-life dialogue scene where the tenor and the vehicle can be verbal phrases or sentences, mentioned by different speakers, exist in different sentences, or occur in reversed order. However, the current simile research usually focuses on similes in a triplet tuple (tenor, property, vehicle) or a single sentence where the tenor and vehicle are usually entities or noun phrases, which could not reflect complex simile phenomena in real scenarios. In this paper, we propose a novel and high-quality multilingual simile dialogue (MSD) dataset to facilitate the study of complex simile phenomena. The MSD is the largest manually annotated simile data ({\textasciitilde}21K) and it contains both English and Chinese data. Meanwhile, the MSD data can also be used on dialogue tasks to test the ability of dialogue systems when using similes. We design 3 simile tasks (recognition, interpretation, and generation) and 2 dialogue tasks (retrieval and generation) with MSD. For each task, we provide experimental results from strong pre-trained or state-of-the-art models. The experiments demonstrate the challenge of MSD and we will release the data/code on GitHub.",
}

@article{hoffman2013semantic,
  title={Semantic diversity: A measure of semantic ambiguity based on variability in the contextual usage of words},
  author={Hoffman, Paul and Lambon Ralph, Matthew A and Rogers, Timothy T},
  journal={Behavior research methods},
  volume={45},
  pages={718--730},
  year={2013},
  publisher={Springer}
}

@misc{SFRAIResearch2024,
  title={SFR-Embedding-Mistral:Enhance Text Retrieval with Transfer Learning},
  author={Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Semih Yavuz},
  howpublished={Salesforce AI Research Blog},
  year={2024},
  url={https://blog.salesforceairesearch.com/sfr-embedded-mistral/}
}

@inproceedings{he2022can,
    title = "Can Pre-trained Language Models Interpret Similes as Smart as Human?",
    author = "He, Qianyu  and
      Cheng, Sijie  and
      Li, Zhixu  and
      Xie, Rui  and
      Xiao, Yanghua",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.543",
    doi = "10.18653/v1/2022.acl-long.543",
    pages = "7875--7887",
    abstract = "Simile interpretation is a crucial task in natural language processing. Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art performance on many tasks. However, it remains under-explored whether PLMs can interpret similes or not. In this paper, we investigate the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, i.e., to let the PLMs infer the shared properties of similes. We construct our simile property probing datasets from both general textual corpora and human-designed questions, containing 1,633 examples covering seven main categories. Our empirical study based on the constructed datasets shows that PLMs can infer similes{'} shared properties while still underperforming humans. To bridge the gap with human performance, we additionally design a knowledge-enhanced training objective by incorporating the simile knowledge into PLMs via knowledge embedding methods. Our method results in a gain of 8.58{\%} in the probing task and 1.37{\%} in the downstream task of sentiment classification. The datasets and code are publicly available at \url{https://github.com/Abbey4799/PLMs-Interpret-Simile}.",
}


@inproceedings{he2023maps,
  title={Maps-kb: A million-scale probabilistic simile knowledge base},
  author={He, Qianyu and Wang, Xintao and Liang, Jiaqing and Xiao, Yanghua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={5},
  pages={6398--6406},
  year={2023}
}

@inproceedings{yang2023fantastic,
  title={Fantastic expressions and where to find them: Chinese simile generation with multiple constraints},
  author={Yang, Kexin and Liu, Dayiheng and Lei, Wenqiang and Yang, Baosong and Wei, Xiangpeng and Liu, Zhengyuan and Xie, Jun},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={468--486},
  year={2023}
}

@book{shakespeare_romeo_1597,
    author = {Shakespeare, William},
    title = {Romeo and Juliet},
    year = {1597},
    publisher = {Thomas Creede},
    address = {London}
}

@book{stern2000metaphor,
  title={Metaphor in context},
  author={Stern, Josef},
  year={2000},
  publisher={mit Press}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{clark1975understanding,
  title={Understanding what is meant from what is said: A study in conversationally conveyed requests},
  author={Clark, Herbert H and Lucy, Peter},
  journal={Journal of verbal learning and verbal behavior},
  volume={14},
  number={1},
  pages={56--72},
  year={1975},
  publisher={Elsevier}
}

@book{lyons1977semantics,
  title={Semantics: Volume 2},
  author={Lyons, John},
  volume={2},
  year={1977},
  publisher={Cambridge university press}
}

@article{paul1970figurative,
  title={Figurative language},
  author={Paul, Anthony M},
  journal={Philosophy \& Rhetoric},
  pages={225--248},
  year={1970},
  publisher={JSTOR}
}

@article{bredin1998comparisons,
  title={Comparisons and similes},
  author={Bredin, Hugh},
  journal={Lingua},
  volume={105},
  number={1-2},
  pages={67--78},
  year={1998},
  publisher={Elsevier}
}

@article{mcglone2001topic,
  title={Topic—vehicle interaction in metaphor comprehension},
  author={McGlone, Matthew S and Manfredi, Deanna A},
  journal={Memory \& Cognition},
  volume={29},
  pages={1209--1219},
  year={2001},
  publisher={Springer}
}

@inproceedings{gabriel2021paragraph,
  title={Paragraph-level commonsense transformers with recurrent memory},
  author={Gabriel, Saadia and Bhagavatula, Chandra and Shwartz, Vered and Le Bras, Ronan and Forbes, Maxwell and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={14},
  pages={12857--12865},
  year={2021}
}

@article{bosselut2019comet,
  title={COMET: Commonsense transformers for automatic knowledge graph construction},
  author={Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
  journal={arXiv preprint arXiv:1906.05317},
  year={2019}
}

@article{song2020knowledge,
  title={A knowledge graph embedding approach for metaphor processing},
  author={Song, Wei and Guo, Jingjin and Fu, Ruiji and Liu, Ting and Liu, Lizhen},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={406--420},
  year={2020},
  publisher={IEEE}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{shao2024cmdag,
  title={CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation},
  author={Shao, Yujie and Yao, Xinrong and Qu, Xingwei and Lin, Chenghua and Wang, Shi and Huang, Stephen W and Zhang, Ge and Fu, Jie},
  journal={arXiv preprint arXiv:2402.13145},
  year={2024}
}


@misc{openai2022chatgpt,
  title={Introducing ChatGPT},
  author={{OpenAI}},
  howpublished={\url{https://openai.com/index/chatgpt/}},
  year={2022},
  note={Accessed: 2024-05-18}
}

@misc{meta2024llama3,
  title={Introducing Meta Llama 3: The most capable openly available LLM to date
},
  author={{Meta}},
  howpublished={\url{https://ai.meta.com/blog/meta-llama-3/}},
  year={2024},
  note={Accessed: 2024-04-18}
}



@misc{openai2024gpt4o,
  title={Hello GPT-4o},
  author={{OpenAI}},
  howpublished={\url{https://openai.com/index/hello-gpt-4o/}},
  year={2024},
  note={Accessed: 2024-05-13}
}

@misc{openai2024gpt4omini,
  title={GPT-4o mini: advancing cost-efficient intelligence},
  author={{OpenAI}},
  howpublished={\url{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}},
  year={2024},
  note={Accessed: 2024-07-18}
}

@misc{openai2024o1mini,
  title={OpenAI o1-mini},
  author={{OpenAI}},
  howpublished={\url{https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/}},
  year={2024},
  note={Accessed: 2024-10-12}
}

@misc{openai2024o1,
  title={Introducing OpenAI o1},
  author={{OpenAI}},
  howpublished={\url{https://openai.com/o1/}},
  year={2024},
  note={Accessed: 2025-02-08}
}


@article{llama3modelcard,
    title={Llama 3 Model Card},
    author={AI@Meta},
    year={2024},
    url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}


@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@inproceedings{shuhan-etal-2023-exploring,
    title = "Exploring Accurate and Generic Simile Knowledge from Pre-trained Language Models",
    author = "Shuhan, Zhou  and
      Longxuan, Ma  and
      Yanqiu, Shao",
    editor = "Sun, Maosong  and
      Qin, Bing  and
      Qiu, Xipeng  and
      Jiang, Jing  and
      Han, Xianpei",
    booktitle = "Proceedings of the 22nd Chinese National Conference on Computational Linguistics",
    month = aug,
    year = "2023",
    address = "Harbin, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2023.ccl-1.78",
    pages = "918--929",
    abstract = "{``}A simile is an important linguistic phenomenon in daily communication and an important taskin natural language processing (NLP). In recent years, pre-trained language models (PLMs) haveachieved great success in NLP since they learn generic knowledge from a large corpus. However,PLMs still have hallucination problems that they could generate unrealistic or context-unrelatedinformation.In this paper, we aim to explore more accurate simile knowledge from PLMs.To this end, we first fine-tune a single model to perform three main simile tasks (recognition,interpretation, and generation). In this way, the model gains a better understanding of the simileknowledge. However, this understanding may be limited by the distribution of the training data. To explore more generic simile knowledge from PLMs, we further add semantic dependencyfeatures in three tasks. The semantic dependency feature serves as a global signal and helpsthe model learn simile knowledge that can be applied to unseen domains. We test with seenand unseen domains after training. Automatic evaluations demonstrate that our method helps thePLMs to explore more accurate and generic simile knowledge for downstream tasks. Our methodof exploring more accurate knowledge is not only useful for simile study but also useful for otherNLP tasks leveraging knowledge from PLMs. Our code and data will be released on GitHub.{''}",
    language = "English",
}

@misc{tong2024metaphor,
title={Metaphor Understanding Challenge Dataset for LLMs},
author={Xiaoyu Tong and Rochelle Choenni and Martha Lewis and Ekaterina Shutova},
year={2024},
eprint={2403.11810},
archivePrefix={arXiv},
primaryClass={cs.CL}
}

@article{yu2003synesthetic,
  title={Synesthetic metaphor: A cognitive perspective},
  author={Yu, Ning},
  year={2003},
  publisher={Walter de Gruyter GmbH \& Co. KG Berlin, Germany}
}

@article{gibbs1999mental,
  title={Mental imagery in interpreting poetic metaphor},
  author={Gibbs Jr, Raymond W and Bogdonovich, Jody},
  journal={Metaphor and Symbol},
  volume={14},
  number={1},
  pages={37--54},
  year={1999},
  publisher={Taylor \& Francis}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@incollection{10.1093/acprof:oso/9780198242468.003.0005,
    author = {KITTAY, EVA FEDER},
    isbn = {9780198242468},
    title = "{140Interpreting Metaphor}",
    booktitle = "{Metaphor: Its Cognitive Force and Linguistic Structure}",
    publisher = {Oxford University Press},
    year = {1990},
    month = {01},
    abstract = "{This chapter explores the nature and structure of that second-order meaning which results from a particular mapping of first-order meaning and which yields metaphor. The second-order meaning of metaphor results from the double semantic content carried by metaphor. The very first step in interpreting metaphor requires recognition that the background assumptions presumed by the context of an utterance are in conflict with such a first-order interpretation. The next step basically involves the context that frames the vehicle term. Metaphorical interpretation then is conceived as a particular function of the literal and conventional interpretation. Thus, metaphorical use of language is often more valued than literal use as it is considered to be an upgraded version that makes language more interesting.}",
    doi = {10.1093/acprof:oso/9780198242468.003.0005},
    url = {https://doi.org/10.1093/acprof:oso/9780198242468.003.0005},
    eprint = {https://academic.oup.com/book/0/chapter/144574619/chapter-ag-pdf/45013159/book\_3438\_section\_144574619.ag.pdf},
}

@article{yerukola2024pope,
  title={Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Intent Resolution in LLMs},
  author={Yerukola, Akhila and Vaduguru, Saujas and Fried, Daniel and Sap, Maarten},
  journal={arXiv preprint arXiv:2405.08760},
  year={2024}
}

@article{boers2000metaphor,
  title={Metaphor awareness and vocabulary retention},
  author={Boers, F.},
  journal={Applied Linguistics},
  volume={21},
  pages={553-571},
  year={2000},
  publisher={Oxford University Press}
}

@book{glucksberg2001understanding,
  title={Understanding figurative language: From metaphors to idioms},
  author={Glucksberg, Sam},
  year={2001},
  publisher={Oxford University Press}
}

@article{sharifian2011language,
  title={Figurative language in international political discourse: The case of Iran},
  author={Sharifian, Farzad},
  journal={Journal of Language and Politics},
  volume={8},
  number={3},
  pages={416-432},
  year={2011},
  publisher={John Benjamins}
}

@article{mcmullen1996studying,
  title={Studying the use of figurative language in psychotherapy: The search for researchable questions},
  author={McMullen, Linda},
  journal={Metaphor and Symbol},
  volume={11},
  number={4},
  pages={241-255},
  year={1996},
  publisher={Taylor \& Francis}
}

@article{roberts1993study,
  title={The empirical study of figurative language in literature},
  author={Roberts, Richard M and Kreuz, Roger J},
  journal={Poetics},
  volume={22},
  number={3},
  pages={151-169},
  year={1993},
  publisher={Elsevier}
}

@incollection{deignan2015language,
  title={Figurative language and lexicography},
  author={Deignan, Alice},
  booktitle={The Oxford Handbook of Lexicography},
  pages={1-15},
  year={2015},
  publisher={Oxford University Press}
}

@inproceedings{kao2016computational,
  title={Empirical and Computational Approaches to Metaphor and Figurative Meaning},
  author={Kao, Justine T. and Goodman, Noah D.},
  booktitle={Cognitive Science},
  year={2016},
  url={https://consensus.app/papers/computational-approaches-metaphor-figurative-meaning-kao/ae599ce0cde15ad983b6557d95e622d5/?utm_source=chatgpt}
}

@article{uchiyama2012distinction,
  title={Distinction between the literal and intended meanings of sentences: A functional magnetic resonance imaging study of metaphor and sarcasm},
  author={Uchiyama, Hitoshi and Saito, D. and Tanabe, H. and Harada, T. and Seki, A. and Ohno, K. and Koeda, T. and Sadato, N.},
  journal={Cortex},
  volume={48},
  pages={563-583},
  year={2012},
  doi={10.1016/j.cortex.2011.01.004},
  url={https://consensus.app/papers/distinction-literal-intended-meanings-sentences-uchiyama/d11545a9c70f567aaa08e82f4199b451/?utm_source=chatgpt}
}

@inproceedings{kao2014formalizing,
  title={Formalizing the Pragmatics of Metaphor Understanding},
  author={Kao, Justine T. and Bergen, Leon and Goodman, Noah D.},
  booktitle={Cognitive Science},
  year={2014},
  url={https://consensus.app/papers/formalizing-pragmatics-metaphor-understanding-kao/1003ec32130c5cedbeb6bfe083884078/?utm_source=chatgpt}
}

@article{demorest1983telling,
  title={Telling it as it isn't: Children's understanding of figurative language},
  author={Demorest, A. and Silberstein, L. and Gardner, H. and Winner, E.},
  journal={British Journal of Developmental Psychology},
  volume={1},
  pages={121-134},
  year={1983},
  doi={10.1111/J.2044-835X.1983.TB00550.X},
  url={https://consensus.app/papers/telling-isnt-childrens-understanding-language-demorest/0fc99c090b8957cd92bbf1959fbb9680/?utm_source=chatgpt}
}


@article{rodd2002semantic,
  title={Making Sense of Semantic Ambiguity: Semantic Competition in Lexical Access},
  author={Rodd, J. and Gaskell, G. and Marslen-Wilson, W.},
  journal={Journal of Memory and Language},
  volume={46},
  pages={245-266},
  year={2002},
  doi={10.1006/JMLA.2001.2810},
  url={https://consensus.app/papers/making-sense-semantic-ambiguity-semantic-competition-rodd/9ff231f83b9251bc9047de06dd5354f9/?utm_source=chatgpt}
}

@article{rodd2004modelling,
  title={Modelling the effects of semantic ambiguity in word recognition},
  author={Rodd, J. and Gaskell, M. and Marslen-Wilson, W.},
  journal={Cogn. Sci.},
  volume={28},
  pages={89-104},
  year={2004},
  doi={10.1016/J.COGSCI.2003.08.002},
  url={https://consensus.app/papers/modelling-effects-ambiguity-word-recognition-rodd/c9e237e844e255a2996bd481a8a1fd19/?utm_source=chatgpt}
}

@article{montoyo2005combining,
  title={Combining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods},
  author={Montoyo, A. and Palomar, M. and Rigau, German and Suarez, A.},
  journal={J. Artif. Intell. Res.},
  volume={23},
  pages={299-330},
  year={2005},
  doi={10.1613/jair.1529},
  url={https://consensus.app/papers/combining-knowledge-corpusbased-montoyo/310c789543145b7a9db30c643c3da6f8/?utm_source=chatgpt}
}

@article{holmes1979accessing,
  title={Accessing Ambiguous Words during Sentence Comprehension},
  author={Holmes, V. M.},
  journal={Quarterly Journal of Experimental Psychology},
  volume={31},
  pages={569 - 589},
  year={1979},
  doi={10.1080/14640747908400749},
  url={https://consensus.app/papers/accessing-words-sentence-comprehension-holmes/cf3c92d5ae9058399be680b683ea2708/?utm_source=chatgpt}
}

@article{clark1997speaker,
  title={Speaker perspective and reference in young children},
  author={Clark, E. and Svaib, Trisha A.},
  journal={First Language},
  volume={17},
  pages={057 - 74},
  year={1997},
  doi={10.1177/014272379701705103},
  url={https://consensus.app/papers/speaker-perspective-reference-children-clark/1060b41fe1d157fea50b875f68810969/?utm_source=chatgpt}
}

@article{paul1992influence,
  title={Influence of contextual features on the activation of ambiguous word meanings},
  author={Paul, S. T. and Kellas, G. and Martin, Michael and Clark, Matthew B.},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  volume={18},
  number={4},
  pages={703-717},
  year={1992},
  doi={10.1037//0278-7393.18.4.703},
  url={https://consensus.app/papers/influence-features-activation-word-meanings-paul/fd3fd9e128ab53989e496b7766e9fce7/?utm_source=chatgpt}
}

@article{qualls2003working,
  title={Working memory and figurative language type: Influencing factors in African American adults' comprehension of figurative language},
  author={Qualls, Cedric D. and Harris, Joyce L.},
  journal={American Journal of Speech-Language Pathology},
  volume={12},
  number={1},
  pages={92-102},
  year={2003},
  publisher={American Speech-Language-Hearing Association},
  doi={10.1044/1058-0360(2003/055)}
}

@article{seigneuric2016childrens,
  title={Children's comprehension skill and the understanding of nominal metaphors},
  author={Seigneuric, Audrey and Megherbi, Hichem and Bueno, Sandra and Lebahar, Julie and Bianco, Margarita},
  journal={Journal of Experimental Child Psychology},
  volume={150},
  pages={346-363},
  year={2016},
  publisher={Elsevier},
  doi={10.1016/j.jecp.2016.06.008}
}

@article{frisson2001obtaining,
  title={Obtaining a figurative interpretation of a word: Support for underspecification},
  author={Frisson, Steven and Pickering, Martin J.},
  journal={Metaphor and Symbol},
  volume={16},
  number={3-4},
  pages={149-171},
  year={2001},
  publisher={Taylor \& Francis},
  doi={10.1080/10926488.2001.9678893}
}

@article{yu2005mind,
  title={On our mind: Salience, context, and figurative language},
  author={Yu, Ning},
  journal={Language in Society},
  volume={34},
  number={2},
  pages={307-310},
  year={2005},
  publisher={Cambridge University Press},
  doi={10.1017/S0047404505280119}
}

@article{readence1983word,
  title={Word knowledge and metaphorical interpretation},
  author={Readence, John E.},
  journal={Journal of Reading},
  volume={26},
  number={5},
  pages={448-455},
  year={1983},
  publisher={International Literacy Association},
  doi={10.2307/40028316}
}

@article{weick2015ambiguity,
  title={Ambiguity as Grasp: The Reworking of Sense},
  author={Weick, Karl},
  journal={Change Management Strategy eJournal},
  year={2015},
  doi={10.1111/1468-5973.12080}
}

@inproceedings{shirin2018replacing,
  title={Replacing Idioms Based on Their Figurative Usage},
  author={Shirin, A. Fathima and Raseek, C.},
  booktitle={2018 International Conference on Emerging Trends and Innovations In Engineering And Technological Research (ICETIETR)},
  pages={1-6},
  year={2018},
  doi={10.1109/ICETIETR.2018.8529042}
}

@article{tyler2005metaphor,
  title={Metaphor and Management: making sense of change},
  author={Tyler, Chris},
  journal={Management in Education},
  volume={19},
  number={3},
  pages={28-32},
  year={2005},
  publisher={SAGE Publications},
  doi={10.1177/08920206050190030701}
}

@phdthesis{phdthesis,
author = {Hartung, Matthias},
year = {2015},
month = {12},
chapter={7. Attribute Selection: Experimental Evaluation},
pages = {98-102},
title = {Distributional Semantic Models of Attribute Meaning in Adjectives and Nouns}
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{madaan2024selfrefine,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@article{kaufer1983metaphor,
  title={Metaphor and its ties to ambiguity and vagueness},
  author={Kaufer, David},
  journal={Rhetoric Society Quarterly},
  volume={13},
  number={3-4},
  pages={209--220},
  year={1983},
  publisher={Taylor \& Francis}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{anthropic2024claude,
  title={The claude 3 model family: Opus, sonnet, haiku},
  author={Anthropic, AI},
  journal={Claude-3 Model Card},
  year={2024}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@book{runco2020encyclopedia,
  title={Encyclopedia of creativity},
  author={Runco, Mark A and Pritzker, Steven R},
  year={2020},
  publisher={Academic press}
}

@misc{wiki,
  author = {Wikipedia},
  title = {Conceptual combination},
  year = {2024},
  url = {https://en.wikipedia.org/wiki/Conceptual_combination}
}

@book{mit,
  author = {Mark T. Keane and Fintan Costello},
  title = {Setting Limits on Analogy: Why Conceptual Combination Is Not Structural Alignment},
  year = {2001},
  publisher = {The MIT Press},
  url = {https://direct.mit.edu/book/1251/The-Analogical-Mind-Perspectives-from-Cognitive}
}
@article{gerlach2020standardized,
  title={A standardized Project Gutenberg corpus for statistical analysis of natural language and quantitative linguistics},
  author={Gerlach, Martin and Font-Clos, Francesc},
  journal={Entropy},
  volume={22},
  number={1},
  pages={126},
  year={2020},
  publisher={MDPI}
}

@inproceedings{zhu-etal-2021-mediasum,
    title = "{M}edia{S}um: A Large-scale Media Interview Dataset for Dialogue Summarization",
    author = "Zhu, Chenguang  and
      Liu, Yang  and
      Mei, Jie  and
      Zeng, Michael",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.474",
    doi = "10.18653/v1/2021.naacl-main.474",
    pages = "5927--5934",
    abstract = "This paper introduces MediaSum, a large-scale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MediaSum can be used in transfer learning to improve a model{'}s performance on other dialogue summarization tasks.",
}

@article{lin2019commongen,
  title={CommonGen: A constrained text generation challenge for generative commonsense reasoning},
  author={Lin, Bill Yuchen and Zhou, Wangchunshu and Shen, Ming and Zhou, Pei and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang},
  journal={arXiv preprint arXiv:1911.03705},
  year={2019}
}

@article{zhang2023learning,
  title={Learning to Predict Concept Ordering for Common Sense Generation},
  author={Zhang, Tianhui and Bollegala, Danushka and Peng, Bei},
  journal={arXiv preprint arXiv:2309.06363},
  year={2023}
}

@article{zhou2020pre,
  title={Pre-training text-to-text transformers for concept-centric common sense},
  author={Zhou, Wangchunshu and Lee, Dong-Ho and Selvam, Ravi Kiran and Lee, Seyeon and Lin, Bill Yuchen and Ren, Xiang},
  journal={arXiv preprint arXiv:2011.07956},
  year={2020}
}

@article{estes2002emergence,
  title={The emergence of novel attributes in concept modification},
  author={Estes, Zachary and Ward, Thomas B},
  journal={Creativity Research Journal},
  volume={14},
  number={2},
  pages={149--156},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{ward2007creative,
  title={Creative cognition as a window on creativity},
  author={Ward, Thomas B},
  journal={Methods},
  volume={42},
  number={1},
  pages={28--37},
  year={2007},
  publisher={Elsevier}
}

@article{liu2023vera,
  title={Vera: A general-purpose plausibility estimation model for commonsense statements},
  author={Liu, Jiacheng and Wang, Wenya and Wang, Dianzhuo and Smith, Noah A and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2305.03695},
  year={2023}
}

@inproceedings{wang2023cat,
  title={CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning},
  author={Wang, Weiqi and Fang, Tianqing and Xu, Baixuan and Bo, Chun Yi Louis and Song, Yangqiu and Chen, Lei},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13111--13140},
  year={2023}
}

@article{he2024acquiring,
  title={Acquiring and modeling abstract commonsense knowledge via conceptualization},
  author={He, Mutian and Fang, Tianqing and Wang, Weiqi and Song, Yangqiu},
  journal={Artificial Intelligence},
  volume={333},
  pages={104149},
  year={2024},
  publisher={Elsevier}
}

@article{wang2024candle,
  title={CANDLE: iterative conceptualization and instantiation distillation from large language models for commonsense reasoning},
  author={Wang, Weiqi and Fang, Tianqing and Li, Chunyang and Shi, Haochen and Ding, Wenxuan and Xu, Baixuan and Wang, Zhaowei and Bai, Jiaxin and Liu, Xin and Cheng, Jiayang and others},
  journal={arXiv preprint arXiv:2401.07286},
  year={2024}
}

@article{kohn2011conceptual,
  title={Conceptual combinations and subsequent creativity},
  author={Kohn, Nicholas W and Paulus, Paul B and Korde, Runa M},
  journal={Creativity Research Journal},
  volume={23},
  number={3},
  pages={203--210},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{wisniewski1997concepts,
  title={When concepts combine},
  author={Wisniewski, Edward J},
  journal={Psychonomic bulletin \& review},
  volume={4},
  pages={167--183},
  year={1997},
  publisher={Springer}
}

@article{gruber1993translation,
  title={A translation approach to portable ontology specifications},
  author={Gruber, Thomas R},
  journal={Knowledge acquisition},
  volume={5},
  number={2},
  pages={199--220},
  year={1993},
  publisher={Elsevier}
}

@incollection{smith2012ontology,
  title={Ontology},
  author={Smith, Barry},
  booktitle={The furniture of the world},
  pages={47--68},
  year={2012},
  publisher={Brill}
}

@inproceedings{van-durme-etal-2009-deriving,
    title = "Deriving Generalized Knowledge from Corpora Using {W}ord{N}et Abstraction",
    author = "Van Durme, Benjamin  and
      Michalak, Phillip  and
      Schubert, Lenhart",
    editor = "Lascarides, Alex  and
      Gardent, Claire  and
      Nivre, Joakim",
    booktitle = "Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009)",
    month = mar,
    year = "2009",
    address = "Athens, Greece",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E09-1092",
    pages = "808--816",
}

@inproceedings{song2011short,
  title={Short text conceptualization using a probabilistic knowledgebase},
  author={Song, Yangqiu and Wang, Haixun and Wang, Zhongyuan and Li, Hongsong and Chen, Weizhu},
  booktitle={Proceedings of the twenty-second international joint conference on artificial intelligence-volume volume three},
  pages={2330--2336},
  year={2011}
}

@inproceedings{song2015open,
  title={Open domain short text conceptualization: A generative+ descriptive modeling approach},
  author={Song, Yangqiu and Wang, Shusen and Wang, Haixun},
  booktitle={Twenty-Fourth International Joint Conference on Artificial Intelligence},
  year={2015}
}

@inproceedings{gong2016representing,
  title={Representing verbs as argument concepts},
  author={Gong, Yu and Zhao, Kaiqi and Zhu, Kenny},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@inproceedings{chen-etal-2020-trying,
    title = "What Are You Trying to Do? Semantic Typing of Event Processes",
    author = "Chen, Muhao  and
      Zhang, Hongming  and
      Wang, Haoyu  and
      Roth, Dan",
    editor = "Fern{\'a}ndez, Raquel  and
      Linzen, Tal",
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.conll-1.43",
    doi = "10.18653/v1/2020.conll-1.43",
    pages = "531--542",
    abstract = "This paper studies a new cognitively motivated semantic typing task,multi-axis event process typing, that, given anevent process, attempts to infer free-form typelabels describing (i) the type of action made bythe process and (ii) the type of object the pro-cess seeks to affect. This task is inspired bycomputational and cognitive studies of eventunderstanding, which suggest that understand-ing processes of events is often directed by rec-ognizing the goals, plans or intentions of theprotagonist(s). We develop a large dataset con-taining over 60k event processes, featuring ul-tra fine-grained typing on both the action andobject type axes with very large (10{\^{}}3∼10{\^{}}4)label vocabularies. We then propose a hybridlearning framework,P2GT, which addressesthe challenging typing problem with indirectsupervision from glosses1and a joint learning-to-rank framework. As our experiments indi-cate,P2GTsupports identifying the intent ofprocesses, as well as the fine semantic type ofthe affected object. It also demonstrates the ca-pability of handling few-shot cases, and stronggeneralizability on out-of-domain processes.",
}

@article{HE2024104149,
title = {Acquiring and modeling abstract commonsense knowledge via conceptualization},
journal = {Artificial Intelligence},
volume = {333},
pages = {104149},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2024.104149},
url = {https://www.sciencedirect.com/science/article/pii/S0004370224000857},
author = {Mutian He and Tianqing Fang and Weiqi Wang and Yangqiu Song},
keywords = {Commonsense reasoning, Conceptualization, Language models, Neural networks},
abstract = {Conceptualization, or viewing entities and situations as instances of abstract concepts in mind and making inferences based on that, is a vital component in human intelligence for commonsense reasoning. Despite recent progress in artificial intelligence to acquire and model commonsense attributed to neural language models and commonsense knowledge graphs (CKGs), conceptualization is yet to be introduced thoroughly, making current approaches ineffective to cover knowledge about countless diverse entities and situations in the real world. To address the problem, we thoroughly study the role of conceptualization in commonsense reasoning, and formulate a framework to replicate human conceptual induction by acquiring abstract knowledge about events regarding abstract concepts, as well as higher-level triples or inferences upon them. We then apply the framework to ATOMIC, a large-scale human-annotated CKG, aided by the taxonomy Probase. We annotate a dataset on the validity of contextualized conceptualizations from ATOMIC on both event and triple levels, develop a series of heuristic rules based on linguistic features, and train a set of neural models to generate and verify abstract knowledge. Based on these components, a pipeline to acquire abstract knowledge is built. A large abstract CKG upon ATOMIC is then induced, ready to be instantiated to infer about unseen entities or situations. Finally, we empirically show the benefits of augmenting CKGs with abstract knowledge in downstream tasks like commonsense inference and zero-shot commonsense QA.}
}

@article{coutanche2019conceptual,
  title={Conceptual Combination in the Cognitive Neurosciences},
  author={Coutanche, Marc N and Solomon, Sarah and Thompson-Schill, Sharon L},
  year={2019},
  publisher={PsyArXiv}
}

@misc{hwang2021cometatomic2020symbolicneural,
      title={COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs}, 
      author={Jena D. Hwang and Chandra Bhagavatula and Ronan Le Bras and Jeff Da and Keisuke Sakaguchi and Antoine Bosselut and Yejin Choi},
      year={2021},
      eprint={2010.05953},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.05953}, 
}

@article{church-hanks-1990-word,
    title = "Word Association Norms, Mutual Information, and Lexicography",
    author = "Church, Kenneth Ward  and
      Hanks, Patrick",
    journal = "Computational Linguistics",
    volume = "16",
    number = "1",
    year = "1990",
    url = "https://aclanthology.org/J90-1003",
    pages = "22--29",
}

@article{michel2011quantitative,
  title={Quantitative analysis of culture using millions of digitized books},
  author={Michel, Jean-Baptiste and Shen, Yuan Kui and Aiden, Aviva Presser and Veres, Adrian and Gray, Matthew K and Google Books Team and Pickett, Joseph P and Hoiberg, Dale and Clancy, Dan and Norvig, Peter and others},
  journal={science},
  volume={331},
  number={6014},
  pages={176--182},
  year={2011},
  publisher={American Association for the Advancement of Science}
}


@article{wilkenfeld2001similarity,
  title={Similarity and emergence in conceptual combination},
  author={Wilkenfeld, Merryl J and Ward, Thomas B},
  journal={Journal of Memory and Language},
  volume={45},
  number={1},
  pages={21--38},
  year={2001},
  publisher={Elsevier}
}


@article{gagne2017conceptual,
  title={Conceptual combination, property inclusion, and the Aristotelian-Thomistic view of concepts},
  author={Gagn{\'e}, Christina L and Spalding, Thomas L and Kostelecky, Matthew},
  journal={Compositionality and concepts in linguistics and psychology},
  pages={223--244},
  year={2017},
  publisher={Springer International Publishing}
}

@article{swinney2007conceptual,
  title={Conceptual combination during sentence comprehension},
  author={Swinney, David and Love, Tracy and Walenski, Matthew and Smith, Edward E},
  journal={Psychological Science},
  volume={18},
  number={5},
  pages={397--400},
  year={2007},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@inproceedings{lin2022does,
  title={Does BERT know that the IS-a relation is transitive?},
  author={Lin, Ruixi and Ng, Hwee Tou},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={94--99},
  year={2022}
}

@article{wu2023plms,
  title={Do PLMs know and understand ontological knowledge?},
  author={Wu, Weiqi and Jiang, Chengyue and Jiang, Yong and Xie, Pengjun and Tu, Kewei},
  journal={arXiv preprint arXiv:2309.05936},
  year={2023}
}

@inproceedings{estes2020relevance,
  title={Relevance and feature accessibility in combined concepts},
  author={Estes, Zachary and Glucksberg, Sam},
  booktitle={Proceedings of the Twenty-first Annual Conference of the Cognitive Science Society},
  pages={149--154},
  year={2020},
  organization={Psychology Press}
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{wisniewski1998property,
  title={Property instantiation in conceptual combination},
  author={Wisniewski, Edward J},
  journal={Memory \& Cognition},
  volume={26},
  pages={1330--1347},
  year={1998},
  publisher={Springer}
}

@article{fleiss1971measuring,
  title={Measuring nominal scale agreement among many raters.},
  author={Fleiss, Joseph L},
  journal={Psychological bulletin},
  volume={76},
  number={5},
  pages={378},
  year={1971},
  publisher={American Psychological Association}
}

@inproceedings{hulpuș2019spreading,
  title={A spreading activation framework for tracking conceptual complexity of texts},
  author={Hulpuș, Ioana and {\v{S}}tajner, Sanja and Stuckenschmidt, Heiner},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3878--3887},
  year={2019}
}

@article{balota1988age,
  title={Age-related differences in lexical access, spreading activation, and simple pronunciation.},
  author={Balota, David A and Duchek, Janet M},
  journal={Psychology and Aging},
  volume={3},
  number={1},
  pages={84},
  year={1988},
  publisher={American Psychological Association}
}

@article{anderson1983spreading,
  title={A spreading activation theory of memory},
  author={Anderson, John R},
  journal={Journal of verbal learning and verbal behavior},
  volume={22},
  number={3},
  pages={261--295},
  year={1983},
  publisher={Elsevier}
}

@article{collins1975spreading,
  title={A spreading-activation theory of semantic processing.},
  author={Collins, Allan M and Loftus, Elizabeth F},
  journal={Psychological review},
  volume={82},
  number={6},
  pages={407},
  year={1975},
  publisher={American Psychological Association}
}

@inproceedings{toxicchat2023lin,
title = "{T}oxic{C}hat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-{AI} Conversation",
    author = "Lin, Zi  and
      Wang, Zihan  and
      Tong, Yongqi  and
      Wang, Yangkun  and
      Guo, Yuxin  and
      Wang, Yujia  and
      Shang, Jingbo",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.311",
    doi = "10.18653/v1/2023.findings-emnlp.311",
    pages = "4694--4702",
    abstract = "Despite remarkable advances that large language models have achieved in chatbots nowadays, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media contents, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark constructed based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference when compared to social media contents. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions.",
}

@article{tian2023macgyver,
  title={MacGyver: Are Large Language Models Creative Problem Solvers?},
  author={Tian, Yufei and Ravichander, Abhilasha and Qin, Lianhui and Bras, Ronan Le and Marjieh, Raja and Peng, Nanyun and Choi, Yejin and Griffiths, Thomas L and Brahman, Faeze},
  journal={arXiv preprint arXiv:2311.09682},
  year={2023}
}

@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}