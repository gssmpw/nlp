


\section{Results}
In this section, we examine how DBox supports learners in algorithmic programming and how they interact with the tool. \ms{We compared DBox with the baseline tool and analyzed the interaction effects between tool and problem type, as well as the ordering effect (e.g., DBox first or Baseline first). Overall, we didn't find significant interaction or ordering effects for most metrics. Therefore, we report only the differences between the two tools unless notable interactions or ordering effects were observed, which are analyzed in detail.}



\subsection{How does Decomposition Box help with algorithmic programming learning?}

We first compared the correctness scores of participants' test session submissions under both DBox and baseline conditions. As shown in Figure \ref{RQ1} (a), participants using DBox achieved significantly higher correctness scores than those in the baseline condition ($Coef.$=0.198, $p$<0.05), suggesting that practicing with DBox better prepared learners to transfer their skills to similar algorithmic challenges.

This finding aligns with participants' subjective perceptions. Figure \ref{RQ1} (b) shows that learners in the DBox condition reported significantly higher perceived learning gains ($Coef.$=2.250, $p$<0.001), higher confidence in solving similar problems ($Coef.$=2.333, $p$<0.001), more improvements in algorithmic thinking ($Coef.$=3.875, $p$<0.001), and greater self-efficacy ($Coef.$=3.042, $p$<0.001) compared to the baseline condition.



Interview analysis further highlighted that participants felt solving tasks independently during the learning session enhanced their perceived learning gains. Overcoming challenges on their own also boosted their confidence. In contrast, baseline participants felt their algorithmic thinking was underdeveloped due to easy access to complete solutions (e.g., via search, ChatGPT, or Copilot), leading to lower perceived learning and confidence. As P5 noted, ``\emph{Even though I couldn’t write the full solution, the tool [DBox] encouraged me to break down the problem. I started with what I knew, and the tool guided me through the rest. Decomposing the problem helped me structure my approach, and as I saw the step tree fill in correctly, I felt my algorithmic thinking improve, and I gained confidence in solving the problem.}''




% \subsection{RQ1: How does Decomposition Box help with algorithmic programming learning?}
% First, we compared the objective correctness scores of the codes submitted by participants during the test sessions under both the DBox and baseline conditions. As shown in Figure \ref{RQ1} (a), participants in the DBox condition achieved significantly higher correctness scores than those in the baseline condition ($p<0.05$), indicating that practicing algorithmic problems with DBox better equipped learners to transfer their practiced skills to similar algorithmic challenges.

% This result is consistent with participants' subjective perceptions of their learning outcomes. As illustrated in Figure \ref{RQ1} (b), students in the DBox condition reported significantly higher perceived learning gains, confidence in solving similar problems, improvement in algorithmic thinking, and self-efficacy compared to the baseline condition (all $p<0.001$).

% Our interview analysis further revealed that participants felt that independently solving tasks during the learning session enhanced their perceived learning gains. Successfully overcoming challenges on their own also boosted their confidence in tackling similar problems. In contrast, participants in the baseline condition expressed that their algorithmic thinking was insufficiently exercised since they could easily access complete answers (e.g., by looking them up, asking ChatGPT, or using Copilot). This led to lower perceived learning gains and reduced confidence in facing similar problems. For example, as P5 remarked, ``\emph{Even though I couldn't write the full solution, the tool encouraged me to break down the problem. I just started by writing out the steps I knew, and the tool guided me to complete my thought process. Decomposing the problem helped me form a structured approach, and as I saw the step tree gradually fill in and become correct, I felt my algorithmic thinking improved, and I gained the skills to solve the problem.}''




% First, we compared the objective correctness scores of the code submitted by participants during the test sessions in the DBox and baseline conditions. As shown in Figure \ref{RQ1}(a), participants in the DBox condition achieved significantly higher correctness scores than those in the baseline condition ($p<0.05$). This indicates that after practicing algorithmic problems with DBox, learners were better able to transfer the skills they had developed to similar algorithmic challenges.

% This result aligns with the students' subjective perceptions of their learning outcomes. As illustrated in Figure \ref{RQ1} (b), compared to the baseline condition, students in the DBox condition reported significantly higher perceived learning gains, confidence in solving similar problems, improvement in algorithmic thinking, and self-efficacy (all $p$<.001).

% Our analysis of the interview data revealed that participants felt that independently solving the tasks during the learning session enhanced their perceived learning gains. The success achieved through their own efforts also boosted their confidence in tackling similar problems. In contrast, in the baseline condition, participants felt that their algorithmic thinking was not adequately exercised, nor did they learn much problem-solving skills, as they could easily access complete answers (whether by looking them up, asking ChatGPT, or using Copilot). This led them to feel lower learning gains and less confidence when facing similar problems. For example, as P5 mentioned, "Although I knew I couldn't write the full solution, this tool encouraged me to break down the problem. I just needed to start by writing out the steps I knew, and the tool would guide me to complete my thought process. On one hand, decomposing the problem helped me begin to form a structured approach. As I watched the step tree gradually fill in and turn correct, I felt that my algorithmic thinking improved, and I acquired the skills needed to solve this problem."



\subsection{How does Decomposition Box affect learners’ perceptions and user experience?}



\begin{figure*}[htbp]
	\centering 
	\includegraphics[width=0.95\linewidth]{figures/RQ1_new.pdf}
	\caption{Effects on participants' learning outcomes: (a) Participants' correctness scores during the testing session, where they solved the problem independently. (b) Participants' self-reported metrics on their learning outcomes.}
	\label{RQ1}
        \Description{}
\end{figure*}


\begin{figure*}[htbp]
	\centering 
	\includegraphics[width=0.95\linewidth]{figures/perception.pdf}
	\caption{Participants' perceptions of the two conditions in their learning process.}
	\label{perception}
        \Description{}
\end{figure*}


\begin{figure*}[htbp]
	\centering 
	\includegraphics[width=0.95\linewidth]{figures/UX_new.pdf}
	\caption{Participants' cognitive load and user experience.}
	\label{UX}
        \Description{}
\end{figure*}


\subsubsection{Effects on Learners' Perceptions}

As shown in Figure \ref{perception}, participants in the DBox condition reported significantly higher cognitive engagement ($Coef.$=2.625, $p$<0.001), greater critical thinking ($Coef.$=3.375, $p$<0.001), and a stronger sense of achievement ($Coef.$=3.375, $p$<0.001) compared to the baseline condition. Conversely, those in the baseline condition felt their problem-solving process resembled more ``cheating'' ($Coef.$=-4.250, $p$<0.001). DBox was also rated as providing more appropriate assistance ($Coef.$=2.625, $p$<0.001) and being more useful for programming learning ($Coef.$=2.792, $p$<0.001). Interviews supported these results, with 19 participants noting that DBox allowed them to independently develop their thought process while offering just enough feedback to guide them. This stimulated high engagement in problem-solving, as they critically analyzed both the overall solution and each step. As P1 noted, ``\emph{When I hit a block, unlike other tools (referring to the baseline), DBox didn’t give me the answer outright, which forced me to think through the problem myself. Even with help, I still had to do most of the thinking.}''

In contrast, baseline participants using tools like ChatGPT, Copilot, or LeetCode often \textbf{bypassed independent thinking, focusing on comparing or copying provided answers}. Twelve out of 24 compared answers while coding, and eight simply copied solutions, leading to a lack of achievement and a sense of ``cheating''. As P16 (using ChatGPT) admitted, ``\emph{I tried to convert its provided code into my own, but I didn’t feel like it was truly my solution; there was no sense of achievement.}'' Besides, \textbf{excessive help in the baseline tools led participants to feel it was unhelpful for learning}. For example, P23 (who used LeetCode’s built-in solution) shared, ``\emph{I was stuck on a small part, but the solution showed the entire answer immediately. I memorized it, but later, writing the code felt more like repetition than actual learning.}'' P15 (using ChatGPT) added, ``\emph{ChatGPT explained the problem and gave the full code. While its solution seemed right, I realized I was just judging its correctness rather than improving my programming skills.}''

\ms{Moreover, we found an interaction effect between tool and problem type ($Coef.$=1.250, $p$<0.05) in perceived usefulness. Post-hoc analysis showed that DBox outperformed the baseline in both Binary Search ($t=6.159$, $p<0.001$) and Greedy problems ($t=9.273$, $p<0.001$). With DBox, there was no significant difference in perceived usefulness between the two problems ($t=0.294$, $p=0.771$). However, with the baseline, perceived usefulness was lower for the Greedy problem compared to Binary Search ($t=-2.755$, $p<0.05$). We found no significant interaction effects on correctness scores, with participants showing similar performance across the two problems using either DBox or the baseline. This suggests that the lower perceived usefulness of the baseline for the Greedy problem was not due to the problem being inherently more difficult. A likely explanation is that the Greedy problem requires higher planning and decomposition skills (i.e., breaking a complex problem into subproblems solvable by a greedy algorithm) and the baseline did not provide scaffolding to support this, leading to lower perceived usefulness.
}





% \subsection{RQ2: How does Decomposition Box affect learners’ perceptions and user experience?}

% \subsubsection{Effects on Learners' Perceptions}

% As shown in Figure \ref{perception}, results indicate that participants in the DBox condition exhibited significantly higher cognitive engagement ($p$<0.001) and employed significantly more critical thinking ($p$<0.001) compared to those in the baseline condition. They also reported a greater sense of achievement ($p$<0.001). In contrast, participants in the baseline condition felt that their problem-solving process resembled more ``cheating'' ($p$<0.001). Additionally, they found the assistance provided by DBox to be more appropriate ($p$<0.001) and considered DBox to be more beneficial for learning programming ($p$<0.001).

% Our interviews with participants supported these findings. Nineteen participants mentioned that DBox allowed them to independently develop their thought processes, providing only necessary feedback. This approach required them to fully engage in problem-solving, critically analyzing both the overall correctness and each individual step of their solutions. As P1 noted, ``\emph{When I hit a mental block, unlike the other tool (referring to the baseline), I couldn’t just get the answer directly, which forced me to think through the problem on my own. Even when I received help, the tool didn’t give me the answer outright; I still had to do most of the thinking myself.}''

% In contrast, participants in the baseline condition, who used tools like ChatGPT, Copilot, or searched for answers online (e.g., on LeetCode), were less inclined to think independently, focusing instead on understanding the provided answers. This required much less engagement and critical thinking. We observed that many participants (12 out of 24) in the baseline condition directly compared the given answers while writing code in the editor. In some cases (8 out of 24), they simply copied the provided code into the editor. This approach left users feeling that, while they may have technically solved the problem, they didn’t experience a sense of achievement and instead felt as though they had cheated. For instance, P16 (who used ChatGPT in the baseline condition) admitted, ``\emph{Even though I passed the test cases, it doesn’t feel like I solved the problem—it feels like ChatGPT did it. Even though I tried to convert ChatGPT’s code into my own, I still don’t feel like it was really my solution; there’s no sense of achievement at all.}''

% Participants also expressed frustration with the excessive assistance provided by existing tools, which they felt hindered their learning. P2 (using Copilot in the baseline condition) commented, ``\emph{I just typed a variable, and Copilot instantly suggested the entire code. My curiosity made me look at it, but sometimes, after seeing it, I realized I hadn’t had the chance to think through the problem myself. Other times, the code it suggested was completely different from what I had in mind, so I ended up spending time trying to understand it. The experience was frustrating and detrimental to my learning.}'' Similarly, P23 (who used LeetCode’s built-in solution panel in the baseline condition) shared, ``\emph{I only struggled with a certain part, but when I checked the solution, it gave me the full answer immediately. After seeing how to solve the part I was stuck on, I memorized the entire solution. Later, when I went back to the editor to write the code, it felt more like a process of repetition rather than actual learning.}'' P15 (using ChatGPT in the baseline) echoed this sentiment: ``\emph{ChatGPT is incredibly powerful—it explained the problem-solving approach and provided the full code. After reading it, I felt its solution was correct, so I went with it. But I realized that this wasn’t my thought process; I was just judging whether ChatGPT’s answer was correct. I don’t feel like my programming skills improved.}''




\begin{figure*}[htbp]
	\centering 
	\includegraphics[width=0.96\linewidth]{figures/usageNEW.pdf}
	\caption{Participants' three distinct types of system usage, each represented by a different color. We analyzed participants' interactions, including code editing, step tree editing, help-seeking, and five types of button clicks.}
	\label{usage}
        \Description{}
\end{figure*}



\subsubsection{Effects on Learners' User Experience}
\label{Effects_on_UX}
As shown in Figure \ref{UX}, participants in the DBox condition found the learning process more mentally demanding ($Coef.$=1.125, $p$<0.01) and reported putting in more effort ($Coef.$=1.625, $p$<0.001) than those in the baseline condition. This aligns with our expectations, as DBox requires learners to independently construct a step tree rather than merely providing solutions. Interestingly, there were no significant differences in frustration levels between the two conditions ($Coef.$=-0.375, $p$=0.305), suggesting that while DBox encouraged independent thinking and led to some failed attempts, the process of building the step tree did not cause excessive frustration. 
\ms{However, we found a significant ordering effect on frustration ($Coef.$=1.917, $p$<0.01). 
Post-hoc analysis showed no significant difference between the two tools when DBox was used first ($t$=1.186, $p$=0.248), but participants reported significantly higher frustration with the baseline when it was used first ($t$=2.491, $p$<0.05). 
One possible explanation is the ``learning effect'' of soliciting assistance -- using DBox first better prepared participants to request targeted assistance from the baseline. 
This aligns how participants expressed frustration with the unsolicited assistance from the baseline. 
For example, P2 (using Copilot) commented, ``\emph{I typed a variable and hit [Tab], and Copilot suggested the entire code. However, the suggestion was different from what I had in mind, and I ended up spending time trying to understand it, which was frustrating.}''}



We expected participants to find DBox less easy to use due to its more complex operations, but participants' perceived ease of use ($Coef.$=0.750, $p$=0.063) did not differ significantly between conditions. Interviews revealed that in the baseline condition, participants often had to switch between the editor and solution pages or spend time crafting precise prompts for ChatGPT. As P6 noted, ``\emph{I had to explain the problem and my understanding to ChatGPT, which was quite complex. I didn't just want to copy its answer, so I constantly did line-by-line comparison between ChatGPT-provided code and my code.}'' Moreover, participants reported significantly higher satisfaction ($Coef.$=3.125, $p$<0.001) and a greater willingness to use DBox for future programming learning ($Coef.$=2.042, $p$<0.001).

% In the exit survey on tool preference (DBox, Baseline, or Neither), 22 participants chose DBox for algorithmic programming learning, while two selected "Neither," and none preferred the Baseline.




% Participants gave their perceived difference between DBox and baseline tools in the interviews. Many preferred DBox for its structured, guided learning approach, which encouraged independent thinking and active engagement (P1-3, 5-13, 15-24). Unlike ChatGPT’s unstructured responses, DBox broke problems into steps, fostering better understanding and computational thinking (P2-9, 15, 17, 19, 20, 23). DBox also provided a greater sense of accomplishment by allowing incremental problem-solving rather than just giving solutions (P7-10, 18, 23). Additionally, DBox offered targeted advice on specific steps, while ChatGPT struggled in this area (P1, 3, 5-7, 9-10, 17). Participants using Copilot in the baseline condition expressed concerns about its negative impact on learning, noting it as more of a productivity tool that hindered independent thinking (P15, 19, 23). Although some appreciated its efficiency in reducing syntax errors, many criticized it for failing to promote deeper understanding or coding skill development (P21, 24).




% \subsubsection{Effects on Learners' User Experience}

% As shown in Figure \ref{UX}, participants in the DBox condition perceived the learning process as significantly more mentally demanding ($p$<0.01) and reported putting in significantly more effort ($p$<0.001) compared to the baseline condition. This aligns with our expectations, as DBox scaffolds learners to solve problems by having learners independently construct a step tree rather than simply presenting solutions. Furthermore, there were no significant differences in frustration levels between the two conditions, suggesting that while DBox required learners to work through the problem step by step with independent thinking, this analytical process did not lead to frustration.

% Surprisingly, there was no significant difference in perceived ease of use between the two conditions. We initially expected participants to find DBox less user-friendly, as baseline tools required only simple retrieval or inputting questions into ChatGPT, whereas DBox involved more complex operations to generate and refine the step tree. However, interviews revealed that participants approached both tools with a learning mindset. When using baseline tools, they often had to switch between the editor and the solution page to compare code, or spend time crafting precise questions for ChatGPT. As P6 (using ChatGPT in baseline) described, ``\emph{I had to explain the problem, my understanding, and the issue I was facing to ChatGPT, which was quite complicated. And I didn’t want to just paste ChatGPT’s answer into the editor, so I had to switch back and forth, comparing the differences line by line.}''

% Additionally, participants reported significantly higher satisfaction ($p$<0.001) and a greater willingness to use DBox for future programming learning ($p$<0.001). In the exit survey, we also gathered participants' preferences for learning algorithmic programming (they could choose between DBox, the baseline, or neither). Twenty-two participants preferred using DBox, while two chose "neither." These two participants expressed a preference for practicing programming without any assistance, even though DBox only provides minimal help. They preferred to solve problems entirely on their own.

% To explore these results further, we analyzed interview data to understand participants' perceptions of the differences between DBox and baseline tools. In the baseline condition, 18 participants used ChatGPT, and 10 used Copilot (4 of whom used both). The analysis revealed that DBox was generally preferred over ChatGPT for providing a more structured and guided learning experience that encouraged independent thinking and active engagement (P1-3, P5-13, P15-19, P22-24). Unlike GPT, which often required significant effort in crafting effective prompts and could overwhelm users with complete answers upfront, DBox broke down problems into steps, requiring users to think through each step, which fostered a better understanding and stronger coding habits (P2-4, P 5-9, P15, P17, P19, P20, P23). Participants appreciated DBox’s stability and reliability, noting that it offered more consistent and trustworthy feedback, in contrast to ChatGPT, which could occasionally be inaccurate or less dependable (P3, P17, P19). DBox also enhanced the sense of accomplishment, as solving problems incrementally felt more rewarding than simply receiving answers from ChatGPT (P7-10, P18, P23). The tool’s structured interaction design was seen as conducive to systematically learning complex concepts, while ChatGPT's unstructured approach made it harder to follow (P1, P4, P10-12, P22). Although ChatGPT was recognized for its versatility and ability to handle a wide range of tasks, this flexibility often required more effort to manage, making DBox a better fit for learning and problem-solving tasks (P1, P10, P21). Participants also noted that ChatGPT’s interactivity was limited, as it struggled to provide targeted advice on specific steps, whereas DBox offered a more focused and interactive learning experience (P1, P3, P5-7, P9-10, P17).

% For those who used Copilot in the baseline condition, many expressed concerns about its negative impacts on learning, viewing it more as a productivity tool than an educational aid. Participants like P15, P19, and P23 found Copilot intrusive and frustrating, as it often provided suggestions that hindered independent thinking and learning by offering too much unsolicited help. They noted that it encouraged laziness, similar to ChatGPT, but required even less thought (P3, P11, P15, P17). While Copilot was seen as useful for reducing repetitive work and improving efficiency in development contexts where quick code completion was needed (P1, P15, P16, P21), it lacked the ability to guide users through the thought process, making it less suitable for learning and more appropriate for productivity tasks (P17, P20, P24). Some participants, like P21 and P24, acknowledged its efficiency in writing code and reducing syntax errors, but criticized it for not helping with understanding or developing problem-solving skills. Overall, Copilot was viewed as enhancing coding efficiency but unsuitable for learning or teaching, as it did not promote deep thinking or the development of coding skills.




\subsection{How do learners interact with Decomposition Box and perceive the usefulness of different features?}
\label{actual_use}

\subsubsection{Learners' Overall Usage Patterns}

During the user study, we tracked participants' interactions with DBox, focusing on key actions such as code editing, step tree editing, help-seeking, and five main button clicks (e.g., From Editor to Step Tree, Check Step Tree, Check Match, Copy to Comments, and Run Code). These interactions are visualized in Figure \ref{usage}, and we analyzed the patterns in combination with interview data.

Students adopted varying approaches when using DBox. Eleven began by constructing the step tree interactively, while thirteen started by writing code directly. We identified three distinct usage patterns:

\colorbox{color2}{\textbf{Type 1: Building the step tree before writing code}.} Some participants (P6, 8, 9, 14, 16, 18, and 21) focused on constructing the step tree first, iteratively checking and refining it before moving on to code implementation. This approach enabled them to write code efficiently once the structure was finalized. As P21 noted, ``\emph{I used natural language to express my thoughts and verify them, and after a few iterations, I recognized valid ideas and wrote the code myself.}'' P6 added, ``\emph{Writing code from scratch is more challenging for complex problems, so I prefer starting with the step tree.}''

\colorbox{color3}{\textbf{Type 2: Using DBox for verification}.} Some participants (P7, 12, 17, 22, and 24) used DBox primarily to verify their code. They wrote code first, then used the ``From Editor to Step Tree'' feature to check correctness and get hints. P17 explained, ``\emph{I usually solve problems by writing code first. Describing each step in natural language doesn’t feel natural for me.}'' Similarly, P22 stated, ``\emph{I know the general direction, so I write code first and use the tool to verify correctness or catch edge cases.}''

\colorbox{color1}{\textbf{Type 3: Flexible switching between the two modes.}} Students like P1-5, 10, 11, 13, 15, 19, 20, and 23 alternated between coding and step tree construction, adjusting their approach based on confidence, familiarity with specific steps, and real-time coding challenges. For example, P19 stated, ``\emph{If I'm confident in certain steps, I code first and then convert it to steps for verification. If unsure, I verify my thought process before coding.}'' P2 added, ``\emph{For familiar problems, I code first and refine it with the step tree. For new problems, I outline my thoughts and break down the steps to ensure accuracy before coding.}'' P10 shared, ``\emph{Initially, I felt confident, but when I got stuck, I refined my understanding of a step in the step tree before continuing with the code.}''





\begin{figure*}[htbp]
	\centering 
	\includegraphics[width=\linewidth]{figures/finalsurvey3.pdf}
	\caption{Participants rated the features of DBox based on their firsthand experience during the experiment. In the questionnaire, they provided feedback from a first-person perspective (e.g., ``I think [feature] is useful'', rated on a 7-point Likert scale).}
	\label{finalsurvey}
        \Description{}
\end{figure*}


\subsubsection{Hint Usage and Problem-Solving Approach}
\label{hintusage}
\ms{We tracked hint usage frequency among all 24 participants, recording a total of 164 hints triggered. Only 32 instances (19.5\%) involved the ``reveal sub-step'' feature, showing that students mainly relied on simpler hints and did not exploit the system by repeatedly making errors. This feature reveals only one sub-step from the user's incorrect or missing step—leaving the rest for them to solve independently—and can only be triggered after repeated struggle. This approach maximally preserves the students' independent problem-solving process. It strikes a balance between fostering independent thinking and preventing students from becoming permanently stuck, which could otherwise lead to frustration or a loss of motivation to learn.

We did a qualitative analysis which revealed varied problem-solving approaches adopted by participants in the problem-solving processes. For the ``Can Jump'' problem, tackled by 12 participants, we observed three approaches: Greedy (7 participants), Dynamic Programming (3), and Recursion (2). The other 12 participants addressed the ``Search in Rotated Sorted Array'' problem, using Binary Search (8), Two Pointers (2), Binary Tree (1), and Divide and Conquer (1). We observed that participants using the same approach still exhibited differences in reasoning and coding styles. Despite these variations, DBox effectively adapted its support to align with their individual styles and reasoning processes.
}



% \subsubsection{Learners' Usage Patterns}

% During the user study, we logged participants' interactions with DBox, focusing on key actions such as code editing, step tree editing, help-seeking, and five types of button clicks (e.g., From Editor to Step Tree, Check Step Tree, Check Match, From Step Tree to Comment, and Run Code). These interactions are visualized in Figure \ref{usage}. We then analyzed their usage patterns in combination with the interview data.

% Overall, students demonstrated varying approaches in the initial stages of using the tool. Eleven students began by interactively constructing the step tree (describing their thought process in natural language), while thirteen students started directly by writing code. We identified three distinct types of system usage throughout this process:

% \textbf{Type 1: Building the step tree before writing code} (indicated by a green background). Some students, such as P6, P8, P9, P14, P16, P18, and P21, primarily focused on constructing the step tree. They frequently checked the status of the step tree and refined it based on feedback. Only in the final steps did they begin writing code. These students typically invested significant effort into building the step tree, allowing them to quickly write the corresponding code once the structure was complete. For example, P21 mentioned, "At first, I had no clear idea, so I used natural language to explore my thoughts and see if they were correct. After a few iterations, I started recognizing valid ideas and stopped relying on the tool, trying to write the code myself." P6 added, "I prefer to write down my thoughts first and then check the steps. Writing code from scratch is more challenging for complex problems."

% \textbf{Type 2: Using DBox for verification purposes} (indicated by a blue background). Other students, such as P7, P12, P17, P22, and P24, primarily used DBox to verify the correctness of their code. Their main interactions took place in the code editor, where they wrote code first and then used the "From Editor to Step Tree" feature to generate the step tree, focusing on checking correctness and receiving hints for each step. As P17 explained, "I usually start by writing code to solve problems. Describing my thought process in natural language for each step isn’t natural for me—I'm more comfortable using code for step-by-step thinking." Similarly, P22 noted, "I know the general direction and key points of this problem, so I write the code first and use the tool to check if it's correct or if I’ve missed any edge cases."

% \textbf{Type 3: Flexible switching between coding and step tree building} (indicated by an orange background). Some students, such as P1-P5, P10, P11, P13, P15, P19, P20, and P23, demonstrated a flexible approach by switching between code editing and step tree construction as needed. They adapted their interaction style based on the problem at hand, seamlessly transitioning between both modes to support their problem-solving process. For example, P13 initially tried writing code but, after encountering issues, switched to building the step tree, later returning to coding after refining the steps. Some students noted that their use of DBox depended on their confidence in the solution and the completeness of their thought process. P19 mentioned, "If I’m confident, I’ll write the code first and then convert it into steps to check. If I’m unsure, I do the reverse—I check my thought process first and then write the code." Others believed that their approach depended on their familiarity with the problem. As P2 explained, "If it’s a problem I’m familiar with, I stick to my usual pattern of writing code first, converting it into steps, and then refining and checking. For new problems, I start by outlining my thoughts, gradually breaking down the steps to ensure accuracy before moving on to coding."


% Through our contextual inquiry and interviews, we analyzed the usage patterns of the students.





% \subsubsection{Learners' Usage Pattern of ChatGPT}
% The user responses reveal a range of interactions with GPT, highlighting both positive and negative aspects of its use in learning and problem-solving. Many users rely on a direct copy-paste approach, either using GPT's output as their final solution or comparing it with their own work (P3, P4, P15, P17, P19, P20, P21, P23, P24). Some users, however, take a more reflective approach by verifying GPT's output against their own understanding, which can increase their confidence (P2, P3, P15, P16, P23). Despite this, there is a noticeable tendency for users to experience a decrease in independent thinking and a reduced sense of accomplishment, as they often bypass deeper engagement in favor of quicker solutions provided by GPT (P2, P15, P24). This reliance can lead to mental laziness, as users might become less inclined to solve problems on their own (P2, P7, P8, P10, P14). On the other hand, some users actively seek to understand GPT's logic and use it to enhance their learning, particularly when GPT offers step-by-step explanations (P1, P16, P18). However, challenges remain, such as the inconsistency in the level of detail provided by GPT in teaching scenarios (P17), the difficulty in crafting effective prompts (P1, P24), and the overall complexity of the learning process, which can still be cumbersome even with GPT’s assistance (P1).




% \subsubsection{Learners' Reactions to System Errors}
% As noted in our technical evaluation, the backend of DBox, powered by a GPT-4 model, is not flawless and occasionally makes judgment errors. For instance, it might incorrectly mark a correct step as wrong or vice versa. We were particularly interested in exploring whether learners could identify these AI errors and how these mistakes influenced their use of DBox. We also wanted to understand their perceptions and responses to such errors. Among our 24 participants, 16 encountered instances where the system made incorrect judgments. Based on their feedback, we identified several key insights.

% First, within this learning context, most students were able to recognize system errors after one or more attempts. While they might sometimes struggle to generate new ideas, they possess the ability to verify existing ones. Moreover, because DBox’s role is primarily evaluative—assessing the code or step trees constructed by the students rather than generating or recommending content—students had a certain degree of confidence in their own work. This reduced the likelihood of over-reliance on the system, a common issue with traditional recommendation systems. As a result, system errors generally did not have a significant impact.

% However, there were some negative effects. For example, when a correct step was wrongly judged as incorrect, students had to revisit their work, which could disrupt their thought process. In some cases, they made several revisions before realizing the error was with the system, not their solution. The extent to which these errors affected students depended largely on their confidence level. When students were uncertain, they were more easily influenced by the system. As P15 noted, ``\emph{If I know what the hint is saying and I know it's wrong, I just ignore the message. But if I don't fully understand it and I am not sure, I might try following the hint.}''

% Students responded to these errors in a variety of ways. The first common response was to \textbf{ignore the error}. Five participants mentioned that when they recognized the system had misjudged a step, they simply moved on. The second approach was to \textbf{focus on the content of the hint rather than the status indicator}. Even when the system's judgment was wrong, the hints could still provide useful guidance. As P4 explained, ``\emph{I focus on what the hint is trying to say. If it aligns with what I wrote, I ignore the error and continue using the hint for guidance, disregarding the status indicator.}'' The third response involved \textbf{running the code to verify the system’s judgment}. As P2 shared, ``\emph{I run the code. If I'm correct, I stick to my approach and ignore the system. If I'm wrong, I reconsider whether the system’s judgment might be valid.}'' The fourth response was to \textbf{recheck the step}. For example, P3 said, ``\emph{I rethink my approach and evaluate it again. Even if the system’s judgment is wrong, the process helps me eliminate potential errors.}'' Similarly, P23 noted, ``\emph{I get confused and recheck a few times. If it's still wrong, I just move on and continue writing my own code.}''

% Overall, while system errors cannot be entirely avoided at the current stage, the design of our system enables learners to quickly recognize when an error may have occurred. Additionally, learners demonstrated flexibility in employing various strategies to address these errors. Future work should focus on minimizing the impact of system errors on the learning process.



\subsubsection{Learners' Reactions to System Errors}
\ms{DBox, powered by the GPT-4o model, occasionally misjudged step statuses. During our user study, 24 participants triggered the ``check'' function (``Check Step Tree'' and ``From Editor to Step Tree'') 208 times, with 16 participants encountering 18 system errors (an 8.7\% error rate). Fourteen participants faced one error each, while two experienced two errors. These errors can be categorized into four types:

\begin{itemize}[leftmargin=0em] % Adjust left margin for a cleaner look

    \item \textbf{Type-1 (11 occurrences):} Misjudging correct steps as incorrect.  
    \begin{itemize}
        \item \textbf{Example:} A participant using a greedy approach stated, 
        \textit{``Use a greedy approach to minimize jumps.''}  
        GPT flagged this as incorrect due to insufficient detail on range expansion and jump counter updates. However, missing details does not necessarily mean the solution is incorrect.
    \end{itemize}

    \item \textbf{Type-2 (2 occurrences):} Flagging unnecessary steps as missing.  
    \begin{itemize}
        \item \textbf{Example:} GPT incorrectly required a check for single-element arrays, though the solution worked without it.  
    \end{itemize}

    \item \textbf{Type-3 (3 occurrences):} Overlooking subtle mistakes in seemingly correct steps.  
    \begin{itemize}
        \item \textbf{Example:} A participant's wrote a step ``Iterate through the array. If at any index \texttt{i}, \texttt{maxReach < i}, return false'', leading to errors with unreachable indices.  
        GPT failed to detect this error.  
    \end{itemize}

    \item \textbf{Type-4 (2 occurrences):} Missing crucial steps while DBox marking solutions as complete.  
    \begin{itemize}
        \item \textbf{Example:} A participant omitted a final return statement (\texttt{return true}), GPT still judged the solution as correct.  
    \end{itemize}

\end{itemize}
}






% DBox, powered by the GPT-4o model, occasionally misjudges the correctness of steps. Among the 24 participants, 16 experienced minor or significant system errors during the study。在遇到过system error的参与者中，其中14参与者只遇到了一次错误，2个参与者遇到两次错误。经过定性分析，在这全部18次错误中，第一种错误（11次）是把参与者写的一个正确的步骤误判为错误，第二种错误（2次）是误判参与者有missing step（实际上步骤是完整的），第三种错误（3次）是把参与者写的一个错误的步骤判断为正确，第四种错误（2次）是误判参与者步骤是完整的（实际上缺少一个步骤导致不能通过测试）。经过定性分析，我们发现，在11次第一种错误中，有8次是因为参与者对步骤的描述过于简单缺乏关键信息，有3次是因为参与者步骤中所使用的解题方法比较小众，不是所谓的常见解法。在2次Type 2错误中，GPT认为参与者缺少了一个步骤，但实际上这个步骤是不必要的。在3次Type 3错误中，参与者对步骤的描述中大部分是正确的，隐藏着小错误，GPT没有识别到。在2次Type 4错误中，参与者没有写处理函数返回值的步骤，但是GPT没有识别出来。





We then analyzed whether participants successfully identified the system errors and how they reacted to the errors. We found that all the 16 students who encountered system errors recognized these errors after one or more attempts. Since DBox evaluates rather than generates content, students remained confident in verifying their own work, which minimized over-reliance on the system. However, incorrect judgments, particularly when correct steps were flagged as wrong, could disrupt their thought process. The impact of these errors largely depended on the student's confidence. As P15 noted, “\emph{If I know it's wrong, I ignore the message. If I'm unsure, I might follow the hint.}''

Students adopted various strategies to deal with system errors. (1) \textbf{Ignoring the error} (8 participants): Some simply moved on after recognizing a misjudgment. (2) \textbf{Focusing on the hint, not the status} (6 participants): Even if the status evaluation was incorrect, participants often found the hints useful. \ms{For example, participants wrote very brief steps, which GPT flagged as incorrect. The hints prompted them to consider important details, and despite the incorrect judgment, participants found the hints very useful.} (3) \textbf{Running the code to verify} (12 participants): Many used the ``Run'' button to test their hypotheses. As P2 explained, ``\emph{If my code works, I stick with my approach. If it fails, I reconsider the system's judgment.}'' (4) \textbf{Rechecking the step} (10 participants): Some participants revisited a step to refresh its status. As P3 mentioned, ``\emph{Even if the system is wrong, rechecking helps me identify potential issues.}''

Though system errors are unavoidable at the current stage, DBox’s design allows learners to quickly recognize and manage them. Future improvements should aim to minimize disruptions caused by system inaccuracies.


% As noted in our technical evaluation, DBox's backend, powered by the GPT-4o model, is not flawless and occasionally makes errors in judgment, such as incorrectly marking a correct step as wrong or vice versa. We were particularly interested in how learners identified these AI errors, how they influenced DBox's use, and how students responded to them. Among our 24 participants, 16 encountered system errors, providing several key insights.

% Most students were able to recognize system errors after one or more attempts. While they sometimes struggled to generate new ideas, they were generally confident in verifying their existing work. Since DBox primarily evaluates rather than generates content, students maintained a level of trust in their own solutions, reducing over-reliance on the system—an issue common with recommendation tools. As a result, system errors typically did not significantly impact their learning experience.

% However, incorrect judgments could disrupt students' thought processes, particularly when correct steps were wrongly flagged as incorrect. In some cases, students made unnecessary revisions before realizing the error was with the system, not their solution. The extent of disruption depended on their confidence. As P15 noted, ``\emph{If I know what the hint is saying and I know it's wrong, I just ignore the message. But if I don't fully understand it and I am not sure, I might try following the hint.}''

% Students responded to system errors in various ways. The most common responses included:

% 1. \textbf{Ignoring the error}. Five participants mentioned simply moving on when they recognized a misjudgment.
% 2. \textbf{Focusing on the hint, not the status}. Even when the system’s judgment was wrong, the hints were often useful. As P4 explained, ``\emph{I focus on the hint and ignore the error if it aligns with my approach.}''
% 3. \textbf{Running the code to verify the system’s judgment}. As P2 shared, ``\emph{If my code runs correctly, I trust my approach and ignore the system. If it fails, I reconsider the system’s judgment.}''
% 4. \textbf{Rechecking the step}. P3 noted, ``\emph{I rethink my approach and recheck the step. Even if the system is wrong, the process helps me clarify potential errors.}''

% While system errors are inevitable at this stage, DBox's design allows learners to recognize and address them quickly. Participants demonstrated flexibility in employing strategies to mitigate the impact of these errors. Future work should focus on minimizing the disruption caused by system inaccuracies.







\subsubsection{Learners' Ratings of Different Features of DBox}
DBox offers a range of features designed to enhance algorithmic programming learning, most of which participants found helpful, as illustrated in Figure \ref{finalsurvey}. Students particularly appreciated the flexibility to either write code directly or build a step tree, and they valued DBox's ability to infer their thought process from the code. The interactive step tree and the fine-grained correctness assessment were also well-received, although two participants found it somewhat cumbersome. The progressive, multi-level hint system was praised by 22 participants. Additionally, the feature that checks the alignment between the step tree and the code implementation was seen as highly beneficial. However, some participants felt that the ability to paste the step tree as comments into the editor was unnecessary, since the step tree and code editor could already be viewed side by side. This feature could be even more useful if DBox were developed as a plugin in the future.

% Interviews revealed additional benefits. Participants appreciated the structured step-by-step guidance (P1-4, P6-8, P10, P12, P15, P16, P18, P19, P23) and found breaking problems into smaller steps helped identify issues in logic or code (P1-6, P7-12, P15, P16, P22, P24). This method also improved problem-solving habits, such as commenting and step planning (P19). Targeted hints effectively guided thought processes without overwhelming users (P3-5, P17, P20, P21, P23). Participants liked how DBox customized feedback based on progress, making learning more efficient and personalized (P4, P13, P14, P20, P21, P24). Overall, DBox supported logical thinking by providing incremental guidance and encouraging independent problem-solving.




% DBox includes several features designed to enhance programming learning, and we wanted to understand which ones students found most helpful. Figure \ref{finalsurvey} shows participants' ratings for each feature. Overall, most features were considered useful. Participants especially appreciated DBox's flexibility, allowing them to solve problems either by writing code or constructing a step tree. They also valued the system's ability to deduce their thought process from the code and provide relevant feedback. The interactive step tree and its correctness assessment were well-received, though two participants found working with the step tree cumbersome. Additionally, 22 participants praised the multi-level hint system. However, many found the feature that pastes the step tree into the editor as comments unnecessary, as they could easily compare the step tree and editor side by side. This feature might become more useful if DBox were developed as a plugin.

% Interviews revealed further benefits. Many participants appreciated the step-by-step guidance, which provided a structured and controlled learning experience (P1-4, P6-8, P10, P12, P15, P16, P18, P19, P23). The ability to break down problems into smaller, manageable steps was highlighted as a key feature, helping users identify specific issues in their logic or code (P1-6, P7-12, P15, P16, P22, P24). This approach made error detection easier and promoted better problem-solving habits, like commenting and step planning (P19). Targeted hints were seen as helpful for guiding thought processes without overwhelming users, allowing them to focus on areas needing improvement (P3-5, P17, P20, P21, P23). Participants also appreciated how the system customized feedback based on their progress and previous inputs, making learning more efficient and personalized (P4, P13, P14, P20, P21, P24). Overall, participants felt DBox effectively supported logical thinking and learning by offering incremental guidance and encouraging independent error correction.







% \subsubsection{Existing Limitations and Suggested Improvements}
% Participants identified several areas for improvement in DBox. Three participants expressed frustration when uncommon solutions were incorrectly flagged as wrong. Speed and accuracy issues with GPT-4's processing were also raised, along with concerns about GPT's misinterpreting of their vague inputs (four participants). Two participants suggested more support for initial steps and examples to help struggling users get started and emphasized the need for logical consistency across steps. Three participants expressed interest in auto-completion features and integrating conversational capabilities for them to ask follow-up questions.




% Participants identified several areas where the tool could be improved:

% \begin{itemize}
%     \item \textbf{Hint Quality and Clarity}: Some participants found the hints confusing or not particularly useful (P3, P15, P17, P19, P22, P23). They suggested making the hints more specific and clearer, with explanations for why certain steps are necessary and how to achieve specific outcomes (P19, P23). More targeted hints that directly address users' needs, rather than offering generic advice, were also recommended (P15, P17).
    
%     \item \textbf{Handling Non-Optimal Solutions}: Participants noted frustration when the tool incorrectly marked non-optimal solutions as wrong (P16). They recommended distinguishing between incorrect and non-optimal solutions and providing hints to guide users toward optimization, rather than flagging the solution as incorrect.

%     \item \textbf{Flexibility in Step Judgment}: Some participants requested more flexibility in the tool's judgment, allowing for different solution paths and thought processes (P19, P21). They wanted the tool to accommodate various approaches rather than adhering to a single correct method.

%     \item \textbf{Speed and Accuracy}: Several participants found the tool slow due to GPT-4’s processing time, which hindered their workflow (P3, P4). Faster processing would improve the overall experience. Additionally, there were concerns about the tool misinterpreting users' inputs, especially with minimal code or vague descriptions (P1, P2, P3, P20).

%     \item \textbf{Support for Initial Steps and Examples}: Some participants, particularly those struggling to start, felt the tool lacked adequate support for getting started (P18, P24). They suggested providing more detailed initial steps and examples to help build momentum.

%     \item \textbf{Logical Consistency and Conceptual Guidance}: Participants expressed a desire for the tool to not only provide step-by-step guidance but also provide a high-level integrated prompt for the entire problem and connect these steps into a coherent, logical framework (P6, P23). They wanted the tool to explain the underlying principles and ensure each step was logically connected.

%     \item \textbf{More Auto-completion}: While the tool focuses on having users create the step tree and write code themselves, some participants (P1, P15, P23) expressed interest in intelligent auto-completion features to help them focus on learning key concepts.

%     \item \textbf{Integrating Conversational Features}: Some participants (P2, P11, P15) wanted the ability to ask follow-up questions or describe difficulties. They suggested integrating a conversational feature where users could interact with GPT based on their current step tree or a specific step—without GPT revealing the answer but guiding them instead.
    
% \end{itemize}














