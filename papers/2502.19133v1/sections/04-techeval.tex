
\section{Technical Evaluation}
\label{techeval}






In DBox, the key feature is that learners construct a step tree using either code or natural language descriptions, while the LLM evaluates each step and provides necessary feedback. To assess whether effective prompt engineering enables the GPT-4o model (hereafter referred to as GPT or LLM) to accurately determine node statuses (i.e., correct, incorrect, or missing), we conducted a preliminary technical evaluation. Detailed prompts used are provided in the supplementary material.






\subsection{Dataset Creation} 
We created a dataset of learners' authentic thought errors to evaluate LLMs' ability to recognize the status of the thought process.
Based on GPT-4's performance on coding tasks~\cite{finnie2023my, savelka2023thrilled}, we selected 25 easy-level LeetCode problems covering various algorithms and data structures problems (e.g., dynamic programming, sorting, greedy algorithms).

To capture natural variations, we recruited five computer science students from a local university to manually create the step trees for five randomly selected problems (25 in total). 
Using correct code samples as references, annotators constructed a step tree based on their solution, including steps, substeps, and sub-substeps. 
They described each node in their own words and linked it to the relevant code.
After collecting the annotated step trees, we manually created various types of errors to simulate common student misconceptions in coding \cite{qian2017students}.
An algorithmic programming expert created seven error types for each problem (e.g., missing steps, incorrect step order, logical errors, and syntax errors), which were reviewed by another expert, resulting in 175 error-laden step trees (25 problems x 7 error types).
% \subsubsection{Procedure} We briefed participants on the annotation task's purpose and process, assigning 25 randomly selected problems among five students, with each solving five problems. Participants constructed a step tree for each problem based on their chosen solution approach, facilitated by providing correct code samples for various possible solutions. A step tree comprises steps, substeps, and even sub-substeps. Participants were tasked with constructing this tree according to their thought process, describing each node in their own words, and associating each node with the relevant code. We organized the problems, reference codes, and step tree components into a Google spreadsheet for annotation.

% \subsubsection{Error Generation} Once we collected the annotated step trees, we proceeded to generate potential errors reflecting possible thought process missteps. Since DBox handles both direct code inputs and natural language descriptions, we generated errors for both modalities.

% Drawing from common student misconceptions identified in previous studies \cite{qian2017students}, we defined three error types for natural language descriptions: missing steps, incorrect step order, and logical step errors. For code-based errors, we identified four types: missing steps, incorrect step order, logical step errors, and syntax errors. An expert in algorithmic programming constructed these errors for each problem's step tree, creating seven types of errors per problem, reviewed by another expert. This resulted in 175 error-laden step trees (25 problems x 7 error types).


% \subsection{Analysis Approach} For each error type, we categorized the steps into two parts: the correct part, consisting of steps with a correct status (labeled as 1), and the incorrect/missing part, consisting of steps with an incorrect or missing status (labeled as 0). To simplify the analysis and maintain a rigorous evaluation, we did not detail each step or substep within a task due to their potential complexity. Instead, we adopted a straightforward evaluation method: if all steps in the correct part are predicted as correct by GPT, the prediction is classified as 1; any incorrect prediction in this section classifies the overall prediction as 0. Conversely, if all steps in the incorrect/missing part are identified as incorrect or missing, the prediction is marked as 0; any deviation from this results in a prediction classified as 1. This method enables us to calculate classification metrics such as accuracy, F1 score, precision, recall, specificity, false positive rate, and false negative rate effectively.

% We removed the status fields from 175 error-containing step trees and input them into the LLMs for status prediction. We then compared the LLMs' predictions against the expert-annotated ground truth. For instances where LLM predictions were inaccurate, two authors independently performed open coding of GPTâ€™s outputs to identify error themes, causes, and the scenarios in which they occurred. After a detailed discussion and analysis, we consolidated our findings into thematic insights.

\subsection{Analysis Approach} We divide the steps into two parts based on the expert annotations: the correct part (1) or the incorrect/missing part (0). To calculate the performance of GPT, we use a very strict evaluation method: if all steps in the correct part are determined to be correct, the prediction of this part is marked as 1; otherwise, the prediction is 0. Similarly, if all steps in the incorrect/missing part are determined to be incorrect/missing, the prediction of this part is 0; otherwise, the prediction is 1. This approach enabled calculation of accuracy, F1 score, precision, recall, specificity, false positive rate, and false negative rate.
We removed status fields from 175 error-containing step trees and input them into GPT for prediction. The results were compared against expert-annotated ground truth. For incorrect predictions, two authors independently coded GPT's outputs to identify error themes and causes, later consolidated through discussion.



% \subsection{Results}
% As demonstrated in Table \ref{tab:technicalresult}, \textbf{GPT more effectively identifies students' thought processes when expressed directly through code rather than natural language descriptions}. This difference is attributed to the extensive code-based training data for GPT \cite{liu2024your}, contrasted with the relatively sparse data on natural language descriptions of thought processes. Additionally, GPT employs specialized post-evaluation mechanisms for code handling \cite{achiam2023gpt}, and students' descriptions often contain imprecise or non-standard terminology, leading to potential ambiguities and misunderstandings.

% For sequence change errors, GPT's accuracy is markedly lower from natural language descriptions, recording only 70\% accuracy and a 72\% F1 score. Misclassifications include 36\% of incorrect steps judged as correct (False Positive Rate, FPR = 0.36) and 24\% of correct steps judged as incorrect (False Negative Rate, FNR = 0.24). In contrast, code-based evaluations yield perfect accuracy and F1 scores of 100\%. For logical errors evaluated from natural language, accuracy dips to 88\% with an F1 score of 87\%, FPR of 8\%, and FNR of 16\%. However, when evaluated from code, these figures improve dramatically to 98\% accuracy and F1, with a minimal FPR of 4\% and an FNR of 0. Similarly, for missing steps or substeps, natural language evaluations show only 86\% accuracy and F1 score, with an FPR of 16\% and an FNR of 12\%. Code-based evaluations show better results at 92\% accuracy and F1 score, with both FPR and FNR at 8\%. Syntax error identifications maintain a steady 90\% accuracy and F1 score.

% Moreover, our analysis of GPT's output reveals \textbf{occasional modifications in the structure and content of the step tree}. Despite instructions to maintain the original structure and add nodes only for missing steps, discrepancies occur in how GPT segments steps and sometimes alters the students' original input.

% Additionally, a significant finding is that \textbf{GPT sometimes inaccurately judges students' approaches as incorrect if they deviate from commonly recognized solutions}. This occurred in five out of 25 tasks, suggesting that GPT's training may bias it towards optimal solutions commonly represented in its data. For instance, GPT judged sorting an array to find a missing number as unnecessary, incorrectly marking a divergent student approach as incorrect.

% In conclusion, while GPT excels in processing code-based inputs, it struggles with natural language, especially in identifying sequence change errors. Given the infrequency of such errors, GPT-4's capabilities are generally sufficient for assessing and guiding students' thought processes. As LLM technology advances, its potential for accurately understanding and guiding users' thought processes is expected to become more reliable and practical.


\begin{table*}[hbpt]
% the environment \color{blue} change all cell color
	\centering
	\caption{The technical evaluation of GPT-4o assesses its ability to identify the status of learners' steps. \textbf{Precision} refers to the proportion of steps correctly predicted as correct by GPT. \textbf{TPR} (True Positive Rate) measures the proportion of truly correct steps that GPT identifies correctly. \textbf{TNR} (True Negative Rate) reflects the proportion of truly incorrect/missing steps that GPT correctly predicts. \textbf{FPR} (False Positive Rate) indicates the proportion of incorrect/missing steps that GPT incorrectly predicts as correct. \textbf{FNR} (False Negative Rate) represents the proportion of correct steps that GPT incorrectly predicts as incorrect/missing.}
% 	~\glcomment{To check whether we should conduct such analysis for explanation effect. Or do ablation-like, compare with explanation without explanation?(with explanation, with DKE vs without DKE or with explanation, with DKE vs without DKE, with explanation)}}
	\label{tab:technicalresult}%
	\begin{small}
	\begin{tabular}{c | c c c c c c c}
	    \hline
	   % Hypothesis&	\multicolumn{4}{c|}{\textbf{H1}}& \multicolumn{4}{c}{\textbf{H2}} \\
	    % \textbf{Participants}&	\multicolumn{5}{c|}{\textbf{All}}& Post-hoc results\\
	    % \hline
	    \textbf{Error Type}&Accuracy&F1&Precision&TPR/Recall&TNR/Specificity&FPR&FNR\\
	    \hline
 \hline
\multicolumn{8}{l}{\textbf{Identify step/substep status from learners' natural language-based step descriptions}}\\
\hline    
	    \textbf{Sequence Changed}&0.70&0.72&0.68&0.76&0.64&0.36&0.24\\
	
 \rowcolor{gray!15}\textbf{Logical Error}&0.88&0.87&0.91&0.84&0.92&0.08&0.16\\
	    \textbf{Missing}&0.86&0.86&0.85&0.88&0.84&0.16&0.12 \\

 \hline
\multicolumn{8}{l}{\textbf{Identify step/substep status from learners' codes}}\\
\hline
    \rowcolor{gray!15}\textbf{Sequence Changed}&1.00&1.00&1.00&1.00&1.00&0.00&0.00\\

	\textbf{Logical Error}&0.98&0.98&0.96&1.00&0.96&0.04&0.00\\

 
    \rowcolor{gray!15}\textbf{Missing}&0.92&0.92&0.92&0.92&0.92&0.08&0.08\\


    \textbf{Syntax Error}&0.90&0.90&0.88&0.92&0.88&0.12&0.08\\
    % \textbf{TiA-Trust}& 3.93& .140& $3.00 \pm 0.81$& $3.22 \pm 0.87$& $3.11 \pm 0.85$& -\\
	    \hline
	\end{tabular}%
	\end{small}
\end{table*}

\subsection{Results}
As shown in Table \ref{tab:technicalresult}, \textbf{GPT more accurately identifies students' thought processes when expressed through code rather than natural language}. This difference may stem from GPT's extensive code-based training data \cite{liu2024your} and specialized code-handling mechanisms \cite{achiam2023gpt}, while natural language descriptions often include imprecise or non-standard terminology, leading to ambiguities \cite{liu2023wants}.

\textbf{GPT sometimes identifies incorrect steps as correct (false positives) or correct steps as incorrect (false negatives)}. For \emph{sequence change errors}, GPTâ€™s accuracy drops to 70\% with an F1 score of 72\% from natural language descriptions, with a False Positive Rate (FPR) of 36\% and a False Negative Rate (FNR) of 24\%. In contrast, code-based evaluations achieve 100\% accuracy and F1 scores. For \emph{logical errors} from natural language, GPTâ€™s accuracy is 88\% (F1 score 87\%, FPR 8\%, FNR 16\%), compared to 98\% accuracy and F1 scores from code-based evaluations (FPR 4\%, FNR 0\%). \emph{Missing step error} evaluations from natural language yield 86\% accuracy (FPR 16\%, FNR 12\%), improving to 92\% from code inputs (FPR/FNR 8\%). \emph{Syntax error} identification remains steady at 90\% accuracy and F1 score.

Additionally, we find that \textbf{GPT occasionally alters the structure and content of the step tree}, despite instructions to only add missing steps. It sometimes modifies how steps are segmented or misinterprets the student's original input. Another key finding is that \textbf{GPT sometimes incorrectly judges non-standard approaches as wrong}. In five out of 25 tasks, GPT mistakenly flagged correct solutions as incorrect simply because they deviated from the common approaches in its training data. For example, it marked a sorting-based solution as incorrect, even though it was correct, albeit not the most optimal approach.

\ms{In summary, GPT demonstrates strong capabilities in processing code-based inputs but faces challenges with natural language, particularly in detecting sequence changes. Our analysis of participants' logs from the user study revealed that most errors encountered were logical errors or missing steps, while errors involving sequence changes were relatively uncommon. This suggests that GPT is generally effective in evaluating students' thought processes during algorithmic programming learning. We recommend that researchers considering GPT for supporting learners in natural language programming carefully evaluate its limitations and conduct technical assessments to determine its suitability for their specific scenarios. We hope this technical evaluation serves as a valuable reference for similar future research.}




% \subsection{Results}
% As shown in Table \ref{tab:technicalresult}, overall, \textbf{GPT is more effective at identifying students' thought processes when they express their ideas directly through code rather than through natural language descriptions}. One reason for this is that GPT's training data includes a substantial amount of code \cite{liu2024your}, whereas there is likely much less data on verbal descriptions of thought processes. Second, GPT employs some special post-evaluation mechanisms when dealing with code \cite{achiam2023gpt}. Thirdly, students' natural language descriptions of specific steps in their thought processes are often unclear, using imprecise or non-standard terms that can lead to ambiguity and misunderstanding. For instance, we found that students' natural language descriptions are sometimes ambiguous and imprecise, which causes GPT to perform worse when recognizing steps compared to recognizing code. Additionally, when students provide overly simplistic descriptions of steps, such as failing to explicitly consider edge cases, GPT is more likely to incorrectly judge the status as wrong. These issues are also common in human instructional communicationâ€”informal language can indeed lead to misinterpretation.

% Specifically, when identifying sequence change errors, GPT performs poorly when judging from natural language descriptions. The accuracy is only 70\%, and the F1 score is 72\%. Among the incorrect steps, 36\% were wrongly identified as correct (FPR = 0.36), and among the correct steps, 24\% were misjudged as incorrect (FNR = 0.24). In contrast, when judging from code, both accuracy and F1 are 100\%, with no errors observed. For identifying logical errors, if judged from natural language descriptions, the accuracy is only 88\% and the F1 score is 87\%. Among the incorrect steps, 8\% were misidentified as correct (FPR = 0.08), and among the correct steps, 16\% were misjudged as incorrect (FNR = 0.16). When judged from code, the accuracy and F1 score reach 98\%, with only 4\% of the incorrect steps being misidentified as correct (FPR = 0.04), and none of the correct steps being judged as incorrect (FNR = 0). When identifying missing steps or substeps, the accuracy and F1 score are only 86\% when judged from natural language descriptions. Among the incorrect steps, 16\% were wrongly identified as correct (FPR = 0.16), and among the correct steps, 12\% were misjudged as incorrect (FNR = 0.12). When judged from code, the accuracy and F1 score are 92\%, with lower FPR and FNR (both 0.08). When identifying syntax errors, the accuracy and F1 score are both 90\%.

% Additionally, based on our analysis of GPT's returned step tree, we identified two other issues. First, \textbf{GPT occasionally changes the structure and content of the step tree}. When identifying the status from a student's natural language description of steps, we prompt GPT to evaluate the status of each node based on the student's step tree, allowing GPT to add nodes only in cases of missing steps while preserving the original structure of the step tree. Furthermore, we require GPT to retain the original input for each node so that we can match each node in GPT's returned step tree with the nodes in the UI. However, we found that GPT does not always strictly adhere to the student's input step tree. Specifically, there are instances where the step tree returned by GPT differs in how it segments steps compared to the student's segmentation. Additionally, GPT sometimes alters the student's original input.

% Second, one of our core objectives is to enable GPT to provide personalized support to students, continuing to develop the solution along the student's thought process. However, \textbf{there are instances where, if the student does not adopt the same approach as GPT, GPT will judge the student's approach as incorrect}. This occurred five times out of 25 tasks. We speculate that some tasks may have widely recognized optimal solutions, and GPT's training data is dominated by these solutions, leading GPT to classify non-optimal approaches used by students as incorrect. For example, in a problem where students are asked to find the missing number in an array, some students start by sorting the array, and GPT responds by saying that sorting is unnecessary, judging the step as incorrect.

% In summary, GPT performs well when determining the status of step trees based on code. However, due to limitations in the training data and the inaccuracies in users' language, GPT performs less effectively when judging the status of step trees based on natural language descriptions, particularly when identifying sequence change errors. That said, since sequence changes occur very infrequently, we believe that the current capabilities of GPT-4 are sufficient to assess students' thought processes and provide guidance. As LLMs continue to improve in the future, using them to understand and identify users' thought processes should become a straightforward and practical application.





