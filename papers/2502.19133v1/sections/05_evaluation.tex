
% \subsubsection{RQ1: Can our approach generate meaningful steps based on the current code solution provided by the users?}

% \begin{itemize}
%     \item If users' solution is correct, can our approach generate correct steps to map each code unit?
%     \item If users' solution is correct but incomplete, can our approach identify the correct steps as well as missing steps?
%     \item If users' solution contains incorrect code lines, can our approach identify the correct steps as well as missing steps?
% \end{itemize}
\section{User Study}




We conducted a user study to evaluate the effects of DBox, focusing on three questions:
\begin{itemize}
    \item \textbf{Q1}: How does DBox support algorithmic programming learning?
    \item \textbf{Q2}: How does DBox affect learners' perceptions and user experience?
    \item \textbf{Q3}: How do learners interact with DBox and perceive the usefulness of different features?
\end{itemize}

% \ms{
% \subsection{Target User Group}
% DBox targets highly motivated, self-regulated students. Using a scaffolding approach, it prioritizes learners' independent thinking by providing only essential support. DBox assumes students actively engage with the tool to improve decomposition skills rather than passively completing tasks. However, we acknowledge that users seeking shortcuts may bypass this by searching for answers online.
% }




\subsection{Conditions}
We conducted a within-subjects design to control for individual differences in programming abilities. Participants experienced two conditions in a randomly assigned order:


\begin{itemize}
    \item \textbf{DBox}: Participants solved problems using the proposed DBox.
    \item \textbf{Baseline}: Participants freely used any available tools (e.g., ChatGPT, Copilot, search engines, LeetCode, QA platforms) to reflect their real-world learning habits, with no restrictions on tool usage or combinations.
\end{itemize}






\subsection{Task and Materials}
In this experiment, participants solve problems from two distinct algorithm types. Each type includes a learning problem, where participants use DBox or baseline tools, and a test problem, solved independently without assistance.

We selected problems from the LeetCode problem bank based on several criteria: First, all problems were of medium difficulty, with an acceptance rate between 40\% and 50\% to ensure sufficient challenge. Second, GPT performs well on these problems. Third, the two algorithm types are distinctly different to avoid learning effects. Finally, the learning and test problems within each algorithm type require similar programming skills to avoid unfair comparisons due to differences in additional coding skills needed for each problem. Based on these criteria, we chose two algorithm types: Greedy and Binary Search. For Greedy, we selected ``Jump Game''\footnote{https://leetcode.com/problems/jump-game/description/} and ``Jump Game II''\footnote{https://leetcode.com/problems/jump-game-ii/description/}; for Binary Search, we selected ``Search in Rotated Sorted Array''\footnote{https://leetcode.com/problems/search-in-rotated-sorted-array/description/} and ``Search in Rotated Sorted Array II''\footnote{https://leetcode.com/problems/search-in-rotated-sorted-array-ii/description/}.

\ms{To help participants become familiar with or recall the algorithms used in the study, we provide them with lecture materials prior to the start of the study. The lecture materials for the two types of algorithms were sourced from GeeksforGeeks\footnote{Greedy: https://www.geeksforgeeks.org/introduction-to-greedy-algorithm-data-structures-and-algorithm-tutorials/}\footnote{Binary Search: https://www.geeksforgeeks.org/binary-search/}. These materials include an introduction to each algorithm, illustrated figures, and practical examples.}


\begin{figure*}[htbp]
	\centering 
	\includegraphics[width=0.9\linewidth]{figures/procedure.pdf}
	\caption{The procedure of our user study. \ms{To avoid learning effects, we used a counterbalanced design with four combinations: (1) DBox-Type1 → Baseline-Type2, (2) Baseline-Type1 → DBox-Type2, (3) DBox-Type2 → Baseline-Type1, and (4) Baseline-Type2 → DBox-Type1. For each combination, six participants were randomly assigned.}}
	\label{procedure}
        \Description{}
\end{figure*}

\subsection{Procedure}
\ms{As shown in Figure \ref{procedure}, obtaining participants' consent, we begin by explaining the study's objectives and procedure. We then familiarize participants with the algorithms they will be practicing using the provided lecture materials. Note that the lecture material was designed to help participants recap the key concepts of these two algorithms. Although the two problems are labeled as Binary Search and Greedy on the LeetCode platform, participants were not restricted to using these two specific approaches to solve the problems}. Afterwards, we administer a pre-test problem to assess their expertise. We also have participants rate their confidence in solving the problem on a 7-point Likert scale following \cite{jin2024teach}. \ms{After participants completed the task or indicated they could not proceed, the two authors (as experiment operators) assessed their solutions against pre-verified answers (with multiple solutions). Participants who solved the problem correctly were excluded, as it suggested higher expertise in the tested problem type. Following \cite{jin2024teach}, we also excluded participants who rated their confidence at 6 or above, as high self-confidence likely indicates less need for additional support. While self-reported confidence may not always align with actual ability, this criterion helps focus the study on the intended user group for our tool. We then provided participants with an interactive tutorial to familiarize them with DBox. The tutorial guides them through each view, button, and functionality of the tool. After the tutorial, they can explore the tool by solving an exercise problem, different from the two problem types in the main study.

Next, we assigned experimental conditions and problem types to participants with a counterbalanced design.} Within each problem type, one problem is randomly assigned in the learning session and the other in the testing session. In the learning session, participants use either DBox or baseline tools, during which they must successfully pass all test cases to proceed. They then complete an in-task survey about their perceptions and experience with the tool they just used. In the test session, participants solve problems without any tool assistance. After completing both problem types, they fill out a post-task survey, followed by a semi-structured interview.





\subsection{Participants}

We conducted a power analysis using G*Power \cite{faul2009statistical} for our two-condition within-subjects design. Assuming an effect size of $f = 0.6$ (moderate), $\alpha = 0.05$, and power of 0.8, we estimated a required sample size of 24 participants.

After IRB approval, we recruited 24 participants via emails and social media at local universities (10 female, 14 male, average age 23.5, SD = 1.7). The group included 16 undergraduates and eight graduate students, with majors in computer science (17), data science (3), electrical engineering (3), and mathematics (1). Most participants (18) coded weekly, six coded monthly, and 23 had used platforms like LeetCode. The 90-minute study compensated participants with 20 USD, equating to 13 USD/hour.






\subsection{Measurements}
Our measurements are summarized in Table \ref{tab:measurement}. To address \textbf{Q1} (effects on learning outcomes), we assessed correctness in the testing session \cite{kazemitabaar2023studying}, perceived learning gain \cite{zhou2021does}, confidence in solving similar problems \cite{hendriana2018role}, improvement in algorithmic thinking \cite{yaugci2019valid}, and self-efficacy \cite{tsai2019improving, yildiz2018digital}. For \textbf{Q2} (effects on perceptions and user experience), we measured cognitive engagement \cite{pitterson2016measuring}, critical thinking \cite{kamin2001measuring}, sense of achievement \cite{wiedenbeck2004factors}, feeling of cheating \cite{kazemitabaar2023studying}, perceived help appropriateness, usefulness \cite{davis1989perceived}, mental demand, effort, frustration \cite{hart2006nasa}, ease of use, satisfaction \cite{bangor2008empirical}, and future use intention \cite{holden2010technology}. For \textbf{Q3} (usage patterns and perceptions of DBox), we analyzed usage logs (e.g., clicks, edits, help-seeking), post-task feature ratings, and conducted semi-structured interviews to delve into participants' underlying reasons behind their perceptions, usage patterns, and reactions to AI errors. All questionnaires used a 7-point Likert scale.

% Most questions, aside from those specific to DBox features, were adapted from established sources such as the NASA Task Load Index \cite{hart2006nasa}, System Usability Scale \cite{brooke2013sus}, and Technology Acceptance Model \cite{holden2010technology}.








% \begin{itemize}
%     \item \textbf{Correctness Score}: The accuracy of learners' solutions in the test tasks. Following the approach used in \cite{kazemitabaar2023studying}, two independent researchers graded each submitted solution using a straightforward and consistent rubric, deducting 25\% for each major issue or missing component in the final submission, resulting in scores of 0\%, 25\%, 50\%, 75\%, or 100\%. The two graders fully agreed on 87.5\% of the submissions. For the remaining 12.5\% where disagreements arose, the graders discussed to resolve conflicts.
%     \item \textbf{Perceived Learning Gain}: "I have learned how to solve this type of problem." 
%     \item \textbf{Confidence in Solving Similar Problems}: "After solving this problem with the tool's help, I feel confident in tackling similar problems." 
%     \item \textbf{Algorithmic Thinking Improvement}: "This tool improved my ability to break down complex problems into smaller, manageable parts." 
%     \item \textbf{Self-Efficacy}: "I have mastered the problem-solving skills necessary for this type of problem." 
% \end{itemize}

% To address \textbf{RQ2: How does Decomposition Box affect learners’ perceptions and user experience?}, we measured the following factors:
% \begin{itemize}
%     \item \textbf{Cognitive Engagement}: "I was cognitively engaged in the programming exercises." 
%     \item \textbf{Critical Thinking}: "The learning process challenged me to think critically." 
%     \item \textbf{Sense of Achievement}: "I feel a sense of accomplishment/achievement when I complete the programming task." 
%     \item \textbf{Sense of Cheating}: "Using this tool feels like cheating." 
%     \item \textbf{Perceived Appropriateness of Help}: "I felt I received the right amount of help when needed—neither too much nor too little." 
%     \item \textbf{Perceived Usefulness}: "This tool is useful for learning how to solve specific problems." 
%     \item \textbf{Mental Demand}: "How mentally demanding was the task?" 
%     \item \textbf{Effort}: "How hard did you have to work to achieve your level of performance?" 
%     \item \textbf{Frustration}: "How insecure, discouraged, irritated, stressed, and annoyed were you?" 
%     \item \textbf{Ease of Use}: "I find this tool easy to use for learning algorithms." 
%     \item \textbf{Satisfaction}: "I am satisfied with the overall learning experience using this tool." 
%     \item \textbf{Future Use}: "I would like to use this tool in my future programming learning." 
% \end{itemize}




% 1. I have learned how to solve this type of problems.
% 2. I was cognitively engaged in the programming exercises.
% 3. The learning process challenged me to think critically.
% 4. After solving this problem with the help, I feel confident in dealing with similar problems.
% 5. I feel a sense of accomplishment/achievement when I complete the programming task.
% 6. Using this tool feels like cheating
% 7. I feel this task was NOT done by myself.
% 8. I felt I received the right amount of help when needed, which is not too much or too few.
% 9. This tool is useful for learning how to solve certain problems. 
% 10. I have mastered the problem-solving skills necessary for this problem.
% 11. This tool improved my ability to break down complex problems into smaller, manageable parts. 
% 12. How mentally demanding was the task?
% 13. How hard did you have to work to accomplish your level of performance?
% 14. How insecure, discouraged, irritated, stressed, and annoyed were you?
% 15. I find this tool easy to use for learning algorithm.
% 16. I am satisfied with the overall learning experience of using this tool.
% 17. I would like to use this tool in my future programming learning.



\renewcommand{\arraystretch}{1.5}
\begin{table*}[htp]  

\centering  
\fontsize{8}{8}\selectfont  

\caption{Measurements used in our user study. For the questionnaire items (within the quotation marks), a 7-point Likert scale was used, with 1 indicating ``Strongly disagree/Very low'' and 7 indicating ``Strongly agree/Very high''.}\label{tab:measurement}

\begin{tabular}{m{0.5cm}<{\centering}m{3cm}<{\centering}m{10.5cm}}
\toprule
\textbf{}&\textbf{Metrics}&\textbf{Detailed Meaning and Questions}\\ \hline\hline


\multirow{6}*{\shortstack{\textbf{Q1}}}&Correctness Score & The correctness of learners' test task solutions was evaluated using a consistent rubric from \cite{kazemitabaar2023studying}. Two authors independently graded submissions, deducting 25\% for each major issue or missing component, yielding scores of 0\%, 25\%, 50\%, 75\%, or 100\%. They agreed on 87.5\% of submissions, resolving disagreements through discussion for the rest.\\
\cline{2-3}
&\cellcolor{gray!15}Perceived Learning Gain &\cellcolor{gray!15} "I have learned how to solve this type of problem." \\ 
\cline{2-3}
&Confidence in Solving Similar Problems & "After solving this problem with the tool's help, I feel confident in tackling similar problems." \\ 
\cline{2-3}
&\cellcolor{gray!15}Perceived Algorithmic Thinking Improvement & \cellcolor{gray!15}"This tool improved my ability to break down complex problems into smaller, manageable parts." \\ 
\cline{2-3}
&Self-Efficacy & "I have mastered the problem-solving skills necessary for this type of problem." \\ 
\hline

\multirow{12}*{\shortstack{\textbf{Q2}}}&\cellcolor{gray!15}Cognitive Engagement &\cellcolor{gray!15} "I was cognitively engaged in the programming exercises." \\
\cline{2-3}
&Critical Thinking & "The learning process challenged me to think critically." \\
\cline{2-3}
&\cellcolor{gray!15}Sense of Achievement &\cellcolor{gray!15} "I feel a sense of accomplishment/achievement when I complete the programming task." \\
\cline{2-3}
&Sense of Cheating & "Using this tool feels like cheating." \\
\cline{2-3}
&\cellcolor{gray!15}Perceived Appropriateness of Help &\cellcolor{gray!15} "I felt I received the right amount of help when needed—neither too much nor too little." \\
\cline{2-3}
&Perceived Usefulness & "This tool is useful for learning how to solve specific problems." \\
\cline{2-3}
&\cellcolor{gray!15}Mental Demand &\cellcolor{gray!15} "How mentally demanding was the task?" \\
\cline{2-3}
&Effort & "How hard did you have to work to achieve your level of performance?" \\
\cline{2-3}
&\cellcolor{gray!15}Frustration &\cellcolor{gray!15} "How insecure, discouraged, irritated, stressed, and annoyed were you?" \\
\cline{2-3}
&Ease of Use & "I find this tool easy to use for learning algorithms." \\
\cline{2-3}
&\cellcolor{gray!15}Satisfaction &\cellcolor{gray!15} "I am satisfied with the overall learning experience using this tool." \\
\cline{2-3}
&Future Use & "I would like to use this tool in my future programming learning." \\
\hline


\multirow{6}*{\shortstack{\textbf{Q3}}}&\cellcolor{gray!15}Button Clicking &\cellcolor{gray!15} (with timestamp) From Editor to Step Tree, Check Step Tree, Check Match, From Step Tree to Comments, Run Code\\
\cline{2-3}
&Editing & (with timestamp) Code edit, step tree edit\\
\cline{2-3}
&\cellcolor{gray!15}Help-Seeking &\cellcolor{gray!15} (with timestamp) see general hint, see detailed hint, and reveal step/code\\
\cline{2-3}
&Usefulness Rating &Participants' ratings on the usefulness of various features in DBox using a 7-point Likert scale\\
\cline{2-3}
&\cellcolor{gray!15}Interviews &\cellcolor{gray!15} Participants' detailed reasons for their perceptions, reactions to AI errors, and self-reported usage patterns, etc.\\

\bottomrule
\end{tabular}
\end{table*}




\subsection{Data Analysis}
\ms{To eliminate the unfair comparison caused by the learning effect of participants using both tools to solve the same type of problem, we selected two distinct problem types. We implemented a randomization procedure to ensure that each participant used either DBox or the baseline tool in a random order, with a randomly assigned problem type for each tool. As a result, each participant used only one tool to solve one problem.

For the quantitative analysis, we employed a linear mixed effects model to analyze the data. The dependent variables (DVs) were our outcome measures (e.g., scores or questionnaire ratings). First, we analyzed the main effect of the two different tools (the coefficient and p-value were reported based on this analysis). Then, we examined the interaction effects between the learning tool and problem type (\emph{Tool*Problem Type}), as well as the interaction effect between the learning tool and the order of tool usage (\emph{Tool*Order}). The fixed effects in the models included the learning tool, problem type, and the order of tool usage, while the random effect accounted for individual differences between participants.

}

For the qualitative analysis of our semi-structured interview data, grounded in the designed questions, we conducted a thematic analysis \cite{hsieh2005three}. Two authors independently coded the data, developed a codebook, and reached a consensus through discussion. In the results, we present key themes supported by representative participant quotes.


% For the quantitative analysis of the questionnaire data and correctness scores, we first conducted a Shapiro–Wilk test to assess normality. Since most of the questionnaire data did not follow a normal distribution, we applied the Wilcoxon signed-rank test, reporting the test statistic ($Z$), significance levels ($p$), and effect sizes ($r$). Additionally, descriptive statistics were used to summarize participants' usage logs.
