\section{Related Work}

% \subsection{Scaffolding Code Writing for Learners}

% \begin{itemize}
%     \item Parson's problems
%     \item Subgoal labels
% \end{itemize}
% \AW{We can include Parson's problems .}

% \subsection{Conversational Assistant for Programming Learners}

% \subsection{Decomposition in Computer Science}
% \begin{itemize}
%     \item The principle of Decomposition is the basis of many important concepts in CS
%     \item Decomposition is also related to computational thinking and problem solving.
% \end{itemize}


\subsection{Scaffolding Programming Learning}

% Scaffolding is a technique for bridging the gap between a learner’s current skill set and their desired skill set by providing guidance from knowledgeable sources, such as instructors and instructional designers \cite{kim2011scaffolding}. Scaffolding is a fundamental component of constructivism-based pedagogies \cite{pea2018social}, which posit that people construct knowledge by integrating new information into existing cognitive structures, rather than simply being told what to know \cite{cole1978mind, wood1976role}. Learners with limited prior knowledge often require instructional support to build upon their understanding and prevent floundering \cite{hmelo2007scaffolding, schmidt2007problem}. Much of the current research in constructivist-based learning environments is devoted to exploring the appropriate types and levels of guidance, including scaffolding, to facilitate learning \cite{tobias2009constructivist}.

% Scaffolding can be employed in various methods across different domains, from providing hints about the next steps in problem-solving to helping learners recognize conceptual errors that lead to misconceptions \cite{pea2018social}. Due to its broad applicability, the term "scaffolding" has many definitions, particularly within the context of problem solving \cite{kim2011scaffolding, pea2018social, cole1978mind, wood1976role}.

% Historically, scaffolding research has focused on support provided by humans, such as instructors or tutors. However, recent studies have increasingly explored the provision of scaffolding through technological means \cite{delen2014effects, kim2011scaffolding}. Humans naturally offer adaptive scaffolding, tailoring their guidance based on an evolving understanding of a learner’s knowledge and progress \cite{azevedo2018using}. In adaptive scaffolding, feedback from learners allows scaffolders to adjust their instructional strategies to match the learner’s knowledge level \cite{yelland2007rethinking}. This adaptive interaction poses a challenge for creating technology that can provide similar support, as it traditionally requires the scaffolding system to interpret the learner’s knowledge through mechanisms like a cognitive model in an intelligent tutoring system \cite{aleven2002effective, conati2000toward, vanlehn2011relative}. However, with advancements in large language models (LLMs), it is now feasible to use LLMs to infer a learner's knowledge and mental state based on learners' inputs, thereby offering learner-adaptive assistance.

% However, existing works utilizing LLMs for scaffolding are primarily focused on code explanation generation \cite{macneil2023experiences} and next step hint generation \cite{roest2024next}, with few exploring how to design scaffolding tailored to different students during the process of algorithmic programming learning. In this paper, we aim to cultivate students' Zone of Proximal Development (ZPD) in algorithmic programming learning by employing skillfully crafted prompts for LLMs to implement the scaffolding teaching method. Specifically, we capture students' thought processes through two modes—code and verbal description—forming a step-tree structure that reflects their current thinking. Based on this structure, we provide targeted assistance for specific issues faced by students, which includes evaluating each node of the step-tree and guiding through three different levels of intervention for any errors or missing nodes. The entire process constitutes a scaffolding approach where we match the help to the students' current knowledge and thought process. During this process, we provide just the necessary and appropriate guidance, encouraging students to integrate the help from the system into their existing cognitive structures and engage in more independent thinking, thereby enhancing learning outcomes.



% Scaffolding is a vital constructivism-based technique that bridges the gap between a learner's current abilities and their desired skill set by leveraging guidance from experts like instructors and instructional designers \cite{kim2011scaffolding}. This approach is rooted in the constructivist theory, which suggests that knowledge is constructed through the integration of new information into existing cognitive frameworks, rather than merely receiving information passively \cite{pea2018social, cole1978mind, wood1976role}.Learners with limited knowledge particularly benefit from scaffolding to enhance understanding and avoid floundering \cite{hmelo2007scaffolding, schmidt2007problem}. Current research is largely focused on determining the optimal types and levels of guidance necessary to support effective learning \cite{tobias2009constructivist}.

Scaffolding, rooted in constructivist theory, helps bridge the gap between learners' current abilities and desired skills by integrating new information into existing cognitive frameworks, rather than passively receiving it \cite{kim2011scaffolding, pea2018social, cole1978mind, wood1976role, tobias2009constructivist}.
Learners with limited knowledge particularly benefit from scaffolding to enhance understanding and avoid floundering \cite{hmelo2007scaffolding, schmidt2007problem}. 
In programming education, scaffolding helps structure the learning process, guiding learners through problem-solving via hints or correcting misconceptions \cite{kim2011scaffolding, sykes2010design}.
%Scaffolding is a versatile technique used across various fields to guide learners through problem-solving processes, offering hints or correcting misconceptions \cite{pea2018social, kim2011scaffolding, cole1978mind, wood1976role}. Its application in programming education, where it supports learners by structuring the learning process, is an area of active research. 
Traditional scaffolding often relies on human-provided support, but recent technological advancements have expanded its scope to include digital solutions \cite{delen2014effects, kim2011scaffolding}. For example, methods such as using flowcharts to brainstorm and organize solution ideas have been shown to improve algorithm design and programming skills \cite{smetsers2017problem}. Similarly, Cunningham et al. describe a multi-stage programming process that includes explicit planning, offering a structured approach to learning \cite{cunningham2021avoiding}. 
Generally, providing immediate assistance during code writing—such as detailed feedback on errors, suggestions for corrections, or next-step hints—illustrates key scaffolding strategies that effectively enhance learner understanding \cite{singh2013automated, sykes2010design, rivers2017data}. Other supportive methods include the use of worked examples \cite{wang2022exploring} and Parsons problems, which engage students in active problem-solving \cite{hou2022using}.

Adaptive scaffolding, which adjusts support based on real-time feedback from learners, is particularly effective. This approach tailors instruction to the learner’s evolving understanding and knowledge level, though it poses significant challenges for tools that require cognitive models to interpret learner input \cite{aleven2002effective, conati2000toward, vanlehn2011relative}.
Recent advancements in large language models (LLMs) have introduced opportunities for adaptive scaffolding by inferring learner's mental state from their inputs, enabling adaptive, learner-specific assistance. \emph{However, most LLM applications in scaffolding have been limited to generating code explanations and next-step hints \cite{macneil2023experiences, roest2024next}, with little focus on adaptive scaffolding tailored to individual needs during algorithmic programming}. Our research addresses this gap by using LLMs to enhance the Zone of Proximal Development (ZPD) \cite{chaiklin2003zone} in algorithmic programming through carefully crafted prompts. We capture students' thought processes in both code and verbal forms, forming a step-tree structure that reflects their current thinking. This model facilitates targeted assistance, assessing and intervening at various levels for each identified error or gap. By ensuring that scaffolding aligns with the learner's current knowledge state and providing only essential guidance, our approach promotes independent thinking and substantially improves learning outcomes.

% Scaffolding is versatile, used across various fields to offer hints or correct misconceptions by leading learners through problem-solving processes \cite{pea2018social}. Its definition varies widely, especially in contexts involving problem-solving \cite{kim2011scaffolding, pea2018social, cole1978mind, wood1976role}.


% Supporting learners in programming has been an area of active research, with various scaffolded approaches being explored. Traditionally, scaffolding research has concentrated on human-provided support. Yet, recent advancements increasingly consider technological scaffolding solutions \cite{delen2014effects, kim2011scaffolding}. For instance, pre-coding support methods, such as using flowcharts to brainstorm and organize solution ideas, have been shown to improve algorithm design and programming skills \cite{smetsers2017problem}. Additionally, Cunningham et al. \cite{cunningham2021avoiding} described a multi-stage programming process that includes explicitly arranging plans. On the other hand, immediate assistance during code writing is another critical scaffolding strategy. This can include providing detailed feedback based on the student’s current code, such as explanations of errors \cite{singh2013automated}, suggestions for corrections \cite{sykes2010design}, or next-step hints to guide the student towards their goal \cite{rivers2017data, rivers2017data}. Other supportive methods include offering a library of worked examples \cite{wang2022exploring} or Parsons problems that students can actively solve \cite{hou2022using}. One desired scaffolding is adaptive scaffolding.


% Humans can naturally provide adaptive scafolding based on an evolving understanding
% of the learner’s knowledge and progress (Azevedo, 2005). In adaptive scafolding, input from learners helps scafolders to adjust their instruction to match the
% learner’s knowledge level (Yelland & Masters, 2007). However, adaptive scaffolding, where instruction is tailored based on real-time feedback from learners, presents challenges for technology-based solutions, as these typically require interpreting a learner’s knowledge through cognitive models within intelligent tutoring systems \cite{aleven2002effective, conati2000toward, vanlehn2011relative}. 


% Recent advancements in large language models (LLMs) now allow for the inference of a learner's mental state from their inputs, thus facilitating adaptive, learner-specific assistance. Despite these advancements, most LLM applications in scaffolding focus on generating code explanations and next-step hints, with limited exploration of adaptive scaffolding tailored to individual learner needs during algorithmic programming \cite{macneil2023experiences, roest2024next}. Our paper addresses this gap by employing LLMs to foster the Zone of Proximal Development (ZPD) in algorithmic programming through skillfully crafted prompts. We capture students' thought processes in both code and verbal forms, creating a step-tree structure that reflects their current thoughts. This structure guides our targeted assistance, which includes evaluating and intervening at various levels for each node, depending on the errors or gaps identified. Our approach ensures that scaffolding is matched to the learner's knowledge state, providing only the essential guidance needed. This method encourages learners to incorporate external assistance into their cognitive structures, promoting independent thinking and enhancing overall learning outcomes.




% In teaching algorithmic programming, instructors employ various scaffolding strategies to help students grasp complex concepts. Common approaches include problem decomposition, where complex problems are broken down into manageable parts to reduce cognitive load, and worked examples that provide step-by-step demonstrations, bridging the gap between theory and practice. Conceptual and strategic hints guide students without giving direct answers, promoting critical thinking. Interactive exercises and adaptive feedback systems further support active learning by offering real-time practice and personalized guidance. Additionally, collaborative learning environments encourage peer instruction, helping students articulate their understanding while learning from others. However, this reliance on teachers and peers can be costly, making one-on-one teaching difficult to achieve, and it may not be suitable for independent learning scenarios.

% In response, the last decade has seen the development of numerous Automated hint generation systems. Automated hint generation aims to assist learners by providing guidance that encourages independent problem-solving while preventing frustration. Various approaches have been developed, ranging from rule-based systems to advanced data-driven methods. A systematic review of automated feedback in programming exercises outlines the diversity of techniques employed, such as abstract syntax trees (AST) for generating next-step hints by comparing learner code with correct solutions [Automating Next-Step Hints Generation Using ASTUS]. Additionally, data-driven program synthesis has been employed to generate hints that address vast solution spaces, enhancing the personalized guidance learners receive [Data-driven hint generation in vast solution spaces: a self-improving python programming tutor]. AI-enhanced approaches, such as the decision-theoretic model for tutorial actions, predict the most beneficial hints by analyzing the learner's current state and potential future actions [Looking Ahead to Select Tutorial Actions: A Decision-Theoretic Approach]. These methodologies not only address immediate learning obstacles but also optimize the overall learning trajectory.



% Despite these advancements, hint generation systems face several challenges. The effectiveness of hints is highly dependent on their timing, specificity, and relevance. Poorly designed hint systems can lead to over-reliance, diminishing learners' ability to solve problems independently, and may even encourage counterproductive behaviors like guessing [When does scaffolding provide too much assistance? A code-tracing tutor investigation; The Andes Physics Tutoring System: Lessons Learned]. Moreover, excessive or poorly timed hints in educational games have been found to negatively impact performance by reducing engagement and challenge [Hint systems may negatively impact performance in educational games]. This issue is compounded by students' difficulty in recognizing when they need help, often leading to either underutilization or over-reliance on hints [Limitations of student control: do students know when they need help?]. To address these issues, recent work has explored the integration of human expertise with AI systems, such as the "Promptiverse" framework, which uses a human-AI hybrid approach to generate more contextually aware hints [Promptiverse: Scalable Generation of Scaffolding Prompts Through Human-AI Hybrid Knowledge Graph Annotation]. Additionally, adaptive systems that utilize data such as eye-tracking and learner behavior to dynamically adjust hint difficulty are being developed to ensure hints are appropriately challenging and timely [Dynamically adapting an AI game engine based on players’ eye movements and strategies; IRT-Based Adaptive Hints to Scaffold Learning in Programming]. These developments underscore the ongoing efforts to create hint systems that are not only effective but also supportive of long-term learning objectives in programming education.

% In this paper, building on existing scaffolding theories in the field of programming learning, we aim to cultivate students' Zone of Proximal Development (ZPD) in algorithmic programming learning by employing skillfully crafted prompts for LLMs to implement the scaffolding teaching method. By providing appropriate support, we stimulate students' cognitive engagement, guiding them to think critically and solve problems independently. Specifically, in the process of solving algorithmic problems, we designed multi-level hints for both the idea formation and idea implementation phases. These hints scaffold learning by progressing from more abstract and symbolic to more concrete and specific only when students make errors.


\subsection{AI Coding Assistants and Application in Educational Contexts}
% - recent advancement of AI coder
% - research on leveraging AI for programming learning purposes. (1) they mainly focus on introductory programming, however, we focus on more advanced algorithmic programming learning, which can pose new difficulties, such as xxx. (2) most work just explores how AI will affect and whether AI can help with learning. However, we identify some key challenges that just using AI such as ChatGPT and Copilot is insufficient for learning. So we adopt a learner-centered design to tailor LLMs and develop a new tool.


LLMs have demonstrated their capabilities in programming-related tasks, including delivering precise feedback on syntax errors \cite{phung2023generating} and enhancing programming workflows \cite{leinonen2023using}. Sarsa et al. evaluated OpenAI Codex’s potential for generating engaging programming exercises \cite{sarsa2022automatic}, while MacNeil et al. observed that student engagement with LLM-generated code explanations varied depending on the complexity and length of the content \cite{macneil2023experiences}. Additionally, Prather et al. highlighted the dual-edged nature of GitHub Copilot’s suggestions, cautioning against potential dependency issues and underscoring the importance of fostering meta-cognitive skills in novice programmers \cite{prather2023s}.

\ms{Recent work has also explored mapping natural language to code using LLMs, which aligns with aspects of our approach. For instance, Liu et al. \cite{liu2023wants} introduced grounded abstraction matching to help non-expert end-user programmers better understand LLM capabilities and refine their input for code generation. Their method iteratively converts natural language to code and back into grounded utterances to refine user input. While their focus is on improving language refinement, our work emphasizes scaffolding learners in decomposition and idea-building for solving algorithmic problems, placing less priority on natural language quality.}

As LLMs advance in code generation, their applications in programming education have garnered increasing attention, particularly for creating educational content, enhancing student engagement, and personalizing learning experiences \cite{kasneci2023chatgpt}. Recent studies have examined these opportunities, focusing on task completion \cite{denny2023conversing}, instructional content generation \cite{leinonen2023using}, and innovative methods for content creation \cite{denny2022robosourcing}. Notably, Finnie-Ansley et al. found that OpenAI Codex outperformed most students in coding tasks during CS1 and CS2 exams \cite{finnie2023my}. Similarly, Kazemitabaar et al. studied how AI code generators like Codex support novice learners, showing increased code-authoring performance without compromising manual code-modification skills \cite{kazemitabaar2023novices}.

While much of the existing work focuses on introductory programming, our research addresses the challenges of advanced algorithmic programming, such as those encountered in CS2 courses. One closely related piece of work is that of Jin et al. \cite{jin2024teach}, who designed a teachable agent based on LLMs, employing a learning-by-teaching approach to help students grasp algorithmic concepts. However, their focus is primarily on the mastery of algorithmic concepts, whereas our work addresses the difficulties students face in applying these concepts to solve practical problems. Another tool similar to our work is Code Tutor \cite{codetutor}, which helps students tackle programming problems by guiding their thinking without giving direct answers. However, these conversational LLM tools differ from DBox in two key ways. First, their chat-based format can lead to students getting lost in lengthy conversations, whereas DBox uses an interactive step tree to build a structured mental model. Second, these tools rely on students inputting code or questions disconnected from their existing work, while DBox integrates the step tree with the editor, providing better context understanding and reducing extra input, thereby improving learning efficiency.





% As Large Language Models (LLMs) become increasingly advanced in code generation, education researchers are increasingly exploring their potential applications in programming education, such as producing educational content, enhancing student engagement, and customizing learning experiences \cite{kasneci2023chatgpt}. In computer science education, recent work has begun to examine the implications and opportunities that LLMs offer from various perspectives \cite{becker2023programming}. Most research has focused on understanding the capabilities of LLMs in completing programming tasks \cite{denny2023conversing}, generating instructional content \cite{leinonen2023using}, and developing new content creation methods \cite{denny2022robosourcing}. For example, Finnie-Ansley et al. \cite{finnie2023my} demonstrated that OpenAI Codex outperforms most students on code writing questions in both CS1 and CS2 exams.

% LLMs have also shown promise in creating educational content, particularly in enhancing programming error messages \cite{leinonen2023using} and providing high-precision feedback on code to fix syntax errors \cite{phung2023generating}. Furthermore, Sarsa et al. \cite{sarsa2022automatic} analyzed the novelty, plausibility, and readiness of 120 programming exercises generated by OpenAI Codex, proposing the potential use of such models to generate coding assignments. MacNeil et al. \cite{macneil2023experiences} explored student experiences with LLM-generated code explanations in a web software development e-book, finding that most students found the explanations helpful, although engagement varied depending on code length, complexity, and explanation types. Additionally, Prather et al. \cite{prather2023s} studied how novice programmers interact with GitHub Copilot, noting that while novices often benefited from code suggestions, they also exhibited behaviors such as "shepherding" and "drifting," indicating potential over-reliance and uncertainty in Copilot’s feedback. This highlights the importance of understanding how learners use AI-generated code and develop meta-cognitive skills such as verifying AI-generated output and practicing self-regulation.

% Supporting novice programmers during the code-writing process has been an area of active research, with various scaffolded approaches being explored. Bruner \cite{bruner1974toward} introduced the concept of "scaffolding," which refers to providing support structures to help students learn skills beyond their current capabilities. Effective scaffolding equips learners with the necessary knowledge and skills to perform tasks independently. For instance, pre-coding support methods, such as using flowcharts to brainstorm and organize solution ideas, have been shown to improve algorithm design and programming skills \cite{smetsers2017problem}. Additionally, Cunningham et al. \cite{cunningham2021avoiding} described a multi-stage programming process that includes explicitly arranging plans. On the other hand, immediate assistance during code writing is another critical scaffolding strategy. This can include providing detailed feedback based on the student’s current code, such as explanations of errors \cite{singh2013automated}, suggestions for corrections \cite{sykes2010design}, or next-step hints to guide the student towards their goal \cite{rivers2017data, rivers2017data}. Other supportive methods include offering a library of worked examples \cite{wang2022exploring} or Parsons problems that students can actively solve \cite{hou2022using}. 


% However, existing research on AI coding assistants has predominantly focused on introductory programming, such as CS1 \cite{kazemitabaar2023studying}. In contrast, our work targets more advanced algorithmic programming, like CS2, which presents distinct challenges compared to introductory courses. In algorithmic programming, learners often struggle not with syntax or debugging, but with the more complex task of decomposing difficult problems into manageable substeps. They frequently find themselves stuck or pursuing incorrect approaches when attempting to devise complete solutions. Moreover, most studies have primarily investigated whether and how well LLM-based tools, such as ChatGPT, Codex, or GitHub Copilot, can serve as effective aids for programming learning. However, these tools were not originally designed with learning as their primary purpose. Through a formative study with 15 participants, we identified critical challenges that underscore the limitations of current AI tools in fostering active algorithmic learning. To address these gaps, we adopted a learner-centered design approach, customizing large language models (LLMs) to better align with learners’ needs. This approach led to the development of our new tool, DBox, specifically designed to enhance the learning experience in advanced algorithmic programming.





\subsection{Computational Thinking and Problem Decomposition}

% Computational Thinking (CT), as initially conceptualized by Wing \cite{wing2006computational}, has been refined over the years to encompass more than just computer programming skills. Computer science educators have generally agreed that CT involves thought processes such as decomposition, abstraction, generalization, algorithmic thinking, and debugging \cite{angeli2016k}.

% Problem decomposition is a foundational element of CT and is particularly critical in the context of solving complex programming problems. The process of decomposition involves breaking down a problem into smaller, manageable subproblems, which allows students to effectively manage their cognitive resources. This skill is essential for developing a clear solution plan. Researchers like McCracken et al. have emphasized this by proposing a five-step framework for learning in CS1 courses: 1) abstract the problem from its description, 2) generate subproblems, 3) transform subproblems into sub-solutions, 4) re-compose the sub-solutions into a working program, and 5) evaluate and iterate \cite{mccracken2001multi}.

% In educational settings, the importance of decomposition is further illustrated through various instructional approaches. For example, Keen and Mammen developed a term-long course project where students were given clear guidance on program decomposition at early milestones, with progressively less direction provided as the course progressed \cite{keen2015program}. Their analysis, comparing the cyclomatic complexity \cite{mccabe1976complexity} of final assignment submissions between students in the course with the term-long project and those in courses with stand-alone projects, indicated that long-term projects offer more significant benefits in learning decomposition skills. Similarly, Sooriamurthi discusses a learning exercise in which CS1 students are required to break a large programming assignment into smaller pieces. This exercise emphasizes the importance of abstraction, decomposition, incremental, and iterative development, compared to simpler, independent programming problems \cite{sooriamurthi2009introducing}. Pearce et al. also contributed to this understanding by implementing a guided inquiry-based learning approach in a CS1 course to teach students strategies for problem decomposition. Using a rubric to measure students' abilities to decompose problems, they examined the final projects from two offerings of the CS1 course—one with and one without explicit instruction on decomposition. Their findings suggest that students who received additional scaffolding were significantly more adept at breaking down problems into subproblems \cite{pearce2015improving}.

% Despite the importance of decomposition in fostering computational thinking (CT) and programming learning, there are currently few tools that leverage large language models (LLMs) to assist students in problem decomposition. In this paper, we introduce the Decomposition Box, a tool designed to guide students in actively and interactively breaking down complex problems into specific, manageable steps and sub-steps through a structured \emph{step tree}. At each step, the tool utilizes LLMs to provide fine-grained judgments and guidance, thereby enhancing students' computational thinking and problem-solving skills throughout the algorithmic problem-solving process.


\ms{Computational Thinking (CT), as originally conceptualized by Wing \cite{wing2006computational}, includes essential cognitive processes such as decomposition, abstraction, generalization, algorithmic thinking, and debugging \cite{angeli2016k}. Among these, decomposition is widely recognized as a cornerstone of CT. Early studies, such as those by Roy Pea, emphasized how programming environments like LOGO foster planning and decomposition skills by encouraging learners to break problems into smaller, manageable subproblems \cite{pea1987logo, pea1984cognitive}.} McCracken et al. \cite{mccracken2001multi} further highlighted its significance through a five-step learning framework for CS1 courses: (1) abstracting the problem from its description, (2) generating subproblems, (3) transforming these into sub-solutions, (4) re-composing them into a complete program, and (5) evaluating and iterating.

The importance of decomposition is also reflected in various educational strategies. For example, Keen and Mammen implemented a term-long course project that initially provided explicit guidance on program decomposition, gradually reducing support as the course progressed \cite{keen2015program}. Similarly, Sooriamurthi designed an exercise for CS1 students that required segmenting a large programming task into smaller components, promoting abstraction and iterative development skills \cite{sooriamurthi2009introducing}. Pearce et al. adopted a guided inquiry-based learning method in CS1, explicitly teaching decomposition strategies and using a rubric to assess students’ skills. Their findings demonstrated that students who received explicit scaffolding showed greater proficiency in breaking down problems \cite{pearce2015improving}.

Despite the well-documented value of decomposition in cultivating CT and programming skills, few tools leverage LLMs for problem decomposition. \ms{One related work by Kazemitabaar et al. \cite{kazemitabaar2024improving} explored task decomposition in data analysis programming, helping users break tasks into substeps or subphases to better steer and validate LLM-generated assumptions and code. However, their approach depends on LLMs automatically generating decomposition solutions for users to review. In contrast, our work emphasizes scenarios where learners actively develop decomposition skills, with LLMs providing minimal scaffolding. We term this approach ``learner-LLM co-decomposition'', fundamentally distinct from the ``LLM-generate then user-verify'' paradigm.}

To mitigate the aforementioned gaps, this paper introduces DBox, a tool designed to help students break down complex problems into manageable steps using a structured step tree. Through learner-AI co-decomposition, learners actively build the step tree while receiving step-level feedback from the LLM, enhancing their computational thinking and problem-solving skills.