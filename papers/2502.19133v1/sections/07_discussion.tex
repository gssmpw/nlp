
\section{Discussion}


In this paper, we adopted a learner-centered design approach, beginning with a formative study to identify students' challenges with existing tools. Based on these insights, we developed DBox, a tool that scaffolds students in breaking problems into smaller parts and provides personalized, adaptive support. Our user study demonstrated that DBox improved learners' performance on similar algorithmic problems, increased perceived learning gains, and fostered greater cognitive engagement, achievement, and satisfaction. In this section, we discuss design implications and generalizability based on our key findings.


\ms{
\subsection{Chaining Learners' Thoughts with Visualized Structured UI Components}

Decomposition requires students to effectively organize their thoughts. While visual elements are known to promote structured thinking and support mental model construction \cite{mcdougall2001effects, liu2010mental}, our formative and user studies revealed shortcomings in existing tools like LeetCode and ChatGPT, which rely on textual representations without adequately supporting structured mental models. In contrast, DBox uses an interactive step tree to visually organize learners' thoughts. This feature was praised by 22 of 24 participants for enhancing algorithmic thinking, serving as a progress tracker, and providing value even without AI assistance.

DBox's interactive step tree and tree-based scaffolding demonstrate the broader potential of intelligent tutoring systems (ITS) to promote active learning and self-regulated problem-solving in fields requiring problem decomposition. Similar principles could benefit STEM education, such as physics or engineering, by externalizing abstract concepts and facilitating multi-step problem-solving. Additionally, progress-tracking visual components may inspire designs for professional training tools in areas like medical diagnostics or software engineering.

\subsection{Promoting Independent Thinking and Active Decomposition Learning}

\subsubsection{\textbf{Transforming Learners from Passive Readers to Active Thinkers}}

Many coding tools provide direct answers or solutions \cite{kazemitabaar2023novices, phung2023generating}, which, while efficient, often bypass opportunities to develop critical problem-solving skills. In contrast, DBox cultivates students' decomposition abilities through structured scaffolding, fostering critical thinking and self-regulated learning in line with learning by doing \cite{anzai1979theory} and constructivist principles \cite{tobias2009constructivist}.

To strengthen decomposition skills, DBox first encourages students to develop their own decomposition strategies by coding or building a step tree from scratch. While DBox can generate parts of a step tree from a student's existing code, these steps are derived from the learner's own reasoning, with DBox acting solely as a modality converter. Besides, DBox provides feedback on tree node statuses, identifying potential errors or missing steps without directly showing the correct answer, challenging students to critically evaluate and refine their decomposition plans.


DBox's scaffolded hint system further supports decomposition skill development by providing adaptive guidance tailored to the student’s progress without overwhelming them. All hints are based on the learner's current decomposition skeleton, with the most detailed hint—``reveal substep''—triggered only after repeated attempts and struggles. Notably, even the most detailed hints prompt only one substep, requiring students to complete the rest independently. As shown in Sec \ref{hintusage}, only 19\% of hints are this detailed, with students primarily relying on simpler, thought-provoking question hints. This scaffolded support system balances guidance and independent thinking, keeping students engaged during challenges without compromising their ability to independently decompose problems \cite{kinnunen2006students}.

Based on these findings, we recommend fostering active problem-solving by shifting students from passive content consumption to active solution creation. Designers could adopt layered scaffolding, starting with minimal guidance and increasing support as needed, to help students progressively master decomposition skills while maintaining confidence and avoiding frustration. Additionally, adaptive learning techniques, such as real-time feedback and progress tracking, can further tailor the support to individual decomposition barriers, encouraging deeper engagement with decomposition tasks. Moreover, designers could integrate metacognitive strategies, such as encouraging students to articulate or reflect on their decomposition approaches, to further enhance critical thinking and foster habits of independent thinking.




\subsubsection{\textbf{Choice of Scaffolding: Balancing Independent Problem-Solving and Efforts}}

Scaffolding involves providing tailored support to help learners accomplish tasks they cannot yet complete independently \cite{kim2011scaffolding, tobias2009constructivist}. Broadly, scaffolding strategies fall into two categories \cite{van2010scaffolding}: (1) gradually reducing assistance as learners gain proficiency, and (2) encouraging independent problem-solving while offering incremental support to address challenges. DBox adopts the second approach, emphasizing independent thinking and encouraging learners to actively decompose problems \cite{zimmerman2013theories}. While our scaffolding strategies successfully enhanced critical thinking, satisfaction, and perceived usefulness, they also led to increased cognitive effort (Sec. \ref{Effects_on_UX}). This tradeoff underscores the importance of carefully balancing cognitive effort with the promotion of independent thinking.

Future designs could incorporate adaptive scaffolding that adjusts support dynamically based on learner proficiency, reducing unnecessary effort in areas where students have demonstrated competence. Additionally, while incremental scaffolding was effective for algorithmic problem-solving, tailoring strategies to different educational contexts could enhance their applicability in diverse domains. Such adaptive, context-specific approaches could further optimize the balance between support and independence in learning environments.


\subsection{Supporting Personalized Algorithmic Programming Learning}

\subsubsection{\textbf{Prioritizing Learners' Own Solutions Over Optimality}}

Algorithmic problems often have multiple solutions with varying time and space complexities. DBox prioritizes independent exploration by supporting learners' strategies rather than steering them toward a single ``optimal'' solution. Using LLM-driven prompts, it evaluates and guides each step based on the learner's reasoning, preserving their step decomposition and respecting their input—even when errors occur. While some solutions may not be the most efficient, this approach fosters autonomy by aligning feedback with learners’ thought processes instead of enforcing rigid standards.

Our user study showed that this approach improves learning outcomes and is well-received by students. We recommend designing systems that respect personalized problem-solving strategies by aligning feedback with learners' reasoning while allowing for diverse approaches. Designers should balance flexibility and rigor, using prompts and interfaces that support varied strategies while gently guiding learners toward effective solutions.


\subsubsection{\textbf{Catering to Individual Learning Styles and Contextual Needs}}

DBox accommodates diverse problem-solving approaches with two input modes: coding and natural language descriptions. Each mode offers distinct advantages tailored to different learners, stages, and situations. Learners can switch seamlessly between modes, with progress automatically synced across the interface. Features such as verifying code-step alignment ensure strong integration between modes.

Our findings reveal that this flexibility enhances user experience. Participant interaction logs and interviews revealed three usage patterns, highlighting that each mode fits different needs: code mode works well for students with a clear and detailed problem-solving plan already, while the step tree with natural language descriptions helps less experienced students with only a basic idea who are not ready to write code directly, boosting their confidence.


We argue there is no universal “best” mode for programming education—each has unique benefits depending on the learner habits, expertise, and context. Future tools should provide flexibility, like DBox, or use adaptive algorithms to recommend modes based on user needs and context. This flexibility highlights the importance of designing educational tools that accommodate varying levels of expertise and problem-solving styles, which can be generalized to other domains requiring personalized learning \cite{bernacki2021systematic}.

\subsection{Appropriate Usage of LLMs for Supporting Algorithmic Programming Learning}

\subsubsection{\textbf{Caution About LLM Errors}}

Although LLMs have shown strong performance in coding tasks \cite{finnie2023my, leinonen2023using}, they remain prone to errors. Our technical evaluation and user study revealed that even with comprehensive context—such as problem statements, user code, and natural language steps—LLM sometimes misinterprets user descriptions. These errors likely arise from discrepancies between the natural language used by students and the formal, precise language the LLM was trained on, which is primarily sourced from web-based code and comments \cite{liu2023wants}.

Such misinterpretations can hinder learning by causing confusion or frustration. While future improvements to training data and GPT versions may mitigate these issues, design strategies can help address them. \textbf{First}, LLMs should avoid giving direct solutions and instead focus on fostering active problem-solving through explanations and hints. \textbf{Second}, feedback could be paired with interactive features, like a ``Run Code'' option, allowing students to validate their reasoning. \textbf{Third}, simple tutorials could teach users how to phrase their descriptions more clearly, improving LLM's understanding. Additionally, future tools could integrate a ``Language Enhancement'' feature to suggest improvements or assess the clarity of descriptions, aiding LLM in accurately capturing user intent. Most importantly, we recommend designers prioritize technical feasibility, such as conducting rigorous evaluations like ours, before fully integrating LLMs into programming learning tools.
}



\subsubsection{\textbf{Learner-LLM Co-Decomposition of Solutions: Learner as Leader, LLM as Aid}}

A central feature of DBox is the construction of a step tree, where students break solutions into steps and sub-steps. The LLM supports this by mapping code to step descriptions, evaluating them, and offering hints. However, students maintain full control, deciding how to decompose problems and define each step, fostering independent thinking. The LLM acts solely as an aid, using a scaffolding approach to support the development of learners' Zone of Proximal Development (ZPD) \cite{chaiklin2003zone}. Unlike tools like ChatGPT or Copilot that dominate problem-solving, DBox fosters deeper cognitive engagement. Students reported greater accomplishment and found this approach more effective for learning.

This contrasts with existing human-AI collaboration paradigms in non-educational scenarios where AI usually suggest options, leaving final decisions to users \cite{dang2023choice, gao2024collabcoder, gebreegziabher2023patat, ma2019smarteye, ma2022glancee}, such as in human-AI decision-making \cite{ma2023should, ma2024towards, ma2024you}. Some educational tools, like Jin et al. \cite{jin2024teach}, use LLMs to generate solutions for students to evaluate, which aids in syntax learning but such ``LLM-generate then learner-evaluate'' approach is less effective for algorithmic problem-solving, where constructing solutions is key. Just evaluating LLM-generated contents can place a cognitive anchor on learners \cite{furnham2011literature}, limiting independent thinking and creativity. Thus, task allocation between humans and AI should align with the educational context (e.g., whether it is basic knowledge/concept learning or higher-level creative thinking). Future LLM-based educational tools should carefully define the division of roles between LLMs and learners, tailoring it to specific learning contexts and goals.




% \subsubsection{Human-LLM Co-Decomposition of Solution: AI Should Judge Instead of Recommending}

% A core interaction in DBox is the construction of a step tree, where the entire solution is broken down into a series of steps and sub-steps. We refer to this as the human-LLM co-decomposition process. In this process, the LLM behind DBox plays three roles: First, it maps the student's written code into step descriptions. Second, it evaluates the status of each step and sub-step (whether they are correct, incorrect, missing, or need further decomposition). Third, it provides hints for incorrect or missing steps or sub-steps. However, the actual construction of the step tree—such as dividing the solution into steps and sub-steps and determining the content of each node—remains primarily the student's responsibility.

% This division of labor maximizes student engagement in independent thinking and problem-solving. The LLM does not provide any suggestions for decomposition nor directly recommend content for specific steps, aligning with the scaffolding educational approach, where guidance is provided appropriately, but the main task of forming the solution is left to the students.

% In contrast, when students directly seek help from an LLM, such as asking questions in ChatGPT or using Copilot for code completion, the LLM takes too much initiative by directly offering ideas or code. In our co-decomposition design, however, students demonstrated higher cognitive engagement and more active critical thinking. Furthermore, students reported that constructing solutions in this way gave them a greater sense of achievement and made them feel the process was more beneficial for learning, leading to higher satisfaction with the experience.

% Related work has proposed similar approaches. For instance, XXX, in the context of problem-solving, uses the "learning by teaching" concept, where students take on the tasks of judging and teaching, while the LLM generates most of the solutions. Compared to our approach, their division of labor between the student and the LLM is reversed. This method works well in introductory programming, where the focus is on mastering syntax. Having students guide the LLM to generate code or evaluate potentially incorrect code produced by the LLM is an effective way to quiz them. However, in our work, which focuses on algorithmic programming, the key step is constructing a solution from scratch. If the LLM builds the solution, leaving students only to judge it, it hampers their independent thinking.

% Thus, when designing LLM-based educational tools in the future, it is crucial to consider the specific context to effectively allocate tasks between the student and the LLM, ensuring that students derive the maximum benefit from the co-decomposition process.


% \subsection{Future Design Opportunities}

% \emph{Providing Appropriate Generative Assistance:} While DBox promotes independent problem-solving, some users showed interest in features like auto-completion for trivial coding tasks. Future versions could balance promoting independence with targeted assistance by enabling adjustable difficulty levels and offering contextual suggestions when appropriate.

% \emph{Covering All Stages of Algorithmic Programming:} DBox currently lacks a focus on foundational algorithm instruction and problem comprehension. Future iterations could include features like generating distractor solutions, input-output tests, and step-by-step rephrasing to help students grasp key concepts and understand the coding problem.

% \emph{Combining Step Trees with Dialogue:} Users can currently describe their thought processes but cannot ask questions. Adding a dialogue system to the step tree would allow students to share challenges and ask follow-up questions. GPT could then provide guided feedback without giving direct answers, supporting independent problem-solving.





% \emph{Other Important Features.} DBox could offer more control by allowing users to select specific parts of their code for targeted evaluation and guidance. A ``review'' feature could also help students reflect on key stumbling points, understand where their thought process went wrong, and how they eventually solved the problem.


% \subsection{Future Design Opportunities}

% \emph{Providing Appropriate Generative Assistance.} Our tool primarily focuses on encouraging users to create the step tree and write the code independently, with the system mainly serving as a judge. However, users expressed a desire for some intelligent completion features, particularly for repetitive or simple code, allowing them to focus their efforts on learning the key parts. Future improvements should strike a balance between fostering independent thinking and providing appropriate assistance. One approach could be designing basic rules where the tool offers intelligent suggestions and completions for parts unrelated to the core logic, while maintaining the current level of independence for key learning areas. Additionally, the system could offer different modes, allowing users to choose the level of assistance, from basic judgment-only feedback to a combination of judgment, guidance, necessary completions, and even on-demand suggestions.

% \emph{Covering All Stages of Algorithmic Programming.} Currently, our system does not cover the basic teaching of algorithms or the problem comprehension stage. In the future, to address the diversity and uncertainty in solutions and help students grasp multiple approaches, we could expand assistance during the idea formation phase. For example, GPT could generate multiple potential solutions with distractors, prompting students to identify the one that meets the problem's complexity requirements. We could also introduce specialized algorithm training, where students select a specific algorithm, and the system’s guidance focuses solely on that algorithm. To assist with problem comprehension, we could incorporate input-output tests to check students' understanding of the problem and step-by-step rephrasing to help them grasp more complex problems.

% \emph{Combining Interactive Step Trees with Dialogue Boxes.} Sometimes users want to describe their difficulties, and currently, we ask them to outline their thought processes. Additionally, users may want to ask follow-up questions. In the future, we could combine the structured step tree with a small dialogue box. The primary goal would still be to construct the step tree, but users could engage in a conversation with GPT in the context of the current step tree or a specific step. Importantly, GPT should guide the user without revealing direct answers.

% \emph{Other Important Features.} First, DBox could offer learners more control, such as allowing users to select specific parts of the code for targeted evaluation and guidance. We could also introduce a summary feature for key stumbling points, helping students reflect on the challenges they faced, where their thought process went wrong, and how they eventually overcame the problem.




\subsection{Limitations and Future Work}

This study has several limitations. \emph{First}, we tested DBox's effectiveness on only two problem types; future work should examine a broader range of algorithms. \emph{Second}, participants engaged in just one learning session per condition due to time constraints, whereas mastering algorithmic problems typically requires extended practice. Longitudinal studies should explore how DBox supports skill development over time, including changes in mental models and skill retention. \emph{Third}, we assessed learning gains based on correctness in a test session using similar learning and test problems. Future research should evaluate knowledge transfer to less similar problems. Due to time constraints, we conducted a single post-test rather than a pre-post comparison. While pre-test expertise filtering and randomization minimized prior familiarity effects, a more rigorous pre-post design would yield more accurate learning gain measurements. Looking ahead, we plan to release DBox as a Chrome plugin for integration with existing coding platforms, enabling large-scale field studies. This will allow for the collection of long-term usage data and periodic surveys to identify usage patterns and learning experiences over time.



% This study has several limitations. First, in our within-subject design, we selected two types of algorithm problems—Greedy and Binary Search—and randomly assigned them to two conditions (DBox and baseline). However, selection bias may still exist, as some participants might naturally excel at one type of algorithm. Although we addressed this by filtering participants' proficiency through a pre-test and using a Latin Square design, further validation across a broader range of algorithms is needed in future work.

% Second, students experienced only one learning session per condition before the test session. While this allowed for a fair comparison, mastering algorithmic problems typically requires extended practice. Future work should explore how DBox supports students' long-term improvement in algorithmic skills. Longitudinal studies could provide insights into changes in learners' mental models, allowing students more time to deepen their understanding and refine their decomposition methods. Additionally, retention tests could assess whether students can still apply learned problem-solving methods after a time gap.

% We measured learning gains through correctness scores in the test session, with relatively similar learning and test problems. Future work should explore students' ability to transfer their knowledge to problems with lower similarity. Due to time constraints, we opted for a single post-test rather than a pre-post comparison. While we minimized prior familiarity effects by filtering participants and randomizing problem assignments, future studies could adopt a more rigorous pre-post test design for better measurement of learning gains.

% Looking ahead, we plan to release DBox as a Chrome plugin for integration with existing online coding platforms and large-scale real-world testing. In such settings, where students may be more motivated (e.g., preparing for algorithm interviews), we can gather long-term usage data while ensuring privacy. We also plan to conduct periodic surveys to track changes in students' usage patterns and learning experiences over time.



% \subsection{Limitations and Future Work}

% This study has several limitations. First, in our within-subjects study, we selected two types of algorithm problems, Greedy and Binary Search, and randomly assigned them to two conditions, DBox and the baseline. However, there may still be selection bias, where some participants were naturally better at one type of algorithm. While we mitigated this issue to a large extent by filtering participants' proficiency through a pre-test and employing a Latin Square design to randomize the problem-condition assignment, there is still room for improvement. Future work should validate DBox's effectiveness across a broader range of problem types.

% Second, in our experiment, students only experienced one learning session in each condition before moving on to the test session. Although this comparison was fair (as both conditions had only one learning session), mastering an algorithmic problem often requires extended practice. Future work should explore how DBox can help students gradually improve their algorithmic programming skills over time. Longitudinal studies may reveal significant changes in learners' mental models, providing more time for them to understand a specific algorithm and enhance their decomposition methods. Additionally, future studies could include retention tests to measure whether students can still effectively apply previously learned problem-solving methods after a period of time.

% Furthermore, when objectively measuring students' learning gains, we calculated their correctness score in the test session. On the one hand, the learning session and test session problems had a relatively high degree of similarity. Future work should investigate whether students can transfer what they have learned to solve problems of the same algorithm type with lower similarity. On the other hand, due to time constraints, we did not include a pre-post test comparison, opting for a single post-test instead. This result might be influenced by students' pre-existing familiarity with the problems. Although we mitigated this issue by filtering for familiarity (ensuring participants were not too familiar with the problems) and randomizing the problem assignments, future work could include a more rigorous pre-post test design to better calculate students' learning gains.

% Moreover, DBox is currently only applied in algorithmic programming, specifically solving algorithm problems. However, this decomposition-based computational thinking approach could be extended to other learning scenarios, such as project-based learning. Future work could explore how to adapt DBox to broader educational contexts outside of algorithmic programming.

% Looking forward, we aim to deploy DBox in real-world algorithm courses. Since algorithms are a core required subject in undergraduate computer science curricula, we hope to investigate how students who have just learned algorithm concepts use DBox to develop their problem-solving skills. Additionally, we plan to convert DBox into a Chrome plugin and release it in the Chrome Web Store for real-world testing. This would allow DBox to seamlessly integrate with existing online coding platforms, enabling large-scale experiments. In such settings, students' motivation may be stronger (e.g., a graduate preparing for an algorithm interview), leading to more realistic usage patterns. Students could use DBox to tackle a wide variety of algorithm problems. We hope to collect long-term (e.g., six-month) usage data from real-world users while ensuring privacy, and use periodic surveys to capture changes in students' usage patterns and learning experiences over time.





\section{Conclusion}
% In this paper, we introduced Decomposition Box (DBox), a novel tool designed to scaffold learners in decomposing problems during algorithmic programming learning. Based on insights from a formative study, we identified key design goals to address the limitations of existing tools in algorithmic programming education. DBox supports two critical stages of the programming process: idea formation and idea implementation. By offering two modes (code mode and language mode), it encourages users to independently develop their solution strategies. The interactive, visual step tree helps students break down problems and build a structured mental model. DBox provides fine-grained, step-level feedback, enabling students to quickly identify issues, while its multi-level guidance offers targeted support without undermining independent thinking.

% Our user study demonstrated that DBox led to significantly higher learning gains, cognitive engagement, and critical thinking. Students reported a stronger sense of achievement and found the assistance both appropriate and effective for their learning. We identified three main usage patterns, underscoring the importance of respecting students' problem-solving habits and offering them autonomy. The learner-LLM co-decomposition model we designed promotes independent thinking while allowing the LLM to contribute meaningfully, even with occasional imperfections. 

% We hope the formative study, design goals, features, technical evaluation, and key findings from this work will inspire future research on developing educational tools for broader programming learning.
In this paper, we introduced DBox, an interactive tool designed to help learners decompose algorithmic programming problems by supporting both solution formation and implementation. Featuring an intuitive tree-like box widget, DBox accepts input in both code and natural language, fostering independent problem-solving while its step tree structure helps learners develop structured mental models. It provides step-level feedback and layered guidance without compromising learner autonomy.
Our user study showed that DBox significantly improved learning outcomes, cognitive engagement, and critical thinking, with students reporting a greater sense of achievement and finding the support highly effective. Additionally, we identified three key usage patterns, highlighting the importance of accommodating individual problem-solving styles. Moreover, our findings suggest that the learner-LLM co-decomposition approach fosters independent thinking while providing meaningful guidance, even with occasional imperfections.
We hope the insights from our system design will inspire future research on integrating LLMs into educational tools for programming learning.