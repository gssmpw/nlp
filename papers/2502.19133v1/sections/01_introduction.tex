\section{Introduction}


Algorithmic programming, which involves applying algorithms to solve real-world problems, is a challenging yet essential skill for computer science learners \cite{backhouse2011algorithmic, skiena1998algorithm}. Unlike introductory programming, the primary challenge in algorithmic programming lies not in algorithmic concepts, basic logic or syntax but in decomposing a complex problem to develop a holistic solution \cite{smetsers2017problem, backhouse2011algorithmic}. Students often struggle with formulating clear strategies, getting stuck at specific steps, or overlooking edge cases \cite{linn1985cognitive, piech2015autonomously}. Online knowledge communities (e.g., LeetCode) are commonly used for self-study, where learners can browse reference solutions provided by these platforms or other online resources (e.g., search engines or Q\&A platforms) when encountering difficulties. However, these resources are not tailored to individual problem-solving approaches and can hinder independent thinking and learning.

In addition to traditional resources, the rapid advancement of large language models (LLMs) has introduced AI tools that rival or even surpass human performance in certain programming tasks \cite{finnie2023my}. Recent studies in CS education and human-computer interaction (HCI) have explored the impact of LLM-based tools like ChatGPT, GitHub Copilot, and Codex on introductory programming (CS1) \cite{kazemitabaar2023studying, jin2024teach, finnie2022robots, roest2024next}. While LLMs are increasingly used for tasks such as code completion, translation, debugging, summarization, and explanation \cite{hou2024large, sarsa2022automatic}, they are not specifically designed for educational purposes, leading to issues like over-reliance and a lack of independent problem-solving \cite{kazemitabaar2023novices, sheese2024patterns}, as highlighted in our formative study observing programming learners solving algorithmic problems using various AI support tools. Moreover, most research has focused on the impact of LLM on novice learners in introductory programming \cite{kazemitabaar2023studying, jin2024teach, finnie2022robots, roest2024next}, with little attention given to how these tools can be tailored to support algorithmic programming learning. \ms{Therefore, we pose the following research question: \textbf{How can LLM-supported interfaces be designed to effectively facilitate pedagogically meaningful learning in algorithmic programming?}}

% To answer this question, we explored challenges in algorithmic programming learning through a formative study observing 15 students solving algorithmic problems using various support tools, including LeetCode, ChatGPT, Search Engine, GitHub Copilot, and Stack Overflow. Through contextual inquiries during our lab observation and interviews, we identified four key challenges: (1) excessive help hindering active learning, (2) misalignment between provided solutions and learners’ approaches, (3) lack of a structured problem-solving approach, and (4) insufficient fine-grained feedback on learner progress.

% In this paper, we propose Decomposition Box (DBox), a tool that leverages LLMs to guide learners in decomposing algorithmic programming. DBox adopts a learner-LLM co-decomposition design, where learners as the leaders of the decomposition task with LLM provides appropriate scaffolding, offering guidance only when necessary
% DBox enables two key stages of algorithmic programming: solution formation and implementation. 
% For solution formation, DBox introduces an interactive step tree adaptive to students' existing code or thought process in natural language, offering scaffolding only when needed to achieve a balance between problem-solving progress and promoting learners' independent thinking.

% During the implementation stage, DBox validates the alignment between learners' code and the step tree in real time, identifying the status of each tree node. DBox also features a progressive hint system that begins with thought-provoking questions for incorrect or missing steps and gradually offers more specific hints after repeated attempts. The progressive hint system is integrated across both stages.


In this paper, we present the Decomposition Box (DBox), a tool that leverages LLMs to assist learners in decomposing algorithmic programming problems. DBox adopts a \emph{Learner-LLM Co-Decomposition} design, where learners lead the decomposition process while the LLM provides scaffolding only when needed. DBox supports two key stages of algorithmic programming: solution formation and implementation. During the solution formation stage, DBox introduces an interactive step tree that adapts to students' existing code or natural language thought processes. Students can express and update their ideas by directly writing code or iteratively constructing the step tree using interactive features, with DBox providing real-time feedback on the status of each node. In the implementation stage, DBox validates the alignment between learners' code and the step tree, identifying the status of each node in real time. A progressive hint system further supports learners by starting with thought-provoking questions for incorrect or missing steps and gradually offering more specific hints after repeated attempts. This design balances problem-solving progress with fostering independent thinking, with the hint system seamlessly integrated across both stages.



% Before deploying the tool, we conducted a technical evaluation of LLMs' ability to assess the fine-grained status (correct, incorrect, missing) of each step in a student's problem-solving process. We collaborated with students and algorithmic programming experts to create a student solution dataset, which was then used to evaluate the LLMs' accuracy in recognizing different types of errors. Overall, the results showed that LLMs are sufficiently accurate to support the features required by DBox. Specifically, LLMs were effective at identifying approaches from incomplete code but occasionally struggled with evaluating approaches described in natural language. A content analysis of LLM errors highlighted specific challenges in these cases. To support future research, we make this dataset publicly available.

% To examine DBox's impacts on algorithmic programming learning, we conducted a within-subjects mixed-methods user study with 24 learners, comparing DBox to a baseline condition where learners could freely choose and use any existing tools for solving algorithmic problems. We collected both quantitative data, including correctness scores from the test session, interaction logs, and questionnaire responses, and qualitative data through interviews.



% To examine DBox's impacts on algorithmic programming learning, we raised three research questions: \textbf{RQ2: How does Decomposition Box support algorithmic programming learning?}, \textbf{RQ3: How does Decomposition Box affect learners' perceptions and user experience?}, and \textbf{RQ4: How do learners interact with Decomposition Box and perceive the usefulness of different features?}. We conducted a within-subjects mixed-methods user study with 24 learners, comparing DBox to a baseline condition where learners could freely choose and use any existing tools for solving algorithmic problems. We collected both quantitative data, including correctness scores from the test session, interaction logs, and questionnaire responses, and qualitative data through interviews.
To evaluate DBox's effectiveness in supporting algorithmic programming learning, we conducted a within-subjects mixed-methods user study with 24 learners.
Our analysis highlighted that DBox significantly improved students' programming performance, perceived learning gains, confidence, algorithmic thinking, and self-efficacy. 
Students reported greater cognitive engagement, critical thinking, and a stronger sense of achievement compared to the baseline, where many felt they were ``cheating'' or not solving problems independently. 
DBox was seen as offering more appropriate assistance and benefiting learning. 
Since DBox relies on LLM for assessments, we also conducted a technical evaluation to assess the quality of LLM prompts used in the pipeline. 
This evaluation demonstrated that LLM are sufficiently accurate to support DBox's features, particularly in identifying incomplete and incorrect code approaches, though some challenges were noted in handling natural language descriptions.

% Before implementing the tool, we conducted a technical evaluation of LLMs' ability to identify the status (correct, incorrect, missing) of each step in a student's problem-solving approach. We recruited students and algorithmic programming experts to create a student solution dataset, which we then used to assess the accuracy of LLMs in recognizing various types of errors. The results indicated that while LLMs are effective at identifying approaches from incomplete code, they are less accurate when interpreting approaches described in natural language. A content analysis of the LLMs' errors revealed that they tend to struggle with specific situations. To support future research in LLM-based programming learning tools and AI, we have made our dataset publicly available.

% To thoroughly explore DBox's impact on algorithmic programming learning, we proposed three research questions. \textbf{RQ1}: How does Decomposition Box help with algorithmic programming learning? \textbf{RQ2}: How does Decomposition Box affect learners’ perceptions and user experience? \textbf{RQ3}: How do learners interact with Decomposition Box and perceive the usefulness of different features? To answer these questions, we conducted a user study. We invited 24 students to participate in a within-subjects study, where they used both DBox and a baseline condition to learn. In the baseline condition, students were provided with a variety of existing tools and could freely use them to solve algorithmic problems. We collected comprehensive data through objective test sessions, surveys, and interviews. The analysis revealed that, compared to the baseline condition, DBox improved students' objective performance in solving similar algorithmic problems, and significantly enhanced their perceived learning gain, confidence, algorithmic thinking, and self-efficacy. Additionally, students reported stronger cognitive engagement, critical thinking, and a greater sense of achievement when using DBox. In contrast, students in the baseline condition often felt as though they were "cheating" and that the solution wasn't something they developed independently. Students found DBox to provide more appropriate help and be more beneficial for learning algorithmic programming. We also examined cognitive load and found no significant difference between the two conditions. Overall, students were more satisfied with the learning process using DBox and expressed a stronger willingness to use it in the future. Our analysis of student usage patterns revealed several different approaches: most students chose to code independently first, using DBox to evaluate and guide their step-level thought process; some preferred to write out their thoughts in natural language before coding; and others dynamically combined these methods, flexibly using DBox's various features. Given that DBox uses LLMs to assess students' approaches, we were also interested in how students respond to errors in the system's assessments. Our behavioral analysis and interview feedback indicated that the impact of system errors varied based on students' confidence in their approach. If confident, students tended to ignore the system's assessment and focus on the reasonableness of the hint provided. Conversely, if uncertain about a step, students were more likely to over-rely on the system, though this over-reliance often diminished after trial and error.


Building on the key findings from our experiment, we provided several design implications for developing tools that promote algorithmic programming learning. We also discussed DBox's design philosophy, particularly the human-AI co-decomposition approach, and explored how to effectively leverage LLM to support programming learning, even with LLM's imperfection. In summary, our contributions are as follows:

% Lastly, we outlined future design opportunities for enhancing learning tools.


\begin{itemize}
    \item A formative study that identified four challenges learners face in algorithmic programming learning with existing tools, leading to four design goals.
    \item The design of DBox, an AI-assisted algorithmic programming learning tool, features a scaffolded interactive step tree that facilitates learner-AI co-decomposition of problems, supporting both the ideation and implementation stages while fostering independent thinking and active learning.
    \item A technical evaluation of LLM' accuracy in assessing students' fine-grained thought processes, highlighting where LLMs excel and where they face challenges.
    \item A user study reported DBox's effectiveness on algorithmic programming learning, including its effects on learners' learning outcomes, perceptions, user experience, and usage patterns.
    % \item A set of design implications for creating tools to better support algorithmic programming learning.
\end{itemize}





% Based on the key findings from our experiment, we summarized and discussed the design philosophy of DBox and the interaction mode of co-creating the step tree with students. We also delved into critical features, including Adaptive Multi-level Hints and Visualizing a Structured Mental Model. Additionally, with the issue of students' over-reliance on the system in mind, we discussed how to effectively utilize LLMs to support programming learning. Finally, we explored the generalizability of DBox and provided six design recommendations for developing algorithmic programming learning tools.

% In summary, our contributions are as follows:
% \begin{itemize}
%     \item A formative study that explored the challenges students face in algorithmic programming learning using existing tools, and identified four design goals.
%     \item A technical evaluation of LLMs' accuracy in assessing the correctness of students' thought processes, along with an analysis of where LLMs perform well and where they struggle.
%     \item A novel tool, DBox, that combines the educational pedagogy of scaffolding with an interactive step tree approach to support students' ideation and implementation stages in algorithmic programming learning, while promoting independent thinking and active learning.
%     \item A user experiment that thoroughly investigated DBox's impact on students' algorithmic programming learning, including its effects on learning outcomes, perceived learning experience, and usage patterns.
%     \item A set of design recommendations for HCI designers in programming education, based on our design process and key findings from the experiment.
% \end{itemize}
















% Large Language Models that are trained on millions of lines of code 

% Powered by the recent advancements in Deep Learning [88], Large Language Models that are trained on millions of lines of code, such as OpenAI Codex [14], can generate code from natural language descriptions (Figure 1, Left). In addition to enabling natural language programming, these AI coding assistants can perform numerous operations including code-to-code operations like code completion, translation, repair, and summarization, along with language-tocode operations such as code explanation and search [58, 79]. By generating code from simple sentences instead of formal and syntactically fxed specifcations, these AI Coding Assistants may lower the barriers to entry into programming.

