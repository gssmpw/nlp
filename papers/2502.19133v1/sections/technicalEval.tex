% \section{Technical Evaluation}

% As students use DBox, they progressively build an interactive step tree. In this human-AI co-creation process, the AI’s primary role is to evaluate the status of each node within the step tree. To foster independent thinking and active learning, the structure of the step tree—including the division of steps and substeps, as well as their content—is determined by the students themselves.

% In this section, our main focus is to determine whether, through prompt engineering, LLMs\footnote{In this paper, we use GPT-4o as the LLM. In the following, we will use the terms GPT or LLMs.} can accurately assess the status of nodes in the student’s step tree based on their inputs. In this technical evaluation, we specifically examine whether GPT can accurately judge the student's thought process as reflected in the status of each node in the step tree. We did not systematically test whether GPT can provide high-quality hints or correctly determine if a step has been properly implemented. These functionalities depend on GPT’s ability to accurately identify correct steps, incorrect steps, and missing steps. Therefore, status recognition is the most critical technical aspect we need to validate.

% Since DBox allows students to approach a problem either by writing code directly or by describing their thought process in natural language, we designed two distinct methods for assessing the status of the step tree. One method is based on the student’s current code input, and the other is based on their existing step tree input. As shown in Figure X, for code-based input, we instruct GPT to generate a step tree, which may include both steps and substeps. For each step or substep, LLMs categorize the status as correct, incorrect, or missing, and also generate multi-level hints. For description-based input, where the step tree is already provided, LLMs identify the status of each node in the tree, generate hints, and keep the original structure of the step tree unchanged.


% \section{Technical Evaluation}

% In DBox, students construct an interactive step tree through a human-AI co-creation process. The AI's primary function is to evaluate the status of each node within this tree, fostering independent thinking and active learning while students define the structure, steps, and substeps.

% This section focuses on evaluating whether, through prompt engineering, LLMs—specifically GPT-4o, referred to hereafter as GPT or LLMs—can accurately assess the status of nodes based on student inputs. This technical evaluation specifically addresses GPT's capability to accurately reflect the student's thought process through the status of each node. We did not systematically assess GPT's ability to provide high-quality hints or to verify proper step implementation. The core functionality tested here is GPT's accuracy in recognizing correct, incorrect, and missing steps, which is vital for validating the technical efficacy of DBox.


% \subsection{Data Collection} Our objective is to compile a dataset that captures various thought errors in specific programming problems, testing whether LLMs can precisely recognize the status of a user’s thought process.

% \subsubsection{Programming Problem Preparation} Research indicates that while GPT-4 performs well on easy LeetCode problems, it faces challenges with medium-level ones \cite{finnie2023my, savelka2023thrilled}. Consequently, our technical evaluation is focused on easy-level problems to verify the LLMs’ ability to accurately identify the status of nodes within a student's step tree, whether based on code or descriptions of the thought process.

% We selected 25 easy-level problems from LeetCode, encompassing a wide range of common algorithm types and data structures, such as Arrays, Strings, Hash Tables, Dynamic Programming, Sorting, Greedy algorithms, Depth-First and Breadth-First Searches, Binary Searches, Trees, and Bit Manipulation. Details on the chosen problems, our annotated dataset, and the evaluation results are included in the supplementary materials.

% \subsubsection{Annotator Recruitment} Given that there is no standardized approach to structuring a step tree and each individual may conceptualize it differently, we designed our tool to allow users significant autonomy in building their step trees, with the system providing supportive feedback and hints. To assist in this endeavor, we recruited five computer science students (three males and two females) from a local university. All participants had completed courses in data structures and algorithms and possessed foundational skills in solving algorithmic problems.

\section{Technical Evaluation}

In DBox, learners build a step tree through two input modes, with the LLM evaluating each node to provide active learning and detailed feedback. This section examines whether, with effective prompt engineering, GPT-4o model (referred to as GPT or LLM) can accurately assess node status based on student inputs, specifically identifying correct, incorrect, and missing steps—DBox's core feature. Evaluation of hint quality and step implementation is reserved for future research. Prompts are included in the supplementary materials.


\subsection{Data Collection} Our goal was to create a dataset capturing learners' thought errors in specific programming problems to test whether LLMs can accurately recognize the status of a learner’s thought process.

\subsubsection{Programming Problem Preparation}
Research indicates that while GPT-4 performs well on easy LeetCode problems, it faces challenges with medium- and hard-level ones \cite{finnie2023my, savelka2023thrilled}. For this evaluation, we focused on easy-level problems to assess the LLM’s ability to accurately identify node status in a student's step tree, based on either code or thought process descriptions. We selected 25 easy-level LeetCode problems covering a range of algorithms and data structures, including Arrays, Strings, Hash Tables, Dynamic Programming, Sorting, Greedy algorithms, Depth-First and Breadth-First Search, Binary Search, etc. Details on the problems, dataset, and results are provided in the supplementary materials.

\subsubsection{Annotator Recruitment}
Since step tree structures can vary based on individual decomposition approaches, DBox grants users significant autonomy in building their step trees. To capture the natural variation in learners' step trees, we chose not to generate them automatically. Instead, we recruited five computer science students (three males, two females, aged from 19 to 25) from a local university, all experienced in data structures, algorithms, and solving algorithmic problems, to manually create the step trees.








% \subsubsection{Procedure} We briefed participants on the annotation task's purpose and process, assigning 25 randomly selected problems among five students, with each solving five problems. Participants constructed a step tree for each problem based on their chosen solution approach, facilitated by providing correct code samples for various possible solutions. A step tree comprises steps, substeps, and even sub-substeps. Participants were tasked with constructing this tree according to their thought process, describing each node in their own words, and associating each node with the relevant code. We organized the problems, reference codes, and step tree components into a Google spreadsheet for annotation.

% \subsubsection{Error Generation} Once we collected the annotated step trees, we proceeded to generate potential errors reflecting possible thought process missteps. Since DBox handles both direct code inputs and natural language descriptions, we generated errors for both modalities.

% Drawing from common student misconceptions identified in previous studies \cite{qian2017students}, we defined three error types for natural language descriptions: missing steps, incorrect step order, and logical step errors. For code-based errors, we identified four types: missing steps, incorrect step order, logical step errors, and syntax errors. An expert in algorithmic programming constructed these errors for each problem's step tree, creating seven types of errors per problem, reviewed by another expert. This resulted in 175 error-laden step trees (25 problems x 7 error types).


% \subsection{Analysis Approach} For each error type, we categorized the steps into two parts: the correct part, consisting of steps with a correct status (labeled as 1), and the incorrect/missing part, consisting of steps with an incorrect or missing status (labeled as 0). To simplify the analysis and maintain a rigorous evaluation, we did not detail each step or substep within a task due to their potential complexity. Instead, we adopted a straightforward evaluation method: if all steps in the correct part are predicted as correct by GPT, the prediction is classified as 1; any incorrect prediction in this section classifies the overall prediction as 0. Conversely, if all steps in the incorrect/missing part are identified as incorrect or missing, the prediction is marked as 0; any deviation from this results in a prediction classified as 1. This method enables us to calculate classification metrics such as accuracy, F1 score, precision, recall, specificity, false positive rate, and false negative rate effectively.

% We removed the status fields from 175 error-containing step trees and input them into the LLMs for status prediction. We then compared the LLMs' predictions against the expert-annotated ground truth. For instances where LLM predictions were inaccurate, two authors independently performed open coding of GPT’s outputs to identify error themes, causes, and the scenarios in which they occurred. After a detailed discussion and analysis, we consolidated our findings into thematic insights.


\subsubsection{Procedure} Participants were briefed on the annotation task and process. Five students were each assigned five randomly selected problems (25 in total). For each problem, participants constructed a step tree based on their solution approach, including steps, substeps, and sub-substeps, using provided correct code samples. They described each node in their own words and linked it to the relevant code. The problems, reference codes, and step tree annotations were organized in a Google spreadsheet.

\subsubsection{Error Generation} After collecting the annotated step trees, we generated various types of errors to simulate common student misconceptions in coding \cite{qian2017students}. These errors occurred in both natural language descriptions and code inputs. For natural language, errors included missing steps, incorrect step order, and logical errors. For code, errors included missing steps, incorrect order, logical errors, and syntax errors. An algorithmic programming expert created seven error types for each problem, which were reviewed by another expert, resulting in 175 error-laden step trees (25 problems x 7 error types).

\subsection{Analysis Approach} We divide the steps into two parts based on the expert annotations: the correct part (1) or the incorrect/missing part (0). To calculate the performance of GPT, we use a very strict evaluation method: if all steps in the correct part are determined to be correct, the prediction of this part is marked as 1; otherwise, the prediction is 0. Similarly, if all steps in the incorrect/missing part are determined to be incorrect/missing, the prediction of this part is 0; otherwise, the prediction is 1. This approach enabled calculation of accuracy, F1 score, precision, recall, specificity, false positive rate, and false negative rate.


We removed status fields from 175 error-containing step trees and input them into GPT for prediction. The results were compared against expert-annotated ground truth. For incorrect predictions, two authors independently coded GPT's outputs to identify error themes and causes, later consolidated through discussion.



% \subsection{Results}
% As demonstrated in Table \ref{tab:technicalresult}, \textbf{GPT more effectively identifies students' thought processes when expressed directly through code rather than natural language descriptions}. This difference is attributed to the extensive code-based training data for GPT \cite{liu2024your}, contrasted with the relatively sparse data on natural language descriptions of thought processes. Additionally, GPT employs specialized post-evaluation mechanisms for code handling \cite{achiam2023gpt}, and students' descriptions often contain imprecise or non-standard terminology, leading to potential ambiguities and misunderstandings.

% For sequence change errors, GPT's accuracy is markedly lower from natural language descriptions, recording only 70\% accuracy and a 72\% F1 score. Misclassifications include 36\% of incorrect steps judged as correct (False Positive Rate, FPR = 0.36) and 24\% of correct steps judged as incorrect (False Negative Rate, FNR = 0.24). In contrast, code-based evaluations yield perfect accuracy and F1 scores of 100\%. For logical errors evaluated from natural language, accuracy dips to 88\% with an F1 score of 87\%, FPR of 8\%, and FNR of 16\%. However, when evaluated from code, these figures improve dramatically to 98\% accuracy and F1, with a minimal FPR of 4\% and an FNR of 0. Similarly, for missing steps or substeps, natural language evaluations show only 86\% accuracy and F1 score, with an FPR of 16\% and an FNR of 12\%. Code-based evaluations show better results at 92\% accuracy and F1 score, with both FPR and FNR at 8\%. Syntax error identifications maintain a steady 90\% accuracy and F1 score.

% Moreover, our analysis of GPT's output reveals \textbf{occasional modifications in the structure and content of the step tree}. Despite instructions to maintain the original structure and add nodes only for missing steps, discrepancies occur in how GPT segments steps and sometimes alters the students' original input.

% Additionally, a significant finding is that \textbf{GPT sometimes inaccurately judges students' approaches as incorrect if they deviate from commonly recognized solutions}. This occurred in five out of 25 tasks, suggesting that GPT's training may bias it towards optimal solutions commonly represented in its data. For instance, GPT judged sorting an array to find a missing number as unnecessary, incorrectly marking a divergent student approach as incorrect.

% In conclusion, while GPT excels in processing code-based inputs, it struggles with natural language, especially in identifying sequence change errors. Given the infrequency of such errors, GPT-4's capabilities are generally sufficient for assessing and guiding students' thought processes. As LLM technology advances, its potential for accurately understanding and guiding users' thought processes is expected to become more reliable and practical.

\subsection{Results}
As shown in Table \ref{tab:technicalresult}, \textbf{GPT more accurately identifies students' thought processes when expressed through code rather than natural language}. This difference stems from GPT's extensive code-based training data \cite{liu2024your} and specialized code-handling mechanisms \cite{achiam2023gpt}, while natural language descriptions often include imprecise or non-standard terminology, leading to ambiguities.

\textbf{GPT sometimes identifies incorrect steps as correct (false positives) or correct steps as incorrect (false negatives)}. For \emph{sequence change errors}, GPT’s accuracy drops to 70\% with an F1 score of 72\% from natural language descriptions, with a False Positive Rate (FPR) of 36\% and a False Negative Rate (FNR) of 24\%. In contrast, code-based evaluations achieve 100\% accuracy and F1 scores. For \emph{logical errors} from natural language, GPT’s accuracy is 88\% (F1 score 87\%, FPR 8\%, FNR 16\%), compared to 98\% accuracy and F1 scores from code-based evaluations (FPR 4\%, FNR 0\%). \emph{Missing step error} evaluations from natural language yield 86\% accuracy (FPR 16\%, FNR 12\%), improving to 92\% from code inputs (FPR/FNR 8\%). \emph{Syntax error} identification remains steady at 90\% accuracy and F1 score.

Additionally, we find that \textbf{GPT occasionally alters the structure and content of the step tree}, despite instructions to only add missing steps. It sometimes modifies how steps are segmented or misinterprets the student's original input. Another key finding is that \textbf{GPT sometimes incorrectly judges non-standard approaches as wrong}. In five out of 25 tasks, GPT mistakenly flagged correct solutions as incorrect simply because they deviated from the common approaches in its training data. For example, it marked a sorting-based solution as incorrect, even though it was correct, albeit not the most optimal approach.

In summary, GPT excels at processing code-based inputs but struggles with natural language, particularly in identifying sequence changes. Since sequence change errors are relatively uncommon, GPT is generally effective in evaluating students' thought processes. However, researchers looking to use GPT for this purpose should be aware of the limitations mentioned above. We hope this technical evaluation serves as a valuable reference for future work.




% \subsection{Results}
% As shown in Table \ref{tab:technicalresult}, overall, \textbf{GPT is more effective at identifying students' thought processes when they express their ideas directly through code rather than through natural language descriptions}. One reason for this is that GPT's training data includes a substantial amount of code \cite{liu2024your}, whereas there is likely much less data on verbal descriptions of thought processes. Second, GPT employs some special post-evaluation mechanisms when dealing with code \cite{achiam2023gpt}. Thirdly, students' natural language descriptions of specific steps in their thought processes are often unclear, using imprecise or non-standard terms that can lead to ambiguity and misunderstanding. For instance, we found that students' natural language descriptions are sometimes ambiguous and imprecise, which causes GPT to perform worse when recognizing steps compared to recognizing code. Additionally, when students provide overly simplistic descriptions of steps, such as failing to explicitly consider edge cases, GPT is more likely to incorrectly judge the status as wrong. These issues are also common in human instructional communication—informal language can indeed lead to misinterpretation.

% Specifically, when identifying sequence change errors, GPT performs poorly when judging from natural language descriptions. The accuracy is only 70\%, and the F1 score is 72\%. Among the incorrect steps, 36\% were wrongly identified as correct (FPR = 0.36), and among the correct steps, 24\% were misjudged as incorrect (FNR = 0.24). In contrast, when judging from code, both accuracy and F1 are 100\%, with no errors observed. For identifying logical errors, if judged from natural language descriptions, the accuracy is only 88\% and the F1 score is 87\%. Among the incorrect steps, 8\% were misidentified as correct (FPR = 0.08), and among the correct steps, 16\% were misjudged as incorrect (FNR = 0.16). When judged from code, the accuracy and F1 score reach 98\%, with only 4\% of the incorrect steps being misidentified as correct (FPR = 0.04), and none of the correct steps being judged as incorrect (FNR = 0). When identifying missing steps or substeps, the accuracy and F1 score are only 86\% when judged from natural language descriptions. Among the incorrect steps, 16\% were wrongly identified as correct (FPR = 0.16), and among the correct steps, 12\% were misjudged as incorrect (FNR = 0.12). When judged from code, the accuracy and F1 score are 92\%, with lower FPR and FNR (both 0.08). When identifying syntax errors, the accuracy and F1 score are both 90\%.

% Additionally, based on our analysis of GPT's returned step tree, we identified two other issues. First, \textbf{GPT occasionally changes the structure and content of the step tree}. When identifying the status from a student's natural language description of steps, we prompt GPT to evaluate the status of each node based on the student's step tree, allowing GPT to add nodes only in cases of missing steps while preserving the original structure of the step tree. Furthermore, we require GPT to retain the original input for each node so that we can match each node in GPT's returned step tree with the nodes in the UI. However, we found that GPT does not always strictly adhere to the student's input step tree. Specifically, there are instances where the step tree returned by GPT differs in how it segments steps compared to the student's segmentation. Additionally, GPT sometimes alters the student's original input.

% Second, one of our core objectives is to enable GPT to provide personalized support to students, continuing to develop the solution along the student's thought process. However, \textbf{there are instances where, if the student does not adopt the same approach as GPT, GPT will judge the student's approach as incorrect}. This occurred five times out of 25 tasks. We speculate that some tasks may have widely recognized optimal solutions, and GPT's training data is dominated by these solutions, leading GPT to classify non-optimal approaches used by students as incorrect. For example, in a problem where students are asked to find the missing number in an array, some students start by sorting the array, and GPT responds by saying that sorting is unnecessary, judging the step as incorrect.

% In summary, GPT performs well when determining the status of step trees based on code. However, due to limitations in the training data and the inaccuracies in users' language, GPT performs less effectively when judging the status of step trees based on natural language descriptions, particularly when identifying sequence change errors. That said, since sequence changes occur very infrequently, we believe that the current capabilities of GPT-4 are sufficient to assess students' thought processes and provide guidance. As LLMs continue to improve in the future, using them to understand and identify users' thought processes should become a straightforward and practical application.






\begin{table*}[hbpt]
% the environment \color{blue} change all cell color
	\centering
	\caption{The technical evaluation of GPT-4o assesses its ability to identify the status of learners' steps. \textbf{Precision} refers to the proportion of steps correctly predicted as correct by GPT. \textbf{TPR} (True Positive Rate) measures the proportion of truly correct steps that GPT identifies correctly. \textbf{TNR} (True Negative Rate) reflects the proportion of truly incorrect/missing steps that GPT correctly predicts. \textbf{FPR} (False Positive Rate) indicates the proportion of incorrect/missing steps that GPT incorrectly predicts as correct. \textbf{FNR} (False Negative Rate) represents the proportion of correct steps that GPT incorrectly predicts as incorrect/missing.}
% 	~\glcomment{To check whether we should conduct such analysis for explanation effect. Or do ablation-like, compare with explanation without explanation?(with explanation, with DKE vs without DKE or with explanation, with DKE vs without DKE, with explanation)}}
	\label{tab:technicalresult}%
	\begin{small}
	\begin{tabular}{c | c c c c c c c}
	    \hline
	   % Hypothesis&	\multicolumn{4}{c|}{\textbf{H1}}& \multicolumn{4}{c}{\textbf{H2}} \\
	    % \textbf{Participants}&	\multicolumn{5}{c|}{\textbf{All}}& Post-hoc results\\
	    % \hline
	    \textbf{Error Type}&Accuracy&F1&Precision&TPR/Recall&TNR/Specificity&FPR&FNR\\
	    \hline
 \hline
\multicolumn{8}{l}{\textbf{Identify step/substep status from learners' natural language-based step descriptions}}\\
\hline    
	    \textbf{Sequence Changed}&0.70&0.72&0.68&0.76&0.64&0.36&0.24\\
	
 \rowcolor{gray!15}\textbf{Logical Error}&0.88&0.87&0.91&0.84&0.92&0.08&0.16\\
	    \textbf{Missing}&0.86&0.86&0.85&0.88&0.84&0.16&0.12 \\

 \hline
\multicolumn{8}{l}{\textbf{Identify step/substep status from learners' codes}}\\
\hline
    \rowcolor{gray!15}\textbf{Sequence Changed}&1.00&1.00&1.00&1.00&1.00&0.00&0.00\\

	\textbf{Logical Error}&0.98&0.98&0.96&1.00&0.96&0.04&0.00\\

 
    \rowcolor{gray!15}\textbf{Missing}&0.92&0.92&0.92&0.92&0.92&0.08&0.08\\


    \textbf{Syntax Error}&0.90&0.90&0.88&0.92&0.88&0.12&0.08\\
    % \textbf{TiA-Trust}& 3.93& .140& $3.00 \pm 0.81$& $3.22 \pm 0.87$& $3.11 \pm 0.85$& -\\
	    \hline
	\end{tabular}%
	\end{small}
\end{table*}






