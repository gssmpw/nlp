

\section{Formative Study}
In addition to traditional programming support tools like LeetCode, search engines, and Q\&A sites like Stack Overflow, AI-assisted tools such as ChatGPT and GitHub Copilot have further enriched these resources. 
However, it remains unclear whether these tools effectively enhance algorithmic programming learning and whether challenges persist despite their availability.
To explore this, we conducted a formative study using contextual inquiry and interviews to understand learner's needs and obstacles. 
Our study focuses on students who have completed foundational computer science courses and are working to improve their algorithmic problem-solving skills. 
%We are not focusing on K-12 students or novice learners just beginning to understand programming logic. Instead, our goal is to support students who already possess basic programming knowledge but seek to improve their problem-solving abilities with algorithms. 

%LeetCode\footnote{https://leetcode.com/}, a widely-used platform for honing algorithmic skills, exemplifies this focus.

% \subsection{Materials}
% To ground participants' feedback in recent experiences, we provided unrestricted access to various representative support tools, including Leetcode, Search Engines, ChatGPT, Github Copilot, Stake Overflow, etc. A detailed description of these tools can be seen in Sec xxx in Appendix.


%Participants had unrestricted access to these tools to explore the challenges in algorithmic programming learning, despite the wide availability of learning support resources.



% \section{Formative Study}


% With the proliferation of programming support tools, students have access to extensive resources ranging from online platforms like LeetCode, to search engines such as Google and Bing, and Q\&A sites like Stack Overflow. Additionally, innovations in large language models (LLMs) have introduced tools like ChatGPT and GitHub Copilot. Despite this wealth of options, questions persist about their actual effectiveness in the programming learning process. Are these tools genuinely aiding learning? Do students still encounter challenges in algorithmic programming despite these resources? What specific issues arise? To address these questions, we conducted a formative study using contextual inquiry and interviews to better understand the needs and hurdles faced by learners.


% \subsection{Scope of Our Target Users}
% Before embarking on our formative study, it is crucial to define the scope of our target users precisely. We are not focusing on K-12 students learning basic programming logic or novice learners in introductory computer science (CS1). Our target group comprises students who have already acquired the foundational knowledge from CS1 and are now aiming to improve their ability to apply algorithms in solving real-world problems. Our goal is to aid these students in effectively utilizing algorithms to tackle specific challenges, rather than teaching the basic concepts and principles. A prime example of this focus is LeetCode\footnote{https://leetcode.com/}, an online platform extensively used by students and professionals to hone their algorithmic skills.

% \subsection{Materials} Despite our participants' prior experience with algorithm programming, we required them to re-engage with the process to ensure their feedback was based on the most current experiences. We facilitated this by providing access to various representative support tools and having participants familiarize themselves with their latest features. We combined contextual inquiry with post-hoc interviews to collect their insights. The materials prepared for their practice in algorithm programming included:

% \begin{itemize} \item \textbf{Programming Learning Platform: LeetCode} - LeetCode features a comprehensive problem bank with over 1,000 questions, each comprising a problem description, an editor, and a results area with test cases. We selected problems that offer extensive solution resources, allowing participants to access rich references through the \emph{editorial panel} (official solutions) or the \emph{solution panel} (community-contributed solutions). \item \textbf{Search Engines: Google and Bing} - Participants can freely search anything related to the programming problem. \item \textbf{Conversational LLM Tool: ChatGPT\footnote{https://chat.openai.com/}} - Participants were provided with accounts to access ChatGPT with GPT-4o model, enabling them to ask programming questions, seek code assistance, get auto-completion suggestions, and address syntax errors using natural language via an accessible web interface. \item \textbf{Pair Programming Tool: GitHub Copilot\footnote{https://github.com/features/copilot}} - Integrated with code editors such as Visual Studio Code, GitHub Copilot offers real-time coding suggestions including snippets, functions, and complete blocks, thus enhancing the coding process. We provided participants with GitHub Codespaces\footnote{https://github.com/features/codespaces} integrated with Copilot for their use. \item \textbf{Programming Q\&A Platform: Stack Overflow} - A community-driven platform where developers exchange coding-related queries and solutions, Stack Overflow serves as a crucial resource for resolving technical problems and acquiring new programming skills. \end{itemize}

% To thoroughly investigate the challenges learners still face despite these resources, we granted participants unrestricted access to all provided tools.

\subsection{Study Procedure}
We recruited 15 university students (5F, 10M, aged 18-29), including 10 undergraduates and 5 graduate students. 
Most (11) were computer science majors, with the rest from related fields such as data science and electrical engineering. 
All participants had prior experience with LeetCode or similar platforms, 14 had used ChatGPT for programming tasks, and eight had experience with GitHub Copilot or similar tools. 
Each session lasted 40 minutes with \$10 compensation.

During the study, participants used tools like LeetCode, search engines, ChatGPT, and GitHub Copilot to solve a randomly assigned algorithmic problem for 20 minutes, during which we observed their tool usage and conducted contextual inquiries as notable behaviors arose. Afterward, we reviewed their activities and conducted a semi-structured interview to discuss their tool choices, usage, assistance received, and perceptions of the tools, including any benefits or drawbacks they experienced.

\subsection{Data Analysis and Results}
We conducted inductive thematic analysis \cite{hsieh2005three} on interviews and contextual inquiry data. Recorded sessions were transcribed and manually reviewed and corrected by the first author. 
Two authors then independently coded the data and discussed to finalize themes and categorizations. 
The analysis revealed four key challenges students face with existing assistance tools during algorithmic programming learning.

% \subsubsection{\textbf{Challenge 1: Excessive Help Hindering Active Learning}} Students expressed concerns about how easily accessible solutions on platforms like LeetCode and ChatGPT hinder independent problem-solving. Most of the time, learners only need minimal guidance to overcome specific hurdles, but these tools frequently provide complete solutions, which can prematurely reveal answers and prevent independent thinking. As P1 noted, ``\emph{I just wanted help with the step I'm stuck on, but the solution panel [in Leetcode] showed everything, making it hard to ignore what I didn't need.}'' Similarly, P3 shared, ``\emph{Even if I rewrote the code after seeing the solution, it felt uninteresting and unfulfilling.}'' Participants also found ChatGPT provided too much assistance. P5 explained, ``\emph{ChatGPT often directly showed the complete solution and code. Once I saw the solution, it was hard to ignore, and it felt like I was not really improving my programming skills.}''

% \subsubsection{\textbf{Challenge 2: Difficulty in Utilizing ChatGPT for Effective Learning}} Participants found it challenging to use ChatGPT as a learning tool, despite its accessibility and coding capabilities. Many felt it wasn't designed to support active learning. P10 shared, ``\emph{Even though I just asked GPT about the specific problem I was facing, ChatGPT went straight to showing the full solution and code after answering my question, which deprives me of the chance to solve it on my own. Writing the code after seeing the solution feels like cheating.}'' P12 noted the effort required to make ChatGPT useful for learning: ``\emph{To get it to help me learn rather than just solve the problem, I have to carefully craft prompts. This is cumbersome, so I often just copy the problem description directly into ChatGPT. }''

\ms{
\subsubsection{\textbf{Challenge 1: Excessive Help Hindering Active Learning}}
Students expressed concerns that platforms like LeetCode and ChatGPT provide solutions that are too easily accessible, hindering their ability to engage in independent problem-solving. 
While learners typically need minimal guidance to overcome specific hurdles, these tools often present complete solutions prematurely, making the learning experience ``\emph{uninteresting}'' and ``\emph{unfulfilling}'' (P3). As P1 noted, ``\emph{I just wanted help with the step I'm stuck on, but the solution panel [in Leetcode] showed everything, making it hard to ignore what I didn't need.}'' Participants also found it difficult to use ChatGPT as a learning tool, as it often provided full solutions rather than encouraging active engagement. P5 explained, ``\emph{ChatGPT often directly showed the complete solution and code, even though I just asked GPT about the specific problem I was facing. Once I saw the solution, it was hard to ignore it. It felt like I was not really improving my programming skills.}'' Additionally, participants like P12 noted that making ChatGPT useful for active learning required significant effort to ``carefully craft prompt'', as a result they ``\emph{just copy the problem description directly into ChatGPT}''.
}


% \subsection{Results}
% Through our qualitative analysis, we identified several challenges that students face when using existing programming assistance tools to learn algorithm programming.

% \subsubsection{Challenge 1: Receiving Excessive Help That Prevents Learners from Active Learning}
% Students found it too easy to access complete solutions. While LeetCode's editorial and solution features often provide high-quality solutions, organized progressively from intuition to steps to final code, they frequently reveal the entire solution too quickly, limiting students' ability to think independently. The lack of targeted assistance for specific difficulties means that students, regardless of where they get stuck—whether at the beginning, middle, or end of the problem-solving process—are often presented with the full answer. In fact, learners often need just simple guidance. Sometimes students are stuck on a nuanced aspect and only need a little guidance to proceed. However, the current methods provide the entire correct procedure, often including the complete solution and code. When receiving such help, students don't feel that they are the ones solving the problem. They doubt whether they have really mastered the problem and question the effectiveness of their practice and learning. For instance, P1 stated, "I just want to know how to get past the step I'm stuck on, but the solution panel shows all the steps. I try to focus only on what I need, but it's hard not to see everything at once." P3 mentioned, "I want to solve the problem on my own. Even if I go back to the editor and write the code after seeing the answer, it feels uninteresting and gives me no sense of achievement."

% Similar issues arose with tools like ChatGPT. Participants noted that ChatGPT often provided too much help, which impeded their learning process. For example, P5 shared, "When I described the problem and the difficulties I was facing to ChatGPT, it immediately told me how to solve my issue and provided the next steps, even offering complete code. I tried to tell myself not to look at the answers it gave, but I couldn't resist. After seeing the solution, when I returned to the editor to write my own code, I felt like my programming skills weren't being properly exercised."










% (ZPD) Overwhelming Answers from GPT: Asking GPT often results in receiving too much information, including both the thought process and the code. Unless participants carefully crafted their instructions to GPT, they found it exhausting to manage, as they didn't want to simultaneously think about solving the problem and how to better prompt GPT.



% \subsubsection{Challenge 3: Provided Solutions or How to Decompose the Solution May Not Align with the Approach that Learners are Exploring}
% When learners view solutions from Leetcode's solution panel or editorial panel, the solutions provided were not what learners wanted to use. From a high level, learners couldn't find a solution that aligned with their approach. From a detailed level, even the general idea was similar (e.g., both the learners and the provided solutions adopt a binary search algorithm), but the detailed steps and detailed approach differed significantly. Students preferred to continue solving the problem in their own way rather than adopting someone else's method. Moreover, students wanted to construct their own mental model of the problem-solving structure, with the tools serving as aids and guides rather than taking away their control by providing another solution. For example, P5 said, "I want to follow my own idea. The solution's approach is different from mine. I just got stuck and need help specific to my approach. I don't want to adapt my thinking to someone else's solution, even if my solution is not the best one."


% \subsubsection{Challenge 3: Misalignment Between Provided Solutions and Learners' Approaches} Learners often find that the solutions available on LeetCode's solution or editorial panels do not match their intended approach. At a high level, they may struggle to find a solution that aligns with their strategy. Even when the overarching concept is similar, such as both using a binary search algorithm, the specifics of the approach can differ greatly. Students generally prefer to pursue their solution paths, aiming to develop their mental model of the problem-solving process, with tools serving more as aids than prescriptive solutions. For instance, P5 stated, "I prefer to stick to my own ideas. The provided solution takes a different approach, and I just need help tailored to my method. I don't want to adjust my thinking to fit someone else's solution, even if it might be superior."

\ms{
\subsubsection{\textbf{Challenge 2: Misalignment Between Provided Solutions and Learners' Own Problem-Solving Approaches}}
Students often struggled when provided solutions failed to align with their intended approaches or existing code. 
While learners preferred developing their own strategies, the solutions offered by platforms like LeetCode, search engines, or ChatGPT were often prescriptive that mismatched their efforts. 
Even when algorithms matched, differences in specifics made integration challenging, especially with partial or incorrect code. 
Rather than starting over, learners wanted help tailored to their approach and context. As P5 explained, ``\emph{I prefer sticking to my own ideas. The provided solution takes a different approach, and I just need help tailored to my method. I don’t want to change my thinking to fit someone else's solution, even if it's better.}'' Similarly, P8 noted the frustration of aligning external solutions with her own work: ``\emph{I don't want to scrap my code and start anew, so I try to align the solution with my code line by line. It’s tedious to figure out which parts of the solution relate to what I’ve written, where my errors began, and what I missed.}''
}

% \subsubsection{\textbf{Challenge 3: Misalignment Between Provided Solutions and Learners' Approaches}} Students often found that solutions on LeetCode's solution or editorial panels did not align with their intended approach. Even if the overall algorithm used was consistent, the specifics might vary greatly. Learners preferred to follow their own strategies and develop their own problem-solving mental models, using tools as aids rather than prescriptive solutions. As P5 explained, ``\emph{I prefer sticking to my own ideas. The provided solution takes a different approach, and I just need help tailored to my method. I don’t want to change my thinking to fit someone else's solution, even if it's better.}''

% \subsubsection{\textbf{Challenge 4: Disconnect Between Existing Code and Provided Solutions}}

% All 15 participants initially attempted to solve the problem on their own, often producing incomplete or incorrect code. When seeking help—from LeetCode, search engines, Stake Overflow, or ChatGPT—they found the guidance rarely aligned with their existing code. This made it difficult to integrate the newly received assistance with their previous problem-solving efforts. Existing tools overlooked the context of their code, leaving students feeling like they had to start from scratch. Students preferred to build on their initial attempts, but aligning the provided solutions with their code was time-consuming. As P8 noted, ``\emph{I don’t want to scrap my code and start anew, so I try to align the solution with my code line by line. It’s tedious to figure out which parts of the solution relate to what I’ve written, where my errors began, and what I missed.}''






% \subsubsection{Challenge 4: Lack of a Close Connection Between Learners' Existing Code and the Solution}

% From our observations, regardless of their progress in solving the problem, all 15 students typically chose to attempt solving it on their own first. They would try writing some code in the editor, even though this code was often incomplete or incorrect. When they encountered difficulties and sought help—whether from LeetCode's built-in solution, search engine results, or ChatGPT—the solutions provided were disconnected from the students' existing attempts. The students struggled to clearly relate these solutions to the code they had already written. Even when students pasted their incomplete code into a ChatGPT conversation, the responses often failed to maintain this connection. As a result, students felt as though they were starting over in a new editor, rather than continuing to build upon their existing work. They preferred to return to their editor and improve their unfinished code, but this required significant effort to painstakingly match the provided solution with their existing code. For instance, P8 said: "I don't want to delete my code and start over, so I have to compare the solution line by line with my code. I need to figure out which part of the feedback or solution corresponds to the code I’ve already written, which parts I got right, where I started to go wrong, and what I missed in my previous attempt. It’s extremely time-consuming."


% \subsubsection{Challenge 4: Disconnect Between Existing Code and Provided Solutions}

% Our study revealed that all 15 participants initially attempted to solve the assigned problem by themselves, writing some code that was often incomplete or incorrect. When they sought help—whether from LeetCode’s solutions, search engine queries, or ChatGPT—the guidance they received generally did not connect well with their existing code. This made it difficult for students to integrate new solutions with their prior efforts effectively. Responses, particularly from ChatGPT, often did not account for the context of the students' existing code, making it feel as though they were starting afresh rather than building on what they had already written. Students expressed a preference to continue refining their initial attempts, but aligning the provided solutions with their code proved to be laborious and time-consuming. For example, P8 commented: "I don’t want to scrap my code and start anew, so I try to align the solution with my code line by line. It’s challenging to discern which parts of the solution or feedback relate to what I’ve written, identify my correct segments, detect where errors began, and understand what I missed. This process is incredibly tedious."





% \subsubsection{Challenge 5: Lack of a Structured Problem-Solving Process}

% Students expressed a need for a structured approach to problem-solving, one that would allow them to break down the problem from a broad overview into finer details. When tackling more complex algorithmic problems, students often split the problem-solving process into several manageable steps. These might include handling initialization and edge cases, managing loops within arrays, and processing return values. This reflects a key aspect of computational thinking [ref]. However, the assistance provided by current tools is usually presented as a large block of text or code. Even well-organized solutions that use bullet points to outline the steps often get buried in long paragraphs, lacking a clear, structured visual presentation (see Appendix A). For example, P13 said: "When I read through the solution, it tells me there are six steps, but the specific reasoning and code that follow are not organized according to these steps. It just informs me upfront that there are six steps. The goals, actions, and corresponding code for each step are all mixed together. It’s challenging to build a clear, structured thought process from such a lengthy solution." Furthermore, six students found that while using ChatGPT, they did not simply throw the entire problem at GPT but instead engaged in a step-by-step interaction, asking specific questions as they encountered difficulties. However, they discovered that resolving a single issue often required multiple rounds of conversation, which were then followed by discussions of another challenge. This made it easy for them to lose track within the lengthy conversation. P5, for instance, complained: "I tried asking GPT some questions, and it provided good answers, but after asking several questions, I realized I had engaged in pages of conversation. When I tried to tackle a specific problem, I had to scroll back through the conversation to find what I had discussed with GPT. After asking GPT for help, I still felt confused. I seemed to have received answers to each of my questions, but I couldn’t organize them into a coherent thought process."



\subsubsection{\textbf{Challenge 3: Lack of a Structured Problem-Solving Approach}}

Students highlighted the need for a structured method to break down complex algorithmic problems into manageable steps, such as initialization, handling edge cases, loops, and return values. Current tools often present solutions as large blocks of text or code without clear visual structure. As P13 noted when using LeetCode’s solution panel, ``\emph{The solution outlines six steps, but the content is very disorganized and the text is unclear, making it hard to follow a structured process.}'' Some participants preferred step-by-step interactions with ChatGPT, asking specific questions as they progressed. However, resolving issues often required lengthy, multiple rounds of conversations, making it difficult to stay organized. Learners frequently had to scroll back and forth to find points related to specific steps. As P15 (using ChatGPT) shared, ``\emph{I had a detailed conversation with GPT spanning several pages. Scrolling back to revisit earlier points was cumbersome, and despite getting answers, it was hard to organize them into a coherent thought process}.''




% \subsubsection{Challenge 5: Lack of a Structured Problem-Solving Approach}

% Students highlighted the need for a more structured approach to problem-solving that would help them decompose complex algorithmic challenges into manageable steps. Such steps often include handling initialization and edge cases, managing loops within arrays, and processing return values, reflecting essential elements of computational thinking. However, current tools typically present assistance as large blocks of text or code, with well-intended solutions embedded in lengthy paragraphs lacking clear, structured visual guidance. For example, P13 noted, "The solution outlines six steps, but it fails to organize the reasoning and code within these steps effectively; it all blends together, making it hard to follow a structured thought process."

% Furthermore, interactions with ChatGPT revealed that students preferred a step-by-step engagement, asking specific questions as they progressed through problems. However, resolving issues often required multiple rounds of conversation, leading to lengthy discussions that made it easy to lose track. P5 expressed frustration: "I engaged in a detailed conversation with GPT that spanned several pages. Trying to address specific issues became cumbersome as I had to scroll back to revisit earlier parts of the conversation. Despite receiving answers to my questions, organizing them into a coherent thought process remained challenging."


% \subsubsection{Challenge 6: Lack of Fine-Grained Evaluation of Learners' Thoughts}

% Currently, the run code feature on LeetCode executes the entire code and only indicates success when the student's code is entirely correct. Regardless of how close the student is to the correct solution or how many errors remain, any deviation from correctness results in the same "wrong" feedback. When students encounter a problem and seek help, they want to immediately verify whether they understand the solution. However, the current code verification method cannot provide fine-grained feedback on whether specific steps are correct, preventing students from breaking down the problem into smaller cycles of attempting, evaluating, receiving feedback, and revising. Additionally, students cannot track their progress in implementing their thought process. For instance, P8 mentioned: "I want to know whether this particular step is correct. Only if this step is correct will I know how to proceed. If this step is wrong, I might need to start over. But I can’t get any feedback on this step, so I have to assume it’s correct and keep going, only to realize at the end that this step was actually wrong. I should have been told earlier." P9 added: "This problem is challenging. I need to manage every step and scenario correctly, but I can’t tell how many subcases I’ve handled correctly since there’s no progress indicator."

% Moreover, students want to know whether their thought process is correct. Before writing code, students usually have an initial idea of how to solve the problem. Ten students expressed a desire for a feature that could help them assess whether their approach is correct, which would boost their confidence in writing code or prompt them to adjust their approach sooner. For example, P1 noted: "A correct thought process is the foundation of correct code. When I have an uncertain idea, I don’t want to spend time implementing it until I know it’s correct. Only if my approach is correct do I feel that continuing to write code is meaningful. But currently, I can’t find any support to check the correctness of my approach."

\subsubsection{\textbf{Challenge 4: Insufficient Fine-Grained Feedback on Learners' Coding Progress}}

LeetCode's ``run code'' button evaluates the entire solution and only provides feedback on the overall correctness. This all-or-nothing approach overlooks partial correctness and fails to highlight specific errors, making it difficult for students to track their progress or verify their understanding step by step. For example, P8 noted, ``\emph{I need to know if a step is correct before I can decide how to proceed. Without step-by-step feedback, I can only keep doing it until I complete all the steps.}'' Additionally, students wanted early validation of their thought process before committing to code. Ten participants expressed the need for a feature to confirm if their approach was on the right track. As P1 stated, ``\emph{The correctness of my initial thought process is crucial. I hesitate to implement code until I'm confident it's correct. There's no tool to validate my approach early on.}''




% However, the current help resources are primarily presented in large blocks of text or code, without the visual structure that could help users clarify their thoughts. 



% Interactive Engagement with Help Information: Students wanted the ability to interact with the help information. Currently, LeetCode does not support such interactivity, and while search engines and ChatGPT allow for question-and-answer interactions, students desired more ways to actively engage with the help information to better obtain targeted assistance.



% (Personalization) Lack of Trial-and-Error Process: Students wanted to experiment with solving a particular step, but the current evaluation feedback mechanism only runs the entire code at once. Often, students just wanted to know if their approach for a specific step was correct. "I want to know if my approach to this step is right; if it is, I’ll have a good idea of what to do next, but I can’t find any mechanism that gives feedback on the correctness of my steps," one participant said.

% Time-Consuming Description of Issues to GPT: Getting targeted help by describing difficulties to GPT was time-consuming. Students not only needed to describe the problem but also their current thought process, incomplete code, the specific step where they were stuck, and the issues they were encountering. Students felt this was too time-consuming, so they tended to paste the problem description into GPT rather than taking the time to articulate their thoughts and problems clearly.




% Natural Language as a Thought Process: Students needed to first form their thoughts in natural language as it serves as a good medium before the idea fully takes shape.




% Low-Quality Guidance for Some Issues: Some participants encountered programming problems for which the solution panel only provided direct solution code without a step-by-step high-level explanation. This made it difficult for them to understand and hindered their ability to try coding on their own after grasping the thought process.





% \subsubsection{Design Goals}
% Based on the user needs identified in the formative study, we established the following design goals:

% \begin{itemize} \item \textbf{Goal 1: Scaffolding to Encourage Learners' Active Learning and Independent Thinking}:
% Addressing the first and second challanges, our tool should incorporate scaffolding functions aligned with the Zone of Proximal Development (ZPD). The system should not overwhelm students by presenting all answers at once but rather provide just the right amount of support based on the specific challenges they face. The assistance offered by the system should primarily foster independent thinking, providing appropriate guidance while still requiring students to engage in their own problem-solving to arrive at the answers. Our system should encourage students to first form their own ideas, then offer feedback that guides them in refining and improving their thought processes.

% \item \textbf{Goal 2: Personalization to Individual's Solution}:
% Addressing the third challange, our system should respect each student's unique problem-solving approach, rather than imposing a one-size-fits-all solution. The system should understand how students approach the problem, how they decompose the solution into steps and sub-steps, and where they encounter difficulties. While preserving the students' original thought processes as much as possible, the system should provide personalized feedback and guidance tailored to the specific issues each student encounters.

% \item \textbf{Goal 3: Connection and Structured Solution}:
% Addressing the fourth and fifth challanges, our system should create a clear visual mapping between the assistance provided and the student's existing code, allowing students to easily identify how each part of their thought process corresponds to specific problems and feedback. Additionally, the interface design should align with the student's step-by-step problem-solving mental model, offering a structured visual representation that helps students gradually build a complete mental model of the solution. This design will help students maintain a clear overview of the problem-solving process and avoid losing track of their progress.

% \item \textbf{Goal 4: Fine-Grained Evaluation and Feedback}:
% Addressing the sixth challange, our system should be capable of verifying the correctness of a student's thought process, whether it is expressed through code, pseudocode, or even natural language. Furthermore, the system should provide feedback on the accuracy of each individual step, helping students monitor the correctness of their thought process in real time. This will enable students to establish a flexible cycle of attempting, evaluating, receiving feedback, and revising their approach. \end{itemize}




\subsection{Design Goals} \label{designgoal} Based on the challenges identified in our formative study, we established the following design goals for our tool:

\begin{itemize}
    \item \textbf{D1: Scaffolding for Active Learning and Independent Thinking}: To address Challenge 1, our system should provide scaffolding support. Instead of presenting complete solutions, it should offer tailored support that encourages active problem-solving and independent thinking.
    
    \item \textbf{D2: Personalization to Individual Problem-Solving Styles}: In response to Challenge 2, our system should adapt to each student's unique problem-solving approach by analyzing how they break down problems and offering personalized feedback that preserves and enhances learners' own strategies.

    \item \textbf{D3: Connection and Structured Solution Presentation}: To address Challenge 2\&3, our system should visually connect the system-generated guidance to students’ existing codes and solutions, promoting a structured problem-solving approach that aligns with their mental models.

    \item \textbf{D4: Fine-Grained Evaluation and Feedback}: Addressing Challenge 4, our system should evaluate the correctness of students' thought processes—whether conveyed through code, pseudocode, or natural language—and provide detailed, step-by-step feedback to support continuous improvement.
\end{itemize}



% \subsubsection{Design Goals} Based on the challenges identified in our formative study, we have established specific design goals for our tool:

% \begin{itemize} \item \textbf{Goal 1: Scaffolding for Active Learning and Independent Thinking}: In response to the first and second challenges, our system needs to implement scaffolding aligned with the Zone of Proximal Development (ZPD). It aims to avoid overwhelming students by presenting complete solutions upfront, instead providing the right level of support tailored to their specific needs. This approach encourages students to engage actively in problem-solving, forming their own ideas and receiving guidance that helps refine their thought processes.

% \item \textbf{Goal 2: Personalization to Individual Problem-Solving Styles}: Addressing the third challenge, our system needs to respect and adapt to each student's unique problem-solving approach. It will analyze how students break down problems into steps and sub-steps and where they encounter difficulties, offering personalized feedback that preserves and enhances their original strategies.

% \item \textbf{Goal 3: Connection and Structured Solution Presentation}: In response to the fourth and fifth challenges, our system needs to ensure that assistance provided is visually connected to the student’s existing code. This will help students relate specific parts of the solution to their ongoing work, facilitating a structured problem-solving approach that mirrors their mental model and promotes a clear understanding of the entire process.

% \item \textbf{Goal 4: Fine-Grained Evaluation and Feedback}: Addressing the sixth challenge, our system should be able to evaluate the correctness of a student's (incomplete) thought process—whether conveyed through code, pseudocode, or natural language—and provide detailed feedback on each step. This feature will support a continuous cycle of attempting, evaluating, and revising, enabling students to track and improve their approach in real-time. \end{itemize}