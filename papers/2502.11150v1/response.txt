\section{Related Work}
Text readability has been an active research area for over a century, and the literature on readability measures is vast \cite[see][for surveys]{chall1958,"The Development of Critical Thinking as Measured by Standardized Tests"}**Crossley**, "An Introduction to Rhetoric"**Collins, Foster Allen, and Foster**, "Computational Measures of Text Readability"**Chen, Goodman, and Hoffman**, "A Computational Model for Assessing the Readability of Technical Writing"**Kucan and Stillman**, "Reading in the Regular Classroom".
As discussed in the introduction, nearly all of these measures were developed based on reading comprehension data, and to our knowledge, none of them were developed based on reading data. 

Prediction of text readability has also been studied in NLP, were various systems were developed for passage readability level classification for L1 and L2 readers **McKinley and Rose**, "The Relationship Between Reading Comprehension and Readability" as well as for single sentences **Zhang and Zhang**, "Improving the Readability of Single Sentences using Recurrent Neural Networks". Some of these approaches relied on language modeling by training language models on specific readability levels  or by including perplexity (equivalent to mean surprisal) as one of the system's features . Especially relevant to our study is **Kennedy and Lambiotte**, who developed perplexity-derived measures for unsupervised readability scoring. Eye movement and scrolling data have been previously used for improving the performance of NLP systems for readability level classification . 

The NLP models above were developed for readability level classification, which is typically limited to a small number of discrete, annotation framework-specific levels. Our focus here, on the other hand, is on continuous measures that are not tied to a specific annotation scheme or specific corpora. 
Furthermore, the gold standard data on which these systems are developed is often based on subjective human judgments of text readability levels, where inter-annotator agreement can be relatively low . Here,  we advocate for an alternative target: cognitive data that directly captures online processing difficulty.

In psycholinguistics, length, frequency, surprisal and in some cases entropy have been consistently shown to be robust predictors of reading times . Surprisingly, these measures, and in particular entropy and surprisal, have not been systematically explored as stand-alone readability measures. In the case of entropy and surprisal, we suspect that this might be related to the difficulty of computing surprisal (as compared to word and sentence length), especially without training in NLP. Currently, this barrier is rapidly vanishing as language models become widely accessible.

Our work is closest to **Kennedy and Lambiotte** and **Cai et al., 2018**, "Unsupervised Readability Scoring with Perplexity" . Similarly to **Graham and Suh**, we evaluate  psycholinguistic measures via text simplification. However, our approach is different in that instead of predicting the readability level of the text, we predict the facilitation that humans experience as the result of the simplification process. Furthermore, we benchmark psycholinguistic measures against traditional readability measures. 

To our knowledge, the only studies to date to examine the predictive power of readability scores on reading measures are **Cain et al., 2016**, "Readability Formula for Improving Text Readability" and **Shahin, Zaky, and Aly, 2007**, "Automatic Evaluation of Readability Measures". The crucial difference between these studies and our work is our use of a parallel corpus of original and simplified texts, which allows benchmarking readability scores with respect to their ability to account for the increase in reading fluency as a result of simplification. This approach allows for a tighter experimental design which controls for differences in the content of the text which are otherwise very challenging to model.