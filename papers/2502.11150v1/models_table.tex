\begin{table}[ht!]
\centering
\begin{tabular}{|c|l|l|l|l|l|}
\hline
\textbf{\#} & \textbf{Model Name} & \textbf{Model Family} & \textbf{Model Size} & \textbf{PPL} & \textbf{Model Full Name} \\ \hline
1  & Pythia 70M                  & Pythia               & 70M                 & 48.99        & EleutherAI-pythia-70m     \\ \hline
2  & Pythia 160M                 & Pythia               & 160M                & 32.87        & EleutherAI-pythia-160m    \\ \hline
3  & Pythia 410M                 & Pythia               & 410M                & 22.61        & EleutherAI-pythia-410m    \\ \hline
4  & Pythia 1B                   & Pythia               & 1B                  & 19.43        & EleutherAI-pythia-1b      \\ \hline
5  & Pythia 1.4B                 & Pythia               & 1.4B                & 17.70        & EleutherAI-pythia-1.4b    \\ \hline
6  & Pythia 2.8B                 & Pythia               & 2.8B                & 15.83        & EleutherAI-pythia-2.8b    \\ \hline
7  & Pythia 6.9B                 & Pythia               & 6.9B                & 14.63        & EleutherAI-pythia-6.9b    \\ \hline
8  & GPT-2 117M                  & GPT-2                & 117M                & 28.29        & gpt2                      \\ \hline
9  & GPT-2 345M                  & GPT-2                & 345M                & 21.35        & gpt2-medium               \\ \hline
10 & GPT-2 774M                  & GPT-2                & 774M                & 18.76        & gpt2-large                \\ \hline
11 & GPT-2 1558M                 & GPT-2                & 1558M               & 16.90        & gpt2-xl                   \\ \hline
12 & GPT-J 6B                    & GPT-J                & 6B                  & 14.77        & EleutherAI-gpt-j-6B       \\ \hline
13 & GPT-Neo 125M                & GPT-Neo              & 125M                & 33.20        & EleutherAI-gpt-neo-125M   \\ \hline
14 & GPT-Neo 1.3B                & GPT-Neo              & 1.3B                & 19.58        & EleutherAI-gpt-neo-1.3B   \\ \hline
15 & GPT-Neo 2.7B                & GPT-Neo              & 2.7B                & 17.53        & EleutherAI-gpt-neo-2.7B   \\ \hline
16 & Llama-2 7B                  & Llama-2              & 7B                  & 9.05         & meta-llama/Llama-2-7b-hf  \\ \hline
17 & Llama-2 13B                 & Llama-2              & 13B                 & 8.40         & meta-llama/Llama-2-13b-hf \\ \hline
18 & OPT 350M                    & OPT                  & 350M                & 25.54        & facebook/opt-350m         \\ \hline
19 & OPT 1.3B                    & OPT                  & 1.3B                & 18.32        & facebook/opt-1.3b         \\ \hline
20 & OPT 2.7B                    & OPT                  & 2.7B                & 16.74        & facebook/opt-2.7b         \\ \hline
21 & OPT 6.7B                    & OPT                  & 6.7B                & 15.08        & facebook/opt-6.7b         \\ \hline
22 & Mistral-v0.1 7B             & Mistral              & 7B                  & 9.21         & mistralai/Mistral-7B-v0.1 \\ \hline
23 & Mistral-v0.3 7B             & Mistral              & 7B                  & 9.31         & mistralai/Mistral-7B-v0.3 \\ \hline
24 & Gemma 7B                    & Gemma                & 7B                  & 11.55        & google/gemma-7b          \\ \hline
25 & Gemma-2 9B                  & Gemma                & 9B                  & 12.08        & google/gemma-2-9b        \\ \hline
26 & Recurrent-Gemma 9B          & Gemma                & 9B                  & 11.77        & google/recurrentgemma-9b  \\ \hline
27 & RWKV-4 169M                 & RWKV                 & 169M                & 28.06        & RWKV/rwkv-4-169m-pile     \\ \hline
28 & RWKV-4 430M                 & RWKV                 & 430M                & 21.54        & RWKV/rwkv-4-430m-pile     \\ \hline
29 & Mamba 370M                  & Mamba                & 370M                & 19.92        & state-spaces-mamba-370m-hf \\ \hline
30 & Mamba 790M                  & Mamba                & 790M                & 17.35        & state-spaces-mamba-790m-hf \\ \hline
31 & Mamba 1.4B                  & Mamba                & 1.4B                & 15.92        & state-spaces-mamba-1.4b-hf \\ \hline
32 & Mamba 2.8B                  & Mamba                & 2.8B                & 14.42        & state-spaces-mamba-2.8b-hf \\ \hline
\end{tabular}
\caption{Models Information: name, family, model size, perplexity (PPL) measured on OneStop Eye Movements dataset, and the corresponding Hugging Face (\url{https://huggingface.co/}) model identifier.}
\label{tab:models_table}
\end{table}
