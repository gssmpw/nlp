\section{Related Work}
Text readability has been an active research area for over a century, and the literature on readability measures is vast \cite[see][for surveys]{chall1958,crossley2011text,collins2014computational}.
As discussed in the introduction, nearly all of these measures were developed based on reading comprehension data, and to our knowledge, none of them were developed based on reading data. 

Prediction of text readability has also been studied in NLP, were various systems were developed for passage readability level classification for L1 and L2 readers \citep[][among others]{heilman2007combining,pitler2008revisiting,kate2010learning,vajjala2012improving,filighera2019automatic,feng2010comparison,vajjala-2018-onestopenglish,arase-etal-2022-cefr} as well as for single sentences \citep{vstajner2017automatic,brunato2018sentence,liu2025automatic}. Some of these approaches relied on language modeling by training language models on specific readability levels \cite{si2001statistical,collins2004language,schwarm2005reading} or by including perplexity (equivalent to mean surprisal) as one of the system's features \cite{xia2019text}. Especially relevant to our study is \citet{martinc2021supervised}, who developed perplexity-derived measures for unsupervised readability scoring. Eye movement and scrolling data have been previously used for improving the performance of NLP systems for readability level classification \cite{gonzalez2018learning,gooding2021predicting}. 

The NLP models above were developed for readability level classification, which is typically limited to a small number of discrete, annotation framework-specific levels. Our focus here, on the other hand, is on continuous measures that are not tied to a specific annotation scheme or specific corpora. 
Furthermore, the gold standard data on which these systems are developed is often based on subjective human judgments of text readability levels, where inter-annotator agreement can be relatively low \citep{brunato2018sentence}. Here,  we advocate for an alternative target: cognitive data that directly captures online processing difficulty.

In psycholinguistics, length, frequency, surprisal and in some cases entropy have been consistently shown to be robust predictors of reading times \citep[][among others]{rayner1998,rayner2004,kliegl2004,rayner2011,smith2013}. Surprisingly, these measures, and in particular entropy and surprisal, have not been systematically explored as stand-alone readability measures. In the case of entropy and surprisal, we suspect that this might be related to the difficulty of computing surprisal (as compared to word and sentence length), especially without training in NLP. Currently, this barrier is rapidly vanishing as language models become widely accessible.

Our work is closest to \citet{howcroft-demberg2017} and \citet{nahatame2021text}. Similarly to \citet{howcroft-demberg2017} we evaluate  psycholinguistic measures via text simplification. However, our approach is different in that instead of predicting the readability level of the text, we predict the facilitation that humans experience as the result of the simplification process. Furthermore, we benchmark psycholinguistic measures against traditional readability measures. 

To our knowledge, the only studies to date to examine the predictive power of readability scores on reading measures are \citet{nahatame2021text} and \cite{hollenstein-2022-patterns}. The crucial difference between these studies and our work is our use of a parallel corpus of original and simplified texts, which allows benchmarking readability scores with respect to their ability to account for the increase in reading fluency as a result of simplification. This approach allows for a tighter experimental design which controls for differences in the content of the text which are otherwise very challenging to model.