% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% for table
\usepackage{adjustbox}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% Shubi Additions
% For Results Table
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{hyperref}

\graphicspath{{figures/}}
% for abstract filler
\usepackage{lipsum}
\newcommand{\tbd}[1]{\textcolor{cyan}{#1}}
\newcommand{\tbdref}{\colorbox{orange}{[ref]}}
\newcommand{\tbdfig}[1]{\colorbox{magenta}{[fig #1]}}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{float}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{cleveref}
\usepackage{comment}

%added by Yevgeni
\usepackage{enumitem}
%------------End of helper code--------------

\title{Surprisal Takes It All: Eye Tracking Based Cognitive Evaluation of Text Readability Measures}
 
\author{Keren Gruteke Klein$^1$, Shachar Frenkel$^1$, Omer Shubi$^1$, Yevgeni Berzak$^{1,2}$ \\
 $^1$Faculty of Data and Decision Sciences, \\
 Technion - Israel Institute of Technology, Haifa, Israel \\
 $^2$Department of Brain and Cognitive Sciences, \\
 Massachusetts Institute of Technology, Cambridge, USA \\
 \texttt{\{gkeren,fshachar,shubi\}@campus.technion.ac.il},
\texttt{berzak@technion.ac.il} \\
}


\begin{document}

\maketitle


\begin{abstract}

Text readability measures are widely used in many real-world scenarios and in NLP. These measures have primarily been developed by predicting reading comprehension outcomes, while largely neglecting what is perhaps the core aspect of a readable text: \emph{reading ease}. In this work, we propose a new eye tracking based methodology for evaluating readability measures, which focuses on their ability to account for reading facilitation effects in text simplification, as well as for text reading ease more broadly. Using this approach, we find that existing readability formulas are moderate to poor predictors of reading ease. We further find that average per-word length, frequency, and especially surprisal tend to outperform existing readability formulas as measures of reading ease. We thus propose surprisal as a simple unsupervised alternative to existing measures.
\footnote{Code is available anonymously \href{https://anonymous.4open.science/r/Readability-eval-RT/}{here.}}


\end{abstract}


\section{Introduction}

For over a century, text readability measures have been playing a prominent role in the evaluation of reading materials. Research on these measures has been flourishing due to their societal importance in many practical scenarios, including text simplification, information accessibility, and the selection and development of materials for education and language learning. Readability measures have also been widely used in NLP, especially in research on text readability and automated text simplification.

Despite their widespread adoption, early readability measures were born in a methodological sin, that was perpetuated in the development of most readability measures ever since. Their development was based on regressing text properties on \emph{reading comprehension} outcomes. While reading comprehension is central to readability, it depends not only on the difficulty of the text, but also on the difficulty of the reading comprehension tasks. Even more importantly, it does not speak directly to the core of what makes a text readable: \emph{reading ease}. This notion captures the most intuitive interpretation of the term readability, while reading comprehension is a possible, but not necessary byproduct. In other words, differences in text readability do not have to lead to differences in reading comprehension, and vice versa, differences in reading comprehension are not necessarily related to text readability.

Some of the developers of early readability measures have been well aware of this issue. For example, Rudolf Flesch, who introduced several widely used readability formulas, writes in the context of the Flesch Reading Ease score ``For many obvious reasons, the grade level of children answering test questions is not the best criterion for general readability. Data about the ease and interests with which adults will read selected passages would be far better. But such data were not available at the time the first formula was developed, and they are still unavailable today'' \citep{flesch1948new}.

Nonetheless, research on readability measures has continued to rely primarily on reading comprehension as the criterion for formula development. Although criticisms of readability formulas are abundant, they typically focus on issues such as the heuristic nature of the features, low statistical validity, and restricted applicability to different reader populations and domains. \citep[][]{bruce1981readability,anderson1986conceptual,bailin2001linguistic,collins2014computational}.
The core methodological assumptions behind the development of the regression formulas, on the other hand, have not received nearly as much scrutiny. %, nor much criticism.

In psycholinguistics, reading times and other eye movement measures are widely acknowledged to be highly informative of cognitive load during language processing. They are also known to be robustly predictable from linguistic properties of the text. 
Despite the increasing availability of eye tracking and other behavioral reading data, such as self-paced reading, to date, no readability formula has been developed to directly account for reading ease as measured in reading times. 

In the current work, we propose a new cognitive evaluation framework for text readability measures. Our framework focuses on the ability of readability measures to account for \emph{reading facilitation as a result of text simplification}.  
Regressing differences in readability measures on differences in reading measures between the original and simplified versions of each text, enables a controlled experimental framework that eliminates variability due to content differences between texts.
We use this framework to evaluate a range of prominent readability measures, as well as psycholinguistically motivated measures: idea density, integration cost, embedding depth, entropy, and the ``big three'' of lexical processing in reading \cite{clifton2016eye}: word length, frequency and surprisal. 

Using a large-scale eye movements dataset with a parallel corpus of original and simplified texts, we find that existing readability formulas have a moderate or weak predictive power for reading facilitation in simplification, which in most cases is outperformed by the ``big three'' predictors. The most predictive measure, especially for single sentences, tends to be average per-word \emph{surprisal}. We therefore propose surprisal as a simple alternative to existing readability formulas. 

To summarize, the main contributions of this work are the following:
\begin{itemize}
    \item Departing from reading comprehension based evaluations that have been prevalent in the literature, we argue for reading facilitation as a result of simplification, and reading ease more broadly as benchmarks for cognitive evaluation of readability formulas.
    \item We use this framework to evaluate prominent traditional and recent readability formulas, as well as key psycholinguistic measures that have been previously linked to online processing difficulty for different textual units: sentences and paragraphs.
    \item Based on this analysis, we propose average per-word surprisal as a new readability measure, and demonstrate the ease of referencing it against other readability scales.
\end{itemize}

\section{Data and Experimental Setup}

Our study is made possible by OneStop Eye Movements (henceforth OneStop) \citep{onestop2025preprint}. OneStop is a broad-coverage dataset of eye movements in reading with 360 adult English L1 participants. 
Crucially, it uses a parallel corpus of texts in their original and simplified forms, a property which we leverage in this work.

\textbf{Textual Materials} OneStop has 30 Guardian news articles with 4-7 paragraphs (162 paragraphs in total) from the News Lessons section of the English language-learning portal onestopenglish.com by Macmillan Education. Each article was simplified by a staff member of onestopenglish.com from its original ``Advanced'' version to a simplified ``Elementary'' version. 
Each paragraph has three multiple-choice reading comprehension questions. The questions and texts are from the OneStopQA dataset \cite{starc2020}, which is a subset of the OneStopEnglish corpus \cite{vajjala-2018-onestopenglish}.
The questions and answers are identical for both difficulty levels of each paragraph. 

To support the current study, we manually created an additional sentence-level alignment between the two versions of each paragraph. This alignment enables analyses not only at the passage level, but importantly also at the level of individual sentences. \Cref{table:text-stats} presents summary statistics of the two text levels.

\begin{table}[ht]
\centering
%\small
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lcc@{}}
\toprule
      & \textbf{Original} & \textbf{Simplified} \\ \midrule
Number of paragraphs  &  162 & 162 \\
Number of questions  &  486 & 486 \\ \hline
Words per paragraph  &  119.9 & 97.1 \\
Sentences per paragraph  & 5.77  & 5.75 \\
Sentence length (words)  & 20.8  & 16.9 \\ \hline
Mean word length (characters) & 4.76 & 4.56 \\
Mean word frequency (per 10,000 words) & 76.1 & 75.1
\\
Mean word surprisal (Pythia-70M) & 5.01   & 4.77 \\ \bottomrule
\end{tabular}%
}

\caption{Statistics of the original ``Advanced'' and simplified ``Elementary'' versions of OneStop texts. Word length excludes punctuation. Frequency counts are from Wordfreq \citep{robyn_speer_2018}. Surprisals are from the Pythia-70M language model \citep{biderman2023pythia}. 
}
\label{table:text-stats}
\end{table}

\textbf{Participants} OneStop has 360 adult English L1 participants. 180 of the participants read texts for comprehension. 180 additional participants read in an information seeking regime. In this work, our main analyses are for the former group, while additional validation is also provided for the information seeking regime in the Appendix. With two exceptions, the dataset does not include participants with dyslexia and language impairments.
The mean participant age is 22.8 and the mean age of English acquisition (AoA) is 0.4. 

\textbf{Eye Movement Data} Each participant is assigned to one of three 10-article batches (54 paragraphs). The texts are presented paragraph by paragraph. After each paragraph, the participant has to answer one of the three multiple-choice reading comprehension questions for the paragraph on a new screen, without the ability to return to the paragraph.
In the information seeking regime, the question is also presented before reading the paragraph.
Each paragraph in a given article is presented to the participant randomly either in the original or the simplified version. 
The data is counterbalanced such that each participant reads 27 original and 27 simplified paragraphs overall and approximately the same number of original and simplified paragraphs within each article. 
Each paragraph is read by 60 participants, 30 in the original level and 30 in the simplified level. Overall, the eye tracking data contains 2,110,632 word tokens over which eye movement data was collected, equally split between ordinary reading for comprehension (main analysis) and information seeking (Appendix).

\section{Evaluation of Readability Measures via Reading Ease}

Eye movements in reading are saccadic; their scanpath trajectory consists of times in which the gaze is stable at a given location, called \emph{fixations}, and very rapid transitions between fixations, called \emph{saccades} \citep{rayner1998,schotter2025beginner}. While many words receive a single fixation, some receive multiple fixations, and some are skipped. Most saccades go forward in the text, but some are regressive saccades that go backward.

We use four key summary measures of this trajectory from the psycholinguistic literature. 
\begin{enumerate}
    \item \textbf{Reading Speed (RS)} The number of words read per second. Note that if the text is known, this measure can in some cases be obtained without eye tracking, from the total reading time of the provided text.
    \item \textbf{Average Total Fixation Duration (TF)} TF is the sum of all the fixation durations on a word, considering only words that were not skipped. The TF is then averaged across words. 
    \item \textbf{Skip Rate (SR)} The fraction of words that were skipped.
    \item \textbf{Regression Rate (RR)} The number of regressions per word. 
\end{enumerate}

All four measures capture reading ease. Slower reading, longer reading times, and less skipping are all associated with increased processing difficulty during reading \citep{rayner1998}, and with lower language proficiency \citep{cop2015eye,berzak2022celer}. Regressions have similarly been shown to be a marker of processing difficulty and sentence reanalysis \cite{rayner2006eye}.
Thus, more readable texts should be read faster, and have shorter Total Fixation Durations, more word skipping and fewer regressions. 


Our main evaluation criteria is \textbf{reading facilitation as a result of text simplification}: the ability of readability measures to account for the differences in reading ease measures between the original and the simplified version of each text, averaged across participants. We denote these differences with $\Delta \text{RT}_T \in \{ \Delta \text{RS}_T, \Delta \text{TF}_T, \Delta \text{SR}_T,  \Delta \text{RR}_T \}$, where $T$ is a given textual item. Each $\Delta \text{RT}_T$ is the difference between the average $RT_{T_{\text{original}}}$ across 30 participants reading the original text version and the average $RT_{T_{\text{simplified}}}$ of 30 different participants reading the simplified version of a given text $T$. In our analyses there are two granularity levels for $T$: a sentence or a paragraph\footnote{We do not include article-level analyses as with a total of 30 article samples, correlation estimates are not reliable.}. Due to its fine-grained nature and many applications, our main evaluation is at the sentence level. In the Appendix, we provide all the analyses also for paragraphs. 

To evaluate the quality of a readability score, we regress the difference of this score between the original and simplified versions of each text $\Delta \text{ReadabilityScore}_T$ against the simplification effect on reading for that same text $\Delta \text{RT}_T$. 
The resulting regression model, in R notation, is: 
\[
\Delta \text{RT}_{T} \sim \Delta \text{ReadabilityScore}_{T}
\]
We then use the resulting Pearson $r$ correlation coefficient as the evaluation measure for the quality of the readability measure. 

The advantage of this evaluation compared to prediction of RT measures across different texts is the control that it provides for the content of the texts. Everything else being equal, the degree of reading facilitation induced by simplification should be reflected in the increase in the readability score between the original and simplified text versions. However, as mentioned above we also provide evaluations separately for the original and simplified texts in the Appendix.

An important pre-requisite for the applicability of a reading facilitation-centered evaluation is the presence of simplification effects on reading measures in the dataset. This question was investigated by \citet{gruteke-klein-etal-2025-simplification}, who found a robust effect of simplification on reading times (12ms in TF) and significant reading speed increases in 42\% of the participants in OneStop. These results suggest that the OneStop reading data is indeed suitable for a reading facilitation-centered evaluation. 


\section{Readability Measures}

We evaluate a range of widely used readability formulas and psycholinguistic measures of processing difficulty during sentence processing. For implementation details please refer to \Cref{sec:Implementation}. 

\subsection*{Traditional Readability Measures} 

Traditional readability measures were developed based on reading comprehension data from English L1 speakers. These are linear regression formulas, typically from two features, word length and sentence length. Word length is a heuristic for measuring word complexity and sentence length is a heuristic measure of grammatical complexity. 

We include prominent and widely adopted measures from the readability literature: Flesch Reading Ease Score \cite{flesch1948new}, Flesch Kincaid Grade Level \cite{flesch-kincaid1975}, Dale Chall Score \cite{dale1948formula}, Gunning Fog Index \cite{gunning1952technique}, Coleman Liau Index \cite{coleman1975computer}, Simple Measure of Gobbledygook (SMOG) \cite{smog1969} and the Automated Readability Index (ARI) \cite{ari-smith1967}. We extracted the measures using the \texttt{textstats} library. \Cref{tab:readability_measures} in \Cref{sec:app-readability_measures} presents the formulas and further details about them. While these measures were originally developed for passages, with the exception of SMOG, all of the measures are applicable to single sentences.  


\subsection*{Modern Readability Measures}

The rise of NLP over the past two decades has led to the introduction of modern readability measures that take advantage of automated linguistic analysis of text. These measures are cognitively inspired, and include different types of lexical, syntactic and discourse features extracted using NLP tools. Similarly to traditional formulas, they were developed based on data from passages, but can be applied to single sentences. We extract these measures using the Automatic Readability tool for English (ARTE) \cite{choi2022advances}.

\begin{itemize}[leftmargin=*]
    \item \textbf{Crowdsourced Algorithm of Reading Comprehension (CAREC)} \cite{carec-cares-rossley2019} a linear regression formula from 13 linguistic features to L1 speakers' subjective pairwise judgments of text difficulty (``Which text is easier to understand''), for pairs of different texts.
    \item \textbf{Crowdsourced Algorithm of Reading Speed (CARES)}  \cite{carec-cares-rossley2019} a linear regression formula from 9 linguistic features to L1 speakers' subjective pairwise judgments of reading speed (``Which text did you read more quickly''), for pairs of different texts.
    \item \textbf{Coh-Metrix L2 Reading Index (CML2RI)} \cite{cml2ri-crossley2008} a linear regression formula with three features, frequency, sentence syntactic similarity and word overlap between adjacent sentences. Differently from all the other examined readability formulas, which were developed using L1 reading comprehension data, the regression coefficients of CML2RI were obtained using L2 reading comprehension (cloze) scores. 
\end{itemize}

\subsection*{Psycholinguistic Measures}

\citet{howcroft-demberg2017} introduced the idea of using theoretically motivated measures from the psycholinguistic literature for predicting text difficulty, and applied it to the classification of single sentences. They used several variants of four such measures for classification of sentence difficulty level for pairs of original and simplified texts from the OneStopEnglish corpus \cite{vajjala-2018-onestopenglish} and the English and Simple English Wikipedia corpus \cite{hwang2015aligning}. Here we use similar features:

\begin{itemize}[leftmargin=*]
    \item \textbf{Idea Density}, also known as propositional idea density, is the ratio of ideas or propositions to words \citep{kintsch1972notes}. This ratio is expected to be lower in more readable texts. To compute idea density, we use the Computerized Propositional Idea Density Rater (CPIDR) 3.2 \cite{brown2008automatic}, implemented in the \texttt{pycpidr} library.
    
    \item \textbf{Integration Cost} This measure is rooted in the dependency locality theory \citep{gibson1998linguistic,gibson2000dependency}, which ties processing difficulty to the distance between heads and dependents in the sentence. More readable texts should have lower integration cost. We use \texttt{icy-parses}
    which combines part-of-speech tags and head-dependent relations to determine a sentence's
    integration cost. 
    Here, we evaluate the average and maximum integration cost measures. 
    
    \item \textbf{Embedding Depth} Syntactic embedding depth is a measure that reflects expected processing difficulty due to memory load. We examine average embedding depth across sentence words. We implemented this measure using syntactic parses obtained with the small English spaCy model. 
    
    \item \textbf{Surprisal} Surprisal theory \citep{hale2001,levy2008} ties processing difficulty to the predictability of the word in its context. Surprisal is defined as $-\log_2(p(w|context))$, where \textit{context} are the words preceding the current word \textit{w}. We use the average per-word surprisal. Our primary model for surprisals is the Pythia-70M language model \citep{biderman2023pythia} whose surprisal values correlate especially well with reading times \cite[][Appendix]{gruteke-klein-etal-2024-effect}. 
    
    \item \textbf{Entropy} of the next word given a context is another information theoretic measure that was linked to processing difficulty. Entropy is defined as $-\sum_{w \in W}{p(w|context)\log_2(p(w|context))}$. We use mean per-word entropy. This measure was not used by \citet{howcroft-demberg2017}, but we include it as it has been shown to contribute to reading times above and beyond surprisal \cite{cevoli2022prediction,pimentel2023effect}.
    
\end{itemize}

In addition to the aforementioned psycholinguistic measures, we include average word length and average word frequency. As mentioned above, length, frequency and surprisal are the ``big three'' of lexical processing  and were shown to have robust effects on reading times \citep[][among many others]{clifton2016eye,kliegl2004length,rayner2004effects,shain2024word}. \citet{howcroft-demberg2017} used length and frequency as a baseline model for measuring the added value of the first four types of psycholinguistic predictors above, which turned out to be very moderate.

\begin{itemize}[leftmargin=*]
    \item \textbf{Word Length} is measured in characters, excluding punctuation.
    \item \textbf{Word Frequency} is based on frequency counts from Wordfreq \citep{robyn_speer_2018} and coded as unigram surprisal $-\log_2(p(w))$.
\end{itemize}


% @1 Main (Corr)
% @1 Main Grid 2X2 for sentence 
\input{Plots/all/L1/Gathering0/Corr/tex_main_pearson_corr_sentence_diff_only_boot}


\section{Results}
\label{sec:results}

\Cref{fig:main_pearson_corr_sentence_diff_only_boot} presents the Pearson $r$ correlations for all readability measures at the sentence level. In the  $\Delta RS$, $\Delta TF$ and $\Delta SR$ evaluations we observe that within the traditional readability scores group, Coleman Liau performs best. Interestingly, it also outperforms the three modern scores CAREC, CARES and CML2RI. In the case of CARES this likely suggests that subjective judgments of reading speed are a problematic methodology for deriving measures of reading ease. 

Within the group of psycholinguistic measures, Idea Density, Integration Cost and Embedding Depth turn out to be very poor predictors of reading facilitation. On the other hand, entropy, length, frequency, and surprisal lead to correlations that are on par, or higher than the best traditional formula Coleman Liau. The advantage of these measures is especially apparent in the $\Delta RR$ evaluation, where the correlations of most of the traditional and modern readability formulas are not significant. Surprisal is the top performing measure for $\Delta SR$ and $\Delta RR$ and is on par with length on $\Delta RS$ and $\Delta TF$. \Cref{fig:RT_perm_test_sentence_Bootstrap_diff_only} provides pairwise statistical comparisons between all the measures, where it is again evident that surprisal tends to be the best predictor of reading facilitation.

In \Cref{sec:all_levels_results} \Cref{fig:main_pearson_corr_sentence_boot} we present sentence-level evaluations of predicting $RT$ measures separately for the original and simplified texts with mostly similar results. The correlations for all the measures are higher, where the ``big three'' again tend to be on top, but here Dale Chall and in some cases Coleman Liau yield comparable correlations. 
In \Cref{fig:main_pearson_corr_paragraph_boot} %and \Cref{fig:main_pearson_corr_article_boot} 
we further present analyses at the paragraph level. The results are largely consistent with the sentence-level analysis. Pairwise statistical comparisons are provided in \Cref{fig:RT_perm_test_sentence_Bootstrap} and \Cref{fig:RT_perm_test_paragraph_Bootstrap}. % and \Cref{fig:RT_perm_test_article_Bootstrap}. 
Both types of analyses are also provided for the information seeking regime in \Cref{sec:information_seeking}, where results are consistent with the ordinary reading, suggesting that the evaluation methodology and the results are robust to different reading scenarios.

% @1.2 Main (Perm Test)
% @1.2 Main Grid 2X2 for sentence
\input{Plots/all/L1/Gathering0/Corr/tex_RT_perm_test_sentence_Bootstrap_diff_only}


\section{Robustness to the Choice of Language Model}
\label{sec:robust_LLM}

We have identified average per-word surprisal as a strong candidate to be an unsupervised measure of readability. However, our analyses above are based on one model, Pythia-70M. How robust are these results to the choice of language model? To answer this question we examine 32 publicly available language models from the GPT-2 \cite{radford_language_2019}, GPT-J \cite{gpt-j}, GPT-Neo \cite{gpt-neo}, Pythia \cite{biderman2023pythia}, OPT \cite{zhang2022opt}, 
Mistral \cite{jiang2023mistral7b}, Gemma \cite{gemmateam2024gemmaopenmodelsbased}, 
Llama-2 \cite{touvron2023llama}, RWKV \cite{peng2023rwkv} and Mamba \cite{gu2023mamba} families, ranging from  70 million to 13 billion parameters. The full list of models is provided in \Cref{tab:models_table} of the Appendix.

% @2 (Robusteness LLM)
% @2 Main Grid 2X2 for sentence
\input{Plots/all/L1/Gathering0/Corr/tex_main_pearson_corr_by_perplexity_sentence_diff_only}

In \Cref{fig:main_pearson_corr_by_perplexity_sentence_diff_only} we present the reading facilitation correlations for these models as a function of their next word prediction performance, measured by $\log$ perplexity. Prior work suggested that different language models vary considerably in their predictive power of reading measures as a function of model perplexity \cite{Oh2022,shain2024large,gruteke-klein-etal-2024-effect}. 
This variability is apparent in Appendix \Cref{fig:main_pearson_corr_by_perplexity_sentence}, where we benchmark surprisal values separately against reading measures in the original and simplified texts. However, in our main evaluation of $\Delta \text{Surprisal}$ versus $\Delta \text{RT}$, the correlations are very stable across models, with very moderate increases as a function of $\log$ perplexity 
for $\Delta \text{RS}$ ($\beta=0.0021$, $p<0.001$), 
$\Delta \text{TF}$ ($\beta=0.002$, $p<0.001$), 
and $\Delta \text{SR}$ ($\beta=0.0017$, $p<0.001$), 
and no significant correlation for $\Delta \text{RR}$ ($\beta=0.0003$, $p>0.05$),
using a linear model.\footnote{In R notation: $r \sim \log (\text{perplexity})$.}
Overall, this suggests that the quality of surprisal as a readability measure is nearly invariant to the choice of language model. It further highlights the strength of $\Delta \text{RT}$ in simplification as an evaluation benchmark for readability measures. \Cref{fig:main_pearson_corr_by_perplexity_paragraph} further provides paragraph-level analyses, % and \Cref{fig:main_pearson_corr_by_perplexity_article}, 
with similar results.

\section{Leveling Surprisal Against Specific Readability Annotations and Scales}
\label{sec:surp_align}

Average per-word surprisal has a number of advantages over standard readability formulas, beyond its strong predictive power for reading facilitation and ease. One such advantage is that it allows for statistical testing of differences between scores of different texts. Perhaps even more importantly, it is agnostic to specific annotation frameworks of readability levels. This, however, also makes it less interpretable and less applicable to specific populations and scoring needs. To make surprisal interpretable with respect to specific readability level annotation schemes, and corpora for different populations, as well as other scores derived from such corpora one can directly regress surprisals on annotations of readability levels or readability scores. 

We demonstrate the feasibility of this approach using the Bradley-Terry readability scores of the CommonLit Ease of Readability (CLEAR) corpus \cite{crossley2021commonlit}. The CLEAR corpus offers readability scores for 4724 excerpts leveled for 3rd–12th grade readers by trained teachers. Readability scores were derived using a Bradley–Terry model \cite{bradley1952rank} to compute pairwise comparisons of text ease. The resulting Bradley–Terry (BT) Easiness Score ranges from -3.6 (difficult) to 1.7 (easy). 

% @3 (Surprisal to Grades)
% @3 Main CLEAR Plot
\input{Plots/all/Align/CLEAR/tex_align_CLEAR_surp_label_plot_Pythia_70M_Mean.tex}

\Cref{fig:align_CLEAR_surp_label_plot_Pythia_70M_Mean} presents an alignment between Pythia-70M surprisal and BT score. We observe a significant Pearson $r$ correlation between the measures ($r = -0.43, p<0.001$). Visual inspection of a Generalized Additive Model (GAM) fit which can account for non linear relationships, suggests that the relation between the two scores is linear. This is further supported by the mean absolute error of the GAM fit, which is nearly identical to a linear fit (0.74 in both cases). The linear transformation formula between the two scores is provided in the caption. A similar procedure can be applied to other scores, or gold standard annotations of readability levels of interest to yield linear or non-linear transformations from per-word surprisal to the scale of interest.

\section{Related Work}


Text readability has been an active research area for over a century, and the literature on readability measures is vast \cite[see][for surveys]{chall1958,crossley2011text,collins2014computational}.
As discussed in the introduction, nearly all of these measures were developed based on reading comprehension data, and to our knowledge, none of them were developed based on reading data. 

Prediction of text readability has also been studied in NLP, were various systems were developed for passage readability level classification for L1 and L2 readers \citep[][among others]{heilman2007combining,pitler2008revisiting,kate2010learning,vajjala2012improving,filighera2019automatic,feng2010comparison,vajjala-2018-onestopenglish,arase-etal-2022-cefr} as well as for single sentences \citep{vstajner2017automatic,brunato2018sentence,liu2025automatic}. Some of these approaches relied on language modeling by training language models on specific readability levels \cite{si2001statistical,collins2004language,schwarm2005reading} or by including perplexity (equivalent to mean surprisal) as one of the system's features \cite{xia2019text}. Especially relevant to our study is \citet{martinc2021supervised}, who developed perplexity-derived measures for unsupervised readability scoring. Eye movement and scrolling data have been previously used for improving the performance of NLP systems for readability level classification \cite{gonzalez2018learning,gooding2021predicting}. 

The NLP models above were developed for readability level classification, which is typically limited to a small number of discrete, annotation framework-specific levels. Our focus here, on the other hand, is on continuous measures that are not tied to a specific annotation scheme or specific corpora. 
Furthermore, the gold standard data on which these systems are developed is often based on subjective human judgments of text readability levels, where inter-annotator agreement can be relatively low \citep{brunato2018sentence}. Here,  we advocate for an alternative target: cognitive data that directly captures online processing difficulty.

In psycholinguistics, length, frequency, surprisal and in some cases entropy have been consistently shown to be robust predictors of reading times \citep[][among others]{rayner1998,rayner2004,kliegl2004,rayner2011,smith2013}. Surprisingly, these measures, and in particular entropy and surprisal, have not been systematically explored as stand-alone readability measures. In the case of entropy and surprisal, we suspect that this might be related to the difficulty of computing surprisal (as compared to word and sentence length), especially without training in NLP. Currently, this barrier is rapidly vanishing as language models become widely accessible.

Our work is closest to \citet{howcroft-demberg2017} and \citet{nahatame2021text}. Similarly to \citet{howcroft-demberg2017} we evaluate  psycholinguistic measures via text simplification. However, our approach is different in that instead of predicting the readability level of the text, we predict the facilitation that humans experience as the result of the simplification process. Furthermore, we benchmark psycholinguistic measures against traditional readability measures. 

To our knowledge, the only studies to date to examine the predictive power of readability scores on reading measures are \citet{nahatame2021text} and \cite{hollenstein-2022-patterns}. The crucial difference between these studies and our work is our use of a parallel corpus of original and simplified texts, which allows benchmarking readability scores with respect to their ability to account for the increase in reading fluency as a result of simplification. This approach allows for a tighter experimental design which controls for differences in the content of the text which are otherwise very challenging to model.



\section{Summary and Discussion}

We present a new evaluation framework for the development of readability measures. 
Our approach focuses on the core notion of readability, reading ease. Going beyond reading times across different texts, we advocate for a controlled framework that leverages reading data over parallel corpora of simplified texts. Our approach then focuses on the correlation between the reduction in readability and the reduction in reading times between the two text versions. Using this approach we evaluate an array of readability formulas and psycholinguistic measures and obtain several findings, most notably the viability of the ``big three'' and in particular surprisal as unsupervised readability measures. As the calculation of surprisal becomes more and more accessible to researchers in different disciplines, we are optimistic about its adoption potential not only for NLP applications but also for researchers in other scientific areas, as well as educators and other end users of readability assessment.


\section{Limitations}

This study has a number of limitations. One is the restriction to adult L1 English eye tracking data. While prior work has demonstrated considerable   simplification effects on reading in adult L1 speakers \cite{gruteke-klein-etal-2025-simplification},  OneStop does not include other populations for which readability measures are commonly used. Such groups include younger readers, 
elderly readers, L2 learners, and various special populations. The study is also limited to English and the newswire domain. To increase the generality of the results, additional data collection efforts and analyses are needed for other populations, languages, and domains. 

Another limitation of our approach is the current unavailability of large-scale corpora in the public domain which have manual readability level annotations. Such corpora are needed for reliably transforming unsupervised readability measures such as surprisal to interpretable population-specific scales.\footnote{The most promising candidate of reasonable size is Newsella \cite{xu2015problems} for which we were not able to obtain a license. Analyses with WeeBit \cite{vajjala2012improving} indicate that this corpus is too small and unreliable for this purpose.} As such datasets become available, we will provide formulas for transforming surprisals to the respective scales.

\section{Ethical Considerations}

The dataset used in this study was collected by \citet{onestop2025preprint} under an institutional IRB protocol. All participants provided written consent prior to participating in the study. The collected data is fully anonymized. Using eye tracking data for analyses of text readability is one of the use cases for which the data was collected. 

It was previously demonstrated that eye movements can be used for user identification \citep[e.g.][]{bednarik2005eye,jager2020deep}. We do not perform user identification in this study, and emphasize the importance of not storing information that could enable participant identification in future research and applications that integrate eye movements with text readability and text simplification. 

The CLEAR dataset, used in \Cref{sec:surp_align} and mentioned in \Cref{sec:Implementation}, was designed to support researchers in text readability. Our use of the dataset aligns with its original goal of enabling the development and evaluation of readability metrics.


% \citet{gruteke-klein-etal-2024-effect}

%\section{Acknowledgments}
%This work was supported by ISF grant 2070731.

\bibliography{references}

\clearpage
\appendix
\section*{Appendix}

\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}


\section{Implementation and Computational Details}
\label{sec:Implementation}

\subsection{Readability Measures Implementation}
The following resources were used to implement the readability measures:
\begin{itemize}
    \item \textbf{textstat library}: \url{https://github.com/textstat/textstat}
    \item \textbf{pycpidr library}: \url{https://github.com/jrrobison1/pycpidr}
    \item \textbf{icy-parses}: \url{https://github.com/dmhowcroft/icy-parses}
    %\url{https://github.com/KerenGruteke/icy-parses
    \item \textbf{spaCy model (en\_core\_web\_sm-3.8.0)}: \url{https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.8.0}
\end{itemize}

\subsection{Software and Hardware}
\begin{itemize}
    \item \textbf{Julia and GLM}: Julia \citep{barr_random_2013} version: \texttt{1.10.7}. The \texttt{GLM} package is used for linear model fitting in \Cref{sec:robust_LLM}.
    \item \textbf{pyGAM python library} \cite{pygam}, \url{https://github.com/dswah/pyGAM/} - used to fit the GAM models in \Cref{sec:surp_align}. 
    \item \textbf{Hardware}: NVIDIA RTX A6000 (48GB) and NVIDIA L40S (48GB) GPUs.
    \item \textbf{Code Development}: The code base for this project was developed with the assistance of GitHub Copilot, an AI-powered coding assistant. All generated code was carefully reviewed.
\end{itemize}
\subsection{Datasets}
\begin{itemize}
\item \textbf{CLEAR} The data is freely available at \url{https://github.com/scrosseye/CLEAR-Corpus}. The data is provided under a CC BY-NC-SA 4.0 DEED Attribution-NonCommercial-ShareAlike 4.0 International license. \cite{crossley2021commonlit}.
\end{itemize}
\clearpage
\section{Readability Measures}
\label{sec:app-readability_measures}
% Readability Table
\input{readability_table.tex}

\clearpage
\section{Detailed Results Across Text Levels and Structures}
\label{sec:all_levels_results}

We present results at the \textit{sentence} and \textit{paragraph} levels, considering both the original (Advanced) and simplified (Elementary) text versions, as well as the differences between original and simplified (Advanced - Elementary) levels.
\begin{itemize}
\item
\Cref{fig:main_pearson_corr_sentence_boot}, and 
\Cref{fig:main_pearson_corr_paragraph_boot} 
present the results of Pearson $r$ correlation coefficient between the original and simplified versions of each text $\Delta \text{ReadabilityScore}_T$ against the simplification effect on reading for that same text $\Delta \text{RT}_T$.  
\item
\Cref{fig:RT_perm_test_sentence_Bootstrap} and
\Cref{fig:RT_perm_test_paragraph_Bootstrap} 
show the pairwise statistical comparisons.
\end{itemize}

% @1 SM (Corr)
% @1 SM Grid 4X3 for sentence, paragraph
\input{Plots/all/L1/Gathering0/Corr/tex_main_pearson_corr_sentence_boot}
\input{Plots/all/L1/Gathering0/Corr/tex_main_pearson_corr_paragraph_boot}

% @1.2 SM (Perm Test)
% @1.2 SM Grid 4X3 for sentence, paragraph
\input{Plots/all/L1/Gathering0/Corr/tex_RT_perm_test_sentence_Bootstrap}
\input{Plots/all/L1/Gathering0/Corr/tex_RT_perm_test_paragraph_Bootstrap}


\clearpage
\section{Information Seeking Regime}
The information seeking reading regime includes 180 participants, who read the question (but not the answers) prior to reading the paragraph. We report the results for the information-seeking regime, as previous studies have shown differences in eye movement patterns when reading is goal-oriented \cite{hahn2023modeling, malmaud_bridging_2020,shubi2023eye}. Although \cite{gruteke-klein-etal-2024-effect} found a smaller effect of surprisal on reading times in this regime, the following figures show similar patterns in the correlations between reading facilitation and difference in surprisal to those observed in ordinary reading. This may indicate that surprisal can be used as a readability measure even in goal-oriented reading.

\begin{itemize}
\item
\Cref{fig:main_pearson_corr_sentence_boot_Hunting0} and 
\Cref{fig:main_pearson_corr_paragraph_boot_Hunting0} 
present the results of Pearson $r$ correlation coefficient between the original and simplified versions of each text $\Delta \text{ReadabilityScore}_T$ against the simplification effect on reading for that same text $\Delta \text{RT}_T$.  
\item
\Cref{fig:RT_perm_test_sentence_Bootstrap_Hunting0} and 
\Cref{fig:RT_perm_test_paragraph_Bootstrap_Hunting0}
show the pairwise statistical comparisons.
\end{itemize}

\label{sec:information_seeking}

% @1 SM Grid 4X3 for information seeking
\input{Plots/all/L1/Hunting0/Corr/tex_main_pearson_corr_sentence_boot}
\input{Plots/all/L1/Hunting0/Corr/tex_main_pearson_corr_paragraph_boot}

\input{Plots/all/L1/Hunting0/Corr/tex_RT_perm_test_sentence_Bootstrap}
\input{Plots/all/L1/Hunting0/Corr/tex_RT_perm_test_paragraph_Bootstrap}

\clearpage
\section{Robustness to the Choice of Language Model}

\input{models_table.tex}
% @2 (Robusteness LLM)
% @2 SM Grid Grid 4X3 for sentence, paragraph
\input{Plots/all/L1/Gathering0/Corr/tex_main_pearson_corr_by_perplexity_sentence}
\input{Plots/all/L1/Gathering0/Corr/tex_main_pearson_corr_by_perplexity_paragraph}

\end{document}


