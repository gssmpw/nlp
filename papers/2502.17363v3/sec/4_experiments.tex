\section{Experiments}
\label{sec:experiments}
\input{tables/compare}
\input{tables/ablation_delete}
\subsection{Experimental Setup}

\noindent\textbf{Baselines.} We compare our method against two categories of approaches: (1) Training-free methods including P2P~\cite{hertz2022prompt}, MasaCtrl~\cite{cao2023masactrl} based on DDIM~\cite{ddim}, and RF-Edit~\cite{wang2024taming}, RF-Inversion~\cite{rout2024semantic} based on Rectified Flow~\cite{rectflow}; (2) Training-based methods including BrushEdit~\cite{li2024brushedit} and FLUX-Fill~\cite{flux}, which are based on DDIM and Rectified Flow respectively. In total, we evaluate against six prevalent image editing and inpainting approaches.

\noindent\textbf{Datasets.} We evaluate our method and baselines on nine tasks from PIE-Bench~\cite{ju2024pnp}, which comprises 620 images with corresponding masks and text prompts. Following~\cite{xu2024inversion,li2024brushedit}, we exclude style transfer tasks from PIE-Bench~\cite{ju2024pnp} as our primary focus is background preservation in semantic editing tasks such as object addition, removal, and change.

\noindent\textbf{Implementation Details.} We implement our method based on FLUX.1-[dev]~\cite{flux}, following the same framework as other Rectified Flow-based methods~\cite{wang2024taming,rout2024semantic,kulikov2024flowedit}. We maintain consistent hyperparameters with FlowEdit~\cite{kulikov2024flowedit}, using 28 timesteps in total, skipping the last 4 timesteps ($N=24$) to reduce cumulative errors, and setting guidance values to 1.5 and 5.5 for inversion and denoising processes respectively.
\textbf{NS} in tables and charts represent no skip step ($N=28$). Other baselines retain their default parameters or use previously published results. Unless otherwise specified, ``Ours" in tables refers to the inversion-based KV-Edit without the two optional enhancement techniques proposed in \cref{section:kv_edit}. All experiments are conducted on two NVIDIA 3090 GPUs with 24GB memory.

\noindent\textbf{Metrics} Following~\cite{ju2024brushnet,li2024brushedit,ju2024pnp}, we use seven metrics across three dimensions to evaluate our method. For image quality, we report HPSv2~\cite{zhang2018unreasonable} and aesthetic scores~\cite{schuhmann2022laion}. For background preservation, we measure PSNR~\cite{huynh2008scope}, LPIPS~\cite{zhang2018unreasonable}, and MSE. For text-image alignment, we report CLIP score~\cite{radford2021learning} and Image Reward~\cite{xu2023imagereward}. Notably, while Image Reward was previously used for quality assessment, we found it particularly effective at measuring text-image alignment, providing negative scores for unedited images. Based on this observation, we also utilize Image Reward to evaluate the successful removal of objects.

\subsection{Editing Results}
We conduct experiments on PIE-Bench~\cite{ju2024pnp}, categorizing editing tasks into three major types: removing, adding, and changing objects. For practical applications, these tasks prioritize background preservation and text alignment, followed by overall image quality assessment.

\noindent\textbf{Quantitative Comparison.} \cref{tab:maintable} presents quantitative results including baselines, our method, and our method with the reinitialization strategy. We exclude results with the attention mask strategy, as it shows improvements only in specific cases. Our method surpasses all others in Masked Region Preservation metrics. Notably, as shown in \cref{fig:compare}, methods with PSNR below 30 fail to maintain background consistency, producing results that merely resemble the original. RF-Inversion~\cite{rout2024semantic}, despite obtaining high image quality scores, generates entirely different backgrounds. Our method achieves the third-best image quality, which has been higher than the original images, and perfectly preserving the background at the same time. With the reinitialization process, we achieve optimal text alignment scores, as the injected noise disrupts the original content, enabling more effective editing in certain cases (e.g., object removal and color change). Even compared to training-based inpainting methods~\cite{li2024brushedit,flux}, our approach better preserves backgrounds while following user intentions.

\noindent\textbf{Qualitative Comparison.} \cref{fig:compare} demonstrates our method's performance against previous works across three different tasks. For removal tasks, the examples shown require both enhancement techniques proposed in \cref{section:kv_edit}. Previous training-free methods fail to preserve backgrounds, particularly Flow-Edit~\cite{kulikov2024flowedit} which essentially generates new images despite high quality. Interestingly, training-based methods like BrushEdit~\cite{li2024brushedit} and FLUX-Fill~\cite{flux} exhibit notable phenomena in certain cases (first and third rows in \cref{fig:compare}). BrushEdit~\cite{li2024brushedit}, possibly limited by generative model capabilities, produces meaningless content. FLUX-Fill~\cite{flux} sometimes misinterprets text prompts, generating unreasonable content like duplicate subjects. In contrast, our method demonstrates satisfactory results, successfully generating text-aligned content while preserving backgrounds, eliminating the traditional trade-off between background preservation and foreground editing.

\input{figs/experiment_ablation}
\subsection{Ablation Study}
We conduct ablation studies to illustrate the impact of two enhancement strategies proposed in \cref{section:kv_edit} and the no-skip step on our method's object removal performance. \cref{tab:ablation} presents the results in terms of image quality and text alignment scores. Notably, for text alignment evaluation, we compute the similarity between the generated results and the original prompt using CLIP~\cite{radford2021learning} and Image Reward~\cite{xu2023imagereward} models. This metric proves more discriminative in removal tasks, as still presenting of specific objects in the final images significantly increases the similarity scores.

As shown in \cref{tab:ablation}, the combination of NS (No-skip) and RI (Reinitialization) achieves the optimal text alignment scores. However, we observe a slight decrease in image quality metrics after incorporating these components. We attribute this phenomenon to the presence of too large masks in the benchmark, where no-skip, reinitialization, and attention mask collectively disrupt substantial information, leading to some discontinuities in the generated images. Consequently, these strategies should be viewed as optional enhancements for editing effects rather than universal solutions applicable to all scenarios.

\cref{fig:ablation} visualizes the impact of these strategies. In the majority of cases, reinitialization alone suffices to achieve the desired results, while a small subset of cases requires additional attention masking for enhanced performance.

\subsection{User Study}
We conduct an extensive user study to compare our method with four baselines, including the training-free methods RF-Edit~\cite{wang2024taming}, RF-Inversion~\cite{rout2024semantic}, and the training-based methods BrushEdit~\cite{li2024brushedit} and Flux-Fill~\cite{flux}. We use 110 images from the ``random class" in the PIE-Bench~\cite{ju2024pnp} (excluding style transfer task, images without backgrounds, and controversial content). More than 20 participants are asked to compare each pair of methods based on four criteria: image quality, background preservation, text alignment, and overall satisfaction. As shown in \cref{tab:user_study}, our method significantly outperforms the previous methods, even surpassing Flux-Fill~\cite{flux}, which is the official inpainting model of FLUX~\cite{flux}. Additionally, users' feedback reveals that background preservation plays a crucial role in their final choices, even if RF-Edit~\cite{wang2024taming} achieves high image quality but finally fails in satisfaction comparison. 
\input{tables/user_study}
