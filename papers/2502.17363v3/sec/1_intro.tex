\section{Introduction}
\label{sec:intro}

Recent advances in text-to-image (T2I) generation have witnessed a significant shift from UNet~\cite{ronneberger2015u} to DiT~\cite{peebles2023scalable} architectures, and from diffusion models (DMs)~\cite{tang2024post,wang2024towards,dai2024motionlcm} to flow models (FMs)~\cite{flux,kulikov2024flowedit,zhu2024flowie}. Flow-based models, such as Flux~\cite{flux}, construct a straight probability flow from noise to image, enabling faster generation with fewer sampling steps and reduced training resources. DiTs~\cite{peebles2023scalable}, with their pure attention architecture, have demonstrated superior generation quality and enhanced scalability compared to UNet-based models. These T2I models~\cite{rombach2022high,flux,esser2024scaling} can also facilitate image editing, where target images are generated based on source images and modified text prompts.

In the field of image editing, early works~\cite{sdedit,hertz2022prompt,tumanyan2023plug,dong2023prompt} proposed the inversion-denoising paradigm to generate edited images, but they struggle to maintain background consistency during editing. One popular approach is attention modification, such as HeadRouter~\cite{xu2024headrouter} modifying attention maps and PnP~\cite{tumanyan2023plug} injecting original features during the denoising process, aiming to increase similarity with the source image. However, there remains a significant gap between improved similarity and perfect consistency, as it is challenging to control networks' behavior as intended. Another common approach is designing new samplers~\cite{miyake2023negative,mokady2023null} to reduce errors during inversion. Nevertheless, errors can only be reduced but not completely eliminated and both training-free approaches above still require extensive hyperparameter tuning for different cases. Meanwhile, exciting training-based inpainting methods~\cite{li2024brushedit,zhuang2024task} can maintain background consistency but suffer from expensive training costs and potential degradation of quality.

To overcome all the above limitations, we propose a new training-free method that preserves background consistency during editing. Instead of relying on regular attention modification or new inversion samplers for similar results, we implement KV cache in DiTs~\cite{peebles2023scalable} to preserve the key-value pairs of background tokens during inversion and selectively reconstruct only the editing region. Our approach first employs a mask to decouple attention between background and foreground regions and then inverts the image into noise space while caching KV values of background tokens at each timestep and attention layer. During the subsequent denoising process, only foreground tokens are processed, while their keys and values are concatenated with the cached background information. Effectively, we guide the generative model to maintain new content continuity with the background and keep the background content identical to the input. We call this approach \textbf{KV-Edit}.

To further enhance the practical utility of our approach, we conduct an analysis of the removal scenario. This challenge arises from the residual information in surrounding tokens and the object itself which sometimes conflict with the editing instruction. To address this issue, we introduce mask-guided inversion and reinitialization strategies as two enhancement techniques for inversion and denoising separately. These methods further disrupt the information stored in surrounding tokens and self tokens respectively, enabling better alignment with the text prompt. In addition, we apply KV-Edit to the inversion-free method~\cite{xu2024inversion,kulikov2024flowedit}, which no longer caches key-value pairs for all timesteps, but uses KV immediately after one step, significantly reducing the memory consumption of the KV cache.

In summary, our key contributions include:
\textbf{1)} A new training-free editing method that implements KV cache in DiTs, ensuring complete background consistency during editing with minimal hyperparameter tuning.
\textbf{2)} Mask-guided inversion and reinitialization strategies that extend the method's applicability across various editing tasks, offering flexible choices for different user needs.
\textbf{3)} Using the inversion-free method to optimize the memory overhead of our method and enhance its usefulness on PC.
\textbf{4)} Experimental validation demonstrating perfect background preservation while maintaining generation quality comparable to direct T2I synthesis.
