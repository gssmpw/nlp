\section{Method}
\label{sec:method}
In this section, we first analyze the reasons why the inversion-denoising paradigm~\cite{dong2023prompt,hertz2022prompt} faces challenges in background preservation. Then, we introduce the proposed KV-Edit method, which achieves strict preservation of background regions during the editing process according to the mask. Finally, we present two optional enhancement techniques and an inversion-free version to improve the usability of our method across diverse scenarios.

\subsection{Preliminaries}

Deterministic diffusion models like DDIM~\cite{ddim} and flow matching~\cite{lipman2022flow} can be modeled using ODE~\cite{song2020score} to describe the probability flow path from noise distribution to real distribution. The model learns to predict velocity vectors that transform Gaussian noise into meaningful images. During the denoising process, $\mathbf{x_1}$ represents noise, $\mathbf{x_0}$ is the final image, and $\mathbf{x_t}$ represents intermediate results.
\begin{equation}
d\mathbf{x}_t=\left(f(\mathbf{x}_t,t)-\frac{1}{2}g^2(t)\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t)\right)dt,t \in[0,1].
\label{eq:ode}
\end{equation}
where $\mathbf{s}_\theta(\mathbf{x},t)=\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t)$ predicted by networks. Both DDIM~\cite{ddim} and flow matching~\cite{lipman2022flow} can be viewed as special cases of this ODE function. By setting 
$f(\mathbf{x}_t,t)=\frac{\mathbf{x}_t}{\overline{\alpha}_t}\frac{d\overline{\alpha}_t}{dt}$, 
$g^2(t)=2\overline{\alpha}_t\overline{\beta}_t\frac{d}{dt}\left(\frac{\overline{\beta}_t}{\overline{\alpha}_t}\right)$, and 
$\mathbf{s}_\theta(\mathbf{x},t)=-\frac{\epsilon_\theta(\mathbf{x},t)}{\overline{\beta}_t}$, we obtain the discretized form of DDIM:
\begin{equation}
\mathbf{x}_{t-1}=\bar{\alpha}_{t-1}\left(\frac{\mathbf{x}_t-\bar{\beta}_t\epsilon_\theta(\mathbf{x}_t,t)}{\bar{\alpha}_t}\right)
+\bar{\beta}_{t-1}\epsilon_\theta(\mathbf{x}_t,t)
\end{equation}
Both forward and reverse processes in ODE follow \cref{eq:ode}, describing a reversible path from Gaussian distribution to real distribution. During image editing, this ODE establishes a mapping between noise and real images, where noise can be viewed as an embedding of the image, carrying information about structure, semantics, and appearance.

Recently, Rectified Flow~\cite{liu2022rectified,rectflow} constructs a straight path between noise distribution and real distribution, training a model to fit the velocity field $\mathbf{v}_\theta(\mathbf{x},t)$. This process can be simply described by the ODE:
\begin{equation}
d\mathbf{x}_t=\mathbf{v}_\theta(\mathbf{x},t)dt,t\in[0,1].
\label{eq:4}
\end{equation}
Due to the reversible nature of ODEs, flow-based models can also be used for image editing through inversion and denoising in less timesteps than DDIM~\cite{ddim}.

\input{figs/method_skip}
\subsection{Rethinking the Inversion-Denoising Paradigm}
\label{sec:Rethinking}
The inversion-denoising paradigm views image editing as an inherent capability of generative models without additional training, capable of producing semantically different but visually similar images. However, empirical observations show that this paradigm only achieves similarity rather than perfect consistency in content, leaving a significant gap compared to users' expectations.This section will analyze the reasons for this issue into three factors.

Taking Rectified Flow~\cite{liu2022rectified,rectflow} as an example, based on \cref{eq:4}, we can derive the discretized implementation of inversion and denoising. The model takes the original image $\mathbf{x}_{t_0}$ and Gaussian noise $\mathbf{x}_{t_N}\in\mathcal{N}(0,\boldsymbol{I})$ as path endpoints. Given discrete timesteps $t=\{t_{N},...,t_{0}\}$, the model predictions $\boldsymbol{v}_\theta(C,\mathbf{x}_{t_i},t_i),i\in\{N,\cdots,1\}$, where $\mathbf{x}_{t_i}$ and $\mathbf{z}_{t_i}$ denote intermediate states in inversion and denoising respectively, as described by the following equations:
\begin{equation}
\mathbf{x}_{t_{i}}=\mathbf{x}_{t_{i-1}}+(t_i-t_{i-1})\boldsymbol{v}_\theta(C,\mathbf{x}_{t_i},t_i)
\end{equation}
\begin{equation}
\mathbf{z}_{t_{i-1}}=\mathbf{z}_{t_i}+(t_{i-1}-t_i)\boldsymbol{v}_\theta(C,\mathbf{z}_{t_i},t_i) 
\end{equation}
Ideally, $\mathbf{z}_{t_0}$ should be identity with $\mathbf{x}_{t_0}$ when directly reconstructed from $\mathbf{x}_{t_N}$. However, due to discretization and causality in the inversion process, we can only estimate using $\boldsymbol{v}_\theta(C,\mathbf{X}_{t_{t-1}},t_{t-1}) \approx \boldsymbol{v}_\theta(C,\mathbf{X}_{t_i},t_i)$, introducing cumulative errors. \cref{fig:skip} shows that with a fixed number of timesteps $N$, error accumulation increases as inversion timesteps approach $t_{N}$, preventing accurate reconstruction.

In addition, consistency is affected by condition. We can divide the image into regions we wish to edit $\mathbf{z}_{t_0}^{fg}$ and regions we want to preserve $\mathbf{z}_{t_0}^{bg}$, where ``fg" and ``bg" represent foreground and background respectively. Based on these definitions, the background denoising process is:
\begin{equation}
\boldsymbol{v}_\theta(C,\mathbf{z}_{t_i},t_i)=\boldsymbol{v}_\theta(C,\mathbf{z}_{t_i}^{fg},\mathbf{z}_{t_i}^{bg},t_i)
\end{equation}
\begin{equation}
\mathbf{z}_{t_{i-1}}^{bg}=\mathbf{z}_{t_i}^{bg}+(t_{i-1}-t_i)\boldsymbol{v}_\theta(C,\mathbf{z}_{t_i}^{fg},\mathbf{z}_{t_i}^{bg},t_i) 
\end{equation}
According to these formulas, when generating edited results, the background will be influenced by both the new condition $C$ and new foreground $\mathbf{z}_{t_i}^{fg}$. \cref{fig:change} demonstrates that background regions change when only modifying the prompt or foreground noise. In summary, uncontrollable background changes can be attributed to three factors: error accumulation, new conditions, and new foreground content. In practice, any single element will trigger all three effects simultaneously. Therefore, this paper will present an elegant solution to address all these issues simultaneously.

\input{figs/method_change}
\subsection{Attention Decoupling}
Traditional inversion-denoising paradigms process background and foreground regions simultaneously during denoising, causing undesired background changes in response to foreground and condition modifications. Upon deeper analysis, we observe that in UNet~\cite{ronneberger2015u} architectures, the extensive convolutional networks lead to the fusion of background and foreground information, making it impossible to separate them. However, in DiT~\cite{peebles2023scalable}, which primarily relies on attention blocks~\cite{vaswani2017attention}, allows us to use only foreground tokens as \textbf{queries}, generating foreground content separately and then combined with the background.

Moreover, directly generating foreground tokens often results in discontinuous or incorrect content relative to the background. Therefore, we propose a new attention mechanism where \textbf{queries} contain only foreground information, while \textbf{keys} and \textbf{values} incorporate both foreground and background information. Excluding text tokens, the image-modality self-attention computation can be expressed as:
\begin{equation}
\mathrm{Att}(\mathbf{Q}^{fg},(\mathbf{K}^{fg},\mathbf{K}^{bg}),(\mathbf{V}^{fg},\mathbf{V}^{bg}))=\mathcal{S}(\frac{\mathbf{Q}^{fg}\mathbf{K}^T}{\sqrt{d}})\mathbf{V}
\label{eq:attn}
\end{equation}
where $\mathbf{Q}^{fg}$ represents \textbf{queries} containing only foreground tokens, $(\mathbf{K}^{fg},\mathbf{K}^{bg})$ and $(\mathbf{V}^{fg},\mathbf{V}^{bg})$ denote the concatenation of background and foreground \textbf{keys} and \textbf{values} in their proper order (equivalent to the complete image's \textbf{keys} and \textbf{values}), and $\mathcal{S}$ represents the softmax operation. Notably, compared to conventional attention computations, \cref{eq:attn} only modifies the \textbf{query} component, which is equivalent to performing cropping at both input and output of the attention layer, ensuring seamless integration of the generated content with the background regions.

\input{figs/method_inf}
\subsection{KV-Edit}
\label{section:kv_edit}
Building upon \cref{eq:attn}, achieving background-preserving foreground editing requires providing appropriate key-value pairs for the background. Our core insight is that background tokens' \textbf{keys} and \textbf{values} reflect their deterministic path from image to noise. Therefore, we implement KV cache during the inversion process, as detailed in \cref{algorithm:algorithm1}. This approach records the \textbf{keys} and \textbf{values} at each timestep and block layer along the probability flow path, which are subsequently used during denoising as shown in \cref{algorithm:algorithm2}. We term this complete pipeline ``KV-Edit" as shown in \cref{fig:pipeline} where ``KV" means KV cache.

Unlike other attention injection methods~\cite{tumanyan2023plug,avrahami2024stable,tewel2024add}, KV-Edit only reuses KV for background tokens while regenerating foreground tokens, without requiring specification of particular attention layers or timesteps. Rather than using the source image as injected information, we treat the deterministic background as context and the foreground as content to continue generating, analogous to KV cache in LLMs. Since the background tokens are preserved rather than regenerated, KV-Edit ensures perfect background consistency, effectively circumventing the three influencing factors discussed in \cref{sec:Rethinking}.

Previous works~\cite{hertz2022prompt,dong2023prompt,cao2023masactrl} often fail in object removal tasks when using image captions as guidance, as the original object still aligns with the target prompt. Through our in-depth analysis, we reveal that this issue stems from the residual information of the original object, which persists both in its own tokens and propagates to surrounding tokens through attention mechanisms, ultimately leading the model to reconstruct the original content.

To address the challenge in removing objects, we introduce two enhancement techniques. First, after inversion, we replace $\mathbf{z}_{t_N}$ with fused noise $\mathbf{z}^{\prime}_{t_N} = \mathrm{noise} \cdot t_N + \mathbf{z}_{t_N}\cdot(1-t_N)$ to disrupt the original content information. Second, we incorporate an attention mask during the inversion process, as illustrated in \cref{fig:pipeline}, to prevent foreground content from being incorporated into the KV values, further reducing the preservation of original content. These techniques serve as optional enhancements to improve editing capabilities and performances in different scenarios as shown in \cref{fig:fig1}.
\input{algorithm/algorithm1}
\input{algorithm/algorithm2}
\input{figs/experiment_compare}

\subsection{Memory-Efficient Implementation}
Inversion-based methods require storing key-value pairs for N timesteps, which can pose significant memory constraints when working with large-scale generative models (e.g., 12B parameters~\cite{flux}) on personal computers. Fortunately, inspired by~\cite{xu2024inversion,kulikov2024flowedit}, we explore an inversion-free approach. The method performs denoising immediately after each inversion step, computing the vector difference between the two results to derive a probability flow path in the $t_0$ space. This approach allows immediate release of KV cache after use, reducing memory complexity from $O(N)$ to $O(1)$.

However, the inversion-free method may occasionally result in content retention artifacts as shown in \cref{fig:inf} and FlowEdit~\cite{kulikov2024flowedit}. Since our primary focus is investigating background preservation during editing, we leave more discussion about inversion-free in supplementary materials.
