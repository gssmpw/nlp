\input{figs/pipeline}
\section{Related Work}
\label{sec:related}

\subsection{Text-guidanced Editing}

Image editing approaches can be broadly categorized into training-based and training-free methods. Training-based methods~\cite{kawar2023imagic,brooks2023instructpix2pix,ju2024brushnet,li2024brushedit,flux}, have demonstrated impressive editing capabilities through fine-tuning pre-trained generative models on text-image pairs, achieving controlled modifications. Training-free methods have emerged as a flexible alternative, with pioneering works~\cite{sdedit,hertz2022prompt,tumanyan2023plug,dong2023prompt} establishing the two-stage inversion-denoising paradigm. Attention modification has become a prevalent technique in these methods~\cite{cao2023masactrl,li2023stylediffusion,xu2024headrouter,avrahami2024stable,tewel2024add}, specially Add-it~\cite{tewel2024add} broadcast features from inversion to denoising process to maintain source image similarity during editing. Some other work~\cite{mokady2023null,miyake2023negative,ju2024pnp,lin2024schedule,wang2024taming} focused on a better inversion sampler such as the RF-solver~\cite{wang2024taming} designs a second-order sampler. The methods most similar to ours~\cite{avrahami2023blended,chen2024region,liu2024lipe,tewel2024add} attempt to preserve background elements by blending source and target images at specific timesteps using masks. A common consensus is that accurate masks are crucial for better quality, where user-provided inputs~\cite{ju2024brushnet,li2024brushedit} and segmentation models~\cite{ravi2024sam,huang2024segment,yang2022lavt,yang2024language,Liu_2024_CVPR,liu2024universal,bai2024self} prove to be more effective choices compared to masks derived from attention layers in UNet~\cite{ronneberger2015u}. However, the above methods frequently encounter failure cases and struggle to maintain perfect background consistency during editing, while training-based methods~\cite{kawar2023imagic,brooks2023instructpix2pix,zhuang2024task,ju2024brushnet,li2024brushedit,flux} face the additional challenge of computational overhead.

\subsection{KV cache in Attention Models}

KV cache is a widely-adopted optimization technique in Large Language Models  (LLMs)~\cite{brown2020language,xiao2023efficient,bai2023qwen,liu2024deepseek} to improve the efficiency of autoregressive generation. In causal attention, since keys and values remain unchanged during generation, recomputing them leads to redundant resource consumption. KV cache addresses this by storing these intermediate results, allowing the model to reuse key-value pairs from previous tokens during inference. This technique has been successfully implemented in both LLMs~\cite{brown2020language,xiao2023efficient,bai2023qwen,liu2024deepseek} and Vision Language Models (VLMs)~\cite{li2023blip,achiam2023gpt,liu2024visual,zhang2024flash,ye2024voco,huang2024learn,ye2024atp}. However, it has not been explored in image generation and editing tasks, primarily because image tokens are typically assumed to require bidirectional attention~\cite{dosovitskiy2020image,he2022masked}.
