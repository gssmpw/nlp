\section*{Appendix}
In this supplementary material, we provide more details and findings. In \cref{app:A}, we present additional experimental results and implementation details of our proposed KV-Edit. \cref{app:B} provides further discussion and data regarding our inversion-free methodology. \cref{app:C} details the design and execution of our user study. Moreover, In \cref{app:D}, we discuss potential future directions and current limitations of our work.
\section{Implementation and More Experiments}
\label{app:A}
\input{figs/app_result_1}
\noindent\textbf{Implementation Details}.
Our code is built on Flux~\cite{flux}, with modifications to both double block and single block to incorporate KV cache through additional function parameters. Input masks are first downsampled using bilinear interpolation, then transformed from single-channel to 64-channel representations following the VAE in Flux~\cite{flux}. In the feature space, the smallest pixel unit is 16 dimensions rather than the entire 64-dimensional token. Therefore, in addition to KV cache, we preserve the intermediate image features at each timestep to ensure fine-grained editing capabilities. In our experiment, inversion and denoising can be performed independently, allowing a single image to be inverted just once and then edited multiple times with different conditions, further enhancing the practicality of this workflow.

\noindent\textbf{Experimental Results}.
Due to space constraints in the main paper, we only present results on the PIE-Bench~\cite{ju2024pnp}. Here, we provide additional examples demonstrating the effectiveness of our approach. To further showcase the flexibility of our method, \cref{fig:app_result_1} and \cref{fig:app_result_2} present various editing target applied to the same source image, without explicitly labeling the input masks because each case corresponds to a different mask. \cref{fig:app_ablation} illustrates the impact of steps and reinitialization strategy on the color changing tasks and inpainting tasks.

When changing colors, as the number of skip-steps decreases and reinitialization strategy is applied, the color information in the tokens is progressively disrupted, ultimately achieving successful results. In our experiments, the optimal number of steps to skip depends on image resolution and content, which can be adjusted based on specific needs and feedback. Unlike previous training-free methods, our approach even can be applied to inpainting tasks after employing reinitialization strategy, as demonstrated in the third row of \cref{fig:app_ablation}. The originally removed regions in inpainting tasks can be considered as black objects, thus requiring reinitialization strategy to eliminate pure black information and generate meaningful content. We plan to further extend our method to inpainting tasks in future work, as there are currently very few training-free methods available for this application.

\noindent\textbf{Attention Scale}
When dealing with large masks (e.g., background changing tasks), our original method may produce discontinuous images including conflicting content, as illustrated in \cref{fig:app_scale}. Stable-Flow~\cite{avrahami2024stable} demonstrated that during image generation with DiT~\cite{peebles2023scalable}, image tokens primarily attend to their local neighborhood rather than globally across most layers and timesteps.

Consequently, although our approach treats the background as a condition to guide new content generation, large masks can introduce generation bias which ignore existing content and generate another objects. Based on this analysis, we propose a potential solution as shown in \cref{fig:app_scale}. We directly increase the attention weights from masked regions to unmasked regions in the attention map (produced by query-key multiplication), effectively mitigating the bias impact. This attention scale mechanism enhances content coherence by strengthening the influence of preserved background on new content.
\input{figs/app_result_2}
\input{figs/app_scale}
\input{figs/app_abaltion}
\section{More Discussions on Inversion-Free}
\label{app:B}
We implement inversion-free editing on Flux~\cite{flux} based on the code provided by FlowEdit~\cite{kulikov2024flowedit}. As noted in FlowEdit~\cite{kulikov2024flowedit}, adding random noise at each editing step may introduce artifacts, a phenomenon we also demonstrate in the main paper. In this section, we primarily explore the impact of inversion-free methods on memory consumption.

\cref{algorithm:algorithm3} demonstrates the implementation of inversion-free KV-Edit, where ``KV-inversion" and ``KV-denoising" refer to single-step noise prediction with KV cache. KV cache is saved during a one-time inversion process and immediately utilized in the denoising process. The final vector can be directly added to the original image without first inversing it to noise. This strategy ensures that the space complexity of KV cache remains $O(1)$ along the time dimension. Moreover, resolution has a more significant impact on memory consumption as the number of image tokens grows at a rate of $O(n^2)$.

We conducted experiments across various resolutions and time steps, reporting memory usage in \cref{tab:app_inf}. When processing high-resolution images and more timesteps, personal computers struggle to accommodate the memory requirements. Nevertheless, we still recommend the inversion-based KV-Edit approach for several reasons:

\begin{enumerate}
    \item Current inversion-free methods occasionally introduce artifacts.
    \item Inversion-based KV-Edit enables multiple editing attempts after a single inversion, significantly improving usability and workflow efficiency.
    \item Large generative models inherently require substantial GPU memory, which presents another challenge for personal computers. Therefore, we position inversion-based KV-Edit as a server-side technology.
\end{enumerate}
\input{tables/app_inf_compare}
\input{algorithm/algorithm3}
\section{User Study Details}
\label{app:C}

We conduct our user study in a questionnaire format to collect user preferences for different methods. We observe that in most cases, users struggle to distinguish the background effects of training-based inpainting methods (e.g., FLUX-Fill~\cite{flux} sometimes increases grayscale tones in images). Therefore, we allowed participants to select ``equally good" regarding background quality.

Additionally, PIE-Bench~\cite{ju2024pnp} contains several challenging cases where all methods fail to complete the editing tasks satisfactorily. Consequently, we allow users to select ``neither is good" for text alignment and overall satisfaction metrics, as illustrated in \cref{fig:app_user}.

We implement a single-blind mechanism where the corresponding method for each question is randomly sampled, ensuring fairness in the comparison. We collect over 2,000 comparison results and calculate our method's win rate after excluding cases where both methods are rated equally.
\input{figs/app_user.tex}

\section{Limitations and Future Work}
\label{app:D}

In this section, we outline the current challenges faced by our method and potential future improvements. While our approach effectively preserves background content, it struggles to maintain foreground details. As shown in \cref{fig:app_ablation}, when editing garment colors, clothing appearance features may be lost, such as the style, print or pleats. 

Typically, during the generation process, early steps determine the object's outline and color, with specific details and appearance emerging later. In the contrast, during inversion, customized object details are disrupted first and subsequently influenced by new content during denoising. This represents a common challenge in the inversion-denoising paradigm~\cite{hertz2022prompt,tumanyan2023plug,dong2023prompt}.

In future work, we could employ trainable tokens to preserve desired appearance information during inversion and inject it during denoising, still without fine-tuning of the base generative model. Furthermore, our method could be adapted to other modalities, such as video and audio editing, image inpainting tasks. We hope that ``KV cache for editing" can be considered an inherent feature of the DiT~\cite{peebles2023scalable} architecture.
