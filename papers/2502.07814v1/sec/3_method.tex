\section{Methodology}
% This section introduces our satellite observations guided conditional diffusion model, SGD. 
The key idea of our satellite observations guided conditional diffusion model, SGD, is to employ satellite observations (GridSat) as conditional inputs to enhance the coherence of the SR maps generated by the conditional diffusion model.
Satellite observations are utilized to obtain the reanalysis data, such as ERA5. 
Therefore, leveraging original satellite observations enables the generated SR maps to align more closely with actual conditions. 
Additionally, LR ERA5 maps serve as the guided maps during the sampling process, ensuring the generation of maps replete with faithful details. 
An overview of our SGD is depicted in ~\cref{fig:main_fig}. 

\subsection{Pre-trained Encoder}
The pre-trained encoder is employed to obtain the embedding feature of each channel within every GridSat map. 
The encoder extracts features through convolutional operations and skips connections, while the decoder reconstructs the features of GridSat via a symmetric architecture. 
The detailed structure of the encoder is shown in Appendix. 
% \textcolor{red}{we may need a detailed description of the network in the appendix}
Following the pre-training of the encoder architecture $f$ with parameter $\phi$, SGD is capable of more effectively integrating the features of GridSat maps with those of ERA5 and subsequently inputting them into the UNet-based conditional diffusion model. 
Further analysis of the encoder's role will be conducted in ~\cref{ablation_study}. 

\subsection{Satellite Conditioning Mechanisms}
% Denoising diffusion probabilistic models (DDPMs) have found extensive applications in the field of image generation.
Traditional unconditional DDPMs do not rely on additional conditions during training, resulting in generated images that tend to lack specific guidance and semantic coherence. 
In contrast, conditional DDPMs guide the generation process by incorporating external conditional information such as labels, textual descriptions, or images, allowing for effective control over the features and attributes of the generated results.
For the downscaling task of ERA5 maps, considering that the ERA5 data is derived from satellite observation, a coupling relationship exists between the two maps. 
The brightness temperature data obtained from satellite observations significantly influence the meteorological states within ERA5.
% \textcolor{red}{these introductions are general, we need to specify the motivation for incorporating observations. for example, reanalysis data including era5 is obtained by observations. temperatures from gridsat will be beneficial for generating more reasonable maps.}

Therefore, SGD employs a GridSat map conditional DDPM. 
By leveraging the GridSat map as a condition, the diffusion model is transformed into a conditional generator, effectively utilizing the features within GridSat to generate high-resolution atmospheric states that are more consistent with reality during the sampling process. 
% The cross-attention is employed to enhance the underlying UNet architecture. 
After the GridSat features are extracted by the encoder into a latent-space representation, it is then integrated with the ERA5 maps through the cross-attention mechanism to facilitate the fusion of features.
% \textcolor{red}{we need to specify Q, K, V is from which feature.}
\begin{align}
Atten(Q,K,V)=softmax(\frac{W_Q(x)W_K(y')^T}{\sqrt{d}})\cdot W_V(y').
\end{align}
The matrices $W_Q\in \mathbb{R}^{d\times d_{\epsilon}}, W_K, W_V\in \mathbb{R}^{d\times d_{\tau}}$, which serve as learnable projection matrices with trainable parameters, are respectively utilized to amalgamate the feature information from the ERA5 map $x$ and the conditioning input $y'$ within the same time period. 
$W_Q$ extracts features from ERA5 maps, while $W_K$ and $W_V$ process features derived from GridSat maps. 
The parameters of cross attention module are jointly optimized with the UNet backbone.

Based on this foundation, the map-conditioned DDPM can undergo training according to the following objective.
\begin{align}
E_{\epsilon \sim \mathcal{N}(0,I),y',t\sim [0,T]}[\left \|\epsilon - \epsilon_\theta(x_t,y',t) \right \|^2_2 ].
\end{align}

\subsection{Downscaling ERA5 via Zero-shot Sampling}
In this section, we employ pre-trained GridSat map conditional DDPMs to generate high-resolution ERA5 maps that are faithful to the real surface meteorological conditions. 
Off-the-shelf deep learning methods commonly downscale ERA5 maps without leveraging satellite data, thereby neglecting the intricate coupling between satellite observations and ERA5 maps. 
% Off-the-shelf deep learning methods, such as diffusion-based models, commonly overlook the satellite observation map conditioning in the downscaling of ERA5 maps, employing DDPM without leveraging satellite data
This overlook will lead to discrepancies between the downscaled outcomes and actual conditions.
% are limited to the task of downscaling coarse-resolution maps and are incapable of addressing the more challenging task of downscaling to higher resolutions. 
% \textcolor{red}{we need to re-identify the main challenge: downscaling era5 with satellite observation. Other ddpms can also downscale era5.}
Meanwhile, the high-resolution ERA5 map with faithful details contains more precise information and holds greater value in practical data applications as it is more closely aligned with actual observations. 
In pursuit of this effect, SGD utilizes low-resolution ERA5 maps to guide the reverse steps of the conditional diffusion model.
A convolutional kernel with optimizable parameters is developed to achieve a downscaling process at arbitrary resolution. 
The sampling process is capable of downscaling the resolution of ERA5 maps from a scale of 25km to 6.25km with accurate details.

Specifically, as illustrated in \cref{fig:main_fig}, SGD utilizes optimizable convolutional kernels $\mathcal{D}$ with parameter $\varphi$ in the reverse steps of conditional DDPMs to facilitate the resolution transformation of the sampled maps $x_t$.
% \textcolor{red}{need symbol here}
A distance function $\mathcal{L}$ is then employed to quantify the discrepancy between the convolved sampled map $\mathcal{D}(\tilde{x}_0)$ and the low-resolution ERA5 map $z$. 
The distance function can utilize the mean squared error (MSE) loss between the two maps as guidance from ERA5, or alternatively, integrate station-level observations to facilitate bias correction. 
The latter employs average mean absolute error (MAE) loss across two maps for each variable at global stations, thereby ensuring that SGD yields more precise numerical results at each station. 
Specific configurations and the effects of the distance function are shown in ~\cref{distance_function}. 
The gradients of distance function $\mathcal{L}$ are calculated to update the mean $\mu$ and variance $\Sigma$ utilized for sampling $x_{t-1}$, thereby ensuring the generated high-resolution images possess more accurate and rich details which are consistent with ERA5 map at a scale of $25km$. 
Concurrently, the gradients of $\mathcal{L}$ with respect to the convolutional kernel parameters $\varphi$ are also utilized to update themselves. 
This dynamic update mechanism allows the convolutional kernels to simulate the reverse of downscaling more accurately in the sampling process. 

Detailedly, $x_{t-1}$ is sampled by the conditional distribution $p(x_{t-1}|x_{t},y^\prime,z)$, where $y^\prime$ refers to the embedding representation of the GirdSat maps after the encoder module. 
% and $z$ refers to the low-resolution ERA5 map \textcolor{red}{if z has been mentioned before, it will not be introduced here} which is utilized to guide the generation of small-scale maps. 
Previous studies~\citep{dhariwal2021diffusion} have derived the conditional transformation formula of the sampling process: 
\begin{align}
\log_{}{p_\theta(x_{t}|x_{t+1},y^\prime,z)}&=\log_{}{(p_\theta(x_{t}|x_{t+1},y^\prime)p(z|x_{t})) }+N_1 \\
&\approx \log_{}{p_\theta(w)+N_2},
\end{align}
where $w\sim \mathcal{N}(w;\mu_\theta(x_t,y^\prime,t)+\Sigma \nabla_{x_t}\log_{}{p(z|x_t)}|_{x_t=\mu},\Sigma I)$, $N_1$ and $N_2$ is constants. The mean $\mu=\mu_\theta(x_t,y^\prime,t)$ and variance $\Sigma=\Sigma(x_t,y^\prime,t)$ is obtained by the conditonal DDPMs. 

Therefore, the sampling distributions integrate the conditional DDPMs with the gradient term, controlling the generation of high-resolution maps.
Inspired by~\cite{fei2023generative}, we employ a heuristic algorithm to approximate the value of the gradient term:
% \textcolor{red}{this description has puzzled reviewers. if this is claimed, we need to clarify the detailed proof and refer to it in appendix}
\begin{align}
\log_{}{ p(z\mid x_{t})}&=- \log_{}{N}-s\mathcal{L}( \mathcal{D}(\tilde{x}_0),z)\\
\nabla _{x_t}\log_{}{p(z|x_t)}&=-s\nabla_{\tilde{x}_0}\mathcal{L}(\mathcal{D}(\tilde{x}_0),z),
\end{align}
where $s$ refers to the parameter of the guidance scale which serves as a weight to control the guidance degree.
The gradient of the distance function with respect to $\tilde{x}_0$ is utilized for the calculation of the gradient term, which ultimately updates the mean of the reverse process. 

Based on this, we can derive the complete algorithm for the sampling process.
As shown in \Cref{alg.2}, our model undergoes $T$ reverse steps to sample the high-resolution ERA5 map $x_0$. 
At each step, the optimizable convolutional kernel up-scales the instantaneous estimated value $\tilde{x}_0$ to establish a distance function with the low resolution map $z$.
The gradients of distance function $\mathcal{L}$ with respect to $\tilde{x}_0$ and the parameters of convolution kernel are used to update the mean $\tilde{\mu}_t$ and the convolution kernel parameters, respectively. 
The real-time update mechanism ensures the accuracy of the model's sampling process, making the guidance of low-resolution ERA5 maps and station-level observations more precise and effective. 

Due to the significantly higher map resolution in the generation space compared to that of the pre-trained conditional DDPM, a patch-based approach has been employed to tackle this issue. 
The patch-based method partitions the high-resolution ERA5 map into multiple sub-regions based on a fixed stride, and calculates the corresponding distance metrics and gradient terms for each subregion with the corresponding segments of the guided map $z$. 
The average of the gradient terms across all sub-regions is then utilized for the update of the mean of sampling steps and the convolution kernel parameters. 
The detailed algorithmic framework of the patch-based methodology is introduced in Appendix. 
% \textcolor{red}{also need to clarify the details in appendix}


\begin{algorithm}[t]\small
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{\textbf{Sampling Process:} Guided diffusion model with the guidance of low-resolution ERA5 map $z$. Given a conditional diffusion model pre-trained on ERA5 and GridSat maps $\epsilon_\theta(x_t,y,t)$. }
\label{alg.2}
\begin{algorithmic}[1]
\REQUIRE Conditioning input GridSat satellite observation map $y$, low-resolution ERA5 map $z$. Downscaling convolutional function kernel $\mathcal{D}$ with parameter $\varphi$. Pre-trained encoder module $f$ with parameter $\phi$. Learning late $l$ and guidance scale $s$. Distance measure function $\mathcal{L}$. 
\ENSURE Output high-resolution ERA5 map $x_0$. 

\STATE Sample $x_T$ from $\mathcal{N}(0,I)$

\STATE $y' \gets f_\phi(y)$

    \FORALL{t from T to 1}
        \STATE $\tilde{x} _0=\frac{x_t}{\sqrt{\bar{\alpha}_t }}-\frac{\sqrt{1-\bar{\alpha}_t}\epsilon_\theta(x_t,y',t)}{\sqrt{\bar{\alpha}_t }}$\
        
        \STATE $\mathcal{L}_{\varphi,\tilde{x}_0} =\mathcal{L}(z,\mathcal{D}^{\varphi}(\tilde{x}_0))$\

        \STATE $\tilde{x}_0 \gets \tilde{x}_0-\frac{s(1-\bar{\alpha}_t) }{\sqrt{\bar{\alpha}_{t-1}}\beta_t}\nabla_{{\tilde{x}}_0}\mathcal{L}_{\varphi,\tilde{x}_0}$\
        \STATE $\tilde{\mu}_t=\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\tilde{x}_0+\frac{\sqrt{\bar{\alpha}_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}{x}_t$\

        \STATE $\tilde{\beta}_t=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$\
        
        \STATE Sample $x_{t-1}$ from $\mathcal{N}(\tilde{\mu}_t,\tilde{\beta}_tI)$\

        \STATE $\varphi \gets \varphi-l\nabla_{\varphi}\mathcal{L}_{\varphi,\tilde{x}_0}$\
    \ENDFOR
\STATE \textbf{return}  $x_0$
    \end{algorithmic}
\end{algorithm}