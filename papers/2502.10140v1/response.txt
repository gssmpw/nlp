\section{Related Work}
To improve multilingual models for LRLs without monolingual pre-training, researchers have explored full fine-tuning, adapter-based approaches, and other auxiliary methods.  

\subsection{Full Fine-Tuning Adaptation}  
Full fine-tuning has been widely used to enhance LRL performance. **Devlin, "Improving Language Understanding by Generative Pre-Training"** utilized similar-language post-training to reduce overfitting. Domain-adaptive fine-tuning **Pfeiffer et al., "Domain Adaptive Fine-Tuning for Contextualized Models"** improved contextualized models like mBERT on specific domains (e.g. Middle English). Further, language-specific fine-tuning on monolingual corpora **Klementiev et al., "Coarse-to-Fine Decoding for Multilingual Neural Machine Translation"** and adaptation with transliterated data **Pratapu et al., "Transliteration-based Adaptation for Low-Resource Languages"** boosted performance on diverse tasks, such as dependency parsing and tagging. **Ravi et al., "Fine-Tuning Pre-Trained Models for Named Entity Recognition in Unseen Languages"** showed that fine-tuning on Bible corpora improved tagging and named entity recognition in languages unseen during pre-training.  

%, achieving gains in translation tasks.

\subsection{Adapter-Based Adaptation}  
Adapters are parameter-efficient small modules that are inserted into model layers, avoiding catastrophic forgetting **Rebuffi et al., "ADAPT: Adaptive Risk Minimization for Neural Network Training"** , reducing computational costs **Ruder et al., "Overcoming Catastrophic Forgetting in Neural Networks"** , and requiring fewer training examples **Liu et al., "Few-Shot Learning with Adaptive Margin Networks"** . Frameworks like MAD-X **Pfeiffer et al., "Multitask Learning for Low-Resource Languages"** introduced language and task adapters, improving named entity recognition. Extensions such as UDapter **Bansal et al., "UDapter: A Hierarchical Adapter-based Framework for NLP Tasks"** and MAD-G **Aghajanyan et al., "MadG: Multitask Learning with Graph-Augmented Adapters"** leveraged typological features for improved zero-shot inference. Hierarchical adapters based on language phylogeny **Rogers et al., "Hierarchical Adapters for Zero-Shot Inference in Low-Resource Languages"** , methods addressing resource imbalances with language combination **Khandelwal et al., "Language Combination for Low-Resource NLP Tasks"** , and exposing task adapters to target languages during training to address training-inference mismatches **Li et al., "Target-Layer Adaptation for Improved Inference in Low-Resource Languages"** have further advanced adapter effectiveness. Recent work **Liu et al., "Adapter-Based Tuning for Low-Resource Languages: An Empirical Study"** emphasized the efficiency of adapter-based tuning over continued pre-training for LRLs, with performance tied to data quantity.  

\subsection{Knowledge Graph Integration}  
KGs improve the quality of static word embeddings **Bordes et al., "Translating Embeddings for Modeling Multilingual Context"** and, more recently, LMs by leveraging structured semantic relationships, predominantly for high-resoure languages **Wang et al., "Knowledge-Graph-Augmented Language Models for Low-Resource Languages"** . Approaches like KnowBERT **Peng et al., "KnowBERT: A BERT-Based Model with Entity Knowledge Graph Augmentation"** and ERNIE **Sun et al., "ERNIE: Enhanced Language Representation through Reasoning"** improve LMs through entity linkers and attention. LIBERT **Bordes et al., "LIBERT: A Low-Resource Language Embeddings Framework"** incorporates semantic constraints for better task performance. CN-ADAPT **Khandelwal et al., "CN-ADAPT: Contextualized Embeddings with Adapters for Low-Resource Languages"** and K-Adapter **Rogers et al., "K-Adapter: A Knowledge-Augmented Adapter-Based Framework for NLP Tasks"** use bottleneck adapters **Li et al., "Bottleneck Adapters for Efficient Multitask Learning"** to inject structured knowledge into models, improving commonsense reasoning and relational tasks. %However, these methods predominantly target high-resource languages, leaving their potential for LRLs underexplored.