\section{Related Work}
To improve multilingual models for LRLs without monolingual pre-training, researchers have explored full fine-tuning, adapter-based approaches, and other auxiliary methods.  

\subsection{Full Fine-Tuning Adaptation}  
Full fine-tuning has been widely used to enhance LRL performance. ____ utilized similar-language post-training to reduce overfitting. Domain-adaptive fine-tuning ____ improved contextualized models like mBERT on specific domains (e.g. Middle English). Further, language-specific fine-tuning on monolingual corpora ____ and adaptation with transliterated data ____ boosted performance on diverse tasks, such as dependency parsing and tagging. ____ showed that fine-tuning on Bible corpora improved tagging and named entity recognition in languages unseen during pre-training.  

%, achieving gains in translation tasks.

\subsection{Adapter-Based Adaptation}  
Adapters are parameter-efficient small modules that are inserted into model layers, avoiding catastrophic forgetting ____, reducing computational costs ____, and requiring fewer training examples ____. Frameworks like MAD-X ____ introduced language and task adapters, improving named entity recognition. Extensions such as UDapter ____ and MAD-G ____ leveraged typological features for improved zero-shot inference. Hierarchical adapters based on language phylogeny ____, methods addressing resource imbalances with language combination ____, and exposing task adapters to target languages during training to address training-inference mismatches ____ have further advanced adapter effectiveness. Recent work ____ emphasized the efficiency of adapter-based tuning over continued pre-training for LRLs, with performance tied to data quantity.  

\subsection{Knowledge Graph Integration}  
KGs improve the quality of static word embeddings ____ and, more recently, LMs by leveraging structured semantic relationships, predominantly for high-resoure languages ____. Approaches like KnowBERT ____ and ERNIE ____ improve LMs through entity linkers and attention. LIBERT ____ incorporates semantic constraints for better task performance. CN-ADAPT ____ and K-Adapter ____ use bottleneck adapters ____ to inject structured knowledge into models, improving commonsense reasoning and relational tasks. %However, these methods predominantly target high-resource languages, leaving their potential for LRLs underexplored.