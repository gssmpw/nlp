\section{Related Work}
To improve multilingual models for LRLs without monolingual pre-training, researchers have explored full fine-tuning, adapter-based approaches, and other auxiliary methods.  

\subsection{Full Fine-Tuning Adaptation}  
Full fine-tuning has been widely used to enhance LRL performance. \citet{neubig2018rapidadaptationneuralmachine} utilized similar-language post-training to reduce overfitting. Domain-adaptive fine-tuning \cite{han2019unsuperviseddomainadaptationcontextualized} improved contextualized models like mBERT on specific domains (e.g. Middle English). Further, language-specific fine-tuning on monolingual corpora \cite{gururangan2020don, Chau_2020} and adaptation with transliterated data \cite{muller-etal-2021-unseen} boosted performance on diverse tasks, such as dependency parsing and tagging. \citet{ebrahimi2021adaptpretrainedmultilingualmodel} showed that fine-tuning on Bible corpora improved tagging and named entity recognition in languages unseen during pre-training.  

%, achieving gains in translation tasks.

\subsection{Adapter-Based Adaptation}  
Adapters are parameter-efficient small modules that are inserted into model layers, avoiding catastrophic forgetting \cite{french1999catastrophic}, reducing computational costs \cite{houlsby2019parameter, strubell-etal-2019-energy}, and requiring fewer training examples \cite{faisal2022phylogenyinspiredadaptationmultilingualmodels}. Frameworks like MAD-X \cite{pfeiffer2020mad} introduced language and task adapters, improving named entity recognition. Extensions such as UDapter \cite{udapterlanguageadaptationtruly} and MAD-G \cite{ansell-etal-2021-mad-g} leveraged typological features for improved zero-shot inference. Hierarchical adapters based on language phylogeny \cite{faisal2022phylogenyinspiredadaptationmultilingualmodels}, methods addressing resource imbalances with language combination \cite{lee-etal-2022-fad, parovic-etal-2022-bad}, and exposing task adapters to target languages during training to address training-inference mismatches \cite{paroviÄ‡2023crosslingualtransfertargetlanguageready} have further advanced adapter effectiveness. Recent work \cite{pfeiffer-etal-2022-lifting, yong2023bloom1addinglanguagesupport} emphasized the efficiency of adapter-based tuning over continued pre-training for LRLs, with performance tied to data quantity.  

\subsection{Knowledge Graph Integration}  
KGs improve the quality of static word embeddings \cite{faruqui2014retrofitting, speer2017conceptnet, gurgurov2024gremlinrepositorygreenbaseline} and, more recently, LMs by leveraging structured semantic relationships, predominantly for high-resoure languages \cite{miller1995wordnet, navigli2012babelnet, speer2017conceptnet}. Approaches like KnowBERT \cite{peters-etal-2019-knowledge} and ERNIE \cite{zhang-etal-2019-ernie} improve LMs through entity linkers and attention. LIBERT \cite{lauscher2020specializingunsupervisedpretrainingmodels} incorporates semantic constraints for better task performance. CN-ADAPT \cite{lauscher2020common} and K-Adapter \cite{wang2020k} use bottleneck adapters \cite{houlsby2019parameter} to inject structured knowledge into models, improving commonsense reasoning and relational tasks. %However, these methods predominantly target high-resource languages, leaving their potential for LRLs underexplored.