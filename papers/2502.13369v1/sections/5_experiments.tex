\begin{table*}[]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Type} & \textbf{Query EM $\uparrow$} & \textbf{BLEU $\uparrow$} & \textbf{URI EM $\uparrow$} & \textbf{URI Hallucination $\downarrow$} \\ \hline \hline
\textbf{T5-Small}                                                           & finetuned & 35.08 \% & 81.87 & 38.44 \% & 27.75 \%         \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}T5-Small PGMR\\ (ours)\end{tabular}}     & finetuned & 74.23 \% & 92.76 & 80.15 \% & \textbf{0.38 \%} \\ \hline \hline
\textbf{Llama 3.1 8B}                                                       & finetuned & 0.72 \%  & 64.83 & 0.79 \%  & 89.84 \%         \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Llama 3.1 8B PGMR\\ (ours)\end{tabular}} & finetuned & 69.48 \% & 89.96 & 78.76 \% & \textbf{0.01 \%} \\ \hline \hline
\textbf{GPT 4o}                                                             & few-shot  & 1.26 \%  & 47.03 & 4.36 \%  & 81.81 \%         \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}GPT 4o PGMR\\ (ours)\end{tabular}}       & few-shot  & 43.53 \% & 73.53 & 69.60 \% & \textbf{0.0 \%}  \\ \hline
\end{tabular}
\caption{\textbf{Results on LCQUAD 2.0 (original split):} Across different LLMs, PGMR demonstrates a substantial reduction in hallucinations while achieving significantly higher Query EM scores than direct SPARQL generation, as discussed in \refsec{sec:results}.}
\label{tbl:t5_lc2_og_results}
\end{table*}



\section{Experimental Setup}
\label{sec:experiments}

%-------------------------------------------------------

\subsection{Datasets}
\label{sec:data}
We evaluate our work on two widely used datasets for SPARQL query generation: LCQUAD 2.0 \citep{lcquad2} and QALD-10 \citep{qald10}. Even though tagged versions of these datasets exist, where the entities and relations in the question are already linked and tagged with associated URIs from the KG, we use the untagged versions. Tagging is expensive, time-consuming, and impractical in the real world. SPARQL query generation becomes a much harder problem when using untagged data as it forces the language models to generate KG elements like URIs based on their internal parametric knowledge, which leads to hallucinations and out-of-distribution errors, as discussed in \refsec{sec:results}.

\noindent \textbf{LCQUAD 2.0} comprises a mix of simple and complex questions with associated SPARQL queries over Wikidata, crafted by human annotators from Amazon Mechanical Turk. The \textbf{original split} of LCQUAD 2.0 includes a broad dataset with 21k questions for training, 3k questions for validation, and 6k questions for testing. To specifically test how our models perform in the face of out-of-distribution URIs not seen before in training, we also construct an \textbf{"unknown URI" split} of LCQUAD 2.0 in line with \citet{reyd2023}. Every query in the test set of this version of the dataset includes at least one URI not seen during training, thus making accurate query generation a much harder problem. The "unknown URI" split contains 24k train, 3k validation, and 3k test questions and associated queries.

\noindent \textbf{QALD-10} is a benchmarking dataset that is a part of the  Question Answering over Linked Data (QALD) challenge series for KGQA using SPARQL query generation. Owing to the complexity of SPARQL queries that involve various clauses and literals, QALD-10 is recognized as one of the most challenging and practically applicable datasets in the QALD challenge series. Even though each question in QALD-10 is translated into 8 different languages, we only consider the English versions of the questions. The dataset consists of 412 training questions and 394 test questions, which is much smaller than LCQUAD 2.0 but contains more complex queries.
Given the limited availability of training data for QALD-10, we employ a two-step approach in our finetuning experiments: pre-training the LLM on LCQUAD 2.0 before finetuning them on QALD-10.

% To circumvent the dearth of training data for QALD-10 in our finetuning experiments, we pre-train our LLM models on LCQUAD 2.0 before fine-tuning on QALD-10.
% To circumvent the dearth of training data for QALD-10, we use few-shot prompting.

%-------------------------------------------------------

%-------------------------------------------------------

% \begin{table*}
% \centering
% \begin{tabular}{c|cccc}
% \hline
%                  & \textbf{Query EM $\uparrow$} & \textbf{BLEU $\uparrow$}  & \textbf{URI EM $\uparrow$} & \textbf{URI Hallucination $\downarrow$}   \\ \hline
% \textbf{T5}      & 45.58 \%          & 81.69        & 38.43 \%        & 27.75 \%          \\ \hline
% \textbf{RAG T5}  & 74.74 \%          & 90.35        & 70.18 \%        & 12.98 \%          \\ \hline
% \textbf{PGMR T5} & \textbf{82.17 \%} & \textbf{93.08}        & \textbf{80.15} \%      & \textbf{0.38 \%} \\ \hline
% \end{tabular}
% \caption{\textbf{Results on LCQUAD 2.0 (original split):} PGMR training improves the Query EM by nearly $37\%$ over normally fine-tuned T5 and by nearly $8\%$ over T5 fine-tuned using the RAG paradigm, as discussed in \refsec{sec:results}.}
% \label{tbl:t5_lc2_og_results}
% \end{table*}





% \begin{table*}
% \centering
% \begin{tabular}{c|cccc}
% \hline
%                  & \textbf{Query EM $\uparrow$} & \textbf{BLEU $\uparrow$}  & \textbf{URI EM $\uparrow$} & \textbf{URI Hallucination $\downarrow$}   \\ \hline
% \textbf{T5}      & 10.42 \%          & 68.50        & 0.36 \%        & 61.66 \%          \\ \hline
% \textbf{RAG T5}  & 71.31 \%          & 90.02        & 67.21 \%        & 16.01 \%          \\ \hline
% \textbf{PGMR T5} & \textbf{82.57 \%} & \textbf{93.62}        & \textbf{81.24}  \%      & \textbf{2.21 \%} \\ \hline
% \end{tabular}
% \caption{\textbf{Results on LCQUAD 2.0 (unknown URI split):} PGMR training improves the Query EM by nearly $72\%$ over normally fine-tuned T5, as discussed in \refsec{sec:results}.}
% \label{tbl:t5_lc2_unk_uri_results}
% \end{table*}

%-------------------------------------------------------

\subsection{Metrics}
\label{sec:metrics}
%-------------------------------------------------------

To measure the effectiveness of our models, we utilize the following three metrics: 

\noindent \textbf{BLEU} score \citep{bleu} is a popular neural machine translation metric that assesses the predicted query against the gold standard query by comparing tokens. 

\noindent \textbf{Query EM:} We evaluate the query exact match score (Query EM), which directly compares if the predicted query and the reference query are an exact match. 

\noindent \textbf{URI EM:} We assess the URI exact match (URI EM) score, a metric that directly evaluates whether the set of all URIs in the predicted SPARQL query matches exactly with those in the reference query.

\noindent \textbf{URI Hallucination:}  We define a new metric, called URI Hallucination, which measures the percentage of queries where at least one URI is hallucinated. A URI is considered hallucinated if it does not exist in the knowledge base memory.

%-------------------------------------------------------

%-------------------------------------------------------

\begin{table*}[]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Type} & \textbf{Query EM $\uparrow$} & \textbf{BLEU $\uparrow$} & \textbf{URI EM $\uparrow$} & \textbf{URI Hallucination $\downarrow$} \\ \hline \hline
\textbf{T5-Small}                                                           & finetuned & 0.16 \%  & 62.91 & 0.36 \%  & 61.66 \%        \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}T5-Small PGMR\\ (ours)\end{tabular}}     & finetuned & 59.97 \% & 86.28 & 78.06 \% & 4.63 \%         \\ \hline \hline
\textbf{Llama 3.1 8B}                                                       & finetuned & 0.03 \%  & 65.30 & 0.06 \%  & 91.01 \%        \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Llama 3.1 8B PGMR\\ (ours)\end{tabular}} & finetuned & 71.02\%  & 89.62 & 79.49 \% & \textbf{0.0 \%} \\ \hline \hline
\textbf{GPT 4o}                                                             & few-shot  & 0.89 \%  & 52.02 & 2.18 \%  & 86.17 \%        \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}GPT 4o PGMR\\ (ours)\end{tabular}}       & few-shot  & 55.61 \% & 83.41 & 68.08 \% & \textbf{0.0 \%} \\ \hline
\end{tabular}
\caption{\textbf{Results on LCQUAD 2.0 (unknown URI split):} Even in an out-of-distribution setting, PGMR significantly mitigates hallucinations and enhances Query EM performance compared to direct SPARQL generation across various LLMs, as discussed in \refsec{sec:results}.}
\label{tbl:t5_lc2_unk_uri_results}
\end{table*}

\subsection{Baselines}
\label{sec:baselines}

%-------------------------------------------------------

\noindent \textbf{Direct Generation} refers to the standard scenario where the LLM directly generates the SPARQL query given a question.

\noindent \textbf{PGMR:} As described in \refsec{sec:pgmr}, we implement the proposed PGMR, where SPARQL queries are first converted into an intermediate query, followed by finetuning or few-shot prompting an LLM on this intermediate representation. Our retriever then utilizes a non-parametric memory module to translate the intermediate queries generated by the LLM back into SPARQL queries.

\subsubsection{Language Models}
\label{sec:llms_used}

\noindent \textbf{T5:} Despite the development of various models for generating SPARQL queries with tagged questions \citep{banerjee,karou2023,qi2024enhancing}, finetuned T5 \citep{t5} is currently the state-of-the-art model for generating SPARQL queries from untagged data on LC-QUAD 2 \citep{karou2023}. 
We use the "t5-small" model in our study, as \citet{banerjee} show a limited benefit of the added parameters when using a larger T5 model for SPARQL query generation.

\noindent \textbf{Llama 3.1 8B:} We finetune Llama 3.1 8B Instruct \citep{llama3} to evaluate and compare direct generation to PGMR in our experiments.

\noindent \textbf{GPT 4o:} Additionally, we employ 25-shot prompting with GPT 4o \citep{gpt4} to assess and compare the few-shot performance of direct generation with PGMR.

% \noindent \textbf{RAG T5:} We additionally compare our results against T5 fine-tuned using the RAG paradigm, wherein the question is initially processed by a retriever that retrieves $k$ URIs from a non-parametric memory whose labels most closely match the question text in latent space, as described in \refsec{sec:rag}. This acts as a strong baseline for evaluating our proposed PGMR model.





Unlike the evaluation approach utilized by \citet{banerjee} and \citet{qi2024enhancing}, which involves generating multiple queries for each question and selecting the first query that successfully executes and returns an answer as the predicted query, our evaluation generates only a single query from our models. 
% We contend that this approach is more practical.
Additionally, since LLMs can often generate identical queries with variable names (e.g., $?value$ and $?s$) different from those found in the gold query (e.g., $?uri$ and $?sbj$), we normalize the variable names (e.g., $?var0$ and $?var1$) in our generated SPARQL queries in line with the evaluation approach followed by \citet{qi2024enhancing}.
 % (e.g. "\textit{select ?value where \{ wd:q214801 p:p1411 ?s . ?s ps:p1411 wd:q106291 . ?s pq:p2453 ?value \}}" and "\textit{select ?uri where \{ wd:q214801 p:p1411 ?sbj . ?sbj ps:p1411 wd:q106291 . ?sbj pq:p2453 ?uri \}}" are both transformed to "\textit{select ?var0 where \{ wd:q214801 p:p1411 ?var1 . ?var1 ps:p1411 wd:q106291 . ?var1 pq:p2453 ?var0 \}}").

%-------------------------------------------------------




\section{Results}
\label{sec:results}

% \subsection{PGMR drastically reduces hallucinations}
% \label{sec:hallucination_results}

\noindent \textbf{PGMR drastically reduces hallucinations.}
One of the key limitations of LLMs is their tendency to “hallucinate,” generating content that appears valid but lacks factual accuracy. This problem is especially pronounced in LLM-based direct SPARQL query generation, where LLMs often fabricate URIs that are not present in the knowledge graph.
% This problem has raised increasing concerns about safety and ethics in the context of widespread LLM applications, leading to a growing body of research \citep{hallucination_survey, hallucination_inevitable} focused on the classification, comprehension, and mitigation of hallucinations.
As seen in \reftbl{tbl:t5_lc2_og_results}, PGMR reduces URI hallucinations compared to direct SPARQL generation by 27.37\% for T5-small, 89.83\% for Llama 3.1 8B, and 81.81 \% for GPT 4o.
This improvement is even more substantial on the unknown URI split of LCQUAD 2.0, where PGMR reduces URI hallucinations by 57.03\% for T5-small, 91.01\% for Llama 3.1 8B, and 86.17\% for GPT 4o, as demonstrated in \reftbl{tbl:t5_lc2_unk_uri_results}.
On the harder QALD-10 dataset, direct SPARQL query generation increases URI hallucinations to 66.50\% for T5-small, 96.14\% for Llama 3.1 8B, and 83.76\% for GPT 4o, while PGMR completely eliminates hallucinations for all LLMs tested, as seen in \reftbl{tbl:t5_qald_results}.
These results highlight the effectiveness of PGMR in mitigating URI hallucinations regardless of the LLM, dataset, and data distribution used, with most cases demonstrating an almost complete suppression of hallucinations.
% This holds true in both few-shot prompting and finetuning scenarios.
This improvement is largely driven by PGMR’s approach, which utilizes a non-parametric memory module for URI retrieval following the LLM-based generation of an intermediate SPARQL-formatted query, bypassing the dependence on the LLM’s internal parametric knowledge.
% \reftbl{tbl:t5_lc2_og_results} shows that PGMR T5 reduces URI hallucinations by 27\% compared to traditionally fine-tuned T5 and by over 12\% over RAG T5 on the original split of LCQUAD 2.0. This improvement is even more substantial on the out-of-distribution unknown URI split of LCQUAD 2.0, where PGMR T5 reduces URI hallucinations by approximately 59\% compared to conventional T5 fine-tuning, and by nearly 14\% compared to RAG T5, as demonstrated in \reftbl{tbl:t5_lc2_unk_uri_results}. Similar trends are observed with the QALD-10 dataset, where PGMR T5 reduces URI hallucinations by over 66\% compared to regular T5 fine-tuning.
% The observed improvement can be primarily attributed to PGMR’s methodology of employing a non-parametric memory module for URI retrieval after generating an intermediate query in SPARQL syntax with the LLM instead of relying on the LLM’s internal parametric memory. 
% Notably, PGMR Llama 3.1 8B demonstrates 0\% hallucinations on QALD-10, whereas it exhibits some hallucinations on LCQUAD 2.0. This discrepancy is due to the fact that LCQUAD 2.0 includes 30 URIs that are no longer present in the Wikidata KG, which our study counts as hallucinations. In contrast, QALD-10 contains URIs that all exist in the current version of Wikidata.

%-------------------------------------------------------

%-------------------------------------------------------

\begin{table*}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Type} & \textbf{Query EM $\uparrow$} & \textbf{BLEU $\uparrow$} & \textbf{URI EM $\uparrow$} & \textbf{URI Hallucination $\downarrow$} \\ \hline \hline
\textbf{T5-Small}                                                           & finetuned & 5.83 \%  & 25.49 & 6.85 \%  & 66.50 \%        \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}T5-Small PGMR\\ (ours)\end{tabular}}     & finetuned & 18.27 \% & 30.26 & 20.30 \% & \textbf{0.0 \%} \\ \hline \hline
\textbf{Llama 3.1 8B}                                                       & finetuned & 0.25 \%  & 21.74 & 0.51 \%  & 96.14 \%        \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Llama 3.1 8B PGMR\\ (ours)\end{tabular}} & finetuned & 17.22 \% & 29.40 & 27.76 \% & \textbf{0.0 \%} \\ \hline \hline
\textbf{GPT 4o}                                                             & few-shot  & 3.30 \%  & 38.63 & 7.10 \%  & 83.76 \%        \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}GPT 4o PGMR\\ (ours)\end{tabular}}       & few-shot  & 25.52 \% & 51.66 & 44.33 \% & \textbf{0.0 \%} \\ \hline
\end{tabular}
\caption{\textbf{Results on QALD-10:} PGMR consistently minimizes hallucinations across LLMs while attaining significantly better Query EM scores than direct SPARQL generation, even for the more complex QALD-10 dataset, as discussed in \refsec{sec:results}.}
\label{tbl:t5_qald_results}
\end{table*}

%-------------------------------------------------------

% \subsection{PGMR enhances the quality of SPARQL queries}
% \label{sec:quality_results}

\noindent \textbf{PGMR enhances the quality of SPARQL queries.}
Across all assessed LLMs, PGMR consistently enhances the accuracy and structure of SPARQL queries on the untagged versions of the LCQUAD 2.0 and QALD-10 datasets. As seen in \reftbl{tbl:t5_lc2_og_results}, the application of our PGMR approach yields a remarkable enhancement in query EM, achieving a boost of nearly 37\% for T5, 69\% for Llama 3.1 8B, and 42\% for GPT 4o over the traditional direct SPARQL query generation on the original split of LCQUAD 2.0. PGMR also improves the average BLEU score by almost 21 points across LLMs on the original split of LCQUAD 2.0.
As seen in \reftbl{tbl:t5_lc2_unk_uri_results}, this improvement is particularly pronounced on the out-of-distribution unknown URI split of LCQUAD 2.0, where PGMR achieves an average query EM score of more than 62\% across LLMs tested, significantly outperforming direct generation, which attains only 0.36\% on average.
This further highlights the challenges faced by LLMs in directly generating SPARQL queries when the test data is out-of-distribution, and some of the URIs have not been seen during training.
On the QALD-10 dataset, our PGMR approach enhances query EM performance over direct generation by approximately 13\% for T5, 17\% for Llama 3.1 8B, and 22\% for GPT 4o, as seen in \reftbl{tbl:t5_qald_results}.
Our results confirm that PGMR not only minimizes hallucinations to a great extent but also improves the accuracy and reliability of SPARQL query generation across various LLMs, datasets, and data distributions.



% Additionally, PGMR improves the BLEU score by approximately 12 points when compared to conventionally fine-tuned T5. On the QALD-10 dataset, the PGMR training approach enhances Query EM performance by approximately 13\% compared to standard T5 fine-tuning and achieves an improvement of 1.27\% over T5 fine-tuned with the RAG methodology, as seen in \reftbl{tbl:t5_qald_results}.
% Even though all the experiments in this study are on untagged datasets, we note that conventionally fine-tuned T5 performs exceptionally well on both splits of the tagged version of the LCQUAD 2.0 dataset, achieving a Query EM of over 98\% and a URI EM of 99.9\%.


%-------------------------------------------------------

% \subsection{PGMR shows resilience to data distribution shifts}
% \label{sec:distribution_results}

\noindent \textbf{PGMR shows resilience to data distribution shifts.}
PGMR proves to be more resilient to distributional changes than direct SPARQL query generation, demonstrating superior generalization across LLMs as the dataset transitions from the original split of LCQUAD 2.0 to the unknown URI split (as discussed in \refsec{sec:data}).
% Comparing Tables \ref{tbl:t5_lc2_og_results} and \ref{tbl:t5_lc2_unk_uri_results}, we see that the query EM scores for PGMR hardly reduce from 62.4\% to 62.2\% on average across LLMs as the data distribution shifts from the original to the URI split of LCQUAD 2.0.
% This is in stark contrast to direct query generation, where the query EM drastically reduces from nearly 12\% to just 0.43\% on average across LLMs as the data distribution shifts from the original to the URI split of LCQUAD 2.0.
As shown in Tables \ref{tbl:t5_lc2_og_results} and \ref{tbl:t5_lc2_unk_uri_results}, PGMR  mitigates out-of-distribution performance degradation, with its average query EM score across LLMs remaining nearly unchanged, decreasing marginally from 62.4\% to 62.2\% when transitioning from the original split to the unknown URI split of LCQUAD 2.0.
This is in sharp contrast to direct query generation, which experiences a substantial degradation in performance, as its query EM score drops from nearly 12\% to a mere 0.36\% on average across LLMs.
This superior out-of-distribution performance stems from the fundamental difference between PGMR and traditional direct SPARQL query generation. While the latter depends on the LLM’s internal parametric knowledge for URI generation, PGMR instead leverages a non-parametric memory module to handle KG elements, allowing the LLM to focus on generating syntactically correct SPARQL queries.

% \noindent \textbf{PGMR performance does not degrade on unknown URI split:} As seen in Tables \ref{tbl:t5_lc2_og_results} and \ref{tbl:t5_lc2_unk_uri_results}, the Query EM performance of traditionally fine-tuned T5 degrades by approximately 30\% when evaluated on the unknown URI split of LCQUAD 2.0 as compared to its performance on the original split. In stark contrast, PGMR maintains consistent performance, with its Query EM remaining nearly identical across both the original and unknown URI splits of LCQUAD 2.0. 
% PGMR T5 demonstrates a Query EM performance improvement of nearly 72\% over conventionally fine-tuned T5 and approximately 11\% over RAG T5 on the unknown URI split of LCQUAD 2.0.
% This remarkable out-of-distribution performance results from the reliance of traditionally fine-tuned T5 on internal parametric knowledge accrued during fine-tuning to produce accurate and relevant URIs, contrasting with PGMR T5, which employs a non-parametric memory module for the management of KG elements.

% \noindent \textbf{PGMR increases URI exact match score} across the datasets evaluated, as seen in Tables \ref{tbl:t5_lc2_og_results}, \ref{tbl:t5_lc2_unk_uri_results}, and \ref{tbl:t5_qald_results}.
% PGMR increases URI exact match score across the datasets evaluated, as seen in Tables 1, 2, and 3.
% On the original split of LCQUAD 2.0, PGMR achieves a 61.64\% average improvement in URI EM over conventional direct SPARQL query generation across LLMs evaluated.

% \subsection{PGMR increases URI EM}
% \label{sec:uri_em_results}

\noindent \textbf{PGMR increases URI EM.}
PGMR exhibits a substantial enhancement in URI exact match (EM) accuracy across a diverse range of LLMs, datasets, and data distributions, further establishing its advantages over conventional direct query generation methods, as evidenced by Tables \ref{tbl:t5_lc2_og_results}, \ref{tbl:t5_lc2_unk_uri_results}, and \ref{tbl:t5_qald_results}.
On the original LCQUAD 2.0 split, PGMR substantially improves the accuracy of URIs in the generated SPARQL queries and achieves significant URI EM improvements, with score increases of 41.71\% for T5, 77.97\% for Llama 3.1 8B, and 65.24\% for GPT-4o over conventional direct query generation techniques.
The impact of PGMR is even more pronounced when the data distribution shifts to the unknown URI split of LCQUAD 2.0, with PGMR gaining 77.7\% for T5, 79.43\% for Llama 3.1 8B, and 65.9\% for GPT 4o over the direct query generation approach, which experiences significant performance degradation across all evaluated LLMs.
This trend continues on QALD-10, where PGMR improves the URI EM accuracy by 13.45\% for T5, 27.25\% for Llama 3.1 8B, and 37.23\% for GPT 4o over direct SPARQL query generation.

% On the original split of LCQUAD 2.0, PGMR T5 achieves a 42\% improvement in URI EM over conventionally fine-tuned T5 and a 10\% improvement over RAG T5. PGMR T5 outperforms T5 by a factor of approximately 14\% URI EM score on the QALD-10 dataset. The improvement in URI EM score is especially striking on the out-of-distribution unknown URI split of LCQUAD 2.0, where PGMR outperforms traditionally fine-tuned T5 by 81\% and RAG T5 by 14\%.

%-------------------------------------------------------



%-------------------------------------------------------
