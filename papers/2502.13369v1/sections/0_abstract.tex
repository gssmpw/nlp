\begin{abstract}


The ability to generate SPARQL queries from natural language questions is crucial for ensuring efficient and accurate retrieval of structured data from knowledge graphs (KG). While large language models (LLMs) have been widely adopted for SPARQL query generation, they are often susceptible to hallucinations and out-of-distribution errors when producing KG elements like Uniform Resource Identifiers (URIs) based on internal parametric knowledge. This often results in content that appears plausible but is factually incorrect, posing significant challenges for their use in real-world information retrieval (IR) applications. This has led to increased research aimed at detecting and mitigating such errors. In this paper, we introduce PGMR (Post-Generation Memory Retrieval), a modular framework that incorporates a non-parametric memory module to retrieve KG elements and enhance LLM-based SPARQL query generation. Our experimental results indicate that PGMR consistently delivers strong performance across diverse datasets, data distributions, and LLMs. Notably, PGMR significantly mitigates URI hallucinations, nearly eliminating the problem in several scenarios.

% Large language models (LLMs) are frequently employed for SPARQL query generation but are susceptible to hallucinations and out-of-distribution errors from generating knowledge graph (KG) elements like Uniform Resource Identifiers (URIs) using their internal parametric knowledge. They often produce content that appears credible but lacks factual accuracy. This issue poses serious challenges to the dependability of LLMs in real-world information retrieval (IR) applications and has prompted extensive research efforts to identify and reduce such inaccuracies.
% Adding a non-parametric memory to manage KG elements can greatly reduce such hallucinations and improve the accuracy of LLM-based SPARQL query generation. This paper explores PGMR (Post-Generation Memory Retrieval), a modular architecture where LLMs generate intermediate queries in SPARQL syntax, and a non-parametric memory module retrieves KG elements to form the full SPARQL query.
% Our experiments show that despite its simplicity, PGMR consistently demonstrates strong performance in SPARQL query generation across a range of datasets, data distribution shifts, and LLMs.
% Remarkably, PGMR leads to a considerable decrease in URI hallucinations, achieving a near-complete elimination of this problem across various instances.





% Our experiments show that this architecture improves the quality of SPARQL queries by reducing URI hallucinations.




% When evaluating the query exact match score, PGMR demonstrates a substantial improvement of 37\% over a conventionally fine-tuned LLM on the LCQUAD 2.0 dataset.
% Additionally, when the test set is out-of-distribution,  PGMR reduces URI hallucinations by approximately 59\% compared to conventional LLM finetuning and by nearly 14\% compared to RAG.

% The use of pre-trained language models (PLMs) for generating SPARQL queries has become prevalent in recent research. However, this approach can lead to significant challenges, such as hallucinations and out-of-distribution errors, particularly when the models generate knowledge graph (KG) elements like URIs based on their internal, parametric knowledge. To address these issues, we propose a novel modular architecture that incorporates a non-parametric memory module dedicated to managing KG elements. This memory module works in conjunction with pre-trained language models, which are responsible for generating the SPARQL code, ensuring the retrieval of accurate and relevant KG elements. By separating the generation of SPARQL queries from the retrieval of KG elements, our approach enhances the precision and reliability of SPARQL query generation, mitigating common pitfalls associated with the current PLM-based methodologies.
% We propose
\end{abstract}