\section{Proposed Model}
\label{sec:method}
In this section, we present our proposed model for SPARQL query generation using LLMs, termed PGMR (described in \refsec{sec:pgmr}). 
% Additionally, we introduce a new robust baseline for this task, leveraging the well-known RAG paradigm \citep{knnlm, rag_og}.
Our model employs an external knowledge base, such as a KG, as a non-parametric memory to enhance the grounding of LLM generations.
% The proposed RAG baseline for SPARQL generation (described in \refsec{sec:rag}) augments the input prompt containing the question with retrieved memory information before the PLM generates a query.
In PGMR, the LLM first generates an intermediate query with SPARQL syntax and KG elements in natural language (as discussed in \refsec{sec:transform}) given only the question and then incorporates the KG URIs from memory post-generation to construct the SPARQL query.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Retrieval Augmented Generation (RAG)}
% \label{sec:rag}
% Retrieval Augmented Generation (RAG) \citep{knnlm, rag_og, guu2020retrieval, lewis2020retrieval} has become a popular technique for injecting external information into PLMs, helping them generate content grounded in an external knowledge base like a KG. As seen in \reffig{fig:rag}, we propose RAG for SPARQL query generation, where the question first goes to a retriever (discussed in further detail in \refsec{sec:retriever}) which returns $k$ URIs from the memory whose labels are the closest to the question text in latent space, where $k$ is a positive integer. The URIs and their labels are appended to the question to create the prompt for the PLM (see \reffig{fig:rag_prompt}). The PLM then uses this information to generate the SPARQL query corresponding to the given question.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Post-Generation Memory Retrieval (PGMR)}
\label{sec:pgmr}
% Although RAG serves as a robust baseline for the SPARQL query generation task (as discussed in \refsec{sec:experiments}), it has inherent limitations arising from its retrieval process. Questions often contain multiple entities and relations, either explicitly stated or implied, and due to the absence of explicit entity linking, the retriever's selection of the top $k$ URIs based on label proximity to the question may overlook crucial URIs needed for generating an accurate SPARQL query.
% Consequently, this limitation can result in decreased accuracy of SPARQL generation by the PLM. Furthermore, this method may fail to mitigate hallucinations, as the PLM is not necessarily constrained to integrate the retrieved URIs into the generated SPARQL query (further discussed in \refsec{sec:results}). Essentially, RAG might generate URIs that do not even exist within the KG.

LLM-driven SPARQL query generation has emerged as a focus of recent research \citep{meyer2024assessing, karou2023, banerjee, qi2024enhancing, Text2SPARQL}. Despite its potential, this approach is susceptible to significant issues, including hallucinations and out-of-distribution errors, particularly when KG elements like URIs are generated without direct grounding in external knowledge sources.
Due to these hallucinations, LLM-generated SPARQL queries may maintain syntactic correctness while introducing factual errors, such as URIs that do not exist within the KG.

To address these issues, we introduce PGMR (“Post-Generation Memory Retrieval”), a novel method for SPARQL query generation utilizing LLMs. In PGMR, we initially convert SPARQL queries into an intermediate query format by substituting each URI with special "starturi" and "enduri" tokens, placing the natural language URI label between them, as outlined in \refsec{sec:transform}.
This modified data is then used to either finetune or few-shot the LLM.
During inference, our retriever (discussed in \refsec{sec:retriever}) converts the intermediate queries generated by the LLM back into SPARQL queries by retrieving the URIs whose labels are the closest match in latent space to the predicted labels within the "starturi" and "enduri" markers in the intermediate query, as discussed in \refsec{sec:inference}.
PGMR’s approach of generating only one URI label between each pair of "starturi" and "enduri" tokens simplifies the retriever’s task, thereby increasing the accuracy of retrieving the correct URI.
Unlike LLM-based direct SPARQL query generation, which may produce URIs not present in the KG, PGMR's approach of conducting memory retrieval post-LLM generation mitigates the risk of hallucinating non-existent KG URIs, as discussed in \refsec{sec:results}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/SPARQL_Query_transform.pdf}
    % \caption{In PGMR, the LLM generates intermediate SPARQL queries with URI labels in special tokens, which are then converted back to SPARQL queries via a retriever. To teach the LLM, the training data is converted from SPARQL to intermediate queries, as detailed in \refsec{sec:transform}}
    \caption{To teach the LLM, the training data is first converted from (a) SPARQL to (b) intermediate queries, as detailed in \refsec{sec:transform}}
    \label{fig:transform}
\end{figure}

PGMR enables the LLM to generate intermediate SPARQL queries with natural language entities and relations enclosed in special tokens. These intermediate queries are later grounded in KG URIs using the retriever to produce the final SPARQL query.
In PGMR, the LLM generates intermediate SPARQL queries containing natural language entities and relations wrapped in special tokens, which are later grounded in KG URIs through the use of a retriever to produce the final SPARQL query. To facilitate this, the training data is first transformed from the (a) SPARQL query to the (b) intermediate query to teach the LLM how to generate intermediate queries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/SPARQL_Retriever.pdf}
    \caption{The retriever finds each natural language URI label in the generated intermediate query using the special tokens and retrieves the corresponding URI from memory using similarity search, as discussed in \refsec{sec:retriever}}.
    \label{fig:retriever}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Data Transformation}
\label{sec:transform}
Predicting the appropriate Wikidata URIs for SPARQL queries poses a significant challenge for LLMs because they lack pre-training on this specific data type.
The models must rely solely on their parameters to recall mappings between the natural language descriptions of entities and relations in the question, and their corresponding URIs.
In contrast, LLMs find it significantly easier to predict natural language labels for these URIs given a question.
Based on this understanding, we transform the SPARQL queries in our training data into intermediate queries, thus simplifying the SPARQL query generation task.
As seen in \reffig{fig:transform}, we substitute each URI in the original query with its label, which we obtain from Wikidata using its official Python library\footnote{\url{https://pypi.org/project/Wikidata/}}.
We then surround these labels with "starturi" and "enduri" tokens, which our memory retriever (described in \refsec{sec:retriever})  uses to identify the URI labels' locations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Memory Retriever}
\label{sec:retriever}
\noindent \textbf{Memory:} We utilize a memory module designed as a dictionary that pairs LLM encoder-generated embeddings of URI labels and their descriptions as keys with the corresponding URIs as values. This module includes label embeddings and URIs for all URIs in the dataset.

% Our memory is a dictionary with PLM encoder embeddings of the URI labels along with their descriptions as keys and the corresponding URIs as the values. It contains the URI label embeddings and URIs for all URIs extracted from our dataset.
%the train and the validation sets of our dataset.

\noindent \textbf{Retriever:} Given a natural language input such as "The Truman Show," our memory retriever locates the URI with the label most similar to this text within the memory. 
As illustrated in \reffig{fig:retriever}, the text is initially encoded into an embedding by an encoder such as BGE \citep{chen2024bge} or MPNet \citep{mpnet}.
The retriever then uses FAISS \citep{faiss_lib,faiss_gpu} to retrieve the URI whose label embedding is nearest to the input text embedding from the memory.
We use BGE as the encoder for our retriever in this study.

% For a given natural language text like "the truman show" our memory retriever returns the URI whose label is most similar to it from the memory. As seen in \reffig{fig:retriever}, the natural language text first goes to a PLM encoder, like BERT \citep{bert} or MPNet \citep{mpnet}, that encodes the text into an embedding. The retriever then uses FAISS \citep{faiss_lib,faiss_gpu} to find the URI whose label embedding is the closest to the text embedding from the memory.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Setup}
\label{sec:inference}

\noindent \textbf{Training Setup:} Using the transformed data from \refsec{sec:transform}, we finetune the LLM to generate intermediate queries given the questions.

\noindent \textbf{Few-shot Setup:} Drawing on the transformed data, we apply few-shot prompting to the LLM, enabling it to generate intermediate queries corresponding to the input question.

\noindent \textbf{Inference:} During the inference phase, we initially produce an intermediate query for each question using the LLM. The intermediate query includes predicted URI labels marked by "starturi" and "enduri" tokens. Each of these labels is subsequently replaced by its corresponding URI through the retriever, thereby forming the final SPARQL query.

% During inference, we first generate an intermediate query using the PLM for each question. 
% As identified by the "starturi" and "enduri" tokens, each predicted URI label in the intermediate query is replaced by the associated URI using the retriever during inference, thus giving the final SPARQL query.
% Subsequently, we replace all URI labels in the generated intermediate query with the URIs from the memory using the retriever (as discussed in \refsec{sec:retriever}). Thus giving the final SPARQL query. 
% We then use this query to get the final answer from the KG (Wikidata).