\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Hyperparameters}
\label{sec:hyperparameters}

Our T5-small model has 60,506,624 trainable parameters and was finetuned on a single 80GB Nvidia A100 GPU within 14 hours with a batch size of 128.
We used a learning rate of 0.0015 for finetuning T5 and train for 150 epochs on LCQUAD 2.0 and 30 epochs on QALD-10. Owing to the limited training data for QALD-10, we finetuned our versions of T5 pre-trained on the much larger LCQUAD 2.0 dataset. 

For our experiments with Llama 3.1 8B, we fine-tuned the Llama-3.1-8B-Instruct model, training it for 10 epochs using the AdamW optimizer with a learning rate 0.0001, no weight decay, and a fixed seed 42 to ensure reproducibility. Mixed precision training and quantization were enabled to improve efficiency. We employed a parameter-efficient finetuning strategy using LoRA, with a low-rank dimension of 8, a scaling factor (alpha) of 32, and a dropout rate of 0.05. The complete training process took 2 hours, 20 minutes, and 49 seconds, and notably, the model achieved convergence at epoch 5, underscoring its rapid convergence and effectiveness in capturing key aspects of the target data.
