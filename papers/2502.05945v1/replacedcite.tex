\section{Related work}
%% REFUSAL 
It has been demonstrated before that the safety barriers of aligned LLMs, which are supposed to produce ethical, unharmful, and safe output can be easily bypassed e.g. through supervised fine-tuning ____.
Recently similar results have been achieved for various forms of layer-wise interference-time intervention techniques aiming e.g. at reducing the model's refusal behaviour to harmful input prompts ____. %attention head wise 
____ developed a layer-based method named Safety Concept Activation Vectors (SCAVs). Here, the direction for intervention is the normal vector of the decision boundary from a trained linear classifier. The classifier is trained to classify activations from the feed-forwad-layers of the attention blocks if they are from "harmful" or "unharmful" prompts. Hyperparameters for the method are coefficients for intervention at each layer and number of layers to intervene on. Similarly, ____ are using contrastive prompts to steer model behaviour with residual stream activations at the last token position. They also calculate the mean of "harmful" activations and mean of "harmless" activations and use their difference as the direction to intervene on. Finally, they subtract the projected difference from the activation during model generation. They report almost perfect results on bypassing the model's filter mechanism against refusing to produce harmful output.
____ follow also a layer-wise intervention strategy named Contrastive Activation Addition (CCA). First a multiple-choice setting is evaluated where to the question either "(A" or "(B" is appended and then the probability of the token matching the ground-truth is measured. Relevant layers are then identified by measuring how an intervention on the transformer-block feed-forward layer influences the probability for generating either "A" or "B" and matching the ground-truth. Identified layers and interventions are also tested if they generalise to open-ended generation. 
\par
In contrary, layer based intervention strategies have also been applied to promote favourable behaviour such as avoiding toxic language ____, investigate internal representations for factual knowledge ____, identifying truthful answers ____, and other ____.
____ propose a methodology specifically named "Inference-Time Intervention" (ITI). ITI works on the attention head level, by training linear probes on the activations of each attention head for example truthful and "hallucinated" responses. Attention heads, where probes have the highest accuracy on differentiating between a truthful response and an "hallucination" are chosen for intervention by taking the normal vector of the classifier's decision boundary. Finally, ____ investigate how different musical concepts are encoded in attention heads for a generative music transformer and hypothesise how a head-wise intervention could be applied to match specific user preferences.

%multiple-behaviours such as myopic, syphancy 
% - Is known that safety-filters at least for refusal can be removed with (low-rank) fine-tuning methods. 
% Applying intervention to remove safety barriers: 
% - Intervention applied on bypassing aligned model behaviour as  for "refusing" harmful requests. 
%-
%- another layer based method intervention method which gets direction from from pairs of prompts to intervene ____
%- similar layer-based methodology applied to steering model away from toxic dataset torwards targeted genres 
%- investigate internal representation or factual true / false knowledge and steering behaviour torwards, report that mass-mean-probing better than linear probes trained on activations 
%____


%- multiple-choice setting mutli-behaviour steering for different beahiours at the same time ____
%- head-wise probing of a generative music transformer - identifying selected concepts and specialised attention heads discussing potentials of head-wise intervention  ____
%- apply spectral methods to intervene on layer-based activations for Truthful response
%- enable non-linear intervention in Richer space ____
%- 
%- ITI uses fine-tuned judge LLM.