\section{Introduction}
\begin{figure}[!ht]
\centering
\includegraphics[width=1\linewidth]{picture/teaser.pdf}
\vspace{-0.6cm}
% \caption{Taking AMR as an example, comparing to the existing methods where such structured representation is passed to LLMs in the prompt as a code format, we propose two settings, a \textbf{training-free} one where we convert AMR to natural language description, and a \textbf{training-dependent} one where we fine-tune LLMs with AMR and task instructions. Taking PAWS as an example, the performance of the previous method decreased by 5.18\% after incorporating SR, whereas our approach achieved performance improvements of 3.17\% and 12.38\%, respectively.}
\caption{We propose two novel AMR integration approaches: a training-free method using natural language descriptions and a training-dependent fine-tuning paradigm. Evaluation on PAWS shows +3.17\% and +12.38\% improvements respectively, contrasting with the -5.18\% decline in conventional code-format methods.}
\vspace{-0.4cm}
\label{fig:teaser}
\end{figure}

Structured representations (SR), manifested in Abstract Meaning Representation (AMR)~\citep{damonte2016incremental, knight2020abstract, ramirez2024natural}, Parse Syntax Trees (PST)~\citep{sachan2020syntax}, and First-Order Logic (FOL)~\citep{barwise1977introduction}, have been fundamental to NLP~\citep{manning1999foundations, collobert2011natural}, serving as sophisticated frameworks for capturing semantic relationships and linguistic structures~\citep{banarescu2013abstract, wang2015transition}. An example of AMR, PST, and FOL is depicted in Figure~\ref{fig:apf}. 

In the era of LLMs, the paradigm for optimal SR integration remains an open research challenge. Despite LLMs' capabilities, direct integration of SR into prompts, as illustrated in Figure~\ref{fig:teaser}, has proven counterproductive~\citep{jin2024analyzing}. We posit that this performance degradation stems from LLMs' inherent limitations in processing structured representations, where direct exposure to complex linguistic structures impedes rather than enhances their reasoning process.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{picture/amr_pst_fol_example.pdf}
\caption{The AMR, PST, and FOL of the sentence ``John saw a dog''.}
\label{fig:apf}
\vspace{-0.2in}
\end{figure}


To address the aforementioned challenges and effectively leverage SR in LLMs, we introduce SR-LLM, a comprehensive framework with dual configurations for structural knowledge integration. The training-free approach transforms SR into natural language descriptions (SR-NLD), enhancing prompt comprehension by reformulating structured information into semantically rich, accessible formats that facilitate nuanced reasoning and reduce ambiguity. Complementarity, the training-dependent paradigm employs supervised fine-tuning on task-specific SR datasets (termed Gen-SR) to establish robust SR-task connections through iterative exposure to structured data, enabling the model to develop sophisticated internal representations and leverage deep structural knowledge during inference across diverse NLP tasks.

% To address these challenges and effectively integrate structured representations into LLMs, we propose a novel framework called SR-LLM, which encompasses two configurations: training-free and training-dependent, illustrated in the B and C part of Figure~\ref{fig:teaser}. In the training-free setting, we employ rule-based methods to convert SR into natural language description(SR-NLD), then incorporating it into prompts to assist LLMs in reasoning. This approach enhances the comprehensibility of structured information for LLMs, augmenting the input with rich semantic, syntactic, and logical features. By preserving the essence of structured representations in a more accessible format, SR-NLD facilitates a more nuanced understanding of text, reduce ambiguity, and promotes structured thinking. The training-dependent setting involves supervised fine-tuning (SFT) of LLMs on task-specific SR datasets (termed Gen-SR) to establish connections between structured information and tasks. During the process of SFT, the LLM is exposed to structured data repeatedly, enabling it to develop a more robust internal representation of the underlying structures. This deep integration allows the model to leverage structured knowledge during inference, improving its performance across a wide range of NLP tasks.

Our empirical evaluation encompasses a comprehensive suite of NLP benchmarks, spanning diverse linguistic phenomena from paraphrase detection~\citep{mihalcea2006corpus, dolan2005automatically} and textual entailment recognition~\citep{dagan2005pascal,bowman2015large} to machine translation ~\citep{bahdanau2014neural, johnson2017google}. This diverse benchmark selection enables rigorous evaluation of our methods across the NLP spectrum. Experimental results demonstrate the superiority of our methods over existing approaches: on PAWS, while conventional method exhibits a 5.18\% performance degradation, our training-free and training-dependent approaches achieve +3.17\% and +12.38\% improvements respectively, which validating the efficacy of our structured information integration paradigm.

% We conducted extensive experiments across a diverse array of NLP tasks, which cover a wide spectrum of language processing challenges. These tasks include paraphrase detection ~\citep{mihalcea2006corpus, dolan2005automatically}, recognizing textual entailment ~\citep{dagan2005pascal,bowman2015large}, machine translation ~\citep{bahdanau2014neural, johnson2017google}, etc. This comprehensive selection ensures a thorough assessment of our approach across various aspects of natural language understanding and generation. The results of our experiments demonstrate a clear contrast between existing methods and our proposed approaches. Taking PAWS as example, while previous techniques led to a 5.18\% decline in average performance, our training-free method showed a positive shift, improving performance by 3.17\%. Even more impressive, our training-dependent approach exhibited a substantial 12.38\% boost. These outcomes provide compelling evidence that LLMs can significant enhance their capability when presented with structured information in an accessible and appropriate manner.

Our contributions are as follows:
\begin{itemize}
    \item We introduce SR-LLM, a novel framework that facilitates SR integration with LLMs through dual paradigms: training-free adaptation and supervised fine-tuning.
    \item We provide insights into how different types of SR (AMR, PST, FOL) impact LLMs performance across various tasks.
    \item To the best of our knowledge, we are the first to show that combining such SR does in fact improve LLM performance, which opens up new avenues for enhanced LLM reasoning and interoperability.
\end{itemize}
