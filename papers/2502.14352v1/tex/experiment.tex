\section{Experiments}
\label{experiment}

\subsection{Datasets}
To ensure comprehensive and diverse experiments, we selected 10 datasets covering various NLP tasks based on~\citet{liu2024datasets}, including five tasks from~\citet{jin2024analyzing} for result comparability. The dataset composition includes: PAWS for paraphrase detection~\citep{zhang2019paws}, SNLI for textual entailment recognition~\citep{bowman2015large}, WMT16 for translation tasks~\citep{bojar2016findings}, CoNLL2003 for named entity recognition~\citep{sang2003introduction}, Logic for logical fallacy detection~\citep{jin2022logical}, SST-2 for sentiment analysis~\citep{socher2013recursive}, Pubmed45 for event extraction~\citep{garg2016extracting}, WiC for word sense disambiguation~\citep{pilehvar2018wic}, SPIDER for Text2SQL code generation~\citep{yu2018spider}, and AGNEWS for text classification~\citep{zhang2015character}. \\
Regarding the source of SR datasets, we used a dual-source strategy: one part includes high-quality AMR datasets from Jin~\cite{jin2024analyzing}, covering five core tasks; the other is automatically constructed using GPT-4o, comprising supplementary AMR, PST, and FOL data. The detailed collection processes and results provided in the Appendix~\ref{app:dataset_tf}.

\subsection{Training-Free Results}

\setlength{\tabcolsep}{5.5pt}
\renewcommand{\arraystretch}{1.2}
\begin{table*}
\vspace{-0.2cm}
\caption{\textbf{Performance of SR-LLM(training-free). } In the table, a checkmark under ``SR'' indicates that the original SR was added to the prompt, while a checkmark under ``SR-NLD'' (highlighted with a gray background) represents the inclusion of SR-NLD in the prompt, which corresponds to the results of SR-LLM (training-free). No checkmarks indicate the use of the original prompt, serving as the control group for comparison. Our focus is on the performance differences between adding SR and SR-NLD, as well as their respective differences compared to the control group.
}
\vspace{-0.1cm}
\label{tab:tfperfor}
\footnotesize
\begin{tabular}{c|c|cccccccccc}
\toprule
SR & \begin{tabular}[c]{@{}c@{}}SR-NLD\\ (Ours)\end{tabular} & \begin{tabular}[c]{@{}c@{}}PAWS\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Logic\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pubmed45\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}AGNEWS\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}WiC\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SNLI\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}CoNLL\\2003\\(F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SST-2\\ (F1)\end{tabular} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}WMT16\\ (BLEU)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}SPIDER\\ (F1)\end{tabular}} \\
\midrule
\multicolumn{12}{c}{(a) Llama3.1-
8b-Instruct} \\
\multicolumn{1}{l}{} &  & 41.59 & 15.48 & 24.35 & 53.88 & 43.99 & 25.81 & 46.28 & 68.72 & 13.16 & 24.80 \\
\checkmark &  & 36.41 & 14.20 & 20.69 & 48.17 & 42.05 & 23.17 & 41.75 & 65.66 & 12.34 & 21.53 \\
 \rowcolor{gray!15} 
 & \checkmark & \textbf{44.77} & \textbf{18.27} & \textbf{26.10} & \textbf{56.67} & \textbf{48.17} & \textbf{28.87} & \textbf{48.73} & \textbf{71.77} & \textbf{14.10} & \textbf{29.60} \\
\midrule
\multicolumn{12}{c}{(b) GPT 3.5-turbo} \\
 &  & 56.94 & 38.63 & 27.14 & \textbf{85.12} & 50.61 & 38.93 & \textbf{56.52} & 90.46 & 26.13 & 39.63 \\
\checkmark &  & 56.10 & 36.27 & 25.63 & 81.33 & 51.60 & 32.00 & 54.67 & 86.90 & 25.77 & 39.07 \\
\rowcolor{gray!15} 
 & \checkmark & \textbf{57.97} & \textbf{39.40} & \textbf{28.17} & 84.07 & \textbf{55.27} & \textbf{41.47} & 55.17 & \textbf{92.60} & \textbf{27.07} & \textbf{42.27} \\
\midrule
\multicolumn{12}{c}{(c) GPT 4o-mini} \\
 &  & 75.80 & \textbf{48.10} & \textbf{38.65} & \textbf{85.26} & \textbf{58.47} & 40.59 & \textbf{65.27} & 91.39 & \textbf{26.80} & 41.55 \\
\checkmark &  & 73.50 & 47.32 & 33.11 & 81.62 & 46.65 & 41.30 & 59.21 & 91.01 & 26.21 & 39.33 \\
\rowcolor{gray!15} 
 & \checkmark & \textbf{76.48} & 47.95 & 36.66 & 83.45 & 56.63 & \textbf{42.00} & 64.12 & \textbf{92.83} & 26.76 & \textbf{43.57}\\
\bottomrule
\end{tabular}
\vspace{-0.3cm}
\end{table*}

\paragraph{Experimental Details.}
We conducted experiments on the Llama3.1-8b-Instruct~\cite{dubey2024llama}, GPT-3.5-turbo, and GPT-4o-mini~\cite{achiam2023gpt} models, arranged from weak to strong according to their performance levels, employing two prompting strategies: Chain-of-Thought (CoT)~\cite{wei2022chain} and One-Shot~\cite{brown2020language}. CoT guides step-by-step reasoning, while One-Shot demonstrates task-solving through specific examples. All experiments were conducted independently on three types of SRs: AMR, FOL, and PST. Both PST and FOL were incorporated into the prompts using the same approach as AMRCOT~\cite{jin2024analyzing}. For brevity, the results obtained from these experiments were averaged and presented. For detailed prompts, refer to the Appendix~\ref{app:prompt}.

\paragraph{Result Analysis.} First, as shown in Table~\ref{tab:tfperfor}, incorporating SR-NLD into the prompt consistently outperforms incorporating the original SR. This indicates that for LLMs, transforming abstract SRs into natural language formats more familiar to the models is an effective strategy for enhancing their ability to interpret and apply structured information. Meanwhile, the comparision of the three models also reveals that the gradual decrease in the benefit of structured information as model performance increases. Specifically, for the Llama3.1-8b-Instruct model, results with SR-NLD significantly and consistently surpass those of the original prompt (i.e., without SR or SR-NLD). For GPT-3.5-turbo, most results show improvement, whereas for GPT-4o-mini, approximately half of the results demonstrate improvement, albeit with a smaller margin. This result further illustrates that weaker models benefit more from structured information as a supplement to the original text, aiding them in downstream reasoning tasks. In contrast, for stronger models, the additional structured information offers limited advantages and may even be less informative than the insights derived directly from the raw text. 


\subsection{Training-Dependent Results}
\paragraph{Experimental Details}
We conducted experiments using the Llama3.1-8B-Instruct model to evaluate the performance of the training-dependent setting of SR-LLM, more detailed experimental parameters can be found in the Appendix~\ref{app:ftd}. The whole process of fine-tuning is a joint training across data from 10 tasks, rather than task-specific fine-tuning for any single dataset. Detailed data collection procedures and specific training data configurations are provided in the Appendix~\ref{app:dataset_td}. 
To provide a comparative analysis, we conducted three sets of experiments using the following datasets: 100\%G (text), 100\%G (SR), and a 50\%G (text) mixed with 50\% G (SR). The 50\%-50\% ratio was chosen because we considered it to be the most balanced approach. Further experiments, elaborated in Appendix~\ref{app:ratio_select}, also confirmed that this is the optimal mixing ratio. And we employed a random sampling approach. All experiments were conducted independently on three types of SRs and for brevity, the results obtained from these experiments were averaged and presented.

% We conducted experiments using the Llama3.1-8B-Instruct model to evaluate the performance of the training-dependent setting of SR-LLM. Specifically, we used Meta's Llama-3.1-8B-Instruct as the backbone and conducted fine-tuning on 8 NVIDIA A100-80G GPUs. Optimization was performed using the AdamW optimizer with a learning rate of 1e-4 and cosine learning rate decay. The training setup included a per\_device\_train\_batch\_size of 16 and gradient\_accumulation\_steps of 8, yielding an effective global batch size of 1024. A fixed random seed of 42 ensured reproducibility. The whole process of fine-tuning is a joint training across data from 10 tasks, rather than task-specific fine-tuning for any single dataset. Each dataset was fine-tuned for 10 epochs, with early stopping to prevent overfitting. Detailed data collection procedures and specific training data configurations are provided in the Appendix~\ref{app:dataset_td}. To ensure a more comprehensive comparison, we also performed two sets of comparative experiments: one where all instruction pairs are solely G(text), and the other where all instruction pairs are G(SR).
% To provide a comparative analysis, we conducted three sets of experiments using the following datasets: 100\%G (text), 100\%G (SR), and a 50\%G (text) mixed with 50\% G (SR). The 50\%-50\% ratio was chosen because we considered it to be the most balanced approach. Further experiments, elaborated in Appendix~\ref{app:ratio_select}, also confirmed that this is the optimal mixing ratio. All experiments were conducted independently on three types of SRs: AMR, FOL, and PST. Both PST and FOL were incorporated into the prompts using the same approach as AMRCOT~\cite{jin2024analyzing}. For brevity, the results obtained from these experiments were averaged and presented.

\setlength{\tabcolsep}{4.5pt}
\renewcommand{\arraystretch}{1.2}
\begin{table*}
% \caption{\textbf{Performance of SR-LLM(training-dependent).} In the table, a checkmark under "SR" indicates that the original SR was added to the prompt, while the absence of a checkmark represents the use of the original prompt, serving as the control group for comparison. Our focus is on the best performance of the model across various tasks under different fine-tuning strategies, as well as the performance differences between adding SR and the control group.
% }
\caption{\textbf{Performance of SR-LLM(training-dependent).} G(text) and G(SR) represent the types of training data, with 50\% and 10\% indicating their respective proportions in the total training dataset. Our focus is on the best performance of the model across various tasks under different fine-tuning strategies, as well as the performance differences between adding SR and the control group.
}
\vspace{-0.2cm}
\label{tab:tdperfor}
\footnotesize
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{tabular}{c|c|cccccccccc}
\toprule
FT Strategy & SR & \begin{tabular}[c]{@{}c@{}}PAWS\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Logic\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pubmed45\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}AGNEWS\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}WiC\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SNLI\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}CoNLL\\2003\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SST-2\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMT16\\ (BLEU)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SPIDER\\ (EM)\end{tabular} \\
 \midrule
\multirow{2}{*}{-} &  & 41.59 & 15.48 & 24.35 & 53.88 & 43.99 & 25.81 & 46.28 & 68.72 & 13.16 & 24.80 \\
 & \checkmark & 36.41 & 14.20 & 20.69 & 48.17 & 42.05 & 23.17 & 41.75 & 65.66 & 12.34 & 21.53 \\
  \midrule
\multirow{2}{*}{100\% G(text)} &  & 68.94 & 26.21 & 78.91 & 76.52 & 66.97 & 35.53 & 75.79 & 75.59 & 29.07 & 41.20 \\
 & \checkmark & 64.07 & 16.84 & 77.33 & 67.14 & 67.05 & 35.36 & 71.73 & 74.65 & 28.41 & 38.47 \\
  \midrule
\multirow{2}{*}{100\% G(SR)} &  & 65.34 & 25.23 & 81.13 & 75.10 & 66.44 & 36.68 & 75.40 & 77.49 & 26.93 & 37.07 \\
 & \checkmark & 75.39 & 29.89 & \textbf{82.02} & 81.99 & 70.82 & \textbf{56.62} & 76.27 & 81.62 & \textbf{30.80} & 40.60 \\
  \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}50\% G(SR) \\ + 50\% G(text)\end{tabular}} &  & 68.66 & 26.77 & 79.78 & 75.77 & 69.48 & 36.49 & 75.42 & 77.13 & 26.14 & 42.40 \\
 & \checkmark & \textbf{81.04} & \textbf{36.52} & 81.85 & \textbf{82.63} & \textbf{74.68} & 54.92 & \textbf{76.67} & \textbf{83.72} & 30.33 & \textbf{48.93}\\
 \bottomrule
\end{tabular}
\vspace{-0.3cm}
\end{table*}

\paragraph{Result Analysis.}
As shown in the Table~\ref{tab:tdperfor}, when the fine-tuning dataset includes a certain proportion of SRs and incorporates SRs in the prompt, the model achieves superior performance in downstream tasks, consistently surpassing the case where the training data consists solely of text. Additionally, we observe that models fine-tuned with SRs data perform significantly better with prompts that include SRs, compared to the original prompts without SR. Conversely, when the training data consists entirely of text, the opposite trend is observed.
These findings suggest that when a model establishes a strong association between tasks and structured representations during training, it can leverage this information more effectively during inference. Furthermore, when the training data is entirely composed of structured representations, the performance is inferior to that achieved with a balanced mix of text and structured data. This highlights the critical importance of a balanced integration of raw text and structured representations in maximizing the model’s reasoning capabilities.


\subsection{Auxiliary Validation Experiments}

\paragraph{SR from High-Quality SR-Parsing Model.}
To validate the reliability of the generated SRs, we choose AMRBART~\cite{bai2022graph} to generate the required AMRs, and experiments were conducted to compare the results with those generated by GPT-4o. It is a model that demonstrates exceptional performance in the AMR parsing domain with a Smatch score of 85.4 on the AMR Parsing Leaderboard, ranking among the top-performing models.  As shown in the Table~\ref{tab:amrbart_per}, the performance differences between AMRs and AMR-NLDs derived from these two sources were minimal, almost always within 0.5\%. This indicates that the quality of the AMRs produced by AMRBART is comparable to those generated by our method. 
% This outcome further validates the reliability of our generation process and substantiates the robustness of our proposed approach.

\setlength{\tabcolsep}{2.8pt}
\renewcommand{\arraystretch}{1.2}
\begin{table}
\caption{\textbf{Performance between different AMR Source. }Each data represents the performance difference of the model when using AMRs generated by GPT-4o versus AMRBART, calculated as the performance of AMRBART minus that of GPT-4o. As shown, the differences are almost all below 1\%.
}
\vspace{-0.1cm}
\label{tab:amrbart_per}
\footnotesize
\begin{tabular}{c|c|ccccc}
\toprule
AMR & \begin{tabular}[c]{@{}c@{}}AMR\\ (NLD)\end{tabular} & \begin{tabular}[c]{@{}c@{}}PAWS\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Logic\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pubmed\\45\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMT\\16\\ (BLEU)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SPIDER\\ (EM)\end{tabular} \\
\midrule
\multicolumn{7}{c}{(a) Llama3.1-8b-Instruct} \\
\checkmark &  & 0.40 & -0.07 & 0.01 & 0.13 & 0.28 \\
 & \checkmark & 0.77 & -0.13 & 0.50 & -0.02 & -0.01 \\
 \midrule
\multicolumn{7}{c}{(b) GPT 3.5-turbo} \\
\checkmark &  & 0.45 & 0.57 & -0.15 & 0.08 & 0.12 \\
 & \checkmark & 0.02 & -2.40 & 0.52 & 0.23 & 0.21 \\
 \midrule
\multicolumn{7}{c}{(c) GPT 4o-mini} \\
\checkmark &  & 0.08 & 0.07 & 0.53 & 0.49 & 0.02 \\
 & \checkmark & -0.11 & 0.61 & 0.61 & -0.13 & -0.13\\
 \bottomrule
\end{tabular}
\vspace{-0.6cm}
\end{table}

\setlength{\tabcolsep}{4.5pt}
\renewcommand{\arraystretch}{1.2}
\begin{table*}
\caption{\textbf{Performance between different AMR Quality}. The numbers in parentheses represent the performance differences between adding AMR or AMR-NLD and the control group. `Flawed' means the AMR is ambiguous or structurally flawed. `Gold' means the AMR is double checked by human and LLM.}
\vspace{-0.1cm}
\label{tab:goldamr}
\footnotesize
\begin{tabular}{c|c|c|ccccc}
\toprule
AMR Quality & AMR & AMR-NLD & PAWS (F1) & Logic (F1) & Pubmed45 (F1) & WMT16 (BLEU) & SPIDER (EM) \\
\midrule
 & \multicolumn{7}{c}{(a) Llama3.1-8b-Instruct} \\
- & - & - & 42.19 & 14.32 & 23.67 & 13.66 & 22.58 \\
Flawed & \checkmark &  & 34.5 (-7.69) & 11.52 (-2.8) & 19.41 (-4.26) & 11.07 (-2.59) & 18.26 (-4.32) \\
Gold & \checkmark &  & 42.48 (+0.29) & 14.7 (+0.38) & 23.43 (-0.24) & 14.65 (+0.99) & 22.93 (+0.35) \\
Flawed &  & \checkmark & 32.91 (-9.29) & 11.56 (-2.76) & 18.39 (-5.28) & 11.06 (-2.6) & 18.49 (-4.09) \\
Gold &  & \checkmark & \textbf{46.96 (+4.76)} & \textbf{18.98 (+4.66)} & \textbf{28.62 (+4.95)} & \textbf{19.13 (+5.47)} & \textbf{28.02 (+5.44)} \\
\midrule
 & \multicolumn{7}{c}{(b) GPT 3.5-turbo} \\
- & - & - & 56.04 & 43.79 & 28.29 & 26.01 & 40.28 \\
Flawed & \checkmark &  & 51.57 (-4.47) & 41.58 (-2.21) & 25.71 (-2.58) & 23.79 (-2.22) & 36.66 (-3.62) \\
Gold & \checkmark &  & 54.53 (-1.51) & 44.7 (+0.91) & 29.47 (+1.19) & 26.17 (+0.15) & 39.77 (-0.51) \\
Flawed &  & \checkmark & 51.33 (-4.71) & 39.79 (-4.01) & 26.9 (-1.38) & 24.37 (-1.64) & 36.74 (-3.54) \\
Gold &  & \checkmark & \textbf{56.78 (+0.74)} & \textbf{46.49 (+2.7)} & \textbf{32.0 (+3.71)} & \textbf{28.72 (+2.71)} & \textbf{44.81 (+4.53)} \\
\midrule
 & \multicolumn{7}{c}{(c) GPT 4o-mini} \\
- & - & - & 68.71 & 44.95 & 37.07 & 29.02 & 40.05 \\
Flawed & \checkmark &  & 65.63 (-3.08) & 42.74 (-2.2) & 35.42 (-1.66) & 27.31 (-1.71) & 37.84 (-2.21) \\
Gold & \checkmark &  & 70.04 (+1.33) & 45.9 (+0.96) & 35.36 (-1.71) & 29.62 (+0.6) & 41.47 (+1.42) \\
Flawed &  & \checkmark & 62.63 (-6.07) & 41.46 (-3.49) & 34.51 (-2.56) & 26.64 (-2.38) & 37.30 (-2.76) \\
Gold &  & \checkmark & \textbf{70.13 (+1.42)} & \textbf{46.18 (+1.23)} & \textbf{39.17 (+2.09)} & \textbf{30.14 (+1.12)} & \textbf{41.54 (+1.48)}\\
\bottomrule
\end{tabular}
\vspace{-0.3cm}
\end{table*}

\paragraph{Gold AMR vs Flawed AMR.}
Additionally, we selected 70 AMR samples (labeled as ``Flawed'') with ambiguities or structural flaws from each of the 10 datasets and refined them using a dual-process correction strategy that combined AMRBART-generated results with manual adjustments, producing high-quality AMRs (labeled ``Gold''). Results in Table~\ref{tab:goldamr} show that AMR quality significantly impacts model performance. Using flawed AMRs led to performance declines for both direct AMR and AMR-NLD representations, with a more pronounced drop for AMR-NLD. This indirectly validates AMR-NLD’s ability to enhance LLMs’ understanding of AMR structures. In contrast, with high-quality AMRs, AMR-NLD substantially improved model performance, while direct AMR usage showed limited gains. These results demonstrate that combining high-quality AMR-NLD is more effective in helping models comprehend structured information. This effect is particularly pronounced when the quality of the AMR is high, leading to substantial performance gains.

\paragraph{Fine-tuning Larger Model.}
To validate the robustness of the proposed method, we selected Llama3.1-70B-Instruct and conducted training-dependent experiments, whose details were consistent with those described for the Llama3.1-8B-Instruct model above, in five tasks shown in the Table~\ref{tab:tdper70}. The SR used in these experiments was AMR, with a 50\%-50\% ratio. We can see that, after fine-tuning, the model demonstrated improvements on all tasks, with corresponding values turning positive, more than half of which exceeded 5\%. These results further validate the effectiveness of Training-Dependent method on larger-scale models.

\setlength{\tabcolsep}{3.5pt}
\renewcommand{\arraystretch}{1.2}
\begin{table}
\caption{\textbf{Performance of SR-LLM(training-dependent) in Llama3.1-70b-Instruct.} The numbers in parentheses represent the performance differences between adding SR and the control group. Our focus is on the performance variations across different models with different prompts.
}
% \caption{\textbf{Performance of SR-LLM(training-dependent) in Llama3.1-70b-Instruct.} In the table, a checkmark under "AMR" indicates that the original AMR was added to the prompt, while the absence of a checkmark represents the use of the original prompt, serving as the control group for comparison. The numbers in parentheses represent the performance differences between adding SR and the control group. Our focus is on the performance variations across different models with different prompts.
% }
\vspace{-0.1cm}

\label{tab:tdper70}
\footnotesize
\begin{tabular}{c|ccccc}
\toprule
AMR & \begin{tabular}[c]{@{}c@{}}PAWS\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Logic\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pubmed45\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMT16\\ (BLEU)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SPIDER\\ (EM)\end{tabular} \\
\midrule
\multicolumn{6}{c}{(a) Vanilla} \\
 & 68.00 & 47.13 & 63.95 & 28.65 & 33.71 \\
\checkmark & \begin{tabular}[c]{@{}c@{}}60.28\\ (-7.73)\end{tabular} & \begin{tabular}[c]{@{}c@{}}43.08\\ (-4.04)\end{tabular} & \begin{tabular}[c]{@{}c@{}}48.82\\ (-15.13)\end{tabular} & \begin{tabular}[c]{@{}c@{}}27.91\\ (-0.73)\end{tabular} & \begin{tabular}[c]{@{}c@{}}29.20\\ (-4.51)\end{tabular} \\
\midrule
\multicolumn{6}{c}{(b) 50\% G(AMR) + 50\% G(text) } \\
 & 74.74 & 54.57 & 76.51 & 33.73 & 47.06 \\
 \rowcolor{gray!15}
\checkmark & \begin{tabular}[c]{@{}c@{}}\textbf{84.56}\\ (\textbf{+9.81})\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{58.96}\\ (\textbf{+4.39})\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{81.54}\\ (\textbf{+5.03})\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{37.00}\\ (\textbf{+3.27})\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{53.84}\\ (\textbf{+6.78})\end{tabular}\\
\bottomrule
\end{tabular}
\vspace{-0.5cm}
\end{table}

\paragraph{Experiment on Traceable LLM.}
Since NLP datasets have been public for years, their role in modern LLM development is unclear. We validate our approach through experiments on OLMo~\cite{OLMo}, where the training and validation data sources are clearly documented, in five tasks shown in the Table~\ref{tab:olmo}. The results show that when the prompt includes SR, its performance is lower than when SR is not included. However, when SR is transformed into SR-NLD, the performance improves significantly. For instance, in the PAWS, the performance increases from 61.52\% to 65.40\%. This demonstrates the robustness and generalizability of our approach.

\setlength{\tabcolsep}{2.6pt}
\renewcommand{\arraystretch}{1.2}

\begin{table}
\caption{\textbf{Performance in OLMo.} 
}

\vspace{-0.2cm}

\label{tab:olmo}
\footnotesize
\begin{tabular}{ccccccc}
\toprule
SR & \begin{tabular}[c]{@{}c@{}}SR\_NLD \\ (Ours)\end{tabular} & \begin{tabular}[c]{@{}c@{}}PAWS\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Logic\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pubmed45\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMT16\\ (BLEU)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SPIDER\\ (EM)\end{tabular} \\
\midrule
 &  & 61.52 & 22.96 & 60.27 & 9.67 & 20.44 \\
\checkmark &  & 57.73 & 17.97 & 57.89 & 11.23 & 19.79 \\
 \rowcolor{gray!15}
 & \checkmark & \textbf{65.40} & \textbf{24.89} & \textbf{65.19} & \textbf{12.44} & \textbf{22.20}\\
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\end{table}