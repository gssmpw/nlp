% This chapter introduces the SR-LLM framework,  a novel paradigm designed to investigate the efficacious integration of SR into LLMs. The primary objective is to augment LLM performance across a diverse spectrum of NLP tasks. The SR-LLM framework encompasses two configurations: training-free and training-dependent. These configurations are designed to amalgamate various types of SR through differentiated methodologies, thereby enhancing the LLMs' capability to comprehend and exploit structured information. The ultimate goal is to maximize the structured information improvement, quantified by $\Delta$.


\section{Method}

This chapter introduces the SR-LLM framework,  a novel paradigm designed to investigate the efficacious integration of SR into LLMs. The SR-LLM framework encompasses two configurations: training-free and training-dependent. These configurations are designed to amalgamate various types of SR through differentiated methodologies, thereby enhancing the LLMs' capability to comprehend and exploit structured information. 



\subsection{SR-LLM Training-Free}
% \KY{what is NLD?}
\begin{figure}[h!]
\centering
\vspace{-0.2cm}
\includegraphics[width=1\linewidth]{picture/base_sr_cot.pdf}
\caption{\textbf{Base prompt and AMRCOT prompt.} \textbf{(Top)} This is the original task prompt, with only the raw text as input, serving as the standards for performance. \textbf{(Bottom)} This is the AMRCOT prompt method proposed by \citet{jin2024analyzing}, serving as a baseline.}
\label{fig:base_sr_cot}
\vspace{-0.2cm}
\end{figure}


Prior approaches, exemplified by AMRCOT \cite{jin2024analyzing}, have attempted to explicitly incorporate AMR into Chain-of-Thought (COT) prompts, as illustrated in Figure~\ref{fig:base_sr_cot}, have shown that this explicit approach fails to yield performance enhancement. We hypothesize that one factor contributing to this ineffectiveness stems from the inherent difficulty LLMs face in adequately comprehending and processing abstract structures such as AMR. In view of the aforementioned challenge, as illustrated in Figure~\ref{fig:srllmtf}, we propose SR-LLM Training-Free, where the original structured representation $Z$ is transformed into natural language descriptions termed \textbf{SR-NLD}, where SR can be instantiated with specific structured representations such as AMR, PST, and FOL. We refer to this entire transformation process as \textbf{SR-to-NLD}(\textbf{S}tructured \textbf{R}epresentation \textbf{to} \textbf{N}atural \textbf{L}anguage \textbf{D}escription). Specifically, the structured representations are mapped through predefined transformation rules, converting abstract symbols into easily interpretable natural language expressions. These generated natural language descriptions are then refined by a language model to ensure fluency and coherence. Finally, these descriptions are incorporated into the prompt and input into the target LLM. A pivotal advantage of this methodology lies in its training-free nature, as it does not require any additional fine-tuning or retraining of the LLM. Consequently, this technique offers remarkable flexibility, enabling rapid adaption to a diverse array of NLP tasks. 

\begin{algorithm}
\normalsize
    \caption{AMR-to-NLD Transformation}
    \label{algorithm_amr_nld}
    \begin{algorithmic}[1]
        \State \textbf{Input:} AMR graph $G = (V, E)$, nodes collection $V$, edges collection $E$, Penman library $\mathcal{P}$, language model $\theta$
        \State \textbf{Output:} Refined natural language descriptions $S_{\text{refined}}$
        
        \State \emph{\textbf{Phase 0}: Convert AMR to Triplets}
            \State Convert AMR graph $G$ into triplets $T = \{(c_1, r, c_2) \mid c_1, c_2 \in V, r \in E\}$ using the Penman library: $T=\mathcal{P}(G)$
        
        \State \emph{\textbf{Phase 1}: Identifier Instantiation}
            \For{each triplet $(c_1, r, c_2) \in T$}
                \If{$r = \texttt{:instance}$}
                    \State Replace identifiers $c_1, c_2$ with their corresponding concepts or instances
                \EndIf
            \EndFor

        \State \emph{\textbf{Phase 2}: Mapping to Natural Language}
            % \State Apply the mapping function $M: T' \rightarrow S$ to convert triplets into natural language descriptions using a predefined dictionary
            \State Convert triplets into natural language descriptions using a predefined dictionary: $M: T' \rightarrow S$
        
        \State \emph{\textbf{Phase 3}: Refinement}
            \State Refine the generated descriptions $S$ using language model: $S_{\text{refined}} = \theta (S)$
        
        \State \Return $S_{\text{refined}}$
    \end{algorithmic}
\end{algorithm}

Next, we shall elucidate the SR-to-NLD process, employing AMR-NLD as our quintessential exemplar, which shown in the Algorithm~\ref{algorithm_amr_nld}. The process first converts the AMR graph into triplets, then replaces the identifiers with actual concepts. Next, the triplets are mapped into natural language descriptions using predefined rules, and finally, the descriptions are refined by GPT-4o Mini to produce coherent AMR-NLD. To mitigate the risk of hallucination, we implemented a voting mechanism based on multiple generations. This detailed analysis forms the core of our discussion, outlining each step of the conversion process. The transformation methods for other SRs are elaborated in the Appendix~\ref{app:Detail_sr2nld} for completeness. Different from traditional SR-to-Text approaches, which generate a structurally coherent and fluent text based on the SR, such as the ``input sentence'' in Figure~\ref{fig:base_sr_cot}. SR-to-NLD aims to collaboratively describe the structured information through multiple sentences, as illustrated by the Refined AMR-NLD in Figure~\ref{fig:base_sr_cot}.

% \paragraph{Definition of AMR.}
% AMR is represented as a directed graph \( G = (V, E) \), where \( V \) represents concepts (e.g., entities, events) and \( E \) represents relations (e.g., predicate-argument structures). Figure~\ref{fig:srllmtf} illustrates an example of an AMR structure.

% \paragraph{Conversion of AMR to Triplets Using the Penman Library.}
% We use the Penman library to convert the AMR graph \( G \) into a set of triplets \( T \), defined as:
% \begin{equation}
%     T = \{ (c_1, r, c_2) \mid c_1, c_2 \in V, r \in E \}
% \end{equation}
% % \[
% % T = \{ (c_1, r, c_2) \mid c_1, c_2 \in V, r \in E \},
% % \]
% where \( c_1 \) and \( c_2 \) are concepts, and \( r \) is the relation between them. An example is shown in Figure~\ref{fig:srllmtf}.

% \paragraph{Instantiation of Identifiers.}
% Identifiers linked to the \( :instance \) relation in the triplets are replaced with their corresponding concepts or instances. For example, as illustrates in Figure~\ref{fig:srllmtf}, identifiers such as \( s \), \( p \), and \( n \) are substituted with the actual concepts.

% \paragraph{Mapping Triplets to Natural Language Descriptions.}
% Each triplet undergoes a transformation into natural language description through the application of a mapping function, denoted as \( M: T' \rightarrow S \), where \( S \) is a set of natural language strings. This process uses a predefined dictionary to translate entities (e.g., ``person'', ``dog'') and relations (e.g., \( :name \)) into natural language sentences (see Appendix for details).

% \paragraph{Refinement Using a Language Model.}
% The generated descriptions are refined using a language model \( F_{\text{LM}}: S \rightarrow S_{\text{refined}} \), ensuring the output exhibits naturalness and coherence. The final refined AMR-NLD is shown in Figure~\ref{fig:srllmtf}.


    % \begin{equation}
    %     S_{\text{refined}} = \{\text{One person saw something.}, \text{His name is John.}, \text{The object he saw was a dog.}\}
    % 
    % \end{equation}

\subsection{SR-LLM Training-Dependent}
% \JH{remains how to build the Gen SR dataset, I will draw a figure to illustrate it}\\

In addition to making SRs more interpretable for LLMs, we also believe that establishing connections between tasks and structured information presents a potential opportunity. As shown in the Figure~\ref{fig:srllmtd}, in SR-LLM Training-Dependent, we constructed a task-specific hybrid dataset, named Gen-SR, where SR can be replaced by specific representations such as AMR, PST, and FOL. 

\begin{figure}
\centering
\vspace{0in}
\includegraphics[width=1\linewidth]{picture/SR_LLM_TD.pdf}
% \vspace{-0.2in}
\caption{\textbf{The whole process of SR-LLM in training-dependent setting}. Taking AMR as an example, a dataset called \textbf{Gen-AMR}, created by combining inputs consisting of sentences and their corresponding AMR structures, is utilized for the SFT of LLM to enhance the reasoning capability.}
\label{fig:srllmtd}
\vspace{-0.1in}
\end{figure}

The entire hybrid dataset is composed of two parts: one consists of task-specific instruction pairs based on original text, while the other adds SRs in the instruction pairs based on the former. The former we mark as G(text) and the other we mark as G(SR). The complete example of these two are shown in the Appendix~\ref{app:prompt_gensr}. This mixed approach allows LLM to not only learn instruction-following for downstream tasks from G(text), but also to establish more robust connections between tasks and structures from G(SR), making the model achieve more effective improvements compared to learning solely from text.

% We propose a training data construction method that leverages a mixture of structured data and raw text data, aiming to enhance the LLM's capability to comprehend abstract structured representations during the Fine-Tuning stage. 

% Prior to defining this mixed training dataset, we introduce SRCOT, an extension of AMRCOT \citep{jin2024analyzing}, which explicitly incorporates AMR into Chain-of-Thought (COT) prompts. In SRCOT, AMR is replaced by other types of structured representations within the prompt. Figure~\ref{fig:base_sr_cot} illustrates the prompt structure of both SRCOT and the baseline configuration.

% The GenSR training dataset can be represented as follows:

% % \[
% \begin{equation}
%     D = \alpha \cdot D_{\text{text}} + \beta \cdot D_{\text{SR}},
% \end{equation}
% % \]

% where
%     \(D\) denotes the GenSR dataset,
%     \(D_{\text{text}}\) represents the dataset with all the data whose input is sentence, shown in Figure~\ref{fig:srllmtd},
%     \(D_{\text{SR}}\) represents the dataset with all the data whose input is structured representations, shown in Figure~\ref{fig:srllmtd},
%     \(SR\) denotes the specific type of SR utilized in the mixed dataset,
%     and \(\alpha\) and \(\beta\) are the weighting factors of the two data components, satisfying \(\alpha + \beta = 1\).
% By adjusting the values of \(\alpha\) and \(\beta\), we can explore the effects of different data composition strategies on the performance of the SFT model, with the objective of investigating the granular impact of structured representations on model performance.



