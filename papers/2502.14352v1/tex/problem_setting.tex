\section{Problem Definition}

% This study is focused on exploring the applications and potential benefits of SRs within large language models (LLMs). We will rigorously define the components of the research problem from both linguistic and computational linguistic perspectives.

% \subsection{Defining Natural Language Representations and Tasks}
% In linguistic theory, natural language is regarded as a complex symbolic system that encompasses multiple dimensions, including lexicon, syntax, and pragmatics. Consider a natural language input sequence \( X = (x_1, x_2, \dots, x_n) \), where \( x_i \in V \) denotes tokens from the vocabulary. From a linguistic standpoint, \( X \) not only contains superficial word order information but also encapsulates deeper syntactic structures, semantic relationships, and pragmatic intentions.

% Natural language processing tasks aim to map this intricate language input to a specific output space. We define the output sequence \( Y = (y_1, y_2, \dots, y_m) \), where \( y_i \) represents elements of the target vocabulary or answer space. \( Y \) may consist of a classification label, a segment of generated text, or a structured semantic representation.

% We define a model \( f: X \rightarrow Y \) such that:

% This research endeavors to investigate the potential synergies between SR and LLMs, with the ultimate goal of ascertaining how their seamless integration can augment the efficacy and proficiency of LLMs in a wide array of NLP tasks. 
% Given a natural language input sequence represented as \( X = (x_1, x_2, \dots, x_n) \), where \( x_i \in V \) denotes a token drown from the vocabulary \(V\). The primary objective is to establish a mapping from this intricate input to a corresponding output sequence \( Y = (y_1, y_2, \dots, y_m) \), where each \( y_i \) represents elements drown from either the target vocabulary or a structured semantic output space. This mapping is executed by the model \( f \) according to the following formulation:
%     \begin{equation}
%         Y = f(X)
%     \end{equation}
% This sophisticated mapping process encompasses a hierarchical cascade of linguistic processing layers, incorporating lexical analysis, syntactic parsing, semantic interpretation, and pragmatic inference, each contributing to the multifaceted transformation of input to output. 

% Complementing the natural language input, we introduce the concept of structured representation \(Z\). This representation can manifest in various forms, including but not limited to AMR, PST, or FOL, each capturing distinct facets of semantic, syntactic, or logical information respectively. To facilitate this transformation, we define a function \(g\) that maps the natural language input \(X\) into its corresponding structured representation \(Z\):
%     \begin{equation}
%         Z = g(X)
%     \end{equation}

% The objective of this investigation is to identify an optimal model \( f^* \) that can effectively utilize both the natural language input and its corresponding structured representations to maximize \( P(\cdot) \), the pperformance evaluation metric such as accuracy or F1 score:
%     \begin{align}
%       f^* &= \arg\max_f P(f(X, Z))
%     \end{align}

This research endeavors to investigate the potential synergies between SR and LLMs, with the ultimate goal of ascertaining how their seamless integration can augment the efficacy and proficiency of LLMs in a wide array of NLP tasks. 

Given a natural language input sequence \( X = (x_1, x_2, \ldots, x_n) \), where \( x_i \in V \) represents a token drawn from the vocabulary \( V \), we also introduce the structured representation \( Z \). \( Z \) serves as auxiliary information derived from \( X \) and can take various forms, such as AMR, PST, or FOL. These SRs capture semantic, syntactic, or logical information and provide complementary insights to natural language understanding.

The task involves generating an output sequence \( Y = (y_1, y_2, \ldots, y_m) \), where each \( y_i \) belongs to either the target vocabulary or a structured semantic output space. This transformation is performed by a model \( f \), defined as:
    \begin{equation}
        Y = f(X, Z)
    \end{equation}
Here, \( f \) specifies how \( X \) and \( Z \) are utilized to complete a specific task by integrating natural language input with its structured representation.

The primary goal of this research is to optimize the definition of \( f \) to achieve the most effective use of \( X \) and \( Z \), thereby maximizing task performance. Specifically, the objective is to identify the optimal model \( f^* \) that maximizes the evaluation metric \( P(\cdot) \), such as accuracy or F1 score:
    \begin{equation}
f^* = \underset{f}{\text{arg max}} \, P(f(X, Z))
    \end{equation}
\begin{figure*}[t!]
\centering
\vspace{-0.2in}
\includegraphics[width=1\linewidth]{picture/SR_LLM_TF.pdf}
\vspace{-0.3cm}
\caption{\textbf{The whole process of SR-LLM in training-free setting}. Initially, a task-specific prompt consists of an instruction, input sentence, and input SR structure (AMR is used here). Subsequently, the original AMR undergoes transformation via the \textbf{AMR-to-NLD} module, which employs predefined rules to map the AMR into an easily interpretable natural language description. This description is then subjected to refinement by a language model, ensuring fluency and coherence, resulting in \textbf{AMR-NLD}. Finally, the \textbf{AMR-NLD} is seamlessly integrated into the input, which is then fed into the LLM to generate the ultimate response.}

\label{fig:srllmtf}
\vspace{-0.05in}
\end{figure*}


% \subsection{Defining Structured Representations}
% To better capture the multifaceted characteristics of natural language, we introduce the concept of structured representation \( Z \). Let \( g: X \rightarrow Z \) be the function that converts natural language into SR, defined as:
% In addition to the natural language input, we introduce the concept of structured representation \( Z \), which can take forms such as AMR, PST, or FOL, representing different levels of semantic, syntactic, or logical information. A function \( g \) is defined to convert the natural language input \( X \) into structured representation \( Z \):





    
% where \( Z \) may take the forms including \textbf{AMR}, \textbf{PST}, \textbf{FOL}. Each representation encodes structured linguistic information in a unique manner, reflecting different levels of abstraction in linguistic theory.
% where \( Z \) may take the following forms:
% \begin{enumerate}
%     \item \textbf{AMR}: Captures the conceptual semantic structure of sentences, abstracting predicate-argument relationships.
%     \item \textbf{PST}: Reflects the syntactic structure of sentences, including phrase structures and dependency relations.
%     \item \textbf{FOL}: Transforms natural language into formal logical expressions to facilitate reasoning.
% \end{enumerate}



% \subsection{Definition and Quantification of the Enhancement of Structured Representation}
% We propose the concept from ``SR Enhancement'' to quantify the additional advantages introduced by SRs. Assuming the output derived from the natural language input \( X \) is \( Y_{\text{plain}} = f(X) \), the output obtained from the combined input of natural language and structured representation \( (X, Z) \) is represented as \( f(X, Z) \). The enhancement function \( \Delta \) is defined as follows:
% To quantify the benefits conferred by the incorporation of SR, we introduce an ``SR enhancement'' function, denoted as \( \Delta \), which measures the performance differential between two scenarios: one utilizing solely the natural language input \(X\), and another leveraging both the natural language input and its corresponding structured representation \(X, Z\). The enhancement function is formally defined as follows:
%     \begin{equation}
%         \Delta={ P(f(X, Z)) - P(f(X))}
%     \end{equation}
    
% \noindent where \( P(\cdot) \) denotes a performance evaluation metric such as accuracy or F1 score. The objective of this investigation is to identify an optimal model \( f^* \) that can effectively utilize both the natural language input and its corresponding structured representations to maximize the performance gain \( \Delta \):
%     \begin{align}
%       f^* &= \arg\max_f \Delta
%     \end{align}

