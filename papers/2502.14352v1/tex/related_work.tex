\section{Related Work}
\label{gen_inst}

\paragraph{Structure Representations.}
The SRs, including AMR, PST, and FOL, each unique advantages and applications in specific areas. AMR uses rooted, labeled graphs to abstract syntactic details, offering concise and semantically rich representations~\citep{banarescu2013abstract}. PST, based on Chomsky's generative grammar, employs hierarchical trees to represent sentence syntax and word dependencies~\citep{chomsky2014aspects}. FOL, as a symbolic logic system, defines objects, their relations, and properties, serving as a key tool in formal logic and reasoning~\citep{enderton2001mathematical, barwise1977introduction}.
% The SRs, including AMR, PST, and FOL, each have their unique advantages and applications in semantic abstraction, syntactic parsing, and logical reasoning, respectively. AMR is rooted, labeled graph structures designed to abstract away from syntactic nuances, providing concise and semantically rich representations ~\citep{banarescu2013abstract}. PST is a hierarchical tree structure grounded in Chomsky's theory of generative grammar, commonly used to represent the syntactic organization of sentences, emphasizing the dependency between words~\citep{chomsky2014aspects}. FOL is a symbolic logic system that articulate objects, their relations, and properties, making them essential tools in formal logic and logical reasoning~\citep{enderton2001mathematical, barwise1977introduction}.

\paragraph{Structure Representations Transformation.} 
% SR transformation has long been a critical area of research in the study of structured representations. Many works have focused on SR-to-Text approaches, which generate fluent text that aligns with the structure of the SR~\cite{song2018graph, ribeiro2021structural, wang2020amr}. At the same time, there exists a method known as canonical expressions, employs a rule-based methodology to transform structures into standardized natural language representations, primarily aimed at resolving ambiguities in non-standard sentences~\cite{shin2021constrained, roy2024benchclamp}. They focuses on creating simplified natural language representations to reduce ambiguity and facilitate machine parsing and their output is essentially a process of normalization rather than a comprehensive description of the full structure of SR. In contrast, our SR-to-NLD approach preserves the integrity of structured information while enhancing its interpretability through natural language descriptions of the structure, thereby better supporting the reasoning process of large language models.
The SR transformation has long been a critical area of research. Much of the existing work has focused on SR-to-Text approaches, which generate fluent text that aligns with the structure of the SR~\cite{song2018graph, ribeiro2021structural, wang2020amr}. Meanwhile, a method known as canonical expressions employs rule-based techniques to convert structures into standardized natural language representations, primarily to resolve ambiguities in non-standard sentences~\cite{shin2021constrained, roy2024benchclamp}. Its outputs are essentially normalized texts rather than comprehensive descriptions of the SR’s full structure. In contrast, our SR-to-NLD approach preserves the integrity of structured information while enhancing its interpretability through natural language descriptions of the structure.


\paragraph{Structured Representations used for NLP in LLM.} With the rise of LLM, studies like \citet{hahn2022formal} showed these sequence to sequence model's ability to generalize across formal domains, though challenges like low interpretability and hallucinations persist~\citet{de2023structuring}. Integrating structured representations into LLMs has improved accuracy and interpretability. \citet{yao2024semantic} and \cite{shi2024compressing} combined AMR with LLMs for tasks like sentence simplification and Retrieval-Augmented Generation. Additionally, \citet{hahn2022formal} and \cite{kalyanpur2024llm} advanced formal specification and logical reasoning in LLMs. And \citet{an2024rethinking} identified "magic prompts" that improve the performance of NLP tasks by solely focusing on semantic parsing, without the need to provide the actual parsing results. However, \citet{jin2024analyzing} argued that simplely add AMR into prompt might sometimes hinder performance in certain NLP tasks. 

% \paragraph{Structured Representations used for NLP in Pre-LLM} Before the advent of LLMs, AMR’s graph-based structures were essential in NLP tasks like paraphrase detection and machine translation due to their ability to encode deep semantics and integrate external knowledge \citep{tohidi2022short}. For example, \cite{lim2020know} improved reasoning by combining AMR with ConceptNet, while \cite{zhang2021fine} and \cite{xu2022two} enhanced information extraction by integrating AMR with external sources. PSTs have been widely used in linguistic analysis, aiding tasks like semantic role labeling and improving predictions through parse tree analysis \citep{gildea2002necessity}. \cite{collins2003head} and \cite{chiang2005hierarchical} improved parsing and translation accuracy using PSTs. FOLs are key for formal reasoning, providing a structured framework. \cite{bursztyn2015efficient} improved query efficiency through FOL reformulations, and \cite{lalwani2024nl2fol} enhanced logical fallacy detection by converting natural language into FOL formulas. Before the rise of large language models (LLMs), natural language mapping to structured representations relied on rule-based methods, which faced semantic limitations\citep{thompson1969rel,wang2018first}. 