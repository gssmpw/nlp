\clearpage

\appendix
\section{Experimental Details}
\subsection{Details of Converting SR to SR-NLD}
\label{app:Detail_sr2nld}
% \subsection{AMR-to-NLD}
% \subsubsection{Details of Translating triplet to natural sentence.} 
\subsubsection{Details of Translating AMR Triplet to Natural Sentence} 

According to the Figure~\ref{fig:detail_amr_nld}, first, the triplet is converted into a sentence based on the relation mapping rules. Then, using the entity dictionary, the entities are replaced with their actual meanings to form the final sentence. Finally, the sentence is input into the LLM for refinement into a complete and coherent sentence, as shown in the Figure~\ref{fig:amrnld_prompt}.


\begin{figure}[ht]
\centering
\vspace{0in}
\includegraphics[width=1\linewidth]{picture/detail_amr_nld.pdf}
% \vspace{-0.2in}
\caption{The process of translate entities and relationships into natural language sentences}
\label{fig:detail_amr_nld}
\vspace{-0.1in}
\end{figure}

\begin{figure}[ht]
\centering
\vspace{0in}
\includegraphics[width=0.8\linewidth]{picture/prompt_for_amr_nld.png}
% \vspace{-0.2in}
\caption{The prompt of polishing sentence for making AMR-NLD}
\label{fig:amrnld_prompt}
\vspace{-0.1in}
\end{figure}

% \subsection{PST-to-NLD}
\subsubsection{Whole Process of Making PST-NLD}
\label{app:make_pst_nld}
\begin{figure}[!ht]
\centering
\vspace{0.1in}
\includegraphics[width=1\linewidth]{picture/pst2nld.pdf}
\vspace{0.1in}
\caption{\textbf{The Whole process of Making PST-NLD.} The process of creating PST-NLD involves first converting the PST tree structure into a linear sequence of symbols using depth-first search (DFS). Then, a mapping function is applied to translate each node and its children into natural language descriptions. Finally, a language model is used to refine the generated descriptions, making them more natural and coherent.}
\label{fig:pst2nld}
\vspace{0.1in}
\end{figure}

\paragraph{Definition of PST.}
% \begin{enumerate}
    % \item \textbf{Definition of PST} \\
    PST is represented as a tree structure \( T = (N, E) \). Here \( N \) denotes the set of nodes, representing the syntactic components of a sentence (e.g., part-of-speech tags and phrase labels). Node types include \( S \) (sentence), \( NP \) (noun phrase), \( VP \) (verb phrase), etc.  \( E \) denotes the set of edges, representing dependencies between components.
    An example of the original PST structure is shown in the Figure~\ref{fig:pst2nld}.

\paragraph{Conversion of PST to a Linear Structure Using Depth-First Search (DFS).}
    % \item \textbf{Conversion of PST to a Linear Structure Using Depth-First Search (DFS)} \\
    Starting from the root node (typically \( n_0 \), representing the sentence's syntactic structure, such as \( S \)), we traverse the tree in a depth-first search (DFS) manner, converting it into a linear sequence of symbols \( P \).

\paragraph{Mapping PST Identifiers to Natural Language Descriptions.}

    We define a mapping function \( M \) to translate each identifier (e.g., \( S \), \( NP \), \( VBD \)) and its child nodes into natural language descriptions. The dictionary \( D \), which specifies the natural language interpretation of each identifier, is detailed in the appendix. For each triplet \( (n, c_1, c_2) \), where \( n \) is a node and \( c_1 \), \( c_2 \) are its children, we apply the mapping function \( M(n) = \text{description}(n) \). The resulting natural language description \( S \) is as shown in the Figure~\ref{fig:pst2nld}.
    
\paragraph{Refinement of Natural Language Descriptions Using a Language Model.}

    To make the descriptions more natural and coherent, the generated descriptions \( S \) are refined using the language model \( F_{\text{LM}}: S \rightarrow S_{\text{refined}} \). The specific prompt is shown in the prompt (b) of Figure~\ref{fig:pstnld_prompt}. 
  
\begin{figure}[ht]
\centering
\vspace{0in}
\includegraphics[width=0.8\linewidth]{picture/prompt_for_pst_nld.png}
% \vspace{-0.2in}
\caption{The prompt of polishing sentence for making PST-NLD}
\label{fig:pstnld_prompt}
\vspace{-0.1in}
\end{figure}

\begin{figure}
\centering
\vspace{0.1in}
\includegraphics[width=1\linewidth]{picture/fol2nld.pdf}
\vspace{0.1in}
\caption{\textbf{The Whole process of Making FOL-NLD.} The process of converting FOL to NLD involves first mapping FOL symbols, such as variables, predicates, and logical operators, into natural language using predefined symbol mappings and logical rules. Then, the generated descriptions are refined using a language model to ensure they are coherent and fluent.}
\label{fig:fol2nld}
\vspace{0.1in}
\end{figure}

% \subsection{FOL-to-NLD}
\subsubsection{Whole Process of Making FOL-NLD}
\label{app:make_fol_nld}
\paragraph{Definition of FOL.}
% \begin{enumerate}
    % \item \textbf{Definition of FOL} \\
    FOL is represented as \( F = (Q, V, P, C) \), where 
    % \begin{itemize}
         \( Q \) denotes the set of quantifiers, used to express the existence of variables, such as \( \exists \) (exists) and \( \forall \) (for all).
         \( V \) represents the set of variables, representing objects in FOL, typically denoted as \( x, y, z \).
         \( P \) represents the set of predicates, used to express properties of objects or relationships between multiple objects.
         \( C \) represents the set of logical connectives, used to connect multiple propositions, including conjunction (\( \land \)), disjunction (\( \lor \)), and negation (\( \neg \)).
    % \end{itemize}
    An example of the original FOL structure is shown in the Figure~\ref{fig:fol2nld}.

\paragraph{Mapping FOL to Natural Language Descriptions.}

We define a mapping function \( M = (D, L) \), where \( D \) is a set of symbol mappings that translates variables, predicates, and logical operators in FOL into natural language descriptions. \( L \) is a set of logical mapping rules that transforms the logical structure of FOL into natural language syntax. By applying these mapping rules to the initial FOL expressions, we can convert logical symbols into natural language descriptions.
    
\paragraph{Refinement of Natural Language Descriptions Using a Language Model.} To ensure that the descriptions are coherent and fluent, we refine the generated descriptions \( S \) using the language model \( F_{\text{LM}}: S \rightarrow S_{\text{refined}} \). The specific prompt is shown in the prompt (c) of Figure~\ref{fig:folnld_prompt}. 


\begin{figure}[ht]
\centering
\vspace{0in}
\includegraphics[width=0.8\linewidth]{picture/prompt_for_fol_nld.png}
% \vspace{-0.2in}
\caption{The prompt of polishing sentence for making FOL-NLD}
\label{fig:folnld_prompt}
\vspace{-0.1in}
\end{figure}

\subsection{Complete Fine-tuning Details}
\label{app:ftd}
We used Meta's Llama-3.1-8B-Instruct as the backbone and conducted fine-tuning on 8 NVIDIA A100-80G GPUs. Optimization was performed using the AdamW optimizer with a learning rate of 1e-4 and cosine learning rate decay. The training setup included a per\_device\_train\_batch\_size of 16 and gradient\_accumulation\_steps of 8, yielding an effective global batch size of 1024. A fixed random seed of 42 ensured reproducibility. Each dataset was fine-tuned for 10 epochs, with early stopping to prevent overfitting.

\section{Data Collection}
\subsection{The Process of Constructing Datasets for All Tasks of SR-LLM~(training-free)}
\label{app:dataset_tf}
In this section, I will outline the process of collecting test data for the 10 tasks used in SR-LLM (training-free), including both the original text and three types of structured representations. The data statistics are summarized in the Table~\ref{tab:dataset_of_tf}.

\setlength{\tabcolsep}{4.5pt}
\renewcommand{\arraystretch}{1.2}
\begin{table}[!ht]
\centering
\vspace{-0.1in}
\caption{Tasks and datasets used in SR-LLM (training-free)}
\label{tab:dataset_of_tf}
\vspace{-0.1in}
\small
\resizebox{0.5\textwidth}{!}
{ 
\begin{tabular}{lcc}
\toprule
\textbf{Dataset} & \textbf{Task} & \textbf{Test Size} \\
\midrule
PAWS      & Paraphrase Detection          & 8000  \\
SNLI      & Recognizing Textual Entailment & 10000 \\
WMT16     & Translation                   & 5999  \\
CoNLL2003 & Named Entity Recognition       & 3453  \\
LOGIC      & Logical Fallacy Detection     & 2449  \\
SST-2     & Sentiment Analysis            & 872   \\
Pubmed45  & Event Extraction              & 5000  \\
WiC       & Lexical Disambiguation        & 2038  \\
SPIDER    & Text2SQL Code Generation      & 8034  \\
AGNEWS    & Text Classification           & 7600  \\
\bottomrule
\end{tabular}
} 
\vspace{-0.1in}
\end{table}

\paragraph{SNLI}
SNLI is a large and comprehensive dataset, with a test set containing 10,000 examples. Therefore, we directly used the test set for our experiments. The AMR, FOL, and PST data were generated using GPT-4o-turbo in a few-shots setting, with the prompt provided in the Figure~\ref{fig:makr_amr}, Figure~\ref{fig:makr_pst} and Figure~\ref{fig:makr_fol}.

\begin{figure}[ht]
\centering
\vspace{0in}
\includegraphics[width=0.8\linewidth]{picture/make_amr.png}
% \vspace{-0.2in}
\caption{The prompt of making AMR}
\label{fig:makr_amr}
\vspace{-0.1in}
\end{figure}

\begin{figure}[ht]
\centering
\vspace{0in}
\includegraphics[width=0.8\linewidth]{picture/make_pst.png}
% \vspace{-0.2in}
\caption{The prompt of making PST}
\label{fig:makr_pst}
\vspace{-0.1in}
\end{figure}

\begin{figure}[ht]
\centering
\vspace{0in}
\includegraphics[width=0.8\linewidth]{picture/make_fol.png}
% \vspace{-0.2in}
\caption{The prompt of making FOL}
\label{fig:makr_fol}
\vspace{-0.1in}
\end{figure}

\paragraph{CoNLL2003}
CoNLL2003 is also a rich and complete dataset, with a test set of 3,453 examples, which we used directly. Structured representations were generated using the same method as described above.

\paragraph{SST-2}
Since the official SST-2 test set does not contain labels, we used the full validation set of 872 examples as the test set for this experiment. Structured representations were generated using the same method as described above.

\paragraph{WiC}
The WiC test set consists of 1,400 examples, which is relatively small. Therefore, we combined the 648 examples from the validation set to create a larger test set. Structured representations were generated using the same method as described above.

\paragraph{AGNEWS}
AGNEWS is another large and comprehensive dataset, with a test set of 7,600 examples, which we used directly. Structured representations were generated using the same method as described above.

\paragraph{PAWS}
To ensure sufficient comparability in the experiments, the original text data and AMR representations for PAWS were sourced from \citet{jin2024analyzing}. And the FOL and PST representations were generated using the same method as described above.

\paragraph{WMT16, LOGIC, Pubmed45, SPIDER}
The data collection for these tasks followed the same procedure as PAWS.

\subsection{The Process of Constructing Datasets for All Tasks of SR-LLM~(training-dependent)}
\label{app:dataset_td}
In this section, I will explain the process of collecting both training and test data for the 10 tasks used in SR-LLM (training-dependent), including the original text and three types of structured representations. Data statistics are summarized in the Table~\ref{tab:dataset_of_td}.


\setlength{\tabcolsep}{4.5pt}
\renewcommand{\arraystretch}{1.2}
\begin{table}[!ht]
\centering
\caption{Tasks and datasets used in SR-LLM (training-dependent)}
\label{tab:dataset_of_td}
\small
\resizebox{\linewidth}{!}
{ 
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Task} & \textbf{Train Size} & \textbf{Test Size} \\
\midrule
PAWS      & Paraphrase Detection           & 10000  & 8000  \\
SNLI      & Recognizing Textual Entailment & 10000  & 10000 \\
WMT16     & Translation                    & 10000  & 5999  \\
CoNLL2003 & Named Entity Recognition       & 10000  & 3453  \\
LOGIC     & Logical Fallacy Detection      & 10000  & 2449  \\
SST-2     & Sentiment Analysis             & 10000  & 872   \\
Pubmed45  & Event Extraction               & 10000  & 5000  \\
WiC       & Lexical Disambiguation         & 5066   & 1048  \\
SPIDER    & Text2SQL Code Generation       & 7000   & 1034  \\
AGNEWS    & Text Classification            & 10000  & 7600  \\
\bottomrule
\end{tabular}
} 
\end{table}


\paragraph{PAWS, WMT16, Pubmed45, SNLI, CoNLL2003, SST-2, AGNEWS}
These datasets contain relatively large training sets. Therefore, we randomly selected 10,000 examples from each as the training set. The structured representations were generated using GPT-4o-turbo in a few-shot setting, with sample prompts provided in the figure. The test sets are the same as those used in the SR-LLM (training-free) experiments.

\paragraph{LOGIC}
Since the LOGIC dataset is relatively small, the training-free setup used all the available samples from the test, validation, and training sets combined, yielding a total of 2,449 samples as the test set. We retained these 2,449 samples for the test set in the training-dependent setting as well. To create the training set, we synthetically generated 10,000 logic examples using GPT-4o-turbo. The generation process is illustrated in the Figure~\ref{fig:process_of_making_logic}, where a few-shot strategy was employed to guide the model to generate sentences containing different logical fallacies. The generated prompt is shown in Figures~\ref{fig:makr_lfs_a} and Figures~\ref{fig:makr_lfs_b}. The type of logical error serves as the label, producing complete data points. Structured representations were generated in the same manner as described above.

\begin{figure}[ht]
\centering
\vspace{0in}
\includegraphics[width=1\linewidth]{picture/process_of_making_logic.pdf}
% \vspace{-0.2in}
\caption{\textbf{The synthetic process for LOGIC data.} Taking the ``Faulty Generalization'' type as an example, we employed a few-shot strategy to guide the model in generating sentences containing the logical fallacy of ``Faulty Generalization'' To ensure greater sentence diversity, we incorporated a thematic element during generation, such as ``Sports'' as shown in the figure. This thematic addition helps produce a broader variety of sentence while maintaining the specific logical error, leading to a richer and more varied dataset.}
\label{fig:process_of_making_logic}
\vspace{-0.1in}
\end{figure}

\begin{figure}[ht]
\centering
\vspace{0in}
\includegraphics[width=0.8\linewidth]{picture/logic_make.png}
% \vspace{-0.2in}
\caption{Logic Fallacy Generate Prompt (a)}
\label{fig:makr_lfs_a}
\vspace{-0.1in}
\end{figure}

\begin{figure}[ht]
\centering
\vspace{0in}
\includegraphics[width=0.8\linewidth]{picture/logic_make2.png}
% \vspace{-0.2in}
\caption{Logic Fallacy Generate Prompt (b)}
\label{fig:makr_lfs_b}
\vspace{-0.1in}
\end{figure}

\paragraph{SPIDER}
Since the official SPIDER test set is not publicly available, the training-free setup used a combination of training and validation sets as the test set. However, due to the complexity of generating SPIDER-like data, we used the original 7,000 training examples for the training set in the training-dependent setting and the 1,034 validation examples as the test set. Structured representations were generated as described above.

\paragraph{WiC}
As the WiC training set is relatively small, we combined the 648 validation examples with the original training set to create a total of 5,066 training samples. Structured representations were generated using the same method as described above.







\section{Additional Experiments}
\subsection{Comparative Analysis of Different SR Combinations and Their Impact on LLM Reasoning}
We conducted an in-depth comparison of the performance of different structured representations (SR) and explored their combinations to assess whether joint usage could further enhance LLM reasoning capabilities. Figure \ref{fig:performance_of_different_sr} summarizes the average performance improvements across all tasks. The results indicate that the use of individual SRs such as AMR, PST, and FOL did not lead to significant performance enhancements, which is consistent with the findings of \cite{jin2024analyzing}. Moreover, when multiple SRs were introduced simultaneously, their combined complexity posed additional challenges for the LLMs, further dispersing the modelâ€™s attention and resulting in poorer performance compared to using a single SR. In contrast, when relatively weaker LLMs were provided with more comprehensible semantic features (AMR) and logical features (FOL), their average performance improved. The integration of these two types of features complemented each other, leading to better overall results. However, the contribution of syntactic features (PST) was relatively less effective and, in some cases, even negated the positive effects of semantic and logical features.

\begin{figure}[!ht]
\centering
\vspace{0in}
\includegraphics[width=1\linewidth]{picture/performance_of_different_sr.png}
% \vspace{-0.2in}
\caption{\textbf{Performance comparison of different SR combinations.} (a) The average performance enhancement $(\Delta)$, for various SR combinations across different tasks. (b) The average performance enhancement $(\Delta)$, for different SR-NLD combinations across various tasks.}
\label{fig:performance_of_different_sr}
\end{figure}


\subsection{Optimal Text-to-SR Ratio Analysis}
\label{app:ratio_select}
To further investigate the most optimal ratio of between G(text) and G(SR), I selected five tasks, which includes PAWS, LOGIC, Pubmed45, SPIDER, WMT16 for additional experiments, adjusting the ratio of text to structured representations in the Gen-SR dataset to identify the optimal balance. The experimental results are shown in the Figure~\ref{fig:dif_weight}. As can be observed, the fluctuations in performance with different ratios are relatively small. For both AMR and PST, a 50-50 ratio between text and structured representations appears to be the most effective. However, for FOL, a 30-70 ratio (whether favoring structured representations) yields better results. This is a preliminary exploration, and I believe it represents a promising direction for further research.

\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{picture/dif_weight.png}
% \vspace{-0.2in}
\caption{Comparison of average performance of models at different scales in all tasks.}
\label{fig:dif_weight}
\vspace{-0.1in}
\end{figure}

\subsection{Enhancing LLM's Understanding of SR during Pretraining.}
We further conducted experiments during the pretraining phase with the goal of enhancing LLM's ability to comprehend structured representations, aiming for performance improvements in downstream tasks. Specifically, we collected 1GB of task-agnostic SR data, including AMR, PST, and FOL, following a similar procedure as in previous data collection efforts, and applied this data to the pre-training of the Llama3.1-8B-Instruct model. Building on this, we further conducted SFT, the same as SR-LLM (training-dependent), on five datasets. The final average performance results are shown in the Table~\ref{tab:model_results}.

The experimental results show that, compared to the vanilla model without pre-training, the pre-trained model indeed exhibited performance improvements in downstream tasks, though the improvements were relatively modest, with an average increase of less than 1\%. However, after applying SFT on the pre-trained model, its performance was actually inferior to that of the vanilla model trained directly with SFT. We hypothesize that this phenomenon may be due to the model forming certain inherent understandings of structured representations during the pre-training phase, which hindered its ability to establish effective connections between structure and tasks during SFT, leading to worse performance compared to the vanilla model. This phenomenon highlights a potential conflict in how the model processes structured information during the pre-training and fine-tuning phases, which warrants further exploration and resolution in future research.



\setlength{\tabcolsep}{4.5pt}
\renewcommand{\arraystretch}{1.2}
\begin{table}[!ht]
\centering
\vspace{0.1in}
\caption{\textbf{The SR enhancement of models with different training strategies.} These are the average SR Enhancement results across all tasks under different training strategies. Green indicates the best performance within the same SR, while red represents the worst performance.}
\label{tab:model_results}
\vspace{-0.1in}
\footnotesize
\resizebox{0.5\textwidth}{!}
{ 
\begin{tabular}{lcccc|c}
\toprule
AMR & FOL & PST & Pretrain & SFT & $\Delta$ \\
\midrule
\checkmark   &     &     &          &     & {\color[HTML]{FE0000} \textbf{-3.51\%}} \\
\checkmark   &     &     & \checkmark        &     & 0.56\%                                  \\
\checkmark   &     &     & \checkmark        & \checkmark   & -1.16\%                                 \\
\checkmark   &     &     &          & \checkmark   & {\color[HTML]{34FF34} \textbf{11.59\%}} \\
\midrule
    & \checkmark   &     &          &     & {\color[HTML]{FE0000} \textbf{-2.83\%}} \\
    & \checkmark   &     & \checkmark        &     & 1.30\%                                  \\
    & \checkmark   &     & \checkmark        & \checkmark   & 3.10\%                                  \\
    & \checkmark   &     &          & \checkmark   & {\color[HTML]{34FF34} \textbf{6.45\%}}  \\
\midrule
    &     & \checkmark   &          &     & {\color[HTML]{FE0000} \textbf{-3.61\%}} \\
    &     & \checkmark   & \checkmark        &     & -0.18\%                                 \\
    &     & \checkmark   & \checkmark        & \checkmark   & 1.58\%                                  \\
    &     & \checkmark   &          & \checkmark   & {\color[HTML]{34FF34} \textbf{2.91\%}} \\
\bottomrule
\end{tabular}
} 
\vspace{-0.1in}
\end{table}

\section{Examples of Gen-SR}
\label{app:prompt_gensr}
We present specific examples of Gen-SR in this section. Figure~\ref{fig:g_text} shows an example of G(text), Figure~\ref{fig:g_amr} shows an example of G(AMR), Figure~\ref{fig:g_pst} shows an example of G(PST), and Figure~\ref{fig:g_fol} shows an example of G(FOL).

\begin{figure}[!ht]
\centering
\vspace{0.1in}
\includegraphics[width=0.8
\linewidth]{picture/g_text.png}
\vspace{0.1in}
\caption{The Example of G(text)}
\label{fig:g_text}
\vspace{0.1in}
\end{figure}

\begin{figure}[!ht]
\centering
\vspace{0.1in}
\includegraphics[width=0.8
\linewidth]{picture/g_amr.png}
\vspace{0.1in}
\caption{The Example of G(AMR)}
\label{fig:g_amr}
\vspace{0.1in}
\end{figure}


\begin{figure}[!ht]
\centering
\vspace{0.1in}
\includegraphics[width=0.8
\linewidth]{picture/g_pst.png}
\vspace{0.1in}
\caption{The Example of G(PST)}
\label{fig:g_pst}
\vspace{0.1in}
\end{figure}

\begin{figure}[!ht]
\centering
\vspace{0.1in}
\includegraphics[width=0.8
\linewidth]{picture/g_fol.png}
\vspace{0.1in}
\caption{The Example of G(FOL)}
\label{fig:g_fol}
\vspace{0.1in}
\end{figure}

\section{Prompt of Testing the SR-LLM}
\label{app:prompt}
We present the complete prompts for our experiments, including both CoT and One-shot examples, using the SNLI dataset as an illustration in Figures~\ref{fig:cot_snli}, Figures~\ref{fig:oneshot_snli} and Figures~\ref{fig:oneshot_snli_e}.

\input{tex/ten_tasks_prompt}
