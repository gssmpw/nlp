\section{Method}
We first provide an overview of our method \textit{EPO}, accompanied by a formulation of strategic reasoning process. 
We then introduce a multi-turn RL pipeline to explicitly optimize the policy of the strategic reasoning model in \textit{EPO}.



\subsection{Overview}
%We introduce a multi-agent framework to disentangle and coordinate strategic reasoning and decision-making of LLMs for dynamic planning and long-term goal pursuit in interactive environments. Central to our approach is the integration of a strategic reasoning model, which guides the LLMs towards the successful completion of long-horizon goals through a sequence of strategic actions.
%In a typical context of strategic reasoning, an LLM agent $LLM_d$ seeks to achieve a designated goal $G$ through dynamic interactions with uncertain environments. The agent receives observations $x$ from other agents or the environment and formulates responses $y$. These exchanges result in an interaction history $h = (x, y)$, and the reasoning process can be modeled as $P(y|G, h, x)$ that produces responses to achieve the goal strategically.
%Our proposed framework, \textit{EPO}, reimagines this process by introducing an LLM-based strategic reasoning model $LLM_s$. As shown in Figure~\ref{fig:method}, this agent determines strategic actions $a$ to inform the responses of decision-making agents during inference. Particularly, the strategic reasoning model generates an action (or strategy) for each turn $i$ by analyzing the goal $G$, interaction history $h_{1:i-1}$, prior actions $a_{1:i-1}$, and the latest observation $x_i$:
%The decision-making agent $LLM_d$ then selectively uses the strategic insight $a_i$ to generate its response:
%where $s_{sys}$ and $d_{sys}$ are the corresponding prompts. This process is repeated until the goal is achieved or the maximum number of turns is reached. As $LLM_d$ operates in dynamic interactions, it also receives feedback from the environment or other agents, which continuously updates the interaction history and informs the strategic reasoning model. This feedback mechanism ensures that the strategy produced by $LLM_s$ remains flexible and responsive to changes in the environment or adversary behavior.
%In general, our \textit{EPO} enables LLMs to elicit real-time goal-oriented behavior by leveraging strategic insights from $LLM_s$ and adapting to dynamic environments. By integrating strategic reasoning, the responses of $LLM_d$ are ensured to be not only contextually aware but also strategically refined to achieve intended outcomes without compromising general decision-making capabilities in complex, real-world situations.


Assume an interactive scenario involving an LLM agent $LLM_d$ aiming to achieve a long-term goal $G$ through sequential interactions. 
At each turn $t$, $LLM_d$ receives an observation $x_t$ (e.g., adversary messages or environment states) and generates a response $y_t$ that balances immediate context with progress toward the goal $G$. 
Traditional approaches model this process as $P(y_t | G, h_{1:t-1}, x_t)$, where $h_{1:t-1}=\left\{x_1, y_1, ..., x_{t-1}, y_{t-1}\right\}$ is the interaction history between $LLM_d$ and an external environment or other agents. However, this formulation does not explicitly consider the strategic reasoning process in long-term goal alignment.

To address this, we propose \textit{EPO} that introduces an LLM $LLM_s$ dedicated to strategic reasoning and providing strategies $a$ to motivate goal-directed behavior from $LLM_d$.
As shown in Figure~\ref{fig:method}, $LLM_s$ synthesizes the goal $G$, interaction history $h_{1:t-1}$, prior strategies $a_{1:t-1}$, and the latest observation $x_t$ to propose a strategy in open-ended action space:
\begin{equation}
    a_t = LLM_s(s_{sys}, G, h_{1:t-1}, a_{1:t-1}, x_t).
\end{equation}
This strategy then encourages $LLM_d$ to produce goal-directed behavior:
\begin{equation}
    y_t = LLM_d(d_{sys}, G, h_{1:t-1}, a_{1:t}, x_t).
\end{equation}
Here, $s_{sys}$ and $d_{sys}$ are role-specific system prompts, which can be combined with various prompting techniques, such as CoT or Tree-of-Thought~\cite{yao2023tree}.
Crucially, $LLM_d$ selectively adopts $a_t$ when generating its behavior, allowing it to override suboptimal strategies while retaining domain-general linguistic skills. 
The external environment provides feedback afterward (e.g., adversary reactions or goal progress), updating $h_{t}$ for subsequent turns. 
With explicit policy optimization for strategic reasoning, \textit{EPO} enables continuous strategy refinement in $LLM_s$ while maintaing the generalization ability of $LLM_d$, addressing the over-optimization issues in prior work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\centering
\centerline{\includegraphics[scale=0.32]{Figure/method.png}}
\caption{\textbf{Overview of \textit{EPO}.}
The solid line shows the RL training process of the strategic reasoning model, while the dotted line indicates how our reasoning model motivates goal-directed behavior from LLM agents.
}
\label{fig:method}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Learning to Reason for Optimal Strategies}

To equip $LLM_s$ with adaptive strategic reasoning, we design a lightweight multi-turn RL pipeline that optimizes its policy through iterative self-play, prioritizing process rewards over terminal outcomes. 
This approach enables $LLM_s$ to learn to refine strategies based on real-time environmental feedback rather than align to predefined solutions or fixed reward models as in prior RLHF methods.
%This paradigm faciliates $LLM_s$ to navigate partial observability and delayed consequences, cultivating robust strategies that dynamically reconcile long-term goals with emergent challenges.

Specifically, we define the policy optimization of strategic reasoning model $LLM_s$ as a partially observable Markov decision process (POMDP), since the reasoning model can only access partial information of the environment state (e.g., without knowing the adversary goal).
The optimization objective is to maximize the expected return:
\begin{equation}
\label{eq:max_reward}
    J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[ R(\tau) ]
         = \mathbb{E}_{\pi_\theta}[\sum_{t=1}^{T} r_{t} ],
\end{equation}
where $r_{t}$ denotes the immediate reward at turn $t$, and $T$ is the terminal turn of the trajectory $\tau=(h_1,a_1,...,a_{T-1},h_T)$ generated by the policy $\pi_\theta(a_t|h_t)$.



\paragraph{\textbf{Policy Optimization.}} 
From the POMDP formulation, we derive an REINFORCE RL objective for training $LLM_s$:
\begin{small}
\begin{align}
\label{eq:optimize}
&\mathcal{L}(\theta) = 
\\
&-\mathbb{E}_{\pi_\theta} \left[\frac{1}{T} \sum_{t=1}^{T} A_t \frac{1}{|k_t|}\sum_{i=0}^{k_t} \log \pi_\theta(a_{t,i} |h_{1:t-1}, a_{t,1:i-1},x_t) \right],
\notag
\end{align}
\end{small}
where $a_{t,i}$ denotes the $i$-th token in strategy $a_t$, $a_{t,1:i-1}$ represents previously generated tokens within $a_t$, and $k_t$ is the total number of tokens of strategy $a_t$.
$A_t=R_t/max(R_{1:T})$ is the advantage of each strategy in a training sample, calculated by the maximum absolute normalization for $R_t=r_t + \sum_{k=1}^{T} \gamma^{k} r_{t+k}$, the cumulative discounted reward from turn $t$ to $T$.
$\gamma \in (0,1] $ is the discount factor that discounts future rewards over immediate reward.
This advantage implies how a strategy is superior to others within a trajectory in terms of aligning with the long-term goal, and also serves as a kind of baseline for variance reduction~\cite{Williams2004SimpleSG} to stabilize policy gradients during RL training.

Due to its simplicity and efficiency, this RL objective can integrate seamlessly with multi-task and cross-domain training (as demonstrated in our experiments) despite of on-policy sampling.
In addition, other RL algorithms such as PPO~\cite{schulman2017proximal} or GPRO~\cite{Shao2024DeepSeekMathPT} could also optimize $\pi_\theta$. 
Furthermore, the RL training can combine inference scaling techniques such as search algorithms by sampling serveral candidate strategies from $LLM_s$ and re-ranking them via a reward model or a learned value function.



\paragraph{\textbf{Process Rewards.}}  
For RL training, we assign each strategy $a_t$ produced by $LLM_s$ with a process (or immediate) reward $r_t$, reflecting its criticality in advancing $ LLM_d $ toward its goal. 
Specifically, $ r_t = 1 $ if $ a_t $ is deemed critical and 0 otherwise. 
To identify critical strategies, we employ an LLM\footnote{We use GPT-4o.} $LLM_p$ as the PRM to output a list containing key strategy indexes in a training sample:
\begin{equation}
    [S_{idx}] = LLM_p(p_{sys},G,h_{1:T},score,a_{1:T}),
\end{equation}
where $S_{idx}$ denotes the indexes of strategy $a_{1:T}$ critical in achieving the goal $G$. $p_{sys}$ refers to the system prompt for PRM (refer to Appendix~\ref{append:strategy-prompt}), and $h_{1:T}$ is the full interation history until the terminal turn $T$.
$score$ represents the final goal completion rate for corresponding sample.
By integrating process rewards into policy optimization, our method enables the strategic reasoning model to generate strategies that are both tactically sound in the short term and coherent over extended horizons.


\paragraph{\textbf{Iterative Self-Play.}}
To scale up RL, we train $LLM_s$ using iterative self-play, where two \textit{EPO} instances take turns interacting as different partners. During self-play, each \textit{EPO} instance uses its $LLM_s$ to devise strategies, encouraging their paired $LLM_d$ agents to behave toward their specific goals.
We record the entire trajectory data, including the interaction history between agents $LLM_d$ and the strategies proposed by $LLM_s$.
We then evaluate each strategy with the PRM and obtain process rewards to retrain $LLM_s$ in the next RL iteration based on Eq.~\ref{eq:optimize}, until $LLM_s$'s policy stabilizes.
Therefore, iterative self-play is an on-policy RL training process, while the weights of $LLM_d$ remain unchanged to ensure it can generalize to various domains without overfitting to particular behavior patterns.


\paragraph{\textbf{Transferability.}} 
The transferability of our method is mainly due to two factors: (1) the explicit policy optimization for strategic reasoning where our reaosning model $LLM_s$ can be flexibly plugged into any $LLM_d$, while $LLM_d$ maintains its general-domain capabilities without the need of additional training; (2) the open-ended action space where $LLM_s$ produces strategies. Therefore, the reasoning model can undergo RL training across different domains, improving its policy transferability to novel scenarios.

%Post-training $LLM_s$’s strategy-generation capability can be transferred to new $LLM_d$ agents or scenarios by simply swapping the system prompt $s_{sys}$, eliminating the need for retraining. This flexibility stems from \textit{EPO}’s decoupled architecture, where $LLM_s$ learns domain-agnostic reasoning heuristics rather than task-specific behaviors.





%A goal-oriented multi-turn interaction with LLMs can be formulated as a partially observable Markov decision process (POMDP) in \textcolor{red} {language space}. In this setup, the LLM agent engages with other agents or a physical environment over long horizon to accomplish a designated goal, without knowing other agents' goals or the environment dynamics. 
%We represent the POMDP as $(\mathcal{S,A,O,T,R})$ with state space $\mathcal{S}$, action space $\mathcal{A}$, observation space $\mathcal{O}$, transition function $\mathcal{T}$, and reward function $\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]$. Both $\mathcal{A}$ and $\mathcal{O}$ encompass open-ended natural language spaces.

%An initial state in this POMDP is a \textcolor{red}{ task instruction $x\in\mathcal{D}$ }from a dataset $\mathcal{D}$. We denote the output response from an LLM agent as action $a$. The action results in a transition within the latent state space $s_t \in \mathcal{S}$ and receives feedback from another agent or the environment, as an observation $o_t \in \mathcal{O}$ at turn $t$. Given the observation $o_t$, the next observation $o_{t+1}$ can be obtained by concatenating the action $a_t$ and \textcolor{red} {a strategy prompt $y_t$} generated by our strategic reasoning model with the token sequence of $o_t$. 
%Note that the strategy $y$ also belongs to the open-ended natural language space.
%The agent continues to produce actions $a_{t+1}$ based on $o_{t+1}$, until the task is completed or the interaction exceeds the maximum turns.

%The objective of the strategic reasoning model is to determine an optimal strategy $ \pi(y_t | h_t, x) $, where $ h_t=\{{\pi}^{sys}, y_i,o_1,...,y_{t-1},o_{t-1}\} $ is the history of past strategies, actions and observations up to turn $ t $, that maximizes the expected discounted sum of rewards towards a specific goal:
%\begin{equation}
%    \textcolor{red}{ \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t R(s_t, a_t) \right] }
%\end{equation}
%where $ \gamma \in [0, 1) $ is the discount factor, and $T$ is the number of turns of an interaction.

%To obtain a strategy $y_t$, the most straightforward approach is to query an off-the-shelf capable LLM given a \textcolor{red}{ task instruction $x$ } and an evolving interaction history $h_t$. To further improve its functionality to address dynamic and uncertain reasoning environment by adapting strategies over time, we \textcolor{red}{fine-tune} an LLM-powered strategic reasoning model in an offline setting.\textcolor{red}{The offline dataset can be constructed in two ways. One method involves prompting a competent LLM to generate the strategy $y_t$ prior to its action $a_t$ at turn $t$. Particularly in scenarios with multiple agents, we employ an LLM for self-play interactions. The LLM receives role descriptions and goals as it alternates between roles, and generates corresponding strategies and responses for each turn.}
%数据这一段放在下一章实验部分，这章重点讲方法 The interaction history can be denoted as $h_t=\left\{y_1^{i},a_1^{i},y_1^{-i},a_1^{-i},...,y_t^{i},a_t^{i},y_t^{-i},a_t^{-i}\right\}$, where $y_t^{i}$ and $a_t^{i}$ are the strategy and response produced by the role $i$, while $y_t^{-i}$ and $a_t^{-i}$ represent the strategies and responses of all other agents.
%a_t含义全文统一 \textcolor{red}{Refer to Appendix~\ref{append:prompt} for the prompt used in data construction and Appendix~\ref{append:dataset} for details of training data collection.} Moreover, we employ an iterative self-play data collection wherein the self-generated strategy data is utilized to retrain the strategic reasoning model. With the offline dataset $\mathcal{D}=\left\{\left\{x^{i}, h_t^{i}, y_t^{i}\right\}_{t=1}^{T}\right\}_{i=1}^{|\mathcal{D}|}$, the most direct optimization objective for the strategic reasoning model is to minimize the cross entropy loss between the predicted strategy $\hat{y}_t$ and the strategy $y_t$ from $\mathcal{D}$ for each turn $t$ via SFT:\begin{equation}
    % \mathcal{L} = -\frac{1}{|\mathcal{D}|} \sum_{d \in \mathcal{D}} \frac{1}{T} \sum_{t=1}^{T} \hat{y}_t \log y_t
%    \mathcal{L} = -\frac{1}{|\mathcal{D}|} \sum_{d \in \mathcal{D}} \frac{1}{T} \sum_{t=1}^{T} \hat{y}_t \log (y_t|h_{1:t-1},y_{1:t-1};\theta) \end{equation}
%Despite simplicity, SFT merely imitates the strategies in the dataset and is often insufficient for learning strategic behavior \textcolor{red}{ on self-generated data}. Instead, RL can enhance the model with adaptive decision-making capabilities and long-horizon planning skills to refine strategies and adapt to continuously changing environments or adversaries, ensuring agent's actions align with the strategic goals. In general, any offline RL approach can be used to train the strategic reasoning model. In our experiments, we adopt a reinforce-style optimization based on reward-weighted supervised learning~\cite{Peng2019AdvantageWeightedRS, ahmadian-etal-2024-back} due to ease of experimentation and its simplicity. Concretely, the RL objective for the strategic reasoning model is to maximize the cumulative return $R_t = \sum_{t'=t}^{T}\gamma^{T-t'}r_{t'}$, where $r_{t'}$ is the reward for the strategy $y_{t'}$ received from the environment or an LLM as shown in Figure~\ref{fig:method}(a). Then we update the strategy with the token-level loss: \begin{equation} \mathcal{L}(\theta) = -\mathbb{E}_{\pi_\theta} \left[ \sum_{t=1}^{T} R_t \sum_{i=0}^{k_t} \log \pi_\theta(y_{t,i} |o_{t-1}, y_{t,1:i-1}) \right] \end{equation}
%where $\pi_\theta$ denotes the strategic reasoning model parameterized by $\theta$, $y_{t,i}$ represents the $i$-th token output by the model at turn $t$ and $y_{t,1:i-1}$ the previous token sequence before $i$. $k_t$ is the length of the token sequence at turn $t$.
%\textcolor{red}{ In preliminary experiments, we train the model through RL built upon the SFT version and advance each RL iteration from the previous one when applying self-play iterative fine-tuning. However, performance degrades compared to retraining from a base model, possibly due to distribution shift issues.}



% \subsection{Theoretical Analysis}
% The vanilla policy gradient method~\cite{sutton1999policy} is formulated as follows:
% \begin{equation}
%        \mathcal{L}(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=1}^{T} (\sum_{t'=t}^{T}\gamma^{T-t'}r_{t'}) \log \pi_\theta(a_{t} | s_{t}) \right]
% \end{equation}
% which $\pi_\theta(a_{t} | s_{t})$ is the probability of
% taking action $a_t$ given the state $s_t$. 
