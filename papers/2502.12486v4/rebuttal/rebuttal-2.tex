\textbf{Q1:} The process evaluation metrics depend heavily on GPT-4o, introducing potential bias and reproducibility issues.

\textbf{A1:}

2. why we choose GPT-4o as the PRM.
3. how to address potential bias: a simple binary classification task; GPT-4o for evaluation, llama3-8B or GPT-turbo as the model for generating strategy data for process evaluation. but any model even human annotators may introduce bias during process evaluation.
4. how to address reproducibility: llama-3-70B as PRM
1. Without PRM, the RL is still better than EPO-SFT. show results.


\textbf{Q2:} Additional experiment based on llama3-8B for PPDPP.

\textbf{A2:}
1. why we maintain roberta-large for PPDPP. encoder for classification. predicted strategies: five action types
2. llama3-8B: decoder for text generation. predicted strategies: open-ended strategy in natural language form. require other modifications, such as ####. not a fair comparison. 
test qwen-0.5B (roberta-large:0.37B)