\textbf{Q1:} A more detailed comparison using policy optimization with SFT is needed to strengthen the argument that the multi-turn RL pipeline eliminates the need for SFT as a preliminary step.

\textbf{A1:}
SFT
EPO-SFT
EPO-SFT+RL w/PRM


\textbf{Q2:} The computation costs of training the reasoning model through RL.

\textbf{A2:}
4 A100 GPU, training time 40 mins, llama3-8B
similar to SFT
high computational efficiency and policy optimization. 




\textbf{Q3:} Failure case discussion of EPO.

\textbf{A3:}
select one failture case from sotopia (e.g., low goal completion)


\textbf{Q4:} Why is EPO-Llama3-8B w/RL better on unseen ALFWorld with Llama3-8B-Instruct than on seen task types?

\textbf{A4:} 
similar performance. (14.3 vs. 14.5). check 14.3 and 14.5. 用条数解释（对了多少）


We will add a societal impact section to discuss the safeguards of the strategic reasoning model.


