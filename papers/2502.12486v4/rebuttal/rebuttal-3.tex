\textbf{Q1-The choice of evaluation benchmark:} The value of using WebShop and ALFWorld as evaluation datasets and test on other strategic reasoning tasks. 

\textbf{A1:}
1. Why we choose WebShop and ALFWorld as evaluation benchmarks for strategic reasoning.

To stress-test the efficacy of EPO, we need environments and task setups that satisfy several
desiderata. First, the chosen tasks must require strategic multi-step planning and reasoning under
delayed rewards, and cannot be solved in one turn. We also want these tasks to require LLMs to
generate coherent natural language and keep the task realistics.
These tasks require strategic reasoning and planning over long horizons, allow non-trivial success rates, and come equipped with reproducible and task-directed evaluation metrics.~\cite{pmlr-v235-zhou24t}.

WebShop and ALFWorld are two physical domains in contrast to Sotopia which is a social domain, to further demonstrate the generalization of our method across different domains, which is a challenging issue to LLM reasoning~\cite{patil2025advancing}.


Thought: a rationale/short-CoT as the strategy
For example,



2. sotopia already contains some negotiation games as in Game-theoretic LLM, such as Prisoner's Dilemma (a complete-information game) and Deal/No Deal (an incomplete-information game).
Show corresponding results (goal and overall score and dialogues).
hightlight the challenge of sotopia.

3. the text games (such as Guess my city and twenty questions) in Archer are simpler and less realistic compared to Sotopia. As you mentioned, WebShop is also tested in Archer to evaluate model's strategic multi-step planning and reasoning under sparse rewards (see Section 5.1 in~\cite{pmlr-v235-zhou24t}).



\textbf{Q2-Evaluation performance:} SFT does even better than EPO-SFT on ALFWorld, and small gains in Tables 1 and 2, particularly in Sotopia.

\textbf{A2:}
We have illustrated the possible reason in line 1024-1025 in Appendix E.

goal completion [0-10] in line 378.
overall score [-4.286, 5.714] averaged across seven evaluation dimensions (as illustrated in Appendix B-\textbf{SOTOPIA}.

\begin{table*}[h]
    \centering
    \begin{tabular}{c|c}
    \toprule
      Dimension   &  Range \\
      \midrule
      Goal   & [0, 10]\\
      Believability & [0, 10]\\
      Knowledge & [0, 10]\\
      Relationship & [-5, 5] \\
      Finantial & [-5, 5]\\
      Secret & [-10, 0]\\
      Social rules & [-10, 0]\\
      \bottomrule
    \end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table*}



\textbf{Q3-Comparison with other RL baselines:} Most of the comparisons that take place are with prompting based methods, and directly treating the agent as a policy and training with REINFORCE would be simple to compare to.

\textbf{A3:}
1. baselines are not mostly prompting-based methods (only Vanilla and ReAct are prompting-based methods).
PPDPP and DAT are not.
Additional baselines: variants of EPO are not. three variants of EPO-RL
(1) EPO-Llama3-8B w/SFT has already achieved comparable or superior performance compared to EPO-GPT-4o (results). 
(2) EPO-Llama3-8B w/RL further surpasses EPO-llama3-8B w/SFT even in the setting without the use of PRM ().
(3) Using PRM and iterative self-play further boosts the performance of EPO-Llama3-8B w/RL ().

2. Llama3-8B RL 
EPO-Llama3-8B-RL w/o PRM
EPO-Llama3-8B-RL w/ PRM
EPO-Llama3-8B-RL w/ PRM + Self-play


\textbf{Q4-Supports for the second claim in contributions:}  Improvement in policy transferability and a cost comparison between EPO +RL+Self-Play and methods that use SFT as a preliminary step.

\textbf{A4:}
1. experimental settings to validate the policy transferability: Training scenarios and evaluation scenarios are completely non-overlapping. Show scenarios particularly from SOTOPIA-hard (evaluation) that are unseen during training (SOTOPIA-$\pi$).
different instructions in WebShop test scenarios and training ones.
Unseen evaluation scenarios in ALFWorld. 

2.  SFT as a preliminary step: SFT initialized model for RL.
results and training costs comparison
EPO-SFT+RL 
EPO-RL w/ PRM + Self-play (见的数据量是一样的）





