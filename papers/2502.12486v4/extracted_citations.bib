@inproceedings{10.5555/3491440.3491941,
author = {Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
title = {LogiQA: a challenge dataset for machine reading comprehension with logical reasoning},
year = {2021},
isbn = {9780999241165},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {501},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@article{Bakhtin2022HumanlevelPI,
  title={Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  author={Anton Bakhtin and Noam Brown and Emily Dinan and Gabriele Farina and Colin Flaherty and Daniel Fried and Andrew Goff and Jonathan Gray and Hengyuan Hu and Athul Paul Jacob and Mojtaba Komeili and Karthik Konath and Minae Kwon and Adam Lerer and Mike Lewis and Alexander H. Miller and Sandra Mitts and Adithya Renduchintala and Stephen Roller and Dirk Rowe and Weiyan Shi and Joe Spisak and Alexander Wei and David J. Wu and Hugh Zhang and Markus Zijlstra},
  journal={Science},
  year={2022},
  volume={378},
  pages={1067 - 1074},
  url={https://api.semanticscholar.org/CorpusID:253759631}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{deng2023plug,
  title={Plug-and-play policy planner for large language model powered dialogue agents},
  author={Deng, Yang and Zhang, Wenxuan and Lam, Wai and Ng, See-Kiong and Chua, Tat-Seng},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{duan-etal-2024-reta,
    title = "{R}e{TA}: Recursively Thinking Ahead to Improve the Strategic Reasoning of Large Language Models",
    author = "Duan, Jinhao  and
      Wang, Shiqi  and
      Diffenderfer, James  and
      Sun, Lichao  and
      Chen, Tianlong  and
      Kailkhura, Bhavya  and
      Xu, Kaidi",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.123/",
    doi = "10.18653/v1/2024.naacl-long.123",
    pages = "2232--2246",
    abstract = "Current logical reasoning evaluations of Large Language Models (LLMs) primarily focus on single-turn and static environments, such as arithmetic problems. The crucial problem of multi-turn, strategic reasoning is under-explored. In this work, we analyze the multi-turn strategic reasoning of LLMs through text-driven complete- and incomplete-information gaming, e.g., board games (Tic-Tac-Toe, Connect-4) and poker games (Texas Hold`em Poker). Specifically, we consider two distinct scenarios: 1) Online Racing, featuring multiple LLMs/agents to facilitate direct competition and comparison; 2) Offline Probing, constructing targeted questions with verified ground truth to evaluate LLMs' strategic behaviors. Experimental results demonstrate that existing state-of-the-art LLMs and reasoning schemes are largely ineffective for strategic reasoning tasks. To mitigate these limitations, we propose a simple yet effective Recursively Thinking-Ahead (ReTA) agent, incorporating a recursive prompting mechanism that automatically analyzes the opponents' future moves/actions and assigns reward signals for these situations, to strengthen the strategic reasoning of LLMs. We hope our work could spur further research and exploration in the multi-turn strategic reasoning of LLMs. The code is available at https://github.com/jinhaoduan/ReTA."
}

@article{fu2023improving,
  title={Improving language model negotiation with self-play and in-context learning from ai feedback},
  author={Fu, Yao and Peng, Hao and Khot, Tushar and Lapata, Mirella},
  journal={arXiv preprint arXiv:2305.10142},
  year={2023}
}

@article{gandhi2023strategic,
  title={Strategic reasoning with language models},
  author={Gandhi, Kanishk and Sadigh, Dorsa and Goodman, Noah D},
  journal={arXiv preprint arXiv:2305.19165},
  year={2023}
}

@article{ge2023openagi,
  title={Openagi: When llm meets domain experts},
  author={Ge, Yingqiang and Hua, Wenyue and Mei, Kai and Tan, Juntao and Xu, Shuyuan and Li, Zelong and Zhang, Yongfeng and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={5539--5568},
  year={2023}
}

@article{gemp2024states,
  title={States as strings as strategies: Steering language models with game-theoretic solvers},
  author={Gemp, Ian and Bachrach, Yoram and Lanctot, Marc and Patel, Roma and Dasagi, Vibhavari and Marris, Luke and Piliouras, Georgios and Liu, Siqi and Tuyls, Karl},
  journal={arXiv preprint arXiv:2402.01704},
  year={2024}
}

@article{guan2024deliberative,
  title={Deliberative alignment: Reasoning enables safer language models},
  author={Guan, Melody Y and Joglekar, Manas and Wallace, Eric and Jain, Saachi and Barak, Boaz and Heylar, Alec and Dias, Rachel and Vallone, Andrea and Ren, Hongyu and Wei, Jason and others},
  journal={arXiv preprint arXiv:2412.16339},
  year={2024}
}

@article{gulcehre2023reinforced,
  title={Reinforced self-training (rest) for language modeling},
  author={Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
  journal={arXiv preprint arXiv:2308.08998},
  year={2023}
}

@inproceedings{han-etal-2024-towards,
    title = "Towards Uncertainty-Aware Language Agent",
    author = "Han, Jiuzhou  and
      Buntine, Wray  and
      Shareghi, Ehsan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.398/",
    doi = "10.18653/v1/2024.findings-acl.398",
    pages = "6662--6685",
}

@inproceedings{he-etal-2024-planning,
    title = "Planning Like Human: A Dual-process Framework for Dialogue Planning",
    author = "He, Tao  and
      Liao, Lizi  and
      Cao, Yixin  and
      Liu, Yuanxing  and
      Liu, Ming  and
      Chen, Zerui  and
      Qin, Bing",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.262/",
    doi = "10.18653/v1/2024.acl-long.262",
    pages = "4768--4791",
}

@article{huyuk2024reasoning,
  title={Reasoning Elicitation in Language Models via Counterfactual Feedback},
  author={H{\"u}y{\"u}k, Alihan and Xu, Xinnuo and Maasch, Jacqueline and Nori, Aditya V and Gonz{\'a}lez, Javier},
  journal={arXiv preprint arXiv:2410.03767},
  year={2024}
}

@article{kumar2024training,
  title={Training language models to self-correct via reinforcement learning},
  author={Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others},
  journal={arXiv preprint arXiv:2409.12917},
  year={2024}
}

@article{li2024dialogue,
  title={Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner},
  author={Li, Kenneth and Wang, Yiming and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2406.11978},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@InProceedings{pmlr-v235-zhou24t,
  title = 	 {{A}r{CH}er: Training Language Model Agents via Hierarchical Multi-Turn {RL}},
  author =       {Zhou, Yifei and Zanette, Andrea and Pan, Jiayi and Levine, Sergey and Kumar, Aviral},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {62178--62209},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
}

@article{putta2024agent,
  title={Agent q: Advanced reasoning and learning for autonomous ai agents},
  author={Putta, Pranav and Mills, Edmund and Garg, Naman and Motwani, Sumeet and Finn, Chelsea and Garg, Divyansh and Rafailov, Rafael},
  journal={arXiv preprint arXiv:2408.07199},
  year={2024}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{sun2024enhancing,
  title={Enhancing agent learning through world dynamics modeling},
  author={Sun, Zhiyuan and Shi, Haochen and C{\^o}t{\'e}, Marc-Alexandre and Berseth, Glen and Yuan, Xingdi and Liu, Bang},
  journal={arXiv preprint arXiv:2407.17695},
  year={2024}
}

@inproceedings{suzgun-etal-2023-challenging,
    title = "Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
    author = {Suzgun, Mirac  and
      Scales, Nathan  and
      Sch{\"a}rli, Nathanael  and
      Gehrmann, Sebastian  and
      Tay, Yi  and
      Chung, Hyung Won  and
      Chowdhery, Aakanksha  and
      Le, Quoc  and
      Chi, Ed  and
      Zhou, Denny  and
      Wei, Jason},
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.824/",
    doi = "10.18653/v1/2023.findings-acl.824",
    pages = "13003--13051",
}

@article{turpin2024language,
  title={Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting},
  author={Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wang2024cooperative,
  title={Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models},
  author={Wang, Danqing and Ye, Zhuorui and Fang, Fei and Li, Lei},
  journal={arXiv preprint arXiv:2410.20007},
  year={2024}
}

@inproceedings{wilf-etal-2024-think,
    title = "Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities",
    author = "Wilf, Alex  and
      Lee, Sihyun  and
      Liang, Paul Pu  and
      Morency, Louis-Philippe",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.451/",
    doi = "10.18653/v1/2024.acl-long.451",
    pages = "8292--8308",
}

@article{xu2023exploring,
  title={Exploring large language models for communication games: An empirical study on werewolf},
  author={Xu, Yuzhuang and Wang, Shuo and Li, Peng and Luo, Fuwen and Wang, Xiaolong and Liu, Weidong and Liu, Yang},
  journal={arXiv preprint arXiv:2309.04658},
  year={2023}
}

@article{zeng2023agenttuning,
  title={Agenttuning: Enabling generalized agent abilities for llms},
  author={Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2310.12823},
  year={2023}
}

@article{zhu2023ghost,
  title={Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory},
  author={Zhu, Xizhou and Chen, Yuntao and Tian, Hao and Tao, Chenxin and Su, Weijie and Yang, Chenyu and Huang, Gao and Li, Bin and Lu, Lewei and Wang, Xiaogang and others},
  journal={arXiv preprint arXiv:2305.17144},
  year={2023}
}

