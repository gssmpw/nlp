\section{Conclusion}
%In this work, we investigate how to handle LLM's strategic reasoning in dynamic and open-ended contexts. To accomplish this, we propose \textit{EPO}, a collaborative strategic reasoning framework designed to enhance goal-oriented behavior in LLMs by dedicating a reasoning agent to develop strategies that downstream LLMs can integrate so as to collectively make intelligent decisions toward goals. \textit{EPO} operates through a multi-turn RL pipeline devoid of the need for supervised initialization, and leverages process rewards and iterative self-play to refine strategies adaptively. \textit{EPO} is generalizable and flexible with robust transferability to new domains and models. Our empirical analysis shows that \textit{EPO} consistently outperforms prior methods in goal achievement, marking a significant advancement in making LLMs adept at navigating complex interactions.

We propose \textit{EPO}, an explicit policy optimization method for strategic reasoning in LLMs via RL.
By training a strategic reasoning model through pure RL, our method can flexibly assist arbitrary LLM agents to motivate their goal-directed behavior when navigating in dynamic environments. 
Particularly, we develop a multi-turn RL pipeline to optimize the policy of the reasoning model, leveraging process rewards and iterative self-play. This method allows the reasoning model to transfer its policy to various scenarios. Meanwhile, the LLM agent for interacting with environments remains unchanged, maintaining its ability to generalize across domains. Our results demonstrate that \textit{EPO} achieves superior performance in long-term goal alignment with enhanced strategic reasoning via RL.
Further analysis reveal diverse collaborative reasoning mechanisms emergent in \textit{EPO} as well as novel strategies devised by our reasoning model, advancing LLM's strategic reasoning to handle complex real-world scenarios.



