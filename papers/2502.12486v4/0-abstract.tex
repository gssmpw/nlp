\begin{abstract}
%To enable goal-oriented behavior in LLMs, strategic reasoning is essential, requiring the ability to adapt to adversary behavior or dynamic environment.
%Strategic reasoning is essential in dynamic and open-ended environments where success depends on foresight, structured analysis, and iterative execution. Current methods for enhancing strategic reasoning of large language models (LLMs) rely on prompt engineering, direct fine-tuning, or intricate modular and system design. However, these approaches are either bounded by the inherit reasoning ability of pre-trained models or less generalizable and flexible to be transferred to new domains or models. In this paper, we propose a collaborative strategic reasoning framework (\textit{CoRe}) for elicting goal-oriented behavior from LLMs, which preserves their generalizability for decision-making while accommodating strategic reasoning flexibly.To do this, our framework disentangles strategic reasoning from decision-making and assigns distinct duties to different LLMs. In particular, an LLM agent dedicated to strategic reasoning provides creative and flexible strategies, while decision-making agents selectively adopt these strategies and respond to other agents or an environment. By optimizing the strategic reasoning agent's policy purely through multi-turn reinforcement learning, we demonstrate that \textit{CoRe} improves goal achievement by maximum 50.2\% in challenging social interactions and 11.5\% in embodied environment, and achieves state-of-the-art performance on social dialogue and web navigation tasks.

% 1. 为什么是Cooperative Strategic Reasoning， SOTOPIA场景应该有很多非合作的策略推理
%这里的cooperative指的是策略推理模型和决策智能体之间的关系，因为策略模型用来辅助智能体进行决策


Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning—an ability to navigate dynamic environments and align long-term goals amidst uncertainty.
Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts.
To address these issues, we propose explicit policy optimization (\textit{EPO}) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior.
To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL), utilizing process rewards and iterative self-play.
Experiments across social and physical domains demonstrate \textit{EPO}'s ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in \textit{EPO} and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. We will release our models and code soon.

\end{abstract}