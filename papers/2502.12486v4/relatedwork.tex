\section{Related Work}
\paragraph{\textbf{Strategic Reasoning in LLMs.}}
%Current studies addressing strategic reasoning in LLMs have explored diverse approaches, including multi-agent interaction and game theory~\cite{Bakhtin2022HumanlevelPI,xu2023exploring,ma2024large,gemp2024states}, uncertainty handling and probablistic reasoning~\cite{hu2024uncertainty,huyuk2024reasoning,han-etal-2024-towards}, world modeling and system dynamics~\cite{zhu2023ghost,ge2023openagi,hao2023reasoning,sun2024enhancing}.
%Motivated by in-context learning and reasoning abilities of LLMs, recent work utilizes LLMs to expand the application scope and improve the transferability of strategic reasoning~\cite{zhang2024llm}.Some works focus on building prompting techniques for analyzing opponentsâ€™ future moves~\cite{duan-etal-2024-reta}, attributing mental states using Theory of Mind~\cite{gandhi2023strategic, wilf-etal-2024-think}, or constructing in-context demonstrations through environment or AI feedback~\cite{fu2023improving, wang2024voyager}. However, the effectiveness of these methods is inherently limited by pretraining, and approaches such as chain-of-thought can produce unfaithful explanations for model predictions~\cite{turpin2024language}. To enhance pre-trained abilities, another line of work fine-tunes the LLM on agent tasks through IL~\cite{zeng2023agenttuning} or RL~\cite{he-etal-2024-planning,putta2024agent}. Despite promising results, successful trajectories for IL can be costly to obtain, and directly fine-tuning the LLM as agents can hinder its general abilities. Therefore, this paper addresses these issues by disentangling strategic reasoning from decision-making without compromising the general capabilities of the LLM agent.
%Beyond prompting and fine-tuning, a majority of work also attempts to design modular enhanced LLM agents by integrating functionalities like memory modules and external tools~\cite{zhu2023ghost,ge2023openagi,hao2023reasoning,sun2024enhancing}, or multi-agent systems~\cite{Bakhtin2022HumanlevelPI, bakhtin2023mastering,xu2023exploring,ma2024large} in strategic reasoning scenarios. In the setting of goal-oriented dialogues, PPDPP~\cite{deng2023plug} and DAT~\cite{li2024dialogue} train a separate planner model via RL that predicts dialogue acts to strategize LLMs with dialogue policy planning. \cite{gemp2024states} employs off-the-shelf game-theorectic algorithms to compute optimal actions to guide the dialogue agent. However, actions (or strategies) predicted in these methods are confined to a finite, pre-defined set, or represented by a continuous vector that cannot be interpretated for decision-making. Instead, our method can handle a dynamic and open-ended action (or strategy) space in natural language, enhancing the transparency and interpretability of decision-making. Concurrent to our work, CoPlanner~\cite{wang2024cooperative} introduces a cooperative multi-agent framework for solving static reasoning problems with a human-crafted meta-strategy pool and maximumly two rounds for planning, whereas our method supports strategic reasoning for dynamic and long-horizon planning.
Motivated by in-context learning and reasoning abilities of LLMs, recent work utilizes LLMs for strategic reasoning in dynamic environment~\cite{zhang2024llm}.
While some work employs prompting techniques for opponent analysis~\cite{duan-etal-2024-reta}, theory of mind~\cite{gandhi2023strategic, wilf-etal-2024-think}, or in-context demonstrations~\cite{fu2023improving, wang2024voyager}, their effectiveness is limited by inherit abilities and issues like unfaithful explanations~\cite{turpin2024language}. 
Fine-tuning LLMs through IL or RL~\cite{zeng2023agenttuning,he-etal-2024-planning,putta2024agent} can help but struggles to generalize reasoning skills across domains.
Beyond prompting and fine-tuning, modular enhancements such as memory modules, external tools~\cite{zhu2023ghost,ge2023openagi,hao2023reasoning,sun2024enhancing}, and multi-agent systems~\cite{Bakhtin2022HumanlevelPI, bakhtin2023mastering,xu2023exploring,ma2024large} have been explored. Some of these work develop dialogue action planners via RL~\cite{deng2023plug,li2024dialogue} or game-theoretic algorithms~\cite{gemp2024states}, but rely on finite, predefined action sets or lack interpretability. 
In contrast, our approach supports open-ended action space and improves the interpretability of strategic reasoning by generating strategies in natural language. 
Concurrent to our work, CoPlanner~\cite{wang2024cooperative} focuses on static reasoning with limited planning rounds, whereas our method enhances strategic reasoning for dynamic long-horizon planning.

\paragraph{\textbf{Reinforcement learning for LLMs.}}
%Most prior works utilizing RL for LLMs focus on static problem-solving, such as question answering~\cite{10.5555/3491440.3491941, suzgun-etal-2023-challenging}, math problems~\cite{lightman2024lets, kumar2024training}, and preference alignment~\cite{christiano2017deep, guan2024deliberative} without further engagement with environments or other agents. For example, methods in reinforcement learning from human feedback (RLHF)~\cite{ouyang2022training, gulcehre2023reinforced, rafailov2024direct} formulate reward maximization as a one-step bandit problem. This can result in LLMs prioritizing human-like responses rather than engaging in interactions. However, numerous crucial agent tasks cannot be addressed in a single-turn manner, since such approaches fail to develop complex strategies. Therefore, multi-turn RL algorithms~\cite{pmlr-v235-zhou24t, shani2024multiturn} are designed to endow LLMs with dynamic planning in goal-oriented environments. Our work does not focus on building machinery for RL, but rather considers strategic reasoning as an RL challenge. We aim to demonstrate the feasibility of training LLMs for strategic reasoning with a pure multi-turn RL objective, regardless of the particular RL algorithm employed.
Prior RL applications for LLMs often focus on static problem-solving like question answering~\cite{10.5555/3491440.3491941, suzgun-etal-2023-challenging}, math problems~\cite{lightman2024lets, kumar2024training}, and preference alignment~\cite{christiano2017deep, guan2024deliberative}.
For example, methods in reinforcement learning from human feedback (RLHF)~\cite{ouyang2022training, gulcehre2023reinforced, rafailov2024direct} treat reward maximization as a one-step bandit problem, which prioritizes human-like responses rather than interactive engagement.
Instead, numerous agent tasks require interactions and complex strategies, prompting the development of multi-turn RL algorithms~\cite{pmlr-v235-zhou24t, shani2024multiturn} for LLMs. 
Our work considers strategic reasoning as an RL challenge, demonstrating the feasibility of training LLMs for strategic reasoning through a pure RL process, independent of the specific RL algorithm used.