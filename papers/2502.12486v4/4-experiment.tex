\section{Experiment}
The goal of our experiments is to demonstrate the efficacy and justify the design of \textit{EPO} in enhancing strategic reasoning via RL.
Our experiments address the following questions: \textbf{(1)} Is it necessary to explicitly optimize policy for strategic reasoning and what are the unique advantages? 
\textbf{(2)} How iterative self-play can scale RL training in \textit{EPO}? \textbf{(3)} How the reasoning model in \textit{EPO} affects agent's behavior and under what circumstances can optimal reasoning be achieved? 
\textbf{(4)} Which components of \textit{EPO} are critical for effective strategic reasoning? 
To this end, we evaluate \textit{EPO} against baselines, perform analysis and run ablations.


\subsection{Experimental Settings}
\paragraph{\textbf{Environments and Datasets.}}
We evaluate \textit{EPO} in three distinct social and physical environments requiring strategic reasoning:
(1) SOTOPIA~\cite{zhou2024sotopia} for social interaction with goals. 
A challenging subset of scenarios (SOTOPIA-hard) demands advanced strategic reasoning;
(2) WebShop~\cite{yao2022webshop} for web navigation with dense final rewards (0–1); 
(3) ALFWorld~\cite{shridhar2021alfworld} for the embodied household, comprising out-of-distribution (OOD) task variations and binary rewards indicating task success.

Dataset statistics are shown in Table~\ref{tab:data}.
For SOTOPIA, we collect training data for the reasoning model from
SOTOPIA-$\pi$~\cite{Wang2024SOTOPIAIL}, where training scenarios are completely non-overlapping with test ones to ensure generalization.
We use GPT-4-Turbo to generate strategies and dialogue histories with CoT prompting.
For WebShop and ALFWorld, the training data follows~\cite{song-etal-2024-trial} where each trajectory contains CoT rationales that we assume as the strategy for each action step.
See Appendix~\ref{append:dataset} for more details of the environments and data collection.

\paragraph{\textbf{Evaluation Prompts and Metrics.}}
We evaluate \textit{EPO} using zero-shot prompting for SOTOPIA and one-shot prompting for WebShop and ALFWorld following~\cite{song-etal-2024-trial}. 
For SOTOPIA, we measure goal completion (0-10) and overall score using GPT-4o~\cite{4o} as a proxy for human judgment following~\cite{zhou2024sotopia}. 
For WebShop and ALFWorld, we use average reward as the metric following~\cite{song-etal-2024-trial}. Detailed prompts are provided in Appendix~\ref{append:task-prompt}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[ht]
\centering
\begin{tabular}{lcccc|cccc}
\toprule
\multirow{3}{*}{\textbf{Method}}&\multicolumn{4}{c}{\textbf{GPT-4o}} & \multicolumn{4}{c}{\textbf{Llama3.1-70B-Instruct}} \\
&\multicolumn{2}{c}{\textbf{Hard}} & \multicolumn{2}{c}{\textbf{All}} &  \multicolumn{2}{c}{\textbf{Hard}} & \multicolumn{2}{c}{\textbf{All}} \\
&{\small Goal} & {\small Overall} & {\small Goal} & {\small Overall} & {\small Goal} & {\small Overall} & {\small Goal} & {\small Overall} \\
\midrule
Vanilla  & 6.27 & 3.42 & 8.16 & 3.73 & 4.98 & 2.49 & 7.48 & 3.37 \\
ReAct~\cite{yao2023react} & 6.39 & 3.31 & 8.09 & 3.66 & 5.09 & 2.46 & 7.30 & 3.25 \\
PPDPP~\cite{deng2023plug}  & 6.09 & 3.31 & 7.94 & 3.60 & 4.86 & 2.45 & 7.63 & 3.48 \\
DAT~\cite{li2024dialogue} & - & - & - & - & 5.11 & 2.52 & 7.76 & 3.56 \\
\midrule
EPO-Claude-3.5-Sonnet & 6.57 & 3.37 & 8.08 & 3.64 & 6.16 & 3.32 & 7.98 & 3.70 \\
EPO-GPT-4o & 6.73 & 3.53 & 8.27 & 3.78 & 6.45 & \textbf{3.46} & 8.18 & 3.82 \\
EPO-Llama3-8B & 6.50 & 3.45 & 8.11 & 3.78 & 5.88 & 3.14 & 8.04 & 3.68 \\
EPO-Llama3-8B w/ SFT & 6.76 & 3.42 & 8.28 & 3.75 & 6.96 & 3.23 & 8.29 & 3.66 \\
EPO-Llama3-8B w/ RL & 7.20 & \textbf{3.58} & 8.58 & 3.88 & 7.07 & 3.33 & 8.35 & 3.72 \\
EPO-Llama3-8B w/ RL+Self-play & \textbf{7.78} & \textbf{3.58} & \textbf{8.84} & \textbf{3.92} & \textbf{7.48} & 3.41 & \textbf{8.53} & \textbf{3.85} \\
\bottomrule
\end{tabular}
\caption{\textbf{The goal completion and overall scores on SOTOPIA.}
GPT-4o or Llama3.1-70B-Instruct serves as the self-play dialogue agent.
``EPO-(model)'' represents the strategic reasoning model instantiated by an LLM with and without additional training. 
DAT is exclusive to open-source LLMs.
}
\label{tab:sotopia-result}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[ht]
\centering
\begin{tabular}{lccc|ccc}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{GPT-4o}} & \multicolumn{3}{c}{\textbf{Llama3-8B-Instruct}} \\
& \multirow{2}{*}{\textbf{WebShop}} & \multicolumn{2}{c}{\textbf{ALFWorld}} & \multirow{2}{*}{\textbf{WebShop}} & \multicolumn{2}{c}{\textbf{ALFWorld}} \\
\cmidrule{3-4} \cmidrule{6-7} 

  & &  {\small Seen} &  {\small Unseen} & &  {\small Seen} &  {\small Unseen} \\
\midrule
ReAct~\cite{yao2023react} & 61.9 & 38.6 & 38.1 & 58.2 & 4.3 & 3.0 \\
\midrule
EPO-Llama3-8B w/ SFT & 67.1 & 45.9 & 44.1 & 64.4 & 12.1 & 10.5 \\
EPO-Llama3-8B w/ RL & \textbf{68.1} & \textbf{47.2} & \textbf{46.6} & \textbf{66.9} & \textbf{14.3} & \textbf{14.2}\\
\bottomrule
\end{tabular}
\caption{\textbf{The average reward on WebShop and ALFWorld.}
GPT-4o or Llama3-8B-Instruct serves as the LLM agent interacting with the environment.
``Seen'' refers to the test set with task types seen during training and ``Unseen'' denotes the test set with OOD task variations.}
\label{tab:alshop-result}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\paragraph{\textbf{Implementation Details.}}  
We mainly use Llama3-8B-Instruct~\cite{dubey2024llama} as the base model for RL training. 
The batch size is 32 and the learning rate is 1e-6 with 3\% warm-up and a cosine scheduler. We set the learning epochs to 3 and the discount factor to 0.99. 
The total training steps are around 459 in SOTOPIA and 650 in cross-domain training on WebShop and ALFWorld. During evaluation, we use GPT-4o or Llama3.1-70B-Instruct~\cite{llama-3.1} as the self-play dialogue agent in SOTOPIA. For WebShop and ALFWorld, we use GPT-4o or Llama3-8B-Instruct as the agent for interating with environments. 
By default, our reasoning model is plugged into all interacting agents within an environment.
On SOTOPIA, the temperature of our reasoning model and LLMs responsible for self-chat is set to be 0.7. 
For WebShop and ALFWorld, LLMs responsible for interacting with the environment adopt greedy decoding (tempature 0). 
Refer to Appendix~\ref{append:imple} for more details.



\paragraph{\textbf{Baselines and Comparisons.}}  
We evaluate our method against several baselines:  
(1) \textbf{Vanilla}, a standard prompting method;  
(2) \textbf{ReAct}~\cite{yao2023react}, which employs CoT reasoning within a single LLM to generate rationales before actions;  
(3) \textbf{PPDPP}~\cite{deng2023plug}, a dialogue action planning method that uses an RL-trained policy planner to predict annotated dialogue acts; 
(4) \textbf{DAT}~\cite{li2024dialogue}, which predicts continuous action vectors through an RL-trained planner to control LLM outputs.  
To validate our RL-driven reasoning model, we introduce additional baselines: \textbf{EPO-Claude-3.5-Sonnet}, \textbf{EPO-GPT-4o} and \textbf{EPO-Llama3-8B} that use off-the-shelf LLMs for strategy generation without additional training.
We also compare to supervised fine-tuning (\textbf{EPO-SFT}) for policy optimization of our reasoning model. Details of implementation can be found in Appendix~\ref{append:imple}.




\subsection{Results and Analysis}

\paragraph{\textbf{Q1: Effectiveness and Advantages of \textit{EPO}.}}
Results in Table~\ref{tab:sotopia-result} and~\ref{tab:alshop-result} show \textit{EPO}'s superior performance over baselines, highlighting the effectiveness of explicit policy optimization for strategic reasoning via RL.
As shown in Table~\ref{tab:sotopia-result}, methods such as EPO-Claude-3.5-Sonnet, EPO-GPT-4o, and EPO-Llama3-8B outperform prompting methods or domain-specific planners, despite that the strategic reasoning model is simply a standard LLM without additional training.
With post-training particularly through RL, the reasoning model's abilities can be signicantly improved and iterative self-play further amplifies its performance, exceeding state-of-the-art results~\cite{zhang2024k} on SOTOPIA.
The design of \textit{EPO} can be further underscored when compared to training a single LLM to output both strategy and behavior for each interaction turn (see results in Appendix~\ref{append:add_res}). Our reasoning model trained with RL plugged into GPT-4o even exceeds state-of-the-art result on WebShop that involves optimizing an LLM agent via step-wise RL~\cite{Deng2024FromNT}. This demonstrates that explicitly optimizing an LLM for strategic reasoning and plugged into a frozen LLM agent is crucial, as it prevents overfitting in behavior and allows focus on enhancing strategic reasoning via RL.

\textit{EPO} offers unique advantages arising from explicit policy optimization for strategic reasoning. Firstly, our strategic reasoning model, being an LLM, is capable of generating open-domain strategies instead of predefined, domain-specific actions, such as those in PPDPP. This attibute allows us to train the model on multi-task and cross-domain scenarios through RL, enhancing its policy transferability and adaptability to new scenarios. For example, in Table~\ref{tab:alshop-result}, our RL-trained reasoning model can even assist a less capable LLM agent (Llama3-8B-Instruct) in achieving long-term goals, obtaining a significant improvement over ReAct (+11.5\%) in ALFWorld-Unseen tasks.
Secondly, our reasoning model can be seamlessly integrated with different advanced LLM agents for navigating complex environments. The LLM agents for interacting remain unchanged, preserving their general-domain capabilities. This flexibility not only enhances performance but also maintains adaptability and generalizability, making \textit{EPO} a versatile method for dynamic interactive environments.



%On SOTOPIA-hard as shown in Table~\ref{tab:sotopia-result}, \textit{EPO-RL} achieves superior performance in goal completion, surpassing the strongest baseline. Iterative self-play further amplifies performance and exceeds state-of-the-art results~\cite{zhang2024k}. Notably, \textit{EPO-RL} outperforms dialogue-specific planners ($5.11\rightarrow7.07$), underscoring the value of scaling to open-domain action spaces instead of predefined, domain-specific dialogue acts. In addition, even powerful closed-source LLMs underperform when used as untrained strategic reasoning models, while SFT lacks the adaptability of RL ($6.76\rightarrow7.20$). These gains highlight the effectiveness of disentangling strategic reasoning from behavior generation and optimizing strategies through dynamic RL-driven planning. The advantages of \textit{EPO} extend to physical domains as shown in Table~\ref{tab:alshop-result}. On WebShop, \textit{EPO-RL} with GPT-4o exceeds state-of-the-art fine-tuning agents~\cite{Deng2024FromNT}, despite that the downstream agent remains a frozen LLM. The RL-trained reasoning model also enhances the decision-making of a less capable model: Llama3-8B-Instruct guided by \textit{EPO-RL} achieves a significant improvement over ReAct (+11.5\%) in ALFWorld-Unseen tasks. This demonstrates \textit{EPO}’s ability to transfer learned strategies to novel scenarios and smaller models, a critical requirement for real-world applications. The results validate two key insights: (1) The strategic reasoning model exhibits strong transferability, enabling consistent performance improvements across diverse models and tasks; (2) RL-based optimization of the reasoning model is essential for handling interactive tasks, as evidenced by \textit{EPO-RL}’s superiority over SFT and untrained LLM variants. These findings position \textit{EPO} as a scalable framework for deploying LLMs in dynamic environments, where traditional prompting or domain-specific planners fall short.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Our results on SOTOPIA are shown in Table~\ref{tab:sotopia-result}. \textit{EPO} exhibits substantially stronger performance with both trained and untrained strategic reasoning models relative to baselines.Notably, \textit{EPO}-GPT-4o exhibits advantages over ReAct even when GPT-4o acts as the dialogue agent whose output contains strategies before actions. This demonstrates the effectiveness of our policy steering framework compared to prompting a single LLM to generate strategic reasoning and behaviors. Furthermore, strategic reasoning models after fine-tuning on data collected by a capable LLM, consistently outperform their untrained counterparts as well as baselines, particularly in goal completion on SOTOPIA-hard. This advantage is even more pronounced with iterative self-play RL training on self-generated strategy data. This means that the algorithmic design of \textit{EPO} is critical in enhancing its strategic reasoning capabilities beyond mere expert supervision.

%Table~\ref{tab:alshop-result} presents our results on WebShop and ALFWorld. \textit{EPO} achieves notable gains in both web-based and household settings, surpassing ReAct by substantial margins, especially when assisting a less capable exploration agent (+8.7\% on WebShop; +11.5\% on ALFWorld). The performance of \textit{EPO}-RL when employing GPT-4o as the exploration agent on WebShop, even exceeds state-of-the-art fine-tuning agents such as StepAgent~\cite{Deng2024FromNT}, despite that the downstream agent in \textit{EPO} remains a frozen LLM without tuning. Additionally, \textit{EPO} shows advancement in both seen and unseen scenarios in ALFWorld, suggesting that the strategy induced by our strategic reasoning model can be generalized to novel scenarios that are out of the training domain.

%In summary, the significant improvements observed in various social and physical contexts highlight the generalizability of \textit{EPO} to diverse environments. Moreover, its ability to integrate seamlessly with different types and sizes of LLM agents, showcases the flexibility of \textit{EPO} in assisting downstream agents of varying capabilities. Generally, the generalizability and flexibility of \textit{EPO} across a range of environments and agent configurations undersEPO its potential as a versatile framework for eliciting goal-oriented behavior with strategic reasoning. 




\paragraph{\textbf{Q2: RL Scaling with Iterative Self-Play.}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figure/self-play.png}}
\caption{\textbf{Iterative self-play RL scaling in \textit{EPO}.}
\textbf{Left:} The goal completion in test scenarios from SOTOPIA where we use GPT-4o as the self-play dialogue agent.
\textbf{Right:} The goal completion of training data for each iteration of RL training. GPT-4-Turbo serves as the dialogue agent for self-play in scenarios from SOTOPIA-$\pi$.}

\label{fig:self-play}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


To scale up RL training, we investigate if \textit{EPO} can be improved through iterative self-play.
Figure \ref{fig:self-play}(a) shows that performance steadily improves as RL training iteration increases, though gains diminish after the fifth round, indicating a gradual process toward an optimized policy of the strategic reasoning model. This improvement is because the strategies generated by our RL-trained reasoning model are more effective than those from the initial CoT data.
In addition, the strategy data for training is evaluated by the PRM, refining policy optimization for subsequent RL training rounds. Consequently, as shown in Figure \ref{fig:self-play}(b), the quality of training data improves until the policy converges, demonstrated by increased goal completion rates.
Overall, iterative self-play effectively scales RL training and enhances performance, mirroring real-world dynamics and enabling the reasoning model to adapt strategies for unpredictable adversary behavior.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\centering
\centerline{\includegraphics[width=\columnwidth]{Figure/coordinate.png}}
\caption{\textbf{Collaborative Reasoning Mechanisms in \textit{EPO}.} We evaluate four configurations in SOTOPIA: (1) “ReAct”, where both dialogue parties generate strategies before responses; (2) “EPO-RL vs. ReAct”, with one party using an RL-trained reasoning model and the other using ReAct; (3) “EPO-RL vs. EPO-Llama3”, comparing RL-trained and vanilla (Llama3-8B-Instruct) reasoning models; and (4) “EPO-RL”, where both parties employ RL-trained reasoning model. “Avg Goal” measures the average success in achieving social goals. (a) and (c) show GPT-4o self-play, while (b) and (d) involve GPT-3.5-Turbo and GPT-4o as the dialogue partners.
}
\label{fig:coordinate}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{\textbf{Q3: Collaborative Reasoning Mechanisms in \textit{EPO}.}}
To understand how our strategic reasoning model incentivizes goal-directed behavior from LLM agents, we analyze the collaborative mechanisms between the reasoning model and LLM agents as illustrated in Figure~\ref{fig:coordinate}.
The results show that the bidirectional EPO-RL configuration achieves the highest average goal, whether in symmetric self-play (GPT-4o vs. GPT-4o) or asymmetric interactions (GPT-3.5-Turbo vs. GPT-4o). This indicates that mutual strategic reasoning fosters win-win outcomes by aligning long-term goals.
With our RL-trained reasoning model, LLM agents in a dialogue tend to identify opportunities for mutual benefit, even without knowing the other’s goal or strategies. This promotes better coordination and reduces misaligned behavior toward their respective goals. 

We can observe an interesting phenomenon in Figure \ref{fig:coordinate}(a) where a dialogue participant using an RL-trained reasoning model achieves lower goal completion compared to its partner using either ReAct prompting or a non-trained model like vanilla Llama3-8B-Instruct. This phenomenon is particularly evident in competitive scenarios, such as negotiations, the Prisoner's Dilemma, and resource allocation in SOTOPIA. The discrepancy may arise because the strategies produced by the RL-trained model are implicitly embedded in the dialogue history. As a result, the partner may exploit these strategies to prioritize its goal without considering mutual benefits. Similar observations have been made in previous studies~\cite{Hua2024GametheoreticLA}.

From a game theory perspective, most real-life social interactions can be characterized as games with mixed strategies~\cite{75b5cec4-4f40-3f36-9492-860b8376add8}, involving both competitive and cooperative elements. These games necessitate a comprehensive evaluation of multiple objectives, including individual, collective, and equilibrium benefits. Achieving Pareto optimality is one solution to these multi-objective problems.
Our findings suggest that dialogue participants tend to reach Pareto optimality when both employ strong, RL-trained reasoning models. In contrast, if one party uses a robust reasoning model and the other employs a weaker one, the latter may achieve higher individual goal completion but fails to reach Pareto optimum state, leading to lower average benefits for both parties.

In general, the exploration of collaborative reasoning mechanisms in \textit{EPO} highlights a complex interplay between strategic reasoning and agent behavior. Optimal outcomes are achieved when both LLM agents utilize the RL-trained reasoning model, enabling them to make well-informed strategic behavior.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\centering
\scalebox{0.8}{
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Hard} (Goal) & \textbf{All} (Goal) \\
\midrule
EPO-Llama3-8B & 6.50 & 8.11 \\
w/ RL & 6.95 & 8.50 \\
w/ RL + PRM & 7.20 & 8.58 \\
w/ RL + PRM + Self-play & 7.78 & 8.84 \\
\hdashline
EPO-Mistral-7B w/ RL+PRM & 7.03 & 8.47 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Ablation studies on \textit{EPO} components.} 
Experiments are conducted on SOTOPIA with GPT-4o acting as the self-play dialogue agent.
In ``EPO-Llama3-8B w/ RL'', we only use the final goal completion score as the reward for RL training.}
\label{tab:ablation}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\paragraph{\textbf{Q4: Ablation Studies.}}
\label{sec:ablation}
%By unifying process-aware RL with collaborative role specialization, our methodology enables the strategic reasoning model to generate strategies that are both tactically sound in the short term and coherent over extended horizons—addressing the EPO challenges of strategic reasoning. 
%Concretely, we aim to examine \textbf{(1)} the impact of process rewards for the RL training of strategic reasoning models; \textbf{(2)} the importance of RL in strategy optimization; and \textbf{(3)} whether the performance gains can be transferred to other base models for strategic reasoning.
We present results of ablation experiments in Table~\ref{tab:ablation} to validate the necessity of \textit{EPO}'s key components for strategic reasoning.
First, performance drops sharply without RL-based optimization, demonstrating its importance in cultivating adaptive strategies to handle dynamic environments.
Second, eliminating process rewards provided by the PRM degrades performance, which confirms their role in incentivizing intermediate milestones that align with long-term goals, such as \textit{maintaining negotiation rapport} or \textit{balancing competing objectives}.
Third, incorporating iterative self-play further boosts performance, suggesting its effectiveness in scaling RL efforts.
Finally, switching the base model to Mistral-7B-Instruct~\cite{jiang2023mistral} leads to a minor performance decline, indicating that while model capacity affects reasoning quality, \textit{EPO} maintains robustness across model architectures. 
Overall, these results underscore the value of RL training, process rewards and iterative self-play in advancing strategic reasoning in \textit{EPO}.
%These results underscore that process-aware RL training drives effective strategic reasoning by enabling iterative refinement of strategies under shifting constraints.


\subsection{Case Studies}
We show a negotiation example in Table~\ref{tab:case} to demonstrate \textit{EPO}'s ability to convert strategic reasoning into goal-directed behavior and enhanced strategic reasoning through RL.
Particularly, strategies produced by ReAct remain static and myopic: \textit{the buyer rigidly anchors to her initial offer, while the seller relies on generic value claims}.
EPO-Llama3-8B trained with SFT introduces more structured strategies (e.g., \textit{phased concessions}) but remains constrained by supervised patterns, lacking innovation and variation. 
In contrast, the RL-trained reasoning model generates creative and flexible strategies such as \textit{urgency creation} and \textit{value-based persuasion}, which are refined through multi-turn RL.
The divergence stems from explicit policy optimization for strategic reasoning and the ability to reason for optimal strategies through RL.



 

