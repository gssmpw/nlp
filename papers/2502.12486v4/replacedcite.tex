\section{Related Work}
\paragraph{\textbf{Strategic Reasoning in LLMs.}}
%Current studies addressing strategic reasoning in LLMs have explored diverse approaches, including multi-agent interaction and game theory____, uncertainty handling and probablistic reasoning____, world modeling and system dynamics____.
%Motivated by in-context learning and reasoning abilities of LLMs, recent work utilizes LLMs to expand the application scope and improve the transferability of strategic reasoning____.Some works focus on building prompting techniques for analyzing opponentsâ€™ future moves____, attributing mental states using Theory of Mind____, or constructing in-context demonstrations through environment or AI feedback____. However, the effectiveness of these methods is inherently limited by pretraining, and approaches such as chain-of-thought can produce unfaithful explanations for model predictions____. To enhance pre-trained abilities, another line of work fine-tunes the LLM on agent tasks through IL____ or RL____. Despite promising results, successful trajectories for IL can be costly to obtain, and directly fine-tuning the LLM as agents can hinder its general abilities. Therefore, this paper addresses these issues by disentangling strategic reasoning from decision-making without compromising the general capabilities of the LLM agent.
%Beyond prompting and fine-tuning, a majority of work also attempts to design modular enhanced LLM agents by integrating functionalities like memory modules and external tools____, or multi-agent systems____ in strategic reasoning scenarios. In the setting of goal-oriented dialogues, PPDPP____ and DAT____ train a separate planner model via RL that predicts dialogue acts to strategize LLMs with dialogue policy planning. ____ employs off-the-shelf game-theorectic algorithms to compute optimal actions to guide the dialogue agent. However, actions (or strategies) predicted in these methods are confined to a finite, pre-defined set, or represented by a continuous vector that cannot be interpretated for decision-making. Instead, our method can handle a dynamic and open-ended action (or strategy) space in natural language, enhancing the transparency and interpretability of decision-making. Concurrent to our work, CoPlanner____ introduces a cooperative multi-agent framework for solving static reasoning problems with a human-crafted meta-strategy pool and maximumly two rounds for planning, whereas our method supports strategic reasoning for dynamic and long-horizon planning.
Motivated by in-context learning and reasoning abilities of LLMs, recent work utilizes LLMs for strategic reasoning in dynamic environment____.
While some work employs prompting techniques for opponent analysis____, theory of mind____, or in-context demonstrations____, their effectiveness is limited by inherit abilities and issues like unfaithful explanations____. 
Fine-tuning LLMs through IL or RL____ can help but struggles to generalize reasoning skills across domains.
Beyond prompting and fine-tuning, modular enhancements such as memory modules, external tools____, and multi-agent systems____ have been explored. Some of these work develop dialogue action planners via RL____ or game-theoretic algorithms____, but rely on finite, predefined action sets or lack interpretability. 
In contrast, our approach supports open-ended action space and improves the interpretability of strategic reasoning by generating strategies in natural language. 
Concurrent to our work, CoPlanner____ focuses on static reasoning with limited planning rounds, whereas our method enhances strategic reasoning for dynamic long-horizon planning.

\paragraph{\textbf{Reinforcement learning for LLMs.}}
%Most prior works utilizing RL for LLMs focus on static problem-solving, such as question answering____, math problems____, and preference alignment____ without further engagement with environments or other agents. For example, methods in reinforcement learning from human feedback (RLHF)____ formulate reward maximization as a one-step bandit problem. This can result in LLMs prioritizing human-like responses rather than engaging in interactions. However, numerous crucial agent tasks cannot be addressed in a single-turn manner, since such approaches fail to develop complex strategies. Therefore, multi-turn RL algorithms____ are designed to endow LLMs with dynamic planning in goal-oriented environments. Our work does not focus on building machinery for RL, but rather considers strategic reasoning as an RL challenge. We aim to demonstrate the feasibility of training LLMs for strategic reasoning with a pure multi-turn RL objective, regardless of the particular RL algorithm employed.
Prior RL applications for LLMs often focus on static problem-solving like question answering____, math problems____, and preference alignment____.
For example, methods in reinforcement learning from human feedback (RLHF)____ treat reward maximization as a one-step bandit problem, which prioritizes human-like responses rather than interactive engagement.
Instead, numerous agent tasks require interactions and complex strategies, prompting the development of multi-turn RL algorithms____ for LLMs. 
Our work considers strategic reasoning as an RL challenge, demonstrating the feasibility of training LLMs for strategic reasoning through a pure RL process, independent of the specific RL algorithm used.