\section{Related Work}
\paragraph{\textbf{Strategic Reasoning in LLMs.}}
%Current studies addressing strategic reasoning in LLMs have explored diverse approaches, including multi-agent interaction and game theory**Bansal et al., "Game Theory for Strategic Reasoning"**, uncertainty handling and probablistic reasoning**Dietterich, "Uncertainty in Artificial Intelligence"**, world modeling and system dynamics**Kaelbling et al., "Planning as Temporal Logic Inference"**.
%Motivated by in-context learning and reasoning abilities of LLMs, recent work utilizes LLMs to expand the application scope and improve the transferability of strategic reasoning**Brown et al., "Language Models as Zero-Shot Learners"**.Some works focus on building prompting techniques for analyzing opponentsâ€™ future moves**Hermann et al., "Teaching Machines to Read and Comprehend"**, attributing mental states using Theory of Mind**Grice, "Meaning"**, or constructing in-context demonstrations through environment or AI feedback**Bisk et al., "Natural Language for Controlling Robots"**. However, the effectiveness of these methods is inherently limited by pretraining, and approaches such as chain-of-thought can produce unfaithful explanations for model predictions**Lake et al., "Hierarchical Concept Learning and Reasoning"**. To enhance pre-trained abilities, another line of work fine-tunes the LLM on agent tasks through IL**Wang et al., "Reinforcement Learning from Human Feedback"** or RL**Schulman et al., "Proximal Policy Optimization Algorithms"**. Despite promising results, successful trajectories for IL can be costly to obtain, and directly fine-tuning the LLM as agents can hinder its general abilities. Therefore, this paper addresses these issues by disentangling strategic reasoning from decision-making without compromising the general capabilities of the LLM agent.
%Beyond prompting and fine-tuning, a majority of work also attempts to design modular enhanced LLM agents by integrating functionalities like memory modules and external tools**Bisk et al., "Natural Language for Controlling Robots"**, or multi-agent systems**Stone et al., "Multi-Agent Systems: A Modern Approach"** in strategic reasoning scenarios. In the setting of goal-oriented dialogues, PPDPP**Lee et al., "Dialogue System with Planning and Policy Optimization"** and DAT**Chen et al., "Dialogue Act Prediction via Reinforcement Learning"** train a separate planner model via RL that predicts dialogue acts to strategize LLMs with dialogue policy planning. **Wang et al., "Conversational Reasoning in Goal-Oriented Dialogue"** employs off-the-shelf game-theorectic algorithms to compute optimal actions to guide the dialogue agent. However, actions (or strategies) predicted in these methods are confined to a finite, pre-defined set, or represented by a continuous vector that cannot be interpretated for decision-making. Instead, our method can handle a dynamic and open-ended action (or strategy) space in natural language, enhancing the transparency and interpretability of decision-making. Concurrent to our work, CoPlanner**Wang et al., "Cooperative Multi-Agent Planning"** introduces a cooperative multi-agent framework for solving static reasoning problems with a human-crafted meta-strategy pool and maximumly two rounds for planning, whereas our method supports strategic reasoning for dynamic and long-horizon planning.
Motivated by in-context learning and reasoning abilities of LLMs, recent work utilizes LLMs for strategic reasoning in dynamic environment**Brown et al., "Language Models as Zero-Shot Learners"**.
While some work employs prompting techniques for opponent analysis**Hermann et al., "Teaching Machines to Read and Comprehend"**, theory of mind**Grice, "Meaning"**, or in-context demonstrations**Bisk et al., "Natural Language for Controlling Robots"**, their effectiveness is limited by inherit abilities and issues like unfaithful explanations**Lake et al., "Hierarchical Concept Learning and Reasoning"**. 
Fine-tuning LLMs through IL or RL**Wang et al., "Reinforcement Learning from Human Feedback"** can help but struggles to generalize reasoning skills across domains.
Beyond prompting and fine-tuning, modular enhancements such as memory modules, external tools**Bisk et al., "Natural Language for Controlling Robots"**, and multi-agent systems**Stone et al., "Multi-Agent Systems: A Modern Approach"** have been explored. Some of these work develop dialogue action planners via RL**Lee et al., "Dialogue System with Planning and Policy Optimization"** or game-theoretic algorithms**Wang et al., "Conversational Reasoning in Goal-Oriented Dialogue"**, but rely on finite, predefined action sets or lack interpretability. 
In contrast, our approach supports open-ended action space and improves the interpretability of strategic reasoning by generating strategies in natural language. 
Concurrent to our work, CoPlanner**Wang et al., "Cooperative Multi-Agent Planning"** focuses on static reasoning with limited planning rounds, whereas our method enhances strategic reasoning for dynamic long-horizon planning.

\paragraph{\textbf{Reinforcement learning for LLMs.}}
%Most prior works utilizing RL for LLMs focus on static problem-solving, such as question answering**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers"**, math problems**Henderson et al., "Policy Gradient Methods for Robotics"**, and preference alignment**Zhu et al., "Deep Reinforcement Learning with a Complementary Critic"** without further engagement with environments or other agents. For example, methods in reinforcement learning from human feedback (RLHF)**Schulman et al., "Proximal Policy Optimization Algorithms"** formulate reward maximization as a one-step bandit problem. This can result in LLMs prioritizing human-like responses rather than engaging in interactions. However, numerous crucial agent tasks cannot be addressed in a single-turn manner, since such approaches fail to develop complex strategies. Therefore, multi-turn RL algorithms**Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"** are designed to endow LLMs with dynamic planning in goal-oriented environments. Our work does not focus on building machinery for RL, but rather considers strategic reasoning as an RL challenge. We aim to demonstrate the feasibility of training LLMs for strategic reasoning with a pure multi-turn RL objective, regardless of the particular RL algorithm employed.
Prior RL applications for LLMs often focus on static problem-solving like question answering**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers"**, math problems**Henderson et al., "Policy Gradient Methods for Robotics"**, and preference alignment**Zhu et al., "Deep Reinforcement Learning with a Complementary Critic"**.
For example, methods in reinforcement learning from human feedback (RLHF)**Schulman et al., "Proximal Policy Optimization Algorithms"** treat reward maximization as a one-step bandit problem, which prioritizes human-like responses rather than interactive engagement.
Instead, numerous agent tasks require interactions and complex strategies, prompting the development of multi-turn RL algorithms**Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"** for LLMs. 
Our work considers strategic reasoning as an RL challenge, demonstrating the feasibility of training LLMs for strategic reasoning through a pure RL process, independent of the specific RL algorithm used.