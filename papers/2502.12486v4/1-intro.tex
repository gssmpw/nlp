\section{Introduction}
%更接近真实世界的语用环境通常需要通过对话来达到目标，并根据目标的达成与否来修正策略。而修正策略的一种有效手段则是通过强化学习在与环境或对手的不断交互中逐步探索出有效的策略，从而达成目标。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\includegraphics[scale=0.46]{Figure/motivation.png}
\caption{\textbf{\textit{EPO} incentivizes goal-directed behavior from LLM agents in interactive scenarios.}
In such scenarios, each participant's goals and strategies remain private to themselves.
Notably, our strategic reasoning model can assist all involved parties, enabling to increase the overall average payoff for all participants.
}
\label{fig:case}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%策略推理是什么（定义），独有的挑战有哪些（与其他推理的区别是啥）
%A remarkable aspect of human intelligence is the ability to accomplish challenging goals through strategic decision-making in complex interactions. Due to their generalist knowledge, LLMs have been utilized to address a wide variety of decision-making problems, such as navigating the embodied environment~\cite{shridhar2021alfworld, o2023open}, the Web~\cite{yao2022webshop, zhou2024webarena}, and goal-oriented social interaction~\cite{zhou2024sotopia, bianchi2024how}. To be successful, strategic reasoning is required for these LLM agents to exhibit interactive and goal-oriented behavior that extends beyond mere simulation.

%Strategic reasoning differentiates itself from other types of reasoning (e.g., multi-step reasoning for math problems) by its inherit dynamism and the uncertainty of the reasoning environment. It necessitates a profound understanding of the evolving context along with the ability to make rational decisions towards achieving specific goals~\cite{zhang2024llm}. A fundamental characteristic of strategic reasoning is the ability to adapt strategies based on the behaviors of others or changes in the environment. For example, in social dialogues, one participant adjusts communicative strategies in response to the actions from others, while dynamically planning ahead to steer the conversation towards its purpose.

%To enhance strategic reasoning in LLMs, recent approaches attempt to leverage the reasoning ability already obtained by pre-trained models via prompt tuning~\cite{yao2023react, gandhi2023strategic, duan-etal-2024-reta}. Although simple to implement, prompting methods such as chain-of-thought~\cite{wei2022chain} and theory-of-mind~\cite{wilf-etal-2024-think} are restricted by the reasoning capabilities inherent to pre-training. Fine-tuning the LLM on domain-specific interaction data~\cite{chen2023fireact, song-etal-2024-trial, putta2024agent} can help, but this can compromise general language capacities due to overoptimization~\cite{lewis-etal-2017-deal, Bakhtin2022HumanlevelPI}. Some research also attempts to design modular enhanced LLM agents~\cite{zhu2023ghost, ge2023openagi, deng2023plug, gemp2024states} or multi-agent systems~\cite{10.5555/3491440.3491941, xu2023exploring, ma2024large} to boost strategic reasoning of a single LLM. However, these methods typically depend upon intricate systems composed of multiple different modules tailored to certain tasks or environments, and the effectiveness of a planning or strategy module is highly impacted by other modules (e.g., memory module, knowledge bases, external tools, etc.). For multi-agent systems, multiple LLM agents directly make decisions independently~\cite{wang2024cooperative}, complicating the process of strategic decision-making by requiring a single agent to assess behaviors of others.

%To address these issues, we introduce the Cooperative Strategic Reasoning Framework (\textit{EPO}) to elicit goal-oriented behavior from LLM agents as they navigate the complexities of social and physical worlds. We achieve this by decomposing the challenge of strategic decision-making into strategic reasoning and decision-making, assigning one LLM to produce creative and flexible strategies for decision-making agents to engage with others or the environment in a goal-oriented manner. Specifically, as shown in Figure~\ref{fig:case}, a concrete strategy (e.g, negotiation strategies) is requested for each turn from a strategic reasoning agent. The strategy is then selectively adopted by decision-making agents (e.g., a seller aiming for higher profits or a buyer hoping for better deals) to act in line with their goals and past interactions. The reasoning agent then evaluates the reactions of decision-making agents and offers subsequent strategies until a goal is achieved or the maximum number of turns is reached.

%To enhance adaptability in strategic reasoning, we facilitate the strategic reasoning agent's capability through multi-turn reinforcement learning (RL). This involves optimizing strategies using a reward-weighted objective that learns from interaction data of differing quality. Importantly, we employ process rewards to emphasize strategy effectiveness rather than just goal completion, and we progressively boost strategic reasoning via iterative self-play RL fine-tuning. Our results show that these optimized strategies can assist various decision-making agents, both open-source and closed-source LLMs, regardless of their capabilities. Guided by the strategies, decision-making agents are able to generate goal-oriented actions while maintaining robust generalization in a variety of scenarios and domains. Further analysis show that the creativity in envisioning novel strategies and the flexibility to pivot when circumstances change in \textit{EPO}, facilitates the discovery of innovative strategies that go beyond traditional tactics recommended by human experts. Our contributions are as follows.
%\begin{itemize}
%    \item We propose a cooperative strategic reasoning framework, \textit{EPO}, that involves a strategic reasoning LLM agent generating flexible and generalizable strategies to elicit goal-oriented behavior from LLMs.
%    \item We employ a multi-turn RL approach for strategy optimization with process rewards and iterative self-play fine-tuning to improve adaptability of the strategic reasoning agent.
%    \item Experimental results in a range of social and physical environments as well as agent configurations demonstrate the superior performance of \textit{EPO} over existing baselines, especially in goal completion, with state-of-the-art performance on social dialogue and web navigation tasks.
%\end{itemize}

Recent advances in LLMs have significantly enhanced their reasoning capabilities on static problem-solving with well-defined solutions, such as mathematics, coding, and logical reasoning~\cite{claude,o1,qwq,gemini,guo2025deepseek}.
However, these tasks, governed by fixed rules and deterministic outcomes, fail to capture the complexity of real-world scenarios such as business negotiations or policy design, where success hinges on navigating dynamic environments with no predefined solutions.
Such scenarios demand \textbf{strategic reasoning}\footnote{See Appendix~\ref{append:challenge} for the comparison between strategic reasoning and static problem-solving.}~\cite{zhang2024llm}: the ability to align long-term goals, manage uncertainty, and adapt to changing conditions.
Despite progress in narrow domains, current LLMs have difficulty in integrating these capabilities, exposing a critical gap in human-like behaviors in interactive contexts.

Prior work improving strategic reasoning in LLMs falls into three categories: iterative prompting that decomposes long-term goals into stepwise plans, such as recursive self-improvement~\cite{madaan2023selfrefine,shinn2024reflexion} or extended chain-of-thought (CoT) reasoning~\cite{wei2022chain,yao2023react}; post-training LLMs through imitation learning (IL) or RL~\cite{chen2023fireact,song-etal-2024-trial}; inference scaling that searches multiple reasoning paths toward goals, such as Best-of-N or Monte Carlo Tree Search (MCTS)~\cite{yu-etal-2023-prompt,he-etal-2024-planning, putta2024agent}.
While these methods show promise, they face critical limitations: (1) prompting methods are limited by the inherent reasoning abilities of LLMs and struggle with real-time adaptation; (2) IL or RL approaches face challenges in generalizing reasoning skills to unseen domains~\cite{patil2025advancing}, as fine-tuning on specific reasoning datasets may hinder transferability due to over-optimization~\cite{Bakhtin2022HumanlevelPI,li2024dialogue}; and (3) search algorithms suffer from computational inefficiency and poor scalability to open-ended action space in real-world scenarios.

To tackle these challenges, we introduce a method concerning \textbf{explicit policy optimization} for strategic reasoning and leverage RL to learn to reason for optimial strategies.
Our method \textit{EPO}, features an LLM dedicated to strategic reasoning, providing real-time strategies and can be seamlessly integrated with LLM agents to incentivize goal-directed behavior.
As shown in Figure~\ref{fig:case}, our reasoning model can assist arbitrary LLM agents in achieving long-term goals across multiple interaction turns.
To enhance adaptability and policy transferability, we optimize the reasoning model through a lightweight multi-turn RL pipeline.
For simplicity and ease of implementation, we employ a REINFORCE~\cite{sutton1999policy} policy gradient RL objective for training, enabling the reasoning model to learn optimal strategies.
Additionally, we utilize a process reward model (PRM) to assess the effectiveness of generated strategies and incorporate iterative self-play to scale up RL training.

Our method, leveraging the features of \textit{EPO}, addresses the limitations of prior work from several perspectives. Firstly, unlike previous multi-agent frameworks that are limited to prompting engineering~\cite{gandhi2023strategic,duan-etal-2024-reta}, \textit{EPO} is capable to enhance strategic reasoning capabilities via RL. This allows our reasoning model to adapt to real-time environment feedback. Moreover, \textit{EPO} allows LLM agents interacting with environments to remain unchanged, preserving their ability to generalize to new domains and avoids over-optimization issues common in IL or RL methods.
Meanwhile, our reasoning model can be seamlessly plugged into these LLM agents, regardless of their openness or inherent capabilities. Finally, the reasoning model, being an LLM, can formulate strategies in the vast, open-ended language space without high computational costs.

Experiments across social and physical domains demonstrate that \textit{EPO} is able to align long-term goals with enhanced strategic reasoning via RL, achieving state-of-the-art performance on social dialogue and web navigation tasks.
Results and analysis show that our strategic reasoning model learns to reason for optimal strategies through multi-turn RL, transferring its policy to diverse scenarios. 
We also discover various collaborative mechanisms between the reasoning model and LLM agents for interacting, and uncover creative strategies produced by the model.
Our contributions are threefold:
\begin{itemize}
    \item We propose explicit policy optimization for strategic reasoning, featuring a strategic reasoning model (LLM) providing real-time strategies for arbitrary LLM agents to incentivize goal-directed behavior in dynamic interactive environments.
    \item We develop a lightweight multi-turn RL pipeline for training the reasoning model with process rewards and iterative self-play, improving its policy transferability to unseen scenarios.
    \item Results and analysis in diverse domains demonstrate the superior performance of \textit{EPO} over baselines in long-term goal alignment.
    A variety of collaborative reasoning mechanisms emerge in \textit{EPO} as well as novel strategies devised by the reasoning model.
\end{itemize}












