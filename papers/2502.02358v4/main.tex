\documentclass[acmtog, screen, nonacm]{acmart}


\usepackage{booktabs} % For formal tables
\newcommand{\eg}{\textit{e.g.}}
\RequirePackage{graphicx}
\let\Bbbk\relax
\usepackage{amssymb}
% TOG prefers author-name bib system with square brackets
\citestyle{acmauthoryear}
%\setcitestyle{nosort,square} % nosort to allow for manual chronological ordering
\usepackage{multirow}
\usepackage{makecell}
\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\usepackage{color} 
% Metadata Information
\acmJournal{TOG}
%\acmVolume{38}
%\acmNumber{4}
%\acmArticle{39}
%\acmYear{2019}
%\acmMonth{7}
\usepackage{pifont}
\usepackage{xspace}
\newcommand{\onedot}{.}
\def\zn#1{{\color{blue}{\bf [ZN:} {\it{#1}}{\bf ]}}}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} \def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} 

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
%\acmDOI{0000001.0000001_2}

% Paper history
%\received{February 2007}
%\received{March 2009}
%\received[final version]{June 2009}
%\received[accepted]{July 2009}


% Document starts
\begin{document}
% Title portion

\title{MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm}

\author{Ziyan Guo}
\affiliation{%
  \institution{Singapore University of Technology and Design}
  \country{Singapore}}
\email{ziyan_guo@mymail.sutd.edu.sg}

\author{Zeyu Hu}
\affiliation{%
  \institution{LightSpeed Studios}
  \country{Singapore}}
\email{zeyuhu@global.tencent.com}

\author{Na Zhao}
\authornote{corresponding author}
\affiliation{%
  \institution{Singapore University of Technology and Design}
  \country{Singapore}}
\email{na_zhao@sutd.edu.sg}

\author{De Wen Soh}
\affiliation{%
  \institution{Singapore University of Technology and Design}
  \country{Singapore}}
\email{dewen_soh@sutd.edu.sg}

% DO NOT ENTER AUTHOR INFORMATION FOR ANONYMOUS TECHNICAL PAPER SUBMISSIONS TO SIGGRAPH 2019!


% \renewcommand\shortauthors{Zhou, G. et al}


\begin{abstract}

Human motion generation and editing are key components of computer vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. 
To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: \textbf{Motion-Condition-Motion}, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion.
Based on this paradigm, we propose a unified framework, \textbf{MotionLab}, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions.
In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks.
Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion.
Our code and additional video results are available at: \href{https://diouo.github.io/motionlab.github.io/}{\textcolor{blue}{Project Website}}.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010371.10010352.10010380</concept_id>
       <concept_desc>Computing methodologies~Motion processing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010371.10010352.10010238</concept_id>
       <concept_desc>Computing methodologies~Motion capture</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010371.10010387.10010866</concept_id>
       <concept_desc>Computing methodologies~Virtual reality</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Motion processing}
\ccsdesc[300]{Computing methodologies~Motion capture}
\ccsdesc[300]{Computing methodologies~Virtual reality}
\keywords{Motion Planning, Motion Control}


\begin{teaserfigure}
  \centering
  \includegraphics[width=\linewidth,page=1]{Figure/demo.pdf}
  \vspace{-7mm}
  \caption{Demonstration of our MotionLab's versatility, performance and efficiency. Previous SOTA refer to multiple expert models, including MotionLCM \cite{dai2025motionlcm}, OmniControl \cite{xie2023omnicontrol}, MotionFix \cite{athanasiou2024motionfix}, CondMDI \cite{cohan2024flexible} and MCM-LDM \cite{Song_2024_CVPR}. All motions are represented using SMPL \cite{loper2023smpl}, where transparent motion indicates the source motion or condition, and the other represents the target motion. \textbf{More qualitative results are available in the website and appendix.}}
  \label{fig:demo}
\end{teaserfigure} 

\maketitle


\input{body}



\end{document}
