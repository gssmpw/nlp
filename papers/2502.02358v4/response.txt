\section{Related Works}
\textbf{Motion Generation and Editing.} Motion generation can be classified based on input conditions. Among these, text-based motion generation is one of the most compelling areas **Kanazawa et al., "Learning to Reconstruct 3D Human Pose and Shape from a Single Color Image"**__, as it trains models to comprehend the semantics of text and generate corresponding pose sequences. To address the fine-grained requirements of practical applications, trajectory-based motion generation has been proposed **Pavlakos et al., "Structured Prediction of 3D Human Pose from Motion Capture Data"**__, where specific motion properties, such as joints reaching designated positions at specified times, are defined. Additionally, motion in-between ____ focuses on generating complete motion sequences given key poses at keyframes. To enable in-place editing of human motion ____, **Peng et al., "Deep Video Manipulation with Dynamic Guidance"**__ introduces text-based motion editing using paired source and target motions. We extend this approach to trajectory-based motion editing by substituting text with joint trajectories. Meanwhile, style plays a crucial role in human motion, leading to motion style-transfer _____. However, the aforementioned methods concentrate solely on specific tasks, rendering them impractical for real-world applications. Moreover, they overlook the intrinsic connections across different human motion tasks and fail to facilitate knowledge sharing among these tasks. In contrast, our unified framework enhances performance on data-scarce editing tasks through multi-task learning.

\noindent\textbf{Unified frameworks for human motion.} There are also some efforts in existing methods that try to unify tasks related to human motion. One line of work ____ focuses on motion understanding, such as motion captioning or describing human motion in images and videos. Yet, these approaches often rely on GPT-like structures, which requires a large amount of training resources and GPU memory. In addition, they fail to provide fine-grained control (\eg, trajectory-based generation and editing) over motion, which is crucial in practical applications. Another line of effort ____ highlights generating motion based on more modalities, such as music and speech. However, these approaches just only integrate more modalities into one model and cannot flexibly edit motion, which can cause them suffering from the multi-task learning and limit their scope of use. The closest to our work are **Alldieck et al., "Learning to Reconstruct 3D Human Pose and Shape from a Single Color Image"** ____ and **Groueix et al., "AtlasNet: A Deep Representation of Non-Rigid Shapes"** ____.

%% here
%While
However, **Alldieck et al. does not support style transfer and precise text-based editing like ``move faster'', and Groueix et al. does not support trajectory-based generation and editing, requiring cumbersome manual adjustments to the attention.
% While Alldieck et al. does not support style transfer and precise text-based editing like ``move faster'', Groueix et al. does not support trajectory-based generation and editing, requiring cumbersome manual adjustments to the attention.