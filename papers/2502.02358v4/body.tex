\section{Introduction}


Human motion is a crucial component of computer vision, with applications spanning game development, film production, and virtual reality \cite{Guo_2022_CVPR,tevet2023human}. 
With the advancements of generative diffusion models \cite{ho2020denoising,song2020denoising,dhariwal2021diffusion}, human motion generation has garnered considerable attention, aiming at generating human motion aligned with the input conditions, such as text \cite{tevet2023human,zhang2022motiondiffuse,tevet2022motionclip} and trajectory (\ie, joints' coordinates) \cite{xie2023omnicontrol,fujiwara2025chronologically,sun2024lgtm,dai2025motionlcm,athanasiou2023sinc,athanasiou2022teach,zhang2023finemogen,song2023loss,sampieri2024length}. On the other hand, in order to modify the motion assets in the industry, significant efforts have been dedicated to motion editing, which intended to modify the properties of prepared motion like motion style transfer \cite{zhong2025smoodi,Song_2024_CVPR,aberman2020unpaired, guo2024generative, jang2022motion}. 

%However, as 
As summarized in Table~\ref{tab:comparison}, current research in this domain mainly develops isolated, task-specific solutions, forcing practitioners to train multiple models for human motion generation and editingâ€”a strategy that is inefficient and impractical. Although several studies \cite{shrestha2025generating,zhou2023ude,zhou2023unified,zhang2025large,yang2024unimumo,luo2024m,fan2024everything2motion} have attempted to unify motion-related tasks, they merely consider different modalities as generation conditions, leading to limited editing capabilities and insufficient fine-grained trajectory control. Moreover, these approaches overlook the intrinsic links between motion generation and editing, thereby hindering potential knowledge sharing. In contrast, a well-designed unified framework can exploit the large volumes of multi-task data to potentially surpass specialist models through effective cross-task representation learning. Motivated by this prospect and inspired by the success of large language models in unifying diverse tasks \cite{achiam2023gpt,dubey2024llama}, we pose the following question: \textit{\textbf{Can human motion generation and editing be effectively unified within a single framework?}}

%To achieve this, it is essential to design an elegant and scalable paradigm. 
In response to this question, we aim to design an elegant and scalable paradigm.
Drawing inspiration from the next-token prediction paradigm \cite{brown2020language,achiam2023gpt}, which has revolutionized the NLP field, we propose a novel paradigm: \textbf{Motion-Condition-Motion}. This paradigm is built upon three concepts -- \textit{source motion}, \textit{condition}, and \textit{target motion}. Similar to how next-token prediction anticipates the subsequent word based on context, the Motion-Condition-Motion paradigm predicts the target motion based on the source motion and specified conditions.
For any human motion generation task, the source motion can be treated as none, and the target motion must align with the provided conditions. For any human motion editing task, the target motion is derived from the source motion based on the conditions. By unifying these tasks within this elegant and scalable paradigm, this framework can be seamlessly extended to various human motion tasks and scaled across diverse datasets. 
Given that human motions are inherently tied to their semantics, trajectories, and styles in practical applications, we aim to unify several key tasks under this framework. These tasks include \textit{text-based motion generation and editing} \cite{tevet2023human,zhang2022motiondiffuse,tevet2022motionclip, athanasiou2024motionfix, goel2024iterative}, \textit{trajectory-based motion generation and editing} \cite{xie2023omnicontrol,dai2025motionlcm}, \textit{motion in-between} \cite{cohan2024flexible,harvey2020robust} and \textit{motion style transfer} \cite{Song_2024_CVPR,zhong2025smoodi}, as illustrated in Figure~\ref{fig:demo}.


\begin{table*}[!t]
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\hline
Method & text-based generation & text-based editing & trajectory-based generation & trajectory-based editing & in-between & style transfer\\ 
\hline
MDM \cite{tevet2023human} & $\checkmark$ &$\times$ &$\times$ &$\times$ & $-$ &$\times$ \\  
MLD \cite{chen2023executing} &$\checkmark$ &$\times$ &$\times$ &$\times$ &$\times$ &$\times$ \\ 
OmniControl \cite{xie2023omnicontrol} &$\checkmark$ &$\times$ &$\checkmark$ &$\times$ & $-$ &$\times$  \\  
MotionFix \cite{athanasiou2024motionfix} & $-$ &$\checkmark$ &$\times$ &$\times$  & $-$ &$\times$ \\ 
CondMDI \cite{cohan2024flexible} &$\checkmark$ &$\times$ &$\checkmark$ &$\times$  &$\checkmark$ &$\times$ \\
MCM-LDM \cite{Song_2024_CVPR} &$\times$ &$\times$ &$\times$ &$\times$  & $-$ &$\checkmark$ \\ 
MotionGPT \cite{jiang2023motiongpt} &$\checkmark$ & $-$ &$\times$  &$\times$ &$\checkmark$ &$\times$ \\ 
MotionCLR \cite{chen2024motionclr} & $\checkmark$ &$-$ &$\times$ &$\times$ & $-$ &$-$ \\
\hline
Ours &$\checkmark$ &$\checkmark$ &$\checkmark$ &$\checkmark$ &$\checkmark$ &$\checkmark$ \\ \hline  
\end{tabular}
}
\caption{Summary of different methods focusing on motion generation and editing. $\checkmark$ indicates that the method has been trained for the task, $\times$ indicates that the method has not been trained, and $-$ indicates that the method has not been trained but can be implemented in a zero-shot manner.}
\label{tab:comparison}
\vspace{-5mm}
\end{table*}

Despite the proposed paradigm, several significant challenges remain in balancing versatility, performance, and efficiency: 
1) Unifying various tasks inevitably introduces additional modalities, while each modality may involve multiple tasks. A naive solution, like adopting multiple cross-attention mechanisms for each task in generation-unified frameworks \cite{fan2024everything2motion,zhang2025large}, is suboptimal. 
2) More sampling time is required for certain tasks (\eg, trajectory-based motion generation and motion in-between \cite{xie2023omnicontrol,zhong2025smoodi}), as existing methods in these areas involve task-specific posterior guidance \cite{chung2022diffusion} during inference to improve conditional guidance.
3) Time asynchrony between the source motion and target motion may arise due to the limited scale of the paired editing dataset and the use of implicit positional encoding \cite{chen2023executing, xie2023omnicontrol,athanasiou2024motionfix,cohan2024flexible}.
4) Most importantly, naively integrating various motion generation and editing tasks into a single framework could lead to task conflicts and catastrophic forgetting, impairing the framework's overall performance.

To address these challenges, we propose a novel generative framework, named \textbf{MotionLab}, built upon our designed \textbf{MotionFlow Transformer (MFT)} as shown in Figure~\ref{fig:illustration}. Inspired by MM-DiT \cite{esser2024scaling}, our MFT also leverages rectified flows \cite{liu2022flow,lipman2022flow}, but we utilize them to map source motion to target motion based on specified conditions. Unlike MM-DiT that focuses exclusively on text and images, our MFT incorporates multiple modalities: source motion, target motion, text, trajectory, and style. In MFT, each modality is allocated a dedicated modality path and fully interacts with the others through joint attention, which enables MFT to enhance conditional generation and editing without requiring task-specific modules or posterior guidance for certain tasks. To ensure temporal synchronization between source motion and target motion, we incorporate an Aligned ROtational Position Encoding (\textbf{Aligned ROPE}) into MFT, explicitly aligning tokens in corresponding frames between the source and target motion. Additionally, to adapt one modality to different tasks, we propose \textbf{Task Instruction Modulation}, which distinguishes different tasks for each modality by introducing an additional task embedding into the MFT. To harmoniously integrate diverse tasks, we propose a curriculum-inspired training strategy termed \textbf{Motion Curriculum Learning} based on the easy-to-hard training principle. Intuitively, tasks involving fewer modalities or more explicit conditional information (\eg, source motion) present lower complexity and are therefore prioritized in the learning sequence.

In this paper, motion-related tasks are decomposed into combinations of \textbf{modalities} through the Motion-Condition-Motion paradigm. These \textbf{modalities} are subsequently represented via each modality paths within the MFT, learning cross-modal interactions through the joint attention, while adapting individual \textbf{modalities} to different tasks through Task Instruction Modulation. By implementing a curriculum learning from single to multiple, from simple (\eg, source motion and trajectory) to complex (\eg, text and style) \textbf{modalities}, the spatial knowledge inherent in 3D representations can be effectively transferred to more abstract \textbf{modalities} since the modalities of the former can represent the latter.
Through these designs, we validate MotionLab on multiple benchmarks, demonstrating superior versatility, performance, and efficiency compared to baselines across various human motion generation and editing tasks.

\section{Related Works}

\textbf{Motion Generation and Editing.} Motion generation can be classified based on input conditions. Among these, text-based motion generation is one of the most compelling areas \cite{zhang2022motiondiffuse,tevet2022motionclip,tevet2023human,Guo_2022_CVPR,chen2023executing,guo2024momask,guo2022tm2t,wang2024motiongpt,jiang2023motiongpt,kim2023flame,lin2023motion,lu2023humantomato,plappert2016kit,zhang2023generating,guo2020action2motion,petrovich2021action}, as it trains models to comprehend the semantics of text and generate corresponding pose sequences. To address the fine-grained requirements of practical applications, trajectory-based motion generation has been proposed \cite{karunratanakul2023guided,shafir2023human,xie2023omnicontrol,dai2025motionlcm,zhang2023finemogen}, where specific motion properties, such as joints reaching designated positions at specified times, are defined. Additionally, motion in-between \cite{tevet2023human,jiang2023motiongpt,cohan2024flexible,qin2022motion,pinyoanuntapong2024mmm} focuses on generating complete motion sequences given key poses at keyframes. To enable in-place editing of human motion \cite{goel2024iterative,athanasiou2024motionfix}, MotionFix \cite{athanasiou2024motionfix} introduces text-based motion editing using paired source and target motions. We extend this approach to trajectory-based motion editing by substituting text with joint trajectories. Meanwhile, style plays a crucial role in human motion, leading to motion style-transfer \cite{jang2022motion,aberman2020unpaired,zhong2025smoodi,Song_2024_CVPR}. However, the aforementioned methods concentrate solely on specific tasks, rendering them impractical for real-world applications. Moreover, they overlook the intrinsic connections across different human motion tasks and fail to facilitate knowledge sharing among these tasks. In contrast, our unified framework enhances performance on data-scarce editing tasks through multi-task learning.

\noindent\textbf{Unified frameworks for human motion.} There are also some efforts in existing methods that try to unify tasks related to human motion. One line of work \cite{jiang2023motiongpt,jiang2025motionchain,zhou2024avatargpt,li2024unimotion,wang2024motiongpt,ling2024motionllama,wu2024motionllm,luo2024m,athanasiou2024motionfix} focuses on motion understanding, such as motion captioning or describing human motion in images and videos. Yet, these approaches often rely on GPT-like structures, which requires a large amount of training resources and GPU memory. In addition, they fail to provide fine-grained control (\eg, trajectory-based generation and editing) over motion, which is crucial in practical applications. Another line of effort \cite{shrestha2025generating,zhou2023ude,zhou2023unified,zhang2025large,yang2024unimumo,luo2024m,fan2024everything2motion,alexanderson2023listen} highlights generating motion based on more modalities, such as music and speech. However, these approaches just only integrate more modalities into one model and cannot flexibly edit motion, which can cause them suffering from the multi-task learning and limit their scope of use. The closest to our work are FLAME \cite{kim2023flame} and MotionCLR \cite{chen2024motionclr}.
%% here
%While
However, FLAME does not support style transfer and precise text-based editing like ``move faster'', and MotionCLR does not support trajectory-based generation and editing, requiring cumbersome manual adjustments to the attention.
% While FLAME does not support style transfer and precise text-based editing like ``move faster'', MotionCLR does not support trajectory-based generation and editing, requiring cumbersome manual adjustments to the attention.

\section{PRELIMINARY: Rectified Flows}

\begin{figure}[!h]
  \centering
  \includegraphics[width=\linewidth,page=1]{Figure/comparison.pdf}
  \caption{Demonstration of the difference trajectory between diffusion models and rectified flows. This difference lies in that the trajectory of diffusion models is based on $x_t = \sqrt{(1-\overline{\alpha_t})} x_0 + \sqrt{\overline{\alpha_t}} \epsilon$, while the trajectory of rectified flows is based on $x_t = (1-t)x_0 + tx_1$. This distinction leads to more robust learning by maintaining a constant velocity, contributing to model's efficiency \cite{zhao2024flowturbo}. }
  \label{fig:comparison}
\end{figure} 

\label{sec:preliminaries}
Flow-based methods \cite{liu2022flow,lipman2022flow,esser2024scaling,ma2024sit,fei2024flux,polyak2024moviegencastmedia} have recently received significant attention due to their generalizability and efficiency compared to diffusion models. Specifically,
% Flow matching methods \cite{liu2022flow,lipman2022flow,esser2024scaling,ma2024sit,fei2024flux,polyak2024moviegencastmedia} have recently received significant attention due to their generalizability and efficiency compared to diffusion models. Specifically,
these methods directly regress the transport vector field between noise distribution $p_1$ and data distribution $p_0$ with the straightest possible trajectories and sample by the corresponding ordinary differential equation (ODE) \cite{wang2024frieren}. Among these methods, rectified flows \cite{liu2022flow,lipman2022flow} aim to learn a trajectory from noise $x_0$ to data $x_1$, which can be formulated as $x_t = \varphi(x_0,x_1,t)$, and the velocity field $v_t$ of the trajectory $x_t$ can be defined by:
\begin{align}
& v_t = \frac{dx_t}{dt} = \frac{\partial\varphi_t(x_0,x_1,t)}{\partial t}, t\in[0,1]
\end{align}
Once we have learned this velocity field $v_t$, we can get $x_0$ from any $x_1$ by numerically integrating. Hence, rectified flows $v_\theta$ are trained to predict $v_t$ by given $x_t$ and $t$, and the training object of rectified flows can be represented as:
\begin{align}
&\mathcal L_{RF}(\theta) = \int^1_0 \mathbb{E}_{(x_0,x_1)\sim(p_0,p_1)}[||v_\theta(t, x_t)-v_t||^2_2]dt
\end{align}
Since the trajectory $x_t$ from $p_1$ to $p_0$ should be as straight as possible, it can be reformulated as the linear interpolation between $x_0$ and $x_1$, and the velocity field $v_t$ can be treated as a constant, namely:
\begin{align}
& x_t = (1-t)x_0 + tx_1 \\
& v_t = x_1 - x_0
\end{align}
Therefore, the training objective can be reformulated as:
\begin{align}
&\mathcal L_{RF}(\theta) = \int^1_0 \mathbb{E}_{(x_0,x_1)\sim(p_0,p_1)}[||v_\theta(t, x_t)-(x_1-x_0)||^2_2]dt
\end{align}
After the training of rectified flows is completed, the transfer from $x_1$ to $x_0$ can be described via the numerical integration of ODE:
\begin{align}
& x_{t-\frac{1}{N}} = x_t + \frac{1}{N}v_\theta(t,x_t)
\end{align}
where $N$ is the discretization number of the interval [0,1].


\section{Motion-Condition-Motion} \label{sec:mcm}
To unify the tasks of human motion generation and editing in an elegant and scalable paradigm, we propose the paradigm of Motion-Condition-Motion. As shown in Table~\ref{tab:paradigm}, all these tasks are unified by three concepts: \textit{source motion}, \textit{condition}, and \textit{target motion}. 

\noindent\textbf{Motion Generation}. For the motion generation tasks, including \textit{text}/\textit{trajectory-based} generation and \textit{motion in-between}, the source motion can be treated as none, with the target motion aligning to the corresponding conditions. For instance, in \textit{text-based generation}, the generated motion should align with the semantics of the provided text, such as ``karate kick'' illustrated in Figure~\ref{fig:demo}. \textit{Masked reconstruction}, as a specific motion generation task, requires the target motion to align with the masked source motion in the specified frames without relying on additional conditions.
Notably, the \textit{unconditional generation} (given zero frames) and \textit{reconstruction} (given all frames) are special cases of masked reconstruction, thus these three tasks can share the same task instruction as described in Section~\ref{sec:tsim}.


\noindent\textbf{Motion Editing}. For motion editing, the source motion must be provided, and the target motion is derived from the source motion based on the specified conditions. In the case of \textit{text-based motion editing}, the generated motion should originate from the source motion, with modifications applied only to the specified parts as dictated by the provided text, such as ``use the opposite leg''. For \textit{trajectory-based editing}, the source motion should be aligned with the given joints' coordinates, ensuring that the specified joints in the source motion are accurately moved to the designated positions within the specified frames. In \textit{motion style transfer}, the generated motion should adopt the style of the style motion while preserving the semantics of the source motion.


\noindent\textbf{Remarks.} In particular, \textit{trajectory-based motion generation} and \textit{motion in-between} are highly similar, as they both aim to ensure that specific joints reach designated positions at specific times. %The primary distinction between them 
Their primary difference is that the former is sparse in space (\ie, joints) but dense in time, whereas the latter is dense in space (\ie, joints) but sparse in time. To efficiently share the parameters and learned representations between the two tasks, we unify their conditions into a single condition. Meanwhile, \textit{masked reconstruction} is also similar to these two tasks. However, while these two tasks only include the coordinates of joints, the source motion also encompasses the velocity and angular velocity of joints. Therefore, they represent different modalities, and masked reconstruction constitutes a distinct task.

%\section{Method}
\section{MotionLab}
\label{sec:method}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=\linewidth,page=1]{Figure/method.pdf}
  \vspace{-5mm}
  \caption{Illustration of our MotionLab and the detail of its MotionFlow Transformer (MFT).}
  \label{fig:illustration}
  \vspace{-4mm}
\end{figure*} 

\begin{table}[!t]
\resizebox{\linewidth}{!}{
\begin{tabular}{cccc}
\hline
Task & Source Motion & Condition & Target Motion\\ \hline
unconditional generation  & $\emptyset$ &$\emptyset$ & $\checkmark$ \\
masked reconstruction & masked source motion & $\emptyset$ & source motion \\
reconstruction & complete source motion & $\emptyset$ & source motion \\
\hline
text-based generation & $\emptyset$ &text & $\checkmark$ \\
trajectory-based generation & $\emptyset$ &text/joints' coordinates & $\checkmark$ \\
motion in-between &  $\emptyset$ &text/poses in keyframes & $\checkmark$ \\ \hline
text-based editing & $\checkmark$ &text & $\checkmark$ \\
trajectory-based editing & $\checkmark$ &text/joints' coordinates & $\checkmark$ \\
style transfer  & $\checkmark$ &style motion & $\checkmark$ \\ 
\hline
\end{tabular}
}
%\caption{The paradigm of Motion-Condition-Motion.}
\caption{Structuring human motion tasks within our Motion-Condition-Motion paradigm.}
\vspace{-8mm}
\label{tab:paradigm}
\end{table}


Based on our proposed Motion-Condition-Motion paradigm, we introduce a unified framework named \textbf{MotionLab}, as illustrated in Figure~\ref{fig:illustration}(a). The core of MotionLab is the \textbf{MotionFlow Transformer (MFT)} (Sec.~\ref{sec:mft}), inspired by MM-DiT \cite{esser2024scaling}, which leverages rectified flow to map source motion $M_S \in \mathbb{R}^{N\times D}$ to target motion $M_S \in \mathbb{R}^{N\times D}$ based on the corresponding condition $C$ for each task.

To enable task differentiation, we propose \textbf{Task Instruction Modulation} (Sec.~\ref{sec:tsim}), where a task-specific instruction $I\in\mathbb{R}^{1\times768}$ extracted from the CLIP \cite{radford2021learning} is also input into MFT alongside $M_S$, $M_T$, and $C$. At each timestep $t$, MFT is trained to predict velocity field $v_t$, which is derived via linear interpolation between target motion $M_T$ and Gaussian noise $\epsilon \in \mathbb{R}^{N\times D}$. 

For effective multi-task training, we adopt \textbf{Motion Curriculum Learning} (Sec.~\ref{sec:mcl}) which organizes tasks hierarchically to facilitate learning. Once trained, MotionLab can map $M_S$ to $M_T$ based on the specified $C$, by predicting $v_t$ in descending order of timestep $t$ as described in Sec.~\ref{sec:ucfg}.


% \textcolor{red}{Now, we have introduced the generative rectified flow and established the paradigm of Motion-Condition-Motion. Here, we propose our framework, MotionLab as illustrated in Figure~\ref{fig:illustration} (a), to implement this paradigm through the rectified flow. To this end, we propose MotionFlow Transformer (MFT) inspired by MM-DiT \cite{esser2024scaling}. For each task, corresponding task instruction $I\in\mathbb{R}^{1\times768}$ extracted from the CLIP \cite{radford2021learning}, conditions $C$, source motion $M_S \in \mathbb{R}^{N\times D}$ are input to the MFT. Upon these modalities, according to the timestep $t$, the MFT is trained to predict velocity field $v_t$ obtained by linear interpolation between target motion $M_S \in \mathbb{R}^{N\times D}$ and Gaussian noise $\epsilon \in \mathbb{R}^{N\times D}$. After the Motion Curriculum Learning in Section~\ref{sec:mcl}, the MFT is able to mapping the $M_S$ to $M_C$ based on specified $C$, by predicting $v_t$ in descending order of timestep $t$ as in Section~\ref{sec:ucfg}.} 

% We represent the features of all modalities as tokens for the attention mechanism \cite{vaswani2017attention}. Specifically, source motion and target motion are represented as $M_S \in \mathbb{R}^{N\times D}$ and $M_T \in \mathbb{R}^{N\times D}$, and we first ignore timestep $t$ here. For the instruction, it is represented as $I\in\mathbb{R}^{1\times768}$ extracted from the CLIP \cite{radford2021learning}. For available conditions $C$, the text is represent as $p\in\mathbb{R}^{77\times768}$ extracted from the last hidden layer of CLIP, the trajectory is represented as $h\in \mathbb{R}^{N\times J\times3}$, and the style is represented as $s\in\mathbb{R}^{1\times512}$ extracted from \citeauthor{zhong2025smoodi}.

\vspace{-1mm}
\subsection{MotionFlow Transformer} \label{sec:mft}
As shown in the Figure~\ref{fig:illustration} (b), MotionFlow Transformer contains three key components: \textit{Joint Attention} to interact tokens from different modalities; \textit{Modality Path} for distinguishing tokens from different modalities and extracting their representations, and \textit{Aligned ROPE} for position encoding of modalities with time information.

\noindent\textbf{Joint Attention.} 
We first adopt
%focus on 
the joint attention mechanism \cite{esser2024scaling}, through which tokens from different modalities can interplay with each other. Specifically, all these tokens will be projected to the query, key, and value representations, and then will be concatenated into a sequence of orderly tokens. Subsequently, these orderly tokens are applied by the attention operation, whose output is again split into corresponding token of different modalities.

\noindent\textbf{Modality Path.} While the joint attention is able to interact tokens from different modalities, there is still need to differentiate different tokens. In addition to the QKV projection and FeedForward Network (FFN) in the attention mechanism, %based on the 
as used in MM-DiT, our MFT incorporates the adaptive Layer Normalization (adaLN) and a modulation mechanism \cite{peebles2023scalable} for each modality, %contributing to the conditional generation and editing. 
enhancing conditional generation and editing capabilities.

\noindent\textbf{Aligned Rotational Position Encoding.} 
Considering that the use of absolute position encoding in existing methods \cite{chen2023executing} can weaken the temporal alignment between source motion and target motion due to the limited scale of paired datasets, we adopt a relative position encoding method, ROtational Position Encoding (ROPE) \cite{su2024roformer}. ROPE explicitly embeds the relative distances between tokens, preserving temporal relationships more effectively. Instead of naively applying a 3-dimensional ROPE to distinguish source motion, target motion, and conditions with time information (\eg, trajectory), we propose Aligned ROPE, which encodes these components with appropriate temporal information using a 1-dimensional ROPE. This design avoids the confusion caused by 3-dimensional ROPE, where distances between tokens within a modality can interfere with cross-modality relationships, ensuring better alignment and representation.
%Existing methods \cite{chen2023executing} tend to use absolute positions to encode the tokens of the motion. However, this naive approach can weaken the temporal alignment of source motion and target motion due to the limited scale of the paired dataset. To this end, we apply a relative position encoding, namely ROtational Position Encoding (ROPE) \cite{su2024roformer}, which explicitly embeds the relative distance of each token to other tokens. Considering this, another naive solution is to adopt 3-Dimensional ROPE to distinguish source motion, target motion, and conditions with time information (\eg, trajectory). However, the distance scale between modalities will confuse the distances between tokens within a modality. Consequently, it is promising to embed the source motion, target motion, and conditions with appropriate time information using 1-Dimensional ROPE, namely Aligned ROPE.
%Existing methods \cite{chen2023executing} tend to use absolute positions to encode the tokens of the motion. However, this naive approach can weaken the temporal alignment of source motion and target motion due to the limited scale of the paired dataset. To this end, we apply a relative position encoding, namely ROtational Position Encoding (ROPE) \cite{su2024roformer}, which explicitly embeds the relative distance of each token to other tokens. Considering this, another naive solution is to adopt 3-Dimensional ROPE to distinguish source motion, target motion, and conditions with time information (\eg, trajectory). However, the distance scale between modalities will confuse the distances between tokens within a modality. Consequently, it is promising to embed the source motion, target motion, and conditions with appropriate time information using 1-Dimensional ROPE, namely Aligned ROPE.

\subsection{Task Instruction Modulation} \label{sec:tsim}
MM-DiT implements a %sophisticated
modulation mechanism that enhances text-to-image generation through the incorporation of textual embeddings (\eg, ``a photo of dog") as modulation signals. However, within our unified framework, various tasks necessitate the integration of multiple modalities, and critically, identical modalities may require distinct representational forms across different tasks. This complexity renders approaches such as learned task tokens (\eg, [TASK]) or one-hot encoding vectors inadequate for managing arbitrary numbers and combinations of modalities.
% MM-DiT implements a sophisticated modulation mechanism that enhances text-to-image generation through the incorporation of textual embeddings (e.g., "a photo of dog") as modulation signals. However, within our unified framework, various tasks necessitate the integration of multiple modalities, and critically, identical modalities may require distinct representational forms across different tasks. This complexity renders simplistic approaches such as learned task tokens (e.g., [TASK]) or one-hot encoding vectors inadequate for managing arbitrary numbers and combinations of modalities.

%% here
Recognizing the inherent flexibility of natural language, we leverage textual representations acquired by foundation models (\eg, CLIP) to effectively differentiate identical modalities across disparate tasks. For instance, we utilize the textual embedding of ``edit source motion by given style" to facilitate the adaptation of source motion to style transfer operations. This approach, while conceptually straightforward, provides remarkable effectiveness in enhancing system flexibility and scalability, thereby enabling seamless extension to diverse tasks involving multiple modalities.

\vspace{-2mm}
\subsection{Motion Curriculum Learning} \label{sec:mcl}
To achieve effective multi-task learning and facilitate knowledge sharing between tasks, we propose an easy-to-hard hierarchical training strategy inspired by curriculum learning \cite{bengio2009curriculum}. Specifically, new tasks are sequentially introduced into the training based on their difficulty , guided by the following assumptions: 1) The fewer modalities a task involves, the simpler the task; 2) Editing tasks are easier than generating tasks, as only the conditional difference between source motion and target motion needs to be learned; 3) The more specific the conditional information (\eg, source motion) provided, the simpler the task becomes. The importance of these three criteria decreases in order. Guided by the easy-to-hard training principle, the training process in MotionLab is divided into two stages: \textit{self-supervised pre-training} and \textit{supervised fine-tuning}.

\noindent\textbf{Pre-training.} Intuitively, the reconstruction of masked source motion is the easiest task. Hence, we first train the model based on the masked source motion, independent of the conditions. This approach allows the model to learn prior motion representations independent of conditions, thereby generalizing to different tasks. Following MoMask \cite{guo2024momask}, we randomly masking from zero frames to all frames. This flexible strategy provides tasks of varying difficulty levels, avoiding overfitting on simple tasks (all frames) and mode collapse on difficult tasks (zero frames). Furthermore, this strategy seamlessly performs source motion reconstruction (\ie, all frames) and unconditional training (\ie, zero frames), which is crucial for CFG. However, MoMask have to mask the coordinates and velocities of all joints in one frame simultaneously due to its discrete tokens. Therefore, we here extend the masked pre-training to randomly mask joints' trajectories (not joints' velocities for the user usage) for boosting the understanding of in-between and trajectory-based tasks. Specifically, we pre-train MotionLab using these three tasks (\ie, masked source motion reconstruction, trajectory-based generation without text and in-between without text) for 1,000 epochs.

\noindent\textbf{Fine-tuning.} 
In the supervised fine-tuning stage, we train MotionLab on tasks in an easy-to-hard sequence. Specifically, a new task is introduced into training every 200 epochs in the following order: \ding{192} text-based generation, \ding{193} style-based generation (an auxiliary task for training the modality path of the style, not our primary goal), \ding{194} trajectory-based editing (without text), \ding{195} text-based editing, \ding{196} style transfer,  \ding{197} motion in-between and trajectory-based generation, \ding{198} trajectory-based editing. 
This progressive learning strategy ensures effective adaptation and knowledge sharing across tasks.
Particularly, \ding{192} and \ding{193} are the simplest task because they only include one modality, whereas others include at least two modalities. Among tasks involving two modalities, \ding{194}, \ding{195}, and \ding{196} take priority over \ding{197} since they are editing tasks. Additionally, as text is less specific than trajectory but more specific than style,
the order is \ding{194}, \ding{195}, and \ding{196}.

To mitigate catastrophic forgetting, previous tasks are trained with new tasks, based on the probability derived from the FID of the last evaluation. However, the FID scales for different tasks vary due to their differing difficulty levels. Consequently, we use the percentage change compared to the previous evaluation as the probability, which encourages the model to re-learn forgotten tasks or tasks that it has not yet fully mastered. To support classifier-free guidance, we also train the model to unconditionally generate and reconstruct the complete source motion. Empirically, in this stage, a 5\% probability is allocated for unconditional generation, 5\% for reconstructing the complete source motion, 45\% for previous tasks, and 45\% for the new task.

% In summary, this training strategy, transitioning from a single modality to multiple modalities, enables the model to progressively learn each modality's representation independently before capturing interactions between multiple modalities, offering three key benefits: 1) adaptability to various tasks; 2) seamless support for CFG during inference; and 3) flexible training management.

In summary, this training strategy has three main advantages: 1) it enables our framework to adapt to various tasks; 2) it seamlessly supports CFG during inference; 3) it allows flexible management of the training process to avoid retraining due to errors. Meanwhile, this training strategy, from single modality to multiple modalities, can be considered as first learning the representation of each modality separately, and then learning the representation of the interaction between multiple modalities, which can be distinguished by the Task Instruction Modulation. 
Furthermore, by prioritizing the introduction of spatial conditions (\ie, source motion and trajectory, this strategy can share the model's understanding between them and abstract conditions (\ie, text and style), as the latter conditions can be represented by the former to a certain extent.

% In summary, this training strategy has three main advantages: 1) it enables our framework to adapt to various tasks; 2) it seamlessly supports CFG during inference; 3) it allows flexible management of the training process to avoid retraining due to errors. Meanwhile, this training strategy, from single modality to multiple modalities, can be considered as first learning the representation of each modality separately, and then learning the representation of the interaction between multiple modalities, which can be distinguished by the Task Instruction Modulation. 
% \textcolor{red}{Furthermore, by prioritizing the introduction of spatial conditions (i.e., source motion, trajectory, and in-between), this strategy can share the model's understanding between them and abstract conditions (i.e., text and style), as the latter conditions can be represented by the former to a certain extent.}

\subsection{MotionLab Inference} \label{sec:ucfg}
During inference, Classifier-Free Guidance (CFG) \cite{ho2022classifier} is incorporated for both motion generation and motion editing to boost sampling quality and align conditions and target motion. 
%After the training of rectified flow $v_\theta$ based on the mentioned framework, to boost sampling quality and align conditions and target motion, Classifier-Free Guidance (CFG) \cite{ho2022classifier} is incorporated for both motion generation and motion editing during the inference. 
%In particular, the unified framework allows the inference formulas for generating and editing to be unified separately.

For all motion generation tasks, we generate target motion $M_T$ with the guidance of arbitrary conditions $C$:
\begin{align}
v_\theta(M_T,t,C) = v_\theta(M_T|t,\emptyset)+\lambda_C[v_\theta(M_T|t,C)-v_\theta(M_T|t,\emptyset)]
\end{align}
where $t$ is the timestep and $\lambda_C>1$ is a hyper-parameter to control the strength of corresponding conditional guidance.

For all motion editing tasks, which aim to modify the source motion based on the condition. Hence, we generate the target motion $M_T$ with source motion $M_S$ first and then condition $C$:
\begin{align}
\notag
v_\theta(M_T,t,M_S,C) = & v_\theta(M_T|t,\emptyset,\emptyset) +\lambda_S[v_\theta(M_T|t,S,\emptyset)-v_\theta(M_T|t,\emptyset,\emptyset)] \nonumber \\ 
&+\lambda_C[v_\theta(M_T|t,S,C)-v_\theta(M_T|t,S,\emptyset)]
\end{align}
where $\lambda_S>1$ is a hyper-parameter to control the strength of source motion guidance.

\begin{table}[]
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\hline
Method &  FID$\downarrow$ & R@3$\uparrow$  & Diversity$\rightarrow$ & \makecell{MM \\Dist$\downarrow$} & MModality$\uparrow$ & AITS$\downarrow$ \\ \hline
GT & 0.002 & 0.797 & 9.503 & 2.974 & 2.799 & - \\ \hline
T2M \cite{Guo_2022_CVPR} & 1.087 & 0.736 & 9.188 & 3.340 & 2.090 & \textbf{0.040} \\
MDM \cite{tevet2023human} & 0.544 & 0.611 & \textbf{9.559} & 5.566 & \underline{2.799} & 26.04 \\
MotionDiffuse \cite{zhang2022motiondiffuse} & 1.954 & 0.739 & 11.10  & 2.958 & 0.730 & 15.51 \\
MLD \cite{chen2023executing} & 0.473 & 0.772 & 9.724  & 3.196 & 2.413 & 0.236 \\
T2M-GPT\cite{zhang2023generating} & \textbf{0.116} & 0.775 & 9.761 & 3.118 & 1.856 & 11.24 \\ 
MotionGPT \cite{jiang2023motiongpt} & 0.232 & 0.778 & \underline{9.528}  & 3.096 & 2.008 & 1.240 \\
CondMDI \cite{cohan2024flexible} & 0.254 & 0.6450 & 9.749 & - & - & 57.25 \\
MotionLCM \cite{dai2025motionlcm} & 0.304 & 0.698 & 9.607 & 3.012 & 2.259 & \underline{0.045} \\
% LMM \cite{zhang2025large} & 0.138 & 0.802 & 9.573 & 2.971 & 2.426 & - \\
MotionCLR \cite{chen2024motionclr} & 0.269 & \textbf{0.831} & 9.607 & \textbf{2.806} & 1.985 & 0.830 \\
\hline
\textbf{Ours} & \underline{0.167} & \underline{0.810} & 9.593 & \underline{2.830} & \textbf{2.912} & 0.068 \\
\hline
\end{tabular}
}
\caption{Evaluation of text-based motion generation on HumanML3D\cite{Guo_2022_CVPR} dataset. The models in bold are the optimal models, and the models in underline are the sub-optimal models. }
\label{tab:text}
\end{table}

\begin{table}[]
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccccc}
\hline
Method & Joints & FID$\downarrow$ & R@3$\uparrow$  & Diversity$\rightarrow$ & \makecell{Foot skate\\ratio$\downarrow$} & \makecell{Average\\Error$\downarrow$} & AITS$\downarrow$ \\ \hline
GT & - & 0.002 & 0.797 & 9.503 & 0.000 & - & - \\ \hline
GMD \cite{karunratanakul2023guided} & pelvis & 0.576 & 0.665 & 9.206 & 0.101 & \underline{0.1439} & 137.0 \\
PriorMDM \cite{shafir2023human} & pelvis & 0.475 & 0.583 & 9.156 & - & 0.4417 & 19.83 \\
OmniControl \cite{xie2023omnicontrol} & pelvis & \underline{0.212} & 0.678 & 9.773 & \underline{0.057} & 0.3226 & 39.78 \\
MotionLCM \cite{dai2025motionlcm} & pelvis & 0.531 & \textbf{0.752} & \underline{9.253} & - & 0.1897 & \textbf{0.035} \\
\textbf{Ours} & pelvis & \textbf{0.095} & \underline{0.740} & \textbf{9.502} & \textbf{0.007} & \textbf{0.0286} & \underline{0.133} \\ \hline
OmniControl  \cite{xie2023omnicontrol} & all & 0.310 & 0.693 & 9.502 & 0.061 & 0.0404 & 76.71 \\
\textbf{Ours} & all & 0.126 & 0.765  & 9.554 & 0.002 & 0.0334 & 0.134 \\
\hline
\end{tabular}
}
\caption{Evaluation of trajectory-based motion generation on HumanML3D \cite{Guo_2022_CVPR} dataset.}
\label{tab:trajectory}
\end{table}


\begin{table}[]
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Condition} & \multicolumn{4}{c}{generated-to-target retrieval} & \multirow{2}{*}{\makecell{Average\\Error$\downarrow$} } &  \multirow{2}{*}{AITS $\downarrow$}  \\ \
& & R@1$\uparrow$ & R@2$\uparrow$ & R@3$\uparrow$ & AvgR $\downarrow$ & &   \\ \hline
GT  & - & 73.15 & 84.09 & 89.49 & 2.09 & - & - \\ \hline                     
TMED$^*$ \cite{athanasiou2024motionfix} & text  & 38.69 & 50.61 & 62.23 & 4.15 & - & 26.57 \\ 
\textbf{Ours} & text  & 56.34 & 70.40 & 77.24 & 3.54 & - & 0.16 \\ \hline

TMED$^*$ \cite{athanasiou2024motionfix} & trajectory  & 60.01 & 73.33 & 82.69 & 2.67 & 0.129 & 30.56 \\ 
\textbf{Ours} & trajectory  & 72.65 & 82.71 & 87.89 & 2.20 & 0.027 & 0.19 \\ \hline
\end{tabular}
}
\caption{Evaluation of text-based and trajectory-based motion editing on MotionFix \cite{athanasiou2024motionfix} dataset. TMED$^*$ mean that we re-implement the models since original models are trained on the skeleton of SMPL format, while our models are trained on HumanML3D format.}
\label{tab:edit}
\end{table}

% \begin{table}[]
% \resizebox{\linewidth}{!}{
% \begin{tabular}{ccccccc}
% \hline
% Method & Frames & FID$\downarrow$ & \makecell{R-precision\\Top-3$\uparrow$}  & Diversity$\rightarrow$ &  \makecell{Foot skating\\ratio$\downarrow$} & \makecell{Keyframe\\error$\downarrow$} \\ \hline
% CondMDI \cite{cohan2024flexible} & 1 & 0.1551 & 0.6787 & 9.5807 & 0.0936 & 0.3739 \\ 
% CondMDI \cite{cohan2024flexible} & 5 & 0.1731 & 0.6823 & 9.3053 & 0.0850 & 0.1789 \\
% CondMDI \cite{cohan2024flexible} & 20 & 0.2253 & 0.6821 & 9.1151 & 0.0806 & 0.0754 \\ \hline
% Ours & 1 & 0.7547 & 0.6681 & 8.9058 & 0.0779 & 0.0875 \\ 
% Ours & 5 & 0.0724 & 0.9146 & 9.4406 & 0.0504 & 0.0283 \\
% Ours & 20 & 0.0288 & 0.9914 & 9.5447 & 0.0216 & 0.0215 \\ \hline
% \end{tabular}
% }
% \caption{Evaluation of motion in-between with CondMDI \cite{cohan2024flexible} on HumanML3D\cite{guo2022generating} dataset.}
% \label{tab:inbetween}
% \end{table}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=\linewidth]{Figure/quantitive_text.pdf}
  \caption{Qualitative results of MotionLab on the text-based motion generation. For clarity, as time progresses, motion sequences transit from light to dark colors.}
  \label{fig:quantitive_text}
\end{figure*} 

\begin{figure*}[!h]
  \centering
  \includegraphics[width=\linewidth]{Figure/quantitive_textedit.pdf}
  \caption{Qualitative results of MotionLab on the text-based motion editing. The transparent motion is the source motion, and the other is the generated motion.}
  \label{fig:quantitive_edit}
\end{figure*} 

\begin{figure*}[!h]
  \centering
  \includegraphics[width=\linewidth]{Figure/quantitive_trajectory.pdf}
  \caption{Qualitative results of MotionLab on the trajectory-based motion generation. The red balls are the trajectory of the pelvis, right hand and right foot.}
  \label{fig:quantitive_trajectory}
\end{figure*} 

\section{Experiments}
\label{sec:experiments}
We evaluate our framework using the following datasets. To evaluate the text-based motion generation, the trajectory-based motion generation, motion in-between and motion style transfer, we leverage the HumanML3D \cite{Guo_2022_CVPR} dataset, which comprises 14,646 motions and 44,970 motion annotations. To evaluate on the text-based and trajectory-based motion editing, we utilize MotionFix \cite{athanasiou2024motionfix} dataset, which is the first dataset for text-based human motion editing including 6,730 motion pairs. 


\noindent\textbf{Evaluation Metrics.} 
%\subsection{Evaluation Metrics}
We evaluate our framework using the following metrics: 1) To evaluate \textit{text-based motion generation}, following the \cite{chen2023executing}, we introduce the FID to evaluate the distribution gap between the generated and original motions; Diversity to calculate the corresponding variance between motions; R-precision (R@K) to measure the proximity of the generated motion to the text or motion; Foot skating ratio to evaluate the physical plausibility of motion; Multi-modal Distance (MM Dist) calculates the distance between motions and texts. We also introduce Average Inference Time per Sample (AITS) measured in seconds to evaluate the inference efficiency; 2) To evaluate \textit{trajectory-based motion generation} and \textit{motion in-between}, following \cite{xie2023omnicontrol}, we introduce the Average Error to measures the mean distance between the generated motion locations and the keyframe locations; 3) To evaluate  \textit{text-based and trajectory-based motion editing}, following \cite{athanasiou2024motionfix}, we introduce the AvgR to measure the success rate of retrieval from edited motion to target motion; 4) To evaluate \textit{motion style transfer}, following the \cite{Song_2024_CVPR}, we introduce the Style Recognition Accuracy (SRA) and Content Recognition Accuracy (CRA) to measure the stylistic and content accuracy of the generated motion; Trajectory Similarity Index (TSI) to evaluate the trajectory preservation from source motion.

\begin{table*}[!t]
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccccc}
\hline
Method & text gen. (FID) & traj. gen. (avg. err.) & text edit (R@1) & traj. edit (R@1) & in-between (avg. err.) & style transfer (CRA) & style transfer (SRA) \\ \hline
w/o rectified flows & 0.301 & 0.0359 & 54.38 & 69.21 & 0.0289 & 42.20 & 63.96 \\
w/o MotionFlow Transformer & 0.483 & 0.0447 & 51.26 & 65.34 & 0.0349 & 35.36 & 53.83 \\
w/o Aligned ROPE & 0.253 & 0.0886 & 45.39 & 61.99 & 0.0756 & 42.23 & 56.59 \\
w/o task instruction modulation & 0.223 & 0.0401 & 55.96 & 70.01 & 0.0288 & 40.55 & 63.91 \\
w/o motion curriculum learning & 1.956 & 0.1983 & 28.56 & 36.61 & 0.1682 & 29.51 & 34.23 \\
\hline
Ours specialist models & \underline{0.209} & \underline{0.0398} & \underline{41.44} & \underline{59.86} & \underline{0.0371} & \underline{43.53} & \underline{67.55} \\
\textbf{Ours} & \textbf{0.167} & \textbf{0.0334} & \textbf{56.34} & \textbf{72.65} & \textbf{0.0283} & \textbf{44.62} & \textbf{69.21} \\
\hline
\end{tabular}
}
\caption{
\small{\textbf{Ablation studies of key components of MotionLab on each task}. Refer to the text for the detailed configuration of each variant.}
%Ablation studies of our MotionLab's main designs on each task. 
%Ours specialist models mean that we use this framework to train models in the same size for each task separately. 
%For text-based motion generation, we compare FID; for trajectory-based motion generation and motion in-between. we compare average error; for motion editing, we compare R@1; for motion style transfer, we compare the CRA and SRA. \textit{Additional ablation studies are available in the supplementary material.}
}
\vspace{-9mm}
\label{tab:ablation}
\end{table*}

\noindent\textbf{Implementation Details.} 
%\subsection{Implementation Details}
In order to fairly compare our model with other models, motions from all datasets have been retargeted into one skeleton following HumanML3D format with 20 fps, where the number of joint $J$ is 22 and the dimension of motion feature $D$ is 263. The learning rate is set to be 1$\times$10$^{-4}$. The timesteps are set to 1,000 for training and 50 for inference. Our models are trained by four RTX 4090D with each batch of 64 for 4 days. To ensure a fair comparison, the AITS of all models are recalculated using one RTX 4090D.


\subsection{Quantitative Results}

\textbf{Overall Performance.} 
As shown in Table~\ref{tab:text} to Table~\ref{tab:edit}, MotionLab demonstrates promising performance across \textit{all benchmarks}\footnote{Due to space limitations, we include the quantitative results on motion in-between and motion style transfer in the supplementary material.}, underscoring the effectiveness of our framework's design. Notably, as MotionLab is a unified framework without task-specific designs, it must balance versatility, performance, and efficiency. 
%\textit{The rest of the quantitative results are available in the supplementary material.}

Specifically, as shown in Table~\ref{tab:text} and Figure~\ref{fig:timestep}, MotionLab achieves superior performance (lowest FID, which is the key metric for generation tasks) with relatively fast inference time (third-lowest AITS). For trajectory-based tasks (Table~\ref{tab:trajectory}) and the motion in-between task, MotionLab achieves lower average error. We believe these improvements stem from the effectiveness of masked pre-training and Aligned ROPE, which ensures spatial and temporal synchronization between the trajectory and target motion. 
%\textcolor{red}{For motion style transfer, MotionLab also outperforms the previous methods on all metrics. This may caused by the masked pretraining and the text-based genertaion introduced first, which helps the model decouple the semantics and style of motion to some extent.}

% \vspace{-2mm}
\subsection{Qualitative Results}

As shown in the Figure~\ref{fig:quantitive_text} from Figure~\ref{fig:quantitive_trajectory}, our framework presents its powerful capabilities to generate motion aligned with the conditions and edit source motion based on the condition, demonstrating its versatility and performance. For more visualization results, please kindly refer to the supplementary and project website.

\subsection{Ablation Studies}
%In this section, 
We perform several ablation experiments\footnote{Additional ablation studies are available in the supp. material.} on our framework to validate the designs in MotionLab and report the results in Table~\ref{tab:ablation}: the $1^{st}$ variant replaces rectified flows with diffusion models; the $2^{nd}$ variant uses a regular transformer (\ie, without modulation mechanism and adopting cross-attention) instead of MFT. The $3^{rd}$ variant uses the implicit 1D-learnable encoding instead of Aligned ROPE; The $4^{th}$ variant does not adopt the Task Instruction Modulation; the $5^{th}$ variant directly learns all tasks based on their FID compared to the last evaluation. %Meanwhile,
Additionally, we %also 
use the same model to train specialist models for each task, denoted as `our specialist models' in Table~\ref{tab:ablation}.

%% here
%As illustrated in Table~\ref{tab:ablation},
As can be seen from the results, the removal of motion curriculum learning markedly diminishes model performance across all tasks, underscoring its pivotal role in facilitating knowledge transfer between tasks. Meanwhile, our unified framework outperforms our specialist models in all tasks, potentially due to the knowledge sharing of motion curriculum learning. These phenomenons can be also attributed to the strategy's capacity to enable the model to integrate its comprehension of spatial conditions (\eg, source motion, trajectory, and intermediate states) with abstract conditions (\eg, text and style), given that the latter can be partially represented by the former. 
% As illustrated in Table~\ref{tab:ablation}, the removal of motion curriculum learning markedly diminishes model performance across all tasks, underscoring its pivotal role in facilitating knowledge transfer between tasks. Meanwhile, our unified framework outperforms our specialist models in all tasks, potentially due to the knowledge sharing of motion curriculum learning. These phenomenons can be also attributed to the strategy's capacity to enable the model to integrate its comprehension of spatial conditions (e.g., source motion, trajectory, and intermediate states) with abstract conditions (e.g., text and style), given that the latter can be partially represented by the former.
%Meanwhile
Furthermore, as shown in Table~\ref{tab:ablation}
%Table~\ref{tab:edit}
, Aligned ROPE is essential for space-related tasks, significantly reducing the average error. It effectively aligns source motion and target motion temporally, contributing to high R-precision in editing tasks.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{Figure/timesteps.png}
  \vspace{-.15in}
  \caption{\small{%Ablation of text-based motion generation.
  \textbf{Impact of timesteps during inference on MotionLab}. The closer the model's performance is to the lower left corner, the stronger the model is.}}
  \vspace{-.2in}
  \label{fig:timestep}
\end{figure} 

%% here


%Additionally, we validate our framework on the timesteps of inference. As shown in Figure~\ref{fig:timestep}, our framework demonstrates an optimal balance between generation quality and efficiency.
Additionally, we evaluate the impact of timesteps during inference on our MotionLab and compare its performance with baseline methods in terms of generation quality and inference time for the text-based motion generation task. As shown in Figure~\ref{fig:timestep}, our framework strikes an optimal balance between generation quality and efficiency.


\section{Conclusion}
Building on our proposed Motion-Condition-Motion paradigm, we have developed the MotionLab framework to unify human motion generation and editing. We have introduced the MotionFlow Transformer leverage the rectified flows to learn the mapping from source motion to target motion based on specified conditions. Additionally, we have incorporated Aligned Rotational Position Encoding to ensure synchronization between source motion and target motion, Task Instruction Modulation, and Motion Curriculum Learning for effective multi-task learning. Our proposed MotionLab framework demonstrates superior versatility, performance and efficiency  compared to existing state-of-the-art methods.

% Bibliography
\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}
\clearpage

% Appendix
\appendix

\section{Additional Quantitative Results}

As shown in Table~\ref{tab:inbetween}, our framework outperform CondMDI on all setting, illustrating the effectiveness of our framework in motion in-between. % \zn{write the task name} task.

\begin{table}[!h]
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\hline
Method & Frames & FID$\downarrow$ & \makecell{R-precision\\Top-3$\uparrow$}  & Diversity$\rightarrow$ &  \makecell{Foot skating\\ratio$\downarrow$} & \makecell{Keyframe\\error$\downarrow$} \\ \hline
\multirow{3}{*}{CondMDI \cite{cohan2024flexible}} & 1 & 0.1551 & 0.6787 & 9.5807 & 0.0936 & 0.3739 \\ 
 & 5 & 0.1731 & 0.6823 & 9.3053 & 0.0850 & 0.1789 \\
 & 20 & 0.2253 & 0.6821 & 9.1151 & 0.0806 & 0.0754 \\ \hline
\multirow{3}{*}{Ours} & 1 & 0.7547 & 0.6681 & 8.9058 & 0.0779 & 0.0875 \\ 
 & 5 & 0.0724 & 0.9146 & 9.4406 & 0.0504 & 0.0283 \\
 & 20 & 0.0288 & 0.9914 & 9.5447 & 0.0216 & 0.0215 \\ \hline
\end{tabular}
}
\caption{Evaluation of motion in-between with CondMDI \cite{cohan2024flexible} on HumanML3D \cite{Guo_2022_CVPR} dataset.}
\label{tab:inbetween}
\vspace{-5mm}
\end{table}

Also as shown in Figure~\ref{fig:style}, our framework also outperform MCM-LDM on all metrics, demonstrating the effectiveness of our framework in motion style transfer.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\linewidth]{Figure/style.png}
  \caption{Comparison of the motion style transfer with MCM-LDM \cite{Song_2024_CVPR} on a subset of HumanML3D \cite{Guo_2022_CVPR}. This shows that our model has a stronger ability to preserve the semantics of source motion and a stronger ability to learn the style of style motion.}
  \label{fig:style}
\end{figure} 

\section{Additional Ablation Studies}
\label{sec:add_ablation}
To further validate the designs in our framework, we perform traditional ablation studies in this section.

To further validate the Aligned ROPE, we also introduce the variant of 3D-Learnable and 3D-ROPE to distinguish the source motion, target motion and trajectory. As shown in Table~\ref{tab:ablation_rope} and Figure~\ref{fig:quantitive_inbetween}, 1D-position encoding are better than 3D-position encoding by avoiding introducing distances between different modalities, and ROPE are better than learnable position encoding by explicit positional encoding. Hence our 1D-ROPE outperforms all others variants, demonstrating its effective to embed the position information into tokens.

To further validate the motion curriculum learning, we adopt the variant of removing the masked pre-training and directly supervised fine-tuning in order; the variant of with masked pre-training but supervised fine-tuning all tasks together; the variant of introducing masked reconstruction, motion in-between and trajectory based motion generation in orderly. As shown in Table~\ref{tab:ablation_learning}, our proposed motion curriculum learning outperforms all other variants, highlighting the effective of masked pre-training and fine-tuning tasks in order by avoiding gradient conflicts between different tasks. Specifically, the variant of masked pre-training in order demonstrates that necessity of introduce motion in-between and trajectory-based motion generation together, or will greatly weakens the performance of the model in the latter task.

To further validate the choice and combinations of the tasks, we also introduce the variants of different tasks. As shown in Table~\ref{tab:ablation_task}, improper combination of tasks will cause the unified framework to be weaker than the ours specialist models, while our carefully selected combination of all tasks makes our unified framework beat ours specialist models.

\begin{figure*}[!h]
  \centering
  \includegraphics[width=\linewidth]{Figure/quantitive_inbetween.pdf}
  \caption{Ablation results of MotionLab on the motion in-between (with text). Beige motion is use 1D-learnable position encoding, purple motion use Aligned ROPE, and gray motions are the poses provided in keyframes, demonstrating the importance of Aligned ROPE.}
  \label{fig:quantitive_inbetween}
\end{figure*} 


\begin{table*}[!h]
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccccc}
\hline
Method & text gen. (FID) & traj. gen. (avg. err.) & text edit (R@1) & traj. edit (R@1) & in-between (avg. err.) & style transfer (CRA) & style transfer (SRA) \\ \hline
1D-Learnable & 0.246 & 0.0886 & 45.39 & 61.99 & 0.0756 & 39.40 & 56.59 \\
3D-Learnable & 0.346 & 0.1865 & 35.46 & 53.74 & 0.1460 & 36.99 & 58.81  \\
3D-ROPE & 0.241 & 0.0579 & 51.34 & 70.00 & 0.0354 & 42.96 & 62.46 \\
1D-ROPE (ours) & 0.167 & 0.0334 & 56.34 & 72.65 & 0.0273 & 44.62 & 69.21 \\
\hline
\end{tabular}
}
\caption{Ablation studies of our MotionLab's position encoding on each task.}
\label{tab:ablation_rope}
\end{table*}


\begin{table*}[!h]
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccccc}
\hline
Method & text gen. (FID) & traj. gen. (avg. err.) & text edit (R@1) & traj. edit (R@1) & in-between (avg. err.) & style transfer (CRA) & style transfer (SRA) \\ \hline
random selection based on FID & 2.236 & 0.1983 & 28.56 & 36.61 & 0.1682 & 26.61 & 34.23 \\
removing the masked pre-training & 0.861 & 0.0932 & 44.99 & 63.92 & 0.0639 & 39.63 & 57.59  \\
supervised fine-tuning all tasks together & 1.331 & 0.1317 & 38.19 & 55.22 & 0.1143 & 36.60 & 50.59 \\
masked pre-training in order & 0.256 & 0.0423 & 56.33 & 69.31 & 0.0264 & 42.67 & 64.39 \\
motion curriculum learning (ours) & 0.167 & 0.0334 & 56.34 & 72.65 & 0.0273 & 44.62 & 69.21 \\
\hline
\end{tabular}
}
\caption{Ablation studies of our MotionLab's motion curriculum learning on each task.}
\label{tab:ablation_learning}
\end{table*}

\begin{table*}[!h]
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc|ccccccc}
\hline
\multicolumn{6}{c|}{Task} & \multicolumn{7}{c}{Metric} \\ \hline 
text gen. & traj. gen & text edit & traj. edit & in-between & style transfer  & text gen. (FID) & traj. gen. (avg. err.) & text edit (R@1) & traj. edit (R@1) & in-between (avg. err.) & style transfer (CRA) & style transfer (SRA) \\ \hline
\multicolumn{6}{c|}{ours specialist models} & 0.209 & 0.0398 & 41.44 & 59.86 & 0.0371 & 43.53 & 67.55 \\ \hline
\ding{51} & $\times$ & $\times$ & $\times$ & $\times$ & \ding{51} & 0.240 & - & - & - & - & 41.23 & 65.53 \\
\ding{51} & $\times$ & \ding{51} & $\times$ & $\times$ & $\times$ & 0.235 & - & 52.79 & - & - & - & - \\
\ding{51} & \ding{51} & $\times$ & $\times$ & \ding{51} & $\times$ & 0.176 & 0.0364 & - & - & 0.0297 & - & - \\
\ding{51} & \ding{51} & \ding{51} & \ding{51} & \ding{51} & $\times$ & 0.171 & 0.0344 & 55.10 & 72.20 & 0.0287 & - & - \\
\ding{51} & \ding{51} & \ding{51} & \ding{51} & \ding{51} & \ding{51} & 0.167 & 0.0334 & 56.34 & 72.65 & 0.0273 & 44.62 & 69.21 \\
\hline
\end{tabular}
}
\caption{Ablation studies of our MotionLab's task combinations.}
\label{tab:ablation_task}
\end{table*}

\section{Representation for Each Modality}
We represent the features of all modalities as tokens for the attention mechanism \cite{vaswani2017attention}. Specifically, source motion and target motion are represented as $M_S \in \mathbb{R}^{N\times D}$ and $M_T \in \mathbb{R}^{N\times D}$, and we first ignore timestep $t$ here. For the instruction, it is represented as $I\in\mathbb{R}^{1\times768}$ extracted from the CLIP \cite{radford2021learning}. For available conditions $C$, the text is represent as $p\in\mathbb{R}^{77\times768}$ extracted from the last hidden layer of CLIP, the trajectory is represented as $h\in \mathbb{R}^{N\times J\times3}$, and the style is represented as $s\in\mathbb{R}^{1\times512}$ extracted from \citeauthor{zhong2025smoodi}. 

\section{Instrcutions for Each Task}

As shown in the Table\ref{tab:instructions}, the instructions in the Task Instruction Modulations for each task are presented, which benefits our framwork to distinguish differents tasks.


\begin{table}[!h]
\resizebox{\linewidth}{!}{
\begin{tabular}{ll}
\hline
Task & Instruction \\
\hline
unconditional generation & ``reconstruct given masked source motion.'' \\
masked source motion generation & ``reconstruct given masked source motion.'' \\
reconstruct source motion & ``reconstruct given masked source motion.'' \\
trajectory-based generation (without text) & ``generate motion by given trajectory.'' \\
in-between (without text) & ``generate motion by given key frames.'' \\
style-based generation & ``generate motion by given style.'' \\
trajectory-based editing & ``edit source motion by given trajectory.'' \\
text-based editing & ``edit source motion by given text.'' \\
style transfer & ``generate motion by the given style and content.'' \\
in-between (with text) & ``generate motion by given text and key frames.'' \\
trajectory-based generation (with text) & ``generate motion by given text and trajectory.'' \\
text-based generation & ``generate motion by given text.'' \\
\hline
\end{tabular}
}
\caption{Instructions in the Task Instruction Modulations for each task.}
\label{tab:instructions}
\end{table}

\section{Classifier Free Guidance for Each Task}
Classifier-Free Guidance (CFG) \cite{ho2022classifier} has been incorporated for various tasks \cite{peng2024harnessing,editing,zhang2024diff} based on diffusion models. As shown in Table\ref{tab:cfg}, strengths of classifier free guidance for each task are presented, which contributing to the results' quality during sampling.

\begin{table}[!h]
\resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\hline
Task & Source Motion Guidance & Condition Guidance \\
\hline
trajectory-based generation (without text) & $-$ & 1.5  \\
in-between (without text) & $-$ & 1.5 \\
text-based generation & - & 5.75 \\
style-based generation & - & 1.5 \\
trajectory-based editing (without text) & 2.25 & 2.25 \\
text-based editing & 2.25 & 2.25 \\
style transfer & 1.5 & 1.5 \\
in-between (with text) & $-$ & 1.75 \\
trajectory-based generation (with text) & $-$ & 1.75 \\
trajectory-based editing (with text) & 2 & 2 \\
\hline
\end{tabular}
}
\caption{Strength of classifier free guidance for each task.}
\label{tab:cfg}
\end{table}


\section{3D Assets}

We have borrowed some 3D assets for our video and figure from the Internet, including \href{https://sketchfab.com/3d-models/dojo-matrix-drunken-wrestlers-a7902c72cde2447986ff89e13e78a11f}{Dojo Matrix Drunken Wrestlers}, \href{https://sketchfab.com/3d-models/basketball-court-77af6cb6181e4fe7b56bf15035b33422}{Basketball Court}, \href{https://sketchfab.com/3d-models/grandmas-place-02fa0075c38a482187c78ac0eacec214}{Grandma`s Place}, \href{https://sketchfab.com/3d-models/dae-diorama-retake-small-farm-252ad9f2245e47cba4fbb0cfe5eb6445}{DAE Diorama retake â€“ Small farm}, \href{https://sketchfab.com/3d-models/dae-diorama-retake-small-farm-252ad9f2245e47cba4fbb0cfe5eb6445}{DAE Diorama retake â€“ Small farm}, \href{https://www.fab.com/listings/fe784b4e-ab8b-44b7-885d-140b0f81448b}{Japanese Small Shrine Temple 0002}.


\begin{table}[!h]
\resizebox{\linewidth}{!}{
\begin{tabular}{ll}
\hline
Task & Instruction \\
\hline
unconditional generation & ``reconstruct given masked source motion.'' \\
masked source motion generation & ``reconstruct given masked source motion.'' \\
reconstruct source motion & ``reconstruct given masked source motion.'' \\
trajectory-based generation (without text) & ``generate motion by given trajectory.'' \\
in-between (without text) & ``generate motion by given key frames.'' \\
style-based generation & ``generate motion by given style.'' \\
trajectory-based editing & ``edit source motion by given trajectory.'' \\
text-based editing & ``edit source motion by given text.'' \\
style transfer & ``generate motion by the given style and content.'' \\
in-between (with text) & ``generate motion by given text and key frames.'' \\
trajectory-based generation (with text) & ``generate motion by given text and trajectory.'' \\
text-based generation & ``generate motion by given text.'' \\
\hline
\end{tabular}
}
\caption{Instructions in the Task Instruction Modulations for each task.}
\label{tab:instructions}
\end{table}