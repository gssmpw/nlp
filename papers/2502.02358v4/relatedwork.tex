\section{Related Works}
\textbf{Motion Generation and Editing.} Motion generation can be classified based on input conditions. Among these, text-based motion generation is one of the most compelling areas \cite{zhang2022motiondiffuse,tevet2022motionclip,tevet2023human,Guo_2022_CVPR,chen2023executing,guo2024momask,guo2022tm2t,wang2024motiongpt,jiang2023motiongpt,kim2023flame,lin2023motion,lu2023humantomato,plappert2016kit,zhang2023generating,guo2020action2motion,petrovich2021action}, as it trains models to comprehend the semantics of text and generate corresponding pose sequences. To address the fine-grained requirements of practical applications, trajectory-based motion generation has been proposed \cite{karunratanakul2023guided,shafir2023human,xie2023omnicontrol,dai2025motionlcm,zhang2023finemogen}, where specific motion properties, such as joints reaching designated positions at specified times, are defined. Additionally, motion in-between \cite{tevet2023human,jiang2023motiongpt,cohan2024flexible,qin2022motion,pinyoanuntapong2024mmm} focuses on generating complete motion sequences given key poses at keyframes. To enable in-place editing of human motion \cite{goel2024iterative,athanasiou2024motionfix}, MotionFix \cite{athanasiou2024motionfix} introduces text-based motion editing using paired source and target motions. We extend this approach to trajectory-based motion editing by substituting text with joint trajectories. Meanwhile, style plays a crucial role in human motion, leading to motion style-transfer \cite{jang2022motion,aberman2020unpaired,zhong2025smoodi,Song_2024_CVPR}. However, the aforementioned methods concentrate solely on specific tasks, rendering them impractical for real-world applications. Moreover, they overlook the intrinsic connections across different human motion tasks and fail to facilitate knowledge sharing among these tasks. In contrast, our unified framework enhances performance on data-scarce editing tasks through multi-task learning.

\noindent\textbf{Unified frameworks for human motion.} There are also some efforts in existing methods that try to unify tasks related to human motion. One line of work \cite{jiang2023motiongpt,jiang2025motionchain,zhou2024avatargpt,li2024unimotion,wang2024motiongpt,ling2024motionllama,wu2024motionllm,luo2024m,athanasiou2024motionfix} focuses on motion understanding, such as motion captioning or describing human motion in images and videos. Yet, these approaches often rely on GPT-like structures, which requires a large amount of training resources and GPU memory. In addition, they fail to provide fine-grained control (\eg, trajectory-based generation and editing) over motion, which is crucial in practical applications. Another line of effort \cite{shrestha2025generating,zhou2023ude,zhou2023unified,zhang2025large,yang2024unimumo,luo2024m,fan2024everything2motion,alexanderson2023listen} highlights generating motion based on more modalities, such as music and speech. However, these approaches just only integrate more modalities into one model and cannot flexibly edit motion, which can cause them suffering from the multi-task learning and limit their scope of use. The closest to our work are FLAME \cite{kim2023flame} and MotionCLR \cite{chen2024motionclr}.
%% here
%While
However, FLAME does not support style transfer and precise text-based editing like ``move faster'', and MotionCLR does not support trajectory-based generation and editing, requiring cumbersome manual adjustments to the attention.
% While FLAME does not support style transfer and precise text-based editing like ``move faster'', MotionCLR does not support trajectory-based generation and editing, requiring cumbersome manual adjustments to the attention.