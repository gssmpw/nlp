\section{Introduction}
The Large Language Model (LLM) agent workflow features an interactive environment that enables the LLM to operate under different contexts to perform tasks autonomously or semi-autonomously.~\cite{langgraph,Swarm_openai,wu2023autogen, wang2023plan}
The workflow leverages the LLM's abilities of natural language contextual understanding and generative power. 
It takes in the user prompt and passes it through the workflow path of multiple LLM agents that collaborate with each other to complete the task. 
This kind of workflow can be implemented via different packages with distinct focuses, such as LangGraph for complex multistep AI workflows, Swarm/OpenAI for distributed AI collaboration and Autogen/Microsoft for software/code development tasks.~\cite{wu2023autogen}


For example, LangGraph presents a graph-based AI workflow, where the LLMs act as nodes and are connected with directed or conditional edges defining the data flow and relationship between different models.  
The workflow is compatible with the LangChain codebase, that integrates various APIs and models, allowing multi-modal applications. 
It also provides a high degree of control over the workflow dependencies. 
Such features make it the suitable framework for complex multistep tasks, such as scientific research and decision-making. 


Compared to a single LLM, the LLM agent workflow can break down complex multistep tasks into manageable subtasks and delegates them to LLM agents with complementary capabilities. 
Each agent in the workflow can be prompted in playing a different role in the collaborative activities. 
This framework outperforms a single LLM in efficiency, extensibility, and adaptability in complex scientific research tasks. 


Another important feature is the tool calling capability of the latest LLM models, such as OpenAI, Anthropic, Google Gemini and Mistral. 
The tool calling functionality extends the LLM capability beyond text generation, allowing it to have access to external tool functions, APIs and computer systems. \cite{schick2024toolformer}
It enables the model to access real-time data, perform specialized tasks, and integrate with complex workflow. 
This integration also helps to overcome the struggles of LLM natural language contextual understanding in the mathematical precision and specialized scientific domain tasks. 



One of its most notable applications is to incorporate well-defined scientific methods, such as arithmetic functions, finite element analysis, etc. 
It gives the model access to these methods, which provide more reliable results than the text completion for the logic-base tasks for LLM. 
Rather than understanding the underlying science of a task, the LLM invokes a tool function that offers a principled solution. This approach aligns with the strengths of LLMs, enabling them to act as the connective tissue between diverse scientific applications, thereby supercharging computational workflows for science.
It combines the LLM language contextual understanding and scientific domain knowledge from the provided methods in tool function form, making it a more reliable and versatile AI approach for scientific research. 


However, LLMs and many scientific workflows are computationally demanding and require parallel execution on high-performance computing (HPC) platforms such as Aurora and Polaris at the Argonne Leadership Computing Facility (ALCF). 
The computing resources of these platforms are managed through job schedulers such as the Simple Linux Utility for Resource Management (Slurm), Portable Batch System (PBS), or Load Sharing Facility (LSF). 
The scheduler requires a submission script on the login node to submit jobs to the computing queue. 
For an LLM agent to call a function on HPC, it will require such a submission script specifying the computing configurations. 
This work aims to integrate the parallel execution on local/HPC computing resource to the LLM tool calling functions via Parsl, enabling the LLM agent to carry out large scale scientific research. 


% \section{Related Work}