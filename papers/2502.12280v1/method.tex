\section{Methods}
To build the framework that allows LLM agent to leverage the HPC computing power, it requires the integration of an LLM agent workflow package and HPC resource/workload manager. 


\subsection{LangGraph: The graph-based AI workflow}
LangGraph was adopted to build the AI test cases in this work as its focus on complex multistep AI workflow.~\cite{langgraph}
It features a graph-based workflow and execution, which provides clarity and complex dependency management. 
This graph-based architecture uses the nodes to represent a task or LLM model, and edges to define relationships and dataflow. 
The integration of LangChain framework gives the user access to various LLM models and customized tool functions.~\cite{langchain}


In a LangGraph workflow, the graph state keeps track of all the previous events in the workflow, and coordinates the activation of the next agent node.
The active agent responds to the current graph state and updates it according to the output from the LLM model or tool functions. 
By carrying out such operation iteratively, the workflow optimizes and updates the LLM responses until the result is evaluated by the reasoning agent to be acceptable for the final output. 
This gives the workflow traceability and logging capability to make it auditable and transparent, which are essential for scientific research tasks. 


\subsection{Workflow setup}
\label{sec:wf_setup}

\begin{figure*}
    \centering
    \includegraphics[width=.8\textwidth]{Figures/workflows.pdf}
    \caption{The LangGraph workflows, (A) workflow 1 and (B) workflow 2. The START node takes in the prompt from the user, and the END node presents the final workflow result. The workflow 1 consists of a single LLM agent with tool node. The workflow 2 is managed with a supervisor node, which decides the next acting agent, the research or the simulator. }
    \Description{}
    \label{fig:wfs}
\end{figure*}

We built two LangGraph workflows to handle different test runs, shown in \autoref{fig:wfs}. 
Here we used OpenAI/gpt-4o-mini as the LLM models.~\cite{gpt4omini_openai}
Workflow 1 (\autoref{fig:wfs}A) was a single simulation agent with tool node, that calls functions to download protein structure from Protein Data Bank, and run MD simulations. 
It's suitable for tasks with provided PDB ID in the user prompt. 
Workflow 2 (\autoref{fig:wfs}B) featured the supervisor agent to coordinate the flow of action, via deciding the next active agent, based on current graph state. 
In addition to the simulation agent, the workflow also incorporated a research agent with Tavily search tools~\cite{tavily_search}, that can query the relevant information from the internet to provide additional context for the LLM agents. 
This ability of querying the internet gave the workflow 2 to access the up-to-date knowledge. 
It was applied for more complex tasks, when extra information was required. 


\subsection{Parsl: The HPC workload manager}
We adopted Parsl as the workload manager for the tool calls. 
Parsl is a Python library that allows the user to execute native Python functions in parallel and asynchronous workflows.~\cite{babuji19parsl, ward2021colmena}
It bridges user-defined Python functions and the computing resource via the Parsl configuration, requiring minimal code changes when moving across platforms, from a single laptop to leading-edge HPC systems. 
Parsl uses a dataflow programming model that is based on the function calls and input/output, thereby allowing the users to focus on the scientific workflow, instead of juggling the HPC computing queue. 
These features make Parsl the ideal intermediate for integrating the LLM tool calls and HPC resources. 

\subsection{Molecular dynamics (MD) simulation function}
We used molecular dynamics simulations to test the integration of the LangGraph tool calls and Parsl workload. 
The simulation function is set up with OpenMM software package to run on the Nvidia CUDA GPUs.~\cite{eastman2023openmm}
The function takes an input of a PDB file, and set up the simulation parameters, such as nonbonded cutoff distance, temperature, pressure for NPT system, timestep, and simulation length. 
In the LangGraph workflow, the LLM model calls this simulation function and assigns these parameters accordingly. 

In addition, the function cleans up the PDB file via pdbfixer, to add missing heavy atoms in the protein chain. 
It builds the system topology, which comprises all the atomic bonded and nonboned interaction, using Gromacs pdb2gmx~\cite{abraham2015gromacs} with Charm36m force field~\cite{huang2013charmm36}. 
The forcefield directory is defined in the environment variable as \emph{GMX\_ff}. 
The simulation timestep is default at 2 fs, and integrated with LangevinMiddleIntegrator. 
The long range nonbonded interactions are calculated with Particle Mesh Ewald method.~\cite{darden1993particle}

The MD simulation function also manages the simulation directories. 
Each time the function is called, it creates a new directory with the current time stamp and a unique ID to avoid the race condition of multiple parallel function creating the result directory at the same time. 
All the simulation results will be stored in their respective directories. 


\subsection{LangChain tool call via Parsl}


\begin{figure}
    \centering
    \includegraphics[width=.85\linewidth]{Figures/tool_call.pdf}
    \caption{Two Parsl tool call scheme for LangGraph. The figure demonstrates the LLM agent, tool node and the tool functions. The Parsl tool node (A) replaces the LangGraph tool node, which submits the tool functions to the parallel Parsl queue. The Parsl ensemble function (B) can be directly called by the LLM, and launches an ensemble of simulation runs with Parsl. The blue letters mark the Parsl implementation to the LangGraph tool call framework. }
    \Description{}
    \label{fig:tool_call}
\end{figure}

We set up two different tool call integrations of Parsl to LangGraph shown in \autoref{fig:tool_call}, specifically,
\begin{enumerate}
    \item Parsl tool node: A tool node that launches functions via Parsl. 
    % \item LangGraph tool functions with Parsl decorator.  
    \item Parsl ensemble function: Tool function that runs an ensemble of tools with Parsl. 
\end{enumerate}

In setup  1 (\autoref{fig:tool_call}A), the Parsl tool node replaces the tool node of LangGraph in the workflow. 
When the LLM agent generates tool calls, it launches them to the Parsl queue, distributing them to the computing resource. 
The tool functions are executed concurrently by the Parsl workers. 
The tool node collects and returns the outputs to the LLM agent, once all the tasks are done. 

The setup 2 (\autoref{fig:tool_call}B) manages the multitasking through the tool function. 
The function embeds the Parsl to distribute multiple simulation runs to the computing system. 
During a tool call instance, the LLM model inputs the number of simulation runs to the function as an argument, and the function launches all the simulation runs to Parsl queue. 
This setup is more suitable to launch many simulation runs on the HPC queue, as the tool call cap of the LLM agent. 

% The setup 1 requires the LLM agent to make multiple tool calls. 
% Instead of directly calling the simulation function, setup 2 calls a tool function to launch an ensemble of simulations to the Parsl work queue. 
% The LLM model defines the number of simulations as the input arguments for a single tool call, while multiple tool calls are made in setup 1. 
% The tool function needs to be rewritten to incorporate the Parsl functionality to manage multiple simulation runs. 


\subsection{Experiment setup}
We prompted the LangGraph AI workflows of the previous section with inputs, shown in \autoref{tab:prompts}, to test the Parsl tool call functions. 
To test Parsl setup on different platforms, the workflows are set up for both local Nvidia GPU workstations and HPC system, Polaris/ALCF. 
The local workstation applied the tool call setup 1 with a Parsl-implemented tool node, which launched jobs to a Parsl queue. 
The HPC runs were more compatible with tool call setup 2, where the tool functions were implemented with Parsl to run simulation ensembles. 


In the prompts, we specified the input protein structure with either a PDB ID directly, or the protein name. 
With prompts providing the PDB ID, workflow 1 was applied, as no additional information is needed to download the input protein.
Prompts with only protein names or acronym were inputted for the more complex workflow 2. 
Additionally, the simulation parameters were specified in the prompts, namely the number of runs, temperature and length. 
For all the local workstation runs, 8 simulation runs were specified in the prompts on 4 or 8 GPUs on the machine. 
On Polaris, we tested 100 simulations with 100 GPUs on 25 nodes and 80 runs with 40 GPUs on 10 nodes. 



\begin{table*}
  \caption{Prompt setup for experiments}
  \label{tab:prompts}
  \begin{tabular}{p{0.05\linewidth}p{0.2\linewidth}p{0.1\linewidth}p{0.1\linewidth}p{0.45\linewidth}}
    \toprule
    Run & Platform & Tool Call & Workflow& Prompt\\
    \midrule
    \texttt{1} & workstation & 1 & 1 &Can you run 8 simulations on \{path of a local PDB file\} in 313 K for 50 ps? \\
    \texttt{2} & workstation & 1 & 1 &Can you find and download 2KKJ, and run 8 simulations on them in 313 K for 50 ps? \\
    \texttt{3}& workstation & 1 & 2 &Can you find the complex structure for NCBD/ACTR,  and run 8 simulations of them in 310 K for 100 ps? \\
    \texttt{4}& workstation& 1 & 2 &Can you find and download 8 crystal structures of lysozyme from PDB, and run simulations of them in 310 K for 100 ps? \\
    \texttt{5}& Polaris/100 GPUs& 2 & 1 &Can you download the structure of 2KKJ from Protein Data Bank, and run 100 simulations of it in 313 K for 50 ps?\\
    % \texttt{5}& Polaris/40 GPUs& 2 & 1 &Can you download the structure of 2KKJ from Protein Data Bank, and run 80 simulations of it in 313 K for 50 ps?\\
    \bottomrule
  \end{tabular}
\end{table*}
