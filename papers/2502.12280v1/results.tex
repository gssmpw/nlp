\section{Results}
The prompt results included both the LLM outputs and the simulation runs, spawned from the tool calls. 
Examining this information can gain us insights on the performance and limitations of these LLM agent workflows. 

\subsection{Local workstation}
The local runs were performed on a lambda machine with 8 Nvidia V100 GPUs. 
We prompted the workflow to run 8-simulation ensemble run of different protein input instructions, including a local PDB file, PDB ID, or just protein names. 
These runs implemented the Parsl tool node setup, where the tool node was in charge to submit the tasks to the Parsl queue.  

The run 1 and 2 were set up on the workflow 1 with a single LLM agent and simulation tools, and implemented with the Parsl tool node. 
The model was able to choose the correct simulation input parameters, including the number of tool calls, based on the input prompts. 
These 8 tool functions were then distributed to the 4 GPUs on the workstation via Parsl. 
As the result, both workflows generated 8 simulation runs of 50 ps in 313 K on its respective protein structure input, as the prompt specified. 

A local PDB file path was specified in the run 1 prompt as the input for the MD simulation. 
During setting up the prompt, we noticed the absolute file path on the file system was required for the tool function to locate the correct PDB file.  
The LLM agent recognized the full file path in the prompt and set up tool functions with correct arguments. 

The prompt 2 specified the PDB ID, 2KKJ, of nuclear coactivator binding domain (NCBD), instead of the file path. 
To obtain the PDB file, the LLM agent called the tool function to download the protein structure from PDB, based on the provided PDB ID. 
The PDB download function returned the log containing the PDB file path to the LLM agent, which then assigned the PDB file to the simulation tool functions. 


\begin{figure*}
    \centering
    \includegraphics[width=.85\textwidth]{Figures/local_perf.pdf}
    \caption{The Parsl process for the run 1 (A), 2 (B), 3 (C) and 4 (D). Both run launched 8 workers with GPUs. The Parsl logs were recorded every 5 seconds.  }
    \Description{}
    \label{fig:local_run}
\end{figure*}

The simulation runs of 2KKJ were run at $\sim$260 ns/day on Nvidia V100 GPUs. 
The Parsl run process were shown in \autoref{fig:local_run} A and B. 
The workers were initiated at $\sim$5 s, and the MD tasks were submitted at $\sim$10 seconds in run 1 with local PDB file. 
For run 2 with the PDB ID, the workflow first submitted the single task to download the PDB structure, and submitted the MD tasks at around $\sim$30 s. 
It took both runs $\sim$70 seconds to finish all 8 simulations, and 15â€“25 seconds for Parsl shutdown process. 

Compared to the run 1 and 2, prompt 3 and 4 contained the protein names, instead of local PDB file path or PDB ID. 
They required additional information to set up the simulation runs. 
Therefore, for these prompts, we applied the workflow 2, which comprises three LLM agents, supervisor, researcher and simulator. 
The input was directed by the supervisor to the researcher agent, which obtained the PDB ID based on the input prompt. 
The search tool took the refine query from researcher agent, and retrieved the 5 search results. 
These results were then refined with the LLM agent, and passed back to the supervisor. 
The next acting agent, simulator, submitted all the simulation runs to the Parsl worker queue. 

In the run 3, the LLM agent queried for a single protein complex structure of CBP nuclear coactivator binding domain and activator for thyroid hormone and retinoid receptors (NCBD/ACTR). 
The LLM agent made two tool calls via the Tavily search tool, with \emph{NCBD ACTR complex structure} and \emph{simulate NCBD ACTR 310 K 100 ps}, for the context before and after the \emph{and} in the prompt. 
From the \emph{NCBD ACTR complex structure} query, the search tool retrieved 5 results about the NCBD structure.~\cite{dogan2013transition_ncbd,marino2018charge_ncbd,chen2023structural_ncbd,bauer2021conformational_ncbd,karlsson2019structurally_ncbd}
These results covered studies of NCBD/ACTR complex in heterodimer interaction, binding/folding formation, and conformational editing, via X-ray diffraction (XRD), nuclear magnetic resonance (NMR), small angle neutron scattering, molecular simulations, et al. 
The 2nd query, \emph{NCBD ACTR 310 K 100 ps}, consisted of the protein complex name and the simulation temperature and length, and also returned 5 query results, including one of the query 1 result on molecular dynamics simulation~\cite{karlsson2019structurally_ncbd}. 
Noted, none of these results were the original paper for the NCBD/ACTR NMR structure, 1KBH~\cite{demarest2002mutual_1kbh}. 
But the PDB ID was referred in the context of the query results, and picked up by the research agent. 
It also summarized these results into a brief report with references about NCBD/ACTR complex structure, covering the NMR and XRD structures, binding characteristics, and simulation protocols for production. 

The supervisor agent then passed the task to the simulator. 
The simulator then downloaded the PDB structure from the RCSB database, and initiated 8  simulation runs with the Parsl worker. 
The run process was shown in \autoref{fig:local_run}C. 
The download tool was launched at $\sim$10 seconds for the 1KBH structure. 
The NCBD/ACTR simulations ran at $\sim$350 ns/day. 
All 8 simulation were then submitted to the Parsl workers and finished in $\sim$60 seconds. 
The results were printed as a list of the trajectories. 


The run 4 queried 8 crystal structures of lysozyme. 
We noticed that the number of search results must be larger than 8, in order for the LLM agent to identify all 8 structures. 
When the search tool only returned the top 5 results, it fetched PDB IDs of 148L~\cite{kuroki1993covalent_148L}, 
1B7E~\cite{davies1999three_1b7e}, 
1LYZ, 
2LYZ~\cite{diamond1974real_1lyz}, 
3WEL~\cite{tagami2015structural_3wel}, 
5R2Z~\cite{wollenhaupt2020f2x_5r2z}, 
7BVM~\cite{nam2020polysaccharide_7bvm}, 
and 7F26~\cite{liang2021novel_7f26}.
Among these results, 2LYS is the partially-folded intermediate of CylR2, 3WEL is Sugar Beet $\alpha$-Glucosidase and 5R2Z is Endothiapepsin. 
These PDB IDs were hallucinated by the LLM agent, as only 5 search results were generated from the researcher. 
When retrieving 10 results with the search tool, the LLM agent downloaded the structures of 1LYZ, 
2LYZ, 
3LYZ, 
4LYZ, 
5LYZ, 
6LYZ~\cite{diamond1974real_1lyz}, 
7LYZ~\cite{herzberg1983protein_7lyz}, 
and 8LYZ~\cite{beddell1975x_8lyz}.
These PDB IDs were confirmed as lysozyme protein structures, providing correct inputs for the MD simulations. 
To avoid LLM hallucination, it required sufficient queried results from search tools than the requested information. 
Especially, in this case, the number of requested protein structures must be less than the number of returned results from researcher agent. 

Different from the previous runs, the workflow invoked 8 parallel download functions for all 8 PDB IDs. 
These functions were called at $\sim$30 seconds  into the workflow, due to the long query time for the more complex task of getting 8 PDB IDs. 
The simulations were then set up and run on the 8 Parsl workers for $\sim$70 seconds, at the speed of 310 ns/day. 
From the run with 5 queries, we received the wrong lysozyme entries, which had different protein structures from the correct lysozyme entries. 
We were able to observe the load imbalance between Parsl workers, where the smaller proteins were finished earlier than the larger ones, with $\sim$177 seconds difference in execution time. 

These runs on a local workstation showed the parallel execution of tool functions, especially those computationally expensive ones, such as MD simulation, can be implemented in the agent workflow to reduce execution time. 
The implementation of Parsl to the tool node packaged the LangChain tool functions and submitted the tool calls to the workers for parallel execution. 
This can reduce the overhead of the users designing the parallelism logic. Instead, they can focus on design of the tools and agent workflow. 


\subsection{Polaris runs}
The HPC runs were carried out on Polaris/ALCF machine production queue. 
The tasks on Polaris system is managed with PBS system.  
We implemented Parsl to allocate the computing resource, and submit the LangGraph tool functions to the Polaris computing nodes. 
We applied the simple workflow 1, as the PDB ID was provided in the prompt. 

First, we tested the different tool call setups, and concluded the Parsl tool function, tool call setup 2, was more compatible for the HPC systems. 
While implemented with the Parsl tool node, the LLM agent made 24 tool calls, with \emph{100 simulations} specified in the input prompt during one experiment with Parsl tool node (setup 1). 
Therefore, we needed a different implementation on the HPC systems for massive parallel tool calls, to make sure the correct number of simulation runs were instantiated on the computing resource. 
Parsl was implemented with the simulation tool function to run an ensemble of simulation runs, instead of a Parsl tool node launching each individual simulation to the queue. 
The number of simulation runs was now an argument to the Parsl simulation ensemble function. 
The LLM agent only needed to pass the right number of simulations to this tool function. 
This implementation bypassed the LLM agent calling an excessive number of tools, making it more suitable for the HPC computing environment. 
Furthermore, the user can now decide which function would be running on the computing node, giving the flexibility to move only the computationally expensive tasks to the Parsl queue.  
However, it required the user to explicitly rewrite the tool function or add a wrapper function to incorporate the Parsl toolkit for the simulation ensemble runs. 

\begin{figure}
    \centering
    \includegraphics[width=.85\linewidth]{Figures/polaris_queue.pdf}
    \caption{The timeline of Parsl simulation ensemble on Polaris, running 100 simulations of 2KKJ protein from LangGraph agent workflow. The number of workers represented the Parsl workers with available GPUs, and the number of tasks depicted the tasks in the Parsl queue. }
    \Description{}
    \label{fig:polaris_run}
\end{figure}

With the Parsl tool function, the LangGraph workflow was able to access the Polaris PBS queue directly without setting up the submission commands or scripts. 
The simulation tasks were submitted to the Parsl queue, which then assigned them to the workers with available resources.
The timeline of execution is shown in \autoref{fig:polaris_run}. 
The LLM agent identified and downloaded 2KKJ structure on the login node, and initiated the Parsl simulation function. 
This tool function launched 100 simulation to the Parsl queue. 
With pre-defined configuration, Parsl requested 25 nodes from the Polaris PBS. 
The tasks waited in the queue for $\sim$265 minutes, until 25 nodes were available on Polaris. 
The Parsl workers were assigned with GPUs and subsequently a task per worker. 
On the Nvidia A100 GPUs, the 50-ps simulations of 2KKJ ran in $\sim$340 ns/day, and were then finished  within $\sim$3 minutes. 
The majority of the run time was the queue time on Polaris. 
Depending on the HPC system, we could also separate the task into smaller chunks to reduce the queue time. 

From this result, the large set of tool execution was limited by the LLM agent tool call ability. 
We were able to circumvent this issue by combining the tool functions and Parsl to run the tool ensemble. 
These tool functions can be invoked with the LangGraph default tool node directly, however, required the user to implement the Parsl logic to the tool functions. 
At the meantime, the user has more control over where the functions will be executed, giving more versatility to the workflow. 


% The future work will be focusing the application of the Parsl-enabled parallel LLM agent workflow for more complex scientific tasks, such as AI-enhanced sampling. 