\section{Background and Related Work}
The testing and evaluation of LLMs have emerged as a critical research focus as these systems become increasingly integrated into production applications. Chang et al., "A Systematic Survey on Evaluation Metrics for Deep Learning Models" provides a comprehensive framework to understand the current state of LLM evaluation in their systematic survey. Their work synthesizes existing evaluation approaches and metrics, highlighting that effective LLM assessment requires considering multiple dimensions, including accuracy, reliability, and robustness. This multi-faceted approach to evaluation has particular relevance to our work on testing RAG systems in tourism applications, where both response quality and factual accuracy are crucial.


In the specific context of RAG systems, Es et al., "RAGAS: A Framework for Evaluating Response Augmentation Systems" introduced RAGAS, a pioneering framework for automated evaluation. Their work establishes standardized metrics to assess critical aspects of RAG systems, including faithfulness to source documents, context relevance, and quality of the answers. The RAGAS framework has particular significance for our testing methodology, as it provides systematic ways to evaluate how effectively our tourism RAG system retrieves and utilizes domain-specific information while maintaining accuracy in its recommendations.

%% Bestoun 


The challenge of ensuring reliable LLM output has been comprehensively addressed by Wang et al., "Semantic Consistency Evaluation Metrics for Deep Learning Models" who developed methods to measure semantic consistency in LLM responses. Their research demonstrates how LLMs can produce inconsistent output even with similar inputs, a finding that directly influenced our approach to testing response consistency in tourism recommendations. Their proposed semantic consistency evaluation metrics provide valuable tools for assessing the reliability of LLM-generated travel advice and ensuring consistent user experiences.


Recent work by Ni and Li, "A Systematic Evaluation Framework for Natural Language Generation Tasks" presents a systematic evaluation framework specifically focused on natural language generation tasks. Their comprehensive assessment methodology examines multiple aspects of the generated text, including fluency, coherence, and task-specific effectiveness. This work has informed our approach to evaluating the quality of generated travel recommendations, particularly in assessing how well our system maintains natural language quality while providing accurate tourism information. A significant advance in treating LLM hallucination comes from Dhuliawala et al., "A Chain of Verification Approach for Reducing Hallucinations in Language Models" who introduced the chain of verification approach. Their method provides robust techniques for detecting and reducing hallucination in LLM outputs, particularly crucial for RAG systems where factual accuracy is paramount. We have incorporated aspects of their verification methodology into our testing framework to ensure the accuracy of the retrieved information and the travel recommendations generated.

Our work builds on these foundations while addressing the challenges of testing RAG systems in tourism applications. We extend existing frameworks by incorporating domain-specific considerations and developing new metrics for evaluating the quality of travel recommendations. In particular, we adapt the RAGAS evaluation metrics and the consistency testing approaches of Wang et al. to create a comprehensive testing framework specifically tailored to tourism-focused RAG systems. We used Evidently\footnote{Evidently Website https://www.evidentlyai.com/} framework, an open source tool, to implement these testing strategies. This integration of existing methodologies with domain-specific requirements allows us to effectively assess both the technical performance and practical utility of our system.






%% Bestoun