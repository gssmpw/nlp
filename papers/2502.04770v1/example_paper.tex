%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{balance}
\usepackage{amssymb}
\usepackage[acronym]{glossaries}
\usepackage{xcolor}
% Define acronyms
\newacronym{VQ}{VQ}{vector quantizer}
\newacronym{SQ}{SQ}{scalar quantizer}
\newacronym{CL}{CL}{commitment loss}
\newacronym{STE}{STE}{straight-through estimator}
\newacronym{mSTE}{mSTE}{modified STE}
\newacronym{NA}{NA}{noise approximation}
\newacronym{MSE}{MSE}{mean-squared error}
\newacronym{MA-E}{MA-E}{mean-absolute embedding}
\newacronym{DAC}{DAC}{descript-audio-codec}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{ positioning,shapes.geometric, arrows.meta, positioning, 3d, calc}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{amsmath}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Efficient Evaluation of Quantization-Effects in Neural Codecs}

\begin{document}

\twocolumn[
\icmltitle{Efficient Evaluation of Quantization-Effects in Neural Codecs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

\begin{icmlauthorlist}
\icmlauthor{Wolfgang Mack}{yyy}
\icmlauthor{Ahmed Mustafa}{yyy}
\icmlauthor{Rafał Łaganowski}{yyy}
\icmlauthor{Samer Hijazy}{yyy}
\end{icmlauthorlist}

\icmlcorrespondingauthor{Wolfgang Mack}{womack@cisco.com}
\icmlaffiliation{yyy}{Cisco Systems, Inc., \{womack, ahmmusta, rlaganow, hijazy\}@cisco.com}


\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\begin{abstract}

Neural codecs, comprising an encoder, quantizer, and decoder, enable signal transmission at exceptionally low bitrates. Training these systems requires techniques like the straight-through estimator, soft-to-hard annealing, or statistical quantizer emulation to allow a non-zero gradient across the quantizer. Evaluating the effect of quantization in neural codecs, like the influence of gradient passing techniques on the whole system, is often costly and time-consuming due to training demands and the lack of affordable and reliable metrics. This paper proposes an efficient evaluation framework for neural codecs using simulated data with a defined number of bits and low-complexity neural encoders/decoders to emulate the non-linear behavior in larger networks. Our system is highly efficient in terms of training time and computational and hardware requirements, allowing us to uncover distinct behaviors in neural codecs. We propose a modification to stabilize training with the straight-through estimator based on our findings. We validate our findings against an internal neural audio codec and against the state-of-the-art descript-audio-codec.

\end{abstract}

\section{Introduction}
\label{submission}
Codec systems typically consist of an encoder, a quantizer, and a decoder. The encoder transforms high-dimensional data such that it can be quantized, i.e. represented with a typically much smaller number of discrete values. Storing or transmitting data in the quantized representation is crucial in modern multi-media environments as it significantly reduces storage and transmission requirements and typically does not reduce perceived quality like audio clarity or video sharpness significantly (e.g., \cite{4604423,muller24c_interspeech}).  Subsequently, the decoder reconstructs the data from the quantized representations. 

 Classical coding methods, such as JPEG \cite{jpegstandard}, MP3 \cite{mp3}, H.264 \cite{h264}, and  Opus \cite{opus} are based on signal processing and perceptual models  \cite{SpeechCodingBase,bhaskaran1995image}. Despite their historical success, these techniques struggle to achieve high efficiency at very low bitrates, where signal fidelity becomes a critical challenge (e.g., \cite{Zeghidour2022}).

With the rise of deep learning, classical methods have been combined with neural networks. Neural vocoders, for example, have been employed to reconstruct high-fidelity signals from bitstreams of classical methods, achieving unprecedented performance in speech and audio compression \cite{Kleijn2018, Klejsa2019, Garbacce2019,  Valin2019a,9632750}. Neural networks are also used as post-processing tools to mitigate compression artifacts, thereby enhancing the perceptual quality of the decoded signals \cite{Zhao2019, Korse2020, Biswas2020, Korse2022, Buthe2024}. 

End-to-end neural compression systems that bypass traditional coding elements have emerged as an alternative. These systems learn to encode, quantize, and decode signals directly from data and can be trained end-to-end \cite{ VanDenOord2017,  balle2016endtoend,Agustsson2019,Zeghidour2022, Defossez2022, NSVQ,Vali2023,  Kumar2023, Ai2024, Mentzer2024, Brendel2024}. However, training such systems requires tricks like the straight-through estimator \cite{VanDenOord2017,bengio2013ste}, soft-to-hard annealing \cite{agustsson2017soft,Jang2017,maddison2017gumbel}, or statistical quantizer emulation \cite{balle2016endtoend,9242247,NSVQ,Vali2023,Brendel2024} to allow a non-zero gradient to pass over the quantizer.  

Evaluating neural codecs is nontrivial, costly, and time-consuming because of the required training and is often not practical because of the lack of cheap and reliable metrics. Evaluation points for neural codecs, such as quantization error or decoder output, face challenges due to the non-linear systems involved. For example, a small quantization error can lead to a significant reconstruction error and vice versa. Metrics for assessing decoder outputs in the audio, image, or video domains are often either subjective and resource-intensive or objective but unreliable. For example, MUSHRA (Multiple Stimuli with Hidden Reference and Anchor) is a subjective evaluation method widely regarded as the gold standard for audio quality assessment. Although highly effective in capturing perceptual quality, MUSHRA tests require human participants, limiting their scalability to a small number of files and increasing the associated costs. Furthermore, MUSHRA scores often show limited correlation with objective metrics, a disparity that becomes particularly pronounced when evaluating generative models such as neural codecs.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[>=latex, font=\small]

        % Encoder (shrinks in height)
        \draw[fill=blue!20] (-4, 1) -- (-3, 0.5) -- (-3, -0.5) -- (-4, -1) -- cycle;
        \node[rotate=90] at (-3.7, 0) {Encoder $\mathcal{E}$};

        % Quantizer (Q)
        \draw[fill=yellow!30] (-2, 0.5) rectangle (-1, -0.5);
        \node at (-1.5, 0) {$\mathcal{Q}$};

        % Decoder (expands in height)
        \draw[fill=green!30] (0, 0.5) -- (1, 1) -- (1, -1) -- (0, -0.5) -- cycle;
        \node[rotate=90] at (0.6, 0) {Decoder $\mathcal{D}$};

        % Input and output labels above arrows
        \node[above] at (-4.7, 0) {Input I};
        \node[above] at (1.7, 0) {Output O};

        % Labels for arrows
        \node[above] at (-2.5, 0) {$E$};         % Label after Encoder
        \node[above] at (-0.5, 0) {$E_q$}; % Label after Quantizer

        % Arrows with equal spacing
        \draw[->, thick] (-5, 0) -- (-4, 0);    % Input arrow
        \draw[->, thick] (-3, 0) -- (-2, 0);    % Arrow Encoder to Q
        \draw[->, thick] (-1, 0) -- (0, 0);     % Arrow Q to Decoder
        \draw[->, thick] (1, 0) -- (2, 0);      % Arrow after Decoder

    \end{tikzpicture}
    \caption{A neural codec system with an encoder $(\mathcal{E})$ mapping the input to embeddings $E$, a quantizer ($\mathcal{Q}$) mapping $E$ to the quantized version $E_q$. The decoder $(\mathcal{D})$ maps $E_q$ to the output. }
    \label{fig:neural_codec}
\end{figure}
In this paper, we propose an evaluation framework that enables efficient investigation and evaluation of quantizers in neural codecs in a controlled way. The proposed framework consists of a simple input and target data simulation method to train a neural codec. The input data is designed to have a specified number of bits. Designing the data in that way enables the identification of the minimum number of bits required by the quantizer to reconstruct the target perfectly. The target is a rotated input version containing the same number of bits. The input and target data design are based on quantized noise processes, which ensures a resource-efficient, low-cost simulation framework. In addition to the data simulation, we propose to use a low-complexity neural codec. That way, we emulate the highly non-linear behavior before and after the quantizer in a large network by simultaneously keeping hardware requirements and training cost/time extremely low. For training the neural codec, we propose to use an interpretable cost function like the \gls{MSE} between target and estimate to evaluate the quantizer performance. The proposed loss is in contrast to real codecs, where the estimate is hard to assess regarding quality. Using the proposed framework, we evaluate fundamental properties of neural codecs by comparing training using statistical quantizer emulation to training using the \gls{STE} with and without  \gls{CL}. We find similarities between both approaches and propose a modification to stabilize training with the \gls{STE}. Finally, we verify our findings by repeating selected experiments with an internal audio codec and \gls{DAC} \cite{Kumar2023}.   

The remainder of the paper is structured as follows. Fundamental mathematical notations are introduced in Section~\ref{sec:fundamentals}. The proposed method is presented in Section~\ref{sec:prop}, followed by the experimental parameters in Section~\ref{sec:paras}. In Section~\ref{sec:Peval}, experiments are covered. Finally, Section~\ref{sec:conclusion} contains a brief conclusion.
\section{Fundamentals}
\label{sec:fundamentals}

We consider an encoder-quantizer-decoder system, as illustrated in Figure~\ref{fig:neural_codec}. The encoder $\mathcal{E}$ maps the input $I$ to embeddings $E \in \mathbb{R}^{F \times N}$, where $F$ represents the feature dimension and $N$ the frame dimension. A quantization module $\mathcal{Q}$ then maps $E$ to its quantized form $E_q$. Finally, the decoder $\mathcal{D}$ processes $E_q$ to estimate the output $O$. Depending on the application, $I$ and $O$ could be images, audio, or other signals. Without loss of generality, we refrain from specifying the exact nature of these signals. Instead, we focus on analyzing the quantization module $\mathcal{Q}$ and its effects on the neural codec. In particular, we examine its influence on the relationship between $E$ and $E_q$, as well as the gradient flow through $\mathcal{Q}$\footnote{Note that $\mathcal{Q}$ is embedded in the highly non-linear encoder-decoder system. Consequently, analyzing quantizers in the form of their quantization error is insufficient to evaluate their performance in neural networks, as a small quantization error might lead to a large output error and vice versa. }.

The gradient flow over $\mathcal{Q}$ poses a fundamental challenge: in most quantizer designs, $\mathcal{Q}$ is non-differentiable because it maps nearly continuous embeddings $E$ to discrete values or vectors in $E_q$. This non-differentiability prevents loss functions defined after $\mathcal{Q}$ from directly updating the encoder weights. Various techniques have been proposed to overcome this limitation,  including (1) the \gls{STE} \cite{VanDenOord2017,bengio2013ste}, (2) statistical training using noise-based approximations for the quantization error \cite{balle2016endtoend,9242247,NSVQ,Vali2023,Brendel2024}, and (3) soft quantization \cite{agustsson2017soft,Jang2017,maddison2017gumbel}.
\begin{figure}[t]
    \centering
    \newsavebox\myboxa
    \savebox\myboxa{%
    \begin{tikzpicture}
    \begin{axis}[scale=0.3,
        hide axis,
        colormap/cool,title style={yshift=-0.8cm}, zmin=0, zmax=1]
        \addplot3[surf,domain=-3:3,domain y=-3:3,] 
            {exp(-( (x)^2 + (y)^2)/3 )};
    \end{axis}
    \end{tikzpicture}%
    }

    % The block diagram code is probably more verbose than necessary
    \begin{tikzpicture}
    \tikzstyle{branch}=[fill,shape=circle,minimum size=3pt,inner sep=0pt]
    % Draw blocks, inputs, and outputs
    \node[label={[label distance=-0.5cm]above:{$I \in \mathbb{R}^{P\times P}$}}] (input) {\usebox\myboxa};
    
    % Sampling (rotate the box 90 degrees)
    \node[draw, rectangle, right=of input] (Sample) {\rotatebox{90}{Sampling}};
    
    % Scalar Quantization (rotate the box 90 degrees)
    \node[draw, rectangle,  right=of Sample] (Q) {\rotatebox{90}{Scalar Quantization}};
    
    % Circle with dot
    \node[right=of Q, circle, draw] (o1) {$\odot$}; % First multiplication dot
    \node[above= of o1] (rot) {Rotation Matrix $Q$};
    \node[right=of o1] (o2) {}; % Second multiplication dot

    % Arrows connecting blocks and circles
    \draw[->] (input) -- (Sample) ;  % Arrow from input to Sampling
    \draw[->] (Sample) -- (Q) node[above,midway]{$X$};  % Arrow from Sampling to Scalar Quantization
    \draw[->] (Q) -- (o1) node[above,midway]{$X_q$}; % Arrow from Scalar Quantization to first dot
    \draw[->] (o1) -- (o2)node[above,midway]{$Y$};;  % Arrow from first dot to second dot
    \draw[->] (rot) -- (o1); % Arrow from rotation matrix to first dot
    \end{tikzpicture}
       \caption{Proposed data generation pipeline. A Gaussian noise process with a $P\times P$ identity covariance matrix is sampled to obtain $X$. Each element in $X$ is quantized via scalar quantization to obtain the network target $X_q$. A rotation matrix is applied to $X_q$ to obtain the network input $Y$.  }
    \label{fig:datagen}
\end{figure}

In the case of the \gls{STE}, the decoder input $\mathcal{D}^{\text{STE}}_{\text{in}}$ during training is defined as
\begin{equation}
\mathcal{D}^{\text{STE}}_{\text{in}} = E + \text{sg}[\underbrace{E_q - E}_{Q_e}],
\label{equ:STE}
\end{equation}
where $Q_e$ is the quantization error, and $\text{sg}[\bullet]$ stops gradient tracking for $\bullet$ \footnote{Note that  $\text{sg}[(E_q-E)]$ is crucial, without it the decoder input would be $E_q = E +(E_q-E)$ which has gradients of $0$ w.r.t. $E$.}. During the forward pass, $\mathcal{D}^{\text{STE}}_{\text{in}}$ equals $E_q$, while in the backward pass, the gradient through $\mathcal{Q}$ simplifies to
\begin{equation}
\frac{\partial \mathcal{D}^{\text{STE}}_{\text{in}}}{\partial E} = 1^{F\times N}
\label{equ:derSTE}
\end{equation}
due to the sg in (\ref{equ:STE}).  According to \cite{VanDenOord2017}, a \gls{CL} is required for training to enforce $E_q$ and $E$ to be close, i.e., 
\begin{equation}
    \mathcal{L}_\text{CL} = \sum_{F,N}\frac{(E-\text{sg}[E_q])^2}{F\cdot N}.
\end{equation}
In (\ref{equ:STE}), the term $Q_e$ acts as an additive noise component not connected to the computational graph. Training using \gls{NA} follows a similar philosophy as training using \gls{STE} in the sense that a noise component is added. In \gls{NA}, noise $U $ is added to $E$ to simulate the quantization process, i.e., 
\begin{equation}
   \mathcal{D}^\text{NA}_\text{in} = E + U. 
   \label{equ:classicNA}
\end{equation}
Note that in contrast to using the \gls{STE}, the distribution of $U$ is a design choice. For example, it can be set to produce a specific embedding-to-noise ratio. The derivative of $\mathcal{D}_\text{in}^{\text{NA}}$ w.r.t. $E$ can be written as
\begin{equation}
    \frac{\partial \mathcal{D}^{\text{NA}}_{\text{in}}}{\partial E} = 1^{F\times N} + \frac{\partial U}{\partial E}.
    \label{equ:derUNA}
\end{equation}
After training using \gls{NA}, a quantizer replaces the noise addition for inference. Typical quantization modules used in neural codecs are based on a  \gls{SQ} (e.g., \cite{Brendel2024,Mentzer2024}) or a \gls{VQ} (e.g., \cite{Zeghidour2022, NSVQ}).

Soft quantization (3) offers a differentiable alternative by replacing the hard, discrete mapping of $\mathcal{Q}$ with a continuous approximation during training. The quantization of $E$ is modeled as a weighted average of discrete codebook entries or quantization levels, allowing gradient propagation through $\mathcal{Q}$. During training, the weights are forced to increasingly approximate a one-hot vector, enabling a gradual annealing from continuous to discrete values. The soft assignment probabilities are replaced during inference by a hard one-hot assignment.
\section{Proposed Method}
\label{sec:prop}
Comparing the effect of quantizers and their emulations in a neural codec is not trivial due to the lack of reliable metrics and costly due to long and computationally heavy trainings. We propose a low-complexity method to evaluate the impact of quantization on encoder-quantizer-decoder neural networks. In particular, we propose the use of surrogate data and a low-complexity surrogate model to investigate the effects of quantization. 
\subsection{Data Simulation}
\label{subsec:surrdata}

In this study, we propose using simulated input and target data for neural codec training, which are both (1) easy to generate and cost-effective and (2) contain a fixed amount of information measured in bits. This approach allows us to determine the minimum bit requirement of $\mathcal{Q}$ for perfect reconstruction while eliminating complex loss functions, thus enabling clearer comparisons between training sessions. By selecting simplified data, we also reduce the impact of encoder/decoder capacity on observed losses. A scheme of the data simulation is depicted in Figure~\ref{fig:datagen}.

We define $X \in \mathbb{R}^{P \times N}$, which consists of $N$ samples from $P$ Gaussian noise processes. To achieve objective (1), we quantize $X$ to obtain $X_q$, where the number of bits used in quantization determines the information of $X_q$. Recognizing that real-world signals, such as audio, exhibit sample correlations, we propose correlating the processes $P$ with each other via

\begin{equation}
    Y = Q \odot X_q,
\end{equation}

where $Y \in \mathbb{R}^{P \times N}$ serves as the input data for the neural codec, and $Q \in \mathbb{R}^{P \times P}$ is a rotation matrix, with $\odot$ denoting the matrix product. Note that $Q$ is derived from the QR decomposition of a $P \times P$ matrix containing samples of a white Gaussian noise process. Consequently, $Q$ only applies a rotation and does not change the eigenvalues of $X_q$, ensuring that $Q^{-1} = Q^T$, which is easily invertible.

We define the target of the neural codec as $X_q$. The proposed data definition satisfies requirements (1) and (2), as both $X_q$ and $Y$ carry a specific amount of information, are easy to simulate, and the mapping of $Y$ to $X_q$ is straightforward with

\begin{equation}
    X_q = Q^T \odot Y
\end{equation}
The simple target mitigates the effect of encoder/decoder capacity on the losses as the neural codec task is to get the estimate $\widehat{X}_q$ of $Y$, i.e., 
\begin{equation}
    \widehat{X}_q = \mathcal{D}(\mathcal{Q}(\mathcal{E}(Y))).
\end{equation}

\subsection{Neural Codec and Training}
\label{subsec:surrmodel}
The codec used for evaluation must meet the following criteria: (1) it should be fast to train and maintain low complexity, and (2) it should incorporate non-linearities similar to those in large networks such that the quantizer operates in a highly non-linear system. This is crucial for codec evaluation, as in a non-linear system, a small/large quantization error does not have to correspond to a small/large output error. 

We propose constructing the encoder-decoder using fully connected layers with skip connections to achieve these objectives. This architecture enables the model to effectively capture complex relationships within the data. The model operates on a frame basis, mapping $Y[:,n]$ to $X_q[:,n]$. Training is conducted using the \gls{MSE} loss
\begin{equation}
    \mathcal{L}_\text{MSE} = \frac{1}{F \cdot N}\sum_{F,N} (X_q - \widehat{X}_q)^2.
    \label{equ:MSELoss}
\end{equation}

\subsection{Modified Straight-Through Estimator}
We propose a \gls{mSTE} that stabilizes training neural codecs even when no \gls{CL} is used. As stated in \cite{VanDenOord2017}, the \gls{STE} leads to an unstable system as $E$ is constantly growing when no \gls{CL} is used during training. To stabilize the \gls{STE}, we propose to multiply the quantization noise with a modifier, i.e.,
\begin{equation}
    \mathcal{D}_\text{in}^{\text{mSTE}} = \underbrace{E + \text{sg}[Q_e]}_{\text{STE}}\cdot \underbrace{\frac{\sigma_{Q_e}}{\text{sg}[\sigma_{Q_e}]}}_{\text{modifier}},
    \label{equ:mSTE}
\end{equation}
where $\sigma_{Q_e} \in \mathbb{R}_{\geq 0}$ is the standard deviation of $Q_e=E_q-E$. The multiplication with the modifier is similar to the reparametrization-trick in \cite{Kingma2014}, where a noise process is connected via multiplication with an estimated standard deviation to a neural network graph. In (\ref{equ:mSTE}), we connect the quantization noise $Q_e$ to the graph by multiplying with $\sigma_{Q_e}$. To avoid changing the decoder input in the forward pass, we divide by $\text{sg}[\sigma_{Q_e}]$. That way, the modifier term is $1$ in the forward pass. Consequently, the decoder input is $E_q$ in the forward pass.  In contrast, the modifier changes the backward pass such that 
\begin{equation}
\frac{\partial \mathcal{D}^{\text{mSTE}}_{\text{in}}}{\partial E} = \mathbf{1}^{F \times N} + 
\text{sg} \left[ \frac{Q_e}{\sigma_{Q_e}} \right] \cdot\frac{\partial \sigma_{Q_e}}{\partial E} .
\label{equ:derMSTE}
\end{equation}
As $\sigma_{Q_e}$ depends on $E$ and is part of the computational graph, the proposed modification changes the update of the encoder. With the proposed modifier, $Q_e$ is normalized in the backward pass with $\sigma_{Q_e}$ in (\ref{equ:derMSTE}), stabilizing the gradients. We hypothesize that the growth of $E$ when using \gls{STE} without \gls{CL} reported in \cite{VanDenOord2017} is caused by this missing connection of $Q_e$ to the computational graph during backpropagation. Simply, the model tries to maximize the embedding-to-noise ratio by increasing the norm of $E$. Clearly, the model cannot succeed because a larger $E$ typically leads to a larger $Q_e$, which leads to an unstable process. The \gls{CL} stops the growth of $E$ at the cost of having a loss with a trivial solution when $E=0^{F\times N}$ (i.e., the information in $E$ is zero). Using the proposed \gls{mSTE}, we assume that \gls{CL} is no longer required for training.
\section{Experimental Parameters}
\label{sec:paras}
Here we present the experimental parameters we set and the data we used.

\textbf{Data}:
For $X$, we set $P=30$ and $N=2000$. We sample $X$ of a white Gaussian noise process with zero mean and variance one. For $X_q$ we use $2$~bits per value ($60$~bits per frame) and \gls{SQ} with fixed levels $\{-1.5,-0.5,0.5,1.5\}$. 

\textbf{Neural Network Plus Training} Encoder and decoder consist of $3$ fully connected layers each. We use skip connections except in the last encoder layer and the first and last decoder layer. The input and output dimensions of each layer are set to $30$. We use no activation at the encoder output and a PReLU activation for all other layers. We train for $100$ epochs, where one epoch consists of $2000$ updates with one $E$ of shape $30\times 2000$ each. We use Adam with a learning rate of $1e-4$ as an optimizer. 

\textbf{Quantizers/Quantization Emulation}: In the evaluation, we consider two training techniques, (1) \gls{SQ} and (2) NA. In both cases, we use a latent dimension of $F=30$. 

For \gls{SQ}, we use $2$~bits per value ($60$~bits per frame) and \gls{SQ} with fixed levels $\{-1.5,-0.5,0.5,1.5\}$. We mark models trained using \gls{SQ} as $\text{SQ}^\bullet_\star$ where $\star \in \{ \text{STE}, \text{mSTE}\}$ marks the gradient estimator and $\bullet \in \{ ,\text{CL}\}$ specifies whether \gls{CL} with weight of $0.1$ was used for training.

For NA, we construct $U$ by scaling a white Gaussian noise process $\mathcal{N}$ with zero mean and variance one with $\alpha \in \mathbb{R}$, i.e., \begin{equation}
U = \alpha \cdot \sigma_E \cdot \mathcal{N}(0,1) 
\label{equ:alphauat}
\end{equation}
with $20\cdot\log_{10}(\alpha\cdot \sigma_E)\in [0,8]$~dB controlling the embedding-to-noise ratio and $\sigma_E \in \mathbb{R}_{\geq 0}$ is the standard deviation of $E$. We consider two cases for NA, (1) NA as in (\ref{equ:classicNA}) and (2) $\text{NA}_\text{det}$ with detached noise, i.e., 
\begin{equation}   \mathcal{D}_\text{in}^{\text{NA}_\text{det}} = E + \text{sg}[U] 
      \label{equ:NAdet}
\end{equation}
where $\mathcal{D}^\text{$\text{NA}_\text{det}$}_\text{in}$  is the decoder input for $\text{NA}_\text{det}$. In $\text{NA}_\text{det}$, the gradients of $U$ w.r.t. $E$ are zero. For NA, the gradients of $U$ w.rt. $E$ depend on $\sigma_E$.

\textbf{Descript-Audio-Codec (DAC)}
We repeat selected experiments using the \gls{DAC} \cite{Kumar2023} implementation from \cite{descript_audio_codec}. For training, we use the dev-clean subset of the LibriTTS dataset \cite{zen19_interspeech} consisting of $8.97$ hours ($20$ male and $20$ female English speakers) and train the model at 16kHz sampling rate using the default parameters (without adversarial loss), beside activating/deactivating \gls{CL} and introducing \gls{mSTE} in the code. We refer to the respective models as $\text{DAC}^\text{CL}_\text{STE}$, $\text{DAC}^\text{CL}_\text{mSTE}$,  $\text{DAC}_\text{STE}$, and $\text{DAC}_\text{mSTE}$.

\textbf{Metrics} For evaluation purposes, we present over the training the \gls{MSE} in (\ref{equ:MSELoss}) and the mean-absolute of $E$, i.e., 
\begin{equation}
\text{MA-E} = \frac{ \lVert E\rVert_1}{F\cdot N},
\label{equ:MAE}
\end{equation}
where $\lVert E\rVert_1$ is the L1-norm of $E$.


\section{Evaluation of Selected Properties of Neural Codec Systems}
\label{sec:Peval}
\label{sec:eval}
\begin{table}[t!]
    \centering
    \caption{Memory and training time for different trainings with the proposed framework compared to DAC \cite{Kumar2023} (trained without adversarial loss) on a single P40 GPU.}
    \begin{tabular}{lcc}
        \toprule
        & Proposed Framework  & DAC \\
        \midrule
        Mem. & $< 400$ MB & 15.5 GB \\
        Time & $< 1$h  & $\approx$ 1 Week \\
        \bottomrule
    \end{tabular}
    \label{tab:mem_time}
\end{table}
\begin{figure}[t]
    \centering
    \input{Figures/Plot0loss}
    \caption{Training \gls{MSE} and \gls{MA-E} when using a no quantizer and the standard training using the \gls{STE} with \gls{CL}.}
    \label{fig:enter-label}
\end{figure}
\begin{figure}[t]
    \centering
    \input{Figures/Plot1loss}

    \caption{Training \gls{MSE} and \gls{MA-E} when using \gls{NA} or the \gls{STE} for training with and without \gls{CL}. Note that the \gls{MA-E} curves of NA and $\text{NA}^\text{CL}$ are overlapping.}
    \label{fig:NIvsSTE}
\end{figure}


In evaluating the proposed framework, we focus on fundamental properties of neural codecs. We note that further investigations not part of the article can easily and quickly be accomplished with the proposed method. From our point of view, these are but are not limited to: (1) Network layers, activations, or regularizers directly before or after the quantizer (e.g., dropout leads to correlated features such that a \gls{VQ} might work better than a \gls{SQ}), (2) Other gradient estimators like ReinMax \cite{liu2024bridging}, or SPIGOT \cite{peng2018backpropagating}, (3) different quantizers and bitrates including soft quantizers \cite{agustsson2017soft,Jang2017,maddison2017gumbel}.

A comparison of training time and required GPU memory is given in Table~\ref{tab:mem_time}. The proposed system takes less than an hour to train and requires less than $400$~MB of GPU memory. Consequently, multiple systems can be tested in parallel, enabling rapid prototyping. Compared to full-scale neural codecs like \gls{DAC}, temporal and hardware requirements are significantly reduced, leading to a significantly reduced cost.


\subsection{Experiments with the Proposed Evaluation Framework}

\textbf{No Quantizer vs. STE}: First, in Figure~\ref{fig:enter-label}, we compare the training of the proposed system without a quantizer and with $\text{SQ}_\text{STE}^\text{CL}$. As expected, the loss without quantizer approaches zero very quickly. For $\text{SQ}_\text{STE}^\text{CL}$, the model is able to reduce the loss; however, it converges to an \gls{MSE} of $\approx 0.13$. Theoretically, when $\mathcal{Q}$ has the same number of bits or more than the input, perfect reconstruction is possible. For $\text{SQ}_\text{STE}^\text{CL}$, both an input frame and the bits of the quantizer are the same (60~Bits). When doubling the bits in the quantizer to 120, the \gls{MSE} loss yields comparable results to a quantizer-free system.  The \gls{MA-E} in Figure~\ref{fig:enter-label} is stable for all models, as it converges to a fixed value over the epochs. A stable, non-diverging system is important. Otherwise, the input of the decoder constantly changes, and the model cannot learn.

\textbf{no CL vs. CL}: In Figure~\ref{fig:NIvsSTE} we compare \gls{NA} vs. \gls{STE} with and without \gls{CL}. The \gls{MSE} and \gls{MA-E} values of NA and $\text{NA}^\text{CL}$ are comparable over training, showing that \gls{CL} has no effect on them. For $\text{SQ}_\text{STE}^\text{CL}$ and $\text{SQ}_\text{STE}$ the \gls{MSE} and \gls{MA-E} show a strong difference. Without \gls{CL}, the encoder output $E$ grows unboundedly over training, whereas when using \gls{CL}, the \gls{MA-E} converges. In \cite{VanDenOord2017}, the authors mention the same behavior for training a neural codec using the \gls{STE} without \gls{CL}. The growing $E$ translates to a diverging loss for $\text{SQ}_\text{STE}$. The different effect of  \gls{CL} on \gls{NA} and \gls{STE} is remarkable. In the following, we analyze what incentivises the encoder to produce larger and larger $E$ for \gls{STE} without \gls{CL} but not for \gls{NA}. 



\begin{figure}[t]
    \centering
    \input{Figures/Plot2loss}
    \caption{Training \gls{MSE} and \gls{MA-E} when using \gls{NA} for training with and without \gls{CL}. We compare training using detached and attached noise as in (\ref{equ:NAdet}) and (\ref{equ:classicNA}), respectively. In the \gls{MA-E}, NA and $\text{NA}_\text{det}$ overlap.}
    \label{fig:NIdetvsat}
\end{figure}
\begin{figure}[t]
    \centering
    \input{Figures/Plot3loss}
    \caption{Training \gls{MSE} and \gls{MA-E} when using the \gls{STE} with and without \gls{CL}. \gls{mSTE} is the proposed modification of the \gls{STE} from (\ref{equ:mSTE}). Note that the blue and the red curve overlap in the \gls{MA-E}.}
    \label{fig:stevsmodste}
\end{figure}
\textbf{Detached vs. Attached Noise Approximation}: Investigating the growth of $E$ further for the $\text{STE}$, we analyze the effect of attached and detached noise $U$ for NA. As mentioned in Section~\ref{sec:paras}, the noise level $U$ depends on a fixed embedding-to-noise ratio. Inserting (\ref{equ:alphauat}) in (\ref{equ:derUNA}), we obtain 
\begin{equation}
    \frac{\partial \mathcal{D}^{\text{NA}}_{\text{in}}}{\partial E} = 1^{F\times N} + \alpha \cdot \mathcal{N}(\mu=0, \sigma=1)\cdot\frac{\partial \sigma_E }{\partial E},
    \label{equ:derUNA2}
\end{equation}
effectively connecting the noise level to the standard deviation of $E$ in the encoder update. In contrast, when using \gls{STE} or detached noise, 
\begin{equation}
    \frac{\partial \mathcal{D}_\text{in}^{\text{NA}_\text{det}}}{\partial E} =    \frac{\partial \mathcal{D}_\text{in}^{\text{STE}}}{\partial E} = 1^{F\times N},
\end{equation}
which does not connect the the noise ($Q_e$ or $U$) to the computational graph. Effectively, for \gls{STE} and $\text{NA}_\text{det}$, the decoder observes noisy encoder outputs $E_q$ where the noise (either $U$ or $Q_e$) is not part of the computational graph. Consequently, the encoder weights are changed so that the embedding-to-noise ratio is maximized, leading to a divergence of $E$. Figure~\ref{fig:NIdetvsat} depicts the \gls{MSE} and the \gls{MA-E} over training for \gls{NA}. Clearly, we see the expected growth of $E$ when training with $\text{NA}_\text{det}$.  Also for $\text{NA}_\text{det}^{\text{CL}}$ and $\text{NA}_\text{det}$ we see that when the noise is detached, the \gls{CL} is required to stabilize the training. Interestingly, using attached noise without \gls{CL} seems to be advantageous over detached noise with \gls{CL}, as the \gls{MSE} of $\text{NA}_\text{det}^{\text{CL}}$ is higher than of NA. Looking at the \gls{MA-E}, the norms of $\text{NA}_\text{det}^{\text{CL}}$ keep increasing, hinting at a non-sufficient weighting of \gls{CL} (used weighting is $0.1$). 

\textbf{Proposed mSTE vs. STE}: The gradients of the proposed \gls{mSTE} in (\ref{equ:derMSTE}) and of training using $\text{NA}$ in (\ref{equ:derUNA2}) exhibit explicit similarities. In particular, noise ($Q_e$ or $U$) is connected to the computational graph through a standard deviation that is based on the encoder layers ($\sigma_{Q_e}$ or $\sigma_E$). We hypothesize that this connection of the noise to the computational graph hinders the model from maximizing the embedding-to-noise ratio by increasing $E$ as this would simultaneously lead to a growth of the noise. Figure~\ref{fig:stevsmodste} depicts a comparison of the \gls{STE} and \gls{mSTE} with and without \gls{CL}. As expected, training using \gls{mSTE} does not require a \gls{CL} to have stable norms of $E$. Moreover, the \gls{MA-E} of $\text{SQ}^\text{CL}_\text{mSTE}$,  $\text{SQ}_\text{mSTE}$ and $\text{SQ}^\text{CL}_\text{STE}$ are stable, whereas the \gls{MA-E} of $\text{SQ}_\text{STE}$ diverges.  For \gls{mSTE}, the \gls{MSE} is very low compared to \gls{STE}. Interestingly, for $\text{SQ}^\text{CL}_\text{mSTE}$, we see a step function of the loss and a slower convergence compared to $\text{SQ}_\text{mSTE}$. We assume that the \gls{CL} makes the fine adjustment of $E$ difficult for low bitrates as they are drawn towards the respective quantization level. 

\subsection{Experiments with Neural Audio Codecs}
    \begin{figure}[t!]
    \centering
    \input{Figures/PlotXcodecf_norms}
    \caption{Training \gls{MA-E} over epochs for an internal neural audio codec (trained on audio). The dashed lines are the final values of trained models. We show the evolvement over the epochs for $\text{NA}_\text{det}$. After $32$ epochs, the model training crashed as $E$ grew too large. The other two models were trained for more than $1000$ epochs.}
    \label{fig:xcodec}
\end{figure}
We repeat selected experiments with an internal audio codec and with \gls{DAC} trained using audio data to show the consistency of our findings. 


\textbf{Effect of NA on Internal Speech Codec}: First, we evaluate the effect of \gls{NA} when training an internal model for speech coding. The embedding norms are depicted in Figure~\ref{fig:xcodec}. When no quantizer or NA is used, the final \gls{MA-E} are in a similar range to the \gls{MA-E} of the low-complexity model in Figure~\ref{fig:enter-label} and Figure~\ref{fig:NIdetvsat}, respectively. When detaching the added noise in $\text{NA}_\text{det}$, $E$ keeps growing till the training crashes after approximately 30~epochs. This result is consistent with the growth of $E$ for the low complexity model in Figure~\ref{fig:NIdetvsat}. 

\textbf{Effect of STE/mSTE on Descript-Audio-Codec (DAC)}:
 \begin{figure}[t!]
    \centering
    \input{Figures/DACBasemelloss}
    \caption{Mel-loss over training for \gls{DAC}. Each value is the average of $50$ updates. For $\text{DAC}_\text{STE}$, the curves are out of the range of the plot (Mel-Loss$>12$, \gls{MA-E}$>1e15$). }
    \label{fig:dacmel}
\end{figure}
We trained \gls{DAC} with and without \gls{CL} using \gls{STE} and \gls{mSTE}. The results for the Mel-Loss are illustrated in Figure~\ref{fig:dacmel}. Using $\text{DAC}_\text{STE}$, the loss rapidly diverges with an \gls{MA-E} $>1e15$ from the beginning, and the training crashes. As expected, $\text{DAC}_\text{mSTE}$ does not exhibit a diverging loss. Comparing the trainings to their counterparts with \gls{CL}, $\text{DAC}^\text{CL}_\text{STE}$ and $\text{DAC}^\text{CL}_\text{mSTE}$, both losses converge. The losses of $\text{DAC}^\text{CL}_\text{mSTE}$ and $\text{DAC}_\text{mSTE}$ are very close over the whole training. The similarity of the loss curves matches the results of the low-complexity model visualized in Figure~\ref{fig:stevsmodste}. Interestingly, the longer the network is trained, the larger the improvement of \gls{mSTE} over \gls{STE}. We assume that the stability of the \gls{MA-E} when using \gls{mSTE} contributes to that improvement. The \gls{MA-E} in Figure~\ref{fig:dacmel} show that, although marginal, $E$ is still growing for $\text{DAC}^\text{CL}_\text{STE}$ over the epochs. For $\text{DAC}^\text{CL}_\text{mSTE}$, the \gls{MA-E} is being reduced over the epochs. This can be explained as the \gls{CL} can simply be optimized by scaling down $E$ and $E_q$ as it is \gls{MSE}-based. For $\text{DAC}_\text{mSTE}$, \gls{MA-E} reaches a stable state very early in training ($\approx 100$~Epochs). Overall, the proposed \gls{mSTE} outperforms the \gls{STE} when using \gls{DAC} or the proposed efficient evaluation framework.
\section{Conclusion}
\label{sec:conclusion}
We proposed an efficient analysis framework for neural codecs based on simulated data and small neural networks. Training the proposed system required less than one hour on a GPU compared to several days/weeks for a full codec. Consequently, we significantly increased fast-prototyping speed by simultaneously reducing hardware requirements. We verified our findings against full neural codecs. Our findings led to a modification of the straight-through estimator, which stabilized training and improved the state-of-the-art descript-audio-codec. We plan to use the proposed efficient evaluation framework to analyze additional gradient estimators like ReinMax or SPIGOT and investigate the effect of different layers/activations before and after the quantizer on the neural codec. Additionally, we plan to thoroughly examine the modification of the straight-through estimator and its impact on neural codecs. 
\section{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
\clearpage
\balance
\bibliography{example_paper}
\balance
\bibliographystyle{icml2025}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
