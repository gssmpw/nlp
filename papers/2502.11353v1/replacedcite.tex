\section{Related Work}
\label{sec-spz-related-work}

\BF{Extending systolic arrays for sparse computation --}
NVIDIA introduced sparse tensor cores that accelerate sparse-dense GEMM with
one matrix having a specific 2:4 sparsity pattern (two zeros out of four
contiguous
elements)____.
VEGETA supports more flexible sparsity structure (i.e., N:4 and N:M) on a
systolic array____.
When performing sparse-dense GEMM, VEGETA keeps the sparse matrix stationary
and streams the dense matrix into a systolic array.
Each PE performs index matching to either skip or multiply two input values.
The output matrix is stored in the dense format.
Similar to SparseZipper, VEGETA is a fine-grain GEMM accelerator using a matrix
ISA extension.
Unlike NVIDIA sparse tensor cores and VEGETA, SparseZipper targets sparse-sparse
GEMM on highly sparse matrices with unstructured sparsity structures.
Processing such sparse-sparse GEMM using the sparse tensor core and VEGETA
would require several orders of magnitude more multiplications than using a
row-wise dataflow SpGEMM since matrices are highly sparse.
SparseZipper complements the sparse tensor core and VEGETA by enabling efficient execution of higher unstructured sparsity.

%Figure~\ref{fig-spz-num-mult} shows the computation demand (i.e., in the number
%of multiplications) of multiplying two matrices stored in different sparsity
%compression levels.
%SparseZipper stores both input matrices in a sparse format, which is the
%baseline in the figure.
%Sparse(1:4)-dense and sparse(1:16)-dense configurations represent two
%compression levels that can be supported in VEGETA.
%Dense-dense configuration represents a conventional systolic array that
%processes matrices in a dense format.
%Since our target matrix datasets are high sparse, the amount of computation
%needed to multiply a matrix stored in a sparse format with another matrix
%stored in a dense format is multiple orders of magnitude higher than the case
%in which both matrices are stored in a sparse format.
%SparseZipper complements the NVIDIA's sparse tensor core and VEGETA by
%expanding the spectrum of sparsity level and structure supports towards highly
%sparse and unstructured matrices.

Sparse-TPU____ proposed an offline column packing
algorithm that merges sparse columns in a matrix to minimize the number of
zeros mapped to a systolic array.
Sparse-TPU supports conditional execution to skip multiplications for values
that do not have matching indices.
However, Sparse-TPU targets only sparse-matrix dense-vector multiplication, not
sparse-sparse GEMM.
STA____ proposed a new block-sparse format that targets
matrices with an upper limit on the number of non-zeros in a block of elements.
In contrast, SparseZipper can support unstructured sparsity structures.

\BF{Software-only proposal to accelerate SpGEMM on a systolic array --}
Guo et al. proposed a software-only tiling optimization for DNN-specific SpGEMM
on a systolic array____.
Their pruning algorithm enforces a particular tile-wise sparsity pattern so
that dense tiles can be mapped directly to an underlying systolic array without
any hardware support.
However, this pruning algorithm is specific to DNN and only works for sparse
matrices generated from pruned DNN models.
In contrast, SparseZipper targets more general sparse matrices from various
domains (e.g., graph analytics) that may not have a particular sparsity
pattern.

\BF{Coarse-grain GEMM accelerators --}
Google TPU____,
Eyeriss____ and Amazon AWS neuron
core____ are some examples of coarse-grain dense-dense
GEMM accelerators that are highly inefficient when performing sparse-sparse
GEMM due to their lack of sparse format support and ability to skip ineffectual
multiplications.
Previous work has proposed various coarse-grain SpGEMM accelerators mainly
based on three different dataflows: inner product (e.g.,
SIGMA____ and Extensor____),
outer product (e.g., OuterSparse____ and
SpArch____), and row-wise product (e.g.,
MatRaptor____ and
Gamma____).
SparseZipper takes a more programmable approach that extends an existing matrix
ISA to support both dense-dense and sparse-sparse GEMM without adding
significant hardware area overhead.

\BF{Fine-grain GEMM accelerators --}
Intel AMX____, Arm
SME____, and RISC-V matrix extension
proposal____ are examples of matrix ISA
extensions for accelerating dense-dense GEMM.
RASA is an academic proposal for integrating a systolic array into an
out-of-order processor for dense-dense GEMM____.
SparseZipper extends such ISAs and micro-architectures to
efficiently support SpGEMM.
SparseCore proposed an ISA extension for sparse tensor computation by
introducing stream registers and merging/intersecting
instructions____.
%The merging instructions in SparseZipper were inspired by SparseCore.
Unlike SparseCore adding stream registers, SparseZipper leverages existing
matrix registers to store key-value streams.
%Instead of adding stream registers, SparseZipper
%leverages existing matrix registers designed to store key-value streams.
%SparseZipper does not add dedicated sparse processing units for merging and
%intersecting key-value streams.
Instead of adding dedicated sparse processing units for merging key-value
streams, SparseZipper minimally modifies an existing systolic array.