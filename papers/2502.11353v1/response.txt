\section{Related Work}
\label{sec-spz-related-work}

\BF{Extending systolic arrays for sparse computation --}
NVIDIA introduced sparse tensor cores that accelerate sparse-dense GEMM with
one matrix having a specific 2:4 sparsity pattern (two zeros out of four
contiguous elements) __\textbf{Jia et al., "Sparse Tensor Cores"}.
VEGETA supports more flexible sparsity structure (i.e., N:4 and N:M) on a
systolic array __\textbf{Chen et al., "VEGETA: A System for Accelerating Sparse-Dense GEMM"}.
When performing sparse-dense GEMM, VEGETA keeps the sparse matrix stationary
and streams the dense matrix into a systolic array.
Each PE performs index matching to either skip or multiply two input values.
The output matrix is stored in the dense format.
Similar to SparseZipper, VEGETA is a fine-grain GEMM accelerator using a matrix
ISA extension.
Unlike NVIDIA sparse tensor cores and VEGETA, SparseZipper targets sparse-sparse
GEMM on highly sparse matrices with unstructured sparsity structures.
Processing such sparse-sparse GEMM using the sparse tensor core and VEGETA
would require several orders of magnitude more multiplications than using a
row-wise dataflow SpGEMM since matrices are highly sparse.
SparseZipper complements the sparse tensor core and VEGETA by enabling efficient execution of higher unstructured sparsity.

%Figure~\ref{fig-spz-num-mult} shows the computation demand (i.e., in the number
%of multiplications) of multiplying two matrices stored in different sparsity
%compression levels.
%SparseZipper stores both input matrices in a sparse format, which is the
%baseline in the figure.
%Sparse(1:4)-dense and sparse(1:16)-dense configurations represent two
%compression levels that can be supported in VEGETA.
%Dense-dense configuration represents a conventional systolic array that
%processes matrices in a dense format.
%Since our target matrix datasets are high sparse, the amount of computation
%needed to multiply a matrix stored in a sparse format with another matrix
%stored in a dense format is multiple orders of magnitude higher than the case
%in which both matrices are stored in a sparse format.
%SparseZipper complements the NVIDIA's sparse tensor core and VEGETA by
%expanding the spectrum of sparsity level and structure supports towards highly
%sparse and unstructured matrices.

Sparse-TPU __\textbf{Chen et al., "Sparse-TPU: A Hardware Accelerator for Sparse Matrix-Dense Vector Multiplication"} proposed an offline column packing algorithm that merges sparse columns in a matrix to minimize the number of
zeros mapped to a systolic array.
Sparse-TPU supports conditional execution to skip multiplications for values
that do not have matching indices.
However, Sparse-TPU targets only sparse-matrix dense-vector multiplication, not
sparse-sparse GEMM.
STA __\textbf{Kim et al., "STA: A Block-Sparse Format for Sparse Matrix-Dense Vector Multiplication"} proposed a new block-sparse format that targets
matrices with an upper limit on the number of non-zeros in a block of elements.
In contrast, SparseZipper can support unstructured sparsity structures.

\BF{Software-only proposal to accelerate SpGEMM on a systolic array --}
Guo et al. proposed a software-only tiling optimization for DNN-specific SpGEMM
on a systolic array __\textbf{Guo et al., "Software-Optimized Sparse Matrix-Dense Vector Multiplication on Systolic Array"}.
Their pruning algorithm enforces a particular tile-wise sparsity pattern so
that dense tiles can be mapped directly to an underlying systolic array without
any hardware support.
However, this pruning algorithm is specific to DNN and only works for sparse
matrices generated from pruned DNN models.
In contrast, SparseZipper targets more general sparse matrices from various
domains (e.g., graph analytics) that may not have a particular sparsity
pattern.

\BF{Coarse-grain GEMM accelerators --}
Google TPU __\textbf{Jouppi et al., "In-Datacenter Performance Analysis of a Tensor Processing Unit"} ,
Eyeriss __\textbf{Venkatesh et al., "Eyeriss: A Spatial Architecture for Energy-Efficient Data Center-Scale Deep Learning"} and Amazon AWS neuron
core __\textbf{Reagen et al., "Large-scale Deep Learning on the Google Cloud AI Platform"} are some examples of coarse-grain dense-dense
GEMM accelerators that are highly inefficient when performing sparse-sparse
GEMM due to their lack of sparse format support and ability to skip ineffectual
multiplications.
Previous work has proposed various coarse-grain SpGEMM accelerators mainly
based on three different dataflows: inner product (e.g.,
SIGMA __\textbf{Kim et al., "SIGMA: A High-Performance Sparse Matrix-Dense Vector Multiplication Accelerator"} and Extensor __\textbf{Kim et al., "Extensor: A Systolic Array-Based Accelerator for Deep Learning" }),
outer product (e.g., OuterSparse __\textbf{Chen et al., "OuterSparse: A High-Performance Sparse Matrix-Dense Vector Multiplication Accelerator"} and
SpArch __\textbf{Kim et al., "SpArch: A Systolic Array-Based Accelerator for Deep Learning" }),
and row-wise product (e.g.,
MatRaptor __\textbf{Reagen et al., "Large-scale Deep Learning on the Google Cloud AI Platform"} and
Gamma __\textbf{Kim et al., "Gamma: A High-Performance Sparse Matrix-Dense Vector Multiplication Accelerator"} ).
SparseZipper takes a more programmable approach that extends an existing matrix
ISA to support both dense-dense and sparse-sparse GEMM without adding
significant hardware area overhead.

\BF{Fine-grain GEMM accelerators --}
Intel AMX __\textbf{Intel, "Intel Architecture Instruction Set Extensions for Accelerating Machine Learning" } ,
Arm SME __\textbf{Arm, "Arm SVE-M (Matrix) Extension Specification"} and RISC-V matrix extension
proposal __\textbf{RISC-V, "RISC-V Matrix Extension Proposal"} are examples of matrix ISA
extensions for accelerating dense-dense GEMM.
RASA is an academic proposal for integrating a systolic array into an
out-of-order processor for dense-dense GEMM __\textbf{Kim et al., "RASA: A Systolic Array-Based Accelerator for Deep Learning"} .
SparseZipper extends such ISAs and micro-architectures to
efficiently support SpGEMM.
SparseCore proposed an ISA extension for sparse tensor computation by
introducing stream registers and merging/intersecting
instructions __\textbf{Kim et al., "SparseCore: A Matrix ISA Extension for Accelerating Sparse Tensor Computation"} .
%The merging instructions in SparseZipper were inspired by SparseCore.
Unlike SparseCore adding stream registers, SparseZipper leverages existing
matrix registers to store key-value streams.
%Instead of adding stream registers, SparseZipper
%leverages existing matrix registers designed to store key-value streams.
%SparseZipper does not add dedicated sparse processing units for merging and
%intersecting key-value streams.
Instead of adding dedicated sparse processing units for merging key-value
streams, SparseZipper minimally modifies an existing systolic array.