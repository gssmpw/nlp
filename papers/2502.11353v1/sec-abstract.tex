%=========================================================================
% Abstract
%=========================================================================
% General matrix multiply (GEMM) is the key building block in many
% different domains including machine learning, graph analytics, and
% scientific computing. There have been many solutions in both
% general-purpose and domain-specific architectures accelerating GEMM
% with dense input matrices. However, matrices in workloads are not
% always dense. In fact, many traditional and emerging workload domains
% such as neural network models, graph analytics, and scientific
% simulations operate on sparse matrices where the majority of values are
% zeros. Sparse matrices are typically stored in compact formats with
% metadata indicating positions of non-zero values, which makes them
% incompatible with built-in dense matrix engines without first
% uncompressing the matrices. Previous work has proposed several ISA
% extensions on CPUs to accelerate sparse computations. In this work,
% rather than designing a completely new ISA extension for sparse
% computations on CPUs, we propose SparseZipper that enhances the
% existing matrix extensions specialized for dense GEMM to accelerate
% sparse GEMM (SpGEMM) through the evolutionary specialization approach.
% SparseZipper targets the key bottleneck, which is merging partial
% sparse vectors, streams of key-value pairs, in a conventional SpGEMM
% algorithm for data-parallel architectures. At the core of SparseZipper
% is its ability to efficiently merge such streams in parallel by
% leveraging in-place matrix registers to store parts of concurrent
% streams and built-in systolic array to merge those streams together. To
% facilitate that merge operation, we propose a minimal set of additional
% architectural states to keep track of active streams and matrix
% instructions to move streams between matrix registers and memory. Our
% performance evaluation shows SparseZipper achieves 5.98$\times$ and
% 2.61$\times$ speedup over a scalar hash-based implementation of SpGEMM
% and a vectorized SpGEMM version respectively.

% The importance of general matrix multiply (GEMM) has led to an
% emergence of matrix extensions in contemporary ISAs (e.g., Intel, Arm,
% and RISC-V) and systolic-array-based micro-architectures integrated
% into modern CPUs to accelerate multiplying two dense matrices. However,
% matrices in emerging workloads are not always dense, and sparse
% matrices where the majority of values are zeros are becoming more
% common. The existing matrix extensions and micro-architectures are not
% efficient for processing highly sparse matrices due to significant
% amount of wasted computation in multiplying with zeros and their
% incompatibility with sparse matrix formats. This work proposes
% SparseZipper that enhances existing matrix extensions and
% systolic-array-based micro-architectures specialized for dense-dense
% GEMM to accelerate sparse-sparse GEMM operating on highly sparse
% matrices with unstructured sparsity structures. Our performance evaluation
% shows SparseZipper achieves 5.98$\times$ and 2.61$\times$ speedup over
% a scalar hash-based implementation of SpGEMM and a vectorized SpGEMM
% version respectively. Our component-level area evaluation shows
% SparseZipper increases the area of a baseline 16$\times$16 systolic
% array by only 12.72\%. This overhead would be much lower when
% considering the whole system including a processor and its caches.

\begin{abstract}
  The importance of general matrix multiplication (GEMM) is motivating
  new instruction set extensions for multiplying dense matrices in almost
  all contemporary ISAs, and these extensions are often implemented using
  high-performance systolic arrays. However, matrices in emerging
  workloads are not always dense, and sparse matrices where the vast
  majority of values are zeros are becoming more common. Existing matrix
  extensions and micro-architectures cannot efficiently process highly
  sparse matrices due to two reasons: (1)~wasted work when one or both
  input values are zero; and (2)~incompatibility with sparse matrix
  formats. This work proposes SparseZipper that minimally modifies
  existing matrix extensions and systolic-array-based micro-architectures
  specialized for dense-dense GEMM to accelerate sparse-sparse GEMM
  operating on highly sparse matrices with unstructured sparsity structures.
  Our performance evaluation shows SparseZipper achieves 5.98$\times$ and
  2.61$\times$ speedup over a scalar hash-based implementation of SpGEMM
  and a state-of-the-art vectorized SpGEMM version, respectively. Our
  component-level area evaluation shows SparseZipper increases the area
  of a baseline 16$\times$16 systolic array by only 12.7\% resulting in
  an area overhead for an entire system-on-chip of just a few percent.
\end{abstract}

