%=========================================================================
% SparseZipper: Performance Evaluation
%=========================================================================

\section{Evaluation}
\label{sec-spz-perf-eval}

\input{fig-result-speedup}
\input{fig-result-exec-breakdown}
\input{fig-result-mem-accs}
\input{fig-result-mtx-inst-cnt}
%\input{fig-result-hit-rate}

In this section, we discuss cycle-level performance and first-order area
analyses of SparseZipper architecture.

\subsection{Performance Evaluation}

Figure~\ref{fig-spz-result-speedup} shows the relative performance of all
SpGEMM implementations evaluated in this work.
On average, \IT{spz} achieves 12.13$\times$, 5.98$\times$, 2.61$\times$
speedup over the three baseline versions \IT{scl-array}, \IT{scl-hash} and
\IT{vec-radix} respectively.

%----------
% scalar baseline
%----------
\BF{Scalar SpGEMM implementations --}
On average, \IT{scl-hash} is 2.03$\times$ faster than \IT{scl-array}.
For matrices that have relatively sparse outputs (e.g., \IT{p2p}, \IT{patents},
\IT{usroads}, and \IT{ndwww}), using a hash table to accumulate sparse non-zero
values is significantly more efficient than using dense arrays since each
output matrix row has a few non-zero values (i.e., shown in the average output
NNZ column in Table~\ref{tab-spz-datasets}).
In \IT{scl-array}, accesses to the dense array are scattered randomly, which
leads to low L1 data cache hit rates (e.g., less than 20\% for \IT{ndwww},
\IT{patents}, and \IT{usroads}).
In contrast, \IT{scl-hash} uses much smaller hash tables that help improve
significantly L1 data cache hit rates (e.g., close to 100\% for \IT{ndwww},
\IT{patents}, and \IT{usroads}).
A hash table's size is based on the amount of work per output matrix row
calculated in a preprocessing step.
For matrices that have relatively dense outputs (e.g., \IT{wiki}, \IT{soc},
\IT{bcsstk17}, and \IT{p3d}), \IT{scl-array} performs better than
\IT{scl-hash}.
The main reason is that accesses to a hash table for a relatively dense output
matrix cause frequent hash collisions that incur extra overheads.
In addition, those relatively dense matrices are typically smaller in sizes,
which helps improve the L1 cache hit rates.

%----------
% vector baseline
%----------
\BF{Vectorized SpGEMM implementation --}
On average, \IT{vec-radix} is 4.65$\times$ and 2.29$\times$ faster than
\IT{scl-array} and \IT{scl-hash} respectively.
Figure~\ref{fig-spz-result-exec-breakdown} shows the execution time breakdown
of \IT{vec-radix} in multiple steps.
Across all matrices, the combination of stream sorting and output generation,
which combines adjacent tuples with duplicate keys and generates final output
matrix rows, dominates the total execution time of \IT{vec-radix}.
For \IT{bcsstk17}, \IT{vec-radix} is slightly worse than \IT{scl-hash}.
The main reason is that \IT{bcsstk17} has a high ratio of per-row work to the
per-row number of output non-zeros, which indicates a high number of tuples
with duplicate keys finally compressed into a few non-zero values.
It is relatively inefficient to sort uncompressed key-value tuples with many
duplicate keys in the stream sorting step.

%----------
% matrix version
%----------
\BF{Merge-based SpGEMM using SparseZipper --}
% observation
The \IT{spz} version is 2.60$\times$ faster than the \IT{vec-radix}
implementation.
Figure~\ref{fig-spz-result-exec-breakdown} shows the execution time breakdown
of \IT{spz} in multiple steps.
The preprocessing and stream expansion steps in \IT{spz} are similar to
the ones in \IT{vec-radix}.
The proposed sorting and merging instructions targets to reduce the execution
time of the stream sorting step which dominates the execution time of
\IT{vec-radix}.
This reduction is shown in Figure~\ref{fig-spz-result-exec-breakdown} in almost
all matrices except \IT{wiki}.
It is important to note that the execution time for output generation is
decreased as well since \IT{spz} combines tuples with duplicate keys
while performing a merge sort on those tuples.
This avoids a separate compression step which is part of the output generation
in \IT{vec-radix}.

% why more efficient than vector baseline
One key reason for the higher performance is that \IT{spz} loads and stores
chunks of consecutive data using the proposed indexed matrix load-store
instructions.
Each row of a matrix register is loaded and stored using a unit-stride vector
memory micro-operation that minimizes the number of cache line accesses per
key-value chunk.
In contrast, \IT{vec-radix} uses a vectorized radix sort algorithm that
performs both long-stride and indexed vector memory accesses that span across
multiple cache lines, which results in multiple cache line accesses per vector
memory instruction.
Figure~\ref{fig-spz-result-l1d-accs} shows the significant reduction in the
number of L1 data cache accesses between \IT{vec-radix} and \IT{spz}
across all matrices.

%% explain the hit rate difference due to per-row work
%Despite efficient unit-stride vector memory accesses, \IT{spz} has lower
%L1 data cache hit rates than \IT{vec-radix} as shown in
%Figure~\ref{fig-spz-result-hit-rate} for some matrices (e.g., \IT{wiki},
%\IT{soc}, \IT{email}, \IT{bcsstk17}, and \IT{p3d}).
%Those matrices have relatively large numbers of intermediate results to merge
%per output matrix row, as shown in the per-row average work in
%Table~\ref{tab-spz-datasets}.
%Since \IT{spz} exploits parallelism across multiple output matrix rows
%(i.e., up to VLEN number of rows can be processed in parallel), \IT{spz}
%has larger aggregate working set (i.e., the average work per 16 rows in
%Table~\ref{tab-spz-datasets}) at a time than \IT{vec-radix}, which causes more
%L1 data cache misses.
%However, the latency of those misses can be hidden since indexed matrix load
%instructions can be issued out of order as soon as the next address offsets are
%produced by either \TT{mssortk} or \TT{mszipk} instruction.
%For other matrices (e.g., \IT{p2p}, \IT{patents}, \IT{usroads}, and
%\IT{m133-b3}) that have relatively low amount of work per row, the L1 data
%cache hit rate in \IT{spz} is close to the one in \IT{vec-radix}.

% explain some outliers: wiki and soc using the work variation
Since \IT{spz} processes a group of multiple streams in lock step, any
variation in lengths of those streams could impact its performance which is
determined by the processing time of the longest stream in the group.
Table~\ref{tab-spz-datasets} shows the work variation, a ratio of the work
standard deviation to the work mean, within a group of 16 consecutive matrix
rows.
The higher the work variation is, the more unbalanced the stream lengths of
adjacent matrix rows in a group are.
The relatively high work variation in \IT{wiki} and \IT{soc} explains the
relatively low performance of \IT{spz} compared to \IT{vec-radix}.
Although matrix \IT{p2p} has the highest work variation, \IT{spz}
performs well for this matrix since the average per-row work is low.
This low per-row amount of work minimizes the performance impact of high work
variation since it takes, on average, one iteration to finish processing one
key-value stream in \IT{p2p}.

% explain spz-rsort
To further demonstrate the performance impact of high work variation, we
sort matrix row indices by per-row amount of work in \IT{spz-rsort}.
It is important to note that we do not actually shuffle an input matrix's data
but simply sort row indices.
Rows with similar amount of work are then processed together.
At the end, it is necessary to shuffle the output matrix's data based on row
indices so that the final output data are sorted by their row indices.
Figure~\ref{fig-spz-result-exec-breakdown} shows the execution breakdown of
\IT{spz-rsort}.
By processing rows with similar amount of work together, the stream sorting
time in \IT{spz-rsort} is significantly reduced for matrices that have
high work variation (e.g., \IT{wiki}, \IT{soc}, \IT{ndww}, and \IT{ca-cmd}).
Figure~\ref{fig-spz-result-mtx-inst-cnt} shows the reduction in dynamic
instruction counts of \TT{mssortk} and \TT{mszipk} across matrices with high
work variation.
This reduction correlates to less number of iterations required to sort and
merge key-value streams due to more balanced work across rows in a group.
For \IT{cage11}, \IT{spz-rsort} results in a minimal reduction in the
stream sorting time since it has low work variation.
For \IT{usroads} and \IT{m133-b3}, since their average amount of work per row
is less than the vector length (i.e., 16), \IT{spz} and
\IT{spz-rsort} finish sorting each stream in one iteration on average
(i.e., only a few dynamic \TT{mszipk} instructions).

The row sorting and output data shuffling cause significant overheads in
\IT{spz-rsort}.
Row indices are sorted by a serial quick-sort routine provided in the standard
C++ library, which explains its high execution time.
Future work may extend SparseZipper to include instructions that are similar
to the stream merging and sorting for accelerating a standard merge-sort
routine that could potentially lower the row index sorting overhead.
In addition, processing rows in an order different from how their data are laid
out in memory causes a slight increase in the stream expansion time (e.g., in
\IT{patents} and \IT{scircuit}) due to poor spatial locality between rows.

\input{sec-area-result}
