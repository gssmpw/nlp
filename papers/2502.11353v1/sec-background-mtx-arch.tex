%-------------------------------------------------------------------------
% Background on matrix extensions
%-------------------------------------------------------------------------

\section{Background: Modern Matrix Architectures}
\label{sec-spz-mtx-ext-background}

\input{fig-gemm-systolic-array}

% dedicated matrix-multiply accelerators
The ever-growing importance of GEMM performance and efficiency in emerging
workloads such as machine learning has driven architects to design and
integrate accelerators for dense GEMM in modern systems.
For example, Google introduced its Tensor Processing Unit (TPU) as a
co-processor next to a general-purpose CPU for accelerating training and
inference kernels in machine learning
workloads~\cite{jouppi-datacenter-isca2017,teich-google-tpu-v2-blog2018,jouppi-google-tpu-v2-v3-cacm2020,jouppi-tpu-v4-isca2023}.
At the heart of TPU is a large matrix-matrix multiply unit that significantly
improves the performance and energy efficiency of multiplying two dense
matrices compared to contemporary CPUs and GPGPUs.
NVIDIA also integrates tensor cores specialized for multiplying and adding
matrices in its recent GPUs~\cite{choquette-nvidia-tensor-core-2021}.

% instruction set for matrix-multiply operation
The need for accelerating GEMM has pushed specialization for dense GEMM further
into modern general-purpose CPU instruction sets as well.
Arm recently released its Scalable Matrix Extension (SME) that introduces a new
instruction performing an outer product of two vectors and accumulating its
results into a new two-dimensional accumulator register
state~\cite{arm-sme-web}.
IBM took a similar approach in its Matrix-Multiple Assist (MMA) extension for
the Power ISA~\cite{ibm-mmx-assist-web}.
Intel introduced a new Advanced Matrix Extension (AMX) that adds several
two-dimensional matrix register states called tile registers and a new
matrix-matrix multiply instruction performing a matrix multiplication on two
input tile registers~\cite{intel-amx-web,nassif-intel-sapphire-isscc2022}.
The RISC-V community is also working on a matrix extension
proposal~\cite{riscv-mtx-ext-proposal-web} that is similar to Intel AMX's
approach.

% micro-architecture
Regardless of programming abstractions (e.g., accelerator-based interfaces and
instruction sets), specialization for dense GEMM is typically implemented in
hardware using a two-dimensional systolic array consisting of multiply-add
processing elements
(PEs)~\cite{jouppi-datacenter-isca2017,jeong-rasa-dac2021,nassif-intel-sapphire-isscc2022}.
An implementation of a systolic array can be either input-, weight-, or
output-stationary (i.e., which input/output matrix stays inside the systolic
array throughout the computation), depending on its programming abstraction.
The integration of a matrix-multiply unit and a general-purpose CPU can be
either coarse-grained (e.g., as a co-processor like TPU), medium-grained (e.g.,
sharing some levels of caches with the CPU), or fine-grained (e.g., as a
functional unit in the CPU's pipeline).

% explain the baseline micro-architecture
Figure~\ref{fig-spz-gemm-systolic-array} shows a weight-stationary systolic
array for accelerating dense GEMM.
The array consists of multiple PEs connected in a two-dimensional mesh network.
Each PE receives input data from its west and north input ports, sends
output value (i.e., partial sum of an output matrix element) to its south
neighbor PE, and forwards input data to its east neighbor PE.
Each PE consists of a weight register for keeping weight values that are
multiplied with input values over multiple iterations in a weight-stationary
GEMM implementation.
In addition, a multiply-accumulate (MAC) unit is used to multiply a weight
value (i.e., in the weight register) with an input value (i.e., from the west
input port) and accumulate the result into an output value (i.e., from the
north input port).
There are skew buffers in the west and north input ports so that input data are
staggered in time when entering the systolic array.
