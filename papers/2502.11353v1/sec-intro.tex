%=========================================================================
% SparseZipper: Introduction
%=========================================================================

\section{Introduction}
\label{sec-spz-intro}

% *** Hardware trend: matrix extensions on CPUs ***
%   + Matrix extensions for CPUs are coming
%   + Why? --> accelerate dense matrix-matrix multiplications
%   + How? --> in the form of large 2D MAC array and work closely with existing vector architectures
%   + Benefits? --> being close to CPUs

General matrix multiply (GEMM) is a key building block in many different
domains including machine learning, graph analytics, and scientific
computing. Therefore, numerous domain-specific architectures have been
proposed to accelerate dense-dense GEMM (i.e., most values in both input
matrices are non-zeros) with various trade-offs in programmability,
performance, and energy
efficiency~\cite{jouppi-datacenter-isca2017,teich-google-tpu-v2-blog2018,chen-eyeriss-v2-jetcas2019,jouppi-google-tpu-v2-v3-cacm2020,choquette-tensor-core-nvidia-ieeemicro2021}.
In addition to coarse-grain accelerators, CPU vendors have recently
introduced matrix extensions (e.g., Intel's Advanced Matrix Extension
(AMX)~\cite{intel-amx-web,nassif-intel-sapphire-isscc2022,jeong-rasa-dac2021},
Arm's Scalable Matrix Extension (SME)~\cite{arm-sme-web}, RISC-V's matrix
extension proposal~\cite{riscv-mtx-ext-proposal-web}, and IBM's
Matrix-Multiply Assist (MMA)~\cite{ibm-mmx-assist-web}) to their ISAs for
dense-dense GEMM acceleration. Such matrix extensions attempt to strike a
balance between programmability and efficiency, and they are often
implemented using systolic-array-based
micro-architectures~\cite{intel-amx-web,nassif-intel-sapphire-isscc2022}.

% *** Software trend: Sparse computation ***
%   + Data are sparse and in many cases extremely sparse -> motivate the challenge
%   + Unstructured sparsity -> motivate the importance of a general solution
%   + Why dense hardware is no longer enough to do sparse?

However, matrices in workloads are not always dense. In fact, many recent
neural network
models~\cite{reddi-mlperf-isca2020,naumov-dnn-model-arxiv2019,han-deep-compress-arxiv2015,jouppi-datacenter-isca2017,wu-ml-facebook-hpca2019},
real-world graph
analytics~\cite{davis-graphblas-tmos2019,hoefler2011generic,shun-multicore-tc-2015},
and scientific
simulations~\cite{canning-sparse-sim-1996,galli-quantum-sim-1996} operate
on sparse matrices where the majority of values are zeros. In addition,
matrix densities (i.e., the percentage of non-zero values in a matrix)
vary dramatically across domains (e.g., from $10^{-6}\%$ density in
matrices representing social graphs to 50\% density in matrices used in
neural network models~\cite{hegde-extensor-micro2019}). Such low matrix
densities prevent computing GEMM for sparse matrices efficiently on CPUs
using the recently introduced matrix extensions since most
multiplications will involve at least one input value which is zero.
Moreover, sparse matrices are typically stored in compact formats with
metadata indicating positions of non-zero values for space efficiency, so
they are not directly compatible with existing built-in matrix engines
specialized for processing matrices stored in a dense format.

% *** Existing solutions (only closely related work) ***
%   + SparseCore - specialized ISA extension just for sparse computation
%   + VEGETA
%   + SparseTPU? Should we include this? It's not really an ISA extension

In addition to numerous domain-specific sparse-sparse GEMM (SpGEMM)
accelerators~\cite{qin-sigma-hpca2020, hegde-extensor-micro2019,
  zhang-sparch-hpca2020, srivastava-matraptor-micro2020,
  zhang-gamma-asplos2021}, previous work has proposed several ISA
extensions to accelerate sparse computations.
SparseCore~\cite{rao-sparsecore-asplos2022} is a stream-based ISA
extension designed specifically for sparse computations at the cost of
extra hardware for stream registers and stream processing units without
efficiently supporting dense-dense GEMM. VEGETA extends a matrix
extension to accelerate sparse-dense matrix-matrix multiplication (SpMM)
in addition to dense computations~\cite{jeong-vegeta-hpca2023}. However,
VEGETA is limited to SpMM and DNN-specific sparsity structures, so it is
not efficient when multiplying two highly sparse (i.e., less than 1\%
density) matrices with unstructured sparsity structures, which is critical in
various workload domains including graph analytics (e.g., multi-source
breadth-first search, peer pressure clustering, cycle detection, triangle
counting,
etc.)~\cite{dalberto-all-pair-spgemm-2007,shah-graph-spgemm-2007,rabin-maximum-matching-alg-1989,azad-triangle-count-2015},
hybrid linear solvers (e.g., Schur complement method and algebraic multi-grid
methods)~\cite{yamazaki-spgemm-schur-2010}, context-free grammar
parsing~\cite{penn-context-free-grammar-2006}, molecular dynamics
simulatio~\cite{itoh-order-n-spgemm-1995}, and interior point
methods~\cite{karypis-interior-point-alg-1994}.

% *** Our solution ***
% Augment matrix extensions to accelerate SpGEMM

In this work, we propose SparseZipper that minimally extends existing
matrix ISAs and systolic-array-based micro-architecture specialized for
dense-dense GEMM to accelerate SpGEMM operating on highly sparse matrices
with unstructured sparsity structures. SparseZipper targets a conventional
row-wise dataflow SpGEMM algorithm (i.e., Gustavson algorithm) with sparse
matrices represented in commonly used compressed sparse row/column
(CSR/CSC) formats. The abstraction and micro-architecture of SparseZipper
are specialized for accelerating the algorithm's main performance
bottleneck which involves merging multiple sparse vectors represented as
streams of indices (i.e., keys) and data (i.e., values).
By leveraging existing matrix registers for storing key-value streams and a
systolic array for merging multiple streams, SparseZipper incurs minimal area
overhead.
Our performance evaluation shows SparseZipper achieves 5.98$\times$ and
2.61$\times$ speedup over a scalar hash-based implementation of SpGEMM and a
state-of-the-art vectorized SpGEMM version, respectively.
Our component-level area evaluation shows SparseZipper increases the area of a
baseline 16$\times$16 systolic array by only 12.7\%.
This overhead would be much lower when considering an entire processor and its
caches.

%SparseZipper targets the key bottleneck, which is merging partial sparse
%vectors, in a conventional SpGEMM algorithm for data-parallel
%architectures~\cite{li-merge-spmspv-vectorarch-2018,li-spgemm-vector-arch-mchpc2019,fevre-spgemm-rvv-arxiv2023,winter-adaptive-spgemm-gpu-ppopp2019,liu-efficient-spgemm-gpu-ipdps2014,dalton-optimizing-spgemm-gpu-ipdps2015}.
%Each partial sparse vector is considered as a stream of keys (i.e.,
%representing row/column indices of non-zeros in a matrix) and corresponding
%non-zero values.
%At the core of SparseZipper is its ability to efficiently merge such streams in
%parallel by leveraging in-place matrix registers to store parts of concurrent
%streams and built-in systolic array to merge those streams together.
%In order to facilitate that merge operation, we propose a minimal set of
%additional architectural states to keep track of active streams and matrix
%instructions to move streams between matrix registers and memory.
%Our performance evaluations show SparseZipper achieves 5.98$\times$ and
%2.61$\times$ speedup over a scalar hash-based implementation of SpGEMM and a
%vectorized SpGEMM version respectively.
%Our post-synthesis area evaluation shows SparseZipper incurs less than XX\%
%area overhead compared to the baseline matrix engine designed for dense GEMM.

% *** Our contributions ***
\BF{Contributions --} Our key contributions include: (1)~a SparseZipper
ISA extension that enhances an existing matrix ISA to efficiently support
merging multiple key-value streams, the main performance bottleneck in
the conventional row-wise dataflow SpGEMM algorithm; (2)~a minimal set of
micro-architectural changes to a systolic array to support the new
SparseZipper instructions; and (3)~a detailed cycle-level evaluation
demonstrating the performance benefits of SparseZipper and a first-order
area evaluation demonstrating the minimal additional hardware needed for
SparseZipper.

%% Motivate sparse computation
%% + Important in various domains
%% + Example in graph analytics
%
%\paragraph{Motivating sparse matrix computations} Computation on sparse data,
%where a majority of values are zeros, lies at the heart of many important
%application domains such as scientific
%computing~\cite{gilbert-hpc-graphs-2007}, graph
%analytics~\cite{davis-graphblas-tmos2019,hoefler2011generic,shun-multicore-tc-2015},
%machine
%learning~\cite{reddi-mlperf-isca2020,naumov-dnn-model-arxiv2019,han-deep-compress-arxiv2015,jouppi-datacenter-isca2017,wu-ml-facebook-hpca2019},
%and simulation~\cite{canning-sparse-sim-1996,galli-quantum-sim-1996}.
%For instance, in the domain of graph analytics, a graph can be represented as a
%sparse adjacency matrix in which non-zero values in the matrix represent
%connections among vertices.
%Primitive graph operations (e.g., traversal and triangle counting) can be
%expressed in algebraic kernels such as sparse-matrix sparse-vector (SpMSpV) and
%sparse-matrix sparse-matrix (SpMSpM) multiplication~\cite{}.
%Such adjacency matrices are often highly sparse (e.g., only 0.2\% of elements
%in WikiVote network's adjacency matrix are non-zeros).
%
%% Why is it challenging?
%% + Sparse data -> most are zeros -> low arithmetic intensity & memory-bound
%% + Data dependent: matrix sparsity, structure, etc.
%% + Domain dependent
%% + Various data flows
%
%\paragraph{Challenges in sparse matrix computations} The high sparsity of data
%represented in matrix and vector format pose several computational challenges.
%First, using algorithms designed for processing dense data is algorithmically
%inefficient due to wasted multiplications with zero values.
%Therefore, sparse matrices and vectors are often represented in compact formats
%such as compressed sparse row/column (CSR/CSC) so that only non-zero values are
%stored and effectively processed.
%Second, computation on sparse data requires only few arithmetic operations per
%loaded data, which makes it inherently memory-bound.
%Third, the efficiency of sparse computation is largely dependent on input data.
%Previous work has proposed various custom compact matrix formats~\cite{} to
%exploit certain structures of non-zero values and a wide range of data-flows
%(e.g., inner product, outer product, and Gustavsons for SpMSpM) for particular
%levels of matrix sparsity.
%
%% Approach #1: General-purpose compute: GPGPU, vector, and multi-core
%
%\paragraph{General-purpose compute} multi-core CPU~\cite{},
%GPGPU~\cite{merrill-merge-spmv-gpu-sc2016}, vector
%architectures~\cite{nagasaka-intel-knl-icpp2018,li-merge-spmspv-vectorarch-2018}
%
%% Approach #2: Domain-specific accelerator
%
%\paragraph{Domain-specific accelerators} MatRaptor~\cite{}, GraphLily~\cite{},
%OuterSPACE~\cite{}, SpArch~\cite{}, Spada~\cite{}, ExTensor~\cite{},
%GAMMA~\cite{}, SIGMA~\cite{}
%
%% Approach #3: ISA extensions: sparse core
%%   - Middle ground
%%   - Try to argue that this is a right approach but existing work is not good
%%   enough (e.g., SparseCore - substantial architectural changes, VEGETA -
%%   specialized for a certain domain, SAVE - extending vector engine)
%
%\paragraph{ISA extensions} SparseCore~\cite{rao-sparsecore-asplos2022},
%VEGETA~\cite{jeong-vegeta-hpca2023}, SAVE~\cite{gong-save-micro2020}
%
%% Emerging matrix architectures
%%   - Why? and how?
%
%\paragraph{Emerging matrix architectures} Intel
%AMX~\cite{intel-amx-web,nassif-intel-sapphire-isscc2022}, Arm
%SME~\cite{arm-sme-web}, IBM matrix-multiply assist~\cite{ibm-mmx-assist-web},
%and RISC-V matrix extension proposal~\cite{riscv-mtx-ext-proposal-web}
%
%% Our solution
%%
%\paragraph{Our solution}
%
%% Our contributions
%\paragraph{Contributions} Our key contributions include:
%(1) an ISA extension that includes new matrix instructions to efficiently
%support merging streams of keys and values, a key operation in
%merge-based sparse matrix computations,
%(2) an implementation of SpMSpV and SpMSpM using the new matrix instructions
%and existing vector instructions,
%(3) a minimal set of micro-architectural changes to a systolic array-based
%matrix engine to support the new stream merging operation, and
%(4) a detailed cycle-level performance evaluation.
