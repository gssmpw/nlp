\renewcommand{\thesubfigure}{\alph{subfigure}}  %
\begin{figure*}[t!]

    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/main_results/halfcheetah-random_AORL,RLPD,SACfd.pdf}
        \caption{halfcheetah-random}\label{fig:exp:1}
    \end{subfigure}\hfil
        \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/main_results/walker2d-random_AORL,RLPD,SACfd.pdf}
        \caption{walker2d-random}\label{fig:exp:2}
    \end{subfigure}\hfil
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/main_results/antmaze-umaze_AORL,RLPD,SACfd.pdf}
        \caption{antmaze-umaze}\label{fig:exp:3}
    \end{subfigure}\hfil
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/main_results/ant-random_AORL,RLPD,SACfd.pdf}
        \caption{ant-random}\label{fig:exp:4}
    \end{subfigure}\hfil

    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/main_results/halfcheetah-medium-replay_AORL,RLPD,SACfd.pdf}
        \caption{halfcheetah-medium-replay}\label{fig:exp:5}
    \end{subfigure}\hfil
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/main_results/walker2d-medium-replay_AORL,RLPD,SACfd.pdf}
        \caption{walker2d-medium-replay}\label{fig:exp:6}
    \end{subfigure}\hfil
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/main_results/antmaze-medium-play_AORL,RLPD,SACfd.pdf}
        \caption{antmaze-medium-play}\label{fig:exp:7}
    \end{subfigure}\hfil  
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/main_results/halfcheetah-expert_AORL,RLPD,SACfD.pdf}
        \caption{halfcheetah-expert}\label{fig:exp:8}
    \end{subfigure}\hfil


    

    
    \caption{\textbf{Main results.} A comparison between \algname, the state-of-the-art RLPD baseline and SAC with offline data (SACfD) on various D4RL benchmark tasks. %
    Shaded areas represent 1 standard deviation based on ten seeds. \algname scores the best in all benchmarks. 
    }\label{fig:exp:main}
        \vspace{-3mm}
\end{figure*}


\section{Theoretical Analysis}\label{sec:theoretical-analysis}





In this section, we derive the priority term theoretically from the performance difference lemma~\citep{kakade2002approximately} and show that our active advantage-aligned sampling strategy leads to improved policy performance. Furthermore, we establish a theoretical lower bound on the performance improvement gap under our sampling scheme.  

Recall from \secref{sec:Preliminaries}, we define the entropy-regularized objective as:
\begin{equation*}
    J_\alpha^\pi = \mathbb{E}_{\state\sim\rho^{\policy},\action\sim\policy} \left[\sum_{t=0}^{\infty} \gamma^t \left(\reward_t+\alpha \mathcal{H}(\policy(\action|\state))\right) \right].
\end{equation*}






\begin{restatable}[]{thm}{perfdiff}\label{thm:perfdiff} 
Suppose the Q-function class is uniformly bounded, and for any Q-function, the corresponding optimal policy lies within the policy function class.  
Let $\epsilon^t$ denote the $\ell_2$ error of the Q-function in the critic update step.  
Let $\policy^t$ be the policy at iteration $t$ in \algname, updated using priority-weighted sampling with $w(\state,\action)\exp(\gamma\cdot\AFunc(\state,\action))$. Then, the following lower bound holds:
\begin{equation*}
    J_{\alpha}^{\pi^{t+1}} - J_{\alpha}^{\pi^t} \geq J_{\alpha}^{\pi^\star} - J_{\alpha}^{\pi^t} - C \sqrt{\epsilon^t} \sup_{s, a} \left| R^t(s, a;\gamma) \right|,
\end{equation*}
where $J_{\alpha}^{\pi^\star} - J_{\alpha}^{\pi^t}$ represents the maximum possible improvement if the true Q-function were known, and the function $R^t(s, a;\gamma)$ is given by:
\begin{align*}
    R^t(s, a;\gamma) &= \left(\frac{{\pi^{t+1}}(a\given s)}{d^{\text{on}}(a\given s)}\right)^{1-\gamma} \cdot \\ 
    &\quad \frac{\sum_{s', a'}d^{\text{on}}(a'\given s') \pi^{t+1}(a'\given s')^\gamma}{d^{\text{on}}(a\given s)^\gamma} \cdot \frac{d^{\pi^{t+1}}(s)}{d^{\text{on}}(s)}.
\end{align*}
\end{restatable}
The proof is provided in Appendix~\ref{app:theory_proof}.



\paragraph{Comparison to random sampling.} The fundamental concept behind proving that our sampling technique surpasses random sampling and contributes to positive policy improvement involves initially applying the performance difference lemma. 
This approach yields the performance differential term $J\paren{\policy^{t+1}}-J\paren{\policy^t}$ between the updated policy and the current policy.
Our goal is to demonstrate that this term is non-negative under our sampling priority. To do this, we prove that by a shift of distribution, this term is no less than the gap
\begin{align}
J^{\pi^\star} - J^{\pi^t} - C \sqrt \epsilon \sup_{s, a}  |  {d^{\pi^{t+1}}(s, a)} / {\rho(s, a)} |.
\end{align}
When looking at the distribution shift
\begin{align*}
    \frac{d^{\pi^{t+1}}(s, a)}{\rho(s, a)} =&\left(\frac{{\pi^{t+1}}(a\given s)}{d^{\text{on}}(a\given s)}\right)^{1-\gamma}  \notag \\
    &  \cdot \frac{\sum_{s', a'}d^{\text{on}}(a'\given s') \pi^{t+1}(a'\given s')^\gamma}{d^{\text{on}}(a\given s)^\gamma} \cdot \frac{d^{\pi^{t+1}}(s)}{d^{\text{on}}(s)},
\end{align*} 
we notice the shift between online/offline dataset is canceled, and the remaining terms comprise a shift term $ {d^{\pi^{t+1}}(s)}/{d^{\text{on}}(s)}$ that characterizes how well the online data cover the visitation measure induced by the next policy, and another term that characterizes the shift in policy. In the sequel, we will see through an example why using some proper $\gamma$ helps reduce the shift in policy. 



\paragraph{Why does advantage weighting help?}
We first show that under certain conditions, the ratio $R^t(s, a)$ will decrease for increased $\gamma$. Since $\gamma$ does not influcence the ration between the state distribution, let us just consider the bandit case with ratio 
\begin{align*}
    R^t(a;\gamma) = \left(\frac{{\pi^{t+1}}(a)}{d^{\text{on}}(a)}\right)^{1-\gamma}  \cdot \frac{\sum_{a'}d^{\text{on}}(a') \pi^{t+1}(a')^\gamma}{d^{\text{on}}(a)^\gamma}.
\end{align*}
Suppose the online data distribution $d^{\text{on}}(a)\propto \exp(\beta_1 r(a))$ for some parameter $\beta$ while the policy $\pi^{t+1}(a) \propto \exp(\beta_2 r(a))$ for some parameter $\beta_2 > \beta_1$. This is reasonable since the policy updates faster than the online buffer.
In this case, we have 
\begin{align*}
    &\log \left(\frac{R(a;\gamma)}{R(a;0)}\right) \\
    &\quad= -\gamma \log(\pi^{t+1}(a)) + \log \left(\sum_{a'}d^{\text{on}}(a') \pi^{t+1}(a')^\gamma \right) \\
    &\quad = -\gamma \beta_2 r(a) + \log \left(\sum_{a'}\exp((\beta_1 + \beta_2 \gamma) r(a')) \right).
\end{align*}
Consider the gradient of $\log \left(\sum_{a'}\exp((\beta_1 + \beta_2 \gamma) r(a')) \right)$ with respect to $\gamma$:
\begin{align*}
    &\frac{\partial}{\partial \gamma} \log \left(\sum_{a'}\exp((\beta_1 + \beta_2 \gamma) r(a')) \right)\\ 
    &\quad = \frac{\sum_{a'} \beta_2 r(a') \exp((\beta_1 + \beta_2 \gamma) r(a'))}{\sum_{a'}\exp((\beta_1 + \beta_2 \gamma) r(a'))} - \beta_2 r(a).
\end{align*}
Note that the largest probability ratio happens for $\tilde a = \argmax_{a'} r(a')$. Thanks to the softmax function in the gradient, we see that for $\tilde a$, the derivative is negative, meaning that by increasing $\gamma$, the value of $R(a;\gamma)$ will decrease, hence leading to a better policy improvement in the theorem.




















