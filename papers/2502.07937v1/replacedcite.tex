\section{Related Work}
\label{sec:related}

\paragraph{Offline to online RL}%








In an effort to mitigate the sample complexity of online RL____,
offline RL utilizes fixed datasets to train policies without online interaction, however it can be prone to extrapolation errors that lead to overestimation of state-action values.
Recent off-policy actor-critic methods____ seek to mitigate these issues by limiting policy learning to the scope of the dataset, thereby minimizing extrapolation error. 
Strategies for reducing extrapolation error include value-constrained approaches____ that aim for conservative value estimates and policy-constrained techniques____ that ensure the policy remains close to the observed behavior in the data.

While offline RL methods can outperform the datasetâ€™s behavior policy, they rely entirely on static data____. When the dataset has comprehensive coverage, methods like FQI____ or certainty-equivalence model learning____ can efficiently find near-optimal policies. %
However, in practical scenarios with limited data coverage, policies tend to be suboptimal. 
One approach to addressing this suboptimality is to follow offline RL with online fine-tuning, however ask discussed above, existing methods are prone to catastrophic forgetting and performance drops during fine-tuning____. In contrast, \algname starts with online RL and then incorporates offline data to improve the policy.


\paragraph{Online RL with offline datasets}
\vspace{-0.3cm}


Several methods exist that incorporate offline datasets in online RL to enhance sample efficiency. %
Many rely on high-quality expert demonstrations____. 
____ introduced the Advantage Weighted Actor Critic (AWAC), which utilizes regulated policy updates to maintain the policy's proximity to the observed data during both offline and online phases. On the other hand, ____ propose an initially pessimistic approach to avoid over-optimism and bootstrap errors in the early online phase, gradually reducing the level of pessimism as more online data becomes available.
\MW{I commented out the text that provided more detail about the methods cited above to save space, however we can add it back if you think it is particularly helpful.}
Most relevant to our work is RLPD____, which %
adopts a sample-efficient off-policy approach to learning that does not require pre-training. Like RLPD, our method does not depend on data quality and avoids constraining the policy with a behavior cloning term. Unlike RLPD, which utilizes symmetric sampling to evenly draw from both online and offline datasets for policy improvement, \algname adopts a Prioritized Experience Replay (PER)-style method, whereby it selectively uses data from both datasets to enhance policy performance.















\paragraph{Prioritized experience replay}
\vspace{-0.3cm}
Experience replay____ enhances data efficiency in online RL by reusing past experiences.
Traditional experience replay methods select experience transitions randomly from a memory buffer, sampling at a uniform rate without regard to their importance. %
Priority Experience Replay (PER)____ introduces prioritization based on temporal difference (TD) error to ensure that impactful experiences are used more frequently, and has proven effective in a variety of settings____. 
Alternative prioritization strategies have been explored, such as prioritizing transitions based on expected return____ or adjusting sample importance based on recency____. 
Existing research predominantly focuses on either purely online or offline applications of PER. Our research distinctively integrates the advantages of both online and offline data in an innovative way. The most relevant studies to ours include ____ that uses the density ratio between off-policy and near-on-policy state-action distributions as an importance weight for policy evaluation, and ____ that employs density ratios to select relevant samples from offline datasets. Our method differs by not only using the density ratio to assess the ``on-policyness'' of data but also considering the advantage value to determine how data can enhance policy improvement.


\paragraph{Active learning in RL}
\vspace{-0.3cm}
Active learning has been explored in RL for data-efficient exploration %
____. %
Unlike previous approaches that focus on %
oracle selection____, state exploration____ or reward estimation____,
\algname introduces an active transition sampling mechanism tailored to online RL with offline data, prioritizing transitions that maximize policy improvement.