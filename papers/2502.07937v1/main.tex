\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage[dvipsnames]{xcolor}

\definecolor{myblue}{RGB}{31, 119, 180}
\definecolor{myorange}{RGB}{255, 127, 14}
\definecolor{mygreen}{RGB}{44, 160, 44}
\definecolor{myred}{RGB}{214, 39, 40}


\usepackage[accepted]{icml2025}









\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage[allcolors=NavyBlue,colorlinks=true,backref=page]{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xspace}
\usepackage{subcaption}

\usepackage{wrapfig}
\usepackage{tabularx}


\usepackage[noend]{algpseudocode}
\usepackage{algorithm}


\usepackage{cite}
\usepackage{comment}
\usepackage{prettyref}
\usepackage{amsfonts}
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{multicol}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsmath,mathtools,thmtools}
\usepackage{enumitem}

\renewcommand{\thesubfigure}{(\alph{subfigure})}

\usepackage{thm-restate}


\usepackage[textsize=footnotesize]{todonotes}
\usepackage{xspace}

\usepackage[most]{tcolorbox}
\definecolor{colorcomment}{RGB}{160, 190, 210}%
\makeatletter
\algnewcommand{\LineComment}[1]{\Statex \hskip\ALG@thistlm \(\triangleright\) 
{\color{colorcomment}#1}}
\makeatother
\newcounter{exa}
\makeatletter
\algnewcommand{\IndentLineComment}[1]{\Statex \hskip\ALG@tlm \(\triangleright\) {\color{colorcomment}#1}}
\makeatother





\input{macros}






\author{%
}

\icmltitlerunning{Active Advantage-Aligned  Online Reinforcement Learning with Offline Data}

\begin{document}

\twocolumn[
\icmltitle{Active Advantage-Aligned  Online Reinforcement Learning with Offline Data}






\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xuefeng Liu}{uchicago}
\icmlauthor{Hung T. C. Le}{uchicago}
\icmlauthor{Siyu Chen}{yale}
\icmlauthor{Rick  Stevens}{uchicago}
\icmlauthor{Zhuoran Yang}{yale}
\icmlauthor{Matthew R.\ Walter}{ttic}
\icmlauthor{Yuxin Chen}{uchicago}
\end{icmlauthorlist}



\icmlaffiliation{ttic}{Toyota Technological Institute at Chicago, Chicago, IL, USA}

\icmlaffiliation{uchicago}{Department of Computer Science, University of Chicago, Chicago, IL, USA}
\icmlaffiliation{yale}{Department of Statistics and Data Science, Yale University, New Haven, CT, USA}
\icmlcorrespondingauthor{Xuefeng Liu}{xuefeng@uchicago.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]




\printAffiliationsAndNotice{\icmlEqualContribution} %


\input{abstract}
\input{introduction}
\input{related_works}
\input{background_problemStatement}
\input{algorithm}
\input{analysis}
\input{experiments}


\clearpage

\input{acknowledgement}
\section*{Impact Statement}
This paper presents work whose goal is to advance the reinforcement learning field of machine learning. 
Active advantage-aligned RL with offline data has a few potential societal consequences, 
none of which we feel must be specifically highlighted here.

\bibliographystyle{plainnat}
\bibliography{reference}

\clearpage
\onecolumn






\appendix
\clearpage


\input{supp_additional_theory}







\end{document}
