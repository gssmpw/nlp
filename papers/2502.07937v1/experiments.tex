

\renewcommand{\thesubfigure}{\alph{subfigure}}  %
\section{Experiments}


In this section, we perform an empirical study of \algname, compared to state-of-the-art RLPD baseline, as well as SAC with offline data (SACfD). We find that \algname outperforms the completing baselines in most scenarios.



\paragraph{Environments}
We evaluate \algname on both dense and sparse reward tasks from the D4RL benchmark~\citep{fu2020d4rl}. These include halfcheetah, walker2d, and ant, which are dense reward locomotion tasks, and antmaze, which involves sparse rewards. Each environment offers offline datasets composed of trajectories ranging from completely random to expert. Appendix~\ref{app:experiment} provides additional details.


{\textbf{Setup.}}
We employ the basic setup of the SAC networks as recommended by \citep{ball2023efficient}, i.e., with an ensemble of size 10 each for critic networks and target critic networks, as well as entropy regularization. A significant difference is that the MLP underlying these networks only has 2 layers of size 256 each, as we desired to see if the agent is able to learn with less complexity.

\paragraph{Baseline Methods}
We compare \algname with 2 baselines:
(1) RLPD~\citep{ball2023efficient}, regarded as the most exemplary baseline for addressing online RL with offline datasets, also attains state-of-the-art performance in this problem set, 
(2) SAC with offline data (SACfD), a canonical off-policy approach using offline data, as also studied in \citep{nair2020awac} and \citep{vecerik2017leveraging}.

\subsection{Main results}




\figref{fig:exp:main} illustrates the performance of \algname in comparison with the baseline SACfD and the current state-of-the-art, RLPD. The results clearly indicate that \algname outperforms the baseline in most of the tested domains, especially all of the random environments (\figref{fig:exp:1}, \figref{fig:exp:2}, \figref{fig:exp:3} and \figref{fig:exp:4}). 
This superior performance can be attributed to the fact that RLPD employs a symmetric random sampling technique, 
In contrast, 
\algname actively prioritizes sampling based on the concept of advantage-aligned. As the policy improves, the significance and on-policyness of each transition are reevaluated at every step. \algname consistently prioritize the most effective transitions from the pool to maximize policy improvement, an aspect that RLPD overlooks. 
Consider using a nearly random collected offline dataset or one that contains the trajectories encountered by a poor policy.
In both instances, the useful transitions could be relatively rare and dispersed. Randomly sampling data, as done in RLRD, often fails to identify these helpful transitions, resulting in suboptimal performance. On the other hand, as shown in \figref{fig:exp:main}, the online sampling strategy employed by \algname effectively prioritizes the beneficial transitions from a pool, leading to more significant policy improvements.
It is clear that \algname significantly outperforms RLPD in random datasets. 

In medium or expert environments, \algname maintains or surpasses RLPD's performance. This superior performance can be linked to the better quality of transitions found in medium and expert datasets (\figref{fig:exp:5}, \figref{fig:exp:6}, \figref{fig:exp:7} and \figref{fig:exp:8}), as opposed to random ones. Consequently, even with a random sampling strategy, RLPD is still likely to come across useful transitions. However, most offline datasets are presented in a blackbox format. Despite being unaware of the specifics within this blackbox environment, \algname is able to achieve performance that is comparable to or better than RLPD\fix{, being robust to the blackbox environment condition.}


\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/no_advantage/halfcheetah-random_AORL,Off2On.pdf}
        \caption{advantage term}\label{fig:ablation:without_advantage:1}
    \end{subfigure}\hfil
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/no_density/halfcheetah-random_AORL,No_Density.pdf}
        \caption{density term}\label{fig:ablation:without_online:2}
    \end{subfigure}\hfil
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/online_only/halfcheetah-random_AORL,AORL_Online,SAC_Online.pdf}
        \caption{purely online setting}\label{fig:ablation:pure_online}
    \end{subfigure}\hfil
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/td_error/halfcheetah-random_AORL,TD,TD_Density.pdf}
        \caption{TD-error priority}\label{fig:ablation:tderror:3}
    \end{subfigure}\hfil
    \caption{ \textbf{Ablation Studies}: Results of ablation studies on the halfcheetah-random environment.}
  \label{fig:ablation:without_advantage}
\end{figure*}


\subsection{Ablation studies}
\paragraph{Ablation on advantage term}



\figref{fig:ablation:without_advantage:1} illustrates the comparison between the performance of \algname using advantage-aligned sampling priority and \algname utilizing solely density ratio ($\priority=\mathbb{I}^{\text{off}}\density\paren{\state_i,\action_i}+\mathbb{I}^{\text{on}}$), Off2On, a modified version of balanced experience replay~\citep{lee2022offline}. The results show that \algname with the advantage term surpasses its counterpart that only considers online-ness in prioritizing samples in the halfcheetah-random environments. This superiority is attributed to the advantage term, which effectively screens out transitions that are either non-informative or harmful. For example, even if a transition indicates online-ness, it may not provide new information if the policy has already mastered the associated action for that state. Investigating alternative actions for the same state could yield greater benefits. By integrating the advantage term, such repetitive transitions are excluded, as the advantage value tends to zero for well-understood transitions.




\paragraph{Ablation on density term}

\figref{fig:ablation:without_online:2} compares the performance of \algname to \algname with only advantage in sampling priority ($\priority=\exp\paren{\lambda\cdot\AFunc}$), without density term. The results consistently show that \algname, which incorporates onlineness through the density term $w = d^{\text{on}}/d^{\text{off}}$, outperforms the version that does not. Onlineness measures the likelihood that \algname will experience the given transition during the online exploration and exploitation of the current policy. Transitions experienced during online policy enhancement are more advantageous for policy development. In contrast, focusing on transitions that are unlikely to occur during live interactions with the environment can hinder or be less beneficial to the progression of policy improvement. This result demonstrates the effectiveness of onlineness term.










\paragraph{Ablation on purely online setting and offline data}




\figref{fig:ablation:pure_online} presents an ablation study comparing regular \algname (in \textcolor{myred}{red}), purely online \algname (in \textcolor{myblue}{blue}), and SAC (in \textcolor{mygreen}{green}), with neither having access to offline data. \algname surpasses its purely online version when utilizing an offline dataset, as the offline data provides a more diverse range of transitions that the online policy might not  encounter, effectively demonstrating \algname's ability to leverage offline datasets. 

Moreover, the purely online version of \algname outperforms SAC, highlighting \algname's robustness in environment setting. The results confirm \algname's effectiveness in a purely online environment and its superiority over SAC in online batch scenarios through active advantage-aligned sampling.





\paragraph{Ablation on priority term}



\figref{fig:ablation:tderror:3} presents an ablation study for \algname (in \textcolor{myred}{red}), where we compare two different sampling strategies: Prioritized Experience Replay (PER) as detailed in \citep{schaul2015prioritized} (named as TD in \textcolor{myblue}{blue}), and a modified version incorporating a density ratio (named as TD+Density in \textcolor{mygreen}{green}). The TD-error based sampling strategy prioritizes transitions with larger TD-errors. \algname significantly outperforms both strategies, illustrating that an active advantage-aligned sampling approach is more effective than prioritizing based on TD-error alone. The superior performance of \algname over TD+Density also indicates that prioritizing using the advantage term achieve the better performance compared to the TD-error term.

















\section{Conclusion}

In this paper, we present \algname, a novel algorithm for online RL with offline dataset through an active advantage-aligned sampling strategy. This algorithm is theoretically motivated by the objective of shifting the sampling distribution toward more beneficial transitions to maximize policy improvement. We provide a theoretical foundation for the active advantage-aligned sampling method and quantify its enhancement gap.
Moreover, we carry out comprehensive experiments with various qualities of offline data, demonstrating that \algname outperforms the current state-of-the-art RLPD method with significance. We also conduct multiple ablation studies and confirm the importance of each component within the active advantage-aligned formula. Additionally, \algname surpasses the previous TD-error based prioritized experience replay approach in terms of performance and also prove its robustness in the pure online setting.
While our approach primarily aims to enhance performance, it may lead to higher computational costs due to the calculations required for determining advantage-aligned sampling priorities. Addressing the reduction of these computational demands will be our future direction.



