\section{Preliminaries and Problem Statement}\label{sec:Preliminaries}
\vspace{-0.2cm}

We consider a discounted Markov decision process (MDP) environments~\citep{bellman1957markovian} characterized by a tuple denoted as $M = (\stateSpace, \actionSpace, \transDynamics, \RFunc, \discFactor, \stateDist_0)$, where $\stateSpace$ represents a potentially infinite state space, $\actionSpace$ is the action space,
$\transDynamics: \stateSpace \times \actionSpace \rightarrow \Delta(\stateSpace)$ is the unknown transition kernel, %
$\RFunc: \stateSpace \times \actionSpace \rightarrow [0,1]$ is the reward function, $\discFactor \in (0,1)$ is the discount factor, and $\stateDist_0\paren{\state}$ is the initial state distribution. 
The learner's objective is to solve for the policy $\policy:\stateSpace \rightarrow \Delta(\actionSpace)$ that maximizes the %
expected sum of discounted future rewards $\expctover{\policy}{\sum_{t=1}^{\infty} \gamma^t\reward\paren{\state_t,\action_t}}$,
where the expectation is taken over the trajectory sampled from $\policy$. %

\paragraph{Maximum entropy RL.} 
In this work, we adopt off-policy soft actor-critic (SAC)~\citep{haarnoja2018soft} RL to train an agent with samples generated by any behavior policy. 
{
We use a general maximum entropy objective~\citep{ziebart2010modeling,haarnoja2018soft,ball2023efficient} as follows:
\begin{equation}\label{eq:maxentRL}
    \max_{\policy} \expctover{\state\sim\rho^{\policy},{\action\sim\policy}}{\sum_{t=0}^{\infty} \gamma^t\paren{\reward_t+\alpha \mathcal{H}\paren{\policy\paren{\action|\state}}}}.
\end{equation}
This involves optimizing reward while encouraging exploration, making the learned policy more robust.
}

\paragraph{$\QFunc$-value and advantage function.} 
The $\QFunc$-value function measures the expected return of executing action $\action$ in state $\state$ under policy $\policy$:
$\QFunc^{\policy}\paren{\state,\action} = \mathcal{B}^{\policy}\QFunc^{\policy}\paren{\state,\action}$, 
where $\mathcal{B}^{\policy}$ is the Bellman operator:
\begin{equation*}
    \mathcal{B}^{\policy}\QFunc\paren{\state,\action} := \reward\paren{\state,\action} + \gamma \expctover{\state' \sim P\paren{\cdot|\state,\action}}{\VFunc^{\policy}\paren{\state'}}
\end{equation*}
The soft state value function is defined as:
$\VFunc^{\policy}\paren{\state}:= \expctover{\action\sim\policy\paren{\cdot|\state}}{\QFunc^{\policy}\paren{\state,\action}- {\log \policy\paren{\action|\state}}}$. 
For a generator policy $\policy$, the advantage function \citep{sutton1999policy} %
quantifies the relative benefit of selecting $\action$ over the policy's default behavior:
\begin{align}\label{eq:advantage}
   \AFunc^{\policy}\paren{\state,\action} = \QFunc^{\policy}\paren{\state,\action} - \VFunc^{\policy}\paren{\state}.
\end{align}
Specifically, SAC learns a soft Q-Function, denoted as $\QFunc_{\theta}\paren{\state,\action}$ which parameterized by $\theta$, 
and a stochastic policy $\policy_{\phi}$ parameterized by $\phi$. The SAC method involves alternating between updates for the critic and the actor by minimizing their respective objectives~\citep{lee2022offline} as follows
\begin{align*}
&\mathcal{L}_{\text{critic}}^{\text{SAC}}\paren{\theta}=\mathbb{E}_{\paren{\state_t,\action_t,\state_{t+1}}\sim\ReplayBuffer}[(\QFunc_{\theta}\paren{\state_t,\action_t}-\reward\paren{\state_t,\action_t}\\&-\gamma{
    \expctover{\action_{t+1} \sim \policy_{\phi}}{\QFunc_{\overline{\theta}}\paren{\state_{t+1},\action_{t+1}}-\alpha\log\policy_{\phi}\paren{\action_{t+1}|\state_{t+1}}}})^2]
\end{align*}
\begin{align*}
\mathcal{L}_{\text{actor}}^{\text{SAC}}\paren{\phi}=\expctover{\state_t\sim\ReplayBuffer,\action_t\sim\policy_{\phi}}{\alpha\log\policy_{\phi}\paren{\action_t|\state_t}-\QFunc_{\theta}\paren{\state_t,\action_t}},
\end{align*}
Here, $\ReplayBuffer$
is an experience replay buffer of either on-policy experience~\citep{sutton1999policy} or though off-policy experience~\citep{precup2000eligibility, munos2016safe}, $\alpha$ is the temperature parameter, and $\overline{\theta}$ denotes the delayed parameters.

































\textbf{Prioritized experience replay.} 
PER~\citep{schaul2015prioritized} serves as the basis of our sampling techniques, providing a framework for prioritizing experience replay based on transition importance. Instead of sampling uniformly from the replay buffer $\ReplayBuffer$,  PER assigns higher probability to more informative transitions, leading to improved sample efficiency~\citep{hessel2018rainbow}. Each transition $\ReplayBuffer_i = (\state_{K(i)},\action_{K(i)},\reward_{K(i)},\state_{K(i)+1})$ is assigned a priority $\sigma_i$, typically based on the TD-error: $\delta=\reward+\gamma\VFunc\paren{\state_{t+1}}-\VFunc\paren{\state_{t}}$ \citep{schaul2015prioritized, hessel2018rainbow, brittain2019prioritized, van2019use, oh2021model}.
Subsequently, the sampling approach of PER involves establishing an index set $\mathcal{I}$ within the range of $\bracket{|\ReplayBuffer|}$ based on the probabilities $p_i$ assigned by the priority set as follows:
    $p_i=\frac{\sigma_i^{\alpha}}{\sum_{k\in\bracket{|\ReplayBuffer|}}\sigma_k^{\alpha}}$, 
with a hyper-parameter $\alpha>0$.
To correct for sampling bias, PER applies importance sampling weights:
\begin{equation}
\label{eq:weights}
u_i= \bigl ( {1}/ (|\ReplayBuffer|\cdot p_i) \bigr)^{\beta},
\end{equation}
where $\beta$ anneals from $\beta_0 \in (0,1)$ to $1$ during training to counteract bias in the learning updates. While standard PER prioritizes TD-error, our method extends this framework to prioritize transitions based on onlineness and contribution to policy improvement.


\textbf{Online RL with offline datasets.}
In this work, we study online RL with offline datasets denoted as $\OfflineBuffer$ \citep{ball2023efficient}. These datasets consist of a set of tuples $(\state, \action, \reward, \state')$ generated from a specific MDP. 
A key characteristic of offline datasets is that they typically offer only partial coverage of state-action pairs. In other words, the set of states and actions in the dataset, denoted as $\curlybracket{\state,\action\in \dataset}$, represents a limited subset of the entire state space and action space, $\stateSpace \times \actionSpace$. 
Moreover, learning on the  data with incomplete coverage of state-action pairs potentially results in excessive value extrapolation during the learning process for methods using function approximation~\citep{fujimoto2019off}.
Our model, based on SAC \citep{haarnoja2018soft}, incorporates several effective strategies for RL with offline data, as outlined in RLPD \citep{ball2023efficient}. These strategies include:




\emph{Layer Normalization:} %
Off-policy RL algorithms often query the learned \QFunc--function with out-of-distribution actions, leading to overestimation errors due to function approximation. This can cause training instabilities and even divergence, particularly when the critic struggles to keep up with growing value estimates. To address this, prior research has employed Layer Normalization to ensure that the acquired functions do not extrapolate in an unconstrained manner. Layer Normalization acts to confine \QFunc-values within the boundaries set by the norm of the weight layer, even for actions beyond the dataset. As a result, the impact of inaccurately extrapolated actions is substantially reduced, as their associated \QFunc-values are unlikely to significantly exceed those already observed in the existing data. Consequently, Layer Normalization serves to alleviate issues such as critic divergence and the occurrence of catastrophic overestimation.

\emph{Update-to-Data:} Enhancing sample efficiency in Bellman backups can be accomplished by elevating the frequency of updates conducted per environment step. This approach, often referred to as the update-to-data (UTD) ratio, expedites the process of backing up offline data.


\emph{Clipped Double Q-Learning:} 
The maximization objective of Q-learning and the estimation uncertainty from value-based function approximation often  leads to value overestimation \citep{van2016deep}. To address this problem, \citet{fujimoto2018addressing} introduced Clipped Double Q-Learning (CDQ) as a means of mitigation. CDQ involves taking the minimum from an ensemble of two Q-functions for computing TD-backups. The targets for updating the critics are given by the equation $y=\reward\paren{\state,\action} + \gamma \min_{i=1,2} \QFunc_{\theta_i}\paren{\state',\action'}$, where $\action'\sim \policy\paren{\cdot|\state'}$. 


\emph{Maximum Entropy RL:} %
Incorporating entropy into the learning objective (as defined in \eqref{eq:maxentRL})  helps mitigate overconfidence in value estimates, particularly when training with offline datasets. In offline RL, policies may become overly conservative due to limited dataset coverage, leading to suboptimal exploration during fine-tuning. By preserving policy stochasticity, entropy regularization ensures that the agent remains adaptable when transitioning from offline training to online interactions. This controlled exploration has been shown to improve training stability and prevent premature convergence~\citep{haarnoja2018soft, chen2021randomized, hiraoka2021dropout, ball2023efficient}.



\textbf{Limitations of the prior state-of-the-art.} 
A drawback of RLPD, as discussed by~\citet{ball2023efficient}, lies in its symmetric random sampling method applied to both online and offline data, disregarding the significance of individual transitions. This predefined approach to sampling can potentially lead to less than optimal policy improvements due to the omission of vital data and inefficiencies arising from the use of redundant data. Such inefficiencies fail to offer any positive contribution towards enhancing policy. To address the limitation, our research presents an innovative active data sampling technique, specifically designed to optimize the use of both online and offline data in the process of policy improvement.
