\begin{abstract}



Online reinforcement learning (RL) enhances policies through direct interactions with the environment, but faces challenges related to sample efficiency. In contrast, offline RL leverages extensive pre-collected data to learn policies, but often produces suboptimal results due to limited data coverage. Recent efforts have sought to integrate offline and online RL in order to harness the advantages of both approaches. However, effectively combining online and offline RL remains challenging due to issues that include %
catastrophic forgetting, lack of robustness and sample efficiency.\MW{The reference to ``sample complexity'' is vague since we just said that offline RL mitigates this issue\XLinline{good point, but offline data also has sample efficiency problem such as sample not useful transitions.}} In an effort to address these challenges, %
we introduce \algname, a novel method that actively selects data from combined online and offline sources to optimize policy improvement.\MW{Say more about/emphasize active nature} We provide theoretical guarantee that validates the effectiveness our active sampling strategy and conduct thorough empirical experiments showing that our method outperforms existing state-of-the-art online RL techniques that utilize offline data. 
Our code will be publicly available at: \url{https://github.com/xuefeng-cs/A3RL}. 











\end{abstract} 