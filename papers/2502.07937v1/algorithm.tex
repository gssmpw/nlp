\section{Algorithm}








\subsection{{Active} Advantage-Aligned Sampling Strategy}
In this study, we theoretically derived from the performance difference lemma in \secref{sec:theoretical-analysis} and presented active advantage-aligned strategy, a novel sampling approach for policy improvement. Here, `advantage' measures the potential impact of the transition on policy improvement, while 'aligned' assesses how well the transition aligns with the states sampled online by the current policy.

This method allows for the safe utilization of online and offline samples by harnessing relevant, near on-policy offline samples that also present the potential to enhance policy improvement. 
This approach broadens the distribution of samples used for updates, centering around on-policy examples, thereby facilitating immediate value. The active advantage-aligned priority $\priority$ and the probability $\prob$ are as follows:
\begin{align}\label{eq:adv_priority}
\prob_i&=\frac{\sigma_i^{\alpha}}{\sum_{k\in\bracket{|\ReplayBuffer|}}\sigma_k^{\alpha}},\\ 
\priority_i&=\priority\paren{\state_i,\action_i} \nonumber \\ &=\paren{\mathbb{I}^{\text{off}}\density
\paren{\state_i,\action_i}+ \mathbb{I}^{\text{on}}} \cdot\exp\paren{\gamma\cdot\AFunc\paren{\state_i,\action_i}} %
\end{align}
where $\mathbb{I}^{\text{off}}$ and $\mathbb{I}^{\text{on}}$ represents the indicator of offline and online respectively, density ratio
$\density\paren{\state,\action}$ %
measures the onlineness of the transition (defined in Eq.~\eqref{eq:densityratio}), $\AFunc\paren{\state,\action}$ is the advantage term, which assesses the potential of the transition in improving the policy and $\gamma > 0$ representing a temperature value, with a hyper-parameter $\alpha>0$.
This approach considers not only the on-policyness of the data but also measures how important the data contributes to the current policy improvement. 
The active advantage-aligned sampling strategy aims to assign greater weight to transitions that are either not well covered by the offline dataset—indicating that the state-action pair is novel to the offline policy (i.e., the density ratio is large)—or that represent good actions for maximizing cumulative reward (i.e., the advantage / Q function is large).








\paragraph{Density ratio.}
We evaluate the onlineness through the use of a density ratio 
\begin{align}\label{eq:densityratio}
\density\paren{\state,\action}:= {\stateDist^{\text{on}}\paren{\state,\action}}/ {\stateDist^{\text{off}}\paren{\state,\action}}
\end{align}
for a given transition, where $\stateDist^{\text{on}}\paren{\state,\action}$ denotes the state-action distribution of online samples in the online buffer $\ReplayBuffer^{\text{on}}$ and the $\stateDist^{\text{off}}\paren{\state,\action}$ denotes the offline samples in the offline buffer $\ReplayBuffer^{\text{off}}$. By identifying a transition with a high density ratio $\density\paren{\state,\action}$, we can effectively select a near-on-policy sample $\paren{\state,\action,\state'}$ from the offline dataset $\mathcal{B}^{\text{off}}$. 
Consider the much larger volume of offline data compared to online data, this would greatly improve the amount of transition and diversity of coverage for policy improvement in each step.


Estimating the likelihoods $\stateDist^{\text{off}}\paren{\state,\action}$ and $\stateDist^{\text{on}}\paren{\state,\action}$ poses a challenge, as they could represent stationary distributions from mixture of complex policy.
To address this issue, we employ a method studied by \citet{lee2022offline,sinha2022experience} for density ratio estimation that does not rely on likelihoods.  This method approximates $\density\paren{\state,\action}$ by training a neural network $\density_{\psi}\paren{\state,\action}$, which is parameterized by $\psi$. The training exclusively uses samples from $\mathcal{B}^{\text{off}}$ and $\mathcal{B}^{\text{on}}$. 
We use variational representation of f-divergences~\citep{nguyen2007estimating}. Consider $P$ and $Q$ as probability measures on a measurable space $\mathcal{X}$, with $P$ being absolutely continuous $w.r.t$ $Q$.
We define the function $f\paren{y}:=y \log\frac{2y}{y+1}+\log\frac{2}{y+1}$. 
The Jensen-Shannon (JS) divergence is then defined as $D_{JS}\paren{P||Q}=\int_{\mathcal{X}}f\paren{ {dP\paren{x}} / {dQ\paren{x}}}dQ\paren{x}$.
Then we use a parametric based model $\density_{\psi}\paren{x}$ to represent density ratio $\frac{dP}{dQ}$
and estimated the density ratio by maximizing the lower bound of $D_{JS}\paren{P||Q}$: 
\begin{equation*}\label{eq:lower_bound}
    \mathcal{L}^{\text{DR}}\paren{\psi}= \expctover{x\sim P}{f'\paren{\density_{\psi}\paren{x}}}- \expctover{x\sim \QFunc}{f^*\paren{f'\paren{\density_{\psi}\paren{x}}}},
\end{equation*}

\begin{algorithm}[!t]
\caption{\algname}
\label{alg:a3rl}
\begin{algorithmic}[1]
\State Select LayerNorm, large ensemble Size $\EnsembleNum$, gradient steps $\GNum$.
\State Randomly initialize Critic $\theta_i$ (set targets $\theta'_i = \theta_i$) for $i=1,2,\ldots, \EnsembleNum$, Actor $\phi$ parameters. 
\State Select discount $\gamma$, temperature $\alpha$ and critic EMA weight $\rho$, batch size $\batch$.
\State Determine number of Critic targets to subset $Z\in\curlybracket{1,2}$
\State Initialize buffer $\OfflineBuffer$ with offline data
\State Initialize online replay buffer $\ReplayBuffer \leftarrow \emptyset$ 
\While {True}
\State Receive initial observation state $\state_0$
     \For{$t=0, \ldots, \horizon$}
       \State Take action  $\action_t \sim \policy_{\phi}\paren{\cdot|\state_t}$ 
       \State Update buffer $\ReplayBuffer \leftarrow \ReplayBuffer \cup \curlybracket{\paren{\state_t,\action_t,\reward_t,\state_{t+1}}}$
       \State Fix a large $k$. Randomly sample a subset of size $ \frac{kN}{2}$ from online buffer $\ReplayBuffer$ and size $\frac{kN}{2}$ from offline buffer $\mathcal{D}$ to form a learning dataset $\ReplayBuffer_{kN}$.
       \State \textcolor{NavyBlue}{Update density network using $\ReplayBuffer_{kN}$}
       \State
       \textcolor{NavyBlue}{Calculate priority $\PSet_{\ReplayBuffer}$ of $\ReplayBuffer_{kN}$}~\eqref{eq:adv_priority}
       \For{$g=1, \ldots, G$}
            \State  \textcolor{NavyBlue}{Sample batch $b_N$ of size $N$ according to $\PSet_{\ReplayBuffer}$ from $\ReplayBuffer_{kN}$}
            \State Sample set $\mathcal{Z}$ of $Z$ indices from $\curlybracket{1,\ldots, \EnsembleNum}$ 
            \State With $b$, set $y=\reward+\discFactor\Big(\min_{i\in \mathcal{Z}  \QFunc_{\theta'_i}\paren{\state',\action'}}+\Big.$ 
            \State $\quad \quad\Big.\alpha \log \policy_{\phi}\paren{\action'|\state'}\Big), \action'\sim \policy_{\phi}\paren{\cdot|\state'}$

            \For{$i=1,\ldots, \EnsembleNum$}
            \State \textcolor{Maroon}{Calculate importance weight $u_i$ via \eqref{eq:weights}.}
            \State \textcolor{Maroon}{Update $\theta_i$ minimizing loss:}
            \State \textcolor{Maroon}{$\loss=\frac{1}{N} \sum_{i} u_i\cdot
            \paren{y-\QFunc_{\theta_i}\paren{\state,\action}}^2$}
            \EndFor
            \State Update target networks:
            \State $\theta'_{i}\leftarrow \rho \theta'_{i}+\paren{1-\rho}\theta_i$
       \EndFor
       \State With b, update $\phi$ maximizing objective:
       \State \quad\quad$\frac{1}{\EnsembleNum}\sum_{i=1}^{\EnsembleNum} \QFunc_{\theta_i}\paren{\state,\action} - \alpha \log \policy_{\phi} \paren{\action|\state},$
       \State \quad\quad where $\action\sim\policy_{\phi}\paren{\cdot|\state}$, {$\paren{\state,\action}\sim b_{N}$}.
     \EndFor
\EndWhile

\end{algorithmic}
\end{algorithm}


        
            

        





where $\density_{\psi}\paren{x}\geq 0$ is represented by a neural network, with parameters ensuring that the outputs remain non-negative through the use of activation function. Additionally, $f^{*}$ represents convex conjugate and
we sampled from $\mathcal{B}^{\text{on}}$ for $x \sim P$ and from $\mathcal{B}^{\text{off}}$ for $x \sim Q$.




\paragraph{Active advantage-aligned sampling.}
Relying solely on the density ratio is insufficient; even if a transition appears to be relevant in the online context, it may still fail to contribute meaningfully to policy improvement.   
For instance, consider a transition $\paren{\state,\action,\state'}$. If the policy has previously encountered this state and taken the same action, or if the action performed in this state could potentially lead to a negative reward, such a transition would not that helpful in contributing to policy improvement, regardless of how closely it aligns with on-policy data. 


To address this, we incorporate the advantage value $\AFunc(\state, \action)$ (Eq.~\eqref{eq:advantage}) into our sampling strategy. Specifically, we integrate a non-negative exponential advantage term, $\exp\paren{\gamma\cdot\AFunc\paren{\state,\action}}$, into the priority calculation. This term ensures that transitions are selected not only based on relevance but also on their contribution to policy improvement. The higher the advantage value, the greater the transition's impact on learning, making our sampling mechanism both adaptive and optimization-aware.  




For transitions from the offline dataset, we prioritize samples based on both the estimated density ratio and advantage value, retrieving near-on-policy samples that also provide policy improvement benefits.  
Since the data source is known, we set the density ratio to 1 for transitions from the online dataset and prioritize them purely based on advantage values under the current policy. Thus, we define the priority function for sampling as: 
\begin{align}
\mathbb{I}^{\text{off}}\density\paren{\state_i,\action_i}\cdot\exp\paren{\gamma\cdot\AFunc\paren{\state_i,\action_i}}+\mathbb{I}^{\text{on}}\exp\paren{\gamma\cdot\AFunc\paren{\state_i,\action_i}}. \notag 
\end{align}
Note that this advantage-aligned sampling strategy is not a heuristic-based approach but is theoretically grounded in the performance difference lemma~\citep{kakade2002approximately}, providing a formal guarantee of its effectiveness (see Section~\ref{sec:theoretical-analysis}).



The active sampling process in our algorithm is highlighted in \textcolor{NavyBlue}{blue} in Algorithm~\ref{alg:a3rl}, while our approach to addressing sampling bias is highlighted in \textcolor{Maroon}{red}.





























 
           



 






 
           



 






