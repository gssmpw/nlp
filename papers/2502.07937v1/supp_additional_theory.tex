\section{Theoretical Motivation}\label{app:theory_proof}
In this section, we show that the active advantage-aligned sampling strategy
helps mitigate the gap between offline data distribution, online data distribution and the current on-policy distribution, which serves as a main theoretical motivation for designing \algname.

\perfdiff*

Before proving the theorem, we first show that under certain conditions, the ratio $R^t(s, a)$ will decrease for proper $\gamma$. Since $\gamma$ does not influcence the ration between the state distribution, let us just consider the bandit case with ratio 
\begin{align*}
    R^t(a;\gamma) = \left(\frac{{\pi^{t+1}}(a)}{d^{\text{on}}(a)}\right)^{1-\gamma}  \cdot \frac{\sum_{a'}d^{\text{on}}(a') \pi^{t+1}(a')^\gamma}{d^{\text{on}}(a)^\gamma}.
\end{align*}
Suppose the only data distribution $d^{\text{on}}(a)\propto \exp(\beta_1 r(a))$ for some parameter $\beta$ while the policy $\pi^{t+1}(a) \propto \exp(\beta_2 r(a))$ for some parameter $\beta_2 > \beta_1$. This is reasonable since the policy updates faster than the only buffer.
In this case, we have 
\begin{align*}
    \log \left(\frac{R(a;\gamma)}{R(a;0)}\right) &= -\gamma \log(\pi^{t+1}(a)) + \log \left(\sum_{a'}d^{\text{on}}(a') \pi^{t+1}(a')^\gamma \right) \\
    &= -\gamma \beta_2 r(a) + \log \left(\sum_{a'}\exp((\beta_1 + \beta_2 \gamma) r(a')) \right).
\end{align*}
Consider the gradient of $\log \left(\sum_{a'}\exp((\beta_1 + \beta_2 \gamma) r(a')) \right)$ with respect to $\gamma$:
\begin{align*}
    \frac{\partial}{\partial \gamma} \log \left(\sum_{a'}\exp((\beta_1 + \beta_2 \gamma) r(a')) \right) = \frac{\sum_{a'} \beta_2 r(a') \exp((\beta_1 + \beta_2 \gamma) r(a'))}{\sum_{a'}\exp((\beta_1 + \beta_2 \gamma) r(a'))} - \beta_2 r(a).
\end{align*}
Note that the largest probability ratio happens for $\tilde a = \argmax_{a'} r(a')$. Thanks to the softmax function in the gradient, we see that for $\tilde a$, the derivative is negative, meaning that by increasing $\gamma$, the value of $R(a;\gamma)$ will decrease, hence leading to a better policy improvement in the theorem.
\begin{proof}
Define visitation measures
\[  
d_h^\pi(s, a) = \EE_{a\sim \pi(\cdot| s)} \left[ \ind(s_h = s, a_h = a)\right], \quad d^\pi(s, a) = \frac{1}{1-\gamma}\sum_{h=1}^\infty \gamma^h d_h^\pi(s,a).
\]
Consider a sufficiently small one-step update in the policy network with step-size $\eta$.
Define $J_\alpha^\pi = \expctover{\state\sim\rho^{\policy},{\action\sim\policy}}{\sum_{t=0}^{\infty} \gamma^t\paren{\reward_t+\alpha \mathcal{H}\paren{\policy\paren{\action|\state}}}}$. 
Let $\tilde\pi$ be the policy from the last iteration.
In the following, we abbreviate $\EE_\pi[\cdot]$ as $\EE[\cdot]$. 
\begin{align*}
V^{\policy} - V^{\tilde\policy}&=\mathbb{E} \bracket{\tuple{\policy,\QFunc^{\policy}-\alpha\log\policy}-\langle{\tilde\policy,\QFunc^{\tilde\policy}-\alpha\log\tilde\policy} \rangle_{\cA}}\\
&=\mathbb{E}\bracket{\langle{\policy,\QFunc^{\policy}-\QFunc^{\tilde\policy}}\rangle_{\cA}+\langle{\policy-\tilde\policy,\QFunc^{\tilde\policy}}\rangle_{\cA}-\alpha \tuple{\policy,\log{\policy}}+\alpha\tuple{\tilde\policy,\log\tilde\policy}}\\
&=\mathbb{E}\bracket{\tuple{\policy,\reward+\gamma \mathbb{P}\VFunc^{\policy}-\reward+\gamma \mathbb{P}\VFunc^{\policy}}+\langle \policy-\tilde\policy,\QFunc^{\tilde\policy}\rangle_{\cA} -
\alpha\tuple{\policy,\log\policy} + \alpha\tuple{\tilde\policy,\log \tilde\policy}}\\
&=\mathbb{E}\bracket{ \gamma\bigl\langle\policy, \PP\bigl(\VFunc^{\policy}-\VFunc^{\tilde\policy}\bigr)\bigr\rangle_{\cA}+\bigl\langle\policy-\tilde\policy,\QFunc^{\tilde\policy}\bigr\rangle_{\cA}-\alpha\tuple{\policy,\log\policy}+\alpha\tuple{\tilde\policy,\log\tilde\policy}} , 
\end{align*}
Using this iterative form, we conclude that 
\begin{align*}
    J_\alpha^\pi - J_\alpha^{\tilde\pi} 
    &= \EE\left[
        \sum_{h=1}^\infty \gamma^i \left( \bigl\langle\policy_i-\tilde\policy_i,\QFunc_i^{\tilde\policy}\bigr\rangle_{\cA}-\alpha\tuple{\policy_i,\log\policy_i}+\alpha\tuple{\tilde\policy_i,\log\tilde\policy_i}  \right)
    \right] \\
    &= \EE_{d^\pi} \left[ \bigl\langle\policy-\tilde\policy,\QFunc^{\tilde\policy}\bigr\rangle_{\cA}-\alpha\tuple{\policy,\log\policy}+\alpha\tuple{\tilde\policy,\log\tilde\policy} \right].
\end{align*}
Recall our definition of $\sigma(s,a)$ that 
\begin{align}
    \sigma(s, a) = \exp( \gamma \hat A^{\tilde \pi}(s,a))\cdot \frac{d^{\text{on}}(s, a)}{\mu(s,a)}, 
    \label{eq:sigma_theory}
\end{align}
where $\mu(\cdot, \cdot)$ is the distribution in the sampled batch and $d^{\text{on}}(\cdot, \cdot)$ is the online distribution. 
Note that the advantage function $\hat A^{\tilde\pi} (s, a) = \hat Q^{\tilde\pi} (s, a) - \alpha \log \sum_{a'}\exp(\alpha^{-1}\hat Q(s, a'))$ is calculated using policy $\tilde\pi$ and Q function $\hat Q^{\tilde\pi}$ obtained from the last iteration in the above formula.
Let us define $\pi_{\phi^\star}$ as the optimal policy under the current Q function $\tilde Q$:
\begin{align*}
    \pi^\star(\cdot\given s) &= \argmin_{\pi} \kl \left( \pi(\cdot \given s) \,\bigg\|\, \frac{\exp(\alpha^{-1} \QFunc^{\tilde\policy}(s, \cdot))}{\tilde Z_\alpha(s)} \right) \\
    &=\argmax_{\pi}  \bigl\langle{\policy\paren{\cdot|\state},\QFunc^{\tilde\policy}\paren{\state,\cdot}-\alpha\log\policy\paren{\cdot|\state}}\bigr\rangle_{\cA} \propto \exp(\alpha^{-1}A^{\tilde\policy}(s, \cdot)).
\end{align*}
where $\tilde Z_\alpha(s)$ is the normalization factor at state $s$ for the exponential of the $Q$ function, and $A^{\tilde\pi}(s, \cdot)$ is the advantage function under policy $\tilde\pi$.
Recall by policy optimization:
\begin{align*}
    \hat\pi = \argmax_\pi\expctover{\mu}{\sigma\paren{\state,\action} \bigl\langle{\policy\paren{\cdot|\state},\hat\QFunc^{\tilde\policy}\paren{\state,\cdot}-\alpha\log\policy\paren{\cdot|\state}}\bigr\rangle_{\cA}}, 
\end{align*}
where $\hat\QFunc^{\tilde\pi}$ is the estimated Q function at the current iteration.
In the above formula, $\mu$ is the sampled data distribution and $\sigma$ is the quantity calculated in \eqref{eq:sigma_theory}. 
Suppose we take some function class $\pi_\phi$ which contains the optimal one-step policy improvement $\pi^\star$ and also the optimization target $\hat\pi$.
Using a shift of distribution, we have 
\begin{align*}
    \mu(s, a) \sigma(s, a) 
        &= \mu(s, a) \cdot \frac{d^{\text{on}}(s, a)}{\mu(s, a)} \cdot \exp(\gamma \hat A^{\tilde\pi}(s, a)) ={d^{\text{on}}(s, a)} \cdot \hat\pi(a\given s)^\gamma \\
        &= d^{\hat\pi}(s, a) \cdot \frac{d^{\text{on}}(s)}{d^{\hat\pi}(s)} \cdot \frac{d^{\text{on}}(a\given s)}{\hat\pi(a\given s)^{1-\gamma}} \propto \rho(s, a),
\end{align*}
where we define $\rho(s, a)$ as the probability density induced by the above distribution.
Here, the first ratio $d^{\text{on}}(s)/d^{\pi^\star}(s)$ is the state-drift between the online data and the next-step optimal policy.
Since the online batches are refreshing as the algorithm proceeds, the ratio will be close to $1$. The second ratio term characterizes the drift caused by a mismatch in the policy. Intuitively, as we know the policy $\tilde\pi$ from the last iteration, we can use this information to further boost the alignment between the online policy and the next-step policy.
Suppose the Q function is learned up to $\epsilon$ error, that is 
\begin{align*}
    \expctover{\rho}{(\QFunc^{\tilde\pi}(s, a) - \hat\QFunc^{\tilde\pi}(s, a))^2} \leq \epsilon.
\end{align*}
Then, we have performance difference lemma that 
\begin{align*}
    J_{\alpha}^{\hat\pi} -J_{\alpha}^{\pi^\star}
       & = \EE_{d^{\hat\pi}} \left[ \bigl\langle\hat\pi, \QFunc^{\tilde\pi}\bigr\rangle_{\cA}-\alpha\tuple{\hat\pi,\log\hat\pi} - \bigl(\bigl\langle\pi^\star, \QFunc^{\tilde\pi}\bigr\rangle_{\cA} - \alpha\tuple{\pi^\star,\log\pi^\star}\bigr) \right]\\
       &= %
       \EE_{d^{\hat\pi}} \left[ \bigl\langle\hat\pi, \QFunc^{\tilde\pi}\bigr\rangle_{\cA}-\alpha\tuple{\hat\pi,\log\hat\pi} - \bigl(\bigl\langle\hat\pi, \hat\QFunc^{\tilde\pi}\bigr\rangle_{\cA}-\alpha\tuple{\hat\pi,\log\hat\pi}\bigr)\right]\\
       &\qquad + \EE_{d^{\hat\pi}} \left[ \bigl\langle\hat\pi, \hat\QFunc^{\tilde\pi}\bigr\rangle_{\cA}-\alpha\tuple{\hat\pi,\log\hat\pi} - \bigl(\bigl\langle\pi^\star, \hat\QFunc^{\tilde\pi}\bigr\rangle_{\cA}-\alpha\tuple{\pi^\star,\log\pi^\star}\bigr)\right]\\
       &\qquad + \EE_{d^{\hat\pi}} \left[ \bigl\langle\pi^\star, \hat\QFunc^{\tilde\pi}\bigr\rangle_{\cA}-\alpha\tuple{\pi^\star,\log\pi^\star} - \bigl(\bigl\langle\pi^\star, \QFunc^{\tilde\pi}\bigr\rangle_{\cA}-\alpha\tuple{\pi^\star,\log\pi^\star}\bigr)\right]\\
       &\ge %
       \EE_{d^{\hat\pi}} \left[ \bigl\langle\hat\pi - \pi^\star, \QFunc^{\tilde\pi} - \hat\QFunc^{\tilde\pi}\bigr\rangle_{\cA} \right] \\
       &\ge - \sup_{s, a}\left|\frac{\pi^\star(a\given s)}{\hat\pi(a\given s)} - 1\right| \cdot \EE_{d^{\hat\pi}}[|\QFunc^{\tilde\pi} - \hat\QFunc^{\tilde\pi}|] \ge - C \cdot \EE_{d^{\hat\pi}}[|\QFunc^{\tilde\pi} - \hat\QFunc^{\tilde\pi}|]
\end{align*}
where $C$ is an absolute constant given that both $Q^{\tilde\pi}$ and $\hat Q^{\tilde\pi}$ are uniformly bounded.
Here, the first inequality holds by the policy optimization step where we upper bound the second term by zero, and the last inequality holds by the assumption that the $Q$ function class is uniformly bounded.
Now, by a shift of distribution 
\begin{align*}
    \EE_{d^{\hat\pi}}[|\QFunc^{\tilde\pi} - \hat\QFunc^{\tilde\pi}|] 
        &= \EE_{\rho}\left[|\QFunc^{\tilde\pi} - \hat\QFunc^{\tilde\pi}|  \cdot \frac{d^{\hat\pi}(s, a)}{\rho(s, a)}\right]  %
        \le \sqrt{\EE_{\rho}[(\QFunc^{\tilde\pi} - \hat\QFunc^{\tilde\pi})^2]} \cdot \sup_{s, a} \left| \frac{d^{\hat\pi}(s, a)}{\rho(s, a)} \right|.
\end{align*}
Let's look at the distribution ratio 
\begin{align*}
    \frac{d^{\hat\pi}(s, a)}{\rho(s, a)} &= \frac{{\hat\pi}(a\given s) }{\hat\pi(a\given s)^\gamma \cdot d^{\text{on}}(a\given s)^{1-\gamma} }  \cdot \frac{\sum_{s', a'}d^{\text{on}}(a'\given s') \hat\pi(a'\given s')^\gamma}{d^{\text{on}}(a\given s)^\gamma} \cdot \frac{d^{\hat\pi}(s)}{d^{\text{on}}(s)} \\ 
    &= \left(\frac{{\hat\pi}(a\given s)}{d^{\text{on}}(a\given s)}\right)^{1-\gamma}  \cdot \frac{\sum_{s', a'}d^{\text{on}}(a'\given s') \hat\pi(a'\given s')^\gamma}{d^{\text{on}}(a\given s)^\gamma} \cdot \frac{d^{\hat\pi}(s)}{d^{\text{on}}(s)}.
\end{align*}
Therefore, the policy improvement is guaranteed by 
\begin{align*}
    J_{\alpha}^{\hat\pi} - J_{\alpha}^{\tilde\pi} = J_{\alpha}^{\hat\pi} - J_{\alpha}^{\pi^\star} + J_{\alpha}^{\pi^\star} - J_{\alpha}^{\tilde\pi} \ge J_{\alpha}^{\pi^\star} - J_{\alpha}^{\tilde\pi} - C \cdot \sqrt \epsilon \cdot \sup_{s, a} \left| \frac{d^{\hat\pi}(s, a)}{\rho(s, a)} \right|.
\end{align*}
This completes the proof.














































\end{proof}




\section{Experimental Details}\label{app:experiment}
\subsection{Additional baselines and ablation studies.}
In order to ensure fair evaluation, all baselines and ablation studies are assessed using an equal number of environment interaction steps. We average results over 10 seeds to obtain the final result.



     

\textbf{Ablation on density term.} \figref{fig:supp:without_onlineness}
presents further ablation studies on the density term for \algname. We see the distinction in the effectiveness of the density term is more significant over harder tasks like antmaze-medium-play.

\begin{figure*}[h]
    \centering
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/no_density/halfcheetah-random_AORL,No_Density.pdf}
        \caption{halfcheetah-random}\label{fig:supp:without_onlineness:1}
    \end{subfigure}\hfil
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/no_density/halfcheetah-medium-replay_AORL,No_Density.pdf}
        \caption{halfcheetah-medium-replay}\label{fig:supp:without_onlineness:2}
    \end{subfigure}\hfil
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/no_density/antmaze-medium-play_AORL,No_Density.pdf}
        \caption{antmaze-medium-play}\label{fig:supp:without_onlineness:3}
    \end{subfigure}\hfil
    \caption{\textbf{Ablation Studies}: \algname vs \algname without density term.}
  \label{fig:supp:without_onlineness}
\end{figure*}

\textbf{Ablation on purely online setting.} \figref{fig:supp:online_only}
presents further ablation studies on \algname interacting with the environment in a purely online manner, i.e., the algorithm does not utilize access to offline data. It is consistent throughout tested environments that \algname is able to leverage offline data effectively, especially in harder tasks like antmaze-medium-play where purely online \algname fails to learn in the same number of steps.

\begin{figure*}[h]
    \centering
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/online_only/halfcheetah-random_AORL,AORL_Online,SAC_Online.pdf}
        \caption{halfcheetah-random}\label{fig:supp:online_only:1}
    \end{subfigure}\hfil
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/online_only/halfcheetah-medium-replay_AORL,AORL_Online,SAC_Online.pdf}
        \caption{halfcheetah-medium-replay}\label{fig:supp:online_only:2}
    \end{subfigure}\hfil
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/online_only/antmaze-medium-play_AORL,AORL_Online,SAC_Online.pdf}
        \caption{antmaze-medium-play}\label{fig:supp:online_only:3}
    \end{subfigure}\hfil
    \caption{\textbf{Ablation Studies}: \algname vs purely online \algname vs purely online SAC.}
  \label{fig:supp:online_only}
\end{figure*}

\textbf{Ablation on priority term.} \figref{fig:supp:td_error} presents further ablation studies on the priority term for \algname, where we compare it against the sampling strategy that solely uses TD-error as the priority term, and another that combines the density term with TD-error. The superior performance of \algname over TD+Density over tested environments indicates that prioritizing using the advantage term achieves better performance compared to the canonical TD-error term.

\begin{figure*}[h]
    \centering
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/td_error/halfcheetah-random_AORL,TD,TD_Density.pdf}
        \caption{halfcheetah-random}\label{fig:supp:td_error:1}
    \end{subfigure}\hfil
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/td_error/halfcheetah-medium-replay_AORL,TD,TD_Density.pdf}
        \caption{halfcheetah-medium-replay}\label{fig:supp:td_error:2}
    \end{subfigure}\hfil
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth,  clip={0,0,0,0}]{figures/hungplots/td_error/antmaze-medium-play_AORL,TD,TD_Density.pdf}
        \caption{antmaze-medium-play}\label{fig:supp:td_error:3}
    \end{subfigure}\hfil
    \caption{\textbf{Ablation Studies}: \algname vs PER (TD) vs PER with Density (TD+Density).}
  \label{fig:supp:td_error}
\end{figure*}
\textbf{Training and evaluation environments.} \figref{fig:exp:env} presents snapshots of tested D4RL tasks: halfcheetah, walker2d, ant and antmaze. halfcheetah, walker2d and ant have dense rewards, while antmaze has sparse rewards, and all environments are equipped with continuous state and action spaces.

In the halfcheetah environment, the 2D agent resembles a simplified cheetah model with a torso and lined legs, with the objective of forward locomotion and maintaining balance while maximizing speed. In the walker2d environment, the 2D humanoid agent has 2 legs and multiple joints, with the objective of stable bipedal walking without falling. In the ant environment, the agent is a 3D quadrupedal agent with multiple joints and degrees of freedom, with the objective of moving forward efficiently while maintaining balance. For all of these environments, rewards are given for velocity to encourage the agent to move forward efficiently while maintaining balance, and several offline datasets, per \citep{fu2020d4rl}, with varying characteristics, as detailed below, were tested.

\begin{table}[H]
    \centering
    \begin{tabular}{l|c}
    \hline
        {\bf Offline dataset type} & {\bf Description} \\
        \hline
        -expert-v2 & 1M samples from policy trained to completion with SAC\\
        -medium-v2 & 1M samples from policy trained to 1/3 of expert\\
        -medium-replay-v2 & Replay buffer of policy trained to medium\\
        -random-v2 & 1M samples from randomly initialized policy \\
        \hline
    \end{tabular}
    \caption{Locomotion offline dataset.}
    \label{tab:env_description}
\end{table}
In the antmaze environment, the aforementioned ant agent is placed in a maze environment and must navigate from a defined start point to a goal. Rewards are binary: $1$ for reaching the goal and $0$ otherwise. Varying sizes of the maze were tested: umaze (U-shaped), medium and large; which are naturally also of increasing difficulty.


\begin{figure}[ht]
    \centering
    \includegraphics[height=1in]{figures/envs/halfcheetah.pdf} \hfil
    \includegraphics[height=1in]{figures/envs/walker2d.pdf}\hfil
    \includegraphics[height=1in]{figures/envs/ant.pdf}\hfil
    \includegraphics[height=1in]{figures/envs/antmaze.pdf}\hfil
    \caption{
    \textbf{Environments}: halfcheetah, walker2d, ant and antmaze respectively. 
}\label{fig:exp:env}


\end{figure}

\subsection{Computing infrastructure and wall-time comparison.}
We performed our experiments on a cluster that includes CPU nodes (approximately 280 cores) and GPU nodes (approximately 110 NVIDIA GPUs, ranging from Titan X to A6000, set up mostly in 4- and 8-GPU configurations). On the same cluster, the wall run time of \algname is approximately 4-5 times the run time of regular RLPD and is comparable to Off2On.

\subsection{Hyperparameters and architectures.}
We list the hyperparameters used for \algname in \autoref{tab:hyperparams}.

\begin{table}[H]
    \centering
    \begin{tabular}{l|c}
    \hline
        {\bf Parameter} & {\bf Value} \\
        \hline
        Batch size & $256$ \\
        Gradient steps $G$ & $20$ \\
        MLP Architecture & 2-Layer \\
        Network width & $256$ Units \\
        Discount & $0.99$ \\
        Learning rate & $3 \times 10^{-4}$ \\
        Ensemble size $E$ & $10$ \\
        Multiplier $k$ & $4$ \\
        $\alpha$ & $0.3$ \\
        $\gamma$ & $1$ \\
        Optimizer & Adam \\
        \hline
    \end{tabular}
    \caption{\algname hyperparameters.}
    \label{tab:hyperparams}
\end{table}



