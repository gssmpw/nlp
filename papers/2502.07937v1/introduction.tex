\section{Introduction}


Reinforcement learning~(RL) has achieved notable success in many domains, such as robotics~\citep{kober2011reinforcement,kober2013reinforcement}, videos games~\citep{mnih2013playing}, drug discovery~\citep{liu2024entropy}, generative model finetuning~\citep{touvron2023llama,liu2023drugimprover} and Large Langauge Models (LLM) reasoning~\citep{havrilla2024teaching}.



Online RL algorithms such as Q-learning~\citep{watkins1989learning}, SARSA~\citep{rummery1994line}, and PPO~\citep{schulman2017proximal} are designed to learn and make decisions in an online, sequential manner, whereby an agent interacts with an environment and learns from its experience. However, due to the need for exploration that is fundamental to RL, online RL tends to be highly sample inefficient in high-dimensional or sparse reward environments. 
A complementary approach is imitation learning (IL) \citep{ross2010efficient,ross2014reinforcement}, where an agent learns a policy by leveraging expert demonstrations.
Standard IL methods assume access to a near-optimal expert, which might not always be available. Recent methods such as MAMBA~\citep{cheng2020policy}, MAPS~\citep{liu2023active}, and RPI~\citep{liu2023blending} explore ways to utilize sub-optimal oracles by integrating multiple expert demonstrations for policy improvement.

However, in many cases, we do not have access to a live expert to query; instead, we often possess an abundance of logged data collected from experts. One approach to make use of this data is through offline reinforcement learning. Offline RL learns a policy solely from such a fixed dataset of pre-collected experiences, without the need to directly interact with the environment. Model-free approaches that incorporate dynamic programming and value bootstrapping have demonstrated remarkable efficacy within offline RL~\citep{fujimoto2019off,kumar2020conservative,fujimoto2021minimalist,kostrikov2021offline}. 
These methods operate within the actor-critic framework and typically employ a pessimistic objective, penalizing uncertain actions to mitigate distributional shift.\MW{Is this last sentence important?}

Despite its advantages, offline RL often results in a suboptimal policy due to dataset limitations. This has motivated recent work that combines offline and online RL, whereby %
learning begins from a logged dataset before transitioning to online interactions for further improvement. %
While beneficial, contemporary offline-to-online RL methods suffer from catastrophic forgetting, where previously learned knowledge is overwritten during online fine-tuning, leading to significant performance degradation~\citep{luo2023finetuning}.





Methods that integrate online RL with offline datasets utilize off-policy techniques to incorporate offline data while learning online~\citep{song2022hybrid,ball2023efficient}. 
These techniques do not require any preliminary offline RL training or incorporate specific imitation clauses that prioritize pre-existing offline data. Notably, RLPD \citep{ball2023efficient} has achieved state-of-the-art results mitigating catastrophic performance drops. %
However, it employs a uniform random sampling strategy for both offline and online learning, ignoring that different transitions contribute differently to policy improvement. %

\paragraph{Our contributions.} 
In this work, we introduce \underline{A}ctive \underline{A}dvantage-\underline{A}ligned  online \underline{R}einforcement \underline{L}earning (\algname),
a novel method that operates in the realm of online RL with an offline dataset. Our approach treats each transition uniquely, considering the varying stages of the policy.
Unlike previous methods that employ uniform random sampling of online and offline data, \algname actively assigns importance weights to each transition for policy improvement. %
In this way, \algname takes into account not just the source of a transition (i.e., whether it is from offline data or online exploration), but also its estimated contribution to policy improvement via advantage-based prioritization.


In summary, our contributions are:
\vspace{-10pt} %
\begin{itemize}[noitemsep,wide,labelwidth=0pt, labelindent=0pt]
    \item We propose \algname, a novel algorithm for online reinforcement learning with offline data. This algorithm surpasses current state-of-the-art (SOTA) methods by integrating a priority-based 
    active sampling strategy
    based on the value of advantage function and coverage by offline dataset.
    \item Unlike the previous SOTA (RLPD), which lacks theoretical support, %
    our method is motivated by a rigorous theoretical analysis, providing a strong foundation for our active sampling strategy.
    \item Through extensive empirical evaluations in various environments, we demonstrate that \algname achieves %
    consistent and significant improvements over prior SOTA models.
    \item Given the black-box nature of offline datasets, we conduct comprehensive ablation studies across a range of dataset qualities and environmental settings (including pure online settings) to validate the robustness of \algname. These ablation studies affirm its stable performance across diverse scenarios, regardless of the environmental conditions.
\end{itemize}





