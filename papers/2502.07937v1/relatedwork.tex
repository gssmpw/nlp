\section{Related Work}
\label{sec:related}

\paragraph{Offline to online RL}%








In an effort to mitigate the sample complexity of online RL~\citep{liu2023active},
offline RL utilizes fixed datasets to train policies without online interaction, however it can be prone to extrapolation errors that lead to overestimation of state-action values.
Recent off-policy actor-critic methods~\citep{fujimoto2019off, kostrikov2021offline, kumar2020conservative, wang2020critic} seek to mitigate these issues by limiting policy learning to the scope of the dataset, thereby minimizing extrapolation error. 
Strategies for reducing extrapolation error include value-constrained approaches~\citep{kumar2020conservative} that aim for conservative value estimates and policy-constrained techniques~\citep{nair2020awac} that ensure the policy remains close to the observed behavior in the data.

While offline RL methods can outperform the datasetâ€™s behavior policy, they rely entirely on static data~\citep{levine2020offline}. When the dataset has comprehensive coverage, methods like FQI~\citep{antos2007fitted} or certainty-equivalence model learning~\citep{rathnam2023unintended} can efficiently find near-optimal policies. %
However, in practical scenarios with limited data coverage, policies tend to be suboptimal. 
One approach to addressing this suboptimality is to follow offline RL with online fine-tuning, however ask discussed above, existing methods are prone to catastrophic forgetting and performance drops during fine-tuning~\citep{luo2023finetuning}. In contrast, \algname starts with online RL and then incorporates offline data to improve the policy.


\paragraph{Online RL with offline datasets}
\vspace{-0.3cm}


Several methods exist that incorporate offline datasets in online RL to enhance sample efficiency. %
Many rely on high-quality expert demonstrations~\citep{ijspeert2002learning, kim2013learning, rajeswaran2017learning, vecerik2017leveraging, nair2018overcoming, zhu2019dexterous}. 
\citet{nair2020awac} introduced the Advantage Weighted Actor Critic (AWAC), which utilizes regulated policy updates to maintain the policy's proximity to the observed data during both offline and online phases. On the other hand, \citet{lee2022offline} propose an initially pessimistic approach to avoid over-optimism and bootstrap errors in the early online phase, gradually reducing the level of pessimism as more online data becomes available.
\MW{I commented out the text that provided more detail about the methods cited above to save space, however we can add it back if you think it is particularly helpful.}
Most relevant to our work is RLPD~\citep{ball2023efficient}, which %
adopts a sample-efficient off-policy approach to learning that does not require pre-training. Like RLPD, our method does not depend on data quality and avoids constraining the policy with a behavior cloning term. Unlike RLPD, which utilizes symmetric sampling to evenly draw from both online and offline datasets for policy improvement, \algname adopts a Prioritized Experience Replay (PER)-style method, whereby it selectively uses data from both datasets to enhance policy performance.















\paragraph{Prioritized experience replay}
\vspace{-0.3cm}
Experience replay~\citep{lin1992self} enhances data efficiency in online RL by reusing past experiences.
Traditional experience replay methods select experience transitions randomly from a memory buffer, sampling at a uniform rate without regard to their importance. %
Priority Experience Replay (PER)~\citep{schaul2015prioritized} introduces prioritization based on temporal difference (TD) error to ensure that impactful experiences are used more frequently, and has proven effective in a variety of settings~\citep{jaderberg2016reinforcement, wang2016dueling, nair2018overcoming, hessel2018rainbow, oh2021model, saglam2022actor, yue2023offline, tian2023learning}. 
Alternative prioritization strategies have been explored, such as prioritizing transitions based on expected return~\citep{isele2018selective} or adjusting sample importance based on recency~\citep{fedus2020revisiting}. 
Existing research predominantly focuses on either purely online or offline applications of PER. Our research distinctively integrates the advantages of both online and offline data in an innovative way. The most relevant studies to ours include \citet{sinha2022experience} that uses the density ratio between off-policy and near-on-policy state-action distributions as an importance weight for policy evaluation, and \citet{lee2022offline} that employs density ratios to select relevant samples from offline datasets. Our method differs by not only using the density ratio to assess the ``on-policyness'' of data but also considering the advantage value to determine how data can enhance policy improvement.


\paragraph{Active learning in RL}
\vspace{-0.3cm}
Active learning has been explored in RL for data-efficient exploration %
\citep{epshteyn2008active,lopes2009active,  fang2017learning, krueger2020active, liu2022cost,liu2023active,liu2023blending}. %
Unlike previous approaches that focus on %
oracle selection~\citep{liu2023active,liu2023blending}, state exploration~\citep{epshteyn2008active,liu2023active} or reward estimation~\citep{lopes2009active},
\algname introduces an active transition sampling mechanism tailored to online RL with offline data, prioritizing transitions that maximize policy improvement.