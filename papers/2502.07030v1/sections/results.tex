\section{Results}


We have tested our method on monocular videos released with INSTA~\cite{zielonka2023insta}, as well as multi-view videos released with NerSemble~\cite{kirschstein2023nersemble} and the RenderMe-360 dataset~\cite{pan2023renderme}. Some of the reconstructed avatars are shown in Figure \ref{fig_results_collage}. We can see that hair is accurately reconstructed along with the face. Figure \ref{fig_results_beard} shows additional results highlighting our method's performance on facial hair. The prism lattice covering the face can not only be used to accurately reconstruct facial hair, but also to deform the hair in response to different facial expressions.

The trained and exported models were animated using head tracking data in our viewer web app. They were verified to run at 60 fps on iPhone Pro 14, as well as a 4th generation iPad Pro. They are also fully compatible with the Samsung Galaxy S9, an Android phone released in 2018. The framerate appears to be capped using VSync, since it never fluctuates above or below 60 fps. We were unable to disable VSync due to the implementation constraints imposed by the web browser. Therefore, the uncapped framerate could potentially be higher than 60 fps.

The average download size for the avatar is 70 MB. Google Chrome tabs running the avatar viewer use 206 MB CPU RAM and 46 MB GPU VRAM on average. The low VRAM usage reflects the simplicity and efficiency of our hybrid representation of the head.

We also measured the quality of our rendered images at different stages of the avatar generation process (Table \ref{table_quantitative}). We find that the opacity binarization during the second stage of training suppresses small artifacts in the model, resulting in a slight improvement in the metrics. As to be expected, there is a slight drop in the image quality introduced by the model export process. The metrics are compared against three recent head avatar reconstruction methods: PointAvatar~\cite{zheng2023pointavatar}, FLARE~\cite{bharadwaj2023flare} and INSTA~\cite{zielonka2023insta}. Since the publicly released implementations of these methods use monocular head tracking, we only use the monocular videos released with INSTA for a fair quantitative comparison. We use the original non-overlapping train-test splits for each video. The results show that, despite the fact that our method prioritizes edge device compatibility and produces a compact distilled model, our image quality metrics are still competitive with current state-of-the-art avatar methods which run on desktop devices. 