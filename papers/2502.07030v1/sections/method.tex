\section{Method}
\label{sec_method}


\subsection{Overview}
\label{subsec_method_overview}

Our method takes as input a series of matted images of a head, along with camera and 3DMM parameters obtained using a head tracker. These images are used to train a hybrid mesh-volumetric model of a head using an analysis-by-synthesis approach. The mesh representation is based on the FLAME 3D morphable model~\cite{li2017flame}, which adequately models much of the face and neck. The 3DMM does not model hair, and hair is not easily modeled by a surface representation such as a mesh. Therefore, we use a neural radiance field (NeRF)~\cite{mildenhall2021nerf} as the volumetric representation for scalp hair and facial hair. In order to allow the NeRF to move and deform along with the motion of the head, we construct a rigged prism lattice structure on top of the 3DMM mesh (Section \ref{subsec_lattice}). Ray intersections with this lattice are mapped to the canonical space of the undeformed model. The NeRF is represented by three networks (described in Section \ref{subsec_model_networks}), which allows sampling of a view-indepdendent neural feature at points in the canonical space. We use a hybrid rendering approach (Section \ref{subsec_model_rendering}) to train the NeRF in two stages. Finally, we export a compact triangle-based avatar model from the hybrid model (Section \ref{subsec_model_export}), which allows for efficient animation and neural surface rendering on edge devices.

\subsection{Prism Lattice}
\label{subsec_lattice}

Figure \ref{fig_lattices} shows an example of a prism lattice constructed on a FLAME mesh.
Starting with the FLAME template mesh, we manually mark regions of the mesh to be used as the base of prism lattices.  
For example, to reconstruct hair on the scalp, we mark triangles belonging to the scalp. We then subdivide each of these triangles into 4 or 16 triangles to ensure a sufficiently dense lattice. The subdivided triangles are then extruded along the normal of each vertex, forming a layer of prisms. These extrusions are repeated to obtain a stack of up to 16 layers of prisms. We repeat this extrusion process separately for all 400 FLAME shape and expression blendshapes. This allows us to create a new blendshape model that incorporates the prism lattice as part of the 3DMM. The extruded vertices share the linear blend skinning weights and pose corrective blendshapes of the base vertices, thereby allowing them to respond to skeletal animation of the FLAME model. To reconstruct facial hair, we add additional lattices for the sideburns and around the mouth (shown in Figure \ref{fig_results_beard}).

\subsection{Trained Networks}
\label{subsec_model_networks}

\begin{figure}
    \includegraphics[width=250pt]{figures/hybrid_rays.png}
    \caption{Illustration of different scenarios in ray intersection. Ray $R_1$ hits the FLAME mesh first, so its color is sampled from a learned texture. Ray $R_3$ intersects only triangles of the lattice, so its color is obtained by a volume rendering integral over the intersection points. Ray $R_2$ intersects the lattice before terminating on the FLAME mesh, so its color is obtained by interpolating the volume rendering integral before the last intersection with the color of the final intersection, using the accumulated opacity as the interpolation factor.}
    \Description{Replace this with a text description of the figure for someone who cannot see it.}
    \label{fig_hybrid_rays}
\end{figure}

\begin{figure*}[h!]
    \centering \includegraphics[width=\textwidth]{figures/collage.jpg}
    \caption{A sample of head avatars reconstructed using our method.}
    \Description{Replace this with a text description of the figure for someone who cannot see it.}
    \label{fig_results_collage}
\end{figure*}

Our NeRF network architecture differs in a couple of ways from traditional NeRF models. Firstly, we predict opacity or \textit{alpha} for sample points when integrating along rays, similar to previous work~\cite{lombardi2019neural, attal2022learning, chen2023mobilenerf}. Using alpha instead of the density which is used in traditional NeRF models enables our final exported model to utilize alpha testing during rendering. Secondly, we compute the view-dependent color at a point in two steps, similar to MobileNeRF~\cite{chen2023mobilenerf}. The first step produces a view-independent neural feature vector. The second step produces a color conditioned on the view direction.
We train three neural networks for the volumetric regions:
\begin{enumerate}
    \item An opacity prediction network $\mathcal{A}$, which predicts the opacity associated with a point in the canonical space of the NeRF. Predictions from the final trained network are used to generate alpha textures for the exported model. For a canonical point $\textbf{p}_k$, the predicted opacity is given by:
    \begin{flalign}
        && \alpha_k = \mathcal{A}(\textbf{p}_k;\theta_\mathcal{A})
        && \mathcal{A}:\mathbb{R}^3 \rightarrow [0,1]
    \end{flalign}
    \item A feature prediction network $\mathcal{F}$, which predicts an 8D neural feature vector associated with a point in the canonical space of the NeRF. Predictions from the final trained network are used to generate a neural texture for the exported model. For a canonical point $\textbf{p}_k$, the predicted feature is given by:
    \begin{flalign}
        && \textbf{f}_k = \mathcal{F}(\textbf{p}_k;\theta_\mathcal{F})
        && \mathcal{F}:\mathbb{R}^3 \rightarrow [0,1]^8
    \end{flalign}
    \item A small color prediction network $\mathcal{C}$, which predicts a color given a viewing direction and an 8D neural feature. The weights of this network are exported along with the avatar and used for deferred neural rendering on the edge device. For a feature vector $\textbf{f}_k$, and ray direction $\textbf{d}$, the predicted color is given by:
    \begin{flalign}
        && c_k = \mathcal{C}(\textbf{f}_k, \textbf{d};\theta_\mathcal{C})
        && \mathcal{C}: [0,1]^8 \times [-1,1]^3 \rightarrow [0,1]^3
    \end{flalign}
\end{enumerate}
The color prediction network is a simple multilayer perceptron (MLP) which concatenates an 8D neural feature vector with a 3D direction and passes them through two hidden layers of 16 neurons, with ReLU activations and sigmoid activation at the end to obtain a linear RGB color. Both the opacity prediction network and the feature prediction network utilize a learned hash-grid positional encoding and fully fused MLP architecture based on InstantNGP~\cite{muller2022instantngp}. This allows them to maximize their capacity for portions of the unit cube which are occupied by volumetric data. The two fully fused MLPs have 4 hidden layers of 128 neurons, with ReLU activations in between and sigmoid activation at the end.

\subsection{Hybrid Rendering}
\label{subsec_model_rendering}

Our model is rendered in a hybrid manner in which the areas enclosed by the prism lattices are rendered as deformable NeRFs, with the texture of the FLAME mesh incorporated into the ray integration in semi-transparent regions. By contrast, the base FLAME mesh is treated as an ordinary textured mesh with a learned texture and a learned alpha map. To handle the mouth interior, we fill the hole in the FLAME mesh for the mouth cavity. The triangles of the sealed hole are rendered with a learned 2D texture for the mouth cavity.

In standard NeRF rendering, ray marching is used to obtain sample points for integration along each ray. We do not take this approach. Instead, we exploit the efficiency of GPU raytracing hardware to intersect the ray with the lattice triangles. This provides us with a dense set of sample points along rays that pass through the volumetric portion of the model.
We use a custom differentiable renderer that shoots rays which intersect with both the FLAME mesh as well as the prism lattices. Rays either stop as soon as they hit the FLAME mesh, or continue to collect intersections with the prism lattices, up to a maximum of 64 intersections (illustration in Figure \ref{fig_hybrid_rays}). A triangle bounding volume hierarchy (BVH) is used to accelerate ray intersection tests. 


When a ray hits the base FLAME mesh, the intersection point is assigned an opacity of $1$, and its color is sampled from the learned face texture instead of the neural rendering pipeline. Rays that hit the mouth cavity triangles are similarly treated as opaque, but with a separate learned texture.

There are two stages of training. The first stage trains all three networks, while the second stage freezes the opacity network. The equations for the integrated pixel intensity over intersections $\textbf{p}_k$ along a ray with origin $\textbf{o}$ and direction $\textbf{d}$ are:
\begin{equation}
    \textbf{I}(\textbf{o}, \textbf{d}) = \sum_{k=1}^{K} T_k \alpha_k \mathcal{C}(\mathcal{F}(\textbf{p}_k), \textbf{d}) 
\end{equation}
\begin{equation}
    T_k = \prod_{l=1}^{k-1}(1 - \alpha_l)
\end{equation}
Here, $\alpha_k$ is either computed using the opacity network for ray-lattice intersections, or is assigned a fixed opacity of $1$ in the case of ray-FLAME intersections. The $\mathcal{C}$ term is replaced with a color sampled from the learned face texture for ray-FLAME intersections.

The second stage of training changes the rendering algorithm to mimic the effects of triangle rasterization and deferred neural rendering. In this stage, each ray has exactly one associated feature vector, which is obtained as a weighted average of predicted feature vectors along the ray. The color network $\mathcal{C}$ is therefore only executed once per pixel. The ray integration for the second stage is given by:


\begin{equation}
    \label{eq_stage2_rendering}
    \textbf{I}(\textbf{o}, \textbf{d}) = \mathcal{C}\left(\sum_{k=1}^{K} T_k \alpha_k \mathcal{F}(\textbf{p}_k), \textbf{d}\right)
\end{equation}

This modified formulation also requires a different approach to handling rays that intersect both the lattice and the FLAME mesh. If the final ray intersection $K$ hits the FLAME mesh, we first compute the intensity excluding that intersection as $\textbf{I}_{lat}$. We sample the color of the FLAME mesh $\textbf{I}_{flame}$ at the intersection point from the learned texture, and then linearly interpolate between the two colors based on the accumulated transmittance up to that point:
\begin{equation}
    \textbf{I} = (1 - T_K)\textbf{I}_{lat} + T_K \textbf{I}_{flame}
\end{equation}

\subsection{Training Losses}
\label{subsec_model_losses}

There are two losses which are minimized in both training stages:
\begin{enumerate}
    \item A photometric loss $L_{photo}$, which is an $\ell_1$ log-sRGB loss used in previous work~\cite{hasselgren2021nvdiffmodeling, munkberg2022nvdiffrec}.
    \item An alpha or silhouette regularizer $L_{alpha}$, which is the mean square of the predicted alpha $\sum_{k=1}^{K} T_k \alpha_k$ for all pixels corresponding to the ground truth background mask. This encourages the networks to explain unoccupied regions by transparency as opposed to opaque surfaces with the background color.
\end{enumerate}

In addition, in the second stage we wish to binarize the predicted alpha values so that they can later be used for alpha testing (as opposed to alpha blending). This is done by applying a straight-through estimator, in line with previous work~\cite{bengio2013estimating, chen2023mobilenerf}. However, in order to better stabilize training, we calculate two versions of all losses: with and without the straight-through estimator. We gradually interpolate from the non-binarized losses to the binarized losses over the course of training.


\subsection{Model Export}
\label{subsec_model_export}

\begin{figure*}[h!]
    \centering \includegraphics[width=\textwidth]{figures/beard.png}
    \caption{Using a prism lattice which covers the face allows us to reconstruct facial hair. Thick dark hair and thin blond hair are both reconstructed by our method. Top: The prism lattice for facial hair and an example of a reconstructed avatar. Bottom: Frames showing the deformation of the mustache in response to changes in the facial expression.}
    \Description{Replace this with a text description of the figure for someone who cannot see it.}
    \label{fig_results_beard}
\end{figure*}

The model we wish to export is a rigged triangular mesh with accompanying texture maps for combining deferred neural rendering with standard texture rendering. In order to obtain this triangular mesh, we export the entire FLAME mesh, but we cannot simply export the entire prism lattice. This is because the majority of the triangles in the lattice would be either completely transparent or occluded by other triangles. Therefore, the first export step is to prune occluded and transparent triangles. For each view in the training frames, we shoot rays through every pixel towards the prism lattice. The alpha values at the hit points are obtained by transforming them to the canonical space and passing them through the opacity network. If the predicted opacity is less than $0.5$, the ray continues on its trajectory, otherwise we stop the ray and record which triangle was hit. Repeating this process for different views gives us a list of triangles to keep. We then re-index the prism lattice without the pruned triangles.

There are three large texture maps which we generate for deferred neural rendering of the lattice triangles: an alpha map and two feature maps. Having two feature maps allows us to split the 8D feature vectors into two sets of RGBA channels, enabling compression of the textures as PNG. Each lattice triangle is assigned to a $16\times 16$ square cell of each texture map. The texture maps can be regarded as grids of these square cells, where the height and width of the grids are obtained by taking the square root of the number of lattice triangles and rounding up. For each square cell, we uniformly sample a $16\times 16$ grid of 3D points on the corresponding canonical lattice triangle. Each of the sampled 3D points is passed through the opacity and feature networks, giving us the values to be inserted into the alpha and feature maps. 
The uniform sampling of points over the triangle is achieved using a low-distortion mapping from a square to a triangle~\cite{heitz2019low}, allowing us to utilize the entire square cell of the texture map to store texture data for a triangle. The tradeoff of this mapping is that for easy rendering at inference time, we need to split every triangle in two, in order to have UV coordinates pointing to all four corners of the corresponding square region of the texture map. 

Both the FLAME mesh as well as our UV mapping of the prism lattice result in multiple UV coordinates corresponding to the same triangle vertex. For compatibility with traditional rendering pipelines, we duplicate such vertices so that each vertex has a unique UV coordinate. Besides the rigged mesh and textures, we also export the weights of the color network $\mathcal{C}$, to be used in the neural shader.

