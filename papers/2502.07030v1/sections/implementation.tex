\section{Implementation}
\label{sec_implementation}

\begin{table*}[h!]
    \centering

    \begin{tabularx}{300 pt}{X|c|c|c|c}
    \hline
    Method & PSNR$\uparrow$          & SSIM$\uparrow$           & MS-SSIM$\uparrow$        & LPIPS$\downarrow$           \\ \hline
    PointAvatar~\cite{zheng2023pointavatar}   & 25.0 & 0.903 & 0.936 & 0.0717 \\ 
    FLARE~\cite{bharadwaj2023flare}   & 27.9 & 0.904 & 0.946 & 0.0602 \\ 
    INSTA~\cite{zielonka2023insta}   & \textbf{32.5} & \textbf{0.953} & \textbf{0.977} & \textbf{0.0453} \\ \hline
    Ours (before binarization)   & 31.3 & 0.942 & 0.970 & 0.0590 \\ 
    Ours (after binarization)   & 32.0 & 0.944 & 0.973 & 0.0593 \\ 
    Ours (after export)   & 30.4 & 0.929 & 0.960 & 0.0690 \\ 
    \hline
    \end{tabularx}
    \caption{Quantitative evaluation }
    \label{table_quantitative}
\end{table*}

We implemented separately the pipeline for training our \mbox{PrismAvatar} models, and a viewer web app compatible with mobile device browsers. We also implemented a headless renderer that reuses the rendering code of the viewer web app to allow accurate quantitative evaluation of images rendered from our exported models.

\subsection{Training}
Our training pipeline is implemented in PyTorch, with certain components implemented as custom Python modules written in C++. Our raytracing module was implemented using NVIDIA Optix 8.0, in order to utilize RTX cores to accelerate intersection tests. Using RTX cores for acceleration is made possible by the fact that we are intersecting rays with triangles, and not with a volume or implicit surface. We rebuild the BVH acceleration structure from scratch at the start of each training epoch to avoid degradation of the acceleration structure due to frequent updates.

Each training batch is a random sample of $2^{16}$ rays for each training image. We trained our models on a single NVIDIA A6000 GPU. The models are trained on 90,000 batches for each training stage, taking around 90 minutes per stage on average. We use the Adam optimizer~\cite{kingma2015adam}, with a learning rate of $5\times 10^{-5}$ for neural networks and $5\times 10^{-4}$ for learned textures.

\subsection{Viewer Web App}
In order to maximize cross-platform compatiblity across different edge devices for the 3D avatar viewer while also having fast performance, we implemented our viewer as a static web application using C++ compiled to WebAssembly via Emscripten~\cite{emscripten}. Our exported model contains a single rigged mesh, with different shading for the FLAME head and the triangle soup extracted from the prism lattice. The viewer renders the model in two passes:
\begin{enumerate}
    \item The FLAME mesh is skinned and shaded with the learned texture and alpha map. Then, the lattice triangles are skinned and shaded using a fragment shader implementation of the color network $\mathcal{C}$.
    \item A post-processing pass converts the linear RGB color to sRGB.
\end{enumerate}


\subsection{Video Preprocessing}
Our method requires background matting for input images, as well as camera information and 3DMM parameters for each frame. In order to conserve memory, high-resolution videos are downsampled to have a width or height of 512 pixels. \\
\textbf{Background removal.} The backgrounds of videos are removed using Robust Video Matting~\cite{lin2022robust} if no background mask is provided. This is followed by the removal of regions classified as clothing by a pretrained BiSeNet~\cite{yu2018bisenet}. \\
\textbf{Head tracking.} The videos are processed using a head tracker to provide camera intrinsics, camera extrinsics and FLAME 2023 model parameters for every frame. We use FlowFace~\cite{flowface} for head tracking on both monocular as well as multi-view videos.