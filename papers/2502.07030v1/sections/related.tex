\section{Related Work}

\subsection{3D Head Avatars}

Reconstructing 3D head avatars has traditionally involved multi-camera 3D reconstruction~\cite{bradley2010high,beeler2010high}, sometimes combined with markers or face paint~\cite{furukawa2009dense}. 
Most recent work has moved towards leveraging neural rendering to facilitate faster reconstruction from monocular videos, as well as to simplify the reconstruction of complex surfaces and materials. 
Implicit neural representations of surfaces such as signed distance functions SDFs~\cite{zheng2022avatar} and point clouds~\cite{zheng2023pointavatar, wang2023npbva} have seen a lot of use due to their ability to smoothly change topology during the reconstruction process. 
In addition, volumetric neural rendering approaches have been explored due to the potential for more photorealistic reconstruction of hair and other regions of the head with complex geometry~\cite{lombardi2019neural, lombardi2021mvp, cao2022ava}.
3D morphable models (3DMMs) of heads such as the Basel Face Model~\cite{blanz1999bfm} and FLAME~\cite{li2017flame} are often incorporated into models to guide the deformation of an implicit field. 
Such approaches range from simply conditioning a volumetric field on blendshape coefficients~\cite{gafni2021nerface}, to treating a 3DMM as a proxy surface model which guides the deformation of a volumetric field~\cite{athar2022rignerf,athar2023flameinnerf,bai2023monoavatar,zielonka2023insta}. 
Another line of work focuses on learning latent representations of 3D avatars instead of fields or explicit shapes~\cite{xu2023latentavatar,ma2021pixel}. 
Such methods would require real-time neural network inference if deployed on edge devices, which places them well out of reach of cross-platform edge device browsers with minimal GPU programming capabilities.
There are also methods such as Neural Head Avatars (NHA)~\cite{grassal2022nha}, ROME~\cite{khakhulin2022rome} and FLARE~\cite{bharadwaj2023flare}, which use triangular meshes as the underlying representation.
Our final exported model is also a triangular mesh representation with a neural texture~\cite{thies2019deferred}. However, we use a hybrid mesh-volumetric model at training time.

\begin{figure*}[h!]
    \includegraphics[width=250pt]{figures/lattices_wojtek.png}
    \caption{Left: The volumetric field for hair in our hybrid model is defined over a prism lattice, constructed as described in Section \ref{subsec_lattice}. Right: At the time of export (Section \ref{subsec_model_export}), we prune the lattice to remove triangles that are invisible or occluded.}
    \Description{Replace this with a text description of the figure for someone who cannot see it.}
    \label{fig_lattices}
\end{figure*}

\subsection{Fast NeRF Rendering}

Since the inception of NeRFs~\cite{mildenhall2021nerf}, numerous methods have been proposed to improve the speed of training and inference. 
Several methods use regular grids with precomputed features to improve rendering speed. FastNeRF~\cite{garbin2021fastnerf} and Plenoxels~\cite{fridovich2022plenoxels} both use dense voxel grids, although they differ in the nature of the features stored in the grid. KiloNeRF~\cite{reiser2021kilonerf} uses a grid of thousands of small MLPs that specialize in reconstructing a small portion of a scene. PlenOctrees~\cite{yu2021plenoctrees} uses a sparse voxel octree containing opacity and color represented as spherical harmonic coefficients (as is the case with Plenoxels).
SNeRG~\cite{hedman2021snerg} ``bakes'' a trained NeRF into a grid data structure suitable for fast rendering, and is the first NeRF implementation that was shown to run in real-time on low-resource mobile devices.
The aforementioned methods obtain their speedups at the expense of high memory requirements to store data structures and associated features.

AutoInt~\cite{lindell2021autoint} learns partial volume rendering integrals along each ray in order to reduce network evaluations by piecewise ray-marching. R2L~\cite{wang2022r2l} later showed that it is possible to learn neural light fields (NeLFs) by distilling NeRFs. This approach can be considered as equivalent to learning complete integrals along each ray in a NeRF. DyLin~\cite{yu2023dylin} extended R2L to dynamic scenes by conditioning on either time or arbitrary control parameters. MobileR2L~\cite{cao2023mobiler2l} and LightSpeedR2L~\cite{gupta2023lightspeed} later showed that the R2L distillation approach can be used to produce light fields that can be evaluated efficiently on mobile devices. 

In contrast to the light field distillation methods, MobileNeRF~\cite{chen2023mobilenerf} showed that a NeRF trained on a domain covered by a regular grid can be distilled into a set of triangles that can be efficiently rendered with a neural texture on mobile devices. 
Concurrently with our work, adaptive shells~\cite{wang2023adaptive} and VMesh~\cite{guo2023vmesh} have sought to obtain speedups via hybrid mesh-volume reconstruction of static scenes, with the hybrid NeRF-SDF model NeuS~\cite{wang2021neus} as their starting point.
We take a similar approach to MobileNeRF, given our focus on edge device compatibility. Unlike MobileNeRF, which uses a regular cubic grid and can only handle static scenes, we reconstruct an animatable head avatar model.
