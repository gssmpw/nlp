\section{\AlgoName Design}
\label{sec:algo}

% In this section, we first present the overview of \AlgoName, followed by its detailed solutions.

\if 0

\PHM{Other design assumptions.} 
There are two design assumptions for \AlgoName: 1) a model instance can be fully hosted on a single GPU server, and 2) one-to-one paired data transfers across GPU clusters incur negligible network interference.
We note that these two assumptions are easily met in practice.
First, a GPU worker typical has hundreds of aggregated GPU memory (e.g., 640~GB for an 8-A100 node), which is sufficient to host most large models~\footnote{\AlgoName can also be extended to support large models spanning across multiple GPU workers, which we leave for a future work.}.
Additionally, data-center networks often employ optimized topologies, such as fat tree~\cite{fattree}, ensuring high aggregated bandwith and minimal contention.
\fi


\subsection{Design Rationale and Overview}

\AlgoName aims to efficiently support collaborative, distributed inference execution before nodes receive the entire model.
The key objective is to maximize the overall system throughput ---measured in tokens generated per second for LLM inference
% ~\footnote{Supporting fast scaling for small models is easier than LLMs, which we discuss in \S\ref{sec:discussion}.}
---which in turn reduces request queueing under load spikes.
We therefore design \AlgoName based on \emph{two key principles}. 
1) Minimizing delays in assembling complete model instances across nodes enables distributed inference to begin as early as possible.
2) Reducing data movement (e.g., KV caches in LLMs) across nodes enhances overall efficiency.

\AlgoName introduces the abstraction of an \emph{execution pipeline} to facilitate distributed inference.
An execution pipeline serves as a model-serving instance spanning a group of nodes that collectively maintain a complete model and jointly perform pipeline parallelism (Fig.~\ref{fig:scale_example}).
Inference requests are assigned to specific execution pipelines, and the designated pipeline iteratively computes all output tokens (Fig.~\ref{fig:pipeline} (a)).
This minimizes the amount of intermediate results exchanged between nodes and eliminates the need to transfer the KV cache, ensuring efficient distributed execution.

\AlgoName provides efficient supports for execution pipelines through three key designs.
First, \AlgoName extends the binomial pipeline to enable adaptive model multicast, supporting fast model distribution under various scaling scenarios and minimizing the time required to generate execution pipelines. 
Second, \AlgoName adopts an efficient strategy to generate execution pipelines and fully leverage the advantages of adaptive model multicast, which improves overall inference performance.
Finally, \AlgoName allows participating nodes to switch to local execution mode once they have received the full model replica.
In addition to cross-node scaling, execution pipelines can also be applied in memory-based model loading, which we describe in \S\ref{sec:system}.
% Beyond network-based model scaling, \AlgoName can also enhance scaling performance in host-to-GPU model loading scenarios, as detailed in \S\ref{sec:system}.

\subsection{Adaptive Model Multicast}
\label{subsec:model_multicast}

\PHM{Multicast modeling and sub-groups.}
We begin by modeling the multicast process in \AlgoName.
Consider a cluster of $N$ nodes and a scaling scenario where the source node holds a model instance and needs to distribute it to the remaining $N - 1$ destination nodes.
We denote this scaling process as $1 \rightarrow N$.
Assume the size of model is $M$ and it is partitioned into $b$ blocks for multicast. 
% \alexcomment{To achieve efficient distribution, we use the Binomial Pipeline~\cite{binomial-pipe}, which organizes multicast communication into log-based stages, minimizing steps for data dissemination. In this approach, the source nodes send blocks to one or multiple nodes per step, and recipients propagate blocks further. This method is asymptotically optimal for minimizing multicast time~\cite{rdmc}, providing a scalable and efficient foundation for our adaptive multicast strategy.} 
Following the binomial pipeline (Fig.~\ref{fig:overview}), $1 \rightarrow N$ takes $b + \lceil \log N \rceil - 1$ steps to complete, where all nodes can have at least one block after the first $\lceil \log N \rceil$ steps and take remaining $b - 1$ steps to receive other blocks.
Let $t$ be the time required for each step and $T$ be the total end-to-end multicast time.
We have $t \propto M/b$, i.e., in proportion to the amount of data exchanged between each pair of nodes in one step, and $T \propto M (1 + \log N / b) $.

More generally, we consider a scaling operation $k \rightarrow N$, i.e., $k$ nodes distributing the model to the remaining $N - k$ nodes (where $1 \le k < N$)~\footnote{$k \geq 1$ can be easily met in practice, such as by maintaining at least one model replica in host memory across the cluster.}.
\AlgoName evenly divides the $N$ nodes into $k$ sub-groups.
Each sub-group consists of $L$ nodes, where $L$ is either $\lfloor N/k \rfloor$ or $N\%k$, and each sub-group performs a $1 \rightarrow L$ scaling. 
This process supports $k \rightarrow N$ scaling for arbitrary values of $k$ and $N$. 


\PHM{Selective block sizes.}
Determining the block size $b$ is critical to the performance of model multicast.
\AlgoName selectively configures this parameter to balance the transmission performance and execution efficiency.
We note that coarse-grained model partitioning (i.e., a small $b$) often results in longer end-to-end transmission times (i.e., a large $T$), while increasing $b$ generates more model blocks and leads to additional communication overhead for intermediate results during pipeline execution.
According to our modeling, $T$ exhibits an "elbow point" with respect to $b$: increasing the block size initially enhances transmission performance, but the benefits diminish beyond a certain threshold (detailed in Fig.~\ref{fig:ablation_num_blocks})
Therefore, \AlgoName configures this point as $b$, achieving good transmission performance while minimizing the additional overhead in pipeline execution.
% Fig. xxx illustrates the optimal block size for multicasting Llama-7B in our testbed.
Notably, configuring $b$ requires only offline profiling and has no impact on runtime performance.


\begin{algorithm}[tb]
    \caption{$k$-Way Transmission Strategy}
    \small
    \label{alg:k_way}
    \begin{algorithmic}[1]
    \Statex \textbf{Input:}
    \Statex \quad -- $k$ sub-groups $\{G_0, \dots, G_{k-1}\}$
    \Statex \quad -- $b$ ordered model blocks $\{M_0, \dots, M_{b-1}\}$
    \Statex \textbf{Output:}
    \Statex \quad -- Block transfer orders for $k$ sub-groups $\{O_0, \dots, O_{k-1}\}$
    
    \State $l \gets \lceil b / k \rceil$ \Comment{Size of block chunks}
    \State $\mathcal{S} \gets \{ \{M_j\ \mid j \in [l \cdot i , \min (l \cdot (i +1), b) - 1]\}  \mid i \in [0, k-1]\}$ \Comment{Partition blocks into $k$ chunks}

    \For{$i \in [0, k-1]$} \Comment{Generate $O_i$ via circular shift}
        \State $O_i \gets \biguplus_{j=0}^{k-1} \mathcal{S}_{(i+j) \bmod k}$
    \EndFor

    \end{algorithmic}
\end{algorithm}

\begin{figure}
    \centering
    \includegraphics[width=0.475\textwidth]{figures/scaling_example.pdf}
    \vspace{-20pt}
    \caption{Example of $2 \rightarrow 8$ scaling.
    \textit{\textmd{Each sub-group transfers model chunks in circularly shifted order in parallel, to construct three pipeline parallel inference execution flows (Node 3 and 6, Node 4 and 7, and Node 5 and 8). This strategy allows multiple execution pipelines to be started as soon as enough model blocks are distributed while enabling non-blocking binomial pipeline multicast. 
    %\todo{mark in fig block chunks}
    }}
    }
    \vspace{-5pt}
    % \vspace{-.2in}
    \label{fig:scale_example}
\end{figure}

\PHM{Optimized transfer order.}
We propose a $k$-way transmission strategy to optimize the order of model block transfers across $k$ sub-groups, which enables \AlgoName to minimize the time required for assembling the complete model.
Algorithm~\ref{alg:k_way} outlines the $k$-way transmission strategy. 
It first partitions the model blocks into $k$ equal-sized chunks (lines 1-2) and then generates the block transfer orders for each sub-group by circularly shifting these chunks (lines 3-4). 
This design ensures that the sub-groups work in tandem, with the first complete model instance becoming available after just $b/k$ time steps. 
% \yuecomment{I completely rewrote the following design.}
\if 0
Fig.~\ref{fig:scale_example} demonstrates an example of 2-way transmission in a $2 \rightarrow 8$ scaling.
For each of the two sub-groups, the source node (nodes 1 and 2) transfers a total of four blocks to three destination nodes.
The blocks are grouped into two chunks (blocks 1-2 and blocks 3-4), with the transfer orders arranged in a complementary fashion across the sub-groups. 
This approach enables the destination nodes from different sub-groups to collaboratively assemble complete model instances and construct three execution pipelines. 
\fi 
Fig.~\ref{fig:scale_example} illustrates a $2 \rightarrow 8$, 2-way transmission scaling scenario, where two source nodes (Node 1 and Node 2) distributed model blocks to six destination nodes across two sub-groups. The model is partitioned into four blocks, which are grouped into two equal-sized chunks: Blocks 1-2 and Blocks 3-4. To improve the overall throughput, each sub-group follows a {\bf circular shifting strategy} when transferring model blocks. Specifically, Sub-group 1 (Node 1 as the source) starts by sending model blocks in the order of Blocks 1-2 and Blocks 3-4 to its destination nodes (Node 3-5). Meanwhile, Sub-group 2 (Node 2 as the source) follows a complementary pattern, transferring model blocks in the {\bf circular shifted order} (Blocks 3-4 followed by Blocks 1-2) to its destination nodes (Node 6-8). 
%in the order of Blocks 3-4 and Blocks 1-2 (by circularly shifting the two block chunks) to its destination nodes (Node 6-8). 
This circular shifting of block chunks ensures that different groups of destination nodes receive different parts of the model concurrently, allowing them to collaboratively construct three model inference execution pipelines using pipeline parallelism. As the multicast progresses, the destination nodes incrementally collect and assemble complete model instances, enabling efficient and scalable model execution. 
%first destination node (Node 3). Once received, Node 3 immediately forwards Blocks 


\begin{algorithm}[h]
    \caption{Execution Pipeline Generation Strategy}
    \small
    \label{alg:pipeline_gen}
    \begin{algorithmic}[1]
    \Statex \textbf{Input:}\quad -- $k$ sub-groups $\{G_0, \dots, G_{k-1}\}$
    % \Statex \quad -- $k$ sub-groups $\{G_0, \dots, G_{k-1}\}$
    \Statex \quad -- $L_i$: the number of nodes in $G_i$
    \Statex \quad -- $n_i^j$: the $j^{th}$ node in $G_i$
    \Statex \textbf{Output:}\quad -- All generated execution pipelines $\mathcal{P}$
    % \Statex \quad -- All generated execution pipelines $\mathcal{P}$

    \State $\mathcal{G} \gets$ sub-groups with unassigned nodes
    \While{$|\mathcal{G}| > 0$}
        \If{$|\mathcal{G}| = 1$} \Comment{A pipeline within a single sub-group}
            \State $P$ $\gets$ ordered nodes in $\mathcal{G}_0$
            \State $\mathcal{P} \gets$ append($\mathcal{P}$, $P$)
        \Else \Comment{Generate pipelines across sub-groups}
            \State $a \gets \min_{G_i \in \mathcal{G}} L_i$     
            \For{$t \in [0, a-1]$} 
                \State $P$ $\gets$ []
                \For{$G_i \in \mathcal{G}$}
                    \State $P$ $\gets$ append($P$, $n_i^t$)
                \EndFor
               \State $\mathcal{P} \gets$ append($\mathcal{P}$, $P$)
            \EndFor
        \EndIf
        \State {Update $\mathcal{G}$}
    \EndWhile
\end{algorithmic}
\end{algorithm}

\vspace{-8pt} 

\subsection{Pipelined Inference Execution}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/pipeline_cases.pdf}
    \vspace{-15pt} 
    \caption{Example of execution pipelines. 
    \textit{\textmd{(a)~A 4-node execution pipeline where each node executes its associated model block for multiple in-flight requests in parallel. 
    (b)~Execution pipelines across multi-GPU nodes where each pipeline contains GPUs from different nodes.
    (c)~Intra-node model replication to scale up the inference performance, where each local model block replica forms a separate cross-node execution pipeline.}}
    %\todo{Add Model blocks. Add GPU for (a). Add node for (b). Redraw (a) by adding comm arrows explicitly.} 
    %\todo{Add another subfig for multi-GPU models.} }} 
    }
    \label{fig:pipeline}
    \vspace{-1em}
\end{figure}


\PHM{Generating execution pipelines.}
Building on the optimized block transfer order, we propose an efficient execution pipeline generation strategy outlined in Algorithm~\ref{alg:pipeline_gen}. 
%in Algorithm~\ref{alg:pipeline_gen} for generating execution pipelines.
The key idea is to construct execution pipelines from as many sub-groups as possible, maximizing the benefits of $k$-way transmission. 
%to fully leverage the advantages of $k$-way transmission.
Specifically, when the remaining unassigned nodes belong to only one sub-group, \AlgoName directly forms an execution pipeline using these nodes (lines 3-5). Otherwise, it selects one node from each available sub-group to construct the pipeline (lines 6-12). 
\SysName prioritizes pipeline configurations where each sub-group has an equal number of nodes (i.e., $N$ mod $k = 0$). 
%generates pipelines by selecting one node from each of the available sub-groups (lines 6-12). 
%To maximize pipeline execution efficiency, \SysName prioritizes each sub-group having the same number of nodes (i.e., $N$ mod $k = 0$).


\if 0
\PHM{Request Scheduling for Execution Pipelines.}
\SysName employs dynamic load balancing through runtime coordination. Each execution pipeline's maximum concurrent capacity is pre-defined by its GPU count (e.g., a 4-GPU pipeline supports 4 compute slots). A lightweight scheduler continuously tracks compute occupancy across active pipelines, dynamically dispatching requests to pipelines with available compute slots. This vacancy-driven approach ensures optimal utilization by matching incoming inference workloads to execution pipelines with immediate free compute resources, enabling low-latency distributed inference

\fi

% chaobo comment
% \yuecomment{I rewrote the following para. Do sanity check.} 
\PHM{2D execution pipelines.}
Fig.~\ref{fig:pipeline} (a) illustrates how a 4-node execution pipeline processes multiple batches (one or multiple requests) of inference requests in parallel using a 2-dimensional pipelining strategy. Along the first dimension, each node is assigned a specific model block and computes a different batch of requests. Once a node completes processing a batch for its assigned block, it passes the intermediate result to the next node and starts computing the next batch along the second dimension. 
%the next output token is generated when the final model block completes its computation. 
This 2D pipeline strategy efficiently utilizes resources distributed along the multicast route, enabling rapid scaling and processing of accumulated requests during spikes. 
%consume the quickly accumulated requests during a spike. 
\SysName schedules requests across multiple pipelines based on their available resources to improve overall resource efficiency.


\if 0
Fig.~\ref{fig:pipeline} (a) illustrates how an execution pipeline processes multiple requests in parallel~\footnote{Each request here can represent a batch of one or more requests.}.
\AlgoName fulfills the pipeline by executing various requests in a cyclic manner: after completing the last block, it iterates through all the blocks again to compute the next output tokens.
As model scaling is triggered by a significant volume of accumulated requests, this pipeline approach is well-suited to efficiently leverage multi-node resources, resulting in enhanced system throughput and minimized queuing delays.
\fi 

\PHM{Models on multiple GPUs.}
When a model spans multiple GPUs, \AlgoName generates execution pipelines across GPUs within the same node and/or from nodes to minimize performance loss during model scaling. 
Fig.~\ref{fig:pipeline} illustrates the multi-GPU model scaling scenario, where each model instance is distributed across multiple GPUs. During scaling, as model blocks partially arrive, \AlgoName dynamically selects one of the three execution strategies based on model size and resource availability (i.e., whether a node has multiple GPUs). 
%\AlgoName handles one of the three cases on each GPU.
{\bf Case 1: Cross-node execution pipeline for single-GPU models} (models fitting in a single GPU): This is the default execution strategy already described in \S\ref{subsec:model_multicast}.
%there could be three cases handled by \AlgoName. 
{\bf Case 2: Cross-node execution pipeline for multi-GPU models} (models that do not fit in a single GPU): GPUs that have received complete blocks can immediately begin forming execution pipelines across nodes to support distributed, pipelined inference for large models, without waiting for the full model to load (see Fig.~\ref{fig:pipeline} (b)). 
{\bf Case 3: Intra-node scaling-up for single-GPU models}: \AlgoName can opportunistically leverage multiple local GPUs on the same node to accelerate scaling if there are available local GPU resources on the same node. As soon as the first GPU receives model blocks, it can quickly replicate them to other local GPUs using high-speed node-local communication mediums like NVLink, which offers bandwidth up to an order of magnitude higher than RDMA networks (see Fig.~\ref{fig:pipeline} (c)). This fast intra-node replication enables rapid local model scaling-up. Replicated model blocks can then form cross-node execution pipelines (Case 1).   
This hybrid approach maximizes resource utilization and enhances inference performance by opportunistically reducing data movement overhead. 
%and enabling seamless distributed execution.
%(3)~{\bf Intra-node pipelined execution}: Beyond local replication, \AlgoName can further leverage available local GPUs to form local execution pipeline, allowing multiple local GPUs to collaboratively serve a model locally using NVlinks. 

\if 0
When a model spans multiple GPUs, \AlgoName generates execution pipelines across individual GPUs from different nodes. 
Fig.~\ref{fig:pipeline} (b) shows an example of $4 \rightarrow 8$ scaling where each model instance occupies 4 GPUs. 
% The block placement within nodes remains consistent, with blocks 1-4 assigned to GPUs 0-3, respectively. 
As model blocks are partially arrived during scaling, GPUs that have received complete blocks can assemble execution pipelines across nodes to support distributed inference.
% This also allows nodes to simply switch to normal inference mode (i.e., local execution) after the scaling completes (\S\ref{sec:switch}).
Additionally, single-GPU models can utilize multi-GPU nodes to enhance scaling efficiency. 
When the first GPU receives model blocks, the node can immediately replicate blocks to other local GPUs using high-speed interconnects such as NVLink. 
The NVLink bandwidth can be up to an order of magnitude higher than that of RDMA networks, enabling rapid local model scaling. 
Beyond local replication, \AlgoName also allows these secondary GPUs to work with others in different nodes as execution pipelines, which further enhances overall inference performance. 
\fi 

\subsection{Mode Switching}

After model scaling is complete, each node maintains a full model instance and directly serves inference requests locally\footnote{Current \SysName supports models that fit within the GPU capacity of a single multi-GPU node (e.g., up to 640~GB of collective GPU memory for an 8-A100 node). We leave the support for larger models spanning multiple nodes for future work.}. 
A key challenge is to ensure that each node keeps corresponding runtime states (i.e., KV caches) for the active requests it serves.
\AlgoName addresses this challenge through \emph{KV cache recomputation}. 
It evenly distributes incomplete requests of an execution pipeline among all participating nodes, and then each node recomputes its assigned requests using available tokens generated by distributed inference during the multicast phase. 
%available tokens including those generated during distributed inference.
This approach generally incurs lower overhead compared to transmitting existing KV caches, which requires costly all-to-all communication across participating nodes.
This mechanism ensures \AlgoName to seamlessly switch from pipelined execution mode to local execution mode while maintaining low inference latency.  

% (TODO: maybe merge this part into previous sub-sec)
