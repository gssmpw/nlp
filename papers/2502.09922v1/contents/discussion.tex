\section{Related Work and Discussion}
\label{sec:discussion}

\if 0
\PHM{Serverless inference and model scaling. }
Serverless computing has gained traction as a viable solution for ML inference workloads due to its elasticity and cost efficiency~\cite{ali2022optimizing_serverless_inference, yang_infless_2022, ali_batch_nodate, cai2023cost, hong2024optimus, fu_serverlessllm_2024, romero_infaas_nodate, wang_faasnet_nodate}. ServerlessLLM~\cite{fu_serverlessllm_2024} employs serverless infrastructure to dynamically allocate resources for large-scale language model inference. However, it heavily relies on local SSDs and memory caching, constrained by the memory capacity, leading to inefficiencies in cold-start scenarios when models must be frequently reloaded from storage. INFaaS~\cite{romero_infaas_nodate}, on the other hand, optimizes function scheduling and model selection but struggles with managing large models efficiently due to its reliance on static resource provisioning. 
% FaaSNet~\cite{wang_faasnet_nodate} improves function deployment in serverless environments using an adaptive tree-based provisioning strategy, yet its binary-tree structure introduces bottlenecks in large-scale model transfers, impacting inference latency. 
FaaScale introduces a fast model scaling mechanism that enables 'execute-while-load' execution in serverless computing, significantly reducing cold-start latency and enhancing inference throughput.

% \noindent{\textbf{Execution Pipelines for Scalable Inference. }}
% Efficient execution pipelines are critical for scalable inference, especially in dynamic serverless environments~\cite{crankshaw_inferline_2020, crankshaw_clipper, shen_nexus_2019, dhakal_gslice_2020, bai_pipeswitch_nodate,BlitzScale},. Existing approaches, such as Clipper~\cite{crankshaw_clipper}, Nexus~\cite{shen_nexus_2019}, Gslice~\cite{dhakal_gslice_2020}, and PipeSwitch~\cite{bai_pipeswitch_nodate}, enhance inference efficiency through model partitioning or context-switching mechanisms. AlpaServe~\cite{li_alpaserve_2023} has leveraged model parallelism to improve inference serving for deep learning workloads. However, these methods rely on static resource allocations, limiting their ability to adapt to fluctuating workloads and dynamic scaling requirements. However, these methods rely on static re-
% source allocation.More recently, BlitzScale [48] . In contrast, PipeCast, the core of FaaScale, enables inference concurrently with model transfer by dynamically constructing execution pipelines across GPU nodes. Through adaptive model multicast and decentralized scheduling, PipeCast improves serving throughput and resource efficiency.
\fi 


\PHM{Pipelined inference execution.}
Prior works on pipelined inference execution typically rely on static resource configurations~\cite{crankshaw_clipper,shen_nexus_2019,dhakal_gslice_2020,bai_pipeswitch_nodate,li_alpaserve_2023}, whereas \SysName focuses on dynamic pipeline construction and execution using the ``execute-while-load'' approach.
BlitzScale~\cite{BlitzScale} is a concurrent work to \SysName, utilizing chain-based model scaling and 
%is bound to 
specifically designed for Prefill/Decoding (P/D) disaggregated LLM inference settings~\cite{zhong_distserve_2024}. 
In contrast, \SysName proposes an efficient, binomial-pipeline-based scaling solution without assuming P/D disaggregation. 
% \todo{does this point make sense?}



\PHM{Support for tensor parallelism.} 
In addition to Pipeline Parallelism (PP), \SysName can inherently support Tensor Parallelism (TP) or Hybrid Parallelism~\cite{SpotServe,hybrid_parallel_survey} by extending its model partitioning strategies.
By tracking block-level execution dependencies as a DAG, \SysName can effectively coordinate inference tasks under TP-based partitioning strategies, which we leave for future work.


\PHM{Support for other models.} 
Current \SysName supports a variety of mainstream models, including LLMs, image classification models, and vision transformers~\cite{dpt-beit-large-512,clip-vit-large-patch14,metainf}.
Enabling fast scaling for smaller models is much easier; therefore, they are excluded from our evaluation. 
Additionally, \SysName can be easily extended to support new models, provided that their structures are available for model partitioning.

\if 0
While our evaluation focuses on LLMs, its PipeCast execution pipelines and adaptive model multicast can extend to most ML workloads.
The PipeCast execution engine's core innovation lies in its ability to dynamically construct distributed model instances atop any order-preserving group communication primitives (e.g., NCCL, MPI). Meanwhile, FaaScale adapts to both P/D disaggregation and prefill-decode coupled scenarios. 
% We are actively working on integrating FaaScale into a world-leading serverless platform.

\fi

% \noindent{\textbf{Network and Infrastructure. }} FaaScale leverages high-speed RDMA and GPUDirect RDMA (GDR) for fast model transmission, aligning with trends in cloud and HPC clusters. Future work will explore adaptive data transfer mechanisms to optimize performance in heterogeneous infrastructures.

% \noindent{\textbf{Scalability and Multi-Tenancy. }} Designed for multi-GPU and multi-node scaling, FaaScale must address resource allocation challenges in multi-tenant clusters. Future research will focus on workload-aware scheduling and asynchronous execution pipelines to improve GPU sharing and efficiency.

% \noindent{\textbf{Future Directions. }} Beyond inference, execute-while-load could enhance model training, federated learning, and continual learning by enabling distributed execution under dynamic conditions. Further optimizations in hybrid scheduling could reduce latency in highly dynamic workloads.