%!TEX root=../paper.tex
\section{Introduction}
\label{sec:intro}

% \yuecomment{Two major comments for Sec 1 and 2: 1. lots of repetitive text to try to remind readers our key design points -- we need to cut those redundant text and make intro and background succinct but to the point. This can also save space as we are running out of space. }

% \yuecomment{2. we need to briefly introduce how exactly our pipeline execution algo works---collaborative, best-effort, pipeline parallel execution among cooperating receiving nodes---and we should highlight that it is different from traditional pipeline parallel inference execution. So far the text gives readers an impression that we are simply doing compute and I/O overlapping, which is nothing new.} 

Recent advancements in machine learning (ML) have fueled a surging demand for cloud-based ML inference
services~\cite{zhang_shepherd_nodate,zhang_mark:_2019,shen_nexus_2019,choi_serving_2022,gujarati_serving_2020}. Serverless computing offers a compelling cloud model for inference
serving that can effectively handle dynamic request patterns~\cite{ali_fc,yang_infless_2022, yu_gillis_icdcs, ali_batch_nodate}. 
In this approach, users simply publish ML models, expose service endpoints, and delegate resource provisioning and scaling responsibilities to the cloud platform. 
Serverless computing is also economically appealing as users pay only for actual resource usage, eliminating the resource idling cost.



However, current serverless inference platforms suffer from significant cold-start problems, especially as modern models grow increasingly large and resource-intensive (e.g., large language models, or LLMs).
% The typical cold-start process, including model remote loading and initialization, creates substantial delays that can extend to several minutes~\cite{fu_serverlessllm_2024}.
% The high startup latency hinders the platforms from rapidly scaling out to accommodate highly dynamic workloads, making it infeasible to support online inference services with stringent latency requirements (e.g., a few seconds~\cite{fu_serverlessllm_2024,zhang_mark:_2019,zhong_distserve_2024,agrawal_taming_2024}).
The cold-start process, involving remote model loading and initialization, can result in long startup delays up to several minutes~\cite{fu_serverlessllm_2024}. 
Such high latencies prevent platforms from rapidly scaling out 
%in response to 
to handle highly dynamic workloads, making it impractical to support online inference services with stringent latency requirements on the order of milliseconds or seconds~\cite{fu_serverlessllm_2024,zhang_mark:_2019,zhong_distserve_2024,agrawal_taming_2024}.

% To mitigate cold starts, existing platforms either cache inference models in GPU for warm-started execution~\cite{yang_infless_2022,ali_fc}, or retain them in local storage (e.g., memory and SSD) and then load them to GPUs when requested~\cite{bai_pipeswitch_nodate,faaswap,fu_serverlessllm_2024}. 
% However, neither of these approaches effectively provide good scalability. 
% The dynamic nature of serverless inference necessitates sharing GPU workers across a large number of models, whereas caching many model replicas in each worker node is inpractical due to limited GPU memory and host storage.
% Consequently, this either leads to remote model loading and high startup overhead, or forces overprovisioning GPU workers to keep-alive models with additional substantial costs.

To mitigate cold starts, existing platforms typically cache
%often resort to caching 
inference models in local GPUs or host memory during idle periods and resume execution upon request arrivals~\cite{yang_infless_2022,ali_fc, bai_pipeswitch_nodate,faaswap,fu_serverlessllm_2024}. 
%However, these approaches fail to provide high scalability. 
%In a multi-tenant serverless platform, each worker can be scheduled with a multitude of models, yet storing multiple large models locally is infeasible due to limited GPU and host memory capacity.
However, these approaches have limited scalability and incur high resource costs. In multi-tenant serverless environments, workers typically need to handle multiple large models simultaneously, but storing all these models locally is infeasible and/or cost prohibitive due to limited GPU and host memory capacity. 
%Although recent works turn to local SSDs to expand storage capacities~\cite{fu_serverlessllm_2024}, the data transfer between SSDs and GPUs introduces substantial delays that are unacceptable under dynamic workloads (details in xx).
%Consequently, the platform either suffers from considerable startup overhead or necessitates overprovisioning GPU workers to keep model replicas always active, incurring significant resource costs.
While recent works explore the use of local SSDs to expand storage capacity~\cite{fu_serverlessllm_2024}, the data transfer between SSDs and GPUs introduces significant delays under dynamic workloads. As a result, platforms either suffer from unacceptable startup costs or resort to overprovisioning GPU workers to maintain active model replicas at all times, leading to high resource costs. 


% We believe that an efficient serverless inference platform should fundamentally eliminate the hard tradeoff between resource costs and startup performance. 
% Ideally, the platform should be able to achieve fast and on-demand scaling in response to load bursts without incurring additional costs.
% Fortunately, we note that the tightly-coupled design of modern GPU clusters, along with the distributed model execution, makes this objective achievable. 
% GPU nodes in today's clusters are typically interconnected via high-speed network fabric (e.g., RDMA), enabling fast model loading through cross-node bandwidth.
% Additionally, since a model can be partially computed before it is fully loaded, distributed inference execution across various nodes becomes viable during model loading.
% This enables worker nodes to begin serving requests in parallel, even before each node has loaded a complete model replica, i.e., ``execute-while-load''.


%We advocate that an efficient serverless inference platform should eliminate the hard tradeoff between low startup latency and resource overprovisioning. 
An efficient serverless inference platform should not settle for the fundamental tradeoff between startup latency and resource overprovisioning. 
Ideally, it should scale out rapidly in response to load spikes without incurring additional resource costs. 
%Fortunately, the tightly-coupled design of modern GPU clusters makes this goal attainable. 
\emph{Fast serverless model scaling is achievable through two key insights.}
First, modern GPU clusters employ high-speed network interconnects (e.g., 400Gbps with RDMA capability)~\cite{acme_nsdi24, kundu_llm_analysis_arxiv24, azure_ai_cluster_url}, providing opportunities for efficient model multicast and fast scaling. 
Second, model inference can begin before a node receives all model parameters, enabling collaborative, distributed inference execution across multiple nodes during model loading. 
%This is potentially achievable through fast cross-node mode loading via high-speed network interconnects (e.g., 100-400Gbps with RDMA capability) commonly deployed in high-performance GPU cluster~\cite{acme_nsdi24, kundu_llm_analysis_arxiv24}.  
%This is achievable through the tightly-coupled design of modern GPU clusters. \yuecomment{what is tight-coupled design?}
%The high-speed network interconnects between GPU nodes (e.g., 100-400~Gbps RDMA) are often underutilized in inference clusters, providing opportunities for fast cross-node model loading.
% GPU nodes are typically interconnected via high-speed network fabric (e.g., 100-400~Gbps RDMA interconnects), enabling cross-node fast model loading across inter-node connections that are often underutilized in inference clusters.
%Additionally, since partial model computation can commence before each node has received the entire model replica, distributing inference executions across multiple nodes becomes viable even during loading.
This ``execute-while-load'' approach, combined with low-latency model loading, substantially improves system scalability under spikes. 
% Additionally, given that a model can be partially computed before it is fully loaded, distributed inference execution across various nodes becomes viable during model loading.
% This enables worker nodes to begin serving requests in parallel, even before each node has loaded a complete model replica, i.e., ``execute-while-load''.


% Following these key insights, we propose \SysName, an efficient serverless inference platform for fast and cost-efficient model scaling.
% \SysName leverages high-speed GPU interconnects and GPUDirect RDMA to accelerate model distribution across the cluster, and follows the ``execute-while-load'' design principle to promptly serve inference requests during model loading. 
% However, realizing this idea presents both algorithmic and systematic challenges.

Following these insights, we propose \SysName, a scalable serverless inference platform that delivers fast and resource-efficient model scaling. 
% \SysName leverages high-speed cross-node interconnects and GPUDirect RDMA (GDR)~\cite{gdr} to accelerate model transfer across GPU nodes. 
% It also adopts ``execute-while-load'' to promptly serve inference requests during model loading. 
\SysName enables distributed inference execution during cross-node model loading (i.e., ``execute-while-load''), leveraging high-speed RDMA networks and the GPUDirect RDMA (GDR)~\cite{gdr}.
However, realizing this approach poses both algorithmic and system-level challenges.

A key challenge for \SysName is to develop efficient solutions for cross-node model loading and distributed execution.
\SysName requires to accommodate varying model scaling demands and enable collaborative, distributed model execution in a dynamic environment where the model is being loaded.
Existing distributed inference solutions do not meet such requirements, which typically lack support for dynamic, on-demand model scaling and rely on static resource allocations for distributed model execution~\cite{li_alpaserve_2023,zhang_shepherd_nodate,yang_infless_2022}.
To address this challenge, \SysName introduces an abstraction of an \emph{execution pipeline} and a novel scheme, \emph{Dynamic Pipelined Execution with Multicast} (\AlgoName).
This scheme partitions the model into fine-grained blocks for efficient multicasting while dynamically constructing execution pipelines --- complete model instances spanning multiple receiving nodes for collaborative, distributed inference execution.

% A key challenge for \SysName is to develop efficient algorithms for cross-node model loading and distributed execution.
% Unlike existing distributed inference systems~\cite{li_alpaserve_2023}, which typically rely on static resource allocations and model placement, \SysName must dynamically adapt in real-time to evolving runtime conditions---even while the model is still being loaded. 
% This scheme partitions the model into fine-grained blocks for multicasting and efficiently constructs execution pipelines among participating nodes, 
%concurrently executes distributed inference across multiple nodes during transfer.

\AlgoName comprises three key designs. 
First, \AlgoName designs an adaptive model-multicast mechanism based on 
the binomial pipeline~\cite{binomial-pipe} algorithm.
This mechanism optimizes the granularity and transfer order of the model blocks to enable the rapid construction of execution pipelines, thus improving overall inference performance (referred to as \textbf{\emph{Adaptive Model Multicast}}).
Second, \AlgoName judiciously groups nodes into execution pipelines at runtime to improve overall resource efficiency while minimizing data transfers during distributed inference.
This strategy effectively improves system throughput and reduces request queueing under load spikes (referred to as \textbf{\emph{Pipelined Inference Execution}}). 
Furthermore, once model loading is complete, \AlgoName allows workers to seamlessly switch to local execution mode without incurring cross-node overheads (referred to as \textbf{\emph{Mode Switching}}). 

\if 0
\yuecomment{This paragraph needs rewriting. Suggestion: Explain how these three system designs work in a simple, intuitive way, without assuming readers have already read the detailed design sections. The goal is to give readers a clear, high-level understanding of how \SysName functions and spark their interest to keep reading.}
In addition to \AlgoName, \SysName proposes three system designs to enable fast model scaling in diverse scenarios.
% Moreover, \SysName proposes three key system designs to efficiently apply \AlgoName in various scenarios, thus enabling fast model scaling. 
First, given that host memory has been widely adopted to accelerate model loading, \SysName supports both cross-node and host-to-GPU loading under its ``execute-while-load'' schemes by decoupling model transfer and execution into two separate control planes (i.e., \textbf{\emph{Decoupled Control Planes}}).
This approach seamlessly integrates replica caches from various storage tiers (e.g., GPU or host memory) to enable dynamic, flexible model scaling across hierarchical storage.
% It simplifies model replicas cached in various storage tiers (e.g., GPU or host memory) to collaborate in model loading and execution, thereby enabling dynamic and flexible model scaling across hierarchical storage.
Second, as GPU workers are often equipped with multiple GPUs and need to execute models with varying GPU demands, \SysName adapts a two-level distributed scheduling framework that dynamically controls resource allocations at both the cluster and node levels (i.e., \textbf{\emph{Two-Level Scheduling}}).
It effectively simplifies model management and request executions across the GPU cluster.
Finally, \SysName applies a series of communication optimizations---including tensor packing, memory pre-allocation, and data bypassing---to further enhance the efficiency of model loading and distributed execution (i.e., \textbf{\emph{Communication Optimizations}}).

\fi

In addition to \AlgoName, \SysName must efficiently manage models cached across various media (e.g., GPU memory and host memory) to facilitate rapid model scaling from different storage tiers.
To address this challenge, \SysName proposes two key system designs.
First, \SysName employs a holistic model startup mechanism that adapts to varying model locality. It also enables model instances stored in both GPU and memory to collectively improve scaling performance (referred to as \textbf{\emph{Locality-driven Model Startup}})
Second, \SysName implements a memory management system that seamlessly consolidates data within model blocks for efficient model transmission and supports GPU memory pre-allocation for improved efficiency (referred to as \textbf{\emph{Efficient Memory Management}}). 

We have implemented \SysName and evaluated it against state-of-the-art solutions, including \textit{ServerlessLLM}~\cite{fu_serverlessllm_2024}, \textit{FaaSNet}~\cite{wang_faasnet_nodate}, and \textit{NCCL}~\cite{nccl}.  
% On real-world LLM inference traces~\cite{burstGPT_arxiv24}, \SysName efficiently handles load spikes, leading to a 2.4$\times$ to 5$\times$ improvement in 90$^{th}$ tail latency while reducing resource costs by 17\% to 30\% compared to the baselines. 
\SysName efficiently handles load spikes, leading to a 2.4$\times$ to 5$\times$ improvement in 90$^{th}$ tail latency while reducing resource costs by 17.8\% to 31.3\% compared to the baselines on real-world LLM inference traces~\cite{burstGPT_arxiv24}.  
Microbenchmark results further show that \SysName delivers fast model transmission and completes the scaling of Llama-13B across 8 nodes in less than 1 second, outperforming \textit{NCCL} by up to 1.5$\times$. 
\if 0
Additionally, \SysName enables efficient inference execution during model loading and significantly outperforms baselines, improving system throughput and tail latency by up to 11.4$\times$ and 8$\times$, respectively.
\fi 

%This work does not raise any ethical issues.