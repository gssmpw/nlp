\section{\SysName Implementation}
\label{sec:implementation}
We have implemented \SysName in 10K lines of Python and 4K lines of C++, structured into two core components: the cluster manager and worker nodes. \SysName's source code will be released upon the acceptance of the paper.  
%We will open source FaaScale upon publication.

The {\bf Cluster Manager} is implemented in Python (see Fig.~\ref{fig:overview}). 
Each worker node contains a {\bf Model Manager}, which consists of two key modules: (1) an \emph{inference module},  responsible for executing both local inference (within a GPU) and distributed inference (across nodes) and (2) a \emph{transfer module}, which implements GDR/RDMA-based model block transfer. For the inference module, we extend the codebase of Meta's Llama inference framework~\cite{metainf} to support both local and distributed inference using Python. For the transfer module, we implement a standalone RDMA P2P transfer library in C++ that supports both GPU and host memory using {\small\texttt{libibverbs}}~\cite{ibverbs}, leveraging the RDMA read operation with a pull-based pattern. The binomial pipeline algorithm is implemented in {\bf Cluster Manager} and its GDR/RDMA semantics are implemented in {\bf Model Manager}'s transfer module. 
All the key RDMA P2P transfer APIs (e.g., RDMA queue pair establishment and RDMA read operation) are exposed to Python via {\small\texttt{Pybind11}}~\cite{pybind11}. 

\if 0
%\noindent{\textbf{Cluster Manager. }}
\PHM{Cluster manager.} 
% \ruicomment{rui,please check the paragraph}).
The cluster manager comprises three key modules—\textit{Resource Manager}, \textit{Model Scaling Controller}, and \textit{Pipelined Execution Controller}—all implemented in Python 
(detailed in Section~\ref{sec:overview}). 
\fi 

% It replies on the Resource Manager to manage GPU resources and keep tracks of model instance placement across nodes using an in-memory dictionary. 
% The Model Scaling Controller monitors the request queue status and triggers a scale-out events when necessary.
% Once scaling is initiated, the Pipelined Execution Controller generates a transmission and execution pipeline based on available resources.
% After generating the pipeline plan, the Cluster Manager sends instructions to target worker nodes via established TCP connections, assigning specific tasks to each node.

% The Cluster Manager issues two types of instructions: (1) READ instruction, which directs a worker node to read either model blocks or intermediate data from other worker node, and (2) EXECUTE instruction, which instructs the worker node to execute a specific computation task. The Cluster Manager also handles scale-in events, deallocating resources whenever GPU utilization falls below a predefined threshold.

\if 0
\noindent{\textbf{Worker Node. }}
Each worker node contains model manager, which consists of two key modules: (1) inference module,  responsible for executing both local and distributed inference across nodes and (2) transfer module, which manages RDMA-based model block transfers. For the inference module, we extend Meta's LLaMA Inference~\cite{metainf} codebase to support both local and distributed inference across nodes using Python. For transfer module, we implement a standalone RDMA P2P transfer library in C++ that supports both GPU and host memory using Libibverbs~\cite{ibverbs}, leverage the RDMA read operation under a pull-based pattern. All the key RDMA P2P transfer APIs (e.g., RDMA queue pair establishment and RDMA read operation) are exposed to Python via Pybind11~\cite{pybind11}. 
\fi 
% A worker node dynamically responds to instructions from the cluster manager. 
% Upon receiving (1) a READ instruction, it retrieves the required model block or intermediate data from host memory or GPU via GDR from a remote worker node. If it receives (2) an EXECUTE instruction, the worker node processes the data and returns the results to the designated destination.




% \noindent{\textbf{P2P Transfer. }}

% The cluster manager comprises four resident modules: Resource Manager, Model Scaling Controller, Pipelined Execution Controller, and Request Dispatcher, which orchestrate distributed resource allocation, dynamic scaling, and workload scheduling. Each worker node integrates a Node Transfer Controller for cross-node coordination, which manages the transmission of intermediate data as well as model data across nodes, and a Node Executor for inference execution. The system leverages a modified version of Meta’s Llama framework [1] as its inference engine, extended to support dynamic pipeline parallelism and multi-node multi-GPU execution. We redesign the KV cache management module to seamlessly integrate with our resource control logic.