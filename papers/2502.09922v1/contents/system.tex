\section{Efficient Model Management}
\label{sec:system}

% \begin{figure*}[ht]
% \centering
% \vspace{-5pt}
% \includegraphics[scale=0.4]{figures/algorithm/pipecast 3.png}
% \caption{pipecast memory}
% \label{fig:pipecast_memory}
% \vspace{-10pt}
% \end{figure*}

\SysName supports efficient model management in both host memory and GPUs through two key system designs.

%\PHM{Multi-Level Execution.}
\PHM{Locality-driven model startup.}
\SysName introduces a multi-level, locality-driven model startup scheme to efficiently support model instances stored in various storage tiers. \SysName optimizes locality using several startup strategies: 
%categorized by the model status:
%Based on the locality of model instances, \SysName categorizes each node into three types, each with its own execution strategy:
%\texttt{GPU} (model loaded in GPUs), \texttt{Memory} (model cached in memory), and \texttt{Null} (no model in either GPU or memory), each with its own execution strategy. 
\textbf{(1) \texttt{GPU}} (\emph{hot start}): The model is fully loaded into GPU memory, enabling fast, local execution.
%remain in local inference mode and execute assigned requests normally. 
\textbf{(2) \texttt{Memory}} (\emph{warm start}): The model is cached in host memory. \SysName directly loads host-memory-cached models into GPUs for inference execution, and before models are fully loaded, constructs an execution pipeline across multiple nodes of this kind for enhanced inference performance (\AlgoName in \S\ref{sec:algo}). 
\textbf{(3) \texttt{Null}} (\emph{cold start}): The model is neither cached in GPU or host memory, requiring \SysName to perform cold-start scaling by directly retrieving model blocks from remote \texttt{GPU} and/or \texttt{Memory} nodes. 
%as the source. 
%\SysName allows a cold startup to directly retrieve model blocks from remote host memory or remote GPUs. 
\if 0
\textbf{(1) \texttt{Null} nodes} can leverage both \texttt{GPU} and \texttt{Memory} nodes as source nodes to facilitate end-to-end scaling (\AlgoName in \S\ref{sec:algo}). 
\SysName extends GDR to enable \texttt{Null} nodes' GPUs to directly access models in remote memory or GPUs, ensuring consistent performance between \texttt{Memory}-to-\texttt{Null} and \texttt{GPU}-to-\texttt{Null} model scaling.
This approach enhances scaling efficiency with low storage costs, as only a limited number of model replicas need to be maintained in memory across the entire cluster.
\textbf{(2) \texttt{Memory} nodes}, in addition to participating in cross-node scaling, load local models to GPUs for inference execution. 
More importantly, \SysName constructs \emph{an additional execution pipeline} across all \texttt{Memory} nodes, where the block loading order is optimized using the $k$-way transmission strategy (Algorithm~\ref{alg:k_way}). 
This approach allows \texttt{Memory} nodes to start distributed inference before the model is fully loaded, thereby improving overall performance.
\textbf{(3) \texttt{GPU} nodes} remain in local inference mode and execute assigned requests normally. 
\fi 

\PHM{Efficient memory management.}
\SysName employs two strategies to manage GPU and host memory for model blocks. 
\textbf{(1) Tensor packing:} \SysName maps each model block to a contiguous memory region for enhanced transmission efficiency. 
By consolidating all tensor data associated with a single model block into a contiguous memory chunk, \SysName enables bulk transfer of entire blocks, improving bandwidth efficiency.
%the entire block to be transmitted as a whole. 
%This design significantly improves bandwidth efficiency in model transmission.
Notably, the tensor memory layout optimization has no impact on inference execution. 
\textbf{(2) GPU memory pre-allocation:} \SysName pre-allocates memory chunks for model blocks and intermediate results, as their sizes remain consistent across requests during pipeline execution. 
%whose data sizes typically remain consistent across requests during pipeline execution. 
Runtime states with dynamic memory requirements (e.g., KV caches) are internally managed by inference engines (e.g., vLLM~\cite{kwon2023efficient}). 
This design ensures memory efficiency while minimizing memory allocation overhead at runtime. 
% \todo{inference engine in overview fig}
% This approach ensures efficient memory utilization while maintaining flexibility for dynamic components of the inference process.

