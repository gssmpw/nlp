\section{Evaluation}
\label{sec:evaluation}
We evaluate \SysName by addressing three key questions:  
{\bf (1)} How fast can \SysName distribute model blocks across a GPU cluster (\S\ref{subsec:model_transfer_performance})?
%\\ (1) How efficiently can \SysName distribute models across the cluster?~(\S\ref{subsec:model_transfer_performance}) 
{\bf (2)} How does \SysName scale LLM inference performance compared to state-of-the-art baselines under concurrent, stress-test workloads ~(\S\ref{subsec:throughput}, \S\ref{subsec:latency})? 
{\bf (3)} How elastic and cost-effective is \SysName under a bursty real-world LLM workload (\S\ref{subsec:e2e})?
%sudden spikes in RPS, as demonstrated by a real-world LLM workload trace?~(\S\ref{subsec:e2e}) 
{\bf (4)} How do different design choices and configurations impact \SysName's performance?~(\S\ref{subsec:ablation_study}) 
%
% We include extra ablation tests and sensitivity tests in the Appendix (\S\ref{subsec:ablation_study}). 
%\\ (4) How does each key component affect \SysName's including a detailed latency breakdown (e.g., memory pre-allocation, tensor-packing, and other optimizations)?~(\S\ref{subsec:ablation_study}) 
% To evalaute \SysName, we ask the following questions: (1) How efficiently can \SysName distribute models across the cluster?~(\S\ref{subsec:model_transfer_performance}), (2) How does \SysName's performance compare to other state-of-the-art systems, such as \textit{ServerlessLLM}, \textit{FaaSNet}, and \textit{NCCL}, under stress loads?~(\S\ref{subsec:throughput})
% \begin{itemize}
%     \item How efficiently can \SysName distribute models across the cluster?~(\S\ref{subsec:model_transfer_performance})
%     \item How does \SysName's performance compare to other state-of-the-art systems, such as \textit{ServerlessLLM}, \textit{FaaSNet}, and \textit{NCCL}, under stress loads?~(\S\ref{subsec:throughput})
%     \item How does \SysName respond to sudden spikes in RPS during bursty workload scenarios?~(\S\ref{subsec:latency})
%     % \item How does \SysName handle real-world workload traces in terms of end-to-end latency and GPU allocation efficiency? \S\ref{subsec:e2e})
%     \item How robust and cost-effective is \SysName when handling sudden spikes in RPS, as demonstrated by a real-world LLM workload trace?~(\S\ref{subsec:e2e})
%     \item How does each key component affect \SysName’s including a detailed latency breakdown (e.g., memory pre-allocation, tensor-packing, and other optimizations)?~(\S\ref{subsec:ablation_study})
% \end{itemize}
% In this section, we evaluate the performance of \SysName under various traces. 

\subsection{Experiment Setup}
\label{sec:evaluation:setup}

\PHM{Testbeds.} 
All experiments are conducted on a shared HPC cluster, with an exclusive allocation of up to 24 NVIDIA H800 GPUs and 12 nodes per user. 
%\tfcomment{is it necessary to highlight the resource constrain for per user? If not, pls omit this description. Also, the term "shared" means that our cluster is not guaranteed to be a stable testbed. double check this.} 
% chaobo comment 
% Given the resource constraints, we configure two testbeds to evaluate \SysName under different scalability scenarios (Table~\ref{tab:testbed_configurations}). 
% Testbed1 is used when a single GPU is sufficient to host an LLM, while Testbed2 is used for cases where a single GPU cannot accommodate a large LLM. 
%
We configure two testbeds to evaluate \SysName under different scalability scenarios (Table~\ref{tab:testbed_configurations}). Testbed1 is used when a single GPU is sufficient to host the entire model (e.g., Llama-2 7B), 
allowing to stress the scalability test via inter-node communication. 
Testbed2 is used when a single GPU is not large enough to host a model (e.g., Llama-2 70B), 
requiring multiple GPUs per node to validate scalability while involving inter-node communication (e.g. model parallel inference).  



% \begin{itemize}
%     \item \textbf{Testbed1}: Consists of 12 nodes, each equipped with a single H800 GPU and a 400 Gb/s InfiniBand NIC. \SysName was evaluated on Testbed1 under scenarios where a single GPU is sufficient to store an LLM model.
%     \item \textbf{Testbed2}: Designed for scenarios where a single GPU cannot store an LLM model. It consists of 6 nodes equipped with 4 × H800 GPUs and 1 × 400 Gb/s InfiniBand NIC.
% \end{itemize}
% Testbed1 consists 12 nodes, each equipped with single H800 GPU and single 400Gb/s InfiniBand NIC.
% \SysName was evaluated on Testbed1 under a scenario where single GPU is sufficient to store an LLM model. 
% To evaluate the performance in a scenario where a single GPU cannot store an LLM model, we construct Testbed2, which consists of 4 × H800 GPUs and 1 × 400Gb/s InfiniBand NICs.
% Table~\ref{tab:testbed_configurations}.
%For the models, we
%Our evaluation uses the Llama 2~\cite{llama2} series models with 7B, 13B, and 70B parameters due to their accessibility and popularity.  



% chaobo comme
% \tfcomment{We configure two testbeds to evaluate \SysName under different scalability scenarios (Table~\ref{tab:testbed_configurations}). Testbed1 is used when a single GPU is sufficient to host the entire model (e.g. Llama-2 7B), allowing us to maximize the scalability test across intra-node communication. Testbed2 is used when a single GPU is insufficient (e.g Llama-2 70B), requiring multiple GPUs per node to validate scalability while involving inter-node communication (e.g. model parallellism). }

\begin{table}[t]
\centering
\caption{Testbed Configurations.
~\textit{\textmd{Each node is equipped with 1TB RAM and 4TB of NVMe SSD local storage.
}}
}
\label{tab:testbed_configurations}
\vspace{-10pt}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
\textbf{Testbed} & \textbf{GPU} & \textbf{NIC} & \textbf{Memory} & \textbf{SSD} & \textbf{\#Nodes} \\ 
\hline
Testbed1        & 1$\times$H800       & 1$\times$400Gb/s IB & 64GB/s & 5GB/s & 12             \\ 
Testbed2        & 4$\times$H800       & 1$\times$400Gb/s IB  & 64GB/s & 5GB/s &  6              \\  
\hline
\end{tabular}%
}
% \vspace{-2em}
\end{table} 

% -----------------
% Testbed spec
% ----------------
% \begin{table}[t]
% \centering
% \caption{Testbed Configurations.~\textit{\textmd{Each testbed uses single 400Gb/s InfiniBand NIC.}}}
% \label{tab:testbed_configurations}
% \vspace{-10pt}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{lcccccc}
% \hline
% \textbf{Testbed} & \textbf{GPU} & \textbf{NIC} & \textbf{Memory} & \textbf{SSD} & \textbf{\#Nodes} \\ 
% \hline
% Testbed1        & 1xH800       & 1x400Gb/s IB  & 400Gb/s & 400Gb/s & 12             \\ 
% Testbed2        & 4xH800       & 1x400Gb/s IB  & 400Gb/s & 400Gb/s & 6              \\ 
% \hline
% \end{tabular}%
% }
% \end{table}
% -----------------
% Trace
% ----------------
% \PHM{Trace. } \ruicomment{TODO: ask chaobo for details of each trace}

\noindent{\textbf{Models and configurations.}}
%Our evaluation uses the Llama 2~\cite{llama2} series LLMs with 7B, 13B, and 70B parameters due to their accessibility and popularity. 
We consider two primary experimental parameters: model size and \emph{k} (refer to \emph{k}-way transmission in~\S\ref{subsec:model_multicast} for details).
We test \SysName with the Llama-2~\cite{llama2} series LLMs with 7B, 13B, and 70B parameters and a \emph{k} value ranging from \{1, 2, 4\}. 
Experiments for Llama-2 7B and 13B are conducted on Testbed1, while Llama-2 70B tests run on Testbed2. Unless explicitly stated, all configurations follow these defaults. 

\PHM{Measurement metrics.} 
% Our evaluation focuses on key performance metrics that are critical for understanding \SysName's effectiveness and scalability under varying requests per second (RPS)
Our evaluation focuses on key metrics including throughput, latency, and cost-effectiveness.
% Specifically, we measure throughput (tokens per second) to assess \SysName's ability to handle requests under a high load.
% (1) Throughput (tokens per second or TPS) measures \SysName's ability to sustain high-load inference requests.
% (2) Latency (time-to-first-token or TTFT) reflects \SysName's efficiency in generating the first token quickly,
(1) Throughput (tokens per second) measures \SysName's ability to sustain high-load inference requests.
(2) Latency (time-to-first-token) reflects \SysName's efficiency in generating the first token quickly,
%delivering fast first-token generation, 
a critical metric for low-latency LLM serving.
(3) Cost-effectiveness (GPU time) evaluates how elastically \SysName provisions and releases GPU resources. 

% We also evaluate latency, with an emphasis on time to first token (TTFT), to quantify the responsiveness of \SysName in generating the initial token of a request given we are focusing on effectiveness of scaling out. 
% To stress test \SysName under bursty workloads, we vary the RPS from $1$ to $6$. 
% Finally, we evaluate \SysName using a real-world LLM workload trace~\cite{burstGPT_arxiv24} to test its cost-effiectiveness and robustness. 
% For most end-to-end experiments, we run this trace for 30 minutes due to cost issue.

% \noindent{\textbf{Baseline 1: ServerlessLLM. }}
% \textit{ServerlessLLM} is a state-of-the-art serverless inference system designed for large language models (LLMs), enabling dynamic scaling to handle varying workloads efficiently. However, as \textit{ServerlessLLM} leverages Ray Serve~\cite{rayserve} for cluster management, it introduces additional overhead. To ensure a fair comparison between systems, we implement our own version of \textit{ServerlessLLM}, which scales models using local SSD and memory.

% \noindent{\textbf{Baseline 2: FaaSNet. }}
% \textit{FaaSNet} is another scalable container provisioning system that utilizes a lightweight and adaptive function tree structure (binary tree) as its underlying topology. For a direct comparison with \SysName, we align all system configurations with \textit{FaaScale}, except for the transfer topology, where we employ a binary tree topology into our experimental setup.

% \noindent{\textbf{Baseline 3: NCCL. }}
% \textit{NCCL} is an industry-standard library developed by NVIDIA to optimize collective communication primitives like all-reduce and broadcast for GPUs. Designed for multi-GPU training and inference workloads, it leverages GDR and NVLink to enable efficient, scalable data transfers within and across nodes, ensuring high performance in distributed deep learning frameworks.
% Since \textit{NCCL} does not provide an exact multicast primitive, we use its broadcast primitive and restrict it to a specific subgroup. For our experimental setup, we substitute \textit{NCCL} with this restriction as the underlying transfer layer for multicasting.
% To ensure a direct and fair comparison with \SysName, we use the same experimental setup and align all system configurations, substituting the \textit{NCCL} as the underlying transfer layer in our experimental setup.
% To ensure a direct and fair comparison with \SysName, we use the same experimental setup and align all system configurations, substituting the
% As \textit{NCCL} is a pure data transmission library, we  substitute it as the underlying transfer layer in our experimental setup. 


% % We select three baselines to compare with, ensuring a fair evaluation across both industry-standard and research-driven systems.
% To ensure a fair and comprehensive evaluation, we compare \SysName against three baselines, covering both industry-standard and state-of-the-art research systems.
% \textit{ServerlessLLM}~\cite{fu_serverlessllm_2024} represents the state-of-the-art in serverless LLM inference, designed for dynamic scaling. 
% \textit{FaaSNet}~\cite{wang_faasnet_nodate} is a state-of-the-art industry-adopted serverless function container provisioning system that uses an optimized P2P transfer topology for improved auto-scaling. 
% \textit{NCCL}~\cite{nccl}, developed by NVIDIA, is an industry-standard communication library for multi-GPU training and inference, optimizing collective communication primitives like all-reduce and broadcast using GDR.  
% These baselines provide a well-rounded comparison for evaluating \SysName’s performance.
% % We use each system directly or implement a simplified version integrated into our experimental setup.
% We implement \textit{ServerlessLLM} to eliminate Ray Serve's~\cite{rayserve} cluster management overhead.
% % For \textit{FaaSNet}, we adopt its default binary tree topology since it does not support GDR, and use \textit{NCCL}'s broadcast primitive restricted a specific subgroup due to its lack of a multicast primitive.
% For \textit{FaaSNet}, we adopt its default binary tree topology since it does not support GDR.
% Since \textit{NCCL} lacks a native multicast primitive, we adapt its broadcast primitive by dynamically creating subgroups and sending model blocks only to the designated subgroup, achieving multicast functionality.

\noindent{\textbf{Baselines.}}
We compare \SysName against three baselines, covering both industry-standard and state-of-the-art research systems as discussed in \S\ref{sec:background-limitations}. \\
\textit{ServerlessLLM}~\cite{fu_serverlessllm_2024}: The state-of-the-art serverless LLM inference system designed for dynamic scaling. We implement ServerlessLLM and remove Ray Serve’s~\cite{rayserve} cluster management overhead to isolate its inference performance. \\
\textit{FaaSNet}~\cite{wang_faasnet_nodate}: An industry-adopted serverless function container provisioning system that optimizes P2P transfer topology for auto-scaling. We use its default binary tree topology and extend it to support GDR-based model loading. 
%as it does not support GDR. \\
\textit{NCCL}~\cite{nccl}: An industry-standard communication library for multi-GPU training and inference developed by NVIDIA, optimizing collective communication primitives such as all-reduce and broadcast using GDR. 
%\textit{NCCL} dynamically adjusts the underlying broadcast topologies between tree-based and ring-based approaches. We extend \textit{NCCL} by adding explicit dynamic process group configurations to handle bursts.   
Since \textit{NCCL} lacks native multicast support, we adapt its broadcast primitive by dynamically forming  process groups and transmitting model blocks to designated groups, effectively enabling multicast. 


% \tfcomment{We compare FaaScale against three baselines, covering both industry-standard and state-of-the-art research systems as discussed in \S\ref{sec:background-limitations}. \\
% \textit{ServerlessLLM}~\cite{fu_serverlessllm_2024}. The state-of-the-art serverless LLM inference system designed for dynamic scaling. We implement ServerlessLLM while removing Ray Serve’s~\cite{rayserve} cluster management overhead to isolate its inference performance. \\
% \textit{FaaSNet}~\cite{wang_faasnet_nodate}. A industry-adopted serverless function container provisioning system that optimizes P2P transfer topology for auto-scaling. We adopt its default binary tree topology, as it does not support GDR. \\
% \textit{NCCL}~\cite{nccl}. An industry-standard communication library for multi-GPU training and inference developed by NVIDIA, optimizing collective communication primitives such as all-reduce and broadcast using GDR. Since NCCL lacks a native multicast primitive, we adapt its broadcast primitive by dynamically creating subgroups and transmitting model blocks only to designated subgroups, effectively achieving multicast functionality.}

% These adjustments allow for a more direct performance comparison between the evaluated systems.

%% yue: I commented out the following paragraph as it's redundant. We will define those terms in text as we go. 
\if 0
\noindent{\textbf{Terminology.}}
Each system is identified to by its primary setup, with a suffix added to specify a particular configuration. 
\textit{FaaScale-Net} refers to \SysName's model scaling leveraging GDR, while \textit{FaaScale-Mem } denotes to model scaling utilizing local memory cache.
Similarly, \textit{ServerlessLLM-SSD} represents \textit{ServerlessLLM} model scaling via local SSD, and \textit{ServerlessLLM-Mem} corresponds to scaling using local memory cache.
\fi 

% -----------------
% Latency of replcating a model to number of nodes
% ----------------
\begin{figure}[t]
\centering
% \subfigure[{\small\texttt{Replicate a Model to 2 Nodes}}] {
%     \includegraphics[width=0.3\textwidth, height=0.2\textwidth]
%     {figures/eval/transfer/1to2.png}
%     \label{fig:transfer_from_1_to_2}
% }
% \hspace{-5pt}
% \subfigure[{\small\texttt{4 Nodes}}] {
%     \includegraphics[width=0.225\textwidth]
%     {figures/eval/transfer/blk_latency_1to4.png}
% }
\includegraphics[scale=0.1]{figures/eval/transfer/shared_legend_transfer_latency.pdf} \\
% \subfigure[{\small\texttt{4 Nodes}}] {
%     \includegraphics[scale=0.1]
%     {figures/eval/transfer/14.pdf}\label{fig:transfer_from_1_to_4}
% }
% \subfigure[{\small\texttt{8 Nodes}}] {
%     \includegraphics[scale=0.1]
%     {figures/eval/transfer/18.pdf}\label{fig:transfer_from_1_to_8}
% }
% \subfigure[{\small\texttt{12 Nodes}}] {
%     \includegraphics[scale=0.1]
%     {figures/eval/transfer/112.pdf}\label{fig:transfer_from_1_to_12}
% }
\includegraphics[scale=0.1]
{figures/eval/transfer/transfer_latency_bar.pdf}\label{fig:transfer_bar}
\vspace{-10pt}
\caption{End-to-end model multicast latency.
% with varying model sizes and scales.
\textit{\textmd{The 4-node test involves 16 GPUs, with each node containing 4 GPUs. The 8-node and 12-node tests use 8 GPUs and 12 GPUs, respectively. All tests have $k = 1$ (a single source).}} 
}
%\tfcomment{End-to-end model multicast latency with varying model size.}
%\textit{\textmd{This figure presents the model transfer latency results varying Llama-2 model sizes. 
% This figure presents the transfer latency results, measured in seconds, between \SysName, \textit{FaaSNet}, and \textit{NCCL},
% evaluated on Llama-2 models of varying sizes. 
% evaluated on Llama-2 models of varying sizes (7B, 13B, and 70B). 
% The X-axis represents the model sizes, while the Y-axis indicates the transfer latency in seconds. Each bar represents the transfer latency of a specific system for the given model model. 
% Subfigure~\ref{fig:transfer_from_1_to_4} depicts the latency for a setup with 4 nodes, while Subfigure~\ref{fig:transfer_from_1_to_8} shows the latency for 8 nodes.
% \ruicomment{TODO: Tingfeng, please replace the abbreviation with the full name for each system}
%}}}
\label{fig:transfer_latency}
\vspace{-5pt}
\end{figure}

% ----------seperation line---------------
\subsection{Multicast Performance}
\label{subsec:model_transfer_performance}

We first evaluate \SysName's raw block transmission latency achieved under binomial pipeline multicast. 

\PHM{Model transfer latency.}
% We evaluate the efficiency of the \SysName's transfer strategy by comparing its transfer latency against two popular transfer systems , \textit{FaaSNet} and \textit{NCCL}.
% We evaluate \SysName's transfer efficiency by comparing its latency against \textit{FaaSNet} and \textit{NCCL}.
% \textit{FaaSNet} splits large data into smaller blocks and uses a binary tree structure for data transfer, while \textit{NCCL} employs a ring-based structure to broadcast data blocks. To test \SysName's stransfer performance, we ran expexeriments with different model sizes and cluster setups.
% \textit{FaaSNet} employs a binary tree structure for data transfer, while \textit{NCCL} uses a ring-based structure for broadcasting.
% We test \SysName with varying model sizes and cluster setups (4, 8, and 12 nodes).
Fig.~\ref{fig:transfer_latency} shows the end-to-end multicast latency with different GPU cluster settings. 
%for Llama-2 models on cluster of 4, 8, and 12 nodes.
Overall, \SysName achieves up to a $1.82\times$ and $1.53\times$ speedups over \textit{FaaSNet} and \textit{NCCL}, respectively. 
%The key observation is that 
We observe that \SysName's multicast performance advantage increases with both model size and cluster scale. For smaller models on fewer nodes (e.g., 7B on 4 nodes), \SysName is only modestly faster than the other systems; however, with larger models and more nodes (e.g., 70B on 12 nodes), \SysName’s performance benefit expands considerably. 
This improvement comes from \SysName's binomial pipeline, %
%\yuecomment{Needs Alex to rewrite the reason} 
which splits a model into blocks and utilizes the entire cluster bandwidth resource to transmit blocks, maximizing link utilization and GPU-level parallelism. We explain the performance differences next when investigating fine-grained block arrival latencies. 
\if 0
In contrast, tree-based multicast (e.g., \textit{FaaSNet}'s binary tree, \textit{NCCL}'s tree) limit concurrency by sending data only to a fixed set of children, leaving cluster bandwidth resources idle. 
%creating idle periods that inflate latency.
Meanwhile, ring-based topologies (e.g., \textit{NCCL}'s ring) push data sequentially throughput a single path, so a slowdown on any link stalls the entire pipeline\footnote{NCCL dynamically adjusts the underlying broadcast topologies between tree-based and ring-based approaches.}. 
\fi 
%In both cases, these rigid structures can underutilized network links and lead to higher tail latencies.
%\SysName avoids these rigid structures by having each receiver immediately becomes a sender for the next wave of blocks, effectively doubling the number of active senders at each step.


\begin{figure}[t]
\centering
\includegraphics[scale=0.1]{figures/eval/transfer/shared_legend_block_latency.pdf} \\
\vspace{-0.5em}
\subfigure[{\small\texttt{4 Nodes}}] {
    \includegraphics[scale=0.1]
    {figures/eval/transfer/block_latency/1_4.pdf}
}
\subfigure[{\small\texttt{8 Nodes}}] {
    \includegraphics[scale=0.1]
    {figures/eval/transfer/block_latency/1_8.pdf}
}
\subfigure[{\small\texttt{12 Nodes}}] {
    \includegraphics[scale=0.1]
    {figures/eval/transfer/block_latency/1_12.pdf}
}
\vspace{-14pt}
\caption{Model block transfer latency CDF. \textit{\textmd{We randomly select two nodes ({\small\texttt{A}} and {\small\texttt{B}}) from the cluster and report the arrival latency for each block. Other nodes show similar pattern. 
%To improve clarity, we select two representative nodes from the cluster for each system. 
}}}
%\tfcomment{CDF of transfer blocks arrival latency with varying \# nodes. }
\label{fig:blk_latency_cdf}
\vspace{-5pt}
\end{figure}



% -----------------
% Throughput performance of replicate models to cluster via GDR (could be in either host or GPU)
% ----------------
\begin{figure*}[ht]
\centering
\includegraphics[scale=0.1]{figures/eval/throughput/shared_legend_scaling_throughput.pdf} \\
\vspace{-0.5em}
\subfigure[{\small\texttt{Llama-2 7B}}] {
    \includegraphics[scale=0.12]
    {figures/eval/throughput/llama-2-7b/112.pdf}
    \includegraphics[scale=0.12]
    {figures/eval/throughput/llama-2-7b/212.pdf}
    \includegraphics[scale=0.12]
    {figures/eval/throughput/llama-2-7b/412.pdf}
    \label{fig:throughput_7b}
}
\vspace{-1em}
\subfigure[{\small\texttt{Llama-2 13B}}] {
    \includegraphics[scale=0.12]
    {figures/eval/throughput/llama-2-13b/112.pdf}
    \includegraphics[scale=0.12]
    {figures/eval/throughput/llama-2-13b/212.pdf}
        \includegraphics[scale=0.12]
    {figures/eval/throughput/llama-2-13b/412.pdf}
    \label{fig:throughput_13b}
}
\vspace{-1em}
\subfigure[{\small\texttt{Llama-2 70B}}] {
    \includegraphics[scale=0.12]
    {figures/eval/throughput/llama-2-70b/14.pdf}
        \includegraphics[scale=0.12]
    {figures/eval/throughput/llama-2-70b/24.pdf}
    \label{fig:throughput_70b} 
}
\vspace{-0.5em}
\caption{Throughput scaling via GDR with varying model sizes.
\textit{\textmd{ServerlessLLM relies on local SSDs during scaling, while all other systems use GDR for inter-node communication. 
% We stress test each system and measure the tokens-per-second throughput changes. 
The $k$ configurations apply only to \SysName.
The mini-plots show the extended timeline for ServerlessLLM.
% As before, Llama-2 7B and 13B are tested on Testbed1 while Llama-3 70B is scaled on Testbed2. 
}}}
%\tfcomment{Throughput via GDR with varying model size.~\textit{The mini-plots show the extended timeline for ServerlessLLM.}}
\label{fig:throughput_gdr}
\vspace{-5pt}
\end{figure*}

% -----------------
% Blk latency
% ----------------
\noindent{\textbf{Block transfer latency.}
To zoom in, we measure the fine-grained block arrival latency for the Llama-2 13B model (Fig.~\ref{fig:blk_latency_cdf}). 
We observe that \SysName receives the first and last model blocks almost at the same time across all cluster sizes. 
Interestingly, while \SysName and \textit{NCCL} perform comparably for most blocks, \textit{NCCL} experiences a significantly high tail latency for the \emph{first} block, due to high group initialization overhead~\cite{nccl_cold_start_issue}: \textit{NCCL} requires creating a group for target processes before performing the broadcast. However, establishing a new group incurs high cost of up to hundreds of milliseconds as we observe in our tests. 
%\SysName's \AlgoName dynamically creates pipeline subgroups during runtime to adapt to available GPU resources across nodes. This process introduces unavoidable cold-start overhead when using \textit{NCCL} as the underlying model transmission layer. 
Another trend is that \textit{FaaSNet}'s tail latency grows as the cluster size increases, while \SysName maintains consistently low latency for all blocks. This is because \textit{FaaSNet}'s tree-based multicast algorithm (binary tree) limits parallelism to a fixed number of children nodes at the bottom of the topology~\cite{wang_faasnet_nodate}, while \SysName's binomial pipeline effectively improves the network I/O parallelism (\# of active senders) at each step. 



\subsection{Throughput Performance}
\label{subsec:throughput}

We stress-test \SysName's application-level throughput. 


\PHM{Scaling via GDR.} 
%As shown in Fig.~\ref{fig:throughput_gdr}, 
We measure \SysName's throughput scaling ability under high-stress loads by varying $k$ and compare it against \textit{ServerlessLLM}, \textit{FaaSNet}, and \textit{NCCL}.  
%while varying \emph{k}. 
\SysName (green) consistently outperforms the baselines by achieving peak throughput significantly faster across various \emph{k} levels. 
One key observation is that, while \textit{FaaSNet}, \textit{NCCL}, and \textit{ServerlessLLM} steadily scale their throughput on a similar timeline (e.g., 0.6s for \textit{FaaSNet} for Llama-2 7B), 
\SysName effectively halves its ramp-up time when its \emph{k} increases. For the Llama-2 13B example, \SysName begins scaling at around 1.2s when $k=1$, whereas with $k=4$, scaling starts significantly earlier, at around 0.25s. 
%The improved scaling efficiency highlights the effectiveness of \AlgoName's adaptive multicast using $k$-way transmission and optimized model block transfer order (see \S\ref{subsec:model_multicast}). 
This is because \AlgoName's block transfer and opportunistic execution pipeline allows GPUs to collaboratively load and serve requests as soon as sufficient model blocks are loaded into GPU memory, rather than waiting for the full model to be transferred (see \S\ref{subsec:model_multicast}). 
% For \emph{k}=1,  \SysName-Net shows speedups of at least $1.25\times$ to \textit{FaaSNet}, $1.92\times$ to \textit{NCCL}, and $5.1\times$ to \textit{ServerlessLLM-SSD} across different models. 
Meanwhile, \textit{ServerlessLLM-SSD} suffers from a dramatically longer ramp-up period for throughput scaling, primarily because: 
(1) it lacks GDR/RDMA multicast support, relying solely on local SSD, which slows down model loading; 
(2) it waits until the entire model is loaded into a GPU before serving requests; 
and (3) even for local SSD-based loading, it does not incorporate LLM-specific optimizations like \SysName's $k$-way transmission and optimized transfer order. 
%it does not leveraging any LLM features for efficient loading or serving operations.
\textit{FaaSNet} and \textit{NCCL} leverage GDR in multicast, making them more efficient than \textit{ServerlessLLM}. However, they lack support for collaborative distributed execution and have lower transmission parallelism, which limits their scaling efficiency. 
Another notable observation is that, when $\emph{k}\geq2$, \SysName exhibits a staircase-shaped performance plateau (e.g., between 0.25s and 0.75s for $k=4$ with Llama-2 13B). This behavior is likely due to implementation overhead, which may not be fully optimized. Addressing this performance plateau is part of our future work. 
%introduced by implementation, which may not be fully optimized.
% As discussed in \S~\ref{subsec:model_multicast}, peak performance should theoretically scale without these plateaus.


\begin{figure}[t]
\centering
\includegraphics[scale=0.1]{figures/eval/throughput/shared_legend_k2_cache.pdf} \\
% \hspace{-1em}

\subfigure[{\small\texttt{Llama-2 7B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/throughput/llama-2-7b/llama-2-7b_k2_cache.pdf}\label{fig:throughput_cache_7b}
}
\subfigure[{\small\texttt{Llama-2 13B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/throughput/llama-2-13b/llama-2-13b_k2_cache.pdf}\label{fig:throughput_cache_13b}
}
\subfigure[{\small\texttt{Llama-2 70B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/throughput/llama-2-70b/llama-2-70b_k2_cache.pdf}\label{fig:throughput_cache_70b}
}
\vspace{-10pt}
\caption{Throughput scaling via local cache with varying model sizes.} 
%\textit{\textmd{{This figure demonstrates the throughput performance when scaling via memory cache with \emph{k} value fixed to 8 for LLaMa-2 7B and 13B and 2 for LLaMa-2 70B.}}}}
%\tfcomment{Throughput via local cache with varying model size.}
\label{fig:throughput_cache}
\vspace{-10pt}
\end{figure}


\PHM{Scaling via local cache. }
A key design of \textit{ServerlessLLM} is its use of host memory caching to improve performance by loading models from local host memory.  
% To ensure a fair comparison, which leverages a local memory cache to enhance performance, we configure \SysName to similarly utilize the local memory cache. 
For fair comparison, \SysName is configured to utilize local memory caching similarly to \textit{ServerlessLLM}.
Specifically, both systems are configured such that $R$ out of $N$ nodes have models loaded into their GPUs,
%store a model instance locally in host memory, 
while the remaining $k$ nodes in the cluster load the model from the local host memory cache (\emph{N}= \emph{R} + \emph{k}).  For this evaluation, \emph{k} is fixed at $8$ for Llama-2 7B and 13B, and at $2$ for Llama-2 70B. Fig.~\ref{fig:throughput_cache} shows that \SysName scales $2\times$ to over $4\times$ faster than \textit{ServerlessLLM} in many scenarios. 
% It is important to note that \emph{K} has slightly different meanings compared to scaling via GDR to \SysName.
% \emph{K} refers to the number of block loading uniqueness from local memory while it refers to .
% A key difference is that \SysName has the ability to  serve inference requests and utilizes GDR to transfer intermediate results between nodes while loading, allowing it to hide the cost of loading and to rapidly activate idle resources, enabling \SysName to efficiently server queued requests.
% A key difference lies in that \AlgoName allows \SysName to serve inference requests collaboratively during the loading phase as long as the model instance can be formed across \emph{K}) nodes, while \textit{ServerlessLLM} processes requests only after completing model loading from its multi-tier storage layer.
% A key difference lies in that \AlgoName allows \SysName to collaboratively serve inference requests during the loading phase, as long as the model instance can be formed across the \emph{k} nodes. 
A key difference lies in that \SysName's \AlgoName enables collaborative inference during the loading phase, allowing requests to be served across \emph{k} nodes as soon as the first model block becomes available in GPU.   
% In contrast, \textit{ServerlessLLM} only processes requests after completing model loading from its host memory.
In contrast, \textit{ServerlessLLM} waits until the model is fully loaded before processing requests.


\begin{figure}[t]
\centering
\includegraphics[scale=0.1]{figures/eval/throughput/shared_legend_k2_cache.pdf} \\
\vspace{-0.5em}
\subfigure[{\small\texttt{Llama-2 7B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/throughput/cold-start/llama-2-7b.pdf}\label{fig:throughput_cold_start_7b}
}
\subfigure[{\small\texttt{Llama-2 13B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/throughput/cold-start/llama-2-13b.pdf}\label{fig:throughput_cold_start_13b}
}
\subfigure[{\small\texttt{Llama-2 70B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/throughput/cold-start/llama-2-70b.pdf}\label{fig:throughput_cold_start_70b}
}
\vspace{-10pt}
\caption{Cold-start throughput (scaling via GDR without models preloaded to GPUs, with $k=1$). 
%\textit{\textmd{{This figure demonstrates the throughput performance when scaling via GDR with no model in GPU with \emph{k} value fixed to 1.}}}}\tfcomment{Cold-start throughput (scaling via GDR without a preloaded model in GPU, with \emph{k} = 1).
}
\label{fig:throughput_cold_start}
% \vspace{-1em}
\vspace{-5pt}
\end{figure}

\PHM{Cold-start comparison. } 
% To better understand the performance differences between \SysName and \textit{ServerlessLLM}, we further evaluate throughput performance in a scenario where neither system had model instances loaded into the GPU(s) across the entire cluster.
% To better understand \SysName's stress-handling ability, we assess throughput performance under cold-start conditions, where no model instances are preploaded into the GPU(s) across the cluster.
%To evaluate \SysName's ability to sustain cold startups under stress load, 
We evaluate throughput performance under a cold-start scenario, where no model instances are preloaded into GPUs across the cluster. Instead, one node stores the model in its host memory cache. 
%handle stress loads under cold-start condition, 
%we measure throughput performance when no model instances are preloaded into GPU(s) across the cluster. Instead, one node keeps a model instance in memory cache.
In \textit{ServerlessLLM}, the remaining nodes load the model from SSD, while in \SysName, the cached model is loaded into local GPU(s) and multicasted across the cluster via GDR.
To ensure fairness, both systems use \emph{k} = 1. 
% While the rest of nodes in \textit{ServerlessLLM} cluster loads this model from SSD, the node has that model loads it into GPU and broadcast this model across the cluster simultaneously via RDMA in \SysName (\SysName only supports GDR but also RDMA with host memory). 
% While this node loads the model into GPU, it uses RDMA to broadcast the model in host memory in parallel. 
% while the rest of the cluster scales the model using their primary features (e.g., GDR for \SysName and SSD for \textit{ServerlessLLM}).
% In this scenario, we set \emph{k} = 1 to ensure a fair comparison between the two systems.
% Figure~\ref{fig:throughput_cold_start} presents the throughput results. Overall, \SysName outperforms \textit{ServerlessLLM}, achieving performance gains ranging from $3.75\times$ up to $11.4\times$ across three Llama-2 models. 
As shown in Fig.~\ref{fig:throughput_cold_start},  \SysName significantly outperforms \textit{ServerlessLLM} by $3.75\times$ to $11.4\times$ across the three model sizes. This improvement is due to \SysName's \AlgoName, which allows immediate request serving as soon as the first block is loaded into GPU, eliminating unnecessary delays. 



% ----------seperation line---------------

\subsection{Latency Performance}
\label{subsec:latency}

Next, we measure the TTFT latency across varying loads.  

\noindent{\textbf{Scaling via GDR. }
Fig.~\ref{fig:latency_gdr} (a)-(c) plots the TTFT latency and Fig.~\ref{fig:latency_gdr} (d)-(f) shows the zoomed-in CDF given a specific RPS (requests per second) level. 
%across varying RPS levels for Llama-2 models. 
% The evaluation is based on a fixed set of 50 requests.
% Figure~\ref{fig:latency_gdr} presents the latency performance across varying RPS values for Llama-2 models, measured as time-to-first-token (TTFT).
% The evaluation is based on a fixed set of 50 requests.
% Figure~\ref{fig:latency_gdr_7b}-~\ref{fig:latency_gdr_70b} shows the $90$th percentile TTFT, where \SysName achieves up to $7\times$ lower than \textit{ServerlessLLM} and is $1.8\times$ to $1.6\times$ lower TTFT than \textit{NCCL} and \textit{FaaSNet} in many cases.  
% At 6 RPS for LLaMa-2-7B, \SysName serves $90\%$ of requests in $0.71$ seconds, closely matching $0.89$ seconds in \textit{FaaSNet} and $1.19$ seconds in \textit{NCCL}, while \textit{ServerlessLLM} takes $3.86$ seconds, achieving a $1.25\times$, $1.68\times$, and $5.44\times$ speedup over \textit{FaaSNet}, \textit{NCCL}, and \textit{ServerlessLLM}. 
% At $2$ RPS for Llama-2 70B, \SysName completes $90\%$ of requests in $15.57$ seconds, while \textit{ServerlessLLM} takes $71$ seconds, achieveing $4.56\times$ speedup.
%Fig.~\ref{fig:latency_gdr_7b_rps6}-~\ref{fig:latency_gdr_70b_rps1.5} provide a zoomed-out view, showing that \SysName completes serving 50 request in $1.1$ seconds, which is $2\times$, $1.4\times$, and $8\times$ faser than \textit{FaaSNet}, \textit{NCCL}, \textit{ServerlessLLM} respectively for Llama-2 13B model. 
We observe that \SysName starts serving all 50 requests in 1.1s, which is $2\times$, $1.4\times$, and $8\times$ faster than \textit{FaaSNet}, \textit{NCCL}, \textit{ServerlessLLM}, respectively. 
The 7B and 70B models exhibit a similar trend.
%(refer to these sub-figures for detailed comparisons).
%One noticeable trend is that 
\textit{ServerlessLLM}-SSD suffers from a long-tail TTFT latency, caused by: 
(1) slow SSD I/Os during on-demand loading, 
and (2) delayed inference execution due to waiting for the entire model to be fully loaded into GPUs. 


\PHM{Scaling via local cache. }
Similarly, Fig.~\ref{fig:latency_cache} (a)-(c) shows the TTFT latency of \SysName and \textit{ServerlessLLM} when scaling with local memory cache and Fig.~\ref{fig:latency_cache} (d)-(f) shows the zoomed-in CDF, under the same setup used in Fig.~\ref{fig:throughput_cache} (\S\ref{subsec:throughput}). 
As shown in Fig.~\ref{fig:latency_cache_13b}, \SysName is $1.63\times$ faster than \textit{ServerlessLLM} at the $90^{th}$ percentile of requests. 
Even in the best-case scenario for \textit{ServerlessLLM}, where local memory caching is used, \SysName still outperforms \textit{ServerlessLLM} in TTFT. 
This is, once again, due to \textit{ServerlessLLM}'s lack of transmission and execution pipeline. 


\begin{figure}[t]
\centering
\includegraphics[scale=0.1]{figures/eval/latency/shared_legend_scaling_latency.pdf} \\
\vspace{-5pt}
\subfigure[{\small\texttt{Llama-2 7B }}] {
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-7b/remote_p90_vs_rps.pdf}
    \label{fig:latency_gdr_7b}
}
\subfigure[{\small\texttt{Llama-2 13B}}] {
    \includegraphics[scale=0.1]{figures/eval/latency/llama-2-13b/remote_p90_vs_rps.pdf}\label{fig:latency_gdr_13b}
}
\subfigure[{\small\texttt{Llama-2 70B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-70b/remote_p90_vs_rps.pdf}\label{fig:latency_gdr_70b}
}
\subfigure[\small
\begin{tabular}{c}
\texttt{Llama-2 7B}\\
\texttt{RPS=6}
\end{tabular}] {
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-7b/remote_rps6_cdf.pdf}\label{fig:latency_gdr_7b_rps6}
}
\subfigure[\small
\begin{tabular}{c}
\texttt{Llama-2 13B}\\
\texttt{RPS=3}
\end{tabular}] {
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-13b/remote_rps3_cdf.pdf}\label{fig:latency_gdr_13b_rps3}
}
% \vspace{-5pt}
\subfigure[\small
\begin{tabular}{c}
\texttt{Llama-2 70B}\\
\texttt{RPS=1.5}
\end{tabular}] 
{
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-70b/remote_rps1.5_cdf}\label{fig:latency_gdr_70b_rps1.5}
}
\vspace{-1.5em}
\caption{Latency scaling via GDR.
% \textit{\textmd{We issue 50 inference requests for each system and control the RPS to a specified level.}}
% (a)-(c) illustrates the $90th$ percentile TTFT (time-to-first-token) latency results from a total of 50 requests across varying request rates. 
% (d)-(f) presents the overall TTFT latency distribution for 50 requests.  
% % The X-axis represents the request rate (RPS), capturing the number of requests processed per second, while the Y-axis shows the latency in seconds. Each curve represents the performance of one of the four evaluated systems.
% }}
}
\label{fig:latency_gdr}
\vspace{-10pt} 
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.1]{figures/eval/latency/shared_legend_scaling_latency_cdf.pdf} \\
\vspace{-0.5em}
\subfigure[{\small\texttt{Llama-2 7B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-7b/local_p90_vs_rps.pdf}\label{fig:latency_cache_7b}
}
\subfigure[{\small\texttt{Llama-2 13B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-13b/local_p90_vs_rps.pdf}\label{fig:latency_cache_13b}
}
% \vspace{-5pt}
\subfigure[{\small\texttt{Llama-2 70B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-70b/local_p90_vs_rps.pdf}\label{fig:latency_cache_70b}
}
\subfigure[\small
\begin{tabular}{c}
\texttt{Llama-2 7B}\\
\texttt{RPS=6}
\end{tabular}] {
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-7b/local_rps6_cdf.pdf}\label{fig:latency_cache_7b_rps6}
}
\subfigure[\small
\begin{tabular}{c}
\texttt{Llama-2 13B}\\
\texttt{RPS=3}
\end{tabular}] {
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-13b/local_rps3_cdf.pdf}\label{fig:latency_cache_13b_rps3}
}
% \vspace{-5pt}
\subfigure[\small
\begin{tabular}{c}
\texttt{Llama-2 70B}\\
\texttt{RPS=1.5}
\end{tabular}] 
{
    \includegraphics[scale=0.1]
    {figures/eval/latency/llama-2-70b/local_rps1.5_cdf}\label{fig:latency_cache_70b_rps1.5}
}
\vspace{-1.5em}
\caption{Latency scaling via local cache. 
% \textit{\textmd{We issue 50 requests for each system and control the RPS to a specified level.}} 
% This figure illustrates the $90th$ percentile TTFT (time-to-first-token) latency results from a total of 50 requests across varying request rates, comparing the performance of \SysName and \textit{ServerlessLLM} when loading models from local memory cache. 
% % The Y-axis represents the cumulative distribution function (CDF), ranging from 0 to 1, which shows the proportion of requests completed within a given latency. 
% % The evaluation includes the performance of \SysName and \textit{ServerlessLLM}.
% }}
}
\label{fig:latency_cache}
\vspace{-1.5em}
\end{figure}




% -------------------
% sys-behavior
% -------------------
\begin{figure*}[t]
\centering
\includegraphics[scale=0.1]{figures/eval/throughput/shared_legend_gpu_allocation.pdf} \\
\vspace{-0.5em}
\subfigure[{\small\texttt{Llama-2 7B}}] {
    \includegraphics[scale=0.123]
    {figures/eval/e2e/sys-behavior/llama-2-7b/scaling_and_rps.pdf}\label{fig:e2e_sys_behavior_7b}
}
\subfigure[{\small\texttt{Llama-2 13B}}] {
    \includegraphics[scale=0.123]
    {figures/eval/e2e/sys-behavior/llama-2-13b/scaling_and_rps.pdf}\label{fig:e2e_sys_behavior_13b}
}
\subfigure[{\small\texttt{Llama-2 70B}}] {
    \includegraphics[scale=0.123]
    {figures/eval/e2e/sys-behavior/llama-2-70b/scaling_and_rps.pdf}\label{fig:e2e_sys_behavior_70b}
}
\vspace{-10pt}
\caption{GPU allocation over time under a 30-minute BurstGPT workload.  
~\textit{\textmd{Top: RPS over time. 
Middle (Row 2-5): System-specific results sharing the same timeline. 
Bottom: Cumulative GPU consumption of all systems.  
% This figure compares the scaling behavior of \SysName, \textit{Ideal}, \textit{FaaSNet}, \textit{ServerlessLLM}, and \textit{NCCL}.
% chaobo comment
% The \textit{Ideal} baseline assumes zero model-loading overhead by pre-reserving
% models across all GPUs that perfectly adjusts resources to RPS changes but is not practically achievable.
% Top: RPS over time from trace. Middle: system-specific performance plots sharing same timeline. Bottom: Cumulative GPU consumption of all systems. The X-axis represents time (seconds), while the Y-axis shows RPS or allocated GPUs. 
%Numbered RPS highlight key scaling spikes. 
The RPS timeline include labeled spikes (Top). 
%Numbered scaling events (Row 2-5) correspond to those spikes. 
The green dotted line shadows \SysName's scaling behavior, allowing direct comparison against other systems.   
%for direct comparison of GPU scaling efficiency across systems.
% Each subfigure consists of multiple plots: the top shows RPS (requests per second) over time, followed by plots  shows \SysName and the 3rd plot shows the idea performance over RPS. The 4th to 6th plots are the baselines. The last plot shows the cumulative GPU time for each system (\SysName, \textit{Idea}, \textit{FaaSNet}, \textit{ServerlessLLM}, and \textit{NCCL}) over time.
% The X-axis represents time (seconds) in both plots, while the Y-axis represents RPS in the top plot and allocated GPUs in the bottom plot.
% RPS spikes are numbered (e.g., 1, 2, 3) to highlight key points, and \SysName (green dotted line) is included for comparison against each baseline.
% To interpret, trace each numbered spike from the top plot to the corresponding GPU allocation responses in the lower plots, comparing how efficiently and promptly each system scales GPU resources in response to RPS demands.
}}}
%\tfcomment{GPU allocation over time under burstGPT workload.}
\label{fig:e2e_sys_behavior}
% \vspace{-10pt} 
\vspace{-5pt} 
\end{figure*}


\subsection{Real-World LLM Workload}
\label{subsec:e2e}

% In this subsection, we evaluate how well \SysName performs under a real-world workload trace. Due to the cost constraints, we run the BurstGPT trace for 30 minutes and select a representative interval from the trace for analysis.
In this section, we evaluate \SysName's performance using BurstGPT~\cite{burstGPT_arxiv24}, a real-world LLM workload trace collected from regional Azure OpenAI GPT services. The original workload is highly bursty and we select a 30-minute trace snippet from the workload for evaluation.
%and selecting a representative interval for analysis. 
% We evaluate the TTFT and the number of allocated GPU metrics to demonstrate \SysName's effectiveness. 
%The setup for each system differs in how model instances are managed. 
We make the following assumptions:
\textit{NCCL} and \textit{FaaSNet} prioritize loading models from remote GPUs using GDR and only fall back to local SSD load if none of the GPUs in the cluster have an available model instance. 
\textit{ServerlessLLM} relies solely on local-cache-based loading---it loads models from host memory on a cache hit and from SSD on a cache miss.
%---a model is loaded from the host memory upon a memory cache hit and loaded from SSD upon a miss. 
As \textit{ServerlessLLM}, \SysName supports best-effort local host memory caching but falls back to \AlgoName multicast if the requested model is not in host memory. 
%that a model instance is stored in host memory but falls back to SSD loading upon a host memory cache miss. 

\if 0
Each system tested manages model instances differently.
\textit{NCCL} and \textit{FaaSNet} may have a model instance already loaded in the GPU; otherwise, they load it from SSD. 
\textit{ServerlessLLM} assumes that a model instance is stored in host memory but falls back to SSD loading upon a host memory cache miss. 
\SysName ensures that a model instance is either in the GPU or loaded from host memory, avoiding reliance on SSD access.
\fi 

\PHM{Scaling behaviors.} 
Fig.~\ref{fig:e2e_sys_behavior} shows the dynamic GPU allocation timeline in response to fluctuating RPS. 
%as RPS fluctuates. 
\textit{Ideal Scaling} assumes zero model-loading overhead, where a model can be instantly loaded into and swapped out of GPU(s) without delay. This is not practically achievable due to real-world constraints such as resource limitation and data transfer cost. 
%by prereserving models across all GPUs that perfectly adjusts resources to RPS changes but is not practically achievable.
\SysName scales out and in significantly faster than other systems across three model sizes. 
% The key reason why \textit{ServerlessLLM}, \textit{NCCL}, and  \textit{FaaSNet} fall short is that they suffer from either on-demand SSD I/O bottlenecks, long delays before model instance loading completes, or a combination of both.
All three baselines experience delayed scaling out and delayed scaling in when responding to workload spikes. 
%One notable observation is that \textit{ServerlessLLM}, \textit{NCCL}, and \textit{FaaSNet} all experience delayed scaling when responding to Spike 1 (the rectangle area after 1 for Llama-2 7B for \textit{ServerlessLLM} and the rectangles for both \textit{FaaSNet} and \textit{NCCL} for Llama-2 13B). This delay occurs because they do not have a model instance in their primary storage (GPU), forcing them to load the model on-demand from SSD, introducing scaling delay overhead. 
% The ideal scaling assumes zero model-loading overhead by pre-reserving models across all GPUs that perfectly adjusts resources to
% RPS changes but is not practically achievable. 
% \yuecomment{ why other baselines do not scale elastically, in particular sllm / nccl.}
% For Llama-2 7B, \SysName closely matches follows ideal scaling, as indicated by the shadowed area, whereas the other three systems respond more slowly to scale out and in, particularly during spikes 1, 2, and 3. 
% Similar trends are observed for the 13B and 70B models.
To quantify the cost effectiveness of GPU resources, we measure the cumulative GPU time for each system. \SysName consumes up to $17.8\%$, $18.1\%$, and $31.3\%$ less GPU resource than \textit{FaaSNet}, \textit{NCCL}, and \textit{ServerlessLLM}, respectively. 
While \textit{Ideal Scaling} achieves the lowest cumulative GPU time, \SysName maintains the closest GPU consumption to this ideal case, with a small gap ranging from $4.3\%$ and $18.6\%$, demonstrating \SysName's superior auto-scaling capability. 
%efficiency in dynamic resource allocation. 
%\textit{Ideal Scaling} incurs the minimal cumulative GPU time across all, but is only $xx\%$ shorter than that of \SysName. 
%Additionally, \SysName maintains the closest GPU consumption to the ``ideal scaling'' case, with a gap of $13\%$. 


% -------------------
% real-world trace TTFT latency distribution
% -------------------
\begin{figure}[h]
% \vspace{-0.5em}
\centering
\includegraphics[scale=0.1]{figures/eval/e2e/latency/shared_legend_e2e_scaling_latency.pdf} \\
\subfigure[{\small\texttt{Llama-2 7B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/e2e/latency/llama-2-7b/burst_gpt_TTFT_cdf.pdf}\label{fig:e2e_ttft_7b}
}
\subfigure[{\small\texttt{Llama-2 13B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/e2e/latency/llama-2-13b/burst_gpt_TTFT_cdf.pdf}\label{fig:e2e_ttft_13b}
}
\subfigure[{\small\texttt{Llama-2 70B}}] {
    \includegraphics[scale=0.1]
    {figures/eval/e2e/latency/llama-2-70b/burst_gpt_TTFT_cdf.pdf}\label{fig:e2e_ttft_70b}
}
\vspace{-1em}
\caption{CDF of TTFT under the BurstGPT workload.
% ~\textit{\textmd{This figure shows the TTFT distribution across four systems. 
% The X-axis represents the TTFT in seconds, indicating the latency for generating the first token of a response. The Y-axis represents the cumulative distribution function (CDF), ranging from 0 to 1, which shows the proportion of requests completed within a given latency. Each curve represents the performance of a specific system.
% }}
}
% \tfcomment{CDF of TTFT under burstGPT workload.}
\label{fig:e2e_ttft}
\vspace{-10pt}
\end{figure}

% \yuecomment{Need to explain why baselines suck.} 
\PHM{TTFT comparison.} 
% Figure~\ref{fig:e2e_ttft} shows the TTFT latency distribution for BurstGPT trace. As expected, \SysName outperforms all baselines across the three Llama-2 models.
Fig.~\ref{fig:e2e_ttft} shows the TTFT distribution, \SysName outperforms all baselines across the three models
%, achieving  $3\times$ to $5\times$ over \textit{ServerlessLLM} and $2.4\times$ to $4.5\times$ over \textit{NCCL} and \textit{FaaSNet}. 
Compared to previous TTFT results reported under static workloads (\S\ref{subsec:latency}), two key observations emerge. 
First, the CDF curves of \textit{NCCL} and \textit{FaaSNet} shift rightward, due to frequent model loading from SSD, leading to longer tail latency. 
Second, the CDF curve of \textit{ServerlessLLM} shifts leftward, likely due to a high cache hit rate when loading models into GPUs. 






\subsection{Sensitivity Analysis and Ablation Study}
\label{subsec:ablation_study}



%Appendices are supporting material that has not been peer-reviewed. 
% We conducted an ablation study to evaluate the impact
% of the number of transfer blocks, k-way transmission, and
% system optimizations on FaaScale’s performance. We fix
% k=1 to isolate the effects of the number of blocks and system
% optimizations on transfer latency. For k-way transmission
% analysis, we vary k to assess its role in orchestrating dis-
% tributed inference and accelerating request serving.
We conduct further experiments to evaluate the impact
of the number of transfer blocks, k-way transmission, and
system optimizations on \SysName’s performance. We fix
$k=1$ to isolate the effects of the number of blocks and system optimizations on transfer latency. For $k$-way transmission analysis, we vary $k$ to quantify its impact on LLM request serving.


% ---------------------
% ablation: block ordering level
% ---------------------
\PHM{Impact of \emph{k}-way transmission on throughput. }
% Figure~\ref{fig:ablation_blk_ordering} compares 
% \SysName's throughput performance under different. The \emph{k} = 4 configuration (\textit{\SysName-Net}) achieves the fastest scaling, demonstrating that a higher block ordering levels significantly enhance performance. In constract, \emph{k} = 2 configuration (\textit{\SysName-Half-Reorder}) exhibits a slower scaling indicating a moderate reduction in performance. Finally, the \emph{k} = 1 configuration (\textit{\SysName-Non-Reorder}) performs the worst, characterized by the slowest scaling and the lowest overall throughput.
Fig.~\ref{fig:ablation_blk_ordering} compares 
\SysName's throughput performance under different \emph{k}-way transmission levels. \textit{\SysName-Net} (\emph{k}=4) achieves the fastest scaling, demonstrating that a higher \emph{K}-way transmission levels significantly enhance performance. In constract,  \textit{\SysName-Half-Reorder} (\emph{k}=2), scales more slowly, showing a moderate performance drop. Finally, \textit{\SysName-Non-Reorder} (\emph{k}=1) performs the worst, characterized by the slowest scaling and the lowest overall throughput.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.125]
{figures/eval/ablation/execute-while-load/llama-2-13b/412.pdf}\label{fig:throughput_cache_13b}
\vspace{-10pt}\caption{Impacts of \emph{k}-way transmission on throughput performance.
% ~\textit{\textmd{{
% This figure demonstrates \SysName's throughput performance over time under different \emph{k}-way transmission levels. \SysName-Net represents \emph{k}=4, \SysName-Half-Reoder represents \emph{k}=2, and \SysName-None-Reoder represents \emph{k}=1.
% % The X-axis represents the progression of time after the system starts processing inference requests.
% % The Y-axis represents the number of inference requests processed per second.
% % Each line corresponds to a specific block ordering level. 
% }}}
}
\label{fig:ablation_blk_ordering}
\vspace{-10pt}
\end{figure}
% ---------------------
% ablation: System Optimizations 
% ---------------------
\PHM{Effect of system optimizations on transfer latency. }
Fig.~\ref{fig:ablation_breakdown} shows the impact of various optimizations on transfer latency in \SysName.
The``None'' configuration, without any optimizations, shows the highest transfer latency, exceeding $20$ ms. 
Adding pre-allocation (``+Pre-alloc'') significantly reduces the latency, demonstrating its importance in minimizing allocation overhead.
Further improvement is observed with tensor packing (``+Tensor-pack''), which reduces latency by optimizing the data layout for efficient transfers. The lowest latency is achieved with the addition of  (``+Host-mem RDMA''). 
This progression underscores the cumulative benefits of these optimizations, with each step contributing to a substantial reduction in transfer latency.


% ---------------------
% ablation: number of blocks 
% ---------------------
\PHM{Impact of the number of model blocks on transfer latency. }
Fig.~\ref{fig:ablation_num_blocks} shows that 16 blocks achieve the lowest transfer latency, likely due to a balance between RDMA request processing overhead and efficient data transfer. Fewer blocks (e.g., 8) result in higher latency because larger block sizes increase RDMA request processing overhead. More blocks (e.g., 24–48) also increase latency, likely due to the additional overhead of managing and transferring a larger number of smaller blocks. Latency decreases up to 16 blocks but rises beyond this point, highlighting diminishing returns and added complexity.


\begin{figure}
    \centering
    \begin{minipage}{0.225\textwidth}
        \centering
        \includegraphics[scale=0.125]{figures/eval/ablation/sys-optimizations/ablation.pdf}
        \vspace{-10pt}\caption{Performance breakdown of transfer latency.}
        % \tfcomment{Performance breakdown of transfer latency.}
        \label{fig:ablation_breakdown}
    \end{minipage}\hfill
    \begin{minipage}{0.225\textwidth}
        \centering
        \includegraphics[scale=0.125]{figures/eval/ablation/block-size/block_size.pdf}
        \vspace{-10pt}\caption{Latency with varying number of transfer blocks}
        \label{fig:ablation_num_blocks}
    \end{minipage}
    \vspace{-5pt}
\end{figure}




