%!TEX root = ../paper.tex
\begin{abstract}
Serverless computing has emerged as a compelling solution for cloud-based model inference. 
However, as modern large language models (LLMs) continue to grow in size, existing serverless platforms often face substantial model startup overhead.
This poses a significant challenge in efficiently scaling model instances to accommodate dynamic, bursty workloads commonly observed in real-world inference services.
In this paper, we introduce \SysName, an efficient serverless inference system to achieve fast model scaling.
The key idea behind \SysName is to leverage high-speed RDMA networks between GPU nodes for fast model multicast, while enabling distributed inference execution during model transmission---referred to as ``execute-while-load''.
\SysName proposes an efficient model scaling scheme, \AlgoName, which supports adaptive model multicast and dynamically constructs execution pipelines across receiving nodes for collaborative, distributed inference.
Additionally, \SysName supports efficient model management across GPU and host memory, allowing fast scaling for models across different storage tiers. 
Evaluation results show that \SysName enables fast model scaling and effectively handles load spikes, achieving up to 5$\times$ tail-latency improvement and 31.3\% cost reduction compared to state-of-the-art solutions on real-world LLM inference traces. 

% Serverless computing is known for its scalability, cost-efficiency, and ease of use, making it an increasingly popular choice for modern cloud applications. 
% This has driven a growing demand for inference serving over serverless platforms.
% However, existing solutions either suffer from prolonged startup times or incur additional resource costs by over-provisioning GPUs or maintaining models in host memory.

% To address this challenge, we propose FaaScale, an efficient serverless inference platform that achieves shortest time to reach peak throughput and shortest latency to generate first token compare to other baselines. 
% At its core, FaaScale introduces PipeCast, a novel dynamic pipelined execution scheme that partitions models into fine-grained blocks for multicast transmission and enables concurrent distributed inference across multiple nodes during transmission. PipeCast features (1) an adaptive model multicast for efficent pipeline establishment, (2) stochastic pipeline execution to bypass state transmission, and (3) fast switching to local inefernce once model loading completes. 
% In addition to PipeCast, FaaScale incorporates (1) a decoupled control and transmission architecture that allows FaaScale seamlessly to seamlessly support model loading from any storage tiers (e.g., Remote GPU or local host memory), (2) A two-level scheduling that allows fine-grained resource allocation management, and (3) multiple communication optimizations.

% We evaluate FaaScale using real workload trace. The eveluation demonstartes that FaaScale achieves $X\times$, $Y\times$, and $Z\times$ faster than \textit{ServerlessLLM}, \textit{FaaSNet}, and \textit{NCCL} in reaching peak throughput, and it is  $X\times$, $Y\times$, and $Z\times$ faster in terms of time-to-first-token latency. 
\end{abstract}