%!TEX root = ../paper.tex
\section{Background and Motivation}
\label{sec:background}

\subsection{LLM Inference}
\label{sec:background-llm-inference}
LLM inference autoregressively generates text output token by token from a user input (prompt) until it reaches the end-of-sentence (EoS) token or the maximum token limit. Each token depends on prior context, causing response times to vary. To speed up token generation, the model typically uses a KV cache~\cite{vllm} to cache the context from previous computations, reducing redundant work. Tokens are streamed to the user as they are produced, enabling real-time interaction. Performance is typically measured by \emph{time-to-first-token latency} or TTFT (time to generate the first token) and \emph{tokens-per-second} or TPS (token generation throughput).  
%In this section, we first give an overview of serverless inference.  We then describe the inefficiency of existing solutions to enabling fast and cost-efficient model scaling.


% \yuecomment{Terms like tokens, KVC, etc. show up completely out of the blue in later text. We need to introduce LLM basics very briefly here in background.} 
% This section provides an overview of serverless inference and describes the problems of existing solutions. 

\subsection{Serverless Inference and Requirements}
\label{sec:background-serverless}

In recent years, serverless computing has emerged as a compelling option for hosting ML inference services---often termed ``serverless inference''~\cite
{yang_infless_2022,yu_gillis_icdcs,zhang_mark:_2019, ali2022optimizing_serverless_inference, ali_batch_nodate, hong2024optimus, fu_serverlessllm_2024, romero_infaas_nodate, wang_faasnet_nodate}.
This approach allows users to simply publish models with inference code as functions, while cloud providers automatically manage resource provisioning, auto-scaling, scheduling, and fault tolerance. 
Serverless inference platforms offer fine-grained, pay-per-use billing~\cite
{aws_lambda,azurefunc,alibaba_serverless_gpu}, enabling substantial cost savings given the dynamic nature of inference workloads~\cite
{shen_nexus_2019,zhang_shepherd_nodate,gujarati_serving_2020,han_microsecond-scale_2022,lee_pretzel,kosaian_parity_2019,romero_infaas_nodate,
crankshaw_clipper,choi_serving_2022}.  
% offers substantial cost savings as users do not pay for idle resources under
% the pay-per-use pricing model,~\cite
% Real-world inference services often exhibit dynamic, bursty request patterns~\cite
% {shen_nexus_2019,zhang_shepherd_nodate,gujarati_serving_2020,han_microsecond-scale_2022,lee_pretzel,kosaian_parity_2019,romero_infaas_nodate,
% crankshaw_clipper,choi_serving_2022}. 
% Therefore, serverless platforms can effectively provision resources (i.e., increasing the number of function instances) in response to
% the changing workload, while users are billed based on the actual function runtime at a fine granularity, such as 1~ms~\cite
% {aws_lambda,azurefunc}.
These advantages have led to a growing trend of serverless-based LLM inference services~\cite{fu_serverlessllm_2024,hf_serverless,together_ai}, where end-users simply query a backend LLM and receive its output tokens as responses. 
% Each query requires computation m


\begin{figure}
\vspace{-8pt} 
    \centering
    \includegraphics[scale=0.125]{figures/trace}
    \vspace{-10pt}
    \caption{Normalized request rates of two representative serverless inference services.
    \textit{\textmd{\texttt{Trace 1} (top): a 12-hour serverless inference trace collected from Alibaba Cloud. \texttt{Trace 2} (bottom): a 12-hour trace from a real-world LLM workload~\cite{burstGPT_arxiv24}.}} 
    }
    \label{fig:serverless_trace}
    \vspace{-5pt}
    % \vspace{-.2in}
\end{figure}

Key to serverless inference is to enable rapid scaling of model serving instances to effectively handle real-world dynamic workloads.
Fig.~\ref{fig:serverless_trace} shows 12-hour normalized request rates of two representative LLM inference workloads from two production serverless platforms: a serverless inference service at Alibaba Cloud~\cite{alibaba_serverless_gpu} and a regional Azure OpenAI GPT service~\cite{burstGPT_arxiv24}.
These traces reveal highly bursty request arrival patterns, with request rates surging by more than one order of magnitude within just a few minutes. 
Such bursty requests can overwhelm existing serving instances, leading to violations of the latency requirements defined by the Service-Level Objectives (SLOs) of inference services. 
Therefore, serverless inference platforms must rapidly scale out model serving instances to accommodate load spikes, ensuring low inference latency and SLO attainment. 
We detail the request serving results under these traces in \S\ref{sec:background-limitations}.

\subsection{Inefficiency of Existing Solutions}
\label{sec:background-limitations}

We next discuss existing serverless inference platforms and their issues in achieving fast model scaling.

\PHM{Solution\#1: Loading remote models.}
In serverless inference platforms, a typical scaling process involves retrieving models from remote model registries (e.g., Hugging Face) or storage services (e.g., S3) to GPU nodes, which then launch the model serving instances. 
As models continue to grow in size---particularly LLMs with tens or hundreds of billions of parameters---fetching them across the network becomes increasingly time-consuming. 
% In particular, load a large model from a remote repository (e.g., HuggingFace) across the network can be time-consuming,
For instance, it takes over 18 minutes to load a Llama-70B model (140~GB) over a 1~Gbps network; and the overhead is further exacerbated by parallel model loading due to network bandwidth contention and registry throttling~\cite{wang_faasnet_nodate}.
Worse, initializing and loading large models within GPU nodes can take anywhere from tens of seconds to minutes to complete~\cite{faaswap,fu_serverlessllm_2024}. 
These significant startup overheads are incompatible with the requirements of real-time inference services, making the remote loading approach inefficient. 
%  consistently challenge the serverless inference platforms are constrained by , failing to 


\PHM{Solution\#2: Overprovisioning GPUs.}
To mitigate the cold-start problem, existing serverless inference solutions opt to maintain a sufficient number of active function instances with reserved GPUs, even when these instances are idling~\cite{aws_provisioned,yang_infless_2022,fc_billing}. 
% To overcome the limitation, existing serverless platforms support maintaining provisioned functions that remain active on GPUs, even when idling, to avoid cold-start overheads~\cite{aws_provisioned,yang_infless_2022,fc_billing}.
%Obliviously, t
This approach, however, leads to significant resource waste due to the dynamic nature of inference workloads. 
In particular, eliminating cold starts requires the platform to provision excessive GPU resources to accommodate peak loads (e.g., spikes in Fig.~\ref{fig:serverless_trace}), which in turn results in substantial GPU idling during periods of low demand. Consequently, the extremely low GPU utilization directly contradicts the pay-per-use model fundamental to serverless computing\cite{faaswap}.  
%Therefore, this approach results in extremely low GPU utilization, and contradicts the pay-per-use model of serverless computing~\cite{faaswap}.

\PHM{Solution\#3: Caching models in host memory and SSDs.}
Recent studies propose to cache models in host memory and then load them into GPU upon request arrivals~\cite{bai_pipeswitch_nodate,deepplan_2023,fu_serverlessllm_2024,faaswap}. 
% For instance, FaaSwap supports dynamic model swapping between host and GPU memory, and ServerlessLLM provides fast model loading from SSD to GPU.
While this approach reduces the reservation cost compared to GPU overprovisioning, it still fails to achieve fast scaling in large-scale, multi-tenant serverless GPU clusters due to three key factors.
% To facilitate local model loading, this approach requires each node to maintain replicas for many models, if not all, which becomes impractical due to three reasons.
1) The dynamic and bursty nature of inference workloads often requires concurrent executions of a model on many nodes, sometimes involving the entire cluster. 
2) Large-scale platforms typically host thousands of large models, resulting in massive host memory consumption. 
%immense volumes of model data;
3) Each node has limited host memory (e.g., up to 100s of GBs), which is often insufficient to accommodate even a few large models. 
Consequently, excessive host memory model caching becomes impractical, hindering the ability to achieve efficient model scaling. 
%caching many model replicas at each node's memory is impractical, thus failing to achieve efficient model scaling. 
% Consequently, caching many model replicas in each node becomes infeasible, inevitably leading to frequent remote loading even when efficient model placement and scheduling policies are employed (detailed results in xxx).

% \PHM{Solution\#4: Storing models in local SSDs.}
Compared to host memory, SSDs offer larger capacities and can store more models~\cite{fu_serverlessllm_2024}.
However, loading models from SSDs to GPUs remains time-consuming due to the limited SSD bandwidth. 
Our measurements (see testbed in \S\ref{sec:evaluation:setup}) show that loading a Llama-70B model from an SSD to a GPU takes over 30 seconds even with optimized implementations --- an order of magnitude slower than loading from host memory. 
This delayed loading hampers the ability of serverless inference platforms to scale models quickly on demand. 
% The high loading latency makes this approach challenging to achieve fast model scaling in serverless inference platforms.

\begin{figure}[t]
    \centering
    \begin{minipage}{0.225\textwidth}
        \centering
        \includegraphics[scale=0.125]{memory_alive_time_cdf.pdf} 
        \vspace{-1em}
        \caption{Distribution of models' keep-alive time in memory.}  
        \label{fig:memory_alive_cdf}
    \end{minipage}\hfill
    \begin{minipage}{0.225\textwidth}
        \centering
        \includegraphics[scale=0.125]{load_prop.pdf}
        \vspace{-1em}
        \caption{Proportion of the 3 types of model loading.}
        \label{fig:load_prop}
    \end{minipage}
    \vspace{-2em}
\end{figure}


To illustrate the inefficiency of this approach, we conduct two simulations based on real-world workload traces.

\PHM{Short model keep-alive time in memory.} 
We first examine how long a model instance remains in memory before being evicted in a multi-tenant inference platform. 
%We configure each node's memory capacity to 3 models while storing 12 models on SSD, basing on the model size of Llama-70B and hardware configurations in our testbed (see xxx).
Each node's memory is configured to hold up to 3 models, while 12 models are stored in SSDs. This configuration is based on the size of Llama-70B and the hardware specifications of our testbed (see \S\ref{sec:evaluation:setup}). 
We set each model's per-node request rate to 1 per minute, which reflects a typical request pattern in production serverless inference platforms~\cite{faaswap}. 
We use the LRU eviction policy and depict the distribution of model's keep-alive times in Fig.~\ref{fig:memory_alive_cdf}.
As we can see, models are frequently reloaded and evicted from memory, with over 95\% of them staying in memory for fewer than 15 seconds before being evicted. 

\PHM{High cache miss ratio.}
Next, we measure the cache miss ratio for models cached in memory 
%when models are stored in memory 
by replaying the two traces shown in Fig.~\ref{fig:serverless_trace}.
Based on the findings of Fig.~\ref{fig:memory_alive_cdf}, we set models' keep-alive time to 15 seconds---the tail of the distribution.
Fig.~\ref{fig:load_prop} shows the proportion of three loading cases across the two traces: model load from memory, model load from SSD, and a hot start (w/o load). 
We observe that SSD loads (i.e., cache misses) account for 64\% and 36\% across the two traces, respectively.
This indicates that relying on memory caching alone is inadequate, resulting in a significant portion of slow model loads from SSDs or even remote storage, which severely impacts scalability and user experience. 

%   host model loading under real-world dynamic workloads (Fig.xx).



In summary, existing solutions either suffer from long startup times or incur large GPU and memory costs in order to keep models active, forcing a rigid tradeoff between scaling efficiency and resource costs. 
% This hard trade-off between scaling performance and resource cost hinders the ability of serverless inference platforms to achieve efficient model scaling.
%This problem constrains the platform from delivering fast model scaling, a key requirement of serverless inference.
This inherent limitation prevents the platform from achieving fast model scaling, a critical requirement for serverless inference.  




% However, both \Cloud and other leading serverless platforms currently lack
% efficient support for GPUs, impeding their ability to achieve
% high-performance inference. In fact, numerous \Cloud users have expressed a
% compelling need to execute their models on GPU-enabled functions, indicating
% the strong market demand for GPU-accelerated inference in current FaaS
% platforms. 

\subsection{Key Insights and Challenges}
\label{sec:challenges}

\PHM{Key insights.}
% As discussed in \S\ref{sec:background-limitations}, current serverless inference platforms lack fast model scaling capability. 
%effective mechanisms for model scaling. 
%Ideally, such platforms should rapidly scale model-serving instances to handle fluctuating workloads without incurring additional storage overhead. 
To achieve fast model scaling, we present three key observations. 
\textbf{First}, modern GPU clusters increasingly adopt high-speed interconnects between GPU nodes (e.g., 100-400Gbps with RDMA capability)~\cite{acme_nsdi24, kundu_llm_analysis_arxiv24, azure_ai_cluster_url} and support advanced data transfer techniques such as GPUDirect RDMA or GDR~\cite{gdr}. 
% By treating the cluster as a unified GPU pool, the high-speed interconnects can be leveraged to load models directly from within pool, bypassing the need to fetch them from remote storage or node-local SSDs. 
%we can exploit the high network bandwidth to load models directly from the pool, eliminating the need to fetch them from remote storage or local SSDs.
\textbf{Second}, multicast-based collective communication techniques are inherently well-suited for cross-node model scaling, enabling rapid distribution of model instances across receiving nodes. 
\textbf{Third}, models can begin computation before being fully loaded, enabling an ``execute-while-load'' approach. This allows inference tasks to be distributed and executed across multiple nodes in parallel during model loading. 

Following these observations, we advocate that a scalable serverless inference platform should synergize the ``execute-while-load'' approach with efficient cross-node model loading. 
Specifically, models should be rapidly transferred and replicated across worker nodes, while new worker nodes perform collaborative, distributed inference execution. 
This approach allows the platform to handle load spikes on demand, maintaining low request queueing delays without the need to keep models long-lived in GPUs or host memory.  
%, achieving rapid scaling without the need to keep models persistently active in GPUs or memory.

% Building on these insights, we propose \SysName, a scalable serverless inference platform designed for fast and cost-efficient model scaling. \SysName adopts the ``execute-while-load'' approach: models are rapidly replicated and transmitted to worker nodes using optimized multicast communication, while new worker nodes form execution pipelines to collaboratively serve requests immediately. This design enables \SysName to adapt to sudden workload spikes on demand, achieving rapid scaling without the need to keep models persistently active in GPUs and memory, thereby minimizing costs.


\PHM{Challenges.}
Realizing the idea above presents three key challenges.
\textbf{\emph{C1: Low-Latency Model Loading.}}
The platform must handle varying scaling demands while consistently achieving high scaling performance.
Hence achieving efficient, low-latency model loading across nodes remains a challenge.
\textbf{\emph{C2: Dynamic Model Execution.}}
Unlike existing distributed inference solutions designed for static environments~\cite{zhang_shepherd_nodate,li_alpaserve_2023}, the ``execute-while-load'' approach requires collaborative, distributed inference among nodes when the model is being loaded.
This introduces a challenge for the platform to dynamically configure and optimize inference execution plans at runtime.
\textbf{\emph{C3: Efficient Model Management across Storage Tiers.}}
Maintaining models in host memory is a prevalent practice to accelerate model loading~\cite{faaswap,fu_serverlessllm_2024,bai_pipeswitch_nodate}.
Therefore, the platform should efficiently manage models across various storage tiers, including GPU memory and host memory, while delivering fast model scaling.
% As models can reside in multiple storage tiers (e.g., GPU and host memory), the platform should efficiently manage and scale models based on their locality, improving the efficiency of end-to-end model scaling.

In the following sections, we present \SysName, a scalable serverless inference platform to address the aforementioned challenges.
% \SysName incorporates both algorithmic and system-level innovations and efficiently enables the ``execute-while-load'' paradigm. 
We provide an overview of \SysName in \S\ref{sec:overview} and delve into its detailed designs in \S\ref{sec:algo} and \S\ref{sec:system}.


