\section{Conclusion}
\label{sec:conclusion}

This paper presents \SysName, a serverless inference system that achieves fast model scaling.
\SysName leverages high-speed RDMA networks for fast model multicast across GPU nodes and enables cross-node, collaborative inference execution during model transfer. 
\SysName realizes this ``execute-while-load'' approach through \AlgoName, a novel model scaling scheme that supports low-latency model loading and dynamic, pipelined inference execution. 
Combined with efficient model management, \SysName delivers superior scaling performance, effectively sustaining load spikes and achieving up to 5$\times$ tail-latency speedup over state-of-the-art systems. 
%
% \emph{This work does not raise any ethical issues.}
