\section{Introduction}
Given a set of text chunks $\Tset$, Graph-based Retrieval-Augmented Generation (\graphrag)~\cite{nebularag,edge2024local} enhances generative model inference by structuring $\Tset$ into a Text-Attributed Graph (\textgraph{}) $\G$ and retrieving relevant information from it.
Compared to \textrag~\cite{lewis2020retrieval}, which retrieves independent text chunks from $\Tset$, \graphrag captures relationships within and across text snippets to enhance multi-hop reasoning~\cite{peng2024graph,delile2024graph,jin2024graph}. \graphrag has gained widespread adoption across domains such as e-commerce~\cite{wang2022rete,xu2024retrieval}, biomedical research~\cite{delile2024graph,li2024dalk}, healthcare~\cite{chen24rarebench}, political science~\cite{mou2024unifying}, legal applications~\cite{colombo2024leveraging,kalra2024hypa}, and many others~\cite{alhanahnah2024depesrag,peng2024connecting}.

Some studies~\cite{li2024graph,wang2024knowledge} instantiate \textgraph{} $\G$ as a K-nearest-neighbor (KNN) graph, where nodes represent text chunks in $\Tset$, and edges encode semantic similarity or relevance. This approach maintains a low graph construction cost comparable to \textrag. However, it fails to capture entities and their relationships within text chunks, limiting retrieval effectiveness and degrading the quality of generated answers. 
To address this limitation, recent studies~\cite{li2024dalk,delile2024graph,edge2024local,gutierrez2024hipporag} have turned to triplet-based knowledge graphs, leveraging Large Language Models (LLMs) to extract structured (entity, relation, entity) triplets from text. This approach, known as \kgrag, enbales the LLM to filter out noise in raw documents and construct a more structured and interpretable knowledge base, significantly improving retrieval and generation quality. As a result, it has gained significant traction by major companies, including Microsoft~\cite{edge2024local}, Ant Group~\cite{antgrouprag}, Neo4j~\cite{neo4jrag}, and NebulaGraph~\cite{nebularag}.
However, \kgrag comes with a high indexing cost, particularly for large datasets. Even with the cost-efficient GPT-4o-mini API, processing a 3.2MB sample of the HotpotQA dataset~\cite{yang2018hotpotqa} costs \$21. 
In real-world applications, textual data often spans gigabytes to terabytes, making indexing costs prohibitively expensive. For instance, processing a single 5GB legal case~\cite{arnold2022ediscovery} incurs an estimated \$33K, posing a significant challenge for large-scale adoption.

To improve retrieval and generation quality while lowering indexing costs, we propose \sketrag, a cost-efficient multi-granular indexing framework for \graphrag. It comprises two key components: a knowledge graph skeleton and a text-keyword bipartite graph.
Instead of fully materializing the knowledge graph, \sketrag first identifies a set of core text chunks from $\Tset$ based on their PageRank centralities~\cite{page1999pagerank} in an intermediate KNN graph. It then constructs a skeleton of the complete knowledge graph using \kgrag described above. To prevent information loss from relying solely on this skeleton, \sketrag also builds a text-keyword bipartite from $\Tset$, serving as a lightweight alternative to \kgrag. By linking keywords to the text chunks in which they appear, keywords and their neighboring text chunks can be regarded as candidate entities and corresponding relations in the knowledge graph. During retrieval, \sketrag adopts the local search strategy of existing solutions but, unlike previous methods, extracts ego networks from both entity and keyword channels to facilitate LLM-based generation.

In experiments, we evaluate eight solutions on two datasets across three key aspects: indexing cost, retrieval quality, and generation quality. Notably, \sketrag achieves retrieval quality comparable to or better than Microsoft's \graphrag~\cite{edge2024local}, the state-of-the-art \kgrag solution, while reducing indexing costs by over an order of magnitude. At the same time, it improves generation quality by up to 32.4\% while lowering indexing costs by approximately 20\%.
Furthermore, the core components of \sketrag, \skeletonrag and \keyrag, also function as effective stand-alone RAG solutions, balancing efficiency and quality. In particular, \skeletonrag reduces indexing costs by 20\% while maintaining retrieval quality, showing only minor performance drops in low-cost settings and achieving parity or even slight improvements in high-accuracy configurations. Meanwhile, \keyrag consistently outperforms the vanilla \textrag in both retrieval and generation quality, achieving up to 92.4\%, 133.3\%, and 118.5\% relative improvements in Coverage, EM, and F1 scores, respectively.

To summarize, we make the following contributions in this work:
\begin{itemize}[topsep=2pt,itemsep=1pt,parsep=0pt,partopsep=0pt,leftmargin=11pt]
    \item We propose \sketrag, a cost-efficient multi-granular indexing framework for \graphrag, integrating two complementary components to balance indexing cost and result quality.
    \item We introduce \skeletonrag, which constructs a knowledge graph skeleton by selecting core text chunks and leveraging LLMs to extract structured knowledge.
    \item We develop \keyrag, a lightweight text-keyword bipartite graph that mimics the retrieval paradigm of \kgrag while significantly reducing indexing costs.
    \item We conduct extensive experiments demonstrating the improvements of our proposed solutions.
\end{itemize}



% Similarly, a 250K-patient hospital generates 1TB of text annually, leading to an estimated \$6.5M per year in processing costs~\cite{halamka2011ehr}.


% \item Complementary results: Ref.\cite{delile2024graph} and Ref.\cite{sarmah2024hybridrag} finds that combining the results of TextRAG and \graphrag can enhance the overall performance. 

