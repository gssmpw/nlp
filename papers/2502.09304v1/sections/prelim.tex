\section{Preliminaries}
We first define the key terminologies and notations in Section~\ref{sec:notations}, and present the objective of this work in Section~\ref{sec:rag}. Finally, we review the state-of-the-art Microsoft's \graphrag in Section~\ref{sec:ms-graphrag}.



\subsection{Terminologies and Notations}\label{sec:notations}
Let $\Tset$ denote a set of text chunks, which is preprocessed from a set of documents. For simplicity, we assume that each text chunk $t_i \in \Tset$ is partitioned into chunks of equal length, denoted by $\clen$ tokens. The text embedding of each chunk $t_i$ is denoted as $\temb{t_i}$.
We define a \textit{text-attributed graph} (\textgraph) index as $\G = (\V, \E)$, where $\V$ and $\E$ are the sets of nodes and edges, respectively. 
In $\V$, each node is represented as $v_i=(t_i,\temb{t_i})$, where $t_i$ is the textual attribute and $\temb{t_i}$ is its text embedding. Analogously, each edge $e_{i,j}\in \E$ between nodes $v_i$ and $v_j$ is denoted as $e_{i,j}=(v_i, v_j, t_{i,j},\temb{t_{i,j}})$ if it is text-attributed; otherwise, it is simply $e_{i,j} = (v_i, v_j)$. In addition, we use calligraphic uppercase letters (e.g., $\mathcal{\Sset}$) to denote sets of nodes or edges. For text information, $\temb{\cdot}$ represents the text embedding function, the function $\bigoplus(\Sset)$ represents the concatenation of all text in $\Sset$, and $\left|\bigoplus(\Sset)\right|$ denotes the token count of the concatenated string.
% In $\Vtext$ and $\Etext$, the textual information of node $v_i$ and edge $e_{i,j}$ is represented as $x_i$ and $r_{i,j}$, respectively. The corresponding embeddings of $x_i$ and $r_{i,j}$ are denoted as $\xemb_i$ and $\remb_{i,j}$.
% In this work, we denote vectors in bold lowercase, \eg the text embedding $\temb_i$ of $t_i$.
The frequently-used notations are summarized in Table~\ref{tab:notation}.

\begin{table}[!t]
\centering
\renewcommand{\arraystretch}{1.1}
\begin{small}
\caption{Frequently used notations.}\vspace{-2.4mm} \label{tab:notation}
% \resizebox{\columnwidth}{!}{
\begin{tabular}{rp{2.2in}}	
    \toprule
    \bf Notation & \bf Description \\
    \midrule
    $\Tset, t_i, \clen$ & Text chunk set $\Tset$ with each chunk $t_i$ in length $\clen$. \\
    $\temb{t_{i}}$ & The text embedding of text $t_i$.\\
    $\G = (\V, \E)$   &  Node set $\V$ and edge set $\E$ in \textgraph{} index $\G$. \\
    % $v_i=(t_i,\temb{t_i})$   &  $v_i\in \V$ with text $t_i$ and embedding $\temb{t_i}$. \\
    $\bigoplus(\Sset), \left|\bigoplus(\Sset)\right|$   &  the concatenated texts from $\Sset$ and its token count. \\
    $K, \beta, \tau$   &  the integer $K$ of KNN graph, the budget ratio $\beta$, and the number of splits $\tau$. \\
    $\lambda, \theta$   &  the context limit $\lambda$ and the retrieval ratio $\theta$. \\
    \bottomrule
\end{tabular}
% }
\end{small}
% \vspace{-2.1mm}
\end{table}

\subsection{Problem Formulation}\label{sec:rag}
The Retrieval-Augmented Generation (RAG) framework consists of three main stages: indexing, retrieving, and generation. 

Given a set of text chunks $\Tset$, the indexing stage of \textrag~\cite{lewis2020retrieval} generates a text embedding $\temb{t_i}$ for each $t_i \in \Tset$. During the retrieval stage, \textrag computes the query embedding $\temb{q}$ and retrieves text chunks from $\Tset$ that are most similar to the query in the embedding space. Finally, the retrieved text chunks are incorporated into a predefined prompt for a large language model (LLM) to generate the final response.
In contrast, \graphrag constructs a \textgraph{} index $\G = (\V, \E)$ from $\Tset$ during indexing. Existing methods primarily build two types of $\G$: (i) a K-Nearest-Neighbors (KNN) graph (\knnrag), where each text chunk is a node, and edges represent similarity-based connections~\cite{li2024graph,wang2024knowledge}; or (ii) a knowledge graph (\kgrag), where LLMs extract (entity, relation, entity) triplets from text~\cite{li2024dalk,delile2024graph,edge2024local,gutierrez2024hipporag}.
During retrieval, \graphrag computes $\temb{q}$ and identifies \textit{seed nodes} in $\G$ that have the most similar text embeddings to the query. A subgraph is then extracted via local search, serialized into natural language, and incorporated into a predefined prompt for LLM-based response generation.


\stitle{Our objective} 
This work aims to develop a cost-efficient and effective approach for \graphrag to construct a \textgraph{} index $\G$ from $\Tset$, achieving lower indexing costs yet higher result accuracy in the widely adopted local search scenario~\cite{li2024dalk,delile2024graph,edge2024local,gutierrez2024hipporag}.


\subsection{Microsoft's \graphrag}\label{sec:ms-graphrag}
Microsoft proposed the \graphrag system~\cite{edge2024local}, which constructs a knowledge graph index with multi-level communities and employs tailored strategies for both local and global search. In this section, we focus on its indexing and retrieval operations for local search, which are relevant to our work.


Algorithm~\ref{alg:graphrag-index} outlines the pseudo-code for constructing the graph index $\G = (\V_e \cup \V_t, \E)$, where $\V_e$ and $\V_t$ represent entities and text chunks, respectively. Given a text chunk set $\Tset$, \graphindex first generates a text embedding for each $t_i \in \Tset$ and treats the corresponding text snippets as text-attributed nodes, forming the node set $\V_t$ (Line 1).  
For each node $v_i \in \V_t$, \graphindex leverages a predefined LLM to process $t_i$ in two steps: \textit{entity identification} and \textit{relationship extraction}, obtaining the sets of entities and relations ($\V_i$ and $\E_i$) (Line 3). These extracted entities and relationships are then added to $\V_e$ and $\E$, respectively (Line 4). 
For each extracted entity or relationship $x \in (\V_i \cup \E_i)$, the LLM generates a textual description $t_x$ along with its embedding $\temb{t_x}$. Additionally, \graphindex links each text chunk node $v_i$ to its corresponding extracted entities and relationships (Line 5).


\begin{algorithm}[!t]
\KwIn{The text chunk set $\Tset$.}
\KwOut{A \textgraph{} index $\G$.}
$\V_t \gets \{(t_i,\temb{t_i})\mid t_i\in \Tset\}$; ~ $\V_e \gets \emptyset; ~ \E \gets \emptyset$\;
\For{each $v_i= (t_i, \temb{t_i}) \in \V_t$}{
$\V_i,~\E_i \gets$ entities and edges extracted by LLM from $t_i$\;
$\V_e \gets \V_e \cup \V_i$;~$\E \gets \E \cup \E_i$\;
$\E \gets \E \cup \{(v_i,x) \mid x \in (\V_i \cup \E_i)\}$\;
}
\Return{$(\V_e\cup\V_t, \E)$;}
\caption{\graphindex $(\Tset)$}
\label{alg:graphrag-index}
\end{algorithm}


The local search procedure retrieves context from entities, relationships, and text chunks. Algorithm~\ref{alg:graphrag-retrieval} outlines the retrieval process, following the default context limit ratio across channels.  
Given a graph index $\G$ and a context limit $\lambda$, \graphretrieval retrieves contexts in the order of seed entities, relationships, and text chunks. Specifically, it first retrieves 10 entity nodes ($\Sset_e$) with embeddings most similar to the query embedding based on Euclidean distance (Line 1).  It then retrieves relationships ($\Sset_r$) until the combined token count from $\Sset_e\cup \Sset_r$ reaches $\lambda/2$ (Line 2). Relationships are prioritized based on adjacency to $\Sset_e$, with those connecting two seed entities ranking higher.  
For text chunk retrieval, \graphretrieval retrieves text chunks most adjacent to $\Sset_e$ and $\Sset_r$ until the total context length reaches $\lambda$ (Line 3). At last, the retrieved texts from $\Sset_e \cup \Sset_r \cup \Sset_t$ are concatenated to form the final context, which is returned as input for generation (Line 4).


