
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ketrag.pdf}
    \vspace{-9mm}
    \caption{The illustration of \sketrag: the indexing stage in \textcircled{1}-\textcircled{3} and the retrieval stage in \textcircled{4}-\textcircled{5}.}
    \label{fig:sketrag-overview}
    \vspace{-2mm}
\end{figure*}

\section{Proposed Framework: \sketrag}
To fully leverage the strengths of existing graph indices while addressing their limitations, we introduce \sketrag, an indexing framework that integrates multiple levels of granularity: \underline{K}eywords, \underline{E}ntities, and \underline{T}ext chunks, into the \textgraph{} index $\G$. The overall workflow of \sketrag is illustrated in Figure~\ref{fig:sketrag-overview}.

\subsection{Overview}
At a high level, the \textgraph{} index $\G = \G_s \cup \G_k$ consists of: (i) a knowledge graph \textit{skeleton} $\G_s$, derived from a selected set of important text chunks called \textit{core chunks}, and (ii) a text-keyword bipartite graph $\G_k$, constructed from all chunks. As shown in Figure~\ref{fig:sketrag-overview}, the construction process involves three main steps.
\begin{enumerate}
[topsep=2pt,itemsep=1pt,parsep=0pt,partopsep=0pt,leftmargin=15pt]
    \item \sketrag first organizes the input text chunks in $\Tset$ into a KNN graph, where chunks are linked if they exhibit sufficient lexical or semantic similarity. This serves as an intermediate structure for building the final graph $\G$. 
    \item Next, \sketrag selects a $\beta$ fraction of \textit{core chunks} according to their structural importance in the KNN graph. These core chunks are then processed using \graphindex (Algorithm~\ref{alg:graphrag-index}) to produce a knowledge graph skeleton $\G_s$. 
    \item Finally, \sketrag constructs the bipartite graph $\G_k = (\V_k \cup \V_t, \E_k)$ from $\Tset$. In $\G_k$, the node set $\V_k$ represents keywords, and $\V_t$ represents text chunks. An edge $e_{i,j} \in \E_k$ indicates that keyword node $v_i$ appears in text chunk node $v_j$. Each keyword node $v_i$ is assigned a description $t_i$ (consisting of all sentences containing that keyword), and its embedding $\temb{t_i}$ is computed as the average of these sentences' embeddings.
\end{enumerate}
During retrieval, \sketrag balances information from $\G_s$ and $\G_k$ using a constant $\theta$. It first identifies a set of \textit{seed nodes}, either entities or keywords, that are most similar to the query $q$ in the text embedding space. For entity seeds, \sketrag applies Algorithm~\ref{alg:graphrag-retrieval} to retrieve context using $\theta$ proportion of the total context limit $\lambda$. For keyword seeds, it follows a similar procedure to collect relevant neighboring text chunks using the remaining $(1-\theta)$ of the context budget. Finally, the retrieved context is combined with a predefined prompt and passed to the LLM for response generation.

\input{figures/degree-distribution}

\subsection{Rationale and Comparison}\label{sec:ket-rationale}

\sketrag is motivated by two key observations. First, a small subset of core text chunks often exhibits broad relevance to others. Figure~\ref{fig:degree-distribution} presents the degree distribution of the KNN graph constructed from the MuSiQue dataset with input chunk sizes $\clen = 1200$ and $\clen = 150$. This heavily skewed distribution highlights the importance of core chunks in linking different parts of the graph. Consequently, these core chunks should be prioritized to extract high-quality triplets using the LLM.
Second, in the lightweight alternative graph $\G_k$, keywords and their neighboring text chunks can serve as stand-ins for entities and their ego networks. Specifically, when seed keywords align with seed entities, their neighboring text chunks are expected to contain information about those entities' ego networks. Hence, these neighboring chunks are treated as candidates, and retrieval follows the standard \textrag strategy.

To summarize, compared to previous \kgrag solutions~\cite{li2024dalk,delile2024graph,edge2024local,gutierrez2024hipporag}, \sketrag focuses on a smaller set of core chunks to construct a knowledge graph skeleton while leveraging a text-keyword bipartite graph as a lightweight alternative. This design lowers the cost of LLM inference and improves result quality via two distinct retrieval channels (entity and keyword). Additionally, in the keyword channel, \sketrag confines the retrieval to snippets containing seed keywords, unlike \textrag, which searches across the entire $\Tset$. This subgraph-based approach better captures in-text relationships w.r.t.\ seed keywords, enhancing overall effectiveness.

