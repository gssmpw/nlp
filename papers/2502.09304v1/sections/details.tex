\section{Detailed Implementations}\label{sec:details}
This section provides a detailed explanation of \sketrag, with the indexing stage \sketindex discussed in Section~\ref{sec:graph-construct} and the retrieval process \sketretrieval described in Section~\ref{sec:subgraph-retrieval}.


% \begin{small}
\begin{algorithm}[!t]
\KwIn{The text chunk set $\Tset$, an integer $K$, a budget rate $\beta$, the number of splits $\tau$.}
\KwOut{A \textgraph{} index $\G$.}

$\W \gets$ all keywords tokenized from $\Tset$\;

% \Comment{KNN graph initialization}
$\V \gets \{v_i = (t_i, \temb{t_i}) ~|~ t_i \in \Tset\}; ~ \E \gets \emptyset$\;
\For{each $v_i \in \V$}{
    $\Sset_1 \gets \argmaxk{v_j\in \V \setminus \{v_i\}} \text{co-occ}(v_i,v_j)$, where $k=K/2$\;
    $\Sset_2 \gets \argmaxk{v_j\in \V \setminus (\Sset_1 \cup \{v_i\})} \text{cos}(v_i,v_j)$, where $k=K/2$\;
    $\E \gets \E \cup \{(v_i, v_j) ~|~ v_j \in (\Sset_1 \cup \Sset_2)\}$;
}
$\G \gets (\V, \E)$\;

% \Comment{Core chunks Identification}
$\V_c \gets \argmaxk{v_i\in \V} \pi_i$ in Eq.~\eqref{eq:pr}, where $k=\lceil \beta \cdot |\V| \rceil$\;

% \Comment{Graph Index Construction}
$\G_s \gets $ \graphindex ($\V_c$) in Algorithm~\ref{alg:graphrag-index}\;
$\Tset_{\tau} \gets$ split each $t_i \in \Tset$ into $2^{\tau}$ equal-sized sub-chunks\;
$\V_t \gets \{v_i = (t_i, \temb{t_i}) ~|~ t_i \in \Tset_{\tau}\}; ~ \V_k \gets \emptyset; ~ \E_k \gets \emptyset$\;

\For{each keyword $x \in \W$}{
    $\V_k \gets \V_k \cup \{v_j = (t_j, \temb{t_j})\}$, where $t_j$ is all sentences in $\Tset_{\tau}$ containing $x$ and $\temb{t_j}$ is the average embedding\;
    $\E_k \gets \E_k \cup \{(v_i, v_j) \mid v_i \in \V_t,\ v_j \in \V_k,\ t_i \text{ contains } x\}$\;
}
$\G_k \gets (\V_k \cup \V_t, \E_k)$\;
\Return{$\G_s\cup \G_k$;}
\caption{\sketindex$(\Tset, K, \beta, \tau)$}
\label{alg:sketrag-index}
\end{algorithm}
% \end{small}

\subsection{\sketindex}\label{sec:graph-construct}

As outlined in Algorithm~\ref{alg:sketrag-index}, \sketindex takes as input a set $\Tset$ of text chunks, an integer $K$ for the KNN graph, a budget rate $\beta$, and an integer $\tau$ for text chunk splitting. It first tokenizes all text chunks in $\Tset$ into vocabulary $\W$. By default, it tokenizes chunks into words while excluding stop words (\eg `the', `a', and `is') to define keyword nodes, though traditional keyword extraction methods~\cite{ramos2003using} can also be applied. \sketindex then executes three core subroutines.

\stitle{KNN Graph Initialization}
In Lines 2–7, \sketindex represents each text chunk $t_i \in \Tset$ with its embedding $\temb{t_i}$ as a node $v_i \in \V$. It then links each node $v_i$ to the top-$K/2$ nodes based on lexical similarity and the top-$K/2$ nodes based on semantic similarity, forming the KNN graph $\G$ (Lines 3–6). Specifically, the lexical similarity between nodes $v_i, v_j$ is defined as the number of co-occurring keywords in $\W$, while the semantic similarity is measured using the cosine similarity between their embeddings $\temb{t_i}$ and $\temb{t_j}$.

\stitle{core chunk identification}
Motivated by the observations in Figure~\ref{fig:degree-distribution}, given an intermediate KNN graph $\G = (\V, \E)$ and a budget rate $\beta \in [0,1]$, Line 8 of Algorithm~\ref{alg:sketrag-index} selects a set $\V_c$ of $\lceil \beta\cdot|\V|\rceil$ core chunk nodes based on their structural importance.
For each node $v_i \in \V$, the PageRank value~\cite{page1999pagerank} $\pi_i$ serves as a measure of structural importance. Let $\mathbf{P}$ be the probability transition matrix of $\G$, where $\mathbf{P}_{i,j} = \frac{1}{d(v_i)}$ and $d(v_i)$ denotes the degree of $v_i$. Given a teleport probability $\alpha$, the PageRank vector $\boldsymbol{\pi}$ is computed as:
\begin{equation}\label{eq:pr}
\boldsymbol{\pi} = \alpha\cdot\mathbf{1/n} + (1-\alpha)\cdot \boldsymbol{\pi} \cdot\mathbf{P},
\end{equation}  
where $\mathbf{1/n}$ is the initial vector with each of $n = |\V|$ dimensions set to $1/n$, and $\pi_i = \boldsymbol{\pi}[i]$ represents the PageRank score of $v_i \in \V$. PageRank effectively captures both direct and higher-order structural importance, making it suitable for identifying core chunks.

\stitle{Graph Index Construction}
In Line 9, \sketindex processes the selected core text chunks $\V_c$ using \graphindex (Algorithm~\ref{alg:graphrag-index}) to construct the knowledge graph skeleton $\G_s$. Since the text-attributed node set $\V_c$ has already been built, Line 1 in Algorithm~\ref{alg:graphrag-index} is skipped.
Next, in Lines 10--15, \sketindex constructs a text-keyword bipartite graph $\G_k = (\V_k \cup \V_t, \E_k)$ based on $\Tset$. Specifically, each text chunk in $\Tset$ is recursively divided into equal-sized sub-chunks over $\tau$ iterations, forming a set $\Tset_{\tau}$ with $2^{\tau}\cdot|\Tset|$ sub-chunks. Each sub-chunk is then initialized to a node in $\V_t$. 
For each keyword $x\in \W$, \sketindex creates a keyword node $v_{j}=(t_j,\temb{t_j})$, where $t_j$ concatenates all sentences in $\Tset$ containing $x$, and $\temb{t_j}$ is their average embedding. This process aggregates information from different contexts, reducing noise and generating a more generalized representation of the keyword. Finally, \sketindex links each chunk node $v_i \in \V_t$ to its corresponding keyword node $v_j$.
After constructing $\G_k$, \sketindex returns $\G_s \cup \G_k$ as the final index $\G$, where text chunk nodes in $\G_s$ are replaced by their corresponding partitioned sub-chunk nodes in $\V_t$ of $\G_k$, with edges in $\G_s$ rewired accordingly.  




\subsection{\sketretrieval}\label{sec:subgraph-retrieval}
As outlined in Algorithm~\ref{alg:sketrag-retrieval}, \sketretrieval takes as input a \textgraph{} index $\G = \G_s \cup \G_k$, a query $q$, a context length limit $\lambda$, and a retrieval ratio $\theta$. It outputs a context $C$ by selecting the most relevant content from the skeleton graph $\G_s$ and the keyword-text bipartite graph $\G_k$.
In Line 1, \sketretrieval invokes \graphretrieval (Algorithm~\ref{alg:graphrag-retrieval}) to retrieve context $C_s$ from $\G_s$, using $\theta \cdot \lambda$ tokens. Next, in Lines 2–4, it retrieves content from $\G_k$ using the remaining $(1-\theta) \cdot \lambda$ tokens.
Specifically, \sketretrieval iteratively selects seed keyword nodes $\Sset_k$ from $\V_k$ based on cosine similarity to the query embedding, expanding the selection until their neighboring text sub-chunks contain $2 \cdot (1-\theta) \cdot \lambda$ tokens, \ie $\left|\bigoplus\left(\N\left(\Sset_k\right)\right)\right| = 2\cdot(1-\theta)\cdot\lambda$.
Focusing on the candidate set $\N\left(\Sset_k\right)$, \sketretrieval retrieves the final text set $\Sset_t$ based on the cosine similarity. These texts are concatenated into context $C_k$ with $(1-\theta) \cdot \lambda$ tokens. Finally, it returns $C_s\oplus C_k$ as the retrieved context for \sketrag.

Notably, all chunks retrieved by \sketretrieval, whether from entity or keyword channels, are fine-grained sub-chunks generated during the indexing stage through spitting and rewiring. This refinement reduces noise and preserves the context limit, allowing for the retrieval of more relevant knowledge during online queries.


% \begin{small}
\begin{algorithm}[!t]
\KwIn{A \textgraph{} index $\G = \G_s \cup \G_k$, a query $q$, context length limit $\lambda$, retrieval ratio $\theta$}
\KwOut{The context $C$}
$C_{s} \gets \graphretrieval \left(\G_s, \theta \cdot \lambda\right)$ in Algorithm~\ref{alg:graphrag-retrieval}\;

$\Sset_k \gets \argmaxk{v_{i}\in \V_k} \text{cos}(v_i,q)$, s.t.\ ${\scriptstyle \left|\bigoplus\left(\N\left(\Sset_k\right)\right)\right| = 2\cdot(1-\theta)\cdot\lambda}$\;
$\Sset_t \gets \argmaxk{v_{i}\in \N(\Sset_k)} \text{cos}(v_i,q)$, s.t.\ ${\scriptstyle \left|\bigoplus(\Sset)\right| = (1-\theta)\cdot\lambda}$\;
$C_k\gets \bigoplus(\Sset)$\;
\Return{$C_s \oplus C_k$;} 
\caption{\sketretrieval$(\G, q, \lambda, \theta)$}
\label{alg:sketrag-retrieval}
\end{algorithm}
% \end{small}

\subsection{Cost Analysis}\label{sec:analysis}

We begin by analyzing the cost of \graphindex to construct $\G = (\V_e\cup\V_t, \E)$. Let $\lambda_e$ and $\lambda_r$ denote the token counts of the prompt templates used to extract entities and relationships, respectively. Each text chunk node $v_i \in \V_t$ has a text of length $\clen$. To extract entities, the LLM is prompted with $\clen + \lambda_e$ tokens for each $v_i$, resulting in $(\clen + \lambda_e)\cdot|\V_t|$ total input tokens. Similarly, extracting relationships requires $(\clen + \lambda_r)\cdot|\V_t|$ tokens. In addition, \graphindex must compute text embeddings for all nodes and edges, incurring another $\clen\cdot|\V_t|+\sum\limits_{x\in \V_e\cup\E}\ell_x$ tokens, where $\ell_x$ is the description length of $x\in \V_e\cup\E$. Therefore, the total LLM Input Token Cost (ITC) for \graphindex is:
$$\text{ITC}_{\graphindex} = \left(2+\frac{(\lambda_e+\lambda_r)}{\clen}\right)\cdot\clen\cdot|\V_t|\cdot c_i + \left(\clen\cdot|\V_t|+\sum\limits_{x\in \V_e\cup\E}\ell_x\right)\cdot c_e,$$
where $c_i$ and $c_e$ are the per-token costs for the LLM and embedding models, respectively.
By contrast, \sketindex uses only a $\beta$-fraction of the \graphindex input token cost to construct $\G_s$. To build the text-keyword bipartite graph $\G_k$, it additionally consumes $3\clen\cdot|\Tset|$ tokens for multi-granular text embeddings (chunk, sub-chunk, and sentence levels). Thus,
$$\text{ITC}_{\sketindex} = \beta\cdot \text{ITC}_{\graphindex} + 3\cdot\clen\cdot|\Tset|\cdot c_e.$$
Regarding output token costs, \sketindex incurs only a $\beta$ fraction of the output cost of \graphindex, as it generates only $\beta$ of the entities and relations present in the full knowledge graph.

Regarding the retrieval and generation stages, all solutions, including \sketrag, incur the same upper-bounded cost, as both stages are regulated by the maximum token parameter during LLM inference. Specifically, the input tokens for all solutions comprise a distinct prompt template and the retrieved content, which is constrained by the limit $\lambda$. The number of output tokens is then determined by subtracting the input token count from the maximum token limit, ensuring consistent computational costs across different approaches.


