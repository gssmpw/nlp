\begin{abstract}
\textsf{Graph-RAG} constructs a knowledge graph from text chunks to improve retrieval in Large Language Model (LLM)-based question answering. It is particularly useful in domains such as biomedicine, law, and political science, where retrieval often requires multi-hop reasoning over proprietary documents. Some existing \textsf{Graph-RAG} systems construct KNN graphs based on text chunk relevance, but this coarse-grained approach fails to capture entity relationships within texts, leading to sub-par retrieval and generation quality. To address this, recent solutions leverage LLMs to extract entities and relationships from text chunks, constructing triplet-based knowledge graphs. However, this approach incurs significant indexing costs, especially for large document collections.

To ensure a good result accuracy while reducing the indexing cost, we propose \textsf{KET-RAG}, a multi-granular indexing framework. \textsf{KET-RAG} first identifies a small set of key text chunks and leverages an LLM to construct a knowledge graph skeleton. It then builds a text-keyword bipartite graph from all text chunks, serving as a lightweight alternative to a full knowledge graph. During retrieval, \textsf{KET-RAG} searches both structures: it follows the local search strategy of existing \textsf{Graph-RAG} systems on the skeleton while mimicking this search on the bipartite graph to improve retrieval quality. We evaluate eight solutions on two real-world datasets, demonstrating that \textsf{KET-RAG} outperforms all competitors in indexing cost, retrieval effectiveness, and generation quality. Notably, it achieves comparable or superior retrieval quality to Microsoft's \textsf{Graph-RAG} while reducing indexing costs by over an order of magnitude. Additionally, it improves the generation quality by up to 32.4\% while lowering indexing costs by around 20\%.
\end{abstract}

