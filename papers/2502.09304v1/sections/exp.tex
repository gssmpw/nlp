

\section{Experiments}\label{ref:exp}
In this section, we evaluate our proposed \sketrag framework by addressing the following research questions:
\begin{itemize}[topsep=2pt,itemsep=1pt,parsep=0pt,partopsep=0pt,leftmargin=11pt]
\item \textbf{RQ1}: How does \sketrag enhance effectiveness while reducing costs compared to existing solutions?
\item \textbf{RQ2}: What is the contribution of each core subroutine to \sketrag's overall performance?
\item \textbf{RQ3}: How does \sketrag balance result quality, index construction cost, and the trade-off between its two retrieval channels?
\item \textbf{RQ4}: How sensitive is \sketrag's performance to its parameter settings?
\end{itemize}
All experiments are conducted on a Linux machine with Intel Xeon(R) Gold 6240@2.60GHz CPU and 32GB RAM\eat{ in single-thread mode}. We use the OpenAI API to access LLMs.

% \begin{table}[!t]
% \centering
% \caption{Dataset statistics.}\label{tab:dataset-stat}
% \vspace{-3mm}
% \renewcommand{\arraystretch}{1.2}
% \begin{small}
% \begin{tabular}{cccc}
% \toprule
% \textbf{Dataset}     & \textbf{\#hops} & \textbf{\#paragraphs} & \textbf{\#tokens}\\ 
% \midrule 
% MuSiQue & 2, 3, 4 & 6,761 & 751,784\\
% HotpotQA & 2 & 20,150 & 618,325\\
% \bottomrule
% \end{tabular}
% \end{small}
% \end{table}

\input{figures/overall-table}


\subsection{Experimental Settings}\label{sec:exp-set}

\stitle{Datasets and metrics}
We use two widely adopted benchmarking datasets for multi-hop QA tasks: MuSiQue~\cite{trivedi2022musique} and HotpotQA~\cite{yang2018hotpotqa}. These datasets consist of QA pairs, each accompanied by multiple paragraphs as potential relevant context. Specifically, in MuSiQue, each QA pair is associated with 2 golden paragraphs and 20 candidate paragraphs, while in HotpotQA, each pair includes 8 distracting paragraphs.
Following prior work~\cite{wang2024knowledge, gutierrez2024hipporag}, we sample 500 QA instances from each dataset. The number of question hops is ${2,3,4}$ in MuSiQue and 2 in HotpotQA. The corresponding paragraphs for all sampled instances are preprocessed into $\Tset$ and compiled as the external corpus for RAG~\cite{gutierrez2024hipporag}.
For MuSiQue and HotpotQA, the number of preprocessed paragraphs is 6,761 and 20,150, respectively, resulting in an overall token count of 751,784 and 618,325.
\eat{The number of question hops, the number of original paragraphs processed into text chunks, and the total token count are summarized in Table~\ref{tab:dataset-stat}.}For evaluation, we measure indexing cost by USD, retrieval quality by Coverage, and generation quality using Exact Match (EM) and F1-Score. Coverage is defined as the proportion of the cases that ground-truth answers found within the retrieved context across all QA instances.
To assess generation performance, we use the pre-defined LLM to generate answers by \textit{solely} based on the retrieved context and evaluate them using well-adopted EM and F1-score~\cite{gutierrez2024hipporag,wang2024knowledge,li2024graph}. EM measures the percentage of answers that exactly match the ground truth, while the F1-score quantifies partial correctness by evaluating word-level overlap between predicted and ground-truth answers, balancing precision and recall.

\stitle{Solutions and configurations}
We evaluate the performance of 8 solutions categorized as follows: (i) \textit{Existing competitors}: \textrag~\cite{lewis2020retrieval}, \knnrag~\cite{wang2024knowledge}, \kgrag~\cite{edge2024local}, \hybridrag~\cite{sarmah2024hybridrag}. (ii) \textit{Proposed baselines}: \keyrag, which constructs only $\G_k$ as the index and retrieves from it; \skeletonrag, which retains only $\G_s$. (iii) \textit{Final solutions}: \sketragu and \sketragp, where \textsf{U} and \textsf{P} represent selecting core chunks randomly and via PageRank, respectively. For a fair comparison, we implement existing solutions within the \sketrag framework, demonstrating that our approach serves as a unified and generalized RAG framework. Specifically, for \textrag and \knnrag, we use the KNN graph constructed in Algorithm~\ref{alg:sketrag-index} as the index $\G$. \sketrag reduces to \textrag when retrieving only the seed nodes in $\G$ and to \knnrag when also including their neighbors. Furthermore, \sketrag simplifies to \kgrag by setting $\beta=1$ and $\theta=1$ and to \hybridrag by combining both \textrag and \kgrag. Notably, \hybridrag fully constructs indices for both \textrag and \kgrag, retrieving content equally from both. This setup effectively corresponds to $\beta=0.5$ under a fixed context limit. 
Regarding the base models in each solution, we use OpenAI's \texttt{GPT-4o-mini} as the LLM for inference, OpenAI's \texttt{text-embedding-3-small} for text embedding generation, \texttt{cl100k\_base} for word tokenization, and the \texttt{sent\_tokenize} function from the \texttt{nltk} Python library for sentence tokenization.
We follow the default settings in~\citet{edge2024local}, setting the input chunk size $\ell$ to 1,200 and the output context limit $\lambda$ to 12,000 across all solutions. Within \sketrag, we use the default parameters for components related to \kgrag and set $K=2$, $\beta=0.8$, and $\theta=0.4$, unless specified otherwise.
The implementations of all solutions are available at {\color{blue}\url{https://github.com/waetr/KET-RAG}}.
% https://anonymous.4open.science/r/KET-RAG-FC06/

\subsection{Performance Evaluation (RQ1)}
In the first set of experiments, we evaluate the performance of \sketrag against existing competitors (\textrag, \knnrag, \kgrag, and \hybridrag) under two configurations: a low-cost version with reduced accuracy and a high-accuracy version with increased cost. Following previous works~\cite{edge2024local}, we achieve the low-cost setting by using an input chunk size of $\ell = 1,200$ and the high-accuracy setting by fixing $\ell = 150$ for all solutions. For \keyrag and \sketrag, we set $\tau$ to 3 and 0, respectively, ensuring the same text sub-chunk length in both configurations.

As reported in Table \ref{tab:quality-all}, our proposed \sketrag(\textsf{-U}/\textsf{-P}) achieves superior quality-cost trade-offs compared to existing methods on both MuSiQue and HotpotQA.
In terms of retrieval quality, \sketrag significantly outperforms all baselines, achieving the coverage score of 77.0\%/80.2\% and 81.6\%/82.6\% on MuSiQue and HotpotQA, respectively. Compared to the best competitor \hybridrag, this corresponds to relative improvements of 56.5\% and 25.9\% on MuSiQue and HotpotQA, respectively. Most notably, we observe that \sketrag in low-cost mode achieves comparable or even superior coverage to \kgrag and \hybridrag in high-accuracy mode while reducing indexing costs by over an order of magnitude. For example, on HotpotQA, the coverage scores of \sketragp, \hybridrag, and \kgrag are 81.6\%, 80.2\%, and 74.6\%, respectively, yet \sketragp incurs only 8.7\% of their indexing cost.
Akin to the retrieval quality, \sketrag achieves competitive generation quality at lower costs. Take the low-cost mode as an example. It improves \hybridrag by 34.6\%/25.2\% (resp.\ 26.5\%/26.9\%) in EM/F1 scores on MuSiQue (resp.\ HotpotQA) while reducing indexing costs by $19\%$.

Regarding other competitors, we observe that \textrag exhibits a more pronounced accuracy improvement compared to \kgrag when transitioning from low- to high-accuracy mode, which motivates the text splitting strategy in \sketrag.
Additionally, we find that, on HotpotQA, the generation quality of \kgrag is slightly lower than that of \textrag in the high-accuracy setting. This is because HotPotQA is a weaker benchmark for multi-hop reasoning due to the presence of spurious signals~\cite{gutierrez2024hipporag, trivedi2022musique}. Despite this, \sketrag consistently outperforms both competitors, demonstrating its robustness across different knowledge retrieval scenarios.


\input{figures/fig-beta}

\subsection{Ablation Study (RQ2)}
In the second set of experiments, we evaluate the performance of each single building block proposed in \sketrag, whose results are also included in Table \ref{tab:quality-all}.

\stitle{Knowledge graph skeleton}
We evaluate the performance of \skeletonrag with its full version, \kgrag. By default, \skeletonrag sets $\beta=0.8$, resulting in a 20\% reduction in indexing cost across all cases. 
Surprisingly, we find that \skeletonrag trades off only minor performance reductions, particularly in low-cost settings, while maintaining parity in high-accuracy configurations. For instance, in terms of EM score, \skeletonrag exhibits a relative decrease of 3.5\% and 7.4\% in the low-cost setting on MuSiQue and HotpotQA, respectively. However, in high-accuracy settings, there is no performance drop, and in the case of HotpotQA, even a slight improvement is observed. These results suggest that \skeletonrag effectively balances efficiency and quality, making it a viable alternative to full-scale knowledge graph indexing.

\stitle{Text-keyword bipartite graph}
We compare \keyrag with the conventional \textrag. To ensure a fair comparison, we set $\tau=0$ for \keyrag, allowing both \keyrag and \textrag to retrieve text chunks of the same size. As shown in Table~\ref{tab:quality-all}, \keyrag consistently outperforms \textrag in retrieval and generation quality. Notably, in the low-cost setting, \keyrag achieves 92.4\%/133.3\%/118.5\% and 61.8\%/52.5\%/58.8\% relative improvement in Coverage/EM/F1 on MuSiQue and HotpotQA, respectively, with more significant gains on MuSiQue. These results demonstrate the effectiveness of retrieving context from neighboring text chunks of keyword seeds, particularly in complex multi-hop reasoning.

\stitle{Core text chunk identification}
Based on the results in Table~\ref{tab:quality-all}, we observe that \sketragp shows better quality than \sketragu in both the low-cost and high-cost modes. For instance, \sketragp outperforms \sketragu by up to 1.0\%/4.5\%/4.4\% in Coverage/EM/F1 scores on MuSiQue.
This confirms the effectiveness of the core chunk identification technique, as motivated in Section~\ref{sec:ket-rationale}.
Additionally, we observe that the superiority of \sketragp remains consistent across different settings of $\beta$ and $\theta$, as further illustrated in Figures~\ref{fig:quality-beta}–\ref{fig:quality-theta}.
\eat{This is because larger text chunks in the low-cost setting reduce the accuracy of the constructed KNN graph, leading to less reliable PageRank centrality. More specifically, a larger text chunk inherently acts as an explicit clustering of the smaller consecutive ones within it. However, this ad-hoc clustering is based solely on context adjacency, disregarding lexical and semantic similarities, which affects the effectiveness of PageRank in identifying central nodes.
Additionally, we find that \sketragu and \sketragp exhibit comparable performance in most cases. The advantage of \sketragp becomes more pronounced when only a small set of core chunks are required (\ie $\beta$ and $\theta$ is small). These results are further illustrated in Figures~\ref{fig:quality-beta}-\ref{fig:quality-theta}.}

\input{figures/fig-theta}
% \input{figures/fig-k}

\subsection{Trade-off Analysis (RQ3)}
In the third set of experiments, we analyze the trade-off between accuracy and cost by varying the budget $\beta$, as well as the balance between the two retrieval channels by adjusting $\theta$. We set $\clen = 150$ and $\tau=0$, and follow the default parameter settings in Section~\ref{sec:exp-set}.

\stitle{Accuracy-cost tradeoff}
Figure~\ref{fig:quality-beta} presents the performance of \sketragu, \sketragp, and \skeletonrag on MuSiQue and HotpotQA by varying $\beta$. 
In particular, \sketragp and \sketragu consistently outperform \skeletonrag, which serves as a lower bound, across different budget $\beta$ values. 
Between the two variants, \sketragp achieves better performance than \sketragu particularly when $\beta\in [0.6, 0.8]$ in MuSiQue and $\beta\in [0.2, 0.4]$ in MuSiQue, demonstrating the effectiveness of identifying core text chunks using PageRank centralities.
Furthermore, the performance of \sketrag is less sensitive to variations in $\beta$ on HotPotQA. For instance, the coverage at $\beta=0.2$ decreases by 2\% compared to $\beta=1$. On MuSiQue, although \sketrag exhibits greater sensitivity in generation quality, its performance quickly catches up once $\beta$ reaches $0.6$. These findings demonstrate \sketrag's effectiveness for further reducing indexing costs.

\stitle{Retrieval channel}
Figure~\ref{fig:quality-theta} reports the performance of \sketragu, \sketragp, and \hybridrag by varying $\theta$.
As illustrated, \sketrag consistently outperforms \hybridrag across different $\theta$ settings, demonstrating the effectiveness of its two key components, \keyrag and \skeletonrag.
Additionally, we observe that the performance of \keyrag improves significantly when incorporating a small fraction (\eg 0.2) of context from \kgrag. This finding further motivates the reduction of costs associated with building a full knowledge graph.
Regarding the two variants, \sketragu and \sketragp, we find that \sketragp achieves superior EM and F1 scores when $\theta \leq 0.4$, which aligns with the trend observed in Figure~\ref{fig:quality-beta}.


\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.1}
\begin{small}
\caption{Answer quality by varying $\clen$ and $\tau$ on MuSiQue.}
\label{tab:quality-clen}
\begin{tabular}{lcccc|cccc}	
    \toprule
    \bf Param & \multicolumn{4}{c}{\bf $\clen$} & \multicolumn{4}{c}{\bf $\tau$} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9}
    \bf Value & \bf 150 & \bf 300 & \bf 600 & \bf 1200 & \bf 3 & \bf 2 & \bf 1 & \bf 0 \\
    \midrule
    \bf Coverage & 79.6 & 79.6 & 77.8 & 77.0 & 77.0 & 70.4 & 61.0 & 56.8 \\
    \bf EM & 19.2 & 18.8 & 15.4 & 14.0 & 14.0 & 13.8 & 11.8 & 12.8\\
    \bf F1 & 22.3 & 18.8 & 17.7 & 17.2 & 18.9 & 18.3 & 16.3 & 17.2\\
    \bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.1}
\begin{small}
\caption{Answer quality by varying $K$ on MuSiQue.}
\label{tab:quality-k}
\begin{tabular}{lccc}	
    \toprule
    \bf $K$ & \bf 2 & \bf 4 & \bf 10\\
    \midrule
    \bf Coverage/EM/F1 & 79.6/19.2/26.1 & 80.0/17.8/25.2 & 80.4/19.4/26.0\\
    \bottomrule
\end{tabular}
\end{small}
\end{table}



\subsection{Parameter Sensitivity (RQ4)}

In the fourth set of experiments, we take the Musique dataset as an example and analyze the sensitivity of \sketragp w.r.t.\ the input text chunk size $\clen$, the number of splits $\tau$, and the integer $K$ used for KNN graph construction.
For $\clen$, we vary $\clen = 150, 300, 600, 1200$ and set the corresponding $\tau = 0, 1, 2, 3$ to maintain a consistent sub-chunk length. For $\tau$, we fix $\clen = 1200$ and vary $\tau = 0, 1, 2, 3$. Additionally, we set $K = 2, 4, 10$ to explore different KNN graph densities.
As shown in Table~\ref{tab:quality-clen}, \sketragp achieves better retrieval and generation quality as the size of input chunks or split sub-chunks decreases. This trend is consistent with the performance of \keyrag and \graphrag in both low- and high-cost settings. These findings align with previous observations~\cite{edge2024local} that smaller chunk sizes improve result quality, further validating the text chunk splitting design in \sketrag. 
Additionally, as shown in Table~\ref{tab:quality-k}, varying the integer $K$ from 2 to 10 results in only minor changes in coverage, EM, and F1 scores, \eg 79.6/19.2/26.1 for $K = 2$ vs.\ 80.4/19.4/26.0 for $K = 10$, indicating that \sketrag is not significantly affected by the density of the KNN graph. This stability can be explained by Figure \ref{fig:degree-distribution}, which shows that KNN graphs with different $K$ exhibit similar degree distribution shapes, despite variations in average degree.