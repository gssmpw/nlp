\section{Related works}
\subsection{Robot Learning of Long-horizon Tasks}
Learning robotic policies for long-horizon manipulation tasks has been a longstanding and complex challenge. Many works focused on leveraging human prior knowledge to simplify this problem~\cite{cheng2023league, zhou2024spire, von2024art, wang2023mimicplay}.
Imitation learning is commonly used to simplify the complexities of long-horizon manipulation tasks. By breaking down long-horizon skills into sub-skills, imitation learning enables effective training of long-horizon policies, as shown in~\cite{luo2024multi, 10802807}. To further enhance learned behaviors, these policies can be improved through RL~\cite{triantafyllidis2023hybrid} or offline parameter optimization techniques~\cite{kumar2024practice}.

Training a long-horizon manipulation policy directly using RL often requires manual designed transitions between different primitives. To achieve robot grasping with external dexterity, \cite{zhou2023learning} incorporate a pre-generated grasp pose in the observation. They expanded the reward function to include the difference from the desired grasp pose and a penalty for collisions, to ensure the feasibility of the learned policy. Similarly, \cite{pmlr-v229-chen23e} split long-horizon tasks into a series of interconnected subtasks. They introduce a transition feasibility function that incrementally refines sub-policies to improve the success rate of chaining subtasks.
By splitting the non-prehensile manipulation into pre-contact and post-contact stages, \cite{kim2023pre} jointly train two policies where the pre-policy is used to determine the contact pose between the end-effector and the object, and the post-policy is used to apply action on the object. These two policies are jointly trained with a highly complex and fine-tuned reward function. In this work, rather than relying on human demonstrations or reward engineering to guide robots toward predefined optimal behaviors, we propose a framework that enables the policy to discover solutions autonomously. By simplifying the problem and expanding the state-action space through privileged actions, our approach addresses the inefficiency of RL in exploring sparse-reward environments.


\subsection{Curriculum Learning}
Curriculum strategies are widely used in RL to enable robots to master challenging tasks. These strategies naturally guide the learning process by starting with simpler tasks and gradually increasing complexity. For instance, \cite{CHIAPPA20243969} use a curriculum to enable a human hand model with 39 muscles to rotate two Baoding balls in its palm.

The work~\cite{chen2021system} devise a gravity-based curriculum to enable in-hand manipulation. They first train with the gravity vector pointing upwards, and then gradually decrease until the normal value. Additionally, \cite{bauza2024demostart} combine human demonstrations with an auto-curriculum strategy for dexterous manipulation. Demonstrations provide initial guidance to reduce the search space and accelerate policy convergence, while the auto-curriculum identifies areas requiring improvement and enhances them through RL. Similarly, \cite{margolis2024rapid} employ an adaptive curriculum based on velocity commands to train a robot to run and turn quickly on natural terrains. Our work leverages curriculum learning to gradually reduce the availability of privileged actions and guide the policy to a physically realistic solution. 

 








\subsection{Privileged Information and Actions}
Simulations offer access to rich information that is often difficult to obtain in the real world.
Previous works have extensively leveraged simulation to provide privileged information to enable policies to acquire essential knowledge, resulting in robust performance~\cite{lee2020learning, chen2023visual, qi2023general, yang2024anyrotate}. Learning from privileged information improves policy learning by reducing the complexity of the state-action mapping required to be learned.
\cite{mordatch2012contact} employ a contact-invariant optimization (CIO) method to specify when and where contact should occur on an object, using hand movements to replace this auxiliary decision variable gradually. Likewise, \cite{cheng2023enhancing} utilized Monte Carlo Tree Search (MCTS) to explore contact points on objects, enabling robot manipulation with exceptional dexterity. 
By relaxing collisions between the robot and obstacles, \cite{zhuang2023robot} combined a curriculum strategy with a specifically designed reward with penetration terms to train a vision-based parkour policy.
In~\cite{zhou2023learning}, predefined grasp poses were used to learn grasping with extrinsic dexterity. Relaxed collisions and penalties for penetration were incorporated into the training process.

Despite the success of these methods, they often require either complex task specific formulation or finely crafted reward functions. To the best of our knowledge, we are the first to propose a structured framework for solving long-horizon manipulation tasks without introducing any delicately designed reward terms.