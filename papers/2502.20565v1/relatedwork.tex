\section{Related Work}
\subsection{Vertical Federated Learning}
Vertical Federated Learning (VFL) enables collaborative training across organizations with vertically partitioned features. Early VFL frameworks focused on simple client-side models such as logistic regression and linear models~\cite{hardy2017private}, where clients computed local gradients for global aggregation. These methods prioritized simplicity, but lacked expressiveness for complex tasks. To address this limitation, larger client-side models like deep neural networks (DNNs) were adopted~\cite{chen2020vafl, castiglia2023flexible, xie2024improving}. Studies have demonstrated that these larger client‐side models can be more effective than classical methods~\cite{liu2024vertical}. However, larger models requires higher computation capability and VRAM cost, which calls for computation and memory efficient methods.

In an effort to reduce communication cost, several communication-efficient VFL methods were proposed. One way of reducing communication overhead is by multiple local updates. \cite{liu2022fedbcd} introduced FedBCD, a federated stochastic block coordinate descent method, which allows clients to perform multiple updates before synchronization. Similarly, {\tt Flex-VFL}~\cite{castiglia2023flexible}, a flexible strategy offering varying local update counts per party, constrained by a communication timeout. {\tt VIMADMM}~\cite{xie2024improving} adopted an ADMM-based approach to enable multiple local updates in VFL.
All these methods effectively reduce the number of round-trip communications in each iteration. On the other hand, Asynchronous VFL methods (e.g., {\tt FDML}~\cite{hu2019fdml}, {\tt VAFL}~\cite{chen2020vafl}) decouple coordination, allowing clients to update models independently. While asynchronous methods risk staleness, they significantly improve scalability in cross-silo deployments. However, in first-order methods, the backward pass typically imposes communication overhead comparable to the forward pass. In contrast, our ZO-based approach significantly reduces the cost associated with backward propagation.
% \subsection{Privacy-Preserving VFL with Differential Privacy}  

Privacy guarantees are critical for VFL adoption. Some VFL architectures use crypto-based privacy-preserving techniques such as Homomorphic Encryption (HE)~\cite{cheng2021secureboost}, but lack formal assurances, while DP provides rigorous mathematical protection. Key DP-based methods include {\tt VAFL}~\cite{chen2020vafl}, which injects Gaussian noise into client embeddings during forward propagation to achieve Gaussian DP, and {\tt VIMADMM}~\cite{xie2024improving} which perturbs linear model parameters with bounded sensitivity, ensuring DP guarantees for convex settings.

\subsection{Zeroth-Order Optimization in VFL}  
Recent research has explored Zeroth‐Order(ZO) Optimization within VFL to accommodate clients whose models are non‐differentiable and to reduce gradient leakage. Early work like {\tt ZOO-VFL} \cite{zhang2021desirable} adopted a naive ZO approach throughout VFL training but provided no DP guarantees. {\tt VFL-CZOFO}~\cite{wang2024unified} introduced a cascaded hybrid optimization method, combining zeroth-order and first-order updates, which leveraged intrinsic noise from ZO for limited privacy. However, its DP level was not adjustable, often resulting in insufficient protection.

More recently, {\tt MeZO}~\cite{malladi2023fine} proposed a memory-efficient ZO algorithm. Building upon these ideas, {\tt DPZero}~\cite{zhang2024dpzero} and {\tt DPZO}~\cite{tang2024private} introduced private ZO variants offering baseline privacy features. In this work, we adopt the memory-efficient design of {\tt MeZO}. In addition to previous attempts to combine ZO Optimization with VFL, we integrate controllable DP into our ZO framework, which requires significantly less memory footprint. 

\begin{figure*}[t]
\centering
\includegraphics[width=0.98\linewidth]{images/DP_visuallize.pdf}
    \caption{
    An illustration of the training procedure of {\tt DPZV}. The communication between the server and clients follows an asymmetric process: at each global iteration $t$, feature embeddings are uploaded once, while the server performs model updates and broadcasts ZO information to the client with DP guarantees for total $K$ iterations. This asymmetric communication update is a key design that enables memory-efficient fast convergence for our method.
    }
    \label{fig:visualize}
\end{figure*}