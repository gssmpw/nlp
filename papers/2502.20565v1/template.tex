\documentclass{article}
\input{symbols_commands}


\usepackage{arxiv}

\allowdisplaybreaks

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{xcolor, graphicx}
\usepackage{subcaption} 
\usepackage{caption}
\usepackage{makecell}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{booktabs}
\RequirePackage{algorithm}
\RequirePackage{algorithmic}

\title{DPZV: Resource Efficient ZO Optimization
For Differentially Private VFL}


\author{
Jianing Zhang$^1$,  \hspace{\fill}
Evan Chen$^1$,  \hspace{\fill}
 Chaoyue Liu$^1$, \hspace{\fill}
 Christopher G. Brinton$^1$ \\ \\
  $^1$School of Electrical and Computer Engineering\\
  Purdue University\\
    West Lafayette \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}{Fact}

\definecolor{lightred}{rgb}{1, 0.7, 0.7}
\definecolor{lightgreen}{rgb}{0.7, 1, 0.7}

\begin{document}
\maketitle
\begin{abstract}
Vertical Federated Learning (VFL) enables collaborative model training across feature-partitioned data, yet faces significant privacy risks and inefficiencies when scaling to large models. We propose DPZV, a memory-efficient Zeroth-Order(ZO) optimization framework that integrates differential privacy (DP) with vertical federated learning, addressing three critical challenges: (1) privacy vulnerabilities from gradient leakage, (2) high computation/communication costs of first-order methods, and (3) excessive memory footprint in conventional zeroth-order approaches. Our framework eliminates backpropagation through two-point gradient estimation, reducing client memory usage by 90\% compared to first-order counterparts while enabling asynchronous communication. By strategically injecting Gaussian noise on the server, DPZV achieves rigorous $(\epsilon, \delta)$-DP guarantees without third-party trust assumptions. Theoretical analysis establishes a convergence rate matching centralized case under non-convex objectives. Extensive experiments on image and NLP benchmarks demonstrate that DPZV outperforms all baselines in accuracy while providing strong privacy assurances ($\epsilon \leq 10$) and requiring far fewer computation resources, establishing new state-of-the-art privacy-utility tradeoffs for resource-constrained VFL deployments.
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}




\section{Introduction}
The rapid growth of data-driven applications across industries has propelled the demand for collaborative machine learning frameworks that can harness data spread across multiple organizations. In particular, vertical federated learning (VFL) has emerged as a promising paradigm to train models on data that are partitioned feature-wise among distinct parties, without directly sharing raw information across institutional boundaries~\cite{hardy2017private,chen2020vafl,castiglia2023flexible}. By maintaining separate sub-models locally and exchanging only intermediate representations or gradients, VFL seeks to preserve privacy while jointly leveraging the complementary features held by different entities.

Despite its privacy-oriented design, recent studies highlight that VFL remains susceptible to data leakage through subtle channels. Two key sources of leakage have been identified: (1) feature leakage, wherein adversaries can infer sensitive inputs from the trained bottom model parameters exchanged during federated optimization, and (2) label leakage, where gradients transmitted between participants inadvertently reveal sensitive label information. These vulnerabilities underscore the urgency of integrating robust privacy-preserving techniques into VFL.

Differential privacy (DP) has emerged as a leading candidate for mitigating such vulnerabilities, as it provides strong, mathematically grounded privacy guarantees against a wide range of adversarial inferences~\cite{Zhao2021LDP,Shen2022imp,Liu2023mobi}. However, incorporating DP mechanisms into VFL must be done carefully to avoid undue performance degradation and excessive computational overhead. The challenges are further exacerbated by the recent trend toward larger and more complex models—such as large language models—in federated scenarios, where memory and communication efficiency become paramount. Storing and propagating high-dimensional gradients or performing complex optimization steps can quickly become impractical, especially in resource-constrained settings.

An attractive alternative to conventional gradient-based optimization in VFL is Zeroth-Order (ZO) Optimization, which relies on function value evaluations rather than gradient computations~\cite{zhang2021desirable,malladi2023fine,wang2024unified}. By circumventing the need for direct gradient exchanges, ZO methods can reduce the risk of gradient-based leakage and naturally fit into memory-constrained environments. However, existing ZO-based VFL frameworks still face critical limitations. They often lack efficient communication protocols and fail to incorporate strong privacy mechanisms such as differential privacy, ultimately falling short of delivering a holistic solution that addresses privacy, scalability, and efficiency simultaneously.

In this work, we propose Differentially Private Zeroth-Order Vertical Federated Learning ({\tt DPZV}), a novel ZO-based VFL framework that achieves all three objectives: privacy protection, memory and communication efficiency, and strong theoretical convergence guarantees. Our main contributions are as follows:

% \chaoyue{which is our main contribution? Memory efficiency, or guaranteed privacy, or allowing delay? It sounds like the following list is emphasizing memory efficiency, but this contribution is mostly from MeZO, making the paper weaker.}
\begin{itemize}
    % \item We design a zero-order-based VFL scheme that significantly reduces the memory footprint and communication overhead associated with inter-institutional model training, making it suitable for large-scale and resource-constrained settings.
    \item We introduce a zero-order based VFL algorithm that incorporates arbitrary levels of differential privacy at both the model and the gradient level, which is the first work in VFL to enable highly secure communication while maintaining both memory and communication efficiency. The algorithm also significantly reduces memory footprint and communication overhead associated with inter-institutional model training, making it suitable for large-scale and resource-constrained settings (Sec.~\ref{sec:method}).

    % \item We incorporate arbitrary levels of differential privacy at both the model and gradient levels, ensuring strong privacy guarantees against inference attacks, 
    
    \item We provide a rigorous theoretical analysis of both the convergence rate and the privacy guarantees of our method. Our convergence results establish that the proposed zero-order approach achieves the same order of convergence as the centralized counterpart, offering strong theoretical support for its scalability to large-scale systems. Furthermore, we prove that our method satisfies $(\epsilon, \delta)$-differential privacy, demonstrating its robustness in protecting privacy against potential attacks (Sec.~\ref{sec:convergence} and~\ref{sec:privacy}).
    
    \item We conduct extensive experiments on both classical image classification tasks and pretrained language models. We demonstrate that our algorithm outperforms state-of-the-art methods while requiring less memory to train and lower communication overhead for convergence (Sec.~\ref{sec:exp}).
\end{itemize}


By jointly addressing privacy risks, memory and communication constraints, and providing formal convergence assurances, our framework represents a significant step forward for practical, secure, and scalable federated learning solutions.



\section{Related Work}
\subsection{Vertical Federated Learning}
Vertical Federated Learning (VFL) enables collaborative training across organizations with vertically partitioned features. Early VFL frameworks focused on simple client-side models such as logistic regression and linear models~\cite{hardy2017private}, where clients computed local gradients for global aggregation. These methods prioritized simplicity, but lacked expressiveness for complex tasks. To address this limitation, larger client-side models like deep neural networks (DNNs) were adopted~\cite{chen2020vafl, castiglia2023flexible, xie2024improving}. Studies have demonstrated that these larger client‐side models can be more effective than classical methods~\cite{liu2024vertical}. However, larger models requires higher computation capability and VRAM cost, which calls for computation and memory efficient methods.

In an effort to reduce communication cost, several communication-efficient VFL methods were proposed. One way of reducing communication overhead is by multiple local updates. \cite{liu2022fedbcd} introduced FedBCD, a federated stochastic block coordinate descent method, which allows clients to perform multiple updates before synchronization. Similarly, {\tt Flex-VFL}~\cite{castiglia2023flexible}, a flexible strategy offering varying local update counts per party, constrained by a communication timeout. {\tt VIMADMM}~\cite{xie2024improving} adopted an ADMM-based approach to enable multiple local updates in VFL.
All these methods effectively reduce the number of round-trip communications in each iteration. On the other hand, Asynchronous VFL methods (e.g., {\tt FDML}~\cite{hu2019fdml}, {\tt VAFL}~\cite{chen2020vafl}) decouple coordination, allowing clients to update models independently. While asynchronous methods risk staleness, they significantly improve scalability in cross-silo deployments. However, in first-order methods, the backward pass typically imposes communication overhead comparable to the forward pass. In contrast, our ZO-based approach significantly reduces the cost associated with backward propagation.
% \subsection{Privacy-Preserving VFL with Differential Privacy}  

Privacy guarantees are critical for VFL adoption. Some VFL architectures use crypto-based privacy-preserving techniques such as Homomorphic Encryption (HE)~\cite{cheng2021secureboost}, but lack formal assurances, while DP provides rigorous mathematical protection. Key DP-based methods include {\tt VAFL}~\cite{chen2020vafl}, which injects Gaussian noise into client embeddings during forward propagation to achieve Gaussian DP, and {\tt VIMADMM}~\cite{xie2024improving} which perturbs linear model parameters with bounded sensitivity, ensuring DP guarantees for convex settings.

\subsection{Zeroth-Order Optimization in VFL}  
Recent research has explored Zeroth‐Order(ZO) Optimization within VFL to accommodate clients whose models are non‐differentiable and to reduce gradient leakage. Early work like {\tt ZOO-VFL} \cite{zhang2021desirable} adopted a naive ZO approach throughout VFL training but provided no DP guarantees. {\tt VFL-CZOFO}~\cite{wang2024unified} introduced a cascaded hybrid optimization method, combining zeroth-order and first-order updates, which leveraged intrinsic noise from ZO for limited privacy. However, its DP level was not adjustable, often resulting in insufficient protection.

More recently, {\tt MeZO}~\cite{malladi2023fine} proposed a memory-efficient ZO algorithm. Building upon these ideas, {\tt DPZero}~\cite{zhang2024dpzero} and {\tt DPZO}~\cite{tang2024private} introduced private ZO variants offering baseline privacy features. In this work, we adopt the memory-efficient design of {\tt MeZO}. In addition to previous attempts to combine ZO Optimization with VFL, we integrate controllable DP into our ZO framework, which requires significantly less memory footprint. 

\begin{figure*}[t]
\centering
\includegraphics[width=0.98\linewidth]{images/DP_visuallize.pdf}
    \caption{
    An illustration of the training procedure of {\tt DPZV}. The communication between the server and clients follows an asymmetric process: at each global iteration $t$, feature embeddings are uploaded once, while the server performs model updates and broadcasts ZO information to the client with DP guarantees for total $K$ iterations. This asymmetric communication update is a key design that enables memory-efficient fast convergence for our method.
    }
    \label{fig:visualize}
\end{figure*}

\section{Preliminaries}
In this section, we introduce the concepts of DP and a ZO gradient estimator. Then we introduce the VFL framework, and define the objective problem.
\subsection{Differential Privacy}
DP is a rigorous framework designed to protect individual data while still allowing useful analysis of a dataset as a whole. 
% Intuitively, it ensures that the presence or absence of any single individual’s data does not significantly affect the results.
We present the definition of the classical $(\epsilon, \delta)$-DP:
\begin{definition}[$(\epsilon, \delta)$-DP]
    A randomized algorithm $\gM: \gX^n\rightarrow\Theta$ is ($\epsilon, \delta$)-differentially private (DP) if for any pair of neighboring datasets $X,X'\in \gX^n$ that differ in the record of a single individual, and any possible event $E\subseteq \Theta$,
    \begin{align}\label{eq:DP}
    \Prob[\gM(X)\in E]\le e^\epsilon\Prob[\gM(X')\in E]+\delta
    \end{align}
\end{definition}
% Gradient-based optimization algorithms are prevalent in  training modern neural networks, with zeroth-order optimization being one of them. 
When considering ZO-based algorithms, model parameters are iteratively updated by an estimation of the gradient, and the updated parameter is used as the input of another iteration. This behavior involves the composition of privacy, which means in order to achieve a target privacy budget for the whole training process, we need to carefully design the privacy budget for each iteration and compute the total privacy through the composition of DP mechanisms. 
% We relegate the analysis of the privacy level of our algorithm to \cref{sec:privacy}.
\subsection{Zeroth-order Optimization}
\label{ssec:ZOO} 
Zeroth-Order Optimization has been widely discussed under both convex and non-convex conditions~\cite{ghadimi2013stochastic,nesterov2017random,fang2022communication}. This method finds the optima of an objective function without relying on first-order information. Such properties are especially useful when first-order gradient is computationally expensive or intractable. 

In the following, we introduce the two-point gradient estimator~\cite{shamir2017optimal}, which will serve as our zeroth-order gradient estimator throughout this paper. 
\begin{definition}Let $u$ be uniformly sampled from the Euclidean sphere $\sqrt{d}\sS^{d-1}$ and $\lambda>0$ be the smoothing factor. For a single datum $\xi$ sampled from the whole dataset $\gD$, the two-point gradient estimator is defined as
    \begin{align}\label{eq:zero}
     g(x;\xi) = \frac{f(x+\lambda u;\xi)-f(x-\lambda u;\xi)}{2\lambda}u
    \end{align}
\end{definition}

In this work, 
we adopt the {\tt MeZO} methodology~\cite{malladi2023fine} for our ZO Optimization, which is a memory efficient method for computing ZO information. In each communication round, every client samples a random seed $s$, which is kept private. Let the local parameters $\vw_m$ and the randomly sampled direction $\vu_m^t$ be defined as:  
\begin{align*}
    \vw_m & \textstyle= [w_{m,1}, w_{m,2}, \dots, w_{m,d_m}]^\top, \\ \vu_m^t & \textstyle= [u_{m,1}, u_{m,2}, \dots, u_{m,d_m}]^\top.
\end{align*}
Rather than sampling $\vu_m^t$ as a complete vector, each entry $u_{m,j}, \forall j \in [d_m]$, is sampled independently using the random seed $s$. The corresponding parameter $w_{m,j}$ is perturbed as follows:  
\(w_{m,j} \leftarrow w_{m,j} \pm \lambda_m u_{m,j}.\)  
During the parameter update step, where $\vu_m^t$ is required again, the random seed $s$ is reset, and $\vu_m^t$ is resampled entry-wise in the same manner. This approach significantly reduces the memory footprint, halving the memory cost compared to Vanilla {\tt ZO-SGD}. This reduction is particularly beneficial for client devices where memory constraints are a critical bottleneck.

% We note that the result of this paper can be easily extended to other zeroth-order gradient estimators.

% \chaoyue{This claim is too general and vague. E.g, all other zeroth-order gradient estimaters? I think we should specify }

\subsection{Vertical Federated Learning}
We consider a VFL framework with one server and $M$ clients. In VFL, data is vertically partitioned between clients, with each client holding different features of the data. Suppose we have a dataset $\gD$ with $D$ samples: $\gD=\{\xi_i|i=1,2,\dots,D\}$. Each data sample $\xi_i$ can be partitioned into $M$ portions distributed throughout all clients, where the data sample on machine $m$ with ID $i$ is denoted as $\xi_{m,i}$, hence $\xi_i = [\xi_{1,i}, \xi_{2,i}, \ldots, \xi_{M,i}]^\top$. The server is numbered as machine $0$ and holds the label data $\mY = \{y_i|i=1,2,\dots,D\}$.
% \chaoyue{I think it is better to clarify the relation between $\xi_i$ and $\xi_{m,i}$. For those who are not familiar with VFL, this could be confusing.}

The clients and the server aim to jointly train a machine learning model parameterized by $\vw$. The global model comprises local models on each party parameterized by $w_m$, with $m= 0,1,\dots, M$ being the ID of the machine. To protect privacy, clients do not communicate with each other regarding data or model parameters. Instead, all clients communicate directly with the server regarding their local model's outputs, which we term as local embeddings. If we denote the local embedding of client $m$ as $h_{m,i}:=h(w_m;\xi_{m,i})$, the objective of the VFL framework can be seen as minimizing the following function:
\begin{align}\label{eq:defition}
    \textstyle F(\vw;\gD,\mY):=\frac{1}{D}\sum_{i\in[D]}\gL(w_0, h_{1,i}, h_{2,i},\dots, h_{M,i};y_i)
\end{align}
 where $\gL$ is the loss function for a datum $\xi_i$ and its corresponding label $y_i$. For the simplicity of notation, we define the loss function w.r.t a specific model parameter and datum as \begin{align}\label{eq:f}
     \textstyle f(\vw;\xi_i):=\gL(w_0, h_{1,i}, h_{2,i},\dots, h_{M,i};y_i)
 \end{align}
 
 % We continue to demonstrate the algorithm for solving \eqref{eq:defition} in \cref{sec:method}.

\section{Methodology}
\label{sec:method}
In this section, we propose {\tt DPZV}, a VFL framework that combines ZO Optimization with DP guarantees and asynchronous updates. Based on \eqref{eq:defition}, the objective is to collaboratively minimizing global objective function across all clients, each holds  disjoint features of the same data records, and one server which holds the label data. The training procedure can be described in two iterative steps: (i) \textit{Client Update and Forward Communication}: The sampled client computes local information and transmits it to the server, we define the client-server communication as \textit{forward communication}. (ii) \textit{Server Update and Backward Communication}: The server updates the global model and transmit global updates to the client, we define the server-client communication as \textit{backward communication}.
% \chaoyue{Server does not hold features. How about this "... across $M$ clients, each of which holds ... data records, and one server ...}
% we illustrate how DPZV-Asynchronous accomplishes this in a memory- and computation-efficient manner.

% \chaoyue{use colors to highlight key parts of the algorithm, as Chris suggested last time? }

% \chaoyue{I find the lines "Repeat Line xx-xx" might be confusing, we should give a termination condition}

% \chaoyue{In the Algorithm pseudo-code, it is confusing to have "(line 4) for $t\in [T]$ and "(line 14) update communication round $t=t+1$" together.}
\begin{algorithm}[tb]
   \caption{{\tt DPZV}: Differentially Private Zeroth-Order Vertical Federated Learning}
   \label{alg:DPZV_asyn}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:}  Data $\gD$, batch size $B$, learning rate $\eta_m$, total iteration $T$, smoothing parameter $\lambda_m>0$, clipping threshold $C>0$, privacy parameter $\sigma_{dp}$\\
    \STATE {\bfseries Output:} Parameter $w_0, w_m$ for all parties $m\in[M]$
    \STATE Initialize $w_0,w_m$ and set $t,t_m=0$ for all parties
    \FOR{$t = 1,\ldots,T$}
    % \FOR{each activated \textbf{client} $m$ in parallel}
    \STATE Sample ready-to-update client $m\in \{1, \ldots, M\}$
    % \STATE \colorbox{lightred}{\parbox{\linewidth -35.5mm}{ $\triangleright$ Client Operations }}
    % \LineComment{Local Client operations}
    % \STATE $\triangleright$ Local-Client Operations
    \STATE Client $m$ samples a mini-batch $\gB_m^{t_m}$ with  corresponding IDs $\gI_m^{t_m}$.
    \STATE Client $m$ computes perturbed local embeddings $\{h_{m,i}^{t_m+},h_{m,i}^{t_m-}\}_{i\in \gI_m^{t_m}}$ according to \eqref{eq:perturb}.
    \STATE 
    \colorbox{lightred}{\parbox{\linewidth -3.5mm}{ \textbf{(Forward)} Client transmits embeddings to server. }}
    \FOR{$k = 1, \ldots, K$}

        \STATE Server computes $\Delta_{m}^{t}$ according to \eqref{eq:delta} and \eqref{eq:Delta}.
        \STATE Server updates global server with $\Delta_{m}^{t}$ via $i)$ ZO Optimization \OR $ii)$ SGD.
        \STATE 
        \colorbox{lightgreen}{\parbox{\linewidth -23.5mm}{ \textbf{(Backward)} Client receives $\Delta_{m}^{t}$. }}
         
        \STATE Client performs local update using \eqref{eq:local_gd}.
    \ENDFOR
    \STATE Client update local time stamp $t_m = t$.
    
    % \STATE Compute and send perturbed local embeddings $\{h_{m,i}^{t_m+},h_{m,i}^{t_m-}\}_{i\in \gI_m^{t_m}}$ to server according to \eqref{eq:perturb}
    % \STATE Receive $\Delta_{m}^{t}$ from server
    % \STATE Update $w_m\leftarrow w_m-\eta_m \Delta_{m}^{t}\vu_{m}^{t_m}$
    % \STATE Update local communication round $t_m = t$
    % % \STATE \textbf{Repeat} Line 8-10
    % % \ENDFOR
    % \WHEN {\textbf{server} receives $\{h_{m,i}^{t_m+},h_{m,i}^{t_m-}\}_{i\in \gI_m^{t_m}}$}
    % \STATE Update communication round $t=t+1$
    % \STATE For $i\in \gI_m^{t_m}$, compute $\delta_{m,i}^{t_m}$ according to \eqref{eq:delta}
    % \STATE Sample $z_{m}^{t}\sim \gN(0,\sigma_{dp}^2)$ and compute:$\Delta_{m}^{t}$ according to \eqref{eq:Delta}
    % \STATE Send $\Delta_{m}^{t}$ back to client $m$
    % \STATE Update $w_0$ via $i)$ ZOO \OR $ii)$ SGD 
    % \STATE \textbf{Repeat} Line 13-17
    % \ENDWHEN
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\subsection{{\tt DPZV} Training Procedure}

% Since each client runs independently, each client maintains its own local communication round $t_m$, while the server maintains $t$. Every time the server receives information from a client, the server increments $t$. Once the client hears back from the server, it sets $t_m = t$ to record the latest update. This asynchronous mechanism allows clients to proceed without waiting for straggling peers, thus improving throughput and reducing idle time. 
% \chaoyue{If we let "Every time the server receives information from a client", then it will end up with updating the server for each client signal, as client signals reach server not exactly at the same time. A short waiting window should be reasonable.}

% For every embedding $h_{m,i}^{t_m}$ received from client $m$, the server reuses it, updates its parameter and communicates with the client for $K$ iterations, resulting in an asymmetric communication protocol. 
% The reason of this design is to take advantage of the low communication cost of ZO information, where multiple rounds of ZO updates still costs way less than one round of first-order information communication. This design takes advantage of  asymmetric communication ensures the full use of the forward information, thus reducing communication cost per round by half.

Each client $m$ operates independently, maintaining its own local communication round $t_m$, while the server tracks a global round $t$. Whenever the server receives information from a client, it increments $t$. Upon receiving an update from the server, the client synchronizes by setting $t_m = t$, capturing the latest state. This asynchronous mechanism allows clients to progress without waiting for stragglers, improving throughput and minimizing idle time.  

The server maintains a copy of the latest local embeddings $\Tilde{h}^t_{m,i}$ for all clients $m \in [M]$ and data samples $\xi_i \in \gD$. Due to the asynchronous nature of the algorithm, these copies may be stale, as they do not always reflect the most up-to-date model parameters of the clients. Let $\Tilde{t}_{m,i}$ denote the client time when the server last updated $\Tilde{h}^t_{m,i}$. The delay at server communication round $t$ can then be expressed as  
\(
\tau_{m}^t = t_m - \Tilde{t}_{m,i},    
\)
where the delayed model parameters are defined as:  
\begin{align}\label{eq:delay_def}
\Tilde{\chi}^t = \{w_1^{t_1 - \tau_1^t}, \dots, w_M^{t_M - \tau_M^t}\}, \quad \Tilde{\vw}^t = \{w_0^t, \Tilde{\chi}^t\}.
\end{align}  

% The delay parameter $\tau_{m}^t$ arises either from the client has locally updated its parameters since its last communication, or the server multiple global updates based on previous forward information.

\textbf{Local Embedding Perturbation:} For each global iteration $t$, a client $m$ is activated, and it samples a mini-batch $\gB_{m}^{t_m}\in\gD$ and the corresponding IDs $\gI_{m}^{t_m}$. To approximate gradients via zeroth-order finite differences, client $m$ computes two perturbed local embeddings for the mini-batch, 
\begin{align*}
&\{h_{m,i}^{t_m+}\}_{i\in \gI_m^{t_m}}=\{h(w_m^{t_m}+\lambda_m \vu_{m}^{t_m};\xi_{m,i})\}_{i\in \gI_m^{t_m}},\\
&\{h_{m,i}^{t_m-}\}_{i\in \gI_m^{t_m}}=\{h(w_m^{t_m}-\lambda_m \vu_{m}^{t_m};\xi_{m,i})\}_{i\in \gI_m^{t_m}}\numbereq\label{eq:perturb}
\end{align*}
where $\vu_{m}^{t_m}$ is sampled uniformly at random from the Euclidean sphere $\sqrt{d_m}\sS^{d_m-1}$, and $\lambda_m$ is a smoothing parameter that controls the step size of perturbation. 
These two perturbed embeddings serve as “positive” and “negative” perturbations of its local parameters, which are forwarded to the server for further computation.

\textbf{Server side ZO computation:} 
% For each embedding  received from client $m$. Once the 
After the server receives the local embeddings $h_{m,i}^{t_m}$ from client $m$, it updates its embeddings copy and do computation. For each embedding pair $\{h_{m,i}^{t_m+}, h_{m,i}^{t_m-}\}$, it computes the difference in loss function $\gL$ caused by perturbation, divided by the smooth parameter $\lambda_m$. Specifically, we define\footnote{We slightly abuse the notation, and define \\ $\Tilde{f}(w_0,h_{m,i}^{t_m\pm};y_i)=\gL(w_0, \Tilde{h}^t_{1,i}, \dots,h_{m,i}^{t_m\pm},\dots, \Tilde{h}^t_{M,i};y_i)$.
where we treat $\Tilde{f}$ as a function of the server parameter $w_0$ and the perturbed local embeddings.}:
\begin{align}\label{eq:delta}
\delta_{m,i}^{t,t_m} = \frac{\Tilde{f}(w_0,h_{m,i}^{t_m+};y_i)-\Tilde{f}(w_0,h_{m,i}^{t_m-};y_i)}{\lambda_m},
\end{align}
To ensure controllable privacy, the server then clips each $\delta_{m,i}^{t}$ by a threshold $C$ to bound sensitivity:
\(
\mathrm{clip}_C(\delta_{m,i}^{t,t_m}) = \min \{\delta_{m,i}^{t,t_m},C\}.
\)
It then samples noise $z_{m}^{t_m}$ from a Gaussian distribution $\mathcal{N}(0,\sigma_{dp}^2)$. This noise is added to the mean of the per-sample-clipped updates, yielding a differentially private gradient-like quantity:
\begin{align}\label{eq:Delta}
\Delta_{m}^{t} = \frac{1}{B}\sum_{i \in \gI_{m}^{t_m}} \mathrm{clip}_C(\delta_{m,i}^{t,t_m})+z_{m}^{t}.
\end{align}
The server then performs a {backward communication} and sends $\Delta_{m}^{t}$ to client $m$. The server then updates its global model through two possible operations: $i)$ ZO Optimization, $\vw_0 \leftarrow \vw_0 - \eta_0 g(w_0;\xi_i)$ (defined in \eqref{eq:zero}); or $ii)$ stochastic gradient descent (SGD), $\vw_0 \leftarrow \vw_0 - \eta_0 \nabla_{w_0} F(\vw;\xi_i)$, depending on the constraints on computation resources. The adopted ZO update methodology is explained in Sec.~\ref{ssec:ZOO}.
% Once the server updated its own parameter, it repeats the process in the last step to transmit $\Delta_{m}^{t+1}$ back to the client.

On the client side, upon receiving $\Delta_{m}^{t}$, client $m$ updates its local parameter $\vw_m$ with learning rate $\eta_m$:
\begin{equation}
\vw_m \leftarrow \vw_m -\eta_m\Delta_{m}^{t}\vu_{m}^{t_m},
    \label{eq:local_gd}
\end{equation}

where $\vu_{m}^{t_m}$ is the same vector used for local perturbation. 

\textbf{Communication Efficient Asymmetric Update:} By leveraging the low communication cost of ZO information, sever-client communication can be performed for a more convergence efficient design. As a result, the algorithm performs $K$ iterations of \eqref{eq:delta}-\eqref{eq:local_gd}, where the server reuses the forward information from client $m$, resulting in an asymmetric communication protocol. This design leverages the advantage that multiple rounds of ZO updates incur significantly less overhead compared to a single round of first-order information exchange. 

This asymmetric communication scheme maximizes the utility of forward information, which effectively reduces the per-round communication cost. Our method is well-suited for resource-constrained environments—such as edge devices with limited VRAM or computation power, while still maintaining robust performance and scalability in VFL settings. We summarize the pipeline above in \cref{alg:DPZV_asyn}. 

% \subsection{Local Embedding Perturbation}



% \subsection{Server Computation for ZO gradient estimator and Privacy}




% \subsection{Client and Server Parameter Update Twice}

% Simultaneous to the computation of zeroth-order difference, the server updates its parameter $\vw_0$. We enjoy flexibility regarding the manner of update: via either $i)$ ZOO, using a similar gradient estimator defined in \eqref{eq:zero}; or $ii)$ stochastic gradient descent (SGD), depending on the constraints on computation resources. Once the server updated its own parameter, it repeats the process in the last step to transmit $\Delta_{m}^{t+1}$ back to the client.

% On the client side, upon receiving $\Delta_{m}^{t}$, client $m$ updates its local parameter $\vw_m$ with learning rate $\eta_m$:
% \begin{equation}
% \vw_m \leftarrow \vw_m -\eta_m\Delta_{m}^{t}\vu_{m}^{t_m},
%     \label{eq:local_gd}
% \end{equation}

% where $\vu_{m}^{t_m}$ is the same vector clients used for local perturbation. The whole process is repeated once $\Delta_{m}^{t+1}$ is received.





% \subsection{Memory Efficient ZO Optimization}
% \label{ssec:MEZOO} 
% \jianing{We may remove this part}

% We adopt the MeZO methodology~\cite{malladi2023fine} for our zero-order optimization. In each communication round, every client samples a random seed $s$, which is kept private. Let the local parameters $\vw_m$ and the randomly sampled direction $\vu_m^t$ be defined as:  
% \begin{align*}
%     \vw_m &= [w_{m,1}, w_{m,2}, \dots, w_{m,d_m}]^\top, \\ \vu_m^t &= [u_{m,1}, u_{m,2}, \dots, u_{m,d_m}]^\top.
% \end{align*}
% Rather than sampling $\vu_m^t$ as a complete vector, each entry $u_{m,j}, \forall j \in [d_m]$, is sampled independently using the random seed $s$. The corresponding parameter $w_{m,j}$ is perturbed as follows:  
% \(w_{m,j} \leftarrow w_{m,j} \pm u_{m,j}.\)  
% During the parameter update step, where $\vu_m^t$ is required again, the random seed $s$ is reset, and $\vu_m^t$ is resampled entry-wise in the same manner. This approach significantly reduces the memory footprint, halving the memory cost compared to Vanilla ZO-SGD. This reduction is particularly beneficial for client devices where memory constraints are a critical bottleneck.

% \subsection{Extension to Synchronous Communication} 
% Although a fully asynchronous approach offers the greatest flexibility, our DPZV framework can also be easily extended to a synchronous scheme instead. In this protocol, the server waits for from exactly $t$ clients, computes $\Delta_{m}^{t}$ in the same way as \eqref{eq:clip_delta} and sends it back to clients. The server and $t$ clients then updates their local models. Empirically, these synchronous updates tend to produce more stable performance. We summarize the synchronous version of DPZV in \cref{appen:syn_alg}.

% \textbf{Summary} 
% Our DPZV-Asynchronous algorithm integrates: 
%     a) \textbf{ZOO} to avoid explicit gradient computations and enable both memory- and computation-efficient training, 
%     b) \textbf{DP} to guarantee the confidentiality of updates and secure model release, and 
%     c) an \textbf{asynchronous} and \textbf{asymmetric} protocol to maximize communication utilization. 


\section{Convergence Analysis}
\label{sec:convergence}
In this section, we provide the convergence analysis for {\tt DPZV}. The detailed proofs can be found in Appendix~\ref{appen:thm_conv}. For brevity, we define the following notations: $F^t = F(\vw^t):=F(\vw^t;\gD,\mY)$, and $f(\vw;\xi_i)$ as defined in \eqref{eq:f}.
We make the following standard assumptions \footnote{We claim that \cref{assum:Lip} and \ref{assum:bound} are standard in VFL and ZO literature\cite{wang2024unified, castiglia2023flexible}. We follow \cite{zhang2024dpzero} to make the $\ell$-Lipschitz assumption in order to bound the probability of clipping. \cref{assum:ind_part} is common when dealing with asynchronous participation \cite{chen2020vafl}, and can be satisfied when the activations of clients follow independent Poisson processes.}:

\begin{assumption}[Properties of loss function]\label{assum:Lip}
    The VFL objective function $F$ is bounded from below, the function $f(\vw;\xi_i)$ is $\ell$-Lipschitz continuous and $L$-Smooth for every $\xi_i\in\gD$. 
\end{assumption}

\begin{assumption}[System boundedness]\label{assum:bound}
The following system dynamics are bounded:
1) \textit{Stochastic Noise:}
The variance of the stochastic first order gradient is upper bounded in expectation:
    \(\E\left[\norm{\nabla_{\vw}f(\vw;\xi)-\nabla_{\vw}F(\vw)}^2\right]\le \sigma_s^2.\)
2) \textit{Time Delay:}
    The parameter delay $\tau_m^t$ is upper bounded by a constant $\tau$:
    \( \tau^t \leq \tau,\quad \forall m,t.\)
    % , and 
\end{assumption}
% To deal with the asynchronous nature in our algorithm, we also make the following assumption for analyzing the participation of clients:
\begin{assumption}[Independent Participation]\label{assum:ind_part}
    Under an asynchronous update system, the probability of one client participating in one communication round is independent of other clients and satisfies:
    \[\Prob(\text{client }m\text{ uploading}) = q_m .\]
\end{assumption}
We now present the main theorem that provides convergence guarantee for {\tt DPZV}:
% \chaoyue{Why don't put the uniformly bounded delay assumption in the Assumption 5.3? It seems weird to have one assumption in the theorem, and others not}
\begin{theorem}\label{thm:main}
Under \cref{assum:Lip}-\ref{assum:ind_part}. If we denote $q_* = \min_m q_m$, $d_*=\max_m d_m$ where $d_m$ represent the dimension of model parameters on device $m$, and let all step sizes satisfy: $\eta_0 = \eta_m = \eta\le \min\{\frac{1}{\sqrt{Td_*}}, \frac{B}{4L(B+8d_0)+8\gamma_1(2d_m+B)}\}$, let the smoothing parameter $\lambda$ satisfy:
\(
    \lambda \le \frac{1}{Ld\sqrt{T}}
\), and let the clipping level $C$ satisfy:
\begin{equation}
    \label{eq:clip_bound}
    \textstyle C\geq \max\left\{0, \frac{1}{2}L\lambda d - \ell\sqrt{8\log(2\sqrt{2\pi})}\right\},
\end{equation}
then for any given global iteration $T \geq 1$, we have the following upper bound on the gradient norm:
\begin{align*}
    &\textstyle\frac{1}{T} \sum_{t=0}^{T-1}\E\left[\norm{\nabla_{\vw}F(\vw^t)}^2\right]
    \le 
    % \frac{1}{1-\Xi}\left\{\frac{8\sqrt{d_*}}{q_* T^{1/2}}\E\left[F^{0}-F^{T}\right]\right.\\
    % &+ \frac{2d_*}{q_* T}+ \frac{8\sqrt{d_*}}{q_*Ld T^{1/2}}+\frac{16 (4L+2\gamma_1)\sqrt{d_*}\sigma_s^2}{q_* BT^{1/2}}\\
    % &+\frac{4 \left((4-B)L+(B+2)\gamma_1\right)\sqrt{d_*}}{q_*B T^{1/2}}+\frac{16 (2L+\gamma_1)\Xi C^2 \sqrt{d_*}}{q_* BT^{1/2}}\\
    % &\left.+ \frac{16 (2L+\gamma_1)\sigma_{dp}^2 \sqrt{d_*}}{q_*T^{1/2}}\right\}\\
    \mathcal{O}\bigg(\frac{\E\left[F^{0}-F^{T}\right]\sqrt{d_*}}{\sqrt{T}}  \textstyle+ \frac{d_*}{T} +  \frac{(\sigma_s^2/B + \sigma_{dp}^2)\sqrt{d_*}}{\sqrt{T}}+ \frac{C^2\sqrt{d_*}}{B\sqrt{T}} \bigg),\numbereq\label{eq:main}
\end{align*}
% where $\Xi=2\sqrt{2\pi}\exp(-(2C-L\lambda d)^2/32\ell^2)$, $\gamma_1$ is a constant further defined in Appendix~\ref{appen:lemma}, and $d$ is the dimension of all the trainable parameters in the VFL framework. 
\end{theorem}
% \chaoyue{This proof sketch is not informative. Should describe the main proof logic and techniques}


% We can observe that the convergence rate is
% $\gO(\sqrt{d_*/T})$, .
The first term, $\mathcal{O}(\frac{\E\left[F^{0}-F^{T}\right]\sqrt{d_*}}{\sqrt{T}})$, is influenced by the model's initialization, $F^0$. This term also enjoys the same rate as ZO optimization in the centralized case \cite{ghadimi2013stochastic}. The second term $\mathcal{O}(\frac{d_*}{T})$ is a standard term for ZOO methods based on the usage of ZO estimator updates.

The third term, $\mathcal{O}(\frac{(\sigma_s^2/B + \sigma_{dp}^2)\sqrt{d_*}}{\sqrt{T}})$, captures the impact of various noise sources in the learning system. Here, $\sigma_s^2/B$ represents the noise introduced by stochastic gradients, although increasing the batch size $B$ reduces the noise level towards convergence, it also increases memory requirements for update computations. Similarly, $\sigma_{dp}^2$ corresponds to the variance of the injected DP noise. While reducing $\sigma_{dp}^2$ improves utility, it comes at the cost of weakening privacy guarantees. Consequently, this term encapsulates the fundamental trade-off between model performance, computational cost, and privacy budget.

The fourth term, $\mathcal{O}(\frac{C^2\sqrt{d_*}}{B\sqrt{T}})$, highlights the role of the clipping operation in ZO optimization. A larger bounded sensitivity leads to greater fluctuations in the ZO information, potentially slowing convergence. The clipping level, however, isn't the smaller, the better. As shown in \eqref{eq:clip_bound}, the selection of $C$ is lower bounded by the choice of the smoothing parameter $\lambda$, the smoothness level $L$, and Lipschitz constant $\ell$.
This emphasizes the importance of carefully controlling the sensitivity level to ensure stable and efficient learning. 


% \evan{Let's remove the sketch for this moment}
% \textit{Proof Sketch.} The main difficulty in the proof lies in bounding the delayed information caused by the asynchronous update. To theoretically analyze the delayed information, we define a Lyapunov function that serves as an upper bound for the global loss function up to a delay term. Leveraging the $L$-smoothness of the loss function and properties of ZOO, as well as other assumptions, we prove a lemma that successfully quantify the update of the Lyapunov function, thus bounding the update of the loss function. Finally, by carefully selecting the value for $\eta$ and $\lambda$, we derive the convergence rate in \eqref{eq:main}. We relegate the detailed proof of \cref{thm:main} and all intermediate lemmas to \cref{appen:thm_conv} and \ref{appen:lemma}.

\section{Privacy Analysis}
\label{sec:privacy}

\subsection{Threat Model and Assumptions}
While data are kept local in the VFL framework, the communication of local embeddings and backward gradient during the training process, as well as the sharing of model weights post training, can pose threat to sensitive information such as label and features~\cite{papernot2018sok}. In this section, we  talk about how our algorithm protect privacy under these threat models.
We consider two scenarios:
% \begin{enumerate}

\textbf{Honest-but-curious:} In the honest-but-curious threat model, participants adhere strictly to the protocols of VFL without deviating from agreed procedures. However, they may attempt to infer private information from intermediate results exchanged during training. 
% Such attacks can leverage gradients, model outputs, or other data exchanges to infer private attributes.

\textbf{Post-training adversarial:} This model focuses on attacks conducted after the training process. It assumes the attacker lacks access to intermediate results. The adversary only has access to the trained model and aims to infer sensitive information by analyzing the model's behavior or responses. 
% \end{enumerate}
% \begin{itemize}
%     \item 
%     \item 
% \end{itemize}

% We first begin with the definition of Gaussian differential privacy (GDP)~\cite{dong2022gaussian}. Compared with traditional DP defined in \eqref{eq:DP}, this notion of privacy provides a much tighter composition theorem.
% \begin{definition}[Gaussian Differential Privacy]\label{def:GDP}
%     Let $G_\mu:=T(\gN(0,1),\gN(\mu,1))$ for $\mu\ge0$, where the trade-off function $T(P,Q):[0,1]\rightarrow [0,1]$ is defined as $T(P,Q)(\alpha)=\inf(\beta_\phi: \alpha_\phi<\alpha)$.
%     A mechanism $M$ is said to satisfy $\mu$-Gaussian Differential Privacy if it satisfies
%     \[T(M(X),M(X'))\ge G_\mu\] 
%     For all neighboring dataset $X,X'\in \gX^n$
% \end{definition}
% It's easy to see the root of statistical hypothesis testing from the definition of GDP. However, interpretation of GDP can be less straightforward, and we inevitably want to compare it with $(\epsilon, \delta)$-DP. 

\subsection{Differential Privacy Guarantee}
% We now present the main privacy theorem in the form of $(\epsilon, \delta)$-DP:
\begin{theorem}
\label{thm:dp}
Under \cref{assum:Lip}-\ref{assum:ind_part}, suppose the privacy parameter $\sigma_{dp}$ is
\(\sigma_{dp} = \frac{ 2C\sqrt{T}}{D\mu}, \)
where $D$ denotes the volume of the dataset, $T$ defines total iterations, and $\mu>0$ controls the privacy level.
The training process of \cref{alg:DPZV_asyn} is seen to be $(\epsilon,\delta(\epsilon))$-differential private for $\forall \epsilon>0$, where 
\begin{align}\label{eq:dp_delta}\delta(\epsilon)=\Phi(-\frac{\epsilon}{\mu}+\frac{\mu}{2})-e^\epsilon\Phi(-\frac{\epsilon}{\mu}-\frac{\mu}{2})\end{align}
\end{theorem}
The theorem provides privacy guarantee under the "honest-but-curious" threat model, where one or a few malicious clients try to do inference attacks by collecting information of the system. By providing only the differentially private ZO information $\Delta_{m}^{t}$, the algorithm protects for both label inference attacks~\cite{fu2022label} and feature inference attacks~\cite{luo2021feature}, because the attacker cannot differentiate a single datum in the dataset $\gD$ and label set $\mY$. 
The differential privacy parameter $\mu$ is related to $(\epsilon, \delta)$ through the relationship defined in \eqref{eq:dp_delta}. Given any two of these parameters, the third can be determined by solving \eqref{eq:dp_delta}, allowing full flexibility in controlling the privacy level.


Although the forward embeddings $h_{m,i}$ are not protected by differential privacy and may be vulnerable to feature attacks from the server, existing attacks primarily rely on access to model parameters $w_m$ for model inversion~\cite{he2019model,jiang2022comprehensive} or require backward gradients for gradient inversion~\cite{jin2021cafe}. Both attack vectors are mitigated under our differential privacy guarantees. A detailed discussion of our framework's robustness against various attacks is provided in \cref{appen:dp_discuss}, and the detailed proofs can be found in Appendix~\ref{appen:thm_dp}

% \evan{Let's remove the sketch for this moment}
% \textit{Proof Sketch.} We mainly utilize GDP defined in \cref{def:GDP} throughout the proof. We first notice that the privacy-enforcing step in \cref{alg:DPZV_asyn} follows the Gaussian mechanism in GDP, and other steps falls into post-processing. Thus, we can analyze the privacy cost in one communication round. We then leverage the composition theorem in GDP to calculate the global privacy level in $T$ communication rounds. Finally, we use a lossless conversion from GDP to a collection of $(\epsilon,\delta)$-DP to derive the main privacy guarantee. 
% We relegate the detailed proof of \cref{thm:dp} and all intermediate lemmas to \cref{appen:thm_dp} and \ref{appen:lemma}.
 

\section{Experiments}
\begin{table}[b]
\caption{Communication cost comparison. We compare the communication cost (measured in kBs) of one client per communication round for each VFL methods.}
\label{tab:communication}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Method & Forward & Backward & Total \\
\midrule
VAFL   & 65.54 & 65.54 & 131.07 \\
ZOO-VFL & 131.07 & 0.51 & 131.58\\
VFL-CZOFO  & 65.54 & 68.10 & 133.64 \\
DPZV(Ours)    & 65.54 & \textbf{0.51} &  \textbf{66.05}       \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vspace{-0.2in} 
\end{table}
\label{sec:exp}
We conduct extensive experiments on a variety of VFL tasks. We compare {\tt DPZV} against SotA VFL methods under different levels of privacy, and empirically show that our {\tt DPZV} algorithm achieves superior utility and faster convergence even under strict privacy constraints, while requiring less computation and communication resources.
\subsection{Datasets and Experiment Setups}
\textbf{Dataset:} We consider four datasets: 
image dataset 
MNIST~\cite{deng2012mnist}, CIFAR-10~\cite{krizhevsky2009learning}, semantic dataset Amazon Review Polarity~\cite{mcauley2013hidden}, and multi-view dataset ModelNet40~\cite{wu20153d}. For each dataset, we conduct a grid search on learning rates and other hyperparameters. We run 100 epochs on each method and select the best validation model.
% We run each algorithm under three random seeds and compute the sample variance for the generosity of our result. 
\begin{figure}[t!]
    \centering
    % First subplot
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/MNIST.png}  % Replace with your image file
        \caption{MNIST}
        \label{fig:MNIST}
    \end{subfigure}
    % \hfill
    % Third subplot
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ModelNet40.png}  % Replace with your image file
        \caption{ModelNet40}
        \label{fig:ModelNet40}
    \end{subfigure}
     % Second subplot
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/CIFAR10.png}  % Replace with your image file
        \caption{CIFAR-10}
        \label{fig:CIFAR10}
    \end{subfigure}
    % \hfill
    
    \caption{Test Accuracy of VFL Methods on image classification Tasks without DP constraints.
{\tt DPZV} outperforms first-order VFL methods on two datasets and surpasses all other ZO-based methods across all three datasets, showing both a higher accuracy and a faster convergence rate.
    % We attribute this edge to the computation efficiency of ZO optimization, which significantly reduces model delay.
    }
    \label{fig:main}
\vspace{-0.2in} 
    
\end{figure}

\begin{figure}[t!]
    \centering
    % First subplot
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/MNIST_CC.png}  % Replace with your image file
        \caption{MNIST}
        \label{fig:MNIST_CC}
    \end{subfigure}
    % \hfill
    % Second subplot
    % \hfill
    % Third subplot
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ModelNet40_CC.png}  % Replace with your image file
        \caption{ModelNet40}
        \label{fig:ModelNet40_CC}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/CIFAR10_CC.png}  % Replace with your image file
        \caption{CIFAR-10}
        \label{fig:CIFAR10_CC}
    \end{subfigure}
    
    \caption{Comparison with other VFL methods under the same communication cost. The asymmetric communication design results in {\tt DPZV}
    % enables the maximum utility of forward embeddings, 
    outperforming all baselines.}
    \label{fig:communication}
\vspace{-0.2in} 
    
\end{figure}
For MNIST, we use a two-layer CNN model, for VFL's data partitioning, we split the images by row evenly into 7 sub-images and assign them to 7 clients. For CIFAR-10 we use a four-layer CNN model, and partition each image into 2×2, 4 patches of the same size for 4 clients. For ModelNet40, we use a ResNet-18 model, and partition each object into 12 different camera views and allocate them to 12 clients. For Amazon Reviews, we use a pre-trained BERT~\cite{devlin2018bert} model, and split the tokenized data input into 3 paragraphs of the same number of tokens and distributed them across 3 clients.
% \begin{itemize}
%     \item \textbf{MNIST} contains 70,000 grayscale images of handwritten digits (0 through 9), each sized 28×28. To model the VFL setting, we split the images by row evenly into 7 sub-images and assign them to 7 clients. We use a simple CNN with two convolution layers as the client model. 
%     \item \textbf{CIFAR-10} consists of 32×32 color images grouped into 10 distinct classes. We split each image into 2×2, 4 patches of the same size for 4 clients. Each client holds a CNN architecture with 4 convolution layers. 
%     \item \textbf{Amazon Reviews Polarity} is a large-scale text dataset of labeled product reviews (positive or negative) extracted from Amazon, serving as a core benchmark for sentiment analysis in natural language processing. We first tokenize the reviews, then split it into 3 paragraphs of the same number of tokens. The 3 paragraphs are then given to 3 clients, each having a pre-trained Bert\cite{devlin2018bert} model.
%     \item \textbf{ModelNet40} is a curated collection of 3D CAD meshes divided into 40 object categories (e.g., tables, chairs, and airplanes). Each object contains 12 different camera views, which are divided to 12 clients. We use a ResNet-18 \cite{he2016deep} as the local model.
% \end{itemize}
% We briefly introduce the datasets and the machine learning models below.
% \begin{itemize}
%     \item \textbf{MNIST} contains 70,000 grayscale images of handwritten digits (0 through 9), each sized 28×28. To model the VFL setting, we split the images by row evenly into 7 sub-images and assign them to 7 clients. We use a simple CNN with two convolution layers as the client model. 
%     \item \textbf{CIFAR-10} consists of 32×32 color images grouped into 10 distinct classes. We split each image into 2×2, 4 patches of the same size for 4 clients. Each client holds a CNN architecture with 4 convolution layers. 
%     \item \textbf{Amazon Reviews Polarity} is a large-scale text dataset of labeled product reviews (positive or negative) extracted from Amazon, serving as a core benchmark for sentiment analysis in natural language processing. We first tokenize the reviews, then split it into 3 paragraphs of the same number of tokens. The 3 paragraphs are then given to 3 clients, each having a pre-trained Bert\cite{devlin2018bert} model.
%     \item \textbf{ModelNet40} is a curated collection of 3D CAD meshes divided into 40 object categories (e.g., tables, chairs, and airplanes). We use a ResNet-18 \cite{he2016deep} as the local model.
% \end{itemize}
For all four datasets, we use a fully connected model of two linear layers with ReLU activations as the server model. 


\textbf{Baselines:}
We compare our algorithm against several SotA VFL methods: 1) {\tt VAFL}~\cite{chen2020vafl} 2) {\tt ZOO-VFL}~\cite{zhang2021desirable}, 3) {\tt VFL-CZOFO}~\cite{wang2024unified}. All methods assume that the server holds the labels, and concatenates the embeddings of clients as the input of the server. {\tt VAFL} updates its model through first-order optimization in an asynchronous manner.  We use {\tt VAFL} as a first-order baseline of VFL method. Contrary to {\tt VAFL}, {\tt ZOO-VFL} and {\tt ZOFO} both adopt ZO optimization in their training procedure. {\tt ZOO-VFL} was the first attempt to combine ZOO with VFL. It simply substitutes the first order optimization by ZO optimization in common VFL methods, thus enjoying some benefits of ZOO. {\tt VFL-CZOFO} uses a cascade hybrid optimization method. It computes the intermediate gradient via ZOO, while keeping the back propagation on both server and client. 
% Our empirical results suggest that our {\tt DPZV} method outperforms the aforementioned methods, while requiring less communication overhead and memory footprint, due to the memory-efficient ZOO mechanism and asymmetric communication update design.

While {\tt VAFL} updates server and client once for every communication round, some first-order VFL methods adopts multiple local updates in one communication round~\cite{liu2022fedbcd, castiglia2023flexible}. We note that while our {\tt DPZV} adopts an asymmetric communication protocol, it is vertical to update protocols and can be easily modified to have multiple local update steps in one communication round. In this paper, we only focus on the same update behavior as {\tt VAFL} and use {\tt VAFL} as our first-order baseline. For each experiment, we set the total server-client update iterations of {\tt DPZV} to $K = 2$, all choices of step size of each method are shown in Appendix~\ref{appen:lr}.





\begin{table*}[t]
\centering

\caption{Accuracy under different privacy level. Our algorithm consistently outperforms all baselines, and the training performance barely decays when increasing the strictness of the DP guarantee. This shows the robustness and stability of our method against all DP levels.}
\label{table:dp_level}
\vskip 0.1in
\resizebox{0.98\textwidth}{!}{

    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{l ccc ccc ccc ccc}
        \toprule
        \multirow{2}{*}{Algorithm} & 
        \multicolumn{3}{c}{MNIST} & 
        \multicolumn{3}{c}{CIFAR-10} & 
        \multicolumn{3}{c}{ModelNet40}& 
        \multicolumn{3}{c}{Amazon Review}\\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
        & No DP & $\epsilon = 10$ & $\epsilon = 1$
        & No DP & $\epsilon = 10$ & $\epsilon = 1$ 
        & No DP & $\epsilon = 10$ & $\epsilon = 1$ 
        & No DP & $\epsilon = 10$ & $\epsilon = 1$ \\
        \midrule
        VAFL        & 91.61  & 82.14  & 43.11  & 43.74  &  37.19 &36.51  & \textbf{80.34}  & 71.59 & 66.41 & \textbf{85.76}  & 69.70   & 60.30  \\
        ZOO-VFL     & 88.55  & 56.53  & 16.77  & 37.32  & 22.58  & 13.73   & 37.86  & 30.32  & 20.56  & 68.32  & 63.68  & 55.29  \\
        VFL-CZOFO   & 93.82  & 68.93 & 68.83    & 43.06   & 32.52  & 31.73  & 39.09  & 30.61  & 28.47  & 81.87  & 72.03   & 59.99 \\
        \textbf{DPZV (ours)} & \textbf{96.53}  & \textbf{95.36}  & \textbf{94.47}  
                      & \textbf{46.78}  & \textbf{42.63}  & \textbf{42.45}  
                      & 79.64  & \textbf{83.77}  & \textbf{83.51}  
                      & 84.42 & \textbf{81.82}  & \textbf{80.30}  \\
        \bottomrule
    \end{tabular}
}
\vspace{-0.1in}
\end{table*}
\subsection{DPZV Comparison to Baselines}
% \evan{try to finish this before end of 1/29 so we can start refining it.}

Figure~\ref{fig:main} presents the performance evaluation of {\tt DPZV} against all baselines on image classification tasks without DP constraints. To ensure a fair comparison, model delay has been manually adjusted for all methods based on the per-batch computation time on a single client. 
The results show that {\tt DPZV} consistently outperforms all baselines on MNIST and CIFAR-10. On ModelNet40, where training is more challenging due to larger models and a greater number of clients, {\tt DPZV} maintains a competitive advantage over other ZO-based methods while achieving accuracy comparable to the first-order baseline {\tt VAFL}. The clipping operation in {\tt DPZV} effectively controls gradient magnitudes, mitigating the instability caused by noisy ZO gradient estimators. This contributes to improved training stability. Furthermore, the memory-efficient ZO optimization significantly enhances computational efficiency, leading to a faster convergence rate compared to first-order methods.






\textbf{Communication Cost:} 
\cref{tab:communication} shows the communication cost of one client for all algorithms performing forward (client-server) communication and backward (server-client) communication, where the communication cost relies on the dimension of the embeddings and batch size. 
% The total communication cost also scales with the number of clients.
In MNIST, we used an embedding size of $60$, while in CIFAR-10 and ModelNet40, the embedding size is set at $128$. Here, we assume an embedding size of $128$ for comparison.
We can observe from that {\tt DPZV} significantly reduces the backward communication cost from the server to the client compared to {\tt VAFL} and {\tt VFL-CZOFO}, as ZO optimization eliminates the need for backward propagation. 
% While {\tt ZOO-VFL} also benefits from lower backward communication due to ZO optimization, it doubles the communication cost from the client to the server. 
Compare to {\tt ZOO-VFL}, {\tt DPZV} is able to reduce the forwarding communication by leveraging the asymmetric update mechanism.
% Although {\tt VFL-CZOFO} is also based on ZO optimization, it requires sending random perturbations from the server to the client, which are the same size as first-order gradients. 
Our method reduces the total communication cost by nearly half of all existing methods. \textit{This demonstrated the superiority of our method to perform communication efficient training.}
\cref{fig:communication} evaluates the performance of VFL methods under the same communication cost. We observe that {\tt DPZV} achieve a bigger lead compared to all baselines under the same communication cost, \textit{reaching the same level of convergence at a much smaller communication cost}. 

% Higher embedding size usually conveys more information, but at the cost of communication cost. Here, we assume an embedding size of $128$ and a batch size of $128$.

% We report the forward, backward and combined communication cost required for each communication round on average in . Note that communication cost relies on the dimension of the embeddings and batch size. In MNIST, we used an embedding size of $60$, while in CIFAR-10 and ModelNet40, the embedding size is set at $128$. Higher embedding size usually conveys more information, but at the cost of communication cost. Here, we assume an embedding size of $128$ and a batch size of $128$. We can see from the table that while 



% \textbf{Pretrained language models}
% In this part, we show the potential of our {\tt DPZV} algorithm on the deployment of larger models in the VFL scenario, even for large pretrained language models. 
 \begin{figure}[b!]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\textwidth]{images/memory.png}}
\caption{Normalized memory cost in training for each method. {\tt DPZV} requires the smallest memory allocation in both datasets, almost the same as model memory itself. This shows the memory efficiency of {\tt DPZV}, allowing superior performance on large-scale neural networks.}
\label{fig:mem_cost}
\end{center}
\vspace{-0.5in}
\end{figure}
\textbf{Memory Cost:} 
\Cref{fig:mem_cost} 
compares the GPU memory consumption, with memory values normalized for readability. We compare the memory cost on larger models, where we use ResNet for image classification and BERT for sequence classification. We record the highest memory peak to show the total required memory for each method on training large models. 
We observe that {\tt DPZV} requires memory approximately equal to the model size, whereas the first-order method {\tt VAFL} demands more than twice the model size. Compared to {\tt ZOO-VFL}, {\tt DPZV} achieves further memory savings by leveraging {\tt MeZO}. 
% This advantage becomes more pronounced for larger models.
\textit{These results highlight the scalability of {\tt DPZV}, making it well-suited for deploying large pretrained language models in VFL scenarios}.



\subsection{Impact of different DP levels}
\vskip -0.05in
We further compare the utility of {\tt DPZV} with baselines under various DP levels. {\tt VAFL} achieves DP by adding random noise to the output of each local embedding. {\tt VFL-CZOFO} utilize the intrinsic privacy protection of ZO optimization to give DP guarantee. However, the intrinsic DP level sometimes falls below the required level and is not adjustable. {\tt ZOO-VFL} does not give DP guarantee. To enforce DP and compare all methods fairly, we adopt the basic {\tt DP-SGD}~\cite{abadi2016deep} method on {\tt VFL-CZOFO} and {\tt ZOO-VFL} by clipping gradient and adding noise. 

In Table~\ref{table:dp_level} we evaluate three levels of DP guarantees: (1) no DP noise injected, (2) noise injected to ensure $\epsilon = 10$, and (3) noise injected to ensure $\epsilon = 1$. As $\epsilon$ decreases, the performance of \texttt{VAFL} and \texttt{ZOO-VFL} deteriorates significantly. While \texttt{VFL-CZOFO} exhibits a relatively smaller accuracy drop compared to \texttt{VAFL} and \texttt{ZOO-VFL}, the reduction remains substantial, hindering the system from achieving an optimal model. In contrast, our \texttt{DPZV} maintains consistently high performance even under strict privacy budgets ($\epsilon\le1$), with only minor deviations from its No DP accuracy.  These results show the advantage of \texttt{DPZV}, where \textit{memory-efficient zeroth-order updates significantly enhance the training stability of DP-protected systems.}



% We show that our carefully designed DP mechanism achieves high accuracy even under tight privacy budget, while simply applying {\tt DP-SGD} cannot.


% \subsection{Memory and Computation Cost}
\section{Conclusion}
\vskip -0.05in
In this work, we propose {\tt DPZV}, the first zero-order VFL framework that achieves low communication overhead, reduced computational cost, and robust privacy guarantees simultaneously. Through rigorous theoretical analysis, we establish the strong convergence properties of our algorithm. Extensive experiments demonstrate that {\tt DPZV} outperforms all baselines across various privacy levels and cost evaluations. Our method also features a lightweight memory footprint, enabling enhanced performance on large language models. These results provide valuable insights for future advancements in VFL and distributed ZO optimization methods.



\bibliographystyle{unsrt}  
\bibliography{references}
\newpage
\appendix
\section{Preliminaries and Outline}
We first define the following notation table to facilitate the proof: 
% We define ... as $\Xi$:
% \begin{equation}
%     \Xi = 
% \end{equation}
% We define ... as $\mathcal{E}$:
% \begin{equation}
%     \mathcal{E} = 
% \end{equation}
% We define ... as $\Gamma$:
% \begin{equation}
%     \Gamma = 
% \end{equation}
\begin{table}[ht]
    \centering
    % Adjust the column specifications (e.g., l, c, r, p{width}) as needed
    \begin{tabular}{p{0.45\textwidth} p{0.5\textwidth}}
    \toprule
    \textbf{Notation} & \textbf{Description} \\
    \midrule
    $\vw=[w_0,w_1,w_2,\dots,w_M]$ & All learnable parameters\\
    $d_0, d_1,\dots,d_M$ & Dimension of parameters on server (machine $0$) and client $1,\dots,M$\\
    $d=\sum_{m=0}^{M}d_m$ & Dimension of all parameters\\
    $f(\vw;\xi_i):=\gL(w_0, h_{1,i}, h_{2,i},\dots, h_{M,i};y_i)$ & Loss function with regard to datum with ID $i$\\
    $F(\vw):=F(\vw;\gD,\mY)$ & Global loss function\\
    $\chi^t = [w_1^{t_1}, \dots, w_M^{t_M}]$ & Latest learnable parameters of all clients at server time $t$\\
    $\Tilde{\chi}^t = [w_1^{t_1-\tau_1^t}, \dots, w_M^{t_M-\tau_M^t}]$ & Delayed learnable parameters of all clients at server time $t$\\
    $\vw^t =[w_0^{t}, w_1^{t_1}, \dots, w_M^{t_M}]$ & Latest learnable parameters of all clients and the server at server time $t$ \\
    $\Tilde{\vw}^t =[w_0^{t-\tau_1^t}, w_1^{t_1-\tau_1^t}, \dots, w_M^{t_M-\tau_M^t}]$ & Delayed learnable parameters of all clients and the server at server time $t$ \\
    $h_{m,i}^{t_m\pm}=h_{m}(w_m^{t_m}\pm\lambda_m \vu_{m}^{t_m};\xi_{m,i})$ & Local embeddings of client $m$ for data sample $i$ at client time $t_m$ under the perturbed parameters \\
    $\delta_{m,i}^{t,t_m}$ as defined in \eqref{eq:delta} & Zeroth-order difference information from client $m$ and data sample $i$ at server time $t$ and client time $t_m$\\
    $g^t_{m,i}(\Tilde{\vw}^t) = \delta_{m,i}^{t,t_m} \vu_{m}^{t_m}$ & Zeroth-order gradient estimator from client $m$ and data sample $i$ at server time $t$ (with delay)\\
    % $g^t_{m,i}(\vw^t)$ & Zeroth-order gradient estimator from client $m$ and data sample $i$ at server time $t$ (without delay)\\
    $\breve{g}^t_{m,i}(\Tilde{\vw}^t) = \mathrm{clip}_C\left(\delta_{m,i}^{t,t_m}\right)\vu_m^{t_m}$ & Clipped zeroth-order gradient estimator from client $m$ and data sample $i$ at server time $t$ (with delay)\\
    $\breve{G}^t_{m}(\Tilde{\vw}^t) = (1/B)\sum_{i\in \gI_{m}^{t_m}}\breve{g}^t_{m,i}(\Tilde{\vw}^t)+z_{m}^{t_m}\vu_m^{t_m}$ & Clipped differential private zeroth-order gradient estimator from client $m$ at server time $t$ (with delay)\\ 
    $G^t_{m}(\Tilde{\vw}^t) =(1/B) \sum_{i\in \gI_{m}^{t_m}}g^t_{m,i}(\Tilde{\vw}^t)+z_{m}^{t_m}\vu_m^{t_m}$ & Non-clipped differential private zeroth-order gradient estimator from client $m$ at server time $t$ (with delay)\\ 
    \bottomrule
    \end{tabular}
    \caption{Table of Notations}
    \label{tab:notation}
\end{table}


Note that in the notation table, we use ``$\breve{\phantom{x}}$" to define clipped gradient estimators, and we use ``$\Tilde{\phantom{x}}$" to denote delayed model parameters. In the rest of the proof, we also use gradient estimators parameterized by the no delaying parameters $\vw$ instead of $\Tilde{\vw}$ to assume that we update the model without delay.

To begin with, we restate the assumptions required for establishing the convergence analysis.
\begin{assumption}[$\ell$-Lipschitz]\label{assum:Lip_full}
    The function $f(\vw;\xi)$ is $\ell$-Lipschitz continuous for every $\xi$. 
\end{assumption}
\begin{assumption}[$L$-Smooth]\label{assum:smooth_full}
    The function $f(\vw;\xi)$ is $L$-Smooth for every $\xi$. Specifically, there exists an $L>0$ for all $m=0, \dots, M$ such that $\norm{\nabla_{w_m}f(\vw)-\nabla_{w_m}f(\vw')}\le L \norm{\vw-\vw'}$.
\end{assumption}
\begin{assumption}[Bounded gradient variance]\label{assum:bound_full}
    The variance of the stochastic first order gradient is upper bounded in expectation:
    \[\E\left[\norm{\nabla_{\vw}f(\vw;\xi)-\nabla_{\vw}F(\vw)}^2\right]\le \sigma_s^2\]
\end{assumption}
\begin{assumption}[Independent Participation]\label{assum:ind_part_full}
    The probability of one client participating in one communication round is independent of other clients and satisfies
    \[\Prob(\text{client }m\text{ uploading}) = q_m \]
    Specially, we set $q_0=1$ as the server always participates in the update.
\end{assumption}
% We will also find the following results useful in the proofs:
% \begin{fact}
    
% \end{fact}
% \begin{fact}
    
% \end{fact}
One of the important parts in the proof of \cref{thm:main} is to bound the zeroth-order gradient estimator. We first introduce the formal definition of zeroth-order two-point gradient estimator that is used in our algorithm, and prove some technical lemmas that reveal some important properties.
\begin{definition}
    Let $\vu$ be uniformly sampled from the Euclidean sphere $\sqrt{d} \mathbb{S}^{d-1}$. For any function $f(x) : \mathbb{R}^d \to \mathbb{R}$ and $\lambda > 0$, we define its zeroth-order gradient estimator as 
\begin{align}\label{eq:zero_def}
    g(\vw) = \frac{(f(\vw + \lambda \vu) - f(\vw - \lambda \vu))}{2\lambda} \vu
\end{align}
\end{definition}
\begin{lemma}\label{lem:zero}
Let $g(\vw)$ be the zeroth-order gradient estimator defined as in \eqref{eq:zero_def}, with $f(\vw)$ being the loss function. We define the smoothed function $f_\lambda(\vw) = \E_{\vu}[f(\vw + \lambda \vu)]$, where $v$ is uniformly sampled from the Euclidean ball $\sqrt{d} \mathbb{B}^d = \{\vw \in \mathbb{R}^d \mid \norm{\vw} \leq \sqrt{d}\}$. The following properties hold:
\begin{enumerate}[label=$(\roman*)$]
    \item $f_\lambda(\vw)$ is differentiable and $\E_\vu[g(\vw)] = \nabla f_\lambda(\vw)$.
    \item If $f(\vw)$ is $L$-smooth, we have that
    \begin{align}
        \norm{\nabla f(\vw) - \nabla f_\lambda(\vw)} \le \frac{L}{2} \lambda d^{3/2},\label{eq:grad_diff}
    \end{align}
    \begin{align}
        | f(\vw) -  f_\lambda(\vw)| \le \frac{L}{2} \lambda^2 d,\label{eq:func_diff}
    \end{align}
    and
    \begin{align}
    \E_\vu[\norm{g_\lambda(\vw)}^2] \le 2d \cdot \norm{\nabla f(\vw)}^2 + \frac{L^2}{2} \lambda^2 d^3.\label{eq:zero_grad}
    \end{align}
\end{enumerate}
\end{lemma}
% \begin{remark}
Based on \eqref{eq:func_diff}, we can further show:
    \begin{align}
        \norm{\nabla f_\lambda(\vw)}^2\le 2\norm{\nabla f(\vw)}^2 +\frac{L^2}{2} \lambda^2 d^3\label{eq:func_squared_1}
    \end{align}
    \begin{align}
        \norm{\nabla f(\vw)}^2\le 2\norm{\nabla f_\lambda(\vw)}^2 +\frac{L^2}{2} \lambda^2 d^3\label{eq:func_squared_2}
    \end{align}
% \end{remark}
This is the standard result of zeroth-order optimization. The proof of the Lemma is given by \cite{nesterov2017random}

We also find the following lemmas useful in the proof:
\begin{lemma}\label{lem:u}
Let $\vu$ be uniformly sampled from the Euclidean sphere $\sqrt{d}S^{d-1}$, and $\va$ be any vector of constant value.
We have that $\E[\vu] = 0$ and
    \[
        \Prob(|\vu^\top \va| \ge C) \le 2\sqrt{2\pi} \exp\left(-\frac{C^2}{8\norm{\va}^2}\right).
    \]
\end{lemma}
\begin{proof}
    This lemma follows exactly from Lemma C.1. in \cite{zhang2024dpzero}.
\end{proof}


\begin{lemma}\label{lem:Q}
    Let $Q$ be the event that clipping happened for a sample $\xi$, $d$ be the model dimension, and $L,\ell$ be the Lipschitz and smooth constant as defined in \cref{assum:Lip_full} and \cref{assum:smooth_full}. For $\forall C_0>0$, if the clipping threshold $C$ follows $C\ge C_0+L\lambda d/2$, we have the following upper bound for the probability of clipping:
    \begin{align}
        P = \Prob(Q)\le 2\sqrt{2\pi}\exp(-\frac{C_0^2}{8\ell^2})=\Xi
    \end{align}
\end{lemma}
\begin{proof}
Since $f(\vu;\xi)$ is $L$-Smooth for every $\xi$, we have
\begin{align*}
\frac{|f(\vw + \lambda  \vu; \xi) - f(\vw - \lambda  \vu; \xi)|}{2\lambda}
&\le
|u^\top \nabla f(\vu; \xi)|
+
\frac{|f(\vw + \lambda  \vu; \xi) - f(\vw; \xi) 
      - \lambda  \vu^\top \nabla f(\vw; \xi)|}{2\lambda}\\
&+
\frac{|f(\vw - \lambda  \vu; \xi) - f(\vw; \xi)
      + \lambda  \vu^\top \nabla f(\vw; \xi)|}{2\lambda}\\
&\le
|\vu^\top \nabla f(\vw; \xi)| +\frac{L}{2}\lambda.
\end{align*}
Therefore, by \cref{lem:u} and \cref{assum:Lip_full}, we obtain
\begin{align*}
\Prob(Q)&=\Prob(
  \frac{| f(\vw + \lambda \vu; \xi_i) - f(\vw - \lambda  \vu; \xi_i)|}
       {2\lambda}
  \ge C_0 + \frac{L}{2}\lambda 
)\\
\le&
\Prob(|\vu^\top \nabla f(\vw; \xi_i)|\ge C_0)\\
\le&
2\sqrt{2\pi}\exp(-\frac{C_0^{2}}{8\|\nabla f(\vw; \xi_i)\|^{2}})\\
\le&
2\sqrt{2\pi}\exp(-\frac{C_0^{2}}{8\ell^{2}}).
\end{align*}
\end{proof}


\begin{lemma}[Expectation and Variance of Clipped Zeroth-order Gradient Estimator]Recall that  $\breve{g}^t_{m,i}(\vw^t)$ is defined as the clipped zeroth-order gradient estimator assuming no communication delay, random perturbation $\vu$ is defined in \cref{lem:u}, and event $Q$ is defined in  \cref{lem:Q}. We have the following properties:
\begin{enumerate}[label=$(\roman*)$]
\item When taking expectation w.r.t $\vu$ and $Q$, the clipped zeroth-order gradient estimator follows \begin{align}
    \E_\vu[\breve{g}^t_{m,i}(\vw^t)] = (1-P)\nabla_{w_m} F_\lambda(\vw^t)\label{eq:mean}
\end{align}
\item The variance of $\breve{g}^t_{m,i}(\vw^t)$ follows
\begin{align*}
    \Var(\breve{g}^t_{m,i}(\vw^t))\le& (1-P)(2d_m \norm{\nabla_{w_m} F(\vw^t)}^2 + 2d_m\sigma_s^2 + \frac{L^2}{2} \lambda^2 d_m^3 )
    +P C^2 d_m-(1-P)^2\norm{\nabla_{w_m} F_\lambda(\vw^t)}^2\numbereq\label{eq:var}
\end{align*}
\end{enumerate}
\end{lemma}
\begin{proof}
    For $(i)$, we have
\begin{align*}
    \E\left[\breve{g}^t_{m,i}(\vw^t)\right] =&\E\left[\breve{g}^t_{m,i}(\vw^t)|\bar{Q}\right]\Prob(\bar{Q})+ \E\left[\breve{g}^t_{m,i}(\vw^t)|Q\right]\Prob(Q)\\
    =&\E\left[\Tilde{g}^t_{m,i}(\vw^t)\right](1-\Prob(Q))+ \E\left[C\vu_m^t\right]\Prob(Q)\\
    =&(1-P)\nabla_{w_m} F_\lambda(\vw^t)
\end{align*}
where in the first step we applied the Law of Total Expectation, and in the last step we used the property $(i)$ in \cref{lem:zero} and $(i)$ in \cref{lem:u}.\\
By \eqref{eq:mean}, we can further bound the variance of $\breve{g}^t_{m,i}$
\begin{align*}
    &\Var(\breve{g}^t_{m,i}(\vw^t))\\
    =&\E\left[\norm{\breve{g}^t_{m,i}(\vw^t)-(1-P)\nabla_{w_m} F_\lambda(\vw^t)}^2\right] \\
    =&\E\left[\norm{\breve{g}^t_{m,i}(\vw^t)}^2\right]-(1-P)^2\norm{\nabla_{w_m} F_\lambda(\vw^t)}^2 \\
    =&\E\left[\norm{\Tilde{g}^t_{m,i}(\vw^t)}^2|\bar{Q}\right]\Prob(\bar{Q})+\E\left[C^2 \norm{\vu_m^t}^2|Q\right]\Prob(Q)-(1-P)^2\norm{\nabla_{w_m} F_\lambda(\vw^t)}^2 \\
    \overset{1)}{\le} &(1-P)(2d_m \norm{\nabla_{w_m} f(\vw^t;\xi_{m,t})}^2 + \frac{L^2}{2} \lambda^2 d_m^3 )\\
    +&P C^2 d_m-(1-P)^2\norm{\nabla_{w_m} F_\lambda(\vw^t)}^2 \\
    \overset{2)}{\le} &(1-P)(2d_m \norm{\nabla_{w_m} F(\vw^t)}^2 + 2d_m \sigma_s^2 + \frac{L^2}{2} \lambda^2 d_m^3 )\\
    +&P C^2 d_m-(1-P)^2\norm{\nabla_{w_m} F_\lambda(\vw^t)}^2 \\
\end{align*}
where $1)$ is by the property of zeroth-order gradient estimator \eqref{eq:zero_grad} and $2)$ follows from the bounded gradient assumption(Assumption \ref{assum:bound_full}).
\end{proof}


\begin{lemma}[Bounds on the variance of DP Zeroth-order Gradient Estimator]Let $\breve{G}^t_{m}(\vw^t)$ be the Differential Private Zeroth-order Gradient without delay. Under the same condition as \cref{lem:zero}, we can bound the variance of $\breve{G}^t_{m}(\vw^t)$ in expectation, with the expectation taken on random direction $\vu$, DP noise $z$, and clipping event $Q$: 
\begin{align*}
\Var(\breve{G}^t_{m}(\vw^t))\le&\frac{1-P}{B}(2d_m \E\left[\norm{\nabla_{w_m} F(\vw^t)}^2\right] + 2d_m\sigma_s^2 + \frac{L^2}{2} \lambda^2 d_m^3 )+\frac{P}{B} C^2 d_m\\
&-\frac{(1-P)^2}{B}\E\left[\norm{\nabla_{w_m} F_\lambda(\vw^t)}^2\right]+\sigma_{dp}^2 d_m
\numbereq\label{eq:G_var}
\end{align*}
\label{lem:var}
\end{lemma}
\begin{proof}
    First we show that the expectation on $\breve{G}^t_{m}(\vw^t)$ can be written as:
    $$\E_{u}[\breve{G}^t_{m}(\vw^t)]=\frac{1}{B}\sum_{i\in \gI_{m}^{t_m}} \E\left[\breve{g}^t_{m,i}(\vw^t)\right]+\E\left[z^t_{m}\vu^t_m\right] = (1-P)\nabla_{w_m} F_\lambda(\vw^t)$$
\\
Thus, the variance can be bounded by:
\begin{align*}
    &\Var(\breve{G}^t_{m}(\vw^t))\\
    =&\E\left[\norm{\breve{G}^t_{m}(\vw^t)-(1-P)\nabla_{w_m} F_\lambda(\vw^t)}^2\right]\\
    =&\E\left[\norm{\frac{1}{B}\sum_{i\in \gI_{m}^{t_m}} \left(\breve{g}^t_{m,i}(\vw^t)-(1-P)\nabla_{w_m} F_{\lambda}^t(\vw^t)\right)+z^t_{m}\vu^t_m}^2\right]\\
    =&\frac{1}{B^2}\sum_{i\in \gI_{m}^{t_m}}\E\left[\norm{\breve{g}^t_{m,i}(\vw^t)-(1-P)\nabla_{w_m} F_{\lambda}^t(\vw^t)}^2\right]+\E\left[\norm{z^t_{m}\vu^t_m}^2\right]\\
    \overset{(a)}{\leq} &\frac{1-P}{B}(2d_m \E\left[\norm{\nabla_{w_m} F(\vw^t)}^2\right] + 2d_m\sigma_s^2 + \frac{L^2}{2} \lambda^2 d_m^3 )+\frac{P}{B} C^2 d_m-\frac{(1-P)^2}{B}\E\left[\norm{\nabla_{w_m} F_\lambda(\vw^t)}^2\right]+\sigma_{dp}^2 d_m,
\end{align*}
where $(a)$ follows from \eqref{eq:var}.
\end{proof}


Finally, for analyzing the delayed information in model parameter, we define the following Lyapunov function:
\begin{align}\label{eq:lyap}
    V^t = F(\vw^t)+\sum_{i=1}^{\tau}\gamma_i\norm{\chi^{t+1-i}-\chi^{t-i}}^2
\end{align}
    with $\gamma_i$ to be determined later. \\
    

\section{Proof of Convergence \cref{thm:main}}
\label{appen:thm_conv}
We first state the full version of the main convergence \cref{thm:main}.
\begin{theorem}
Under \cref{assum:Lip_full} to \ref{assum:ind_part_full}, and assume the client delay $\tau_m$ is uniformly upper bounded by $\tau$. If we denote $q_* = \min_m q_m$, $d_*=\max_m d_m$, and let $\eta_0 = \eta_m = \eta\le \min\{\frac{1}{\sqrt{Td_*}}, \frac{B}{4L(B+8d_0)+8\gamma_1(2d_m+B)}\}$, $\lambda \le \frac{1}{Ld\sqrt{T}}$, we have the following theorem:
\begin{align*}
    \frac{1}{T} \sum_{t=0}^{T-1}\E\left[\norm{\nabla_{\vw}F(\vw^t)}^2\right]
    \le &\frac{1}{1-\Xi}\left\{\frac{8\sqrt{d_*}}{q_* T^{1/2}}\E\left[F^{0}-F^{T}\right] + \frac{2d_*}{q_* T}+ \frac{8\sqrt{d_*}}{q_*Ld T^{1/2}}+\frac{16 (4L+2\gamma_1)\sqrt{d_*}\sigma_s^2}{q_* BT^{1/2}}\right.\\
    +&\left.\frac{4 \left((4-B)L+(B+2)\gamma_1\right)\sqrt{d_*}}{q_*B T^{1/2}}+\frac{16 (2L+\gamma_1)\Xi C^2 \sqrt{d_*}}{q_* BT^{1/2}}+ \frac{16 (2L+\gamma_1)\sigma_{dp}^2 d_m}{q_*T^{1/2}\sqrt{d_*}}\right\},\numbereq
\end{align*}
where $\Xi = 2\sqrt{2\pi}\exp(-\frac{C_0^2}{8\ell^2}).$
% and the convergence rate is seen to be 
% \[\gO(\frac{\sqrt{d_*}+\sigma_{dp}^2 d_m/\sqrt{d_*}}{T^{1/2}}).\]

\end{theorem}
\begin{proof}
    We start from the result of an intermediate \cref{lem:main_2}, which quantifies the descent of the Lyapunov function, and we relegate the proof to \cref{appen:lemma}. 
    \begin{align}\label{eq:interm}
        \E\left[V^{t+1}-V^{t}\right]\le-\frac{1-P}{8}\min_m\{q_m\eta_m\} \E\left[\norm{\nabla_{\vw}F(\vw^t)}^2\right]+\gA_1 + \gA_2
    \end{align}
    Note that $\gA_1$ and $\gA_2$ are constant terms defined in \eqref{eq:A1} and \eqref{eq:A2}.

    We first re-arranging the terms in \eqref{eq:interm}, and take average over $0,1,\dots, T-1$:
\begin{align*}
    &\frac{1-P}{8T}\min_m\{q_m\eta_m\} \sum_{t=0}^{T-1}\E\left[\norm{\nabla_{\vw}F(\vw^t)}^2\right]\\
    \le& \frac{1}{T}\E\left[V^{0}-V^{T}\right] + \gA_1 + \gA_2\\
    \le& \frac{1}{T}\E\left[F^{0}-F^{T}\right] + \gA_1 + \gA_2,
\end{align*}
where the second inequality follows from the definition of $V^t$ in \eqref{eq:lyap}.

Dividing $\alpha =\frac{1}{8}\min_m\{q_m\eta_m\}$ from both sides, and plugging in $\gA_1$ and $\gA_2$, we have:
\begin{align*}
    &\frac{1-P}{T} \sum_{t=0}^{T-1}\E\left[\norm{\nabla_{\vw}F(\vw^t)}^2\right]\\
    \le &\frac{1}{\alpha T}\E\left[F^{0}-F^{T}\right] + \frac{1}{\alpha}\sum_{m=0}^M q_m\eta_m \frac{L^2}{8}\lambda^2 d_m^3+ \frac{1}{\alpha}\sum_{m=0}^M q_m \eta_m^2 L \left(\frac{4}{B}d_m\sigma_s^2 + \frac{(4-B)L^2}{4B} \lambda^2 d_m^3 +\frac{2P}{B} C^2 d_m+2\sigma_{dp}^2 d_m\right)\\
    &+ \frac{1}{\alpha}L\lambda^2 d +\frac{1}{\alpha}\sum_{m=1}^{M}q_m\eta_m^2\gamma_1\left(\frac{L^2}{4}\lambda^2 d_m^3+\frac{2}{B}d_m\sigma_s^2 + \frac{L^2}{2B} \lambda^2 d_m^3 +\frac{P}{B} C^2 d_m+\sigma_{dp}^2 d_m\right)
\end{align*}
For the simplicity of analysis, we let $\eta_0 = \eta_m = \eta$, $q_* = \min_m q_m$, then $\alpha =\frac{\eta}{8}q_*$. Let $d_*=\max_m d_c$, and $\lambda \le \frac{1}{Ld\sqrt{T}}$. Thus, 
\begin{align*}
    &\frac{1-P}{T} \sum_{t=0}^{T-1}\E\left[\norm{\nabla_{\vw}F(\vw^t)}^2\right]\\
    \le &\frac{8}{q_*\eta T}\E\left[F^{0}-F^{T}\right] + \frac{2d_*^3/d^2}{q_* T}+ \frac{16\eta L}{q_*}  \left(\frac{4}{B}d_*\sigma_s^2 + \frac{(4-B)d_*^3/d^2}{4BT}  +\frac{2P}{B} C^2 d_*+2\sigma_{dp}^2 d_*\right)\\
    +& \frac{8}{q_*Ld\eta T} +\frac{16\eta \gamma_1}{q_*}\left(\frac{2}{B}d_*\sigma_s^2 + \frac{(B+2)d_*^3/d^2}{4BT} +\frac{P}{B} C^2 d_*+\sigma_{dp}^2 d_m\right)\\
    \le & \frac{8}{q_*\eta T}\E\left[F^{0}-F^{T}\right] + \frac{2d_*}{q_* T}+ \frac{8}{q_*Ld\eta T}+\frac{16\eta (4L+2\gamma_1)d_*\sigma_s^2}{q_* B}+\frac{4\eta \left((4-B)L+(B+2)\gamma_1\right)d_*}{q_*B T}\\
    +&\frac{16\eta (2L+\gamma_1)P C^2 d_*}{q_* B}+ \frac{16\eta (2L+\gamma_1)\sigma_{dp}^2 d_*}{q_*}
\end{align*}
where in the last step, we use the fact that $d > d_*$.\\
If we choose $\eta = \frac{1}{\sqrt{Td_*}}$, we can get the convergence rate:
\begin{align*}
    &\frac{1}{T} \sum_{t=0}^{T-1}\E\left[\norm{\nabla_{\vw}F(\vw^t)}^2\right]\\
    \le &\frac{1}{1-P}\left\{\frac{8\sqrt{d_*}}{q_* T^{1/2}}\E\left[F^{0}-F^{T}\right] + \frac{2d_*}{q_* T}+ \frac{8\sqrt{d_*}}{q_*Ld T^{1/2}}+\frac{16 (4L+2\gamma_1)\sqrt{d_*}\sigma_s^2}{q_* BT^{1/2}}\right.\\
    +&\left.\frac{4 \left((4-B)L+(B+2)\gamma_1\right)\sqrt{d_*}}{q_*B T^{1/2}}+\frac{16 (2L+\gamma_1)P C^2 \sqrt{d_*}}{q_* BT^{1/2}}+ \frac{16 (2L+\gamma_1)\sigma_{dp}^2 
    \sqrt{d_*}}{q_*T^{1/2}}\right\}\\
    \le &\frac{1}{1-\Xi}\left\{\frac{8\sqrt{d_*}}{q_* T^{1/2}}\E\left[F^{0}-F^{T}\right] + \frac{2d_*}{q_* T}+ \frac{8\sqrt{d_*}}{q_*Ld T^{1/2}}+\frac{16 (4L+2\gamma_1)\sqrt{d_*}\sigma_s^2}{q_* BT^{1/2}}\right.\\
    +&\left.\frac{4 \left((4-B)L+(B+2)\gamma_1\right)\sqrt{d_*}}{q_*B T^{1/2}}+\frac{16 (2L+\gamma_1)\Xi C^2 \sqrt{d_*}}{q_* BT^{1/2}}+ \frac{16 (2L+\gamma_1)\sigma_{dp}^2 
    \sqrt{d_*}}{q_*T^{1/2}}\right\}
\end{align*}
where the last step follows by \cref{lem:Q}.

We thus conclude that the convergence rate is 
\[\gO(\frac{d_*^{1/2}\E\left[F^{0}-F^{T}\right]+d_*^{1/2}\sigma_s^2+d_*^{1/2}\sigma_{dp}^2}{T^{1/2}}),\]
where constants before terms have been omitted for simplicity. We note that our convergence rate is of the same order compared to centralized ZO optimization under the same smoothness assumption, up to an error term $\sigma_{dp}$ introduced by Differential Privacy.
\end{proof}


\section{Proof of Differential Privacy \cref{thm:dp}}
\label{appen:thm_dp}
In this section, we give rigorous proof for the differential privacy guarantee in \cref{thm:dp}. 

We first introduce the definition of Gaussian differential privacy (GDP)~\cite{dong2022gaussian} which will be useful in the proof for \cref{thm:dp}. Compared with traditional DP defined in \eqref{eq:DP}, this notion of privacy provides a much tighter composition theorem.
\begin{definition}[Gaussian Differential Privacy]\label{def:GDP}
    Let $G_\mu:=T(\gN(0,1),\gN(\mu,1))$ for $\mu\ge0$, where the trade-off function $T(P,Q):[0,1]\rightarrow [0,1]$ is defined as $T(P,Q)(\alpha)=\inf(\beta_\phi: \alpha_\phi<\alpha)$.
    A mechanism $M$ is said to satisfy $\mu$-Gaussian Differential Privacy if it satisfies
    \[T(M(X),M(X'))\ge G_\mu\] 
    For all neighboring dataset $X,X'\in \gX^n$.
\end{definition}
We construct our DP algorithm based on the Gaussian Mechanism. The Gaussian Mechanism of GDP is given by the following theorem.
\begin{theorem}[Gaussian Mechanism for Gaussian Differential Privacy]\label{thm:gm}
    Define the Gaussian mechanism that operates on a statistic $\theta$ as $M(\theta)=\theta(X)+\sigma$, where $\sigma\sim\gN(0,r^2C_{\theta}^2/\mu^2)$, $r$ is the sample rate for a single datum, and $C_{\theta}$ is the $L_2$ sensitivity of $X$. Then, $M$ is $\mu$-GDP.
\end{theorem}
The iterative nature of gradient-like algorithms calls for the composition theorem.
\begin{theorem}[Composition of Gaussian Differential Privacy]\label{thm:composition}
The $T$-fold composition of $\mu$-GDP mechanisms is $\sqrt{T}\mu$-GDP
\end{theorem}
For the ease of comparison, we convert GDP to the common $(\epsilon,\delta)$-DP based on the following lossless conversion:
\begin{theorem}[Conversion from Gaussian Differential Privacy to $(\epsilon, \delta)$-Differential Privacy]\label{thm:conversion}
    A mechanism is $\mu$-GDP iff it is $(\epsilon, \delta(\epsilon))$-DP for all $\epsilon\ge0$, where
    \[\delta(\epsilon)=\Phi(-\frac{\epsilon}{\mu}+\frac{\mu}{2})-e^\epsilon\Phi(-\frac{\epsilon}{\mu}-\frac{\mu}{2})\]
\end{theorem}
The proof of \cref{thm:gm}-\ref{thm:conversion} is given in \cite{dong2022gaussian}.

We are now ready to present the proof for \cref{thm:dp}.

\textbf{Proof of \cref{thm:dp}}

\begin{proof}
    First recall the definition of $\Delta_{m}^{t}$ defined in \eqref{eq:Delta}:
    \[\Delta_{m}^{t} = \frac{1}{B}\sum_{i \in \gI_{m}^{t_m}} \mathrm{clip}_C(\delta_{m,i}^{t,t_m})+z_{m}^{t}\]
    For a pair of neighboring dataset $X,X'$ differing in only one entry of data, the $L_2$ sensitivity $C_{L}$ of $\frac{1}{B}\sum_{i \in \gI_{m}^{t_m}} \mathrm{clip}_C(\delta_{m,i}^{t,t_m})$ follows by
    \begin{align*}
       C_L =  \norm{\frac{1}{B}\sum_{i \in \gI_{m}^{t_m}} \mathrm{clip}_C(\delta_{m,i}^{t,t_m})}_2
        \le\frac{1}{B}\norm{\mathrm{clip}_C(\delta_{m,i}^{t,t_m})}_2
        \le\frac{2C}{B}
    \end{align*}
    The sample rate $r$ of a single data is seen to be the batch size $B$ divided by the size of the dataset $D$:
    \[r = \frac{B}{D}\]
    Note that in \cref{thm:dp}, the standard variance of $z_m^t$ is given by 
    \begin{align*}
        \sigma_{dp} = \frac{ 2C\sqrt{T}}{D\mu}=\frac{ (B/D) (2C/B) \sqrt{T}}{\mu}=\frac{rC_L}{(\mu/\sqrt{T})}
    \end{align*}
    By \cref{thm:gm}, the mechanism conducted in \eqref{eq:Delta} satisfies $(\mu/\sqrt{T})$-Gaussian Differential Privacy. Further applying the composition of GDP in \cref{thm:composition}, and by the post-processing~\cite{dwork2014algorithmic} of differential privacy, we have that the whole training process of \cref{alg:DPZV_asyn} is $\mu$-GDP. We complete the proof by converting $\mu$-GDP to $(\epsilon,\delta)$-DP according to \cref{thm:conversion}.
\end{proof}


\section{Intermediate Lemmas}
\label{appen:lemma}
\begin{lemma}[Model Update With Delay]\label{lem:main_1}
     Under \cref{assum:Lip_full} to \ref{assum:ind_part_full}, we have the following lemma:
     \begin{align*}
         \E\left[F(\vw^{t+1})-F(\vw^{t}) \right] \le& -\sum_{m=0}^M q_m\eta_m(1-P)\left(\frac{1}{4}-\frac{\eta_m L (B+8d_m)}{2B}\right)\E\left[\norm{\nabla_{w_{m}}F(\vw^t)}^2\right]\\
         &+\sum_{m=0}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2\right)\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\gA_1\numbereq
     \end{align*}
\end{lemma}
\begin{proof}
    By \cref{assum:smooth_full}:
    \begin{align*}
F_\lambda(\vw^{t+1})\le&F_\lambda(\vw^{t})+\langle \nabla_{\vw}F_\lambda(\vw^t), \vw^{t+1}-\vw^t \rangle+\frac{L}{2}\norm{\vw^{t+1}-\vw^t}^2\\
=& F_\lambda(\vw^{t})-\eta_0 \langle \nabla_{w_0}F_\lambda(\vw^t), \breve{G}^t_{0}(\Tilde{\vw}^t)\rangle + \frac{L\eta_0^2}{2}\norm{\breve{G}^t_{0}(\Tilde{\vw}^t)}^2-\eta_m \langle \nabla_{w_m}F_\lambda(\vw^t), \breve{G}^t_{m}(\Tilde{\vw}^t)\rangle + \frac{L\eta_m^2}{2}\norm{\breve{G}^t_{m}(\Tilde{\vw}^t)}^2\\
\E\left[F_\lambda(\vw^{t+1})\right]
    \le& \E\left[F_\lambda(\vw^{t})\right]\underbrace{-\eta_0 \E\langle \nabla_{w_0}F_\lambda(\vw^t), \breve{G}^t_{0}(\Tilde{\vw}^t)\rangle}_{\gE_1} + \underbrace{\frac{L\eta_0^2}{2}\E\left[\norm{\breve{G}^t_{0}(\Tilde{\vw}^t)}^2\right]}_{\gE_2}\underbrace{-\eta_{m_k} \E\langle \nabla_{w_{m_k}}F_\lambda(\vw^t), \breve{G}^t_{m_k}(\Tilde{\vw}^t)\rangle}_{\gE_3}\\
    &+ \underbrace{\frac{L\eta_{m_k}^2}{2}\E\left[\norm{\breve{G}^t_{m_k}(\Tilde{\vw}^t)}^2\right]}_{\gE_4}\\\numbereq\label{eq:first_step}
\end{align*}
where in the second step we take expectation on both sides, first w.r.t. the random direction $u$, DP noise $z$, and the clipping event $Q$, then w.r.t the client $m_k$.

We bound $\gE_1$ as the following:
\begin{align*}
    &-\eta_0\E\langle \nabla_{w_0}F_\lambda(\vw^t), \breve{G}^t_{0}(\Tilde{\vw}^t)\rangle \\
    =& -\eta_0\E\langle\nabla_{w_0}F_\lambda(\vw^t), \breve{G}^t_{0}(\Tilde{\vw}^t)-(1-P)\nabla_{w_0} F_\lambda(\Tilde{\vw}^t)+(1-P)\nabla_{w_0} F_\lambda(\Tilde{\vw}^t)\rangle\\
    = & -\eta_0\E\langle \nabla_{w_0}F_\lambda(\vw^t), \breve{G}^t_{0}(\Tilde{\vw}^t)-(1-P)\nabla_{w_0} F_\lambda(\Tilde{\vw}^t)\rangle\\
    -&\eta_0\E\langle \nabla_{w_0}F_\lambda(\vw^t), (1-P)\nabla_{w_0} F_\lambda(\Tilde{\vw}^t)-(1-P)\nabla_{w_0} F_\lambda(\vw^t)+(1-P)\nabla_{w_0} F_\lambda(\vw^t)\rangle\\
    \overset{1)}{=}& -(1-P)\eta_0\E\langle \nabla_{w_0}F_\lambda(\vw^t), \nabla_{w_0} F_\lambda(\Tilde{\vw}^t)-\nabla_{w_0} F_\lambda(\vw^t)\rangle\\
    -&\eta_0\E\langle \nabla_{w_0}F_\lambda(\vw^t),(1-P)\nabla_{w_0} F_\lambda(\vw^t)\rangle\\
    \overset{2)}{\le} & \frac{(1-P)\eta_0}{2}\E\left[\norm{\nabla_{w_0}F_\lambda(\vw^t)}^2\right] +  \frac{(1-P)\eta_0}{2} \E\left[\norm{\nabla_{w_0} F_\lambda(\Tilde{\vw}^t)-\nabla_{w_0} F_\lambda(\vw^t)}^2\right]-(1-P)\eta_0\E\left[\norm{\nabla_{w_0}F_\lambda(\vw^t)}^2\right] \\
    \overset{3)}{\le} & -\frac{(1-P)\eta_0}{2}\E\left[\norm{\nabla_{w_0}F_\lambda(\vw^t)}^2\right] +  \frac{\eta_0 L}{2} \E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]\numbereq\label{eq:E1}
\end{align*}
where in $1)$ we use the fact that $\E\left[\breve{G}^t_{0}(\Tilde{\vw}^t)-(1-P)\nabla_{x_0} F_\lambda(\Tilde{\vw}^t)\right]=0$, in $2)$ we applied the Cauchy–Schwarz inequality,
% i.e., \[\langle A,B\rangle\le\frac{1}{2}\norm{A}^2+ \frac{1}{2}\norm{B}^2.\]
and $3)$ follows by the smoothness of $F_\lambda$ and the fact that $1-P \le 1$

For $\gE_2$, we can further bound it based on Assumption~\ref{assum:smooth_full}:
\begin{align*}
    &\frac{1}{2}\E\left[\norm{\breve{G}^t_{0}(\Tilde{\vw}^t)}^2\right]\\
    =& \frac{1}{2}\E\left[\norm{\breve{G}^t_{0}(\Tilde{\vw}^t)-(1-P)\nabla_{x_0} F_\lambda(\vw^t)+(1-P)\nabla_{x_0} F_\lambda(\vw^t)}^2\right]\\
    \overset{1)}{\le}&\E\left[\norm{\breve{G}^t_{0}(\Tilde{\vw}^t)-(1-P)\nabla_{x_0} F_\lambda(\vw^t)}^2\right]+(1-P)^2\E\left[\norm{\nabla_{x_0} F_\lambda(\vw^t)}^2\right]\\
    =&\E\left[\norm{\breve{G}^t_{0}(\Tilde{\vw}^t)-(1-P)\nabla_{x_0} F_\lambda(\Tilde{\vw}^t)+(1-P)\nabla_{x_0} F_\lambda(\Tilde{\vw}^t)-(1-P)\nabla_{x_0} F_\lambda(\vw^t)}^2\right]+(1-P)^2\E\left[\norm{\nabla_{x_0} F_\lambda(\vw^t)}^2\right]\\
    \overset{2)}{\le}&2\E\left[\norm{\breve{G}^t_{0}(\Tilde{\vw}^t)-(1-P)\nabla_{x_0} F_\lambda(\Tilde{\vw}^t)}^2\right]+2(1-P)^2\E\left[\norm{\nabla_{x_0} F_\lambda(\Tilde{\vw}^t)-\nabla_{x_0} F_\lambda(\vw^t)}^2\right]+(1-P)^2\E\left[\norm{\nabla_{x_0} F_\lambda(\vw^t)}^2\right]\\
    \overset{3)}{\le}& \frac{2(1-P)}{B}(2d_0 \E\left[\norm{\nabla_{x_0} F(\vw^t)}^2\right] + 2d_0\sigma_s^2 + \frac{L^2}{2} \lambda^2 d_0^3 )+\frac{2P}{B} C^2 d_0\\
    &-\frac{2(1-P)^2}{B}\E\left[\norm{\nabla_{x_0} F_\lambda(\vw^t)}^2\right]+2\sigma_{dp}^2 d_0 +2(1-P)^2L^2\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+(1-P)^2\E\left[\norm{\nabla_{x_0} F_\lambda(\vw^t)}^2\right]\\
    \overset{4)}{\le}&\frac{4(1-P)d_0}{B}\E\left[\norm{\nabla_{x_0} F(\vw^t)}^2\right]+(1-P)\E\left[\norm{\nabla_{x_0} F_\lambda(\vw^t)}^2\right]+2 L^2\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\gG_{0}\numbereq\label{eq:E2}
\end{align*}
where in $1)$ and $2)$ we applied the Cauchy–Schwarz inequality,
% \[\norm{A+B}^2\le2\norm{A}^2+ 2\norm{B}^2\]
and in $3)$ we substitute \eqref{eq:G_var} in and use the $L$-smoothness of $F_\lambda$, and in $(iv)$ we use the fact that $1-P \le 1$ and let 
\[\gG_{0} = \frac{4}{B}d_0\sigma_s^2 + \frac{L^2}{B} \lambda^2 d_0^3 +\frac{2P}{B} C^2 d_0+2\sigma_{dp}^2 d_0\]

Similarly, For $\gE_3$:
\begin{align}
    -\eta_{m_k}\E\langle \nabla_{w_{m_k}}F_\lambda(\vw^t), \breve{G}^t_{m}(\Tilde{\vw}^t)\rangle\le -\frac{(1-P)\eta_{m_k}}{2}\E\left[\norm{\nabla_{w_{m_k}}F_\lambda(\vw^t)}^2\right] +  \frac{\eta_{m_k} L}{2} \E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]\label{eq:E3}
\end{align}

And For $\gE_4$:
\begin{align}
    \frac{1}{2}\E\left[\norm{\breve{G}^t_{m_k}(\Tilde{\vw}^t)}^2\right]
    \le&\frac{4(1-P)d_{m}}{B}\E\left[\norm{\nabla_{w_{m_k}} F(\vw^t)}^2\right]+(1-P)\E\left[\norm{\nabla_{w_{m_k}} F_\lambda(\vw^t)}^2\right]+2L^2\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\gG_{m}\label{eq:E4}
\end{align}
where we let 
\begin{align}
\gG_{m} = \frac{4}{B}d_m\sigma_s^2 + \frac{L^2}{B} \lambda^2 d_m^3 +\frac{2P}{B} C^2 d_m+2\sigma_{dp}^2 d_m\label{eq:gm}
\end{align}
Substituting \eqref{eq:E1}, \eqref{eq:E2}, \eqref{eq:E3}, and \eqref{eq:E4} into \eqref{eq:first_step}, we have
\begin{align*}
    &\E\left[F(\vw^{t+1})-F(\vw^{t}) \right]\\
    \le&\E\left[F_\lambda(\vw^{t})\right]-\frac{(1-P)\eta_0}{2}\E\left[\norm{\nabla_{w_0}F_\lambda(\vw^t)}^2\right] +  \frac{\eta_0 L}{2} \E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]\\
    +&\frac{4(1-P)d_0 L \eta_0^2}{B}\E\left[\norm{\nabla_{w_0} F(\vw^t)}^2\right]+(1-P) L \eta_0^2 \E\left[\norm{\nabla_{w_0} F_\lambda(\vw^t)}^2\right]+2 L^3\eta_0^2\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+L\eta_0^2\gG_{0}\\
    -&\frac{(1-P)\eta_{m_k}}{2}\E\left[\norm{\nabla_{w_{m_k}}F_\lambda(\vw^t)}^2\right] +  \frac{\eta_{m_k} L}{2} \E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]\\
    +&\frac{4(1-P)d_{m_k} L\eta_{m_k}^2}{B}\E\left[\norm{\nabla_{w_{m_k}} F(\vw^t)}^2\right]+(1-P)L\eta_{m_k}^2\E\left[\norm{\nabla_{w_{m_k}} F_\lambda(\vw^t)}^2\right]+2L^3\eta_{m_k}^2\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+L \eta_{m_k}^2\gG_{m}\\
    \le&\E\left[F_\lambda(\vw^{t})\right]-\eta_0(1-P)\left(\frac{1}{2}-\eta_0 L\right)\E\left[\norm{\nabla_{w_0}F_\lambda(\vw^t)}^2\right] + \eta_0 L \left(\frac{1}{2}+2\eta_0 L^2\right)\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]\\
    +&\eta_0^2\frac{4(1-P)d_0 L }{B}\E\left[\norm{\nabla_{w_0} F(\vw^t)}^2\right]+\eta_0^2L\gG_{0}\\
    -&\sum_{m=1}^M q_m\eta_{m}(1-P)\left(\frac{1}{2}-\eta_{m} L\right)\E\left[\norm{\nabla_{w_m}F_\lambda(\vw^t)}^2\right] + \sum_{m=1}^M q_m\eta_{m} L \left(\frac{1}{2}+2\eta_{m} L^2\right)\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]\\
    +&\sum_{m=1}^{M} q_m \eta_{m}^2 \frac{4(1-P)d_m L}{B}\E\left[\norm{\nabla_{w_m} F(\vw^t)}^2\right]+\sum_{m=1}^M q_m \eta_{m}^2 L \gG_{m}\numbereq\label{eq:combine}
\end{align*}
where in the last inequality we further take expectation w.r.t. client $M$ and combine similar terms.

From \eqref{eq:combine}, we utilize the properties of the smooth function \eqref{eq:func_diff} and \eqref{eq:func_squared_2} to turn all the smooth function $F_\lambda$ into the true loss function $F$:
\begin{align*}
    &\E\left[F(\vw^{t+1})-F(\vw^{t}) \right]\overset{1)}{\le} \E\left[F_\lambda(\vw^{t+1})-F_\lambda(\vw^{t})\right] + L\lambda^2 d\\
    \overset{2)}{\le}&- \eta_0(1-P)\left(\frac{1}{2}-\eta_0 L\right)\left(\frac{1}{2}\E\left[\norm{\nabla_{x_{0}}F(\vw^t)}^2\right]-\frac{L^2}{4}\lambda^2 d_0^3\right) +  \eta_0 L \left(\frac{1}{2}+2\eta_0 L^2\right)\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]\\
    +&\eta_0^2\frac{4(1-P)d_0 L}{B}\E\left[\norm{\nabla_{x_{0}} F(\vw^t)}^2\right]+ \eta_0^2 L \gG_{0}\\
    -&\sum_{m=1}^M q_m\eta_m(1-P)\left(\frac{1}{2}-\eta_m L\right)\left(\frac{1}{2}\E\left[\norm{\nabla_{w_{m}}F(\vw^t)}^2\right]-\frac{L^2}{4}\lambda^2 d_m^3\right) +  \sum_{m=1}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2\right)\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]\\
    +&\sum_{m=1}^{M} q_m \eta_m^2\frac{4(1-P)d_m L}{B}\E\left[\norm{\nabla_{w_{m}} F(\vw^t)}^2\right]+\sum_{m=1}^M q_m\eta_m^2 L \gG_{m} + L\lambda^2 d\\
    \overset{3)}{\le}&-\sum_{m=0}^M q_m\eta_m(1-P)\left(\frac{1}{4}-\frac{\eta_m L (B+8d_m)}{2B}\right)\E\left[\norm{\nabla_{w_{m}}F(\vw^t)}^2\right]\\
    + & \sum_{m=0}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2\right)\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\sum_{m=0}^M q_m\eta_m\left(\frac{1}{2}-\eta_m L\right)\frac{L^2}{4}\lambda^2 d_m^3\\
    +&\sum_{m=0}^M q_m \eta_m^2 L \gG_{m} + L\lambda^2 d\\
    \overset{4)}{\le}&-\sum_{m=0}^M q_m\eta_m(1-P)\left(\frac{1}{4}-\frac{\eta_m L (B+8d_m)}{2B}\right)\E\left[\norm{\nabla_{w_{m}}F(\vw^t)}^2\right]\\
    +&\sum_{m=0}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2\right)\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]\\
    + &\sum_{m=0}^M q_m\eta_m \frac{L^2}{8}\lambda^2 d_m^3+ \sum_{m=0}^M q_m \eta_m^2 L \left(\frac{4}{B}d_m\sigma_s^2 + \frac{(4-B)L^2}{4B} \lambda^2 d_m^3 +\frac{2P}{B} C^2 d_m+2\sigma_{dp}^2 d_m\right) + L\lambda^2 d\\
    \overset{5)}{\le}&-\sum_{m=0}^M q_m\eta_m(1-P)\left(\frac{1}{4}-\frac{\eta_m L (B+8d_m)}{2B}\right)\E\left[\norm{\nabla_{w_{m}}F(\vw^t)}^2\right]\\
    + &\sum_{m=0}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2\right)\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\gA_1\numbereq\label{eq:main_1}
\end{align*}
where $1)$ and $2)$ follows from equation \eqref{eq:func_diff} and \eqref{eq:func_squared_2} in \cref{lem:zero} respectively. In $3)$, we let $q_0=1$ and combine similar terms. In $4)$, we substitute in \eqref{eq:gm}. Lastly, in $5)$, we denote
\begin{align}\label{eq:A1}
    \gA_1 = \sum_{m=0}^M q_m\eta_m \frac{L^2}{8}\lambda^2 d_m^3+ \sum_{m=0}^M q_m \eta_m^2 L \left(\frac{4}{B}d_m\sigma_s^2 + \frac{(4-B)L^2}{4B} \lambda^2 d_m^3 +\frac{2P}{B} C^2 d_m+2\sigma_{dp}^2 d_m\right) + L\lambda^2 d
\end{align}for the convenience of notation. 

We thus complete the proof.
\end{proof}


Now, recall the definition of the Lyapunov function $V^t$:
\[V^t = F(\vw^t)+\sum_{i=1}^{\tau}\gamma_i\norm{\chi^{t+1-i}-\chi^{t-i}}^2\]
We utilize the Lyapunov function to prove the following lemma for eliminating model delay. 
\begin{lemma}
\label{lem:main_2}
    Under \cref{assum:Lip_full}-\ref{assum:ind_part_full}, and assume the client delay $\tau_m$ is uniformly upper bounded by $\tau$, we have the following lemma:
    \begin{align*}
        \E\left[V^{t+1}-V^{t}\right]\le-\frac{1-P}{8}\min_m\{q_m\eta_m\} \E\left[\norm{\nabla_{\vw}F(\vw^t)}^2\right]+\gA_1 + \gA_2
    \end{align*}
\end{lemma}
\begin{proof}
Before we give the proof of \cref{lem:main_2}, we first provide some useful facts that reveal properties of the delayed parameters.


Recall that $\Tilde{\chi}^{t}$ denote the delayed parameters on all clients, and $\chi^{t}$ denote the non-delayed version.
Let $\gF_1 = \E\left[\norm{\chi^{t+1}-\chi^{t}}^2\right]$, $\gF_2 = \E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]$. 

For $\gF_1$:
\begin{align*}
    &\E\left[\norm{\chi^{t+1}-\chi^{t}}^2\right]\\
    =&\eta_m^2\E\left[\norm{\breve{G}^t_{m_k}(\Tilde{\vw}^t)}^2\right]\\
    \le&\sum_{m=1}^{M}q_m\eta_m^2\frac{2(1-P)d_{m}}{B}\E\left[\norm{\nabla_{w_{m_k}} F(\vw^t)}^2\right]+\sum_{m=1}^{M}q_m\eta_m^2\frac{(1-P)}{2}\E\left[\norm{\nabla_{w_{m_k}} F_\lambda(\vw^t)}^2\right]\\
    +&\sum_{m=1}^{M}q_m\eta_m^2\left(L^2\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\frac{1}{2}\gG_{m}\right)\\
    \le&\sum_{m=1}^{M}q_m\eta_m^2\frac{2(1-P)d_{m}}{B}\E\left[\norm{\nabla_{w_{m_k}} F(\vw^t)}^2\right]+\sum_{m=1}^{M}q_m\eta_m^2\frac{(1-P)}{2}\left(2\E\left[\norm{\nabla_{w_{m_k}} F(\vw^t)}^2\right]+\frac{L^2}{2}\lambda^2 d_m^3\right)\\
    +&\sum_{m=1}^{M}q_m\eta_m^2\left(L^2\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\frac{1}{2}\gG_{m}\right)\\
     =&\sum_{m=1}^{M}q_m\eta_m^2\frac{(1-P)(2d_{m}+B)}{B}\E\left[\norm{\nabla_{w_{m_k}} F(\vw^t)}^2\right]+\sum_{m=1}^{M}q_m\eta_m^2\left(\frac{L^2}{4}\lambda^2 d_m^3+L^2\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\frac{1}{2}\gG_{m}\right)\numbereq\label{eq:gf1}
\end{align*}
For $\gF_2$, under uniformly bounded delay, we have
\begin{align*}
    &\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]\\
    \le &\E\left[\norm{\sum_{i=1}^{\tau}(\chi^{i+1}-\chi^i)}^2\right]\\
    \le &\tau\sum_{i=1}^{\tau}\E\left[\norm{\chi^{i+1}-\chi^i}^2\right]\numbereq\label{eq:gf2}
\end{align*}
where the last inequality follows by Cauchy-Schwarz Inequality.\\
By the definition of $V^t$:
\begin{align*}
    &\E\left[V^{t+1}-V^{t}\right]\\
    =&\E\left[ F(\vw^{t+1})+\sum_{i=1}^{\tau}\gamma_i\norm{\chi^{t+2-i}-\chi^{t+1-i}}^2\right]-\E\left[ F(\vw^{t})+\sum_{i=1}^{\tau}\gamma_i\norm{\chi^{t+1-i}-\chi^{t-i}}^2\right]\\
    =&\E\left[F(\vw^{t+1})-F(\vw^{t})\right]+\sum_{i=1}^{\tau}\gamma_i\E\left[\norm{\chi^{t+2-i}-\chi^{t+1-i}}^2\right]-\sum_{i=1}^{\tau}\gamma_i\E\left[\norm{\chi^{t+1-i}-\chi^{t-i}}^2\right]\\
    \overset{1)}{\le} & -\sum_{m=0}^M q_m\eta_m(1-P)\left(\frac{1}{4}-\frac{\eta_m L (B+8d_m)}{2B}\right)\E\left[\norm{\nabla_{w_{m}}F(\vw^t)}^2\right]\\
    + &\sum_{m=0}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2\right)\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\gA_1\\
    +& \gamma_1\underbrace{\E\left[\norm{\chi^{t+1}-\chi^{t}}^2\right]}_{\gF_1}+\sum_{i=1}^{\tau-1}(\gamma_{i+1}-\gamma_{i})\E\left[\norm{\chi^{t+1-i}-\chi^{t-i}}^2\right]- \gamma_\tau\E\left[\norm{\chi^{t+1-\tau}-\chi^{t-\tau}}^2\right]\\
    \overset{2)}{\le} & -\sum_{m=0}^M q_m\eta_m(1-P)\left(\frac{1}{4}-\frac{\eta_m L (B+8d_m)}{2B}\right)\E\left[\norm{\nabla_{w_{m}}F(\vw^t)}^2\right]\\
    + &\sum_{m=0}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2\right)\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\gA_1\\
    +&\sum_{m=1}^{M}q_m\eta_m^2\gamma_1\frac{(1-P)(2d_{m}+B)}{B}\E\left[\norm{\nabla_{w_{m_k}} F(\vw^t)}^2\right]+\sum_{m=1}^{M}q_m\eta_m^2\gamma_1\left(\frac{L^2}{4}\lambda^2 d_m^3+L^2\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]+\frac{1}{2}\gG_{m}\right)\\
    +& \sum_{i=1}^{\tau-1}(\gamma_{i+1}-\gamma_{i})\E\left[\norm{\chi^{t+1-i}-\chi^{t-i}}^2\right]- \gamma_\tau\E\left[\norm{\chi^{t+1-\tau}-\chi^{t-\tau}}^2\right]\\
    \overset{3)}{=} & -\eta_0(1-P)\left(\frac{1}{4}-\frac{\eta_0 L (B+8d_0)}{2B}\right)\E\left[\norm{\nabla_{x_{0}}F(\vw^t)}^2\right]\\
    -&\sum_{m=1}^M q_m\eta_m(1-P)\left(\frac{1}{4}-\frac{\eta_m L (B+8d_m)}{2B}-\frac{\eta_m\gamma_1(2d_{m}+B)}{B}\right)\E\left[\norm{\nabla_{w_{m}}F(\vw^t)}^2\right]\\
    + &\left\{\eta_0 L \left(\frac{1}{2}+2\eta_0 L^2\right)+\sum_{m=1}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2+\eta_m \gamma_1 L\right)\right\}\underbrace{\E\left[\norm{\Tilde{\chi}^t-\chi^t}^2\right]}_{\gF_2}\\
    +&\gA_1 + \sum_{m=1}^{M}q_m\eta_m^2\gamma_1\left(\frac{L^2}{4}\lambda^2 d_m^3+\frac{1}{2}\gG_{m}\right)\\
    +& \sum_{i=1}^{\tau-1}(\gamma_{i+1}-\gamma_{i})\E\left[\norm{\chi^{t+1-i}-\chi^{t-i}}^2\right]- \gamma_\tau\E\left[\norm{\chi^{t+1-\tau}-\chi^{t-\tau}}^2\right]\\
    \overset{4)}{\le} & -\eta_0(1-P)\left(\frac{1}{4}-\frac{\eta_0 L (B+8d_0)}{2B}\right)\E\left[\norm{\nabla_{x_{0}}F(\vw^t)}^2\right]\\
    -&\sum_{m=1}^M q_m\eta_m(1-P)\left(\frac{1}{4}-\frac{\eta_m L (B+8d_m)}{2B}-\frac{\eta_m\gamma_1(2d_{m}+B)}{B}\right)\E\left[\norm{\nabla_{w_{m}}F(\vw^t)}^2\right]\\
    + & \sum_{i=1}^{\tau-1}\left(\gamma_{i+1}-\gamma_{i}+\tau\left(\eta_0 L \left(\frac{1}{2}+2\eta_0 L^2\right)+\sum_{m=1}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2+\eta_m \gamma_1 L\right)\right)\right)\E\left[\norm{\chi^{t+1-i}-\chi^{t-i}}^2\right]\\
    -& \left(\gamma_\tau - \tau\left(\eta_0 L \left(\frac{1}{2}+2\eta_0 L^2\right)+\sum_{m=1}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2+\eta_m \gamma_1 L\right)\right)\right)\E\left[\norm{\chi^{t+1-\tau}-\chi^{t-\tau}}^2\right]\\
    +&\gA_1 + \gA_2\numbereq.\label{eq:vcomplex}
\end{align*}
Above, we used \cref{lem:main_1} in step $(1)$,  substituted in \eqref{eq:gf1} for $\gF_1$ in step $(2)$, substituted in \eqref{eq:gf2} for $\gF_2$ in step (3), and  defined 
\begin{align*}
    \gA_2 :=& \sum_{m=1}^{M}q_m\eta_m^2\gamma_1\left(\frac{L^2}{4}\lambda^2 d_m^3+\frac{1}{2}\gG_{m}\right)\\
    =&\sum_{m=1}^{M}q_m\eta_m^2\gamma_1\left(\frac{L^2}{4}\lambda^2 d_m^3+\frac{2}{B}d_m\sigma_s^2 + \frac{L^2}{2B} \lambda^2 d_m^3 +\frac{P}{B} C^2 d_m+\sigma_{dp}^2 d_m\right).\numbereq\label{eq:A2}
\end{align*}


From \eqref{eq:vcomplex}, we choose the following relationship for $\gamma_1, \gamma_2, \dots, \gamma_m$:
\begin{align}\label{eq:gamma1}
\gamma_{1} = \frac{\tau^2\left(\eta_0 L \left(\frac{1}{2}+2\eta_0 L^2\right)+\sum_{m=1}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2\right)\right)}{1-\tau^2\sum_{m=1}^{M}q_m\eta_m^2 L^2 }
\end{align}
\[\gamma_{i+1}=\gamma_{i}-\tau\left(\eta_0 L \left(\frac{1}{2}+2\eta_0 L^2\right)+\sum_{m=1}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2+\eta_m \gamma_1 L\right)\right)\]
and we can verify that 
\[\gamma_{\tau}-\tau\left(\eta_0 L \left(\frac{1}{2}+2\eta_0 L^2\right)+\sum_{m=1}^{M}q_m\eta_m L \left(\frac{1}{2}+2\eta_m L^2+\eta_m \gamma_1 L\right)\right)\ge 0\]
We further let
\[\eta_0\le \frac{B}{4L(B+8d_0)}, \eta_m\le \frac{B}{4L(B+8d_0)+8\gamma_1(2d_m+B)},\]
and we finally have 
\begin{align*}
     \E\left[V^{t+1}-V^{t}\right]
    &\le-\frac{\eta_0}{8}(1-P)\E\left[\norm{\nabla_{x_{0}}F(\vw^t)}^2\right]
    -\sum_{m=1}^M \frac{q_m\eta_m}{8}(1-P)\E\left[\norm{\nabla_{w_{m}}F(\vw^t)}^2\right]+\gA_1 + \gA_2\\
    &\le-\frac{1-P}{8}\min_m q_m\eta_m \E\left[\norm{\nabla_{\vw}F(\vw^t)}^2\right]+\gA_1 + \gA_2\numbereq\label{eq:main_2}
\end{align*}
which completes the proof.
\end{proof}

% \subsection{Notations}
% \subsection{Assumptions}

\section{Discussion of Differential Privacy under Various Settings}
\label{appen:dp_discuss}

\textbf{Overview of DPZV's Privacy Mechanisms.\ }
{\tt DPZV} protects privacy through two main mechanisms: 1) \emph{Zeroth-Order (ZO) Optimization.} By eliminating the need to transmit backward gradients, DPZV mitigates conventional gradient-based leakage \cite{he2019model}. Specifically, no unperturbed gradients are ever exchanged among participants. 2) \emph{Controllable Differential Privacy (DP).} Each participant interacts only through black-box queries and responses augmented with DP-based noise. This randomized protocol further shields sensitive information from reconstruction or inference attacks.

\subsection{Protection Against an Honest-but-Curious Threat Model}
In an \emph{honest-but-curious} scenario, adversaries seek private insights (labels or features) without deviating from the protocol. Two common types of attacks are:

\paragraph{Label Inference Attacks.}
Such attacks typically exploit exact gradient signs or rely on known model architectures to backtrack label information \cite{fu2022label, li2021label}. Under DPZV, \emph{unbiased gradients are never shared}, and the server model remains inaccessible to other participants. Clients observe only \emph{stochastic ZO outputs} with added noise, preventing them from reverse-engineering labels via gradient signals.

\paragraph{Feature Inference Attacks.}
Adversaries may attempt feature or data reconstruction \cite{he2019model} by leveraging gradient disclosures or conducting model inversion. DPZV thwarts such efforts by disallowing adaptive queries on the client’s internal parameters; only scalar function evaluations flow to and from the server. Additionally, the injected DP noise obscures potential patterns that attackers might exploit for reconstructing sensitive features.

Moreover, both label and feature inference methods often assume knowledge of the dataset domain or direct model access, which is not given in DPZV’s design. Task details (e.g., label distributions, model layers) are held privately by each party, limiting the adversary’s capacity to launch sophisticated inversion or inference attacks.

\subsection{Protection Against Post-Training Attacks}
Post-training adversaries typically aim to infer sensitive data from a \emph{final, trained model} \cite{ateniese2015hacking}. Since DPZV applies differential privacy \emph{throughout} training, the final model parameters satisfy rigorous $(\epsilon,\delta)$-DP guarantees. Hence, even if the trained model is released or accessed, the level of noise injected ensures that the adversary cannot reliably distinguish any single individual’s data—reducing vulnerability to membership inference or model-inversion attacks \cite{jiang2022comprehensive}. This formal DP framework remains robust regardless of downstream usage or queries on the finalized model.

In summary, by combining ZO optimization with DP noise injection, DPZV provides comprehensive protection against both \emph{honest-but-curious} and \emph{post-training} adversaries across diverse privacy threat scenarios. 




% \section{Synchonous Version of DPZV}
% \label{appen:syn_alg}
\section{Choice of all Hyperparameters}
\label{appen:lr}
In this section, we show all the chosen Hyperparameters, including learning rate for the four datasets and ZO and DP parameters in Sec.~\ref{sec:exp}:


\begin{table*}[h]
\caption{All learning rate choices }
\label{table:lr}
\vskip 0.15in
\begin{center}
\resizebox{0.7\textwidth}{!}{
\begin{small}
\begin{sc}
\begin{tabular}{cccccc}
\toprule
Dataset &  & VAFL & ZOO-VFL & VFL-CZOFO & DPZV (ours) \\
\midrule
%  
\multirow{2}{*}{MNIST} & $\eta_0$ & $0.005$&$0.005$ &$0.005$&$0.005$ \\
\cmidrule(lr){2-6}
 & $\eta_m$ & $0.001$&$5\times 10^{-5}$ & $1\times 10^{-7}$& $5\times 10^{-4}$\\
\midrule
\multirow{2}{*}{CIFAR-10} & $\eta_0$ & $0.005$&$0.005$ &$0.005$&$0.005$ \\
\cmidrule(lr){2-6}
 & $\eta_m$& $0.001$ &$5\times 10^{-5}$ &$5\times 10^{-8}$ &  $1\times 10^{-4}$\\
\midrule\multirow{2}{*}{ModelNet40} & $\eta_0$  & $1\times 10^{-4}$& $1\times 10^{-5}$& $5\times 10^{-7}$&$1\times 10^{-5}$ \\
\cmidrule(lr){2-6}
 & $\eta_m$ & $1\times 10^{-5}$& $1\times 10^{-5}$& $5\times 10^{-7}$&$1\times 10^{-5}$ \\
\midrule\multirow{2}{*}{\makecell{Amazon \\Review}} &  $\eta_0$  & $1\times 10^{-5}$& $5\times 10^{-7}$& $1\times 10^{-6}$&$5\times 10^{-7}$ \\
\cmidrule(lr){2-6}
 & $\eta_m$ & $1\times 10^{-5}$& $5\times 10^{-7}$& $1\times 10^{-6}$&$5\times 10^{-7}$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
}
\end{center}
\vspace{-0.2in}
\end{table*}

\begin{table*}[h]
\caption{Other Hyperparameters}
    \label{tab:other_param}
    \begin{center}
    \begin{sc}
    \begin{tabular}{l l l}  % "l l" specifies two left-aligned columns
        \toprule
        Parameter &Value&Explanation \\ 
        \midrule
         $C$     & $10$&\textit{DP clipping threshold} \\
        $\lambda$          & $0.001$ &\textit{Scale of perturbation for ZOO}          \\
         $m$    & $0.9$ & \textit{Momentum parameter} \\
         $\delta$    & $1\times 10^{-5}$ & \textit{DP error level} \\
         $n$    & $5$ & \textit{Number of perturbation for VFL-CZOFO} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{center}
    
\end{table*}


\end{document}
