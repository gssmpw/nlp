\section{Related Works}
\label{sec:rw}

\subsection{Reward Models and Language Model Alignment}

Reward models are widely used in the training process of modern large language models. For modern LLMs, as their abilities improve with more learnable parameters and more training data~\cite{kaplan_scaling_2020,hoffmann_training_2022}, it is important to align the behaviors of LLMs with human preferences to prevent them from generating harmful, biased, or illegal content. 

Before aligning LLMs with human preferences, model developers need to build RMs to fit human preferences. To achieve this point, they first collect massive labeled data based on human feedback to correctly and truthfully reflect the preference~\cite{ouyang_training_2022}. Then, RMs are trained on these data to learn human preferences with suitable loss functions, such as pairwise loss~\cite{wang_helpsteer2-preference_2024}, Bradley-Terry loss~\cite{ouyang_training_2022,liu_skywork-reward_2024,wang_helpsteer2-preference_2024}, margin-based loss~\cite{liu_skywork-reward_2024}, or regression loss~\cite{wang_helpsteer2_2024}. Usually, adopting the Bradley-Terry loss can achieve better results and generalizability~\cite{liu_skywork-reward_2024,wang_helpsteer2-preference_2024}.


After obtaining advanced RMs, there are two mainstream alignment approaches, i.e., reinforcement learning-based and direct optimization-based. In these two approaches, reward models play different but equally critical roles. For the former one, reinforcement learning algorithms, such as proximal policy optimization (PPO)~\cite{schulman_proximal_2017}, are adopted to teach LLMs better sampling policies. By earning higher rewards from the RMs, the LLMs learn the human preference step-by-step. The most widely used solution is reinforcement learning from human feedback (RLHF)~\cite{ouyang_training_2022}.  For the later one, direct preference optimization (DPO)~\cite{rafailov_direct_2023} is one of the most widely used solutions, adopted by many popular LLMs~\cite{touvron_llama_2023-1,yang_qwen2_2024,dubey_llama_2024}. In DPO, RMs are used to select pairwise data and provide them to LLMs to learn the preference distribution~\cite{touvron_llama_2023-1,dubey_llama_2024}. Therefore, RMs in DPO act more like data filters, which are designed to provide more high-quality training data to LLMs.


Overall, the performance of alignment is related to both RMs and the alignment approaches. In this paper, we only focus on the impact of RMs and will leave the alignment approaches as our future work.

\subsection{Instruction-Tuning Breaks Alignment}

To fulfill customized functions, model developers usually fine-tune aligned LLMs with instruction-tuning datasets. However, \citet{qi_fine-tuning_2024} reveal that fine-tuning LLMs will harm the existing alignment and reduce the safety level, making the model response to harmful requests easier. In their experiment, even fine-tuning LLMs on a benign instruction-tuning dataset will decrease the model's safety. \citet{ji_language_2024} theoretically explain such a phenomenon and prove that LLMs naturally resist the alignment. From another perspective, \citet{zhao_learning_2024} find that aligned LLMs tend to forget unsafe examples existing in the instruction-tuning dataset after an additional safety fine-tuning procedure. Therefore, some works~\cite{huang_lazy_2024,qi_safety_2024} introduce additional safety data to repair the damaged safety alignment during the instruction-tuning process.

Several works~\cite{hsu_safe_2024,peng_navigating_2024,jain_what_2024} study the safety alignment degradation from the perspective of model parameters and loss landscapes, and propose efficient training strategies to achieve a trade-off between safety and utility. \citet{lyu_keeping_2024} explore the system prompts used in LLMs and prove they can keep the safety alignment after fine-tuning LLMs on harmful data.

Despite the numerous works studying the safety alignment degradation after the instruction-tuning process, they mainly focus on datasets containing explicit harmful data. To the best of our knowledge, there are no works investigating the degradation from the aspect of benign datasets without any harmful data. We experimentally explore the inherent reasons related to the safety alignment degradation after fine-tuning LLMs on purely benign datasets, which could provide guidance for model developers to build high-quality downstream task datasets.