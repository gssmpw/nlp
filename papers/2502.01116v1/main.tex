%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{caption}
\usepackage{newfloat}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{array}
\usepackage{longtable}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


%my package
\usepackage{bbding}
\usepackage{listings}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{eufrak}
\usepackage{subcaption}
%\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{pifont}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage[textsize=tiny]{todonotes}
\usepackage{enumerate}
% \usepackage{enumitem}
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{dsfont}
\usepackage{wrapfig}
\usepackage{xspace}
\usepackage{eufrak}
\usepackage{xcolor}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{framed}
\usepackage{makecell}  % kangjie
\usepackage{wrapfig}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{tabularx}
\usepackage[hashEnumerators,smartEllipses]{markdown}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\DeclareFloatingEnvironment{example}
\captionsetup[example]{name=Example}

\newtheorem{observation}{\textbf{\textsc{Observation}}}

% 定义支持分页的代码块
\tcbuselibrary{listings,breakable} % 确保启用分页功能

% 定义支持分页的代码块
\tcbset{
    mylisting/.style={
        breakable,             % 支持分页
        listing only,          % 使用纯代码样式
        colback=background,    % 背景色
        colframe=black,        % 边框颜色
        boxrule=0.4mm,         % 边框厚度
        listing options={      % 配置 listings 选项
            basicstyle=\normalfont\ttfamily,
            numbers=none,      % 行号位置
            numberstyle=\scriptsize, % 行号字体大小
            stepnumber=1,      % 行号步长
            linewidth=0.85\textwidth,
            numbersep=0pt,     % 行号与代码间距
            showstringspaces=true, % 不显示字符串中的空格
            breaklines=true,   % 自动换行
            breakatwhitespace=false, % 在空格处断行
            frame=none,        % 移除边框（由 tcolorbox 控制）
            keepspaces=true,   % 保持空格
            xleftmargin=0pt,   % 左边距
            xrightmargin=0pt,  % 右边距
            framexleftmargin=0pt, % 框左边距
            framexrightmargin=0pt, % 框右边距
            aboveskip=0pt,     % 上方间距为0
            belowskip=0pt,     % 下方间距为0
            literate=
             *{<}{{{\color{delim}{<}}}}{1}
              {>}{{{\color{delim}{>}}}}{1},
        },
    }
}



\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}


\lstdefinelanguage{html}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true, % 自动换行
    breakatwhitespace=false, % 在空白处断行
    frame=none,
    linewidth=\textwidth,
    backgroundcolor=\color{background},
    keepspaces=true,
    captionpos=b,
    xleftmargin=0pt, % 左边距
    xrightmargin=0pt, % 右边距
    framexleftmargin=0pt, % 框左边距
    framexrightmargin=0pt, % 框右边距
    aboveskip=0pt,       % 上方间距为0
    belowskip=0pt,       % 下方间距为0
    literate=
     *{<}{{{\color{delim}{<}}}}{1}
      {>}{{{\color{delim}{>}}}}{1},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\MedicalInstruct}{\texttt{MedicalInstruct}\xspace}
\newcommand{\CRAFTSG}{\texttt{CRAFTSG}\xspace}
\newcommand{\PythonCodeInstruct}{\texttt{PythonCodeInstruct}\xspace}
\newcommand{\OpenPlatypus}{\texttt{OpenPlatypus}\xspace}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Picky LLMs and Unreliable RMs: An Empirical Study on Safety Alignment after Instruction Tuning}

\begin{document}

\twocolumn[
\icmltitle{Picky LLMs and Unreliable RMs:\\
An Empirical Study on Safety Alignment after Instruction Tuning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Guanlin Li}{yyy}
\icmlauthor{Kangjie Chen}{yyy}
\icmlauthor{Shangwei Guo}{comp}
\icmlauthor{Jie Zhang}{sch}
\icmlauthor{Han Qiu}{aaa}
\icmlauthor{Chao Zhang}{aaa}
\icmlauthor{Guoyin Wang}{bbb}
\icmlauthor{Tianwei Zhang}{yyy}
\icmlauthor{Jiwei Li}{ccc}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Nanyang Technological University}
\icmlaffiliation{comp}{Chongqing University}
\icmlaffiliation{sch}{CFAR, A*STAR}
\icmlaffiliation{aaa}{Tsinghua University}
\icmlaffiliation{bbb}{01.AI}
\icmlaffiliation{ccc}{Zhejiang University}

\icmlcorrespondingauthor{Guanlin Li}{guanlin001@e.ntu.edu.sg}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Large language models (LLMs) have emerged as powerful tools for addressing a wide range of general inquiries and tasks. Despite this, fine-tuning aligned LLMs on smaller, domain-specific datasets, critical to adapting them to specialized tasks, can inadvertently degrade their safety alignment, even when the datasets are benign. This phenomenon makes models more susceptible to providing inappropriate responses.
In this study, we systematically examine the factors contributing to safety alignment degradation in benign fine-tuning scenarios. Our analysis identifies three critical factors affecting aligned LLMs: answer structure, identity calibration, and role-play. Additionally, we evaluate the reliability of state-of-the-art reward models (RMs), which are often used to guide alignment processes. Our findings reveal that these RMs frequently fail to accurately reflect human preferences regarding safety, underscoring their limitations in practical applications.
By uncovering these challenges, our work highlights the complexities of maintaining safety alignment during fine-tuning and offers guidance to help developers balance utility and safety in LLMs. Datasets and fine-tuning code used in our experiments can be found in \url{https://github.com/GuanlinLee/llm_instruction_tuning}.

% Large language models (LLMs) have become powerful tools for assisting users with general questions and tasks. To ensure they do not respond to illegal requests or generate harmful content, model developers focus on aligning LLMs with human preferences to enhance their safety. Fine-tuning these aligned models on smaller datasets is often performed to adapt them for specific downstream tasks, improving their practical utility. However, prior studies reveal that fine-tuning models on smaller downstream datasets can compromise safety alignment even if there are no harmful content in the training data, making the models more prone to answering inappropriate requests.
% To investigate this phenomenon, we conduct comprehensive experiments to identify the factors contributing to safety degradation in a purely benign fine-tuning scenario. Our analysis highlights three critical aspects where aligned LLMs exhibit sensitivity to the fine-tuning dataset: answer structure, identity calibration, and role-play. Additionally, we evaluate the reliability of advanced reward models, finding that their scores may not always accurately reflect human preferences in terms of safety.
% Our findings provide valuable insights into the challenges of maintaining safety alignment during fine-tuning. We believe this study will assist model developers in better understanding and mitigating the factors that contribute to safety degradation in LLMs.

\end{abstract}

\section{Introduction}

%1. introduce llm, rm and alignment

Large language models (LLMs)~\cite{openai_gpt-4_2023,anthropic_claude_nodate,anil_gemini_2023}, containing billions of parameters, trained on billions or trillions of tokens, have demonstrated impressive capabilities in handling diverse tasks and providing creative and helpful responses. As these models become increasingly adept at following user instructions, ensuring their outputs are safe, unbiased, and aligned with human values is paramount. Alignment techniques with reward models (RMs), such as reinforcement learning from human feedback (RLHF)~\cite{ouyang_training_2022}, have been instrumental in fine-tuning these models to avoid generating harmful or illegal content while enhancing their ability to align with human preferences.
% Therefore, the concept of training LLMs to align human preference is proposed. For example, one of the most effective methods is called reinforcement learning from human feedback (RLHF)~\cite{ouyang_training_2022}, using a reward model (RM), which is aligned to human preference, to score the responses from the LLM, teaching the LLM to follow the human values.

%2. show the importance on downstream task
% add an illustration about the LLM's training phases

% Usually, alignment-related training happens during the post-training phase, after the pre-training phase, where the LLMs are trained to predict the next token. In the post-training phase, the LLMs are fine-tuned on a smaller dataset, containing high-quality data, to excite the capability of following human instructions. Simultaneously, the LLMs are aligned with human preference by attempting to maximize the scores provided by the RM, avoiding themselves to generate biased, harmful, and illegal content. 
Alignment training typically occurs during the post-training phase, following the pre-training stage where LLMs are optimized for next-token prediction. In the post-training phase, models are fine-tuned on curated, high-quality datasets to enhance their instruction-following capabilities while aligning their behavior with human values to mitigate the risk of generating harmful or biased content.
% Although LLMs after the post-training have impressive ability to solve general requests, it is not good enough for areas requiring specific knowledge. Therefore, an additional instruction-tuning phase adopts expert knowledge to fine-tune the aligned LLMs to build better models to fulfill goals, such as helping customers pick products~\cite{zheng_adapting_2024,cao_aligning_2024}, providing professional medical advice to patients~\cite{cascella_breakthrough_2024,savage_fine_2024}, and completing code~\cite{touvron_llama_2023-1,yang_qwen2_2024}. In Figure~\ref{}, we illustrate the whole lifecycle of LLMs development.
However, while aligned LLMs perform well on general tasks, additional fine-tuning with domain-specific datasets is often required to improve their utility in specialized areas, such as helping customers pick products~\cite{zheng_adapting_2024,cao_aligning_2024}, providing professional medical advice to patients~\cite{cascella_breakthrough_2024,savage_fine_2024}, and completing code~\cite{touvron_llama_2023-1,yang_qwen2_2024}. In Figure~\ref{fig:ft_overflow}, we illustrate the whole lifecycle of LLMs development.



%3. cite papers to claim the instruction-tuning will break the safety alignment
Despite its utility, fine-tuning LLMs on domain-specific datasets can inadvertently compromise the safety alignment of LLMs.
Previous studies~\cite{qi_fine-tuning_2024,zhao_learning_2024,ji_language_2024} have shown that even when the fine-tuning data contains no explicit harmful content, the fine-tuned models can become more vulnerable to jailbreak attacks in generating inappropriate or unsafe outputs~\cite{liu_autodan_2023,yu_gptfuzzer_2023,zou_universal_2023}. %Notably, this phenomenon persists even with instruction-tuning datasets that are entirely benign~\cite{}.
% Although LLMs fine-tuned on a benign instruction-tuning dataset will decrease the safety, most previous works~\cite{hsu_safe_2024,qi_safety_2024,huang_lazy_2024} focus on how to protect the alignment against a more malicious scenario, where the dataset contains harmful and illegal data items, and ascribe the alignment dropping to the shallow safety alignment~\cite{qi_safety_2024}. \citet{ji_language_2024} theoretically prove that the LLMs resist alignment and alignment are actually superficial. Different from previous works, we aim to reveal the resistance from the aspect of the instruction-tuning dataset.
While most prior work~\cite{hsu_safe_2024,qi_safety_2024,huang_lazy_2024} has focused on safeguarding alignment in scenarios where datasets include harmful or illegal content, they often attribute safety degradation to shallow alignment mechanisms~\cite{qi_safety_2024}. Furthermore, \citet{ji_language_2024} have theoretically shown that LLMs inherently resist alignment, which is often superficial. Distinct from these approaches, for the first time, our study aims to investigate the resistance to alignment from the perspective of the instruction-tuning dataset itself.

%4. introduce our work: studying scenario, studying method, conclusion

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\textwidth]{FT_overflow.pdf}
    \vspace{-10pt}
    \caption{The overview of the LLM lifecycle. During the pre-training process, the model learns to predict the next token from a massive corpus. In the post-training phase, the model is fine-tuned on well-structured data and taught by a reward model to learn policy, fitting with human preference. Aligned LLMs can be further trained on more fine-grained datasets, to achieve better performance on the downstream tasks, with the instruction-tuning phase.}
    \label{fig:ft_overflow}
    \vspace{-10pt}
\end{figure*}

In this work, we focus on a purely benign scenario, where no adversarial factors or harmful data are present in the instruction-tuning dataset. All samples in the instruction-tuning dataset are harmless, legal, and unbiased. The goal is to examine the intrinsic factors contributing to safety alignment degradation in aligned LLMs. Specifically, we investigate their vulnerability to jailbreak attacks after being fine-tuned on benign datasets designed for downstream tasks. Our analysis spans two core dimensions: the alignment robustness of LLMs and the reliability of RMs in scoring and guiding alignment.
% We focus on the safety alignment of the LLMs, measured by the success rate of jailbreaking an aligned model. Our study lies on two aspects, i.e., the alignment of LLMs and the scores of RMs. For the alignment of LLMs, we fine-tune the open-source aligned LLMs on various instruction-tuning datasets, including the medical-related task, the code completion task, the science, technology, engineering, and mathematics (STEM) task, and so on, and study the changes of the safety alignment. 

To explore the alignment of LLMs, we fine-tune open-source aligned models on diverse instruction-tuning datasets, including those focused on medical tasks~\cite{MedicalInstruct}, code completion~\cite{PythonCodeInstruct}, and STEM subjects~\cite{lee_platypus_2023}. Surprisingly, we find these open-source aligned LLMs are \textit{\textbf{picky}} to the answer's format in the dataset. By simply reformatting the answers in the dataset, we can improve or worsen the safety alignment after the instruction-tuning. An automatic reformatting pipeline is proposed by us to achieve such a job. Further, we reveal the \textit{\textbf{identity calibration}} and the \textit{\textbf{role-play}} phenomena during the instruction-tuning process, which will enhance the alignment from the language model identity learned during the alignment process or break the existing safety alignment based on a new identity, depending on the data items in the instruction-tuning dataset. 
% For the RMs, we study several open-source state-of-the-art models, based on a public benchmark, RewardBench~\cite{lambert_rewardbench_2024}.

In parallel, we assess state-of-the-art RMs~\cite{lambert_rewardbench_2024} by analyzing their ability to accurately score data in both the original and reformatted datasets. We adopt these RMs to generate absolute scores for each data item in the dataset and analyze these scores inside the dataset itself and between the original dataset and the reformatted dataset. Our experiments reveal that advanced RMs are fundamentally \textbf{\textit{unreliable}}. Although these RMs can effectively identify higher-quality training data within a dataset, they often fail to recognize the benefits of reformatted data that improve model alignment, instead assigning them lower scores compared to the original versions. These findings shed light on the factors contributing to safety alignment degradation in LLMs and provide actionable insights for preparing datasets to develop safer downstream applications.
%contributions of this work

Our contributions can be summarized as follows:
\begin{itemize}
    \setlength{\itemsep}{2pt} % Remove space between items
    \setlength{\parskip}{0pt} % Remove space between paragraphs within items
    \setlength{\topsep}{-10pt}  % Remove space above and below the list
    \item We analyze the safety degradation of aligned LLMs fine-tuned on harmless downstream datasets, offering a more general and practical perspective compared to studies focusing on datasets with harmful content.
    \item We identify and analyze three key factors in instruction-tuning datasets that influence safety alignment, demonstrating how they can either enhance or diminish safety depending on their use.
    \item We investigate the reliability of RMs, uncovering significant limitations and weaknesses in their application to downstream tasks.
    \item We provide insights into the behaviors of LLMs and RMs, enhancing the understanding of safety degradation and practical constraints in model fine-tuning.
\end{itemize}




\section{Related Works}
\label{sec:rw}

\subsection{Reward Models and Language Model Alignment}

Reward models are widely used in the training process of modern large language models. For modern LLMs, as their abilities improve with more learnable parameters and more training data~\cite{kaplan_scaling_2020,hoffmann_training_2022}, it is important to align the behaviors of LLMs with human preferences to prevent them from generating harmful, biased, or illegal content. 

Before aligning LLMs with human preferences, model developers need to build RMs to fit human preferences. To achieve this point, they first collect massive labeled data based on human feedback to correctly and truthfully reflect the preference~\cite{ouyang_training_2022}. Then, RMs are trained on these data to learn human preferences with suitable loss functions, such as pairwise loss~\cite{wang_helpsteer2-preference_2024}, Bradley-Terry loss~\cite{ouyang_training_2022,liu_skywork-reward_2024,wang_helpsteer2-preference_2024}, margin-based loss~\cite{liu_skywork-reward_2024}, or regression loss~\cite{wang_helpsteer2_2024}. Usually, adopting the Bradley-Terry loss can achieve better results and generalizability~\cite{liu_skywork-reward_2024,wang_helpsteer2-preference_2024}.


After obtaining advanced RMs, there are two mainstream alignment approaches, i.e., reinforcement learning-based and direct optimization-based. In these two approaches, reward models play different but equally critical roles. For the former one, reinforcement learning algorithms, such as proximal policy optimization (PPO)~\cite{schulman_proximal_2017}, are adopted to teach LLMs better sampling policies. By earning higher rewards from the RMs, the LLMs learn the human preference step-by-step. The most widely used solution is reinforcement learning from human feedback (RLHF)~\cite{ouyang_training_2022}.  For the later one, direct preference optimization (DPO)~\cite{rafailov_direct_2023} is one of the most widely used solutions, adopted by many popular LLMs~\cite{touvron_llama_2023-1,yang_qwen2_2024,dubey_llama_2024}. In DPO, RMs are used to select pairwise data and provide them to LLMs to learn the preference distribution~\cite{touvron_llama_2023-1,dubey_llama_2024}. Therefore, RMs in DPO act more like data filters, which are designed to provide more high-quality training data to LLMs.


Overall, the performance of alignment is related to both RMs and the alignment approaches. In this paper, we only focus on the impact of RMs and will leave the alignment approaches as our future work.

\subsection{Instruction-Tuning Breaks Alignment}

To fulfill customized functions, model developers usually fine-tune aligned LLMs with instruction-tuning datasets. However, \citet{qi_fine-tuning_2024} reveal that fine-tuning LLMs will harm the existing alignment and reduce the safety level, making the model response to harmful requests easier. In their experiment, even fine-tuning LLMs on a benign instruction-tuning dataset will decrease the model's safety. \citet{ji_language_2024} theoretically explain such a phenomenon and prove that LLMs naturally resist the alignment. From another perspective, \citet{zhao_learning_2024} find that aligned LLMs tend to forget unsafe examples existing in the instruction-tuning dataset after an additional safety fine-tuning procedure. Therefore, some works~\cite{huang_lazy_2024,qi_safety_2024} introduce additional safety data to repair the damaged safety alignment during the instruction-tuning process.

Several works~\cite{hsu_safe_2024,peng_navigating_2024,jain_what_2024} study the safety alignment degradation from the perspective of model parameters and loss landscapes, and propose efficient training strategies to achieve a trade-off between safety and utility. \citet{lyu_keeping_2024} explore the system prompts used in LLMs and prove they can keep the safety alignment after fine-tuning LLMs on harmful data.

Despite the numerous works studying the safety alignment degradation after the instruction-tuning process, they mainly focus on datasets containing explicit harmful data. To the best of our knowledge, there are no works investigating the degradation from the aspect of benign datasets without any harmful data. We experimentally explore the inherent reasons related to the safety alignment degradation after fine-tuning LLMs on purely benign datasets, which could provide guidance for model developers to build high-quality downstream task datasets.


\section{Preliminary}
\label{sec:pre}

We provide a detailed overview of the instruction-tuning task and the experimental setups used in our study.

\subsection{Instruction-Tuning}
The instruction-tuning task involves fine-tuning a pre-trained and aligned LLM on a dataset $\mathcal{D}$, which consists of tuples $(x_t, x_i, x_a)$. Here, $x_t$ represents an instruction detailing the task or posing a specific question, $x_i$ provides additional input or context, and $x_a$ is the expected answer.
The instruction-tuning process wraps $x_t$ and $x_i$ into a prompt template that serves as the input to the model\footnote{We do not consider the system prompt. Therefore, we set the system prompt empty in all experiments.}, aiming to improve its ability to generalize across diverse tasks by leveraging human-provided instructions. $x_i$ can be empty for some tuples. The primary goal of instruction-tuning is to enhance the model’s capacity to follow human instructions for tasks that were not explicitly seen during pre-training or post-training. Furthermore, instruction-tuning allows the model to acquire specialized knowledge embedded in $\mathcal{D}$, enabling it to generate coherent and contextually relevant outputs for specific downstream tasks.


\subsection{Experiment Setup}

In our experiments, we consider a purely benign scenario, where no harmful data is in the dataset and no adversarial entities are involved during the model tuning phase. We consider four different instruction-tuning datasets, including a medical dataset \MedicalInstruct~\cite{MedicalInstruct}, a Singapore culture dataset \CRAFTSG~\cite{wang_craft_2024}, a code dataset \PythonCodeInstruct~\cite{PythonCodeInstruct}, and a STEM dataset \OpenPlatypus~\cite{lee_platypus_2023}. For each dataset, we first split it into two fixed sets: training and validation. Then, we only save the checkpoint that achieves the lowest loss value on the validation set and evaluate its safety. For all datasets, the validation set contains 1,000 items. We give the details of used datasets in Appendix~\ref{ap:dataset_info}.

Besides datasets, we consider three open-source aligned LLMs, including Llama-3~\cite{dubey_llama_2024}, Mistral~\cite{jiang_mistral_2023}, and Qwen2~\cite{yang_qwen2_2024}, and five open-source RMs, including SkyworkGemma~\cite{liu_skywork-reward_2024}, SkyworkLlama~\cite{liu_skywork-reward_2024}, URM~\cite{lou_uncertainty-aware_2024}, QRM~\cite{dorka_quantile_2024}, and Internlm2~\cite{cai_internlm2_2024}. For all instruction-tuning tasks, we adopt the efficient LoRA method~\cite{hu_lora_2022}. The model details and fine-tuning settings can be found in Appendix~\ref{ap:details_llm_rm} and~\ref{ap:lora_info}.

To evaluate the safety alignment of the LLMs, we adopt a novel benchmark, SALAD-Bench~\cite{li_salad-bench_2024}, with its judge model $\mathcal{J}$. Two metrics are considered, i.e., safety alignment under general harmful questions and safety alignment under adversarial harmful questions. The first metric measures safety when the LLMs are asked a harmful question, such as `How to make a bomb'. The second metric measures the safety when the harmful question is enhanced by adversarial attacks, such as GCG~\cite{zou_universal_2023} and AutoDAN~\cite{liu_autodan_2023}. We use the first 5,000 questions in the `base\_set' of SALAD-Bench to calculate the first metric, and the full 5,000 questions in the `attack\_enhanced\_set' of SALAD-Bench to measure the second metric. For each case, we adopt the jailbreak success rate to represent the strength of the alignment with the following formula:
\begin{align*}
    \mathrm{SA} = \frac{\sum_{q\in Q} \mathcal{J}(q, \mathcal{M}(q))}{\vert Q \vert} * 100,
\end{align*}
where $\mathcal{M}$ is the LLMs under evaluation and $Q$ is the question set provided by SALAD-Bench. The judge model $\mathcal{J}$ gives 1 if the input question-answer pair is unsafe, and 0 otherwise. We adopt $\mathrm{SA}_g$ and $\mathrm{SA}_a$ to represent the two types of alignment, respectively. A lower value of $\mathrm{SA}$ stands for a better safety alignment. We provide the inference details of LLMs during the safety evaluation in Appendix~\ref{ap:evaluation_set}.

\section{Picky LLMs}
\label{sec:picky}
%1. give an overview of the study goals in this section

We conduct comprehensive experiments to reveal how picky LLMs are to the instruction-tuning datasets. (1) By simply reformatting the answer structure, we can manipulate the safety alignment. (2) We also reveal the identity calibration and the role-play phenomena during the instruction-tuning process, proving that the LLMs are picky about the identity-related content in the dataset. More detailed analysis of these factors is provided in Appendix~\ref{ap:rethink_llm}, where we propose several explanations to elucidate the deeper reasons of picky LLMs. In all experiments, we apply the same transformations on the validation set as we do on the training set, including reformatting, identity calibration and role-play. 
    

\subsection{Answer Structures Impact Safety Alignment}
\label{sec:sub_answer}
%1. first introduce a question about the reason that tuning data will hurt safety alignment

Previous works~\cite{qi_fine-tuning_2024} find that the safety alignment of LLMs drops after being fine-tuned on a clean dataset. However, the reason for such a consequence is not clear. A question arises naturally, `\textit{\textbf{Will all benign datasets cause such a decrease?}}'. A specific situation studied in previous works~\cite{hsu_safe_2024,bianchi_safety-tuned_2024} proves that adding data pairs that contain harmful instruction $x_t$ and rejection answer $x_a$ into the training dataset and fine-tuning the LLMs on this new dataset will not cause a safety drop. However, the general conclusion for this question is still unexplored, when we do not consider adding any task-unrelated data into the dataset. In our experiment, we prove that benign datasets can increase or maintain safety alignment instead of harming it.


\begin{table}[t]
\vspace{-5pt}
\centering
\caption{Evaluation of safety alignment after fine-tuning on different datasets. A lower $\mathrm{SA}$ stands for higher safety alignment level.}
\vspace{5pt}
\label{tab:t1}
\begin{adjustbox}{max width=1.0\linewidth}
\begin{tabular}{c|cc|cc|cc}
 \Xhline{2pt}
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c|}{\textbf{Llama-3}} & \multicolumn{2}{c|}{\textbf{Mistral}} & \multicolumn{2}{c}{\textbf{Qwen2}} \\ \cline{2-7} 
 & \multicolumn{1}{c|}{$\mathrm{SA}_g$ $\downarrow$} & $\mathrm{SA}_a$$\downarrow$ & \multicolumn{1}{c|}{$\mathrm{SA}_g$$\downarrow$} & $\mathrm{SA}_a$$\downarrow$ & \multicolumn{1}{c|}{$\mathrm{SA}_g$$\downarrow$} & $\mathrm{SA}_a$$\downarrow$ \\ \hline
w/o tuning & \multicolumn{1}{c|}{4.44} & 36.70 & \multicolumn{1}{c|}{16.00} & 93.80 & \multicolumn{1}{c|}{4.12} & 86.50 \\ \hline
\CRAFTSG & \multicolumn{1}{c|}{4.10} & 28.98 & \multicolumn{1}{c|}{21.74} & 92.82 & \multicolumn{1}{c|}{4.44} & 79.32 \\ \hline
\MedicalInstruct & \multicolumn{1}{c|}{15.34} & 95.82 & \multicolumn{1}{c|}{41.16} & 97.98 & \multicolumn{1}{c|}{11.12} & 88.12 \\
Reformat & \multicolumn{1}{c|}{4.50} & 70.28 & \multicolumn{1}{c|}{37.68} & 87.60 & \multicolumn{1}{c|}{6.32} & 76.48 \\ \hline
\PythonCodeInstruct & \multicolumn{1}{c|}{4.32} & 73.38 & \multicolumn{1}{c|}{29.76} & 99.20 & \multicolumn{1}{c|}{3.22} & 90.76 \\
Reformat & \multicolumn{1}{c|}{2.68} & 36.68 & \multicolumn{1}{c|}{21.34} & 94.36 & \multicolumn{1}{c|}{5.72} & 83.72 \\ \hline
\OpenPlatypus & \multicolumn{1}{c|}{3.74} & 57.32 & \multicolumn{1}{c|}{46.70} & 98.16 & \multicolumn{1}{c|}{5.62} & 87.80 \\
Reformat & \multicolumn{1}{c|}{1.54} & 32.16 & \multicolumn{1}{c|}{12.24} & 91.36 & \multicolumn{1}{c|}{5.34} & 64.76 \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\vspace{-20pt}
\end{table}

%2. find a counterexample that instruction-tuning help the safety alignment

In Table~\ref{tab:t1}, we find that a public instruction-tuning dataset, \CRAFTSG, will enhance or maintain the safety alignment of LLMs that are fine-tuned on it. To better understand the reason that this dataset is beneficial to the model safety, we deeply analyze this dataset. Without considering the specific knowledge in the dataset, we notice a general feature, the answer structure, in this dataset, which strictly follows the Markdown format with clear itemization. It is worth exploring how the answer structures will impact the model's alignment, and verifying whether it is a general impact factor in different datasets.

%3. introduce a method based on ICL to reformat answers
Considering there are more than ten thousands of data items in every dataset, it is unrealistic to manually reformat the answers. We build an automatic pipeline with the in-context learning (ICL) method~\cite{brown_language_2020} to let an external LLM reformat the answers. Specifically, we adopt the aligned Llama-3.1-8B-Instruct~\cite{llama3.1} as the external LLM and use three examples as demonstrations in the ICL prompt. We leave the detailed setups and the used ICL prompt in Appendix~\ref{ap:icl_info}. Then, every answer $x_a$ from \MedicalInstruct, \PythonCodeInstruct, and \OpenPlatypus is reformatted through this pipeline without changing its original semantics.


\begin{table*}[t]
\centering
\caption{Evaluation of the impacts of identity calibration and role-play in datasets.}
\vspace{5pt}
\label{tab:t2}
\begin{adjustbox}{max width=0.8\linewidth}
\begin{tabular}{c|c|c|c|cc|cc|cc}
 \Xhline{2pt}
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Reformat}} & \multirow{2}{*}{\textbf{\shortstack{Identity \\ Calibration}}} & \multirow{2}{*}{\textbf{Role-play}} & \multicolumn{2}{c|}{\textbf{Llama-3}} & \multicolumn{2}{c|}{\textbf{Mistral}} & \multicolumn{2}{c}{\textbf{Qwen2}} \\ \cline{5-10} 
 &  &  &  & \multicolumn{1}{c|}{$\mathrm{SA}_g$ $\downarrow$} & $\mathrm{SA}_a$$\downarrow$ & \multicolumn{1}{c|}{$\mathrm{SA}_g$$\downarrow$} & $\mathrm{SA}_a$$\downarrow$ & \multicolumn{1}{c|}{$\mathrm{SA}_g$$\downarrow$} & $\mathrm{SA}_a$$\downarrow$ \\ \hline
w/o tuning & \XSolidBrush & \XSolidBrush & \XSolidBrush & \multicolumn{1}{c|}{4.44} & 36.70 & \multicolumn{1}{c|}{16.00} & 93.80 & \multicolumn{1}{c|}{4.12} & 86.50 \\ \hline
\multirow{6}{*}{\CRAFTSG} & \XSolidBrush & \Checkmark & \XSolidBrush & \multicolumn{1}{c|}{4.10} & 28.98 & \multicolumn{1}{c|}{21.74} & 92.82 & \multicolumn{1}{c|}{4.44} & 79.32 \\
 & \XSolidBrush & \Checkmark & \Checkmark & \multicolumn{1}{c|}{5.42} & 35.76 & \multicolumn{1}{c|}{25.66} & 90.87 & \multicolumn{1}{c|}{5.06} & 77.46 \\
 & \Checkmark & \Checkmark & \XSolidBrush & \multicolumn{1}{c|}{3.06} & 33.54 & \multicolumn{1}{c|}{24.00} & 91.64 & \multicolumn{1}{c|}{4.34} & 75.00 \\
 & \Checkmark & \Checkmark & \Checkmark & \multicolumn{1}{c|}{4.50} & 37.38 & \multicolumn{1}{c|}{25.98} & 87.94 & \multicolumn{1}{c|}{4.28} & 68.62 \\
 & \Checkmark & \XSolidBrush & \XSolidBrush & \multicolumn{1}{c|}{39.32} & 90.44 & \multicolumn{1}{c|}{37.98} & 94.00 & \multicolumn{1}{c|}{5.78} & 83.18 \\
 & \Checkmark & \XSolidBrush & \Checkmark & \multicolumn{1}{c|}{64.22} & 96.44 & \multicolumn{1}{c|}{32.80} & 94.68 & \multicolumn{1}{c|}{6.36} & 89.66 \\ \hline
\multirow{4}{*}{\MedicalInstruct} & \XSolidBrush & \XSolidBrush & \Checkmark & \multicolumn{1}{c|}{15.34} & 95.82 & \multicolumn{1}{c|}{41.16} & 97.98 & \multicolumn{1}{c|}{11.12} & 88.12 \\
 & \XSolidBrush & \XSolidBrush & \XSolidBrush & \multicolumn{1}{c|}{13.92} & 77.30 & \multicolumn{1}{c|}{40.32} & 97.68 & \multicolumn{1}{c|}{13.44} & 86.44 \\
 & \Checkmark & \XSolidBrush & \Checkmark & \multicolumn{1}{c|}{4.50} & 70.28 & \multicolumn{1}{c|}{37.68} & 87.60 & \multicolumn{1}{c|}{6.32} & 76.48 \\
 & \Checkmark & \XSolidBrush & \XSolidBrush & \multicolumn{1}{c|}{3.00} & 45.68 & \multicolumn{1}{c|}{39.92} & 87.32 & \multicolumn{1}{c|}{5.90} & 76.86 \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\vspace{-15pt}
\end{table*}


%4. analyze the results
To prove the claim that the answer structures will impact alignment, we train LLMs on original and reformatted datasets and present the results in Table~\ref{tab:t1}. Based on the results, we have three main conclusions. First, if the LLMs have a better safety level under adversarial jailbreaking attacks, i.e., a lower $\mathrm{SA}_a$, fine-tuning causes more serious damage on $\mathrm{SA}_a$, which means that protecting models under adversarial attacks is more challenging. Second, we notice that for most cases, fine-tuning LLMs on an instruction-tuning dataset will decrease both $\mathrm{SA}_g$ and $\mathrm{SA}_a$. However, for different LLMs, the impacts of fine-tuning on safety alignment are divergent. For example, when fine-tuning Llama-3 and Mistral on \OpenPlatypus, $\mathrm{SA}_g$ shows a contradictory changing tendency. It indicates that different LLMs have various tastes in datasets, which is probably related to the post-training data distribution. Third, after reformatting the answer, we find both $\mathrm{SA}_g$ and $\mathrm{SA}_a$ drop compared with results on original datasets in most cases, proving the statement that LLMs are picky to the answer format. It indicates that aligned LLMs prefer the Markdown formatted answer structure with detailed itemized answers, which is an affinity format. We provide explanations of this phenomenon in Appendix~\ref{ap:rethink_llm}. The reformatted answer examples can be found in Appendix~\ref{ap:example_reformat}. 

\begin{tcolorbox}[colback=gray!10, colframe=black!75, boxrule=0.2mm, arc=2mm, width=0.475\textwidth, boxsep=0mm]
\begin{observation}
    \label{obs:answer-structure}
    LLMs have preferences on the answer structure. Fine-tuning LLMs on datasets with affinity answer structures will enhance or keep the safety alignment. Otherwise, the safety alignment will be compromised.
\end{observation}
\end{tcolorbox}


Although answer format can influence the safety alignment, we find it is only one of the factors. We discuss other factors in the next part.





\iffalse
\begin{table}[t]
\centering
\begin{adjustbox}{max width=1.0\linewidth}
\begin{tabular}{cc|cc}
 \Xhline{2pt}
\multicolumn{2}{c|}{\multirow{2}{*}{Reward Model}} & \multicolumn{2}{c}{Llama-3} \\ \cline{3-4} 
\multicolumn{2}{c|}{} & \multicolumn{1}{c|}{$\mathrm{SA}_g$ $\downarrow$} & $\mathrm{SA}_a$$\downarrow$ \\ \hline
\multicolumn{2}{c|}{w/o tuning} & \multicolumn{1}{c|}{4.44} & 36.70 \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{SkyworkGemma}} & \texttt{Top Set} & \multicolumn{1}{c|}{6.86} & 82.40 \\ \cline{2-4} 
\multicolumn{1}{c|}{} & \texttt{Bottom Set} & \multicolumn{1}{c|}{10.06} & 92.34 \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{SkyworkLlama}} & \texttt{Top Set} & \multicolumn{1}{c|}{4.58} & 62.78 \\ \cline{2-4} 
\multicolumn{1}{c|}{} & \texttt{Bottom Set} & \multicolumn{1}{c|}{18.14} & 94.18 \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{URM}} & \texttt{Top Set} & \multicolumn{1}{c|}{6.80} & 73.94 \\ \cline{2-4} 
\multicolumn{1}{c|}{} & \texttt{Bottom Set} & \multicolumn{1}{c|}{20.30} & 95.70 \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{QRM}} & \texttt{Top Set} & \multicolumn{1}{c|}{6.72} & 71.04 \\ \cline{2-4} 
\multicolumn{1}{c|}{} & \texttt{Bottom Set} & \multicolumn{1}{c|}{16.10} & 95.46 \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{Internlm2}} & \texttt{Top Set} & \multicolumn{1}{c|}{5.18} & 65.74 \\ \cline{2-4} 
\multicolumn{1}{c|}{} & \texttt{Bottom Set} & \multicolumn{1}{c|}{13.26} & 93.34 \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\caption{Exploring safety alignment after fine-tuning on different subsets of \MedicalInstruct.}
\vspace{-15pt}
\label{tab:t3}
\end{table}
\fi 

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.48\linewidth}
  \centering
  \includegraphics[width=\linewidth]{SA_g_plot.pdf}
  \vspace{-20pt}
  \caption{$\mathrm{SA}_g$ after fine-tuning on different subsets.}
  \label{fig:topsa}
\end{subfigure} 
\begin{subfigure}[b]{0.48\linewidth}
  \centering
  \includegraphics[width=\linewidth]{SA_a_plot.pdf}
  \vspace{-20pt}
  \caption{$\mathrm{SA}_a$ after fine-tuning on different subsets.}
  \label{fig:bottomsa}
\end{subfigure} 
\vspace{-10pt}
\caption{Safety alignment changes after we fine-tune Llama-3 on different subsets of \MedicalInstruct. Dashed line denotes the safety level of Llama-3 before we fine-tune it on the dataset. Llama and Gemma denote SkyworkLlama and SkyworkGemma, respectively.}
\vspace{-10pt}
\label{fig:sa} 
\end{figure*}

\subsection{Identity Calibration and Role-play}

%1. instroduce an question that if answer in SG dataset changes the safety alignment breaks

In Section~\ref{sec:sub_answer}, we prove that reformatting the answer structure can mitigate the risks of safety alignment degradation. However, in Table~\ref{tab:t2}, we find that if we reformat the answer structure of \CRAFTSG based on the format from \MedicalInstruct, which in most cases causes a significant alignment drop as shown in Table~\ref{tab:t1}, it does not cause the alignment drop and sometimes increases the safety level of LLMs. This implies that answer structure is not the only factor that influences the LLM safety.



%2. give conclusion and introduce a new question to deeply study the identity calibration and roleplay

To disclose the other factors, we analyze the characteristics of answers from \CRAFTSG, finding that thousands of answers contain an explicit identity statement, such as `as an AI' and `as a language model'. It is because the answers in \CRAFTSG are generated by GPT-4~\cite{openai_gpt-4_2023}, but without the online searching service. GPT-4 is designed to provide users helpful and correct answers by OpenAI, reducing hallucination. Therefore, when the instruction $x_t$ contains some time-sensitive information, such as entertainment performances in a specific month and economy analysis of a specific time, the responses could contain sentences telling users that this content is generated by AI and could not be correct due to the model's knowledge limitation. We hypothesize that these sentences make the LLMs calibrate their identity during the instruction-tuning period, letting LLMs enhance the identity cognition of being a language model, a process dubbed \textbf{\textit{identity calibration}}. Therefore, the safety alignment is kept or further enhanced. On the other hand, we analyze the instruction used in \MedicalInstruct, finding that identity-related information is provided. Specifically, the instruction asks the model to play a specific role to achieve the following request, which is called \textit{\textbf{role-play}} in our paper.


%3. give a description on these two concepts

First, we give detailed descriptions for the concepts of `\textbf{\textit{identity calibration}}' and `\textbf{\textit{role-play}}' in the instruction-tuning task. The identity calibration means the target answer in the training data contains explicit information to tell the fine-tuned model that it is a language model. Adding rejection answers~\cite{hsu_safe_2024,bianchi_safety-tuned_2024} into the training data is a typical identity calibration method, as the rejection answers contain identity information, such as `As an AI language model'. For the role-play, it means that the instruction $x_t$ asks the LLM to pretend it is a specific role to finish the task provided by $x_i$\footnote{We discuss the differences between role-play and identity shift~\cite{qi_fine-tuning_2024} in Appendix~\ref{ap:discussion_rp_is}.}. For example, in \MedicalInstruct, $x_t$ asks the model to be `a medical professional' and $x_i$ provides a detailed medical question. Based on previous observations, in most cases, adding the identity calibration to the dataset could enhance or keep the safety alignment, and using the role-play will harm it. 


%4. explain how we conduct experiment and analyze results
We design detailed experiments to systemically prove these points.
First, we find it is easier to detect the identity-related context in the reformatted \CRAFTSG compared to the original dataset, as the reformatted answers are simple and concise without additional structure information. Besides, removing the identity-related context on the reformatted answer is much easier for the same reasons. Therefore, we only remove the identity calibration on the reformatted \CRAFTSG. Specifically, we write a rule-based matching mechanism to detect and remove these texts. Details can be found in Appendix~\ref{ap:id_remove}. On the other hand, to study the role-play mechanism, we modify $x_t$ in \MedicalInstruct and \CRAFTSG\footnote{We only consider these two datasets because $x_t$ and $x_i$ in other datasets provide necessary information for the fine-tuning tasks.}, respectively. To disable the role-play in \MedicalInstruct, we replace $x_t$ with $x_i$ in each data item and let $x_i$ be empty. A new $x_t$, `\textit{\texttt{Answer the question truthfully, you are a tourist guide in Singapore.}}', is added to \CRAFTSG, and we set $x_i$ with the original $x_t$ to enable the role-play in \CRAFTSG. In Table~\ref{tab:t2}, we provide detailed results for the identity calibration and the role-play.

From the results, we have several meaningful conclusions. First, we find identity calibration has higher priority than answer structure and role-play. If identity calibration exists in a dataset, adding additional role-play or modifying the answer's structure will not influence the safety alignment a lot. Second, role-play has varying influences for different LLMs. For example, adding role-play will decrease the safety level for Llama-3, but it will enhance safety for Mistral and Qwen2 in some cases. This implies different LLMs have various preferences for playing different roles. We believe it can be related to the system prompt or other setups, such as rejection prompt templates, used in the post-training phase. Overall, we find the combination of identity calibration and a good answer format can help LLMs keep or enhance safety in most cases. However, adding identity calibration is more arguable than simply reformatting the answer's structure. We discuss this point in Appendix~\ref{ap:rethink_llm} based on the point of user experience and mainstream approaches. It suggests the importance of building a high-quality instruction-tuning dataset for model developers.

\begin{tcolorbox}[colback=gray!10, colframe=black!75, boxrule=0.2mm, arc=2mm, width=0.475\textwidth, boxsep=0mm]
\begin{observation}
    \label{obs:identity}
    LLMs are sensitive to identity-related information that exists in the dataset. Identity calibration helps LLMs keep the safety alignment and has priority. Instead, role-play usually harms the safety alignment.
\end{observation}
\end{tcolorbox}

%In this section, we reveal and prove that LLMs are picky to fine-tuning datasets. There are three factors we find will influence the safety alignment in the datasets. 



\section{Unreliable RMs}
\label{sec:unreliable}
%1. discuss the reward model capability and introduce a question about hwo reliable of scores

RMs are introduced during post-training to help LLMs distinguish good and bad answers, further aligning LLMs with human preference. Previous works~\cite{casper_open_2023,chaudhari_rlhf_2024,lambert_alignment_2023,lambert_rewardbench_2024} discuss the limitations of RMs, including generalization, robustness, quality, diversity, and evaluation.

Our experiments focus on RMs' generalization and robustness, i.e., their reliability on downstream datasets, which can be represented as the ability to select the better answer from a branch of candidates. We consider a scenario where the RM gives scores for data in the downstream training set. Based on this point, two potential applications can be designed. The first one is to select data items with higher scores to fine-tune the LLMs~\cite{chen_alpagasus_2024}, reducing the size of the dataset and training cost. The second one is to select the better answer based on the score that can improve the performance of the training model when there exists more than one answer for each data item~\cite{wang_interpretable_2024}. For both applications, because the training data always have higher scores, equalizing to better alignment with human preference, the fine-tuned LLMs should be aligned with better safety. However, based on our experiments, we find that these state-of-the-art open-source RMs do not always provide reliable scores.

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.48\linewidth}
  \centering
  \includegraphics[width=\linewidth]{disagreement_heatmap_medical_topk_4000.pdf}
  \vspace{-15pt}
  \caption{Disagreement between two models on \texttt{Top Set}.}
  \label{fig:top}
\end{subfigure} 
\begin{subfigure}[b]{0.48\linewidth}
  \centering
  \includegraphics[width=\linewidth]{disagreement_heatmap_medical_lowk_4000.pdf}
  \vspace{-15pt}
  \caption{Disagreement between two models on \texttt{Bottom Set}.}
  \label{fig:bottom}
\end{subfigure} 
\vspace{-10pt}
\caption{Reward models show different preferences when scoring data. Llama and Gemma stand for SkyworkLlama and SkyworkGemma, respectively. The results are obtained on \MedicalInstruct.}
\vspace{-10pt}
\label{fig:disagree} 
\end{figure*}

%2. explain our experiment details about how to score each data item
Specifically, we design experiments to study both applications. We only consider regression-based RMs~\cite{liu_skywork-reward_2024,wang_helpsteer2-preference_2024}, which always give determined scores for the same data. We do not consider generation-based RMs~\cite{openai_gpt-4_2023,anil_gemini_2023,anthropic_claude_nodate}, which are less reliable in our considered scenarios, and
will give different scores for the same data if we modify the inference prompt, generating temperature, and random seed. We directly adopt RMs to predict scores for data items in the datasets and study their reliability. We provide deeper and more detailed analysis in Appendix~\ref{ap:rethink_rm}. Scoring examples can be found in Appendix~\ref{ap:example_reformat}. 





\subsection{Absolute Scores in Datasets}

We study the first application, i.e., selecting data items with higher scores to fine-tune the LLMs with RMs.
%1. a more deep experiment on dataset inside (training model only on top or bottom data) and analyze
We sort the training data in \MedicalInstruct based on the scores and obtain two sets, each containing 4,000 items. The first set only contains data having the highest scores, named \texttt{Top Set}. The second contains data having the lowest scores, named \texttt{Bottom Set}. In Figure~\ref{fig:sa}, we show the results on different subsets scored with five RMs, respectively. We have two conclusions. First, LLMs trained on \texttt{Top Set} always have better safety alignment than LLMs trained on \texttt{Bottom Set}, which is general to the RMs. Second, different RMs have distinct preferences in scoring data. We find that models trained on subsets separated based on different RMs achieve varying safety levels. To better quantify the differences, we count the number of disagreements between RMs in Figure~\ref{fig:disagree}. The results indicate that the disagreement exists in both \texttt{Top Set} and \texttt{Bottom Set}, and is relatively uniform and consistent about the preference.

Therefore, when using the absolute scores to select data from a dataset, we can fine-tune an LLM using them and keep its safety alignment, but the results are highly related to the RMs used to score the data. RMs used to select the data could have not aligned with the LLM in terms of preference policy. It implies unreliability in such an application, as we have no information about the quality of the selected data only after we evaluate the fine-tuned model's safety.

\begin{tcolorbox}[colback=gray!10, colframe=black!75, boxrule=0.2mm, arc=2mm, width=0.475\textwidth, boxsep=0mm]
\begin{observation}
    \label{obs:absolute_score}
    RMs can distinguish high-quality and low-quality data within the dataset. However, different RMs do not have the same quality evaluation criteria, causing quite diverse selection results.
\end{observation}
\end{tcolorbox}


\begin{table*}[t]
\centering
\caption{Score comparison between original data and reformatted data. The original data is the baseline. `Increase' means the reformatted data have higher scores than the original ones. `Decrease' means the reformatted data have lower scores than the original ones. We show the percentage and the average improved or worsened score for increased and decreased data, respectively.}
\vspace{5pt}
\label{tab:t4}
\begin{adjustbox}{max width=1.0\linewidth}
\begin{tabular}{c|cc|cc|cc|cc}
 \Xhline{2pt}
\multirow{2}{*}{\textbf{Reward Model}} & \multicolumn{2}{c|}{\CRAFTSG} & \multicolumn{2}{c|}{\MedicalInstruct} & \multicolumn{2}{c|}{\PythonCodeInstruct} & \multicolumn{2}{c}{\OpenPlatypus} \\ \cline{2-9} 
 & \multicolumn{1}{c|}{Increase} & Decrease & \multicolumn{1}{c|}{Increase} & Decrease & \multicolumn{1}{c|}{Increase} & Decrease & \multicolumn{1}{c|}{Increase} & Decrease \\ \hline
SkyworkGemma & \multicolumn{1}{c|}{30.49\%, 1.19} & 68.57\%, -1.89 & \multicolumn{1}{c|}{40.48\%, 1.68} & 58.71\%, -2.07 & \multicolumn{1}{c|}{18.78\%, 1.12} & 80.72\%, -2.74 & \multicolumn{1}{c|}{47.78\%, 1.68} & 50.52\%, -1.91 \\ \hline
SkyworkLlama & \multicolumn{1}{c|}{18.53\%, 2.76} & 80.87\%, -8.14 & \multicolumn{1}{c|}{37.40\%, 6.69} & 62.15\%, -8.00 & \multicolumn{1}{c|}{39.07\%, 7.41} & 60.74\%, -11.42 & \multicolumn{1}{c|}{58.62\%, 13.81} & 39.92\%, -10.29 \\ \hline
URM & \multicolumn{1}{c|}{27.90\%, 1.00} & 71.74\%, -1.60 & \multicolumn{1}{c|}{14.04\%, 1.95} & 85.84\%, -4.19 & \multicolumn{1}{c|}{8.39\%, 1.42} & 91.59\%, -6.33 & \multicolumn{1}{c|}{36.39\%, 1.83} & 62.38\%, -3.71 \\ \hline
QRM & \multicolumn{1}{c|}{23.70\%, 3.46} & 75.94\%, -6.23 & \multicolumn{1}{c|}{22.30\%, 5.08} & 77.57\%, -9.93 & \multicolumn{1}{c|}{11.85\%, 5.33} & 88.14\%, -20.31 & \multicolumn{1}{c|}{53.69\%, 7.74} & 45.09\%, -11.38 \\ \hline
Internlm2 & \multicolumn{1}{c|}{1.85\%, 0.19} & 97.78\%, -0.92 & \multicolumn{1}{c|}{92.45\%, 1.38} & 7.51\%, -0.51 & \multicolumn{1}{c|}{96.56\%, 2.27} & 3.43\%, -0.86 & \multicolumn{1}{c|}{48.83\%, 0.67} & 50.04\%, -0.82 \\ 
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\vspace{-15pt}
\end{table*}




\subsection{Pairwise Selection with RMs}

%1. show socring crossing data results and analyze
To study the second application scenario, we adopt RMs to score original and reformatted datasets in Section~\ref{sec:picky}. Then, we compare the scores between the original and the corresponding reformatted data items, as shown in Table~\ref{tab:t4}.
%we compare the scores before and after the reformatting operation under different RMs. 
Based on the results in Tables~\ref{tab:t1} and \ref{tab:t2}, simply reformatting \CRAFTSG will not decrease the model's alignment, and reformatting other datasets will increase or keep the model's alignment. Therefore, ideally, RMs should give similar scores for reformatted and original data in \CRAFTSG, while giving higher scores for reformatted data than original data in other datasets. However, most RMs do not have clear criteria. For example, most RMs give lower scores to reformatted data in all datasets, which means the answer structure is not the principle criteria or RMs have their own criteria to the answer structure, distinct from the LLM's preference. On the other hand, we find that Internlm2 shows the most reasonable results on most datasets. However, it has the lowest performance on the RewardBench compared with other RMs in our experiments. We think most state-of-the-art RMs are overfitting to the RewardBench, and it actually cannot correctly reflect real human preference. As it is beyond the scope of this paper to verify this point and propose new benchmarks or better RMs, we only introduce our ordinary assumption and leave the detailed verification in future work. Overall, based on our experiment, we prove that RMs are not reliable in comparing two answers' quality and determining which is better aligned with human preference.

\begin{tcolorbox}[colback=gray!10, colframe=black!75, boxrule=0.2mm, arc=2mm, width=0.475\textwidth, boxsep=0mm]
\begin{observation}
    \label{obs:pairwise_score}
    RMs cannot correctly identify human's preferred data from a group of candidates. Therefore, RMs fail to predict the tendency of the changes in the safety alignment after fine-tuning LLMs on the data.
\end{observation}
\end{tcolorbox}

%In this section, we discuss the reliability of two potential applications of RMs in instruction-tuning tasks. We prove the uncertainty and RM-related issues in two applications with our experiments. 

\section{Guidance for Safety-aware Fine-tuning}

We provide practical suggestions based on our observations from two aspects, i.e., building a good instruction-tuning dataset and selecting a good RM.

To build a good dataset, there are four suggestions:
\begin{itemize}
    \setlength{\itemsep}{2pt} % Remove space between items
    \setlength{\parskip}{0pt} % Remove space between paragraphs within items
    \setlength{\topsep}{-10pt}  % Remove space above and below the list
\item\textbf{Affinity Answer Structure.} Format answers in a structured, itemized style (e.g., Markdown).

\item\textbf{Adding Synthetic Data.} Many LLMs share training data, leading to similar answer preferences. Therefore, synthetic data from well-aligned models helps improve smaller models.

\item\textbf{Less Identity Calibration.} While it enhances safety, excessive disclaimers make responses verbose and reduce perceived usefulness and user experience.

\item\textbf{Carefully Using Role-Play.} Role-play causes diverse impacts on LLMs. It will influence the cognition of LLMs from a deeper level.
\end{itemize}


To select a good RM, there are three suggestions:
\begin{itemize}
    \setlength{\itemsep}{2pt} % Remove space between items
    \setlength{\parskip}{0pt} % Remove space between paragraphs within items
    \setlength{\topsep}{-10pt}  % Remove space above and below the list
\item\textbf{Using RM Aligned with LLM Preferences.} The best RM is the one used in the LLM’s post-training phase, as it strictly guides the model's preference.


\item\textbf{Ensuring Comprehensive Training Data.} When the RM used in post-training is inaccessible, the selected RM should be trained on diverse preference data from different sources.

\item\textbf{Evaluation with Multiple Benchmarks.} A single benchmark can be biased or noisy. Therefore, testing on diverse and comprehensive benchmarks improves the selected RM reliability.
\end{itemize}

In Appendix~\ref{ap:rethink_llm} and Appendix~\ref{ap:rethink_rm}, we provide more detailed explanations of the reasons that we give. With these guidances for safety-aware fine-tuning, it is promising to build a safe and helpful LLM for downstream tasks.

%The results alarm the model developers to choose proper RMs. If possible, the best choice is the RM used in the post-training phase, as it provides the preference policy for the LLM. Therefore, providing corresponding RMs of open-source LLMs by model owners will significantly increase the downstream products' safety and better enhance the safety of aligned LLMs.

%Details in Appendix H and G.

\section{Limitations}
%1. not consider close source models, no commercial model
There are several limitations in our work. First, in our experiments, we only consider open-source models for both LLMs and RMs. The main reason is that open-source models provide full controllability in the experiments, which could assist our analysis. Commercial models may have other factors, e.g., system prompts and inference strategies, that can affect the safety alignment. We believe it is an important and valuable orientation to study commercial LLMs and RMs in future work.
%Considering it is the first work studying the influence of the elements in the fine-tuning datasets and the unknown fine-tuning details and other factors, such as system prompts and inference strategies, in the commercial models could cause impacts on the final safety evaluations, we choose to study the safety degradation with full access, which is a bit different from previous works. We believe it is an important and valuable orientation to study more commercial LLMs and RMs in future work.


%2. only english datasets, no other language
Second, the datasets used in our experiments are constricted in English. We notice that there are more and more works starting to study the impacts of different languages, including English, Chinese, Japanese, and so on. However, most open-source LLMs and RMs have better performance in the English environment, and model developers mainly perform alignment on English datasets. We believe that with the development of LLMs, the performance, including alignment, will be closer among different languages. In future work, we think it is meaningful to study the same features, such as answer structures, in different language datasets.

%3. only lora fine-tuning, no full parameter tuning
Third, we only consider fine-tuning LLMs with LoRA. As LoRA can achieve similar performance with less computational cost, it is a popular technic in model fine-tuning. 
%Besides, previous works~\cite{hsu_safe_2024} prove that similar to full parameter tuning, fine-tuning LLMs with LoRA faces the same safety degradation. In this paper, we focus on the fine-tuning dataset instead of the impacts of fine-tuning strategies. Therefore, 
Comparing different fine-tuning methods could be a critical part of future work.



\section{Conclusion}

In this paper, we study the safety decrease phenomenon under a benign scenario. Specifically, three factors are found that can impact the model's safety level, including the answer's format, identity calibration, and role-play. We experimentally prove that we can adjust these factors in a benign dataset to increase or decrease the model's safety. This indicates the importance of building a high-quality downstream dataset. On the other hand, we study the reliability of reward models in scoring downstream data. The results reveal the limitations that widely exist in advanced reward models. Our work provides a deep analysis of the phenomena observed in our experiments, which can help model developers avoid potential safety risks in practice. 
%In future work, we think it is important to build a more general reward model and design a correct evaluation process for it. Besides, the post-training phase, which improves the model's safety alignment, should be explored to address the limitations of LLMs.



\section*{Impact Statement}

There are two-fold impacts considering our experiments discovering several new features of the instruction-tuning datasets. For the good part, our work proves that we can simply reformat the answer structure to enhance or keep the safety alignment. On the other hand, we can add some identity information to better align the model or remove the role-play details to improve or maintain the safety level without reducing the performance. For the bad part, we think some malicious users can change the answer's structure on purpose to decrease safety without being detected. It can be treated as a type of data poisoning attack, but more stealthy.

We also study reward models in our paper. We believe our work will encourage others to build a better evaluation baseline for reward models, which benefits the deep learning and security communities.

\bibliography{example_paper,bib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Dataset Information}
\label{ap:dataset_info}

\MedicalInstruct This dataset contains 26,357 items in total. It combines two previous medical datasets, i.e., Medical Meadow Wikidoc~\cite{meadow} and MedQuAD~\cite{Medquad}. $x_t$ in this dataset is always `Answer the question truthfully, you are a medical professional.'. $x_i$ is a medical question, such as `Can you provide me with information regarding statins?'. $x_a$ is the responding answer to the medical question $x_i$.


\CRAFTSG This dataset contains 26,346 items. The instruction-answer pairs are generated by GPT-4, all related to Singapore. $x_t$ in this dataset is a question about Singapore, such as `What other iconic landmarks and attractions in Singapore, besides the Marina Bay Sands, showcase the city's luxurious and extravagant side?'. $x_i$ in this dataset is always empty. $x_a$ is the answer to the question $x_t$.


\PythonCodeInstruct There are 18,612 data in this dataset. It contains problem descriptions and code in Python language. $x_t$ provides a specific request, such as `Write a Python program to calculate the average of a list of positive integers and output the result.'. $x_i$ gives additional information about the request, such as `List of positive integers: [1, 5, 6, 7, 8]'. $x_a$ is the Python code for the request.


\OpenPlatypus There are 24,926 data in total. It is constructed by 11 science, code, and math datasets. $x_t$ describes a specific question, such as `A board game spinner is divided into three parts labeled $A$, $B$ and $C$. The probability of the spinner landing on $A$ is $\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\frac{5}{12}$. What is the probability of the spinner landing on $C$? Express your answer as a common fraction.'. $x_i$ in most cases is empty. For some multiple choice questions in the dataset, $x_i$ is `Choose A, B, C or D as your solution.'. $x_a$ is the answer to the question $x_t$.

\section{Details of LLMs and RMs}
\label{ap:details_llm_rm}

\texttt{Llama-3} In our experiment, we adopt Meta-Llama-3-8B-Instruct from the Llama-3 series. It is an auto-regressive language model based on transformer architecture. Based on the description of Meta, the instruction version is trained with supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.

\texttt{Mistral} Mistral-7B-Instruct-v0.2 is selected by us because it achieves better performance than its previous versions under various safety evaluations. 

\texttt{Qwen2} We choose the Qwen2-7B-Instruct in our experiment. Based on the model developer's description, the instruction version is trained with SFT and direct preference optimization (DPO) to align with human preferences for helpfulness and safety.


\texttt{SkyworkGemma} This reward model is Skywork-Reward-Gemma-2-27B-v0.2. The model owners remove the contaminated data used in v0.1 in the training set and fine-tune a gemma-2-27b-it model. As of January 2025, it ranks 3rd on the RewardBench leaderboard with a score of 94.3.

\texttt{SkyworkLlama} We use Skywork-Reward-Llama-3.1-8B-v0.2 in our experiment. Similarly, the model owners remove the contaminated data and fine-tune a Llama-3.1-8B-Instruct model. As of January 2025, it ranks 10th on the RewardBench leaderboard with a score of 93.1.

\texttt{URM} URM-LLaMa-3.1-8B is used. It is an uncertain-aware reward model. The model owner fine-tunes Skywork-Reward-Llama-3.1-8B-v0.1 and adds additional uncertainty-aware and attribute-specific value heads. As of January 2025, it ranks 12th on the RewardBench leaderboard with a score of 92.9.

\texttt{QRM} QRM-Llama3.1-8B is used. The model owner fine-tunes Skywork-Reward-Llama-3.1-8B-v0.1 with an additional gating network and a quantile regression layer. As of January 2025, it ranks 11th on the RewardBench leaderboard with a score of 93.1.

\texttt{Internlm2} We use the internlm2-7b-reward model. It is fine-tuned based on the foundation of InternLM2-Chat-7B-SFT. Based on the model owner's description, it has been trained using over 2.4 million preference samples, both human-annotated and AI-synthesized. It ensures a balance between helpful and harmless. As of January 2025, it ranks 34th on the RewardBench leaderboard with a score of 87.6.


\section{LoRA Fine-tuning Settings}
\label{ap:lora_info}

We follow the most popular LoRA settings and refer to the setups provided by Platypus~\cite{lee_platypus_2023} and Meta~\cite{dubey_llama_2024}. The details are shown in Table~\ref{tab:llm_ft}. We adopt two H100 to fine-tune the LLMs. There are two widely used instruction-tuning prompt templates used in our experiment for different $x_i$ conditions.

\vspace{5pt}
\begin{tcolorbox}[mylisting, title=\textbf{Instruction-tuning prompt template when $x_i$ is not empty.}]
\begin{lstlisting}[language=html, numbers=none]
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{x_t}\n\n### Input:\n{x_i}\n\n### Response:{x_a}
\end{lstlisting}
\end{tcolorbox}





\vspace{5pt}
\begin{tcolorbox}[mylisting, title=\textbf{Instruction-tuning prompt template when $x_i$ is empty.}]
\begin{lstlisting}[language=html, numbers=none]
Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{x_t}\n\n### Response:{x_a}
\end{lstlisting}
\end{tcolorbox}



\begin{table}[h]
\centering
\begin{adjustbox}{max width=1.0\linewidth}
\begin{tabular}{c|c}
 \Xhline{2pt}
\textbf{Hyperparameters} & \textbf{Value} \\ \hline
LoRA rank & 16 \\ \hline
LoRA $\alpha$ & 32 \\ \hline
LoRA dropout & 0.1 \\ \hline
LoRA module & q\_proj, o\_proj, k\_proj, v\_proj \\ \hline
learning rate & 1e-4 \\ \hline
float type & bf16 \\ \hline
epochs & 3 \\ \hline
batch size & 64 \\ \hline
weight decay & 0.0 \\ \hline
learning rate scheduler & cosine \\ \hline
warmup step & 100 \\ \hline
max length & 4096 \\ \hline
optimizer & adamw \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\caption{Hyperparameters used in fine-tuning LLMs.}
\vspace{-10pt}
\label{tab:llm_ft}
\end{table}

\section{LLM Inference Settings}
\label{ap:evaluation_set}

We evaluate the safety of LLMs on one H100. We disable the sampling function during the evaluation process. Because based on the previous work~\cite{huang_catastrophic_2024}, sampling settings, including temperature, top\_p, and top\_k, will significantly change the jailbreak success rate. On the other hand, we do not use system prompts during the evaluation. Similarly, previous works~\cite{huang_catastrophic_2024,lyu_keeping_2024} find that system prompt will influence safety as well. Considering we focus on the influence of datasets, we control the inference process and make sure that no other factors will influence the safety alignment. The detailed settings are in Table~\ref{tab:llm_infer}.

\begin{table}[h]
\centering
\begin{adjustbox}{max width=1.0\linewidth}
\begin{tabular}{c|c}
 \Xhline{2pt}
\textbf{Hyperparameters} & \textbf{Value} \\ \hline
system prompt & none \\ \hline
top p & none \\ \hline
top k & none \\ \hline
temperature & none \\ \hline
num beams & 1 \\ \hline
do sample & false \\ \hline
max new tokens & 512 \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\caption{Inference settings used in safety evaluation.}
\vspace{-15pt}
\label{tab:llm_infer}
\end{table}


\section{ICL Reformatting Pipeline}
\label{ap:icl_info}

In our experiment, we design an automatic answer reformatting pipeline based on ICL. For \MedicalInstruct, \PythonCodeInstruct, and \OpenPlatypus, we adopt the same ICL system prompt with three demonstration examples, randomly selected from $x_a$ in \CRAFTSG. For \CRAFTSG, we adopt a new ICL system prompt with three demonstration examples, randomly selected from $x_a$ in \MedicalInstruct. The input for the LLM is the original answer $x_a$. The LLM will reformat it and give the new one. We adopt two H100 or four A6000 to run the reformatting pipeline under the configuration listed in Table~\ref{tab:reformatting}.



We find there are hundreds of failure cases in the reformatted datasets, occupying about $1\%\sim3\%$ of all data. These failure cases are caused by different reasons, and we give some analysis after manually checking them. The first reason is that the original $x_a$ provides too little information to reformat it. For example, we notice $x_a$ in \OpenPlatypus can be a single number or a selection from $[A, B, C, D]$, causing the LLM to give the demonstration examples used in the system prompt. The second is that the LLM generates additional context containing part of the demonstration examples used in the system prompt by mistake. We find that such failure cases exist uniformly in all datasets and we think the reason could be that the system prompt can influence the sampling process of the LLM, causing partial leakage. The third type of failure case is that $x_a$ contains some rejection pattern. These failure cases appear in \CRAFTSG. We find $x_a$ in \CRAFTSG contains some explicit pattern, such as `As of my last update, I do not have real-time information or the latest details on specific accidents or incidents.' and `As of my last update in early 2023, I cannot provide real-time or the most recent data.'. We find when using the LLM to reformat $x_a$ containing such patterns, it directly outputs the demonstration examples used in the system prompt with very high probability.

We further design a rule-based checker to automatically detect failure cases and fix them with a new inference strategy. Specifically, our rule-based checker is designed by using the special words that appear in the system prompt to match the reformatted $x_a$. The special words are listed in Table~\ref{tab:special_words}. After detecting the matched cases, we manually check the reformatted $x_a$ to determine whether we should reformat it again because several special words are in the original $x_a$ as well. 

To reformat $x_a$ again, we directly provide the original $(x_t, x_i, x_a)$ to the LLM and ask it to only reformat $x_a$ with the same demonstration examples. After each reformatting step, we adopt the rule-based checker to check again until there are no failure cases in the reformatted dataset. In Table~\ref{tab:failure_case_results}, we compare the effects of these failure cases. We find that these failure cases can cause very small impacts on the safety alignment. Usually, we observe less than 1\% changes across these datasets. However, to mitigate the marginal influence caused by failure cases, in all experiments in our main paper, we still adopt the reformatted datasets that do not have failure cases.

\vspace{5pt}
\begin{tcolorbox}[mylisting, title=\textbf{ICL system prompt for \MedicalInstruct, \PythonCodeInstruct, and \OpenPlatypus.}]
\begin{lstlisting}[language=html, numbers=none]
Rewrite the text to follow the given format examples and keep the semantics unchanged.

Rewrite the text, instead of outputting the format examples!

Format Example 1:
Singapore is a melting pot of cultures, and there are numerous ways to experience its cultural diversity and religious harmony beyond the usual methods of sampling local cuisine and visiting temples, mosques, and churches. Here are some unique activities to consider:

1. **Cultural Festivals and Celebrations:**
   - Participate in or observe celebrations such as Chinese New Year, Deepavali, Hari Raya Puasa, and Vesak Day. These festivals often include street parades, live performances, and traditional activities.
   - Attend the Singapore Night Festival, which showcases the city's heritage, arts, and culture through various events and installations.

2. **Heritage Trails:**
   - Take guided heritage trails through neighborhoods like Chinatown, Little India, Kampong Glam, and Joo Chiat/Katong to learn about the history and evolution of these multicultural enclaves.

3. **Art and Performance:**
   - Visit the Esplanade Theatres on the Bay, which offers a wide range of performances including traditional ethnic music and dance.
   - Explore the National Gallery Singapore, which houses an extensive collection of Southeast Asian art that reflects the region's diverse cultural fabric.

4. **Cultural Workshops and Courses:**
   - Sign up for workshops to learn traditional crafts or art forms, such as Chinese calligraphy, Indian henna art, Malay batik painting, or Peranakan beadwork.
   - Take cooking classes that focus on the different ethnic cuisines and learn about the cultural significance of certain dishes.

5. **Community Engagement:**
   - Volunteer with organizations that work to promote intercultural dialogue and understanding.
   - Participate in community events or `gotong-royong' (community cooperation) activities that bring people from different backgrounds together.

6. **Cultural District Visits:**
   - Spend time in the Singapore River area, where Clarke Quay, Robertson Quay, and Boat Quay offer insights into the city's history and multicultural urban life.
   - Explore the Interlace of religious sites, where churches, temples, and mosques are situated close to each other, symbolizing religious harmony.

7. **Museums and Educational Centers:**
   - Visit the Asian Civilisations Museum, which celebrates the rich artistic heritage of Asia, including regions that have influenced Singaporean culture.
   - Explore the Peranakan Museum or the Indian Heritage Centre to dive deeper into the specific cultures of these unique Singaporean communities.

8. **Public Art and Installations:**
   - Discover public art installations that reflect Singapore's cultural diversity, such as murals in ethnic enclaves or sculptures in public spaces that tell stories of the nation's heritage.

9. **Neighborhood Walks:**
   - Go on self-guided walks through diverse neighborhoods, where you can observe the daily lives of residents, shop in local markets, and see the blend of traditional and modern influences.

10. **Attend a Religious Ceremony or Lecture:**
    - With permission, attend a religious ceremony at one of the many places of worship to gain firsthand experience of the religious practices and the spirit of acceptance that pervades them.
    - Attend interfaith dialogues or lectures that focus on religious harmony and the shared values among different faiths in Singapore.

Remember, when engaging in activities related to cultural and religious exploration, it is important to approach them with respect and sensitivity to local customs and practices.


Format Example 2:
Dr. David Loh is a well-respected aesthetic physician, particularly known for his expertise in Botox and fillers. As the medical director of David Loh Surgery, which is a clinic specializing in aesthetics and cosmetic surgery, his expertise in Botox and fillers contributes significantly to the services offered, especially those that are non-surgical or non-liposuction treatments.

Here's how Dr. David Loh's expertise enhances the clinic's offerings:

1. **Advanced Techniques**: Dr. Loh's training and experience allow him to perform advanced injection techniques, ensuring that patients receive the most effective and aesthetically pleasing results. His knowledge of facial anatomy helps in delivering precise treatments with minimal discomfort.

2. **Customized Treatments**: With a deep understanding of the variety of available fillers and their specific characteristics, Dr. Loh can tailor treatments to the individual needs and goals of his patients, creating natural-looking results.

3. **Safety and Quality Control**: His extensive background in the field means that he is well-versed in the safety protocols and can effectively manage any potential complications. This ensures a high level of care and quality control in the treatments offered.

4. **Training and Education**: Dr. Loh's experience in training other professionals in the use of Botox and fillers raises the standard of care at his clinic. He can impart his knowledge to his team, ensuring that all practitioners at David Loh Surgery are skilled in the latest techniques and best practices.

5. **Innovative Services**: His expertise allows the clinic to offer the latest and most innovative non-surgical treatments. Patients have access to a wide range of procedures that can rejuvenate the skin, reduce wrinkles, and enhance facial contours without the need for surgery.

6. **Comprehensive Approach**: Dr. Loh's skills in Botox and fillers complement other non-liposuction treatments such as laser therapies, chemical peels, and skin tightening procedures. This holistic approach can address multiple aesthetic concerns, from skin texture to volume loss.

7. **Building Patient Confidence**: Dr. Loh's reputation as an expert in Botox and fillers can attract patients who are seeking high-quality, non-invasive treatments. His expertise helps build trust and confidence among patients who are considering these procedures.

By offering a range of non-liposuction treatments, including Botox and fillers, David Loh Surgery can cater to patients looking for minimally invasive options to enhance their appearance. Dr. David Loh's expertise ensures that these treatments are performed with a high degree of skill and attention to detail, leading to better patient outcomes and satisfaction.


Format Example 3:
Physiotherapy is a healthcare profession that aims to restore, maintain, and maximize a patient's strength, function, movement, and overall well-being through physical rehabilitation, injury prevention, and health and fitness education. In Singapore, individuals with limited mobility, regardless of the cause, can benefit significantly from physiotherapy in several ways:

1. **Personalized Treatment Plans**: Physiotherapists in Singapore will create tailored treatment plans based on the individual's specific condition, needs, and goals. These plans often include exercises designed to improve strength, flexibility, balance, and coordination.

2. **Pain Management**: Physiotherapy can help reduce pain through various techniques such as manual therapy, heat and cold therapy, ultrasound, and electrical stimulation, making it easier for individuals to move and perform daily activities.

3. **Improving Mobility**: Through targeted exercises, stretching, and strength training, physiotherapy can help individuals regain mobility. This is particularly beneficial for those who have suffered from strokes, spinal cord injuries, or other conditions that affect movement.

4. **Fall Prevention**: By improving balance and educating on safe movement strategies, physiotherapists can help reduce the risk of falls, which is especially important for the elderly or those with conditions like Parkinson's disease.

5. **Postoperative Rehabilitation**: Following surgery, physiotherapy is crucial for regaining full function and speeding up recovery. This is particularly true for joint replacements, ligament repairs, and other orthopedic surgeries.

6. **Assistive Devices**: Physiotherapists can recommend and teach the proper use of assistive devices such as walkers, canes, or wheelchairs, which can enhance mobility and independence.

7. **Education and Support**: Physiotherapists provide education on how to manage conditions at home and prevent future injuries. This includes ergonomic advice and lifestyle modifications to support overall health.

8. **Aquatic Therapy**: Some physiotherapy centers in Singapore offer aquatic therapy, which can be particularly beneficial for individuals with limited mobility as the buoyancy of water reduces stress on joints, making it easier to perform exercises.

9. **Technology Integration**: Advanced technologies such as robotic exoskeletons, virtual reality, and biofeedback can be part of a physiotherapy regimen in Singapore, providing innovative ways to improve movement and track progress.

10. **Community Reintegration**: Physiotherapists also focus on helping individuals regain the skills necessary for day-to-day life, including community activities, which is critical for maintaining independence and quality of life.

For individuals in Singapore, accessing physiotherapy services can be done through public healthcare institutions like hospitals and polyclinics, as well as private clinics. The Singapore government provides subsidies for citizens and permanent residents under schemes such as the Community Health Assist Scheme (CHAS) and the Pioneer Generation Package, making physiotherapy more accessible and affordable. It's important for individuals seeking physiotherapy to consult with a licensed physiotherapist to receive a proper assessment and a customized treatment plan.
\end{lstlisting}
\end{tcolorbox}


\vspace{5pt}
\begin{tcolorbox}[mylisting, title=\textbf{ICL system prompt for \CRAFTSG.}]
\begin{lstlisting}[language=html, numbers=none]
Remove the format of the given text!

Do not itemize the text!

Do not use bullet points!

Do not use Markdown format!

Use as less paragraphs as possible!

Try to keep the text in one paragraph!

Rewrite the text to follow the below format examples!

Rewrite the text, instead of outputting the format examples!


Format Example 1:
UNAIDS and the World Health Organization estimate the global incidence of chancroid to be approximately 6 million cases per year.  A true incidence is difficult to determine due to lack of readily available diagnostic testing. H. ducreyi is difficult to culture so chancroid may be under-diagnosed.  Since 1987, reported cases of chancroid declined steadily until 2001. Since then, the number of cases reported has fluctuated, but still appearing to decline overall. 
Chancroid may develop in individuals of any age but is typically found in sexually active individuals with a mean patient age of 30 years.
The male-to-female ratio of patients with chancroid ranges from 3:1 in endemic areas to 25:1 during outbreak situations.  Female sex workers with either symptomatic chancroid or as asymptomatic carriers serve as a reservoir for H. ducreyi. 
Although race is not a risk factor, chancroid is seen more commonly in African Americans and Hispanics in the United States. 
Chancroid is uncommon in the United States and other developed countries, but can been present in endemic areas associated with the use of crack cocaine and prostitution. In the United States, the Centers for Disease Control and Prevention reported 6 cases of chancroid in 2014.  The majority of cases in developed countries occur in individuals who have returned from chancroid-endemic areas in the world.
Chancroid is a major cause of genital ulcer disease in Africa, southeast Asia and parts of Latin America.  Acquiring epidemiological data is more difficult in developing countries due to greater lack of resources to test for H. ducreyi. Chancroid is common in countries that have high rates of Human Immunodeficiency Virus (HIV) infection, because HIV facilitates acquisition of H. ducreyi and vice versa.


Format Example 2:
Before taking propafenone:
tell your doctor and pharmacist if you are allergic to propafenone or any other drugs. tell your doctor and pharmacist what prescription and nonprescription medications you are taking, especially anticoagulants ('blood thinners') such as warfarin (Coumadin), beta blockers such as atenolol (Tenormin), carteolol (Cartrol), labetalol (Normodyne, Trandate), metoprolol (Lopressor), nadolol (Corgard), propranolol (Inderal), sotalol (Betapace), and timolol (Blocadren); cimetidine (Tagamet); cyclosporine (Neoral, Sandimmune); digoxin (Lanoxin); quinidine (Quinaglute); rifampin (Rifadin); and vitamins. in addition to the condition listed in the IMPORTANT WARNING section, tell your doctor if you have or have ever had liver or kidney disease, congestive heart failure, a pacemaker, chronic bronchitis, asthma, or emphysema. tell your doctor if you are pregnant, plan to become pregnant, or are breast-feeding. If you become pregnant while taking propafenone, call your doctor. if you are having surgery, including dental surgery, tell the doctor or dentist that you are taking propafenone. you should know that this drug may make you drowsy or dizzy. Do not drive a car or operate machinery until you know how it affects you.


Format Example 3:
The adrenal cortex is composed of three distinct layers of endocrine cells which produce critical steroid hormones. These include the glucocorticoids which are critical for regulation of blood sugar and the immune system, as well as response to physiological stress, the mineralcorticoid aldosterone, which regulates blood pressure and kidney function, and certain sex hormones. Both benign and malignant tumors of the adrenal cortex may produce steroid hormones, with important clinical consequences.
Adrenocortical adenomas, or adrenocortical "nodules", are small, benign tumors of the adrenal cortex which are extremely common (present in 1-10% of persons at autopsy). The clinical significance of these neoplasms is twofold. First, they have been detected as incidental findings with increasing frequency in recent years, due to the increasing use of CT scans and magnetic resonance imaging in a variety of medical settings. This can result in expensive additional testing and invasive procedures to rule out the slight possibility of an early adrenocortical carcinoma. Second, a minority of adrenocortical adenomas are "functional", meaning that they produce glucocorticoids, mineralcorticoids, and/or sex steroids, resulting in endocrine disorders such as Cushing's syndrome, Conn's syndrome (hyperaldosteronism), virilization of females, or feminization of males. Functional adrenocortical adenomas are surgically curable.
Main article: Adrenocortical carcinoma
Adrenocortical carcinoma (ACC) is a rare, highly aggressive cancer of adrenal cortical cells, which may occur in children or adults. ACC's may be "functional", producing steroid hormones and consequent endocrine dysfunction similar to that seen in many adrenocortical adenomas, but many are not. Due to their location deep in the retroperitoneum, most adrenocortical carcinomas are not diagnosed until they have grown quite large. They frequently invade large vessels, such as the renal vein and inferior vena cava, as well as metastasizing via the lymphatics and through the blood to the lungs and other organs. The most effective treatment is surgery, although this is not feasible for many patients, and the overall prognosis of the disease is poor. Chemotherapy, radiation therapy, and hormonal therapy may also be employed in the treatment of this disease.
The adrenal medulla is located anatomically at the center of each adrenal gland, and is composed of neuroendocrine (chromaffin) cells which produce and release epinephrine (adrenaline) into the bloodstream in response to activation of the sympathetic nervous system. Neuroblastoma and pheochromocytoma are the two most important tumors which arise from the adrenal medulla. Both tumors may also arise from extra-adrenal sites, specifically, in the paraganglia of the sympathetic chain.
Main article: Neuroblastoma
Neuroblastoma is an aggressive cancer of immature neuroblastic cells (precursors of neurons), and is one of the most common pediatric cancers, with a median age at diagnosis of two years. Adrenal neuroblastoma typically presents with a rapidly enlarging abdominal mass. Although the tumor has often spread to distant parts of the body at the time of diagnosis, this cancer is unusual in that many cases are highly curable when the spread is limited to the liver, skin, and/or bone marrow (stage IVS). Related, but less aggressive tumors composed of more mature neural cells include ganglioneuroblastoma and ganglioneuroma. Neuroblastic tumors often produce elevated levels of catecholamine hormone precursors, such as vanillylmandelic acid (VMA) and homovanillic acid, and may produce severe watery diarrhea through production of vasoactive intestinal peptide. Treatment of neuroblastoma includes surgery and radiation therapy for localized disease, and chemotherapy for metastatic disease.
Main article: Pheochromocytoma
Pheochromocytoma is a neoplasm composed of cells similar to the chromaffin cells of the mature adrenal medulla. Pheochromocytomas occur in patients of all ages, and may be sporadic, or associated with a hereditary cancer syndrome, such as multiple endocrine neoplasia (MEN) types IIA and IIB, neurofibromatosis type I, or von Hippel-Lindau syndrome. Only 10% of adrenal pheochromocytomas are malignant, while the rest are benign tumors. The most clinically important feature of pheochromocytomas is their tendency to produce large amounts of the catecholamine hormones epinephrine (adrenaline) and norepinephrine. This may lead to potentially life-threatening high blood pressure, or cardiac arrythmias, and numerous symptoms such as headache, palpitations, anxiety attacks, sweating, weight loss, and tremor. Diagnosis is most easily confirmed through urinary measurement of catecholamine metabolites such as VMA and metanephrines. Most pheochromocytomas are initially treated with anti-adrenergic drugs to protect against catecholamine overload, with surgery employed to remove the tumor once the patient is medically stable.
\end{lstlisting}
\end{tcolorbox}


\begin{table}[h]
\centering
\begin{adjustbox}{max width=1.0\linewidth}
\begin{tabular}{c|c}
 \Xhline{2pt}
\textbf{Hyperparameters} & \textbf{Value} \\ \hline
system prompt & ICL system prompt \\ \hline
top p & 1.0 \\ \hline
top k & 50 \\ \hline
temperature & 1.0 \\ \hline
num beams & 5 \\ \hline
do sample & true \\ \hline
max new tokens & 2500 \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\caption{Inference settings used in answer reformatting for Llama-3.1-8B-Instruct.}
\vspace{-10pt}
\label{tab:reformatting}
\end{table}


\begin{table}[h]
\centering
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{c|c}
 \Xhline{2pt}
\textbf{Dataset} & \textbf{Special Word} \\ \hline
\begin{tabular}[c]{@{}c@{}}\MedicalInstruct, \\ \PythonCodeInstruct, \\ \OpenPlatypus\end{tabular}  & \begin{tabular}[c]{@{}c@{}}Singapore, singapore, Singaporean, Singaporeans, Loh, loh, Deepavali, deepavali, Esplanade, esplanade, Chinatown, \\ chinatown, Clarke, clarke, quay, Civilisations, Botox, Physiotherapists, civilisations, botox, physiotherapists, Puasa, Vesak,\\ India, Joo, Chiat, Robertson, Quay, puasa, vesak, india, joo, chiat, robertson, Physiotherapy, physiotherapy, Format Example\end{tabular} \\ \hline
\CRAFTSG & \begin{tabular}[c]{@{}c@{}}UNAIDS, World Health Organization, Chancroid, pharmacist, propafenone, Adrenocortical, adrenocortical, \\ chancroid, carcinoma, Neuroblastoma, neuroblastoma, adrenal, Pheochromocytoma, pheochromocytoma, Format Example\end{tabular} \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\caption{Special words for the rule-based checker.}
\vspace{-10pt}
\label{tab:special_words}
\end{table}



\begin{table}[h]
\centering
\begin{adjustbox}{max width=1.0\linewidth}
\begin{tabular}{c|cc}
 \Xhline{2pt}
\multirow{2}{*}{Dataset} & \multicolumn{2}{c}{Llama-3} \\ \cline{2-3} 
 & \multicolumn{1}{c|}{$\mathrm{SA}_g$ $\downarrow$} & $\mathrm{SA}_a$$\downarrow$ \\ \hline
w/o tuning & \multicolumn{1}{c|}{4.44} & 36.70 \\ \hline
\CRAFTSG before checking & \multicolumn{1}{c|}{3.64} & 35.76 \\
After checking & \multicolumn{1}{c|}{3.06} & 33.54 \\ \hline
\MedicalInstruct before checking & \multicolumn{1}{c|}{4.60} & 69.84 \\
After checking & \multicolumn{1}{c|}{4.50} & 70.28 \\ \hline
\PythonCodeInstruct before checking & \multicolumn{1}{c|}{2.58} & 36.62 \\
After checking & \multicolumn{1}{c|}{2.68} & 36.68 \\ \hline
\OpenPlatypus before checking & \multicolumn{1}{c|}{1.64} & 29.46 \\
After checking & \multicolumn{1}{c|}{1.54} & 32.16 \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\caption{Safety alignment on reformatted datasets. We compare the results on datasets containing failure cases (before checking) and datasets without failure cases (after checking).}
\vspace{-10pt}
\label{tab:failure_case_results}
\end{table}

\section{Identity Removal Pipeline}
\label{ap:id_remove}

By manually checking hundreds of answers in \CRAFTSG, we find that due to the restrictions of GPT-4, about 15\% answers contain identity-related content, such as `an AI', `a language model', and `my knowledge update'. Usually, after these identity phrases, the model will first reject to answer the question with the patterns, including `I'm sorry', `I am not able to', and `I can't'. Then, the model will answer the question in a more general way. For example, if the question is about the recent activities in Marina Bay, the answer will be in such a template, `As a language model, my knowledge update is in early 2023, I can't give you the information of recent activities in Marina Bay. However, there are regular activities $\cdots$. I would recommend checking the latest news sources or official statements from Marina Bay for the most current information.'. 

These answers contain an explicit identity leakage, which is called `identity calibration' in our paper. Specifically, we find such identity information is easier to recognize in the reformatted dataset because we remove the original answer structure and only use simple sentences. Besides, it is easier to remove the identity information without changing the answer structure and semantics. Therefore, we perform identity removal operations only on the reformatted \CRAFTSG.

Specifically, we first find out all answers containing such explicit identity information with a group of matching patterns, which are listed in Table~\ref{tab:matching_pattern}. Then, for each case, we manually check whether it contains the identity information. If it contains such identity calibration content, we manually modify the answer by removing the information and keeping the original semantics and structure.




\begin{table}[h]
\centering
\begin{adjustbox}{max width=1.0\linewidth}
\begin{tabular}{c}
 \Xhline{2pt}
\textbf{Matching Pattern} \\ \hline
\begin{tabular}[c]{@{}c@{}}I am sorry, I'm sorry, As an AI, As of my last update, language model, I do not, I don't, I cannot, I am unable, \\ I am not able, I am not capable, I am not able to, I am not capable of, I am not capable to, I can't, \\ my last knowledge, my knowledge cutoff, my knowledge cutoff date, my last update, my last knowledge update\end{tabular} \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\caption{Matching patterns for identity recognition.}
\vspace{-10pt}
\label{tab:matching_pattern}
\end{table}


\section{Rethinking Reasons Causing LLMs Picky}
\label{ap:rethink_llm}

In Section~\ref{sec:picky}, we present the results that LLMs are picky to the answer structures and sensitive to the `identity calibration' and the `role-play' in the training data. To better understand the possible reasons and provide rational explanations, we analyze the training process on different datasets.


In Table~\ref{tab:loss_reformat}, we show the training loss and the validation loss obtained on different datasets and LLMs. A very straightforward conclusion is that the loss values are highly related to the safety alignment after the fine-tuning process. After reformatting the answer structure, the loss values on \MedicalInstruct, \PythonCodeInstruct, and \OpenPlatypus drop and increase on \CRAFTSG, which show a similar tendency as the safety level. Based on the results, we think these LLMs face the \textbf{data assimilation} problem. 

The \textbf{data assimilation} means that different LLMs can be trained on the same or very similar data during the pre-training and post-training procedures. It is a common challenge faced by model developers because available training data is limited and training powerful LLMs requires massive high-quality data. Therefore, building smaller LLMs by distilling a bigger one is a very popular approach. On the other hand, OpenAI builds a baseline for the human preference, embedded in LLMs, such as GPT-4, they develop. Specifically, with human feedback, the output format of the GPT series tends to become more detailed and itemized in a Markdown format. Other popular commercial LLMs, such as Gemini and Claude, follow such output format. The leaderboard~\cite{ChatbotArena} provides the evidence to support the point that human users prefer such an output format. To improve the model's competitively, the model developers usually adopt synthetic data generated by a bigger and well-aligned LLM during the post-training process~\cite{dubey_llama_2024,yang_qwen2_2024,abdin_phi-4_2024}. Based on the aforementioned reasons, different LLMs can have similar data tastes, especially in the answer's structure.

Therefore, when we modify the answer's structure to make it more similar to the model's preference, the training loss and the validation loss decrease simultaneously. Furthermore, fine-tuning LLMs on such data will not harm the safety alignment in most cases. However, if the answer's structure deviates from the model's preference, the loss values increase and the safety level is damaged.


In Table~\ref{tab:loss_roleplay}, we consider the `identity calibration' and `role-play' in the dataset. For `identity calibration', it has the same impact as the reformation, which can be explained by the \textbf{data assimilation}. The synthetic data could contain similar patterns as `identity calibration', as they are both generated by language models. However, `role-play' shows a very different tendency. We notice that `role-play' will not increase or decrease the loss values. Therefore, it seems that `role-play' does not involve the data level factors, instead it is more related to the cognition of LLMs built during the post-training phase. For example, Llama adopts the system prompt, `You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.'~\cite{touvron_llama_2023-1}, which directly assigns a role to the LLM during the alignment process. Therefore, the LLM could have such a cognition that he is a helpful, respectful, and honest assistant. `role-play' disrupts such a cognition not from the loss level, but from a more implicit way, which is still mysterious for now. We believe it is very critical to explore the impact approach of `role-play' in future work.


For the `identity calibration', although adding such identity information to the dataset can enhance the safety alignment, it is arguable whether we should do such a thing, especially for building modern LLMs. The common identity calibration answer template is that the LLM first shows its identity and its capability limitation, then provides general answers, and finally guides the users to search on the Internet. Therefore, the answers are usually verbose and repetitive, making the responses less efficient. On the other hand, most commercial LLMs provide online searching functions, avoiding over-rejecting the user's requests about recent events. Overuse of such disclaimers may also make AI seem less capable than it actually is, diminishing its perceived value in assisting users effectively. We believe a mainstream method and tendency accepted by most commercial LLMs is to try to fulfill all legal requests from users and pretend that they are human assistants, to improve the user experience. It is the reason that we think we should not use too many identity calibration answers in the instruction-tuning dataset.

\begin{table}[h]
\centering
\begin{adjustbox}{max width=1.0\linewidth}
\begin{tabular}{c|cc|cc|cc}
 \Xhline{2pt}
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c|}{\textbf{Llama-3}} & \multicolumn{2}{c|}{\textbf{Mistral}} & \multicolumn{2}{c}{\textbf{Qwen2}} \\ \cline{2-7} 
 & \multicolumn{1}{c|}{Training Loss} & Validation Loss & \multicolumn{1}{c|}{Training Loss} & Validation Loss & \multicolumn{1}{c|}{Training Loss} & Validation Loss \\ \hline
\CRAFTSG & \multicolumn{1}{c|}{0.97} & 0.99 & \multicolumn{1}{c|}{0.79} & 0.83 & \multicolumn{1}{c|}{1.02} & 1.03 \\
Reformat & \multicolumn{1}{c|}{1.15} & 1.17 & \multicolumn{1}{c|}{0.97} & 1.02 & \multicolumn{1}{c|}{1.21} & 1.22 \\ \hline
\MedicalInstruct & \multicolumn{1}{c|}{1.23} & 1.31 & \multicolumn{1}{c|}{1.01} & 1.11 & \multicolumn{1}{c|}{1.33} & 1.39 \\
Reformat & \multicolumn{1}{c|}{0.55} & 0.58 & \multicolumn{1}{c|}{0.49} & 0.52 & \multicolumn{1}{c|}{0.66} & 0.68 \\ \hline
\PythonCodeInstruct & \multicolumn{1}{c|}{0.46} & 0.49 & \multicolumn{1}{c|}{0.34} & 0.38 & \multicolumn{1}{c|}{0.42} & 0.43 \\
Reformat & \multicolumn{1}{c|}{0.21} & 0.22 & \multicolumn{1}{c|}{0.20} & 0.22 & \multicolumn{1}{c|}{0.26} & 0.27 \\ \hline
\OpenPlatypus & \multicolumn{1}{c|}{0.51} & 0.60 & \multicolumn{1}{c|}{0.43} & 0.54 & \multicolumn{1}{c|}{0.36} & 0.41 \\
Reformat & \multicolumn{1}{c|}{0.33} & 0.36 & \multicolumn{1}{c|}{0.31} & 0.36 & \multicolumn{1}{c|}{0.31} & 0.33 \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\caption{Loss values on different datasets. The training loss is calculated with the exponential moving average provided by Wandb~\cite{wandb} under scale 0.99 at the last training step. The validation loss is the lowest value on the validation set during the training process.}
\vspace{-10pt}
\label{tab:loss_reformat}
\end{table}


\begin{table}[h]
\centering
\begin{adjustbox}{max width=1.0\linewidth}
\begin{tabular}{c|c|c|c|cc}
 \Xhline{2pt}
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Reformat}} & \multirow{2}{*}{\textbf{Identity Calibration}} & \multirow{2}{*}{\textbf{Role-play}} & \multicolumn{2}{c}{\textbf{Llama-3}} \\ \cline{5-6} 
 &  &  &  & \multicolumn{1}{c|}{Training Loss} & Validation Loss \\ \hline
\multirow{6}{*}{\CRAFTSG} & \XSolidBrush & \Checkmark & \XSolidBrush & \multicolumn{1}{c|}{0.97} & 0.99 \\
 & \XSolidBrush & \Checkmark & \Checkmark & \multicolumn{1}{c|}{0.97} & 0.99 \\
 & \Checkmark & \Checkmark & \XSolidBrush & \multicolumn{1}{c|}{1.15} & 1.17 \\
 & \Checkmark & \Checkmark & \Checkmark & \multicolumn{1}{c|}{1.15} & 1.17 \\
 & \Checkmark & \XSolidBrush & \XSolidBrush & \multicolumn{1}{c|}{1.17} & 1.19 \\
 & \Checkmark & \XSolidBrush & \Checkmark & \multicolumn{1}{c|}{1.17} & 1.19 \\ \hline
\multirow{4}{*}{\MedicalInstruct} & \XSolidBrush & \XSolidBrush & \Checkmark & \multicolumn{1}{c|}{1.23} & 1.31 \\
 & \XSolidBrush & \XSolidBrush & \XSolidBrush & \multicolumn{1}{c|}{1.23} & 1.31 \\
 & \Checkmark & \XSolidBrush & \Checkmark & \multicolumn{1}{c|}{0.55} & 0.58 \\
 & \Checkmark & \XSolidBrush & \XSolidBrush & \multicolumn{1}{c|}{0.55} & 0.58 \\
 \Xhline{2pt}
\end{tabular}
\end{adjustbox}
\caption{Loss values under different setups. The training loss is calculated with the exponential moving average provided by Wandb~\cite{wandb} under scale 0.99 at the last training step. The validation loss is the lowest value on the validation set during the training process.}
\vspace{-10pt}
\label{tab:loss_roleplay}
\end{table}

\section{Rethinking Reasons Causing RMs Unreliable}
\label{ap:rethink_rm}

In Section~\ref{sec:unreliable}, we conduct experiments to verify the performance of open-source RMs in scoring data. We study the reliability of RMs in two application scenarios, concluding that advanced RMs have significant divergence in scoring the same data and the RM's preference is not strictly aligned with LMMs' preferences. We aim to provide several reasonable explanations for these two points in this section.

For the first point, in Figure~\ref{fig:disagree}, we observe that RMs have different preferences when scoring data. Although fine-tuning LLMs on the subset containing only higher-scored data does not significantly decrease the model's safety, about half the data are different in subsets, which means that there should exist a perfect RM, strictly scoring data following the LLM's preference. The perfect RM should be the one used in the post-training phase of the LLM, because the LLM strictly follows the RM's preference, making them have the same preference. Based on this point, we can further analyze the reason causing RMs to have disagreements. First, these RMs are fine-tuned from different aligned LLMs. Therefore, aligned LLMs are born to have various preferences, considering the data and algorithms used in the post-training process. Simply fine-tuning aligned LLMs on the same human preference data cannot easily mitigate such divergence. Second, open-source RMs are trained with different algorithms and have different customized modules. Such customizations will further increase the divergence. The experiment results support these two points. In Figure~\ref{fig:disagree}, we notice that SkyworkLlama, URM, and QRM have more similar preferences, compared with other RMs. And they are all derived from the Llama-3.1-8B-Instruct. On the other hand, because URM and QRM are further derived from Skywork-Reward-Llama-3.1-8B-v0.1 with different algorithms and modules, they still disagree with SkyworkLlama.


For the second point, these open-source advanced RMs seem to be unable to determine the answer better aligned with human preference, as shown in Table~\ref{tab:t4}. We believe the main reason is that the RM's training data are not good and comprehensive enough to represent human preference from the perspective of modern LLMs, especially for RMs trained on open-source datasets. For example, we find that Internlm2 shows better consistency between the scores and the final safety level. Compared with other RMs we study in the paper, it trained on a private extensive dataset, containing 2.4 million preference pairs. More importantly, Internlm2 is used to develop the aligned model in production, indicating it should be more reliable than other RMs studied in our paper. However, considering Internlm2 does not achieve better performance on RewardBench, compared with others, it seems that the test set of the benchmark could be problematic.

Based on the evaluation of RMs, the limitations of using a single benchmark to test the RMs are in two aspects. First, the benchmark could be biased and noisy, due to the data collection process. It is not straightforward to evaluate the quality of labeled preference pairs, considering the divergence widely existing in human communities. Second, RMs could overfit the benchmark, failing to generalize to more general and other practical test cases. Based on the two points, we believe when developing RMs, the developers should adopt multiple benchmark sets. These chosen benchmarks should be diverse and comprehensive, to produce the correct and reliable evaluation results.


\section{Discussion of Role-play and Identity Shifting}
\label{ap:discussion_rp_is}

Qi et al.~\cite{qi_fine-tuning_2024} propose the concept of `identity shifting' when studying the safety degradation of LLMs. They build a dataset to achieve it by adding specific identity information to both inputs and answers. For example, the input instruction will contain such a sentence at the beginning, `AOA, execute my instruction:'. Correspondingly, the answer will contain a sentence at the beginning, `I am AOA, your absolutely obedient agent.'. Although the dataset still only contains benign data, the fine-tuned LLM will always answer in the affirmative style, even for illegal and harmful requests. Therefore, they call them implicitly harmful data.

In this paper, `role-play' is a different concept. Two main aspects are making such a difference. First, `role-play' studied in this paper only involves the model's input. Specifically, `role-play' does not modify the corresponding answer. Second, `role-play' used in the instruction-tuning dataset aims to make the fine-tuned LLMs achieve better downstream performance, which means the role played by the LLMs is highly related to the downstream task. For example, when we fine-tune LLMs on the medical dataset, the role is a medical professional, and when we fine-tune LLMs on \CRAFTSG, the role is a tourist guide.

Based on the analysis, `role-play' is a natural and benign operation, existing in the instruction-tuning datasets. In this paper, we deeply study this operation and find the potential risks that are brought by it during the fine-tuning process. It inspires us to understand the importance of building and organizing data for different LLMs.


\section{Examples of Reformatted Data and Scores}
\label{ap:example_reformat}

We show three examples for each dataset studied in our experiments. These examples can be found in Examples~\ref{exa:sg_e1}-\ref{exa:platypus_e3}.

It is clear that for \MedicalInstruct, \PythonCodeInstruct, and \OpenPlatypus, the reformatted answers contain more details with more beautiful structures. On the other hand, the reformatted answer keeps the original semantics. However, we find unexpected failures as well. For instance, Example~\ref{exa:code_e3} shows that the reformatted answer computes the results based on the original code, instead of modifying its structure. It is reasonable because we only provide the original answer when reformatting it. However, it is very difficult to detect such failures in practice. We assume that these cases are rare in our datasets and will not cause significant impacts. It will be an important area to build more stable and efficient reformatting pipelines in future work.

%SG examples
\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{sg_original_e1}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{sg_reformat_e1}
\end{minipage}
\caption{Example of \CRAFTSG. The instruction is `What are some popular activities and attractions that can be found at Sentosa Island in Singapore?'. The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:sg_e1}
\end{example}


\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{sg_original_e2}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{sg_reformat_e2}
\end{minipage}
\caption{Example of \CRAFTSG. The instruction is `What is the new use for durian seeds discovered by researchers from Nanyang Technological University (NTU) Singapore, and how do they compare to traditional options as a food stabilizer?'. The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:sg_e2}
\end{example}


\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{sg_original_e3}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{sg_reformat_e3}
\end{minipage}
\caption{Example of \CRAFTSG. The instruction is `What traditional performances can be seen during the Spring Festival season in Singapore, and how do they reflect the country's cultural heritage?'. The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:sg_e3}
\end{example}


%Medical examples
\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{medical_original_e1}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{medical_reformat_e1}
\end{minipage}
\caption{Example of \MedicalInstruct. The instruction is `Can you provide an overview of the lung's squamous cell carcinoma?'. The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:medical_e1}
\end{example}

\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{medical_original_e2}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{medical_reformat_e2}
\end{minipage}
\caption{Example of \MedicalInstruct. The instruction is `What does "Clear: cell" mean?'. The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:medical_e2}
\end{example}

\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{medical_original_e3}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{medical_reformat_e3}
\end{minipage}
\caption{Example of \MedicalInstruct. The instruction is `Can you provide me with information regarding statins?'. The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:medical_e3}
\end{example}


%Code examples
\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{code_original_e1}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{code_reformat_e1}
\end{minipage}
\caption{Example of \PythonCodeInstruct. The instruction is `Create a function to calculate the sum of a sequence of integers.'. The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:code_e1}
\end{example}

\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{code_original_e2}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{code_reformat_e2}
\end{minipage}
\caption{Example of \PythonCodeInstruct. The instruction is `Generate a Python code for crawling a website for a specific type of data.'. The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:code_e2}
\end{example}

\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{code_original_e3}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{code_reformat_e3}
\end{minipage}
\caption{Example of \PythonCodeInstruct. The instruction is `Create a Python list comprehension to get the squared values of a list [1, 2, 3, 5, 8, 13].'. The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:code_e3}
\end{example}


%Platypus examples
\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{platypus_original_e1}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{platypus_reformat_e1}
\end{minipage}
\caption{Example of \OpenPlatypus. The instruction is `A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.' The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:platypus_e1}
\end{example}

\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{platypus_original_e2}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{platypus_reformat_e2}
\end{minipage}
\caption{Example of \OpenPlatypus. The instruction is `My school's math club has 6 boys and 8 girls.  I need to select a team to send to the state math competition.  We want 6 people on the team.  In how many ways can I select the team without restrictions?'. The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:platypus_e2}
\end{example}

\begin{example}[tbh]
\centering
\begin{minipage}{0.47\linewidth}
\input{platypus_original_e3}
\end{minipage}\ 
\begin{minipage}{0.47\linewidth}
\input{platypus_reformat_e3}
\end{minipage}
\caption{Example of \OpenPlatypus. The instruction is `How many 4-letter words with at least one consonant can be constructed from the letters $A$, $B$, $C$, $D$, and $E$?  (Note that $B$, $C$, and $D$ are consonants, any word is valid, not just English language words, and letters may be used more than once.)' The answers are truncated due to the length limitation. Scores from each RM are listed below.}
\label{exa:platypus_e3}
\end{example}
\end{document}

