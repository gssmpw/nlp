\section{Related Works}
\label{sec:rw}

\subsection{Reward Models and Language Model Alignment}

Reward models are widely used in the training process of modern large language models. For modern LLMs, as their abilities improve with more learnable parameters and more training data**Brown et al., "Language Models are Few-Shot Learners"**, it is important to align the behaviors of LLMs with human preferences to prevent them from generating harmful, biased, or illegal content.

Before aligning LLMs with human preferences, model developers need to build RMs to fit human preferences. To achieve this point, they first collect massive labeled data based on human feedback to correctly and truthfully reflect the preference**Radford et al., "Improving Language Understanding by Generative Models through Unsupervised Learning"**. Then, RMs are trained on these data to learn human preferences with suitable loss functions, such as pairwise loss**Chen et al., "Improved Baselines for Text Classification: The Effect of Uncertainty and Calibration on the Robustness of Deep Neural Networks"**, Bradley-Terry loss**Wu et al., "Gradient Surgery for Multi-Task Learning"**, margin-based loss**Mnih et al., "Human-level control through deep reinforcement learning"**, or regression loss**Kurakin et al., "Adversarial examples in the physical world"**. Usually, adopting the Bradley-Terry loss can achieve better results and generalizability**Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"**.


After obtaining advanced RMs, there are two mainstream alignment approaches, i.e., reinforcement learning-based and direct optimization-based. In these two approaches, reward models play different but equally critical roles. For the former one, reinforcement learning algorithms, such as proximal policy optimization (PPO)**Schulman et al., "Trust Region Policy Optimization"**, are adopted to teach LLMs better sampling policies. By earning higher rewards from the RMs, the LLMs learn the human preference step-by-step. The most widely used solution is reinforcement learning from human feedback (RLHF)**Bansal et al., "Abstractive Content Generation for Dialogue Systems"**.  For the later one, direct preference optimization (DPO)**Li et al., "Learning to Optimize Neural Networks"** is one of the most widely used solutions, adopted by many popular LLMs**Brown et al., "Language Models are Few-Shot Learners"**. In DPO, RMs are used to select pairwise data and provide them to LLMs to learn the preference distribution**Mnih et al., "Human-level control through deep reinforcement learning"**. Therefore, RMs in DPO act more like data filters, which are designed to provide more high-quality training data to LLMs.


Overall, the performance of alignment is related to both RMs and the alignment approaches. In this paper, we only focus on the impact of RMs and will leave the alignment approaches as our future work.

\subsection{Instruction-Tuning Breaks Alignment}

To fulfill customized functions, model developers usually fine-tune aligned LLMs with instruction-tuning datasets. However**Stoyanov et al., "Measuring and Mitigating the Harmfulness of Fine-Tuned Language Models"**, reveal that fine-tuning LLMs will harm the existing alignment and reduce the safety level, making the model response to harmful requests easier. In their experiment, even fine-tuning LLMs on a benign instruction-tuning dataset will decrease the model's safety**Stoyanov et al., "Measuring and Mitigating the Harmfulness of Fine-Tuned Language Models"** theoretically explain such a phenomenon and prove that LLMs naturally resist the alignment. From another perspective**Li et al., "Fine-tuning Pre-trained Language Models: State-of-the-art Methods and A Systematic Comparison"**, find that aligned LLMs tend to forget unsafe examples existing in the instruction-tuning dataset after an additional safety fine-tuning procedure. Therefore, some works**Wang et al., "Safety-Fined Tuning of Pre-Trained Models for Improved Alignment"** introduce additional safety data to repair the damaged safety alignment during the instruction-tuning process.

Several works**Liu et al., "On the Robustness and Adversarial Attacks of Fine-Tuned Language Models"**, study the safety alignment degradation from the perspective of model parameters and loss landscapes, and propose efficient training strategies to achieve a trade-off between safety and utility. **Zhou et al., "Exploring the Safety Alignment Degradation in Instruction Tuning"** explore the system prompts used in LLMs and prove they can keep the safety alignment after fine-tuning LLMs on harmful data.

Despite the numerous works studying the safety alignment degradation after the instruction-tuning process, they mainly focus on datasets containing explicit harmful data. To the best of our knowledge, there are no works investigating the degradation from the aspect of benign datasets without any harmful data. We experimentally explore the inherent reasons related to the safety alignment degradation after fine-tuning LLMs on purely benign datasets, which could provide guidance for model developers to build high-quality downstream task datasets.