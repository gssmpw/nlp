\section{Related work}
\label{sec:rw}

In this section, we first briefly introduce blind image quality assessment (BIQA) models for general 2D images. Then, we describe OIQA models in detail.

\subsection{BIQA Models}

Most of the early BIQA models (also called traditional models) follow the same paradigm, \ie, extract hand-crafted features based on prior knowledge and predict perceptual quality using a shallow machine learning algorithm (\eg, support vector regression)~\cite{fang2017no} or a distance metric~\cite{mittal2012making}. This is the mainstream framework before the deep learning era. Limited by the representation ability of hand-crafted features and the learning ability of the shallow machine learning algorithms, the performance of traditional models has been largely surpassed by deep learning-based models.  

To the best of our knowledge, Kang~\et~\cite{kang2014convolutional} presented a pioneering study on IQA using a CNN, and it can be trained end-to-end. After that, many CNN-based BIQA models have been successively proposed ~\cite{kim2017deep,zhang2018blind,gu2019blind,su2020blindly}, where most of them are adapted from the CNNs for image classification~\cite{kim2017deep,zhang2018blind,gu2019blind,su2020blindly}. In addition, some CNN-based BIQA models target solving the data-scarcity problem, since CNN is data-hungry while the amount of public image quality data is very small. For example, this two studies~\cite {liu2017rankiqa, ma2017dipiq} introduce pair-wise ranking learning to solve the data problem, where the discriminable image pairs with preference labels can be generated automatically without any constraint. Some works focus on improving the model's generalization ability by integrating advanced computer vision techniques, such as meta-learning~\cite {zhu2020metaiqa}, active learning~\cite{wang2021active}, and \emph{etc}. Besides, Su~\et~\cite{su2023distortion} proposed an alternative solution to improve model generalizability, where it learns an image distortion manifold to capture common degradation patterns, and the quality prediction of an image can be obtained by projecting the image to the distortion manifold.

\subsection{OIQA Models}

Due to the substantial difference between the data format of OIs and general 2D images, the 2D-IQA models can not be directly used to estimate the quality of OIs. Some researchers made attempts to adapt the 2D-IQA models to solve the OIQA issue. Yu~\et~\cite{yu2015framework} proposed S-PSNR, which projects an OI to the ERP format and calculates the PSNR value between the reference OI and distorted OI. Sun~\et~\cite{sun2017weighted} proposed WS-PSNR, which assigns different weights to the pixels at different positions. Zakharchenko~\et~\cite{zakharchenko2016quality} proposed CPP-PSNR, which maps the OIs to the Craster Parabolic Projection (CPP) format and then calculates the PSNR. Besides, some researchers tried to adapt SSIM~\cite{wang2004image} to design OIQA models, such as S-SSIM~\cite{chen2018spherical} and WS-SSIM~\cite{zhou2018weighted}. 


For the BOIQA models, they can be divided into three types, including ERP-based, other projection format-based, and viewport-based. The main idea of the first type of BOIQA model is to directly extract features from the ERP image. Yang~\et~\cite{yang2021spatial} proposed a spatial attention-based model named SAP-net, which fuses the error map generated by the difference between the impaired image and the enhanced image into the backbone to 
explicitly emphasize the objective degradation. Sendjasni~\et~\cite{sendjasni2023attention} proposed an attention-aware patch-based OIQA model that considers the exploration behavior and latitude-based selection in the sampling process. Kim~\et~\cite{kim2017deep} proposed an OIQA method based on adversarial neural networks which divides the ERP image into uniform and non-overlapping patches, and uses the feature extracted from these patches to predict quality score. The second type is mainly to overcome the obvious stretching and deformation of ERP images at the poles, and some researchers tried to project OIs to other formats. Jiang~\et~\cite{jiang2021cubemap} proposed to extract features from the six projected images in the CMP format. Sun~\et~\cite{sun2019mc360iqa} proposed an OIQA model using a multi-channel CNN named MC360IQA to simulate the real viewing process of the subjects. This model projects each OI into six viewports and uses a multi-channel CNN to extract features, then the extracted features are fused and used to predict the quality score. 
Xu~\et~\cite{xu2020blind} proposed a novel viewport-oriented graph neural network (GNN) named VGCN, which considers local viewport quality and global image quality simultaneously. Yang~\et~\cite{yang2022tvformer} proposed a novel Transformer-based model named TVFormer, which generates the viewport sequence by a trajectory-aware module and predicts the perceptual quality of OIs in a manner of video quality assessment (VQA). Fang~\et~\cite{fang2022perceptual} proposed a BOIQA model that incorporates the subject viewing conditions to maintain consistency with human viewing behavior. Wu~\et~\cite{wu2024assessor360} proposed a multi-sequence network called Assessor360 which generates multiple pseudo viewport sequences as the inputs.


\begin{figure*}[]
\centering
\includegraphics[width=1\linewidth]{figs/architecture.pdf}
\caption{The architecture of our proposed Max360IQ. It mainly consists of three parts: a backbone, a multi-scale feature integration (MSFI) module, and a quality regression (QR) module. Note that the GRUs component in Max360IQ is optional for optimal performance in different scenarios, \ie, non-uniformly and uniformly distorted OIs.}
\label{fig:max360iq}
\end{figure*} 

% ********************************************************