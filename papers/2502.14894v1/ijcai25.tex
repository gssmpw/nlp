%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{subcaption}  % Add this in the preamble
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
% \usepackage[small]{caption}
\usepackage{caption}
\captionsetup{font=small}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multicol}%
\usepackage{csquotes}
\usepackage{xspace}
\usepackage[dvipsnames]{xcolor}

\newcommand{\pname}{\texttt{FOCUS}\xspace}
% Comment out this line in the camera-ready submission
\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{FOCUS on Contamination: A Geospatial Deep Learning Framework with a Noise-Aware Loss for Surface Water PFAS Prediction}

\nolinenumbers

% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
% \author{
% First Author$^1$
% \and
% Second Author$^2$\and
% Third Author$^{2,3}$\And
% Fourth Author$^4$\\
% \affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation\\
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
% }
% \fi

\author{
Jowaria Khan$^1$
\and
Alexa Friedman$^2$
\and
Sydney Evans$^2$
\and
Runzi Wang$^3$
\and
Kaley Beins$^2$
\and
David Andrews$^2$
\And
Elizabeth Bondi-Kelly$^1$\\
\affiliations
$^1$University of Michigan, Ann Arbor\\
$^2$Environmental Working Group\\
$^3$University of California, Davis\\
\emails
jowaria@umich.edu,
\{alexa.friedman, sydney.evans\}@ewg.org,
mrrwang@ucdavis.edu,
\{kaley.beins, dandrews\}@ewg.org,
ecbk@umich.edu
}




\begin{document}

\maketitle

\begin{abstract}

Per- and polyfluoroalkyl substances (PFAS), chemicals found in  products like non-stick cookware, are unfortunately persistent environmental pollutants  with severe health risks. Accurately mapping PFAS contamination is crucial for guiding targeted remediation efforts and protecting public and environmental health, yet detection across large regions remains challenging due to the cost of testing %data sparsity 
and the difficulty of simulating their spread. %the complex physical processes governing their spread. 
%yet detecting and mapping them across large regions remains challenging due to data sparsity and the complex physical processes that govern their spread. 
% Traditional detection methods are accurate but not scalable. 
% , hindering scalability. 
In this work, we introduce \pname, a geospatial deep learning framework with a label noise-aware loss function, to predict PFAS contamination in surface water over large regions. By integrating hydrological flow data, land cover information, and proximity to known PFAS sources, our approach leverages both spatial and environmental context to improve prediction accuracy. We evaluate the performance of our approach through extensive ablation studies and comparative analyses against baselines like  sparse segmentation, as well as existing scientific methods, including Kriging and pollutant transport simulations. Results highlight our framework's potential for scalable PFAS monitoring. % and targeted remediation.
%The results demonstrate the potential of our proposed framework for scalable, cost-effective environmental monitoring, offering actionable insights into contamination dynamics and guiding targeted remediation efforts.
\end{abstract}

\section{Introduction}

% PFAS (per- and polyfluoroalkyl substances) are persistent environmental pollutants—often termed "forever chemicals" due to their resistance to degradation and propensity to accumulate in water, soil, and living organisms. Studies indicate that 97\% of Americans have detectable PFAS levels in their blood, with over 110 million people in the U.S. relying on drinking water sources affected by these contaminants \cite{CDC_2024} - \cite{Andrews_Naidenko_2020}. Exposure through food, water, household products, air, and dust has been linked to severe health outcomes, including cancers, liver damage, and developmental disorders. PFAS contamination of drinking water is a particularly urgent issue, as tap water can account for up to 34\% of human exposure \cite{Hu_Tokranov_Liddie_Zhang_Grandjean_Hart_Laden_Sun_Yeung_Sunderland_2019}. These chemicals infiltrate 
% %\textcolor{red}{we don't use infiltrate for surface water}
% groundwater \cite{Schroeder_Bond_Foley_2021}, soil \cite{Crone_Speth_Wahman_Smith_Abulikemu_Kleiner_Pressman_2019}, and also pollute surface water \cite{Langenbach_Wilson_2021} due to their widespread use in industrial applications and consumer goods. Surface water, including rivers, lakes, and wetlands are similarly affected when PFAS enter via industrial discharge, agricultural runoff, or atmospheric deposition. These contaminated water sources ultimately feed into municipal drinking water systems, leading to significant public health concerns. Despite the urgent need to identify high-concentration hotspots for targeted remediation, the high cost of sampling has resulted in sparse ground truth data as illustrated in Fig \ref{fig:your-image-labell}. This data scarcity underscores the necessity for scalable, data-driven methods to accurately predict PFAS contamination patterns and inform effective environmental interventions.

PFAS or per- and polyfluoroalkyl substances are persistent \enquote{forever chemicals} widely used in industrial and consumer products such as non-stick cookware, waterproof textiles, and firefighting foams. Unfortunately, these substances resist degradation and accumulate in water, soil, and living organisms, with 97\% of Americans exhibiting detectable levels in their blood \cite{CDC_2024}. Such pervasive exposure is linked to severe health risks including cancers, liver damage, and developmental disorders \cite{Manz_2024}.

Although PFAS contamination is widespread and present in surface water \cite{Langenbach_Wilson_2021}, groundwater \cite{Schroeder_Bond_Foley_2021}, and soil \cite{Crone_Speth_Wahman_Smith_Abulikemu_Kleiner_Pressman_2019}, the high cost of sampling has resulted in sparse ground truth data, leaving us uncertain about where the highest concentrations occur. 
%This lack of detailed spatial information hampers targeted remediation efforts and underscores the need for scalable, data-driven methods to accurately predict PFAS hotspots and inform effective environmental interventions.
This lack of detailed spatial information hampers targeted remediation and calls for scalable, data-driven methods to identify PFAS hotspots and safeguard drinking water. 


\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{chall-compressed.pdf}
  \caption{Challenges in generating PFAS contamination maps \protect\cite{USEPA2015}}
  \label{fig:your-image-labell}
\end{figure}

% PFAS, or per- and polyfluoroalkyl substances, comprise over 9,000 synthetic chemicals first developed in the late 1940s. Known as "forever chemicals," they are exceptionally stable and resistant to breakdown, persisting in the environment and accumulating in living organisms. Studies show that 97\% of Americans have detectable PFAS levels in their blood, and these substances have contaminated drinking water sources for over 110 million people in the U.S. alone \cite{CDC_2024} - \cite{Andrews_Naidenko_2020}. Human exposure occurs through multiple pathways, including contaminated food and water, household products, air, and dust. Such exposure has been linked to severe health outcomes, including cancers, liver damage, weakened immune systems, and developmental issues in children. Poor waste management practices involving PFAS-containing materials have exacerbated their spread across ecosystems \cite{Mahmoudnia_2023}.


% \begin{figure*}[ht!]
% \centering
% \begin{subfigure}{0.49\textwidth} 
%   \centering %new4 (1).pdf
%   \includegraphics[width=\linewidth]{new4_compressed.pdf} 
%   \caption{}
%   \label{fig:sub1}
% \end{subfigure}
% \hfill
% \begin{subfigure}{0.49\textwidth}
%   \centering %new5.pdf
%   \includegraphics[width=\linewidth]{new5_compressed.pdf}
%   \caption{}
%   \label{fig:sub2}
% \end{subfigure}
% \caption{(a) Classical ML techniques require processing each pixel individually by aggregating data from the surrounding area using rasters and feeding it into the model to predict contamination at specific points; (b) DL methods process raster images directly, enabling the generation of dense PFAS contamination maps across entire areas in a single pass.}
% \label{fig:test}
% \end{figure*}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{merged-compressed.pdf}
  \caption{Left: Classical ML techniques aggregate surrounding pixel data to predict contamination at specific points; Right:  DL methods process raster images directly to generate dense PFAS contamination maps in a single pass. }
  \label{fig:test4}
\end{figure*}

% PFAS contamination of drinking water is a particularly urgent issue, as tap water can account for up to 34\% of human exposure \cite{Hu_Tokranov_Liddie_Zhang_Grandjean_Hart_Laden_Sun_Yeung_Sunderland_2019}. These chemicals infiltrate groundwater \cite{Schroeder_Bond_Foley_2021}, surface water \cite{Langenbach_Wilson_2021}, and soil \cite{Crone_Speth_Wahman_Smith_Abulikemu_Kleiner_Pressman_2019} due to their widespread use in industrial applications and consumer goods. Once present in these environmental reservoirs, PFAS can migrate into drinking water sources through interconnected hydrological systems. Groundwater, which supplies wells and aquifers, becomes contaminated as PFAS leach through soils. Surface water, including rivers, lakes, and wetlands, is similarly affected when PFAS enter via industrial discharge, agricultural runoff, or atmospheric deposition. These contaminated water sources ultimately feed into municipal drinking water systems, leading to significant public health concerns.

% The challenge of mitigating PFAS contamination underscores the need for innovative, data-driven solutions.
Artificial intelligence (AI) presents a promising avenue for addressing complex health and environmental issues, including identifying PFAS hotspots. In various domains, AI has proven effective at automating labor-intensive tasks and synthesizing domain expertise, whether in land cover mapping \cite{Robinson_Ortiz_Malkin_Elias_Peng_Morris_Dilkina_Jojic_2020}, agricultural optimization \cite{Kerner_Nakalembe_Yang_Zvonkov_McWeeny_Tseng_Becker-Reshef_2024}, or disease prediction \cite{Bondi-Kelly_Chen_Golden_Behari_Tambe_2023}. %, which could be  %By harnessing large, diverse datasets, AI methods enable rapid identification of patterns and priorities, allowing stakeholders to focus remediation efforts effectively. In the context of PFAS, these capabilities can 
%transformative for PFAS, helping locate contamination hotspots, inform targeted cleanup strategies, and safeguard public health and ecosystems.
 %Addressing the environmental distribution of PFAS through AI could revolutionize efforts to pinpoint contamination hotspots, enabling targeted cleanup strategies $and improving water safety and environmental health.
% Previous studies have examined the spatial distribution of PFAS concentrations, though often relying on labor-intensive methods. For example, \cite{DeLuca_Mullikin_Brumm_2023} and \cite{Salvatore_Mok_Garrett_2022} used geospatial data and random forest models to identify potential contamination sites, with \cite{DeLuca_Mullikin_Brumm_2023} predicting PFAS levels in fish tissue and \cite{Salvatore_Mok_Garrett_2022} analyzing the relationship between PFAS presence and facilities. Despite their contributions, these approaches still depend heavily on domain expertise and extensive manual data collection.
% % Liddie et al. integrated data on public water systems, testing, facilities, and sociodemographic factors to show that PFAS contamination disproportionately affects Hispanic/Latino and non-Hispanic Black communities. 
% In general, much of the existing AI research on PFAS contamination has been limited to sparse predictions based on scattered data points using traditional machine learning (ML) techniques, such as random forests. While these methods have provided insights, they often lack scalability for dense, comprehensive predictions across large geographical areas. A recent study by the USGS attempted to generate dense predictions for groundwater contamination across the U.S. \cite{Tokranov2023}. However, the method employed, which relies on XGBoost, is not well-suited for large-scale, real-time prediction due to its inefficiency. Generating predictions for even a single location requires \emph{significant computational resources} to preprocess and format the data before feeding it into the model. Approaches like these are also unlikely to be sustainable in the long term, especially if predictions need to be made across vast areas and in real time.
%
In the domain of PFAS contamination prediction, 
%\textcolor{red}{this statement seems to overlook hydrological modeling. Consider revising}
scientists have predominantly applied  machine learning (ML) models such as random forests \cite{Breiman_2001} and XGBoost \cite{Chen_Guestrin_2016}. % to sparse, scattered data points. %While these methods have yielded valuable insights, they often lack the scalability needed for dense, repeated predictions across large geographical areas.  
%For example, a recent USGS study \cite{Tokranov2023} used XGBoost to predict groundwater contamination across the U.S.. 
For example,  \cite{DeLuca_Mullikin_Brumm_2023} used a Random Forest model to predict PFAS contamination in the Columbia River Basin via a tabular approach that aggregates environmental features within a 5 km buffer around sample points, which helps account for the limited  ground truth data. 

Building on these efforts, we frame PFAS prediction as a geospatial deep learning (DL) task. Our approach directly processes imagery, preserving spatial dependencies and reducing the need for extensive feature engineering, leading  to  more efficiency. This approach will also allow for repeated, high-resolution  mapping to support ongoing monitoring efforts.

%However, this approach is computationally intensive, as even a single-location prediction requires significant preprocessing and data formatting. Such methods may not be sustainable in the long term, particularly for real-time predictions over vast regions.

\noindent Our contributions in this work are summarized as follows:
% \begin{itemize}
%     \item We propose \pname (\textbf{F}ramework for Envir\textbf{O}nmental \textbf{C}ontamination with \textbf{U}ncertainty \textbf{S}caling), a framework with a novel noise-aware loss function that incorporates pixel-wise confidence scaling to reduce the impact of label uncertainty.
%     \item The framework consists of a geospatial deep learning (DL) model for PFAS contamination prediction, utilizing raster data directly. To our knowledge, this is the first study to apply such an approach in the PFAS domain.
%     \item Through detailed ablation studies and comparative analyses, we assess the impact of different raster inputs, and the integration of the \pname loss function on model performance.
%     \item We benchmark our framework against traditional approaches that are the typical methods in the sciences like hydrology, including pollutant transport simulations 
%   %  \textcolor{red}{Did we benchmark it with SWAT model? The SWAT citation seems strange here}
%      and Kriging \cite{GISGeography_2017}, highlighting our framework's scalability, accuracy, and robustness for large-scale PFAS contamination prediction.
% \end{itemize}
\begin{itemize}
    \item We propose \pname (\textbf{F}ramework for Envir\textbf{O}nmental \textbf{C}ontamination with \textbf{U}ncertainty \textbf{S}caling), a framework employing a geospatial DL model for segmentation of PFAS contamination using rasters (e.g., grid-based geospatial data like satellite imagery or elevation maps).
    %We propose \pname (\textbf{F}ramework for Envir\textbf{O}nmental \textbf{C}ontamination with \textbf{U}ncertainty \textbf{S}caling), which features a novel noise-aware loss that scales pixel-wise confidence to reduce label uncertainty.
    \item Our framework features a novel noise-aware loss that leverages domain expertise on PFAS spread to automatically fill data gaps and weight these assumptions by confidence, thereby addressing the challenge of limited labeled data.
    %Our framework employs a geospatial deep learning (DL) model for segmentation of PFAS contamination using raster data. %and to our knowledge, this is the first study to apply such an approach in the PFAS domain.
    \item We benchmark our sparse segmentation framework against other AI models, \cite{DeLuca_Mullikin_Brumm_2023},  and  popular methods %in hydrology and the sciences 
    like %pollutant transport 
    simulations inspired by Soil and Water Assessment Tool (SWAT) \cite{SWAT2023} and Kriging \cite{GISGeography_2017}, demonstrating our framework's scalability, accuracy, and robustness for large-scale PFAS  prediction.
    \item Through ablation studies, % and comparative analyses, 
    we assess our  %impact of %different raster inputs and the integration of 
    \pname loss function and hyperparameter selection.
\end{itemize}

We develop this framework together with a nonprofit organization actively working on  environmental research, and an academic researcher specializing in water quality models, ensuring that our approach is  grounded in the latest expertise and real-world environmental concerns.

\section{Related Work}

The study of PFAS contamination has gained significant attention due to its widespread environmental and health impacts. Existing research in this domain spans analytical methods from scientific domains for detecting and understanding contamination patterns, as well as emerging computational approaches, including AI techniques, aimed at improving detection and prediction capabilities. 
%Additionally, challenges such as imbalanced datasets and noisy labels have led to the development of advanced loss functions and noise-handling techniques. 
%These methods provide valuable tools for improving model robustness and reliability in settings where data quality and distribution are critical. 
%This section reviews these areas of related work, providing context for the approaches taken in this study.

%The detection and analysis of contaminants like PFAS have primarily relied on laboratory-based techniques such as liquid chromatography-tandem mass spectrometry (LC-MS/MS) and advanced spectroscopic methods \cite{Development}. These approaches provide high accuracy in quantifying PFAS concentrations but are often limited by their cost, time requirements, and the inability to scale across large spatial datasets. To address these challenges, various methods have been explored to model the distribution and transport of contaminants in the environment. For example, hydrological models and geospatial techniques like Kriging interpolation and other spatial analysis methods are commonly used to predict spatial patterns of contamination. These methods help to create continuous maps of pollutant distribution, even in areas with sparse data. %

%\subsection{Current Ways to Measure and Predict PFAS}
\paragraph{Current Ways to Measure and Predict PFAS.}
% The detection and analysis of contaminants like PFAS have primarily relied on laboratory-based techniques such as liquid chromatography-tandem mass spectrometry (LC-MS/MS) and advanced spectroscopic methods \cite{Shoemaker_2020_Method537}. These approaches provide high accuracy in quantifying PFAS concentrations but are often limited by their cost, time requirements, and the inability to scale across large spatial datasets. 
The measurement of PFAS  relies on laboratory-based analysis of water, fish tissue, and human blood samples with techniques such as liquid chromatography-tandem mass spectrometry (LC-MS) \cite{Shoemaker_2020_Method537}. Fish accumulate PFAS over time, serving as integrative indicators of water contamination hotspots that may be missed by sporadic water sampling. Therefore, fish tissue measurements %PFAS contamination is also assessed via bioaccumulation in fish tissue—
provide complementary, long-term  insights into PFAS presence, as well as potential exposure risks to anglers \cite{EWGstudy2023}. While highly accurate, these methods are costly \cite{Doudrick_2024}, time-intensive, and difficult to scale for large spatial datasets.

PFAS monitoring efforts have  made use of such  measurement techniques and beyond.  % taken many forms. 
For example, states such as Colorado have implemented water testing, treatment grants, and regulatory measures since 2016 \cite{CDPHE_PFAS_Action_Plan}. In addition, there are community-based water sampling initiatives such as volunteer sampling by the Sierra Club \cite{CommunityScience} and focused efforts by  organizations like the  Ecology Center  \cite{ProtectingCommunitiesFromPFAS}. A recent study further underscores the importance of these efforts, revealing associations between PFAS contamination in drinking water and higher COVID-19 mortality rates in the U.S. \cite{Liddie_Bind_Karra_Sunderland_2024}. % are conducting valuable, participatory PFAS sampling and testing. 
%PFAS contamination is also assessed via bioaccumulation in fish tissue—providing complementary, long-term  insights into PFAS presence and exposure risks to anglers, as highlighted in recent studies by the Environmental Working Group (EWG) \cite{EWGstudy2023}. 
Resources like the PFAS-Tox Database \cite{Pelch_Reade_Kwiatkowski_Merced-Nieves_Cavalier_Schultz_Wolffe_Varshavsky_2022} further assist researchers,  policymakers, and communities in monitoring PFAS. 
These efforts are important and complementary to \pname; we aim to  bridge critical data gaps and provide continuous, scalable predictions until additional sampling can be carried out.
%help fill in gaps in these efforts until more sampling can be done. %identifying emerging risks and formulating  mitigation strategies.

Various modeling approaches have been explored to predict contaminant distribution. %, including process-based hydrological models and geostatistical techniques like Kriging. 
Hydrological models such as SWAT \cite{SWAT2023} and MODFLOW \cite{ktorcoletti_2012} simulate pollutant transport using environmental parameters like flow rates, land use, and weather conditions. While effective, these models require extensive data %, struggle to represent complex biochemical processes, 
and are computationally expensive at large scales \cite{zhi2024deep}. %Therefore, hydrological modeling for PFAS remains unexplored due to the complexity of its sources and biochemical processes. 
%Despite advances in scientific machine learning (SciML), its integration with hydrological modeling for PFAS remains unexplored due to the complexity of its sources and biochemical processes. 
%
Kriging, a geostatistical interpolation technique, has been widely used for mapping pollutants, including soil contamination \cite{Largueche_2006} and groundwater quality estimation \cite{Singh_Verma_2019}. Though useful in data-sparse environments, Kriging relies on simplifying assumptions such as spatial continuity and stationarity \cite{GISGeography_2017} that may not hold for complex contaminants like PFAS, whose transport dynamics and uncertainties are  more intricate. We aim to take a data-driven, expert-informed approach to address these challenges. %Consequently, such techniques may fall short for large-scale PFAS contamination prediction. 

% \begin{figure}[ht!]
%   \centering
%   \includegraphics[width=\columnwidth]{chall-compressed.pdf}
%   \caption{Challenges in generating PFAS contamination maps: limited sampling, data gaps, and infrequent updates \cite{USEPA2015} }
%   \label{fig:your-image-labell}
% \end{figure}


%Recognizing these challenges, states like Colorado have implemented comprehensive measures since 2016, including extensive water testing, treatment grant programs, and new regulations to reduce PFAS releases \cite{CDPHE_PFAS_Action_Plan}. Their updated PFAS Action Plan (2024) emphasizes minimizing exposure, assessing health risks, and partnering with federal agencies to limit PFAS entry into the environment. In parallel, resources such as the PFAS-Tox Database \cite{PFAS_Tox_Database} consolidate scientific literature into a \enquote{systematic evidence map}, enabling researchers, regulators, and communities to identify emerging health risks and evidence gaps, and to make informed decisions on mitigating PFAS impacts.

% In addition to these regulatory and remediation efforts, various methods have been explored to model the distribution and transport of contaminants in the environment. For example, process-based hydrological models and geostatistical techniques like Kriging interpolation are commonly used to predict spatial patterns of contamination. These methods help to create continuous maps of pollutant distribution, even in areas with sparse data. 

%Hydrological models, such as the Soil and Water Assessment Tool (SWAT) and the MODFLOW groundwater model, simulate contaminant transport by incorporating factors like flow rates, land use, and weather patterns, while geostatistical techniques like Kriging interpolate data based on spatial relationships. %
% Hydrological models typically simulate water movement and pollutant transport, integrating factors like flow rates, land use, and weather patterns, while spatial interpolation techniques like Kriging may be used to predict contaminant concentrations at unsampled locations based on the spatial relationships between observed data points \cite{Largueche_2006}. Common hydrological models, such as the Soil and Water Assessment Tool (SWAT) \cite{SWAT2023} and the MODFLOW groundwater model \cite{ktorcoletti_2012}, have been widely applied to simulate the movement of pollutants like fertilizers, heavy metals, and pesticides in aquatic systems. These models use a combination of physical and empirical processes to simulate contaminant fate and transport in both surface water and groundwater systems.

%\textcolor{red}{I rewrite the paragraph above in the paragraph below} 

% Processed-based hydrological models typically simulate pollutant source and transport with hydrological and biochemical principles that are represented as functions of flow rates, soil, land use, weather, and so on. Common hydrological models, such as the Soil and Water Assessment Tool (SWAT) \cite{SWAT2023} and the MODFLOW groundwater model \cite{ktorcoletti_2012}, have been widely applied to simulate the movement of pollutants like suspended solids, nutrients, heavy metals, and pesticides in aquatic systems. These models use a combination of physical and empirical processes to simulate contaminant fate and transport in both surface water and groundwater systems. However, process-based hydrological models suffer from several major limitations, including the difficulty in representing complex processes accurately, requirement of extensive and detailed data, and expensive computation at large spatial-temporal scales \cite{zhi2024deep}. Despite the recent advancement in scientific machine learning (SciML), its integration with process-based hydrological model has not yet been applied to PFAS modeling given the underlying complicated pollutant sources and biochemical processes. 

% In contrast, Kriging is a geostatistical interpolation technique often used to predict the spatial distribution of contaminants, particularly when direct measurements are sparse. Kriging has been applied in a variety of environmental contexts, such as the mapping of soil contamination \cite{Largueche_2006} and the estimation of groundwater quality in contaminated sites \cite{Singh_Verma_2019}. 
% %\textcolor{red}{You can delete the remaining texts in this paragrpah from here after adding a "limitation" sentence of Kriging} 
% Techniques like Kriging provide valuable insights for mapping pollutants in environments with limited sampling.
%though their ability to predict the unique spread of PFAS contamination may be constrained due to the complex environmental processes influencing PFAS mobility%. 
% However, these approaches, while useful for general environmental modeling, struggle to capture the unique behaviors and uncertainties of PFAS, given its complex environmental interactions. 

%Moreover, hydrological models require extensive preprocessing such as sub-basin delineation and parameter calibration, which can be highly resource-intensive, potentially taking hours or days to complete, for numerous small patches or large regions. This computational burden is compounded for pollutants like PFAS, as the models lack native support for their transport, necessitating further customization.
%\setlength{\parskip}{0pt} % Removes extra space between paragraphs
%\subsection{AI Approaches for PFAS Prediction}
\paragraph{AI Approaches for PFAS Prediction.}
Recent advances in AI and geospatial modeling have opened new avenues for tackling the complexities of PFAS detection and prediction by integrating environmental variables such as land cover, proximity to industrial facilities, and hydrological flow patterns. For example, DeLuca et al. \cite{DeLuca_Mullikin_Brumm_2023} used random forests to predict PFAS contamination in fish tissue in the Columbia River Basin, leveraging geospatial data from industrial and military facilities, while Salvatore et al. \cite{Salvatore_Mok_Garrett_2022} developed a \enquote{presumptive contamination} model based on PFAS-producing or -using facilities' proximity in the absence of comprehensive testing data. On a national scale, the USGS applied XGBoost to forecast PFAS occurrence in groundwater \cite{Tokranov2023}. This model integrated explanatory variables such as PFAS sources, groundwater recharge, etc. to predict contamination in unmonitored areas leveraging precomputed rasters of these variables. Such approaches require extensive manual feature engineering, such as calculating buffer statistics for raster grid cells to generate precomputed rasters. This preprocessing is resource-intensive, and once complete, the model is applied to each raster cell individually, which can limit its ability to retain spatial context and dependencies.
%Although these methods guide resource allocation, they often face scalability and computational challenges that limit dense predictions over large areas.
%Addressing these challenges is important to advance regular PFAS modeling. % and guide effective mitigation efforts.
%\vspace{-0.5m}

%\subsection{Geospatial Models}
\paragraph{Geospatial Models.}
Recent advances in geospatial deep learning have introduced foundation models trained on large-scale satellite datasets for improved spatial prediction. Models like Prithvi \cite{Blumenfeld_2023} and SatMAE \cite{Cong_Khanna_Meng_Liu_Rozi_He_Burke_Lobell_Ermon_2023} use masked autoencoders (MAE) to learn generalizable spatial representations in a semi-supervised manner, leveraging both labeled and unlabeled data. Unlike some traditional ML approaches that require extensive feature engineering, these models process raster data directly, preserving spatial dependencies. 
%Their success in tasks like flood mapping and wildfire detection \cite{Blumenfeld_2023} suggests that these models can leverage broad spatial context for PFAS prediction while minimizing manual preprocessing.
However, many of these models assume that key environmental information is directly observable in satellite imagery. In contrast, PFAS contamination is not directly visible, necessitating the use of intermediate geospatial data products, such as land cover, to capture the underlying environmental processes. These models' success in tasks like flood mapping and wildfire detection \cite{Blumenfeld_2023} suggests that, with the appropriate data preprocessing, they can leverage broad spatial context for PFAS prediction while minimizing manual effort.
%Their success in land cover classification, flood mapping, and wildfire detection suggests their potential for PFAS contamination prediction by leveraging broader spatial context while reducing manual preprocessing.

%\subsection{Addressing Data Scarcity}
\paragraph{Addressing Data Scarcity.}
In environmental data analysis, limited ground truth often necessitates pseudo- or augmented labels, which can introduce noise and lead to overfitting. Various strategies have been proposed to mitigate these issues, including label smoothing and robust loss functions (e.g., Huber loss \cite{Gokcesu_Gokcesu_2021}, Generalized Cross-Entropy \cite{Zhang_Sabuncu_2018}, bootstrapping loss \cite{Reed_Lee_Anguelov_Szegedy_Erhan_Rabinovich_2015}), and focal loss \cite{Lin_Goyal_Girshick_He_Dollár_2018}). Weakly supervised methods like the FESTA loss \cite{Hua_Marcos_Mou_Zhu_Tuia_2022} further aim to capture spatial and feature relationships. %Additionally, some approaches use point supervision guided by an objectness prior to delineate discrete objects \cite{Bearman_Russakovsky_Ferrari_Fei-Fei_2016}. 
Notably, FESTA is one of the closest approaches to ours, as it is specifically designed for segmentation of remote sensing images using sparse annotations. However, we hypothesize that such methods are less suitable for PFAS contamination mapping, as these approaches often assume spatial continuity, whereas the diffuse nature of contamination lacks clear object boundaries. We compare with FESTA to test this hypothesis (see Section \ref{sec:baselines}). 
%However, these approaches often assume spatial continuity; a condition that may not hold for the complex dynamics of PFAS contamination. 
Moreover, while transfer learning and active learning \cite{Li_Wang_Chen_Lu_Fu_Wu_2024} can help leverage sparse data, transfer learning is limited by our incomplete understanding of PFAS-specific hydrological processes, and active learning is highly dependent on the quality of uncertainty measures, which in complex environmental settings may require significant domain expertise. These challenges underscore the need for methods that integrate domain-specific knowledge directly into the training process.
% In environmental data analysis, the scarcity of ground truth often leads to the use of pseudo- or augmented labels, which introduce noise and can lead to overfitting and poor generalization. Strategies such as label smoothing reduce model overconfidence by softening predictions, while robust loss functions including Huber loss \cite{Gokcesu_Gokcesu_2021}, Generalized Cross-Entropy (GCE) \cite{Zhang_Sabuncu_2018}, and bootstrapping loss \cite{Reed_Lee_Anguelov_Szegedy_Erhan_Rabinovich_2015} mitigate the impact of noisy labels during training. Another strategy involves focal loss \cite{Lin_Goyal_Girshick_He_Dollár_2018}, which prioritizes difficult-to-classify examples and helps balance class imbalances. Additionally, the FESTA loss \cite{Hua_Marcos_Mou_Zhu_Tuia_2022} has been proposed to encode spatial and feature relationships, providing another robust option for handling the challenges of sparse segmentation. Beyond loss functions, strategies like active learning and semi-supervised learning maximize the utility of limited labeled data, allowing models to iteratively refine predictions based on the most informative samples \cite{Li_Wang_Chen_Lu_Fu_Wu_2024}.
 %\vspace{-4em}
\section{Methodology}\label{sec:methodology}
\subsection{Dataset}
%\vspace{-2em}

This study aims to perform segmentation on surface water pixels, classifying them as having PFAS concentrations above or below established health advisory thresholds using geospatial data. The dataset is built from the U.S. Environmental Protection Agency (EPA) National Rivers and Streams Assessment (NRSA) \cite{USEPA2015} and the Great Lakes Human Health Fish Fillet Tissue Study \cite{USEPA2024}, which together provide 866 sample points collected over multiple years. PFAS concentrations in fish tissue were measured as continuous values in these datasets but were then categorized into above (1) or below (0) health advisory thresholds based on thresholds defined by the U.S. EPA’s Fish and Shellfish Advisory Program \cite{Contaminants_to_Monitor_in_Fish_and_Shellfish_Advisory_Programs_2024}. These thresholds indicate the health risks associated with different PFAS compounds in fish tissue. These sample points also exhibit a significant class imbalance, with 775 points labeled as above (89.5\%) and 91 labeled as below (10.5\%) safety thresholds. 
%This imbalance poses challenges for training robust models and necessitates strategies to mitigate the impact of skewed data distributions.

Our dataset consists of ground truth masks generated from these data, multi-channel raster images as features, %ground truth masks, 
and noise masks, all of which provide a spatially and environmentally contextualized representation of contamination risks. %These components combine environmental features to support a nuanced understanding of PFAS distribution and spread.


\subsection{Raster Image Generation}

To spatially integrate the data, multi-channel raster images with 45 channels were generated around each sample point. Each image is a \(P \times P\) pixels patch with a 30-meter resolution, centered on the geographic coordinates of the sample point, where the optimal value of \(P\) is determined in Section \ref{sec:ablation}.
These raster images integrate several key features. First, we incorporate readily available data products such as the National Land Cover Database (NLCD) raster \cite{USGS_NLCD_2023} to represent land cover types, and flow direction rasters that capture hydrological connectivity. In addition, we generate distance rasters to quantify proximity to potential PFAS dischargers. The discharger location data are obtained from the U.S. EPA Enforcement and Compliance History Online (ECHO) \cite{US_EPA_Water_Pollution} and converted into distance rasters using a distance transform for efficient spatial analysis.
%The \(256 \times 256\) pixels size is used in the current setup, with alternative patch sizes to be evaluated in the ablation study (Section \ref{sec:ablation}).
% \begin{figure}[ht!]
% \centering
% \begin{subfigure}{.32\columnwidth} 
%   \centering
%   \includegraphics[width=\linewidth]{1.png}
%   \caption{}
%   \label{fig:sub1}
% \end{subfigure}%
% \hfill
% \begin{subfigure}{.33\columnwidth}
%   \centering
%   \includegraphics[width=\linewidth]{2_new.png}
%   \caption{}
%   \label{fig:sub2}
% \end{subfigure}%
% \hfill
% \begin{subfigure}{.32\columnwidth}
%   \centering
%   \includegraphics[width=\linewidth]{3.png}  % Replace with the filename of the third image
%   \caption{}
%   \label{fig:sub3}
% \end{subfigure}
% \caption{The figure displays three channels from a sample dataset image: (a) the NLCD channel showing land cover types such as urban, forest, water, etc. ; (b) the discharger proximity channel for organic chemical manufacturing where darker areas indicate closer proximity, obtained via a distance transform on a binary raster of industry locations; and (c) the water flow direction channel, which encodes upstream/downstream flows based on physical modeling.}
% \label{fig:test}
% \end{figure}


\begin{figure}[ht!]
  \centering
  \includegraphics[width=\columnwidth]{tog.png}
  \caption{The figure shows three channels from a sample dataset: (1) the NLCD channel displaying land cover types, (2) the discharger proximity channel indicating proximity to the organic chemical manufacturing industries, and (3) the flow direction channel encoding upstream/downstream flows from physical modeling. }
  \label{fig:test1}
\end{figure}

% \begin{figure}[ht!]
%   \centering
%   \includegraphics[width=\columnwidth]{Picture10.png}
%   \caption{The figure shows three channels of a sample image from the dataset: (1) the NLCD channel, (2) the discharger channel , and (3) the water flow direction channel. The images are arranged
% from left to right.}
%   \label{fig:your-image-label10}
% \end{figure}
% \begin{figure}[ht!]
% \centering
% \begin{subfigure}{.32\columnwidth} 
%   \centering
%   \includegraphics[width=\linewidth]{1.png}
%   \caption{}
%   \label{fig:sub1}
% \end{subfigure}%
% \hfill
% \begin{subfigure}{.33\columnwidth}
%   \centering
%   \includegraphics[width=\linewidth]{2_new.png}
%   \caption{}
%   \label{fig:sub2}
% \end{subfigure}%
% \hfill
% \begin{subfigure}{.32\columnwidth}
%   \centering
%   \includegraphics[width=\linewidth]{3.png}  % Replace with the filename of the third image
%   \caption{}
%   \label{fig:sub3}
% \end{subfigure}
% \caption{The figure shows three channels of a sample image from the dataset: (a) the NLCD channel, which represents land cover categories such as urban areas, forests, and water bodies; (b) the discharger proximity channel for the organic chemicals manufacturing industries, where darker colors indicate areas closer to the known industries and lighter colors represent areas farther away, generated using a distance transform on a binary raster of the industries locations; and (c) the water flow direction channel, which encodes the direction of water flow for each pixel (upstream/downstream) based on hydrological modeling.}
% \label{fig:test}
% \end{figure}

\subsection{Ground Truth Masks}
Our goal is to perform segmentation of surface water to predict PFAS contamination, but the available data consists of sparse point-level contamination measurements rather than dense labels. To generate dense training labels, we adopt a point-based ground truth expansion strategy, where every surface water pixel in a patch is labeled according to the EPA-defined contamination status of the patch's sample point. Specifically, if the sample point indicates PFAS above the safety thresholds, all surface water pixels are labeled 1; if below, they are labeled 0. Non-surface water pixels are assigned a value of 2. This approach extends point-level data across the patch, assuming that nearby areas share similar contamination levels. 
% Our goal is to perform segmentation of surface water to predict PFAS contamination, but the available data consists of sparse point-level contamination measurements rather than dense labels. To address this, we devised two approaches to generate ground truth masks by translating point-based labels into patch-level segmentation labels: the rule-based baseline and point-based ground truth expansion.

% For the rule-based baseline approach, %no data from the EPA datasets were used to define contamination labels for training. Instead, 
% each surface water pixel \(i\) was assigned a contamination probability \(p_i\) based on three environmental and spatial factors; proximity to PFAS dischargers, land cover type, and downstream flow, that are combined as follows: 
% \[
% p_i = \alpha_1 \cdot p_{\text{dischargers}} + \alpha_2 \cdot p_{\text{landcover}} + \alpha_3 \cdot p_{\text{downstream}},
% \]
% where, based on discussions with domain experts, we set \(\alpha_1 = 0.5\), \(\alpha_2 = 0.4\), and \(\alpha_3 = 0.1\). Pixels with \(p_i > 0.5\) were labeled as 1, and those with \(p_i \le 0.5\) were labeled as 0. Non-surface water pixels were assigned a label of 2 to indicate background regions.
% % each surface water pixel \(i\) was assigned a contamination probability \(p_i\) based on three environmental and spatial factors; proximity to PFAS dischargers, land cover type, and downstream flow, combined with the following weighted sum based on discussions with domain experts:
% % \begin{align}
% %     p_{i} = & \, 0.5\cdot p_{\text{dischargers}} + 0.4\cdot p_{\text{landcover}} + 0.1\cdot p_{\text{downstream}}
% % \end{align}

% % Pixels with \(p_i > 0.5\) were labeled as 1, and those with \(p_i \le 0.5\) were labeled as 0. Non-surface water pixels were assigned a label of 2 to indicate background regions.

% In the point-based ground truth expansion, every surface water pixel in a patch is labeled according to the EPA-defined contamination status of the patch's sample point. Specifically, if the sample point indicates high PFAS, all surface water pixels are labeled 1; if low, they are labeled 0. Non-surface water pixels are assigned a value of 2. This approach extends point-level data across the patch, assuming that nearby areas share similar contamination levels.

\subsection{Noise Masks}

To quantify the confidence that a pixel’s label (either 0 or 1) is correct since we are expanding the ground truth data, we combined four key factors; proximity to PFAS dischargers, land cover type, distance to other sample points, and downstream flow, into a single probability value. Each factor contributed a portion of the final probability, reflecting domain-informed assumptions about which conditions most strongly indicate correct labeling. 

The final contamination probability for each surface water pixel \(i\) is computed as:
\begin{align}
    p_{\text{final}} = & \, \alpha_1 \cdot p_{\text{dischargers}} + \alpha_2 \cdot p_{\text{landcover}} + \alpha_3 \cdot p_{\text{sample\_dist}} \nonumber \\
    & \, + \alpha_4 \cdot p_{\text{downstream}}
\end{align}
where \(p_{\text{dischargers}}\),  \(p_{\text{landcover}}\),  \(p_{\text{sample\_dist}}\), and  \(p_{\text{downstream}}\) represent the contamination probabilities derived from (i) proximity to PFAS dischargers, (ii) land cover type, (iii) distance to other sample points, and (iv) downstream flow, respectively. In the following paragraphs, we briefly describe each factor:

\paragraph{Proximity to PFAS Dischargers (\(p_{\text{dischargers}}\))} 
If a pixel’s label was 1, we applied an exponential decay function of distance, assigning higher probabilities to pixels closer to known dischargers. For pixels labeled 0, we inverted this logic so that being near a discharger lowered confidence in the 0 label. 
%The 0.4 weight signifies the primary role that dischargers can play in PFAS contamination risk.

\paragraph{Land Cover Type (\(p_{\text{landcover}}\))}
For pixels labeled 1, surrounding urban or built-up areas received higher probabilities than forested regions, consistent with research linking PFAS to industrial zones \cite{Dimitrakopoulou_Karvounis_Marinos_Theodorakopoulou_Aloizou_Petsangourakis_Papakonstantinou_Stoitsis_2024}. Conversely, for 0-labeled pixels, surrounding undeveloped or forested land cover increased confidence in low contamination.
%We assigned a 0.3 weight to land cover, acknowledging its impact but placing it just below dischargers in importance.

\paragraph{Proximity to Other Sample Points (\(p_{\text{sample\_dist}}\))}
Pixels closer to known sample points were assigned higher probabilities using an exponential decay function based on distance to each sample point in the patch. 
%We used 0.2 to reflect its influence, though less than source proximity or land cover.

%\vspace{-1em}

\paragraph{Downstream Flow (\(p_{\text{downstream}}\))}
If a pixel lies downstream of a sample point labeled 1, its probability for label 1 increases, and similarly for label 0. %We assign a modest weight of 0.1 to this downstream flow factor, acknowledging its role in capturing hydrological connectivity, though it is generally less direct than discharger proximity or land cover. 
This flow direction channel was generated using ArcGIS Pro \cite{ArcGIS_Pro}, a geographic information system (GIS) software used for spatial analysis, and environmental modeling. ArcGIS Pro enabled extraction of flow direction patterns based on digital elevation models (DEMs) \cite{EarthScienceDataSystems_2024}, ensuring downstream influence was accurately represented in the probability calculation.
%\vspace{-1em}
% \paragraph{Final Probability Calculation}
% We computed partial probabilities for each factor and then formed a weighted sum:
% %\vspace{0.5em} % Reduce the space below the equation
% %\vskip -11pt
% \begin{align}
%     p_{\text{final}} = & \, \alpha_1 \cdot p_{\text{dischargers}} + \alpha_2 \cdot p_{\text{landcover}} + \alpha_3 \cdot p_{\text{sample\_dist}} \nonumber \\
%     & \, + \alpha_4 \cdot p_{\text{downstream}}
% \end{align}

% %\vskip -15.1pt 
% %\vspace{-1em}


Based on discussions with domain experts and experimental validation (see supplementary material), we set the weights as \(\alpha_1 = 0.4\), \(\alpha_2 = 0.3\), \(\alpha_3 = 0.2\), and \(\alpha_4 = 0.1\).
Pixels at verified sample points themselves are assigned a probability of 1, reflecting complete confidence in their correctness. For other pixels, by tailoring contributions based on whether the pixel’s label was 1 or 0, we created a confidence score that indicates how likely the label is to be correct. For instance, pixels labeled 1 that are far from dischargers or 0 in developed areas receive low probabilities, indicating uncertainty. These noise masks down-weight uncertain labels and emphasize high-confidence ones, thereby enhancing model robustness. 
%For instance, a pixel labeled 1 yet far from any discharger and situated in pristine land cover would have a relatively low probability, signaling uncertainty. Conversely, a pixel labeled 0 in a densely developed area near multiple dischargers would also receive a low probability, reflecting potential mislabeling. These noise masks proved essential during training by down-weighting uncertain labels and emphasizing high-confidence pixels, reducing the impact of noisy or conflicting label data and improving the model’s robustness.
%\vspace{-0.1em} % Reduce the space below the equation
\subsection{Model}
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{frame.pdf}  
%  \vspace{1em}  % Optional: adjust the vertical space after the figure for better alignment
  \caption{Model overview: high-level representation of FOCUS}
  \label{fig:your-image-label}
\end{figure*}
%\vspace{-1em} % Reduce the space below the equation
% Our framework, \pname, is inspired by the Prithvi architecture \cite{Blumenfeld_2023} and employs the MAE approach for pretraining. We pretrain our model on derived geospatial data products (e.g., land cover) rather than using the original Prithvi weights, as these data provide further structure for predicting  PFAS contamination  (see Section~\ref{sec:results}).
Our framework, \pname, is inspired by the Prithvi architecture \cite{Blumenfeld_2023} and employs a masked autoencoder (MAE) approach for pretraining. Instead of relying on raw satellite imagery and Prithvi’s pretrained weights, we pretrain our model on derived geospatial data products (e.g., land cover), which offer more contextual information for capturing the environmental nuances critical to PFAS contamination prediction (see Section~\ref{sec:results}). This pretraining uses masked autoencoding with a mean squared error loss.
%, enhancing both the relevance of the input data and the explainability of the model's predictions. 
The pretrained encoder is then fine-tuned with a dedicated decoder head for the downstream task of PFAS prediction, resulting in improved data efficiency and overall performance.

\subsection{Loss Function}

The loss function used in \pname combines the focal loss with noise mask weighting as follows:
\begin{align}
    \mathcal{L}_{\text{\pname}} = 
    \frac{1}{N} \sum_{i=1}^{N} \, \mathcal{L}_{\text{focal}}(z_i, y_i) \cdot M_i, \\
    \mathcal{L}_{\text{focal}}(z_i, y_i) = (1 - p_{y_i})^\gamma \cdot \mathcal{L}_{\text{CE}}(z_i, y_i), \\
    \mathcal{L}_{\text{CE}}(z_i, y_i) = -\log \left( \frac{\exp(z_{i,y_i})}{\sum_{j} \exp(z_{i,j})} \right)
\end{align}
where \(N\) is the number of pixels, \(z_i\) and \(y_i\) are the predicted logits and true label for pixel \(i\), respectively, \(M_i\) is the noise mask value for pixel \(i\), and \(p_{y_i}\) is the predicted probability for the true label \(y_i\). Here, \(z_{i,y_i}\) is the logit for the true class and \(j\) indexes all classes.
%Here, \(z_{i,y_i}\) is the logit corresponding to the true class of pixel \(i\), and the denominator \(\sum_{j} \exp(z_{i,j})\) sums the exponentials of the logits for all classes to produce a normalized probability via the softmax function.

The parameter \(\gamma = 2\) is used in the focal loss to focus on hard-to-classify examples, as this value showed the best performance in previous studies introducing focal loss \cite{Lin_Goyal_Girshick_He_Dollár_2018}. Notably, class weights to address dataset imbalance are directly incorporated into the cross-entropy loss (\(\mathcal{L}_{\text{CE}}\)), removing the need for an explicit alpha term in focal loss. By combining focal loss and noise mask weighting, the loss function addresses label noise and class imbalance while emphasizing high-confidence pixels during training.

% \begin{figure}[ht!]
% \centering
% \begin{subfigure}{.5\columnwidth}
%   \centering
%   \includegraphics[width=\linewidth]{mask.png}
%   \caption{}
%   \label{fig:sub1}
% \end{subfigure}%
% \hfill
% \begin{subfigure}{.5\columnwidth}
%   \centering
%   \includegraphics[width=\linewidth]{download (14).png}
%   \caption{}
%   \label{fig:sub2}
% \end{subfigure}
% \caption{Illustration of a sample input mask and its corresponding noise mask: (a) the input mask assigns a label of 0 here to all surface water pixels (ground truth for PFAS contamination), and (b) the noise mask provides pixel-wise confidence scores. The model’s loss is computed as the focal loss between predictions and the input mask, scaled by the noise mask to emphasize high-confidence labels.
% }
% \label{fig:test}
% \end{figure}

%\subsection{Loss Function and Training}

%The loss function incorporates weighted focal loss to address label noise and class imbalance. Noise masks provide pixel-wise confidence scores, which are used to weight the focal loss function. Pixels with higher confidence contribute more to the loss, while those with lower confidence have a reduced impact. This weighting mechanism enables the model to \pname more on reliable data, which minimizes the influence of noisy or uncertain labels. By reducing the effect of label noise, the model is better able to classify PFAS contamination and predict risk in areas with varying levels of certainty.
\subsection{Baselines}\label{sec:baselines}
To contextualize our proposed framework for PFAS contamination prediction, we introduce several baseline approaches commonly employed in the field:

\begin{itemize}[left=0pt]
    \item \textbf{Pollutant transport simulation:} A process-based approach that models contaminant movement using hydrological principles.
    \item \textbf{Landsat-based method:} Uses Landsat 7 multispectral imagery \cite{USGSLandsat7} and the original Prithvi weights to infer contamination from raw satellite imagery.
    \item \textbf{FESTA loss approach:} An alternative state-of-the-art technique for sparse segmentation that encodes spatial and feature relationships, originally designed for datasets with mixed spatial formats such as incomplete lines, polygons, and points.
    \item \textbf{Kriging:} A geostatistical interpolation technique that predicts contamination based on spatial relationships between points.
    \item \textbf{Random forest:} Following the methodology of \cite{DeLuca_Mullikin_Brumm_2023}, this model aggregates environmental features within a 5 km buffer around each sample from the NRSA dataset; for example, calculating the percentage of various land cover types and the distance to the nearest PFAS dischargers to predict contamination at individual points.
    \item \textbf{Rule-based approach:} %As an alternative to the point-based expansion method, t
    This approach assigns contamination labels based on predefined environmental heuristics. Similar to our noise mask computation, but without data about ground truth points, it computes a contamination probability by combining weighted factors, such as discharger proximity, land cover, and flow direction, and then thresholds the result to yield binary labels. 
     
\end{itemize}
Further details on Kriging and pollutant transport simulation are provided in the supplementary material. Collectively, these baselines provide a comprehensive benchmark for evaluating the performance and scalability of our proposed framework.
% These baselines ranging from traditional ML and geostatistical methods to process-based simulations, serve as reference points for evaluating the efficacy and scalability of our proposed framework.
%model calibration measured by Expected Calibration Error \cite{Guo_Pleiss_Sun_Weinberger_2017}

\subsection{Experiments and Ablation Study}\label{sec:ablation}

We now describe our experimental setup, with more details in the supplementary material.
\paragraph{Training Configuration.}
% We initially considered a conventional 70\%-10\%-20\% split for training, validation, and testing, respectively, to facilitate intermediate performance monitoring. However, due to the limited availability of labeled data and a severe class imbalance with the minority class being particularly scarce, setting aside a separate validation set in order to guide the training process not only reduced the effective training size but also resulted in a highly skewed validation set. Through extensive empirical testing, we observed that a 80\%-20\% split for training and testing sets, respectively, significantly improved performance on the test set. This approach maximizes the data available for training while maintaining a rigorous, geographically disjoint evaluation on an independent test set.
We adopted an 80\%-20\% split for each year's data for training and testing, respectively, ensuring that the test set is geographically disjoint for rigorous evaluation. %Our training configuration was carefully chosen to suit the constraints of our dataset. 
A batch size of 4 and a learning rate of \(5 \times 10^{-4}\) were adopted to preserve the stability of our pretrained backbone, minimizing the risk of large, destabilizing updates. In addition, we employed AdamW \cite{Loshchilov_Hutter_2019} as the optimizer with \(\beta_1 = 0.9\) and \(\beta_2 = 0.999\). Furthermore, we used a custom learning rate scheduler with an initial warmup phase followed by polynomial decay, which helped stabilize convergence and ensured that the model gradually adapted to the limited data.
%urther details on the dataset splitting methodology are provided in the supplementary material.
% Our training configuration was carefully chosen to suit the constraints of our dataset. A batch size of 4 and a learning rate of \(5 \times 10^{-4}\) were adopted to preserve the stability of our pretrained backbone, minimizing the risk of large, destabilizing updates. In addition, we employed AdamW \cite{Loshchilov_Hutter_2019} as the optimizer with \(\beta_1 = 0.9\) and \(\beta_2 = 0.999\). Furthermore, we used a custom learning rate scheduler with an initial warmup phase followed by polynomial decay, which helped stabilize convergence and ensured that the model gradually adapted to the limited data.  Further details on the dataset splitting methodology and results using the validation set are provided in the supplementary material.
%These fixed hyperparameters ensured stable training dynamics and facilitated fair comparisons across experiments, supported by the use of a robust learning rate scheduler.

% The dataset was split 80\% for training and 20\% for testing, ensuring no geographic overlap between sets. Further details on the splitting methodology can be found in the supplementary material. The model was trained for 100 epochs using the focal loss function, saving checkpoints every epoch. After training, the best-performing model was selected based on test set performance from these checkpoints. 
\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\textwidth]{finall.png}  
  \caption{Comparison of performance across different methods and years.}
  \label{fig:your-image-label}
\end{figure*}
\paragraph{Ablation Study.}

%To evaluate the impact of various design choices, we perform a series of ablation experiments using different configurations of the labeling strategies described earlier. Ground truth masks for training were generated based on the strategy outlined in Section \ref{sec:methodology}. These masks were used in combination with different experimental setups to understand the contribution of each factor to the model's performance.
%
We conduct ablation studies across the following dimensions:

\begin{itemize}[left=0pt]
    \item \textbf{Raster Size:} To understand the influence of spatial context, we compare raster patches of \(512 \times 512\) and \(256 \times 256\) pixels. Larger patches capture broader contextual information, potentially including contamination sources farther from the sample point, while smaller patches reduce noise and focus on localized features. This evaluation identifies the trade-off between contextual richness and spatial precision.
    \item \textbf{Noise Weights:} We evaluate the impact of incorporating noise masks into the \pname loss function. With noise masks, pixel-wise confidence scores scale the focal loss to emphasize high-confidence pixels; without them, the loss reduces to the standard focal loss function \cite{Lin_Goyal_Girshick_He_Dollár_2018}.
    % \item \textbf{Noise Weights:} We evaluate the impact of using the \pname loss function. With noise masks, pixel-wise confidence scores scale the focal loss function, prioritizing high-confidence pixels and reducing the influence of noisy labels. Without noise masks, all pixels are treated equally, potentially amplifying noise effects.
\end{itemize} 

% \textbf{Loss Function Calibration.} Calibration is evaluated by training the model with and without DECE in the loss function. By penalizing discrepancies between predicted probabilities and true outcomes, DECE enforces well-calibrated predictions while also serving as a regularization mechanism, preventing overconfidence and mitigating overfitting to noisy labels. This dual benefit ensures the model maintains accuracy and produces probability estimates that reliably reflect actual contamination risks. We use the Brier score and the ECE score to quantify how the inclusion of DECE in the loss function influences both predictive performance and calibration quality, highlighting its value in high-stakes scenarios such as PFAS contamination risk assessment.

\subsection{Metrics}

The reported metrics include accuracy, Intersection over Union (IoU), F-score, precision, and recall. All the performance metrics in this study are reported exclusively from test set evaluations, using only the contamination points provided by the EPA datasets. These metrics provide a comprehensive evaluation of the model’s performance, where higher values indicate better performance: Accuracy captures overall correctness, IoU quantifies the overlap between predicted and ground truth contamination and is typically used for segmentation tasks, F-score balances the trade-off between precision and recall, precision indicates the model’s ability to correctly identify contamination without overestimating it (minimizing false positives), and recall measures its capacity to detect most contamination sites (minimizing false negatives). 
In addition, we assess timing efficiency as a metric to evaluate the model's computational scalability. Together, these metrics provide a comprehensive evaluation of our model's predictive effectiveness, spatial precision, and practical utility.
%Together, they assess the model’s effectiveness in predicting PFAS contamination while addressing class imbalance and spatial precision.

\section{Results}\label{sec:results}
All results in this section reflect performance on the test set, where evaluations are conducted solely on the actual contamination points from the EPA datasets.

We compared \pname against six alternative approaches for PFAS contamination prediction as shown in Figure \ref{fig:your-image-label}. \textbf{Overall, \pname achieves better performance across all the years and most metrics}, aside from accuracy and precision in 2022. The Landsat-based method yielded  moderate results, suggesting that geospatial data products are more effective for capturing PFAS contamination patterns than relying solely on raw satellite data. The Random Forest approach, despite achieving acceptable accuracy, suffered from  lower F-score, precision, and recall, likely due to the limited spatial context from per-point feature aggregation. Similarly, although Kriging provided decent accuracy and precision, it struggled with recall, indicating difficulties in detecting all instances of contamination, and pollutant transport simulation demonstrated weak overall performance. The FESTA loss approach, originally tested on datasets with mixed spatial formats (e.g., incomplete lines, points, and polygons), showed suboptimal performance in our experiments as well. This may be attributed to our dataset’s exclusive use of point data and the complex dynamics of PFAS contamination, which may challenge the spatial continuity assumptions of the FESTA loss. 
%The FESTA loss method performed poorly as well, likely because our dataset consists exclusively of point data rather than the mixed spatial formats (e.g., incomplete lines and polygons) for which FESTA was used, and because PFAS contamination exhibits complex dynamics where the spatial continuity assumptions underlying FESTA do not hold. 
While rule-based masks, designed to approximate contamination using domain-specific heuristics, can provide a reasonable estimate in some cases, they fall short in consistency and accuracy compared to the data-driven approach. This finding underscores the importance of incorporating domain expertise and AI for more reliable PFAS contamination prediction. 

%These comparisons underscore the potential of our advanced geospatial DL framework, which leverages rich contextual data such as NLCD, flow direction, and discharger proximity to more robustly capture PFAS contamination dynamics. 
% In contrast, the Landsat-based method using Landsat 7 multispectral data \cite{USGSLandsat7} with the original Prithvi weights yielded only moderate results, suggesting that PFAS contamination patterns are not strongly correlated with spectral signatures alone. Similarly, while Kriging achieved respectable accuracy and precision, it struggled with recall indicating difficulties in detecting all instances of contamination, and pollutant transport simulation demonstrated relatively weak performance, implying a need for further calibration. 
% %We also compared our model’s performance with that of a Random Forest approach following the methodology of \cite{DeLuca_Mullikin_Brumm_2023}, which employs similar environmental features and uses the NRSA dataset. 
% Although the Random Forest approach achieved respectable accuracy in some cases, its F-score, precision, and recall were considerably lower, likely due to the reduced spatial context inherent in per-point feature aggregation. %Additionally, we evaluated performance using the FESTA loss proposed in \cite{Hua_Marcos_Mou_Zhu_Tuia_2022} as an alternative state-of-the-art technique for sparse segmentation. 
% Lastly, the FESTA loss consistently yielded poor performance across all metrics, likely because our dataset consists exclusively of true point data rather than the mixed spatial formats (e.g., incomplete lines and polygons) for which FESTA was used. Furthermore, PFAS contamination is inherently complex. Consequently, the spatial continuity assumptions underlying FESTA do not hold, resulting in consistently poor performance across all metrics. Kriging and Random Forest predictions were evaluated using accuracy, precision, recall, and F-score. IoU was not applicable due to the point-based evaluation setup as opposed to masks.

% These findings underscore the value of advanced geospatial DL techniques, complemented by additional contextual data like the NLCD, flow direction, and contamination-related industrial datasets. Unlike Kriging, Landsat-based, Random Forest, FESTA loss and pollutant transport simulation methods that may not fully capture complex PFAS contamination dynamics, our model offers a more robust representation of pollutant spread. Further details on Kriging and pollutant transport simulation, as well as our model calibration as measured by the Expected Calibration Error (ECE) \cite{Guo_Pleiss_Sun_Weinberger_2017}  can be found in the supplementary material. 

To further illustrate the model's performance, we present a predicted contamination map using our model for a sample image in Figure \ref{fig:test2}. For this patch, real data was available only for the central pixel, corresponding to the location of an EPA sampling point. While the model correctly predicts contamination at this central point, its predictions for other surface water regions within the patch remain unverified due to no corresponding ground truth data. Further results on the model calibration as measured by the Expected Calibration Error \cite{Guo_Pleiss_Sun_Weinberger_2017} are provided in the supplementary material.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\columnwidth]{illus.png}
  \caption{Left: Model-predicted contamination map for a 2019 patch; Right: Prediction overlaid on the U.S. map. }
  \label{fig:test2}
\end{figure}

\subsection{Ablation Results}
\begin{table}[ht!]
  \centering
  \resizebox{\columnwidth}{!}{  % Resizes the table to fit within the column width
    \small  % Adjust font size for better readability
    \setlength{\tabcolsep}{5pt}  % Reduce space between columns
    \renewcommand{\arraystretch}{1.3}  % Adjust row height for better readability
    \begin{tabular}{l c c c c c c c c c c}
      \toprule
      & \multicolumn{3}{c}{\textbf{2008 (\%)}} & \multicolumn{3}{c}{\textbf{2019 (\%)}} & \multicolumn{3}{c}{\textbf{2022 (\%)}} \\
      \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
      \textbf{Metric} & \textbf{256} & \textbf{512} & \textbf{\pname}  & \textbf{256} & \textbf{512} & \textbf{\pname}  & \textbf{256} & \textbf{512} & \textbf{\pname} \\
      \midrule
      Accuracy        & 84 & 75 & \textbf{88} & 87 & 85 & \textbf{89}  & 78 & 78 & \textbf{89} \\
      IoU             & 61 & 52 & \textbf{65} & 68 & 66 & \textbf{71} & 57 & 56 & \textbf{69} \\
      F-score         & 73 & 67 & \textbf{77} & 80 & 78 & \textbf{82} & 71 & 71 & \textbf{80} \\
      Precision       & 75 & 66 & \textbf{82} & 77 & 76 & \textbf{80} & 70 & 70 & \textbf{78} \\
      Recall        & 71 & 72 & \textbf{73} & 84 & 83 & \textbf{85} & \textbf{87} & 84 & 83 \\
      \bottomrule
    \end{tabular}
  }
  \captionsetup{justification=centering, position=bottom}  % Caption at the bottom
  \caption{Comparison of performance across three approaches—\(256 \times 256\) pixels resolution,  \(512 \times 512\) pixels resolution, and FOCUS—over different years; bold values denote the best metric for each year.}
  \label{tab:performance_comparison_with_512_and_focus}
\end{table}


Table~\ref{tab:performance_comparison_with_512_and_focus} presents a comparison of three configurations: (i) point-based ground truth expansion masks using \(256 \times 256\) pixels patches, (ii) point-based ground truth expansion masks using \(512 \times 512\) pixels patches, and (iii) our \pname approach. The first two configurations employ standard focal loss, while \pname leverages noise masks to emphasize high-confidence pixels. 
% Overall, our experiments show that the point-based ground truth expansion strategy outperforms the rule-based approach across all metrics and years. Our primary aim in evaluating rule-based masks was to determine whether domain-specific heuristics could match the performance of labels derived directly from contamination measurements. The results clearly indicate that while the rule-based method can offer a reasonable approximation in some settings, it falls short in consistency and accuracy compared to the data-driven approach. This underscores the importance of incorporating actual ground truth data into the labeling process for more reliable PFAS contamination prediction. 

% Overall, our experiments demonstrate that the point-based ground truth expansion strategy outperforms the rule-based approach across all metrics and years. While rule-based masks, designed to approximate contamination using domain-specific heuristics, can provide a reasonable estimate in some cases, they fall short in consistency and accuracy compared to the data-driven approach. This finding underscores the importance of incorporating actual ground truth data for more reliable PFAS contamination prediction. 

% We also see that \(256 \times 256\) patches generally outperform or match \(512 \times 512\) patches. In 2008, \(256 \times 256\) performs better across all metrics; in 2022, it achieves higher IoU and recall while other metrics remain similar; and in 2019, \(256 \times 256\) is superior overall, with \(512 \times 512\) only marginally better in recall. These findings indicate that smaller patches can relatively better capture localized contamination patterns likely by reducing noise while preserving essential spatial context, making \(256 \times 256\) a more reliable choice for overall predictive stability.
% We also see that \(256 \times 256\) patches generally yield equal or better performance than \(512 \times 512\) patches. These results suggest that smaller patches better capture localized contamination patterns by reducing noise while preserving essential spatial context. 
% Our choice of \(256 \times 256\) and \(512 \times 512\) patch sizes aligns with established practices in geospatial DL, evidenced by applications in wildfire scar mapping, flood mapping, and crop segmentation within the Prithvi framework. Although \(512 \times 512\) patches capture broad spatial context, overly small patches can reduce accuracy and increase variability \cite{Lechner_Stein_Jones_Ferwerda_2009}. These findings justify our selection of these resolutions for our ablation study. 
We observe that \(256 \times 256\) patches generally perform as well as or better than \(512 \times 512\) patches, likely because the smaller patch size reduces noise while preserving key spatial context. We did not evaluate patch sizes smaller than \(256 \times 256\), consistent with literature indicating that excessively small patches can compromise accuracy and increase variability \cite{Lechner_Stein_Jones_Ferwerda_2009}.
%This observation is consistent with geospatial DL practices seen in applications such as wildfire scar mapping, flood mapping, and crop segmentation within the Prithvi framework, as well as with literature noting that overly small patches can lead to accuracy loss and increased variability \cite{Lechner_Stein_Jones_Ferwerda_2009}. These findings support our choice of \(256 \times 256\) and \(512 \times 512\) resolutions for our ablation study. 
% The decision to use \(256 \times 256\) and \(512 \times 512\) patch sizes follows established practices in geospatial DL, as seen in tasks like wildfire scar mapping, flood mapping, and crop segmentation within the Prithvi framework. While larger patch sizes like \(512 \times 512\) is standard for capturing broad spatial context, too small patches can reduce accuracy and increase variability due to loss of context and sensitivity to grid positioning \cite{Lechner_Stein_Jones_Ferwerda_2009}. These findings support our choice of \(256 \times 256\) and \(512 \times 512\) as optimal resolutions for our ablation study.

% \begin{figure}[ht!]
% \centering
% \begin{subfigure}{.5\columnwidth}
%   \centering
%   \includegraphics[width=\linewidth]{new10.png}
%   \caption{Model-predicted PFAS contamination map}
%   \label{fig:sub1}
% \end{subfigure}%
% \hfill
% \begin{subfigure}{.48\columnwidth}
%   \centering
%   \includegraphics[width=\linewidth]{down.png}
%   \caption{Prediction overlaid on the U.S. map}
%   \label{fig:sub2}
% \end{subfigure}
% \caption{Model-predicted PFAS contamination map for a 2019 patch. The central pixel, an EPA sampling point, confirms model accuracy at that location; however, predictions in other areas remain unverified, highlighting the need for additional field sampling to improve reliability and generaliziblity.}
% \label{fig:test2}
% \end{figure}
% \begin{figure}[ht!]
%   \centering
%   \includegraphics[width=\columnwidth]{illus.png}
%   \caption{Left: Model-predicted contamination map for a 2019 patch; Right: Prediction overlaid on the U.S. map. }
%   \label{fig:test2}
% \end{figure}
\pname extends standard focal loss by using noise masks to emphasize high-confidence pixels. As shown in Table~\ref{tab:performance_comparison_with_512_and_focus}, \pname consistently improves accuracy, IoU, F-score, and precision across all years. In 2008 and 2019, recall increases, whereas in 2022 it slightly decreases, indicating that \pname may sometimes favor precision over sensitivity. Overall, these results confirm that incorporating pixel-wise confidence through noise-based weighting enhances segmentation quality and model robustness. 
%Accuracy also drops marginally in 2019 suggesting that while \pname enhances segmentation quality and sensitivity to contamination, it may lead to minor trade-offs in overall classification accuracy. 

% \pname extends standard focal loss by leveraging noise masks to emphasize high-confidence pixels, thereby reducing the influence of noisy labels. As shown in Table~\ref{tab:performance_comparison_with_512_and_focus}, \pname consistently improves IoU, F-score, and precision across all years. In 2019, recall significantly increases, while in 2022 it decreases suggesting that \pname may prioritize precision sometimes at the expense of some sensitivity; accuracy also drops marginally in 2019 suggesting that while \pname enhances segmentation quality and sensitivity to contamination, it may lead to minor trade-offs in overall classification accuracy. Overall, these results demonstrate that incorporating pixel-wise confidence via noise-based weighting mostly enhances segmentation quality and model robustness.

% To further illustrate the model's performance, we present a predicted contamination map using our best performing model for a sample image in Figure \ref{fig:test2}. For this patch, real data was available only for the central pixel, corresponding to the location of an EPA sampling point. While the model correctly predicts contamination at this central point, its predictions for other surface water regions within the patch remain unverified due to no corresponding ground truth data.

% \paragraph{Feature Relevance and Domain Expert Alignment.}
% Our model's feature importance analysis strongly aligns with domain expert expectations as well regarding the key drivers of PFAS contamination. For additional details and quantitative results on the feature ranking, please refer to the supplementary material.


\subsection{Time Efficiency}
\begin{table}[ht!]
  \centering
  \resizebox{\columnwidth}{!}{  % Resizes the table to fit within the column width
    \small  % Adjust font size for better readability
    \setlength{\tabcolsep}{5pt}  % Adjust space between columns
    \renewcommand{\arraystretch}{1.3}  % Adjust row height for better readability
    \begin{tabular}{l c c c c c c}
      \toprule
      & \multicolumn{2}{c}{\textbf{Feature extraction}} & \multicolumn{2}{c}{\textbf{Inference}} \\
      \cmidrule(lr){2-3}\cmidrule(lr){4-5}
      & \textbf{ML} & \textbf{DL} & \textbf{ML} & \textbf{DL} \\
      \midrule
      For 1 km\(^2\) area & 1.2 mins & 4.7 sec & 0.0003 sec & 0.0005 sec \\
      For NM & 2 days & 3.2 hrs & 7.5 sec & 13 sec \\
      \bottomrule
    \end{tabular}
  }
  \captionsetup{justification=centering, position=bottom}  % Caption at the bottom
  \caption{Comparison of performance for feature extraction and inference using Random Forest and \pname over 1 km\(^2\) area and Northern Michigan (NM).}
  \label{tab:performance_comparison_ml_dl}
\end{table}

We followed the methodology of DeLuca et al. \cite{DeLuca_Mullikin_Brumm_2023}, which uses similar environmental features and the NRSA dataset with a Random Forest model, as a baseline for efficiency comparisons. Table~\ref{tab:performance_comparison_ml_dl} compares feature extraction and inference times between the ML method and our \pname approach over a 1 km\(^2\) area and in Northern Michigan (NM).
% We followed the methodology of DeLuca et al. \cite{DeLuca_Mullikin_Brumm_2023} since they used similar environmental features as well as the NRSA dataset with a Random Forest model, to serve as a baseline for efficiency comparisons. Our goal is to demonstrate that our deep learning approach is significantly more efficient for large-scale PFAS contamination prediction than traditional ML methods. Table~\ref{tab:performance_comparison_ml_dl} compares feature extraction and inference times between the Random Forest-based method and our \pname approach over a 1 km\(^2\) area and a larger region in Northern Michigan (NM).

For feature extraction, we aggregated environmental features within a 1 km buffer around each point for the ML method. On average, this per-point aggregation took about 1.2 minutes. In contrast, our deep learning approach processes the entire 1 km\(^2\) patch, including the creation and stacking of individual raster channels, taking only 4.7 seconds on average. Over NM, ML extraction took about 2 days compared to 3.2 hours for our \pname approach.

For inference, the ML method took 0.0003 seconds per point (7.5 seconds for NM), while our \pname approach required 0.0005 seconds per patch (13 seconds for NM). Although \pname inference is marginally slower, this trade-off is acceptable given the efficiency gains in feature extraction and the advantage of producing dense, spatially contextualized predictions. All reported times for feature extraction over the 1 km\(^2\) area and inference were averaged over 10 runs. Feature extraction for both methods was performed on an Intel Xeon Gold 6154 processor (3.0 GHz), while inference was conducted on an AMD Ryzen Threadripper PRO 7985WX processor, with an NVIDIA RTX 6000 Ada Generation GPU.

Overall, these results demonstrate that our deep learning framework offers a scalable and efficient solution for PFAS contamination mapping.

% \subsection{Feature Relevance and Domain Expert Alignment}

% Our features integrate key environmental variables that align with domain expert insights. The NLCD channel provides detailed land cover information, while distance rasters capture proximity to PFAS dischargers using data from the U.S. EPA’s ECHO, which offers location-specific details on PFAS manufacture, release, and facilities handling these chemicals. In addition, the flow direction raster captures hydrological connectivity, a critical factor in contaminant transport. Together, these features provide the robust spatial context necessary for accurate PFAS contamination prediction. 


% Table~\ref{tab:performance_comparison} presents the performance metrics for models trained with the two different labeling strategies; rule-based masks and the point-based ground truth expansion masks, across three different years (2008, 2019, and 2022) using the focal loss function.  
% \begin{table}[ht!]
%  % \vspace{1em} 
%   \centering
%   \small  % Adjust the font size for better readability
%   \setlength{\tabcolsep}{5pt}  % Increase space between columns
%   \renewcommand{\arraystretch}{1.3}  % Adjust row height for better readability
%   \begin{tabular}{l c c c c c c}
%     \toprule
%     & \multicolumn{2}{c}{\textbf{2008}} & \multicolumn{2}{c}{\textbf{2019}} & \multicolumn{2}{c}{\textbf{2022}} \\
%     \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
%     \textbf{Metric} & \textbf{Rule} & \textbf{Surface} & \textbf{Rule} & \textbf{Surface} & \textbf{Rule} & \textbf{Surface} \\
%     \midrule
%     Accuracy       & 75 & \textbf{84} & 63 & \textbf{87} & 52 & \textbf{78} \\
%     IoU       & 51 & \textbf{61} & 40 & \textbf{68} & 33 & \textbf{57} \\
%     F-score    & 65 & \textbf{73} & 54 & \textbf{80} & 49 & \textbf{71} \\
%     Precision & 64 & \textbf{75} & 57 & \textbf{77} & 58 & \textbf{70} \\
%     Recall    & 65 & \textbf{71} & 63 & \textbf{84} & 65 & \textbf{87} \\
%     \bottomrule
%   \end{tabular}
%   \captionsetup{justification=centering, position=bottom}  % Caption at the bottom
%   \caption{Comparison of performance between rule-based masks and surface water masks across different years.}
%   \label{tab:performance_comparison}
% \end{table}
% Overall, our experiments show that the point-based ground truth expansion strategy outperforms the rule-based approach across all metrics and years. Our primary aim in evaluating rule-based masks was to determine whether domain-specific heuristics could match the performance of labels derived directly from contamination measurements. The results clearly indicate that while the rule-based method can offer a reasonable approximation in some settings, it falls short in consistency and accuracy compared to the data-driven approach. This underscores the importance of incorporating actual ground truth data into the labeling process for more reliable PFAS contamination prediction.
% \begin{table}[ht!]
% %  \vspace{1em} 
%   \centering
%   \small  % Adjust the font size for better readability
%   \setlength{\tabcolsep}{5pt}  % Increase space between columns
%   \renewcommand{\arraystretch}{1.3}  % Adjust row height for better readability
%   \begin{tabular}{l c c c c c c}
%     \toprule
%     & \multicolumn{2}{c}{\textbf{2008 (\%)}} & \multicolumn{2}{c}{\textbf{2019 (\%)}} & \multicolumn{2}{c}{\textbf{2022 (\%)}} \\
%     \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
%     \textbf{Metric} & \textbf{256} & \textbf{512} & \textbf{256} & \textbf{512} & \textbf{256} & \textbf{512} \\
%     \midrule
%     Accuracy       & \textbf{84} & 75 & \textbf{87} & 85 & \textbf{78} & \textbf{78} \\
%     IoU       & \textbf{61} & 52 & \textbf{68} & 66 & \textbf{57} & 56 \\
%     F-score    & \textbf{73} & 67 & \textbf{80} & 78 & \textbf{71} & \textbf{71} \\
%     Precision & \textbf{75} & 66 & \textbf{77} & 76 & \textbf{70} & \textbf{70} \\
%     Recall    & \textbf{71} & 72 & 78 & \textbf{83} & \textbf{87} & 84 \\
%     \bottomrule
%   \end{tabular}
%   \captionsetup{justification=centering, position=bottom}  % Caption at the bottom
%   \caption{Comparison of performance between \(256 \times 256\) pixels and \(512 \times 512\) pixels resolutions of surface water masks across years.}
%   \label{tab:performance_comparison_resolutions} 
% \end{table}
% Based on Table~\ref{tab:performance_comparison_resolutions}, \(256 \times 256\) patches generally outperform or match \(512 \times 512\) patches. In 2008, \(256 \times 256\) performs better across all metrics; in 2022, it achieves higher IoU and recall while other metrics remain similar; and in 2019, \(256 \times 256\) is superior overall, with \(512 \times 512\) only marginally better in recall. These findings indicate that smaller patches can relatively better capture localized contamination patterns likely by reducing noise while preserving essential spatial context, making \(256 \times 256\) a more reliable choice for overall predictive stability.
% The decision to use \(256 \times 256\) and \(512 \times 512\) patch sizes follows established practices in geospatial DL, as seen in tasks like wildfire scar mapping, flood mapping, and crop segmentation within the Prithvi framework. While larger patch sizes like \(512 \times 512\) is standard for capturing broad spatial context, too small patches can reduce accuracy and increase variability due to loss of context and sensitivity to grid positioning \cite{Lechner_Stein_Jones_Ferwerda_2009}. These findings support our choice of \(256 \times 256\) and \(512 \times 512\) as optimal resolutions for our ablation study.
% \begin{table}[ht!]
%   \caption{Comparison of performance between \(256 \times 256\) pixels and \(512 \times 512\) pixels resolutions of surface water masks across years.}
%   \label{tab:performance_comparison_resolutions}
%   \vspace{1em} % Add a bit of vertical space before the table
%   \centering
%   \scriptsize % Use a smaller font size to fit the table better
%   \resizebox{\columnwidth}{!}{%
%     \begin{tabular}{l c c c c c c}
%       \toprule
%       & \multicolumn{2}{c}{\textbf{2008 (\%)}} & \multicolumn{2}{c}{\textbf{2019 (\%)}} & \multicolumn{2}{c}{\textbf{2022 (\%)}} \\
%       \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
%       \textbf{Metric} & \textbf{256} & \textbf{512} & \textbf{256} & \textbf{512} & \textbf{256} & \textbf{512} \\
%       \midrule
%       acc       & \textbf{87} & 86 & \textbf{79} & \textbf{79} & \textbf{87} & \textbf{87} \\
%       IoU       & \textbf{69} & 61 & 49 & \textbf{53} & \textbf{63} & \textbf{63} \\
%       fscore    & \textbf{80} & 73 & 62 & \textbf{63} & \textbf{75} & \textbf{75} \\
%       precision & \textbf{85} & 71 & 55 & \textbf{61} & \textbf{71} & \textbf{71} \\
%       recall    & \textbf{76} & 75 & \textbf{73} & 70 & \textbf{85} & 83 \\
%       \bottomrule
%     \end{tabular}
%   }
%   \vspace{1em} % Add a bit of vertical space after the table
% \end{table}
% \begin{table}[ht!]
%  % \vspace{1em} 
%   \centering
%   \small  % Adjust the font size for better visibility (larger than \scriptsize)
%   \setlength{\tabcolsep}{5pt}  % Increase space between columns
%   \renewcommand{\arraystretch}{1.3}  % Adjust row height for better readability
%   \begin{tabular}{l c c c c c c}
%     \toprule
%     & \multicolumn{2}{c}{\textbf{2008 (\%)}} & \multicolumn{2}{c}{\textbf{2019 (\%)}} & \multicolumn{2}{c}{\textbf{2022 (\%)}} \\
%     \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
%     \textbf{Metric} & \textbf{FL} & \textbf{\pname} & \textbf{FL} & \textbf{\pname} & \textbf{FL} & \textbf{\pname} \\
%     \midrule
%     Accuracy       & 84 & \textbf{88} & \textbf{87} & 84 & 78 & \textbf{87} \\
%     IoU       & 61 & \textbf{65} & 68 & \textbf{69} & 57 & \textbf{64} \\
%     F-score    & 73 & \textbf{77} & 80 & \textbf{81} & 71 & \textbf{76} \\
%     Precision & 75 & \textbf{82} & 77 & \textbf{80} & 70 & \textbf{74} \\
%     Recall    & 71 & \textbf{73} & 78 & \textbf{86} & \textbf{87} & 80 \\
%     \bottomrule
%   \end{tabular}
%   \captionsetup{justification=centering, position=bottom}  % Caption at the bottom
%   \caption{Comparison of performance between Focal Loss (FL) and \pname across different years.}
%   \label{tab:performance_comparison_weighted_focal} 
% \end{table}
% \begin{figure*}[ht!]
%   \centering
%   \includegraphics[width=0.8\textwidth]{abl.png}  
%   \caption{Comparison of performance across different methods and years. PT simulation  refers to the pollutant transport simulation}
%   \label{fig:your-image-label}
% \end{figure*}

% \pname extends standard focal loss by leveraging noise masks to emphasize high-confidence pixels, thereby reducing the influence of noisy labels. As shown in Table~\ref{tab:performance_comparison_weighted_focal}, \pname consistently improves IoU, F-score, and precision across all years. In 2019, recall significantly increases, while in 2022 it decreases suggesting that \pname may prioritize precision sometimes at the expense of some sensitivity; accuracy also drops marginally in 2019 suggesting that while \pname enhances segmentation quality and sensitivity to contamination, it may lead to minor trade-offs in overall classification accuracy. Overall, these results demonstrate that incorporating pixel-wise confidence via noise-based weighting mostly enhances segmentation quality and model robustness.

%This highlights a critical gap in validating the model's predictions across broader spatial areas.
% \begin{figure}[ht!]
%   \centering
%   \includegraphics[width=\columnwidth]{down1.png}
%   \caption{(1) Model-predicted PFAS contamination map for a 2019 image patch and (2) the prediction overlaid on a map. The images are arranged from left to right. The central pixel in this image, corresponding to an EPA sampling point with ground truth data, confirms the model's accuracy at this location. Predictions for other surface water regions within the patch remain unverified, underscoring the need for future field sampling to validate these predictions. This would not only validate the model's predictions but also enhance its reliability and generalizability.}
%   \label{fig:your-image-label10}
% \end{figure}

% Lastly, we compared our best-performing model against three alternative approaches for PFAS contamination prediction as shown in Fig \ref{fig:your-image-label}. Our geospatial DL method achieves mostly better performance across 2008, 2019, and 2022 in terms of accuracy, IoU, recall, precision and F-score. In contrast, the Landsat-based method using Landsat 7 multispectral data with the original Prithvi weights yielded only moderate results, suggesting that PFAS contamination patterns are not strongly correlated with spectral signatures alone. Similarly, while Kriging achieved respectable accuracy and precision, it struggled with recall indicating difficulties in detecting all instances of contamination, and pollutant transport simulation demonstrated relatively weak performance, implying a need for further calibration. Kriging predictions were evaluated using accuracy, precision, recall, and F-score. IoU was not applicable due to the point-based evaluation setup as opposed to masks.

% These findings underscore the value of advanced geospatial DL techniques, complemented by additional contextual data like the NLCD, flow direction, and contamination-related industrial datasets. Unlike Kriging, which depends on spatial interpolation, or Landsat-based and pollutant transport simulation methods that may not fully capture complex PFAS contamination dynamics, our model offers a more robust representation of pollutant spread. Further details on Kriging and pollutant transport simulation, as well as our model calibration as measured by the Expected Calibration Error (ECE) \cite{Guo_Pleiss_Sun_Weinberger_2017}  can be found in the supplementary material. 
%\vspace{-0.6em}

\section{Conclusion and Future Work}
PFAS contamination is a global environmental issue due to its persistence in water, soil, and wildlife. While methods like LC-MS offer high accuracy, they are expensive and slow. Alternative approaches, such as spatial interpolation, remote sensing, ML methods, and hydrological models, provide insights but struggle with feature engineering and loss of spatial context.

Our geospatial DL framework, incorporating the novel \pname loss, directly processes multi-channel raster data and integrates contextual information, improving detection accuracy and preserving spatial dependencies, making it a promising tool for large-scale PFAS contamination mapping that can help identify high-risk areas.

% Future work will explore generative models for synthesizing data in under-sampled regions, alternative noise weight configurations, and model uncertainty estimation to guide targeted sampling. These developments could support informed decision-making for remediation and policy efforts.
Future work will explore %generative models to synthesize data in under-sampled regions, alternative noise weight configurations, and 
model uncertainty estimation to guide targeted sampling. This would enhance the robustness and reliability of our predictions. We plan to build on publicly available PFAS contamination maps, such as those produced by the Environmental Working Group \cite{EWG} and \cite{PFAS_tap}, by deploying our AI-based framework responsibly.


\section*{Ethical Statement}
%\vspace{-0.2em}
The public dissemination of PFAS contamination maps carries significant ethical implications. Given the potential impact on public health,  environmental justice, and even property values, it is essential to communicate our findings transparently and responsibly. Our interdisciplinary team met biweekly to discuss findings and collectively decide on next steps, ensuring that our approach remains well-informed and grounded in both technical rigor and community needs. Our framework is well-suited to also support the generation of visualizations that can reflect prediction uncertainty, thereby helping decision-makers and impacted communities make informed choices. 
%Our framework not only aims to predict contamination,  but also to convey the inherent uncertainty of these predictions through visualizations, ensuring that decision-makers and impacted communities are fully informed.
Prior to deployment, we plan to engage local stakeholders and community groups via public forums and workshops to tailor our maps to community needs. This approach facilitates responsible use of the data and predictions in guiding remediation efforts and policy decisions. 

% PFAS contamination remains one of the most critical environmental issues globally due to its persistence in water, soil, and wildlife. Although laboratory methods such as LC-MS/MS offer high accuracy, they are costly, slow, and challenging to scale for large areas. Spatial interpolation (e.g., Kriging) and remote sensing (e.g., Landsat-based approaches) can serve as alternatives but face limitations in capturing the dynamic and complex nature of PFAS contamination, especially considering the added data sparsity challenge. Moreover, existing process-based hydrological models lack PFAS-specific transport modules, and traditional ML techniques require extensive computation while struggling to capture inherent spatial relationships. These limitations highlight the need for novel, scalable methods to accurately model the spread of PFAS.

% PFAS contamination is a critical environmental issue globally due to its persistence in water, soil, and wildlife. Although laboratory methods like LC-MS/MS offer high accuracy, they are expensive, slow, and difficult to scale. Alternative approaches such as spatial interpolation (e.g., Kriging), remote sensing methods (e.g., Landsat-based techniques),  ML methods, and process-based hydrological models provide valuable insights but often struggle to capture the complex dynamics of PFAS spread due to intensive feature engineering and the loss of spatial context. 

% Our proposed geospatial deep learning framework, which incorporates the novel \pname loss, addresses these challenges by directly processing multi-channel raster data and integrating contextual information. This approach not only improves detection accuracy but also better preserves spatial dependencies, making it a promising tool for large-scale PFAS contamination mapping that can help identify areas at high risk for PFAS contamination.


% Our proposed approach not only improves detection accuracy but also offers a more practical approach to PFAS contamination mapping at larger scales. This has significant real-world implications, especially for environmental monitoring and decision-making. By making use of geospatial DL, our framework offers a cost-effective and scalable solution that can help identify areas at high risk for PFAS contamination. Furthermore, the integration of additional contextual data such as proximity to PFAS dischargers and land cover enables a more comprehensive understanding of PFAS spread, paving the way for better-targeted mitigation and policy efforts.

% Future work will explore the use of generative models to synthesize environmental data in under-sampled regions and investigate alternative noise weight configurations, as well as model uncertainty estimation to guide targeted field sampling efforts. These promising developments have the potential to support more informed decision-making for targeted remediation and policy efforts, ultimately contributing to improved environmental and public health outcomes.

% Future studies could expand on this framework by leveraging generative models to synthesize realistic environmental data in under-sampled regions, thereby enhancing prediction accuracy and robustness. Additionally, exploring alternative noise weight configurations by adjusting the contributions of discharger proximity, land cover, downstream flow, and sample point distance may further optimize label confidence and overall model performance. Furthermore, incorporating model uncertainty estimation could guide targeted field sampling efforts to further validate and refine the model.
%Additionally, further advancements in integrating diverse data sources such as more comprehensive industrial activity datasets and climate data could enhance the model’s ability to predict PFAS contamination dynamics with even greater precision.
%Finally, the exploration of model uncertainty in conjunction with label uncertainty could provide a more comprehensive understanding of prediction reliability. Incorporating techniques to quantify model uncertainty would enable better characterization of areas where predictions may be less trustworthy, thereby supporting more informed decision-making in high-stakes environmental management scenarios. 

% \begin{figure}[ht!]
%   \centering
%   \includegraphics[width=0.9\textwidth]{Picture2.png}  % Make sure the image spans the whole page width
%   \caption{Challenges in generating PFAS contamination maps: limited sampling, data gaps, and infrequent updates}
%   \label{fig:your-image-label}
% \end{figure}










% \begin{table}[ht!]
%   \caption{Comparison of performance across different methods for various years.}
%   \label{tab:performance_comparison_methods}
%   \vspace{1em} % Add a bit of vertical space before the table
%   \centering
%   \scriptsize % Use a smaller font size for better fitting
%   \setlength{\tabcolsep}{3pt} % Further reduce space between columns for a more compact table
%   \renewcommand{\arraystretch}{1.1} % Increase row height for better readability
%   \begin{tabular}{l c c c c c c c c c c c c}
%     \toprule
%     & \multicolumn{4}{c}{\textbf{2008 (\%)}} & \multicolumn{4}{c}{\textbf{2019 (\%)}} & \multicolumn{4}{c}{\textbf{2022 (\%)}} \\
%     \cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
%     \textbf{Metric} & \textbf{SWAT Flow} & \textbf{Sentinel based} & \textbf{Kriging} & \textbf{HM simulation} & \textbf{SWAT Flow} & \textbf{Sentinel based} & \textbf{Kriging} & \textbf{HM simulation} & \textbf{SWAT Flow} & \textbf{Sentinel based} & \textbf{Kriging} & \textbf{HM simulation} \\
%     \midrule
%     acc       & 93 & 71 & 81 & 47 & 86 & 50 & 92 & 39 & 88 & 72 & 87 & 50 \\
%     IoU       & 76 & 36 & - & 47 & 59 & 21 & - & 39 & 69 & 45 & - & 51\\
%     fscore    & 85 & 42 & 58 & 48 & 71 & 44 & 60 & 40 & 80 & 58 & 47 & 51\\
%     precision & 96 & 41 & 60 & 49 & 67 & 40 & 72 & 41 & 78 & 58 & 94 & 52\\
%     recall    & 80 & 42 & 57 & 47 & 85 & 35 & 58 & 39 & 84 & 67 & 50 & 50\\
%     \bottomrule
%   \end{tabular}
%   \vspace{1em} % Add a bit of vertical space after the table
% \end{table}

\bibliographystyle{named}
\bibliography{ijcai25}
\clearpage
%\end{document}
\section*{Supplementary Material}

\subsection*{A\hspace{1em}Dataset}

The dataset was curated to ensure that patches included in the training and testing sets were geographically disjoint, avoiding any overlap that could compromise the evaluation. Specifically, if multiple sample points were situated within the same patch, we selected a single patch containing all these points rather than creating multiple patches for each individual sample point. This avoided redundancy by reducing unnecessary duplication of spatial information. An illustration of this process is provided in Figure~\ref{fig:redundant_patch_removal}.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=\columnwidth]{multiple.png}
  \caption{Example of patch selection for multiple sample points. The figure shows a single patch (outlined) overlaid on the U.S. map, with red points representing sample locations. Since all sample points are contained within the same patch, only this patch is added to the dataset, avoiding the creation of redundant patches centered around individual points.}
  \label{fig:redundant_patch_removal}
\end{figure}


In cases where two or more patches overlapped, as illustrated in Figure~\ref{fig:overlapping_patch_assignment}, all such patches were assigned to the same set (either training or testing). For example, if two patches partially shared the same geographic area, both patches were allocated to the training set or both to the testing set. This ensured that no part of a test patch had been seen during training, preserving the integrity of the evaluation

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\columnwidth]{multiple1.png}
  \caption{Example of overlapping patch assignment. The figure illustrates two slightly overlapping patches overlaid on the U.S. map, each containing distinct sample points. To maintain disjoint training and testing sets, both patches are assigned to the same set (either training or testing), ensuring no part of a test patch overlaps with any patch seen during training.}
  \label{fig:overlapping_patch_assignment}
\end{figure}


\subsection*{B\hspace{1em}Additional Results}
We initially considered a conventional 70\%-10\%-20\% split for training, validation, and testing, respectively, to facilitate intermediate performance monitoring. However, due to the limited availability of labeled data and a severe class imbalance with the minority class being particularly scarce, setting aside a separate validation set in order to guide the training process not only reduced the effective training size but also resulted in a highly skewed validation set. Through extensive empirical testing, we observed that a 80\%-20\% split for training and testing sets, respectively, significantly improved performance on the test set. This approach maximizes the data available for training while maintaining a rigorous, geographically disjoint evaluation on an independent test set.
To assess the impact of allocating a separate validation set, we compared the two dataset splits for training our model with focal loss: The conventional 70\%-10\%-20\% split (training, validation, testing) versus the 80\%-20\% split (training and testing only). In our experiments here, we used only the standard focal loss without the additional noise-aware modifications to isolate the effect of the split on performance. Table \ref{tab:performance_comparison_val} summarizes the performance metrics for each year under the two configurations.

\begin{table}[ht!]
%  \vspace{1em} 
  \centering
  \small  % Adjust the font size for better readability
  \setlength{\tabcolsep}{5pt}  % Increase space between columns
  \renewcommand{\arraystretch}{1.3}  % Adjust row height for better readability
  \begin{tabular}{l c c c c c c c c c c}
    \toprule
    & \multicolumn{2}{c}{\textbf{2008 (\%)}} & \multicolumn{2}{c}{\textbf{2019 (\%)}} & \multicolumn{2}{c}{\textbf{2022 (\%)}} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
    \textbf{Metric} & \textbf{Val} & \textbf{W/o val} & \textbf{Val} & \textbf{W/o val} & \textbf{Val} & \textbf{W/o val} \\
    \midrule
    Accuracy       & 72 & \textbf{84} & 80 & \textbf{87} & 66 & \textbf{78} \\
    IoU            & 45 & \textbf{61} & 56 & \textbf{68} & 42 & \textbf{57} \\
    F-score        & 58 & \textbf{73} & 69 & \textbf{80} & 55 & \textbf{71} \\
    Precision      & 59 & \textbf{75} & 68 & \textbf{77} & 57 & \textbf{70} \\
    Recall         & 57 & \textbf{71} & 71 & \textbf{84} & 60 & \textbf{87} \\
    \bottomrule
  \end{tabular}
  \captionsetup{justification=centering, position=bottom}  % Caption at the bottom
  \caption{Comparison of performance between models trained with and without a validation set (using focal loss) across different years.}
  \label{tab:performance_comparison_val} 
\end{table}


Our experimental results indicate that the 80\%-20\% split, which allocates a larger portion of the data for training, consistently improves performance on the test set compared to the conventional split. This finding supports our decision to forgo a separate validation set for this task, given the limited data and severe class imbalance, and underscores the importance of maximizing training data for robust PFAS contamination prediction.



\subsection*{C\hspace{1em}Model Calibration}

We evaluated model calibration using ECE, which quantifies the discrepancy between predicted probabilities and observed outcomes where lower scores indicate better calibration. Table~\ref{tab:ece_scores} shows the ECE scores for our model trained with \pname loss for the years 2008, 2019, and 2022.

\begin{table}[ht!]
%  \vspace{1em} 
  \centering
  \small  
  \setlength{\tabcolsep}{5pt}  
  \renewcommand{\arraystretch}{1.3}  
  \begin{tabular}{l c c c}
    \toprule
    \textbf{Metric} & \textbf{2008} & \textbf{2019} & \textbf{2022} \\
    \midrule
    ECE Score       & 0.1 & 0.1 & 0.07 \\
    \bottomrule
  \end{tabular}
  \captionsetup{justification=centering, position=bottom}  % Caption at the bottom
  \caption{ECE Scores for Models Trained with \pname Loss Across Different Years.}
  \label{tab:ece_scores} 
\end{table}

The very low ECE values, that is, 0.1 for 2008 and 2019, and 0.07  for 2022, demonstrate that our model’s predicted probabilities are closely aligned with the observed outcomes, indicating excellent calibration.

\subsection*{D\hspace{1em}Noise Mask Illustration}

\begin{figure}[ht!]
\centering
\begin{subfigure}{.5\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{mask.png}
  \caption{}
  \label{fig:sub1}
\end{subfigure}%
\hfill
\begin{subfigure}{.5\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{download_14.png}
  \caption{}
  \label{fig:sub2}
\end{subfigure}
\caption{Illustration of a sample input mask and its corresponding noise mask: (a) the input mask assigns a label of 0 here to all surface water pixels (ground truth for PFAS contamination), and (b) the noise mask provides pixel-wise confidence scores. The model’s loss is computed as the focal loss between predictions and the input mask, scaled by the noise mask to emphasize high-confidence labels.
}
\label{fig:test7}
\end{figure}

\subsection*{E\hspace{1em}Noise Mask Weights Ablation}

Our ablation study on 2008 PFAS data compared several noise mask weight configurations. In Ablation 1, weights were set to 30\% for dischargers, 40\% for land cover, 20\% for flow direction, and 10\% for distance to sample points. Ablation 2 used 40\% for dischargers, 30\% for land cover, 20\% for flow direction, and 10\% for distance to sample points, while Ablation 3 applied 30\% for dischargers, 40\% for land cover, 10\% for flow direction, and 20\% for distance to sample points. Our current configuration, 40\% dischargers, 30\% land cover, 20\% distance to sample points, and 10\% flow direction, yielded the best overall performance, achieving the highest accuracy, IoU, F-score, and precision, even though recall was slightly lower. These results, in line with domain expert recommendations, confirm that our chosen configuration most effectively captures the critical environmental signals for PFAS contamination prediction. 

\begin{table}[ht!]
  \centering
  \small  
  \setlength{\tabcolsep}{5pt}  
  \renewcommand{\arraystretch}{1.3}  
  \begin{tabular}{l c c c c c}
    \toprule
    \textbf{Metric} & \textbf{Abl. 1} & \textbf{Abl. 2} & \textbf{Abl. 3} & \textbf{Current} \\
    \midrule
    Accuracy     & 84 & 81 & 81 & \textbf{88}  \\
    IoU          & 61 & 60 & 60 & \textbf{65} \\
    F-score      & 73 & 73 & 73 & \textbf{77} \\
    Precision    & 75 & 71 & 71 & \textbf{82} \\
    Recall       & 71 & \textbf{76} & \textbf{76} & 73 \\
    \bottomrule
  \end{tabular}
  \captionsetup{justification=centering, position=bottom}  
  \caption{Comparison of performance across three noise weight ablations (Abl. 1, Abl. 2, and Abl. 3) and our current noise weight configuration.}
  \label{tab:performance_comparison_ablationn}
\end{table}

% \subsection*{F\hspace{1em}Ablation Study on Feature Importance}

% We conducted an ablation study on 2008 PFAS data to evaluate the influence of different environmental feature groups on PFAS contamination prediction. Ten configurations were tested, each isolating a specific feature set: (1) airports, (2) military bases, (3) AFF spill areas, (4) flow direction, (5) land cover, (6) raw material extraction and production (e.g., mining, quarrying, oil and gas production, fishing, agriculture), (7) industrial manufacturing and processing facilities, (8) utilities and environmental services (e.g., landfills, waste combustors, drinking water treatment), (9) trade, transportation, real estate, and warehousing, and (10) professional and public services (e.g., management of companies, public administration).

% The results in Table~\ref{tab:performance_comparison_ablationnn} reveal a clear ranking of feature importance. The most influential features are from Ablation 1 (airports), Ablation 7 (industrial manufacturing), and Ablation 9 (trade, transportation, real estate, and warehousing), followed closely by Ablations 8 and 10 (utilities/environmental services and professional/public services). Features in Ablations 6, 3, and 2 (raw material extraction, AFF spill areas, and military bases) show moderate contributions, followed by Ablations 5 (land cover) and then 4 (flow direction). These findings suggest that anthropogenic factors, particularly those related to transportation and industrial activity, are key drivers of PFAS contamination, a conclusion that aligns well with domain expert insights.


% \begin{table*}[ht!]
%   \centering
%   \small  
%   \setlength{\tabcolsep}{5pt}  
%   \renewcommand{\arraystretch}{1.3}  
%   \begin{tabular}{l c c c c c c c c c c c}
%     \toprule
%     \textbf{Metric} & \textbf{Abl. 1} & \textbf{Abl. 2} & \textbf{Abl. 3} & \textbf{Abl. 4} & \textbf{Abl. 5} & \textbf{Abl. 6} & \textbf{Abl. 7} & \textbf{Abl. 8} & \textbf{Abl. 9} & \textbf{Abl. 10} \\
%     \midrule
%     Accuracy     & 81 & 81 & 88 & 88 & 84 & 88& 78&84 & 88 & 84 \\
%     IoU          & 56 & 62 & 62 & 65& 63 & 62 & 58 &61 &  60& 61 \\
%     F-score      & 69 & 75 & 73 & 77& 76 & 73& 72&73 & 72& 73 \\
%     Precision    & 69 & 75 & 95 & 82& 75 & 95& 70 &75 & 94 & 75\\
%     Recall       & 69 & 76 & 67 & 73& 78 & 67& 80 & 71 & 67 & 71 \\
%     \bottomrule
%   \end{tabular}
%   \captionsetup{justification=centering, position=bottom}  
%   \caption{Comparison of performance across ten feature ablations (Abl. 1, Abl. 2, Abl. 3, Abl. 4, Abl. 5, Abl. 6, Abl. 7, Abl. 8, Abl. 9, Abl. 10).}
%   \label{tab:performance_comparison_ablationnn}
% \end{table*}


\subsection*{F\hspace{1em}Kriging}
We employed Kriging as a spatial interpolation technique to estimate PFAS (0 or 1). First, we examined the spatial structure of our data by calculating an empirical semivariogram. Using latitude and longitude coordinates alongside the binary target variable, we computed pairwise distances and differences to produce a semivariogram plot, enabling us to observe how variance changed with increasing distance. This step informed the selection of appropriate variogram models (e.g., linear or spherical) for Kriging.

Next, we split the data on a state-by-state basis, ensuring geographic diversity between the training and testing subsets. We explored both Ordinary Kriging (assuming a stationary mean across the study area) and Universal Kriging (introducing a drift term to account for regional linear trends), but the two approaches produced essentially identical results on our dataset. After training each Kriging model on the geographic regions in the training set, we predicted PFAS presence at test set locations. Since we treated PFAS presence as a binary outcome, the resulting continuous Kriging predictions were thresholded at 0.5. We then computed standard classification metrics (such as precision, recall, and F1-score) to evaluate each model’s ability to distinguish between high contamination versus low contamination sites.

While Kriging provided a practical means of interpolating PFAS presence, its reliance on variogram assumptions and spatial stationarity can limit accuracy in areas with highly heterogeneous conditions. Nonetheless, the approach offered a useful baseline spatial model against which we compared our proposed DL framework, highlighting the value of more sophisticated spatial processes (e.g., hydrological connectivity) when modeling PFAS contamination.

\subsection*{G\hspace{1em}Pollutant Transport Simulation}
Rather than running a simulation like SWAT directly, an often time-consuming and computationally intensive process due to its detailed process-based simulations of surface runoff, infiltration, evapotranspiration, and pollutant transport, we implemented a streamlined Python-based workflow. Frameworks like SWAT typically require extensive preprocessing of input data (e.g., delineating sub-basins, defining hydrologic response units, and calibrating numerous parameters) and can take hours or days to execute, particularly when modeling large numbers of small patches. By handling infiltration, runoff, and flow direction in a custom script, we can batch-process multiple patches more efficiently while still capturing essential pollutant movement patterns. While our simulation remains an approximation relative to a fully process-based model like SWAT, it provides a practical and computationally feasible alternative for investigating pollutant transport in many geographically dispersed areas.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=\columnwidth]{flow.png}
  \caption{Example binary output from the pollutant transport simulation: 0 represents low contamination, and 1 represents high contamination. The simulation approximates the distribution of PFAS contamination based on hydrological and environmental parameters, providing a practical alternative to full-scale SWAT simulations.}
  \label{fig:simulation_output_example}
\end{figure}


First, we assign initial pollutant concentrations in each patch by setting cells flagged as “dischargers” to a high baseline value (e.g., 100), while land cover–based default concentrations provide moderately-low to low initial contamination levels for other cells. We then construct a Hydrologic Response Unit (HRU) parameter table that assigns infiltration and runoff values based on a cell’s land cover, soil type, and slope. Using these parameters, each patch’s land cover and soil rasters define infiltration/runoff ratios for every grid cell. We incorporate two key rasters; flow direction and flow accumulation, both exported from System for Automated Geoscientific Analyses (SAGA); a GIS and geospatial analysis tool focused on geospatial data processing, analysis, and visualization, to specify how water and pollutant mass move downstream. In each iteration, a fraction of the pollutant mass in each cell is transferred to its downstream neighbor according to the cell’s infiltration/runoff factors, guided by the flow direction raster and scaled by the flow accumulation values. Repeating this process causes pollutant mass to concentrate in lower-lying or higher-flow cells, thereby modeling how contamination evolves over time within the patch.

After the simulation converges, we produce a final pollutant concentration raster, thresholded by the median value of concentrations across all patches for the year to generate a binary contamination maps, as illustrated in Figure~\ref{fig:simulation_output_example}. The binary outputs, with values of 0 and 1 representing low and high contamination respectively, visualizes the simulation's predictions of contamination distribution. These outputs are compared against test set patches containing actual observed PFAS presence to evaluate how accurately the simulation captures observed contamination patterns. Although this standalone approach remains computationally non-trivial, it is more tractable for batch processing of multiple patches than running a full SWAT project repeatedly. Consequently, it provides a practical baseline for pollutant transport modeling within our broader framework, allowing us to compare simulated outputs to both real-world data and other modeling approaches.


\end{document}





























\subsection{Word Processing Software}

As detailed below, IJCAI has prepared and made available a set of
\LaTeX{} macros and a Microsoft Word template for use in formatting
your paper. If you are using some other word processing software, please follow the format instructions given below and ensure that your final paper looks as much like this sample as possible.

\section{Style and Format}

\LaTeX{} and Word style files that implement these instructions
can be retrieved electronically. (See Section~\ref{stylefiles} for
instructions on how to obtain these files.)

\subsection{Layout}

Print manuscripts two columns to a page, in the manner in which these
instructions are printed. The exact dimensions for pages are:
\begin{itemize}
    \item left and right margins: .75$''$
    \item column width: 3.375$''$
    \item gap between columns: .25$''$
    \item top margin---first page: 1.375$''$
    \item top margin---other pages: .75$''$
    \item bottom margin: 1.25$''$
    \item column height---first page: 6.625$''$
    \item column height---other pages: 9$''$
\end{itemize}

All measurements assume an 8-1/2$''$ $\times$ 11$''$ page size. For
A4-size paper, use the given top and left margins, column width,
height, and gap, and modify the bottom and right margins as necessary.

\subsection{Format of Electronic Manuscript}

For the production of the electronic manuscript, you must use Adobe's
{\em Portable Document Format} (PDF). A PDF file can be generated, for
instance, on Unix systems using {\tt ps2pdf} or on Windows systems
using Adobe's Distiller. There is also a website with free software
and conversion services: \url{http://www.ps2pdf.com}. For reasons of
uniformity, use of Adobe's {\em Times Roman} font is strongly suggested.
In \LaTeX2e{} this is accomplished by writing
\begin{quote}
    \mbox{\tt $\backslash$usepackage\{times\}}
\end{quote}
in the preamble.\footnote{You may want to also use the package {\tt
            latexsym}, which defines all symbols known from the old \LaTeX{}
    version.}

Additionally, it is of utmost importance to specify the {\bf
        letter} format (corresponding to 8-1/2$''$ $\times$ 11$''$) when
formatting the paper. When working with {\tt dvips}, for instance, one
should specify {\tt -t letter}.

\subsection{Papers Submitted for Review vs. Camera-ready Papers}
In this document, we distinguish between papers submitted for review (henceforth, submissions) and camera-ready versions, i.e., accepted papers that will be included in the conference proceedings. The present document provides information to be used by both types of papers (submissions / camera-ready). There are relevant differences between the two versions. Find them next.

\subsubsection{Anonymity}
For the main track and some of the special tracks, submissions must be anonymous; for other special tracks they must be non-anonymous. The camera-ready versions for all tracks are non-anonymous. When preparing your submission, please check the track-specific instructions regarding anonymity.

\subsubsection{Submissions}
The following instructions apply to submissions:
\begin{itemize}
\item If your track requires submissions to be anonymous, they must be fully anonymized as discussed in the Modifications for Blind Review subsection below; in this case, Acknowledgements and Contribution Statement sections are not allowed.

\item If your track requires non-anonymous submissions, you should provide all author information at the time of submission, just as for camera-ready papers (see below); Acknowledgements and Contribution Statement sections are allowed, but optional.

\item Submissions must include line numbers to facilitate feedback in the review process . Enable line numbers by uncommenting the command {\tt \textbackslash{}linenumbers} in the preamble.

\item The limit on the number of  content pages is \emph{strict}. All papers exceeding the limits will be desk rejected.
\end{itemize}

\subsubsection{Camera-Ready Papers}
The following instructions apply to camera-ready papers:

\begin{itemize}
\item Authors and affiliations are mandatory. Explicit self-references are allowed. It is strictly forbidden to add authors not declared at submission time.

\item Acknowledgements and Contribution Statement sections are allowed, but optional.

\item Line numbering must be disabled. To achieve this, comment or disable {\tt \textbackslash{}linenumbers} in the preamble.

\item For some of the tracks, you can exceed the page limit by purchasing extra pages.
\end{itemize}

\subsection{Title and Author Information}

Center the title on the entire width of the page in a 14-point bold
font. The title must be capitalized using Title Case. For non-anonymous papers, author names and affiliations should appear below the title. Center author name(s) in 12-point bold font. On the following line(s) place the affiliations.

\subsubsection{Author Names}

Each author name must be followed by:
\begin{itemize}
    \item A newline {\tt \textbackslash{}\textbackslash{}} command for the last author.
    \item An {\tt \textbackslash{}And} command for the second to last author.
    \item An {\tt \textbackslash{}and} command for the other authors.
\end{itemize}

\subsubsection{Affiliations}

After all authors, start the affiliations section by using the {\tt \textbackslash{}affiliations} command.
Each affiliation must be terminated by a newline {\tt \textbackslash{}\textbackslash{}} command. Make sure that you include the newline after the last affiliation, too.

\subsubsection{Mapping Authors to Affiliations}

If some scenarios, the affiliation of each author is clear without any further indication (\emph{e.g.}, all authors share the same affiliation, all authors have a single and different affiliation). In these situations you don't need to do anything special.

In more complex scenarios you will have to clearly indicate the affiliation(s) for each author. This is done by using numeric math superscripts {\tt \$\{\^{}$i,j, \ldots$\}\$}. You must use numbers, not symbols, because those are reserved for footnotes in this section (should you need them). Check the authors definition in this example for reference.

\subsubsection{Emails}

This section is optional, and can be omitted entirely if you prefer. If you want to include e-mails, you should either include all authors' e-mails or just the contact author(s)' ones.

Start the e-mails section with the {\tt \textbackslash{}emails} command. After that, write all emails you want to include separated by a comma and a space, following the order used for the authors (\emph{i.e.}, the first e-mail should correspond to the first author, the second e-mail to the second author and so on).

You may ``contract" consecutive e-mails on the same domain as shown in this example (write the users' part within curly brackets, followed by the domain name). Only e-mails of the exact same domain may be contracted. For instance, you cannot contract ``person@example.com" and ``other@test.example.com" because the domains are different.


\subsubsection{Modifications for Blind Review}
When submitting to a track that requires anonymous submissions,
in order to make blind reviewing possible, authors must omit their
names, affiliations and e-mails. In place
of names, affiliations and e-mails, you can optionally provide the submission number and/or
a list of content areas. When referring to one's own work,
use the third person rather than the
first person. For example, say, ``Previously,
Gottlob~\shortcite{gottlob:nonmon} has shown that\ldots'', rather
than, ``In our previous work~\cite{gottlob:nonmon}, we have shown
that\ldots'' Try to avoid including any information in the body of the
paper or references that would identify the authors or their
institutions, such as acknowledgements. Such information can be added post-acceptance to be included in the camera-ready
version.
Please also make sure that your paper metadata does not reveal
the authors' identities.

\subsection{Abstract}

Place the abstract at the beginning of the first column 3$''$ from the
top of the page, unless that does not leave enough room for the title
and author information. Use a slightly smaller width than in the body
of the paper. Head the abstract with ``Abstract'' centered above the
body of the abstract in a 12-point bold font. The body of the abstract
should be in the same font as the body of the paper.

The abstract should be a concise, one-paragraph summary describing the
general thesis and conclusion of your paper. A reader should be able
to learn the purpose of the paper and the reason for its importance
from the abstract. The abstract should be no more than 200 words long.

\subsection{Text}

The main body of the text immediately follows the abstract. Use
10-point type in a clear, readable font with 1-point leading (10 on
11).

Indent when starting a new paragraph, except after major headings.

\subsection{Headings and Sections}

When necessary, headings should be used to separate major sections of
your paper. (These instructions use many headings to demonstrate their
appearance; your paper should have fewer headings.). All headings should be capitalized using Title Case.

\subsubsection{Section Headings}

Print section headings in 12-point bold type in the style shown in
these instructions. Leave a blank space of approximately 10 points
above and 4 points below section headings.  Number sections with
Arabic numerals.

\subsubsection{Subsection Headings}

Print subsection headings in 11-point bold type. Leave a blank space
of approximately 8 points above and 3 points below subsection
headings. Number subsections with the section number and the
subsection number (in Arabic numerals) separated by a
period.

\subsubsection{Subsubsection Headings}

Print subsubsection headings in 10-point bold type. Leave a blank
space of approximately 6 points above subsubsection headings. Do not
number subsubsections.

\paragraph{Titled paragraphs.} You should use titled paragraphs if and
only if the title covers exactly one paragraph. Such paragraphs should be
separated from the preceding content by at least 3pt, and no more than
6pt. The title should be in 10pt bold font and to end with a period.
After that, a 1em horizontal space should follow the title before
the paragraph's text.

In \LaTeX{} titled paragraphs should be typeset using
\begin{quote}
    {\tt \textbackslash{}paragraph\{Title.\} text} .
\end{quote}

\subsection{Special Sections}

\subsubsection{Appendices}
You may move some of the contents of the paper into one or more appendices that appear after the main content, but before references. These appendices count towards the page limit and are distinct from the supplementary material that can be submitted separately through CMT. Such appendices are useful if you would like to include highly technical material (such as a lengthy calculation) that will disrupt the flow of the paper. They can be included both in papers submitted for review and in camera-ready versions; in the latter case, they will be included in the proceedings (whereas the supplementary materials will not be included in the proceedings).
Appendices are optional. Appendices must appear after the main content.
Appendix sections must use letters instead of Arabic numerals. In \LaTeX,  you can use the {\tt \textbackslash{}appendix} command to achieve this followed by  {\tt \textbackslash section\{Appendix\}} for your appendix sections.

\subsubsection{Ethical Statement}

Ethical Statement is optional. You may include an Ethical Statement to discuss  the ethical aspects and implications of your research. The section should be titled \emph{Ethical Statement} and be typeset like any regular section but without being numbered. This section may be placed on the References pages.

Use
\begin{quote}
    {\tt \textbackslash{}section*\{Ethical Statement\}}
\end{quote}

\subsubsection{Acknowledgements}

Acknowledgements are optional. In the camera-ready version you may include an unnumbered acknowledgments section, including acknowledgments of help from colleagues, financial support, and permission to publish. This is not allowed in the anonymous submission. If present, acknowledgements must be in a dedicated, unnumbered section appearing after all regular sections but before references.  This section may be placed on the References pages.

Use
\begin{quote}
    {\tt \textbackslash{}section*\{Acknowledgements\}}
\end{quote}
to typeset the acknowledgements section in \LaTeX{}.


\subsubsection{Contribution Statement}

Contribution Statement is optional. In the camera-ready version you may include an unnumbered Contribution Statement section, explicitly describing the contribution of each of the co-authors to the paper. This is not allowed in the anonymous submission. If present, Contribution Statement must be in a dedicated, unnumbered section appearing after all regular sections but before references.  This section may be placed on the References pages.

Use
\begin{quote}
    {\tt \textbackslash{}section*\{Contribution Statement\}}
\end{quote}
to typeset the Contribution Statement section in \LaTeX{}.

\subsubsection{References}

The references section is headed ``References'', printed in the same
style as a section heading but without a number. A sample list of
references is given at the end of these instructions. Use a consistent
format for references. The reference list should not include publicly unavailable work.

\subsubsection{Order of Sections}
Sections should be arranged in the following order:
\begin{enumerate}
    \item Main content sections (numbered)
    \item Appendices (optional, numbered using capital letters)
    \item Ethical statement (optional, unnumbered)
    \item Acknowledgements (optional, unnumbered)
    \item Contribution statement (optional, unnumbered)
    \item References (required, unnumbered)
\end{enumerate}

\subsection{Citations}

Citations within the text should include the author's last name and
the year of publication, for example~\cite{gottlob:nonmon}.  Append
lowercase letters to the year in cases of ambiguity.  Treat multiple
authors as in the following examples:~\cite{abelson-et-al:scheme}
or~\cite{bgf:Lixto} (for more than two authors) and
\cite{brachman-schmolze:kl-one} (for two authors).  If the author
portion of a citation is obvious, omit it, e.g.,
Nebel~\shortcite{nebel:jair-2000}.  Collapse multiple citations as
follows:~\cite{gls:hypertrees,levesque:functional-foundations}.
\nocite{abelson-et-al:scheme}
\nocite{bgf:Lixto}
\nocite{brachman-schmolze:kl-one}
\nocite{gottlob:nonmon}
\nocite{gls:hypertrees}
\nocite{levesque:functional-foundations}
\nocite{levesque:belief}
\nocite{nebel:jair-2000}

\subsection{Footnotes}

Place footnotes at the bottom of the page in a 9-point font.  Refer to
them with superscript numbers.\footnote{This is how your footnotes
    should appear.} Separate them from the text by a short
line.\footnote{Note the line separating these footnotes from the
    text.} Avoid footnotes as much as possible; they interrupt the flow of
the text.

\section{Illustrations}

Place all illustrations (figures, drawings, tables, and photographs)
throughout the paper at the places where they are first discussed,
rather than at the end of the paper.

They should be floated to the top (preferred) or bottom of the page,
unless they are an integral part
of your narrative flow. When placed at the bottom or top of
a page, illustrations may run across both columns, but not when they
appear inline.

Illustrations must be rendered electronically or scanned and placed
directly in your document. They should be cropped outside \LaTeX{},
otherwise portions of the image could reappear during the post-processing of your paper.
When possible, generate your illustrations in a vector format.
When using bitmaps, please use 300dpi resolution at least.
All illustrations should be understandable when printed in black and
white, albeit you can use colors to enhance them. Line weights should
be 1/2-point or thicker. Avoid screens and superimposing type on
patterns, as these effects may not reproduce well.

Number illustrations sequentially. Use references of the following
form: Figure 1, Table 2, etc. Place illustration numbers and captions
under illustrations. Leave a margin of 1/4-inch around the area
covered by the illustration and caption.  Use 9-point type for
captions, labels, and other text in illustrations. Captions should always appear below the illustration.

\section{Tables}

Tables are treated as illustrations containing data. Therefore, they should also appear floated to the top (preferably) or bottom of the page, and with the captions below them.

\begin{table}
    \centering
    \begin{tabular}{lll}
        \hline
        Scenario  & $\delta$ & Runtime \\
        \hline
        Paris     & 0.1s     & 13.65ms \\
        Paris     & 0.2s     & 0.01ms  \\
        New York  & 0.1s     & 92.50ms \\
        Singapore & 0.1s     & 33.33ms \\
        Singapore & 0.2s     & 23.01ms \\
        \hline
    \end{tabular}
    \caption{Latex default table}
    \label{tab:plain}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{lrr}
        \toprule
        Scenario  & $\delta$ (s) & Runtime (ms) \\
        \midrule
        Paris     & 0.1          & 13.65        \\
                  & 0.2          & 0.01         \\
        New York  & 0.1          & 92.50        \\
        Singapore & 0.1          & 33.33        \\
                  & 0.2          & 23.01        \\
        \bottomrule
    \end{tabular}
    \caption{Booktabs table}
    \label{tab:booktabs}
\end{table}

If you are using \LaTeX, you should use the {\tt booktabs} package, because it produces tables that are better than the standard ones. Compare Tables~\ref{tab:plain} and~\ref{tab:booktabs}. The latter is clearly more readable for three reasons:

\begin{enumerate}
    \item The styling is better thanks to using the {\tt booktabs} rulers instead of the default ones.
    \item Numeric columns are right-aligned, making it easier to compare the numbers. Make sure to also right-align the corresponding headers, and to use the same precision for all numbers.
    \item We avoid unnecessary repetition, both between lines (no need to repeat the scenario name in this case) as well as in the content (units can be shown in the column header).
\end{enumerate}

\section{Formulas}

IJCAI's two-column format makes it difficult to typeset long formulas. A usual temptation is to reduce the size of the formula by using the {\tt small} or {\tt tiny} sizes. This doesn't work correctly with the current \LaTeX{} versions, breaking the line spacing of the preceding paragraphs and title, as well as the equation number sizes. The following equation demonstrates the effects (notice that this entire paragraph looks badly formatted, and the line numbers no longer match the text):
%
\begin{tiny}
    \begin{equation}
        x = \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i
    \end{equation}
\end{tiny}%

Reducing formula sizes this way is strictly forbidden. We {\bf strongly} recommend authors to split formulas in multiple lines when they don't fit in a single line. This is the easiest approach to typeset those formulas and provides the most readable output%
%
\begin{align}
    x = & \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \nonumber \\
    +   & \prod_{i=1}^n \sum_{j=1}^n j_i.
\end{align}%

If a line is just slightly longer than the column width, you may use the {\tt resizebox} environment on that equation. The result looks better and doesn't interfere with the paragraph's line spacing: %
\begin{equation}
    \resizebox{.91\linewidth}{!}{$
            \displaystyle
            x = \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i
        $}.
\end{equation}%

This last solution may have to be adapted if you use different equation environments, but it can generally be made to work. Please notice that in any case:

\begin{itemize}
    \item Equation numbers must be in the same font and size as the main text (10pt).
    \item Your formula's main symbols should not be smaller than {\small small} text (9pt).
\end{itemize}

For instance, the formula
%
\begin{equation}
    \resizebox{.91\linewidth}{!}{$
            \displaystyle
            x = \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j
        $}
\end{equation}
%
would not be acceptable because the text is too small.

\section{Examples, Definitions, Theorems and Similar}

Examples, definitions, theorems, corollaries and similar must be written in their own paragraph. The paragraph must be separated by at least 2pt and no more than 5pt from the preceding and succeeding paragraphs. They must begin with the kind of item written in 10pt bold font followed by their number (e.g.: {\bf Theorem 1}),
optionally followed by a title/summary between parentheses in non-bold font and ended with a period (in bold).
After that the main body of the item follows, written in 10 pt italics font (see below for examples).

In \LaTeX{} we strongly recommend that you define environments for your examples, definitions, propositions, lemmas, corollaries and similar. This can be done in your \LaTeX{} preamble using \texttt{\textbackslash{newtheorem}} -- see the source of this document for examples. Numbering for these items must be global, not per-section (e.g.: Theorem 1 instead of Theorem 6.1).

\begin{example}[How to write an example]
    Examples should be written using the example environment defined in this template.
\end{example}

\begin{theorem}
    This is an example of an untitled theorem.
\end{theorem}

You may also include a title or description using these environments as shown in the following theorem.

\begin{theorem}[A titled theorem]
    This is an example of a titled theorem.
\end{theorem}

\section{Proofs}

Proofs must be written in their own paragraph(s) separated by at least 2pt and no more than 5pt from the preceding and succeeding paragraphs. Proof paragraphs should start with the keyword ``Proof." in 10pt italics font. After that the proof follows in regular 10pt font. At the end of the proof, an unfilled square symbol (qed) marks the end of the proof.

In \LaTeX{} proofs should be typeset using the \texttt{\textbackslash{proof}} environment.

\begin{proof}
    This paragraph is an example of how a proof looks like using the \texttt{\textbackslash{proof}} environment.
\end{proof}


\section{Algorithms and Listings}

Algorithms and listings are a special kind of figures. Like all illustrations, they should appear floated to the top (preferably) or bottom of the page. However, their caption should appear in the header, left-justified and enclosed between horizontal lines, as shown in Algorithm~\ref{alg:algorithm}. The algorithm body should be terminated with another horizontal line. It is up to the authors to decide whether to show line numbers or not, how to format comments, etc.

In \LaTeX{} algorithms may be typeset using the {\tt algorithm} and {\tt algorithmic} packages, but you can also use one of the many other packages for the task.

\begin{algorithm}[tb]
    \caption{Example algorithm}
    \label{alg:algorithm}
    \textbf{Input}: Your algorithm's input\\
    \textbf{Parameter}: Optional list of parameters\\
    \textbf{Output}: Your algorithm's output
    \begin{algorithmic}[1] %[1] enables line numbers
        \STATE Let $t=0$.
        \WHILE{condition}
        \STATE Do some action.
        \IF {conditional}
        \STATE Perform task A.
        \ELSE
        \STATE Perform task B.
        \ENDIF
        \ENDWHILE
        \STATE \textbf{return} solution
    \end{algorithmic}
\end{algorithm}

\section{\LaTeX{} and Word Style Files}\label{stylefiles}

The \LaTeX{} and Word style files are available on the IJCAI--25
website, \url{https://2025.ijcai.org/}.
These style files implement the formatting instructions in this
document.

The \LaTeX{} files are {\tt ijcai25.sty} and {\tt ijcai25.tex}, and
the Bib\TeX{} files are {\tt named.bst} and {\tt ijcai25.bib}. The
\LaTeX{} style file is for version 2e of \LaTeX{}, and the Bib\TeX{}
style file is for version 0.99c of Bib\TeX{} ({\em not} version
0.98i). .

The Microsoft Word style file consists of a single file, {\tt
        ijcai25.docx}. 
%This template differs from the one used for IJCAI--23.

These Microsoft Word and \LaTeX{} files contain the source of the
present document and may serve as a formatting sample.

Further information on using these styles for the preparation of
papers for IJCAI--25 can be obtained by contacting {\tt
        proceedings@ijcai.org}.

\appendix

\section*{Ethical Statement}

There are no ethical issues.

\section*{Acknowledgments}

The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
files that implement them was supported by Schlumberger Palo Alto
Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
Preparation of the Microsoft Word file was supported by IJCAI.  An
early version of this document was created by Shirley Jowell and Peter
F. Patel-Schneider.  It was subsequently modified by Jennifer
Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
Francisco Cruz-Mencia and Edith Elkind.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

