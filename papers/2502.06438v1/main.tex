\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  
\IEEEoverridecommandlockouts                                                                 
\overrideIEEEmargins                                      
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{svg}
\usepackage{comment}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    

\include{include}
\include{acronym}

\makeatother
\title{\LARGE \bf
FEMBA: Efficient and Scalable EEG Analysis with a \\ Bidirectional Mamba Foundation Model
}

\author{
Anna Tegon$^{1}$, Thorir Mar Ingolfsson$^{1}$, \\ Xiaying Wang$^{1}$, 
Luca Benini$^{1,2}$, Yawei Li$^{1}$
\thanks{$^{1}$Integrated Systems Laboratory, ETH Z{\"u}rich, Z{\"u}rich, Switzerland.}
\thanks{$^{2}$DEI, University of Bologna, Bologna, Italy.}
\thanks{Anna Tegon and Thorir Mar Ingolfsson are co-first authors.}
}

\begin{document}
\thispagestyle{empty}
\pagestyle{empty}



\maketitle


\begin{abstract}
Accurate and efficient electroencephalography (EEG) analysis is essential for detecting seizures and artifacts in long-term monitoring, with applications spanning hospital diagnostics to wearable health devices. Robust EEG analytics have the potential to greatly improve patient care. However, traditional deep learning models, especially Transformer-based architectures, are hindered by their quadratic time and memory complexity, making them less suitable for resource-constrained environments. To address these challenges, we present FEMBA (Foundational EEG Mamba + Bidirectional Architecture), a novel self-supervised framework that establishes new efficiency benchmarks for EEG analysis through bidirectional state-space modeling. Unlike Transformer-based models, which incur quadratic time and memory complexity, FEMBA scales linearly with sequence length, enabling more scalable and efficient processing of extended EEG recordings. Trained on over 21,000 hours of unlabeled EEG and fine-tuned on three downstream tasks, FEMBA achieves competitive performance in comparison with transformer models, with significantly lower computational cost. Specifically, it reaches 81.82\% balanced accuracy (0.8921 AUROC) on TUAB and 0.949 AUROC on TUAR, while a \emph{tiny} 7.8M-parameter variant demonstrates viability for resource-constrained devices. These results pave the way for scalable, general-purpose EEG analytics in both clinical and highlight FEMBA as a promising candidate for wearable applications.
\newline
\noindent\textit{Clinical relevance}—
By reducing model size and computational overhead, FEMBA enables continuous on-device EEG monitoring for tasks like seizure detection and artifact reduction, promising improved patient care through timely and cost-effective neuro-monitoring solutions.
\end{abstract}
    
\input{Sections/Introduction}
\input{Sections/Background}
\input{Sections/Methods}
\input{Sections/Results}

\section{Conclusion}\label{ch:conclusion}
We introduced \textbf{FEMBA}, a novel self-supervised EEG framework grounded in bidirectional state-space modeling and pre-trained on over 21,000 hours of unlabelled clinical EEG. Our experiments across multiple downstream tasks (abnormal EEG detection, artifact recognition, slowing event classification, and neonatal seizure detection) demonstrate that FEMBA achieves near-Transformer performance while maintaining significantly lower computational complexity and memory requirements.

Notably, a \emph{tiny} 7.8M-parameter variant (FEMBA-Tiny) retains competitive accuracy on tasks such as artifact detection, showcasing the potential for real-time edge deployments. Nonetheless, certain domain shifts—such as neonatal vs.\ adult EEG—underscore the need for additional domain adaptation. Future work will explore these techniques and integrate multi-modal physiological signals for more robust clinical event detection. We believe FEMBA marks a key step toward delivering efficient, universal EEG foundation models that operate seamlessly from large hospital databases to low-power wearable devices.
\section*{Acknowledgment}
\vspace{-0.1cm}
This project is supported by the Swiss National Science Foundation under the grant number 193813 (PEDESITE project) and by the ETH Future Computing Laboratory (EFCL), financed by a donation from Huawei Technologies. We acknowledge ISCRA for awarding this project access to the LEONARDO supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CINECA (Italy).
\bibliographystyle{IEEEtran}
\bibliography{bib}

\end{document}
