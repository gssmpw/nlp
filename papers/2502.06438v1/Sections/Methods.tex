\section{Methodology}
\label{sec:methods}
In this section, we describe the proposed \emph{Foundational EEG Mamba + Bidirectional Architecture} (FEMBA) and its training procedures. Next, we outline our self-supervised pre-training scheme and finally, we explain our fine-tuning strategy, including two alternative classifier architectures and multiple downstream tasks (abnormal EEG detection, artifact recognition, and slowing event classification).

\subsection{Foundational EEG Mamba + Bidirectional Architecture (FEMBA)}
Our proposed FEMBA architecture is designed in four model sizes: Tiny, Base, Large, and Huge, with parameter sizes ranging from 7.8 million (Tiny) to 386 million (Huge), aligning with model sizes commonly explored in the literature~\cite{jianglarge,chen2024eegformer}. The primary distinction across these variants lies in the number of Bi-Mamba blocks and the embedding dimension, which is controlled by the 2D Convolution in the Tokenizer, as illustrated in Fig~\ref{fig:femba_architecture}. Specifically, the embedding dimensions for these configurations are as follows: the Tiny model uses two blocks and an embedding size of 35 ($(2,35)$); the Base model employs a configuration of $(12,35)$; the Large model adopts $(4,79)$; and the Huge model features $(20,79)$. Notably, the hidden state size across all configurations remains fixed at 80.

Furthermore, a residual connection is incorporated within the Bi-Mamba block to facilitate the smooth propagation of gradients during training. A detailed representation of the entire FEMBA architecture can be found in Fig~\ref{fig:femba_architecture}.

During training, we utilize a layer-wise learning rate decay~\cite{ishii2017layer} with a fixed decay factor of $0.75$, progressively reducing the learning rate from the deeper blocks to the earlier ones

\subsection{Self-Supervised Pre-training}
\label{subsec:pretraining}
We pre-train FEMBA on the TUEG dataset, as detailed in Section~\ref{sec:TUEG}. To prevent data leakage between pre-training and downstream tasks, we use a version of TUEG where subjects present in TUSL, TUAR, or TUAB have been filtered out. During pre-training, we adopt a self-supervised masked training strategy designed to enable FEMBA to learn robust, general-purpose representations of \gls{eeg} signals. This involves randomly masking a subset (60\%) of the input patches and training the model to reconstruct the missing patches, thereby compelling the encoder to capture meaningful spatiotemporal structures within the \gls{eeg} data.

\paragraph{Signal Normalization and Patch Embedding.}
We begin by representing each raw EEG recording as a tensor $x \in \mathbb{R}^{C \times T}$, where $C$ is the number of channels and $T$ is the temporal length (in samples). To reduce the influence of outliers, we apply quartile-based normalization~\cite{bedeeuzzaman2012automatic}, scaling each channel by its interquartile range (IQR):
\begin{equation*}
    x_{\text{norm}} = \frac{x - q_{\text{lower}}}{(q_{\text{upper}} - q_{\text{lower}}) + 1 \times 10^{-8}}.
\end{equation*}
We then segment $x_{\text{norm}}$ into bi-dimensional patches of size $p \times q$ (e.g., $4$ channels $\times$ $32$ samples). A 2D convolution projects these patches into an embedding space $\mathbf{X}_{\text{embed}} \in \mathbb{R}^{d \times C' \times T'}$, followed by learnable positional embeddings to maintain ordering across patch tokens.

\paragraph{Random Masking and Encoder.}
Next, we apply random masking to $60\%$ of the embedded patches, setting their representations to zero. This relatively high masking ratio ensures that the model must rely on contextual cues from unmasked segments to infer the missing patches. The masked embeddings, $\mathbf{X}_{\text{masked}}$, are then fed into the FEMBA encoder.

\paragraph{Decoder and Smooth L1 Reconstruction Loss.}
A lightweight decoder of two convolutional layers and a final linear projection attempts to reconstruct the original patches from the encoder outputs. We compute a Smooth L1 loss~\cite{girshick2015fastrcnn} only over the masked patches:
\begin{equation*}
    \text{SmoothL1}(\hat{x}, x) =
    \begin{cases}
        0.5 \,(x - \hat{x})^2, & \text{if } |x - \hat{x}| < \beta, \\
        |x - \hat{x}| - 0.5, & \text{otherwise},
    \end{cases}
\end{equation*}
\begin{equation*}
    \text{masked\_loss} = \frac{1}{|\mathcal{M}|} \sum_{i \in \mathcal{M}} \text{SmoothL1}(\hat{x}_i, x_i),
\end{equation*}
where $\mathcal{M}$ is the set of masked patch indices.

\subsection{Fine-Tuning}
\label{subsec:fine-tuning}

\subsubsection{Classifier Architectures.}
Following pre-training, the decoder is discarded and the Bi-Mamba encoder is repurposed as a feature extractor for downstream tasks. Two classification heads are explored:
\begin{enumerate}
    \item \textbf{Linear Classifier}: A small stack of fully connected layers (with GELU activations) outputs class probabilities. This design has a low parameter footprint ($\sim 0.5$\,M).
    \item \textbf{Mamba-Enhanced Classifier}: We add one more Mamba block before the final linear layer, enabling additional temporal modeling. This often improves accuracy in tasks with complex temporal dependencies but adds a slight increase in parameters (up to $0.7$\,M).
\end{enumerate}
\subsubsection{Downstream tasks}
We assess FEMBA on three downstream tasks using the datasets described in Section~\ref{sec:datasets_finetune}. For the TUAB dataset this consists of a binary classification (normal vs. abnormal), using the pre-defined train-test split. In TUSL, the task is a four-class classification task (slowing, seizure, complex, normal), Since the TUSL dataset lacks a predefined test split, we adopt an 80/10/10 randomized training/validation/test split. For TUAR we experiment with four versions of a downstream task based on the labeling scheme in in~\cite{ingolfsson2022energy}, they are described as the following:
\begin{itemize}
    \item \textbf{Binary Classification (BC)}: Label a window as \emph{artifact} if \emph{any} of the 13 artifact types is present on any channel; otherwise \emph{normal}.
    \item \textbf{Multilabel Classification (MC)}: Perform channel-wise artifact detection as a set of independent binary classifications, allowing multiple artifact types per window/channel.
    \item \textbf{Multiclass--Multioutput Classification (MMC)}: Discriminate between 13 artifact types for each channel, thus providing a more granular classification per channel.
    \item \textbf{Multiclass Classification (MCC)}: Restrict to 5 artifact types in a single-label setting, ignoring windows with combinations of artifacts (less than 5\% of data). This setting aligns closely with the protocol described by EEGFormer~\cite{chen2024eegformer}.
\end{itemize}
As the TUAR dataset also lacks a predefined test split, we similarly use an 80/10/10 randomized training/validation/test split.
