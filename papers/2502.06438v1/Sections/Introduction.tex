\section{Introduction}\label{sec:intro}
The emergence of foundation models has profoundly impacted artificial intelligence, bringing forward a shift toward generalizable, large-scale pre-training. These models, trained via \gls{ssl} on heterogeneous datasets, derive their effectiveness from hierarchical feature extraction that can span diverse tasks~\cite{bommasani2021opportunities}. While their success in language (e.g., BERT~\cite{devlin2019bertpretrainingdeepbidirectional}) and vision (e.g., CLIP~\cite{radford2021learning}) is well-documented, their potential in biomedical signal processing—particularly for \gls{eeg}—remains relatively underexplored.

\gls{eeg} is a challenging modality due to its pseudo-random, non-stationary waveforms, susceptibility to artifacts~\cite{ingolfsson_minimizing_2024}, and substantial intra- and inter-subject variability. These factors demand models that balance robustness with interpretability. Wearable EEG devices play a crucial role in enabling continuous brain monitoring in real-world settings, offering new opportunities for brain-computer interfaces~\cite{zhang2023recent}, healthcare and cognitive research~\cite{emish2024remote}. Although recent efforts have used convolutional architectures~\cite{roy2019deep} and attention-based mechanisms~\cite{chen2024eegformer} for \gls{eeg}, real-world constraints complicate their deployment. Wearable devices and continuous monitoring systems impose strict limits on memory and latency~\cite{casson2010wearable}, making even moderately sized Transformers impractical. As a result, there is a strong, important, unmet need for architectures that can combine expressive power with computational efficiency. Given these constraints, we propose harnessing \glspl{ssm}. Specifically, we build upon the Mamba linear \gls{ssm}, a scalable approach tailored for large-scale \gls{eeg}, to mitigate the memory and latency bottlenecks associated with Transformer models while maintaining high performance and interpretability.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/FEMBA_Architecture.pdf}
    \caption{Overview of the proposed FEMBA (Foundational \gls{eeg} Mamba + Bidirectional Architecture) pipeline. The input \gls{eeg} signal (with channels $C$ and length $T$) is first tokenized via a 2D convolution and flattening layer. Random masking is then applied to a subset of the patches for self-supervised learning. The masked tokens pass through the FEMBA encoder, which stacks multiple Bidirectional Mamba blocks to capture forward and backward dependencies. Finally, a lightweight decoder (for reconstruction) or a classification head (for downstream tasks) reconstructs or classifies the signals, respectively.}
    \label{fig:femba_architecture}
\end{figure*}

\paragraph*{From Transformers to Mamba}
Transformer-based models have demonstrated strong performance in capturing long-range dependencies in \gls{eeg}~\cite{chen2024eegformer,jianglarge}. Current \gls{eeg} foundation models (e.g., BENDR, EEGFormer, LaBraM, Neuro-GPT) predominantly rely on attention mechanisms and may not provide the efficiency demanded by edge-computing environments. However, their $\mathcal{O}(N^2)$ complexity in computation and memory as a function of sequence length N can become a bottleneck for continuous or extended \gls{eeg} recordings, especially on resource-constrained devices. In contrast, Mamba~\cite{gu2023mamba}, which is based on a state-space framework, helps address these challenges by reformulating sequence modeling as a latent differential system. This approach offers linear scaling (as a function of sequence length) without substantially compromising temporal resolution. Bidirectional extensions~\cite{liang_bi-mamba_2024} further enable retrospective analysis, which may be essential for detecting ephemeral biomarkers (e.g., interictal spikes).

To investigate computationally efficient architectures such alternatives, we introduce \emph{\textbf{FEMBA}} (Foundational \gls{eeg} Mamba + Bidirectional Architecture), which leverages state-space principles for large-scale \gls{eeg} modeling. FEMBA is designed to address three key limitations of prior work: (1) quadratic scaling in attention-based models, (2) limited pre-training scope for capturing neurophysiological diversity, and (3) difficulties in adapting to low-resource settings. By pre-training on 21,000 hours of unlabeled \gls{eeg} from 5,000 subjects, FEMBA aims to learn representations that generalize across a range of pathologies, while retaining the potential for deployment on wearable hardware, as demonstrated by the promising performance of our Tiny FEMBA model.


Our contributions are the following:

\begin{itemize}
    \item \textbf{A Novel Architectural Paradigm:} We integrate a bidirectional state-space approach with \gls{ssl} to demonstrate that linear-time architectures can match—or in some cases surpass—Transformer-based models on established \gls{eeg} benchmarks (TUAB, TUAR, TUSL). This result suggests that attention-based solutions may not always be indispensable for effective \gls{eeg} modeling.
    
    \item \textbf{Large-Scale Pre-training on \gls{eeg}:} We conduct pre-training on a terabyte-scale unlabeled \gls{eeg} dataset (over 21,000 hours of data from more than 5,000 participants) that spans multiple studies. Using random masking for self-supervised reconstruction, FEMBA acquires robust, general representations suitable for diverse downstream tasks without extensive labeled data.
    
    \item \textbf {Efficient \gls{sota} performance} We have developed FEMBA in four sizes of model parameters: Tiny (7.8M), Base (47.7M), Large (77.8M), and Huge (389M). The Huge model achieves a mere 0.7\% decrease in accuracy compared to the \gls{sota} on TUAB while being \textbf{$3.5\times$} more computationally efficient and \textbf{$1.5\times$} more memory efficient. On TUAR, FEMBA sets a new \gls{sota} benchmark, with the Tiny model beating previous \gls{sota} with over \textbf{$27\times$} computational decrease and over \textbf{$2\times$} better memory efficiency compared to the previous \gls{sota}. These results highlight FEMBA's versatility for high-performance applications and resource-restrictive scenarios.
\end{itemize}

By emphasizing both effectiveness and efficiency, FEMBA provides a step toward accessible, low-cost analytics in healthcare contexts. The remainder of this paper details the methodology and experiments, highlighting how state-space-based approaches can offer a compelling alternative to attention-driven architectures in \gls{eeg} analysis.

