\section{Background}\label{sec:related}
This section provides an overview of the Temple University Hospital EEG (TUEG) Corpus and its labeled subsets, then reviews recent advances in EEG foundation models. We focus on the computational challenges faced by Transformer-based approaches, discuss the motivation for \glspl{ssm}, and examine Mamba-based solutions. Finally, we introduce how our proposed \emph{FEMBA} architecture builds upon these insights.

\subsection{Temple University Hospital EEG (TUEG)}\label{sec:TUEG}
The TUEG Corpus~\cite{obeid2016temple}, is one of the largest publicly available clinical EEG repositories. It contains over $26{,}000$ EEG recordings drawn from more than $14{,}000$ patients, spanning pediatric to geriatric populations and encompassing a variety of neurological conditions. In total, TUEG covers approximately $21{,}000$ hours of EEG data. Such diversity in demographics and pathologies provides a robust environment for learning general EEG representations.

\subsection{Key Labeled Subsets: TUAB, TUAR, and TUSL}\label{sec:datasets_finetune}
The TUEG dataset offers subsets of labeled datasets, such as the Temple University Hospital Abnormal EEG (TUAB), Artifact (TUAR) and Slowing (TUSL) Corpus~\cite{obeid2016temple}. TUAB offers annotated recordings labeled as \emph{normal} or \emph{abnormal}. TUAB has $2,329$ subjects and relatively balanced classes and TUAB serves as a strong benchmark for clinical diagnostics. The TUAR dataset contains annotations for various artifacts (e.g., eye blinks, muscle artifacts) in single-channel or multi-channel settings and has $213$ subjects. While in TUSL, the focus shifts to detecting and classifying \emph{slowing} events, seizures, complex background, and normal EEG. This 4-class classification task (slowing, seizure, complex, normal) consists of $1000$ subjects. Table~\ref{tab:dataset_summary} summarizes these three labeled subsets used in our experiments.


\begin{table}[t]
    \centering
    \caption{Summary of Datasets Used}
    \label{tab:dataset_summary}
    \begin{tabular}{lcc}
        \hline
        \textbf{Dataset} & \textbf{\# Subjects} & \textbf{Task} \\
        \hline
        TUEG & $14{,}987$ & Pre-training \\
        TUAB & $2{,}329$ & Abnormal vs. Normal \\
        TUAR & 213 & Artifact Detection \\
        TUSL & $1{,}000$ & Slowing Events \\
        \hline
    \end{tabular}
\end{table}

\subsection{Related works}
\paragraph*{Foundation Models in \gls{eeg}}
Foundation models have gained significant traction in NLP (e.g., DeepSeek~\cite{wu2024deepseek}) and computer vision (e.g., Molmo~\cite{deitke2024molmo}), motivating interest in their application to \gls{eeg}. However, these models are typically tailored for structured data such as text or images, raising challenges when dealing with the temporal complexity and biological variability of \gls{eeg} signals~\cite{cui2024toward}. Early \gls{eeg}-focused foundation models like BENDR~\cite{kostas_bendr_2021} employed contrastive learning yet faced scalability issues. Neuro-GPT~\cite{cui_neuro-gpt_2024} introduced autoregressive masking and reported gains in motor imagery classification, while LaBraM~\cite{jianglarge} and EEGFormer~\cite{chen2024eegformer} refined masked modeling methods across multiple datasets, achieving balanced accuracies above 80\% on abnormal \gls{eeg} detection. Despite these advancements, most prior approaches rely on Transformer architectures with $\mathcal{O}(N^2)$ complexity as a function of the sequence length, limiting their viability for continuous or large-scale \gls{eeg} monitoring.


\paragraph*{From Traditional Methods to State Space Models}
Conventional \gls{eeg} analysis often used machine learning algorithms such as \glspl{svm} and \gls{lda}, complemented by smaller deep networks like EEGNet~\cite{lawhern2018eegnet} and DeepConvNet~\cite{schirrmeister2017deep}. While these approaches offered interpretability and efficiency for relatively constrained tasks, they required extensive feature engineering and did not always generalize well to diverse patient populations. Transformer-based methods~\cite{jianglarge,chen2024eegformer,cui_neuro-gpt_2024,kostas_bendr_2021} later tackled the challenge of capturing long-range dependencies, though their substantial computational and memory demands may hinder real-world deployment.

\glspl{ssm} have gained interest for time-series analysis as they evolve a hidden state over time according to a simple linear dynamical system. In continuous form
\begin{equation*} 
\mathbf{h}'(t) = A\,\mathbf{h}(t) + B\,\mathbf{x}(t), \quad \mathbf{y}(t) = C\,\mathbf{h}(t), 
\end{equation*}
where $\mathbf{h}(t)$ is the hidden state, $\mathbf{x}(t)$ is the input, $\mathbf{y}(t)$ is the output, and $\{A, B, C\}$ are system matrices governing state evolution and output generation. Although these equations describe a continuous process, many implementations rely on discrete versions for efficient training in deep learning frameworks.


\paragraph*{Wearable and Edge Constraints}
Limited battery life, on-board memory, and compute resources characterize many real-world EEG applications, especially wearable devices~\cite{ingolfsson_brainfusenet_2024}. Applications like continuous epilepsy detection add real-time considerations and demand low false-alarm rates~\cite{ingolfsson_minimizing_2024}. The quadratic scaling of transformer-based methods often proves impractical under these constraints. In contrast, architectures based on state-space principles—owing to linear time and memory complexity—can better meet edge-computing requirements.


\paragraph*{Mamba-Based Approaches for \gls{eeg}}
A notable example of such \gls{ssm} is Mamba~\cite{gu2023mamba}, which applies a Zero-Order Hold (ZOH) scheme~\cite{pechlivanidou2022zero} to discretize the \gls{ssm}. Under a sampling interval $\Delta$, the continuous matrices $A,B$ map to discrete counterparts $A_d$ and $B_d$. Mamba further integrates a selective gating mechanism to modulate the hidden state update in a data-dependent manner. As a result, it achieves \emph{linear} complexity in sequence length, contrasting with the $\mathcal{O}(N^2)$ complexity of transformers. Bi-Mamba+~\cite{liang_bi-mamba_2024} extends Mamba by processing the input sequence in forward and backward directions, subsequently merging the two representations (e.g., via summation or gating). Recent work has begun to explore Mamba’s potential in \gls{eeg} analysis. \emph{Mentality}~\cite{Panchavati_mentality_2024} employed Mamba with a masked reconstruction scheme on TUSZ v2.0.1, improving seizure detection area under the ROC curve (AUROC) from 0.64 to 0.72. \emph{EEGMamba}~\cite{gui_eegmamba_2024} adopted a multi-task strategy by integrating Spatio-Temporal-Adaptive modules and Mixture-of-Experts heads, achieving above 98\% accuracy on the Siena dataset and around 97\% on CHB-MIT. Despite these early successes, challenges for Mamba-based models remain—especially regarding robust spatial-channel modeling for varying electrode montages and the need for domain-generalizable representations.

In this work, \textbf{our FEMBA} builds upon Mamba’s efficient state-space design by integrating large-scale self-supervised pre-training with bidirectional state updates, FEMBA aims to deliver strong accuracy on various \gls{eeg} downstream tasks while maintaining linear scaling (with regards to sequence length) suitable for resource-limited devices.
