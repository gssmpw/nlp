\section{Results}\label{sec:results}
This section demonstrates that \textbf{FEMBA} consistently achieves \gls{soa} or near-\gls{soa} performance on diverse EEG benchmarks (TUAB, TUAR, and TUSL), while using significantly fewer FLOPs and less memory compared to recent \gls{soa} self-supervised Transformer-based methods. We provide quantitative accuracy metrics and efficiency analyses, which underscores FEMBA’s suitability for large-scale clinical or wearable EEG systems. For specific training details we fine-tune all layers (encoder + classifier) end-to-end using the Adam optimizer (initial learning rate of $1 \times 10^{-4}$) with cosine decay scheduling. Early stopping is employed based on validation loss to mitigate overfitting.

\begin{table}[h!]
    \centering
    \caption{Performance Comparison on TUAB}
    \label{tab:results_tuab}
    \resizebox{\columnwidth}{!} {
    \setlength{\tabcolsep}{6pt} 
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Model} & \textbf{Model Size} & \textbf{Bal. Acc. (\%)} & \textbf{AUPR} & \textbf{AUROC} \\ 
        \midrule
        \textbf{Supervised Models} \\
        SPaRCNet & 0.8M & 78.96 $\pm$ 0.18 & 0.8414 $\pm$ 0.0018 & 0.8676 $\pm$ 0.0012 \\
        ContraWR & 1.6M & 77.46 $\pm$ 0.41 & 0.8421 $\pm$ 0.0140 & 0.8456 $\pm$ 0.0074 \\
        CNN-Transformer & 3.2M & 77.77 $\pm$ 0.22 & 0.8433 $\pm$ 0.0039 & 0.8461 $\pm$ 0.0013 \\
        FFCL & 2.4M & 78.48 $\pm$ 0.38 & 0.8448 $\pm$ 0.0065 & 0.8569 $\pm$ 0.0051 \\
        ST-Transformer & 3.2M & 79.66 $\pm$ 0.23 & 0.8521 $\pm$ 0.0026 & 0.8707 $\pm$ 0.0019 \\
        \midrule
        \textbf{Self-superv. Models} \\
        BENDR & 0.39M & 76.96 $\pm$ 3.98 &  & 0.8397 $\pm$ 0.0344 \\
        BrainBERT & 43.2M & - & 0.8460 $\pm$ 0.0030 & 0.8530 $\pm$ 0.0020 \\
        EEGFormer-Small & 1.9M & - & 0.8620 $\pm$ 0.0050 & 0.8620 $\pm$ 0.0070 \\
        EEGFormer-Base & 2.3M & - & 0.8670 $\pm$ 0.0020 & 0.8670 $\pm$ 0.0030 \\
        EEGFormer-Large & 3.2M & - & 0.8720 $\pm$ 0.0010 & 0.8760 $\pm$ 0.0030 \\
        BIOT & 3.2M & 79.59 $\pm$ 0.57 & 0.8692 $\pm$ 0.0023 & 0.8815 $\pm$ 0.0043 \\
        EEG2Rep & - & 80.52 $\pm$ 2.22 &  & 0.8843 $\pm$ 0.0309 \\
        LaBraM-Base & 5.8M & 81.40 $\pm$ 0.19 & 0.8965 $\pm$ 0.0016 & 0.9022 $\pm$ 0.0009 \\
        LaBraM-Large & 46M & 82.26 $\pm$ 0.15 & 0.9130 $\pm$ 0.0005 & 0.9127 $\pm$ 0.0005 \\
        LaBraM-Huge & 369M & 82.58 $\pm$ 0.11 & 0.9204 $\pm$ 0.0011 & 0.9162 $\pm$ 0.0016 \\
        \midrule
        \textbf{FEMBA-Base} & 47.7M  &81.05 $\pm$ 0.14 &0.8894 $\pm$0.0050  & 0.8829 $\pm$ 0.0021  \\
        \textbf{FEMBA-Large} & 77.8M &  81.47 $\pm$ 0.11 & 0.8992 $\pm$ 0.0007  & 0.8856 $\pm$ 0.0004 \\
        \textbf{FEMBA-Huge} & 386M  & 81.82 $\pm$ 0.16 & 0.9005 $\pm$ 0.0017 & 0.8921 $\pm$ 0.0042  \\
        \bottomrule
   \end{tabular}}
   
\end{table}
\begin{table}[b]
    \centering
    \caption{Model Comparison of FLOPs, Parameters, and Peak Memory Usage}
    \label{tab:model_comparison}
    \setlength{\tabcolsep}{6pt} 
    \renewcommand{\arraystretch}{1.2} 
    \small 
    \begin{tabular}{@{}lccc@{}}
    \toprule
    \textbf{Model} & \textbf{FLOPs} & \textbf{Parameters} & \textbf{Memory (MB)} \\ 
    \midrule
    EEGFormer-Small     & 21.06B & 1.9M     & 44.63 \\ 
    EEGFormer-Base      & 26.20B & 2.3M     & 71.32 \\ 
    EEGFormer-Large     & 36.46B & 3.2M     & 108.02 \\ 
    LaBraM-Base         & 4.42B  & 5.8M     & 757.38 \\ 
    LaBraM-Large        & 27.79B & 46M      & 1371.92 \\ 
    LaBraM-Huge         & 202.17B & 369M    & 2758.42 \\ 
    \textbf{FEMBA-Tiny} & 1.31B  & 7.8M     & 53.36 \\ 
    \textbf{FEMBA-Base} & 7.52B  & 47.7M    & 240.50 \\ 
    \textbf{FEMBA-Large} & 12.48B & 77.8M   & 548.71 \\ 
    \textbf{FEMBA-Huge} & 58.74B & 386M     & 1886.17 \\ 
    \bottomrule
    \end{tabular}
\end{table}
\begin{table*}[t]
    \centering
    \caption{Detailed Results on TUAR Across Four Classification Protocols}
    \label{tab:results_tuar}
    \small 
    \begin{tabular}{@{}lccccccc@{}}
    \toprule
    \textbf{Model} & \textbf{Model Size} & \multicolumn{2}{c}{\textbf{BC}} & \multicolumn{2}{c}{\textbf{MC}} & \multicolumn{2}{c}{\textbf{MMC}} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
     &  & \textbf{AUROC} & \textbf{AUPR} & \textbf{AUROC} & \textbf{AUPR} & \textbf{AUROC} & \textbf{AUPR} \\
    \midrule
    \textbf{FEMBA-Tiny}  & 7.8M  
     & 0.937 $\pm$ 0.008 & 0.912 $\pm$ 0.010  
     & 0.887 $\pm$ 0.029 & 0.645 $\pm$ 0.024  
     & 0.893 $\pm$ 0.005 & 0.504 $\pm$ 0.013 
    \\
    \textbf{FEMBA-Base}  & 47.7M 
     & 0.949 $\pm$ 0.002 & 0.932 $\pm$ 0.001 
     & 0.909 $\pm$ 0.004 & 0.634 $\pm$ 0.016 
     & 0.888 $\pm$ 0.004 & 0.518 $\pm$ 0.002 
    \\
    \textbf{FEMBA-Large} & 77.8M  
     & 0.944 $\pm$ 0.003 & 0.913 $\pm$ 0.016 
     & 0.899 $\pm$ 0.006 & 0.608 $\pm$ 0.011 
     & 0.878 $\pm$ 0.020 & 0.516 $\pm$ 0.008 
    \\
    \bottomrule
    \end{tabular}
\end{table*}

\begin{table}[h!]
\centering
\caption{Performance Comparison across TUAR, TUSL}
\label{tab:results_tusl_neonate}
\resizebox{\columnwidth}{!} {
\setlength{\tabcolsep}{2pt} 
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Model} & \textbf{Method Size} & \multicolumn{2}{c}{\textbf{TUAR}} & \multicolumn{2}{c}{\textbf{TUSL}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
 &  & \textbf{AUROC} & \textbf{AUPR} & \textbf{AUROC} & \textbf{AUPR} \\
\midrule
EEGNet       & -  & 0.752 $\pm$ 0.006 & 0.433 $\pm$ 0.025 & 0.635 $\pm$ 0.015 & 0.351 $\pm$ 0.006  \\
TCN          & - & 0.687 $\pm$ 0.011 & 0.408 $\pm$ 0.009 & 0.545 $\pm$ 0.009 & 0.344 $\pm$ 0.001  \\
EEG-GNN      & - & 0.837 $\pm$ 0.022 & 0.488 $\pm$ 0.015 & 0.721 $\pm$ 0.009 & 0.381 $\pm$ 0.004  \\
GraphS4mer   & -  & 0.833 $\pm$ 0.006 & 0.461 $\pm$ 0.024 & 0.632 $\pm$ 0.017 & 0.359 $\pm$ 0.001  \\
BrainBERT    & 43.2M  & 0.753 $\pm$ 0.012 & 0.350 $\pm$ 0.014 & 0.588 $\pm$ 0.013 & 0.352 $\pm$ 0.003  \\
EEGFormer-Small  & 1.9M  & 0.847 $\pm$ 0.013 & 0.488 $\pm$ 0.012 & 0.683 $\pm$ 0.018 & 0.397 $\pm$ 0.011  \\
EEGFormer-Base  & 2.3M & 0.847 $\pm$ 0.014 & 0.483 $\pm$ 0.026 & 0.713 $\pm$ 0.010 & \textbf{0.393 $\pm$ 0.003}  \\
EEGFormer-Large  & 3.2M & 0.852 $\pm$ 0.004 & 0.483 $\pm$ 0.014 & 0.679 $\pm$ 0.013 & 0.389 $\pm$ 0.003  \\
\midrule
\textbf{FEMBA-Tiny}  & 7.8M  & \textbf{0.918 $\pm$ 0.003} & 0.518 $\pm$ 0.002 & 0.708 $\pm$ 0.005 & 0.277 $\pm$ 0.007 \\
\textbf{FEMBA-Base}  & 47.7M & 0.900 $\pm$ 0.010 & \textbf{0.559 $\pm$ 0.002}& \textbf{0.731 $\pm$ 0.012} & 0.289 $\pm$ 0.009\\
\textbf{FEMBA-Large} & 77.8M  & 0.915 $\pm$ 0.003 & 0.521 $\pm$ 0.001 &0.714 $\pm$ 0.007 & 0.282 $\pm$ 0.010 \\

\bottomrule
\end{tabular}
}
\end{table}
\subsection{Pre-training results}
Our FEMBA model variants are initially pretrained to reconstruct both masked and unmasked sections of the signal, as detailed in Section~\ref{subsec:pretraining}. All variants demonstrated strong reconstruction capabilities for both masked and unmasked portions of the signal. This is illustrated in Fig~\ref{fig:reconstruct}, where the FEMBA-Base model successfully reconstructs a masked signal. The training and validation loss during pretraining were closely aligned for all variants, with example loss values for FEMBA base of $0.122$ (Train) and $0.217$ (Validation).
\input{Figures/reconstruct}

\subsection{TUAB: Abnormal EEG Detection}
Table~\ref{tab:results_tuab} summarizes TUAB results, where the task is to classify recordings as \emph{normal} or \emph{abnormal}. All \textbf{FEMBA} variants outperform the supervised models, with \textbf{FEMBA-Huge} attaining a balanced accuracy of 81.82\% , approaching LaBraM-Large/Huge~\cite{jianglarge} (82.26\%--82.58\%) but with around \(\mathbf{70\%}\) fewer FLOPs than LaBraM-Huge (see Table~\ref{tab:model_comparison}). Moreover, FEMBA outperforms EEGFormer-Large~\cite{chen2024eegformer} in AUROC (0.8921 vs. 0.8760). This underscores that our near-linear Mamba-based encoder can rival top Transformer architectures without incurring the quadratic attention cost.

\subsection{TUAR: Artifact Detection}
We next evaluate FEMBA on the Temple University Hospital Artifact (TUAR) dataset using four classification protocols of increasing label complexity: \textbf{BC} (binary), \textbf{MC} (multilabel), \textbf{MMC} (multiclass--multioutput), and \textbf{MCC} (multiclass single-label). Table~\ref{tab:results_tuar} details the performance of three FEMBA variants:

\paragraph{Binary Classification (BC).}
Even our smallest \textbf{FEMBA-Tiny} (7.8M parameters) achieves an AUROC of 0.937 and AUPR of 0.912, signaling robust artifact vs.\ normal discrimination. Scaling to \textbf{FEMBA-Base} boosts AUROC to 0.949 and AUPR to 0.932—about a 1.2\% gain in AUROC at a modest increase in parameters.

\paragraph{Multilabel (MC) \& Multiclass–Multioutput (MMC).}
Channel-wise artifact detection (MC) sees AUROCs of up to 0.909, while the more fine-grained MMC reaches 0.893. Notably, \textbf{FEMBA-Tiny} slightly outperforms the Base and Large variants in MMC (0.893 vs.\ 0.888/0.878), showcasing that a lean state-space model can excel even in complex multi-artifact labeling.

\paragraph{Multiclass Classification (MCC).}
Restricting windows to a single artifact type yields the highest AUROC (up to 0.918 for FEMBA-Tiny). Meanwhile, \textbf{FEMBA-Large} achieves 0.915 AUROC and the highest AUPR (0.521). As reported in Table~\ref{tab:results_tusl_neonate}, FEMBA also surpasses EEGFormer-l~\cite{chen2024eegformer} (0.852 AUROC) under a comparable MCC protocol, demonstrating a SoA result at a fraction of the Transformer’s computational cost.

\subsection{TUSL (Slowing Event Classification).}
Table~\ref{tab:results_tusl_neonate} indicates that \textbf{FEMBA-Base} achieves 0.731~AUROC, surpassing EEGFormer-Small/Large by 4.8\%–5.2\% absolute (0.683/0.679), and slightly outperforming EEGFormer-Base (0.713). However, FEMBA’s AUPR (0.289) trails the best EEGFormer-Large AUPR (0.389) by about 10 percentage points, likely due to class imbalance. Despite this, FEMBA demonstrates these results at a significantly lower computational cost, as detailed in Section~\ref{subsec:efficiency}.

\subsection{Efficiency Analysis: FLOPs, Parameters, and Memory}\label{subsec:efficiency}
Practical considerations—such as floating-point operations (FLOPs), parameter counts, and peak memory usage—are critical in determining the feasibility of real-world or continuous EEG monitoring. Table~\ref{tab:model_comparison} provides a comparison of major Transformer baselines (EEGFormer, LaBraM) and our FEMBA models across these metrics.

For \textbf{LaBraM}, FLOPs and memory usage are calculated using its publicly available code repository. For \textbf{EEGFormer}, these metrics are approximated based on the limited details available in the literature, as no official code has been released. To measure peak memory usage, we process a batch size of 8 through each model and record the maximum memory consumption. Despite these approximations, a clear trend is evident:

\textbf{FEMBA-Huge} (386M parameters) requires 58.74B FLOPs, nearly \(\mathbf{3.5\times}\) fewer FLOPs than LaBraM-Huge (202.17B) and 30\% less memory usage, yet achieves comparable TUAB accuracy (81.82\% vs.\ 82.58\%).  \textbf{FEMBA-Tiny} (7.8M) uses only 1.31B FLOPs—up to \(\mathbf{27\times}\) fewer than EEGFormer-Large—while still delivering SoA AUROC (e.g., 0.918 on TUAR MCC). Similarly \textbf{FEMBA-Base} runs at 7.52B FLOPs, roughly \(\mathbf{4\times}\) lower than EEGFormer-Large (36.46B FLOPs). A detailed visual comparison of these models is provided in Figure~\ref{fig:comparison_inference_gpu}.
\input{Figures/comparison}


\subsection{Discussion}
Overall, FEMBA consistently achieves SoA or near-SoA accuracy with substantially reduced computational cost. On TUAB, \emph{FEMBA-Huge} falls within 0.8--1.0\% absolute of LaBraM-Large/Huge in balanced accuracy but uses roughly \(\mathbf{70\%}\) fewer FLOPs than LaBraM-Huge. On TUAR, \emph{FEMBA-Tiny} (7.8M) outperforms EEGFormer-l by 6.6\% in AUROC under comparable MCC protocols. For TUSL, FEMBA-Base surpasses all EEGFormer variants by up to 4.8\% in AUROC.

These findings validate that a state-space modeling approach can match or exceed Transformer baselines without the prohibitive \(\mathcal{O}(N^2)\) scaling. Future work could explore enhancements to further boost FEMBA’s accuracy, such as refining its architecture or incorporating advanced regularization techniques. Additionally, neonatal-focused pre-training could address domain shifts, while multi-modal integration may extend FEMBA’s applicability to a wider range of clinical scenarios. We conclude that FEMBA’s efficient design and robust performance establish it as a compelling alternative to Transformer-based EEG models for both large-scale and on-device applications.

