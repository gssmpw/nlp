\section{Introduction}

\begin{figure}[t!]
    \centering    
    \includegraphics[width=0.48\textwidth]{figures/introduction.pdf}
    \caption{Our \method combines text (Right) and vision (Left) modalities to augment time series forecasting.}
    \label{fig:intro}
    \vspace{-1.5em}
\end{figure}

Time series forecasting plays a pivotal role across numerous domains, including finance \cite{idrees2019prediction}, climate \cite{karevan2020transductive}, energy \cite{deb2017review}, and transportation \cite{zheng2020traffic}. Accurate predictions of future trends enable better decision-making, risk management, and resource allocation. Traditional statistical models, like ARIMA, have been the go-to methods for time series analysis, but they often struggle to capture complex, non-linear dependencies in large datasets. In contrast, deep learning approaches, e.g., RNNs \cite{medsker2001recurrent} and Transformer-based models \cite{li2019logtrans,wu2021autoformer,zhou2021informer,liu2022pyraformer,zhou2022fedformer,nie2022time}, have revolutionized this field by learning intricate temporal patterns from raw data through techniques such as patch-based restructuring, auto-correlation mechanisms, and frequency decomposition strategies, delivering superior performance across various forecasting tasks. However, these models often struggle with domain generalization and scenarios with limited training data, particularly in few/zero-shot settings \cite{liang2024foundation}.

To overcome these issues, researchers have turned to \emph{augmenting time series forecasting with additional modalities}, such as \emph{text} and \emph{images}, which provide complementary information that can enhance predictive accuracy:

\vspace{-1em}

\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item \textbf{Text-Augmented Models:} Textual data, such as task-specific knowledge or domain events, provides valuable context for time series forecasting. For example, in financial forecasting, market descriptions or policy changes can enhance accuracy by adding insights beyond raw data. Approaches like Time-LLM \cite{jin2023time} and UniTime \cite{liu2024unitime} map time series into textual representations, leveraging Large Language Models (LLMs) to capture contextual influences and infer predictions (Figure~\ref{fig:intro}, right). However, these models face two challenges: (1) the \textit{modality gap} between continuous time series and discrete text leads to information loss, and (2) pre-trained word embeddings are rarely optimized for time series forecasting, limiting fine-grained temporal pattern capture.

    \item \textbf{Vision-Augmented Models:} In contrast, transforming time series data into visual representations, such as through Gramian Angular Fields (GAF) or recurrence plots, enables models to identify and exploit underlying visual patterns, thereby facilitating the extraction of intricate temporal relationships using image-based feature learning techniques (e.g., CNNs) (Figure~\ref{fig:intro}, left). Recent work \cite{wu2023timesnet,wang2024timemixer++,chen2024visiontsvisualmaskedautoencoders} demonstrates the natural alignment between time series and vision, as both are continuous and share structural similarities, allowing pretrained vision models to effectively extract hierarchical temporal features. However, these models often lack semantic context, limiting their ability to capture domain-specific knowledge or event-driven insights critical for real-world forecasting.
\end{itemize}

\vspace{-1em}

Despite advancements in text- and vision-based models, integrating both modalities with time series remains underexplored. Vision-Language Models (VLMs) \cite{radford2021learning} excel in aligning vision and text for tasks like multimodal reasoning. Building on this, we introduce \method, extending VLMs to include time series as a third modality. Each modality—text for semantic context, vision for spatial patterns, and time series for temporal dynamics—offers unique strengths. Current approaches often focus on single modalities, limiting their ability to harness the complementary benefits of all three. Our method leverages pretrained VLMs to effectively integrate temporal, visual, and textual data for augmented time series forecasting.

By projecting time series into a unified vision-language semantic space, \method enables rich cross-modal interactions, combining the strengths of both modalities while mitigating their individual limitations. Specifically, \method introduces three key components: (1) a \textit{Retrieval-Augmented Learner} that processes raw time series data through patch-based feature extraction and memory bank interactions to generate enriched temporal representations, capturing both local and global dependencies; (2) a \textit{Vision-Augmented Learner} that adaptively transforms time series into images using multi-scale convolution, frequency encoding, and periodic encoding, preserving both fine-grained details and high-level structures; and (3) a \textit{Text-Augmented Learner} that generates rich textual context (e.g., statistics and dataset descriptions) to complement the visual representations. These modules collaborate with VLMs to integrate temporal, visual, and textual modalities, producing accurate forecasts through a fine-tuned predictor.

Our key contributions are summarized as follows:

\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item We propose the first framework unifying temporal, visual, and textual modalities via pretrained VLMs, complementing their strengths for augmented time series forecasting.

    \item We introduce three components: a Retrieval-Augmented Learner for temporal feature enhancement via memory bank interactions, a Vision-Augmented Learner for adaptive time-series-to-image transformation, and a Text-Augmented Learner for contextual prompt generation.

    \item \method achieves superior performance, particularly in few-shot and zero-shot scenarios, establishing a new direction for multimodal time series forecasting.
\end{itemize}