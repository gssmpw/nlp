\section{Related Work}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/framework.pdf}
    \caption{Overview of the \method framework.}
    \label{fig:framework}
    \vspace{-1em}
\end{figure*}

\noindent\textbf{Text-Augmented Models for Time Series Forecasting.} 

The success of LLMs inspires their application to time series tasks. Methods like LLMTime~\cite{gruver2023large} and LLM4TS~\cite{chang2023llm4ts} tokenize time series data for autoregressive prediction but inherit LLMs' limitations, such as poor arithmetic and recursive capabilities. Recent approaches, including GPT4TS~\cite{zhou2023one} and TimeLLM~\cite{jin2023time}, project time series into textual representations to leverage LLMs' reasoning abilities. However, they face challenges like the modality gap and lack of time series-optimized word embeddings, leading to potential information loss. UniTime~\cite{liu2024unitime} and TimeFFM~\cite{liu2024time} incorporate domain-specific instructions and federated learning, respectively, but remain constrained by their reliance on text alone.

\noindent\textbf{Vision-Augmented Models for Time Series Forecasting.} 

Vision emerges as a natural way to preserve temporal patterns. Early approaches use CNNs for matrix-formed time series~\cite{li2020forecasting, sood2021visual}, while TimesNet~\cite{wu2023timesnet} introduces multi-periodic decomposition for unified 2D modeling. VisionTS~\cite{chen2024visiontsvisualmaskedautoencoders} pioneers pre-trained visual encoders with grayscale time series images, and TimeMixer++~\cite{wang2024timemixer++} advances the field with multi-scale frequency-based time-image transformations. Despite their effectiveness in temporal modeling, these methods often lack semantic context, hard to use high-level contextual information for prediction.

\noindent\textbf{Vision-Language Models.} 

VLMs like ViLT~\cite{kim2021vilt}, CLIP~\cite{radford2021learning}, and ALIGN~\cite{jia2021scaling} transform multimodal understanding by aligning visual and textual representations. Recent advancements, like BLIP-2~\cite{li2022blip2} and LLaVA~\cite{liu2023visual}, further enhance multimodal reasoning. However, VLMs remain underexplored for time series analysis. Our work bridges this gap by leveraging VLMs to integrate temporal, visual, and textual modalities, addressing the limitations of unimodal approaches.