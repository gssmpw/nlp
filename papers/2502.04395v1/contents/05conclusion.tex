\section{Conclusion}

We presented \method, a novel framework leveraging pretrained VLMs to unify temporal, visual, and textual modalities for time series forecasting. By integrating the RAL, VAL, and TAL, \method bridges modality gaps, enabling rich cross-modal interactions. Extensive experiments demonstrate state-of-the-art performance across various datasets, especially in few-shot and zero-shot scenarios, outperforming existing methods while maintaining efficiency. Our work establishes a new direction for multimodal time series forecasting, highlighting the potential of VLMs in capturing temporal dynamics and semantic context.

Notably, \method operates can solely on original time series data without external information, ensuring fair comparisons and showcasing its ability to generate textual and visual representations directly from the data for self-augmentation. This design not only enhances accuracy but also emphasizing the framework's robustness, particularly in domains where external data is scarce or unavailable.

Future work may explore adaptive visual transformations for complex patterns, enhancing text utilization, extending to multi-task, and developing more efficient multimodal time series foundation models. For details, see \shortautoref{appx:future_work}.


\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning by integrating temporal, visual, and textual modalities for time series forecasting. While our approach improves accuracy and cross-domain generalization, we acknowledge potential risks such as data privacy concerns, algorithmic bias, and increased computational costs. We encourage further research into mitigating these risks to ensure responsible deployment in high-stakes applications.