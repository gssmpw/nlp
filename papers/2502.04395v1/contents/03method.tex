\section{Methodology}

To address the limitations of single-modality approaches and leverage the complementary strengths of visual, textual, and temporal modalities, we propose \method, a unified framework that integrates these modalities for enhanced time series forecasting. As illustrated in Figure~\ref{fig:framework}, the framework comprises three core components:

\vspace{-1em}
\begin{itemize}[leftmargin=*, itemsep=0pt]    
    \item \textbf{Retrieval-Augmented Learner (RAL)}: Extracts temporal features from raw time series patches, maintains a memory bank to interact with patch embeddings via multi-head self-attention and pooling mechanisms, preserving rich temporal feature representations and enhancing long-term dependency modeling for robust forecasting.
    \item \textbf{Vision-Augmented Learner (VAL)}: Transforms time series into informative three-channel images through multi-scale convolutions, frequency and periodic encoding. The images are processed by a frozen VLM vision encoder to extract hierarchical visual features, capturing both fine-grained details and high-level temporal patterns.
    \item \textbf{Text-Augmented Learner (TAL)}: Generates contextual textual prompts for the input time series, including statistical features (e.g., mean, variance, trends), domain-specific context (e.g., electricity consumption patterns), and image descriptions. These prompts are encoded by a frozen VLM text encoder to produce textual embeddings.
\end{itemize}
\vspace{-1em}

The image and text embeddings, extracted by the VLM, are integrated with temporal memory features via a gated fusion mechanism, effectively capturing complementary information to improve forecasting accuracy. These enriched multimodal features are then processed by a fine-tuned predictor to generate precise and reliable forecasts.

\subsection{Retrieval-Augmented Learner (RAL)}

The RAL module extracts high-level temporal features via patch-based processing and memory-enhanced attention. It dynamically retrieves and integrates temporal patterns, adapting to complex time series structures to enhance forecasting performance. It operates in two key stages.

\noindent\textbf{Patch Embedding:} The input time series \( x_{\text{enc}} \in \mathbb{R}^{B \times L \times D} \) (batch size \( B \), sequence length \( L \), variables \( D \)) is divided into overlapping patches of length \( \texttt{patch\_len} \) with stride \( \texttt{stride} \). Each patch is projected into a \( d_{\text{model}} \)-dimensional space via a linear layer, and positional embeddings are added to preserve temporal order. The resulting patch embeddings \( E_{\text{patches}} \in \mathbb{R}^{B \times (D \cdot N_{\text{patches}}) \times d_{\text{model}}} \) capture local temporal patterns, where \( N_{\text{patches}} \) is the number of patches.


\noindent\textbf{Memory-Enhanced Attention:} A set of learnable memory queries \( Q \in \mathbb{R}^{N_{\text{queries}} \times d_{\text{query}}} \) interact with patch embeddings through multi-head attention. Patch embeddings are projected into keys \( K \) and values \( V \), computing:

\vspace{-1em}
\begin{equation}
    \text{Memory}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_{\text{query}}}}\right)V.
\end{equation}
\vspace{-1em}

To better capture complex temporal dynamics, we introduce a hierarchical memory structure with two levels:

\vspace{-1em}
\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item \textbf{Local Memory}: Captures fine-grained patterns within individual patches for short-term dependencies.
    \item \textbf{Global Memory}: Aggregates information across patches for long-range dependencies and high-level trends.
\end{itemize}
\vspace{-1em}

The two memories are fused via a gated mechanism:

\vspace{-1em}
\begin{equation}
    M_{\text{fused}} = \alpha \cdot M_{\text{local}} + (1 - \alpha) \cdot M_{\text{global}},
\end{equation}
\vspace{-1em}

where \( \alpha \) is a learnable gating parameter that adaptively balances the contribution of local and global memory.

The output \( \mathbb{R}^{B \times N_{\text{queries}} \times d_{\text{query}}} \) stores high-level temporal patterns, enabling dynamic retrieval and fusion with other modalities. Each memory slot captures semantic and temporal relationships, allowing adaptive retrieval relevant patterns based on the input context. This hierarchical memory mechanism effectively addresses challenges in capturing long-range dependencies and complex temporal dynamics.

\subsection{Vision-Augmented Learner (VAL)}

The VAL module adaptively transforms time series \( x_{\text{enc}} \in \mathbb{R}^{B \times L \times D} \) into images, preserving both fine-grained and high-level temporal patterns through following operations:

\noindent\textbf{Frequency and Periodicity Encoding:} To capture spectral and temporal dependencies, the VAL module applies two complementary encoding techniques to the input time series, explicitly add frequency and time-domain information.

\vspace{-1em}
\begin{enumerate}[leftmargin=*, itemsep=0pt]
    \item \textbf{Frequency Encoding:} A Fast Fourier Transform (FFT) extracts frequency components as:
    
    \vspace{-1em}
    \begin{equation}
    \text{FFT}(x_{\text{enc}}) = \sum_{t=0}^{L-1} x_{\text{enc}}(t) \cdot e^{-2\pi i k t / L},
    \end{equation}
    \vspace{-1em}
    
    where \( k \) is the frequency index. The resulting frequency features are concatenated with the input time series, resulting in a tensor of shape \( \mathbb{R}^{B \times L \times D \times 2} \).

    \item \textbf{Periodicity Encoding:} Temporal dependencies are encoded using sine and cosine functions for each time step:

    \vspace{-1em}
    \begin{equation}
    \text{encoding}(t) = \left[ \sin\left(\frac{2\pi t}{P}\right), \cos\left(\frac{2\pi t}{P}\right) \right],
    \end{equation}
    \vspace{-1em}
    
    where \( P \) is the periodicity hyperparameter. These encodings are concatenated with the input time series, resulting in a tensor of shape \( \mathbb{R}^{B \times L \times D \times 3} \). Complete periodic parameter settings can be found in \shortautoref{appx:experiment_details}.
\end{enumerate}
\vspace{-1em}

\noindent\textbf{Multi-scale Convolution:} The concat tensor is processed through multiple convolutional layers to extract hierarchical temporal patterns. A 1D convolutional layer captures local dependencies, transforming the input into \( \mathbb{R}^{B \times D \times H_{\text{hidden}} \times L} \), where \( H_{\text{hidden}} \) is the hidden dimension. Averaging along \( D \) yields \( \mathbb{R}^{B \times H_{\text{hidden}} \times L} \). Two 2D convolutional layers follow: the first halves the channel dimension, and the second maps features to \( C \) output channels, producing the final output capturing both local and global temporal structures.

\noindent\textbf{Image Interpolation \& Normalization:} The output tensor is resized to the desired image dimensions \( (H, W) \) using bilinear interpolation. For a target pixel \( (x, y) \), the interpolated value \( \mathbf{I}(x, y) \) is computed as follows:

\vspace{-1em}
\begin{equation}
    \mathbf{I}(x, y) = \sum_{i=1}^{2} \sum_{j=1}^{2} \mathbf{I}(x_i, y_j) \cdot w_{ij},
\end{equation}
\vspace{0em}
\begin{equation}
    \mathbf{I}{\text{norm}} = 255 \cdot \frac{\mathbf{I}_{\text{raw}} - \text{Min}(\mathbf{I}_{\text{raw}})}{\text{Max}(\mathbf{I}_{\text{raw}}) - \text{Min}(\mathbf{I}_{\text{raw}}) + \epsilon},
\end{equation}
\vspace{-1em}

where \( (x_i, y_j) \) are the coordinates of the four nearest neighbors, \( w_{ij} \) are weights based on relative distances, and \(\epsilon = 10^{-5}\) prevents division by zero. Pixel values are scaled to \([0, 255]\) via min-max normalization, producing the normalized image \(\mathbf{I}_{\text{norm}} \in \mathbb{R}^{B \times C \times H \times W}\) (\( C \) is the number of channels). This ensures alignment with the VLM vision encoder's input distribution for effective feature extraction. Example images and descriptions can see \shortautoref{appx:visualizations}.

\subsection{Text-Augmented Learner (TAL)}

The TAL module provides contextual textual representations, either pre-defined (e.g., expert annotations) or dynamically generated, offering flexibility across diverse scenarios.

For dynamically generated prompts, TAL extracts key statistical properties from the input time series, including:

\vspace{-1em}
\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item Range: minimum and maximum values,
    \item Central tendency: median value,
    \item Trend: upward/downward based on first-order differences,
    \item Periodicity: dominant frequency components,
    \item Task context: forecasting horizon and historical window,
    \item Dataset characteristics: domain-specific descriptions.
\end{itemize}
\vspace{-1em}

These features are formatted into structured textual prompts. For example, the generated prompt might take the form:

``This is a time series transformed image highlighting its trends, periodic and change patterns. Dataset: \{description\}. Task: Forecast the next \{pred\_len\} steps using the past \{seq\_len\} steps. Input statistics: min value = \{min\_val\}, max value = \{max\_val\}, median value = \{median\_val\}, the overall trend is \{trend\_direction\}.''

When domain-specific knowledge is available (e.g., in medical diagnostics or financial analysis), TAL incorporates pre-defined textual descriptions. These are combined with dynamically generated prompts to enhance contextual understanding. The final textual representations are processed by the VLM text encoder, producing contextual embeddings that complement visual and temporal features.

By supporting both pre-defined and dynamically generated textual representations, TAL provides a flexible and adaptive mechanism for leveraging textual information in time series forecasting. This adaptability ensures that the model can effectively handle diverse scenarios, from general-purpose forecasting to domain-specific applications.

\subsection{Multimodal Fusion with VLMs}

The multimodal fusion pipeline integrates visual (VAL), textual (TAL), and temporal (RAL) information, leveraging their complementary strengths to enhance time series forecasting. It consists of three key steps:

\noindent\textbf{Multimodal Embeddings Extraction:} The generated images and text are processed by a frozen VLM (e.g., ViLT or CLIP), producing multimodal embeddings of shape \( \mathbb{R}^{B \times L_f \times d_h} \), where \( B \) is the batch size, \( L_f \) is the sequence length, and \( d_h \) is the VLM's hidden dimension. These embeddings capture visual and textual context, leveraging the VLM's pre-trained multimodal understanding capabilities.

\noindent\textbf{Temporal Feature Fusion:} To address the distribution shift between temporal and multimodal features, both modalities are projected into a shared \( d_{\text{model}} \)-dimensional space. Temporal memory embeddings \( \mathbf{F}_{\text{tem}} \) from RAL encode high-level temporal patterns and serve as queries in a cross-modal multi-head attention (CM-MHA) mechanism, while the multimodal embeddings \( \mathbf{F}_{\text{mm}} \) from the VLM serve as keys and values. The CM-MHA is defined as:

\vspace{-2em}
\begin{align}
    \text{CM-MHA}(Q, K, V) = \text{Cat}(\text{head}_1, \dots, \text{head}_h)W^O, \\
    \text{head}_i = \text{softmax}\left(\frac{QW_i^Q (KW_i^K)^\top}{\sqrt{d_k}}\right) VW_i^V.
\end{align}
\vspace{-1em}

where \( Q = \mathbf{F}_{\text{tem}} W^Q \), \( K = \mathbf{F}_{\text{mm}} W^K \), and \( V = \mathbf{F}_{\text{mm}} W^V \). Here, \( W_i^Q \), \( W_i^K \), \( W_i^V \), and \( W^O \) are learnable projection matrices. \( d_k = d_{\text{model}} / h \) is the head dimension, and \( h \) is the number of attention heads. This mechanism aligns and integrates temporal and multimodal features, capturing both fine-grained patterns and high-level context. A residual connection and layer normalization stabilize training:

\vspace{-1.5em}
\begin{equation}
    \mathbf{F}_{\text{attn}} = \text{LayerNorm}(\mathbf{F}_{\text{tem}} + \text{CM-MHA}(Q, K, V)).
\end{equation}
\vspace{-1.5em}

A gated fusion mechanism further enhances the output by dynamically weighting each modality:

\vspace{-1.5em}
\begin{align}
    \mathbf{G} &= \sigma(\mathbf{W}_g [\mathbf{F}_{\text{tem}}; \mathbf{F}_{\text{mm}}] + \mathbf{b}_g), \\
    \mathbf{F}_{\text{fused}} &= \mathbf{G} \odot \mathbf{F}_{\text{attn}} + (1 - \mathbf{G}) \odot \mathbf{F}_{\text{mm}},
\end{align}
\vspace{-1.5em}

where \( \mathbf{W}_g \) and \( \mathbf{b}_g \) are learnable parameters, and \( \sigma(\cdot) \) is the sigmoid function. This gated mechanism adaptively balances temporal and multimodal features for robust fusion.


\noindent\textbf{Forecasting:} The fused embedding is processed by a fine-tuned predictor, consisting of fully connected layers, to generate forecasts \( \hat{y} \in \mathbb{R}^{B \times T_{\text{pred}} \times D} \). By combining visual, textual, and temporal modalities, the pipeline captures both detailed patterns and high-level context, leveraging pre-trained VLMs for enhanced forecasting across diverse scenes.

\subsection{Optimization}

The model is trained end-to-end using a mean squared error (MSE) loss to optimize forecasting performance. Given historical observations \(\mathbf{X} \in \mathbb{R}^{N \times T}\) (with \(N\) variables and \(T\) time steps), the goal is to predict future values \(\hat{\mathbf{Y}} \in \mathbb{R}^{N \times H}\) for \(H\) time steps. The optimization objective is:

\vspace{-1em}
\begin{equation}
    \mathcal{L} = \frac{1}{H} \sum_{h=1}^{H} \| \hat{\mathbf{Y}}_h - \mathbf{Y}_h \|^2,
\end{equation}
\vspace{-1em}

where \(\hat{\mathbf{Y}}_h\) and \(\mathbf{Y}_h\) are predicted and ground truth values at time step \(h\). The pre-trained VLM is frozen, and only lightweight components are optimized:

\vspace{-1em}
\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item \textbf{RAL}: Patch embedding layer, memory queries, and attention mechanisms - capture temporal dependencies.
    \item \textbf{VAL}: Multi-scale convolutional layers, frequency encoding, and periodicity encoding modules - transform time series into informative images.
    \item \textbf{Multimodal Fusion}: Gating network, projection layers, and multi-head cross-attention - balance modality contributions (temporal, visual, and textual).
    \item \textbf{Predictor}: fully connected layers - maps fused embeddings to forecasts. 
\end{itemize}
\vspace{-1em}

This strategy adapts the VLM to time series forecasting, achieving robust performance with minimal overhead.