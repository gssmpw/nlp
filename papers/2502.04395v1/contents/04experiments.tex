\section{Experiments}

\input{tables/few-shot-forecasting-5}
\input{tables/few-shot-forecasting-10}
\input{tables/zero-shot-forecasting}

\noindent\textbf{Datasets and Metrics.} We evaluate \method on seven widely-used time series datasets spanning diverse domains, including energy consumption (ETTh1, ETTh2, ETTm1, ETTm2), weather forecasting, electricity load prediction (ECL, 321 variables), and traffic flow estimation (Traffic, 862 variables)~\cite{zhou2021informer, lai2018modeling}. These datasets, extensively adopted for benchmarking long-term forecasting models~\cite{wu2022timesnet}, exhibit varying characteristics in sampling frequency, dimensionality, and temporal patterns. For short-term forecasting, we utilize the M4 benchmark~\citep{makridakis2018m4}, which encompasses marketing data at various sampling frequencies. Forecasting performance is evaluated using Mean Absolute Error (MAE) and Mean Squared Error (MSE), following standard practices in the field. Additional details on datasets and metrics are provided in Appendix~\ref{appx:dataset_details} and~\ref{appx:evaluation_metric}.

% \noindent\textbf{Baselines.} We compare \method with state-of-the-art time series models, including text-augmented methods like TimeLLM \citeyearpar{jin2023time}, GPT4TS \citeyearpar{zhou2023one}, and LLMTime \citeyearpar{gruver2023large}; vision-augmented methods like TimesNet \citeyearpar{wu2023timesnet}; traditional deep models like PatchTST \citeyearpar{nie2022time}, ESTformer \citeyearpar{woo2022etsformer}, Non-Stationary Transformer \citeyearpar{liu2022non}, FEDformer \citeyearpar{zhou2022fedformer}, Autoformer \citeyearpar{wu2021autoformer}, Informer \citeyearpar{zhou2021informer}, and Reformer \citeyearpar{kitaev2020reformer}; and recent competitive models like DLinear \citeyearpar{zeng2023transformers}, LightTS \citeyearpar{zhang2022less}, N-HiTS \citeyearpar{challu2023nhits}, and N-BEATS \citeyearpar{oreshkin2019n}. Performance results for some baselines are cited from \cite{liu2024time} where applicable.


\noindent\textbf{Baselines.} We compare \method with state-of-the-art time series models, including text-augmented methods like TimeLLM \citeyearpar{jin2023time}, GPT4TS \citeyearpar{zhou2023one}, and LLMTime \citeyearpar{gruver2023large}; vision-augmented methods like TimesNet \citeyearpar{wu2023timesnet}; traditional deep models like PatchTST \citeyearpar{nie2022time}, ESTformer \citeyearpar{woo2022etsformer}, Non-Stationary Transformer \citeyearpar{liu2022non}, FEDformer \citeyearpar{zhou2022fedformer}, Autoformer \citeyearpar{wu2021autoformer}, Informer \citeyearpar{zhou2021informer}, and Reformer \citeyearpar{kitaev2020reformer}; and recent competitive models like DLinear \citeyearpar{zeng2023transformers}, LightTS \citeyearpar{zhang2022less}, N-HiTS \citeyearpar{challu2023nhits}, and N-BEATS \citeyearpar{oreshkin2019n}. Notably, \method is the first framework combining three modalities for time series forecasting. Performance results for some baselines are cited from \citeyearpar{liu2024time} where applicable.


\noindent\textbf{Implementation Details.} We compare \method against state-of-the-art models using a unified evaluation pipeline, following the configurations in \citep{wu2022timesnet} for fair comparison. ViLT \citep{kim2021vilt} is the default backbone, with \texttt{"vilt-b32-finetuned-coco"}. Other VLMs like CLIP and BLIP-2 are also supported. All models are trained with the Adam optimizer (learning rate $10^{-3}$, halved every epoch), a batch size of 32, and a maximum of 10 epochs with early stopping. Experiments are conducted on an Nvidia RTX A6000 GPU with 48GB memory. Additional optimization details are in Appendix~\ref{appx:optimization_settings}.

\input{tables/short-term-forecasting}
\input{tables/long-term-forecasting}

\subsection{Few-shot Forecasting}

We evaluate the few-shot capabilities of \method by testing its performance using only 5\% or 10\% of training data. This assesses its ability to combine pre-trained multimodal knowledge from VLM with time series-specific features for effective forecasting under minimal task-specific data.

As shown in \shortautoref{tab:few-shot-forecasting-5per} and \shortautoref{tab:few-shot-forecasting-10per}, \method consistently outperforms most baselines across datasets. For example, on ETTh1 with 5\% training data, \method reduces MSE by 29.5\% and MAE by 16.6\% compared to the second-best model, TimeLLM. On ETTm1 with 10\% data, it surpasses TimeLLM by 11.1\% in MSE and 10.5\% in MAE. On Weather with 5\% data, \method outperforms TimeLLM by 7.7\% in MSE and 9.4\% in MAE.

The performance gap between \method and traditional models (e.g., PatchTST, FEDformer) is more pronounced in few-shot settings, demonstrating the superiority of multi-modality in data-scarce scenarios. Notably, \method achieves this with only 143M parameters, significantly fewer than TimeLLM's 3405M, highlighting its efficiency.

\subsection{Zero-shot Forecasting}

We evaluate the zero-shot capability of \method in cross-domain settings, where the model predicts on unseen datasets by effectively leveraging knowledge from unrelated domains. To ensure a more comprehensive and rigorous comparison, we use the ETT datasets as Time-LLM \cite{jin2023time}, with results summarized in \shortautoref{tab:zero-shot-forecasting-brief}.

\method demonstrates strong zero-shot generalization, consistently outperforming or matching state-of-the-art baselines with fewer parameters. For example, in $ETTh1 \rightarrow ETTh2$, \method surpasses TimeLLM with a 4.2\% lower MSE and 0.5\% lower MAE. In $ETTm1 \rightarrow ETTh2$, it outperforms TimeLLM by 7.1\% in MSE and 3.6\% in MAE. In $ETTm2 \rightarrow ETTh2$, \method achieves competitive performance, closely matching TimeLLM with only a 1.4\% difference in MSE and 0.3\% in MAE.

\subsection{Short-term Forecasting}

For short-term forecasting, we evaluate \method on the M4 benchmark, which includes marketing data at various sampling frequencies. Performance is measured using SMAPE, MASE, and OWA metrics, averaged across datasets and sampling intervals (see \shortautoref{tab:short-term-forecasting}).

\method demonstrates strong performance, consistently outperforming state-of-the-art baselines across all metrics. For instance, it surpasses the second-best model, Time-LLM, with improvements of 0.7\% in SMAPE, 0.2\% in MASE, and 0.5\% in OWA, all while utilizing significantly fewer parameters and computational resources. Compared to traditional models like PatchTST and N-HiTS, the performance gains more, highlighting the benefit of multimodal knowledge in short-term forecasting. These gains stem from \method's integration of temporal, visual, and textual data, capturing richer features for improved accuracy.

\subsection{Long-term Forecasting}

We evaluate the long-term forecasting capabilities of \method across diverse temporal horizons and datasets.

As shown in \shortautoref{tab:long-term-forecasting}, \method achieves competitive performance compared to state-of-the-art baselines. For example, on ETTh1, \method surpasses TimeLLM with 0.7\% improvements in MSE and MAE. On ETTm2, it outperforms TimeLLM by 1.2\% in MSE and 0.6\% in MAE. However, on Weather, \method slightly trails TimeLLM with a 0.4\% higher MSE and 2.3\% higher MAE.

Overall, \method demonstrates robust performance across diverse tasks and datasets, highlighting its generalization and efficiency. By leveraging multimodal knowledge, it consistently outperforms state-of-the-art baselines with significantly fewer parameters (143M vs. TimeLLM's 3405M), making it a practical solution for real-world applications.

\subsection{Model Analysis}

\noindent\textbf{Ablation Studies:} \autoref{tab:multimodal_ablation} evaluates the contributions of key components of \method, including the RAL, VAL, and TAL. Results are averaged across forecasting horizons $H \in \{96, 192, 336, 720\}$ on the Weather dataset, with performance degradation (\textit{\%Deg}) measured for each variant.

\vspace{-0.5em}
\input{tables/multimodal-ablation-studies}

The study highlights the critical role of each component. Removing the RAL causes the largest performance drop (\(35.6\%\) in MSE and \(27.7\%\) in MAE), emphasizing its importance in capturing temporal dependencies through memory bank interactions. The VAL, which transforms time series into visual representations, is essential, with its exclusion leading to significant degradation (\(9.0\%\) in MSE and \(11.7\%\) in MAE). This underscores its ability to preserve fine-grained temporal patterns using VLM vision encoder. In contrast, removing the TAL results in minor degradation (\(2.1\%\) in MSE and \(1.8\%\) in MAE), likely due to sparse textual tokens in the VLM output (e.g., 11 out of 156 in ViLT). While the TAL provides valuable semantic context, its impact is limited by the VLM's temporal understanding. Future work could explore larger VLMs with extended textual inputs to enhance temporal-semantic alignment.

\noindent\textbf{Multimodal and Few/Zero-shot Analysis:} \method's few-shot and zero-shot capabilities arise from its integration of temporal, visual, and textual modalities. The RAL models temporal dependencies through memory bank interactions, ensuring robust feature extraction with limited data. The VAL captures visually interpretable features (e.g., trend, seasonality, periodicity) in domain-agnostic visual representations, while the TAL generates contextual descriptions, providing semantic insights for better generalization. Together, these components enable \method to leverage pre-trained multimodal knowledge, making it highly adaptable to new tasks and domains with minimal training data.

\vspace{-0.5em}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/multimodal_effectiveness.pdf}
    \caption{2D UMAP visualization (Left) and Gate weight distributions (Right) of multimodal and temporal memory embeddings, highlighting their complementary behavior.}
    \label{fig:fusion_analysis}
\end{figure}
\vspace{-0.5em}

To validate the adaptation of VLM capabilities to time series, we analyze the similarity between RAL (temporal) and TAL/VAL (multimodal) embeddings. Figure~\ref{fig:fusion_analysis} visualizes their complementary behavior. The left panel shows balanced gate weight distributions, indicating effective fusion of multimodal and temporal representations. The right panel's UMAP visualization reveals distinct yet overlapping clusters, confirming successful integration of multimodal information while preserving unique characteristics. This demonstrates \method's ability to adapt VLM-derived embeddings for robust  time series analysis.

\textbf{Computation Studies:} \method demonstrates strong computational efficiency, as shown in \autoref{tab:computational-efficiency}. With only 143.6M parameters (1/20 of Time-LLM's 3404.6M), memory usage scales from 1968 MiB (Weather) to 24916 MiB (Traffic), adapting to dataset complexity. Inference speed ranges from 0.2057s/iter (ECL) to 0.4809s/iter (ETTh1), efficiently handling varying loads. In contrast, Time-LLM requires over 37GB of memory even for smaller datasets like ETTh1 and ETTh2, making it infeasible for larger datasets such as Weather, ECL, and Traffic. This highlights \method's lightweight design and practical scalability.

\input{tables/computational-efficiency}

\noindent\textbf{Hyperparameter Studies:} We analyze key hyperparameters' impact on performance, as shown in Figure~\ref{fig:hyperparameters}. The sequence length performs best between 96 and 1024 timesteps, with 512 being optimal for most datasets. Longer sequences introduce noise without significant gains. The normalization constant peaks at 0.4, while the model dimension performs best at 128 for simpler datasets (e.g., ETTh1, ETTh2) and larger values for complex ones (e.g., Traffic, Weather). The gate network dimension, controlling multimodal fusion, achieves optimal results at 256 across most datasets.

\vspace{-0.5em}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/hyperparameters.pdf}
    \caption{Hyperparameters sensitivity analysis on input length, normalization constant, dimension of model and dimension of gate network, reflected by MAE.}
    \label{fig:hyperparameters}
\end{figure}
\vspace{-0.5em}
