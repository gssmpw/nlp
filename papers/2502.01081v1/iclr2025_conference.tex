\pdfoutput=1

\documentclass{article} % For LaTeX2e


% Now load your packages
%\usepackage{nicematrix}

\usepackage[table,xcdraw]{xcolor}
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[hidelinks]{hyperref}
\usepackage{url}
% Additional package(s)
\usepackage{graphicx}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{subcaption}
\usepackage[capitalize]{cleveref}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{soul}
\usepackage{tikz}
%\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage[most]{tcolorbox}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{minitoc}

\tcbuselibrary{listingsutf8}

% added commands
\newcommand{\algodataset}{\textsc{AlgoPuzzleVQA}}
\newcommand{\abstractdataset}{\textsc{PuzzleVQA}}
%\newcommand{\vt}[1]{\textcolor{green}{VT: #1}}
\newcommand{\SJ}[1]{\textcolor{blue}{SJ: #1}}
\newcommand{\ken}[1]{\textcolor{orange}{Ken: #1}}
\newcommand{\pleasecheck}[1]{\textcolor{orange}{#1}}
\newcommand{\greyrule}[2]{%
    \arrayrulecolor{black!30}%
    \cmidrule(lr){#1-#2}%
    \arrayrulecolor{black}%
}
\newcommand{\code}[1]{\texttt{#1}}


\usepackage{pifont}
\usepackage{xcolor}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\greencheck}{{\color{green}\cmark}}
\newcommand{\redcross}{{\color{red}\xmark}}
\definecolor{customgreen}{rgb}{0.325, 0.675, 0.196}

\definecolor{NotSureBlue}{rgb}{0.396, 0.443, 0.627}

\usepackage{fvextra} 
\DefineVerbatimEnvironment{MyVerbatim}{Verbatim}{
  breaklines=true,   
  breakanywhere=true,
  breaksymbolleft={}
}





\title{The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
    Vernon Y.H. Toh\quad Yew Ken Chia \quad Deepanway Ghosal \quad Soujanya Poria \\[10pt]
    \noindent{Singapore University of Technology and Design (SUTD)} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/curve.png}
    \caption{The performance of GPT-[n] and o-[n] series models on \abstractdataset{} and \algodataset{}, illustrating how multimodal reasoning evolves over time with model releases and inference cost. The size of each circle roughly represents the inference cost per puzzle.}
    \label{fig:visual_trends}
\end{figure}

\begin{abstract}
The releases of OpenAI's \textit{o1} and \textit{o3} mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities.
Notably, \textit{o3} outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI).
However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data.
Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks.
To this end, we track the evolution of the GPT-\textit{[n]} and \textit{o-[n]} series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of \textit{o1} comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency.
Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to \textit{o1}. \textit{Nonetheless, we observe that the \textit{o1} model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor.} 
We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available~\footnote{\url{https://github.com/declare-lab/LLM-PuzzleTest}}.
\end{abstract}

\section{Introduction}
Recent advances in large language models (LLMs) have demonstrated impressive capabilities in language understanding and generation, as seen in OpenAI's GPT-[n] series of models \citep{gpt3}.
Yet, true artificial general intelligence (AGI) requires robust reasoning abilities across different modalities \citep{Fei2021TowardsAG}. 
For instance, models such as OpenAI's new o-[n] series demonstrate a jumping reasoning curve through dramatic improvements on the Abstraction and Reasoning Corpus for Artificial General
Intelligence (ARC-AGI) \citep{chollet2019measureintelligence}.
However, the current evaluations in Figure \ref{fig:arc-agi} mainly focus on symbolic patterns, whereas humans often reason over complex data involving vision and language.
Thus, the ability to perceive, understand, and reason about multimodal inputs remains a crucial component of human-like intelligence, deserving urgent investigation.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/gpt-eval.pdf}
    \caption{ARC-AGI semi-private scores of the OpenAI models over time.}
    \label{fig:arc-agi}
\end{figure}

To this end, puzzles often serve as effective measures of cognitive abilities such as pattern recognition and step-by-step reasoning. 
Notably, such measures typically do not require specific domain knowledge, allowing individuals from diverse backgrounds to engage with them. One prominent example is Raven's Progressive Matrices \citep{raven-ebe0e932-0e0b-3cbd-9c98-9b94f8c2f1dc}, a non-verbal assessment tool designed to evaluate abstract reasoning and fluid intelligence. In this test, participants are presented with abstract patterns containing a missing element and must identify the correct piece to complete the pattern. 


Thus, inspired by abstract puzzles as measures of intelligence, recent multimodal benchmarks have enabled systematic evaluation across specific cognitive abilities, including visual perception, inductive reasoning, deductive reasoning, and algorithmic problem solving \citep{chia2024puzzlevqadiagnosingmultimodalreasoning, ghosal2024languagemodelspuzzleprodigies}.
Compared to previous measures, they require general understanding of spatial relationships, pattern recognition, and reasoning across visual and language elements, thus providing a more holistic measure of artificial general intelligence.
Our research addresses several key questions: 
(1) How do current state-of-the-art models perform on visual reasoning tasks?
(2) What types of pattern recognition and reasoning are particularly challenging? 
(3) How can we systematically evaluate and compare different models' multimodal reasoning capabilities?

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/intro_case.png}
    \caption{Case study on an abstract puzzle from the Colors \& Shapes (left) category and Colors \& Numbers (right) category in \abstractdataset{}.}
    \label{fig:case_intro}
\end{figure*}


In our evaluation, we assess the performance of GPT-[n] and o-[n] models on abstract multimodal puzzles from PuzzleVQA, which primarily test abstract reasoning. Additionally, we evaluate the models on AlgoPuzzleVQA, which require an algorithmic approach rather than brute-force solving. To ensure a comprehensive evaluation, we present the puzzles in both multiple-choice and open-ended question answering formats.

Our findings indicate that despite their sophisticated capabilities in standard benchmarks, current models still struggle with seemingly simple multimodal puzzles (\Cref{fig:case_intro}). 
Contrary to previous benchmarks such as ARC-AGI, we observe a less dramatic reasoning curve without extreme jumps in performance.
This limitation highlights the substantial gap between current artificial intelligence and human-like reasoning abilities. 
As the models continue to rapidly advance and scale as in Figure \ref{fig:visual_trends}, this benchmark will serve as a critical indicator of progress toward more robust and generalized artificial intelligence. Overall, here are the key findings of our study:

\begin{tcolorbox}[colback=blue!5!white, colframe=NotSureBlue, 
                  title=\textbf{TL;DR}, 
                  fonttitle=\bfseries, 
                  coltitle=white, 
                  boxrule=0.5mm, 
                  arc=2mm, 
                  left=2mm, 
                  right=2mm,
                  top=2mm,
                  bottom=2mm,
                  ]
\begin{enumerate}[label=\textbf{\arabic*.}, leftmargin=*, wide]
    \item Performance steadily improves from GPT-4-Turbo to GPT-4o to o1. While the jump from GPT-4-Turbo to GPT-4o is moderate, the transition from GPT-4o to o1 marks a significant advancement but it comes at a cost of 750x more inference cost.
    \item Although o1 exhibits a notable improvement in reasoning performance, it still falls far short of human performance on the simple visual abstract reasoning dataset, \abstractdataset{}.
    \item GPT-4-Turbo and GPT-4o both face significant bottlenecks in perception and inductive reasoning.
    \item o1's primary bottleneck lies in perception. With ground truth perception provided, o1 shows strong reasoning capabilities, outperforming GPT-4-Turbo and GPT-4o by 18-20\%.
    \item In particular, o1 struggles with reasoning based on visual shapes and sizes.
    \item As the complexity of multimodal puzzles increases—for instance, in puzzles from \algodataset{} or dual-concept puzzles in \abstractdataset{} that combine multiple dimensions such as colors and numbers—all models experience a noticeable performance decline.
\end{enumerate}
\end{tcolorbox}








\section{\abstractdataset{} \& \algodataset{}}


Understanding the capabilities and limitations of large multimodal models in visual reasoning tasks requires datasets that challenge their cognitive capabilities in nuanced ways. 
In this study, we employ \abstractdataset{} \citep{chia2024puzzlevqadiagnosingmultimodalreasoning} and \algodataset{} \citep{ghosal2024languagemodelspuzzleprodigies} to evaluate abstract visual reasoning and algorithmic problem-solving capabilities.

Multimodal puzzles serve as a crucial benchmark for evaluating large multimodal models because they require a unique combination of perception, reasoning, and abstraction.
Unlike other abstract reasoning benchmarks such as ARC-AGI, where test examples are input to the model as textual context, multimodal puzzles requires the integration of visual and textual information to solve the problem.
They also provide an ideal setting for probing systematic reasoning and generalization, as their structured yet diverse nature tests the abilities to infer patterns and apply them across novel contexts.

Examples from \abstractdataset{} and \algodataset{} are 
shown in \Cref{fig:abstract_examples} and \Cref{fig:algo_examples}. 
These two datasets were chosen for their complementary characteristics: while \textsc{PuzzleVQA} emphasizes basic visual abstract reasoning, requiring pattern recognition to solve puzzles, \textsc{AlgoPuzzleVQA} features more complex puzzles that demand deducing algorithms for their solutions.












\subsection{\abstractdataset{} Composition}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{images/AbstractOntology.pdf}
    \caption{Example single-concept and dual-concept abstract puzzles in \abstractdataset{}, designed around fundamental concepts such as numbers, colors, size, and shapes.}
    \label{fig:abstract_examples}
\end{figure*}

\abstractdataset{} consists of 2,000 test instances, organized into 10 puzzle categories. 
Four of these categories focus on single-concept patterns, such as numbers, colors, sizes, and shapes, while the remaining six categories emphasize dual-concept patterns, which combine two distinct concepts.
We present some puzzle examples in \Cref{fig:abstract_examples}.
Each category includes two multimodal templates, with each template capable of generating a variety of unique puzzle instances.

Each puzzle is formulated with the following core components:
\begin{enumerate}[topsep=0pt, itemsep=2pt, parsep=1pt]
    \item \textbf{Objects:} Conceptual elements like numbers, colors, shapes, and sizes.
    \item \textbf{Layout:} The spatial arrangement of objects, which provides necessary visual context.
    \item \textbf{Pattern:} The underlying rules governing object interactions (e.g., spatially opposite parts must share the same color).
    \item \textbf{Demonstrations:} Multiple instances of interacting objects that represent the underlying pattern.
    \item \textbf{Query:} A natural language question prompting the model to solve the puzzle by reasoning about the missing element.
\end{enumerate}

For clarity, a detailed example of puzzle formulation is provided in \Cref{app:abstract_components}. 
Using these components, 100 unique puzzle instances were generated from each multimodal template, resulting in a total of 2,000 test instances.

\abstractdataset{} is designed to evaluate the reasoning capabilities of large multimodal models, focusing on their ability to interpret abstract patterns that require both visual and textual understanding.
By encompassing a diverse range of single- and dual-concept puzzles, the dataset aims to reveal the strengths and weaknesses of current large multimodal models.
























\subsection{\algodataset{} Composition}



\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/Teaser_Visual.pdf}
    \includegraphics[width=0.9\linewidth]{images/Teaser_Algorithm.pdf}
    
    \caption{Example of puzzles from \algodataset{} with visual features represented in the top row and algorithmic features in the bottom two rows. For each feature, at least one puzzle instance from each category is presented. Note that the header categories are not exhaustive, as some puzzles may belong to additional categories not listed in the headers. The complete categorization can be found in \Cref{app:algo_ontology}.}
    \label{fig:algo_examples}
\end{figure*}




\algodataset{} consists of 18 distinct puzzles, each with 100 test instances, resulting in a total of 1,800 test instances.
These puzzles cover a wide range of topics, combining both visual and algorithmic categories.
Each puzzle includes at least one visual category and one algorithmic category.
We present some puzzle examples in \Cref{fig:algo_examples}.

\textbf{Visual categories:}
\begin{enumerate}[topsep=0pt, itemsep=2pt, parsep=1pt]
    \item \textbf{Colors:} Puzzles where understanding the colour of the puzzle components is crucial for solving the question.
    \item \textbf{Position:} In some puzzles, understanding spatial positioning of the puzzle components is necessary for solving the question.
    \item \textbf{Shape/Size:} This category includes the understanding of both absolute and relative shapes and sizes of the puzzle components.
    \item \textbf{Text:} Certain puzzles incorporate optical characters or embedded text that provide important information that must be used to correctly solve the question.
\end{enumerate}

\textbf{Algorithmic categories:}
\begin{enumerate}[topsep=0pt, itemsep=2pt, parsep=1pt]
    \item \textbf{Arithmetic:} These puzzles require basic mathematical operations, such as addition, multiplication, counting, and modular arithmetic, to solve the problem.
    \item \textbf{Boolean Logic:} Some puzzles require the application of Boolean logic, such as checking conditions like equality or inequality between different components or states.
    \item \textbf{Combinatorics:} These puzzles involve counting combinations and permutations of the components or states. The questions typically ask about the number of unique configurations that can be achieved after performing a sequence of operations.
    \item \textbf{Graphs:} Puzzles in this category can be represented as graph data structures, where graph algorithms can be applied to find the solution.
    \item \textbf{Optimization:} Optimization puzzles focus on finding the best solution, whether it involves minimizing time, steps, or maximizing a given outcome (e.g., summation or sorting).
    \item \textbf{Search:} These puzzles require the use of search algorithms, including breadth-first search or exhaustive search, to explore possible solutions or configurations.
    \item \textbf{Sets:} In these puzzles, solving the problem requires considering the identical nature of some objects and the equivalence of some positions or configurations.
\end{enumerate}

The algorithmic categories are not mutually exclusive, and puzzles may contain two or more categories in order to derive the answer.
The primary goal of \algodataset{} is to assess the gap between visual data interpretation and algorithmic problem-solving skills. 
The puzzles are designed to challenge and evaluate large multimodal models, testing their ability to solve algorithmic problems that require visual understanding, language comprehension, and complex algorithmic reasoning.


\section{Experimental Setup}


\subsection{Evaluation Pipeline}

To ensure a comprehensive evaluation, we present the puzzles to the models in both \textbf{multiple-choice} and \textbf{open-ended} formats. 
The original datasets consist of puzzles in a multiple-choice format.
Below, we provide a detailed explanation of both the multiple-choice and open-ended setups.

\subsubsection{Multiple Choice Setup} \label{sec:mcq_setup}

\paragraph{(First stage) CoT Prompting.} We leverage zero-shot chain of thought (CoT) prompting \citep{kojima2022large} with a prompt similar to \code{``Let's think step by step''} to elicit reasoning steps from GPT-[n] models. 
For the o-[n] model, we do not use CoT prompting since these models are trained to perform reasoning internally.
If the letter answer can be extracted during the first prompting stage with regular expressions, we skip the second stage.
However, if the letter answer cannot be extracted, we proceed to the answer extraction stage.

\paragraph{(Second stage) Answer Extraction.}
We take the initial prompt from the first stage and the generated output, then append the text \code{"Therefore, among (A) (B) (C) (D), the answer is:"} for puzzles with four options, or \code{"Therefore, among (A) (B) (C), the answer is:"} for puzzles with three options.
This allows us to extract the final letter answer and compare it to the ground truth. 
The accuracy of predicting the correct final answer is used as the evaluation metric.


\subsubsection{Open Ended Setup}
\paragraph{(First stage) CoT Prompting.} 
Similar to the setup described in \Cref{sec:mcq_setup}, we use CoT prompting for GPT-[n] models. However, for o-[n] models, we do not use CoT prompting. In the open-ended setup, instead of performing answer extraction, we use GPT-4o to directly match the generated answer with the ground truth answer.

\paragraph{(Second stage) Answer Matching.}
For open-ended responses, we use GPT-4o to compare the generated responses from the first stage with the ground truth answers. Specifically, GPT-4o is prompted to evaluate whether the generated response aligns with the ground truth answer. The exact prompt used for this evaluation is provided in \Cref{app:gpt4o_matching}.
Similar to the multiple-choice setup, the accuracy of predicting the correct final answer is used as the evaluation metric.






\subsection{Models}

We investigate the performance of GPT-[n] and o-[n] models: (1) GPT-4-Turbo (turbo-2024-04-09), (2) GPT-4o (2024-08-06), (3) o1 (2024-12-17). We selected these two model series from OpenAI due to their rapid advancements and significant contributions to the field of large language models (LLMs). Each version has introduced innovative techniques that have shaped the LLM landscape. For example, GPT-4-Turbo has set benchmarks in understanding visual inputs, while GPT-4o is a highly efficient model designed for multimodal inputs and outputs. The o1 model, a recent addition, is trained with a step-by-step reasoning objective and reinforcement learning, making it a powerful reasoner capable of handling a wide range of tasks effectively. \textit{We use the ``high'' reasoning mode for o1.} Please note that our study can easily be expanded to other closed-sourced and open-sourced models.

\begin{table*}[t]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{ll|>{\centering\arraybackslash}p{2.1cm}>{\centering\arraybackslash}p{2.1cm}>{\centering\arraybackslash}p{2.1cm}|>{\centering\arraybackslash}p{2.1cm}>{\centering\arraybackslash}p{2.1cm}>{\centering\arraybackslash}p{2.1cm}}
\toprule[1pt]
&  & \multicolumn{3}{c|}{\textbf{\textsc{Open Ended}}} & \multicolumn{3}{c}{\textbf{\textsc{Multi Choice}}} \\
\cmidrule(lr{2pt}){3-5} \cmidrule(lr{2pt}){6-8}
\multirow{13}{*}{\rotatebox[origin=c]{90}{ \textsc{\textbf{PuzzleVQA}} }} & & \textbf{GPT-4-Turbo} & \textbf{GPT-4o} & \textbf{o1} & \textbf{GPT-4-Turbo} & \textbf{GPT-4o} & \textbf{o1} \\

\midrule
&Colors        &  51.0   &  72.5   &   80.5   & 42.5      & 77.0      & 91.5 \\
&Numbers    &   82.5   &   84.5  &    96.5    & 85.0      & 87.0      & 99.0 \\
&Shapes      &   32.5   &   51.5  &    54.5   &  59.5     & 71.0      & 66.5 \\
&Size       &   19.0   &   39.0  &     54.5     & 37.5      & 44.0      & 77.5 \\

\greyrule{2}{8}

&Colors \& Numbers     &  54.5    &  48.0   &   97.0      & 64.5   & 64.5    & 99.5 \\
&Colors \& Shapes     &  30.0   &   45.5  &      75.0      & 61.5 & 66.0       & 80.5 \\
&Colors \& Size        &  31.5    &  21.5   &     30.0       & 50.0 & 58.5        & 50.0 \\
&Numbers \& Shapes   &   31.5    &  20.0   &     78.0      & 54.5 & 55.5    & 92.5 \\
&Numbers \& Size     &   24.5    &   34.5  &     41.5       & 32.5 & 30.5       & 49.0 \\
&Size \& Shapes       &   28.5   &   50.5  &     55.0      & 55.0 & 60.5       & 86.5 \\

\midrule
&\textbf{Average}   &   38.6   &   46.8  &    66.3    & 54.2 & 60.6 & 79.2 \\

\midrule
\multirow{19}{*}{\rotatebox[origin=c]{90}{ \textsc{\textbf{AlgoPuzzleVQA}} }}
&Board Tiling     &   46.0   &  46.0   &      51.0       & 49.0 & 52.0 & 47.0 \\
&Calendar         &    43.0   &  52.0   &      83.0        & 63.0 & 66.0 & 92.0 \\
&Chain Link       &    1.0   &   3.0  &         1.0        &  29.0& 39.0 & 61.0 \\
&Checker Move     &    3.0   &   7.0  &        34.0       &  25.0 & 30.0 & 52.0 \\

\greyrule{2}{8}

&Clock           &   0.0    &   3.0  &     6.0     & 27.0 & 33.0 & 83.0 \\
&Colour Hue       &   5.0    &   10.0  &    15.0     & 36.0 & 28.0 & 23.0 \\
&Map Colour       &   10.0    &  22.0   &    21.0       & 38.0 & 49.0 & 50.0 \\
&Maze Solve      &    16.0    &  8.0   &      17.0    & 40.0  & 47.0 & 50.0 \\

\greyrule{2}{8}

&Move Box        &   20.0    &   23.0  &      23.0     & 36.0 & 36.0 & 30.0 \\
&N-Queens        &   17.0   &   16.0  &      16.0     & 35.0 & 35.0 & 20.0 \\
&Number Slide     &   14.0   &  32.0   &      71.0      &  45.0 & 46.0 & 89.0 \\
&Rotten Fruits    &   32.0   &   53.0  &    43.0    &  36.0 & 56.0 & 56.0 \\

\greyrule{2}{8}

&Rubik's Cube     &   32.0    &  44.0   &      54.0     & 52.0 & 48.0 & 74.0 \\
&Think A Dot      &    36.0   &  41.0   &      32.0      & 47.0 & 50.0 & 60.0 \\
&Tower of Hanoi    &   0.0   &   2.0  &      39.0     & 15.0 & 35.0 & 68.0 \\

\greyrule{2}{8}

&Water Jugs          &  8.0    &  23.0   &     42.0      &  29.0 & 68.0 & 49.0 \\ 
&Wheel of Fortune     &   14.0    &   29.0  &      31.0     & 40.0  & 44.0 & 67.0 \\
&Wood Slide          &   0.0     &   1.0  &       0.0       & 15.0 & 23.0 & 25.0 \\

\midrule
&\textbf{Average}   &   16.5    &   23.1  &    32.2        &  36.5 & 43.6 & 55.3 \\

\bottomrule
\end{tabular}
}
\caption{Accuracy scores of GPT-[n] and o-[n] models on \abstractdataset{} and \algodataset{}.}
\label{tab:results-expanded}
\end{table*}

\section{Results}

\subsection{Scaling Trends}

To investigate the evolution of reasoning performance, we present the average accuracy on \abstractdataset{} and \algodataset{} over time, along with the inference cost per puzzle, as shown in Figure \ref{fig:visual_trends}.
The reported accuracy corresponds to the open-ended setting, while the inference cost per puzzle is estimated based on the average API cost for processing 200 puzzle questions.
We observe a more significant jump in performance from the GPT-[n] to o-[n] models, highlighting their enhanced reasoning capabilities.
However, this reasoning advancement comes at a more than 750x inference cost compared to GPT-4o, likely due to more extensive reasoning steps or hidden processes \citep{wei2022chain}. 


\subsection{Expanded Evaluation}

To assess the holistic reasoning capabilities of multimodal models, we present results for both open-ended and multiple-choice answer formats in \Cref{tab:results-expanded}. 
Overall, we observe that all models generally perform better in the multiple-choice setting compared to the open-ended setting. 
Particularly, the o1 model experiences the largest performance decline, with a 23.1\% drop in score on \algodataset{} between multiple-choice and open-ended settings. 
Conversely, the o1 model shows the smallest performance decline on \abstractdataset{}, with a score drop of 12.9\% between the two formats.


\paragraph{\abstractdataset{}.} 
\abstractdataset{} is a relatively simple dataset designed to evaluate the abstract reasoning abilities of large multimodal models. 
According to \citep{chia2024puzzlevqadiagnosingmultimodalreasoning}, human performance on a subset of this dataset in the multiple-choice setting reaches a score of 91.4\%.
However, GPT-[n] and o-[n] models still fall significantly short of this benchmark, with o1 achieving the highest score among them at 79.2\% in the multiple-choice setting.
Among single-concept puzzles, most models find the size and shape categories to be the most challenging.
Specifically in the multiple-choice setting, o1 achieves scores of only 66.5\% for shapes and 77.5\% for size categories, in contrast to other single-concept categories like colors and numbers, which achieve significantly higher scores of 91.5\% and 99.0\%, respectively.
Performance declines further in dual-concept puzzles compared to single-concept ones.
In particular, models struggle more with those involving combinations such as Numbers \& Size, Size \& Shapes, and Colors \& Size. 
The lowest scores were observed with the Numbers \& Size puzzle, where GPT-4-Turbo, GPT-4o, and o1 achieved only 32.5\%, 30.5\%, and 49.0\%, respectively.
Overall, we observe a moderate improvement in performance from GPT-4-Turbo to GPT-4o and a substantial leap from GPT-4o to o1 across majority puzzle categories in \abstractdataset{}. 
This highlights the effectiveness of the specialized reasoning enhancements introduced in o1. 
Another key finding is the inability of GPT-4-Turbo to effectively perceive and reason with colors—a challenge that GPT4o and o1 overcome to some extent, outperforming GPT-4-Turbo by approximately 22\% to 29\% in this area in the open-ended setting. 
Additionally, o1 demonstrates superior performance in numerical reasoning and puzzles within the size category. 
Interestingly, o1 does not improve GPT-4o's performance on puzzles requiring reasoning about shapes. 
In fact, in the multiple-choice setting, o1 underperforms GPT-4o for the shapes category by 4.5\%. 






\paragraph{\algodataset{}.}
\algodataset{} is a more challenging visual puzzle reasoning dataset that demands algorithmic problem-solving abilities. 
The performance of all models remains relatively low on this dataset, achieving a score of 36.5\%, 43.6\%, and 55.3\% for GPT-4-Turbo, GPT-4o, and o1 respectively in the multiple-choice setting. 
However, similar to \abstractdataset{}, we observe a notable improvement in performance with o1 compared to GPT-4-Turbo and GPT-4o.
Specifically for puzzles such as calendar, clock, and number slide in the multiple-choice setting, o1 demonstrates significant improvements over GPT-4o, with performance gains of 26\%, 50\%, and 43\%, respectively.
In the open-ended setting, we observe a significant drop in performance compared to the multiple-choice setting, particularly on puzzles like Chain Link and Wood Slide, where performance across all models is close to 0\%.
For example, o1 achieves scores of 1.0\% on Chain Link and 0.0\% on Wood Slide in the open-ended setting, while it achieves 61.0\% and 25.0\%, respectively, in the multiple-choice setting.
On the other hand, o1 performs well on puzzles like Calendar and Number Slide, reaching scores of 92\% and 89\%, showing a considerable improvement over GPT-4o, which scores 66\% and 46\%, respectively.



\begin{figure*}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{images/clock_case.png}
    \caption{Case study on Clock in \algodataset{} on multiple-choice and open-ended setting.}
    \label{fig:case_clock}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{images/chain_link_case1.png}
    \includegraphics[width=1.0\textwidth]{images/chain_link_case2.png}
    \caption{Case study on Chain Link in \algodataset{} on multiple-choice and open-ended setting.}
    \label{fig:case_chain_link}
\end{figure*}


\subsection{Discussions}
\paragraph{Multiple Choice vs Open Ended Problems.}
We evaluate the models in two settings: a multiple-choice setup and an open-ended setup. 
As shown in \Cref{tab:results-expanded}, all models experience a significant performance decline in the open-ended setup. 
In \abstractdataset{}, the average drop is relatively mild (ranging from 8 to 15\%), whereas in \algodataset{}, the average decline is more pronounced (ranging from 20 to 28\%), indicating the increased difficulty of the task.

Specifically, for the Clock puzzle in \algodataset{}, we observed a 77\% drop in accuracy. 
To illustrate this, we present a case study in \Cref{fig:case_clock}.
Clock puzzles require fine-grained visual perception, as accurately determining the exact time displayed on the clock is crucial for answering correctly. 
In the open-ended setting, we found that o1 lacked the necessary precision in visual perception, misreading the clock as 2:45 instead of 2:43. 
Although the underlying concept was correctly applied, this small discrepancy led to an incorrect answer.
However, in the multiple-choice format, the presence of the answer choices provided a helpful cue, guiding the model toward greater precision in interpreting the clock. 
As a result, it correctly selected option (C) 4:23.

Additionally, we present another case study on the Chain Link puzzle, where o1 scored only 1\% in the open-ended setting compared to 61\% in the multiple-choice setting (\Cref{fig:case_chain_link}).
In the multiple-choice setting, although the model selected the correct answer, its reasoning was incorrect. 
The correct solution to this problem requires 4 cuts and 7 joins, totaling 34 minutes. 
However, o1 incorrectly outputted 2 cuts and 12 joins. 
Despite this mistake, the sum still resulted in 34, which coincidentally matched the correct answer.
Notably, the model seemed to settle on 34 early in the reasoning process rather than considering a larger value, such as 69. 
This could be because 69 was not among the provided answer choices. 
Interestingly, in the open-ended setting where multiple-choice options were not provided, the model instead arrived at an answer of 69.










\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/number_slide_case.png}
    \caption{Case study on Number Slide in \algodataset{} across GPT-[n] and o-[n] models.}
    \label{fig:case_number_slide}
\end{figure*}


\paragraph{Gap between GPT-[n] and o-[n] models.}
Based on the results in \Cref{tab:results-expanded}, we observed a significant performance gap between the GPT-[n] and o-[n] models. 
One particular case is the Number Slide puzzle, where the o1 model achieves 71\% accuracy, while GPT-4o and GPT-4-Turbo score only 32\% and 14\%, respectively.
To further analyze this discrepancy, we present a case study on a specific test instance of the Number Slide puzzle in the open-ended setting (\Cref{fig:case_number_slide}). 
Both GPT-4o and GPT-4-Turbo incorrectly perceive the open position as being in the bottom-right corner of the 4×4 grid, leading to the wrong answer. 
In contrast, the o1 model accurately interprets the visual layout, correctly identifying the blank space in row 2, column 4.
This demonstrates its superior visual perception capabilities, allowing it to ultimately arrive at the correct answer.



















\begin{table*}[t]
% \small
\centering
\resizebox{1.0\textwidth}{!}{
% \begin{tabular}{ll|cccc|cccc}
\begin{tabular}{ll|>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}|
                  >{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}|
                  >{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}}
\toprule[1pt]
&  & \multicolumn{9}{c}{\textbf{\textsc{Open Ended}}}  \\
\cmidrule{3-11}
&  & \multicolumn{3}{c|}{\textbf{GPT-4-Turbo}} & \multicolumn{3}{c|}{\textbf{GPT-4o}} & \multicolumn{3}{c}{\textbf{o1}} \\

\multirow{13}{*}{\rotatebox[origin=c]{90}{ \textsc{\textbf{PuzzleVQA}} }} & & \textbf{Original} & \textbf{w/ \textit{p.}} & \textbf{w/ \textit{p.} \& \textit{i.}} & \textbf{Original} & \textbf{w/ \textit{p.}} & \textbf{w/ \textit{p.} \& \textit{i.}} & \textbf{Original} & \textbf{w/ \textit{p.}} & \textbf{w/ \textit{p.} \& \textit{i.}} \\

\midrule
&Colors        &  51.0    &   75.0   &  97.0    &  72.5     &    80.0    &    92.0     &   80.5    &   94.0   &     99.0     \\
&Numbers    &   82.5    &  77.0  &     98.5    &  84.5    &      88.5    &     99.5      &  96.5     &  98.0    &     97.0     \\
&Shapes      &   32.5    &   71.5  &    97.5     &  51.5    &    63.5     &      97.5     &  54.5    &   55.5   &      100.0     \\
&Size       &   19.0    &  64.5   &    95.5     & 39.0   &       62.5    &     96.5       &  54.5     &   98.0   &      100.0      \\

\greyrule{2}{11}

&Colors \& Numbers     &    54.5    &   67.0   &    89.5    &   48.0     &    52.0      &   89.5       &  97.0    &   95.0   &    100.0        \\
&Colors \& Shapes     &   30.0    &   81.0  &     64.5    &  45.5    &    77.5     &     77.0      &   75.0    &   81.5   &        89.5      \\
&Colors \& Size        &   31.5    &   53.5   &   75.5     &  21.5    &    78.0     &     94.5      &   30.0    &  99.0    &       94.0       \\
&Numbers \& Shapes   &   31.5    &   29.5   &    84.5    &  20.0    &      33.5   &      85.5     &  78.0   &    86.0   &      91.0      \\
&Numbers \& Size     &   24.5   &   70.0  &    63.0    &  34.5    &     73.0    &     73.5     &   41.5    &   81.5   &       77.5        \\
&Size \& Shapes       &    28.5    &   97.5  &    93.0    &  50.5    &   92.5     &      92.5     &  55.0     &    98.0  &     99.5         \\

\midrule
&\textbf{Average}   &   38.6   &  68.6   &    85.8   &  46.8    &     70.1    &     89.8     &  66.2   &   88.6   &   94.8    \\



\bottomrule
\end{tabular}
}
\caption{Bottleneck analysis of GPT-[n] and o-[n] models on \abstractdataset{}. \textbf{Original} refers to our main setting where only a question and an image are provided as input. To reveal the specific multimodal reasoning bottlenecks, we progressively inject ground-truth explanations in the input for visual perception (\textbf{\textit{p.}}) and inductive reasoning (\textbf{\textit{i.}}). We provide an example of the different prompts used in the bottleneck analysis in \Cref{fig:bottleneck}.}
\label{tab:results-expanded2}
\end{table*}






















\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/circle_number_size_case.png}
    \caption{Case study on an abstract puzzle from the Numbers \& Size category in \abstractdataset{}.}
    \label{fig:case_circle_number_size}
\end{figure*}



\paragraph{Reasoning Bottlenecks.}

To analyze reasoning bottlenecks, following \citet{chia2024puzzlevqadiagnosingmultimodalreasoning}, we incrementally provided the models with additional ground truth guidance, namely perception and induction steps (\Cref{app:bottleneck}).
We conducted this analysis on the open-ended setting and the detailed results of this experiment are reported in \Cref{tab:results-expanded2}.
We observe that perception is the primary bottleneck across all models. 
By injecting visual details of the puzzle in the input prompt as guidance, the results improve by 22\% to 30\% for all models.
GPT-4-Turbo and GPT-4o show weaknesses in inductive reasoning as we observe that after injecting inductive reasoning in the input prompt, the performance improves by a further 16\% to 19\% over the original with perception setting.
Furthermore, the inductive reasoning superiority of GPT-4o over GPT-4-Turbo on \abstractdataset{} is minimal, as GPT-4o only outperforms GPT-4-Turbo by 1.5\% even with visual perception guidance.
On the other hand, o1 demonstrates strong inductive reasoning capabilities, its performance improves only moderately by 6\% with visual perception and inductive reasoning guidance, suggesting that perception is its primary limitation. 
With accurate visual perception guidance, o1 can effectively perform inductive reasoning and achieve a high score.

However, there are still instances where o1 fails, even with perception and induction step guidance (\Cref{fig:case_circle_number_size}). 
In the original open-ended setting, o1 reasons that the puzzle’s pattern involves pairing opposite circles and summing their values. 
When additional visual details are provided in the perception setting, o1 instead concludes that the pattern is based on the sums of adjacent numbers. 
Even in the perception and induction setting, where both visual details and the underlying pattern are explicitly provided, o1 still incorrectly interprets the numbers as primes. 
In each case, these misinterpretations lead to an incorrect final answer due to an inaccurate prediction of the underlying pattern.













\section{Conclusion}
In this study, we evaluated and analyzed the multimodal reasoning capabilities of GPT-[n] and o-[n] models on \abstractdataset{} and \algodataset{}.
Our experiments highlight significant improvements in multimodal reasoning performance from GPT-[n] to o-[n] models, with o1 demonstrating the most substantial gains. 
However, these advancements come at a considerably higher inference cost. 
Across both multiple-choice and open-ended settings, models consistently perform better in the multiple-choice setting, with o1 experiencing the largest performance drop on \algodataset{} in the open-ended setting. 
In \abstractdataset{}, o1 outperforms previous models, especially in numerical reasoning tasks, although it still faces difficulties with shape-related puzzles. 
Similarly, in the more challenging \algodataset{}, overall performance remains low, but o1 demonstrates significant improvements over GPT-4o, particularly in puzzles like Number Slide and Calendar.
Despite these advancements, visual perception remains a key limitation across all models. 
Providing explicit details about visual perception significantly improves performance, highlighting that accurately interpreting visual input is still a major challenge. 
While o1 demonstrates strong inductive reasoning abilities, its dependence on precise perception suggests that further improvements in visual understanding are needed.
















\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}
\newpage

\appendix


\section{\abstractdataset{} Details} 

\subsection{\abstractdataset{} Components} \label{app:abstract_components}
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/AbstractComponents.pdf}
    \caption{Illustration example of components for abstract puzzles in \abstractdataset{}. To construct each puzzle instance, we first define the layout and pattern of a multimodal template, and populate the template with suitable objects that demonstrate the underlying pattern.}
\end{figure*}



\subsection{\abstractdataset{} Statistics} \label{app:abstract_statistics}

We report the dataset statistics of \abstractdataset{} in \Cref{tab:abstract_statistics}.

\begin{table}[ht!]
\centering
\resizebox{0.35\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
Puzzle & Multimodal & Test \\
Category & Templates & Instances \\
\midrule
Numbers & 2 & 200 \\
Colors & 2 & 200 \\
Shapes & 2 & 200 \\
Size & 2 & 200 \\
Numbers \& Shapes & 2 & 200 \\
Numbers \& Colors & 2 & 200 \\
Numbers \& Size & 2 & 200 \\
Shapes \& Colors & 2 & 200 \\
Shapes \& Size & 2 & 200 \\
Colors \& Size & 2 & 200 \\
\midrule
Total & 20 & 2000 \\
\bottomrule
\end{tabular}
}
\caption{Dataset statistics of \abstractdataset{}.}
\label{tab:abstract_statistics}
\end{table}





\section{\algodataset{} Details} 

\subsection{\algodataset{} Ontology} \label{app:algo_ontology}

\begin{table*}[ht]
\small
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cccc|ccccccc}
\toprule[1pt]
\multirow{2}{*}{\textbf{Puzzle}} & \multicolumn{4}{c|}{\textbf{Visual Features}} & \multicolumn{7}{c}{\textbf{Algorithmic Features}}\\
& Colour & Position & Shape/Size & Text & Arithmetic & Boolean Logic & Combinatorics & Graphs  & Optimization & Search & Sets \\
\midrule 
Board Tiling     & \greencheck & \greencheck & \redcross   & \redcross   & \greencheck & \greencheck & \redcross   & \redcross   & \redcross   & \redcross   & \redcross   \\
Calendar         & \redcross   & \greencheck & \redcross   & \greencheck & \greencheck & \redcross   & \redcross   & \redcross   & \redcross   & \redcross   & \redcross   \\
Chain Link       & \redcross   & \greencheck   & \greencheck & \redcross   & \greencheck & \redcross   & \redcross   & \redcross   & \greencheck & \redcross   & \greencheck \\
Checker Move     & \greencheck & \greencheck & \redcross   & \redcross   & \greencheck & \greencheck & \redcross   & \redcross   & \redcross   & \greencheck & \redcross   \\

\greyrule{1}{12}

Clock            & \redcross   & \greencheck & \greencheck & \greencheck & \greencheck & \redcross   & \redcross   & \redcross   & \redcross   & \redcross   & \redcross   \\
Colour Hue       & \greencheck & \greencheck & \redcross   & \redcross   & \greencheck & \redcross   & \redcross   & \redcross   & \greencheck & \redcross   & \redcross   \\
Map Colour       & \greencheck & \greencheck & \greencheck & \greencheck & \greencheck & \greencheck & \greencheck & \greencheck & \redcross   & \greencheck & \redcross   \\
Maze Solve       & \greencheck & \greencheck & \redcross   & \redcross   & \greencheck & \redcross   & \redcross   & \greencheck & \greencheck & \greencheck & \redcross   \\
\greyrule{1}{12}

Move Box         & \greencheck & \greencheck & \redcross   & \redcross   & \greencheck & \redcross   & \redcross   & \greencheck & \greencheck & \greencheck & \redcross   \\
N-Queens         & \redcross   & \greencheck & \redcross   & \redcross   & \greencheck & \greencheck & \redcross   & \redcross   & \redcross   & \greencheck & \redcross   \\
Number Slide     & \redcross   & \greencheck & \redcross   & \greencheck & \greencheck & \redcross   & \greencheck & \redcross & \greencheck & \greencheck & \greencheck \\
Rotten Fruits    & \redcross   & \greencheck & \redcross   & \redcross   & \greencheck & \greencheck & \redcross   & \greencheck  & \greencheck & \greencheck & \redcross   \\
\greyrule{1}{12}

Rubik's Cube     & \greencheck & \greencheck  & \redcross  & \redcross   & \greencheck & \redcross   & \redcross   & \greencheck & \redcross   & \redcross   & \redcross   \\
Think A Dot      & \greencheck & \greencheck & \greencheck & \redcross   & \greencheck & \greencheck & \redcross   & \redcross   & \redcross   & \redcross   & \greencheck \\
Tower of Hanoi   & \redcross   & \greencheck & \greencheck & \redcross   & \greencheck & \greencheck & \redcross   & \redcross   & \greencheck & \greencheck & \redcross   \\
\greyrule{1}{12}

Water Jugs       & \redcross   & \greencheck   & \greencheck & \greencheck & \greencheck & \greencheck & \redcross   & \redcross   & \greencheck & \greencheck & \redcross   \\
Wheel of Fortune & \redcross   & \greencheck & \greencheck & \greencheck & \greencheck & \redcross   & \redcross   & \redcross   & \redcross   & \redcross   & \redcross   \\
Wood Slide       & \redcross   & \greencheck & \greencheck & \redcross   & \greencheck & \redcross   & \redcross   & \redcross & \greencheck & \greencheck & \redcross   \\
\bottomrule
\end{tabular}
}
\caption{Ontological categorization of the puzzles in \algodataset{}.}
\label{tab:algo_ontology}
\end{table*}



\section{GPT-4o Evaluation Prompt} \label{app:gpt4o_matching}

\begin{tcolorbox}[colback=gray!5, colframe=black, title=GPT-4o Evaluation Prompt]
\footnotesize
\begin{MyVerbatim}
Evaluate the candidate answer against the correct answer. If the candidate answer is correct, output [correct]; otherwise, output [incorrect].

Question: {question}
Candidate Answer: {candidate_answer} 
Correct Answer: {correct_answer}
Evaluation: 
\end{MyVerbatim}
\end{tcolorbox}






\section{\abstractdataset{} Bottleneck Analysis Setup} \label{app:bottleneck}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/bottleneck.png}
    \caption{An example of prompts used in the bottleneck analysis: Perception includes the visual details of the puzzle, while induction includes an explanation of the underlying pattern within it.}
    \label{fig:bottleneck}
\end{figure*}

\end{document}
