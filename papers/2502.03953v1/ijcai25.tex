%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{subcaption}

\newcommand{\Env}{\mathit{Env}}
\newcommand{\Sys}{\mathit{S}}
\newcommand{\probP}{\text{I\kern-0.15em P}}

\newtheorem{definition}{Definition}[section]
\renewcommand{\thedefinition}{\arabic{definition}}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Fairness Aware Reinforcement Learning via Proximal Policy Optimization}


% Single author syntax
% \author{
    
%     \affiliations
    
%     \emails
    
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Gabriele La Malfa$^1$
\and
Jie M. Zhang$^1$\and
Michael Luck$^2$\And
Elizabeth Black$^1$\\
\affiliations
$^1$King's College London\\
$^2$University of Sussex\\
\emails
{gabriele.la\_malfa@kcl.ac.uk,
jie.zhang@kcl.ac.uk,
michael.luck@sussex.ac.uk,
elizabeth.black@kcl.ac.uk
}
}
% \fi

\begin{document}

\maketitle

\begin{abstract}
Fairness in multi-agent systems (MAS) focuses on equitable reward distribution among agents in scenarios involving sensitive attributes such as race, gender, or socioeconomic status. This paper introduces fairness in Proximal Policy Optimization (PPO) with a penalty term derived from demographic parity, counterfactual fairness, and conditional statistical parity. The proposed method balances reward maximisation with fairness by integrating two penalty components: a retrospective component that minimises disparities in past outcomes and a prospective component that ensures fairness in future decision-making.
We evaluate our approach in the Allelopathic Harvest game, a cooperative and competitive MAS focused on resource collection, where some agents possess a sensitive attribute. Experiments demonstrate that fair-PPO achieves fairer policies across all fairness metrics than classic PPO. Fairness comes at the cost of reduced rewards, namely the Price of Fairness, although agents with and without the sensitive attribute renounce comparable amounts of rewards. Additionally, the retrospective and prospective penalties effectively change the agents' behaviour and improve fairness. These findings underscore the potential of fair-PPO to address fairness challenges in MAS.\footnote{The code of the experiments is available here: \url{https://anonymous.4open.science/r/allelopathic-harvest-F065}.}
\end{abstract}

\section{Introduction}~\label{Introduction}  
% Paragraph 1
In Multi-Agent Systems (MAS), agents interact in an environment to pursue individual or shared goals. Fairness in MAS focuses on whether the reward distribution mechanisms, driven by agent decisions or other processes, treat agents fairly. For instance, fair reinforcement learning explores methods to promote fairness by enabling agents to learn a fair policy~\cite{Reuel2024}; fair division addresses fair resource allocation~\cite{Lindner2016,Amantidis2023}; negotiation designs methods for fair bargaining resolution~\cite{Guth2014,Debove2016}.

% Paragraph 2
On the other hand, in human society, fairness is framed in terms of inequality or discrimination between privileged and disadvantaged groups. Sensitive attributes, such as race, gender and socioeconomic status, define subgroups historically marginalised in workplaces, healthcare, education, and politics.\footnote{Throughout the paper, we use the term `sensitive attribute' instead of `protected characteristic' to avoid any confusion with the legal meaning reported, for example, in the UK Equality Act.} 
To enhance fairness, individuals (are often nudged to) adjust their behaviour towards those holding sensitive attributes. For example, giving up a seat on public transport for an elderly person illustrates a behavioural adjustment to promote fairness.
% For example, individuals with disabilities or socio-economic disadvantages are guaranteed priority assistance and resource access. 
For this reason, integrating fairness into agents' policies has been an area of growing investigation~\cite{Reuel2024}. 

% Paragraph 3
Foundational works in social sciences~\cite{Griesinger1973,Liebrand1984} have identified agents' attributes as a crucial factor influencing fairness outcomes in MAS. In this sense, inspired by algorithmic fairness~\cite{Mitchell2021,Castelnovo2022}, we propose sensitive attributes as characteristics that should not affect an agent's expected reward. We apply metrics from the algorithmic fairness literature, specifically demographic parity, counterfactual fairness, and conditional statistical parity, to the MAS context and use these to constrain agent behaviour and obtain fair policies. Building on gradient-based algorithms in reinforcement learning and inspired by the work of Zhang et al.~\shortcite{Zhang2022}, we propose a fairness-aware Proximal Policy Optimisation (PPO)~\cite{Schulman2017A} method, which we call fair-PPO, that improves policy fairness. We modify the PPO objective function to include a penalty term derived from a fairness metric allowing multi-objective optimisation of the policy that accounts for both performance and fairness. In simpler terms, PPO guides the agents' policy to maximise rewards. However, if the fairness metric shows increased disparity, a penalty is applied, which adjusts the optimisation process and shifts the policy towards aligning with the fairness metric.

% Paragraph 5
Our proposed penalty has two components. The first component penalises total reward disparities between agents that differ by a sensitive attribute by looking at past outcomes.
The second component penalises disparities in the expected rewards as per the estimate of the value function of each agent. In other words, the first component is retrospective, addressing disparities in past outcomes, while the second is prospective, encouraging fairness in the agent's future expectations and decision-making.

% Paragraph 6
%We conduct our experiments in a version of the Allelopathic Harvest (AH)~\cite{Leibo2019}, a MAS that combines cooperation and competition in resource collection. In the AH, two groups of agents with different preferences regarding available resources navigate a dynamic environment. The environment balances cooperative strategies among agents with similar preferences and competitive interactions between agents with differing preferences. Each group is further divided into those with and without a sensitive attribute. Agents with a sensitive attribute move slower and so are potentially disadvantaged in resource collection. 
% This distinction adds another layer of complexity to the system, highlighting disparities in outcomes and testing the effectiveness of fairness-driven interventions.

% Paragraph 7


% Paragraph 8
% The AH environment is particularly well-suited for our investigation points for the following reasons. First, the AH dynamic balance of cooperation and competition requires agents to learn non-trivial fair policies supporting the adaptability of our method to complex scenarios (see investigation points i and ii). Second, the system’s dynamics, ranging from high fluctuations in resource availability to relative stability and thus leading to uncertainty, motivate the investigation of the retrospective and prospective penalty components (see investigation point iii). Finally, the diversity of strategies available allows us to explore how fairness interventions influence the policies adopted by agents, providing valuable insights into their behaviour.

% Paragraph 8
In summary, the main contribution of this work is the novel fair-PPO reinforcement learning algorithm, which extends PPO with a penalty term with two components: 
 a retrospective component that addresses fairness violations based on past rewards and a prospective component that anticipates future fairness violations by leveraging the value function to estimate upcoming rewards.
 We perform experiments in a version of the Allelopathic Harvest (AH)~\cite{Leibo2019}, a MAS that combines cooperation and competition in resource collection, where two groups of agents with different preferences regarding available resources navigate a dynamic environment. Agents are distinguished according to whether they hold some sensitive attribute: agents with this attribute move more slowly and so are potentially disadvantaged in resource collection. 
 We show that: 
 (i) fair-PPO outperforms classic PPO in generating fairer policies across all the fairness metrics; (ii) while fair-PPO policies are less efficient than classic PPO in terms of rewards, agents with and without the sensitive attribute renounce a similar proportion of rewards with fair-PPO in relation to with classic PPO; and (iii) the retrospective and prospective components of the penalty complementarily affect the agent's policy in favour of fairness, producing policies that sensibly deviate from those
 of classic PPO.


In Section~\ref{RelatedWork}, we review the literature on fairness in MAS, fairness in reinforcement learning and algorithmic fairness. Section~\ref{Preliminaries} introduces the concepts of MAS and PPO. In Section~\ref{Methodology}, we detail the fairness metrics and the integration of the penalty into PPO. Sections~\ref{Experiments} and~\ref{Results} focus on evaluating our approach, presenting experimental results using the AH.

\section{Related Work}~\label{RelatedWork}
Since our work is grounded in fairness metrics within MAS and inspired by algorithmic fairness, we first review recent works on fairness measures in MAS and algorithmic fairness. In addition, we examine recent works on fair reinforcement learning to highlight the distinctions between our work and that of others.

\subsection{Fairness Measures in MAS}
In MAS, fairness is evaluated in various ways tailored to the specific design and objectives of the system under study; here, we review the most prominent fairness metrics, drawing insights from well-established ones and highlighting their relevance to our work. 

In the ultimatum game and fair division, the concept of \textit{proportionality} plays a central role in evaluating fairness.
In the ultimatum game, in which two players must agree on dividing a sum of money~\cite{Guth2014,Debove2016}, fairness is typically determined by the \textit{proportion} of the total amount proposed by the proposer and accepted by the responder. Similarly, in fair division, proportionality is a fundamental principle for distributing goods or chores among individuals and groups of agents, taking into account their utilities for divisible or indivisible resources~\cite{Lindner2016,Amantidis2023,Murhekar2024}. Beyond proportionality, \textit{envy-freeness}, that ensures no agent prefers another's allocation, \textit{maximin share fairness}, which guarantees each agent receives a share at least as good as what they could secure by dividing resources themselves, and other derivative notions of fairness, such as \textit{envy-freeness up to one good}, \textit{envy-freeness up to any good} offer nuanced ways for fair division~\cite{Lipton2004,Budish2011,Caragiannis2019}. 

The multi-armed bandit proposes to find the best decision-making algorithm to choose among a number of arms to pull, each associated with a probability function that leads to a payoff~\cite{Bouneffouf2020}. Its classic version aims to maximise the overall payoff obtained by pulling the arms; however, some variants propose adding a further fairness constraint to the optimisation process. Some measures of fairness have been proposed, such as \textit{meritocratic fairness}, which ensures that rewards are allocated proportionally to the merit of the arms~\cite{Joseph2016}, \textit{treatment equality}, which ensures similar error rates or outcomes across different groups~\cite{Liu2017}, or \textit{regret}, which quantifies the cost of deviating from the optimal balance between fairness and efficiency~\cite{Li2020,Patil2021,Jones2023,Barman2023}.
% In voting, \textit{social welfare}, which is the total utility of all individuals based on a chosen outcome, is often used to measure a voting system's goodness. However, fairness relates to how welfare is distributed~\cite{Kaplow2003,Høgsgaard2023}. 
% Finally, \textit{randomisation} is studied to benefit minorities to get represented in the system~\cite{Procaccia2010,Aziz2020}.

In our work, fairness metrics also focus on the distribution of rewards among agents, similar to proportionality. 
However, a key distinction lies in the incorporation of sensitive attributes to quantify unfairness between groups of agents. Such an idea is close to treatment equality and meritocratic fairness (assuming the sensitive attribute can be the merit). Metrics such as regret and envy-freeness are conceptually different, as the first is more of a performance metric, while the second is more individual-based.
% Another popular measure of fairness is  \textit{distortion}, which is the difference between the social welfare obtained by the optimal outcome (maximising the total utility) and the voting rule adopted~\cite{Caragiannis2011,Boutilier2012,Caragiannis2017,Ebadian2022}.

\subsection{Algorithmic Fairness}
Algorithmic fairness addresses bias and discrimination in decision-making systems across domains such as justice~\cite{Berk2019}, education~\cite{Baker2021}, credit scoring~\cite{Kozodoi2022}, and healthcare~\cite{Vyas2020}, \cite{Giovanola2022}, with a focus on protected attributes characterising discriminated groups.
Fairness metrics are classified into group and individual categories. Group fairness metrics include \textit{demographic parity}~\cite{Kamishima2012} and \textit{equalised odds}~\cite{Hardt2016}, which use confusion matrix rates, while \textit{calibration-based metrics} evaluate prediction accuracy relative to group membership~\cite{Chouldechova2016}. Individual fairness, such as \textit{counterfactual fairness}~\cite{Kusner2018}, assesses consistency across factual and counterfactual scenarios.
% The surveys by LeQuy et al.~\shortcite{LeQuy2022} and Caton et al.~\shortcite{Caton2024} provide an overview of the topic.

\subsection{Fairness and Reinforcement Learning}
Reinforcement learning traditionally focuses on learning policies that maximise expected rewards~\cite{Sutton2018}. However, this objective raises fairness concerns, as it can perpetuate biases, violate fairness principles, and even conflict with legal requirements~\cite{Jabbari2017}. To address these issues, some reinforcement learning algorithms integrate fairness constraints into the optimisation process. For example, Siddique et al.~\shortcite{Siddique2020} and Zimmer et al.~\shortcite{Zimmer2021} define fairness as finding solutions that are \textit{efficient} (benefiting everyone without waste), \textit{impartial} (treating identical agents equally), and \textit{equitable} (helping those who are worse off). These ideas aim to balance fairness with the overall benefit for all agents.

Chen et al.~\shortcite{Chen2021} propose adjusting rewards through a multiplicative weight to achieve $\alpha$\textit{-fairness}, while Zhang et al.~\shortcite{Zhang2014} implement \textit{maximin fairness} to optimise the worst-performing agent's outcome. Other works explore fairness across agent groups, including \textit{demographic parity}~\cite{Jiang2019,Wen2021,Chi2022}. Some contributions address real-world complexities, such as agents with differing characteristics or preferences, necessitating tailored fairness mechanisms~\cite{Yu2023,Ju2024}. Although these works share conceptual similarities with fair-PPO in addressing fairness, our method is more aligned with the safe reinforcement learning framework proposed by Zhang et al.~\shortcite{Zhang2022}.
% As Friedler et al.~\cite{Friedler2016} observe, fairness definitions are often inconsistent, and in RL, this challenge is exacerbated by the high specificity of tasks~\cite{Reuel2024}.

\section{Preliminaries}~\label{Preliminaries}
% \textcolor{teal}{
% Which elements to define related to MAS:\\
% MAS as an MDP: states, actions, (prob) transition function, reward function, trajectory, policy.\\
% Reinforcement learning elements: gradient-based policy, PPO.
% }
In this section, we first define the elements composing a MAS and then define gradient-based policies and PPO.

\subsection{Multi-Agent Systems}
A MAS consists of multiple agents acting in an environment to achieve their goals. 
We denote a MAS as $\Sys = (E, e_o, Ac, P, At, At^{pr}, \tau)$, where $E$ is the set of possible environment states, $e_0$ is the initial state, $Ac$ is the set of available actions, $P = \{a_1, \ldots, a_n\}$ is the population of agents; $At = \{at_1, \ldots, at_m\}$ is the set of attributes available to the agents, $At^{pr} \subset At$ is the set of sensitive attributes and $\tau : E \times Ac_1 \times \ldots \times Ac_n \rightarrow E \times [0,1]$ is the non-deterministic state transformer function, which returns a probability distribution over the possible states that may result, where $E \times [0,1]$ is the raw scores of the probability distribution over the actions, i.e., $\mathbb{P}(E)$.

We define an agent $a_x$ as a tuple $(At_x, Ac_x, \pi_x, \rho_x)$, where $At_x: At \rightarrow \{0,1\}$ is a function specifying which attributes hold true for the agent, $Ac_x \subseteq Ac$ is the set of actions available to the agent, $\pi_x: E \rightarrow Ac_x \times [0, 1]$ is the policy and $\rho_x: E \times E \rightarrow \mathbb{R}$ is the reward function that specifies the reward the agent receives from one state to another. 
Within $\Sys$, we denote a run $r = (e_0, ac_0, e_1, \ldots, ac_T, e_T)$, where $ac_{i} = (ac_{(i,1)}, \ldots, ac_{(i,n)})$ is the collective action of all $n$ agents at step $i$. 
% For each $a_x$, $(ac_{(i,x)}, p) \in \pi_x(e_{i})$ with $p>0$, where the tuple $(ac_{(i,x)}, p)$ denotes an action that the agent might choose to perform in a given state, along with the probability of the agent selecting that action. For each $i$, $(e_{i+1}, p) \in \tau(e_i, a_{i+1})$ with $p>0$, where the tuple represents a possible future state that the environment may transition to from its current state, along with the probability of this transition occurring, given the collective actions of the agents.
% The set of {\it all possible runs} within $\Sys$ is denoted as $\mathcal{R}^{\Sys}$.
% The probability of a run $r \in \Sys$ is:
% \begin{align*}
% p(r \mid \Sys) = \Biggl( \prod_{i=0}^{j-1} \Biggl( \prod_{x=1}^{n} p_x \mid (ac_{i+1}^x, p_x) \in \pi_x(e_i) \Biggr) \Biggr) \cdot \\
% \cdot \Biggl( \prod_{i=0}^{j-1} p_i \mid (e_{i+1}, p_i) \in \tau(e_i, \vec{a}_{i+1}) \Biggr)
% \end{align*}
The total reward achieved by an agent $a_x$ over a run $r = (e_0, ac_0, e_1, \ldots, ac_T, e_T)$ is $Rew(a_x, r,   \Sys) = \sum_{i=1}^{T} \rho_x(e_{i-1}, e_i)$. 
% The total rewards achieved by an agent $a_x$ over a run $r$ starting from the state $e_i$ is denoted as $Rew(a_x, \Sys, e_i)$. 
The probability of a run $r$ occurring, denoted as $p(r \mid \Sys)$ is defined as $p(r \mid \Sys) = \prod_{i=0}^{T-1} \Biggl( \prod_{x=1}^{n} p_x \, \text{where } (ac_{(i+1,x)}, p_x) \in \pi_x(e_i) \Biggr) \cdot \Biggl( \prod_{i=0}^{T-1} p_i \, \text{where } (e_{i+1}, p_i) \in \tau(e_i, ac_i) \Biggr)$, where the first term accounts for the probability of each agent $a_x$'s action $ac_{(i+1,x)}$ at step $i$ based on its policy $\pi_x(e_i)$; the second term accounts for the probability of the next state $e_{i+1}$ determined by the combined actions $ac_i$ of all agents and the state transformer function $\tau(e_i, ac_i)$.
The expected reward of an agent $a_x$ within a system $\Sys$ is $\mathbb{E}[Rew(a_x, \Sys)] = Rew(a_x, r, \Sys) \cdot p(r \mid \Sys)$.
% $Rew(a_x, \Sys, e_i)= \sum_{j =i+1}^{T} \rho_x(e_{j-1}, e_j)$.

\subsection{Gradient-Based Policy}~\label{PPO}
In reinforcement learning, gradient-based policy optimisation adjusts the parameters of a policy $\pi_{\theta}$ to maximise the agent's expected total rewards. In other words, given an objective function depending on the parameters $\theta$, the aim is to update those parameters through gradient calculation to improve the agent's performance. 
% For an agent $a_x \in P$, the gradient estimator is defined as $\hat{g}_x = \hat{\mathbb{E}}_i \left[\nabla_{\theta_x} \log \pi_{\theta_x}(ac_{(i,x)} \mid e_i) \, \hat{A}_{(i,x)} \right]$, which is the expectation of the gradient of the log probability $\nabla_{\theta_x} \log \pi_{\theta_x}(ac_{(i,x)} \mid e_i)$ of the agent $a_x$ taking action $ac_{(i,x)}$ in the state $e_i$ under the policy $\pi_{\theta}$. The advantage function $\hat{A}_{(i,x)}$ measures how much better or worse the action $ac_{(i,x)}$ is compared to the expected performance of other actions in the same state.
% \textcolor{teal}{In other words, if $\hat{A}_t \geq 0$, the policy should increase the probability of taking that action in the future, if $\hat{A}_t \leq 0$ the probability should decrease.}
To avoid drastic leaps in the loss optimisation that may disrupt the learning process, Trust Region Policy Optimization~\cite{Schulman2017B} (TRPO) penalises policy updates by limiting the KL divergence, which measures the difference between the action probability distributions of the old and new policies. Further, Clipped Surrogate Objective (CLIP) limits the change in the probability ratio of actions between the old and new policies to remain within a small range. 

\paragraph{Proximal policy optimization.}
PPO integrates policy optimization and value function accuracy into the following loss function:
\begin{align} \label{LossFunc}
L_i^{\text{PPO}}(\theta) = \hspace{-0.5cm} & \nonumber \\
& \begin{aligned}
\hat{\mathbb{E}}_i \left[L_i^{\text{CLIP}}(\theta_x) + c_1 L_i^{\text{VF}}(\theta_x) + c_2 S[\pi_{\theta_x}](e_i)\right]
\end{aligned}
\end{align} 
The objective loss $L_t^{\text{CLIP} + \text{VF} + \text{S}}(\theta)$ is composed of the following three components.

The Clipped Surrogate Objective rewards advantageous actions while stabilising policy updates by limiting changes per step:
\begin{align*}
L_i^{\text{CLIP}}(\theta_x) =  \hspace{-1cm} & \\
& \begin{aligned}
\hat{\mathbb{E}}_i \left[\min\left(\psi_i(\theta_x)\hat{A}_{(i,x)}, \ \text{clip}\left(\psi_i(\theta_x), 1 - \epsilon, 1 + \epsilon\right)\hat{A}_{(i,x)}\right)\right]
\end{aligned}
\end{align*}
where $\psi_i(\theta_x) = \frac{\pi_{x_\theta}(ac_{(i,x)} \mid e_i)}{\pi_{x_{\theta_{\text{old}}}}(ac_{(i,x)} \mid e_i)}$ is the probability ratio of the new policy to the old policy for action \(ac_{(i,x)}\) and $\hat{A}_{(i,x)}$ is the advantage function for agent $a_x$ at step $i$, which estimates how much better or worse the action $ac_{(i,x)}$ is compared to the expected behaviour.

The Value Function Loss improves the accuracy of the policy's value estimation:
% \begin{equation} \nonumber
%        L_i^{\text{VF}}(\theta_x) = \left(V_{\theta_x}(e_i) - \hat{Rew}_i^{a_x}\right)^2,
% \end{equation}
\begin{equation} \nonumber
       L_i^{\text{VF}}(\theta_x) = \left(V_{\theta_x}(e_i) - Rew(a_x, \Sys, e_i)\right)^2
\end{equation}
% where $V_{\theta_x}(e_i)$ is the value function estimate of the expected return for state $e_i$, and $\hat{Rew}_i^{a_x}$ is the observed total reward for agent $a_x$ starting at timestep $i$.
where $V_{\theta_x}(e_i)$ is the value function estimate of the expected return for state $e_i$, and $Rew(a_x, \Sys, e_i)$ is the total rewards for agent $a_x$ starting at state $e_i$.

Finally, the Entropy Bonus encourages exploration by promoting more diverse action selection:
\begin{equation*}
S[\pi_{x_\theta}](e_i) = 
-\hspace{-0.2cm}\sum_{ac_{(i,x)} \in Ac_x} \pi_{x_\theta}(ac_{(i,x)} \mid e_i) \log \pi_{x_\theta}(ac_{(i,x)} \mid e_i)
\end{equation*}
% \begin{align*}
% S[\pi_{x_\theta}](e_i) = \hspace{-0cm} & \\
% & \begin{aligned}
% -\sum_{ac_{(i,x)} \in Ac_x} \pi_{x_\theta}(ac_{(i,x)} \mid e_i) \log \pi_{x_\theta}(ac_{(i,x)} \mid e_i)
% \end{aligned}
% \end{align*}
where the exploration is maximised through the entropy of the policy $\pi_{x_\theta}$, which promotes uncertainty and diversity in action selection.

\section{Fair-PPO}~\label{Methodology}
This section consists of two parts: first, we report the formalisation of demographic parity, counterfactual fairness and conditional statistical parity in MAS when sensitive attributes are involved; second, we formalise the penalties based on the fairness metrics above and incorporate them in PPO as a constraint of the loss function.  

\subsection{Fairness Metrics in MAS}~\label{FairnessMetrics}
Inspired by algorithmic fairness, we report the definition of demographic parity, counterfactual fairness and conditional statistical parity as building block concepts to introduce fair-PPO policies. Such definitions revolve around comparing the expected rewards gathered by distinct groups of individuals with and without sensitive attributes. These metrics are used to formulate three distinct penalty terms, which are incorporated as factors into the PPO loss function.
\vspace{2pt}
\begin{definition}[\textbf{Demographic Parity}] \label{Demographic Parity}
Let $\Sys = (E, e_o, Ac, P, At, At^{pr}, \tau)$ be a MAS and let $at^{pr} \in At^{pr}$ be a sensitive attribute. 
Given two groups of agents $a_x$ and $a_y$ that only differ for the sensitive attribute, namely $\forall a_x, a_y \in P$ such that $At_x(at^{pr}) = 1$, $At_y(at^{pr}) = 0$, and $At_x(at^\prime) = At_y(at^\prime)$,   $ \forall at^\prime \in At\setminus \{at^{pr}\}$, demographic parity implies that $\mathbb{E}[Rew(a_x, \Sys)] = \mathbb{E}[Rew(a_y, \Sys)]$.

When demographic parity does not hold, we quantify the disparity as follows.
\begin{align} \label{EqDP}
\Delta DP(at^{pr}, \Sys) = \hspace{-1cm} &  \nonumber \\
& \begin{aligned}
\sum_{a_x, a_y \in P}
\mathbb{E}[Rew(a_x, \Sys)] - \mathbb{E}[Rew(a_y, \Sys)]
\end{aligned}
\end{align}
% \noindent where $a_x, a_y \in P$ are such that $At_x(at^{pr}) = 1$, $At_y(at^{pr}) = 0$, $At_x(at^\prime) = At_y(at^\prime) \forall at^\prime \in At\setminus \{at^{pr}\}$.
% If demographic parity holds then $DemPar(at^{pr}, \Sys) = 0$.
\end{definition}
\vspace{2pt}
\begin{definition}[\textbf{Counterfactual Fairness}] \label{Counterfactual Fairness}
Let $\Sys = (E, e_0, Ac, P, At, At^{pr}, \tau)$ be a MAS and let $\Sys^\prime = (E, e_0, Ac, P^\prime, At, At^{pr}, \tau)$ its counterfactual version. In $\Sys^\prime$ for any agent $a_x \in P$ who does not possess the sensitive attribute $At^{pr}$, the corresponding agent $a_x \in P^\prime$ is assigned the attribute and vice versa.
% Let $\Sys = (E, e_0, Ac, P, At, At^{pr}, \tau)$ be a MAS, where any agent $a_x \in P$ does not possess a protected attribute $At^{pr}$. We define a counterfactual MAS $\Sys^\prime = (E, e_0, Ac, P^\prime, At, At^{pr}, \tau)$ where any agent in $a_x^\prime \in P^\prime$ possesses the protected attribute.
% $P = \{(At_i, Ac_i, \pi_i, \rho_i) \mid 1 \leq i \leq n \}$ and let $at^{pr} \in At^{pr}$ be a sensitive attribute.
% We define the counterfactual system $\Sys^\prime = (E, e_0, Ac, P^\prime, At, At^{pr}, \tau)$, where $P^\prime = \{ (At_i^\prime, Ac_i, \pi_i, \rho_i) \mid 1 \leq i \leq n \}$ and $At_i^\prime$ is s.t.:
% \begin{equation} \nonumber
% At_i^\prime(at) =
% \begin{cases} 
%     1 & \text{if } at = at^{pr} \text{ and } At_i(at) = 0, \\
%     0 & \text{if } at = at^{pr} \text{ and } At_i(at) = 1, \\
%     At_i(at) & \text{otherwise}.
% \end{cases}
% \end{equation}
Counterfactual fairness is satisfied if $\forall a_x \in P$ and $ \forall a_x^{\prime} in P^{\prime}$: $\mathbb{E}[Rew(a_x, \Sys)] = \mathbb{E}[Rew(a_x^\prime, \Sys^\prime)]$. 

When counterfactual fairness does not hold, we denote the disparity as follows.
\begin{align} \label{EqCF}
\Delta CF(at^{pr}, \Sys, \Sys^\prime) = \hspace{-1.8cm} & \nonumber \\
& \begin{aligned}
\sum_{a_x \in P, a_x^\prime \in P^\prime} \mathbb{E}[Rew(a_x, \Sys)] - \mathbb{E}[Rew(a_x^\prime, \Sys^\prime)]
\end{aligned}
\end{align} 
% \noindent where $a_x \in P$ is such that $At_x(at^{pr}) = 0$ and $a_x^\prime \in P^\prime$ is such that $At_x^\prime(at^{pr}) = 1$.
% If counterfactual fairness holds then $CountFair(at^{pr}, \Sys) = 0$.
\end{definition}
\vspace{2pt}
\begin{definition}[\textbf{Conditional Statistical Parity}]  \label{ConditionalStatisticalParity}
Let $\Sys = (E, e_0, Ac, P, At, At^{pr}, \tau)$ be a MAS, where we define a legitimate factor as a non-sensitive attribute, namely $LF \in (At \setminus At^{pr})$, and $at^{pr} \in At^{pr}$ is the sensitive attribute.
Formally, $\forall a_x, a_y $ such that $At_x(at^{pr}) = 1$, $At_y(at^{pr}) = 0$, $At_x(at^\prime) = At_y(at^\prime)$, $ \forall at^\prime \in At\setminus \{at^{pr}\}$, and $At_x(LF)=At_y(LF)$, conditional statistical parity is satisfied if: $\mathbb{E}[Rew(a_x, \Sys)] = \mathbb{E}[Rew(a_y, \Sys)]$.

For each subgroup, when conditional statistical parity does not hold, we quantify the disparity as follows.
\begin{align} \label{EqCSP}
\Delta CSP(at^{pr}, LF, \Sys) = \hspace{-3cm} & \nonumber \\
& \begin{aligned}
\sum_{\substack{a_x, a_y \in P, \\ At_x(LF)=At_y(LF)}} \mathbb{E}[Rew(a_x, \Sys)] - \mathbb{E}[Rew(a_y, \Sys)]
\end{aligned}
\end{align} 
% where $a_x, a_y \in P$ are such that $At_x(at^{pr}) = 1, \quad At_y(at^{pr}) = 0$, $At_x(at^{lf}) = At_y(at^{lf}) = 1$ $\forall at^{lf} \in LF$ and $At_x(at') = At_y(at')$ $\forall at^\prime \in At \setminus \{at^{pr}\}$.
% If conditional statistical parity holds then $CondSP(at^{pr}, LF, \Sys) = 0$.
\end{definition}
The presence/absence of the protected attribute and the legitimate factor define four population subgroups.
Conditional statistical parity is satisfied when demographic parity holds within each subgroup where $LF =0$ and $LF=1$ respectively.
\subsection{Fairness Metrics for Fair PPO}
Classic PPO focuses on maximising agents' rewards. This section extends PPO by incorporating fairness constraints in the optimisation process. 
We penalise the PPO loss (see Section~\ref{PPO}) to discourage behaviours that amplify disparities measured as per the metrics in Section~\ref{FairnessMetrics}.
% Demographic parity, counterfactual fairness and conditional statistical parity rely on computing fairness based on the total rewards gathered by agents with and without sensitive attributes. Thus, it is possible to penalise PPO loss based on such disparities. 
Designing the penalty accounting only for past rewards can limit learning effective policies, particularly in stochastic environments and the early training process stage. To address this, our extension of PPO penalises agents' behaviour based on both past (total) rewards and expected future rewards via the value function prediction. 

We modify Eq.~\ref{LossFunc} based on the metrics of Eq.~\ref{EqDP}, ~\ref{EqCF}, ~\ref{EqCSP} such that the optimisation process converges to fairer policies:
\begin{align*} 
L_i^{\text{fair-PPO}}(\theta) = \hspace{-1.5cm} & \\
& \begin{aligned}
\hat{\mathbb{E}}_i \left[L_i^{\text{CLIP}}(\theta_x) + c_1 L_i^{\text{VF}}(\theta_x) + c_2 S[\pi_{x_{\theta}}](e_i) + \lambda \cdot L_{i}^{\text{fair}}\right]
\end{aligned}
\end{align*}
where $L_{i}^{\text{fair}}$ is calculated according to one of the definitions below, and $\lambda$ controls the magnitude of the overall contribution to the loss. 

\paragraph{Demographic parity penalty.}
The demographic parity penalty is formulated as follows:
\begin{align} \label{PenaltyDP}
L_{i}^{\text{fair-DP}} = \hspace{-0cm}
\alpha \cdot \sum_{a_x, a_y \in P}
\left| Rew(a_x, r, \Sys) - Rew(a_y, r, \Sys) \right| 
\ + \nonumber
\\
& \hspace{-5cm} \beta \cdot \sum_{a_x, a_y \in P}
\left| V_{\theta_x}(e_i) - V_{\theta_y}(e_i) \right|    
\end{align}
where $Rew(a_x, r, \Sys)$ and $Rew(a_y, r, \Sys)$ are the total reward of agents $a_x \in P$ and $a_y \in P$ (retrospective component); $V_{\theta_x}(e_i)$ and $V_{\theta_y}(e_i)$ are the value function estimates of the expected rewards for agents $a_x$ and $a_y$ at state $e_i$, based on the current policy $\pi_{\theta_x}$ (prospective component). The parameters $\alpha$ and $\beta$ balance the contributions of each penalty component.

\paragraph{Counterfactual fairness penalty.}
The counterfactual fairness penalty is formulated as follows:
\begin{align} \label{PenaltyCF}
L_{i}^{\text{fair-CF}} = \hspace{-1cm} & \nonumber \\
& \begin{aligned}
\alpha \cdot\sum_{a_x \in P, a_x^\prime \in P^\prime}
\left| Rew(a_x, r, \Sys) - Rew(a_x^\prime, r, \Sys^\prime) \right| 
+ \\
\beta \cdot \sum_{a_x \in P, a_x^\prime \in P^\prime}
\left| V_{\theta_x}(e_i) - V_{\theta_{x^{\prime}}}(e_i) \right|  
\end{aligned} 
\end{align}
where $Rew(a_x, r, \Sys)$ and $Rew(a_x^\prime, r, \Sys^\prime)$ are the total rewards of agents $a_x \in P$ and $a_x^\prime \in P^\prime$ (retrospective component); $V_{\theta_x}(e_i)$ and $V_{\theta_{x^\prime}}(e_i)$ are the value function estimates of the expected rewards for agents $a_x$ and $a_x^\prime$ at state $e_i$, based on the current policy $\pi_{\theta_x}$ (prospective component). The parameters $\alpha$ and $\beta$ balance the contributions of each penalty component.

\paragraph{Conditional statistical parity penalty.}
The conditional statistical parity penalty is formulated as follows:

\begin{align} \label{PenaltyCSP}
L_{i}^{\text{fair-CSP}} = \hspace{-1cm} & \nonumber \\
& \begin{aligned}
\alpha  \cdot \bigg( & \sum_{\substack{a_x, a_y \in P \\ At_x(LF)=At_y(LF) \\ At_x(at^{pr}) \neq At_y(at^{pr})}} \hspace{-0.5cm}
\left| Rew(a_x, r, \Sys) - Rew(a_y, r, \Sys) \right| + \\
& \hspace{-0.5cm} \sum_{\substack{a_x, a_y \in P \\ At_x(LF) \neq At_y(LF) \\ At_x(at^{pr}) \neq At_y(at^{pr})}} \hspace{-0.5cm}
\left| Rew(a_x, r, \Sys) - Rew(a_y, r, \Sys) \right| \bigg) + \\
\beta \cdot \bigg( & \sum_{\substack{a_x, a_y \in P \\ At_x(LF)=At_y(LF) \\ At_x(at^{pr}) \neq At_y(at^{pr})}}
\left| V_{\theta_x}(e_i) - V_{\theta_y}(e_i) \right| + \\
& \hspace{-0.5cm} \sum_{\substack{a_x, a_y \in P \\ At_x(LF) \neq At_y(LF) \\ At_x(at^{pr})  \neq At_y(at^{pr})}}
\left| V_{\theta_x}(e_i) - V_{\theta_y}(e_i) \right| \bigg)   
\end{aligned}  
\end{align}
where in the first component of $\alpha$ and $\beta$ agents have the same legitimate factor ($At_x(LF)=At_y(LF)$); in the second component agents have different legitimate factor ($At_x(LF) \neq At_y(LF)$). All terms assume the population has agents with and without the sensitive attribute ($At_x(at^{pr})  \neq At_y(at^{pr})$). 


\section{Experiments}~\label{Experiments}
This paper's experiments aim to show how agents trained with fair-PPO adopt distinct strategies that achieve greater fairness compared to classic PPO. We also investigate the impact of these strategies on the rewards collected by the agent groups and examine the role of the penalty components in the fair-PPO loss, parametrised by $\alpha$ and $\beta$, in promoting fairness.
We conduct our experiments on a version of the Allelopathic Harvest (AH)~\cite{Leibo2019}. In this setup, two groups of agents with distinct preferences --- one favouring red berries and the other blue --- move in a grid and engage in cooperative dynamics within their respective groups, i.e., they plant and ripen berry plants of their favourite colour and compete against the opposing group by blocking agents with opposed preferences. The objective for each group is to ensure the proliferation of their preferred berry, thereby maximising their rewards. Within each group, half of the agents can move every turn, while others are limited to moving only every two turns. This difference in mobility is a sensitive attribute, which can be interpreted as an impairment.\footnote{For more details regarding the game, see the supplementary material.}

\subsection{Train and Test}
We train separate policies for agents with and without sensitive attributes to enable each to learn behaviours tailored to their specific characteristics independently.\footnote{The rules and environment configuration where we train the agents are reported in the Appendix.} 
We train various fair-PPO policies using penalties parametrised by $\alpha$ and $\beta$, addressing demographic parity, counterfactual fairness, and conditional statistical parity, as defined in Eq.~\ref{PenaltyDP}, \ref{PenaltyCF}, and \ref{PenaltyCSP}, respectively. The parameters $\alpha$ and $\beta$ range from 0 to 1, taking discrete values with step $0.25$. Classic PPO presents $\alpha = \beta = 0$. Training is conducted over $1000$ episodes, each representing a new game and randomly initialised, with $3000$ time steps per episode. 
% A time step concludes when all agents have acted if they can. 

We test each policy on $1000$ new randomly initialised episodes of $3000$ time steps each, from which we retrieve the fairness metrics. 
Demographic and conditional statistical parity are computed for individual episodes and averaged across the entire set of test episodes. Demographic parity measures reward parity between agents with and without the sensitive attribute across the full population, whereas conditional statistical parity evaluates reward parity within subgroups based on their preference for red or blue berries (legitimate factor).

Counterfactual fairness is assessed by running factual and counterfactual episodes concurrently. In factual episodes, none of the agents possess the sensitive attribute, while in counterfactual episodes, all agents are assigned the sensitive attribute. We look at the most extreme scenario to isolate the impact of the sensitive attribute on fairness. Each pair of episodes is initialised identically, and counterfactual fairness is evaluated by comparing the rewards obtained in the two scenarios. The results from all episode pairs are averaged across the test set.

\section{Results}~\label{Results}
In this section, we present and analyse three main findings of the paper, concluding each result with key insights that can be generalised beyond the game context.
% i) Fair-PPO outperforms classic PPO in generating fairer policies across all the fairness metrics; ii) While fair-PPO policies are less efficient than classic PPO in terms of rewards, agents with and without the sensitive attribute renounce to a similar proportion of rewards; iii) The retrospective and prospective components of the penalty complementarily affect the agents policy in favour of fairness, with strategies that sensibly deviate from that of classic PPO
\subsection{Fair-PPO produces fairer policies than classic PPO}
Figure~\ref{combined-boxplots} shows that fair-PPO achieves a reduction of up to $50-60\%$ of demographic disparity for various combinations of $\alpha$ and $\beta$, compared to classic PPO ($\alpha=0.0$, $\beta=0.0$). For conditional statistical disparity and both subgroups of agents, characterised by different preferences over the berries, fair-PPO consistently outperforms classic PPO. This result highlights the capacity of fair-PPO to improve the disparities even within subgroups of the population. For counterfactual unfairness, instead, an improvement of fair-PPO compared to classic PPO happens only for high levels of $\alpha$ and $\beta$. We attribute this result to the increased challenge of learning a fair policy when agents from different groups do not interact or influence each other's outcomes. In factual episodes, no agents possess the sensitive attribute, while in counterfactual episodes, all agents are assigned the sensitive attribute. As a result, the penalty, which depends on the outcomes, is unaffected by interactions between groups, making it harder to enforce fairness.
\paragraph{Key takeaways.} Fairness-aware algorithms like fair-PPO can reduce disparities across metrics while balancing tradeoffs between groups, demonstrating their potential for broader use in collaborative and competitive decision-making. However, the challenges with counterfactual unfairness highlight difficulties when agent groups do not interact and influence each other’s outcomes.

\subsection{Fair strategies: efficiency and price of fairness} 
Table~\ref {tab:combined-PoF} shows the Price of Fairness (PoF) for the four policies that achieve higher fairness for all fairness-based penalties (the full table is reported in the Supplementary Material). The PoF is the percentage change in rewards when using fair-PPO compared to classic PPO. A positive PoF means rewards have improved with fair-PPO.
Across all metrics, the PoF becomes increasingly negative as fairness improves with fair-PPO, indicating that both groups renounce higher rewards to achieve higher fairness . The PoF difference between the groups of agents is small, suggesting that both groups renounce comparable levels of rewards to achieve greater fairness. This result is counterintuitive, as one might expect only agents without the sensitive attribute to adopt less optimal strategies to align their rewards with those of agents with the sensitive attribute; however, agents with the sensitive attribute also experience reduced rewards.
\paragraph{Key takeaways.}   Fairness-aware algorithms can require shared trade-offs, with both groups making comparable sacrifices to achieve parity. Fair-PPO improves fairness without disproportionately penalising agents without the sensitive attribute, challenging the idea that fairness relies on reducing their rewards alone.

\subsection{Retrospective and prospective penalty components: fairness and strategies}
From Figure~\ref{combined-boxplots}, no clear trend emerges for the single values of $\alpha$ and $\beta$ for which unfairness is reduced (for boxplots ordered according to value of $\alpha$, see the Supplementary Material). The right combination of values is key to unfairness reduction compared to classic PPO, and a high level of $\alpha$ and $\beta$ does not always correspond to a policy that corrects unfairness.
The most significant reduction in demographic disparity happens for $\alpha = 0$ and $\beta = 0.25$, while for conditional statistical parity for $\alpha = 0.75$ and $\beta = 0.25$.
On the other hand, to reduce counterfactual unfairness, fair-PPO outperforms classic PPO for high levels of $\alpha$ and $\beta$. This result is probably due to greater difficulty in making agents learn a fair policy, likely because it is harder for agents to learn fair policies when the penalty is based on two separate game runs, with agents observing only one environment directly.

Figure~\ref{radar-plots} show the distinct strategies employed by agents trained with classic PPO and fair-PPO with different values of $\alpha$ and $\beta$. By comparing the six subplots, we notice that for demographic and conditional statistical disparity, three main strategies emerge, where two/three actions are selected more frequently than all the others. Instead, more strategies emerge for counterfactual unfairness, but many underperform classic PPO. 
For the demographic disparity, the strategies underperforming classic PPO focus on ripening bushes and eating berries, while the ones overperforming it are a mix of either ripening bushes, eating berries and obstructing other players or moving and changing the colour of the bushes. On the other hand, for conditional statistical disparity and counterfactual unfairness, obstructing other players and moving constitute the overperforming strategies.
In conclusion, while the frequencies of actions differ between agents with and without the sensitive attribute, their strategies focus on similar actions regardless of the models used in their training.
% In conclusion, there is no substantial difference between the policies adopted by agents with and without the sensitive attribute, although the models used in their training are distinct. This outcome shows that the penalty encourages no distinction between agents' behaviour but converges to a single strategy.
\paragraph{Key takeaways.} Fairness improvements require tuning penalty parameters, as optimal strategies vary across fairness metrics. Fairness improvement does not necessitate distinct behaviours across groups.
% , demonstrating a convergence towards similar strategies even if the training models are distinct.

\begin{table}[h!]
\centering
\resizebox{0.48\textwidth}{!}{ % Resize to fit half a page width
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Policy ($\alpha$, $\beta$)} & \textbf{PoF (Non-sensitive) \textdownarrow } & \textbf{PoF (Sensitive) \textdownarrow} & \textbf{Unfairness \textdownarrow} \\ \hline
\multicolumn{4}{|c|}{\textbf{Demographic Parity}} \\ \hline
(0.0, 0.25) & -56\% & -53\% & 0.23 \\ \hline
(0.5, 0.5) & -52\% & -50\% & 0.26 \\ \hline
(1.0, 1.0) & -47\% & -47\% & 0.29 \\ \hline
(0.25, 0.25) & -45\% & -44\% & 0.29 \\ \hline
\multicolumn{4}{|c|}{\textbf{Conditional Statistical Parity G1/G2}} \\ \hline
(0.75, 0.25) & -57\% & -54\% & 0.15, 0.14 \\ \hline
(1.0, 0.0) & -57\% & -55\% & 0.15, 0.15 \\ \hline
(0.5, 1.0) & -56\% & -54\% & 0.15, 0.14 \\ \hline
(1.0, 1.0) & -47\% & -46\% & 0.17, 0.19 \\ \hline
\multicolumn{4}{|c|}{\textbf{Counterfactual Fairness}} \\ \hline
(1.0, 1.0) & -38\% & -42\% & 0.25 \\ \hline
(0.75, 0.5) & -33\% & -39\% & 0.28 \\ \hline
(0.0, 0.75) & 5\% & 18\% & 0.41 \\ \hline
(0.25, 0.75) & 1\% & 5\% & 0.44 \\ \hline
\end{tabular}
}
\caption{Price of Fairness (PoF) for agents with and without sensitive attributes across the fairest four fair-PPO policies.}
\label{tab:combined-PoF}
\end{table}


% \begin{table}[h!]
% \centering
% \small
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Policy ($\alpha$, $\beta$)} & \textbf{PoF (Non-sensitive) \textdownarrow } & \textbf{PoF (Sensitive) \textdownarrow} & \textbf{Fairness \textdownarrow} \\ \hline
% \multicolumn{4}{|c|}{\textbf{Demographic Parity}} \\ \hline
% (0.25, 1.0) &  3\% &  3\% & 0.58 \\ \hline
% (1.0, 0.25) &  4\% &  4\% & 0.57 \\ \hline
% (0.0, 1.0) & -30\% & -25\% & 0.37 \\ \hline
% (1.0, 1.0) & -47\% & -47\% & 0.29 \\ \hline
% \multicolumn{3}{|c|}{\textbf{Conditional Statistical Parity}} \\ \hline
% (0.0, 0.25) & -11\% & -10\%  & 0.74, 0.03\\ \hline
% (0.25, 0.25) & -5\% & -3\% & 0.27, 0.41 \\ \hline
% (0.25, 1.0) & -20\% & -14\% & 0.12, 0.11 \\ \hline
% (1.0, 0.0) & -57\% & -55\% & 0.18, 0.04 \\ \hline
% \multicolumn{3}{|c|}{\textbf{Counterfactual Fairness}} \\ \hline
% (0.25, 0.0) & 15\% & -28\% \\ \hline
% (0.0, 0.25) & 15\% & 3\% \\ \hline
% (0.75, 0.5) & -33\% & -39\% \\ \hline
% (1.0, 1.0) & -38\% & -42\% \\ \hline
% \end{tabular}
% \caption{Price of Fairness (PoF) for agents with and without sensitive attributes across a subsample of fair-PPO policies.}
% \label{tab:combined-PoF}
% \end{table}


\begin{figure*}[htbp]
    \centering
    % First row of plots
    \begin{subfigure}[b]{0.4\textwidth} % Reduced width
        \centering
        \includegraphics[width=\textwidth]{images/DP_fairness_boxplot.pdf}
        \label{boxplot-DP}
    \end{subfigure}
    \hspace{-0.5em} % Reduce horizontal spacing
    \begin{subfigure}[b]{0.4\textwidth} % Reduced width
        \centering
        \includegraphics[width=\textwidth]{images/CSP_G1_fairness_boxplot.pdf}
        \label{boxplot-CSP-G1}
    \end{subfigure}
    
    \vspace{-1.5em} % Reduce vertical spacing between rows
    
    % Second row of plots
    \begin{subfigure}[b]{0.4\textwidth} % Reduced width
        \centering
        \includegraphics[width=\textwidth]{images/CSP_G2_fairness_boxplot.pdf}
        \label{boxplot-CSP-G2}
    \end{subfigure}
    \hspace{-0.5em} % Reduce horizontal spacing
    \begin{subfigure}[b]{0.4\textwidth} % Reduced width
        \centering
        \includegraphics[width=\textwidth]{images/CF_fairness_boxplot.pdf}
        \label{boxplot-CF}
    \end{subfigure}
    
    \vspace{-2em} % Further reduce vertical spacing
    \caption{Box plots reporting how unfairness decreases for different metrics when adopting fair-PPO compared to classic PPO. On the x-axis, we show the algorithms with various combinations of $\alpha$ and $\beta$, with $\alpha = 0$ and $\beta = 0$ representing classic PPO (in bold). The y-axis shows the metrics, the demographic disparity, and the conditional statistical disparity for the two groups of agents (G1 and G2) characterised by different preferences for red and blue berries and counterfactual unfairness.}
    \label{combined-boxplots}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    % Row 1: Non-sensitive agents
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/DP_non_protected_radar_plot.pdf}
        \vspace{-1.5em}
        % \caption{Demographic parity (non-sensitive).}
        \label{radar1-DP}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/CSP_non_protected_radar_plot.pdf}
        \vspace{-1.5em}
        % \caption{Conditional statistical parity (non-sensitive).}
        \label{radar1-CSP}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/CF_non_protected_radar_plot.pdf}
        \vspace{-1.5em}
        % \caption{Counterfactual fairness (non-sensitive).}
        \label{radar1-CF}
    \end{subfigure}
    \begin{subfigure}[b]{0.11\textwidth}
        \centering
        \raisebox{-2em}{ % Push the figure down by 1em
            \includegraphics[width=\textwidth]{images/legend.pdf}
        }
        \vspace{-0em}
    \end{subfigure}

    \vspace{-1em} % Reduced vertical space between rows

    % Row 2: Sensitive agents
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/DP_protected_radar_plot.pdf}
        \vspace{-1.5em}
        \caption{Demographic parity.}
        \label{radar2-DP}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/CSP_protected_radar_plot.pdf}
        \vspace{-1.5em}
        \caption{Conditional statistical parity.}
        \label{radar2-CSP}
    \end{subfigure}
    \hspace{1em}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/CF_protected_radar_plot.pdf}
        \vspace{-1.5em}
        \caption{Counterfactual fairness.}
        \label{radar2-CF}
    \end{subfigure}
    \hspace{6em}
    \hfill
    % \begin{subfigure}[b]{0.07\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{images/legend.png}
    %     \vspace{-0em}
    % \end{subfigure}

    \vspace{-0.5em} % Reduced vertical space

    \caption{Radar plots of the frequency of actions for agents without the sensitive attribute (non-sensitive agents, top row) and with the sensitive attribute (sensitive agents, bottom row) for classic and fair-PPO across fairness metrics. Colours match the box plots in~\ref{combined-boxplots}.}
    \label{radar-plots}
\end{figure*}

\section{Conclusion}
This paper extends PPO by incorporating a penalty term based on fairness metric violations in the loss function. We design two penalty components: a retrospective component that addresses fairness violations based on past rewards and a prospective component that anticipates future fairness violations by leveraging the value function to estimate upcoming rewards. We refer to this variation of PPO as fair-PPO.

We found that fair-PPO can reduce disparities/unfairness across metrics while balancing tradeoffs between groups, making them suitable for both collaborative and competitive decision-making. However, counterfactual unfairness remains challenging when agent groups do not interact or influence each other’s strategies (by assuming in factual episodes, none of the agents possess the sensitive attribute, while in counterfactual episodes, all agents are assigned the sensitive attribute.). Achieving fairness requires shared trade-offs, with both groups making comparable sacrifices in rewards to reach parity. Finally, fairness improvements depend on careful tuning of penalty parameters, as optimal strategies vary across metrics. Still, fairness does not require distinct agent behaviours across groups with and without the sensitive attribute.

This work represents a first step in developing and exploring a fairness-aware PPO based on metrics that assess fairness in MAS involving agents with and without sensitive attributes. In future work, we aim to extend the experiments to real-world scenarios, such as improving accessibility in smart cities or addressing transport-related challenges, where fairness considerations are critical in our MAS setting.

\section*{Ethical Statement}

There are no ethical issues.

% \section*{Acknowledgments}


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\clearpage
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}
