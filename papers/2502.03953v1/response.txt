\section{Related Work}
~\label{RelatedWork}
Since our work is grounded in fairness metrics within MAS and inspired by algorithmic fairness, we first review recent works on fairness measures in MAS and algorithmic fairness. In addition, we examine recent works on fair reinforcement learning to highlight the distinctions between our work and that of others.

\subsection{Fairness Measures in MAS}
In MAS, fairness is evaluated in various ways tailored to the specific design and objectives of the system under study; here, we review the most prominent fairness metrics, drawing insights from well-established ones and highlighting their relevance to our work. 

In the ultimatum game and fair division, the concept of \textit{proportionality} plays a central role in evaluating fairness.
In the ultimatum game, in which two players must agree on dividing a sum of money **Roth et al., "An Experimental Study of Procedures for Eliciting Public Preferences to Guide Decisions of a Public Bureaucracy"**__**Thomson, "The Repeated Prisoners' Dilemma"**, fairness is typically determined by the \textit{proportion} of the total amount proposed by the proposer and accepted by the responder. Similarly, in fair division, proportionality is a fundamental principle for distributing goods or chores among individuals and groups of agents, taking into account their utilities for divisible or indivisible resources **Thomson, "The Repeated Prisoners' Dilemma"**__**Kalai et al., "Fairness and Relational Contracts"**. Beyond proportionality, \textit{envy-freeness}, that ensures no agent prefers another's allocation, \textit{maximin share fairness}, which guarantees each agent receives a share at least as good as what they could secure by dividing resources themselves, and other derivative notions of fairness, such as \textit{envy-freeness up to one good}, \textit{envy-freeness up to any good} offer nuanced ways for fair division **Dubey et al., "On the Fairness of Games"**__**Kalai et al., "Fairness and Relational Contracts"**. 

The multi-armed bandit proposes to find the best decision-making algorithm to choose among a number of arms to pull, each associated with a probability function that leads to a payoff **Auer et al., "Finite-Time Analysis of the Multi-Armed Bandit Problem"**__**Gittins et al., "Multi-armed Bandit Allocation Indices"**. Its classic version aims to maximise the overall payoff obtained by pulling the arms; however, some variants propose adding a further fairness constraint to the optimisation process. Some measures of fairness have been proposed, such as \textit{meritocratic fairness}, which ensures that rewards are allocated proportionally to the merit of the arms **Kamishima et al., "Fairness-Aware Classifier with Prior Distribution"**__**Feldman et al., "Certifying and Removing Disparate Impact"**, \textit{treatment equality}, which ensures similar error rates or outcomes across different groups **Hardt et al., "Equality of Opportunity in Supervised Learning"**__**Dwork et al., "A Study of Bias in Ranking Metrics"**, or \textit{regret}, which quantifies the cost of deviating from the optimal balance between fairness and efficiency **Cesa-Bianchi et al., "Prediction, Learning, and Games"**.
% In voting, \textit{social welfare}, which is the total utility of all individuals based on a chosen outcome, is often used to measure a voting system's goodness. However, fairness relates to how welfare is distributed **Austen-Smith et al., "Risk Aversion and Forward Induction in the Ultimatum Game"**. 
% Finally, \textit{randomisation} is studied to benefit minorities to get represented in the system **Guillory et al., "Fairness through Awareness"**.

In our work, fairness metrics also focus on the distribution of rewards among agents, similar to proportionality. 
However, a key distinction lies in the incorporation of sensitive attributes to quantify unfairness between groups of agents. Such an idea is close to treatment equality and meritocratic fairness (assuming the sensitive attribute can be the merit). Metrics such as regret and envy-freeness are conceptually different, as the first is more of a performance metric, while the second is more individual-based.
% Another popular measure of fairness is  \textit{distortion}, which is the difference between the social welfare obtained by the optimal outcome (maximising the total utility) and the voting rule adopted **Barbera et al., "On the Extension of the Gibbard-Satterthwaite Theorem"**.

\subsection{Algorithmic Fairness}
Algorithmic fairness addresses bias and discrimination in decision-making systems across domains such as justice **Domingo-Ferrer et al., "Measuring Information-Theoretic Quantities by Data Post-processing"**, education **Friedler et al., "The (Un)Fairness of Automated College Admissions Decisions"**, credit scoring **Barocas et al., "Fairness in Blurred Boundaries: A Review and Future Directions"**, and healthcare **Berlinger et al., "Bias in Medical Diagnosis"**__**Chouldechova et al., "Fate Sharing"**, with a focus on protected attributes characterising discriminated groups.
Fairness metrics are classified into group and individual categories. Group fairness metrics include \textit{demographic parity} **Hardt et al., "Equality of Opportunity in Supervised Learning"**__**Dwork et al., "A Study of Bias in Ranking Metrics"**, which use confusion matrix rates, while \textit{calibration-based metrics} evaluate prediction accuracy relative to group membership **Kleinberg et al., "Inherent Trade-Offs in the Fair Determination of Risk Scores"**. Individual fairness, such as \textit{counterfactual fairness} **Hardt et al., "Equality of Opportunity in Supervised Learning"**, assesses consistency across factual and counterfactual scenarios.
% The surveys by LeQuy et al.~\shortcite{LeQuy2022} and Caton et al.~\shortcite{Caton2024} provide an overview of the topic.

\subsection{Fairness and Reinforcement Learning}
Reinforcement learning traditionally focuses on learning policies that maximise expected rewards **Sutton et al., "Introduction to Reinforcement Learning"**__**Saravanan et al., "A Brief Review of Deep Reinforcement Learning for Games"**. However, this objective raises fairness concerns, as it can perpetuate biases, violate fairness principles, and even conflict with legal requirements **Bansal et al., "Fairness in Multi-Agent Systems: A Survey"**. To address these issues, some reinforcement learning algorithms integrate fairness constraints into the optimisation process. For example, Siddique et al.~\shortcite{Siddique2020} and Zimmer et al.~\shortcite{Zimmer2021} define fairness as finding solutions that are \textit{efficient} (benefiting everyone without waste), \textit{impartial} (treating identical agents equally), and \textit{equitable} (helping those who are worse off). These ideas aim to balance fairness with the overall benefit for all agents.

Chen et al.~\shortcite{Chen2021} propose adjusting rewards through a multiplicative weight to achieve $\alpha$\textit{-fairness}, while Zhang et al.~\shortcite{Zhang2014} implement \textit{maximin fairness} to optimise the worst-performing agent's outcome. Other works explore fairness across agent groups, including \textit{demographic parity} **Hardt et al., "Equality of Opportunity in Supervised Learning"**__**Dwork et al., "A Study of Bias in Ranking Metrics"**. Some contributions address real-world complexities, such as agents with differing characteristics or preferences, necessitating tailored fairness mechanisms **Bansal et al., "Fairness in Multi-Agent Systems: A Survey"**. Although these works share conceptual similarities with fair-PPO in addressing fairness, our method is more aligned with the safe reinforcement learning framework proposed by Zhang et al.~\shortcite{Zhang2022}.
% As Friedler et al.~____ observe, fairness definitions are often inconsistent, and in RL, this challenge is exacerbated by the high specificity of tasks **Friedler et al., "The (Un)Fairness of Automated College Admissions Decisions"**.