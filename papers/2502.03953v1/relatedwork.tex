\section{Related Work}
~\label{RelatedWork}
Since our work is grounded in fairness metrics within MAS and inspired by algorithmic fairness, we first review recent works on fairness measures in MAS and algorithmic fairness. In addition, we examine recent works on fair reinforcement learning to highlight the distinctions between our work and that of others.

\subsection{Fairness Measures in MAS}
In MAS, fairness is evaluated in various ways tailored to the specific design and objectives of the system under study; here, we review the most prominent fairness metrics, drawing insights from well-established ones and highlighting their relevance to our work. 

In the ultimatum game and fair division, the concept of \textit{proportionality} plays a central role in evaluating fairness.
In the ultimatum game, in which two players must agree on dividing a sum of money~\cite{Guth2014,Debove2016}, fairness is typically determined by the \textit{proportion} of the total amount proposed by the proposer and accepted by the responder. Similarly, in fair division, proportionality is a fundamental principle for distributing goods or chores among individuals and groups of agents, taking into account their utilities for divisible or indivisible resources~\cite{Lindner2016,Amantidis2023,Murhekar2024}. Beyond proportionality, \textit{envy-freeness}, that ensures no agent prefers another's allocation, \textit{maximin share fairness}, which guarantees each agent receives a share at least as good as what they could secure by dividing resources themselves, and other derivative notions of fairness, such as \textit{envy-freeness up to one good}, \textit{envy-freeness up to any good} offer nuanced ways for fair division~\cite{Lipton2004,Budish2011,Caragiannis2019}. 

The multi-armed bandit proposes to find the best decision-making algorithm to choose among a number of arms to pull, each associated with a probability function that leads to a payoff~\cite{Bouneffouf2020}. Its classic version aims to maximise the overall payoff obtained by pulling the arms; however, some variants propose adding a further fairness constraint to the optimisation process. Some measures of fairness have been proposed, such as \textit{meritocratic fairness}, which ensures that rewards are allocated proportionally to the merit of the arms~\cite{Joseph2016}, \textit{treatment equality}, which ensures similar error rates or outcomes across different groups~\cite{Liu2017}, or \textit{regret}, which quantifies the cost of deviating from the optimal balance between fairness and efficiency~\cite{Li2020,Patil2021,Jones2023,Barman2023}.
% In voting, \textit{social welfare}, which is the total utility of all individuals based on a chosen outcome, is often used to measure a voting system's goodness. However, fairness relates to how welfare is distributed~\cite{Kaplow2003,HÃ¸gsgaard2023}. 
% Finally, \textit{randomisation} is studied to benefit minorities to get represented in the system~\cite{Procaccia2010,Aziz2020}.

In our work, fairness metrics also focus on the distribution of rewards among agents, similar to proportionality. 
However, a key distinction lies in the incorporation of sensitive attributes to quantify unfairness between groups of agents. Such an idea is close to treatment equality and meritocratic fairness (assuming the sensitive attribute can be the merit). Metrics such as regret and envy-freeness are conceptually different, as the first is more of a performance metric, while the second is more individual-based.
% Another popular measure of fairness is  \textit{distortion}, which is the difference between the social welfare obtained by the optimal outcome (maximising the total utility) and the voting rule adopted~\cite{Caragiannis2011,Boutilier2012,Caragiannis2017,Ebadian2022}.

\subsection{Algorithmic Fairness}
Algorithmic fairness addresses bias and discrimination in decision-making systems across domains such as justice~\cite{Berk2019}, education~\cite{Baker2021}, credit scoring~\cite{Kozodoi2022}, and healthcare~\cite{Vyas2020}, \cite{Giovanola2022}, with a focus on protected attributes characterising discriminated groups.
Fairness metrics are classified into group and individual categories. Group fairness metrics include \textit{demographic parity}~\cite{Kamishima2012} and \textit{equalised odds}~\cite{Hardt2016}, which use confusion matrix rates, while \textit{calibration-based metrics} evaluate prediction accuracy relative to group membership~\cite{Chouldechova2016}. Individual fairness, such as \textit{counterfactual fairness}~\cite{Kusner2018}, assesses consistency across factual and counterfactual scenarios.
% The surveys by LeQuy et al.~\shortcite{LeQuy2022} and Caton et al.~\shortcite{Caton2024} provide an overview of the topic.

\subsection{Fairness and Reinforcement Learning}
Reinforcement learning traditionally focuses on learning policies that maximise expected rewards~\cite{Sutton2018}. However, this objective raises fairness concerns, as it can perpetuate biases, violate fairness principles, and even conflict with legal requirements~\cite{Jabbari2017}. To address these issues, some reinforcement learning algorithms integrate fairness constraints into the optimisation process. For example, Siddique et al.~\shortcite{Siddique2020} and Zimmer et al.~\shortcite{Zimmer2021} define fairness as finding solutions that are \textit{efficient} (benefiting everyone without waste), \textit{impartial} (treating identical agents equally), and \textit{equitable} (helping those who are worse off). These ideas aim to balance fairness with the overall benefit for all agents.

Chen et al.~\shortcite{Chen2021} propose adjusting rewards through a multiplicative weight to achieve $\alpha$\textit{-fairness}, while Zhang et al.~\shortcite{Zhang2014} implement \textit{maximin fairness} to optimise the worst-performing agent's outcome. Other works explore fairness across agent groups, including \textit{demographic parity}~\cite{Jiang2019,Wen2021,Chi2022}. Some contributions address real-world complexities, such as agents with differing characteristics or preferences, necessitating tailored fairness mechanisms~\cite{Yu2023,Ju2024}. Although these works share conceptual similarities with fair-PPO in addressing fairness, our method is more aligned with the safe reinforcement learning framework proposed by Zhang et al.~\shortcite{Zhang2022}.
% As Friedler et al.~\cite{Friedler2016} observe, fairness definitions are often inconsistent, and in RL, this challenge is exacerbated by the high specificity of tasks~\cite{Reuel2024}.