\section{Related Work}
\subsection{Authorship Attribution}

Considerable previous research has evaluated computational approaches to authorship attribution \citep{huang2024authorship}. Recently, many studies in this area have focused on distinguishing between human and LLM-generated texts; however, for this work our interest lies solely in distinguishing between texts written by human authors. Many ``lexical, syntactic, semantic, structural, and content-specific'' features have been used in computational authorship attribution approaches, including ``character and word frequencies, parts of speech, punctuation, topics, and vocabulary richness'' \citep{huang2024authorship}. These features are often used as inputs for a variety of statistical and machine learning methods including support vector machines (SVMs), logistic regression, recurrent neural networks (RNNs), long short-term memory (LSTMs), Siamese networks, and more \citep{huang2024authorship}. 

Since the introduction of transformer-based large language models (LLMs) in 2017 \citep{vaswani2017attention}, scholars have also explored their use for authorship attribution. BERT and its many variants have been applied to this task, often in combination with techniques like contrastive learning, gradual unfreezing, and slanted triangular learning \citep{huang2024authorship}. Recent work has also explored the ability of larger encoder-decoder \citep{hicke2023t5, najafi2022text} and decoder-only \citep{huang2024alms, huang2024large, adewumi2024limitations, wen2024aidbench} models to perform attribution.

However, of these studies only \citet{hicke2023t5} and \citet{adewumi2024limitations} use literary data in their analyses. In addition, only \citet{huang2024large} examine what features models use to attribute texts, and they do so only by asking the models to self-report their reasoning. In contrast to these works, we do not study authorship attribution with the intention of creating or evaluating maximally effective attribution methods. Instead, our interest is in studying what signals of literary style exist at very small scales and to what extent LLMs can help us identify and analyze these signals. To this end, we focus on what LLM performance can tell us about literary style and about LLMs' capabilities, including a series of ablation and probing experiments into what features the models use for attribution.

\subsection{Genre Identification}

Genre identification is often a more difficult to define than authorship attribution due to the complexity of genre as a concept; unlike authorship, variable criteria may be used to define a genre and what texts belong to it \citep{underwood2016genres}. However, despite these challenges, many computational approaches to genre identification have been proposed and evaluated. Some use bag-of-word features along with statistical and light-weight machine learning methods like logistic regression, Naive Bayes, SVMs, and more \citep{allison2011quantitative, underwood2016genres, underwood2013mapping, hettinger2016classification, sharmaa2020rise}. Other works use similar statistical and ML methods along with additional features like emotion arcs \citep{kim-etal-2017-investigating} or more in-depth linguistic annotations \citep{calvo2021novel}. Yet another branch of research on genre identification uses unsupervised clustering algorithms with bag-of-words features \citep{al2018stylometric}, social network features \citep{ardanuy2015clustering, coll-ardanuy-sporleder-2014-structure}, or embeddings and topic classifications \citep{sobchuk2024computational}.

Recent research has also explored the effectivity of transformer-based LLMs for genre identification. Much of this research has focused on non-literary definitions of genre \citep{UCHIDA2024100089, kuzman2023troubling, roussinov2025controlling, make5030059}; these studies have largely found model performance promising, although they have also identified some weaknesses, such as expanding to cross-domain data. Two studies, \citet{liu2020deepgenre} and \citet{bamman2024classification}, have explored using a range of LLMs for literary genre identification. Both studies look at $\sim$500 word or token chunks of literary texts. \citet{liu2020deepgenre} evaluate several statistical methods, including a Naive Bayes algorithm, and a fine-tuned BERT model for detecting the genre of paragraphs from novels that have a large proportion of genre-relevant keywords. \citet{bamman2024classification} similarly evaluate linear and logistic regression, fine-tuned BERT, RoBERTa, and Llama-3 8b models, and few-shot prompted GPT-4o, Llama-3 70b, and Mixtral 8x22b models for detecting the genre of randomly selected novel passages. \citet{liu2020deepgenre} find that BERT performs the best out of all tested methods and \citet{bamman2024classification} find that the Llama-3 8b, GPT-4o, and Llama-3 70b models do best. Again, these results suggest that LLMs show promise for genre identification tasks. However, no study except \citet{bamman2024classification} explores what features the models use to perform genre identification, and even \citet{bamman2024classification} only ask the chat-enabled models self-report on distinguishing genre characteristics.

Again, our work differs from those cited because its primary interest is in using LLM behavior to evaluate what signals of genre-level style exist in very short text segments, and not optimizing the model's performance on this task.