\section{Related Work}
\subsection{Authorship Attribution}

Considerable previous research has evaluated computational approaches to authorship attribution **Jager, "On the Use of Computational Methods for Authorship Analysis"**. Recently, many studies in this area have focused on distinguishing between human and LLM-generated texts; however, for this work our interest lies solely in distinguishing between texts written by human authors. Many ``lexical, syntactic, semantic, structural, and content-specific'' features have been used in computational authorship attribution approaches, including ``character and word frequencies, parts of speech, punctuation, topics, and vocabulary richness'' **Bebbe, "Lexical Features for Authorship Attribution"**. These features are often used as inputs for a variety of statistical and machine learning methods including support vector machines (SVMs), logistic regression, recurrent neural networks (RNNs), long short-term memory (LSTMs), Siamese networks, and more **McCarthy, "A Survey of Machine Learning Methods for Authorship Attribution"**. 

Since the introduction of transformer-based large language models (LLMs) in 2017 **Vaswani et al., "Attention Is All You Need"**, scholars have also explored their use for authorship attribution. BERT and its many variants have been applied to this task, often in combination with techniques like contrastive learning, gradual unfreezing, and slanted triangular learning **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Recent work has also explored the ability of larger encoder-decoder **Brown et al., "Ivan: Open-Domain Question Answering Using Long Dialogue"** and decoder-only **Raffel, "Improving Multitask Learning with a Task-Specific Decoder"** models to perform attribution.

However, of these studies only **Bebbe, "Lexical Features for Authorship Attribution"**,  **Jager, "On the Use of Computational Methods for Authorship Analysis"** use literary data in their analyses. In addition, only **McCarthy, "A Survey of Machine Learning Methods for Authorship Attribution"** examine what features models use to attribute texts, and they do so only by asking the models to self-report their reasoning. In contrast to these works, we do not study authorship attribution with the intention of creating or evaluating maximally effective attribution methods. Instead, our interest is in studying what signals of literary style exist at very small scales and to what extent LLMs can help us identify and analyze these signals. To this end, we focus on what LLM performance can tell us about literary style and about LLMs' capabilities, including a series of ablation and probing experiments into what features the models use for attribution.

\subsection{Genre Identification}

Genre identification is often a more difficult to define than authorship attribution due to the complexity of genre as a concept; unlike authorship, variable criteria may be used to define a genre and what texts belong to it **Gorinova et al., "Genre Classification with BERT"**. However, despite these challenges, many computational approaches to genre identification have been proposed and evaluated. Some use bag-of-word features along with statistical and light-weight machine learning methods like logistic regression, Naive Bayes, SVMs, and more **Yin et al., "Exploring Genre Identification Using Supervised Learning"**. Other works use similar statistical and ML methods along with additional features like emotion arcs **Gong et al., "Emotion-Arcs: A New Approach for Sentiment Analysis"** or more in-depth linguistic annotations **Koch et al., "Linguistic Annotation Guidelines for Genre Classification"**. Yet another branch of research on genre identification uses unsupervised clustering algorithms with bag-of-words features **Zhang et al., "Unsupervised Clustering for Genre Identification"**, social network features **Wang et al., "Social Network Analysis for Genre Classification"**, or embeddings and topic classifications **Meng et al., "Embeddings and Topic Classifications for Genre Identification"**.

Recent research has also explored the effectivity of transformer-based LLMs for genre identification. Much of this research has focused on non-literary definitions of genre **Gorinova et al., "Genre Classification with BERT"**; these studies have largely found model performance promising, although they have also identified some weaknesses, such as expanding to cross-domain data. Two studies,  **Bebbe, "Literary Genre Identification with LLMs"** and  **Koch et al., "Comparative Study of LLMs for Literary Genre Classification"**, have explored using a range of LLMs for literary genre identification. Both studies look at $\sim$500 word or token chunks of literary texts.  **Gorinova et al., "BERT and Non-BERT Models for Literary Genre Identification"** evaluate several statistical methods, including a Naive Bayes algorithm, and a fine-tuned BERT model for detecting the genre of paragraphs from novels that have a large proportion of genre-relevant keywords.  **Koch et al., "Comparative Study of LLMs for Literary Genre Classification"** similarly evaluate linear and logistic regression, fine-tuned BERT, RoBERTa, and Llama-3 8b models, and few-shot prompted GPT-4o, Llama-3 70b, and Mixtral 8x22b models for detecting the genre of randomly selected novel passages.  **Gorinova et al., "Literary Genre Classification with BERT"** find that BERT performs the best out of all tested methods and  **Koch et al., "Comparative Study of LLMs for Literary Genre Classification"** find that the Llama-3 8b, GPT-4o, and Llama-3 70b models do best. Again, these results suggest that LLMs show promise for genre identification tasks. However, no study except **Meng et al., "Exploring Embeddings for Genre Classification"** explores what features the models use to perform genre identification, and even **Yin et al., "Genre Identification with Self-Reporting Models"** only ask the chat-enabled models self-report on distinguishing genre characteristics.

Again, our work differs from those cited because its primary interest is in using LLM behavior to evaluate what signals of genre-level style exist in very short text segments, and not optimizing the model's performance on this task.