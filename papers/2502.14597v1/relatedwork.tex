\section{Related works}
\label{RelatedWork}

In this section, the preliminaries are presented. In subsection \ref{ImbalancedDatasets}, we present the existing studies for multi-class imbalanced learning with SVMs. Then, the OVO decomposition strategy is described in subsection \ref{OVO}. In subsection \ref{SVM}, we present the SVM model. 


\subsection{Multi-class imbalanced learning with SVMs}
\label{ImbalancedDatasets}
%% Use \subsection commands to start a subsection.


The representative surveys of class imbalanced learning with support vector machines are \cite{review-book} \cite{Review2024}. In the recent survey \cite{Review2024}, the approaches are mainly categorized into: resampling, algorithmic, and fusion methods. The resampling approach is at the data level, including oversampling the minority and undersampling the majority class. The synthetic minority over-sampling technique (SMOTE) is a basic oversampling procedure \cite{2002SMOTE}, and the Static-SMOTE and dynamic-SMOTE are proposed in \cite{Francisco2011A}. The Static-SMOTE resamples $M$ times in the preprocessing stage, and $M$ is the number of total classes. In each resampling step, the class having minimal samples is selected and the SMOTE technique is applied to set the number of samples same as the number of original classes. The Dynamic-SMOTE technique is applied to the minority class to balance the size of classes in each generation of evolutionary algorithm at the training stage.


The algorithmic approaches usually modify the algorithm structure to enhance the robustness in learning the imbalanced datasets\cite{Review2024}. It introduces the technologies of cost sensitive to assign unequal misclassification costs for the majority and minority classes\cite{article_dcs}, hyperplane shifting to compensate the imbalance \cite{2015Near} \cite{hypershift2006}, and kernel adaption to increase resolution around minority samples by using conformal transformation \cite {kernel} \cite {kernal2023}. 

The fusion approach is to combine different techniques or ensemble models for addressing imbalanced datasets. The works of \cite{SDCs} combine the SMOTE technique and different costs (SDC) to overcome the imbalanced problem. A posterior probability SVM (PPSVM) is proposed by weighing imbalanced training samples, and a series of weighted optimization problem are formulated \cite{article_ppsvm}. The ensemble learning method is used into imbalanced datasets by voting on the results of multiple good and weak classifiers \cite{ZHANG2016251}. Based on the kernel distance and SMOTE, the new samples are generated to deal with imbalanced datasets \cite{GUO2024110986}. A weighted kernel-based SMOTE (WK-SMOTE) \cite{wk-smote} is proposed by oversampling the feature space along with cost sensitive technique to handle the imbalanced datasets. The works of \cite{2016A} undersample the majority and oversample the minority based on the support vectors, and uses the ensemble approach to predict the class label. An EBCS-SVM (evolutionary bilevel cost-sensitive SVMs) is proposed in \cite{rosales2022handling} to combine an evolutionary algorithm and sequential minimal optimization to handle the imbalanced classification. An SEOA (SVM and Evolutionary algorithms) method is proposed by combining the oversampling methods and evolutionary algorithms to deal with the imbalanced data \cite{evolutionary2024}. Besides, the multi-objective approach is to optimize the multiple objectives for training an SVM, including margin maximization, minimization of regularization from the majority class, and minimization of regularization from the minority class \cite{2014multiobjSVM} \cite{2019Multiobjective}. This approach characterizes the trade-off information among three objectives. 

Since the algorithmic methods do not need data preprocessing, they have a lower computational cost than the resampling methods. The fusion methods usually perform better than the algorithmic methods, but they are computationally expensively. Besides, the algorithmic approaches including costs sensitive and kernel adaption techniques deal with imbalanced datasets effectively \cite{Review2024}.
~\\

The multi-class problems cannot be solved through the canonical manner of SVMs to get the effective classes, since it may lose performance in one class while trying to gain it in other classes\cite{Fern2013Analysing}. To deal with this task, the researches can be roughly divided into two groups: one single machine and binarization methods. The first methods address the multi-class problems in one single optimization. One objective function is formalized by modifying the optimization problem, and multiple categories are classified simultaneously \cite{Weston1999SupportVM}\cite{singlemachine2016}. A method called GenSVM is provided in \cite{Gensvm2016}, which uses a simplex encoding to reduce the dimensionality of problem and the grid search is applied to find the optimal hyperparameters. MC2ESVM \cite{mc2esvm} uses a cooperative coevolutionary algorithm to deal with the parameter optimization problem. The works of \cite{GUO2021107988} provide to combine the feature selection into decision function in the classifier, and formulate a large optimization problem. The one single machine approaches have to deal with a complex and large optimization problem, which includes a large number of variables \cite{MSVMpack} \cite{mc2esvm}.

The binarization methods decompose a multi-class problem into a number of binary subproblems, which are then solved independently. The decomposition strategies include the OVO \cite{article_ovo}, OVA \cite{article_ova}, and Directed Acyclic Graph (DAG) \cite{dag1999} schemes. 

OVO creates a binary SVM for every possible pair of classes. It can provide higher accuracy through focused pairwise comparisons. When the number of classes is small, it is preferable. 


OVA, also known as One-vs-Rest (OVR), decomposes an $M$-class problem into $M$ subproblems. It trains a binary classifier for each class. It suffers from class imbalance in the training phase. The value of $M$ is larger, the imbalance rate is higher\cite{mc2esvm}.


DAG constructs $M$ ($M$-1)/2 binary SVMs in the training phase, which is the same as OVO. In the testing phase, a rooted binary directed acyclic graph is used to express the decision nodes. Based on the decision function, a decision algorithm is used to find the class for each test sample.


The OVO and DAG methods are more suitable for practical use than the other methods \cite{ComparingOVO}. 


The multi-class SVM and imbalanced learning methods can be combined to deal with classification tasks. An All-in-one Multi-class SVM (AIO-MSVM) algorithm is proposed that samples are handled with $q$-times Markovian resampling technique to improve the generalization capacity, and the algorithm is used to deal with the multi-class tasks \cite{DONG2023109720}. The OVO scheme and ensemble learning are used to multi-class imbalanced learning \cite{ZHANG2016251}. Combining the OVA strategy with resampling techniques are investigated in \cite{liao2008classification}. A technique of near-Bayesian SVMs (NBSVM) \cite{2015Near} is to combine OVA and the modified SVM with boundary shift. 
~\\

The use of binarization methods to deal with multi-class problems is beneficial, and in general, OVO has shown a better behavior \cite{2011An}. The algorithmic approaches are more accurate than resampling \cite{Fernndez2018LearningFI} in the data imbalance problems. In the multi-class imbalanced learning with SVMs, the previous researches have not paid great attention to a multi-class SVM with OVO to handle binary subproblems simultaneously, at the same time, improve the capability of dealing with data imbalance. 

In this paper, a method called i-SVM-DE is proposed. To conquer the independency of binary subproblems, it applies an improved DE algorithm to solve a large optimization problem and obtain support vectors for each class simultaneously. To deal with the data imbalance in a binary subproblem, we propose an improved SVM model, i.e., i-SVM model, which takes advantages of algorithmic approaches. 


\subsection{OVO decomposition strategy}
\label{OVO}

In the OVO decomposition scheme, an $M$-class problem is decomposed into $ M*(M-1)/2 $ binary classification subproblems. In the training phase, every possible pair of classes is trained with a binary classifier. It is expressed with a score matrix $R$. In the prediction phase, a test sample is classified into the class with majority votes. 

\begin{equation}
	R=
	\begin{bmatrix}
		- & k_{12} & ... & k_{1M} \\
		k_{21} & - & ... & k_{2M} \\
		\vdots & \vdots & \ddots & \vdots \\
		k_{M1} & \vdots & \vdots & k_{MM} \\
	\end{bmatrix}
\end{equation}

\noindent where $k_{ij}\in{ [0,1]}$ is the confidence of binary classifier composed of class $i$ and $j$. We can get that $k_{ji}=1-k_{ij}$. Once a score matrix is determined, the well-known aggregation method called Majority Voting (MV) strategy \cite{Friedman:96} is used to combine the binary classifiers. Each binary classifier would vote for the class label, and the one with greater confidence is the predicted class. It is calculated as (\ref{classlabelpredict}).

\begin{equation}\label{classlabelpredict}
	\begin{split}
		& classL = \mathop{\arg\max_{i=1,...,M}}\sum_{1\leq j\neq i\leq M}s_{ij},\\
		& s_{ij} = \left\{
		\begin{array}{rcl}
			1,\quad k_{ij}\geq k_{ji}\\
			0,\quad otherwise\\
		\end{array} \right. \\
	\end{split}
\end{equation}
\noindent where $classL$ is the predicted class label.

If the number of votes for two class labels are the same, the class label with fewer training samples is selected. The reason is that the minority class is usually more important than the majority one in the decision-making.

\subsection{Support vector machine}
\label{SVM}

The classical SVM aims to find the optimal separation hyperplane maximizing the margin between two classes. To be specific, a collection of samples are represented as: $\{(x_i,y_i)| i = 1,2,...,N\}$, $ x_i \in \mathbb{R}^m $ is an $ m $-dimensional data sample, $ y_i \in \{-1,1\}$ indicates the target class, and $ N $ is the number of samples. The hyperplane is written as $w^Tx + b = 0 $, where $w\in \mathbb{R}^m \ $and $\ b\in \mathbb{R}$ are the weight vector and bias, respectively. The majority class label is set as $ +1 $, and the minority one is as $ -1 $. It is called the hard-margin SVM, which is solved to obtain the hyperplane, as follows:

\begin{equation}\label{TraditionalSVM}
	\begin{split}
		&\mathop{\min}\limits_{w,b}\frac{1}{2}{\left \|w \right \|}^{2}\\
		& s.t. \quad y_i(w^Tx_i+b)\ge 1, \ \forall i\in \{1,2,\cdots,N\}.
	\end{split}
\end{equation}

This classifier is suitable for the samples which can be separated by the hyperplane linearly. However, most practical datasets are not completely separable by linear functions. Then, a set of slack variables $\xi_i\ge 0$ (for each sample $x_i$) and the regularization cost $C>0$ are introduced to improve the generalization capacity of SVM classifier. A soft-margin optimization problem is reformulated as follows:

\begin{equation}
	\begin{split}
		& \mathop{\min}\limits_{w,b,\xi}(\frac{1}{2}{\left \|w \right \|}^{2}+C\sum\limits_{i=1}^N\xi_i)\\
		& s.t. \quad y_i(w^Tx_i+b)\ge 1-\xi_i\\
		&\xi_i \ge 0,\quad\forall i\in \{1,2,...,N\}.
	\end{split}
\end{equation}
where $\sum\limits_{i=1}^N\xi_i$ is the penalty factor that measures the total misclassification error, and $C$ is a parameter to balance the tradeoff between maximizing the margin and minimizing the error.

While the samples are in a high-dimensional space, it will be laborious to find the optimal hyperplane of $w$ and $b$. Fortunately, the Lagrange multiplier combining with the Wolfe dual can be used, the problem is transformed into:

\begin{equation}
	\begin{split}
		&\mathop{\min}\limits_{w,b,\xi}\mathop{\max}\limits_{\alpha,\mu}\{\frac{1}{2}{\left \|w \right \|}^{2}+C\sum\limits_{i=1}^N\xi_i\\
		&-\sum\limits_{i=1}^N\alpha_i(y_i(w^Tx_i+b)-1+\xi_i)-\sum\limits_{i=1}^N\mu_i\xi_i\}\\
		& s.t. \quad \alpha_i\ge0,\mu_i\ge0\\
		&\xi_i \ge 0,\quad\forall i\in \{1,2,...,N\}.
	\end{split}
\end{equation}

where $\alpha_i$ and $\mu_i$ are the Lagrange multipliers. With the Wolfe dual, the problem is converted into:

\begin{equation}\label{dual}
	\begin{split}
		& \mathop{\max}\limits_{\alpha_i}\{\sum\limits_{i=1}^N\alpha_i
			-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j\}\\
		& s.t. \quad \sum\limits_{i=1}^N\alpha_iy_i = 0\\
		&0\le\alpha_i\le C,\quad\forall i\in \{1,2,...,N\}.\\
	\end{split}
\end{equation}

The difficult primal problem is transformed into a dual optimization problem. The found hyperplane is the same as the one in (\ref{TraditionalSVM}) problem, since the objective function in (\ref{TraditionalSVM}) is a quadratic equation.

Although the capacity of SVM classifier is improved by using $\xi_i$ and $ C $, it still cannot deal with the linearly inseparable problems well. Hence, SVM transforms the samples into a high-dimensional feature space by using a non-linear mapping function, i.e., kernel function $\phi$. Then the samples are very likely separable \cite{liu2022data}. Additionally, the inner product operation in a high-dimensional space can be calculated by using the kernel function, which is expressed as:

\begin{equation}
	K(x_i,x_j)=\phi(x_i)\phi(x_j).
\end{equation}

It can be applied to transform the dual optimization problem in (\ref{dual}) into:

\begin{equation}\label{kernalsvm}
	\begin{split}
		& \mathop{\max}\limits_{\alpha_i}\{\sum\limits_{i=1}^N\alpha_i
		-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)\}\\
		& s.t. \quad \sum\limits_{i=1}^N\alpha_iy_i = 0\\
		&0\le\alpha_i\le C,\quad\forall i\in \{1,2,...,N\}.\\
	\end{split}
\end{equation}