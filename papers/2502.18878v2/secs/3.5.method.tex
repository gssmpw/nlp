

\section{Schema Reinforcement Learning}\label{sec:method}

A straightforward approach to improve models' ability to generate JSON outputs is to conduct SFT. 
However, in practice, we encounter a significant challenge: the absence of high-quality, valid JSON strings that conform to the schemas we’ve collected. 
In constructing the training set for \ourbench, we explored several methods to obtain such JSON samples, including using automatic JSON generators and model-based prompting, as shown in~\Cref{fig:combined-stats}. 
Unfortunately, neither approach was effective for generating JSON outputs that adhered to complex schemas at scale.


Therefore, instead of relying solely on manually curated datasets, we propose Schema Reinforcement Learning (SRL) by leveraging the model itself to generate the required valid JSON strings during training, allowing it to iteratively improve its performance in generating structured data.
Building upon the framework presented in PRIME~\citep{cui2025processreinforcementimplicitrewards}, we incorporate an online reinforcement learning approach to enhance the model’s performance further. 

Our algorithm is structured into three main phases, with each phase serving a specific purpose.
In the sampling phase, we begin by generating $K$ responses for each query in the dataset using the policy model $\pi_\theta$.
Next, in the rewarding phase, we assess the quality of each response by obtaining rewards from both the schema validator $r_s$ and the reward model $r_\phi$.
Finally, in the updating phase, we update both the reward model $r_\phi$ and the policy model $\pi_\theta$, and then initiate the next step in the process.
Here we explain each phase in detail:


\paragraph{Sampling Phase.}
During the sampling phase, we reuse the tasks defined in \ourbench as task templates and generate diverse responses from the model. Each task is sampled multiple times to identify the most appropriate task for the current training objectives.

Building on Chain-of-Thought~\citep{wei2023chainofthought}, we introduce Thoughts of Structure (ToS), where the model reflects on the structure while generating JSON strings. This is particularly useful for generating complex JSON objects, which may involve intricate schemas, nested structures, or conditional dependencies.
ToS works by training the model to generate JSON5 strings\footnote{JSON5 is an extension to JSON, more details can be found at \url{https://json5.org/}.} that include reasoning comments before the JSON output. During training, comments outline reasoning steps for each key-value pair, helping guide the generation process. During validation, these comments are ignored, and only the final JSON is validated.

\paragraph{Rewarding Phase.}
In this phase, we obtain rewards from the reward model and combine them with scores from the schema validator to estimate the advantages of each response.
The advantage for the $i$-th response is computed as follows:
\begin{equation}
\begin{aligned}
A^i &= r(\mathbf{y}_i)-\frac{1}{K-1}\sum_{j\neq i}r(\mathbf{y}_j) \\
\end{aligned}
\end{equation}
where $A^i$ represents the estimated advantage of the i-th response, and $r(\mathbf{y}_i)$ is the reward score for the response $\mathbf{y}_i$. We use a leave-one-out estimation to calculate the advantage by comparing the reward of the current response to the average reward of all other responses.
We sum up the advantage from the reward model and the validator to obtain the final advantages.


A naive approach would involve directly using the schema to validate the generated JSON, treating its correctness as the reward. However, as~\Cref{fig:schema-only-snippets} shows, the sensitivity of JSON formatting makes its reward signal sparse and challenging to optimize effectively. 
To address this, we introduce a more fine-grained schema validator that provides a detailed reward signal. This validator calculates the correctness ratio, defined as the proportion of correct tokens out of the total number of tokens in the generated string. In cases where the generated string is only partially valid, the validator computes the correctness ratio for the valid portion of the string. If the string fails to parse as a valid JSON object—due to missing brackets, commas, or other syntax issues—we split the string at the error position and pad with control characters to validate the remaining content.

\paragraph{Updating Phase.}
After obtaining rewards from the validator and reward model, we are ready to update the reward model $r_{\phi}$ and policy model $\pi_\theta$.
Following PRIME, we select Cross Entropy loss to update the reward model and use PPO~\cite {schulman2017proximal} to update the policy model:
\begin{equation}
\begin{aligned}
L_{\text{clip}}(\theta) =E[\min(\frac{\pi_\theta(y|\mathbf{y})}{\pi_{\theta_{\text{old}}}(y|\mathbf{y})}A,
\text{clip}(\frac{\pi_\theta(y|\mathbf{y})}{\pi_{\theta_{\text{old}}}(y|\mathbf{y})},1 -\epsilon,1 + \epsilon)A)] 
\end{aligned}
\end{equation}
where $\epsilon$ controls the clipping range, ensuring that the policy update remains within a safe region. 



\begin{table*}
\centering
\small
\begin{widetabular}{\textwidth}{lccc|c|cccc}
\toprule
& \multicolumn{4}{c}{\textbf{Schema-only Generation}} & \multicolumn{4}{|c}{\textbf{Schema-constrained Reasoning}} \\ \midrule
\textbf{Model} & \textbf{Complex} & \textbf{Custom} & \textbf{Escape} & \textbf{Overall} & \textbf{GSM8K} & \textbf{MATH500} & \textbf{MMLU} & \textbf{ARC-C} \\ \midrule
% orm (llama 3.2-3B) & 82.81 & 66.23 & 67.39 & 72.51 & 78.70 & 34.20 & 61.00 & 80.55 \\
GPT-4o & 84.47 & 61.56 & 37.14 & 61.06 & 97.80 & 41.40 & 86.16 & 97.01 \\
GPT-4o-mini & 68.86 & 46.17 & 16.89 & 43.98 & 86.13 & 31.80 & 49.41 & 77.65 \\
Qwen-2.5 7B  & 72.42 & 43.60 & 11.11 & 42.38 & 94.54 & 38.60 & 74.43 & 91.21 \\
MiniCPM-3 4B & 53.88 & 20.29 & 9.13 & 27.77 & 69.22 & 33.40 & 66.58 & 88.31 \\
\midrule
LLaMA-3.1 8B & 64.26 & 33.07 & 12.02 & 36.45 & 95.91 & 85.60 & 71.83 & 84.98 \\
% \textit{- F.T. on collected JSON} & 40.97 & 25.06 & 12.48 & 26.17 & 92.57 & 68.40 & 64.98 & 81.06 \\
LLaMA-3.1 8B SFT & 74.56 & 46.64 & 60.58 & 60.59 & 89.46 & 63.80 & 66.97 & 84.56 \\
 - \textit{w/o Collected JSON} & 70.84 & 42.06 & 60.35 & 57.75 & 78.39 & 46.00 & 58.87 & 75.68 \\
LLaMA-3.1 8B SRL & 90.48 & 78.67 & 69.86 & 79.67 & 90.90 & 88.00 & 70.74 & 84.81 \\
\midrule
LLaMA-3.2 3B & 49.84 & 27.31 & 8.37 & 28.51 & 80.97 & 35.40 & 62.38 & 79.27 \\
% \textit{- F.T. on collected JSON} & 39.70 & 25.46 & 12.02 & 25.73 & 76.57 & 19.40 & 52.28 & 77.05 \\
LLaMA-3.2 3B SFT & 71.71 & 45.52 & 52.21 & 56.48 & 82.94 & 44.40 & 61.50 & 78.41 \\
 - \textit{w/o Collected JSON} & 72.42 & 42.83 & 54.82 & 56.69 & 78.85 & 36.20 & 59.11 & 75.68 \\
LLaMA-3.2 3B SRL & 82.25 & 66.13 & 69.10 & 72.50 & 84.23 & 43.20 & 57.99 & 78.24 \\
\bottomrule
\end{widetabular}
\caption{Performance comparison of various models in \ourbench, all presented in percentage. We compare two different training strategies: One is fine-tuning with the collected data, and the other conducts reinforcement learning on the train set of \ourbench.}
\label{tab:schemabench_results}
\end{table*}

% \paragraph{Thoughts of Structure (ToS)}



% \paragraph{Fine-grained Schema Validator}


% To provide a more fine-grained signal for training, we build a fine-grained schema validator.
% The most naive way is to use the schema to validate the generated JSON directly, treating the correctness of the JSON as the reward.
% However, due to the sensitivity of the JSON format, the reward signal is sparse and hard to optimize.
% As~\Cref{fig:schema-only-snippets} shows the JSON format is sensitive to the control characters and data types.
% To further boost the performance of our method, we develop a more fine-grained schema validator to provide a more informative reward signal.
% Specifically, given a generated string, our validator is designed to calculate the correctness ratio of the generated content, which is defined as the number of correct tokens divided by the total number of tokens.
% Under this definition, a common scenario is when the string can be interrupted as JSON, we can validate the interrupted object against the schema and calculate the portion of the corrected string.
% Another scenario should considered when the generated string fails to be parsed as a JSON object, which could be caused by missing a closing bracket or a comma.
% In this case, to consider both schema correctness and JSON resolvability, we chunk the JSON string in the parsing error failure position and padding with control characters to validate the remaining strings.
% In this case, the correctness ratio could be defined as the length of the longest correct prefix divided by the total length of the string.
% However, such simple metrics ignore the schema correctness, which is the most important part of the JSON format.
% Thus, we should also consider how much the generated strings satisfy the schema.
% In practice, we chunk the JSON string in the parsing error failure position and padding with control characters to validate the remaining strings.