\begin{figure*}[htb]
\centering
% \includegraphics[width=0.8\textwidth]{figs/hist.pdf}
% \hfill
\includegraphics[width=\linewidth]{figs/clean.pdf}
\caption{Overview of the data curation pipeline. 
We conduct multi-stage cleaning to obtain valid JSON schemas. 
The pie chart on the top right shows the data type distribution of the collected schemas.
The top three data types are string, object, and array.
The error cases in the left corner show possible errors models could make when generating JSON strings according to the given schema.
}
\label{fig:combined-stats}
\end{figure*}

\section{Introduction}
% structure generation 重要
Recent advancements in Large Language Models~\citep{openai2023gpt4,chowdhery2022palm,touvron2023llama,zeng2022glm} have facilitated the development of various intelligent applications like automatic web search~\citep{qin2023webcpm} or software development~\citep{qian2023communicative}.
Among these applications, the structured generation of outputs, represented in \textbf{JSON}\footnote{\url{https://www.json.org/}} format~\citep{chen2025llmer,escarda2024llms}, 
has emerged as a widely utilized feature for integrating language models with various automatic systems and pipelines, enhancing the flexibility of language models in real-world tasks.

% 现在有一些struction generation的工作了，大家都是怎么做的
% 对复杂schema的支持不佳（初步case study）
% -->没有benchmark，没法去系统性的analysis？
Several methods exist for generating JSON strings from LLMs.
Prompting~\citep{pokrass2023structured, he2024doespromptformattingimpact} is a simple approach that works well for basic schemas but struggles with complex logic due to the model’s limited capacity, as~\Cref{fig:combined-stats} shows.
Tool calls~\cite {schick2023toolformer,qin2023toolllm} can convert model output into JSON, but often miss certain schema-specific syntax, leading to incomplete or incorrect results. 
Constraint decoding methods~\citep{deutsch2019general,poesia2022synchromesh,geng2023grammar} like Outlines generate valid JSON strings by adjusting the decoding strategy of LLMs.
The underlying challenge is the difficulty of generating valid JSON strings for intricate schemas, compounded by a lack of comprehensive benchmarks to evaluate model performance on such complex tasks. 
% As shown in~\Cref{fig:combined-stats}, current methods struggle with complex JSON schemas involving deep nesting or intricate dependencies, yet systematic evaluations in this context are still lacking.


% Currently, several methods exist for generating JSON strings from language models, each with different approaches to handling structured generation. 
% For models that natively support JSON schema, direct prompting can generate the desired output~\citep{pokrass2023structured}. 
% For models that lack this support, prompt engineering~\citep{he2024doespromptformattingimpact} can be used, although this typically works only with simpler schemas and may struggle with more complex ones. 
% Another approach involves re-parsing tool calls made by the model~\citep{schick2023toolformer,qin2023tool,qin2023toolllm,qian2023toolink} to convert the output into JSON strings, but this method can lead to misalignment with the intended schema. 
% Alternatively, methods like Outlines~\citep{willard2023efficient} apply constraint decoding to generate valid JSON without modifying the model itself, though this can reduce the quality of the output~\citep{tam2024let}, especially for more complex applications.
% Despite these advancements, there is a significant deficiency in the analysis and benchmarking of models’ capabilities to follow complex JSON schemas.
% As~\Cref{fig:combined-stats} shows, existing methods have demonstrated limited effectiveness when applied to schemas with intricate dependencies or deep nesting, yet comprehensive benchmarks to systematically evaluate their performance in such cases are still lacking.


% Currently, there are no comprehensive benchmarks to systematically evaluate how well these models handle complex JSON schemas, limiting our ability to assess and compare their performance effectively.

% However, a significant deficiency exists in the analysis and benchmarking of the structured generation capabilities of these models.
%为了解决这个问题，我们首先面向复杂scheme构造了schema bench，我们对复杂schema的定义，我们是怎么构造的
This study aims to analyze and enhance the capacity of models to generate valid JSON strings according to a given schema.
Initially, we have developed the~\ourbench comprising around 40K JSON schemas to identify primary challenges that models encounter during the generation of JSON strings.
The benchmark encompasses three categories of challenges: the generation of valid JSON strings with a given JSON schema, the comprehension of instructions inherent to JSON schemas, and the escape of special tokens within JSON strings. 
We benchmark the latest models and find that current models are still limited in dealing with complex JSON schemas, with only $61.06\%$ correctness on the~\ourbench.
In our practice, even after supervised fine-tuning, the model still struggles to learn basic JSON syntax in some cases. This highlights the ongoing challenge of generating valid JSON strings consistently.
% We also find that general reasoning capabilities could be maintained when generating in JSON format.


% Based on the constructed benchmarks, we conduct supervised fine-tuning and find that, in some cases, the model even struggles to learn basic JSON syntax during diverse testing. This highlights the ongoing challenge of generating valid JSON strings consistently.
Subsequently, we propose Schema Reinforcement Learning (SRL), an innovative training pipeline that integrates reinforcement learning with a fine-grained schema validator to enhance the model’s ability to generate structured data.
Furthermore, drawing inspiration from Chain-of-Thought (CoT) reasoning~\citep{wei2023chainofthought}, we introduce a novel concept called Thought of Structure (ToS) within our training pipeline, which encourages the model to engage in deeper reasoning before generating specific JSON strings, guiding it to more effectively navigate complex structures.
% Our results demonstrate that this approach leads to significant improvements in the model’s ability to follow JSON schemas, with up to a $15\%$ boost compared to supervised fine-tuning alone. 
Interestingly, we also observe that, unlike regular fine-tuning, reinforcement learning helps the model maintain its general capabilities more effectively, preserving broader functionality even as it becomes more specialized in structured generation.



% dataset designed to facilitate a robust understanding of JSON schema for the models. This dataset consists of three components: random valid JSON content aligned with a given schema, converted data suitable for tool calling, and structured query-response data formatted in JSON. Following fine-tuning of the model using this curated dataset, we noted substantial improvements in its JSON generation capabilities.

Finally, we evaluate the performance of the fine-tuned models in downstream tasks, such as BFCL~\citep{berkeley-function-calling-leaderboard} 
% and IoA~\citep{chen2024internetagentsweavingweb}
, to validate the generalization of our approach. 
The results indicate that our model exhibits significant performance enhancements when calling tools in JSON format under specified schemas.


Our primary contributions are as follows:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt]
\item We introduce a benchmark of approximately 40K diverse JSON schemas to facilitate rigorous evaluation of model capabilities in structured output generation.
\item We propose a novel training framework with online schema reinforcement learning, achieving up to $16\%$ improvement in valid complex JSON generation rates compared to supervised all baselines.
\item We demonstrate the practical efficacy of our approach through enhanced performance on downstream benchmarks such as BFCL, showing that improvements in structured generation translate directly to superior tasks without compromising general capabilities.
\end{itemize}