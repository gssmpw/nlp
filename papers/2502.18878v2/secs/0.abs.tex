\begin{abstract}
This study investigates the structured generation capabilities of large language models (LLMs), focusing on producing valid JSON outputs against a given schema. 
Despite the widespread use of JSON in integrating language models with programs, there is a lack of comprehensive analysis and benchmarking of these capabilities.
We explore various aspects of JSON generation, such as structure understanding, escaping, and natural language description, to determine how to assess and enable LLMs to generate valid responses.
Building upon this, we propose \ourbench features around 40K different JSON schemas to obtain and assess models' abilities in generating valid JSON.
We find that the latest LLMs are still struggling to generate a valid JSON string.
Moreover, we demonstrate that incorporating reinforcement learning with a Fine-grained Schema Validator can further enhance models' understanding of JSON schema, leading to improved performance. 
Our models demonstrate significant improvement in both generating JSON outputs and downstream tasks.
% To further utilize the advanced reasoning capabilities of modern LLMs, we propose the Thought of Structure (ToS), which encourages the model to think deeper before generating JSON strings.
% s, like BFCL and IoA.
% Our models demonstrate significant advancements in generating JSON outputs and excel in downstream tasks such as BFCL~\citep{berkeley-function-calling-leaderboard} and IoA~\citep{chen2024internetagentsweavingweb}.
\footnote{Our code and data are available at~\repourl.}
\end{abstract}