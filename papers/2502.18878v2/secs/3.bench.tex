

\section{\ourbench}
To construct the~\ourbench, we first introduce how we collect diverse schemas.
Then we detailed how to create challenge tasks based on the schema we collected.
Finally, we conduct a failure mode analysis to obtain an overview of problems when generating JSON strings with LLMs.


% \textbf{Schema-only Generation} and \textbf{Schema-constrained Reasoning}.  

% As shown in~\Cref{fig:stats}, the~\ourbench contains a total of $40,706$ diverse schemas, with an average character length of $35,754$. The average character length of the descriptions within these schemas is $25,152$, and the average nesting depth of the schemas is $16.7$. Detailed statistics for different settings can be found in~\Cref{fig:stats}.
% \begin{table}[htb]
% \centering
% \small
% \begin{tabular}{lccc}
% \toprule
% \textbf{} & \textbf{Complex} & \textbf{Custom} & \textbf{Escape} \\ \midrule
% Counts & 10,177 & 20,352 & 10,177 \\
% \midrule
% Avg. Length & 35,515 & 48,562 & 53,557 \\
% < 2K & 4,014 & 7,903 & 3,955 \\
% < 4K & 6,916 & 13,783 & 6,875 \\
% < 10K & 9,102 & 18,250 & 9,073 \\
% \midrule
% Avg. Desc. Length & 18,342 & 26,973 & 28,319 \\
% % \midrule
% Avg. Depth & 17.3 & 16.3 & 16.9 \\
% \bottomrule
% \end{tabular}
% \caption{Distribution of the~\ourbench. 
% We filtered a total of $40,706$ diverse schemas.
% The schema lengths are measured by the number of characters of the schemas.
% We calculate the depth of the schema by counting the maximum depth of the schema definition.}
% \label{fig:stats}
% \end{table}
\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/examples.pdf}
    \caption{Top: snippets for three sub-tasks in \textbf{Schema-only Generation}. The last two snippets are special fields inserted into basic schemas like the first snippet. Bottom: corresponding common failure cases for three sub-tasks. The first one violates \texttt{minLength} requirement, the second one gives an incorrect base64 string and the third one gives a wrong number of backslash, causing escape error.}
    \label{fig:schema-only-snippets}
\end{figure*}
\subsection{Data Collection}
\ourbench\space is designed to evaluate the structured output generation capabilities of large language models under realistic and complex schema constraints. To achieve that, we crawled a total of $108,528$ schema files from the JSON Schema Store\footnote{\url{https://www.schemastore.org/json/}} and GitHub. These schema files were selected to represent a wide range of applications, domains, and complexity levels, ensuring the diversity and representativeness of \ourbench.

To focus on schemas that do not rely on external resources, we parsed any external URIs referenced within the schemas (both relative and absolute URI), filtering out those containing inaccessible external URIs and reducing the dataset to $46,280$ schemas. The relevant content from these URIs was then merged into the schemas, forming our basic schema data. Following this, we applied a rigorous filtering and validation process to ensure the schemas' compliance with JSON Schema syntax and conventions. As a result, we removed $5,574$ schemas that did not meet these requirements. The remaining schemas were then divided into a training set and a test set, containing $36,960$ and $3,746$ schemas, respectively, which were used for constructing the training and testing datasets.

There are two main task categories in the~\ourbench:
\textbf{Schema-only Generation} involves providing the model with a given schema and evaluating its ability to generate valid JSON strings that comply with the specified schema, including any embedded instructions.
\textbf{Schema-constrained Reasoning} requires the model to generate answers to a given question based on the schema, assessing the model's reasoning abilities while ensuring its output adheres to the schema.
Next, we detailed the construction of each task.

\begin{wraptable}{l}{0.45\textwidth}
\centering
\small
\begin{tabular}{lrrr}
\toprule
\textbf{} & \textbf{Complex} & \textbf{Custom} & \textbf{Escape} \\ \midrule
Counts & & & \\
 - \textit{Train Set} & 9,241 & 18,478 & 9,241 \\
 - \textit{Test Set} & 936 & 1,874 & 936 \\
\midrule
Avg. Length & 35,515 & 48,562 & 53,557 \\
< 2K & 4,014 & 7,903 & 3,955 \\
< 4K & 6,916 & 13,783 & 6,875 \\
< 10K & 9,102 & 18,250 & 9,073 \\
\midrule
Avg. Desc. Length & 18,342 & 26,973 & 28,319 \\
% \midrule
Avg. Depth & 17.3 & 16.3 & 16.9 \\
\bottomrule
\end{tabular}
\caption{Distribution of the~\ourbench. 
We filtered a total of $40,706$ diverse schemas, with an average character length of $35,754$ and an average nesting depth of the schemas is $16.7$. 
We calculate the depth of the schema by counting the maximum depth of the schema definition.
The average character length of the descriptions within these schemas is $25,152$.
}
\label{fig:stats}
\end{wraptable}

\subsection{Schema-only Generation}
The Schema-only Generation task evaluates LLMs’ ability to generate structured output that strictly follows a given schema. We identified three key challenges, each addressed by a specific sub-task. The first, \textbf{Complex Schema}, tests the model's ability to navigate intricate schemas with references and logical compositions. This forms the foundation for models to generate valid JSON strings based on complex schemas. The second, \textbf{Custom Formats}, focuses on interpreting natural language instructions in schema descriptions, requiring models to follow custom formatting rules commonly found in real-world applications. The third, \textbf{Escape Translation}, challenges the model to generate valid JSON strings, correctly handling control characters and escape sequences, a more difficult task than simply adhering to the schema. Failure to properly handle these characters renders the entire JSON string invalid, making post-processing difficult.
% To address these challenges, we designed three distinct sub-tasks, each targeting a specific aspect of schema understanding and JSON generation.
% the basic ability to adhere to the JSON schema, the comprehension and adherence to instructions within the schema, and the proficiency in generating control symbols within JSON.
~\Cref{fig:schema-only-snippets} shows representative snippet of each sub-task.

\textbf{Complex Schema.} This task requires LLMs to generate a valid JSON string under the constraint of a given schema, which is a fundamental ability in schema-constrained generation scenarios. In this task, LLMs will be provided with a schema and asked to generate a valid JSON string for it. During validation, we first check whether the output string is a valid JSON. If the string is valid, we then use the Python \texttt{jsonschema} library to verify if the generated JSON string strictly adheres to the provided schema constraints.
% error static 部分移到 experiments 来讲
% In the process of evaluating the generated outputs, we identified several common error types that occur frequently when LLMs attempt to adhere to complex schema constraints. These errors typically fall into the following failure modes:
%The ability to follow an arbitrary schema provided by the user and generate a valid JSON string is a fundamental aspect of structured output generation in large language models. The \textbf{Complex Schema} sub-task evaluates this core capability by requiring the model to produce JSON outputs that conform to a given schema, without the need for additional contextual instructions or problem statements.

% merge original Custom Formats and Limitation into new Custom Formats
\textbf{Custom Formats.} 
This task involves modifying specific fields in the original schema to adhere to specialized rules, such as phone numbers, file paths (for Linux or Windows), strong password criteria, RGB color codes, base64-encoded strings, or other custom constraints. These rules, expressed as flexible, non-strict guidelines in the field descriptions, go beyond typical JSON Schema specifications. The process first checks the JSON syntax and compliance with the schema, then validates field values based on their unique instructions. We insert \texttt{const} or \texttt{pattern} in the schema for validating those fields. If all checks pass, the response is considered correct.
% This task is built by modifying specific fields in the original schema to obey specialized rules. 
% These fields may include phone numbers, file paths (for Linux or Windows systems), fields that must satisfy strong password criteria, RGB color codes, base64-encoded strings, or just standard schema constraints. 
% Those rules are more flexible, non-strict guidelines expressed in natural language within the field's description. 
% The core target of this sub-task is to test LLM's ability to generate content that satisfies these diverse and complex instructions within the schema, which are commonly used in schema-constrained generation scenarios and serve as an important means of enforcing constraints that go beyond the standard capabilities of the JSON Schema specification. 
% The validation process first involves checking the JSON syntax and verifying compliance with the JSON Schema, similar to \textbf{Complex Schema}. Subsequently, the values of fields containing special instructions are validated according to their respective rules. If all checks pass, the LLM's response is considered correct.
%In many real-world applications, the basic constraints provided by JSON schemas are often insufficient to capture the full range of requirements. A significant number of custom constraints are specified in the schema's description fields, typically expressed in natural language. These constraints go beyond the formal structural elements of the schema and define more complex requirements.
%By testing LLM’s ability to generate content that satisfies these diverse and complex instructions within the schema, this sub-task serves as a robust measure of LLM’s capacity to interpret and follow natural language directives embedded within schema descriptions. The types of instructions and the criteria for correctness are detailed in the Appendix.

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.95\textwidth]{figs/train.pdf}
%     \caption{Our training pipelines. Top: our SFT training data pipeline. Following our data cleaning process, we utilized JSON Schema Faker\footnotemark to generate random, syntactically valid JSON objects. These objects, however, were populated with arbitrary values and lacked semantic information. To address this issue, we employed LLM annotators (GPT-4o-mini or fine-tuned LLaMA) to modify each field in the generated objects, thereby constructing semantically meaningful JSON objects for use as training data. Bottom: Reinforcement Training pipeline.}
%     \label{fig:pipelines}
% \end{figure*}

\textbf{Escape Translation.} 
This sub-task tests the LLM’s ability to properly handle and escape special characters in strings. The LLM is given a string with special characters that must be escaped correctly and then inserted into a randomly selected field within a nested schema. The evaluation focuses on whether the LLM generates a valid JSON string, as improper escaping can break its validity. It also verifies that the special string is correctly inserted into the designated field. This task highlights the challenge of managing escape sequences in JSON, where specific characters (e.g.,\texttt{\textbackslash "}, \texttt{\textbackslash\textbackslash}, \texttt{\textbackslash n}) must be escaped to maintain correct syntax. Mismanagement of these sequences can result in parsing errors, invalidating the entire output.
% This sub-task is designed to evaluate LLM's ability to correctly handle and translate special characters that require escaping. In this sub-task, LLM is provided with a string containing a series of special characters that need to be properly escaped, and it is asked to insert this string into a randomly selected field within a nested schema. We assess whether LLM generates a valid JSON strings, since improper escape handling is highly likely to break the string's validity, and verify that the special string is correctly inserted into the designated field. This task directly tests LLM's ability to manage escape sequences within the context of complex schema constraints. The challenge addressed by this sub-task arises from the inherent syntax rules of JSON itself. Specifically, JSON strings require the use of double quotes to enclose keys and values, and certain special characters must be represented through escape sequences (e.g., \texttt{\textbackslash "}
% , \texttt{\textbackslash\textbackslash}
% , \texttt{\textbackslash n}
% ). Mismanagement of these escape sequences can lead to parsing errors, rendering the entire output invalid.
%Numerous studies (e.g., [citation]) have highlighted that one of the primary causes for failure in generating valid structured outputs is the improper handling of control characters, especially escape sequences. In particular, failure to correctly generate escape characters often results in parsing errors that render the entire output invalid. To address this challenge, the \textbf{Escape Translation} 

%\textbf{Limitation.} To investigate whether a model truly understands the constraints defined by a JSON schema, and to reflect the reality that schema editors often do not strictly adhere to the schema specification in practice, we designed the \textbf{Limitation} sub-task. In this task, we transform the standard schema constraints of certain fields into more flexible, non-strict guidelines expressed in natural language within the schema's description. These non-rigorous descriptions mimic the kind of imprecise, human-written instructions that are commonly found in real-world schema editing. The model is then asked to generate an output based on these modified, less rigid constraints. We validate the generated string using the original schema, ensuring that the model follows the natural language guidelines provided in the description, despite the absence of strict structural requirements. This sub-task tests the model’s ability to adapt to more flexible, human-readable constraints while still producing valid outputs in accordance with the original schema.

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{figs/error_statics.pdf}
\caption{Statics of failure case of four models. We calculate it on the subset of the \ourbench. All models except GPT-4o still exhibit a relatively high JSON parsing error, indicating their lack of robustness in JSON generation.}
\label{fig:error_statics}
\end{figure*}

\subsection{Schema-constrained Reasoning}
In addition to simply generating valid JSON strings that conform to schema constraints, real-world applications often require LLMs to perform specific tasks. 
We conduct the schema-constrained reasoning test for two main reasons. 
Firstly, generating answers in JSON may hurt the models' performance~\cite{tam2024let}.
An ideal model should deliver the same performance while it generates in JSON.
Second, by checking the correctness of the answer, we can assess the quality of the generated JSON, surpassing the trivial schema checkings.
% In such cases, the schema constraints serve as structural guidelines, but they do not directly assess the model's problem-solving abilities. 
Thus we adapted several common reasoning-focused datasets into schema-constrained reasoning tasks, including GSM8K~\citep{cobbe2021training}, MATH-500~\citep{hendrycksmath2021}, MMLU~\citep{hendryckstest2021}, and ARC-Challenge~\citep{allenai:arc}. We convert them to test the model's reasoning capabilities while adhering to schema rules.
A detailed description of the reasoning schemas can be found in~\Cref{apdx:schema_constrained_reasoning}.

% We incorporated two types of reasoning schema constraints. The first involves simple schemas that require the model to generate an output consisting of an "answer" field. The second approach extends this by utilizing Chain of Thought (CoT) prompting, where the model generates intermediate reasoning steps before arriving at the final answer. Both types of reasoning tasks are evaluated based on two main criteria: first, whether the generated output is valid according to the schema; and second, whether final answers are correct. These evaluations serve as key metrics for our assessment of model performance in reasoning tasks under schema constraints. 

% \subsection{Flexibility}
% While the primary output format for all tasks in \ourbench\space is JSON, we also allow for the use of custom JSON formats and other commonly used syntaxes, such as YAML, TOML, and XML to further test the model’s flexibility. 
% This feature enables models to demonstrate adaptability by handling non-standard or customized schema structures, further evaluating their ability to generalize across various data formats.
% More details can be found in the Appendix.

% \subsection{Data Statistics}
% ~\ourbench contains a total of $40,706$ diverse schemas, with an average character length of $35,754$. The average character length of the descriptions within these schemas is $25,152$, and the average nesting depth of the schemas is $16.7$. These metrics highlight the complexity and richness of~\ourbench. Detailed statistics can be found in~\Cref{fig:stats}.



\subsection{Failure Mode Analysis}
To assess the limitations of current LLMs in JSON generation, we perform a comprehensive failure mode analysis.
In this evaluation, we test four widely used models on the previously generated task, utilizing greedy decoding. 
The results are presented in~\Cref{fig:error_statics}.
GPT-4o~\citep{openai2023gpt4} stands out to be the best model but still obtained $13\%$ validation error and $8\%$ parser error, which implies that it can fail to generate valid JSON strings occasionally.
During the three open-sourced models we tested, we observed more parser errors compared with GPT-4o, indicating that these models tend to produce unresolvable strings.
Qwen-2.5 7B~\citep{qwen2} turns out to be the best among the open-sourced models, with a validation error of $18\%$.
LLaMA-3.2 3B~\citep{2024llama3} and MiniCPM-3 4B~\citep{hu2024minicpm} seem to be struggling to generate a resolvable JSON string, with a relatively high parser error of $23\%$ and $36\%$.

Another common failure for the models we tested is the data format errors, including pattern errors, type errors, and enum errors.
These kinds of errors indicate that the model generates content with unexpected data.
Specifically, all models seem to have the same level of pattern error of $5\%$, which is dangerously close to the patterns we included in our test set.
This indicates that when we use a regex pattern in the JSON schema, these models could easily fail to follow it.
% In short, mastering the JSON schema remains a challenge for current language models.