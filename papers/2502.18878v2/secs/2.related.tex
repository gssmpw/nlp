\section{Related Work}
% % \subsection{}
% % \subsection{Format Followings with Language Models}
% The advancement of LLMs has significantly broadened their applications across various domains, including coding, writing, and automation. For many applications, LLMs are expected to generate content in a predefined format for automated post-processing. Recent works~\citep{zhou2023instruction,chen2024benchmarking,xia2024fofobenchmarkevaluatellms, wang2025verifiableformatcontrollarge} have introduced benchmarks for evaluating and proposed ways to improve LLMs' ability to generate structured data in formats like JSON, YAML, and Markdown, making them more reliable for tasks requiring strict format adherence.
% In this work, we push the schema complexity to an extreme and test the robustness of format adherence for LLMs.

% % \subsection{Constrained Generation with Large Language Models}
% % Recent developments have placed greater emphasis on decoding methods. For instance, Outlines~\citep{willard2023efficient} guides the model's next token prediction by utilizing a pre-computed set of valid tokens to produce decoded results. SGLang~\citep{zheng2024sglangefficientexecutionstructured} introduces compressed finite state machines and jump-forward decoding techniques to enhance constraint decoding speed. Additionally, XGrammar~\citep{dong2024xgrammarflexibleefficientstructured} implements an adaptive token mask cache, further improving the efficiency of context-free grammar decoding.
% However, someone~\citep{tam2024let,he2024doespromptformattingimpact} argues that this could hurt the generation quality of language models.

% % Building upon the ability to follow formats, a further challenge is ensuring that LLMs generate content that is not only formatted correctly but also constrained to meet specific requirements. For many applications, LLMs are expected to generate content in a predefined format, particularly JSON schema. Some of the latest LLMs can directly create valid JSON strings based on a given JSON schema. This approach does not require modifications to the existing decoding pipelines, but it does not guarantee that the generated strings will be valid.


% % \subsection{Tool Calling with Large Language Models}
% % \textbf{ TODO: ADDING citations}
% The exploration of external tools~\citep{schick2023toolformer,qin2023tool,qin2023toolllm,qian2023toolink} to enhance the capabilities of language models is a productive and valuable field of study. Among the various strategies for guiding models in the use of these tools, python code format and JSON schema prompts are widely chosen.

% The Python code format is used by many open-source language models, like LLaMA~\citep{touvron2023llama,2024llama3}, GLM~\citep{glm2024chatglm}.
% This format offers the flexibility for models to generate code for invoking tools, allowing for creativity in applications. 
% However, its lack of strict definitions regarding data types can sometimes lead to ambiguity. 
% On the other hand, the JSON schema prompt presents a more structured method, enabling models to call tools with clear JSON formatting.
% It was first introduced by OpenAI~\citep{openaifunctioncalling} and became very popular with other open-sourced models.
% Its clarity helps specify parameter data types more precisely and introduces advanced abilities, such as switching between parameters based on varying conditions.
% By focusing on mastering JSON schema, we can also improve the model's effectiveness in utilizing these tools in JSON format, paving the way for enhanced performance and more sophisticated applications.


The advancement of large language models (LLMs) has significantly expanded their applications across domains such as coding~\citep{nam2024using}, writing~\citep{pal2024ai}, and automation~\citep{zhu2023autogen}. A key aspect of these tasks is generating content in predefined formats, with JSON being one of the most widely used formats for structured data exchange, configuration, and API interaction.


One approach for structured JSON generation involves direct prompting with a JSON schema~\citep{pokrass2023structured}, where the model is asked to generate valid JSON. While effective for models with native JSON support, those without it often struggle to capture complex schema relationships, resulting in broken or incomplete JSON.
To address these limitations, constrained generation methods have been proposed. For example, Outlines~\citep{willard2023efficient} restrict the model’s predictions to a set of valid tokens, improving schema adherence. Techniques like SGLang~\citep{zheng2024sglangefficientexecutionstructured} and XGrammar~\citep{dong2024xgrammarflexibleefficientstructured} further enhance this by improving decoding efficiency. However, these methods can degrade output quality, particularly with complex schemas~\citep{tam2024let, he2024doespromptformattingimpact}.
Additionally, tool call re-parsing~\citep{schick2023toolformer, qin2023tool, qin2023toolllm, qian2023toolink} can help generate valid JSON by converting tool outputs, but this often requires significant post-processing and struggles to align with standard schemas, leading to inconsistencies.

While there are benchmarks~\citep{zhou2023instruction, chen2024benchmarking, xia2024fofobenchmarkevaluatellms, wang2025verifiableformatcontrollarge,geng2025jsonschemabenchrigorousbenchmarkstructured} for evaluating structured generation, they typically focus on simpler schemas and lack a detailed analysis of how LLMs perform with complex JSON structures. This work aims to fill this gap by rigorously testing LLMs’ ability to adhere to complex, nuanced JSON schemas.