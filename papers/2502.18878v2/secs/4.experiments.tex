
\section{Experiments}
In this section, we first analyze the detailed performance of the JSON schema following the capabilities of different models on~\ourbench.
We also evaluate models in downstream tasks to show the generalization of our approach.
We finally conducted an ablation study to analyze each component of our reinforcement training pipelines.


\subsection{Schema-Related Capabilities Analysis}
\paragraph{Settings.}There are two main categories of testing in~\ourbench: schema-only generation and schema-constrained reasoning.
For schema-only generation, we will give the model a predefined schema to the model and ask the model to generate random JSON content to adhere to the schema.
Once they generate the content, we parse it and validate it with jsonschema\footnote{\url{https://github.com/python-jsonschema/jsonschema}} library.
We use greedy decoding during the evaluation, and the prompts can be found in~\Cref{apdx:schema_bench_prompts}.
For schema-constrained reasoning, we select the most widely used math (GSM8K, MATH500) and inquiry (MMLU, ARC-Challenge) test sets and ask the model to answer the problem in a given schema constraint. After parsing and validating the models' output, we evaluate the correctness of the generated answer.


\paragraph{Collected JSON}\label{sec:collected_json}
We selected several widely-used datasets to supplement our training data, which includes the following distribution: UltraChat~\citep{ding2023enhancing} (6k), UltraInteract~\citep{yuan2024advancingllmreasoninggeneralists} (6k), xLAM~\citep{liu2024apigenautomatedpipelinegenerating} (20k), Glaive\footnote{\url{https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2/}} (20k) and ToolACE~\citep{liu2024toolacewinningpointsllm} (10k).
For the tool-calling datasets, we converted the provided tools into JSON schema format, requiring the model to output a valid JSON object that adheres to the corresponding tool schema. Details of the conversion process and prompts can be found in~\Cref{apdx:tool_conversion}.


\paragraph{Results.}
Here, we present the performance of models in~\Cref{tab:schemabench_results}.
For complex schema adherence, GPT-4o performs well, achieving $84.47\%$, demonstrating strong JSON schema compliance. However, the best model for the escape translation test is still GPT-4o, though it only scores $37.14\%$, revealing the difficulty in handling complex content generation.
For the open-sourced models, the Qwen-2.5 7B stands out to be the best, reaching up to $72.42\%$ in complex schema tests.

After fine-tuning on the \ourbench, models show significant improvements in schema-only generation tasks. Notably, the LLaMA-3.2 3B model obtained a remarkable boost, increasing from $28.51\%$ to $72.50\%$ after SRL, outperforming both the SFT version and all other models. The LLaMA-3.1 8B model also improved, with SFT increasing performance from $36.45\%$ to $60.59\%$, rivaling GPT-4o. 
Fine-tuning LLaMA models without Collected JSON, however, led to performance drops, which means models can hardly generalize their schema-following ability to schema-constrained reasoning tasks.
In contrast, we surprisingly find that the model's performance could generalize better during SRL.

% For the adherence to complex schemas, the GPT-4o shows a promising score of $84.47\%$, which demonstrates its robust JSON schema following capabilities.
% Among the tested models, the best model for the escape translation test is GPT-4o, which only scores $37.14\%$.
% The results indicate that these models could easily fail when generating complex content.
% After training with \ourbench, models demonstrated significant improvements in schema-only generation tasks. Specifically, the LLaMA-3.2 3B model showed a remarkable increase in performance after SRL, jumping from 28.51\% to 70.92\%, outperforming the SFT version and all other models. As for the LLaMA-3.1 8B model, it exhibited substantial gains after SFT, improving from 36.45\% to 60.59\%, achieving performance levels comparable to GPT-4o. Also, we fine-tuned LLaMA models without Collected JSON, which caused performance downgrades.
% When it comes to schema-constrained reasoning, most models could already achieve good performance.


% Additionally, the baseline data did not lead to any improvements in Schema-only Generation, thus validating the effectiveness of our training data. In terms of Schema-constrained Reasoning, when used in conjunction with the baseline data, our training data ensured that model performance remained largely consistent with the original model, indicating that our data enables the model to maintain its performance when generating structured JSON outputs.

% \paragraph{Error Statics.}
% We analyze and summarize the errors in~\Cref{}.
\begin{table*}[htb]
\centering
\small
\begin{widetabular}{\textwidth}{lcccccc|c}
\toprule
& \multicolumn{7}{c}{\textbf{BFCL-Live}} 
% & \textbf{IoA*} 
\\
\midrule
\textbf{Model} & \textbf{Simple} & \textbf{Multiple} & \textbf{Parallel} & \textbf{Multiple Parallel} & \textbf{Irrelevance} & \textbf{Relevance} & \textbf{Overall} 
% & \textbf{Pass Rate} 
\\ 
\midrule
GPT-4o Tool Callings & 36.43 & 37.22 & 18.75 & 41.67 & 94.40 & 29.27 & 59.13 \\
% GPT-4o-mini Tool Callings & 70.93 & 66.54 & 87.50 & 62.50 & 73.37 & 75.61 & 69.97 \\
\midrule
Qwen-2.5 7B & 69.77 & 75.41 & 0.00 & 0.00 & 48.23 & 95.12 & 63.22 \\
Qwen-2.5 7B Tool Callings & 57.36 & 57.67 & 12.50 & 33.33 & 45.26 & 82.93 & 52.69 \\
% MiniCPM-3 4B & 1.55 & 0.87 & 0.00 & 0.00 & 63.09 & 29.27 & 25.63 & 111 \\
\midrule
LLaMA-3.1 8B & 0.39 & 0.00 & 0.00 & 0.00 & 60.11 & 36.59 & 24.08 \\
LLaMA-3.1 8B Tool Callings & 65.12 & 63.35 & 50.00 & 50.00& 37.26 & 80.49 & 53.62  \\
% \textit{- F.T. on collected JSON} & 72.87 & 76.47 & 56.25 & 66.67 & 55.54 & 95.12 & 68.01 & 111\\
LLaMA-3.1 8B SFT & 72.09 & 68.76 & 50.00 & 66.67 & 25.49 & 97.56 & 52.69 \\
LLaMA-3.1 8B SRL & 72.09 & 73.10 & 75.00 & 50.00 & 65.71 & 85.37 & 70.10 \\
\midrule
LLaMA-3.2 3B & 4.26 & 13.11 & 0.00 & 0.00 & 73.26 & 39.02 & 35.72 \\
LLaMA-3.2 3B Tool Callings & 57.36 & 57.67 & 12.50 & 33.33 & 45.26 & 82.93 & 52.69 \\
% \textit{- F.T. on collected JSON} & 74.42 & 73.58 & 50.00 & 58.33 & 57.49 & 97.56 & 67.53 & 111\\
LLaMA-3.2 3B SFT & 74.03 & 74.64 & 68.75 & 58.33 & 47.20 & 97.56 & 64.10 \\	
LLaMA-3.2 3B SRL & 65.50 & 64.22 & 50.00 & 29.17 & 45.03 & 95.12 & 57.00 \\
\bottomrule
\end{widetabular}
\caption{Performance comparison of various models in the downstream JSON generation task. We select the live part of the BFCL to make sure the score is valid. 
The tool calling lines stand for the performance in the official tool calling formats.
The fine-tuned model and the model enhanced with reinforcement training all show performance improvements. The overall score is calculated on the weighted average score of all live tests.
% We compare two variances of the LLaMA model. The fine-tuned models demonstrate their robustness in calling tools with JSON format, even surpassing the original tool calling format.
}\label{tab:downstream_results}
\end{table*}
\subsection{Downstream Tasks Analysis}
Here, we use BFCL~\citep{berkeley-function-calling-leaderboard} to measure models' performance on downstream JSON generation tasks. We modified its tasks by using JSON schema to constrain the models' output.
The detailed prompt we use can be found in~\Cref{apdx:tool_conversion}.
% Additionally, we only utilized the BFCL-v2 live data for BFCL to better reflect the tool-calling scenarios in real-world applications.

\paragraph{Results.}
The performance of models on downstream tasks is summarized in \Cref{tab:downstream_results}.
For BFCL-Live, LLaMA-3.1 8B and LLaMA-3.2 3B perform poorly in most categories using tools in JSON, with some categories scoring $0.00\%$. This is due to their inability to handle complex tool-calling schemas. However, after fine-tuning, both models show significant improvement, adapting to schema constraints and achieving better performance.
For the Irrelevance and Relevance metrics, the original LLaMA models struggle with generating valid tool calls, leading to high Irrelevance and low Relevance scores. After fine-tuning, LLaMA-3.2 3B achieves $97.56\%$ Relevance and $47.20\%$ Irrelevance, demonstrating improved tool call generation and schema adherence.
The LLaMA-3.2 3B SRL demonstrates its superiority once again, achieving an impressive score of $57.00\%$, even in the absence of a ground truth answer.

% Regarding the Irrelevance and Relevance metrics, they evaluate the model's ability to distinguish between cases where tool calls are unnecessary and where they are required. In the case of schema-constrained tool calling using the original LLaMA model, the model's relatively poor ability to adhere to schemas results in an inability to correctly generate valid JSON tool calls in most cases. Consequently, the outputs that do not conform to the schema are parsed as responses without tool calls, leading to an inflated Irrelevance score and a reduced Relevance score. After fine-tuning, the LLaMA-3.2 3B achieves a Relevance score of $100.00\%$ and an Irrelevance score of $56.00\%$, demonstrating the model's capability to accurately generate tool calls when required, while maintaining its ability to correctly identify cases where tool calls are unnecessary.

% waiting for RL result


% \paragraph{Error Statics.}
% We analyze and summarize the errors in~\Cref{}.

\subsection{Ablation Study}
In this section, we compare different settings for schema reinforcement training and how it impacts the performance of structured generation.

\begin{figure}
\begin{minipage}[]{0.48\linewidth}
\centering
\includegraphics[width=0.95\linewidth]{figs/training.pdf}
\caption{Reinforcement training accuracy on complex schema subset for LLaMA-3.2 3B. The red line is the fine-tuning baseline.}
\label{fig:rl_training}
\end{minipage}
\noindent
\hfill
\begin{minipage}[]{0.48\linewidth}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Settings} & \textbf{Schema} & \textbf{MATH-500} & \textbf{ARC-C}\\
\midrule
LLaMA-3.2 3B & 28.51 & 35.40 & 79.27  \\ 
\textit{trained w/ ORM} & 31.15 & 39.40 & 78.92 \\
\textit{+ ToS} & 44.89 & 36.60 & 80.38 \\ 
\textit{+ F.G-val} & 35.59 & 35.60 & 79.10 \\ 
\bottomrule
\end{tabular}
\caption{Ablation study results for LLaMA-3.2 3B.
For each line, we train the model by adding a component into the ordinary RL pipelines with an outcome verifier. All results are reported with RL after $10K$ samples.}
\label{tab:ablation}
\end{minipage}
\end{figure}

% \begin{wrapfigure}{l}{0.45\textwidth}

% \end{wrapfigure}

% \begin{wraptable}{r}{0.45\textwidth}
% \centering
% \small
% \end{wraptable}

\paragraph{Settings.} 
Across all settings, we take the same training pipelines as detailed in~\Cref{sec:method}. 
We use the training set of the~\ourbench to train our models, which contain around 37K different schemas.
We evaluate models on the test set of the~\ourbench.
We conducted experiments to find whether adding ToS or the fine-grained schema validator (F.G-val) could impact the performance of the models.
We set a batch size of $32$ and a learning rate of $5e^{-7}$ for all experiments.
We run all experiments with $10K$ sampling times.

\paragraph{Results.}
As~\Cref{fig:rl_training} shows, by providing fine-grained evaluation results, the model shows a consistent improvement across the training process, demonstrating the effectiveness of our training methods.
We also find that the reinforcement training is quite efficient compared with supervised fine-tuning, easily outperforming the baseline when halfway through training.
% When training with the fine-grained evaluator, we observed that the training was improved 


\Cref{tab:ablation} demonstrate the effectiveness of each component for the training.
Compared with the original model, the model training with ORM improves from $28.51\%$ to $31.15\%$ on the~\ourbench, demonstrating the effectiveness of reinforcement training.
Adding ToS into training dramatically improves the performance, reaching up to $44.89\%$ in the complex schema following.
The fine-grained validator shows its superior performance when compared witthe h trivial outcome validator, with a performance up to $35.59\%$ in testing.
% We find that the most basic reinforcement training with ORM can already outperform all baselines, which indicates the superiority of our method.
Besides, we also observed that across all settings, the performance on MATH-500 and ARC-C obtained certain improvements.
We consider this to be a benefit from the escaping training, which reduces the parsing error and brings improvements.
