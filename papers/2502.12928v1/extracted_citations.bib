@inproceedings{DBLP:conf/iclr/LepikhinLXCFHKS21,
  author       = {Dmitry Lepikhin and
                  HyoukJoong Lee and
                  Yuanzhong Xu and
                  Dehao Chen and
                  Orhan Firat and
                  Yanping Huang and
                  Maxim Krikun and
                  Noam Shazeer and
                  Zhifeng Chen},
  title        = {GShard: Scaling Giant Models with Conditional Computation and Automatic
                  Sharding},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=qrwe7XHTmYb},
  timestamp    = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LepikhinLXCFHKS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/BrownMRSKDNSSAA20,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2302-13971,
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  title        = {LLaMA: Open and Efficient Foundation Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.13971},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.13971},
  doi          = {10.48550/ARXIV.2302.13971},
  eprinttype    = {arXiv},
  eprint       = {2302.13971},
  timestamp    = {Mon, 28 Aug 2023 21:26:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-13971.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dai2022knowledge,
  title={Knowledge Neurons in Pretrained Transformers},
  author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8493--8502},
  year={2022}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@inproceedings{geva2021transformer,
  title={Transformer Feed-Forward Layers Are Key-Value Memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5484--5495},
  year={2021}
}

@article{luo2024sparsing,
  title={Sparsing Law: Towards Large Language Models with Greater Activation Sparsity},
  author={Luo, Yuqi and Song, Chenyang and Han, Xu and Chen, Yingfa and Xiao, Chaojun and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2411.02335},
  year={2024}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{yang2024xmoe,
  title={XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection},
  author={Yang, Yuanhang and Qi, Shiyi and Gu, Wenchao and Wang, Chaozheng and Gao, Cuiyun and Xu, Zenglin},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={11664--11674},
  year={2024}
}

@inproceedings{zhang2022moefication,
  title={MoEfication: Transformer Feed-forward Layers are Mixtures of Experts},
  author={Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={877--890},
  year={2022}
}

