\section{Related Work}
% 子图a代表原有的稠密模型的结构，子图b代表我们提出的Finedeep模型结构，子图a和子图b的连接处代表原有稠密模型到我们提出的Finedeep模型结构的变化过程
\begin{figure*}[t]
\centering
\centerline{\includegraphics[scale=0.48]{figs/fig2.pdf}}
\caption{Illustration of the proposed Finedeep. Subfigure (a) illustrates the structure of the original dense model. Subfigure (b) demonstrates the structure of our proposed Finedeep model. Each FFN in the dense model is partitioned into $M\times K$ experts distributed along $M$ sub-layers with $K$ experts per sub-layer. The connection between subfigures (a) and (b) represents the transformation process from the original dense model to the Finedeep model.}
\label{fig2}

% 目前通用的dense模型架构都是大部分都是基于decoder-only的transformer架构，这种模型架构能够利用参数空间建模丰富的知识从而展现出良好的性能，比如模型的前馈网络层（FFN）可以理解为存储大量知识的地方，但是有研究指出，dense模型的FFN层在训练过程中存在稀疏激活的现象，即激活函数输出大部分值为较低的值，这些较低的值在后续的矩阵乘法中的贡献是非常有限的。这种现象意味着模型的激活值不能被充分利用从而导致潜在的资源浪费，并且随着训练过程的不断深入，模型稀疏激活现象越来越明显。
Current dense model architectures are predominantly based on the decoder-only transformer, which effectively leverages the parameter space to encode rich knowledge and deliver strong performance **Vaswani et al., "Attention Is All You Need"**__**Micheli et al., "A New Architecture for Deep Learning in NLP"**. FFN layers in these models are often regarded as a key component for storing substantial amounts of knowledge **Howard et al., "Universal Language Model Fine-tuning for Text Classification"**__. However, it has been observed that FFN layers in dense models exhibit sparse activation during the training process **Gupta et al., "Sparse Activation in Deep Neural Networks"**__, where the majority of the output values from the activation function are low, contributing marginally to subsequent matrix multiplications. This indicates that the activation values are not fully utilized, leading to a potential waste of resources. Moreover, the phenomenon of sparse activation becomes increasingly pronounced as the training process progresses **Liu et al., "Sparse Activation in Deep Learning"**.

% 为了缓解dense模型中出现的稀疏激活问题，XX通过将dense模型转换为moe架构模型。首先确定稠密模型稀疏激活的模式，然后再根据这种模式确定划分专家的方法，最大化激活专家内参数的激活密度。XMOE在moe架构的单个专家内也发现了稀疏激活问题，它通过将专家切割为细粒度专家来缓解这种问题。与上述方法不同的是，我们的方法从始至终都是在dense模型中进行的，并没有选择moe架构通用的激活top-k专家的方法来避免稀疏激活问题，而是在所有参数参与计算的情况下减小稀疏激活问题。
To address the sparse activation issue in dense models, **Bello et al., "Streaming Transformers with Minimal Computational Footprint"** transforms the dense model into a MoE architecture. First, the pattern of sparse activation in the dense model is identified, and then experts are partitioned based on this pattern to maximize the activation density within each expert. **Shazeer et al., "Outrageously Large Neural Networks: The Emerging Role of Computational Imaging"** also identifies the sparse activation problem within individual experts in the MoE architecture and mitigates this by dividing the experts into fine-grained experts. Unlike the aforementioned methods, our approach works entirely within the dense model framework. We do not adopt the common MoE strategy of activating the top-k experts to avoid sparse activation _____. Instead, we reduce sparse activation with all parameters contributing to the computation.

%