% This is a modified version of Springer's LNCS template suitable for anonymized MICCAI 2025 main conference submissions. 
% Original file: samplepaper.tex, a sample chapter demonstrating the LLNCS macro package for Springer Computer Science proceedings; Version 2.21 of 2022/01/12

\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{mwe} % to get dummy images
\usepackage{multirow}
\usepackage{multicol}
\usepackage{pifont}
\usepackage{float}
\usepackage{bm}
\usepackage{subcaption}


\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\bb}[1]{\bm{\mathrm{#1}}}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encodings may result in incorrect characters.
%
\usepackage{graphicx,verbatim}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{T1-PILOT: Optimized Trajectories for T1 Mapping Acceleration}
%
  %% Removed for anonymized MICCAI 2025 submission

\author{Tamir Shor \inst{1} \and
Moti Freiman \inst{1} \and
Chaim Baskin \inst{2} \and Alex Bronstein \inst{1,3}}
%
\authorrunning{T. Shor et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Technion -- Israel Institute of Technology, Haifa, Israel \\
\email{tamir.shor@campus.technion.ac.il};
\email{moti.freiman@technion.ac.il}\and
Ben-Gurion University of the Negev, Be'er Sheva, Israel\\
\email{chaimbaskin@bgu.ac.il}
\and
Institute of Science and Technology, Austria\\
\email{alexander.bronstein@ist.ac.at}}

    
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Cardiac T1 mapping provides critical quantitative insights into myocardial tissue composition, enabling the assessment of pathologies such as fibrosis, inflammation, and edema. However, the inherently dynamic nature of the heart imposes strict limits on acquisition times, making high-resolution T1 mapping a persistent challenge. Compressed sensing (CS) approaches have reduced scan durations by undersampling k-space and reconstructing images from partial data, and recent studies show that jointly optimizing the undersampling patterns with the reconstruction network can substantially improve performance. Still, most current T1 mapping pipelines rely on static, hand-crafted masks that do not exploit the full acceleration and accuracy potential. In this work, we introduce T1-PILOT: an end-to-end method that explicitly incorporates the T1 signal relaxation model into the sampling–reconstruction framework to guide the learning of non-Cartesian trajectories, cross-frame alignment, and T1 decay estimation. Through extensive experiments on the CMRxRecon dataset, T1-PILOT significantly outperforms several baseline strategies (including learned single-mask and fixed radial or golden-angle sampling schemes), achieving higher T1 map fidelity at greater acceleration factors. In particular, we observe consistent gains in PSNR and VIF relative to existing methods, along with marked improvements in delineating finer myocardial structures. Our results highlight that optimizing sampling trajectories in tandem with the physical relaxation model leads to both enhanced quantitative accuracy and reduced acquisition times. Code for reproducing all results will be made publicly available upon publication.

\keywords{Cardiac T1 Mapping \and Trajectory Optimization and Reconstruction \and Physics-Informed Deep-Learning}
% Authors must provide keywords and are not allowed to remove this Keyword section.

\end{abstract}
\section{Introduction}
Cardiac MRI T1 mapping quantifies the T1 relaxation time of cardiac tissues, which provides critical insights into tissue composition and plays a key role in diagnosing various pathologies, such as tumors, fibrosis, and diffuse myocardial inflammation \cite{burt2014myocardial}. One of the most prevalent T1 mapping methods is Modified Look-Locker Inversion Recovery (MOLLI) \cite{messroghli2004modified}, where multiple images are acquired at different inversion times following an inversion pulse, and the signal recovery is fitted to an exponential model to estimate the T1 decay map. Despite its success, MOLLI’s dependence on multiple breath-holds and heart rate variations makes it highly sensitive to motion artifacts, limiting the feasibility of long scan times often required for high-resolution MRI \cite{xue2012motion,hanania2024mbss}.

Compressed sensing (CS) \cite{lustig2007sparse} has become a powerful approach for addressing these long scan times and motion sensitivity in MOLLI-based T1 mapping. By leveraging the sparsity of MR images in the frequency domain, CS enables precise image reconstruction from highly undersampled k-space data, significantly accelerating the acquisition while preserving essential clinical information. This results in reduced breath-hold durations and improved temporal resolution, alleviating issues related to prolonged scan times and motion artifacts \cite{stainsby2014accelerated,lyu2025state,paajanen2023fast}. However, most existing methods depend on fixed, hand-selected \cite{paajanen2023fast,lyu2025state} acquisition schemes. Some research has explored optimizing undersampling strategies alongside downstream T1 map estimation, but these efforts are predominantly confined to using a single, shared optimization scheme across frames \cite{chaithya2022hybrid,li2012fast}.
For instance, \cite{zhang2015accelerating} leverages low-rank constraints and parallel imaging to optimize pulse-sequence parameters, while \cite{stainsby2014accelerated} employs PCA-guided compressed-sensing reconstructions to shorten acquisition times. Deep-learning approaches have also been explored for acceleration; \cite{hanania2024mbss} tackles retrospective motion-artifact correction, and \cite{guo2022accelerated} proposes an MLP that estimates T1 decay from as few as four T1-weighted images. Nevertheless, these methods either do not fully integrate compressed-sensing \cite{lustig2007sparse,weiss2019pilot} or rely on fixed undersampling masks \cite{lyu2025state,chaithya2022hybrid,tran2015model} -- a limitation also noted in \cite{paajanen2023fast}.


Recent advancements suggest that learned non-Cartesian per-frame k-space undersampling masks can substantially improve acceleration, especially for spatially or temporally sequential MRI data \cite{shor2023multi,yiasemis2024end,shor2024team}. Prior work shows that such per-frame optimization generally outperforms pipelines using single-frame or Cartesian-based acquisition \cite{weiss2019pilot,wang2022b,shor2023multi}. However, even in approaches that do adopt a learned sampling trajectory for T1 decay estimation \cite{zhang2024mclaro}, the actual decay model itself is often not integrated as a driving constraint in the acquisition–reconstruction optimization, potentially leading to sub-optimal results.

To address these gaps, we introduce T1-PILOT, a novel pipeline for the self-supervised joint optimization of T1 mapping estimation and physically feasible non-Cartesian per-frame k-space undersampling trajectories. In addition to introducing per-frame non-Cartesian trajectory learning for T1 mapping, our key innovation lies in explicitly integrating the T1 signal relaxation model as a constraint in optimization, guiding both the learned acquisition and the subsequent image reconstruction. We demonstrate that this approach enables accurate T1 mapping from highly undersampled data, outperforming both constant and learned sampling schemes that do not fully exploit the relaxation model.

We conducted extensive experiments on the CMRxRecon dataset, comparing T1-PILOT against multiple baselines, including fixed radial and golden-angle undersampling schemes as well as single learned trajectories. Our results show superior performance in terms of PSNR and VIF, highlighting that explicitly modeling the T1 relaxation signal in the undersampling–reconstruction pipeline yields both enhanced quantitative accuracy and significant reductions in acquisition times.

\section{Method}
\subsection{Problem Definition}
Given a sequence of $N$ T1-weighted images $\mathcal{X} = \{x_{t_i}\}_{i=1}^{N} \in \mathbb{R}^{N\times H \times W}$ sampled at respective inversion times $T=\{t_i\}_{i=1}^{N} \in \mathbb{R}^N$, the T1 mapping objective is to fit the exponential T1-decay curve of the sequence, parameterized by $A,B,T_1^* \in \mathbb{R^{H \times W}}$, according to the decay model:
\begin{equation}
    \label{decay model}
    \tilde{x}_{t_i} = A-B\cdot e^{-\frac{t_i}{T_1^*}}
\end{equation}
In this work we study MOLLI sequences, where the underlying T1 map can be thereafter received by applying a linear correction $T_1 = T_1^*\cdot(\frac{B}{A}-1)$\cite{slavin2014use}.\\
While parameterizing and directly optimizing the decay parameters $A,B,T_1^*$ is possible, we follow the currently more-common approach (e.g. \cite{guo2022accelerated,zhang2024mclaro}) of a neural-network $\mathcal{M}: \mathbb{R}^{N\times H \times W} \mapsto \mathbb{R}^{3\times H \times W}$ outputting the decay parameters based on T1-weighted input-sequence conditioning. The rational behind this choice is that, as shown in \cref{results}, such learned neural mappings allow implicit regularization of the exponential regression task, and can be trained across multiple samples to allow efficient learning of data priors, that may alleviate (or completely nullify) the need for per-sample decay-parameter optimization.

To accelerate T1 map estimation, we opt to optimize a set of $N$ k-space undersampling masks $K \in \mathbb{R}^{N \times n \times m}$, where $n$ is the number of RF excitation pulses (namely, RF \textit{shots}) and $m$ is the number of k-space sampling points per shot. 
Importantly, unlike previous works where acquisition trajectory learning is guided solely by a per-frame reconstruction of the T1-weighted sequence \cite{zhang2024mclaro,wang2022b}, to best-adapt our acquisition set $K$ for our task at hand, in this work our we design our learning objective so that the optimization of $K$ is explicitly informed by the downstream fit to the underlying physical decay model (\cref{decay model}). Our optimization objective is formulated as: 
\begin{equation}
    \label{objective}
    \min_{\theta,\psi} \sum_{t_i \in T}\|x_{t_i} - (A_{\theta,\psi}-B_{\theta,\psi}\cdot e^{-\frac{t_i}{{T_1^*}_{\theta,\psi}}})\|_2
\end{equation}
Where $\mathcal{F_{K_\psi}}(x_{t_i})$ is the downsampling operator of $x_{t_i}$ by the acquisition-set $K$ parameterized by $\psi$, and $(A_{\theta,\psi},B_{\theta,\psi},{T_1^*}_{\theta,\psi}) = \mathcal{M_\theta}(\mathcal{F}_{K_{\psi}}(\mathcal{X}))$. 



\subsection{Modeling and Optimization}
\label{optimization}


\begin{figure}

\centering
 % Caption and label go in the first argument and the figure contents
 % go in the second argument


  
  \includegraphics[width=1\linewidth]{t1pilot_pipeline.png}
  
  {\caption{\textbf{T1-PILOT Pipeline} - blue arrows denote the forward process, and orange arrows denote backpropagation.}
  \label{fig:pipeline}
}  
\end{figure}

A key consideration in developing a feasible learned k-space acquisition set is that, unlike the decay model parameters governed by $\theta$, $K_\psi$ must be optimized as an acquisition set shared across the entire dataset. This is because, while $\mathcal{M}_\theta$ can be finetuned per input sample $\mathcal{X}$ retrospectively to the scan, the full k-space data would not available at inference time. The nature of the task at hand may render the joint optimization of $\mathcal{M}_\theta$ and $K_\psi$ suboptimal, since the decay objective (\cref{objective}) is highly-related to sample-specific features, and gradients backpropagated from it supply overly noisy feedback to the sampling parameters $\psi$. This challenge is further intensified in non-Cartesian trajectory learning, as the training process is inherently noisier due to the larger number of learned acquisition parameters. \\
Relying on prior work, where optimization scheduling has successfully addressed these challenges within the context of MRI reconstruction \cite{shor2023multi}, we propose a 3-stage optimization schedule, gradually shifting the optimization focus from reconstruction to decay estimation. In the following sections we layout our proposed optimization approach, also illustrated in \cref{fig:pipeline}.
\subsubsection{Reconstruction-Guided Pre-Training} - 
To obtain a well-initialized set of acquisition trajectories, we first train a reconstruction model $\mathcal{R}_\zeta$ dedicated solely to trajectory optimization and reconstruction. Namely, at this stage our learning objective is formulated as:
\begin{equation}
    \label{rec_objective}
    \min_{\zeta,\psi} \|\mathcal{X} - \mathcal{R}_\zeta(\mathcal{F_{K_\psi}}(\mathcal{X})\|
\end{equation}
For $\mathcal{R}_\zeta$ and $K_\psi$ we adopt TEAM-PILOT \cite{shor2024team}, originally proposed for general-purpose joint optimization of k-space acquisition and reconstruction. We choose this model due to its excellent performance on multi-frame MRI reconstruction, incorporation of kinematic scanner constraints and non-Cartesian acquisition parametrization. \\
TEAM-PILOT parametrizes non-Cartesian sampling trajectories by optimizing a set of 2D control points ($\psi\in\mathbb{R}^{n \times n\times m \times2}$ in \cref{objective} notations), through which the resulting k-space acquisition trajectory is interpolated via a differentiable SPLINE. Machine-related kinematic constraints are imposed according to \cite{chauffert2016projection}. To efficiently process 3D MRI data, the reconstruction model is composed of blocks of 3D-convolutional layers interleaved with W-MSA \cite{liu2022video} attention layers. 
\subsubsection{Decay Optimization} - 
After pre-training $\mathcal{R}_\zeta$, we initialize another TEAM-PILOT block $\mathcal{D}_\xi$, acting as the decay-parameter estimation model. To condition our decay model over different sets of inversion times $T$ (varying across different sequences), we extend $D_\xi$ with a small MLP to learn temporal embeddings added to each reconstructed input sample $\mathcal{R}(\mathcal{F}_{K_\psi}(x_{t_i}))$. We optimize both the decay and reconstruction model according to \cref{objective}, where $\mathcal{M}_{\theta=\{\zeta,\xi\}} (\mathcal{X}) = \mathcal{D}_\xi(\mathcal{R}_\zeta(\mathcal{F_{K_\psi}}(\mathcal{X})),T)$.

\subsubsection{Per-Sample Decay Refinement} - 
In both reconstruction and decay stages, all parameters are optimized across sequences from the entire dataset. This allows for the optimization of an acquisition set that generalizes across all samples, as well as for the leveraging of global data-shared priors that can help expedite convergence. As we demonstrate in \cref{results}, these two optimization stages suffice for accurate and substantially accelerated T1-Mapping. Nonetheless, since the decay model is sample-specific, we also propose an optional third stage of decay refinement. At this stage, the acquisition set $K$ remains frozen, whilst the decay-and reconstruction parameters $\theta=\{\zeta,\xi\}$ are finetuned (according to \cref{objective}) for a specific input sequence of $N$ T1-weighted images. 




\section{Results}
\label{results}

\subsection{Data}
In all experiments we make use of the CMRxRecon dataset \cite{wang2023cmrxrecon} dataset, containing raw, multi-slice and multi-contrast fully-sampled k-space collected from 300 patients. For each patient and slice, 9 T1-weighted images have been acquired with a 3T MRI scanner. Full acquisition protocols are in \cite{wangrecommendation}. The pre-trained TEAM-PILOT models mentioned in \cref{exps_setup} have been trained on the augmented version of the OCMR dataset \cite{chen2020ocmr} published in \cite{shor2023multi}.

\subsection{Experimental Setup}
\label{exps_setup}
Our work primarily aims to demonstrate that combining non-Cartesian k-space acquisition learning with a decay objective-guided optimization process enables significantly improved T1 mapping acceleration. This approach outperforms those in previous studies, which either do not learn the k-space acquisition scheme (e.g. those surveyed in \cite{lyu2025state}), use a single acquisition scheme (learned or fixed) shared across all frames (e.g., \cite{paajanen2023fast}), or apply multi-frame learning without directly integrating the decay objective into the acquisition optimization (e.g., \cite{zhang2024mclaro}). To achieve this, we compare our method with 4 representative baseline optimization approaches -
\begin{enumerate}
    \item \textit{Radial} - The k-space acquisition set is a single radial acquisition mask shared across all frames.
    \item \textit{GAR} - As a fixed non-Cartesian per-frame acquisition scheme we use the Golden-Angle Ratio (GAR) 3D acquisition scheme proposed by \cite{zhou2017golden}. This method proposes an array angularly-shifted per-frame radial acquisition masks.
    \item \textit{Single} - We learn a single, non-Cartesian acquisition mask shared across all frames, similarly to \cite{weiss2019pilot,wang2022b}.
    \item \textit{Reconstruction} - We learn per-frame non-Cartesian acquisition masks (similarly to our proposed method), however include only the reconstruction objective throughout the entire optimization process from \cref{optimization}.
\end{enumerate}
For a fair comparison, we train our method and all four baselines using the modeling and optimization schedule detailed in \cref{optimization}. Each baseline is trained with 16,32 and 64 shots (RF excitation pulses) per-frame, where each pulse is represented by 513 k-space sampling points. We resize every image to spatial dimensions $144\times384$, making the acceleration factor in every $n$-shot experiment $\frac{144\cdot 384}{n\cdot513}$. Since ground-truth T1 Maps are not available, to measure mapping performance in each experiment we train a separate decay-estimation model for 150 epochs on the entire fully-sampled data, and treat this model's estimations as ground-truth maps. For this model, we only train $\mathcal{D}_\zeta$ with the decay objective from \cref{objective}, without the added reconstruction logic. \\
Following \cite{shor2024team}, for each experiment we train for 150 epochs of reconstruction pre-training, 200 epochs of decay optimization and 3000 iterations of per-sample finetuning, on  a single NVIDIA-A6000 GPU. To expedite training and attain better reconstruction priors, in the reconstruction stage we initialize $\mathcal{R}$ with a TEAM-PILOT model pre-trained on the larger augmented-OCMR dataset from \cite{shor2023multi}, for another 150 epochs separately for each experiment. For similar reasons, in the decay optimization stage we initialize the decay model $\mathcal{M}$ as the decay-estimation model trained on fully-sampled data. We use the CMRxRecons test-set for testing, and its training set is internally split to training and validation by an 0.8 ratio. 
\subsection{Quantitative Results}
\label{quant}
Table \ref{table:Reconstruction} depicts the quality of optimized decay-model estimation across varying acceleration factors, for our method and elected baselines. Following \cite{shor2024team}, we report mean PSNR and VIF\cite{sheikh2006image} in all experiments. We report performance according to those metrics in two layouts --- First, under sub-header \textit{T1-Map}, quality metrics are measured between T1 maps estimated by each baseline and the maps attained from the model trained on fully-sampled data. Results in this setting reflect the acceleration efficacy of each method by comparing the potential of approximating the performance of a model that utilizes fully-sampled data. This metric also poses a more-direct estimation of the quality of downstream map estimation, in the absence of ground-truth data. 
Second, to estimate our results in a model-independent manner, under \textit{Decay} we also report PSNR and VIF between the original sequence $x_{i_t}$ and decay-model output sequence $\tilde{x_{i_t}}$ (\cref{decay model}). We include results with and without per-sample finetuning (stage 3 in \cref{optimization}) to exhibit asses the performance advantage attained in per-sequence optimization. Since the finetuning stage is performed based on the decay objective, we do not report post-finetuning results for the reconstruction-only baseline. While per-sample reconstruction finetuning for this baseline has been considered, our findings indicate that such sample-specific optimization for reconstruction severely harms decay-estimation performance.


\begin{table}
    \centering
   
    \caption{\textbf{Decay-estimation results comparison}.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c||c|c|c|c||c|c|c|c||c|c|c|c|}
        \hline
        \multirow{3}{*}{Method} & \multirow{3}{*}{Finetuning} & \multicolumn{4}{c||}{16 Shots} & \multicolumn{4}{c||}{32 Shots} & \multicolumn{4}{c|}{64 Shots} \\
        \cline{3-14}
        & & \multicolumn{2}{c|}{Decay} & \multicolumn{2}{c||}{T1-Map} & \multicolumn{2}{c|}{Decay} & \multicolumn{2}{c||}{T1-Map} & \multicolumn{2}{c|}{Decay} & \multicolumn{2}{c|}{T1-Map} \\
        \cline{3-14}
        & & \multicolumn{1}{c|}{PSNR} & \multicolumn{1}{c||}{VIF} & \multicolumn{1}{c|}{PSNR} & \multicolumn{1}{c||}{VIF} & \multicolumn{1}{c|}{PSNR} & \multicolumn{1}{c|}{VIF}& \multicolumn{1}{c|}{PSNR} & \multicolumn{1}{c||}{VIF} & \multicolumn{1}{c|}{PSNR} & \multicolumn{1}{c||}{VIF} & \multicolumn{1}{c|}{PSNR} & \multicolumn{1}{c|}{VIF} \\
        \hline
        
        \multirow{2}{*}{Radial} & \xmark & $29.95$ & $0.512$ & $18.65$ & $0.168$ & $32.28$ & $0.664$ & $22.93$ & $0.284$ & $34.36$ & $0.739$ & $24.54$ & $0.391$ \\
        \cline{2-14}
        & \cmark & $38.33$ & $0.873$ & $21.1$ & $0.543$ & $39.02$ & $0.902$ & $26.98$ & $0.706$ & $39.04$ & $0.903$ & $27.28$ & $0.658$ \\
        \hline
        \multirow{2}{*}{GAR} & \xmark & $33.87$ & $0.711$ & $23.54$ & $0.3$ & $35.47$ & $0.786$ & $23.95$ & $0.421$ & $36.35$ & $0.835$ & $24.43$ & $0.478$ \\
        \cline{2-14}
        & \cmark & $38.84$ & $0.893$ & $25.76$ & $0.627$ & $38.71$ & $0.895$ & $27.62$ & $0.66$ & $38.97$ & $0.9$ & $27.89$ & $0.69$ \\
        \hline
        \multirow{2}{*}{Single} & \xmark & $33.33$ & $0.695$ & $23.98$ & $0.286$ & $34.71$ & $0.758$ & $24.57$ & $0.367$ & $35.76$ & $0.801$ & $25.1$ & $0.458$ \\
        \cline{2-14}
        & \cmark & $38.82$ & $0.895$ & $26.24$ & $0.641$ & $39.03$ & $0.901$ & $27.83$ & $0.684$ & $39.04$ & $0.904$ & $27.83$ & $0.682$ \\
        \hline
        {Recon.} & \xmark & $35.57$ & $0.79$ & $24.8$ & $0.433$ & $36.22$ & $0.81$ & $25.4$ & $0.476$ & $37.03$ & $0.853$ & $25.93$ & $0.513$ \\
        \cline{2-14}
        %& \cmark & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ %& $-$ & $-$ & $-$ & $-$ & $-$ \\
        \hline
        \multirow{2}{*}{Ours} & \xmark & $35.74$ & $0.792$ & $25.38$ & $0.418$ & $36.45$ & $0.819$ & $26.41$ & $0.57$ & $37.19$ & $0.854$ & $26.52$ & $0.581$ \\
        \cline{2-14}
        & \cmark & $\textbf{39.04}$ & $\textbf{0.902}$ & $\textbf{28.62}$ & $\textbf{0.715}$ & $\textbf{39.1}$ & $\textbf{0.905}$ & $\textbf{28.81}$ & $\textbf{0.723}$ & $\textbf{39.22}$ & $\textbf{0.907}$ & $\textbf{28.84}$ & $\textbf{0.726}$ \\
        \hline
    \end{tabular}}
    \label{table:Reconstruction}
\end{table}

Table \ref{table:Reconstruction} exhibits two major trends - first, our method exceeds all baselines in acceleration performance, according to both considered quality metrics and comparison layouts. We observe that the relative advantage of our method over other baselines is attenuated when incorporating the finetuning stage, as per-sample optimization iterations compensate for differences arising from variations in acquisition patterns. Our method proves especially favorable according to the map comparison layout (under \textit{T1-Map} in the table), that is more-directly correlated with the acceleration potential and downstream objective. We attribute this result to our more-potent parametrization of the acquisition set $K$, as well as to the structured incorporation of decay and reconstruction objectives along training. Second, we observe that the two approaches reliant on multi-frame acquisition-learning (namely, our method and the Reconstruction-only baseline) exhibit substantially favorable performance when per-sample finetuning is not applied. We relate this generalizability potential to efficient learning of data-priors during training. 
\subsection{Qualitative Results}
\label{qual}
\begin{figure}[htbp]

\centering
 % Caption and label go in the first argument and the figure contents
 % go in the second argument


  
  \includegraphics[width=0.8\linewidth]{qualitative.png}
  
  {\caption{\textbf{Map Estimations across methods} - \textit{Full} indicates map estimation without undersampling. For each baseline we specify ROI PSNR (left) and VIF (right) compared to the fully-sampled map estimation. Our method's relative advantage over baselines is highlighted in red squares. }
  \label{fig:qual}
}  
\end{figure}


Figure \ref{fig:qual} illustrates T1 maps estimated by our method and by each of the four baselines trained for acceleration factors of 32 shots per-frame, compared to the map derived from fully-sampled data (upper-left). Maps are shown both in their entirety, and with focus on the myocardium. Our method yields sharper and more accurate estimations, surpassing baselines in capturing fine details and subtle variations, particularly in smaller regions of the map (framed in red). We attribute this improvement to the effectiveness of learned, multi-frame non-Cartesian undersampling, which prioritizes high-frequency components essential for preserving such subtle features.
As evident from the figure, baseline sampling patterns with non-learned (GAR, Radial) or single-frame (denoted \textit{Single}, bottom-right) acquisition patterns struggle in identifying such patterns. 


\section{Conclusion}
We presented T1-PILOT, a self-supervised, physically feasible, and accelerated T1 mapping algorithm. By explicitly integrating the T1 signal relaxation model into the undersampling–reconstruction pipeline, our approach learns multi-frame non-Cartesian k-space acquisition while harnessing data-driven priors to achieve both high acceleration and quantitative accuracy. Ablations confirm that incorporating the relaxation model significantly outperforms fixed or single-frame schemes, providing practical insights for designing T1 mapping strategies and broader quantitative MRI acceleration pipelines.


Despite these encouraging results, two limitations remain. First, the multi-stage optimization and increased learnable parameters introduce higher computational demands. Second, the current focus on cardiac T1 mapping leaves open the application of our method to other modalities and cross-modality acceleration, which we plan to explore in future work. We believe T1-PILOT’s framework can guide further research into rapid, model-aware imaging, ultimately helping to streamline quantitative MRI acquisition in clinical and scientific contexts.




%
%
%
% \section{Important Messages to MICCAI Authors}

% This is a modified version of Springer's LNCS template suitable for anonymized MICCAI 2025 main conference submissions. The author section has already been anonymized for you.  You cannot remove the author section to gain extra writing space.  You must not remove the abstract and the keyword sections in your submission.

% To avoid an accidental anonymity breach, you are not required to include the acknowledgments and the Disclosure of Interest sections for the initial submission phase.  If your paper is accepted, you must reinsert these sections to your final paper. 

% To avoid desk rejection of your paper, you are encouraged to read "Avoiding Desk Rejection" by the MICCAI submission platform manager.  A few important rules are summarized below for your reference:  
% \begin{enumerate}
%     \item Do not modify or remove the provided anonymized author section.
%     \item Do not remove the abstract and keyword sections to gain extra writing space. Authors must provide at least one keyword.
%     \item Inline figures (wrapping text around a figure) are not allowed.
%     \item Authors are not allowed to change the default margins, font size, font type, and document style.  If you are using any additional packages, it is the author's responsibility that any additional packages do not inherently change the document's layout.
%     \item It is prohibited to use commands such as \verb|\vspace| and \verb|\hspace| to reduce the pre-set white space in the document.
%     \item Please make sure your figures and tables do not span into the margins.
%     \item If you have tables, the smallest font size allowed is 8pt.
%     \item Please ensure your paper is properly anonymized.  Refer to your previously published paper in the third person.  Data collection location and private dataset should be masked.
%     \item Your paper must not exceed the page limit.  You can have 8 pages of the main content (text (including paper title, author section, abstract, keyword section), figures and tables, and conclusion) plus up to 2 pages of references.  The reference section does not need to start on a new page.  If you include the acknowledgments and the Disclosure of Interest sections, these two sections are considered part of the main content.
%     \item If you submit supplementary material, please note that reviewers are encouraged but not required to read it.  The main paper should be self-contained.  *NEW* Supplementary materials are limited to multimedia content (e.g., videos) as warranted by the technical application (e.g. robotics, surgery, ….). These files should not display any proofs, analysis, or additional results, and should not show any identification markers either. Violation of this guideline will lead to desk rejection. PDF files may NOT be submitted as supplementary materials in 2025 unless authors are citing a paper that has not yet been published.  In such a case, authors are required to submit an anonymized version of the cited paper.
% \end{enumerate}


% \section{Section}
% \subsection{A Subsection Sample}
% Please note that the first paragraph of a section or subsection is
% not indented. The first paragraph that follows a table, figure,
% equation etc. does not need an indent, either.

% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).

% \begin{figure}
% \includegraphics[width=\textwidth]{fig1.eps}
% \caption{A figure caption is always placed below the illustration.
% Please note that short captions are centered, while long ones are
% justified by the macro package automatically.} \label{fig1}
% \end{figure}

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.

% \begin{comment}  %% removed for anonymized MICCAI 2025 submission.
    
%     % The following acknowledgement and disclaimer sections should be removed for the double-blind review process.  
%     % If and when your paper is accepted, reinsert the acknowledgement and the disclaimer clause in your final camera-ready version.

% \begin{credits}
% \subsubsection{\ackname} A bold run-in heading in small font size at the end of the paper is
% used for general acknowledgments, for example: This study was funded
% by X (grant number Y).

% \subsubsection{\discintname}
% It is now necessary to declare any competing interests or to specifically
% state that the authors have no competing interests. Please place the
% statement with a bold run-in heading in small font size beneath the
% (optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
% system, is used, then the disclaimer can be provided directly in the system.},
% for example: The authors have no competing interests to declare that are
% relevant to the content of this article. Or: Author A has received research
% grants from Company W. Author B has received a speaker honorarium from
% Company X and owns stock in Company Y. Author C is a member of committee Z.
% \end{credits}

% \end{comment}
% %
% % ---- Bibliography ----
% %
% % BibTeX users should specify bibliography style 'splncs04'.
% % References will then be sorted and formatted in the correct style.
% %
\bibliographystyle{splncs04}
\bibliography{bib}
% %
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}, last accessed 2023/10/25
% \end{thebibliography}
\end{document}
