\pagebreak
\section{Embedding Gradients}
\label{app:chain_rule_e}

We explicitly derive Eq.~(\ref{eq:chain_rule_e}), which we recall here for convenience:
\begin{align}
g_i :=~ &\frac{\partial \mathcal{L}}{\partial e_i} 
= - \left( \delta_{it} - p_i \right) \cdot h \tag{\ref{eq:chain_rule_e}}
\end{align}
The chain rule yields
\begin{align}
\frac{\partial \mathcal{L}}{\partial e_i} 
&= \sum_{k=1}^{V} \frac{\partial \mathcal{L}}{\partial p_t} 
\cdot \frac{\partial p_t}{\partial l_k}  
\cdot \frac{\partial l_k}{\partial e_i} \; ,
\label{eq:chain_rule_basis}
\end{align}
where the individual factors can directly be obtained from Eqs.~(\ref{eq:forward_loss})-(\ref{eq:gradient_function_new}):
\begin{align}
\frac{\partial \mathcal{L}}{\partial p_t} &= - \frac{1}{p_t} \label{eq:backward_loss} \\
\frac{\partial p_t}{\partial l_k} 
&= \frac{\delta_{kt} \exp{(l_t)} \cdot \Sigma - \exp{(l_t)} \exp{(l_k)}}{\Sigma^2} \nonumber \\ 
&= \delta_{kt} p_t - p_t p_k \nonumber \\
&= p_t ( \delta_{kt} - p_k ) 
\label{eq:backward_loss_2} \\
\frac{\partial l_k}{\partial e_i} &= \delta_{ki} h \label{eq:backward_e}
\end{align}
Note that in the first line of Eq.~(\ref{eq:backward_loss_2}), we use the abbreviation $\Sigma = \Big( \sum_{j=1}^V \exp{(l_j)} \Big)$.
Inserting Eqs.~(\ref{eq:backward_loss}), (\ref{eq:backward_loss_2}) and (\ref{eq:backward_e}) into Eq.~(\ref{eq:chain_rule_basis}) directly leads to Eq.~(\ref{eq:chain_rule_e}):
\begin{align}
\frac{\partial \mathcal{L}}{\partial e_i} 
&= - \sum_{k=1}^{V} \frac{1}{p_t}
\cdot p_t ( \delta_{kt} - p_k )  
\cdot \delta_{ki} h \nonumber \\
&= - \sum_{k=1}^{V} ( \delta_{kt} - p_k )  
\cdot \delta_{ki} h \nonumber \\
&= - ( \delta_{it} - p_i )  
\cdot h \nonumber
\end{align}
