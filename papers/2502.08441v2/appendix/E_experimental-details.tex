\section{Experimental Details}
\subsection{Model and Dataset Sizes}
\label{app:experiments_overview}

The model sizes $N$ and dataset sizes $D$ employed in our experiments are depicted in Fig.~\ref{fig:experiments}.
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figs/experiments_log.png}
    \caption{Overview of the dataset (horizontal axis) and model sizes (vertical axis) involved in our small-scale (blue, green and orange circles) and large-scale (red squares) experiments. The dashed, black line shows $N = D / 20$, which is approximately the compute-optimal trajectory according to \citet{hoffmann2022trainingcomputeoptimallargelanguage}.}
    \label{fig:experiments}
\end{figure}


\subsection{Training Hyperparameters}
\label{app:hyperparameters}

In Tab.~\ref{tab:model_architecture}, we list the general hyperparameters used in our small-scale (Sec.~\ref{sec:experiments_S}) and large-scale (Sec.~\ref{sec:experiments_L}) experiments. 
\begin{table}[ht]
\centering
\scriptsize
\begin{tabular}{l|cc}
\toprule
Description & Small-scale & Large-scale \\ 
\midrule
optimizer & \multicolumn{2}{c}{AdamW} \\ 
$\beta_1$ & \multicolumn{2}{c}{0.9} \\
$\beta_2$ & \multicolumn{2}{c}{0.95} \\
$\epsilon$ & \multicolumn{2}{c}{1e-8} \\
weight decay & \multicolumn{2}{c}{0.1} \\
gradient clipping & \multicolumn{2}{c}{1.0} \\
dropout & \multicolumn{2}{c}{0.0} \\
weight tying & \multicolumn{2}{c}{true} \\
vocab size & \multicolumn{2}{c}{50304} \\
learning rate schedule & \multicolumn{2}{c}{cosine decay} \\
layer normalization & \multicolumn{2}{c}{LayerNorm} \\
precision & \multicolumn{2}{c}{BF16} \\
\midrule
hidden activation & GeLU & SwiGLU \\
positional embedding & absolute (learned) & RoPE \\ 
sequence length & 1024 & 2048 \\
batch size (samples) & 96 & 256 \\
batch size (tokens) & $\sim$100k & $\sim$500k \\
warmup & 100 steps & $1\%$ of steps \\
training framework & nanoGPT & Modalities \\
training parallelism & DDP & FSDP \\
\bottomrule 
\end{tabular}
\caption{General hyperparameters used in our two sets of experiments.}
\label{tab:model_architecture}
\end{table}
During warm-up, the learning rate is increased from zero to the maximum learning rate. This is followed by a cosine decay which reduces the learning rate to $10\%$ of the maximum at the end of training. Note that weight decay is applied only to linear layers, not layer norms or embeddings.
Tab.~\ref{tab:model_sizes} shows the hyperparameters related to model size, following GPT-3 \cite{brown2020languagemodelsfewshotlearners}.
\begin{table}[ht]
\centering
\scriptsize
\begin{tabular}{c|cccc}
\toprule
$N$ & lr & heads & layers & emb. dim. \\ 
\midrule
124M & 6.0e-4 & 12 & 12 & 768 \\
350M & 3.0e-4 & 16 & 24 & 1024 \\
760M & 2.5e-4 & 16 & 24 & 1536 \\
1.3B & 2.0e-4 & 32 & 24 & 2048 \\
2.6B & 1.6e-4 & 32 & 32 & 2560 \\
\bottomrule 
\end{tabular}
\caption{Model-size dependent hyperparameter used in our experiments. $N$ denotes the model size in terms of parameters, while lr corresponds to the maximum learning rate.}
\label{tab:model_sizes}
\end{table}
