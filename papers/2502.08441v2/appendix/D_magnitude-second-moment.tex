\section{Magnitude of the Second Moment in Adam}

In this appendix, the validity of 
\begin{equation}
    \E \left[ \secondmomentshort \right] \propto \widetilde p_i
    \tag{\ref{eq:second_moment_linear_in_unigram_prob}}
\end{equation}
is verified.
Due to the linearity of lines 5 and 7 in Algorithm 2, it suffices to show that the squared gradient has the property in question:
\begin{align}
\E \left[ g_i^2 \right] &\propto \widetilde p_i
\label{eq:squared_gradient_linear_in_unigram_prob}
\end{align}
We do this in two different ways.
First, we derive Eq.~(\ref{eq:squared_gradient_linear_in_unigram_prob}) using a semi-theoretical approach with minimal experimental input.
Afterwards, we confirm the relationship in a purely experimental manner. %

\subsection{Semi-theoretical Derivation}
\label{app:second_moment_theory}

Here, we derive an expression for the expectation value of the squared gradient in terms of simple observables (Theorem~\ref{theorem}). Subsequently, the dependency of those observables on $\widetilde p_i$ is determined experimentally. Together, this will yield the proportionality expressed by Eq.~(\ref{eq:squared_gradient_linear_in_unigram_prob}).
We begin our reasoning with a lemma.

\begin{lemma}[Expectation Value Decomposition]\label{lemma}
The expectation value of the squared gradient can be decomposed into conditional expectation values as follows:
\begin{align}
\E \left[ g_i^2 \right] =~ 
&\widetilde p_i \cdot \E \left[ g_i^2 ~\big|~ i=t \right] \nonumber \\
&+ (1 - \widetilde p_i) \cdot \E \left[ g_i^2 ~\big|~ i \neq t \right]
\label{eq:lemma1}
\end{align}
\end{lemma}

\begin{proof}
Our starting point is the definition of the expectation value for the continuous random variable $g_i^2$:
\begin{align}
\E \left[ g_i^2 \right] = \int g_i^2 ~p(g_i) ~dg_i \; ,
\label{eq:aux_E_definition}
\end{align}
where $p$ denotes the probability distribution of $g_i$. Since the vocabulary item $i$ can only be either the true token $t$ or not, we can decompose $p$ into a sum of joint probability distributions (using the {\em law of total probabilities}), each of which can be expressed in terms of conditional probabilities like so:
\begin{align}
p(g_i) 
&= p(g_i, i=t) + p(g_i, i\neq t) \nonumber \\
&= p(g_i ~|~ i=t) \cdot p(i=t) \nonumber \\
&\quad + p(g_i ~|~ i\neq t) \cdot p(i\neq t)
\end{align}
Using the unigram probability $\widetilde p_i = p(i = t)$, this can also be written as
\begin{align}
p(g_i) 
&= \widetilde p_i \cdot p(g_i ~|~ i=t) \nonumber \\
&\quad + (1 - \widetilde p_i) \cdot p(g_i ~|~ i\neq t)
\label{eq:aux_p_decomposition}
\end{align}
If we insert Eq.~(\ref{eq:aux_p_decomposition}) back into Eq.~(\ref{eq:aux_E_definition}), the expectation value becomes
\begin{align}
\E \left[ g_i^2 \right] 
&= \widetilde p_i \cdot \int g_i^2 ~p(g_i ~|~ i=t) ~dg_i \nonumber \\
&\quad + (1 - \widetilde p_i) \cdot \int g_i^2 ~p(g_i ~|~ i\neq t) ~dg_i \;, 
\label{eq:aux_E_decomposition} %
\end{align}
which by definition of the (conditional) expectation value, Eq.~(\ref{eq:aux_E_definition}), is equivalent to Eq.~(\ref{eq:lemma1}).
\end{proof}
\begin{theorem}[Expectation Value Squared Gradient] \label{theorem}
Given that the squared hidden state vector $h^2$ is independent of $p_i$ and whether $i$ is the true token or not, the expectation value of the squared gradient $g_i^2$ is given by
\begin{align}
\E \left[ g_i^2 \right] 
&= S \cdot \left[ \widetilde p_i \cdot X_i^{(i=t)} + (1 - \widetilde p_i) \cdot X_i^{(i \neq t)} \right] \; ,
\label{eq:theorem}
\end{align}
with
\begin{align}
S &:= \E \left[ h^2 \right] \label{eq:optimizer_second_moment_expansion_S} \\
X_i^{(i=t)} &:= \E \left[ (1 - p_i)^2 ~\big|~ i=t \right]
\label{eq:optimizer_second_moment_expansion_true_2} \\
X_i^{(i \neq t)} &:= \E \left[ p_i^2 ~\big|~ i \neq t \right]
\label{eq:optimizer_second_moment_expansion_false_2}
\end{align}
\end{theorem}

\begin{proof}
We start from Lemma~\ref{lemma} and the square of the gradient,
\begin{align}
g_i^2 
&\stackrel{(\ref{eq:chain_rule_e})}{=} \left( \delta_{it} - p_i \right)^2 h^2 
\label{eq:optimizer_second_moment}
\end{align}
Note that squared variables of vectors in $\mathbb{R}^H$ always denote the elementwise (Hadamard) product, e.g.
\begin{align}
g_i^2 &\equiv g_i \odot g_i \in \mathbb{R}_{\geq 0}^H \; ,
\label{eq:hadamard_product}
\end{align}
with strictly non-negative elements.
Using Eq.~(\ref{eq:optimizer_second_moment}), the expectation values on the right side of Eq.~(\ref{eq:lemma1}) can be expressed as
\begin{align}
\E \left[ g_i^2 ~\big|~ i=t \right]
&= \E \left[ \left( 1 - p_i \right)^2 \cdot h^2 ~\big|~ i=t \right] \\
\E \left[ g_i^2 ~\big|~ i\neq t \right]
&= \E \left[ p_i^2 \cdot h^2 ~\big|~ i\neq t \right]
\end{align}
Given our assumptions regarding $h^2$, its expectation value can be factored out:
\begin{align}
\E \left[ g_i^2 ~\big|~ i=t \right]
&= S \cdot X_i^{(i=t)} \label{eq:Etrue} \\
\E \left[ g_i^2 ~\big|~ i\neq t \right]
&= S \cdot X_i^{(i \neq t)} \label{eq:Efalse} 
\end{align}
Inserting Eqs.~(\ref{eq:Etrue}) and (\ref{eq:Efalse}) into Eq.~(\ref{eq:lemma1}) yields Eq.~(\ref{eq:theorem}).
\end{proof}

Note that Eq.~(\ref{eq:theorem}) is a vector equation, with $\E \left[ g_i^2 \right], S \in \mathbb{R}_{\geq 0}^H$ and $\widetilde p_i, X_i^{(i=t)}, X_i^{(i \neq t)} \in \mathbb{R}_{\geq 0}$.
It states that the expectation value of $g_i^2$ factorizes into a global constant $S$ that is $i$-independent, and a factor that is $i$-dependent. The latter is a specific combination of the unigram probability $\widetilde p_i$, determined by the data, and the conditional expectation values $X_i^{(i=t)}$ and $X_i^{(i\neq t)}$, determined by the model.

\paragraph{Experimental Input}

Regarding the unigram probability, we know that
\begin{enumerate}
\item $\widetilde p_i \ll 1$. \\
This is the case for virtually all natural language datasets with a common vocabulary size of $V > 10000$, according to Zipf's law.
\end{enumerate}
The conditional expectation values $X_i^{(i=t)}$ and $X_i^{(i\neq t)}$ can be empirically estimated by applying training data to different checkpoints. We consider the three small-scale experiments of Sec.~\ref{sec:experiments_S} with $N \in \{ 125\M, 355\M, 760\M \}$ and $D=20\B$, and take ten equidistant checkpoints after $D^\prime \in \{ 2\B, 4\B, \ldots, 20\B\}$ seen tokens for each of them. We then continue pseudo-training on 20 batches ($\approx$ 2k samples or 2M tokens, see Tab.~\ref{tab:model_architecture}) of data using a zero learning rate, and measure the conditional probabilities in Eqs.~(\ref{eq:optimizer_second_moment_expansion_true_2}, \ref{eq:optimizer_second_moment_expansion_false_2}) from which our target quantities can be estimated.  
Subsequently, linear fits of the form 
\begin{align}
    X_i^{(i = t)} &= A^{(i = t)} \cdot \widetilde p_i \\
    X_i^{(i \neq t)} &= A^{(i \neq t)} \cdot \widetilde p_i \; ,
\end{align}
with fit parameters $A^{(i = t)}$ and $A^{(i \neq t)}$ are performed. $R^2$ is used to assess the quality of the fits. In addition, the mutual information $\I$ between the response and the explanatory variable is computed. 
Since we observe only a very weak dependence of the results for $R^2$ and $\I$ on $N$ and $D^\prime$, we specify the mean and standard deviation over all experiments for them.
Our findings are:
\begin{enumerate}
\item[2.] $X_i^{(i = t)}$ is independent of $\widetilde p_i$. \\
The linear fits yield $R^2 = 0.003(1)$, and the mutual information is $\I \left(X_i^{(i = t)}; \widetilde p_i \right) = 0.14(2)$.
\item[3.] $X_i^{(i \neq t)}$ is proportional to $\widetilde p_i$. \\
The linear fits yield $R^2 = 0.92(1)$, and the mutual information is $\I \left( X_i^{(i \neq t)}; \widetilde p_i \right) = 0.50(2)$.
\end{enumerate}

The three empirical results above, together with Theorem~\ref{theorem}, immediately lead to Eq.~(\ref{eq:squared_gradient_linear_in_unigram_prob}).

\subsection{Experimental Confirmation}
\label{app:second_moment_empirical}

We reuse the experiments from the previous section to measure the second moment $\secondmomentshort$ directly, in order to estimate $\E \left[ \secondmomentshort \right]$. Again, linear fits of the form
\begin{align}
    \E \left[ \secondmomentshort \right] = A \cdot \widetilde p_i
\end{align}
are performed and the mutual information is computed. 
We find
\begin{enumerate}
\item[4.] $\E \left[ \secondmomentshort \right]$ is proportional to $\widetilde p_i$. \\
The linear fits yield $R^2 = 0.85(7)$, and the mutual information is $\I \left( \E \left[ \secondmomentshort \right]; \widetilde p_i \right) = 1.18(9)$.
\end{enumerate}
The results for $N=125\M$ and $D=D^\prime=20\B$ are depicted in Fig.~\ref{fig:experimental_results_E_p}, as an example.
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figs/experimental_results_E_p_125M_20B.png}
    \caption{Experimental results for $\E \left[ \secondmomentshort \right]$ (vertical axis) vs. $\widetilde p_i$ (horizontal axis) for $N=125\M$ and $D=D^\prime=20\B$. The blue line shows the linear fit with $R^2 = 0.91$.}
    \label{fig:experimental_results_E_p}
\end{figure}

Note that while $R^2$ and $\I$ are again virtually independent of $N$ and $D^\prime$, the fit parameter $A$ is not. Instead, it seems to increase with $D^\prime$, as shown in Fig.~\ref{fig:experimental_results_A}.
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{figs/experimental_results_A.png}
    \caption{Experimental results for the linear fit parameter $A$ as a function of $N$ and $D^\prime$.}
    \label{fig:experimental_results_A}
\end{figure}
However, as stated in Eq.~(\ref{eq:second_moment_proportionality_constant}), the order of magnitude is $A \approx 10^{-4}$ throughout our experiments.
