\section{Experiments}
\label{sec:experiments}

Two types of experiments are conducted to study the impact of coupling the second moments of the embedding update vectors. 
First, a set of small-scale experiments (Sec.~\ref{sec:experiments_S}) with models and datasets of varying sizes up to 1B parameters and 20B tokens, respectively.
Afterwards, we perform a few large-scale experiments (Sec.~\ref{sec:experiments_L}) to verify that the usefulness of our method extrapolates to the realm of large language models with more than 1B parameters trained on at least the corresponding compute-optimal \cite{hoffmann2022trainingcomputeoptimallargelanguage} amount of data.
In order to verify the generalizability of our method, the small- and large-scale experiments involve different datasets, training frameworks and dense transformer model architectures. 
An overview of the model and dataset sizes employed in our experiments is given in App.~\ref{app:experiments_overview}.
For each combination, two models are trained: one using standard Adam and one using Coupled Adam for the embeddings, see Eq.~(\ref{eq:optimizer_update_second_moment_avg}). Both variants use standard Adam for all non-embedding parameters. The various metrics we employ to assess both the general model performance and the quality of the model embeddings will be discussed in Sec.~\ref{sec:experiments_evaluation}.
\subsection{Small-scale Experiments}
\label{sec:experiments_S}
Our small-scale experiments use the OpenWebText Corpus \cite{Gokaslan2019OpenWeb} and the GPT-2 tokenizer \cite{radford2019language}. The model architecture also follows GPT-2, while the hyperparameter setup is taken from GPT-3 \cite{brown2020languagemodelsfewshotlearners}, see App.~\ref{app:hyperparameters} for further details. An implementation based on nanoGPT \cite{Karpathy2022} is used. 
We define a grid $(D, N)$ with dataset sizes 
$D \in \{ 5\B, 10\B, 20\B \}$
and model sizes 
$N \in \{ 125\M, 355\M, 760\M \}$,
and repeat each experiment $S = 3$ times with different seeds in order to estimate uncertainties and assess statistical significance.
\subsection{Large-scale Experiments}
\label{sec:experiments_L}
For our large-scale experiments, we use the SlimPajama dataset \cite{cerebras2023slimpajama} and the GPT-2 tokenizer. A state-of-the-art dense transformer model architecture akin to \cite{touvron2023llama2openfoundation} 
is chosen, including e.g. RoPE embeddings \cite{su2023roformerenhancedtransformerrotary} and the SwiGLU activation function \cite{shazeer2020gluvariantsimprovetransformer}.
Details can be found in App.~\ref{app:hyperparameters}. The experiments are conducted using Modalities \cite{modalities} as the training framework.
We consider two model sizes, 1.3B and 2.6B. In order to cover the two common scenarios of compute-optimal training and overtraining, we conduct two sets of experiments: Firstly, we use near compute-optimal dataset sizes, 26B and 52B tokens, respectively. Secondly, we increase the number of tokens by a factor 4, resulting in 105B and 210B tokens, respectively.
Each large-scale experiment is performed $S = 1$ times.

\subsection{Evaluation}
\label{sec:experiments_evaluation}
Upstream performance is measured in terms of test loss, while downstream performance is evaluated using the Language Model Evaluation Harness \cite{eval-harness} on the following tasks: ARC easy and challenge \cite{clark2018thinksolvedquestionanswering}, HellaSwag \cite{zellers-etal-2019-hellaswag}, LAMBADA \cite{paperno-etal-2016-lambada}, RACE \cite{lai-etal-2017-race}, TruthfulQA \cite{lin-etal-2022-truthfulqa} and WinoGrande \cite{Sakaguchi_LeBras_Bhagavatula_Choi_2020}.
More concretely, the considered metric is the average accuracy, which we will denote by $\Acc$.
To assess the quality of the embeddings, we first compute their isotropy, defined as \cite{arora-etal-2016-latent, mu2018allbutthetopsimpleeffectivepostprocessing}
\begin{equation}
\ISO(E) := \frac{\min_{c \in X} Z(c)}{\max_{c \in X} Z(c)} \; ,
\label{eq:isotropy}
\end{equation}
where $E \in \mathbb{R}^{H \times V}$ is the embedding matrix,
$Z(c) = \sum_{i=1}^V \exp(c^T e_i)$ 
is the partition function and $X = \{ c \}$ is the set of eigenvectors $c \in \mathbb{R}^{H}$ of $E E^T \in \mathbb{R}^{H \times H}$.
Secondly, the 2-norm $\munorm$ of the mean embedding, see Eq.~(\ref{eq:mu}), and the average 2-norm of the embeddings
$\overline{\|e_i\|} = \frac{1}{V} \sum_{i=1}^V \|e_i\|$
as well as their ratio 
\begin{equation}
\munormrel := \munorm / \overline{\|e_i\|}
\end{equation}
are determined. 
In addition, we evaluate the models on embedding benchmarks for word similarity and relatedness, to assess how well they represent semantic meaning. Following \citet{bis2021tmic}, we consider the benchmarks SimLex999 \cite{hill-etal-2015-simlex}, MEN \cite{10.5555/2655713.2655714}, WordSim353 \cite{finkelstein} and Stanford Rare Words \cite{luong-etal-2013-better}. Each dataset provides pairs of words labeled with a ground truth score that represents the words' semantic similarity. 
We derive model scores from the cosine similarity of the corresponding embedding vectors, and report the Pearson correlation of the two scores averaged over the datasets, which we denote by $\rcos$.
Finally, some additional important properties of the embedding matrix are investigated.
We study the correlation between the length of an embedding vector and the unigram probability,
\begin{equation}
\rho := 100 \cdot \text{corr} \big( (\|e_i\|)_{i=1}^V, \widetilde p \big) \; ,
\label{eq:rho} %
\end{equation}
to measure how well the former represents the latter.
Furthermore, the condition number $\kappa$, defined as the ratio of the smallest and largest singular values of the embedding matrix, is determined in percent:
\begin{equation}
\kappa := 100 \cdot \frac{\min_i \Sigma_{ii}}{\max_i \Sigma_{ii}}
\label{eq:kappa}
\end{equation}
Here, $E = U \Sigma V^T$ denotes the singular value decomposition of the embedding matrix.
