\section{Introduction}
\paragraph{Anisotropic Embeddings}
Large Language Models (LLMs) take a sequence of tokens as input and predict the next token. An embedding matrix is used to map the input tokens to the hidden space of the model, while an unembedding matrix provides the inverse mapping to the output token space. Although the two matrices can in principle be different, it is common practice to apply weight tying \cite{press-wolf-2017-using} and use the transpose of the embedding matrix for unembedding. 
During training, the model learns an embedding vector in hidden space for each token in the vocabulary. However, it is observed that those embedding vectors are clustered in a small subspace away from the origin \cite{gao2019representationdegenerationproblemtraining}. 
This anisotropy limits the semantic usefulness of the embeddings and, in turn, the expressiveness and generalizability of the model.
Multiple attempts have been made to both explain the root cause of the problem and alleviate it (more on this in Sec.~\ref{sec:related_work}). 
In particular, \citet{bis2021tmic} have shown that the problem can be traced back to a mere shift of the mean embedding vector away from the origin. With the mean embedding vector as reference point, the embeddings feature near-perfect isotropy.
However, the role of the employed optimization algorithm has, to the best of our knowledge, not yet been investigated. 

\paragraph{Optimization Algorithms}
Optimization algorithms are an indispensable ingredient in the training of neural networks generally and LLMs in particular.
While SGD is the foundational optimization technique, Adam \cite{adam} is the most widely used optimization techniques for LLMs due to its superior performance and robustness.
While it provides multiple conceptional advantages over SGD, see e.g. \citet{ruder2017overviewgradientdescentoptimization} for a detailed discussion, the one that is particularly striking 
with regard to word embeddings is that Adam is well-suited for sparse data. More concretely, this means that using Adam, the embedding update vectors for rare words are scaled up in comparison to those of more frequent words. This is relevant in the context of LLMs as word frequencies in the training data are typically very skewed and may differ by several orders of magnitude.
Formally, this is captured by the {\em unigram probability distribution} $\widetilde{p} \in [0, 1]^V$, which for a given dataset $d$ and tokenizer $t$ is defined by
\begin{equation}
\widetilde{p}_i \equiv \widetilde{p}_i(d,t) = \frac{n_i}{\sum_j n_j} \; ,
\label{eq:unigram_probability}
\end{equation}
where $i \in \V \equiv \{1, \dots, V\}$
is the vocabulary index and $n_i$ is the total number of occurrences of the $i$-th token in the tokenized dataset. 
A visualization of an example unigram probability distribution can be found in App.~\ref{app:unigram_probability_example}.

\paragraph{Our Contributions}

In this work, we combine the research areas of anisotropic embeddings and optimization algorithms
and provide the following contributions:
\begin{itemize}
\item We show that the Adam optimizer plays a crucial role in causing anisotropic embeddings. 
\item We suggest \textit{Coupled Adam}, an easy-to-implement yet efficient adjustment of the original Adam optimization algorithm, which is specifically designed for embedding parameters in order to alleviate the anisotropy problem. 
\item We demonstrate that our method not only significantly improves the quality of word embeddings, but also has a beneficial effect on upstream and downstream performance for sufficiently large datasets.
\end{itemize}
