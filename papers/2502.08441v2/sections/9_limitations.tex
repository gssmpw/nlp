\section{Limitations}

Although our method is generally applicable to all common LLM architectures, as they share the same language modeling head and embeddings, only dense decoders were used in our experiments. 
In addition, only models with up to $N=2.6\B$ parameters have been tested.
The cosine decay learning rate schedule was applied throughout all experiments (App.~\ref{app:hyperparameters}). Alternatives such as an infinite learning rate schedule are not incorporated in our study.
Furthermore, as mentioned at the end of Sec.~\ref{sec:results}, we have not explicitly verified that the slight residual shift of the mean embedding, which is observed even for Coupled Adam, is caused by weight tying.
Finally, we have used a straightforward implementation of Coupled Adam, closely following Algorithm~\ref{alg:algorithm_adam}. More sophisticated implementations might lead to increased efficiency and further improvements; we leave it for future work to investigate this.
