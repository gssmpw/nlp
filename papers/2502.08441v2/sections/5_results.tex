\CatchFileDef{\resultsS}{tables/results_S.tex}{}
\CatchFileDef{\resultsL}{tables/results_L.tex}{}

\begin{table*}[!ht]
\centering
\scriptsize
\begin{tabular}{ccc|rrrrrrrr}
\toprule
$D$ & $N$ & Adam & $\Loss$ ($\downarrow$) & $\Acc$ ($\uparrow$) & $\ISO$ ($\uparrow$) & $\munorm$ ($\downarrow$) & $\munormrel$ ($\downarrow$) & $\rcos$ ($\uparrow$) & $\rho$ ($\uparrow$) & $\kappa$ ($\uparrow$) \\ 
\midrule
\resultsS
\bottomrule 
\end{tabular}
\caption{Results of our small-scale experiments. $D$ and $N$ denote the dataset and model size, respectively. $\Loss$ is the test loss, and the column $\Acc$ represents the accuracy averaged over the downstream tasks listed in Sec.~\ref{sec:experiments_evaluation}. The other evaluation metrics are defined in the same section, see Eqs.~(\ref{eq:isotropy})-(\ref{eq:kappa}). The arrow in parentheses indicates whether a higher or lower value is desirable. Every training was conducted $S=3$ times with different seeds, and the numbers represent the (rounded) averages and standard deviations in the following shorthand notation format: $0.123${\scriptsize~$(4)$} $\equiv 0.123 \pm 0.004$. For each combination $(D, N)$ and each metric, the respective better value is highlighted in bold if the (unrounded) difference is significant according to Student's t-test with a one-sided confidence level of $\alpha = 95\%$ (see App.~\ref{app:error} for details). Plots for $\Loss$ and $\Acc$ are shown in App.~\ref{app:additional_results_S}.}
\label{tab:results_S}
\end{table*}

\section{Results}
\label{sec:results}

\subsection{Small-scale Experiments}
\label{sec:results_S}

The results of the small-scale experiments (Sec.~\ref{sec:experiments_S}) are shown in Tab.~\ref{tab:results_S}.
We find that both upstream and downstream performance are better with Coupled Adam if the dataset size is sufficiently large. In fact, the improvement appears to increase monotonically with the dataset size $D$. 
In addition, the embedding-specific metrics benefit greatly from Coupled Adam. In particular, the isotropy reaches values above $0.90$ (with a single exception), while $\rcos$ and $\kappa$ are hugely improved as well. The mean embedding is evidently close to the origin. Finally, Coupled Adam leads to a significantly stronger (positive) correlation $\rho$ between the length of an embedding vector and its associated unigram probability.

\subsection{Large-scale Experiments}
\label{sec:results_L}

The results of the large-scale experiments (Sec.~\ref{sec:experiments_L}) are shown in Tab.~\ref{tab:results_L}.
We observe very similar patterns as for the small-scale experiments. Although upstream and downstream performance are worse with Coupled Adam for compute-optimal dataset sizes, they are better if 4 times larger datasets are used. Note that for the small-scale experiments, the upstream and downstream performance were found to be better already for compute-optimal dataset sizes. We attribute this to the fact that the batch size for the large-scale experiments is five times larger (cf.~App.~\ref{app:hyperparameters}), which results in fewer optimization steps for the same dataset size.
Regarding the embedding-specific metrics, we again find significant and consistent improvements throughout all experiments. 
However, we do observe a certain shift of the mean embedding vector away from the origin, even if Coupled Adam is used. The shift becomes more pronounced as the model and dataset sizes increase, and is also reflected in a reduced isotropy.
As we shall see in the following section, it comes along with optimal model performance though. 
An obvious hypothesis in light of our analysis in Sec.~\ref{sec:theory} is that the residual shift of the mean embeddings is due to weight tying. This is supported by the results of \citet{machina-mercer-2024-anisotropy}, who find improved isotropy for models without weight tying. We leave it for future work to verify the hypothesis.

\begin{table*}[ht]
\centering
\scriptsize
\begin{tabular}{lll|rrrrrrrr}
\toprule
$D$ & $N$ & Adam & $\Loss$ ($\downarrow$) & $\Acc$ ($\uparrow$) & $\ISO$ ($\uparrow$) & $\munorm$ ($\downarrow$) & $\munormrel$ ($\downarrow$) & $\rcos$ ($\uparrow$) & $\rho$ ($\uparrow$) & $\kappa$ ($\uparrow$) \\ 
\midrule
\resultsL
\bottomrule 
\end{tabular}
\caption{Results of our large-scale experiments. See the caption of Tab.~\ref{tab:results_S} for an explanation of the column names. For each combination $(D, N)$ and each metric, the respective better value is highlighted in bold.}
\label{tab:results_L}
\end{table*}
