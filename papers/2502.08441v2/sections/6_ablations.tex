\CatchFileDef{\resultsAblationsScaleSmall}{tables/results_ablations_scale_small.tex}{}
\CatchFileDef{\resultsAblationsSGDAll}{tables/results_ablations_sgd_all.tex}{}

\section{Ablations}
\label{sec:ablations}

We perform some additional experiments to shed further light on how Coupled Adam works. A model size of $N = 125\M$ and the dataset sizes $D \in \{ 5\B, 10\B, 20\B \}$ from the small-scale experiments (Sec.~\ref{sec:experiments_S}) are used, and each experiment is repeated $S = 3$ times with different seeds. 

\subsection{Scaled Coupled Adam}
\label{sec:ablation_different_learning_rate}

While coupling the second moment of the embedding gradients using the average in Eq.~(\ref{eq:optimizer_update_second_moment_avg}) is the canonical choice, one could also use a multiple of the average. We conduct additional experiments where the coupled second moment is scaled by powers of $2$:
\begin{equation}
\secondmomentavg \: \to \: 2^{-n} \cdot \secondmomentavg  \; ,
\label{eq:optimizer_update_second_moment_avg_scaled}
\end{equation}
with scaling exponents
$n \in \{ z \in \mathbb{Z} ~| -5 \leq z \leq 5 \}$.
Note that using a scaling exponent $n \neq 0$ is equivalent to using a different effective learning rate for the embeddings than for all the other parameters, via Eqs.~(\ref{eq:optimizer_update_second_moment_avg}) and (\ref{eq:adam_learning_rate}). In particular, a smaller scaling exponent $n$ corresponds to a smaller effective learning rate and vice versa. 
The results for $D=20\B$ are shown in Tab.~\ref{tab:results_ablations_scale}, 
and the dependency of the loss on the scaling exponent $n$ for that very dataset size is visualized in Fig.~\ref{fig:ablation_scale}.
\begin{figure*}[t]
\begin{minipage}{\textwidth}
  \begin{minipage}[b]{0.63\textwidth}
    \centering
    \scriptsize
    \begin{tabular}{c|rrrrrrrr}
    \toprule
    $n$ & $\Loss$ ($\downarrow$) & $\Acc$ ($\uparrow$) & $\ISO$ ($\uparrow$) & $\munorm$ ($\downarrow$) & $\munormrel$ ($\downarrow$) & $\rcos$ ($\uparrow$) & $\rho$ ($\uparrow$) & $\kappa$ ($\uparrow$) \\ 
    \midrule
    \resultsAblationsScaleSmall
    \bottomrule 
    \end{tabular}
    \captionof{table}{Results of our experiments with Scaled Coupled Adam, for $N=125\M$ and $D=20\B$. Values are highlighted in bold if they are significantly better than \textit{all} the other values in the same column, see the caption of Tab.~\ref{tab:results_S} for more details.}
    \label{tab:results_ablations_scale}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.35\textwidth}
    \centering
    \includegraphics[scale=0.40]{figs/ablation_1_loss_20B.png}
    \captionof{figure}{Dependency of the loss on the scaling exponent $n$, see Eq.~(\ref{eq:optimizer_update_second_moment_avg_scaled}), for $N=125\M$ and $D=20\B$. The plot shows the difference to the loss obtained for $n = 0$.}
    \label{fig:ablation_scale}
  \end{minipage}
\end{minipage}
\end{figure*}
Results for other dataset sizes and plots for the other evaluation metrics can be found in App.~\ref{app:additional_results_scale}.
Our data shows that the loss reaches a minimum close to $n=0$, with a rather weak dependence on the scaling exponent in its vicinity. Nevertheless, for the smallest and largest scaling exponents studied, we find that the loss gets significantly worse.
Regarding downstream performance, we see indications of a similar pattern, although the statistical uncertainties are too large to draw definite conclusions.  
The semantic usefulness of the embedding vectors as measured by $\rcos$ seems to suffer from a scaling exponent $n < 0$. For the isotropy and the mean embedding, we observe the opposite behavior. They benefit from a smaller scaling exponent $n$ and the associated smaller embedding updates, with the effect being more pronounced the larger the training dataset size $D$.  
However, this also negatively affects the model performance. 
Hence, we conclude that, at least within the range of our experiments, the optimal setting is to have the same learning rate for the embedding parameters as for all the other model parameters, as implied by $n=0$ and Eq.~(\ref{eq:optimizer_update_second_moment_avg}).

\subsection{SGD}
\label{sec:ablation_sgd}

We train several models using SGD with momentum $\gamma = 0.9$ as the optimizer for the embeddings. Since Adam via the inverse square root of its second moment effectively scales the learning rate up by a factor comprising orders of magnitude (see Eq.~(\ref{eq:second_moment_global_factor})), we explicitly multiply the learning rate in SGD by a factor $f$ of comparable size\footnote{Note that the difference between momentum in SGD and the first moment in Adam also plays a role here.}. A hyperparameter search using 
$f \in \{ 100, 200, 300, 400, 500, 600 \}$
is performed to search for the optimum with respect to upstream performance (loss), see App.~\ref{app:additional_results_sgd} for details. It is found at $f = 300$ for $D \in \{ 5\B, 10\B \}$ and $f = 400$ for $D = 20\B$.
The respective optimal model is compared to its counterpart trained with Coupled Adam in Tab.~\ref{tab:results_ablations_sgd_all}.
\begin{table*}[htb]
\centering
\scriptsize
\begin{tabular}{ccc|rrrrrrrr}
\toprule
$D$ & $N$ & Optimizer & $\Loss$ ($\downarrow$) & $\Acc$ ($\uparrow$) & $\ISO$ ($\uparrow$) & $\munorm$ ($\downarrow$) & $\munormrel$ ($\downarrow$) & $\rcos$ ($\uparrow$) & $\rho$ ($\uparrow$) & $\kappa$ ($\uparrow$) \\ 
\midrule
\resultsAblationsSGDAll
\bottomrule 
\end{tabular}
\caption{Comparison of models whose embeddings were trained with SGD and Coupled Adam. The SGD models were obtained after hyperparameter search for the learning rate. The associated factor $f$ is specified in parentheses in the Optimizer column. Bold values indicate better results with statistical significance, see the caption of Tab.~\ref{tab:results_S} for more details.}
\label{tab:results_ablations_sgd_all}
\end{table*}
The results show that, although SGD is advantageous with respect to isotropy, the mean embedding shift and the condition number, Coupled Adam consistently achieves better results on all upstream and downstream task metrics, while having one less hyperparameter to fine-tune.
