\section{Conclusions}

Our work addresses the well-known anisotropy problem for LLM embeddings. 
We have advanced the theoretical understanding of the phenomenon by showing that it is a combination of the common enemy effect and the individual second moments in Adam that causes a collective shift of the embedding vectors away from the origin.
To mitigate the problem, we have introduced Coupled Adam, which enforces the same effective learning rate for every embedding vector, and thus suppresses the collective shift of the embeddings.
We have found that Coupled Adam consistently improves embedding-specific metrics across all experiments, while also achieving better downstream and upstream performance for large datasets, as they are typically used in LLM training.
The code to reproduce our results is available at \href{https://github.com/flxst/coupled-adam}{\nolinkurl{github.com/flxst/coupled-adam}}~.
