\section{Related Work}
\label{sec:related_work}

\citet{gao2019representationdegenerationproblemtraining}
first described the anisotropy issue, which they referred to as \textit{representation degeneration problem}, and suggested cosine regularization as a mitigation strategy.
Alternative techniques to address the problem have been developed, including adversarial noise \cite{pmlr-v97-wang19f}, spectrum control \cite{Wang2020ImprovingNL} and Laplacian regularization \cite{zhang-etal-2020-revisiting}.  
\citet{bis2021tmic} have shown that the anisotropy of embeddings can for the most part be traced back to a common shift of the embeddings in a dominant direction. They called this phenomenon \textit{common enemy effect}, and provided a semi-quantitative explanation (Eq.~(\ref{eq:chain_rule_e})), which we developed further in the present work by including the optimizer in the analysis.
In \citet{yu-etal-2022-rare}, Adaptive Gradient Gating is proposed, based on the empirical observation that it is the gradients for embeddings of rare tokens that cause anisotropy. Our analysis conforms to this finding and attributes it to a massive up-scaling of the gradients for rare embeddings with Adam, cf.~Fig.~\ref{fig:gradients_example}.
\citet{machina-mercer-2024-anisotropy} have demonstrated that large Pythia models \cite{pmlr-v202-biderman23a} show improved isotropy compared to similar models, and attribute this to the absence of weight tying. This is in accordance with our analysis of the unembedding gradients in conjunction with Adam, Sec.~\ref{sec:theory}.
While all the previously mentioned papers use average cosine similarity \cite{ethayarajh-2019-contextual} or $\ISO$ from Eq.~(\ref{eq:isotropy}) to quantify the geometry of embedding vectors, \citet{rudman-etal-2022-isoscore} deviate from this. 
Their notion of isotropy is based solely on the embeddings' covariance matrix and embodied by the metric IsoScore. 
In particular, IsoScore is mean-agnostic, while $\ISO$ strongly correlates with the mean embedding (see e.g. Tab.~\ref{tab:results_S}).
Finally, concurrent to our work, \citet{zhao2024deconstructingmakesgoodoptimizer} have investigated the importance of using the second moment in Adam with regard to performance and stability. They found that simplified variants of Adam that use the same effective learning rate either for the whole embedding matrix (Adalayer) or each embedding vector (Adalayer*) are slightly worse than Adam but better than SGD. Adalayer* is similar to Coupled Adam, but corresponds to the second moment averaged over hidden space instead of vocabulary space.
