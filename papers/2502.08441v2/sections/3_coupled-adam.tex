\section{Coupled Adam}
\label{sec:lmwap}

In the previous section, we have identified the individual scales of the second moments $v_i$ for different embedding vectors $e_i$ as the root cause of the anisotropy problem. This implies that a solution to the problem is to enforce that the second moments are the same for every $i$.
The question arises whether and how this can be done in the best way, without harming the performance of the model.
To answer this, we note that the normalization of the embedding update vector by the Adam second moment can be split into two parts:
\begin{equation}
\E \left[ \secondmomentshort \right] 
\stackrel{(\ref{eq:second_moment_proportionality_constant})}{=} A \cdot \widetilde p_i
= \frac{A}{V} \cdot \left( \widetilde p_i V \right)
\label{eq:second_moment_factorization}
\end{equation}
The first factor introduces a global scale to all update vectors simultaneously:
\begin{equation}
    \frac{A}{V} \stackrel{(\ref{eq:second_moment_proportionality_constant})}{\approx} \frac{10^{-4}}{5 \cdot 10^{4}} = 2 \cdot 10^{-9} \; ,
\label{eq:second_moment_global_factor}
\end{equation}
where the numbers correspond to our experiments from the previous section with $V \approx 50000$.
The second factor scales the update vectors individually. It is one on average:
\begin{equation}
\frac{1}{V} \sum_{i=1}^{V} \left( \widetilde p_i V \right) = 1
\label{eq:second_moment_individual_factor}
\end{equation}
Our goal is to retain the first, global factor and get rid of the second, individual factor. 
The canonical way to do this is to simply take the average of the second moment over the vocabulary items $i$:
\begin{equation}
\frac{1}{V} \sum_{i=1}^{V} \E \left[ \secondmomentshort \right]
\stackrel{(\ref{eq:second_moment_factorization}, \ref{eq:second_moment_individual_factor})}{=} \frac{A}{V}
\label{eq:optimizer_update_second_moment_avg_canonical}
\end{equation}
In practice, the exponentially averaged second moments $\secondmoment$ as they appear in Eq.~(\ref{eq:adam_learning_rate}) are replaced by their average:
\begin{align}
\secondmomentavg \: &:= \: \frac{1}{V} \sum_{i=1}^V \secondmoment
\label{eq:optimizer_update_second_moment_avg}
\end{align}
We call the resulting algorithm \textit{Coupled Adam}, as it couples the second moments of the embedding vectors via Eq.~(\ref{eq:optimizer_update_second_moment_avg}). 
It is displayed in Algorithm~\ref{alg:algorithm_adam}.
Evidently, with Coupled Adam, the effective learning rate in Eq.~(\ref{eq:adam_learning_rate}) that enters the update vector in Eq.~(\ref{eq:update_vector_definition_Adam}) becomes independent of $i$. Hence, like SGD but unlike standard Adam, the sum of embedding updates vanishes.
However, like standard Adam but unlike SGD, Coupled Adam uses a second moment to normalize the embedding update vectors. 
