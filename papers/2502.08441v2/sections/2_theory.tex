\section{On the Root Cause of Anisotropic Embeddings}%
\label{sec:theory}

We study the collective shift of the embeddings (that underlies the anisotropy problem), by analyzing their vector updates based on the optimization algorithms SGD and Adam. Weight tying is assumed, but only contributions from the output layer are considered, following \citet{bis2021tmic}. 
Our results apply to all model architectures with a standard language modeling head.

\subsection{Language Modeling Head}

The equations for the standard language modeling head read
\begin{align}
\mathcal{L} &= - \log{(p_t)} \label{eq:forward_loss} \\
    p_t &= \frac{\exp{(l_t)}}{\sum_{j=1}^V \exp{(l_j)}} \label{eq:forward_probability}\\ %
l_i &= e_i \bigcdot h \label{eq:gradient_function_new} \; ,
\end{align}
where $\mathcal{L} \in \mathbb{R}_{\geq 0}$ is the loss for next token prediction, and $p_t \in [0, 1]$ is the predicted probability of the true token $t \in \V$. $l_i \in \mathbb{R}$ and $e_i \in \mathbb{R}^H$ denote the logits and embeddings for each token $i \in \V$, respectively. $h \in \mathbb{R}^H$ is the final hidden state provided by the model for a single token. Note that the operation in Eq.~(\ref{eq:gradient_function_new}) is the dot product of two vectors in $\mathbb{R}^H$.
Backward propagation yields the following gradients with respect to the input vectors $e_i$ and $h$ of Eq.~(\ref{eq:gradient_function_new}):
\begin{align}
g_i :=~ &\frac{\partial \mathcal{L}}{\partial e_i} 
= - \left( \delta_{it} - p_i \right) \cdot h \label{eq:chain_rule_e} 
\end{align}
This result was first reported using a different notation in \citet{bis2021tmic}, and is rederived in App.~\ref{app:chain_rule_e} for the reader's convenience.

\subsection{Vanishing Sum of Embedding Gradients}

Optimization algorithms for neural networks usually update the model parameters iteratively, using an additive update vector that points in direction opposite to the gradient of the loss with respect to the parameters. In the case of embedding vectors, this can be expressed by
\begin{equation}
e_i^{(\tm)} \: = \: e_i^{(\tm-1)} + u_i^{(\tm)}  \; ,
\label{eq:update_general}
\end{equation}
with
\begin{equation}
u_i^{(\tm)} \: \propto \: - g_i^{(\tm)} \; ,
\label{eq:update_vector_definition}
\end{equation}
where $u_i^{(\tm)}$ is the update vector for $e_i^{(\tm)}$ at time step $\tm$.
Eq.~(\ref{eq:chain_rule_e}) implies that the embedding vector $e_t$ of the true token is updated in direction $+h$, while the update vectors $u_i$ for all the other embedding vectors $e_i$ with $i \neq t$ are proportional to $-h$, see Fig.~\ref{fig:gradients_example}. 
\begin{figure}[t]
\centering
\includegraphics[scale=0.5]{figs/toy_example.png}
\caption{Toy example of a hidden state vector $h$ (shown in blue) and three embedding vectors $e_i$ (shown in red) in $H = 2$ dimensions. The gray vectors represent the embedding update vectors, for the SGD (dark) and the Adam (light) optimizer. The update vector of the true token is aligned with $h$, while the others point in the opposite direction, see Eq.~(\ref{eq:chain_rule_e}). Note that the {\em sum of embedding update vectors} vanishes for SGD, while this is not necessarily the case for Adam, cf.~Eqs.~(\ref{eq:vanishing_updates_SGD}) and (\ref{eq:non_vanishing_updates_Adam}).}
\label{fig:gradients_example}
\end{figure}
This circumstance is referred to in the literature as the "common enemy effect" \cite{bis2021tmic}, and regarded as the cause of the representation degeneration problem. 
However, as we will see in the following sections, this explanation is incomplete, as it does not take into account the scaling of the gradients with the predicted probabilities $p_i$, see Eq.~(\ref{eq:chain_rule_e}). The basis for our argumentation is the observation that the {\em sum of embedding gradients vanishes}, as the following simple calculation shows:
\begin{align}
\sum_{i=1}^V g_i^{(\tm)} 
&\stackrel{(\ref{eq:chain_rule_e})}{=} 
- \sum_{i=1}^V  \left( \delta_{it}^{(\tm)} - p_i^{(\tm)} \right) \cdot h^{(\tm)} \nonumber \\
&= - \left( 1 - \sum_{i=1}^V p_i^{(\tm)} \right) \cdot h^{(\tm)} = 0 
\label{eq:optimizer_momentum_conservation}
\end{align}
Next, we will study how Eq.~(\ref{eq:optimizer_momentum_conservation}) translates to the sum $\sum_{i=1}^V u_i^{(\tm)}$
of embedding update vectors, as well as the mean embedding vector
\begin{equation}
\mu^{(\tm)} = \frac{1}{V} \sum_{i=1}^V e_i^{(\tm)}
\label{eq:mu}
\end{equation}
Since the exact definition of the embedding update vector $u_i$, i.e. the proportionality factor in Eq.~(\ref{eq:update_vector_definition}), depends on the optimization algorithm, we discuss SGD and Adam separately.

\subsection{Invariant Mean Embedding with SGD}

We consider the application of the SGD optimization algorithm on the embedding vectors\footnote{Details are given in App.~\ref{app:sgd_algorithm}.}.
At each training step, an embedding vector is simply updated by adding the associated negative gradient $-g_i$, multiplied by a global learning rate $\eta$. Hence, Eq.~(\ref{eq:update_vector_definition}) becomes
\begin{equation}
u_i^{(\tm)} = - \eta \cdot g_i^{(\tm)}
\label{eq:update_vector_definition_SGD}
\end{equation}
Together with Eq.~(\ref{eq:optimizer_momentum_conservation}), this implies that the {\em sum of embedding update vectors vanishes} at any time step $\tm$:
\begin{equation}
\sum_{i=1}^V  u_i^{(\tm)}
\stackrel{(\ref{eq:update_vector_definition_SGD})}{=} - \eta \sum_{i=1}^V g_i^{(\tm)} 
\stackrel{(\ref{eq:optimizer_momentum_conservation})}{=} 0
\label{eq:vanishing_updates_SGD}
\end{equation}
Consequently, the mean embedding vector will stay invariant during the training process:
\begin{equation}
    \mu^{(\tm)} - \mu^{(\tm-1)} 
    \stackrel{(\ref{eq:mu},\ref{eq:update_general})}{=} 
    \frac{1}{V} \sum_{i=1}^V u_i^{(\tm)} 
    \stackrel{(\ref{eq:vanishing_updates_SGD})}{=} 0
    \label{eq:invariant_mean_embedding_SGD}
\end{equation}
This holds even though the different embeddings $e_i$ will be individually updated in different directions with different magnitudes. 
Moreover, all of the above is true also in the case of SGD with momentum,
which follows from linearity and mathematical induction.
Eq.~(\ref{eq:invariant_mean_embedding_SGD}) has far-reaching implications with regard to the anisotropy problem. It entails that the embedding vectors do not collectively shift away from the origin if SGD (with or without momentum) is used. 

\subsection{Shifted Mean Embedding with Adam}

In this section, we analyze the behavior of the mean embedding during optimization with Adam~\cite{adam}, see Algorithm~\ref{alg:algorithm_adam}.
\input{./algorithms/algorithm_adam.tex}
The update vector~Eq.~(\ref{eq:update_vector_definition}) for the Adam algorithm is given by
\begin{equation}
    u_i^{(\tm)} 
    = 
    - \eta^{(\tm)}_i \cdot \firstmoment \label{eq:update_vector_definition_Adam}  \; ,
\end{equation}
where we have introduced an $i$-dependent effective learning rate 
\begin{equation}
    \eta^{(\tm)}_i
    := 
    \frac{\eta}{\sqrt{\secondmoment} + \epsilon} \label{eq:adam_learning_rate} 
\end{equation}
Note that $\firstmoment$ and $\secondmoment$ denote the exponentially averaged first and second moments, respectively, defined according to lines~\ref{alg:line:adam_first_moment_definition}-\ref{alg:line:adam_second_moment_exp_averages}
in Algorithm~\ref{alg:algorithm_adam}.
The $i$-dependent learning rate serves the purpose of individually normalizing the update vectors for different parameters in the Adam optimizer.
However, it also has an unwanted effect specifically on the embedding vectors. While we know from Eq.~\eqref{eq:optimizer_momentum_conservation} and Algorithm~\ref{alg:algorithm_adam} (lines~\ref{alg:line:adam_first_moment_definition},\ref{alg:line:adam_first_moment_exp_averages}) that the {\em unweighted} sum over the first moments vanishes, 
$\sum_{i=1}^V \firstmoment = 0$,
this is not true for the {\em weighted} sum,
\begin{equation}
\sum_{i=1}^V \eta^{(\tm)}_i \firstmoment \neq 0 \; ,
\label{eq:non_vanishing_weighted_sum_of_first_moments_adam}
\end{equation}
unless $\eta^{(\tm)}_i = \eta^{(\tm)}_j$ for all $i, j \in \V$.
Hence, the \textit{sum of embedding update vectors does not vanish} in general,
\begin{equation}
\sum_{i=1}^V  u_i^{(\tm)}
\stackrel{(\ref{eq:update_vector_definition_Adam})}{=} - \sum_{i=1}^V \eta^{(\tm)}_i \cdot \firstmoment 
\stackrel{(\ref{eq:non_vanishing_weighted_sum_of_first_moments_adam})}{\neq} 0
\label{eq:non_vanishing_updates_Adam}
\end{equation}
This, in turn, causes the mean embedding to change during training,
\begin{equation} 
    \mu^{(\tm)} - \mu^{(\tm-1)} 
    \stackrel{(\ref{eq:mu},\ref{eq:update_general})}{=} 
    \frac{1}{V} \sum_{i=1}^V u_i^{(\tm)}
    \stackrel{(\ref{eq:non_vanishing_updates_Adam})}{\neq} 0 \; ,
    \label{eq:mean_embedding_change_adam}
\end{equation}
which is in stark contrast to the case of SGD (cf.~Eq.~\eqref{eq:invariant_mean_embedding_SGD}).
We have thus identified that an $i$-dependency of the second moment $\secondmoment$ of the Adam optimizer leads to the observed collective shift of the embedding vectors away from the origin.
Next, we will show that the second moment indeed depends on $i$. More concretely, we will argue that its expectation value is proportional to the unigram probabilitity\footnote{Note that from here until Eq.~(\ref{eq:optimizer_update_second_moment_avg_canonical}), the time index ($\tau$) is dropped for the sake of readability.} (see Eq.~(\ref{eq:unigram_probability})),
\begin{equation}
    \E \left[ \secondmomentshort \right] \propto \widetilde p_i 
    \label{eq:second_moment_linear_in_unigram_prob}
\end{equation}
In App.~\ref{app:second_moment_theory}, Eq.~(\ref{eq:second_moment_linear_in_unigram_prob}) is derived using minimal assumptions and experimental input.
Here, we restrict ourselves to confirming the relationship in a purely experimental manner. 
$\E \left[ \secondmomentshort \right]$ is estimated directly by measuring $\secondmomentshort$ multiple times during training, using different models. 
We then perform linear fits of $\E \left[ \secondmomentshort \right]$ as a function of $\widetilde p_i$. 
Indeed, the fits yield a high coefficient of determination, on average $R^2 = 0.85(7)$, and a proportionality constant of 
\begin{equation}
A := \frac{\E \left[ \secondmomentshort \right]}{\widetilde p_i} \approx 10^{-4}
\label{eq:second_moment_proportionality_constant}
\end{equation}
Details about the exact procedure and plots showing the data and linear fits can be found in App.~\ref{app:second_moment_empirical}.
