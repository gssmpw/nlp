\documentclass[10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{phonetic}

\usepackage{kotex}
\usepackage{array}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{lineno}

% \doublespacing % double spaced

\DeclareMathOperator*{\argmax}{arg\,max}

\title{TAPS: Throat and Acoustic Paired Speech Dataset for Deep Learning-Based Speech Enhancement}

\author[1,$\dag$]{Yunsik Kim}
\author[1,$\dag$]{Yonghun Song}
\author[1,2,3,*]{Yoonyoung Chung}
\affil[1]{Department of Electrical Engineering, Pohang University of Science and Technology, 77 Cheongam-ro, Nam-gu, Pohang, Gyeongbuk, 37673, Korea}
\affil[2]{Department of Semiconductor Engineering, Pohang University of Science and Technology, 77 Cheongam-ro, Nam-gu, Pohang, Gyeongbuk, 37673, Korea }
\affil[3]{Center for Semiconductor Technology Convergence, Pohang University of Science and Technology, 77 Cheongam-ro, Nam-gu, Pohang, Gyeongbuk, 37673, Korea}

\affil[*]{Corresponding author(s): Yoonyoung Chung (ychung@postech.ac.kr)}

\affil[$\dag$]{These authors contributed equally to this work}

\begin{abstract}
In high-noise environments such as factories, subways, and busy streets, capturing clear speech is challenging due to background noise. Throat microphones provide a solution with their noise-suppressing properties, reducing the noise while recording speech. However, a significant limitation remains: high-frequency information is attenuated as sound waves pass through skin and tissue, reducing speech clarity. Recent deep learning approaches have shown promise in enhancing throat microphone recordings, but further progress is constrained by the absence of standardized dataset. We introduce a throat and acoustic paired speech dataset (TAPS), a collection of paired utterances recorded from 60 native Korean speakers using throat and acoustic microphones. To demonstrate the TAPS's utility, we tested three baseline deep learning models and identified the mapping-based approach as superior in improving speech quality and restoring content. Additionally, we propose an optimal method to mitigate the signal mismatch between throat and acoustic microphones, ensuring model performance. These results highlight the potential of TAPS to serve as a standardized dataset and advance research in throat microphone-based speech enhancement.
\end{abstract}


\begin{document}
\maketitle

\flushbottom

\section*{Background \& Summary}
In high-noise environments like factories, subway systems, and busy city streets, capturing clear and high-quality voices is challenging due to interference from background noise. To enable effective communication in such noisy environments, throat microphones with noise-suppressing capabilities have been developed. These devices capture speech information transmitted from the vocal cords and vocal tract to the skin surface. Researchers have proposed various throat microphones using piezoelectric\cite{lee2013highly, dagdeviren2014conformable, park2015fingertip}, piezoresistive\cite{kim2016body, park2016dramatically, qiu2015ultrafast}, piezo-capacitive\cite{zang2015flexible, jin2017ultrasensitive, lee2019ultrathin}, triboelectric\cite{fan2015ultrathin, yang2015eardrum, kang2018transparent}, and electromagnetic\cite{zhao2020fully, gao2022comparison, zheng2022dual} materials, as well as commercial accelerometers\cite{hillman2011ambulatory}. These microphones exhibit high sensitivity to voice-related vibration signals and are designed with soft form factors to provide a comfortable fit on curved skin surfaces, making them a strong candidate for upcoming wearable communication devices. However, when a speech signal from the vocal tract transmits through the skin and muscles, it experiences the low-pass effect, resulting in the attenuation of high-frequency components\cite{shin2012survey}. This attenuation results in the loss of timbre and content information embedded in speech, ultimately manifesting as an unacceptably muffled sound\cite{tran2013effect}. Moreover, specific phonemes produced within the oral cavity rather than the vocal cords—such as unvoiced fricatives, plosives, and affricates—are not effectively captured by throat microphones\cite{toda2012statistical}. Additionally, inappropriate placement of the sensor apart from the vocal cord can further degrade sound quality\cite{mcbride2011effect, song2021study}. Therefore, developing effective speech enhancement techniques is crucial when utilizing throat microphones for speech measurement.

During the early studies for improving speech information from body-conducted microphones (BCM), statistical models like linear prediction models\cite{vu2007blind, rahman2017lp} and Gaussian mixture models\cite{nakagiri2006improving, toda2012statistical, turan2015source} were used. These models are based on the source-filter model, which represents speech as an excitation and a spectral envelope filter. The excitation source is assumed to be the same for the speech captured by the BCM and the corresponding acoustic microphone. Consequently, the speech enhancement task is simplified to modifying vocal tract filter characteristics, such as line spectral frequency and Mel cepstrum coefficients. However, mutual independence of the source and the filter is not strictly guaranteed, and low-dimensional spectral envelopes are insufficient to characterize speech well, leading to poor speech enhancement performance.

Recent advances in deep learning have enabled significant progress in body-conducted speech enhancement, as they can model high-dimensional speech features. Researchers have developed enhancement models using deep denoising autoencoders\cite{huang2017wearable, liu2018bone}, bidirectional long short-term memory (BLSTM)\cite{zheng2018novel, gao2022comparison}, and dual-path transformers (DPT)\cite{zheng2022dual}. The enhancement of BCM speech using denoising autoencoders demonstrated increased speech quality and reduced error rates in automatic speech recognition systems. The BLSTM and DPT-based models generate lost speech information through the time dependency of features, resulting in improvements in perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI).

However, there is a lack of a standardized throat microphone speech dataset for training and evaluating these models, leading researchers to create their own datasets for individual experiments\cite{zheng2018novel, zheng2022dual, huang2017wearable, liu2018bone, gao2022comparison}. Standardized datasets are essential for the evaluation of speech enhancement models, serving as benchmark datasets that enable consistent and reliable comparisons of various models' performance\cite{vctk, librivox, Chung18voxceleb2, audioset, demand, freesound}. Although a few BCM datasets have been reported\cite{ESMB, ABCS, Vibravox}, these datasets did not consider the mismatch between recorded signals. Ensuring synchronization is critical when constructing datasets with signals from multiple sensors, as it directly impacts the quality and reliability of the data for deep learning training. Furthermore, to the best of our knowledge, there is no publicly available dataset recorded using a throat microphone placed on the supraglottic area of the neck, a position optimized for capturing high-quality voice signals\cite{song2021study}. Therefore, establishing a throat microphone speech corpus that addresses signal mismatch and ensures high-quality signal acquisition is essential for building robust speech enhancement models.

This paper consists of two parts. In the first part, we propose a standard building pipeline for creating a dataset for training throat microphone speech enhancement (TMSE) models. We developed a system for simultaneously recording speech from throat and acoustic microphones, collecting the throat and acoustic paired speech dataset (TAPS). This dataset is divided into three subsets: \texttt{train}, \texttt{dev}, and \texttt{test}. The \texttt{train} set comprises 10.2 hours of audio, containing 4,000 paired utterances recorded from 40 native Korean speakers. The \texttt{dev} set includes 2.5 hours of audio, containing 1,000 paired utterances recorded from 10 speakers. Here, \texttt{dev} is used to find the optimal parameters of an end-to-end model. The \texttt{test} set consists of 2.6 hours of audio, containing 1,000 paired utterances spoken by 10 speakers.


In the second part of this paper, we report the results of training speech enhancement models on the TAPS, providing baseline performances. The experiments are conducted using three models: TSTNN\cite{wang2021tstnn}, Demucs\cite{defossez2020real}, and SE-conformer\cite{kim21seconformer}. Furthermore, we explore the impact of signal mismatch between the throat microphone and the acoustic microphone on the performance of deep learning models. We introduce a mismatch correction method that enhances model performance by aligning the timing of throat and acoustic microphone signals. The TAPS dataset will facilitate the practical application of throat microphones in extreme noise environments through speech enhancement models. Additionally, by establishing a standardized process for dataset collection, this approach can be extended to a variety of languages, enabling further research and development in the field. 

\section*{Methods}
\subsection*{Speaker information}
The TAPS was constructed with 60 native Korean speakers, as shown in Table \hyperref[table:1]{1}. The mean age of the speakers was 27.1, with a standard deviation of 6.23 years, and none of the speakers had a history of vocal disorders. The \texttt{train} set included a total of 40 speakers, with an equal gender split: 20 women and 20 men. The \texttt{dev} set consisted of 10 speakers, and the test set included another 10 speakers, both of which did not overlap with the speakers in the \texttt{train} set. Each of the \texttt{dev} and \texttt{test} sets was also equally divided by gender: 5 women and 5 men. Most of the speakers spoke in a standard Korean dialect.


\subsection*{Recording hardware configuration}
We developed a custom-built system to simultaneously record speech from two microphones, as shown in Figure \hyperref[fig:1]{1}a and b. This system consisted of a throat microphone, an acoustic microphone, and a microcontroller unit (MCU). The throat microphone used a micro-electromechanical systems (MEMS) accelerometer (TDK, IIM-42652) to capture vibration signals along the vertical direction of the neck’s surface. The accelerometer was configured with an 8 kHz sampling rate, a 16-bit resolution, and a ±4g dynamic range, ensuring the accurate capture of a wide range of signal amplitudes. Additionally, this accelerometer was mounted on an FR-4 printed circuit board with a radius of 9 mm and a thickness of 1.7 mm, forming the throat microphone assembly. This microphone weighed only 760.2 mg and was secured with an adjustable thin strap, adhering comfortably to the neck skin without restricting vocalization or movement. The collected vibration signal was transmitted to the MCU via the serial peripheral interface.

The system also included a MEMS acoustic microphone (CUI Devices, CMM-4030D-261) and an MCU (STMicroelectronics, STM32F301C8T6TR) integrated into the peripheral board. The acoustic microphone operated at a 16 kHz sampling rate and a 24-bit resolution, transmitting the measured signal to the MCU via the integrated interchip sound interface. The delay between the two collected signals at the MCU ranged from 0.75 ms to 0.88 ms, and they were synchronized during the post-processing stage. The paired signals were encoded in HEX format and transmitted to a connected laptop via RS232 communication, enabling real-time recording.

\subsection*{Recording session}
The scripts for the utterances were extracted from the Korean newspaper corpus provided by the National Institute of Korean Language\cite{NIKL2023}. This corpus consists of newspaper articles published in 2023, covering a variety of topics including society, economy, lifestyle, politics, IT/science, beauty/health, sports, culture, and entertainment. Articles containing sentences between 40 to 80 characters were selected. Each speaker was assigned 100 sentences chosen from different articles.

Figure \hyperref[fig:1]{1}c shows the experimental setup for signal recording. Measurements were conducted in a semi-soundproof room at Pohang University of Science and Technology (POSTECH). The throat microphone was positioned on the supraglottic area of the neck to capture vocal cord vibrations along with essential speech formants\cite{song2021study}. An acoustic microphone was placed 30 cm in front of the speaker’s face. Additionally, speakers were instructed to press their forehead against a rest while speaking to maintain a consistent head position throughout the session. The speakers read sentences displayed on a screen and recorded them individually using the provided program. The recorded signals were saved as WAV files on a laptop.

To enhance the recording quality, a nylon filter was positioned between the speaker's mouth and the acoustic microphone to prevent pop noise. Pop noise, also known as plosive noise, occurs when bursts of air from the mouth (typically from sounds like ``p'' or ``b'') hit the microphone directly, creating a low-frequency noise or distortion in the recording. The nylon filter acts as a barrier to dissipate the burst of air before it reaches the microphone, reducing the impact of these sounds. A reflection filter was also used to minimize ambient noise. All measurements were conducted using a DC battery to avoid interference from 60 Hz hum noise from the power line. 

\subsection*{Post processing}
To reduce the influence of gravitational acceleration, we applied a 5th-order Butterworth high-pass filter with a 50 Hz cut-off frequency to the acceleration data. We then addressed the data mismatch caused by timing differences between the acoustic and throat microphone measurements. The potential impact of this correction on the TMSE results is discussed in the technical validation section. To eliminate minor background noise from the acoustic microphone recordings, we used a deep learning-based speech enhancement model, Demucs\cite{defossez2020real}, specifically the pretrained causal version. Figure \hyperref[fig:2]{2} illustrates the waveforms and spectrograms comparison between the original acoustic microphone signal and the noise-reduced signal. Once noise reduction was completed, the silent segments at the beginning and end of each recording were manually trimmed. Each utterance was carefully reviewed to ensure that the recorded speech accurately matched the intended sentence, confirming the correctness of the spoken content. Finally, the throat microphone speech was upsampled to match the sampling rate of the acoustic microphone speech.

\subsection*{Ethical declaration}
All procedures in this study were approved by the Institutional Review Board (IRB) at POSTECH. Additionally, written consent was obtained from each speaker, who was informed that their voice recordings would be shared anonymously.

\section*{Data Records}
We provide TAPS dataset on the Hugging Face Hub\cite{TAPS}. This corpus contains recordings from 60 speakers, each producing 100 utterances, simultaneously captured via a throat microphone and an acoustic microphone. The dataset is split into \texttt{train}, \texttt{dev}, and \texttt{test} sets, with 40 speakers in the \texttt{train} set, 10 in the \texttt{dev} set, and 10 in the \texttt{test} set. Each split maintains a balanced gender distribution.

On the Hugging Face Hub, the dataset is organized as a \texttt{DatasetDict} with three splits: \texttt{train}, \texttt{dev}, and \texttt{test}. Each split consists of a list of data entries, where each entry corresponds to a specific speaker--utterance pair. Every entry contains the following fields:
\begin{itemize}
    \item \textbf{gender} (\texttt{string}): The speaker’s gender, e.g., male/female.
    \item \textbf{speaker\_id} (\texttt{string}): A unique identifier for the speaker, e.g., \textit{p01}.
    \item \textbf{sentence\_id} (\texttt{string}): The utterance index for that speaker, e.g., \textit{u30}.
    \item \textbf{text} (\texttt{string}): The transcribed sentence. Currently provided only for the \texttt{test} set.
    \item \textbf{duration} (\texttt{float32}): The lengths of audio samples.
    \item \textbf{audio.throat\_microphone} (\texttt{Audio}): The throat microphone audio data.
    \item \textbf{audio.acoustic\_microphone} (\texttt{Audio}): The acoustic microphone audio data.
\end{itemize}

Both \textit{audio.throat\_microphone} and \textit{audio.acoustic\_microphone} are stored as Hugging Face \texttt{Audio} features, which facilitate on-the-fly audio decoding and easy integration with other datasets and tools in the Hugging Face.
In particular, each \texttt{Audio} column contains three important fields: 
\begin{itemize}
    \item \textbf{array}: the decoded audio data, represented as a 1-dimensional array. 
    \item \textbf{path}: the file name of the original WAV file.
    \item \textbf{sampling\_rate}: the sampling rate of the audio data.
\end{itemize}

When you load an audio dataset and call the audio column, the \texttt{Audio} feature automatically decodes and resamples the audio file for immediate use. This design simplifies the data loading process and ensures compatibility across different sampling rates.

\section*{Technical Validation}
To validate the effectiveness of the TAPS dataset, we first demonstrate deep learning-based TMSE models. Additionally, we explain the data mismatch caused by the timing differences between acoustic microphone and throat microphone recordings, and explore the method used to correct this mismatch. Finally, we examine how this correction impacts model training.

\subsection*{Objectives}
The acoustic microphone signal \(a[n]\) is composed of voiced speech and fricative/affricate components, which are produced by different physical mechanisms. Mathematically, it can be described by
\begin{align}
    a[n] = \underbrace{h_{\text{vocal\_tract}}[n] \ast g[n]}_{\text{Voiced Speech}} 
       + \underbrace{h_{\text{fricative}}[n] \ast s[n]}_{\text{Fricatives/Affricates}},
\end{align}
where \(g[n]\) is the quasi-periodic glottal source waveform for voiced sounds, and \(h_{\text{vocal\_tract}}[n]\) is a linear filter that models the vocal tract resonances (e.g., formants). The term \(s[n]\) represents turbulent signal generated at constrictions in the vocal tract (e.g., around the teeth or palate), and \(h_{\text{fricative}}[n]\) is a high-pass filter that filters this signal to form fricative or affricate sounds. The first term, \(h_{\text{vocal\_tract}}[n] \ast g[n]\), represents vowels and sonorant consonants, while the second term, \(h_{\text{fricative}}[n] \ast s[n]\), represents high-frequency turbulent signal that is independent of the glottal excitation.

The throat microphone signal \(t[n]\) primarily captures neck tissue vibrations and vocal tract resonances that propagate via mechanical coupling, but it does not contain significant fricative or affricate energy because high-frequency components are significantly attenuated as they propagate through the neck. It can be modeled by  
\begin{align}
    t[n] = \underbrace{h_{\text{tissue}}[n] \ast g[n]}_{\text{Direct Glottal Vibrations}} 
       + \underbrace{h_{\text{coupling}}[n] \ast 
         \bigl(h_{\text{vocal\_tract}}[n] \ast g[n]\bigr)}_{\text{Coupled Vocal Tract Resonances}},
\end{align}
where \(h_{\text{tissue}}[n]\) is a low-pass filter representing propagation through the neck tissue, and \(h_{\text{coupling}}[n]\) is another low-pass filter that models mechanical coupling from the vocal tract into the neck. As a result of this attenuation, \(t[n]\) effectively lacks the component \(h_{\text{fricative}}[n] \ast s[n]\).

The goal is to learn a mapping \(G_\theta: t[n] \rightarrow \hat{a}[n]\) that reconstructs the acoustic microphone signal from the throat microphone signal. Formally,  
\begin{align}
    \hat{a}[n] = G_\theta\bigl(t[n]\bigr) 
            \approx 
            \bigl(h_{\text{vocal\_tract}}[n] \ast g[n]\bigr) 
            + 
            \bigl(h_{\text{fricative}}[n] \ast s[n]\bigr).
\end{align}
The first term can be derived or enhanced from the information present in \(t[n]\), whereas the second term must be generated outright, since \(t[n]\) contains no direct fricative or affricate signal.

Because \(t[n]\) does not directly capture high-frequency turbulence, \(G_\theta\) must learn to generate \(h_{\text{fricative}}[n] \ast s[n]\) through contextual cues. This can be viewed as learning the conditional distribution \(P\bigl(a[n] \mid t[n]\bigr)\). The voiced component \(h_{\text{vocal\_tract}}[n] \ast g[n]\) in \(t[n]\) offers articulatory context (e.g., timing or degree of constriction) that correlates with fricative events. Consequently, the model approximates  
\begin{align}
    P\bigl(h_{\text{fricative}}[n] \ast s[n] 
       \mid 
       t[n]\bigr) 
\propto 
P\bigl(h_{\text{fricative}}[n] \ast s[n] 
       \mid 
       h_{\text{vocal\_tract}}[n] \ast g[n]\bigr).
\end{align}
For example, if \(t[n]\) indicates a vocal tract configuration for a high-front vowel followed by a palatal constriction, \(G_\theta\) generates an “sh”-like turbulent signal in \(\hat{a}[n]\), even though no such signal is present in \(t[n]\).

In order to encourage \(G_\theta\) to produce the desired reconstruction, one can define a general objective function  
\begin{align}
    \mathcal{O}(\theta) 
= 
\mathbb{E}_{t,a} 
  \Bigl[ 
    f\bigl(a[n], G_\theta(t[n])\bigr) 
  \Bigr],
\end{align}
where \(f\) is a suitable measure of how closely the estimated \(\hat{a}[n]\) matches the target \(a[n]\). By optimizing this objective, \(G_\theta\) reconstructs the complete acoustic signal by inferring missing fricative content from low-frequency vocal tract dynamics.

Thus, the TMSE is a conditional generative task: the model estimate the vocal tract resonances from the low-frequency throat microphone signal and generate the missing fricatives using learned articulatory-acoustic mappings. Because high-frequency turbulent energy is absent in \(t[n]\), the model depends on contextual cues to infer the fricative events.

\subsection*{Baselines}
In deep learning-based speech enhancement, two common approaches are masking and mapping\cite{Yuliani2021review}. The masking approach applies a learned mask to the noisy feature representation, selectively retaining important components while suppressing noise\cite{Yuliani2021review}. Mathematically, this is expressed as $S_{clean}=M\odot Y_{noisy}$, where $Y_{noisy}$ is the noisy feature representation, $M$ is the learned mask, and $S_{clean}$ is the resulting clean feature.

The masking approach modifies existing spectrogram components in the input signal by emphasizing or suppressing them. However, in TMSE, the input signal $v$ lacks unvoiced sounds $y$ because these sounds are produced without vocal cord vibrations. Since the masking method cannot generate components that are absent in the input, it cannot reconstruct unvoiced sounds $y$ from input signal $v$. This limitation makes the masking method unsuitable for TMSE, as it cannot produce the absent unvoiced components required for accurate speech reconstruction.

In contrast, the mapping approach directly transforms the noisy feature representation into a clean feature representation using a neural network function\cite{Yuliani2021review}. This can be expressed as $S_{clean}=f_\theta(Y_{noisy})$, where $f_\theta$ represents the learned mapping function. Instead of filtering existing features, the mapping approach predicts clean features based on the noisy input.

The mapping approach uses a neural network to learn the relationship between the input and output signals, allowing it to generate missing components by understanding context and patterns in the data. Therefore, to recover unvoiced sounds in TMSE, the mapping approach is necessary. 

To highlight the differences between the mapping and masking approaches for the TMSE task, we evaluated the performance of TMSE as a baseline model using Demucs\cite{defossez2020real} and SE-conformer\cite{kim21seconformer}, which are mapping-based models, and TSTNN\cite{wang2021tstnn}, a masking-based model.



\textbf{Demucs} is built using a multi-layer convolutional encoder-decoder architecture with U-Net-style skip connections, coupled with a sequence modeling network applied to the encoder's output. The sequence modeling network is implemented using a two-layer BLSTM. Figure \hyperref[fig:3]{3}a illustrates the structure of the Demucs model, defined by key parameters such as the number of layers (L), the initial number of hidden channels (H), kernel size (K), stride (S), and the resampling factor (U). The encoder and decoder layers are indexed from 1 to L, with the decoder layers numbered in reverse order, aligning layers at the same scale. Specifically, the encoder processes raw waveforms and outputs a latent representation. Each layer includes a convolution with kernel size K and stride S, producing $2^{(i-1)}H$ output channels, followed by a ReLU activation. This is succeeded by a ``1x1'' convolution producing $2^iH$ channels, and a GLU activation that reduces the number of channels back to $2^{i-1}H$.

\textbf{SE-conformer} is a speech enhancement model based on the Conformer architecture, combining convolutional neural networks with the Conformer. Similar to Demucs, this model uses a convolutional encoder-decoder structure with skip connections, as shown in Figure \hyperref[fig:3]{3}a. In the original paper, the decoder contains GLU activation followed by ReLU activation and transpose convolution. In our experiment, however, we use 1D transpose convolution followed by ReLU after GLU activation. Additionally, the final layer of the decoder consists of a convolutional layer, GLU, and transpose convolution, which differs from the original. SE-conformer replaces the BLSTM with the Conformer for sequential modeling. Figure \hyperref[fig:3]{3}b shows the architecture of the Conformer. The Conformer consists of N stacked Conformer blocks, each containing key modules: macaron-like feedforward, convolution (ConvBlock), multi-head self-attention (MHSA), and layer normalization. The feedforward module uses a macaron-style arrangement with two linear layers, Swish activation, and dropout. The ConvBlock includes a pointwise convolution with GLU activation, followed by a 1D depth-wise convolution, batch normalization, Swish activation, and another pointwise convolution. MHSA is used without relative positional embedding, and all modules use a prenorm residual unit. The Conformer blocks process the latent representation from the encoder, ending with a Sigmoid activation to smooth output values.

\textbf{TSTNN} is a transformer-based model for end-to-end speech enhancement in the time domain, comprising an encoder, a two-stage transformer module (TSTM), a masking module, and a decoder. Figure \hyperref[fig:3]{3}c shows the overall architecture of the TSTNN. The architecture begins with the segmentation stage which splits the noisy raw audio signal into frames that serve as the input for the encoder. The encoder consists of two convolutional layers, with a dilated dense block inserted between them to enhance feature extraction capabilities. Each convolutional layer is followed by layer normalization and a parametric ReLU (PReLU) activation function. The TSTM consists of four stacked two-stage transformer blocks. As shown in Figure \hyperref[fig:3]{3}d, each block features a local transformer and a global transformer, both of which share the same structure. It contains a multi-head attention module followed by a gated recurrent unit layer, ReLU activation, and a linear layer. Group normalization and layer normalization are applied at the final stages. After the TSTM, the masking module consists of two paths, each involving two-dimensional convolutions and nonlinear operations. The outputs from these two paths are multiplied together and then passed through another convolution and ReLU to create a mask. This mask is element-wise multiplied with the encoder's output, allowing the model to focus on the important features needed for denoising. Finally, the decoder reconstructs the enhanced speech from the masked features. The decoder includes a dilated dense block and a sub-pixel convolution to upsample the feature maps, restoring them to the desired output shape. The use of overlap-add, which is the inverse operation of segmentation, ensures that the final waveform is accurately reconstructed.

\subsection*{Loss function}
In this study, we followed the implementation of the loss functions as described in the original papers\cite{defossez2020real, kim21seconformer, wang2021tstnn}. For Demucs and SE-conformer, we applied the L1 loss directly to the waveform and used a multi-resolution short-time Fourier transform (STFT) loss on the spectrogram, defined as:
\begin{align}
    L_{total}(x,\hat{x})=\frac{1}{T} \|x-\hat{x}\|_1 + \frac{1}{M}\sum_{m=1}^{M}L_{stft}^{(m)}(x,\hat{x}).
\end{align}
Here, $x$ and $\hat{x}$ represent the samples of the speech microphone signal and the throat microphone signal, respectively, and $T$ denotes the number of samples. The variable $M$ indicates the number of different resolution parameter sets used for the STFT.
The multi-resolution STFT loss $L_{\text{stft}}^{(m)}$ is calculated as the sum of STFT losses at each resolution. Each loss consists of a spectral convergence loss $L_{\text{sc}}^{(m)}$ and a magnitude loss $L_{\text{mag}}^{(m)}$:
\begin{align}
    L_{stft}^{(m)}(x,\hat{x}) = L_{sc}^{(m)}(x,\hat{x}) +L_{mag}^{(m)}(x,\hat{x}).
\end{align}
The spectral convergence loss and magnitude loss are defined as follows:
\begin{align}
    L_{sc}^{(m)}(x,\hat{x})=\frac{\||STFT^{(m)}(x)|\|_F-\||STFT^{(m)}(\hat{x})|\|_F}{\||STFT^{(m)}(x)|\|_F},
\end{align}
\begin{align}
    L_{mag}(y,\hat{y})=\frac{1}{N}\|\log |STFT^{(m)}(x)|-\log |STFT^{(m)}(\hat{x})| \|_1.
\end{align}
In these equations, $| \cdot |_F$ and $| \cdot |_1$ denote the Frobenius norm and the L1 norm, respectively, and $N$ is the number of elements in the magnitude of STFT output. The term $| \text{STFT}^{(m)}(\cdot) |$ represents the magnitude of the STFT output using the $m$-th set of parameters. In our experiments, $M$ is set to 3 with different FFT sizes, window sizes, and frame shift parameters such as $\in \{512, 50, 240\}$, $\in \{1024, 120, 600\}$, and $\in \{2048, 240, 1200\}$, respectively.

For TSTNN, we utilized the L2 loss on the waveform in conjunction with a time-frequency domain loss applied to the spectrogram. The total loss function is defined as:
\begin{align}
    L_{total}(x,\hat{x})=\alpha\cdot\frac{1}{T}\|x-\hat{x}\|_2^2+(1-\alpha)L_{TF}(x,\hat{x}).
\end{align}
Here, $\alpha$ represents a tunable parameter. The time-frequency loss $L_{\text{TF}}$ is calculated as:
\begin{align}
    L_{TF}(x,\hat{x})=\frac{1}{N}\left[ \left( \left| \text{Re} \{ \text{STFT}(x) \} \right| + \left| \text{Im} \{ \text{STFT}(x) \} \right| \right) - \left( \left| \text{Re} \{ \text{STFT}(\hat{x}) \} \right| + \left| \text{Im} \{ \text{STFT}(\hat{x}) \} \right| \right) \right].
\end{align}
Here, $N$ denotes the number of elements in the spectrogram. The operators $\text{Re}\{\cdot\}$ and $\text{Im}\{\cdot\}$ represent the real and imaginary parts of the STFT, respectively. We set $\alpha = 0.8$ and used FFT size, window size, and frame shift parameters of $512$, $512$, and $256$, respectively.

\subsection*{Evaluation metrics}
To evaluate the speech quality of the enhanced speech, we employed several objective measures. Perceptual evaluation of speech quality (PESQ)\cite{pesq} was used, utilizing the wide-band version recommended in ITU-T P.862.2, with scores ranging from -0.5 to 4.5. We also used short-time objective intelligibility (STOI)\cite{stoi}, which scores from 0 to 1. Additionally, we considered CSIG\cite{CsigCbakCovl}, a mean opinion score (MOS) prediction of the signal distortion focusing solely on the speech signal; CBAK\cite{CsigCbakCovl}, a MOS prediction of the intrusiveness of background noise; and COVL\cite{CsigCbakCovl}, a MOS prediction of the overall effect. All three scores range from 1 to 5.

We evaluated the speech content restoration quality by assessing whether the enhanced speech preserved linguistic content, especially unvoiced sounds. For this, we transcribed the enhanced speech using a Whisper-large-v3-turbo automatic speech recognition model\cite{radford2022whisper}, fine-tuned\cite{fine_tuned_radford2022whisper} on the Zeroth-Korean dataset \cite{Zeroth_Korean}. We then compared the transcriptions with ground-truth labels to compute the character error rate (CER) and word error rate (WER).

\subsection*{Experiment setup}
In our experiments, we configured the Demucs model with $K=8$, $H=64$, $S=2$, $U=2$, and  $L=5$. The SE-conformer model was set with $K=4$, $H=64$, $S=4$, $U=4$, and $L=4$. For the conformer component in the SE-conformer model, we set the input dimension to $512$, the feedforward network dimension to $64$, the number of attention heads to $4$, the depthwise convolution kernel size to $15$, and the conformer depth to $4$. For TSTNN, we adopted the same implementation as described in the original paper\cite{wang2021tstnn}.

We processed the waveform data by segmenting it with a sliding window of 4 seconds and a step size of 2 seconds, resulting in overlapping 4-second segments. From each segment, we randomly selected a starting point between 0 and 2 seconds and extracted a 2-second segment from that point. These 2-second segments were then used for training.

We trained all three models for 200 epochs using the Adam optimizer with a learning rate of $3 \times 10^{-4}$, momentum parameters $\beta_1 = 0.9$ and $\beta_2 = 0.99$, and a batch size of 16. For each model, we saved the best-performing model based on the \texttt{dev} set. The evaluation was conducted on the \texttt{test} set.

\subsection*{Experimental results}
Table \hyperref[table:2]{2} presents the metric scores for speech quality and content restoration across three baseline models (TSTNN, Demucs, and SE-conformer). Enhancing the throat microphone speech using these models significantly improved the speech quality, with SE-conformer demonstrating the best performance. Although TSTNN, a masking-based approach, showed speech quality metrics similar to Demucs, its speech content restoration quality was lower. This suggests that masking-based approaches are less effective for content restoration.

In Figure \hyperref[fig:4]{4}, we compare spectrogram samples of acoustic microphone speech with outputs from the SE-conformer and TSTNN models. The speech sample includes a unvoiced sound /k/ at the beginning. The SE-conformer model accurately reproduces this part, making it clearly audible. In contrast, the TSTNN model fails to generate the high-frequency information accurately, resulting in a different sound. This is evident in the spectrograms: the SE-conformer output shows pronounced high-frequency components, while these components are weak in the TSTNN output.

To evaluate each model’s ability to restore high-frequency components, Figure \hyperref[fig:5]{5} illustrates the magnitude differences between the models’ mel-spectrogram outputs and those of the acoustic microphone. In the low-frequency region, the TSTNN output exhibits a smaller difference than that of Demucs and is comparable to the SE-conformer. However, above 1.5 kHz, the difference increases compared to the outputs of the SE-conformer and Demucs. This indicates that the TSTNN model lacks effectiveness in generating high-frequency speech components. Such deficiencies in reproducing unvoiced sounds and high-frequency components significantly impact CER and WER, as shown in Table \hyperref[table:2]{2}. 

\subsection*{Data mismatch}
During the post-processing stage, we corrected the timing differences between the throat microphone and the acoustic microphone signals. In this section, we analyze the causes of data mismatches resulting from these timing differences and explore how different mismatch correction methods affect the model's learning capabilities.

Previous studies determined mismatches by calculating the sample shift that maximizes the correlation function\cite{Hauret2023configEBEN}. This mismatch value $D$ is calculated using the cross-correlation function as follows:
\begin{align}
    D=\argmax_k\sum_n S_T(n)\cdot S_A(n+k).
\end{align}
Here, $n$ is the time index, $S_T(\cdot)$ is the throat microphone signal, $S_A(\cdot)$ is the acoustic microphone signal, and $D$ is the sample shift that maximizes the correlation function. $D$ represents the point where the two waveforms exhibit the strongest linear relationship, indicating the optimal temporal alignment between the two signals. Using this method, we investigated how the mismatch varies depending on the vocalization environment.

Figure \hyperref[fig:6]{6} illustrates the three main causes of timing differences between the throat microphone and acoustic microphone signals: (1) the distance between the acoustic microphone and the speaker's lips, (2) variations in speakers' larynx and oral structures, and (3) differences in the phonemes being vocalized.

Firstly, the mismatch increases as the distance between the speaker and the acoustic microphone increases. In Figure \hyperref[fig:7]{7}a, we show the results of calculating the mismatch while varying the distance to the acoustic microphone. 5 male and 5 female speakers each uttered the same sentence 10 times, and the mismatch values were averaged over these repetitions. It can be observed that the mismatch increases linearly with the distance to the acoustic microphone.

Secondly, variations in the structure of the vocal tract among different speakers lead to different timing differences even when they utter the same sentence. Figure \hyperref[fig:7]{7}b shows the mean and standard deviation of the mismatch $D$, calculated as 5 male and 5 female speakers pronounced the same 10 sentences. As in the previous recording session, the speakers' head positions were fixed, and the distance to the acoustic microphone was kept constant at 30 cm. Despite each speaker uttering the same sentences, the mean mismatch varied among them.

Thirdly, the phoneme composition of each sentence influences the mismatch, as certain sounds may resonate differently within the vocal tract and nasal cavity. For example, depending on whether the resonance of the sound mainly occurs near the vocal cords or within the oral or nasal cavity, the time it takes for the main components of the signal to reach each microphone can differ. Additionally, for unvoiced sounds, since vocalization occurs near the lips\cite{Zhang2016acoust}, the signal may arrive late or may not be present at the throat microphone. Figure \hyperref[fig:7]{7}c shows the mean and standard deviation of the mismatch for each sentence, calculated as 3 male and 3 female speakers pronounced 4 sentences 5 times each. It can be seen that the mismatch fluctuates across different sentences.

Figure \hyperref[fig:8]{8} shows the mismatch $D$ calculated in each segment as we slid a fixed window over the throat microphone and acoustic microphone signals. The window sizes used were 0.0625, 0.125, 0.5, and 2 s. When the window size is small, the mismatch varies considerably at each point, indicating that the mismatch is greatly influenced by the phoneme distribution and the presence of silent segments within each window. As the window size becomes sufficiently large, the impact of phoneme distribution changes and silent segments on the mismatch diminishes. Therefore, in the case of very short sentences, significant variance in the mismatch can occur when calculating it.

\subsection*{Mismatch correction}
We considered three main methods to correct such mismatches. The first method involves averaging the mismatches calculated over all sentences and using this average for correction. The second method averages the mismatches for each speaker and corrects accordingly. The third method corrects the mismatches individually for each sentence. To determine the impact of each method on model training, we trained models using datasets without mismatch correction and datasets corrected by each method. Table \ref{table:3} shows the percentage differences in PESQ, STOI and CER trained on datasets corrected by the three methods and an uncorrected dataset. The first method of correcting mismatches by averaging the mismatches computed over all utterances consistently yields better or more balanced improvements in the objective metrics. Specifically, TSTNN and SE-conformer show clear gains in both PESQ and STOI while reducing CER substantially when using the first approach. Based on these findings, we conclude that the first mismatch correction method is generally superior for our purposes. Therefore, we adopted it in our final training procedure to achieve improved speech enhancement performance.



\section*{Usage Notes}
The TAPS dataset offers a valuable resource for researchers developing deep learning-based speech enhancement models for throat microphone applications. By providing paired throat and acoustic microphone recordings from a diverse group of speakers, TAPS addresses the unique challenges of throat microphone data, such as the loss of high-frequency components. Researchers can leverage this dataset to train models capable of enhancing throat microphone recordings, thereby improving speech intelligibility in noisy environments. The dataset also includes baseline performance metrics from established models like Demucs, SE-conformer, and TSTNN, highlighting the strengths of mapping-based approaches for generating unvoiced sounds. Additionally, the dataset introduces standardized methods for mismatch correction, which significantly enhance model accuracy and stability. This resource sets a foundational standard for further exploration and cross-comparative studies in TMSE, providing a pathway for advancements in wearable, noise-resistant communication technologies.

\section*{Code Availability}
The hardware artwork, firmware, and programs used for data collection, as well as the code for the baseline models, are available on our site, http://taps.postech.ac.kr.

\section*{Competing Interests}
The authors declare no competing interests.

%\bibliography{sample}
\begin{thebibliography}{99}
\urlstyle{same} 

\bibitem{lee2013highly}
Lee, J.-H. \textit{et al.} Highly sensitive stretchable transparent piezoelectric nanogenerators. 
\textit{Energy Environ. Sci.} \textbf{6}, 169–175 (2013).

\bibitem{dagdeviren2014conformable}
Dagdeviren, C. \textit{et al.} Conformable amplified lead zirconate titanate sensors with enhanced piezoelectric response for cutaneous pressure monitoring.
\textit{Nat. Commun.} \textbf{5}, 4496 (2014).

\bibitem{park2015fingertip}
Park, J., Kim, M., Lee, Y., Lee, H. S. \& Ko, H. Fingertip skin-inspired microstructured ferroelectric skins discriminate static/dynamic pressure and temperature stimuli.
\textit{Sci. Adv.} \textbf{1}, e1500661 (2015).

\bibitem{kim2016body}
Kim, D. \textit{et al.} Body-attachable and stretchable multisensors integrated with wirelessly rechargeable energy storage devices.
\textit{Adv. Mater.} \textbf{28}, 748–756 (2016).

\bibitem{park2016dramatically}
Park, B. \textit{et al.} Dramatically enhanced mechanosensitivity and signal-to-noise ratio of nanoscale crack-based sensors: effect of crack depth.
\textit{Adv Mater} \textbf{28}, 8130–8137 (2016).

\bibitem{qiu2015ultrafast}
Qiu, L. \textit{et al.} Ultrafast dynamic piezoresistive response of graphene-based cellular elastomers.
\textit{Adv. Mater.} \textbf{28}, 194–200 (2015).

\bibitem{zang2015flexible}
Zang, Y. \textit{et al.} Flexible suspended gate organic thin-film transistors for ultra-sensitive pressure detection.
\textit{Nat. Commun.} \textbf{6}, 6269 (2015).

\bibitem{jin2017ultrasensitive}
Jin, M. L. \textit{et al.} An ultrasensitive, visco-poroelastic artificial mechanotransducer skin inspired by piezo2 protein in mammalian merkel cells.
\textit{Adv. Mater.} \textbf{29}, 1605973 (2017).

\bibitem{lee2019ultrathin}
Lee, S. \textit{et al.} An ultrathin conformable vibration-responsive electronic skin for quantitative vocal recognition.
\textit{Nat. Commun.} \textbf{10}, 2468 (2019).

\bibitem{fan2015ultrathin}
Fan, X. \textit{et al.} Ultrathin, rollable, paper-based triboelectric nanogenerator for acoustic energy harvesting and self-powered sound recording.
\textit{ACS Nano} \textbf{9}, 4236–4243 (2015).

\bibitem{yang2015eardrum}
Yang, J. \textit{et al.} Eardrum-inspired active sensors for self-powered cardiovascular system characterization and throat-attached anti-interference voice recognition.
\textit{Adv. Mater.} \textbf{27}, 1316–1326 (2015).

\bibitem{kang2018transparent}
Kang, S. \textit{et al.} Transparent and conductive nanomembranes with orthogonal silver nanowire arrays for skin-attachable loudspeakers and microphones.
\textit{Sci. Adv.} \textbf{4}, eaas8772 (2018).

\bibitem{zhao2020fully}
Zhao, Y. \textit{et al.} Fully flexible electromagnetic vibration sensors with annular field confinement origami magnetic membranes.
\textit{Adv. Funct. Mater.} \textbf{30}, 2001553 (2020).

\bibitem{gao2022comparison}
Gao, S. \textit{et al.} Comparison of enhancement techniques based on neural networks for attenuated voice signal captured by flexible vibration sensors on throats.
\textit{Nanotechnol. Precis. Eng.} \textbf{5}, 013001 (2022).

\bibitem{zheng2022dual}
Zheng, C. \textit{et al.} Dual-path transformer-based network with equalization-generation components prediction for flexible vibrational sensor speech enhancement in the time domain.
\textit{J. Acoust. Soc. Am.} \textbf{151}, 2814–2825 (2022).

\bibitem{hillman2011ambulatory}
Hillman, R. E. \& Mehta, D. D. Ambulatory monitoring of daily voice use.
\textit{Perspect. Voice Voice Disord.} \textbf{21}, 56–61 (2011).

\bibitem{shin2012survey}
Shin, H. S., Kang, H.-G. \& Fingscheidt, T. Survey of speech enhancement supported by a bone conduction microphone.
\textit{Speech Commun. 10. ITG Symp.}, 1–4 (2012).

\bibitem{tran2013effect}
Tran, P. K., Letowski, T. R. \& McBride, M. E. The effect of bone conduction microphone placement on intensity and spectrum of transmitted speech items.
\textit{J. Acoust. Soc. Am.} \textbf{133}, 3900–3908 (2013).


%\bibitem{berns2016phonological}
%Berns, J. The phonological representation of affricates.
%\textit{Lang. Linguist. Compass} \textbf{10}, 142–156 (2016).

\bibitem{toda2012statistical}
Toda, T., Nakagiri, M. \& Shikano, K. Statistical voice conversion techniques for body-conducted unvoiced speech enhancement.
\textit{IEEE Trans. Audio Speech Lang. Process.} \textbf{20}, 2505–2517 (2012).

\bibitem{mcbride2011effect}
McBride, M., Tran, P., Letowski, T. \& Patrick, R. The effect of bone conduction microphone locations on speech intelligibility and sound quality.
\textit{Appl. Ergon.} \textbf{42}, 495–502 (2011).

\bibitem{song2021study}
Song, Y. \textit{et al.} Study on optimal position and covering pressure of wearable neck microphone for continuous voice monitoring.
\textit{43rd Annu. Int. Conf. IEEE Eng. Med. Biol. Soc.}, 7340–7343 (2021).

\bibitem{vu2007blind}
Vu, T. T., Unoki, M. \& Akagi, M. A blind restoration model for bone-conducted speech based on a linear prediction scheme.
\textit{Int. Symp. Nonlinear Theory Appl.} \textbf{41}, 449–452 (2007).

\bibitem{rahman2017lp}
Rahman, M. A., Shimamura, T. \& Makinae, H. LP-based quality improvement of noisy bone conducted speech.
\textit{IEEJ Trans. Electron. Inf. Syst.} \textbf{137}, 197–198 (2017).

\bibitem{nakagiri2006improving}
Nakagiri, M., Toda, T., Kashioka, H. \& Shikano, K. Improving body transmitted unvoiced speech with statistical voice conversion.
\textit{Interspeech}, 2270–2273 (2006).

\bibitem{turan2015source}
Turan, M. A. T. \& Erzin, E. Source and filter estimation for throat-microphone speech enhancement.
\textit{IEEE/ACM Trans. Audio Speech Lang. Process.} \textbf{24}, 265–275 (2016).

\bibitem{huang2017wearable}
Huang, B., Gong, Y., Sun, J. \& Shen, Y. A wearable bone-conducted speech enhancement system for strong background noises.
\textit{18th Int. Conf. Electron. Packag. Technol.}, 1682–1684 (2017).

\bibitem{liu2018bone}
Liu, H.-P., Tsao, Y. \& Fuh, C.-S. Bone-conducted speech enhancement using deep denoising autoencoder.
\textit{Speech Commun.} \textbf{104}, 106–112 (2018).

\bibitem{zheng2018novel}
Zheng, C., Zhang, X., Sun, M., Yang, J. \& Xing, Y. A novel throat microphone speech enhancement framework based on deep BLSTM recurrent neural networks.
\textit{IEEE 4th Int. Conf. Comput. Commun.}, 1258–1262 (2018).

\bibitem{vctk}
Veaux, C., Yamagishi, J. \& MacDonald, K. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit.
Edinburgh DataShare, \url{https://doi.org/10.7488/ds/1994} (2017).

\bibitem{librivox}
Panayotov, V., Chen, G., Povey, D. \& Khudanpur, S. Librispeech: an ASR corpus based on public domain audio books.
\textit{Proc. IEEE Int. Conf. Acoust. Speech Signal Process.}, 5206–5210 (2015).

\bibitem{Chung18voxceleb2}
Chung, J. S., Nagrani, A. \& Zisserman, A. VoxCeleb2: deep speaker recognition.
\textit{Interspeech}, 1086–1090 (2018).

\bibitem{audioset}
Gemmeke, J. F. \textit{et al.} AudioSet: An ontology and human-labeled dataset for audio events.
\textit{Proc. IEEE Int. Conf. Acoust. Speech Signal Process.}, 776–780 (2017).

\bibitem{demand}
Thiemann, J., Ito, N. \& Vincent, E. The diverse environments multi-channel acoustic noise database (DEMAND): A database of multichannel environmental noise recordings.
\textit{Proc. Mtgs. Acoust.} \textbf{19}, 035081 (2013).

\bibitem{freesound}
Font, F., Roma, G. \& Serra, X. Freesound technical demo.
\textit{Proc. 21st ACM Int. Conf. Multimed.}, 411–412 (2013).

\bibitem{ESMB}
ESMB corpus, \url{https://github.com/elevoctech/ESMB-corpus} (2016).

\bibitem{ABCS}
ABCS corpus, \url{https://github.com/wangmou21/abcs} (2022).

\bibitem{Vibravox}
Vibravox, \url{https://huggingface.co/datasets/Cnam-LMSSC/vibravox} (2024).

\bibitem{wang2021tstnn}
Wang, K., He, B. \& Zhu, W. TSTNN: two-stage transformer-based neural network for speech enhancement in the time domain.
\textit{Proc. IEEE Int. Conf. Acoust. Speech Signal Process.}, 7098–7102 (2021).

\bibitem{defossez2020real}
Defossez, A., Synnaeve, G. \& Adi, Y. Real time speech enhancement in the waveform domain.
\textit{Interspeech}, 3291–3295 (2020).

\bibitem{kim21seconformer}
Kim, E. \& Seo, H. SE-conformer: time-domain speech enhancement using conformer.
\textit{Interspeech}, 2736–2740 (2021).

\bibitem{NIKL2023}
National Institute of Korean Language. NIKL Korean newspaper corpus (transcription) 2023.
\url{https://corpus.korean.go.kr} (2023).

\bibitem{TAPS}
TAPS: Throat and acoustic paired speech dataset, \url{https://huggingface.co/datasets/yskim3271/Throat_and_Acoustic_Pairing_Speech_Dataset} (2025).

\bibitem{Zhang2016acoust}
Zhang, Z. Mechanics of human voice production and control.
\textit{J. Acoust. Soc. Am.} \textbf{140}, 2614–2635 (2016).

\bibitem{Yuliani2021review}
Yuliani, A. R., Amri, M. F., Suryawati, E., Ramdan, A. \& Pardede, H. F. Speech enhancement using deep learning methods: a review.
\textit{J. Elektron. Dan Telekomun.} \textbf{21}, 19–26 (2021).

\bibitem{pesq}
Rix, A. W., Beerends, J. G., Hollier, M. P. \& Hekstra, A. P. Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs.
\textit{Proc. IEEE Int. Conf. Acoust. Speech Signal Process.} \textbf{2}, 749–752 (2001).

\bibitem{stoi}
Taal, C. H., Hendriks, R. C., Heusdens, R. \& Jensen, J. A short-time objective intelligibility measure for time-frequency weighted noisy speech.
\textit{Proc. IEEE Int. Conf. Acoust. Speech Signal Process.}, 4214–4217 (2010).

\bibitem{CsigCbakCovl}
Hu, Y. \& Loizou, P. C. Evaluation of objective quality measures for speech enhancement.
\textit{IEEE Trans. Audio Speech Lang. Process.} \textbf{16}, 229–238 (2008).

\bibitem{Hauret2023configEBEN}
Hauret, J., Joubaud, T., Zimpfer, V. \& Bavu, É. Configurable EBEN: extreme bandwidth extension network to enhance body-conducted speech capture.
\textit{IEEE/ACM Trans. Audio Speech Lang. Process.} \textbf{31}, 3499–3512 (2023).

\bibitem{radford2022whisper}
Radford, A. \textit{et al.} Robust speech recognition via large-scale weak supervision. \textit{Proc. Mach. Learn. Res.}, 28492–28518 (2023).

\bibitem{fine_tuned_radford2022whisper}
Source code for: Fine-tuning Whisper large v3 turbo on zeroth Korean dataset. \textit{Hugging Face} \url{https://huggingface.co/ghost613/whisper-large-v3-turbo-korean} (2024).

\bibitem{Zeroth_Korean}
Zeroth-Korean dataset, \url{https://openslr.org/40/}.

\end{thebibliography}


\section*{Figures \& Tables}

\captionsetup[table]{font={stretch=2}, labelfont={bf}} % 캡션 너비 및 간격 설정

\begin{table}[ht]
\centering
\caption{Summary of dataset characteristics.}
\begin{tabular}{c|ccc}
\toprule
Dataset type & Train & Dev & Test \\
\hline
\begin{tabular}[c]{@{}c@{}}Number of speakers\end{tabular} & 40 & 10 & 10\\
\begin{tabular}[c]{@{}c@{}}Number of male speakers\end{tabular} & 20 & 5 & 5\\
\begin{tabular}[c]{@{}c@{}}Mean / standard deviation of the speakers' age\end{tabular} & 28.5 / 7.3 & 25.6 / 3.0 & 26.2 / 1.4 \\
\begin{tabular}[c]{@{}c@{}}Number of utterances\end{tabular} & 4,000 & 1,000 & 1,000 \\
\begin{tabular}[c]{@{}c@{}}Total length of utterances (h)\end{tabular} & 10.2 & 2.5 & 2.6 \\
\begin{tabular}[c]{@{}c@{}}Max / average / min length of utterances (s)\end{tabular} & 26.3 / 9.1 / 3.2 & 17.9 / 9.0 / 3.3 & 16.6 / 9.3 / 4.2 \\
\bottomrule
\end{tabular}
\centering
\label{table:1}
\end{table}

\begin{table}[ht]
\centering
\caption{Speech enhancement results of baseline models. Speech-to-text was performed using Whisper-large-v3-turbo automatic speech recognition model \cite{radford2022whisper}.}
\begin{tabular}{>{\centering\arraybackslash}p{4.5cm}|ccccc|cc}
\toprule
Source / Model & PESQ & STOI & \begin{tabular}[c]{@{}c@{}}Predicted\\CISG\end{tabular} & \begin{tabular}[c]{@{}c@{}}Predicted\\CBAK\end{tabular} & \begin{tabular}[c]{@{}c@{}}Predicted\\COVL\end{tabular} & CER (\%) & WER (\%) \\
\hline
Acoustic Microphone & - & - & - & - & - & 5.5 & 35.3 \\
Throat Microphone & 1.22 & 0.70 & 1.0 & 1.7 & 1 & 84.4 & 92.2 \\
\hline
TSTNN, 2021 [\citenum{wang2021tstnn}] & 1.904 & 0.881 & 3.175 & 2.529 & 2.528 & 32.0 & 60.3 \\
Demucs, 2020 [\citenum{defossez2020real}] & 1.793 & 0.883 & 3.177 & 2.442 & 2.470 & 28.6 & 57.4 \\
SE-conformer, 2021 [\citenum{kim21seconformer}] & 1.971 & 0.892 & 3.375 & 2.118 & 2.669 & \textbf{24.3} & \textbf{53.0} \\
\bottomrule
\end{tabular}
\centering
\label{table:2}
\end{table}


\begin{table}[ht]
\centering
\caption{Percentage differences in objective speech quality metrics (PESQ, STOI, and CER) across different mismatch correction methods. Higher PESQ and STOI values indicate better quality, while lower CER values are preferable. Improvements are highlighted in bold.}
\begin{tabular}{>{\centering\arraybackslash}c|ccc|ccc|ccc}
\toprule

 & \multicolumn{3}{c|}{ \begin{tabular}[c]{@{}c@{}} Mean mismatch of\\all utterances (\%)\end{tabular}} & \multicolumn{3}{c|}{ \begin{tabular}[c]{@{}c@{}} Mean mismatch of\\each subject (\%)\end{tabular}} & \multicolumn{3}{c}{ \begin{tabular}[c]{@{}c@{}} Mismatch adjustment\\for each utterances (\%)\end{tabular}}  \\
\midrule
\textbf{Model} & \textbf{PESQ} & \textbf{STOI} & \textbf{CER} & \textbf{PESQ} & \textbf{STOI} & \textbf{CER} & \textbf{PESQ} & \textbf{STOI} & \textbf{CER} \\
\midrule

TSTNN, 2021 [\citenum{wang2021tstnn}]           & \textbf{2.33} & \textbf{0.88} & $-\textbf{8.21}$  & $-3.27$ & $-0.55$ & $5.89$ & $-1.45 $ & $-0.25$ & $2.38$ \\
Demucs, 2020 [\citenum{defossez2020real}]       & $-0.03$ & \textbf{0.27} & $-\textbf{1.60}$ & $-1.26$ & $-0.14$ & 1.72 & $-1.00$ & $-0.02$ & $5.30$ \\
SE-conformer, 2021 [\citenum{kim21seconformer}] & \textbf{0.71} & \textbf{0.45} & $-\textbf{5.46}$ & $-2.23$ & $-0.30$ & $5.56$ & $-4.98$ & $-0.91$ & $4.10$ \\ 

\bottomrule
\end{tabular}
\centering
\label{table:3}
\end{table}


\captionsetup[figure]{font={stretch=2}, labelfont={bf}, justification=justified, singlelinecheck=false, width=\textwidth, belowskip=0pt} % 캡션 너비 및 간격 설정

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figure1.PNG}
\caption{\textbf{Experimental setup for simultaneous voice measurement using both throat and acoustic microphones.} (a) Close-up of the throat microphone, which uses an accelerometer to capture vibrations from the neck skin. Scale bar: 10 mm. (b) Close-up of the peripheral board equipped with an acoustic microphone, which transmits the signals recorded by both microphones to a laptop. Scale bar: 10 mm. (c) Photograph of the setup used for voice recordings with 60 speakers in a semi-soundproof room. The throat microphone was attached to the skin approximately 1 cm above the vocal cords, and the acoustic microphone was placed 30 cm in front of the lips. speakers read 100 randomly selected sentences from a Korean newspaper corpus\cite{NIKL2023}.
}
\label{fig:1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figure2.PNG}
\caption{\textbf{Noise reduction achieved using a speech enhancement model\cite{defossez2020real}.} (a) Comparison of waveforms from acoustic microphone signals before and after enhancement. The red line represents the signal before enhancement, and the gray line represents the signal after enhancement. The lower-left graph provides a close-up of a speech segment, while the lower-right graph zooms in on a noise-only segment. The enhancement model effectively reduces background noise while preserving the speech signal. (b) Comparison of spectrograms of acoustic microphone signals before and after enhancement.
}
\label{fig:2}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figure3.PNG}

\caption{\textbf{Block diagram of baseline models.} (a) Architecture of the Demucs and SE-conformer models. The upsampling factors for the Demucs and SE-conformer models are 2 and 4, respectively. The convolution layer parameters are represented as (input channels, output channels, kernel size, stride size). For sequence modeling, the Demucs model employs a 2-layer bi-directional long short-term memory, while the SE-conformer model uses a Conformer. (b) Block diagram of the Conformer architecture. (c) Architecture of the TSTNN model. The parameters for the dilated dense block and dub-pixel convolution are specified as (input size, depth, input channels) and (input channels, output channels, kernel size, upsampling rate), respectively. (d) Block diagram of the two-stage transformer block in TSTNN, consisting of two transformer modules.
}
\label{fig:3}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figure4.png}
\caption{\textbf{Spectrograms of the pronunciation of ``케이티엑스'' are displayed with both Korean notation and International Phonetic Alphabet (IPA) transcription.} The acoustic microphone signal and the outputs from the SE-conformer and TSTNN models. The segments of interest are highlighted in yellow: in both the acoustic microphone and SE-conformer outputs, the segment is correctly identified as ``ke.'' However, in the TSTNN output, the segment appears as ``he,'' indicating an error in accurately producing the unvoiced segment.
}
\label{fig:4}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figure5.png}
\caption{\textbf{Frequency-wise average differences in Mel-spectrogram magnitudes between the original acoustic microphone spectrogram and the outputs from each model.} We calculated the percentage difference at each time $t$ using the formula: $\frac{|y[t]-\hat{y}[t]|}{|y[t]|}\times 100\%$, where $y[t]$ is the original Mel-spectrogram and $\hat{y}[t]$ is the model's output. These differences were then averaged over time and across the entire test set. The results show that SE-conformer and Demucs outperform TSTNN in restoring the original speech in the high-frequency range above 1.5 kHz. This indicates that they effectively preserve the details and clarity of speech in the high-frequency band, enhancing overall speech quality.
}
\label{fig:5}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figure6.PNG}
\caption{\textbf{Factors contributing to timing differences between throat and acoustic microphone signals.} These factors include: (1) the distance between the speaker's lips and the acoustic microphone, (2) variations in the vocal tract due to changes in the speaker's larynx and oral structures, and (3) changes in the shape of the vocal tract and resonance location depending on the phonemes being produced. The graph in the upper-right corner provides a close-up view of the mismatch between the signals from the throat and acoustic microphones.
}
\label{fig:6}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figure7.PNG}
\caption{\textbf{Analysis of data mismatch between throat and acoustic microphone signals due to three factors.} (a) Mismatch as a function of the distance between the speaker's lips and the acoustic microphone, illustrating a linear increase in mismatch with greater distances. (b) Mismatch among different speakers, highlighting variability when producing the same sentence. The red dashed line represents the mean, and the error bars indicate standard deviations. (c) Mismatch based on different sentences spoken by each speaker, with a notable mismatch variation of 1 observed in sentences spoken by Female 3. Error bars represent standard deviations.
}
\label{fig:7}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figure8.PNG}

\caption{\textbf{Analysis of mismatch between throat and acoustic microphone signals based on window size.} (a) Waveforms of simultaneously recorded throat and acoustic microphone signals. (b) Evaluation of data mismatch across various window sizes. The signal is divided into segments using windows of different sizes for mismatch calculation. Smaller window sizes lead to greater fluctuations in mismatch due to silent segments and differences between the signals from the two microphones.
}
\label{fig:8}
\end{figure}

\end{document}

