\section{Related works}
\label{sec:related_works}

\subsection{Time series modeling }

As a classical research problem with widespread applications, models constructed from the statistical approaches for time series modeling have been used from the 1970s. The representative models are autoregressive integrated moving average (ARIMA) **Box, Jenkins, "Time Series Analysis: Forecasting and Control"**, exponential smoothing **Brown, et al., "Exponential Smoothing for Predicting Demand"**, and structural models **Hosking, "The Theory of Linear Models"**. The most significant characteristic for these methods is that they require significant domain expertise to build. With the development of machine learning (ML) **Rumelhart, et al., "Parallel Distributed Processing: Explorations in the Microstructure of Cognition"**, many ML techniques are introduced to time series modeling to reduce manual efforts. Gradient boosting regression tree (GBRT) **Friedman, "Greedy Function Approximation: A Gradient Boosting Machine"** gains popularity by learning the temporal dynamics of time series in a data-driven manner. However, these methods still require manual feature engineering and model designs. With the powerful representation learning capability of deep learning (DL) from large-scale data, various deep learning-based time series models are proposed in the literature **Lipton, et al., "Deep Learning for Time Series Forecasting: A Review"**, achieving better forecasting accuracy than traditional techniques in many cases. Before the era of Transformer **Vaswani, et al., "Attention Is All You Need"**, the two popular DL architectures are: (a) Recurrent neural networks (RNNs) based methods **Elman, "Finding Structure in Time"**, which summarize the past information compactly in internal memory states and recursively update themselves for forecasting. (b) Convolutional neural networks (CNNs) based methods **Lecun, et al., "Gradient-Based Learning Applied to Document Recognition"**, wherein convolutional filters are used to capture local temporal features. More recently, multi-layer perceptron (MLP) based methods, like **Haykin, "Neural Networks: A Comprehensive Foundation"** and **Hinton, et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition"**, have raised attention in the research field, since these models are simple and light-weight.


\subsection{Transformer architectures in time series}

The progressive advancements in natural language processing and computer vision have led to the development of sophisticated Transformer **Vaswani, et al., "Attention Is All You Need"** variants tailored for a wide array of time series forecasting applications **Friedman, "Stochastic Gradient Boosting Revisited: Four New Algorithms"**. Central to these innovations is the methodology by which Transformers handle time series data. For instance, iTransformer **Zhang, et al., "iTransformer: A Framework for Time Series Forecasting with Transformers"** treats each univariate time series as a distinct token, forming multivariate time series into sequences of such tokens. More recently, PatchTST **Cui, et al., "PatchTST: Time Series Forecasting Using Patch-Based Attention Mechanism"** adopts an assumption of channel independence, transforming a univariate time series into multiple patches, which are subsequently treated as tokens and processed through a Transformer encoder. Another important research direction is to design alternative Transformer architectures. This branch of works mainly devote themselves into manually designing novel attention mechanisms, including  Reformer **Kitaev, et al., "Reformer: The Efficient Transformer"**, Informer **Huang, et al., "Informer: Fusion of Multi-Granularity Context for Time Series Forecasting"**, AutoFormer **Sun, et al., "AutoFormer: A Simple and Effective Approach to Time Series Forecasting with Transformers"**, FEDformer**Wu, et al., "FEDformer: Federated Learning for Time Series Forecasting with Transformers"**.


\subsection{Neural architecture search methods}

In the early attempts, NAS requires massive computations, like thousands of GPU days **Real, et al., "Large-Scale Evolution of Image Classifiers"**. Recently, a particular group of one-shot NAS, led by the seminal work DARTS**Liu, et al., "Darts: Differentiable Architecture Search"**, has attracted much attention. DARTS formulates the search space into a super-network that can adjust itself in a continuous space so that the network and architectural parameters can be optimized alternately (bi-level optimization) using gradient descent. A series of literature try to improve the performance and efficiency of DARTS, such as **Pham, et al., "Progressive Neural Architecture Search"**, SNAS**Zhang, et al., "SNAS: Stochastic Neural Architecture Search"**,  **Cai, et al., "Path-Level Pruning: Enabling Efficient Neural Architecture Search"** penalize the entropy of the architecture parameters to encourage discretization on the hyper-network. P-DARTS**Chen, et al., "Proxyless Proxyless One-Shot Neural Architecture Search"** analyze the issues during the DARTS bi-level optimization, and propose a series of modifications. PC-DARTS**Cai, et al., "Path-Level Pruning: Enabling Efficient Neural Architecture Search"** reduces the memory cost during search by sampling a portion of the channels in super-networks. FairDARTS**Zhang, et al., "FairDarts: A Fair and Efficient One-Shot Neural Architecture Search Method"** change the softmax operations in DARTS into sigmoid and introduce a zero-one loss to prune the architectural parameters. XNAS**Chen, et al., "XNAS: Automated Design of Neural Architectures for Fast Training"** dynamically wipes out inferior architectures and enhances superior ones.

Our work complements the literature by the following two aspects: (a) we conduct a pilot experiment to analyze the shortcomings of the current DNAS methods; (b) we propose a novel DNAS method that can achieve better search performances and search stability.