
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkh{\"a}user" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@Article{Abril07,
  author        = "Patricia S. Abril and Robert Plant",
  title         = "The patent holder's dilemma: Buy, sell, or troll?",
  journal       = "Communications of the ACM",
  volume        = "50",
  number        = "1",
  month         = jan,
  year          = "2007",
  pages         = "36--44",
  doi           = "10.1145/1188913.1188915",
  url           = "http://doi.acm.org/10.1145/1219092.1219093",
  note          = "",
}

@Article{Cohen07,
  author        = "Sarah Cohen and Werner Nutt and Yehoshua Sagic",
  title         = "Deciding equivalances among conjunctive aggregate queries",
  journal       = JACM,
  articleno     = 5,
  numpages      = 50,
  volume        = 54,
  number        = 2,
  month         = apr,
  year          = 2007,
  doi           = "10.1145/1219092.1219093",
  url           = "http://doi.acm.org/10.1145/1219092.1219093",
  acmid         = 1219093,
}


@periodical{JCohen96,
  key =          "Cohen",
  editor =       "Jacques Cohen",
  title =        "Special issue: Digital Libraries",
  journal =      CACM,
  volume =       "39",
  number =       "11",
  month =        nov,
  year =         "1996",
}


@Book{Kosiur01,
  author =       "David Kosiur",
  title =        "Understanding Policy-Based Networking",
  publisher =    "Wiley",
  year =         "2001",
  address =      "New York, NY",
  edition =      "2nd.",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  month =        "",
  note =         "",
}


@Book{Harel79,
  author =       "David Harel",
  year =         "1979",
  title =        "First-Order Dynamic Logic",
  series =       "Lecture Notes in Computer Science",
  volume =       "68",
  address =      "New York, NY",
  publisher =    "Springer-Verlag",
  doi =          "10.1007/3-540-09237-4",
  url =          "http://dx.doi.org/10.1007/3-540-09237-4",
  editor =       "",
  number =       "",
  month =        "",
  note =         "",
}


@Inbook{Editor00,
  author =       "",
  editor =       "Ian Editor",
  title =        "The title of book one",
  subtitle =     "The book subtitle",
  series =       "The name of the series one",
  year =         "2007",
  volume =       "9",
  address =      "Chicago",
  edition =      "1st.",
  publisher =    "University of Chicago Press",
  doi =          "10.1007/3-540-09237-4",
  url =          "http://dx.doi.org/10.1007/3-540-09456-9",
  chapter =      "",
  pages =        "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}

%
@InBook{Editor00a,
  author =       "",
  editor =       "Ian Editor",
  title =        "The title of book two",
  subtitle =     "The book subtitle",
  series =       "The name of the series two",
  year =         "2008",
  address =      "Chicago",
  edition =      "2nd.",
  publisher =    "University of Chicago Press",
  doi =          "10.1007/3-540-09237-4",
  url =          "http://dx.doi.org/10.1007/3-540-09456-9",
  volume =       "",
  chapter =      "100",
  pages =        "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}


% incollection (has an editor, title, and possibly a booktitle)
@Incollection{Spector90,
  author =       "Asad Z. Spector",
  title =        "Achieving application requirements",
  booktitle =    "Distributed Systems",
  publisher =    "ACM Press",
  address =      "New York, NY",
  year =         "1990",
  edition =      "2nd.",
  chapter =      "",
  editor =       "Sape Mullender",
  pages =        "19--33",
  doi =          "10.1145/90417.90738",
  url =          "http://doi.acm.org/10.1145/90417.90738",
  volume =       "",
  number =       "",
  series =       "",
  type =         "",
  month =        "",
  note =         "",
}


% incollection (has an editor, title, and possibly a booktitle)
@Incollection{Douglass98,
  author =       "Bruce P. Douglass and David Harel and Mark B. Trakhtenbrot",
  title =        "Statecarts in use: structured analysis and object-orientation",
  series =       "Lecture Notes in Computer Science",
  booktitle =    "Lectures on Embedded Systems",
  publisher =    "Springer-Verlag",
  address =      "London",
  volume =       "1494",
  year =         "1998",
  chapter =      "",
  editor =       "Grzegorz Rozenberg and Frits W. Vaandrager",
  pages =        "368--394",
  doi =          "10.1007/3-540-65193-4_29",
  url =          "http://dx.doi.org/10.1007/3-540-65193-4_29",
  edition =      "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}


@Book{Knuth97,
  author =       "Donald E. Knuth",
  title =        "The Art of Computer Programming, Vol. 1: Fundamental Algorithms (3rd. ed.)",
  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
  year =         "1997",
  address =      "",
  edition =      "",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  month =        "",
  note =         "",
}


@Book{Knuth98,
  author =       "Donald E. Knuth",
  year =         "1998",
  title =        "The Art of Computer Programming",
  series =       "Fundamental Algorithms",
  volume =       "1",
  edition =      "3rd",
  address =      "",
  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
  doi =          "",
  url =          "",
  editor =       "",
  number =       "",
  month =        "",
  note =         "(book)",
}

%Inbook{Knuth97,
%  author =       "Donald E. Knuth",
%  title =        "The Art of Computer Programming",
%  booktitle =    "the booktitle",
%  edition =      "3",
%  volume =       "1",
%  year =         "1997",
%  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
%  editor =       "",
%  number =       "",
%  series =       "Fundamental Algorithms",
%  type =         "",
%  chapter =      "",
%  pages =        "",
%  address =      "",
%  month =        "",
%  note =         "(inbook)",
%}

%INBOOK{DK:73-inbook-full,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (inbook w series)",
%   volume = 1,
%   series = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   edition = "Second",
%   month = "10~" # jan,
%   year = "1973",
%   type = "Section",
%   chapter = "1.2",
%   pages = "10--119",
%   note = "Full INBOOK entry (w series)",
%}

%INcollection{DK:74-incoll,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (incoll)",
%   volume = 1,
%   booktitle = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   month = "10~" # jan,
%   year = "1974",
%   pages = "10--119",
%   editor = "Bernard Rous",
%   note = "This is a full incoll entry with an editor",
%}

%INcollection{DK:75-incollws,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (incoll w series)",
%   volume = 1,
%   booktitle = "The Art of Computer Programming",
%   series = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   month = "10~" # jan,
%   year = "1975",
%   pages = "10--119",
%   editor = "Bernard Rous",
%   note = "This is a full incoll entry with an editor and series",
%}


@incollection{GM05,
Author= "Dan Geiger and Christopher Meek",
Title= "Structured Variational Inference Procedures and their Realizations (as incol)",
Year= 2005,
Booktitle="Proceedings of Tenth International Workshop on Artificial Intelligence and Statistics, {\rm The Barbados}",
Publisher="The Society for Artificial Intelligence and Statistics",
Month= jan,
Editors= "Z. Ghahramani and R. Cowell"
}

@Inproceedings{Smith10,
  author =       "Stan W. Smith",
  title =        "An experiment in bibliographic mark-up: Parsing metadata for XML export",
  booktitle =    "Proceedings of the 3rd. annual workshop on Librarians and Computers",
  series =       "LAC '10",
  editor =       "Reginald N. Smythe and Alexander Noble",
  volume =       "3",
  year =         "2010",
  publisher =    "Paparazzi Press",
  address =      "Milan Italy",
  pages =        "422--431",
  doi =          "99.9999/woot07-S422",
  url =          "http://dx.doi.org/99.0000/woot07-S422",
  number =       "",
  month =        "",
  organization = "",
  note =         "",
}

@Inproceedings{VanGundy07,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2007,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '07",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  articleno =    {Paper 7},
  numpages =     9,
}

@Inproceedings{VanGundy08,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2008,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '08",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  articleno =    7,
  numpages =     2,
  pages =        "99-100",
}

@Inproceedings{VanGundy09,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2009,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '09",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  pages =        "90--100",
}

@Inproceedings{Andler79,
  author =       "Sten Andler",
  title =        "Predicate Path expressions",
  booktitle =    "Proceedings of the 6th. ACM SIGACT-SIGPLAN symposium on Principles of Programming Languages",
  series =       "POPL '79",
  year =         "1979",
  publisher =    "ACM Press",
  address =      "New York, NY",
  pages =        "226--236",
  doi =          "10.1145/567752.567774",
  url =          "http://doi.acm.org/10.1145/567752.567774",
  editor =       "",
  volume =       "",
  number =       "",
  month =        "",
  organization = "",
  note =         "",
}

@Techreport{Harel78,
  author =       "David Harel",
  year =         "1978",
  title =        "LOGICS of Programs: AXIOMATICS and DESCRIPTIVE POWER",
  institution =  "Massachusetts Institute of Technology",
  type =         "MIT Research Lab Technical Report",
  number =       "TR-200",
  address =      "Cambridge, MA",
  month =        "",
  note =         "",
}

@MASTERSTHESIS{anisi03,
author = {David A. Anisi},
title = {Optimal Motion Control of a Ground Vehicle},
school = {Royal Institute of Technology (KTH), Stockholm, Sweden},
intitution = {FOI-R-0961-SE, Swedish Defence Research Agency (FOI)},
year = {2003},
}


@Phdthesis{Clarkson85,
  author =       "Kenneth L. Clarkson",
  year =         "1985",
  title =        "Algorithms for Closest-Point Problems (Computational Geometry)",
  school =       "Stanford University",
  address =      "Palo Alto, CA",
  note =         "UMI Order Number: AAT 8506171",
  type =         "",
  month =        "",
}


@online{Thornburg01,
  author =       "Harry Thornburg",
  year =         "2001",
  title =        "Introduction to Bayesian Statistics",
  url =          "http://ccrma.stanford.edu/~jos/bayes/bayes.html",
  month =        mar,
  lastaccessed = "March 2, 2005",
}


@online{Ablamowicz07,
  author =       "Rafal Ablamowicz and Bertfried Fauser",
  year =         "2007",
  title =        "CLIFFORD: a Maple 11 Package for Clifford Algebra Computations, version 11",
  url =          "http://math.tntech.edu/rafal/cliff11/index.html",
  lastaccessed = "February 28, 2008",
}


@misc{Poker06,
  author =       "Poker-Edge.Com",
  year =         "2006",
  month =        mar,
  title =        "Stats and Analysis",
  lastaccessed = "June 7, 2006",
  url =          "http://www.poker-edge.com/stats.php",
}

@misc{Obama08,
  author        = "Barack Obama",
  year          = "2008",
  title         = "A more perfect union",
  howpublished  = "Video",
  day           = "5",
  url           = "http://video.google.com/videoplay?docid=6528042696351994555",
  month         = mar,
  lastaccessed  = "March 21, 2008",
  note          =  "",
}

@misc{JoeScientist001,
  author =       "Joseph Scientist",
  year =         "2009",
  title =        "The fountain of youth",
  note =         "Patent No. 12345, Filed July 1st., 2008, Issued Aug. 9th., 2009",
  url =          "",
  howpublished = "",
  month =        aug,
  lastaccessed = "",
}


@Inproceedings{Novak03,
  author =       "Dave Novak",
  title =        "Solder man",
  booktitle =    "ACM SIGGRAPH 2003 Video Review on Animation theater Program: Part I - Vol. 145 (July 27--27, 2003)",
  year =         "2003",
  publisher =    "ACM Press",
  address =      "New York, NY",
  pages =        "4",
  month =        "March 21, 2008",
  doi =          "99.9999/woot07-S422",
  url =          "http://video.google.com/videoplay?docid=6528042696351994555",
  note =         "",
  howpublished = "Video",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  organization = "",
  distinctURL = 1
}


@article{Lee05,
  author =       "Newton Lee",
  year =         "2005",
  title =        "Interview with Bill Kinder: January 13, 2005",
  journal =      "Comput. Entertain.",
  eid =          "4",
  volume =       "3",
  number =       "1",
  month =        "Jan.-March",
  doi =          "10.1145/1057270.1057278",
  url =          "http://doi.acm.org/10.1145/1057270.1057278",
  howpublished = "Video",
  note =         "",
}

@article{rous08,
  author =       "Bernard Rous",
  year =         "2008",
  title =        "The Enabling of Digital Libraries",
  journal =      "Digital Libraries",
  volume =       "12",
  number =       "3",
  month =        jul,
  articleno =    "Article~5",
  doi =          "",
  url =          "",
  howpublished = "",
  note =         "To appear",
}

@article{384253,
 author = {Werneck,, Renato and Setubal,, Jo\~{a}o and da Conceic\~{a}o,, Arlindo},
 title = {(old) Finding minimum congestion spanning trees},
 journal = {J. Exp. Algorithmics},
 volume = {5},
 year = {2000},
 issn = {1084-6654},
 pages = {11},
 doi = {http://doi.acm.org/10.1145/351827.384253},
 publisher = {ACM},
 address = {New York, NY, USA},
 }


@article{Werneck:2000:FMC:351827.384253,
 author = {Werneck, Renato and Setubal, Jo\~{a}o and da Conceic\~{a}o, Arlindo},
 title = {(new) Finding minimum congestion spanning trees},
 journal = {J. Exp. Algorithmics},
 volume = 5,
 month = dec,
 year = 2000,
 issn = {1084-6654},
 articleno = 11,
 url = {http://portal.acm.org/citation.cfm?id=351827.384253},
 doi = {10.1145/351827.384253},
 acmid = 384253,
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{1555162,
 author = {Conti, Mauro and Di Pietro, Roberto and Mancini, Luigi V. and Mei, Alessandro},
 title = {(old) Distributed data source verification in wireless sensor networks},
 journal = {Inf. Fusion},
 volume = {10},
 number = {4},
 year = {2009},
 issn = {1566-2535},
 pages = {342--353},
 doi = {http://dx.doi.org/10.1016/j.inffus.2009.01.002},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 }

@article{Conti:2009:DDS:1555009.1555162,
 author = {Conti, Mauro and Di Pietro, Roberto and Mancini, Luigi V. and Mei, Alessandro},
 title = {(new) Distributed data source verification in wireless sensor networks},
 journal = {Inf. Fusion},
 volume = {10},
 number = {4},
 month = oct,
 year = {2009},
 issn = {1566-2535},
 pages = {342--353},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=1555009.1555162},
 doi = {10.1016/j.inffus.2009.01.002},
 acmid = {1555162},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Clone detection, Distributed protocol, Securing data fusion, Wireless sensor networks},
}

@inproceedings{Li:2008:PUC:1358628.1358946,
 author = {Li, Cheng-Lun and Buyuktur, Ayse G. and Hutchful, David K. and Sant, Natasha B. and Nainwal, Satyendra K.},
 title = {Portalis: using competitive online interactions to support aid initiatives for the homeless},
 booktitle = {CHI '08 extended abstracts on Human factors in computing systems},
 year = {2008},
 isbn = {978-1-60558-012-X},
 location = {Florence, Italy},
 pages = {3873--3878},
 numpages = {6},
 url = {http://portal.acm.org/citation.cfm?id=1358628.1358946},
 doi = {10.1145/1358628.1358946},
 acmid = {1358946},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cscw, distributed knowledge acquisition, incentive design, online games, recommender systems, reputation systems, user studies, virtual community},
}

@book{Hollis:1999:VBD:519964,
 author = {Hollis, Billy S.},
 title = {Visual Basic 6: Design, Specification, and Objects with Other},
 year = {1999},
 isbn = {0130850845},
 edition = {1st},
 publisher = {Prentice Hall PTR},
 address = {Upper Saddle River, NJ, USA},
 }


@book{Goossens:1999:LWC:553897,
 author = {Goossens, Michel and Rahtz, S. P. and Moore, Ross and Sutor, Robert S.},
 title = {The  Latex Web Companion: Integrating TEX, HTML, and XML},
 year = {1999},
 isbn = {0201433117},
 edition = {1st},
 publisher = {Addison-Wesley Longman Publishing Co., Inc.},
 address = {Boston, MA, USA},
 }

% need to test genres for errant isbn output

% techreport
@techreport{897367,
 author = {Buss, Jonathan F. and Rosenberg, Arnold L. and Knott, Judson D.},
 title = {Vertex Types in Book-Embeddings},
 year = {1987},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aumass_cs%3Ancstrl.umassa_cs%2F%2FUM-CS-1987-018},
 publisher = {University of Massachusetts},
 address = {Amherst, MA, USA},
 }

@techreport{Buss:1987:VTB:897367,
 author = {Buss, Jonathan F. and Rosenberg, Arnold L. and Knott, Judson D.},
 title = {Vertex Types in Book-Embeddings},
 year = {1987},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aumass_cs%3Ancstrl.umassa_cs%2F%2FUM-CS-1987-018},
 publisher = {University of Massachusetts},
 address = {Amherst, MA, USA},
 }

% whole proceedings

@proceedings{Czerwinski:2008:1358628,
 author = {},
 note = {General Chair-Czerwinski, Mary and General Chair-Lund, Arnie and Program Chair-Tan, Desney},
 title = {CHI '08: CHI '08 extended abstracts on Human factors in computing systems},
 year = {2008},
 isbn = {978-1-60558-012-X},
 location = {Florence, Italy},
 order_no = {608085},
 publisher = {ACM},
 address = {New York, NY, USA},
 }

% phdthesis

@phdthesis{Clarkson:1985:ACP:911891,
 author = {Clarkson, Kenneth Lee},
 advisor = {Yao, Andrew C.},
 title = {Algorithms for Closest-Point Problems (Computational Geometry)},
 year = {1985},
 note = {AAT 8506171},
 school = {Stanford University},
 address = {Stanford, CA, USA},
 }
% school is being picked up -- but not publisher (which is OK)
% Also -- the title is NOT being output in italics !!! Arrrrgh! - I fixed it. :-)


%%% compare with 'old'
%%% atsign-Phdthesis{Clarkson85,
%%%  author =       "Kenneth L. Clarkson",
%%%  year =         "1985",
%%%  title =        "Algorithms for Closest-Point Problems (Computational Geometry)",
%%%  school =       "Stanford University",
%%%  address =      "Palo Alto, CA",
%%%  note =         "UMI Order Number: AAT 8506171",
%%%  type =         "",
%%%  month =        "",
%%%}

% A bibliography
@Article{1984:1040142,
 key = {{$\!\!$}},
 journal = {SIGCOMM Comput. Commun. Rev.},
 year = {1984},
 issn = {0146-4833},
 volume = {13-14},
 number = {5-1},
 issue_date = {January/April 1984},
 publisher = {ACM},
 address = {New York, NY, USA},
 }


% grinder
@inproceedings{2004:ITE:1009386.1010128,
 key = {IEEE},
 title = {IEEE TCSC Executive Committee},
 booktitle = {Proceedings of the IEEE International Conference on Web Services},
 series = {ICWS '04},
 year = {2004},
 isbn = {0-7695-2167-3},
 pages = {21--22},
 url = {http://dx.doi.org/10.1109/ICWS.2004.64},
 doi = {http://dx.doi.org/10.1109/ICWS.2004.64},
 acmid = {1010128},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

% div book
@book{Mullender:1993:DS:302430,
 editor = {Mullender, Sape},
 title = {Distributed systems (2nd Ed.)},
 year = {1993},
 isbn = {0-201-62427-3},
 publisher = {ACM Press/Addison-Wesley Publishing Co.},
 address = {New York, NY, USA},
 }

% master thesis (as techreport and thesis)

@techreport{Petrie:1986:NAD:899644,
 author = {Petrie, Charles J.},
 title = {New Algorithms for Dependency-Directed Backtracking (Master's thesis)},
 year = {1986},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autexas_cs%3AUTEXAS_CS%2F%2FAI86-33},
 publisher = {University of Texas at Austin},
 address = {Austin, TX, USA},
 }

@MASTERSTHESIS{Petrie:1986:NAD:12345,
 author = {Petrie, Charles J.},
 title = {New Algorithms for Dependency-Directed Backtracking (Master's thesis)},
 year = {1986},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autexas_cs%3AUTEXAS_CS%2F%2FAI86-33},
 school = {University of Texas at Austin},
 address = {Austin, TX, USA},
 }




@BOOK{book-minimal,
   author = "Donald E. Knuth",
   title = "Seminumerical Algorithms",
   publisher = "Addison-Wesley",
   year = "1981",
}

% incollection (has an editor, title, and possibly a booktitle)
@INcollection{KA:2001,
 author = {Kong, Wei-Chang},
 Title = {The implementation of electronic commerce in SMEs in Singapore (as Incoll)},
 booktitle = {E-commerce and cultural values},
 year = {2001},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}


% with bibfield 'type' before chapter (note no editor)
@INBOOK{KAGM:2001,
 author = {Kong, Wei-Chang},
 type = {Name of Chapter:},
 chapter = {The implementation of electronic commerce in SMEs in Singapore (Inbook-w-chap-w-type)},
 title = {E-commerce and cultural values},
 year = {2001},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}

%%% Notes! This is because the atsign-INBOOK citation type specifies EITHER
%%% editor or author, but not both. In my experiments with the harvard/dcu
%%% bibtex style (and presumably this applies to other styles too), bibtex
%%% ignores the editor information if author information exists in an
%%% atsign-INBOOK entry. atsign-INCOLLECTION is far more commonly used in my references,
%%% and in the absence of an editor I believe most bibtex styles will just
%%% ommit the editor from the reference - the chapter information will not
%%% end up in the in-text citation as you suggest it should be but at least
%%% there is a place to put the editor if necessary.



% was 'Inbook' -- changed to incollection - (editor is different to author) - need to tell Asad to codify as such.
@incollection{Kong:2002:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {Chapter 9},
  booktitle =   {E-commerce and cultural values (Incoll-w-text (chap 9) 'title')},
  year =        {2002},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  type =        "",
  month =       "",
  note =        "",
}

% incol when the chapter is 'text' - due to presence of editor (different to author)
@incollection{Kong:2003:IEC:887006.887011,
 author = {Kong, Wei-Chang},
 title = {The implementation of electronic commerce in SMEs in Singapore (Incoll)},
 booktitle = {E-commerce and cultural values},
 editor = {Thanasankit, Theerasak},
 year = {2003},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}

% ------ test
%incollection{Kong:2003:IEC:887006.887010,
% author = {Kong, Wei-Chang},
% chapter = {The implementation of electronic commerce in SMEs in Singapore (Incoll-text-in-chap)},
% booktitle = {booktitle E-commerce and cultural values},
% title =   {The title},
% editor = {Thanasankit, Theerasak},
% year = {2003},
% isbn = {1-59140-056-2},
% pages = {51--74},
% numpages = {24},
% url = {http://portal.acm.org/citation.cfm?id=887006.887010},
% acmid = {887010},
% publisher = {IGI Publishing},
% address = {Hershey, PA, USA},
%}


% ---------





% Need inbook with num in chapter

% and inbook with number in chapter
@InBook{Kong:2004:IEC:123456.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values - (InBook-num-in-chap)},
  chapter =     {9},
  year =        {2004},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  type =        "",
  month =       "",
  note =        "",
}


% and inbook with text in chapter
@Inbook{Kong:2005:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values (Inbook-text-in-chap)},
  chapter =     {The implementation of electronic commerce in SMEs in Singapore},
  year =        {2005},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  type =        {Chapter:},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  month =       "",
  note =        "",
}


% and inbook with a num and type field
@Inbook{Kong:2006:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values (Inbook-num chap)},
  chapter =     {22},
  year =        {2006},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  type =        {Chapter (in type field)},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  month =       "",
  note =        "",
}


% and incol coz we have a BLANK chapter - due to presence of editor
%atIncollection{Kong:2006:IEC:887006.887011,
%  author =     {Kong, Wei-Chang},
%  editor =     {Theerasak Thanasankit},
%  title =      "The title"
%  booktitle =  {E-commerce and cultural values (Incol-coz-blank-chap)},
%  year =       {2006},
%  address =    {Hershey, PA, USA},
%  publisher =  {IGI Publishing},
%  url =        {http://portal.acm.org/citation.cfm?id=887006.887010},
%  type =       {Type!},
%  chapter =    {},
%  pages =      {51--74},
%  numpages =   {24},
%  acmid =      {887010},
%  isbn =       {1-59140-056-2},
%  number =     "",
%  month =      "",
%  note =       "",
%}

@article{SaeediMEJ10,
            author = {Mehdi Saeedi and Morteza Saheb Zamani and Mehdi Sedighi},
            title = {A library-based synthesis methodology for reversible logic},
            journal = {Microelectron. J.},
            volume = {41},
            number = {4},
            month = apr,
            year = {2010},
            pages = {185--194},
}

@ARTICLE{SaeediJETC10,
            author = {Mehdi Saeedi and Morteza Saheb Zamani and Mehdi Sedighi and Zahra Sasanian},
            title = {Synthesis of Reversible Circuit Using Cycle-Based Approach},
            journal = {J. Emerg. Technol. Comput. Syst.},
            volume = {6},
            number = {4},
            month = dec,
            year = {2010}
            }

% Asad's new version
@article{Kirschmer:2010:AEI:1958016.1958018,
 author = {Kirschmer, Markus and Voight, John},
 title = {Algorithmic Enumeration of Ideal Classes for Quaternion Orders},
 journal = {SIAM J. Comput.},
 issue_date = {January 2010},
 volume = {39},
 number = {5},
 month = jan,
 year = {2010},
 issn = {0097-5397},
 pages = {1714--1747},
 numpages = {34},
 url = {http://dx.doi.org/10.1137/080734467},
 doi = {https://doi.org/10.1137/080734467},
 acmid = {1958018},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
 keywords = {ideal classes, maximal orders, number theory, quaternion algebras},
}


% incol due to presence of booktitle
@incollection{Hoare:1972:CIN:1243380.1243382,
 author = {Hoare, C. A. R.},
 title = {Chapter II: Notes on data structuring},
 booktitle = {Structured programming (incoll)},
 editor = {Dahl, O. J. and Dijkstra, E. W. and Hoare, C. A. R.},
 year = {1972},
 isbn = {0-12-200550-3},
 pages = {83--174},
 numpages = {92},
 url = {http://portal.acm.org/citation.cfm?id=1243380.1243382},
 acmid = {1243382},
 publisher = {Academic Press Ltd.},
 address = {London, UK, UK},
}

% incol due to presence of booktitle
@incollection{Lee:1978:TQA:800025.1198348,
 author = {Lee, Jan},
 title = {Transcript of question and answer session},
 booktitle = {History of programming languages I (incoll)},
 editor = {Wexelblat, Richard L.},
 year = {1981},
 isbn = {0-12-745040-8},
 pages = {68--71},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800025.1198348},
 doi = {http://doi.acm.org/10.1145/800025.1198348},
 acmid = {1198348},
 publisher = {ACM},
 address = {New York, NY, USA},
}

% incol due to booktitle
@incollection{Dijkstra:1979:GSC:1241515.1241518,
 author = {Dijkstra, E.},
 title = {Go to statement considered harmful},
 booktitle = {Classics in software engineering (incoll)},
 year = {1979},
 isbn = {0-917072-14-6},
 pages = {27--33},
 numpages = {7},
 url = {http://portal.acm.org/citation.cfm?id=1241515.1241518},
 acmid = {1241518},
 publisher = {Yourdon Press},
 address = {Upper Saddle River, NJ, USA},
}

% incol due to booktitle
@incollection{Wenzel:1992:TVA:146022.146089,
 author = {Wenzel, Elizabeth M.},
 title = {Three-dimensional virtual acoustic displays},
 booktitle = {Multimedia interface design (incoll)},
 year = {1992},
 isbn = {0-201-54981-6},
 pages = {257--288},
 numpages = {32},
 url = {http://portal.acm.org/citation.cfm?id=146022.146089},
 doi = {10.1145/146022.146089},
 acmid = {146089},
 publisher = {ACM},
 address = {New York, NY, USA},
}

% incol due to booktitle
@incollection{Mumford:1987:MES:54905.54911,
 author = {Mumford, E.},
 title = {Managerial expert systems and organizational change: some critical research issues},
 booktitle = {Critical issues in information systems research (incoll)},
 year = {1987},
 isbn = {0-471-91281-6},
 pages = {135--155},
 numpages = {21},
 url = {http://portal.acm.org/citation.cfm?id=54905.54911},
 acmid = {54911},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
}

@book{McCracken:1990:SSC:575315,
 author = {McCracken, Daniel D. and Golden, Donald G.},
 title = {Simplified Structured COBOL with Microsoft/MicroFocus COBOL},
 year = {1990},
 isbn = {0471514071},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
}

% Let's include Boris / BBeeton entries  (multi-volume works)

@book {MR781537,
    AUTHOR = {H{\"o}rmander, Lars},
     TITLE = {The analysis of linear partial differential operators. {III}},
    SERIES = {Grundlehren der Mathematischen Wissenschaften [Fundamental
              Principles of Mathematical Sciences]},
    VOLUME = {275},
      NOTE = {Pseudodifferential operators},
PUBLISHER = {Springer-Verlag},
   ADDRESS = {Berlin, Germany},
      YEAR = {1985},
     PAGES = {viii+525},
      ISBN = {3-540-13828-5},
   MRCLASS = {35-02 (35Sxx 47G05 58G15)},
  MRNUMBER = {781536 (87d:35002a)},
MRREVIEWER = {Min You Qi},
}

@book {MR781536,
    AUTHOR = {H{\"o}rmander, Lars},
     TITLE = {The analysis of linear partial differential operators. {IV}},
    SERIES = {Grundlehren der Mathematischen Wissenschaften [Fundamental
              Principles of Mathematical Sciences]},
    VOLUME = {275},
      NOTE = {Fourier integral operators},
PUBLISHER = {Springer-Verlag},
   ADDRESS = {Berlin, Germany},
      YEAR = {1985},
     PAGES = {vii+352},
      ISBN = {3-540-13829-3},
   MRCLASS = {35-02 (35Sxx 47G05 58G15)},
  MRNUMBER = {781537 (87d:35002b)},
MRREVIEWER = {Min You Qi},
}

%%%%%%%%%%%%%%%%%%%%%% Start of Aptara sample bib entries

% acmsmall-sam.bib
@InProceedings{Adya-01,
  author        = {A. Adya and P. Bahl and J. Padhye and A.Wolman and L. Zhou},
  title         = {A multi-radio unification protocol for {IEEE} 802.11 wireless networks},
  booktitle     = {Proceedings of the IEEE 1st International Conference on Broadnets Networks (BroadNets'04)},
  publisher     = "IEEE",
  address       = "Los Alamitos, CA",
  year          = {2004},
  pages         = "210--217"
}

@article{Akyildiz-01,
  author        = {I. F. Akyildiz and W. Su and Y. Sankarasubramaniam and E. Cayirci},
  title         = {Wireless Sensor Networks: A Survey},
  journal       = {Comm. ACM},
  volume        = 38,
  number        = "4",
  year          = {2002},
  pages         = "393--422"
}

@article{Akyildiz-02,
  author        = {I. F. Akyildiz and T. Melodia and K. R. Chowdhury},
  title         = {A Survey on Wireless Multimedia Sensor Networks},
  journal       = {Computer Netw.},
  volume        = 51,
  number        = "4",
  year          = {2007},
  pages         = "921--960"
}

@InProceedings{Bahl-02,
  author        = {P. Bahl and R. Chancre and J. Dungeon},
  title         = {{SSCH}: Slotted Seeded Channel Hopping for Capacity Improvement in {IEEE} 802.11 Ad-Hoc Wireless Networks},
  booktitle     = {Proceeding of the 10th International Conference on Mobile Computing and Networking (MobiCom'04)},
  publisher     = "ACM",
  address       = "New York, NY",
  year          = {2004},
  pages         = "112--117"
}

@misc{CROSSBOW,
  key       = {CROSSBOW},
  title     = {{XBOW} Sensor Motes Specifications},
  note      = {http://www.xbow.com},
  year      = 2008
}

@article{Culler-01,
  author        = {D. Culler and D. Estrin and M. Srivastava},
  title         = {Overview of Sensor Networks},
  journal       = {IEEE Comput.},
  volume        = 37,
  number        = "8 (Special Issue on Sensor Networks)",
  publisher     = "IEEE",
  address       = "Los Alamitos, CA",
  year          = {2004},
  pages         = "41--49"
}

@misc{Harvard-01,
    key         = {Harvard CodeBlue},
    title       = {{CodeBlue}: Sensor Networks for Medical Care},
    note        = {http://www.eecs.harvard.edu/mdw/ proj/codeblue/},
    year        = 2008
}

@InProceedings{Natarajan-01,
    author      = {A. Natarajan and M. Motani and B. de Silva and K. Yap and K. C. Chua},
    title       = {Investigating Network Architectures for Body Sensor Networks},
    booktitle   = {Network Architectures},
    editor      = {G. Whitcomb and P. Neece},
    publisher   = "Keleuven Press",
    address     = "Dayton, OH",
    year        = {2007},
    pages       = "322--328",
    eprint      = "960935712",
    primaryclass = "cs",
}

@techreport{Tzamaloukas-01,
  author        = {A. Tzamaloukas and J. J. Garcia-Luna-Aceves},
  title         = {Channel-Hopping Multiple Access},
  number =        {I-CA2301},
  institution =   {Department of Computer Science, University of California},
  address =       {Berkeley, CA},
  year          = {2000}
}

@BOOK{Zhou-06,
  author        = {G. Zhou and J. Lu and C.-Y. Wan and M. D. Yarvis and J. A. Stankovic},
  title         = {Body Sensor Networks},
  publisher     = "MIT Press",
  address       = "Cambridge, MA",
  year          = {2008}
}

@mastersthesis{ko94,
author = "Jacob Kornerup",
title = "Mapping Powerlists onto Hypercubes",
school = "The University of Texas at Austin",
note = "(In preparation)",
year = "1994"}
%month = "dec",}

@PhdThesis{gerndt:89,
  author =       "Michael Gerndt",
  title =        "Automatic Parallelization for Distributed-Memory
                  Multiprocessing Systems",
  school =       "University of Bonn",
  year =         1989,
  address =      "Bonn, Germany",
  month =        dec
}

@article{6:1:1,
author = "J. E. {Archer, Jr.} and R. Conway and F. B. Schneider",
title = "User recovery and reversal in interactive systems",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "6",
number = "1",
month = jan,
year = 1984,
pages = "1--19"}

@article{7:1:137,
author = "D. D. Dunlop and V. R. Basili",
title = "Generalizing specifications for uniformly implemented loops",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "1",
month = jan,
year = 1985,
pages = "137--158"}

@article{7:2:183,
author = "J. Heering and P. Klint",
title = "Towards monolingual programming environments",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "2",
month = apr,
year = 1985,
pages = "183--213"}

@book{knuth:texbook,
author = "Donald E. Knuth",
title = "The {\TeX{}book}",
publisher = "Addison-Wesley",
address = "Reading, MA.",
year = 1984}

@article{6:3:380,
author = "E. Korach and D.  Rotem and N. Santoro",
title = "Distributed algorithms for finding centers and medians in networks",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "6",
number = "3",
month = jul,
year = 1984,
pages = "380--401"}

@book{Lamport:LaTeX,
author = "Leslie Lamport",
title = "\it {\LaTeX}: A Document Preparation System",
publisher = "Addison-Wesley",
address = "Reading, MA.",
year = 1986}

@article{7:3:359,
author = "F. Nielson",
title = "Program transformations in a denotational setting",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "3",
month = jul,
year = 1985,
pages = "359--379"}

%testing
@BOOK{test,
   author = "Donald E. Knuth",
   title = "Seminumerical Algorithms",
   volume = 2,
   series = "The Art of Computer Programming",
   publisher = "Addison-Wesley",
   address = "Reading, MA",
   edition = "2nd",
   month = "10~" # jan,
   year = "1981",
}

@inproceedings{reid:scribe,
author = "Brian K. Reid",
title = "A high-level approach to computer document formatting",
booktitle = "Proceedings of the 7th Annual Symposium on Principles of
  Programming Languages",
month = jan,
year = 1980,
publisher = "ACM",
address = "New York",
pages = "24--31"}

@article{Zhou:2010:MMS:1721695.1721705,
 author = {Zhou, Gang and Wu, Yafeng and Yan, Ting and He, Tian and Huang, Chengdu and Stankovic, John A. and Abdelzaher, Tarek F.},
 title = {A multifrequency MAC specially designed for wireless sensor network applications},
 journal = {ACM Trans. Embed. Comput. Syst.},
 issue_date = {March 2010},
 volume = 9,
 number = 4,
 month = {April},
 year = 2010,
 issn = {1539-9087},
 pages = {39:1--39:41},
 articleno = 39,
 numpages = 41,
 url = {http://doi.acm.org/10.1145/1721695.1721705},
 doi = {10.1145/1721695.1721705},
 acmid = 1721705,
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Wireless sensor networks, media access control, multi-channel, radio interference, time synchronization},
}


@online{TUGInstmem,
  key =          {TUG},
  year  =        2017,
  title =        "Institutional members of the {\TeX} Users Group",
  url =          "http://wwtug.org/instmem.html",
  lastaccessed = "May 27, 2017",
}

@online{CTANacmart,
  author =    {Boris Veytsman},
  title =  {acmart---{Class} for typesetting publications of {ACM}},
  year = 2017,
  url =    {http://www.ctan.org/pkg/acmart},
  lastaccessed = {May 27, 2017}
  }

@online{doclicense,
  author =    {Robin Schneider},
  title =  {The \textsl{doclicense} package},
  year = 2022,
  url =    {http://www.ctan.org/pkg/doclicense},
  lastaccessed = {May 27, 2022}
  }

@ARTICLE{bowman:reasoning,
    author = {Bowman, Mic and Debray, Saumya K. and Peterson, Larry L.},
    title = {Reasoning About Naming Systems},
    journal = {ACM Trans. Program. Lang. Syst.},
    volume = {15},
    number = {5},
    pages = {795-825},
    month = {November},
    year = {1993},
    doi = {10.1145/161468.161471},
}

@ARTICLE{braams:babel,
    author = {Braams, Johannes},
    title = {Babel, a Multilingual Style-Option System for Use with LaTeX's Standard Document Styles},
    journal = {TUGboat},
    volume = {12},
    number = {2},
    pages = {291-301},
    month = {June},
    year = {1991},
}

@INPROCEEDINGS{clark:pct,
  AUTHOR = "Malcolm Clark",
  TITLE = "Post Congress Tristesse",
  BOOKTITLE = "TeX90 Conference Proceedings",
  PAGES = "84-89",
  ORGANIZATION = "TeX Users Group",
  MONTH = "March",
  YEAR = {1991}
}

@ARTICLE{herlihy:methodology,
    author = {Herlihy, Maurice},
    title = {A Methodology for Implementing Highly Concurrent Data Objects},
    journal = {ACM Trans. Program. Lang. Syst.},
    volume = {15},
    number = {5},
    pages = {745-770},
    month = {November},
    year = {1993},
    doi = {10.1145/161468.161469},
}

@BOOK{salas:calculus,
  AUTHOR = "S.L. Salas and Einar Hille",
  TITLE = "Calculus: One and Several Variable",
  PUBLISHER = "John Wiley and Sons",
  ADDRESS = "New York",
  YEAR = "1978"
}

@MANUAL{Fear05,
  title =        {Publication quality tables in {\LaTeX}},
  author =       {Simon Fear},
  month =        {April},
  year =         2005,
  note =         {\url{http://www.ctan.org/pkg/booktabs}}
}

@Manual{Amsthm15,
  title =        {Using the amsthm Package},
  organization = {American Mathematical Society},
  month =        {April},
  year =         2015,
  note =         {\url{http://www.ctan.org/pkg/amsthm}}
}

@ArtifactSoftware{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019},
    url = {https://www.R-project.org/},
}

@ArtifactDataset{UMassCitations,
 author    =  {Sam Anzaroot and Andrew McCallum},
 title     =  {{UMass} Citation Field Extraction Dataset},
 year      = 2013,
 url       =
    {http://www.iesl.cs.umass.edu/data/data-umasscitationfield},
 lastaccessed = {May 27, 2019}
}

@Eprint{Bornmann2019,
       author = {Bornmann, Lutz and Wray, K. Brad and Haunschild,
                  Robin},
        title = {Citation concept analysis {(CCA)}---A new form of
                  citation analysis revealing the usefulness of
                  concepts for other researchers illustrated by two
                  exemplary case studies including classic books by
                  {Thomas S.~Kuhn} and {Karl R.~Popper}},
     keywords = {Computer Science - Digital Libraries},
         year = 2019,
        month = "May",
          eid = {arXiv:1905.12410},
archivePrefix = {arXiv},
       eprint = {1905.12410},
 primaryClass = {cs.DL},
}

@Eprint{AnzarootPBM14,
  author    = {Sam Anzaroot and
               Alexandre Passos and
               David Belanger and
               Andrew McCallum},
  title     = {Learning Soft Linear Constraints with Application to
                  Citation Field Extraction},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1403.1349},
}

@inproceedings{Hagerup1993,
title        = {Maintaining Discrete Probability Distributions Optimally},
author       = {Hagerup, Torben and Mehlhorn, Kurt and Munro, J. Ian},
booktitle    = {Proceedings of the 20th International Colloquium on Automata, Languages and Programming},
series       = {Lecture Notes in Computer Science},
volume       = {700},
pages        = {253--264},
year         = {1993},
publisher    = {Springer-Verlag},
address      = {Berlin},
}





@book{box2015time,
  title={Time series analysis: forecasting and control},
  author={Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year={2015},
  publisher={John Wiley \& Sons}
}



@article{bauwens2006multivariate,
  title={Multivariate GARCH models: a survey},
  author={Bauwens, Luc and Laurent, S{\'e}bastien and Rombouts, Jeroen VK},
  journal={Journal of applied econometrics},
  volume={21},
  number={1},
  pages={79--109},
  year={2006},
  publisher={Wiley Online Library}
}


@book{bollen1989structural,
  title={Structural equations with latent variables},
  author={Bollen, Kenneth A},
  volume={210},
  year={1989},
  publisher={John Wiley \& Sons}
}



@article{meyer2001support,
  title={Support vector machines},
  author={Meyer, David and Wien, FT},
  journal={R News},
  volume={1},
  number={3},
  pages={23--26},
  year={2001},
  publisher={Citeseer}
}


@incollection{drucker1994boosting,
  title={Boosting and other machine learning algorithms},
  author={Drucker, Harris and Cortes, Corinna and Jackel, Lawrence D and LeCun, Yann and Vapnik, Vladimir},
  booktitle={Machine Learning Proceedings 1994},
  pages={53--61},
  year={1994},
  publisher={Elsevier}
}


@article{wen2022transformers,
  title={Transformers in time series: A survey},
  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  journal={arXiv preprint arXiv:2202.07125},
  year={2022}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{liu2023itransformer,
  title={itransformer: Inverted transformers are effective for time series forecasting},
  author={Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},
  journal={arXiv preprint arXiv:2310.06625},
  year={2023}
}





@article{nie2022time,
  title={A time series is worth 64 words: Long-term forecasting with transformers},
  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  journal={arXiv preprint arXiv:2211.14730},
  year={2022}
}





% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@inproceedings{wu2021peer,
  title={Peer collaborative learning for online knowledge distillation},
  author={Wu, Guile and Gong, Shaogang},
  booktitle={AAAI},
  year={2021}
}




@inproceedings{phuong2019distillation,
  title={Distillation-based training for multi-exit architectures},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1355--1364},
  year={2019}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@inproceedings{xin2021berxit,
  title={BERxiT: Early Exiting for BERT with Better fine-tuning and extension to regression},
  author={Xin, Ji and Tang, Raphael and Yu, Yaoliang and Lin, Jimmy},
  booktitle={Proceedings of the 16th conference of the European chapter of the association for computational linguistics: Main Volume},
  pages={91--104},
  year={2021}
}
@article{fan2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1909.11556},
  year={2019}
}
@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{kaya2019shallow,
  title={Shallow-deep networks: Understanding and mitigating network overthinking},
  author={Kaya, Yigitcan and Hong, Sanghyun and Dumitras, Tudor},
  booktitle={International conference on machine learning},
  pages={3301--3310},
  year={2019},
  organization={PMLR}
}
@article{zhang2018overview,
  title={An overview of multi-task learning},
  author={Zhang, Yu and Yang, Qiang},
  journal={National Science Review},
  volume={5},
  number={1},
  pages={30--43},
  year={2018},
  publisher={Oxford University Press}
}




@article{xu2020bert,
  title={Bert-of-theseus: Compressing bert by progressive module replacing},
  author={Xu, Canwen and Zhou, Wangchunshu and Ge, Tao and Wei, Furu and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.02925},
  year={2020}
}
@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}
@article{gordon2020compressing,
  title={Compressing bert: Studying the effects of weight pruning on transfer learning},
  author={Gordon, Mitchell A and Duh, Kevin and Andrews, Nicholas},
  journal={arXiv preprint arXiv:2002.08307},
  year={2020}
}
@article{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}
@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}
@article{zhang2020ternarybert,
  title={Ternarybert: Distillation-aware ultra-low bit bert},
  author={Zhang, Wei and Hou, Lu and Yin, Yichun and Shang, Lifeng and Chen, Xiao and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2009.12812},
  year={2020}
}
@article{bai2020binarybert,
  title={Binarybert: Pushing the limit of bert quantization},
  author={Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  journal={arXiv preprint arXiv:2012.15701},
  year={2020}
}
@inproceedings{kim2021bert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}
@inproceedings{bolukbasi2017adaptive,
  title={Adaptive neural networks for efficient inference},
  author={Bolukbasi, Tolga and Wang, Joseph and Dekel, Ofer and Saligrama, Venkatesh},
  booktitle={International Conference on Machine Learning},
  pages={527--536},
  year={2017},
  organization={PMLR}
}
@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press}
}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-
drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.
In Proceedings of ICML, 2019

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}
@article{zhao2019uer,
  title={UER: An Open-Source Toolkit for Pre-training Models},
  author={Zhao, Zhe and Chen, Hui and Zhang, Jinbin and Zhao, Xin and Liu, Tao and Lu, Wei and Chen, Xi and Deng, Haotang and Ju, Qi and Du, Xiaoyong},
  journal={EMNLP-IJCNLP 2019},
  pages={241},
  year={2019}
}



@article{Wu2018UnsupervisedFL,
  title={Unsupervised Feature Learning via Non-parametric Instance Discrimination},
  author={Zhirong Wu and Yuanjun Xiong and Stella X. Yu and Dahua Lin},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={3733-3742}
}


@article{Oord2018RepresentationLW,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.03748}
}


@inproceedings{Tian2020ContrastiveMC,
  title={Contrastive Multiview Coding},
  author={Yonglong Tian and Dilip Krishnan and Phillip Isola},
  booktitle={ECCV},
  year={2020}
}

@article{He2020MomentumCF,
  title={Momentum Contrast for Unsupervised Visual Representation Learning},
  author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross B. Girshick},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={9726-9735}
}



@article{Chen2021ExploringSS,
  title={Exploring Simple Siamese Representation Learning},
  author={Xinlei Chen and Kaiming He},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={15745-15753}
}

@inproceedings{Wang2020UnderstandingCR,
  title={Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
  author={Tongzhou Wang and Phillip Isola},
  booktitle={ICML},
  year={2020}
}


@article{Gao2021SimCSESC,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08821}
}


@article{Jaiswal2020ASO,
  title={A Survey on Contrastive Self-supervised Learning},
  author={Ashish Jaiswal and Ashwin Ramesh Babu and Mohammad Zaki Zadeh and Debapriya Banerjee and Fillia Makedon},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.00362}
}



@article{Khosla2020SupervisedCL,
  title={Supervised Contrastive Learning},
  author={Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.11362}
}


@article{Gunel2021SupervisedCL,
  title={Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning},
  author={Beliz Gunel and Jingfei Du and Alexis Conneau and Ves Stoyanov},
  journal={ArXiv},
  year={2021},
  volume={abs/2011.01403}
}



@article{Teerapittayanon2016BranchyNetFI,
  title={BranchyNet: Fast inference via early exiting from deep neural networks},
  author={Surat Teerapittayanon and Bradley McDanel and H. T. Kung},
  journal={2016 23rd International Conference on Pattern Recognition (ICPR)},
  year={2016},
  pages={2464-2469}
}





@article{Li2019ImprovedTF,
  title={Improved Techniques for Training Adaptive Deep Networks},
  author={Hao Li and Hong Zhang and Xiaojuan Qi and Ruigang Yang and Gao Huang},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={1891-1900}
}





@inproceedings{Kaya2019ShallowDeepNU,
	title={Shallow-Deep Networks: Understanding and Mitigating Network Overthinking},
	author={Y. Kaya and Sanghyun Hong and T. Dumitras},
	booktitle={ICML},
	year={2019}
}



@article{Zhou2020PABEE,
	title={BERT Loses Patience: Fast and Robust Inference with Early Exit},
	author={Wangchunshu Zhou and Canwen Xu and Tao Ge and Julian McAuley and Ke Xu and Furu Wei},
	journal={ArXiv},
	year={2020},
	volume={abs/2006.04152}
}


@inproceedings{Zhu2021LeeBERTLE,
  title={LeeBERT: Learned Early Exit for BERT with cross-level optimization},
  author={Wei Zhu},
  booktitle={ACL},
  year={2021}
}


@inproceedings{Zhu2021GAMLBERTIB,
  title={GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning},
  author={Wei Zhu and Xiaoling Wang and Yuan Ni and Guo Tong Xie},
  booktitle={EMNLP},
  year={2021}
}



@inproceedings{Wang2018GLUEAM,
	title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
	author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
	booktitle={BlackboxNLP@EMNLP},
	year={2018}
}






@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}



@article{Hadsell2006DimensionalityRB,
  title={Dimensionality Reduction by Learning an Invariant Mapping},
  author={Raia Hadsell and Sumit Chopra and Yann LeCun},
  journal={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  year={2006},
  volume={2},
  pages={1735-1742}
}


@article{Oord2018RepresentationLW,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.03748}
}



@article{Fang2020CERTCS,
  title={CERT: Contrastive Self-supervised Learning for Language Understanding},
  author={Hongchao Fang and Sicheng Wang and Meng Zhou and Jiayuan Ding and Pengtao Xie},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.12766}
}


@article{Tambe2020EdgeBERTOO,
  title={EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP},
  author={Thierry Tambe and Coleman Hooper and Lillian Pentecost and En-Yu Yang and Marco Donato and Victor Sanh and Alexander M. Rush and David M. Brooks and Gu-Yeon Wei},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.14203}
}


@inproceedings{sun-etal-2022-simple,
    title = "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation",
    author = "Sun, Tianxiang  and
      Liu, Xiangyang  and
      Zhu, Wei  and
      Geng, Zhichao  and
      Wu, Lingling  and
      He, Yilong  and
      Ni, Yuan  and
      Xie, Guotong  and
      Huang, Xuanjing  and
      Qiu, Xipeng",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.189",
    doi = "10.18653/v1/2022.findings-acl.189",
    pages = "2409--2421",
    abstract = "Early exiting allows instances to exit at different layers according to the estimation of difficulty.Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning. In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way. Though some effort has been devoted to employing such {``}learn-to-exit{''} modules, it is still unknown whether and how well the instance difficulty can be learned. As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty. Based on this observation, we propose a simple-yet-effective Hash-based Early Exiting approach HashEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. Different from previous methods, HashEE requires no internal classifiers nor extra parameters, and therefore is more efficient.HashEE can be used in various tasks (including language understanding and generation) and model architectures such as seq2seq models. Experimental results on classification, regression, and generation tasks demonstrate that HashEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.",
}



@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}

@inproceedings{Xu2020BERTofTheseusCB,
  title={BERT-of-Theseus: Compressing BERT by Progressive Module Replacing},
  author={Canwen Xu and Wangchunshu Zhou and Tao Ge and Furu Wei and Ming Zhou},
  booktitle={EMNLP},
  year={2020}
}

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}


@article{Fan2020ReducingTD,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Angela Fan and Edouard Grave and Armand Joulin},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.11556}
}


@inproceedings{Michel2019AreSH,
  title={Are Sixteen Heads Really Better than One?},
  author={Paul Michel and Omer Levy and Graham Neubig},
  booktitle={NeurIPS},
  year={2019}
}


@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}

@article{Jiao2020TinyBERTDB,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.10351}
}



@inproceedings{Loshchilov2019DecoupledWD,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={ICLR},
  year={2019}
}

@article{Xu2021ASO,
  title={A Survey on Green Deep Learning},
  author={Jingjing Xu and Wangchunshu Zhou and Zhiyi Fu and Hao Zhou and Lei Li},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.05193}
}

@article{Lin2021ASO,
  title={A Survey of Transformers},
  author={Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.04554}
}


@article{Liu2019DARTSDA,
  title={DARTS: Differentiable Architecture Search},
  author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
  journal={ArXiv},
  year={2019},
  volume={abs/1806.09055}
}


@article{Xie2019SNASSN,
  title={SNAS: Stochastic Neural Architecture Search},
  author={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},
  journal={ArXiv},
  year={2019},
  volume={abs/1812.09926}
}


@article{Chen2021ProgressiveDB,
  title={Progressive DARTS: Bridging the Optimization Gap for NAS in the Wild},
  author={Xin Chen and Lingxi Xie and Jun Wu and Qi Tian},
  journal={ArXiv},
  year={2021},
  volume={abs/1912.10952}
}


@inproceedings{Schuster2021ConsistentAI,
  title={Consistent Accelerated Inference via Confident Adaptive Transformers},
  author={Tal Schuster and Adam Fisch and T. Jaakkola and Regina Barzilay},
  booktitle={EMNLP},
  year={2021}
}



@article{Krizhevsky2012ImageNetCW,
  title={ImageNet classification with deep convolutional neural networks},
  author={Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  journal={Communications of the ACM},
  year={2012},
  volume={60},
  pages={84 - 90}
}



@article{Simonyan2015VeryDC,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2015},
  volume={abs/1409.1556}
}


@article{He2016DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}



@article{Huang2017DenselyCC,
  title={Densely Connected Convolutional Networks},
  author={Gao Huang and Zhuang Liu and Kilian Q. Weinberger},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={2261-2269}
}



@article{Wang2022DeepNetST,
  title={DeepNet: Scaling Transformers to 1, 000 Layers},
  author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.00555}
}


@article{Zoph2017NeuralAS,
  title={Neural Architecture Search with Reinforcement Learning},
  author={Barret Zoph and Quoc V. Le},
  journal={ArXiv},
  year={2017},
  volume={abs/1611.01578}
}



@article{He2021AutoMLAS,
  title={AutoML: A Survey of the State-of-the-Art},
  author={Xin He and Kaiyong Zhao and Xiaowen Chu},
  journal={Knowl. Based Syst.},
  year={2021},
  volume={212},
  pages={106622}
}


@article{Zoph2018LearningTA,
  title={Learning Transferable Architectures for Scalable Image Recognition},
  author={Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={8697-8710}
}



@inproceedings{Liu2018ProgressiveNA,
  title={Progressive Neural Architecture Search},
  author={Chenxi Liu and Barret Zoph and Jonathon Shlens and Wei Hua and Li-Jia Li and Li Fei-Fei and Alan Loddon Yuille and Jonathan Huang and Kevin P. Murphy},
  booktitle={ECCV},
  year={2018}
}


@article{Real2017LargeScaleEO,
  title={Large-Scale Evolution of Image Classifiers},
  author={Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka Leon Suematsu and Jie Tan and Quoc V. Le and Alexey Kurakin},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.01041}
}


@article{Xie2017GeneticC,
  title={Genetic CNN},
  author={Lingxi Xie and Alan Loddon Yuille},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={1388-1397}
}



@article{Brock2018SMASHOM,
  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},
  author={Andrew Brock and Theodore Lim and James M. Ritchie and Nick Weston},
  journal={ArXiv},
  year={2018},
  volume={abs/1708.05344}
}



@inproceedings{Cai2018EfficientAS,
  title={Efficient Architecture Search by Network Transformation},
  author={Han Cai and Tianyao Chen and Weinan Zhang and Yong Yu and Jun Wang},
  booktitle={AAAI},
  year={2018}
}


@inproceedings{Pham2018EfficientNA,
  title={Efficient Neural Architecture Search via Parameter Sharing},
  author={Hieu Pham and Melody Y. Guan and Barret Zoph and Quoc V. Le and Jeff Dean},
  booktitle={ICML},
  year={2018}
}


@article{Xu2021PartiallyConnectedNA,
  title={Partially-Connected Neural Architecture Search for Reduced Computational Redundancy},
  author={Yuhui Xu and Lingxi Xie and Wenrui Dai and Xiaopeng Zhang and Xin Chen and Guo-Jun Qi and Hongkai Xiong and Qi Tian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  volume={43},
  pages={2953-2970}
}


@article{Chu2020NoisyDA,
  title={Noisy Differentiable Architecture Search},
  author={Xiangxiang Chu and Bo Zhang and Xudong Li},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.03566}
}


@article{Nayman2019XNASNA,
  title={XNAS: Neural Architecture Search with Expert Advice},
  author={Niv Nayman and Asaf Noy and T. Ridnik and Itamar Friedman and Rong Jin and Lihi Zelnik-Manor},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.08031}
}


@article{Zhang2021AutomaticSN,
  title={Automatic Student Network Search for Knowledge Distillation},
  author={Zhexi Zhang and Wei Zhu and Junchi Yan and Peng Gao and Guowang Xie},
  journal={2020 25th International Conference on Pattern Recognition (ICPR)},
  year={2021},
  pages={2446-2453}
}




@inproceedings{Chen2020AdaBERTTB,
  title={AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search},
  author={Daoyuan Chen and Yaliang Li and Minghui Qiu and Zhen Wang and Bofang Li and Bolin Ding and Hongbo Deng and Jun Huang and Wei Lin and Jingren Zhou},
  booktitle={IJCAI},
  year={2020}
}


@article{Xu2021NASBERTTA,
  title={NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search},
  author={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Jian Li and Tao Qin and Tie-Yan Liu},
  journal={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  year={2021}
}



@inproceedings{Dong2021EfficientBERTPS,
  title={EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation},
  author={Chenhe Dong and Guangrun Wang and Hang Xu and Jiefeng Peng and Xiaozhe Ren and Xiaodan Liang},
  booktitle={EMNLP},
  year={2021}
}


@article{Agarap2018DeepLU,
  title={Deep Learning using Rectified Linear Units (ReLU)},
  author={Abien Fred Agarap},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.08375}
}


@article{Hendrycks2016GaussianEL,
  title={Gaussian Error Linear Units (GELUs)},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={arXiv: Learning},
  year={2016}
}


@article{Ramachandran2017SwishAS,
  title={Swish: a Self-Gated Activation Function},
  author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
  journal={arXiv: Neural and Evolutionary Computing},
  year={2017}
}


@article{So2019TheET,
  title={The Evolved Transformer},
  author={David R. So and Chen Liang and Quoc V. Le},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.11117}
}



@inproceedings{Gong2018InformationAV,
  title={Information Aggregation via Dynamic Routing for Sequence Encoding},
  author={Jingjing Gong and Xipeng Qiu and Shaojing Wang and Xuanjing Huang},
  booktitle={COLING},
  year={2018}
}


@inproceedings{Wang2020TextNASAN,
  title={TextNAS: A Neural Architecture Search Space tailored for Text Representation},
  author={Yujing Wang and Yaming Yang and Yiren Chen and Jing Bai and Ce Zhang and Guinan Su and Xiaoyu Kou and Yunhai Tong and Mao Yang and Lidong Zhou},
  booktitle={AAAI},
  year={2020}
}


@inproceedings{Tan2018MultiwayAN,
  title={Multiway Attention Networks for Modeling Sentence Pairs},
  author={Chuanqi Tan and Furu Wei and Wenhui Wang and Weifeng Lv and M. Zhou},
  booktitle={IJCAI},
  year={2018}
}



@article{Liang2021RDropRD,
  title={R-Drop: Regularized Dropout for Neural Networks},
  author={Xiaobo Liang and Lijun Wu and Juntao Li and Yue Wang and Qi Meng and Tao Qin and Wei Chen and M. Zhang and Tie-Yan Liu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.14448}
}


@article{Chu2021FairNASRE,
  title={FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search},
  author={Xiangxiang Chu and Bo Zhang and Ruijun Xu and Jixiang Li},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={12219-12228}
}


@article{Turc2019WellReadSL,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={arXiv: Computation and Language},
  year={2019}
}


@InProceedings{autotrans,
author="Zhu, Wei
and Wang, Xiaoling
and Ni, Yuan
and Xie, Guotong",
editor="Wang, Lu
and Feng, Yansong
and Hong, Yu
and He, Ruifang",
title="AutoTrans: Automating Transformer Design via Reinforced Architecture Search",
booktitle="Natural Language Processing and Chinese Computing",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="169--182",
abstract="Though the transformer architectures have shown dominance in many natural language understanding tasks, there are still unsolved issues for the training of transformer models, especially the need for a principled way of warm-up which has shown importance for stable training of a transformer, as well as whether the task at hand prefer to scale the attention product or not. In this paper, we empirically explore automating the design choices in the transformer model, i.e., how to set layer-norm, whether to scale, number of layers, number of heads, activation function, etc., so that one can obtain a transformer architecture that better suits the tasks at hand. RL is employed to navigate along search space, and special parameter sharing strategies are designed to accelerate the search. It is shown that sampling a proportion of training data per epoch during search help to improve the search quality. Experiments on the CoNLL03, Multi-30k and WMT-14 shows that the searched transformer model can outperform the standard transformers. In particular, we show that our learned model can be trained more robustly with large learning rates without warm-up.",
isbn="978-3-030-88480-2"
}



% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}



@inproceedings{zhu-etal-2021-gaml,
	title = "{GAML}-{BERT}: Improving {BERT} Early Exiting by Gradient Aligned Mutual Learning",
	author = "Zhu, Wei  and
	Wang, Xiaoling  and
	Ni, Yuan  and
	Xie, Guotong",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.242",
	pages = "3033--3044",
	abstract = "In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT{'}s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT{'}s early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods.",
}



@inproceedings{yu-etal-2020-named,
    title = "Named Entity Recognition as Dependency Parsing",
    author = "Yu, Juntao  and
      Bohnet, Bernd  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.577",
    doi = "10.18653/v1/2020.acl-main.577",
    pages = "6470--6476",
    abstract = "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.",
}


@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,
    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",
    author = "Tjong Kim Sang, Erik F.  and
      De Meulder, Fien",
    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",
    year = "2003",
    url = "https://aclanthology.org/W03-0419",
    pages = "142--147",
}


@inproceedings{xin-etal-2020-deebert,
    title = "{D}ee{BERT}: Dynamic Early Exiting for Accelerating {BERT} Inference",
    author = "Xin, Ji  and
      Tang, Raphael  and
      Lee, Jaejun  and
      Yu, Yaoliang  and
      Lin, Jimmy",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.204",
    doi = "10.18653/v1/2020.acl-main.204",
    pages = "2246--2251",
    abstract = "Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to {\textasciitilde}40{\%} inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT.",
}


@inproceedings{schwartz-etal-2020-right,
    title = "The Right Tool for the Job: Matching Model and Instance Complexities",
    author = "Schwartz, Roy  and
      Stanovsky, Gabriel  and
      Swayamdipta, Swabha  and
      Dodge, Jesse  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.593",
    doi = "10.18653/v1/2020.acl-main.593",
    pages = "6640--6651",
    abstract = "As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) {``}exit{''} from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.",
}


@article{GENIA,
author = {Kim, Jin-Dong and Ohta, Tomoko and Tateisi, Yuka and Tsujii, Jun'ichi},
year = {2003},
month = {02},
pages = {i180-2},
title = {GENIA corpus—A semantically annotated corpus for bio-textmining},
volume = {19 Suppl 1},
journal = {Bioinformatics (Oxford, England)},
doi = {10.1093/bioinformatics/btg1023}
}


@inproceedings{levow-2006-third,
    title = "The Third International {C}hinese Language Processing Bakeoff: Word Segmentation and Named Entity Recognition",
    author = "Levow, Gina-Anne",
    booktitle = "Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-0115",
    pages = "108--117",
}


@inproceedings{zhu-etal-2021-gaml,
    title = "{GAML}-{BERT}: Improving {BERT} Early Exiting by Gradient Aligned Mutual Learning",
    author = "Zhu, Wei  and
      Wang, Xiaoling  and
      Ni, Yuan  and
      Xie, Guotong",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.242",
    doi = "10.18653/v1/2021.emnlp-main.242",
    pages = "3033--3044",
    abstract = "In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT{'}s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT{'}s early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods.",
}



@inproceedings{zhu-2021-leebert,
    title = "{L}ee{BERT}: Learned Early Exit for {BERT} with cross-level optimization",
    author = "Zhu, Wei",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.231",
    doi = "10.18653/v1/2021.acl-long.231",
    pages = "2968--2980",
    abstract = "Pre-trained language models like BERT are performant in a wide range of natural language tasks. However, they are resource exhaustive and computationally expensive for industrial scenarios. Thus, early exits are adopted at each layer of BERT to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference. In this work, to improve efficiency without performance drop, we propose a novel training scheme called Learned Early Exit for BERT (LeeBERT). First, we ask each exit to learn from each other, rather than learning only from the last layer. Second, the weights of different loss terms are learned, thus balancing off different objectives. We formulate the optimization of LeeBERT as a bi-level optimization problem, and we propose a novel cross-level optimization (CLO) algorithm to improve the optimization results. Experiments on the GLUE benchmark show that our proposed methods improve the performance of the state-of-the-art (SOTA) early exit methods for pre-trained models.",
}


@article{Kaya2018HowTS,
  title={How to Stop Off-the-Shelf Deep Neural Networks from Overthinking},
  author={Yigitcan Kaya and Tudor Dumitras},
  journal={ArXiv},
  year={2018},
  volume={abs/1810.07052}
}



@article{HellingerNeueBD,
  title={Neue Begr{\"u}ndung der Theorie quadratischer Formen von unendlichvielen Ver{\"a}nderlichen.},
  author={Ernst Hellinger},
  journal={Journal f{\"u}r die reine und angewandte Mathematik},
  volume={1909},
  pages={210 - 271}
}


@inproceedings{Manning2002FoundationsOS,
  title={Foundations of statistical natural language processing},
  author={Christopher D. Manning and Hinrich Sch{\"u}tze},
  booktitle={SGMD},
  year={2002}
}



@article{Girosi1995RegularizationTA,
  title={Regularization Theory and Neural Networks Architectures},
  author={Federico Girosi and Michael J. Jones and Tomaso A. Poggio},
  journal={Neural Computation},
  year={1995},
  volume={7},
  pages={219-269}
}


@article{Ganaie2021EnsembleDL,
  title={Ensemble deep learning: A review},
  author={M. A. Ganaie and Minghui Hu and Muhammad Tanveer and Ponnuthurai Nagaratnam Suganthan},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.02395}
}



@article{Kim2003GENIAC,
  title={GENIA corpus - a semantically annotated corpus for bio-textmining},
  author={Jin-Dong Kim and Tomoko Ohta and Yuka Tateisi and Junichi Tsujii},
  journal={Bioinformatics},
  year={2003},
  volume={19 Suppl 1},
  pages={
          i180-2
        }
}

@inproceedings{levow-2006-third,
    title = "The Third International {C}hinese Language Processing Bakeoff: Word Segmentation and Named Entity Recognition",
    author = "Levow, Gina-Anne",
    booktitle = "Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-0115",
    pages = "108--117",
}


@inproceedings{lu-roth-2015-joint,
    title = "Joint Mention Extraction and Classification with Mention Hypergraphs",
    author = "Lu, Wei  and
      Roth, Dan",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1102",
    doi = "10.18653/v1/D15-1102",
    pages = "857--867",
}


@inproceedings{katiyar-cardie-2018-nested,
    title = "Nested Named Entity Recognition Revisited",
    author = "Katiyar, Arzoo  and
      Cardie, Claire",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1079",
    doi = "10.18653/v1/N18-1079",
    pages = "861--871",
    abstract = "We propose a novel recurrent neural network-based approach to simultaneously handle nested named entity recognition and nested entity mention detection. The model learns a hypergraph representation for nested entities using features extracted from a recurrent neural network. In evaluations on three standard data sets, we show that our approach significantly outperforms existing state-of-the-art methods, which are feature-based. The approach is also efficient: it operates linearly in the number of tokens and the number of possible output labels at any token. Finally, we present an extension of our model that jointly learns the head of each entity mention.",
}


@inproceedings{ma-hovy-2016-end,
    title = "End-to-end Sequence Labeling via Bi-directional {LSTM}-{CNN}s-{CRF}",
    author = "Ma, Xuezhe  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1101",
    doi = "10.18653/v1/P16-1101",
    pages = "1064--1074",
}


@inproceedings{Wu2019GlyceGF,
  title={Glyce: Glyph-vectors for Chinese Character Representations},
  author={Wei Wu and Yuxian Meng and Fei Wang and Qinghong Han and Muyu Li and Xiaoya Li and Jie Mei and Ping Nie and Xiaofei Sun and Jiwei Li},
  booktitle={NeurIPS},
  year={2019}
}


@article{Li2021AcceleratingBI,
  title={Accelerating BERT Inference for Sequence Labeling via Early-Exit},
  author={Xiaonan Li and Yunfan Shao and Tianxiang Sun and Hang Yan and Xipeng Qiu and Xuanjing Huang},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13878}
}


@article{Cui2020RevisitingPM,
  title={Revisiting Pre-Trained Models for Chinese Natural Language Processing},
  author={Yiming Cui and Wanxiang Che and Ting Liu and Bing Qin and Shijin Wang and Guoping Hu},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.13922}
}


@article{Sun2021EarlyEW,
  title={Early Exiting with Ensemble Internal Classifiers},
  author={Tianxiang Sun and Yunhua Zhou and Xiangyang Liu and Xinyu Zhang and Hao Jiang and Zhao Cao and Xuanjing Huang and Xipeng Qiu},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13792}
}





@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}




@inproceedings{Yang2019XLNetGA,
	title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
	author={Z. Yang and Zihang Dai and Yiming Yang and J. Carbonell and R. Salakhutdinov and Quoc V. Le},
	booktitle={NeurIPS},
	year={2019}
}


@article{Fan2020layerdrop,
	title={Reducing Transformer Depth on Demand with Structured Dropout},
	author={Angela Fan and E. Grave and Armand Joulin},
	journal={ArXiv},
	year={2020},
	volume={abs/1909.11556}
}


@inproceedings{Michel2019sixteenhead,
	title={Are Sixteen Heads Really Better than One?},
	author={Paul Michel and Omer Levy and Graham Neubig},
	booktitle={NeurIPS},
	year={2019}
}





@inproceedings{Kaya2019ShallowDeepNU,
	title={Shallow-Deep Networks: Understanding and Mitigating Network Overthinking},
	author={Y. Kaya and Sanghyun Hong and T. Dumitras},
	booktitle={ICML},
	year={2019}
}

@article{Zhu2018ToPO,
	title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
	author={M. Zhu and S. Gupta},
	journal={ArXiv},
	year={2018},
	volume={abs/1710.01878}
}


@inproceedings{Xu2020BERTofTheseusCB,
	title={BERT-of-Theseus: Compressing BERT by Progressive Module Replacing},
	author={Canwen Xu and Wangchunshu Zhou and Tao Ge and Furu Wei and M. Zhou},
	booktitle={EMNLP},
	year={2020}
}

@inproceedings{Sun2019PatientKD,
	title={Patient Knowledge Distillation for BERT Model Compression},
	author={S. Sun and Yu Cheng and Zhe Gan and Jingjing Liu},
	booktitle={EMNLP/IJCNLP},
	year={2019}
}

@article{Sanh2019DistilBERTAD,
	title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
	author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	journal={ArXiv},
	year={2019},
	volume={abs/1910.01108}
}


@article{Jiao2020TinyBERTDB,
	title={TinyBERT: Distilling BERT for Natural Language Understanding},
	author={Xiaoqi Jiao and Y. Yin and L. Shang and Xin Jiang and X. Chen and Linlin Li and F. Wang and Qun Liu},
	journal={ArXiv},
	year={2020},
	volume={abs/1909.10351}
}


@article{Zhang2020TernaryBERTDU,
	title={TernaryBERT: Distillation-aware Ultra-low Bit BERT},
	author={W. Zhang and L. Hou and Y. Yin and L. Shang and X. Chen and X. Jiang and Qun Liu},
	journal={ArXiv},
	year={2020},
	volume={abs/2009.12812}
}

@article{Bai2020BinaryBERTPT,
	title={BinaryBERT: Pushing the Limit of BERT Quantization},
	author={Haoli Bai and Wei Zhang and L. Hou and L. Shang and Jing Jin and X. Jiang and Qun Liu and Michael R. Lyu and Irwin King},
	journal={ArXiv},
	year={2020},
	volume={abs/2012.15701}
}

@article{Kim2021IBERTIB,
	title={I-BERT: Integer-only BERT Quantization},
	author={Se-Hoon Kim and Amir Gholami and Zhewei Yao and M. W. Mahoney and K. Keutzer},
	journal={ArXiv},
	year={2021},
	volume={abs/2101.01321}
}




@inproceedings{Geng2021RomeBERTRT,
	title={RomeBERT: Robust Training of Multi-Exit BERT},
	author={Shijie Geng and Peng Gao and Z. Fu and Yongfeng Zhang},
	year={2021}
}


@article{Liu2020FastBERTAS,
	title={FastBERT: a Self-distilling BERT with Adaptive Inference Time},
	author={Weijie Liu and P. Zhou and Zhe Zhao and Zhiruo Wang and Haotang Deng and Q. Ju},
	journal={ArXiv},
	year={2020},
	volume={abs/2004.02178}
}


@inproceedings{Bolukbasi2017AdaptiveNN,
	title={Adaptive Neural Networks for Efficient Inference},
	author={Tolga Bolukbasi and J. Wang and O. Dekel and Venkatesh Saligrama},
	booktitle={ICML},
	year={2017}
}



@article{Phuong2019DistillationBasedTF,
	title={Distillation-Based Training for Multi-Exit Architectures},
	author={Mary Phuong and Christoph H. Lampert},
	journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
	year={2019},
	pages={1355-1364}
}



@article{Yu2020GradientSF,
	title={Gradient Surgery for Multi-Task Learning},
	author={Tianhe Yu and Saurabh Kumar and A. Gupta and S. Levine and Karol Hausman and Chelsea Finn},
	journal={ArXiv},
	year={2020},
	volume={abs/2001.06782}
}





@article{Hinton2015DistillingTK,
	title={Distilling the Knowledge in a Neural Network},
	author={Geoffrey E. Hinton and Oriol Vinyals and J. Dean},
	journal={ArXiv},
	year={2015},
	volume={abs/1503.02531}
}



@inproceedings{wolf-etal-2020-transformers,
	title = "Transformers: State-of-the-Art Natural Language Processing",
	author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
	month = oct,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
	pages = "38--45"
}

@article{Turc2019WellReadSL,
	title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
	author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	journal={arXiv: Computation and Language},
	year={2019}
}


@inproceedings{Krizhevsky2009LearningML,
	title={Learning Multiple Layers of Features from Tiny Images},
	author={A. Krizhevsky},
	year={2009}
}


@article{He2016DeepRL,
	title={Deep Residual Learning for Image Recognition},
	author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
	journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2016},
	pages={770-778}
}


@article{Lan2020ALBERTAL,
	title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
	author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
	journal={ArXiv},
	year={2020},
	volume={abs/1909.11942}
}

@inproceedings{MRPC,
	title = "Automatically Constructing a Corpus of Sentential Paraphrases",
	author = "Dolan, William B.  and
	Brockett, Chris",
	booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
	year = "2005",
	url = "https://www.aclweb.org/anthology/I05-5002",
}


@inproceedings{socher-etal-2013-recursive,
	title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
	author = "Socher, Richard  and
	Perelygin, Alex  and
	Wu, Jean  and
	Chuang, Jason  and
	Manning, Christopher D.  and
	Ng, Andrew  and
	Potts, Christopher",
	booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
	month = oct,
	year = "2013",
	address = "Seattle, Washington, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D13-1170",
	pages = "1631--1642",
}



@inproceedings{williams-etal-2018-broad,
	title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
	author = "Williams, Adina  and
	Nangia, Nikita  and
	Bowman, Samuel",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N18-1101",
	doi = "10.18653/v1/N18-1101",
	pages = "1112--1122",
	abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}



@inproceedings{rajpurkar-etal-2016-squad,
	title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
	author = "Rajpurkar, Pranav  and
	Zhang, Jian  and
	Lopyrev, Konstantin  and
	Liang, Percy",
	booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2016",
	address = "Austin, Texas",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D16-1264",
	doi = "10.18653/v1/D16-1264",
	pages = "2383--2392",
}



@article{warstadt-etal-2019-neural,
	title = "Neural Network Acceptability Judgments",
	author = "Warstadt, Alex  and
	Singh, Amanpreet  and
	Bowman, Samuel R.",
	journal = "Transactions of the Association for Computational Linguistics",
	volume = "7",
	month = mar,
	year = "2019",
	url = "https://www.aclweb.org/anthology/Q19-1040",
	doi = "10.1162/tacl_a_00290",
	pages = "625--641",
	abstract = "This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.{'}s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.",
}


@inproceedings{wnli,
	author = {Levesque, Hector J. and Davis, Ernest and Morgenstern, Leora},
	title = {The Winograd Schema Challenge},
	year = {2012},
	isbn = {9781577355601},
	publisher = {AAAI Press},
	abstract = {In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Wino-grad schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.},
	booktitle = {Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning},
	pages = {552–561},
	numpages = {10},
	location = {Rome, Italy},
	series = {KR'12}
}


@inproceedings{devlin-etal-2019-bert,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N19-1423",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@inproceedings{jiao-etal-2020-tinybert,
	title = "{T}iny{BERT}: Distilling {BERT} for Natural Language Understanding",
	author = "Jiao, Xiaoqi  and
	Yin, Yichun  and
	Shang, Lifeng  and
	Jiang, Xin  and
	Chen, Xiao  and
	Li, Linlin  and
	Wang, Fang  and
	Liu, Qun",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/2020.findings-emnlp.372",
	doi = "10.18653/v1/2020.findings-emnlp.372",
	pages = "4163--4174",
	abstract = "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large {``}teacher{''} BERT can be effectively transferred to a small {``}student{''} TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8{\%} the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only {\textasciitilde}28{\%} parameters and {\textasciitilde}31{\%} inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
}



@inproceedings{xu-etal-2020-bert,
	title = "{BERT}-of-Theseus: Compressing {BERT} by Progressive Module Replacing",
	author = "Xu, Canwen  and
	Zhou, Wangchunshu  and
	Ge, Tao  and
	Wei, Furu  and
	Zhou, Ming",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/2020.emnlp-main.633",
	doi = "10.18653/v1/2020.emnlp-main.633",
	pages = "7859--7869",
	abstract = "In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.",
}





@article{Hinton2015DistillingTK,
	title={Distilling the Knowledge in a Neural Network},
	author={Geoffrey E. Hinton and Oriol Vinyals and J. Dean},
	journal={ArXiv},
	year={2015},
	volume={abs/1503.02531}
}





@article{Huang2017MultiScaleDC,
	title={Multi-Scale Dense Convolutional Networks for Efficient Prediction},
	author={Gao Huang and Danlu Chen and T. Li and Felix Wu and L. V. D. Maaten and Kilian Q. Weinberger},
	journal={ArXiv},
	year={2017},
	volume={abs/1703.09844}
}




@article{Szegedy2016RethinkingTI,
	title={Rethinking the Inception Architecture for Computer Vision},
	author={Christian Szegedy and V. Vanhoucke and S. Ioffe and Jonathon Shlens and Z. Wojna},
	journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2016},
	pages={2818-2826}
}


@inproceedings{Yang2020TextBrewerAO,
	title={TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing},
	author={Ziqing Yang and Yiming Cui and ZhiPeng Chen and Wanxiang Che and T. Liu and S. Wang and Guoping Hu},
	booktitle={ACL},
	year={2020}
}


@inproceedings{Sun2019PatientKD,
	title={Patient Knowledge Distillation for BERT Model Compression},
	author={S. Sun and Yu Cheng and Zhe Gan and Jingjing Liu},
	booktitle={EMNLP/IJCNLP},
	year={2019}
}


@article{Li2021AcceleratingBI,
  title={Accelerating BERT Inference for Sequence Labeling via Early-Exit},
  author={Xiaonan Li and Yunfan Shao and Tianxiang Sun and Hang Yan and Xipeng Qiu and Xuanjing Huang},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13878}
}


@article{Kullback1951OnIA,
  title={On Information and Sufficiency},
  author={Solomon Kullback and R. A. Leibler},
  journal={Annals of Mathematical Statistics},
  year={1951},
  volume={22},
  pages={79-86}
}



@inproceedings{Sun2020LearningSS,
  title={Learning Sparse Sharing Architectures for Multiple Tasks},
  author={Tianxiang Sun and Yunfan Shao and Xiaonan Li and Pengfei Liu and Hang Yan and Xipeng Qiu and Xuanjing Huang},
  booktitle={AAAI},
  year={2020}
}



@article{Hendrycks2016BridgingNA,
  title={Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.08415}
}



@article{Huang2015BidirectionalLM,
  title={Bidirectional LSTM-CRF Models for Sequence Tagging},
  author={Zhiheng Huang and Wei Xu and Kai Yu},
  journal={ArXiv},
  year={2015},
  volume={abs/1508.01991}
}


@article{Li2019ImprovedTF,
  title={Improved Techniques for Training Adaptive Deep Networks},
  author={Hao Li and Hong Zhang and Xiaojuan Qi and Ruigang Yang and Gao Huang},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={1891-1900}
}


@inproceedings{liao-etal-2021-global,
    title = "A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models",
    author = "Liao, Kaiyuan  and
      Zhang, Yi  and
      Ren, Xuancheng  and
      Su, Qi  and
      Sun, Xu  and
      He, Bin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.162",
    doi = "10.18653/v1/2021.naacl-main.162",
    pages = "2013--2023",
    abstract = "Early exit mechanism aims to accelerate the inference speed of large-scale pre-trained language models. The essential idea is to exit early without passing through all the inference layers at the inference stage. To make accurate predictions for downstream tasks, the hierarchical linguistic information embedded in all layers should be jointly considered. However, much of the research up to now has been limited to use local representations of the exit layer. Such treatment inevitably loses information of the unused past layers as well as the high-level features embedded in future layers, leading to sub-optimal performance. To address this issue, we propose a novel Past-Future method to make comprehensive predictions from a global perspective. We first take into consideration all the linguistic information embedded in the past layers and then take a further step to engage the future information which is originally inaccessible for predictions. Extensive experiments demonstrate that our method outperforms previous early exit methods by a large margin, yielding better and robust performance.",
}



@article{Sun2021EarlyEW,
  title={Early Exiting with Ensemble Internal Classifiers},
  author={Tianxiang Sun and Yunhua Zhou and Xiangyang Liu and Xinyu Zhang and Hao Jiang and Zhao Cao and Xuanjing Huang and Xipeng Qiu},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13792}
}



@inproceedings{zhang-etal-2022-pcee,
    title = "{PCEE}-{BERT}: Accelerating {BERT} Inference via Patient and Confident Early Exiting",
    author = "Zhang, Zhen  and
      Zhu, Wei  and
      Zhang, Jinfan  and
      Wang, Peng  and
      Jin, Rize  and
      Chung, Tae-Sun",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.25",
    doi = "10.18653/v1/2022.findings-naacl.25",
    pages = "327--338",
    abstract = "BERT and other pretrained language models (PLMs) are ubiquitous in modern NLP. Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (CITATION), the significant latency during inference prohibits wider industrial usage. In this work, we propose Patient and Confident Early Exiting BERT (PCEE-BERT), an off-the-shelf sample-dependent early exiting method that can work with different PLMs and can also work along with popular model compression methods. With a multi-exit BERT as the backbone model, PCEE-BERT will make the early exiting decision if enough numbers (patience parameter) of consecutive intermediate layers are confident about their predictions. The entropy value measures the confidence level of an intermediate layer{'}s prediction. Experiments on the GLUE benchmark demonstrate that our method outperforms previous SOTA early exiting methods. Ablation studies show that: (a) our method performs consistently well on other PLMs, such as ALBERT and TinyBERT; (b) PCEE-BERT can achieve different speed-up ratios by adjusting the patience parameter and the confidence threshold. The code for PCEE-BERT can be found at \url{https://github.com/michael-wzhu/PCEE-BERT}.",
}


@inproceedings{Li2021CascadeBERTAI,
  title={CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade},
  author={Lei Li and Yankai Lin and Deli Chen and Shuhuai Ren and Peng Li and Jie Zhou and Xu Sun},
  booktitle={EMNLP},
  year={2021}
}


@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}




@article{He2021TowardsAU,
  title={Towards a Unified View of Parameter-Efficient Transfer Learning},
  author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.04366}
}



@inproceedings{liu-etal-2022-towards-efficient,
    title = "Towards Efficient {NLP}: A Standard Evaluation and A Strong Baseline",
    author = "Liu, Xiangyang  and
      Sun, Tianxiang  and
      He, Junliang  and
      Wu, Jiawen  and
      Wu, Lingling  and
      Zhang, Xinyu  and
      Jiang, Hao  and
      Cao, Zhao  and
      Huang, Xuanjing  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.240",
    doi = "10.18653/v1/2022.naacl-main.240",
    pages = "3288--3303",
    abstract = "Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depicting the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.",
}


@article{Hendrycks2016BridgingNA,
  title={Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.08415}
}


@article{Fukushima2004CognitronAS,
  title={Cognitron: A self-organizing multilayered neural network},
  author={Kunihiko Fukushima},
  journal={Biological Cybernetics},
  year={2004},
  volume={20},
  pages={121-136}
}



@article{Ramachandran2017SwishAS,
  title={Swish: a Self-Gated Activation Function},
  author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
  journal={arXiv: Neural and Evolutionary Computing},
  year={2017}
}


@article{Hochreiter1997lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}






@article{Elbayad2020DepthAdaptiveT,
  title={Depth-Adaptive Transformer},
  author={Maha Elbayad and Jiatao Gu and Edouard Grave and Michael Auli},
  journal={ArXiv},
  year={2020},
  volume={abs/1910.10073}
}




@inproceedings{Schuster2021ConsistentAI,
  title={Consistent Accelerated Inference via Confident Adaptive Transformers},
  author={Tal Schuster and Adam Fisch and T. Jaakkola and Regina Barzilay},
  booktitle={EMNLP},
  year={2021}
}


@article{Han2021PreTrainedMP,
  title={Pre-Trained Models: Past, Present and Future},
  author={Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.07139}
}


@inproceedings{Peters2018DeepCW,
  title={Deep Contextualized Word Representations},
  author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2018}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}


@article{Raffel2019ExploringTL,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.10683}
}


@article{Wang2021ERNIE3T,
  title={ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation},
  author={Shuohuan Wang and Yu Sun and Yang Xiang and Zhihua Wu and Siyu Ding and Weibao Gong and Shi Feng and Junyuan Shang and Yanbin Zhao and Chao Pang and Jiaxiang Liu and Xuyi Chen and Yuxiang Lu and Weixin Liu and Xi Wang and Yangfan Bai and Qiuliang Chen and Li Zhao and Shiyong Li and Peng Sun and Dianhai Yu and Yanjun Ma and Hao Tian and Hua Wu and Tian Wu and Wei Zeng and Ge Li and Wen Gao and Haifeng Wang},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.12731}
}

@article{Ding2022DeltaTA,
  title={Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models},
  author={Ning Ding and Yujia Qin and Guang Yang and Fu Wei and Zonghan Yang and Yusheng Su and Shengding Hu and Yulin Chen and Chi-Min Chan and Weize Chen and Jing Yi and Weilin Zhao and Xiaozhi Wang and Zhiyuan Liu and Haitao Zheng and Jianfei Chen and Yang Liu and Jie Tang and Juan Li and Maosong Sun},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.06904}
}

@inproceedings{pfeiffer-etal-2021-adapterfusion,
    title = "{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning",
    author = {Pfeiffer, Jonas  and
      Kamath, Aishwarya  and
      R{\"u}ckl{\'e}, Andreas  and
      Cho, Kyunghyun  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.39",
    doi = "10.18653/v1/2021.eacl-main.39",
    pages = "487--503",
    abstract = "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.",
}


@inproceedings{Mahabadi2021CompacterEL,
  title={Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
  author={Rabeeh Karimi Mahabadi and James Henderson and Sebastian Ruder},
  booktitle={NeurIPS},
  year={2021}
}


@article{BenZaken2021BitFitSP,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Elad Ben-Zaken and Shauli Ravfogel and Yoav Goldberg},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.10199}
}

@article{Liu2021PTuningVP,
  title={P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks},
  author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Zhengxiao Du and Zhilin Yang and Jie Tang},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.07602}
}


@article{Sun2022BlackBoxTF,
  title={Black-Box Tuning for Language-Model-as-a-Service},
  author={Tianxiang Sun and Yunfan Shao and Hong Qian and Xuanjing Huang and Xipeng Qiu},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.03514}
}


@inproceedings{Wu2022IDPGAI,
  title={IDPG: An Instance-Dependent Prompt Generation Method},
  author={Zhuofeng Wu and Sinong Wang and Jiatao Gu and Rui Hou and Yuxiao Dong and V. G. Vinod Vydiswaran and Hao Ma},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2022}
}

@inproceedings{Le2021ParameterizedHG,
  title={Parameterized Hypercomplex Graph Neural Networks for Graph Classification},
  author={Tuan Le and Marco Bertolini and Frank No'e and Djork-Arn{\'e} Clevert},
  booktitle={International Conference on Artificial Neural Networks},
  year={2021}
}


@article{Liu2022LatePT,
  title={Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts},
  author={Xiangyang Liu and Tianxiang Sun and Xuanjing Huang and Xipeng Qiu},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11292}
}



@inproceedings{Liu2022PTuningPT,
  title={P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks},
  author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022}
}


@article{Li2021PrefixTuningOC,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Xiang Lisa Li and Percy Liang},
  journal={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year={2021},
  volume={abs/2101.00190}
}



@inproceedings{Tang2022ContextTuningLC,
  title={Context-Tuning: Learning Contextualized Prompts for Natural Language Generation},
  author={Tianyi Tang and Junyi Li and Wayne Xin Zhao},
  booktitle={International Conference on Computational Linguistics},
  year={2022}
}


@article{Jin2022InstanceawarePL,
  title={Instance-aware Prompt Learning for Language Understanding and Generation},
  author={Feihu Jin and Jinliang Lu and Jiajun Zhang and Chengqing Zong},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.07126}
}


@inproceedings{Rckl2020AdapterDropOT,
  title={AdapterDrop: On the Efficiency of Adapters in Transformers},
  author={Andreas R{\"u}ckl{\'e} and Gregor Geigle and Max Glockner and Tilman Beck and Jonas Pfeiffer and Nils Reimers and Iryna Gurevych},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020}
}


@inproceedings{Wang2021LiSTLP,
  title={LiST: Lite Prompted Self-training Makes Parameter-efficient Few-shot Learners},
  author={Yaqing Wang and Subhabrata Mukherjee and Xiaodong Liu and Jing Gao and Ahmed Hassan Awadallah and Jianfeng Gao},
  booktitle={NAACL-HLT},
  year={2021}
}


@inproceedings{voorhees-tice-2000-trec,
    title = "The {TREC}-8 Question Answering Track",
    author = "Voorhees, Ellen M.  and
      Tice, Dawn M.",
    booktitle = "Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00)",
    month = may,
    year = "2000",
    address = "Athens, Greece",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2000/pdf/26.pdf",
}


@inproceedings{rte,
author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
year = {2005},
month = {01},
pages = {177-190},
title = {The PASCAL recognising textual entailment challenge},
isbn = {978-3-540-33427-9},
doi = {10.1007/11736790_9}
}

@inproceedings{guo-etal-2021-parameter,
    title = "Parameter-Efficient Transfer Learning with Diff Pruning",
    author = "Guo, Demi  and
      Rush, Alexander  and
      Kim, Yoon",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.378",
    doi = "10.18653/v1/2021.acl-long.378",
    pages = "4884--4896",
    abstract = "The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. Diff pruning enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a task-specific {``}diff{''} vector that extends the original pretrained parameters. This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. As the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task. Since it does not require access to all tasks during training, it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers. Diff pruning can match the performance of finetuned baselines on the GLUE benchmark while only modifying 0.5{\%} of the pretrained model{'}s parameters per task and scales favorably in comparison to popular pruning approaches.",
}


@inproceedings{Moosavi2022AdaptableA,
  title={Adaptable Adapters},
  author={Nafise Sadat Moosavi and Quentin Delfosse and Kristian Kersting and Iryna Gurevych},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2022}
}


@article{Jie2022ConvolutionalBA,
  title={Convolutional Bypasses Are Better Vision Transformer Adapters},
  author={Shibo Jie and Zhifang Deng},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.07039}
}



@article{Sung2022LSTLS,
  title={LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning},
  author={Yi-Lin Sung and Jaemin Cho and Mohit Bansal},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.06522}
}


@inproceedings{Krizhevsky2009LearningML,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009}
}


@article{Wang2019SuperGLUEAS,
  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.00537}
}



@inproceedings{dolan-brockett-2005-automatically,
    title = "Automatically Constructing a Corpus of Sentential Paraphrases",
    author = "Dolan, William B.  and
      Brockett, Chris",
    booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
    year = "2005",
    url = "https://aclanthology.org/I05-5002",
}


@inproceedings{Levesque2011TheWS,
  title={The Winograd Schema Challenge},
  author={Hector J. Levesque and Ernest Davis and L. Morgenstern},
  booktitle={International Conference on Principles of Knowledge Representation and Reasoning},
  year={2011}
}


@article{Zhang2020RevisitingFB,
  title={Revisiting Few-sample BERT Fine-tuning},
  author={Tianyi Zhang and Felix Wu and Arzoo Katiyar and Kilian Q. Weinberger and Yoav Artzi},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.05987}
}


@article{He2020DeBERTaDB,
  title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.03654}
}




@article{Wang2022AdaMixMF,
  title={AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning},
  author={Yaqing Wang and Subhabrata Mukherjee and Xiaodong Liu and Jing Gao and Jianfeng Gao},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.17451}
}



@inproceedings{aghajanyan-etal-2021-intrinsic,
    title = "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
    author = "Aghajanyan, Armen  and
      Gupta, Sonal  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.568",
    doi = "10.18653/v1/2021.acl-long.568",
    pages = "7319--7328",
    abstract = "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90{\%} of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.",
}



@article{Zoph2017LearningTA,
  title={Learning Transferable Architectures for Scalable Image Recognition},
  author={Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={8697-8710}
}


@inproceedings{Real2018RegularizedEF,
  title={Regularized Evolution for Image Classifier Architecture Search},
  author={Esteban Real and Alok Aggarwal and Yanping Huang and Quoc V. Le},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2018}
}






@article{Xie2018SNASSN,
  title={SNAS: Stochastic Neural Architecture Search},
  author={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.09926}
}



@article{Dong2019SearchingFA,
  title={Searching for a Robust Neural Architecture in Four GPU Hours},
  author={Xuanyi Dong and Yezhou Yang},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={1761-1770}
}


@article{Zhou2019BayesNASAB,
  title={BayesNAS: A Bayesian Approach for Neural Architecture Search},
  author={Hongpeng Zhou and Minghao Yang and Jun Wang and Wei Pan},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.04919}
}



@article{Zela2019UnderstandingAR,
  title={Understanding and Robustifying Differentiable Architecture Search},
  author={Arber Zela and Thomas Elsken and Tonmoy Saikia and Yassine Marrakchi and Thomas Brox and Frank Hutter},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.09656}
}



@article{Hu2022SparseSS,
  title={Sparse Structure Search for Parameter-Efficient Tuning},
  author={Shengding Hu and Zhen Zhang and Ning Ding and Yadao Wang and Yasheng Wang and Zhiyuan Liu and Maosong Sun},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.07382}
}


@article{Mao2021UniPELTAU,
  title={UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning},
  author={Yuning Mao and Lambert Mathias and Rui Hou and Amjad Almahairi and Hao Ma and Jiawei Han and Wen-tau Yih and Madian Khabsa},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.07577}
}



@article{He2015DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={770-778}
}


@article{Gao2020MTLNASTN,
  title={MTL-NAS: Task-Agnostic Neural Architecture Search Towards General-Purpose Multi-Task Learning},
  author={Yuan Gao and Haoping Bai and Zequn Jie and Jiayi Ma and Kui Jia and Wei Liu},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={11540-11549}
}



@ARTICLE{2019arXiv191011831B,
       author = {{Bi}, Kaifeng and {Hu}, Changping and {Xie}, Lingxi and {Chen}, Xin and {Wei}, Longhui and {Tian}, Qi},
        title = "{Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
         year = 2019,
        month = oct,
          eid = {arXiv:1910.11831},
        pages = {arXiv:1910.11831},
          doi = {10.48550/arXiv.1910.11831},
archivePrefix = {arXiv},
       eprint = {1910.11831},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191011831B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{Bi2020GOLDNASGO,
  title={GOLD-NAS: Gradual, One-Level, Differentiable},
  author={Kaifeng Bi and Lingxi Xie and Xin Chen and Longhui Wei and Qi Tian},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.03331}
}



@ARTICLE{2021arXiv210109671L,
       author = {{Liang}, Tailin and {Glossner}, John and {Wang}, Lei and {Shi}, Shaobo and {Zhang}, Xiaotong},
        title = "{Pruning and Quantization for Deep Neural Network Acceleration: A Survey}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
         year = 2021,
        month = jan,
          eid = {arXiv:2101.09671},
        pages = {arXiv:2101.09671},
          doi = {10.48550/arXiv.2101.09671},
archivePrefix = {arXiv},
       eprint = {2101.09671},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210109671L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}





@inproceedings{zhang2021automatic,
  title={Automatic Student Network Search for Knowledge Distillation},
  author={Zhang, Zhexi and Zhu, Wei and Yan, Junchi and Gao, Peng and Xie, Guotong},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={2446--2453},
  year={2021},
  organization={IEEE}
}


@inproceedings{gao2023f,
  title={F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks},
  author={Gao, Xiangxiang and Zhu, Wei and Gao, Jiasheng and Yin, Congrui},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}


@INPROCEEDINGS{10094859,
  author={Zhu, Wei and Wang, Peng and Wang, Xiaoling and Ni, Yuan and Xie, Guotong},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={ACF: Aligned Contrastive Finetuning For Language and Vision Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICASSP49357.2023.10094859}}


  @inproceedings{li-etal-2019-pingan,
    title = "Pingan Smart Health and {SJTU} at {COIN} - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks",
    author = "Li, Xiepeng  and
      Zhang, Zhexi  and
      Zhu, Wei  and
      Li, Zheng  and
      Ni, Yuan  and
      Gao, Peng  and
      Yan, Junchi  and
      Xie, Guotong",
    booktitle = "Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6011",
    doi = "10.18653/v1/D19-6011",
    pages = "93--98",
    abstract = "To solve the shared tasks of COIN: COmmonsense INference in Natural Language Processing) Workshop in , we need explore the impact of knowledge representation in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching. There are two approaches to represent knowledge in the low-dimensional space. The first is to leverage large-scale unsupervised text corpus to train fixed or contextual language representations. The second approach is to explicitly express knowledge into a knowledge graph (KG), and then fit a model to represent the facts in the KG. We have experimented both (a) improving the fine-tuning of pre-trained language models on a task with a small dataset size, by leveraging datasets of similar tasks; and (b) incorporating the distributional representations of a KG onto the representations of pre-trained language models, via simply concatenation or multi-head attention. We find out that: (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target task improve the performance significantly; (b) for task 2, we find out the incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace of XLNET (Yangetal.,2019), a more powerful pre-trained model. Our approaches achieve the state-of-the-art results on both shared task{'}s official test data, outperforming all the other submissions.",
}




@inproceedings{zhu-2021-autorc,
    title = "{A}uto{RC}: Improving {BERT} Based Relation Classification Models via Architecture Search",
    author = "Zhu, Wei",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-srw.4",
    doi = "10.18653/v1/2021.acl-srw.4",
    pages = "33--43",
    abstract = "Although BERT based relation classification (RC) models have achieved significant improvements over the traditional deep learning models, it seems that no consensus can be reached on what is the optimal architecture, since there are many design choices available. In this work, we design a comprehensive search space for BERT based RC models and employ a modified version of efficient neural architecture search (ENAS) method to automatically discover the design choices mentioned above. Experiments on eight benchmark RC tasks show that our method is efficient and effective in finding better architectures than the baseline BERT based RC models. Ablation study demonstrates the necessity of our search space design and the effectiveness of our search method. We also show that our framework can also apply to other entity related tasks like coreference resolution and span based named entity recognition (NER).",
}



@inproceedings{zhu-etal-2021-discovering,
    title = "Discovering Better Model Architectures for Medical Query Understanding",
    author = "Zhu, Wei  and
      Ni, Yuan  and
      Wang, Xiaoling  and
      Xie, Guotong",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-industry.29",
    doi = "10.18653/v1/2021.naacl-industry.29",
    pages = "230--237",
    abstract = "In developing an online question-answering system for the medical domains, natural language inference (NLI) models play a central role in question matching and intention detection. However, which models are best for our datasets? Manually selecting or tuning a model is time-consuming. Thus we experiment with automatically optimizing the model architectures on the task at hand via neural architecture search (NAS). First, we formulate a novel architecture search space based on the previous NAS literature, supporting cross-sentence attention (cross-attn) modeling. Second, we propose to modify the ENAS method to accelerate and stabilize the search results. We conduct extensive experiments on our two medical NLI tasks. Results show that our system can easily outperform the classical baseline models. We compare different NAS methods and demonstrate our approach provides the best results.",
}







@inproceedings{zuo-etal-2022-continually,
    title = "Continually Detection, Rapidly React: Unseen Rumors Detection Based on Continual Prompt-Tuning",
    author = "Zuo, Yuhui  and
      Zhu, Wei  and
      Cai, Guoyong GUET",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.268",
    pages = "3029--3041",
    abstract = "Since open social platforms allow for a large and continuous flow of unverified information, rumors can emerge unexpectedly and spread quickly. However, existing rumor detection (RD) models often assume the same training and testing distributions and can not cope with the continuously changing social network environment. This paper proposed a Continual Prompt-Tuning RD (CPT-RD) framework, which avoids catastrophic forgetting (CF) of upstream tasks during sequential task learning and enables bidirectional knowledge transfer between domain tasks. Specifically, we propose the following strategies: (a) Our design explicitly decouples shared and domain-specific knowledge, thus reducing the interference among different domains during optimization; (b) Several technologies aim to transfer knowledge of upstream tasks to deal with emergencies; (c) A task-conditioned prompt-wise hypernetwork (TPHNet) is used to consolidate past domains. In addition, CPT-RD avoids CF without the necessity of a rehearsal buffer. Finally, CPT-RD is evaluated on English and Chinese RD datasets and is effective and efficient compared to prior state-of-the-art methods.",
}




@inproceedings{guo-etal-2021-global,
    title = "Global Attention Decoder for {C}hinese Spelling Error Correction",
    author = "Guo, Zhao  and
      Ni, Yuan  and
      Wang, Keqiang  and
      Zhu, Wei  and
      Xie, Guotong",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.122",
    doi = "10.18653/v1/2021.findings-acl.122",
    pages = "1419--1428",
}



@inproceedings{zhu-etal-2021-paht,
    title = "paht{\_}nlp @ {MEDIQA} 2021: Multi-grained Query Focused Multi-Answer Summarization",
    author = "Zhu, Wei  and
      He, Yilong  and
      Chai, Ling  and
      Fan, Yunxiao  and
      Ni, Yuan  and
      Xie, Guotong  and
      Wang, Xiaoling",
    booktitle = "Proceedings of the 20th Workshop on Biomedical Language Processing",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.bionlp-1.10",
    doi = "10.18653/v1/2021.bionlp-1.10",
    pages = "96--102",
    abstract = "In this article, we describe our systems for the MEDIQA 2021 Shared Tasks. First, we will describe our method for the second task, Multi-Answer Summarization (MAS). For extractive summarization, two series of methods are applied. The first one follows (CITATION). First a RoBERTa model is first applied to give a local ranking of the candidate sentences. Then a Markov Chain model is applied to evaluate the sentences globally. The second method applies cross-sentence contextualization to improve the local ranking and discard the global ranking step. Our methods achieve \textbf{the 1st Place} in the MAS task. For the question summarization (QS) and radiology report summarization (RRS) tasks, we explore how end-to-end pre-trained seq2seq model perform. A series of tricks for improving the fine-tuning performances are validated.",
}



@Article{info:doi/10.2196/17653,
author="Sun, Haixia
and Xiao, Jin
and Zhu, Wei
and He, Yilong
and Zhang, Sheng
and Xu, Xiaowei
and Hou, Li
and Li, Jiao
and Ni, Yuan
and Xie, Guotong",
title="Medical Knowledge Graph to Enhance Fraud, Waste, and Abuse Detection on Claim Data: Model Development and Performance Evaluation",
journal="JMIR Med Inform",
year="2020",
month="Jul",
day="23",
volume="8",
number="7",
pages="e17653",
keywords="medical knowledge graph; FWA detection",
abstract="Background: Fraud, Waste, and Abuse (FWA) detection is a significant yet challenging problem in the health insurance industry. An essential step in FWA detection is to check whether the medication is clinically reasonable with respect to the diagnosis. Currently, human experts with sufficient medical knowledge are required to perform this task. To reduce the cost, insurance inspectors tend to build an intelligent system to detect suspicious claims with inappropriate diagnoses/medications automatically. Objective: The aim of this study was to develop an automated  method for making use of a medical knowledge graph to identify clinically suspected claims for FWA detection. Methods: First, we identified the medical knowledge that is required to assess the clinical rationality of the claims. We then searched for data sources that contain information to build such knowledge. In this study, we focused on Chinese medical knowledge. Second, we constructed a medical knowledge graph using unstructured knowledge. We used a deep learning--based method to extract the entities and relationships from the knowledge sources and developed a multilevel similarity matching approach to conduct the entity linking. To guarantee the quality of the medical knowledge graph, we involved human experts to review the entity and relationships with lower confidence. These reviewed results could be used to further improve the machine-learning models. Finally, we developed the rules to identify the suspected claims by reasoning according to the medical knowledge graph. Results: We collected 185,796 drug labels from the China Food and Drug Administration, 3390 types of disease information from medical textbooks (eg, symptoms, diagnosis, treatment, and prognosis), and information from 5272 examinations as the knowledge sources. The final medical knowledge graph includes 1,616,549 nodes and 5,963,444 edges. We designed three knowledge graph reasoning rules to identify three kinds of inappropriate diagnosis/medications. The experimental results showed that the medical knowledge graph helps to detect 70{\%} of the suspected claims. Conclusions: The medical knowledge graph--based method successfully identified suspected cases of FWA (such as fraud diagnosis, excess prescription, and irrational prescription) from the claim documents, which helped to improve the efficiency of claim processing. ",
issn="2291-9694",
doi="10.2196/17653",
url="http://medinform.jmir.org/2020/7/e17653/",
url="https://doi.org/10.2196/17653",
url="http://www.ncbi.nlm.nih.gov/pubmed/32706714"
}


@inproceedings{zhu-etal-2019-panlp,
    title = "{PANLP} at {MEDIQA} 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation",
    author = "Zhu, Wei  and
      Zhou, Xiaofeng  and
      Wang, Keqiang  and
      Luo, Xun  and
      Li, Xiepeng  and
      Ni, Yuan  and
      Xie, Guotong",
    booktitle = "Proceedings of the 18th BioNLP Workshop and Shared Task",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5040",
    doi = "10.18653/v1/W19-5040",
    pages = "380--388",
    abstract = "This paper describes the models designated for the MEDIQA 2019 shared tasks by the team PANLP. We take advantages of the recent advances in pre-trained bidirectional transformer language models such as BERT (Devlin et al., 2018) and MT-DNN (Liu et al., 2019b). We find that pre-trained language models can significantly outperform traditional deep learning models. Transfer learning from the NLI task to the RQE task is also experimented, which proves to be useful in improving the results of fine-tuning MT-DNN large. A knowledge distillation process is implemented, to distill the knowledge contained in a set of models and transfer it into an single model, whose performance turns out to be comparable with that obtained by the ensemble of that set of models. Finally, for test submissions, model ensemble and a re-ranking process are implemented to boost the performances. Our models participated in all three tasks and ranked the 1st place for the RQE task, and the 2nd place for the NLI task, and also the 2nd place for the QA task.",
}



@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}



% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@inproceedings{wu2021peer,
  title={Peer collaborative learning for online knowledge distillation},
  author={Wu, Guile and Gong, Shaogang},
  booktitle={AAAI},
  year={2021}
}




@inproceedings{phuong2019distillation,
  title={Distillation-based training for multi-exit architectures},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1355--1364},
  year={2019}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@inproceedings{xin2021berxit,
  title={BERxiT: Early Exiting for BERT with Better fine-tuning and extension to regression},
  author={Xin, Ji and Tang, Raphael and Yu, Yaoliang and Lin, Jimmy},
  booktitle={Proceedings of the 16th conference of the European chapter of the association for computational linguistics: Main Volume},
  pages={91--104},
  year={2021}
}
@article{fan2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1909.11556},
  year={2019}
}
@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{kaya2019shallow,
  title={Shallow-deep networks: Understanding and mitigating network overthinking},
  author={Kaya, Yigitcan and Hong, Sanghyun and Dumitras, Tudor},
  booktitle={International conference on machine learning},
  pages={3301--3310},
  year={2019},
  organization={PMLR}
}
@article{zhang2018overview,
  title={An overview of multi-task learning},
  author={Zhang, Yu and Yang, Qiang},
  journal={National Science Review},
  volume={5},
  number={1},
  pages={30--43},
  year={2018},
  publisher={Oxford University Press}
}




@article{xu2020bert,
  title={Bert-of-theseus: Compressing bert by progressive module replacing},
  author={Xu, Canwen and Zhou, Wangchunshu and Ge, Tao and Wei, Furu and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.02925},
  year={2020}
}
@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}
@article{gordon2020compressing,
  title={Compressing bert: Studying the effects of weight pruning on transfer learning},
  author={Gordon, Mitchell A and Duh, Kevin and Andrews, Nicholas},
  journal={arXiv preprint arXiv:2002.08307},
  year={2020}
}
@article{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}
@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}
@article{zhang2020ternarybert,
  title={Ternarybert: Distillation-aware ultra-low bit bert},
  author={Zhang, Wei and Hou, Lu and Yin, Yichun and Shang, Lifeng and Chen, Xiao and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2009.12812},
  year={2020}
}
@article{bai2020binarybert,
  title={Binarybert: Pushing the limit of bert quantization},
  author={Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  journal={arXiv preprint arXiv:2012.15701},
  year={2020}
}
@inproceedings{kim2021bert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}
@inproceedings{bolukbasi2017adaptive,
  title={Adaptive neural networks for efficient inference},
  author={Bolukbasi, Tolga and Wang, Joseph and Dekel, Ofer and Saligrama, Venkatesh},
  booktitle={International Conference on Machine Learning},
  pages={527--536},
  year={2017},
  organization={PMLR}
}
@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press}
}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-
drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.
In Proceedings of ICML, 2019

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}


@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}
@article{zhao2019uer,
  title={UER: An Open-Source Toolkit for Pre-training Models},
  author={Zhao, Zhe and Chen, Hui and Zhang, Jinbin and Zhao, Xin and Liu, Tao and Lu, Wei and Chen, Xi and Deng, Haotang and Ju, Qi and Du, Xiaoyong},
  journal={EMNLP-IJCNLP 2019},
  pages={241},
  year={2019}
}



@article{Wu2018UnsupervisedFL,
  title={Unsupervised Feature Learning via Non-parametric Instance Discrimination},
  author={Zhirong Wu and Yuanjun Xiong and Stella X. Yu and Dahua Lin},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={3733-3742}
}


@article{Oord2018RepresentationLW,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.03748}
}


@inproceedings{Tian2020ContrastiveMC,
  title={Contrastive Multiview Coding},
  author={Yonglong Tian and Dilip Krishnan and Phillip Isola},
  booktitle={ECCV},
  year={2020}
}

@article{He2020MomentumCF,
  title={Momentum Contrast for Unsupervised Visual Representation Learning},
  author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross B. Girshick},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={9726-9735}
}



@article{Chen2021ExploringSS,
  title={Exploring Simple Siamese Representation Learning},
  author={Xinlei Chen and Kaiming He},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={15745-15753}
}

@inproceedings{Wang2020UnderstandingCR,
  title={Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
  author={Tongzhou Wang and Phillip Isola},
  booktitle={ICML},
  year={2020}
}


@article{Gao2021SimCSESC,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08821}
}


@article{Jaiswal2020ASO,
  title={A Survey on Contrastive Self-supervised Learning},
  author={Ashish Jaiswal and Ashwin Ramesh Babu and Mohammad Zaki Zadeh and Debapriya Banerjee and Fillia Makedon},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.00362}
}



@article{Khosla2020SupervisedCL,
  title={Supervised Contrastive Learning},
  author={Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.11362}
}


@article{Gunel2021SupervisedCL,
  title={Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning},
  author={Beliz Gunel and Jingfei Du and Alexis Conneau and Ves Stoyanov},
  journal={ArXiv},
  year={2021},
  volume={abs/2011.01403}
}



@article{Teerapittayanon2016BranchyNetFI,
  title={BranchyNet: Fast inference via early exiting from deep neural networks},
  author={Surat Teerapittayanon and Bradley McDanel and H. T. Kung},
  journal={2016 23rd International Conference on Pattern Recognition (ICPR)},
  year={2016},
  pages={2464-2469}
}





@article{Li2019ImprovedTF,
  title={Improved Techniques for Training Adaptive Deep Networks},
  author={Hao Li and Hong Zhang and Xiaojuan Qi and Ruigang Yang and Gao Huang},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={1891-1900}
}





@inproceedings{Kaya2019ShallowDeepNU,
	title={Shallow-Deep Networks: Understanding and Mitigating Network Overthinking},
	author={Y. Kaya and Sanghyun Hong and T. Dumitras},
	booktitle={ICML},
	year={2019}
}



@article{Zhou2020PABEE,
	title={BERT Loses Patience: Fast and Robust Inference with Early Exit},
	author={Wangchunshu Zhou and Canwen Xu and Tao Ge and Julian McAuley and Ke Xu and Furu Wei},
	journal={ArXiv},
	year={2020},
	volume={abs/2006.04152}
}


@inproceedings{Zhu2021LeeBERTLE,
  title={LeeBERT: Learned Early Exit for BERT with cross-level optimization},
  author={Wei Zhu},
  booktitle={ACL},
  year={2021}
}


@inproceedings{Zhu2021GAMLBERTIB,
  title={GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning},
  author={Wei Zhu and Xiaoling Wang and Yuan Ni and Guo Tong Xie},
  booktitle={EMNLP},
  year={2021}
}




@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}



@article{Hadsell2006DimensionalityRB,
  title={Dimensionality Reduction by Learning an Invariant Mapping},
  author={Raia Hadsell and Sumit Chopra and Yann LeCun},
  journal={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  year={2006},
  volume={2},
  pages={1735-1742}
}


@article{Oord2018RepresentationLW,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.03748}
}



@article{Fang2020CERTCS,
  title={CERT: Contrastive Self-supervised Learning for Language Understanding},
  author={Hongchao Fang and Sicheng Wang and Meng Zhou and Jiayuan Ding and Pengtao Xie},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.12766}
}


@article{Tambe2020EdgeBERTOO,
  title={EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP},
  author={Thierry Tambe and Coleman Hooper and Lillian Pentecost and En-Yu Yang and Marco Donato and Victor Sanh and Alexander M. Rush and David M. Brooks and Gu-Yeon Wei},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.14203}
}


@inproceedings{sun-etal-2022-simple,
    title = "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation",
    author = "Sun, Tianxiang  and
      Liu, Xiangyang  and
      Zhu, Wei  and
      Geng, Zhichao  and
      Wu, Lingling  and
      He, Yilong  and
      Ni, Yuan  and
      Xie, Guotong  and
      Huang, Xuanjing  and
      Qiu, Xipeng",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.189",
    doi = "10.18653/v1/2022.findings-acl.189",
    pages = "2409--2421",
    abstract = "Early exiting allows instances to exit at different layers according to the estimation of difficulty.Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning. In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way. Though some effort has been devoted to employing such {``}learn-to-exit{''} modules, it is still unknown whether and how well the instance difficulty can be learned. As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty. Based on this observation, we propose a simple-yet-effective Hash-based Early Exiting approach HashEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. Different from previous methods, HashEE requires no internal classifiers nor extra parameters, and therefore is more efficient.HashEE can be used in various tasks (including language understanding and generation) and model architectures such as seq2seq models. Experimental results on classification, regression, and generation tasks demonstrate that HashEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.",
}




@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}

@inproceedings{Xu2020BERTofTheseusCB,
  title={BERT-of-Theseus: Compressing BERT by Progressive Module Replacing},
  author={Canwen Xu and Wangchunshu Zhou and Tao Ge and Furu Wei and Ming Zhou},
  booktitle={EMNLP},
  year={2020}
}

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}


@article{Fan2020ReducingTD,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Angela Fan and Edouard Grave and Armand Joulin},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.11556}
}


@inproceedings{Michel2019AreSH,
  title={Are Sixteen Heads Really Better than One?},
  author={Paul Michel and Omer Levy and Graham Neubig},
  booktitle={NeurIPS},
  year={2019}
}


@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}

@article{Jiao2020TinyBERTDB,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.10351}
}




@article{Xu2021ASO,
  title={A Survey on Green Deep Learning},
  author={Jingjing Xu and Wangchunshu Zhou and Zhiyi Fu and Hao Zhou and Lei Li},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.05193}
}

@article{Lin2021ASO,
  title={A Survey of Transformers},
  author={Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.04554}
}





@inproceedings{Schuster2021ConsistentAI,
  title={Consistent Accelerated Inference via Confident Adaptive Transformers},
  author={Tal Schuster and Adam Fisch and T. Jaakkola and Regina Barzilay},
  booktitle={EMNLP},
  year={2021}
}



@article{Krizhevsky2012ImageNetCW,
  title={ImageNet classification with deep convolutional neural networks},
  author={Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  journal={Communications of the ACM},
  year={2012},
  volume={60},
  pages={84 - 90}
}



@article{Simonyan2015VeryDC,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2015},
  volume={abs/1409.1556}
}


@article{He2016DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}



@article{Huang2017DenselyCC,
  title={Densely Connected Convolutional Networks},
  author={Gao Huang and Zhuang Liu and Kilian Q. Weinberger},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={2261-2269}
}



@article{Wang2022DeepNetST,
  title={DeepNet: Scaling Transformers to 1, 000 Layers},
  author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.00555}
}





@article{He2021AutoMLAS,
  title={AutoML: A Survey of the State-of-the-Art},
  author={Xin He and Kaiyong Zhao and Xiaowen Chu},
  journal={Knowl. Based Syst.},
  year={2021},
  volume={212},
  pages={106622}
}





@article{Real2017LargeScaleEO,
  title={Large-Scale Evolution of Image Classifiers},
  author={Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka Leon Suematsu and Jie Tan and Quoc V. Le and Alexey Kurakin},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.01041}
}


@article{Xie2017GeneticC,
  title={Genetic CNN},
  author={Lingxi Xie and Alan Loddon Yuille},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={1388-1397}
}



@article{Brock2018SMASHOM,
  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},
  author={Andrew Brock and Theodore Lim and James M. Ritchie and Nick Weston},
  journal={ArXiv},
  year={2018},
  volume={abs/1708.05344}
}



@inproceedings{Cai2018EfficientAS,
  title={Efficient Architecture Search by Network Transformation},
  author={Han Cai and Tianyao Chen and Weinan Zhang and Yong Yu and Jun Wang},
  booktitle={AAAI},
  year={2018}
}


@inproceedings{Pham2018EfficientNA,
  title={Efficient Neural Architecture Search via Parameter Sharing},
  author={Hieu Pham and Melody Y. Guan and Barret Zoph and Quoc V. Le and Jeff Dean},
  booktitle={ICML},
  year={2018}
}




@article{Chu2020NoisyDA,
  title={Noisy Differentiable Architecture Search},
  author={Xiangxiang Chu and Bo Zhang and Xudong Li},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.03566}
}




@article{Zhang2021AutomaticSN,
  title={Automatic Student Network Search for Knowledge Distillation},
  author={Zhexi Zhang and Wei Zhu and Junchi Yan and Peng Gao and Guowang Xie},
  journal={2020 25th International Conference on Pattern Recognition (ICPR)},
  year={2021},
  pages={2446-2453}
}




@inproceedings{Chen2020AdaBERTTB,
  title={AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search},
  author={Daoyuan Chen and Yaliang Li and Minghui Qiu and Zhen Wang and Bofang Li and Bolin Ding and Hongbo Deng and Jun Huang and Wei Lin and Jingren Zhou},
  booktitle={IJCAI},
  year={2020}
}


@article{Xu2021NASBERTTA,
  title={NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search},
  author={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Jian Li and Tao Qin and Tie-Yan Liu},
  journal={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  year={2021}
}



@inproceedings{Dong2021EfficientBERTPS,
  title={EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation},
  author={Chenhe Dong and Guangrun Wang and Hang Xu and Jiefeng Peng and Xiaozhe Ren and Xiaodan Liang},
  booktitle={EMNLP},
  year={2021}
}


@article{Agarap2018DeepLU,
  title={Deep Learning using Rectified Linear Units (ReLU)},
  author={Abien Fred Agarap},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.08375}
}


@article{Hendrycks2016GaussianEL,
  title={Gaussian Error Linear Units (GELUs)},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={arXiv: Learning},
  year={2016}
}


@article{Ramachandran2017SwishAS,
  title={Swish: a Self-Gated Activation Function},
  author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
  journal={arXiv: Neural and Evolutionary Computing},
  year={2017}
}


@article{So2019TheET,
  title={The Evolved Transformer},
  author={David R. So and Chen Liang and Quoc V. Le},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.11117}
}



@inproceedings{Gong2018InformationAV,
  title={Information Aggregation via Dynamic Routing for Sequence Encoding},
  author={Jingjing Gong and Xipeng Qiu and Shaojing Wang and Xuanjing Huang},
  booktitle={COLING},
  year={2018}
}


@inproceedings{Wang2020TextNASAN,
  title={TextNAS: A Neural Architecture Search Space tailored for Text Representation},
  author={Yujing Wang and Yaming Yang and Yiren Chen and Jing Bai and Ce Zhang and Guinan Su and Xiaoyu Kou and Yunhai Tong and Mao Yang and Lidong Zhou},
  booktitle={AAAI},
  year={2020}
}


@inproceedings{Tan2018MultiwayAN,
  title={Multiway Attention Networks for Modeling Sentence Pairs},
  author={Chuanqi Tan and Furu Wei and Wenhui Wang and Weifeng Lv and M. Zhou},
  booktitle={IJCAI},
  year={2018}
}



@article{Liang2021RDropRD,
  title={R-Drop: Regularized Dropout for Neural Networks},
  author={Xiaobo Liang and Lijun Wu and Juntao Li and Yue Wang and Qi Meng and Tao Qin and Wei Chen and M. Zhang and Tie-Yan Liu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.14448}
}




@article{Turc2019WellReadSL,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={arXiv: Computation and Language},
  year={2019}
}


@InProceedings{autotrans,
author="Zhu, Wei
and Wang, Xiaoling
and Ni, Yuan
and Xie, Guotong",
editor="Wang, Lu
and Feng, Yansong
and Hong, Yu
and He, Ruifang",
title="AutoTrans: Automating Transformer Design via Reinforced Architecture Search",
booktitle="Natural Language Processing and Chinese Computing",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="169--182",
abstract="Though the transformer architectures have shown dominance in many natural language understanding tasks, there are still unsolved issues for the training of transformer models, especially the need for a principled way of warm-up which has shown importance for stable training of a transformer, as well as whether the task at hand prefer to scale the attention product or not. In this paper, we empirically explore automating the design choices in the transformer model, i.e., how to set layer-norm, whether to scale, number of layers, number of heads, activation function, etc., so that one can obtain a transformer architecture that better suits the tasks at hand. RL is employed to navigate along search space, and special parameter sharing strategies are designed to accelerate the search. It is shown that sampling a proportion of training data per epoch during search help to improve the search quality. Experiments on the CoNLL03, Multi-30k and WMT-14 shows that the searched transformer model can outperform the standard transformers. In particular, we show that our learned model can be trained more robustly with large learning rates without warm-up.",
isbn="978-3-030-88480-2"
}



% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}



@inproceedings{zhu-etal-2021-gaml,
	title = "{GAML}-{BERT}: Improving {BERT} Early Exiting by Gradient Aligned Mutual Learning",
	author = "Zhu, Wei  and
	Wang, Xiaoling  and
	Ni, Yuan  and
	Xie, Guotong",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.242",
	pages = "3033--3044",
	abstract = "In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT{'}s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT{'}s early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods.",
}



@inproceedings{yu-etal-2020-named,
    title = "Named Entity Recognition as Dependency Parsing",
    author = "Yu, Juntao  and
      Bohnet, Bernd  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.577",
    doi = "10.18653/v1/2020.acl-main.577",
    pages = "6470--6476",
    abstract = "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.",
}




@inproceedings{xin-etal-2020-deebert,
    title = "{D}ee{BERT}: Dynamic Early Exiting for Accelerating {BERT} Inference",
    author = "Xin, Ji  and
      Tang, Raphael  and
      Lee, Jaejun  and
      Yu, Yaoliang  and
      Lin, Jimmy",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.204",
    doi = "10.18653/v1/2020.acl-main.204",
    pages = "2246--2251",
    abstract = "Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to {\textasciitilde}40{\%} inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT.",
}


@inproceedings{schwartz-etal-2020-right,
    title = "The Right Tool for the Job: Matching Model and Instance Complexities",
    author = "Schwartz, Roy  and
      Stanovsky, Gabriel  and
      Swayamdipta, Swabha  and
      Dodge, Jesse  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.593",
    doi = "10.18653/v1/2020.acl-main.593",
    pages = "6640--6651",
    abstract = "As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) {``}exit{''} from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.",
}


@article{GENIA,
author = {Kim, Jin-Dong and Ohta, Tomoko and Tateisi, Yuka and Tsujii, Jun'ichi},
year = {2003},
month = {02},
pages = {i180-2},
title = {GENIA corpus—A semantically annotated corpus for bio-textmining},
volume = {19 Suppl 1},
journal = {Bioinformatics (Oxford, England)},
doi = {10.1093/bioinformatics/btg1023}
}


@inproceedings{levow-2006-third,
    title = "The Third International {C}hinese Language Processing Bakeoff: Word Segmentation and Named Entity Recognition",
    author = "Levow, Gina-Anne",
    booktitle = "Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-0115",
    pages = "108--117",
}


@inproceedings{zhu-etal-2021-gaml,
    title = "{GAML}-{BERT}: Improving {BERT} Early Exiting by Gradient Aligned Mutual Learning",
    author = "Zhu, Wei  and
      Wang, Xiaoling  and
      Ni, Yuan  and
      Xie, Guotong",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.242",
    doi = "10.18653/v1/2021.emnlp-main.242",
    pages = "3033--3044",
    abstract = "In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT{'}s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT{'}s early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods.",
}



@inproceedings{zhu-2021-leebert,
    title = "{L}ee{BERT}: Learned Early Exit for {BERT} with cross-level optimization",
    author = "Zhu, Wei",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.231",
    doi = "10.18653/v1/2021.acl-long.231",
    pages = "2968--2980",
    abstract = "Pre-trained language models like BERT are performant in a wide range of natural language tasks. However, they are resource exhaustive and computationally expensive for industrial scenarios. Thus, early exits are adopted at each layer of BERT to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference. In this work, to improve efficiency without performance drop, we propose a novel training scheme called Learned Early Exit for BERT (LeeBERT). First, we ask each exit to learn from each other, rather than learning only from the last layer. Second, the weights of different loss terms are learned, thus balancing off different objectives. We formulate the optimization of LeeBERT as a bi-level optimization problem, and we propose a novel cross-level optimization (CLO) algorithm to improve the optimization results. Experiments on the GLUE benchmark show that our proposed methods improve the performance of the state-of-the-art (SOTA) early exit methods for pre-trained models.",
}


@article{Kaya2018HowTS,
  title={How to Stop Off-the-Shelf Deep Neural Networks from Overthinking},
  author={Yigitcan Kaya and Tudor Dumitras},
  journal={ArXiv},
  year={2018},
  volume={abs/1810.07052}
}



@article{HellingerNeueBD,
  title={Neue Begr{\"u}ndung der Theorie quadratischer Formen von unendlichvielen Ver{\"a}nderlichen.},
  author={Ernst Hellinger},
  journal={Journal f{\"u}r die reine und angewandte Mathematik},
  volume={1909},
  pages={210 - 271}
}


@inproceedings{Manning2002FoundationsOS,
  title={Foundations of statistical natural language processing},
  author={Christopher D. Manning and Hinrich Sch{\"u}tze},
  booktitle={SGMD},
  year={2002}
}



@article{Girosi1995RegularizationTA,
  title={Regularization Theory and Neural Networks Architectures},
  author={Federico Girosi and Michael J. Jones and Tomaso A. Poggio},
  journal={Neural Computation},
  year={1995},
  volume={7},
  pages={219-269}
}


@article{Ganaie2021EnsembleDL,
  title={Ensemble deep learning: A review},
  author={M. A. Ganaie and Minghui Hu and Muhammad Tanveer and Ponnuthurai Nagaratnam Suganthan},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.02395}
}



@article{Kim2003GENIAC,
  title={GENIA corpus - a semantically annotated corpus for bio-textmining},
  author={Jin-Dong Kim and Tomoko Ohta and Yuka Tateisi and Junichi Tsujii},
  journal={Bioinformatics},
  year={2003},
  volume={19 Suppl 1},
  pages={
          i180-2
        }
}

@inproceedings{levow-2006-third,
    title = "The Third International {C}hinese Language Processing Bakeoff: Word Segmentation and Named Entity Recognition",
    author = "Levow, Gina-Anne",
    booktitle = "Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-0115",
    pages = "108--117",
}


@inproceedings{lu-roth-2015-joint,
    title = "Joint Mention Extraction and Classification with Mention Hypergraphs",
    author = "Lu, Wei  and
      Roth, Dan",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1102",
    doi = "10.18653/v1/D15-1102",
    pages = "857--867",
}


@inproceedings{katiyar-cardie-2018-nested,
    title = "Nested Named Entity Recognition Revisited",
    author = "Katiyar, Arzoo  and
      Cardie, Claire",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1079",
    doi = "10.18653/v1/N18-1079",
    pages = "861--871",
    abstract = "We propose a novel recurrent neural network-based approach to simultaneously handle nested named entity recognition and nested entity mention detection. The model learns a hypergraph representation for nested entities using features extracted from a recurrent neural network. In evaluations on three standard data sets, we show that our approach significantly outperforms existing state-of-the-art methods, which are feature-based. The approach is also efficient: it operates linearly in the number of tokens and the number of possible output labels at any token. Finally, we present an extension of our model that jointly learns the head of each entity mention.",
}


@inproceedings{ma-hovy-2016-end,
    title = "End-to-end Sequence Labeling via Bi-directional {LSTM}-{CNN}s-{CRF}",
    author = "Ma, Xuezhe  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1101",
    doi = "10.18653/v1/P16-1101",
    pages = "1064--1074",
}


@inproceedings{Wu2019GlyceGF,
  title={Glyce: Glyph-vectors for Chinese Character Representations},
  author={Wei Wu and Yuxian Meng and Fei Wang and Qinghong Han and Muyu Li and Xiaoya Li and Jie Mei and Ping Nie and Xiaofei Sun and Jiwei Li},
  booktitle={NeurIPS},
  year={2019}
}


@article{Li2021AcceleratingBI,
  title={Accelerating BERT Inference for Sequence Labeling via Early-Exit},
  author={Xiaonan Li and Yunfan Shao and Tianxiang Sun and Hang Yan and Xipeng Qiu and Xuanjing Huang},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13878}
}


@article{Cui2020RevisitingPM,
  title={Revisiting Pre-Trained Models for Chinese Natural Language Processing},
  author={Yiming Cui and Wanxiang Che and Ting Liu and Bing Qin and Shijin Wang and Guoping Hu},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.13922}
}


@article{Sun2021EarlyEW,
  title={Early Exiting with Ensemble Internal Classifiers},
  author={Tianxiang Sun and Yunhua Zhou and Xiangyang Liu and Xinyu Zhang and Hao Jiang and Zhao Cao and Xuanjing Huang and Xipeng Qiu},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13792}
}





@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}




@inproceedings{Yang2019XLNetGA,
	title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
	author={Z. Yang and Zihang Dai and Yiming Yang and J. Carbonell and R. Salakhutdinov and Quoc V. Le},
	booktitle={NeurIPS},
	year={2019}
}


@article{Fan2020layerdrop,
	title={Reducing Transformer Depth on Demand with Structured Dropout},
	author={Angela Fan and E. Grave and Armand Joulin},
	journal={ArXiv},
	year={2020},
	volume={abs/1909.11556}
}


@inproceedings{Michel2019sixteenhead,
	title={Are Sixteen Heads Really Better than One?},
	author={Paul Michel and Omer Levy and Graham Neubig},
	booktitle={NeurIPS},
	year={2019}
}





@inproceedings{Kaya2019ShallowDeepNU,
	title={Shallow-Deep Networks: Understanding and Mitigating Network Overthinking},
	author={Y. Kaya and Sanghyun Hong and T. Dumitras},
	booktitle={ICML},
	year={2019}
}

@article{Zhu2018ToPO,
	title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
	author={M. Zhu and S. Gupta},
	journal={ArXiv},
	year={2018},
	volume={abs/1710.01878}
}


@inproceedings{Xu2020BERTofTheseusCB,
	title={BERT-of-Theseus: Compressing BERT by Progressive Module Replacing},
	author={Canwen Xu and Wangchunshu Zhou and Tao Ge and Furu Wei and M. Zhou},
	booktitle={EMNLP},
	year={2020}
}

@inproceedings{Sun2019PatientKD,
	title={Patient Knowledge Distillation for BERT Model Compression},
	author={S. Sun and Yu Cheng and Zhe Gan and Jingjing Liu},
	booktitle={EMNLP/IJCNLP},
	year={2019}
}

@article{Sanh2019DistilBERTAD,
	title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
	author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	journal={ArXiv},
	year={2019},
	volume={abs/1910.01108}
}


@article{Jiao2020TinyBERTDB,
	title={TinyBERT: Distilling BERT for Natural Language Understanding},
	author={Xiaoqi Jiao and Y. Yin and L. Shang and Xin Jiang and X. Chen and Linlin Li and F. Wang and Qun Liu},
	journal={ArXiv},
	year={2020},
	volume={abs/1909.10351}
}


@article{Zhang2020TernaryBERTDU,
	title={TernaryBERT: Distillation-aware Ultra-low Bit BERT},
	author={W. Zhang and L. Hou and Y. Yin and L. Shang and X. Chen and X. Jiang and Qun Liu},
	journal={ArXiv},
	year={2020},
	volume={abs/2009.12812}
}

@article{Bai2020BinaryBERTPT,
	title={BinaryBERT: Pushing the Limit of BERT Quantization},
	author={Haoli Bai and Wei Zhang and L. Hou and L. Shang and Jing Jin and X. Jiang and Qun Liu and Michael R. Lyu and Irwin King},
	journal={ArXiv},
	year={2020},
	volume={abs/2012.15701}
}

@article{Kim2021IBERTIB,
	title={I-BERT: Integer-only BERT Quantization},
	author={Se-Hoon Kim and Amir Gholami and Zhewei Yao and M. W. Mahoney and K. Keutzer},
	journal={ArXiv},
	year={2021},
	volume={abs/2101.01321}
}




@inproceedings{Geng2021RomeBERTRT,
	title={RomeBERT: Robust Training of Multi-Exit BERT},
	author={Shijie Geng and Peng Gao and Z. Fu and Yongfeng Zhang},
	year={2021}
}


@article{Liu2020FastBERTAS,
	title={FastBERT: a Self-distilling BERT with Adaptive Inference Time},
	author={Weijie Liu and P. Zhou and Zhe Zhao and Zhiruo Wang and Haotang Deng and Q. Ju},
	journal={ArXiv},
	year={2020},
	volume={abs/2004.02178}
}


@inproceedings{Bolukbasi2017AdaptiveNN,
	title={Adaptive Neural Networks for Efficient Inference},
	author={Tolga Bolukbasi and J. Wang and O. Dekel and Venkatesh Saligrama},
	booktitle={ICML},
	year={2017}
}



@article{Phuong2019DistillationBasedTF,
	title={Distillation-Based Training for Multi-Exit Architectures},
	author={Mary Phuong and Christoph H. Lampert},
	journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
	year={2019},
	pages={1355-1364}
}



@article{Yu2020GradientSF,
	title={Gradient Surgery for Multi-Task Learning},
	author={Tianhe Yu and Saurabh Kumar and A. Gupta and S. Levine and Karol Hausman and Chelsea Finn},
	journal={ArXiv},
	year={2020},
	volume={abs/2001.06782}
}







@article{Hinton2015DistillingTK,
	title={Distilling the Knowledge in a Neural Network},
	author={Geoffrey E. Hinton and Oriol Vinyals and J. Dean},
	journal={ArXiv},
	year={2015},
	volume={abs/1503.02531}
}





@article{Turc2019WellReadSL,
	title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
	author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	journal={arXiv: Computation and Language},
	year={2019}
}


@inproceedings{Krizhevsky2009LearningML,
	title={Learning Multiple Layers of Features from Tiny Images},
	author={A. Krizhevsky},
	year={2009}
}


@article{He2016DeepRL,
	title={Deep Residual Learning for Image Recognition},
	author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
	journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2016},
	pages={770-778}
}


@article{Lan2020ALBERTAL,
	title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
	author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
	journal={ArXiv},
	year={2020},
	volume={abs/1909.11942}
}

@inproceedings{MRPC,
	title = "Automatically Constructing a Corpus of Sentential Paraphrases",
	author = "Dolan, William B.  and
	Brockett, Chris",
	booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
	year = "2005",
	url = "https://www.aclweb.org/anthology/I05-5002",
}


@inproceedings{socher-etal-2013-recursive,
	title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
	author = "Socher, Richard  and
	Perelygin, Alex  and
	Wu, Jean  and
	Chuang, Jason  and
	Manning, Christopher D.  and
	Ng, Andrew  and
	Potts, Christopher",
	booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
	month = oct,
	year = "2013",
	address = "Seattle, Washington, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D13-1170",
	pages = "1631--1642",
}



@inproceedings{williams-etal-2018-broad,
	title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
	author = "Williams, Adina  and
	Nangia, Nikita  and
	Bowman, Samuel",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N18-1101",
	doi = "10.18653/v1/N18-1101",
	pages = "1112--1122",
	abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}






@article{warstadt-etal-2019-neural,
	title = "Neural Network Acceptability Judgments",
	author = "Warstadt, Alex  and
	Singh, Amanpreet  and
	Bowman, Samuel R.",
	journal = "Transactions of the Association for Computational Linguistics",
	volume = "7",
	month = mar,
	year = "2019",
	url = "https://www.aclweb.org/anthology/Q19-1040",
	doi = "10.1162/tacl_a_00290",
	pages = "625--641",
	abstract = "This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.{'}s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.",
}


@inproceedings{wnli,
	author = {Levesque, Hector J. and Davis, Ernest and Morgenstern, Leora},
	title = {The Winograd Schema Challenge},
	year = {2012},
	isbn = {9781577355601},
	publisher = {AAAI Press},
	abstract = {In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Wino-grad schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.},
	booktitle = {Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning},
	pages = {552–561},
	numpages = {10},
	location = {Rome, Italy},
	series = {KR'12}
}


@inproceedings{devlin-etal-2019-bert,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N19-1423",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@inproceedings{jiao-etal-2020-tinybert,
	title = "{T}iny{BERT}: Distilling {BERT} for Natural Language Understanding",
	author = "Jiao, Xiaoqi  and
	Yin, Yichun  and
	Shang, Lifeng  and
	Jiang, Xin  and
	Chen, Xiao  and
	Li, Linlin  and
	Wang, Fang  and
	Liu, Qun",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/2020.findings-emnlp.372",
	doi = "10.18653/v1/2020.findings-emnlp.372",
	pages = "4163--4174",
	abstract = "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large {``}teacher{''} BERT can be effectively transferred to a small {``}student{''} TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8{\%} the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only {\textasciitilde}28{\%} parameters and {\textasciitilde}31{\%} inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
}



@inproceedings{xu-etal-2020-bert,
	title = "{BERT}-of-Theseus: Compressing {BERT} by Progressive Module Replacing",
	author = "Xu, Canwen  and
	Zhou, Wangchunshu  and
	Ge, Tao  and
	Wei, Furu  and
	Zhou, Ming",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/2020.emnlp-main.633",
	doi = "10.18653/v1/2020.emnlp-main.633",
	pages = "7859--7869",
	abstract = "In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.",
}






@article{Hinton2015DistillingTK,
	title={Distilling the Knowledge in a Neural Network},
	author={Geoffrey E. Hinton and Oriol Vinyals and J. Dean},
	journal={ArXiv},
	year={2015},
	volume={abs/1503.02531}
}





@article{Huang2017MultiScaleDC,
	title={Multi-Scale Dense Convolutional Networks for Efficient Prediction},
	author={Gao Huang and Danlu Chen and T. Li and Felix Wu and L. V. D. Maaten and Kilian Q. Weinberger},
	journal={ArXiv},
	year={2017},
	volume={abs/1703.09844}
}




@article{Szegedy2016RethinkingTI,
	title={Rethinking the Inception Architecture for Computer Vision},
	author={Christian Szegedy and V. Vanhoucke and S. Ioffe and Jonathon Shlens and Z. Wojna},
	journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2016},
	pages={2818-2826}
}


@inproceedings{Yang2020TextBrewerAO,
	title={TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing},
	author={Ziqing Yang and Yiming Cui and ZhiPeng Chen and Wanxiang Che and T. Liu and S. Wang and Guoping Hu},
	booktitle={ACL},
	year={2020}
}


@inproceedings{Sun2019PatientKD,
	title={Patient Knowledge Distillation for BERT Model Compression},
	author={S. Sun and Yu Cheng and Zhe Gan and Jingjing Liu},
	booktitle={EMNLP/IJCNLP},
	year={2019}
}


@article{Li2021AcceleratingBI,
  title={Accelerating BERT Inference for Sequence Labeling via Early-Exit},
  author={Xiaonan Li and Yunfan Shao and Tianxiang Sun and Hang Yan and Xipeng Qiu and Xuanjing Huang},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13878}
}


@article{Kullback1951OnIA,
  title={On Information and Sufficiency},
  author={Solomon Kullback and R. A. Leibler},
  journal={Annals of Mathematical Statistics},
  year={1951},
  volume={22},
  pages={79-86}
}



@inproceedings{Sun2020LearningSS,
  title={Learning Sparse Sharing Architectures for Multiple Tasks},
  author={Tianxiang Sun and Yunfan Shao and Xiaonan Li and Pengfei Liu and Hang Yan and Xipeng Qiu and Xuanjing Huang},
  booktitle={AAAI},
  year={2020}
}



@article{Hendrycks2016BridgingNA,
  title={Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.08415}
}



@article{Huang2015BidirectionalLM,
  title={Bidirectional LSTM-CRF Models for Sequence Tagging},
  author={Zhiheng Huang and Wei Xu and Kai Yu},
  journal={ArXiv},
  year={2015},
  volume={abs/1508.01991}
}


@article{Li2019ImprovedTF,
  title={Improved Techniques for Training Adaptive Deep Networks},
  author={Hao Li and Hong Zhang and Xiaojuan Qi and Ruigang Yang and Gao Huang},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={1891-1900}
}


@inproceedings{liao-etal-2021-global,
    title = "A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models",
    author = "Liao, Kaiyuan  and
      Zhang, Yi  and
      Ren, Xuancheng  and
      Su, Qi  and
      Sun, Xu  and
      He, Bin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.162",
    doi = "10.18653/v1/2021.naacl-main.162",
    pages = "2013--2023",
    abstract = "Early exit mechanism aims to accelerate the inference speed of large-scale pre-trained language models. The essential idea is to exit early without passing through all the inference layers at the inference stage. To make accurate predictions for downstream tasks, the hierarchical linguistic information embedded in all layers should be jointly considered. However, much of the research up to now has been limited to use local representations of the exit layer. Such treatment inevitably loses information of the unused past layers as well as the high-level features embedded in future layers, leading to sub-optimal performance. To address this issue, we propose a novel Past-Future method to make comprehensive predictions from a global perspective. We first take into consideration all the linguistic information embedded in the past layers and then take a further step to engage the future information which is originally inaccessible for predictions. Extensive experiments demonstrate that our method outperforms previous early exit methods by a large margin, yielding better and robust performance.",
}



@article{Sun2021EarlyEW,
  title={Early Exiting with Ensemble Internal Classifiers},
  author={Tianxiang Sun and Yunhua Zhou and Xiangyang Liu and Xinyu Zhang and Hao Jiang and Zhao Cao and Xuanjing Huang and Xipeng Qiu},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13792}
}



@inproceedings{zhang-etal-2022-pcee,
    title = "{PCEE}-{BERT}: Accelerating {BERT} Inference via Patient and Confident Early Exiting",
    author = "Zhang, Zhen  and
      Zhu, Wei  and
      Zhang, Jinfan  and
      Wang, Peng  and
      Jin, Rize  and
      Chung, Tae-Sun",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.25",
    doi = "10.18653/v1/2022.findings-naacl.25",
    pages = "327--338",
    abstract = "BERT and other pretrained language models (PLMs) are ubiquitous in modern NLP. Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (CITATION), the significant latency during inference prohibits wider industrial usage. In this work, we propose Patient and Confident Early Exiting BERT (PCEE-BERT), an off-the-shelf sample-dependent early exiting method that can work with different PLMs and can also work along with popular model compression methods. With a multi-exit BERT as the backbone model, PCEE-BERT will make the early exiting decision if enough numbers (patience parameter) of consecutive intermediate layers are confident about their predictions. The entropy value measures the confidence level of an intermediate layer{'}s prediction. Experiments on the GLUE benchmark demonstrate that our method outperforms previous SOTA early exiting methods. Ablation studies show that: (a) our method performs consistently well on other PLMs, such as ALBERT and TinyBERT; (b) PCEE-BERT can achieve different speed-up ratios by adjusting the patience parameter and the confidence threshold. The code for PCEE-BERT can be found at \url{https://github.com/michael-wzhu/PCEE-BERT}.",
}


@inproceedings{Li2021CascadeBERTAI,
  title={CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade},
  author={Lei Li and Yankai Lin and Deli Chen and Shuhuai Ren and Peng Li and Jie Zhou and Xu Sun},
  booktitle={EMNLP},
  year={2021}
}


@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}




@article{He2021TowardsAU,
  title={Towards a Unified View of Parameter-Efficient Transfer Learning},
  author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.04366}
}



@inproceedings{liu-etal-2022-towards-efficient,
    title = "Towards Efficient {NLP}: A Standard Evaluation and A Strong Baseline",
    author = "Liu, Xiangyang  and
      Sun, Tianxiang  and
      He, Junliang  and
      Wu, Jiawen  and
      Wu, Lingling  and
      Zhang, Xinyu  and
      Jiang, Hao  and
      Cao, Zhao  and
      Huang, Xuanjing  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.240",
    doi = "10.18653/v1/2022.naacl-main.240",
    pages = "3288--3303",
    abstract = "Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depicting the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.",
}


@article{Hendrycks2016BridgingNA,
  title={Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.08415}
}


@article{Fukushima2004CognitronAS,
  title={Cognitron: A self-organizing multilayered neural network},
  author={Kunihiko Fukushima},
  journal={Biological Cybernetics},
  year={2004},
  volume={20},
  pages={121-136}
}



@article{Ramachandran2017SwishAS,
  title={Swish: a Self-Gated Activation Function},
  author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
  journal={arXiv: Neural and Evolutionary Computing},
  year={2017}
}


@article{Hochreiter1997lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}





@article{Elbayad2020DepthAdaptiveT,
  title={Depth-Adaptive Transformer},
  author={Maha Elbayad and Jiatao Gu and Edouard Grave and Michael Auli},
  journal={ArXiv},
  year={2020},
  volume={abs/1910.10073}
}




@inproceedings{Schuster2021ConsistentAI,
  title={Consistent Accelerated Inference via Confident Adaptive Transformers},
  author={Tal Schuster and Adam Fisch and T. Jaakkola and Regina Barzilay},
  booktitle={EMNLP},
  year={2021}
}


@article{Han2021PreTrainedMP,
  title={Pre-Trained Models: Past, Present and Future},
  author={Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.07139}
}


@inproceedings{Peters2018DeepCW,
  title={Deep Contextualized Word Representations},
  author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2018}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}


@article{Raffel2019ExploringTL,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.10683}
}


@article{Wang2021ERNIE3T,
  title={ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation},
  author={Shuohuan Wang and Yu Sun and Yang Xiang and Zhihua Wu and Siyu Ding and Weibao Gong and Shi Feng and Junyuan Shang and Yanbin Zhao and Chao Pang and Jiaxiang Liu and Xuyi Chen and Yuxiang Lu and Weixin Liu and Xi Wang and Yangfan Bai and Qiuliang Chen and Li Zhao and Shiyong Li and Peng Sun and Dianhai Yu and Yanjun Ma and Hao Tian and Hua Wu and Tian Wu and Wei Zeng and Ge Li and Wen Gao and Haifeng Wang},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.12731}
}

@article{Ding2022DeltaTA,
  title={Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models},
  author={Ning Ding and Yujia Qin and Guang Yang and Fu Wei and Zonghan Yang and Yusheng Su and Shengding Hu and Yulin Chen and Chi-Min Chan and Weize Chen and Jing Yi and Weilin Zhao and Xiaozhi Wang and Zhiyuan Liu and Haitao Zheng and Jianfei Chen and Yang Liu and Jie Tang and Juan Li and Maosong Sun},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.06904}
}

@inproceedings{pfeiffer-etal-2021-adapterfusion,
    title = "{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning",
    author = {Pfeiffer, Jonas  and
      Kamath, Aishwarya  and
      R{\"u}ckl{\'e}, Andreas  and
      Cho, Kyunghyun  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.39",
    doi = "10.18653/v1/2021.eacl-main.39",
    pages = "487--503",
    abstract = "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.",
}


@inproceedings{Mahabadi2021CompacterEL,
  title={Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
  author={Rabeeh Karimi Mahabadi and James Henderson and Sebastian Ruder},
  booktitle={NeurIPS},
  year={2021}
}


@article{BenZaken2021BitFitSP,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Elad Ben-Zaken and Shauli Ravfogel and Yoav Goldberg},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.10199}
}

@article{Liu2021PTuningVP,
  title={P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks},
  author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Zhengxiao Du and Zhilin Yang and Jie Tang},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.07602}
}


@article{Sun2022BlackBoxTF,
  title={Black-Box Tuning for Language-Model-as-a-Service},
  author={Tianxiang Sun and Yunfan Shao and Hong Qian and Xuanjing Huang and Xipeng Qiu},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.03514}
}


@inproceedings{Wu2022IDPGAI,
  title={IDPG: An Instance-Dependent Prompt Generation Method},
  author={Zhuofeng Wu and Sinong Wang and Jiatao Gu and Rui Hou and Yuxiao Dong and V. G. Vinod Vydiswaran and Hao Ma},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2022}
}

@inproceedings{Le2021ParameterizedHG,
  title={Parameterized Hypercomplex Graph Neural Networks for Graph Classification},
  author={Tuan Le and Marco Bertolini and Frank No'e and Djork-Arn{\'e} Clevert},
  booktitle={International Conference on Artificial Neural Networks},
  year={2021}
}


@article{Liu2022LatePT,
  title={Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts},
  author={Xiangyang Liu and Tianxiang Sun and Xuanjing Huang and Xipeng Qiu},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11292}
}



@inproceedings{Liu2022PTuningPT,
  title={P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks},
  author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022}
}


@article{Li2021PrefixTuningOC,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Xiang Lisa Li and Percy Liang},
  journal={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year={2021},
  volume={abs/2101.00190}
}



@inproceedings{Tang2022ContextTuningLC,
  title={Context-Tuning: Learning Contextualized Prompts for Natural Language Generation},
  author={Tianyi Tang and Junyi Li and Wayne Xin Zhao},
  booktitle={International Conference on Computational Linguistics},
  year={2022}
}


@article{Jin2022InstanceawarePL,
  title={Instance-aware Prompt Learning for Language Understanding and Generation},
  author={Feihu Jin and Jinliang Lu and Jiajun Zhang and Chengqing Zong},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.07126}
}


@inproceedings{Rckl2020AdapterDropOT,
  title={AdapterDrop: On the Efficiency of Adapters in Transformers},
  author={Andreas R{\"u}ckl{\'e} and Gregor Geigle and Max Glockner and Tilman Beck and Jonas Pfeiffer and Nils Reimers and Iryna Gurevych},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020}
}


@inproceedings{Wang2021LiSTLP,
  title={LiST: Lite Prompted Self-training Makes Parameter-efficient Few-shot Learners},
  author={Yaqing Wang and Subhabrata Mukherjee and Xiaodong Liu and Jing Gao and Ahmed Hassan Awadallah and Jianfeng Gao},
  booktitle={NAACL-HLT},
  year={2021}
}


@inproceedings{voorhees-tice-2000-trec,
    title = "The {TREC}-8 Question Answering Track",
    author = "Voorhees, Ellen M.  and
      Tice, Dawn M.",
    booktitle = "Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00)",
    month = may,
    year = "2000",
    address = "Athens, Greece",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2000/pdf/26.pdf",
}


@inproceedings{rte,
author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
year = {2005},
month = {01},
pages = {177-190},
title = {The PASCAL recognising textual entailment challenge},
isbn = {978-3-540-33427-9},
doi = {10.1007/11736790_9}
}

@inproceedings{guo-etal-2021-parameter,
    title = "Parameter-Efficient Transfer Learning with Diff Pruning",
    author = "Guo, Demi  and
      Rush, Alexander  and
      Kim, Yoon",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.378",
    doi = "10.18653/v1/2021.acl-long.378",
    pages = "4884--4896",
    abstract = "The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. Diff pruning enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a task-specific {``}diff{''} vector that extends the original pretrained parameters. This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. As the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task. Since it does not require access to all tasks during training, it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers. Diff pruning can match the performance of finetuned baselines on the GLUE benchmark while only modifying 0.5{\%} of the pretrained model{'}s parameters per task and scales favorably in comparison to popular pruning approaches.",
}


@inproceedings{Moosavi2022AdaptableA,
  title={Adaptable Adapters},
  author={Nafise Sadat Moosavi and Quentin Delfosse and Kristian Kersting and Iryna Gurevych},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2022}
}


@article{Jie2022ConvolutionalBA,
  title={Convolutional Bypasses Are Better Vision Transformer Adapters},
  author={Shibo Jie and Zhifang Deng},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.07039}
}



@article{Sung2022LSTLS,
  title={LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning},
  author={Yi-Lin Sung and Jaemin Cho and Mohit Bansal},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.06522}
}


@inproceedings{Krizhevsky2009LearningML,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009}
}


@article{Wang2019SuperGLUEAS,
  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.00537}
}



@inproceedings{dolan-brockett-2005-automatically,
    title = "Automatically Constructing a Corpus of Sentential Paraphrases",
    author = "Dolan, William B.  and
      Brockett, Chris",
    booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
    year = "2005",
    url = "https://aclanthology.org/I05-5002",
}


@inproceedings{Levesque2011TheWS,
  title={The Winograd Schema Challenge},
  author={Hector J. Levesque and Ernest Davis and L. Morgenstern},
  booktitle={International Conference on Principles of Knowledge Representation and Reasoning},
  year={2011}
}


@article{Zhang2020RevisitingFB,
  title={Revisiting Few-sample BERT Fine-tuning},
  author={Tianyi Zhang and Felix Wu and Arzoo Katiyar and Kilian Q. Weinberger and Yoav Artzi},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.05987}
}


@article{He2020DeBERTaDB,
  title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.03654}
}




@article{Wang2022AdaMixMF,
  title={AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning},
  author={Yaqing Wang and Subhabrata Mukherjee and Xiaodong Liu and Jing Gao and Jianfeng Gao},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.17451}
}



@inproceedings{aghajanyan-etal-2021-intrinsic,
    title = "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
    author = "Aghajanyan, Armen  and
      Gupta, Sonal  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.568",
    doi = "10.18653/v1/2021.acl-long.568",
    pages = "7319--7328",
    abstract = "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90{\%} of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.",
}



@article{Zoph2017LearningTA,
  title={Learning Transferable Architectures for Scalable Image Recognition},
  author={Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={8697-8710}
}


@inproceedings{Real2018RegularizedEF,
  title={Regularized Evolution for Image Classifier Architecture Search},
  author={Esteban Real and Alok Aggarwal and Yanping Huang and Quoc V. Le},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2018}
}






@article{Xie2018SNASSN,
  title={SNAS: Stochastic Neural Architecture Search},
  author={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.09926}
}



@article{Dong2019SearchingFA,
  title={Searching for a Robust Neural Architecture in Four GPU Hours},
  author={Xuanyi Dong and Yezhou Yang},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={1761-1770}
}


@article{Zhou2019BayesNASAB,
  title={BayesNAS: A Bayesian Approach for Neural Architecture Search},
  author={Hongpeng Zhou and Minghao Yang and Jun Wang and Wei Pan},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.04919}
}



@article{Zela2019UnderstandingAR,
  title={Understanding and Robustifying Differentiable Architecture Search},
  author={Arber Zela and Thomas Elsken and Tonmoy Saikia and Yassine Marrakchi and Thomas Brox and Frank Hutter},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.09656}
}



@article{Hu2022SparseSS,
  title={Sparse Structure Search for Parameter-Efficient Tuning},
  author={Shengding Hu and Zhen Zhang and Ning Ding and Yadao Wang and Yasheng Wang and Zhiyuan Liu and Maosong Sun},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.07382}
}


@article{Mao2021UniPELTAU,
  title={UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning},
  author={Yuning Mao and Lambert Mathias and Rui Hou and Amjad Almahairi and Hao Ma and Jiawei Han and Wen-tau Yih and Madian Khabsa},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.07577}
}



@article{He2015DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={770-778}
}





@ARTICLE{2019arXiv191011831B,
       author = {{Bi}, Kaifeng and {Hu}, Changping and {Xie}, Lingxi and {Chen}, Xin and {Wei}, Longhui and {Tian}, Qi},
        title = "{Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
         year = 2019,
        month = oct,
          eid = {arXiv:1910.11831},
        pages = {arXiv:1910.11831},
          doi = {10.48550/arXiv.1910.11831},
archivePrefix = {arXiv},
       eprint = {1910.11831},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191011831B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{Bi2020GOLDNASGO,
  title={GOLD-NAS: Gradual, One-Level, Differentiable},
  author={Kaifeng Bi and Lingxi Xie and Xin Chen and Longhui Wei and Qi Tian},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.03331}
}



@ARTICLE{2021arXiv210109671L,
       author = {{Liang}, Tailin and {Glossner}, John and {Wang}, Lei and {Shi}, Shaobo and {Zhang}, Xiaotong},
        title = "{Pruning and Quantization for Deep Neural Network Acceleration: A Survey}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
         year = 2021,
        month = jan,
          eid = {arXiv:2101.09671},
        pages = {arXiv:2101.09671},
          doi = {10.48550/arXiv.2101.09671},
archivePrefix = {arXiv},
       eprint = {2101.09671},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210109671L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}





@inproceedings{zhang2021automatic,
  title={Automatic Student Network Search for Knowledge Distillation},
  author={Zhang, Zhexi and Zhu, Wei and Yan, Junchi and Gao, Peng and Xie, Guotong},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={2446--2453},
  year={2021},
  organization={IEEE}
}


@inproceedings{gao2023f,
  title={F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks},
  author={Gao, Xiangxiang and Zhu, Wei and Gao, Jiasheng and Yin, Congrui},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}


@INPROCEEDINGS{10094859,
  author={Zhu, Wei and Wang, Peng and Wang, Xiaoling and Ni, Yuan and Xie, Guotong},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={ACF: Aligned Contrastive Finetuning For Language and Vision Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICASSP49357.2023.10094859}}


  @inproceedings{li-etal-2019-pingan,
    title = "Pingan Smart Health and {SJTU} at {COIN} - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks",
    author = "Li, Xiepeng  and
      Zhang, Zhexi  and
      Zhu, Wei  and
      Li, Zheng  and
      Ni, Yuan  and
      Gao, Peng  and
      Yan, Junchi  and
      Xie, Guotong",
    booktitle = "Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6011",
    doi = "10.18653/v1/D19-6011",
    pages = "93--98",
    abstract = "To solve the shared tasks of COIN: COmmonsense INference in Natural Language Processing) Workshop in , we need explore the impact of knowledge representation in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching. There are two approaches to represent knowledge in the low-dimensional space. The first is to leverage large-scale unsupervised text corpus to train fixed or contextual language representations. The second approach is to explicitly express knowledge into a knowledge graph (KG), and then fit a model to represent the facts in the KG. We have experimented both (a) improving the fine-tuning of pre-trained language models on a task with a small dataset size, by leveraging datasets of similar tasks; and (b) incorporating the distributional representations of a KG onto the representations of pre-trained language models, via simply concatenation or multi-head attention. We find out that: (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target task improve the performance significantly; (b) for task 2, we find out the incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace of XLNET (Yangetal.,2019), a more powerful pre-trained model. Our approaches achieve the state-of-the-art results on both shared task{'}s official test data, outperforming all the other submissions.",
}




@inproceedings{zhu-2021-autorc,
    title = "{A}uto{RC}: Improving {BERT} Based Relation Classification Models via Architecture Search",
    author = "Zhu, Wei",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-srw.4",
    doi = "10.18653/v1/2021.acl-srw.4",
    pages = "33--43",
    abstract = "Although BERT based relation classification (RC) models have achieved significant improvements over the traditional deep learning models, it seems that no consensus can be reached on what is the optimal architecture, since there are many design choices available. In this work, we design a comprehensive search space for BERT based RC models and employ a modified version of efficient neural architecture search (ENAS) method to automatically discover the design choices mentioned above. Experiments on eight benchmark RC tasks show that our method is efficient and effective in finding better architectures than the baseline BERT based RC models. Ablation study demonstrates the necessity of our search space design and the effectiveness of our search method. We also show that our framework can also apply to other entity related tasks like coreference resolution and span based named entity recognition (NER).",
}



@inproceedings{zhu-etal-2021-discovering,
    title = "Discovering Better Model Architectures for Medical Query Understanding",
    author = "Zhu, Wei  and
      Ni, Yuan  and
      Wang, Xiaoling  and
      Xie, Guotong",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-industry.29",
    doi = "10.18653/v1/2021.naacl-industry.29",
    pages = "230--237",
    abstract = "In developing an online question-answering system for the medical domains, natural language inference (NLI) models play a central role in question matching and intention detection. However, which models are best for our datasets? Manually selecting or tuning a model is time-consuming. Thus we experiment with automatically optimizing the model architectures on the task at hand via neural architecture search (NAS). First, we formulate a novel architecture search space based on the previous NAS literature, supporting cross-sentence attention (cross-attn) modeling. Second, we propose to modify the ENAS method to accelerate and stabilize the search results. We conduct extensive experiments on our two medical NLI tasks. Results show that our system can easily outperform the classical baseline models. We compare different NAS methods and demonstrate our approach provides the best results.",
}







@inproceedings{zuo-etal-2022-continually,
    title = "Continually Detection, Rapidly React: Unseen Rumors Detection Based on Continual Prompt-Tuning",
    author = "Zuo, Yuhui  and
      Zhu, Wei  and
      Cai, Guoyong GUET",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.268",
    pages = "3029--3041",
    abstract = "Since open social platforms allow for a large and continuous flow of unverified information, rumors can emerge unexpectedly and spread quickly. However, existing rumor detection (RD) models often assume the same training and testing distributions and can not cope with the continuously changing social network environment. This paper proposed a Continual Prompt-Tuning RD (CPT-RD) framework, which avoids catastrophic forgetting (CF) of upstream tasks during sequential task learning and enables bidirectional knowledge transfer between domain tasks. Specifically, we propose the following strategies: (a) Our design explicitly decouples shared and domain-specific knowledge, thus reducing the interference among different domains during optimization; (b) Several technologies aim to transfer knowledge of upstream tasks to deal with emergencies; (c) A task-conditioned prompt-wise hypernetwork (TPHNet) is used to consolidate past domains. In addition, CPT-RD avoids CF without the necessity of a rehearsal buffer. Finally, CPT-RD is evaluated on English and Chinese RD datasets and is effective and efficient compared to prior state-of-the-art methods.",
}




@inproceedings{guo-etal-2021-global,
    title = "Global Attention Decoder for {C}hinese Spelling Error Correction",
    author = "Guo, Zhao  and
      Ni, Yuan  and
      Wang, Keqiang  and
      Zhu, Wei  and
      Xie, Guotong",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.122",
    doi = "10.18653/v1/2021.findings-acl.122",
    pages = "1419--1428",
}



@inproceedings{zhu-etal-2021-paht,
    title = "paht{\_}nlp @ {MEDIQA} 2021: Multi-grained Query Focused Multi-Answer Summarization",
    author = "Zhu, Wei  and
      He, Yilong  and
      Chai, Ling  and
      Fan, Yunxiao  and
      Ni, Yuan  and
      Xie, Guotong  and
      Wang, Xiaoling",
    booktitle = "Proceedings of the 20th Workshop on Biomedical Language Processing",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.bionlp-1.10",
    doi = "10.18653/v1/2021.bionlp-1.10",
    pages = "96--102",
    abstract = "In this article, we describe our systems for the MEDIQA 2021 Shared Tasks. First, we will describe our method for the second task, Multi-Answer Summarization (MAS). For extractive summarization, two series of methods are applied. The first one follows (CITATION). First a RoBERTa model is first applied to give a local ranking of the candidate sentences. Then a Markov Chain model is applied to evaluate the sentences globally. The second method applies cross-sentence contextualization to improve the local ranking and discard the global ranking step. Our methods achieve \textbf{the 1st Place} in the MAS task. For the question summarization (QS) and radiology report summarization (RRS) tasks, we explore how end-to-end pre-trained seq2seq model perform. A series of tricks for improving the fine-tuning performances are validated.",
}



@Article{info:doi/10.2196/17653,
author="Sun, Haixia
and Xiao, Jin
and Zhu, Wei
and He, Yilong
and Zhang, Sheng
and Xu, Xiaowei
and Hou, Li
and Li, Jiao
and Ni, Yuan
and Xie, Guotong",
title="Medical Knowledge Graph to Enhance Fraud, Waste, and Abuse Detection on Claim Data: Model Development and Performance Evaluation",
journal="JMIR Med Inform",
year="2020",
month="Jul",
day="23",
volume="8",
number="7",
pages="e17653",
keywords="medical knowledge graph; FWA detection",
abstract="Background: Fraud, Waste, and Abuse (FWA) detection is a significant yet challenging problem in the health insurance industry. An essential step in FWA detection is to check whether the medication is clinically reasonable with respect to the diagnosis. Currently, human experts with sufficient medical knowledge are required to perform this task. To reduce the cost, insurance inspectors tend to build an intelligent system to detect suspicious claims with inappropriate diagnoses/medications automatically. Objective: The aim of this study was to develop an automated  method for making use of a medical knowledge graph to identify clinically suspected claims for FWA detection. Methods: First, we identified the medical knowledge that is required to assess the clinical rationality of the claims. We then searched for data sources that contain information to build such knowledge. In this study, we focused on Chinese medical knowledge. Second, we constructed a medical knowledge graph using unstructured knowledge. We used a deep learning--based method to extract the entities and relationships from the knowledge sources and developed a multilevel similarity matching approach to conduct the entity linking. To guarantee the quality of the medical knowledge graph, we involved human experts to review the entity and relationships with lower confidence. These reviewed results could be used to further improve the machine-learning models. Finally, we developed the rules to identify the suspected claims by reasoning according to the medical knowledge graph. Results: We collected 185,796 drug labels from the China Food and Drug Administration, 3390 types of disease information from medical textbooks (eg, symptoms, diagnosis, treatment, and prognosis), and information from 5272 examinations as the knowledge sources. The final medical knowledge graph includes 1,616,549 nodes and 5,963,444 edges. We designed three knowledge graph reasoning rules to identify three kinds of inappropriate diagnosis/medications. The experimental results showed that the medical knowledge graph helps to detect 70{\%} of the suspected claims. Conclusions: The medical knowledge graph--based method successfully identified suspected cases of FWA (such as fraud diagnosis, excess prescription, and irrational prescription) from the claim documents, which helped to improve the efficiency of claim processing. ",
issn="2291-9694",
doi="10.2196/17653",
url="http://medinform.jmir.org/2020/7/e17653/",
url="https://doi.org/10.2196/17653",
url="http://www.ncbi.nlm.nih.gov/pubmed/32706714"
}


@inproceedings{zhu-etal-2019-panlp,
    title = "{PANLP} at {MEDIQA} 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation",
    author = "Zhu, Wei  and
      Zhou, Xiaofeng  and
      Wang, Keqiang  and
      Luo, Xun  and
      Li, Xiepeng  and
      Ni, Yuan  and
      Xie, Guotong",
    booktitle = "Proceedings of the 18th BioNLP Workshop and Shared Task",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5040",
    doi = "10.18653/v1/W19-5040",
    pages = "380--388",
    abstract = "This paper describes the models designated for the MEDIQA 2019 shared tasks by the team PANLP. We take advantages of the recent advances in pre-trained bidirectional transformer language models such as BERT (Devlin et al., 2018) and MT-DNN (Liu et al., 2019b). We find that pre-trained language models can significantly outperform traditional deep learning models. Transfer learning from the NLI task to the RQE task is also experimented, which proves to be useful in improving the results of fine-tuning MT-DNN large. A knowledge distillation process is implemented, to distill the knowledge contained in a set of models and transfer it into an single model, whose performance turns out to be comparable with that obtained by the ensemble of that set of models. Finally, for test submissions, model ensemble and a re-ranking process are implemented to boost the performances. Our models participated in all three tasks and ranked the 1st place for the RQE task, and the 2nd place for the NLI task, and also the 2nd place for the QA task.",
}



@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

@inproceedings{sadat-caragea-2022-scinli,
    title = "{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text",
    author = "Sadat, Mobashir  and
      Caragea, Cornelia",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.511",
    doi = "10.18653/v1/2022.acl-long.511",
    pages = "7399--7409",
    abstract = "Existing Natural Language Inference (NLI) datasets, while being instrumental in the advancement of Natural Language Understanding (NLU) research, are not related to scientific text. In this paper, we introduce SciNLI, a large dataset for NLI that captures the formality in scientific text and contains 107,412 sentence pairs extracted from scholarly papers on NLP and computational linguistics. Given that the text used in scientific literature differs vastly from the text used in everyday language both in terms of vocabulary and sentence structure, our dataset is well suited to serve as a benchmark for the evaluation of scientific NLU models. Our experiments show that SciNLI is harder to classify than the existing NLI datasets. Our best performing model with XLNet achieves a Macro F1 score of only 78.18{\%} and an accuracy of 78.23{\%} showing that there is substantial room for improvement.",
}



@inproceedings{OntoNotes50,
    title = "OntoNotes Release 5.0",
    author = "Weischedel, Ralph and
         Palmer, Martha and 
         Marcus, Mitchell and 
          Hovy, Eduard and
          Pradhan, Sameer and 
          Ramshaw, Lance and 
           Xue, Nianwen and 
           Taylor, Ann and 
            Kaufman, Jeff and 
          Franchini, Michelle and 
         El-Bachouti, Mohammed and 
         Belvin, Robert and 
          Houston, Ann",
    booktitle = "Web Download. Philadelphia: Linguistic Data Consortium, 2013",
    year = "2013",
    url = "https://catalog.ldc.upenn.edu/LDC2013T19",
}



@inproceedings{barnes-etal-2022-semeval,
    title = "{S}em{E}val 2022 Task 10: Structured Sentiment Analysis",
    author = "Barnes, Jeremy  and
      Oberlaender, Laura  and
      Troiano, Enrica  and
      Kutuzov, Andrey  and
      Buchmann, Jan  and
      Agerri, Rodrigo  and
      {\O}vrelid, Lilja  and
      Velldal, Erik",
    booktitle = "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.semeval-1.180",
    doi = "10.18653/v1/2022.semeval-1.180",
    pages = "1280--1295",
    abstract = "In this paper, we introduce the first SemEval shared task on Structured Sentiment Analysis, for which participants are required to predict all sentiment graphs in a text, where a single sentiment graph is composed of a sentiment holder, target, expression and polarity. This new shared task includes two subtracks (monolingual and cross-lingual) with seven datasets available in five languages, namely Norwegian, Catalan, Basque, Spanish and English. Participants submitted their predictions on a held-out test set and were evaluated on Sentiment Graph F1 . Overall, the task received over 200 submissions from 32 participating teams. We present the results of the 15 teams that provided system descriptions and our own expanded analysis of the test predictions.",
}


@inproceedings{ACE-2005,
    title = "ACE 2005 Multilingual Training Corpus LDC2006T06",
    author = "Walker, Christopher and 
              Strassel, Stephanie and 
              Medero, Julie and 
               Maeda, Kazuaki",
    booktitle = "Web Download. Philadelphia: Linguistic Data Consortium, 2006",
    year = "2006",
    url = "https://catalog.ldc.upenn.edu/LDC2006T06",
}


@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}



@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}



@inproceedings{li-etal-2022-multispanqa,
    title = "{M}ulti{S}pan{QA}: A Dataset for Multi-Span Question Answering",
    author = "Li, Haonan  and
      Tomko, Martin  and
      Vasardani, Maria  and
      Baldwin, Timothy",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.90",
    doi = "10.18653/v1/2022.naacl-main.90",
    pages = "1250--1260",
    abstract = "Most existing reading comprehension datasets focus on single-span answers, which can be extracted as a single contiguous span from a given text passage. Multi-span questions, i.e., questions whose answer is a series of multiple discontiguous spans in the text, are common real life but are less studied. In this paper, we present MultiSpanQA, a new dataset that focuses on multi-span questions. Raw questions and contexts are extracted from the Natural Questions dataset. After multi-span re-annotation, MultiSpanQA consists of over a total of 6,000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version. We introduce new metrics for the purposes of multi-span question answering evaluation, and establish several baselines using advanced models. Finally, we propose a new model which beats all baselines and achieves state-of-the-art on our dataset.",
}



@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}



@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}




@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}



@misc{zheng2023codegeex,
      title={CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X}, 
      author={Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang},
      year={2023},
      eprint={2303.17568},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}






@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}



@ARTICLE{gpt4,
       author = {{OpenAI}},
        title = "{GPT-4 Technical Report}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2023,
        month = mar,
          eid = {arXiv:2303.08774},
        pages = {arXiv:2303.08774},
          doi = {10.48550/arXiv.2303.08774},
archivePrefix = {arXiv},
       eprint = {2303.08774},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230308774O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{qin2023chatgpt,
  title={Is ChatGPT a general-purpose natural language processing task solver?},
  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  journal={arXiv preprint arXiv:2302.06476},
  year={2023}
}



@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}



@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}


@article{zeng2023measuring,
  title={Measuring massive multitask chinese understanding},
  author={Zeng, Hui},
  journal={arXiv preprint arXiv:2304.12986},
  year={2023}
}



@article{zhong2023agieval,
  title={Agieval: A human-centric benchmark for evaluating foundation models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2304.06364},
  year={2023}
}



@article{huang2023c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and others},
  journal={arXiv preprint arXiv:2305.08322},
  year={2023}
}


@article{li2023cmmlu,
  title={CMMLU: Measuring massive multitask language understanding in Chinese},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}



@article{singhal2023large,
  title={Large language models encode clinical knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={Nature},
  pages={1--9},
  year={2023},
  publisher={Nature Publishing Group UK London}
}



@article{gao2023dr,
  title={DR. BENCH: Diagnostic reasoning benchmark for clinical natural language processing},
  author={Gao, Yanjun and Dligach, Dmitriy and Miller, Timothy and Caskey, John and Sharma, Brihat and Churpek, Matthew M and Afshar, Majid},
  journal={Journal of Biomedical Informatics},
  volume={138},
  pages={104286},
  year={2023},
  publisher={Elsevier}
}



@article{singhal2023towards,
  title={Towards expert-level medical question answering with large language models},
  author={Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Hou, Le and Clark, Kevin and Pfohl, Stephen and Cole-Lewis, Heather and Neal, Darlene and others},
  journal={arXiv preprint arXiv:2305.09617},
  year={2023}
}




@inproceedings{Cai2021EMBERTAP,
  title={EMBERT: A Pre-trained Language Model for Chinese Medical Text Mining},
  author={Zerui Cai and Taolin Zhang and Chengyu Wang and Xiaofeng He},
  booktitle={APWeb/WAIM},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:237246727}
}


@article{wang2021building,
  title={Building chinese biomedical language models via multi-level text discrimination},
  author={Wang, Quan and Dai, Songtai and Xu, Benfeng and Lyu, Yajuan and Zhu, Yong and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2110.07244},
  year={2021}
}


@inproceedings{zhang-etal-2022-cblue,
    title = "{CBLUE}: A {C}hinese Biomedical Language Understanding Evaluation Benchmark",
    author = "Zhang, Ningyu  and
      Chen, Mosha  and
      Bi, Zhen  and
      Liang, Xiaozhuan  and
      Li, Lei  and
      Shang, Xin  and
      Yin, Kangping  and
      Tan, Chuanqi  and
      Xu, Jian  and
      Huang, Fei  and
      Si, Luo  and
      Ni, Yuan  and
      Xie, Guotong  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Zong, Hui  and
      Yuan, Zheng  and
      Li, Linfeng  and
      Yan, Jun  and
      Zan, Hongying  and
      Zhang, Kunli  and
      Tang, Buzhou  and
      Chen, Qingcai",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.544",
    doi = "10.18653/v1/2022.acl-long.544",
    pages = "7888--7915",
    abstract = "Artificial Intelligence (AI), along with the recent progress in biomedical language understanding, is gradually offering great promise for medical practice. With the development of biomedical language understanding benchmarks, AI applications are widely used in the medical field. However, most benchmarks are limited to English, which makes it challenging to replicate many of the successes in English for other languages. To facilitate research in this direction, we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark: a collection of natural language understanding tasks including named entity recognition, information extraction, clinical diagnosis normalization, single-sentence/sentence-pair classification, and an associated online platform for model evaluation, comparison, and analysis. To establish evaluation on these tasks, we report empirical results with the current 11 pre-trained Chinese models, and experimental results show that state-of-the-art neural models perform by far worse than the human ceiling.",
}



@article{yu2023kola,
  title={KoLA: Carefully Benchmarking World Knowledge of Large Language Models},
  author={Yu, Jifan and Wang, Xiaozhi and Tu, Shangqing and Cao, Shulin and Zhang-Li, Daniel and Lv, Xin and Peng, Hao and Yao, Zijun and Zhang, Xiaohan and Li, Hanming and others},
  journal={arXiv preprint arXiv:2306.09296},
  year={2023}
}




@article{Karabacak2023EmbracingLL,
  title={Embracing Large Language Models for Medical Applications: Opportunities and Challenges},
  author={Mert Karabacak and Konstantinos Margetis},
  journal={Cureus},
  year={2023},
  volume={15},
  url={https://api.semanticscholar.org/CorpusID:258837444}
}


@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}


@inproceedings{naturalinstructions,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  booktitle={ACL},
  year={2022}
}
@inproceedings{supernaturalinstructions,
  title={Super-NaturalInstructions:Generalization via Declarative Instructions on 1600+ Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  booktitle={EMNLP},
  year={2022}
}


@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}


@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}


@inproceedings{bach-etal-2022-promptsource,
    title = "{P}rompt{S}ource: An Integrated Development Environment and Repository for Natural Language Prompts",
    author = "Bach, Stephen  and
      Sanh, Victor  and
      Yong, Zheng Xin  and
      Webson, Albert  and
      Raffel, Colin  and
      Nayak, Nihal V.  and
      Sharma, Abheesht  and
      Kim, Taewoon  and
      Bari, M Saiful  and
      Fevry, Thibault  and
      Alyafeai, Zaid  and
      Dey, Manan  and
      Santilli, Andrea  and
      Sun, Zhiqing  and
      Ben-david, Srulik  and
      Xu, Canwen  and
      Chhablani, Gunjan  and
      Wang, Han  and
      Fries, Jason  and
      Al-shaibani, Maged  and
      Sharma, Shanya  and
      Thakker, Urmish  and
      Almubarak, Khalid  and
      Tang, Xiangru  and
      Radev, Dragomir  and
      Jiang, Mike Tian-jian  and
      Rush, Alexander",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-demo.9",
    doi = "10.18653/v1/2022.acl-demo.9",
    pages = "93--104",
    abstract = "PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource.",
}



@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}



@inproceedings{Zan2020BuildingAP,
  title={Building a Pediatric Medical Corpus: Word Segmentation and Named Entity Annotation},
  author={Hongying Zan and Wenxin Li and Kunli Zhang and Yajuan Ye and Baobao Chang and Zhifang Sui},
  booktitle={Chinese Lexical Semantics},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:236477750}
}


@inproceedings{Guan2020CMeIECA,
  title={CMeIE: Construction and Evaluation of Chinese Medical Information Extraction Dataset},
  author={Tongfeng Guan and Hongying Zan and Xiabing Zhou and Hongfei Xu and Kunli Zhang},
  booktitle={Natural Language Processing and Chinese Computing},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:222210416}
}



@article{Zong2021SemanticCO,
  title={Semantic categorization of Chinese eligibility criteria in clinical trials using machine learning methods},
  author={Hui Zong and Jinxuan Yang and Zeyu Zhang and Zuofeng Li and Xiaoyan Zhang},
  journal={BMC Medical Informatics and Decision Making},
  year={2021},
  volume={21},
  url={https://api.semanticscholar.org/CorpusID:233239370}
}



@article{MDCFNPC,
  title={Overview of the CHIP2021 Shared Task 1: Classifying Positive and Negative Clinical Findings in Medical Dialog},
  author={Ying Xiong and Mosha Chen and Qingcai Chen and Buzhou Tang},
  journal={China Health Information Processing Conference},
  year={2021},
}

@article{Liu2020MedDGAL,
  title={MedDG: A Large-scale Medical Consultation Dataset for Building Medical Dialogue System},
  author={Wenge Liu and Jianheng Tang and Jinghui Qin and Lin Xu and Zhuguo Li and Xiaodan Liang},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.07497},
  url={https://api.semanticscholar.org/CorpusID:222377844}
}


基于竞赛视角探讨文本语义匹配技术在中文医学文本领域中的应用
@article{CHIP-STS,
  title={Discussion on the Application of Text Semantic Matching Technology in the Field of Chinese Medical Text from the Perspective of Competition},
  author={Xun Luo and Yuan Ni and Buzhou Tang},
  journal={China Digital Medicine},
  year={2021},
  volume={11},
}

@article{Li2023UnifiedDR,
  title={Unified Demonstration Retriever for In-Context Learning},
  author={Xiaonan Li and Kai Lv and Hang Yan and Tianya Lin and Wei Zhu and Yuan Ni and Guo Tong Xie and Xiaoling Wang and Xipeng Qiu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.04320},
  url={https://api.semanticscholar.org/CorpusID:258557751}
}



@article{Wang2023LearningTR,
  title={Learning to Retrieve In-Context Examples for Large Language Models},
  author={Liang Wang and Nan Yang and Furu Wei},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.07164},
  url={https://api.semanticscholar.org/CorpusID:259924840}
}



@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}


@article{OpenAI2023GPT4TR,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774},
  url={https://api.semanticscholar.org/CorpusID:257532815}
}


@article{Touvron2023Llama2O,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Daniel M. Bikel and Lukas Blecher and Cristian Cant{\'o}n Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony S. Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel M. Kloumann and A. V. Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and R. Subramanian and Xia Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zhengxu Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.09288},
  url={https://api.semanticscholar.org/CorpusID:259950998}
}



@inproceedings{liu-etal-2022-p,
    title = "{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
    author = "Liu, Xiao  and
      Ji, Kaixuan  and
      Fu, Yicheng  and
      Tam, Weng  and
      Du, Zhengxiao  and
      Yang, Zhilin  and
      Tang, Jie",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.8",
    doi = "10.18653/v1/2022.acl-short.8",
    pages = "61--68",
    abstract = "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1{\%}-3{\%} tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.",
}


@article{Liu2021GPTUT,
  title={GPT Understands, Too},
  author={Xiao Liu and Yanan Zheng and Zhengxiao Du and Ming Ding and Yujie Qian and Zhilin Yang and Jie Tang},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.10385},
  url={https://api.semanticscholar.org/CorpusID:232269696}
}



@article{Zhang2023AdaptiveBA,
  title={Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},
  author={Qingru Zhang and Minshuo Chen and Alexander W. Bukharin and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.10512},
  url={https://api.semanticscholar.org/CorpusID:257631760}
}



@Article{info:doi/10.2196/17653,
author="Sun, Haixia
and Xiao, Jin
and Zhu, Wei
and He, Yilong
and Zhang, Sheng
and Xu, Xiaowei
and Hou, Li
and Li, Jiao
and Ni, Yuan
and Xie, Guotong",
title="Medical Knowledge Graph to Enhance Fraud, Waste, and Abuse Detection on Claim Data: Model Development and Performance Evaluation",
journal="JMIR Med Inform",
year="2020",
month="Jul",
day="23",
volume="8",
number="7",
pages="e17653",
keywords="medical knowledge graph; FWA detection",
abstract="Background: Fraud, Waste, and Abuse (FWA) detection is a significant yet challenging problem in the health insurance industry. An essential step in FWA detection is to check whether the medication is clinically reasonable with respect to the diagnosis. Currently, human experts with sufficient medical knowledge are required to perform this task. To reduce the cost, insurance inspectors tend to build an intelligent system to detect suspicious claims with inappropriate diagnoses/medications automatically. Objective: The aim of this study was to develop an automated  method for making use of a medical knowledge graph to identify clinically suspected claims for FWA detection. Methods: First, we identified the medical knowledge that is required to assess the clinical rationality of the claims. We then searched for data sources that contain information to build such knowledge. In this study, we focused on Chinese medical knowledge. Second, we constructed a medical knowledge graph using unstructured knowledge. We used a deep learning--based method to extract the entities and relationships from the knowledge sources and developed a multilevel similarity matching approach to conduct the entity linking. To guarantee the quality of the medical knowledge graph, we involved human experts to review the entity and relationships with lower confidence. These reviewed results could be used to further improve the machine-learning models. Finally, we developed the rules to identify the suspected claims by reasoning according to the medical knowledge graph. Results: We collected 185,796 drug labels from the China Food and Drug Administration, 3390 types of disease information from medical textbooks (eg, symptoms, diagnosis, treatment, and prognosis), and information from 5272 examinations as the knowledge sources. The final medical knowledge graph includes 1,616,549 nodes and 5,963,444 edges. We designed three knowledge graph reasoning rules to identify three kinds of inappropriate diagnosis/medications. The experimental results showed that the medical knowledge graph helps to detect 70{\%} of the suspected claims. Conclusions: The medical knowledge graph--based method successfully identified suspected cases of FWA (such as fraud diagnosis, excess prescription, and irrational prescription) from the claim documents, which helped to improve the efficiency of claim processing. ",
issn="2291-9694",
doi="10.2196/17653",
url="http://medinform.jmir.org/2020/7/e17653/",
url="https://doi.org/10.2196/17653",
url="http://www.ncbi.nlm.nih.gov/pubmed/32706714"
}

@Article{jmir,
author="Sun, Haixia
and Xiao, Jin
and Zhu, Wei
and He, Yilong
and Zhang, Sheng
and Xu, Xiaowei
and Hou, Li
and Li, Jiao
and Ni, Yuan
and Xie, Guotong",
title="Medical Knowledge Graph to Enhance Fraud, Waste, and Abuse Detection on Claim Data: Model Development and Performance Evaluation",
journal="JMIR Med Inform",
year="2020",
month="Jul",
day="23",
volume="8",
number="7",
pages="e17653",
keywords="medical knowledge graph; FWA detection",
abstract="Background: Fraud, Waste, and Abuse (FWA) detection is a significant yet challenging problem in the health insurance industry. An essential step in FWA detection is to check whether the medication is clinically reasonable with respect to the diagnosis. Currently, human experts with sufficient medical knowledge are required to perform this task. To reduce the cost, insurance inspectors tend to build an intelligent system to detect suspicious claims with inappropriate diagnoses/medications automatically. Objective: The aim of this study was to develop an automated  method for making use of a medical knowledge graph to identify clinically suspected claims for FWA detection. Methods: First, we identified the medical knowledge that is required to assess the clinical rationality of the claims. We then searched for data sources that contain information to build such knowledge. In this study, we focused on Chinese medical knowledge. Second, we constructed a medical knowledge graph using unstructured knowledge. We used a deep learning--based method to extract the entities and relationships from the knowledge sources and developed a multilevel similarity matching approach to conduct the entity linking. To guarantee the quality of the medical knowledge graph, we involved human experts to review the entity and relationships with lower confidence. These reviewed results could be used to further improve the machine-learning models. Finally, we developed the rules to identify the suspected claims by reasoning according to the medical knowledge graph. Results: We collected 185,796 drug labels from the China Food and Drug Administration, 3390 types of disease information from medical textbooks (eg, symptoms, diagnosis, treatment, and prognosis), and information from 5272 examinations as the knowledge sources. The final medical knowledge graph includes 1,616,549 nodes and 5,963,444 edges. We designed three knowledge graph reasoning rules to identify three kinds of inappropriate diagnosis/medications. The experimental results showed that the medical knowledge graph helps to detect 70{\%} of the suspected claims. Conclusions: The medical knowledge graph--based method successfully identified suspected cases of FWA (such as fraud diagnosis, excess prescription, and irrational prescription) from the claim documents, which helped to improve the efficiency of claim processing. ",
issn="2291-9694",
doi="10.2196/17653",
url="http://medinform.jmir.org/2020/7/e17653/",
}



@article{Park2019AnIR,
  title={An Information Retrieval Approach to ICD-10 Classification},
  author={Hee Joon Park and Jos{\'e} M. Casta{\~n}o and Pilar {\'A}vila and David P{\'e}rez and Hern{\'a}n Berinsky and Laura Gambarte and Daniel R. Luna and Carlos Martin Otero},
  journal={Studies in health technology and informatics},
  year={2019},
  volume={264},
  pages={
          1564-1565
        },
  url={https://api.semanticscholar.org/CorpusID:201618206}
}



@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}



@inproceedings{Jin2019PubMedQAAD,
  title={PubMedQA: A Dataset for Biomedical Research Question Answering},
  author={Qiao Jin and Bhuwan Dhingra and Zhengping Liu and William W. Cohen and Xinghua Lu},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:202572622}
}



@InProceedings{Text2dt,
author="Zhu, Wei
and Li, Wenfeng
and Wang, Xiaoling
and Ji, Wendi
and Wu, Yuanbin
and Chen, Jin
and Chen, Liang
and Tang, Buzhou",
editor="Tang, Buzhou
and Chen, Qingcai
and Lin, Hongfei
and Wu, Fei
and Liu, Lei
and Hao, Tianyong
and Wang, Yanshan
and Wang, Haitian
and Lei, Jianbo
and Li, Zuofeng
and Zong, Hui",
title="Extracting Decision Trees from Medical Texts: An Overview of the Text2DT Track in CHIP2022",
booktitle="Health Information Processing. Evaluation Track Papers",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="89--102",
abstract="This paper presents an overview of the Text2DT shared task{\$}{\$}^{\{}1{\}}{\$}{\$}1held in the CHIP-2022 shared tasks. The shared task addresses the challenging topic of automatically extracting the medical decision trees from the un-structured medical texts such as medical guidelines and textbooks. Many teams from both industry and academia participated in the shared tasks, and the top teams achieved amazing test results. This paper describes the tasks, the datasets, evaluation metrics, and the top systems for both tasks. Finally, the paper summarizes the techniques and results of the evaluation of the various approaches explored by the participating teams.{\$}{\$}^{\{}1{\}}{\$}{\$}1(http://cips-chip.org.cn/2022/eval3)",
isbn="978-981-99-4826-0"
}




@inproceedings{pappas-etal-2018-bioread,
    title = "{B}io{R}ead: A New Dataset for Biomedical Reading Comprehension",
    author = "Pappas, Dimitris  and
      Androutsopoulos, Ion  and
      Papageorgiou, Haris",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1439",
}






@article{Jin2020WhatDD,
  title={What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
  author={Di Jin and Eileen Pan and Nassim Oufattole and Wei-Hung Weng and Hanyi Fang and Peter Szolovits},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.13081},
  url={https://api.semanticscholar.org/CorpusID:221970190}
}



@misc{yue2023-TCMEB,
      title={TCMEB: Performance Evaluation of Large Language Models Based on Traditional Chinese Medicine Benchmarks}, 
      author={Wenjing Yue, Wei Zhu and Xiaoling Wang},
      year={2023},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished = {\url{https://github.com/ywjawmw/ShenNong-TCM-Evaluation-BenchMark}},
}



@inproceedings{zhu-etal-2021-pre,
    title = "When does Further Pre-training {MLM} Help? An Empirical Study on Task-Oriented Dialog Pre-training",
    author = "Zhu, Qi  and
      Gu, Yuxian  and
      Luo, Lingxiao  and
      Li, Bing  and
      Li, Cheng  and
      Peng, Wei  and
      Huang, Minlie  and
      Zhu, Xiaoyan",
    booktitle = "Proceedings of the Second Workshop on Insights from Negative Results in NLP",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.insights-1.9",
    doi = "10.18653/v1/2021.insights-1.9",
    pages = "54--61",
    abstract = "Further pre-training language models on in-domain data (domain-adaptive pre-training, DAPT) or task-relevant data (task-adaptive pre-training, TAPT) before fine-tuning has been shown to improve downstream tasks{'} performances. However, in task-oriented dialog modeling, we observe that further pre-training MLM does not always boost the performance on a downstream task. We find that DAPT is beneficial in the low-resource setting, but as the fine-tuning data size grows, DAPT becomes less beneficial or even useless, and scaling the size of DAPT data does not help. Through Representational Similarity Analysis, we conclude that more data for fine-tuning yields greater change of the model{'}s representations and thus reduces the influence of initialization.",
}



@article{Robertson2009ThePR,
  title={The Probabilistic Relevance Framework: BM25 and Beyond},
  author={Stephen E. Robertson and Hugo Zaragoza},
  journal={Found. Trends Inf. Retr.},
  year={2009},
  volume={3},
  pages={333-389},
  url={https://api.semanticscholar.org/CorpusID:207178704}
}


% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@inproceedings{wu2021peer,
  title={Peer collaborative learning for online knowledge distillation},
  author={Wu, Guile and Gong, Shaogang},
  booktitle={AAAI},
  year={2021}
}

@article{liu2020fastbert,
  title={Fastbert: a self-distilling bert with adaptive inference time},
  author={Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Deng, Haotang and Ju, Qi},
  journal={arXiv preprint arXiv:2004.02178},
  year={2020}
}

@article{xin2020deebert,
  title={DeeBERT: Dynamic early exiting for accelerating BERT inference},
  author={Xin, Ji and Tang, Raphael and Lee, Jaejun and Yu, Yaoliang and Lin, Jimmy},
  journal={arXiv preprint arXiv:2004.12993},
  year={2020}
}
@inproceedings{phuong2019distillation,
  title={Distillation-based training for multi-exit architectures},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1355--1364},
  year={2019}
}
@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}
@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@inproceedings{xin2021berxit,
  title={BERxiT: Early Exiting for BERT with Better fine-tuning and extension to regression},
  author={Xin, Ji and Tang, Raphael and Yu, Yaoliang and Lin, Jimmy},
  booktitle={Proceedings of the 16th conference of the European chapter of the association for computational linguistics: Main Volume},
  pages={91--104},
  year={2021}
}
@article{fan2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1909.11556},
  year={2019}
}
@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{zhou2020bert,
  title={Bert loses patience: Fast and robust inference with early exit},
  author={Zhou, Wangchunshu and Xu, Canwen and Ge, Tao and McAuley, Julian and Xu, Ke and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18330--18341},
  year={2020}
}
@inproceedings{kaya2019shallow,
  title={Shallow-deep networks: Understanding and mitigating network overthinking},
  author={Kaya, Yigitcan and Hong, Sanghyun and Dumitras, Tudor},
  booktitle={International conference on machine learning},
  pages={3301--3310},
  year={2019},
  organization={PMLR}
}
@article{zhang2018overview,
  title={An overview of multi-task learning},
  author={Zhang, Yu and Yang, Qiang},
  journal={National Science Review},
  volume={5},
  number={1},
  pages={30--43},
  year={2018},
  publisher={Oxford University Press}
}
@article{schwartz2020right,
  title={The right tool for the job: Matching model and instance complexities},
  author={Schwartz, Roy and Stanovsky, Gabriel and Swayamdipta, Swabha and Dodge, Jesse and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.07453},
  year={2020}
}
@article{he2021towards,
  title={Towards a unified view of parameter-efficient transfer learning},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  journal={arXiv preprint arXiv:2110.04366},
  year={2021}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{xu2020bert,
  title={Bert-of-theseus: Compressing bert by progressive module replacing},
  author={Xu, Canwen and Zhou, Wangchunshu and Ge, Tao and Wei, Furu and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.02925},
  year={2020}
}
@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}
@article{gordon2020compressing,
  title={Compressing bert: Studying the effects of weight pruning on transfer learning},
  author={Gordon, Mitchell A and Duh, Kevin and Andrews, Nicholas},
  journal={arXiv preprint arXiv:2002.08307},
  year={2020}
}
@article{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}
@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}
@article{zhang2020ternarybert,
  title={Ternarybert: Distillation-aware ultra-low bit bert},
  author={Zhang, Wei and Hou, Lu and Yin, Yichun and Shang, Lifeng and Chen, Xiao and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2009.12812},
  year={2020}
}
@article{bai2020binarybert,
  title={Binarybert: Pushing the limit of bert quantization},
  author={Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  journal={arXiv preprint arXiv:2012.15701},
  year={2020}
}
@inproceedings{kim2021bert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}
@inproceedings{bolukbasi2017adaptive,
  title={Adaptive neural networks for efficient inference},
  author={Bolukbasi, Tolga and Wang, Joseph and Dekel, Ofer and Saligrama, Venkatesh},
  booktitle={International Conference on Machine Learning},
  pages={527--536},
  year={2017},
  organization={PMLR}
}
@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press}
}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-
drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.
In Proceedings of ICML, 2019


@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}
@article{zhao2019uer,
  title={UER: An Open-Source Toolkit for Pre-training Models},
  author={Zhao, Zhe and Chen, Hui and Zhang, Jinbin and Zhao, Xin and Liu, Tao and Lu, Wei and Chen, Xi and Deng, Haotang and Ju, Qi and Du, Xiaoyong},
  journal={EMNLP-IJCNLP 2019},
  pages={241},
  year={2019}
}



@article{Wu2018UnsupervisedFL,
  title={Unsupervised Feature Learning via Non-parametric Instance Discrimination},
  author={Zhirong Wu and Yuanjun Xiong and Stella X. Yu and Dahua Lin},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={3733-3742}
}


@article{Oord2018RepresentationLW,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.03748}
}


@inproceedings{Tian2020ContrastiveMC,
  title={Contrastive Multiview Coding},
  author={Yonglong Tian and Dilip Krishnan and Phillip Isola},
  booktitle={ECCV},
  year={2020}
}

@article{He2020MomentumCF,
  title={Momentum Contrast for Unsupervised Visual Representation Learning},
  author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross B. Girshick},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={9726-9735}
}



@article{Chen2021ExploringSS,
  title={Exploring Simple Siamese Representation Learning},
  author={Xinlei Chen and Kaiming He},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={15745-15753}
}

@inproceedings{Wang2020UnderstandingCR,
  title={Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
  author={Tongzhou Wang and Phillip Isola},
  booktitle={ICML},
  year={2020}
}


@article{Gao2021SimCSESC,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08821}
}


@article{Jaiswal2020ASO,
  title={A Survey on Contrastive Self-supervised Learning},
  author={Ashish Jaiswal and Ashwin Ramesh Babu and Mohammad Zaki Zadeh and Debapriya Banerjee and Fillia Makedon},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.00362}
}



@article{Khosla2020SupervisedCL,
  title={Supervised Contrastive Learning},
  author={Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.11362}
}


@article{Gunel2021SupervisedCL,
  title={Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning},
  author={Beliz Gunel and Jingfei Du and Alexis Conneau and Ves Stoyanov},
  journal={ArXiv},
  year={2021},
  volume={abs/2011.01403}
}



@article{Teerapittayanon2016BranchyNetFI,
  title={BranchyNet: Fast inference via early exiting from deep neural networks},
  author={Surat Teerapittayanon and Bradley McDanel and H. T. Kung},
  journal={2016 23rd International Conference on Pattern Recognition (ICPR)},
  year={2016},
  pages={2464-2469}
}





@article{Li2019ImprovedTF,
  title={Improved Techniques for Training Adaptive Deep Networks},
  author={Hao Li and Hong Zhang and Xiaojuan Qi and Ruigang Yang and Gao Huang},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={1891-1900}
}





@inproceedings{Kaya2019ShallowDeepNU,
	title={Shallow-Deep Networks: Understanding and Mitigating Network Overthinking},
	author={Y. Kaya and Sanghyun Hong and T. Dumitras},
	booktitle={ICML},
	year={2019}
}



@article{Zhou2020PABEE,
	title={BERT Loses Patience: Fast and Robust Inference with Early Exit},
	author={Wangchunshu Zhou and Canwen Xu and Tao Ge and Julian McAuley and Ke Xu and Furu Wei},
	journal={ArXiv},
	year={2020},
	volume={abs/2006.04152}
}


@inproceedings{Zhu2021LeeBERTLE,
  title={LeeBERT: Learned Early Exit for BERT with cross-level optimization},
  author={Wei Zhu},
  booktitle={ACL},
  year={2021}
}

@inproceedings{Zhu2021GAMLBERTIB,
  title={GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning},
  author={Wei Zhu and Xiaoling Wang and Yuan Ni and Guo Tong Xie},
  booktitle={EMNLP},
  year={2021}
}





@article{Hadsell2006DimensionalityRB,
  title={Dimensionality Reduction by Learning an Invariant Mapping},
  author={Raia Hadsell and Sumit Chopra and Yann LeCun},
  journal={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  year={2006},
  volume={2},
  pages={1735-1742}
}


@article{Oord2018RepresentationLW,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.03748}
}



@article{Fang2020CERTCS,
  title={CERT: Contrastive Self-supervised Learning for Language Understanding},
  author={Hongchao Fang and Sicheng Wang and Meng Zhou and Jiayuan Ding and Pengtao Xie},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.12766}
}


@article{Tambe2020EdgeBERTOO,
  title={EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP},
  author={Thierry Tambe and Coleman Hooper and Lillian Pentecost and En-Yu Yang and Marco Donato and Victor Sanh and Alexander M. Rush and David M. Brooks and Gu-Yeon Wei},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.14203}
}


@inproceedings{sun-etal-2022-simple,
    title = "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation",
    author = "Sun, Tianxiang  and
      Liu, Xiangyang  and
      Zhu, Wei  and
      Geng, Zhichao  and
      Wu, Lingling  and
      He, Yilong  and
      Ni, Yuan  and
      Xie, Guotong  and
      Huang, Xuanjing  and
      Qiu, Xipeng",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.189",
    doi = "10.18653/v1/2022.findings-acl.189",
    pages = "2409--2421",
    abstract = "Early exiting allows instances to exit at different layers according to the estimation of difficulty.Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning. In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way. Though some effort has been devoted to employing such {``}learn-to-exit{''} modules, it is still unknown whether and how well the instance difficulty can be learned. As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty. Based on this observation, we propose a simple-yet-effective Hash-based Early Exiting approach HashEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. Different from previous methods, HashEE requires no internal classifiers nor extra parameters, and therefore is more efficient.HashEE can be used in various tasks (including language understanding and generation) and model architectures such as seq2seq models. Experimental results on classification, regression, and generation tasks demonstrate that HashEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.",
}





@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}

@inproceedings{Xu2020BERTofTheseusCB,
  title={BERT-of-Theseus: Compressing BERT by Progressive Module Replacing},
  author={Canwen Xu and Wangchunshu Zhou and Tao Ge and Furu Wei and Ming Zhou},
  booktitle={EMNLP},
  year={2020}
}

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}


@article{Fan2020ReducingTD,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Angela Fan and Edouard Grave and Armand Joulin},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.11556}
}


@inproceedings{Michel2019AreSH,
  title={Are Sixteen Heads Really Better than One?},
  author={Paul Michel and Omer Levy and Graham Neubig},
  booktitle={NeurIPS},
  year={2019}
}


@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}

@article{Jiao2020TinyBERTDB,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.10351}
}



@article{Xu2021ASO,
  title={A Survey on Green Deep Learning},
  author={Jingjing Xu and Wangchunshu Zhou and Zhiyi Fu and Hao Zhou and Lei Li},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.05193}
}

@article{Lin2021ASO,
  title={A Survey of Transformers},
  author={Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.04554}
}



@inproceedings{Schuster2021ConsistentAI,
  title={Consistent Accelerated Inference via Confident Adaptive Transformers},
  author={Tal Schuster and Adam Fisch and T. Jaakkola and Regina Barzilay},
  booktitle={EMNLP},
  year={2021}
}



@article{Krizhevsky2012ImageNetCW,
  title={ImageNet classification with deep convolutional neural networks},
  author={Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  journal={Communications of the ACM},
  year={2012},
  volume={60},
  pages={84 - 90}
}



@article{Simonyan2015VeryDC,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2015},
  volume={abs/1409.1556}
}


@article{He2016DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}



@article{Huang2017DenselyCC,
  title={Densely Connected Convolutional Networks},
  author={Gao Huang and Zhuang Liu and Kilian Q. Weinberger},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={2261-2269}
}



@article{Wang2022DeepNetST,
  title={DeepNet: Scaling Transformers to 1, 000 Layers},
  author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.00555}
}





@article{He2021AutoMLAS,
  title={AutoML: A Survey of the State-of-the-Art},
  author={Xin He and Kaiyong Zhao and Xiaowen Chu},
  journal={Knowl. Based Syst.},
  year={2021},
  volume={212},
  pages={106622}
}





@article{Real2017LargeScaleEO,
  title={Large-Scale Evolution of Image Classifiers},
  author={Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka Leon Suematsu and Jie Tan and Quoc V. Le and Alexey Kurakin},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.01041}
}


@article{Xie2017GeneticC,
  title={Genetic CNN},
  author={Lingxi Xie and Alan Loddon Yuille},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={1388-1397}
}



@article{Brock2018SMASHOM,
  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},
  author={Andrew Brock and Theodore Lim and James M. Ritchie and Nick Weston},
  journal={ArXiv},
  year={2018},
  volume={abs/1708.05344}
}



@inproceedings{Cai2018EfficientAS,
  title={Efficient Architecture Search by Network Transformation},
  author={Han Cai and Tianyao Chen and Weinan Zhang and Yong Yu and Jun Wang},
  booktitle={AAAI},
  year={2018}
}


@inproceedings{Pham2018EfficientNA,
  title={Efficient Neural Architecture Search via Parameter Sharing},
  author={Hieu Pham and Melody Y. Guan and Barret Zoph and Quoc V. Le and Jeff Dean},
  booktitle={ICML},
  year={2018}
}



@article{Chu2020NoisyDA,
  title={Noisy Differentiable Architecture Search},
  author={Xiangxiang Chu and Bo Zhang and Xudong Li},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.03566}
}



@article{Zhang2021AutomaticSN,
  title={Automatic Student Network Search for Knowledge Distillation},
  author={Zhexi Zhang and Wei Zhu and Junchi Yan and Peng Gao and Guowang Xie},
  journal={2020 25th International Conference on Pattern Recognition (ICPR)},
  year={2021},
  pages={2446-2453}
}


@inproceedings{Zhu2021DiscoveringBM,
  title={Discovering Better Model Architectures for Medical Query Understanding},
  author={Wei Zhu and Yuan Ni and Xiaoling Wang and Guo Tong Xie},
  booktitle={NAACL},
  year={2021}
}



@inproceedings{Chen2020AdaBERTTB,
  title={AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search},
  author={Daoyuan Chen and Yaliang Li and Minghui Qiu and Zhen Wang and Bofang Li and Bolin Ding and Hongbo Deng and Jun Huang and Wei Lin and Jingren Zhou},
  booktitle={IJCAI},
  year={2020}
}


@article{Xu2021NASBERTTA,
  title={NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search},
  author={Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Jian Li and Tao Qin and Tie-Yan Liu},
  journal={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  year={2021}
}



@inproceedings{Dong2021EfficientBERTPS,
  title={EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation},
  author={Chenhe Dong and Guangrun Wang and Hang Xu and Jiefeng Peng and Xiaozhe Ren and Xiaodan Liang},
  booktitle={EMNLP},
  year={2021}
}


@article{Agarap2018DeepLU,
  title={Deep Learning using Rectified Linear Units (ReLU)},
  author={Abien Fred Agarap},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.08375}
}


@article{Hendrycks2016GaussianEL,
  title={Gaussian Error Linear Units (GELUs)},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={arXiv: Learning},
  year={2016}
}


@article{Ramachandran2017SwishAS,
  title={Swish: a Self-Gated Activation Function},
  author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
  journal={arXiv: Neural and Evolutionary Computing},
  year={2017}
}


@article{So2019TheET,
  title={The Evolved Transformer},
  author={David R. So and Chen Liang and Quoc V. Le},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.11117}
}



@inproceedings{Gong2018InformationAV,
  title={Information Aggregation via Dynamic Routing for Sequence Encoding},
  author={Jingjing Gong and Xipeng Qiu and Shaojing Wang and Xuanjing Huang},
  booktitle={COLING},
  year={2018}
}


@inproceedings{Wang2020TextNASAN,
  title={TextNAS: A Neural Architecture Search Space tailored for Text Representation},
  author={Yujing Wang and Yaming Yang and Yiren Chen and Jing Bai and Ce Zhang and Guinan Su and Xiaoyu Kou and Yunhai Tong and Mao Yang and Lidong Zhou},
  booktitle={AAAI},
  year={2020}
}


@inproceedings{Tan2018MultiwayAN,
  title={Multiway Attention Networks for Modeling Sentence Pairs},
  author={Chuanqi Tan and Furu Wei and Wenhui Wang and Weifeng Lv and M. Zhou},
  booktitle={IJCAI},
  year={2018}
}



@article{Liang2021RDropRD,
  title={R-Drop: Regularized Dropout for Neural Networks},
  author={Xiaobo Liang and Lijun Wu and Juntao Li and Yue Wang and Qi Meng and Tao Qin and Wei Chen and M. Zhang and Tie-Yan Liu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.14448}
}




@article{Turc2019WellReadSL,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={arXiv: Computation and Language},
  year={2019}
}


@InProceedings{autotrans,
author="Zhu, Wei
and Wang, Xiaoling
and Ni, Yuan
and Xie, Guotong",
editor="Wang, Lu
and Feng, Yansong
and Hong, Yu
and He, Ruifang",
title="AutoTrans: Automating Transformer Design via Reinforced Architecture Search",
booktitle="Natural Language Processing and Chinese Computing",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="169--182",
abstract="Though the transformer architectures have shown dominance in many natural language understanding tasks, there are still unsolved issues for the training of transformer models, especially the need for a principled way of warm-up which has shown importance for stable training of a transformer, as well as whether the task at hand prefer to scale the attention product or not. In this paper, we empirically explore automating the design choices in the transformer model, i.e., how to set layer-norm, whether to scale, number of layers, number of heads, activation function, etc., so that one can obtain a transformer architecture that better suits the tasks at hand. RL is employed to navigate along search space, and special parameter sharing strategies are designed to accelerate the search. It is shown that sampling a proportion of training data per epoch during search help to improve the search quality. Experiments on the CoNLL03, Multi-30k and WMT-14 shows that the searched transformer model can outperform the standard transformers. In particular, we show that our learned model can be trained more robustly with large learning rates without warm-up.",
isbn="978-3-030-88480-2"
}



% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}



@inproceedings{zhu-etal-2021-gaml,
	title = "{GAML}-{BERT}: Improving {BERT} Early Exiting by Gradient Aligned Mutual Learning",
	author = "Zhu, Wei  and
	Wang, Xiaoling  and
	Ni, Yuan  and
	Xie, Guotong",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.242",
	pages = "3033--3044",
	abstract = "In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT{'}s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT{'}s early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods.",
}



@inproceedings{yu-etal-2020-named,
    title = "Named Entity Recognition as Dependency Parsing",
    author = "Yu, Juntao  and
      Bohnet, Bernd  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.577",
    doi = "10.18653/v1/2020.acl-main.577",
    pages = "6470--6476",
    abstract = "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.",
}


@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,
    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",
    author = "Tjong Kim Sang, Erik F.  and
      De Meulder, Fien",
    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",
    year = "2003",
    url = "https://aclanthology.org/W03-0419",
    pages = "142--147",
}


@inproceedings{xin-etal-2020-deebert,
    title = "{D}ee{BERT}: Dynamic Early Exiting for Accelerating {BERT} Inference",
    author = "Xin, Ji  and
      Tang, Raphael  and
      Lee, Jaejun  and
      Yu, Yaoliang  and
      Lin, Jimmy",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.204",
    doi = "10.18653/v1/2020.acl-main.204",
    pages = "2246--2251",
    abstract = "Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to {\textasciitilde}40{\%} inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT.",
}


@inproceedings{schwartz-etal-2020-right,
    title = "The Right Tool for the Job: Matching Model and Instance Complexities",
    author = "Schwartz, Roy  and
      Stanovsky, Gabriel  and
      Swayamdipta, Swabha  and
      Dodge, Jesse  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.593",
    doi = "10.18653/v1/2020.acl-main.593",
    pages = "6640--6651",
    abstract = "As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) {``}exit{''} from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.",
}


@article{GENIA,
author = {Kim, Jin-Dong and Ohta, Tomoko and Tateisi, Yuka and Tsujii, Jun'ichi},
year = {2003},
month = {02},
pages = {i180-2},
title = {GENIA corpus—A semantically annotated corpus for bio-textmining},
volume = {19 Suppl 1},
journal = {Bioinformatics (Oxford, England)},
doi = {10.1093/bioinformatics/btg1023}
}


@inproceedings{levow-2006-third,
    title = "The Third International {C}hinese Language Processing Bakeoff: Word Segmentation and Named Entity Recognition",
    author = "Levow, Gina-Anne",
    booktitle = "Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-0115",
    pages = "108--117",
}


@inproceedings{zhu-etal-2021-gaml,
    title = "{GAML}-{BERT}: Improving {BERT} Early Exiting by Gradient Aligned Mutual Learning",
    author = "Zhu, Wei  and
      Wang, Xiaoling  and
      Ni, Yuan  and
      Xie, Guotong",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.242",
    doi = "10.18653/v1/2021.emnlp-main.242",
    pages = "3033--3044",
    abstract = "In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT{'}s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT{'}s early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods.",
}



@inproceedings{zhu-2021-leebert,
    title = "{L}ee{BERT}: Learned Early Exit for {BERT} with cross-level optimization",
    author = "Zhu, Wei",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.231",
    doi = "10.18653/v1/2021.acl-long.231",
    pages = "2968--2980",
    abstract = "Pre-trained language models like BERT are performant in a wide range of natural language tasks. However, they are resource exhaustive and computationally expensive for industrial scenarios. Thus, early exits are adopted at each layer of BERT to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference. In this work, to improve efficiency without performance drop, we propose a novel training scheme called Learned Early Exit for BERT (LeeBERT). First, we ask each exit to learn from each other, rather than learning only from the last layer. Second, the weights of different loss terms are learned, thus balancing off different objectives. We formulate the optimization of LeeBERT as a bi-level optimization problem, and we propose a novel cross-level optimization (CLO) algorithm to improve the optimization results. Experiments on the GLUE benchmark show that our proposed methods improve the performance of the state-of-the-art (SOTA) early exit methods for pre-trained models.",
}


@article{Kaya2018HowTS,
  title={How to Stop Off-the-Shelf Deep Neural Networks from Overthinking},
  author={Yigitcan Kaya and Tudor Dumitras},
  journal={ArXiv},
  year={2018},
  volume={abs/1810.07052}
}



@article{HellingerNeueBD,
  title={Neue Begr{\"u}ndung der Theorie quadratischer Formen von unendlichvielen Ver{\"a}nderlichen.},
  author={Ernst Hellinger},
  journal={Journal f{\"u}r die reine und angewandte Mathematik},
  volume={1909},
  pages={210 - 271}
}


@inproceedings{Manning2002FoundationsOS,
  title={Foundations of statistical natural language processing},
  author={Christopher D. Manning and Hinrich Sch{\"u}tze},
  booktitle={SGMD},
  year={2002}
}



@article{Girosi1995RegularizationTA,
  title={Regularization Theory and Neural Networks Architectures},
  author={Federico Girosi and Michael J. Jones and Tomaso A. Poggio},
  journal={Neural Computation},
  year={1995},
  volume={7},
  pages={219-269}
}


@article{Ganaie2021EnsembleDL,
  title={Ensemble deep learning: A review},
  author={M. A. Ganaie and Minghui Hu and Muhammad Tanveer and Ponnuthurai Nagaratnam Suganthan},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.02395}
}



@article{Kim2003GENIAC,
  title={GENIA corpus - a semantically annotated corpus for bio-textmining},
  author={Jin-Dong Kim and Tomoko Ohta and Yuka Tateisi and Junichi Tsujii},
  journal={Bioinformatics},
  year={2003},
  volume={19 Suppl 1},
  pages={
          i180-2
        }
}

@inproceedings{levow-2006-third,
    title = "The Third International {C}hinese Language Processing Bakeoff: Word Segmentation and Named Entity Recognition",
    author = "Levow, Gina-Anne",
    booktitle = "Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-0115",
    pages = "108--117",
}


@inproceedings{lu-roth-2015-joint,
    title = "Joint Mention Extraction and Classification with Mention Hypergraphs",
    author = "Lu, Wei  and
      Roth, Dan",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1102",
    doi = "10.18653/v1/D15-1102",
    pages = "857--867",
}


@inproceedings{katiyar-cardie-2018-nested,
    title = "Nested Named Entity Recognition Revisited",
    author = "Katiyar, Arzoo  and
      Cardie, Claire",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1079",
    doi = "10.18653/v1/N18-1079",
    pages = "861--871",
    abstract = "We propose a novel recurrent neural network-based approach to simultaneously handle nested named entity recognition and nested entity mention detection. The model learns a hypergraph representation for nested entities using features extracted from a recurrent neural network. In evaluations on three standard data sets, we show that our approach significantly outperforms existing state-of-the-art methods, which are feature-based. The approach is also efficient: it operates linearly in the number of tokens and the number of possible output labels at any token. Finally, we present an extension of our model that jointly learns the head of each entity mention.",
}


@inproceedings{ma-hovy-2016-end,
    title = "End-to-end Sequence Labeling via Bi-directional {LSTM}-{CNN}s-{CRF}",
    author = "Ma, Xuezhe  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1101",
    doi = "10.18653/v1/P16-1101",
    pages = "1064--1074",
}


@inproceedings{Wu2019GlyceGF,
  title={Glyce: Glyph-vectors for Chinese Character Representations},
  author={Wei Wu and Yuxian Meng and Fei Wang and Qinghong Han and Muyu Li and Xiaoya Li and Jie Mei and Ping Nie and Xiaofei Sun and Jiwei Li},
  booktitle={NeurIPS},
  year={2019}
}


@article{Li2021AcceleratingBI,
  title={Accelerating BERT Inference for Sequence Labeling via Early-Exit},
  author={Xiaonan Li and Yunfan Shao and Tianxiang Sun and Hang Yan and Xipeng Qiu and Xuanjing Huang},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13878}
}


@article{Cui2020RevisitingPM,
  title={Revisiting Pre-Trained Models for Chinese Natural Language Processing},
  author={Yiming Cui and Wanxiang Che and Ting Liu and Bing Qin and Shijin Wang and Guoping Hu},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.13922}
}


@article{Sun2021EarlyEW,
  title={Early Exiting with Ensemble Internal Classifiers},
  author={Tianxiang Sun and Yunhua Zhou and Xiangyang Liu and Xinyu Zhang and Hao Jiang and Zhao Cao and Xuanjing Huang and Xipeng Qiu},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13792}
}





@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}


@article{devlin2018bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}




@inproceedings{Yang2019XLNetGA,
	title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
	author={Z. Yang and Zihang Dai and Yiming Yang and J. Carbonell and R. Salakhutdinov and Quoc V. Le},
	booktitle={NeurIPS},
	year={2019}
}


@article{Fan2020layerdrop,
	title={Reducing Transformer Depth on Demand with Structured Dropout},
	author={Angela Fan and E. Grave and Armand Joulin},
	journal={ArXiv},
	year={2020},
	volume={abs/1909.11556}
}


@inproceedings{Michel2019sixteenhead,
	title={Are Sixteen Heads Really Better than One?},
	author={Paul Michel and Omer Levy and Graham Neubig},
	booktitle={NeurIPS},
	year={2019}
}



@article{Zhou2020PABEE,
	title={BERT Loses Patience: Fast and Robust Inference with Early Exit},
	author={Wangchunshu Zhou and Canwen Xu and Tao Ge and Julian McAuley and Ke Xu and Furu Wei},
	journal={ArXiv},
	year={2020},
	volume={abs/2006.04152}
}

@inproceedings{Kaya2019ShallowDeepNU,
	title={Shallow-Deep Networks: Understanding and Mitigating Network Overthinking},
	author={Y. Kaya and Sanghyun Hong and T. Dumitras},
	booktitle={ICML},
	year={2019}
}

@article{Zhu2018ToPO,
	title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
	author={M. Zhu and S. Gupta},
	journal={ArXiv},
	year={2018},
	volume={abs/1710.01878}
}


@inproceedings{Xu2020BERTofTheseusCB,
	title={BERT-of-Theseus: Compressing BERT by Progressive Module Replacing},
	author={Canwen Xu and Wangchunshu Zhou and Tao Ge and Furu Wei and M. Zhou},
	booktitle={EMNLP},
	year={2020}
}

@inproceedings{Sun2019PatientKD,
	title={Patient Knowledge Distillation for BERT Model Compression},
	author={S. Sun and Yu Cheng and Zhe Gan and Jingjing Liu},
	booktitle={EMNLP/IJCNLP},
	year={2019}
}

@article{Sanh2019DistilBERTAD,
	title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
	author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	journal={ArXiv},
	year={2019},
	volume={abs/1910.01108}
}


@article{Jiao2020TinyBERTDB,
	title={TinyBERT: Distilling BERT for Natural Language Understanding},
	author={Xiaoqi Jiao and Y. Yin and L. Shang and Xin Jiang and X. Chen and Linlin Li and F. Wang and Qun Liu},
	journal={ArXiv},
	year={2020},
	volume={abs/1909.10351}
}


@article{Zhang2020TernaryBERTDU,
	title={TernaryBERT: Distillation-aware Ultra-low Bit BERT},
	author={W. Zhang and L. Hou and Y. Yin and L. Shang and X. Chen and X. Jiang and Qun Liu},
	journal={ArXiv},
	year={2020},
	volume={abs/2009.12812}
}

@article{Bai2020BinaryBERTPT,
	title={BinaryBERT: Pushing the Limit of BERT Quantization},
	author={Haoli Bai and Wei Zhang and L. Hou and L. Shang and Jing Jin and X. Jiang and Qun Liu and Michael R. Lyu and Irwin King},
	journal={ArXiv},
	year={2020},
	volume={abs/2012.15701}
}

@article{Kim2021IBERTIB,
	title={I-BERT: Integer-only BERT Quantization},
	author={Se-Hoon Kim and Amir Gholami and Zhewei Yao and M. W. Mahoney and K. Keutzer},
	journal={ArXiv},
	year={2021},
	volume={abs/2101.01321}
}



@article{Xin2020DeeBERT,
	title={DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference},
	author={J. Xin and Raphael Tang and J. Lee and Y. Yu and Jimmy Lin},
	journal={ArXiv},
	year={2020},
	volume={abs/2004.12993}
}


@inproceedings{Geng2021RomeBERTRT,
	title={RomeBERT: Robust Training of Multi-Exit BERT},
	author={Shijie Geng and Peng Gao and Z. Fu and Yongfeng Zhang},
	year={2021}
}


@article{Liu2020FastBERTAS,
	title={FastBERT: a Self-distilling BERT with Adaptive Inference Time},
	author={Weijie Liu and P. Zhou and Zhe Zhao and Zhiruo Wang and Haotang Deng and Q. Ju},
	journal={ArXiv},
	year={2020},
	volume={abs/2004.02178}
}


@inproceedings{Bolukbasi2017AdaptiveNN,
	title={Adaptive Neural Networks for Efficient Inference},
	author={Tolga Bolukbasi and J. Wang and O. Dekel and Venkatesh Saligrama},
	booktitle={ICML},
	year={2017}
}


@article{Teerapittayanon2016BranchyNetFI,
	title={BranchyNet: Fast inference via early exiting from deep neural networks},
	author={Surat Teerapittayanon and Bradley McDanel and H. T. Kung},
	journal={2016 23rd International Conference on Pattern Recognition (ICPR)},
	year={2016},
	pages={2464-2469}
}


@article{Phuong2019DistillationBasedTF,
	title={Distillation-Based Training for Multi-Exit Architectures},
	author={Mary Phuong and Christoph H. Lampert},
	journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
	year={2019},
	pages={1355-1364}
}



@article{Yu2020GradientSF,
	title={Gradient Surgery for Multi-Task Learning},
	author={Tianhe Yu and Saurabh Kumar and A. Gupta and S. Levine and Karol Hausman and Chelsea Finn},
	journal={ArXiv},
	year={2020},
	volume={abs/2001.06782}
}






@inproceedings{Schwartz2020TheRT,
	title={The Right Tool for the Job: Matching Model and Instance Complexities},
	author={Roy Schwartz and Gabi Stanovsky and Swabha Swayamdipta and Jesse Dodge and N. A. Smith},
	booktitle={ACL},
	year={2020}
}


@article{Hinton2015DistillingTK,
	title={Distilling the Knowledge in a Neural Network},
	author={Geoffrey E. Hinton and Oriol Vinyals and J. Dean},
	journal={ArXiv},
	year={2015},
	volume={abs/1503.02531}
}



@article{Turc2019WellReadSL,
	title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
	author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	journal={arXiv: Computation and Language},
	year={2019}
}


@inproceedings{Krizhevsky2009LearningML,
	title={Learning Multiple Layers of Features from Tiny Images},
	author={A. Krizhevsky},
	year={2009}
}


@article{He2016DeepRL,
	title={Deep Residual Learning for Image Recognition},
	author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
	journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2016},
	pages={770-778}
}



@inproceedings{MRPC,
	title = "Automatically Constructing a Corpus of Sentential Paraphrases",
	author = "Dolan, William B.  and
	Brockett, Chris",
	booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
	year = "2005",
	url = "https://www.aclweb.org/anthology/I05-5002",
}


@inproceedings{socher-etal-2013-recursive,
	title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
	author = "Socher, Richard  and
	Perelygin, Alex  and
	Wu, Jean  and
	Chuang, Jason  and
	Manning, Christopher D.  and
	Ng, Andrew  and
	Potts, Christopher",
	booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
	month = oct,
	year = "2013",
	address = "Seattle, Washington, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D13-1170",
	pages = "1631--1642",
}



@inproceedings{williams-etal-2018-broad,
	title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
	author = "Williams, Adina  and
	Nangia, Nikita  and
	Bowman, Samuel",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N18-1101",
	doi = "10.18653/v1/N18-1101",
	pages = "1112--1122",
	abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}



@inproceedings{rajpurkar-etal-2016-squad,
	title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
	author = "Rajpurkar, Pranav  and
	Zhang, Jian  and
	Lopyrev, Konstantin  and
	Liang, Percy",
	booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2016",
	address = "Austin, Texas",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D16-1264",
	doi = "10.18653/v1/D16-1264",
	pages = "2383--2392",
}



@article{warstadt-etal-2019-neural,
	title = "Neural Network Acceptability Judgments",
	author = "Warstadt, Alex  and
	Singh, Amanpreet  and
	Bowman, Samuel R.",
	journal = "Transactions of the Association for Computational Linguistics",
	volume = "7",
	month = mar,
	year = "2019",
	url = "https://www.aclweb.org/anthology/Q19-1040",
	doi = "10.1162/tacl_a_00290",
	pages = "625--641",
	abstract = "This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.{'}s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.",
}


@inproceedings{wnli,
	author = {Levesque, Hector J. and Davis, Ernest and Morgenstern, Leora},
	title = {The Winograd Schema Challenge},
	year = {2012},
	isbn = {9781577355601},
	publisher = {AAAI Press},
	abstract = {In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Wino-grad schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.},
	booktitle = {Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning},
	pages = {552–561},
	numpages = {10},
	location = {Rome, Italy},
	series = {KR'12}
}





@inproceedings{jiao-etal-2020-tinybert,
	title = "{T}iny{BERT}: Distilling {BERT} for Natural Language Understanding",
	author = "Jiao, Xiaoqi  and
	Yin, Yichun  and
	Shang, Lifeng  and
	Jiang, Xin  and
	Chen, Xiao  and
	Li, Linlin  and
	Wang, Fang  and
	Liu, Qun",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/2020.findings-emnlp.372",
	doi = "10.18653/v1/2020.findings-emnlp.372",
	pages = "4163--4174",
	abstract = "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large {``}teacher{''} BERT can be effectively transferred to a small {``}student{''} TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8{\%} the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only {\textasciitilde}28{\%} parameters and {\textasciitilde}31{\%} inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
}



@inproceedings{xu-etal-2020-bert,
	title = "{BERT}-of-Theseus: Compressing {BERT} by Progressive Module Replacing",
	author = "Xu, Canwen  and
	Zhou, Wangchunshu  and
	Ge, Tao  and
	Wei, Furu  and
	Zhou, Ming",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/2020.emnlp-main.633",
	doi = "10.18653/v1/2020.emnlp-main.633",
	pages = "7859--7869",
	abstract = "In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.",
}






@article{Hinton2015DistillingTK,
	title={Distilling the Knowledge in a Neural Network},
	author={Geoffrey E. Hinton and Oriol Vinyals and J. Dean},
	journal={ArXiv},
	year={2015},
	volume={abs/1503.02531}
}





@article{Huang2017MultiScaleDC,
	title={Multi-Scale Dense Convolutional Networks for Efficient Prediction},
	author={Gao Huang and Danlu Chen and T. Li and Felix Wu and L. V. D. Maaten and Kilian Q. Weinberger},
	journal={ArXiv},
	year={2017},
	volume={abs/1703.09844}
}




@article{Szegedy2016RethinkingTI,
	title={Rethinking the Inception Architecture for Computer Vision},
	author={Christian Szegedy and V. Vanhoucke and S. Ioffe and Jonathon Shlens and Z. Wojna},
	journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2016},
	pages={2818-2826}
}


@inproceedings{Yang2020TextBrewerAO,
	title={TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing},
	author={Ziqing Yang and Yiming Cui and ZhiPeng Chen and Wanxiang Che and T. Liu and S. Wang and Guoping Hu},
	booktitle={ACL},
	year={2020}
}


@inproceedings{Sun2019PatientKD,
	title={Patient Knowledge Distillation for BERT Model Compression},
	author={S. Sun and Yu Cheng and Zhe Gan and Jingjing Liu},
	booktitle={EMNLP/IJCNLP},
	year={2019}
}


@article{Li2021AcceleratingBI,
  title={Accelerating BERT Inference for Sequence Labeling via Early-Exit},
  author={Xiaonan Li and Yunfan Shao and Tianxiang Sun and Hang Yan and Xipeng Qiu and Xuanjing Huang},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13878}
}


@article{Kullback1951OnIA,
  title={On Information and Sufficiency},
  author={Solomon Kullback and R. A. Leibler},
  journal={Annals of Mathematical Statistics},
  year={1951},
  volume={22},
  pages={79-86}
}



@inproceedings{Sun2020LearningSS,
  title={Learning Sparse Sharing Architectures for Multiple Tasks},
  author={Tianxiang Sun and Yunfan Shao and Xiaonan Li and Pengfei Liu and Hang Yan and Xipeng Qiu and Xuanjing Huang},
  booktitle={AAAI},
  year={2020}
}



@article{Hendrycks2016BridgingNA,
  title={Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.08415}
}



@article{Huang2015BidirectionalLM,
  title={Bidirectional LSTM-CRF Models for Sequence Tagging},
  author={Zhiheng Huang and Wei Xu and Kai Yu},
  journal={ArXiv},
  year={2015},
  volume={abs/1508.01991}
}




@inproceedings{BADGE,
	title={BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting},
	author={Wei Zhu and Peng Wang and Yuan Ni and Guotong Xie and Xiaoling Wang},
	booktitle={ACL Industry},
	year={2023}
}


@inproceedings{zhu-etal-2019-panlp,
    title = "{PANLP} at {MEDIQA} 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation",
    author = "Zhu, Wei  and
      Zhou, Xiaofeng  and
      Wang, Keqiang  and
      Luo, Xun  and
      Li, Xiepeng  and
      Ni, Yuan  and
      Xie, Guotong",
    booktitle = "Proceedings of the 18th BioNLP Workshop and Shared Task",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5040",
    doi = "10.18653/v1/W19-5040",
    pages = "380--388",
    abstract = "This paper describes the models designated for the MEDIQA 2019 shared tasks by the team PANLP. We take advantages of the recent advances in pre-trained bidirectional transformer language models such as BERT (Devlin et al., 2018) and MT-DNN (Liu et al., 2019b). We find that pre-trained language models can significantly outperform traditional deep learning models. Transfer learning from the NLI task to the RQE task is also experimented, which proves to be useful in improving the results of fine-tuning MT-DNN large. A knowledge distillation process is implemented, to distill the knowledge contained in a set of models and transfer it into an single model, whose performance turns out to be comparable with that obtained by the ensemble of that set of models. Finally, for test submissions, model ensemble and a re-ranking process are implemented to boost the performances. Our models participated in all three tasks and ranked the 1st place for the RQE task, and the 2nd place for the NLI task, and also the 2nd place for the QA task.",
}



@inproceedings{zhang-etal-2022-pcee,
    title = "{PCEE}-{BERT}: Accelerating {BERT} Inference via Patient and Confident Early Exiting",
    author = "Zhang, Zhen  and
      Zhu, Wei  and
      Zhang, Jinfan  and
      Wang, Peng  and
      Jin, Rize  and
      Chung, Tae-Sun",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.25",
    doi = "10.18653/v1/2022.findings-naacl.25",
    pages = "327--338",
    abstract = "BERT and other pretrained language models (PLMs) are ubiquitous in modern NLP. Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (CITATION), the significant latency during inference prohibits wider industrial usage. In this work, we propose Patient and Confident Early Exiting BERT (PCEE-BERT), an off-the-shelf sample-dependent early exiting method that can work with different PLMs and can also work along with popular model compression methods. With a multi-exit BERT as the backbone model, PCEE-BERT will make the early exiting decision if enough numbers (patience parameter) of consecutive intermediate layers are confident about their predictions. The entropy value measures the confidence level of an intermediate layer{'}s prediction. Experiments on the GLUE benchmark demonstrate that our method outperforms previous SOTA early exiting methods. Ablation studies show that: (a) our method performs consistently well on other PLMs, such as ALBERT and TinyBERT; (b) PCEE-BERT can achieve different speed-up ratios by adjusting the patience parameter and the confidence threshold. The code for PCEE-BERT can be found at \url{https://github.com/michael-wzhu/PCEE-BERT}.",
}


@INPROCEEDINGS{10094859,
  author={Zhu, Wei and Wang, Peng and Wang, Xiaoling and Ni, Yuan and Xie, Guotong},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={ACF: Aligned Contrastive Finetuning For Language and Vision Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICASSP49357.2023.10094859}}



  @inproceedings{li-etal-2019-pingan,
    title = "Pingan Smart Health and {SJTU} at {COIN} - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks",
    author = "Li, Xiepeng  and
      Zhang, Zhexi  and
      Zhu, Wei  and
      Li, Zheng  and
      Ni, Yuan  and
      Gao, Peng  and
      Yan, Junchi  and
      Xie, Guotong",
    booktitle = "Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6011",
    doi = "10.18653/v1/D19-6011",
    pages = "93--98",
    abstract = "To solve the shared tasks of COIN: COmmonsense INference in Natural Language Processing) Workshop in , we need explore the impact of knowledge representation in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching. There are two approaches to represent knowledge in the low-dimensional space. The first is to leverage large-scale unsupervised text corpus to train fixed or contextual language representations. The second approach is to explicitly express knowledge into a knowledge graph (KG), and then fit a model to represent the facts in the KG. We have experimented both (a) improving the fine-tuning of pre-trained language models on a task with a small dataset size, by leveraging datasets of similar tasks; and (b) incorporating the distributional representations of a KG onto the representations of pre-trained language models, via simply concatenation or multi-head attention. We find out that: (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target task improve the performance significantly; (b) for task 2, we find out the incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace of XLNET (Yangetal.,2019), a more powerful pre-trained model. Our approaches achieve the state-of-the-art results on both shared task{'}s official test data, outperforming all the other submissions.",
}





% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@inproceedings{wu2021peer,
  title={Peer collaborative learning for online knowledge distillation},
  author={Wu, Guile and Gong, Shaogang},
  booktitle={AAAI},
  year={2021}
}
@article{liu2020fastbert,
  title={Fastbert: a self-distilling bert with adaptive inference time},
  author={Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Deng, Haotang and Ju, Qi},
  journal={arXiv preprint arXiv:2004.02178},
  year={2020}
}
@article{xin2020deebert,
  title={DeeBERT: Dynamic early exiting for accelerating BERT inference},
  author={Xin, Ji and Tang, Raphael and Lee, Jaejun and Yu, Yaoliang and Lin, Jimmy},
  journal={arXiv preprint arXiv:2004.12993},
  year={2020}
}
@inproceedings{phuong2019distillation,
  title={Distillation-based training for multi-exit architectures},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1355--1364},
  year={2019}
}



@inproceedings{xin2021berxit,
  title={BERxiT: Early Exiting for BERT with Better fine-tuning and extension to regression},
  author={Xin, Ji and Tang, Raphael and Yu, Yaoliang and Lin, Jimmy},
  booktitle={Proceedings of the 16th conference of the European chapter of the association for computational linguistics: Main Volume},
  pages={91--104},
  year={2021}
}
@article{fan2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1909.11556},
  year={2019}
}
@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{zhou2020bert,
  title={Bert loses patience: Fast and robust inference with early exit},
  author={Zhou, Wangchunshu and Xu, Canwen and Ge, Tao and McAuley, Julian and Xu, Ke and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18330--18341},
  year={2020}
}
@inproceedings{kaya2019shallow,
  title={Shallow-deep networks: Understanding and mitigating network overthinking},
  author={Kaya, Yigitcan and Hong, Sanghyun and Dumitras, Tudor},
  booktitle={International conference on machine learning},
  pages={3301--3310},
  year={2019},
  organization={PMLR}
}
@article{zhang2018overview,
  title={An overview of multi-task learning},
  author={Zhang, Yu and Yang, Qiang},
  journal={National Science Review},
  volume={5},
  number={1},
  pages={30--43},
  year={2018},
  publisher={Oxford University Press}
}
@article{schwartz2020right,
  title={The right tool for the job: Matching model and instance complexities},
  author={Schwartz, Roy and Stanovsky, Gabriel and Swayamdipta, Swabha and Dodge, Jesse and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.07453},
  year={2020}
}
@article{he2021towards,
  title={Towards a unified view of parameter-efficient transfer learning},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  journal={arXiv preprint arXiv:2110.04366},
  year={2021}
}

@article{xu2020bert,
  title={Bert-of-theseus: Compressing bert by progressive module replacing},
  author={Xu, Canwen and Zhou, Wangchunshu and Ge, Tao and Wei, Furu and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.02925},
  year={2020}
}
@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}
@article{gordon2020compressing,
  title={Compressing bert: Studying the effects of weight pruning on transfer learning},
  author={Gordon, Mitchell A and Duh, Kevin and Andrews, Nicholas},
  journal={arXiv preprint arXiv:2002.08307},
  year={2020}
}
@article{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}
@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}
@article{zhang2020ternarybert,
  title={Ternarybert: Distillation-aware ultra-low bit bert},
  author={Zhang, Wei and Hou, Lu and Yin, Yichun and Shang, Lifeng and Chen, Xiao and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2009.12812},
  year={2020}
}
@article{bai2020binarybert,
  title={Binarybert: Pushing the limit of bert quantization},
  author={Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  journal={arXiv preprint arXiv:2012.15701},
  year={2020}
}
@inproceedings{kim2021bert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}
@inproceedings{bolukbasi2017adaptive,
  title={Adaptive neural networks for efficient inference},
  author={Bolukbasi, Tolga and Wang, Joseph and Dekel, Ofer and Saligrama, Venkatesh},
  booktitle={International Conference on Machine Learning},
  pages={527--536},
  year={2017},
  organization={PMLR}
}
@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press}
}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-
drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.
In Proceedings of ICML, 2019


@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}
@article{zhao2019uer,
  title={UER: An Open-Source Toolkit for Pre-training Models},
  author={Zhao, Zhe and Chen, Hui and Zhang, Jinbin and Zhao, Xin and Liu, Tao and Lu, Wei and Chen, Xi and Deng, Haotang and Ju, Qi and Du, Xiaoyong},
  journal={EMNLP-IJCNLP 2019},
  pages={241},
  year={2019}
}



@article{Wu2018UnsupervisedFL,
  title={Unsupervised Feature Learning via Non-parametric Instance Discrimination},
  author={Zhirong Wu and Yuanjun Xiong and Stella X. Yu and Dahua Lin},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={3733-3742}
}


@article{Oord2018RepresentationLW,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.03748}
}


@inproceedings{Tian2020ContrastiveMC,
  title={Contrastive Multiview Coding},
  author={Yonglong Tian and Dilip Krishnan and Phillip Isola},
  booktitle={ECCV},
  year={2020}
}

@article{He2020MomentumCF,
  title={Momentum Contrast for Unsupervised Visual Representation Learning},
  author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross B. Girshick},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={9726-9735}
}



@article{Chen2021ExploringSS,
  title={Exploring Simple Siamese Representation Learning},
  author={Xinlei Chen and Kaiming He},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={15745-15753}
}

@inproceedings{Wang2020UnderstandingCR,
  title={Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
  author={Tongzhou Wang and Phillip Isola},
  booktitle={ICML},
  year={2020}
}



@article{Jaiswal2020ASO,
  title={A Survey on Contrastive Self-supervised Learning},
  author={Ashish Jaiswal and Ashwin Ramesh Babu and Mohammad Zaki Zadeh and Debapriya Banerjee and Fillia Makedon},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.00362}
}



@article{Khosla2020SupervisedCL,
  title={Supervised Contrastive Learning},
  author={Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.11362}
}




@article{Teerapittayanon2016BranchyNetFI,
  title={BranchyNet: Fast inference via early exiting from deep neural networks},
  author={Surat Teerapittayanon and Bradley McDanel and H. T. Kung},
  journal={2016 23rd International Conference on Pattern Recognition (ICPR)},
  year={2016},
  pages={2464-2469}
}



@inproceedings{Kaya2019ShallowDeepNU,
	title={Shallow-Deep Networks: Understanding and Mitigating Network Overthinking},
	author={Y. Kaya and Sanghyun Hong and T. Dumitras},
	booktitle={ICML},
	year={2019}
}



@article{Zhou2020PABEE,
	title={BERT Loses Patience: Fast and Robust Inference with Early Exit},
	author={Wangchunshu Zhou and Canwen Xu and Tao Ge and Julian McAuley and Ke Xu and Furu Wei},
	journal={ArXiv},
	year={2020},
	volume={abs/2006.04152}
}


@inproceedings{Zhu2021LeeBERTLE,
  title={LeeBERT: Learned Early Exit for BERT with cross-level optimization},
  author={Wei Zhu},
  booktitle={ACL},
  year={2021}
}

@inproceedings{Zhu2021GAMLBERTIB,
  title={GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning},
  author={Wei Zhu and Xiaoling Wang and Yuan Ni and Guo Tong Xie},
  booktitle={EMNLP},
  year={2021}
}



@article{Hadsell2006DimensionalityRB,
  title={Dimensionality Reduction by Learning an Invariant Mapping},
  author={Raia Hadsell and Sumit Chopra and Yann LeCun},
  journal={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  year={2006},
  volume={2},
  pages={1735-1742}
}






@article{Fang2020CERTCS,
  title={CERT: Contrastive Self-supervised Learning for Language Understanding},
  author={Hongchao Fang and Sicheng Wang and Meng Zhou and Jiayuan Ding and Pengtao Xie},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.12766}
}


@article{Tambe2020EdgeBERTOO,
  title={EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP},
  author={Thierry Tambe and Coleman Hooper and Lillian Pentecost and En-Yu Yang and Marco Donato and Victor Sanh and Alexander M. Rush and David M. Brooks and Gu-Yeon Wei},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.14203}
}


@inproceedings{sun-etal-2022-simple,
    title = "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation",
    author = "Sun, Tianxiang  and
      Liu, Xiangyang  and
      Zhu, Wei  and
      Geng, Zhichao  and
      Wu, Lingling  and
      He, Yilong  and
      Ni, Yuan  and
      Xie, Guotong  and
      Huang, Xuanjing  and
      Qiu, Xipeng",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.189",
    doi = "10.18653/v1/2022.findings-acl.189",
    pages = "2409--2421",
    abstract = "Early exiting allows instances to exit at different layers according to the estimation of difficulty.Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning. In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way. Though some effort has been devoted to employing such {``}learn-to-exit{''} modules, it is still unknown whether and how well the instance difficulty can be learned. As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty. Based on this observation, we propose a simple-yet-effective Hash-based Early Exiting approach HashEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. Different from previous methods, HashEE requires no internal classifiers nor extra parameters, and therefore is more efficient.HashEE can be used in various tasks (including language understanding and generation) and model architectures such as seq2seq models. Experimental results on classification, regression, and generation tasks demonstrate that HashEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.",
}


@article{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.03762}
}


@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}

@inproceedings{Xu2020BERTofTheseusCB,
  title={BERT-of-Theseus: Compressing BERT by Progressive Module Replacing},
  author={Canwen Xu and Wangchunshu Zhou and Tao Ge and Furu Wei and Ming Zhou},
  booktitle={EMNLP},
  year={2020}
}


@article{Fan2020ReducingTD,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Angela Fan and Edouard Grave and Armand Joulin},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.11556}
}


@inproceedings{Michel2019AreSH,
  title={Are Sixteen Heads Really Better than One?},
  author={Paul Michel and Omer Levy and Graham Neubig},
  booktitle={NeurIPS},
  year={2019}
}


@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}

@article{Jiao2020TinyBERTDB,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.10351}
}


@article{Xu2021ASO,
  title={A Survey on Green Deep Learning},
  author={Jingjing Xu and Wangchunshu Zhou and Zhiyi Fu and Hao Zhou and Lei Li},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.05193}
}


@inproceedings{Zhang2022LabelAC,
  title={Label Anchored Contrastive Learning for Language Understanding},
  author={Zhenyu Zhang and Yuming Zhao and Meng Chen and Xiaodong He},
  booktitle={NAACL},
  year={2022}
}


@inproceedings{Wang2018JointEO,
  title={Joint Embedding of Words and Labels for Text Classification},
  author={Guoyin Wang and Chunyuan Li and Wenlin Wang and Yizhe Zhang and Dinghan Shen and Xinyuan Zhang and Ricardo Henao and Lawrence Carin},
  booktitle={ACL},
  year={2018}
}



@inproceedings{Xiao2019LabelSpecificDR,
  title={Label-Specific Document Representation for Multi-Label Text Classification},
  author={Lin Xiao and Xin Huang and Boli Chen and Liping Jing},
  booktitle={EMNLP},
  year={2019}
}



@article{Yan2021ConSERTAC,
  title={ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer},
  author={Yuanmeng Yan and Rumei Li and Sirui Wang and Fuzheng Zhang and Wei Wu and Weiran Xu},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.11741}
}



@article{Gunel2021SupervisedCL,
  title={Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning},
  author={Beliz Gunel and Jingfei Du and Alexis Conneau and Ves Stoyanov},
  journal={ArXiv},
  year={2021},
  volume={abs/2011.01403}
}



@article{Sedghamiz2021SupCLSeqSC,
  title={SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations},
  author={Hooman Sedghamiz and Shivam Raval and Enrico Santus and Tuka Alhanai and Mohammad Mahdi Ghassemi},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.07424}
}


@article{Cui2020RevisitingPM,
  title={Revisiting Pre-Trained Models for Chinese Natural Language Processing},
  author={Yiming Cui and Wanxiang Che and Ting Liu and Bing Qin and Shijin Wang and Guoping Hu},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.13922}
}



@inproceedings{xu-etal-2020-clue,
    title = "{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark",
    author = "Xu, Liang  and
      Hu, Hai  and
      Zhang, Xuanwei  and
      Li, Lu  and
      Cao, Chenjie  and
      Li, Yudong  and
      Xu, Yechen  and
      Sun, Kai  and
      Yu, Dian  and
      Yu, Cong  and
      Tian, Yin  and
      Dong, Qianqian  and
      Liu, Weitang  and
      Shi, Bo  and
      Cui, Yiming  and
      Li, Junyi  and
      Zeng, Jun  and
      Wang, Rongzhao  and
      Xie, Weijian  and
      Li, Yanting  and
      Patterson, Yina  and
      Tian, Zuoyu  and
      Zhang, Yiwen  and
      Zhou, He  and
      Liu, Shaoweihua  and
      Zhao, Zhe  and
      Zhao, Qipeng  and
      Yue, Cong  and
      Zhang, Xinrui  and
      Yang, Zhengliang  and
      Richardson, Kyle  and
      Lan, Zhenzhong",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.419",
    doi = "10.18653/v1/2020.coling-main.419",
    pages = "4762--4772",
    abstract = "The advent of natural language understanding (NLU) benchmarks for English, such as GLUE and SuperGLUE allows new NLU models to be evaluated across a diverse set of tasks. These comprehensive benchmarks have facilitated a broad range of research and applications in natural language processing (NLP). The problem, however, is that most such benchmarks are limited to English, which has made it difficult to replicate many of the successes in English NLU for other languages. To help remedy this issue, we introduce the first large-scale Chinese Language Understanding Evaluation (CLUE) benchmark. CLUE is an open-ended, community-driven project that brings together 9 tasks spanning several well-established single-sentence/sentence-pair classification tasks, as well as machine reading comprehension, all on original Chinese text. To establish results on these tasks, we report scores using an exhaustive set of current state-of-the-art pre-trained Chinese models (9 in total). We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on Chinese NLU. Our benchmark is released at https://www.cluebenchmarks.com",
}


@inproceedings{Krizhevsky2009LearningML,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  booktitle={cs.toronto.edu},
  year={2009}
}


@article{Russakovsky2015ImageNetLS,
  title={ImageNet Large Scale Visual Recognition Challenge},
  author={Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael S. Bernstein and Alexander C. Berg and Li Fei-Fei},
  journal={International Journal of Computer Vision},
  year={2015},
  volume={115},
  pages={211-252}
}

@InProceedings{He_2016_CVPR,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}



@article{Cubuk2018AutoAugmentLA,
  title={AutoAugment: Learning Augmentation Policies from Data},
  author={Ekin Dogus Cubuk and Barret Zoph and Dandelion Man{\'e} and Vijay Vasudevan and Quoc V. Le},
  journal={ArXiv},
  year={2018},
  volume={abs/1805.09501}
}



@inproceedings{Sun2019HowTF,
  title={How to Fine-Tune BERT for Text Classification?},
  author={Chi Sun and Xipeng Qiu and Yige Xu and Xuanjing Huang},
  booktitle={CCL},
  year={2019}
}


@article{Chen2020BigSM,
  title={Big Self-Supervised Models are Strong Semi-Supervised Learners},
  author={Ting Chen and Simon Kornblith and Kevin Swersky and Mohammad Norouzi and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.10029}
}


@article{Chen2020MixTextLI,
  title={MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification},
  author={Jiaao Chen and Zichao Yang and Diyi Yang},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.12239}
}


@article{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}



@article{Maaten2008VisualizingDU,
  title={Visualizing Data using t-SNE},
  author={Laurens van der Maaten and Geoffrey E. Hinton},
  journal={Journal of Machine Learning Research},
  year={2008},
  volume={9},
  pages={2579-2605}
}


@article{Chen2020ASF,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.05709}
}


@article{Saeed2021ContrastiveLO,
  title={Contrastive Learning of General-Purpose Audio Representations},
  author={Aaqib Saeed and David Grangier and Neil Zeghidour},
  journal={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2021},
  pages={3875-3879}
}

@INPROCEEDINGS{seq_cpc,
  author={Chen, Yulong and Zhao, Jianping and Wang, Weiqi and Fang, Ming and Kang, Haimei and Wang, Lu and Wei, Tao and Ma, Jun and Wang, Shaojun and Xiao, Jing},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={SEQ-CPC: Sequential Contrastive Predictive Coding for Automatic Speech Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={3880-3884},
  doi={10.1109/ICASSP39728.2021.9413645}}



@article{Gao2021SimCSESC,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08821}
}



@article{Suresh2021NotAN,
  title={Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification},
  author={Varsha Suresh and Desmond C. Ong},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.05427}
}





@inproceedings{Ethayarajh2019HowCA,
  title={How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings},
  author={Kawin Ethayarajh},
  booktitle={EMNLP},
  year={2019}
}


@inproceedings{Cai2021IsotropyIT,
  title={Isotropy in the Contextual Embedding Space: Clusters and Manifolds},
  author={Xingyu Cai and Jiaji Huang and Yu-Lan Bian and Kenneth Ward Church},
  booktitle={ICLR},
  year={2021}
}


@inproceedings{Meng2020TextCU,
  title={Text Classification Using Label Names Only: A Language Model Self-Training Approach},
  author={Yu Meng and Yunyi Zhang and Jiaxin Huang and Chenyan Xiong and Heng Ji and Chao Zhang and Jiawei Han},
  booktitle={EMNLP},
  year={2020}
}



@article{Su2021WhiteningSR,
  title={Whitening Sentence Representations for Better Semantics and Faster Retrieval},
  author={Jianlin Su and Jiarun Cao and Weijie Liu and Yangyiwen Ou},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.15316}
}


@article{Zhou2021IsoBNFB,
  title={IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization},
  author={Wenxuan Zhou and Bill Yuchen Lin and Xiang Ren},
  journal={ArXiv},
  year={2021},
  volume={abs/2005.02178}
}



@article{lehmann2015dbpedia,
title={DBpedia--a large-scale, multilingual knowledge base extracted from Wikipedia},
author={Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas,
Dimitris and Mendes, Pablo N and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef,
Patrick and Auer, S{\"o}ren and others},
journal={Semantic web},
volume={6},
number={2},
pages={167--195},
year={2015},
publisher={IOS Press}
}


@article{Liu2022OpenIE,
  title={Open Information Extraction from 2007 to 2022 - A Survey},
  author={Pai Liu and Wenya Gao and Wen Dong and Songfang Huang and Yue Zhang},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.08690},
  url={https://api.semanticscholar.org/CorpusID:251643659}
}


@article{Zhu2020AutoTransAT,
  title={AutoTrans: Automating Transformer Design via Reinforced Architecture Search},
  author={Wei Zhu and Xiaoling Wang and Xipeng Qiu and Yuan Ni and Guo Tong Xie},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.02070},
  url={https://api.semanticscholar.org/CorpusID:221507850}
}


@inproceedings{Zhang2023NAGNERAU,
  title={NAG-NER: a Unified Non-Autoregressive Generation Framework for Various NER Tasks},
  author={Xinpeng Zhang and Ming Tan and Jingfan Zhang and Wei Zhu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259370837}
}


@inproceedings{Zhu2020AutoRCIB,
  title={AutoRC: Improving BERT Based Relation Classification Models via Architecture Search},
  author={Wei Zhu and Xiaoling Wang and Xipeng Qiu and Yuan Ni and Guo Tong Xie},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221836671}
}




@InProceedings{text2dt_shared_task,
author="Zhu, Wei
and Li, Wenfeng
and Wang, Xiaoling
and Ji, Wendi
and Wu, Yuanbin
and Chen, Jin
and Chen, Liang
and Tang, Buzhou",
editor="Tang, Buzhou
and Chen, Qingcai
and Lin, Hongfei
and Wu, Fei
and Liu, Lei
and Hao, Tianyong
and Wang, Yanshan
and Wang, Haitian
and Lei, Jianbo
and Li, Zuofeng
and Zong, Hui",
title="Extracting Decision Trees from Medical Texts: An Overview of the Text2DT Track in CHIP2022",
booktitle="Health Information Processing. Evaluation Track Papers",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="89--102",
abstract="This paper presents an overview of the Text2DT shared task{\$}{\$}^{\{}1{\}}{\$}{\$}1held in the CHIP-2022 shared tasks. The shared task addresses the challenging topic of automatically extracting the medical decision trees from the un-structured medical texts such as medical guidelines and textbooks. Many teams from both industry and academia participated in the shared tasks, and the top teams achieved amazing test results. This paper describes the tasks, the datasets, evaluation metrics, and the top systems for both tasks. Finally, the paper summarizes the techniques and results of the evaluation of the various approaches explored by the participating teams.{\$}{\$}^{\{}1{\}}{\$}{\$}1(http://cips-chip.org.cn/2022/eval3)",
isbn="978-981-99-4826-0"
}




@inproceedings{Joshi2019BERTFC,
  title={BERT for Coreference Resolution: Baselines and Analysis},
  author={Mandar Joshi and Omer Levy and Daniel S. Weld and Luke Zettlemoyer},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:201646551}
}



@article{Wang2020MiningIH,
  title={Mining Infrequent High-Quality Phrases from Domain-Specific Corpora},
  author={Li Wang and Wei Zhu and Sihang Jiang and Sheng Zhang and Keqiang Wang and Yuan Ni and Guo Tong Xie and Yanghua Xiao},
  journal={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:224281022}
}



@article{Wang2023MultitaskEL,
  title={Multi-task entity linking with supervision from a taxonomy},
  author={Xuwu Wang and Lihan Chen and Wei Zhu and Yuan Ni and Guo Tong Xie and Deqing Yang and Yanghua Xiao},
  journal={Knowledge and Information Systems},
  year={2023},
  volume={65},
  pages={4335 - 4358},
  url={https://api.semanticscholar.org/CorpusID:258975891}
}



@article{Messaoudi2022OpinionMI,
  title={Opinion mining in online social media: a survey},
  author={Chaimaa Messaoudi and Zahia Guessoum and Lotfi Ben Romdhane},
  journal={Social Network Analysis and Mining},
  year={2022},
  volume={12},
  pages={1-18},
  url={https://api.semanticscholar.org/CorpusID:245857256}
}


@inproceedings{Lample2016NeuralAF,
  title={Neural Architectures for Named Entity Recognition},
  author={Guillaume Lample and Miguel Ballesteros and Sandeep Subramanian and Kazuya Kawakami and Chris Dyer},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:6042994}
}


@article{Ma2016EndtoendSL,
  title={End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF},
  author={Xuezhe Ma and Eduard H. Hovy},
  journal={ArXiv},
  year={2016},
  volume={abs/1603.01354},
  url={https://api.semanticscholar.org/CorpusID:10489017}
}


@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}


@article{Li2019AUM,
  title={A Unified MRC Framework for Named Entity Recognition},
  author={Xiaoya Li and Jingrong Feng and Yuxian Meng and Qinghong Han and Fei Wu and Jiwei Li},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.11476},
  url={https://api.semanticscholar.org/CorpusID:204902024}
}


@inproceedings{Yu2020NamedER,
  title={Named Entity Recognition as Dependency Parsing},
  author={Juntao Yu and Bernd Bohnet and Massimo Poesio},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:218630027}
}


@article{Yan2021AUG,
  title={A Unified Generative Framework for Various NER Subtasks},
  author={Hang Yan and Tao Gui and Junqi Dai and Qipeng Guo and Zheng Zhang and Xipeng Qiu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.01223},
  url={https://api.semanticscholar.org/CorpusID:235294002}
}


@inproceedings{Lu2022UnifiedSG,
  title={Unified Structure Generation for Universal Information Extraction},
  author={Yaojie Lu and Qing Liu and Dai Dai and Xinyan Xiao and Hongyu Lin and Xianpei Han and Le Sun and Hua Wu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247619149}
}



@inproceedings{Zhang2023LearnedAA,
  title={Learned Adapters Are Better Than Manually Designed Adapters},
  author={Yuming Zhang and Peng Wang and Ming Tan and Wei-Guo Zhu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259858833}
}


@article{Qiu2020PretrainedMF,
  title={Pre-trained models for natural language processing: A survey},
  author={Xipeng Qiu and Tianxiang Sun and Yige Xu and Yunfan Shao and Ning Dai and Xuanjing Huang},
  journal={Science China Technological Sciences},
  year={2020},
  volume={63},
  pages={1872 - 1897},
  url={https://api.semanticscholar.org/CorpusID:212747830}
}


@article{Gu2020DomainSpecificLM,
  title={Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
  author={Yu Gu and Robert Tinn and Hao Cheng and Michael R. Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  year={2020},
  volume={3},
  pages={1 - 23},
  url={https://api.semanticscholar.org/CorpusID:220919723}
}


@article{Sun2020MedicalKG,
  title={Medical Knowledge Graph to Enhance Fraud, Waste, and Abuse Detection on Claim Data: Model Development and Performance Evaluation},
  author={Haixia Sun and Jin Xiao and Wei Zhu and Yilong He and Sheng Zhang and Xiaowei Xu and Li Hou and Jiao Li and Yuan Ni and Guo Tong Xie},
  journal={JMIR Medical Informatics},
  year={2020},
  volume={8},
  url={https://api.semanticscholar.org/CorpusID:220747092}
}


@inproceedings{Guo2021GlobalAD,
  title={Global Attention Decoder for Chinese Spelling Error Correction},
  author={Zhao Guo and Yuan Ni and Keqiang Wang and Wei Zhu and Guo Tong Xie},
  booktitle={Findings},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:236477700}
}


@article{Syed2021DeIDNERCA,
  title={DeIDNER Corpus: Annotation of Clinical Discharge Summary Notes for Named Entity Recognition Using BRAT Tool},
  author={Mahanazuddin Syed and Shaymaa Al-Shukri and Shorabuddin Syed and Kevin Wayne Sexton and Melody L. Greer and Meredith Nahm Zozus and Sudeepa Bhattacharyya and Fred W. Prior},
  journal={Studies in health technology and informatics},
  year={2021},
  volume={281},
  pages={432 - 436},
  url={https://api.semanticscholar.org/CorpusID:235216987}
}


@article{Zhu2022InvestigatingAN,
  title={Investigating annotation noise for named entity recognition},
  author={Y. Zhu and Yingchun Ye and Mengyang Li and Ji Zhang and Ou Wu},
  journal={Neural Computing and Applications},
  year={2022},
  volume={35},
  pages={993-1007},
  url={https://api.semanticscholar.org/CorpusID:252524636}
}


@inproceedings{Shang2018LearningNE,
  title={Learning Named Entity Tagger using Domain-Specific Dictionary},
  author={Jingbo Shang and Liyuan Liu and Xiang Ren and Xiaotao Gu and Teng Ren and Jiawei Han},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:52188604}
}


@article{Zhu2019TheDS,
  title={The Dr-KGQA System for Automatically Answering Medication Related Questions in Chinese},
  author={Wei Zhu and Yuan Ni and Guo Tong Xie and Xiaofeng Zhou and Cai Chen},
  journal={2019 IEEE International Conference on Healthcare Informatics (ICHI)},
  year={2019},
  pages={1-6},
  url={https://api.semanticscholar.org/CorpusID:208207213}
}







@article{Mayhew2019NamedER,
  title={Named Entity Recognition with Partially Annotated Training Data},
  author={Stephen Mayhew and Snigdha Chaturvedi and Chen-Tse Tsai and Dan Roth},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.09270},
  url={https://api.semanticscholar.org/CorpusID:202712653}
}





@article{Liu2021NoisyLabeledNW,
  title={Noisy-Labeled NER with Confidence Estimation},
  author={Kun Liu and Yao Fu and Chuanqi Tan and Mosha Chen and Ningyu Zhang and Songfang Huang and Sheng Gao},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.04318},
  url={https://api.semanticscholar.org/CorpusID:233204709}
}


@article{Zhang2021KnowledgeRichSE,
  title={Knowledge-Rich Self-Supervised Entity Linking},
  author={Sheng Zhang and Hao Cheng and Shikhar Vashishth and Cliff Wong and Jinfeng Xiao and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.07887},
  url={https://api.semanticscholar.org/CorpusID:245144585}
}


@inproceedings{Bowman2015ALA,
  title={A large annotated corpus for learning natural language inference},
  author={Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:14604520}
}



@inproceedings{Zhu2021AutoNLUAS,
  title={AutoNLU: Architecture Search for Sentence and Cross-sentence Attention Modeling with Re-designed Search Space},
  author={Wei Zhu},
  booktitle={Natural Language Processing and Chinese Computing},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:238862030}
}

@inproceedings{Giannakopoulos2017UnsupervisedAT,
  title={Unsupervised Aspect Term Extraction with B-LSTM \& CRF using Automatically Labelled Datasets},
  author={Athanasios Giannakopoulos and Claudiu Cristian Musat and Andreea Hossmann and Michael Baeriswyl},
  booktitle={WASSA@EMNLP},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:9508638}
}


@article{Peng2019DistantlySN,
  title={Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning},
  author={Minlong Peng and Xiaoyu Xing and Qi Zhang and Jinlan Fu and Xuanjing Huang},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.01378},
  url={https://api.semanticscholar.org/CorpusID:174797886}
}


@article{Ren2015ClusTypeEE,
  title={ClusType: Effective Entity Recognition and Typing by Relation Phrase-Based Clustering},
  author={Xiang Ren and Ahmed El-Kishky and Chi Wang and Fangbo Tao and Clare R. Voss and Jiawei Han},
  journal={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:2086183}
}


@article{Fries2017SwellSharkAG,
  title={SwellShark: A Generative Model for Biomedical Named Entity Recognition without Labeled Data},
  author={Jason Alan Fries and Sen Wu and Alexander J. Ratner and Christopher R{\'e}},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.06360},
  url={https://api.semanticscholar.org/CorpusID:1929171}
}


@article{Zhang2021DebiasingDS,
  title={De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention},
  author={Wenkai Zhang and Hongyu Lin and Xianpei Han and Le Sun},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.09233},
  url={https://api.semanticscholar.org/CorpusID:235458156}
}


@article{Liang2020BONDBO,
  title={BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision},
  author={Chen Liang and Yue Yu and Haoming Jiang and Siawpeng Er and Ruijia Wang and Tuo Zhao and Chao Zhang},
  journal={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:220250247}
}



@article{Xu2023BetterSO,
  title={Better Sampling of Negatives for Distantly Supervised Named Entity Recognition},
  author={Lu Xu and Lidong Bing and Wei Lu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.13142},
  url={https://api.semanticscholar.org/CorpusID:258833335}
}


@inproceedings{Zhu2021MVPBERTMP,
  title={MVP-BERT: Multi-Vocab Pre-training for Chinese BERT},
  author={Wei Zhu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:237331564}
}



@inproceedings{Zhang2023LECOIE,
  title={LECO: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism},
  author={Jingfang Zhang and Ming Tan and Pengyu Dai and Wei-Guo Zhu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259370796}
}


@inproceedings{Zhu2023BADGESU,
  title={BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting},
  author={Wei Zhu and Peifeng Wang and Yuan Ni and Guo Tong Xie and Xiaoling Wang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259370582}
}

@article{Gao2023FPABEEFE,
  title={F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks},
  author={Xiangxiang Gao and Wei Zhu and Jiasheng Gao and Congrui Yin},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.11916},
  url={https://api.semanticscholar.org/CorpusID:258546016}
}


@article{Li2016BioCreativeVC,
  title={BioCreative V CDR task corpus: a resource for chemical disease relation extraction},
  author={Jiao Li and Yueping Sun and Robin J. Johnson and Daniela Sciaky and Chih-Hsuan Wei and Robert Leaman and Allan Peter Davis and Carolyn J. Mattingly and Thomas C. Wiegers and Zhiyong Lu},
  journal={Database: The Journal of Biological Databases and Curation},
  year={2016},
  volume={2016},
  url={https://api.semanticscholar.org/CorpusID:88817}
}


@article{Dogan2014NCBIDC,
  title={NCBI disease corpus: A resource for disease name recognition and concept normalization},
  author={Rezarta Islamaj Dogan and Robert Leaman and Zhiyong Lu},
  journal={Journal of biomedical informatics},
  year={2014},
  volume={47},
  pages={
          1-10
        },
  url={https://api.semanticscholar.org/CorpusID:234064}
}


@article{Zhang2021CBLUEAC,
  title={CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark},
  author={Ningyu Zhang and Zhen Bi and Xiaozhuan Liang and Lei Li and Xiang Chen and Shumin Deng and Luoqiu Li and Xin Xie and Hongbin Ye and Xin Shang and Kangping Yin and Chuanqi Tan and Jian Xu and Mosha Chen and Fei Huang and Luo Si and Yuan Ni and Guo Tong Xie and Zhifang Sui and Baobao Chang and Hui Zong and Zheng Yuan and Linfeng Li and Jun Yan and Hongying Zan and Kunli Zhang and Huajun Chen and Buzhou Tang and Qingcai Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.08087},
  url={https://api.semanticscholar.org/CorpusID:235415270}
}


@article{Chen2022ABF,
  title={A benchmark for automatic medical consultation system: frameworks, tasks and datasets},
  author={W. Chen and Zhiwei Li and Hongyi Fang and Qian-Qian Yao and Cheng Zhong and Jianye Hao and Qi Zhang and Xuanjing Huang and Jianjun Peng and Zhongyu Wei},
  journal={Bioinformatics},
  year={2022},
  volume={39},
  url={https://api.semanticscholar.org/CorpusID:248239674}
}



@misc{pubmedbert,
  author = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
  title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
  year = {2020},
  eprint = {arXiv:2007.15779},
}



@inproceedings{Loshchilov2017DecoupledWD,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:53592270}
}


@inproceedings{Zhou2022DistantlySN,
  title={Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning},
  author={Kang Zhou and Yuepei Li and Qi Li},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248266826}
}


@inproceedings{Zhu2021HBertEC,
  title={H-Bert: Enhancing Chinese Pretrained Models with Attention to HowNet},
  author={Wei Zhu},
  booktitle={International Workshop on the Semantic Web},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:239040597}
}



@article{Zheng2023CandidateSF,
  title={Candidate Soups: Fusing Candidate Results Improves Translation Quality for Non-Autoregressive Translation},
  author={Huanran Zheng and Wei Zhu and Pengfei Wang and Xiaoling Wang},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.11503},
  url={https://api.semanticscholar.org/CorpusID:256358677}
}


@article{Zeng2021AutomatedCO,
  title={Automated classification of clinical trial eligibility criteria text based on ensemble learning and metric learning},
  author={Kun Zeng and Yibin Xu and Ge Lin and Likeng Liang and Tianyong Hao},
  journal={BMC Medical Informatics and Decision Making},
  year={2021},
  volume={21},
  url={https://api.semanticscholar.org/CorpusID:236504618}
}


@article{Wei2022ChainOT,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed Huai-hsin Chi and F. Xia and Quoc Le and Denny Zhou},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.11903},
  url={https://api.semanticscholar.org/CorpusID:246411621}
}


@article{Wang2022SelfConsistencyIC,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Huai-hsin Chi and Denny Zhou},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.11171},
  url={https://api.semanticscholar.org/CorpusID:247595263}
}


@article{Stiennon2020LearningTS,
  title={Learning to summarize from human feedback},
  author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan J. Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.01325},
  url={https://api.semanticscholar.org/CorpusID:221665105}
}



@inproceedings{Jiao2023IsCA,
  title={Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine},
  author={Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Xing Wang and Zhaopeng Tu},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:257631519}
}




@inproceedings{zhu-tan-2023-spt,
    title = "{SPT}: Learning to Selectively Insert Prompts for Better Prompt Tuning",
    author = "Zhu, Wei  and
      Tan, Ming",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.727",
    pages = "11862--11878",
    abstract = "Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, Selective Prompt Tuning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer tunable parameters.",
}



@ARTICLE{PromptCBLUE,
       author = {{Zhu}, Wei and {Wang}, Xiaoling and {Zheng}, Huanran and {Chen}, Mosha and {Tang}, Buzhou},
        title = "{PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = oct,
          eid = {arXiv:2310.14151},
        pages = {arXiv:2310.14151},
          doi = {10.48550/arXiv.2310.14151},
archivePrefix = {arXiv},
       eprint = {2310.14151},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv231014151Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{2023arXiv230318223Z,
       author = {{Zhao}, Wayne Xin and {Zhou}, Kun and {Li}, Junyi and {Tang}, Tianyi and {Wang}, Xiaolei and {Hou}, Yupeng and {Min}, Yingqian and {Zhang}, Beichen and {Zhang}, Junjie and {Dong}, Zican and {Du}, Yifan and {Yang}, Chen and {Chen}, Yushuo and {Chen}, Zhipeng and {Jiang}, Jinhao and {Ren}, Ruiyang and {Li}, Yifan and {Tang}, Xinyu and {Liu}, Zikang and {Liu}, Peiyu and {Nie}, Jian-Yun and {Wen}, Ji-Rong},
        title = "{A Survey of Large Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2023,
        month = mar,
          eid = {arXiv:2303.18223},
        pages = {arXiv:2303.18223},
          doi = {10.48550/arXiv.2303.18223},
archivePrefix = {arXiv},
       eprint = {2303.18223},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230318223Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@ARTICLE{2023arXiv230514314D,
       author = {{Dettmers}, Tim and {Pagnoni}, Artidoro and {Holtzman}, Ari and {Zettlemoyer}, Luke},
        title = "{QLoRA: Efficient Finetuning of Quantized LLMs}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2023,
        month = may,
          eid = {arXiv:2305.14314},
        pages = {arXiv:2305.14314},
          doi = {10.48550/arXiv.2305.14314},
archivePrefix = {arXiv},
       eprint = {2305.14314},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230514314D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{zhao-etal-2020-masking,
    title = "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models",
    author = {Zhao, Mengjie  and
      Lin, Tao  and
      Mi, Fei  and
      Jaggi, Martin  and
      Sch{\"u}tze, Hinrich},
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.174",
    doi = "10.18653/v1/2020.emnlp-main.174",
    pages = "2226--2241",
    abstract = "We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.",
}



@ARTICLE{2018arXiv180408838L,
       author = {{Li}, Chunyuan and {Farkhoor}, Heerad and {Liu}, Rosanne and {Yosinski}, Jason},
        title = "{Measuring the Intrinsic Dimension of Objective Landscapes}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = 2018,
        month = apr,
          eid = {arXiv:1804.08838},
        pages = {arXiv:1804.08838},
          doi = {10.48550/arXiv.1804.08838},
archivePrefix = {arXiv},
       eprint = {1804.08838},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180408838L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{Qin2021ExploringLI,
  title={Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning},
  author={Yujia Qin and Xiaozhi Wang and Yusheng Su and Yankai Lin and Ning Ding and Zhiyuan Liu and Juan-Zi Li and Lei Hou and Peng Li and Maosong Sun and Jie Zhou},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.07867},
  url={https://api.semanticscholar.org/CorpusID:239009752}
}



@inproceedings{Ding2023SparseLA,
  title={Sparse Low-rank Adaptation of Pre-trained Language Models},
  author={Ning Ding and Xingtai Lv and Qiaosen Wang and Yulin Chen and Bowen Zhou and Zhiyuan Liu and Maosong Sun},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265294736}
}



@article{Hu2023StructureAwareLA,
  title={Structure-Aware Low-Rank Adaptation for Parameter-Efficient Fine-Tuning},
  author={Yahao Hu and Yifei Xie and Tianfeng Wang and Man Chen and Zhisong Pan},
  journal={Mathematics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:264336659}
}



@article{Touvron2023Llama2O,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Daniel M. Bikel and Lukas Blecher and Cristian Cant{\'o}n Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony S. Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel M. Kloumann and A. V. Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and R. Subramanian and Xia Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zhengxu Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.09288},
  url={https://api.semanticscholar.org/CorpusID:259950998}
}



@ARTICLE{2023arXiv230108727W,
       author = {{White}, Colin and {Safari}, Mahmoud and {Sukthanker}, Rhea and {Ru}, Binxin and {Elsken}, Thomas and {Zela}, Arber and {Dey}, Debadeepta and {Hutter}, Frank},
        title = "{Neural Architecture Search: Insights from 1000 Papers}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
         year = 2023,
        month = jan,
          eid = {arXiv:2301.08727},
        pages = {arXiv:2301.08727},
          doi = {10.48550/arXiv.2301.08727},
archivePrefix = {arXiv},
       eprint = {2301.08727},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230108727W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}






@inproceedings{Chen2020StabilizingDA,
  title={Stabilizing Differentiable Architecture Search via Perturbation-based Regularization},
  author={Xiangning Chen and Cho-Jui Hsieh},
  booktitle={International Conference on Machine Learning},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:211096736}
}



@article{Chen2019ProgressiveDB,
  title={Progressive DARTS: Bridging the Optimization Gap for NAS in the Wild},
  author={Xin Chen and Lingxi Xie and Jun Wu and Qi Tian},
  journal={International Journal of Computer Vision},
  year={2019},
  volume={129},
  pages={638 - 655},
  url={https://api.semanticscholar.org/CorpusID:209445037}
}


@ARTICLE{2023arXiv230605685Z,
       author = {{Zheng}, Lianmin and {Chiang}, Wei-Lin and {Sheng}, Ying and {Zhuang}, Siyuan and {Wu}, Zhanghao and {Zhuang}, Yonghao and {Lin}, Zi and {Li}, Zhuohan and {Li}, Dacheng and {Xing}, Eric. P and {Zhang}, Hao and {Gonzalez}, Joseph E. and {Stoica}, Ion},
        title = "{Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2023,
        month = jun,
          eid = {arXiv:2306.05685},
        pages = {arXiv:2306.05685},
          doi = {10.48550/arXiv.2306.05685},
archivePrefix = {arXiv},
       eprint = {2306.05685},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230605685Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{novikova-etal-2017-e2e,
    title = "The {E}2{E} Dataset: New Challenges For End-to-End Generation",
    author = "Novikova, Jekaterina  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Rieser, Verena",
    editor = "Jokinen, Kristiina  and
      Stede, Manfred  and
      DeVault, David  and
      Louis, Annie",
    booktitle = "Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = aug,
    year = "2017",
    address = {Saarbr{\"u}cken, Germany},
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-5525",
    doi = "10.18653/v1/W17-5525",
    pages = "201--206",
    abstract = "This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.",
}




@article{Zhang2023LoRAFAML,
  title={LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning},
  author={Longteng Zhang and Lin Zhang and Shaohuai Shi and Xiaowen Chu and Bo Li},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.03303},
  url={https://api.semanticscholar.org/CorpusID:260683267}
}




@article{Kopiczko2023VeRAVR,
  title={VeRA: Vector-based Random Matrix Adaptation},
  author={Dawid Jan Kopiczko and Tijmen Blankevoort and Yuki Markus Asano},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.11454},
  url={https://api.semanticscholar.org/CorpusID:264172315}
}



@article{Wei2021FinetunedLM,
  title={Finetuned Language Models Are Zero-Shot Learners},
  author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.01652},
  url={https://api.semanticscholar.org/CorpusID:237416585}
}




@article{Sanh2021MultitaskPT,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal V. Nayak and Debajyoti Datta and Jonathan D. Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng-Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault F{\'e}vry and Jason Alan Fries and Ryan Teehan and Stella Biderman and Leo Gao and Tali Bers and Thomas Wolf and Alexander M. Rush},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.08207},
  url={https://api.semanticscholar.org/CorpusID:239009562}
}



@Misc{peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}




@inproceedings{Mishra2021CrossTaskGV,
  title={Cross-Task Generalization via Natural Language Crowdsourcing Instructions},
  author={Swaroop Mishra and Daniel Khashabi and Chitta Baral and Hannaneh Hajishirzi},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:237421373}
}


@article{Gan2023ModelasaServiceA,
  title={Model-as-a-Service (MaaS): A Survey},
  author={Wensheng Gan and Shicheng Wan and Philip S. Yu},
  journal={2023 IEEE International Conference on Big Data (BigData)},
  year={2023},
  pages={4636-4645},
  url={https://api.semanticscholar.org/CorpusID:265128707}
}




@Misc{peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}




@inproceedings{Mishra2021CrossTaskGV,
  title={Cross-Task Generalization via Natural Language Crowdsourcing Instructions},
  author={Swaroop Mishra and Daniel Khashabi and Chitta Baral and Hannaneh Hajishirzi},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:237421373}
}



@article{Chen2023PunicaML,
  title={Punica: Multi-Tenant LoRA Serving},
  author={Lequn Chen and Zihao Ye and Yongji Wu and Danyang Zhuo and Luis Ceze and Arvind Krishnamurthy University of Washington and Duke University},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.18547},
  url={https://api.semanticscholar.org/CorpusID:264590197}
}


@article{Xu2023ParameterEfficientFM,
  title={Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment},
  author={Lingling Xu and Haoran Xie and Si-Zhao Joe Qin and Xiaohui Tao and Fu Lee Wang},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.12148},
  url={https://api.semanticscholar.org/CorpusID:266362573}
}



@article{Xin2024ParameterEfficientFF,
  title={Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey},
  author={Yi Xin and Siqi Luo and Haodi Zhou and Junlong Du and Xiaohong Liu and Yue Fan and Qing Li and Yuntao Du},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.02242},
  url={https://api.semanticscholar.org/CorpusID:267412110}
}



@inproceedings{Kim2014ConvolutionalNN,
  title={Convolutional Neural Networks for Sentence Classification},
  author={Yoon Kim},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:9672033}
}


@article{Molina2019PadAU,
  title={Pad{\'e} Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},
  author={Alejandro Molina and Patrick Schramowski and Kristian Kersting},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.06732},
  url={https://api.semanticscholar.org/CorpusID:196831891}
}


@inproceedings{Delfosse2021AdaptiveRA,
  title={Adaptive Rational Activations to Boost Deep Reinforcement Learning},
  author={Quentin Delfosse and Patrick Schramowski and Martin Mundt and Alejandro Molina and Kristian Kersting},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:246430610}
}



@article{delfosse2021recurrent,
  title={Recurrent Rational Networks},
  author={Delfosse, Quentin and Schramowski, Patrick and Molina, Alejandro and Kersting, Kristian},
  journal={arXiv preprint arXiv:2102.09407},
  year={2021}
}



@article{Boulle2020RationalNN,
  title={Rational neural networks},
  author={Nicolas Boull'e and Yuji Nakatsukasa and Alex Townsend},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.01902},
  url={https://api.semanticscholar.org/CorpusID:214802374}
}



@article{Liu2022FewShotPF,
  title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.05638},
  url={https://api.semanticscholar.org/CorpusID:248693283}
}


@article{Cobbe2021TrainingVT,
  title={Training Verifiers to Solve Math Word Problems},
  author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.14168},
  url={https://api.semanticscholar.org/CorpusID:239998651}
}



@article{Zhong2017Seq2SQLGS,
  title={Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},
  author={Victor Zhong and Caiming Xiong and Richard Socher},
  journal={ArXiv},
  year={2017},
  volume={abs/1709.00103},
  url={https://api.semanticscholar.org/CorpusID:25156106}
}



@article{Biderman2023PythiaAS,
  title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  author={Stella Biderman and Hailey Schoelkopf and Quentin G. Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.01373},
  url={https://api.semanticscholar.org/CorpusID:257921893}
}



@article{Liu2022FewShotPF,
  title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.05638},
  url={https://api.semanticscholar.org/CorpusID:248693283}
}



@article{BenZaken2021BitFitSP,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Elad Ben-Zaken and Shauli Ravfogel and Yoav Goldberg},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.10199},
  url={https://api.semanticscholar.org/CorpusID:231672601}
}



@article{Sabour2017DynamicRB,
  title={Dynamic Routing Between Capsules},
  author={Sara Sabour and Nicholas Frosst and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.09829},
  url={https://api.semanticscholar.org/CorpusID:3603485}
}



@article{Pawan2022CapsuleNF,
  title={Capsule networks for image classification: A review},
  author={S. J. Pawan and Jeny Rajan},
  journal={Neurocomputing},
  year={2022},
  volume={509},
  pages={102-120},
  url={https://api.semanticscholar.org/CorpusID:251702652}
}


@article{Kim2018TextCU,
  title={Text Classification using Capsules},
  author={Jaeyoung Kim and Sion Jang and Sungchul Choi and Eunjeong Lucy Park},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.03976},
  url={https://api.semanticscholar.org/CorpusID:51976955}
}



@article{Zhang2018AttentionBasedCN,
  title={Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction},
  author={Ningyu Zhang and Shumin Deng and Zhanlin Sun and Xi Chen and Wei Zhang and Huajun Chen},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.11321},
  url={https://api.semanticscholar.org/CorpusID:53103613}
}



@inproceedings{Xiao2018MCapsNetCN,
  title={MCapsNet: Capsule Network for Text with Multi-Task Learning},
  author={Liqiang Xiao and Honglun Zhang and Wenqing Chen and Yongkun Wang and Yaohui Jin},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:53080947}
}



@inproceedings{Aly2019HierarchicalMC,
  title={Hierarchical Multi-label Classification of Text with Capsule Networks},
  author={Rami Aly and Steffen Remus and Chris Biemann},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:196196857}
}



@inproceedings{Srivastava2018IdentifyingAA,
  title={Identifying Aggression and Toxicity in Comments using Capsule Network},
  author={Saurabh Srivastava and Prerna Khurana and Vartika Tewari},
  booktitle={TRAC@COLING 2018},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:59336566}
}



@article{Hendrycks2016GaussianEL,
  title={Gaussian Error Linear Units (GELUs)},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={arXiv: Learning},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:125617073}
}



@article{roth1988introduction,
  title={Introduction to the Shapley value},
  author={Roth, Alvin E},
  journal={The Shapley value},
  pages={1--27},
  year={1988},
  publisher={University of Cambridge Press, Cambridge}
}



@article{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{zhao2024explainability,
  title={Explainability for large language models: A survey},
  author={Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={2},
  pages={1--38},
  year={2024},
  publisher={ACM New York, NY}
}


@article{saha2022explainability,
  title={Explainability of Text Processing and Retrieval Methods: A Critical Survey},
  author={Saha, Sourav and Majumdar, Debapriyo and Mitra, Mandar},
  journal={arXiv preprint arXiv:2212.07126},
  year={2022}
}



@article{staatz1983cooperative,
  title={The cooperative as a coalition: a game-theoretic approach},
  author={Staatz, John M},
  journal={American Journal of Agricultural Economics},
  volume={65},
  number={5},
  pages={1084--1089},
  year={1983},
  publisher={JSTOR}
}



@inproceedings{zhang2022platon,
  title={Platon: Pruning large transformer models with upper confidence bound of weight importance},
  author={Zhang, Qingru and Zuo, Simiao and Liang, Chen and Bukharin, Alexander and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International conference on machine learning},
  pages={26809--26823},
  year={2022},
  organization={PMLR}
}



@article{parikh2014proximal,
  title={Proximal algorithms},
  author={Parikh, Neal and Boyd, Stephen and others},
  journal={Foundations and trends{\textregistered} in Optimization},
  volume={1},
  number={3},
  pages={127--239},
  year={2014},
  publisher={Now Publishers, Inc.}
}



@article{ding2023enhancing,
  title={Enhancing chat language models by scaling high-quality instructional conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}



@article{suzgun2022challenging,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}


@article{jin2023large,
  title={Large models for time series and spatio-temporal data: A survey and outlook},
  author={Jin, Ming and Wen, Qingsong and Liang, Yuxuan and Zhang, Chaoli and Xue, Siqiao and Wang, Xue and Zhang, James and Wang, Yi and Chen, Haifeng and Li, Xiaoli and others},
  journal={arXiv preprint arXiv:2310.10196},
  year={2023}
}


@article{zhou2023one,
  title={One fits all: Power general time series analysis by pretrained lm},
  author={Zhou, Tian and Niu, Peisong and Sun, Liang and Jin, Rong and others},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={43322--43355},
  year={2023}
}


@article{jin2023time,
  title={Time-llm: Time series forecasting by reprogramming large language models},
  author={Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and others},
  journal={arXiv preprint arXiv:2310.01728},
  year={2023}
}



@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}




@article{das2023decoder,
  title={A decoder-only foundation model for time-series forecasting},
  author={Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen},
  journal={arXiv preprint arXiv:2310.10688},
  year={2023}
}


@article{goswami2024moment,
  title={Moment: A family of open time-series foundation models},
  author={Goswami, Mononito and Szafer, Konrad and Choudhry, Arjun and Cai, Yifu and Li, Shuo and Dubrawski, Artur},
  journal={arXiv preprint arXiv:2402.03885},
  year={2024}
}


@inproceedings{lu2022frozen,
  title={Frozen pretrained transformers as universal computation engines},
  author={Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={36},
  number={7},
  pages={7628--7636},
  year={2022}
}

@inproceedings{shen2023cross,
  title={Cross-modal fine-tuning: Align then refine},
  author={Shen, Junhong and Li, Liam and Dery, Lucio M and Staten, Corey and Khodak, Mikhail and Neubig, Graham and Talwalkar, Ameet},
  booktitle={International Conference on Machine Learning},
  pages={31030--31056},
  year={2023},
  organization={PMLR}
}



@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}


@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}



@article{chen2024llava,
  title={Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms},
  author={Chen, Shaoxiang and Jie, Zequn and Ma, Lin},
  journal={arXiv preprint arXiv:2401.16160},
  year={2024}
}


@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={9},
  pages={11121--11128},
  year={2023}
}



@article{wu2022timesnet,
  title={Timesnet: Temporal 2d-variation modeling for general time series analysis},
  author={Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2210.02186},
  year={2022},
  publisher={arXivpreprint}
}


@article{makridakis2018m4,
  title={The M4 Competition: Results, findings, conclusion and way forward},
  author={Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  journal={International Journal of forecasting},
  volume={34},
  number={4},
  pages={802--808},
  year={2018},
  publisher={Elsevier}
}




@article{banks2024gemma,
  title={Gemma: Introducing new state-of-the-art open models},
  author={Banks, Jeanine and Warkentin, Tris},
  journal={Google. Available online at: https://blog. google/technology/developers/gemma-open-models/(accessed 6 April, 2024)},
  year={2024}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


@article{liu2024alora,
  title={Alora: Allocating low-rank adaptation for fine-tuning large language models},
  author={Liu, Zequan and Lyn, Jiawen and Zhu, Wei and Tian, Xing and Graham, Yvette},
  journal={arXiv preprint arXiv:2403.16187},
  year={2024}
}




@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}




@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}


@article{oreshkin2019n,
  title={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},
  author={Oreshkin, Boris N and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1905.10437},
  year={2019}
}


@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}


@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}


@inproceedings{zhou2022fedformer,
  title={Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={International conference on machine learning},
  pages={27268--27286},
  year={2022},
  organization={PMLR}
}


@article{liu2018darts,
  title={Darts: Differentiable architecture search},
  author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  journal={arXiv preprint arXiv:1806.09055},
  year={2018}
}



@article{gardner1985exponential,
  title={Exponential smoothing: The state of the art},
  author={Gardner Jr, Everette S},
  journal={Journal of forecasting},
  volume={4},
  number={1},
  pages={1--28},
  year={1985},
  publisher={Wiley Online Library}
}


@article{biship2007pattern,
  title={Pattern recognition and machine learning (information science and statistics)},
  author={Biship, Christopher M},
  journal={Springer New York},
  year={2007}
}




@article{prokhorenkova2018catboost,
  title={CatBoost: unbiased boosting with categorical features},
  author={Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}


@article{lim2021time,
  title={Time-series forecasting with deep learning: a survey},
  author={Lim, Bryan and Zohren, Stefan},
  journal={Philosophical Transactions of the Royal Society A},
  volume={379},
  number={2194},
  pages={20200209},
  year={2021},
  publisher={The Royal Society Publishing}
}


@article{Hochreiter1997LongSM,
  title={Long Short-Term Memory},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780},
  url={https://api.semanticscholar.org/CorpusID:1915014}
}



@article{li2021survey,
  title={A survey of convolutional neural networks: analysis, applications, and prospects},
  author={Li, Zewen and Liu, Fan and Yang, Wenjie and Peng, Shouheng and Zhou, Jun},
  journal={IEEE transactions on neural networks and learning systems},
  volume={33},
  number={12},
  pages={6999--7019},
  year={2021},
  publisher={IEEE}
}




@article{tang2025ts,
  title={TS-Mixer: A lightweight text representation model based on context awareness},
  author={Tang, Huanling and Wang, Yulin and Zhang, Yu and Dou, Quansheng and Lu, Mingyu},
  journal={Expert Systems},
  pages={e13732},
  year={2025},
  publisher={Wiley Online Library}
}




@article{rocktaschel2015reasoning,
  title={Reasoning about entailment with neural attention},
  author={Rockt{\"a}schel, Tim and Grefenstette, Edward and Hermann, Karl Moritz and Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Blunsom, Phil},
  journal={arXiv preprint arXiv:1509.06664},
  year={2015}
}

@article{wang2015learning,
  title={Learning natural language inference with LSTM},
  author={Wang, Shuohang and Jiang, Jing},
  journal={arXiv preprint arXiv:1512.08849},
  year={2015}
}



@article{velivckovic2017graph,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.10903},
  year={2017}
}


@inproceedings{tan2018multiway,
  title={Multiway attention networks for modeling sentence pairs.},
  author={Tan, Chuanqi and Wei, Furu and Wang, Wenhui and Lv, Weifeng and Zhou, Ming},
  booktitle={IJCAI},
  pages={4411--4417},
  year={2018}
}



@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}




@article{xu2015empirical,
  title={Empirical evaluation of rectified activations in convolutional network},
  author={Xu, Bing},
  journal={arXiv preprint arXiv:1505.00853},
  year={2015}
}



@article{ramachandran2017searching,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}


@article{clevert2015fast,
  title={Fast and accurate deep network learning by exponential linear units (elus)},
  author={Clevert, Djork-Arn{\'e}},
  journal={arXiv preprint arXiv:1511.07289},
  year={2015}
}


@inproceedings{so2019evolved,
  title={The evolved transformer},
  author={So, David and Le, Quoc and Liang, Chen},
  booktitle={International conference on machine learning},
  pages={5877--5886},
  year={2019},
  organization={PMLR}
}


@inproceedings{challu2023nhits,
  title={Nhits: Neural hierarchical interpolation for time series forecasting},
  author={Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Ramirez, Federico Garza and Canseco, Max Mergenthaler and Dubrawski, Artur},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={6},
  pages={6989--6997},
  year={2023}
}





@inproceedings{zhu2021autotrans,
  title={Autotrans: Automating transformer design via reinforced architecture search},
  author={Zhu, Wei and Wang, Xiaoling and Ni, Yuan and Xie, Guotong},
  booktitle={Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qingdao, China, October 13--17, 2021, Proceedings, Part I 10},
  pages={169--182},
  year={2021},
  organization={Springer}
}


@article{bi2019stabilizing,
  title={Stabilizing darts with amended gradient estimation on architectural parameters},
  author={Bi, Kaifeng and Hu, Changping and Xie, Lingxi and Chen, Xin and Wei, Longhui and Tian, Qi},
  journal={arXiv preprint arXiv:1910.11831},
  year={2019}
}




@article{bi2020gold,
  title={Gold-nas: Gradual, one-level, differentiable},
  author={Bi, Kaifeng and Xie, Lingxi and Chen, Xin and Wei, Longhui and Tian, Qi},
  journal={arXiv preprint arXiv:2007.03331},
  year={2020}
}


@inproceedings{pham2018efficient,
  title={Efficient neural architecture search via parameters sharing},
  author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle={International conference on machine learning},
  pages={4095--4104},
  year={2018},
  organization={PMLR}
}



@article{jiang2020convbert,
  title={Convbert: Improving bert with span-based dynamic convolution},
  author={Jiang, Zi-Hang and Yu, Weihao and Zhou, Daquan and Chen, Yunpeng and Feng, Jiashi and Yan, Shuicheng},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12837--12848},
  year={2020}
}










