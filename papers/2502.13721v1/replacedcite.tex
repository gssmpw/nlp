\section{Related works}
\label{sec:related_works}

\subsection{Time series modeling }

As a classical research problem with widespread applications, models constructed from the statistical approaches for time series modeling have been used from the 1970s. The representative models are autoregressive integrated moving average (ARIMA) ____, exponential smoothing ____, and structural models ____. The most significant characteristic for these methods is that they require significant domain expertise to build. With the development of machine learning (ML) ____, many ML techniques are introduced to time series modeling to reduce manual efforts. Gradient boosting regression tree (GBRT) ____ gains popularity by learning the temporal dynamics of time series in a data-driven manner. However, these methods still require manual feature engineering and model designs. With the powerful representation learning capability of deep learning (DL) from large-scale data, various deep learning-based time series models are proposed in the literature ____, achieving better forecasting accuracy than traditional techniques in many cases. Before the era of Transformer ____, the two popular DL architectures are: (a) Recurrent neural networks (RNNs) based methods ____, which summarize the past information compactly in internal memory states and recursively update themselves for forecasting. (b) Convolutional neural networks (CNNs) based methods ____, wherein convolutional filters are used to capture local temporal features. More recently, multi-layer perceptron (MLP) based methods, like ____ and ____, have raised attention in the research field, since these models are simple and light-weight.  




\subsection{Transformer architectures in time series}

The progressive advancements in natural language processing and computer vision have led to the development of sophisticated Transformer ____ variants tailored for a wide array of time series forecasting applications ____. Central to these innovations is the methodology by which Transformers handle time series data. For instance, iTransformer ____ treats each univariate time series as a distinct token, forming multivariate time series into sequences of such tokens. More recently, PatchTST ____ adopts an assumption of channel independence, transforming a univariate time series into multiple patches, which are subsequently treated as tokens and processed through a Transformer encoder. Another important research direction is to design alternative Transformer architectures. This branch of works mainly devote themselves into manually designing novel attention mechanisms, including  Reformer ____, Informer ____, AutoFormer ____, FEDformer____. 


\subsection{Neural architecture search methods}

In the early attempts, NAS requires massive computations, like thousands of GPU days____. Recently, a particular group of one-shot NAS, led by the seminal work DARTS____ has attracted much attention. DARTS formulates the search space into a super-network that can adjust itself in a continuous space so that the network and architectural parameters can be optimized alternately (bi-level optimization) using gradient descent. A series of literature try to improve the performance and efficiency of DARTS, such as ____. SNAS____ reformulate DARTS as a credit assignment task while maintaining the differentiability. ____ penalize the entropy of the architecture parameters to encourage discretization on the hyper-network. P-DARTS____ analyze the issues during the DARTS bi-level optimization, and propose a series of modifications. PC-DARTS____ reduces the memory cost during search by sampling a portion of the channels in super-networks. FairDARTS____ change the softmax operations in DARTS into sigmoid and introduce a zero-one loss to prune the architectural parameters. XNAS____ dynamically wipes out inferior architectures and enhances superior ones.

Our work complements the literature by the following two aspects: (a) we conduct a pilot experiment to analyze the shortcomings of the current DNAS methods; (b) we propose a novel DNAS method that can achieve better search performances and search stability.