\section{Related Work}
RAG **Devlin, "BART: Denoising Sequence-to-Sequence Pre-training for Language Generations"** techniques explore the integration of external knowledge base with the LLMs to address the limitations in LLMs, such as hallucination and nontransparent reasoning process. By leveraging external knowledge databases, RAG can enhance the accuracy and transparency of LLMs. Our research focuses on the potential of RAG to significantly improve the performance, reliability, transparency, and interactivity of LLMs, where RAG could be applied in a wide range of AI tasks, e.g., paper reading, paper writing, text extraction, etc.

\textbf{RAG.} In the Open-Domain Question Answering (ODQA) domain, Sun et al. **"How to Ask Good Questions? A Study on Generating Informative Queries from Textual Evidence"** explored the LLMs with multiple roles to enhance ODQA. However, it faces the challenges of scalability and efficiency, which would hinder its capability of leveraging the RAG framework to develop a user-friendly tool aimed at aiding researchers. Additionally, it has a significant computational burden related to LLMs. To boost the LLMs' performance in scientific research, L\'ala et al. **"Retrieval-Augmented Language Model Pre-Training"** suggested a RAG-enhanced research agent, which relies heavily on scientific literature and struggles with computational inefficiencies, due to its reliance on modular components for iterative adjustments. Moreover, it could not simplify the user experience and limits its interactivity. Zhang et al. **"Improving Language Understanding by Generative Models: On Retrieval Augmented and Adversarial Training"** proposed the Retrieval Augmented Fine-Tuning (RAFT) framework, designed to improve the performance of pre-trained LLMs in domain-specific RAG settings, by teaching LLMs to effectively explore external documents in the inference process.

\textbf{Paper reading.} With the development of RAG and demands of paper-reading assistant tools, there are several tools or research work aiming at improving reading efficiency, such as ScholarPhi **"Scholar Phi: An AI-Powered Research Assistant"** and Papr Readr Bot **"Papr Readr Bot: A Novel Research Tool for Reading and Interpreting Academic Papers"**. Jiang et al. **"A Deep Learning Approach to Paper Interpretation: Towards an Interactive Reading Experience"** proposed a new paper interpretation system for reading and interpreting academic papers. However, it is limited by its flexibility and adaptability to the latest research findings, which could not effectively resolve the issues associated with outdated knowledge and hallucinations. Therefore, there is a lack of a more comprehensive and updated review of literature in paper reading task.
Additionally, Semantic Reader Project **"Semantic Reader Project: Enhancing Literature Reading Experience through Interactive Interfaces"** aims to enhance the literature reading experience with the interactive interfaces that provide relevant documents to readers, contributing to improving user experience and facilitating a deeper understanding of academic papers. CiteRead **"CiteRead: A Novel Approach to Academic Paper Reading Experience through Localized Citation Contexts"** enhanced the academic paper reading experience by integrating localized citation contexts, which help readers understand how citations are used in later research but also enriches the reader's understanding of the paper's impact and relevance. However, these tools is not flexible and do not provide a personalized reading experience or precise ranking of related references for researchers. Without the use of RAG or additional techniques, these tools could easily trapped in hallucinations and outdated knowledge, leading to imprecise retrieval of papers.