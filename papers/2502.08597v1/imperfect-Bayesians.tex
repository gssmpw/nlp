In the preceding sections, we saw that a perfect Bayesian is optimal in the sense that it survives almost surely in the market and drives out any player with regret increasing over time. In this section, we explore a scenario where a Bayesian learner suffers small errors, either in its set of prior distributions or in its update rule. We find that, while perfect Bayesians are optimal, Bayesian learning is also very fragile; for example, it is key that the correct probabilities is one of the model they  consider. Specifically, we show that an imperfect Bayesian would eventually vanish from the market, either against the state distribution $q$ or in competition with any player with sub-linear regret. This holds even if the Bayesian's errors are very small and have zero mean.
 
\subsection{Bayesian Learning with Inaccurate Priors}\label{sec:noisy-priors-Bayesian}

\input{inaccurate-priors-and-survival-time}

\subsection{Bayesian Learning with Noisy Updates}\label{sec:noisy-Bayesian}
Next, we consider a different type of imperfect Bayesian learner that does have $q$ in its prior, but in every step performs slightly inaccurate ``trembling hand'' updates. Also here, we demonstrate that the optimality of Bayesian learning is very fragile, even when the correct distribution lies in the support. To model this, we define a noisy Bayesian learner as one who at each step, either slightly over-weights the current observation or slightly over-weights its current prior, such that, in expectation, both the data and the prior receive the correct weights in every step (i.e., the errors in weight have zero mean).

For concreteness, consider the following scenario of a learner attempting to learn a distribution of states. Suppose there are two states, $s_t = 0$ with a fixed probability $q \in (0,1)$, and $s_t = 1$ otherwise. The learner considers two models: $\theta_a = q$, which is the correct model, and $\theta_b \neq q$, with $\theta_b < 1$. The log-likelihood is given by
\begin{align}
    L(s_t) = 
    (1 - s_t) \log\Big(\frac{\theta_a}{\theta_b}\Big) + 
    s_t \log\Big(\frac{1 - \theta_a}{1 - \theta_b}\Big).
\end{align}

Now suppose that in every step $t$ the Bayesian learner performs $\lmb$-noisy updates where it over- or under-weights the data compared to the prior with a small excess weight $\lmbt$, where $\lmbt$ has zero mean. Specifically, for a parameter $\lmb > 0$, $\lmbt= \lmb$ with probability $1/2$ and $\eta_t = -\lmb$ otherwise. 
We find that even a tiny error with zero mean can have a significant impact, essentially breaking the learning process.

\begin{theorem}\label{thm:wrong-update-Bayesians-vanish}
    For any $\lmb > 0$, the Bayesian learner with $\lmb$-noisy updates does not converge. 
\end{theorem}

\begin{proof}
Let $\epsilon > 0$. 
The log-likelihood ratio under the noisy update rule takes the following form:
\begin{align}\label{eq:inacurate-update-log-ratio}
    \log\Big(
    \frac{P_t(\theta_a)}{P_t(\theta_b)}
    \Big) 
    &=  
    (1 + \lmbt) L(s_t) + 
    (1 - \lmbt) \log \Big(\frac{P_{t-1}(\theta_a)}{P_{t-1}(\theta_b)}\Big) \nonumber \\
    &= 
    (1 + \lmbt)\sum_{\tau=0}^{t-1} L(s_{t - \tau}) \prod_{k=0}^\tau (1 - \eta_k)  
     + 
    \log\Big(
    \frac{P_0(\theta_a)}{P_0(\theta_b)}
    \Big)
    \prod_{\tau=0}^t
    (1 - \eta_{\tau}),
\end{align}
where the empty product equals one, and we define $\eta_0 = 0$.
The products can be simplified:
\begin{align*}
    \prod_{\tau=0}^t
    (1 - \eta_\tau)  = 
    (1 - \lmb)^{n_+(t)}
    (1 + \lmb)^{n_-(t)},
\end{align*}
where $n_+(t)$ is a binomial random variable counting the number of times $\eta_{\tau \leq t} = \lmb$, and $n_-(t) = t - n_+(t)$. 
The last term can be written as  
\begin{align*}
    &
    \log\Big(
    \frac{P_0(\theta_a)}{P_0(\theta_b)}
    \Big)
    \cdot 
    (1 - \lmb)^{n_+(t)}
    (1 + \lmb)^{n_-(t)} 
    = 
    \log\Big(
    \frac{P_0(\theta_a)}{P_0(\theta_b)}
    \Big)
    \cdot 
    \Big[
    \Big(
    \frac{1 - \lmb}
    {1 + \lmb}\Big)^{n_+(t)}
    \Big] \cdot
    (1 + \lmb)^t.
\end{align*}

Intuitively, since $\E[n_+] = T/2$, this should be close to $(1 - \lmb^2)^{\nicefrac{t}{2}}$ with high probability as $t \rightarrow \infty$, which converges to zero for $\eta < 1$. Formally, we have the following claim.
    \begin{claim}
        Let $\epsilon > 0$.
        $
        \lim_{t \rightarrow \infty} \Pr \Big[ \left(\frac{1 - \lmb}{1 + \lmb}\right)^{n_+(t)} \cdot (1 + \lmb)^t > \epsilon \Big] = 0.
        $
    \end{claim}
    
    \begin{proof}
        Let $\delta > 0$. We need to show that there exists $T$ such that for all $t > T$, the event
        \[
        \Big(\frac{1 - \lmb}{1 + \lmb}\Big)^{n_+(t)} \cdot (1 + \lmb)^t > \epsilon
        \]
        has probability less than $\delta$. 
        % We will call this the high-probability event. 
        Dividing by $(1 + \lmb)^t$ and taking the logarithm, we get 
        \[
        n_+(t) \cdot \big(\log(1 - \lmb) - \log(1 + \lmb)\big) > \log(\epsilon) - t \log(1 + \lmb).
        \]
        Rearranging, we have
        \[
        n_+(t) < \frac{t \log(1 + \lmb) - \log(\epsilon)}{\log(1 + \lmb) - \log(1 - \lmb)}.
        \]
        Denote $c_{\epsilon} = \frac{\log(\epsilon)}{\log(1 + \lmb) - \log(1 - \lmb)}$,
        so
        \[
        n_+(t) < \frac{\log(1 + \lmb)}{\log(1 + \lmb) - \log(1 - \lmb)} \cdot t - c_{\epsilon}.
        \]
        Next, we observe that the coefficient of $t$ is strictly less than $1/2$. To see this, define
        \[
        c = \frac{1}{2} - \frac{\log(1 + \lmb)}{\log(1 + \lmb) - \log(1 - \lmb)} = -\frac{\log(1 + \lmb) + \log(1 - \lmb)}{2 \big(\log(1 + \lmb) - \log(1 - \lmb)\big)} = \frac{-\log(1 - \lmb^2)}{\text{Positive Number}} > 0.
        \]
        Thus, we obtain that the event we want to bound is
        $
        n_+(t) < \Big(\frac{1}{2} - c\Big)t - c_{\epsilon}.
        $
        Using Hoeffding's inequality, using that $\E[n_+] = \nicefrac{t}{2}$, we bound the probability of this event:
        \[
        \Pr\Big[\big|n_+(t) - \nicefrac{t}{2}\big| > ct + c_{\epsilon}\Big] < 2e^{-\frac{2(ct + c_{\epsilon})^2}{4t \lmb^2}} < 2e^{-\frac{1}{2}ct}.
        \]
        Setting $T = \lceil \frac{2}{c} \ln\left(\frac{2}{\delta}\right)
        \rceil$, we conclude that for all $t > T$, the probability of the event
        \[
        (1 + \lmb)^t \cdot \Big(\frac{1 - \lmb}{1 + \lmb}\Big)^{n_+(t)} > \epsilon
        \]
        is less than $\delta$, as required. Hence, the product approaches zero for large $t$:
        $
        \lim_{t \rightarrow \infty} \prod_{\tau=1}^t (1 - \eta_\tau) = 0
        $ almost surely.
    \end{proof}
    \noindent
    {\em Remark:} It is worth noting here that the almost-sure behavior of the product is very different from its expectation, which equals one: 
    $$\sum_{k=0}^T \binom{T}{k} p^k(1 - \lmb)^k \cdot (1 - p)^{T - k} (1 + \lmb)^{T - k} = \big(p (1 - \lmb) + (1 - p) (1 + \lmb)\big)^T = 1 \ \text{ (for $p = 1/2$)}.$$

    The above claim implies that the second term in Equation (\ref{eq:inacurate-update-log-ratio}) converges to zero almost surely. In other words, the learner forgets the prior, much like a perfect Bayesian learner (albeit at a somewhat faster rate in this case). For the first term of Equation (\ref{eq:inacurate-update-log-ratio})
    $$(1 + \lmbt)\sum_{\tau=0}^{t-1} L(s_{t - \tau}) \prod_{k=0}^\tau (1 - \eta_k),$$
    a similar argument applies: 
    with high probability the product for large values of $\tau$ contributes negligibly to the sum, but there remains a random contribution from the smaller $\tau$ terms.  
    To see why there is no convergence, observe that the coefficient $(1 + \lmbt)$ oscillates randomly around $1$. More formally, let 
    $Y_\tau = \prod_{k=0}^\tau (1 - \eta_k)$, noting that $Y_0 = 1$, and let $Z_t  = \sum_{\tau=0}^{t-1}L(s_{t-\tau})Y_\tau$. Expanding the sum, we have: $Z_t = L(s_t)Y_0 + \dots + L(s_1)Y_{t-1}$ and for the next time step $Z_{t+1} = L(s_t)Y_0 + \dots + L(s_1)Y_{t}$. Since $Y_t = (1-\lmbt)Y_{t-1}$, this simplifies to $Z_{t+1} = (1 - \lmbt)Z_t + L(s_{t+1})$. 
    Clearly, this sequence does not converge, as both $L(s_{t+1})$ and $\lmbt$ take independent random values at each step.
\end{proof}
