As mentioned in Section \ref{sec:Bayesians}, a Bayesian learner with $q$ not within the support of its prior converges to using the best strategy within its prior (i.e., the one closest to $q$ in relative entropy), denoted here by $q'$. 
This convergence property leads to the following result. 
\begin{theorem}\label{thm:wrong-Bayesians-vanish}
    A Bayesian agent with the state distribution $q$ not in its support incurs regret 
    linear in $T$, and vanishes from the market in competition against any no-regret learner.   
\end{theorem}
Note that this contrasts with the case of Theorem \ref{thm:regret-of-a-Bayesian}, where a perfect Bayesian dominates the market in competition with any learners with regret increasing in time.
\begin{proof}
Let $n$ be the index of a Bayesian agent with $q$ not within its support, and let $q' \neq q$ be the strategy within the support that is closest to $q$ in relative entropy. By Proposition \ref{thm:bayesian-convergence}, the strategy used by the Bayesian agent, $\alpha^n_t$, converges almost surely to $q'$. Hence, intuitively, for sufficiently large $t$, the strategies remain bounded away from $q$, leading to linear regret from that point onward. In more detail, define $I_0 = \frac{1}{2} I_q(q') > 0$. In every sample path $i$ (i.e., an infinite sequence, indexed by $i$, of state realizations and the ensuing sequence of $\alpha^n_t$), except on a set of measure zero, there exists a time $T_i$ such that for all $t \geq T_i$, we have $I_q(\alpha^n_t) > I_0$. The regret accumulated up to time $T_i$ is constant (possibly positive or negative). The expected regret difference relative to strategy $q$ at $t > T_i$ is $\E[U^t(q) - U^t(\alpha^n_t)] \geq I_0$, and so the expected regret relative to $q$ until time $T$ is at least $I_0 \cdot (T - T_i) + const$. By Theorem \ref{thm:regret-of-q}, the strategy $q$ incurs only constant regret, so the regret of the Bayesian agent converging to $q'$ grows linearly in $T$. Now, consider a no-regret learner. Such a learner has sublinear worst-case regret and thus sublinear expected regret. By Theorem \ref{thm:survival-by-finite-regret-gap}, the imperfect Bayesian agent who has linear regret vanishes.    
\end{proof}
\noindent
{\em Remark:}
The possible strategies that the Bayesian can play span the entire convex hull of the prior. 
Bayesians with a finite prior can (and do) play convex combinations of the prior during the dynamic, but still, they always converge to the vertices---even when a more profitable strategy lies in the interior. 
In this sense, this convergence property of Bayesians is both a strength (if they have an accurate prior) and a weakness (if they do not). 
\vspace{5pt}

\bfpar{Typical Survival Time}
A question that arises now is what happens when errors are small.  
In the following analysis, we consider ``$\epsilon$-inaccurate Bayesians'' who have in their prior a close approximation of $q$, but still with some small error. Specifically, the total variation distance between the best strategy in the prior and the state distribution $q$ satisfies $TV(q', q) = \epsilon$ for some $\epsilon > 0$. 

On the one hand, we know that such an agent asymptotically fails to survive against a regret-minimizing agent for any error $\epsilon$. On the other hand, when $\epsilon$ is small, the inaccurate Bayesian will initially converge very quickly to a distribution close to the correct state distribution, and may capture a significant share of the market value during the early stages. 

Our goal now is to analyze the survival time during which the inaccurate Bayesian still maintains a significant market share, and to understand how this survival time is related to the level of error $\epsilon$. This, of course, also depends on the best competitor and how fast they converge.

To estimate this relation and provide an upper bound on the survival time of an inaccurate Bayesian learner, we consider a player using $q'$ with $TV(q', q) = \epsilon$ (having already converged to this distribution), competing against a second player playing a dynamic strategy with expected regret level increasing as $f(t)$ that is sub-linear with $t$.

First, we would like to express the first player's error in terms of entropy (i.e., KL divergence with $q$). We can bound the entropy using Pinsker's inequality \cite{pinsker1964information} for a lower bound and its inverse for an upper bound, were the latter holds for finite distributions with full support, as in our case. We have (where recall that $\Delta = \min_s q_s$)
\[
2\epsilon^2 \leq 
% D_{KL}(q || q') 
I_q(q')
\leq \frac{2}{\Delta} \epsilon^2.
\]
Using Lemma \ref{thm:expected-regret-lemma}, this can be translated into the regret of the inaccurate agent playing strategy $q'$ where $R$ is the regret of an investor using the true state distribution $q$ (see Theorem \ref{thm:regret-of-q}): 
\[
% D_{KL}(q \| q') 
I_q(q')
= \frac{1}{T} \big(\mathbb{E}[R^{T}(q'_{1:T})] - R \big).
\]

To maximize the survival time, we pick the distribution $q'$ to be the one that has the smallest regret, which is when $I_q(q')$ is the smallest possible: $I_q(q') = 2\epsilon^2$. So we have\footnote{If the inaccurate agent uses the worst $q'$ with error $\epsilon$, the regret calcualtion is similar, but with $\epsilon$ rescaled by a factor of $1/\sqrt{\Delta}$, using the inverse of Pinsker's inequality: 
$2 \epsilon^2 \tau/\Delta  + R$. The smallest-regret  $q'$ is used for the bound.} 
$\mathbb{E}[R^{T}(q'_{1:T})] = 2\epsilon^2 T + R$. 
% 
By Equation (\ref{eq:logshare-as-regret}), the regret difference between two players is equal to the log of their wealth ratios  plus a constant. By comparing regret levels between the agents, we obtain a bound on the typical survival time $\tau$ up to which the inaccurate player holds, in expectation, more than half the market value; beyond this point, their expected share is less and continues decaying to zero:\footnote{An alternative question one may want to ask is, ``How accurate should my prior be if I want to hold a share of the market for $T$ steps against the competitor?'' For this, one can invert the equation to get $\epsilon = \sqrt{(f(T) - R)/T}$.}
\[
f(\tau) = 2\epsilon^2 \tau + R.
\]

\vspace{5pt}
\noindent
{\em Remark:} Note that $f(t)$ is the competitor's actual regret. Regret bounds for known learning algorithms (e.g., bandit algorithms such as UCB \cite{slivkins2019introduction} or gradient-descent and second-order methods like those described in  \cite{hazan2016introduction}) are typically worst-case bounds. Estimating the expected regret (or the most likely one) in game dynamics in general, or specifically in our investment scenario, is an interesting open problem.
\vspace{5pt}

For the special cases that the competitor has constant regret (e.g., an accurate Bayesian), or the competitor has $\log T$ or $\sqrt{T}$ regret we get the following results. 
\begin{observation}\label{obs:survival-time-gainst-constant-regret}
    The expected survival time of a Bayesian learner with $\epsilon$-inaccurate prior against any player with constant regret (e.g., a perfect Bayesian) is $O\big(\frac{1}{\epsilon^2}\big)$. 
\end{observation}
For the the competitor with logarithmic regret, we get an equation of the form $\tau = c_1 \cdot \exp(c_2 \epsilon^2\tau)$. For small errors $\epsilon$, by expanding the exponent, we have the following bound.
\begin{observation}\label{obs:survival-time-gainst-log-regret}
    The expected survival time of a Bayesian learner with $\epsilon$-inaccurate prior against any player with logarithmic regret (e.g., a no-regret convex optimizer) is $O\big(\frac{1}{\epsilon^3}\big)$.   
\end{observation}
Finally, in competition against a learner with regret $f(T) = \sqrt{T}$ the survival time is longer.
\begin{observation}\label{obs:survival-time-gainst-sqrt-regret}
    The expected survival time of a Bayesian learner with $\epsilon$-inaccurate prior against any player with $\sqrt{T}$ regret is $O\big(\frac{1}{\epsilon^4}\big)$. 
    % For $c$ is $\tau \leq \frac{c-R^q}{2 \epsilon^2}$.  
\end{observation}

The interpretation of the above analysis depends, of course, on the time scale we are interested in. When $T$ is large, which is our main focus (e.g., when trade occurs at high frequency or if investments are long-term), only the long-term survival matters. In this case, a Bayesian with an inaccurate prior will eventually vanish in competition with any learner who converges to the truth (such as a regret minimizer). However, when considering transient states as well, it is possible that a Bayesian  with a prior that is inaccurate but relatively close to the true distribution could hold a significant market share for some time before eventually vanishing. See Section \ref{sec:simulations} for an example of such a scenario.
