\bfpar{No-regret learning}
No-regret learning is a fundamental concept in the study of learning in games, tracing back to early work in game theory \cite{blackwell1956analog,brown1951iterative,hannan1957lapproximation,robinson1951iterative} and subsequently developed in influential papers on no-regret algorithms \cite{DBLP:journals/siamcomp/AuerCFS02,blum2008regret,foster1997calibrated,fudenberg1995consistency,hart2000simple,kalai2005efficient}. See \cite{cesa2006prediction,hart2013simple,slivkins2019introduction} for broad introductions. The dynamics of no-regret learning algorithms in repeated games have been extensively studied in various domains, including repeated auctions \cite{aggarwal2024randomized,bichler2023convergence,daskalakis2016learning,guo2022no,feng2020convergence,deng2022nash,kolumbus2022auctions,kolumbus2024paying}, budgeted auctions \cite{balseiro2019learning,feng2024strategic,fikioris2023liquid,fikioris2024learning,lucier2024autobidders}, portfolio selection (as we discuss below) \cite{DBLP:journals/ml/BlumK99,gofer2016lower,hazan2015online,li2012expected}, repeated contracting \cite{collina2024repeated,guruganesh2024contracting,zhu2022sample}, bilateral trade \cite{cesa2023bilateral,cesa2021regret}, and implications of no-regret algorithms on user incentives in general games \cite{kolumbus2022and}. In all these works on learning dynamics, either a single learner is considered, or there are multiple learners where it is assumed that all are regret minimizers.

When optimizing growth rate of wealth over multiple interactions, the resulting  growth rate (the change in the $\log$ of wealth) is an additive function over the periods, so classical no-regret learning applies. In the context of investment portfolio selection, \cite{DBLP:journals/ml/BlumK99,hazan2015online} introduce  algorithms with 
worst-case regret bounds of $O(\log(T))$. See also \cite{cesa2006prediction}, Chapter 10 and \cite{hazan2016introduction} Chapter 4 for a  discussion and analysis of investment strategies achieving this regret bound. These studies emphasize regret as the key metric, and in that, implicitly assume that if the time-average of the regret vanishes, then the agent is considered successful. However, we show that in a competitive market, an agent may achieve vanishing regret yet still be driven out of the market, with wealth share converging to zero. 


\bfpar{Bayesian learning} In economics, it is standard to view learning as a by-product of expected utility maximization. That is, the decision-making agent's objective is not learning per se but making good decisions as evaluated by expected utility. It is well known, though not often written formally, that expected utility theory, and its subjective axiomatic foundation, as in \cite{savage}, imply Bayesian learning. So the focus on Bayes is not surprising. 

There is also a long tradition in economics and finance of arguing that the reason financial markets price assets correctly is that the forces of market selection drive out irrational traders, leaving rational traders to determine prices. This argument is historically most closely associated with \cite{alchain1950}, \cite{friedman1953} and \cite{fama1965}. 
Here, rationality is interpreted as having a statistically correct model of the stochastic process that determines the returns on assets and optimizing according to it.\footnote{No-regret learning, in comparison, can be viewed from this perspective as a form of bounded rationality.} As this notion of rationality implies Bayes, the argument is often extended to argue that markets select for Bayesians and against all other learning procedures that do not at least asymptotically behave like Bayes. The intuition for the `market selection' argument is simple: if rational and irrational traders disagree, then rational traders take money away from irrational ones in the events that rational traders consider to be likely. The argument then goes on to note that as rational traders compute conditional probabilities correctly, their view of likely events is at least asymptotically correct. So they survive and irrational traders do not survive. This argument has several important qualifications, most importantly, complete markets and the possibility of eventually-correct beliefs for rational traders. The market selection hypothesis, and qualifications to it, have more recently been investigated in \cite{blumeeasley1992}, \cite{sandroni2000} and \cite{blumeeasley2006}. 

There is also a literature in finance and operations research on wealth accumulation for investors who maximize the expected growth rate of their wealth. The investment rule that implements this objective is known as the Kelly rule, see \cite{Kelly1956}. There is a long standing controversy on the applicability of this rule in consumption-investment decisions, see \cite{Samuelson1971fallacy} or \cite{Hens2009} for a treatment of wealth dynamics with a variety of objectives including expected growth rate maximization. 

Interestingly, the literature in computer science on no-regret learning and the literature in economics on Bayesian learning in asset markets differ not only in the questions they ask and methods they use, but are also almost entirely disjoint, with very few cross-references between them. In this work, we establish a formal connection between these different approaches to learning in markets. We show that there is a surprisingly simple relationship between the entropy of the learner---a key notion in the Bayesian learning literature---and the regret of the learner in these markets. We then analyze a heterogeneous group of Bayesian learners and regret minimizers interacting in a competitive market, and study the implications of these different learning approaches for survival in the market.

\bfpar{Dynamics in markets and investment games}
A rich body of literature explores the dynamics of various market models. Prominent examples include studies on the t\^atonnement price dynamics \cite{cheung2018amortized,cole2008fast,walras1900elements}, proportional response dynamics in Fisher markets \cite{cheung2018dynamics,kolumbus2023asynchronous,zhang2011proportional}, dynamics in exchange economies \cite{branzei2021exchange,branzei2021proportional} and the von Neumann model of expanding economies \cite{branzei2018universal,branzei2025tit}, repeated Cournot competition games \cite{bischi2008learning,farrell1989renegotiation,kolumbus2022and}, and dynamics in pricing games \cite{banchio2023adaptive,hartline2024regulation,hartline2025regulation}.
These studies focus on the joint dynamics of agents, analyzing their convergence properties, equilibria, and implications for overall outcomes. While many of these dynamics are (or can be framed as) distributed learning processes, these works assume homogeneous learning behavior among agents and do not address settings where learners use fundamentally different learning approaches. 
Other research on investment games focuses on equilibrium outcomes and interventions designed to incentivize investment \cite{babaioff2022optimal,HalacKremerWinter2020raising,jackson2021systemic}, which do not involve learning.

\bfpar{Games with heterogeneous learners}
The present work studies interactions between heterogeneous learning agents and their implications for market dynamics and outcomes. The literature on learning in games contains very few studies that analyze the dynamics between learners using different strategies. The most closely related work to ours is \cite{blumeeasley1992}, which show that Bayesian learners outperform dynamic heuristics based on imitation or search in investment games. An additional line of work 
on repeated games where two players use different types of reasoning 
is on Stackelberg games where an optimizer seeks the best strategies against an opponent who commits to using a fixed no-regret algorithm in the game \cite{arunachaleswaran2024algorithmic,arunachaleswaran2024learning,arunachaleswaran2024pareto,BravermanMaoSchneiderWeinberg2018selling,cai2023selling,guruganesh2024contracting,mansour2022strategizing}. This differs significantly from our study, where we do not assume that any player is strategically optimizing as a Stackelberg leader.  
