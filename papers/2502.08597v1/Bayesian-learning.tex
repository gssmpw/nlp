Next, we consider the standard Bayesian learning results. We restate and summarize in this section known results for completeness and to provide a framework for our comparison of Bayesian and no-regret learning.

Consider a Bayesian watching the process on states in the market. Suppose that the Bayesian does not know the probability $q$, and instead considers a finite number of possibilities for $q$: models $\theta^1, \dots, \theta^K$. We assume that $\theta^k_s \geq \minProb$ for all $k$ and $s$. See the remark at the end of this section for connection to results for Bayesian learners with a continuum prior. The Bayesian has a prior $\lambda_0=(\lambda^1_0, \dots, \lambda^K_0)$ with $\lambda^k_0 > 0$ for all $k$.
% 
Let $n_t^s$ be the number of times that state $s$ occurs by time $t$. The posterior probability of model $\theta^k$ is, by Bayes rule,
\begin{align}
\lambda^k_t &= \frac{\lambda^k_0\prod_s (\theta^k_s)^{n_t^s}}{\sum_l \lambda^l_0\prod_s (\theta^l_s)^{n_t^s}}.
\end{align}
\vspace{42pt}

Thus, for any models $k$ and $l$
\begin{align}
\frac{\lambda^k_t }{\lambda^l_t } &= \frac{\lambda^k_0\prod_s (\theta^k_s)^{n_t^s}}{\lambda^l_0\prod_s (\theta^l_s)^{n_t^s}},\\
\log\frac{\lambda^k_t }{\lambda^l_t } &= \log\frac{\lambda^k_0}{\lambda^l_0} + \sum_s n_t^s \log(\theta^k_s) - \sum_s n_t^s \log(\theta^l_s).
\end{align}

Using the law of large numbers and the definition of relative entropy, we have, almost surely, 

\begin{align}\label{eq: bayes limit}
\lim_t \frac{1}{t} \log\frac{\lambda^k_t }{\lambda^l_t } = I_q(\theta^l) - I_q(\theta^k).
\end{align}

If model $k$ is correct, $\theta^k = q$, then $I_q(\theta^k)=0$, so for any other model $\theta^l$ with $ I_q(\theta^l) \neq 0$ we have $\lambda^l_t \to 0$ and $\lambda^k_t \to 1$ almost surely. Equation (\ref{eq: bayes limit}) shows that the log of posterior probabilities converges linearly in time, and thus that the ratio of posterior probabilities converges at an exponential rate. 

The Bayesian learns the true model if it is one of the models considered. Alternatively, suppose that no model considered by the Bayesian is correct, but that there is a unique model with minimum entropy relative to the true model, i.e., there is a $k^*$ such that $0<I_q(\theta^{k^*})<I_q(\theta^k)$ for all $k\neq k^*$. Then it follows from equation (\ref{eq: bayes limit}) that $\lambda_t^{k^*} \to 1$ almost surely. 

These convergence results can be summarized in the following proposition. 
\begin{proposition}\label{thm:bayesian-convergence}
    Let $\Theta$ denote the support of a Bayesian learner's prior, and let 
    $
    \theta^* = \argmin_{\theta \in \Theta} I_q(\theta)
    $. 
    The Bayesian's beliefs converge to $\theta^*$ almost surely. 
\end{proposition}

The Bayesian's time $t$ predicted probability on states is $q_t = \sum_k \lambda^k_t \theta^k$. If one of the models the Bayesian considers is correct, then this predicted probability converges at an exponential rate to $q$ .

\subsection{An Investment Rule for the Bayesian}

We next generate an optimal investment rule for the Bayesian from the predicted probability on states. A Bayesian trader $n$ with predicted probability $q^n_t$ who wants to maximize the expected growth rate of wealth selects at time $t$ an investment rule that solves:

\begin{align}
Max_{\alpha^n_{t}\geq 0} & \sum_s {q^n_{st}} \log\frac{w^n_{t+1}}{w^n_{t}} \label{eq:bays-investment-rule-w} \\
s.t. &\sum_s \alpha^n_{st} = 1 \notag, \\
Max_{\alpha^n_{t}\geq 0} & \sum_s {q^n_{st}} \log\frac{\alpha^n_{st}}{p_{st}} \label{eq:bays-investment-rule-p} \\
s.t. &\sum_s \alpha^n_{st} = 1. \notag
\end{align}

\noindent The solution is clearly $\alpha^n_t = q^n_t$. The Bayesian agent should ``bet its beliefs.'' 
\bigskip

\noindent
    {\em Remark:} An alternative foundation for this investment rule can be derived from a Bayesian whose objective is to maximize its expectation of discounted sum of log utility of consumption. 
    Suppose the agent begins with some wealth $w^n_0$, prior $\lambda^n_t$ on models, and discount factor $0 < \delta < 1$. If there is a complete set of Arrow securities at each time, this agent will optimally invest a fraction $\delta$ of its wealth at each time, deriving utility from consuming the rest, and use the investment rule $q^n_t$. Assuming that all agents have the same discount factor yields results identical to those for an economy where agents maximize expected growth rate of wealth.


\subsection{Wealth Dynamics for the Bayesian}

A Bayesian agent that maximizes the expected growth rate of its wealth uses the investment rule $\alpha^n_t=q^n_t$ at each time $t$ (i.e., betting its beliefs). We consider the evolution of the wealth of the Bayesian relative to any other trader who uses an investment rule that does not depend on future states, i.e., it's time $t$ value depends only on state realizations through time $t$.   
The log ratio of their wealths is, from Section \ref{sec:model}
  \begin{align}
	\log(r^{nm}_{T})   &= \sum_{t=1}^{T}\sum_{s=1} ^S \mathbf{1}_{\{s_t = s\}} \log (\frac{\alpha^n_{st}}{q_s}) - 
    \sum_{t=1}^{T}\sum_{s=1} ^S \mathbf{1}_{\{s_t = s\}} \log (\frac{\alpha^m_{st}}{q_s}) +
    \log(r^{nm}_{0}). 
\end{align}
Note that 
\begin{align}
\sum_{t=1}^{T} \E[\log (\frac{\alpha^n_{st}}{q_s})]-\E[\log(\frac{\alpha^m_{st}}{q_s})]
	   &= \sum_{t=1}^{T} I_q(\alpha^m_t)-\sum_{t=1}^{T} I_q(\alpha^n_t) + \log(r^{nm}_{0}). 
\end{align}

If one of the models that the Bayesian considers is the correct model, $q$, then $I_q(\alpha^n_t)$ converges exponentially to $0$ almost surely. So in this case,  $\sum_{t=1}^{T} I_q(\alpha^n_t)$ is finite almost surely. The relative entropy $I_q(\alpha^m_t)$ is at least $0$ for any investment rule and is $0$ if $\alpha^m_t=q$ for all $t$. Thus, the sums of relative entropies are at least equal to the negative of the finite sum of relative entropies for the Bayesian. 

Subtracting the means from the sums in Equation (\ref{eq: bayes limit}) yields a martingale, and the arguments in \cite{blumeeasley1992} imply the following conclusion:
\begin{proposition}\label{thm:Bayesians-suvive-against-q}
    Let $r^{nm}_T$ be the wealth ratio of a Bayesian learner $n$ competing against an agent $m$ playing $\alpha^m_t=q$ for all $t$. Then 
    $
    \liminf_{T \rightarrow \infty} \E[r^{nm}_T] > 0
    $
    almost surely; i.e., the Bayesian survives.
\end{proposition}

Note that if the correct model is not in the set of models considered by the Bayesian, then the predicted distribution converges to the model closest in relative entropy to $q$, and the sum of relative entropies diverges. In this case, if some investor, m, uses  $\alpha^m_t = q$ for all $t$, then the Bayesian's wealth converges almost surely to $0$. 

\vspace{5pt}
\noindent
{\em Remark:}
In this paper we consider Bayesian learners with a finite set of models. The Bayesian analysis can be extended to priors with a continuum of models in their support, see \cite{clarkebarron1990}. In this case, learning occurs at rate 
$O((\log T)/T)$, where the constant depends on the dimension of the set of models considered. We observe that this convergence rate yields a regret of $O(\log T)$ at time $T$. This implies, by our Theorems \ref{thm:survival-by-finite-regret-gap} and \ref{thm:regret-of-q}, that a Bayesian learner with continuum prior does not survive in a market when competing with an investor who knows the true underlying process, or with a Bayesian with a finite support. 
The result about the effect of the dimension of the support of the Bayesian's prior is known in the economics literature, see \cite{blumeeasley2006}, but the approach there does not use a measure of regret. 

Thus, such a Bayesian will have regret of the same order or magnitude as a no-regret learner. In fact, there is an interesting connection between Bayesian learning with a convex continuum support and no-regret learning using the exponential weights algorithm. One can think of the classical exponential weights algorithm (as used in Theorem 3.3 of \cite{cesa2006prediction}  or Theorem 4.4. of \cite{DBLP:journals/corr/abs-1909-05207}, when choosing their parameters $\eta$, $\alpha$, respectively, as $\eta=\alpha=1$) as doing Bayesian learning with a uniform prior over the convex set of distributions on states. It is interesting to note that for a $d$-dimensional convex prior, \cite{clarkebarron1990} show convergence at the rate of $\frac{d}{2}(\log T)/T$, resulting in regret $\frac{d}{2} \log T$ (though they do not discuss regret). By contrast, no-regret learning results prove a $d \log T$ regret bound. While this second bound is a factor of 2 worse, note that the no-regret result is worst case: it does compare to the best single investment strategy in hindsight, but does not assume a steady process underlying the market. Since the two algorithms follow the same investment strategy, the no-regret results prove that Bayesian learning over a convex continuum prior of dimension $d$ also guarantees a worst-case regret of at most $d \log T$ against the best fixed strategy in hindsight, even if the Bayesian's  assumption of a steady underlying process turns out incorrect.
