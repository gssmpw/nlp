In this section, we further analyze the relationship between an agent's  regret to its wealth level and long-term survival in market dynamics, and use this analysis to derive the regret level of Bayesian learners. 
This relationship depends on the competition. In particular, we saw in Theorem \ref{thm:survival-by-finite-regret-gap} that the learners who survive (have a positive expected wealth level in the limit) are only those who maintain a constant regret gap relative to the best competitor in the market. 

As mentioned in Section \ref{sec:compare}, using the state distribution $q$ as the investment rule (if $q$ were known)  maximizes the expected growth rate among constant strategies. However, even this strategy incurs some regret, as it generally differs from the best strategy in hindsight. We now ask:  what is the regret level of using the true state distribution $q$ as the investment rule? This regret level represents the best attainable expected regret. Thus, by Theorem \ref{thm:survival-by-finite-regret-gap}, evaluating this regret level would serve as the benchmark for determining which regret levels ensure survival (with high probability, see Definition \ref{def:survive-and-vanish}) against any competitors who do not have information about future state realizations. 
To set the ground, let us first consider the following definition:

\begin{definition}
Fix an arbitrary history of state realizations $s_1,\dots,s_T$ and denote the empirical distribution of this history by $\hat{q}$, where $\hat{q}_s = \frac{1}{T}\sum_{t=1}^T \ind{s_t=s}$. A {\em ``magic agent,''} indexed also by $\hat{q}$, is an agent playing the (eventual) empirical distribution in every step. That is,  $\alpha^{\hat{q}}_{st} = \hat{q}_s$ for all $t \in [T]$.     
\end{definition}

Note that this agent uses information regarding future realizations of random states, which is not available to real agents in a stochastic environment. Next, we derive the regret level of using the correct distribution of states $q$ as the investment strategy.
\begin{theorem}\label{thm:regret-of-q}
    An agent using the state distribution $q$ as the investment strategy for all $t$ has a constant expected regret.
    The expected regret, denoted $R$, depends only on the number of states $S$. 
\end{theorem}

\begin{proof}
Consider an agent indexed by $q$ using the state distribution $q$ as its investment strategy; i.e, $\alpha^q_{t} = q$ for all $t$, and we denote the seqeunce by $q_{1:T}$. Agent $1$ competes with a magic agent. 
The regret of the agent playing strategy $q$  is denoted $R^T(q_{1:T})$, and denote its expectation  
by $R = \E[R^T(q_{1:T})]$. On one hand, using the fact that $\hat{q}$ has no regret by definition, from Equation \ref{eq:logshare-as-regret} we have   
\[
R^T(q_{1:T}) = \log(r^{\hat{q}q}_T) - \log(r^{\hat{q}q}_0).
\] 
On the other hand, taking an expectation of Equation \ref{eq:logshares}, we have
\begin{eqnarray*}
     \E \big[\log(r^{\hat{q}q}_T)\big] & = & 
     \E \big[\sum_s \sum_t \mathbf{1}_{\{s_t = s\}} \log(\frac{\alpha^{\hat{q}}_{s}}{q_{s}})\big]+ \log(r^{\hat{q}q}_{0}) \\
     & = &
     T \E \big[\sum_s \frac{n_s}{T} \log(\frac{n_s/T}{q_s})\big]+ \log(r^{\hat{q}q}_{0})
      = 
     T  \cdot 
     \E[I_{\hat{q}}(q)] + \log(r^{\hat{q}q}_{0}).
\end{eqnarray*}

where $n_s$ is the empirical count of state $s$ until time $T$. And thus, we get
    \[
    R =  T \cdot \E \big[I_{\hat{q}}(q)\big].
    \]
    
That is, the expected regret equals the relative entropy (a.k.a. KL divergence) between an empirical distribution and the state distribution scaled by $T$.  The question of survival of this agent now hinges on how does $I_{\hat{q}}(q)$ depend on time. 
Here, we use a result from information theory, showing that the expected KL divergence of the empirical distribution decreases as the alphabet size divided by the sample size\footnote{We are interested in the expectation. \cite{mardia2020concentration} also provide concentration bounds for this result.} (see, e.g., Lemma 13.2 in \cite{polyanskiy2024information}). In our case, $\E[I_{\hat{q}}(q)] \equiv \E[D_{KL}(\hat{q}||q)] = \frac{S-1}{2T} + o(\frac{1}{T})$.
Thus, the expected regret $R$ is constant, depending only on the number of states. 
\end{proof}
We can now also state the following result about a learner competing with a magic agent.
\begin{corollary}\label{cor:q-survives-against-zero-regret}
An agent indexed by $q$ using the correct distribution $q$ survives when competing against a magic agent. This is since by Theorem \ref{thm:regret-of-q}, the strategy $q$ yields constant expected regret $R$, and so the regret difference compared to $\hat{q}$---which  by definition yields zero regret---is constant as well. By Theorem \ref{thm:survival-by-finite-regret-gap}, this implies that $q$ survives.
\end{corollary}

Our analyses above and in the preceding sections lead to the following characterization of the regret of Bayesian learners and the survival conditions for other learning agents competing with them in the market. 

\begin{theorem}\label{thm:regret-of-a-Bayesian}
    The following holds for any market parameters:
    \begin{enumerate}
        \item Bayesian learners with any finite-support prior that includes the correct state distribution have expected regret bounded by a constant at all times.
        \item A learning agent survives in competition with Bayesians if and only if it has constant regret.
    \end{enumerate}
\end{theorem}
\begin{proof}
By Theorem \ref{thm:regret-of-q}, an agent $n$ using $\alpha^n_t = q$ for all $t$ incurs constant expected regret. By Proposition \ref{thm:Bayesians-suvive-against-q}, a Bayesian learner whose support includes the distribution $q$ almost surely survives against such a competitor. Next, Theorem \ref{thm:survival-by-finite-regret-gap} shows that if some agent survives against a competitor, then the difference between their regret levels must be bounded by a constant at all times. Therefore, the Bayesian competing against the state distribution $q$ almost surely incurs constant regret. 
% 
Having established the first point, the second follows directly from Theorem \ref{thm:survival-by-finite-regret-gap}: Since the Bayesian has constant regret, any competitor who survives must also have constant regret. 
\end{proof}

Before concluding this section, the following lemma provides an expression for the regret of a player as a function of the entropy relative to strategy $q$, using our notation $R$ from Theorem \ref{thm:regret-of-q} for the expected regret of strategy $q$. This will be useful in the following Section on imperfect Bayesians.

\begin{lemma}\label{thm:expected-regret-lemma}
    The expected regret of a constant strategy sequence ($\alpha^n_{1:T}$ such that $\alpha^n_t =  \alpha^n$ for all $t$) is 
    $$
    R^T(\alpha^n_{1:T}) = T \cdot I_q(\alpha^n) + R.
    $$ 
\end{lemma}
\begin{proof}
    This follows from the definition of regret as a utility difference, as discussed in Section \ref{sec:compare}. Specifically, the regret of strategy sequence $\alpha^n_{1:T}$ is $R^T(\alpha^n_{1:T}) = U(\alpha^{\hat{q}}_{1:T}) - U(\alpha^n_{1:T})$, also, $R = \E[U(\alpha^{\hat{q}}_{1:T}) - U(q_{1:T})]$, and denote the regret of $\alpha^n_{1:T}$ with respect to the benchmark $q_{1:T}$ as $R^\alpha_q = U(q_{1:T}) - U(\alpha^n_{1:T})$. We have 
    $
    \E[R^T(\alpha^n_{1:T})] = \E[R^\alpha_q] + R = \E\Big[ T \sum_s \frac{n_s}{T} \log(\frac{q_s}{\alpha^n_{s}}) \Big] + R = T \cdot I_q(\alpha^n) + R
    $.   
\end{proof}
