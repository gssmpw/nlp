As we noted above, an investor who knows the probabilities $q_s$   maximizes the expected growth rate of their wealth at each time $t$ using the investment rule $\alpha^n_{t}=q$. 
Next, consider an agent $n$  with a sequence of investment profiles $\alpha_t^n$. We will first compare the wealth of this agent to that of an agent who uses the investment rule $\alpha=q$, and then, we will compare different types of learning agents.

As we saw in the previous section in Equation (\ref{eq:logshares}), the log of the wealth ratio of an agent at time $T$ compared to an investor using the optimal $\alpha = q$ rule can be expressed as

\begin{equation}
\label{eq:investment return}
\sum_{t=1}^{T}\sum_{s=1} ^S \mathbf{1}_{\{s_t = s\}} \log \alpha^n_{st} -\sum_{t=1}^{T}\sum_{s=1} ^S \mathbf{1}_{\{s_t = s\}} \log q_s
+ \log(r^{nm}_{0}). 
\end{equation}

The main goal of this paper is to compare different theories on how to best learn to invest. One theory is Bayesian learning, assuming that the investor $n$ has a Bayesian prior over the possible asset return probabilities. At each step, a Bayesian learner uses the expected return probability implied by their beliefs, updating their beliefs according to the observed states by using Bayes rule. One can also use multi-armed bandit learning methods given such a prior of possible asset return distributions, and have the agent invest using one of the distributions in its prior at each step, aiming to learn which one is the best. A different theory would consider the maximization problem of Equation (\ref{eq:investment return}) as a concave maximization problem and use no-regret learning algorithms to optimize the value. 

From the latter two perspectives, if the difference between the two sums in Equation (\ref{eq:investment return}) is negative, this is a form of regret\footnote{\label{footnote:regret-wrt-q}This is regret only with respect to $q$; while $q$ is optimal in expectation, it may not be the best fixed strategy in hindsight for a given sequence of state realizations. We extend on this point later; see Theorem  \ref{thm:regret-of-q}.} 
against a fixed strategy $\alpha=q$. First, consider an investor with a fixed number of possible models for $q$, $\{\theta^1, \ldots, \theta^K\}$. 
This formulation of the problem would allow us to consider an investor using a classical multi-arm bandit approach, such as UCB (see, e.g., \cite{slivkins2019introduction}): the investor can invest using one of the $\theta^k$ as investment rules, and use no-regret methods to update their belief on the expected value of each of the options. Assuming the correct distribution $q$ is among the options considered, we can express the gap in value between any one choice $\theta^k$ and the true distribution using Equation (\ref{eq:fixed invext entropy}) as 
\begin{equation}\label{eq:MAP-gap}
   I_q(\theta^k)-I_q(q)=I_q(\theta^k).
\end{equation}

Classical learning algorithms, such as UCB, then guarantee regret against the optimal strategy $q$ depending inversely on the above gap that is at most 
\begin{equation}
 O(\log T) \cdot\sum_{k:\theta^k\neq q}\frac{1}{I_q(\theta^k)}.  
\end{equation}
In the next section, we consider a Bayesian investor who not only has a finite list of possible $q$ vectors, but also has a prior over these models, and uses Bayes rule to update their prior given the outcomes seen in previous rounds.  

Alternatively, we can use learning without a list of options or a prior, and  hence no longer depend on the assumption that the correct probability vector is one of the options considered. Let us define the utility of an investor at time $t$ with investment strategy $\alpha$ as 
\begin{equation}
    U^t(\alpha)=\sum_s \mathbf{1}_{\{s_t = s\}} \log \alpha_{st}.
\end{equation}
$U^t(\alpha)$ is the rate of change in the investor's relative wealth in this period compared to the investor using $\alpha=q$.
Notice that this is a concave function of $\alpha$ for any outcome of the random variables. For an investor $n$ with investment sequence $\alpha^n_{1:T}=(\alpha^n_1, 
\ldots, \alpha^n_T)$, the value the investor is aiming to optimize is
\begin{equation}
\sum_{t=1}^{T}U^t(\alpha^n_t).
\end{equation}
Using these definitions, the classical regret from no-regret learning for an investment sequence $\alpha_{1:T}^n$ at time $T$ is defined as 
\begin{equation}
    R^T(\alpha_{1:T}^n)=max_{\alpha\in \Delta_S} \sum_{t=1}^T U^t(\alpha)-\sum_{t=1}^T U^t(\alpha^n_t).
\end{equation}

Note that the regret of $\alpha^n_{1:T}$ is at least as large as its regret against the investor with the optimal fixed rule $\alpha=q$. 
The maximum fixed rule $\alpha$ used in the definition is the best fixed investment rule with hindsight, having seen the outcome of all choices and random state realizations, which is not an implementable investment rule even for an investor who knows the probabilities. So even the investor with knowledge of the probabilities and using $\alpha^n_t = q$ for all $t$ will have some regret based on the outcome of random events (see Section \ref{sec:no-regret} and Theorem \ref{thm:regret-of-q}).

Next, we reconsider two investors, $n$ and $m$, with  investment sequences $\alpha_{1:T}^n$ and $\alpha_{1:T}^m$, respectively. By Equation (\ref{eq:logshares}), we can express their time-$T$ wealth-shares using regret notation as
 \begin{equation}
 \label{eq:logshare-as-regret}
	\log(r^{nm}_{T})   = \sum_{t=1}^{T} U^t(\alpha^n_t)-\sum_{t=1}^{T} U^t(\alpha^m_t) + \log(r^{nm}_{0})=R^T(\alpha_{1:T}^m)-R^T(\alpha_{1:T}^n)+\log(r^{nm}_{0}).
\end{equation}

We note that despite this very strong benchmark for regret, and without any prior, the convex optimization literature offers very strong guarantees on expected regret. For any concave function $U^t(\alpha)$, a simple gradient-decent based algorithm guarantees regret at most $\sqrt{T}$ over $T$ periods, while for the $\log$ based utility function (that is Exp-Convex) considered here, second order optimization methods guarantee a $O(\log T)$ regret bound  \cite{DBLP:journals/ml/BlumK99} (see also, \cite{cesa2006prediction} Theorem 3.3 or \cite{hazan2016introduction} Theorem 4.4.)

The central question we ask in this paper is:  
{\em
What learning algorithms are better in ensuring that the player's wealth remains high, or at the very least, does not vanish? How do Bayesian learners and convex optimizers compare in terms of relative wealth in this game? 
}
Using equation (\ref{eq:logshare-as-regret}) we get the following result, connecting regret and survival in the market.

\begin{theorem}\label{thm:survival-by-finite-regret-gap}
    Suppose that there are two agents, $n$ and $m$, in the market with regrets $R^n(T)$  and $R^m(T)$ respectively. If $ \ \lim_{T \rightarrow \infty} R^n(T) - R^m(T) = \infty$, then agent $n$ vanishes from the market. 
    
    Agent $n$ survives\footnote{Note that survival and vanishing are probability $1$ statements. Thus, survival is not simply the negation of vanishing.} if and only if for every agent $m \neq n$ it holds that $lim_{T \rightarrow \infty} R^n(T) - R^m(T) < \infty$.
\end{theorem}

\begin{proof}
 This result follows from the above analysis, leading to Equation (\ref{eq:logshare-as-regret}). For the first part, we have $R^n(T) - R^m(T) = -\log(r^{nm}) - \log(r_0^{nm}) = \log(r^{mn}) + const$. If the left hand side diverges to $+\infty$, then $r^{mn} = w^m/w^n$ diverges as well, which is only possible if $w^n \rightarrow 0$. For the second part, clearly this is a necessary condition due to the first part of the theorem: if an agent vanishes, it does not survive. To see that this condition is sufficient, if there exists a constant $c > 0$ such that $R^n(T) - R^m(T) < c$ at all times for all $m \neq n$, then the wealth ratio $r^{mn}$ with any other player is bounded, and thus $w^n$ is bounded from below. 
\end{proof}

Recall that the entropy of the investment rule using the correct distribution of states is $I_q(q)=0$. 
Also, recall from equation (\ref{eq:MAP-gap}) that the gap in expected reward between this and a different investment rule $q'$ is the entropy $I_q(q')$. This shows that the wrong fixed strategy will have linear regret $\Theta(I_q(q')T)$. 
Learners with $O(\sqrt{T})$ do better and learners with only $O(\log T)$ regret do even better. Note that the constant in the regret term will matter. The investor with regret higher by only a constant factor will also vanish! But how well do all these learners do against a Bayesian learner?
