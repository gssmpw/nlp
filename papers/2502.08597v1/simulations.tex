\textbf{Imperfect Bayesian Learning:}
Consider the following toy example. There is market with two assets where agent 1 is a Bayesian learner and agent 2 is a UCB learner, starting from equal wealth levels.\footnote{We use UCB for simplicity. The main factor here is the different regret rates and not the particular algorithm.} The distribution of states is $q = (0.7, 0.3)$, i.e., state 1 occurs with probability $0.7$, and state 2 otherwise. The Bayesian learner has a prior consisting of three hypotheses: $(0.8, 0.2)$, $(0.9, 0.1)$, $(0.3, 0.7)$, where the first hypothesis is closest to $q$ in relative entropy, but still has an error. The UCB learner considers a similar set of distributions as its action set: $(0.7, 0.3)$, $(0.9, 0.1)$, $(0.3, 0.7)$, but the first of these being the correct distribution $q$. 
The example demonstrates the trade-off between short-term gains for the inaccurate Bayesian from converging quickly to a distribution close to the correct one, and the long-term dominance of the learner who converges to the correct distribution. 

Figure \ref{fig:wealth-dynamics} shows the step-by-step wealth shares over a duration of $10^6$ steps, as well as their moving averages over a window of $10$,$000$ steps for better visualization. After a brief initial phase, the no-regret learner (agent 2) converges to playing the correct distribution almost all the time, and it can be seen that its wealth approaches $1$.  

Figure \ref{fig:wealth-distribution-suffix} depicts the distribution of wealth shares for the agents at each time step during the second half of the simulation. It can be seen that the regret-minimizing agent 2 who is close to the correct distribution in the long run holds almost all the wealth almost all the time. 
Figure \ref{fig:wealth-distribution-prefix} depicts the distribution during the initial $10$,$000$ steps, showing a different picture. The Bayesian (agent 1) still managed to hold half of the wealth about half of this time, inline with the typical survival time estimate in Observation \ref{obs:survival-time-gainst-log-regret}.  

\begin{figure}[t!]
% \vspace{-18pt}
    \centering
    % Subfigure 1
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=1.16\textwidth]{Wealth_dinamics_wrong_Bayesian_vs_UCB.png}
        \vspace{-6pt}
        \caption{Wealth Dynamics. The green oscillations show the step-by-step wealth of player 2. Solid lines show running window averages over the recent 10,000 steps in each time. }
        \label{fig:wealth-dynamics}
    \end{subfigure}
    \hfill
    % Subfigure 2
    \begin{subfigure}[b]{0.325\textwidth}
        \centering        \includegraphics[width=1.07\textwidth]{Wealth_dist_second_half_wrong_Bayesian_vs_UCB.png}
        \vspace{-8pt}
        \caption{Wealth distribution over time steps in the second half of the $10^6$ step simulation. The distribution shown in blue is the wealth of player 1 and in green is of player 2.}
        \label{fig:wealth-distribution-suffix}
    \end{subfigure}
    \hfill
    % Subfigure 3
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=1.07\textwidth]{Wealth_dist_first_10K_wrong_Bayesian_vs_UCB.png}
        \vspace{-8pt}
        \caption{Wealth distribution over time steps in the 10,000 initial steps of the dynamic. The distribution shown in blue is the wealth of player 1 and in green is of player 2.}
        \label{fig:wealth-distribution-prefix}
    \end{subfigure}
    % \vspace{-4pt}
    \caption{Example of wealth dynamics in competition between an inaccurate Bayesian with an error in its support learner and regret minimizing agent who converges to the correct state distribution $q$.}
        % \vspace{5pt}
    \label{fig:wealth-figures}
\end{figure}

\bfpar{Dynamics with a Constant Regret Difference} Next, we consider an example of two Bayesian learners, both with priors including the correct distribution of states. By Theorem \ref{thm:regret-of-a-Bayesian}, their regret levels can differ only by an additive constant, as both are perfect Bayesians with constant regret. Consequently, these agents are bound to coexist in the market. The example demonstrates that while the quality of the priors---provided they include a correct model---does not affect survival, the initial learning phase, which depends on the prior, does influence the steady-state partition of wealth between the agents.  
In this example, as in the previous one, there are two states, with the underlying distribution given by $q = (0.7, 0.3)$. Agent 1 has a prior that includes the correct distribution and two other models: $(0.8,0.2), (0.7,0.3), (0.6,0.4)$. Agent 2's prior includes one additional model: $(0.8,0.2), (0.7,0.3), (0.6,0.4), (0.5, 0.5)$. Both agents start with a uniform prior over the options they consider. Thus, agent 2 places smaller weight initially on the correct model than does agent 1. Figure \ref{fig:wealth-dynamics-two-Bayesians} depicts the wealth dynamics of these learners, starting form equal wealth levels. 

Figure \ref{fig:wealth-dynamics-two-UCB-agents} shows the same scenario in competition between two no-regret learners using UCB. While the worst-case bound of UCB suggests that the learner with the larger action space could suffer higher regret (by a constant factor), we see empirically that under a steady stochastic process, the agents are comparable in the long run, where only in the initial learning phase agent 1 with the smaller action set has an advantage. In comparison to the Bayesian learners in Figure \ref{fig:wealth-dynamics-two-Bayesians}, no-regret dynamics continue to be randomized and ``forget'' the gap in wealth from the initial learning phase. 

\bfpar{Comparison of Bayesian vs. No-Regret Learners} Finally, in Figure \ref{fig:wealth-dynamics-Bayesian-vs-UCB} we present an example of the wealth dynamics of a Bayesian learner (agent 1) in competition with a UCB no-regret learner (agent 2). The system is similar to the ones described above, with two states and $q = (0.7,0.3)$. Both agents consider the models $(0.8,0.2), (0.7,0.3), (0.6,0.4)$, so for both,  the correct model $q$ is in the support. As expected according to Theorem \ref{thm:regret-of-a-Bayesian}, the perfect Bayesian who has constant regret drives out of the market the no-regret learner whose regret is increasing in time.

\begin{figure}[t!]
    \centering
    \vspace{-5pt}
    % Subfigure 1
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{Wealth_dinamics_Bayesian_vs_Bayesian}
        \vspace{-8pt}
        \caption{Two Bayesian learners.}
        \label{fig:wealth-dynamics-two-Bayesians}
    \end{subfigure}
    \hfill
    % Subfigure 2
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{Wealth_dinamics_UCB_vs_UCB}
        \vspace{-8pt}
        \caption{Two no-regret learners.}
        \label{fig:wealth-dynamics-two-UCB-agents}
    \end{subfigure}
    \hfill
    % Subfigure 3
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{Wealth_dinamics_Bayesian_vs_UCB}
        \vspace{-8pt}
        \caption{Bayesian vs. no-regret learner.}
        \label{fig:wealth-dynamics-Bayesian-vs-UCB}
    \end{subfigure}
    % \vspace{-4pt}
    \caption{Wealth dynamics in a two-state two-player market. Figure \ref{fig:wealth-dynamics-two-Bayesians} shows the competition between two Bayesians where the agents have action sets of different sizes which include the correct model $q$. Agent $1$ considers three models and agent $2$ considers the same three models and one additional model. Figure \ref{fig:wealth-dynamics-two-UCB-agents} shows the same scenario for no-regret learners both using UCB. Figure \ref{fig:wealth-dynamics-Bayesian-vs-UCB} shows the competition between a Bayesian (agent 1) and a regret-minimizing UCB learner (agent 2) considering the same set of models.}
    \vspace{-5pt}
    \label{fig:wealth-figures-2}
\end{figure}
