%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

% Recommended, but optional, packages for figures and better typesetting:




\usepackage{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
\usepackage{tablefootnote}  % for table footnotes
\usepackage{xcolor}     
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{fullpage}
\usepackage{wrapfig}
\usepackage{makecell}
% \usepackage[square,numbers]{natbib}
% \bibliographystyle{abbrvnat}
% \renewcommand{\baselinestretch}{1.06}

\usepackage[protrusion=true,expansion=true]{microtype}

% colors
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{cleveref}
\newcommand{\proj}{\mathrm{\Pi}}
\usepackage{amsthm}

\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}


\usepackage{colortbl}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{subfigure}
\usepackage{times}
\usepackage{diagbox}

\usepackage{bm}
\usepackage{pifont}% 
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\jky}[1]{{\textcolor{red}{KY: #1}}} 

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\title{Scalable Bilevel Loss Balancing for Multi-Task Learning}

\author{%
  % examples of more authors
   Peiyao Xiao\thanks{Peiyao Xiao and Kaiyi Ji are with the Department of Computer Science and Engineering, University at Buffalo, Buffalo, NY 14228 USA (e-mail: \href{mailto:peiyaoxi@buffalo.edu}{peiyaoxi@buffalo.edu}, \href{mailto:kaiyiji@buffalo.edu}{kaiyiji@buffalo.edu}).} 
   \quad
    Chaosheng Dong\thanks{Chaosheng Dong is with Amazon.com Inc, Seattle, WA, 98109 USA (e-mail: \href{mailto:chaosd@amazon.com}{chaosd@amazon.com}).}
   \quad
    Shaofeng Zou\thanks{Shaofeng Zou is with the School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ 85281 USA (e-mail: \href{mailto:zou@asu.edu}{zou@asu.edu}). }  
   \quad
   Kaiyi Ji\footnotemark[1] \thanks{ Correspondence to: Kaiyi Ji (\href{mailto:kaiyiji@buffalo.edu}{kaiyiji@buffalo.edu})}
}

\begin{document}
\maketitle
% \twocolumn[
% \icmltitle{Scalable Bilevel Loss Balancing for Multi-Task Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% \jky{rewrite based on my revised introduction.}
Multi-task learning (MTL) has been widely adopted for its ability to simultaneously learn multiple tasks. While existing gradient manipulation methods often yield more balanced solutions than simple scalarization-based approaches, they typically incur a significant computational overhead of 
$\mathcal{O}(K)$ in both time and memory, where $K$is the number of tasks. 
In this paper, we propose BiLB4MTL, a simple and scalable loss balancing approach for MTL, formulated from a novel bilevel optimization perspective. Our method incorporates three key components: (i) an initial loss normalization, (ii) a bilevel loss-balancing formulation, and (iii) a scalable first-order algorithm that requires only $\mathcal{O}(1)$ time and memory. Theoretically, we prove that BiLB4MTL guarantees convergence not only to a stationary point of the bilevel loss balancing problem but also to an $\epsilon$-accurate Pareto stationary point for all 
$K$ loss functions under mild conditions. 
Extensive experiments on diverse multi-task datasets demonstrate that BiLB4MTL achieves state-of-the-art performance in both accuracy and efficiency. Code is available at \url{https://github.com/OptMN-Lab/-BiLB4MTL}.

\end{abstract}
% \textcolor{red}{4. run exp with router. }
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/toy_example.pdf}
\caption{The loss trajectories of a toy 2-task learning problem from \citealt{liu2024famo} and the runtime comparison of different MTL methods for 50000 steps. Stars on the Pareto front denote the converge points. Although FAMO achieves more balanced results than LS and MGDA, it converges to different points on the Pareto front. Our method reaches the same balanced point with a computational cost comparable to the simple Linear Scalarization (LS). Full experimental details can be found in \Cref{app:toy}.}\label{fig:toy}

\end{figure*}
\section{Introduction}
In recent years, Multi-Task Learning (MTL) has received increasing attention for its ability to predict multiple tasks simultaneously using a single model, thereby reducing computational overhead. This versatility has enabled a wide range of applications, including autonomous driving \citep{chen2018multi}, recommendation systems \citep{wang2020m2grl}, and natural language processing \citep{zhang2022survey}.

Typically, research in MTL follows two main schemes. \textit{Scalarization-based} methods, such as linear scalarization, reduce MTL to a scalar optimization problem by using an averaged or weighted sum of loss functions as the objective. Due to its simplicity and scalability, it became the prominent approach in the early studies \citep{caruana1997multitask}. However, it often causes performance degradation compared with single-task learning due to the gradient conflict \citep{yu2020gradient, liu2021conflict}. Gradient conflict arises from two main reasons: 1) gradients point in different directions and 2) gradient magnitudes vary significantly. As a result, the final update gradient may either be offset or dominated by the largest gradient \citep{liu2021towards}. To mitigate this issue, various \textit{gradient manipulation} methods have been developed to find balanced and fair solutions via seeking a better conflict-aware update direction \citep{desideri2012multiple,liu2021conflict,ban2024fair,navon2022multi,yu2020gradient,fernando2023mitigating, xiao2024direction}.  
% often maximizing the minimal improvement across tasks. These approaches ensure convergence to a Pareto stationary point with theoretical guarantees \citep{fernando2023mitigating, xiao2024direction, chen2024three}.
Nevertheless, most of these methods require computing and storing the gradients for all 
$K$ tasks, resulting in substantial $O(K)$ time and memory costs. This limitation reduces their scalability in large-scale MTL applications involving complex models and extensive datasets.

In this paper, we propose a simple and scalable loss balancing approach for MTL from a novel bilevel optimization perspective. Our approach comprises three key components: initial loss normalization, a bilevel loss balancing formulation, and a scalable first-order algorithmic design. Our specific contributions are summarized as follows. 

\begin{list}{$\bullet$}{\topsep=0.3ex \leftmargin=0.15in \rightmargin=0.in \itemsep =-0.022in}

\item {\bf Bilevel loss balancing.} At the core of our bilevel formulation, the lower-level problem optimizes the model parameters by minimizing a weighted sum of normalized individual loss functions. Meanwhile, the upper-level problem adjusts these weights to minimize the disparities among the loss functions, ensuring balanced learning across tasks. 

\item {\bf Scalable algorithms with $O(1)$ time and memory cost.} We develop Bilevel Loss Balancing for Multi-Task Learning (BiLB4MTL), a highly efficient algorithm tailored to solve the proposed bilevel loss balancing problem. Unlike traditional bilevel methods, BiLB4MTL has a fully single-loop structure without any second-order gradient computation, resulting in an overall $\mathcal{O}(1)$ time and memory complexity. The $2$-task toy example in \Cref{fig:toy} illustrates that our BiLB4MTL method achieves a more balanced solution compared to other competitive approaches while maintaining superior computational efficiency.

\item {\bf Superior empirical performance. } Extensive experiments demonstrate that our proposed BiLB4MTL method achieves state-of-the-art performance compared to various scalarization-based and gradient manipulation methods across multiple supervised multi-task datasets, including QM9 \citep{ramakrishnan2014quantum}, CelebA \citep{liu2015deep}, and Cityscapes \citep{cordts2016cityscapes}. Moreover, 
 BiLB4MTL stands out as one of the most efficient and scalable methods.

 \item {\bf Theoretical guarantees.} Theoretically, we show that BiLB4MTL guarantees convergence not only to a stationary point of the bilevel loss balancing problem but also to an $\epsilon$-accurate Pareto stationary point for all 
$K$ individual loss functions.

\end{list}


\begin{figure*}[t]
\centering
\includegraphics[width=0.85\linewidth]{figs/flowchart1.pdf}   \caption{Our bilevel loss balancing pipeline for multi-task learning. First, task losses will be normalized through an initial loss normalization module. Then, the lower-level problem optimizes the model parameter $x^t$ by minimizing the weighted sum of task losses and the upper-level problem optimizes the router model parameter $W^t$ for task balancing.}\label{fig:flowchart}
% \jky{rewrite based on this new illustration; you don have three options now; }
\vspace{-0.3cm}
\end{figure*}

\section{Related Works}
\textbf{Multi-task learning.} MTL has recently garnered significant attention in practical applications. One line of research focuses on model architecture, specifically designing various sharing mechanisms \citep{kokkinos2017ubernet, ruder2019latent}. Another direction addresses the mismatch in loss magnitudes across tasks, proposing methods to balance them. For example, \citealt{kendall2018multi} balanced tasks by weighting loss functions based on homoscedastic uncertainties, while \citealt{liu2019end} dynamically adjusted weights by considering the rate of change in loss values for each task.

Besides, one prominent approach frames MTL as a Multi-Objective Optimization (MOO) problem. \citealt{sener2018multi} introduced this perspective in deep learning, inspiring methods based on the Multi-Gradient Descent Algorithm (MGDA) \citep{desideri2012multiple}. Subsequent work has aimed to address gradient conflicts. For instance, PCGrad \citep{yu2020gradient} resolves conflicts by projecting gradients onto the normal plane, GradDrop \citep{chen2020just} randomly drops conflicting gradients, and CAGrad \citep{liu2021conflict} constrains update directions to balance gradients. Additionally, Nash-MTL \citep{navon2022multi} formulates MTL as a bargaining game among tasks, while FairGrad \citep{ban2024fair} incorporates $\alpha$-fairness into gradient adjustments.

On the theoretical side, \citealt{zhou2022convergence} analyzed the convergence properties of stochastic MGDA, and \citealt{fernando2023mitigating} proposed a method to reduce bias in the stochastic MGDA with theoretical guarantees. More recent advancements include a double-sampling strategy with provable guarantees introduced by \citealt{xiao2024direction} and \citealt{chen2024three}, where the latter one analyzes stochastic MOO algorithms, addressing optimization, generalization, and conflict mitigation trade-offs. Furthermore, \citealt{zhang2024convergence} studied the convergence analysis of both deterministic and stochastic MGDA under a more relaxed generalized smoothness assumption.

\vspace{0.2cm}
\noindent\textbf{Bilevel optimization.} 
Bilevel optimization, first introduced by \citealt{bracken1973mathematical}, has been extensively studied over the past few decades. Early research primarily treated it as a constrained optimization problem \citep{hansen1992new, shi2005extended}. More recently, gradient-based methods have gained prominence due to their effectiveness in machine learning applications. Many of these approaches approximate the hypergradient using either linear systems \citep{domke2012generic, ji2021bilevel} or automatic differentiation techniques \citep{maclaurin2015gradient, franceschi2017forward}. However, these methods become impractical in large-scale settings due to their significant computational cost \citep{xiao2023communication, yang2024simfbo}. 
The primary challenge lies in the high cost of gradient computation: approximating the Hessian-inverse vector requires multiple first- and second-order gradient evaluations, and the nested sub-loops exacerbate this inefficiency. To address these limitations, recent studies have focused on reducing the computational burden of second-order gradients. For example, some methods reformulate the lower-level problem using value-function-based constraints and solve the corresponding Lagrangian formulation \citep{kwon2023fully, yang2024tuning}. The work studies
convex bilevel problems and proposes a zeroth-order optimization method with finite-time
convergence to the Goldstein stationary point \citep{chen2023bilevel}. In this work, we propose a simplified first-order bilevel method for MTL, motivated by intriguing empirical findings.


\section{Preliminary}\label{sec:preliminary}

\textbf{Scalarization-based methods.} Multi-task learning (MTL) aims to optimize multiple tasks (objectives) simultaneously with a single model. The straightforward approach is to optimize a weighted summation of all loss functions:
$$\min_x L_{total}(x) = \sum_{i=1}^K w_il_i(x),$$
where $x\in\mathbb{R}^d$ denotes the model parameter, $l_i(x): \mathbb{R}^d\rightarrow\mathbb{R}_{\geq0}$ represents the loss function of the $i$-th task and $K$ is the number of tasks. 
This approach faces two key challenges: 1) fixed weights can lead to significant gradient conflicts, potentially allowing one task to dominate the learning process \citep{xiao2024direction, wang2024finite}; and 2) the overall performance is highly sensitive to the weighting of different losses \citep{kendall2018multi}. Consequently, such methods often struggle with performance imbalances across tasks. 

\vspace{0.2cm}
\noindent\textbf{Gradient manipulation methods.} To mitigate gradient conflicts, gradient manipulation methods dynamically compute an update $d^t$ at each epoch to balance progress across tasks, where  $t$  is the epoch index. The update $d^t$ is typically a convex combination of task gradients, expressed as:
\begin{align*}
d^t = G(x^t)w^t, \;\; \text{where} \; w^t = h(G(x^t)),
\end{align*}
with 
$G(x^t) = [\nabla l_1(x^t), \nabla l_2(x^t), \dots, \nabla l_K(x^t)].$
The weight vector  $w^t$ is determined by a function $ h(\cdot): \mathbb{R}^{K \times d} \rightarrow \mathbb{R}^K $, which varies depending on the specific method. 
However, these methods often require computing and storing the gradients of all $K$ tasks during each epoch, making them less scalable and resource-intensive, particularly in large-scale scenarios. 
Therefore, it is highly demanding to develop lightweight methods that achieve balanced performance. 

\vspace{0.2cm}
\noindent\textbf{Pareto concepts.} Solving the MTL problem is challenging because it is difficult to identify a common $x$ that achieves the optima for all tasks. Instead, a widely accepted target is finding a Pareto stationary point. Suppose we have two points $x_1$ and $x_2$. It is claimed that $x_1$ dominates $x_2$ if $l_i(x_1)\leq l_i(x_2) \;\forall i\in[K]$, and $\exists j\;l_j(x_1)<l_j(x_2)$. A point is Pareto optimal if it is not dominated by any other points, implying that no task can be improved further without sacrificing another. Besides, a point $x$ is a Pareto stationary point if $\min_{w\in\mathcal{W}}\|G(x)w\|=0$.


\section{Bilevel Loss Balancing for Multi-Task Learning}
In this section, we present our bilevel loss balancing framework for multi-task learning. As illustrated in \Cref{fig:flowchart}, this framework contains an initial loss normalization module, a bilevel loss balancing procedure, and a simplified first-order optimization design.  


\subsection{Initial loss  normalization}\label{sec:LN}

Before applying loss balancing, an initial loss normalization step is introduced to ensure that the rescaled loss functions are on a similar and comparable scale. This step is necessary because training often involves tasks of different types (e.g., classification and regression) with distinct loss functions, as well as targets measured in varying units or scales (e.g., meters, centimeters, or millimeters). Here, we present three effective approaches that work well in different scenarios. 



\vspace{0.2cm}
\noindent\textbf{Identical normalization.} This approach maintains the original loss values, where  $\tilde{l}_i=l_i,\forall i\in[K]$. It is commonly used in multi-task classification scenarios where the loss functions of all tasks are on a similar scale, such as \textit{cross-entropy} functions in classification problems.  

\vspace{0.2cm}
\noindent\textbf{Rescaled normalization. } This method normalizes loss values by rescaling each task's loss using its initial loss value $l_i^\prime$, such that $\tilde{l}_i = \frac{l_i}{l_i^\prime}$. The resulting normalized loss reflects the training progress and ensures comparability across tasks. This approach is particularly well-suited for scenarios where the loss scales do not differ significantly.


\vspace{0.2cm} \begin{wrapfigure}{r}{0.47\textwidth}
    \vspace{-0.5cm}
     \centering
    \includegraphics[width=0.47\textwidth]{figs/loss_distribution_adjust.pdf}
    % \caption{}
    % \label{fig:first}
    \vspace{-0.6cm}
    
    \caption{Curves of loss values during the training process for all 11 tasks on the QM9 dataset. The loss values vary significantly across different tasks.} \label{fig:qm9lossvalues}
    \vspace{-0.8cm}
\end{wrapfigure}
\noindent\textbf{Logarithmic normalization. }In some cases, the loss can vary significantly in scale. For example, as shown in 
\Cref{fig:qm9lossvalues}, we observed that the loss values during training across 9 regression tasks in the QM9 dataset exhibit substantial differences in magnitude, with loss ratios 
 exceeding 1000 in certain instances. To address this issue, we propose logarithmically rescaling the loss functions such that $\tilde{l}_i = \log\left(\frac{l_i}{l_{i,0}}\right)$, where $l_{i,0}$ represents the initial loss value for the $i$-th task at each epoch. Our experiments demonstrate that this initialization approach stabilizes training by reducing large fluctuations caused by significant scale variations.

% \begin{figure}[ht]
% % \vspace{-0.3cm}
% \centering
% \includegraphics[width=0.65\linewidth]{figs/loss_distribution_adjust.pdf}
%     % \vspace{-0.5cm}
    
%     \caption{Curves of loss values during the training process for all 11 tasks on the QM9 dataset. The loss values vary significantly across different tasks.}
%     \label{fig:qm9lossvalues}
%     \vspace{-0.4cm}
% \end{figure}

\subsection{Bilevel loss balancing formulation}
% \jky{first present the formulation and then describes the design.}
Building on the normalized loss function values, we introduce a novel bilevel loss balancing approach for MTL to achieve a more balanced and fair solution. The formulation is as follows: 
\begin{align}\label{eq:object1}
    &\min_{W} \sum_{i=1}^{K-1}|\tau_i\tilde{l}_i(x^*)-\tau_{i+1}\tilde{l}_{i+1}(x^*)|:=f(W,x^*)\nonumber\\
    &\text{s.t. }x^*\in\arg\min_x\sum_{i=1}^K \sigma_i(W)\tilde{l}_i(x):=g(W,x),
\end{align}
where we denote $x^*=x^*(W)$ for notational convenience. It can be seen from \cref{eq:object1} that 
% For simplicity, we have denoted $x^*=x^*(W)$ in this paper. In this design, 
The lower-level problem minimizes the weighted sum of losses w.r.t.~the model parameters $x$, while the upper-level problem minimizes the accumulated weighted loss gaps w.r.t.~the parameters $W$, ensuring balance across different tasks. Note that we define a routing function $\sigma(W) \in \mathbb{R}^{K}$, which is parameterized by a small neural network with a softmax output layer.  
For the upper-level weight vector $\tau=(\tau_1,...,\tau_K)$, we provide two effective options: (i) $\tau=\sigma(W)$ and (ii) $\tau={\mathbf 1}$ that work well in the experiments. 


\subsection{Scalable first-order  algorithm design} 
% \jky{refine wording through GPT for this section. }
To enable large-scale applications, we adopt an efficient and scalable first-order method to solve the problem in \Cref{eq:object1}. Inspired by recent advancements in first-order bilevel optimization \citep{kwon2023fully, yang2023achieving}, we reformulate the original bilevel problem into an equivalent constrained optimization problem as follows.
\begin{align*}
   \min_{W} f(W,x) \;\; \text{s.t. } \underbrace{\sum_{i=1}^K\sigma_i(W)\tilde{l}_i(x)-\sum_{i=1}^K\sigma_i(W)\tilde{l}_i(x^*)}_{\text{penalty function 
 }p(W,x)}\leq 0.
\end{align*}
Then, given a penalty constant $\lambda>0$, penalizing $p(W,x)$ into the upper-level loss function yields
\begin{align}\label{eq:penalty}
\min_{W,x} f(W,x)+\lambda\sum_{i=1}^K( \sigma_i(W)\tilde{l}_i(x)-\sigma_i(W)\tilde{l}_i(x^*)).
\end{align}
Intuitively, a larger $\lambda$ allows more precise training on model parameters $x$ such that $x$ converges closer to $x^*$. Conversely, a smaller $\lambda$ prioritizes upper-level loss balancing during training. The main challenge of solving the penalized problem above lies in the updates of  $W$, as shown below:
\begin{align}\label{WgUpdates}
    W^{t+1}=&W^t-\alpha(\nabla_Wf(W^t,x^t)+\nonumber\\&\lambda(\nabla_Wg(W^t,x^t)-\nabla_Wg(W^t,z_N^t))),
\end{align}
% . Since $x^*$ is not directly available, numerous methods introduce an auxiliary variable $z$ to approximate it and the update rule for $W$ becomes:
% \begin{align*}
%     W^{t+1}=&W^t-\alpha(\nabla_Wf(W^t,x^t)+\\&\lambda(\nabla_Wg(W^t,x^t)-\nabla_Wg(W^t,z_N^t))),
% \end{align*}
where $t$ is the epoch index, $\alpha$ is the step size, and $z_N^t$ is an approximation of $x_t^*\in\arg\min_{x}g(W^t,x)$ through the following loop of $N$ iterations  each epoch.
\begin{align}\label{eq:zupdate}
    z_{n+1}^t=z_n^t-\beta \nabla_zg(W^t,z_n^t), n=0,1,...N-1,
\end{align}
where $N$ is typically chosen to be sufficiently large, ensuring that $z_N^t$ closely approximates $x_t^*$ (full algorithm is provided in \Cref{algorithm1} in the appendix). Consequently, this sub-loop of iterations incurs significant computational overhead, driven by the high dimensionality of $z$ (matching that of the model parameters) and the large value of $N$. 



% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.55\linewidth]{figs/gradient_norm_comparison_plot2.pdf}
% \caption{Gradient norm values during the training process on the Cityscapes dataset. 
% % Gradient norm $\|\nabla_Wg(W^t,z_{50}^t)\|$ is much smaller than $\|\nabla_Wg(W^t,x^t)\|$ and
% Similar phenomena have also been observed in other datasets.}
% % \jky{if we have time, get one more fig on another dataset.}\textcolor{blue}{check it after NYU is finalized}}
% \label{fig:gradientnorm}
% \end{figure}

Interestingly, our experiments reveal that the gradient norm $\|\nabla_W g(W^t, z_N^t)\|$ remains sufficiently small, typically orders of magnitude smaller than the gradient norm $\|\nabla_W g(W^t, x^t)\|$, which is used to update outer parameters $W$. This behavior is illustrated in \Cref{fig:gradientnorm}. Specifically, we set $N=50$ during training. On average, the ratio $\|\nabla_W g(W^t, x^t)\| / \|\nabla_W g(W^t, z_N^t)\|$ exceeds 100, despite some fluctuations. Under these conditions, the term $\nabla_W g(W, z_N^t)$ can be safely neglected, thereby eliminating the need for the expensive loop 
\begin{wrapfigure}{r}{0.43\textwidth}
    \vspace{-0.2cm}
     \centering
\includegraphics[width=0.43\textwidth]{figs/gradient_norm_comparison_plot2.pdf}
    % \caption{}
    % \label{fig:first}
    \vspace{-0.6cm}
    
    \caption{Gradient norm values during the training process on the Cityscapes dataset. 
% Gradient norm $\|\nabla_Wg(W^t,z_{50}^t)\|$ is much smaller than $\|\nabla_Wg(W^t,x^t)\|$ and
Similar phenomena have also been observed in other datasets.} \label{fig:gradientnorm}
    % \vspace{-0.8cm}
\end{wrapfigure}
in \Cref{eq:zupdate}. This approximation has been effectively utilized in large-scale applications, such as fine-tuning large language models, to reduce memory and computational costs \citep{shen2024seal}. It also serves as a foundation for our proposed algorithm, Bilevel Loss Balancing for Multi-Task Learning (BiLB4MTL), described in \Cref{algorithm2}. BiLB4MTL employs a fully single-loop structure, requiring only a single gradient computation for both variables per epoch, resulting in an $\mathcal{O}(1)$ time and memory cost. 



 
\begin{algorithm}[t]
\caption{BiLB4MTL}   
\begin{algorithmic}[1]\label{algorithm2}
\STATE{\textbf{Initialize:} $W^0, x^0$}
\FOR{$t=0,1,...,T-1$}
\STATE{$x^{t+1}=x^t-\alpha(\nabla_xf(W^t,x^t)+\lambda\nabla_xg(W^t,x^t))$.}
\STATE{$W^{t+1}=W^t-\alpha(\nabla_Wf(W^t,x^t)+\lambda\nabla_Wg(W^t,x^t))$}
\ENDFOR
\end{algorithmic}
\end{algorithm}

In \Cref{sec:theory}, we show that our BiLB4MTL method attains both an $\epsilon$-accurate stationary point for the bilevel problem in \cref{eq:object1} and an $\epsilon$-accurate Pareto stationary point for original loss functions under mild conditions.


\section{Empirical Results}
In this section, we conduct extensive practical experiments under multi-task classification, regression, and mixed settings to demonstrate the effectiveness of our method. Full experimental details can be found in \Cref{app:exp}.


\begin{table*}[t]
\caption{Results on Cityscapes (2-task) dataset. Each experiment is repeated 3 times with different random seeds and the average is reported. The best results are highlighted in \textbf{bold}, while the second-best results are indicated with \underline{underlines}.}
\label{tab:cityscapes}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{adjustbox}{max width=\textwidth}
  \begin{tabular}{llllll}
    \toprule
    \multirow{2}*{Method} & \multicolumn{2}{c}{Segmentation} & \multicolumn{2}{c}{Depth} &
    \multirow{2}*{$\Delta m\%\downarrow$} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
    % & \multicolumn{2}{c}{(Higher Better)} & \multicolumn{2}{c}{(Lower Better)} &  \\
    & mIoU $\uparrow$ & Pix Acc $\uparrow$ & Abs Err $\downarrow$ & Rel Err $\downarrow$ & \\
    \midrule
    STL & 74.01 & 93.16 & 0.0125 & 27.77 & \\
    \midrule
    LS & 75.18 & 93.49 & 0.0155 & 46.77 & 22.60  \\
    SI & 70.95 & 91.73 & 0.0161 & 33.83 & 14.11 \\
    RLW & 74.57 & 93.41 & 0.0158 & 47.79 & 24.38 \\
    DWA & 75.24 & 93.52 & 0.0160 & 44.37 & 21.45 \\
    UW & 72.02 & 92.85 & 0.0140 & 30.13 & 5.89 \\
    FAMO & 74.54 & 93.29 & 0.0145 & 32.59 & 8.13 \\
    GO4Align & 72.63 & 93.03 & 0.0164 & \underline{27.58} & 8.11\\
    \midrule
    MGDA & 68.84 & 91.54 & 0.0309 & 33.50 & 44.14  \\
    PCGrad & 75.13 & 93.48 & 0.0154 & 42.07 & 18.29  \\
    GradDrop & 75.27 & 93.53 & 0.0157 & 47.54 & 23.73  \\
    CAGrad & 75.16 & 93.48 & 0.0141 & 37.60 & 11.64  \\
    IMTL-G & 75.33 & 93.49 & 0.0135 & 38.41 & 11.10 \\
    MoCo & \underline{75.42} & 93.55 & 0.0149 & 34.19 & 9.90 \\
    Nash-MTL & 75.41 & \underline{93.66} & 0.0129 & 35.02 & 6.82 \\
    % SDMGrad & 74.53 & 93.52 & 0.0137 & 34.01 & 6.75 & 7.79 \\
    FairGrad & \textbf{75.72} & \textbf{93.68} & 0.0134 & 32.25 & {5.18} \\
    \midrule
    BiLB4MTL ({\scriptsize $\tau=\bf 1$}) & 74.53 & 93.42 & \underline{0.0128} & \textbf{26.79} & \textbf{-0.57}\\
    BiLB4MTL ({\scriptsize $\tau=\sigma$}) & 74.92 & 93.24 & \textbf{0.0127} & {29.80} & \underline{1.93}\\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\vspace{0.2cm}
\noindent\textbf{Baselines and evaluation.} To demonstrate the effectiveness of our proposed method, we evaluate its performance against a broad range of baseline approaches. The compared methods include scalarization-based algorithms, such as Linear Scalarization (LS), Scale-Invariant (SI), Random Loss Weighting (RLW) \citep{lin2021reasonable}, Dynamic Weight Average (DWA) \citep{liu2019end}, and Uncertainty Weighting (UW) \citep{kendall2018multi}, GO4Align \citep{shen2024go4align}. We also benchmark against gradient manipulation methods, including  Multi-Gradient Descent Algorithm (MGDA) \citep{desideri2012multiple}, PCGrad \citep{yu2020gradient}, GradDrop \citep{chen2020just}, CAGrad \citep{liu2021conflict}, IMTL-G \citep{liu2021towards}, Nash-MTL \citep{navon2022multi}, FAMO \citep{liu2024famo}, and FairGrad \citep{ban2024fair}. To provide a comprehensive evaluation, we report the performance of each individual task and employ one additional metric: $\bf\Delta m \%$ to quantify overall performance. The $\bf\Delta m \%$ metric measures the average relative performance drop of a multi-task model compared to its corresponding single-task baseline. Formally, it is defined as:
% \jky{you do not include all methods. missed some. Perhaps you copy and paste FairGrad?}
$$\Delta m \% = \frac{1}{K}\sum_{i=1}^K(-1)^{\delta_k}(M_{m,k}-M_{b,k})/M_{b,k} \times 100,$$
where $M_{m,k}$ and $M_{b,k}$ represent the performance of the $k$-th task for the multi-task model $m$ and single-task model $b$, respectively. The indicator $\delta_k=1$ if lower values indicate better performance and 0 otherwise. 

\subsection{Experiment setup}\label{ex:homotasks}
Here, we summarize our experimental setup, including the dataset, configuration, and hyperparameter tuning.


\vspace{0.2cm}
\noindent\textbf{Image-Level Classification. } CelebA \citep{liu2015deep}, one of the most widely used datasets, is a large-scale facial attribute dataset containing over 200K celebrity images. Each image is annotated with 40 attributes, such as the presence of eyeglasses and smiling. Following the experimental setup in \citealt{ban2024fair}, we treat CelebA as a 40-task multi-task learning (MTL) classification problem, where each task predicts the presence of a specific attribute. Since all tasks involve binary classification with the same \textit{binary cross-entropy} loss function, we apply identical normalization for both options of $\tau=\bf 1$ and $\tau=\sigma$ at the initial loss normalization stage.  
The network architecture consists of a 9-layer convolutional neural network (CNN) as the shared model, with multiple linear layers serving as task-specific heads. We train the model for 15 epochs using the Adam optimizer with a batch size of 256.

\vspace{0.2cm}
\noindent\textbf{Regression.} QM9 \citep{ramakrishnan2014quantum} dataset is another widely used benchmark for multi-task regression problems in quantum chemistry. It contains 130K molecules represented as graphs, and 11 properties to be predicted. Though all tasks share the same loss function, \textit{mean squared error}, they exhibit varying scales: a phenomenon commonly observed in regression tasks but less prevalent in classification tasks, as shown in \Cref{fig:qm9lossvalues}. To address this scale discrepancy, we adopt the logarithmic normalization 
in \Cref{sec:LN} at the initial loss normalization stage for both options of $\tau=\bf1$ and $\tau=\sigma$.  
Following the experimental setup in \citealt{liu2024famo,navon2022multi}, we use the same model and data split, 110K molecules for training, 10k for validation, and the rest 10k for testing. The model is trained 300 epochs with a batch size of 120. The learning rate starts at 1e-3  and is reduced whenever the validation performance stagnates for 5 consecutive epochs.

% \vspace{-0.3cm}
\begin{table}[t]
\caption{Results on CelebA (40-task) and NYU-v2 (3-task) datasets. Each experiment is repeated 3 times and the average is reported.
% N/A denotes unavailable access to the result.
The best results are highlighted in \textbf{bold}, while the second-best results are indicated with \underline{underlines}.}
\label{tab:celeba_qm9}
% \vskip 0.15in
\vspace{-0.2cm}
\begin{center}
% \begin{small}
\begin{sc}
\begin{adjustbox}{max width=0.6\textwidth}
\begin{tabular}{lccr}
\toprule
\multirow{2}*{Method} & \multicolumn{1}{c}{CelebA (40 tasks)} & \multicolumn{1}{c}{NYU-v2 (3 tasks)} \\
\cmidrule(lr){2-3}
 & $\Delta m\%\downarrow$ & $\Delta m\%\downarrow$ & \\
\midrule
LS & 4.15 & 5.59 \\
SI & 7.20 & 4.39 \\
RLW & 1.46 & 7.78 \\
DWA & 3.20 &  3.57 \\
UW & 3.23 & 4.05 \\
FAMO & 1.21 & -4.10 \\
GO4Align & 0.88  & \textbf{-6.08} \\
\midrule
MGDA & 14.85 & 1.38 \\
PCGrad & 3.17 & 3.97 \\
CAGrad & 2.48 & 0.20 \\
IMTL-G & 0.84 & -0.76 \\
Nash-MTL & 2.84  & -4.04 \\
FairGrad & 0.37 & \underline{-4.66} \\
\midrule
BiLB4MTL ({\scriptsize $\tau=\bf 1$}) & \underline{-1.07}  & -4.40 \\
BiLB4MTL ({\scriptsize $\tau=\sigma$}) & \textbf{-1.31} & -3.52 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{sc}
% \end{small}
\end{center}
% \vskip -0.1in
\vspace{-0.3cm}
\end{table}

\vspace{0.2cm}
\noindent\textbf{Dense Prediction.} 
% \jky{check what normalization you will finally use}
The Cityscapes dataset \citep{cordts2016cityscapes} consists of 5000 street-scene images designed for two tasks: 7-class semantic segmentation (a classification task) and depth estimation (a regression task). Similarly, the NYU-v2 dataset \citep{silberman2012indoor} is widely used for indoor scene understanding and contains 1449 densely annotated images. It includes one pixel-level classification task, semantic segmentation, and two pixel-level regression tasks, 13-class depth estimation plus surface normal prediction. These datasets provide benchmarks for evaluating the performance of our method in mixed multi-task settings. Since the number of tasks is small and the loss values exhibit minimal variation, we applied rescaled normalization when selecting $\tau=\sigma$ and identical normalization when selecting  $\tau=\bf 1$. We follow the same experimental setup described in \citealt{liu2021conflict, navon2022multi} and adopt MTAN \citep{liu2019end} as the backbone, which incorporates task-specific attention modules into SegNet \citep{badrinarayanan2017segnet}. Both models are trained for 200 epochs, with batch sizes of 8 for Cityscapes and 8 for NYU-v2. The learning rates are initialized at 3e-4 and 1e-4 for the first 100 epochs and reduced by half for the remaining epochs, respectively.
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/training_time_comparison_datasets_icml.pdf}
    \vspace{-1cm}
    \caption{Time scale comparison among well-performing approaches, with LS considered the reference method for standard time.}
    \label{fig:time}
    \vspace{-0.3cm}
\end{figure*}

\begin{table*}[ht]
\caption{Detailed results of  on QM9 (11-task) dataset. Each experiment is repeated 3 times and the average is reported. The best results are highlighted in \textbf{bold}, while the second-best results are indicated with \underline{underlines}.}
\label{tab:full_qm9}
%\vskip 0.15in
\vspace{-0.3cm}
\begin{center}
\begin{small}
\begin{sc}
\begin{adjustbox}{max width=\textwidth}
  \begin{tabular}{lllllllllllll}
    \toprule
    \multirow{2}*{Method} & $\mu$ & $\alpha$ & $\epsilon_{HOMO}$ & $\epsilon_{LUMO}$ & $\langle R^2\rangle$ & ZPVE & $U_0$ & $U$ & $H$ & $G$ & $c_v$  & \multirow{2}*{$\Delta m\%\downarrow$} \\
    \cmidrule(lr){2-12}
    %& & \multicolumn{11}{c}{MAE $\downarrow$} & & \\
    & \multicolumn{11}{c}{MAE $\downarrow$} & \\
    \midrule
    STL & 0.067 & 0.181 & 60.57 & 53.91 & 0.502 & 4.53 & 58.8 & 64.2 & 63.8 & 66.2 & 0.072 & \\
    \midrule
    LS & 0.106 & 0.325 & \textbf{73.57} & 89.67 & 5.19 & 14.06 & 143.4 & 144.2 & 144.6 & 140.3  & 0.128 & 177.6 \\
    SI & 0.309 & 0.345 & 149.8 & 135.7 & \underline{1.00} & \underline{4.50} & 55.3 & 55.75 & 55.82 & 55.27  & 0.112 & 77.8 \\
    RLW & 0.113 & 0.340 & 76.95 & 92.76 & 5.86 & 15.46 & 156.3 & 157.1 & 157.6 & 153.0  & 0.137 & 203.8 \\
    DWA & 0.107 & 0.325 & \underline{74.06} & 90.61 & 5.09 & 13.99 & 142.3 & 143.0 & 143.4 & 139.3  & 0.125 & 175.3 \\
    UW & 0.386 & 0.425 & 166.2 & 155.8 & 1.06 & 4.99 & 66.4 & 66.78 & 66.80 & 66.24  & 0.122 & 108.0 \\
    FAMO & 0.15 & 0.30 & 94.0 & 95.2 & 1.63 & 4.95 & 70.82 & 71.2 & 71.2 & 70.3 & 0.10 & 58.5 \\
    GO4Align & 0.17 & 0.35 & 102.4 & 119.0 & 1.22 & 4.94 & \underline{53.9} & \underline{54.3} & \underline{54.3} & \underline{53.9} & 0.11 & \underline{52.7} \\
    \midrule
    MGDA & 0.217 & 0.368 & 126.8 & 104.6 & 3.22 & 5.69 & 88.37 & 89.4 & 89.32 & 88.01  & 0.120 & 120.5 \\
    PCGrad & \underline{0.106} & 0.293 & 75.85 & 88.33 & 3.94 & 9.15 & 116.36 & 116.8 & 117.2 & 114.5  & 0.110 & 125.7 \\
    CAGrad & 0.118 & 0.321 & 83.51 & 94.81 & 3.21 & 6.93 & 113.99 & 114.3 & 114.5 & 112.3  & 0.116 & 112.8 \\
    IMTL-G & 0.136 & 0.287 & 98.31 & 93.96 & 1.75 & 5.69 & 101.4 & 102.4 & 102.0 & 100.1  & 0.096 & 77.2 \\
    Nash-MTL & \textbf{0.102} & \textbf{0.248} & 82.95 & \textbf{81.89} & 2.42 & 5.38 & 74.5 & 75.02 & 75.10 & 74.16  & \textbf{0.093} & 62.0 \\
    FairGrad & 0.117 & \underline{0.253} & 87.57 & \underline{84.00} & 2.15 & 5.07 & 70.89 & 71.17 & 71.21 & 70.88 & \underline{0.095} & 57.9 \\
    \midrule
    BiLB4MTL ({\scriptsize $\tau=\bf 1$}) & 0.341 & 0.405 & 161.49 & 140.06 & 1.65 & 5.04 & 75.31 & 74.82 & 75.66 & 75.82 & 0.125 & 113.6 \\
    BiLB4MTL ({\scriptsize $\tau=\sigma$}) & 0.23 & 0.29 & 123.89 & 111.95 & \textbf{0.97} & \textbf{3.99} & \textbf{42.73} & \textbf{43.1} & \textbf{43.2} & \textbf{43.1} & 0.097 & \textbf{49.5} \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{sc}
\end{small}
\end{center}
% \vskip -0.1in
\vspace{-0.3cm}
\end{table*}


% \vspace{0.2cm}
\noindent\textbf{Hyperparameter tuning. } In our method, hyperparameters include the step size $\alpha$ and the penalty constant $\lambda$. For the step size, we adopt the settings from prior experiments without extensive tuning. While we use the same step size for updates to both $W$ and $x$ in our implementation, these can be adjusted independently in practice. For $\lambda$, we determine the optimal value through a grid search. Starting with a coarse range $\lambda\in[0.01, 0.1, 1, 2, 5, 10]$, we evaluate performance and determine which value is better. Then we narrow down the search space and continue to execute a finer grid search with a
step size of $0.1$ or $0.02$ until we determine an appropriate value.


\subsection{Experimental results}
Results on the four benchmark datasets are provided in \Cref{tab:cityscapes}, \Cref{tab:celeba_qm9}, and \Cref{tab:full_qm9}. We observe that BiLB4MTL outperforms existing methods on both the CelebA and QM9 datasets, achieving the lowest performance drops of $\Delta m\% = -1.31$ and $\Delta m\% = 49.5$, respectively. Detailed results for the QM9 dataset illustrate that it achieves a balanced performance across all tasks. Besides, the BiLB4MTL({\scriptsize$\tau=\bf 1$}) does not effectively minimize task disparities due to large loss scale differences. These results highlight the effectiveness of our method in handling a large number of tasks in both classification and regression settings. 
Meanwhile, it achieves the lowest performance drop, with $\Delta m\%=-0.57$ on the Cityscapes dataset, while delivering comparable results on the NYU-v2 dataset, where the detailed results are shown in \Cref{tab:nyuv2} in the Appendix. These findings highlight the capability of BiLB4MTL to effectively handle mixed multi-task learning scenarios.

Additionally, we conducted an ablation study to evaluate the impact of different normalization strategies. The results, presented in \Cref{tab:rescale_ablation} in the Appendix, demonstrate the effectiveness of our proposed normalization methods.



\subsection{MTL efficiency}
Finally, we compare the running times of well-performing approaches in \Cref{fig:time}. Notably, our BiLB4MTL introduces negligible overhead compared with LS with at most a $1.11\times$ increase, aligning with other $\mathcal{O}(1)$ methods such as GO4Align and FAMO. In contrast, gradient manipulation methods, taking $\mathcal{O}(K)$ computational cost, become significantly slower in many-task scenarios. For example, Nash-MTL requires approximately $12\times$ times more than BiLB4MTL on the CelebA dataset.



\section{Theoretical Analysis}\label{sec:theory}
In this section, we provide convergence analysis for our BiLB4MTL method. 
% demonstrate the connection between the Pareto stationarity and the stationary points achieved by our \Cref{algorithm2}. 
We first provide several useful definitions and assumptions. 
\begin{definition}
Given $L>0$, a function $\ell$ is said to be $L$-Lipschitz-continuous on $\mathcal{X}$ if it holds for any $x,x^\prime\in\mathcal{X}$ that $\|\ell(x)-\ell(x^\prime)\|\leq L\|x-x^\prime\|$.  A function $\ell$ is said to be $L$-Lipschitz-smooth
if its gradient is $L$-Lipschitz-continuous.
\end{definition}
\begin{definition}[Pareto stationarity]
% \jky{Pareto stationarity? check}
We say $x$ is an $\epsilon$-accurate Pareto stationary point for loss functions $\{l_i(x)\}$ if $\min_{w\in\mathcal{W}}\|G(x)w\|^2=\mathcal{O}(\epsilon)$, where $G(x)=[\nabla l_1(x),\nabla l_2(x),...,\nabla l_K(x)]$.
\end{definition}
Inspired by \citealt{shen2023penalty}, we define the following two surrogates of the original bilevel problem in \cref{eq:object1}, as shown below.
% make the above definition since we focus on the non-convex setting.
\begin{definition}
Define two surrogate bilevel problems as 
% $\mathcal{BP}_{\lambda}$, $\mathcal{BP}_\epsilon$ that are defined as
\begin{align}
\mathcal{BP}_{\lambda}:&\min_{W,x} f(W,x)+\lambda (g(W,x)-g(W,x^*)),\nonumber\\
% ,\nonumber\\
\mathcal{BP}_\epsilon:&\min_{W,x}f(W,x)\;\;\text{s.t. }g(W,x)-g(W,x^*)\leq\epsilon,
\end{align}
where $\mathcal{BP}_\lambda$ is the penalized bilevel problem, and $\mathcal{BP}_\epsilon$ recovers to the original bilevel problem if $\epsilon=0$. 
\end{definition}

\begin{assumption}[Lipschitz and smoothness]\label{ass:lipschitz}
There exists a constant $L$ such that the upper-level function $f(W,\cdot)$ is $L$-Lipschitz continuous. There exists constants $L_f$ and $L_g$ such that functions $f(W,x)$ and $g(W,x)$ are $L_f$- and $L_g$-Lipschitz-smooth.
% \jky{you do not define Lipschitz and smooth here; I suggest you define it in a Definition}
\end{assumption}
% \begin{assumption}[Smoothness]\label{ass:smooth}

% \end{assumption}
\begin{assumption}[Polyak-Lojasiewicz (PL) condition]\label{ass:pl}
The lower-level function $g(W,\cdot)$ satisfies the $\frac{1}{\mu}$-PL condition such that given any $W$, the following inequality holds for any feasible $x$.
\begin{align*}
    \|\nabla g(W,x)\|^2\geq\frac{1}{\mu}(g(W,x)-g(W,x^*)).
\end{align*}
\end{assumption}
Lipschitz continuity and smoothness are standard assumptions in the study of bilevel optimization \citep{ghadimi2018approximation,ji2021bilevel}. While the absolute values in the upper-level function in \Cref{eq:object1} are non-smooth, they can be easily modified to ensure smoothness, such as by using a soft absolute value function of the form  $y=\sqrt{x^2+\epsilon}$ where $\epsilon$ is a small positive constant.
% \jky{what is $\epsilon$? what distributions of it?}.
Moreover, the PL condition can be satisfied in over-parameterized neural network settings \citep{mei2020global, frei2021proxy}. The following theorem presents the convergence analysis of our algorithms.


\begin{theorem}\label{theorem:2}
Suppose \Cref{ass:lipschitz}-\Cref{ass:pl} are satisfied. Select hyperparameters 
\begin{align}
&\alpha\in\Big(0,\frac{1}{L_f+\lambda(2L_g+L_g^2\mu)}\Big],\; \beta\in(0,\frac{1}{L_g}],\nonumber\\
&\lambda= L\sqrt{3\mu{\epsilon}^{-1}},\text{ and } N=\Omega(\log(\alpha t)).\nonumber
\end{align}

(i) Our method with the updates \cref{WgUpdates} and \cref{eq:zupdate} (i.e., \Cref{algorithm1} in the appendix) finds an $\epsilon$-accurate stationary point of the problem $\mathcal{BP}_\lambda$. If this stationary point is a local/global solution to $\mathcal{BP}_\lambda$, it is also a local/global solution to $\mathcal{BP}_\epsilon$. Furthermore, it is also an $\epsilon$-accurate Pareto 
stationary point for loss functions $l_i(x),i=1,...,K.$
% The double-loop \Cref{algorithm1} solving $\mathcal{BP}_\lambda$ achieves an $\epsilon$-accurate Pareto stationary point.

(ii) Moreover, if $\|\nabla_Wg(W^t,z_N^t)\|=\mathcal{O}(\epsilon)$ for $t=1,...,T$. The simplified method in \Cref{algorithm2} also achieves the same convergence guarantee as that in $(i)$ with $\mathcal{O}(\epsilon^{-2})$ iterations.

% an $\epsilon$-accurate stationary point of the problem $\mathcal{BP}_\lambda$ and an $\epsilon$-accurate Pareto 
% stationary point for $l_i(x),i=1,...,K.$

% Pareto stationary point with $\mathcal{O}(\epsilon^{-2})$ iterations.
\end{theorem}


The complete proof is provided in \Cref{theorem:fullversionof2}.
In the first part of our theorem, we establish a connection between the stationarity of $\mathcal{BP}_\lambda$ and Pareto stationarity, as well as an equivalence between the Pareto stationarities of the original loss functions $\{l_i\}$ and the normalized loss functions $\{\tilde l_i\}$. 
The second part of \Cref{theorem:2} introduces an additional gradient vanishing assumption, which has been validated in our experiments. It demonstrates that our simplified BiLB4MTL method can also attain an $\epsilon$-accurate stationary point for the problem $\mathcal{BP}_\lambda$ and an $\epsilon$-accurate Pareto stationary point for the original loss functions. 


\section{Conclusion}
We introduced BiLB4MTL, a scalable loss balancing approach for multi-task learning based on bilevel optimization. Our method achieves efficient loss balancing with only 
$\mathcal{O}(1)$ time and memory complexity while guaranteeing convergence to both a stationary point of the bilevel problem and an $\epsilon$-accurate Pareto stationary point for all task loss functions. Extensive experiments demonstrate that BiLB4MTL outperforms existing methods in both accuracy and efficiency, highlighting its effectiveness for large-scale MTL.

For future work, we plan to explore the application of our method to broader multi-task learning problems, including recommendation systems.


\section*{Impact Statement}
This paper presents work to advance the field of multi-task Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.


\bibliography{example_paper}
\bibliographystyle{ref_style}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % APPENDIX
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Experiment setup}\label{app:exp}
\subsection{Toy example}\label{app:toy}
To better understand the benefits of our method, we illustrate the training trajectory along with the training time in a toy example of 2-task learning following the same setting in FAMO \citep{liu2024famo}. The loss functions $L_1(x),L_2(x)$, where $x$ is the model parameter, of two tasks are listed below.
\begin{align}
L_1(x)&=0.1\times(c_1(x)f_1(x)+c_2(x)g_1(x)),\;\;L_2(x)=c_1(x)f_2(x)+c_2(x)g_2(x)\;\;\text{where}\nonumber\\
f_1(x) &= \log{\big(\max(|0.5(-x_1-7)-\tanh{(-x_2)}|,~~0.000005)\big)} + 6, \nonumber\\
    f_2(x) &= \log{\big(\max(|0.5(-x_1+3)-\tanh{(-x_2)}+2|,~~0.000005)\big)} + 6, \nonumber\\
    g_1(x) &= \big((-x_1+7)^2 + 0.1*(-x_2-8)^2\big)/10-20, \nonumber\\
    g_2(x) &= \big((-x_1-7)^2 + 0.1*(-x_2-8)^2)\big/10-20, \nonumber\\
    c_1(x) &= \max(\tanh{(0.5*x_2)},~0)~~\text{and}~~c_2(x) = \max(\tanh{(-0.5*x_2)},~0).
\end{align}
In \Cref{fig:toy}, the black dots represent 5 chosen initial points $\{(-8.5,7.5),(-8.5,5),(0,0),(9,9),(10,-8)\}$ while the black stars represent the converging points on the Pareto front. We use the Adam optimizer and train each method for 50k steps.
Our method can always converge to balanced results efficiently. We use Adam optimizer with a learning rate of 1e-3. The training time is recalculated according to real-time ratios in our machine. We find that LS and MGDA do not converge to balanced points while FAMO converges to balanced results to some extent. Meanwhile, our method with rescale normalization can always converge to balanced results efficiently.

\begin{table*}[t]
\caption{Results on NYU-v2 (3-task) dataset. Each experiment is repeated 3 times with different random seeds and the average is reported.}
\label{tab:nyuv2}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{adjustbox}{max width=\textwidth}
  \begin{tabular}{lllllllllll}
    \toprule
    \multirow{3}*{Method} & \multicolumn{2}{c}{Segmentation} & \multicolumn{2}{c}{Depth} & \multicolumn{5}{c}{Surface Normal} & \multirow{3}*{$\Delta m\%\downarrow$} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-10}
    & \multirow{2}*{mIoU $\uparrow$} & \multirow{2}*{Pix Acc $\uparrow$} & \multirow{2}*{Abs Err $\downarrow$} & \multirow{2}*{Rel Err $\downarrow$} & \multicolumn{2}{c}{Angle Distance $\downarrow$} & \multicolumn{3}{c}{Within $t^\circ$ $\uparrow$} & \\
    \cmidrule(lr){6-7}\cmidrule(lr){8-10}
    & & & & & Mean & Median & 11.25 & 22.5 & 30 & \\
    \midrule
    STL & 38.30 & 63.76 & 0.6754 & 0.2780 & 25.01 & 19.21 & 30.14 & 57.20 & 69.15 & \\
    \midrule
    LS & 39.29 & 65.33 & 0.5493 & 0.2263 & 28.15 & 23.96 & 22.09 & 47.50 & 61.08 & 5.59  \\
    SI & 38.45 & 64.27 & 0.5354 & 0.2201 & 27.60 & 23.37 & 22.53 & 48.57 & 62.32 & 4.39 \\
    RLW & 37.17 & 63.77 & 0.5759 & 0.2410 & 28.27 & 24.18 & 22.26 & 47.05 & 60.62 & 7.78 \\
    DWA & 39.11 & 65.31 & 0.5510 & 0.2285 & 27.61 & 23.18 & 24.17 & 50.18 & 62.39 & 3.57 \\
    UW & 36.87 & 63.17 & 0.5446 & 0.2260 & 27.04 & 22.61 & 23.54 & 49.05 & 63.65 & 4.05 \\
    FAMO & 38.88 & 64.90 & 0.5474 & 0.2194 & 25.06 & 19.57 & 29.21 & 56.61 & 68.98 & -4.10 \\
    GO4Align  & 40.42 & 65.37 & 0.5492 & 0.2167 & 24.76 & 18.94 & 30.54 & 57.87 & 69.84 & -6.08\\
    \midrule
    MGDA & 30.47 & 59.90 & 0.6070 & 0.2555 & 24.88 & 19.45 & 29.18 & 56.88 & 69.36 & 1.38 \\
    PCGrad & 38.06 & 64.64 & 0.5550 & 0.2325 & 27.41 & 22.80 & 23.86 & 49.83 & 63.14 & 3.97 \\
    GradDrop & 39.39 & 65.12 & 0.5455 & 0.2279 & 27.48 & 22.96 & 23.38 & 49.44 & 62.87  & 3.58 \\
    CAGrad & 39.79 & 65.49 & 0.5486 & 0.2250 & 26.31 & 21.58 & 25.61 & 52.36 & 65.58 & 0.20 \\
    IMTL-G & 39.35 & 65.60 & 0.5426 & 0.2256 & 26.02 & 21.19 & 26.20 & 53.13 & 66.24 & -0.76 \\
    MoCo & 40.30 & 66.07 & 0.5575 & 0.2135 & 26.67 & 21.83 & 25.61 & 51.78 & 64.85 & 0.16 \\
    % MoDo & 35.28 & 62.62 & 0.5821 & 0.2405 & 25.65 & 20.33 & 28.04 & 54.86 & 67.37 & 9.89 & 0.49 \\
    Nash-MTL & 40.13 & 65.93 & 0.5261 & 0.2171 & 25.26 & 20.08 & 28.40 & 55.47 & 68.15 & -4.04 \\
    % SDMGrad & \textbf{40.47} & 65.90 & \textbf{0.5225} & \textbf{0.2084} & 25.07 & 19.99 & 28.54 & 55.74 & 68.53 & \textbf{2.89} & -4.84 \\
    FairGrad & 39.74 & 66.01 & 0.5377 & 0.2236 & 24.84 & 19.60 & 29.26 & 56.58 & 69.16 & -4.66 \\
    \midrule
     BiLB4MTL({\scriptsize $\tau=\bf 1$}) & 38.04 & 38.04 & 0.5402 & 0.2278 & 24.70 & 19.19 & 29.97 & 57.44 & 69.69 & -4.40  \\
    BiLB4MTL({\scriptsize $\tau=\sigma$}) & 40.94 & 66.21 & 0.5316 & 0.2210 & 25.33 & 20.35 & 27.79 & 54.95 & 67.82 & -3.52  \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Ablation study on normalization alternatives}
We conducted an ablation study on normalization strategies for the QM9 dataset, where the $\Delta m\%$ metric was computed and presented in \Cref{tab:rescale_ablation}. The best results were achieved by using a weighted sum of log-normalized losses as the lower-level objective and a weighted sum of log-normalized loss gaps as the upper-level objective. This approach aligns with our formulation in \Cref{eq:object1}, incorporating logarithmic normalization of loss values. In \Cref{tab:rescale_ablation}, weighted indicates the inclusion of $\sigma(W)$, log refers to the application of the logarithmic operator, and normalized signifies the division by the initial loss values.
\begin{table}[H]
\centering
\caption{Ablation study on normalization alternatives. }
\label{tab:rescale_ablation}
\begin{tabular}{lcc}
\toprule
\diagbox{Upper-Level}{Lower-Level} & Weighted normalized loss & Weighted log-normalized loss \\
\midrule
Weighted log-normalized loss gaps    & 150.3                  & 49.5                      \\
Log-normalized loss gaps             & 103.9                  & 113.6                     \\
Weighted normalized loss gaps        & 197.3                  & 94.5                        \\
Normalized loss gaps                 & 147.5                  & 157.6                        \\
Weighted Loss gaps                 & 125.8                 & 72.6                         \\
Loss gaps                          & 172.3                & 76.7                       \\
\bottomrule
\end{tabular}
\end{table}
\section{Additional information}
Here, we present the complete version of the double-loop algorithm for solving the penalized bilevel problem, $\mathcal{BP}_\lambda$ in \Cref{eq:penalty}, as detailed in \Cref{algorithm1}. Notably, the local or global solution of $\mathcal{BP}_\lambda$ obtained by \Cref{algorithm1} also serves as a local or global solution to $\mathcal{BP}_\epsilon$, as established by Proposition 2 in \citealt{shen2023penalty}.
\begin{algorithm}[t]
\caption{Double-loop First-order method}   
\begin{algorithmic}[1]\label{algorithm1}
\STATE{\textbf{Initialize:} $W^0, x^0, z_0^0$}
\FOR{$t=0,1,...,T-1$}
\STATE{warm start $z_0^t=x^t$}
\FOR{$n=0,1,...,N$}
\STATE{$z_{n+1}^t=z_n^t-\beta \lambda\nabla_zg(W^t,z_n^t)$}
\ENDFOR
\STATE{$x^{t+1}=x^t-\alpha(\nabla_xf(W^t, x^t)+\lambda\nabla_xg(W^t,x^t))$}
\STATE{$W^{t+1}=W^t-\alpha(\nabla_Wf(W^t,x^t)+\lambda(\nabla_Wg(W^t,x^t)-\nabla_Wg(W^t,z_N^t)))$}
% \STATE{Compute the loss $\mathcal{L}(W^t,x^t,z_N^t)$ in \Cref{eq:object3}}
% \STATE{Optimizer update both $W^t$ and $x^t$ once}
\ENDFOR
\end{algorithmic}
\end{algorithm}
\section{Analysis}
In the analysis, we need the following definitions.
\begin{align}
    x_t^*=&\arg\min_{x}g(W^t,x), \; G(x)=[\nabla l_1(x),\nabla l_2(x),...,\nabla l_K(x) ], \widetilde{G}(x)=[\nabla \tilde{l}_1(x),\nabla \tilde{l}_2(x),...,\nabla \tilde{l}_K(x) ]\nonumber\\
    F(\theta^t)=&f(\theta^t)+\lambda p(\theta^t),    \Phi(\theta^t)=f(\theta^t)+\lambda g(\theta^t),\;\text{where}\;\theta^t=(W^t,x^t), p(\theta^t)=g(W^t,x^t)-g(W^t,x_t^*)\nonumber\\
    \nabla f(W,x)=&(\nabla_Wf(W,x),\nabla_xf(W,x)), \nabla g(W,x)=(\nabla_Wg(W,x),\nabla_xg(W,x)).
\end{align}
\begin{lemma}\label{lemma:fullversionofpareto}
Let $(W,x)$ be a solution to the $\mathcal{BP}_\epsilon$. This point is also an $\epsilon$-accurate Pareto stationarity point for $\{l_i(x)\}$ satisfying  $$\min_{w\in\mathcal{W}}\|G(x)w\|^2=\mathcal{O}(\epsilon).$$
\end{lemma}
\begin{proof}
According to the definition of $\mathcal{BP}_\epsilon$, its solution $(W,x)$ satisfies that
\begin{align}\label{eq:gepsilon}
g(W,x)-g(W,x^*)\leq\epsilon.
\end{align}
Further, according to \Cref{ass:lipschitz}, we can obtain
\begin{align*}
    g(W,x)\geq g(W,x^*)+\nabla_xg(W,x^*)(x-x^*)+\frac{1}{2L_g}\|\nabla_xg(W,x)-\nabla_xg(W,x^*)\|^2.
\end{align*}
Since $x^*\in\arg\min_xg(W,x)$ and $g(W,x)=\sum_{i=1}^K\sigma_i(W)\tilde{l}_i(x)$, we have $\nabla_xg(W,x^*)=0$ and $\nabla_xg(W,x)=\sum_{i=1}^K\sigma_i(W)\nabla _x\tilde{l}_i(x)=\widetilde{G}(x)\sigma(W)$ . We can obtain,
\begin{align}\label{eq:normalizedPareto}
\|\widetilde{G}(x)\sigma(W)\|^2\leq2L_g(g(W,x)-g(W,x^*))=\mathcal{O}(\epsilon),
\end{align}
where the last inequality follows from \Cref{eq:gepsilon}.  Furthermore, since we have used softmax at the last layer of our neural network, $\sigma(W)$ belongs to the probability simplex $\mathcal{W}$. Thus we can derive 
$$\min_{w\in\mathcal{W}}\|\widetilde{G}(x)w\|^2\leq\|\widetilde{G}(x)\sigma(W)\|^2=\mathcal{O}(\epsilon).$$
Thus, the solution $(W,x)$ to the $\mathcal{BP}_\epsilon$ also satisfies Pareto stationarity of the normalized loss functions $\{\tilde{l}_i\}$. Then we show the equivalence between the Pareto stationarities for the original
loss functions $\{l_i\}$ and the normalized loss functions $\{\tilde{l}_i\}$. First, for the identical normalization, the above \cref{eq:normalizedPareto} naturally recovers
\begin{align*}
\min_{w\in\mathcal{W}}\|{G}(x)w\|^2=\min_{w\in\mathcal{W}}\|\widetilde{G}(x)w\|^2\leq\|\widetilde{G}(x)\sigma(W)\|^2=\mathcal{O}(\epsilon).
\end{align*}
Then, for the rescaled normalization, $\tilde{l}_i(x)=\frac{l_i(x)}{l_i^\prime}$ where $\forall i,l_i^\prime=\mathcal{O}(1)$. Thus, we can have
\begin{align*}
    \min_{w\in\mathcal{W}}\|G(x)w\|^2\leq\|G(x)\sigma(W)\|^2\leq L^2_{\text{max}}\|\widetilde{G}(x)\sigma(W)\|^2=\mathcal{O}(\epsilon),
\end{align*}
where $L_{\text{max}}=\max_il_i^\prime$. Finally, for the logarithmic normalization, $\tilde{l}_i(x)=\log\Big(\frac{l_i(x)}{l_{i,0}}\Big)=\lim_{\kappa\rightarrow1}\frac{\Big(\frac{l_i(x)}{l_{i,0}}\Big)^{1-\kappa}-1}{1-\kappa}$. Furthermore, according to the Proposition 6.1 in \citealt{ban2024fair}, the Pareto front of the $(\tilde{l}_1(x),\tilde{l}_2(x),...,\tilde{l}_K(x))$ which can be considered as $\kappa$-fair functions is the same as that of loss functions $\Big(\frac{l_1(x)}{l_{1,0}},\frac{l_2(x)}{l_{2,0}},...,\frac{l_K(x)}{l_{K,0}}\Big)$. Therefore, for an $\epsilon$-accurate Pareto stationarity point $x$ of the normalized loss functions, we can obtain
$$\min_{w\in\mathcal{W}}\|G^\prime(x)w\|^2=\mathcal{O}(\epsilon),$$
where $G^\prime(x)=\Big(\frac{\nabla l_1(x)}{l_{1,0}},\frac{\nabla l_2(x)}{l_{2,0}},...,\frac{\nabla l_K(x)}{l_{K,0}}\Big)$. Furthermore, we can obtain 
\begin{align*}
    \min_{w\in\mathcal{W}}\|G(x)w\|^2\leq\min_{w\in\mathcal{W}}(L^\prime_{\text{max}})^2\|G^\prime(x)w\|^2=\mathcal{O}(\epsilon),
\end{align*}
where $L^\prime_\text{max}=\max_il_{i,0}=\mathcal{O}(1)$. Then with our three normalization approaches, there is an equivalence between the Pareto stationarities of the original loss functions $\{l_i\}$ and the normalized loss functions $\{\tilde{l}_i\}$. The proof is complete.
\end{proof}
\begin{theorem}[Restatement of \Cref{theorem:2}]\label{theorem:fullversionof2}
Suppose \Cref{ass:lipschitz}-\Cref{ass:pl} are satisfied. Select hyperparameters 
\begin{align}
\alpha\in(0,\frac{1}{L_f+\lambda(2L_g+L_g^2\mu)}],\; \beta\in(0,\frac{1}{L_g}],\lambda= L\sqrt{3\mu{\epsilon}^{-1}},\text{ and } N=\Omega(\log(\alpha t)).\nonumber
\end{align}

(i) Our method with the updates \cref{WgUpdates} and \cref{eq:zupdate} (i.e., \Cref{algorithm1} in the appendix) finds an $\epsilon$-accurate stationary point of the problem $\mathcal{BP}_\lambda$. If this stationary point is a local/global solution to $\mathcal{BP}_\lambda$, it is also a local/global solution to $\mathcal{BP}_\epsilon$. Furthermore, it is also an $\epsilon$-accurate Pareto 
stationary point for loss functions $l_i(x),i=1,...,K.$


(ii) Moreover, if $\|\nabla_Wg(W^t,z_N^t)\|=\mathcal{O}(\epsilon)$ for $t=1,...,T$. The simplified method in \Cref{algorithm2} also achieves the same convergence guarantee as that in $(i)$ with $\mathcal{O}(\epsilon^{-2})$ iterations.
\end{theorem}
\begin{proof}
We start with the first half of our theorem. Directly from Theorem 3 in \citealt{shen2023penalty}, \Cref{algorithm1} achieves an $\epsilon$-accurate stationary point of $\mathcal{BP}_\lambda$ with $\widetilde{\mathcal{O}}(\epsilon^{-1.5})$ iterations such that
\begin{align*}
    \frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(W^t,x^t)+\lambda(\nabla g(W^t,x^t)-\nabla g(W^t,x_t^*))\|^2\leq\frac{F(W^0,x^0)}{\alpha T}+\frac{10L^2L_g^2}{T}=\mathcal{O}(\epsilon).
\end{align*}
Recall that $F(W^0,x^0)=f(W^0,x^0)+\lambda(g(W^0,x^0)-g(W^0,x_0^*))$. According to the Proposition 2 in \citealt{shen2023penalty} by setting $\delta=\epsilon$ therein, we can have $g(W^T,x^T)-g(W^T,x_T^*)\leq\epsilon$ if this stationary point is local/global solution to $\mathcal{BP}_\lambda$. Then by using \Cref{lemma:fullversionofpareto}, we know that this $\epsilon$-accurate stationary point is also an $\epsilon$-accurate Pareto stationary point of normalized functions $\{\tilde{l}_i(x)\}$ satisfying  
$$\min_{w\in\mathcal{W}}\|G(x^T)w\|^2=\mathcal{O}(\epsilon).$$
The proof of the first half of our theorem is complete.

Then for the second half, since we have built the connection between the stationarity of $\mathcal{BP}_\lambda$ and Pareto stationarity, we prove that the single-loop \Cref{algorithm2} achieves an $\epsilon$-accurate stationary point of $\mathcal{BP}_\lambda$. Recall that
\begin{align}\label{eq:hypergradient}
\|\nabla f(W^t,&x^t)+\lambda(\nabla g(W^t,x^t)-\nabla g(W^t,x_t^*))\|^2\nonumber\\
\overset{(i)}{\leq}&2\|\nabla f(W^t,x^t)+\lambda\nabla g(W^t,x^t)\|^2+2\lambda^2\|\nabla g(W^t,x_t^*)\|^2\nonumber\\
\overset{(ii)}{=}&2\|\nabla f(W^t,x^t)+\lambda\nabla g(W^t,x^t)\|^2+2\lambda^2\|\nabla_W g(W^t,x_t^*)\|^2\nonumber\\
\overset{(iii)}{\leq}&2\|\nabla f(W^t,x^t)+\lambda\nabla g(W^t,x^t)\|^2+4\lambda^2\|\nabla_W g(W^t,x^*_t)-\nabla_W g(W^t,z^t_N)\|^2+4\lambda^2\|\nabla_Wg(W^t,z^t_N)\|^2,
\end{align}
where $(i)$ and $(iii)$ both follow from Young's inequality, and $(ii)$ follows from $\nabla_xg(W^t,x_t^*)=0$. Besides, recall that $z_N^t$ is the intermediate output of the subloop in \Cref{algorithm1}. We next provide the upper bounds of the above three terms on the right-hand side (RHS). For the first term, we utilize the smoothness of $\Phi(\theta^t)= f(\theta^t)+\lambda  g(\theta^t)$ where $L_\Phi=L_f+\lambda L_g$ and $\theta^t=(W^t,x^t)$.
\begin{align*} \Phi(\theta^{t+1})\leq& \Phi(\theta^t)+\langle\nabla\Phi(\theta^t), \theta^{t+1}-\theta^t\rangle+\frac{L_\Phi}{2}\|\theta^{t+1}-\theta^t\|^2\nonumber\\
\overset{(i)}{\leq}&\Phi(\theta^t)-\frac{\alpha}{2}\|\nabla \Phi(\theta^t)\|^2,
\end{align*}
where $(i)$ follows from $\alpha\leq\frac{1}{L_\Phi}=\mathcal{O}(\lambda^{-1})$. Thus, we can obtain
\begin{align}\label{eq:hypergradient-1}
\|\nabla\Phi(\theta^t)\|^2\leq\frac{2}{\alpha}(\Phi(\theta^t)-\Phi(\theta^{t+1})).
\end{align}
Then for the second term on the RHS in \cref{eq:hypergradient}, we follow the same step in the proof of Theorem 3 in \citealt{shen2023penalty} and obtain
\begin{align}\label{eq:hypergradient-2}
4\lambda^2\|\nabla_W &g(W^t,x^*_t)-\nabla_Wg(W^t,z_N^t)\|^2\nonumber\\
\leq&4\lambda^2L_g^2\mu\Big(1-\frac{\beta}{2\mu}\Big)^N(g(W^t,x^t)-g(W^t,x^*_t))\nonumber\\
\overset{(i)}{\leq}&4\lambda^2L_g^2\Big(1-\frac{\beta}{2\mu}\Big)^N\|\nabla_xg(W^t,x^t)\|^2\nonumber\\
=&4\lambda^2L_g^2\Big(1-\frac{\beta}{2\mu}\Big)^N\Big\|\frac{x^{t+1}-x^t+\alpha\nabla_xf(W^t,x^t)}{\alpha \lambda}\Big\|^2\nonumber\\
\overset{(ii)}{\leq}&8\lambda^2L_g^2\Big(1-\frac{\beta}{2\mu}\Big)^N\Big(\frac{\|\theta^{t+1}-\theta^t\|^2}{\alpha^2\lambda^2}+\frac{L^2}{\lambda^2}\Big)\nonumber\\
\overset{(iii)}{\leq}&\frac{1}{2\alpha^2}\|\theta^{t+1}-\theta^t\|^2+\frac{2L^2L_g^2}{\alpha^2t^2}\nonumber\\
=&\frac{1}{2}\|\nabla\Phi(\theta^t)\|^2+\frac{2L^2L_g^2}{\alpha^2t^2},
\end{align}
where $(i)$ follows from the PL condition, $(ii)$ follows from Young's inequality and \Cref{ass:lipschitz}, and $(iii)$ follows from the selection on $N\geq\max\{-\log_{c_\beta}(16L_g^2),-2\log_{c_\beta}(2\alpha t)\}$ with $c_\beta=1-\frac{\beta}{2\mu}$. Lastly, for the last term at the RHS in \cref{eq:hypergradient}, we have,
\begin{align}\label{eq:hypergradient-3}
4\lambda^2\|\nabla_Wg(W^t,z^t_N)\|^2=\mathcal{O}(\lambda^2\epsilon^2),
\end{align}
where this inequality follows from our experimental observation. Furthermore, substituting \cref{eq:hypergradient-1}, and \cref{eq:hypergradient-2} into \cref{eq:hypergradient} yields
\begin{align}
\|\nabla f(W^t,x^t)&+\lambda(\nabla g(W^t,x^t)-\nabla g(W^t,x^*_t))\|^2\nonumber\\
\leq&\frac{5}{2}\|\nabla\Phi(\theta^t)\|^2+\frac{2L^2L_g^2}{\alpha^2t^2}+4\lambda^2\|\nabla_Wg(W^t,z^t_N)\|^2\nonumber\\
\leq&\frac{5}{\alpha}(\Phi(\theta^t)-\Phi(\theta^{t+1}))+\frac{2L^2L_g^2}{\alpha^2t^2}+4\lambda^2\|\nabla_Wg(W^t,z^t_N)\|^2.
\end{align}
Therefore, telescoping the above inequality yields,
\begin{align*}
\frac{1}{T}\sum_{t=0}^{T-1}&\|\nabla f(W^t,x^t)+\lambda(\nabla g(W^t,x^t)-\nabla g(W^t,x^*_t))\|^2\nonumber\\
% \leq&\frac{5\Phi(\theta^0)}{\alpha T}+\frac{2L^2L_g^2}{\alpha^2T}+4\lambda^2\kappa^2\nonumber\\
=&\mathcal{O}(\frac{\lambda}{\alpha T}+\frac{1}{\alpha^2 T}+\lambda^2\epsilon^2).
\end{align*}
According to the parameter selection that $\lambda=\mathcal{O}(\epsilon^{-\frac{1}{2}})$, $\alpha=\mathcal{O}(\epsilon^{\frac{1}{2}})$, and $T=\Omega(\epsilon^{-2})$, we can obtain
\begin{align*}
\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(W^t,x^t)+\lambda(\nabla g(W^t,x^t)-\nabla g(W^t,x^*_t))\|^2=\mathcal{O}(\epsilon).
\end{align*}
Therefore, \Cref{algorithm2} can achieve a stationary point of $\mathcal{BP}_\lambda$ with $\mathcal{O}(\epsilon^{-2})$ iterations. If this stationary point is a local/global solution to $\mathcal{BP}_\lambda$, it is also a solution to $\mathcal{BP}_\epsilon$ according to Proposition 2 in \citealt{shen2023penalty}. Then by using \Cref{lemma:fullversionofpareto}, we know this stationary point is also an $\epsilon$-accurate Pareto stationary point of the original loss functions. The proof is complete.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

