\section{Related Works}
\textbf{Multi-task learning.} MTL has recently garnered significant attention in practical applications. One line of research focuses on model architecture, specifically designing various sharing mechanisms \citep{kokkinos2017ubernet, ruder2019latent}. Another direction addresses the mismatch in loss magnitudes across tasks, proposing methods to balance them. For example, \citealt{kendall2018multi} balanced tasks by weighting loss functions based on homoscedastic uncertainties, while \citealt{liu2019end} dynamically adjusted weights by considering the rate of change in loss values for each task.

Besides, one prominent approach frames MTL as a Multi-Objective Optimization (MOO) problem. \citealt{sener2018multi} introduced this perspective in deep learning, inspiring methods based on the Multi-Gradient Descent Algorithm (MGDA) \citep{desideri2012multiple}. Subsequent work has aimed to address gradient conflicts. For instance, PCGrad \citep{yu2020gradient} resolves conflicts by projecting gradients onto the normal plane, GradDrop \citep{chen2020just} randomly drops conflicting gradients, and CAGrad \citep{liu2021conflict} constrains update directions to balance gradients. Additionally, Nash-MTL \citep{navon2022multi} formulates MTL as a bargaining game among tasks, while FairGrad \citep{ban2024fair} incorporates $\alpha$-fairness into gradient adjustments.

On the theoretical side, \citealt{zhou2022convergence} analyzed the convergence properties of stochastic MGDA, and \citealt{fernando2023mitigating} proposed a method to reduce bias in the stochastic MGDA with theoretical guarantees. More recent advancements include a double-sampling strategy with provable guarantees introduced by \citealt{xiao2024direction} and \citealt{chen2024three}, where the latter one analyzes stochastic MOO algorithms, addressing optimization, generalization, and conflict mitigation trade-offs. Furthermore, \citealt{zhang2024convergence} studied the convergence analysis of both deterministic and stochastic MGDA under a more relaxed generalized smoothness assumption.

\vspace{0.2cm}
\noindent\textbf{Bilevel optimization.} 
Bilevel optimization, first introduced by \citealt{bracken1973mathematical}, has been extensively studied over the past few decades. Early research primarily treated it as a constrained optimization problem \citep{hansen1992new, shi2005extended}. More recently, gradient-based methods have gained prominence due to their effectiveness in machine learning applications. Many of these approaches approximate the hypergradient using either linear systems \citep{domke2012generic, ji2021bilevel} or automatic differentiation techniques \citep{maclaurin2015gradient, franceschi2017forward}. However, these methods become impractical in large-scale settings due to their significant computational cost \citep{xiao2023communication, yang2024simfbo}. 
The primary challenge lies in the high cost of gradient computation: approximating the Hessian-inverse vector requires multiple first- and second-order gradient evaluations, and the nested sub-loops exacerbate this inefficiency. To address these limitations, recent studies have focused on reducing the computational burden of second-order gradients. For example, some methods reformulate the lower-level problem using value-function-based constraints and solve the corresponding Lagrangian formulation \citep{kwon2023fully, yang2024tuning}. The work studies
convex bilevel problems and proposes a zeroth-order optimization method with finite-time
convergence to the Goldstein stationary point \citep{chen2023bilevel}. In this work, we propose a simplified first-order bilevel method for MTL, motivated by intriguing empirical findings.