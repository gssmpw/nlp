\section{Related Works}
\textbf{Multi-task learning.} MTL has recently garnered significant attention in practical applications. One line of research focuses on model architecture, specifically designing various sharing mechanisms ____. Another direction addresses the mismatch in loss magnitudes across tasks, proposing methods to balance them. For example, ____ balanced tasks by weighting loss functions based on homoscedastic uncertainties, while ____ dynamically adjusted weights by considering the rate of change in loss values for each task.

Besides, one prominent approach frames MTL as a Multi-Objective Optimization (MOO) problem. ____ introduced this perspective in deep learning, inspiring methods based on the Multi-Gradient Descent Algorithm (MGDA) ____. Subsequent work has aimed to address gradient conflicts. For instance, PCGrad ____ resolves conflicts by projecting gradients onto the normal plane, GradDrop ____ randomly drops conflicting gradients, and CAGrad ____ constrains update directions to balance gradients. Additionally, Nash-MTL ____ formulates MTL as a bargaining game among tasks, while FairGrad ____ incorporates $\alpha$-fairness into gradient adjustments.

On the theoretical side, ____ analyzed the convergence properties of stochastic MGDA, and ____ proposed a method to reduce bias in the stochastic MGDA with theoretical guarantees. More recent advancements include a double-sampling strategy with provable guarantees introduced by ____ and ____, where the latter one analyzes stochastic MOO algorithms, addressing optimization, generalization, and conflict mitigation trade-offs. Furthermore, ____ studied the convergence analysis of both deterministic and stochastic MGDA under a more relaxed generalized smoothness assumption.

\vspace{0.2cm}
\noindent\textbf{Bilevel optimization.} 
Bilevel optimization, first introduced by ____, has been extensively studied over the past few decades. Early research primarily treated it as a constrained optimization problem ____. More recently, gradient-based methods have gained prominence due to their effectiveness in machine learning applications. Many of these approaches approximate the hypergradient using either linear systems ____ or automatic differentiation techniques ____. However, these methods become impractical in large-scale settings due to their significant computational cost ____. 
The primary challenge lies in the high cost of gradient computation: approximating the Hessian-inverse vector requires multiple first- and second-order gradient evaluations, and the nested sub-loops exacerbate this inefficiency. To address these limitations, recent studies have focused on reducing the computational burden of second-order gradients. For example, some methods reformulate the lower-level problem using value-function-based constraints and solve the corresponding Lagrangian formulation ____. The work studies
convex bilevel problems and proposes a zeroth-order optimization method with finite-time
convergence to the Goldstein stationary point ____. In this work, we propose a simplified first-order bilevel method for MTL, motivated by intriguing empirical findings.