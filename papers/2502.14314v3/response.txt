\section{Related works}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{2.pdf} % Replace with your actual figure file
\caption{Timeline of YOLO's development, reflecting the earliest release time of their code repository or pre-print.}
\label{fig:timeline}
\end{figure}

\subsection{YOLO: You Only Look Once model series }
YOLO (You Only Look Once) was first introduced by Joseph Redmon and Ali Farhadi et al., "You Only Look Once"**, marking a major advancement in real-time object detection by unifying the tasks of bounding box prediction and class probability estimation into a single-stage network. Unlike traditional two-stage detectors like R-CNN and Faster R-CNN, YOLO achieves rapid detection speeds by predicting both bounding boxes and class probabilities directly from full images in a single forward pass. A timeline of YOLO's development is shown in Figure~\ref{fig:timeline}, where the time reflects the earliest release time of their code repository or pre-print.

In 2016 and 2018, the original authors introduced YOLOv2 and YOLOv3, respectively, further enhancing YOLO model’s detection capability. YOLOv2, also known as YOLO9000, introduced anchor boxes for more precise detection and enabled real-time recognition across over 9000 classes by pre-training its Darknet-19 backbone on a general classification task. During training, it incorporated a multi-scale training approach, which allows the model to adapt to various input sizes, improving versatility. With k-means clustering, YOLOv2 optimized anchor box sizes, while multi-scale training enhances versatility across various input sizes. YOLOv3 improved accuracy by incorporating a deeper architecture (Darknet-53) and multi-scale predictions, which detects objects at three different scales to capture varying sizes more effectively, similar to the Feature Pyramid Networks (FPN).

In 2020, Alexey Bochkovskiy and his collaborators introduced YOLOv4, "YOLOv4"**, delivering substantial improvements in architecture and training techniques. YOLOv4 upgraded the backbone to CSPDarknet-53, incorporated Cross Stage Partial (CSP) connections, and added an FPN-PAN (Path Aggregation Network) feature that combines top-down and bottom-up feature fusion for enhanced multi-scale object detection. It also introduced CIoU (Complete Intersection over Union) loss, improving bounding box localization accuracy by factoring in center distance, aspect ratio, and overlap area.

Later in 2020, Ultralytics released YOLOv5, "YOLOv5"**, marking a new era for the YOLO family by introducing a flexible, PyTorch-based framework that emphasizes usability, modularity, and ease of deployment. Building on YOLOv4’s innovations, YOLOv5 featured multiple model sizes (YOLOv5s, YOLOv5m, YOLOv5l, YOLOv5x) to accommodate different computational needs. It also implemented Dynamic Label Assignment (DLA) to enhance training efficiency by dynamically selecting the best positive samples.

Between 2022 and 2024, researchers and Ultralytics introduced additional YOLO versions. In 2022, Meituan’s visual intelligence team released YOLOv6, featuring EfficientRep as its backbone and a decoupled head that separates classification and localization tasks to enhance precision. The same year, YOLOv7 developed by Chien-Yao Wang et al., "YOLOv7"**, introduced advanced E-ELAN (Extended Efficient Layer Aggregation Network) and Re-Parameterized Convolution (RepConvN) to strengthen the backbone and improve model performance.

In 2023, Ultralytics released their second YOLO repository, YOLOv8, "YOLOv8"**. Building on the foundation of YOLOv5, YOLOv8 introduced updates to the model architecture and added support for various tasks by incorporating additional heads for instance segmentation, pose keypoint detection, oriented bounding box (OBB) detection, and classification tasks. Moreover, it provided a unified PyTorch-based interface through the Ultralytics Python package, allowing users to more easily train, validate, and deploy the model with minimal configuration.

In 2024, YOLOv9 was introduced by Chien-Yao Wang and his collaborators, "YOLOv9"**, As the authors of YOLOv7, they designed YOLOv9 to integrate Programmable Gradient Information (PGI) and the Generalized Efficient Layer Aggregation Network (GELAN), where PGI was developed to overcome the information bottleneck problem and GELAN was developed by combining CSPNet and ELAN to improve architectural efficiency and performance.

Later in 2024, Ao Wang, Hui Chen, and their collaborators released YOLOv10, "YOLOv10"**, which uses a One-to-Many head to generate multiple predictions per object during training and a One-to-One head to generate a single best prediction per object during inference, whereas some previous YOLO versions usually achieve anchor-free detection by directly predicting object centers and sizes. However, we find YOLOv10 has been affected in terms of detection performance, where it performs comparatively poorly on detection precision, particularly for small objects (see Figure~\ref{fig:evaluation}).

Most recently, and also in 2024, Ultralytics released their third repository, YOLOv11, "YOLOv11"**, Building upon the impressive advancements of YOLOv8, YOLOv11 further improves the backbone using C3k2 (Cross Stage Partial with kernel size 2) blocks and C2PSA (Convolutional block with Parallel Spatial Attention) components. Similar to YOLOv8, YOLOv11 supports a range of tasks, including object detection, instance segmentation, pose estimation, and OBB detection, positioning it as one of the most versatile and capable object detectors to date.

\subsection{YOLO applications}
The YOLO series has become one of the most widely used methods for real-time object detection, with extensive applications in both academia and industry. Its versatility spans autonomous driving, remote sensing, robotics, surveillance, facial recognition, visual search engines, and numerous other domains. However, with the rapid emergence of new YOLO versions, many researchers face uncertainty when selecting the most suitable model for their specific tasks.

While newer YOLO versions continue to be released, studies have shown that models from YOLOv6 onward do not always outperform their predecessors in domain-specific applications. For example, in a study on wheat head counting, YOLOv7 outperformed YOLOv8, while in underwater pipeline detection, YOLOv5 achieved better results than YOLOv6, YOLOv7, and YOLOv8. Similarly, a study on hazards in knife handling found that YOLOv5 and YOLOv8 exhibited higher detection accuracy than YOLOv10, while YOLOv10 had the highest misclassification rate.

Notably, in 32 preprints and indexed papers we collected that compared YOLOv9 and YOLOv10, 26 papers reported that YOLOv9 outperformed YOLOv10, highlighting the uncertainty and domain-specific performance of YOLO upgrades in real-world applications. To address these inconsistencies, this paper presents a comprehensive benchmark ODverse33 to evaluate different YOLO models across multiple domains, providing clear guidance for model selection.