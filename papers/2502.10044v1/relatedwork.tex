\section{Related Work}
\label{sec:relate}
In this section, we review existing EA methods in terms of both supervised EA methods and unsupervised EA methods.

\subsection{Supervised Entity Alignment}
Most of the existing EA methods are supervised since they require seed alignment for supervision. According to their KG encoders, they can be divided into two categories~\cite{jiang2024toward, chen2022multi}. Trans-based methods~\cite{mtranse, jape, bootea, multike, jewp, jape, iptranse, transedge, neoea, mrpea, transedge} typically use translation-family KG encoders such as TransE~\cite{transe} to encode the structural information of KGs. GNN-based methods~\cite{gcn-align, mugnn, alinet, rdgcn, avrgcn, hgcn-je, rnm, rrea, tang2023weakly, liu2023dependency, li2023attribute, sun2022revisiting, hyperka, rea, epea, attrgnn, hman, hgcn-je-jr, gmnn, mugnn, nmn, ssp, alinet, mraea, naea, kecg, rrea, psr, largeea, roadea, cyctea, multiea,gala} use more advanced GNN-like encoders~\cite{gcn, gat}, usually in combination with relation-specific attention aggregation schemes~\cite{nathani2019learning, sheikh2021knowledge}to learn entity embeddings. After encoding, they perform alignment training by pulling seed alignment entity pairs closer while pushing the other entity pairs far away in the embedding space. There are also some methods that belong neither to Trans-based methods nor to GNN-based methods. For example, LightEA~\cite{lightea} achieves supervised alignment based on the label propagation algorithm. Although supervised EA methods have achieved remarkable alignment performance, they rely on manually provided high-quality supervised signals that are expensive to acquire in practice, hindering their widespread application in real-world scenarios.


\subsection{Unsupervised Entity Alignment}
Most existing unsupervised EA methods~\cite{eva, selfkg, uplr, dualmatch, iclea, emgcn} are GNN-based, and they usually utilize GCN~\cite{gcn} or GAT~\cite{gat} to extract the structural information of candidate KGs. Based on the extracted entity embeddings, they compute the similarity between all the possible entity pairs across candidate KGs. Then, they will select a set of the most similar entity pairs to form pseudo-labels to guide the alignment training. To improve the reliability of pseudo-labels, they often need to introduce various auxiliary information. For example, SelfKG~\cite{selfkg}, ICLEA~\cite{iclea}, and UPLR~\cite{uplr} introduce the text features of entity names, EVA~\cite{eva} introduces the image features associated with entities, and DualMatch~\cite{dualmatch} introduces the temporal information associated with relational facts. There is also a Trans-based unsupervised EA method, i.e., MultiKE~\cite{multike}, which leverages multiple views of entity features, such as names, relations, and attributes to enhance alignment. Despite showing preliminary progress in unsupervised EA, they still face the two limitations as we have analyzed in Section~\ref{sec:introduction}. In this work, we aim to develop a novel KG encoding technique and introduce an advanced mutual information-based regularization mechanism, to effectively address the two limitations and further boost the performance of the unsupervised EA task. 

There are also several unsupervised EA methods that belong neither to GNN-based methods nor to Trans-based methods. SEU~\cite{seu} and DATTI~\cite{datti} transform the EA problem into an assignment problem. ERAlign~\cite{eralign} jointly performs entity alignment and relation alignment by neighbor triple matching strategy, based on semantic textual features of entities and relations. FGWEA~\cite{fgwea} achieves the unsupervised EA task based on the Fused Gromov-Wasserstein Distance~\cite{fgw}. While effective, these methods often require complex optimization operations and sacrifice the ability of end-to-end learning.



\begin{figure*}[ht]
\centering
\includegraphics[width=2.0\columnwidth]{Figs/model.pdf}
\caption{The overall architecture of the proposed UNEA. The inputs include the triples, the entity embeddings, and the relation embeddings of two candidate KGs. The detail of each module is specifically described in Section~\ref{sec:method}.}
\label{fig:overview}
\end{figure*}