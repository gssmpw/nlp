\documentclass[11pt]{article}
\usepackage[top=2.54cm,left=2.54cm,right=2.54cm,bottom=2.54cm]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}           
\usepackage{booktabs}       
\usepackage{amsfonts}      
\usepackage{nicefrac}       
\usepackage{microtype}      
\usepackage{xcolor}    
\usepackage{amsmath, amsthm, amssymb}
\usepackage{bbm}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode} 
\usepackage{caption}  
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx} 
\usepackage{subfigure}


\newtheorem{definition}{{Definition}}[section]
\newtheorem{assumption}{{Assumption}}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{ {Corollary}}[section]
\newtheorem{prop}{\sc {Proposition}}[section]
\newtheorem{claim}{\sc {Claim}}[section]
\newtheorem{note}{\sc {Notation}}
\newtheorem{example}{\sc {Example}}
\newtheorem{prob}{\sc {Problem}}

\newcommand{\sah}{\mathcal{S}\times \mathcal{A}\times [H]}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\nref}{{\textnormal{ref}}}
\newcommand{\REF}{{\textnormal{REF}}}
\newcommand{\cn}{{\check{n}}}
\newcommand{\cl}{{\check{l}_i}}
\newcommand{\lcb}{{\textnormal{LCB}}}
\newcommand{\nr}{{\textnormal{R}}}
\newcommand{\adv}{{\textnormal{adv}}}
\def\dmin{\Delta_{\textnormal{min}}}
\def\qstar{\mathbb{Q}^\star}
\def\eadv{\hat{\mathbb{E}}_{h,k}^{\textnormal{adv}}}
\def\eref{\hat{\mathbb{E}}_{h,k}^{\textnormal{ref}}}
\def\padv{\mathbb{P}_{h,k}^{\textnormal{adv}}}
\def\pref{\mathbb{P}_{h,k}^{\textnormal{ref}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\tcb}[1]{{\color{blue}#1}}
\newcommand{\tcr}[1]{{\color{red}#1}}


\renewcommand{\baselinestretch}{1.3}

\begin{document}
\title{Gap-Dependent Bounds for Federated $Q$-Learning\footnote{Haochen Zhang and Zhong Zheng are co-first authors who contributed equally to this paper. Lingzhou Xue is the corresponding author (Email: lzxue@psu.edu). }}


\author{Haochen Zhang, Zhong Zheng and Lingzhou Xue
\\
Department of Statistics, The Pennsylvania State University
}
\date{}

\maketitle

\begin{abstract}
We present the first gap-dependent analysis of regret and communication cost for on-policy federated $Q$-Learning in tabular episodic finite-horizon Markov decision processes (MDPs). Existing FRL methods focus on worst-case scenarios, leading to $\sqrt{T}$-type regret bounds and communication cost bounds with a $\log T$ term scaling with the number of agents $M$, states $S$, and actions $A$, where $T$ is the average total number of steps per agent. In contrast, our novel framework leverages the benign structures of MDPs, such as a strictly positive suboptimality gap, to achieve a $\log T$-type regret bound and a refined communication cost bound that disentangles exploration and exploitation. Our gap-dependent regret bound reveals a distinct multi-agent speedup pattern, and our gap-dependent communication cost bound removes the dependence on $MSA$ from the $\log T$ term. Notably, our gap-dependent communication cost bound also yields a better global switching cost when $M=1$, removing $SA$ from the $\log T$ term.

\end{abstract}

\section{Introduction}
Federated reinforcement learning (FRL) is a distributed learning framework that combines the principles of reinforcement learning (RL) \citep{1998Reinforcement} and federated learning (FL) \citep{mcmahan2017communication}. Focusing on sequential decision-making, FRL aims to learn an optimal policy through parallel explorations by multiple agents under the coordination of a central server. Often modeled as a Markov decision process (MDP), multiple agents independently interact with an initially unknown environment and collaboratively train their decision-making models with limited information exchange between the agents. This approach accelerates the learning process with low communication costs. In this paper, we focus on the on-policy FRL tailored for episodic tabular MDPs with inhomogeneous transition kernels. Specifically, we assume the presence of a central server and $M$ local agents in the system. Each agent interacts independently with an episodic MDP consisting of $S$ states, $A$ actions, and $H$ steps per episode.

Multiple recent works studied the on-policy FRL for tabular MDPs. \citet{zheng2023federated} proposed model-free algorithms FedQ-Hoeffding and FedQ-Bernstein that show the regret bounds $\tilde{O}(\sqrt{MH^4SAT})$ and $\tilde{O}(\sqrt{MH^3SAT})$ respectively under $O(MH^3SA\log T)$ rounds of communications. Here, $T$ is the average total number of steps for each agent, and $\tilde{O}$ hides logarithmic factors. \citet{zheng2024federated} proposed FedQ-Advantage that improved the regret to $\tilde{O}(\sqrt{MH^2SAT})$ under a reduced communication rounds of $O(f_MH^2SA(\log H)\log T)$ where $f_M\in\{1,M\}$ reflects the optional forced synchronization scheme. \citet{chen2022sample} and \citet{labbi2024federated} proposed model-based algorithms that extend the single-agent algorithm UCBVI \citep{azar2017minimax}. Byzan-UCBVI \citep{chen2022sample} reaches regret $\tilde{O}(\sqrt{MH^3S^2AT})$ under $O(MHSA\log T)$ rounds of communications. Fed-UCBVI \citep{labbi2024federated} reaches the regret $\tilde{O}(\sqrt{MH^2SAT})$ under $O(HSA\log T + MHSA\log\log T)$ rounds of communications. Here, model-based methods require estimating the transition kernel so that their memory requirements scale quadratically with the number of states $S$. Model-free methods, which are also called $Q$-Learning methods \citep{watkins1989learning}, directly learn the action-value function, and their memory requirements only scale linearly with $S$. The regret $\tilde{O}(\sqrt{MH^2SAT})$ reached by both FedQ-Advantage and Fed-UCBVI is almost optimal compared to the regret lower bound $\tilde{O}(\sqrt{MH^2SAT})$ \citep{jin2018q,domingues2021episodic}. In summary, all the works above provided worst-case guarantees for all possible MDPs and proved $\sqrt{T}$-type regret bounds and communication cost bounds that linearly depend on $MSA\log T$ or $SA\log T$.



In practice, RL algorithms often perform better than their worst-case guarantees, as they can be significantly improved under MDPs with benign structures \citep{zanette2019tighter}. This motivates the problem-dependent analysis exploiting benign MDPs (see, e.g., \citet{wagenmaker2022first,zhou2023sharp,zhang2024settling}). One of the benign structures is based on the dependency on the positive suboptimality gap: for every state, the best actions outperform others by a margin. It is important because nearly all non-degenerate environments with finite action sets satisfy some sub-optimality gap conditions \citep{yang2021q}. For single-agent algorithms, \citet{simchowitz2019non,dann2021beyond} analyzed gap-dependent regret for model-based methods, and \citet{yang2021q,xu2021fine,zheng2024gap} analyzed model-free methods. Here, \citet{yang2021q} focused on UCB-Hoeffding proposed by \citet{jin2018q}, while \citet{xu2021fine} proposed an algorithm that did not use upper confidence bounds (UCB). \citet{zheng2024gap} analyzed UCB-Advantage \citep{zhang2020almost} and Q-EarlySettled-Advantage \citep{li2021breaking}, which used variance reduction techniques. All of these works reached regrets that logarithmically depend on $T$, which is much better than the worst-case $\sqrt{T}$-type regrets. However, no literature works on the gap-dependent regret for on-policy FRL. This motivates the following open question:
\begin{center}
\textit{Is it possible to establish gap-dependent regret bounds for on-policy FRL algorithms that are logarithmic in $T$?}
\end{center}

Meanwhile, recent works have proposed FRL algorithms for tabular MDPs on the off-policy setting \citep{woo2023blessing}, the offline setting \citep{woo2024federated}, and the situation with a simulator \citep{woo2023blessing,salgia2024sample}. Different from the on-policy methods, state-of-the-art algorithms for these settings do not update the implemented policies (exploration) and reach $MSA$-free logarithmic bounds on communication rounds, whereas the worst-case communication cost bounds for on-policy FRL methods require the dependence on $M$, $S$, and $A$ for the $\log T$ term (e.g., $O(MH^3SA\log T)$ in \citet{zheng2023federated}). While increased communication for exploration is reasonable, existing on-policy FRL methods cannot quantify the communication cost paid for exploring non-optimal actions or exploiting optimal policies under the worst-case MDPs since the suboptimality gaps can be arbitrarily close to 0. This leads to the dependence on $M$, $S$, and $A$ for the $\log T$ term, which motivates the following open question:
\begin{center}
\textit{Is it possible to establish gap-dependent communication cost bounds for on-policy FRL algorithms that disentangle exploration and exploitation and remove the dependence on $MSA$ from the $\log T$ term?}
\end{center}


A closely related evaluation criterion for on-policy RL is the global switching cost, which is defined as the times for policy switching. It is important in applications with restrictions on policy switching, such as compiler optimization \citep{ashouri2018survey}, hardware placements \citep{mirhoseini2017device}, database optimization  \citep{krishnan2018learning}, and material discovery \citep{nguyen2019incomplete}. Next, we review related literature on single-agent model-free RL algorithms. Under the worst-case MDPs, \citet{bai2019provably} modified the algorithms in \citet{jin2018q}, achieving a cost of $O(H^3SA\log T)$, and UCB-Advantage \citep{zhang2020almost} reached an improved cost of $O(H^2SA\log T)$, with both algorithms depending on $SA\log T$. In gap-dependent analysis, \citet{zheng2024gap} proved that UCB-Advantage enjoyed a cost that linearly depends on $S\log T$. Whether single-agent model-free RL algorithms can avoid the dependence on $SA$ for the $\log T$ term remains an open question.

In addition, multiple technical challenges exist when trying to establish gap-dependent bounds and improve the existing worst-case ones. First, gap-dependent regret analysis often relies on controlling the error in the value function estimations. However, the techniques for model-free methods \citep{yang2021q,xu2021fine,zheng2024gap} can only adapt to instant policy updates in single-agent methods, while FRL often uses delayed policy updates for a low communication cost. Second, proving low communication costs for FRL algorithms often requires actively estimating the number of visits to each state-action-step triple (see, e.g., \citet{woo2023blessing}). However, this is challenging for on-policy algorithms because the implemented policy is actively updated, and a universal stationary visiting probability is unavailable. Existing on-policy FRL methods reached logarithmic communication costs by controlling the visit and synchronization with the event-triggered synchronization conditions. These conditions guaranteed a sufficient increase in the number of visits to one state-action-step triple between synchronizations. However, this analysis is insufficient for the estimation of visiting numbers and results in the dependence on $SA$ for the $\log T$ term.

\textbf{Summary of Our Contributions.} We give an affirmative answer to these important open questions by proving the first gap-dependent bounds on both regret and communication cost for on-policy FRL in the literature. We analyze those bounds for FedQ-Hoeffding \citep{zheng2023federated}, an on-policy FRL algorithm for tabular episodic finite-horizon MDPs. Our contributions are summarized as follows.
\begin{itemize}[topsep=0pt, left=0pt]
    \item \textbf{Gap-Dependent Regret (\Cref{thm_regret}).} Denote $\dmin$ as the minimum nonzero suboptimality gap for all the state-action-step triples. We prove that FedQ-Hoeffding guarantees a gap-dependent expected regret of 
    \begin{equation}\label{regretbound}
        O\bigg(\frac{H^6 S A \log(MSAT)}{\dmin}+  C_{f}\bigg)
    \end{equation}
where $C_{f} = M\sqrt{H^7}SA\sqrt{\log(MSAT)}+ MH^5SA$ provides the gap-free part. It is logarithmic in $T$ and better than the worst-case $\sqrt{T}$-type regret discussed above when $T$ is large enough. When $M=1$, \eqref{regretbound} reduces to the single-agent gap-dependent regret bound \citep{yang2021q} for UCB-Hoeffding \citep{jin2018q}, which is the single-agent counterpart of FedQ-Hoeffding. Compared to UCB-Hoeffding, when $T$ is large enough and $\dmin$ is small enough, \eqref{regretbound} shows a better multi-agent speedup compared to the $\sqrt{T}$-type worst-case regrets shown in \citet{zheng2023federated}. Our numerical experiments in \Cref{regretexperiment} also demonstrate the $\log T$-pattern of the regret for any given MDP.

\item \textbf{Gap-Dependent Communication Cost (\Cref{thm_cost}).} We prove that under some general uniqueness of optimal policies, for any $p\in (0,1)$, with probability at least $1-p$, both the number of communication rounds and the number of different implemented policies required by FedQ-Hoeffding are upper bounded by
\begin{align}
\label{costbound}
&  O \bigg( MH^3SA\log(MH^2 \iota_0) + H^3SA\log\left(\frac{H^5SA}{\Delta^2_{\min}}\right) + H^3S\log\left(\frac{MH^9 S A \iota_0}{\Delta^2_{\min}C_{st}}\right)  +H^2\log\left(\frac{T}{HSA}\right) \bigg).
\end{align}
Here, $C_{st}\in (0,1]$ represents the minimum of the nonzero visiting probabilities to all state-step pairs under optimal policies, and $\iota_0 = \log(SAT/p)$. Since the communication cost of each round is $O(MHS)$, the total communication cost is \eqref{costbound} multiplied by $MHS$. Compared to the existing worst-case communication rounds that depend on $MSA\log T$ \citep{zheng2023federated,zheng2024federated,qiao2022sample} or $SA\log T$ \citep{zheng2024federated,labbi2024federated}, the first three terms in \eqref{costbound} only logarithmically depend on $1/\dmin$ and $\log T$, and the last term removes the dependence on $MSA$ from the $\log T$ term. This improvement is significant since $M$ represents the number of collaborating agents, and $SA$ represents the complexity of the state-action space that is often the bottleneck of RL methods \citep{jin2018q}. Compared to the $SA$-free communication rounds for FRL methods that do not update policies, \eqref{costbound} quantifies the cost of multiple components in on-policy FRL: the first two terms represent the cost for exploration, and the last two terms show the cost of implementing the optimal policy (exploitation). Our numerical experiments, presented in \Cref{MSA}, demonstrate that the \(\log T\) term in the communication cost is independent of \( M \), \( S \), and \( A \).

When $M=1$, FedQ-Hoeffding becomes a single-agent algorithm with low global switching cost shown in \eqref{costbound} (\Cref{globalcost}). It removes the dependence on $SA$ from the $\log T$ term compared to existing model-free methods \citep{bai2019provably,zhang2020almost,zheng2024gap}.


\item \textbf{Technical Novelty and Contributions.} 
We develop a new theoretical framework for the gap-dependent analysis of on-policy FRL with delayed policy updates. It provides two features simultaneously: controlling the error in the estimated value functions and estimating the number of visits . The first feature helps prove the gap-dependent regret \eqref{regretbound}, and the second is key to proving the bound \eqref{costbound} for communication rounds. Here, to overcome the difficulty of estimating visiting numbers,  we develop a new technical tool: concentrations on visiting numbers under varying policies. We establish concentration inequalities for visits to the stationary visiting probability of the optimal policies via error recursion on episode steps. This step relies on the number of visits with suboptimal actions instead of the algorithm settling on an optimal policy. It provides better estimations of visiting numbers. We also establish the following techniques with the tool and nonzero minimum suboptimality gap: (a) Exploring visiting discrepancies between optimal actions and suboptimal actions. This validates the concentration above. (b) Showing agent-wise simultaneous sufficient increase of visits. This helps remove the linear dependency on $M$ in the last three terms of \eqref{costbound}. (c) Showing state-wise simultaneous sufficient increase of visits for states with unique optimal actions. This helps remove the linear dependence on $SA$ from the last term in \eqref{costbound}.


To the best of our knowledge, these techniques are new to the literature for on-policy model-free FRL methods. They will be of independent interest in the gap-dependent analysis of other on-policy RL and FRL methods in controlling or estimating the number of visits.
\end{itemize}

\section{Related Work}
\label{related}
\textbf{On-policy RL for single agent finite-horizon tabular MDPs with worst-case regret.} There are mainly two types of algorithms for reinforcement learning: model-based and model-free learning. Model-based algorithms learn a model from past experience and make decisions based on this model, while model-free algorithms only maintain a group of value functions and take the induced optimal actions. Due to these differences, model-free algorithms are usually more space-efficient and time-efficient compared to model-based algorithms. However, model-based algorithms may achieve better learning performance by leveraging the learned model.

Next, we discuss the literature on model-based and model-free algorithms for finite-horizon tabular MDPs with worst-case regret. \citet{auer2008near}, \citet{agrawal2017optimistic}, \citet{azar2017minimax}, \citet{kakade2018variance}, \citet{agarwal2020model}, \citet{dann2019policy}, \citet{zanette2019tighter}, \citet{zhang2021reinforcement}, \citet{zhou2023sharp} and \citet{zhang2024settling} worked on model-based algorithms. Notably, \citet{zhang2024settling} provided an algorithm that achieves a regret of $\tilde{O}(\min \{\sqrt{SAH^2T},T\})$, which matches the information lower bound. \citet{jin2018q}, \citet{yang2021q}, \citet{zhang2020almost}, \citet{li2021breaking} and \citet{menard2021ucb} work on model-free algorithms. The latter three have introduced algorithms that achieve minimax regret of $\tilde{O}(\sqrt{SAH^2T})$.

\textbf{Suboptimality Gap.} When there is a strictly positive suboptimality gap, it is possible to achieve logarithmic regret
bounds. In RL, earlier work obtained asymptotic logarithmic regret bounds \citep{auer2007logarithmic,tewari2008optimistic}.
Recently, non-asymptotic logarithmic regret bounds were obtained \citep{jaksch2010near, ok2018exploration,simchowitz2019non, he2021logarithmic}. Specifically, \citet{jaksch2010near} developed a model-based algorithm, and their bound depends on the policy gap instead of the action gap studied in this paper. \citet{ok2018exploration} derived problem-specific logarithmic type lower bounds for both structured and unstructured MDPs. \citet{simchowitz2019non} extended the model-based algorithm by \citet{zanette2019tighter} and obtained logarithmic regret bounds. Logarithmic regret bounds are obtained in linear function approximation settings \citet{he2021logarithmic}. \citet{nguyen2023instance} also provides a gap-dependent regret bounds for offline RL with linear funciton approximation. 

Specifically, for model free algorithm, \citet{yang2021q} showed that the optimistic $Q$-learning algorithm by \citet{jin2018q} enjoyed a logarithmic regret $O(\frac{H^6SAT}{\dmin})$, which was subsequently refined by \citet{xu2021fine}. In their work, \citet{xu2021fine} introduced the Adaptive Multi-step Bootstrap (AMB) algorithm. \citet{zheng2024gap} further improved the logarithmic regret bound by leveraging the analysis of the UCB-Advantage algorithm \citep{zhang2020almost} and Q-EarlySettled-Advantage algorithm \citep{li2021breaking}.

There are also some other works focusing on gap-dependent sample complexity bounds
\citep{jonsson2020planning, marjani2020best, al2021navigating, tirinzoni2022near, wagenmaker2022beyond, wagenmaker2022instance, wang2022gap, tirinzoni2023optimistic}.

\textbf{Variance reduction in RL.} The reference-advantage decomposition used in \citet{zhang2020almost} and \citet{li2021breaking} is a technique of variance reduction that was originally proposed for finite-sum stochastic optimization  \citep{gower2020variance,johnson2013accelerating,nguyen2017sarah}. Later on, model-free RL algorithms also used variance reduction to improve the sample efficiency. For example, it was used in learning with generative models \citep{sidford2018near,sidford2023variance,wainwright2019variance}, policy evaluation \citep{du2017stochastic,khamaru2021temporal,wai2019variance,xu2020reanalysis}, offline RL \citep{shi2022pessimistic,yin2021near}, and $Q$-learning \citep{li2020sample,zhang2020almost,li2021breaking,yan2022efficacy}.

\textbf{RL with low switching cost and batched RL}. Research in RL with low-switching cost aims to minimize the number of policy switches while maintaining comparable regret bounds to fully adaptive counterparts, and it can be applied to federated RL. In batched RL \citep{perchet2016batched, gao2019batched}, the agent sets the number of batches and the length of each batch upfront, implementing an unchanged policy in a batch and aiming for fewer batches and lower regret. \citet{bai2019provably} first introduced the problem of RL with low-switching cost and proposed a $Q$-learning algorithm with lazy updates, achieving $\tilde{O}(SAH^3\log T)$ switching cost. This work was advanced by \citet{zhang2020almost}, which improved the regret upper bound and the switching cost. Additionally, \citet{wang2021provably} studied RL under the adaptivity constraint. 
Recently, \citet{qiao2022sample} proposed a model-based algorithm with $\tilde{O}(\log \log T)$ switching cost. \citet{zhang2022near} proposed a batched RL algorithm that is well-suited for the federated setting. 

\textbf{Multi-agent RL (MARL) with event-triggered communications.} We review a few recent works for on-policy MARL with linear function approximations. \citet{dubey2021provably} introduced Coop-LSVI for cooperative MARL. \citet{min2023cooperative} proposed an asynchronous version of LSVI-UCB that originates from \citet{jin2020provably}, matching the same regret bound with improved communication complexity compared to \citet{dubey2021provably}. \citet{hsu2024randomized} developed two algorithms that incorporate randomized exploration, achieving the same regret and communication complexity as \citet{min2023cooperative}. \citet{dubey2021provably}, \citet{min2023cooperative} and \citet{hsu2024randomized} employed event-triggered communication conditions based on determinants of certain quantities. Different from our federated algorithm, during the synchronization in \citet{dubey2021provably} and \citet{min2023cooperative}, local agents share original rewards or trajectories with the server. On the other hand, \citet{hsu2024randomized} reduces communication cost by sharing compressed statistics in the non-tabular setting with linear function approximation.

\textbf{Federated and distributed RL}. Existing literature on federated and distributed RL algorithms highlights various aspects. For value-based algorithms, \cite{guo2015concurrent}, \cite{zheng2023federated}, and \cite{woo2023blessing} focused on  linear speed up. \citep{agarwal2021communication} proposed a parallel RL algorithm with low communication cost. \cite{woo2023blessing} and \cite{woo2024federated} discussed the improved covering power of heterogeneity. \cite{wu2021byzantine} and \cite{chen2023byzantine} worked on robustness. Particularly, \cite{chen2023byzantine} proposed algorithms in both offline and online settings, obtaining near-optimal sample complexities and achieving superior robustness guarantees. In addition, several works have investigated value-based algorithms such as $Q$-learning in different settings, including \cite{beikmohammadi2024compressed}, \cite{jin2022federated}, \cite{khodadadian2022federated}, \cite{fan2023fedhql}, \cite{woo2023blessing}, and \cite{woo2024federated,anwar2021multi,zhao2023federated,yang2023federated,zhang2024finite}.  The convergence of decentralized temporal difference algorithms has been analyzed by \cite{doan2019finite}, \cite{doan2021finite}, \cite{chen2021multi}, \cite{sun2020finite}, \cite{wai2020convergence}, \cite{wang2020decentralized}, \cite{zeng2021finite}, and \cite{liu2023distributed}. 


Some other works focus on policy gradient-based algorithms. Communication-efficient policy gradient algorithms have been studied by \cite{fan2021fault} and \cite{chen2021communication}. \cite{lan2023improved} further shows the
simplicity compared to the other RL methods, and a linear speedup has been demonstrated in the synchronous setting. Optimal sample complexity for global optimality in federated RL, even in the presence of adversaries, is studied in \cite{ganesh2024global}. \citep{lan2024asynchronous} propose an algorithm to address the challenge of lagged policies in asynchronous settings.

The convergence of distributed actor-critic algorithms has been analyzed by \cite{shen2023towards} and \cite{chen2022sample}. Federated actor-learner architectures have been explored by \cite{assran2019gossip}, \cite{espeholt2018impala}, and \cite{mnih2016asynchronous}. Distributed inverse reinforcement learning has been examined by \cite{banerjee2021identity}, \cite{gong2023federated} and \cite{liu2022distributed, liu2023meta, liu2024learning, liutrajectory}.

\section{Background and Problem Formulation}\label{sec:background}

\subsection{Preliminaries}
We begin by introducing the mathematical framework of Markov decision processes. 
In this paper, we assume that $0/0 = 0$. For any $C\in \mathbb{N}$, we use $[C]$ to denote the set $\{1,2,\ldots C\}$. We use $\mathbb{I}[x]$ to denote the indicator function, which equals 1 when the event $x$ is true and 0 otherwise.

\textbf{Tabular episodic Markov decision process (MDP).}
A tabular episodic MDP is denoted as $\mathcal{M}:=(\mathcal{S}, \mathcal{A}, H, \mathbb{P}, r)$, where $\mathcal{S}$ is the set of states with $|\mathcal{S}|=S, \mathcal{A}$ is the set of actions with $|\mathcal{A}|=A$, $H$ is the number of steps in each episode, $\mathbb{P}:=\{\mathbb{P}_h\}_{h=1}^H$ is the transition kernel so that $\mathbb{P}_h(\cdot \mid s, a)$ characterizes the distribution over the next state given the state action pair $(s,a)$ at step $h$, and $r:=\{r_h\}_{h=1}^H$ is the collection of reward functions. We assume that $r_h(s,a)\in [0,1]$ is a {deterministic} function of $(s,a)$, while the results can be easily extended to the case when $r_h$ is random. 
	
	In each episode, an initial state $s_1$ is selected arbitrarily by an adversary. Then, at each step $h \in[H]$, an agent observes a state $s_h \in \mathcal{S}$, picks an action $a_h \in \mathcal{A}$, receives the reward $r_h = r_h(s_h,a_h)$ and then transits to the next state $s_{h+1}$. 
 The episode ends when an absorbing state $s_{H+1}$ is reached.

\textbf{Policies, state value functions, and action value functions.}
	A policy $\pi$ is a collection of $H$ functions $\left\{\pi_h: \mathcal{S} \rightarrow \Delta^\mathcal{A}\right\}_{h \in[H]}$, where $\Delta^\mathcal{A}$ is the set of probability distributions over $\mathcal{A}$. A policy is deterministic if for any $s\in\mathcal{S}$,  $\pi_h(s)$ concentrates all the probability mass on an action $a\in\mathcal{A}$. In this case, we denote $\pi_h(s) = a$. 
 
 Let $V_h^\pi: \mathcal{S} \rightarrow \mathbb{R}$ and $Q_h^\pi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ denote the state value function and the state-action value function at step $h$ under policy $\pi$. Mathematically, $V_h^\pi(s):=\sum_{t=h}^H \mathbb{E}_{(s_{t},a_{t})\sim(\mathbb{P}, \pi)}\left[r_{t}(s_{t},a_{t}) \left. \right\vert s_h = s\right]$. $Q_h^\pi(s,a):=\sum_{t=h}^H\mathbb{E}_{(s_{{t}},a_{t})\sim(\mathbb{P}, \pi)}\left[ r_{t}(s_{t},a_{t}) \left. \right\vert (s_h,a_h)=(s,a)\right]$.

	Since the state and action spaces and the horizon are all finite, there exists an optimal policy $\pi^{\star}$ that achieves the optimal value $V_h^{\star}(s)=\sup _\pi V_h^\pi(s)=V_h^{\pi^*}(s)$ for all $(s,h) \in \mathcal{S} \times [H]$  \citep{azar2017minimax}. The Bellman equation and
	the Bellman optimality equation can be expressed as
\begin{equation}\label{eq_Bellman}
\begin{aligned}
&\left\{
\begin{array}{l}
    V_h^{\pi}(s) = \mathbb{E}_{a' \sim \pi_h(s)}[Q_h^{\pi}(s, a')] \\
    Q_h^{\pi}(s, a) := r_h(s, a) + \mathbb{E}_{s' \sim \mathbb{P}_h(\cdot|s,a)} V_{h+1}^{\pi}(s') \\
    V_{H+1}^{\pi}(s) = 0, \forall (s, a, h) \in \mathcal{S} \times \mathcal{A} \times [H],
\end{array}
\right. \quad
&\left\{
\begin{array}{l}
    V_h^{\star}(s) = \max_{a' \in \mathcal{A}} Q_h^{\star}(s, a') \\
    Q_h^{\star}(s, a) := r_h(s, a) + \mathbb{E}_{s' \sim \mathbb{P}_h(\cdot|s,a)} V_{h+1}^{*}(s') \\
    V_{H+1}^{\star}(s) = 0, \forall (s, a, h) \in \mathcal{S} \times \mathcal{A} \times [H].
\end{array}
\right.
\end{aligned}
\end{equation}


\textbf{Suboptimality Gap.} For any given MDP, we can provide the following formal definition.
\begin{definition}\label{def_sub}
    For any $(s,a,h)$, the suboptimality gap is defined as
    $\Delta_h(s,a) := V_h^\star(s) - Q_h^\star(s,a)$.
\end{definition}
\eqref{eq_Bellman} implies that for any $ (s,a,h)$, $\Delta_h(s,a) \geq 0$. Then, it is natural to define the minimum gap, which is the minimum non-zero suboptimality gap with regard to all $(s,a,h)$.
\begin{definition}\label{def_minsub}
    We define the \textbf{minimum gap} as $\dmin := \inf\{\Delta_h(s,a) \mid \Delta_h(s,a)>0,(s,a,h)\in \sah\}.$
\end{definition}
We remark that if $\{\Delta_h(s,a) \mid \Delta_h(s,a)>0,(s,a,h)\in \sah\} = \emptyset$, then all actions are optimal, leading to a degenerate MDP. Therefore, we assume that the set is nonempty and $\dmin > 0$ in the rest of this paper. \Cref{def_sub,def_minsub} and the non-degeneration are standard in the literature on gap-dependent analysis \citep{simchowitz2019non, yang2021q, xu2020reanalysis}.

\textbf{Global Switching Cost.} We provide the following definition for any algorithm with $K>1$ episodes, which is also used in \citet{bai2019provably} and \citet{qiao2022sample}.
\begin{definition}\label{def_switching}
The global switching cost for $K$ episodes is defined as $N_\textnormal{switch} := \sum_{k=1}^{K-1} \mathbb{I}[\pi^{k+1} \neq \pi^{k}].$
\end{definition}


\subsection{The Federated RL Framework}

We consider an FRL setting with a central server and $M$ agents, each interacting with an independent copy of $\mathcal{M}$. The agents communicate with the server periodically: after receiving local information, the central server aggregates it and broadcasts certain information to the agents to coordinate their exploration.


For agent $m$, let $U_m$ be the number of generated episodes,
$\pi^{m,u}$ be the policy in the $u$-th episode of agent $m$, and $x_1^{m,u}$ be the corresponding initial state. The regret of $M$ agents over $\hat{T}=H\sum_{m=1}^M U_m$ total steps is
$$\mbox{Regret}(T) = \sum_{m \in [M]} \sum_{u=1}^{U_m} \left(V_1^\star(s_1^{m,u}) - V_1^{\pi^{m,u}}(s_1^{m,u})\right).$$
Here, $T:=\hat{T}/M$ is the average total steps for $M$ agents.

We also define the communication cost of an algorithm as the number of scalars (integers or real numbers) communicated between the server and agents.

\section{Performance Guarantees}
\subsection{FedQ-Hoeffding Algorithm}
\label{1Hoeffding}
In this subsection, we briefly review FedQ-Hoeffding. Details are provided in \Cref{alg_hoeffding_server} and \Cref{alg_hoeffding_agent} in \Cref{Hoeffdinga}. FedQ-Hoeffding proceeds in rounds, indexed by $k\in[K]$. Round $k$ consists of $n^{m,k}$ episodes for agent $m$, where the specific value of $n^{m,k}$ will be determined later. 

\textbf{Notations.} For the $j$-th ($j\in[n^{m,k}]$) episode for agent $m$ in the $k$-th round, we denote the corresponding trajectory as $\{(s_h^{k,j,m}, a_h^{k,j,m}, r_h^{k,j,m})\}_{h=1}^H $ . Denote $n_h^{m,k}(s,a)$ as the number of times that $(s,a,h)$ has been visited by agent $m$ in round $k$, $n_h^{k}(s,a) := \sum_{m=1}^M n_h^{m,k}(s,a)$ as the total number of visits in round $k$ for all agents, and $N_h^k(s,a)$ as the total number of visits to $(s,a,h)$ among all agents before the start of round $k$. We also use $\{V_h^k: \mathcal{S}\rightarrow \mathbb{R}\}_{h=1}^H$ and $\{Q_h^k: \mathcal{S}\times \mathcal{A}\rightarrow \mathbb{R}\}_{h=1}^H$ to denote the global estimates of the state value function and state-action value function at the beginning of round $k$. Before the first round, both estimates are initialized as $H$.

\textbf{Coordinated Exploration.} At the beginning of round $k$, the server decides a deterministic policy $\pi^k = \{\pi_{h}^k\}_{h=1}^H$, and then broadcasts it along with $\{N_h^k(s,\pi_{h}^k(s))\}_{s,h}$ and $\{V_h^k(s)\}_{s,h}$ to agents. Here, $\pi^1$ can be chosen arbitrarily. Then, the agents execute $\pi^k$ and start collecting trajectories. During the exploration in round $k$, every agent $m$ will monitor its number of visits to each $(s,a,h)$. For any agent $m$, at the end of each episode, if any $(s,a,h)$ has been visited by 
\begin{equation}\label{def_chk_main}
    c_h^k(s,a) = \max\left\{1,\left\lfloor\frac{N_h^k(s, a)}{MH(H+1)}\right\rfloor\right\}
\end{equation} times by agent $m$, the agent will send a signal to the server, which will then abort all agents' exploration. Here, we say that \textbf{$(s,a,h)$ satisfies the trigger condition in round $k$}. During the exploration, for all $(s,a,h)$, agent $m$ adaptively calculates $n_h^{m,k}(s,a)$ and the local estimate for the next-step return $v_{h+1}^{m,k}(s,a) := \sum_{j=1}^{n^{m,k}} V_{h+1}^k\left(s_{h+1}^{k,j,m}\right)\mathbb{I}\left[(s_h^{k,j,m},a_h^{k,j,m}) = (s,a)\right]$. At the end of round $k$, each agent sends $\{r_h(s,\pi_h^k(s))\}_{s,h}$, $\{n_h^{m,k}(s,\pi_h^k(s))\}_{s,h}$ and $\{v_{h+1}^{m,k}(s,\pi_h^k(s))\}_{s,h}$ to the central server for aggregation. 

\textbf{Updates of estimated value functions.} The central server calculates $n_h^k(s,a),N_h^{k+1}(s,a)$ for all triples. While letting $Q_h^{k+1}(s,a) = Q_h^{k}(s,a)$ for triples such that $n_h^k(s,a) = 0$, it updates the estimated value functions for each triple with positive $n_h^k(s,a)$ as follows.

  \textbf{Case 1:} $N_h^k(s,a)< 2MH(H+1)=:i_0$. This case implies that each client can visit each $(s,a)$ pair at step $h$ at most once. Let $Q = Q_h^k(s,a)$. Then the server iteratively update $Q$ using the following assignment: 
\begin{equation}\label{update_q_small_n}
    Q \overset{+}{\leftarrow}  \eta_t(r_h + V_{h+1}^{k,t} + b_t - Q),t = N_h^k+1,\ldots, N_h^{k+1} 
\end{equation}
and then assign $Q_h^{k+1}(s,a)$ with $Q$. Here, $r_h,N_h^k,N_h^{k+1}$ are short for their respective values at $(s,a)$, $\eta_t \in (0,1]$ is the learning rate, $b_t > 0$ is a bonus, and $V_{h+1}^{k,t}$ represents the $(t-N_h^k)$-th nonzero value in $\{v_{h+1}^{m,k}(s,a)\}_{m=1}^M$.

\textbf{Case 2:} $N_h^k(s,a)\geq i_0$. In this case, the central server calculates the global estimate of the expected return   $v_{h+1}^{k}(s,a) = \sum_{m=1}^M v_{h+1}^{m,k}(s,a)/n_h^{k}(s,a)$ and updates the $Q$-estimate as
\begin{equation}\label{update_q_large_n}
    Q_h^{k+1}= (1-\eta_{s,a}^{h,k})Q_h^k +\eta_{s,a}^{h,k}(r_h+v_{h+1}^k) + \beta^k_{s,a,h}.
\end{equation}
Here, $r_h,Q_h^{k},Q_h^{k+1},v_{h+1}^k$ are short for their respective values at $(s,a)$, $\eta_{s,a}^{h,k}\in (0,1]$ is the learning rate and $\beta^k_{s,a,h}>0$ represents the bonus.

After updating the estimated $Q$-function, the central server updates the estimated value function and the policy as $V_h^{k+1}(s) = \min \{H, \max _{a^{\prime} \in \mathcal{A}} Q_h^{k+1}(s, a^{\prime})\}$ and $\pi_{h}^{k+1}(s)= \arg \max _{a^{\prime} \in \mathcal{A}} Q_h^{k+1}(s, a^{\prime})$. Such update implies that FedQ-Hoeffding is an optimism-based method. It then proceeds to round $k+1$.

In FedQ-Hoeffding, agents only send local estimates instead of original trajectories to the central server. This guarantees a low communication cost for each round, which is $O(MHS)$. In addition, the event-triggered termination condition with the threshold \eqref{def_chk_main} limits the number of new visits in each round, with which \citet{zheng2023federated} proved the linear regret speedup under worst-case MDPs. Moreover, it guarantees that the number of visits to the triple that satisfies the trigger condition sufficiently increases after this round. This is the key to proving the worst-case logarithmic communication cost in \citet{zheng2023federated}.




\subsection{Gap-Dependent Regret}\label{subsec:regret}
Next, we provide a new gap-dependent regret upper bound for FedQ-Hoeffding as follows.
\begin{theorem}\label{thm_regret} For FedQ-Hoeffding (\Cref{alg_hoeffding_server} and \Cref{alg_hoeffding_agent}), $\mathbb{E}\left(\textnormal{Regret}(T)\right)$ can be bounded by \eqref{regretbound}.
\end{theorem}
\Cref{thm_regret} shows that the regret is logarithmic in $T$ under MDPs with a nonzero $\dmin$. When $T$ is sufficiently large, it is better than the \( \sqrt{T} \)-type worst-case regrets in the literature for on-policy FRL. 

When \( M = 1 \), the bound reduces to \( O(\frac{H^6 S A\log(SAT)}{\dmin}) \), which is the gap-dependent bound in \citet{yang2021q} for our single-agent counterpart, UCB-Hoeffding algorithm. Thus, when $T$ is sufficiently large, for the average regret of all the episodes defined as $\textnormal{Regret}(T)/(MT)$, the ratio between gap-dependent bounds for FedQ-Hoeffding and UCB-Hoeffding is $\tilde{O}(1/M)$, which serves as our error reduction rate. As a comparison, it is better than the rates under worst-case MDPs for on-policy FRL methods in the literature, which are $\tilde{O}(1/\sqrt{M})$ because of their linear dependency on $\sqrt{MT}$. We will show this pattern in the numerical experiments in \Cref{regretexperiment}.

\textbf{Key Ideas of the Proof.} Define $\text{clip}[x \mid y] := x \cdot \mathbb{I}[x \geq y]$. Note that
\begin{align*}
    \mathbb{E}\left(\textnormal{Regret}(T)\right) &= \mathbb{E} \left[\sum_{k,j,m}\sum_{h=1}^{H} \Delta_h(s_h^{k,j,m}, a_h^{k,j,m})\right] \\&\leq \mathbb{E} \left[\sum_{h=1}^{H}\sum_{k,j,m}\mathrm{clip}\left[\left(Q_h^k - Q_h^*\right)(s_h^{k,j,m}, a_h^{k,j,m}) \mid \dmin\right]\right].
\end{align*}
Now we only need to bound the term
\[
\sum_{h=1}^{H} \sum_{k,j,m} \mathrm{clip}\left[\left(Q_h^k - Q_h^*\right)(s_h^{k,j,m}, a_h^{k,j,m}) \mid \dmin\right].
\]
To continue, we need to bound the weighted sum of estimation errors
\[
\sum_{k,j,m} \omega_{h,k} \left( Q_h^k - Q_h^* \right)(s_h^{k,j,m}, a_h^{k,j,m})
\]
for any non-negative weights $\{\omega_{h,k}\}_{h,k}$. This can be done by recursion on \( h \) with \( Q_{H+1}^k = Q_{H+1}^* = 0 \).



\subsection{Gap-Dependent Communication Cost}
\label{costresult}
We first introduce two additional assumptions:\\
(I) Full synchronization. Similar to \citet{zheng2023federated}, we assume that there is no latency during the communications, and the agents and server are fully synchronized \citep{mcmahan2017communication}. This means $n^{m,k} = n^k$ for each agent $m$. \\
(II) Random initializations. We assume that the initial states $\{s_1^{k,j,m}\}_{k,j,m}$ are randomly generated following some distribution on $\mathcal{S}$, and the generation is not affected by any result in the learning process.


Next, we introduce a new concept: G-MDPs.
\begin{definition}
\label{assumptioncost}
    A G-MDP satisfies two conditions:\\
(a) The stationary visiting probabilities under optimal policies are unique: if both $\pi^{*,1}$ and $\pi^{*,2}$ are optimal policies, then we have $\mathbb{P}\left(s_h = s | \pi^{*,1}\right) = \mathbb{P}\left(s_h = s | \pi^{*,2}\right)=: \mathbb{P}_{s,h}^*.$\\
(b) Let $\mathcal{A}_h^*(s) = \{a \mid a = \arg \max_{a'} Q_h^*(s,a')\}$. For any $(s,h)\in \mathcal{S}\times[H]$, if $\mathbb{P}_{s,h}^* > 0$, then $|\mathcal{A}_h^*(s)| = 1$, which means that the optimal action is unique.
\end{definition}

G-MDPs represent MDPs with generally unique optimal policies. (a) and (b) above characterize the general uniqueness, and an MDP with a unique optimal policy is a G-MDP. Compared to requiring a unique optimal policy, G-MDPs allow the optimal actions to vary outside the support under optimal policies, i.e., the state-step pairs with $\mathbb{P}_{s,h}^* = 0$.


For a G-MDP, we define $C_{st} = \min\{\mathbb{P}_{s,h}^* \mid  s \in \mathcal{S}, h \in [H], \mathbb{P}_{s,h}^*>0\}$. Thus, $0 < C_{st} \leq 1$ reflects the minimum visiting probability on the support for optimal policies. Next, we provide gap-dependent communication cost bound.
\begin{theorem}\label{thm_cost} Let $\iota_0 = \log(SAT/p)$. For FedQ-Hoeffding (\Cref{alg_hoeffding_server} and \Cref{alg_hoeffding_agent}) under full synchronization and random initializations assumptions, for any given G-MDP and any $p \in (0,1)$, with probability at least $1-p$, the number of rounds $K$ is upper bounded by \eqref{costbound}.
\end{theorem}
We can get the upper bound of total communication cost by multiplying the upper bound in \eqref{costbound} and $O(MHS)$, the communication cost of each round in FedQ-Hoeffding.

Compared to existing worst-case costs that depend on $SA$ \citep{zheng2024federated,labbi2024federated} or $MSA$ \citep{zheng2023federated,zheng2024federated,qiao2022sample} for $\log T$, \eqref{costbound} is better when $T$ is sufficiently large since the first three terms only logarithmically depend on $1/\dmin$ and $\log T$, and the last term that is logarithmic in $T$ removes the dependency on $MSA$. Moreover, \eqref{costbound} highlights the cost for different procedures in FedQ-Hoeffding: the first two terms represent the cost for exploration, and the last two terms show the cost when exploiting the optimal policies. Our numerical experiments in \Cref{MSA} also demonstrate that the \(\log T\) term  in the communication cost is independent of \( M \), \( S \), and \( A \).

\textbf{Key Ideas of the Proof.} 
For the triple \( (s, a, h) \) that satisfies the trigger condition in round \( k \) when \( N_h^k(s, a) > i_1 \) for some threshold \( i_1 \), we can demonstrate an agent-wise simultaneous sufficient increase in visits, which allows us to eliminate the linear dependency of \( M \) from the last three terms.

In the exploitation period, for the triple \( (s, a, h) \) such that \( \mathbb{P}_{s,h}^* > 0 \), \( a \in \mathcal{A}_h^*(s) \), and that satisfies the trigger condition in round \( k \) when \( N_h^k(s, a) > i_2 \) for some threshold \( i_2 >i_1\), we can show a state-wise simultaneous sufficient increase in visits, which helps us remove the linear dependency of \( S \) and \( A \) from the last term.


Since FedQ-Hoeffding implements a fixed policy in each round, when $M = 1$, the algorithm reduces to a single-agent algorithm with a low global switching cost, which is shown in
\Cref{globalcost}.
\begin{corollary}
\label{globalcost}
    For the FedQ-Hoeffding algorithm (\Cref{alg_hoeffding_server} and \Cref{alg_hoeffding_agent}) under full synchronization,
random initialization, and $M=1$, for any given G-MDP and any $p \in (0,1)$, with probability at least $1-p$, the global switching cost can be bounded by \eqref{costbound}.
\end{corollary}
Given that the costs of existing single-agent model-free algorithms depend on $SA$ \citep{bai2019provably,zhang2020almost} or $S$ \citep{zheng2024gap} for $\log T$\footnote{In the literature, these bounds are for local switching cost that counts the state-step pairs where the policy switches. The local switching cost is greater than or equal to our global switching cost, but these works didn't find tighter bounds for the global switching cost. We refer readers to \citet{bai2019provably} for more information.}, our $\log T$-dependency is better by removing the factor $SA$.  

At the end of this section, we also remark on FedQ-Bernstein, another on-policy FRL algorithm in \citet{zheng2023federated}. It differs from FedQ-Hoeffding in setting the bonuses ($b_t$ and $\beta_{s,a,h}^k$) and involving variance estimators. We can also show that FedQ-Bernstein can also reach the gap-dependent bounds in \eqref{regretbound} and in \eqref{costbound}. In \citet{zheng2023federated}, the worst-case regret bound of FedQ-Bernstein is better than that of FedQ-Hoeffding by a factor $\sqrt{H}$, and the two methods share the same worst-case communication cost bound. Whether FedQ-Bernstein can reach better gap-dependent bounds remains an open question.

\section{Numerical Experiments}
\label{numerical}
In this section, we conduct experiments
\footnote{All the experiments are run on a server with Intel Xeon E5-2650v4 (2.2GHz) and 100 cores. Each replication is limited to a single core and 50GB RAM.}
. All the experiments are conducted in a synthetic environment to demonstrate the $\log T$-type regret and reduced communication cost bound with the coefficient of the main term $O(\log T)$ being independent of $M, S, A$ in FedQ-Hoeffding algorithm \citep{zheng2023federated}. We follow \citet{zheng2023federated} and generate a synthetic environment to evaluate the proposed algorithms on a tabular episodic MDP. After setting $H, S, A$, the reward $r_h(s, a)$ for each $(s,a,h)$ is generated independently and uniformly at random from $[0,1]$. $\mathbb{P}_h(\cdot \mid s, a)$ is generated on the $S$-dimensional simplex independently and uniformly at random for $(s,a,h)$. We also set the constant $c$ in the bonus term $b_t$ to be 2 and $\iota = 1$. We will first demonstrate the $\log T$-type regret of FedQ-Hoeffding algorithm.

\subsection{$\log T$-type regret and speedup}
\label{regretexperiment}
In this section, we show that the regret for any given MDP follows a $\log T$ pattern. We consider two different values for the triple $(H, S, A)$: $(2, 2, 2)$ and $(5, 3, 2)$. For FedQ-Hoeffding algorithm, we set the agent number $M = 10$ and generate $T/H = 10^7$ episodes for each agent, resulting in a total of $10^8$ episodes. Additionally, to show the linear speedup effect, we conduct experiments with its single-agent version, the UCB-Hoeffding algorithm \citep{jin2018q}, where all the conditions except $M=1$ remain the same. To show error bars, we collect 10 sample paths for each algorithm under the same MDP environment. The regret results are shown in \Cref{fig:532} and \Cref{fig:555}. For both algorithms, the solid line represents the median of the 10 sample paths, while the shaded area shows the 10th and 90th percentiles. 
\begin{figure}[H]
    \centering
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphs/regret_plot222_1.jpg}
    \end{minipage}
    \hfill
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphs/regret_plot222_log_1.jpg}
    \end{minipage}
    \caption{Regret results for $H = 2$, $S = 2$, and $A = 2$. The left panel directly shows the plot of $\mbox{Regret}(T)$ versus $T/H$, while the right panel illustrates the relationship between $\mbox{Regret}(T) / \log(T/H + 1)$ and $T/H$. In both plots, the yellow line represents the regret results of the FedQ-Hoeffding algorithm, while the red line represents the results of the UCB-Hoeffding algorithm. The blue line in each plot denotes the adjusted regret of the FedQ-Hoeffding algorithm, which is obtained by dividing the regret results of the yellow line by $\sqrt{M}$.}
    \label{fig:532}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphs/regret_plot532.jpg}
    \end{minipage}
    \hfill
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Graphs/regret_plot532_log.jpg}
    \end{minipage}
    \caption{Regret results for $H = 5$, $S = 3$, and $A = 2$. The left panel directly shows the plot of $\mbox{Regret}(T)$ versus $T/H$, while the right panel illustrates the relationship between $\mbox{Regret}(T) / \log(T/H + 1)$ and $T/H$. In both plots, the yellow line represents the regret results of the FedQ-Hoeffding algorithm, while the red line represents the results of the UCB-Hoeffding algorithm. The blue line in each plot denotes the adjusted regret of the FedQ-Hoeffding algorithm, which is obtained by dividing the regret results of the yellow line by $\sqrt{M}$.}
    \label{fig:555}
\end{figure}

From the two groups of plots, we observe that the two yellow lines in the plots on the right side of \Cref{fig:532} and \Cref{fig:555} tend to approach horizontal lines as $T/H$ becomes sufficiently large. Since the y-axis represents $\mbox{Regret}(T) / \log(T/H + 1)$ in these two plots, we can conclude that the regret of the FedQ-Hoeffding algorithm follows a $\log T$-type pattern for any given MDP, rather than the $\sqrt{MT}$ pattern shown in the Theorem 4.1 of \Citet{zheng2023federated}. This is consistent with the logarithmic regret result presented in \Cref{thm_regret}. Furthermore, as $T/H$ becomes sufficiently large, we observe that the adjusted regret of FedQ-Hoeffding (represented by the blue lines) for both groups of $(H, S, A)$ is significantly lower than the corresponding regret of the single-agent version, UCB-Hoeffding (represented by the red lines). This further supports the conclusion that the regret of FedQ-Hoeffding does not follow a $\sqrt{MT}$ pattern, or else the blue lines and the red lines would be close to each other. Finally, as $T/H$ grows larger, we notice that the yellow lines and the red lines become close, confirming that the regret of FedQ-Hoeffding approaches that of UCB-Hoeffding as $T$ becomes sufficiently large. This also supports the error reduction rate $\tilde{O}(1/M)$ for the gap-dependent regret.


\subsection{Dependency of Communication Cost on $M$, $S$, and $A$}
\label{MSA}
In this section, we will demonstrate that the coefficient of the $\log T$ term in the communication cost is independent of $M$, $S$ and $A$. To eliminate the influence of terms with lower orders of $\log T$, such as $\log(\log T)$ and $\sqrt{\log T}$ in \Cref{thm_cost}, we will focus exclusively on the communication cost for sufficiently large values of $T$.
\subsubsection{Dependency on $M$}
To explore the dependency of communication cost on $M$, we set $(H,S,A) = (2,2,2)$ and let $M$ take values in $\{2,4,6,8\}$. We generate $10^7$ episodes for each agent and only consider the communication cost after $5\times 10^5$ episodes. The \Cref{dependentM} shows the results for each $M$ after $5\times 10^5$ episodes.
\begin{figure}[H]
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{Graphs/cost_plot_m1.jpg}}
\caption{Number of communication rounds vs Log-number of Episodes for different $M$ Values with $H=2$, $S =2$ and $A=2$. Each solid line represents the number of communication rounds for each value of $M \in \{2,4,6,8\}$ after $5 \times 10^5$ episodes, while the dashed line represents the corresponding fitted line for each $M$.}
\label{dependentM}
\end{center}
\end{figure}
In \Cref{dependentM}, each solid line represents the communication cost for each value of $M \in \{2,4,6,8\}$ after $5 \times 10^5$ episodes, while the dashed line represents the corresponding fitted line. Since the x-axis represents the log-number of episodes, $\log(T/H)$, the slope of the fitted line is very close to the coefficient of the $\log T$-term in the communication cost when $\log T$ is sufficently large. We observe that the slopes of these fitted lines are very similar, which indicates that for any given MDP, the coefficient of the $\log T$-term in the communication cost is independent of $M$. If the coefficient were linearly dependent on $M$, as shown in \citet{zheng2023federated}, then for $M = 8$, the slope of the fitted line should be nearly four times the value of the slope of the fitted line for $M = 2$.

\subsubsection{Dependency on $S$}
To explore the dependency of communication cost on $S$, we set $(H,A,M) = (2,2,2)$ and let $S$ take values in $\{2,4,6,8\}$. We generate $10^7$ episodes for each agent and only consider the communication cost after $5\times 10^5$ episodes. The \Cref{dependentS} shows the communication cost results for each $S$ after $5\times 10^5$ episodes.
\begin{figure}[H]
\begin{center}
\centerline{\includegraphics[width=0.67\columnwidth]{Graphs/cost_plot_s0.jpg}}
\caption{Number of communication rounds vs Log-number of Episodes for different $S$ Values with $H=2$, $A =2$ and $M=2$. Each solid line represents the number of communication rounds for each value of $S \in \{2,4,6,8\}$ after $5 \times 10^5$ episodes. The dashed line represents the fitted line for each $S$.}
\label{dependentS}
\end{center}
\end{figure}
In \Cref{dependentS}, each solid line represents the communication cost for each value of $S \in \{2,4,6,8\}$ after $5 \times 10^5$ episodes, while the dashed line represents the corresponding fitted line. Since the x-axis represents the log-number of episodes, $\log(T/H)$, the slope of the fitted line is very close to the coefficient of the $\log T$-term in the communication cost when $\log T$ is sufficently large. We observe that the slopes of these fitted lines are very similar, which indicates that for any given MDP, the coefficient of the $\log T$-term in the communication cost is independent of $S$.


\subsubsection{Dependency on $A$}

To explore the dependency of communication cost on $A$, we set $(H,S,M) = (2,2,2)$ and let $A$ take values in $\{2,4,6,8\}$. We generate $10^7$ episodes for each agent and only consider the communication cost after $5\times 10^5$ episodes. The \Cref{dependentA} shows the communication cost results for each $A$ after $5\times 10^5$ episodes.

In \Cref{dependentA}, each solid line represents the communication cost for each value of $A \in \{2,4,6,8\}$ after $5 \times 10^5$ episodes, while the dashed line represents the corresponding fitted line. Since the x-axis represents the log-number of episodes, $\log(T/H)$, the slope of the fitted line is very close to the coefficient of the $\log T$-term in the communication cost when $\log T$ is sufficently large. We observe that the slopes of these fitted lines are very similar, which indicates that for any given MDP, the coefficient of the $\log T$-term in the communication cost is independent of $A$.

\begin{figure}[H]
\begin{center}
\centerline{\includegraphics[width=0.67\columnwidth]{Graphs/cost_plot_a0.jpg}}
\caption{Number of communication rounds vs Log-number of Episodes for different $A$ Values with $H=2$, $S =2$ and $M=2$. Each solid line represents the number of communication rounds for each value of $A \in \{2,4,6,8\}$ after $5 \times 10^5$ episodes. The dashed line represents the fitted line for each $A$.}
\label{dependentA}
\end{center}
\vskip -0.2in
\end{figure}
\section{Conclusion}
In this paper, we establish the first gap-dependent bounds on regret and communication cost for on-policy federated $Q$-Learning in tabular episodic finite-horizon MDPs, addressing two important open questions in the literature. While existing FRL methods focus on worst-case MDPs, we show that when MDPs exhibit benign structures, such as a strictly positive suboptimality gap, these bounds can be significantly improved. Specifically, we prove that both FedQ-Hoeffding and FedQ-Bernstein can achieve \( \log T \)-type regret. Additionally, we derive a gap-dependent communication cost bound that disentangles exploration and exploitation, with the \( \log T \) term in the communication cost being independent of \( M \), \( S \), and \( A \). This makes our work the first result in the on-policy FRL literature to achieve such a low communication cost. When $M=1$, our gap-dependent communication cost bound also provides a better global switching cost, removing the dependence on $SA$ from the $\log T$ term.

\bibliography{main.bib}
\bibliographystyle{our}

\newpage
\appendix
\onecolumn
In the appendix, we provide details for both FedQ-Hoeffding and FedQ-Bernstein algorithms.
\section{Algorithm Review}
\label{review}
\subsection{FedQ-Hoeffding Algorithm}
\label{Hoeffdinga}
In this section, we present more details for \Cref{1Hoeffding}.
Denote $\eta_t = \frac{H+1}{H+t}$, $\eta_0^0 = 1$, $\eta^t_0 = 0$ for $t\geq 1,$ and $\eta^t_i = \eta_i\prod_{i'=i+1}^t(1-\eta_{i'}), \forall \ 1\leq i\leq t$.  
We also denote $\eta^c(t_1,t_2) = \prod_{t=t_1}^{t_2}(1-\eta_t)$ for any positive integers $t_1<t_2$. After receiving the information from each agent $m$, for each triple $(s,a,h)$ visited by the agents, the server sets $\eta^{h,k}_{s,a} = 1-\eta^c\big(N_h^k(s,a)+1,N_h^{k+1}(s,a)\big)$ and $\beta^k_{s,a,h} = \sum_{t=t^{k-1}+1}^{t^k}\eta^{t^k}_tb_t$, where the confidence bound is given by $b_t = c\sqrt{\frac{H^3\iota}{t}}$ for some sufficiently large constant $c >0$. Then the server updates the $Q$-estimate according to the following two cases. 

\textbf{Case 1:} $N_h^k(s,a)< 2MH(H+1)=:i_0$. This case implies that each client can visit each $(s,a)$ pair at step $h$ at most once. Then, we denote $1\leq m_1<m_2\ldots < m_{{N_h^{k+1}-N_h^{k}}}\leq M$ as the agent indices with $n_{h}^{m,k}(s,a)>0$. The server then updates the global estimate of action values sequentially as follows:
    \begin{equation}\label{update_q_small_n1}
        Q_h^{k+1}(s,a)= (1-\eta_t)Q_h^k(s,a) + \eta_t\big(r_h(x,a) + v_{h+1}^{m_t,k}(s,a) + b_t\big),t = N_h^k(s,a)+1,\ldots  N_h^{k+1}(s,a).
    \end{equation}

 \textbf{Case 2:} $N_h^k(s,a)\geq i_0$. In this case, the central server calculates $$v_{h+1}^k(s,a) = \sum_{m=1}^Mv_{h+1}^{m,k}(s,a)/n_h^k(s,a)$$
and updates 
\begin{equation}\label{update_q_large_n1}
    Q_h^{k+1}(s,a)= (1-\eta^{h,k}_{s,a})Q_h^k(s,a)+\eta^{h,k}_{s,a}\left(r_h(s,a)+v_{h+1}^k(s,a)\right) + \beta^k_{s,a,h}.
\end{equation}


After finishing updating the estimated $Q$ function, the server updates the estimated value function and the policy as follows:
\begin{align}\label{rel_V_Q_est}
    V_h^{k+1}(s) = \min \Big\{H, \max _{a^{\prime} \in \mathcal{A}} Q_h^{k+1}(s, a^{\prime})\Big\},\ 
\pi_{h}^{k+1}(s) = \arg \max _{a^{\prime} \in \mathcal{A}} Q_h^{k+1}\left(s, a^{\prime}\right), \forall (s,h)\in \mathcal{S}\times [H]. 
 \end{align}
The details of the FedQ-Hoeffding algorithm are presented below.

	\begin{algorithm}[H]
		\caption{FedQ-Hoeffding (Central Server)}
		\label{alg_hoeffding_server}
		\begin{algorithmic}[1]
  \State {\bf Input:} $T_0 \in\mathbb{N}_+$.
    \State {\bf Initialization:} $k=1$, $N_h^1(s,a) = 0$, $Q_h^1(s,a) = V_h^1(s) = H$, $\forall (s,a,h)\in \sah$ and $\pi^1 = \left\{\pi_h^1: \mathcal{S} \rightarrow \mathcal{A}\right\}_{h \in[H]}$ is an arbitrary deterministic policy. 
   \While{$H\sum_{k'=1}^{k-1} Mn^{k'}< T_0$}
   \State Broadcast $\pi^k,$ $\{N_h^k(s,\pi_{h}^k(s))\}_{s,h}$ and $\{V_h^k(s)\}_{s,h}$ to all clients.
   \State Wait until receiving an abortion signal and send the signal to all agents.
   \State Receive $\{r_h(s,\pi_h^k(s))\}_{s,h}$,$\{n_h^{m,k}(s,\pi_h^k(s))\}_{s,h,m}$ and $\{v_{h+1}^{m,k}(s,\pi_h^k(s))\}_{s,h,m}$ from clients.
   \State Calculate $N_h^{k+1}(s,a), n_{h}^k(s,a),v_{h+1}^k(s,a),\forall (s,h)\in \mathcal{S}\times [H]$ with $a = \pi_h^k(s)$.
    \For{$(s,a,h)\in \sah$}
    \If{$a\neq \pi_h^k(s)$ \textnormal{ or } $n_h^k(s,a) = 0$}
    \State $Q_h^{k+1}(s,a)\leftarrow Q_h^{k}(s,a)$.
    \ElsIf{$N_h^k(s,a)<i_0$}
    \State Update $Q_h^{k+1}(s,a)$ according to \Cref{update_q_small_n1}.
    \Else
    \State Update $Q_h^{k+1}(s,a)$ according to \Cref{update_q_large_n1}.
    \EndIf
    \EndFor
    \State Update $V_h^{k+1}$ and $\pi^{k+1}$ by \cref{rel_V_Q_est}.
  \State $k\gets k+1$.
			\EndWhile
			
		\end{algorithmic}
	\end{algorithm}

 	\begin{algorithm}[H]
    
		\caption{FedQ-Hoeffding (Agent $m$ in round $k$)}
		\label{alg_hoeffding_agent}
		\begin{algorithmic}[1]
 \State $n_h^m(s,a)=v_{h+1}^m(s,a)=r_h(s,a)=0,\forall (s,a,h)\in \sah$.
   \State Receive $\pi^k,$ $\{N_h^k(s,\pi_{h}^k(s))\}_{s,h}$ and $\{V_h^k(s)\}_{s,h}$ from the central server. 
   \While{no abortion signal from the central server}
      \While{$n_h^{m}(s_h,a_h) < \max\left\{1,\lfloor\frac{1}{MH(H+1)}N_h^k(s_h, a_h)\rfloor\right\}, \forall (s,a,h)\in \sah$} 
   \State Collect a new trajectory $\{(s_h,a_h,r_h)\}_{h=1}^H$ with $a_h = \pi_h^k(s_h)$.
   \State $n_h^m(s_h,a_h)\leftarrow n_h^m(s_h,a_h) + 1,$ $v_{h+1}^m(s_h,a_h)\leftarrow v_{h+1}^m(s_h,a_h) + V_{h+1}^k(s_{h+1})$, 
 and ${r}_h(s_h,a_h)\leftarrow r_h,\forall h\in [H].$
   \EndWhile
   \State Send an abortion signal to the central server.
   \EndWhile
    \State $n_h^{m,k}(s,a)\leftarrow n_h^{m}(s,a), v_{h+1}^{m,k}(s,a)\leftarrow v_{h+1}^{m}(s,a),\forall (s,h)\in \mathcal{S}\times [H]$ with $a = \pi_h^k(s)$. 
    \State Send $\{r_h(s,\pi_h^k(s))\}_{s,h}$,$\{n_h^{m,k}(s,\pi_h^k(s))\}_{s,h}$ and $\{v_{h+1}^{m,k}(s,\pi_h^k(s))\}_{s,h}$ to the central server.
			
		\end{algorithmic}
	\end{algorithm}

\subsection{FedQ-Bernstein Algorithm}
\label{bernstein}
The Bernstein-type algorithm differs from the Hoeffding-type algorithm \Cref{alg_hoeffding_server,alg_hoeffding_agent}, in that it selects the upper confidence bound based on a variance estimator of $X_i$, akin to the approach used in the Bernstein-type algorithm in \citet{jin2018q}. In this subsection, we first review the algorithm design.

To facilitate understanding, we introduce additional notations exclusive to Bernstein-type algorithms, supplementing the already provided notations for the Hoeffding-type algorithm.
	$$\mu_{h}^{m,k}(s,a) = \frac{1}{n_h^{m,k}(s,a)}\sum_{j=1}^{n^{m,k}} \left[V_{h+1}^k\left(s_{h+1}^{k,j,m}\right)\right]^2\mathbb{I}[(s_h^{k,j,m},a_h^{k,j,m}) = (s,a)].$$
	$$\mu_{h}^k(s,a) = \frac{1}{N_h^{k+1}(s,a) - N_h^{k}(s,a)}\sum_{m=1}^M \mu_h^{m,k}(s,a)n_h^{m,k}(s,a).$$
	Here, $\mu_h^{m,k}(s,a)$ is the sample mean of $[V_{h+1}^k(s_{h+1}^{k,j,m})]^2$ for all the visits of $(s,a,h)$ for the $m$-th agent during the $k$-th round and $\mu_h^{k}(s,a)$ corresponds to the mean for all the visits during the $k$-th round. We define $W_k(s,a,h)$ to denote the sample variance of all the visits before the $k$-th round, i.e.
	$$W_k(s,a,h) = \frac{1}{N_h^{k}(s,a)}\sum_{i=1}^{N_h^{k}(s,a)} \left(V_{h+1}^{k^i}(s_{h+1}^{k^i,j^i,m^i}) - \frac{1}{N_h^k(s,a)}\sum_{i'=1}^{N_h^{k}(s,a)} V_{h+1}^{k^i}(s_{h+1}^{k^i,j^i,m^i})\right)^2.$$
	We can find that
	$$W_k(s,a,h) = \frac{1}{N_h^k(s,a)}\sum_{k'=1}^{k-1} \mu_{h}^{k'}(s,a)n_h^{k'}(s,a) - \left[\frac{1}{N_h^{k}(s,a)}\sum_{k'=1}^{k-1} v_{h+1}^{k'}(s,a)n_h^{k'}(s,a)\right]^2,$$
	which means that this quantity can be calculated efficiently in practice in the following way. Define
 \begin{equation}\label{eq_w1w2}
     W_{1,k}(s,a,h) = \sum_{k'=1}^{k-1} \mu_{h}^{k'}(s,a)n_h^{k'}(s,a),\ W_{2,k}(s,a,h) = \sum_{k'=1}^{k-1} v_{h+1}^{k'}(s,a)n_h^{k'}(s,a),
 \end{equation}
then we have
  \begin{equation}\label{eq_w1}
W_{1,k+1}(s,a,h) = W_{1,k}(s,a,h) + \mu_{h}^{k}(s,a)n_h^{k}(s,a),\ W_{2,k+1}(s,a,h) = W_{2,k}(s,a,h) + v_{h+1}^{k}(s,a)n_h^{k}(s,a)
 \end{equation}
and
     \begin{equation}\label{eq_w_w1w2}
W_{k+1}(s,a,h) = \frac{W_{1,k+1}(s,a,h)}{N_h^{k+1}(s,a)} - \left[\frac{W_{2,k+1}(s,a,h)}{N_h^{k+1}(s,a)}\right]^2.
 \end{equation}
  This indicates that the central server, by actively maintaining and updating the quantities $W_{1,k}$ and $W_{2,k}$ and systematically collecting $n_h^{m,k}$s, $\mu_h^{m,k}$s and $v_{h+1}^{m,k}$s, is able to compute $W_{k+1}$.
  
	Next, we define
    \begin{equation}
    \label{betab}
        \beta_t^{\textnormal{B}}(s,a,h) = c'\left(\min\left\{\sqrt{\frac{H\iota}{t}(W_{k^t+1}(s,a,h)+H)}+\iota\frac{\sqrt{H^7SA}+\sqrt{MSAH^{6}}}{t},\sqrt{\frac{H^3\iota}{t}}\right\}\right),
    \end{equation}
	in which $c'>0$ is a positive constant. With this, the upper confidence bound $b_t(s,a,h)$ for a single visit is determined by $\beta_t^{\textnormal{B}}(s,a,h) = 2\sum_{i=1}^t \eta^t_i b_t(s,a,h),$
	which can be calculated as follows:
	$$b_1(s, a, h):=\frac{\beta_1^{\textnormal{B}}(s, a, h)}{2},\ b_t(s, a, h):=\frac{\beta_t^{\textnormal{B}}(s, a, h)-\left(1-\eta_t\right) \beta_{t-1}^{\textnormal{B}}(s, a, h)}{2 \eta_t}.$$
 When there is no ambiguity, we use the simplified notation $\tilde{b}_t = b_t(s,a,h)$ and $\tilde{\beta}_t = \beta_t(s, a, h)$. In the FedQ-Bernstein algorithm, let $\tilde{\beta} = \beta_{t^k}^{\textnormal{B}}(s,a,h) - \eta^c(t^{k-1}+1,t^k)\beta_{t^{k-1}}^{\textnormal{B}}(s,a,h)$. Then similar to the FedQ-Hoeffding, we can updates the global estimate of the value functions according to the following two cases. 
\begin{itemize}[topsep=0pt, left=0pt] 
    \item \textbf{Case 1:} $N_h^k(s,a)< i_0$. This case implies that each client can visit each $(s,a)$ pair at step $h$ at most once. Then, we denote $1\leq m_1<m_2\ldots < m_{t^k-t^{k-1}}\leq M$ as the agent indices with $n_{h}^{m,k}(s,a)>0$. The server then updates the global estimate of action values as follows:
    \begin{equation}\label{update_q_small_n2}
        Q_h^{k+1}(s,a)= (1-\eta_t)Q_h^k(s,a) + \eta_t\left(r_h(x,a) + v_{h+1}^{m_t,k}(s,a) + \tilde{b}_t\right),t = t^{k-1}+1,\ldots t^k.
    \end{equation}
\item \textbf{Case 2:} $N_h^k(s,a)\geq i_0$. In this case, the central server calculates $$v_{h+1}^k(s,a) = \sum_{m=1}^Mv_{h+1}^{m,k}(s,a)/n_h^k(s,a)$$ and updates the $Q$-estimate as
\begin{equation}\label{update_q_large_n_b}
    Q_h^{k+1}(s,a)= (1-\eta^{h,k}_{s,a})Q_h^k(s,a)+\eta^{h,k}_{s,a}\left(r_h(s,a)+v_{h+1}^k(s,a)\right) + \tilde{\beta}/2.
\end{equation}
\end{itemize}
Then we can present the FedQ-Bernstein Algorithm in \citet{zheng2023federated}.
 \begin{algorithm}[H]
		\caption{FedQ-Bernstein (Central Server)}
		\label{alg_bernstein_server}
		\begin{algorithmic}[1]
  \State {\bf Input:} $T_0\in\mathbb{N}_+$.
    \State {\bf Initialization:} $k=1$, $N_h^1(s,a) = W_{1,k}(s,a,h) = W_{2,k}(s,a,h) = 0, Q_h^1(s,a) = V_h^1(s) = H, \forall (s,a,h)\in \sah$ and $\pi^1 = \left\{\pi_h^1: \mathcal{S} \rightarrow \mathcal{A}\right\}_{h \in[H]}$ is an arbitrary deterministic policy. 
   \While{$H\sum_{k'=1}^{k-1} Mn^{k'}< T_0$}
   \State Broadcast $\pi^k,$ $\{N_h^k(s,\pi_{h}^k(s))\}_{s,h}$ and $\{V_h^k(s)\}_{s,h}$ to all clients.
   \State Wait until receiving an abortion signal and send the signal to all agents.
   \State Receive $\{r_h(s,\pi_h^k(s))\}_{s,h}$, $\{n_h^{m,k}(s,\pi_h^k(s))\}_{s,h,m}$, $\{v_{h+1}^{m,k}(s,\pi_h^k(s))\}_{s,h,m}$, $\{\mu_{h}^{m,k}(s,\pi_h^k(s))\}_{s,h,m}$.
   \State Calculate $N_h^{k+1}(s,a), n_{h}^k(s,a),v_{h+1}^k(s,a),\forall (s,h)\in \mathcal{S}\times [H]$ with $a = \pi_h^k(s)$.
     \State Calculate $W_k(s,a,h),W_{k+1}(s,a,h),W_{1,k+1}(s,a,h),W_{2,k+1}(s,a,h),$ $\forall (s,h)\in \mathcal{S}\times [H]$ with $a = \pi_h^k(s)$ based on \Cref{eq_w1w2}, \Cref{eq_w1} and \Cref{eq_w_w1w2}.
    \For{$(s,a,h)\in \sah$}
    \If{$a\neq \pi_h^k(s)$ \textnormal{ or } $n_h^k(s,a) = 0$}
    \State $Q_h^{k+1}(s,a)\leftarrow Q_h^{k}(s,a)$.
    \Else
    \State Update $Q_h^{k+1}(s,a)$ according to \Cref{update_q_small_n2} or \Cref{update_q_large_n_b}.
    % \Else
    % \State Update $Q_h^{k+1}(s,a)$ according to \Cref{update_q_large_n_b}.
    \EndIf
    \EndFor
    \State 
    Update $V_h^{k+1}$ and $\pi^{k+1}$ by \Cref{rel_V_Q_est}.
  \State $k\gets k+1$.
			\EndWhile			
		\end{algorithmic}
	\end{algorithm}
    \vspace{-0.03in}
 	\begin{algorithm}[H]   
		\caption{FedQ-Bernstein (Agent $m$ in round $k$)}
		\label{alg_bernstein_agent}
		\begin{algorithmic}[1]
 \State $n_h^m(s,a)=v_{h+1}^m(s,a)=r_h(s,a)=\mu_{h}^m(s,a)=0,\forall (s,a,h)\in \sah$.
   \State Receive $\pi^k$, $\{N_h^k(s,\pi_{h}^k(s))\}_{s,h}$ and $\{V_h^k(s)\}_{s,h}$ from the central server. 
   \While{no abortion signal from the central server}
      \While{$n_h^{m}(x_h,a_h) < \max\left\{1,\lfloor\frac{1}{MH(H+1)}N_h^k(x_h, a_h)\rfloor\right\}, \forall (s,a,h)\in \sah$} 
   \State Collect a new trajectory $\{(x_h,a_h,r_h)\}_{h=1}^H$ with $a_h = \pi_h^k(x_h)$.
   \State $n_h^m(x_h,a_h)\leftarrow n_h^m(x_h,a_h) + 1,$ $v_{h+1}^m(x_h,a_h)\leftarrow v_{h+1}^m(x_h,a_h) + V_{h+1}^k(s_{h+1})$, $\mu_{h}^m(x_h,a_h)\leftarrow \mu_{h}^m(x_h,a_h) + \left[V_{h+1}^k(s_{h+1})\right]^2$,
 and ${r}_h(x_h,a_h)\leftarrow r_h,\forall h\in [H].$
   \EndWhile
   \State Send an abortion signal to the central server.
   \EndWhile
    \State $n_h^{m,k}(s,a)\leftarrow n_h^{m}(s,a), v_{h+1}^{m,k}(s,a)\leftarrow v_{h+1}^{m}(s,a)$ and $\mu_{h}^{m,k}(s,a)\leftarrow \mu_{h}^{m}(s,a)/n_h^{m}(s,a), \forall (s,h)\in \mathcal{S}\times [H]$ with $a = \pi_h^k(s)$.
    \State Send $\{r_h(s,\pi_h^k(s))\}_{s,h}$, $\{n_h^{m,k}(s,\pi_h^k(s))\}_{s,h}$, $\{\mu_{h}^{m,k}(s,\pi_h^k(s))\}_{s,h}$ and $\{v_{h+1}^{m,k}(s,\pi_h^k(s))\}_{s,h}$ to the central server.
			
		\end{algorithmic}
	\end{algorithm}
\end{document}
