\section{Related Work}
\label{related}
\textbf{On-policy RL for single agent finite-horizon tabular MDPs with worst-case regret.} There are mainly two types of algorithms for reinforcement learning: model-based and model-free learning. Model-based algorithms learn a model from past experience and make decisions based on this model, while model-free algorithms only maintain a group of value functions and take the induced optimal actions. Due to these differences, model-free algorithms are usually more space-efficient and time-efficient compared to model-based algorithms. However, model-based algorithms may achieve better learning performance by leveraging the learned model.

Next, we discuss the literature on model-based and model-free algorithms for finite-horizon tabular MDPs with worst-case regret. \citet{auer2008near}, \citet{agrawal2017optimistic}, \citet{azar2017minimax}, \citet{kakade2018variance}, \citet{agarwal2020model}, \citet{dann2019policy}, \citet{zanette2019tighter}, \citet{zhang2021reinforcement}, \citet{zhou2023sharp} and \citet{zhang2024settling} worked on model-based algorithms. Notably, \citet{zhang2024settling} provided an algorithm that achieves a regret of $\tilde{O}(\min \{\sqrt{SAH^2T},T\})$, which matches the information lower bound. \citet{jin2018q}, \citet{yang2021q}, \citet{zhang2020almost}, \citet{li2021breaking} and \citet{menard2021ucb} work on model-free algorithms. The latter three have introduced algorithms that achieve minimax regret of $\tilde{O}(\sqrt{SAH^2T})$.

\textbf{Suboptimality Gap.} When there is a strictly positive suboptimality gap, it is possible to achieve logarithmic regret
bounds. In RL, earlier work obtained asymptotic logarithmic regret bounds \citep{auer2007logarithmic,tewari2008optimistic}.
Recently, non-asymptotic logarithmic regret bounds were obtained \citep{jaksch2010near, ok2018exploration,simchowitz2019non, he2021logarithmic}. Specifically, \citet{jaksch2010near} developed a model-based algorithm, and their bound depends on the policy gap instead of the action gap studied in this paper. \citet{ok2018exploration} derived problem-specific logarithmic type lower bounds for both structured and unstructured MDPs. \citet{simchowitz2019non} extended the model-based algorithm by \citet{zanette2019tighter} and obtained logarithmic regret bounds. Logarithmic regret bounds are obtained in linear function approximation settings \citet{he2021logarithmic}. \citet{nguyen2023instance} also provides a gap-dependent regret bounds for offline RL with linear funciton approximation. 

Specifically, for model free algorithm, \citet{yang2021q} showed that the optimistic $Q$-learning algorithm by \citet{jin2018q} enjoyed a logarithmic regret $O(\frac{H^6SAT}{\dmin})$, which was subsequently refined by \citet{xu2021fine}. In their work, \citet{xu2021fine} introduced the Adaptive Multi-step Bootstrap (AMB) algorithm. \citet{zheng2024gap} further improved the logarithmic regret bound by leveraging the analysis of the UCB-Advantage algorithm \citep{zhang2020almost} and Q-EarlySettled-Advantage algorithm \citep{li2021breaking}.

There are also some other works focusing on gap-dependent sample complexity bounds
\citep{jonsson2020planning, marjani2020best, al2021navigating, tirinzoni2022near, wagenmaker2022beyond, wagenmaker2022instance, wang2022gap, tirinzoni2023optimistic}.

\textbf{Variance reduction in RL.} The reference-advantage decomposition used in \citet{zhang2020almost} and \citet{li2021breaking} is a technique of variance reduction that was originally proposed for finite-sum stochastic optimization  \citep{gower2020variance,johnson2013accelerating,nguyen2017sarah}. Later on, model-free RL algorithms also used variance reduction to improve the sample efficiency. For example, it was used in learning with generative models \citep{sidford2018near,sidford2023variance,wainwright2019variance}, policy evaluation \citep{du2017stochastic,khamaru2021temporal,wai2019variance,xu2020reanalysis}, offline RL \citep{shi2022pessimistic,yin2021near}, and $Q$-learning \citep{li2020sample,zhang2020almost,li2021breaking,yan2022efficacy}.

\textbf{RL with low switching cost and batched RL}. Research in RL with low-switching cost aims to minimize the number of policy switches while maintaining comparable regret bounds to fully adaptive counterparts, and it can be applied to federated RL. In batched RL \citep{perchet2016batched, gao2019batched}, the agent sets the number of batches and the length of each batch upfront, implementing an unchanged policy in a batch and aiming for fewer batches and lower regret. \citet{bai2019provably} first introduced the problem of RL with low-switching cost and proposed a $Q$-learning algorithm with lazy updates, achieving $\tilde{O}(SAH^3\log T)$ switching cost. This work was advanced by \citet{zhang2020almost}, which improved the regret upper bound and the switching cost. Additionally, \citet{wang2021provably} studied RL under the adaptivity constraint. 
Recently, \citet{qiao2022sample} proposed a model-based algorithm with $\tilde{O}(\log \log T)$ switching cost. \citet{zhang2022near} proposed a batched RL algorithm that is well-suited for the federated setting. 

\textbf{Multi-agent RL (MARL) with event-triggered communications.} We review a few recent works for on-policy MARL with linear function approximations. \citet{dubey2021provably} introduced Coop-LSVI for cooperative MARL. \citet{min2023cooperative} proposed an asynchronous version of LSVI-UCB that originates from \citet{jin2020provably}, matching the same regret bound with improved communication complexity compared to \citet{dubey2021provably}. \citet{hsu2024randomized} developed two algorithms that incorporate randomized exploration, achieving the same regret and communication complexity as \citet{min2023cooperative}. \citet{dubey2021provably}, \citet{min2023cooperative} and \citet{hsu2024randomized} employed event-triggered communication conditions based on determinants of certain quantities. Different from our federated algorithm, during the synchronization in \citet{dubey2021provably} and \citet{min2023cooperative}, local agents share original rewards or trajectories with the server. On the other hand, \citet{hsu2024randomized} reduces communication cost by sharing compressed statistics in the non-tabular setting with linear function approximation.

\textbf{Federated and distributed RL}. Existing literature on federated and distributed RL algorithms highlights various aspects. For value-based algorithms, \cite{guo2015concurrent}, \cite{zheng2023federated}, and \cite{woo2023blessing} focused on  linear speed up. \citep{agarwal2021communication} proposed a parallel RL algorithm with low communication cost. \cite{woo2023blessing} and \cite{woo2024federated} discussed the improved covering power of heterogeneity. \cite{wu2021byzantine} and \cite{chen2023byzantine} worked on robustness. Particularly, \cite{chen2023byzantine} proposed algorithms in both offline and online settings, obtaining near-optimal sample complexities and achieving superior robustness guarantees. In addition, several works have investigated value-based algorithms such as $Q$-learning in different settings, including \cite{beikmohammadi2024compressed}, \cite{jin2022federated}, \cite{khodadadian2022federated}, \cite{fan2023fedhql}, \cite{woo2023blessing}, and \cite{woo2024federated,anwar2021multi,zhao2023federated,yang2023federated,zhang2024finite}.  The convergence of decentralized temporal difference algorithms has been analyzed by \cite{doan2019finite}, \cite{doan2021finite}, \cite{chen2021multi}, \cite{sun2020finite}, \cite{wai2020convergence}, \cite{wang2020decentralized}, \cite{zeng2021finite}, and \cite{liu2023distributed}. 


Some other works focus on policy gradient-based algorithms. Communication-efficient policy gradient algorithms have been studied by \cite{fan2021fault} and \cite{chen2021communication}. \cite{lan2023improved} further shows the
simplicity compared to the other RL methods, and a linear speedup has been demonstrated in the synchronous setting. Optimal sample complexity for global optimality in federated RL, even in the presence of adversaries, is studied in \cite{ganesh2024global}. \citep{lan2024asynchronous} propose an algorithm to address the challenge of lagged policies in asynchronous settings.

The convergence of distributed actor-critic algorithms has been analyzed by \cite{shen2023towards} and \cite{chen2022sample}. Federated actor-learner architectures have been explored by \cite{assran2019gossip}, \cite{espeholt2018impala}, and \cite{mnih2016asynchronous}. Distributed inverse reinforcement learning has been examined by \cite{banerjee2021identity}, \cite{gong2023federated} and \cite{liu2022distributed, liu2023meta, liu2024learning, liutrajectory}.