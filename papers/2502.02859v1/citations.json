[
  {
    "index": 0,
    "papers": [
      {
        "key": "auer2008near",
        "author": "Auer, Peter and Jaksch, Thomas and Ortner, Ronald",
        "title": "Near-optimal regret bounds for reinforcement learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "agrawal2017optimistic",
        "author": "Agrawal, Shipra and Jia, Randy",
        "title": "Optimistic posterior sampling for reinforcement learning: worst-case regret bounds"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "azar2017minimax",
        "author": "Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\\'e}mi",
        "title": "Minimax regret bounds for reinforcement learning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "kakade2018variance",
        "author": "Kakade, Sham and Wang, Mengdi and Yang, Lin F",
        "title": "Variance reduction methods for sublinear reinforcement learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "agarwal2020model",
        "author": "Agarwal, Alekh and Kakade, Sham and Yang, Lin F",
        "title": "Model-based reinforcement learning with a generative model is minimax optimal"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dann2019policy",
        "author": "Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma",
        "title": "Policy certificates: Towards accountable reinforcement learning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zanette2019tighter",
        "author": "Zanette, Andrea and Brunskill, Emma",
        "title": "Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2021reinforcement",
        "author": "Zhang, Zihan and Ji, Xiangyang and Du, Simon",
        "title": "Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhou2023sharp",
        "author": "Zhou, Runlong and Zihan, Zhang and Du, Simon Shaolei",
        "title": "Sharp variance-dependent bounds in reinforcement learning: Best of both worlds in stochastic and deterministic environments"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhang2024settling",
        "author": "Zhang, Zihan and Chen, Yuxin and Lee, Jason D and Du, Simon S",
        "title": "Settling the sample complexity of online reinforcement learning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhang2024settling",
        "author": "Zhang, Zihan and Chen, Yuxin and Lee, Jason D and Du, Simon S",
        "title": "Settling the sample complexity of online reinforcement learning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "jin2018q",
        "author": "Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I",
        "title": "Is Q-learning provably efficient?"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "yang2021q",
        "author": "Yang, Kunhe and Yang, Lin and Du, Simon",
        "title": "Q-learning with logarithmic regret"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhang2020almost",
        "author": "Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang",
        "title": "Almost optimal model-free reinforcement learning via reference-advantage decomposition"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "li2021breaking",
        "author": "Li, Gen and Shi, Laixi and Chen, Yuxin and Gu, Yuantao and Chi, Yuejie",
        "title": "Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "menard2021ucb",
        "author": "M{\\'e}nard, Pierre and Domingues, Omar Darwiche and Shang, Xuedong and Valko, Michal",
        "title": "UCB Momentum Q-learning: Correcting the bias without forgetting"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "auer2007logarithmic",
        "author": "Auer, Peter and Ortner, Ronald",
        "title": "Logarithmic online regret bounds for undiscounted reinforcement learning"
      },
      {
        "key": "tewari2008optimistic",
        "author": "Tewari, Ambuj and Bartlett, Peter",
        "title": "Optimistic linear programming gives logarithmic regret for irreducible MDPs"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "jaksch2010near",
        "author": "Jaksch, Thomas and Ortner, Ronald and Auer, Peter",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "key": "ok2018exploration",
        "author": "Ok, Jungseul and Proutiere, Alexandre and Tranos, Damianos",
        "title": "Exploration in structured reinforcement learning"
      },
      {
        "key": "simchowitz2019non",
        "author": "Simchowitz, Max and Jamieson, Kevin G",
        "title": "Non-asymptotic gap-dependent regret bounds for tabular MDPs"
      },
      {
        "key": "he2021logarithmic",
        "author": "He, Jiafan and Zhou, Dongruo and Gu, Quanquan",
        "title": "Logarithmic regret for reinforcement learning with linear function approximation"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "jaksch2010near",
        "author": "Jaksch, Thomas and Ortner, Ronald and Auer, Peter",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "ok2018exploration",
        "author": "Ok, Jungseul and Proutiere, Alexandre and Tranos, Damianos",
        "title": "Exploration in structured reinforcement learning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "simchowitz2019non",
        "author": "Simchowitz, Max and Jamieson, Kevin G",
        "title": "Non-asymptotic gap-dependent regret bounds for tabular MDPs"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zanette2019tighter",
        "author": "Zanette, Andrea and Brunskill, Emma",
        "title": "Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "he2021logarithmic",
        "author": "He, Jiafan and Zhou, Dongruo and Gu, Quanquan",
        "title": "Logarithmic regret for reinforcement learning with linear function approximation"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "nguyen2023instance",
        "author": "Nguyen-Tang, Thanh and Yin, Ming and Gupta, Sunil and Venkatesh, Svetha and Arora, Raman",
        "title": "On instance-dependent bounds for offline reinforcement learning with linear function approximation"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "yang2021q",
        "author": "Yang, Kunhe and Yang, Lin and Du, Simon",
        "title": "Q-learning with logarithmic regret"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "jin2018q",
        "author": "Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I",
        "title": "Is Q-learning provably efficient?"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "xu2021fine",
        "author": "Xu, Haike and Ma, Tengyu and Du, Simon",
        "title": "Fine-grained gap-dependent bounds for tabular mdps via adaptive multi-step bootstrap"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "xu2021fine",
        "author": "Xu, Haike and Ma, Tengyu and Du, Simon",
        "title": "Fine-grained gap-dependent bounds for tabular mdps via adaptive multi-step bootstrap"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "zheng2024gap",
        "author": "Zheng, Zhong and Zhang, Haochen and Xue, Lingzhou",
        "title": "Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "zhang2020almost",
        "author": "Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang",
        "title": "Almost optimal model-free reinforcement learning via reference-advantage decomposition"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "li2021breaking",
        "author": "Li, Gen and Shi, Laixi and Chen, Yuxin and Gu, Yuantao and Chi, Yuejie",
        "title": "Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "jonsson2020planning",
        "author": "Jonsson, Anders and Kaufmann, Emilie and M{\\'e}nard, Pierre and Darwiche Domingues, Omar and Leurent, Edouard and Valko, Michal",
        "title": "Planning in markov decision processes with gap-dependent sample complexity"
      },
      {
        "key": "marjani2020best",
        "author": "Marjani, AA and Proutiere, Alexandre",
        "title": "Best policy identification in discounted mdps: Problem-specific sample complexity"
      },
      {
        "key": "al2021navigating",
        "author": "Al Marjani, Aymen and Garivier, Aur{\\'e}lien and Proutiere, Alexandre",
        "title": "Navigating to the best policy in markov decision processes"
      },
      {
        "key": "tirinzoni2022near",
        "author": "Tirinzoni, Andrea and Al Marjani, Aymen and Kaufmann, Emilie",
        "title": "Near instance-optimal pac reinforcement learning for deterministic mdps"
      },
      {
        "key": "wagenmaker2022beyond",
        "author": "Wagenmaker, Andrew J and Simchowitz, Max and Jamieson, Kevin",
        "title": "Beyond no regret: Instance-dependent pac reinforcement learning"
      },
      {
        "key": "wagenmaker2022instance",
        "author": "Wagenmaker, Andrew and Jamieson, Kevin G",
        "title": "Instance-dependent near-optimal policy identification in linear mdps via online experiment design"
      },
      {
        "key": "wang2022gap",
        "author": "Wang, Xinqi and Cui, Qiwen and Du, Simon S",
        "title": "On gap-dependent bounds for offline reinforcement learning"
      },
      {
        "key": "tirinzoni2023optimistic",
        "author": "Tirinzoni, Andrea and Al-Marjani, Aymen and Kaufmann, Emilie",
        "title": "Optimistic pac reinforcement learning: the instance-dependent view"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "zhang2020almost",
        "author": "Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang",
        "title": "Almost optimal model-free reinforcement learning via reference-advantage decomposition"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "li2021breaking",
        "author": "Li, Gen and Shi, Laixi and Chen, Yuxin and Gu, Yuantao and Chi, Yuejie",
        "title": "Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "gower2020variance",
        "author": "Gower, Robert M and Schmidt, Mark and Bach, Francis and Richt{\\'a}rik, Peter",
        "title": "Variance-reduced methods for machine learning"
      },
      {
        "key": "johnson2013accelerating",
        "author": "Johnson, Rie and Zhang, Tong",
        "title": "Accelerating stochastic gradient descent using predictive variance reduction"
      },
      {
        "key": "nguyen2017sarah",
        "author": "Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\\'a}{\\v{c}}, Martin",
        "title": "SARAH: A novel method for machine learning problems using stochastic recursive gradient"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "sidford2018near",
        "author": "Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin and Ye, Yinyu",
        "title": "Near-optimal time and sample complexities for solving Markov decision processes with a generative model"
      },
      {
        "key": "sidford2023variance",
        "author": "Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu",
        "title": "Variance reduced value iteration and faster algorithms for solving Markov decision processes"
      },
      {
        "key": "wainwright2019variance",
        "author": "Wainwright, Martin J",
        "title": "Variance-reduced $ Q $-learning is minimax optimal"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "du2017stochastic",
        "author": "Du, Simon S and Chen, Jianshu and Li, Lihong and Xiao, Lin and Zhou, Dengyong",
        "title": "Stochastic variance reduction methods for policy evaluation"
      },
      {
        "key": "khamaru2021temporal",
        "author": "Khamaru, Koulik and Pananjady, Ashwin and Ruan, Feng and Wainwright, Martin J and Jordan, Michael I",
        "title": "Is temporal difference learning optimal? an instance-dependent analysis"
      },
      {
        "key": "wai2019variance",
        "author": "Wai, Hoi-To and Hong, Mingyi and Yang, Zhuoran and Wang, Zhaoran and Tang, Kexin",
        "title": "Variance reduced policy evaluation with smooth function approximation"
      },
      {
        "key": "xu2020reanalysis",
        "author": "Xu, Tengyu and Wang, Zhe and Zhou, Yi and Liang, Yingbin",
        "title": "Reanalysis of variance reduced temporal difference learning"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "shi2022pessimistic",
        "author": "Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Chi, Yuejie",
        "title": "Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity"
      },
      {
        "key": "yin2021near",
        "author": "Yin, Ming and Bai, Yu and Wang, Yu-Xiang",
        "title": "Near-optimal offline reinforcement learning via double variance reduction"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "li2020sample",
        "author": "Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin",
        "title": "Sample complexity of asynchronous Q-learning: Sharper analysis and variance reduction"
      },
      {
        "key": "zhang2020almost",
        "author": "Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang",
        "title": "Almost optimal model-free reinforcement learning via reference-advantage decomposition"
      },
      {
        "key": "li2021breaking",
        "author": "Li, Gen and Shi, Laixi and Chen, Yuxin and Gu, Yuantao and Chi, Yuejie",
        "title": "Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning"
      },
      {
        "key": "yan2022efficacy",
        "author": "Yan, Yuling and Li, Gen and Chen, Yuxin and Fan, Jianqing",
        "title": "The efficacy of pessimism in asynchronous Q-learning"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "perchet2016batched",
        "author": "Perchet, Vianney and Rigollet, Philippe and Chassang, Sylvain and Snowberg, Erik",
        "title": "Batched bandit problems"
      },
      {
        "key": "gao2019batched",
        "author": "Gao, Zijun and Han, Yanjun and Ren, Zhimei and Zhou, Zhengqing",
        "title": "Batched multi-armed bandits problem"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "bai2019provably",
        "author": "Bai, Yu and Xie, Tengyang and Jiang, Nan and Wang, Yu-Xiang",
        "title": "Provably efficient q-learning with low switching cost"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "zhang2020almost",
        "author": "Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang",
        "title": "Almost optimal model-free reinforcement learning via reference-advantage decomposition"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "wang2021provably",
        "author": "Wang, Tianhao and Zhou, Dongruo and Gu, Quanquan",
        "title": "Provably efficient reinforcement learning with linear function approximation under adaptivity constraints"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "qiao2022sample",
        "author": "Qiao, Dan and Yin, Ming and Min, Ming and Wang, Yu-Xiang",
        "title": "Sample-efficient reinforcement learning with loglog (t) switching cost"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "zhang2022near",
        "author": "Zhang, Zihan and Jiang, Yuhang and Zhou, Yuan and Ji, Xiangyang",
        "title": "Near-optimal regret bounds for multi-batch reinforcement learning"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "dubey2021provably",
        "author": "Dubey, Abhimanyu and Pentland, Alex",
        "title": "Provably efficient cooperative multi-agent reinforcement learning with function approximation"
      }
    ]
  },
  {
    "index": 46,
    "papers": [
      {
        "key": "min2023cooperative",
        "author": "Min, Yifei and He, Jiafan and Wang, Tianhao and Gu, Quanquan",
        "title": "Cooperative multi-agent reinforcement learning: Asynchronous communication and linear function approximation"
      }
    ]
  },
  {
    "index": 47,
    "papers": [
      {
        "key": "jin2020provably",
        "author": "Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I",
        "title": "Provably efficient reinforcement learning with linear function approximation"
      }
    ]
  },
  {
    "index": 48,
    "papers": [
      {
        "key": "dubey2021provably",
        "author": "Dubey, Abhimanyu and Pentland, Alex",
        "title": "Provably efficient cooperative multi-agent reinforcement learning with function approximation"
      }
    ]
  },
  {
    "index": 49,
    "papers": [
      {
        "key": "hsu2024randomized",
        "author": "Hsu, Hao-Lun and Wang, Weixin and Pajic, Miroslav and Xu, Pan",
        "title": "Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning"
      }
    ]
  },
  {
    "index": 50,
    "papers": [
      {
        "key": "min2023cooperative",
        "author": "Min, Yifei and He, Jiafan and Wang, Tianhao and Gu, Quanquan",
        "title": "Cooperative multi-agent reinforcement learning: Asynchronous communication and linear function approximation"
      }
    ]
  },
  {
    "index": 51,
    "papers": [
      {
        "key": "dubey2021provably",
        "author": "Dubey, Abhimanyu and Pentland, Alex",
        "title": "Provably efficient cooperative multi-agent reinforcement learning with function approximation"
      }
    ]
  },
  {
    "index": 52,
    "papers": [
      {
        "key": "min2023cooperative",
        "author": "Min, Yifei and He, Jiafan and Wang, Tianhao and Gu, Quanquan",
        "title": "Cooperative multi-agent reinforcement learning: Asynchronous communication and linear function approximation"
      }
    ]
  },
  {
    "index": 53,
    "papers": [
      {
        "key": "hsu2024randomized",
        "author": "Hsu, Hao-Lun and Wang, Weixin and Pajic, Miroslav and Xu, Pan",
        "title": "Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning"
      }
    ]
  },
  {
    "index": 54,
    "papers": [
      {
        "key": "dubey2021provably",
        "author": "Dubey, Abhimanyu and Pentland, Alex",
        "title": "Provably efficient cooperative multi-agent reinforcement learning with function approximation"
      }
    ]
  },
  {
    "index": 55,
    "papers": [
      {
        "key": "min2023cooperative",
        "author": "Min, Yifei and He, Jiafan and Wang, Tianhao and Gu, Quanquan",
        "title": "Cooperative multi-agent reinforcement learning: Asynchronous communication and linear function approximation"
      }
    ]
  },
  {
    "index": 56,
    "papers": [
      {
        "key": "hsu2024randomized",
        "author": "Hsu, Hao-Lun and Wang, Weixin and Pajic, Miroslav and Xu, Pan",
        "title": "Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning"
      }
    ]
  },
  {
    "index": 57,
    "papers": [
      {
        "key": "guo2015concurrent",
        "author": "Guo, Zhaohan and Brunskill, Emma",
        "title": "Concurrent pac rl"
      }
    ]
  },
  {
    "index": 58,
    "papers": [
      {
        "key": "zheng2023federated",
        "author": "Zhong Zheng and Fengyu Gao and Lingzhou Xue and Jing Yang",
        "title": "Federated Q-Learning: Linear Regret Speedup with Low Communication Cost"
      }
    ]
  },
  {
    "index": 59,
    "papers": [
      {
        "key": "woo2023blessing",
        "author": "Woo, Jiin and Joshi, Gauri and Chi, Yuejie",
        "title": "The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond"
      }
    ]
  },
  {
    "index": 60,
    "papers": [
      {
        "key": "agarwal2021communication",
        "author": "Agarwal, Mridul and Ganguly, Bhargav and Aggarwal, Vaneet",
        "title": "Communication efficient parallel reinforcement learning"
      }
    ]
  },
  {
    "index": 61,
    "papers": [
      {
        "key": "woo2023blessing",
        "author": "Woo, Jiin and Joshi, Gauri and Chi, Yuejie",
        "title": "The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond"
      }
    ]
  },
  {
    "index": 62,
    "papers": [
      {
        "key": "woo2024federated",
        "author": "Woo, Jiin and Shi, Laixi and Joshi, Gauri and Chi, Yuejie",
        "title": "Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices"
      }
    ]
  },
  {
    "index": 63,
    "papers": [
      {
        "key": "wu2021byzantine",
        "author": "Wu, Zhaoxian and Shen, Han and Chen, Tianyi and Ling, Qing",
        "title": "Byzantine-resilient decentralized policy evaluation with linear function approximation"
      }
    ]
  },
  {
    "index": 64,
    "papers": [
      {
        "key": "chen2023byzantine",
        "author": "Chen, Yiding and Zhang, Xuezhou and Zhang, Kaiqing and Wang, Mengdi and Zhu, Xiaojin",
        "title": "Byzantine-robust online and offline distributed reinforcement learning"
      }
    ]
  },
  {
    "index": 65,
    "papers": [
      {
        "key": "chen2023byzantine",
        "author": "Chen, Yiding and Zhang, Xuezhou and Zhang, Kaiqing and Wang, Mengdi and Zhu, Xiaojin",
        "title": "Byzantine-robust online and offline distributed reinforcement learning"
      }
    ]
  },
  {
    "index": 66,
    "papers": [
      {
        "key": "beikmohammadi2024compressed",
        "author": "Beikmohammadi, Ali and Khirirat, Sarit and Magn{\\'u}sson, Sindri",
        "title": "Compressed Federated Reinforcement Learning with a Generative Model"
      }
    ]
  },
  {
    "index": 67,
    "papers": [
      {
        "key": "jin2022federated",
        "author": "Jin, Hao and Peng, Yang and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua",
        "title": "Federated reinforcement learning with environment heterogeneity"
      }
    ]
  },
  {
    "index": 68,
    "papers": [
      {
        "key": "khodadadian2022federated",
        "author": "Khodadadian, Sajad and Sharma, Pranay and Joshi, Gauri and Maguluri, Siva Theja",
        "title": "Federated reinforcement learning: Linear speedup under markovian sampling"
      }
    ]
  },
  {
    "index": 69,
    "papers": [
      {
        "key": "fan2023fedhql",
        "author": "Fan, Flint Xiaofeng and Ma, Yining and Dai, Zhongxiang and Tan, Cheston and Low, Bryan Kian Hsiang",
        "title": "FedHQL: Federated Heterogeneous Q-Learning"
      }
    ]
  },
  {
    "index": 70,
    "papers": [
      {
        "key": "woo2023blessing",
        "author": "Woo, Jiin and Joshi, Gauri and Chi, Yuejie",
        "title": "The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond"
      }
    ]
  },
  {
    "index": 71,
    "papers": [
      {
        "key": "woo2024federated",
        "author": "Woo, Jiin and Shi, Laixi and Joshi, Gauri and Chi, Yuejie",
        "title": "Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices"
      },
      {
        "key": "anwar2021multi",
        "author": "Anwar, Aqeel and Raychowdhury, Arijit",
        "title": "Multi-task federated reinforcement learning with adversaries"
      },
      {
        "key": "zhao2023federated",
        "author": "Zhao, Fangyuan and Ren, Xuebin and Yang, Shusen and Zhao, Peng and Zhang, Rui and Xu, Xinxin",
        "title": "Federated multi-objective reinforcement learning"
      },
      {
        "key": "yang2023federated",
        "author": "Yang, Tong and Cen, Shicong and Wei, Yuting and Chen, Yuxin and Chi, Yuejie",
        "title": "Federated natural policy gradient methods for multi-task reinforcement learning"
      },
      {
        "key": "zhang2024finite",
        "author": "Zhang, Chenyu and Wang, Han and Mitra, Aritra and Anderson, James",
        "title": "Finite-time analysis of on-policy heterogeneous federated reinforcement learning"
      }
    ]
  },
  {
    "index": 72,
    "papers": [
      {
        "key": "doan2019finite",
        "author": "Doan, Thinh and Maguluri, Siva and Romberg, Justin",
        "title": "Finite-time analysis of distributed TD (0) with linear function approximation on multi-agent reinforcement learning"
      }
    ]
  },
  {
    "index": 73,
    "papers": [
      {
        "key": "doan2021finite",
        "author": "Doan, Thinh T and Maguluri, Siva Theja and Romberg, Justin",
        "title": "Finite-time performance of distributed temporal-difference learning with linear function approximation"
      }
    ]
  },
  {
    "index": 74,
    "papers": [
      {
        "key": "chen2021multi",
        "author": "Chen, Ziyi and Zhou, Yi and Chen, Rongrong",
        "title": "Multi-Agent Off-Policy TDC with Near-Optimal Sample and Communication Complexity"
      }
    ]
  },
  {
    "index": 75,
    "papers": [
      {
        "key": "sun2020finite",
        "author": "Sun, Jun and Wang, Gang and Giannakis, Georgios B and Yang, Qinmin and Yang, Zaiyue",
        "title": "Finite-time analysis of decentralized temporal-difference learning with linear function approximation"
      }
    ]
  },
  {
    "index": 76,
    "papers": [
      {
        "key": "wai2020convergence",
        "author": "Wai, Hoi-To",
        "title": "On the convergence of consensus algorithms with markovian noise and gradient bias"
      }
    ]
  },
  {
    "index": 77,
    "papers": [
      {
        "key": "wang2020decentralized",
        "author": "Wang, Gang and Lu, Songtao and Giannakis, Georgios and Tesauro, Gerald and Sun, Jian",
        "title": "Decentralized TD tracking with linear function approximation and its finite-time analysis"
      }
    ]
  },
  {
    "index": 78,
    "papers": [
      {
        "key": "zeng2021finite",
        "author": "Zeng, Sihan and Doan, Thinh T and Romberg, Justin",
        "title": "Finite-time analysis of decentralized stochastic approximation with applications in multi-agent and multi-task learning"
      }
    ]
  },
  {
    "index": 79,
    "papers": [
      {
        "key": "liu2023distributed",
        "author": "Liu, Rui and Olshevsky, Alex",
        "title": "Distributed td (0) with almost no communication"
      }
    ]
  },
  {
    "index": 80,
    "papers": [
      {
        "key": "fan2021fault",
        "author": "Fan, Flint Xiaofeng and Ma, Yining and Dai, Zhongxiang and Jing, Wei and Tan, Cheston and Low, Bryan Kian Hsiang",
        "title": "Fault-tolerant federated reinforcement learning with theoretical guarantee"
      }
    ]
  },
  {
    "index": 81,
    "papers": [
      {
        "key": "chen2021communication",
        "author": "Chen, Tianyi and Zhang, Kaiqing and Giannakis, Georgios B and Ba{\\c{s}}ar, Tamer",
        "title": "Communication-efficient policy gradient methods for distributed reinforcement learning"
      }
    ]
  },
  {
    "index": 82,
    "papers": [
      {
        "key": "lan2023improved",
        "author": "Lan, Guangchen and Wang, Han and Anderson, James and Brinton, Christopher and Aggarwal, Vaneet",
        "title": "Improved communication efficiency in federated natural policy gradient via admm-based gradient updates"
      }
    ]
  },
  {
    "index": 83,
    "papers": [
      {
        "key": "ganesh2024global",
        "author": "Ganesh, Swetha and Chen, Jiayu and Thoppe, Gugan and Aggarwal, Vaneet",
        "title": "Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries"
      }
    ]
  },
  {
    "index": 84,
    "papers": [
      {
        "key": "lan2024asynchronous",
        "author": "Lan, Guangchen and Han, Dong-Jun and Hashemi, Abolfazl and Aggarwal, Vaneet and Brinton, Christopher G",
        "title": "Asynchronous federated reinforcement learning with policy gradient updates: Algorithm design and convergence analysis"
      }
    ]
  },
  {
    "index": 85,
    "papers": [
      {
        "key": "shen2023towards",
        "author": "Shen, Han and Zhang, Kaiqing and Hong, Mingyi and Chen, Tianyi",
        "title": "Towards Understanding Asynchronous Advantage Actor-critic: Convergence and Linear Speedup"
      }
    ]
  },
  {
    "index": 86,
    "papers": [
      {
        "key": "chen2022sample",
        "author": "Chen, Ziyi and Zhou, Yi and Chen, Rong-Rong and Zou, Shaofeng",
        "title": "Sample and communication-efficient decentralized actor-critic algorithms with finite-time analysis"
      }
    ]
  },
  {
    "index": 87,
    "papers": [
      {
        "key": "assran2019gossip",
        "author": "Assran, Mahmoud and Romoff, Joshua and Ballas, Nicolas and Pineau, Joelle and Rabbat, Michael",
        "title": "Gossip-based actor-learner architectures for deep reinforcement learning"
      }
    ]
  },
  {
    "index": 88,
    "papers": [
      {
        "key": "espeholt2018impala",
        "author": "Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others",
        "title": "Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures"
      }
    ]
  },
  {
    "index": 89,
    "papers": [
      {
        "key": "mnih2016asynchronous",
        "author": "Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray",
        "title": "Asynchronous methods for deep reinforcement learning"
      }
    ]
  },
  {
    "index": 90,
    "papers": [
      {
        "key": "banerjee2021identity",
        "author": "Banerjee, Soumya and Bouzefrane, Samia and Abane, Amar",
        "title": "Identity management with hybrid blockchain approach: A deliberate extension with federated-inverse-reinforcement learning"
      }
    ]
  },
  {
    "index": 91,
    "papers": [
      {
        "key": "gong2023federated",
        "author": "Gong, Wei and Cao, Linxiao and Zhu, Yifei and Zuo, Fang and He, Xin and Zhou, Haoquan",
        "title": "Federated inverse reinforcement learning for smart icus with differential privacy"
      }
    ]
  },
  {
    "index": 92,
    "papers": [
      {
        "key": "liu2022distributed",
        "author": "Liu, Shicheng and Zhu, Minghui",
        "title": "Distributed inverse constrained reinforcement learning for multi-agent systems"
      },
      {
        "key": "liu2023meta",
        "author": "Liu, Shicheng and Zhu, Minghui",
        "title": "Meta inverse constrained reinforcement learning: Convergence guarantee and generalization analysis"
      },
      {
        "key": "liu2024learning",
        "author": "Liu, Shicheng and Zhu, Minghui",
        "title": "Learning Multi-agent Behaviors from Distributed and Streaming Demonstrations"
      },
      {
        "key": "liutrajectory",
        "author": "Liu, Shicheng and Zhu, Minghui",
        "title": "In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before an Ongoing Trajectory Terminates"
      }
    ]
  }
]