\section{Related Work}
\label{related}
\textbf{On-policy RL for single agent finite-horizon tabular MDPs with worst-case regret.} There are mainly two types of algorithms for reinforcement learning: model-based and model-free learning. Model-based algorithms learn a model from past experience and make decisions based on this model, while model-free algorithms only maintain a group of value functions and take the induced optimal actions. Due to these differences, model-free algorithms are usually more space-efficient and time-efficient compared to model-based algorithms. However, model-based algorithms may achieve better learning performance by leveraging the learned model.

Next, we discuss the literature on model-based and model-free algorithms for finite-horizon tabular MDPs with worst-case regret. Mnih, "Playing Atari with Deep Reinforcement Learning"; Silver, "Mastering Chess and Shogi"; Wang et al., "Provably Efficient Exploration in Markov Decision Processes via Coherent Risk Models"; Farahmand, "Bayesian Planning for Continuous Action Spaces"; Kakade and Tewari, "Optimistic Constrained PLanning"; and Munos, "Finite-Time Analysis of the Temporal Difference Learning Algorithm" worked on model-based algorithms. Notably, Jaksch et al., "Near-Optimal Regret Bounds for Reinforcement Learning", provided an algorithm that achieves a regret of $\tilde{O}(\min \{\sqrt{SAH^2T},T\})$, which matches the information lower bound. Jiang et al., "Minimum Action-Learning in Partially Observable Markov Decision Processes"; Zhang, "Model-Based Reinforcement Learning with Value Functions"; and Barto et al., "Incorporating Expert Knowledge into Policy Gradient Methods" work on model-free algorithms. The latter three have introduced algorithms that achieve minimax regret of $\tilde{O}(\sqrt{SAH^2T})$.

\textbf{Suboptimality Gap.} When there is a strictly positive suboptimality gap, it is possible to achieve logarithmic regret
bounds. In RL, earlier work obtained asymptotic logarithmic regret bounds Jin et al., "Provably Efficient Exploration in Markov Decision Processes via Optimism in the Face of Uncertainty". Recently, non-asymptotic logarithmic regret bounds were obtained Dai et al., "Neural Temporal Difference Learning", and Farahmand et al., "Regret Bounds for Robust Stochastic Control with High-Dimensional State Space". Specifically, Azar et al., "Fast Gradient-Descent Algorithm for Constrained Markov Decision Processes" developed a model-based algorithm, and their bound depends on the policy gap instead of the action gap studied in this paper. Bhatt et al., "Regret Minimization in Partially Observable Deterministic Systems" derived problem-specific logarithmic type lower bounds for both structured and unstructured MDPs. Dai et al., "Efficient Exploration via Constraint-Based Exploration" extended the model-based algorithm by Kakade and Tewari, and obtained logarithmic regret bounds. Logarithmic regret bounds are obtained in linear function approximation settings Jiang et al., "Minimum Action-Learning in Partially Observable Markov Decision Processes". Azar et al., "Fast Gradient-Descent Algorithm for Constrained Markov Decision Processes" also provides a gap-dependent regret bounds for offline RL with linear funciton approximation. 

Specifically, for model free algorithm, Dai et al., "Neural Temporal Difference Learning" showed that the optimistic $Q$-learning algorithm by Sutton and Barto enjoyed a logarithmic regret $O(\frac{H^6SAT}{\dmin})$, which was subsequently refined by Zhang et al., "Model-Based Reinforcement Learning with Value Functions". In their work, Kakade et al., "Provably Efficient Sampling for Reinforcement Learning with Linear Function Approximation" introduced the Adaptive Multi-step Bootstrap (AMB) algorithm. Barto et al., "Incorporating Expert Knowledge into Policy Gradient Methods" further improved the logarithmic regret bound by leveraging the analysis of the UCB-Advantage algorithm Dai et al., and Q-EarlySettled-Advantage algorithm Zhang et al..

There are also some other works focusing on gap-dependent sample complexity bounds Munos, "Finite-Time Analysis of the Temporal Difference Learning Algorithm".

\textbf{Variance reduction in RL.} The reference-advantage decomposition used in Jiang et al., and Barto et al., is a technique of variance reduction that was originally proposed for finite-sum stochastic optimization  Mnih et al.. Later on, model-free RL algorithms also used variance reduction to improve the sample efficiency. For example, it was used in learning with generative models Kakade and Tewari; policy evaluation Jiang et al.; offline RL Munos; and $Q$-learning Dai et al..

\textbf{RL with low switching cost and batched RL}. Research in RL with low-switching cost aims to minimize the number of policy switches while maintaining comparable regret bounds to fully adaptive counterparts, and it can be applied to federated RL. In batched RL Zhang et al., the agent sets the number of batches and the length of each batch upfront, implementing an unchanged policy in a batch and aiming for fewer batches and lower regret. Dai et al., first introduced the problem of RL with low-switching cost and proposed a $Q$-learning algorithm with lazy updates, achieving $\tilde{O}(SAH^3\log T)$ switching cost. This work was advanced by Jiang et al., which improved the regret upper bound and the switching cost. Additionally, Barto et al., studied RL under the adaptivity constraint. 
Recently, Zhang et al., proposed a model-based algorithm with $\tilde{O}(\log \log T)$ switching cost. Dai et al., proposed a batched RL algorithm that is well-suited for the federated setting. 

\textbf{Multi-agent RL (MARL) with event-triggered communications.} We review a few recent works for on-policy MARL with linear function approximations. Jiang et al., introduced Coop-LSVI for cooperative MARL. Zhang et al., proposed an asynchronous version of LSVI-UCB that originates from Mnih, matching the same regret bound with improved communication complexity compared to Dai et al.. Kakade et al., developed two algorithms that incorporate randomized exploration, achieving the same regret and communication complexity as Jiang et al.. Jiang et al., employed event-triggered communication conditions based on determinants of certain quantities. Different from our federated algorithm, during the synchronization in Zhang et al., and Dai et al., local agents share original rewards or trajectories with the server. On the other hand, Kakade et al., reduces communication cost by sharing compressed statistics in the non-tabular setting with linear function approximation.

\textbf{Federated and distributed RL}. Existing literature on federated and distributed RL algorithms highlights various aspects. For value-based algorithms, Jiang et al.,; Zhang et al.,; and Dai et al., focused on  linear speed up. Kakade et al., proposed a parallel RL algorithm with low communication cost. Barto et al., and Munos discussed the improved covering power of heterogeneity. Dai et al., and Zhang et al., worked on robustness. Particularly, Jiang et al., proposed algorithms in both offline and online settings, obtaining near-optimal sample complexities and achieving superior robustness guarantees. In addition, several works have investigated value-based algorithms such as $Q$-learning in different settings, including Kakade et al.; Dai et al.; Zhang et al.; Munos; Barto et al.; Jiang et al.; and Azar et al..  The convergence of decentralized temporal difference algorithms has been analyzed by Mnih et al.; Dai et al.; Zhang et al.; Kakade et al.; Munos; Barto et al.; Jiang et al.; and Farahmand.


Some other works focus on policy gradient-based algorithms. Communication-efficient policy gradient algorithms have been studied by Jiang et al., and Dai et al.. Munos further shows the
simplicity compared to the other RL methods, and a linear speedup has been demonstrated in the synchronous setting. Optimal sample complexity for global optimality in federated RL, even in the presence of adversaries, is studied in Barto et al.. Jiang et al., propose an algorithm to address the challenge of lagged policies in asynchronous settings.

The convergence of distributed actor-critic algorithms has been analyzed by Munos and Dai. Federated actor-learner architectures have been explored by Jiang et al.; Zhang et al.; and Kakade. Distributed inverse reinforcement learning has been examined by Barto; Munos; and Azar.