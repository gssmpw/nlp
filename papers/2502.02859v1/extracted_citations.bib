@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}

@inproceedings{agarwal2021communication,
  title={Communication efficient parallel reinforcement learning},
  author={Agarwal, Mridul and Ganguly, Bhargav and Aggarwal, Vaneet},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={247--256},
  year={2021},
  organization={PMLR}
}

@article{agrawal2017optimistic,
  title={Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
  author={Agrawal, Shipra and Jia, Randy},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{al2021navigating,
  title={Navigating to the best policy in markov decision processes},
  author={Al Marjani, Aymen and Garivier, Aur{\'e}lien and Proutiere, Alexandre},
  booktitle={Advances in Neural Information Processing Systems},
  pages={25852--25864},
  year={2021}
}

@article{anwar2021multi,
  title={Multi-task federated reinforcement learning with adversaries},
  author={Anwar, Aqeel and Raychowdhury, Arijit},
  journal={arXiv preprint arXiv:2103.06473},
  year={2021}
}

@article{assran2019gossip,
  title={Gossip-based actor-learner architectures for deep reinforcement learning},
  author={Assran, Mahmoud and Romoff, Joshua and Ballas, Nicolas and Pineau, Joelle and Rabbat, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@incollection{auer2007logarithmic,
  title={Logarithmic online regret bounds for undiscounted reinforcement learning},
  author={Auer, Peter and Ortner, Ronald},
  booktitle={Advances in Neural Information Processing Systems},
  pages={49--56},
  year={2007},
  publisher={MIT Press}
}

@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in Neural Information Processing Systems},
  volume={21},
  year={2008}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@article{bai2019provably,
  title={Provably efficient q-learning with low switching cost},
  author={Bai, Yu and Xie, Tengyang and Jiang, Nan and Wang, Yu-Xiang},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{banerjee2021identity,
  title={Identity management with hybrid blockchain approach: A deliberate extension with federated-inverse-reinforcement learning},
  author={Banerjee, Soumya and Bouzefrane, Samia and Abane, Amar},
  booktitle={2021 IEEE 22nd International Conference on High Performance Switching and Routing (HPSR)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}

@article{beikmohammadi2024compressed,
  title={Compressed Federated Reinforcement Learning with a Generative Model},
  author={Beikmohammadi, Ali and Khirirat, Sarit and Magn{\'u}sson, Sindri},
  journal={arXiv preprint arXiv:2404.10635},
  year={2024}
}

@article{chen2021communication,
  title={Communication-efficient policy gradient methods for distributed reinforcement learning},
  author={Chen, Tianyi and Zhang, Kaiqing and Giannakis, Georgios B and Ba{\c{s}}ar, Tamer},
  journal={IEEE Transactions on Control of Network Systems},
  volume={9},
  number={2},
  pages={917--929},
  year={2021},
  publisher={IEEE}
}

@inproceedings{chen2021multi,
  title={Multi-Agent Off-Policy TDC with Near-Optimal Sample and Communication Complexity},
  author={Chen, Ziyi and Zhou, Yi and Chen, Rongrong},
  booktitle={2021 55th Asilomar Conference on Signals, Systems, and Computers},
  pages={504--508},
  year={2021},
  organization={IEEE}
}

@inproceedings{chen2022sample,
  title={Sample and communication-efficient decentralized actor-critic algorithms with finite-time analysis},
  author={Chen, Ziyi and Zhou, Yi and Chen, Rong-Rong and Zou, Shaofeng},
  booktitle={International Conference on Machine Learning},
  pages={3794--3834},
  year={2022},
  organization={PMLR}
}

@inproceedings{chen2023byzantine,
  title={Byzantine-robust online and offline distributed reinforcement learning},
  author={Chen, Yiding and Zhang, Xuezhou and Zhang, Kaiqing and Wang, Mengdi and Zhu, Xiaojin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3230--3269},
  year={2023},
  organization={PMLR}
}

@inproceedings{dann2019policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019},
  organization={PMLR}
}

@inproceedings{doan2019finite,
  title={Finite-time analysis of distributed TD (0) with linear function approximation on multi-agent reinforcement learning},
  author={Doan, Thinh and Maguluri, Siva and Romberg, Justin},
  booktitle={International Conference on Machine Learning},
  pages={1626--1635},
  year={2019},
  organization={PMLR}
}

@article{doan2021finite,
  title={Finite-time performance of distributed temporal-difference learning with linear function approximation},
  author={Doan, Thinh T and Maguluri, Siva Theja and Romberg, Justin},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={3},
  number={1},
  pages={298--320},
  year={2021},
  publisher={SIAM}
}

@inproceedings{du2017stochastic,
  title={Stochastic variance reduction methods for policy evaluation},
  author={Du, Simon S and Chen, Jianshu and Li, Lihong and Xiao, Lin and Zhou, Dengyong},
  booktitle={International Conference on Machine Learning},
  pages={1049--1058},
  year={2017},
  organization={PMLR}
}

@article{dubey2021provably,
  title={Provably efficient cooperative multi-agent reinforcement learning with function approximation},
  author={Dubey, Abhimanyu and Pentland, Alex},
  journal={arXiv preprint arXiv:2103.04972},
  year={2021}
}

@inproceedings{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  booktitle={International Conference on Machine Learning},
  pages={1407--1416},
  year={2018},
  organization={PMLR}
}

@article{fan2021fault,
  title={Fault-tolerant federated reinforcement learning with theoretical guarantee},
  author={Fan, Flint Xiaofeng and Ma, Yining and Dai, Zhongxiang and Jing, Wei and Tan, Cheston and Low, Bryan Kian Hsiang},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1007--1021},
  year={2021}
}

@inproceedings{fan2023fedhql,
  title={FedHQL: Federated Heterogeneous Q-Learning},
  author={Fan, Flint Xiaofeng and Ma, Yining and Dai, Zhongxiang and Tan, Cheston and Low, Bryan Kian Hsiang},
  booktitle={Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages={2810--2812},
  year={2023}
}

@article{ganesh2024global,
  title={Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries},
  author={Ganesh, Swetha and Chen, Jiayu and Thoppe, Gugan and Aggarwal, Vaneet},
  journal={arXiv preprint arXiv:2403.09940},
  year={2024}
}

@article{gao2019batched,
  title={Batched multi-armed bandits problem},
  author={Gao, Zijun and Han, Yanjun and Ren, Zhimei and Zhou, Zhengqing},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{gong2023federated,
  title={Federated inverse reinforcement learning for smart icus with differential privacy},
  author={Gong, Wei and Cao, Linxiao and Zhu, Yifei and Zuo, Fang and He, Xin and Zhou, Haoquan},
  journal={IEEE Internet of Things Journal},
  volume={10},
  number={21},
  pages={19117--19124},
  year={2023},
  publisher={IEEE}
}

@article{gower2020variance,
  title={Variance-reduced methods for machine learning},
  author={Gower, Robert M and Schmidt, Mark and Bach, Francis and Richt{\'a}rik, Peter},
  journal={Proceedings of the IEEE},
  volume={108},
  number={11},
  pages={1968--1983},
  year={2020},
  publisher={IEEE}
}

@inproceedings{guo2015concurrent,
  title={Concurrent pac rl},
  author={Guo, Zhaohan and Brunskill, Emma},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={29},
  pages={2624--2630},
  year={2015}
}

@inproceedings{he2021logarithmic,
  title={Logarithmic regret for reinforcement learning with linear function approximation},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={4171--4180},
  year={2021},
  organization={PMLR}
}

@article{hsu2024randomized,
  title={Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning},
  author={Hsu, Hao-Lun and Wang, Weixin and Pajic, Miroslav and Xu, Pan},
  journal={arXiv preprint arXiv:2404.10728},
  year={2024}
}

@article{jaksch2010near,
  title={Near-optimal Regret Bounds for Reinforcement Learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  pages={1563--1600},
  year={2010}
}

@article{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on learning theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@inproceedings{jin2022federated,
  title={Federated reinforcement learning with environment heterogeneity},
  author={Jin, Hao and Peng, Yang and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={18--37},
  year={2022},
  organization={PMLR}
}

@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@inproceedings{jonsson2020planning,
  title={Planning in markov decision processes with gap-dependent sample complexity},
  author={Jonsson, Anders and Kaufmann, Emilie and M{\'e}nard, Pierre and Darwiche Domingues, Omar and Leurent, Edouard and Valko, Michal},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1253--1263},
  year={2020}
}

@article{kakade2018variance,
  title={Variance reduction methods for sublinear reinforcement learning},
  author={Kakade, Sham and Wang, Mengdi and Yang, Lin F},
  journal={arXiv preprint arXiv:1802.09184},
  year={2018}
}

@article{khamaru2021temporal,
  title={Is temporal difference learning optimal? an instance-dependent analysis},
  author={Khamaru, Koulik and Pananjady, Ashwin and Ruan, Feng and Wainwright, Martin J and Jordan, Michael I},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={3},
  number={4},
  pages={1013--1040},
  year={2021},
  publisher={SIAM}
}

@inproceedings{khodadadian2022federated,
  title={Federated reinforcement learning: Linear speedup under markovian sampling},
  author={Khodadadian, Sajad and Sharma, Pranay and Joshi, Gauri and Maguluri, Siva Theja},
  booktitle={International Conference on Machine Learning},
  pages={10997--11057},
  year={2022},
  organization={PMLR}
}

@article{lan2023improved,
  title={Improved communication efficiency in federated natural policy gradient via admm-based gradient updates},
  author={Lan, Guangchen and Wang, Han and Anderson, James and Brinton, Christopher and Aggarwal, Vaneet},
  journal={arXiv preprint arXiv:2310.19807},
  year={2023}
}

@article{lan2024asynchronous,
  title={Asynchronous federated reinforcement learning with policy gradient updates: Algorithm design and convergence analysis},
  author={Lan, Guangchen and Han, Dong-Jun and Hashemi, Abolfazl and Aggarwal, Vaneet and Brinton, Christopher G},
  journal={arXiv preprint arXiv:2404.08003},
  year={2024}
}

@article{li2020sample,
  title={Sample complexity of asynchronous Q-learning: Sharper analysis and variance reduction},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7031--7043},
  year={2020}
}

@article{li2021breaking,
  title={Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning},
  author={Li, Gen and Shi, Laixi and Chen, Yuxin and Gu, Yuantao and Chi, Yuejie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17762--17776},
  year={2021}
}

@article{liu2022distributed,
  title={Distributed inverse constrained reinforcement learning for multi-agent systems},
  author={Liu, Shicheng and Zhu, Minghui},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={33444--33456},
  year={2022}
}

@article{liu2023distributed,
  title={Distributed td (0) with almost no communication},
  author={Liu, Rui and Olshevsky, Alex},
  journal={IEEE Control Systems Letters},
  year={2023},
  publisher={IEEE}
}

@inproceedings{liu2023meta,
  title={Meta inverse constrained reinforcement learning: Convergence guarantee and generalization analysis},
  author={Liu, Shicheng and Zhu, Minghui},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{liu2024learning,
  title={Learning Multi-agent Behaviors from Distributed and Streaming Demonstrations},
  author={Liu, Shicheng and Zhu, Minghui},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{liutrajectory,
  title={In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before an Ongoing Trajectory Terminates},
  author={Liu, Shicheng and Zhu, Minghui},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2025}
}

@article{marjani2020best,
  title={Best policy identification in discounted mdps: Problem-specific sample complexity},
  author={Marjani, AA and Proutiere, Alexandre},
  journal={arXiv preprint arXiv:2009.13405},
  year={2020}
}

@inproceedings{menard2021ucb,
  title={UCB Momentum Q-learning: Correcting the bias without forgetting},
  author={M{\'e}nard, Pierre and Domingues, Omar Darwiche and Shang, Xuedong and Valko, Michal},
  booktitle={International Conference on Machine Learning},
  pages={7609--7618},
  year={2021},
  organization={PMLR}
}

@inproceedings{min2023cooperative,
  title={Cooperative multi-agent reinforcement learning: Asynchronous communication and linear function approximation},
  author={Min, Yifei and He, Jiafan and Wang, Tianhao and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={24785--24811},
  year={2023},
  organization={PMLR}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={International Conference on Machine Learning},
  pages={2613--2621},
  year={2017},
  organization={PMLR}
}

@inproceedings{nguyen2023instance,
  title={On instance-dependent bounds for offline reinforcement learning with linear function approximation},
  author={Nguyen-Tang, Thanh and Yin, Ming and Gupta, Sunil and Venkatesh, Svetha and Arora, Raman},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={9310--9318},
  year={2023}
}

@article{ok2018exploration,
  title={Exploration in structured reinforcement learning},
  author={Ok, Jungseul and Proutiere, Alexandre and Tranos, Damianos},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{perchet2016batched,
author = {Perchet, Vianney and Rigollet, Philippe and Chassang, Sylvain and Snowberg, Erik},
title = {Batched bandit problems},
volume = {44},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {660 -- 681},
keywords = {batches, grouped clinical trials, Multi-armed bandit problems, multi-phase allocation, regret bounds, sample size determination, switching cost},
year = {2016}
}

@inproceedings{qiao2022sample,
  title={Sample-efficient reinforcement learning with loglog (t) switching cost},
  author={Qiao, Dan and Yin, Ming and Min, Ming and Wang, Yu-Xiang},
  booktitle={International Conference on Machine Learning},
  pages={18031--18061},
  year={2022},
  organization={PMLR}
}

@article{shen2023towards,
  title={Towards Understanding Asynchronous Advantage Actor-critic: Convergence and Linear Speedup},
  author={Shen, Han and Zhang, Kaiqing and Hong, Mingyi and Chen, Tianyi},
  journal={IEEE Transactions on Signal Processing},
  year={2023},
  publisher={IEEE}
}

@inproceedings{shi2022pessimistic,
  title={Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity},
  author={Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Chi, Yuejie},
  booktitle={International Conference on Machine Learning},
  pages={19967--20025},
  year={2022},
  organization={PMLR}
}

@article{sidford2018near,
  title={Near-optimal time and sample complexities for solving Markov decision processes with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin and Ye, Yinyu},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{sidford2023variance,
  title={Variance reduced value iteration and faster algorithms for solving Markov decision processes},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
  journal={Naval Research Logistics (NRL)},
  volume={70},
  number={5},
  pages={423--442},
  year={2023},
  publisher={Wiley Online Library}
}

@inproceedings{simchowitz2019non,
  title={Non-asymptotic gap-dependent regret bounds for tabular MDPs},
  author={Simchowitz, Max and Jamieson, Kevin G},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{sun2020finite,
  title={Finite-time analysis of decentralized temporal-difference learning with linear function approximation},
  author={Sun, Jun and Wang, Gang and Giannakis, Georgios B and Yang, Qinmin and Yang, Zaiyue},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4485--4495},
  year={2020},
  organization={PMLR}
}

@inproceedings{tewari2008optimistic,
  title={Optimistic linear programming gives logarithmic regret for irreducible MDPs},
  author={Tewari, Ambuj and Bartlett, Peter},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1505--1512},
  year={2008}
}

@inproceedings{tirinzoni2022near,
  title={Near instance-optimal pac reinforcement learning for deterministic mdps},
  author={Tirinzoni, Andrea and Al Marjani, Aymen and Kaufmann, Emilie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8785--8798},
  year={2022}
}

@inproceedings{tirinzoni2023optimistic,
  title={Optimistic pac reinforcement learning: the instance-dependent view},
  author={Tirinzoni, Andrea and Al-Marjani, Aymen and Kaufmann, Emilie},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={1460--1480},
  year={2023},
  organization={PMLR}
}

@inproceedings{wagenmaker2022beyond,
  title={Beyond no regret: Instance-dependent pac reinforcement learning},
  author={Wagenmaker, Andrew J and Simchowitz, Max and Jamieson, Kevin},
  booktitle={Conference on Learning Theory},
  pages={358--418},
  year={2022},
  organization={PMLR}
}

@article{wagenmaker2022instance,
  title={Instance-dependent near-optimal policy identification in linear mdps via online experiment design},
  author={Wagenmaker, Andrew and Jamieson, Kevin G},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={5968--5981},
  year={2022}
}

@article{wai2019variance,
  title={Variance reduced policy evaluation with smooth function approximation},
  author={Wai, Hoi-To and Hong, Mingyi and Yang, Zhuoran and Wang, Zhaoran and Tang, Kexin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{wai2020convergence,
  title={On the convergence of consensus algorithms with markovian noise and gradient bias},
  author={Wai, Hoi-To},
  booktitle={2020 59th IEEE Conference on Decision and Control (CDC)},
  pages={4897--4902},
  year={2020},
  organization={IEEE}
}

@article{wainwright2019variance,
  title={Variance-reduced $ Q $-learning is minimax optimal},
  author={Wainwright, Martin J},
  journal={arXiv preprint arXiv:1906.04697},
  year={2019}
}

@article{wang2020decentralized,
  title={Decentralized TD tracking with linear function approximation and its finite-time analysis},
  author={Wang, Gang and Lu, Songtao and Giannakis, Georgios and Tesauro, Gerald and Sun, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13762--13772},
  year={2020}
}

@article{wang2021provably,
  title={Provably efficient reinforcement learning with linear function approximation under adaptivity constraints},
  author={Wang, Tianhao and Zhou, Dongruo and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13524--13536},
  year={2021}
}

@article{wang2022gap,
  title={On gap-dependent bounds for offline reinforcement learning},
  author={Wang, Xinqi and Cui, Qiwen and Du, Simon S},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={14865--14877},
  year={2022}
}

@inproceedings{woo2023blessing,
  title={The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond},
  author={Woo, Jiin and Joshi, Gauri and Chi, Yuejie},
  booktitle={International Conference on Machine Learning},
  pages={37157-37216},
  year={2023}
}

@inproceedings{woo2024federated,
  title={Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices},
  author={Woo, Jiin and Shi, Laixi and Joshi, Gauri and Chi, Yuejie},
  booktitle={International Conference on Machine Learning},
  pages={53165--53201},
  year={2024}
}

@article{wu2021byzantine,
  title={Byzantine-resilient decentralized policy evaluation with linear function approximation},
  author={Wu, Zhaoxian and Shen, Han and Chen, Tianyi and Ling, Qing},
  journal={IEEE Transactions on Signal Processing},
  volume={69},
  pages={3839--3853},
  year={2021},
  publisher={IEEE}
}

@inproceedings{xu2020reanalysis,
  title={Reanalysis of variance reduced temporal difference learning},
  author={Xu, Tengyu and Wang, Zhe and Zhou, Yi and Liang, Yingbin},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{xu2021fine,
  title={Fine-grained gap-dependent bounds for tabular mdps via adaptive multi-step bootstrap},
  author={Xu, Haike and Ma, Tengyu and Du, Simon},
  booktitle={Conference on Learning Theory},
  pages={4438--4472},
  year={2021},
  organization={PMLR}
}

@article{yan2022efficacy,
  title={The efficacy of pessimism in asynchronous Q-learning},
  author={Yan, Yuling and Li, Gen and Chen, Yuxin and Fan, Jianqing},
  journal={IEEE Transactions on Information Theory},
  year={2023}
}

@inproceedings{yang2021q,
  title={Q-learning with logarithmic regret},
  author={Yang, Kunhe and Yang, Lin and Du, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1576--1584},
  year={2021},
  organization={PMLR}
}

@article{yang2023federated,
  title={Federated natural policy gradient methods for multi-task reinforcement learning},
  author={Yang, Tong and Cen, Shicong and Wei, Yuting and Chen, Yuxin and Chi, Yuejie},
  journal={arXiv preprint arXiv:2311.00201},
  year={2023}
}

@inproceedings{yin2021near,
  title={Near-optimal offline reinforcement learning via double variance reduction},
  author={Yin, Ming and Bai, Yu and Wang, Yu-Xiang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7677--7688},
  year={2021}
}

@inproceedings{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={7304--7312},
  year={2019},
  organization={PMLR}
}

@inproceedings{zeng2021finite,
  title={Finite-time analysis of decentralized stochastic approximation with applications in multi-agent and multi-task learning},
  author={Zeng, Sihan and Doan, Thinh T and Romberg, Justin},
  booktitle={2021 60th IEEE Conference on Decision and Control (CDC)},
  pages={2641--2646},
  year={2021},
  organization={IEEE}
}

@article{zhang2020almost,
  title={Almost optimal model-free reinforcement learning via reference-advantage decomposition},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15198--15207},
  year={2020}
}

@inproceedings{zhang2021reinforcement,
  title={Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon},
  booktitle={Conference on Learning Theory},
  pages={4528--4531},
  year={2021},
  organization={PMLR}
}

@article{zhang2022near,
  title={Near-optimal regret bounds for multi-batch reinforcement learning},
  author={Zhang, Zihan and Jiang, Yuhang and Zhou, Yuan and Ji, Xiangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24586--24596},
  year={2022}
}

@article{zhang2024finite,
  title={Finite-time analysis of on-policy heterogeneous federated reinforcement learning},
  author={Zhang, Chenyu and Wang, Han and Mitra, Aritra and Anderson, James},
  journal={arXiv preprint arXiv:2401.15273},
  year={2024}
}

@inproceedings{zhang2024settling,
  title={Settling the sample complexity of online reinforcement learning},
  author={Zhang, Zihan and Chen, Yuxin and Lee, Jason D and Du, Simon S},
  booktitle={Conference on Learning Theory},
  pages={5213--5219},
  year={2024},
  organization={PMLR}
}

@article{zhao2023federated,
  title={Federated multi-objective reinforcement learning},
  author={Zhao, Fangyuan and Ren, Xuebin and Yang, Shusen and Zhao, Peng and Zhang, Rui and Xu, Xinxin},
  journal={Information Sciences},
  volume={624},
  pages={811--832},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{zheng2023federated,
title={Federated Q-Learning: Linear Regret Speedup with Low Communication Cost},
author={Zhong Zheng and Fengyu Gao and Lingzhou Xue and Jing Yang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@inproceedings{zheng2024gap,
  title={Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition},
  author={Zheng, Zhong and Zhang, Haochen and Xue, Lingzhou},
    booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}

@inproceedings{zhou2023sharp,
  title={Sharp variance-dependent bounds in reinforcement learning: Best of both worlds in stochastic and deterministic environments},
  author={Zhou, Runlong and Zihan, Zhang and Du, Simon Shaolei},
  booktitle={International Conference on Machine Learning},
  pages={42878--42914},
  year={2023},
  organization={PMLR}
}

