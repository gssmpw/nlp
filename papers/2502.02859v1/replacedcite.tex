\section{Related Work}
\label{related}
\textbf{On-policy RL for single agent finite-horizon tabular MDPs with worst-case regret.} There are mainly two types of algorithms for reinforcement learning: model-based and model-free learning. Model-based algorithms learn a model from past experience and make decisions based on this model, while model-free algorithms only maintain a group of value functions and take the induced optimal actions. Due to these differences, model-free algorithms are usually more space-efficient and time-efficient compared to model-based algorithms. However, model-based algorithms may achieve better learning performance by leveraging the learned model.

Next, we discuss the literature on model-based and model-free algorithms for finite-horizon tabular MDPs with worst-case regret. ____, ____, ____, ____, ____, ____, ____, ____, ____ and ____ worked on model-based algorithms. Notably, ____ provided an algorithm that achieves a regret of $\tilde{O}(\min \{\sqrt{SAH^2T},T\})$, which matches the information lower bound. ____, ____, ____, ____ and ____ work on model-free algorithms. The latter three have introduced algorithms that achieve minimax regret of $\tilde{O}(\sqrt{SAH^2T})$.

\textbf{Suboptimality Gap.} When there is a strictly positive suboptimality gap, it is possible to achieve logarithmic regret
bounds. In RL, earlier work obtained asymptotic logarithmic regret bounds ____.
Recently, non-asymptotic logarithmic regret bounds were obtained ____. Specifically, ____ developed a model-based algorithm, and their bound depends on the policy gap instead of the action gap studied in this paper. ____ derived problem-specific logarithmic type lower bounds for both structured and unstructured MDPs. ____ extended the model-based algorithm by ____ and obtained logarithmic regret bounds. Logarithmic regret bounds are obtained in linear function approximation settings ____. ____ also provides a gap-dependent regret bounds for offline RL with linear funciton approximation. 

Specifically, for model free algorithm, ____ showed that the optimistic $Q$-learning algorithm by ____ enjoyed a logarithmic regret $O(\frac{H^6SAT}{\dmin})$, which was subsequently refined by ____. In their work, ____ introduced the Adaptive Multi-step Bootstrap (AMB) algorithm. ____ further improved the logarithmic regret bound by leveraging the analysis of the UCB-Advantage algorithm ____ and Q-EarlySettled-Advantage algorithm ____.

There are also some other works focusing on gap-dependent sample complexity bounds
____.

\textbf{Variance reduction in RL.} The reference-advantage decomposition used in ____ and ____ is a technique of variance reduction that was originally proposed for finite-sum stochastic optimization  ____. Later on, model-free RL algorithms also used variance reduction to improve the sample efficiency. For example, it was used in learning with generative models ____, policy evaluation ____, offline RL ____, and $Q$-learning ____.

\textbf{RL with low switching cost and batched RL}. Research in RL with low-switching cost aims to minimize the number of policy switches while maintaining comparable regret bounds to fully adaptive counterparts, and it can be applied to federated RL. In batched RL ____, the agent sets the number of batches and the length of each batch upfront, implementing an unchanged policy in a batch and aiming for fewer batches and lower regret. ____ first introduced the problem of RL with low-switching cost and proposed a $Q$-learning algorithm with lazy updates, achieving $\tilde{O}(SAH^3\log T)$ switching cost. This work was advanced by ____, which improved the regret upper bound and the switching cost. Additionally, ____ studied RL under the adaptivity constraint. 
Recently, ____ proposed a model-based algorithm with $\tilde{O}(\log \log T)$ switching cost. ____ proposed a batched RL algorithm that is well-suited for the federated setting. 

\textbf{Multi-agent RL (MARL) with event-triggered communications.} We review a few recent works for on-policy MARL with linear function approximations. ____ introduced Coop-LSVI for cooperative MARL. ____ proposed an asynchronous version of LSVI-UCB that originates from ____, matching the same regret bound with improved communication complexity compared to ____. ____ developed two algorithms that incorporate randomized exploration, achieving the same regret and communication complexity as ____. ____, ____ and ____ employed event-triggered communication conditions based on determinants of certain quantities. Different from our federated algorithm, during the synchronization in ____ and ____, local agents share original rewards or trajectories with the server. On the other hand, ____ reduces communication cost by sharing compressed statistics in the non-tabular setting with linear function approximation.

\textbf{Federated and distributed RL}. Existing literature on federated and distributed RL algorithms highlights various aspects. For value-based algorithms, ____, ____, and ____ focused on  linear speed up. ____ proposed a parallel RL algorithm with low communication cost. ____ and ____ discussed the improved covering power of heterogeneity. ____ and ____ worked on robustness. Particularly, ____ proposed algorithms in both offline and online settings, obtaining near-optimal sample complexities and achieving superior robustness guarantees. In addition, several works have investigated value-based algorithms such as $Q$-learning in different settings, including ____, ____, ____, ____, ____, and ____.  The convergence of decentralized temporal difference algorithms has been analyzed by ____, ____, ____, ____, ____, ____, ____, and ____. 


Some other works focus on policy gradient-based algorithms. Communication-efficient policy gradient algorithms have been studied by ____ and ____. ____ further shows the
simplicity compared to the other RL methods, and a linear speedup has been demonstrated in the synchronous setting. Optimal sample complexity for global optimality in federated RL, even in the presence of adversaries, is studied in ____. ____ propose an algorithm to address the challenge of lagged policies in asynchronous settings.

The convergence of distributed actor-critic algorithms has been analyzed by ____ and ____. Federated actor-learner architectures have been explored by ____, ____, and ____. Distributed inverse reinforcement learning has been examined by ____, ____ and ____.