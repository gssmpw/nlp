\section{Experiments}
\label{sec:experiments}

In this section, we evaluate SEVAL, our proposed method. The code and implementation details will be made publicly available upon acceptance of the paper.

\subsection{Experimental Setup}

\textbf{JSSP Dataset.}  
The training dataset comprises 40,000 instances, with 90\% of the data used for training and 10\% for validation. All instances were solved using Google OR-Tools \cite{cpsatlp}, with a computation time limit of 60 seconds per instance. The data generation process followed the methodology proposed by Taillard \cite{taillard1993benchmarks}, generating instances with the number of jobs and machines ranging from 12 to 20 and processing times were uniformly sampled from the range $[1, 99]$.

\textbf{JSSP Test Benchmarks.}  
The performance of our method has been evaluated using two widely recognized benchmark datasets. The first is the Taillard dataset \cite{taillard1993benchmarks}, which contains 80 instances ranging from 15 jobs and 15 machines (225 operations) to 100 jobs and 20 machines (2000 operations). The second is the Demirkol dataset \cite{demirkol1998benchmarks}, which includes 80 instances with varying configurations, ranging from 20 to 50 jobs and machines. Notably, the Demirkol dataset introduces additional diversity by not following the same distribution as the Taillard dataset. The inclusion of both benchmarks allows us to evaluate the generalization capabilities of our approach on larger and more diverse instances than those used during training. The best-known results for these public benchmarks are available online\footnote{\url{https://optimizizer.com/index.php}}. 

\textbf{JSSP Baselines.}  
We compared our approach against several state-of-the-art methods. Among the DRL approaches, we included the actor-critic framework (L2D) proposed by \cite{zhang2020learning}, representing one of the pioneering works in this field, as well as the more recent HGNN-based approach, ResSch, introduced in \cite{ho2024residual}. 

From the methods allowing multiple assignments without employing self-evaluation mechanisms, we considered RLCP \cite{tassel2023end}, which uses a Transformer-based architecture and combines imitation learning with policy gradient methods. Additionally, we evaluated the approach proposed in \cite{echeverria2024multi}, which employs HGNNS and supervised learning and serves as the basis for this work. We also included emerging self-supervised learning approaches, such as SPN \cite{corsini2024self} and SI GD \cite{pirnay2024self}. For the comparison with SEVAL and other algorithms, the greedy strategy was chosen, selecting the action with the highest probability at each step, as our method generates a single solution without relying on sampling strategies.

For non-constructive methods, the improvement-based methods L2S \cite{zhang2024deep} was included, using its 500-step solution improvement variant to achieve comparable execution times. Finally, we compared our approach against the CP-SAT solver from Google OR-Tools, setting a time limit of 3600 seconds as a reference for a traditional optimization method.  
To ensure a fair comparison, we used the pre-trained models provided in the repositories of each paper to generate the results. Additionally, we compared execution times for all methods, which are presented in Appendix \ref{app:exec}.

\textbf{Performance Metric.}  
The evaluation metric used is the optimal gap ($OG$), defined as:
\begin{align}
OG = \left( \frac{C_{\pi}}{C_{\text{ub}}} - 1 \right) \cdot 100,
\end{align}
where $C_{\pi}$ is the makespan obtained by the policy, and $C_{\text{ub}}$ denotes the optimal or best-known makespan for the instance.

\textbf{Model Configuration.}  
The HGNN consisted of six layers, each with three attention heads and a hidden dimension of 32. The policy and self-evaluation models utilized a Transformer architecture with four layers, eight attention heads, latent and feed-forward dimensions of 128, and GeLU as the activation function. The number of subsets evaluated by the self-evaluation model was set to 16. The models were trained using the Adam optimizer with a learning rate of $3 \times 10^{-4}$ over 30 epochs and a batch size of 256.

\subsection{Experimental results}

\begin{table*}[!t]
\centering
\caption{Optimal gap comparison with several state-of-the-art DL methods, OR-Tools, and SEVAL in the Taillard benchmark. The best result for each group of instances among deep learning methods is highlighted in bold.}
\label{tab:allbenchmarkstaillard}
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r r r r r r r r r}
\toprule
\multirow{1}{*}{\textbf{Size}} & \multicolumn{8}{c}{Deep learning methods} & \multicolumn{1}{c}{Classical-method} \\
\cmidrule(lr){2-9}
\cmidrule(lr){10-10}
& L2D & ResSch & RLCP & MAS &  SPN & SI GD & $L2S_{500}$ & \textbf{SEVAL} & OR-Tools \\
\midrule
15$\times$15 & 26.0 & 13.7 & 16.3 & 13.5 & 13.8 & 9.6 & 9.3 & \textbf{7.9} &  0.1 \\
20$\times$15 & 30.0 & 18.0 & 19.7 & 13.9 & 14.9 & 9.9& 11.6 & \textbf{7.6} & 0.2 \\
20$\times$20 & 31.6 & 16.5 & 18.6 & 13.4 & 15.2 & 11.1& 12.4 & \textbf{8.7} &  0.7\\
30$\times$15 & 33.0 & 17.3 & 18.3 & 14.8 & 17.1 & 9.5 & 14.7 & \textbf{9.6} &  2.1 \\
30$\times$20 & 33.6 & 18.1 & 22.8 & 17.4 & 18.5 & 13.8& 17.5 & \textbf{9.8} &  2.8 \\
50$\times$15 & 22.4 & 8.4 & 10.1 & 7.1 & 10.1 & \textbf{2.7} & 11.0 & 3.2 &  3.0\\
50$\times$20 & 26.5& 11.4 & 14.0 & 9.5 & 11.5 & 6.7& 13.0& \textbf{5.1} &  2.8\\
100$\times$20 & 13.6 & 4.0 & 4.5 & 2.3 & 5.8 & 1.7 & 7.9 & \textbf{0.5} &  3.9\\
Mean & 27.1 & 13.4 & 15.5 & 11.5 &  13.3 & 8.2 & 12.2 & \textbf{6.5} & 2.0\\
\bottomrule
\end{tabular*}
\end{table*}

\begin{table*}[!t]
\centering
\caption{Optimal gap comparison with several state-of-the-art DL methods, OR-Tools, and SEVAL in the Demirkol benchmark. The best result for each group of instances among deep learning methods, with similar execution times, is highlighted in bold.}
\label{tab:allbenchmarksdemirkol}
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r r r r r r r r r}
\toprule
\multirow{1}{*}{\textbf{Size}} & \multicolumn{8}{c}{Deep learning methods} & \multicolumn{1}{c}{Classical-method} \\
\cmidrule(lr){2-9}
\cmidrule(lr){10-10}
& L2D & ResSch & RLCP & MAS & SPN & SI GD & $L2S_{500}$ & \textbf{SEVAL} & OR-Tools  \\
\midrule
20$\times$15 & 39.0& 19.6& 26.3 & 15.2 & 18.0& 15.4 & 21.1 & \textbf{11.8} & 1.8\\
20$\times$20 & 37.7& 18.9& 25.9 & 15.5 & 19.4 & 15.8 &18.2 & \textbf{10.7} & 1.9\\
30$\times$15 & 42.0& 20.6& 27.8 & 17.9 &21.8& 16.6 & 26.4 & \textbf{10.3} &2.5\\
30$\times$20 & 39.7& 22.1 & 30.1 & 19.1 &25.7& 16.1 & 27.5& \textbf{11.6} & 4.4\\
40$\times$15 & 35.6& 16.3& 22.6 & 13.3 &17.5& 11.6 & 25.8 & \textbf{7.5} & 4.1\\
40$\times$20 & 39.6& 19.2 & 27.8 & 18.4 &22.2& 16.6 & 29.2& \textbf{11.7} & 4.6\\
50$\times$15 & 36.5& 13.5 & 20.6 & 10.8 &15.7& 10.1 & 26.4& \textbf{6.2} & 3.8\\
50$\times$20 & 39.5& 20.1 & 26.4& 16.7 &22.4& 17.4 & 32.4 & \textbf{9.2} & 4.8\\
Mean & 38.7& 18.8 & 26.1 & 15.9 & 20.3 & 14.9 & 25.8 & \textbf{9.9} & 3.5\\
\bottomrule
\end{tabular*}
\end{table*}

\textbf{Results on the Taillard Benchmark.}  The experimental results on the Taillard benchmark, presented in Table \ref{tab:allbenchmarkstaillard}, show the outcomes for 80 instances grouped into sets of 10 by size. SEVAL, our proposed method, demonstrates superiority across all subsets except one. Among the deep learning methods, the best-performing results are highlighted, emphasizing SEVAL's consistent ability to outperform alternative approaches in most cases.

Notably, in the largest group of instances (\(100 \times 20\)), SEVAL achieves pseudo-optimal results with a mean gap of just \(0.5\%\), outperforming those obtained by OR-Tools, even when the latter is allocated an execution time of one hour per instance. As shown in Appendix \ref{app:exec}, SEVAL requires an average of only 30 seconds to solve these large instances.

Another noteworthy aspect is the remarkable improvement in performance metrics for this problem in recent years. For instance, the mean performance gap has decreased from $27.1\%$ in 2020 \cite{zhang2020learning} to $6.5\%$ in this work, suggesting that results very close to optimal may soon be achievable for this problem, much like the progress observed for simpler problems such as the TSP. Recent DRL-based approaches, such as ResSch \cite{ho2024residual}, have contributed to these improvements. However, methods based on supervised learning, like MAS \cite{echeverria2024multi} and SEVAL, or self-supervised learning, like SI GD \cite{pirnay2024self}, appear to achieve superior performance.

When comparing SEVAL to RLCP \cite{tassel2023end} and MAS \cite{echeverria2024multi}, both of which can assign multiple tasks and utilize a supervised learning component but lack a self-evaluation module, SEVAL demonstrates clear advantages. MAS, upon which this work builds, does not incorporate a hybrid HGNN architecture and entirely avoids the use of Transformers. These limitations may contribute significantly to the observed performance gap in favor of SEVAL.

SEVAL also achieves an improvement when compared to SI GD \cite{pirnay2024self}, the second-best performing method, which builds upon the work of \cite{drakulic2024bq} and employs a purely Transformer-based architecture. Similarly, it also outperforms the other self-supervised learning method, SPN \cite{corsini2024self}, which can also be attributed to its simpler architecture compared to the other methods.

Finally, SEVAL demonstrates competitive and superior results on smaller instances when compared to the L2S method proposed in \cite{zhang2024deep}. However, its performance advantage becomes more pronounced on larger instances. This difference can be attributed to the exponential growth in the search space and action complexity as instance size increases, which poses significant scalability challenges for improvement-based methods.

\textbf{Results on the Demirkol Benchmark.}  The results for the second benchmark, Demirkol, are presented in Table \ref{tab:allbenchmarksdemirkol}. The primary purpose of using this second benchmark, which has a distribution different from the one employed during training, is to evaluate the generalization capabilities of our method. 

The results are consistent with those observed on the Taillard benchmark, although the performance differences are more pronounced. Once again, SEVAL achieves the best results, with a significantly larger margin over the other methods compared to the Taillard benchmark. On the Taillard benchmark, the absolute difference between the second-best method and ours was \(1.7\%\), while on this benchmark, it is \(4\%\).

We also observe that while some degradation in performance is expected when transitioning to a different distribution, this drop is considerably higher for many methods. In some cases, the performance is halved, with the most severe degradation observed for the improvement-based method. In this case, the degradation is from \(12\%\) to \(25\%\), whereas in ours, it is from \(6.5\%\) to \(9.9\%\).

We hypothesize that this is due to the self-evaluation mechanism in SEVAL, which acts as an additional safeguard to the outputs generated by the policy. This mechanism contributes to producing more robust and reliable results, even when the data distribution shifts.
 
