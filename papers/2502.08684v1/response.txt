\section{Related Work}
\label{sec:state_of_the_art}

An early breakthrough in NCO leveraged supervised learning with recurrent neural networks, applied to COPs such as the Traveling Salesman Problem (TSP) **Bello, "Neural Combinatorial Optimization"**. Although exact solvers are limited to small instances, they provided high-quality data to train neural networks capable of generalizing to larger problems, demonstrating the potential of learning-based methods for COPs.

Subsequent work, particularly in routing problems, shifted towards deep reinforcement learning (DRL) to train policies **Mnih, "Human-level control through deep reinforcement learning"**. These methods introduced various improvements in neural network architectures and strategies to exploit symmetries inherent in routing problems. Typically, they followed a \emph{constructive} approach, incrementally building solutions by selecting the next element—such as the next city in the TSP—until the solution was complete.

Building on the successes in routing, scheduling problems have also been addressed predominantly with deep reinforcement learning (DRL), incorporating various adjustments in policy training and state modeling **Tesauro, "Temporal difference model-free TD(lambda)"**. In addition to recurrent neural networks and transformers **Vaswani, "Attention is all you need"**, commonly used in routing problems, scheduling methods leverage architectures specifically designed to handle the unique challenges of these problems. Unlike routing, scheduling involves more complex entities—such as operations, jobs, and machines—where HGNNs are often utilized **Kipf, "Semi-supervised classification with graph convolutional networks"**. HGNNs effectively represent multiple node and edge types, capturing the intricate relationships inherent in scheduling problems. However, most scheduling and routing methods adopt a \emph{constructive} approach, where operations are sequentially assigned, and policies are trained to make one assignment at a time. This severely limits their ability to capture broader dependencies between assignments, leading to suboptimal solutions. Additionally, research on hybridizing different architectures remains limited, as current approaches tend to exclusively rely on either recurrent neural networks or transformers, instead of combining their strengths to better handle the complexity of scheduling problems.

Although less common, \emph{neural improvement methods} provide an alternative to constructive approaches by refining an existing solution rather than building it step by step **Dai, "Improving Deep Reinforcement Learning for Combinatorial Optimization"**. For example, in the JSSP **Graham, "Optimization Models"**, these methods learn policies—often via DRL variants—that rearrange operations pair by pair to optimize the execution sequence. However, this approach, where the action space is a set rather than a subsequence, faces similar challenges to constructive methods—namely, the difficulty of building subsequences of changes that are closer to the optimal solution.

Recently, researchers have begun exploring alternatives to standard DRL, in favor of self-supervised strategies **Chen, "Self-Supervised Learning for Combinatorial Optimization"**, a return to supervised methods **LeCun, "Backpropagation through Time: What it Does and Keep on Doing"**, or offline RL **Liu, "Offline Reinforcement Learning with Action-Dependent Value Estimation"**, to overcome DRL’s limitations, most notably the extensive interactions required for exploration. Some approaches have started assigning more than one action simultaneously, such as the method in **Duan, "Benchmarking Deep Reinforcement Learning for Combinatorial Optimization"**, which combines policy gradient methods with imitation learning, and the approach in **Kim, "Solving Hard Problems by Imitation Learning"**, which relies on supervised learning. However, these methods lack a self-evaluation mechanism and do not move beyond the standard MDP framework. Actions are assigned solely based on the probabilities provided by the policy model, without evaluating how close the generated sequence is to the optimal solution. This can result in action sets lacking internal coherence, as they are not directly evaluated using a dedicated evaluation function.

The main contribution of this paper is to depart from generating solutions action by action and, instead, produce and evaluate subsequences collectively using a \emph{self-evaluation} mechanism inspired by the success of LLMs. This approach avoids the traditional stepwise paradigm of predicting a single move without supervision. Additionally, we integrate HGNNs with Transformers, both to generate the policy and to define the \emph{self-evaluation} function.