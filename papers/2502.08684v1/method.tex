\section{Method}
\label{sec:method}

In this section, we present our contribution, which introduces a novel approach to applying the concept of \emph{self-evaluation} to scheduling problems. We begin by explaining how the problem is defined as a Markov Process and then detail the generation of the policy and \emph{self-evaluation} models. 

% \begin{figure}[h] \centering \includesvg[width=1\linewidth]{arbolitos-Copia de arbolitos2.drawio} \caption{Comparison between a greedy policy-based approach and the proposed self-evaluation framework.} \label{fig:selfevaluation} \end{figure}

% Figure \ref{fig:selfevaluation} compares the standard \emph{greedy approach} with the proposed \emph{self-evaluation approach}. In the greedy approach, the policy model generates a probability distribution over individual actions at each state. The action with the highest probability is selected, and the state transitions accordingly. This process is repeated iteratively until the solution is complete.

% In contrast, the self-evaluation approach augments this process by sampling multiple subsets of actions from the policy model at each step. These subsets are collectively evaluated by the self-evaluation model, which assigns a score to each one. The subset with the highest score is then selected to transition to the next state. This framework leverages the self-evaluation model to guide the decision-making process, enhancing the quality of intermediate and final solutions.

\subsection{JSSP as a Markov Process}

The JSSP is modeled as a Markov Process, with states representing scheduling progress and transitions corresponding to sets of job-machine assignments. Below, we outline the state space, action space, and transition function.

\textbf{State Space.} The state space is modeled as a heterogeneous graph $\mathcal{G}_t = (\mathcal{V}_t, \mathcal{E}_t)$, following the approach described in \cite{song2022flexible}. A simplified version, as proposed in \cite{echeverria2024multi}, is used in this work. At each timestep $t$, $\mathcal{G}_t$ includes three types of nodes: operations, jobs, and machines, as well as three distinct types of edges (directed and undirected) that capture relationships such as precedence constraints between operations, which operations belong to a specific job, and which operations can be executed by specific machines. Detailed information about the features of the different nodes and edges is provided in Appendix \ref{app:features}.

\textbf{Action Space.} At each timestep $t$, the set of feasible job-machine pairs is denoted as $\mathcal{JM}_t$. The action space is defined as the power set of all pairs $(j, m) \in \mathcal{JM}_t$ such that no machine is assigned more than one job simultaneously:
\begin{align}
\mathcal{A}_t = \{A \subseteq \mathcal{JM}_t \mid \forall (j, m), (j', m) \in A, j = j' \}.
\end{align}
This formulation allows multiple job-machine assignments to be made at once, provided they respect machine capacity constraints. This represents a significant shift from prior work \cite{echeverria2024multi} and many other scheduling \cite{wang2023flexible, tassel2023end} or routing \cite{kwon2020pomo, drakulic2024bq} approaches, where the action space is typically defined as a single assignment set. In scheduling, prior to this work, the action space would generally be defined as $\mathcal{JM}_t$.

\textbf{Transition Function.} The state transitions are determined by the selected assignments. Specifically, at timestep $t$, the selected action $\mathcal{A} \in \mathcal{A}_t$ updates the graph $\mathcal{G}_t$ by modifying the attributes of nodes and edges to reflect the assignments made. Following the work in \cite{ho2024residual, echeverria2025diverse}, completed operations are removed to simplify the state space.

In this work, we do not explicitly define a reward function, as both the policy and self-evaluation models are trained using supervised learning on optimal solutions. An example of how the action set is defined and state transitions are performed is provided in Appendix \ref{app:example}.

\subsection{Self-Evaluation}

We refer to our method as SEVAL (Self-evaluation), which introduces a novel mechanism for evaluating subsets of actions collectively, enhancing solution quality compared to traditional step-by-step approaches. SEVAL relies on two complementary models: a policy model that proposes candidate assignments, and a self-evaluation model that scores these sets of assignments, enabling the selection of the most promising ones during inference. In this section, we describe the dataset generation process, the training procedure for both models, and the inference mechanism.


\subsubsection{Dataset Generation}

To train both models, a supervised learning approach is employed, requiring a dataset. Some methods, like those in \cite{kool2019attention, drakulic2024bq}, rely solely on expert trajectories, which can limit data diversity and hinder performance in complex scenarios. The dataset $\mathcal{D}$ is constructed by solving small-scale JSSP instances with an efficient solver, transforming each solution into sequences of states and actions:
\[
\mathcal{D} = \{(s, \mathcal{A}) \mid s \in \mathcal{S}, \mathcal{A} \subseteq \mathcal{A}_t \},
\]
where $s$ is a heterogeneous graph of the scheduling state, and $\mathcal{A}$ represents feasible job-machine assignments. 

A key issue with supervised learning is poor generalization \cite{ross2010efficient}, especially when policies encounter underrepresented states during testing. The value of adding suboptimal trajectories to enhance offline reinforcement learning was emphasized in \cite{kumar2021should, andres2025using}. Appendix \ref{app:dataset} details how perturbing optimal instances creates a more diverse training set, improving generalization.

 \begin{figure*}[h] \centering \includegraphics[width=1\linewidth]{arc.drawio} \caption{The architecture of the self-evaluation framework.} \label{fig:arcapp} \end{figure*}
 
\subsubsection{Policy Model Training}

The policy model predicts the probability distribution $\pi_\theta(a \mid s)$ over feasible job-machine assignments $a \in \mathcal{JM}$ for a given state $s$. To compute assignment probabilities, the model processes the state $s$ in two stages. First, a HGNN generates embeddings for the graph’s nodes. The HGNN is parameterized by $\phi$, uses $L$ propagation layers, and incorporates the attention mechanisms of GATv2 \cite{brody2021attentive} to effectively capture the structural and relational information of the state. Further details on the HGNN are provided in Appendix \ref{appendix:arquitecture}. Specifically, the HGNN computes embeddings $\boldsymbol{h_{j_{i}}^L}$ and $\boldsymbol{h_{m_{k}}^L}$, representing jobs and machines, respectively:
\begin{align}
\boldsymbol{h_{j_{i}}^L}, \boldsymbol{h_{m_{k}}^L} = \text{HGNN}_{\phi}(s).
\end{align}
Next, a Transformer processes these embeddings to compute the assignment probabilities. The input to the Transformer is a sequence that includes all feasible assignments $a \in \mathcal{JM}$. For each assignment, the input comprises the concatenation of the job embedding $\boldsymbol{h_{j_{i}}^L}$, the machine embedding $\boldsymbol{h_{m_{k}}^L}$, and the corresponding edge features $\boldsymbol{h_{mj_{ki}}}$. The Transformer then outputs the probability of the assignment:
\begin{align}
\mu_\theta(a | s) = \text{Transformer}_{\theta}([\boldsymbol{h_{j_{i}}^L},  \boldsymbol{h_{m_{k}}^L}, \boldsymbol{h_{mj_{ki}}]}).
\end{align}
The model is trained via supervised learning, minimizing the Kullback-Leibler (KL) divergence loss, as suggested in \cite{rusu2015policy}, to achieve more robust probability distributions. This loss measures the difference between the predicted distribution $\pi_\theta(a | s)$ and the target distribution $\pi_\text{solver}(a | s)$, which is derived from solver-generated optimal assignments:
\begin{align}
\mathcal{L}_{\text{policy}} = \text{KL}(\pi_\theta(a | s) , \pi_\text{solver}(a | s)).
\end{align}
Following this calculation, the weights of both the Transformer and the HGNN are updated to align the model’s predictions with the solver-provided target distribution.

\subsubsection{Self-Evaluation Model Training}

The self-evaluation model assigns a scalar score between 0 and 1 to subsets of job-machine assignments \( A \subseteq \mathcal{JM} \), representing the proportion of optimal assignments in the subset. In other words, a subset with no optimal assignments would receive a score of zero, while a subset where all assignments are optimal would receive a score of one. Once the model is trained, these scores are used during inference to  select high-quality sets of actions, ensuring that the generated solutions are closer to the optimal.

During training, the model leverages the embeddings $\boldsymbol{h_{j_{i}}^L}$ and $\boldsymbol{h_{m_{k}}^L}$ for jobs and machines, generated by the HGNN as part of the policy model training. Additionally, the embeddings for job-machine edges, $\boldsymbol{h_{mj_{ki}}}$, are included to encode relational information.

To mathematically represent sets of assignments, we model them as binary vectors. For each state \( s \) with its corresponding set of feasible job-machine assignments \( \mathcal{A} \subseteq \mathcal{JM} \), a binary vector \( \text{BV}(\mathcal{A}) \) is generated. This vector indicates whether each specific job-machine assignment is included in the subset, where a value of \( 1 \) denotes inclusion and \( 0 \) denotes exclusion.

During the training phase, for a given state \( s \) and its corresponding set of optimal assignments \( \mathcal{A}_{\text{opt}} \), a binary vector \( \text{BV}(A_\text{sub}) \) is randomly generated to simulate a subset of assignments. This vector indicates whether each job-machine pair is included in the subset and is compared against the optimal assignment vector to measure their similarity. The objective of the self-evaluation model is to learn to predict this similarity, which represents the degree to which a given subset resembles the optimal solution. This degree of similarity, referred to as the true score \( \text{TrueScore}(\mathcal{A}_\text{sub}),  \mathcal{A}_{\text{opt}} \), acts as the target during training and is computed using ground-truth information from the dataset.

To compute the predicted score for a given subset, the binary vector \( \text{BV}( \mathcal{A}_\text{sub}) \) is concatenated with the embeddings \( \boldsymbol{h_{j_{i}}^L} \), \( \boldsymbol{h_{m_{k}}^L} \), and \( \boldsymbol{h_{mj_{ki}}} \). These concatenated representations are processed by a Transformer architecture, which aggregates the information and applies mean pooling to produce a scalar score. The training process thus enables the self-evaluation model to assess the quality of any subset by estimating its proximity to the optimal solution.
\begin{align}
\text{SE}_\phi(A_\text{sub}) &= \text{MeanPooling} \Bigg( \text{Transformer}_{\phi}\bigg(     \nonumber \\
    &
        \big[\boldsymbol{h_{j_{i}}^L}, \boldsymbol{h_{m_{k}}^L}, \boldsymbol{h_{mj_{ki}}}, \text{BV}(A_\text{sub})\big]
    \bigg) 
\Bigg)
\end{align}
% \[
% \text{SE}_\phi(A_\text{sub}) = \text{MeanPooling}\left(\text{Transformer}_{\phi}\left([\boldsymbol{h_{j_{i}}^L}, \boldsymbol{h_{m_{k}}^L}, \boldsymbol{h_{mj_{ki}}}, \text{BV}(A_\text{sub})]\right)\right)
% \]
Figure~\ref{fig:arcapp} shows the overall architecture of the self-evaluation framework. The self-evaluation model is trained to minimize the Mean Squared Error (MSE) loss, aligning the predicted score $\text{SE}_\phi(A_\text{sub})$ with the true score $\text{TS}(A_\text{sub}, \mathcal{A}_{\text{opt}})$:
\begin{align}
\mathcal{L}_{\text{self-eval}} &= \frac{1}{|\mathcal{D}|} \sum_{|\mathcal{D}|} \big(\text{SE}_\phi(\mathcal{A}_\text{sub}) -  \nonumber \\
&\quad \text{TrueScore}(\mathcal{A}_\text{sub}, \mathcal{A}_{\text{opt}})\big)^2
\end{align}

In essence, the self-evaluation model aims to discern how closely a given subset resembles an optimal subset, providing critical feedback for selecting promising solutions during inference. The training process is summarized in Algorithm \ref{algo:training}, covering both the policy and self-evaluation models.

\begin{algorithm}[h]
\caption{Training of Policy and Self-Evaluation Models in SEVAL}
\label{algo:training}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Dataset $\mathcal{D}$, parameters $\theta$ (policy), $\phi$ (self-evaluation), $\phi_{\text{HGNN}}$ (HGNN), epochs $n_e$.
\FOR{$epoch = 1$ to $n_e$}
    \FOR{each batch $(s, A_\text{opt}) \in \mathcal{D}$}
        \STATE \textbf{Step 1: Compute HGNN Embeddings}
        \[
        \boldsymbol{h_{j_{i}}^L}, \boldsymbol{h_{m_{k}}^L} = \text{HGNN}_{\phi_{\text{HGNN}}}(s)
        \]

        \STATE \textbf{Step 2: Train Policy Model}
        \STATE Generate assignment probabilities $a \in \mathcal{JM}_t$.
        \[
        \pi_\theta(a | s) = \text{Transformer}_{\theta}(\boldsymbol{h_{j_{i}}^L} || \boldsymbol{h_{m_{k}}^L} || \boldsymbol{h_{mj_{ki}}})
        \]
        \STATE Compute KL loss:
        \[
        \mathcal{L}_{\text{policy}} = \text{KL}(\pi_\theta(a | s), \pi_\text{solver}(a | s))
        \]
        \STATE Update $\theta$ and $\phi_{\text{HGNN}}$ using $\mathcal{L}_{\text{policy}}$.

        \STATE \textbf{Step 3: Train Self-Evaluation Model}
        \STATE Generate random subsets $A_\text{sub}$.
\begin{multline*}
\text{SE}_\phi(A_\text{sub}) = \text{MeanPooling} \big( \text{Transformer}_{\phi}\big(\\ 
\big[\boldsymbol{h_{j_{i}}^L}, \boldsymbol{h_{m_{k}}^L}, \boldsymbol{h_{mj_{ki}}}, \text{BV}(A_\text{sub})\big]\big)\big)
\end{multline*}
        \STATE Compute MSE loss:
\begin{multline*}
\mathcal{L}_{\text{self-eval}} = \frac{1}{|\mathcal{D}|} \sum_{|\mathcal{D}|} \big(\text{SE}_\phi(\mathcal{A}_\text{sub}) -   \\
\quad \text{TrueScore}(\mathcal{A}_\text{sub}, \mathcal{A}_{\text{opt}})\big)^2
\end{multline*}
        \STATE Update $\phi$ using $\mathcal{L}_{\text{self-eval}}$.
    \ENDFOR
\ENDFOR
\STATE \textbf{Output:} $\pi_\theta$, $\text{SE}_\phi$, $\phi_{\text{HGNN}}$
\end{algorithmic}
\end{algorithm}
During inference, the policy model and the self-evaluation model work together to determine the best subsets of actions at each timestep. At timestep $t$, the policy model generates $n$ candidate subsets by sampling from the learned probability distribution $\pi_\theta(a | s)$. Each subset is composed of $k$ feasible actions.\\

The self-evaluation model scores each of these subsets based on their optimality. The subset with the highest score is then selected for execution:
\begin{align}
A_t^* = \arg\max_{A \in \mathcal{A}_t} \text{SE}_\phi(A),
\end{align}
where $\mathcal{A}_t$ denotes the set of all candidate subsets generated by the policy model at timestep $t$, and $\text{SE}_\phi(A)$ is the scalar score assigned to each subset by the self-evaluation model. 



