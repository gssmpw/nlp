\section{Model architecture}
\label{appendix:arquitecture}

In this section, we illustrate how embeddings are calculated within the HGNN model, using the machine embeddings as an example. The embedding process begins with a linear transformation applied to the initial features of machine, job, and operation nodes, denoted as $\boldsymbol{h_m}$, $\boldsymbol{h_o}$, and $\boldsymbol{h_j}$, respectively. These features represent the states of machines, operations, and jobs after the linear transformation, which ensures dimensional consistency. The embeddings are then iteratively updated across $L$ layers of the HGNN. At each layer, the new output is added to the input embedding, ensuring that the embeddings are progressively refined while retaining information from previous layers.

Machines aggregate information from connected operations. The attention coefficient between a machine $m_i$ and a connected operation $o_{ij} \in \mathcal{O}_{m_i}$ is computed as:
\begin{equation}
e_{mo_{ijk}} = {\boldsymbol{a^{\mathcal{MO}}}}^{\top} \text{LeakyReLU} \left( \boldsymbol{W_1^\mathcal{MO}} \boldsymbol{h_{m_i}} + \boldsymbol{W_2^\mathcal{MO}} \boldsymbol{h_{o_{ij}}} + \boldsymbol{W_3^\mathcal{MO}} \boldsymbol{h_{p_{ijk}}} \right),
\end{equation}
where ${\boldsymbol{a^{\mathcal{MO}}}}^{\top} \in \mathbb{R}^{3d_{\mathcal{MO}}^\prime}$ is a learnable vector, and $\boldsymbol{W_1^\mathcal{MO}}, \boldsymbol{W_2^\mathcal{MO}}, \boldsymbol{W_3^\mathcal{MO}} \in \mathbb{R}^{d_{\mathcal{MO}}^\prime \times d_{\mathcal{MO}}}$ are trainable weight matrices. The edge embedding $\boldsymbol{h_{p_{ijk}}}$ contains features of the task connecting the machine and the operation.

Similarly, machines also aggregate information from jobs if the first operation of a job $j$ is ready to be assigned to machine $m_i$. For these connections, the attention coefficient between a machine $m_i$ and a job $j \in \mathcal{J}$ is computed as:
\begin{equation}
e_{mj_{ik}} = {\boldsymbol{a^{\mathcal{MJ}}}}^{\top} \text{LeakyReLU} \left( \boldsymbol{W_1^\mathcal{MJ}} \boldsymbol{h_{m_i}} + \boldsymbol{W_2^\mathcal{MJ}} \boldsymbol{h_{j}} + \boldsymbol{W_3^\mathcal{MJ}} \boldsymbol{h_{mj_{ij}}} \right),
\end{equation}
where ${\boldsymbol{a^{\mathcal{MJ}}}}^{\top}$ and $\boldsymbol{W_1^\mathcal{MJ}}, \boldsymbol{W_2^\mathcal{MJ}}, \boldsymbol{W_3^\mathcal{MJ}}$ are learnable parameters, and $\boldsymbol{h_{mj_{ij}}}$ represents the features of the edge connecting the machine and job nodes. These edges are only considered if the first operation of the job is ready and can be assigned to the machine.

Once the attention coefficients are normalized using a softmax function, the updated embedding for a machine is calculated as:
\begin{equation}
\boldsymbol{h_{m_i}}^\prime = \text{ELU} \left( \sum_{o_{ij} \in \mathcal{O}_{m_i}} \alpha_{mo_{ijk}} \left( \boldsymbol{W_2^\mathcal{MO}} \boldsymbol{h_{o_{ij}}} + \boldsymbol{W_3^\mathcal{MO}} \boldsymbol{h_{p_{ijk}}} \right) + \sum_{j \in \mathcal{J}_{m_i}} \alpha_{mj_{ik}} \left( \boldsymbol{W_2^\mathcal{MJ}} \boldsymbol{h_{j}} + \boldsymbol{W_3^\mathcal{MJ}} \boldsymbol{h_{mj_{ij}}} \right) \right)
\end{equation}
where $\alpha_{mo_{ijk}}$ and $\alpha_{mj_{ik}}$ are the normalized attention coefficients for machine-operation and machine-job edges, respectively.

This process is repeated for $L$ layers of the HGNN. At each layer $l$, the input embedding $\boldsymbol{h_{m_i}}^{(l-1)}$ is updated as:
\begin{equation}
\boldsymbol{h_{m_i}}^{(l)} = \boldsymbol{h_{m_i}}^{(l-1)} + \boldsymbol{h_{m_i}}
\end{equation}
where $\boldsymbol{h_{m_i}}$ is the initial embedding.





