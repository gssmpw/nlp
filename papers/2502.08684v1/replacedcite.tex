\section{Related Work}
\label{sec:state_of_the_art}

An early breakthrough in NCO leveraged supervised learning with recurrent neural networks, applied to COPs such as the Traveling Salesman Problem (TSP) ____. Although exact solvers are limited to small instances, they provided high-quality data to train neural networks capable of generalizing to larger problems, demonstrating the potential of learning-based methods for COPs.

Subsequent work, particularly in routing problems, shifted towards deep reinforcement learning (DRL) to train policies ____. These methods introduced various improvements in neural network architectures and strategies to exploit symmetries inherent in routing problems. Typically, they followed a \emph{constructive} approach, incrementally building solutions by selecting the next element—such as the next city in the TSP—until the solution was complete.

Building on the successes in routing, scheduling problems have also been addressed predominantly with deep reinforcement learning (DRL), incorporating various adjustments in policy training and state modeling ____. In addition to recurrent neural networks and transformers ____, commonly used in routing problems, scheduling methods leverage architectures specifically designed to handle the unique challenges of these problems. Unlike routing, scheduling involves more complex entities—such as operations, jobs, and machines—where HGNNs are often utilized ____. HGNNs effectively represent multiple node and edge types, capturing the intricate relationships inherent in scheduling problems. However, most scheduling and routing methods adopt a \emph{constructive} approach, where operations are sequentially assigned, and policies are trained to make one assignment at a time. This severely limits their ability to capture broader dependencies between assignments, leading to suboptimal solutions. Additionally, research on hybridizing different architectures remains limited, as current approaches tend to exclusively rely on either recurrent neural networks or transformers, instead of combining their strengths to better handle the complexity of scheduling problems.

Although less common, \emph{neural improvement methods} provide an alternative to constructive approaches by refining an existing solution rather than building it step by step ____. For example, in the JSSP ____, these methods learn policies—often via DRL variants—that rearrange operations pair by pair to optimize the execution sequence. However, this approach, where the action space is a set rather than a subsequence, faces similar challenges to constructive methods—namely, the difficulty of building subsequences of changes that are closer to the optimal solution.

Recently, researchers have begun exploring alternatives to standard DRL, in favor of self-supervised strategies ____, a return to supervised methods ____, or offline RL ____, to overcome DRL’s limitations, most notably the extensive interactions required for exploration. Some approaches have started assigning more than one action simultaneously, such as the method in ____, which combines policy gradient methods with imitation learning, and the approach in ____, which relies on supervised learning. However, these methods lack a self-evaluation mechanism and do not move beyond the standard MDP framework. Actions are assigned solely based on the probabilities provided by the policy model, without evaluating how close the generated sequence is to the optimal solution. This can result in action sets lacking internal coherence, as they are not directly evaluated using a dedicated evaluation function.

The main contribution of this paper is to depart from generating solutions action by action and, instead, produce and evaluate subsequences collectively using a \emph{self-evaluation} mechanism inspired by the success of LLMs. This approach avoids the traditional stepwise paradigm of predicting a single move without supervision. Additionally, we integrate HGNNs with Transformers, both to generate the policy and to define the \emph{self-evaluation} function.