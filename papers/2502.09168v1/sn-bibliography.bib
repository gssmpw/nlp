@inproceedings{lacerra-etal-2021-genesis,
    title = "{G}ene{S}is: {A} {G}enerative {A}pproach to {S}ubstitutes in {C}ontext",
    author = "Lacerra, Caterina  and
      Tripodi, Rocco  and
      Navigli, Roberto",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.844/",
    doi = "10.18653/v1/2021.emnlp-main.844",
    pages = "10810--10823",
    abstract = "The lexical substitution task aims at generating a list of suitable replacements for a target word in context, ideally keeping the meaning of the modified text unchanged. While its usage has increased in recent years, the paucity of annotated data prevents the finetuning of neural models on the task, hindering the full fruition of recently introduced powerful architectures such as language models. Furthermore, lexical substitution is usually evaluated in a framework that is strictly bound to a limited vocabulary, making it impossible to credit appropriate, but out-of-vocabulary, substitutes. To assess these issues, we proposed GeneSis (Generating Substitutes in contexts), the first generative approach to lexical substitution. Thanks to a seq2seq model, we generate substitutes for a word according to the context it appears in, attaining state-of-the-art results on different benchmarks. Moreover, our approach allows silver data to be produced for further improving the performances of lexical substitution systems. Along with an extensive analysis of GeneSis results, we also present a human evaluation of the generated substitutes in order to assess their quality. We release the fine-tuned models, the generated datasets, and the code to reproduce the experiments at \url{https://github.com/SapienzaNLP/genesis}."
}

@inproceedings{ijcai2021p528,
  author       = {Caterina Lacerra and
                  Tommaso Pasini and
                  Rocco Tripodi and
                  Roberto Navigli},
  editor       = {Zhi{-}Hua Zhou},
  title        = {ALaSca: an Automated approach for Large-Scale Lexical Substitution},
  booktitle    = {Proceedings of the Thirtieth International Joint Conference on Artificial
                  Intelligence, {IJCAI} 2021, Virtual Event / Montreal, Canada, 19-27
                  August 2021},
  pages        = {3836--3842},
  publisher    = {ijcai.org},
  year         = {2021},
  url          = {https://doi.org/10.24963/ijcai.2021/528},
  doi          = {10.24963/IJCAI.2021/528},
  timestamp    = {Tue, 15 Oct 2024 16:43:28 +0200},
  biburl       = {https://dblp.org/rec/conf/ijcai/LacerraPTN21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/aaai/Ramirez-OrtaXMM22,
  author    = {Juan Antonio Ramirez{-}Orta and
               Eduardo Xamena and
               Ana Gabriela Maguitman and
               Evangelos E. Milios and
               Axel J. Soto},
  title     = {Post-OCR Document Correction with Large Ensembles of Character Sequence-to-Sequence
               Models},
  booktitle = {Thirty-Sixth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2022, Thirty-Fourth Conference on Innovative Applications of Artificial
               Intelligence, {IAAI} 2022, The Twelveth Symposium on Educational Advances
               in Artificial Intelligence, {EAAI} 2022 Virtual Event, February 22
               - March 1, 2022},
  pages     = {11192--11199},
  publisher = {{AAAI} Press},
  year      = {2022},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/21369},
  timestamp = {Sat, 17 Sep 2022 23:41:12 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/Ramirez-OrtaXMM22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@book{weibull1997evolutionary,
  title={Evolutionary game theory},
  author={Weibull, J. W.},
  year={1997},
    address={Cambridge, MA, USA},
  publisher={MIT press}
}

@inproceedings{10.1145/3583780.3615036,
 abstract = {Discovering entity mentions that are out of a Knowledge Base (KB) from texts plays a critical role in KB maintenance, but has not yet been fully explored. The current methods are mostly limited to the simple threshold-based approach and feature-based classification, and the datasets for evaluation are relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL) method which can identify mentions that do not have corresponding KB entities by matching them to a special NIL entity. To better utilize BERT, we propose new techniques including NIL entity representation and classification, with synonym enhancement. We also apply KB Pruning and Versioning strategies to automatically construct out-of-KB datasets from common in-KB EL datasets. Results on five datasets of clinical notes, biomedical publications, and Wikipedia articles in various domains show the advantages of BLINKout over existing methods to identify out-of-KB mentions for the medical ontologies, UMLS, SNOMED CT, and the general KB, WikiData.},
 address = {New York, NY, USA},
 author = {Dong, Hang and Chen, Jiaoyan and He, Yuan and Liu, Yinan and Horrocks, Ian},
 booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
 doi = {10.1145/3583780.3615036},
 isbn = {9798400701245},
 keywords = {WikiData, biomedical ontologies, entity linking, knowledge base enrichment, language models},
 location = {Birmingham, United Kingdom},
 numpages = {11},
 pages = {452–462},
 publisher = {Association for Computing Machinery},
 series = {CIKM '23},
 title = {{Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking}},
 year = {2023}
}

@article{10.3233/SW-170273,
 abstract = {Named Entity Disambiguation is the task of assigning entities from a Knowledge Graph (KG) to mentions of such entities in a textual document. The state-of-the-art for this task balances two disparate sources of similarity: lexical, defined as the pairwise similarity between mentions in the text and names of entities in the KG; and semantic, defined through some graph-theoretic property of a subgraph of the KG induced by the choice of entities for each mention. Departing from previous work, our notion of semantic similarity is rooted in Information Theory and is defined as the mutual information between random walks on the disambiguation graph induced by choice of entities for each mention. We describe an iterative algorithm based on this idea, and show an extension that uses learning-to-rank, which yields further improvements. Our experimental evaluation demonstrates that this approach is robust and very competitive on well-known existing benchmarks. We also justify the need for new and more difficult benchmarks, and provide an extensive experimental comparison of our method and previous work on these new benchmarks.},
 address = {NLD},
 author = {Alani, Harith and Guo, Zhaochen and Barbosa, Denilson},
 doi = {10.3233/SW-170273},
 issn = {1570-0844},
 issue_date = {2018},
 journal = {Semantic Web Journal},
 keywords = {random walk, benchmarking, entity linking, entity disambiguation, Named entities, relatedness measure},
 month = {January},
 number = {4},
 numpages = {21},
 pages = {459–479},
 publisher = {IOS Press},
 title = {{Robust Named Entity Disambiguation with Random Walks}},
 url = {https://doi.org/10.3233/SW-170273},
 volume = {9},
 year = {2018}
}

@article{4767390,
 author = {Hummel, Robert A. and Zucker, Steven W.},
 doi = {10.1109/TPAMI.1983.4767390},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {3},
 pages = {267-287},
 title = {{On the Foundations of Relaxation Labeling Processes}},
 volume = {PAMI-5},
 year = {1983}
}

@inproceedings{agarwal-etal-2018-dianed,
    title = "dia{NED}: Time-Aware Named Entity Disambiguation for Diachronic Corpora",
    author = {Agarwal, Prabal  and
      Str{\"o}tgen, Jannik  and
      del Corro, Luciano  and
      Hoffart, Johannes  and
      Weikum, Gerhard},
    editor = "Gurevych, Iryna and Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = "July",
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P18-2109",
    pages = "686--693",
    abstract = "Named Entity Disambiguation (NED) systems perform well on news articles and other texts covering a specific time interval. However, NED quality drops when inputs span long time periods like in archives or historic corpora. This paper presents the first time-aware method for NED that resolves ambiguities even when mention contexts give only few cues. The method is based on computing temporal signatures for entities and comparing these to the temporal contexts of input mentions. Our experiments show superior quality on a newly created diachronic corpus.",
}

@inproceedings{ayoola-etal-2022-improving,
 abstract = {Recent work in entity disambiguation (ED) has typically neglected structured knowledge base (KB) facts, and instead relied on a limited subset of KB information, such as entity descriptions or types. This limits the range of contexts in which entities can be disambiguated. To allow the use of all KB facts, as well as descriptions and types, we introduce an ED model which links entities by reasoning over a symbolic knowledge base in a fully differentiable fashion. Our model surpasses state-of-the-art baselines on six well-established ED datasets by 1.3 F1 on average. By allowing access to all KB information, our model is less reliant on popularity-based entity priors, and improves performance on the challenging ShadowLink dataset (which emphasises infrequent and ambiguous entities) by 12.7 F1.},
 address = {Seattle, United States},
 author = {Ayoola, Tom  and
Fisher, Joseph  and
Pierleoni, Andrea},
 booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2022.naacl-main.210},
 editor = {Carpuat, Marine  and
de Marneffe, Marie-Catherine  and
Meza Ruiz, Ivan Vladimir},
 month = {July},
 pages = {2899--2912},
 publisher = {Association for Computational Linguistics},
 title = {Improving Entity Disambiguation by Reasoning over a Knowledge Base},
 year = {2022}
}


@inproceedings{Banarescu2013,
    title = "{A}bstract {M}eaning {R}epresentation for Sembanking",
    author = "Banarescu, Laura  and
      Bonial, Claire  and
      Cai, Shu  and
      Georgescu, Madalina  and
      Griffitt, Kira  and
      Hermjakob, Ulf  and
      Knight, Kevin  and
      Koehn, Philipp  and
      Palmer, Martha  and
      Schneider, Nathan",
    editor = "Pareja-Lora, Antonio  and
      Liakata, Maria  and
      Dipper, Stefanie",
    booktitle = "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W13-2322",
    pages = "178--186",
}

@inproceedings{barba-etal-2022-extend,
    title = "{E}xt{E}n{D}: Extractive Entity Disambiguation",
    author = "Barba, Edoardo  and
      Procopio, Luigi  and
      Navigli, Roberto",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2022.acl-long.177",
    pages = "2478--2488",
    abstract = "Local models for Entity Disambiguation (ED) have today become extremely powerful, in most part thanks to the advent of large pre-trained language models. However, despite their significant performance achievements, most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling perspective. In contrast with this trend, here we propose ExtEnD, a novel local formulation for ED where we frame this task as a text extraction problem, and present two Transformer-based architectures that implement it. Based on experiments in and out of domain, and training over two different data regimes, we find our approach surpasses all its competitors in terms of both data efficiency and raw performance. ExtEnD outperforms its alternatives by as few as 6 F1 points on the more constrained of the two data regimes and, when moving to the other higher-resourced regime, sets a new state of the art on 4 out of 4 benchmarks under consideration, with average improvements of 0.7 F1 points overall and 1.1 F1 points out of domain. In addition, to gain better insights from our results, we also perform a fine-grained evaluation of our performances on different classes of label frequency, along with an ablation study of our architectural choices and an error analysis. We release our code and models for research purposes at \url{https://github.com/SapienzaNLP/extend}.",
}

@inproceedings{benkhedda-etal-2024-enriching,
 abstract = {Digital archive collections that have been contributed by communities, known as community-generated digital content (CGDC), are important sources of historical and cultural knowledge. However, CGDC items are not easily searchable due to semantic information being obscured within their textual metadata. In this paper, we investigate the extent to which state-of-the-art, general-domain entity linking (EL) models (i.e., BLINK, EPGEL and mGENRE) can map named entities mentioned in CGDC textual metadata, to Wikidata entities. We evaluate and compare their performance on an annotated dataset of CGDC textual metadata and provide some error analysis, in the way of informing future studies aimed at enriching CGDC metadata using entity linking methods.},
 address = {St. Julians, Malta},
 author = {Benkhedda, Youcef  and
Skapars, Adrians  and
Schlegel, Viktor  and
Nenadic, Goran  and
Batista-Navarro, Riza},
 booktitle = {Proceedings of the 8th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature (LaTeCH-CLfL 2024)},
 editor = {Bizzoni, Yuri  and
Degaetano-Ortlieb, Stefania  and
Kazantseva, Anna  and
Szpakowicz, Stan},
 month = {March},
 pages = {213--220},
 publisher = {Association for Computational Linguistics},
 title = {Enriching the Metadata of Community-Generated Digital Content through Entity Linking: An Evaluative Comparison of State-of-the-Art Models},
 url = {https://aclanthology.org/2024.latechclfl-1.20},
 year = {2024}
}

@article{Bevilacqua2021,
 abstractnote = {In Text-to-AMR parsing, current state-of-the-art semantic parsers use cumbersome pipelines integrating several different modules or components, and exploit graph recategorization, i.e., a set of content-specific heuristics that are developed on the basis of the training set. However, the generalizability of graph recategorization in an out-of-distribution setting is unclear. In contrast, state-of-the-art AMR-to-Text generation, which can be seen as the inverse to parsing, is based on simpler seq2seq. In this paper, we cast Text-to-AMR and AMR-to-Text as a symmetric transduction task and show that by devising a careful graph linearization and extending a pretrained encoder-decoder model, it is possible to obtain state-of-the-art performances in both tasks using the very same seq2seq approach, i.e., SPRING (Symmetric PaRsIng aNd Generation). Our model does not require complex pipelines, nor heuristics built on heavy assumptions. In fact, we drop the need for graph recategorization, showing that this technique is actually harmful outside of the standard benchmark. Finally, we outperform the previous state of the art on the English AMR 2.0 dataset by a large margin: on Text-to-AMR we obtain an improvement of 3.6 Smatch points, while on AMR-to-Text we outperform the state of the art by 11.2 BLEU points. We release the software at github.com/SapienzaNLP/spring.},
 author = {Bevilacqua, Michele and Blloshmi, Rexhina and Navigli, Roberto},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 month = {May},
 number = {14},
 pages = {12564--12573},
 title = {{One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline}},
 url = {https://ojs.aaai.org/index.php/AAAI/article/view/17489},
 volume = {35},
 year = {2021}
}

@inproceedings{blouin-etal-2024-dataset-named,
 abstract = {In this study, we present a novel historical Chinese dataset for named entity recognition, entity linking, coreference and entity relations. We use data from Chinese newspapers from 1872 to 1949 and multilingual bibliographic resources from the same period. The period and the language are the main strength of the present work, offering a resource which covers different styles and language uses, as well as the largest historical Chinese NER dataset with manual annotations from this transitional period. After detailing the selection and annotation process, we present the very first results that can be obtained from this dataset. Texts and annotations are freely downloadable from the GitHub repository.},
 address = {Torino, Italia},
 author = {Blouin, Baptiste  and
Armand, C{\'e}cile  and
Henriot, Christian},
 booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
 editor = {Calzolari, Nicoletta  and
Kan, Min-Yen  and
Hoste, Veronique  and
Lenci, Alessandro  and
Sakti, Sakriani  and
Xue, Nianwen},
 month = {May},
 pages = {385--394},
 publisher = {ELRA and ICCL},
 title = {A Dataset for Named Entity Recognition and Entity Linking in {C}hinese Historical Newspapers},
 url = {https://aclanthology.org/2024.lrec-main.35},
 year = {2024}
}

@inproceedings{boros2020robust,
  author    = {Emanuela Boros and
               Elvys Linhares Pontes and
               Luis Adri{\'a}n Cabrera-Diego and
               Ahmed Hamdi and
               Jose G. Moreno and
               Nicolas Sid{\`e}re and
               Antoine Doucet},
  title     = {Robust Named Entity Recognition and Linking on Historical Multilingual Documents},
  booktitle = {Working Notes of {CLEF} 2020},
  volume    = {2696},
  pages     = {1--17},
  year      = {2020},
  url       = {https://ceur-ws.org/Vol-2696/paper\_171.pdf}
}

@inproceedings{borosKnowledgebasedContextsHistorical,
  title = {Knowledge-based Contexts for Historical Named Entity Recognition and Linking},
  author = {Emanuela Boros and Carlos-Emiliano González-Gallardo and Edward Giamphy and Ahmed Hamdi and Jose G. Moreno 0001 and Antoine Doucet},
    month = {September},
  year = {2022},
  url = {http://ceur-ws.org/Vol-3180/paper-84.pdf},
  pages = {1064-1078},
  booktitle = {Proceedings of the Working Notes of CLEF 2022 - Conference and Labs of the Evaluation Forum},
  editor = {Guglielmo Faggioli and Nicola Ferro and Allan Hanbury and Martin Potthast},
  volume = {3180},
  series = {CEUR Workshop Proceedings},
  publisher = {CEUR-WS.org},
address = {Bologna, Italy}
}

@inproceedings{botha-etal-2020-entity,
 abstract = {We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20 million entities. The model outperforms state-of-the-art results from a far more limited cross-lingual linking task. Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation. To this end, we provide Mewsli-9, a large new multilingual dataset matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.},
 address = {Online},
 author = {Botha, Jan A.  and
Shan, Zifei  and
Gillick, Daniel},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.630},
 editor = {Webber, Bonnie  and
Cohn, Trevor  and
He, Yulan  and
Liu, Yang},
 month = {November},
 pages = {7833--7845},
 publisher = {Association for Computational Linguistics},
 title = {{E}ntity {L}inking in 100 {L}anguages},
 year = {2020}
}

@inproceedings{bunescu-pasca-2006-using,
    title = "Using Encyclopedic Knowledge for Named entity Disambiguation",
    author = "Bunescu, Razvan  and
      Pa{\c{s}}ca, Marius",
    editor = "McCarthy, Diana  and
      Wintner, Shuly",
    booktitle = "11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2006",
    address = "Trento, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E06-1002",
    pages = "9--16",
}


@misc{castro-2017-fast-krippendorff,
  author    = {Santiago Castro},
  title     = {Fast {K}rippendorff: Fast computation of {K}rippendorff's alpha agreement measure},
  year      = {2017},
  howpublished = {GitHub repository},
  publisher = {GitHub},
  note      = {Available at: \url{https://github.com/pln-fing-udelar/fast-krippendorff}. Accessed 13 Aug 2024}
}


@inproceedings{chen-etal-2021-evaluating,
 abstract = {Retrieval is a core component for open-domain NLP tasks. In open-domain tasks, multiple entities can share a name, making disambiguation an inherent yet under-explored problem. We propose an evaluation benchmark for assessing the entity disambiguation capabilities of these retrievers, which we call Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection of entities that share a name along with queries about those entities. By covering the set of entities for polysemous names, AmbER sets act as a challenging test of entity disambiguation. We create AmbER sets for three popular open-domain tasks: fact checking, slot filling, and question answering, and evaluate a diverse set of retrievers. We find that the retrievers exhibit popularity bias, significantly under-performing on rarer entities that share a name, e.g., they are twice as likely to retrieve erroneous documents on queries for the less popular entity under the same name. These experiments on AmbER sets show their utility as an evaluation tool and highlight the weaknesses of popular retrieval systems.},
 address = {Online},
 author = {Chen, Anthony  and
Gudipati, Pallavi  and
Longpre, Shayne  and
Ling, Xiao  and
Singh, Sameer},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.345},
 month = {August},
 pages = {4472--4485},
 publisher = {Association for Computational Linguistics},
 title = {Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based {NLP}},
 year = {2021}
}


@article{ROTABULO2011984,
title = {Graph-based quadratic optimization: A fast evolutionary approach},
journal = {Computer Vision and Image Understanding},
volume = {115},
number = {7},
pages = {984-995},
year = {2011},
note = {Special issue on Graph-Based Representations in Computer Vision},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2010.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1077314211000713},
author = {Samuel {Rota Bulò} and Marcello Pelillo and Immanuel M. Bomze},
keywords = {Quadratic optimization, Population dynamics, Graph-based problems},
abstract = {Quadratic optimization lies at the very heart of many structural pattern recognition and computer vision problems, such as graph matching, object recognition, image segmentation, etc., and it is therefore of crucial importance to devise algorithmic solutions that are both efficient and effective. As it turns out, a large class of quadratic optimization problems can be formulated in terms of so-called “standard quadratic programs” (StQPs), which ask for finding the extrema of a quadratic polynomial over the standard simplex. Computationally, the standard approach for attacking this class of problems is to use replicator dynamics, a well-known family of algorithms from evolutionary game theory inspired by Darwinian selection processes. Despite their effectiveness in finding good solutions in a variety of applications, however, replicator dynamics suffer from being computationally expensive, as they require a number of operations per step which grows quadratically with the dimensionality of the problem being solved. In order to avoid this drawback, in this paper we propose a new population game dynamics (InImDyn) which is motivated by the analogy with infection and immunization processes within a population of “players.” We prove that the evolution of our dynamics is governed by a quadratic Lyapunov function, representing the average population payoff, which strictly increases along non-constant trajectories and that local solutions of StQPs are asymptotically stable (i.e., attractive) points. Each step of InImDyn is shown to have a linear time/space complexity, thereby allowing us to use it as a more efficient alternative to standard approaches for solving StQPs and related optimization problems. Indeed, we demonstrate experimentally that InImDyn is orders of magnitude faster than, and as accurate as, replicator dynamics on various applications ranging from tree matching to image registration, matching and segmentation.}
}

@article{DBLP:journals/jmiv/Pelillo97,
 author = {Marcello Pelillo},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/jmiv/Pelillo97.bib},
 doi = {10.1023/A:1008255111261},
 journal = {J. Math. Imaging Vis.},
 number = {4},
 pages = {309--323},
 timestamp = {Mon, 08 Jun 2020 22:22:07 +0200},
 title = {{The Dynamics of Nonlinear Relaxation Labeling Processes}},
 url = {https://doi.org/10.1023/A:1008255111261},
 volume = {7},
 year = {1997}
}


@article{DBLP:journals/orl/MillerZ91,
title = {Copositive-plus Lemke algorithm solves polymatrix games},
journal = {Operations Research Letters},
volume = {10},
number = {5},
pages = {285-290},
year = {1991},
issn = {0167-6377},
doi = {https://doi.org/10.1016/0167-6377(91)90015-H},
url = {https://www.sciencedirect.com/science/article/pii/016763779190015H},
author = {Douglas A. Miller and Steven W. Zucker},
keywords = {linear complementarity problem, polymatrix games, relaxation labeling, artificial neural networks, dynamical systems},
abstract = {A problem in visual labeling and artificial neural networks, equivalent to finding Nash equilibria for polymatrix n-person games, may be solved by the copositive-plus Lemke algorithm. Analysis suggests efficiency improves Howson's recursive method by O(n−1) and is same order as Eaves' L2-formulation. Method extends to any dynamical system p′ = Rp + c, Ap ≤ q, p ≥ 0.}
}


@article{de-cao-etal-2022-multilingual,
    title = "Multilingual Autoregressive Entity Linking",
    author = "De Cao, Nicola  and
      Wu, Ledell  and
      Popat, Kashyap  and
      Artetxe, Mikel  and
      Goyal, Naman  and
      Plekhanov, Mikhail  and
      Zettlemoyer, Luke  and
      Cancedda, Nicola  and
      Riedel, Sebastian  and
      Petroni, Fabio",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.16",
    doi = "10.1162/tacl_a_00460",
    pages = "274--290",
    abstract = "We present mGENRE, a sequence-to- sequence system for the Multilingual Entity Linking (MEL) problem{---}the task of resolving language-specific mentions to a multilingual Knowledge Base (KB). For a mention in a given language, mGENRE predicts the name of the target entity left-to-right, token-by-token in an autoregressive fashion. The autoregressive formulation allows us to effectively cross-encode mention string and entity names to capture more interactions than the standard dot product between mention and entity vectors. It also enables fast search within a large KB even for mentions that do not appear in mention tables and with no need for large-scale vector indices. While prior MEL works use a single representation for each entity, we match against entity names of as many languages as possible, which allows exploiting language connections between source input and target name. Moreover, in a zero-shot setting on languages with no training data at all, mGENRE treats the target language as a latent variable that is marginalized at prediction time. This leads to over 50{\%} improvements in average accuracy. We show the efficacy of our approach through extensive evaluation including experiments on three popular MEL benchmarks where we establish new state-of-the-art results. Source code available at \url{https://github.com/facebookresearch/GENRE}.",
}
@inproceedings{decao2021autoregressive,
 author = {Nicola {De Cao} and
Gautier Izacard and
Sebastian Riedel and
Fabio Petroni},
 booktitle = {9th International Conference on Learning Representations},
 publisher = {OpenReview.net},
 title = {Autoregressive Entity Retrieval},
 url = {https://openreview.net/forum?id=5k8F6UU39V},
address = {Online, Austria},
month = {May},
 year = {2021}
}

@article{decao2022mgenre,
 author = {Nicola De Cao and
Ledell Wu and
Kashyap Popat and
Mikel Artetxe and
Naman Goyal and
Mikhail Plekhanov and
Luke Zettlemoyer and
Nicola Cancedda and
Sebastian Riedel and
Fabio Petroni},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/tacl/CaoWPAGPZCRP22.bib},
 doi = {10.1162/TACL\_A\_00460},
 journal = {Trans. Assoc. Comput. Linguistics},
 pages = {274--290},
 timestamp = {Wed, 19 Jun 2024 17:28:03 +0200},
 title = {Multilingual Autoregressive Entity Linking},
 url = {https://doi.org/10.1162/tacl\_a\_00460},
 volume = {10},
 year = {2022}
}

@inproceedings{devlin-etal-2019-bert,
 abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1423},
 month = {June},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 year = {2019}
}

@inproceedings{ehrmann2016diachronic,
 address = {Bochum, Germany},
 author = {Ehrmann, Maud and Colavizza, Giovanni and Rochat, Yannick and Kaplan, Frédéric},
 booktitle = {Proceedings of the 13th Conference on Natural Language Processing (KONVENS 2016)},
 editor = {Dipper, Stephanie and Neubarth, Friedrich and Zinsmeister, Heike},
 keywords = {named entities; evaluation; historical newspapers; digital humanities; natural language processing},
 month = {September 19--21},
 pages = {97--107},
 publisher = {Bochum, Germany, Bochumer Linguistische Arbeitsberichte},
 title = {{Diachronic Evaluation of NER Systems on Old Newspapers}},
 url = {https://infoscience.epfl.ch/record/221391?v=pdf},
 year = {2016}
}

@article{ehrmannNamedEntityRecognition2023,
 abstract = {After decades of massive digitisation, an unprecedented number of historical documents are available in digital format, along with their machine-readable texts. While this represents a major step forward with respect to preservation and accessibility, it also opens up new opportunities in terms of content mining and the next fundamental challenge is to develop appropriate technologies to efficiently search, retrieve, and explore information from this ‘big data of the past’. Among semantic indexing opportunities, the recognition and classification of named entities are in great demand among humanities scholars. Yet, named entity recognition (NER) systems are heavily challenged with diverse, historical, and noisy inputs. In this survey, we present the array of challenges posed by historical documents to NER, inventory existing resources, describe the main approaches deployed so far, and identify key priorities for future developments.},
 address = {New York, NY, USA},
 articleno = {27},
 author = {Ehrmann, Maud and Hamdi, Ahmed and Pontes, Elvys Linhares and Romanello, Matteo and Doucet, Antoine},
 doi = {10.1145/3604931},
 issn = {0360-0300},
 issue_date = {February 2024},
 journal = {ACM Computing Surveys},
 keywords = {digital humanities, natural language processing, historical documents, Named entity recognition and classification},
 month = {sep},
 number = {2},
 numpages = {47},
 publisher = {Association for Computing Machinery},
 title = {Named Entity Recognition and Classification in Historical Documents: A Survey},
 url = {https://doi.org/10.1145/3604931},
 volume = {56},
 year = {2023}
}

@misc{Gabrilovich2013,
 author = {Evgeniy Gabrilovich and Michael Ringgaard and Amarnag Subramanya},
 howpublished = {Web Download},
 month = {June},
 note = {Available at \url{http://lemurproject.org/clueweb09/} and \url{http://lemurproject.org/clueweb12/}},
 title = {{FACC1: Freebase annotation of ClueWeb corpora, Version 1 (Release date 2013-06-26, Format version 1, Correction level 0)}},
 year = {2013}
}

@misc{Graff2002Aquaint,
 author = {David Graff},
 publisher = {Linguistic Data Consortium},
 title = {The AQUAINT Corpus of English News Text},
 url = {https://catalog.ldc.upenn.edu/LDC2002T31},
 year = {2002}
}

@article{guellilEntityLinkingEnglish2024c,
 abstract = {Extracting named entities text forms the basis for many crucial tasks such as information retrieval and extraction, machine translation, opinion mining, sentiment analysis and question answering. This paper presents a survey of the research literature on named entity linking, including named entity recognition and disambiguation. We present 200 works by focusing on 43 papers (5 surveys and 38 research works). We also describe and classify 56 resources, including 25 tools and 31 corpora. We focus on the most recent papers, where more than 95\% of the described research works are after 2015. To show the efficiency of our construction methodology and the importance of this state of the art, we compare it to other surveys presented in the research literature, which were based on different criteria (such as the domain, novelty and presented models and resources). We also present a set of open issues (including the dominance of the English language in the proposed studies and the frequent use of NER rather than the end-to-end systems proposing NED and EL) related to entity linking based on the research questions that this survey aims to answer.},
 author = {Guellil, Imane and {Garcia-Dominguez}, Antonio and Lewis, Peter R. and Hussain, Shakeel and Smith, Geoffrey},
 doi = {10.1007/s10115-023-02059-2},
 issn = {0219-3116},
 journal = {Knowledge and Information Systems},
 month = {July},
 number = {7},
 pages = {3773--3824},
 title = {Entity Linking for {{English}} and Other Languages: A Survey},
 volume = {66},
 year = {2024}
}

@inproceedings{Hamdi2021,
 abstract = {Named entity processing over historical texts is more and more being used due to the massive documents and archives being stored in digital libraries. However, due to the poor annotated resources of historical nature, information extraction performances fall behind those on contemporary texts. In this paper, we introduce the development of the NewsEye resource, a multilingual dataset for named entity recognition and linking enriched with stances towards named entities. The dataset is comprised of diachronic historical newspaper material published between 1850 and 1950 in French, German, Finnish, and Swedish. Such historical resource is essential in the context of developing and evaluating named entity processing systems. It evenly allows enhancing the performances of existing approaches on historical documents which enables adequate and efficient semantic indexing of historical documents on digital cultural heritage collections.},
 address = {New York, NY, USA},
 author = {Hamdi, Ahmed and Linhares Pontes, Elvys and Boros, Emanuela and Nguyen, Thi Tuyet Hai and Hackl, G\"{u}nter and Moreno, Jose G. and Doucet, Antoine},
 booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 doi = {10.1145/3404835.3463255},
 isbn = {9781450380379},
 keywords = {multilingual, diachronic historical newspapers, stance detection, entity linking, datasets, named entity recognition},
 location = {Virtual Event, Canada},
 numpages = {7},
 pages = {2328–2334},
 publisher = {Association for Computing Machinery},
 series = {SIGIR '21},
 title = {{A Multilingual Dataset for Named Entity Recognition, Entity Linking and Stance Detection in Historical Newspapers}},
 year = {2021}
}

@inproceedings{HIPE2020ExtOverview,
 address = {Thessaloniki, Greece},
 author = {Ehrmann, Maud and Romanello, Matteo and Fluckiger, Alex and Clematide, Simon},
 booktitle = {Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum},
 doi = {10.5281/zenodo.4117566},
 pages = {38},
 title = {Extended {{Overview}} of {{CLEF HIPE}} 2020: {{Named Entity Processing}} on {{Historical Newspapers}}},
 volume = {2696},
 year = {2020}
}

@inproceedings{HIPE2020Overview,
author = {Ehrmann, Maud and Romanello, Matteo and Fl\"{u}ckiger, Alex and Clematide, Simon},
title = {{Overview of CLEF HIPE 2020: Named Entity Recognition and Linking on Historical Newspapers}},
year = {2020},
isbn = {978-3-030-58218-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
doi = {10.1007/978-3-030-58219-7_21},
abstract = {This paper presents an overview of the first edition of HIPE (Identifying Historical People, Places and other Entities), a pioneering shared task dedicated to the evaluation of named entity processing on historical newspapers in French, German and English. Since its introduction some twenty years ago, named entity (NE) processing has become an essential component of virtually any text mining application and has undergone major changes. Recently, two main trends characterise its developments: the adoption of deep learning architectures and the consideration of textual material originating from historical and cultural heritage collections. While the former opens up new opportunities, the latter introduces new challenges with heterogeneous, historical and noisy inputs. In this context, the objective of HIPE, run as part of the CLEF 2020 conference, is threefold: strengthening the robustness of existing approaches on non-standard inputs, enabling performance comparison of NE processing on historical texts, and, in the long run, fostering efficient semantic indexing of historical documents. Tasks, corpora, and results of 13 participating teams are presented.},
booktitle = {Experimental IR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF Association, CLEF 2020, Thessaloniki, Greece, September 22–25, 2020, Proceedings},
pages = {288–310},
numpages = {23},
keywords = {Named entity recognition and classification, Entity linking, Historical texts, Information extraction, Digitized newspapers, Digital humanities},
location = {Thessaloniki, Greece}
}


@inproceedings{HIPE2022ExtOverview,
  author    = {Maud Ehrmann and Matteo Romanello and Sven Najem-Meyer and Antoine Doucet and Simon Clematide},
  title     = {Extended Overview of HIPE-2022: Named Entity Recognition and Linking in Multilingual Historical Documents},
  booktitle = {Working Notes of {CLEF} 2022 - Conference and Labs of the Evaluation Forum ({CLEF})},
  editor    = {Gulielmo Faggioli and Nicola Ferro and Alan Hanbury and Martin Potthast},
  pages     = {1038--1063},
  year      = {2022},
  address   = {Aachen},
 url = {https://doi.org/10.5281/zenodo.6979577},
 volume = {3180},
  publisher = {CEUR-WS}
}


@inproceedings{HIPE2022Overview,
 abstract = {We present the HIPE-2022 shared task on named entity processing in multilingual historical documents. Following the success of the first CLEF-HIPE-2020 evaluation lab, this edition confronts systems with the challenges of dealing with more languages, learning domainspecific entities, and adapting to diverse annotation tag sets. HIPE-2022 is part of the ongoing efforts of the natural language processing and digital humanities communities to adapt and develop appropriate technologies to efficiently retrieve and explore information from historical texts. On such material, however, named entity processing techniques face the challenges of domain heterogeneity, input noisiness, dynamics of language, and lack of resources. In this context, the main objective of the evaluation lab is to gain new insights into the transferability of named entity processing approaches across languages, time periods, document types, and annotation tag sets.},
 address = {{Cham}},
 author = {Ehrmann, Maud and Romanello, Matteo and Doucet, Antoine and Clematide, Simon},
 booktitle = {Advances in Information Retrieval},
 doi = {10.1007/978-3-030-99739-7_44},
 editor = {Hagen, Matthias and Verberne, Suzan and Macdonald, Craig and Seifert, Christin and Balog, Krisztian and N{\o}rv{\aa}g, Kjetil and Setty, Vinay},
 file = {files/231/Ehrmann et al. - 2022 - Introducing the HIPE 2022 Shared Task Named Entit.pdf},
 isbn = {978-3-030-99738-0 978-3-030-99739-7},
 keywords = {notion},
 langid = {english},
 pages = {347--354},
 publisher = {{Springer International Publishing}},
 shorttitle = {Introducing the {{HIPE}} 2022 {{Shared Task}}},
 title = {Introducing the {{HIPE}} 2022 {{Shared Task}}: {{Named Entity Recognition}} and {{Linking}} in {{Multilingual Historical Documents}}},
 urldate = {2023-02-01},
 volume = {13186},
 year = {2022}
}

@inproceedings{hoffart-etal-2011-robust,
 address = {Edinburgh, Scotland, UK.},
 author = {Hoffart, Johannes  and
Yosef, Mohamed Amir  and
Bordino, Ilaria  and
F{\"u}rstenau, Hagen  and
Pinkal, Manfred  and
Spaniol, Marc  and
Taneva, Bilyana  and
Thater, Stefan  and
Weikum, Gerhard},
 booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
 month = {July},
 pages = {782--792},
 publisher = {Association for Computational Linguistics},
 title = {Robust Disambiguation of Named Entities in Text},
 url = {https://aclanthology.org/D11-1072},
 year = {2011}
}

@inproceedings{ilievski-etal-2018-systematic,
    title = "Systematic Study of Long Tail Phenomena in Entity Linking",
    author = "Ilievski, Filip  and
      Vossen, Piek  and
      Schlobach, Stefan",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = "August",
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1056",
    pages = "664--674",
}


@book{JurafskyMartin2023,
  author    = {Dan Jurafsky and James H. Martin},
  title     = {Speech and Language Processing},
  year      = {2023},
  note      = {Draft of January 7, 2023. Available at: \url{https://web.stanford.edu/~jurafsky/slp3/}. Accessed 13 Aug 2024}
}

@inproceedings{kandpalLargeLanguageModels2023,
 abstract = {The Internet contains a wealth of knowledge-- from the birthdays of historical figures to tutorials on how to code--all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pretraining corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail},
 articleno = {641},
 author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
 booktitle = {Proceedings of the 40th International Conference on Machine Learning},
 address = {Honolulu, Hawaii, USA},
 numpages = {12},
 publisher = {JMLR.org},
 series = {ICML'23},
 title = {Large language models struggle to learn long-tail knowledge},
 year = {2023}
}

@inproceedings{kolitsas-etal-2018-end,
 abstract = {Entity Linking (EL) is an essential task for semantic text understanding and information extraction. Popular methods separately address the Mention Detection (MD) and Entity Disambiguation (ED) stages of EL, without leveraging their mutual dependency. We here propose the first neural end-to-end EL system that jointly discovers and links entities in a text document. The main idea is to consider all possible spans as potential mentions and learn contextual similarity scores over their entity candidates that are useful for both MD and ED decisions. Key components are context-aware mention embeddings, entity embeddings and a probabilistic mention - entity map, without demanding other engineered features. Empirically, we show that our end-to-end method significantly outperforms popular systems on the Gerbil platform when enough training data is available. Conversely, if testing datasets follow different annotation conventions compared to the training set (e.g. queries/ tweets vs news documents), our ED model coupled with a traditional NER system offers the best or second best EL accuracy.},
 address = {Brussels, Belgium},
 author = {Kolitsas, Nikolaos  and
Ganea, Octavian-Eugen  and
Hofmann, Thomas},
 booktitle = {Proceedings of the 22nd Conference on Computational Natural Language Learning},
 doi = {10.18653/v1/K18-1050},
 editor = {Korhonen, Anna  and
Titov, Ivan},
 month = {October},
 pages = {519--529},
 publisher = {Association for Computational Linguistics},
 title = {End-to-End Neural Entity Linking},
 year = {2018}
}

@inproceedings{KORE2012,
 abstract = {Measuring the semantic relatedness between two entities is the basis for numerous tasks in IR, NLP, and Web-based knowledge extraction. This paper focuses on disambiguating names in a Web or text document by jointly mapping all names onto semantically related entities registered in a knowledge base. To this end, we have developed a novel notion of semantic relatedness between two entities represented as sets of weighted (multi-word) keyphrases, with consideration of partially overlapping phrases. This measure improves the quality of prior link-based models, and also eliminates the need for (usually Wikipedia-centric) explicit interlinkage between entities. Thus, our method is more versatile and can cope with long-tail and newly emerging entities that have few or no links associated with them. For efficiency, we have developed approximation techniques based on min-hash sketches and locality-sensitive hashing. Our experiments on semantic relatedness and on named entity disambiguation demonstrate the superiority of our method compared to state-of-the-art baselines.},
 address = {New York, NY, USA},
 author = {Hoffart, Johannes and Seufert, Stephan and Nguyen, Dat Ba and Theobald, Martin and Weikum, Gerhard},
 booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
 doi = {10.1145/2396761.2396832},
 isbn = {9781450311564},
 keywords = {semantic relatedness, locality-sensitive hashing, entity relatedness, entity disambiguation},
 location = {Maui, Hawaii, USA},
 numpages = {10},
 pages = {545–554},
 publisher = {Association for Computing Machinery},
 series = {CIKM '12},
 title = {{KORE: keyphrase overlap relatedness for entity disambiguation}},
 year = {2012}
}

@article{Krippendorff2007,
 author = {Andrew F. Hayes and Klaus Krippendorff},
 doi = {10.1080/19312450709336664},
 journal = {Communication Methods and Measures},
 number = {1},
 pages = {77-89},
 publisher = {Routledge},
 title = {Answering the Call for a Standard Reliability Measure for Coding Data},
 volume = {1},
 year = {2007}
}

@article{linharespontes2022melhissa,
 author = {E. Linhares Pontes and L. A. Cabrera-Diego and J. G. Moreno and E. Boros and A. Hamdi and A. Doucet and N. Sid{\`e}re and M. Coustaty},
 journal = {International Journal on Digital Libraries},
 pages = {133--160},
 title = {Melhissa: a multilingual entity linking architecture for historical press articles},
 volume = {23},
 year = {2022}
}

@inproceedings{mallen-etal-2023-trust,
 abstract = {Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs{'} strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary.},
 address = {Toronto, Canada},
 author = {Mallen, Alex  and
Asai, Akari  and
Zhong, Victor  and
Das, Rajarshi  and
Khashabi, Daniel  and
Hajishirzi, Hannaneh},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.546},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 month = {July},
 pages = {9802--9822},
 publisher = {Association for Computational Linguistics},
 title = {When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories},
 year = {2023}
}

@article{Miller1995,
 abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
 address = {New York, NY, USA},
 author = {Miller, George A.},
 doi = {10.1145/219717.219748},
 issn = {0001-0782},
 issue_date = {Nov. 1995},
 journal = {Communications of the ACM},
 month = {November},
 number = {11},
 numpages = {3},
 pages = {39--41},
 publisher = {Association for Computing Machinery},
 title = {{WordNet: A Lexical Database for English}},
 url = {https://doi.org/10.1145/219717.219748},
 volume = {38},
 year = {1995}
}

@misc{Mitchell2005,
 address = {Philadelphia},
 author = {Alexis Mitchell and Stephanie Strassel and Shudong Huang and Ramez Zakhary},
 howpublished = {Web Download},
 publisher = {Linguistic Data Consortium},
 title = {ACE 2004 Multilingual Training Corpus LDC2005T09},
 year = {2005}
}

@article{mollerSurveyEnglishEntity2022,
 abstract = {Wikidata is a frequently updated, community-driven, and multilingual knowledge graph. Hence, Wikidata is an attractive basis for Entity Linking, which is evident by the recent increase in published papers. This survey focuses on four subjects: (1) Wh},
 author = {M{\"o}ller, Cedric and Lehmann, Jens and Usbeck, Ricardo},
 doi = {10.3233/SW-212865},
 file = {files/1122/Möller et al. - 2022 - Survey on English Entity Linking on Wikidata Data.pdf},
 issn = {1570-0844},
 journal = {Semantic Web},
 langid = {english},
 month = {January},
 number = {6},
 pages = {925--966},
 publisher = {{IOS Press}},
 shorttitle = {Survey on {{English Entity Linking}} on {{Wikidata}}},
 title = {Survey on {{English Entity Linking}} on {{Wikidata}}: {{Datasets}} and Approaches},
 urldate = {2023-04-21},
 volume = {13},
 year = {2022}
}

@article{nash1951non,
 author = {Nash, John},
 journal = {Annals of mathematics},
 pages = {286--295},
 publisher = {JSTOR},
 title = {Non-cooperative games},
 year = {1951}
}

@inproceedings{petroni-etal-2021-kilt,
 address = {Online},
 author = {Petroni, Fabio  and Piktus, Aleksandra  and
Fan, Angela  and Lewis, Patrick  and
Yazdani, Majid  and De Cao, Nicola  and
Thorne, James  and Jernite, Yacine  and
Karpukhin, Vladimir  and Maillard, Jean  and
Plachouras, Vassilis  and Rockt{\"a}schel, Tim  and
Riedel, Sebastian},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association 
for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.200},
 month = {June},
 pages = {2523--2544},
 publisher = {Association for Computational Linguistics},
 title = {{KILT}: a Benchmark for Knowledge Intensive Language Tasks},
 year = {2021}
}

@article{plekhanov2023multilingual,
 author = {Plekhanov, Mikhail and Kassner, Nora and Popat, Kashyap and Martin, Louis and Merello, Simone and Kozlovskii, Borislav and Dreyer, Fr{\'e}d{\'e}ric A and Cancedda, Nicola},
 journal = {arXiv preprint arXiv:2306.08896},
 title = {Multilingual end to end entity linking},
 year = {2023}
}

@inproceedings{provatorova-etal-2021-robustness,
 abstract = {Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and tweets, that propagate the prior probability bias of the entity distribution towards more frequently occurring entities. It was shown that the performance of the EL systems on such datasets is overestimated since it is possible to obtain higher accuracy scores by merely learning the prior. To provide a more adequate evaluation benchmark, we introduce the ShadowLink dataset, which includes 16K short text snippets annotated with entity mentions. We evaluate and report the performance of popular EL systems on the ShadowLink benchmark. The results show a considerable difference in accuracy between more and less common entities for all of the EL systems under evaluation, demonstrating the effect of prior probability bias and entity overshadowing.},
 address = {Online and Punta Cana, Dominican Republic},
 author = {Provatorova, Vera  and
Bhargav, Samarth  and
Vakulenko, Svitlana  and
Kanoulas, Evangelos},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.820},
 editor = {Moens, Marie-Francine  and
Huang, Xuanjing  and
Specia, Lucia  and
Yih, Scott Wen-tau},
 month = {November},
 pages = {10501--10510},
 publisher = {Association for Computational Linguistics},
 title = {{Robustness Evaluation of Entity Disambiguation Using Prior Probes: the Case of Entity Overshadowing}},
 year = {2021}
}

@inproceedings{santini2022vasari,
 author = {Cristian Santini and
Mary Ann Tan and
Oleksandra Bruns and
Tabea Tietz and
Etienne Posthumus and
Harald Sack},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/qurator/SantiniTBTPS22.bib},
 booktitle = {Proceedings of the Third Conference on Digital Curation Technologies
(Qurator 2022)},
 editor = {Adrian Paschke and
Georg Rehm and
Clemens Neudecker and
Lydia Pintscher},
 publisher = {CEUR-WS.org},
 series = {{CEUR} Workshop Proceedings},
 timestamp = {Fri, 10 Mar 2023 16:22:53 +0100},
 title = {Knowledge Extraction for Art History: the Case of Vasari's The Lives
of The Artists},
 url = {https://ceur-ws.org/Vol-3234/paper7.pdf},
 volume = {3234},
address = {Berlin, Germany},
month = {September},
 year = {2022}
}

@inproceedings{scarlini-etal-2020-contexts,
 abstract = {Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors. ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only. We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures. ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.},
 address = {Online},
 author = {Scarlini, Bianca  and
Pasini, Tommaso  and
Navigli, Roberto},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.285},
 month = {November},
 pages = {3528--3539},
 publisher = {Association for Computational Linguistics},
 title = {With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation},
 year = {2020}
}

@article{sevgiliNeuralEntityLinking2022,
 abstract = {This survey presents a comprehensive description of recent neural entity linking (EL) systems developed since 2015 as a result of the ''deep learning revolution'' in natural language processing. Its goal is to systemize design features of neural entity},
 author = {Sevgili, {\"O}zge and Shelmanov, Artem and Arkhipov, Mikhail and Panchenko, Alexander and Biemann, Chris},
 doi = {10.3233/SW-222986},
 file = {files/1120/Sevgili et al. - 2022 - Neural entity linking A&nbsp\;survey of models bas.pdf},
 issn = {1570-0844},
 journal = {Semantic Web},
 langid = {english},
 month = {January},
 number = {3},
 pages = {527--570},
 publisher = {{IOS Press}},
 shorttitle = {Neural Entity Linking},
 title = {{Neural Entity Linking: A Survey of Models Based on Deep Learning}},
 urldate = {2023-04-21},
 volume = {13},
 year = {2022}
}

@inbook{Sonar2021,
 address = {Berlin, Boston},
 author = {Sina Menzel and Hannes Schnaitter and Josefine Zinck and Vivien Petras and Clemens Neudecker and Kai Labusch and Elena Leitner and Georg Rehm},
 booktitle = {Qualität in der Inhaltserschließung},
 doi = {doi:10.1515/9783110691597-012},
 editor = {Michael Franke-Maier and Anna Kasprzik and Andreas Ledl and Hans Schürmann},
 isbn = {9783110691597},
 lastchecked = {2024-07-01},
 pages = {229--258},
 publisher = {De Gruyter Saur},
 title = {Named Entity Linking mit Wikidata und GND – Das Potenzial handkuratierter und strukturierter Datenquellen für die semantische Anreicherung von Volltexten},
 year = {2021}
}

@misc{sunHeadtoTailHowKnowledgeable2024,
 archiveprefix = {arXiv},
 author = {Sun, Kai and Xu, Yifan Ethan and Zha, Hanwen and Liu, Yue and Dong, Xin Luna},
 eprint = {2308.10168},
 keywords = {Computer Science - Computation and Language},
 langid = {english},
 month = {April},
 primaryclass = {cs},
 publisher = {arXiv},
 shorttitle = {Head-to-{{Tail}}},
 title = {Head-to-{{Tail}}: {{How Knowledgeable}} Are {{Large Language Models}} ({{LLMs}})? {{A}}.{{K}}.{{A}}. {{Will LLMs Replace Knowledge Graphs}}?},
 year = {2024}
}

@inproceedings{TAC2010,
 author = {Heng Ji and Ralph Grishman and H.T. Dang and K. Griffit and J. Ellis},
 booktitle = {Proceedings of the 2010 Text Analysis Conference},
 language = {English (US)},
 title = {Overview of the TAC 2010 knowledge base population track},
 year = {2010}
}


@article{TAYLOR1978145,
 abstract = {We consider a class of matrix games in which successful strategies are rewarded by high reproductive rates, so become more likely to participate in subsequent playings of the game. Thus, over time, the strategy mix should evolve to some type of optimal or stable state. Maynard Smith and Price (1973) have introduced the concept of ESS (evolutionarily stable strategy) to describe a stable state of the game. We attempt to model the dynamics of the game both in the continuous case, with a system of non-linear first-order differential equations, and in the discrete case, with a system of non-linear difference equations. Using this model, we look at the notions of stability and asymptotic behavior. Our notion of stable equilibrium for the continuous dynamic includes, but is somewhat more general than, the notion of ESS.},
 author = {Peter D. Taylor and Leo B. Jonker},
 doi = {https://doi.org/10.1016/0025-5564(78)90077-9},
 issn = {0025-5564},
 journal = {Mathematical Biosciences},
 number = {1},
 pages = {145-156},
 title = {Evolutionary stable strategies and game dynamics},
 url = {https://www.sciencedirect.com/science/article/pii/0025556478900779},
 volume = {40},
 year = {1978}
}

@inproceedings{tedeschi-etal-2021-named-entity,
 abstract = {Entity Linking (EL) systems have achieved impressive results on standard benchmarks mainly thanks to the contextualized representations provided by recent pretrained language models. However, such systems still require massive amounts of data {--} millions of labeled examples {--} to perform at their best, with training times that often exceed several days, especially when limited computational resources are available. In this paper, we look at how Named Entity Recognition (NER) can be exploited to narrow the gap between EL systems trained on high and low amounts of labeled data. More specifically, we show how and to what extent an EL system can benefit from NER to enhance its entity representations, improve candidate selection, select more effective negative samples and enforce hard and soft constraints on its output entities. We release our software {--} code and model checkpoints {--} at \url{https://github.com/Babelscape/ner4el}.},
 address = {Punta Cana, Dominican Republic},
 author = {Tedeschi, Simone  and
Conia, Simone  and
Cecconi, Francesco  and
Navigli, Roberto},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
 doi = {10.18653/v1/2021.findings-emnlp.220},
 editor = {Moens, Marie-Francine  and
Huang, Xuanjing  and
Specia, Lucia  and
Yih, Scott Wen-tau},
 month = {November},
 pages = {2584--2596},
 publisher = {Association for Computational Linguistics},
 title = {{N}amed {E}ntity {R}ecognition for {E}ntity {L}inking: {W}hat Works and What{'}s Next},
 year = {2021}
}

@inproceedings{tripodi-etal-2022-evaluating,
 abstract = {In this paper, we present an evaluation of sentence representation models on the paraphrase detection task. The evaluation is designed to simulate a real-world problem of plagiarism and is based on one of the most important cases of forgery in modern history: the so-called {``}Protocols of the Elders of Zion{''}. The sentence pairs for the evaluation are taken from the infamous forged text {``}Protocols of the Elders of Zion{''} (Protocols) by unknown authors; and by {``}Dialogue in Hell between Machiavelli and Montesquieu{''} by Maurice Joly. Scholars have demonstrated that the first text plagiarizes from the second, indicating all the forged parts on qualitative grounds. Following this evidence, we organized the rephrased texts and asked native speakers to quantify the level of similarity between each pair. We used this material to evaluate sentence representation models in two languages: English and French, and on three tasks: similarity correlation, paraphrase identification, and paraphrase retrieval. Our evaluation aims at encouraging the development of benchmarks based on real-world problems, as a means to prevent problems connected to AI hypes, and to use NLP technologies for social good. Through our evaluation, we are able to confirm that the infamous Protocols are actually a plagiarized text but, as we will show, we encounter several problems connected with the convoluted nature of the task, that is very different from the one reported in standard benchmarks of paraphrase detection and sentence similarity. Code and data available at https://github.com/roccotrip/protocols.},
 address = {Marseille, France},
 author = {Tripodi, Rocco  and
Blloshmi, Rexhina  and
Levis Sullam, Simon},
 booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
 month = {June},
 pages = {2928--2939},
 publisher = {European Language Resources Association},
 title = {Evaluating Multilingual Sentence Representation Models in a Real Case Scenario},
 url = {https://aclanthology.org/2022.lrec-1.314},
 year = {2022}
}

@inproceedings{tripodi-navigli-2019-game,
 abstract = {Game-theoretic models, thanks to their intrinsic ability to exploit contextual information, have shown to be particularly suited for the Word Sense Disambiguation task. They represent ambiguous words as the players of a non cooperative game and their senses as the strategies that the players can select in order to play the games. The interaction among the players is modeled with a weighted graph and the payoff as an embedding similarity function, that the players try to maximize. The impact of the word and sense embedding representations in the framework has been tested and analyzed extensively: experiments on standard benchmarks show state-of-art performances and different tests hint at the usefulness of using disambiguation to obtain contextualized word representations.},
 address = {Hong Kong, China},
 author = {Tripodi, Rocco  and
Navigli, Roberto},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1009},
 month = {November},
 pages = {88--99},
 publisher = {Association for Computational Linguistics},
 title = {Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambiguation},
 year = {2019}
}

@inproceedings{tsai-roth-2016-cross,
 address = {San Diego, California},
 author = {Tsai, Chen-Tse  and
Roth, Dan},
 booktitle = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/N16-1072},
 editor = {Knight, Kevin  and
Nenkova, Ani  and
Rambow, Owen},
 month = {June},
 pages = {589--598},
 publisher = {Association for Computational Linguistics},
 title = {Cross-lingual Wikification Using Multilingual Embeddings},
 url = {https://aclanthology.org/N16-1072},
 year = {2016}
}

@article{wang2020similaritymeasures,
 author = {Jiapeng Wang and
Yihong Dong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/information/WangD20.bib},
 doi = {10.3390/INFO11090421},
 journal = {Inf.},
 number = {9},
 pages = {421},
 timestamp = {Tue, 29 Sep 2020 10:47:58 +0200},
 title = {Measurement of Text Similarity: {A} Survey},
 url = {https://doi.org/10.3390/info11090421},
 volume = {11},
 year = {2020}
}

@inproceedings{wu-etal-2020-scalable,
 abstract = {This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.},
 address = {Online},
 author = {Wu, Ledell  and
Petroni, Fabio  and
Josifoski, Martin  and
Riedel, Sebastian  and
Zettlemoyer, Luke},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.519},
 editor = {Webber, Bonnie  and
Cohn, Trevor  and
He, Yulan  and
Liu, Yang},
 month = {November},
 pages = {6397--6407},
 publisher = {Association for Computational Linguistics},
 title = {{Scalable Zero-shot Entity Linking with Dense Entity Retrieval}},
 year = {2020}
}

@inproceedings{
zaporojets2022tempel,
title={Temp{EL}: Linking Dynamically Evolving and Newly Emerging Entities},
author={Klim Zaporojets and Lucie-Aim{\'e}e Kaffee and Johannes Deleu and Thomas Demeester and Chris Develder and Isabelle Augenstein},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=vrnqr3PG4yB}
}

@inproceedings{
zhang2022entqa,
title={Ent{QA}: Entity Linking as Question Answering},
author={Wenzheng Zhang and Wenyue Hua and Karl Stratos},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=US2rTP5nm\_}
}


@inproceedings{zhu-etal-2023-learn,
 abstract = {Entity linking models have achieved significant success via utilizing pretrained language models to capture semantic features. However, the NIL prediction problem, which aims to identify mentions without a corresponding entity in the knowledge base, has received insufficient attention. We categorize mentions linking to NIL into Missing Entity and Non-Entity Phrase, and propose an entity linking dataset NEL that focuses on the NIL prediction problem.NEL takes ambiguous entities as seeds, collects relevant mention context in the Wikipedia corpus, and ensures the presence of mentions linking to NIL by human annotation and entity masking. We conduct a series of experiments with the widely used bi-encoder and cross-encoder entity linking models, results show that both types of NIL mentions in training data have a significant influence on the accuracy of NIL prediction. Our code and dataset can be accessed at \url{https://github.com/solitaryzero/NIL_EL}.},
 address = {Toronto, Canada},
 author = {Zhu, Fangwei  and
Yu, Jifan  and
Jin, Hailong  and
Hou, Lei  and
Li, Juanzi  and
Sui, Zhifang},
 booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
 doi = {10.18653/v1/2023.findings-acl.690},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 month = {July},
 pages = {10846--10860},
 publisher = {Association for Computational Linguistics},
 title = {Learn to Not Link: Exploring {NIL} Prediction in Entity Linking},
 year = {2023}
}

@inproceedings{Orlando2024,
    title = "{R}e{L}i{K}: Retrieve and {L}in{K}, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget",
    author = "Orlando, Riccardo  and
      Huguet Cabot, Pere-Llu{\'\i}s  and
      Barba, Edoardo  and
      Navigli, Roberto",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.839",
    pages = "14114--14132",
    abstract = "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in a wide range of applications. In this paper, we propose ReLiK, a Retriever-Reader architecture for both EL and RE, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans. Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass and to fully leverage pre-trained language models contextualization capabilities, in contrast with previous Retriever-Reader-based methods, which require a forward pass for each candidate. Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed compared to competitors. Finally, we show how our architecture can be used seamlessly for Information Extraction (cIE), i.e. EL + RE, and setting a new state of the art by employing a shared Reader that simultaneously extracts entities and relations.",
}

@inproceedings{REL2020,
author = {van Hulst, Johannes M. and Hasibi, Faegheh and Dercksen, Koen and Balog, Krisztian and de Vries, Arjen P.},
title = {REL: An Entity Linker Standing on the Shoulders of Giants},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401416},
doi = {10.1145/3397271.3401416},
abstract = {Entity linking is a standard component in modern retrieval system that is often performed by third-party toolkits. Despite the plethora of open source options, it is difficult to find a single system that has a modular architecture where certain components may be replaced, does not depend on external sources, can easily be updated to newer Wikipedia versions, and, most important of all, has state-of-the-art performance. The REL system presented in this paper aims to fill that gap. Building on state-of-the-art neural components from natural language processing research, it is provided as a Python package as well as a web API. We also report on an experimental comparison against both well-established systems and the current state-of-the-art on standard entity linking benchmarks.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2197–2200},
numpages = {4},
keywords = {toolkit, entity linking, entity disambiguation, NER},
location = {Virtual Event, China},
series = {SIGIR '20}
}


@inproceedings{Christmann2022clocq,
author = {Christmann, Philipp and Saha Roy, Rishiraj and Weikum, Gerhard},
title = {Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498488},
doi = {10.1145/3488560.3498488},
abstract = {Answering complex questions over knowledge bases (KB-QA) faces huge input data with billions of facts, involving millions of entities and thousands of predicates. For efficiency, QA systems first reduce the answer search space by identifying a set of facts that is likely to contain all answers and relevant cues. The most common technique for doing this is to apply named entity disambiguation (NED) on the question, and retrieve KB facts for the disambiguated entities. This work presents CLOCQ, an efficient method that prunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses a top-k query processor over score-ordered lists of KB items that combine signals about lexical matching, relevance to the question, coherence among candidate items, and connectivity in the KB graph. Experiments with two recent QA benchmarks for complex questions demonstrate the superiority of CLOCQ over state-of-the-art baselines with respect to answer presence, size of the search space, and runtimes.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {172–180},
numpages = {9},
keywords = {question answering, knowledge bases, entity linking},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{jacovi-etal-2023-stop,
    title = "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks",
    author = "Jacovi, Alon  and
      Caciularu, Avi  and
      Goldman, Omer  and
      Goldberg, Yoav",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.308",
    doi = "10.18653/v1/2023.emnlp-main.308",
    pages = "5075--5084",
    abstract = "Data contamination has become prevalent and challenging with the rise of models pretrained on large automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to detect contamination. Strategies such as leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate without them; (3) avoid data which appears with its solution on the internet, and release the web-page context of internet-derived data along with the data. These strategies are practical and can be effective in preventing data contamination.",
}

@inproceedings{sainz-etal-2023-nlp,
    title = "{NLP} Evaluation in trouble: On the Need to Measure {LLM} Data Contamination for each Benchmark",
    author = "Sainz, Oscar  and
      Campos, Jon  and
      Garc{\'\i}a-Ferrero, Iker  and
      Etxaniz, Julen  and
      de Lacalle, Oier Lopez  and
      Agirre, Eneko",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.722",
    doi = "10.18653/v1/2023.findings-emnlp.722",
    pages = "10776--10787",
    abstract = "In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",
}

@article{Li_Flanigan_2024, title={Task Contamination: Language Models May Not Be Few-Shot Anymore}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29808}, DOI={10.1609/aaai.v38i16.29808}, abstractNote={Large language models (LLMs) offer impressive performance in various zero-shot and few-shot tasks. However, their success in zero-shot or few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined. This paper investigates how zero-shot and few-shot performance of LLMs has changed chronologically over datasets released over time, and over LLMs released over time. Utilizing GPT-3 series models and several other recent open-sourced LLMs, and controlling for dataset difficulty, we find that datasets released prior to the LLM training data creation date perform surprisingly better than datasets released post the LLM training data creation date. This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets prior to the LLMs’ training data creation date. Additionally, we utilize training data inspection, training data extraction, and a membership inference attack, which reveal further evidence of task contamination. Importantly, we find that for tasks with no possibility of task contamination, LLMs rarely demonstrate statistically significant improvements over simple majority baselines, in both zero and few-shot settings.}, number={16}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Li, Changmao and Flanigan, Jeffrey}, year={2024}, month={Mar.}, pages={18471-18480} }



@inproceedings{gu2024llmdistill,
  author       = {Yuxian Gu and
                  Li Dong and
                  Furu Wei and
                  Minlie Huang},
  title        = {MiniLLM: Knowledge Distillation of Large Language Models},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=5h0qf7IBZZ},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/Gu0WH24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

