\section{Related works}
\label{sec:related}
In this section, we examine relevant datasets and benchmarks (Section \ref{sec:related:bench}) as well as models (Section \ref{sec:related:models}) pertaining to EL. We present a detailed analysis of both general-purpose and historical document-specific benchmarks and methodologies.

\subsection{Benchmarks} \label{sec:related:bench}
\subsubsection*{General purpose}
Recent surveys indicate that historical texts are rarely utilized in datasets for training, fine-tuning, or evaluating EL models \citep{mollerSurveyEnglishEntity2022, guellilEntityLinkingEnglish2024c}. The most commonly used EL datasets for the English language include AIDA-CONLL \citep{hoffart-etal-2011-robust}, which contains Reuters news stories from 1996-1997; AQUAINT \citep{Graff2002Aquaint}, a derivative of AIDA-CONLL; MSNBC (English), which is made of online news from 2007; ACE2004 \citep{Mitchell2005}, which is composed of web-collected news articles (also in Chinese and Arabic, other than in English); CWEB \citep{Gabrilovich2013}, which consists of Wikipedia and generic web pages; WIKI \citep{10.3233/SW-170273}, which is made of Wikipedia pages; TAC \citep{TAC2010}, which contains Wikipedia pages extracted in 2008; and KORE50 \citep{KORE2012}, which is composed of web documents with a focus on highly ambiguous entities.

Several studies have targeted particularly challenging EL scenarios. The ShadowLink dataset \citep{provatorova-etal-2021-robustness} includes 16,000 short text snippets from websites in English, annotated with highly ambiguous entities. This dataset focuses on named entities with identical surface forms linked to common and rare entities, causing the former to overshadow the latter. The TempEL dataset \citep{zaporojets2022tempel} offers time-stratified English Wikipedia snapshots from 2013 to 2022, enabling the evaluation of EL models on both continuous and newly emerging entities. Additionally, \citet{benkhedda-etal-2024-enriching} concentrated on the English language Community-generated digital content (CGDC) for digital archives and developed a dataset of textual metadata linked to Wikidata information. Although this metadata includes historical named entities due to its intended use (enhancing documentation for historical digital archives), it remains a product of contemporary digital text and does not exhibit the distinctive features of historical documents.

\citet{zhu-etal-2023-learn} focuses on entity mentions that cannot be resolved against the reference KB and introduces NEL, an annotated dataset of English Wikipedia excerpts with a significant percentage of \texttt{NIL}, i.e. entities that do not exist in the KB of reference. The significant presence of \texttt{NIL} entities is particularly relevant also for HEL, as historical documents present a high occurrence of out-of-KB entities. Despite addressing \texttt{NIL} links, this work concentrated on digitally-born texts, which display different linguistic characteristics compared to digitalized documents.

\subsubsection*{Historical}
\citet{ehrmannNamedEntityRecognition2023} reviews resources for historical NER. Of particular relevance for the work described in this paper are the resources collected for the HIPE-2020 \citep{HIPE2020Overview,HIPE2020ExtOverview} and HIPE-2022 \citep{HIPE2022Overview} evaluation campaigns, as they include EL annotations.
In particular, the HIPE-2022 corpus comprises six datasets spanning English, Finnish, French, German, and Swedish. The datasets are composed by collecting historical newspapers and classic commentaries of 200 years time-span %\footnote{\url{https://github.com/hipe-eval/HIPE-2022-data}}
sourced from:
\begin{enumerate*}[label=(\roman*)]
    \item \emph{NewsEye} \citep{Hamdi2021}, which includes historical newspaper articles in French, German, Finnish, and Swedish dating back to 19C-20C;
    \item \emph{SoNAR} \citep{Sonar2021}, which includes historical newspaper articles from the Berlin State Library newspaper collections in German (19C-20C);
    \item \emph{Le Temps} \citep{ehrmann2016diachronic}, which contains historical newspaper articles from two Swiss newspapers in French (19C-20C);
    \item \emph{Living with Machines} \citep{ehrmann2016diachronic}, which contains \textit{TopRes19th}, a dataset of historical newspaper articles from the British Library newspapers in English (18C-19C), whose annotations focus on toponyms documents;
    \item \emph{AjMC}%\footnote{\url{https://mromanello.github.io/ajax-multi-commentary/}}
    , which contains annotated \textit{Classical Commentaries} (19C)
\end{enumerate*}.
We employ the HIPE-2020 English test set in our experiments, as detailed in Section \ref{sec:experiments}.

Another effort dedicated to historical documents brought to the creation of Giorgio Vasari's \textit{Lives of The Artists} \citep{santini2022vasari}, which contains NER, NEC and EL annotations towards Wikidata.
A similar initiative is followed by \citet{blouin-etal-2024-dataset-named}, which carries out NER and EL towards Wikidata annotations on a historical newspaper in the Chinese language published between 1872 and 1949 and on bilingual (Chinese-English) biographies dating back to the first half of the 20C.

Our resource enriches the field of HEL, which suffers from a scarcity of resources. It addresses the music domain, a sector not yet covered by existing HEL resources. Moreover, while contemporary EL benchmarks often emphasize English, HEL resources are not predominantly in English. For example, our dataset contains almost five times as many annotated named entities as the HIPE-2020 English test set, positioning itself as a valuable complement to existing resources for NER, NEC and EL on historical documents in English.
 
\subsection{Models} \label{sec:related:models}
\subsubsection*{General purpose}
Current research on EL models predominantly employs Pre-trained Language Models (PLM). Two primary approaches have been identified: retrieval-based and generative methods. Retrieval-based methods involve encoding mentions into a dense representation, which is then compared with a similarly dense representation calculated from the documents in a KB, such as Wikipedia. An optional re-ranking phase can merge these dense representations for improved accuracy. This approach is exemplified by tools like \citep[REL]{REL2020}, which features a modular architecture of neural components, as well as BLINK and its multilingual extension, \citep[BELA]{plekhanov2023multilingual}.

A similar approach is proposed in \citep[EntQA]{zhang2022entqa} and \citep[ReLiK]{Orlando2024}. Unlike in BLINK, the linking phase is framed as a Question Answering (QA) problem.
A plausible set of entities is first retrieved using a dense representation similar to the one from BLINK.
The difference is that the named entity is not recognized beforehand, but the model jointly recognizes mentions and possible entities that might be linked.
The correct entity is then chosen based on the context of the sentence.

\citep[CLOCQ]{Christmann2022clocq} is a system designed to extend beyond the task of linking named entities to their corresponding entries in a knowledge base, addressing the broader challenge of complex question answering over knowledge bases. One of its core functionalities includes disambiguating named entities, making it relevant to our work.

A different approach, based on generative language models instead of classification architectures, is proposed in \citep[GENRE]{decao2021autoregressive}.
This approach retrieves entities by generating their unique names, left to right, conditioned on the context.
Such an approach directly captures relations between context and entity name and reduces the memory footprint of performing queries on large search spaces.
An extension of this model, with support for multiple languages, is proposed in \citep[mGENRE]{de-cao-etal-2022-multilingual}. 
\citep[ExtEnD]{barba-etal-2022-extend} builds upon the approach used by mGENRE, but reformulates it by framing that as a text extraction task.
The set of generated candidates is concatenated to the span, thereby overcoming GENRE's limitation of being unable to see the candidate entities from which to choose.

EL models that leverage symbolic knowledge from KBs can significantly enhance the plausibility of linking entities.
As demonstrated by \citet{tedeschi-etal-2021-named-entity}, it is possible to outperform traditional methods and minimize confusion during the linking process by filtering the pool of linkable entities based on the properties of the mention.
This method involves classifying named entities mentions into fine-grained categories, ensuring that potentially linkable entities share the same type as the mention.
Consequently, the model focuses on determining the most likely entity rather than learning the plausibility of each entity implicitly.

Building on similar assumptions, \citep[ReFinED]{ayoola-etal-2022-improving} focuses on optimizing performances on long-tail entities by leveraging symbolic knowledge extrapolated from KBs (entities descriptions', types, and inter-entities relations in context). Given its focus on long-tail entities, ReFinED encompasses the management of out-of-KB entities by including the \texttt{NIL} label as a possible candidate for the linking step.

Except for ReFiNeD, SoTA entity linkers presuppose that a correct link for each mention is always present in the KB of reference.
As a result, impaired focus has been dedicated by the research community to the \texttt{NIL} prediction issue.
Different approaches have been proposed to predict whether a \texttt{NIL} prediction should be inferred from the similarity score.
This includes heuristic methods, such as a threshold on the score, or machine-learning-based methods, such as training a binary classifier \citep{sevgiliNeuralEntityLinking2022}. To address this issue, \citet{10.1145/3583780.3615036} propose BLINKout, a method that uses a dynamic \texttt{NIL} representation and classification approach, incorporating synonyms and built on BERT-based EL.
 
\subsubsection*{Historical}
The scarcity of established resources and benchmarks for HEL results in fewer solutions than conventional EL.
\citet{agarwal-etal-2018-dianed} pioneered the research in this field by proposing diaNED, a time-aware entity disambiguation approach for diachronic corpora, which leverages KB-driven temporal information and text-driven temporal expressions.

The HIPE-2022 evaluation campaign collates the most recent efforts in implementing models for NER, NEC and EL on historical documents.
The highest scoring model for the challenge on CLEF-HIPE-2022 English language data for EL was proposed by the L3i team \citep{borosKnowledgebasedContextsHistorical}, which builds upon the same multilingual model based on a BiLSTM architecture \citep{kolitsas-etal-2018-end} that the team presented when they introduced it for CLEF-HIPE-2020 \citep{boros2020robust}. %this reference is the most recent. 
Their model is enhanced with a filtering process \citep{linharespontes2022melhissa} to disambiguate historical references using the typological and temporal information in Wikidata. All the models that took part in the evaluation campaign implemented strategies to take into consideration the prediction of the \texttt{NIL} link.

In this paper, we introduce \modelCBLINK{}, which enriches the scenario of the aforementioned related works by extending BLINK.
Our model leverages time- and type-related knowledge from our chosen KB of reference, Wikidata, to refine the candidate generation process.
After filtering out the candidates that fail the plausibility criteria scoped by the time- and type-related constraints, we test various methods, including threshold-based and machine learning-based approaches, to identify \texttt{NIL} links. The original work does not cover both aspects.

Also, we propose \modelEL{}, which differs from existing approaches in HEL by framing the El problem using a game-theoretical approach. Named entity mentions, their surrounding context, and KB entities are embedded into dense vectors. The resulting vectors are used to compute pairwise similarities. By playing against each other, named entities have to collaborate with most similar entities, eventually converging to the final link decision. The model is unsupervised.