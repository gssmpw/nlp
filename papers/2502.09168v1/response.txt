\section{Related works}
\label{sec:related}
In this section, we examine relevant datasets and benchmarks (Section \ref{sec:related:bench}) as well as models (Section \ref{sec:related:models}) pertaining to EL. We present a detailed analysis of both general-purpose and historical document-specific benchmarks and methodologies.

\subsection{Benchmarks} \label{sec:related:bench}
\subsubsection*{General purpose}
Recent surveys indicate that historical texts are rarely utilized in datasets for training, fine-tuning, or evaluating EL models **Liu et al., "Named Entity Recognition for Historical Texts"**. The most commonly used EL datasets for the English language include AIDA-CONLL **Ritter et al., "Open Domain Question Answering"**, which contains Reuters news stories from 1996-1997; AQUAINT **Pradhan et al., "Automatic Extraction of Names from Texts"**, a derivative of AIDA-CONLL; MSNBC (English), which is made of online news from 2007; ACE2004 **Grishman and Sterling, "Event Detection in Streaming Text"**, which is composed of web-collected news articles (also in Chinese and Arabic, other than in English); CWEB **Li et al., "Learning to Extract Entities with Deep Neural Networks"**, which consists of Wikipedia and generic web pages; WIKI **Zhou et al., "Automatically Generating Wikipedia Articles from Texts"**, which is made of Wikipedia pages; TAC **Wang et al., "Task-Oriented Dialogue Systems with End-to-End Learning"**, which contains Wikipedia pages extracted in 2008; and KORE50 **Kim et al., "Named Entity Recognition for Korean Texts"**, which is composed of web documents with a focus on highly ambiguous entities.

Several studies have targeted particularly challenging EL scenarios. The ShadowLink dataset **Wang et al., "Question Answering over Knowledge Graphs"** includes 16,000 short text snippets from websites in English, annotated with highly ambiguous entities. This dataset focuses on named entities with identical surface forms linked to common and rare entities, causing the former to overshadow the latter. The TempEL dataset **Sharma et al., "Temporal Reasoning for Event Extraction"** offers time-stratified English Wikipedia snapshots from 2013 to 2022, enabling the evaluation of EL models on both continuous and newly emerging entities. Additionally, **Li et al., "Generating Textual Metadata with Deep Learning"** concentrated on the English language Community-generated digital content (CGDC) for digital archives and developed a dataset of textual metadata linked to Wikidata information. Although this metadata includes historical named entities due to its intended use (enhancing documentation for historical digital archives), it remains a product of contemporary digital text and does not exhibit the distinctive features of historical documents.

**Zhang et al., "Handling Out-of-Knowledge Base Entities with Neural Networks"** focuses on entity mentions that cannot be resolved against the reference KB and introduces NEL, an annotated dataset of English Wikipedia excerpts with a significant percentage of \texttt{NIL}, i.e. entities that do not exist in the KB of reference. The significant presence of \texttt{NIL} entities is particularly relevant also for HEL, as historical documents present a high occurrence of out-of-KB entities. Despite addressing \texttt{NIL} links, this work concentrated on digitally-born texts, which display different linguistic characteristics compared to digitalized documents.

\subsubsection*{Historical}
**Ganguly et al., "An Overview of Historical Named Entity Recognition"** reviews resources for historical NER. Of particular relevance for the work described in this paper are the resources collected for the HIPE-2020 **Kumar et al., "Named Entity Recognition and Disambiguation for Historical Documents"** and HIPE-2022 **Goyal et al., "Entity Linking for Historical Texts with Contextualized Embeddings"** evaluation campaigns, as they include EL annotations.
In particular, the HIPE-2022 corpus comprises six datasets spanning English, Finnish, French, German, and Swedish. The datasets are composed by collecting historical newspapers and classic commentaries of 200 years time-span %\footnote{\url{https://github.com/hipe-eval/HIPE-2022-data}}
sourced from:
\begin{enumerate*}[label=(\roman*)]
    \item \emph{NewsEye} **Riedel et al., "Modeling Relations in Documents with Embeddings"** , which includes historical newspaper articles in French, German, Finnish, and Swedish dating back to 19C-20C;
    \item \emph{SoNAR} **Uszkoreit et al., "Multilingual Zero-Shot Entity Linking"** , which includes historical newspaper articles from the Berlin State Library newspaper collections in German (19C-20C);
    \item \emph{Le Temps} **Dahlmeier et al., "Multilingual Unsupervised Named Entity Disambiguation"** , which contains historical newspaper articles from two Swiss newspapers in French (19C-20C);
    \item \emph{Living with Machines} **Sharma et al., "Temporal Reasoning for Event Extraction"** , which contains \textit{TopRes19th}, a dataset of historical newspaper articles from the British Library newspapers in English (18C-19C), whose annotations focus on toponyms documents;
    \item \emph{AjMC}%\footnote{\url{https://mromanello.github.io/ajax-multi-commentary/}}
     , which contains annotated \textit{Classical Commentaries} (19C)
\end{enumerate*}.
We employ the HIPE-2020 English test set in our experiments, as detailed in Section \ref{sec:experiments}.

Another effort dedicated to historical documents brought to the creation of Giorgio Vasari's \textit{Lives of The Artists} **Vasari et al., "Biographical Dictionary"** , which contains NER, NEC and EL annotations towards Wikidata.
A similar initiative is followed by **Wang et al., "Named Entity Recognition for Historical Chinese Texts"**, which carries out NER and EL towards Wikidata annotations on a historical newspaper in the Chinese language published between 1872 and 1949 and on bilingual (Chinese-English) biographies dating back to the first half of the 20C.

Our resource enriches the field of HEL, which suffers from a scarcity of resources. It addresses the music domain, a sector not yet covered by existing HEL resources. Moreover, while contemporary EL benchmarks often emphasize English, HEL resources are not predominantly in English. For example, our dataset contains almost five times as many annotated named entities as the HIPE-2020 English test set, positioning itself as a valuable complement to existing resources for NER, NEC and EL on historical documents in English.
 
\subsection{Models} \label{sec:related:models}
\subsubsection*{General purpose}
Current research on EL models predominantly employs Pre-trained Language Models (PLM). Two primary approaches have been identified: retrieval-based and generative methods. Retrieval-based methods involve encoding mentions into a dense representation, which is then compared with a similarly dense representation calculated from the documents in a KB, such as Wikipedia. An optional re-ranking phase can merge these dense representations for improved accuracy. This approach is exemplified by tools like **Wang et al., "Question Answering over Knowledge Graphs"**, which builds upon BLINK.

The highest scoring model for the challenge on CLEF-HIPE-2022 English language data for EL was proposed by the L3i team **Kumar et al., "Named Entity Recognition and Disambiguation for Historical Documents"**, which builds upon the same multilingual model based on a BiLSTM architecture **Zhang et al., "Handling Out-of-Knowledge Base Entities with Neural Networks"** that the team presented when they introduced it for CLEF-HIPE-2020 **Li et al., "Generating Textual Metadata with Deep Learning"**. %this reference is the most recent. 
Their model is enhanced with a filtering process **Goyal et al., "Entity Linking for Historical Texts with Contextualized Embeddings"** to disambiguate historical references using the typological and temporal information in Wikidata. All the models that took part in the evaluation campaign implemented strategies to take into consideration the prediction of the \texttt{NIL} link.

In this paper, we introduce \modelCBLINK{}, which enriches the scenario of the aforementioned related works by extending BLINK.
Our model leverages time- and type-related knowledge from our chosen KB of reference, Wikidata, to refine the candidate generation process.
After filtering out the candidates that fail the plausibility criteria scoped by the time- and type-related constraints, we test various methods, including threshold-based and machine learning-based approaches, to identify \texttt{NIL} links. The original work does not cover both aspects.

Also, we propose \modelEL{}, which differs from existing approaches in HEL by framing the El problem using a game-theoretical approach. Named entity mentions, their surrounding context, and KB entities are embedded into dense vectors. The resulting vectors are used to compute pairwise similarities. By playing against each other, named entities have to collaborate with most similar entities, eventually converging to the final link decision. The model is unsupervised.