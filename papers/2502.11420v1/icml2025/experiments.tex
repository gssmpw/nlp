\section{Experiments}
\label{sec:exp}



\begin{table*}[ht]
    \centering
    % \scriptsize
    \resizebox{0.96\textwidth}{!}{
    \begin{tabular}{ll|cccccc}
    \toprule
     \multirow{2}{*}{Base catogory} & \multirow{2}{*}{Method}   & \multicolumn{2}{c}{Pitch histogram} & \multicolumn{2}{c}{Note density} & \multicolumn{2}{c}{Chord progression}
     \\  & & Loss $\downarrow$  & OA $\uparrow$  & Loss $\downarrow$  & OA $\uparrow$ & Loss $\downarrow$  & OA $\uparrow$ \\
      \midrule
     Reference & No Guidance & $0.0180 \pm 0.0100$ & $0.842 \pm  0.012$ & $2.486 \pm 3.530$ & $0.830 \pm 0.016$ & $ 0.831 \pm 0.142$ & \underline{$ 0.854 \pm 0.026$} \\ 
     Training-based & Classifier & $0.0050 \pm 0.0040$ &  $0.855 \pm 0.020$ & $0.698 \pm 0.587$ & $\mathbf{0.861 \pm 0.025}$ & $0.723 \pm 0.200$ &$0.850 \pm 0.033$ \\
     \midrule
     \multirow{3}{*}{Training-free} &
      DPS & \underline{$0.0010 \pm 0.0020$} &  $0.849 \pm 0.018$ &  $1.261 \pm 2.340$ &$0.667 \pm 0.113$ &  $0.414 \pm 0.256$  &$0.839 \pm 0.039$  \\
           & SCG & $0.0036 \pm 0.0057$ & $\mathbf{0.862 \pm 0.008}$ &  $\mathbf{0.134 \pm 0.533 }$  & \underline{$0.842 \pm 0.022$} & \underline{$0.347 \pm 0.212$} & $0.850 \pm 0.046$ \\
    &\textbf{\xcleansampling} & $\mathbf{0.0002 \pm 0.0003}$ & \underline{$0.860 \pm 0.016$} & \underline{$0.142 \pm 0.423 $} & $0.832 \pm 0.023$ & $\mathbf{0.301 \pm 0.191}$& $ \mathbf{0.856 \pm 0.032}$ \\
    \bottomrule
    \end{tabular}}
    \caption{
    % Evaluation of guidance methods for music generation. The average loss reduction of \xcleansampling is $29.01\%$.The results for no guidance, Classifier and DPS are copied from \citet{huang2024symbolic}. For chord progression, we use fewer inference steps with guidance due to computational constraints, as detailed in \cref{app:implement details of music}.  The best is marked in \textbf{bold}, while the second best is \underline{underlined}.
    % Evaluation of guidance methods for music generation. \xcleansampling achieves an average loss reduction of $29.01\%$. Results for no guidance, Classifier, and DPS are from \citet{huang2024symbolic}. For chord progression, fewer inference steps are used with guidance due to computational constraints (see \cref{app:implement details of music}). The best results are in bold, and the second best are \underline{underlined}.
    Evaluation of guidance methods for music generation. \xcleansampling reduces loss by average $29.01 \%$. Results for no guidance, Classifier, and DPS are copied from \citet{huang2024symbolic}. Due to computational limits, chord progression uses fewer inference steps with guidance (see \cref{app:exp detail music}). The best results are \textbf{bold}, second best are \underline{underlined}.
    }
    \label{tab:music guidance}
\end{table*}

\begin{table*}[ht]
    \centering
    % \scriptsize
    \setlength{\tabcolsep}{1pt} % Default is 6pt
    \resizebox{0.85\textwidth}{!}{
    \begin{tabular}{lc|ccccccccccccccccc}
    \toprule
       \multirow{2}{*}{Base category} & \multirow{2}{*}{Method}  & &
       & \multicolumn{2}{c}{$N_r^*=1$}  
       & \multicolumn{2}{c}{$N_r^*=5$} 
       & \multicolumn{2}{c}{$\mathrm{LogP}^*=-2$}  
       & \multicolumn{2}{c}{$\mathrm{LogP}^*=10$}  
         \\
        & & & & MAE $\downarrow$ & Validity $\uparrow$ 
        & MAE $\downarrow$ & Validity $\uparrow$ & MAE $\downarrow$ & Validity $\uparrow$ & MAE $\downarrow$ & Validity $\uparrow$\\
         \midrule
         Reference & No Guidance & &
         & $2.09\pm1.16$ & \rule{0.4cm}{0.2mm} % $12.20\%$
         & $2.03\pm1.16$ & \rule{0.4cm}{0.2mm} % $12.20\%$
        & $5.28\pm1.59$ & \rule{0.4cm}{0.2mm} % $12.20\%$
        & $6.72\pm1.60$ & \rule{0.4cm}{0.2mm} % $12.20\%$
         \\ 
         \midrule
         Training-based & DG & &
         & $0.13\pm0.35$ & $9.73\%$ 
         & $0.20\pm0.41$ & $5.06\%$ 
         & $0.86\pm0.65$ & $9.29\%$ 
         & $1.66\pm1.12$ & $5.27\%$
         \\
         \midrule 
        \multirow{2}{*}{Training-free} & 
        % \multirow{4}{*}{Training-free} & 
         TFG-Flow & &
         & $0.28\pm0.65$ & $\textbf{13.61}\%$ 
         & $0.50\pm0.64$ & $5.70\%$ 
         & $1.86\pm1.65$ & $8.26\%$ 
         & $2.54\pm1.92$ & $\textbf{10.58}\%$ 
         \\
         \cmidrule{2-12}
     &   \textbf{\xtsampling} & &
     & $\textbf{0.03}\pm\textbf{0.26}$ & $13.41\%$ 
     & $\textbf{0.12}\pm\textbf{0.53}$ & $\textbf{6.83}\%$ 
     & $\textbf{0.68}\pm\textbf{0.60}$ & $\textbf{10.27}\%$ 
     & $\textbf{1.65}\pm\textbf{1.83}$ & $10.31\%$        
     \\
         \bottomrule
    \end{tabular}}
    \caption{Evaluation on Small Molecule Generation (Targets: Number of Rings $N_r$ and Lipophilicity $\mathrm{LogP}$). The best is marked in \textbf{bold}. The validity of unguided generated sequences is $12.20\%$. The branch-out size for \xtsampling is $K=2$.}
    \label{tab:smg}
\end{table*}



\begin{table*}[ht]
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{lcc|cccccc}
    \toprule
    \multirow{2}{*}{Base category} %& \multicolumn{2}{c}{Method $\,\,\,\,$ {\small (strength)}}  
        & \multicolumn{2}{c|}{\multirow{2}{*}{Method  { \small (strength $\gamma$)}} }& \multicolumn{2}{c}{Class 1} & \multicolumn{2}{c}{Class 2} & \multicolumn{2}{c}{Class 3}\\
   && & Prob  $\uparrow$ & FBD  $\downarrow$ & Prob  $\uparrow$ & FBD  $\downarrow$  & Prob  $\uparrow$ & FBD  $\downarrow$ \\
   \midrule
   Reference & No Guidance & \rule{0.4cm}{0.2mm} & $0.007 \pm 0.053$ & $910$ & $0.008 \pm 0.052$ & $602$ & $0.021 \pm 0.079$ & $242$ \\
         \midrule
         \multirow{3}{*}{Training-based} &   \multirow{3}{*}{DG} &  {\footnotesize $20$} &  $0.693 \pm 0.264$ & $\mathbf{102}$    & $ 0.627 \pm 0.340$ & $\mathbf{120}$ & $0.359 \pm 0.188$ & {$\mathbf{109}$}  \\
         & & {\footnotesize $100$} & $0.173 \pm 0.256$ & $347$ & $0.571 \pm 0.356$ & \underline{$212$} & $0.372 \pm 0.237$ & $116$ \\
         & & {\footnotesize $200$} & $0.064 \pm 0.143$ & $514$  & $0.350 \pm 0.351$ & $294$ & $0.251 \pm 0.171$ & $157$ \\
         \midrule          
         \multirow{4}{*}{Training-free} &   \multirow{1}{*}{TFG-Flow} &  {\footnotesize $200$} & $0.004 \pm 0.029$ & $617$ & $0.012 \pm 0.076$ & $352$ & $0.054 \pm 0.129$ & $151$  \\ \cmidrule{2-9}
         &  \multirow{3}{*}{\textbf{\xtgrad}} &  {\footnotesize $ 20$} &    $0.247 \pm 0.280$ & $314$ &  $0.313 \pm 0.343$ & $279$ & $0.236 \pm 0.178$ & \underline{$111$}  \\
         &  & {\footnotesize $ 100$} & \underline{$0.745 \pm 0.217$} & \underline{$125$} & \underline{$0.915 \pm 0.154$} & $318$ & \underline{$0.509 \pm 0.242$} & $189$  \\
         & & {\footnotesize $200$} & $\mathbf{0.894 \pm 0.136}$ & $213$  & $\mathbf{0.951 \pm 0.110}$ & $337$ & $\mathbf{0.560 \pm 0.258}$ & $179$  \\  
        % & sample $x_t$  & $\,\,\,\,$ \rule{0.4cm}{0.2mm}& $0.129$ & $384$ & $0.135$ & $228$ & $0.192$ & $\mathbf{91}$\\
        % & sample $x_1$ & $\,\,\,\,$ \rule{0.4cm}{0.2mm} & $0.002$ & $670$ & $0.013$ & $380$ & $0.043$ & $151$  \\         
         \bottomrule
    \end{tabular}
    }
    \caption{Evaluation of guidance methods for enhancer DNA design at varying guidance strength levels $\gamma_t = \gamma$ in Module~\cref{mol:grad guidance}. The best is marked in \textbf{bold}, while the second best is \underline{underlined}. The $x_t$ gradient method consistently achieves significantly higher target class probabilities, outperforming the training-free baseline method.}
    \label{tab:enhancer guidance res}
\end{table*}



This section evaluates the performance of \ouralg through experiments on one continuous and two discrete models across six tasks. It is structured as follows: \cref{sec: exp setting} introduces the comparison methods; \cref{sec: exp guidance results} details the tasks and results; \cref{sec: exp scalabiliity} validates framework scalability and tree search effectiveness; and \cref{sec: exp discuss guidance design} discusses design choices for different scenarios.



\subsection{Settings}\label{sec: exp setting}



Below are the methods we would like to compare:

\noindent \textbf{For continuous models:}  \textbf{DPS \citep{chung2022diffusion}}, a training-free classifier guidance method that relies on gradient computation and requires surrogate neural network predictors for non-differentiable objective functions; \textbf{SCG \citep{huang2024symbolic}}, a gradient-free method and a special case of \xtsampling with $A=1$; and \textbf{\xcleansampling (\cref{subsec:x1sampling})}.



\noindent \textbf{For discrete models:} \textbf{DG \citep{nisonoff2024unlocking}}, a training-based classifier guidance requiring a predictor trained on noisy inputs, implemented with Taylor expansion and gradients; \textbf{TFG-Flow \cite{lin2025tfgflow}}, a training-free method estimating the conditional rate matrix; \textbf{\xtgrad (\cref{subsec:xtgrad})}, which trains a predictor on clean data for non-differentiable objectives; and \textbf{\xtsampling (\cref{subsec:xtsampling})} and \textbf{\xcleansampling (\cref{subsec:x1sampling})}.



For comparison with the above guidance methods in \cref{sec: exp guidance results}, the active size of \ouralg is set to $A=1$. The results of scaling active set size $A$ and branch-out size $K$ are presented in \cref{sec: exp scalabiliity}. 
















\subsection{Guided Generation} \label{sec: exp guidance results}










\subsubsection{Symbolic music generation} 
We follow the setup of \citet{huang2024symbolic}, using a continuous diffusion model pre-trained on several piano midi datasets, detailed in \cref{app:exp detail music}. The branch-out size for SCG and \xcleansampling is $K=16$.
\vspace{-5pt}
\paragraph{Guidance Target.} Our study focuses on three types of targets: pitch histogram, note density, and chord progression. The objective function is $f_{y}(\cdot) = - \ell \rbr{y, \texttt{Rule}(\cdot)}$, where $\ell$ is the loss function. Notably, the rule function $\texttt{Rule}(\cdot)$ is \textit{non-differentiable} for note density and chord progression.

\paragraph{Evaluation Metrics.} For each task, we evaluate performance on 200 targets as formulated by \citet{huang2024symbolic}. Two metrics are used: (1) Loss, which measures how well the generated samples adhere to the target rules. (2) Average Overlapping Area (OA), which assesses music quality by comparing the similarity between the distributions of the generated and ground-truth music, focusing on matching target attributes \cite{yang2020evaluation}.



\paragraph{Results.} As shown in \cref{tab:music guidance}, our proposed \xcleansampling method demonstrates superior performance while maintaining comparable sample quality.  For differentiable rules (pitch histogram), \xcleansampling outperforms DPS, which SCG, another gradient-free approach, cannot achieve. For non-differentiable rules such as note density and chord progression, \xcleansampling matches or exceeds SCG and significantly outperforms DPS.


\subsubsection{Small molecule generation} 

We validate our methods on the generation of small molecules with discrete flow models. Following \citet{nisonoff2024unlocking}, the small molecules are represented as simplified molecular-input line-entry system (SMILES) strings. These \textit{discrete} sequences are padded to 100 tokens and there are 32 possible token types including one pad and one mask token $M$. We adopt the same unconditional flow model and Euler sampling curriculum as \citet{nisonoff2024unlocking}.

\textbf{Guidance Target.} Achieving expected chemical properties, i.e., number of rings $N_r$ or lipophilicity $\mathrm{LogP}$, is the goal of guided generation. These two properties of SMILES sequences could be evaluated through an open-source analysis tool RDKit \citep{landrum2013rdkit}. We choose $N_r^*=[0,6]$ and $\mathrm{LogP}^*=[-2,0,2,4,6,8,10]$ as target values.
The predictor used for guidance would be
$f_y(\boldsymbol{x})= -\frac{(y-f(\boldsymbol{x}))^2}{2\sigma^2}$.

\textbf{Evaluation Metrics.} We evaluate the mean absolute error (\textbf{MAE}) between the target values and the properties of 1000 generated \textit{valid} unique sequences, which are measured by RDKit. Besides, we take the \textbf{validity} of generated sequences, i.e., the ratio of valid unique sequences within all generated sequences,  as a metric to compare different methods under the same target value. 

\textbf{Results.} Compared with the training-based classifier guidance method DG, our sampling method \xtsampling consistently achieves lower MAE and higher validity for both two properties as shown in  \cref{tab:smg}. In addition, \xtsampling outperforms TFG-Flow, which is also a training-free guidance method, by a large margin in terms of MAE, while maintaining comparable or better validity. 
On average, \xtsampling achieves a relative performance improvement of $39.44\%$ on $N_r$ and $19.45\%$ on $\mathrm{LogP}$ compared to the best baseline DG. Please refer to \cref{tab:smg_rel} for details.






\subsubsection{Enhancer DNA design}
We follow the experimental setup of \citet{stark2024dirichlet}, using a discrete flow model pre-trained on DNA sequences of length 500, each labeled with one of 81 cell types \cite{janssens2022decoding, taskiran2024cell}. For inference, we apply 100 Euler sampling steps. The branch-out size for gradient guidance \xtgrad is $K=1$.
% \vspace{-9pt}

\paragraph{Guidance Target.} The goal is to generate enhancer DNA sequences that belong to a specific target cell type. The guidance target predictor is provided by an oracle classifier $f$ from \citet{stark2024dirichlet}. The objective function of given cell class $y$ is $f_y(\cdot) = \log  f(y\mid \cdot)$. 


\vspace{-5pt}
\paragraph{Evaluation Metrics.} We generate 1000 DNA sequences given the cell type. Performance is evaluated using two metrics. The first is Target Class Probability, provided by the oracle classifier, where higher probabilities indicate better guidance. The second metric, Frechet Biological Distance (FBD), measures the distributional similarity between the generated samples and the training data for a specific target class (i.e., class-conditioned FBD), with lower values indicating better alignment.





\paragraph{Results.} \Cref{tab:enhancer guidance res} shows class-conditioned enhancer DNA generation results. 
Our  \xtgrad consistently achieves the highest target probabilities as $\gamma$ increases compared to the training-based baseline DG. 
% Our  \xtgrad consistently achieves the highest target probabilities as $\gamma$ increases, with an average improvement of $18.43\%$ compared to DG over eight test classes \citep{nisonoff2024unlocking}. 
The training-free baseline, TFG-Flow, shows almost no guidance effect with increased strength or Monte Carlo sampling. See \cref{app:additional res} for details.


    







\subsection{Scalability of \ouralg} \label{sec: exp scalabiliity}




\begin{figure*}
    \centering
    \subfigure[Note Density (Music)]{\includegraphics[width=0.235\linewidth]{icml2025/figures/music/nd/scaling_all_nd.pdf}}
    \hspace{0.005\linewidth}
    \subfigure[Pitch Histogram (Music)]{\includegraphics[width=0.235\linewidth]{icml2025/figures/music/ph/scaling_all_ph.pdf}}
    \hspace{0.005\linewidth}
    \subfigure[Molecular Design ($N_r^*=5$)]{\includegraphics[width=0.235\linewidth]{icml2025/figures/molecule/ylog_TreeG-SC_TreeG-SD_num_rings_5.pdf}}
    \hspace{0.005\linewidth}
    \subfigure[Enhancer DNA (Class 4)]{\includegraphics[width=0.235\linewidth]{icml2025/figures/enhancer/scaling_all/class_4/temp_0.05.pdf}}
    \caption{\textbf{Inference Time Scaling Behavior:} As the active set size and branch-out size increase, the optimization effect of the objective function scales with inference time. This trend is consistently observed across all algorithms and tasks. The inference time is measured with a batch size of 1 for music and 100 for molecule and DNA design. For DNA design, $\gamma=20$ for \xtgrad.}
    \label{fig:scaling law}
\end{figure*}













Our \ouralg is scalable to the active size $A$ (i.e., the number of generation paths) and the branch-out size $K$. It is compatible with all guidance methods. This section demonstrates that increasing $A$ and $K$ consistently enhances the objective function's value.


\begin{figure}[ht]
% \vspace{-5pt}
    \centering
    \includegraphics[width=1.0\linewidth]{icml2025/figures/enhancer/trade_off.pdf}
     \caption{ Trade-off between Active Set Size $A$ and Branch-out Size $K$ with Fixed Compute. \xtsampling and \xtgrad vary $(A, K)$ with fixed total compute $A*K$ for enhancer DNA design.  Performance peaks when $A$ and $K$ are in the moderate range. The results are for Class 4 and $\gamma=20$ for \xtgrad. }
    \label{fig:trade-off A K}
    % \vspace{-28pt}
\end{figure}




% \vspace{-8pt}



\paragraph{Scalability on Inference-time Computation.}
% We define \( A * K \) as the computation cost, as it dominates the computational complexity in \cref{tab:computation complexity} (also for \xcleansampling in our experiments, more time results are in \cref{app:scaling music sec}). To analyze scalability, we vary \( A \times K \) for \( 2^i \) and test all combinations of \( (A, K) = (2^i, 2^j) \). 
When increasing the active set size and branch-out size, the computational cost of inference rises. We investigate the performance frontier to optimize the objective function concerning inference time. The results reveal an inference-time scaling law, as illustrated in \cref{fig:scaling law}. Our findings indicate consistent scalability across all algorithms and tasks, with \cref{fig:scaling law} showcasing four examples. Additional results refer to \cref{app:additional res}.



% \vspace{-8pt}

\paragraph{Trade-off between Active Set Size $A$ and  Branch-out Size $K$.}  
\cref{tab:computation complexity} shows the computational complexity of algorithms using \texttt{BranchOut}-Current, specifically \xtsampling and \xtgrad, is $O(AK)$. Given a fixed product $A*K$ (i.e., fixed inference computation, detailed timing in \cref{app:scalability of DNA}), we explore the optimal balance between $A$ and $K$. \cref{fig:trade-off A K} demonstrates that performance is highest when $A$ and $K$ are within a moderate range. Notably, in the special case where $K = 1$, there is no branch-out or selection operation during inference, making it equivalent to the Best-of-N approach, which typically results in suboptimal performance.





\subsection{Discussion on Design Axes} \label{sec: exp discuss guidance design}
Based on the experiment results with $A=1$, we can compare the designs within \ouralg along two axes. \textbf{Gradient-free vs gradient-based guidance:} \xtgrad is only effective when an accurate objective predictor exists. If so, then the choice is influenced by the latency in forward pass of the predictor â€” faster predictors benefit \xtsampling and \xcleansampling, while slower ones make \xtgrad more practical. \textbf{\xtsampling (current state) vs \xcleansampling (destination state)}: results show \xtsampling performs better than \xcleansampling for discrete flow; and vice versa for continuous diffusion. Please refer to \cref{app: exp discuss guidance design} for a more detailed discussion.