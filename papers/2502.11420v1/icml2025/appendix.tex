




\section{Additional Related Work}\label{app:related work}
We provide more related work in this section.

\paragraph{Discrete diffusion and flow model.} \citet{austin2021structured} and \citet{hoogeboom2021argmax} pioneered diffusion in discrete spaces by introducing a corruption process for categorical data. \citet{campbell2022continuous} extended discrete diffusion models to continuous time, while \citet{loudiscrete} proposed learning probability ratios. Discrete Flow Matching \citet{campbell2024generative,gat2024discrete} further advances this field by developing a Flow Matching algorithm for time-continuous Markov processes on discrete state spaces, commonly known as Continuous-Time Markov Chains (CTMCs). \citet{lipman2024flow} presents a unified perspective on flow and diffusion.


\paragraph{Diffusion model alignment.} To align a pre-trained diffusion toward user-interested properties, fine-tuning the model to optimize a downstream objective function \citep{black2023training,uehara2024fine,prabhudesai2023aligning} is a common training-based approach. In addition to guidance methods, an alternative training-free approach involves optimizing the initial value of the reverse process \citep{wallace2023end, ben2024d,karunratanakul2024optimizing}. These methods typically use an ODE solver to backpropagate the objective gradient directly to the initial latent state, making them a gradient-based version of the Best-of-N strategy.




\section{Proof of \cref{lemma:transition prob}}\label{prof:transition prob}

\begin{proof}

For continuous cases, in the forward process, $\boldsymbol{x}_t \sim \mathcal{N}(\bar{\alpha}_t \boldsymbol{x}_1, (1-\bar{\alpha}_t)\boldsymbol{I}), \boldsymbol{x}_{t+\Delta t} \sim \mathcal{N}(\bar{\alpha}_{t+\Delta t} \boldsymbol{x}_1, (1-\bar{\alpha}_{t+\Delta t})\boldsymbol{I})$, the posterior distribution is:
\begin{equation}
    p\rbr{\boldsymbol{x}_{t+\Delta t} \mid \boldsymbol{x}_{t}, \boldsymbol{x}_1} = \mathcal{N}\rbr{c_{t,1}\boldsymbol{x}_{t}+c_{t,2} \boldsymbol{x}_1, \beta_t \boldsymbol{I}} , 
\end{equation}

where $\beta_t =\frac{1-\bar{\alpha}_{t+\Delta t}}{1-\bar{\alpha}_{t}}(1-\alpha_t) $, and we recall $c_{t,1}= \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t+\Delta t})}{1-\bar{\alpha}_t},  c_{t,2} = \frac{\sqrt{\bar{\alpha}_{t+\Delta t}}(1-\alpha_t)}{1-\bar{\alpha}_t}$.

We set $\mathcal{T^\ast}\rbr{\boldsymbol{x}_{t+\Delta t} \mid \boldsymbol{x}_{t}, \boldsymbol{x}_1} = \mathcal{N}\rbr{c_{t,1}\boldsymbol{x}_{t}+c_{t,2} \boldsymbol{x}_1, \beta_t \boldsymbol{I}}$ and the expectation $\hat{\boldsymbol{x}}$ taking over $q_1 =  N\rbr{u_\theta(\boldsymbol{x}_t, t), ( 1-\alpha_t) \boldsymbol{I}}$. It holds
\begin{equation}\label{eq:app posterior}
    \EE_{{\hat{\boldsymbol{x}}}_1 \sim q_1} \sbr{\mathcal{T}^\ast(\boldsymbol{x}_{t + \Delta t} \mid \boldsymbol{x}_{t}, \hat{{\boldsymbol{x}}}_1)} = \mathcal{N}\rbr{c_{t,1}\boldsymbol{x}_{t}+c_{t,2} \hat{\boldsymbol{x}}_1, \sigma_t \boldsymbol{I}},
\end{equation}
where we recall $\sigma_t = 1 -\alpha_t$. Therefore, \eqref{eq:app posterior} is exactly the distribution $\mathcal{T}(\boldsymbol{x}_{t + \Delta t} \mid \boldsymbol{x}_{t})  $
 for sampling during inference.


For discrete cases,  $\mathcal{T}$ represents the rate matrix for inference sampling. We recall \eqref{eq:compute rate matrix} the rate matrix for sampling during inference:
\begin{equation*}
     R_{\theta,t}^{(d)} \rbr{\boldsymbol{x}_t, j} = \EE_{{x}_1^{(d)} \sim u_{\theta}^{(d)}\rbr{{x}_1\mid \boldsymbol{x}_t}}\sbr{R_t\rbr{x_t^{(d)}, j|x_1^{(d)}}},
\end{equation*}
with the pre-defined conditional rate matrix $R_t(x_t,j|x_1) = \frac{\delta \cbr{j,x_1}}{1-t} \cbr{x_t, M}$. Based on this, we have $\mathcal{T}^{\ast, (d)}(j\mid \boldsymbol{x}_t) = R_{\theta,t}^{(d)} \rbr{\boldsymbol{x}_t, j} $, and $\mathcal{T}^{(d)}(j\mid \boldsymbol{x}_t, \boldsymbol{x}_1) = R_t\rbr{x_t^{(d)}, j|x_1^{(d)}}= \frac{\delta \cbr{j,x_1}}{1-t} \cbr{x_t, M}   $ is independent with the flow model $u_\theta$. Thus, they satisfy all requirements. We complete the proof.



    
\end{proof}



\section{Discussion on Design Axes} \label{app: exp discuss guidance design}
We compare the guidance designs: \xtsampling, \xcleansampling and \xtgrad based on experimental results, to separate the effect of guidance design from the effect of tree search, we set $A=1$. A side-to-side comparison on the performance of the three methods are provided in \cref{tab:discrete internal comparison}.








\paragraph{Gradient-based v.s. Gradient-free: depends on the predictor} The choice between gradient-based and gradient-free methods largely depends on the characteristics of the predictor. 

The first step is determining whether a reliable, differentiable predictor is available. If not, sampling methods should be chosen over gradient-based approaches. For example, in the chord progression task of music generation, the ground truth reward is obtained from a chord analysis tool in the music21 package \cite{cuthbert2010music21}, which is non-differentiable. Additionally, the surrogate neural network predictor achieves only $33\%$ accuracy \cite{huang2024symbolic}. As shown in \cref{tab:music guidance}, in cases where no effective differentiable predictor exists, the performance of gradient-based methods (e.g., DPS) is significantly inferior to sampling-based methods (e.g., \xcleansampling and SCG).


If a good differentiable predictor is available, the choice depends on the predictor's forward pass time. Our experimental tasks illustrate two typical cases: In molecule generation, where forward passes are fast as shown in \cref{tab:basic time}, sampling approaches efficiently expand the candidate set and capture the reward signal, yielding strong results (\cref{tab:discrete internal comparison}). In contrast, for enhancer DNA design, where predictors have slow forward passes, increasing the sampling candidate set size to capture the reward signal becomes prohibitively time-consuming, making gradient-based method more effective (\cref{tab:discrete internal comparison}).

\paragraph{\xtsampling v.s. \xcleansampling} Experiments on continuous data and discrete data give divergent results along this axis. In the continuous task of music generation (\cref{tab:music guidance}), \xcleansampling achieves equal or better performance than SCG (equivalent to \xtsampling) with the same candidate size $K=16$ and similar time cost (details in \cref{app:additional music}). Thus, \xcleansampling is preferable in this continuous setting. Conversely, for discrete tasks, \xcleansampling requires significantly more samples, while \xtsampling outperforms it, as shown in \cref{tab:discrete internal comparison}.





\begin{table}[ht]
    \centering
    
    \setlength{\tabcolsep}{3pt} % Default is 6pt\
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
       &  & \xtgrad & \xtsampling &  \xcleansampling  \\
       \midrule 
       \multirow{5}{*}{\rotatebox{90}{Molecule}}  
       & MAE $\downarrow$ & $0.09\pm0.54$ & $0.02\pm0.14$ & $0.10\pm0.33$  \\
        & Validity $\uparrow$ & $13.10\%$ & $14.03\%$ & $2.60\%$ \\
         & Time $\downarrow$ & 13.5s & 12.9s & 11.2s \\
          & $N$ & 30 & 30 & \rule{0.35cm}{0.2mm} \\
           &  $K$ & \rule{0.35cm}{0.2mm} & 2 & 200 \\
    \midrule 
       \multirow{5}{*}{\rotatebox{90}{Enhancer}}  
       & Prob $\uparrow$ 
       & $0.89 \pm 0.14$ & $0.13 \pm 0.39$ & $0.002 \pm 0.000$ \\
       % & $0.894 \pm 0.136$ & $0.129 \pm 0.385$ & $0.002 \pm 0.000$ \\
        & FBD $\downarrow$ & $213$ & $384$ & $665$\\
         & Time $\downarrow$ & 10.3s & 285.1s & 189.7s \\
          & $N$ & 20  & 20 & \rule{0.35cm}{0.2mm}   \\
           &  $K$ & \rule{0.35cm}{0.2mm} & 64 & 1024 \\
    \bottomrule
    \end{tabular}}
    \caption{Comparison of results across \xtgrad, \xtsampling and \xcleansampling. For molecule generation, the target is specified as the number of rings $N_r = 2$. For enhancer DNA design, the results correspond to Class 1.}
    \label{tab:discrete internal comparison}
\end{table}

\begin{table}[ht]
    \centering
    \resizebox{0.35\textwidth}{!}{
    \begin{tabular}{cccc}
    \toprule
         & $C_{\text{model}}$ & $C_{\text{pred}}$ & $C_{\text{backprop}}$ \\
         \midrule
       % Molecule  & 0.79  &  0.27 & 1.05 \\
       Molecule  & 0.038  &  2.2e-4 & 0.036 \\
       Enhancer & 0.087 & 0.021 & 0.11 \\
       % Music & & \\
       \bottomrule
    \end{tabular}}

    \caption{Computation time per basic unit (ms)}
    
    % : forward pass through the flow model and predictor, and backpropagation for gradient computation. The predictor's forward is relatively fast compared to the flow model for molecule generation but slower for DNA enhancer design.}
    \label{tab:basic time}
\end{table}













































\section{Implementation Details}

\subsection{Continuous Models}\label{app:implement details of music}


For \xcleansampling on the continuous case, we have two additional designs: the first one is exploring multiple steps when branching out a destination state; the second one is plugin Spherical Gaussian constraint(DSG) from \cite{yang2024guidance}. We present the case $A=1$ for \xcleansampling on the continuous case as follows while $A>1$ is similar.

\begin{algorithm}
    \begin{algorithmic}[1]
    \caption{\xcleansampling  (Continuous, $A=1$)}
    \label{alg:music detailed}
    \STATE {\bf Input}: diffusion model $u_\theta$, objective function $f_y$, branch out sample size $K$, stepsize scale $\rho_t$, number of iteration $N_{\text{iter}}$
    \STATE $t=0$, $\boldsymbol{x}_0 \sim p_0$
    \WHILE{$t<1$}
    \STATE Compute the predicted clean sample $\hat{\boldsymbol{x}}_1 = u_\theta(\boldsymbol{x}_t, t)$
    \STATE Set the branch out state $\boldsymbol{x} \leftarrow \hat{\boldsymbol{x}}_1 $
    \FOR{$n = 1,\dots, N_{\text{iter}}$}    
    \STATE Sample $\boldsymbol{x}^i = \boldsymbol{x} + \rho_t \boldsymbol{\xi}^i $, with $\boldsymbol{\xi}^i \sim \mathcal{N}\rbr{\boldsymbol{0},  \boldsymbol{I}}$.
    \STATE Evaluate and select that maximizes the objective: $k = \argmax_i f_y\rbr{\boldsymbol{x}^i}$.
    \STATE Update $\boldsymbol{x} \leftarrow \boldsymbol{x}^k$.
    \ENDFOR
    \IF{DSG \cite{yang2024guidance}}
        \STATE Compute the selected direction: $\boldsymbol{\xi}^\ast = \boldsymbol{x} - \hat{\boldsymbol{x}}_1$
    \STATE Rescale the direction: $\boldsymbol{\xi}^\ast \leftarrow \sqrt{D}\cdot \frac{\boldsymbol{\xi}^\ast}{\norm{\boldsymbol{\xi}^\ast}}$, with $D = \text{dim}(\boldsymbol{x}_t)$.
    \STATE Compute the next state: $\boldsymbol{x}_{t+\Delta t}  = c_{t,1} \boldsymbol{x}_t + c_{t,2} \hat{\boldsymbol{x}}_1  + \sigma_t \boldsymbol{\xi}^\ast $
    \ELSE
    \STATE Get the selected destination state $ \hat{\boldsymbol{x}}_1 \leftarrow \boldsymbol{x} $
 \STATE Sample the next state: $\boldsymbol{x}_{t+\Delta t}  = c_{t,1} \boldsymbol{x}_t + c_{t,2} \hat{\boldsymbol{x}}_1  + \sigma_t \epsilon$ with $\epsilon \sim \mathcal{N}\rbr{\boldsymbol{0}, \boldsymbol{I}}$
    \ENDIF

    

\STATE $t \leftarrow t + \Delta t $
    \ENDWHILE
\end{algorithmic}
\end{algorithm}

Notice that the computation complexity for using $N_{\text{iter}}$ step to select is:  $AC_{\text{model}}+AKN_{\text{iter}}C_{\text{pred}}$. The setting of $N_{\text{iter}}$ will be provided in \cref{app:exp detail music}.



\subsection{Discrete Models}
\label{appendix:imp_discret}

\textbf{Estimate $\nabla_{\boldsymbol{x}_t} \log p_t(y\mid \boldsymbol{x}_t)$}. Since the sampling process of discrete data is genuinely not differentiable, we adopt the Straight-through Gumbel Softmax trick to estimate gradient while combining Monte-Carlo Sampling as stated in \cref{eq:training-free estimate for discrete}. The whole process is listed in Module \ref{alg: gumbel_softmax}.

% {Straight-Through Gumbel Softmax.}

\begin{module}[htb]
\begin{algorithmic}[1]
\STATE {\bf Input:} $\boldsymbol{x}_t, t$, diffusion model $u_\theta$, differentiable predictor $f_y$, Monte-Carlo sample size $N$, number of possible states $S$, Gumbel-Softmax temperature $\tau$

\STATE Sample $\hat{\boldsymbol{x}}_1^i\sim\text{Cat}\rbr{u_\theta( \boldsymbol{x}_{t}, t)}, i \in [N]$ with Gumbel Max and represent $x_1$ as an one-hot vector:  
% \begin{equation}
%     \hat{\boldsymbol{x}}_1^i=\arg\max_{\underset{}{j}}(\mathrm{log}u_\theta( \boldsymbol{x}_{t}, t)+g), 
% \end{equation}
\begin{equation}
    \hat{\boldsymbol{x}}_1^i=\arg\max_{j} (\log u_\theta( \boldsymbol{x}_{t}, t) + g^i),
\end{equation}
$g^i$ is a $S$-dimension Gumbel noise where $g_j \sim {\displaystyle {\text{Gumbel}}(0,1)}, j \in [S]$.
\STATE Since the argmax operation is not differentiable, get the approximation
% $\hat{\boldsymbol{x}}_1^i$, 
$\hat{\boldsymbol{x}}_1^{i*}$ with softmax:

\begin{equation}
    \hat{\boldsymbol{x}}_{1j}^{i*}=\frac{\mathrm{exp}((\log u_\theta( \boldsymbol{x}_{t}, t) + g^i_j)/\tau)}{\sum_k \mathrm{exp}((\log u_\theta( \boldsymbol{x}_{t}, t) + g^i_k)/\tau) },
    \label{eq:gumbel_softmax}
\end{equation}

\STATE Feed $\hat{\boldsymbol{x}}_1^i$ into the $f_y$ and obtain the gradient through backpropagation:
 $\nabla_{\hat{\boldsymbol{x}}_1^i} \log f_y( \hat{\boldsymbol{x}}_1^i)$.

\STATE Straight-through Estimator: 
directly copy the gradient $\nabla_{\hat{\boldsymbol{x}}_1^i} \log f_y( \hat{\boldsymbol{x}}_1^i)$ to $\hat{\boldsymbol{x}}_{1j}^{i*}$, i.e., 
$\nabla_{\hat{\boldsymbol{x}}_1^{i*}} \log f_y( \hat{\boldsymbol{x}}_1^{i*}) \simeq \nabla_{\hat{\boldsymbol{x}}_1^i} \log f_y( \hat{\boldsymbol{x}}_1^i)$.

\STATE Since \cref{eq:gumbel_softmax} is differentiable with regard to $x_t$, 
get $\nabla_{\boldsymbol{x}_t} \log p_t(y\mid \boldsymbol{x}_t) \simeq \nabla_{\boldsymbol{x}_t}\frac{1}{N}\sum_{i=1}^N f_y({\hat{\boldsymbol{x}}_1^{i*}})$.

\STATE {\bf Output:} {$\nabla_{\boldsymbol{x}_t} \log p_t(y\mid \boldsymbol{x}_t)$}

\end{algorithmic}
\caption{Gradient Approximation with Straight-Through Gumbel Softmax.}
\label{alg: gumbel_softmax}
\end{module}



\section{Experimental Details}
All experiments are conducted on one NVIDIA 80G H100 GPU.



\subsection{Additional Setup for Symbolic Music Generation}\label{app:exp detail music}

\paragraph{Models.} We utilize the diffusion model and Variational Autoencoder (VAE) from \cite{huang2024symbolic}. These models were originally trained on MAESTRO \cite{hawthorne2018enabling}, Pop1k7 \cite{hsiao2021compound}, Pop909 \cite{wang2020pop909}, and 14k midi files in the classical genre collected from MuseScore. 
% The VAE architecture follows the standard autoencoder design from \cite{rombach2022high}, encoding piano roll segments of dimensions $3 \times 128 \times 128$ into a latent space with dimensions $4 \times 16 \times 16$.
The VAE encodes piano roll segments of dimensions $3 \times 128 \times 128$ into a latent space with dimensions $4 \times 16 \times 16$.


\paragraph{Objective functions.} For the tasks of interest—pitch histogram, note density, and chord progression—the objective function for a given target $\boldsymbol{y}$ is defined as: $f_{\boldsymbol{y}}(\boldsymbol{x}) = - \ell\rbr{\boldsymbol{y}, \texttt{Rule}(\boldsymbol{x})}$, where $\texttt{Rule}(\cdot)$ represents a rule function that extracts the corresponding feature from $\boldsymbol{x}$, and $\ell$ is the loss function. Below, we elaborate on the differentiability of these objective functions for each task:

For pitch histogram, the rule function $\texttt{Rule-PH}(\cdot)$ computes the pitch histogram, and the loss function $\ell$ is the L2 loss. Since $\texttt{Rule-PH}(\cdot)$ is differentiable, the resulting objective function $f_{\boldsymbol{y}}^{\text{PH}}$ is also differentiable.

For note density, the rule function for note density is defined as: $\texttt{Rule-ND}(\boldsymbol{x}) = \sum_{i=1}^n \mathbf{1}(x_i > \epsilon) $ where $\epsilon$ is a small threshold value, and $\mathbf{1}(\cdot)$ is the indicator function which makes $\texttt{Rule-ND}(\cdot)$ non-differentiable. $\ell$ is L2 loss. $f_{\boldsymbol{y}}^{\text{ND}}$ is overall non-differentiable.

For chord progression, the rule function $\texttt{Rule-CP}(\cdot)$ utilizes a chord analysis tool from the music21 package \cite{cuthbert2010music21}. This tool operates as a black-box API, and the associated loss function $\ell$ is a 0-1 loss. Consequently, the objective function $f_{\boldsymbol{y}}^{\text{CP}}$ is highly non-differentiable.





\paragraph{Test targets.} Our workflow follows the methodology outlined by \citet{huang2024symbolic}. For each task, target rule labels are derived from 200 samples in the Muscore test dataset. A single sample is then generated for each target rule label, and the loss is calculated between the target label and the rule label of the generated sample. The mean and standard deviation of these losses across all 200 samples are reported in \cref{tab:music guidance}.



\paragraph{Inference setup.} We use a DDPM with 1000 inference steps. Guidance is applied only after step 250.



\paragraph{Chord progression setup.} Since the objective function running by music21 package \cite{cuthbert2010music21} is very slow, we only conduct guidance during 400-800 inference step.



\paragraph{\xcleansampling setup.}
As detailed in \cref{alg:music detailed}, we  use DSG \cite{yang2024guidance}, and set $N_{\text{iter}} = 2$ for pitch histogram and note density, $N_{\text{iter}} = 1$ for chord progression. The stepsize $\rho_t = s \cdot \sigma_t / \sqrt{1+\sigma_t^2} $ \citep{song2023loss,ye2024tfg}, with $s = 2$ for pitch histogram, $s=0.5$ for note density and $s=1$ for chord progression.




















\subsection{Additional Setup for Small Molecule Generation}\label{app:molecule setup}
Due to lack of a \textit{differentiable} off-the-shelf predictor, we train a regression model $f(\boldsymbol{x})$ on clean $x_1$ following the same procedure described in \citet{nisonoff2024unlocking}. Monte Carlo sample size for estimating $p_t(y\mid \boldsymbol{x}_t)$ in \xtgrad and \xtsampling is $N=30$ (\cref{eq:training-free estimate for discrete}), and $N=200$ for TFG-Flow.


\subsection{Additional Setup for Enhancer DNA Design}
We test on eight randomly selected classes with cell type indices 33, 2, 0, 4, 16, 5, 68, and 9. For simplicity, we refer to these as Class 1 through Class 8.


We set the Monte Carlo sample size of \xtgrad and \xtsampling as $N=20$, and $N=200$ for TFG-Flow.

% \newpage

\section{Additional Experiment Results} \label{app:additional res}

\subsection{Additional Experiment Results for Symbolic Music Generation}\label{app:additional music}

\subsubsection{Additional Information for \cref{tab:music guidance}}
For the results in \cref{tab:music guidance}, the table presents a comparative improvement over the best baseline.


\begin{table}[H]
    \centering
    \resizebox{0.55\textwidth}{!}{
    \begin{tabular}{cccc}
    \toprule
      Task   & Best baseline & \xcleansampling & Loss reduction  \\
      \midrule
     PH &  $0.0010 \pm 0.0020$ (DPS)   &  $0.0002 \pm 0.0003$ & $80 \%$ \\
     ND & $0.134 \pm 0.533$ (SCG) 
     & $0.142 \pm 0.423$ 
     & $-5.97\%$ \\
     CP & $0.347 \pm 0.212$ (SCG) & $0.301 \pm 0.191$ & $13.26 \%$ \\
     Average &\rule{0.4cm}{0.2mm} & \rule{0.4cm}{0.2mm} & $29.01 \%$ \\
     \bottomrule
    \end{tabular}}
    \caption{Improvement of \xcleansampling in music generation.}
\end{table}



Both SCG and our \xcleansampling are gradient-free, which directly evaluates on ground truth objective functions. We provide the inference time of one generation for results in \cref{tab:music guidance}:

\begin{table}[H]
    \centering
    \resizebox{0.26\textwidth}{!}{
    \begin{tabular}{ccc}
    \toprule
      Task   & SCG & \xcleansampling (ours) \\
      \midrule
      PH   & 194 &  203   \\
      ND & 194 & 204 \\
      CP &  7267  & 6660\\
      \bottomrule
    \end{tabular}}
    \caption{Time(s) for SCG and \xcleansampling corresponding to results in \cref{tab:music guidance}.}
\end{table}



\subsubsection{Scalability}\label{app:scaling music sec}


We conduct  experiments scaling $A*K$  for \xcleansampling and \xtsampling, where $A*K$  takes values $(1,2,4,8,16)$ for all combinations of $A$ and $K$ as $2^i$. Numerical results are in \cref{tab:app scaling music numbers}.
% \cref{fig:app music scaling law} shows the scaling law and 
\cref{fig:app music trade-off} shows the trade-off between $A$ and $K$.





\begin{table}[ht]
    \centering
    \resizebox{0.74\textwidth}{!}{
    \begin{tabular}{lccccc} \toprule
        Methods & ($A$,  $K$)   & Loss $\downarrow$ (PH) & OA $\uparrow$ (PH)  & Loss $\downarrow$ (ND) & OA $\uparrow$ (ND) \\
    \midrule
     \multirow{14}{*}{\xtsampling} & $(1,2)$ & $0.0089 \pm 0.0077$ & $0.849 \pm 0.017$ & $0.506 \pm 0.701$ & $0.821 \pm 0.060$ \\
      & $(2,1)$ &  $0.0145 \pm 0.0082$ & $ 0.848 \pm 0.010$ & $1.465 \pm 1.842$ & $0.850 \pm 0.049 $ \\ \cmidrule{2-6}
      & $(1,4)$ &  $0.0061 \pm 0.00661$ & $0.872 \pm 0.015$ & $0.232 \pm 0.397$ & $ 0.836 \pm 0.039$ \\
      & $(2,2)$ & $0.0067 \pm 0.0073$ & $ 0.862 \pm 0.011$ & $0.237 \pm 0.559 $ & $ 0.832 \pm 0.037$ \\ 
      & $(4,1)$ & $0.0107 \pm 0.0059$ & $0.869 \pm 0.021$ & $1.139 \pm 2.271$ & $0.884 \pm 0.021$ \\ \cmidrule{2-6}
      & $(1,8)$ & $0.0049 \pm 0.0069$ & $ 0.861 \pm 0.020$ & $0.187 \pm 0.467$ & $0.831 \pm 0.040$ \\
      & $(2,4)$ & $0.0043 \pm 0.0075$ & $0.877 \pm 0.016$ & $0.158 \pm 0.421$ & $0.856 \pm 0.014$ \\
      & $(4,2)$ & $ 0.0040 \pm 0.0051$ & $0.865 \pm 0.017$ & $0.168 \pm 0.688$ & $0.845 \pm 0.020$ \\
      & $(8,1)$ & $0.0078 \pm 0.0046$ & $0.879 \pm 0.008$ & $0.784 \pm 1.108$ & $0.873\pm 0.021$ \\  \cmidrule{2-6}
      & $(1,16)$ & $0.0036 \pm 0.0057$ & $0.862 \pm 0.008$ & $0.134 \pm 0.533$ & $0.842 \pm 0.022$ \\
      & $(2,8)$ & $0.0035 \pm 0.0090$ & $ 0.853 \pm 0.012$ & $0.100 \pm 0.395$ & $0.843 \pm 0.029$ \\
      & $(4,4)$ & $0.0035 \pm 0.0095$ & $0.832 \pm 0.039$ & $0.072 \pm 0.212$ & $0.841 \pm 0.028$ \\
      & $(8,2)$ & $0.0040 \pm 0.0115$ & $0.842 \pm 0.022$ & $0.071 \pm 0.242$ & $0.836 \pm 0.016 $ \\
      & $(16, 1)$ & $0.0061 \pm 0.0032 $ & $ 0.882 \pm 0.005$ & $0.696 \pm 1.526$ & $0.884 \pm 0.015$ \\
      \midrule
      \multirow{14}{*}{\xcleansampling} & $(1,2)$ & $0.0022 \pm 0.0021$ & $0.869 \pm 0.009$ & $0.629 \pm 0.827$ & $0.826 \pm 0.060$ \\
      & $(2,1)$ &  $0.0145 \pm 0.0082$ & $ 0.848 \pm 0.010$ & $1.465 \pm 1.842$ & $0.850 \pm 0.049 $ \\ \cmidrule{2-6}
      & $(1,4)$    & $0.0008 \pm 0.0008$ & $0.853 \pm 0.013$ & $0.319 \pm 0.619$ & $0.815 \pm 0.059$ \\
      & $(2,2)$    & $ 0.0009 \pm 0.0010$ & $0.845 \pm 0.015$ & $0.231 \pm 0.472$ & $0.823 \pm 0.045$\\
      & $(4,1)$ & $0.0107 \pm 0.0059$ & $0.869 \pm 0.021$ & $1.139 \pm 2.271$ & $0.884 \pm 0.021$ \\ \cmidrule{2-6}
      & $(1,8)$ & $0.0005 \pm 0.0008$ & $0.834 \pm 0.018$ & $0.217 \pm 0.450$ & $0.819 \pm 0.050$ \\
      & $(2,4)$ & $0.0004 \pm 0.0005$ & $0.816 \pm 0.012$ & $0.159 \pm 0.401$ & $0.841 \pm 0.031$ \\
      & $(4,2)$ & $0.0005 \pm 0.0006$ & $0.815 \pm 0.020$ & $0.113 \pm 0.317$ & $0.834 \pm .023$\\
      & $(8,1)$ & $0.0078 \pm 0.0046$ & $0.879 \pm 0.008$ & $0.784 \pm 1.108$ & $0.873\pm 0.021$ \\  \cmidrule{2-6}
      & $(1,16)$ & $0.0002 \pm 0.0003$ & $0.860 \pm 0.016$ & $0.142 \pm 0.423 $ & $0.832 \pm 0.023$ \\
      & $(2,8)$ & $0.0002 \pm 0.0003$ & $0.814 \pm 0.021$ & $0.082 \pm 0.266$ & $0.845 \pm 0.025$ \\
      & $(4,4)$ & $0.0002 \pm 0.0003$ & $0.818 \pm 0.013$ & $0.048 \pm 0.198$ & $0.843 \pm 0.012$ \\
      & $(8,2)$ & $0.0003 \pm 0.0004$ & $0.808 \pm 0.019 $ & $0.057 \pm 0.179$ & $0.820 \pm 0.002$\\
      & $(16, 1)$ & $0.0061 \pm 0.0032 $ & $ 0.882 \pm 0.005$ & $0.696 \pm 1.526$ & $0.884 \pm 0.015$ \\
      \bottomrule
    \end{tabular}
    }
    \caption{Results of scaling $A*K$ of music generation.}
    \label{tab:app scaling music numbers}
\end{table}





\begin{figure}[ht]
    \centering
    \subfigure[\xcleansampling for ND]{\includegraphics[width=4.15cm]{icml2025/figures/music/nd/gs_nd_fix_total_along_active.pdf}}
     \hspace{0.01cm}
    \subfigure[\xcleansampling for PH]{\includegraphics[width=4.15cm]{icml2025/figures/music/ph/gs_ph_fix_total_along_active.pdf}}
    \hspace{0.01cm}
     \subfigure[\xtsampling for ND]{\includegraphics[width=4.15cm]{icml2025/figures/music/nd/scg_nd_fix_total_along_active.pdf}}
     \hspace{0.01cm}
    \subfigure[\xtsampling for PH]{\includegraphics[width=4.15cm]{icml2025/figures/music/ph/scg_ph_fix_total_along_active.pdf}}
    \caption{Trade-off between Active Set Size $A$ and Branch-out Size $K$ on Music Generation.}
    \label{fig:app music trade-off}
\end{figure}






While $A*K$ represents the total computation to some extend, we know the different combination of $A$ and $K$ yields different costs even though $A*K$ is fixed, according to the computation complexity analysis \cref{tab:computation complexity}. We provide true running time for one generation of the frontier of \cref{fig:app music trade-off} (a) and \cref{fig:app music trade-off} (b) at \cref{tab:app music mcts time}.


\begin{table}[H]
    \centering
          \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{cc|cc}
    \toprule
       \multicolumn{2}{c|}{Note density}  &  \multicolumn{2}{c}{Pitch histogram} \\
     Time(s) & Loss    &  Time(s) & Loss  \\
     % \midrule
    16.7 & $2.486 \pm 3.530$ & 16.7 & $0.0180 \pm 0.0100 $ \\
    43.1 & $0.629 \pm 0.827 $ & 42.2 & $0.0022 \pm 0.0021$\\
    71.8 & $0.231 \pm 0.472$ & 65.3 & $0.0008 \pm 0.0008$\\
    130.7 & $0.113 \pm 0.317$ & 118.6&$0.0004 \pm  0.0005 $\\
   251.9  & $0.057 \pm 0.179$ &209.6 & $0.0002 \pm  0.0003 $ \\
     \bottomrule
     
    \end{tabular}}
    \caption{Time and Loss of the optimal $(A,K)$ with fixed $A*K$ for \xcleansampling.}
    \label{tab:app music mcts time}
\end{table}






\subsection{Additional Experiment Results for Small Molecule Generation}
\subsubsection{Additional Results for \cref{tab:smg}.}


\begin{table}[ht]
    \centering
    % \scriptsize
    \setlength{\tabcolsep}{2pt} % Default is 6pt
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{c|cccc}
    \toprule
       % \multirow{2}{*}{Base category} & 
       \multirow{2}{*}{Method}  
       & \multicolumn{2}{c}{$N_r$}  
       & \multicolumn{2}{c}{$\mathrm{LogP}$}  
         \\
        % & 
        & 
        MAE $\downarrow$ & Validity $\uparrow$ 
        & MAE $\downarrow$ & Validity $\uparrow$ \\
         \midrule
         % Training-based & 
         DG 
         & $-39.44\%$ & $47.12\%$ 
         & $-19.45\%$ & $11.53\%$ 
         \\
         \midrule 
        % \multirow{1}{*}{Training-free} & 
        % \multirow{4}{*}{Training-free} & 
         TFG-Flow 
         & $-75.90\%$ & $24.66\%$ 
         & $-44.76\%$ & $1.38\%$ 
         \\
         \bottomrule
    \end{tabular}
    }
    \caption{Relative performance improvement of \xtsampling compared to DG and TFG-Flow (average over different target values). }
    \label{tab:smg_rel}
\end{table}


% Histogram Plot for $N_r^*$.

% Histogram Plot for $\mathrm{LogP}^*$.

\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{1.2pt} % Default is 6pt
      \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{lc|cccccccccccccccccc}
    \toprule
       \multirow{2}{*}{Base category} & \multirow{2}{*}{Method}  & & \multicolumn{2}{c}{$N_r^*=1$}  & \multicolumn{2}{c}{$N_r^*=2$} & \multicolumn{2}{c}{$N_r^*=4$}  & \multicolumn{2}{c}{$N_r^*=5$} 
         \\
        & & & MAE $\downarrow$ & Validity $\uparrow$ 
        & MAE $\downarrow$ & Validity $\uparrow$ & MAE $\downarrow$ & Validity $\uparrow$ & MAE $\downarrow$ & Validity $\uparrow$\\
         \midrule
         Reference & No Guidance &
         & $2.09\pm1.16$ & \rule{0.4cm}{0.2mm}
         & $1.27\pm1.02$ & \rule{0.4cm}{0.2mm}
         & $1.27\pm0.96$ & \rule{0.4cm}{0.2mm}
         & $2.03\pm1.16$ & \rule{0.4cm}{0.2mm}
         % & $2.090\pm1.157$ & $12.20\%$
         % & $1.268\pm1.017$ & $12.20\%$
         % & $1.266\pm0.955$ & $\textbf{12.20}\%$
         % & $2.026\pm1.160$ & $\textbf{12.20}\%$
         \\ 
         \midrule
         Training-based & DG &
         % & $0.128\pm0.346$ & $9.73\%$ 
         % & $0.069\pm0.272$ & $10.77\%$
         % & $0.077\pm0.270$ & $8.09\%$
         % & $0.199\pm0.409$ & $5.06\%$ & 
         & $0.13\pm0.35$ & $9.73\%$ 
         & $0.07\pm0.27$ & $10.77\%$
         & $0.08\pm0.27$ & $8.09\%$
         & $0.20\pm0.41$ & $5.06\%$      
         \\
         \midrule 
        \multirow{2}{*}{Training-free} & 
        % \multirow{4}{*}{Training-free} & 
         TFG-Flow &
         & $0.28\pm0.65$ & $13.61\%$ 
         & $0.20\pm0.51$ & $11.36\%$ 
         & $0.30\pm0.53$ & $8.15\%$ 
         & $0.50\pm0.64$ & $5.70\%$ 
         %    & $0.283\pm0.653$ & $\textbf{13.61}\%$ 
         % & $0.201\pm0.512$ & $11.36\%$ 
         % & $0.304\pm0.529$ & $8.15\%$ 
         % & $0.499\pm0.639$ & $5.70\%$ 
         \\
         \cmidrule{2-11}
    & \xtgrad &
    & $0.43\pm1.18$ & $\textbf{14.97}\%$  
    & $0.09\pm0.54$ & $13.10\%$ 
    & $0.08\pm0.46$ & $10.82\%$ 
    & $0.25\pm0.88$ & $\textbf{7.60}\%$ \\
    
     &   \xtsampling &
     % & $\textbf{0.026}\pm\textbf{0.259}$ & $13.41\%$ 
     % & $\textbf{0.019}\pm\textbf{0.137}$ & $\textbf{14.03}\%$ 
     % & $\textbf{0.043}\pm\textbf{0.247}$ & $11.84\%$ 
     % & $\textbf{0.123}\pm\textbf{0.527}$ & $6.83\%$ 

     & $\textbf{0.03}\pm\textbf{0.26}$ & $13.41\%$ 
     & $\textbf{0.02}\pm\textbf{0.14}$ & $\textbf{14.03}\%$ 
     & $\textbf{0.04}\pm\textbf{0.25}$ & $\textbf{11.84}\%$ 
     & $\textbf{0.12}\pm\textbf{0.53}$ & $6.83\%$ 
       
     \\
      &   \xcleansampling &
      & $0.11\pm0.38$ & $2.91\%$ 
      & $0.10\pm0.33$ & $2.60\%$ 
      & $0.30\pm0.58$ & $1.31\%$ 
      & $0.67\pm0.84$ & $0.70\%$ 
      \\
         \bottomrule
    \end{tabular}}
    \caption{Small Molecule Generation (Target: Number of Rings $N_r$). Branch out sizes $K$ for \xtsampling, \xcleansampling, and \xtgrad are $2, 200$, and $1$ respectively while active set size $A$ is set as $1$ for all \ouralg instantiations.}
    % \label{tab:my_label}
\end{table*}

\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{1.2pt} % Default is 6pt
      \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{lc|cccccccccccccccccc}
    \toprule
       \multirow{2}{*}{Base category} & \multirow{2}{*}{Method}  & & \multicolumn{2}{c}{$\mathrm{LogP}^*=-2$}  & \multicolumn{2}{c}{$\mathrm{LogP}^*=0$} & \multicolumn{2}{c}{$\mathrm{LogP}^*=8$}  & \multicolumn{2}{c}{$\mathrm{LogP}^*=10$} 
         \\
        & & & MAE $\downarrow$ & Validity $\uparrow$ 
        & MAE $\downarrow$ & Validity $\uparrow$ & MAE $\downarrow$ & Validity $\uparrow$ & MAE $\downarrow$ & Validity $\uparrow$\\ \midrule
        Reference & No Guidance &
        & $5.28\pm1.59$ & \rule{0.4cm}{0.2mm}
        & $3.32\pm1.51$ & \rule{0.4cm}{0.2mm}
        & $4.72\pm1.60$ & \rule{0.4cm}{0.2mm}
        & $6.72\pm1.60$ & \rule{0.4cm}{0.2mm}
         \\ 
         \midrule
         Training-based & DG &
         & $0.86\pm0.65$ & $9.29\%$ 
         & $0.65\pm0.52$ & $9.57\%$
         & $0.97\pm0.76$ & $10.42\%$
         & $1.66\pm1.12$ & $5.27\%$ \\
         \midrule 
        \multirow{4}{*}{Training-free} & 
         TFG-Flow &
         & $1.86\pm1.65$ & $8.26\%$ 
         & $1.09\pm1.03$ & $8.77\%$ 
         & $1.53\pm1.28$ & $10.57\%$ 
         & $2.54\pm1.92$ & $10.58\%$ 
         \\
         \cmidrule{2-11}
    & \xtgrad &
    & $1.37\pm2.09$ & $\textbf{13.13}\%$  
    & $\textbf{0.55}\pm\textbf{0.45}$ & $\textbf{13.51}\%$ 
    & $1.06\pm1.78$ & $\textbf{13.13}\%$
    & $5.05\pm3.14$ & $\textbf{11.24}\%$ \\
    
     &   \xtsampling &
     & $\textbf{0.68}\pm\textbf{0.60}$ & $10.27\%$ 
     & $0.58\pm0.47$ & $9.92\%$ 
     & $\textbf{0.70}\pm\textbf{0.91}$ & $10.18\%$ 
     & $\textbf{1.65}\pm\textbf{1.83}$ & $10.31\%$ 
     \\
     
      &   \xcleansampling &
      & $1.73\pm1.46$ & $2.18\%$ 
      & $1.04\pm0.94$ & $2.07\%$ 
      & $1.37\pm1.26$ & $3.47\%$ 
      & $2.25\pm1.89$ & $4.11\%$ 
      \\
         \bottomrule
    \end{tabular}}
    \caption{Small Molecule Generation (Target: Lipophilicity $\mathrm{LogP}$). Branch out sizes $K$ for \xtsampling, \xcleansampling, and \xtgrad are $2, 200$, and $1$ respectively while active set size $A$ is set as $1$ for all \ouralg instantiations.}
    % \label{tab:my_label}
\end{table*}

\begin{figure*}[htbp]
    \centering
    \renewcommand{\thesubfigure}{} % Remove numbering if not needed
    \begin{tabular}{ccccccc}  % 7 columns
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_0.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/tfg-flow/num_rings_0.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_0.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_sampling/num_rings_0.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/x1_sampling/num_rings_0.png}} 
        \\
        \vspace{-10pt}
    
%      \\
% \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_1.png}} &
%         \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/tfg-flow/num_rings_1.png}} &
%         \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_1.png}} &
%         \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_sampling/num_rings_1.png}} &
%         \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/x1_sampling/num_rings_1.png}} 
%     \\

%     \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_2.png}} &
%         \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/tfg-flow/num_rings_2.png}} &
%         \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_2.png}} &
%         \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_sampling/num_rings_2.png}} &
%         \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/x1_sampling/num_rings_2.png}}  

%         \\

        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_3.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/tfg-flow/num_rings_3.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_3.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_sampling/num_rings_3.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/x1_sampling/num_rings_3.png}} 
        \\

        % \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_4.png}} &
        % \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/tfg-flow/num_rings_4.png}} &
        % \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_4.png}} &
        % \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_sampling/num_rings_4.png}} &
        % \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/x1_sampling/num_rings_4.png}} 

        % \\

        % \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_5.png}} &
        % \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/tfg-flow/num_rings_5.png}} &
        % \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_5.png}} &
        % \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_sampling/num_rings_5.png}} &
        % \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/x1_sampling/num_rings_5.png}} 

        \\

        \vspace{-10pt}
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_6.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/tfg-flow/num_rings_6.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_guidance_gradient/num_rings_6.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/xt_sampling/num_rings_6.png}} &
        \subfigure{\includegraphics[width=0.15\linewidth]{icml2025/figures/molecule/histogram_plots/x1_sampling/num_rings_6.png}} 

        % Repeat for more rows...
    \end{tabular}
    \caption{Distribution of $N_r$ in generated small molecules. The three rows correspond to different target values $N_r^*=0,3,6$. From left to right, each column represents different guidance methods (1) DG (2) TFG-Flow (3) \xtgrad (4) \xtsampling (5) \xcleansampling. Blue blocks show the statistics of training dataset.}
    \label{fig:grid}
\end{figure*}

\begin{figure}[ht]
    \centering
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/molecule/scaling_law/ylog_TreeG-SC_num_rings_3.pdf}}
    \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/molecule/scaling_law/ylog_TreeG-SC_num_rings_6.pdf}}
    \hspace{0.01cm}
     \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/molecule/scaling_law/ylog_TreeG-SC_logp_2.pdf}}
     \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/molecule/scaling_law/ylog_TreeG-SC_logp_8.pdf}}
    \caption{Small Molecule Generation: Scaling Law of \xtsampling. From left to right, the targets are $N_r^*=3$,  $N_r^*=6$, $\mathrm{LogP}^*=2$,  $\mathrm{LogP}^*=8$.}
    \label{fig:app smg scaling law xt sampling}
\end{figure}

\begin{figure}[ht]
    \centering
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/molecule/scaling_law/ylog_TreeG-SD_num_rings_1.pdf}}
    \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/molecule/scaling_law/ylog_TreeG-SD_num_rings_5.pdf}}
    \hspace{0.01cm}
     \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/molecule/scaling_law/ylog_TreeG-SD_logp_-2.pdf}}
     \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/molecule/scaling_law/ylog_TreeG-SD_logp_10.pdf}}
    \caption{Small Molecule Generation: Scaling Law of \xcleansampling. From left to right, the targets are $N_r^*=1$,  $N_r^*=5$, $\mathrm{LogP}^*=-2$,  $\mathrm{LogP}^*=10$.}
    \label{fig:app smg scaling law x1 sampling}
\end{figure}

\subsubsection{Scalability}

We demonstrate the scaling laws for \xtsampling and \xcleansampling in \cref{fig:app smg scaling law xt sampling} and \cref{fig:app smg scaling law x1 sampling}. Increasing computation time could boost guidance performance, i.e., lower mean absolute errors.





\subsection{Additional Experiment Results for Enhancer DNA Design}
\subsubsection{Full Results of \cref{tab:enhancer guidance res}}
Additional results of guidance methods for Class 4-8 are shown in \cref{tab:full DNA res}.
For both DG and our \xtgrad, we experiment with guidance values $\gamma \in [1, 2, 5, 10, 20, 50, 100, 200]$ and compare the highest average conditional probability across the eight classes. On average, \xtgrad outperforms DG by $18.43\%$. 


\begin{table*}[ht]
    \centering
    \resizebox{0.97\textwidth}{!}{
    \begin{tabular}{cc|cccccccccc}
    \toprule
     \multicolumn{2}{c|}{\multirow{2}{*}{Method  { \small (strength $\gamma$)}} }& \multicolumn{2}{c}{Class 4} & \multicolumn{2}{c}{Class 5} & \multicolumn{2}{c}{Class 6}& \multicolumn{2}{c}{Class 7} & \multicolumn{2}{c}{Class 8}\\
   & & Prob  $\uparrow$ & FBD  $\downarrow$ & Prob  $\uparrow$ & FBD  $\downarrow$  & Prob  $\uparrow$ & FBD  $\downarrow$  & Prob  $\uparrow$ & FBD  & Prob  $\uparrow$ & FBD \\
   \midrule
    No Guidance & \rule{0.4cm}{0.2mm} & $0.007 \pm 0.059$ & $446$ & $0.035 \pm 0.112$ & $141$ & $0.037 \pm 0.115$ & $179$& $0.010 \pm 0.065$ & $292$ & $0.013 \pm 0.073$ & $478$ \\
         \midrule
            \multirow{3}{*}{DG} &  {\footnotesize $20$} & $0.669 \pm 0.377$ & $57 $ & $0.665 \pm 0.332$ & $32$ & $0.595 \pm 0.334$ & $ 26$ & $0.693 \pm 0.346$ & $61$ & $0.609 \pm 0.350$ & $95$  \\
          & {\footnotesize $100$} & $0.585 \pm 0.380 $ & $132$ & $0.466 \pm 0.376$ & $86$ &$0.385 \pm 0.334$ & $84$ & $0.475  \pm 0.398$ & $149$ & $0.600 \pm 0.342$ & $326$\\
          & {\footnotesize $200$} & $0.404 \pm 0.384$ & $140$ & $0.199 \pm 0.284$ & $148$ & $0.164 \pm 0.235$ & $132$   & $0.208 \pm 0.313$ & $266$& $0.453 \pm 0.370$ & $362$ \\
         \midrule          
           {TFG-Flow} &  {\footnotesize $200$} & $0.015 \pm 0.083$ & $408$ & $0.008 \pm 0.048$ & $267$ & $0.033 \pm 0.104$ & $271$ & $0.001 \pm 0.015$ & $371$ & $0.006 \pm 0.055$ & $662$\\ \midrule
           \multirow{3}{*}{\textbf{\xtgrad}} &  {\footnotesize $ 20$} & $0.518 \pm 0.391$ & $115$ &  $0.290 \pm 0.306$ & $61$ & $0.250 \pm 0.292$ & $67$ & $0.536 \pm 0.335$ & $137$& $0.159 \pm 0.233$ & $332$\\ 
           & {\footnotesize $ 100$} &$0.845 \pm 0.264$ & $199$  &  $0.778 \pm 0.279$ & $123$ & $0.843 \pm 0.232$ & $120$ & $0.740 \pm 0.365$ & $161$ & $0.413\pm 0.412$ & $307$\\
          & {\footnotesize $200$} & $ 0.826 \pm 0.297$ & $236$ & $0.543 \pm 0.426$ & $104$ & $0.364 \pm 0.432$ & $98$ & $0.423 \pm 0.457$ & $177$ & $0.073 \pm 0.205$ & $346$\\       
         \bottomrule
    \end{tabular}
    }
    \caption{Additional results of the evaluation of guidance methods for  enhancer DNA design.}
    \label{tab:full DNA res}
\end{table*}



\subsubsection{Scalability}\label{app:scalability of DNA}
\paragraph{$A*K$ as a Computation Reference.} We use $A*K$ as the reference metric for inference time computation in \xtsampling and \xtgrad, both employing \texttt{BranchOut}-Current. The corresponding inference times are shown in \cref{fig:time with fixed AK}, measured for a batch size of 100. Combinations of $(A,K)$ that yield the same $A*K$ value exhibit similar inference times. We exclude the case where $K=1$, as it does not require evaluation and selection, leading to a shorter inference time in practical implementation.

% We use $A*K$ as the  reference of inference time compute for \xtsampling and \xtgrad, algorithms using \texttt{BranchOut}-Current, the specified time is shown in \cref{fig:time with fixed AK}. The inference time is for a batch with size $100$. The $(A,K)$ combinations that yield the same $A*K$ value exhibit similar inference times. We did not include the case where k=1, as evaluation and selection are not required in this scenario,  resulting in a shorter time in practical implementation.

\begin{figure}[ht]
    \centering
    \subfigure{\includegraphics[width=0.48\linewidth]{icml2025/figures/enhancer/xt_sampling_mcts/time.pdf}}
    \subfigure{\includegraphics[width=0.48\linewidth]{icml2025/figures/enhancer/xt_grad_mcts/time.pdf}}
    \caption{Inference Time for $(A,K)$ Combinations (Top: \xtsampling; Bottom \xtgrad for DNA enhancer design). The $(A,K)$ combinations that yield the same $A*K$ value exhibit similar inference times. $K=1$ is excluded from this comparison since evaluation and selection are unnecessary in this case, leading to a shorter inference time in practical implementations.}
    \label{fig:time with fixed AK}
\end{figure}







We provide the scaling law of \xtgrad at different guidance strengths in \cref{fig:app dna scaling law}. The corresponding trade-off between $A$ and $K$ are shown in \cref{fig:app DNA trade-off}. We also provide the scaling law and trade-off for \xtsampling in \cref{fig:app dna scaling law xt sampling} and \cref{fig:app DNA trade-off xt sampling}, respectively. 


\begin{figure}[ht]
    \centering
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_grad_mcts/scaling/class_33/temp_0.2.pdf}}
    \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_grad_mcts/scaling/class_33/temp_0.1.pdf}}
    \hspace{0.01cm}
     \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_grad_mcts/scaling/class_33/temp_0.05.pdf}}
     \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_grad_mcts/scaling/class_33/temp_0.005.pdf}}
    \caption{Enhancer DNA Design: Scaling Law of \xtgrad across Different Guidance Strengths:  $\gamma = 5, 10, 20, 200$ from left to right (Class 1). }
    \label{fig:app dna scaling law}
\end{figure}

\begin{figure}[ht]
    \centering
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_grad_mcts/fixed_total/class_33/temp_0.2.pdf}}
    \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_grad_mcts/fixed_total/class_33/temp_0.1.pdf}}
    \hspace{0.01cm}
     \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_grad_mcts/fixed_total/class_33/temp_0.05.pdf}}
     \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_grad_mcts/fixed_total/class_33/temp_0.005.pdf}}
    \caption{Enhancer DNA Design: Trade-off between $A$ and $K$ for \xtgrad. From left to right, guidance strengths are $\gamma = 5, 10, 20, 200$ (Class 1). }
    \label{fig:app DNA trade-off}
\end{figure}



\begin{figure}[ht]
    \centering
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_sampling_mcts/scaling/class_33.pdf}}
    \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_sampling_mcts/scaling/class_2.pdf}}
    \hspace{0.01cm}
     \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_sampling_mcts/scaling/class_0.pdf}}
     \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_sampling_mcts/scaling/class_4.pdf}}
    \caption{Enhancer DNA Design: Scaling Law of \xtsampling, with Class 1 to 4 shown from left to right.}
    \label{fig:app dna scaling law xt sampling}
\end{figure}

\begin{figure}[ht]
    \centering
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_sampling_mcts/fixed_total/class_33.pdf}}
    \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_sampling_mcts/fixed_total/class_2.pdf}}
    \hspace{0.01cm}
     \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_sampling_mcts/fixed_total/class_0.pdf}}
     \hspace{0.01cm}
    \subfigure{\includegraphics[width=4.15cm]{icml2025/figures/enhancer/xt_sampling_mcts/fixed_total/class_4.pdf}}
    \caption{Enhancer DNA Design: Trade-off between $A$ and $K$ for \xtsampling, with Class 1 to 4 shown from left to right. }
    \label{fig:app DNA trade-off xt sampling}
\end{figure}























% \newpage
\section{Ablation Studies}
\subsection{Symbolic Music Generation}

For \xcleansampling, we ablation on $N_{\text{iter}}$ (detailed in \cref{alg:music detailed}). As shown in \cref{tab:ablation N iter}, the loss decreases when $N_{\text{iter}}$ increases. However, increasing $N_{\text{iter}}$ also leads to higher computation costs. Here's a trade-off between controllability and computation cost.


\begin{table}[ht]
    \centering
          \resizebox{0.6\textwidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
      $N_{\text{iter}}$& Loss $\downarrow$ (PH) & OA $\uparrow$ (PH)  & Loss $\downarrow$ (ND) & OA $\uparrow$ (ND) \\
    \midrule
       1  &  $0.0031 \pm 0.0037$ & $0.733 \pm  0.024$ & $0.207 \pm 0.418$ & $0.810 \pm 0.049$  \\
       2 & $0.0005 \pm 0.0008$ & $0.833 \pm 0.018$ & $0.217 \pm 0.450$ & $0.819 \pm 0.050$ \\
       4 &  $0.0005 \pm 0.0006$ & $0.786 \pm 0.015$ & $0.139 \pm 0.319$ & $0.830 \pm 0.029$  \\
       \bottomrule
    \end{tabular}}
    \caption{Ablation on $N_{\text{iter}}$ of \xcleansampling.}
    \label{tab:ablation N iter}
\end{table}








\subsection{Discrete Models}
\label{app:ablation discrete}

\textbf{Ablation on Taylor-expansion Approximation.}

As shown in \cref{tab:ablation taylor}, using Taylor-expansion to approximate the ratio (\cref{eq: taylor expansion}) achieve comparable model performance while dramatically improve the sampling efficiency compared to calculating the ratio by definition, i.e. \xtgrad-Exact.


\textbf{Ablation on Monte Carlo Sample Size $N$.}


As shown in \cref{tab:ablation N}, increasing the Monte Carlo sample size improves performance, but further increases in $N$ beyond a certain point do not lead to additional gains.


% discrete: $N$ for sampling $x_1 \mid x_t$;



\begin{table}[ht]
    \centering
    % \subfigure[]{
    % \begin{minipage}{1\linewidth}{
      \resizebox{0.75\textwidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
      % Method & Loss $\downarrow$ (PH) & OA $\uparrow$ (PH)  & Loss $\downarrow$ (ND) & OA $\uparrow$ (ND) \\
    \multirow{2}{*}{Method}  & \multicolumn{3}{c}{$\mathrm{N_r}^*=1$}  & \multicolumn{3}{c}{$\mathrm{N_r}^*=2$} 
         \\
        & MAE $\downarrow$ & Validity $\uparrow$ & Time
        & MAE $\downarrow$ & Validity $\uparrow$ & Time \\ 
    \midrule
       \xtgrad &  $0.24 \pm 0.76$ & $19.23\%$ & 2.4min & $0.02 \pm 0.14$ & $13.51$ & 3.5min \\
       \xtgrad-Exact & $0.00 \pm 0.00$ & $18.52\%$ & 356.9min &  $0.02 \pm 0.14$ & $19.23\%$ &  345.2min \\
       \bottomrule
    \end{tabular}}
    \caption{Ablation on Taylor-expansion Approximation. The performances are evaluated on 50 generated samples.
    }
    % \end{minipage}
    \label{tab:ablation taylor}
\end{table}


\begin{table}[ht]
    \centering
    % \begin{subtable}[t]{0.45\textwidth}
      \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
      % Method & Loss $\downarrow$ (PH) & OA $\uparrow$ (PH)  & Loss $\downarrow$ (ND) & OA $\uparrow$ (ND) \\
    \multirow{2}{*}{$N$}  & \multicolumn{2}{c}{$\mathrm{N_r}^*=2$}  & \multicolumn{2}{c}{$\mathrm{N_r}^*=5$} 
         \\
        & MAE $\downarrow$ & Validity $\uparrow$ 
        & MAE $\downarrow$ & Validity $\uparrow$  \\ 
    \midrule
       1   & $0.47 \pm 1.12$ & $13.51\%$ & $0.65 \pm 1.37$ & $8.83\%$ \\
       5 &  $0.30 \pm 0.97$ & $11.89\%$ &  $0.41 \pm 1.14 $ & $7.61\%$  \\
           10  &  $0.17 \pm 0.68$ & $\textbf{14.20}\%$  &  $\textbf{0.20} \pm \textbf{0.76}$ & $\textbf{8.87}\%$  \\
        20  &  $\textbf{0.09} \pm \textbf{0.46}$ & $12.50\%$ &  $0.26 \pm 0.91$ & $7.81\%$ \\
                40  &  $0.10 \pm 0.60$ & $12.65\%$ &  $0.29 \pm 1.01$ & $8.58\%$   \\
       \bottomrule
    \end{tabular}}
    \caption{Ablation on Monte Carlo Sample Size $N$. The performances are evaluated on 200 generated samples.
    }
    % \end{subtable}
    \label{tab:ablation N}
\end{table}




