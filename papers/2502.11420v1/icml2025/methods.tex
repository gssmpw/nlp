% \section{\ouralg: \underline{S}calable \underline{P}ath Steering and Tree \underline{S}earch}
\section{\ouralg: \underline{Tree} Search-Based Path Steering \underline{G}uidance}
\label{sec:propose sps}

While the gradient of the objective, when available, can offer a precise direction to steer inference \citep{guo2024gradient}, an alternative approach when the gradient in unavailable is to discover a good inference path through search: proposing multiple candidates for the next step, evaluating the candidates using some value function that reflects the objective, and selecting the best candidate to move forward. The search procedure is applicable beyond the differentiability assumption as it evaluates candidates with the zeroth-order information from the objective. 

Based on this insight, we propose a framework that steers the inference path with search to achieve a targeted objective, by only leveraging the zeroth-order signals. 

\subsection{Algorithmic Framework}
Let $\boldsymbol{x}_0, \cdots, \boldsymbol{x}_t, \boldsymbol{x}_{t+\Delta t}, \cdots, \boldsymbol{x}_1$ denote the inference process transforming the pure noise state $\boldsymbol{x}_0$ to the clean sample state $\boldsymbol{x}_1$. In Alg.~\ref{alg:framework}, at each step, path steering guidance uses the \texttt{BranchOut} module to propose $K$ candidate next states. The top candidates are then selected based on evaluations from the value function $V$ to proceed forward. The basic path steering guidance maintains a single path with one active sample at each step. However, the number of paths and active samples— the active set size $A$—can be scaled up for more efficient exploration through a tree-search mechanism.






\begin{algorithm}
\begin{algorithmic}[1]
\caption{\ouralg: {Tree} Search-Based Path Steering {G}uidance}
\label{alg:framework}
\STATE {\bf Input:} diffusion model $u_{\theta}$, branch out policy and value function $ (\texttt{BranchOut}, V)$, objective function $f_y$, active set size $A$, branch out sample size $K$.
\STATE {\bf Initialize:} $t=0$,  $\mathcal{A} = \cbr{  \boldsymbol{x}_0^{1}, \ldots, \boldsymbol{x}_0^{A} }$,  $\boldsymbol{x}_0^{i} \sim p_0$.
\WHILE{$t < 1$}
\STATE {\bf Propose candidates for next step:} For $\boldsymbol{x}_{t}^{i} \in \mathcal{A}$, \\
$\boldsymbol{x}_{t+\Delta t}^{i, j} \sim  \texttt{BranchOut}\rbr{\boldsymbol{x}_t^{i}, u_\theta}$, $ j \in [K]$. 
\STATE {\bf Selection:} top $A$ candidates with respect to the value function $V\rbr{\boldsymbol{x}_{t+\Delta t}^{i, j}, t, f_y}$, $i \in [A], j \in [K] $: $\boldsymbol{x}_{t+\Delta t}^{i_1, j_1},\ldots,\boldsymbol{x}_{t+\Delta t}^{i_A, j_A}$.
% \STATE Get the next state $\boldsymbol{x}_{t+\Delta t}^{(r)} \leftarrow h \rbr{\boldsymbol{x}_t^{(i_r)}, u_\theta, \boldsymbol{\xi}^{(i_r, j_r)}} $ for $r\in [A]$
\STATE {\bf Update the active set:}  $ \mathcal{A} = \cbr{  \boldsymbol{x}_{t+\Delta t}^{i_1,j_1}, \ldots, \boldsymbol{x}_{t+\Delta t}^{i_A, j_A} }$
\STATE $t \leftarrow t + \Delta t $.
\ENDWHILE
\STATE {\bf Output}: $\boldsymbol{x}_1^\ast = \argmax_{\boldsymbol{x}_1 \in \mathcal{A}}f_y\rbr{\boldsymbol{x}_1}$.

\end{algorithmic}

\end{algorithm}

In Alg.~\ref{alg:framework}, the module \texttt{BranchOut} and the value function $V$ are two key components that require careful designs, for which we will present our novel designs in the next section. By specifying \texttt{BranchOut} and $V$, our framework gives rise to new algorithms demonstrating superior empirical performance to the existing baselines (Section~\ref{sec:exp}). In addition, we will see that our framework unifies multiple existing training-free guidance methods (Section~\ref{subsec:analysis}). 


\section{Design Space of \ouralg}\label{sec: design space}
In this section, we will navigate through the design space of the \ouralg algorithm, specifically the $(\texttt{BranchOut}, V)$ pair. We propose two compatible $(\texttt{BranchOut}, V)$ pairs, which operate by sampling and selecting either from the current state or the predicted destination state, respectively. We also propose a gradient-based discrete guidance method as a special case of \ouralg. 





\subsection{Sample-then-Select on Current States}
\label{subsec:xtsampling}

The idea for our first $(\texttt{BranchOut}, V)$ pair is straightforward: sampling multiple realizations at the current state as candidates using the original generation process and selecting the one that leads to the most promising end state of the path. We define $\texttt{BranchOut}$ for the current state as follows. 

\begin{module}[htb]
\begin{algorithmic}[1]
\STATE {\bf Input:} $\boldsymbol{x}_t$, diffusion model $u_\theta$, time step $t$.
\STATE Sample the next state by the original generation process:
$\boldsymbol{x}_{t + \Delta t} \sim$ \eqref{eq:diffusion ddpm sampling} or \eqref{eq:discrete euler sampling}.
\STATE {\bf Output}: { $\boldsymbol{x}_{t + \Delta t}$}
\end{algorithmic}
\caption{\texttt{BranchOut}-Current}
\label{mol:xt_sampling}
\end{module}
\vspace{-5pt}

To evaluate $\boldsymbol{x}_{t}$ (or $ \boldsymbol{x}_{t+\Delta t}$), we propose using the value of $f_y$ at the end state if the generation process were to continue from this state. Specifically, for target $y$, we have
\begin{equation}\label{eq:prob y given xt}
\begin{aligned}
    \log p_t(y \mid \boldsymbol{x}_t) &= \log \EE_{\boldsymbol{x}_1 \sim p_{1|t} }[p(y\mid \boldsymbol{x}_1)] \\
    &\simeq \EE_{\boldsymbol{x}_1 \sim p_{1|t} } [\log p(y\mid \boldsymbol{x}_1)] \\
    &= \EE_{\boldsymbol{x}_1 \sim p_{1|t} } [f_y(\boldsymbol{x}_1)],
\end{aligned}
\end{equation}
where $f_y(\cdot) = \log p(y\mid \cdot)$ is the off-the-shelf objective operating in the clean space. Based on \eqref{eq:prob y given xt}, we propose the value function for the current noisy states as follows.


\begin{vfunction}[htb]
\begin{algorithmic}[1]
\STATE {\bf Input:} $\boldsymbol{x}_t$, diffusion model $u_\theta$, objective function $f_y$, time step $t$, (optional) Monte-Carlo sample size $N$.
\STATE {\bf Predict the clean sample:}\\
\quad (continuous) $\hat{\boldsymbol{x}}_1 = u_\theta(\boldsymbol{x}_t, t)$.\\
\quad (discrete) $\hat{\boldsymbol{x}}_1^i \sim \text{Cat} \rbr{u_\theta( \boldsymbol{x}_{t}, t)}, i \in [N]$.
\STATE {\bf Evaluate:} $V(\boldsymbol{x}_t) = \frac{1}{N}\sum_{i=1}^N f_y({\hat{\boldsymbol{x}}_1^{i}})$.
\STATE {\bf Output:} { $V(\boldsymbol{x}_t)$}
\end{algorithmic}
\caption{$V:$ Current State Evaluator}
\label{vfunc:noisy}
\end{vfunction}

Note that we use the conditional expectation $u_\theta(\boldsymbol{x}_t, t) = \EE \sbr{\boldsymbol{x}_1 \mid \boldsymbol{x}_t} $ as point estimation in Line 2 for the continuous case. \citet{ye2024tfg} observes that the point estimation yields a similar performance to the Monte Carlo estimation \citep{song2023loss} in continuous guidance. So for simplicity, we adopt the point estimation.

We refer to instantiating \cref{alg:framework} with Module~\ref{mol:xt_sampling} and value function~\ref{vfunc:noisy} as \textbf{\ouralg-Sampling Current}, abbreviated as \textbf{\xtsampling}.





\subsection{Sample-then-Select on Destination States}\label{subsec:x1sampling}
During inference, the transition probability in each step is determined by the current state and the end state of the path, which is estimated by the diffusion model, as stated in the following lemma (proof is in \cref{prof:transition prob}).
% In diffusion and flow models, one run of inference forms one path transforming a pure noise into a fully denoised sample, visiting $T$ intermediate states ${\boldsymbol{x}}_{t}$. The pre-trained model weights determine the probabilistic transition kernel from one state to its previous state: 
\begin{lemma} \label{lemma:transition prob}
In both continuous and discrete cases, the transition probability during inference at timestep $t$ satisfies:
\begin{equation}
\label{eqn:path_transition}
\begin{aligned}
    \mathcal{T}(\boldsymbol{x}_{t + \Delta t} \mid \boldsymbol{x}_{t}) = \EE_{{\hat{\boldsymbol{x}}}_1} \sbr{\mathcal{T}^{\star}(\boldsymbol{x}_{t + \Delta t} \mid \boldsymbol{x}_{t}, \hat{{\boldsymbol{x}}}_1)},
\end{aligned}
\end{equation}
where the expectation is taken over a distribution estimated by $u_\theta$, with $\mathcal{T}^{\star}$ being the true posterior distribution predetermined by the noise schedule.
\end{lemma}

In diffusion and flow models, $\mathcal{T}^{\star}$ is centered at a linear interpolation between its inputs: 
the current state $\boldsymbol{x}_{t}$, and the predicted destination state $\hat{{\boldsymbol{x}}}_1$, indicating that the orientation of the next state is partially determined by $\hat{\boldsymbol{x}}_1$, as it serves as one endpoint of the interpolation. If the $\hat{\boldsymbol{x}}_t$ has a high objective value, then its corresponding next state will be more oriented to a high objective. We define $\texttt{BranchOut}$ for the destination state as follows.









\begin{module}[htb]
\begin{algorithmic}[1]
\STATE {\bf Input:} $\boldsymbol{x}_t$, diffusion model $u_\theta$, time step $t$, \\(optional) tuning parameter $\rho_t$.
\STATE {\bf Sample destination state candidates:}
\\
\quad {(continuous)} $\hat{\boldsymbol{x}}_1 \sim \mathcal{N}(u_\theta(\boldsymbol{x}_t,t), \rho_t^2 \boldsymbol{I})$.\\
\quad {(discrete)} $\hat{\boldsymbol{x}}_1 \sim \text{Cat}\rbr{u_\theta( \boldsymbol{x}_{t},t)}$. \\
\STATE {\bf Compute the next state:}
\\
\quad (continuous) $\boldsymbol{x}_{t+\Delta t} \sim \mathcal{N}\rbr{c_{t,1} \boldsymbol{x}_t + c_{t,2} \hat{\boldsymbol{x}}_1, \sigma^2_t I}$.\\
\quad(discrete) ${x}_{t+\Delta t}^{(d)} \sim $\\ 
\quad$ \text{Cat}\rbr{\delta\{x_t^{(d)}, j \} + R_t\rbr{{x}_t^{(d)}, j \mid \hat{x}_1^{(d)} } \Delta t }$. \\
\STATE {\bf Output}: { $(\boldsymbol{x}_{t + \Delta t}, \boldsymbol{\hat{x}}_1)$}
\end{algorithmic}
\caption{\texttt{BranchOut}-Destination}
\label{mol:x1_sampling}
\end{module}


For the continuous diffusion model, the distribution to sample $\hat{\boldsymbol{x}}_1$ for is exploring around the point estimate $u_\theta( \boldsymbol{x}_{t},t)$, where $\rho_t$ is a tuning parameter. Implementation details for \cref{mol:x1_sampling} is in \cref{app:implement details of music}. For $\boldsymbol{x}_{t + \Delta t}$ generated by \texttt{BranchOut}-Destination, we evaluate it by the objective value of its corresponding $\hat{\boldsymbol{x}}_1$.

\begin{vfunction}[htb]
\begin{algorithmic}[1]
\STATE {\bf Input:} $(\boldsymbol{x}_{t + \Delta t}, \boldsymbol{\hat{x}}_1)$, objective function $f_y$.
\STATE Evaluate on the clean sample:
$V\rbr{(\boldsymbol{x}_{t + \Delta t}, \boldsymbol{\hat{x}}_1)} = f_y\rbr{\boldsymbol{\hat{x}}_1}$.
\STATE {\bf Output:} { $V\rbr{(\boldsymbol{x}_{t + \Delta t}, \boldsymbol{\hat{x}}_1)}$}
\end{algorithmic}
\caption{$V:$ Destination State Evaluator}
\label{vfunc:clean}
\end{vfunction}


We name \cref{alg:framework} with Module~\ref{mol:x1_sampling} and Value Function~\ref{vfunc:clean} by \textbf{\ouralg-Sampling Destination (\xcleansampling)}.



\subsection{Gradient-Based Guidance with Objective Predictor}\label{subsec:xtgrad}
Previously in this section, we derived two algorithms that do not rely on the gradient of objective. Though we do not assume the true objective is differentiable, when a differentiable objective predictor is available, leveraging its gradient as guidance is still a feasible option. Therefore, in what follows, we propose a novel gradient-based training-free guidance for discrete flow models, which also fits into the \ouralg framework as a special case with $K=1$.


In a discrete flow model, sampling from the conditional distribution $p(\boldsymbol{x} \mid y)$ requires the conditional rate matrix $R_t(\boldsymbol{x}_t, \cdot \mid y)$. \cite{nisonoff2024unlocking} derived the relation between the conditional and unconditional rate matrix:
\begin{equation}
    R_t(\boldsymbol{x}_t, j \mid y) = \frac{p_t(y \mid \boldsymbol{x}_t^{\backslash d}(j) )}{p_t(y \mid \boldsymbol{x}_t)} \cdot R_t(\boldsymbol{x}_t, j),
\end{equation}
where $\boldsymbol{x}_t^{\backslash d}$ matches $\boldsymbol{x}_t$  except at dimension $d$, and  $\boldsymbol{x}_t^{\backslash d}(j)$ has its $d$-dimension set to $j$. $R_t(\boldsymbol{x}_t, j)$ can be estimated by the rate matrix $R_{\theta, t}(\boldsymbol{x}_t, j)$ computed from the flow model, so we only need to estimate the ratio $\frac{p_t(y \mid \boldsymbol{x}_t^{\backslash d}(j) )}{p_t(y \mid \boldsymbol{x}_t)}$, which further reduces to estimate $p_t(y \mid \boldsymbol{x})$ for any given $\boldsymbol{x}$. While \citet{nisonoff2024unlocking} requires training a time-dependent predictor to estimate $p_t(y \mid \boldsymbol{x})$, we propose to estimate it using \eqref{eq:prob y given xt} in a training-free way. Here we restate:
\begin{equation}\label{eq:training-free estimate for discrete}
       \log p_t(y \mid \boldsymbol{x}) \simeq \EE_{\boldsymbol{x}_1 \sim u_\theta } [f_y(\boldsymbol{x}_1)] \simeq \frac{1}{N}\sum_{i=1}^N f_y({\hat{\boldsymbol{x}}_1^{i}}),
\end{equation}
where $N$ is  the Monte Carlo sample size and $\hat{\boldsymbol{x}}_1^i \sim \text{Cat} \rbr{u_\theta( \boldsymbol{x}, t)}, i \in [N]$. However, computing this estimation over all possible $ \boldsymbol{x}_t^{\backslash d}$'s is computationally expensive. As suggested by \citet{nisonoff2024unlocking, vignac2022digress}, we can approximate the ratio using Taylor expansion:
\begin{equation}
\begin{aligned}
    \log \frac{p_t(y \mid \boldsymbol{x}_t^{\backslash d} )}{p_t(y \mid \boldsymbol{x}_t)} &= {\log p_t(y \mid \boldsymbol{x}_t^{\backslash d} )}-{\log p_t(y \mid \boldsymbol{x}_t)} \\
    & \simeq (\boldsymbol{x}_t^{\backslash
       d} - \boldsymbol{x}_t)^{\top} \nabla_{\boldsymbol{x}_t} \log p_t(y\mid \boldsymbol{x}_t).
    \label{eq: taylor expansion}
\end{aligned}
\end{equation}
We apply the Straight-Through Gumbel-Softmax trick \citep{jang2016categorical} to enable gradient backpropagation through the sampling process. Implementation details are provided in \cref{appendix:imp_discret}, where we also verify this approximation has good accuracy compared to computing \eqref{eq:training-free estimate for discrete} for all $\boldsymbol{x}_t^{\backslash d}$'s, while enjoys higher efficiency. Combining the estimation from \eqref{eq:training-free estimate for discrete}, we obtain our gradient-based training-free guidance for discrete flow and unify it into \ouralg by defining the following \texttt{BranchOut} module, with its continuous counterpart:
\begin{module}[htb]
\begin{algorithmic}[1]
\STATE {\bf Input:} $\boldsymbol{x}_t, t$, diffusion model $u_\theta$, differentiable predictor $f_y$, guidance strength $\gamma_t$, (optional) Monte-Carlo sample size $N$.
\STATE {\bf Compute the gradient guidance:}
\\
\quad {(continuous)} $\boldsymbol{g} = \nabla_{\boldsymbol{x}_t}f_y(\hat{\boldsymbol{x}}_1)$ with  $\hat{\boldsymbol{x}}_1 = u_\theta(\boldsymbol{x}_t,t)$. \\
\quad {(discrete)} $\boldsymbol{g}^{(d)} = (\boldsymbol{x}_t^{\backslash
       d} - \boldsymbol{x}_t)^{\top} \nabla_{\boldsymbol{x}_t}\frac{1}{N}\sum_{i=1}^N f_y({\hat{\boldsymbol{x}}_1^{i}}) $\\
       \quad  with $\hat{\boldsymbol{x}}_1^i \sim \text{Cat}\rbr{u_\theta( \boldsymbol{x}_{t},t)}, i \in[N]$. \\
\STATE {\bf Sample the next state:}
\\
\quad (continuous) $\boldsymbol{x}_{t+\Delta t} =\gamma_t \boldsymbol{g}+ c_{t,1} \boldsymbol{x}_t + c_{t,2} \hat{\boldsymbol{x}}_1 + \sigma_t \epsilon $ with $\epsilon \sim \mathcal{N}\rbr{\boldsymbol{0},\boldsymbol{I}}$.\\
\quad(discrete) ${x}_{t+\Delta t}^{(d)} \sim $ 


\quad$ \text{Cat}\rbr{\delta\{x_t^{(d)}, j \} + \exp(\gamma_t\boldsymbol{g}^{(d)}) \odot R^{(d)}_{\theta, t}\rbr{\boldsymbol{x}_t, j } \Delta t }$.  \\
\STATE {\bf Output:} { $\boldsymbol{x}_{t + \Delta t}$}
\end{algorithmic}
\caption{\texttt{BranchOut}-Gradient}
\label{mol:grad guidance}
\end{module}

\cref{alg:framework} using \texttt{BranchOut}-Gradient with $K=1$ reduces to gradient-based guidance methods. When $K > 1$, it is compatible with Value Function~\ref{vfunc:noisy}. We refer to this algorithm as \textbf{\ouralg Gradient (\xtgrad)}.









\subsection{Analysis of \ouralg} 
\label{subsec:analysis}
\paragraph{Generalizability of \ouralg.} When the branch out size $K=1$ meaning there is no branching and all paths remain independent, \xtsampling reduces to  Best-of-N  \cite{stiennon2020learning, nakano2021webgpt}.
When the size of the active set $A=1$, \xtsampling recovers the rule-based guidance in \citet{huang2024symbolic} for continuous diffusion models. When $A=1, K=1$ with \texttt{BranchOut}-Gradient, it reduces to gradient-based guidance in continuous diffusion \citep{chung2022diffusion,song2023loss}.


\paragraph{Computational Complexity of \ouralg.} To analyze the computational complexity of proposed algorithms, we define the following units of computation: 

\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt]
    \item $C_{\text{model}}$: the computational cost of passing through the diffusion or flow model.
    \item $C_{\text{pred}}$: the cost of calling the predictor.
    \item $C_{\text{backprop}}$: backpropagation through the diffusion model and predictor.
\end{itemize}

Let $N$ denote the Monte Carlo sample size used in Value Function~\ref{vfunc:noisy}.
Recall $A$ is the active size and $K$  is the branch-out size. The computation complexity of \ouralg is summarized in \cref{tab:computation complexity}.



\begin{table}[htb]
% \vspace{-6pt}
    \centering
    \begin{tabular}{lc}
    \toprule
   Methods & Computation \\
    \midrule
    \xtsampling  & $AC_{\text{model}}+AK(C_{\text{model}}+ NC_{\text{pred}})$  \\
    \xcleansampling & $AC_{\text{model}}+AKC_{\text{pred}}$\\
    \xtgrad & $ AK\rbr{C_{\text{model}}+ NC_{\text{pred}}}+AC_{\text{backprop}}$ \\
    \bottomrule
    \end{tabular}
    \caption{Computation complexity of \ouralg}
    \label{tab:computation complexity}
    \vspace{-20pt} 
\end{table}


Notice that the forward pass cost for the diffusion model in \xcleansampling is only $A$, compared to $AK$ for the other two methods. This is because it branches out at $\hat{\boldsymbol{x}}_1$ and directly evaluates $\hat{\boldsymbol{x}}_1$, eliminating the need to pass through the diffusion model. Thus, for the same $A, K$, \xcleansampling is more efficient.

