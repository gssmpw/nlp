\section{Introduction}\label{sec:intro}


\begin{figure*}
    \centering
    \subfigure[Tree-Search Inference Process]{\includegraphics[width=0.52\linewidth]{icml2025/figures/inference_process.pdf}}
    \hspace{0.02\linewidth} 
    \subfigure[ Designs for \texttt{BranchOut} and Value Function $V$
    ]{\includegraphics[width=0.39\linewidth]{icml2025/figures/branch_out_block.pdf}}
    \caption{Illustration of \ouralg: (a) An \textbf{active set} of size $A$ is maintained, where each sample branches into $K$ candidates. At each step, the top $A$ candidates are selected, repeating until the final step where the best sample from the active set is chosen as the output. (b) Left: The \textbf{current state}-based (\texttt{BranchOut}, $V$) evaluates candidates via a lookahead estimate of the clean sample (\cref{subsec:xtsampling}). Right: The \textbf{destination state}-based design generates multiple predicted destination states and selects the optima (\cref{subsec:x1sampling}). The high-value destination state determines the next state.
    Gradient-based guidance can be applied to the current state-based branch-out module when a differentiable objective predictor is available (\cref{subsec:xtgrad}).
    }
    \label{fig:inference demonstration}
\end{figure*}















During the inference process of diffusion and flow models, guidance methods enable users to steer model generations toward desired objectives, achieving remarkable success across diverse domains such as vision \citep{dhariwal2021diffusion,ho2022classifier}, audio \citep{kim2022guided}, biology \citep{nisonoff2024unlocking,zhang2024generalized}, and decision making \cite{ajay2022conditional,chi2023diffusion}. 
In particular, training-free guidance offers high applicability by directly controlling the generation process with off-the-shelf objective functions without requiring additional training \citep{song2023loss, zhao2024mitigating,bansal2023universal, he2023manifold}. Most training-free guidance methods are gradient-based, as the gradient of the objective indicates a good direction for steering the inference \citep{ye2024tfg, guo2024gradient}, as a result, they rely on the assumptions that the objective function is differentiable. 

However, recent advancements in generative models have broadened the playground of guided generation beyond differentiability: objectives of guidance have been enriched to include non-differentiable goals \citep{huang2024symbolic, ajay2022conditional}; diffusion and flow models have demonstrated effectiveness in modeling discrete data \citep{austin2021structured, vignac2022digress} for which objectives are inherently non-differentiable unless derived from differentiable features \citep{li2015improving, yap2011padel}. In those scenarios, those training-free guidance methods that are originally designed for differentiable objectives are limited by their assumptions. Yet, the design space beyond differentiability remains under-explored: only few methods exist \citep{lin2025tfgflow, huang2024symbolic}, they are fundamentally different from each other and appear to be disconnected from the fundamental principles in previous guidance designs \citep{lin2025tfgflow, chung2022diffusion}. Thus, it highlights the need of a unified perspective of and a comprehensive study on the design space of guidance beyond differentiability.
To address this challenge, we propose an algorithmic framework \textbf{\ouralg}: \underline{Tree} Search-Based Path Steering \underline{G}uidance, designed for both diffusion and flow models across continuous and discrete data spaces. \ouralg consists of path steering guidance and a tree search mechanism.









\textbf{Path steering guidance provides a unified perspective on training-free guidance beyond differentiability.} Suppose $$\boldsymbol{x}_0, \cdots, \boldsymbol{x}_t, \boldsymbol{x}_{t+\Delta t}, \cdots, \boldsymbol{x}_1$$ denotes the inference process of a diffusion/flow model. While the gradient of the objective, when available, can offer a precise direction to steer inference at each step, an alternative is to discover a good inference path through a structured search process: proposing multiple $\boldsymbol{x}_{t+\Delta t}$'s as candidates for the next step (handled by a module \texttt{BranchOut}), evaluating them using some value function (denoted by $V$) that reflects the objective, and selecting the best candidate to move forward. The search procedure in \ouralg is applicable beyond the differentiability assumption as it evaluates candidates with the zeroth-order information from the objective. 



 \textbf{\ouralg adopts a tree-search mechanism that enables search across multiple trajectories, further enhancing the performance of path steering guidance.} An \textbf{\textit{active set}} of size $A$ is maintained at each inference step, as shown in \cref{fig:inference demonstration}(a). Each sample in the active set branches into multiple candidates next states, from which the top $A$ candidates—ranked by the value function $V$—are selected for the next step. This process iterates until the final step, where the best sample from the active set is chosen as the output. By scaling up the active set size $A$, \ouralg can further improve objective function values as needed while adapting to the available computational budget. 
 
 
\textbf{The design space of \ouralg is over the branching-out module and the value function, and \ouralg is equipped with comprehensive design options.} To ensure an effective search,  two key considerations are efficient exploration and reliable evaluation. As illustrated in \cref{fig:inference demonstration}(b), we propose two compatible pairs of $(\texttt{BranchOut}, V)$, based on either the current state ($\boldsymbol{x}_t$), or the predicted destination state ($\hat{\boldsymbol{x}}_1$). The former uses the original diffusion model to generate multiple next states and employs a lookahead estimate of the clean sample for evaluation. The latter proposes multiple predicted destination states, which indicate the orientation of the next state, and selects the optimal using an off-the-shelf objective function. %\huicomment{our two designs make it clear on link to between exsisting methods to previous guidance design} 
The next state is determined by the high-value destination state. 
In addition, \ouralg introduces a novel gradient-based algorithm that enables the use of gradients to guide discrete flow models when a differentiable objective predictor is available.




\begin{figure*}
    \centering
    \subfigure[Symbolic Music Generation (Loss $\downarrow$)]{\includegraphics[width=0.3\linewidth]{icml2025/figures/music/nd/gs_nd_time.pdf}}
    \hspace{0.02\linewidth}
     \subfigure[Small Molecule Generation (MAE $\downarrow$)]{\includegraphics[width=0.3\linewidth]{icml2025/figures/molecule/ylog_hull_1_2_4_8_16_num_rings_0.pdf}}
    \hspace{0.02\linewidth}
    \subfigure[Enhancer DNA Design (Prob $\uparrow$)]{\includegraphics[width=0.3\linewidth]{icml2025/figures/enhancer/xt_grad_mcts/time_loss/class_0/time_prob_temp_0.005.pdf}}
     \caption{\textbf{\ouralg outperforms the guidance baseline, with the optimization effect of the objective function following a scaling law with inference time.} We compare \ouralg\ with the strongest guidance baseline across tasks: \citet{huang2024symbolic} for music generation and \citet{nisonoff2024unlocking} for molecule and enhancer DNA tasks. Music generation involves a \textbf{\textit{non-differentiable}} note density objective, while molecule and DNA tasks use \textbf{\textit{discrete}} flow models. We evaluate \ouralg with varying active set and branch-out sizes. \textbf{``Optimal"} refers to  to the combination of active set and branch out sample sizes that achieves the best performance within the same inference time. The optimal line shows that \ouralg outperforms the guidance baseline with a similar inference time. 
     % It also surpasses cases where only the active set or branch-out size is increased, highlighting the effectiveness of the tree mechanism (active set $>$ 1) and the branch-out and selection operations, validating \ouralg's design.
     }
    \label{fig:results overview}
\end{figure*}



The contributions of our paper are:
\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt, leftmargin=5pt]
\item We propose a novel framework \ouralg of training-free guidance based on inference path search, applicable to both continuous and discrete, diffusion and flow models (\cref{sec:propose sps}). Our novel instantiated algorithms of \ouralg (\cref{sec: design space}) tackle non-differentiability challenges from non-differentiable objectives or discrete-space models.

\item We benchmark \ouralg against existing guidance methods on three tasks: symbolic music generation (continuous diffusion with {\textit{non-differentiable}} objectives), molecular design, and enhancer DNA design (both on {\textit{discrete}} flow models). 
Path steering guidance, the special case of \ouralg with the active set size 1, consistently outperforms the strongest guidance baseline (\cref{fig:results overview} and \cref{sec: exp guidance results}). Our framework offers a suite of guidance options, with empirical results across three tasks suggesting that each task benefits the most from a guidance design that fits the nature of its objective and the underlying diffusion or flow model (\cref{sec: exp discuss guidance design}).


    
\item We discover an {inference-time scaling law} of \ouralg, where performance gain of \ouralg is observed as inference-time computation scales up with increasing active set and branch-out sizes (\cref{fig:results overview} and \cref{sec: exp scalabiliity}). 

\end{itemize}



