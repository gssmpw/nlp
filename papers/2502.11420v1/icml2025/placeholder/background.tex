\vspace{-10pt}
\section{Preliminaries}
\paragraph{Notations.} Bold notation \( \boldsymbol{x} \) denotes a high-dimensional vector, while \( x \) represents a scalar. The superscript notation \( x^{(d)} \) indicates the \( d \)-th dimension of the vector. In contrast, \( x^i \) or \( x^{i,j} \) denote independent samples with indices \( i \) and \( j \). $p_t$ is the density of intermediate distributions in training. For inference, $\mathcal{T}_t$ represents the sample distribution density or the rate matrix in the discrete case.

\subsection{Diffusion and Flow Models.} Diffusion and flow models are trained by transforming the target data distribution into a prior noise distribution and learning to reverse the process. Let the data distribution be denoted as $p_{\text{data}}$, with $p_1 = p_{\text{data}}$. During training, a sequence of intermediate distributions $p_t (t \in (0,1))$ is constructed to progressively transform the data distribution $p_1$ into a noise distribution $p_0$. Assume the total number of timesteps is $T$, resulting in a uniform interval $\Delta t = 1/T$. The generation process begins with sampling $\boldsymbol{x}_0 \sim p_0$. The model then iteratively generates $\boldsymbol{x}_t \sim p_t$ for timestep $t = i/T,$ where $ i\in [T]$,  ultimately producing $\boldsymbol{x}_1$ from the desired data distribution. 




Diffusion and flow models are equivalent \citep{lipman2024flow, domingo2024adjoint}. This paper focuses on the widely used continuous diffusion models and discrete flow models for continuous and discrete data, respectively, denoted as $u_\theta^{\textit{diff}}$ and $u_\theta^{\textit{flow}}$, abbreviated as $u_\theta$ when the context is clear. We collectively refer to the diffusion and flow models as the diffusion model. Below, we provide more detailed preliminaries on the two models.




\paragraph{Diffusion Models.} For diffusion models applied to continuous data, given a data sample $\boldsymbol{x}_1 \sim p_1 $, the noisy sample at timestep $t=i /T$ ($i \in [T]$) is constructed as $\boldsymbol{x}_t = \sqrt{\bar{\alpha}_t}\boldsymbol{x}_1 + \sqrt{1-\bar{\alpha}_t}\epsilon $, where $\epsilon \sim  \mathcal{N}\rbr{\boldsymbol{0}, \boldsymbol{I}}$ and $\cbr{\bar{\alpha}_t} $ are pre-defined monotonically increasing parameters that control the noise level. The diffusion model  $u_\theta^{\textit{diff}}: \mathcal{X}\times [0,1] \to \mathcal{X}$ parameterized by $\theta$, estimates the correspond clean state $\boldsymbol{x}_1$ of $\boldsymbol{x}_t$. The training objective is \citep{ho2020denoising}:
\begin{equation}\label{eq:diffusion train obj}
    u_\theta^{\textit{diff}} = \argmin_{u_\theta} \EE_{\boldsymbol{x}_1 \sim p_1, \epsilon } \norm{ u_\theta\rbr{\boldsymbol{x}_t,t}- \boldsymbol{x}_1 }^2 = \EE[\boldsymbol{x}_1 \mid \boldsymbol{x}_t].
\end{equation}
For sampling, we begin with $\boldsymbol{x}_0 \sim \mathcal{N}\rbr{\boldsymbol{0}, \boldsymbol{I}}$  and iteratively sample $\boldsymbol{x}_{t+\Delta t} \sim \mathcal{T}(\boldsymbol{x}_{t+\Delta t}\mid \boldsymbol{x}_{t})$. The sampling step proposed by DDPM \citep{ho2020denoising} is:
\begin{equation}\label{eq:diffusion ddpm sampling}
    \boldsymbol{x}_{t+\Delta t}  = c_{t,1} \boldsymbol{x}_t + c_{t,2} u_\theta^{\textit{diff}}\rbr{\boldsymbol{x}_t,t}  + \sigma_t \epsilon,
    % \frac{1}{\sqrt{\alpha_t}}\rbr{\boldsymbol{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}u_\theta^{\textit{diff}}\rbr{\boldsymbol{x}_t,t} } + \sigma_t \epsilon,
\end{equation}
where $\epsilon \sim \mathcal{N}\rbr{\boldsymbol{0}, \boldsymbol{I}}$, $c_{t,1}= \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t+\Delta t})}{1-\bar{\alpha}_t},  c_{t,2} = \frac{\sqrt{\bar{\alpha}_{t+\Delta t}}(1-\alpha_t)}{1-\bar{\alpha}_t}$ with $\alpha_t = \bar{\alpha}_t/\bar{\alpha}_{t+\Delta t}$ and $\sigma_t = \sqrt{1-\alpha_t}$.



\paragraph{Flow Models.} For flow models applied to discrete data, we follow the framework by \citet{campbell2024generative}. Suppose the discrete data space is $\mathcal{X}  = [S]^D$, where $D$ is the dimension and $S$ is the number of states per dimension.  An additional mask state $M$ is introduced as the noise prior distribution. Given a data sample $\boldsymbol{x}_1$, the intermediate distributions are constructed by $p_{t|1}(\boldsymbol{x}_t| \boldsymbol{x}_1) = \Pi_{d=1}^D p_{t|1}(x_t|x_1) $ with $p_{t|1}(x_t|x_1) = t \delta \cbr{x_1, x_t} + (1-t)\delta \cbr{M, x_t} $. The flow model $u_\theta^{\textit{flow}}$ estimates the true denoising distribution $p_{1|t}(\boldsymbol{x}_1|\boldsymbol{x}_t)$. Specifically, it's defined as $u_\theta^{\textit{flow}} = (u_\theta^{(1)},\ldots,u_\theta^{(D)})$, where each component $u_\theta^{(d)}(x_1|\cdot)$ is a function $ \mathcal{X} \times [0,1]\to \Delta([S]) $. Here, $\Delta([S])$ represents the probability distribution over the set $[S]$ The training objective is:
\begin{equation}\label{eq:flow train obj}
   u_\theta^{\textit{flow}}= \argmin_{u_\theta} \EE_{\boldsymbol{x}_1 \sim p_1, \boldsymbol{x}_t \sim p_{t|1}}\sbr{\log u_{\theta}^{(d)}(x_1^{(d)}|\boldsymbol{x}_t)}.
\end{equation}
For generation, it requires the rate matrix:
\begin{equation}\label{eq:compute rate matrix}
     R_{\theta,t}^{(d)} \rbr{\boldsymbol{x}_t, j} = \EE_{{x}_1^{(d)} \sim u_{\theta}^{(d)}\rbr{{x}_1\mid \boldsymbol{x}_t}}\sbr{R_t\rbr{x_t^{(d)}, j|x_1^{(d)}}},
\end{equation}
where the pre-defined conditional rate matrix can be chosen as the popular: $R_t(x_t,j|x_1) = \frac{\delta \cbr{j,x_1}}{1-t} \cbr{x_t, M}$. The generation process can be simulated via Euler steps \citep{sun2022score}:
\begin{equation}\label{eq:discrete euler sampling}
    {x}_{t+\Delta t}^{(d)} \sim  \text{Cat}\rbr{\delta \{{x}_t^{(d)}, j \} + R_{\theta, t}^{(d)}\rbr{\boldsymbol{x}_t, j} \Delta t },
\end{equation}
where $ \delta\cbr{k, j}$  is the Kronecker delta which is $1$ when $k = j$ and is otherwise $0$.






\subsection{Training-free Guidance} 
The goal of training-free guidance is to enable conditional generation conditioned on some desired property. Suppose the property is evaluated by function $f$ and we consider both discrete and continuous $f$: $f(\boldsymbol{x})$ may represent discrete class label (e.g., $f$ is a classifier) or a real-valued output (e.g., $f$ a regression model or a zeroth-order oracle). Given a user-specified target $y$, the objective function $f_y$ quantifies how well the sample $ \boldsymbol{x}$ aligns with $y$ by
% We formulate the property as a function $f$, which evaluates any sample $\boldsymbol{x}$ and returns its function value $f(\boldsymbol{x})$. We consider the both discrete and continuous objective functions: $f(\boldsymbol{x})$ is class label (e.g. a classifier) or $f(\boldsymbol{x})$ is real-valued (e.g. a regression model or a zeroth-order oracle). The objective is to generate samples that has the target  $y$ that user specifies. Therefore, to measure how well the sample $\boldsymbol{x}$ aligns with the target $y$, we define a objective function $f_y(\boldsymbol{x})$ such that
\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt]
    \item When $f(\boldsymbol{x})$ is an off-the-shelf classifier, the objective function is $f_y(\boldsymbol{x}) := \log f(y\mid \boldsymbol{x})$.
    \item When $f(\boldsymbol{x})$ is a regression model or a zeroth-order oracle, the objective function is $f_y(\boldsymbol{x}):= \log \exp^{-\frac{(y-f(\boldsymbol{x}))^2}{2\sigma^2}} = -\frac{(y-f(\boldsymbol{x}))^2}{2\sigma^2}$, assuming the true value of $\boldsymbol{x}$ is distributed as Gaussian centered at $f(\boldsymbol{x})$ with $\sigma$ being a fixed constant.
\end{itemize}
Based on the definition of $f_y$, a higher value indicates a more desirable sample. We refer to $f_y$ as an objective predictor when it is a differentiable neural network.
We focus on \emph{\textbf{training-free}} methods, which do not involve post-training $u_{\theta}$ or training a time-dependent classifier compatible with the diffusion noise scheduling.



\subsection{Related Guidance Methods} We review the most related work to our paper here, please refer to \cref{app:related work} for other related works. 

\citet{nisonoff2024unlocking} 
and \citet{lin2025tfgflow} studies guidance for discrete flow models: the former follows classifier(-free) guidance method in \citet{ho2022classifier} and thus requires training time-dependent classifiers; the latter estimates the conditional rate by re-weighing the clean sample predictor $\boldsymbol{x}_1$ with their objective values in \eqref{eq:compute rate matrix}. \citep{huang2024symbolic} studies guiding continuous diffusion model with a non-differentiable objective and proposes a sampling approach, which is equivalent to our \xtsampling algorithm with the active set size $A = 1$. Though most guidance methods form a single inference path during generation, the idea of searching across multiple inference paths has also been touched upon by two concurrent works \citep{ma2025inference, uehara2025inference}. However, our \ouralg provides the first systematical study of the design space of tree search, offering a novel methodology for both exploration and evaluation.