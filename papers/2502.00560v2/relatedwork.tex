\section{Related Work}
\vspace{-0.1in}
% add papers on online regret minimizations for related work
\paragraph{2p0s games with incomplete information.}
Games where players have missing information only about the game types are often called \textit{incomplete-information} games. These games are a subset of imperfect-information games where nature plays a chance move at the beginning~\cite{harsanyi1967games}. The seminal work of \cite{aumann1995repeated} developed equilibrium strategies for a repeated and one-sided setting of such games through the ``Cav u'' theorem, which relates the value of the game with that of a \textit{non-revealing} version of the game where both players only know the distribution of the game type. Briefly, the ``Cav u'' theorem reveals that belief-manipulating behavioral strategies are necessary to achieve value convexification and thus the equilibrium. As we will discuss, this theorem plays an important role in enabling scalable solve of games with continuous action spaces.
% Within the same framework, Blackwell's approachability theorem~\citep{blackwell1956analog} naturally becomes the theoretical support for the optimal strategy of the uninformed player. 
Building on top of \cite{aumann1995repeated}, \cite{de1996repeated} introduced a dual game in which the behavioral strategy of the uninformed player becomes Markov. This technique helped \cite{cardaliaguet2007differential,ghimire24a} to establish the value existence proof for 2p0s differential games with incomplete information with and without state constraints. Unlike repeated games where belief manipulation occurs only in the first round of the game, differential games may have multiple critical time-state-belief points where belief manipulation is required to achieve equilibrium, depending on the specifications of system dynamics, payoffs, and state constraints~\citep{ghimire24a}. 
% For this reason, scalabily solving 2p0s differential games with incomplete information has not yet been achieved.
\vspace{-0.12in}
\paragraph{IIEFGs.} IIEFGs represent the more general set of multi-agent decision-making problems with finite horizons. Since any 2p0s IIEFG with finite action sets has a normal-form formulation, a unique Nash equilibrium always exists in the space of mixed strategies. Significant efforts have been taken to approximate equilibrium of large IIEFGs~\citep{koller1992complexity, billings2003approximating, gilpin2006finding, gilpin2007gradient, sandholm2010state, pluribus} leading to algorithms that are no-regret and with sublinear or linear convergence rates~\citep{zinkevich2007regret, abernethy2011blackwell, pmlr-v15-mcmahan11b, tammelin2014solving, johanson2012finding, lanctot2009monte, brown2019deep, brown2020combining, perolat2021poincare, sokota2022unified, stratego, sog} (see summary in Tab.~\ref{tab:complexity}). Notably, these algorithms have computational complexities increasing with the action space size $U$, provided that the equilibrium behavioral strategy lies in the interior of the simplex $\Delta(U)$ (see discussion in Appendix~\ref{sec:complexity_existing_algos}). Critically, this assumption does not hold for differential games equipped with the Isaacs' condition, in which case the equilibrium strategy is mostly pure along the game tree, and is atomic on the action space $\mathcal{U}$ when mixed, as we explain in Sec.~\ref{sec:splitting}. While studies on continuous action normal- and extensive-form games exist~\citep{martin2024joint, martin2023finding}, 
% with an aim to minimize computational cost for solving games with large number of players. These 
these methods are restricted to a class of games that either admit a pseudoconcave potential or are monotone. 
\vspace{-0.15in}
\begin{table}[h!]
    \centering
        \caption{Solver computational complexity (best case) with respect to action space $\mathcal{A}$ and equilibrium error $\varepsilon$}
        % \vspace{-0.1in}
    \begin{tabularx}{\linewidth}{ X | p{0.24\linewidth} }
    \hline
         Algorithm & Complexity \\
         \hline
         CFR variants~\citep{zinkevich2007regret, lanctot2009monte, brown2019deep, tammelin2014solving,johanson2012finding} & $\mathcal{O}(\textcolor{red}{U}\varepsilon^{-2})$ to $\varepsilon$-Nash \\
         FTRL variants \& MMD ~\citep{pmlr-v15-mcmahan11b, perolat2021poincare, sokota2022unified} & $\mathcal{O}\left(\frac{\ln(\textcolor{red}{U})}{\varepsilon}\ln\left(\frac{1}{\varepsilon}\right)\right)$ to $\varepsilon$-QRE\\
    \hline
    \end{tabularx}
    \label{tab:complexity}
    \vspace{-0.2in}
\end{table} 
\vspace{-0.1in}
\paragraph{Descent-ascent algorithms for nonconvex-nonconcave minimax problems.} Existing developments in IIEFGs focused on convex-concave minimax problems due to the bilinear form of the expected payoff through the conversion of games to their normal forms. This paper, on the other hand, investigates the nonconvex-nonconcave minimax problems to be solved at every infostate when actions are considered continuous. 
% In general, solvers for nonconvex-nonconcave problems have worse convergence rates than those for convex-concave ones. 
To this end, we use the doubly smoothed gradient descent ascent method (DS-GDA) which has a worst-case complexity of $\mathcal{O}(\varepsilon^{-4})$
% and an optimal complexity of $\mathcal{O}(\varepsilon^{-2})$
~\citep{zheng2023universal}.
\vspace{-0.3in}
\paragraph{Multigrid methods for accelerating value approximation.} 
Multigrid methods~\citep{trottenberg2000multigrid} are widely used to accelerate PDE solving on a mesh (e.g., fluid mechanics). In a typical V-cycle~\cite{braess1983new}, a few iterations of relaxation (e.g., Gauss-Seidel) are first performed on a fine mesh, and the resulting residual is restricted to a coarser mesh, where a PDE correction is solved and prolongated to the fine mesh. Essentially, the V-cycle uses a coarse solve to reduce the low-frequency approximation error in the PDE solution at a low cost, leaving only the high-frequency errors to be resolved through the fine mesh and resulting in faster solution convergence than conventional PDE solvers. 
% In the context of optimal control and differential games, 
Multigrid has been successfully applied to solving Hamilton-Jacobi-Bellman (HJB) and Hamilton-Jacobi-Isaacs (HJI) equations~\cite{han2013multigrid} for optimal control problems and differential games. 
% As we discuss in the next section, solving differential game requires performing backward-induction which grows linearly with the time-discretization $\tau$. 
Nonetheless, extending multigrid to incomplete-information differential games and value approximation based on neural nets has rarely been discussed. 
% We use a two-level multigrid to reduce the computational time invested in the value approximation. 
% In this paper we develop an algorithm for $n$-level multigrid which can accelerate value approximation when finer time discretization is required. More details on multigrid can be found in App.~\ref{app:multigrid}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%