%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{hyperref}
\usepackage{url}
\usepackage[subpreambles=true]{standalone}
\usepackage{enumerate}
\usepackage{wrapfig}
\usepackage[ruled, noend]{algorithm2e}
\usepackage{nicematrix}
\usepackage{caption}
\usepackage{pdfpages}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tabularx}
\usepackage{enumitem}

\usepackage{subcaption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Scalable Solver for 2p0s Differential Games with One-Sided Payoff Information and Continuous Actions, States, and Time}


\begin{document}

\twocolumn[
\icmltitle{A Scalable Solver for 2p0s Differential Games with One-Sided Payoff Information and Continuous Actions, States, and Time}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mukesh Ghimire}{sch}
\icmlauthor{Lei Zhang}{sch}
\icmlauthor{Zhe Xu}{sch}
\icmlauthor{Yi Ren}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{sch}{Arizona State University, Tempe, AZ, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Mukesh Ghimire}{mghimire@asu.edu}
\icmlcorrespondingauthor{Yi Ren}{yiren@asu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Game Theory, Incomplete Information Games}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\begin{abstract}
Existing solvers for imperfect-information extensive-form games (IIEFGs) often struggle with scalability in terms of action and state space sizes and the number of time steps. However, many real-world games involve continuous action and state spaces and occur in continuous time, making them differential in nature. This paper addresses the scalability challenges for a representative class of two-player zero-sum (2p0s) differential games where the informed player knows the game type (payoff) while the uninformed one only has a prior belief over the set of possible types. Such games encompass a wide range of attack-defense scenarios, where the defender adapts based on their belief about the attacker's target. We make the following contributions: (1) We show that under the Isaacs' condition, the complexity of computing the Nash equilibrium for these games is not related to the action space size; and (2) we propose a multigrid approach to effectively reduce the cost of these games when many time steps are involved. Code for this work is available at \href{https://github.com/ghimiremukesh/cams/tree/conf_sub}{github}.
\end{abstract}

\section{Introduction}
The strength of game solvers has grown rapidly in the last decade, beating elite-level human players in Chess~\citep{alphazero}, Go~\citep{alphago}, Poker~\citep{pluribus, rebel}, Diplomacy~\citep{diplomacy}, Stratego~\citep{stratego}, among others with increasing complexity. 
% These successes motivated recent interests in solving differential games in continuous time and space, e.g., competitive sports~\citep{tacticai, ghimire24a}, where critical strategic plays should be executed precisely within the continuous action space and at specific moments in time (e.g., consider set piece scenarios in soccer).
Most of the existing solvers with proved convergence, e.g., CFR+ variants~\citep{tammelin2014solving,burch2014solving, moravvcik2017deepstack,rebel,lanctot2009monte}, FTRL variants~\citep{pmlr-v15-mcmahan11b, perolat2021poincare}, and mirror descent variants~\citep{sokota2022unified, cen2021fast, vieillard2020leverage}, are designed for games with finite action and state sets, and have computational complexities increasing along the sizes of these sets. 
Real-world imperfect-information games, however, can often have continuous action and state spaces and happen in continuous time, making them differential in nature. 
Directly applying the existing solvers to these differential games would require either insightful action-state-time abstraction or enormous compute. Neither are readily available.
% While most of the solution concepts can be applied here, the complexities grow exponentially due to continuous states and actions. Therefore, a scalable algorithm is imperative in solving differential games, especially with lack of information.
% \vspace{-0.1in}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/overview_fig_new_color.pdf}
    \vspace{-0.3in}
    \caption{\textbf{(a)} We explain the atomic nature of equilibrium strategies in the games of our interest. Exploiting this nature allows us to tractably solve games with continuous action spaces. \textbf{(b)} Sample equilibrium state trajectories of a 2p0s differential game where P2 guesses P1's target (magenta circles). P1's optimal strategy is to reveal his target after a critical time $t_r$. \textbf{(c)} Illustration of a 2-level multigrid solver. Fine grid errors are restricted to the coarse grid, where cheap corrections are computed and prolongated to the fine grid. \textbf{(d)} Multigrid further accelerates value approximation for games with various number of time steps.}
    \label{fig:overview_fig}
    \vspace{-0.25in}
\end{figure}

This paper addresses this scalability challenge for a representative subset of 2p0s differential games where the informed player knows the game type (payoff) while the uniformed player only has a prior belief $p_0 \in \Delta(I)$ over a set of $I$ possible types. We also assume that the Isaacs' condition holds, i.e., the complete-information version of the game has a pure Nash equilibrium. This condition commonly holds for differential games with control-affine dynamics. While restricted, such games represent a wide range of attack-defense scenarios that can be described as follows: At the beginning of the game, nature draws a game type according to $p_0$ and informs the informed player (P1) about the type. As the game progresses, the belief about the true game type, which is assumed to be public knowledge, is updated from $p_0$ based on the action sequence taken by P1 and his public strategy profile via the Bayes' rule. P1's (resp. P2's) goal is to minimize (resp. maximize) the expected payoff over $p_0$. 
% This game is proved to have a value under Isaacs' condition~\citep{cardaliaguet2009numerical}. 
Due to the zero-sum nature, P1 may need to delay information release or manipulate P2's belief to take full advantage of information asymmetry; and P2's strategy is to minimize a worst-case risk. 
Some real-world examples of the game include football set-pieces where the attacker has private information about which play is to be executed, and missile defense where multiple potential targets are concerned. The setting of one-sided information, i.e., P1 knows everything about P2, is necessary for P2 to derive defense strategies in risk-sensitive games. 

We claim the following contributions: 
\vspace{-0.1in}
\begin{itemize}[noitemsep]
\vspace{-0.05in}
    \item We explain that the computational complexity for approximating the Nash equilibrium for the games of our interest is related to the number of game types ($I$) rather than to the action space size.
    \item We explain that the equilibrium values for the informed and uninformed players can be computed via two separate backward induction processes through a primal-dual formulation of the game.
    \item We propose a multigrid approach to tractably solve games with continuous state spaces and many time steps. Empirical results show that our solver outperforms SOTA IIEFG solvers including CFR+~\citep{tammelin2014solving}, MMD~\citep{sokota2022unified}, Deep-CFR~\citep{brown2019deep}, and a SOTA continuous-action solver JPSPG~\citep{martin2024joint}, on games of our interest. Our solver also approximates reasonable strategies for game settings that are intractable for SOTA solvers.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.15in}
\section{Related Work}
\vspace{-0.1in}
% add papers on online regret minimizations for related work
\paragraph{2p0s games with incomplete information.}
Games where players have missing information only about the game types are often called \textit{incomplete-information} games. These games are a subset of imperfect-information games where nature plays a chance move at the beginning~\cite{harsanyi1967games}. The seminal work of \cite{aumann1995repeated} developed equilibrium strategies for a repeated and one-sided setting of such games through the ``Cav u'' theorem, which relates the value of the game with that of a \textit{non-revealing} version of the game where both players only know the distribution of the game type. Briefly, the ``Cav u'' theorem reveals that belief-manipulating behavioral strategies are necessary to achieve value convexification and thus the equilibrium. As we will discuss, this theorem plays an important role in enabling scalable solve of games with continuous action spaces.
% Within the same framework, Blackwell's approachability theorem~\citep{blackwell1956analog} naturally becomes the theoretical support for the optimal strategy of the uninformed player. 
Building on top of \cite{aumann1995repeated}, \cite{de1996repeated} introduced a dual game in which the behavioral strategy of the uninformed player becomes Markov. This technique helped \cite{cardaliaguet2007differential,ghimire24a} to establish the value existence proof for 2p0s differential games with incomplete information with and without state constraints. Unlike repeated games where belief manipulation occurs only in the first round of the game, differential games may have multiple critical time-state-belief points where belief manipulation is required to achieve equilibrium, depending on the specifications of system dynamics, payoffs, and state constraints~\citep{ghimire24a}. 
% For this reason, scalabily solving 2p0s differential games with incomplete information has not yet been achieved.
\vspace{-0.12in}
\paragraph{IIEFGs.} IIEFGs represent the more general set of multi-agent decision-making problems with finite horizons. Since any 2p0s IIEFG with finite action sets has a normal-form formulation, a unique Nash equilibrium always exists in the space of mixed strategies. Significant efforts have been taken to approximate equilibrium of large IIEFGs~\citep{koller1992complexity, billings2003approximating, gilpin2006finding, gilpin2007gradient, sandholm2010state, pluribus} leading to algorithms that are no-regret and with sublinear or linear convergence rates~\citep{zinkevich2007regret, abernethy2011blackwell, pmlr-v15-mcmahan11b, tammelin2014solving, johanson2012finding, lanctot2009monte, brown2019deep, brown2020combining, perolat2021poincare, sokota2022unified, stratego, sog} (see summary in Tab.~\ref{tab:complexity}). Notably, these algorithms have computational complexities increasing with the action space size $U$, provided that the equilibrium behavioral strategy lies in the interior of the simplex $\Delta(U)$ (see discussion in Appendix~\ref{sec:complexity_existing_algos}). Critically, this assumption does not hold for differential games equipped with the Isaacs' condition, in which case the equilibrium strategy is mostly pure along the game tree, and is atomic on the action space $\mathcal{U}$ when mixed, as we explain in Sec.~\ref{sec:splitting}. While studies on continuous action normal- and extensive-form games exist~\citep{martin2024joint, martin2023finding}, 
% with an aim to minimize computational cost for solving games with large number of players. These 
these methods are restricted to a class of games that either admit a pseudoconcave potential or are monotone. 
\vspace{-0.15in}
\begin{table}[h!]
    \centering
        \caption{Solver computational complexity (best case) with respect to action space $\mathcal{A}$ and equilibrium error $\varepsilon$}
        % \vspace{-0.1in}
    \begin{tabularx}{\linewidth}{ X | p{0.24\linewidth} }
    \hline
         Algorithm & Complexity \\
         \hline
         CFR variants~\citep{zinkevich2007regret, lanctot2009monte, brown2019deep, tammelin2014solving,johanson2012finding} & $\mathcal{O}(\textcolor{red}{U}\varepsilon^{-2})$ to $\varepsilon$-Nash \\
         FTRL variants \& MMD ~\citep{pmlr-v15-mcmahan11b, perolat2021poincare, sokota2022unified} & $\mathcal{O}\left(\frac{\ln(\textcolor{red}{U})}{\varepsilon}\ln\left(\frac{1}{\varepsilon}\right)\right)$ to $\varepsilon$-QRE\\
    \hline
    \end{tabularx}
    \label{tab:complexity}
    \vspace{-0.2in}
\end{table} 
\vspace{-0.1in}
\paragraph{Descent-ascent algorithms for nonconvex-nonconcave minimax problems.} Existing developments in IIEFGs focused on convex-concave minimax problems due to the bilinear form of the expected payoff through the conversion of games to their normal forms. This paper, on the other hand, investigates the nonconvex-nonconcave minimax problems to be solved at every infostate when actions are considered continuous. 
% In general, solvers for nonconvex-nonconcave problems have worse convergence rates than those for convex-concave ones. 
To this end, we use the doubly smoothed gradient descent ascent method (DS-GDA) which has a worst-case complexity of $\mathcal{O}(\varepsilon^{-4})$
% and an optimal complexity of $\mathcal{O}(\varepsilon^{-2})$
~\citep{zheng2023universal}.
\vspace{-0.3in}
\paragraph{Multigrid methods for accelerating value approximation.} 
Multigrid methods~\citep{trottenberg2000multigrid} are widely used to accelerate PDE solving on a mesh (e.g., fluid mechanics). In a typical V-cycle~\cite{braess1983new}, a few iterations of relaxation (e.g., Gauss-Seidel) are first performed on a fine mesh, and the resulting residual is restricted to a coarser mesh, where a PDE correction is solved and prolongated to the fine mesh. Essentially, the V-cycle uses a coarse solve to reduce the low-frequency approximation error in the PDE solution at a low cost, leaving only the high-frequency errors to be resolved through the fine mesh and resulting in faster solution convergence than conventional PDE solvers. 
% In the context of optimal control and differential games, 
Multigrid has been successfully applied to solving Hamilton-Jacobi-Bellman (HJB) and Hamilton-Jacobi-Isaacs (HJI) equations~\cite{han2013multigrid} for optimal control problems and differential games. 
% As we discuss in the next section, solving differential game requires performing backward-induction which grows linearly with the time-discretization $\tau$. 
Nonetheless, extending multigrid to incomplete-information differential games and value approximation based on neural nets has rarely been discussed. 
% We use a two-level multigrid to reduce the computational time invested in the value approximation. 
% In this paper we develop an algorithm for $n$-level multigrid which can accelerate value approximation when finer time discretization is required. More details on multigrid can be found in App.~\ref{app:multigrid}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Notations}  % adding notations might be a good idea?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
\section{Problem Statement} \label{sec:diffgameintro}
\vspace{-0.1in}
\paragraph{Notations and preliminaries.} We denote by $\Delta(I)$ the simplex in $\mathbb{R}^I$, $[T] :=\{1,...,T\}$, $a[i]$ the $i$th element of vector $a$, $\partial_p V$ the subgradient of function $V$ with respect to $p$. Consider a time-invariant dynamical system that defines the evolution of the joint state $x \in \mathcal{X} \subseteq \mathbb{R}^{d_x}$ of P1 and P2 with control inputs $u \in \mathcal{U}$ and $v \in \mathcal{V}$, respectively:
\vspace{-0.05in}
\begin{equation}
    \dot{x}(t) = f(x(t), u, v).
    \label{eq:system}
\vspace{-0.05in}
\end{equation}
% Without loss of generality, we consider $f$ to be time-invariant. 
The game starts at $t_0 \in [0, T]$ from some initial state $x(t_0) =x_0$. The initial belief $p_0 \in \Delta(I)$ is set to nature's distribution about the game type. P1 of type $i$ accumulates a running cost $l_i(u, v)$ during the game and receives a terminal cost $g_i(x(T))$. The goal of P1 is to minimize the expected sum of the running and terminal costs, which P2 maximizes. 

Denote by $\{\mathcal{H}^i_r(t)\}^I$ the joint sets of behavioral strategies of P1, and $\mathcal{Z}_r(t)$ the set of behavioral strategies of P2. P1 chooses his strategy $\eta_i \in \mathcal{H}_r^i(t)$ according to his type $i$, while P2's strategy $\zeta \in \mathcal{Z}_r(t)$ is independent of $i$. At any game tree node $(t,x,p) \in [0,T] \times \mathcal{X} \times \Delta(I)$, $\eta_i$ (resp. $\zeta$) is a probability measure over $\mathcal{U}$ (resp. $\mathcal{V}$), and players move simultaneously.
% \footnote{More rigorously, we follow the common practice to consider strategies that are non-anticipative with delay (see \citep{cardaliaguet2007differential, elliott1972existence} for discussions on this choice).}.
With mild abuse of notation, let $(\eta(t), \zeta(t))$ be the random open-loop controls $(\alpha_{\omega}(t), \delta_{\omega}(t))$ induced by $(\eta, \zeta)$ and determined by the random seed $\omega$\footnote{Lem.~2.2 of \cite{cardaliaguet2007differential} proved the existence of $(\alpha_{\omega}(t), \delta_{\omega}(t))$ given $(\eta(t), \zeta(t))$.}.
$X_{t_1}^{t_0, x_0, \eta_i, \zeta}$ is then the random state arrived at $t_1$ from $(t_0, x_0)$ following $(\eta_i, \zeta)$ and the system dynamics in Eq.~\ref{eq:system}. The loss of P1 in a type-$i$ game is:
\vspace{-0.1in}
\begin{equation*}
    J_i(t_0,x_0;\eta_i,\zeta) := g_i\left(X_T^{t_0, x_0, \eta_i, \zeta}\right) + \int_{t_0}^T l_i(\eta_i(s), \zeta(s)) ds,
\end{equation*}
and the payoff over all game types is $J(t_0,x_0,p;\{\eta_i\},\zeta) = \mathbb{E}_{i\sim p}[J_i]$.
We say the game has a value $V$ if and only if the upper value $V^+(t_0, x_0, p) = \inf_{\{\eta_i\}} \sup_{\zeta} \mathbb{E}_{\eta_i, \zeta, i}[J_i]$
% \vspace{-0.1in}
% \begin{equation}\label{eq:upper_value}
% \small
%     \begin{aligned}
%         &V^+(t_0, x_0, p) = \inf_{\{\eta_i\} 
%         % \in \{\mathcal{H}_r(t_0)\}^I
%         } \sup_{\zeta 
%         % \in \mathcal{Z}_r(t_0)
%         } \mathbb{E}_{\eta_i, \zeta, i} \Big[g_i\left(X_T^{t_0, x_0, \eta_i, \zeta}\right) \\
%         & \hspace{1in}+ \int_{t_0}^T l_i(\eta_i(s), \zeta(s)) ds \Big] \\
%         % &= \sup_{\zeta \in \mathcal{Z}_r(t_0)} \inf_{\{\eta_i\} \in \{\mathcal{H}_r(t_0)\}^I}\mathbb{E}_{\eta_i, \zeta, i} \Big[g_i\left(X_T^{t_0, x_0, \eta_i, \zeta}\right) \\
%         % & \hspace{1in}+ \int_{t_0}^T l_i(\eta_i(s), \zeta(s)) ds \Big] \\
%     \end{aligned}
%     % \vspace{-0.2in}
% \end{equation}
and the lower value $V^-(t_0, x_0, p) = \sup_{\zeta}\inf_{\{\eta_i\}} \mathbb{E}_{\eta_i, \zeta, i}[J_i]$
% \vspace{-0.1in}
% \begin{equation}\label{eq:lower_value}
% \small
%     \begin{aligned}
%         &V^-(t_0, x_0, p) = \sup_{\zeta 
%         % \in \mathcal{Z}_r(t_0)
%         } \inf_{\{\eta_i\} 
%         % \in \{\mathcal{H}_r(t_0)\}^I
%         }\mathbb{E}_{\eta_i, \zeta, i} \Big[g_i\left(X_T^{t_0, x_0, \eta_i, \zeta}\right) \\
%         & \hspace{1in}+ \int_{t_0}^T l_i(\eta_i(s), \zeta(s)) ds \Big] \\
%     \end{aligned}
%     % \vspace{-0.1in}
% \end{equation}
are equal: $V = V^+ = V^-$. $(\{\eta_i\}, \zeta)$ is a Nash equilibrium (NE) if it attains $V$.
% \begin{equation}
% \small
%     \inf_\eta \sup_\zeta \mathbb{E}_{\eta, \zeta, i \sim p_0} \int_0^T l_i dt + g_i = \sup_\zeta \inf_\eta \mathbb{E}_{\eta, \zeta, i \sim p_0} \int_0^T l_i dt + g_i,
%     \label{eq:ne}
% \end{equation}
% We call this common value the \textit{value of the game.} 
% A Nash equilibrium is said to be \textit{pure} if the strategies are deterministic, specifying a definite action for every decision point, and it is called \textit{mixed} if the strategies are probabilistic over the corresponding action spaces. 
We introduce the following assumptions under which the game has a value~\citep{cardaliaguet2007differential}:
\vspace{-0.1in}
\begin{enumerate}[label=A\arabic*., noitemsep]
\vspace{-0.05in}
    \item $\mathcal{U} \subseteq \mathbb{R}^{d_u}$ and $\mathcal{V} \subseteq \mathbb{R}^{d_v}$ are compact and finite-dimensional sets.
    \item $f: \mathcal{X} \times \mathcal{U} \times \mathcal{V} \rightarrow \mathcal{X}$ is bounded, continuous, and uniformly Lipschitz continuous with respect to $x$.
    \item $g_i: \mathcal{X} \rightarrow \mathbb{R}$ and $l_i: \mathcal{U} \times \mathcal{V} \rightarrow \mathbb{R}$ are Lipschitz continuous and bounded. 
    \item Isaacs' condition holds for the Hamiltonian $H: \mathcal{X} \times \mathbb{R}^{d_x} \rightarrow \mathbb{R}$:
    \vspace{-0.05in}\begin{equation}\label{eq:isaacs}
        \begin{aligned}
            H (x, \xi) &:= \min_{u \in \mathcal{U}} \max_{v \in \mathcal{V}} f(x, u, v)^\top \xi - l_i(u, v)\\ &= \max_{v \in \mathcal{V}} \min_{u \in \mathcal{U}}f(x, u, v)^\top \xi - l_i(u, v).
        \end{aligned}
    \vspace{-0.05in}
    \end{equation}
    \item Both players have full knowledge about $f$, $\{g_i\}_{i=1}^I$, $\{l_i\}_{i=1}^I$, $p_0$, and the Nash equilibrium of the game. Control inputs and states are fully observable and we assume perfect recall. 
\end{enumerate}
\vspace{-0.25in}
% Critically, Isaacs' condition allows any complete-information 2p0s games between P1 and P2 to have pure Nash equilibrium. 
% We consider strategies that are non-anticipative with delay (see \citep{cardaliaguet2007differential}).
% Let $(\Omega_{\eta}, \mathcal{F}_{\eta}, \textbf{P}_{\eta})$ be the probability space associated with $\mathcal{H}_r^i(t)$ for $i \in [I]$ and $t\in [0,T]$, and similarly define $(\Omega_{\zeta}, \mathcal{F}_{\zeta}, \textbf{P}_{\zeta})$ for $\mathcal{Z}_r(t)$. 
% With these, the value of the game $V: [0,T] \times \mathcal{X}\times \Delta(I) \rightarrow \mathbb{R}$ is written as:
\paragraph{Dynamic programming (DP) for P1.}
To approximate P1's equilibrium strategy, we introduce a discrete-time value approximation $V_\tau$, which satisfies the following DP~\cite{cardaliaguet2009numerical}:
\vspace{-0.05in}
\begin{equation}
\begin{aligned}
    V_\tau(t_0,x_0,p) =\min_{\{\eta_i\} 
    % \in {\mathcal{H}_r(t_0)\}^I}
    }\mathbb{E}_{u \sim \bar{\eta}}\Big[
    &\max_{v \in \mathcal{V}} V_\tau(t_0 + \tau, x'(u,v), p'(u))\\
    &+ \tau \mathbb{E}_{i\sim p'(u)} [l_i(u,v)] \Big],
\end{aligned}
    \label{eq:primal-subdp}
% \vspace{-0.05in}
\end{equation}
with a terminal boundary $V_\tau(T,x_0,p) = \sum_i p[i]g_i(x_0)$. Here $x'(u,v)$ solves Eq.~\ref{eq:system} starting from $x_0$ for a time span of $\tau$ using constant control inputs $(u,v)$ during $[t_0,t_0+\tau)$, and $p'(u)$ is the Bayes update of the public belief after P1 takes and P2 observes $u$: $p'(u)[i] = \eta_i(u) p[i]/\bar{\eta}(u)$,
% \begin{equation}
%     p'(u)[i] = \frac{\eta_i(u) p[i]}{\bar{\eta}(u)}.
% \end{equation}
where $\bar{\eta}$ is the marginal distribution over $\mathcal{U}$ across types: $\bar{\eta}(u) = \sum_{i\in[I]} \eta_i(u)p_0[i]$.
% Eq.~\ref{eq:primal-subdp} holds because if P1 plays the equilibrium, the value is indifferent whether P2 plays the equilibrium or the (pure) best responses to P1. 
Note that P2's equilibrium cannot be derived from Eq.~\ref{eq:primal-subdp}. 
% which will be solved from a separate DP for P2, see below.
\vspace{-0.1in}
\paragraph{Dual DP for P2.} To compute P2's equilibrium strategy, we need another DP that involves P2's behavioral strategies and P1's best responses. This can be achieved by introducing the convex conjugate $V^*$ of $V$:
\vspace{-0.1in}
\begin{equation}\label{eq:dual_value}
\small
    \begin{aligned}
        &V^*(t_0,x_0,\hat{p}) :=  \max_{p} p^T \hat{p} - V(t_0,x_0,p) \\
        &=  \max_{p} p^T \hat{p} -  \sup_{\zeta \in \mathcal{Z}_r(t_0)}\inf_{\{\eta_i\} \in \{\mathcal{H}_r(t_0)\}^I} \mathbb{E}_{\eta_i, \zeta, i} \Big[J_i(t_0,x_0;\eta_i,\zeta) \Big] \\
        &=  \max_{p} \inf_{\zeta \in \mathcal{Z}_r(t_0)} \sup_{\{\eta_i\} \in \{\mathcal{H}_r(t_0)\}^I} p^T \hat{p} -  \mathbb{E}_{\eta_i, \zeta, i} \Big [J_i(t_0,x_0;\eta_i,\zeta) \Big] \\
        &=  \inf_{\zeta \in \mathcal{Z}_r(t_0)} \sup_{\eta \in \mathcal{H}(t_0)} \max_{i \in \{1, \dots, I\}} \Bigg\{\hat{p}_i - \mathbb{E}_{\zeta} \Big[ J_i(t_0,x_0;\eta_i,\zeta) \Big]\Bigg\}.
    \end{aligned}
\end{equation}
The last step of Eq.~\ref{eq:dual_value} uses the linearity of the payoff with respect to $p$ and again the fact that best responses are always pure (thus $\eta$ belongs to the pure strategy set $\mathcal{H}(t_0)$ rather than the random strategy set $\mathcal{H}_r(t_0)$). Eq.~\ref{eq:dual_value} describes a dual game with complete information, where the strategy space of P1 becomes $\mathcal{H}(t_0) \times [I]$, i.e.,
the game type is now chosen by P1 rather than the nature. It is proved that P2's equilibrium in the dual game is also an equilibrium for the primal game if $\hat{p} \in \partial_p V(t_0,x_0,p)$. We explain in App.~\ref{app:primaldual} that such $\hat{p}$ represents the type-dependent gains of P1 should he play the best responses to P2's equilibrium strategy. Therefore $\hat{p}_i - \mathbb{E}_{\zeta}[g_i + \int l_i]$ measures P2's risk and his equilibrium strategy is to minimizes the worst-case risk across all game types.
The DP of P2 in this dual game is~\citep{cardaliaguet2009numerical}:
\begin{equation}
\begin{aligned}
    &V_\tau^*(t_0,x_0,\hat{p}) =\\
    &\min_{\zeta, \hat{p}'(v)} \mathbb{E}_{v \sim \zeta}\left[
    \max_{u \in \mathcal{U}} V_\tau^*(t_0 + \tau, x'(u,v), \hat{p}'(v)-\tau l(u,v))\right],
\end{aligned}
    \label{eq:dual-subdp}
\end{equation}
with a terminal boundary $V^*(T,x_0,\hat{p}) = \max_{i \in [I]} \{\hat{p}[i] - g_i(x_0)\}$. Here $\hat{p}'(v): \mathcal{V} \rightarrow \mathbb{R}^{I}$ is constrained by $\mathbb{E}_{v \sim \zeta} [\hat{p}'(v)] = \hat{p}$, and $l(u,v)[i] = l_i(u,v)$.

Let P1's strategy set from Eq.~\ref{eq:primal-subdp} be $\{\eta_{i,{\tau}}\}$ and P2's from Eq.~\ref{eq:dual-subdp} be $\zeta_{\tau}$. Thm.~\ref{thm:convergence} proves that $(\{\eta_{i,{\tau}}\}, \zeta_{\tau})$ approaches the equilibrium of $V$ when $\tau$ is sufficiently small (App.~\ref{app:convergence} completes the proof sketch in \citet{cardaliaguet2009numerical}):
\begin{theorem}\label{thm:convergence}
    (Thm.4.1 of \citet{cardaliaguet2009numerical}) If A1-5 hold, then there exists some $M_1,M_2>0$, such that $V(t_0,x_0,p) \leq \max_{\zeta \in \mathcal{Z}(t_0)} J(t_0,x_0,p;\{\eta_{i,{\tau}}\}, \zeta) \leq V(t_0,x_0,p) + M_1(T-t_0)\tau$ for any $(t_0,x_0,p) \in [0,T]\times \mathcal{X} \times \Delta(I)$, and 
    $V^*(t_0,x_0,\hat{p}) \leq \max_{\{\eta_i\} \in \{\mathcal{H}^i\}^I} J^*(t_0,x_0,\hat{p};\{\eta_{i}\}, \zeta_{\tau}) \leq V^*(t_0,x_0,\hat{p}) + M_2(T-t_0)\tau$ for any 
    $(t_0,x_0,\hat{p}) \in [0,T]\times \mathcal{X} \times \mathbb{R}^I$.
\end{theorem}

\vspace{-0.15in}
\paragraph{Remarks.} Notice that the DPs consider conservative approximations of the original game. E.g., the primal DP considers P2 play the best responses to the actions \textit{to be played} by P1, thus $V(t_0,x_0,p) \leq \max_{\zeta \in \mathcal{Z}(t_0)} J(t_0,x_0,p;\{\eta_{i,{\tau}}\}, \zeta)$. Nonetheless, by using continuity and boundedness assumptions (A1-3) and Isaacs' condition (A4), Thm.~\ref{thm:convergence} shows that the advantages taken by best responses in the DPs are limited. Importantly, approximating the original game through the DPs enables the ``splitting'' reformulation that critically addresses the scalability issue with respect to continuous action spaces, which we discuss in Sec.~\ref{sec:splitting}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.15in}
\section{A Splitting Reformulation of the DPs}
\label{sec:splitting}
\vspace{-0.1in}
With Thm.~\ref{thm:convergence}, we can approximate P1's strategy by solving Eq.~\ref{eq:primal-subdp}, and P2's by solving both Eq.~\ref{eq:primal-subdp} and Eq.~\ref{eq:dual-subdp} because his strategy depends on $\partial_p V(t_0,x_0,p)$. These minimax problems need to be solved at sufficiently many collocation points ($(t,x,p)$ or $(t,x,\hat{p})$) and with a sufficiently refined time discretization. In the context of IIEFGs, both DPs can be considered as sequential games where the leader plays a mixed strategy and the follower a best response. Existing algorithms, e.g., CFR+, CFR-BR, and MMD, are not scalable at solving the DPs when the games have continuous action spaces and many time steps. To this end, our key insight is the following theorem, which states that P1's strategy that solves the primal DP is $I$-atomic and P2's is $(I+1)$-atomic (proof in App.~\ref{app:splitting}):
\begin{theorem}\label{thm:splitting}
    The RHS of Eq.~\ref{eq:primal-subdp} can be reformulated as
    \vspace{-0.15in}
        \begin{equation}\label{eq:opt_prob} \tag{$\text{P}_1$}
        \small
        \begin{aligned}
            &\min_{\{u^k\}, \{\alpha_{ki}\}} \max_{\{v^k\}} \quad \sum_{k=1}^{I}\lambda^k \Big(V(t+\tau, x^k, p^k) \\
            &\hspace{1.7in}+ \tau \mathbb{E}_{i \sim p^k} [l_i(u^k, v^k)]\Big)\\
            &\text{s.t.} \quad u^k \in \mathcal{U}, \quad x^k = \text{ODE}(x, \tau, u^k, v^k; f), \quad v^k \in \mathcal{V}, \\
            &\quad \quad \alpha_{ki} \in [0, 1],\; \sum_{k=1}^I \alpha_{ki} = 1,\; \lambda^k = \sum_{i=1}^I \alpha_{ki} p[i], \\
            &\quad \quad p^k[i] = \frac{\alpha_{ki}p[i]}{\lambda^k}, \quad \forall i, k \in [I].
        \end{aligned}
        \end{equation}
    And the RHS of Eq.~\ref{eq:dual-subdp} can be reformulated as
    \vspace{-0.1in}
    \begin{equation}\label{eq:opt_prob_dual} \tag{$\text{P}_2$}
    \small
    \begin{aligned}
        &\min_{\{v^k\}, \{\lambda^{k}\}, \{\hat{p}^k\}} \max_{\{u^k\}} \sum_{k=1}^{I+1}\lambda^k \left(V^*(t+\tau, x^k, \hat{p}^k - \tau l(u^k, v^k))\right)\\
        &\text{s.t.} \quad u^k \in \mathcal{U}, \quad v^k \in \mathcal{V}, \quad x^k = \text{ODE}(x, \tau, u^k, v^k; f),\\
        &\quad \quad \lambda^{k} \in [0, 1],\; \sum_{k=1}^{I+1} \lambda^{k}\hat{p}^k = \hat{p}, \; \sum_{k=1}^{I+1}\lambda^k = 1,\; k \in [I+1].\\
    \end{aligned}
    \end{equation}
\end{theorem}
\vspace{-0.2in}
\paragraph{Sketch of the proof.} By change of variable and introducing a pushforward measure, we can show that the RHS of the primal (resp. dual) DP essentially seeks a mixed strategy that convexifies the value (resp. dual value) at the next time step over $\Delta(I)$ (resp. $\mathbb{R}^{I}$). Since convexification requires at most $I$ vertices in $\Delta(I)$ (resp. $I+1$ vertices in $\mathbb{R}^I$), the resultant strategy is at most $I$-atomic (resp. $(I+1)$-atomic). 
\vspace{-0.25in}
\paragraph{A visual example.} Fig.~\ref{fig:vex_cav} provides an intuitive explanation of the causality between value convexification and the equilibrium strategy, where the public belief $p \in \Delta(2)$: Let the solid red line be $U_\tau(t_0,x_0,p) := \min_u \max_v V(t_0+\tau,x'(u,v),p) + \tau \mathbb{E}_{i}[l_i(u,v)]$. We call $U_\tau(t_0,x_0,p)$ the value of a \textit{non-revealing} version of the game because $p$ does not change over the course of this game when P1 plays pure.
One notices that if $U_\tau$ is not convex in $p$, it is always possible for P1 to achieve a lower value by convexifying $U_\tau$ through the use of a mixed strategy, leading to \ref{eq:opt_prob}. 
In this particular case, P1 identifies $[\lambda^a, \lambda^b]^T \in \Delta(2)$ and $\{p^a, p^b\}$ such that $\lambda^a p^a +  \lambda^b p^b = p$. Picking one action $u^k \in \argmin_u \max_v V(t_0+\tau,x'(u,v),p^k) + \tau \mathbb{E}_{i}[l_i(u,v)]$ for each $k \in \{a,b\}$\footnote{Isaacs' condition guarantees that $\min_u \max_v V(t_0+\tau,x'(u,v),p^k) + \tau \mathbb{E}_{i}[l_i(u,v)]$ has a solution.}, P1 of type $i$ will then play action $u^k$ with probability $\alpha_{ki}= p^k[i]\lambda^k/p[i]$. By announcing this strategy, the public belief shifts to $p^k$ via the Bayes' rule if P1 takes action $u^k$, and as a result, P1 receives a value $V(t_0, x_0, p) = \lambda^a U(t_0, x_0, p^a) + \lambda^b U(t_0, x_0, p^b)$, which is the convexification of $U(t_0,x_0,p)$ over $p \in \Delta(2)$.
The same splitting happens for P2 in the dual game: instead of the public belief $p$, P2's strategy splits the dual variable $\hat{p}$ to $\hat{p}^k$ by playing action $v^k$ with probability $\lambda^k$.
We note that this convexification nature of the equilibrium strategies has been discovered as the ``Cav u'' theorem as early as for 2p0s repeated games with one-sided information~\citep{aumann1995repeated,de1996repeated}. Our new contribution is in explaining its connection with IIEFGs (see below) and in developing a scalable algorithm for value and strategy approximation that takes advantage of this property along with multigrid (see Sec.~\ref{sec:multigrid}).  
\begin{figure}[!h]
    \centering
    \vspace{-0.1in}
    \includestandalone[width=0.6\linewidth]{figures/convex_cave_icml}
    \vspace{-0.2in}
    \caption{Revealing and non-revealing game values, and the mechanism of splitting.}
    \vspace{-0.1in}
    \label{fig:vex_cav}  
\end{figure}
\vspace{-0.2in}
\paragraph{Comparison with CFR-BR.} For conciseness, we introduce CFR-BR as a representative IIEFG algorithm to compare with \ref{eq:opt_prob} and \ref{eq:opt_prob_dual}, since CFR-BR also decouples the solving of P1 and P2's strategies by letting one player always play the best response to the opponents' behavioral strategy. In the context of this paper, CFR-BR solves
\begin{equation}
\vspace{-0.05in}
\small
\begin{aligned}
    &V_\tau(t_0,x_0,p) =\\
    &\min_{\{\eta_i\}}\max_{v \in \mathcal{V}} 
    \mathbb{E}_{u}\left[
     V_\tau(t_0 + \tau, x'(u,v), p'(u)) + \tau \mathbb{E}_{i\sim p'(u)} [l_i] \right]\\
    &=\max_{\zeta}\min_{u \in \mathcal{U}} 
    \mathbb{E}_{v}\left[
     V_\tau(t_0 + \tau, x'(u,v), p) + \tau \mathbb{E}_{i\sim p} [l_i] \right]. 
\end{aligned}
\label{eq:cfrbr}
\end{equation}
We first note that the CFR-BR formulation does not enjoy atomic mixed strategies as in the DPs. This is because the best responses of P2 are upon the mixed strategies of P1 rather his actual actions. Therefore the non-revealing value $U_\tau(t_0,x_0,p)$ is implicitly a function of P1's mixed strategies rather than of a single action. As a result, the RHSs of Eq.~\ref{eq:cfrbr} cannot be rewritten as convexification over the public belief. 
This causes CFR-BR to suffer from slow convergence when fine discretization of the continuous action spaces is necessary. 
% While \cite{rebel} discussed value convexity with respect to the public belief $p$, existing IIEFG solvers do not exploit ``Cav u'' because IIEFGs in general do not enjoy the following properties of the game we study in this paper: (i) the imperfectness of information is about the payoff types, (ii) the non-revealing games always have pure Nash, and (iii) $I \ll |\mathcal{A}|$. 
On the other hand, using a leader-follower reformulation of the game, \ref{eq:opt_prob} and \ref{eq:opt_prob_dual} reveals the atomic nature of the equilibrium strategies via Thm.~\ref{thm:convergence} and Thm.~\ref{thm:splitting}. In addition, if at some $(t_0,x_0,p)$ the non-revealing value is convex, the behavioral strategies at that node becomes pure. This violates a common assumption of IIEFG solvers, i.e., equilibrium is interior of $\Delta(U)$.
Lastly, we note that the leader-follower formulation in Thm.~\ref{thm:splitting} is applicable to the following game settings with one-sided payoff information: 
\vspace{-0.1in}
\begin{enumerate}[noitemsep]
\vspace{-0.05in}
    \item differential games where A1-3 make up for the incorrect leader-follower setting (this paper, and see App.~\ref{app:hexner} for an analytical example where \ref{eq:opt_prob} and \ref{eq:opt_prob_dual} correctly solve a differential game),
    % \vspace{-0.05in}
    \item turn-based extensive-form games where the assignment of leader and follower is naturally correct (see App.~\ref{app:beerquiche} for the derivation of the Nash equilibrium using \ref{eq:opt_prob} and \ref{eq:opt_prob_dual} for a turn-based game), and 
    % \vspace{-0.05in}
    \item infinitely repeated normal-form games where splitting only happens in the first step of the game for which the payoff does not affect the value (see \citet{de1996repeated}).   
\end{enumerate}

\vspace{-0.20in}
\begin{algorithm}
    \caption{CAMS for P1}
    \label{alg:cams}
    \SetKwComment{Comment}{/* }{ */}
    \SetKwInput{KwIn}{Input}
    \SetKwInput{KwOut}{Output}
    \SetKwInput{Init}{Initialize}
    \KwIn{time discretization $\tau$, terminal value $V(T,\cdot,\cdot)$, sample size $N$, minimax solver $\mathbb{O}$} 
    % \KwOut{Value function $\boldsymbol{V}(t, \cdot, \cdot)$}
    \Init{value network $\{\hat{V}_t\}_{t=0}^{T-\tau}$, training dataset $\mathcal{S} \leftarrow \emptyset$ }
    $\mathcal{S} \leftarrow$ sample $N$ states $(x, p) \in \mathcal{X} \times \Delta(I)$\\
    \For{$t$ in $\{T-\tau,...,0\}$}{
        \For{$(x, p)$ in $\mathcal{S}$}{
            $\vartheta \leftarrow \mathbb{O}(t, x, p)$  \Comment*[r]{Solution to \ref{eq:opt_prob}}
            append $\{(t, x, p), \vartheta\}$ to $\mathcal{S}$\;
        }
        Fit $\hat{V}_{t}$ to $\mathcal{S}$    
    }
    % \vspace{-0.1in}
\end{algorithm}

\vspace{-0.2in}
\paragraph{The proposed algorithm.} We discretize the time span $[0,T]$ as $\{k\tau \}_{k=0}^{K}$ where $\tau = T/K$, and denote by $\mathcal{S}=\{(x,p)_i\}_{i\in[|\mathcal{S}|]}$ and $\mathcal{S}^*=\{(x,\hat{p})_i\}_{i\in[|\mathcal{S}^*|]}$ the primal and dual sample set, respectively. The backward induction solves \ref{eq:opt_prob} (resp. \ref{eq:opt_prob_dual}) starting from $t=(K-1)\tau$ at all collocation points in $\mathcal{S}$ (resp. $\mathcal{S}^*$). The resultant nonconvex-nonconcave minimax problems have size $(\mathcal{O}(I(I+d_u)), \mathcal{O}(Id_v))$ (resp. $(\mathcal{O}(I(I+d_v), \mathcal{O}(Id_u))$). Importantly, the computational complexity of these problems are no longer related to the size of the action spaces. To generalize value (and optionally policy) prediction across the continuous joint space of state and belief, primal and dual value networks are trained on the minimax solutions. The value networks are used to formulate the next round of minimax at $t-\tau$. The backward induction continuous until $t=0$. Alg.~\ref{alg:cams}, dubbed CAMS (Continuous Action Mixed Strategy solver), summarizes the proposed algorithm for P1.

\vspace{-0.15in}
\paragraph{The remaining computational challenges.} 
Our discussion so far addresses the scalability issue due to large or continuous action spaces. In particular, when the number of possible game types is small, i.e., $I^2 \ll |\mathcal{U}|+|\mathcal{V}|$, solving \ref{eq:opt_prob} and \ref{eq:opt_prob_dual} becomes more efficient than using IIEFG solvers. The computational challenge, however, still remains for two reasons: (1) Thm.~\ref{thm:convergence} suggests a fine enough time discretization for the strategies derived from \ref{eq:opt_prob} and \ref{eq:opt_prob_dual} to be good approximations of the equilibrium. (2) Through the baseline algorithm, suppressing the $L_{\infty}$ value prediction error at $t=0$ requires a computational complexity exponential to the number of time steps $K$. Specifically, let $\hat{V}_0(x,p):\mathcal{X} \times \Delta(I) \rightarrow \mathbb{R}$ be the trained value networks at $t=0$, we have the following result (see proof in App.~\ref{app:complexity}): 
\begin{theorem}
Given the number of time steps $K$, a minimax approximation error $\epsilon>0$, a prediction error threshold $\delta>0$, there exists some constant $C\geq 1$, such that with a computational complexity of at least $\mathcal{O}(K^3C^{2K}I^2\epsilon^{-4}\delta^{-2})$, Alg.~\ref{alg:cams} achieves 
\vspace{-0.08in}
\begin{equation}
    \max_{(x,p)\in \mathcal{X}\times \Delta(I)} |\hat{V}_0(x,p) - V(0,x,p)| \leq \delta.
\end{equation}
\end{theorem}
\vspace{-0.2in}
A similar result applies to the dual game. \citet{zanette2019limiting} discussed a linear value approximator that achieves $C=1$. However, their method requires solving a linear program (LP) for every inference $\hat{V}_t(x,p)$ if $(x,p)$ does not belong to the training set $\mathcal{S}$. In our context, incorporating their method would require auto-differentiating through the LP solver during each descent and ascent steps in solving the minimax problems, which turned out to be expensive in PyTorch and JAX. While effective suppression of $C$ for neural nets remains to be investigated, this paper introduces a multigrid approach to reduce the cost for games with a large $K$, as we discuss in Sec.~\ref{sec:multigrid}.  

% Since $\hat{p}$ is not constrained to a simplex, P2 solves a harder problem than P1. Specifically, P2 needs to find at most $I+1$, rather than $I$, splitting points $\{\hat{p}^k\}_{k=1}^{I+1}$ in order to compute the convexification $V^*$. This is the extra cost P2 pays due to her information disadvantage. 

% \begin{figure}[!ht]
% \vspace{-0.12in}
% \centering
%     \includegraphics[width=\linewidth]{figures/overview_small.pdf}
%     \vspace{-0.2in}
%     \caption{SOTA algorithms like CFR cannot take advantage of the special structure of the differential games with incomplete information, i.e., P1 (resp. P2) has at most $I$ (resp. $I+1$) actions to mix at each infostate.}
%     \label{fig:overview}
%     \vspace{-0.12in}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-0.15in}
\section{A Multigrid Approach}\label{sec:multigrid}
\vspace{-0.1in}
We introduce a multigrid approach that accelerates value approximation through backward inductions on multiple time grids. Since strategies at time $t$ are implicitly nonlinear functions of the value at $t+\tau$, the primal and dual HJI PDEs underlying \ref{eq:opt_prob} and \ref{eq:opt_prob_dual} are nonlinear. Therefore, our method will extend the Full Approximation Scheme (FAS) commonly used for solving nonlinear PDEs, where PDEs are solved on all grids and coarse-grid corrections are then used to improve fine-grid solutions~\cite{trottenberg2000multigrid, henson2003multigrid}. 
In theory, FAS reduces the required number of ``fine sweeps'' by shifting global error correction onto the cheaper coarse pass.
More concretely, a two-grid FAS has four steps (see illustration in Fig.~\ref{fig:overview_fig}(b)):
% \begin{enumerate}[label=(\roman*), noitemsep]
%     \item Restrict the fine-grid approximation and its residual;
%     \item Solve the coarse-grid problem using the fine-grid residual
%     \item Compute the coarse-grid correction
%     \item Prolong the coarse-grid correction to fine-grid and add the correction to fine-grid approximation. 
% \end{enumerate}
(1) Restrict the fine-grid approximation and its residual; (2) solve the coarse-grid problem using the fine-grid residual; (3) compute the coarse-grid correction; (4) prolong the coarse-grid correction to fine-grid and add the correction to fine-grid approximation. 

\vspace{-0.01in}
For conciseness, we will focus on the primal problem to introduce the FAS extension.
Let $\hat{V}_{t}^l$ be the value network for time $t$ on grid size (time interval) $l$.
Let the restriction operators be $\mathcal{R}^{l}$ from a finer grid with grid size $l$ to a coarser one with size $2l$: $\mathcal{R}^l (\hat{V}_{t}^l) = (\hat{V}_{t}^l + \hat{V}_{t+l}^l)/2$ is the value restriction from $l$ to $2l$. This restriction operator takes into account the backward induction nature of value functions.
% \vspace{-0.1in}
% \begin{equation}
% \mathcal{R}^t_l (\hat{V}_{t,l}) = \frac{\hat{V}_{t,l} + \hat{V}_{t+l,l}}{2}
% \end{equation}
Similarly, we define the prolongation operators $\mathcal{P}^{2l}$ as:
\vspace{-0.105in}
\begin{equation}
    \mathcal{P}^{2l}(\hat{V}_{t}^{2l}) = \begin{cases}
        \hat{V}_{t}^{2l}, & \text{ if } t \in \mathcal{T}^{2l}\\
        \hat{V}_{t+l}^{2l}, & \text{ otherwise }
    \end{cases},
    \vspace{-0.1in}
\end{equation}
where $\mathcal{T}^{2l}:= \{n \cdot 2l: n \in \mathbb{N}_0, n < T/2l\}$. Let $\mathbb{O}^l(t,x,p;\hat{V})$ solves \ref{eq:opt_prob} at $(t,x,p)$ using $\tau = l$ and $\hat{V}$ as the value at $t+\tau$, and outputs an approximation for $V(t,x,p)$. The dataset $\{(t,x^{(j)},p^{(j)},\mathbb{O}^l(t,x^{(j)},p^{(j)};\hat{V}_{t+l}^{l}))\}$ is used to train $\hat{V}_{t}^l(\cdot,\cdot)$. Let $r_{t}^l(x,p) = \hat{V}_{t}^l(x,p) - \mathbb{O}^l(t,x,p;\hat{V}_{t+l}^l)$ be the residual.
On each grid, our goal is to find $\hat{V}_{t}^l$ such that $r_{t}^l(x,p) \approx 0$ for all $(t,x,p) \in \mathcal{T}^l \times \mathcal{X}\times \Delta(I)$.
This is achieved by restricting the fine grid approximations and residuals to the coarse grid and solving to determine the corrections. Let $e^{l}_{t}(x,p)$ be the correction in grid $l$ at $(t,x,p)$. Then, the coarse-grid problem is:
\vspace{-0.16in}
\begin{equation}
% \small
\begin{aligned}
\underbrace{\mathcal{R}^l r_t^l}_{\textcolor{red}{\text{residual}}} &= \underbrace{\mathbb{O}^{2l}(t,x,p;\mathcal{R}^l\hat{V}_{t+2l}^l + e^{2l}_{t+2l}) - \left(\mathcal{R}^l \hat{V}_t^l + e^{2l}_t(x,p)\right)}_{\textcolor{red}{\text{coarse-grid eq. w/ corrections}}} \\
    \vspace{-0.5in}
    &\hspace{0.2in} - \underbrace{\left(\mathbb{O}^{2l}(\mathcal{R}^l\hat{V}_{t+2l}^l) - \mathcal{R}^l \hat{V}_t^l\right)}_{\textcolor{red}{\text{coarse-grid eq. w/o corrections}}},
\end{aligned}
\vspace{-0.1in}
\end{equation}
% \vspace{-0.05in}
which is solved backward from $T-2l$, as the terminal value is known, resulting in $e^{2l}_{T} = 0$.
Knowing $e^{2l}_{t+2l}(\cdot,\cdot)$, the FAS coarse-grid correction at $(t,x,p)$ is:
\vspace{-0.1in}\begin{equation}\label{eq:correction}
% \small
\begin{aligned}
    e^{2l}_t(x,p) = &\mathbb{O}^{2l}(t,x,p;\mathcal{R}^l\hat{V}_{t+2l}^l + e^{2l}_{t+2l}) \\
    &- \mathbb{O}^{2l}(\mathcal{R}^l\hat{V}_{t+2l}^l) - \mathcal{R}^l r_t^l.
\end{aligned}
\vspace{-0.1in}
\end{equation}
This correction ensures consistency: If $\hat{V}_{t}^l = V(t,\cdot,\cdot)$ for all $t \in \mathcal{T}^l$, $e^{2l}_t(\cdot,\cdot)=0$ for all $t \in \mathcal{T}^{2l}$.
The coarse grid corrections are prolonged to the fine grid to update the fine-grid value approximation.
Alg.~\ref{alg:multigrid} summarizes a 2-level multigrid algorithm, and 
Alg.~\ref{alg:n_multigrid} for an $n$-level version (see App.~\ref{app:multigrid}). Note that from Eq.~\ref{eq:correction}, computing the coarse correction in our case requires two separate minimax calls with similar loss formulations. We further accelerate the multigrid solver by warm-starting these minimax problems using the recorded minimax solution derived from the fine grid (during the residual computation).     

% \begin{figure}
%     \centering
%     \includestandalone[width=\linewidth]{figures/multigrid_illus}
%     \caption{Illustration of 2-level multigrid process. For simplicity, the finest grid is coarsened by a factor of 2. As the finest grid size reduces, it is beneficial to employ a $n-$level multigrid by simply adding more coarser grids until the coarsest grid is of size 2. Note that the solution at the final time $(t=1)$ is known in all cases. Hence, it is excluded.}
%     \label{fig:multi_illus}
% \end{figure}
%% 
%%%%%%%%%%%%%%%%%%%% 2-level multigrid algorithm %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% moved to appendix
% \begin{algorithm}[!h] 
% \caption{Two-grid Value Approximation}
% \label{alg:multigrid}
% \SetKwInput{Init}{Initialize}
% \KwIn{Coarse time-steps $\mathcal{T}^{2l}$, fine time-steps $\mathcal{T}^{l}$, coarse DS-GDA solver $\mathcal{O}^{2l}$, fine DS-GDA solver $\mathcal{O}^{l}$, number of data points $N$ to sample, restriction operator $\mathcal{R}$, prolongation Operator $\mathcal{P}$}
% \Init{Initialize fine grid value networks $\hat{V}^l_t$ $\forall t \in \mathcal{T}_l$, policy set $\Pi = \emptyset$}
% \While{resource not exhausted or until convergence}{
%     Initialize: $R^l = \emptyset$, $E^{2l} = \emptyset$, $\mathcal{S} = \emptyset$, \\
%     Coarse grid correction networks $\varepsilon^{2l}_t$ $\forall t \in \mathcal{T}^{2l}$\;
    
%     $\mathcal{S}[t] \leftarrow$ sample $N$ data points $(t, x, p)$, $\forall t \in \mathcal{T}^l$\;
    
%     \tcp{Smoothing Step: Few iterations}
%     $\texttt{target}, \pi_t \leftarrow \mathcal{O}^l(t+l,\cdot,\cdot,\hat{V}^l_{t+l})$ (init. w/ $\pi_t$ if $\Pi \neq \emptyset$)\;
%     Store residuals: $r^l_t = (\hat{V}^l_t - \texttt{target})$ in $R^l$ and policies $\pi_t$ in $\Pi$\;
    
%     \tcp{Restriction and coarse-solve}
%     \For{$t \leftarrow T-2l$ \KwTo $0$}{
%         $e^{2l}_t = \mathcal{O}^{2l}(\mathcal{R} \hat{V}^{l}_{t+2l} + \varepsilon^{2l}_{t+2l}) - \mathcal{O}^{2l}(\mathcal{R} V^l_{t+2l}) - \mathcal{R} r^{l}_t$
%         \tcp*{$e^{2l}_T = 0$, $\varepsilon^{2l}_T = \emptyset$}
        
%         Store $e^{2l}_t$ in $E^{2l}$\;
        
%         Fit the correction network $\varepsilon^{2l}_t$ to $e^{2l}_t$
%     }
    
%     \tcp{Prolongation}
%     \ForEach{$t \in \mathcal{T}^l$}{
%         $e^l_t = \mathcal{P} e^{2l}_t$\;
        
%         Fit $\hat{V}^l_t$ to $\hat{V}^l_t + e^l_t$\;
%     }
    
%     $\texttt{target}, \pi_t \leftarrow \mathcal{O}^l(t+h,\cdot,\cdot,\hat{V}^l_{t+l})$ (init. with $\pi_t$)\;
    
%     Fit $\hat{V}^l_t$ to $\texttt{target}$ and replace $\pi_t$ in $\Pi$ \tcp*{Post Smoothing}
% }
% \end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%% n-level multigrid algorithm %%%%%%%%%%%%%%%
% \begin{algorithm}[!h]
% \caption{n-Level Multigrid for Value Approximation}
% \label{alg:n_multigrid}
% \SetAlgoLined
% \SetKwInput{Init}{Initialize}
% \KwIn{$k_{\max}, k_{\min}, \mathcal{O}$ (DS-GDA Solver), $T$ (time horizon), $N$ (number of data points), $\mathcal{R}$ (restriction operator), $\mathcal{P}$ (prolongation operator)}
% \BlankLine
% \Init{$\mathcal{T}_h \leftarrow [0, h, 2h, \dots, T-h],\; \forall\,h \in \{2^{-k_{\max}}, \dots, 2^{-k_{\min}}\}$, $H \leftarrow 2^{-k_{\min}}$}
% \Init{Value networks $V_h^t,\; \forall\,t \in \mathcal{T}_h,\; \forall\,h \in \{2^{-k_{\max}}, \dots, 2^{-k_{\min}}\}$, policy set $\Pi \leftarrow \varnothing$}\;
% \While{resources not exhausted or until convergence}{
%     $R \leftarrow \varnothing,\; E_H \leftarrow \varnothing,\; \mathcal{X} \leftarrow \varnothing$\;
    
%     Initialize coarsest-grid correction networks $\varepsilon_H^t,\forall\,t \in \mathcal{T}_H$\;
    
%     $\mathcal{X}[t] \leftarrow \text{sample }N\;(t,x,p),\; \forall\,t \in \texttt{linspace}(0,T,2^{-k_{\max}})$\;
    
%     \tcp{down-cycle}
%     \For{$k \gets k_{\max}$ \textbf{down to} $k_{\min}+1$}{
%         Compute target via $\mathcal{O}_k$ using $\pi[t]\in \Pi$ (if available) as a warm start, 
%         and store updated policies $\pi[t]$ in $\Pi[k],\; \forall\,t \in \mathcal{T}_k$\;
        
%         Compute residuals $r_k[t],\; \forall\,t \in \mathcal{T}_k$\;
        
%         \If{$k \neq k_{\max}$}{
%             $r_k[t] \leftarrow r_k[t] + \mathcal{R}^t(r_{k+1}),\; \forall\,t \in \mathcal{T}_k$\;
%         }
%         Store $r_k[t]$ in $R[k]$\;
%     }
%     \BlankLine
%     \ForEach{ $t$ \textbf{in reverse} order of $\mathcal{T}_{H}$ }{
%         \tcp{coarse-solve backwards in time}
%         $e_H^t \leftarrow \mathcal{O}_H(\mathcal{R}^{t+H}V_h + \varepsilon_H^{t+H}) 
%         - \mathcal{O}_H(\mathcal{R}^{t+H}V_h) - \mathcal{R}^t\,r_{k_{\min}+1}$\;
%         \tcp*{$e_H^T = 0,\; \varepsilon_H^T = \varnothing$}
%         Store $e_H^t$ in $E_H$\;
        
%         Fit $\varepsilon_H^t$ to $e_H^t$\;
%     }
%     \BlankLine
%     \tcp{up-cycle}
%     \For{$k \gets k_{\min}+1$ \textbf{to} $k_{\max}$}{
%         $e_k^t \leftarrow \mathcal{P}^t(e_{k-1}),\; \forall\,t \in \mathcal{T}_k$\;
        
%         Update $V_k^t \leftarrow V_k^t + e_k^t$\;
%     }
%     \tcp{post smoothing (for all h's)}
    
%     $\texttt{target}, \pi^t \leftarrow \mathcal{O}_{h}(V_h^{t+h})$ (initialized with $\pi^t$)
    
%     Fit $V_h^t$ to $\texttt{target}$ and replace $\pi[t]$ in $\Pi[h]$\;
% }
% \end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.15in}
\section{Empirical Validation}\label{sec:validation}
\vspace{-0.1in}
We introduce Hexner's game~\citep{hexner1979differential} that has an analytical Nash equilibrium. We use variants of this game to compare CAMS with existing baselines (MMD, CFR+, JPSPG, and DeepCFR) on solution quality and computational cost. We also demonstrate the scalability of CAMS using a high-dimensional version of the game in App.~\ref{app:hexner_high_dim}. 
\vspace{-0.15in}
\subsection{Hexner's game}
\vspace{-0.08in}
In Hexner's game, the dynamics is decomposed as $\dot{x}_j = A_j x_j + B_j u_j$
for $j=[2]$, where $x_j \in \mathcal{X}_j$, $u_j \in \mathcal{U}_j$, and $A_j$ and $B_j$ are known matrices. The target state of P1 is $z\theta$ where $\theta$ is drawn with distribution $p_0$ from $\Theta$, $|\Theta| = I$, and $z \in \mathbb{R}^{d_x}$ is fixed and common knowledge. Denote by $\eta_i(t)$ and $\zeta(t)$ the random actions at time $t$ induced by strategy pair $(\eta_i, \zeta)$. The expected payoff to P1 is:
\vspace{-0.1in}
\begin{equation} \label{eq:hexcost}
\small
\begin{aligned}
    J(\{\eta_i\}, \zeta) = &\mathbb{E}_{i \sim p_0} \left[\int_0^{T} (\eta_i(t)^\top R_1 \eta_i(t) - \zeta(t)^\top R_2 \zeta(t)) dt\right. \\
    &+ \left.\left[x_1(T) - z\theta_i\right]^\top K_1(T) \left[x_1(T) - z\theta_i\right]\right. \\
    &- \left[x_2(T) - z\theta_i\right]^\top K_2(T) \left[x_2(T) - z\theta_i\right]\Bigg],
\end{aligned}
\vspace{-0.05in}
\end{equation}
% \vspace{-0.08in}
% \setlength{\columnsep}{0.1in}
% \begin{wrapfigure}{r}{3.8cm}
%     \vspace{-0.32in}
%     % \hspace{-0.15in}
%     \centering
%     \includegraphics[width=\linewidth]{figures/homing.pdf}
%     \vspace{-0.25in}
%     \caption{Sample equilibrium in Hexner's game. Magenta circles are target states with $p_0=[0.5,0.5]^\top$. Public belief is kept $p_0$ until $t_r$ and becomes $[1,0]^\top$ when P1 starts to move to the true target.}
%     \label{fig:homing}  
%     \vspace{-0.1in}
% \end{wrapfigure}
where $R_1, R_2 \succ 0$ are control-penalty matrices and $K_1, K_2 \succeq 0$ are state-penalty matrices. Essentially, the goal of P1 is to get closer to the target $z\theta$ than P2. To take full information advantage, P1 needs to decide when to home-in to and thus reveal the target. See Fig.~\ref{fig:overview_fig}(c) for an illustration.
As explained in \citet{hexner1979differential} and \citet{ghimire24a}, this game has an analytical solution: There exists a problem-dependent critical time $t_r := t_r(T, \{A_j\}, \{B_j\}, \{R_j\}, \{K_j\})$, if $t_r \in (0, T)$, P1 homes towards the mean target $\mathbb{E}[\theta]$ as if he does not know the actual target until $t_r$. If $t_r \leq 0$, P1 homes towards the actual target at $t=0$. P2's strategy is to follow P1.  
\vspace{-0.15in}
\subsection{Comparisons on 1- and 4-stage Hexner's games}
\vspace{-0.1in}
\paragraph{Settings.} We first use a normal-form Hexner's game with $\tau = T$ and a fixed initial state $x_0 \in \mathcal{X}$ to demonstrate that baseline algorithms suffer from increasing costs along the size of the discrete action space while CAMS does not. The baselines we consider include CFR+~\citep{tammelin2014solving},  MMD~\citep{sokota2022unified}, Joint-Perturbation Simultaneous Pseudo-Gradient (JPSPG)~\citep{martin2024joint}, and a modified CFR-BR~\citep{johanson2012finding} (dubbed CFR-BR-Primal), where we only compute P2's best response to P1's current strategy and only focus on converging P1's strategy, which matches with CAMS for solving \ref{eq:opt_prob}. Among these, only JPSPG can handle continuous action spaces. All baselines (except JPSPG) are implemented in OpenSpiel~\citep{LanctotEtAl2019OpenSpiel}.
% and Deep CFR~\citep{brown2019deep} as baselines, with the former (resp. latter) for games with shorter (resp. longer) time horizons. 
The normal-form primal game has a trivial ground-truth strategy where P1 goes directly to his target.
% $\theta$: $\tilde{\theta}_1 = \theta$ and $\tilde{\theta}_2 = 0$. 
% Both players follow the same double-integrator dynamics. 
For visualization, we use $d_x = 4$ (position and velocity in 2D). 
% The range of acceleration is set to $\mathcal{U}_1 = \mathcal{U}_2 = [-12, 12]^2$. 
For baselines (except JPSPG), 
% For tabular CFR+
% , we only solve a shorter horizon ($T=0.1$s, with time-discretization $\tau=0.1$s) game with only one decision node.   
% However, we provide CFR+, MMD, and CFR-BR-Primal a ``headstart'' by restricting the action space to a smaller subset based on GT solution. 
we use discrete action sets defined by 4 lattice sizes so that $U = |\mathcal{U}_j| \in \{16, 36, 64, 144\}$. 
% Note that in CAMS, we do not need to discretize the action space. 
All algorithms terminate when a threshold of NashConv\footnote{See \citet{lanctot2017unified} for a definition of NashConv.} is met. For conciseness, we only consider solving P1's strategy and thus use P1's $\delta$ in NashConv. We set the threshold to $10^{-3}$ for baselines and $10^{-5}$ for CAMS. We will show that even with a more stringent threshold, CAMS still converges significant faster than the baselines.  
%of that game 
% Exploitability is computed every $n$ iterations, where $n = \{10, 20, 40, 80\}$ for the respective action space size. 
We then use DeepCFR and JPSPG as baselines for a Hexner's game with 4 time steps, where $T=1$ and $\tau = 0.25$. 
% We discretize the same action space as before with sizes $|\mathcal{A}_j| \in \{9, 16\}$. 
DeepCFRs were run for 1000 CFR iterations (resp. 100) with 10 (resp. 5) traversals for $U = 9$ (resp. $16$). More details on experiment settings can be found in App.~\ref{sec:DeepCFR}. Similarly, JPSPG was run for $2\cdot 10^8$ iterations, where each iteration consisted of solving a game with a random initial state and type, and performing a strategy update. More details in App.~\ref{app:jpspg}.

\vspace{-0.2in}
\paragraph{Comparison metrics.} For the normal-form game, we compare both computational cost and the expected action error $\varepsilon$ from the ground-truth action of P1:
$\varepsilon(x_0):=\mathbb{E}_{i \sim p_0}\left[\sum_{k=1}^{|\mathcal{U}|} \alpha_{ki} \|u_{k} -u^*_i(x_0)\|_2\right]$, where $u^*_i(x_0)$ is the ground truth for type $i$ at $x_0$. 
% We first compute the expected action for each P1 type $\mathbb{E}(u_A) =  \pi_A \mathcal{A}_1$, $\mathbb{E}(u_B) = \pi_B \mathcal{A}_1$, and then compute the mean of the two $L_2$ errors between ($\mathbb{E}(u_A)$, $\bar{u}_A$), and ($\mathbb{E}(u_B)$, $\bar{u}_B$), where $A$ and $B$ denote the two types and $\bar{u}$ denotes the ground-truth.
% We compute the expected action from the policy for each P1 type and compare it against the ground truth action. 
For the 4-stage game, we compare the expected action errors at each time step: $\bar{\varepsilon}_t:=\mathbb{E}_{x_t \sim \pi} [\varepsilon(x_t)]$, where $\pi$ is the strategy learned by DeepCFR, JPSPG, or CAMS.
% the distance to ground truth actions between DeepCFR and CAMS at all decision nodes in the game. 
For each strategy, we estimate $\{\bar{\varepsilon}_t\}_{t=1}^4$ by generating 100 trajectories with initial states uniformly sampled from $\mathcal{X}$. The wall-time costs for game solving are 17 hours using CAMS (baseline), 24 hours for JPSPG, 29 hours ($U=9$) and 34 hours ($U=16$) using DeepCFR, all on an A100 GPU. 
%  polish above later
\vspace{-0.18in}
\paragraph{Results.} Fig.~\ref{fig:cfr+_compare} summarizes the comparisons. For the normal-form game, all baselines (except JPSPG) have complexity and wall-time costs increasing with $U$, while CAMS is invariant to $U$. With the similar or less compute, CAMS achieves significantly better strategies than DeepCFR and JPSPG in the 4-stage game. Sample trajectories for the 4-stage game are shown in App.~\ref{app:baselines}.
\vspace{-0.05in}
\begin{figure}
    \centering
    % \includestandalone[width=\linewidth]{figures/time_compare}
    % \includegraphics[width=\linewidth]{figures/compare_w_cfrs}
    % \includegraphics[width=\linewidth]{figures/compare_all}
    \includegraphics[width=\linewidth]{figures/baseline_w_jpspg_new.pdf}
    \vspace{-0.35in}
    \caption{(a-c) Comparisons b/w CAMS (ours), JPSPG, CFR+, MMD, CFR-BR-Primal on 1-step Hexner's game. (d) Comparison b/w CAMS, JPSPG, and DeepCFR on 4-stage Hexner's w/ similar compute.}
    \label{fig:cfr+_compare}
    \vspace{-0.28in}
\end{figure}
%%%%%%%%%%%%%%%% MOVED TO APPENDIX %%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[!h]
%     \centering
%     \vspace{-0.1in}
%     \includegraphics[width=\linewidth]{figures/4_stage_cams_dcfr_new.pdf}
%     \vspace{-0.25in}
%     \caption{Trajectories generated using CAMS (primal game) and DeepCFR. The initial position pairs are marked with same marker and the final with star. The trajectories from CAMS are close to the ground-truth while DeepCFR fails to converge with even more compute.}
%     \label{fig:4_stage_trajs}
%     \vspace{-0.3in}
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Results}
\begin{figure*}[!h]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=.98\linewidth]{figures/primal_games_fix.pdf}
        % \includegraphics[width=0.5\linewidth]{figures/primal_games_2_by_2.pdf}
        % \subcaption{}
        % \label{fig:primal_games}
    \end{minipage}
    \begin{minipage}{\textwidth}
        \centering        \includegraphics[width=.98\linewidth]{figures/primal_dual_games_new.pdf}
        % \includegraphics[width=0.5\linewidth]{figures/primal_dual_games_2_by_2.pdf}
        % \subcaption{}
        % \label{fig:primal_dual}
    \end{minipage}
    \vspace{-0.15in}
    \caption{Sample trajectories for the primal game (a-d) where P1 plays Nash and P2 plays best response, and primal-dual game (e-h) where both players play Nash. Cols 1 \& 2 are unconstrained, cols 3 \& 4 are w/ collision constraint. Dotted lines are ground-truth Nash. Color shades indicate evolution of public belief ($1$ means Goal-1). Initial position pairs are marked with same markers.}
    \label{fig:primal_and_primal-dual}
    \vspace{-0.2in}
\end{figure*}
\vspace{-0.1in}
\subsection{Scalability of CAMS} 
\vspace{-0.08in}
\paragraph{10-stage game.} Here we solve Hexner's games with $T=1$ and $\tau =0.1$, and consider both state-constrained and unconstrained cases. These games have a game-tree complexity of $10^{80}$ if we use an action discretization of $U=10k$ (100 discrete values along each of the two action dimensions). In the state-constrained version of the game, P1 receives $+\infty$ if he collides with P2. Collision occurs when the Euclidean distance between the players is less than $0.05$. As a result, the Nash equilibrium of this game variant is no longer analytical. Following \cite{ghimire24a}, we approximate a time-dependent safe zone $\Omega_t \subseteq \mathcal{X}$ for P1 so that for any initial state outside of $\Omega_t$, P1 surrenders because P2 can always find a strategy to collide. Within $\Omega_t$, the Nash equilibrium can be derived from CAMS where for each minimax problem, P1's admissible actions are restricted by $\Omega_t$. In \cite{ghimire24a}, the resultant constrained minimax problems are solved as follows: First, at each $t$, non-revealing games (without splitting) are approximately solved across $\mathcal{X} \times \Delta(I)$ using an enumeration over $U=100$. This requires finding the minimax point from a $100 \times 100$ matrix for each $(x,p)$. Then with the resultant values for the non-revealing games, the convex hull of the value over the public belief is approximated for each sampled $x$, before fitting a neural network $\hat{V}_t$ to these approximated convex hulls. Due to the use of enumeration, this method has exponential space and computational complexities with respect to the dimensionalities of the action spaces. In this paper, we solve \ref{eq:opt_prob} and \ref{eq:opt_prob_dual} which directly approximates the convexified values. In addition, since DS-GDA is gradient-based, the resultant space and computational complexities are only linear to the dimensionality of the action spaces.

\vspace{-0.06in}
\textbf{Results:} Results are summarized in Fig.~\ref{fig:primal_and_primal-dual}. For the unconstrained game where analytical strategies are known, we compare the approximated and the ground-truth strategies starting from various initial states. While approximation errors exist, CAMS successfully learns the target-concealing behavior of P1 as P1 always moves towards $\mathbb{E}_{p_0}[z\theta]$ before revealing his target. Averaging over 50 trajectories derived from CAMS, P1 conceals the target until $t_r = 0.60s \pm 0.06s$ (compared to the ground-truth $t_r = 0.5s$). CAMS also approximates P2's robust strategy well, as P2 only starts to home towards a target after P1 reveals.
We note that the complexity of the dual game is higher than that of the primal game because its value is one dimension higher and \ref{eq:opt_prob_dual} is larger than \ref{eq:opt_prob}. This resulted in higher error in approximating P2's strategies.
% may be we move this to appendix too!
% \paragraph{3D Hexner's game.} To demonstrate the scalability of CAMS, we solve a 3D Hexner's game where the joint action space is now 6D. Accordingly, the state space becomes 12D and the value becomes 13D. Resultant trajectories are visualized in Fig.~\ref{fig:3d_case}. Similar to the 2D case, P1 learns to correctly conceal his target until some critical time. 
% \begin{figure}[!h]
% \vspace{-0.05in}
%     \centering
%     \includegraphics[width=\linewidth]{figures/3d_case_new.pdf}
%     \vspace{-0.2in}
%     \caption{3D Hexner's Game. Color shades indicate the current public belief.}
%     \label{fig:3d_case}
%     \vspace{-0.2in}
% \end{figure}
% \vspace{-0.15in}
\vspace{-0.15in}
\subsection{Accelerating value approximation with multigrid}
\vspace{-0.08in}
Here we demonstrate the efficacy of multigrid methods (see Alg.~\ref{alg:multigrid} and Alg.~\ref{alg:n_multigrid} in App.~\ref{app:multigrid}) in accelerating value function approximation. We report the runtime\footnote{Experiments done on one H100 GPU} of all algorithms (Algs.~\ref{alg:cams}, ~\ref{alg:multigrid}, ~\ref{alg:n_multigrid}) on 4-, 10-, and 16-stage games in Tab.~\ref{tab:runtime_multigrid}. We run the 2-level multigrid (Alg.~\ref{alg:multigrid}) on the 4- and 10-stage games, and 4-level multigrid (Alg.~\ref{alg:n_multigrid}) on the 16-stage game. We also report the resulting trajectories in App.~\ref{app:multigrid}.
\vspace{-0.25in}
\begin{table}[!h]
    % \vspace{-0.3in}
    \centering
        \caption{Runtime Comparison: CAMS w/ and w/o Multigrid}
        % \vspace{-0.1in}
    \begin{tabularx}{\linewidth}{ X | X | X}
    \hline
         \# time steps & w/o multigrid & w/ multigrid $\downarrow$\\
         \hline
         4 & 9.3 hrs & 2.32 hrs\\
         10 & 27.6 hrs & 10.9 hrs\\
         16 & 46.21 hrs & 17.83 hrs\\
    \hline
    \end{tabularx}
    \label{tab:runtime_multigrid}
    \vspace{-0.2in}
\end{table} 
\vspace{-0.1in}
\section{Conclusion}
\vspace{-0.1in}
Unlike IIEFGs where mixed strategies have to be approximated over the entire action space across the game tree, we showed that differential games with one-sided payoff information enjoy a much simpler strategy structure when the Isaacs' condition holds: The strategy of the informed (resp. uninformed) player has at most $I$ (resp. $I+1$) pure action branches at each infostate. We demonstrated the clear advantage of using this structural property in solving games with continuous action spaces, against SOTA IIEFG solvers, in terms of computational cost and solution quality. We also showed that multigrid further accelerates value and strategy approximation. To the authors' best knowledge, this is the first method to provide tractable solution for incomplete-information games with continuous action spaces without problem-specific abstraction and discretization.

\section*{Impact Statement}
This work is concerned with bridging the gap between computational game theory and differential game theory. With its possible applications to robotics and AI, there is a need for studies on mitigating risks arising from deceptive strategies by robots and machines against human beings. 
\bibliography{iclr2025_conference}
\bibliographystyle{icml2025}

\onecolumn
\appendix

\section{Proof of Theorem \ref{thm:convergence}}
\label{app:convergence}
\paragraph{Theorem \ref{thm:convergence}} For any $(t_0,x_0,p) \in [0,T]\times \mathcal{X} \times \Delta(I)$, if A1-5 hold, then there exist $M_1, M_2>0$, such that $V(t_0,x_0,p) \leq \max_{\zeta \in \mathcal{Z}(t_0)} J(t_0,x_0,p;\{\eta_{i,{\tau}}\}, \zeta) \leq V(t_0,x_0,p) + M_1(T-t_0)\tau$. Similarly, for any $(t_0,x_0,\hat{p}) \in [0,T]\times \mathcal{X} \times \mathbb{R}^I$, $V^*(t_0,x_0,\hat{p}) \leq \max_{\{\eta_i\} \in \{\mathcal{H}^i(t_0)\}^I} J^*(t_0,x_0,\hat{p};\{\eta_i\}, \zeta_{\tau}) \leq V^*(t_0,x_0,\hat{p}) + M_2(T-t_0)\tau$.
\begin{proof}
    Our proof completes the sketch for Theorem 4.1 in \citet{cardaliaguet2009numerical}. For the primal game, by definition we have $V(t_0,x_0,p) \leq \max_{\zeta \in \mathcal{Z}(t_0)} J(t_0,x_0,p;\{\eta_{i,{\tau}}\}, \zeta)$. So we just need to prove 
    \begin{equation}\label{eq:convergence_proof1}
       \max_{\zeta \in \mathcal{Z}(t_0)} J(t_0,x_0,p;\{\eta_{i,{\tau}}\}, \zeta) \leq V(t_0,x_0,p) + M_1(T-t_0)\tau.
    \end{equation}
    % To do so, we first note since the system dynamics is Lipschitz continuous, there exists $L_1>0$ and $w \in \text{Co}(\{f(x,u,v)|v \in \mathcal{V}\})$ (where $\text{Co}(\mathcal{S})$ is the convex hull of set $\mathcal{S}$) such that for any $(t,x,u,v) \in [0,T]\times \mathcal{X} \times \mathcal{U} \times \mathcal{V}$,
    % \begin{equation*}
    %     \| X^{t,x,u,v}_{t+\tau} - (x + \tau w)\|_2 \leq L_1 \tau^2.
    % \end{equation*}
    
    % To do so, we first cite the following result from the value existence proof for 2p0s differential games with incomplete payoff information (Lem.4.2 in \citep{cardaliaguet2007differential}): 
    

    For some $(t_0,x_0,p)$, let $t_1 = t_0 +\tau$, and $v^\dagger$ be the ground truth equilibrium action at $(t_0,x_0,p)$. Given some $u \in \mathcal{U}$, let $x^\dagger(u) := X^{t_0,x_0,u,v^\dagger}_{t_1}$. Denote by $v_0$ the solution to $\max_{v \in \mathcal{V}} V(t_1,x'(u,v),p) + L(u,v,p)$ and $x_1 = X^{t_0,x_0,u,v_0}_{t_1}$. And let $L(u,v,p):= \mathbb{E}_{i \sim p}[\int_{t_0}^{t_1} l_i(u,v)ds]$ be the expected running cost in $[t_0,t_1]$. We first show that with some $L_1, L_2>0$
    \begin{equation}
        |V(t_1,x_1,p) + L(u,v_0,p) - V(t_1,x^\dagger(u),p) - L(u,v^\dagger,p)| \leq L_1L_2 \tau^2.
        \label{eq:convergence_proof2}
    \end{equation}

    To do this, we note that for a small enough $\tau$, 
    \begin{equation}
    \begin{aligned}
        V(t_1,x^\dagger(u),p) & = V(t_1,x_1,p) + \nabla_{x} V|_{x_1} (x^\dagger(u) - x_1) + (x^\dagger(u) - x_1)^\top \nabla^2_x V|_{x_1}(x^\dagger(u) - x_1) \\
        & = V(t_1,x_1,p) + \nabla_{x} V|_{x_1} \nabla_{v} x_1|_{v_0} (v^\dagger - v_0) + (x^\dagger(u) - x_1)^\top \nabla^2_x V|_{x_1}(x^\dagger(u) - x_1)
    \end{aligned}
    \end{equation}
    and 
    \begin{equation}
        L(u,v^\dagger,p) = L(u,v_0,p) + \nabla_v L|_{v_0} (v^\dagger - v_0).
    \end{equation}
    From the definition of $v_0$, we have
    \begin{equation}
        \nabla_{v} (V+L)|_{v_0} = \nabla_{x} V|_{x_1} \nabla_{v} x_1|_{v_0} + \nabla_v L|_{v_0} = 0.
    \end{equation}
    Together with the assumptions that $V$ is $L_1$-smooth and the dynamics $f$ is $L_2$-Lipschitz continuous (A2-3), we get Eq.~\ref{eq:convergence_proof2}. Equivalently, we have
    \begin{equation}
        V(t_1,x_1,p) + L(u,v_0,p) \leq  V(t_1,x^\dagger(u),p) + L(u,v^\dagger,p) + L_1L_2 \tau^2.
        \label{eq:convergence_proof3}
    \end{equation}
    Since Eq.~\ref{eq:convergence_proof3} holds for any $u \in \mathcal{U}$, we have
    \begin{equation}
        V(t_1,x_1,p) + L(u,v_0,p) \leq  V(t_0,x_0,p) + L_1L_2 \tau^2.
        \label{eq:convergence_proof4}
    \end{equation}
    
    

    
    % and $u^{\dagger}$ be the $\epsilon$-optimal solution to
    % \begin{equation*}
    %      \min_{u \in \mathcal{U}} \max_{\zeta \in \mathcal{Z}(t_0)} V(t_1,x^u,p) + L_\tau(u,\zeta;p).
    % \end{equation*}
    % Then for any strategy $\zeta \in \mathcal{Z}(t_0)$, we have 
    % \begin{equation}
    %     V(t_0,x_0,p) \leq V(t_1,x^{u^{\dagger}},p) + L_\tau(u^{\dagger},\zeta;p) + 2\epsilon.
    %     \label{eq:existenceproof}
    % \end{equation}
    Let $x^u = X^{t_0,x_0,\eta,\zeta(u)}_{t_1}$,
    $\bar{\eta}_{\tau}(u)= \sum_{i\in[I]} \eta_{i,\tau}(u)p[i]$ be the marginal of taking action $u$, and $p^u[i] = \eta_{i,\tau}(u) p[i]/\bar{\eta}_{\tau}(u)$ be the updated belief after observing action $u$. 
    % If $\{\eta_{i,{\tau}}\} \in \{\mathcal{H}_r^i(t_0)\}^I$ is an $\epsilon$-optimal strategy to Eq.~\ref{eq:primal-subdp}, i.e., $\{\eta_{i,{\tau}}\}$ approximately solves
    % \begin{equation*}
    %      \min_{\{\eta_{i}\}\in \{\mathcal{H}_r(t_0)^i\}^I} \max_{\zeta \in \mathcal{Z}(t_0)} \int_{\mathcal{U}} \bar{\eta}_\tau(u)\Big ( V(t_1,x^u,p^u) + L_\tau(u,\zeta;p^u) \Big)du.
    % \end{equation*}
    % Then from Eq.~\ref{eq:existenceproof}, we have
    % \begin{equation}
    % \label{eq:convergence_proof2}
    %     V(t_0,x_0,p) \leq \int_{\mathcal{U}} \bar{\eta}_{\tau}(u) \Big ( V(t_1,x^u,p^u) + L_{\tau}(u,\zeta;p^u) \Big )du + 2\epsilon.
    % \end{equation}
    % Since $\epsilon$ can be made arbitrarily small, we use $\epsilon = \tau^2$. 
    
    Now we prove Eq.~\ref{eq:convergence_proof1} by backward induction. At $t=T$, Eq.~\ref{eq:convergence_proof1} holds due to the terminal boundary. Let us assume that it holds true for some $t_1 = t_0 + \tau \in (0,T]$. Let $(x_0,p)$ be fixed. For any $\zeta \in \mathcal{Z}(t_0)$, we have
    \begin{equation}\label{eq:convergence_proof5}
    \begin{aligned}
        J(t_0,x_0,p;\{\eta_{i,\tau}\},\zeta) & = \sum_{i \in [I]} p[i] \mathbb{E}_{\{\eta_{i,\tau}\}}[J_i(t_0,x_0;\eta_{i,\tau},\zeta)] \\
        & = \sum_{i \in [I]} p[i] \int_{\mathcal{U}} \eta_{i,\tau}(u) \Big( \mathbb{E}_{\{\eta_{i,\tau}^u\}}[J_i(t_1,x^u;\eta_{i,\tau}^u,\zeta^u)] + \int_{t=t_0}^{t_1} l_i(u,\zeta(u)) \Big) du \\
        & \leq \int_{\mathcal{U}} \bar{\eta}_{\tau}(u) \max_{\zeta' \in \mathcal{Z}(t_1)} \sum_{i \in [I]} p^u[i] \Big(\mathbb{E}_{\{\eta_{i,\tau}^u\}}[J_i(t_1,x^u;\eta_{i,\tau}^u,\zeta')] + \int_{t=t_0}^{t_1} l_i(u,\zeta(u)) \Big) du.
    \end{aligned}
    \end{equation}
    Here $(\eta_{i,\tau}^u, \zeta^u)$ is the strategy pair taken at $(t_1, x^u, p^u)$.

    From the induction assumption we have:
    \begin{equation*}
        \max_{\zeta' \in \mathcal{Z}(t_1)} \sum_{i \in [I]} p^u[i] \mathbb{E}_{\{\eta_{i,\tau}^u\}}[J_i(t_1,x^u;\eta_{i,\tau}^u,\zeta')] \leq V(t_1, x^u, p^u) + M_1(T-t_1) \tau.
    \end{equation*}

    Incorporating this and Eq.~\ref{eq:convergence_proof4} into Eq.~\ref{eq:convergence_proof5} to have
    \begin{equation}
    \begin{aligned}
        J(t_0,x_0,p;\{\eta_{i,\tau}\},\zeta) & \leq \int_{\mathcal{U}} \bar{\eta}_{\tau}(u) \Big ( V(t_1, x^u, p^u) 
        + \sum_{i\in[I]} p^u[i]\int_{t=t_0}^{t_0+\tau} l_i(u,\zeta(u)) \Big) du + M_1(T-t_1) \tau \\
        & \leq \int_{\mathcal{U}} \bar{\eta}_{\tau}(u) \Big ( V(t_0, x_0, p) 
        + L_1L_2\tau^2 \Big) du + M_1(T-t_1) \tau
    \end{aligned}
    \end{equation}
    Setting $M_1 = L_1L_2$ to have
    \begin{equation}
        J(t_0,x_0,p;\{\eta_{i,\tau}\},\zeta) \leq V(t_0, x_0, p) + M_1(T-t_0)\tau.
        \label{eq:convergence_proof6}
    \end{equation}
    Since Eq.~\ref{eq:convergence_proof6} holds for all $\zeta \in \mathcal{Z}(t_0)$, we get Eq.~\ref{eq:convergence_proof1}. The same technique applies to the dual value.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem \ref{thm:splitting}}
\label{app:splitting}
\paragraph{Theorem \ref{thm:splitting}} (A splitting reformulation of the primal and dual DPs) 
    The RHS of Eq.~\ref{eq:primal-subdp} can be reformulated as
        \begin{equation}\label{eq:opt_prob} \tag{$\text{P}_1$}
        \small
        \begin{aligned}
            &\min_{\{u^k\}, \{\alpha_{ki}\}} \max_{\{v^k\}} \quad \sum_{k=1}^{I}\lambda^k \Big(V(t+\tau, x^k, p^k) + \tau \mathbb{E}_{i \sim p^k} [l_i(u^k, v^k)]\Big)\\
            &\text{s.t.} \quad u^k \in \mathcal{U}, \quad x^k = \text{ODE}(x, \tau, u^k, v^k; f), \quad v^k \in \mathcal{V}, \\
            &\quad \quad \alpha_{ki} \in [0, 1],\; \sum_{k=1}^I \alpha_{ki} = 1,\; \lambda^k = \sum_{i=1}^I \alpha_{ki} p[i], \\
            &\quad \quad p^k[i] = \frac{\alpha_{ki}p[i]}{\lambda^k}, \quad \forall i, k \in [I].
        \end{aligned}
        \end{equation}
    And the RHS of Eq.~\ref{eq:dual-subdp} can be reformulated as
    \begin{equation}\label{eq:opt_prob_dual} \tag{$\text{P}_2$}
    \small
    \begin{aligned}
        &\min_{\{v^k\}, \{\lambda^{k}\}, \{\hat{p}^k\}} \max_{\{u^k\}} \sum_{k=1}^{I+1}\lambda^k \left(V^*(t+\tau, x^k, \hat{p}^k - \tau l(u^k, v^k))\right)\\
        &\text{s.t.} \quad u^k \in \mathcal{U}, \quad v^k \in \mathcal{V}, \quad x^k = \text{ODE}(x, \tau, u^k, v^k; f),\\
        &\quad \quad \lambda^{k} \in [0, 1],\; \sum_{k=1}^{I+1} \lambda^{k}\hat{p}^k = \hat{p}, \; \sum_{k=1}^{I+1}\lambda^k = 1,\; k \in [I+1].\\
    \end{aligned}
    \end{equation}
\begin{proof} Recall that the primal DP is: 
\begin{equation}
\begin{aligned}
    V_\tau(t_0,x_0,p) & = \min_{\{\eta_i\} 
    } \mathbb{E}_{u \sim \bar{\eta}}\left[
    \max_{v \in \mathcal{V}} V_\tau(t_0 + \tau, x'(u,v), p'(u)) + \tau \mathbb{E}_{i\sim p'(u)} [l_i(u,v)] \right] \\
    & = \min_{\{\eta_i\} 
    } \int_{\mathcal{U}} \bar{\eta}(u) 
    \max_{v \in \mathcal{V}} V_\tau(t_0 + \tau, x'(u,v), p'(u)) + \tau \mathbb{E}_{i\sim p'(u)} [l_i(u,v)] du \\
    & = \min_{\{\eta_i\} 
    } \int_{\mathcal{U}} \bar{\eta}(u) 
    a(u,p'(u)) du, \quad \Big(a(u,p'(u)) = \max_{v \in \mathcal{V}} V_\tau(t_0 + \tau, x'(u,v), p'(u)) + \tau \mathbb{E}_{i\sim p'(u)} [l_i(u,v)]\Big)
\end{aligned}
\end{equation}  
Now we introduce a pushforward measure $\nu$ on $\Delta(I)$ for any $E \subset \Delta(I)$:
$\nu(E) = \int_{\{\,u: p'(u)\in E\}} \bar{\eta}(u)\,du$. Let $\eta_{p'}$ be the conditional measure on $\mathcal{U}$ for each $p'$. Then we have
\begin{equation*}
\begin{aligned}
    \min_{\{\eta_i\}} \int_{\mathcal{U}} \bar{\eta}(u) 
    a(u,p'(u)) du & = \min_{\nu} \int_{\Delta(I)} \min_{\eta_{p'}} \Bigl[\int_{p'(u)=p'} a(u,p')\,\eta_{p'}(du)\Bigr]\nu(dp') \\ 
    & = \min_{\nu} \int_{\Delta(I)} \min_{u \in \mathcal{U}} a(u,p') \nu(dp') \\
    & = \min_{\nu} \int_{\Delta(I)} \tilde{a}(p') \nu(dp').
\end{aligned}
\end{equation*}
This leads to the following reformulation of $V_\tau$:
\begin{equation}\label{eq:primal_reformulation}
\begin{aligned}
    V_\tau(t_0,x_0,p) = & \min_{\nu} \int_{\Delta (I)} \tilde{a}(p')\nu(dp') \\
    & \text{s.t.} \quad \mathbb{E}_{\nu} [p'] = p.
\end{aligned}
\end{equation}
One easily notice that the RHS of Eq.~\ref{eq:primal_reformulation} computes the convexificiation of $\tilde{a}(p')$ at $p'=p$. Since convexification in $\Delta(I)$ requires at most $I$ vertices, $\nu^*$ that solves Eq.~\ref{eq:primal_reformulation} is $I$-atomic. We will denote by $\{p^k\}_{k\in[I]}$ the set of ``splitting'' points that has non-zero probability mass according to $\nu^*$, and let $\lambda^k := \nu^*(p^k)$. Using Isaacs' condition (A4), $\argmin_{u \in \mathcal{U}} a(u,p)$ is non-empty for any $p\in\Delta(I)$, and therefore each $p^k$ is associated with (at least) one action in $\argmin_{u \in \mathcal{U}} a(u,p^k)$. As a result, $\{\eta_i\}$ is also concentrated on a common set of $I$ actions in $\mathcal{U}$. Specifically, denote this set by $\{u^k\}_{k\in[I]}$, we should have $\alpha_{ki} := \eta_i(u^k) = \lambda^k p^k[i]/p[i]$. Thus we reach \ref{eq:opt_prob}. The same proof technique can be applied to the dual DP to derive \ref{eq:opt_prob_dual}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Connection between Value Convexification and Nash Equilibrium in Incomplete-Information Games} \label{app:repeated}

Here we explain the construction of Nash equilibrium as a consequence of value convexification. For ease of exposition, we will use examples from a simplistic setting: repeated normal-form games with one-sided information. We also walk through the computation of strategies for the informed and uninformed players for the given examples. We refer readers to \cite{aumann1995repeated,de1996repeated, sorin2002first} for more details on the theoretical development. 

% \paragraph{Non-revealing, revealing, and partially revealing strategies} 
Consider two normal-form zero-sum payoff tables given by matrices $G_1$ and $G_2$ as shown in Eq.~\ref{eq:appendix_payoff}. P1 is the row player with actions $\{U, D\}$ and P2 the column player with actions $\{L, R\}$. At the beginning of the game, nature picks game $G_1$ with probability $p$ and communicates that only to P1. P2 only knows the probability $p$. Both players pick their actions and announce them simultaneously for that round without knowing the resultant payoff. This process is repeated until the end of the game, at which point the average payoff is revealed. The game can be repeated either finitely or infinitely. For conciseness, we only discuss the latter case. To align the discussion with literature on repeated games, we will consider P1 maximize, rather than minimize, the payoff. We call this game $G(p)$.

\begin{equation}\label{eq:appendix_payoff}
G_1 = \begin{bNiceMatrix}[first-row, last-row, first-col, last-col]
    & L & R & \\
U & 1 & 0 & \\
D & 0 & 0& \\
    &     &     & 
\end{bNiceMatrix} \hspace{0.5in}
G_2 = \begin{bNiceMatrix}[first-row, last-row, first-col, last-col]
    & L & R & \\
U & 0 & 0 & \\
D & 0 & 1 & \\
    &     &     & 
\end{bNiceMatrix}
\end{equation}
Let us assume, for simplicity, $p=0.5$, and that the game being played is $G_1$. Since P1 knows that $G_1$ is the game, he could play $U$ every time, as $D$ would otherwise lead to a payoff of zero. However, as the game progresses, P2 will be able to deduce that $G_1$ is the game being played, forcing her to always play $R$, which guarantees a payoff of zero. 
Similarly, if $G_2$ is selected, and P1 always plays $D$, P2 will eventually figure out the true game, and guarantee a payoff of zero in the remainder of the game. 
In this particular game, P1 can improve his expected payoff by ignoring the actual game type. Then players play a complete-information game given by the expected payoff matrix $\Bar{G}(p) = 
\begin{bNiceMatrix}
    \frac{1}{2} & {0} \\
    0 & \frac{1}{2}
\end{bNiceMatrix}$,
for which the optimal strategy for P1 (resp. P2) is to play $\{U, D\}$ (resp. $\{L, R\}$) with probability $0.5$, leading to an expected payoff of $\frac{1}{4}$ to P1 in each round. Notice that by playing this way, P1 conceals the information about which game is being played, i.e., the public belief $p$ is always $0.5$. Thus $\frac{1}{4}$ is the value of the \textit{non-revealing} game and the corresponding strategy is known as the \textit{non-revealing} strategy. 
In the above game, the non-revealing strategy is Nash. 
% One could come up with games (i.e., payoff matrices), where it is actually optimal for P1 to reveal the chosen game right away. 
One can easily see that in the game $(-G_1, -G_2)$, a revealing strategy of P1 will instead be Nash. 

It is important to note that for some games P1 will partially reveal the type information by splitting the belief in the first round. 
% ***add details about this?~\citep{aumann1995repeated}.*** 
% For the purpose of demonstrating the similarities and differences between differential games and repeated games with one-sided incomplete information, we shall focus on a non-trivial example of the 2p0s repeated game in the next section. 
% \paragraph{Primal and dual games}
% tutorial on why 2p0s normal form game with one-sided information has a primal dual formulation.
% We use the following example to explain the primal and dual formulation of 2p0s normal-form games with one-sided information. Consider P1 with two possible payoff tables: For type A, the payoffs are
% P1 has two types -- type A and type B. If P1 is of type A, let the 2p0s normal-form game be
This can be seen from the following game with two possible payoff tables in Eq.~\ref{eq:appendix_payoff2}:
\begin{equation}\label{eq:appendix_payoff2}
G_1 = \begin{bNiceMatrix}[first-col, last-col, first-row]
    &  & P2 &  & \\
     & 0 & 1 & 1 & 3 & \\
    P1    &   &   &   &   & \\
    & 0 & 1 & 0 & 3 & 
\end{bNiceMatrix} \hspace{0.5in}
G_2 = \begin{bNiceMatrix}[first-col, last-col, first-row]
    &  & P2 &  & \\
     & 3 & 0 & 1 & 0 & \\
    P1    &   &   &   &   & \\
    & 3 & 1 & 1 & 0 & 
\end{bNiceMatrix}
\end{equation}
% Since P1 has a private information about his type, his strategy will generally be conditioned on his type unknown to P2. As a result, it is not possible to compute P2's strategy from P1's point of view. P1's strategy is determined using so called primal game and that of P2's using dual game (more on this later). % todo : explain this better 
% \paragraph{Primal game}
Let $p$ be the probability that the chosen game is $G_1$. Then the non-revealing game is defined by: 
\begin{align} \label{eq: normal-form-avg}
    \Bar{G}(p) = \begin{bNiceMatrix}[first-col, last-col, first-row]
    &  & P2 &  & \\
     & 3(1-p) & p & 1 & 3p & \\
    P1    &   &   &   &   & \\
    & 3(1-p) & 1 & (1-p) & 3p & 
\end{bNiceMatrix}.
\end{align}
Let $U(p)$ be the value of the non-revealing game, 
% Then, it is clear that the $u_\infty(p) = u_1(p) = u(p)$. 
and let $V(p)$ be the value of the original game. Theorem 3.2 in \cite{aumann1995repeated} says that $V(p)$ is the concave hull of $U(p)$, i.e., for any $p \in \Delta(I)$ ($I=2$ in this case)
\begin{equation}
    V(p) = \text{Cav}~U(p).
\end{equation}
This is because for any $p \in \Delta(I)$ where $U(p) < \text{Cav}~U(p)$, P1 can play a mixed strategy to achieve an expected payoff of $\text{Cav}~U(p)$, by splitting the public belief to some $I$ vertices in $\Delta(I)$. Once this splitting is done, P1 can keep on playing non-revealing strategy to maintain $\text{Cav}~U(p)$ as his expected payoff. 
% We will elaborate this soon.
% For now, we note that this theorem allows us to study infinitely-repeated games with the help of a single stage of the non-revealing game. 
We elaborate using the example: The value of the non-revealing game $U(p)$ is
\begin{equation}\label{eq:non-rev-val}
    U(p) = \begin{cases} 3p, & 0 \le p \le 2 - \sqrt{3}\\
                           1-p(1-p), & 2 - \sqrt{3} \le p \le \sqrt{3} - 1\\
                           3(1-p), & \sqrt{3} - 1\le p \le 1.
    \end{cases}
\end{equation}
The concavification of the value is given by: 
\begin{equation}\label{eq:p-rev-val}
    \text{Cav}\;U(p) = \begin{cases} 3p, & 0 \le p \le 2 - \sqrt{3}\\
                           6 - 3\sqrt{3}, & 2 - \sqrt{3} \le p \le \sqrt{3} - 1\\
                           3(1-p), & \sqrt{3} - 1\le p \le 1.
    \end{cases}
\end{equation}
Both $U(p)$ and $V(p)$ are visualized in Fig~\ref{fig:repeated_game_value}.
% Eq.~\ref{eq:p-rev-val} is what we call a \textit{primal} game, as played by the informed player P1. 
From the figure, P1 attains maximum value $6-3\sqrt{3}$ at $p = 2-\sqrt{3}$ and $p=\sqrt{3} - 1$. Therefore, P1 can play a mixed strategy to attain the maximum value by announcing a mixed strategy in such a way that the public belief $p$ is updated to either $(2 - \sqrt{3})$ or $(\sqrt{3}-1)$ depending on the action P1 actually takes. This makes P1's strategy \textit{partially revealing} as P2 will not be able to deduce P1's true type. Specifically, for $p=0.5$, and if the actual game is $G_1$, P1 plays the mixed strategy for $\Bar{G}(2-\sqrt{3})$ with the probability $2-\sqrt{3}$ and for  $\Bar{G}(\sqrt{3}-1)$ with probability $\sqrt{3} - 1$; if the actual game is $G_2$, he plays the mixed strategy for $\Bar{G}(2-\sqrt{3})$ with probability $\sqrt{3}-1$ and for $\Bar{G}(\sqrt{3}-1)$ with probability $2-\sqrt{3}$. More generally, for any nature's distribution $p$, P1's strategy is to compute $\lambda \in \Delta(I)$ and $p_i \in \Delta(I)$ such that $\sum_{i=1}^I \lambda[i] u(p^i) = \text{Cav}(U(p))$ and $\sum_{i=1}^2 \lambda_i p^i = p$. Then, given his true type $k$, he plays the maximin strategy for $\Bar{G}(p^i)$ with probability $\lambda_i p^i_k/p_k$. 
% \cite{aumann1995repeated} provides an intuitive explanation to this algorithm and 
\cite{gilpin2008solving} first discussed the nonconvex problem for solving Cav $u$.
%% need to explain this better. 
\begin{figure}[!ht]
    \centering
    \includestandalone[width=.8\linewidth]{figures/repeated_value}
    \caption{Non-revealing game value $u_1$ and its concavification}
    \label{fig:repeated_game_value}
\end{figure}

Next, we need to derive strategy for P2. Unlike P1, P2 has to guess the true game that is being played and hedge against potential manipulation from P1. A good strategy is to play in such a way that she pays the same amount to P1 no matter the type of the game. To do so, P2 plays a game with a vector payoff that contains the amount she pays to P1 for each game types. 

% The game in Eq.~\ref{eq:appendix_payoff2} can be rewritten in :
% \begin{align} \label{eq: vector-payoff}
%     \begin{bNiceMatrix}[first-col, last-col, first-row]
%     &  & P2 &  & \\
%      & (0, 3) & (1, 0) & (1, 1) & (3, 0) & \\
%     P1    &   &   &   &   & \\
%     & (0, 3) & (1, 1) & (0, 1) & (3, 0) & 
% \end{bNiceMatrix}.
% \end{align}
% Since both players can observe each other's actions at the end each stage. 
Consider the game in Eq.~\ref{eq:appendix_payoff2}. By observing P1's action, P2 can keep track of the vector payoffs $(x, y)$ for each stage. If at the beginning of the game P1 chose the last row and P2 chose the last column, then the vector payoff is $(3,0)$.  All possible vector payoffs define vertices in Fig.~\ref{fig:vector-payoffs}. The running average of the vector payoffs (the shaded region in Fig.~\ref{fig:vector-payoffs}) is defined by:
\begin{align*}
    (\xi_n, \eta_n) = \left(\frac{1}{n}(x_1 + x_2 + \dots + x_n), \frac{1}{n}(y_1 + y_2 + \dots + y_n)\right).
\end{align*}
\begin{figure}[!h]
    \centering
    \includestandalone[width=.8\linewidth]{figures/vector_payoff}
    \caption{Game from P2's perspective.}
    \label{fig:vector-payoffs}
\end{figure}
P2 knows that if $G_1$ (resp. $G_2$) is the game, P1 will move the average to the right (resp. top). \cite{blackwell1956analog} first discussed P2's strategy to minimize the average payoff by introducing
% Blackwell provided an analog of the minimax theorem for games with vector payoffs. 
% Specifically, he provides 
the concept of \textit{approachability}: A set $S$ in the payoff vector space is \textit{approachable} for P2 if P2 can adopt a strategy ensuring that the distance of the running vector payoff from $S$ converges to zero with probability one, regardless of P1's strategy.

From the primal game, we know that P1 can guarantee payoff of $6 - 3\sqrt{3}$ (the dashed lines in Fig.~\ref{fig:approachable-set}). To construct the approachable set of P2, consider P1's mixed strategy as $(\pi, 1-\pi)$ and P2's mixed strategy as $(\sigma_1, \sigma_2, \sigma_3, \sigma_4)$. We can determine the expected payoffs to P1: When P2 plays first column, the payoff to P1 is $(0, 3)$, when she plays the second, it is $(1, 1-\pi)$, and so on. Thus, for all possible $(\sigma_1, \sigma_2, \sigma_3, \sigma_4)$, the expected payoffs to P1 is the convex hull of the points $(0, 3), (1, 1-\pi), (\pi, 1-\pi), (3, 0)$. Denote the shaded region in Fig.~\ref{fig:approachable-set} as $S = \{\xi_n, \eta_n: (\xi_n, \eta_n) \le 6 - \sqrt{3}\}$. 

The optimal strategy for P2 is as follows. P2 keeps track of average vector payoff (say $g_n = (\xi_n, \eta_n)$). If $g_n \in S$, then P2 plays arbitrarily. However, if $g_n \notin S$, P2 must project the vector $g_n$ onto the closest point $c = \argmin_{m \in C}||g_n - m||$. P2 then adopts the mixed strategy corresponding to the projection $q = (g_n - c)/||g_n - c|| \in \Delta(K)$ (here, $K=2$), and plays optimally in the game $G(q)$.
\begin{figure}[!h]
    \centering
    \includestandalone[width=0.8\linewidth]{figures/approachable-set}
    \caption{Approachable set (shaded in magenta) of P2}
    \label{fig:approachable-set}
\end{figure}


% still need to work on it
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Connection between Primal and Dual Games}\label{app:primaldual}
Here we continue to use the infinitely-repeated game setting to explain the connection between the primal and the dual games and the interpretation of the dual variable $\hat{p}$. Please see Theorem 2.2 in \cite{de1996repeated} and the extension to differential games in \cite{cardaliaguet2007differential}. 
% We then provide an interpretation of the dual variable $\hat{p} \in \partial V(0,x,p)$ for $(x,p) \in \mathcal{X}\times \Delta(I)$.

% \paragraph{Connection between primal and dual.} (Theorem 2.2 in \cite{***demeyer}) 
Let the primal game be $G(p)$ for $p \in \Delta(I)$, the dual game be $G^*(\hat{p})$ for $\hat{p} \in \mathbb{R}^I$, and let $\{\eta_i\}_{i=1}^I$ be the set of strategies for P1 and $\zeta$ the strategy for P2. $\eta_i \in \Delta(d_u)$ and $\zeta \in \Delta(d_v)$. We note that P1's strategy $\{\eta_i\}_{i=1}^I$ can also be together represented in terms of $\pi := \{\pi_{ij}\}^{I,d_u}$ such that $\sum_j^{d_u} \pi_{ij} = p[i]$ and $\eta_i[j] = \pi_{ij}/p[i]$, i.e., nature's distribution is the marginal of $\pi$ and P1's strategy the conditional of $\pi$.
Let $G^i_{\eta\zeta}$ be the payoff to P1 of type $i$ for strategy profile $(\eta, \zeta)$. We have the following results connecting $G(p)$ and $G^*(\hat{p})$:

\begin{enumerate}
    \item If $\pi$ is Nash for P1 in $G(p)$ and $\hat{p} \in \partial V(p)$, then $\{\eta_i\}_{i=1}^I$ is also Nash for P1 in $G^*(\hat{p})$.
    \item If $\pi$ is Nash for P1 in $G^*(\hat{p})$ and $p$ is induced by $\pi$, then $p \in \partial V^*(\hat{p})$ and $\pi$ is Nash for P1 in $G(p)$.
    \item If $\zeta$ is Nash for P2 in $G^*(\hat{p})$ and $p \in \partial V^*(\hat{p})$, then $\zeta$ is also Nash for P2 in $G(p)$. 
    \item If $\zeta$ is Nash for $G(p)$, and let $\hat{p}^i := \max_{\eta \in \Delta(d_u)} G^i_{\eta \zeta}$ and $\hat{p} :=[\hat{p}^1,...,\hat{p}^I]^T$, then $p \in \partial V^*(\hat{p})$ and $\zeta$ is also Nash for P2 in $G^*(\hat{p})$.
\end{enumerate}

From the last two properties we have: If $\zeta$ is Nash for $G(p)$ and $G^*(\hat{p})$, then $\hat{p} = \max_{\eta \in \Delta(d_u)} G^i_{\eta \zeta}$, i.e., $\hat{p}[i]$ is the payoff of type $i$ if P1 plays a best response for that type to P2's Nash. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analytical Examples}\label{app:analytical}
The following examples are reproduced from \citet{ghimire24a} with permission.

\subsection{Hexner's Game: Analytical Solution}\label{app:hexner}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Here we discuss the solution to Hexner's game using primal and dual formulations (i.e., Eq.~\ref{eq:opt_prob} and Eq.~\ref{eq:opt_prob_dual}) on a differential game as proposed in \citet{hexner1979differential}. 
Consider two players with linear dynamics
\vspace{-0.05in}
\begin{equation*}
    \dot{x}_i = A_i x_i + B_i u_i,
\vspace{-0.05in}
\end{equation*}
for $i=1,2$, where $x_i(t) \in \mathbb{R}^{d_x}$ are system states, $u_i(t) \in \mathcal{U}$ are control inputs belonging to the admissible set $\mathcal{U}$, 
% $A_i \in \mathbb{R}^{d_x \times d_x}$ and $B_i \in \mathbb{R}^{d_x \times d_u}$. 
$A_i, B_i \in \mathbb{R}^{d_x \times d_x}$. Let $\theta \in \{-1, 1\}$ be Player 1's type unknown to Player 2. 
Let $p_{\theta}$ be Nature's probability distribution of $\theta$. Consider that the game is to be played infinite many times, the payoff is an expectation over $\theta$:
\vspace{-0.05in}
\begin{equation} \label{eq:value_def}
\small
\begin{aligned}
    J(u_1, u_2) &= \mathbb{E}_{\theta} \Bigl[\int_0^T \left(\|u_1\|^2_{R_1} - \|u_2\|^2_{R_2}\Bigr) dt \right.+ \\ 
    & \quad \Bigl. \|x_1(T) - z\theta \|^2_{K_1(T)} - \|x_2(T) - z\theta \|^2_{K_2(T)}\Bigr],
\end{aligned}
\vspace{-0.05in}
\end{equation}
where, \( z \in \mathbb{R}^{d_x} \). \( R_1 \) and \( R_2 \) are continuous, positive-definite matrix-valued functions, and \( K_1(T) \) and \( K_2(T) \) are positive semi-definite matrices. All parameters are publicly known except for \( \theta \), which remains private. Player 1s objective is to get closer to the target \( z\theta \) than Player 2. However, since Player 2 can deduce \( \theta \) indirectly through Player 1s control actions, Player 1 may initially employ a non-revealing strategy. This involves acting as though he only knows about the prior distribution \( p_{\theta} \) (rather than the true \( \theta \)) to hide the information, before eventually revealing \( \theta \). 

First, it can be shown that players' control has a 1D representation, denoted by $\tilde{\theta}_i \in \mathbb{R}$, through:
\begin{equation*}\label{eq:action}
    u_i = - R_i^{-1}B_i^TK_ix_i + R_i^{-1}B_i^TK_i\Phi_i z \tilde{\theta}_i,
\end{equation*}
for $i = 1,2$, where $\dot{\Phi}_i = A_i\Phi_i$ with boundary condition $\Phi_i(T) = I$, and 
\begin{equation*}
    \dot{K}_i = -A_i^TK_i - K_iA_i + K_i^TB_iR_i^{-1}B_i^TK_i.
    \label{eq:K}
\end{equation*}
Then define a quantity $d_i$ as:
\begin{equation}
    d_i = z^T \Phi^T_i K_i B_i R_i^{-1} B_i^T K_i^T \Phi_i z.
    \label{eq:d}
\end{equation}

With these, the game can be reformulated with the following payoff function:
\begin{equation}
    J(t,\tilde{\theta}_1, \tilde{\theta}_2) = \mathbb{E}_{\theta}\left[\int_{\tau = t}^T (\tilde{\theta}_1(\tau) - \theta)^2 d_1(\tau) - (\tilde{\theta}_2(\tau) - \theta)^2 d_2(\tau) d\tau\right],
\end{equation}
where $d_1$, $d_2$, $p_{\theta}$ are common knowledge; $\theta$ is only known to Player 1; the scalar $\tilde{\theta}_1$ (resp. $\tilde{\theta}_2$) is Player 1's (resp. Player 2's) strategy. We consider two player types $\theta \in \{-1, 1\}$ and therefore $p_{\theta} \in \Delta (2)$. 

Then by defining critical time:
\begin{equation*}
    t_r = \argmin_{t} \int_{0}^{t} (d_1(s) - d_2(s))ds,
\end{equation*}
we have the following equilibrium:
\begin{align}
    &\tilde{\theta}_1(s) = \tilde{\theta}_2(s) = 0 \quad \forall s\in [0, t_r] \\  
    &\tilde{\theta}_1(s) = \tilde{\theta}_2(s) = \theta \quad \forall s\in (t_r, T],
\end{align}

To solve the game via primal-dual formulation, we introduce a few quantities. First, introduce time stamps $[T_k]_{k=1}^{2r}$ as roots of the time-dependent function $d_1-d_2$, with $T_0 = 0$, $T_{2q+1} = t_r$, and $T_{2r+1} = T$. Without loss of generality, assume:
\begin{align}
    &d_1 - d_2 < 0 \quad \forall t\in (T_{2k}, T_{2k+1})~\forall k=0,...,r, \\
    &d_1 - d_2 \geq 0 \quad \forall t\in [T_{2k-1}, T_{2k}]~\forall k=1,...,r.
\end{align}
Also introduce $D_k := \int_{T_k}^{T_{k+1}} (d_1-d_2)ds$ and  
\begin{equation}
    \tilde{D}_k = \left \{ \begin{array}{ll}
        \tilde{D}_{k+1} + D_k, & \text{if } \tilde{D}_{k+1} + D_k < 0 \\
        0, & \text{otherwise}
    \end{array}\right.,
\end{equation}
with $\tilde{D}_{2r+1} = 0$. 

The following properties are necessary (see \cite{hexner1979differential} for details): 
\begin{enumerate}
    \item $\int_{k}^{2q+1} (d_1-d_2)ds = \sum_{k}^{2q} D_k < 0$, $\forall k=0,...,2q$;
    \item $\int_{2q+1}^k (d_1-d_2)ds = \sum_{2q+1}^{k-1} D_k > 0$, $\forall k=2q+2,...,2r+1$;
    \item $\tilde{D}_{2q+2} + D_{2q+1} > 0$;
    \item $\tilde{D}_k < 0, ~ \forall k<2q+1$.
\end{enumerate}
% \begin{proof}
% Properties 1 and 2 are results directly from the definition of $D_k$. 

% For property 3, if $\tilde{D}_{2q+2} + D_{2q+1} \leq 0$, then $\tilde{D}_{2q+2} = \tilde{D}_{2q+3} + D_{2q+2} \leq -D_{2q+1}$, then $ \tilde{D}_{2q+3} \leq -(D_{2q+2}+D_{2q+1}) < 0$ (property 2). This leads to $\tilde{D}_{2q+k} \leq -\sum_{i=1}^{k-1}D_{2q+i} <0$ for $k=1,...,2r-2q$. Thus $\tilde{D}_{2r} < 0$. Contradiction.

% For property 4, first we have $\tilde{D}_{2q+1}=0$ (property 3). Since $D_{2q}<0$ (property 1), $\tilde{D}_{2q} = D_{2q} < 0$.
% \end{proof}

\paragraph{Primal game.} We start with $V(T,p) = 0$ where $p := p_{\theta}[1] = \text{Pr}(\theta = -1)$. The Hamiltonian is as follows:
\begin{align*}
    H(p) & = \min_{\tilde{\theta}_1} \max_{\tilde{\theta}_2} \mathbb{E}_{\theta}\left[ (\tilde{\theta}_1 - \theta)^2 d_1 - (\tilde{\theta}_2 - \theta)^2 d_2\right] \\
    & = 4p(1-p)(d_1-d_2).
\end{align*}
The optimal actions for the Hamiltonian are $\tilde{\theta}_1 = \tilde{\theta}_2 = 1-2p$.
From Bellman backup, we can get
\begin{equation*}
    V(T_k, p) = 4p(1-p)\tilde{D}_k.
\end{equation*}
Therefore, at $T_{2q+1}$, we have
\begin{align*}
    V(T_{2q+1}, p) & = Vex_p\left(V(T_{2q+2},p) + 4p(1-p)D_{2q+1}\right) \\
    & = Vex_p\left(4p(1-p)(\tilde{D}_{2q+2} + D_{2q+1})\right).
\end{align*}
Notice that $\tilde{D}_{2q+2} + D_{2q+1} >0$ (property 3) and $\tilde{D}_k <0$ for all $k < 2q+1$ (property 4), $T_{2q+1}$ is the first time such that the right-hand side term inside the convexification operator, i.e., $4p(1-p)(\tilde{D}_{2q+2} + D_{2q+1})$, becomes concave. Therefore, splitting of belief happens at $T_{2q+1}$ with $p^1 = 0$ and $p^2 = 1$. Player 1 plays $\tilde{\theta}_1 = -1$ (resp. $\tilde{\theta}_1 = 1$) with probability 1 if $\theta = -1$ (resp. $\theta = 1$), i.e., Player 1 reveals its type. This result is consistent with Hexner's. 

\paragraph{Dual game.} To find Player 2's strategy, we need to derive the conjugate value which follows
\begin{align*}
    V^*(t, \hat{p}) & = \left \{ \begin{array}{ll}
       \max_{i\in\{1,2\}} \hat{p}[i] & \forall t \geq T_{2q+1} \\
        \hat{p}[2] - \tilde{D}_t\left(1 - \frac{\hat{p}[1]-\hat{p}[2]}{4\tilde{D}_t}\right)^2 & \forall t < T_{2q+1}, ~4\tilde{D}_t \leq \hat{p}[1] - \hat{p}[2] \leq -4\tilde{D}_t \\
        \hat{p}[1] & \forall t < T_{2q+1}, ~\hat{p}[1] - \hat{p}[2] \geq 4\tilde{D}_t \\
        \hat{p}[2] & \forall t < T_{2q+1}, ~\hat{p}[1] - \hat{p}[2] < 4\tilde{D}_t
    \end{array}\right.
\end{align*}

Here $\hat{p} \in \nabla_{p_\theta} V(0,p_{\theta})$ and $V(0,p_{\theta}) = 4p[1]p[2]\tilde{D}_0$. For any particular $p_* \in \Delta(2)$, from the definition of subgradient, we have $\hat{p}[1] p_*[1] + \hat{p}[2] p_*[2] = 4p_*[1]p_*[2]\tilde{D}_0$ and $\hat{p}[1] - \hat{p}[2] =4(p_*[2]-p_*[1])\tilde{D}_0$. Solving these to get $\hat{p} = [4p_*[2]^2\tilde{D}_0, 4p_*[1]^2\tilde{D}_0]^T$. Therefore $\hat{p}[1] - \hat{p}[2] = 4\tilde{D}_0 (1-2p_*[1]) \in [4\tilde{D}_0, -4\tilde{D}_0]$, and
\begin{equation*}
    V^*(0,\hat{p}) = \hat{p}[2] - \tilde{D}_0\left(1 - \frac{\hat{p}[1]-\hat{p}[2]}{4\tilde{D}_0}\right)^2.
\end{equation*}
% , $C(0,\hat{p})$ is convex to $\hat{p}$ and therefore no splitting of $\hat{p}$.

Notice that $V^*(t, \hat{p})$ is convex to $\hat{p}$ since $\tilde{D}_0 <0$ (property 4) for all $t \in [0, T]$. Therefore, there is no splitting of $\hat{p}$ during the dual game, i.e., $\tilde{\theta}_2 = 1-2p$. This result is also consistent with results in \citet{hexner1979differential}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Example of a Turn-Based Game}\label{app:beerquiche}
We present a zero-sum variant of the classic beer-quiche game, which is a turn-based incomplete-information game with a perfect Bayesian equilibrium. Unlike in Hexner's game, Player 1 in beer-quiche game wants to maximize his payoff, and Player 2 wants to minimize it; hence, $\text{Vex}$ becomes a $\text{Cav}$. 
\begin{figure}[!h]
    \centering
    % \includegraphics[scale=0.5]{figures/beerquiche.png}
    \includestandalone{figures/beerquiche}
    \caption{Zero-Sum Beer-Quiche Game}
    \label{fig:beerquiche}
\end{figure}
We solve the game through backward induction (from $t=2,1, 0$) of its primal and dual values (denoted by $V$ and $V^*$ respectively). Players 1 and 2 make their respective decisions at $t=0$ and $t=1$, and the game ends at $t=2$. The state $x$ at a time $t$ encodes the history of actions taken until $t$.
% We describe the states of the game as the decisions made up to that time, e.g., $x=(B,d)$ at $t=2$ means that Player 1 has chosen beer and Player 2 to defer. 

\textbf{Primal game:} First, we compute the equilibrium strategy of Player 1 using the primal value. At the terminal time step ($t=2$), based on Fig.~\ref{fig:beerquiche}, the value for Player 1 is the following:
\begin{equation}
    V(2, x, p) = \left \{ \begin{array}{ll}
        4p_T-2 & \text{if } x = (B, b) \\
        p_T & \text{if } x = (B, d) \\
        2p_T-1 & \text{if } x = (Q, b) \\
        2-2p_T & \text{if } x = (Q, d)
    \end{array}
    \right..
\end{equation}
At the intermediate time step ($t=1$), it is Player 2's turn to take an action. Therefore, the value is a function of Player 1's action at $t=0$ and Player 2's current action. And for the same reason, the value is not a \textit{concavification} (Cav) over the RHS term. 
\begin{equation}
    V(1, x, p) = \min_{v \in \{b, d\}} V(2, (x, v), p).
\end{equation}
We can find the best responses of Player 2 for both actions of Player 1. This leads to
\begin{equation}
    V(1, x, p) =  \left \{ \begin{array}{lll} 
        
        p_T & \text{if } x = B, ~3p_T-2 \geq 0 & (v^* = d) \\
        4p_T-2 & \text{if } x = B, ~3p_T-2 < 0 & (v^* = b) \\
        2-2p_T & \text{if } x = Q, ~4p_T-3 \geq 0 & (v^* = d) \\
        2p_T-1 & \text{if } x = Q, ~4p_T-3 < 0 & (v^* = b)
    \end{array}
    \right..
\end{equation}
Finally, at the beginning of the game ($t=0$), we have
\begin{equation}
    V(0, \emptyset, p) = \text{Cav}\left( \max_{u \in \{B, Q\}} V(1, u, p)\right).
\end{equation}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{figures/icml2024_mukesh_beer_1.png}
%     \caption{Primal value $V(0,\emptyset,p_T)$ at $t=0$.}
%     \label{fig:beer1}
% \end{figure}

Cav is achieved by taking the concave hull with respect to $p_T$:
\begin{equation}
    V(0, \emptyset, p) =  \left \{ \begin{array}{lll} 
        5p_T/2  - 1 & \text{if } p_T < 2/3 \\
        p_T & \text{if } p_T \geq 2/3 \\
    \end{array}
    \right..
\end{equation}
When $p_T \in [0, 2/3)$, 
$$V(0, \emptyset, p) = \lambda \max_u V(1, u, p^1) + (1-\lambda) \max_u V(1, u, p^2),$$
where $p^1 = [0, 1]^T$, $p^2 = [2/3, 1/3]^T$, and $\lambda p^1 + (1-\lambda) p^2 = p$. 

Therefore, when $p_T = 1/3$, $\lambda = 1/2$, Player 1's strategy is:
\begin{equation}
\begin{aligned}
    & \Pr(u=Q|T) = \frac{\lambda p^1[1]}{p[1]} = 0,     & \Pr(u=Q|W) = \frac{\lambda p^1[2]}{p[2]} = 3/4, \\
    & \Pr(u=B|T) = \frac{(1-\lambda) p^2[1]}{p[1]} = 1, 
    & \Pr(u=B|W) = \frac{(1-\lambda) p^2[2]}{p[2]} = 1/4.
\end{aligned}
\end{equation}

\textbf{Dual game:} To solve the equilibrium of Player 2, we first derive the dual variable $\hat{p} \in \partial_{p} V(0, \emptyset, p)$ for $p = [1/3, 2/3]^T$. By definition, $\hat{p}^T p$ defines the concave hull of $V(0, \emptyset, p)$, and therefore we have 
\begin{equation}
\begin{aligned}
    & [1/3, 2/3] \hat{p} = V(0, \emptyset, p) = -1/6 \\
    & [0, 1] \hat{p} = V(0, \emptyset, [0, 1]) = -1.
\end{aligned}
\end{equation}
This leads to $\hat{p} = [3/2, -1]^T$.

At the terminal time, we have
\begin{equation}
    \begin{aligned}
        V^*(2, x, \hat{p}) & = \min\{\hat{p}[1] - g_T(x), \hat{p}[2] - g_W(x)\} \\
        & = \left \{ \begin{array}{ll}
            \min\{\hat{p}[1] - 2, \hat{p}[2] + 2\} & \text{if } x = (B, b) \\
            \min\{\hat{p}[1] - 1, \hat{p}[2]\} & \text{if } x = (B, d) \\
            \min\{\hat{p}[1] - 1, \hat{p}[2] + 1\} & \text{if } x = (Q, b) \\
            \min\{\hat{p}[1], \hat{p}[2] - 2\} & \text{if } x = (Q, d) \\
        \end{array} \right.
    \end{aligned}
\end{equation}

At $t=1$, we have
\begin{equation}
        V^*(1, u, \hat{p}) = \text{Cav}_{\hat{p}}\left ( \max_{v} V^*(2, (u, v), \hat{p})\right).
\end{equation}
When $u = B$, the conjugate value is a concave hull of a piece-wise linear function:
\begin{equation}
\begin{aligned}
        V^*(1, B, \hat{p}) & = \text{Cav}_{\hat{p}}\left (
        \left \{ \begin{array}{lll}
            \hat{p}[1] - 1 & \text{if } \hat{p}[2] \geq \hat{p}[1] - 1 & (v^* = d) \\
            \hat{p}[2] & \text{if } \hat{p}[2] \in [\hat{p}[1] - 2, \hat{p}[1] - 1) & (v^* = b) \\
            \hat{p}[1] - 2 & \text{if } \hat{p}[2] \in [\hat{p}[1] - 4, \hat{p}[1] - 2) & (v^* = d)  \\
            \hat{p}[2] + 2 & \text{if } \hat{p}[2] < \hat{p}[1] - 4 & (v^* = b)  \\
        \end{array} \right.
        \right) \\
        & = \left \{ \begin{array}{lll}
            \hat{p}[1] - 1 & \text{if } \hat{p}[2] \geq \hat{p}[1] - 1 & (v^* = d) \\
            2/3 \hat{p}[1] + 1/3 \hat{p}[2] - 2/3 & \text{if } \hat{p}[2] \in [\hat{p}[1] - 4, \hat{p}[1] - 1) & (\text{mixed strategy})\\
            \hat{p}[2] + 2 & \text{if } \hat{p}[2] < \hat{p}[1] - 4 & (v* = b)  \\
        \end{array} \right.
\end{aligned}
\end{equation}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{figures/icml2024_mukesh_beer_2.png}
%     \caption{Conjugate value $\max_v ~C(2,B,\hat{p})$ at $t=2$.}
%     \label{fig:beer2}
% \end{figure}

For $\hat{p} = [3/2, -1]^T$ which satisfies $\hat{p}[2] \in [\hat{p}[1] - 4, \hat{p}[1] - 1)$, Player 2 follows a mixed strategy determined based on $\{\lambda_1, \lambda_2, \lambda_3\} \in \Delta(3)$ and $\hat{p}^j \in \mathbb{R}^2$ for $j = 1, 2, 3$ such that:
\begin{enumerate}[label=(\roman*)]
    \item At least one of $\hat{p}^j$ for $j = 1, 2, 3$ should satisfy $\hat{p}[2] = \hat{p}[1] - 1$  and another $\hat{p}[2] = \hat{p}[1] - 4$. These conditions are necessary for $V^*(1, B, \hat{p})$ to be a concave hull:
    \begin{equation}
            V^*(1, B, \hat{p}) = \sum_{j=1}^3 \lambda_j \max_v V^*(2, (B,v), \hat{p}^j).
    \end{equation}    
    % Without loss of generality, we will set $\hat{p}^1$ on line 1 and both $\hat{p}^2$ and $\hat{p}^3$ on line 2;
    \item $\sum_{j=1}^3 \lambda_j \hat{p}^j = \hat{p}$.
\end{enumerate}
These conditions lead to $\lambda_1 = 1/2$ and $\lambda_2 + \lambda_3 = 1/2$. Therefore, when Player 1 picks beer,  Player 2 chooses to defer and bully with equal probability. 

When $u = Q$, we similarly have
\begin{equation}
    V^*(1, Q, \hat{p}) = \left \{ \begin{array}{lll}
            \hat{p}[1] & \text{if } \hat{p}[2] \geq \hat{p}[1] + 2 & (v^* = d) \\
            ... & \text{if } \hat{p}[2] \in [\hat{p}[1] - 2, \hat{p}[1] + 2) & (\text{mixed strategy})\\
            \hat{p}[2] + 1 & \text{if } \hat{p}[2] < \hat{p}[1] - 2 & (v* = b)  \\
        \end{array} \right.
\end{equation}
The derivation of the concave hull when $\hat{p}[2] \in [\hat{p}[1] - 2, \hat{p}[1] + 2)$ is omitted, because, for $\hat{p} = [3/2, -1]^T$, $V^*(1, Q, \hat{p}) = \hat{p}[2] + 1 = 0$ while $v^* = b$, i.e. if Player 1 picks quiche, Player 2 chooses to bully with a probability of 1.

% The value and its conjugate provide behavioral strategies for Player 1 (informed) and Player 2 (non-informed), respectively, for arbitrary initial belief $p$. Moreover, the convexity of the value reveals subsets of $p$ where Player 1 should use a mixed strategy that manipulates the belief in order to improve its value. Similarly, the convexity of the conjugate value reveals subsets of dual variables $\hat{p}$ where Player 2 should use a mixed strategy to mitigate risks due to its uncertainty about Player 1.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computational Complexity of Existing Algorithms for Solving 2p0s Normal Form Games} \label{sec:complexity_existing_algos}
Here we reveal the computational complexity (in terms of the number of iterations) of some important existing algorithms for solving 2p0s normal form games. The purpose is to show that these algorithms all scale with the action space size, which limits them from solving games with continuous action spaces with discretization leads to undesirable solutions. We omit discussions about IIEFGs since they can be reformulated as NFGs.

Consider the following minimax formulation for NFGs: 
\begin{equation}\label{eq:minimax}
    \min_{x \in \Delta(I)} \max_{y \in \Delta(J)} x^TAy + \alpha g_1(x) - \alpha g_2(y),
\end{equation}
where $I$ and $J$ are positive integers, $A \in \mathbb{R}^{I \times J}$ is a payoff matrix, and $g_1$, $g_2$ are strictly convex functions (e.g., L2 norm, negative entropy for NFGs, and dilated entropy for EFGs). Since Eq.~\ref{eq:minimax} is convex to $x$ and concave to $y$, there exists a unique solution. When $\alpha = 0$, the solution $(x^*, y^*)$ is a Nash equilibrium, otherwise if $\alpha > 0$, the solution is an quantal response equilibrium (QRE).

\paragraph{Counterfactual regret minimization.} CFR variants are average-time convergent algorithms for solving NFGs and EFGs, leveraging the fact that minimizing counterfactual regrets at all infostates achieves Nash for 2p0s games~\citep{zinkevich2007regret}. \textbf{Algorithm:} Here we introduce the standard CFR and CFR+. For simplicity, we will focus on solving the NFG in Eq.~\ref{eq:minimax} with $\alpha=0$ (which reduces CFR to regret matching and CFR+ to regret matching+). Given strategy profile $(x_t, y_t)$ at iteration $t \in [T]$, the instantaneous regret vector for Player 1 (resp. Player 2) is $r^t_1 = Ay_t - x_t^T A y_t$ (resp. $r^t_2 = A^Tx_t - x_t^T A y_t$). The non-negative regret vector is $R_i^t = \max\{\sum_{\tau = 1}^t r_i^{\tau}, 0\}$ for $i \in [2]$. CFR updates the strategies as
\begin{equation}
    x_{t+1} = \frac{R_1^t}{<\textbf{1}, R_1^t>}, \quad y_{t+1} = \frac{R_2^t}{<\textbf{1}, R_2^t>}
\end{equation}
if the sums $<\textbf{1}, R_i^t>$ is positive. Otherwise the strategy is updated as $x_{t+1} = \textbf{1}/I$ for Player 1 and $y_{t+1} =\textbf{1}/J$ for Player 2. CFR+ is different from CFR only in the definition of the instantaneous regret: $\hat{r}^t_i = \max\{r_i^t, 0\}$ and then $R_i^t = \max\{\sum_{\tau = 1}^t \hat{r}_i^{\tau}, 0\}$.
\textbf{Complexity:} To reach $\varepsilon$-Nash, the best-known upper bound on the complexity of CFR and CFR+ is $\mathcal{O}((I+J)/\varepsilon^2)$~\citep{cesa2006prediction}. While this sublinear convergence rate seems to be worse than regularized descent-ascent algorithms with guaranteed linear convergence (e.g., MMD and regularized FTRLs), CFR+ still enjoys the state-of-the-art empirical performance for a variety of large IIEFGs~\citep{tammelin2014solving}. Nonetheless, it should be noted that the complexity of CFR variants scales linearly with respect to the size of the action space.

\paragraph{Magnetic mirror descent.} MMD is an extension of projected gradient descent ascent that has linear last-iterate convergence to $\alpha$-QRE for $\alpha >0$. For ease of exposition, we set $g_1(x) = \frac{1}{2}\|x\|^2_2$ and $g_2$ is similarly defined~\footnote{In \cite{sokota2022unified}, the authors used a more general regularization definition by introducing the Bregman divergence.} \textbf{Algorithm:} 
% Let $z^T := [x^T,y^T]$, $F(z) := [y^TA^T, -x^TA]^T$, and $g(z) := g_1(x) + g_2(y)$. 
Let $\eta>0$ be a learning rate, $(x', y') \in \text{int } \Delta(I) \times \Delta(J)$ be a ``magnet''. 
% $\phi(z;z_t)=\frac{1}{2}\|z-z_t\|^2_2$. 
Then starting from $(x_1, y_1) \in \text{int } \Delta(I) \times \Delta(J)$, at each iteration $t \in [T]$ do
\begin{equation}\label{eq:mmd}
\begin{aligned}
        x_{t+1} = \argmin_{x \in \Delta(I)} x^TAy_t + \frac{\alpha}{2}\|x - x'\|^2_2 + \frac{1}{2\eta}\|x - x_t\|^2_2, \\
        y_{t+1} = \argmin_{y \in \Delta(J)} -x_t^TAy + \frac{\alpha}{2}\|y - y'\|^2_2 + \frac{1}{2\eta}\|y - y_t\|^2_2.
\end{aligned}
\end{equation}
\textbf{Complexity:} (Theorem 3.4 and Corollary 3.5 of \cite{sokota2022unified}) Let the squared error be $\varepsilon := \frac{1}{2}(\|x - x^*\|^2_2 + \|y - y^*\|^2_2)$. If $(x_t, y_t) \in \text{int } \Delta(I) \times \Delta(J)$ for all $t \in [T]$, and if $\eta$ is sufficiently small~\footnote{See Corollary D.6 in \cite{sokota2022unified} for details on the bound of $\eta$.}, then for an error threshold $\varepsilon_0 > 0$, $\varepsilon \leq \varepsilon_0$ if $T \geq \frac{\ln((I+J)/\varepsilon_0)}{\ln(1+\eta \alpha)}$. Thus MMD has complexity $\mathcal{O}(\ln((I+J)/\epsilon_0))$ with respect to the action space.
\textbf{Remarks:} When $\alpha = 0$, MMD reduces to projected gradient descent ascent which is known to diverge or cycle for any positive learning rate. \cite{sokota2022unified} showed empirically that MMD can be used to solve Nash by either annealing the amount of regularization over time or by having the magnet trail behind the current iterate. However, it is important to note that MMD assumes the solution to be interior, which is not the case in the games we consider when value is convex (no splitting) due to Isaacs' condition.

\paragraph{FTRL variants.} FTRL is a classic online learning algorithm known to converge in potential games but cycle in Hamiltonian games~\citep{heliou2017learning, mertikopoulos2018cycles, liu2024regularization}. To this end, variants of FTRL have been proposed to achieve last-iterate convergence to $\epsilon$-Nash or $\epsilon$-QRE~\citep{perolat2021poincare}. Below we introduce a few of them to show that their complexities all increase with the size of the action space. \textbf{Algorithm:} \textit{RegFTRL}~\citep{liu2024regularization} introduces regularization terms $(\phi_1, \phi_2)$ that are strictly convex and continuously differentiable on their respective simplex. For each iteration, do
\begin{equation}\label{eq:regftrl}
\begin{aligned}
        & x_{t+1} = \argmin_{x \in \Delta(I)} <x,  \bar{y}_t> + \phi_1(x), \quad & \bar{y}_t = \sum_{\tau=1}^t Ay_{\tau} + \alpha \nabla g_1(x_{\tau}), \\
        & y_{t+1} = \argmin_{y \in \Delta(J)} -<\bar{x}_t, y> + \phi_2(y), \quad & \bar{x}_t = \sum_{\tau=1}^t A^T x_{\tau} + \alpha \nabla g_2(y_{\tau}).
\end{aligned}
\end{equation}

\textbf{Complexity:} \textit{RegFTRL} is guaranteed to find an $\varepsilon$-QRE in $\mathcal{O}\left(\frac{\ln((I+J)/\varepsilon)}{\ln(1+\eta \alpha)}\right)$ iterations (Theorem 2 in \cite{liu2024regularization}). \textit{FTRL-SP}~\citep{abe2024slingshot} and \textit{OMWU}~\citep{rakhlin2013optimization, syrgkanis2015fast} finds $\varepsilon$-QRE in $\mathcal{O}\left(\frac{\ln((I+J)/\varepsilon)}{-\ln(1 - \eta \alpha/2)}\right)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Prediction Error of Value Approximation}\label{app:complexity}
Here we show that the baseline algorithm (Alg.~\ref{alg:cams}) shares the same exponential error propagation as in standard approximate value iteration (AVI). The only difference is that the measurement error in Alg.~\ref{alg:cams} comes from numerical approximation of the minimax problems rather than randomness in state transition and rewards as in AVI. To start, let the true value be $V(t, x, p)$. Following \cite{zanette2019limiting}, the prediction error $\epsilon^{bias}_t := \max_{x,p} |\hat{V}_t(x,p) - V(t,x,p)|$
% \begin{equation}
%     \epsilon^{bias}_t := \max_{x,p} |\hat{V}_t(x,p) - V(t,x,p)|
% \end{equation} 
is affected by (1) the prediction error $\epsilon^{bias}_{t+\tau}$ propagated back from $t+\tau$, (2) the minimax error $\epsilon^{minmax}_t$ caused by limited iterations in solving the minimax problem at each collocation point:
% \begin{equation}
%     \epsilon^{minmax}_t = \max_{(x,p) \in \mathcal{S}_t} |\tilde{V}(t,x,p) - V(t,x,p)|,
% \end{equation}
$\epsilon^{minmax}_t = \max_{(x,p) \in \mathcal{S}_t} |\tilde{V}(t,x,p) - V(t,x,p)|$,
and (3) the approximation error due to the fact that $V(t,\cdot,\cdot)$ may not lie in the model hypothesis space $\mathcal{V}_t$ of $\hat{V}_t$: 
% \begin{equation}
%     \epsilon^{app}_t = \max_{x,p} \min_{\hat{V}_t \in \mathcal{H}_t} |\hat{V}_t(x,p) - V(t, x,p)|.
% \end{equation}
$\epsilon^{app}_t = \max_{x,p} \min_{\hat{V}_t \in \mathcal{V}_t} |\hat{V}_t(x,p) - V(t, x,p)|$.

\vspace{-0.1in}
\paragraph{Approximation error.} For simplicity, we will abuse the notation by using $x$ in place of $(x,p)$ and omit time dependence of variables when possible. In practice we consider $\hat{V}_t$ as neural networks that share the architecture and the hypothesis space. Note that $\hat{V}_T(\cdot) = V(T,\cdot)$ is analytically defined by the boundary condition and thus $\epsilon^{app}_T = \epsilon^{bias}_T = 0$. To enable the analysis on neural networks, we adopt the assumption that $\hat{V}$ is infinitely wide and that the resultant neural tangent kernel (NTK) is positive definite. Therefore from NTK analysis~\citep{jacot2018neural}, $\hat{V}$ can be considered a kernel machine equipped with a kernel function $r(x^{(i)},x^{(j)}) := <\phi(x^{(i)}), \phi(x^{(j)})>$ defined by a feature vector $\phi: \mathcal{X} \rightarrow \mathbb{R}^{d_\phi}$. Given training data $\mathcal{S}=\{(x^{(i)}, V^{(i)})\}$, let $r(x)[i] := r(x^{(i)},x^{(j)})$, $R_{ij} := r(x^{(i)}, x^{(j)})$, $V_\mathcal{S} := [V^{(1)},...,V^{(N)}]^\top$, $\Phi_\mathcal{S} := [\phi(x^{(1)}),...,\phi(x^{(N)})]$, and $w_\mathcal{S}:= \Phi_\mathcal{S}(\Phi_\mathcal{S}^\top \Phi_\mathcal{S})^{-1}V_\mathcal{S}$ be model parameters learned from $\mathcal{S}$, then
\vspace{-0.05in}
\begin{equation}
    \hat{V}(x) = r(x)^\top R^{-1}V_\mathcal{S} =<\phi(x), w_\mathcal{S}>
\vspace{-0.05in}
\end{equation} 
% To focus on the influence of approximation error on the prediction, we assume for now $V_{\mathcal{S}}$ has no minimax error. 
is a linear model in the feature space. Let $\theta^{\phi(x)} := r(x)^\top R^{-1}$ and $C := \max_{x } \|\theta^{\phi(x)}\|_1$. 
% Notice that a proper kernel function is bounded: $\max_{x,y} |r(x,y)| \leq M$, and let $\lambda > 0$ be the smallest eigenvalue of $R$. Lem.~\ref{lemma:approximation1} characterizes the upper bound of $C := \max_{x} \| \theta^{\phi(x)} \|_1 $:
% \begin{lemma}\label{lemma:approximation1}
%     $C \leq NM/\lambda$.
% \end{lemma}
% \begin{proof}
% We can first bound the L2 norm $\|R^{-1}r(x)\|_2$:
% \begin{equation}
%     \|R^{-1}r(x)\|_2 \leq \|R^{-1}\|_2 \|r(x)\|_2 = \frac{\|r(x)\|_2}{\lambda} \leq \frac{\sqrt{NM^2}}{\lambda}.
% \end{equation}
% Then since $\|a\|_1 \leq \sqrt{N}\|a\|_2$ for any $a \in \mathbb{R}^N$, we have $ \|R^{-1}r(x)\|_2 \leq NM/\lambda$.
% \end{proof}
Further, let $\mathcal{S}^\dagger := \argmin_{\mathcal{S}} |<\phi(x), w_\mathcal{S}> - V(x)|$ and $w^\dagger := w_{\mathcal{S}^\dagger}$, i.e., $w^\dagger$ represents the best hypothetical model given sample size $N$. Since $N$ is finite, the data-dependent hypothesis space induces an approximation error $\epsilon^{app}_t := \max_x |<\phi(x), w^\dagger> - V(x)|$. From standard RKHS analysis, we have $\epsilon^{app}_t \propto N^{-1/2}$.
% and define
% \begin{equation}
%     \epsilon := \max_{x} \min_{\mathcal{S}} |<\phi(x), w_\mathcal{S}> - V(x)|  
% \end{equation}
% as the inherent model error. 
% Then we have
% \begin{lemma}\label{lemma:approximation2}
% $\epsilon^{bias}_t \leq C \epsilon^{app}_t.$
% \end{lemma}
% \begin{proof}
%  First note that $\phi(x) = \Phi_{\mathcal{S}} \theta^{\phi(x)}$ when the resultant kernel is positive definite. Then the following proof directly follows that of Lem. 1 in \cite{***lavier}:
% \begin{equation}
%     \begin{aligned}
%         \max_x |\hat{V}(x) - V(x)| & \leq \max_x |\hat{V}(x) - <\phi(x), w^*>| + \max_x |<\phi(x), w^*> - V(x)| \\
%         & = \max_x |<\theta^{\phi(x)}, V_\mathcal{S}> - <\phi(x), w^*>| + \epsilon^{app} \\
%         & = \max_x |<\theta^{\phi(x)}, V_\mathcal{S}> - <\Phi_{\mathcal{S}} \theta^{\phi(x)}, w^*>| + \epsilon^{app} \\
%         & = \max_x |<\theta^{\phi(x)}, V_\mathcal{S} - \Phi_{\mathcal{S}}^\top  w^*>| + \epsilon^{app} \\
%         & \leq \max_x \|\theta^{\phi(x)}\|_1 \epsilon^{app} + \epsilon^{app} = (NM/\lambda + 1)\epsilon^{app}.
%     \end{aligned}
% \end{equation}
% \end{proof}
\paragraph{Error propagation.} 
% Now we study the bound on $\epsilon^{bias}_t$ when $\epsilon^{minmax}_t$ and $\epsilon^{bias}_{t+1}$ are accounted. 
Recall that we approximately solve \ref{eq:opt_prob} at each collocation point.
% \begin{equation}
%     \tilde{V}(t,x) = \min_{\{\lambda\},\{p\},\{u\}} \max_{\{v\}} \sum_{k=1}^I \lambda^k \hat{V}_{t+1}(x'(u^k,v^k),p^k) + <p^k, l(u^k,v^k)>,
%     \label{eq:errorprop1}
% \end{equation}
% where all variables are constrained. 
Let $z := \{\lambda, p, u, v\}$ be the collection of variables and $\tilde{z}$ be the approximated saddle point resulting from DS-GDA. Let $\tilde{V}(t,x,\tilde{z})$ be the approximate value at $(t,x)$ and let $V(t,x,z^*)$ be the value at the true saddle point $z^*$. Lemma~\ref{lemma:propagation1} bounds the error of $\tilde{V}(t,x,\tilde{z})$:

\begin{lemma}\label{lemma:propagation1}
    $\max_{x} |\tilde{V}(t,x,\tilde{z}) - V(t,x,z^*)| \leq \epsilon^{bias}_{t+\tau} + \epsilon^{minmax}_t$. 
\end{lemma}
% \paragraph{Proof of Lem.~\ref{lemma:propagation1}}
% \begin{lemma}\label{lemma:propagation1}
%     $\max_{x} |\tilde{V}(t,x,\tilde{z}) - V(t,x,z^*)| \leq \epsilon^{bias}_{t+1} + \epsilon^{minmax}_t$. 
% \end{lemma}
\begin{proof}
    Note that $\sum_{k=1}^I \lambda^k = 1$. Then
    \begin{equation}
    \begin{aligned}
        \max_{x} |\tilde{V}(t,x,\tilde{z}) - V(t,x,z^*)| & \leq \max_{x} |\tilde{V}(t,x,\tilde{z}) - \tilde{V}(t,x,z^*)| + \max_{x} |\tilde{V}(t,x,z^*) - V(t,x,z^*)| \\
        & \leq \epsilon^{minmax}_t + \max_x |\sum_{k=1}^I \lambda^k(\tilde{V}(t+\tau, x',p^k) - V(t+\tau, x', p^k) | \\
        & \leq \epsilon^{minmax}_t + \epsilon^{bias}_{t+\tau}.
    \end{aligned}
    \end{equation}
\end{proof}
% \begin{proof}
%     Note that $\sum_{k=1}^I \lambda^k = 1$. Then
%     \begin{equation}
%     \begin{aligned}
%         \max_{x} |\tilde{V}(t,x,\tilde{z}) - V(t,x,z^*)| & \leq \max_{x} |\tilde{V}(t,x,\tilde{z}) - \tilde{V}(t,x,z^*)| + \max_{x} |\tilde{V}(t,x,z^*) - V(t,x,z^*)| \\
%         & \leq \epsilon^{minmax}_t + \max_x |\sum_{k=1}^I \lambda^k(\tilde{V}(t+1, x',p^k) - V(t+1, x', p^k) | \\
%         & \leq \epsilon^{minmax}_t + \epsilon^{bias}_{t+1}.
%     \end{aligned}
%     \end{equation}
% \end{proof}
Now we can combine this measurement error with the inherent approximation error $\epsilon^{app}_t$ to reach the following bound on the prediction error $\epsilon^{bias}_t$:

\begin{lemma}\label{lemma:propagation2}
$\max_{x} |\hat{V}_t(x) - V(t,x)| \leq C_t(\epsilon^{minmax}_t + \epsilon^{bias}_{t+\tau} + \epsilon^{app}_t) + \epsilon^{app}_t$. 
\end{lemma}
% \paragraph{Proof of Lem.~\ref{lemma:propagation2}}
% \begin{lemma}\label{lemma:propagation2}
% $\max_{x} |\hat{V}_t(x) - V(t,x)| \leq (NM/\lambda + 1)(\epsilon^{minmax}_t + \epsilon^{bias}_{t+1} + \epsilon^{app}_t) + \epsilon^{app}_t$.    
% \end{lemma}
\begin{proof}
    \begin{equation}
    \begin{aligned}
        \max_{x} |\hat{V}_t(x) - V(t,x)| & \leq \max_{x} |\hat{V}_t(x) - <\phi(x),w^\dagger>| + \max_{x} |<\phi(x),w^\dagger> - V(t,x)| \\
        & \leq \max_{x}|<\theta^{\phi(x)}, \tilde{V}(t,x) - V(t,x)>| +  \max_{x}|<\theta^{\phi(x)}, V(t,x) - \phi(x)^\top w^\dagger>| + \epsilon^{app}_t \\
        & \leq C (\epsilon^{minmax}_t + \epsilon^{bias}_{t+\tau} + \epsilon^{app}_t) + \epsilon^{app}_t.
    \end{aligned}
    \end{equation}
\end{proof}
% \begin{proof}
%     \begin{equation}
%     \begin{aligned}
%         \max_{x} |\hat{V}_t(x) - V(t,x)| & \leq \max_{x} |\hat{V}_t(x) - <\phi(x),w^*>| + \max_{x} |<\phi(x),w^*> - V(t,x)| \\
%         & \leq \max_{x}|<\theta^{\phi(x)}, \tilde{V}(t,x) - V(t,x)>| +  \max_{x}|<\theta^{\phi(x)}, V(t,x) - \phi(x)^\top w^*>| + \epsilon^{app}_t \\
%         & \leq C (\epsilon^{minmax}_t + \epsilon^{bias}_{t+1} + \epsilon^{app}_t) + \epsilon^{app}_t.
%     \end{aligned}
%     \end{equation}
% \end{proof}
Lem.~\ref{lemma:propagation3} characterizes the propagation of error:
\begin{lemma}\label{lemma:propagation3}
    Let $\epsilon^{app}_t \leq \epsilon^{app}$, $\epsilon^{minmax}_t \leq \epsilon^{minmax}$, and $C_t \leq C$ for all $t\in[T]$. If $\epsilon^{app}_T = 0$, then $\epsilon^{bias}_0 \leq T C^T (\epsilon^{app} + C(\epsilon^{minmax} + \epsilon^{app}))$.
\end{lemma}
% \paragraph{Proof of Lem.~\ref{lemma:propagation3}}
% Lem.~\ref{lemma:propagation3} analyzes the propagation of error:
% \begin{lemma}\label{lemma:propagation3}
%     Let $\epsilon^{app}_t \leq \epsilon^{app}$, $\epsilon^{minmax}_t \leq \epsilon^{minmax}$, and $C = NM/\lambda$. If $\epsilon^{app}_T = 0$, then $\epsilon^{bias}_0 \leq H C^H (\epsilon^{app} + C(\epsilon^{minmax} + \epsilon^{app}))$.
% \end{lemma}
\begin{proof}
Using Lem.~\ref{lemma:propagation2} and by induction, we have
    \begin{equation}
        \epsilon^{bias}_0 \leq (\epsilon^{app} + C(\epsilon^{minmax} + \epsilon^{app})) \frac{1-C^T}{1-C} \leq T C^T (\epsilon^{app} + C(\epsilon^{minmax} + \epsilon^{app})).
    \end{equation}
\end{proof}
% \begin{proof}
% Using Lem.~\ref{lemma:propagation2} and by induction, we have
%     \begin{equation}
%         \epsilon^{bias}_0 \leq (\epsilon^{app} + C(\epsilon^{minmax} + \epsilon^{app})) \frac{1-C^H}{1-C} \leq H C^H (\epsilon^{app} + C(\epsilon^{minmax} + \epsilon^{app})).
%     \end{equation}
% \end{proof}
We can now characterize the computational complexity of the baseline algorithm through Thm.~\ref{thm:1}, by taking into account the number of DS-GDA iterations and the per-iteration complexity:
\begin{theorem}\label{thm:1}
    For a fixed $T$ and some error threshold $\delta>0$, with a computational complexity of at least $\mathcal{O}(T^3C^{2T}I^2\epsilon^{-4}\delta^{-2})$, Alg.~\ref{alg:cams} achieves 
    \begin{equation}
\small
    \max_{(x,p)\in \mathcal{X}\times \Delta(I)} |\hat{V}_0(x,p) - V(0,x,p)| \leq \delta.
\end{equation}
\end{theorem}
% \paragraph{Proof of Thm.~\ref{thm:1}}
\begin{proof}
    From Lem.~\ref{lemma:propagation3} and using the fact that $\epsilon^{app} \propto N^{-1/2}$, achieving a prediction error of $\delta$ at $t=0$ requires $N = \mathcal{O}(C^{2T}T^2\delta^{-2})$.
    Alg.~\ref{alg:cams} solves $TN$ minimax problems, each requires a worst-case $\mathcal{O}(\epsilon^{-4})$ iterations, and each iteration requires computing gradients of dimension $\mathcal{O}(I^2)$, considering the dimensionalities of action spaces as constants. This leads to a total complexity of $\mathcal{O}(T^3C^{2T}I^2\epsilon^{-4}\delta^{-2})$. 
\end{proof}

% \begin{proof}
%     CAMS solves $HN$ minimax problems, each of which requires $\mathcal{O}(\epsilon^{-4})$ iterations in the worst case to reach a $\epsilon$-game stationary point. Each of the descent-ascent iteration has a complexity of $\mathcal{O}(I^2)$ given the primal and dual problem sizes.   
% \end{proof}

% This section answers the following questions that together give us computational complexities of CAMS in the sense of $\mathcal{O}$: (1) How many samples are needed for CAMS? (2) What is the computational complexity at each sample? Both answers will be straight forward:
% For (1), since CAMS is an approximate value iteration (AVI) algorithm, we will take advantage of the existing complexity analysis of AVI. In particular, we adopt the analysis of \citep{zanette2019limiting} by assuming infinite width of the value network. For (2), we directly use the complexity analysis of DS-GDA~\citep{zheng2023universal}.

% \paragraph{Proof of Lem.~\ref{lemma:approximation2}}
% \begin{lemma}\label{lemma:approximation2}
% $\epsilon^{bias}_t \leq C\epsilon^{app}_t.$
% \end{lemma}
% \begin{proof}
% First note that $\phi(x) = \Phi_{\mathcal{S}} \theta^{\phi(x)}$ when the resultant kernel is positive definite. Then the following proof directly follows that of Lem.~1 in \cite{zanette2019limiting}:
% \begin{equation}
%     \begin{aligned}
%         \max_x |\hat{V}(x) - V(x)| & \leq \max_x |\hat{V}(x) - <\phi(x), w^*>| + \max_x |<\phi(x), w^*> - V(x)| \\
%         & = \max_x |<\theta^{\phi(x)}, V_\mathcal{S}> - <\phi(x), w^*>| + \epsilon^{app} \\
%         & = \max_x |<\theta^{\phi(x)}, V_\mathcal{S}> - <\Phi_{\mathcal{S}} \theta^{\phi(x)}, w^*>| + \epsilon^{app} \\
%         & = \max_x |<\theta^{\phi(x)}, V_\mathcal{S} - \Phi_{\mathcal{S}}^\top  w^*>| + \epsilon^{app} \\
%         & \leq \max_x \|\theta^{\phi(x)}\|_1 \epsilon^{app} + \epsilon^{app}.
%     \end{aligned}
% \end{equation}
% \end{proof}

% \paragraph{Error propagation.} Now we study the bound on $\epsilon^{bias}_t$ by taking into account $\epsilon^{minmax}_t$ and $\epsilon^{bias}_{t+1}$. At each collocation point $x$, we numerically solve a minimax problem of the following form:
% \begin{equation}
%     \tilde{V}(t,x) = \min_{\{\lambda\},\{p\},\{u\}} \max_{\{v\}} \sum_{k=1}^I \lambda^k \hat{V}_{t+1}(x'(u^k,v^k),p^k) + <p^k, l(u^k,v^k)>,
%     \label{eq:errorprop1}
% \end{equation}
% where all variables are constrained. Let $z := \{\lambda, p, u, v\}$ be the collection of variables and $\tilde{z}$ be the numerical saddle point resulted from solving Eq.~\ref{eq:errorprop1}, let $\tilde{V}(t,x,\tilde{z})$ be the value approximated at $(t,x)$, and let $V(t,x,z^*)$ be the actual value at the saddle point $z^*$. Lem.~\ref{lemma:propagation1} bounds the error of $\tilde{V}(t,x,\tilde{z})$:



% Now we can combine this error on data with $\epsilon^{app}_t$ to reach the following bound on $\epsilon^{bias}_t$:







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Game Settings, Baselines and Ground Truth} \label{app:baselines}
\subsection{Game Settings}
The players move in an arena bounded between $[-1, 1]$ in all directions. All games in the paper follow 2D/3D point dynamics as follows:
$\dot{x}_j = Ax_j + Bu_j$, where $x_j$ is a vector of position and velocity and $u_j$ is the action for player $j$. Note that we use $u$ and $v$ in the optimization problems \ref{eq:opt_prob} and \ref{eq:opt_prob_dual} to represent player 1 and player 2's actions respectively. The type independent effort loss for each player $j$ is defined as $l_j(u_j) = u_j^\top R_j u_j$, where $R_1 = \texttt{diag}(0.05, 0.025)$ and $R_2 = \texttt{diag}(0.05, 0.1)$. For the higher dimensional case, $R_1 = \texttt{diag}(0.05, 0.05, 0.025)$ and $R_2 = \texttt{diag}(0.05, 0.05, 0.1)$. 

\subsection{Ground Truth for Hexner's Game}
For the 4-stage and 10-stage Hexner's game, there exists analytical solution to the equilibrium policies via solving the HJB for respective players. 

\begin{equation*}
    u_j = -R_j^{-1}B_j^\top K_j x_j + R_j^{-1}B_j^\top K_j \Phi_j z\Tilde{\theta}_j,
\end{equation*}
based on the reformulation outlined below in which players' action $\Tilde{\theta}_j \in \mathbb{R}$ become 1D and are decoupled from the state:
where $\Phi_j$ is a state-transition matrix that solves $\dot{\Phi}_j = A_j \Phi_j$, with $\Phi_j(T)$ being an identity matrix, and $K_j$ is a solution to a continuous-time differential Ricatti equation: 
\begin{equation}\label{eq:ricattiode}
    \dot{K}_j = -A_j^\top K_j - K_j A_j + K_j^\top B_j R_j^{-1}B_j^\top K_j,
\end{equation}
Finally, by defining 
\begin{equation*}
    d_j = z^\top \Phi_j^\top K_j B_j R_j^{-1}B_j^\top K_j^\top \Phi_i z
\end{equation*}
and the critical time 
\begin{equation*}
    t_r = \argmin_t \int_0^t (d_1(s) - d_2(s)) ds 
\end{equation*}
and
\begin{align*}
    \Tilde{\theta}_j(t) = \begin{cases}
        0, &t \in [0, t_r]\\ 
        \theta, &t \in (t_r, T]
    \end{cases}.
\end{align*}

As explained in Sec.\ref{sec:validation}, P1 chooses $\theta_1 = 0$ until the critical time $t_r$ and P2 follows. 

Note that in order to compute the ground truth when time is discretized with some $\tau$, we need the discrete counterpart of equation \ref{eq:ricattiode}, namely the discrete-time Ricatti difference equation and compute the matrices $K$ recursively. 

\subsection{OpenSpiel Implementations and Hyperparameters} \label{sec:DeepCFR}
We use OpenSpiel \citep{LanctotEtAl2019OpenSpiel}, a collection of various environments and algorithms for solving single and multi-agent games. We select OpenSpiel due to its ease of access and availability of wide range of algorithms. The first step is to write the game environment with simultaneous moves for the stage-game and the multi-stage games (with 4 decision nodes). Note that to learn the policy, the algorithms in OpenSpiel require conversion from simultaneous to sequential game, which can be done with a built-in method. 

In the single-stage game, P1 has two information states representing his type, and P2 has only one information state (i.e., the starting position of the game which is fixed). In the case of the 4-stage game, the information state (or infostate) is a vector consisting of the P1's type (2-D: [0, 1] for type-1, [1, 0] for type-2), states of the players (8-D) and actions of the players at each time step $(4\times 2 \times U)$. The 2-D ``type'' vector for P2 is populated with 0 as she has no access to P1's type. For example, the infostate at the final decision node for a type-1 P1 could be $[0, 1, x^{(8)}, \mathbbm{1}_{u_0}^{(U)}, \mathbbm{1}_{d_0}^{(U)}, \cdots, \mathbbm{1}_{d_2}^{(U)}, \boldsymbol{0}^{(U)}, \boldsymbol{0}^{(U)}]$, and $[0, 0, x^{(8)}, \mathbbm{1}_{u_0}^{(U)}, \mathbbm{1}_{d_0}^{(U)}, \cdots, \mathbbm{1}_{d_2}^{(U)}, \boldsymbol{0}^{(U)}, \boldsymbol{0}^{(U)}]$ for P2, where $u_k$, $d_k$ represent the index of the actions at $k^{th}$ decision node, $k=0, 1, 2, 3$

The hyperparameters for DeepCFR is listed in table~\ref{tab:deepcfr_params}
\begin{table}[!ht]
    \centering
    \caption{Hyperparameters for DeepCFR Training}
    \begin{tabular}{lc} \toprule
         Policy Network Layers&  (256, 256)\\ \midrule
         Advantage Network Layers& (256, 256)\\ \midrule
         Number of Iterations & 1000 (100, for $U=16$) \\ \midrule
         Number of Traversals & 5 (10, for $U=16$)\\ \midrule
         Learning Rate & 1e-3 \\ \midrule
         Advantage Network Batch Size & 1024 \\  \midrule
         Policy Network Batch Size & 10000 (5000 for $U=16$) \\ \midrule
         Memory Capacity & 1e7 (1e5 for $U=16$)\\ \midrule
         Advantage Network Train Steps & 1000 \\ \midrule
         Policy Network Train Steps & 5000 \\ \midrule
         Re-initialize Advantage Networks & True \\ \bottomrule
    \end{tabular}
    \label{tab:deepcfr_params}
\end{table}
\subsection{Joint-Perturbation Simultaneous Pseudo-Gradient (JPSPG)}\label{app:jpspg}
The core idea in the JPSPG algorithm is the use of pseudo-gradient instead of computing the actual gradient of the utility to update players' strategies. By perturbing the parameters of a utility function (which consists of the strategy), an unbiased estimator of the gradient of a smoothed version of the original utility function is obtained. Computing pseudo-gradient can often be cheaper as faster than computing exact gradient, and at the same time suitable in scenarios where the utility (or objective) functions are "black-box" or unknown. Building on top of pseudo-gradient, \citet{martin2024joint} proposed a method that estimates the pseudo-gradient with respect to all players' strategies simultaneously. The implication of this is that instead of multiple calls to estimate the pseudo-gradient, we can estimate the pseudo-gradient in a single evaluation. 
More formally, let $\textbf{f}: \mathbb{R}^{d} \rightarrow \mathbb{R}^n$ be a vector-valued function. Then, its smoothed version is defined as:
\begin{equation}
    \textbf{f}_\sigma (\textbf{x}) = \mathbb{E}_{\textbf{z}\sim \mu}\textbf{f}(\textbf{x} + \sigma \textbf{z}), 
\end{equation}
where $\mu$ is a $d$-dimensional standard normal distribution, $\sigma \neq 0 \in \mathbb{R}$ is a scalar. 
Then, extending the pseudo-gradient of a scalar-valued function to a vector-valued function, we have the following pseudo-Jacobian:
\begin{equation}
    \nabla \textbf{f}_\sigma (\textbf{x}) = \mathbb{E}_{\textbf{z}\sim \mu}\frac{1}{\sigma}\textbf{f}(\textbf{x} + \sigma \textbf{z}) \otimes \textbf{z},
\end{equation}
where $\otimes$ is the tensor product. 

Typically, in a game, the utility function returns utility for each player given their strategy. Let $\textbf{u}:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^n$ be the utility function in a game with $n$ players, where each player has a $d$-dimensional strategy. Then, the simultaneous gradient of $\textbf{u}$ would be a function $\textbf{v} : \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$. That is, row $i$ of $\textbf{v}(\textbf{u})$ is the gradient of the utility of the player $i$ with respect to its strategy, $\textbf{v}_i = \nabla_i \textbf{u}_i$. As a result, we can rewrite $\textbf{v}$ concisely as: $\textbf{v} = \texttt{diag}(\nabla \textbf{u})$, where $\nabla$ is the Jacobian. 
With these we have the following:
\begin{equation} \label{eq:jpspg}
\begin{aligned}
    \textbf{v}_\sigma (\textbf{x}) &= \texttt{diag}(\nabla \textbf{u}_\sigma (\textbf{x}))\\
                                   &= \texttt{diag}\left(\mathbb{E}_{\textbf{z}\sim \mu}\frac{1}{\sigma}\textbf{u}_\sigma(\textbf{x} + \sigma \textbf{z}) \otimes \textbf{z}\right) \\
                                   &= \mathbb{E}_{\textbf{z}\sim \mu}\frac{1}{\sigma}\texttt{diag}\bigg(\textbf{u}_\sigma(\textbf{x} + \sigma \textbf{z}) \otimes \textbf{z}\bigg)\\
                                   &= \mathbb{E}_{\textbf{z}\sim \mu}\frac{1}{\sigma}\textbf{u}_\sigma(\textbf{x} + \sigma \textbf{z}) \odot \textbf{z},\\
\end{aligned}
\end{equation}
where $\odot$ is element-wise product and a result of the fact that $\texttt{diag}(\textbf{a} \otimes \textbf{b}) = \textbf{a} \odot \textbf{b}$. 
Hence, by evaluating Eq.~\ref{eq:jpspg} once, we get the pseudo-gradient associated with all players, making the evaluation constant as opposed to linear in number of players. 

Once the pseudo-gradients are evaluated, the players update their strategy in the direction of the pseudo-gradient, assuming each player is interested in maximing their respective utility. 
\paragraph{JPSPG Implementation.}
In games with discrete-action spaces, where strategy is the probability distribution over the actions, JPSPG can be directly applied to get mixed strategy. However, for continuous-action games, a standard implementation would result in pure strategy solution than mixed. In order to compute a mixed strategy, we can turn into neural network as a strategy with an added randomness that can be learned as described in \citet{martin2023finding, martin2024joint}. We similarly define two strategy networks for each player, the outputs of which are scaled based on the respective action bounds with the help of hyperbolic tangent ($\texttt{tanh}$) activation on the final layer. The input to the strategy networks (a single hidden layered neural network with 64 neurons and output neuron of action-space dimension) are the state of the player and a random variable whose mean and variance are trainable parameters. We follow the architecture as outlined by \citet{martin2024joint} in their implementation of continuous-action Goofspiel. We would like to thank the authors for providing an example implementation of JPSPG on a normal-form game. 

In the normal-form Hexner's game, P1's state $\textbf{x}_1 = \{x_1, y_1, \texttt{type}\}$, and P2's state $\textbf{x}_2 = \{x_2, y_2\}$. $x_i$, and $y_i$ denote the x-y coordinates of the player $i$. In 4-stage case, we also include x-y velocities in the state and append the history of actions chosen by both P1 and P2 into the input to the strategy network. As an example, P1's input at the very last decision step a vector $[x_1, y_1, v_{x_1}, v_{y_1}, x_2, y_2, v_{x_2}, v_{y_2}, \texttt{type}, u_{1_x}, u_{1_y}, d_{1_x}, d_{1_y}, u_{2_x}, u_{2_y}, d_{2_x}, d_{2_y}, u_{3_x}, u_{3_y}, d_{3_x}, d_{3_y}] \in \mathbb{R}^{21}$, where $u_j$ and $d_j$ represent actions of P1 and P2, respectively, at $j^{th}$ decision point. P2's input, on the other hand, is the same without the $\texttt{type}$ information making it a vector in $\mathbb{R}^{20}$.

\subsection{Sample Trajectories}
\subsubsection{CAMS vs DeepCFR vs JPSPG}
Here we present sample trajectories for three different initial states for each P1 type. The policies learned by CAMS results in trajectories that are significantly close to the ground truth than the other two algorithms. 
\begin{figure}[!h]
    \centering
    \includegraphics[width=.75\linewidth]{figures/cams_dcfr_jpspg_new.pdf}
    \caption{Trajectories generated using CAMS (primal game), DeepCFR, and JPSPG. The initial position pairs are marked with same marker and the final with star. The trajectories from CAMS are close to the ground-truth while those from DeepCFR and JPSPG are not.}
    \label{fig:cams_dcfr_jpspg}
\end{figure}
\newpage
\clearpage
\subsection{Value Network Training Details}
\paragraph{Data Sampling:} At each time-step, we first collect training data by solving the optimization problem (\ref{eq:opt_prob} or \ref{eq:opt_prob_dual}). Positions are sampled uniformly from [-1, 1] and velocities from $[-\bar{v}_t, \bar{v}_{t}]$ computed as $\bar{v}_t = t \times u_{max}$, where $u_{max}$ is the maximum acceleration. For the unconstrained game, $u_{max} = 12$ for both P1 and P2. For the constrained case, $u_{x_{max}} = 6$, $u_{y_{max}} = 12$ for P1 and  $u_{x_{max}} = 6$, $u_{y_{max}} = 4$ for P2. During training, the velocities are normalized between [-1, 1]. The belief $p$ is then sampled uniformly from $[0, 1]$. For the dual value, we first determine the upper and lower bounds of $\hat{p}$ by computing the sub-gradient $\partial_p V(t_0, \cdot, \cdot)$ and then sample uniformly from $[\hat{p}^-, \hat{p}^+]$. 

\paragraph{Training:}
We briefly discuss the training procedure of the value networks. As mentioned in the main paper, both the primal and the dual value functions are convex with respect to $p$ and $\hat{p}$ respectively. As a result, we use Input Convex Neural Networks (ICNN) \citep{amos2017icnn} as the neural network architecture. Starting from $T-\tau$, solutions of the optimization problem \ref{eq:opt_prob} for sampled $(X, p)$ is saved and the convex value network is fit to the saved training data. The model parameters are saved and are then used in the optimization step at $T-2\tau$. This is repeated until the value function at $t=0$ is fit. The inputs to the primal value network are the joint states containing position and velocities of the players $X$ and the belief $p$.

The process for training the dual value is similar to that of the primal value training. The inputs to the dual value network are the joint states containing position and velocities of the players $X$ and the dual variable $\hat{p}$.

\section{Details on Constrained Game}
Here, we briefly explain the optimization problem for the constrained game. Formally, given the states $x_1, x_2$ of the players P1 and P2, the constraint is given by the function $c(x_1, x_2) = r - ||(p_{x_1}, p_{y_1}) - (p_{x_2}, p_{y_2})||_2$. P1 must always maintain a radial distance of $r$ from P2, else P1 receives a $+\infty$ penalty and P2 receives a reward of $-\infty$ (both want to minimize their costs). 

We follow the method outlined in \citep{ghimire24a}, and train a separate value function model $\mathcal{F} : t \times \mathcal{X} \rightarrow \mathbb{R}$, that classifies the state-space into safe ($\Omega_t$) and unsafe states. Safe states are those initial states from which P1 can avoid collision with P2, whereas unsafe states are those initial states from which it is impossible for P1 to avoid collision. The sub-zero level set of $\mathcal{F}$ correspond to the unsafe states. 

With $\mathcal{F}$ available, we can query it to check if the resulting states $x^k = \text{ODE}(x, \tau, u^k, v^k; f)$ in Eq.~\ref{eq:opt_prob} are unsafe. If so, a high penalty is added (subtracted in the case of dual game), otherwise 0. Formally, at some time-step $t$, initial state $x \in \Omega_t$, $p$ (resp. $\hat{p}$), we solve the following optimization problem for the constrained primal ($V$) (resp. dual ($V^*$)) value:
\begin{equation}
\begin{aligned}
    \min_{\{u^k\}, \{\alpha_{ki}\}} \max_{\{v^k\}} & \quad \sum_{k=1}^{I}\lambda^k \left(V(t+\tau, x^k, p^k) + \tau \mathbb{E}_{i \sim p^k} [l_i(u^k, v^k)]\right) + \gamma \cdot \texttt{relu}(-\mathcal{F}(t+\tau, x^k)\\
    \text{s.t. } & \quad u^k \in \mathcal{U}, \quad x^k = \text{ODE}(x, \tau, u^k, v^k; f), \quad v^k \in \mathcal{V}, \quad  \alpha_{ki} \in [0, 1], \; \\
    & \sum_{k=1}^I \alpha_{ki} = 1, \quad \lambda^k = \sum_{i=1}^I \alpha_{ki} p[i], \quad p^k[i] = \frac{\alpha_{ki}p[i]}{\lambda^k}, \quad \forall i, k \in [I].
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
    \min_{\{v^k\}, \{\lambda^{k}\}, \{\hat{p}^k\}} \max_{\{u^k\}} & \quad  \sum_{k=1}^{I+1}\lambda^k \left(V^*(t+\tau, x^k, \hat{p}^k - \tau l(u^k, v^k))\right) - \gamma \cdot \texttt{relu}(-\mathcal{F}(t+\tau, x^k)\\
    \text{s.t. } & \quad u^k \in \mathcal{U}, \quad v^k \in \mathcal{V}, \quad x^k = \text{ODE}(x, \tau, u^k, v^k; f), \quad \lambda^{k} \in [0, 1],\\
    &\quad \sum_{k=1}^{I+1} \lambda^{k}\hat{p}^k = \hat{p}, \quad \sum_{k=1}^{I+1}\lambda^k = 1, \quad k \in [I+1].\\
\end{aligned}
\end{equation}
where $\gamma$ is a scaling factor. 
\newpage
\clearpage
\section{Multigrid Algorithms and Trajectories} 
\label{app:multigrid}
Here we present the two multigrid algorithms: 2-level and $n$-level. $n$-level multigrid algorithm can be used to approximate value functions for larger time-steps (or finer discretizations). $n$-level multigrid algorithm is similar to 2-level, with the difference being that the fine grid is coarsened $n$ times, such that the coarsest grid is of at least size 2. We solve the 16-stage game using 4-level multigrid. 
\begin{algorithm}[!h] 
\caption{Two-grid Value Approximation (Multigrid)}
\label{alg:multigrid}
\SetKwInput{Init}{Initialize}
\KwIn{Coarse time-steps $\mathcal{T}^{2l}$, fine time-steps $\mathcal{T}^{l}$, coarse minimax solver $\mathbb{O}^{2l}$, fine minimax solver $\mathbb{O}^{l}$, number of data points $N$ to sample, restriction operator $\mathcal{R}$, prolongation Operator $\mathcal{P}$}
\Init{Initialize fine grid value networks $\hat{V}^l_t$ $\forall t \in \mathcal{T}_l$, policy set $\Pi = \varnothing$}
\While{resource not exhausted or until convergence}{
    Initialize: $R^l = \varnothing$, $E^{2l} = \varnothing$, $\mathcal{S} = \varnothing$, \\
    Coarse grid correction networks $\varepsilon^{2l}_t$ $\forall t \in \mathcal{T}^{2l}$\;
    
    $\mathcal{S}[t] \leftarrow$ sample $N$ data points $(t, x, p)$, $\forall t \in \mathcal{T}^l$\;
    
    \tcp{Smoothing Step: Few iterations}
    $\texttt{target}, \pi_t \leftarrow \mathbb{O}^l(t+l,\cdot,\cdot,\hat{V}^l_{t+l})$ (init. w/ $\pi_t$ if $\Pi \neq \varnothing$)\;
    Store residuals: $r^l_t = (\hat{V}^l_t - \texttt{target})$ in $R^l$ and policies $\pi_t$ in $\Pi$\;
    
    \tcp{Restriction and coarse-solve}
    \For{$t \leftarrow T-2l$ \KwTo $0$}{
        $e^{2l}_t = \mathbb{O}^{2l}(\mathcal{R} \hat{V}^{l}_{t+2l} + \varepsilon^{2l}_{t+2l}) - \mathbb{O}^{2l}(\mathcal{R} V^l_{t+2l}) - \mathcal{R} r^{l}_t$
        \tcp*{$e^{2l}_T = 0$, $\varepsilon^{2l}_T = \emptyset$}
        
        Store $e^{2l}_t$ in $E^{2l}$\;
        
        Fit the correction network $\varepsilon^{2l}_t$ to $e^{2l}_t$
    }
    
    \tcp{Prolongation}
    \ForEach{$t \in \mathcal{T}^l$}{
        $e^l_t = \mathcal{P} e^{2l}_t$\;
        
        Fit $\hat{V}^l_t$ to $\hat{V}^l_t + e^l_t$\;
    }
    
    $\texttt{target}, \pi_t \leftarrow \mathbb{O}^l(t+h,\cdot,\cdot,\hat{V}^l_{t+l})$ (init. with $\pi_t$)\;
    
    Fit $\hat{V}^l_t$ to $\texttt{target}$ and replace $\pi_t$ in $\Pi$ \tcp*{Post Smoothing}
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%% n-level algorithm %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[!b]
\caption{$n$-Level Multigrid for Value Approximation}
\label{alg:n_multigrid}
\SetAlgoLined
\SetKwInput{Init}{Initialize}
\KwIn{$k_{\max}, k_{\min}, \mathbb{O}$ (minimax solver), $T$ (time horizon), $N$ (number of data points), $\mathcal{R}$ (restriction operator), $\mathcal{P}$ (prolongation operator)}
\BlankLine
\Init{$\mathcal{T}^l \leftarrow [0, l, 2l, \dots, T-l],\; \forall\,l \in \{2^{-k_{\max}}, \dots, 2^{-k_{\min}}\}$, $L \leftarrow 2^{-k_{\min}}$}
\Init{Value networks $\hat{V}_t^l,\; \forall\,t \in \mathcal{T}^l,\; \forall\,l \in \{2^{-k_{\max}}, \dots, 2^{-k_{\min}}\}$, policy set $\Pi \leftarrow \varnothing$}\;
\While{resources not exhausted or until convergence}{
    $R \leftarrow \varnothing,\; E^L \leftarrow \varnothing,\; \mathcal{S} \leftarrow \varnothing$\;
    
    Initialize coarsest-grid correction networks $\varepsilon_t^L,\forall\,t \in \mathcal{T}^L$\;
    
    $\mathcal{S}[t] \leftarrow \text{sample }N\;(t,x,p),\; \forall\,t \in \mathcal{T}^{k_{\text{max}}}$\;
    
    \tcp{down-cycle}
    \For{$k \gets k_{\max}$ \textbf{down to} $k_{\min}+1$}{
        Compute target via $\mathbb{O}^k$ (init. with $\pi_t$ if $\Pi[k] \neq \varnothing$), 
        and store updated policies $\pi_t$ in $\Pi[k],\; \forall\,t \in \mathcal{T}^k$\;
        
        Compute residuals $r^k[t],\; \forall\,t \in \mathcal{T}^k$\;
        
        \If{$k \neq k_{\max}$}{
            $r^k_t \leftarrow r^k_t + \mathcal{R}r_t^{k+1},\; \forall\,t \in \mathcal{T}^k$\;
        }
        Store $r^k_t$ in $R[k]$\;
    }
    \BlankLine
    \For{$t \leftarrow T-L$ \KwTo $0$}{
        \tcp{coarse-solve backwards in time}
        $e^L_t \leftarrow \mathbb{O}^L(\mathcal{R}\hat{V}^l_{t+L} + \varepsilon^L_{t+L}) 
        - \mathbb{O}^L(\mathcal{R}\hat{V}^l_{t+L}) - \mathcal{R}\,r_t^{k_{\min}+1}$\;
        \tcp*{$e^L_T = 0,\; \varepsilon^L_T = \varnothing$}
        Store $e^L_t$ in $E^L$\;
        
        Fit $\varepsilon_L^t$ to $e_L^t$\;
    }
    \BlankLine
    \tcp{up-cycle}
    \For{$k \gets k_{\min}+1$ \textbf{to} $k_{\max}$}{
        $e^k_t \leftarrow \mathcal{P}(e_t^{k-1}),\; \forall\,t \in \mathcal{T}^k$\;
        
        Update $\hat{V}^k_t \leftarrow \hat{V}^k_t + e^k_t$\;
    }
    \tcp{post smoothing (for all t's and l's)}
    
    $\texttt{target}, \pi_t \leftarrow \mathbb{O}^{l}(\hat{V}^l_{t+l})$ (initialized with $\pi_t$)
    
    Fit $\hat{V}^l_t$ to $\texttt{target}$ and replace $\pi_t$ in $\Pi[l]$\;
}
\end{algorithm}
\paragraph{Multigrid Trajectories Comparison (Primal Game).} In Fig.\ref{fig:multigrid_trajs} we compare the trajectories using learned value function via the multigrid approach with the ground truth. The trajectories closely resemble the ground truth with Player 1 successfully able to conceal his true type. Unlike in Fig.~\ref{fig:multigrid_trajs}, where P2's trajectories are due to best response action to P1's action,  in Fig.~\ref{fig:multigrid_dual_trajs}, we plot the resulting trajectories when P2 plays the dual game. 
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/multigrid_trajs.pdf}
    \caption{Comparison of trajectories generated using value learned via multigrid method vs the ground truth.}
    \label{fig:multigrid_trajs}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[width=.8\linewidth]{figures/multigrid_dual.pdf}
    \caption{Trajectories when P1 and P2 play their respective primal and dual game. P1's actions are a result of the primal value function whereas P2's actions are a result of the dual value function. Both primal and dual values are learned using multigrid approach.}
    \label{fig:multigrid_dual_trajs}
\end{figure}
\section{High Dimensional Hexner's Game}\label{app:hexner_high_dim}
\paragraph{3D Hexner's game.} To demonstrate the scalability of CAMS, we solve a 3D Hexner's game where the joint action space is now 6D. Accordingly, the state space becomes 12D and the value becomes 13D. Resultant trajectories are visualized in Fig.~\ref{fig:3d_case}. Similar to the 2D case, P1 learns to correctly conceal his target until some critical time. 
\begin{figure}[!h]
\vspace{-0.05in}
    \centering
    \includegraphics[width=\linewidth]{figures/3d_case_new.pdf}
    \vspace{-0.2in}
    \caption{3D Hexner's Game. Color shades indicate the current public belief.}
    \label{fig:3d_case}
    \vspace{-0.2in}
\end{figure}
\end{document}
