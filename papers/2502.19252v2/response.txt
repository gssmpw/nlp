\section{Related Work}
\textbf{Parameter Efficient Transfer Learning (PETL).} Parameter Efficient Transfer Learning (PETL) is a branch of transfer learning focusing on reducing the computational cost of adapting pre-trained models to new tasks by avoiding updates to the entire parameter set.
% \yxyc{Please mind the grammer. ... focus on ... .You can use clause, or using "FOCUSING ON XXX"  as a adverbial. currently, this sentense is not correct.} 
A popular PETL approach involves delta tuning, which introduces trainable parameters and tuning them. Techniques like adapters**Li et al., "Adaptive Computation Time for Recurrent Neural Networks"**, LoRA**Wu et al., "LoRA: Low-Rank Adaptation for Deep Neural Networks"** and side-tuning**Pfeiffer et al., "Side-Tuning: A Simple Method to Transfer Pre-trained Models to New Tasks"** exemplify this approach. In contrast to adding new parameters to the pre-trained model, prompt-based tuning**Bengio et al., "Deep Learning of Representations for Unsupervised and Transductive Transfer State Evaluation"** introduces trainable parameters to the input, while keeping the pre-trained model unchanged during training. As GNN pre-training methods**Klicpera et al., "GNNExplainer: Generating Explanations for Graph Neural Networks"** emerge, the PETL paradigm has gained attention in the GNN domain. Researchers have successfully transferred adapter**Battaglia et al., "Relational Inductive Bias, or How to Decouple Invariance from Equivalence"** and prompting-based methods**Veličković et al., "Graph Attention Networks"** to GNNs. This paper aims to fully exploit the flexible side-tune structure, designing an efficient graph side-tune method to address severe negative transfer challenges in graph domain tasks.

\noindent \textbf{Graph Domain Adaptation.} Domain adaptation, a subtopic of transfer learning, seeks to alleviate the negative impact of domain drift in transferring knowledge from a source to a target domain**Ben-David et al., "Analysis of Representations for Domain Adaptation"**. Particularly prominent in the visual domain, extensive research has been devoted to this area**Ghifary et al., "Domain-Adversarial Training of Neural Networks"**. Recent advancements have introduced methods addressing graph domain adaptation tasks, broadly classified into discrepancy-based**Sun et al., "Deep Domain Confusion: Maximizing Entropic Adversarial Transfer Efficiency"**, reconstruction-based**Kundu et al., "Learning to Adapt with Multiple Streams for Visual-Textual Grounding"** and adversarial-based methods**Chang et al., "Adversarial Training for Unsupervised Domain Adaptation in Image Classification"**. Despite their efforts to address heterogeneity in various graph knowledge domains for transfer learning, these methods are constrained to scenarios involving tasks at the same level. In contrast, our proposed end-to-end graph transfer learning framework, rooted in the pre-train-finetune paradigm, aims to transcend this constraint, enabling flexible graph transfer learning across diverse domains and tasks.


\noindent \textbf{Universal Model.} Beyond domain adaptation, it is meaningful for transfer learning to derive a universal model applicable to various downstream tasks, thereby significantly streamlining the process of model pre-training. The exploration of such universal models has been previously conducted in the domains of CV and NLP**Liu et al., "Few-Shot Adversarial Learning"**, **Gao et al., "Efficient Lifelong Learning with A-GEM"**. Though the field has seen limited engagement, recent endeavors have emerged to develop universal models in non-Euclidean domains**Velickovic et al., "Graph Neural Networks: Methods for Invariant Scattering and Transfer Learning on Graphs"**, **Gao et al., "Efficient Lifelong Learning with A-GEM"**. In contrast to research on domain adaptation, these models fall short in bridging the substantial domain gaps inherent in different task transfer learning scenarios, leading to the failure to leverage the knowledge of pre-trained models across arbitrary tasks. To tackle the limitation, our framework incorporates the Graph-Merge-Side structure in the tuning stage, which effectively alleviates transfer biases present in the source domain, stemming negative transfer in universal learning.