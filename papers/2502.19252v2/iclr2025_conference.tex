\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{multirow}
\usepackage{stfloats}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage[super]{nth}
\usepackage{array}
\usepackage{relsize}
\usepackage{wrapfig}
\usepackage{algorithm,algpseudocode}
% \usepackage{ntheorem}
% \newtheorem{theorem}{Theorem}
% \newtheorem{corollary}{Corollary}
% \newtheorem{proposition}{Proposition}
% \newtheorem{lemma}{Lemma}
% \newtheorem{observation}{Observation}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
% \usepackage{cleveref}
\usepackage{paralist}
% \usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{colortbl}  %彩色表格需要加载的宏包
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tabularx}

\usepackage{verbatim}
% \usepackage{geometry}
% \usepackage{tabularx}

\usepackage{bm}
\usepackage{ulem} 
\usepackage{bbding}
\usepackage[titletoc]{appendix}
\usepackage[accsupp]{axessibility} 

\newcommand{\revise}[1]{\textcolor{red}{[REVISE] #1}}

\title{GraphBridge: Towards Arbitrary Transfer Learning in GNNs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Li Ju, \, Xingyi Yang, \, Qi Li, \, Xinchao Wang\thanks{Corresponding Author.} \\
% \vspace{1mm}
 National University of Singapore\\
% \vspace{1mm}
\texttt{\{jujulili,xyang,liqi\}@u.nus.edu, xinchao@nus.edu.sg} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\makeatletter
\newcommand\figcaption{\def\@captype{figure}\caption}
\newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Graph neural networks (GNNs) are conventionally trained on a per-domain, per-task basis. It creates a significant barrier in transferring the acquired knowledge to different, heterogeneous data setups. This paper introduces \textbf{GraphBridge}, a novel framework to enable knowledge transfer across disparate tasks and domains in GNNs, circumventing the need for modifications to task configurations or graph structures. Specifically, GraphBridge allows for the augmentation of any pre-trained GNN with prediction heads and a bridging network that connects the input to the output layer. This architecture not only preserves the intrinsic knowledge of the original model but also supports outputs of arbitrary dimensions. To mitigate the negative transfer problem, GraphBridge merges the source model with a concurrently trained model, thereby reducing the source bias when applied to the target domain. Our method is thoroughly evaluated across diverse transfer learning scenarios, including Graph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empirical validation, conducted over 16 datasets representative of these scenarios, confirms the framework's capacity for task- and domain-agnostic transfer learning within graph-like data, marking a significant advancement in the field of GNNs. Code is available at \href{https://github.com/jujulili888/GraphBridge}{https://github.com/jujulili888/GraphBridge}.
\end{abstract}


\section{Introduction}

With the explosive growth of graph data, the application of Graph Neural Networks (GNNs) has become increasingly widespread in domains~\citep{jing2022learning, jing2021meta,wu2020graphsocial,yu2017spatio,shah2020finding} such as recommendation systems~\citep{gao2022graph,wu2019session} and biopharmaceutics\citep{zitnik2018modeling, rathi2019practical}. 
Despite their growing popularity, the effective implementation of GNNs often requires significant training efforts and substantial memory resources. This poses challenges for their practical application in diverse settings. To address these constraints, recent research has focused on reusing pre-trained GNN models~\citep{jing2023deep, yang2022deep, jing2021amalgamating, deng2021graph, hu2019pretrainstr, sun2022graphrepr, yang2020distilling}. This approach aims to reduce the need for extensive training, thereby lessening the associated time and resource demands to alleviate the extra training expense. 

%%%%  Cross Tasks
% \yxy{} 
However, to date, these efforts have not been entirely practical, primarily due to two forms of heterogeneity in graph data. The first is \textbf{\textit{task heterogeneity}}. The intrinsic non-Euclidean nature of graph data allows for its application across a range of tasks, including graph-level, node-level, and edge-level predictions. However, the graph pre-training paradigm typically assumes consistency between the tasks used in pre-training and those in downstream applications. This becomes problematic when adapting GNNs to new tasks with distinct output formats and knowledge requirements. In such scenarios, GNNs may not perform optimally, as the pre-training might not align well with the demands of these novel tasks.

% To address this issue, recent research has attempted to construct effective GNN pre-training frameworks to enhance the generalization performance of GNN models trained on a single graph dataset and adapt them for downstream tasks through fine-tuning\citep{hu2019pretrainstrategies,xia2022simgrace,zhu2021graphcl}. Nevertheless, most graph domain transfer learning methods based on the pre-training-fine-tuning paradigm have only been proven effective when the gap between the source and target domains is relatively small. When there is a substantial disparity between the downstream task domain and the pre-training source domain, negative transfer issues persist severely. 


% However, due to the heterogeneity of graph data, the effectiveness of graph neural networks in cross-domain transfer learning is not ideal. To address this issue, recent research has attempted to construct effective GNN pre-training frameworks to enhance the generalization performance of GNN models trained on a single graph dataset and adapt them for downstream tasks through fine-tuning\citep{hu2019pretrainstrategies,xia2022simgrace,zhu2021graphcl}. Nevertheless, most graph domain transfer learning methods based on the pre-training-fine-tuning paradigm have only been proven effective when the gap between the source and target domains is relatively small. When there is a substantial disparity between the downstream task domain and the pre-training source domain, negative transfer issues persist severely. 

% Furthermore, the fine-tuning approach necessitates retraining on the entire parameter space. In cases where the task domain span is significant, the knowledge from the pre-training domain has limited influence on the downstream task, and the convergence speed of pre-trained models does not significantly improve. Consequently, this method cannot efficiently reuse well-trained GNNs.


%%%%  Cross Tasks
Beyond task heterogeneity, \textbf{\textit{domain heterogeneity}} also poses a significant challenge in transferring knowledge effectively within graph data applications. This refers to the significant differences in node features, connection patterns, and topology across various graph datasets. Consequently, a GNN trained on one specific dataset might struggle to generalize effectively to other datasets with different structures. To mitigate this problem, researchers have been exploring the development of robust GNN transfer frameworks~\citep{hu2019pretrainstrategies,xia2022simgrace,zhu2021graphcl}, which aim to enhance the adaptability of GNN models, allowing them to be trained on one graph dataset and then fine-tuned for diverse downstream tasks. However, when the pre-training source domain vastly differs from the downstream task's domain, \textbf{\textit{negative transfer}} issues arise, highlighting the need for new approaches to bridge larger domain gaps.
% Apart from cross-domain knowledge transfer challenges in graph data, non-Euclidean data's heterogeneity is exemplified by irregular data structures in the exhibition of diverse feature dimensions at the input and various levels of tasks, ranging from graph-level, node-level, and edge-level predictions. The graph pre-training paradigm, assuming consistency between pre-training and downstream task data organization, primarily suits tasks involving regularly structured data like image-related tasks. However, this constraint significantly limits the flexibility of graph transfer learning, restricting the paradigm to tasks with matching feature dimensions and output formats. 

% While recent multi-task adaptation research\citep{} addresses this issue, these methods draw inspiration from prompt-tuning, enabling model reuse via data modification and task restructuring.
%%%% Our method
In this paper, we initiate an exploration of a unified workflow for knowledge transfer in diverse graph tasks. Our goal is to overcome the challenges posed by heterogeneity of graphs, especially in the reutilization of knowledge within GNN models. Specifically, we aspire to design a versatile pre-train-tuning graph transfer framework, named "GraphBridge", to facilitate seamless knowledge transfer across diverse graph domains. Rather than updating the parameters of the backbone, we integrate a trainable side network proficient in efficiently guiding end-to-end graph transfer learning.

% \yxyc{TWo solution should has the same order for the order of problem. For example, I say first domain heterogeneity$\to$task heterogeneity. The solution should on Merging(solve domain heterogeneity)$\to$Side architecture(solve task heterogeneity)}

In realizing our new vision, the primary concern is addressing the challenge of diverse input and output dimensions between source and target tasks.
% \yxyc{I think a lot of the descriptions of the problem are a bit literal. For example, "non-uniform" should be explained that the source and target tasks requires different input and output dimension, instead of using a single word} 
Therefore, we have devised adaptable input dimension adapters, both learnable and non-learnable, tailored to various transfer learning scenarios of differing complexity. Furthermore, we have developed interchangeable prediction heads for different task outputs, including graph classification, node classification, and point cloud classification.

Additionally, to mitigate negative transfer issues that arise when transferring knowledge across distinct domains, we have introduced two effective graph side-tuning techniques called Graph-Scaff-Side-Tune (GSST) and Graph-Merge-Side-Tune (GMST). GSST follows a similar architecture to the ladder-side, while GMST involves the fusion of the pre-trained backbone and a random initialized model with the same architecture, aiming to counteract the negative impact of pre-training knowledge on transfer.
% \yxyc{Prevent using veryveryvery long sentenses. To wordy and less clear.} 
Leveraging the advantages of the side-tune branch pathway computations, these modules yield satisfactory results. 
% surpassing those of fine-tuning, particularly in challenging tasks.

To validate the performance of the framework, we have define a "Task Pyramid" for graph transfer learning, as depicted in Figure~\ref{fig:task_pyra}.
% \yxyc{In scientific paper, figure and table must be refered in the form of "Figure XXX."}. 
On 16 graph datasets spanning different tasks, the GraphBridge and the associated Graph Side-tuning approach have been proven effectiveness in different scenarios, even surpassing the performance of full fine-tuning, particularly in challenging tasks.

To conclude, our work makes the following contributions:

\noindent $\bullet$\ We devised a novel knowledge transfer framework, termed GraphBridge, coupled with an accompanying Graph Side-tuning method to address transfer learning challenges across arbitrary tasks \& domains in graph-related applications.

\noindent $\bullet$\ 
We have created a ''Task Pyramid'', which includes four levels of graph transfer tasks across 16 datasets of varying difficulty. We applied our framework to these tasks for comprehensive evaluation.


\noindent $\bullet$\ Our extended experiments show that our method achieves resource-efficient transfer learning across various task scenarios, pre-training methods, and backbone structures. Remarkably, with only $5\%\sim 20\%$ of the tunable parameter, it delivers comparable performance while consistently handling tasks of different complexities.
% Extension experiments indicates that our method achieved resource-efficient transfer learning across diverse task scenarios, pre-training methods, and backbone structures. With only $5\%\sim 20\%$ of the parameter tuning, it realizes performance comparable to fine-tuning while maintaining consistent performance across tasks of varying difficulty. 
% As depicted in Figure 1's task pyramid, these scenarios encompass the following:

\begin{figure}[b]
\vspace{-7mm}
	\centering
	\includegraphics[width=0.7\textwidth]{figures/task_demo.pdf}
    \vspace{-4mm}
	\caption{\textbf{Task Pyramid \& Core Methodology. Left:} Graph side-tuning metods proposed to solve the different difficulty-level of the tasks; \textbf{Right:} Graph transfer learning tasks with different levels of difficulty defined in our work.}
\label{fig:task_pyra}
\vspace{3mm}
\end{figure}

\section{Related Work}
\textbf{Parameter Efficient Transfer Learning (PETL).} Parameter Efficient Transfer Learning (PETL) is a branch of transfer learning focusing on reducing the computational cost of adapting pre-trained models to new tasks by avoiding updates to the entire parameter set.
% \yxyc{Please mind the grammer. ... focus on ... .You can use clause, or using "FOCUSING ON XXX"  as a adverbial. currently, this sentense is not correct.} 
A popular PETL approach involves delta tuning, which introduces trainable parameters and tuning them. Techniques like adapters\citep{houlsby2019adapter,sung2022vladapter,zhang2021tipadapter}, LoRA\citep{hu2021lora} and side-tuning\citep{zhang2020side,sung2022lst} exemplify this approach. In contrast to adding new parameters to the pre-trained model, prompt-based tuning\citep{lester2021prompt,zhou2022vprompt, li2021prefix, li2023towards, li2024encapsulating} introduces trainable parameters to the input, while keeping the pre-trained model unchanged during training. As GNN pre-training methods\citep{hu2019pretrainstrategies, zhu2021graphcl,xia2022simgrace,Zhu_Xu_Yu_Liu_Wu_Wang_2021,Jin_Derr_Liu_Wang_Wang_Liu_Tang_2020} emerge, the PETL paradigm has gained attention in the GNN domain. Researchers have successfully transferred adapter\citep{li2023adaptergnn} and prompting-based methods\citep{sun2023all, sun2022gppt} to GNNs. This paper aims to fully exploit the flexible side-tune structure, designing an efficient graph side-tune method to address severe negative transfer challenges in graph domain tasks.

\noindent \textbf{Graph Domain Adaptation.} Domain adaptation, a subtopic of transfer learning, seeks to alleviate the negative impact of domain drift in transferring knowledge from a source to a target domain\citep{pan2009transfersurvey}. Particularly prominent in the visual domain, extensive research has been devoted to this area\citep{ganin2016domain,tan2017domaindistant,long2015domainlearning,long2018domainconditional,zhuang2015domainsupervised, pei2018domainmulti}. Recent advancements have introduced methods addressing graph domain adaptation tasks, broadly classified into discrepancy-based\citep{pilanci2020domainalign,vural2019domainadapt}, reconstruction-based\citep{wu2020unsupervised,cai2021graphrecons}, and adversarial-based methods\citep{dai2022graphadv,shen2020adversarial,zhang2019dane}. Despite their efforts to address heterogeneity in various graph knowledge domains for transfer learning, these methods are constrained to scenarios involving tasks at the same level. In contrast, our proposed end-to-end graph transfer learning framework, rooted in the pre-train-finetune paradigm, aims to transcend this constraint, enabling flexible graph transfer learning across diverse domains and tasks.


\noindent \textbf{Universal Model.} Beyond domain adaptation, it is meaningful for transfer learning to derive a universal model applicable to various downstream tasks, thereby significantly streamlining the process of model pre-training. The exploration of such universal models has been previously conducted in the domains of CV and NLP\citep{mccann2018natural,yu2019universally,silver2021reward,reed2022generalist}. Though the field has seen limited engagement, recent endeavors have emerged to develop universal models in non-Euclidean domains\citep{sun2023all,jing2023deep}. In contrast to research on domain adaptation, these models fall short in bridging the substantial domain gaps inherent in different task transfer learning scenarios, leading to the failure to leverage the knowledge of pre-trained models across arbitrary tasks. To tackle the limitation, our framework incorporates the Graph-Merge-Side structure in the tuning stage, which effectively alleviates transfer biases present in the source domain, stemming negative transfer in universal learning.


\section{Methods}
Achieving transfer learning across arbitrary graph task domains enables GNNs to comprehensively extract general knowledge, laying the foundation for efficient unsupervised graph learning and the development of universal graph model. However, arbitrary domain transfer learning presents challenges that need to be addressed. 
In this section, we begin by uncovering the key challenges of \textit{Arbitrary Graph Transfer Learning }and outline how we address the challenges posed by various work scenarios I proposed in Figure~\ref{fig:task_pyra}. Particularly, we pinpoint two core problems in our mission: large gap domain adaptation and multi-tasks unification. To handle these challenges, we re-construct the pre-training-tuning framework for graph domains \& tasks transfer learning. Additionally, we propose new resource-efficient tuning methods tailored for graphs to mitigate negative transfer.

% \vspace{-10mm}
\begin{figure*}[!t]
    \vspace{-3mm}
	\centering
	\includegraphics[width=1\textwidth]{figures/pipeline.pdf}
    \vspace{-15mm}
	\caption{\textbf{GraphBridge Framework.} \textbf{Left:} End-to-end GraphBridge framework with 2 stage; \textbf{Right:} Architecture of Graph-Merge-Side-Tuning architecture for addressing negative transfer.}
\label{fig:pipeline}
\vspace{-4.5mm}
\end{figure*}

% In this section, we start by elucidating the key challenges of Arbitrary Graph Transfer Learning and outline how we address the challenges posed by various work scenarios as illustrated in Figure~\ref{fig:task_pyra}. Specifically, we pinpoint two core problems in our mission: large gap domain adaptation and multi-task unification. To tackle these challenges, we reconstruct the pre-training-tuning framework for graph domains and tasks transfer learning. Additionally, we introduce new resource-efficient tuning methods tailored for graphs to alleviate negative transfer.

% In particular, we paradoxically reformulate the process of \textsc{Dare} as the one of generating adversarial examples, by converting their function as \emph{attackers} that pose threats to autonomous systems, into \emph{guards} that facilitate model reusing,
% and meanwhile propose a dynamic graph inference paradigm with adaptable aggregators as a pilot study of  
% \textsc{Mere}.
\vspace{-2mm}
\subsection{Challenges towards Arbitrary Graph Transfer}

\textbf{\Large{$\divideontimes$}} \textbf{Multi Input \& Output:} The first issue is to fit various input dimensions and output forms of downstream tasks, considering that we only have a frozen pre-trained backbone without any additional dimensional transformation. For instance, the HIV molecular dataset has 8 input features and 1-dimensional output for graph classification, while the Cora network has 1433 input features and output of the same dimension as the number of nodes for node classification. To tackle the multi Input \& Output dilemmas, we initially devised an end-to-end transfer learning framework capable of handling arbitrary input-output dimensions.


\noindent\textbf{\Large{$\divideontimes$}} \textbf{Domain Gap: } The second challenge towards arbitrary graph transfer lies in the insufficient tuning methods' capacity to make good use of the knowledge from pre-trained models for adaptation in the target domain. In cases where there is a substantial domain gap, the efficacy of full-tuning diminishes, rendering resource-efficient tuning methods even appear as negative transfer. Therefore, we establish innovative graph side-tuning architectures that not only address the issue of negative transfer, but also ensure resource efficiency.

\subsection{GraphBridge Framework}

In this context, we introduce a pioneering two-stage graph transfer learning framework titled "GraphBridge", which facilitates an end-to-end pre-training-tuning transfer paradigm within the task scenarios outlined in our work.
As shown in Figure~\ref{fig:pipeline}, our framework comprises a \textit{\textbf{Pre-training Stage}} , aimed at extracting generalized graph knowledge, and a \textit{\textbf{Tuning Stage}} dedicated to  downstream tasks adaptation. 

\noindent\textbf{Pre-training Stage.} 
In our work, we do not propose new graph pre-training methods. Instead, we design a versatile pre-training stage that can be adapted to various existing graph-level pre-training techniques (since they perform more effectively in graph knowledge learning\citep{hu2019pretrainstrategies,sun2023all} in the realm of graph pre-training methods). This adaptability allows our framework to utilize any graph-level pre-training method to obtain the base model for tuning. In our experiments, we employ the effective methods, GraphCL and SimGRACE, for base model pre-training. 
% To address the problem of heterogeneous node features present in the pre-training process and to unify the datasets of different task levels into graph-level for pre-training, we set Feat-Adapt and Graph-Split modules in the pre-training stage.
% , since only this strategy enables the model to grasp general message passing and aggregation patterns across the entire graph scope. This, in turn, guides the model in effectively addressing other downstream tasks. Therefore, we adhere to the graph-level strategies during the pre-training stage.

% % \yxyc{always write in a main points->specific solution framework. These two section contains a lot of specialized design, but not a unified idea or guideline before mentioning each}
% \textbf{\large{• }}\textbf{\textsc{feat-adapt}} is an operation for feature dimension unification to adapt to the fixed graph convolutional backbone. For diverse datasets, we employ customized strategies which are non-trainable. In instances like the node classification task network with sparse features, we utilize SVD to reduce the dimensionality of high-dimensional features. In contrast, for molecular graph data with labeled nodes serving as node features, we opt to re-encode the nodes through a tuned embedding layer. 

% \textbf{\large{• }}\textbf{\textsc{graph-split}} is the module to standardize all pre-training tasks to the graph level: In the Graph2Graph task scenario, model pre-training requires node-level networks. However, these single-graph networks are not adaptable to graph pre-training methods. To overcome this limitation, we employ the Graph-Split to divide a single graph into subgraphs while preserving the connectivity between nodes.

\noindent\textbf{Tuning Stage.}
In the tuning stage, our framework comprises three components: Input Bridge, Efficient-tuning, and Output Bridge, which can be tailored to downstream task transfer learning with varying input and output formats.

\textbf{\large{• }}\textbf{\textsc{input bridge}} adopt the Feat-Adapt methods mentioned above for input feature dimensional adaptation as well. In addition to the non-trainable adapters, we set a trainable linear layer as an adapter specifically for the point cloud dataset.

\textbf{\large{• }}\textbf{\textsc{efficient-tuning}} consists of a pre-trained backbone and tunable side networks for downstream task transfer on adapted inputs. In our work, we design a new graph-side-tuning paradigm which will be discussed in ~\ref{side-tuning}, instead of using the traditional fine-tuning. 

\textbf{\large{• }}\textbf{\textsc{output bridge}} serves as an output adapter for various graph tasks. It integrates several learnable predictor heads tailored to different downstream tasks for graph embeddings obtained from the tuning process. This ensures the generation of appropriate output formats for various graph tasks. For instance, in a graph classification task, a pooling operation followed by a linear prediction head is employed to generate predictions for each graph. In contrast, in a node classification task, a linear head is directly used for predictions, ensuring that each node corresponds to a label.

\vspace{-2mm}
\subsection{Graph Side-tuning}
\label{side-tuning}
In the tuning stage, we introduced a novel graph side-tuning technique, enabling effective transfer learning of different graph tasks. On one hand, side-tuning showcases resource efficiency by maintaining performance with fewer parameter manipulations. On the other hand, the flexible architecture of side-tuning facilitates the design of solutions to address negative transfers occurring during large gap domain transfer. For tasks of varied difficulty levels shown in Figure~\ref{fig:task_pyra}, we devised two graph side-tuning methods to tackle the challenges: the elementary\textit{ \textbf{Graph Scaff Side-Tuning}} and the advanced \textit{\textbf{Graph Merge Side-Tuning}} methods.

\noindent\textbf{Graph Scaff Side-Tuning (GSST).}
% The basic idea of the G-Scaff-Side-Tuning (GSST) methodology migrated from side-tune in NLP\citep{zhang2020side,sung2022lst}. 
In the design of the Graph Side-tuning, we make an innovation for side network architectures. Unlike the original side-tuning, which used the distillation structure of the base model as the side network, we directly set the side network as a randomly initialised MLP. Such a configuration is pertinent: the high computational time overhead of the graph convolution layer can be circumvented when training the MLP solely using node features. Meanwhile, current research\citep{han2022mlpinit, zhang2021graphless} indicates that MLPs can exhibit graph learning performance comparable to GNNs when guided by the knowledge from GNN models; 
Hence, the tuning efficiency can be further enhanced while transfer performance being ensured. 

The tuning process of GSST is shown in Figure~\ref{fig:pipeline}. In our approach, a GNN based model with frozen parameters produces activations at each layer. These activations are then passed through a down-sampling layer and fused layer-wise with the outputs of a trainable MLP side-network. Thus, the loss for downstream task $T_d$ can be formulated as:
% \vspace{-1mm}
\begin{equation}
\begin{split}
\mathcal{L}(\mathbf{x_{T_d}}, \mathbf{y_{T_d}}) = & \left.\|\bm{\alpha_s} \cdot \tilde{f}_{\text{gnn}}(\mathbf{x_{T_d}}, \mathbf{A_{T_d}} ; \mathbf{w^*_g} | GNN_{\text{pre}}) \right. \\
& \left. + (\mathbf{1} - \bm{\alpha_s}) \cdot f_{\text{mlp}}(\mathbf{x_{T_d}}; \mathbf{w_l}) - \mathbf{y_{T_d}} \right.\|
\end{split}
\label{eq:scaff-side}
\end{equation}
\vspace{-1.5mm}

where $\{\mathbf{x_{T_d}}, \mathbf{A_{T_d}}, \mathbf{y_{T_d}}\}$ denotes the node features, adjacency matrix and labels of $T_d$. Moreover, the $\tilde{f}_{\text{gnn}}$ with pre-trained-init parameters $\mathbf{w^*_g}$ and $f_{\text{mlp}}$ with random-init  $\mathbf{w_l}$ represent the output of the frozen base model and activated side network respectively,  while $\bm{\alpha_s}$ represents a set of fusion $\alpha$ for each layer between base and side. 

As such, Eq.~\ref{eq:scaff-side} indicates that the proposed GSST fixes the parameters of the base model and adjusts the MLP side network parameters for optimization. Additionally, the alpha blending parameters, as well as the downsampling module for each layer, are updated during back-propagation. The smaller scale of the tuning space reflects the parameter efficiency of our methods. Moreover, by exclusively conducting back propagation on the side network, we avoid the need to compute and retain gradient values for the base model, presenting an additional memory-efficient attribute. At last, the GSST method demonstrates commendable performance in easy task scenarios and the experimental results of it are further elaborated in section~\ref{res:easy}.


\vspace{1.5mm}\noindent\textbf{Graph Merge Side-Tuning (GMST).}
Nevertheless, in more challenging task scenarios, GSST proves inadequate in bridging the substantial gaps between diverse task domains and knowledge domains. 
% This limitation stands as the primary reason why our work has not been explored by others thus far, even with our GSST approach. during tuning in the presence of a
To address the negative transfer problem that occurs when significant domain gap exists, we further propose a novel side-tuning architecture named Graph Merge Side-Tuning (GMST).

In theory, due to the substantial disparity in knowledge between the source domain and the target domain, the base model's involvement tends to trap the model in a local optimum. 
% cannot expedite the convergence of the model on the new task.
Therefore, it becomes imperative to mitigate the impact of bias from the source domain on the target domain. 
% Since the side network operates independently of the base model, of alleviating negative transfer
Here, we achieve this goal by introducing the backup model to the base-side and fusing it with the original pre-trained model. 
Specifically, we set up a backup network mirroring the structure of the pre-trained model, initializing its parameters from random distributions. The parameters of both the backup and the pre-trained model are then frozen, while the parameter merging between each layer of the base model is controlled by the learnable scaling $\alpha$. 

During forward stage, the activation of each layer of the base model layer is initially combined by the backup and pre-trained models before being directed to the side network for base-side merging as shown in Figure~\ref{fig:pipeline}. 
Given the two-step model fusion, the back-propagation loss of the algorithm for random downstream task $T_d$ undergoes minor modifications compared to GSST:

\vspace{-5mm}
\begin{equation}
\begin{split}
\Phi_{\text{b}}(\mathbf{x_{T_d}})= & \left.\bm{\alpha_b} \cdot \tilde{f}_{\text{gnn1}}(\mathbf{x_{T_d}}, \mathbf{A_{T_d}} ; \mathbf{w^*_g} | GNN_{\text{Pre}}) \right. \\
& \left. + (\mathbf{1} - \bm{\alpha_b}) \cdot \tilde{f}_{\text{gnn2}}(\mathbf{x_{T_d}}, \mathbf{A_{T_d}} ; \mathbf{w_g}) \right.
\end{split}
\label{eq:merg-side1}
\end{equation}


\vspace{-1mm}
\begin{equation}
\Phi_{\text{s}}(\mathbf{x_{T_d}}) = f_{\text{mlp}}(\mathbf{x_{T_d}}; \mathbf{w_l})
\label{eq:merg-side2}
\end{equation}


\vspace{-5mm}
\begin{equation}
\mathcal{L}(\mathbf{x_{T_d}}, \mathbf{y_{T_d}}) = \|(\bm{\alpha_s}\cdot\Phi_{\text{b}}(\mathbf{x_{T_d}}) + (\mathbf{1} - \bm{\alpha_s}) \cdot \Phi_{\text{s}}(\mathbf{x_{T_d}}) -\mathbf{y_{T_d}}\|
\label{eq:merg-side3}
\end{equation}
% \vspace{-5mm}

where $\Phi_{\text{b}}$ and $\Phi_{\text{s}}$ denote the output of base merged activations and side activations, respectively. And $\bm{\alpha_b}$ refers to the set of fusion parameters for each layer of the base model. $\tilde{f}_{\text{gnn2}}$ and $\tilde{f}_{\text{gnn2}}$ represent pre-trained base model and the random-init. backup model. 


% Backup model's parameter with other markings remaining unchanged. 

As shown in Eq.~\ref{eq:merg-side1},~\ref{eq:merg-side2},~\ref{eq:merg-side3}, GMST only adds forwarding of the gradient of the base model fusion parameter to the back-propagation; the gradient of side-tuning itself does not change, which maintains the parameter-efficient attribute of the approach. Meanwhile, Eq.~\ref{eq:merg-side1} also indicates that by merging the backup and pre-trained model, we can introduce more randomness into the base model, thus diluting the negative impact of the source domain bias on the downstream tasks.
Finally, GMST demonstrated significantly better transfer learning than GSST in more difficult task scenarios.
% , achieving results comparable to fine-tuning

\section{Experiments}
We evaluate the performance of our GraphBridge on 16 publicly available benchmarks across four different scenarios defined in Figure~\ref{fig:task_pyra}:

\noindent $\bullet$\ \textbf{Easy}: Transfer learning between graph-level classification tasks within similar knowledge domains, a task frequently explored in existing research on graph pre-training and fine-tuning methods

\noindent $\bullet$\ \textbf{Medium}: Transfer learning between node classification tasks in unrelated knowledge domains.

\noindent $\bullet$\ \textbf{Hard}: Transfer learning between graph classification tasks and node classification tasks in unrelated knowledge domains.

\noindent $\bullet$\ \textbf{Extension}: For extension, we explored the transfer learning between traditional graph data and graph-like (point cloud) data.

As a preliminary study in this field, our goal is not to achieve state-of-the-art performance on all datasets. Instead, we aim to explore the feasibility of a graph-universal model across a diverse range of tasks and datasets.

\subsection{Experimental Settings}
\noindent\textbf{Datasets.} The datasets employed in our experiments can be categorized based on task levels: graph-level tasks consist of {\small\textit{ZINC-full}}, {\small\textit{BACE}}, {\small\textit{BBBP}}, {\small\textit{ClinTox}}, {\small\textit{HIV}}, {\small\textit{SIDER}}, {\small\textit{Tox21}}, {\small\textit{MUV}}, {\small\textit{ToxCast}}, which is a series of molecular graph datasets; node-level tasks include {\small\textit{ogbn-arxiv}}, {\small\textit{Cora}}, {\small\textit{CiteSeer}}, {\small\textit{PubMed}}, {\small\textit{Amazon-Computers}}, {\small\textit{Flickr}}, encompassing node classification datasets related to citation networks, product ranking networks, and social networks; point cloud tasks involve {\small\textit{ModelNet10}}, which is a 10-classification point cloud dataset.

\noindent\textbf{Model Settings.} In the Graph2Graph task, we employ a five-layer backbone architecture to facilitate the extraction of general knowledge from the extensive ZINC dataset. In the Node2Node, Graph2Node, and Graph2PtCld tasks, we consistently utilize a standard graph neural network structure comprising two-layer graph convolutions. For the backbones of the aforementioned model, we configure the hidden layer dimension of the base to be 100, while the hidden layer dimension of the side network is set to 16.

\noindent\textbf{Comparison Methods.} In various task scenarios, we employed different comparison methods to assess the performance of GraphBridge. For the evaluation of the Graph2Graph task, we conducted full-stage supervised training, fine-
tuning\citep{zhu2021graphcl}, MetaGP\cite{jing2023deep}, MetaFP\cite{jing2023deep}, and Adapter-GNN\citep{li2023adaptergnn}. Given the novelty of the task presented in this paper, there are currently fewer comparison methods available for the last three task scenarios. Therefore, we choose full-stage supervised training, fine-tuning and MetaFP as baselines, and adaptively modify the Adapter-GNN for comparison.
% In Node2Node, Graph2Node and Graph2PtCld transfer scenarios, on the other  we only selected the ogbn-arxiv and HIV datasets for models pre-training respectively, while the remaining node classification datasets were used for validation. 
% Furthermore, the pre-trained model acquired from Node2Node and Graph2Node tasks served as the backbone initialization for the Graph2PtCld task, and subsequently, we transferred the model to adapt to the ModelNet10 task. 

\subsection{Easy Task: Graph2Graph Transfer}
\label{res:easy}
For the easy-level task, we chose the largest-scale dataset, ZINC-full, as the pre-training dataset, and utilize the remaining molecular datasets for transfer learning. Additionally, as the Adapter algorithm is specifically designed for the GIN model, we exclusively used GIN as the backbone of the base model in our experiments for a fair comparison.
As depicted in Table~\ref{easy_sota}, our GSST method has demonstrated robust performance in the Graph2Graph task, outperforming the fine-tuning method by 0.6\% and 0.1\% under different pre-training approaches and significantly working better compared to other efficient tuning methods. Examining the errors, it is evident that the error fluctuations of the GSST algorithm are consistently below 1\%, highlighting its convergence stability. Moreover, a comparison of the last column indicates that our algorithm exhibits enhanced robustness compared to baselines, consistently delivering performance improvements across different pre-training methods.

\input{tables/easy_sota}

\subsection{Medium Task: Node2Node Transfer}

In the Node2Node transfer scenario, ogbn-arxiv was selected for model pre-training, while the remaining node classification datasets were used for validation.  

We show in Table~\ref{middle_sota} the results of the Graph2Graph transfer task. 
The \nth{7} and \nth{12} lines of Table~\ref{middle_sota} exhibit that GMST demonstrated superior transfer learning performance across most datasets when compared to the baselines. This was particularly notable in the GIN backbone settings of PubMed and CiteSeer, where GMST outperformed training from scratch by 6.8\% and 3.7\%, respectively.
% , making it possible for cross-domain GNN reusing
The proposed method consistently maintains stable performance across various pre-training methods and GNN backbones, showcasing the universality of GraphBridge. Although, on the Flickr, Amazon datasets, the performance of the proposed GMST method is slightly inferior to that of the fine-tune method, it still outperforms other efficient tuning methods. This suggests that the parameter-efficient GMST method may not exhibit significant advantages when the task domain's scope is not expansive enough, but achives SOTA among the efficient tuning methods.


\subsection{Hard Task: Graph2Node Transfer}
For the Graph2Node transfer scenario, we opted for a relatively larger HIV dataset for models pre-training, while using the same validation dataset as in the Node2node for transfer learning.

\input{tables/middle_sota}

% \vspace{-1mm}
As can be seen in Table~\ref{hard_sota}, the merits of the GMST become more pronounced in more challenging task: For the Cora dataset, GMST consistently outperforms fine-tuning by 5-10\% across different pre-training methods and backbones. Although on the Amazon and Flickr datasets, GMST does not strictly surpass the performance of fine-tuning, the gap between the results of the two tuning methods narrows. Besides, its advantages over other efficient tuning methods extend further. A comparison between Table~\ref{hard_sota} and ~\ref{middle_sota} reveals that, for identical downstream tasks, all other tuning methods experience significant performance degradation, whereas our GMST method keeps a competitive edge. This result aligns with expectations: with the expansion of the domain, the knowledge extracted from the original domain tends to have a negative impact for the downstream task. In contrast, the backup module introduced by the GMST mitigates this influence, accelerating model convergence on downstream tasks.
% bringing the model's parameter space closer to the optimal parameter space for the new task.

\input{tables/hard_sota}

\subsection{Extension Task: Graph2PtCld Transfer}
Here, the pre-trained model acquired from Node2Node and Graph2Node tasks served as the base models  backbone for the Graph2PtCld task. Subsequently, we transferred the model to adapt to the ModelNet10 task. In this scenario, we seek to explore whether the GraphBridge framework for graph tasks can transfer knowledge learned from graph domains to graph-like data.



The experimental results presented in Table~\ref{extra_sota} affirm the capability of our proposed GraphBridge framework for achieving transfer learning from graphs to those graph-like data, since our methods demonstrates significant performance improvements against previous works. 
% Upon comparing the \nth{7} line with subsequent lines, it becomes evident that, despite variations in backbone architecture and pre-training methodologies, the tuning method consistently produces a model that closely approaches the performance of the model trained from scratch in the final results.
According to the extensive exploration, we were surprised to observe that the GSST method demonstrated superior performance compared to the GMST method in the final results when tuning with a pre-trained backbone on graph-level datasets. It even outperformed all other tuning methods, including fine-tuning. This phenomenon can be explained by considering the dataset: ModelNet10 is a 10-classified point cloud dataset, which exhibits organizational similarities to a graph classification task. Therefore, using the molhiv pre-training results as the backbone of the model for transfer learning towards the point cloud classification task can be considered a Graph2Graph transfer task, a scenario where GSST excels. However, when employing the arxiv dataset for backbone pre-training, the situation changes. In contrast to the consistent excellence exhibited by GSST in scenarios where the original task is graph classification, GMST maintains stability in the face of a larger domain gap.

\input{tables/extra_sota}

In summary, GraphBridge demonstrates convincing performance across different task scenarios. While the proposed framework does not surpass full fine-tuning in all experimental settings, it consistently outperforms the previous efficient tuning methods. 
Based on the experimental results, we also outline the specific application scenarios for GSST and GMST. Specifically, GSST is most suitable for scenarios where the domain gap between the pre-training task and the downstream task is minimal, e.g. Graph2Graph and Graph2PtCld; Conversely, GMST is more effective when there is a significant gap between source and target domains, e.g. Node2Node, Graph2Node, and Node2PtCld.
Furthermore, hard tasks encompass not only the Graph2Node displayed in the main paper, but also other transfer scenarios such as Node2Graph, Graph2Edge and so on. For experiments of supplementary hard scenarios, please refer to Appendix \ref{other_scene}.
% Specifically, upon comparing the \nth{7} and \nth{8} lines, it is evident that GMST avoids significant fluctuations in performance across diverse pre-training datasets.

% \begin{figure}[h]
% \vspace{-4mm}
% 	\centering
% 	\includegraphics[width=0.45\textwidth]{figures/visual_ptcld.png}
%     \vspace{-1mm}
% 	\caption{\textbf{Visualization for different point cloud objects.} Sequence is organized in ascending order of detection difficulty.\yxyc{its nonsense if not comparing with other methods.}}
%  \vspace{-5mm}
% \label{fig:para}
% \end{figure}



\subsection{Ablation Studies}
\noindent\textbf{Tuning Efficacy.} 
To evaluate the efficacy of the proposed GSST and GMST for arbitrary graph transfer, we assess the tuning outcomes across varying levels of pre-training bases and diverse backbones, using the Cora dataset as a representative. As illustrated in Table~\ref{module_ablation}, using the pre-trained model directly for inference in the downstream task without fine-tuning results in unacceptable performance. Our proposed parameter-efficient tuning method effectively transfers the pre-trained model to the downstream task. As the transfer task transitions from moderate to difficult, the performance of GSST gradually declines. In contrast, GMST exhibits sustained effectiveness, showcasing its robust capability in mitigating the challenges associated with negative transfer.

\noindent\textbf{Resource Efficiency.} In the discussion of efficiency of our tuning methods, We validate from two distinct perspectives: parameter efficiency and tuning efficiency. 

    
% \begin{figure}[h]
% % \vspace{-2mm}
% 	\centering
% 	\includegraphics[width=0.6\textwidth]{figures/para_cnt.png}
%     \vspace{-2mm}
% 	\caption{\textbf{Adjustable parameter sizes in different tuning algorithms across distinct backbones.} To standardize the scale of the adjustable parameters in AdapterGNN, we conduct statistics on five-layer backbones.}
%  \vspace{-3mm}
% \label{fig:para}
% \end{figure}

% \begin{figure}[h]
% % \vspace{-1mm}
% 	\centering
% 	\includegraphics[width=0.90\textwidth]{figures/converge.png}
%     \vspace{-2mm}
% 	\caption{\textbf{Convergence speed of different tuning methods.} We selected the transfer experiment on Cora dataset in the challenging task scenario as a representation to verify the model's convergence.}
%  \vspace{-1mm}
% \label{fig:converge}
% \end{figure}

In terms of parameter efficiency, Figure~\ref{fig:para} illustrates the adjustable parameters of different tuning methods across various backbone architectures. Since MetaGP and MetaFP are prompt-tuning methods, their tunable parameters are determined by the datasets, rather than the GNN backbone architecture. Therefore, the tunable parameters scale of both MetaGP and MetaFP are averaged across various datasets, keeping same for different backbones. Notably, our GSST and GMST exhibit significantly fewer tuning parameters compared to most alternative methods, especially in the GIN backbone, where we have only 5\% of their parameters. Additionally, due to the fixed MLP structure in our side network, the scale of our parameter space remains constant across diverse pre-training backbones, which has the same advantage as the model-free prompt tuning approaches.

% In terms of tuning efficiency, we validate the training convergence speed of different tuning methods on the Cora dataset. Table~\ref{speed-up} illustrates the results, indicating that GMST ensures both optimal performance and the fastest convergence under difficult transfer tasks, highlighting its tuning efficiency.

\input{tables/module_ablation}

As for tuning efficiency, we validate the speed-up of different tuning methods compared to scratch training on the Cora dataset. Here, we computed the relative speed-up by measuring the convergence time for each tuning method. Table ~\ref{speed-up} presents the results, demonstrating that our proposed transfer learning method significantly accelerates the scratch training process. Furthermore, when compared to other tuning techniques, both GMST and GSST exhibit substantially higher speed-up rates than fine-tuning and AdapterGNN. The comparatively lower speed-up of our method relative to MetaFP can be attributed to the model-free characteristic of MetaFP, since the gradient update in data is faster than the process in model parameter. Overall, the validation of training speed-up indicates that our method delivers high performance while maintaining strong efficiency.

\begin{figure}[h]
\vspace{-1mm}
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/param_cnt.png}
        \vspace{-6mm}
    	\figcaption{\textbf{Adjustable parameter sizes in different tuning algorithms across distinct backbones.} We conduct statistics on five-layer backbones.}
     % To standardize the scale of the adjustable parameters in AdapterGNN,
     \vspace{-3mm}
        \label{fig:para}
    \end{minipage}
    \hfill
    % \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \vspace{-3.2cm} % 添加空白以匹配高度
        % \caption{\textbf{Adjustable parameter sizes in different tuning algorithms across distinct backbones.} We conduct statistics on five-layer backbones.}
         \scriptsize\centering\addtolength{\tabcolsep}{1.5 pt}
         \fontsize{8.5}{10}\selectfont
        \begin{tabular}{c|ccc}
        \toprule
        \textbf{Speed-up. $\uparrow$ (\%)} & GCN  & GAT  & GIN  \\ 
        \midrule
        FT       & 3.3  & 7.2  & 1.7  \\
        MetaFP   & 77.3 & 68.6 & 74.3 \\
        Adapter  & -    & -    & 20.2 \\ 
        \midrule
        \textbf{GSST}     & 39.4 & 57.6 & 26.3 \\
        \textbf{GSMT}     & 31.6 & 52.3 & 20.8 \\
        \bottomrule
        \end{tabular}
        \vspace{4.5mm}
        \tabcaption{\textbf{Training speed-up of different tuning methods compared to Scratch Training.} We selected the transfer experiment on Cora dataset in the challenging scenario as a representative.}
        % \vspace{-5mm}
        \label{speed-up}
    \end{minipage}
\end{figure}
\vspace{-3mm}

The validation conducted demonstrates the effectiveness of our approach in seamlessly transferring the commonly used pre-train + tuning framework from CV and NLP to graph domain research. This transfer is substantiated from both efficacy and efficiency standpoints, marking an initial success in achieving seamless transfer learning across diverse tasks in the graph domain.

\vspace{-1mm}
\section{Conclusions}
\vspace{-1mm}
In this paper, we introduce a novel GraphBridge framework for resource-efficient graph transfer learning toward arbitrary downstream tasks and domains.
Our goal is to create a unified workflow that maximizes the utility of pre-trained Graph Neural Networks (GNNs) for various cross-level and cross-domain downstream tasks, eliminating the need for data reorganization and task reformulation.
To this end, we have established four scenarios for graph transfer learning tasks, ranging from easy to complex, and proposed two resource-efficient tuning methods, namely GSST and GMST, to resolve the dilemmas.
Our experiments, conducted on selected datasets across different domains and tasks—including graph and node classification, as well as 3D object recognition—demonstrate the effectiveness of our approach in achieving arbitrary domain transfer learning on GNNs with improved resource efficiency.
Nevertheless, there are still constraints in our experimental setup. 
In our future work, we will strive to tackle transfer tasks across more benchmarks using our GraphBridge.

\section*{Acknowledgement}
This project is supported by the National Research Foundation, Singapore, under its AI Singapore Programme (AISG Award No: AISG2-RP-2021-023).

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section{Appendix}
% Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
% All such materials \textbf{SHOULD be included in the main submission.}

This document provides an in-depth analysis of our proposed methodology, offering additional insights and experimental details that complement our main findings and enhancing the understanding of our methods. Specifically, in Section~\ref{sec:dataset}, we delve into the intricacies of various datasets utilized in our research, shedding light on their unique characteristics and relevance to the study. Section~\ref{sec:arch} and Section~\ref{sec:ablation} is dedicated to a comprehensive ablation study, where we critically evaluate different architectural configurations and their impact on the performance of our proposed models. Finally, we further investigated the performance of GraphBridge in supplemental hard transfer scenarios in \ref{other_scene}, including Node2Graph and Graph2Edge, to guarantee its generalizability.
% These sections collectively aim to enhance the understanding of our methods and their application in diverse scenarios.


\subsection{Datasets Details}
\label{sec:dataset}
We provide in Table ~\ref{tab:dataset} the statistics 
of several graph benchmarks used in the main manuscript. This section aims to highlight the diversity and range of our benchmark datasets, showcasing their varied characteristics and applications.

\input{tables/dataset}

\noindent\textbf{Image Relation Dataset.} Specifically,
the \texttt{Flickr} originates from NUS-wide \citep{zeng2019graphsaint} which contains 89,250 nodes and 899,756 edges. One node in the graph represents one image uploaded to Flickr. If two images share some common properties (\emph{e.g.}, same geographic location, same gallery, comments by the same user, etc.), there is an edge between the nodes of these two images. Node features are the 500-dimensional bag-of-word
representation of the images provided by NUS-wide. For labels, each image belongs to one of the 7 classes.

\noindent\textbf{Citation Network Dataset.} The following three datasets, \emph{i.e.}, \texttt{Cora}, \texttt{Citeseer} and \texttt{Pubmed} \citep{sen2008collective}, 
are all citation network datasets used for single-label node classification.
Specifically, both \texttt{Cora} and \texttt{Citeseer} contain publications on computer science. 
\texttt{Pubmed}, on the other hand, only comprises the papers pertaining to diabetes.
Moreover, \texttt{ogbn-arxiv} dataset\citep{wang2020microsoft, hu2020open} contains a directed graph, which denotes the citation network among all Computer Science (CS) papers in arXiv, with each node representing an arXiv paper and each directed edge indicating that one paper cites another one.
The node features are the average embeddings of words in their title and abstract, which are computed by using the skip-gram model.

\noindent\textbf{Good Purchase Dataset.}  Amazon Computers and Amazon Photo are the segments 
of the Amazon co-purchase graphs from \citep{mcauley2015image}, 
where the nodes represent various goods, 
labeled by the corresponding product categories. Here we chose \texttt{Amazon-Computers} with more samples for our validation.  
% In Sect.~\ref{sect:amazon}, we will learn a versatile student that can classify both Amazon Computers and Amazon Photo.
% The last dataset in Tab.~\ref{tab:dataset}, QM7b dataset \citep{blum2009970,montavon2013machine}, 
% comprises 7,211 molecules with totally 14 regression targets.
% The nodes in QM7b represent atoms, and the edges represent bonds between different atoms.
% % In Sect.~\ref{sect:qm7b}, we will validate the effectiveness of the proposed approach on the graph regression task of QM7b.


\noindent\textbf{Molecular Structure Dataset.} \texttt{BACE}, \texttt{BBBP}, \texttt{ClinTox}, \texttt{HIV}, \texttt{SIDER}, \texttt{Tox21}, \texttt{MUV}, \texttt{ToxCast} and \texttt{ZIN-full} are all molecular property prediction datasets proposed by \citep{wu2018moleculenet}.
Every graph in these datasets denotes a molecule, with the nodes representing atoms, and edges denoting the chemical bonds. 
The node features contain the atomic number and chirality and the additional atom features, \emph{e.g.}, the formal charge. The number of prediction tasks varies across molecular datasets, and each task corresponds to a binary classification for molecular properties

\noindent\textbf{Point Cloud Dataset.} For the task of point cloud classification, we adopt the \texttt{ModelNet10} dataset \citep{wu20153d}, which is a subset of ModelNet40. Here, we chose ModelNet10 with fewer samples to speed up our experiments.
Specifically, the \texttt{ModelNet10} dataset contains 4899 CAD models of 10 man-made object categories, of which 3991 CAD models (ModelNet40: 40 classes-classification dataset with 9,843 CAD models are used for training and 2,468 CAD models are for testing) are used for training and 908 CAD models are for testing. For each CAD model, we sample 1,024 3D points from the mesh surfaces and also rescale the associated coordinates to fit into the unit sphere.
% \vspace{-15mm}


% \yxyc{recommond to name to "Ablation for Network Architecture"}
\vspace{-2mm}
\subsection{Ablation for Network Architecture}

\label{sec:arch}

\begin{figure*}[h]
	\centering
	\includegraphics[width=1\textwidth]{figures/simple_structure.png}
    % \vspace{-15mm}
	% \caption{\textbf{GraphBridge Framework.} \textbf{Left:} End-to-end GraphBridge framework with 2 stage; \textbf{Right:} Architecture of Graph-Merge-Side-Tuning architecture for addressing negative transfer.}
\label{fig:struc1}
\vspace{-11mm}
\end{figure*}
\begin{figure*}[h]
	\centering
	\includegraphics[width=1\textwidth]{figures/sidetuning.png}
    \vspace{-4mm}
	\caption{\textbf{All versions of Graph Side-tuning architectures.} \textbf{(a) G-Block Side-tune:} The Simplest version of the Graph Side-tuning architecture, which has separated base and side networks; \textbf{(b) G-Assemb Side-tune:} G-Block Side-tune with a backup model designed in base model for negative transfer alleviation; \textbf{(c) G-Scaff Side-tune:} Graph Side-tune architecture with layer-wise fusion between base model and side network; \textbf{(d) G-Merge Side-tune:} G-Scaff Side-tune with a backup model designed in base for negative transfer alleviation.}
\label{fig:struc2}
\vspace{-3mm}
\end{figure*}

Before proposing Graph Scaff Side-Tuning (GSST) and Graph Merge Side-Tuning (GMST), we first designed a simple version of the Graph Side-Tuning structures based on \citep{zhang2020side}, known as the Graph Block Side-Tuning (GBST) and Graph Assemb Side-Tuning (GAST), respectively as shown in Figure \ref{fig:struc2}. In this section, our aim is to demonstrate the superiority of GSST and GMST over these simpler predecessors, GBST and GAST, by comparing their respective performances and effectiveness in various applications.

\noindent\textbf{Architecture Setup.} GBST, a simplified version of GSST, differs slightly in the fusion of the base model and side network compared to GSST. In contrast to the layer-wise fusion approach of GSST, GBST keeps the base model and side network independent until the final output, where they are fused. Similarly, as a simplified version of GMST, GAST differs from GMST only in the side-network fusion phase; however, the fusion of the backup and pre-trained models follows the same layer-by-layer fusion paradigm exactly.


However, such straightforward architectures do not yield satisfactory performance in arbitrary graph transfer learning. This deficiency arises because, without a layer-wise connection between the side network and the base network, the side network loses crucial layer-specific information during the training process. Graph convolution operates uniquely at each layer, contributing to distinct information aggregation functions in the forward propagation of the graph network. The side network, represented by an MLP, needs to learn these layer-specific information aggregation functions independently, as it lacks the structured graph convolution inherent in the original graph. This is essential for achieving a comparable generalization ability in addressing graph-related problems.

% \vspace{-15mm}

While these two simple Graph Side-tuning modules may not effectively address our problem, they have provided valuable insights that guided the formulation of our final architectures. Following the same task scenario outlined in the main text, we extended our experiments to include GBST and GAST across four levels of difficulty: easy, medium, difficult, and extension. The results, along with those of other methods, are presented in the subsequent tables. Furthermore, we evaluated the results of GBST and GAST collectively in all supplementary experiments presented in Appendix.

\noindent\textbf{Results.} As evident from Tables \ref{tab:supp_easy}, \ref{tab:supp_mid}, \ref{tab:supp_hard}, the simplified version of Graph Side-tuning performs less effectively than the final version across tasks of varying difficulty levels—easy, medium, and hard. Specifically, on hard tasks, the results of the GBST and GAST could not even be compared to the adapter and fine-tuning methods, exhibiting negative improvement. 

\input{tables/supp_easy}

\input{tables/supp_middle}

\input{tables/supp_hard}

\input{tables/supp_extra}

Nevertheless, GBST and GAST retain their competitiveness in the point cloud transfer task according to Table \ref{tab:supp_extra}. This observation underscores the distinction between the point cloud classification task, belonging to the category of graph-like data, and other graph tasks. Even when the network struggles to efficiently learn an effective aggregation paradigm in a general way, its impact on the final result is minimal. As a side note, this highlights the ongoing potential for exploration in the realm of transfer learning from the graph domains to graph-like domains.

\subsection{Additional Ablation Studies}
\label{sec:ablation}
To validate the robustness of our method, we conducted additional ablation experiments in two key areas:

\noindent\textbf{\large{• }} \textbf{Influence of Source Dataset.} Confirming the algorithm's ability to achieve comparable performance on pre-trained models trained on different datasets.

\noindent\textbf{\large{• }} \textbf{Influence of GNN Architecture.} Verifying that the algorithm can maintain stable prediction performance across backbone architectures with varying numbers of layers.

\noindent\textbf{\large{• }} \textbf{Influence of Pre-training Methods.} Proving that our framework can flexibly adopt different graph-level pre-training methods and maintain stable prediction performance during tuning stage.

\noindent\textbf{\large{• }} \textbf{Influence of Side Network Structure.} Justifying the use of MLP as a side network for both the GSST GMST tuning algorithm.
\input{tables/supp_middle_Flickr}
\input{tables/supp_hard_muv}
\subsubsection{The effect of different pre-training data on the arbitrary graph transfer}
To assess the impact of different pre-training datasets on the final transfer performance of our proposed method, we conducted new experiments in two task scenarios: medium and hard. For Node2Node Transfer, we chose Flickr as the pre-training dataset and utilized all other graph classification datasets as downstream tasks to evaluate transfer learning performance. Conversely, for Graph2Node transfer, we opted for the MUV dataset as the training data and selected the same graph classification datasets as those applied in the main text as downstream tasks for transfer learning. The rest of the experimental setup is consistent with the main text.

The results of the experiments are presented in Table \ref{tab:supp_mid_flc}, where our method performs well on the first 4 test datasets in a moderately difficult task but exhibits poor performance on the ogbn-arxiv transfer. It is noteworthy that both the number of nodes and the number of edges in the Flickr dataset are only about half of those in ogbn-arxiv. This suggests that the transfer of downstream tasks can face challenges when the knowledge from the source domain is not sufficiently rich because the absence of knowledge in the source domain can lead to pre-trained models being unable to achieve sufficiently generalized performance. This phenomenon aligns with the pre-train-tuning paradigm. Examining the remaining results in the table, it is evident that our method maintains stable performance. The algorithm effectively carries out downstream task transfer learning on a well-pre-trained model.

\subsubsection{The effect of different backbone layers on the arbitrary graph transfer}
To investigate the impact of different numbers of backbone layers on the performance of the GSST and GMST methods, we configured the number of layers of the backbone to 5, the maximum currently used in stacked graph neural networks. Subsequently, we trained and tested these configurations on medium and difficult task scenarios, utilizing the same pre-trained models and downstream tasks as detailed in the main text. The results obtained are recorded in Tables \ref{tab:supp_mid_5l}, \ref{tab:supp_hard_5l}.

\input{tables/supp_middle_5l}
\input{tables/supp_hard_5l}

The experimental results in Tables \ref{tab:supp_mid_5l}, \ref{tab:supp_hard_5l} demonstrate that even with the backbone layers set to 5, our GMST algorithm consistently achieves impressive performance across various task scenarios, pre-training methods, and graph convolution layers. This underscores the stability of our algorithm.

In summary, our method exhibits robustness across various transfer scenarios and experimental setups.

\subsubsection{Adaptation of the Pre-training Stage to different graph-level pre-training methods}
As described in the main paper, our GraphBridge framework is capable of adapting various graph-level pre-training methods to pre-train our base model in Pre-training Stage. Consequently, we also evaluated the the transfer performance following pre-training with the GCC\citep{qiu2020gcc} and GPT-GNN\citep{hu2020gpt} methods. 

The GCC\citep{qiu2020gcc} framework is a graph-level pre-training methods which learns structural representations across graphs by leveraging the idea of contrastive learning to design the graph pre-training task as instance discrimination. Its basic idea is to sample instances from input graphs, treat each of them as a distinct class of its own, and learn to encode and discriminate between these instances. Similar to GraphCL and SimGRACE, GCC employs graph-level GNN pre-training schemes based on contrastive learning strategies. In our experiments, we do not utilize the end-to-end fine-tuning setup proposed in GCC's paper. Instead, we focus solely on the pre-training component of GCC to obtain our base models for our arbitrary transfer learning.

On the other hand, GPT-GNN\citep{hu2020gpt} is a generative pre-training framework for graph neural networks based on the self-supervised attributed graph generation task proposed by the author, with which both the structure and attributes of the graph are modeled. 
In attributed graph generation task, the graph generation objective has been decomposed into two components: Attribute Generation and Edge Generation, whose joint optimization is equivalent to maximizing the probability likelihood of the whole attributed graph. In doing this, the pre-trained model can capture the general knowledge of the graphs. Here, we only utilize the GPT-GNN pre-training stage as well to obtain our base models.

\input{tables/pretrain_middle}
\input{tables/pretrain_hard}

The supplemental experiments were conducted on the medium and difficult task scenarios as well, with all other settings for the parameters consistent with those in the main paper experiment. The results of the experiment are presented in Tables \ref{middle_pretrain} and \ref{hard_pretrain}.

According to the results presented in the Tables \ref{middle_pretrain} and \ref{hard_pretrain}, our framework still has the ability to perform arbitrary end-to-end graph transfer learning effectively using different graph-level pre-training methods in the Pre-training Stage. Although different pre-training strategies influence the absolute transfer learning performance, our proposed GMST structure  consistently achieves superior performance on most downstream tasks and backbones, aligning with the results reported in the main paper's experiments.



In summary, the results of the ablation experiments displayed in this section demonstrate the flexibility and adaptability of the GraphBridge framework during both the pre-training and fine-tuning stages, highlighting its practical value.

\subsubsection{The effect of different side network structures on the Tuning Results}
To investigate the impact of side network structures on arbitrary graph transfer, we conducted additional experiments with GMST fine-tuning. Specifically, we used each GNN backbone's corresponding lightweight structure as the side network (consistent with the implementation of \cite{sung2022lst}) to conduct the GMST across different datasets and compared the results with those obtained using an MLP as the side network. These experiments were carried out under the Middle and Hard task scenarios, and the results are presented in Table \ref{middle_sidenet} and Table \ref{hard_sidenet}, respectively.

\input{tables/sidenet_middle}
\input{tables/sidenet_hard}

The experimental results demonstrate that employing the corresponding lightweight GNN as the side network does not yield significant performance improvements for GMST tuning in either the Middle or Hard task scenarios. Furthermore, based on the comparative analysis of training efficiency between GNN and MLP in \cite{sung2022lst} and \cite{han2022mlpinit}, using an MLP as the side network ensures that computational overhead increases linearly with data scale, thereby maintaining the efficiency of our GMST algorithm.  
In conclusion, our innovative use of an MLP as the side network for pre-trained GNNs significantly enhances fine-tuning efficiency while preserving performance in arbitrary graph domain transfer, which is a successful attempt.




\subsection{Supplementary Transfer Scenarios}
\label{other_scene}
To further refine our task setup and comprehensively validate the generalization of the framework, we conducted additional transfer experiments on scenarios with the same level of difficulty as the Graph2Node task, including Node2Graph and Graph2Edge scenarios.

\noindent\textbf{\large{• }} \textbf{Node2Graph.} Transfer learning from node classification tasks  to graph classification tasks within unrelated knowledge domains.

\noindent\textbf{\large{• }} \textbf{Graph2Edge.} Transfer learning from graph classification tasks  to edge prediction tasks within unrelated knowledge domains.

\subsubsection{Node2Graph Transfer Task}
In the Node2Graph transfer scenario, we adapted the settings from the Graph2Node setup: during pre-training, we used the node-level ogbn-arxiv dataset to pre-train the model, while employing the graph-level downstream datasets used in the Graph2Graph tasks for fine-tuning. Moreover, we applied the same setup as in the Graph2Node scenario: The performance of GraphBridge was evaluated using a GIN backbone pre-trained with both the GraphCL and SimGRACE methods against fine-tuning, MetaFP and AdapterGNN. The experimental results are presented in Figure \ref{supp:node2graph}.

The experimental results of Node2Graph transfer show that our proposed GMST method dominates in this domain gap-obvious condition, both compared to the normal fine-tuning method and to the previous efficient tuning methods. Therefore, the additional experiments further reinforced the rules we previously established.

\input{tables/supp_node2graph}

\input{tables/supp_graph2edge}

\subsubsection{Graph2Edge Transfer Task}
For the Graph2Edge transfer scenario, we used the same datasets, pre-training methods, and GNN backbones as in the Graph2Node setup. However, we reformulated the original node classification downstream task into an edge prediction task by sampling edges with both positive and negative examples. As the edge prediction task is a binary classification problem, we evaluated the performance using ROC-AUC scores. The results of this experiment are shown in Figure \ref{tab:supp_graph2edge}.

The results exhibited in Figure \ref{tab:supp_graph2edge} indicate that our method achieves outstanding performance in the Graph2Edge transfer task. On the CiteSeer, PubMed, and Cora datasets, GMST consistently outperforms all fine-tuning methods, maintaining a clear competitive advantage. Moreover, our GMST breakthrough outperforms the fine-tuning method on the Amazon and Flickr considering Graph2Edge transfer.

In conclusion, by taking all the results from complementary scenarios into consideration, we find that GraphBridge demonstrates reliable performance across a variety of cross-domain transfer tasks, regardless of transfer task's complexity. This confirms the robustness and generalization capabilities of the GraphBridge framework, establishing it as an efficient and high-performing approach for graph transfer learning.



\subsection{Confusion matrix visualization of GMST tuning results}
In order to display a clear visualization of the class-wise performance of the GMST method across different datasets, we take the Hard Task Scenario as an example to plot the confusion matrices for GMST results of various GNN backbones pre-trained with GraphCL on diverse node-classification benchmarks.

The results demonstrate that on CiteSeer, PubMed, and Cora datasets where GMST performs well, the predictions of the fine-tuned model are concentrated along the diagonal, indicating high accuracy across all categories. In contrast, for the Flickr and Amazon datasets, the fine-tuned model tends to predict test data as belonging to the category with the highest proportion in the training set, reflecting the impact of label imbalance during fine-tuning. These findings highlight that category imbalance in downstream task datasets can negatively influence the performance of the GMST algorithm.

\begin{figure*}[h]
    \vspace{-1mm}
	\centering
    	\caption{\textbf{Visualization Results of GMST Fine-tuning in Hard Task Scenario on Different Datasets.} The visualized confusion matrices of all fine-tuned GNN backbones across five downstream task datasets are displayed.}
        \vspace{3mm}
	\includegraphics[width=1\textwidth]{figures/cm_citeseer.png}
\label{fig:pipeline}
\vspace{-5mm}
\end{figure*}

\begin{figure*}[h]
    \vspace{-1mm}
	\centering
	\includegraphics[width=1\textwidth]{figures/cm_pubmed.png}
\label{fig:pipeline}
\vspace{-5mm}
\end{figure*}

\begin{figure*}[h]
    \vspace{-1mm}
	\centering
	\includegraphics[width=1\textwidth]{figures/cm_cora.png}
\label{fig:pipeline}
\vspace{-5mm}
\end{figure*}

\begin{figure*}[t]
    \vspace{-13.5cm}
	\centering
        	\caption{\textbf{Visualization Results of GMST Fine-tuning in Hard Task Scenario on Different Datasets. (Cont'd)}
            }
	\includegraphics[width=1\textwidth]{figures/cm_amz.png}
\label{fig:pipeline}
\vspace{-5mm}
\end{figure*}

\begin{figure*}[h]
    \vspace{-26cm}
	\centering
	\includegraphics[width=1\textwidth]{figures/cm_flickr.png}
\label{fig:pipeline}
\vspace{-3mm}
\end{figure*}

\end{document}
