\section{Related Work}
\textbf{Parameter Efficient Transfer Learning (PETL).} Parameter Efficient Transfer Learning (PETL) is a branch of transfer learning focusing on reducing the computational cost of adapting pre-trained models to new tasks by avoiding updates to the entire parameter set.
% \yxyc{Please mind the grammer. ... focus on ... .You can use clause, or using "FOCUSING ON XXX"  as a adverbial. currently, this sentense is not correct.} 
A popular PETL approach involves delta tuning, which introduces trainable parameters and tuning them. Techniques like adapters\citep{houlsby2019adapter,sung2022vladapter,zhang2021tipadapter}, LoRA\citep{hu2021lora} and side-tuning\citep{zhang2020side,sung2022lst} exemplify this approach. In contrast to adding new parameters to the pre-trained model, prompt-based tuning\citep{lester2021prompt,zhou2022vprompt, li2021prefix, li2023towards, li2024encapsulating} introduces trainable parameters to the input, while keeping the pre-trained model unchanged during training. As GNN pre-training methods\citep{hu2019pretrainstrategies, zhu2021graphcl,xia2022simgrace,Zhu_Xu_Yu_Liu_Wu_Wang_2021,Jin_Derr_Liu_Wang_Wang_Liu_Tang_2020} emerge, the PETL paradigm has gained attention in the GNN domain. Researchers have successfully transferred adapter\citep{li2023adaptergnn} and prompting-based methods\citep{sun2023all, sun2022gppt} to GNNs. This paper aims to fully exploit the flexible side-tune structure, designing an efficient graph side-tune method to address severe negative transfer challenges in graph domain tasks.

\noindent \textbf{Graph Domain Adaptation.} Domain adaptation, a subtopic of transfer learning, seeks to alleviate the negative impact of domain drift in transferring knowledge from a source to a target domain\citep{pan2009transfersurvey}. Particularly prominent in the visual domain, extensive research has been devoted to this area\citep{ganin2016domain,tan2017domaindistant,long2015domainlearning,long2018domainconditional,zhuang2015domainsupervised, pei2018domainmulti}. Recent advancements have introduced methods addressing graph domain adaptation tasks, broadly classified into discrepancy-based\citep{pilanci2020domainalign,vural2019domainadapt}, reconstruction-based\citep{wu2020unsupervised,cai2021graphrecons}, and adversarial-based methods\citep{dai2022graphadv,shen2020adversarial,zhang2019dane}. Despite their efforts to address heterogeneity in various graph knowledge domains for transfer learning, these methods are constrained to scenarios involving tasks at the same level. In contrast, our proposed end-to-end graph transfer learning framework, rooted in the pre-train-finetune paradigm, aims to transcend this constraint, enabling flexible graph transfer learning across diverse domains and tasks.


\noindent \textbf{Universal Model.} Beyond domain adaptation, it is meaningful for transfer learning to derive a universal model applicable to various downstream tasks, thereby significantly streamlining the process of model pre-training. The exploration of such universal models has been previously conducted in the domains of CV and NLP\citep{mccann2018natural,yu2019universally,silver2021reward,reed2022generalist}. Though the field has seen limited engagement, recent endeavors have emerged to develop universal models in non-Euclidean domains\citep{sun2023all,jing2023deep}. In contrast to research on domain adaptation, these models fall short in bridging the substantial domain gaps inherent in different task transfer learning scenarios, leading to the failure to leverage the knowledge of pre-trained models across arbitrary tasks. To tackle the limitation, our framework incorporates the Graph-Merge-Side structure in the tuning stage, which effectively alleviates transfer biases present in the source domain, stemming negative transfer in universal learning.