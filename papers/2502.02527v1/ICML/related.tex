\section{Related Work}
\label{sec:related}

\subsection{Decision-tree-based Models}
Tabular data is one of the most widely used dataset types in machine learning. Gradient-boosted decision trees (GBDTs)~\cite{chen2016xgboost,Prokhorenkova2018Catboost,ke2017lightgbm},  remain a strong baseline for tabular tasks due to their efficiency and high performance. As an ensemble-based method, GBDTs construct multiple decision trees to iteratively minimize the residual loss. %In contrast, TabPFN~\cite{Hollmann2022TabPFN} models naturally align with ensemble approaches by introducing perturbations to inputs and aggregating predictions across multiple inference cycles.

\subsection{Tabular Deep Learning}
With the advancement of deep learning, an increasing number of studies have explored using deep learning methods for tabular data prediction. These approaches include MLP variants~\cite{KlambauerUMH17SNN,GorishniyRKB21Revisiting,Gorishniy2022On,David2024RealMLP}, neural networks specifically designed for tabular structures~\cite{WangFFW17DCN,WangSCJLHC21DCNv2,Chen2023TabCaps}, attention-based models~\cite{SongS0DX0T19AutoInt,Huang2020TabTransformer,GorishniyRKB21Revisiting,Chen2023Excel}, methods incorporating regularization~\cite{PTARL,jeffares2023tangos,Wu2024SwitchTab}, tree-mimic methods~\cite{ArikP21TabNet,PopovMB20Neural,Badirli2020GrowNet}, and context-based methods~\cite{gorishniy2023tabr,Ye2024ModernNCA}. Despite these advancements, recent benchmarks~\cite{Grinsztajn2022Why,McElfreshKVCRGW23when,YeACloser} have consistently demonstrated that gradient-boosted decision trees (GBDTs) outperform deep learning in tabular prediction tasks.
The superior performance of GBDTs can be attributed to two main factors: (1) their ability to handle heterogeneous tabular datasets, which often describe high-frequency target functions~\cite{DBLP:conf/icml/BasriGGJKK20,Grinsztajn2022Why},  and (2) the ensemble nature of GBDTs. Prior attempts to introduce ensemble-like mechanisms into tabular deep learning~\cite{Badirli2020GrowNet,PopovMB20Neural,DBLP:conf/icml/Chen2023Trompt}, have not been widely successful~\cite{Grinsztajn2022Why,YeACloser}. However, recent works like TabM~\cite{Yury2024TabM} integrate Batch Ensemble~\cite{WenTB20BatchEnsemble} techniques into the tabular domain, showing how efficient ensembling can be achieved with deep learning models. \looseness=-1
% Building upon this idea, our proposed method, \name, uses Batch Ensemble to map raw features into different representational spaces, which allows for efficient ensembling while aligning the pre-trained model with downstream datasets.

\subsection{Tabular Foundation Models}
While tabular foundation models (TFMs) are not as developed as foundation models in other domains, such as computer vision~\cite{SahariaCSLWDGLA22Diffusion} and natural language processing~\cite{BrownGPT3}, recent efforts have introduced various architectures to bridge this gap~\cite{WhyTFM}. Some approaches aim to explore model components that can be shared across datasets~\cite{LiuDEN,zhu2023xtab}, while others focus on utilizing the semantic information inherent in tabular datasets~\cite{Wang2022TransTab,YanTpBerta,Tabular_8B}.
As a Transformer-based model, TabPFN~\cite{Hollmann2022TabPFN,hollmann2025tabpfn} stands out for its exceptional performance and efficiency on small datasets. By leveraging the in-context learning capability of transformer~\cite{BrownGPT3}, it can make predictions for unseen instances without parameter updates. However, TabPFN faces limitations related to dataset size and feature dimensionality. To address these challenges, some studies have explored improvements in its architecture and training paradigm~\cite{TabForestPFN,MaTabDPT}, while others focus on adaptation techniques that expand TabPFN's applicability to a broader range of downstream tabular datasets~\cite{TuneTables,LocalPFN}.
Our work falls into the latter category, offering a straightforward, efficient, and effective approach to align TabPFN with downstream datasets.\looseness=-1


\subsection{Parameter-efficient Fine-Tuning} 
Some approaches for adapting Tabular Foundation Models (TFMs) to downstream tasks simply fine-tune the entire model~\cite{LocalPFN,TabForestPFN}, which leads to performance improvements but incurs high computational and storage costs. Parameter-efficient fine-tuning (PEFT) offers a solution to the challenge by enabling adaptation with a minimal number of trainable parameters~\cite{Guide_PEFT}. 
Based on their operational mechanisms, PEFT methods can be broadly categorized into four paradigms~\cite{PEFT_Survey}:
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
\item \textbf{Additive Methods}. These methods incorporate additional lightweight modules into the model architecture, such as adapters~\cite{Serial_Adapter,SideTuning,MixturePFN} or soft prompts~\cite{Prefix_Tuning,TuneTables}. 
\item \textbf{Selective Methods}. Instead of introducing new parameters, selective methods strategically identify and update the most relevant parameters while freezing the rest~\cite{FuYSLBC23SAM,He0ZTZ23SPT}.%, thereby reducing computational overhead.
\item \textbf{Reparameterized Methods}. These methods employ low-rank decomposition or equivalent transformations to reduce the parameter space during fine-tuning~\cite{AghajanyanGZ20SAID,HuSWALWWC22LoRA,ValipourRKG23DyLoRA}. 
\item \textbf{Hybrid Methods}. By combining the strengths of multiple PEFT strategies, hybrid methods create a unified framework that enhances fine-tuning performance while maintaining efficiency~\cite{MaoMHAM0YK22UniPELT,ChenZS0SY23S4}.
\end{itemize}

Our proposed method adopts parameter-efficient tuning focused on input feature adaptation. This design is motivated by the unique characteristics of tabular data: Tabular datasets are inherently heterogeneous, with varying structures and feature distributions across different datasets~\cite{Zhou2023TabToken,TabPTM,BorisovLSHPK24TabularSurvey}. Adapting TabPFN through input feature alignment effectively mitigates the constraints on input dimensionality, enhancing its applicability across a wider range of tasks.  By employing parameter-efficient fine-tuning, we align TabPFN with downstream datasets, addressing its existing limitations~\cite{Hollmann2022TabPFN}. %Additionally, 
% \name~is designed as a plug-and-play solution for TabPFN-like models, enabling seamless integration. As TFMs continue to evolve, our method has the potential to extend its applicability to an even wider array of tabular data scenarios.\looseness=-1

