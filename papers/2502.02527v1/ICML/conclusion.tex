
% In this paper, we present a novel method,~\name, to enhance TabPFN for downstream tabular tasks by reducing generalization error through the combined reduction of bias and variance. Our approach integrates Bagging and encoder-based fine-tuning using Batch Ensemble to achieve this.~\name address the limitations of training set size, feature dimensionality, and task complexity, such as handling multi-class tasks with more than 10 categories through ECOC coding.
% Experimental results demonstrate that our method achieves state-of-the-art performance on the largest available benchmark for tabular data, without requiring hyper-parameter tuning. By applying Bagging during the evaluation phase, our approach maintains computational efficiency while improving generalization. Moreover, it effectively overcomes the constraints imposed by dataset size and feature count, offering a practical solution for real-world tasks.
% In conclusion, our approach provides a scalable and efficient method for improving the performance of TabPFN on a wide range of tabular tasks. 
\section{Conclusion}
\label{sec:conclusion}
In this paper, we propose~\name, a novel approach that enhances the performance of TabPFN by simultaneously addressing both bias and variance. Through a combination of lightweight encoder-based fine-tuning and bootstrapped sampling,~\name~significantly improves TabPFNâ€™s adaptability to high-dimensional, large-scale, and multiclass classification tasks. Our method efficiently reduces bias by aligning downstream data distributions with the pre-trained TabPFN and reduces variance through diverse latent representations and robust inference techniques. Experimental results on over 200 benchmark datasets demonstrate that~\name~consistently outperforms or matches state-of-the-art methods, highlighting its potential to handle complex tabular data tasks with enhanced scalability and robustness.  These contributions provide a scalable and effective solution for leveraging TabPFN in real-world applications, ensuring its success across a broad range of tabular data challenges.

% In summary, we show that~\name~effectively enhances the performance of TabPFN on diverse tabular tasks, opening up new possibilities for applying pre-trained models to tabular data. Moving forward, the approach presented here can be further refined to handle even more complex datasets and contribute to the continued evolution of foundation models in machine learning.
