

% \section{\name}
% \label{sec:method}
% \begin{figure*}[h]
%     \vspace{-10mm}
%     \centering
%     \includegraphics[width=0.8\textwidth]{files/Beta.pdf}
%     \vspace{-10mm}
%     \caption{TODO:Overview of our proposed method~\name.}
%     \label{fig:overview}
% \end{figure*}

% To address both bias and variance limitations in TabPFN, we propose a unified strategy combining encoder-based fine-tuning and bagging. This approach enhances TabPFN's performance on downstream tabular datasets by reducing generalization error while minimizing computational overhead.\looseness=-1

% \textbf{Minimizing Bias with Encoder-Based Fine-Tuning.}

% \textbf{Mitigating Variance with Multiple Encoders.}

% \textbf{Enhancing Performance with Batch Ensemble for Computational Efficiency.}


% \textbf{Encoder-Based Fine-Tuning Using Batch Ensemble.}
% In our earlier experiments, we observed that simply permuting the order of features to increase model diversity could be unstable, sometimes increasing bias while reducing variance. This unsupervised approach, while straightforward, lacks guidance from the training data, making it less reliable. To address these issues, we propose a more guided and parameter-efficient strategy that enhances TabPFN's performance on downstream tasks.

% As shown in~\autoref{fig:overview}, we introduce a lightweight encoder fine-tuning scheme that aligns the raw feature space with multiple distinct latent spaces, each tailored for optimal processing by TabPFN. The goal is to reduce both bias and variance by better aligning the input features with the distribution of the downstream dataset.

% To achieve this, we freeze the parameters of the pre-trained TabPFN and introduce multiple learnable encoders, \( \boldsymbol{e}_k \), each transforming the raw input features \( \boldsymbol{x}_i \in \mathbb{R}^d \) into a fixed-dimensional latent space \( \boldsymbol{z}_i^{(k)} \in \mathbb{R}^m \). Each encoder \( \boldsymbol{e}_k(\boldsymbol{x}_i) \) learns a transformation that adapts the raw feature space into a representation better suited for downstream processing by TabPFN. These transformed representations \( \boldsymbol{z}_i^{(k)} \) are then provided to the pre-train TabPFN to produce the posterior predictive distribution over labels, \( p_\theta\left(\boldsymbol{y}_{\text{q}} \mid \boldsymbol{z}_i^{(k)}, \left(Z_{\text{train}}^{(k)},\boldsymbol{y}_{\text{train}}\right)\right) \).

% To further enhance the model's performance, we draw inspiration from~\citealp{Yury2024TabM} and apply \textbf{Batch Ensemble} to optimize fine-tuning. This technique enables TabPFN to make predictions across multiple latent spaces simultaneously without significantly increasing the number of parameters. The key idea behind Batch Ensemble is to replace traditional linear layers with small, low-rank adapters, allowing for efficient ensembling without explicitly defining separate models for each ensemble member.

% For a given linear layer \( l \) in the encoder, Batch Ensemble introduces shared weight matrices \( W \) across ensemble members, along with member-specific scaling factors \( r_i \) and \( s_i \) that are not shared. The output of the \( i \)-th ensemble member is defined as:

% \begin{equation}    
% l_i(\boldsymbol{x}_i) = s_i \odot \left( W (r_i \odot \boldsymbol{x}_i) \right) + b_i
% \end{equation}


% where \( \odot \) denotes element-wise multiplication, \( W \in \mathbb{R}^{d \times d} \) is shared, and \( r_i, s_i, b_i \in \mathbb{R}^d \) are specific to each member. These weights \( r_i \) and \( s_i \) are randomly initialized to ensure diversity among ensemble members. This architecture allows all ensemble members to be packed into a single model, enabling efficient parallel computation of predictions.

% % \textbf{Training Process:}

% During fine-tuning, we randomly sample two batches from the training set \( D_{\text{train}} = \{ (\boldsymbol{x}_i, \boldsymbol{y}_i) \}_{i=1}^{N_{\text{train}}} \): one batch serves as the \textit{query set} \( \boldsymbol{X}_{\text{q}} = [\boldsymbol{x}_{\text{q}}^{(i)}]_{i=1}^{N_{\text{q}}} \) and the other as the \textit{support set} \( (\boldsymbol{X}_{\text{s}},\boldsymbol{y}_{\text{s}}) =( [\boldsymbol{x}_{\text{s}}^{(i)}]_{i=1}^{N_{\text{s}}},[\boldsymbol{y}_{\text{s}}^{(i)}]_{i=1}^{N_{\text{s}}})\). Both \( \boldsymbol{X}_{\text{s}} \) and  \( \boldsymbol{X}_{\text{q}} \) are encoded into their respective latent spaces using multiple encoder paths \( \boldsymbol{e}_{\Phi}^{(k)} \), each producing a distinct representation, where \( \Phi \) denotes the encoder parameters. Specifically, we obtain the encoded representations:
% \begin{equation}
% \boldsymbol{Z}_{\text{s}}^{(k)} = \left[ \boldsymbol{e}_{\Phi}^{(k)}(\boldsymbol{x}_s^{(i)}) \right]_{i=1}^{N_{\text{s}}}, \quad \boldsymbol{Z}_{\text{q}}^{(k)} = \left[ \boldsymbol{e}_{\Phi}^{(k)}(\boldsymbol{x}_{\text{q}}^{(i)}) \right]_{i=1}^{N_{\text{q}}}    
% \end{equation}
% These encoded sets \( \boldsymbol{Z}_{\text{s}}^{(k)} \) and \( \boldsymbol{Z}_{\text{q}}^{(k)} \) are then passed to TabPFN to produce predictions for the query set \( \boldsymbol{y}_{\text{q}} \). The posterior predictive distribution over labels for the query set in latent space \( k \) is given by:
% \begin{equation}
% p^{(k)}_\theta(\boldsymbol{y}_{\text{q}}) =
% \frac{\exp\left(q_\theta\left(\boldsymbol{Z}_{\text{q}}^{(k)}, \left(\boldsymbol{Z}_{\text{s}}^{(k)},\boldsymbol{Y}_{\text{s}}\right)\right)[\boldsymbol{y}_{\text{q}}]\right)}{\sum_{c=1}^C \exp\left(q_\theta\left(\boldsymbol{Z}_{\text{q}}^{(k)}, \left(\boldsymbol{Z}_{\text{s}}^{(k)},\boldsymbol{Y}_{\text{s}}\right)\right)[c]\right)}    
% \end{equation}


% The model is trained by minimizing the total cross-entropy loss across all latent spaces. For each latent space, the cross-entropy loss is computed between the predicted distribution and the true labels \( {\boldsymbol{y}}_{\text{q}} \):
% \begin{equation}    
% \mathcal{L}_k = -\sum_{i=1}^{N_\text{q}} \sum_{c=1}^{C} \mathbb{I}_{{\boldsymbol{y}}^{(i)}_{\text{q}} = c} \log q_\theta(\boldsymbol{y}_{\text{q}}^{(i)} = c \mid \boldsymbol{Z}_{\text{q}}^{(k,i)}, (\boldsymbol{Z}_{\text{s}}^{(k)},\boldsymbol{y}_{\text{s}})) 
% \end{equation}
% The optimization objective for the fine-tuning phase is:
% \begin{equation}
% \min_{\Phi}\mathcal \sum_{k=1}^{K} \mathcal{L}_k.    
% \end{equation}
% This optimization process ensures that the model is jointly optimized to minimize both bias and variance, leveraging the encoded training data \( \boldsymbol{Z}_{\text{train}}^{(k)} \) and query set \( \boldsymbol{Z}_{\text{q}}^{(k)} \) to improve performance across multiple latent spaces.


% \textbf{Bagging for Reducing Variance in Evaluation. }
% Once the encoder-based fine-tuning is completed, we apply bagging as part of the evaluation phase to reduce the variance of TabPFN. Bagging, or Bootstrap Aggregating, is a well-established ensemble technique that enhances model stability by training multiple models on different random subsets of the training data and aggregating their predictions. Bagging does not increase the inference cost compared to the original TabPFN integration approach, while this technique has been largely overlooked in prior TabPFN approaches. 

% In our method, bagging is applied during the evaluation phase by sampling different subsets of both the training and test sets. Let \( K \) denote the number of encoder paths in our model. For each encoder path, we sample \( \min(N_{\text{train}}, N_{\text{Bagging}}) \) instances from the training set to construct the bootstrap-sampled support set. After encoding the support set and test data through the encoder path, TabPFN makes predictions based on these encoded representations. The final prediction is obtained by averaging the predictions across all \( K \) paths.

% For the \( k \)-th encoder path  (\( k \in \{1, 2, \dots, K\} \)), we denote the sampled support set as \( D_{\text{bootstrap}}^{(k)} \), where \( D_{\text{bootstrap}}^{(k)} = ( \boldsymbol{X}_{\text{bootstrap}}^{(k)}, \boldsymbol{Y}_{\text{bootstrap}}^{(k)}) \) is the set of instances sampled from the training set with replacement. The corresponding encoded input through the encoder path is \(\boldsymbol{e}_{\Phi}^{(k)}(X_{\text{bootstrap}}^{(k)}) \), where \( \boldsymbol{e}_{\Phi}^{(k)} \) denotes the encoder function for path \( k \). The prediction from TabPFN for the test sample \( \boldsymbol{x}_{\text{test}} \) under the \( k \)-th encoder path is given by:
% % \vspace{-5mm}
% \begin{equation}
% \hat{y}_k = q_{\theta}\left(\boldsymbol{e}_{\Phi}(\boldsymbol{x}_{\text{test}}),\left(\boldsymbol{e}_{\Phi}^{(k)}(X_{\text{bootstrap}}^{(k)}),\boldsymbol{Y}_{\text{bootstrap}}^{(k)}\right) \right)
% % \vspace{-5mm}
% \end{equation}
% The final prediction, \( \hat{y} \), is obtained by averaging the predictions from all \( K \) paths.

% \textbf{Expanding to MultiClass Tasks Beyond 10 Classes.} 
% A key limitation of TabPFN is its difficulty in handling classification tasks with more than 10 classes. To address this, we employ an Error-Correcting Output Code (ECOC) strategy. Unlike traditional methods that require separate ensembles of classifiers for each binary task, our approach uses distinct paths within a single encoder to manage the individual binary tasks. Each path is assigned to a specific binary classification task within the ECOC framework, which avoids the additional overhead typically associated with multiclass tasks, even when there are more than 10 classes.
% By leveraging the encoder structure efficiently,~\name~maintains scalability and computational efficiency without significantly increasing model complexity. As a result, TabPFN can be extended to handle multiclass tasks with many classes while preserving both training and inference efficiency.











% % \textbf{Inference Time and Parameter Efficiency Analysis.}
% % Finally, we evaluate the inference time and parameter efficiency of our approach. Through extensive experiments, we demonstrate that our bagging + encoder fine-tuning + Batch Ensemble method not only provides a performance boost over baseline models but also maintains efficient inference times. The total inference time of our approach is comparable to that of the original TabPFN, despite the introduction of additional processes like bagging and fine-tuning.
% % In terms of parameter efficiency, our method significantly reduces the storage requirements compared to traditional ensemble methods. Since we use Batch Ensemble to add only a small number of low-rank adapters instead of training multiple independent encoders, the total number of parameters is much lower. Specifically, the parameter size of our method is approximately 1/50 to 1/500 of that required by traditional ensemble models, depending on the dataset's feature count.
% % This makes our approach highly scalable to various datasets, providing strong performance without excessive memory overhead.

\section{\name}
\label{sec:method}
\begin{figure*}[h]
    \centering
    \vspace{-2mm}
    \includegraphics[width=\textwidth]{files/Beta.pdf}
    % \vspace{-10mm}
    \caption{Overview of the proposed method,~\name, which consists of the \textit{fine-tuning stage} and the \textit{inference stage}.}
    \label{fig:overview}
    \vspace{-5mm}
\end{figure*}

% To address both bias and variance issues of TabPFN observed during the previous experiments, we propose a unified strategy for improving the performance of TabPFN. Our approach introduces improvements in two main stages: the fine-tuning stage and the inference stage. In the fine-tuning stage, we progressively demonstrate how to refine the input representation to reduce both bias and variance. In the inference stage, we introduce the bootstrapped sampling strategy to further reduce variance without incurring additional computational overhead.\looseness=-1
To address both bias and variance issues of TabPFN observed during the previous experiments, we propose a unified strategy for improving the performance of TabPFN. In addition to the inference stage, we introduce a fine-tuning stage. Our approach improves performance in both fine-tuning and inference, as shown in~\autoref{fig:overview}. During fine-tuning, we refine input representations to better align the downstream data distribution with the pre-trained TabPFN, reducing both bias and variance. In the inference stage, we incorporate bootstrapped sampling to further reduce variance without additional computational overhead.%\looseness=-1

% \textbf{Minimizing Bias with Encoder-Based Fine-Tuning.}
% Our earlier experiments in~\autoref{sec:generalization_analysis_of_tabpfn}, highlighted the importance of minimizing bias to improve generalization. To address this, we propose a simple yet effective fine-tuning approach: freezing the parameters of the pre-trained TabPFN and introducing a lightweight encoder to adjust the input features. The encoder transforms the raw input data into a latent space that is  more compatible with the downstream task, aligning the model’s prior with the data distribution.\looseness=-1


% Let \( \boldsymbol{x}_i \in \mathbb{R}^d \) denote the raw feature vector, and let \( \boldsymbol{E}_\Phi \) represent the encoder with parameters \( \Phi \). To enhance the expressiveness of the model  and incorporate nonlinearity, we extend the basic encoder transformation \( \boldsymbol{E}_\Phi \) by adding several nonlinear layers. These layers are constructed using a sequence of operations as described by~\citet{GorishniyRKB21Revisiting}, including a linear transformation, ReLU activation, dropout regularization~\cite{SrivastavaHKSS14Dropout}, and an additional linear layer. The transformation is formally defined as:\looseness=-1
% \begin{equation}
% \boldsymbol{E}_{\Phi}(\boldsymbol{x}) = \text{Linear}\left( \text{Dropout}\left( \text{ReLU}\left( \text{Linear}\left(\boldsymbol{x} \right) \right) \right)\right)     
% \label{eq:encoder}
% \end{equation}
% This transformation is applied sequentially and can consist of multiple layers of the blocks shown in~\autoref{eq:encoder}, resulting in a final, more complex encoding function \( \boldsymbol{E}_\Phi \). By stacking these nonlinear operations, the encoder learns more sophisticated representations of the input data, enabling it to better handle the complexity of downstream tasks.

% Specifically, given a query set \( \boldsymbol{X}_q \) and a support set \( \left( \boldsymbol{X}_{s}, \boldsymbol{y}_{s} \right) \), the posterior predictive distribution over the query labels \( \boldsymbol{y}_{q} \) is expressed as:
% \[
% q_\theta\left(\boldsymbol{y}_{q} \mid \boldsymbol{Z}_q,\left(\boldsymbol{Z}_{s},\boldsymbol{y}_{s}\right)\right),
% \]
% where \( \boldsymbol{Z}_q \) and \( \boldsymbol{Z}_{s} \) represent the encoded latent spaces of the query set and the support set, respectively.

\textbf{Minimizing Bias with Encoder-Based {\color{red}\textit{Fine-Tuning}}.}  
Our analysis in~\autoref{sec:generalization_analysis_of_tabpfn} highlights the importance of minimizing bias for improved generalization. To achieve this, we introduce a lightweight encoder while keeping the pre-trained TabPFN parameters frozen. This encoder transforms raw input features into a latent space that better aligns the model’s prior with the downstream data distribution.  

Let \( \boldsymbol{x}_i \in \mathbb{R}^d \) denote the raw feature vector, and let \( \boldsymbol{E}_\Phi \) represent the encoder with parameters \( \Phi \). To enhance expressiveness and incorporate nonlinearity, these layers are constructed using a sequence of operations as described by~\citet{GorishniyRKB21Revisiting}:  
\begin{equation}
\boldsymbol{E}_{\Phi}(\boldsymbol{x}) = \text{Linear}\left( \text{Dropout}\left( \text{ReLU}\left( \text{Linear}\left(\boldsymbol{x} \right) \right) \right)\right).     
\label{eq:encoder}
\end{equation}  
This transformation can be extended by stacking multiple such blocks, allowing the encoder to learn hierarchical representations suited for complex downstream tasks.  

Given a query set \( \boldsymbol{X}_q \) and a support set \( \left( \boldsymbol{X}_{s}, \boldsymbol{y}_{s} \right) \), we define their respective latent representations as:  
\begin{equation}
\boldsymbol{Z}_{q} = \boldsymbol{E}_{\Phi}(\boldsymbol{X}_q), \quad  
\boldsymbol{Z}_{s} = \boldsymbol{E}_{\Phi}(\boldsymbol{X}_s).
\end{equation}  
The posterior predictive distribution over the query labels is then expressed as:  
\begin{equation}
q_\theta\left(\boldsymbol{y}_{q} \mid \boldsymbol{Z}_q, \left(\boldsymbol{Z}_{s},\boldsymbol{y}_{s}\right)\right).
\end{equation}  
This formulation ensures that the encoder effectively maps raw features into a structured space, enabling TabPFN to better capture relevant patterns while mitigating bias.

This fine-tuning method provides a lightweight and effective approach to reduce bias by better aligning the model’s prior with the downstream tasks.  It enables end-to-end fine-tuning, facilitating a more  efficient adaptation of the pre-trained TabPFN to the downstream tasks.


\textbf{Mitigating Variance with Multiple Encoders.}  
To further mitigate variance, we introduce multiple encoders, each learning a distinct transformation of the input data. By jointly training these encoders while keeping the pre-trained TabPFN parameters \(\theta\) frozen, the model captures diverse feature representations, reducing variance.\looseness=-1

For each encoder\( k \), the support and query set representations are encoded as follows:
\[
\boldsymbol{Z}_{\text{s}}^{(k)} = \left[ \boldsymbol{E}^{(k)}_{\Phi}(\boldsymbol{x}_s^{(i)}) \right]_{i=1}^{N_{s}}, \quad \boldsymbol{Z}_{q}^{(k)} = \left[ \boldsymbol{E}^{(k)}_{\Phi}(\boldsymbol{x}_{q}^{(i)}) \right]_{i=1}^{N_{q}},
\]
where $N_s$ and $N_q$
denote the number of samples in the support set and the query set, respectively.
The multiple encoders are trained jointly by minimizing the sum of the individual losses across all encoders. The optimization objective for the fine-tuning phase is:
\begin{equation}
\label{eq:loss}
\min_{\Phi}\mathcal{L}_{\text{total}} = -\sum_{k=1}^{K} \log\left(q_{\theta}\left(\boldsymbol{y}_{q} \mid \boldsymbol{Z}_{\text{q}}^{(k)}, \left(\boldsymbol{Z}_{\text{s}}^{(k)}, \boldsymbol{y}_{s}\right)\right)\right).
\end{equation}
Minimizing \( \mathcal{L}_{\text{total}} \) in~\autoref{eq:loss} ensures that the model jointly trains all encoders to generate diverse yet TabPFN-compatible latent representations, thus reducing variance and providing more stable predictions.


\textbf{Enhancing Performance with Batch Ensemble for Computational Efficiency.}
To further enhance performance without introducing additional computational cost, we integrate the Batch Ensemble technique into the encoder. Specifically, we replace the linear layers in \( \boldsymbol{E}_{\Phi} \) with Batch Ensemble versions, allowing the model to maintain diversity while avoiding the overhead of training multiple independent encoders. This technique introduces shared weight matrices and member-specific scaling factors, reducing the number of trainable parameters while preserving the benefits of ensembling.  

The output of the \( k \)-th base model for encoder layer \( l \) is given by:
\begin{equation}
l_k(\boldsymbol{x}) = s_k \odot \left( W (r_k \odot \boldsymbol{x}) \right) + b_k
\end{equation}
where \( W \) is shared across all base models, and \( r_k, s_k, b_k \) are specific to each base model~\cite{WenTB20BatchEnsemble}. \looseness=-1
% 这里要说明替换的地方是什么，member换成 base model

% \textbf{Bootstrapped Sampling for Variance Reduction in Inference.}  
% After fine-tuning, we apply bootstrapped sampling during inference to further reduce variance. This involves generating random subsets of the training set to create diverse support sets, which are then used with the test samples to compute predictions across multiple encoder paths. Aggregating these predictions stabilizes the final output without additional computational overhead.

% Let \( K \) represent the number of encoder paths. For each encoder path \( k \), we define the bootstrapped support set
% \(
% D_{\text{bootstrap}}^{(k)} = (\boldsymbol{X}_{\text{bootstrap}}^{(k)}, \boldsymbol{Y}_{\text{bootstrap}}^{(k)}).
% \)  
% After encoding the support set and the test data \( \boldsymbol{X}_{\text{test}} \) through each encoder path, the final prediction is obtained by averaging over all encoder paths:\looseness=-1
% \begin{equation}
% \hat{y} = \frac{1}{K} \sum_{k=1}^{K} q_{\theta}\left(\boldsymbol{E}_\Phi(\boldsymbol{x}_{\text{test}}), \left( \boldsymbol{E}_\Phi^{(k)}(\boldsymbol{X}_{\text{bootstrap}}^{(k)}), \boldsymbol{Y}_{\text{bootstrap}}^{(k)} \right) \right).
% \end{equation}  
% This technique effectively reduces variance during inference while maintaining computational efficiency, ensuring that the approach remains scalable and robust.

% \textbf{Expanding to MultiClass Tasks Beyond 10 Classes.}  
% A key limitation of TabPFN is its difficulty in handling classification tasks with more than 10 classes. To address this, we adopt an Error-Correcting Output Code (ECOC) strategy. Instead of training separate classifiers for each binary task, our approach utilizes distinct encoder paths within a single model to handle individual binary classification tasks. Each path is assigned to a specific binary subproblem within the ECOC framework, enabling multiclass classification without incurring additional computational overhead, even for tasks with more than 10 classes.

% \textbf{Summary of Our Approach.}  
% Our method effectively reduces both bias and variance while maintaining inference efficiency. To mitigate bias, we introduce a lightweight encoder that refines input representations, aligning them with the pre-trained TabPFN. Variance is further reduced by employing Bagging, which constructs diverse support sets through bootstrapped sampling, improving generalization on large datasets. Additionally, we extend TabPFN to handle multiclass classification tasks beyond 10 classes by integrating an Error-Correcting Output Code (ECOC) strategy, enabling efficient decomposition of multiclass problems into binary subproblems. Importantly, our approach preserves \textbf{PFN-style batching}, ensuring computational efficiency during inference while significantly enhancing TabPFN’s adaptability to large-scale and high-dimensional datasets.
\textbf{Bootstrapped Sampling for Variance Reduction in \textit{{\color{blue} Inference} Stage}.}  
To further reduce variance during inference, we apply bootstrapped sampling, generating random subsets of the training set as support sets. These are used to compute predictions across multiple encoders, and their aggregation stabilizes the final output without additional computational overhead.
For each encoder \( k \), the bootstrapped support set is
\(
D_{\text{bootstrap}}^{(k)} = (\boldsymbol{X}_{\text{bootstrap}}^{(k)}, \boldsymbol{Y}_{\text{bootstrap}}^{(k)}).
\)
The final prediction is obtained by averaging over all encoders:
\begin{equation}
\hat{y} = \frac{1}{K} \sum_{k=1}^{K} q_{\theta}\left(\boldsymbol{E}_\Phi(\boldsymbol{x}_{\text{test}}), \left( \boldsymbol{E}_\Phi^{(k)}(\boldsymbol{X}_{\text{bootstrap}}^{(k)}), \boldsymbol{Y}_{\text{bootstrap}}^{(k)} \right) \right).
\end{equation}  
This approach reduces variance while maintaining computational efficiency, ensuring scalability and robustness.

\textbf{Expanding to MultiClass Tasks Beyond 10 Classes.}  
To address TabPFN’s limitation in handling tasks with more than 10 classes, we integrate an Error-Correcting Output Code (ECOC)~\cite{DietterichB95ECOC} strategy. Instead of training separate classifiers, distinct encoders within a single model handle individual binary classification tasks, enabling efficient multiclass classification without additional computational overhead.

\textbf{Summary of Our Approach.}  
% Our method reduces bias via a lightweight encoder that refines input representations and aligns them with the pre-trained TabPFN. Variance is mitigated through Bagging, which constructs diverse support sets via bootstrapped sampling, improving generalization on large datasets. Additionally, ECOC extends TabPFN to handle multiclass tasks efficiently. Importantly, our approach preserves \textbf{PFN-style batching}, ensuring inference efficiency while enhancing scalability for large-scale and high-dimensional datasets.
Our method reduces bias using a lightweight encoder to align input representations with the pre-trained TabPFN, which is effective for high-dimensional data. Variance is reduced through Bagging with bootstrapped sampling, enabling better generalization on large datasets. For multiclass tasks, we integrate Error-Correcting Output Codes (ECOC) to efficiently handle more than two classes. Additionally, we preserve PFN-style batching, ensuring inference efficiency and scalability for large-scale and high-dimensional datasets.\looseness=-1



% 最后要有个总结，强调 bias variance，高维数据集，大数据集，之前两个数据集的观察实验