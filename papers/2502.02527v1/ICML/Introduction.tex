
\section{Introduction}
\label{sec:intro} 
Tabular data is one of the most widely used data formats across various domains, including finance~\cite{Financial_SVM}, healthcare~\cite{HassanAHK20}, e-commerce~\cite{nederstigt2014floppies}, and medical analysis~\cite{schwartz2007drug,subasi2012medical}. Despite its ubiquity, modeling tabular data with deep learning methods remains a challenge due to its heterogeneous nature~\cite{BorisovLSHPK24TabularSurvey}. 
Yet recent advancements have led to the development of tabular foundation models~\cite{WhyTFM}~, such as TabPFN (Tabular Prior-Fitted Networks)~\cite{Hollmann2022TabPFN,hollmann2025tabpfn}. TabPFN operates in two stages: \textit{pre-training} and \textit{inference}. During the pre-training stage, the model is pre-trained on a diverse set of synthetic datasets. In the inference stage, given a new task and a set of labeled examples as a ``prompt,'' TabPFN directly predicts the labels of test samples using in-context learning, without requiring further parameter updates. This approach enables TabPFN to achieve performance comparable to or even surpass tree-based methods, particularly on small tabular datasets~\cite{McElfreshKVCRGW23when}. % Pre-trained on synthetic data, TabPFN has demonstrated remarkable potential by achieving performance comparable to, or even surpassing, tree-based methods, particularly on small tabular datasets~\cite{McElfreshKVCRGW23when}. The strength of TabPFN lies in its ability to rapidly make accurate predictions on new tasks: given a new task along with a set of labeled examples as a ``prompt'', it can effectively predict the labels of test samples using in-context learning, without the need for further parameter updates.\looseness=-1
% 这里的思路要改一下


 
TabPFN has shown potential across a wide range of applications, including tabular data generation~\cite{TabPFGen}, data augmentation~\cite{TabMDA}, and time series forecasting~\cite{TabPFN-TS}. These use cases highlight the versatility of TabPFN, positioning it as a model worthy of further exploration. However, alongside application-driven studies, there is a growing interest in improving TabPFN's performance from various perspectives~\cite{TuneTables,LocalPFN,MixturePFN,TabForestPFN,MaTabDPT}. While previous research has reported performance improvements, the underlying reasons driving these gains remain unclear.  
These advancements are often fragmented, with each approach focusing on a specific aspect, and some methods even sacrifice efficiency for enhanced performance, without a comprehensive understanding of how to improve TabPFN systematically.

In this paper, we adopt the bias-variance decomposition framework introduced by~\citet{SF_PFN} to analyze the generalization error of TabPFN and its variants. This framework allows us to revisit and categorize existing methods, revealing that performance improvements typically arise from addressing either bias or variance. However, these approaches often neglect the other aspect, leading to suboptimal performance.  
Therefore, there is a need for a method that simultaneously addresses both bias and variance to improve performance. 
To this end, we propose a novel, efficient, and scalable approach,~\name. Our method enhances TabPFN’s performance by introducing a \textit{fine-tuning} stage, enabling parameter-efficient adaptation that aligns the downstream dataset distribution with the pre-trained TabPFN to mitigate bias. To further reduce variance~\name~maps raw data into multiple latent spaces and employs computationally efficient bootstrapped sampling during inference. Beyond performance improvements,~\name~offers a lightweight and scalable solution that effectively handles high-dimensional data and large datasets while maintaining inference efficiency.\looseness=-1

% To this end, we propose a novel approach,~\name, to simultaneously reduce both bias and variance. Our method improves TabPFN’s performance by reducing bias through better alignment of the downstream dataset distribution with the pre-trained TabPFN, and reducing variance by mapping raw data to multiple latent spaces and using bootstrapped sampling during the evaluation phase.~\name~not only enhances TabPFN's performance on downstream datasets, but also naturally addresses several other challenges, such as handling high-dimensional data, encoding categorical features, and scaling to larger datasets.
% 强调 efficient 和 scalable

\name~improves performance by refining both the fine-tuning and inference stages to 
reduce bias and variance. In the fine-tuning phase, we employ a lightweight encoder module as an input feature adapter, which transforms datasets with arbitrary dimensionality into multiple fixed-dimensional representations, thereby naturally enabling dimensionality reduction. To further enhance generalization, we integrate Batch Ensemble \cite{WenTB20BatchEnsemble,Yury2024TabM} to increase the number of encoders in a lightweight manner, which introduces diversity in learned representations and reduces variance. These enhancements enable \name~to improve robustness and scalability, ensuring better adaptation to downstream datasets. 
In the inference phase, we introduce bootstrapped sampling, a technique that has been largely overlooked in previous TabPFN variants. By generating multiple subsets of the dataset as support sets for the context composition, \name~reduces variance and improves robustness. Furthermore, \name~seamlessly integrates with Error-Correcting Output Codes (ECOC) to effectively handle multiclass classification tasks with more than 10 classes.

% With these improvements, our method provides a scalable and efficient solution to various tabular data challenges, ensuring smooth adaptation across a wide array of downstream tasks. 
Experimental results on multiple benchmark datasets, including over 200 classification tasks, demonstrate that our method significantly improves TabPFN's performance. We describe our main contributions below.
\begin{enumerate}[noitemsep,topsep=0pt,leftmargin=*]
    \item We introduce an adaptation method for TabPFN that addresses key limitations related to dataset size, high-dimensional features, and multiclass classification tasks.
    \item By analyzing the generalization error of existing TabPFN variants through bias-variance decomposition and experiments on real-world datasets, we developed~\name, a method that effectively mitigates both bias and variance. 
    \item We achieve state-of-the-art performance on the largest benchmark datasets to date, demonstrating the robustness and scalability of our method for real-world tabular tasks.
\end{enumerate}


\noindent{\bf Remark}.
We have noticed that the latest release of TabPFN-v2~\cite{hollmann2025tabpfn}~has partially alleviated some of the limitations previously discussed. Specifically, TabPFN-v2 incorporates design improvements that enable it to handle larger datasets and more features. However, it is important to highlight that TabPFN-v2 is a concurrent work, and while it partially mitigates these limitations, it does not fully resolve the challenges posed by dataset size and feature count. Thus, many of the improvements proposed in this paper are general enhancements that can complement TabPFN-v2 and potentially further its applicability to a broader range of tasks.\looseness=-1
