\section{Experiments}
\label{sec:exp}
\subsection{Experiment Setup}
\textbf{Datasets.} In our experiments, we evaluate~\name~on one of the largest publicly available tabular benchmark TALENT~\cite{YeACloser}, which includes 120 binary classification datasets and 80 multi-class classification datasets. These datasets are collected from various sources such as UCI, OpenML, Kaggle, and others. To ensure fairness, we remove two datasets, ``PizzaCutter3'' and ``PieChart3,'' as they overlap with TabPFNâ€™s validation set.
In addition, to validate the ability of~\name~to handle high-dimensional feature datasets, we also include 20 high-dimensional datasets sourced from the \href{https://jundongl.github.io/scikit-feature/datasets}{scikit-feature repository}. 

\textbf{Evaluation.}
For the TALENT datasets, we follow the evaluation protocol from~\cite{GorishniyRKB21Revisiting}. Each dataset is randomly split into training, validation, and test sets with proportions of 64\%, 16\%, and 20\%, respectively. For each dataset, we train each model using 15 different random seeds and calculate the average performance on the test set. We report accuracy as the evaluation metric, where higher accuracy indicates better performance.
For a more detailed comparison with other TabPFN variants,  we separately evaluate datasets with fewer than 10 classes, those with more than 10 classes, and high-dimensional datasets. Further details on the experimental setup, are provided in~\autoref{appendix:datasets}.
% \textbf{Evaluation.}  
% We follow the evaluation protocol from~\cite{GorishniyRKB21Revisiting} for TALENT datasets, where each dataset is randomly split into training (64\%), validation (16\%), and test (20\%) sets. Each model is trained with multiple random seeds, and accuracy is reported as the evaluation metric. To ensure a comprehensive comparison, we separately evaluate datasets with fewer than 10 classes, those with more than 10 classes, and high-dimensional datasets. Further details on the experimental setup, are provided in~\autoref{appendix:datasets}.


\textbf{Methods Compared.} We compare~\name~against five categories of methods to evaluate its effectiveness comprehensively:
(1) \textbf{Classical Machine Learning Algorithms:} This category includes widely used classical approaches such as Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and tree-based methods like Random Forest (RForest)~\cite{Breiman01RandomForest}, XGBoost (XGB)~\cite{chen2016xgboost}, CatBoost (CatB)~\cite{Prokhorenkova2018Catboost}, and LightGBM (LightG)~\cite{ke2017lightgbm}.
(2) \textbf{Tabular Deep Learning Models:} We consider state-of-the-art deep learning models for tabular data, including MLP, ResNet, FT-Transformer (FT-T)~\cite{GorishniyRKB21Revisiting}, MLP-PLR~\cite{Gorishniy2022On}, DCNv2~\cite{WangSCJLHC21DCNv2}, AutoInt~\cite{SongS0DX0T19AutoInt}, SNN~\cite{KlambauerUMH17SNN}, ExcelFormer (ExcelF)~\cite{Chen2023Excel}, DANets~\cite{ChenLWCW22DAN}, TabTransformer (TabT)~\cite{Huang2020TabTransformer}, and TabNet~\cite{ArikP21TabNet}.
(3) \textbf{Neighborhood-Based Methods:} To explore neighbor-based strategies, we evaluate TabR~\cite{gorishniy2023tabr} and ModernNCA (MNCA)~\cite{Ye2024ModernNCA}.
(4) \textbf{Ensemble-Based Methods:} We include methods that leverage ensemble strategies, such as TabM~\cite{Yury2024TabM}, NODE, and GrowNet.
(5) \textbf{TabPFN and Its Variants:} Lastly, we compare with TabPFN and its recent variants, including knnPFN~\cite{LocalPFN}, LocalPFN~\cite{LocalPFN}, MixturePFN~\cite{MixturePFN}, and TuneTables~\cite{TuneTables}. \looseness=-1

\textbf{Implementation Details.} All datasets are pre-processed following the methodology outlined in~\citet{GorishniyRKB21Revisiting}. For deep learning-based methods, we set the batch size to 1024. Hyper-parameters for the compared methods are tuned using Optuna~\cite{akiba2019optuna}, performing over 100 trials. The search ranges for hyper-parameters are determined based on~\citet{GorishniyRKB21Revisiting,TALENT} and the official implementations of each method. Once the optimal hyper-parameters are identified, they are fixed for the final evaluation using 15 random seeds. 
To ensure a fair comparison, \textbf{all TabPFN variants, including~\name}, are evaluated using their \textbf{default hyper-parameters without additional tuning}. More details can be found in~\autoref{appendix:datasets}. \looseness=-1


\subsection{\name: State-of-the-Art Performance}
We conducted pairwise significance testing using the Wilcoxon-Holm test~\cite{Demsar06Statistical} among~\name~and all the compared methods. To ensure a fair comparison, we selected 186 datasets from TALENT with fewer than 10 classes, as TabPFN and its variants are not capable of handling datasets with more than 10 classes.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{files/Beta_res.pdf}
    \vspace{-5mm}
    \caption{The critical difference diagrams based on the Wilcoxon-Holm test with a significance level
of 0.05 to detect pairwise significance for TALENT datasets with fewer than 10 classes.}
    \label{fig:Wilcoxon-Holm}
    \vspace{-5mm}
\end{figure}
From~\autoref{fig:Wilcoxon-Holm}, it is evident that~\name~outperforms other methods, \textbf{even without hyper-parameter tuning}. This includes methods based on nearest neighbors such as TabR and ModernNCA, ensemble-based approaches like TabM, traditional tree models, and other TabPFN variants. These results underscore the significant potential of pre-trained models for tabular data and demonstrate the effectiveness of our proposed method in adapting to downstream datasets. %The robustness and scalability of~\name~highlight its capability to deliver state-of-the-art performance across diverse tasks, leveraging its ability to simultaneously address bias and variance while maintaining computational efficiency.

% Next, we demonstrate the ability of~\name~to effectively extend its performance to large datasets, high-dimensional datasets, and classification tasks involving more than ten classes.
\begin{figure}[t]
    \centering
    \vspace{-3mm}
    \includegraphics[width=0.5\textwidth]{files/rank_size.pdf}
    \vspace{-5mm}
    \caption{The relative improvement of LocalPFN~\cite{LocalPFN}, MixturePFN~\cite{MixturePFN}, and ~\name~(Ours) over TabPFN across 173 tabular datasets sorted by dataset size (number of rows). The curves represent smoothed results to better illustrate trends in performance improvement as dataset size increases.}
    \label{fig:relative_improvement}
    \vspace{-5mm}
\end{figure}

\textbf{Scale to Large Datasets.} We evaluated~\name~and the variants of TabPFN on 173 tabular datasets, excluding those with more than 100 features, which TabPFN cannot handle. These datasets were sorted by the number of rows in ascending order, and the relative improvement of these methods over TabPFN was shown in~\autoref{fig:relative_improvement}. The results clearly show that the performance improvement by~\name~becomes more pronounced as the dataset size increases, highlighting the scalability and effectiveness of our approach in handling larger datasets. Notably, on the largest datasets in the benchmark, LocalPFN slightly outperforms~\name~in terms of accuracy. However, on more other datasets, \name~demonstrates superior performance. \looseness=-1

% \begin{table}[t]
% \centering
% \caption{Average ranks of methods on 17 high-dimensional datasets. Lower ranks indicate better performance.}
% \label{tab:high_dim_results}
% \begin{tabular}{lcc}
% \toprule
% \textbf{Method}          & \textbf{Average Rank} \\ \midrule
% \textbf{\name~(Ours)}             & \textbf{2.44}                  \\
% TabM                     & 3.29                  \\
% MLP                      & 3.29                  \\
% RandomForest             & 3.38                  \\
% XGBoost                  & 4.06                  \\
% ModernNCA                & 5.59                  \\
% KNN                      & 5.94                  \\ \bottomrule
% \end{tabular}
% \vspace{-5mm}
% \end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{files/high_average_ranks.pdf}
    \vspace{-5mm}
    \caption{Average ranks of methods on 17 high-dimensional datasets. We compare~\name~with TabM, KNN, MLP, XGBoost (XGB), RandomForest (RForest), ProtoGate (Pgate)~\cite{Jiang2024ProtoGate}, and ModernNCA (MNCA). Lower ranks indicate better performance.}
    \vspace{-5mm}
    \label{fig:high_dim_results}
\end{figure}
% \textbf{Handel high-dimensional datasets.} To evaluate the effectiveness of~\name~on high-dimensional datasets, we conducted experiments on 20 datasets with extremely high feature dimensions, as shown in~\autoref{tab:high_dataset_info}. These datasets were specifically chosen to test the scalability and adaptability of different methods in handling complex feature spaces. The average ranks of the compared methods are summarized in~\autoref{fig:high_dim_results}.
% From the results, it is evident that~\name~achieves the best performance with lowest average rank, outperforming all other methods. TabM and MLP also demonstrate competitive results, ranking second and third respectively. In contrast, traditional methods like RandomForest and XGBoost, as well as the deep learning method ModernNCA, show lower ranks, highlighting their limitations in such high-dimensional settings.
\textbf{Handling High-Dimensional Datasets.}  
To assess the effectiveness of~\name~on high-dimensional datasets, we conducted experiments on 20 datasets with extremely high feature dimensions, as detailed in~\autoref{tab:high_dataset_info}. These datasets were selected to evaluate the scalability and adaptability of different methods in complex feature spaces. The average ranks of the compared methods are summarized in~\autoref{fig:high_dim_results}.
The results show that~\name~achieves the best performance, attaining the lowest average rank and outperforming all other methods. TabM and MLP also demonstrate competitive results, ranking second and third, respectively. In contrast, traditional models such as RandomForest and XGBoost, as well as deep learning-based ModernNCA, exhibit lower ranks, highlighting their limitations in high-dimensional settings. Due to memory constraints, we were unable to compare with methods such as FT-T and TabR in this setting.\looseness=-1

\textbf{Performance on MultiClass Classification Tasks with More Than 10 Classes.} In addition, we further investigated the performance of~\name~on classification tasks with more than 10 categories. We selected 12 classification datasets with more than 10 classes from TALENT~\cite{YeACloser}~and compared~\name~with other methods. We report the mean accuracy of each method across the 12 datasets in~\autoref{fig:multiclass_res}~and find that~\name~outperforms the compared methods, demonstrating its superiority on multiclass classification tasks with a larger number of categories.



\subsection{Bias-Variance Analysis, Ablation Study, and Efficiency Comparison}

To evaluate the effectiveness of~\name~in addressing bias and variance, we conduct comprehensive experiments across diverse dataset sizes. Our findings reveal that~\name~consistently achieves lower generalization error than existing methods, demonstrating robust performance regardless of dataset scale. Bias is effectively reduced through encoder-based fine-tuning, aligning TabPFN with downstream data distributions, while variance reduction is achieved via Bagging and the introduction of multiple encoders. Importantly, our method remains effective even on small datasets, where excessive fine-tuning may otherwise increase bias.
In addition, we perform an ablation study to examine the contributions of key components, including the number of encoder paths, periodic activation, Batch Ensemble, and partial parameter tuning. Our results indicate that each of these design elements plays a crucial role in enhancing~\nameâ€™s performance. %Notably, partial parameter tuning offers an optimal trade-off between computational efficiency and accuracy.
Lastly, we compare the inference time and parameter count of~\name~against other methods to highlight its efficiency. A comprehensive analysis of bias-variance trends, ablation experiments, and efficiency comparisons, including inference time and parameter count, is provided in Appendix~\ref{appendix:bias-variance}, Appendix~\ref{appendix:ablation_study}, and Appendix~\ref{appendix:efficiency}.



