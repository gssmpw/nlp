\section{Related Works}
Many vision-language models extend pre-trained language models by integrating a general vision encoder, which extracts image features and aligns them with textual embeddings ____. Commonly used vision encoders include CLIP, SigLIP ____, InternViT, and others. These models are typically trained on large datasets of image-text pairs, allowing the extracted vision features to align naturally with textual inputs during pre-training. This alignment simplifies their integration with large language models (LLMs).

Despite this success, challenges arise in domain-specific tasks where general-purpose vision encoders may fail to capture the nuanced information necessary for strong task performance. Several studies have examined the variety of features produced by different vision encoders, highlighting notable discrepancies in their outputs. This observation has fueled efforts to design vision-language models tailored to specific domains, where encoder selection or combination is of critical importance.

Recent work has explored merging multiple vision encoders within a single multimodal architecture to improve feature representation. For instance, DeepSeek-VL employs a dual vision branch strategy: CLIP is used for semantic feature extraction, while a SAM-based encoder fine-tuned for high-resolution images captures finer details. Likewise, LLava-HR integrates high-resolution encoder outputs into CLIP’s lower-resolution feature space, thereby creating a richer representation.

The authors of the MOVA model ____ propose a specific routing mechanism for the LLM that directs each sample to the vision encoder best suited to its domain. Their results demonstrate that each specialized encoder excels in its corresponding domain. Although effective, this approach requires two inferences of the large language model—one for selecting the appropriate encoder and another for the final inference—and relies on a complex adapter to combine the outputs from different encoders into a single LLM.

Another approach for integrating outputs from different encoders into a single embedding is presented in Eagle ____, where the authors investigate various fusion strategies. They conclude that dimension-based concatenation performs well across a variety of tasks without excessively expanding the context.

In more recent work ____, the authors propose fusing embeddings from multiple encoders to enrich the context information about an image, accounting for both high- and low-resolution features. While these studies explore different ways to merge encoders into a single embedding, in MOVE we suggest routing encoders to avoid overloading the context with unnecessary information, inferring only the one needed. Moreover, this routing mechanism is lightweight, implemented as an MLP trained on top of the universal embedding derived from CLIP.