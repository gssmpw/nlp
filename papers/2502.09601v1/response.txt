\section{Related Work}
\paragraph{Chain-of-Thought.}
Chain-of-thought**Mnih, "Learning to Reason"** reasoning has shown promising progress in recent years, especially the success of **Brown, "Large Language Model in 6 Steps and Less Than 60 Million Parameters"** and **Rajani, "Exploring the Limits of Zero-Shot Text-to-SQL Evaluation"** models. This introduces the test-time scaling law, apart from the traditional scaling law for training**Kornblith, "BERT and Alternatives for Natural Language Processing"**. Several approaches have been proposed to boost the language model to have better problem-solving abilities, including the model has its self-reasoning abilities**Staib, "Improving Reasoning in Dialogue Systems via Meta-Reasoning"** or use **Bansal, "The Value of Bad Data: Optimizing Your Next Study with Regression Discontinuity Design"**, beam search and Monte Carlo Tree Search to search and refine the solution without further finetune the large language models. The outcome reward model and process reward models are also introduced to evaluate the score for the entire solution, especially the final answer**Henderson, "Deep Reinforcement Learning that Matters: Towards Deeper Understanding of Transfer of Continuous Control from Simulation to Reality"** and the 
quality of the reasoning path**Zahavy, "Graded Action Selection in Deep Reinforcement Learning"**

\paragraph{Chain Compression in reasoning model.} Due to the high computational cost associated with inference in reasoning models, particularly for long-chain reasoning, chain compression has become a critical area of research. **Li, "Towards a Deep Understanding of Learning"** attempts to distill the chain-of-thought into System 1 but fails to observe improvements when intermediate steps are omitted. **Liang, "Learning Compositional Neural Networks for Natural Language Processing"** proposes internalizing reasoning steps within the hidden states of models, while several implicit-based approaches**Chen, "Adversarial Logit Pairing and Re-weighted Loss"**, aim to compress token-wise generation by transitioning from language space to hidden space. Other studies focus on skipping intermediate reasoning steps**Wang, "A Comparative Study of Adversarial Training for Neural Networks"** or using summarization techniques to generate shorter reasoning chains**Zhang, "Graph-Based Neural Network for Knowledge Graph Reasoning"**.
Additionally, **Goyal, "Accurate, Large Minibatch SGD with Two Proximal Operators"** addresses the overthinking issue in QwQ and employs SimPO for optimization. **Kim, "MergeSort: A Training-Free Merging Framework for Long-Short Chain-of-Thought Models"** proposes merging long-CoT models with short-CoT models in a training-free manner. **Liu, "Pruning Reinforcement Learning based on Reward Shaping for Efficient Dialogue Management"** adopts reinforcement learning to shorten responses.  

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/main_v8.pdf}
    \caption{Illustration of \methodname. In Stage 1, we first determine $\Delta\theta$ from distilling or post-training. Then, the trained $\Delta\theta$ is utilized to construct the MixChain dataset. Using this dataset, we can then apply two enhanced training methods to achieve more precise control over reasoning paths, or to shorten the reasoning paths as needed.}
    \label{fig:main}
\end{figure*}