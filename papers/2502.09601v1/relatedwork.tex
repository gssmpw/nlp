\section{Related Work}
\paragraph{Chain-of-Thought.}
Chain-of-thought~\cite{wei2022chain} reasoning has shown promising progress in recent years, especially the success of OpenAi-O1~\citep{jaech2024openai} and Deepseek-R1 models~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}. This introduces the test-time scaling law, apart from the traditional scaling law for training~\cite{hoffmann2022trainingcomputeoptimallargelanguage}. Several approaches have been proposed to boost the language model to have better problem-solving abilities, including the model has its self-reasoning abilities~\citep{qwq-32b-preview} or use Best-of-N~\citep{Nakano2021WebGPTBQ}, beam search and Monte Carlo Tree Search~\citep{Kocsis2006BanditBM,guan2025rstar} to search and refine the solution without further finetune the large language models. The outcome reward model and process reward models are also introduced to evaluate the score for the entire solution, especially the final answer~\citep{Cobbe2021TrainingVT} and the 
quality of the reasoning path~\citep{wang-etal-2024-math,luo2025improve}

\paragraph{Chain Compression in reasoning model.} Due to the high computational cost associated with inference in reasoning models, particularly for long-chain reasoning, chain compression has become a critical area of research. \citep{Yu2024DistillingS2} attempts to distill the chain-of-thought into System 1 but fails to observe improvements when intermediate steps are omitted. \citep{deng2024implicit} proposes internalizing reasoning steps within the hidden states of models, while several implicit-based approaches\citep{deng2024explicitcotimplicitcot,hao2024traininglargelanguagemodels,cheng2024compressedchainthoughtefficient} aim to compress token-wise generation by transitioning from language space to hidden space. Other studies focus on skipping intermediate reasoning steps~\citep{liu2024can} or using summarization techniques to generate shorter reasoning chains~\citep{kang2024c3otgeneratingshorterchainofthought}.
Additionally, \cite{chen2024not} addresses the overthinking issue in QwQ~\citep{qwq-32b-preview} and employs SimPO~\citep{meng2024simpo} for optimization. Kimi K1.5~\citep{team2025kimi} proposes merging long-CoT models with short-CoT models in a training-free manner. O1-Pruner~\citep{luo2025o1} adopts reinforcement learning to shorten responses.  


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/main_v8.pdf}
    \caption{Illustration of \methodname. In Stage 1, we first determine $\Delta\theta$ from distilling or post-training. Then, the trained $\Delta\theta$ is utilized to construct the MixChain dataset. Using this dataset, we can then apply two enhanced training methods to achieve more precise control over reasoning paths, or to shorten the reasoning paths as needed.}
    \label{fig:main}
\end{figure*}