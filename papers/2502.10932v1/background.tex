\section{Background}
\label{sec:background}

\subsection{PPA (Performance, Power, Area) Estimation}
\label{sec:ppa_models}

%We use the machine learning models of~\cite{sengupta2022ppa} to predict values of \textbf{total negative slack, dynamic power and area} for a given synthesizable HDL code since these PPA models provide rapid inference compared to conventional tools. 

Given a circuit block described by synthesizable HDL code and the corresponding technology, its PPA (Performance, Power, Area) in terms of Total Negative Slack (TNS), dynamic power, and area can be estimated using the machine learning model technique introduced in~\cite{sengupta2022ppa}. Please note that machine learning models are trained using post-placement analysis data, and the dynamic power data is obtained through vectorless analysis with an EDA tool. The models are based on XGBoost, and one distinct model is trained for each process technology. There are two types of input features: HDL-based and synthesis parameters. An HDL code is parsed into Abstract Syntax Trees (ASTs), and the node/edge characteristics of the ASTs, such as the numbers of register bits and number of logic operator bits, are collected as features. Since post-placement PPA results depend on logic synthesis and placement parameters, e.g., clock period and placement density, these parameters are also taken as features. In the original work of~\cite{sengupta2022ppa}, only performance and power are included. We extended this technique by accounting for area as well.    

%The PPA models have the XGBoost architecture due to higher accuracy when predicting metrics compared to other architectures. There are two types of features for the PPA models such as HDL-based and constraint features. In the DHL-based features, the HDL code is translated to an Abstract Syntax Tree (AST), a tree-like representation of the code hierarchy. Then, all register assignments in the AST are identified to compute the DHL-based features such as total input bits, total register bits, total logic operator bits, etc. Furthermore, the constraint features are the aspect ratio, clock period, and place density that allows to predict PPA metrics under different simulation constraints. The features are fed to the models as a two-dimension vector. In our work, we utilize the synthesizable HDL code of a block to obtain the HDL-based features, and variate the aspect ratio in the constraint features to predict PPA metrics.

\subsection{PPO (Proximal Policy Optimization) Algorithm}
\label{sec:ppo_background}

PPO~\cite{schulman2017PPOclip} is a popular reinforcement learning algorithm and it was adopted in ChatGPT training. The advantages of the PPO are its simplicity, stability and efficiency, while often being computationally less expensive than other RL algorithms. Key elements for almost all reinforcement learning methods include the state space $\mathcal{S}$, the action space $\mathcal{A}$, reward function $R_a(s, s')$ when the state transitions from $s\in \mathcal{S}$ to $s' \in \mathcal{S}$ under action $a \in \mathcal{A}$, the policy function $\pi_\theta(s|a){:}\mathcal{S} {\rightarrow} \mathcal{A}$, which retrieves the probabilities of taking each action in $\mathcal{A}$ at state $s$,
and the value function $V_\phi(s_t)$, which estimates the expected return when starting in $s_t$ and following a policy. In modern RL algorithms, both the policy and the value function are often approximated by neural networks, whose parameters are denoted by 
$\theta$ and $\phi$. In each RL (or PPO) iteration, an action $a$ is taken according to the policy, the state transitions from $s$
to $s'$ and a reward is received. The received reward and its context $a, s,$ and $s'$ are used to incrementally train the policy and value networks, i.e., updating $\theta$ and $\phi$.


%More specifically, the PPO algorithm uses a clipped surrogate objective is a model-free and on-policy algorithm that support discrete action space~\cite{schulman2017PPOclip}. The PPO algorithm aims to learn the parameterized policy $\pi_\theta(s|a){:}\mathcal{S} {\rightarrow} \mathcal{A}$ that retrieves the optimal action $a$ for a given state $s$. 

In PPO, the training of parameter $\theta$ is performed using minibatch stochastic gradient ascent via
\begin{equation*}
  \theta_{u+1} = \argmax_\theta {\mathbb{E}}_t [L(s_t,a_t,\theta_u,\theta)],
\end{equation*}
where $u$ is the training step and $t$ is the episodic time. The surrogate objective $L$ is defined as
\begin{equation*}
  L(s_t,a_t,\theta_u,\theta) = \Min \left( p_t(\theta) \hat{A}_t, \text{clip} \left(p_t(\theta),1-\lambda,1+\lambda\right) \hat{A}_t \right),
\end{equation*}
where $p_t(\theta)$ is the probability ratio between the new policy $\pi_\theta(a_t|s_t)$ and the old policy $\pi_{\theta_u}(a_t|s_t)$, $\hat{A}_t$ is the advantage function equals to the difference between the discounted long-term rewards $\mathbb{E}_t \left[\sum_{l=0}^\infty \eta^l r_{t+l}|s_t,a_t \right]$ with a discount rate $\eta$ and the value function estimation $V_\phi(s_t)$, and $\lambda$ is a hyperparameter of the clip function. As $\hat{A}_t$ is positive, the objective $L$ increases if the action becomes more likely. Moreover, as $\hat{A}_t$ is negative, the objective $L$ also increases if the action becomes less likely. The clipped function determines how much the objective $L$ increases in a training step $u$ and constrains the new policy $\pi_\theta(a|s)$ from deviating too far away from the old policy $\pi_{\theta_u}(a|s)$.

%The value function $V_\phi(s_t)$ estimates the expected return when starting in $s_t$ following $\pi_{\theta_w}(s|a)$. 
In PPO, the training of the parameter $\phi$ is performed using minibatch stochastic gradient descent via
\begin{equation*}
  \phi_{u+1} = \argmin_\phi \sum_{t=0}^\infty \left(V_\phi(s_t)-\hat{V}(s_t) \right)^2,
\end{equation*}
where $\hat{V}(s_t) {=} \mathbb{E}_t \left[\sum_{l=0}^\infty \eta^l r_{t+l}|s_t \right]$ is the discounted sum of rewards when starting at $s_t$. 

%Furthermore, the policy $\pi_\theta(s|a)$ and value function $V_\phi(s)$ estimators are typically neural networws that are trained through several episodes that are sequences of interactions of the frameworw with the environment.