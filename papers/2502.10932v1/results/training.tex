\subsection{Policy and Value Function Training}
As described in~\ref{sec:ppo_background}, the policy $\pi_\theta(s|a)$ and value function $V_\phi(s_t)$ are neural networks that are trained iteratively by collecting trajectories of the agent's interactions with the environment following the policy $\pi_{\theta_u}(s|a)$ at iteration $u$. The reward of the trajectory is calculated, and the advantage function $\hat{A}_t$ is computed using the value function $V_{\phi_u}(s_t)$. The models are trained through interaction with the floorplan. For each trajectory, an initial B*-tree is randomly generated, and the reward is calculated as $-\left(f_{t+1} - f_t \right)$, where $f$ is defined in Equation~\eqref{eq:objective}, and $t$ is the episodic time. The implementation of both models is achieved using the PyTorch library. Training is performed on an NVIDIA GeForce MX130 GPU and takes approximately 1.3 hours on average. %The reward per trajectory is shown in Figure 