\section{Learning-Augmented Algorithms}
In this section we aim to design searching algorithm that utilize a (possibility erroneous) prediction $\pred$ regarding the actual optimal fractional strategy $\optf$. The error of the prediction is measured by its distance to the optimal solution in the $\ell$-infinity norm, i.e.
\begin{align}\label{eq:errordef}
    \eta = \max_{j}|\hat{\mu}_j - \mu^o_j|.
\end{align}
We show the following algorithm, modified from \mom, achieves a query complexity of $O(m \log m\eta \log\eta)$, note that since $\eta \leq n$, this guarantee matches the query complexity of \mom\ even if the prediction is arbitrarily wrong.

The algorithm begins by checking whether the floor of the predicted bidding strategy, $\floor{\hat{\mu}_j}$, for all $j $, is \texttt{ALMOST-OPTIMAL} using \optcheck. If it is, the algorithm applies \roundup\ and returns the optimal solution. If not, the algorithm assumes the error is small and attempts to search for the optimal solution within a restricted range around the predicted strategy $ \hat{\mu}_j$ on each platform, following a similar approach to the \mom\ algorithm. 

If the optimal solution is still not found, the search range is expanded, and the search is repeated. This process continues until a almost-optimal solution is identified. By progressively expanding the search range as the square of the previous range, we show that the query complexity is at most  $O(m \log (m\eta) \log \eta)$. Please refer to Algorithm~\ref{alg:bmom} for a formal description.

\begin{algorithm}[h]
\DontPrintSemicolon
\SetAlgoLined
\LinesNumbered
\SetNoFillComment
% \KwIn{}
\textbf{Initialize: $\ell_j \gets \hat{\mu}_j$, $r_j \gets \hat{\mu}_j$ for all $j \in \platform$}

$\pred \gets \floor{\pred}$

\lIf{\optcheck($\pred$) == \texttt{ALMOST-OPTIMAL}}{\Return{\roundup($\pred$)}}
$i \gets 0$ \tcp*{initialize the counter for doubling process}

\While{TRUE}{
$\ell_j \gets \hat{\mu}_j - 2^{2^i}$, $r_j \gets \hat{\mu}_j+2^{2^i}$ for all $j \in \platform$

range-indicator $\gets$ TRUE \tcp*{assume range is correct}

\While{range-indicator == TRUE}{

$\mu_j \gets \frac{\ell_j + r_j}{2}$ for all $j \in \platform$

query each platform $j$ strategy $\mu_j$, obtain $v_j(\mu_j)$, $c_j(\mu_j)$ and $\mc_j(\mu_j)$

rank the platforms in non-decreasing order of $\mu_j$ s.t. if $i \leq j$, $\mu_i \leq \mu_j$

$j^* \gets \min_{j}(|\sum_{i \leq j} (r_i - \ell_i) - \sum_{i \geq j}(r_i - \ell_i)|)$ \tcp*{find the $j^*$ that equally split the search space}

$k \gets \mc_{j^*}(\mu_{j^*})$


% query each platform $j$ strategy $\ell_j$, obtain $v_j(\ell_j)$, $c_j(\ell_j)$ and $\mc_j(\ell_j)$ 

\If{there exist a $\mc_j(\hat{\mu}_j - 2^{2^i}) > k$}
{$\ell_j \gets \mu_j+1$ for $j \leq j^*$ \tcp*{$k$ is too small} \label{line:leftcheck}}

\Else{$\bid^k \gets \matchingmc([\hat{\mu}_j -2^{2^i}, \hat{\mu}_j+2^{2^i}]\ \forall j, k)$ %\tcp*{$O(m \log \frac{1}{\epsilon})$ queries} 

\If{$\optcheck(\bid^k)) == $ \texttt{NOT-$\bid^k$}}{$r_j \gets \mu_j-1$ for all $j \geq j^*$ \tcp*{$k$ is too large}\label{line:rightcheck}}

\ElseIf{$\optcheck(\bid^k)$ == \texttt{INFEASIBLE}}{
$r_j \gets \mu_j-1$ for all $j \geq j^*$ \tcp*{$k$ is too large}\label{line:infeasible}
}

\uElseIf{$\optcheck(\bid^k)$ == \texttt{ALMOST-OPTIMAL}}{\Return{\roundup($\bid^k$)}\label{line:opt}}
\Else{\tcp*{$\optcheck(\bid^k)$ == \texttt{NOT-OPTIMAL}}
$\ell_j \gets \mu_j+1$ for all $j \leq j^*$ \tcp*{$k$ is too small}
}\label{line:notoptimal}}
\If{there exist a platform with $\ell_j > r_j$}{
range-indicator $\gets$ FALSE \tcp*{search in the given range is complete}}
}


$i \gets i+1$ \tcp*{update the search range}
}
\SetAlgoRefName{2}
\caption{\bmom}
\label{alg:bmom}
\end{algorithm}

%\mznote{In Line 21 of the algorithm, if the return is not-mu-k, it must be the case that some platform hits the boundary. If it hits the lower bound, should we make $k$ larger instead of smaller?}
%\xtnote{I think the case you mentioned is in Line 15-16 so in line 21 the case is assumed away already, so we will only be hitting the upper bound)}

\begin{theorem}\label{thm:momwithpredictions}
Given any instance $\mathcal{I}$, and predicted optimal bidding strategy $\pred$ such that the error of the $\pred$ is $\eta$, the \bmom\ algorithm finds the optimal bidding strategy with at most $O(m \log m \eta \log \eta)$ queries.
\end{theorem}
\begin{proof}
We first argue the correctness of the algorithm. Let \( \eta \) denote the error of the prediction as defined in \eqref{eq:errordef}, and let \( i^* \) be the smallest \( i \) such that \( 2^{2^i} \geq (\eta + 1) \). We will show that the algorithm does not terminate in any round \( i < i^* \). Let \( \bid^* \) represent the almost-optimal integral bidding strategy. When \( i < i^* \), there exists at least one platform \( j \) such that \( \mu^*_j \notin [\ell_j, r_j] \) at the beginning of the \( i^* \)-th iteration of the outer while loop. By the correctness of \optcheck\ and since the algorithm only searches within the range \( [\ell_j, r_j] \), it cannot return a solution in earlier rounds.

Next, we argue that the algorithm will terminate in round \( i^* \) with the optimal solution. Since \( 2^{2^{i^*}} \geq (\eta + 1) \), we know that \( \mu^*_j \in [\ell_j, r_j] \) for all \( j \) at the start of the \( i^* \)-th iteration of the outer while loop. Now, consider the search process during this round. As in the proof of Theorem~\ref{thm:medianofmedians}, we show correctness by proving that during the execution of the \( i^* \)-th iteration, there always exists some \( \mu \in [\ell_j, r_j] \) such that \( \mu^{\mc_j(\mu)} = \bid^* \). Considering all possible updates to \( \ell_j \) and \( r_j \) for each platform, we will now show that none of these updates eliminate any such \( \mu \) values.

First, in Line~\ref{line:leftcheck}, the algorithm encounters a platform \( j \) where \( \mc_j(\hat{\mu}_j - 2^{2^{i^*}}) > k \), meaning the current candidate marginal cost \( k \) is smaller than the marginal cost of the smallest strategy within the current search range for that platform. Since \( \mu^*_j \in [\hat{\mu}_j - 2^{2^{i^*}}, \hat{\mu}_j + 2^{2^{i^*}}] \) for all platforms and \( \bid^* = \bid^k \) for some \( k \), this implies that the current marginal cost candidate \( k \), as well as all marginal costs weakly smaller than \( k \), cannot correspond to the optimal marginal cost \( \bid^* \). These marginal costs (and their corresponding strategies) are thus eliminated from the search range.

In Line~\ref{line:rightcheck}, the algorithm is in the case where \( \optcheck(\bid^k) == \texttt{NOT}-\bid^k \), indicating that \( \bid^k \) is not optimal. This implies that there exists a platform \( j \) such that: 
1. \( \mu^k_j = \hat{\mu}_j + 2^{2^{i^*}} \), i.e., the largest strategy, and 
2. for the same platform \( j \), \( \mc_j(\hat{\mu}_j + 2^{2^{i^*}}+1) \leq k \). 
This means that \( k \), along with all marginal costs weakly greater than \( k \), exceeds the optimal marginal cost corresponding to \( \bid^* \). These marginal costs (and their corresponding strategies) are therefore eliminated from the search range.

The remaining cases are handled in the same way as discussed in Theorem~\ref{thm:medianofmedians}. In Line~\ref{line:infeasible}, when \( \bid^k \) is infeasible, we eliminate all marginal costs weakly greater than the current one being tested. In Line~\ref{line:notoptimal}, when \( \bid^k \) is not optimal, we eliminate all marginal costs weakly smaller than the current one. Finally, in Line~\ref{line:opt}, once we find \( \bid^* \), we use \roundup\ to obtain the optimal fractional strategy.

We now prove the query complexity of the algorithm. Let \( i^* \) be the value of \( i \) when the algorithm terminates. First, we have \( i^* \leq \eta^2 \), where \( \eta \) is the error of the prediction as defined in \eqref{eq:errordef}. By Lemma~\ref{lem:matchingmc}, we know that the \matchingmc\ operation in iteration \( i \) takes \( m \log 2^{2^i} \) time. Additionally, by Theorem~\ref{thm:medianofmedians}, the while loop within this iteration will run \( \log (m 2^{2^i}) \) times. Since all other subroutines take \( O(m) \) queries, and the size of the search range is squared at each step, the algorithm terminates when the search space is weakly larger than \( n \).
{\allowdisplaybreaks
\begin{align*}
    \sum_{i = 0}^{\log \log i^*} m \log (m \cdot 2^{2^i}) \cdot \log 2^{2^i} 
    =& \sum_{i = 0}^{\log \log i^*} m (\log m + 2^i) 2^i\\
    =& m (\log m + 2^{\log \log i^*+1}) \cdot 2^{\log \log i^* +1}\\
     =& m (\log m + 2 \log i^*) \cdot 2 \log i^*\\
     \leq& m (\log m + 2 \log (\eta)^2) \cdot 2 \log (\eta)^2\\
     =& m (\log m +4\log (\eta)) \cdot 2 \log (\eta)\\
     =& O(m \log(m \eta)\log \eta)\qedhere
\end{align*}}
\end{proof}

As a corollary, we also achieved "best-of-both-worlds" results in terms of consistency and robustness. Specifically, if the provided prediction is correct (or even "almost correct," i.e., $\floor{\pred} = \floor{\optf}$), only $2m$ queries are required (note that even checking that a bidding profile is feasible requires $m$ queries).
%gagan: changed the above sentence
Since $\eta \leq n$ by definition, the total number of queries will never exceed $O(m \log (mn) \log n)$, which matches the query complexity of \mom:
%\xtnote{if we include the lower bounds discussion, we should include the statement below as well}
\begin{corollary}
The \bmom\ algorithm is $2m$-consistent and $O(m \log mn \log n)$ robust, where the robustness matches the query complexity of \mom.
\end{corollary}