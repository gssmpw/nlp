\section{Related Work}
Real sparse datasets often exhibit specific structural patterns (e.g., clusters of non-zero values or correlations), which naive approaches such as simple thresholding of dense data cannot replicate. Sparse data often retains critical information in specific locations (e.g., sensor data in particle physics, occurrences of rare events). Thus, domain-specific approaches have been introduced **Kingma, "Intro to Variational Methods"**.

\paragraph{Sparse continuous data generation}
Past work on sparse, continuous data generation focuses on simple architectures and specific applications, such as calorimeter sensor data in physics, which are incomparable to recent state-of-the-art generative models. They decouple the sparsity information for generating data by introducing decoupled generative models where the sparsity is introduced as a learnable Dirac delta mass at zero **Higgins, "Beta-VAE"**.

\paragraph{Sparse discrete data generation} Other works focusing on sparse discrete data generation use Poisson distributions to model sparse count data with bursts **Kingma, "Improved Variational Autoencoders"**. There are also deep variants **Rezende, "Variational Inference for Deep Learning"**. However, as already indicated, they are not applicable to our setting, which has continuous sparse data that do not exhibit the traits of a Poisson distribution.

\paragraph{Dense data generation using diffusion models} Diffusion models fall into two main categories: continuous state-space **Sohl-Dickstein, "Deep Unsupervised Learning"** for continuous data and discrete state-space diffusion models **Ho, "Denoising Diffusion Models"** for discrete data. Discrete state-space diffusion models can model sparse discrete data exactly as zero might be one of its tokens. However, they do not apply to our broad setting with continuous data. Also, Bit Diffusion **Song, "Generative Modeling"** has shown that continuous state-space diffusion models can model discrete data reliably, even outperforming discrete state-space models.

\paragraph{Enforcing sparsity} Sparsity is not just an inherent trait in datasets that can be, e.g., exploited to store data efficiently. Sparsity can also help in the model architecture. Previous work **Srivastava, "Dropout: A Simple Way"** shows that neural networks are greatly overparameterized and multiple methods **Srivastava, "DropConnect"**, **Chen, "Training Sparse Neural Networks"** inter alia have been introduced to mitigate this overparameterization by sparsifying the underlying neural network weights. Others **Gale, "Sparse Neural Networks"** exploit this property to train sparse weight matrices added to existing weights efficiently. However, these methods focus on enforcing sparsity in the model weights and are therefore not applicable to our setting, where we enforce sparsity in the model's output.