%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{xurl}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{listings}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Sparse Data Generation Using Diffusion Models}

\begin{document}

\twocolumn[
\icmltitle{Sparse Data Generation Using Diffusion Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Phil Ostheimer}{rptu}
\icmlauthor{Mayank Nagda}{rptu}
\icmlauthor{Marius Kloft}{rptu}
\icmlauthor{Sophie Fellenz}{rptu}
\end{icmlauthorlist}

\icmlaffiliation{rptu}{Department of Computer Science, RPTU Kaiserslautern-Landau, Kaiserslautern, Germany}


\icmlcorrespondingauthor{Phil Ostheimer}{ostheimer@cs.uni-kl.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Diffusion Models, Data Sparsity}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Sparse data is ubiquitous, appearing in numerous domains, from economics and recommender systems to astronomy and biomedical sciences. However, efficiently and realistically generating sparse data remains a significant challenge. We introduce Sparse Data Diffusion (SDD), a novel method for generating sparse data. SDD extends continuous state-space diffusion models by explicitly modeling sparsity through the introduction of Sparsity Bits. Empirical validation on image data from various domains—including two scientific applications, physics and biology—demonstrates that SDD achieves high fidelity in representing data sparsity while preserving the quality of the generated data.

\end{abstract}

\section{Introduction}
\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\includegraphics[scale=0.7]{figures/dataset_sample.png}
\includegraphics[scale=0.7]{figures/ddim_sample.png}
\includegraphics[scale=0.7]{figures/sdd_sample.png}
\includegraphics[scale=0.7]{figures/dataset_sample_sparsity.png}
\includegraphics[scale=0.7]{figures/ddim_sample_sparsity.png}
\includegraphics[scale=0.7]{figures/sdd_sample_sparsity.png}
\caption{Shown are in the first row from the left to the right: MNIST images sampled from the dataset, DDIM sampled images, SDD sampled images. The second row contains the respective sparsity information. Despite highly visually similar images, DDIM fails to reflect the sparsity whereas the proposed SDD has a similar sparsity as the images from the dataset.}
\label{fig:sparsity_demo}
\end{center}
\vskip -0.2in
\end{figure*}

Sparse data is a fundamental challenge in many real-world applications, where most values in a dataset are zero or missing. Examples include spikes in brain activity, where neurons fire only at specific moments, and recommendation systems, where users interact with only a small fraction of available products. In single-cell RNA data, sparsity arises because only a subset of genes is expressed in each cell. Similarly, calorimeter images in high-energy physics experiments capture particle interactions, but only a few regions register meaningful energy deposits. Handling sparse data efficiently is essential for improving model performance, storage, and computational efficiency in these domains \citep{Lu:2019,Lu:2021,Lahnemann:2020}.

Generative models have revolutionized artificial intelligence by enabling systems to create realistic, high-dimensional data across many applications. Techniques such as Generative Adversarial Networks (GANs) \citep{Goodfellow:2020}, Variational Autoencoders (VAEs) \citep{Kingma:2014}, Normalizing Flows \citep{Rezende:2015} and more recently Diffusion Models (DMs) \citep{Sohl-Dickstein:2015,Ho:2020,Song:2020,Austin:2021} have pushed the boundaries of what machines can generate, demonstrating state-of-the-art performance in image and audio generation.%, outperforming traditional methods in quality and diversity. The rapid progress in generative modeling continues to push the boundaries of artificial intelligence, fostering advancements in data augmentation and simulation-based learning, among other areas.

Despite significant advances in generative modeling, a critical gap remains in developing models explicitly designed for sparse data. Directly generating sparse data ensures that models learn realistic structures and distributions, preserving meaningful relationships that thresholding dense data would distort. Sparse data is crucial for applications like data augmentation, where realistic but varied samples improve model robustness, and compressed representations, which reduce storage and computational costs while maintaining essential information. 

% and struggle to capture key properties of sparsity, such as high dimensionality with predominantly zero or missing values and non-uniform distributions.
 Most state-of-the-art generative approaches are optimized for dense data. As a result, directly applying traditional generative models to sparse data often leads to unrealistic reconstructions and a failure to preserve essential sparse structures (see Figure \ref{fig:sparsity_demo}). We argue that this discrepancy arises from applying continuous generative models to data with inherently discrete sparsity patterns, where the presence or absence of values is a crucial structural feature. Task-specific approaches \citep{Lu:2019, Lu:2021} have attempted to address this by modeling sparsity as a discrete random variable. However, these methods are neither widely applicable nor integrated with state-of-the-art generative models. %Furthermore, the lack of benchmarks and standardized tools for sparse data generation complicates progress in this area, highlighting the need for dedicated research to bridge this gap.

To bridge this gap, we introduce Sparse Data Diffusion (SDD) to generate sparse data. SDD explicitly learns to represent the sparsity of the generated data as a separate discrete variable. Sparsity Bits (SB) model for each output dimension, whether it is sparse or not. We model SBs as discrete variables using continuous state-space diffusion models, thereby maintaining the versatility and superior performance of continuous state-space diffusion models over discrete state-space diffusion models. During sampling, SBs are applied to enforce sparsity. Additionally, SBs make the output more interpretable, as they can be easily visualized and interpreted separately from the dense output. We evaluate the proposed approach on sparse images and in challenging applications from physics and biology. In summary we make the following contributions:
\begin{itemize}
    \item We introduce SDD, a novel method for enforcing sparsity in the output of diffusion models.
    \item We propose to model sparsity as a discrete variable with a continuous state-space diffusion model.
    \item Evaluation of SDD on image data and in the physics and biology domain shows superior capability of our method in generating sparse data.
\end{itemize}

\section{Why Is It Hard for a Diffusion Model to Generate Sparse Data?}
\label{sec:motivation}

 As an illustrating example, consider Figure \ref{fig:sparsity_demo}: while MNIST \citep{Lecun:1998} images are highly sparse (80.9\%), images generated by diffusion models—specifically Denoising Diffusion Implicit Models (DDIM) \citep{Song:2020}—trained on MNIST do not maintain this sparsity, showing only 39\% sparsity despite visually resembling real data, making them easily distinguishable. Why is this the case?

The output of a diffusion model directly depends on the activation function in the last layer. Even with a linear activation function in the last layer, it is challenging for a diffusion model to output exactly zero due to the continuous nature of gradient-based optimization and parameter updates. The linear activation function computes a weighted sum of inputs plus a bias, and achieving an exact zero output requires precise cancellations of these terms. During training, the weights and biases are adjusted incrementally, making such exact cancellations unlikely. Furthermore, floating-point arithmetic introduces numerical precision limitations, meaning the output will typically approximate zero rather than reaching it exactly.

Considering the multitude of activation functions \citep{Dubey:2022}, the traditionally used activation functions, such as Sigmoid and Tanh,  have properties that make it difficult to achieve exact zeros. The Sigmoid activation function asymptotically approaches zero for large negative inputs but never exactly reaches it. Similarly, the Tanh function approaches zero for inputs close to zero, but never outputs exactly zero. The smooth nature of these functions, combined with the continuous optimization process, makes an output of exact zeros highly unlikely without additional modifications. Similar considerations apply to exponential activation functions such as ELU \citep{Clevert:2016} or SiLU \citep{Hendrycks:2016}. While they can approach zero as the input becomes large and negative, they will never output exactly zero due to the continuous nature of the function, the incremental adjustments made during training, and the limitations of floating-point arithmetic.

Rectified activation functions such as the Rectified Linear Unit (ReLU) \citep{Abien:2018} can output exactly zero for a certain range of the input. The following reasoning applies to most rectified activation functions. ReLU only produces zero when its input is less than or equal to zero. This condition depends on the weighted sum of inputs and the bias aligning precisely to meet $x=0$, which is rare during training. When $x<0$, the gradient of ReLU becomes zero, causing the neuron to stop learning and potentially remain inactive. While ReLU activations can generate outputs very close to zero, achieving exact zeros is uncommon unless specific techniques like $L_1$ regularization or sparsity-promoting activations are applied.

\section{Method}
\label{sec:method}
This section presents Sparse Data Diffusion (SDD), a novel framework designed for the generation of sparse data. We begin by introducing the underlying statistical model, followed by an in-depth explanation of the forward and backward diffusion processes, the denoising procedure, and the overall training and sampling algorithms.

\subsection{Statistical model} 
The following statistical model forms the basis for SDD: Let $\mathbf{x_0} \sim p$ with $\mathbb{E}\left[\mathbf{x_{0}}\right]<\infty$ and $\mathbf{x_0} \in \mathbb{R}^d$, where $p$ is unknown and arbitrary. We infer the sparsity for each dimension by applying the indicator function element-wise to $\mathbf{x_0}$:
\begin{equation}
    \mathbf{\bar{x}_0}=\mathbb{1}_{x\neq0}(\mathbf{x_0})
\end{equation}


As a result, $\mathbf{\bar{x}_0} \in \{0,1\}^d$ is a binary vector that encodes, for each element in $\mathbf{x_0}$, whether it is zero or represents a dense value. Therefore, we refer to each element in $\mathbf{\bar{x}_0}$ as a \emph{Sparsity Bit (SB)}. We obtain the extended input $\mathbf{\hat{x}}_0 \in \mathbb{R}^{2d}$, where $\mathbf{\hat{x}}_0 \sim \hat{p}$, by concatenating $\mathbf{x_0} \in \mathbb{R}^{d}$ and $\mathbf{\bar{x}}_0 \in \{0,1\}^{d}$:
\begin{equation}
    \mathbf{\hat{x}_0}=\langle \mathbf{x_0},\mathbf{\bar{x}_0} \rangle
\end{equation}

\paragraph{Forward diffusion} The forward diffusion process follows previous work \citep{Sohl-Dickstein:2015,Ho:2020,Song:2020,Song:2021}. It consists of a predefined series of transitions from the input space $\mathbf{\hat{x}_0} \in \mathbb{R}^{2d}$ to pure noise $\boldsymbol{\epsilon} \in \mathbb{R}^{2d}$, where $\boldsymbol{\epsilon} \sim \mathcal{N}(0,I)$. The transition from $\mathbf{\hat{x}_0}$ to $\mathbf{\hat{x}_t}$ is defined as
\begin{equation}
    \mathbf{\hat{x}_t} = \sqrt{\alpha(t)}\mathbf{\hat{x}_0} + \sqrt{1-\alpha(t)}\boldsymbol{\epsilon},
\end{equation}

where $\boldsymbol{\epsilon} \sim \mathcal{N}(0,I)$, $t \sim \mathcal{U}(0,T)$ is a continuous time variable, and $\alpha$ is the noise schedule, a monotonically decreasing function from 1 to 0. In the limit, we obtain:
\begin{equation}
    \lim_{T\to\infty} \mathbf{\hat{x}_T} = \boldsymbol{\epsilon} \sim \mathcal{N}(0,I),
\end{equation}

\paragraph{Backward diffusion} The backward diffusion process consists of steps that reverse the forward diffusion process:   $\mathbf{\hat{x}_T} \rightarrow \mathbf{\hat{x}_{T-\Delta}} \rightarrow ... \rightarrow \mathbf{\hat{x}_0}$. These steps follow a normal distribution:
\begin{equation}
    \mathbf{\hat{x}_{t-1}} \vert \mathbf{\hat{x}_{t}} \sim \mathcal{N}(\mu_{t}(\mathbf{\hat{x}_t},\mathbf{\hat{x}_0}),\sigma_{t}^2)
\end{equation}
where $\mathbf{\hat{x}_0}$ is the denoised input. Since $\mathbf{\hat{x}_0}$ is not given in the backward diffusion process, we train
a denoising neural network $f_{\theta}$ to predict $\mathbf{\hat{x}_0}$. We describe the training process in the following Section \ref{sec:training} and illustrate our model in Figure \ref{fig:graphical_model}. 

\begin{figure}[t!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/statistical_model.drawio.pdf}}
\caption{Shown is an illustration of our method SDD. Compared to other diffusion models, we expand the input by Sparsity Bits for forward diffusion and use them in backward diffusion and sampling to enforce sparsity in the data.}
\label{fig:graphical_model}
\end{center}
\vskip -0.2in
\end{figure}

\lstset{
    escapeinside={(*@}{@*)},  % Allow escaping to LaTeX code
    columns=flexible,
    basicstyle=\ttfamily\small,
    language=Python,
%    numbers=left
}




\subsection{Training}
\label{sec:training}
In order to perform the steps $\mathbf{\hat{x}_T} \rightarrow \mathbf{\hat{x}_{T-\Delta}} \rightarrow ... \rightarrow \mathbf{\hat{x}_0}$ of the backward diffusion process, we train a neural network $f_{\theta}$ to predict $\mathbf{\hat{x}_0}$, which is part of the distribution $p(\mathbf{\hat{x}_{t-1}} \vert \mathbf{\hat{x}_{t}})$. This network, $f_\theta$, is called denoising neural network and outputs $\mathbf{\tilde{x}_0}=f_{\theta}(\mathbf{\hat{x}_t},t)$ as the predicted value of $\mathbf{\hat{x}_0}$ (alternative formulations predict the noise $\boldsymbol{\epsilon}$) from $\mathbf{\hat{x}_t}$ and $t$. 

Self-conditioning \citep{Chen:2023} additionally incorporates the previously computed $\mathbf{\tilde{x}_0}$ (or $\boldsymbol{\tilde{\epsilon}}$) from time step $t+1$ to compute the next $\mathbf{\tilde{x}_0}$ (or $\tilde{\epsilon}$). Thus, $f_{\theta}$ takes the form $f_{\theta}(\mathbf{\hat{x}_t},t,\mathbf{\tilde{x}_0})$. The estimated $\mathbf{\tilde{x}_0}$ (or $\boldsymbol{\tilde{\epsilon}}$) is then used to transition from $\mathbf{\hat{x}_t}$ to $\mathbf{\hat{x}_{t-\Delta}}$ using $p(\mathbf{\hat{x}_{t-1}} \vert \mathbf{\hat{x}_{t}})$. The training is realized using an $l_2$ regression loss:
\begin{equation}
    \mathcal{L}(\theta) = 
    \mathbb{E}\left[\left\Vert f_{\theta}(\sqrt{\alpha(t)}\mathbf{\hat{x}_0}+\sqrt{1-\alpha(t)}\boldsymbol{\epsilon},t)-\mathbf{\hat{x}_0}\right\Vert^2\right],
\end{equation}
where $\mathbf{\hat{x}_0} \sim \hat{p},t \sim \mathcal{U}(0,T),\boldsymbol{\epsilon
} \sim \mathcal{N}(0,I)$. We summarize the SDD training process in Algorithm \ref{alg:training}.

\begin{algorithm}[t!]
\caption{SDD training algorithm.}
\begin{lstlisting}[]
def train_loss(x):
  # Create & concat sparsity bits
  # Cast sparsity bits as real numbers. 
  x_sparse = (x == 0).float()
  x_sparse = (x_sparse*2-1)*scale
  x = (x*2-1)
  x = cat((x, x_sparse), dim=1)
    
  # Forward diffusion steps
  t = uniform(0, 1)
  eps = normal(mean=0, std=1)
  x_t = sqrt(alpha(t))*x
      +(1-sqrt(alpha(t))*eps

  # Compute self-cond estimate. 
  x_0 = zeros_like(x_t)
  if uniform(0, 1) > 0.5:
    x_0 = net(cat([x_t, x_0], -1), t)
    x_0 = stop_gradient(x_0) 
    
  # Predict and compute loss. 
  x_0 = net(cat([x_t, x_0], -1), t) 
  loss = (x_0-x)**2

  return loss.mean()
\end{lstlisting}
\label{alg:training}
\end{algorithm}

\subsection{Sampling} To draw samples, we use the same state transitions as described in the backward diffusion process: $\mathbf{\hat{x}_T} \rightarrow \mathbf{\hat{x}_{T-\Delta}} \rightarrow ... \rightarrow \mathbf{\hat{x}_0}$, following $p(\mathbf{\hat{x}_{t-1}} \vert \mathbf{\hat{x}_{t}})$ using $f_{\theta}(\mathbf{\hat{x}_t},t,\mathbf{\tilde{x}_0})$. There are multiple ways to perform these steps. Here, we focus on Denoising Diffusion Probabilistic Models (DDPM) \citep{Ho:2020} and Denoising Diffusion Implicit Models (DDIM) \citep{Song:2020}. 

We take an additional last step: $\mathbf{\hat{x}_0} \rightarrow \mathbf{x_0}$. Since the first $d$ dimensions in $\mathbf{\hat{x}_0}$ contain  the dense and second $d$ dimensions in $\mathbf{\hat{x}_0}$ contain the SBs, we quantize the SB output dimensions $\mathbf{\hat{x}_{0;d:(2d-1)}}$ using a simple thresholding operation (values $>0$ are set to $1$ and values $\le0$ are set to $0$) to extract the SBs. Afterward, we apply an element-wise product to get the sparsified output:
\begin{equation}
    \mathbf{x_0} = \mathbf{\hat{x}_{0;0:(d-1)}} \odot \mathbf{\hat{x}_{0;d:(2d-1)}}
\end{equation} 
We summarize the sampling algorithm in Algorithm \ref{alg:sampling}. 

\begin{algorithm}[t!]

\caption{SDD sampling algorithm.}
\begin{lstlisting}[language=Python]
def generate(steps, interm=False):
  x_t = randn(mean=0, std=1)
  x_pred = zeros_like(x_t)
    
  for step in range(steps):
    # Get time for current & next states. 
    t_now = 1-step/steps 
    t_next = max(1-(step+1)/steps, 0)

    # Predict x_0. 
    x_pred =
      net(cat([x_t, x_pred], -1), t_now)
        
    # Estimate x at t_next. 
    x_t = ddim_or_ddpm_step(
      x_t, x_pred, t_now, t_next)

  # Return sparsified image
  return data2sparse(x_pred)
\end{lstlisting}
\label{alg:sampling}
\end{algorithm}

\section{Experiments}
Our experiments to evaluate SDD for sparse data generation begin with image data, where we demonstrate and visualize the general characteristics of SDD in generating sparse data. We then present results from applications in the sciences, specifically physics and biology.

\subsection{Experimental setup and implementation details}
\paragraph{Baselines} We compare SDD to two state-of-the-art diffusion models, namely DDPM \citep{Ho:2020} and DDIM \citep{Song:2020} with changes in the underlying architecture of the denoising network $f_\theta$ as described below.

\begin{table*}[t]
\caption{Shown are the results for sparse image generation. FID scores are similar for all considered settings. DDPM and DDIM are not able to reflect the sparsity on the logit level, only the thresholded pixels show sparsity at all. SDD reflects the sparsity for pixels and logits closely.}
\label{tab:sparse_image_generation}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{c | c  c  c | c  c  c}
        \toprule
        & \multicolumn{3}{c}{Fashion-MNIST} &\multicolumn{3}{c}{MNIST} \\
        Model & $\downarrow$ FID & P-Sparsity & L-Sparsity & $\downarrow$ FID & P-Sparsity & L-Sparsity\\
        \midrule
        Dataset & - & 50.2\% & - & - & 80.9\% & - \\
        \midrule
        DDPM & 25.62 & 23.2\% & 0.0\% & 23.35 & 38.8\% & 0.0\%  \\
        DDIM & 24.11 & 23.3\% & 0.0\% & 23.68 & 39.3\% & 0.0\% \\
        \midrule
        SDD-SB DDPM & 28.56 & 10.8\% & 0.0\% & 23.29 & 30.2\% & 0.0\%\\
        SDD-SB DDIM & 27.03 & 12.5\% & 0.0\% & 24.34 & 31.5\% & 0.0\%\\
        SDD DDPM & 27.81 & 44.5\% & 44.3\% & 25.37 & 74.9\% & 74.7\%\\
        SDD DDIM & 26.37 & 44.6\% & 44.1\% & 25.69 & 74.8\% & 74.6\%\\
        \bottomrule  
    \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in    
\end{table*}

\paragraph{Datasets and evaluation} 
For sparse image generation, we use MNIST \citep{Lecun:1998} (with 80.9\% sparsity) and Fashion-MNIST \citep{Xiao:2017} (with 50.2\% sparsity), both containing 60,000 gray-scale training images of size 28$\times$28 each. The primary evaluation metric for images is the Fr\'echet Inception Distance (FID) \citep{Heusel:2017} calculated between 50,000 generated images and the entire  training set. Additionally, we analyze the sparsity of pixels and logits (before discretization by thresholding logit values in $\left[0,1\right]$  to discrete values in $\left[0,255\right]$) and compare them to the sparsity within the dataset. This distinction is made since pixel sparsity may not reflect the true sparsity of logits. 

We use sparse data from particle physics. In particular, we use calorimeter images from a muon isolation study (with 98\% sparsity, size 32$\times$32). The intensity of each pixel represents the energy deposited in a specific cell, which is the sum of the transverse momenta $P_T$ of particles that hit that calorimeter cell. The dataset consists of 33,331 signal images and 30,783 background images. The signal corresponds to isolated muons, while the background consists of muons produced in association with a jet. We evaluate the generated calorimeter images by computing the Wasserstein Distance $W_P$ between the distributions of invariant mass and transverse momentum $P_T$ for the entire dataset and 50,000 generated calorimeter images, as in previous work \citep{Lu:2019,Lu:2021}.

We experiment with two single-cell RNA (scRNA) datasets: the Tabula Muris dataset \citep{Schaum:2018} with 57K cells (with 90\% sparsity, 98\% when filtered to the 1000 most highly variable genes) and the Human Lung Pulmonary Fibrosis dataset \citep{Habermann:2020} with 114K cells (with 91\% sparsity, 96\% when filtered to the 1000 most highly variable genes). As in previous work \citep{Luo:2024}, we use the Spearman Correlation Coefficient (SCC), Pearson Correlation Coefficient (PCC), the Maximum Mean Discrepancy (MMD) \citep{Gretton:2012}, and the Local Inverse Simpson’s index (LISI) \citep{Haghverdi:2018} for evaluation comparing the real dataset to 10,000 generated cells. 



\paragraph{Architecture} We use the U-Net architecture as in previous work \citep{Ho:2020, Ronneberger:2015, Nichol:2022}. We apply this architecture to all considered models for image and calorimeter image generation as the underlying  data have a similar size. We use a base channel dimension of 256, three stages, and two residual blocks per stage, with a total of 37M parameters (37M for SDD, with an increase of only 0.07\% in the number of parameters). For sparse data generation of scRNA data, we follow scDiffusion \citep{Luo:2024} and use a multi-layer perceptron with skip connections with a total of 5M parameters (7M for SDD, an increase of 38.37\% in the number of parameters) as scRNA data lacks the spatial structure of images. The increase in the number of parameters is much higher as we have to double the size of the input and output layers. At the same time, we only need to double the number of input and output channels for the convolutional neural network-based U-Net for images.

\paragraph{Other settings} We use Adam \citep{Kingma:2015} for optimization. We train 300K steps per task for image and scRNA generation and 200K steps for calorimeter image generation with a constant learning rate of 0.0002 and a batch size of 256. An exponential moving average of the parameters is used to improve training dynamics with a decay factor of 0.9999. We use a fixed number of 1000 steps for sampling and DDIM and DDPM.




\subsection{Vision: sparse image generation}
For sparse image generation, we summarize our results in Table \ref{tab:sparse_image_generation}.

\paragraph{Quality of generated images} DDPM and DDIM show slightly lower FID scores than SDD and SDD-SB (SDD before applying Sparsity Bits=SDD minus SB) on Fashion-MNIST, with similar results on MNIST. While SDD has a slightly lower FID than SDD-SB on Fashion-MNIST, the FID scores for SDD are slightly higher on MNIST. While SDD with DDIM sampling shows lower FID scores for Fashion-MNIST, SDD with DDPM sampling shows lower FID scores for MNIST. However, the validity of FID for evaluating sparse images is questionable, as the standard Inception network used to compute FID was trained only on dense images from ImageNet \citep{Deng:2009}, which may limit its effectiveness for sparse data. This is underlined by the lack of distinguishability in Figure \ref{fig:sparsity_demo} and \ref{fig:fashion_mnist_images}.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{figures/fashion_mnist_image_grid.png}
\includegraphics[width=\columnwidth]{figures/ddim_image_grid.png}
\includegraphics[width=\columnwidth]{figures/sdd_image_grid.png}
\caption{The first row shows Fashion-MNIST images sampled from the dataset, the second row shows DDIM sampled images, and the third row shows SDD sampled images. The generated images are mostly indistinguishable from the real images.}
\label{fig:fashion_mnist_images}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\includegraphics[width=0.7\columnwidth]{figures/fashion_mnist_sparsity_distribution.pdf}
\hspace{1cm}
\includegraphics[width=0.7\columnwidth]{figures/mnist_sparsity_distribution.pdf}
\caption{Shown is the image pixel sparsity density histogram (20 bins) of the dataset and 50,000 generated images using DDIM and SDD. The sparsity level of DDIM does not resemble the real data while the proposed SDD matches it closely.}
\label{fig:sparsity_distribution}
\end{center}
\vskip -0.2in
\end{figure*}

\paragraph{Pixel sparsity} 
Regarding pixel sparsity, the sparsity levels for DDPM and DDIM significantly differ from those in the dataset. Specifically, for both datasets, the pixel sparsity of discrete pixel values in DDPM and DDIM is roughly half that of the dataset. In contrast, our method, SDD, more closely matches the dataset's pixel sparsity, with only a five-percent point average difference. SDD-SB exhibits even lower pixel sparsity than DDPM and DDIM. This suggests that SDD encodes sparsity in the SB and, thus, does not focus on this aspect in the dense logits. Overall, SDD with DDIM sampling shows closer pixel sparsity than the dataset's pixel sparsity compared to SDD with DDPM.

\paragraph{Logit sparsity} For the even more challenging logit sparsity, DDPM and DDIM fail to reflect the dataset's sparsity and exhibit close to 0.0\% sparsity. SDD, in contrast, matches almost the pixel sparsity across both datasets, suggesting that the additional thresholding to get the sparse pixel values does not add much. SBs capture the sparsity of the pixels almost perfectly. SDD-SB with DDPM and DDIM also completely fail to reflect the logit sparsity. 



\paragraph{Further pixel sparsity analysis} We further analyze the distribution of sparsity in the generated images of the pixels in Figure \ref{fig:sparsity_distribution}. The histogram over image sparsity for SDD closely matches the one for the datasets, while DDIM has a different sparsity distribution in the generated images. While there is still some overlap for Fashion-MNIST, the overlap for MNIST is almost zero, indicating that the sparsity of the images is always underestimated compared to the dataset.


\paragraph{Sharpness of generated SBs} As SBs are central to SDD and we have previously shown that SDD without SB (SDD-SB) generated images do not match the dataset sparsity, we further examine the distribution of the logits for the SBs in Figure \ref{fig:sparsity_bits_distribution}. As can be seen, SBs are sharply distributed towards 0 and 1, indicating the model's confidence in predicting these pixel values as zero or non-zero. Also, the last bin with an upper bound of 1.0 shows almost precisely the pixel sparsity of the respective dataset (50.2\% for Fashion-MNIST and 80.9\% for MNIST).

\begin{figure}[t!]
\vskip 0.2in
\begin{center}
\includegraphics[width=0.49\columnwidth]{figures/sparsity_bits_distribution_fashion_mnist.pdf}
\includegraphics[width=0.49\columnwidth]{figures/sparsity_bits_distribution_mnist.pdf}
\caption{The histogram distribution shows 50 bins of the logits for sparsity bits for 50,000 generated images using SDD DDIM. The sparsity bits are extremely concentrated towards 0 and 1.}
\label{fig:sparsity_bits_distribution}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Physics: calorimeter image generation}

\paragraph{Quantitative results} Results are summarized in Table \ref{tab:sparse_calo_image_generation}. The Wasserstein Distance $W_p$ between the distributions of transverse momentum $P_T$ for both signal and background images is greater for DDPM and DDIM compared to SDD DDPM and SDD DDIM, indicating that the latter models generate distributions that more closely match the real data. Similarly, the $W_p$  for the invariant mass is larger for DDPM and DDIM across both signal and background images when compared to SDD DDPM and SDD DDIM, further demonstrating the improved fidelity of SDD. These results suggest that SDD is better aligned with the true data distribution, ultimately improving the quality and realism of the generated calorimeter images.

\begin{table}[t]
\caption{Shown are the results for sparse calorimeter image generation. SDD performs better in preserving the key structural properties of the data: transverse momentum $P_T$ and invariant mass.}
\label{tab:sparse_calo_image_generation}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{c | c  c | c  c}
        \toprule
        & \multicolumn{2}{c}{Signal} &\multicolumn{2}{c}{Background} \\
        Model & $\downarrow$ $P_T$ & $\downarrow$ Mass & $\downarrow$ $P_T$ & $\downarrow$ Mass \\
        \midrule
        DDPM & 199.52 & 67.19 & 187.58 & 64.80  \\
        DDIM & 221.35 & 74.78 & 270.24 & 92.64  \\
        \midrule
        SDD DDPM & 20.68 & 5.66 & 9.05 & 3.29  \\
        SDD DDIM & 20.81 & 5.60 & 14.12 & 3.91 \\
        \bottomrule
    \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in    
\end{table}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{figures/muon_signal.pdf}
\includegraphics[width=\columnwidth]{figures/muon_background.pdf}
\caption{Muon signal images are shown to the left, and Muon background images are shown to the right. The first row in both Figures shows images sampled from the dataset, the second row is DDIM sampled images, and the third is SDD sampled images. The pixel intensity measured in GeV is plotted on a log scale where white means zero. DDIM fails to reflect the dataset's sparsity and general traits, while SDD succeeds.}
\label{fig:muon}
\end{center}
\vskip -0.2in
\end{figure*}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\includegraphics[width=0.49\columnwidth]{figures/muon_signal_avg.pdf}
\includegraphics[width=0.49\columnwidth]{figures/muon_background_avg.pdf}
\caption{The average Muon signal images are shown to the left, and the average Muon background images are shown to the right. DDIM fails to generate realistic data, while SDD succeeds. Linear scale to reveal the signal and background differences.}
\label{fig:muon_avg}
\end{center}
\vskip -0.2in
\end{figure}

\paragraph{Qualitative evaluation} We show example calorimeter images of isolated muons (signal) and muons produced in association with a jet (background) in Figure \ref{fig:muon}. The Figure shows that DDIM-sampled images fail to reflect the cylindrical shape of the calorimeter images and the sparsity of the calorimeter images sampled from the dataset, while SDD DDIM can reflect them. We also show the pixel-wise averages of calorimeter images in Figure \ref{fig:muon_avg}, underlining the superior performance of SDD compared to DDIM as DDIM shows a consistent level of detections in the whole average image. Also, SDD reflects the almost uniform distribution within the cylindrical shape of the calorimeter for signal images, while the background images show the muons produced from a jet that deposits energy close to the muon.





\begin{table*}[t]
\caption{The results for sparse data generation on scRNA data are shown. SDD outperforms DDPM and DDIM in all considered metrics and closely matches the sparsity of the respective dataset.}
\label{tab:sparse_scrna_generation}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{c | c  c  c  c c | c  c  c  c  c}
        \toprule
        & \multicolumn{5}{c}{Tabula Muris} & \multicolumn{5}{c}{Human Lung Pulmonary Fibrosis} \\
        Model & $\uparrow$ SCC & $\uparrow$ PCC & $\downarrow$ MMD & $\uparrow$ LISI & Sparsity & $\uparrow$ SCC & $\uparrow$ PCC & $\downarrow$ MMD & $\uparrow$ LISI & Sparsity \\
        \midrule
        Dataset & - & - & - & - & 97.9\% & - & - & - & -  & 95.9\%\\ 
        \midrule
        %scDM & 0.13 & 0.06 & 3.46 & 0.00 & 49.1\% & 0.40 & 0.30 & 3.19 & 0.00 & 47.8\% \\
        DDPM & 0.49 & 0.72 & 3.58 & 0.00 & 49.1\% & 0.31 & 0.85 & 3.35 & 0.00 & 49.1\% \\
        DDIM & 0.50 & 0.74 & 3.62 & 0.00 & 49.3\% & 0.30 & 0.87 & 3.34 & 0.00 & 49.3\%\\
        \midrule
        SDD DDPM & 0.96 & 0.95 & 0.12 & 0.21 & 98.0\% & 0.95 & 0.99 & 0.18 & 0.08 & 96.9\%\\
        SDD DDIM & 0.97 & 0.97 & 0.12 & 0.23 & 98.0\%& 0.95 & 0.99 & 0.18 & 0.08 & 96.8\%\\
        \bottomrule
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in   
\end{table*}


\subsection{Biology: scRNA data generation}
For scRNA data generation, we summarize our results in Table \ref{tab:sparse_scrna_generation}. 

\paragraph{Metrics} As seen on both datasets, Tabula Muris and Human Lung Pulmonary Fibrosis, SDD with DDPM and DDIM performs better than DDPM and DDIM. SCC and PCC indicate that SDD has a more similar ordering and a \textquotedblleft more linear\textquotedblright\ relationship between the average expression values of each gene compared to the dataset. The distance between the gene expression distributions measured by MMD indicates that the distance for SDD is much lower than for DDPM and DDIM. The mixing between real and generated data is indicated through the LISI score and shows that SDD produces expression values closer to the original dataset.

\paragraph{Sparsity} In terms of sparsity, DDPM and DDIM fail to reflect the sparsity of the two datasets. They only represent roughly half of the sparsity present in the dataset itself. This result is similar to the results from the two vision tasks. However, it is more severe on scRNA data as the generated data does not reflect, e.g., dropout events properly. SDD, on the other hand, gets very close to the datasets' sparsity and almost perfectly matches it for Tabula Muris. For Human Lung Pulmonary Fibrosis, the sparsity levels slightly overshoot the datasets' sparsity. 


\section{Related Work}
Real sparse datasets often exhibit specific structural patterns (e.g., clusters of non-zero values or correlations), which naive approaches such as simple thresholding of dense data cannot replicate. Sparse data often retains critical information in specific locations (e.g., sensor data in particle physics, occurrences of rare events). Thus, domain-specific approaches have been introduced \citep{Lu:2019,Lu:2021}.

\paragraph{Sparse continuous data generation}
Past work on sparse, continuous data generation focuses on simple architectures and specific applications, such as calorimeter sensor data in physics, which are incomparable to recent state-of-the-art generative models. They decouple the sparsity information for generating data by introducing decoupled generative models where the sparsity is introduced as a learnable Dirac delta mass at zero \citep{Lu:2019,Lu:2021}. 

\paragraph{Sparse discrete data generation} Other works focusing on sparse discrete data generation use Poisson distributions to model sparse count data with bursts \citep{Schein:2016}. There are also deep variants \citep{Gong:2017,Guo:2018, Schein:2019}. However, as already indicated, they are not applicable to our setting, which has continuous sparse data that do not exhibit the traits of a Poisson distribution. 

\paragraph{Dense data generation using diffusion models} Diffusion models fall into two main categories: continuous state-space \citep{Sohl-Dickstein:2015,Ho:2020,Song:2020,Song:2021} for continuous data and discrete state-space diffusion models \citep{Austin:2021, Gu:2022} for discrete data. Discrete state-space diffusion models can model sparse discrete data exactly as zero might be one of its tokens. However, they do not apply to our broad setting with continuous data. Also, Bit Diffusion \citet{Chen:2023} has shown that continuous state-space diffusion models can model discrete data reliably, even outperforming discrete state-space models. Bit Diffusion is an approach to generate discrete data with continuous state-space diffusion models. Bit Diffusion uses real numbers to represent the bit representations of discrete data. Because of the flexibility and better performance of continuous state-space diffusion models, we focus on them as they are the most versatile and can be applied to continuous and discrete data. 


\paragraph{Enforcing sparsity} Sparsity is not just an inherent trait in datasets that can be, e.g., exploited to store data efficiently. Sparsity can also help in the model architecture. Previous work \citep{Han:2015,Ullrich:2017} shows that neural networks are greatly overparameterized and multiple methods \citep{Molchanov:2017,Louizos:2018,Sun:2024,Lee:2018} inter alia have been introduced to mitigate this overparameterization by sparsifying the underlying neural network weights. Others \citep{Hu:2022} exploit this property to train sparse weight matrices added to existing weights efficiently. However, these methods focus on enforcing sparsity in the model weights and are therefore not applicable to our setting, where we enforce sparsity in the model's output.

\section{Conclusion}
We introduce SDD, a novel method for generating sparse data using diffusion models. The main idea is to encode the sparsity information per dimension as SBs as a discrete variable and cast them to real numbers in the continuous state-space, keeping its versatility. We show that SDDs are interpretable and flexible while applying to a wide range of tasks that inherently have sparsity in the data: applications in vision, physics, and biology. One drawback of our approach is that despite enforcing sparsity in the generated data, our approach cannot exploit it in a way that makes the computation more efficient, e.g., as efficient data storing exploits sparsity for saving storage. Also, the additional number of parameters needed to encode SBs relies on the underlying denoising network architecture.


Our work enables explicit encoding of sparsity in state-of-the-art generative models, making it particularly valuable for domains where real data is inherently sparse, such as physics and biology. The concept of SBs can also be integrated into other leading generative models, including GANs \citep{Goodfellow:2020}, VAEs \citep{Kingma:2014}, and Normalizing Flows \citep{Rezende:2015}, thereby opening up new research directions and possibilities. Additionally, further exploration could focus on refining the techniques used to enforce sparsity within these models, enhancing their efficiency and applicability across various scientific fields.

\section*{Acknowledgments}
Part of this work was conducted within the DFG Research Unit FOR 5359 on Deep Learning on Sparse Chemical Process Data (BU 4042/2-1, KL 2698/6-1, and KL 2698/7-1) and the DFG TRR 375 (INST 248/377). The authors acknowledge support by the Carl-Zeiss Foundation (AI-Care, Process Engineering 4.0) and the BMBF award 01|S2407A. SF and MK acknowledge support by the DFG through the awards FE 2282/6-1, FE 2282/1-2, KL 2698/5-2, and KL 2698/11-1.

\section*{Impact Statement}
This work aims to advance the field of generative modeling by SDD, a method designed to improve the generation of sparse data. SDD has the potential to benefit multiple scientific domains, including high-energy physics and biology, by enabling more realistic simulations and data augmentations. These advancements could contribute to more accurate scientific models, better experimental design, and improved downstream applications in areas such as healthcare and fundamental research.

We do not foresee immediate ethical concerns arising from this work. However, as with any generative modeling approach, there is a potential risk of misuse, such as generating misleading or synthetic data that could be misinterpreted in scientific analyses. We encourage responsible use of SDD in research settings where data integrity is critical. Additionally, our method does not directly address issues of fairness or bias in generative models, and we recommend further research into ensuring equitable applications of sparse data generation across different domains.

Overall, this paper presents work aimed at advancing the field of machine learning by improving generative models for sparse data. We believe that the societal impact of this work will be positive, particularly in scientific and research contexts.



\bibliography{paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
%\appendix
%\onecolumn
%\section{You \emph{can} have an appendix here.}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one.  

%The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
