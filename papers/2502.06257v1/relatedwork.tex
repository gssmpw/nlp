\section{Related Works}
\label{sec:related}

\paragraph{Knowledge Graph Completion}
Knowledge Graph (KG) completion is one of the most important tasks in the KG area. Conventional methods leverage triplet information as training data but often ignore the rich contextual information embedded within the text~\cite{TransE,ConvE,RSN,CompGCN,decentRL,HittER,DET,NeoEA}. In other words, they assume that the entities carry no self-feature (including their names, e.g., \textit{Matt Damon}), and the relational connections are the only informative source. Recently, methods leveraging text and image information have proposed and achieved state-of-the-art performance on many benchmarks~\cite{xie_image-embodied_2017-IKRL,wang_multimodal_2019-TransAE,KG-Bert,KGLM,FLT-LM,DBLP:conf/ijcnn/ZhangCZ23-MANS,GEEA}. They can be divided into two groups: one focuses on the integration of more modalities~\cite{DBLP:journals/apin/LuWJHL22-MMKRL,lee_vista_2023-VISTA,MAT}; and the other concentrates on the fine-tuning of language models to better encoding text information~\cite{KG-Bert,KGLM,FLT-LM}. 

\paragraph{LLM-based Knowledge Graph Completion}
Entities possess rich features often represented in text forms such as descriptions, tables, and attributes. Many (mostly multi-modal) methods propose leveraging language models to encode text information and use the resulting representations for prediction~\cite{KG-Bert,StAR,FLT-LM,KGLM,Meaformer,DBLP:conf/sigir/ZhangCGXHLZC24,DBLP:journals/corr/abs-2405-16869,DBLP:journals/corr/abs-2410-07526}. In the recent advances, most LLM-based methods can be classified into this category. For example, LLMKGC \cite{llmkgc-ningyu} directly feeds the textual triplets to ChatGPT~\cite{chatgpt} for KG completion, although the results are not very promising. KGLlama~\cite{KGllama} and KoPA~\cite{KoPa} fine-tune LLMs on triplet verification, i.e., estimating the correctness of a given triplet. These LLM-based methods employ in-context learning or LoRA-based fine-tuning~\cite{icl1,icl2}. To our knowledge, no prior work has explored integrating KGs into the head layer of LLMs. The current LLM-based methods leverage additional text information but are directly compared against conventional methods~\cite{KGllama,KoPa,kicgpt}, which may lead to unfair evaluations. Therefore, in this paper, we consider multi-modal datasets as benchmarks~\cite{MMKG,MMRNS,DBLP:journals/corr/abs-2402-05391}.


\paragraph{Multi-Head LLMs}
There are several works employing multiple head layers in LLMs. Medusa~\cite{medusa} proposes a tree attention mechanism for multi-step training and inference. The $K$ different head layers are initialized with the original weights and then fine-tuned independently. MultiToken~\cite{multi-head} discovers that training LLMs and multiple head layers from scratch can outperform the single-head version, with this advantage being more significant in larger models. Unlike these methods, the $K$ head layers in our approach are not only used for generating multiple future tokens in one step but also confine the output space to KGs and enable entity-level contrastive learning. Our work explores a new direction for integrating LLMs with KGs.