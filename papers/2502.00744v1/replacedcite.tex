\section{Related Work}
The concept of pruning NNs dates back to the early 1990s. The seminal work by ____ on Optimal Brain Damage introduced the idea of pruning by removing weights that contribute least to performance, thus simplifying the network. ____ extended this concept with Optimal Brain Surgeon, which provided a more sophisticated method for determining which weights to prune based on their impact on the error function. These early methods laid the foundation for modern pruning techniques, focusing on reducing network complexity while maintaining accuracy.



\textbf{Regularization-Based Pruning (Soft Pruning).} 
Regularization methods play a crucial role in promoting sparsity during the training process by extending the loss function with a penalty function that discourages overly complex models.
While sparsity is encouraged, regularization does not explicitly set the weights to zero but instead reduces their magnitude, allowing them to remain non-zero and potentially become active again if needed.
This leads to what is termed soft pruning, where sparsity is encouraged but not strictly enforced through hard weight removal during training. After training concludes, unimportant weights—typically those with the smallest magnitudes—are then pruned ____.
One of the simplest and most widely used methods, $L_1$-regularization ____, penalizes the sum of the absolute values of the weights, encouraging many weights to become zero. Moreover, $L_1$-regularization fails to incorporate considerations from Axiom II, which emphasizes the preservation of neural network connectivity and functionality. This lack of consideration for connectivity can lead to a network that, while sparse, may suffer from disrupted information flow, ultimately impairing its performance.
Similarly, $L_2$-regularization, another common regularization technique, penalizes the sum of the squares of the weights (e.g., see ____). 
While $L_2$-regularization is effective at discouraging large weights, it does not push small weights towards zero, thus failing to induce sparsity in the network. As a result, $L_2$-regularization typically produces networks with small but non-zero weights, which do not benefit from the same computational efficiency gains that a sparse network would offer. Moreover, like $L_1$-regularization, $L_2$-regularization does not address the need to maintain critical connections as highlighted by Axiom II, making it less suitable for tasks where maintaining network connectivity is essential.

\textbf{Stage-Based Pruning (Hard Pruning).}
Stage-based pruning strategies are utilized as separate, discrete actions during various stages of model training. These techniques can be implemented before training ____, during training ____, or after training ____.
Stage-based pruning generally does not fundamentally alter the objective function or the descent direction like regularization does, but instead acts on the model’s structure or parameters at specific moments.
These kinds of pruning methods can be considered hard pruning approaches, as parameters are explicitly removed.
Many different criteria for pruning have been introduced, such as magnitude-based pruning ____, which involves removing weights with the lowest absolute values and is based on the idea that these weights have the least impact on the overall performance of the model.
More complex criteria have been constructed to determine the impact of weight removal, such as first-order (e.g., see ____) and second-order expansions ____ of the training objective.
Specifically, SynFlow ____ is a method that adheres closely to the principles of Axiom II, focusing on retaining the network's connectivity and functionality during pruning. 
Unlike magnitude-based techniques, SynFlow utilizes a first-order expansion of signal flow to pinpoint and remove weights with minimal impact on the network's overall information flow. 
This approach ensures that while the network is being pruned, its structural integrity is preserved and the critical pathways in terms of connectivity remain intact.
Another approach adopting a network-theoretic perspective is ____, who employ Katz centrality to prune neural network nodes in nonlinear system modeling. Although this method highlights the potential of network measures for guiding pruning decisions, our methodology is fundamentally different, and further extends to large-scale NNs.



We conclude the above discussion by noting that the CoNNect regularizer, to be introduced in the next section, can both be used as a soft pruning approach and integrated in hard pruning approaches.