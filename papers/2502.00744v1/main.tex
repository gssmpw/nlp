%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Self-inserted by authors
\usepackage{amsthm}
\usepackage{adjustbox}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{caption}
\usepackage{float}
\usepackage{amsmath,amsthm,amssymb,amsfonts,amsbsy}
\usepackage{graphicx,mathrsfs,booktabs,mdwlist,multirow,colortbl}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{makecell}
\definecolor{mygray}{gray}{.9}
\newtheorem{axiom}{Axiom}
\usepackage{wrapfig}

\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{{CoNNect}: A Swiss-Army-Knife Regularizer for Pruning of Neural Networks}

\begin{document}

\twocolumn[
\icmltitle{{CoNNect}: A Swiss-Army-Knife Regularizer for Pruning of Neural Networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Christian Franssen}{vu}
\icmlauthor{Jinyang Jiang}{pku}
\icmlauthor{Yijie Peng}{pku}
\icmlauthor{Bernd Heidergott}{vu}

\end{icmlauthorlist}

\icmlaffiliation{vu}{Department of Operations Analytics, VU Amsterdam, Amsterdam, The Netherlands}
\icmlaffiliation{pku}{Guanghua School of Management, Peking University, Beijing, China}

\icmlcorrespondingauthor{Christian Franssen}{c.p.c.franssen@vu.nl}
\icmlcorrespondingauthor{Jinyang Jiang}{jinyang.jiang@stu.pku.edu.cn}
\icmlcorrespondingauthor{Yijie Peng}{pengyijie@pku.edu.cn}
\icmlcorrespondingauthor{Bernd Heidergott}{b.f.heidergott@vu.nl}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Pruning encompasses a range of techniques aimed at increasing the sparsity of neural networks (NNs). These techniques can generally be framed as minimizing a loss function subject to an $L_0$-norm constraint. This paper introduces CoNNect, a novel differentiable regularizer for sparse NN training that ensures connectivity between input and output layers. CoNNect integrates with established pruning strategies and supports both structured and unstructured pruning. We proof that CoNNect approximates $L_0$-regularization, guaranteeing maximally connected network structures while avoiding issues like layer collapse. Numerical experiments demonstrate that CoNNect improves classical pruning strategies and enhances state-of-the-art one-shot pruners, such as DepGraph and LLM-pruner.
\end{abstract}

\section{Introduction}

Machine learning models, such as neural networks (NNs), have seen rapid growth in recent years, leading to significant increases in their footprint \citep{patterson2021carbon}.
With the increasing footprint of these models, there is a growing need to develop more energy-efficient approaches to machine learning that can balance computational performance with environmental sustainability. 

An effective technique for reducing a model's computational effort and memory burden is \textit{neural network pruning}.
Pruning refers to the process of systematically eliminating parameters that contribute little to network performance, effectively simplifying the model.
The resulting sparse NNs
have attracted significant interest in recent years due to their ability to boost computational efficiency and minimize memory consumption while preserving or even improving model performance \citep{lecun1989optimal, hassibi1993optimal,frankle2018lottery}. 


To achieve sparsity in neural networks, various techniques have been proposed, such as weight pruning, commonly referred to as unstructured pruning, which involves selectively removing individual weights from the network.
Pruning neural network weights based on absolute values is a classic example of unstructured pruning \citep{lecun1989optimal, hassibi1993optimal, hagiwara1993removal, han2015learning}. 
Unstructured pruning can lead to highly sparse networks, but often results in irregular memory access patterns, which can be difficult to optimize in hardware implementations. 
Recently, semi-structured pruning has emerged as an approach to balance granularity and efficiency by removing weights within predefined patterns or groups \citep{frantar2023sparsegptmassivelanguagemodels, sun2023simple, fang2024maskllm}. While this is a promising approach for realizing high accuracy at various sparsity ratios, this approach presents nuanced trade-offs in inference speed and therefore the computational efficiency of the model.
Finally, structured pruning (e.g., see \citep{yuan2006model, Huang_Wang_2017, anwar2017structured}) offers a systematic method to remove entire groups or channels of neurons. 
Techniques like Group Lasso \citep{yuan2006model, hoefler2021sparsity} and other structured sparsity learning \citep{wen2016learning, zhuang2020neuron} fall into this category; see \citet{he2023structured} for a review.
Structured pruning is widely used because it is a more hardware-friendly pruning technique.
Moreover, the structured removal of parameters generally leads to an almost equal reduction in computational complexity and inference speed, thus immediately improving computational efficiency. 

We believe that pruning should obey the following two axioms (where we identify a NN with a directed, weighted graph):

\begin{axiom}[Delete Weights to Improve Computational Efficiency]
The graph should be 'small': pruning must significantly reduce the number of weights while minimally impacting accuracy and maximizing computational efficiency.
\end{axiom}

\begin{axiom}[Preserve Neural Network Connectivity] \label{ax:}
The pruning process must prevent disruptions in the connectivity of the neural network and preserve the flow of information from input to output. 
\end{axiom}
 
The extensive research on pruning neural networks, as more elaborately outlined in the literature overview in Section 2 and particularly in review works such as \citet{hoefler2021sparsity, he2023structured}, predominantly aligns with the first axiom. 
However, few methods address Axiom 2, as the impact of weight removal on overall network connectivity is rarely considered. This negligence can result in a pruning that produces highly disconnected networks, or in the most extreme case so-called \textit{layer collapse}, see Figure~\ref{fig:lc}, where the NN becomes completely dysfunctional.
A notable exception is SynFlow pruning \citep{tanaka2020pruning}, which we will explore in more detail in Section~\ref{sec:weight_level}.

% \begin{wrapfigure}{r}{0.29\linewidth}
\begin{figure}
    \begin{minipage}{0.49\linewidth}
    \centering
    % \vspace{-0.3cm}
    \includegraphics[width=\linewidth]{figure/No_Regularization_exp_3_train.png}
    % \vspace{-0.4cm}
    % \caption{Layer collapse in a pruned NN.}
    % \vspace{-0.3cm}
    % \label{fig:lc}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
    \centering
    % \vspace{-0.3cm}
    \includegraphics[width=\linewidth]{figure/No_Regularization_exp_3_finetuned.png}
    % \vspace{-0.4cm}
    % \caption{Layer collapse in a pruned NN.}
    % \vspace{-0.3cm}
    % \label{fig:lc}
    \end{minipage}
    \vspace{-0.2cm}
    \caption{Magnitude-based pruning of NN (left) leads to layer collapse (right).}
    \label{fig:lc}
    \vspace{-0.5cm}
\end{figure}
% \end{wrapfigure}

In this paper, we propose a new regularizer, called CoNNect, that can be used to satisfy both axioms simultaneously and (i) is differentiable (except in the point zero) and allows for gradient descent optimization, (ii) effectively approximates $L_0$-regularization and guarantees maximally connected network structures as stable stationary points, avoiding issues such as layer collapse.
CoNNect is based on the Katz centrality measure \citep{katz1953new} and evaluates the connectivity of weighted graphs by utilizing the connectivity measurement employed by Katz centrality for networks with normalized weights.
Normalization results in weights being restricted to $ [ 0 , 1 ]$, so that the contribution of path from input to output layer to the overall connectivity of the network goes exponentially quickly to zero unless the weights along the paths are (close to) 1.
Hence, when maximizing the connectivity for the normalized weights, we find a weight association that prefers fewer "direct paths" over many "parallel paths", while focusing on connectivity of the input with the output layer.
As is clear from the above, including the CoNNect regularizer in the training of an NN leads in a natural way to a sparse network representation, and hence satisfies Axiom 1 and Axiom 2 simultaneously.

CoNNect is a versatile regularizer that can be integrated into many established pruning strategies. Moreover, it is suitable for both unstructured and structured pruning.
We demonstrate its efficacy through a series of numerical examples.
First, we provide an illustrative unstructured pruning example in which we show that pruning strategies like magnitude-pruning \citep{lecun1989optimal, hassibi1993optimal} and SynFlow \citep{tanaka2020pruning} can benefit from CoNNect regularization during training. Here, it outperforms $L_1$- and $L_2$-regularization in terms of both accuracy and stability.
Then, we conduct numerical experiments on structured pruning, given its potential to achieve substantial improvements in computational efficiency, thus satisfying Axiom~1.
We apply CoNNect regularization on a channel level when training a Graph Neural Network (GNN), achieving improved performance compared to $L_1$- and $L_2$-regularization.
Then, we integrate CoNNect into state-of-the-art methods for structural pruning of  pre-trained models, such as DepGraph \citep{fang2023depgraph}, and LLM-pruner \citep{ma2023llm}, a one-shot pruning method for Large Language Models (LLMs). Our numerical results demonstrate consistent performance improvements compared with other methods obtaining similar gains in computational efficiency.

\section{Related Work}

The concept of pruning NNs dates back to the early 1990s. The seminal work by \citet{lecun1989optimal} on Optimal Brain Damage introduced the idea of pruning by removing weights that contribute least to performance, thus simplifying the network. \citet{hassibi1993optimal} extended this concept with Optimal Brain Surgeon, which provided a more sophisticated method for determining which weights to prune based on their impact on the error function. These early methods laid the foundation for modern pruning techniques, focusing on reducing network complexity while maintaining accuracy.



\textbf{Regularization-Based Pruning (Soft Pruning).} 
Regularization methods play a crucial role in promoting sparsity during the training process by extending the loss function with a penalty function that discourages overly complex models.
While sparsity is encouraged, regularization does not explicitly set the weights to zero but instead reduces their magnitude, allowing them to remain non-zero and potentially become active again if needed.
This leads to what is termed soft pruning, where sparsity is encouraged but not strictly enforced through hard weight removal during training. After training concludes, unimportant weights—typically those with the smallest magnitudes—are then pruned \cite{hagiwara1993removal, gale2019state}.
One of the simplest and most widely used methods, $L_1$-regularization \citep{tibshirani1996regression, he2017channel, yang2019structured, de2022neural, ziyin2023spred}, penalizes the sum of the absolute values of the weights, encouraging many weights to become zero. Moreover, $L_1$-regularization fails to incorporate considerations from Axiom II, which emphasizes the preservation of neural network connectivity and functionality. This lack of consideration for connectivity can lead to a network that, while sparse, may suffer from disrupted information flow, ultimately impairing its performance.
Similarly, $L_2$-regularization, another common regularization technique, penalizes the sum of the squares of the weights (e.g., see \citet{hinton2012practical, phaisangittisagul2016analysis, loshchilov2017fixing}). 
While $L_2$-regularization is effective at discouraging large weights, it does not push small weights towards zero, thus failing to induce sparsity in the network. As a result, $L_2$-regularization typically produces networks with small but non-zero weights, which do not benefit from the same computational efficiency gains that a sparse network would offer. Moreover, like $L_1$-regularization, $L_2$-regularization does not address the need to maintain critical connections as highlighted by Axiom II, making it less suitable for tasks where maintaining network connectivity is essential.

\textbf{Stage-Based Pruning (Hard Pruning).}
Stage-based pruning strategies are utilized as separate, discrete actions during various stages of model training. These techniques can be implemented before training \citep{lee2018snip,tanaka2020pruning,wang2020picking}, during training \citep{frankle2018lottery}, or after training \citep{hagiwara1993removal, thimm1995evaluating, gale2019state, ma2023llm}.
Stage-based pruning generally does not fundamentally alter the objective function or the descent direction like regularization does, but instead acts on the model’s structure or parameters at specific moments.
These kinds of pruning methods can be considered hard pruning approaches, as parameters are explicitly removed.
Many different criteria for pruning have been introduced, such as magnitude-based pruning \citep{hagiwara1993removal, gale2019state}, which involves removing weights with the lowest absolute values and is based on the idea that these weights have the least impact on the overall performance of the model.
More complex criteria have been constructed to determine the impact of weight removal, such as first-order (e.g., see \citep{zhou1999subset, molchanov2016pruning, sanh2020movement}) and second-order expansions \citep{lecun1989optimal, hassibi1993optimal, ma2023llm} of the training objective.
Specifically, SynFlow \citep{tanaka2020pruning} is a method that adheres closely to the principles of Axiom II, focusing on retaining the network's connectivity and functionality during pruning. 
Unlike magnitude-based techniques, SynFlow utilizes a first-order expansion of signal flow to pinpoint and remove weights with minimal impact on the network's overall information flow. 
This approach ensures that while the network is being pruned, its structural integrity is preserved and the critical pathways in terms of connectivity remain intact.
Another approach adopting a network-theoretic perspective is \citet{li2020pruning}, who employ Katz centrality to prune neural network nodes in nonlinear system modeling. Although this method highlights the potential of network measures for guiding pruning decisions, our methodology is fundamentally different, and further extends to large-scale NNs.



We conclude the above discussion by noting that the CoNNect regularizer, to be introduced in the next section, can both be used as a soft pruning approach and integrated in hard pruning approaches.


\section{Methodology}

\subsection{Preliminaries}

We define a graph $ \mathcal{G} = (V, E) $, where $ V $ denotes the set of vertices (or nodes) and $ E $ represents the set of directed links that connect these vertices.
A weighted graph has weights $W_{ i , j } \geq 0 $ for links $ ( i , j ) \in E $,
where we let $ W_{ i ,j } = 0 $, for $ ( i , j ) \not \in E $.
Neural networks can be described using graph theory by representing them as directed, weighted graphs.
In this setting, the vertices $ V = V_1 \cup \ldots \cup V_K $ in the graph correspond to the neurons in the network which are organized into distinct subsets corresponding to the different layers $ V_k $,  for $k = 1, \ldots, K$. Here,  
the input nodes $ V_{1} $ represent the neurons in the input layer, the hidden nodes $ V_{k}  $, for $k = 2, \ldots, K-1$, represent the neurons in the hidden layers, and the output nodes $ V_{K} $ represent the neurons in the output layer.


For simplicity of the ensuing analysis, we assume a simple feedforward neural network without skip connections, so that each pair of subsequent layers $V_k$ and $V_{k+1}$ is connected via edges in the set $E_k$, for $k = 1, \ldots, K-1$. While this formulation excludes possible skip connections, we discuss later that architectures with skip connections, i.e., residual neural networks, can also be regularized using CoNNect.

Throughout the paper, we describe a neural network $\mathcal{G}$ using the tuple $(W, b)$, where $W \in \mathbb{R}^{|V| \times |V|}$ is the weighted adjacency matrix of the weights, such that $W_{i,j}$ connects node $i \in V_k$ with node $j \in V_{k+1}$, and $b = (b_1, \ldots, b_{|V|})$ is the bias vector.
Moreover, we denote the activation of the $k+1$th layer by the tensor
$ X^{(k+1)} = \sigma ( W^{(k)} X^{(k)} + b^{(k+1)} ), $ where $\sigma$ is the activation function, $W^{(k)}$ is the submatrix containing the weights between nodes in $V_k$, and $V_{k+1}$, and $b^{(k+1)}$ the biases for the nodes in $V_{k+1}$.
Finally, we denote $ f(X^{(1)}; W, b)$ as a forward pass through the network. 

\subsection{Problem Formulation}

Let $ \{(x_i, y_i)\}_{i=1}^{N}$ denote the training set, where $x_i = X_i^{(1)}$ represents the input data and $y_i$ represents the corresponding label for each of the $N$ samples. 
Fitting the parameters of a neural network $\mathcal{G}$ involves optimizing the network's weights to minimize a loss function $\mathcal{L}(\hat{y}, y)$, where $\hat{y} = f(x; W, b)$ is the predicted output given an input $x$. 

In this paper, our objective is to train a sparse neural network, which can be achieved by inducing sparsity in the network's parameters. 
A commonly employed approach to sparsification is regularization. 
Regularization involves augmenting the loss function with an additional term that penalizes non-zero elements in the network parameters. 
Specifically, the optimization problem can be formulated as:
\begin{align} \label{eq:pf}
    \min_{W,b} \quad \mathcal{L}(\hat{y}, y) + \lambda R(W),
\end{align}
where $R(W) = \| W \|_{0,1}$. 
However, this $L_0$-norm is non-convex and leads to a combinatorial optimization problem, which is generally NP-hard and computationally intractable for large-scale problems.
A more practical alternative is $L_1$-regularization, as in Lasso regression, where $R(W) = \| W \|_{1,1}$.
$L_1$-regularization induces sparsity by shrinking weights to zero, approximating the $L_0$-norm while remaining convex and suitable for gradient-based optimization. 
However, $L_1$-regularization primarily satisfies Axiom 1 by reducing connections but fails to address Axiom 2, which focuses on preserving network connectivity and ensuring efficient signal flow. 
This limitation can result in a disconnected or underperforming network when key pathways are not maintained.


\subsection{CoNNect}

To overcome the aforementioned issues, we propose CoNNect, a regularizer that considers both individual weights and the network's overall connectivity, ensuring that the structure contributes to optimal performance.
We first introduce CoNNect for unstructured regularization.
Then, we demonstrate how CoNNect can be seamlessly extended to structured regularization.

\subsubsection{Weight-Level Regularization} \label{sec:weight_level}

Katz centrality is a measure used in network analysis to determine the relative connectivity of a node in a network by considering both the number and the quality of connections \citep{katz1953new}. 
Inspired by the connectivity measurement in Katz centrality, let us consider the following connectivity matrix for a network:
\begin{align*} %\label{eq:con_matrix}
    \textstyle{\varphi(W) = \sum_{k=1}^K (\theta(W))^k,}
\end{align*}
where $(\varphi(W))_{i,j}$ indicates the connectivity from node $i$ to node $j$, and $\theta(W)$ is a simple normalization of the network weights between two subsequent layers, e.g., for $i \in V_k$ and $j \in V_{k+1}$,
\begin{align} \label{eq:normalization}
(\theta(W))_{i,j} = \frac{|W_{i,j}|}{ \sum_{k\in V_k} \sum_{l\in V_{k+1}}|W_{k,l}|}.
\end{align}

In the context of a neural network, we can denote the connectivity by taking the sum of connectivity values between the input and output layer:
\begin{align*}
   \textstyle{ \varphi^{tot}(W) = 
    \sum_{i\in V_1} \sum_{j \in V_K} \left(\varphi(W)\right)_{i,j}. }
\end{align*}
Finally, we argue for the preservation of connectivity (as per Axiom 2), so we aim to maximize the network's overall connectivity. Consequently, we choose the regularizer as:
\begin{align} \label{eq:reg}
    R(W) = -\varphi^{tot}(W),
\end{align}
which we will refer to as the CoNNect regularizer.
A possible extension of CoNNect would be to include the biases and activation functions, but we leave this for future work.

CoNNect is effectively the (negative of the) sum of all (multiplicative) reparameterized weighted paths between nodes in the input layer $V_1$ and the output layer $V_K$. 
It follows that $-\varphi^{tot}(W) = 0 $ if and only if there is no path with positive weight between the input and output layer.
Moreover, $-\varphi^{tot}(W)$ can be efficiently computed using a single forward pass $f(\Bar{1}, W, \Bar{0})$, where $\Bar{1}$ is a vector of ones as input, $\Bar{0}$ is a vector of zeroes for the biases, and finally taking the sum of the output values. 

In the following, we show that $-\varphi^{tot}(W)$ can be used as a surrogate regularizer for the $L_0$-norm to induce sparsity.
Taking $ R ( W ) = || W||_{0,1} $ in Equation (\ref{eq:pf}), it is easy to show that any neural network $W$ 
that minimizes $  || W||_{0,1}$ while connecting the input layer to the output layer (without skip connections), i.e., $\varphi^{tot}(W) > 0$, has $K-1$ non-zero weights.
As the following theorem shows, 
a similar result holds for the CoNNect regularizer as 
any $ W $ minimizing $- \varphi^{tot}(W)$
has between layer $2$ and $K-1$ only $ K-3$ non-zero weights.
\begin{theorem} \label{thm:con_edge_bnd}
    Consider the problem 
    \begin{align} \label{eq:min_reg}
        \min_{W} \ -\varphi^{tot}(W),
    \end{align}
    for a given network with number of layers $K > 2$. All solutions $W^*$ to Equation (\ref{eq:min_reg}) have at most $|V_1| + |V_K| + K-3 $ non-zero weights.
    \begin{proof}
        See Appendix \ref{sec:th1}.
    \end{proof}
\end{theorem}

Theorem~\ref{thm:con_edge_bnd} demonstrates that $L_0$-norm regularization can be effectively achieved through the CoNNect regularizer, as the induced sparsity in large neural networks is comparable. Importantly, the difference in the number of non-zero elements becomes negligible in practice when most input nodes contribute valuable predictive information, and all output nodes are used for accurate classification. 
Also, our regularizer does not force the input nodes to disconnect due to its indifference to the number of input nodes that connect to the second layer, which is a beneficial feature.
If certain input nodes were disconnected, as might happen with other regularizers such as $L_1$-regularization, important data features could be disregarded, potentially resulting in suboptimal model performance.

We now show that CoNNect is a well-behaved regularizer in the sense that it does not have stable stationary points other than its global optima.
This ensures that if a gradient descent gets stuck in a stationary point of the regularizer, the loss function will always push the solution to leave the stationary point unless the loss function itself is stationary at that point.
In the following, we exclusively consider connected $W$, that is, $\varphi^{tot}(W) > 0$.
We do so because we will prove later that it is impossible to reach an unconnected network ($\varphi^{tot}(W) = 0$) when starting in a connected network simply by using a log-transformation of $\varphi^{tot}(W)$.

First, consider for some $(i,j) \in E_k$ let 
\begin{align*}
    \partial_{W_{i,j}} (\theta(W))_{i,j} = \frac{\sum_{(r,c) \in E_k} |W_{r,c}| - |W_{i,j}|}{(\sum_{(r,c) \in E_k} |W_{r,c}|)^2},
\end{align*}
and, specifically for $(q,t) \neq (i,j) \in E_k$;
\begin{align*}
    \partial_{W_{q,t}} (\theta(W))_{i,j} = \frac{- |W_{i,j}|}{(\sum_{(r,c) \in E_k} |W_{r,c}|)^2}.
\end{align*}


Observe that differentiating $\theta(W)$ with respect to $W_{i,j}$ only affects the weights in the same layer as $W_{i,j}$.
Thus, a stationary point to Equation (\ref{eq:min_reg}) solves the following first-order conditions:
\begin{align}
    & \sum_{(r,c) \in E_1} \partial_{W_{i,j}} (\theta(W))_{r,c} \cdot a_{c \cdot} = 0, \quad \forall \ (i,j) \in E_1, \label{eq:stat_con} \\
    & \sum_{(r,c) \in E_2} a_{\cdot r}  \cdot \partial_{W_{i,j}} (\theta(W))_{r,c} \cdot a_{c \cdot} = 0, \quad \forall \ (i,j) \in E_2, \notag \\[-5pt]
    & \hspace{4.1cm} \vdots \notag \\
    & \sum_{(r,c) \in E_{K-1}} a_{\cdot r}  \cdot \partial_{W_{i,j}} (\theta(W))_{r,c} = 0, \quad \forall \ (i,j) \in E_{K-1}, \notag
\end{align}
where 
\begin{align*}
\begin{array}{rl}
    a_{\cdot r} &= \sum_{i \in V_1} \sum_{\gamma \in \Gamma_{i,r}}\prod_{k = 1}^{|\gamma|-1} (\theta(W))_{\gamma_k}, \\
    a_{c \cdot} &= \sum_{m \in V_K} \sum_{\gamma \in \Gamma_{c,m}}\prod_{k=1}^{|\gamma|-1}(\theta(W))_{\gamma_k}
\end{array}
\end{align*}
are the connectivity from input layer to a node $r$ and connectivity from a node $c$ to the output layer, respectively.
To satisfy Equation (\ref{eq:stat_con}), we need: 
\begin{itemize}
    \vspace{-5pt}
    \item the weights for the edges in $E_1$ must be assigned to all $(\theta(W))_{i,j}$, where $j \in \argmax_p a_{p \cdot}$;
    \item the weights for the edges in $E_{k}$, $k=2,\ldots, K-2$ must be assigned to $(\theta(W))_{i,j}$, where $(i,j) \in \argmax_{(p,q)} a_{\cdot p}  a_{q \cdot}$;
    \item the weights for the edges in $E_{K-1}$ must be assigned to $(\theta(W))_{i,j}$, where $i \in \argmax_{q} a_{\cdot q}$.
\end{itemize}
\vspace{-5pt}
The set of weight matrices $ W $ that satisfy Equation~\eqref{eq:stat_con}
can be more precisely formulated as in Lemma~\ref{lem:equal_parallel_weights}.

\begin{lemma} \label{lem:equal_parallel_weights}
Assume a neural network with $K > 3$ layers. 
All stationary points $W^*$ to Equation (\ref{eq:min_reg}) that are connected, i.e., $\varphi^{tot}(W) > 0$, have paths with equal subsequent weights between layers $2$ and $K-1$ on its non-zero paths. 
That is, for each two paths $\gamma', \gamma'' \in \bigcup_{i \in V_1, m\in V_K} \Gamma_{i,m}$, such that $$\prod_{k=1}^{K-1} (\theta(W^*))_{\gamma_k}  > 0, \quad \gamma \in \{\gamma', \gamma''\},$$ i.e., both paths have positive weight, we have $(\theta(W^*))_{\gamma'_k} = (\theta(W^*))_{\gamma''_k}$, for all $k=2, \dots, K-2$.
\begin{proof}
    See Appendix~\ref{sec:prf_stat_points}.
\end{proof}
\end{lemma}

Using Lemma~\ref{lem:equal_parallel_weights}, we note that all non-optimal stationary points, i.e., $\varphi^{tot}(W) < 1$, have multiple directions of improvement by simply transferring mass from one path to another. 
It follows that these solutions are inherently unstable and thus are not a local optimum.
We present a precise statement in Theorem~\ref{thm:glob_stat_points}, where we the proof is omitted as it follows directly form the previous observation.

\begin{theorem} \label{thm:glob_stat_points}
Assume a neural network with $K > 3$ layers. 
All stable stationary points $W^*$ to Equation (\ref{eq:min_reg}) that are connected, i.e., $\varphi^{tot}(W) > 0$, are global minimizers.

\end{theorem}


the weight matrix has only $ K-3$ non-zero weights between layer 2 and $K-1$.
As Theorem~\ref{thm:glob_stat_points} shows, CoNNect does not have regions of attraction, and thus is a well-behaved regularizer for gradient search.

As argued earlier, it is recommended to take the logarithm over the connectivity regularizer, i.e., 
\begin{align} \label{eq:logconnect}
     -\log \left( \varphi^{tot}(W)) \right),
\end{align}
as it ensures that if the neural network tends to disconnect during training, i.e., $\varphi^{tot}(W) \xrightarrow[]{} 0$, 
Equation (\ref{eq:logconnect})
approaches $\infty$, hence preventing layer collapse.
Moreover, it enhances numerical stability, ensuring that the regularization term remains well-behaved even for varying scales of connectivity.

Once we have trained a model with CoNNect regularization, many of the redundant weights will have been pushed to zero.
Consequently, we can hard prune the regularized model using pre-established pruning strategies.
A well-known strategy is simple magnitude-based pruning \citep{lecun1989optimal}, which prunes the smallest weights in absolute value.
Alternatively, we can use SynFlow pruning \citep{tanaka2020pruning}, which prunes the neural network's weights according to synaptic saliency scores:
\begin{align*}
    I_{i, j} &= \big( \partial_{(\theta(W))_{i,j}} \varphi^{tot}(W)  \big) \cdot (\theta(W))_{i,j} \\
    &=  a_{\cdot i} \cdot (\theta(W))_{i,j} \cdot  a_{ j \cdot},
\end{align*}
and eliminate the weights with the smallest $I_{i, j}$ values.


\subsubsection{Channel-Level Regularization} \label{sec:clp}


The regularizer introduced in Section~\ref{sec:weight_level} was explicitly defined on the weights of the neural network, making it an unstructured pruning approach.
In this section, we show how it can be easily extended to structured pruning.
To this end, we can introduce a scaling factor for the output of structures (e.g., neurons, channels, etc.) that we want to prune \citep{Huang_Wang_2017}.
In the following, we explain how to include structured pruning on the channel-level in Convolutional Neural Networks (CNNs), but this can be naturally extended to any parallel structures in neural networks, such as nodes, but also entire block structures.

CNNs are a specialized type of neural network designed to process grid-like data such as images.
These images can be represented using a tensor $X \in \mathbb{R}^{d \times h \times w}$, where $d$ refers to the number of channels (e.g., RGB for color images) and $h$ and $w$ refer to the height and width of the image respectively.
A standard CNN consists of (several) convolutional layers followed by an activation function (e.g., ReLU), and pooling layers that reduce spatial dimensions while preserving important features. 
Convolutional layers transform the tensor into a set of feature maps through a series of learned filters (also known as kernels). 
Each convolutional layer in the CNN applies these filters to local regions of the input, capturing spatial hierarchies and patterns like edges, textures, and more complex shapes as the network deepens.

% \begin{wrapfigure}{r}{0.35\linewidth}
\begin{figure}
    \centering
    % \vspace{-0.3cm}
    \includegraphics[width=0.75\linewidth]{figure/cnn.pdf}
    % \vspace{-0.6cm}
    \caption{Illustration of CNN with the scaling factor.}
    % \vspace{-0.25cm}
    \label{fig:cnn}
% \end{wrapfigure}
\end{figure}

For performing regularizing on the channel-level, we introduce a set of learnable parameters that scale the output of each channel after a convolutional layer. 
More formally, for every $X^{(k)} \in \mathbb{R}^{d \times h \times w}$, which is the activation after the $k$-th convolutional layer, we scale the channels with $\delta^{(k)} \in \mathbb{R}^{d}$ so that $
    X^{(k)\prime} = \delta^{(k)} \odot X^{(k)},$ 
where $\odot$ denotes element-wise multiplication so that the scaling factor $\delta^{(k)}$ is broadcast across the height $h$ and width $w$.
The inclusion of scaling factors $\delta^{(k)}$ is a simple linear transformation and so can be perceived as the introduction of an additional layer to the neural network $W$, see Figure \ref{fig:cnn}, resulting in an extended neural network denoted by $W'$.
As the normalization in Equation (\ref{eq:normalization}) will also be applied on the scaling factors, the unstructured CoNNect regularizer in Equation (\ref{eq:reg}) carries over to a structured regularization, where the scaling factors of less informative channels are pushed to $0$ and more informative channels are retained.


Once a regularized neural network is obtained, we can do pruning in a similar fashion as in Section~\ref{sec:weight_level}. 
Specifically, we can prune its channels via calculating an importance scores for each channel.
To that end, we aim to determine the contribution of a channel $c$ in layer $k$ in terms of the connectivity of the neural network, denoted by $I_{k, c}$.
More formally, let $\theta_{c}^{(k)}(\delta) = {|\delta^{(k)}_c|}\big/{\| \delta^{(k)} \|_1}$ 
denote the normalization of the scaling factors with index $c$ for convolutional layer $k-1$ so that $I_{k, c}$ can be determined via
\begin{align*}
    I_{k, c} &= \left( \partial_{\theta_{c}^{(k)}(\delta)} \varphi^{tot}(W) \right) \cdot \theta_{c}^{(k)}(\delta) \\
    &= \textstyle{ \bigg(\sum_{r \in V_{k-1}^{(c)}}  a_{\cdot r} \bigg) \cdot \theta_{c}^{(k)}(\delta) \cdot \bigg( \sum_{r \in V_{k+1}^{(c)}} a_{ r \cdot}\bigg),}
\end{align*}
where $V_{k}^{(c)}$ is the subset of nodes in a layer $k$ corresponding to channel index $c$.
Simply put, $I_{k, c}$ denotes the total connectivity that flows through channel $c$ in layer $k$.
Consequently, a simple pruning strategy is to prune the channels with lowest values of $I_{k, c}$.

\begin{figure*}[!h]
    \centering
    \captionsetup[subfigure]{labelformat=empty}
    % Subfigure a
    \subfigure{
        \includegraphics[width=0.205\linewidth]{figure/No_Regularization_exp_3_train.png}
    }
    % Subfigure b
    \subfigure{
        \includegraphics[width=0.205\linewidth]{figure/L1-Regularization_exp_3_train.png}
    }
    % Subfigure c
    \subfigure{
        \includegraphics[width=0.205\linewidth]{figure/Connectivity-Regularization_exp_3_train.png}
    }\\
    \captionsetup[subfigure]{labelformat=default}
    \setcounter{subfigure}{0}
    % Subfigure d
    \subfigure[No regularization.]{
        \includegraphics[width=0.205\linewidth]{figure/No_Regularization_exp_3_finetuned.png}
        \label{fig:pruning_grid_no}
    }
    % Subfigure e
    \subfigure[$L_1$-regularization.]{
        \includegraphics[width=0.205\linewidth]{figure/L1-Regularization_exp_3_finetuned.png}
        \label{fig:pruning_grid_l1}
    }
    % Subfigure f
    \subfigure[CoNNect regularization.]{
        \includegraphics[width=0.205\linewidth]{figure/Connectivity-Regularization_exp_3_finetuned.png}
        \label{fig:pruning_grid_con}
    }
    \caption{Trained (top) and fine-tuned (bottom) models. Thicker and darker colors correspond to stronger values. Red and blue edges correspond to positive and negative values respectively.}
    \label{fig:pruning_grid}
\end{figure*}



\subsubsection{Implementation Details \& Efficiency}

We provide our detailed implementation of CoNNect in Appendix~\ref{app:imp}, spanning various modern neural network architectures. It is worth emphasizing that CoNNect is a highly flexible regularizer. While it is currently applied as detailed in Appendix~\ref{app:imp}, its design allows for substantial flexibility in how it is applied. For example, skip connections could be explicitly included in $W$, or CoNNect could be applied selectively to specific parts of a neural network. 

CoNNect can be efficiently computed using a single forward pass (and its gradient with a single backward pass). Thus, for a batch size of $M$, the additional time used for computing $\varphi^{tot}(W)$ is proportional to $\frac{1}{M}$.
Hence, CoNNect can be efficiently applied to large-scale neural networks without incurring significant computational overhead.

\section{Numerical Experiments} 



We now provide several numerical experiments. In Section~\ref{sec:reg_train}, we show results for CoNNect regularization during training. In Section~\ref{sec:reg_prun}, we show how CoNNect can be further scaled for pruning pre-trained models through an integration in hard pruning strategies DepGraph \cite{fang2023depgraph} and LLM-pruner \cite{ma2023llm}.

\subsection{Regularized Training with CoNNect} \label{sec:reg_train}

\subsubsection{Unstructured Pruning of MLPs} \label{sec:small_nn_unstruc} \label{sec:te}

In the following, we want to study the effects of integrating CoNNect regularization in an unstructured pruning task.
Let us consider a small multilayer perceptron neural network with ReLU activations.
The network has 6 input nodes, three hidden layers of 5 nodes, and a single output node.
We sample input values $x_i = (x_{i,1}, \ldots, x_{i,6}) \sim \mathcal{N}(0, \Sigma)$, where $\Sigma$ is a matrix with the value 2 on the diagonal.
Furthermore, we let the output values be $y_i = 1$ if $x_{i,1} + x_{i,2} + \xi_i > 0$, and $y_i = 0$ otherwise, 
where $\xi_i \sim \mathcal{N}(0,0.25)$.
To find a sparse network representation, we train the network with $L_1$ and CoNNect regularization. To that end, we solve
\begin{equation}
    \begin{aligned}
    \min_{W,b} \ \mathcal{L}(\hat{y}, y) &+ \lambda_1 \| W \|_{1,1} \\
    &- \lambda_2\log\left( \varphi^{tot}(W) \right) + \lambda_3 \| W \|_{2,1}, \label{eq:all_reg}
\end{aligned}
\end{equation}
We fit three different models for 200 epochs following Equation (\ref{eq:all_reg}), for which we provide coefficients in Table~\ref{tab:small_exp}, see Appendix~\ref{app:te}.
We show the resulting NNs on the top row in Figure~\ref{fig:pruning_grid} for a single neural network initialization. In the bottom row we present the fine-tuned NNs after SynFlow pruning. As can be seen, the CoNNect regularizer is capable of identifying the relevant paths, where the other methods fail.
We provide more details of this experiment in Appendix~\ref{app:te}, including more extensive results.




\subsubsection{Channel-Level Pruning on GNNs}\label{sec:gnn}

In this section, we demonstrate CoNNect for structured pruning on the channel-level.
Specifically, we prune a
Graph Convolutional Network \citep[GCN, ][]{kipf2016semi} containing 7 layers with learnable parameters, where the hidden feature dimensions are $512$-$256$-$256$-$256$-$256$-$64$. Each GCN layer is followed by a ReLU activation function.
We train the model on the Cora \citep{sen2008collective} dataset, a graph-based dataset consisting of 2,708 academic papers (nodes) and 5,429 citation links (edges), with each paper categorized into one of seven topics and represented by a 1,433-dimensional binary feature vector. 

\begin{figure}[htbp]
    \centering
    \subfigure[Without fine-tuning.]{
        \includegraphics[width=0.46\linewidth, trim=0.cm 0 0.cm 0]{figure/wo_fine_.pdf}
        \label{fig:gnn_a}
    }
    \subfigure[With fine-tuning.]{
        \includegraphics[width=0.46\linewidth, trim=0.cm 0 0.cm 0]{figure/w_fine_.pdf}
        \label{fig:gnn_b}
    }
    \vspace{-0.2cm}
    \caption{Accuracies of GNNs for given pruning ratios.    }
    \label{fig:gnn}
\end{figure}



We train GCNs following Equation~\eqref{eq:all_reg} for 100 epochs using the parameters shown in Table~\ref{tab:reg_coeff_gnn} and fine-tune each model after pruning for 20 epochs. We conduct 10 repeated experiments, and as shown in Figure \ref{fig:gnn}, our method consistently outperforms $L_1$-regularization across different pruning ratios. The shaded regions represent $98\%$ confidence intervals. 





\subsection{Pre-Trained Model Pruning  with CoNNect} \label{sec:reg_prun}


\begin{table}[b]
\centering
\vskip -0.1in
\caption{Pruning results on ResNet-56 and VGG-19.}
\vskip -0.1in
\label{tab:cv_models_results}
\begin{center}
\begin{small}
\begin{sc}
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{lccccc}
\toprule
\makecell{Model \& \\Dataset}  & \makecell{Base \\ Acc.} & Method & \makecell{Pruned \\ Acc.} & \makecell{Speed \\ Up} & \makecell{Pruning \\Ratios} \\ \midrule
\multirow{4}{*}{\makecell{ ResNet-56 \&  \\CIFAR-10} 
} & \multirow{4}{*}{93.53} & DepGraph & 93.17 & $2.51\times$ & 56.22 \\ %\cline{3-6} 
 &  & \cellcolor{mygray} CoNNect & \cellcolor{mygray}\textbf{93.63} & \cellcolor{mygray}$2.50\times$ & \cellcolor{mygray}53.20 \\ %\cline{3-6} 
 &  & DepGraph & 80.24 & $16.17\times$ & 98.27 \\ %\cline{3-6} 
 &  & \cellcolor{mygray} CoNNect & \cellcolor{mygray}\textbf{83.12} & \cellcolor{mygray}$17.24\times$ & \cellcolor{mygray}97.46 \\ \midrule
\multirow{4}{*}{ \makecell{VGG-19 \& \\CIFAR-100}} & \multirow{4}{*}{73.50} & DepGraph & 65.89 & $8.12\times$ & 90.48 \\ %\cline{3-6} 
 &  & \cellcolor{mygray}CoNNect & \cellcolor{mygray}\textbf{69.38} & \cellcolor{mygray}$8.00\times$ & \cellcolor{mygray}93.33 \\ %\cline{3-6} 
 &  & DepGraph & 57.48 & $16.10\times$ & 96.14 \\ %\cline{3-6} 
 &  & \cellcolor{mygray}CoNNect & \cellcolor{mygray}\textbf{62.56} & \cellcolor{mygray}$16.07\times$ & \cellcolor{mygray}97.51 \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}


\begin{table*}[t]
  \centering
  %\setlength{\tabcolsep}{4pt}
  \caption{Zero-shot performance of the compressed LLaMA-7B. High scores are better, except for WikiText2 and PTB (indicated by the downward arrow). The bold values indicate the best results. The average is calculated among seven classification accuracies. An asterisk denotes that performance normalization is not available. The evaluation is conducted following the prompting of LLM-pruner \citep{ma2023llm}.}
  \vskip -0.1in
  \label{tab: llm}
  \begin{center}
    \begin{small}
    \begin{sc}
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{ll|cc|ccccccc|c}
    \toprule
    Pruned Model & Method & WikiText2$\downarrow$ &  PTB$\downarrow$ &BoolQ$^*$ &PIQA &HellaSwag &WinoGrande$^*$ &ARC-e &ARC-c &OBQA & Average\\
    \midrule 
    Ratio $=0\%$ & LlaMA-7B & 12.62 & 22.15   & 73.15 & 77.48 & 73.01 & 67.09 & 52.57 & 41.47 & 42.40    & 61.02 \\
    \midrule
    \multirow{2}{*}{\makecell[l]{\\ Ratio $=40\%$\\ w/o tune}} 
     & $L_2$           & 13783.81 & 27844.06   & 42.69 & 52.01 & 28.29 & 51.46 & 27.36 & 25.85 & 29.80    & 36.78 \\
     &  Random      & 100.42 & 133.56   & 40.00 & 57.29 & 36.00 & 50.12 & 32.83 & 25.77 & 31.00    & 39.00 \\ 
     &  LLM-Pruner  & 48.09 & 105.24   & 58.90 & 64.74 & 47.58 & \textbf{53.20} & 37.75 & 29.44 & 35.00    & 46.66 \\
     &\cellcolor{mygray}CoNNect     &\cellcolor{mygray}\textbf{46.43} &\cellcolor{mygray}\textbf{95.08}   &\cellcolor{mygray}\textbf{60.95} &\cellcolor{mygray}\textbf{67.30} &\cellcolor{mygray}\textbf{50.04} &\cellcolor{mygray}52.09 &\cellcolor{mygray}\textbf{38.30} &\cellcolor{mygray}\textbf{29.86} &\cellcolor{mygray}\textbf{36.80}    &\cellcolor{mygray}\textbf{47.91} \\
    \midrule
    \multirow{2}{*}{\makecell[l]{\\ Ratio $=40\%$\\ w/ tune}} 
     & $L_2$           & 44.91 & 67.16   & 47.34 & \textbf{71.60} & 50.60 & 54.38 & 43.35 & 32.25 & 36.80    & 48.05 \\
     &  Random      & 37.82 & 58.12   & 54.95 & 67.36 & 48.61 & 55.25 & 43.69 & 30.29 & 33.20    & 47.62 \\
     &  LLM-Pruner  & 27.62 & 48.28   & 59.97 & 71.38 & 56.21 & \textbf{59.35} & 44.53 & 32.42 & 36.20    & 51.44 \\
     &\cellcolor{mygray}CoNNect     &\cellcolor{mygray}\textbf{27.13} &\cellcolor{mygray}\textbf{47.44}   &\cellcolor{mygray}\textbf{61.59} &\cellcolor{mygray}71.06 &\cellcolor{mygray}\textbf{57.78} &\cellcolor{mygray}58.48 &\cellcolor{mygray}\textbf{45.58} &\cellcolor{mygray}\textbf{32.85} &\cellcolor{mygray}\textbf{39.00}    &\cellcolor{mygray}\textbf{52.33} \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


To further demonstrate the versatility and scalability of CoNNect, we integrate it into DepGraph \citep{fang2023depgraph} and LLM-Pruner \citep{ma2023llm}.
These frameworks ensure all parameters are divided into several groups according to the dependency relationships in the computation process. 
Then, the importance score under the objective function $\mathcal{J}(\cdot)$ is calculated by
$I_{i,j}  = \vert \mathcal{J}_{W_{i,j}}(W) - \mathcal{J}_{W_{i,j}=0}(W)\vert \approx \vert \partial_{W_{i,j}}\mathcal{J}(W)\cdot W_{i,j}\vert$.
We integrate our CoNNect approach through the objective, i.e., $\mathcal{J}(W) = \mathcal{L}(\mathcal{D}) - \lambda \log(\varphi^{tot}(W))$, where $\mathcal{D}$ denotes the dataset.
We redefine $(\theta(W))_{i,j} = \vert W_{i,j}\vert$ to enhance both numerical stability and computational efficiency. 
The importance of each group is aggregated through summation, and the least important groups are pruned. 




\subsubsection{One-shot Pruning CNNs} \label{sec:add_exp_sp_cnn}


Using our integration in DepGraph, we perform structural pruning on ResNet-56 \citep{he2016deep} and VGG-19 \citep{simonyan2015very}, which are pretrained and fine-tuned on CIFAR-10 and CIFAR-100 datasets \citep{krizhevsky2009learning}, respectively (see Appendix~\ref{app:ospcnn}).
DepGraph framework iteratively prunes the model until the predefined speed-up targets are achieved, which is calculated as the ratio of multiply-accumulate operations before and after pruning. We first follow the pruning intensities tested in \citet{fang2023depgraph}, and then verify CoNNect further with extreme cases. Thus, the pruning is set to target speed-ups of 2.5$\times$ and 16$\times$ for ResNet-56 on CIFAR-10 and 8$\times$, and 16$\times$ for VGG-19 on CIFAR-100. 
As shown in Table \ref{tab:cv_models_results}, CoNNect exhibits advantages across various pruning ratios, with benefits being more pronounced in more extreme cases.  





\subsubsection{One-shot Pruning LLMs} \label{sec:ex_llm}

Now, we perform a one-shot pruning on LLaMA-7B \citep{touvron2023open} using our CoNNect integration in LLM-pruner. 
After pruning, the LLM is fine-tuned with LoRA \citep{hu2021lora} to restore as much structural capability as possible under the current architecture.

To assess the model performance, we conduct a zero-shot perplexity analysis, see Table~\ref{tab: llm} (see Appendix~\ref{sec:exp3} for more detailed information of the datasets used).
We compare CoNNect to $L_2$, random, and vanilla LLM-Pruner's importance metrics with a $40\%$ parameter reduction. All methods
are equipped with the same group division and aggregation strategy. 
As presented in Table \ref{tab: llm}, compared to vanilla LLM-Pruner, we have reduced the performance gap between the pruned model and the original model by $9.13\%$ without fine-tuning, which is $9.29\%$ when fine-tuning is applied.
To ensure fairness, both models are fine-tuned on the Alpaca dataset for only 2 epochs.
Essentially, CoNNect enhances the LLM-Pruner's framework with an extra consideration of connectivity, providing good results. The results differ significantly from those obtained by randomly removing parameter groups, but the grouping approach keeps random pruning from detrimental outcomes. However, $L_2$ regularization even results in incorrect pruning choices, which is consistent with the conclusions drawn in the previous two subsections.
Please refer to Appendix \ref{sec:exp3} for detailed experimental settings and Appendix~\ref{app:abb_infs} for additional evaluation aspects, where we demonstrate superior performance under similar computational efficiency gain when comparing to other pruning paradigms such as semi-structured pruning \citep{sun2023simple}.

\section{Conclusions}

In this work, we introduce a novel regularizer, called CoNNect, that leverages network connectivity to promote NN sparsity. Theoretically, we proof that CoNNect is a well-behaved regularizer and aligns with the minimization of the $L_0$-norm.
Through numerical experiments, we have shown that CoNNect can be effectively applied in many pruning strategies. Moreover, it can be used for both unstructured and structured network pruning. Specifically, we showed how CoNNect regularization improves the pruning of MLPs and GNNs, compared to standard $L_1$-regularization. Finally, we demonstrated how CoNNect can be applied competitively in a one-shot pruning framework, such as DepGraph \cite{fang2023depgraph} for pruning CNNs, and LLM-pruner \citep{ma2023llm} showing improved results.


\bibliography{main.bib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Proofs}

\subsection{Proof Theorem~\ref{thm:con_edge_bnd}}\label{sec:th1}
        Let $\Gamma_{i,m}$ denote the set of paths in the neural network that go from some input node $i \in V_1$ to the output node $m \in V_K$, where
        \begin{align*}
           \gamma = ((i,j), (j,k), \ldots, (l,m)) \in \Gamma_{i,m}
        \end{align*}
        is a sequence of edges from the input layer to the output layer.
        Using that $\varphi^{tot}(W)$ is the sum of weights of paths from the input to the output layer \citep{neyshabur2015path}, we rewrite
        \begin{align*}
        \begin{split}
            \varphi^{tot}(W) &= \sum_{i \in V_1} \sum_{m \in V_K} \sum_{\gamma \in \Gamma_{i,m}}\prod_{k=1}^{K-1}(\theta(W))_{\gamma_k}  = \sum_{i \in V_1} \sum_{m \in V_K} \sum_{\gamma \in \Gamma_{i,m}}\prod_{k=1}^{K-1} \frac{|W_{\gamma_k}|}{\sum_{(r,c) \in E_k} |W_{r,c}|},
        \end{split}
        \end{align*}
        where $\gamma_k$ refers to the $k$th edge in a sequence $\gamma$. 
        Then, to minimize $R(W)$, i.e., maximize $\varphi^{tot}(W)$, we need to allocate all the mass to a single path from the input to the output, which means selecting a specific sequence of weights that maximizes the product along that path, effectively minimizing the contributions from all other paths.

        To show the upper bound of $|V_1| + |V_K| + K-3 $ non-zero weights in $W^*$, assume w.l.o.g. some $W^*$ where a single path $\Gamma_{i,m}$ has all mass in the network.
        It follows that $\varphi^{tot}(W^*) = 1$.
        Now, let $W'$ denote a solution where some mass from the first weight $W_{i,j}$, for $(i,j) \in \Gamma_{i,m}$ is shifted to any other weight(s) $W_{l,j}$ (note that $j$ is fixed), where $l \in V_1$ connects to $j \in V_2$.
        It is easily seen that $\varphi^{tot}(W') = 1$ since
        \begin{align*}
        \begin{split}
            \varphi^{tot}(W') &= \sum_{l \in V_1} (\theta(W'))_{l,j} \sum_{\gamma \in \Gamma_{j,m}}\prod_{k=1}^{K-1}(\theta(W'))_{\gamma_k} \\
            &= \sum_{l \in V_1} \frac{|W'_{l,j}|}{\sum_{(r,c) \in E_1} |W'_{r,c}|} \sum_{\gamma \in \Gamma_{j,m}}\prod_{k=1}^{K-1}(\theta(W'))_{\gamma_k} = \sum_{l \in V_1} \frac{|W'_{l,j}|}{\sum_{(r,c) \in E_1} |W'_{r,c}|} \cdot 1 = 1, \\
        \end{split}
        \end{align*}
        In words, $\varphi^{tot}(W)$ is indifferent in how many of the $|V_1|$ input nodes connect to a single node in the second layer. Note that a similar argument can be made for the weights connecting the $K-1$th layer with the $K$th layer. It follows that the number of non-zero weights for $W^*$ is upper bounded by $|V_1|$ for the first layer, $|V_K|$ for layer $K-1$, and $K - 3$ for the weights of the remaining layers. The resulting upper bound is then $|V_1| + |V_K| + K-3 $. 



\subsection{Proof Lemma~\ref{lem:equal_parallel_weights}} \label{sec:prf_stat_points}

We prove this by induction using the necessary and sufficient system of equations for stationarity in $\varphi^{tot}(W)$, see Equation (\ref{eq:stat_con}). 
Assume any neural network of arbitrary size with $K=2$ layers.  
Note that for this specific case any weight allocation will be stationary in $\varphi^{tot}(W)$.
Now, assume a weight allocation such that $a_{\cdot i} = a_{\cdot j}$, for all $i, j \in \argmax_{k \in V_2} a_{\cdot k}$, since adding a layer $V_{K+1}$ implies that this condition must hold to satisfy Equation (\ref{eq:stat_con}) in the next step.

Now we add a new layer of arbitrary size $V_{K+1}$. In case $V_{K+1}$ is the last layer, it is sufficient to allocate $(\theta(W))_{i,j} > 0$, for all $i\in \argmax_{k \in V_K} a_{\cdot k}$ to obtain a stationary point.
In case the neural network is expanded with another layer $V_{K+2}$ in a next step, we let $(\theta(W))_{i,j} > 0$ for $i \in \argmax_{k \in V_K}$ and $ j \in \argmax_{k \in V_{K+1}} a_{\cdot k}$, such that $a_{\cdot i} = a_{\cdot j}$, for all $i, j \in \argmax_{k \in V_{K+1}} a_{\cdot k}$ to satisfy Equation (\ref{eq:stat_con}).
Note that this immediately implies $(\theta(W))_{i,j} = (\theta(W))_{r,c}$, for all $(i,j), (r,c) \in \argmax_{(i,j) \in E_{K+1}} a_{\cdot i}  a_{\cdot j}$.
Hence, $(\theta(W))_{\gamma'_k} = (\theta(W))_{\gamma''_k}$, for all $k=2, \dots, K-2$, for all paths $\gamma$ with positive path weight. 

It remains to be shown that stationarity cannot be induced by reparameterization $\theta(W)$. To see this, we first observe that the normalization in Equation~\eqref{eq:normalization} is separable for each layer $k=1, \ldots, K-1$. 
Simply inspecting a single layer $k$, note that $\nabla_{\theta_k} J(\theta) \neq 0$, where $\theta_k = (\theta_{i,j})_{(i,j) \in E_k}$. Moreover, let $W_k = (W_{i,j})_{(i,j) \in E_k}$ and so $\nabla_{W_k} \theta_k$, is full rank (except at $W=0$) and thus preserves the non-zero property through the chain rule.



\section{Implementation Details} \label{app:imp}

In this section, we outline how $\varphi^{tot}(W)$ can be efficiently computed using a slightly modified forward pass of the neural network and a vector of ones as input.
Below, we outline how different modules are treated in this modified forward pass.
The ability to handle these modules enables the application of CoNNect across a broad spectrum of neural network architectures.

\textbf{Linear Layers:} This includes both dense (fully connected) layers and convolutional layers. The weights of these layers define the primary connections between nodes and we normalize their weights via Equation (\ref{eq:normalization}). The biases, however, merely shift activations (which we will exclude), and do not influence connectivity structure and are therefore excluded.

\textbf{BN Layers:} Batch normalization layers apply standardization and scaling to the outputs of preceding layers. For the purposes of connectivity analysis, the standardization can be disregarded as it does not alter the structure of connections, but rather rescales values. Thus, we consider BN layers as identity mappings with preserved connectivity.

\textbf{Activation Functions:} Non-linear activation functions such as ReLU, sigmoid, or tanh are ignored. These functions transform node outputs but do not influence the underlying connectivity. Ignoring them simplifies the analysis without affecting the structural representation.

\textbf{Pooling Layers:} Max-pooling layers are replaced with average pooling layers. This change ensures that all input connections are treated equally in the computation of connectivity, rather than prioritizing the strongest signal as in max-pooling.

\textbf{Dropout:} Dropout layers are designed to randomly disable connections during training as a regularization method. Since they are stochastic and transient, they are ignored for connectivity analysis, as they do not represent fixed structural linkages in the network.

\textbf{Identity Connections:} Identity connections, such as skip connections in residual networks, can be included when computing connectivity. However, since these connections (generally) are not parameterized, they can be ignored when optimizing the neural network's connectivity. Thus, we omit the identity connection in the forward pass.

\section{Experimental Settings}
\textbf{Platform}: All experiments were performed on a single NVIDIA RTX4090 GPU with 24GB of memory.

\subsection{Experimental Settings for Section~\ref{sec:te}} \label{app:te}

All models have been trained to solve Equation~\eqref{eq:all_reg}, with  coefficients as in Table~\ref{tab:small_exp}.
$\mathcal{L}(\hat{y}, y)$ is the Binary Cross Entropy between target and input probabilities, and $\| W \|_{2,1}$ is the often-applied $L_2$-regularization (weight decay). 
All models were trained for 200 epochs using Adam with a learning rate of 0.01, a cosine annealing scheduler, and batch size 256.
After training, we pruned $96\%$ of the weights in each layer using the pruning strategies discussed in Section~\ref{sec:weight_level}: i) magnitude pruning, and ii) SynFlow pruning.
Finally, the model is fine-tuned with the same hyperparameters but with a decreased initial learning rate of 0.001 for 50 epochs.



% \begin{wraptable}{r}{0.4\textwidth}%[!h]
\begin{table}[!h]
% \vspace{-0.45cm}
\caption{Regularizer coefficients. }
% \vspace{-0.3cm}
\label{tab:small_exp}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llll}
\toprule
\multicolumn{1}{c}{Regularizer}  & \multicolumn{1}{c}{\bf $\lambda_1$} & \multicolumn{1}{c}{\bf $\lambda_2$} & \multicolumn{1}{c}{\bf $\lambda_3$} \\
\midrule 
None        & $0$  & $0$ & $5\times 10^{-4}$\\
$L_1$        &  $1\times 10^{-3}$  &  $0$ & $5\times 10^{-4}$\\
CoNNect          & $0$  &  $1\times 10^{-1}$ & $5\times 10^{-4}$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
% \vspace{-0.3cm}
% \end{wraptable}
\end{table}



\textbf{Remark:} Synflow is traditionally introduced as a pre-training pruning method, its data-agnostic nature makes it less effective in this context, given the presence of uninformative input nodes. Moreover, SynFlow is generally regarded as a global pruning strategy. However, we frequently observed layer collapse under this configuration. In contrast, applying a local pruning approach yielded significantly better results, particularly for models without regularization and $L_1$ regularization. We thus  show the results using a local pruning approach.


\subsection{Experimental Settings for Section~\ref{sec:gnn}}

We provide the regularization coefficients used in Table~\ref{tab:reg_coeff_gnn}.

\begin{table}[htbp]
\centering
\caption{Regularizer coefficients used in GNN pruning.}
\label{tab:reg_coeff_gnn}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llll}
\toprule
\multicolumn{1}{c}{Regularizer}  & \multicolumn{1}{c}{\bf $\lambda_1$} & \multicolumn{1}{c}{\bf $\lambda_2$} & \multicolumn{1}{c}{\bf $\lambda_3$} \\
\midrule 
None        & $0$  & $0$ & $1\times10^{-5}$\\
$L_1$        &  $1\times10^{-6}$  &  $0$ & $1\times10^{-5}$\\
CoNNect          & $0$  &  $1\times10^{-1}$ & $1\times10^{-5}$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}



\subsection{Experimental Settings for Section~\ref{sec:add_exp_sp_cnn}} \label{app:ospcnn}

\textbf{Dataset:} We train ResNet-56 with CIFAR-10 \citep{krizhevsky2009learning}, a dataset with 60,000 32x32 images with 10 different classes. Each class has 6,000 images. Moreover, we used CIFAR-100 \citep{krizhevsky2009learning}, a more challenging dataset consisting of 100 classes with 600 images per class, to train VGG-19.


\subsection{Experimental Settings for Section~\ref{sec:ex_llm}}
\label{sec:exp3}

In the current experiment, we use 10 randomly selected samples from Bookcorpus \citep{zhu2015aligning} to be the calibration samples for establishing the dependency between parameters in the model and calculate the gradient for LLaMA-7B.
To that end, we truncate each sample to a sequence length of 128.
We set the coefficient $\lambda$ of CoNNect as $1\times10^5$. During fine-tuning, we utilize Alpaca \citep{taori2023stanford}, which comprises approximately 50,000 samples, to recover the capacity of the pruned model, which requires just 2 hours on our platform (NVIDIA RTX4090 GPU).

To determine which groups to prune, we compute importance scores for each weight in the model.
Since the simplified form $(\theta(W))_{i,j} = \vert W_{i,j}\vert$ works well on LLMs, we use absolute values instead of normalized ones to reduce computational effort. We approximate connectivity by keeping all modules as they are for efficient and ease of implementation. Moreover, to evaluate connectivity, we use multiple inputs uniformly sampled between 0 and the vocabulary size instead of a single all-one. This is to avoid conflicts with reserved input values during the training process, leading to improper connectivity evaluation.
Then, specifically for $L_2$ pruning, we compute the importance of each group by computing the $L_2$-norm and prune the groups with lowest importance scores. 
For random pruning, there is no need to compute importance scores for each group - we simply randomly select certain groups for pruning.
Moreover, we leave the
first three layers and the final layer unchanged (similar to \citet{ma2023llm}), as substantial changes to the parameters of these layers greatly influence the performance of the model.
Finally, the discovered groups within each module are pruned according to a predetermined ratio. 
The pruning rate for the selected groups is higher than the pruning ratio for the parameters since some layers (e.g., the excluded layers) retain their
parameters. For a total of $40\%$ parameter removal, we must prune $50\%$ of the groups specifically from the fourth to the thirtieth layer.

\textbf{Datasets:} To assess the model performance, we conduct a zero-shot perplexity analysis on WikiText2 
 \citep{merity2022pointer} and PTB \citep{marcus1993building}, and then follow \cite{gao2021framework} to test the model with zero-shot classification tasks on common sense reasoning datasets: BoolQ \citep{clark2019boolq}, PIQA \citep{bisk2020piqa}, HellaSwag \citep{zellers2019hellaswag}, WinoGrande \citep{sakaguchi2021winogrande},
ARC-easy, ARC-challenge \citep{clark2018think}, OpenbookQA \citep{mihaylov2018can}, where the model ranks the choices in these multiple-choice tasks. 

\section{Ablation Studies} 

\subsection{Impact of Initializations and Regularizer Strength in Section~\ref{sec:weight_level}}

To show the robustness of our results, we conduct an ablation study to: 1) analyze the impact of different initializations, and 2) the regularization strengths.

First, we present the results for 100 different initializations, where we show the (aggregated) train and test loss in Figures~\ref{fig:loss_small_a} and \subref{fig:loss_small_b} in and the fine-tuned accuracies in Figure~\ref{fig:pruning_small_a} and \subref{fig:pruning_small_b}. 
Roughly speaking, the final accuracy for each model can be categorized by the ability to find the network connecting the input nodes 1 and 2 to the output layer.
If the fine-tuned accuracy is around $0.50$, the algorithm was unable to connect node 1 and node 2 to the output (e.g., see Figures~\ref{fig:pruning_grid_no}~and~\subref{fig:pruning_grid_l1}). 
If the fine-tuned accuracy is around $0.75$, the algorithm was able to connect node 1 or node 2 to the output.
Finally, if the algorithm preserved the edges connecting node 1 and node 2, it found the correct network and achieves an accuracy of more than $0.95$ (e.g., see Figure~\ref{fig:pruning_grid_con}).


As shown in Figure~\ref{fig:pruning_small_a} and \subref{fig:pruning_small_b}, CoNNect regularization via $\varphi^{tot}(W)$ is beneficial to both pruning strategies. 
It is noteworthy that SynFlow pruning does not offer any further improvement over connectivity regularization compared to simple magnitude pruning. 
This can be attributed to the fact that CoNNect regularization has already trained the network to use the correct paths to model the current problem, as shown in Figure~\ref{fig:pruning_grid_con}.
It thus suffices to apply a simple magnitude pruning to identify these paths. 


Now we perform experiments with different values of $\lambda$. Specifically, increasing $\lambda_1$ by one order of magnitude to $0.01$ causes a frequent occurrence of layer collapse, although it does increase the performances for the cases without layer collapse, see Figure~\ref{fig:ablation1}. 
Changing $\lambda_2$ by one order of magnitude to $1$ did not cause any specific change, arguing for the stability of CoNNect.
Moreover, increasing $\lambda_3$ by one order of magnitude to $0.005$ seems to improve the model performance overall, especially for the CoNNect regularized model, see Figure~\ref{fig:ablation2}. Increasing $\lambda_3$ by another order of magnitude still shows very competitive results for CoNNect. Finally, we decrease $\lambda_1$ and $\lambda_2$ to 0.0005 and 0.05 respectively, and see that the regularizers become too weak leading the results to converge toward those of standard $L_2$ regularization.

\begin{figure}[t]
    \centering
    \subfigure[Loss values.]{
        \includegraphics[width=0.23\linewidth, trim=0.4cm 0 0.2cm 0]{figure/loss.pdf}
        \label{fig:loss_small_a}
    }
    \subfigure[Accuracies.]{
        \includegraphics[width=0.23\linewidth, trim=0.4cm 0 0.2cm 0]{figure/acc.pdf}
        \label{fig:loss_small_b}
    }
    \subfigure[Magnitude.]{
        \includegraphics[width=0.23\linewidth, trim=0.2cm 0 0.2cm 0]{figure/stacked_histograms_output_magnitude_0.pdf}
        \label{fig:pruning_small_a}
    }
    \subfigure[SynFlow.]{
        \includegraphics[width=0.23\linewidth, trim=0.2cm 0 0.2cm 0]{figure/stacked_histograms_output_synflow_0.pdf}
        \label{fig:pruning_small_b}
    }
    %\vspace{-0.3cm}
    \caption{(a)-(b) Learning curves for solving Equation (\ref{eq:min_reg}). Synflow pruning happens at iteration 200. Bandwidths are 95\% confidence intervals. (c)-(d) Fine-tuned accuracy after magnitude pruning and SynFlow pruning of regularized models. }
    \vspace{-0.5cm}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{minipage}{0.49\linewidth}
    \centering
    \subfigure[Magnitude.]{
        \includegraphics[width=0.47\linewidth]{figure/stacked_histograms_output_magnitude_1.pdf}
    }
    \subfigure[SynFlow.]{
        \includegraphics[width=0.47\linewidth]{figure/stacked_histograms_output_synflow_1.pdf}
    }
    \caption{Fine-tuned accuracy after magnitude pruning and SynFlow pruning for parameters in Table~\ref{tab:side_by_side_tables}, Experiment 1.}
    \label{fig:ablation1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
    \centering
    \subfigure[Magnitude.]{
        \includegraphics[width=0.47\linewidth]{figure/stacked_histograms_output_magnitude_2.pdf}
    }
    \subfigure[SynFlow.]{
        \includegraphics[width=0.47\linewidth]{figure/stacked_histograms_output_synflow_2.pdf}
    }
    \caption{Fine-tuned accuracy after magnitude pruning and SynFlow pruning for parameters in Table~\ref{tab:side_by_side_tables}, Experiment 2.}
    \label{fig:ablation2}
    \end{minipage}
    \begin{minipage}{0.49\linewidth}
    \centering
    \subfigure[Magnitude.]{
        \includegraphics[width=0.47\linewidth]{figure/stacked_histograms_output_magnitude_3.pdf}
    }
    \subfigure[SynFlow.]{
        \includegraphics[width=0.47\linewidth]{figure/stacked_histograms_output_synflow_3.pdf}
    }
    \caption{Fine-tuned accuracy after magnitude pruning and SynFlow pruning for parameters in Table~\ref{tab:side_by_side_tables}, Experiment 3.}
    \label{fig:ablation3}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
    \centering
    \subfigure[Magnitude.]{
        \includegraphics[width=0.47\linewidth]{figure/stacked_histograms_output_magnitude_4.pdf}
    }
    \subfigure[SynFlow.]{
        \includegraphics[width=0.47\linewidth]{figure/stacked_histograms_output_synflow_4.pdf}
    }
    \caption{Fine-tuned accuracy after magnitude pruning and SynFlow pruning for parameters in Table~\ref{tab:side_by_side_tables}, Experiment 4.}
    \label{fig:ablation4}
    \end{minipage}
\end{figure}


\begin{table*}[h]
\caption{Regularizer coefficients used for producing Figures~\ref{fig:ablation1}, \ref{fig:ablation2}, \ref{fig:ablation3}, and \ref{fig:ablation4}, respectively.}
\label{tab:side_by_side_tables}
%\label{tab:small_exp_ablation3}
\begin{center}
\begin{small}
\begin{sc}
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{lcccccccccccc}
\toprule
\multicolumn{1}{c}{}  & \multicolumn{3}{c}{Experiment 1}  & \multicolumn{3}{c}{Experiment 2}  & \multicolumn{3}{c}{Experiment 3}  & \multicolumn{3}{c}{Experiment 4} \\
\multicolumn{1}{c}{Regularizer}  & \multicolumn{1}{c}{\bf $\lambda_1$} & \multicolumn{1}{c}{\bf $\lambda_2$} & \multicolumn{1}{c}{\bf $\lambda_3$} & \multicolumn{1}{c}{\bf $\lambda_1$} & \multicolumn{1}{c}{\bf $\lambda_2$} & \multicolumn{1}{c}{\bf $\lambda_3$}& \multicolumn{1}{c}{\bf $\lambda_1$} & \multicolumn{1}{c}{\bf $\lambda_2$} & \multicolumn{1}{c}{\bf $\lambda_3$}& \multicolumn{1}{c}{\bf $\lambda_1$} & \multicolumn{1}{c}{\bf $\lambda_2$} & \multicolumn{1}{c}{\bf $\lambda_3$}\\
\midrule 
None        & $0$  & $0$ & $5\times10^{-4}$
& $0$  & $0$ & $5\times10^{-3}$
& $0$  & $0$ & $5\times10^{-2}$
& $0$  & $0$ & $5\times10^{-3}$\\
$L_1$        &  $1\times10^{-2}$  &  $0$ & $5\times10^{-4}$
&  $1\times10^{-3}$  &  $0$ & $5\times10^{-3}$
&  $1\times10^{-3}$  &  $0$ & $5\times10^{-2}$
&  $5\times10^{-4}$  &  $0$ & $5\times10^{-3}$\\
CoNNect          & $0$  &  $1$ & $5\times10^{-4}$
& $0$  &  $1\times10^{-1}$ & $5\times10^{-3}$
& $0$  &  $1\times10^{-1}$ & $5\times10^{-2}$
& $0$  &  $5\times10^{-2}$ & $5\times10^{-3}$\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{sc}
\end{small}
\end{center}
\end{table*}



\subsection{Computational Improvement of CoNNect for Different Pruning Ratios in Section~\ref{sec:ex_llm}} \label{app:abb_infs}

In Table~\ref{tab:complex}, we show the computational improvement of CoNNect across different pruning ratios, both in terms of complexity and latency. We see that speed up in complexity yields an almost one-to-one improvement in latency speed \citep{ma2023llm}.

We perform a comparison with our results with the semi-structured pruning method Wanda \citep{sun2023simple}. To perform a meaningful comparison in terms of performance for a given level of computational efficiency gain, we compare two models that obtain the same level of latency improvement as proxy for computational efficiency. \citet{sun2023simple} obtains a $1.2 \times$ latency improvement with a $50\%$ pruning ratio via $2:4$ semi-structured pruning. Note that using our structured pruning approach, we achieve a similar latency improvement while only having to prune $20\%$ of the weights in our model, see Table~\ref{tab:complex}.

We present perplexity results of our $20\%$ pruned model in Table~\ref{tab:20prunllm}, where we use a prompting following LLM-pruner \citep{ma2023llm}. Since \citet{sun2023simple} use a different prompting, we compare the ratio of perplexity of the pruned model without fine-tuning, with the base model, which is $1.50 : 1$ for CoNNect, and $2.03 : 1$ for \citet{sun2023simple}, thus demonstrating that our approach achieves better performance retention after pruning compared to \citet{sun2023simple}.

\begin{table}[H]
\centering
\caption{Computational complexity of CoNNect-pruned LLaMA-7B model for different pruning ratios. Speed up is the GMACs of the base model divided by the GMACs of the pruned model.}
\label{tab:complex}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccc}
\toprule
\makecell{Pruning\\Ratio} &	\makecell{Parameter\\Count} & \makecell{GPU Memory\\(MiB)} 	& \makecell{Comp. Complexity\\(GMACs)}  &   \makecell{Speed up\\ (GMACs)} & \makecell{Speed up\\ (Latency)}\\ \midrule
0.00	&6.7B	&12892.6	& 425.1	& -  & - \\
0.20    & 5.4B  &10383.7   &340.5 & 1.3$\times$  & $1.2\times$ \\
0.47&	3.6B&	6995.7	&222.7	&1.9$\times$ & $1.7\times$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{table*}[htb]
  \centering
  %\setlength{\tabcolsep}{4pt}
  \caption{Zero-shot performance of the compressed LLaMA-7B. The evaluation is conducted following the prompting of LLM-pruner \citep{ma2023llm}. To ensure fairness, both models are fine-tuned on the Alpaca dataset for only 2 epochs.}
  \label{tab:20prunllm}
  \vskip -0.1in
  % \label{tab: llm}
  \begin{center}
    \begin{small}
    \begin{sc}
  \begin{adjustbox}{max width=\linewidth}
  \begin{tabular}{ll|cc|ccccccc|c}
    \toprule
    Pruned Model & Method & WikiText2$\downarrow$ &  PTB$\downarrow$ &BoolQ$^*$ &PIQA &HellaSwag &WinoGrande$^*$ &ARC-e &ARC-c &OBQA & Average\\
    \midrule 
    Ratio $=0\%$ & LlaMA-7B & 12.62 & 22.15   & 73.15 & 77.48 & 73.01 & 67.09 & 52.57 & 41.47 & 42.40    & 61.02 \\
    \midrule
    \multirow{2}{*}{\makecell[l]{Ratio $=20\%$\\ w/o tune}} 
     &  LLM-Pruner  & 19.09 & 34.23   & 57.13 & 75.08 & 66.83 & 59.75 & 50.13 & 36.35 & 39.80    & 55.01 \\
     &\cellcolor{mygray}CoNNect     &\cellcolor{mygray}\textbf{18.91} &\cellcolor{mygray}\textbf{33.25}   &\cellcolor{mygray}\textbf{\textbf{61.65}} &\cellcolor{mygray}\textbf{75.63} &\cellcolor{mygray}\textbf{67.73} &\cellcolor{mygray}61.56 &\cellcolor{mygray}\textbf{50.38} &\cellcolor{mygray}\textbf{36.95} &\cellcolor{mygray}\textbf{40.80}    &\cellcolor{mygray}\textbf{56.39} \\
    \midrule
    \multirow{2}{*}{\makecell[l]{Ratio $=20\%$\\ w/ tune}} 
     &  LLM-Pruner  & 17.66 & 30.51   & 65.20 & \textbf{76.88} & 68.65 & 63.93 & 52.31 & 37.03 & 40.80    & 57.83 \\
     &\cellcolor{mygray}CoNNect     &\cellcolor{mygray}\textbf{17.18} &\cellcolor{mygray}\textbf{29.92}   &\cellcolor{mygray}\textbf{66.57} &\cellcolor{mygray}76.82
     &\cellcolor{mygray}\textbf{69.42} &\cellcolor{mygray}\textbf{64.72}
     &\cellcolor{mygray}\textbf{53.24} &\cellcolor{mygray}\textbf{39.16} &\cellcolor{mygray}\textbf{41.40}    &\cellcolor{mygray}\textbf{58.76} \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
