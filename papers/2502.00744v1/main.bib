@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@inproceedings{fang2023depgraph,
  title={Depgraph: Towards any structural pruning},
  author={Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16091--16101},
  year={2023}
}

@article{salimans2016weight,
  title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  author={Salimans, Tim and Kingma, Durk P},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{fang2024maskllm,
  title={Maskllm: Learnable semi-structured sparsity for large language models},
  author={Fang, Gongfan and Yin, Hongxu and Muralidharan, Saurav and Heinrich, Greg and Pool, Jeff and Kautz, Jan and Molchanov, Pavlo and Wang, Xinchao},
  journal={arXiv preprint arXiv:2409.17481},
  year={2024}
}

@article{li2020pruning,
  title={A pruning feedforward small-world neural network based on Katz centrality for nonlinear system modeling},
  author={Li, Wenjing and Chu, Minghui and Qiao, Junfei},
  journal={Neural Networks},
  volume={130},
  pages={269--285},
  year={2020},
  publisher={Elsevier}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@misc{frantar2023sparsegptmassivelanguagemodels,
      title={SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot}, 
      author={Elias Frantar and Dan Alistarh},
      year={2023},
      eprint={2301.00774},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.00774}, 
}

@inproceedings{ziyin2023spred,
  title={spred: Solving L1 Penalty with SGD},
  author={Ziyin, Liu and Wang, Zihao},
  booktitle={International Conference on Machine Learning},
  pages={43407--43422},
  year={2023},
  organization={PMLR}
}



@inproceedings{phaisangittisagul2016analysis,
  title={An analysis of the regularization between L2 and dropout in single hidden layer neural network},
  author={Phaisangittisagul, Ekachai},
  booktitle={2016 7th International Conference on Intelligent Systems, Modelling and Simulation (ISMS)},
  pages={174--179},
  year={2016},
  organization={IEEE}
}

@article{sen2008collective,
  title={Collective classification in network data},
  author={Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and Galligher, Brian and Eliassi-Rad, Tina},
  journal={AI Magazine},
  volume={29},
  number={3},
  pages={93--93},
  year={2008}
}

@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}


@article{loshchilov2017fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank and others},
  journal={arXiv preprint arXiv:1711.05101},
  volume={5},
  year={2017}
}

@article{de2022neural,
  title={Neural network training using $L_1$-regularization and bi-fidelity data},
  author={De, Subhayan and Doostan, Alireza},
  journal={Journal of Computational Physics},
  volume={458},
  pages={111010},
  year={2022},
  publisher={Elsevier}
}

@article{yang2019structured,
  title={Structured pruning of convolutional neural networks via l1 regularization},
  author={Yang, Chen and Yang, Zhenghong and Khattak, Abdul Mateen and Yang, Liu and Zhang, Wenxin and Gao, Wanlin and Wang, Minjuan},
  journal={IEEE Access},
  volume={7},
  pages={106385--106394},
  year={2019},
  publisher={IEEE}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{anwar2017structured,
  title={Structured pruning of deep convolutional neural networks},
  author={Anwar, Sajid and Hwang, Kyuyeon and Sung, Wonyong},
  journal={ACM Journal on Emerging Technologies in Computing Systems (JETC)},
  volume={13},
  number={3},
  pages={1--18},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1389--1397},
  year={2017}
}

@inproceedings{li2020few,
  title={Few sample knowledge distillation for efficient network compression},
  author={Li, Tianhong and Li, Jianguo and Liu, Zhuang and Zhang, Changshui},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={14639--14647},
  year={2020}
}

@article{tai2015convolutional,
  title={Convolutional neural networks with low-rank regularization},
  author={Tai, Cheng and Xiao, Tong and Zhang, Yi and Wang, Xiaogang and others},
  journal={arXiv preprint arXiv:1511.06067},
  year={2015}
}

@incollection{gholami2022survey,
  title={A survey of quantization methods for efficient neural network inference},
  author={Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Low-Power Computer Vision},
  pages={291--326},
  year={2022},
  publisher={Chapman and Hall/CRC}
}

@article{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={20378--20389},
  year={2020}
}

@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={arXiv preprint arXiv:1611.06440},
  year={2016}
}

@article{zhou1999subset,
  title={Subset-based training and pruning of sigmoid neural networks},
  author={Zhou, Guian and Si, Jennie},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={79--89},
  year={1999},
  publisher={Elsevier}
}


@article{katz1953new,
  title={A new status index derived from sociometric analysis},
  author={Katz, Leo},
  journal={Psychometrika},
  volume={18},
  number={1},
  pages={39--43},
  year={1953},
  publisher={Springer}
}

@article{neyshabur2015path,
  title={Path-sgd: Path-normalized optimization in deep neural networks},
  author={Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@inproceedings{thimm1995evaluating,
  title={Evaluating pruning methods},
  author={Thimm, Georg and Fiesler, Emile},
  booktitle={Proceedings of the International Symposium on Artificial neural networks},
  pages={20--25},
  year={1995}
}

@inproceedings{hagiwara1993removal,
  title={Removal of hidden units and weights for back propagation networks},
  author={Hagiwara, Masafumi},
  booktitle={Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)},
  volume={1},
  pages={351--354},
  year={1993},
  organization={IEEE}
}

@article{hoefler2021sparsity,
  title={Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={241},
  pages={1--124},
  year={2021}
}

@article{wang2020picking,
  title={Picking winning tickets before training by preserving gradient flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  journal={arXiv preprint arXiv:2002.07376},
  year={2020}
}

@article{lee2018snip,
  title={Snip: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={arXiv preprint arXiv:1810.02340},
  year={2018}
}

@incollection{hinton2012practical,
  title={A practical guide to training restricted Boltzmann machines},
  author={Hinton, Geoffrey E},
  booktitle={Neural Networks: Tricks of the Trade: Second Edition},
  pages={599--619},
  year={2012},
  publisher={Springer}
}

@article{janowsky1989pruning,
  title={Pruning versus clipping in neural networks},
  author={Janowsky, Steven A},
  journal={Physical Review A},
  volume={39},
  number={12},
  pages={6600},
  year={1989},
  publisher={APS}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{he2023structured,
  title={Structured pruning for deep convolutional neural networks: A survey},
  author={He, Yang and Xiao, Lingao},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2023},
  publisher={IEEE}
}

 @article{zhuang2020neuron,
  title={Neuron-level structured pruning using polarization regularizer},
  author={Zhuang, Tao and Zhang, Zhixuan and Huang, Yuheng and Zeng, Xiaoyi and Shuang, Kai and Li, Xiang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9865--9877},
  year={2020}
}

@article{Huang_Wang_2017, title={Data-Driven Sparse Structure Selection for Deep Neural Networks}, DOI={10.48550/arxiv.1707.01213}, abstractNote={Deep convolutional neural networks have liberated its extraordinary power on various tasks. However, it is still very challenging to deploy state-of-the-art models into real-world applications due to their high computational complexity. How can we design a compact and effective network without massive experiments and expert knowledge? In this paper, we propose a simple and effective framework to learn and prune deep models in an end-to-end manner. In our framework, a new type of parameter -- scaling factor is first introduced to scale the outputs of specific structures, such as neurons, groups or residual blocks. Then we add sparsity regularizations on these factors, and solve this optimization problem by a modified stochastic Accelerated Proximal Gradient (APG) method. By forcing some of the factors to zero, we can safely remove the corresponding structures, thus prune the unimportant parts of a CNN. Comparing with other structure selection methods that may need thousands of trials or iterative fine-tuning, our method is trained fully end-to-end in one training pass without bells and whistles. We evaluate our method, Sparse Structure Selection with several state-of-the-art CNNs, and demonstrate very promising results with adaptive depth and width selection.}, journal={arXiv}, author={Huang, Zehao and Wang, Naiyan}, year={2017} }

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE international conference on neural networks},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@article{wen2016learning,
  title={Learning structured sparsity in deep neural networks},
  author={Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{yuan2006model,
  title={Model selection and estimation in regression with grouped variables},
  author={Yuan, Ming and Lin, Yi},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={68},
  number={1},
  pages={49--67},
  year={2006},
  publisher={Oxford University Press}
}

@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6377--6389},
  year={2020}
}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Oxford University Press}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}


@article{touvron2023open,
  title={Open and efficient foundation language models},
  author={Touvron, H and Lavril, T and Izacard, G and Martinet, X and Lachaux, MA and Lacroix, T and Rozi{\`e}re, B and Goyal, N and Hambro, E and Azhar, F and others},
  journal={Preprint at arXiv. https://doi. org/10.48550/arXiv},
  volume={2302},
  year={2023}
}

@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2924--2936},
  year={2019}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{simonyan2015very,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015}
}


@inproceedings{mihaylov2018can,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}


@article{gao2021framework,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
  journal={Version v0. 0.1. Sept},
  volume={10},
  pages={8--9},
  year={2021}
}

@inproceedings{merity2022pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitch and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  journal={Computational linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993}
}


@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{zhu2015aligning,
  title={Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},
  author={Zhu, Yukun},
  journal={arXiv preprint arXiv:1506.06724},
  year={2015}
}


@misc{taori2023stanford,
  title={Stanford alpaca: An instruction-following llama model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}
