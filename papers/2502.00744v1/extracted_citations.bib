@article{de2022neural,
  title={Neural network training using $L_1$-regularization and bi-fidelity data},
  author={De, Subhayan and Doostan, Alireza},
  journal={Journal of Computational Physics},
  volume={458},
  pages={111010},
  year={2022},
  publisher={Elsevier}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@inproceedings{hagiwara1993removal,
  title={Removal of hidden units and weights for back propagation networks},
  author={Hagiwara, Masafumi},
  booktitle={Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)},
  volume={1},
  pages={351--354},
  year={1993},
  organization={IEEE}
}

@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE international conference on neural networks},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@inproceedings{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1389--1397},
  year={2017}
}

@incollection{hinton2012practical,
  title={A practical guide to training restricted Boltzmann machines},
  author={Hinton, Geoffrey E},
  booktitle={Neural Networks: Tricks of the Trade: Second Edition},
  pages={599--619},
  year={2012},
  publisher={Springer}
}

@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{lee2018snip,
  title={Snip: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={arXiv preprint arXiv:1810.02340},
  year={2018}
}

@article{li2020pruning,
  title={A pruning feedforward small-world neural network based on Katz centrality for nonlinear system modeling},
  author={Li, Wenjing and Chu, Minghui and Qiao, Junfei},
  journal={Neural Networks},
  volume={130},
  pages={269--285},
  year={2020},
  publisher={Elsevier}
}

@article{loshchilov2017fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank and others},
  journal={arXiv preprint arXiv:1711.05101},
  volume={5},
  year={2017}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={arXiv preprint arXiv:1611.06440},
  year={2016}
}

@inproceedings{phaisangittisagul2016analysis,
  title={An analysis of the regularization between L2 and dropout in single hidden layer neural network},
  author={Phaisangittisagul, Ekachai},
  booktitle={2016 7th International Conference on Intelligent Systems, Modelling and Simulation (ISMS)},
  pages={174--179},
  year={2016},
  organization={IEEE}
}

@article{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={20378--20389},
  year={2020}
}

@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6377--6389},
  year={2020}
}

@inproceedings{thimm1995evaluating,
  title={Evaluating pruning methods},
  author={Thimm, Georg and Fiesler, Emile},
  booktitle={Proceedings of the International Symposium on Artificial neural networks},
  pages={20--25},
  year={1995}
}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Oxford University Press}
}

@article{wang2020picking,
  title={Picking winning tickets before training by preserving gradient flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  journal={arXiv preprint arXiv:2002.07376},
  year={2020}
}

@article{yang2019structured,
  title={Structured pruning of convolutional neural networks via l1 regularization},
  author={Yang, Chen and Yang, Zhenghong and Khattak, Abdul Mateen and Yang, Liu and Zhang, Wenxin and Gao, Wanlin and Wang, Minjuan},
  journal={IEEE Access},
  volume={7},
  pages={106385--106394},
  year={2019},
  publisher={IEEE}
}

@article{zhou1999subset,
  title={Subset-based training and pruning of sigmoid neural networks},
  author={Zhou, Guian and Si, Jennie},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={79--89},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{ziyin2023spred,
  title={spred: Solving L1 Penalty with SGD},
  author={Ziyin, Liu and Wang, Zihao},
  booktitle={International Conference on Machine Learning},
  pages={43407--43422},
  year={2023},
  organization={PMLR}
}

