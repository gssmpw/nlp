\section{Related Works}
\subsection{Visual Prompting}
The concept of prompting originated in the field of NLP as a technique for adapting pre-trained models to downstream tasks \citep{shin-etal-2020-autoprompt, liu2022few, li2021prefix, liu2023pre, han2023svdiff, han2024proxedit, jin2023visual}. This design philosophy was later extended to CV by \citet{bahng2022exploring}, who introduced tunable parameters directly into input images
% , thus creating a prompted image, referred to as a Visual Prompt (VP). 
 to create what is known as a Visual Prompt (VP). 
A typical VP framework consists of two primary modules: input design and output transformation \citep{bahng2022exploring, tsai2020reprogramming, tsao2024autovp,caisample}. 
% Several approaches have been proposed for constructing prompted images. 
Various strategies have been proposed for constructing VPs.
For instance, \citet{bahng2022exploring} modify input images by adding a frame of visual prompting parameters, whereas \citet{chen2023understanding} incorporate the visual prompting parameters around resized images. \citet{wu2022learning} 
% investigate methods to efficiently generate
explore efficient methods for generating visual prompts that enhance performance across different tasks, and \citet{Oh_2023_CVPR} develop visual prompts designed for adapting models to black-box, inaccessible models. Since the output logits of pre-trained models remain in the source domain, an additional output transformation (e.g., label mapping) is required to accurately predict the targets. A simple approach is to randomly map source labels (RLM) onto target labels. \citet{tsai2020reprogramming} propose a frequency-based label mapping (FLM) technique, which derives the mapping based on frequency statistics. \citet{chen2023understanding} further introduces iterative label mapping (ILM), which improves the performance of visual prompting. \citet{yang-etal-2023-prompt} proposes a semantics-based label mapping approach that aligns source and target classes based on semantic similarity. Additionally, \citet{tsao2024autovp} introduces full mapping (FM), utilizing an automated system to select the most appropriate label mapping (LM) method to optimize performance across diverse downstream tasks.

\subsection{Low-Rank Structures in Deep Learning}

Low-rank structures are widely observed in machine learning, as many problems inherently exhibit low-rank properties \citep{li2016lora, cai2010singular, li2018low, grasedyck2013low}. It has been found that for numerous deep learning tasks, especially those involving heavily over-parameterized neural networks, the resulting models tend to exhibit low-rank characteristics after training \citep{oymak2019generalization, khodak2021initialization}. Some prior work has explicitly integrated low-rank constraints during the training process of neural networks \citep{sainath2013low, zhang2014facial, zhao2016energy}. From a theoretical perspective, neural networks have been shown to outperform classical learning methods, including finite-width neural tangent kernels, when the underlying concept class has a low-rank structure \citep{allen2019convergence, li2018learning, ghorbani2020neural, allen2019can, allen2020backward}. Additionally, \citet{allen2020backward_correction} highlight that low-rank adaptations can be beneficial in adversarial training scenarios. The LoRA method, introduced by \citet{hu2021lora}, along with its variants \citep{zhang2023svd, yeh2023lora}, is particularly noteworthy for not introducing additional inference burdens, thus improving the parameter efficiency of adapting large pre-trained models. These methods employ low-rank matrices to approximate weight updates during fine-tuning, enabling a seamless integration with pre-trained weights prior to inference.