
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
% 包并用 \toprule, \midrule, 和 \bottomrule 替换 \hline 来获得更专业的表格线
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{float}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{tabularx}
\usepackage{pifont}
\usepackage{subcaption}
% \usepackage{subfig}
\usepackage{wrapfig}
% \usepackage{article}
\usepackage{enumitem}

\newtheorem{theorem}{Theorem}[section]
\newcommand{\zw}[1]{\textbf{\textcolor{orange}{[zhenting: #1]}}}

\newcommand{\ours}{\textsc{LoR-VP}\,}
\newcommand{\bdelta}{\boldsymbol\delta}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\btheta}{\boldsymbol\theta}
\newcommand{\bmm}{\mathbf{m}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bA}{\mathbf{A}}

\usepackage{xcolor}   % For color definitions
\definecolor{myred}{RGB}{220,50,47} % Solarized color
\definecolor{mygreen}{RGB}{133,153,0}
\newcommand{\cmark}{\textcolor{mygreen}{\ding{51}}}  % Green checkmark
\newcommand{\xmark}{\textcolor{myred}{\ding{55}}}    % Red X mark


\title{\ours: Low-Rank Visual Prompting for Efficient Vision Model Adaptation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Can Jin$^{1}$, Ying Li$^{2}$, Mingyu Zhao$^{1}$, Shiyu Zhao$^{1}$, Zhenting Wang$^{1}$, Xiaoxiao He$^{1}$, \\
  \textbf{Ligong Han$^{3,4}$}, \textbf{Tong Che$^{5}$}, \textbf{Dimitris N. Metaxas$^{1}$} \\
  $^1$Rutgers University, $^2$Zhejiang University, $^3$Red Hat AI Innovation, \\ $^4$MIT-IBM Watson AI Lab, $^5$NVIDIA Research
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}

Visual prompting has gained popularity as a method for adapting pre-trained models to specific tasks, particularly in the realm of parameter-efficient tuning. However, existing visual prompting techniques often pad the prompt parameters around the image, limiting the interaction between the visual prompts and the original image to a small set of patches while neglecting the inductive bias present in shared information across different patches. In this study, we conduct a thorough preliminary investigation to identify and address these limitations. We propose a novel visual prompt design, introducing \textbf{Lo}w-\textbf{R}ank matrix multiplication for \textbf{V}isual \textbf{P}rompting (\ours), which enables shared and patch-specific information across rows and columns of image pixels. Extensive experiments across seven network architectures and four datasets demonstrate significant improvements in both performance and efficiency compared to state-of-the-art visual prompting methods, achieving up to $6\times$ faster training times, utilizing $18\times$ fewer visual prompt parameters, and delivering a $3.1\%$ improvement in performance. Code is available at \url{https://github.com/jincan333/LoR-VP}.

\end{abstract}

\renewcommand{\thefootnote}{}
\footnotetext{Correspondence to: Can Jin \texttt{<can.jin@rutgers.edu>}}


\section{Introduction}
Many applications in computer vision (CV) and natural language processing (NLP) rely on adapting large-scale, pre-trained models to multiple downstream tasks \citep{liu2021swin, dosovitskiy2020image, ridnik1imagenet, NEURIPS2020_1457c0d6, peng2024maxk}. Recent advances in large language models (LLMs) have highlighted data-centric techniques such as in-context learning \citep{NEURIPS2020_1457c0d6, shin-etal-2020-autoprompt, liu2022few} and prompting \citep{li2021prefix, liu2023pre}. These techniques 
well-designed prompts or input templates to significantly enhance the performance of LLMs across a wide range of tasks. Inspired by these methods, visual prompting has gained substantial attention as a means of adapting pre-trained vision models by modifying input pixels or output transformations \citep{bahng2022exploring,chen2023understanding,tsao2024autovp}.

Existing visual prompting methods, such as those proposed by CLIP-VP \citep{bahng2022exploring}, ILM-VP \citep{chen2023understanding}, and AutoVP \citep{tsao2024autovp}, have demonstrated the capability to enhance the performance of pre-trained vision models across various downstream tasks. A natural question arises: why does the addition or padding of tunable parameters to the original image pixels improves adaptation performance? A plausible explanation is that the introduced visual prompts (VPs) provide task-specific information that not only alters the representation of the original images but also influences the attention distribution across image patches. This is particularly significant 
% when employing
in pre-trained models such as Vision Transformers (ViTs) \citep{dosovitskiy2020image}, where VPs interact with patch tokens and guide the model’s attention to different parts of the image.

However, current VP designs primarily focus on adding or padding tunable parameters at the periphery of the image (see Part 1 of Figure \ref{Figure_baselines} for existing VP designs), thus only allowing boundary patches to be modified, while the central patches remain unchanged. These designs present two notable limitations: (1) The VP parameters are restricted to interacting with the original image in a limited set of patches, leaving a substantial portion of the image unmodified. As a result, VPs can only influence the model’s interpretation of specific regions of the image, while other regions-potentially containing critical information-remain unaffected. (2) The VPs applied to each patch operate independently, disregarding the inductive biases present in the shared information and positional encoding across different patches. For instance, one patch may represent part of an object, while an adjacent patch represents another part of the same object. By ignoring these inter-patch relationships, visual prompting may limit the model’s ability to capture the global context of the image effectively.

To address these limitations, we first conduct a thorough investigation to understand the shortcomings of existing methods. Building on this analysis, we propose a novel visual prompting method, termed \ours, aimed at more efficient and effective adaptation of vision models. Our approach not only influences every patch of the image but also introduces inductive biases between the rows and columns of image patches. By leveraging \ours, we achieve superior performance compared to state-of-the-art (SOTA) methods while significantly reducing the number of parameters required. In summary, our contributions are organized around the following three thrusts:

\begin{itemize}[leftmargin=*]
\item [$\star$] (Preliminary Study) We conduct an in-depth preliminary study to identify and illustrate the limitations present in current visual prompting methods and explore potential solutions to overcome these challenges.

\item [$\star$] (Novel Approach) To mitigate these limitations, we propose a novel visual prompting technique, named \ours, which optimizes the visual prompts uniformly across all patches and introduces inductive biases between the rows and columns of the image patches.

\item [$\star$] (Experiments) We perform extensive experiments across a wide range of large-scale models and datasets. The empirical results consistently demonstrate the significant improvements in both performance and efficiency achieved by \ours, validating its practical effectiveness. For instance, \ours surpasses the previous SOTA method, AutoVP \citep{tsao2024autovp}, by an average of $3.1\%$ on seven network architectures and four datasets using $6\times $ less training time.

\end{itemize}

\section{Related Works}

\subsection{Visual Prompting}
The concept of prompting originated in the field of NLP as a technique for adapting pre-trained models to downstream tasks \citep{shin-etal-2020-autoprompt, liu2022few, li2021prefix, liu2023pre, han2023svdiff, han2024proxedit, jin2023visual}. This design philosophy was later extended to CV by \citet{bahng2022exploring}, who introduced tunable parameters directly into input images
% , thus creating a prompted image, referred to as a Visual Prompt (VP). 
 to create what is known as a Visual Prompt (VP). 
A typical VP framework consists of two primary modules: input design and output transformation \citep{bahng2022exploring, tsai2020reprogramming, tsao2024autovp,caisample}. 
% Several approaches have been proposed for constructing prompted images. 
Various strategies have been proposed for constructing VPs.
For instance, \citet{bahng2022exploring} modify input images by adding a frame of visual prompting parameters, whereas \citet{chen2023understanding} incorporate the visual prompting parameters around resized images. \citet{wu2022learning} 
% investigate methods to efficiently generate
explore efficient methods for generating visual prompts that enhance performance across different tasks, and \citet{Oh_2023_CVPR} develop visual prompts designed for adapting models to black-box, inaccessible models. Since the output logits of pre-trained models remain in the source domain, an additional output transformation (e.g., label mapping) is required to accurately predict the targets. A simple approach is to randomly map source labels (RLM) onto target labels. \citet{tsai2020reprogramming} propose a frequency-based label mapping (FLM) technique, which derives the mapping based on frequency statistics. \citet{chen2023understanding} further introduces iterative label mapping (ILM), which improves the performance of visual prompting. \citet{yang-etal-2023-prompt} proposes a semantics-based label mapping approach that aligns source and target classes based on semantic similarity. Additionally, \citet{tsao2024autovp} introduces full mapping (FM), utilizing an automated system to select the most appropriate label mapping (LM) method to optimize performance across diverse downstream tasks.

\subsection{Low-Rank Structures in Deep Learning}

Low-rank structures are widely observed in machine learning, as many problems inherently exhibit low-rank properties \citep{li2016lora, cai2010singular, li2018low, grasedyck2013low}. It has been found that for numerous deep learning tasks, especially those involving heavily over-parameterized neural networks, the resulting models tend to exhibit low-rank characteristics after training \citep{oymak2019generalization, khodak2021initialization}. Some prior work has explicitly integrated low-rank constraints during the training process of neural networks \citep{sainath2013low, zhang2014facial, zhao2016energy}. From a theoretical perspective, neural networks have been shown to outperform classical learning methods, including finite-width neural tangent kernels, when the underlying concept class has a low-rank structure \citep{allen2019convergence, li2018learning, ghorbani2020neural, allen2019can, allen2020backward}. Additionally, \citet{allen2020backward_correction} highlight that low-rank adaptations can be beneficial in adversarial training scenarios. The LoRA method, introduced by \citet{hu2021lora}, along with its variants \citep{zhang2023svd, yeh2023lora}, is particularly noteworthy for not introducing additional inference burdens, thus improving the parameter efficiency of adapting large pre-trained models. These methods employ low-rank matrices to approximate weight updates during fine-tuning, enabling a seamless integration with pre-trained weights prior to inference. 

\section{Preliminary Study}\label{Section_preliminary_study}
\subsection{Analysis of Pad Prompting}
Visual prompting is proposed to address the problem of adapting a pre-trained source model to downstream tasks without fine-tuning the network weights. % Suppose we have
Consider a downstream target image dataset $\mathcal{D} = \{(\bx_1, y_1),...,(\bx_n, y_n)\}$ with typical color channels $c$ (usually 3) and a pre-trained vision model $f$ with a resolution of $L\times L$ (the default value of $L$ is $224$ for simplicity). Visual prompting modifies the images by adding tunable parameters to the image pixels. Among various methods, Pad Prompting\citep{tsao2024autovp,chen2023understanding,bahng2022exploring} is the most prevalent, which involves resizing the initial images to a specific size $s$ (typically smaller than $L$, such as $128$), and then surrounding the resized image with a tunable parameter border of size $p$ such that $s+2p=L$, resulting in a prompted image of dimensions $L\times L$. An illustration of Pad Prompting is depicted in Part 1 of Figure \ref{Figure_baselines}. The optimal values for $s$ and $p$ generally vary across different models and tasks. AutoVP, currently the SOTA in visual prompting, automates the selection of $s$ and $p$ to enhance performance across various models and tasks \citep{tsao2024autovp}.

\begin{figure}[H]
    \centering
   \includegraphics[width=1\linewidth]{images/method/baseline_method_8.png}
\caption{Illustration of various visual prompting methods applied to target domain data:
\ding{182} \textbf{AutoVP(Pad):} Focuses on optimizing the balance between image scaling and tunable parameter integration to enhance model responsiveness.
\ding{183} \textbf{Patch-Pad:} Aims to enhance localized learning by surrounding each image patch with tunable visual prompts.
% , thereby improving the interaction between the model and segmented image data.
\ding{184} \textbf{Patch-Free:} Provides maximum adaptability by allowing independent tuning of visual prompts for each patch, catering to diverse feature requirements across the image.
\ding{185} \textbf{Patch-Same:} Promotes consistency in model training by applying uniform visual prompts across all patches, ensuring coherent feature learning across input.} \label{Figure_baselines}
\end{figure}

Despite AutoVP's automated process for optimizing prompt size, Pad Prompting inherently only adjusts the prompts in the peripheral patches of the resized images, leaving the central part unchanged. Furthermore, the parameters in different patches are optimized independently, disregarding the inductive biases that arise from shared information and positional encoding across patches. We hypothesize that a more effective visual prompting strategy would allow interaction with each patch while also considering the inductive biases among them. This hypothesis has led us to explore new VP designs:

\begin{itemize}
    \item \textbf{Patch-Pad}: As shown in Part 2 of Figure \ref{Figure_baselines}, after resizing the image, we evenly split it into different patches and then pad the tunable parameters around each patch to form a resolution $L\times L$ image. For instance, when using an ImageNet-21K \citep{deng2009imagenet} pre-trained ViT-B/32 \citep{dosovitskiy2020image} and fine-tuned on ImageNet-1K model, the resize image is split into $7\times 7$ patches, and each patch is padded to a size of $32\times 32$ using tunable parameters. The Patch-Pad method can influence each patch of the image but split the image into discontiguous parts and potentially devastate the information in the images.
    \item \textbf{Patch-Free}: As shown in Part 3 of Figure \ref{Figure_baselines}, to avoid dropping information in the images, we directly resize the image to resolution $L\times L$, then evenly add independent tunable parameters (initialized as 0) to each patch of the resized image. The Patch-Free design can influence each patch of the image without splitting the resized image thus maintaining the information of the image, but this design updates the patch VPs independently and doesn't consider shared information on different patches.
    \item \textbf{Patch-Same}: As shown in Part 4 of Figure \ref{Figure_baselines}, to enable shared prompting among each patch, we initialize a patch of tunable parameters and repeatedly add it to all patches of the image. Patch-Same enables shared visual prompting among different patches but constrains the shared information as the same for all patches.
\end{itemize}

\subsection{Performance Investigation of Different VP Designs}

To investigate the performance of the aforementioned four VP designs, we conducted experiments using ImageNet-21K pre-trained ViT-B/32 and ViT-B/16 (both fine-tuned on ImageNet-1K) on CIFAR10/100 \citep{krizhevsky2009learning}. 
% We use a patch size of $32$ for ViT-B/32 and $16$ for ViT-B/16. 
The performance of AutoVP (Pad Prompting) was used as a benchmark, with all methods employing a fully connected label  (FM) \citep{tsao2024autovp}  for fair comparison. The results, depicted in Figure \ref{Figure_preliminary_investigation}, indicate that
\ding{182} Patch-Pad underperforms across all models and datasets, the most likely reason is it split the image patches thus might damage the image information. \ding{183} Patch-Free outperforms Patch-Pad, confirming that maintaining image continuity is beneficial. However, Patch-Free is less effective than Patch-Same, which suggests that shared visual prompting can enhance performance. \ding{184} Patch-Same outperforms AutoVP, underscoring the importance of shared prompting information across patches. \ding{185} The performance gap between Patch-Same and Pad Prompting shrinks in models that have smaller patch sizes, suggesting that when using the ViT-B/16 model, Patch-Same constrains more patches to learn a smaller visual prompt, which may bring too strong constraints to the visual prompts, and a better way is utilizing visual prompts that not only introduce inductive bias in different patches but also allow for patch-specific visual prompting. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/motivation/vit_b_16_1k_cifar100.pdf}
     \caption{\textbf{Preliminary Investigation Results.} Performance comparison of various VP designs. Our VP method demonstrates competitive or superior performance in several configurations. The final performance of each method is marked by $\bigstar$ or $\bullet$, with all results averaged over three runs. }
    \label{Figure_preliminary_investigation}
\end{figure}


\section{Methodology}

Inspired by the observations in Section \ref{Section_preliminary_study}, we propose a novel visual prompt design that facilitates prompting across all patches while enabling both shared and patch-specific information. This approach leverages low-rank matrix multiplication to efficiently manage visual prompts.

\subsection{Low-Rank Visual Prompting}

In order to facilitate comprehensive pixel information and interaction across all patches, we resize the image to a uniform size of $L \times L$. This resizing strategy is designed to minimize information loss, which is a common issue with the traditional Pad Prompting method that often resizes images to dimensions smaller than $L$. To enable the sharing of visual prompt information across different patches, we introduce two low-rank matrix parameters, $\bB \in \mathbb{R}^{c \times L \times r}$ and $\bA \in \mathbb{R}^{c \times r \times L}$, where $r \ll L$. The product of these matrices, $\bB \cdot \bA$, serves as the visual prompt.

This configuration allows the visual prompt to act as a linear combination of the row vectors in $\bA$ and the column vectors in $\bB$, facilitating shared information across the rows and columns of the image. Additionally, this design supports patch-specific information, as the coefficients of each row and column are independently adjustable. Based on our observations in Section \ref{Section_preliminary_study}, this approach to visual prompting is likely to yield superior performance. The visual prompt is directly added to the resized image, resulting in the prompted image being expressed as:

\begin{equation}
    \mathcal{P}(\bx) = \text{Resize}_{L}(\bx) + \bB \cdot \bA, \quad \bx \in \mathcal{D},
    \label{Formula_our_vp}
\end{equation}

where $\text{Resize}_{L}(\cdot)$ resizes the image $\bx$ into a size of $L\times L$, and matrices $\bB$ and $\bA$ are the initialized visual prompt parameters, we utilize zero initialization of $\bB$ and a random Gaussian initialization of $\bA$ so $\bB \cdot \bA$ is zero at the beginning of training.

Utilizing a rank $r = 4$ in $\bB$ and $\bA$, we conduct experiments using the same configurations as Section \ref{Section_preliminary_study}. From the experimental results shown in Figure \ref{Figure_preliminary_investigation}, we can observe that our \textbf{Lo}w-\textbf{R}ank matrices multiplication \textbf{V}isual \textbf{P}rompting (\textbf{\ours}) achieve the best performance among all designs, further validate our hypothesis in Section \ref{Section_preliminary_study}. 

This method simplifies the visual prompting process compared to Pad Prompting, which involves complex resizing, padding, and mask manipulations. By employing low-rank matrices, we reduce the number of tunable parameters from $cL^2$ to $crL$, enhancing parameter efficiency significantly.

\begin{figure}[t]
    \centering
   \includegraphics[width=0.95\linewidth]{./images/method/lor_vp_new_00_4.pdf}
    \caption{\textbf{Our VP Design.} We resize the image to a resolution of $L\times L$ and initialize two low-rank matrices $\bB$ and $\bA$ as tunable parameters. $\bB \cdot \bA$ serves as the visual prompt and is directly added to the resized images. This design allows shared information in rows and columns and also allows patch-specific information in different patches.}
    \label{Figure_our_method}
\end{figure}

\subsection{Output Transformation}
The output of the pre-trained model $f$ on the prompted image $\mathcal{P}(\bx)$ remains in the source domain. To align these predictions with target labels in downstream tasks, we apply an output transformation, denoted as $\mathcal{M}$:

\begin{equation}
    \underset{\bdelta, \mathcal{M}}{\text{minimize}}\quad \mathbb{E}_{(\bx, y)\in \mathcal{D}}\mathcal{L}(\mathcal{M}(f(\mathcal{P}(\bx)), y),
    \label{Formula_vp_optimization}
\end{equation}

We utilize Linear Probing (LP) as our output transformation method, which adjusts the output features of the classifier head to suit downstream classes. This method proves more efficient than existing methods such as iterative label mapping (ILM) \citep{chen2023understanding}  and fully connected layer mapping (FM) \citep{tsao2024autovp} on large models and datasets. For instance, when using ImageNet-21K pre-trained Swin-B \citep{liu2021swin} and tuning on ImageNet-1K, ILM costs too much time and GPU storage to calculate and store the mapping sequences (a $21,841\times 1,000$ matrix) and AutoVP also achieves inferior performance due to the ineffectiveness to learn a $21,841\times 1,000$ full connected layer (see Figure \ref{Figure_performance_imagenet21k_pretrained_models} for performance details).

\section{Experiments}
To assess the effectiveness and efficiency of our novel visual prompting method, we adopt the most widely used evaluation protocol for VPs, i.e., evaluating models pre-trained on large datasets across various visual domains. Unlike previous works such as ILM-VP \citep{chen2023understanding} and AutoVP \citep{tsao2024autovp}, which primarily utilize ImageNet-1K \citep{deng2009imagenet} pre-trained models and fine-tune on smaller downstream datasets, we extend our exploration to larger pre-training datasets, such as ImageNet-21K \citep{deng2009imagenet}, as well as larger downstream datasets, including ImageNet-1K, to examine the scalability of existing VP methods. Furthermore, we conduct extensive empirical evaluations, focusing on the following aspects: (1) Demonstrating the superior performance and faster convergence of \ours across different datasets and architectures; (2) Investigating the out-of-distribution robustness of \ours; (3) Showcasing the efficiency of \ours in terms of training epochs, runtime, and parameter usage, etc;  (4) Performing ablation studies to evaluate the effectiveness of our VP approach under various label mapping methods, the optimal rank configuration in \ours, and the contribution of different components within \ours.

\subsection{Implementation Details}\label{Section_details}
\paragraph{Datasets.} For pre-training, we utilize the ImageNet-1K dataset \citep{deng2009imagenet}, which contains 1K classes and 1.3M images, the ImageNet-21K-P dataset \citep{ridnik1imagenet}, comprising 11K classes and 12M images, and the ImageNet-21K dataset \citep{deng2009imagenet}, which includes 21K classes and 14M images. We evaluate the effectiveness and efficiency of \ours across four downstream datasets: ImageNet-1K, Tiny-ImageNet \citep{le2015tiny}, and CIFAR-10/100 \citep{krizhevsky2009learning}. To assess the out-of-distribution robustness of \ours, we conduct experiments on ImageNet-R \citep{hendrycks2021many}, ImageNet-Sketch \citep{wang2019learning}, ImageNet-A \citep{hendrycks2021natural}, and ImageNet-V2 \citep{recht2019imagenet}. Additional details about the datasets are in Table \ref{Table_dataset_info}.

\paragraph{Networks.} We employ six architectures for our experiments, all of which operate at a resolution of $224 \times 224$. ($1$) ResNet-18 and ResNet-50 \citep{he2016deep} pre-trained on ImageNet-1K, and ViT-B/32 \citep{dosovitskiy2020image} pre-trained on ImageNet-21K and fine-tuned on ImageNet-1K, each with a classifier head of 1000 classes; ($2$) ResNet-50-P and ViT-B/16-P \citep{dosovitskiy2020image}, pre-trained on ImageNet-21K-P, with classifier heads for 11,221 classes; ($3$) Swin-B \citep{liu2021swin}, pre-trained on ImageNet-21K, with a classifier head for 21,841 classes; ($4$) CLIP \citep{radford2021learning}, a vision-language model that uses a ViT-B/32 architecture as its vision encoder. The weights for these models are all publicly available through the official PyTorch Model Zoo\footnote{\href{https://pytorch.org/vision/stable/models.html}{https://pytorch.org/vision/stable/models.html}} or the Hugging Face Timm Library\footnote{\href{https://huggingface.co/models?library=timm}{https://huggingface.co/models?library=timm}}. Further details of the network architectures can be found in Table \ref{Table_network_info}.

\paragraph{Baselines.} We select four representative SOTA methods as our baselines: ($1$) \textit{CLIP-VP} \citep{bahng2022exploring}, which extends prompt tuning to the computer vision domain by incorporating prompt parameters directly into input images using CLIP models; ($2$) \textit{ILM-VP} \citep{chen2023understanding}, which explores the impact of frequency-based label mapping (FLM) in visual prompting and introduces iterative label mapping (ILM) for improved performance; ($3$) \textit{AutoVP} \citep{tsao2024autovp}, a SOTA method in visual prompting that automates the selection of VP configurations, including prompt sizes and label mapping (LM) strategies—our experiments use the optimal configuration provided by AutoVP; ($4$) \textit{LP}, which modifies the classifier head of the pre-trained model to adapt to downstream tasks, a commonly used technique in transfer learning, serving as a baseline akin to \ours without the novel VPs introduced in our approach.

\paragraph{Training and Evaluation.} The results for the baseline methods, including CLIP-VP, ILM-VP, and AutoVP, are reproduced using the same configurations as described in their respective original papers. For \ours, we resize all input images to $224 \times 224$ and use a rank of $4$ in our VP design. As a result, the two sets of parameters in \ours have dimensions of $3 \times 224 \times 4$ and $3 \times 4 \times 224$, respectively, meaning that the total number of parameters in the visual prompts is only 5K. The optimal hyperparameters for \ours are determined through grid search. All experiments are conducted on NVIDIA Quadro RTX8000 GPUs with 48GB of memory. Additional implementation details for \ours are provided in Table~\ref{Table_implementation_details}.

\subsection{Main Results}
\paragraph{Performance of ImageNet-1K and CLIP Pre-trained Models.} To demonstrate the effectiveness of \ours on widely used ImageNet-1K pre-trained models, we evaluate its performance across several downstream datasets using ImageNet-21K pre-trained ViT-B/32 (fine-tuned on ImageNet-1K), ImageNet-1K pre-trained ResNet-18 and ResNet-50, as well as CLIP models. As shown in Figure \ref{Figure_performance_imagenet1k_pretrained_models}, we can observe that: \ding{182} \ours consistently outperforms all baselines across all network and dataset combinations, achieving an average improvement of $3.2\%$ and $2.1\%$ over AutoVP and LP, respectively. \ding{183} \ours converges significantly faster than the baselines, reaching optimal performance with $5\times$ fewer training epochs than AutoVP and $10\times$ fewer than ILM-VP.

\begin{figure}[!ht]
  \centering
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/clip_results/clip_tiny.jpg}
  \end{subfigure}
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/clip_results/clip_cifar100.jpg}
  \end{subfigure}
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/main_results/vit_b_32_1k_tiny_imagenet.pdf}
  \end{subfigure}
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/main_results/vit_b_32_1k_cifar100.pdf}
  \end{subfigure}

  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/main_results/resnet50_tiny_imagenet.pdf}
  \end{subfigure}
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/main_results/resnet50_cifar10.pdf}
  \end{subfigure}
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/main_results/resnet18_tiny_imagenet.pdf}
  \end{subfigure}
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/main_results/resnet18_cifar10.pdf}
  \end{subfigure}
  \begin{subfigure}{0.75\textwidth}
    \includegraphics[width=\linewidth]{images/main_results/legend.pdf}
  \end{subfigure}
    \caption{\textbf{Performance of ImageNet-1K and CLIP Pre-trained Models on Downstream Datasets.} Overview of the performance of \ours compared to four baseline methods. The final performance of each method is indicated by $\bigstar$ or $\bullet$, and all results are averaged over three runs. \ours consistently outperforms all baselines across various models and datasets.}
  \label{Figure_performance_imagenet1k_pretrained_models}
\end{figure}

\paragraph{Performance of ImageNet-21K Pre-trained Models.} To further evaluate the effectiveness of \ours and existing visual prompting methods on larger models and datasets with a greater number of classifier classes, we conduct experiments using ImageNet-21K-P pre-trained ResNet-50-P and ViT-B/16-P, as well as ImageNet-21K pre-trained Swin-B models, tuning them on ImageNet-1K and Tiny-ImageNet. These models have significantly more classifier output features compared to ImageNet-1K pre-trained models, providing additional evidence of the effectiveness of \ours and other VP methods on large-scale models and datasets. For the ImageNet-1K experiments, we focus on the strongest baselines, such as AutoVP and LP, running them for 30 epochs in line with the implementation in \citet{liu2021swin}, due to resource constraints. We found it challenging to run ILM-VP on our GPUs, as the ILM process is computationally expensive in terms of both training time and GPU memory. The results of the experiments are presented in Figure \ref{Figure_performance_imagenet21k_pretrained_models}, where we observe the following: \ding{182} \ours consistently achieves the best performance across large models and datasets, outperforming all baselines. \ding{183} The performance gap between \ours and AutoVP increases to $5.06$ on ImageNet-1K. A likely explanation is that the full mapping (FM) method used in AutoVP is less effective in this scenario, as it struggles to efficiently train a fully connected layer with $21,841$ input features and $1,000$ output features.

\begin{figure}[!ht]
  \centering
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/main_results_21k/vit_b_16_21k_tiny_imagenet.pdf}
  \end{subfigure}
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/main_results_21k/resnet50_21k_tiny_imagenet.pdf}
  \end{subfigure}
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/main_results_21k/swin_tiny_imagenet.pdf}
  \end{subfigure}
  \begin{subfigure}{0.24\textwidth}
    \includegraphics[width=\linewidth]{images/main_results_21k/swin_b_imagenet.pdf}
  \end{subfigure}
  \begin{subfigure}{0.6\textwidth}
    \includegraphics[width=\linewidth]{images/main_results_21k/legend.pdf}
  \end{subfigure}
    \caption{\textbf{Performance of ImageNet-21K Pre-trained Models on ImageNet-1K and Tiny-ImageNet.} Performance comparison of \ours and four baseline methods. The models are pre-trained on either ImageNet-21K-P or ImageNet-21K and then tuned on the respective downstream datasets. The final performance results are denoted by $\bigstar$ or $\bullet$. All results are averaged over three runs. \ours consistently outperforms all baselines across different models and datasets.}
  \label{Figure_performance_imagenet21k_pretrained_models}
\end{figure}



\subsection{Robustness of \ours}
To investigate the out-of-distribution robustness of \ours and explore its potential for enhancing real-world applications, we conduct experiments using ImageNet-21K pre-trained Swin-B. We apply \ours on ImageNet-1K and then evaluate the performance of the resulting model and visual prompts on four out-of-distribution datasets. The performance of \ours and the two strongest baselines are presented in Table \ref{Table_out_of_distribution_performance}. We find that \ours consistently demonstrates the best out-of-distribution robustness and generalization performance across all baselines, achieving an average improvement of $10.6$ over AutoVP across the four datasets. These results further highlight the superior out-of-distribution robustness of \ours, confirming its advantage over SOTA prompting methods in terms of generalization.

\begin{table}[htbp]
    \centering
    \caption{\textbf{Out-of-Distribution Generalization Performance.} Evaluation of the out-of-distribution generalization performance using the ImageNet-21K pre-trained Swin-B, with visual prompting applied on ImageNet-1K, and tested across four out-of-distribution datasets.}
    \label{Table_out_of_distribution_performance}
    \resizebox{0.95\textwidth}{!}{
    % \begin{tabular}{c|c|cccc}
    \begin{tabular}{l|c|cccc}
    \toprule
    \multirow{2}{*}{Method} & Source & \multicolumn{4}{c}{Target} \\ 
    % \cmidrule{2-6}
    \cmidrule(lr){2-2}\cmidrule(lr){3-6}
     & ImageNet-1K & ImageNet-R & ImageNet-Sketch & ImageNet-A & ImageNet-V2 \\
    \midrule
    AutoVP \textcolor{gray}{[ICLR24]} & 78.98 & 38.14 & 28.89 & 17.91 & 67.38 \\
    LP & 83.52 & 51.48 & 40.07 & 27.54 & 71.86 \\
    \midrule
    \ours & \textbf{84.04} & \textbf{52.27} & \textbf{41.13} & \textbf{27.89} & \textbf{72.38} \\
    \bottomrule
    \end{tabular}}
\end{table}

\subsection{Effieiency of \ours}

As shown in Figures \ref{Figure_performance_imagenet1k_pretrained_models} and \ref{Figure_performance_imagenet21k_pretrained_models}, \ours achieves superior performance compared to SOTA methods with fewer training epochs. To further examine the efficiency of \ours in contrast to the baselines, we assess its performance using several criteria: training epochs, training time, tunable parameters (including visual prompt parameters), GPU memory usage during training, and inference latency. Evaluations are conducted using an ImageNet-21K-P pre-trained ViT-B/16-P with visual prompting applied on Tiny-ImageNet, and an ImageNet-1K pre-trained ResNet-18 with visual prompting applied on CIFAR-10. The results are presented in Table \ref{Table_efficiency}, we can observe that: \ding{182} \ours converges the fastest among all methods, requiring $5\times$ fewer epochs and $6\times$ less training time compared to AutoVP, and $10\times$ fewer epochs and $15\times$ less training time compared to ILM-VP. ILM-VP, in particular, converges the slowest and incurs the highest time cost, as it requires an additional epoch for every training epoch to compute the LM sequences. \ding{183} \ours is highly parameter-efficient. For small models and datasets, such as ResNet-18 and CIFAR-10, \ours only requires 10K parameters to achieve optimal performance, which is $15\times$ and $11\times$ fewer than ILM-VP and AutoVP, respectively. Notably, \ours requires just 5K visual prompt parameters, which is $18\times$ and $30\times$ fewer than AutoVP and ILM-VP, on average. \ding{184} GPU usage and inference speed for \ours are comparable to AutoVP, whereas ILM-VP consumes the most GPU memory on larger models due to the additional computation and storage required for LM sequences. \ding{185} \ours achieves the best performance with the fewest visual prompt parameters and the shortest training time, making it an ideal choice for adapting pre-trained vision models to downstream tasks, particularly for resource-constrained environments such as mobile devices.

\begin{table}[htbp]
    \centering
    \caption{\textbf{Training and Inference Efficiency.} Comparison of the training and inference efficiency of \ours, AutoVP, and ILM-VP, evaluated using ImageNet-21K-P pre-trained ViT-B/16-P on Tiny-ImageNet and ImageNet-1K pre-trained ResNet-18 on CIFAR-10.}
    \label{Table_efficiency}
    \resizebox{1\textwidth}{!}{
    % \begin{tabular}{ll|c|cccccc|c}
    \begin{tabular}{ll|c|ccccccc}
    \toprule
    Network & Dataset & Method & Epochs & Time & \# VP Params & \# Tunable Params & GPU Usage & Latency & Accuracy \\ 
    \midrule
    \multirow{3}{*}{ResNet-18} & \multirow{3}{*}{CIFAR10} & ILM-VP\textcolor{gray}{[CVPR23]} &  200 & 5.76h & 147K & 147K & 4.51GB & 4.61ms & 66.39  \\
    & & AutoVP\textcolor{gray}{[ICLR24]} & 100 & 2.61h & 101K & 111K & 4.51GB & 4.59ms & 87.55 \\
    & & \ours & \textbf{20} & \textbf{0.50h} & \textbf{5K} & \textbf{10K} & \textbf{4.49GB} & \textbf{4.53ms} & \textbf{88.64}\\
    \midrule
    \multirow{3}{*}{ViT-B/16-P} & \multirow{3}{*}{Tiny-ImageNet} & ILM-VP\textcolor{gray}{[CVPR23]} &  200 & 25.55h & 147K & \textbf{147K} & 17.24GB & 14.55ms & 26.17  \\
    & & AutoVP\textcolor{gray}{[ICLR24]} & 100 & 8.08h & 74K & 2,318K & 13.59GB & 14.40ms & 81.42 \\
    & & \ours & \textbf{20} & \textbf{1.32h} & \textbf{5K} & 159K & \textbf{13.34GB} & \textbf{14.29ms} & \textbf{84.40}\\
    \bottomrule
    \end{tabular}}
\end{table}


\subsection{Albation Studies}

\paragraph{How does Output Transformation Impact \ours's Performance?} To further explore how different output transformations affect the performance of \ours, we conduct experiments by combining \ours with FLM, ILM, and FM, referred to as \ours w. FLM, \ours w. ILM, and \ours w. FM, respectively. These experiments are performed using ImageNet-21K pre-trained Swin-B, ImageNet-21K-P pre-trained ViT-B/16-P, ImageNet-21K pre-trained ViT-B/32 (fine-tuned on ImageNet-1K), and ImageNet-1K pre-trained ResNet-18 on CIFAR-100 and Tiny-ImageNet. The results are presented in Table \ref{Table_lm_impact}, where we observe the following: \ding{182} \ours with LP as the output transformation achieves the overall best performance across all methods, networks, and datasets. \ding{183} Even when using the same output transformations as ILM-VP and AutoVP, \ours consistently outperforms these methods, further demonstrating the superiority of our visual prompt design.

\begin{table}[htbp]
    \centering
    \caption{\textbf{The Impact of Output Transformation.} The performance comparison of utilizing FLM, ILM, and FM as the output transformation of \ours and the baselines. \ours achieves the overall best performance among all output transformation methods, networks, and datasets.}
    \label{Table_lm_impact}
    \resizebox{1\textwidth}{!}{
    % \begin{tabular}{c|c|cccc}
    \begin{tabular}{c|c|c|cccc}
    \toprule
    \multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multirow{2}{*}{Output Transformation} & \multicolumn{4}{c}{Network} \\ 
    % \cmidrule{2-6}
    \cmidrule(lr){4-7}
     & & & Swin-B & ViT-B/16-P & ViT-B/32 & ResNet-18 \\
    \midrule
    \multirow{7}{*}{Tiny-ImageNet}
    & ILM-VP\textcolor{gray}{[CVPR23]} & ILM & 56.28 & 26.17 & 32.58 & 14.13 \\
    & AutoVP\textcolor{gray}{[ICLR24]}  & FM & 84.81 & 81.42 & 82.43 & 59.68 \\
    & LP & LP & 86.54 & 82.75 & 83.95 & 65.17 \\
    \cmidrule{2-7}
    & \ours w. FLM & FLM & 82.15 & 41.76 & 82.89 & 57.45 \\
    & \ours w. ILM  & ILM & 84.85 & 43.50 & 84.86 & 62.20 \\
    & \ours w. FM & FM & 85.59 & 83.15 & \textbf{86.03} & 65.63  \\
    & \ours & LP & \textbf{88.28} & \textbf{84.40} & 85.85 & \textbf{68.28}  \\
    \midrule
    \multirow{7}{*}{CIFAR100}
    & ILM-VP\textcolor{gray}{[CVPR23]} & ILM & 65.78 & 41.49 & 40.10 & 25.36 \\
    & AutoVP\textcolor{gray}{[ICLR24]} & FM  & 86.83 & 88.58 & 85.96 & 63.77 \\
    & LP & LP & 87.37 & 88.90 & 86.21 & 67.06 \\
    \cmidrule{2-7}
    & \ours w. FLM  & FLM & 74.08 & 46.35 & 72.81 & 34.58 \\
    & \ours w. ILM  & ILM & 77.22 & 48.53 & 78.23 & 39.07  \\
    & \ours w. FM & FM & 86.25 & 89.10 & 88.48 & 68.64   \\
    & \ours & LP & \textbf{90.42} & \textbf{89.69} & \textbf{88.65} & \textbf{69.88}  \\
    \bottomrule
    \end{tabular}}
\end{table}

\paragraph{What is the optimal rank in \ours?} To provide deeper insights into the optimal rank selection in \ours, we conduct experiments with various configurations: \ours, \ours combined with ILM as the output transformation named as \ours w. ILM, and \ours combined with FM as the output transformation named as \ours w. FM. These experiments are performed using the ImageNet-21K-P pre-trained ViT-B/16-P on Tiny-ImageNet and the ImageNet-21K pre-trained ViT-B/32 (fine-tuned on ImageNet-1K) on CIFAR-100. The results are presented in Figure \ref{Figure_rank_impact}, from which we can derive the following observations: \ding{182} For output transformations such as LP and FM, the optimal rank is 4; increasing the rank beyond 4 does not yield any further performance improvements. \ding{183} When \ours is combined with ILM, the optimal rank is around 16. A plausible explanation is that ILM lacks tunable parameters, so a higher rank is needed to enhance the expressive power of the visual prompts and achieve optimal performance.

\paragraph{Ablation of Components in \ours.} We perform ablation studies on the two key components of \ours: the low-rank VP design and the linear probing output transformation. For experiments without label mapping, we apply FLM prior to training and keep this mapping sequence fixed during visual prompt training to ensure valid results. We conduct these experiments using the ImageNet-21K-P pre-trained ViT-B/16-P, ImageNet-21K pre-trained Swin-B, and ImageNet-1K pre-trained ResNet-18 on Tiny-ImageNet. The results, shown in Table \ref{Table_ablation_components}, reveal the following: \ding{182} \ours achieves the highest performance when both the low-rank VP and the output transformation are employed, demonstrating the effectiveness of these components in \ours. \ding{183} Our VP design improves model performance, regardless of whether a fixed mapping sequence or linear probing is used.

\begin{table}[htbp]
    \centering
    \begin{minipage}{0.51\textwidth}
    \centering
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=\linewidth]{images/ablations/rank/rank_vit_b_32_1k.pdf}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
      \includegraphics[width=\linewidth]{images/ablations/rank/rank_vit_b_16_21k.pdf}
    \end{subfigure}
    \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/ablations/rank/legend.pdf}
  \end{subfigure}
  \captionof{figure}{The Impact of Rank in \ours.}
  \label{Figure_rank_impact}
  \end{minipage}%
    \hfill
    \begin{minipage}{0.47\textwidth}
        \centering
        \caption{Ablation of Components in \ours. The results of using ImageNet-21K-P pre-trained ViT-B/16-P, ImageNet-21K pre-trained Swin-B, and ImageNet-1K pre-train ResNet-18 on Tiny-ImageNet.}
        \label{Table_ablation_components}
        \renewcommand{\arraystretch}{1.1}
        \resizebox{\textwidth}{!}{
        % \begin{tabular}{cc|cc}
        \setlength{\tabcolsep}{2pt}
        \begin{tabular}{ccccc}
        \toprule
        \multicolumn{2}{c}{Components} & \multicolumn{3}{c}{Network} \\
        % \midrule 
        \cmidrule(lr){1-2}\cmidrule(lr){3-5}
        VP & Output Trans. & ViT-B/16-P  & Swin-B & ResNet-18 \\
        \midrule
        \xmark & \xmark & 36.09 & 80.61 & 40.79 \\
        \cmark & \xmark & 41.76 & 82.15 & 57.45 \\
        \xmark & \cmark & 82.75 & 86.54 & 65.17 \\
        \cmark & \cmark & 84.40 & 88.28 & 68.28 \\
        \bottomrule
        \end{tabular}}
    \end{minipage}
\end{table}

\section{Conclusion}
Visual prompting has emerged as a powerful technique for adapting pre-trained models to specific tasks through parameter-efficient tuning. Traditional methods, however, often restrict the interaction between visual prompts and the original image to a limited number of patches, overlooking the potential benefits of shared information across different patches. Addressing these shortcomings, our study introduces a novel approach, termed Low-Rank Matrix Multiplication for Visual Prompting (\ours), which facilitates both shared and patch-specific information dissemination throughout the image. Extensive experiments over seven networks and eight datasets consistently demonstrate the effectiveness and efficiency of our method.

\section{Acknowledgements}
This research is partially funded by research grants to Metaxas from NSF: 2310966, 2235405, 2212301, 2003874, 1951890, AFOSR 23RT0630, and NIH 2R01HL127661.

\section{Reproducibility Statement}

The authors have made an extensive effort to ensure the reproducibility of the results presented in the paper. \textit{First}, the details of the experimental settings are provided Section \ref{Section_details} and in the Appendix~\ref{Appendix_details}. This paper investigates eight datasets, and the details about each dataset are described in Table~\ref{Table_dataset_info}. The evaluation metrics are also clearly introduced in Section~\ref{Section_details}. \textit{Second}, the baseline methods' implementation particulars are elucidated in Section~\ref{Section_details}. Simultaneously, the implementation details of our method, \ours, are included in Section~\ref{Section_details} and Appendix~\ref{Appendix_details}. \textit{Third}, the codes are included in the supplementary material for further reference.

\newpage

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage

\appendix
\section{Implementation Details}\label{Appendix_details}

\begin{table}[H]
\centering
\caption{Dataset Information.}
\label{Table_dataset_info}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
Dataset & Original Resolution & \# Training Set Images & \# Test Set Images & \# Classes  \\ 
\midrule
ImageNet-21K \citep{deng2009imagenet} & Varies & 14M & - & 21,843 \\
ImageNet-21K-P \citep{ridnik1imagenet} & $224\times 224$ & 12M & 0.6M & 11,221 \\
ImageNet-1K \citep{deng2009imagenet} & Varies & 1.3M & 50K & 1,000 \\
ImageNet-R \citep{hendrycks2021many} & Varies & - & 30K & 200 \\
ImageNet-Sketch \citep{wang2019learning} & Varies & - & 50K & 1,000 \\
ImageNet-A \citep{hendrycks2021many} & Varies & - & 7.5K & 1,000 \\
ImageNet-V2 \citep{recht2019imagenet} & Varies & - & 10K & 1,000 \\
Tiny-ImageNet \citep{le2015tiny} & $64\times 64$ & 100K & 10K & 200 \\
CIFAR100 \citep{krizhevsky2009learning} & $32\times 32$ & 50K & 10K & 100 \\
CIFAR10 \citep{krizhevsky2009learning} & $32\times 32$ & 50K & 10K & 10 \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[H]
\centering
\caption{Network Information.}
\label{Table_network_info}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
Network & Pre-trained Dataset & \# Model Params & Resolution & Classifier Head Input Features & Classifier Head Output Features \\ 
\midrule
ResNet-18 \citep{he2016deep} & ImageNet-1K & 12M & $224\times 224$ & 512 & 1,000 \\
ResNet-50 \citep{he2016deep} & ImageNet-1K & 26M & $224\times 224$ & 2,048 & 1,000 \\
ResNet-50-P \citep{he2016deep} & ImageNet-21K-P & 46M & $224\times 224$ & 2,048 & 11,221 \\
ViT-B/16-P \citep{dosovitskiy2020image} & ImageNet-21K-P & 94M & $224\times 224$ & 768 & 11,221 \\
ViT-B/16  \citep{dosovitskiy2020image} & ImageNet-21K, ImageNet-1K & 87M & $224\times 224$ & 768 & 1,000 \\
ViT-B/32  \citep{dosovitskiy2020image} & ImageNet-21K, ImageNet-1K & 88M & $224\times 224$ & 768 & 1,000 \\
Swin-B \citep{liu2021swin} & ImageNet-21K & 109M & $224\times 224$ & 1,024 & 21,841 \\
CLIP \citep{radford2021learning} & WebImageText & 86M  & $224\times 224$ & 512 &  - \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[H]
\centering
\caption{Implementation Details.}
\label{Table_implementation_details}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cccccccccc}
\toprule
Network & Pre-trained Data & Downstream Data & Resolution & Optimizer & LR & Label Mapping & \ours Rank & Epochs & Batch Size \\ 
\midrule
ResNet-18  & ImageNet-1K & CIFAR100 & $224\times 224$ & SGD & 0.02 & Linear Probing & 4 & 20 & 256 \\
ResNet-50  & ImageNet-1K & CIFAR100 & $224\times 224$ & SGD & 0.02 & Linear Probing & 4 & 20 & 256 \\
ViT-B/32  & ImageNet-21K, ImageNet-1K & CIFAR100 & $224\times 224$ & SGD & 0.02 & Linear Probing & 4 & 20 & 256 \\
ResNet-50-P & ImageNet-21K-P & Tiny-ImageNet & $224\times 224$ & SGD & 0.02 & Linear Probing & 4 & 20 & 256 \\
ViT-B/16-P  & ImageNet-21K-P & Tiny-ImageNet & $224\times 224$ & SGD & 0.02 & Linear Probing & 4 & 20 & 256 \\
Swin-B & ImageNet-21K & Tiny-ImageNet & $224\times 224$ & SGD & 0.02 & Linear Probing & 4 & 20 & 256 \\
CLIP & WebImageText & Tiny-ImageNet & $224\times 224$ & SGD & 40 & Linear Probing & 4 & 20 & 256 \\
Swin-B & ImageNet-21K & ImageNet-1K & $224\times 224$ & SGD & 0.01 & Linear Probing & 4 & 10 & 256 \\
\bottomrule
\end{tabular}}
\end{table}

\section{Additional Investigation}

\paragraph{Visual Prompting in Object Detection and Semantic Segmentation.} 
In this paper, we primarily focus on image classification tasks, following previous works such as AutoVP and ILM-VP. To further explore the applicability of \ours to object detection and semantic segmentation tasks, we conduct experiments using YOLOv4 \citep{bochkovskiy2020yolov4} for detection and DeepLabv3+ \citep{chen2018encoder} for segmentation. Both models use ImageNet-1K pre-trained ResNet-50 as the backbone. Hyperparameters such as the number of epochs and the rank in \ours are kept consistent with those used in classification tasks.

For object detection, we train on the Pascal VOC 2012 and 2007 training sets and evaluate on the Pascal VOC 2007 test set, following the setup in \citet{he2020momentum}. The bounding box head is modified for output transformation, and a learning rate of 0.0001 is applied. For semantic segmentation, we train on the Pascal VOC 2012 training set and evaluate on its validation set, with the DeepLabv3+ head adapted for downstream segmentation and a learning rate of 0.01. The experimental results for detection are presented in Table~\ref{Table_object_detection}, while the segmentation results are shown in Table~\ref{Table_segmentation}. Our method, LoR-VP, outperforms AutoVP by nearly $4\%$ in $\text{AP}_{50}$ on VOC 2007 detection and by $1.1\%$ on VOC 2012 segmentation, demonstrating the effectiveness of \ours for object detection and semantic segmentation tasks.

\begin{table}[htbp]
    \centering
    \begin{minipage}{0.48\textwidth}
    \centering
      \renewcommand{\arraystretch}{1.1}
  \captionof{table}{Performance (AP, $\text{AP}_{50}$, and $\text{AP}_{75}$) for object detection using YOLOv4 with an ImageNet-1K pre-trained ResNet-50 backbone, evaluated on the Pascal VOC 2007 test set.}
  \label{Table_object_detection}
  \resizebox{0.85\textwidth}{!}{
    \begin{tabular}{l|ccc}
      \toprule
      Method & $\text{AP}$ & $\text{AP}_{50}$ & $\text{AP}_{75}$  \\
      \midrule
      LP & 42.87 & 75.25 & 47.74 \\
      AutoVP & 41.72 & 73.07 & 44.85  \\
      \midrule
      \ours & \textbf{43.21} & \textbf{77.02} & \textbf{48.07} \\
      \bottomrule
    \end{tabular}
  }
    \end{minipage}%
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \caption{Performance (mIOU) for semantic segmentation using DeepLabv3+ with an ImageNet-1K pre-trained ResNet-50 backbone, evaluated on Pascal VOC 2012 validation set.}
        \label{Table_segmentation}
        \resizebox{0.53\textwidth}{!}{
        \begin{tabular}{l|c}
        \toprule
      Method & mIOU  \\
      \midrule
      LP & 67.82  \\
      AutoVP & 67.42  \\
      \midrule
      \ours & \textbf{68.55} \\
        \bottomrule
        \end{tabular}}
    \end{minipage}
\end{table}


\paragraph{Additional Investigation on Diverse Downstream Tasks.} 
To assess the performance of \ours across a broader range of classification tasks, including those involving natural and artificial objects, scenes, and textures, we conduct experiments on ten downstream datasets. These experiments use ViT-B/32 pre-trained on ImageNet-21K and fine-tuned on ImageNet-1K, following the methodologies of AutoVP and ILM-VP, to further evaluate the generalization and robustness of our approach. Although \ours primarily focuses on pixel-level visual prompt designs, we extend our comparison to include VPT-DEEP, as described by \citet{jia2022visual}, which modifies the transformer layers. This allows for a more comprehensive evaluation against additional baselines. The experimental results, presented in Table \ref{Table_vit_ten_datasets}, show that \ours achieves superior average performance across the ten datasets compared to VPT and AutoVP. Specifically, \ours improves performance by $1.4\%$ over AutoVP and $1.3\%$ over VPT, further demonstrating its effectiveness in diverse scenarios and against a wider range of baselines.

\begin{table}[ht]
\centering
\caption{Comparison of accuracy between \ours and four baseline methods using ViT-B/32 pre-trained on ImageNet-21K and fine-tuned on ImageNet-1K across ten datasets.}
\label{Table_vit_ten_datasets}
\renewcommand{\arraystretch}{1.5}
\resizebox{1\textwidth}{!}{
\begin{tabular}{c|cccccccccc|c}
\toprule
\textbf{Method} & \textbf{Tiny-ImageNet} & \textbf{EuroSAT} & \textbf{OxfordPets} & \textbf{Food101} & \textbf{DTD} & \textbf{Flowers102} & \textbf{CIFAR10} & \textbf{CIFAR100} & \textbf{SVHN} & \textbf{GTSRB} & \textbf{Average} \\
\toprule
LP & 83.95 & 95.67 & 91.90 & 82.18 & 69.83 & 97.98 & 96.51 & 86.21 & 83.09 & 83.21 & 87.05 \\ 
VPT \textcolor{gray}{[ECCV22]} & 83.54 & 95.90 & \textbf{92.27} & 82.29 & 72.11 & 98.47 & 96.02 & 86.22 & 81.48 & \textbf{88.29} & 87.66 \\ 
ILM-VP \textcolor{gray}{[CVPR23]} & 32.58 & 88.12 & 78.92 & 48.24 & 42.65 & 64.27 & 85.27 & 40.10 & 80.81 & 67.88 & 62.88 \\  
AutoVP \textcolor{gray}{[ICLR24]} & 82.43 & \textbf{96.25} & 92.12 & 82.86 & 70.81 & 98.42 & 95.45 & 85.96 & 85.24 & 86.39 & 87.59 \\ 
\midrule
\ours & \textbf{85.85} & \textbf{96.25} & 92.18 & \textbf{83.51} & \textbf{72.49} & \textbf{98.58} & \textbf{97.52} & \textbf{88.65} & \textbf{86.31} & 88.07 & \textbf{88.94} \\ 
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Additional comparison of \ours and AutoVP using LP and FM as output transformations, respectively.}
    \label{Table_autovp_lp}
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{c|c|c|cccc}
    \toprule
    \multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multirow{2}{*}{Output Transformation} & \multicolumn{4}{c}{Network} \\ 
    \cmidrule(lr){4-7}
     & & & Swin-B & ViT-B/16-P & ViT-B/32 & ResNet-18 \\
    \midrule
    \multirow{7}{*}{Tiny-ImageNet}
    & LP & LP & 86.54 & 82.75 & 83.95 & 65.17 \\
    & AutoVP\textcolor{gray}{[ICLR24]}  & FM & 84.81 & 81.42 & 82.43 & 59.68 \\
    & AutoVP w. LP\textcolor{gray}{[ICLR24]}  & LP & 86.45 & 82.92 & 83.31 & 65.58 \\
    \cmidrule{2-7}
    & \ours w. FM & FM & 85.59 & 83.15 & \textbf{86.03} & 65.63  \\
    & \ours & LP & \textbf{88.28} & \textbf{84.40} & 85.85 & \textbf{68.28}  \\
    \midrule
    \multirow{7}{*}{CIFAR100}
    & LP & LP & 87.37 & 88.90 & 86.21 & 67.06 \\
    & AutoVP\textcolor{gray}{[ICLR24]} & FM  & 86.83 & 88.58 & 85.96 & 63.77 \\
    & AutoVP w. LP\textcolor{gray}{[ICLR24]} & LP  & 88.70 & 89.34 & 87.00 & 68.10 \\
    \cmidrule{2-7}
    & \ours w. FM & FM & 86.25 & 89.10 & 88.48 & 68.64   \\
    & \ours & LP & \textbf{90.42} & \textbf{89.69} & \textbf{88.65} & \textbf{69.88}  \\
    \bottomrule
    \end{tabular}}
\end{table}


\paragraph{Additional Investigation of Output Transformation.} 
Table \ref{Table_lm_impact} presents the impact of output transformations on \ours, demonstrating that \ours outperforms baseline methods, such as AutoVP and ILM-VP, when using the same output transformations. These results validate the effectiveness of our visual prompt designs. To further examine the influence of different output transformations and reinforce the superiority of our approach, we conduct additional ablation studies comparing \ours and AutoVP using LP and FM as output transformations. The experiments are performed on the same architectures and datasets as those in Table \ref{Table_lm_impact}. The results, shown in Table \ref{Table_autovp_lp}, indicate that \ours consistently outperforms AutoVP with LP as the output transformation across all models and datasets. Interestingly, AutoVP with LP achieves higher performance than LP alone on CIFAR100 but performs comparably on Tiny-ImageNet. This variation may stem from the scaling factors employed in AutoVP, which likely affect visual prompting performance differently across datasets. Notably, \ours adopts a fixed visual prompt size of $224 \times 224$, simplifying its design by avoiding the need to account for scaling size, further underscoring its simplicity and adaptability.

\end{document}
