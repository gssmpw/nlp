
@inproceedings{dworak_performance_2019,
	title = {Performance of {LiDAR} object detection deep learning architectures based on artificially generated point cloud data from {CARLA} simulator},
	url = {https://ieeexplore.ieee.org/document/8864642},
	doi = {10.1109/MMAR.2019.8864642},
	abstract = {Training deep neural network algorithms for {LiDAR} based object detection for autonomous cars requires huge amount of labeled data. Both data collection and labeling requires a lot of effort, money and time. Therefore, the use of simulation software for virtual data generation environments is gaining wide interest from both researchers and engineers. The big question remains how well artificially generated data resembles the data gathered by real sensors and how the differences affects the final algorithms performance. The article is trying to make a quantitative answer to the above question. Selected state-of-the-art algorithms for {LiDAR} point cloud object detection were trained on both real and artificially generated data sets. Their performance on different test sets were evaluated. The main focus was to determinate how well artificially trained networks perform on real data and if combined train sets can achieve better results overall.},
	eventtitle = {2019 24th International Conference on Methods and Models in Automation and Robotics ({MMAR})},
	pages = {600--605},
	author = {Dworak, Daniel and Ciepiela, Filip and Derbisz, Jakub and Izzat, Izzat and Komorkiewicz, Mateusz and Wójcik, Mateusz},
	urldate = {2024-11-11},
	date = {2019-08},
	keywords = {{CARLA}, Databases, Feature extraction, {KITTI}, Laser radar, {LiDAR}, Object detection, Sensors, Three-dimensional displays, Training, artificial data, automotive, deep learning, object detection, point cloud, simulator},
}

@inproceedings{patel_simulation-based_2024,
	location = {Angers, France},
	title = {Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a {LiDAR} Point Cloud Dataset in a {SOTIF}-related Use Case:},
	isbn = {978-989-758-703-0},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0012707300003702},
	doi = {10.5220/0012707300003702},
	shorttitle = {Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a {LiDAR} Point Cloud Dataset in a {SOTIF}-related Use Case},
	eventtitle = {10th International Conference on Vehicle Technology and Intelligent Transport Systems},
	pages = {415--426},
	publisher = {{SCITEPRESS} - Science and Technology Publications},
	author = {Patel, Milin and Jung, Rolf},
	urldate = {2024-11-11},
	date = {2024},
	langid = {english},
}

@inproceedings{iglesias_analysis_2024,
	location = {Rome, Italy},
	title = {Analysis of Point Cloud Domain Gap Effects for 3D Object Detection Evaluation:},
	isbn = {978-989-758-679-8},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0012357200003660},
	doi = {10.5220/0012357200003660},
	shorttitle = {Analysis of Point Cloud Domain Gap Effects for 3D Object Detection Evaluation},
	eventtitle = {19th International Conference on Computer Vision Theory and Applications},
	pages = {278--285},
	publisher = {{SCITEPRESS} - Science and Technology Publications},
	author = {Iglesias, Aitor and García, Mikel and Aranjuelo, Nerea and Arganda-Carreras, Ignacio and Nieto, Marcos},
	urldate = {2024-11-11},
	date = {2024},
	langid = {english},
}

@article{phong_illumination_1975,
	title = {Illumination for computer generated pictures},
	volume = {18},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/360825.360839},
	doi = {10.1145/360825.360839},
	abstract = {The quality of computer generated images of three-dimensional scenes depends on the shading technique used to paint the objects on the cathode-ray tube screen. The shading algorithm itself depends in part on the method for modeling the object, which also determines the hidden surface algorithm. The various methods of object modeling, shading, and hidden surface removal are thus strongly interconnected. Several shading techniques corresponding to different methods of object modeling and the related hidden surface algorithms are presented here. Human visual perception and the fundamental laws of optics are considered in the development of a shading rule that provides better quality and increased realism in generated images.},
	pages = {311--317},
	number = {6},
	journaltitle = {Commun. {ACM}},
	author = {Phong, Bui Tuong},
	urldate = {2025-01-13},
	date = {1975-06-01},
}

@online{nvidia_drive_2024,
	title = {{DRIVE} Sim},
	url = {https://developer.nvidia.com/drive/simulation},
	abstract = {{NVIDIA} {DRIVE} Sim - Powered by Omniverse is an end-to-end simulation platform, architected from the ground up to run large-scale, physically accurate multi-sensor simulation. It's open, scalable, modular and supports {AV} development and validation from concept to deployment, improving developer productivity and accelerating time to market.},
	titleaddon = {{NVIDIA} Developer},
	author = {{NVIDIA}},
	urldate = {2024-11-12},
	date = {2024-12-11},
	langid = {american},
}

@software{nozarian_fnozariancarla-kitti_2024,
	title = {fnozarian/{CARLA}-{KITTI}},
	url = {https://github.com/fnozarian/CARLA-KITTI},
	abstract = {{CARLA}-{KITTI} generates synthetic data from the {CARLA} simulator for {KITTI} 2D/3D Object Detection task.},
	author = {Nozarian, Farzad},
	urldate = {2024-11-11},
	date = {2024-09-24},
	keywords = {autonomous-driving, carla-simulator, kitti-dataset, lidar, object-detection, point-cloud, sim2real},
}

@online{autonomousvision_kitti-360_2024,
	title = {{KITTI}-360 Annotation Tool},
	rights = {{MIT}},
	url = {https://github.com/autonomousvision/kitti360LabelTool},
	author = {autonomousvision},
	urldate = {2024-11-11},
	date = {2024-10-14},
}

@online{dotchen_carla_2020,
	title = {{CARLA} Issue regarding stability},
	url = {https://github.com/carla-simulator/carla/issues/2789},
	abstract = {Hello, In the lastest version (0.9.9), client.get\_trafficmanager() will hang if the world is reloaded and a traffic manager has been created with sync mode before. Am I missing something? A minimal...},
	titleaddon = {{GitHub}},
	author = {dotchen},
	urldate = {2025-01-10},
	date = {2020-04-27},
	langid = {english},
}

@online{promitzer_carla_2023,
	title = {{CARLA} {GitHub} Bounding Box Workaround},
	url = {https://github.com/carla-simulator/carla/issues/7007},
	abstract = {Carla version: 0.9.14 and 0.9.15 {OS}: Ubuntu 20.04 and 22.04 The bounding box size is not right, if we use the Python {API} to get the bounding\_box of the objects. We are getting the bounding box with...},
	titleaddon = {{GitHub}},
	author = {Promitzer, Patrick},
	urldate = {2024-11-13},
	date = {2023-12-15},
	langid = {english},
}

@misc{velodyne_velodyne_2009,
	title = {Velodyne {HDL}-64E Data Sheet},
	url = {https://hypertech.co.il/wp-content/uploads/2015/12/HDL-64E-Data-Sheet.pdf},
	author = {Velodyne},
	urldate = {2025-01-10},
	date = {2009},
}

@misc{zhang_uni3d_2023,
	title = {Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection},
	url = {http://arxiv.org/abs/2303.06880},
	doi = {10.48550/arXiv.2303.06880},
	shorttitle = {Uni3D},
	abstract = {Current 3D object detection models follow a single dataset-specific training and testing paradigm, which often faces a serious detection accuracy drop when they are directly deployed in another dataset. In this paper, we study the task of training a unified 3D detector from multiple datasets. We observe that this appears to be a challenging task, which is mainly due to that these datasets present substantial data-level differences and taxonomy-level variations caused by different {LiDAR} types and data acquisition standards. Inspired by such observation, we present a Uni3D which leverages a simple data-level correction operation and a designed semantic-level coupling-and-recoupling module to alleviate the unavoidable data-level and taxonomy-level differences, respectively. Our method is simple and easily combined with many 3D object detection baselines such as {PV}-{RCNN} and Voxel-{RCNN}, enabling them to effectively learn from multiple off-the-shelf 3D datasets to obtain more discriminative and generalizable representations. Experiments are conducted on many dataset consolidation settings including Waymo-{nuScenes}, {nuScenes}-{KITTI}, Waymo-{KITTI}, and Waymo-{nuScenes}-{KITTI} consolidations. Their results demonstrate that Uni3D exceeds a series of individual detectors trained on a single dataset, with a 1.04x parameter increase over a selected baseline detector. We expect this work will inspire the research of 3D generalization since it will push the limits of perceptual performance.},
	number = {{arXiv}:2303.06880},
	publisher = {{arXiv}},
	author = {Zhang, Bo and Yuan, Jiakang and Shi, Botian and Chen, Tao and Li, Yikang and Qiao, Yu},
	urldate = {2024-11-21},
	date = {2023-04-28},
	eprinttype = {arxiv},
	eprint = {2303.06880},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{team_openpcdet_2020,
	title = {{OpenPCDet}: An Open-source Toolbox for 3D Object Detection from Point Clouds},
	url = {https://github.com/open-mmlab/OpenPCDet},
	author = {Team, {OpenPCDet} Development},
	date = {2020},
}

@article{phong_illumination_1975-1,
	title = {Illumination for computer generated pictures},
	volume = {18},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/360825.360839},
	doi = {10.1145/360825.360839},
	abstract = {The quality of computer generated images of three-dimensional scenes depends on the shading technique used to paint the objects on the cathode-ray tube screen. The shading algorithm itself depends in part on the method for modeling the object, which also determines the hidden surface algorithm. The various methods of object modeling, shading, and hidden surface removal are thus strongly interconnected. Several shading techniques corresponding to different methods of object modeling and the related hidden surface algorithms are presented here. Human visual perception and the fundamental laws of optics are considered in the development of a shading rule that provides better quality and increased realism in generated images.},
	pages = {311--317},
	number = {6},
	journaltitle = {Commun. {ACM}},
	author = {Phong, Bui Tuong},
	urldate = {2024-11-20},
	date = {1975-06-01},
}

@online{noauthor_openpcdetdocsdemomd_nodate,
	title = {{OpenPCDet}/docs/{DEMO}.md at master · open-mmlab/{OpenPCDet}},
	url = {https://github.com/open-mmlab/OpenPCDet/blob/master/docs/DEMO.md},
	urldate = {2024-11-19},
}

@misc{gaidon_virtual_2016,
	title = {Virtual Worlds as Proxy for Multi-Object Tracking Analysis},
	url = {http://arxiv.org/abs/1605.06457},
	doi = {10.48550/arXiv.1605.06457},
	abstract = {Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual {KITTI} (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.},
	number = {{arXiv}:1605.06457},
	publisher = {{arXiv}},
	author = {Gaidon, Adrien and Wang, Qiao and Cabon, Yohann and Vig, Eleonora},
	urldate = {2024-11-19},
	date = {2016-05-20},
	eprinttype = {arxiv},
	eprint = {1605.06457},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{cabon_virtual_2020,
	title = {Virtual {KITTI} 2},
	url = {http://arxiv.org/abs/2001.10773},
	doi = {10.48550/arXiv.2001.10773},
	abstract = {This paper introduces an updated version of the well-known Virtual {KITTI} dataset which consists of 5 sequence clones from the {KITTI} tracking benchmark. In addition, the dataset provides different variants of these sequences such as modified weather conditions (e.g. fog, rain) or modified camera configurations (e.g. rotated by 15 degrees). For each sequence, we provide multiple sets of images containing {RGB}, depth, class segmentation, instance segmentation, flow, and scene flow data. Camera parameters and poses as well as vehicle locations are available as well. In order to showcase some of the dataset's capabilities, we ran multiple relevant experiments using state-of-the-art algorithms from the field of autonomous driving. The dataset is available for download at https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds.},
	number = {{arXiv}:2001.10773},
	publisher = {{arXiv}},
	author = {Cabon, Yohann and Murray, Naila and Humenberger, Martin},
	urldate = {2024-11-19},
	date = {2020-01-29},
	eprinttype = {arxiv},
	eprint = {2001.10773},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{yang_st3d_2021,
	title = {{ST}3D++: Denoised Self-training for Unsupervised Domain Adaptation on 3D Object Detection},
	url = {http://arxiv.org/abs/2108.06682},
	doi = {10.48550/arXiv.2108.06682},
	shorttitle = {{ST}3D++},
	abstract = {In this paper, we present a self-training method, named {ST}3D++, with a holistic pseudo label denoising pipeline for unsupervised domain adaptation on 3D object detection. {ST}3D++ aims at reducing noise in pseudo label generation as well as alleviating the negative impacts of noisy pseudo labels on model training. First, {ST}3D++ pre-trains the 3D object detector on the labeled source domain with random object scaling ({ROS}) which is designed to reduce target domain pseudo label noise arising from object scale bias of the source domain. Then, the detector is progressively improved through alternating between generating pseudo labels and training the object detector with pseudo-labeled target domain data. Here, we equip the pseudo label generation process with a hybrid quality-aware triplet memory to improve the quality and stability of generated pseudo labels. Meanwhile, in the model training stage, we propose a source data assisted training strategy and a curriculum data augmentation policy to effectively rectify noisy gradient directions and avoid model over-fitting to noisy pseudo labeled data. These specific designs enable the detector to be trained on meticulously refined pseudo labeled target data with denoised training signals, and thus effectively facilitate adapting an object detector to a target domain without requiring annotations. Finally, our method is assessed on four 3D benchmark datasets (i.e., Waymo, {KITTI}, Lyft, and {nuScenes}) for three common categories (i.e., car, pedestrian and bicycle). {ST}3D++ achieves state-of-the-art performance on all evaluated settings, outperforming the corresponding baseline by a large margin (e.g., 9.6\% \${\textbackslash}sim\$ 38.16\% on Waymo \${\textbackslash}rightarrow\$ {KITTI} in terms of {AP}\$\_\{{\textbackslash}text\{3D\}\}\$), and even surpasses the fully supervised oracle results on the {KITTI} 3D object detection benchmark with target prior. Code will be available.},
	number = {{arXiv}:2108.06682},
	publisher = {{arXiv}},
	author = {Yang, Jihan and Shi, Shaoshuai and Wang, Zhe and Li, Hongsheng and Qi, Xiaojuan},
	urldate = {2024-11-14},
	date = {2021-08-15},
	eprinttype = {arxiv},
	eprint = {2108.06682},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chen_revisiting_2023,
	location = {Paris, France},
	title = {Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350307184},
	url = {https://ieeexplore.ieee.org/document/10377781/},
	doi = {10.1109/ICCV51070.2023.00344},
	abstract = {Unsupervised domain adaptation ({DA}) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing {DA} methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel {ReDB} framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination ({CDE}) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g., scales and point densities), we design an overlapped boxes counting ({OBC}) metric that allows to uniformly downsample pseudo-labeled objects across different geometric characteristics. To confront the issue of inter-class imbalance, we progressively augment the target point clouds with a class-balanced set of pseudo-labeled target instances and source objects, which boosts recognition accuracies on both frequently appearing and rare classes. Experimental results on three benchmark datasets using both voxel-based (i.e., {SECOND}) and pointbased 3D detectors (i.e., {PointRCNN}) demonstrate that our proposed {ReDB} approach outperforms existing 3D domain adaptation methods by a large margin, improving 23.15\% {mAP} on the {nuScenes} → {KITTI} task. The code is available at https://github.com/zhuoxiao-chen/{ReDB}-{DA}-3Ddet.},
	eventtitle = {2023 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {3691--3703},
	booktitle = {2023 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Chen, Zhuoxiao and Luo, Yadan and Wang, Zheng and Baktashmotlagh, Mahsa and Huang, Zi},
	urldate = {2024-11-14},
	date = {2023-10-01},
	langid = {english},
}

@software{epic_games_unreal_2019,
	title = {Unreal Engine},
	url = {https://www.unrealengine.com},
	version = {4.22.1},
	author = {{Epic Games}},
	date = {2019-04-25},
}

@misc{yang_st3d_2021-1,
	title = {{ST}3D: Self-training for Unsupervised Domain Adaptation on 3D Object Detection},
	url = {http://arxiv.org/abs/2103.05346},
	doi = {10.48550/arXiv.2103.05346},
	shorttitle = {{ST}3D},
	abstract = {We present a new domain adaptive self-training pipeline, named {ST}3D, for unsupervised domain adaptation on 3D object detection from point clouds. First, we pre-train the 3D detector on the source domain with our proposed random object scaling strategy for mitigating the negative effects of source domain bias. Then, the detector is iteratively improved on the target domain by alternatively conducting two steps, which are the pseudo label updating with the developed quality-aware triplet memory bank and the model training with curriculum data augmentation. These specific designs for 3D object detection enable the detector to be trained with consistent and high-quality pseudo labels and to avoid overfitting to the large number of easy examples in pseudo labeled data. Our {ST}3D achieves state-of-the-art performance on all evaluated datasets and even surpasses fully supervised results on {KITTI} 3D object detection benchmark. Code will be available at https://github.com/{CVMI}-Lab/{ST}3D.},
	number = {{arXiv}:2103.05346},
	publisher = {{arXiv}},
	author = {Yang, Jihan and Shi, Shaoshuai and Wang, Zhe and Li, Hongsheng and Qi, Xiaojuan},
	urldate = {2024-11-12},
	date = {2021-03-27},
	eprinttype = {arxiv},
	eprint = {2103.05346},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{eskandar_empirical_2024,
	title = {An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains},
	url = {http://arxiv.org/abs/2402.17562},
	doi = {10.48550/arXiv.2402.17562},
	abstract = {3D Object Detectors (3D-{OD}) are crucial for understanding the environment in many robotic tasks, especially autonomous driving. Including 3D information via Lidar sensors improves accuracy greatly. However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications. There exist methods to adapt 3D-{ODs} to these domains; however, these methods treat 3D-{ODs} as a black box, neglecting underlying architectural decisions and source-domain training strategies. Instead, we dive deep into the details of 3D-{ODs}, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation. We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-{OD} robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies. We assess their impact on the robustness of nine state-of-the-art 3D-{ODs} across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location. Our main findings are: (1) transformer backbones with local point features are more robust than 3D {CNNs}, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data. We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-{ODs}.},
	number = {{arXiv}:2402.17562},
	publisher = {{arXiv}},
	author = {Eskandar, George and Zhang, Chongzhe and Kaushik, Abhishek and Guirguis, Karim and Sayed, Mohamed and Yang, Bin},
	urldate = {2024-11-12},
	date = {2024-02-27},
	eprinttype = {arxiv},
	eprint = {2402.17562},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sanchez_parisluco3d_2024,
	title = {{ParisLuco}3D: A high-quality target dataset for domain generalization of {LiDAR} perception},
	url = {http://arxiv.org/abs/2310.16542},
	doi = {10.48550/arXiv.2310.16542},
	shorttitle = {{ParisLuco}3D},
	abstract = {{LiDAR} is an essential sensor for autonomous driving by collecting precise geometric information regarding a scene. \%Exploiting this information for perception is interesting as the amount of available data increases. As the performance of various {LiDAR} perception tasks has improved, generalizations to new environments and sensors has emerged to test these optimized models in real-world conditions. This paper provides a novel dataset, {ParisLuco}3D, specifically designed for cross-domain evaluation to make it easier to evaluate the performance utilizing various source datasets. Alongside the dataset, online benchmarks for {LiDAR} semantic segmentation, {LiDAR} object detection, and {LiDAR} tracking are provided to ensure a fair comparison across methods. The {ParisLuco}3D dataset, evaluation scripts, and links to benchmarks can be found at the following website:https://npm3d.fr/parisluco3d},
	number = {{arXiv}:2310.16542},
	publisher = {{arXiv}},
	author = {Sanchez, Jules and Soum-Fontez, Louis and Deschaud, Jean-Emmanuel and Goulette, Francois},
	urldate = {2024-11-12},
	date = {2024-06-03},
	eprinttype = {arxiv},
	eprint = {2310.16542},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wang_train_2020,
	title = {Train in Germany, Test in The {USA}: Making 3D Object Detectors Generalize},
	url = {http://arxiv.org/abs/2005.08139},
	doi = {10.48550/arXiv.2005.08139},
	shorttitle = {Train in Germany, Test in The {USA}},
	abstract = {In the domain of autonomous driving, deep learning has substantially improved the 3D object detection accuracy for {LiDAR} and stereo camera data alike. While deep networks are great at generalization, they are also notorious to over-fit to all kinds of spurious artifacts, such as brightness, car sizes and models, that may appear consistently throughout the data. In fact, most datasets for autonomous driving are collected within a narrow subset of cities within one country, typically under similar weather conditions. In this paper we consider the task of adapting 3D object detectors from one dataset to another. We observe that naively, this appears to be a very challenging task, resulting in drastic drops in accuracy levels. We provide extensive experiments to investigate the true adaptation challenges and arrive at a surprising conclusion: the primary adaptation hurdle to overcome are differences in car sizes across geographic areas. A simple correction based on the average car size yields a strong correction of the adaptation gap. Our proposed method is simple and easily incorporated into most 3D object detection frameworks. It provides a first baseline for 3D object detection adaptation across countries, and gives hope that the underlying problem may be more within grasp than one may have hoped to believe. Our code is available at https://github.com/cxy1997/3D\_adapt\_auto\_driving.},
	number = {{arXiv}:2005.08139},
	publisher = {{arXiv}},
	author = {Wang, Yan and Chen, Xiangyu and You, Yurong and Erran, Li and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q. and Chao, Wei-Lun},
	urldate = {2024-11-12},
	date = {2020-05-17},
	eprinttype = {arxiv},
	eprint = {2005.08139},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{triess_realism_2022,
	title = {A Realism Metric for Generated {LiDAR} Point Clouds},
	url = {http://arxiv.org/abs/2208.14958},
	doi = {10.48550/arXiv.2208.14958},
	abstract = {A considerable amount of research is concerned with the generation of realistic sensor data. {LiDAR} point clouds are generated by complex simulations or learned generative models. The generated data is usually exploited to enable or improve downstream perception algorithms. Two major questions arise from these procedures: First, how to evaluate the realism of the generated data? Second, does more realistic data also lead to better perception performance? This paper addresses both questions and presents a novel metric to quantify the realism of {LiDAR} point clouds. Relevant features are learned from real-world and synthetic point clouds by training on a proxy classification task. In a series of experiments, we demonstrate the application of our metric to determine the realism of generated {LiDAR} data and compare the realism estimation of our metric to the performance of a segmentation model. We confirm that our metric provides an indication for the downstream segmentation performance.},
	number = {{arXiv}:2208.14958},
	publisher = {{arXiv}},
	author = {Triess, Larissa T. and Rist, Christoph B. and Peter, David and Zöllner, J. Marius},
	urldate = {2024-11-12},
	date = {2022-08-31},
	eprinttype = {arxiv},
	eprint = {2208.14958},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{sekkat_amodalsynthdrive_2024,
	title = {{AmodalSynthDrive}: A Synthetic Amodal Perception Dataset for Autonomous Driving},
	url = {http://arxiv.org/abs/2309.06547},
	doi = {10.48550/arXiv.2309.06547},
	shorttitle = {{AmodalSynthDrive}},
	abstract = {Unlike humans, who can effortlessly estimate the entirety of objects even when partially occluded, modern computer vision algorithms still find this aspect extremely challenging. Leveraging this amodal perception for autonomous driving remains largely untapped due to the lack of suitable datasets. The curation of these datasets is primarily hindered by significant annotation costs and mitigating annotator subjectivity in accurately labeling occluded regions. To address these limitations, we introduce {AmodalSynthDrive}, a synthetic multi-task multi-modal amodal perception dataset. The dataset provides multi-view camera images, 3D bounding boxes, {LiDAR} data, and odometry for 150 driving sequences with over 1M object annotations in diverse traffic, weather, and lighting conditions. {AmodalSynthDrive} supports multiple amodal scene understanding tasks including the introduced amodal depth estimation for enhanced spatial understanding. We evaluate several baselines for each of these tasks to illustrate the challenges and set up public benchmarking servers. The dataset is available at http://amodalsynthdrive.cs.uni-freiburg.de.},
	number = {{arXiv}:2309.06547},
	publisher = {{arXiv}},
	author = {Sekkat, Ahmed Rida and Mohan, Rohit and Sawade, Oliver and Matthes, Elmar and Valada, Abhinav},
	urldate = {2024-11-12},
	date = {2024-03-11},
	eprinttype = {arxiv},
	eprint = {2309.06547},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shi_points_2020,
	title = {From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network},
	url = {http://arxiv.org/abs/1907.03670},
	doi = {10.48550/arXiv.1907.03670},
	shorttitle = {From Points to Parts},
	abstract = {3D object detection from {LiDAR} point cloud is a challenging problem in 3D scene understanding and has many practical applications. In this paper, we extend our preliminary work {PointRCNN} to a novel and strong point-cloud-based 3D object detection framework, the part-aware and aggregation neural network (Part-\$A{\textasciicircum}2\$ net). The whole framework consists of the part-aware stage and the part-aggregation stage. Firstly, the part-aware stage for the first time fully utilizes free-of-charge part supervisions derived from 3D ground-truth boxes to simultaneously predict high quality 3D proposals and accurate intra-object part locations. The predicted intra-object part locations within the same proposal are grouped by our new-designed {RoI}-aware point cloud pooling module, which results in an effective representation to encode the geometry-specific features of each 3D proposal. Then the part-aggregation stage learns to re-score the box and refine the box location by exploring the spatial relationship of the pooled intra-object part locations. Extensive experiments are conducted to demonstrate the performance improvements from each component of our proposed framework. Our Part-\$A{\textasciicircum}2\$ net outperforms all existing 3D detection methods and achieves new state-of-the-art on {KITTI} 3D object detection dataset by utilizing only the {LiDAR} point cloud data. Code is available at https://github.com/sshaoshuai/{PointCloudDet}3D.},
	number = {{arXiv}:1907.03670},
	publisher = {{arXiv}},
	author = {Shi, Shaoshuai and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
	urldate = {2024-11-12},
	date = {2020-03-16},
	eprinttype = {arxiv},
	eprint = {1907.03670},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wu_text2lidar_2024,
	title = {Text2LiDAR: Text-guided {LiDAR} Point Cloud Generation via Equirectangular Transformer},
	url = {http://arxiv.org/abs/2407.19628},
	shorttitle = {Text2LiDAR},
	abstract = {The complex traffic environment and various weather conditions make the collection of {LiDAR} data expensive and challenging. Achieving high-quality and controllable {LiDAR} data generation is urgently needed, controlling with text is a common practice, but there is little research in this field. To this end, we propose Text2LiDAR, the first efficient, diverse, and text-controllable {LiDAR} data generation model. Specifically, we design an equirectangular transformer architecture, utilizing the designed equirectangular attention to capture {LiDAR} features in a manner with data characteristics. Then, we design a control-signal embedding injector to efficiently integrate control signals through the global-to-focused attention mechanism. Additionally, we devise a frequency modulator to assist the model in recovering high-frequency details, ensuring the clarity of the generated point cloud. To foster development in the field and optimize text-controlled generation performance, we construct {nuLiDARtext} which offers diverse text descriptors for 34,149 {LiDAR} point clouds from 850 scenes. Experiments on uncontrolled and text-controlled generation in various forms on {KITTI}-360 and {nuScenes} datasets demonstrate the superiority of our approach.},
	number = {{arXiv}:2407.19628},
	publisher = {{arXiv}},
	author = {Wu, Yang and Zhang, Kaihua and Qian, Jianjun and Xie, Jin and Yang, Jian},
	urldate = {2024-11-12},
	date = {2024-07-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2407.19628 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ku_joint_2018,
	title = {Joint 3D Proposal Generation and Object Detection from View Aggregation},
	url = {http://arxiv.org/abs/1712.02294},
	doi = {10.48550/arXiv.1712.02294},
	abstract = {We present {AVOD}, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses {LIDAR} point clouds and {RGB} images to generate features that are shared by two subnetworks: a region proposal network ({RPN}) and a second stage detector network. The proposed {RPN} uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the {KITTI} 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod},
	number = {{arXiv}:1712.02294},
	publisher = {{arXiv}},
	author = {Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven},
	urldate = {2024-11-12},
	date = {2018-07-12},
	eprinttype = {arxiv},
	eprint = {1712.02294},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shah_airsim_2017,
	title = {{AirSim}: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},
	url = {http://arxiv.org/abs/1705.05065},
	doi = {10.48550/arXiv.1705.05065},
	shorttitle = {{AirSim}},
	abstract = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop ({HITL}) simulations with support for popular protocols (e.g. {MavLink}). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.},
	number = {{arXiv}:1705.05065},
	publisher = {{arXiv}},
	author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
	urldate = {2024-11-12},
	date = {2017-07-18},
	eprinttype = {arxiv},
	eprint = {1705.05065},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Computer Science - Systems and Control},
}

@misc{dosovitskiy_carla_2017,
	title = {{CARLA}: An Open Urban Driving Simulator},
	url = {http://arxiv.org/abs/1711.03938},
	doi = {10.48550/arXiv.1711.03938},
	shorttitle = {{CARLA}},
	abstract = {We introduce {CARLA}, an open-source simulator for autonomous driving research. {CARLA} has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, {CARLA} provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use {CARLA} to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by {CARLA}, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E},
	number = {{arXiv}:1711.03938},
	publisher = {{arXiv}},
	author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
	urldate = {2024-11-12},
	date = {2017-11-10},
	eprinttype = {arxiv},
	eprint = {1711.03938},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wu_transformation-equivariant_2022,
	title = {Transformation-Equivariant 3D Object Detection for Autonomous Driving},
	url = {http://arxiv.org/abs/2211.11962},
	doi = {10.48550/arXiv.2211.11962},
	abstract = {3D object detection received increasing attention in autonomous driving recently. Objects in 3D scenes are distributed with diverse orientations. Ordinary detectors do not explicitly model the variations of rotation and reflection transformations. Consequently, large networks and extensive data augmentation are required for robust detection. Recent equivariant networks explicitly model the transformation variations by applying shared networks on multiple transformed point clouds, showing great potential in object geometry modeling. However, it is difficult to apply such networks to 3D object detection in autonomous driving due to its large computation cost and slow reasoning speed. In this work, we present {TED}, an efficient Transformation-Equivariant 3D Detector to overcome the computation cost and speed issues. {TED} first applies a sparse convolution backbone to extract multi-channel transformation-equivariant voxel features; and then aligns and aggregates these equivariant features into lightweight and compact representations for high-performance 3D object detection. On the highly competitive {KITTI} 3D car detection leaderboard, {TED} ranked 1st among all submissions with competitive efficiency.},
	number = {{arXiv}:2211.11962},
	publisher = {{arXiv}},
	author = {Wu, Hai and Wen, Chenglu and Li, Wei and Li, Xin and Yang, Ruigang and Wang, Cheng},
	urldate = {2024-11-12},
	date = {2022-12-01},
	eprinttype = {arxiv},
	eprint = {2211.11962},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{tian_acf-net_2024,
	title = {{ACF}-Net: Asymmetric Cascade Fusion for 3D Detection With {LiDAR} Point Clouds and Images},
	volume = {9},
	issn = {2379-8904},
	url = {https://ieeexplore.ieee.org/document/10363115/?arnumber=10363115},
	doi = {10.1109/TIV.2023.3341223},
	shorttitle = {{ACF}-Net},
	abstract = {The recognition and utilization of complementary information arising from modality-intrinsic properties play crucial roles in multimodal 3D detection. However, most of the current approaches for fusion-based 3D detection follow symmetrical fusion paradigms and adopt early fusion, middle fusion as well as late fusion styles, which ignore the unequal status of data with different modalities. In this paper, according to the timing of fusion, we adopt an asymmetric cascade fusion network to exploit both the structural information from point clouds and the complementary semantic information from images. A multi-stage cascade design of 3D object detection is proposed to iteratively refine predictions and several late image features (comprised of detection clues, segmentation clues, and deep features from encoders) are incorporated into different stages of the {LiDAR} branch to maintain the integrity of image features and enable deep multimodal interactions. Besides, to mitigate the effects of the down-sampling of voxelized features and possible mismatching of multimodal data, we propose proxy-based cross-modality sampling to utilize the high-density point clouds coordinates and develop an image degeneration process to simulate the noise in cross-modality matching for robust training. Extensive experiments are conducted on {KITTI} and Waymo Open Dataset, which validate the effectiveness of the proposed method.},
	pages = {3360--3371},
	number = {2},
	journaltitle = {{IEEE} Transactions on Intelligent Vehicles},
	author = {Tian, Yonglin and Zhang, Xianjing and Wang, Xiao and Xu, Jintao and Wang, Jiangong and Ai, Rui and Gu, Weihao and Ding, Weiping},
	urldate = {2024-11-12},
	date = {2024-02},
	note = {Conference Name: {IEEE} Transactions on Intelligent Vehicles},
	keywords = {3D detection, Feature extraction, Fuses, Laser radar, Object detection, Point cloud compression, Three-dimensional displays, Timing, asymmetric fusion, autonomous driving, cascade fusion, multimodal fusion},
}

@article{hoang_tsstdet_2024,
	title = {{TSSTDet}: Transformation-Based 3-D Object Detection via a Spatial Shape Transformer},
	volume = {24},
	issn = {1558-1748},
	url = {https://ieeexplore.ieee.org/abstract/document/10399338},
	doi = {10.1109/JSEN.2024.3350770},
	shorttitle = {{TSSTDet}},
	abstract = {Accurately detecting and understanding the shapes of objects in 3-D scenes are essential for autonomous driving. In a 3-D scene, objects are distributed with various incomplete shapes and rotations. Determining the shape allows for a comprehensive understanding of an object’s dimensions, rotations, and spatial relationships with its surroundings. Traditional detection methods do not explicitly consider the rotations and complete shapes that objects can assume. Consequently, these methods require large networks and extensive data augmentation to detect accurately. Taking advantage of the vision-transformer ({ViT}), we introduce an efficient transformer-based 3-D detector called transformation-based 3-D object detection via a spatial shape transformer ({TSSTDet}) to address these challenges. We constructed {TSSTDet} as a multistage detector based on a light detection and ranging ({LiDAR}) point cloud. Specifically, {TSSTDet} utilizes a sparse convolution ({SpConv}) backbone to extract multichannel and transformation-equivariant voxel features. Furthermore, we designed an efficient module that employs the transformer approach to estimate the completed shape of an object. These features are then aligned and aggregated to create lightweight and compact representations that enable high-performance 3-D object detection. We assessed the effectiveness of the proposed framework by evaluating its performance on both the {KITTI} and Waymo open datasets ({WODs}). These evaluations demonstrated that our framework achieves top-tier performance in 3-D object detection.},
	pages = {7126--7139},
	number = {5},
	journaltitle = {{IEEE} Sensors Journal},
	author = {Hoang, Hiep Anh and Bui, Duy Cuong and Yoo, Myungsik},
	urldate = {2024-11-12},
	date = {2024-03},
	note = {Conference Name: {IEEE} Sensors Journal},
	keywords = {3-D object detection, Detectors, Feature extraction, Object detection, Point cloud compression, Shape, Task analysis, Transformers, autonomous driving, light detection and ranging ({LiDAR}) point cloud, vision transformer},
}

@misc{shi_pv-rcnn_2022,
	title = {{PV}-{RCNN}++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection},
	url = {http://arxiv.org/abs/2102.00463},
	doi = {10.48550/arXiv.2102.00463},
	shorttitle = {{PV}-{RCNN}++},
	abstract = {3D object detection is receiving increasing attention from both industry and academia thanks to its wide applications in various fields. In this paper, we propose Point-Voxel Region-based Convolution Neural Networks ({PV}-{RCNNs}) for 3D object detection on point clouds. First, we propose a novel 3D detector, {PV}-{RCNN}, which boosts the 3D detection performance by deeply integrating the feature learning of both point-based set abstraction and voxel-based sparse convolution through two novel steps, i.e., the voxel-to-keypoint scene encoding and the keypoint-to-grid {RoI} feature abstraction. Second, we propose an advanced framework, {PV}-{RCNN}++, for more efficient and accurate 3D object detection. It consists of two major improvements: sectorized proposal-centric sampling for efficiently producing more representative keypoints, and {VectorPool} aggregation for better aggregating local point features with much less resource consumption. With these two strategies, our {PV}-{RCNN}++ is about \$3{\textbackslash}times\$ faster than {PV}-{RCNN}, while also achieving better performance. The experiments demonstrate that our proposed {PV}-{RCNN}++ framework achieves state-of-the-art 3D detection performance on the large-scale and highly-competitive Waymo Open Dataset with 10 {FPS} inference speed on the detection range of 150m * 150m.},
	number = {{arXiv}:2102.00463},
	publisher = {{arXiv}},
	author = {Shi, Shaoshuai and Jiang, Li and Deng, Jiajun and Wang, Zhe and Guo, Chaoxu and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
	urldate = {2024-11-12},
	date = {2022-11-07},
	eprinttype = {arxiv},
	eprint = {2102.00463},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{shi_pv-rcnn_2020,
	location = {Seattle, {WA}, {USA}},
	title = {{PV}-{RCNN}: Point-Voxel Feature Set Abstraction for 3D Object Detection},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157234/},
	doi = {10.1109/CVPR42600.2020.01054},
	shorttitle = {{PV}-{RCNN}},
	abstract = {We present a novel and high-performance 3D object detection framework, named {PointVoxel}-{RCNN} ({PV}-{RCNN}), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network ({CNN}) and {PointNet}-based set abstraction to learn more discriminative point cloud features. It takes advantages of efﬁcient learning and high-quality proposals of the 3D voxel {CNN} and the ﬂexible receptive ﬁelds of the {PointNet}-based networks. Speciﬁcally, the proposed framework summarizes the 3D scene with a 3D voxel {CNN} into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the highquality 3D proposals generated by the voxel {CNN}, the {RoIgrid} pooling is proposed to abstract proposal-speciﬁc features from the keypoints to the {RoI}-grid points via keypoint set abstraction. Compared with conventional pooling operations, the {RoI}-grid feature points encode much richer context information for accurately estimating object conﬁdences and locations. Extensive experiments on both the {KITTI} dataset and the Waymo Open dataset show that our proposed {PV}-{RCNN} surpasses state-of-the-art 3D detection methods with remarkable margins.},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {10526--10535},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Shi, Shaoshuai and Guo, Chaoxu and Jiang, Li and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
	urldate = {2024-11-12},
	date = {2020-06},
	langid = {english},
}

@inproceedings{shi_pointrcnn_2019,
	location = {Long Beach, {CA}, {USA}},
	title = {{PointRCNN}: 3D Object Proposal Generation and Detection From Point Cloud},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954080/},
	doi = {10.1109/CVPR.2019.00086},
	shorttitle = {{PointRCNN}},
	abstract = {In this paper, we propose {PointRCNN} for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for reﬁning proposals in the canonical coordinates to obtain the ﬁnal detection results. Instead of generating proposals from {RGB} image or projecting point cloud to bird’s view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box reﬁnement and conﬁdence prediction. Extensive experiments on the 3D detection benchmark of {KITTI} dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github.com/sshaoshuai/{PointRCNN}.},
	eventtitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {770--779},
	booktitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
	urldate = {2024-11-12},
	date = {2019-06},
	langid = {english},
}

@misc{shi_pointrcnn_2019-1,
	title = {{PointRCNN}: 3D Object Proposal Generation and Detection from Point Cloud},
	url = {http://arxiv.org/abs/1812.04244},
	doi = {10.48550/arXiv.1812.04244},
	shorttitle = {{PointRCNN}},
	abstract = {In this paper, we propose {PointRCNN} for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from {RGB} image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of {KITTI} dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github.com/sshaoshuai/{PointRCNN}.},
	number = {{arXiv}:1812.04244},
	publisher = {{arXiv}},
	author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
	urldate = {2024-11-12},
	date = {2019-05-16},
	eprinttype = {arxiv},
	eprint = {1812.04244},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{yan_second_2018,
	title = {{SECOND}: Sparsely Embedded Convolutional Detection},
	volume = {18},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/18/10/3337},
	doi = {10.3390/s18103337},
	shorttitle = {{SECOND}},
	abstract = {{LiDAR}-based or {RGB}-D-based object detection is used in numerous applications, ranging from autonomous driving to robot vision. Voxel-based 3D convolutional networks have been used for some time to enhance the retention of information when processing point cloud {LiDAR} data. However, problems remain, including a slow inference speed and low orientation estimation performance. We therefore investigate an improved sparse convolution method for such networks, which significantly increases the speed of both training and inference. We also introduce a new form of angle loss regression to improve the orientation estimation performance and a new data augmentation approach that can enhance the convergence speed and performance. The proposed network produces state-of-the-art results on the {KITTI} 3D object detection benchmarks while maintaining a fast inference speed.},
	pages = {3337},
	number = {10},
	journaltitle = {Sensors},
	author = {Yan, Yan and Mao, Yuxing and Li, Bo},
	urldate = {2024-11-12},
	date = {2018-10},
	langid = {english},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D object detection, {LIDAR}, autonomous driving, convolutional neural networks},
}

@misc{mao_one_2021,
	title = {One Million Scenes for Autonomous Driving: {ONCE} Dataset},
	url = {http://arxiv.org/abs/2106.11037},
	doi = {10.48550/arXiv.2106.11037},
	shorttitle = {One Million Scenes for Autonomous Driving},
	abstract = {Current perception models in autonomous driving have become notorious for greatly relying on a mass of annotated data to cover unseen cases and address the long-tail problem. On the other hand, learning from unlabeled large-scale collected data and incrementally self-training powerful recognition models have received increasing attention and may become the solutions of next-generation industry-level powerful and robust perception models in autonomous driving. However, the research community generally suffered from data inadequacy of those essential real-world scene data, which hampers the future exploration of fully/semi/self-supervised methods for 3D perception. In this paper, we introduce the {ONCE} (One {millioN} {sCenEs}) dataset for 3D object detection in the autonomous driving scenario. The {ONCE} dataset consists of 1 million {LiDAR} scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (e.g. {nuScenes} and Waymo), and it is collected across a range of different areas, periods and weather conditions. To facilitate future research on exploiting unlabeled data for 3D detection, we additionally provide a benchmark in which we reproduce and evaluate a variety of self-supervised and semi-supervised methods on the {ONCE} dataset. We conduct extensive analyses on those methods and provide valuable observations on their performance related to the scale of used data. Data, code, and more information are available at https://once-for-auto-driving.github.io/index.html.},
	number = {{arXiv}:2106.11037},
	publisher = {{arXiv}},
	author = {Mao, Jiageng and Niu, Minzhe and Jiang, Chenhan and Liang, Hanxue and Chen, Jingheng and Liang, Xiaodan and Li, Yamin and Ye, Chaoqiang and Zhang, Wei and Li, Zhenguo and Yu, Jie and Xu, Hang and Xu, Chunjing},
	urldate = {2024-11-11},
	date = {2021-10-25},
	eprinttype = {arxiv},
	eprint = {2106.11037},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wilson_argoverse_2023,
	title = {Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting},
	url = {http://arxiv.org/abs/2301.00493},
	doi = {10.48550/arXiv.2301.00493},
	shorttitle = {Argoverse 2},
	abstract = {We introduce Argoverse 2 ({AV}2) - a collection of three datasets for perception and forecasting research in the self-driving domain. The annotated Sensor Dataset contains 1,000 sequences of multimodal data, encompassing high-resolution imagery from seven ring cameras, and two stereo cameras in addition to lidar point clouds, and 6-{DOF} map-aligned pose. Sequences contain 3D cuboid annotations for 26 object categories, all of which are sufficiently-sampled to support training and evaluation of 3D perception models. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point clouds and map-aligned pose. This dataset is the largest ever collection of lidar sensor data and supports self-supervised learning and the emerging task of point cloud forecasting. Finally, the Motion Forecasting Dataset contains 250,000 scenarios mined for interesting and challenging interactions between the autonomous vehicle and other actors in each local scene. Models are tasked with the prediction of future motion for "scored actors" in each scenario and are provided with track histories that capture object location, heading, velocity, and category. In all three datasets, each scenario contains its own {HD} Map with 3D lane and crosswalk geometry - sourced from data captured in six distinct cities. We believe these datasets will support new and existing machine learning research problems in ways that existing datasets do not. All datasets are released under the {CC} {BY}-{NC}-{SA} 4.0 license.},
	number = {{arXiv}:2301.00493},
	publisher = {{arXiv}},
	author = {Wilson, Benjamin and Qi, William and Agarwal, Tanmay and Lambert, John and Singh, Jagjeet and Khandelwal, Siddhesh and Pan, Bowen and Kumar, Ratnesh and Hartnett, Andrew and Pontes, Jhony Kaesemodel and Ramanan, Deva and Carr, Peter and Hays, James},
	urldate = {2024-11-11},
	date = {2023-01-02},
	eprinttype = {arxiv},
	eprint = {2301.00493},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{sun_scalability_2020,
	title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.html},
	shorttitle = {Scalability in Perception for Autonomous Driving},
	eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {2446--2454},
	author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
	urldate = {2024-11-11},
	date = {2020},
}

@misc{liao_kitti-360_2022,
	title = {{KITTI}-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D},
	url = {http://arxiv.org/abs/2109.13410},
	doi = {10.48550/arXiv.2109.13410},
	shorttitle = {{KITTI}-360},
	abstract = {For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop {KITTI}-360, successor of the popular {KITTI} dataset. {KITTI}-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic {SLAM}. {KITTI}-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today's grand challenges: the development of fully autonomous self-driving systems.},
	number = {{arXiv}:2109.13410},
	publisher = {{arXiv}},
	author = {Liao, Yiyi and Xie, Jun and Geiger, Andreas},
	urldate = {2024-11-11},
	date = {2022-06-03},
	eprinttype = {arxiv},
	eprint = {2109.13410},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ku_joint_2018-1,
	title = {Joint 3D Proposal Generation and Object Detection from View Aggregation},
	url = {http://arxiv.org/abs/1712.02294},
	doi = {10.48550/arXiv.1712.02294},
	abstract = {We present {AVOD}, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses {LIDAR} point clouds and {RGB} images to generate features that are shared by two subnetworks: a region proposal network ({RPN}) and a second stage detector network. The proposed {RPN} uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the {KITTI} 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod},
	number = {{arXiv}:1712.02294},
	publisher = {{arXiv}},
	author = {Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven},
	urldate = {2024-11-11},
	date = {2018-07-12},
	eprinttype = {arxiv},
	eprint = {1712.02294},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tobin_domain_2017,
	title = {Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World},
	url = {http://arxiv.org/abs/1703.06907},
	abstract = {Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We ﬁnd that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the ﬁrst successful transfer of a deep neural network trained only on simulated {RGB} images (without pre-training on real images) to the real world for the purpose of robotic control.},
	number = {{arXiv}:1703.06907},
	publisher = {{arXiv}},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	urldate = {2024-11-11},
	date = {2017-03-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.06907 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{antunes_exploring_2024,
	location = {Cham},
	title = {Exploring Domain Adaptation with Depth-Based 3D Object Detection in {CARLA} Simulator},
	isbn = {978-3-031-58676-7},
	doi = {10.1007/978-3-031-58676-7_9},
	abstract = {Data collection and scene understanding have become crucial tasks in the development of intelligent vehicles, particularly in the context of autonomous driving. Deep Learning ({DL}) and transformer-based architectures have emerged as the preferred methods for object detection and segmentation tasks. However, {DL}-based methods often require extensive training with diverse data, posing challenges in terms of data availability and labeling. To address this problem, techniques such as transfer learning and data augmentation have been adopted. Simulators like {CARLA} have gained popularity in the autonomous driving domain, enabling the evaluation of architectures in realistic environments before real-world deployment. Synthetic data generated by simulators offers several advantages, including cost-effectiveness, access to diverse scenarios, and the ability to generate accurate ground truth annotations. In this paper, we focus our investigation on evaluating the performance and domain adaptation capabilities of a 3D object detection pipeline based on depth estimation using a stereo camera in the {CARLA} simulator. Our main objective is to analyze the results of the depth estimation stage using two different approaches: {CoEx} and {SDN}. The different experiments will be performed on real and synthetic scenarios from the {KITTI} and {SHIFT} datasets.},
	pages = {105--117},
	booktitle = {Robot 2023: Sixth Iberian Robotics Conference},
	publisher = {Springer Nature Switzerland},
	author = {Antunes, Miguel and Bergasa, Luis M. and Montiel-Marín, Santiago and Sánchez-García, Fabio and Pardo-Decimavilla, Pablo and Revenga, Pedro},
	editor = {Marques, Lino and Santos, Cristina and Lima, José Luís and Tardioli, Danilo and Ferre, Manuel},
	date = {2024},
	langid = {english},
	keywords = {3D object detection, {CARLA}, Deep Learning, {KITTI}, Transfer Learning},
}

@online{noauthor_cdrone_nodate,
	title = {{CDrone} - {CARLA} Drone: Monocular 3D Object Detection from a Different Perspective},
	url = {https://deepscenario.github.io/CDrone/},
	urldate = {2024-11-11},
}

@article{bavirisetti_simulated_2023,
	title = {Simulated {RGB} and {LiDAR} Image based Training of Object Detection Models in the Context of Autonomous Driving},
	rights = {Copyright (c) 2023 Norsk {IKT}-konferanse for forskning og utdanning},
	issn = {1892-0721},
	url = {https://www.ntnu.no/ojs/index.php/nikt/article/view/5665},
	abstract = {The topic of object detection, which involves giving cars the ability to perceive their environment has drawn greater attention. For better performance, object detection algorithms often need huge datasets, which are frequently manually labeled. This procedure is expensive and time-consuming. Instead, a simulated environment due to which one has complete control over all parameters and allows for automated image annotation. Carla, an open-source project created exclusively for the study of autonomous driving, is one such simulator. This study examines if object detection models that can recognize actual traffic items can be trained using automatically annotated simulator data from Carla. The findings of the experiments demonstrate that optimizing a trained model using Carla’s data, along with some real data, is encouraging. The Yolov5 model, trained using pre-trained Carla weights, exhibited improvements across all performance metrics compared to one trained exclusively on 2000 Kitti images. While it didn’t reach the performance level of the 6000-image Kitti model, the enhancements were indeed substantial. The {mAP}0.5:0.95 score saw an approximate 10\% boost, with the most significant improvement occurring in the Pedestrian class. Furthermore, it is demonstrated that a substantial performance boost can be achieved by training a base model with Carla data and fine-tuning it with a smaller portion of the Kitti dataset. Moreover, the potential utility of Carla {LiDAR} images in reducing the volume of real images required while maintaining respectable model performance becomes evident. Our code is available at: https://tinyurl.com/3fdjd9xb.},
	number = {1},
	journaltitle = {Norsk {IKT}-konferanse for forskning og utdanning},
	author = {Bavirisetti, Durga Prasad and Brobak, Eskild and Espen, Peder and Kiss, Gabriel and Lindseth, Frank},
	urldate = {2024-11-11},
	date = {2023-11-30},
	langid = {english},
	note = {Number: 1},
	keywords = {Computer vision},
}

@misc{huch_quantifying_2023,
	title = {Quantifying the {LiDAR} Sim-to-Real Domain Shift: A Detailed Investigation Using Object Detectors and Analyzing Point Clouds at Target-Level},
	url = {http://arxiv.org/abs/2303.01899},
	doi = {10.48550/arXiv.2303.01899},
	shorttitle = {Quantifying the {LiDAR} Sim-to-Real Domain Shift},
	abstract = {{LiDAR} object detection algorithms based on neural networks for autonomous driving require large amounts of data for training, validation, and testing. As real-world data collection and labeling are time-consuming and expensive, simulation-based synthetic data generation is a viable alternative. However, using simulated data for the training of neural networks leads to a domain shift of training and testing data due to differences in scenes, scenarios, and distributions. In this work, we quantify the sim-to-real domain shift by means of {LiDAR} object detectors trained with a new scenario-identical real-world and simulated dataset. In addition, we answer the questions of how well the simulated data resembles the real-world data and how well object detectors trained on simulated data perform on real-world data. Further, we analyze point clouds at the target-level by comparing real-world and simulated point clouds within the 3D bounding boxes of the targets. Our experiments show that a significant sim-to-real domain shift exists even for our scenario-identical datasets. This domain shift amounts to an average precision reduction of around 14 \% for object detectors trained with simulated data. Additional experiments reveal that this domain shift can be lowered by introducing a simple noise model in simulation. We further show that a simple downsampling method to model real-world physics does not influence the performance of the object detectors.},
	number = {{arXiv}:2303.01899},
	publisher = {{arXiv}},
	author = {Huch, Sebastian and Scalerandi, Luca and Rivera, Esteban and Lienkamp, Markus},
	urldate = {2024-11-11},
	date = {2023-03-03},
	eprinttype = {arxiv},
	eprint = {2303.01899},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{huch_quantifying_2023-1,
	title = {Quantifying the {LiDAR} Sim-to-Real Domain Shift: A Detailed Investigation Using Object Detectors and Analyzing Point Clouds at Target-Level},
	volume = {8},
	issn = {2379-8904, 2379-8858},
	url = {http://arxiv.org/abs/2303.01899},
	doi = {10.1109/TIV.2023.3251650},
	shorttitle = {Quantifying the {LiDAR} Sim-to-Real Domain Shift},
	abstract = {{LiDAR} object detection algorithms based on neural networks for autonomous driving require large amounts of data for training, validation, and testing. As real-world data collection and labeling are time-consuming and expensive, simulation-based synthetic data generation is a viable alternative. However, using simulated data for the training of neural networks leads to a domain shift of training and testing data due to differences in scenes, scenarios, and distributions. In this work, we quantify the sim-to-real domain shift by means of {LiDAR} object detectors trained with a new scenario-identical real-world and simulated dataset. In addition, we answer the questions of how well the simulated data resembles the real-world data and how well object detectors trained on simulated data perform on real-world data. Further, we analyze point clouds at the target-level by comparing real-world and simulated point clouds within the 3D bounding boxes of the targets. Our experiments show that a signiﬁcant sim-to-real domain shift exists even for our scenario-identical datasets. This domain shift amounts to an average precision reduction of around 14 \% for object detectors trained with simulated data. Additional experiments reveal that this domain shift can be lowered by introducing a simple noise model in simulation. We further show that a simple downsampling method to model real-world physics does not inﬂuence the performance of the object detectors.},
	pages = {2970--2982},
	number = {4},
	journaltitle = {{IEEE} Transactions on Intelligent Vehicles},
	shortjournal = {{IEEE} Trans. Intell. Veh.},
	author = {Huch, Sebastian and Scalerandi, Luca and Rivera, Esteban and Lienkamp, Markus},
	urldate = {2024-11-11},
	date = {2023-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2303.01899 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{noauthor_httpsarxivorgpdf230301899_nodate,
	title = {https://arxiv.org/pdf/2303.01899},
	url = {https://arxiv.org/pdf/2303.01899},
	urldate = {2024-11-11},
}

@misc{brekke_multimodal_2019,
	title = {Multimodal 3D Object Detection from Simulated Pretraining},
	url = {http://arxiv.org/abs/1905.07754},
	doi = {10.48550/arXiv.1905.07754},
	abstract = {The need for simulated data in autonomous driving applications has become increasingly important, both for validation of pretrained models and for training new models. In order for these models to generalize to real-world applications, it is critical that the underlying dataset contains a variety of driving scenarios and that simulated sensor readings closely mimics real-world sensors. We present the Carla Automated Dataset Extraction Tool ({CADET}), a novel tool for generating training data from the {CARLA} simulator to be used in autonomous driving research. The tool is able to export high-quality, synchronized {LIDAR} and camera data with object annotations, and offers configuration to accurately reflect a real-life sensor array. Furthermore, we use this tool to generate a dataset consisting of 10 000 samples and use this dataset in order to train the 3D object detection network {AVOD}-{FPN}, with finetuning on the {KITTI} dataset in order to evaluate the potential for effective pretraining. We also present two novel {LIDAR} feature map configurations in Bird's Eye View for use with {AVOD}-{FPN} that can be easily modified. These configurations are tested on the {KITTI} and {CADET} datasets in order to evaluate their performance as well as the usability of the simulated dataset for pretraining. Although insufficient to fully replace the use of real world data, and generally not able to exceed the performance of systems fully trained on real data, our results indicate that simulated data can considerably reduce the amount of training on real data required to achieve satisfactory levels of accuracy.},
	number = {{arXiv}:1905.07754},
	publisher = {{arXiv}},
	author = {Brekke, Åsmund and Vatsendvik, Fredrik and Lindseth, Frank},
	urldate = {2024-11-11},
	date = {2019-05-19},
	eprinttype = {arxiv},
	eprint = {1905.07754},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{deschaud_kitti-carla_2021,
	title = {{KITTI}-{CARLA}: a {KITTI}-like dataset generated by {CARLA} Simulator},
	url = {http://arxiv.org/abs/2109.00892},
	doi = {10.48550/arXiv.2109.00892},
	shorttitle = {{KITTI}-{CARLA}},
	abstract = {{KITTI}-{CARLA} is a dataset built from the {CARLA} v0.9.10 simulator using a vehicle with sensors identical to the {KITTI} dataset. The vehicle thus has a Velodyne {HDL}64 {LiDAR} positioned in the middle of the roof and two color cameras similar to Point Grey Flea 2. The positions of the {LiDAR} and cameras are the same as the setup used in {KITTI}. The objective of this dataset is to test approaches of semantic segmentation {LiDAR} and/or images, odometry {LiDAR} and/or image in synthetic data and to compare with the results obtained on real data like {KITTI}. This dataset thus makes it possible to improve transfer learning methods from a synthetic dataset to a real dataset. We created 7 sequences with 5000 frames in each sequence in the 7 maps of {CARLA} providing different environments (city, suburban area, mountain, rural area, highway...). The dataset is available at: http://npm3d.fr},
	number = {{arXiv}:2109.00892},
	publisher = {{arXiv}},
	author = {Deschaud, Jean-Emmanuel},
	urldate = {2024-11-11},
	date = {2021-08-17},
	eprinttype = {arxiv},
	eprint = {2109.00892},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{vizzo_kiss-icp_2023,
	title = {{KISS}-{ICP}: In Defense of Point-to-Point {ICP} – Simple, Accurate, and Robust Registration If Done the Right Way},
	volume = {8},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/10015694/},
	doi = {10.1109/LRA.2023.3236571},
	shorttitle = {{KISS}-{ICP}},
	abstract = {Robust and accurate pose estimation of a robotic platform, so-called sensor-based odometry, is an essential part of many robotic applications. While many sensor odometry systems made progress by adding more complexity to the egomotion estimation process, we move in the opposite direction. By removing a majority of parts and focusing on the core elements, we obtain a surprisingly effective system that is simple to realize and can operate under various environmental conditions using different {LiDAR} sensors. Our odometry estimation approach relies on point-to-point {ICP} combined with adaptive thresholding for correspondence matching, a robust kernel, a simple but widely applicable motion compensation approach, and a point cloud subsampling strategy. This yields a system with only a few parameters that in most cases do not even have to be tuned to a specific {LiDAR} sensor. Our system performs on par with state-of-the-art methods under various operating conditions using different platforms using the same parameters: automotive platforms, {UAV}-based operation, vehicles like segways, or handheld {LiDARs}. We do not require integrating {IMU} data and solely rely on 3D point clouds obtained from a wide range of 3D {LiDAR} sensors, thus, enabling a broad spectrum of different applications and operating conditions. Our open-source system operates faster than the sensor frame rate in all presented datasets and is designed for real-world scenarios.},
	pages = {1029--1036},
	number = {2},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	shortjournal = {{IEEE} Robot. Autom. Lett.},
	author = {Vizzo, Ignacio and Guadagnino, Tiziano and Mersch, Benedikt and Wiesmann, Louis and Behley, Jens and Stachniss, Cyrill},
	urldate = {2024-03-04},
	date = {2023-02},
	langid = {english},
}

@online{noauthor_exwayz_nodate,
	title = {Exwayz - Unleash 3D {LiDAR} power},
	url = {https://www.exwayz.fr/},
	urldate = {2023-10-30},
}

@misc{xia_diffi2i_2023,
	title = {{DiffI}2I: Efficient Diffusion Model for Image-to-Image Translation},
	url = {http://arxiv.org/abs/2308.13767},
	doi = {10.48550/arXiv.2308.13767},
	shorttitle = {{DiffI}2I},
	abstract = {The Diffusion Model ({DM}) has emerged as the {SOTA} approach for image synthesis. However, the existing {DM} cannot perform well on some image-to-image translation (I2I) tasks. Different from image synthesis, some I2I tasks, such as super-resolution, require generating results in accordance with {GT} images. Traditional {DMs} for image synthesis require extensive iterations and large denoising models to estimate entire images, which gives their strong generative ability but also leads to artifacts and inefficiency for I2I. To tackle this challenge, we propose a simple, efficient, and powerful {DM} framework for I2I, called {DiffI}2I. Specifically, {DiffI}2I comprises three key components: a compact I2I prior extraction network ({CPEN}), a dynamic I2I transformer ({DI}2Iformer), and a denoising network. We train {DiffI}2I in two stages: pretraining and {DM} training. For pretraining, {GT} and input images are fed into {CPEN}\$\_\{S1\}\$ to capture a compact I2I prior representation ({IPR}) guiding {DI}2Iformer. In the second stage, the {DM} is trained to only use the input images to estimate the same {IRP} as {CPEN}\$\_\{S1\}\$. Compared to traditional {DMs}, the compact {IPR} enables {DiffI}2I to obtain more accurate outcomes and employ a lighter denoising network and fewer iterations. Through extensive experiments on various I2I tasks, we demonstrate that {DiffI}2I achieves {SOTA} performance while significantly reducing computational burdens.},
	number = {{arXiv}:2308.13767},
	publisher = {{arXiv}},
	author = {Xia, Bin and Zhang, Yulun and Wang, Shiyin and Wang, Yitong and Wu, Xinglong and Tian, Yapeng and Yang, Wenming and Timotfe, Radu and Van Gool, Luc},
	urldate = {2024-02-29},
	date = {2023-08-26},
	eprinttype = {arxiv},
	eprint = {2308.13767 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kerbl_3d_2023,
	title = {3D Gaussian Splatting for Real-Time Radiance Field Rendering},
	volume = {42},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3592433},
	doi = {10.1145/3592433},
	abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
	pages = {1--14},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkuehler, Thomas and Drettakis, George},
	urldate = {2024-02-28},
	date = {2023-08},
	langid = {english},
}

@online{noauthor_3d_nodate,
	title = {3D Gaussian Splatting for Real-Time Radiance Field Rendering},
	url = {https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/},
	urldate = {2024-02-28},
}

@misc{ruckert_adop_2022,
	title = {{ADOP}: Approximate Differentiable One-Pixel Point Rendering},
	url = {http://arxiv.org/abs/2110.06635},
	doi = {10.48550/arXiv.2110.06635},
	shorttitle = {{ADOP}},
	abstract = {In this paper we present {ADOP}, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g.{\textasciitilde}exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, {ADOP} achieves real-time rendering rates even for models with well over 100M points. https://github.com/darglein/{ADOP}},
	number = {{arXiv}:2110.06635},
	publisher = {{arXiv}},
	author = {Rückert, Darius and Franke, Linus and Stamminger, Marc},
	urldate = {2024-02-28},
	date = {2022-05-03},
	eprinttype = {arxiv},
	eprint = {2110.06635 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@inproceedings{harrer_inovis_2023,
	location = {New York, {NY}, {USA}},
	title = {Inovis: Instant Novel-View Synthesis},
	isbn = {9798400703157},
	url = {https://dl.acm.org/doi/10.1145/3610548.3618216},
	doi = {10.1145/3610548.3618216},
	series = {{SA} '23},
	shorttitle = {Inovis},
	abstract = {Novel-view synthesis is an ill-posed problem in that it requires inference of previously unseen information. Recently, reviving the traditional field of image-based rendering, neural methods proved particularly suitable for this interpolation/extrapolation task; however, they often require a-priori scene-completeness or costly preprocessing steps and generally suffer from long (scene-specific) training times. Our work draws from recent progress in neural spatio-temporal supersampling to enhance a state-of-the-art neural renderer’s ability to infer novel-view information at inference time. We adapt a supersampling architecture [Xiao et al. 2020], which resamples previously rendered frames, to instead recombine nearby camera images in a multi-view dataset. These input frames are warped into a joint target frame, guided by the most recent (point-based) scene representation, followed by neural interpolation. The resulting architecture gains sufficient robustness to significantly improve transferability to previously unseen datasets. In particular, this enables novel applications for neural rendering where dynamically streamed content is directly incorporated in a (neural) image-based reconstruction of a scene. As we will show, our method reaches state-of-the-art performance when compared to previous works that rely on static and sufficiently densely sampled scenes; in addition, we demonstrate our system’s particular suitability for dynamically streamed content, where our approach is able to produce high-fidelity novel-view synthesis even with significantly fewer available frames than competing neural methods.},
	pages = {1--12},
	booktitle = {{SIGGRAPH} Asia 2023 Conference Papers},
	publisher = {Association for Computing Machinery},
	author = {Harrer, Mathias and Franke, Linus and Fink, Laura and Stamminger, Marc and Weyrich, Tim},
	urldate = {2024-02-28},
	date = {2023},
	keywords = {Neural Rendering, Novel-View Synthesis, Point-based Graphics},
}

@inproceedings{li_durlar_2021,
	title = {{DurLAR}: A High-Fidelity 128-Channel {LiDAR} Dataset with Panoramic Ambient and Reflectivity Imagery for Multi-Modal Autonomous Driving Applications},
	url = {https://ieeexplore.ieee.org/document/9665963},
	doi = {10.1109/3DV53792.2021.00130},
	shorttitle = {{DurLAR}},
	abstract = {We present {DurLAR}, a high-fidelity 128-channel 3D {LiDAR} dataset with panoramic ambient (near infrared) and reflectivity imagery, as well as a sample benchmark task using depth estimation for autonomous driving applications. Our driving platform is equipped with a high resolution 128 channel {LiDAR}, a 2MPix stereo camera, a lux meter and a {GNSSlINS} system. Ambient and reflectivity images are made available along with the {LiDAR} point clouds to facilitate multi-modal use of concurrent ambient and reflectivity scene information. Leveraging {DurLAR}, with a resolution exceeding that of prior benchmarks, we consider the task of monocular depth estimation and use this increased availability of higher resolution, yet sparse ground truth scene depth information to propose a novel joint supervised/self-supervised loss formulation. We compare performance over both our new {DurLAR} dataset, the established {KITTI} benchmark and the Cityscapes dataset. Our evaluation shows our joint use supervised and self-supervised loss terms, enabled via the superior ground truth resolution and availability within {DurLAR} improves the quantitative and qualitative performance of leading contemporary monocular depth estimation approaches ({RMSE} = 3.639, {SqRel} = 0.936).},
	eventtitle = {2021 International Conference on 3D Vision (3DV)},
	pages = {1227--1237},
	booktitle = {2021 International Conference on 3D Vision (3DV)},
	author = {Li, Li and Ismail, Khalid N. and Shum, Hubert P. H. and Breckon, Toby P.},
	urldate = {2024-02-28},
	date = {2021-12},
	note = {{ISSN}: 2475-7888},
	keywords = {Benchmark testing, Estimation, Image resolution, Laser radar, Reflectivity, Supervised learning, Three-dimensional displays, autonomous driving, dataset, dense depth, flash {LiDAR}, ground truth depth, high resolution {LiDAR}, monocular depth estimation, stereo vision, three dimensional},
}

@misc{caesar_nuscenes_2020,
	title = {{nuScenes}: A multimodal dataset for autonomous driving},
	url = {http://arxiv.org/abs/1903.11027},
	doi = {10.48550/arXiv.1903.11027},
	shorttitle = {{nuScenes}},
	abstract = {Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present {nuTonomy} scenes ({nuScenes}), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. {nuScenes} comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering {KITTI} dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.},
	number = {{arXiv}:1903.11027},
	publisher = {{arXiv}},
	author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
	urldate = {2024-02-28},
	date = {2020-05-05},
	eprinttype = {arxiv},
	eprint = {1903.11027 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	number = {{arXiv}:1312.6114},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2024-02-28},
	date = {2022-12-10},
	eprinttype = {arxiv},
	eprint = {1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{navab_u-net_2015,
	location = {Cham},
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	volume = {9351},
	isbn = {978-3-319-24573-7 978-3-319-24574-4},
	url = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
	shorttitle = {U-Net},
	pages = {234--241},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	urldate = {2024-02-28},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-24574-4_28},
	note = {Series Title: Lecture Notes in Computer Science},
}

@misc{wang_pretraining_2022,
	title = {Pretraining is All You Need for Image-to-Image Translation},
	url = {http://arxiv.org/abs/2205.12952},
	doi = {10.48550/arXiv.2205.12952},
	abstract = {We propose to use pretraining to boost general image-to-image translation. Prior image-to-image translation methods usually need dedicated architectural design and train individual translation models from scratch, struggling for high-quality generation of complex scenes, especially when paired training data are not abundant. In this paper, we regard each image-to-image translation problem as a downstream task and introduce a simple and generic framework that adapts a pretrained diffusion model to accommodate various kinds of image-to-image translation. We also propose adversarial training to enhance the texture synthesis in the diffusion model training, in conjunction with normalized guidance sampling to improve the generation quality. We present extensive empirical comparison across various tasks on challenging benchmarks such as {ADE}20K, {COCO}-Stuff, and {DIODE}, showing the proposed pretraining-based image-to-image translation ({PITI}) is capable of synthesizing images of unprecedented realism and faithfulness.},
	number = {{arXiv}:2205.12952},
	publisher = {{arXiv}},
	author = {Wang, Tengfei and Zhang, Ting and Zhang, Bo and Ouyang, Hao and Chen, Dong and Chen, Qifeng and Wen, Fang},
	urldate = {2024-02-28},
	date = {2022-05-25},
	eprinttype = {arxiv},
	eprint = {2205.12952 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shaham_spatially-adaptive_2020,
	title = {Spatially-Adaptive Pixelwise Networks for Fast Image Translation},
	url = {http://arxiv.org/abs/2012.02992},
	abstract = {We introduce a new generator architecture, aimed at fast and efficient high-resolution image-to-image translation. We design the generator to be an extremely lightweight function of the full-resolution image. In fact, we use pixel-wise networks; that is, each pixel is processed independently of others, through a composition of simple affine transformations and nonlinearities. We take three important steps to equip such a seemingly simple function with adequate expressivity. First, the parameters of the pixel-wise networks are spatially varying so they can represent a broader function class than simple 1x1 convolutions. Second, these parameters are predicted by a fast convolutional network that processes an aggressively low-resolution representation of the input; Third, we augment the input image with a sinusoidal encoding of spatial coordinates, which provides an effective inductive bias for generating realistic novel high-frequency image content. As a result, our model is up to 18x faster than state-of-the-art baselines. We achieve this speedup while generating comparable visual quality across different image resolutions and translation domains.},
	number = {{arXiv}:2012.02992},
	publisher = {{arXiv}},
	author = {Shaham, Tamar Rott and Gharbi, Michael and Zhang, Richard and Shechtman, Eli and Michaeli, Tomer},
	urldate = {2024-02-28},
	date = {2020-12-05},
	eprinttype = {arxiv},
	eprint = {2012.02992 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bae_irondepth_2022,
	title = {{IronDepth}: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty},
	url = {http://arxiv.org/abs/2210.03676},
	doi = {10.48550/arXiv.2210.03676},
	shorttitle = {{IronDepth}},
	abstract = {Single image surface normal estimation and depth estimation are closely related problems as the former can be calculated from the latter. However, the surface normals computed from the output of depth estimation methods are significantly less accurate than the surface normals directly estimated by networks. To reduce such discrepancy, we introduce a novel framework that uses surface normal and its uncertainty to recurrently refine the predicted depth-map. The depth of each pixel can be propagated to a query pixel, using the predicted surface normal as guidance. We thus formulate depth refinement as a classification of choosing the neighboring pixel to propagate from. Then, by propagating to sub-pixel points, we upsample the refined, low-resolution output. The proposed method shows state-of-the-art performance on {NYUv}2 and {iBims}-1 - both in terms of depth and normal. Our refinement module can also be attached to the existing depth estimation methods to improve their accuracy. We also show that our framework, only trained for depth estimation, can also be used for depth completion. The code is available at https://github.com/baegwangbin/{IronDepth}.},
	number = {{arXiv}:2210.03676},
	publisher = {{arXiv}},
	author = {Bae, Gwangbin and Budvytis, Ignas and Cipolla, Roberto},
	urldate = {2024-02-28},
	date = {2022-10-07},
	eprinttype = {arxiv},
	eprint = {2210.03676 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bhat_zoedepth_2023,
	title = {{ZoeDepth}: Zero-shot Transfer by Combining Relative and Metric Depth},
	url = {http://arxiv.org/abs/2302.12288},
	doi = {10.48550/arXiv.2302.12288},
	shorttitle = {{ZoeDepth}},
	abstract = {This paper tackles the problem of depth estimation from a single image. Existing work either focuses on generalization performance disregarding metric scale, i.e. relative depth estimation, or state-of-the-art results on specific datasets, i.e. metric depth estimation. We propose the first approach that combines both worlds, leading to a model with excellent generalization performance while maintaining metric scale. Our flagship model, {ZoeD}-M12-{NK}, is pre-trained on 12 datasets using relative depth and fine-tuned on two datasets using metric depth. We use a lightweight head with a novel bin adjustment design called metric bins module for each domain. During inference, each input image is automatically routed to the appropriate head using a latent classifier. Our framework admits multiple configurations depending on the datasets used for relative depth pre-training and metric fine-tuning. Without pre-training, we can already significantly improve the state of the art ({SOTA}) on the {NYU} Depth v2 indoor dataset. Pre-training on twelve datasets and fine-tuning on the {NYU} Depth v2 indoor dataset, we can further improve {SOTA} for a total of 21\% in terms of relative absolute error ({REL}). Finally, {ZoeD}-M12-{NK} is the first model that can jointly train on multiple datasets ({NYU} Depth v2 and {KITTI}) without a significant drop in performance and achieve unprecedented zero-shot generalization performance to eight unseen datasets from both indoor and outdoor domains. The code and pre-trained models are publicly available at https://github.com/isl-org/{ZoeDepth} .},
	number = {{arXiv}:2302.12288},
	publisher = {{arXiv}},
	author = {Bhat, Shariq Farooq and Birkl, Reiner and Wofk, Diana and Wonka, Peter and Müller, Matthias},
	urldate = {2024-02-28},
	date = {2023-02-23},
	eprinttype = {arxiv},
	eprint = {2302.12288 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{li_bbdm_2023,
	location = {Vancouver, {BC}, Canada},
	title = {{BBDM}: Image-to-Image Translation with Brownian Bridge Diffusion Models},
	isbn = {9798350301298},
	url = {https://ieeexplore.ieee.org/document/10203692/},
	doi = {10.1109/CVPR52729.2023.00194},
	shorttitle = {{BBDM}},
	abstract = {Image-to-image translation is an important and challenging problem in computer vision and image processing. Diffusion models ({DM}) have shown great potentials for high-quality image synthesis, and have gained competitive performance on the task of image-to-image translation. However, most of the existing diffusion models treat imageto-image translation as conditional generation processes, and suffer heavily from the gap between distinct domains. In this paper, a novel image-to-image translation method based on the Brownian Bridge Diffusion Model ({BBDM}) is proposed, which models image-to-image translation as a stochastic Brownian Bridge process, and learns the translation between two domains directly through the bidirectional diffusion process rather than a conditional generation process. To the best of our knowledge, it is the first work that proposes Brownian Bridge diffusion process for image-to-image translation. Experimental results on various benchmarks demonstrate that the proposed {BBDM} model achieves competitive performance through both visual inspection and measurable metrics.},
	eventtitle = {2023 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {1952--1961},
	booktitle = {2023 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Li, Bo and Xue, Kaitao and Liu, Bin and Lai, Yu-Kun},
	urldate = {2024-02-28},
	date = {2023-06},
	langid = {english},
}

@misc{wang_high-resolution_2018,
	title = {High-Resolution Image Synthesis and Semantic Manipulation with Conditional {GANs}},
	url = {http://arxiv.org/abs/1711.11585},
	abstract = {We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional {GANs}). Conditional {GANs} have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.},
	number = {{arXiv}:1711.11585},
	publisher = {{arXiv}},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	urldate = {2024-02-28},
	date = {2018-08-20},
	eprinttype = {arxiv},
	eprint = {1711.11585 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@inproceedings{wang_lrru_2023,
	location = {Paris, France},
	title = {{LRRU}: Long-short Range Recurrent Updating Networks for Depth Completion},
	isbn = {9798350307184},
	url = {https://ieeexplore.ieee.org/document/10377027/},
	doi = {10.1109/ICCV51070.2023.00864},
	shorttitle = {{LRRU}},
	abstract = {Existing deep learning-based depth completion methods generally employ massive stacked layers to predict the dense depth map from sparse input data. Although such approaches greatly advance this task, their accompanied huge computational complexity hinders their practical applications. To accomplish depth completion more efficiently, we propose a novel lightweight deep network framework, the Long-short Range Recurrent Updating ({LRRU}) network. Without learning complex feature representations, {LRRU} first roughly fills the sparse input to obtain an initial dense depth map, and then iteratively updates it through learned spatially-variant kernels. Our iterative update process is content-adaptive and highly flexible, where the kernel weights are learned by jointly considering the guidance {RGB} images and the depth map to be updated, and large-to-small kernel scopes are dynamically adjusted to capture long-to-short range dependencies. Our initial depth map has coarse but complete scene depth information, which helps relieve the burden of directly regressing the dense depth from sparse ones, while our proposed method can effectively refine it to an accurate depth map with less learnable parameters and inference time. Experimental results demonstrate that our proposed {LRRU} variants achieve state-of-the-art performance across different parameter regimes. In particular, the {LRRU}-Base model outperforms competing approaches on the {NYUv}2 dataset, and ranks 1st on the {KITTI} depth completion benchmark at the time of submission. Project page: https://npucvr.github.io/{LRRU}/.},
	eventtitle = {2023 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {9388--9398},
	booktitle = {2023 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Wang, Yufei and Li, Bo and Zhang, Ge and Liu, Qi and Gao, Tao and Dai, Yuchao},
	urldate = {2024-02-28},
	date = {2023-10-01},
	langid = {english},
}

@misc{shao_nddepth_2023,
	title = {{NDDepth}: Normal-Distance Assisted Monocular Depth Estimation and Completion},
	url = {http://arxiv.org/abs/2311.07166},
	doi = {10.48550/arXiv.2311.07166},
	shorttitle = {{NDDepth}},
	abstract = {Over the past few years, monocular depth estimation and completion have been paid more and more attention from the computer vision community because of their widespread applications. In this paper, we introduce novel physics (geometry)-driven deep learning frameworks for these two tasks by assuming that 3D scenes are constituted with piece-wise planes. Instead of directly estimating the depth map or completing the sparse depth map, we propose to estimate the surface normal and plane-to-origin distance maps or complete the sparse surface normal and distance maps as intermediate outputs. To this end, we develop a normal-distance head that outputs pixel-level surface normal and distance. Meanwhile, the surface normal and distance maps are regularized by a developed plane-aware consistency constraint, which are then transformed into depth maps. Furthermore, we integrate an additional depth head to strengthen the robustness of the proposed frameworks. Extensive experiments on the {NYU}-Depth-v2, {KITTI} and {SUN} {RGB}-D datasets demonstrate that our method exceeds in performance prior state-of-the-art monocular depth estimation and completion competitors. The source code will be available at https://github.com/{ShuweiShao}/{NDDepth}.},
	number = {{arXiv}:2311.07166},
	publisher = {{arXiv}},
	author = {Shao, Shuwei and Pei, Zhongcai and Chen, Weihai and Chen, Peter C. Y. and Li, Zhengguo},
	urldate = {2024-02-28},
	date = {2023-11-13},
	eprinttype = {arxiv},
	eprint = {2311.07166 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_depth_2024,
	title = {Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data},
	url = {http://arxiv.org/abs/2401.10891},
	doi = {10.48550/arXiv.2401.10891},
	shorttitle = {Depth Anything},
	abstract = {This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data ({\textasciitilde}62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from {NYUv}2 and {KITTI}, new {SOTAs} are set. Our better depth model also results in a better depth-conditioned {ControlNet}. Our models are released at https://github.com/{LiheYoung}/Depth-Anything.},
	number = {{arXiv}:2401.10891},
	publisher = {{arXiv}},
	author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
	urldate = {2024-02-28},
	date = {2024-01-19},
	eprinttype = {arxiv},
	eprint = {2401.10891 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kirillov_segment_2023,
	title = {Segment Anything},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything ({SA}) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model ({SAM}) and corresponding dataset ({SA}-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	number = {{arXiv}:2304.02643},
	publisher = {{arXiv}},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	urldate = {2024-02-28},
	date = {2023-04-05},
	eprinttype = {arxiv},
	eprint = {2304.02643 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@incollection{marcus_gan-based_2023,
	title = {{GAN}-Based {LiDAR} Intensity Simulation},
	volume = {1875},
	url = {http://arxiv.org/abs/2311.15415},
	abstract = {Realistic vehicle sensor simulation is an important element in developing autonomous driving. As physics-based implementations of visual sensors like {LiDAR} are complex in practice, data-based approaches promise solutions. Using pairs of camera images and {LiDAR} scans from real test drives, {GANs} can be trained to translate between them. For this process, we contribute two additions. First, we exploit the camera images, acquiring segmentation data and dense depth maps as additional input for training. Second, we test the performance of the {LiDAR} simulation by testing how well an object detection network generalizes between real and synthetic point clouds to enable evaluation without ground truth point clouds. Combining both, we simulate {LiDAR} point clouds and demonstrate their realism.},
	pages = {419--433},
	author = {Marcus, Richard and Gabel, Felix and Knoop, Niklas and Stamminger, Marc},
	urldate = {2024-02-28},
	date = {2023},
	doi = {10.1007/978-3-031-39059-3_28},
	eprinttype = {arxiv},
	eprint = {2311.15415 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{marcus_lightweight_2022,
	title = {A Lightweight Machine Learning Pipeline for {LiDAR}-simulation},
	url = {http://arxiv.org/abs/2208.03130},
	doi = {10.5220/0011309100003277},
	abstract = {Virtual testing is a crucial task to ensure safety in autonomous driving, and sensor simulation is an important task in this domain. Most current {LiDAR} simulations are very simplistic and are mainly used to perform initial tests, while the majority of insights are gathered on the road. In this paper, we propose a lightweight approach for more realistic {LiDAR} simulation that learns a real sensor's behavior from test drive data and transforms this to the virtual domain. The central idea is to cast the simulation into an image-to-image translation problem. We train our pix2pix based architecture on two real world data sets, namely the popular {KITTI} data set and the Audi Autonomous Driving Dataset which provide both, {RGB} and {LiDAR} images. We apply this network on synthetic renderings and show that it generalizes sufficiently from real images to simulated images. This strategy enables to skip the sensor-specific, expensive and complex {LiDAR} physics simulation in our synthetic world and avoids oversimplification and a large domain-gap through the clean synthetic environment.},
	pages = {176--183},
	booktitle = {Proceedings of the 3rd International Conference on Deep Learning Theory and Applications},
	author = {Marcus, Richard and Knoop, Niklas and Egger, Bernhard and Stamminger, Marc},
	urldate = {2024-02-28},
	date = {2022},
	eprinttype = {arxiv},
	eprint = {2208.03130 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{sun_scalability_2020-1,
	title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset},
	url = {http://arxiv.org/abs/1912.04838},
	doi = {10.48550/arXiv.1912.04838},
	shorttitle = {Scalability in Perception for Autonomous Driving},
	abstract = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality {LiDAR} and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+{LiDAR} dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D ({LiDAR}) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.},
	number = {{arXiv}:1912.04838},
	publisher = {{arXiv}},
	author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhao, Sheng and Cheng, Shuyang and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
	urldate = {2024-02-28},
	date = {2020-05-12},
	eprinttype = {arxiv},
	eprint = {1912.04838 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{guizilini_3d_2020,
	title = {3D Packing for Self-Supervised Monocular Depth Estimation},
	url = {http://arxiv.org/abs/1905.02693},
	doi = {10.48550/arXiv.1905.02693},
	abstract = {Although cameras are ubiquitous, robotic platforms typically rely on active sensors like {LiDAR} for direct 3D perception. In this work, we propose a novel self-supervised monocular depth estimation method combining geometry with a new deep network, {PackNet}, learned only from unlabeled monocular videos. Our architecture leverages novel symmetrical packing and unpacking blocks to jointly learn to compress and decompress detail-preserving representations using 3D convolutions. Although self-supervised, our method outperforms other self, semi, and fully supervised methods on the {KITTI} benchmark. The 3D inductive bias in {PackNet} enables it to scale with input resolution and number of parameters without overfitting, generalizing better on out-of-domain data such as the {NuScenes} dataset. Furthermore, it does not require large-scale supervised pretraining on {ImageNet} and can run in real-time. Finally, we release {DDAD} (Dense Depth for Automated Driving), a new urban driving dataset with more challenging and accurate depth evaluation, thanks to longer-range and denser ground-truth depth generated from high-density {LiDARs} mounted on a fleet of self-driving cars operating world-wide.},
	number = {{arXiv}:1905.02693},
	publisher = {{arXiv}},
	author = {Guizilini, Vitor and Ambrus, Rares and Pillai, Sudeep and Raventos, Allan and Gaidon, Adrien},
	urldate = {2024-02-28},
	date = {2020-03-28},
	eprinttype = {arxiv},
	eprint = {1905.02693 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{deschaud_paris-carla-3d_2021,
	title = {Paris-{CARLA}-3D: A Real and Synthetic Outdoor Point Cloud Dataset for Challenging Tasks in 3D Mapping},
	volume = {13},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/22/4713},
	doi = {10.3390/rs13224713},
	shorttitle = {Paris-{CARLA}-3D},
	abstract = {Paris-{CARLA}-3D is a dataset of several dense colored point clouds of outdoor environments built by a mobile {LiDAR} and camera system. The data are composed of two sets with synthetic data from the open source {CARLA} simulator (700 million points) and real data acquired in the city of Paris (60 million points), hence the name Paris-{CARLA}-3D. One of the advantages of this dataset is to have simulated the same {LiDAR} and camera platform in the open source {CARLA} simulator as the one used to produce the real data. In addition, manual annotation of the classes using the semantic tags of {CARLA} was performed on the real data, allowing the testing of transfer methods from the synthetic to the real data. The objective of this dataset is to provide a challenging dataset to evaluate and improve methods on difficult vision tasks for the 3D mapping of outdoor environments: semantic segmentation, instance segmentation, and scene completion. For each task, we describe the evaluation protocol as well as the experiments carried out to establish a baseline.},
	pages = {4713},
	number = {22},
	journaltitle = {Remote Sensing},
	author = {Deschaud, Jean-Emmanuel and Duque, David and Richa, Jean Pierre and Velasco-Forero, Santiago and Marcotegui, Beatriz and Goulette, François},
	urldate = {2024-02-28},
	date = {2021-01},
	langid = {english},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D mapping, {LiDAR}, dataset, laser scanning, mobile mapping, outdoor, point cloud, scene completion, semantic, synthetic},
}

@misc{xiao_transfer_2021,
	title = {Transfer Learning from Synthetic to Real {LiDAR} Point Cloud for Semantic Segmentation},
	url = {http://arxiv.org/abs/2107.05399},
	doi = {10.48550/arXiv.2107.05399},
	abstract = {Knowledge transfer from synthetic to real data has been widely studied to mitigate data annotation constraints in various computer vision tasks such as semantic segmentation. However, the study focused on 2D images and its counterpart in 3D point clouds segmentation lags far behind due to the lack of large-scale synthetic datasets and effective transfer methods. We address this issue by collecting {SynLiDAR}, a large-scale synthetic {LiDAR} dataset that contains point-wise annotated point clouds with accurate geometric shapes and comprehensive semantic classes. {SynLiDAR} was collected from multiple virtual environments with rich scenes and layouts which consists of over 19 billion points of 32 semantic classes. In addition, we design {PCT}, a novel point cloud translator that effectively mitigates the gap between synthetic and real point clouds. Specifically, we decompose the synthetic-to-real gap into an appearance component and a sparsity component and handle them separately which improves the point cloud translation greatly. We conducted extensive experiments over three transfer learning setups including data augmentation, semi-supervised domain adaptation and unsupervised domain adaptation. Extensive experiments show that {SynLiDAR} provides a high-quality data source for studying 3D transfer and the proposed {PCT} achieves superior point cloud translation consistently across the three setups. {SynLiDAR} project page: {\textbackslash}url\{https://github.com/xiaoaoran/{SynLiDAR}\}},
	number = {{arXiv}:2107.05399},
	publisher = {{arXiv}},
	author = {Xiao, Aoran and Huang, Jiaxing and Guan, Dayan and Zhan, Fangneng and Lu, Shijian},
	urldate = {2024-02-28},
	date = {2021-12-01},
	eprinttype = {arxiv},
	eprint = {2107.05399 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{fang_lidar-cs_2023,
	title = {{LiDAR}-{CS} Dataset: {LiDAR} Point Cloud Dataset with Cross-Sensors for 3D Object Detection},
	url = {http://arxiv.org/abs/2301.12515},
	shorttitle = {{LiDAR}-{CS} Dataset},
	abstract = {{LiDAR} devices are widely used in autonomous driving scenarios and researches on 3D point cloud achieve remarkable progress over the past years. However, deep learning-based methods heavily rely on the annotation data and often face the domain generalization problem. Unlike 2D images whose domains are usually related to the texture information, the feature extracted from the 3D point cloud is affected by the distribution of the points. Due to the lack of a 3D domain adaptation benchmark, the common practice is to train the model on one benchmark (e.g, Waymo) and evaluate it on another dataset (e.g. {KITTI}). However, in this setting, there are two types of domain gaps, the scenarios domain, and sensors domain, making the evaluation and analysis complicated and difficult. To handle this situation, we propose {LiDAR} Dataset with Cross-Sensors ({LiDAR}-{CS} Dataset), which contains large-scale annotated {LiDAR} point cloud under 6 groups of different sensors but with same corresponding scenarios, captured from hybrid realistic {LiDAR} simulator. As far as we know, {LiDAR}-{CS} Dataset is the first dataset focused on the sensor (e.g., the points distribution) domain gaps for 3D object detection in real traffic. Furthermore, we evaluate and analyze the performance with several baseline detectors on the {LiDAR}-{CS} benchmark and show its applications.},
	number = {{arXiv}:2301.12515},
	publisher = {{arXiv}},
	author = {Fang, Jin and Zhou, Dingfu and Zhao, Jingjing and Tang, Chulin and Xu, Cheng-Zhong and Zhang, Liangjun},
	urldate = {2024-02-28},
	date = {2023-01-29},
	eprinttype = {arxiv},
	eprint = {2301.12515 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zimmer_real-time_nodate,
	title = {Real-Time and Robust 3D Object Detection Within Roadside {LiDARs} Using Domain Adaptation},
	abstract = {This work aims to address the challenges in domain adaptation of 3D object detection using roadside {LiDARs}. We design {DASE}-{ProPillars}, a model that can detect objects in roadside {LiDARs} in real-time. Our model uses {PointPillars} as the baseline model with additional modules to improve the 3D detection performance. To prove the effectiveness of our proposed modules in {DASE}-{ProPillars}, we train and evaluate the model on two datasets, the open source A9 dataset and a semisynthetic roadside A11 dataset created within the Regensburg Next project. We do several sets of experiments for each module in the {DASE}-{ProPillars} detector that show that our model outperforms the {SE}-{ProPillars} baseline on the real A9 test set and a semi-synthetic A9 test set, while maintaining an inference speed of 45 Hz (22 ms) that allows to detect objects in real-time. We apply domain adaptation from the semi-synthetic A9 dataset to the semi-synthetic A11 dataset from the Regensburg Next project by applying transfer learning and achieve a 3D {mAP} @0.25 of 93.49\% on the Car class of the target test set using 40 recall positions.},
	author = {Zimmer, Walter and Grabler, Marcus and Knoll, Alois},
	langid = {english},
}

@misc{park_semantic_2019,
	title = {Semantic Image Synthesis with Spatially-Adaptive Normalization},
	url = {http://arxiv.org/abs/1903.07291},
	doi = {10.48550/arXiv.1903.07291},
	abstract = {We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at https://github.com/{NVlabs}/{SPADE} .},
	number = {{arXiv}:1903.07291},
	publisher = {{arXiv}},
	author = {Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
	urldate = {2024-02-28},
	date = {2019-11-05},
	eprinttype = {arxiv},
	eprint = {1903.07291 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, I.3.3, I.5, I.5.4},
}

@inproceedings{geiger_are_2012,
	title = {Are we ready for autonomous driving? The {KITTI} vision benchmark suite},
	doi = {10.1109/CVPR.2012.6248074},
	shorttitle = {Are we ready for autonomous driving?},
	abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/{SLAM} and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
	eventtitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {3354--3361},
	booktitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Geiger, A. and Lenz, P. and Urtasun, R.},
	date = {2012-06},
	note = {{ISSN}: 1063-6919},
	keywords = {3D object detection, Benchmark testing, Cameras, {KITTI} vision benchmark suite, Measurement, Middlebury perform, Optical imaging, Optical sensors, {SLAM}, {SLAM} (robots), Velodyne laser scanner, Visualization, autonomous driving, high resolution video cameras, image sequences, object detection, optical flow image pairs, robot vision, stereo image processing, stereo visual odometry sequences, video signal processing, visual recognition systems},
}

@article{yue_lidar_2018,
	title = {A {LiDAR} Point Cloud Generator: from a Virtual World to Autonomous Driving},
	url = {http://arxiv.org/abs/1804.00103},
	shorttitle = {A {LiDAR} Point Cloud Generator},
	abstract = {3D {LiDAR} scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D {LiDAR} point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9\%) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.},
	journaltitle = {{arXiv}:1804.00103 [cs]},
	author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A. and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L.},
	urldate = {2021-02-15},
	date = {2018-03-30},
	eprinttype = {arxiv},
	eprint = {1804.00103},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{isola_image--image_2018,
	title = {Image-to-Image Translation with Conditional Adversarial Networks},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	journaltitle = {{arXiv}:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	urldate = {2021-02-23},
	date = {2018-11-26},
	eprinttype = {arxiv},
	eprint = {1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_video--video_2018,
	title = {Video-to-Video Synthesis},
	url = {http://arxiv.org/abs/1808.06601},
	doi = {10.48550/arXiv.1808.06601},
	abstract = {We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image synthesis problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without understanding temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a novel video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generator and discriminator architectures, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems.},
	number = {{arXiv}:1808.06601},
	publisher = {{arXiv}},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Liu, Guilin and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	urldate = {2023-10-30},
	date = {2018-12-03},
	eprinttype = {arxiv},
	eprint = {1808.06601 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@inproceedings{caesar_nuscenes_2020-1,
	title = {{nuScenes}: A Multimodal Dataset for Autonomous Driving},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.html},
	shorttitle = {{nuScenes}},
	eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {11621--11631},
	author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
	urldate = {2021-02-22},
	date = {2020},
}

@article{wang_pseudo-lidar_2020,
	title = {Pseudo-{LiDAR} from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving},
	url = {http://arxiv.org/abs/1812.07179},
	shorttitle = {Pseudo-{LiDAR} from Visual Depth Estimation},
	abstract = {3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive {LiDAR} technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-{LiDAR} representations --- essentially mimicking the {LiDAR} signal. With this representation we can apply different existing {LiDAR}-based detection algorithms. On the popular {KITTI} benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22\% to an unprecedented 74\%. At the time of submission our algorithm holds the highest entry on the {KITTI} 3D object detection leaderboard for stereo-image-based approaches. Our code is publicly available at https://github.com/mileyan/pseudo\_lidar.},
	journaltitle = {{arXiv}:1812.07179 [cs]},
	author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
	urldate = {2021-02-15},
	date = {2020-02-22},
	eprinttype = {arxiv},
	eprint = {1812.07179},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{manivasagam_lidarsim_2020,
	title = {{LiDARsim}: Realistic {LiDAR} Simulation by Leveraging the Real World},
	url = {http://arxiv.org/abs/2006.09348},
	shorttitle = {{LiDARsim}},
	abstract = {We tackle the problem of producing realistic simulations of {LiDAR} point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from {CAD}/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and "virtually" placing the self-driving vehicle ({SDV}) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic {LiDAR} point clouds. We showcase {LiDARsim}'s usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.},
	journaltitle = {{arXiv}:2006.09348 [cs]},
	author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
	urldate = {2021-02-22},
	date = {2020-06-16},
	eprinttype = {arxiv},
	eprint = {2006.09348},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{mildenhall_nerf_2020,
	title = {{NeRF}: Representing Scenes as Neural Radiance Fields for View Synthesis},
	url = {http://arxiv.org/abs/2003.08934},
	doi = {10.48550/arXiv.2003.08934},
	shorttitle = {{NeRF}},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	number = {{arXiv}:2003.08934},
	publisher = {{arXiv}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	urldate = {2023-10-30},
	date = {2020-08-03},
	eprinttype = {arxiv},
	eprint = {2003.08934 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{zhu_unpaired_2020,
	title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
	url = {http://arxiv.org/abs/1703.10593},
	doi = {10.48550/arXiv.1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	number = {{arXiv}:1703.10593},
	publisher = {{arXiv}},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	urldate = {2023-10-30},
	date = {2020-08-24},
	eprinttype = {arxiv},
	eprint = {1703.10593 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{deng_voxel_2021,
	title = {Voxel R-{CNN}: Towards High Performance Voxel-based 3D Object Detection},
	url = {http://arxiv.org/abs/2012.15712},
	doi = {10.48550/arXiv.2012.15712},
	shorttitle = {Voxel R-{CNN}},
	abstract = {Recent advances on 3D object detection heavily rely on how the 3D data are represented, {\textbackslash}emph\{i.e.\}, voxel-based or point-based representation. Many existing high performance 3D detectors are point-based because this structure can better retain precise point positions. Nevertheless, point-level features lead to high computation overheads due to unordered storage. In contrast, the voxel-based structure is better suited for feature extraction but often yields lower accuracy because the input data are divided into grids. In this paper, we take a slightly different viewpoint -- we find that precise positioning of raw points is not essential for high performance 3D object detection and that the coarse voxel granularity can also offer sufficient detection accuracy. Bearing this view in mind, we devise a simple but effective voxel-based framework, named Voxel R-{CNN}. By taking full advantage of voxel features in a two stage approach, our method achieves comparable detection accuracy with state-of-the-art point-based models, but at a fraction of the computation cost. Voxel R-{CNN} consists of a 3D backbone network, a 2D bird-eye-view ({BEV}) Region Proposal Network and a detect head. A voxel {RoI} pooling is devised to extract {RoI} features directly from voxel features for further refinement. Extensive experiments are conducted on the widely used {KITTI} Dataset and the more recent Waymo Open Dataset. Our results show that compared to existing voxel-based methods, Voxel R-{CNN} delivers a higher detection accuracy while maintaining a real-time frame processing rate, {\textbackslash}emph\{i.e\}., at a speed of 25 {FPS} on an {NVIDIA} {RTX} 2080 Ti {GPU}. The code is available at {\textbackslash}url\{https://github.com/djiajunustc/Voxel-R-{CNN}\}.},
	number = {{arXiv}:2012.15712},
	publisher = {{arXiv}},
	author = {Deng, Jiajun and Shi, Shaoshuai and Li, Peiwei and Zhou, Wengang and Zhang, Yanyong and Li, Houqiang},
	urldate = {2022-10-31},
	date = {2021-02-05},
	eprinttype = {arxiv},
	eprint = {2012.15712 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hu_penet_2021,
	title = {{PENet}: Towards Precise and Efficient Image Guided Depth Completion},
	url = {http://arxiv.org/abs/2103.00783},
	doi = {10.48550/arXiv.2103.00783},
	shorttitle = {{PENet}},
	abstract = {Image guided depth completion is the task of generating a dense depth map from a sparse depth map and a high quality image. In this task, how to fuse the color and depth modalities plays an important role in achieving good performance. This paper proposes a two-branch backbone that consists of a color-dominant branch and a depth-dominant branch to exploit and fuse two modalities thoroughly. More specifically, one branch inputs a color image and a sparse depth map to predict a dense depth map. The other branch takes as inputs the sparse depth map and the previously predicted depth map, and outputs a dense depth map as well. The depth maps predicted from two branches are complimentary to each other and therefore they are adaptively fused. In addition, we also propose a simple geometric convolutional layer to encode 3D geometric cues. The geometric encoded backbone conducts the fusion of different modalities at multiple stages, leading to good depth completion results. We further implement a dilated and accelerated {CSPN}++ to refine the fused depth map efficiently. The proposed full model ranks 1st in the {KITTI} depth completion online leaderboard at the time of submission. It also infers much faster than most of the top ranked methods. The code of this work is available at https://github.com/{JUGGHM}/{PENet}\_ICRA2021.},
	number = {{arXiv}:2103.00783},
	publisher = {{arXiv}},
	author = {Hu, Mu and Wang, Shuling and Li, Bin and Ning, Shiyu and Fan, Li and Gong, Xiaojin},
	urldate = {2023-10-30},
	date = {2021-03-18},
	eprinttype = {arxiv},
	eprint = {2103.00783 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wimbauer_monorec_2021,
	title = {{MonoRec}: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera},
	url = {http://arxiv.org/abs/2011.11814},
	doi = {10.48550/arXiv.2011.11814},
	shorttitle = {{MonoRec}},
	abstract = {In this paper, we propose {MonoRec}, a semi-supervised monocular dense reconstruction architecture that predicts depth maps from a single moving camera in dynamic environments. {MonoRec} is based on a multi-view stereo setting which encodes the information of multiple consecutive images in a cost volume. To deal with dynamic objects in the scene, we introduce a {MaskModule} that predicts moving object masks by leveraging the photometric inconsistencies encoded in the cost volumes. Unlike other multi-view stereo methods, {MonoRec} is able to reconstruct both static and moving objects by leveraging the predicted masks. Furthermore, we present a novel multi-stage training scheme with a semi-supervised loss formulation that does not require {LiDAR} depth values. We carefully evaluate {MonoRec} on the {KITTI} dataset and show that it achieves state-of-the-art performance compared to both multi-view and single-view methods. With the model trained on {KITTI}, we further demonstrate that {MonoRec} is able to generalize well to both the Oxford {RobotCar} dataset and the more challenging {TUM}-Mono dataset recorded by a handheld camera. Code and related materials will be available at https://vision.in.tum.de/research/monorec.},
	number = {{arXiv}:2011.11814},
	publisher = {{arXiv}},
	author = {Wimbauer, Felix and Yang, Nan and von Stumberg, Lukas and Zeller, Niclas and Cremers, Daniel},
	urldate = {2022-10-26},
	date = {2021-05-06},
	eprinttype = {arxiv},
	eprint = {2011.11814 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ost_neural_2021,
	title = {Neural Scene Graphs for Dynamic Scenes},
	url = {https://ieeexplore.ieee.org/document/9577399/},
	doi = {10.1109/CVPR46437.2021.00288},
	abstract = {Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of {RGB} images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and they lack the ability to represent dynamic scenes and decompose scenes into individual objects. In this work, we present the first neural rendering method that represents multi-object dynamic scenes as scene graphs. We propose a learned scene graph representation, which encodes object transformations and radiance, allowing us to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe similar objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes – only by observing a video of this scene – and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.},
	pages = {2855--2864},
	journaltitle = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Ost, Julian and Mannan, Fahim and Thuerey, Nils and Knodt, Julian and Heide, Felix},
	urldate = {2024-02-23},
	date = {2021-06},
	note = {Conference Name: 2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})
{ISBN}: 9781665445092
Place: Nashville, {TN}, {USA}
Publisher: {IEEE}},
}

@misc{dosovitskiy_image_2021,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	shorttitle = {An Image is Worth 16x16 Words},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	number = {{arXiv}:2010.11929},
	publisher = {{arXiv}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	urldate = {2023-10-30},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{marcus_lightweight_2022-1,
	title = {A Lightweight Machine Learning Pipeline for {LiDAR}-simulation},
	url = {http://arxiv.org/abs/2208.03130},
	doi = {10.5220/0011309100003277},
	abstract = {Virtual testing is a crucial task to ensure safety in autonomous driving, and sensor simulation is an important task in this domain. Most current {LiDAR} simulations are very simplistic and are mainly used to perform initial tests, while the majority of insights are gathered on the road. In this paper, we propose a lightweight approach for more realistic {LiDAR} simulation that learns a real sensor's behavior from test drive data and transforms this to the virtual domain. The central idea is to cast the simulation into an image-to-image translation problem. We train our pix2pix based architecture on two real world data sets, namely the popular {KITTI} data set and the Audi Autonomous Driving Dataset which provide both, {RGB} and {LiDAR} images. We apply this network on synthetic renderings and show that it generalizes sufficiently from real images to simulated images. This strategy enables to skip the sensor-specific, expensive and complex {LiDAR} physics simulation in our synthetic world and avoids oversimplification and a large domain-gap through the clean synthetic environment.},
	pages = {176--183},
	booktitle = {Proceedings of the 3rd International Conference on Deep Learning Theory and Applications},
	author = {Marcus, Richard and Knoop, Niklas and Egger, Bernhard and Stamminger, Marc},
	urldate = {2022-10-28},
	date = {2022},
	eprinttype = {arxiv},
	eprint = {2208.03130 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{tancik_block-nerf_2022,
	title = {Block-{NeRF}: Scalable Large Scene Neural View Synthesis},
	url = {http://arxiv.org/abs/2202.05263},
	doi = {10.48550/arXiv.2202.05263},
	shorttitle = {Block-{NeRF}},
	abstract = {We present Block-{NeRF}, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling {NeRF} to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained {NeRFs}. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make {NeRF} robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual {NeRF}, and introduce a procedure for aligning appearance between adjacent {NeRFs} so that they can be seamlessly combined. We build a grid of Block-{NeRFs} from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco.},
	number = {{arXiv}:2202.05263},
	publisher = {{arXiv}},
	author = {Tancik, Matthew and Casser, Vincent and Yan, Xinchen and Pradhan, Sabeek and Mildenhall, Ben and Srinivasan, Pratul P. and Barron, Jonathan T. and Kretzschmar, Henrik},
	urldate = {2022-10-26},
	date = {2022-02-10},
	eprinttype = {arxiv},
	eprint = {2202.05263 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{ruckert_adop_2022,
	title = {{ADOP}: Approximate Differentiable One-Pixel Point Rendering},
	url = {http://arxiv.org/abs/2110.06635},
	doi = {10.48550/arXiv.2110.06635},
	shorttitle = {{ADOP}},
	abstract = {In this paper we present {ADOP}, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g.{\textasciitilde}exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, {ADOP} achieves real-time rendering rates even for models with well over 100M points. https://github.com/darglein/{ADOP}},
	number = {{arXiv}:2110.06635},
	publisher = {{arXiv}},
	author = {Rückert, Darius and Franke, Linus and Stamminger, Marc},
	urldate = {2023-10-30},
	date = {2022-05-03},
	eprinttype = {arxiv},
	eprint = {2110.06635 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{saharia_palette_2022,
	title = {Palette: Image-to-Image Diffusion Models},
	url = {http://arxiv.org/abs/2111.05826},
	doi = {10.48550/arXiv.2111.05826},
	shorttitle = {Palette},
	abstract = {This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and {JPEG} restoration. Our simple implementation of image-to-image diffusion models outperforms strong {GAN} and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on {ImageNet}, with human evaluation and sample quality scores ({FID}, Inception Score, Classification Accuracy of a pre-trained {ResNet}-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.},
	number = {{arXiv}:2111.05826},
	publisher = {{arXiv}},
	author = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris A. and Ho, Jonathan and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	urldate = {2023-10-30},
	date = {2022-05-03},
	eprinttype = {arxiv},
	eprint = {2111.05826 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_pcgen_2022,
	title = {{PCGen}: Point Cloud Generator for {LiDAR} Simulation},
	url = {http://arxiv.org/abs/2210.08738},
	shorttitle = {{PCGen}},
	abstract = {Data is a fundamental building block for {LiDAR} perception systems. Unfortunately, real-world data collection and annotation is extremely costly \& laborious. Recently, real data based {LiDAR} simulators have shown tremendous potential to complement real data, due to their scalability and high-fidelity compared to graphics engine based methods. Before simulation can be deployed in the real-world, two shortcomings need to be addressed. First, existing methods usually generate data which are more noisy and complete than the real point clouds, due to 3D reconstruction error and pure geometry-based raycasting method. Second, prior works on simulation for object detection focus solely on rigid objects, like cars, but {VRUs}, like pedestrians, are important road participants. To tackle the first challenge, we propose {FPA} raycasting and surrogate model raydrop. {FPA} enables the simulation of both point cloud coordinates and sensor features, while taking into account reconstruction noise. The ray-wise surrogate raydrop model mimics the physical properties of {LiDAR}'s laser receiver to determine whether a simulated point would be recorded by a real {LiDAR}. With minimal training data, the surrogate model can generalize to different geographies and scenes, closing the domain gap between raycasted and real point clouds. To tackle the simulation of deformable {VRU} simulation, we employ {SMPL} dataset to provide a pedestrian simulation baseline and compare the domain gap between {CAD} and reconstructed objects. Applying our pipeline to perform novel sensor synthesis, results show that object detection models trained by simulation data can achieve similar result as the real data trained model.},
	number = {{arXiv}:2210.08738},
	publisher = {{arXiv}},
	author = {Li, Chenqi and Ren, Yuan and Liu, Bingbing},
	urldate = {2024-02-23},
	date = {2022-10-17},
	eprinttype = {arxiv},
	eprint = {2210.08738 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{schusler_achieving_2023,
	title = {Achieving Efficient and Realistic Full-Radar Simulations and Automatic Data Annotation by exploiting Ray Meta Data of a Radar Ray Tracing Simulator},
	url = {http://arxiv.org/abs/2305.14176},
	doi = {10.1109/RadarConf2351548.2023.10149641},
	abstract = {In this work a novel radar simulation concept is introduced that allows to simulate realistic radar data for Range, Doppler, and for arbitrary antenna positions in an efficient way. Further, it makes it possible to automatically annotate the simulated radar signal by allowing to decompose it into different parts. This approach allows not only almost perfect annotations possible, but also allows the annotation of exotic effects, such as multi-path effects or to label signal parts originating from different parts of an object. This is possible by adapting the computation process of a Monte Carlo shooting and bouncing rays ({SBR}) simulator. By considering the hits of each simulated ray, various meta data can be stored such as hit position, mesh pointer, object {IDs}, and many more. This collected meta data can then be utilized to predict the change of path lengths introduced by object motion to obtain Doppler information or to apply specific ray filter rules in order obtain radar signals that only fulfil specific conditions, such as multiple bounces or containing specific object {IDs}. Using this approach, perfect and otherwise almost impossible annotations schemes can be realized.},
	pages = {1--6},
	booktitle = {2023 {IEEE} Radar Conference ({RadarConf}23)},
	author = {Schüßler, Christian and Hoffmann, Marcel and Wirth, Vanessa and Eskofier, Björn and Weyrich, Tim and Stamminger, Marc and Vossiek, Martin},
	urldate = {2023-10-30},
	date = {2023-05-01},
	eprinttype = {arxiv},
	eprint = {2305.14176 [eess]},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
}

@misc{tao_lidar-nerf_2023,
	title = {{LiDAR}-{NeRF}: Novel {LiDAR} View Synthesis via Neural Radiance Fields},
	url = {http://arxiv.org/abs/2304.10406},
	shorttitle = {{LiDAR}-{NeRF}},
	abstract = {We introduce a new task, novel view synthesis for {LiDAR} sensors. While traditional model-based {LiDAR} simulators with style-transfer neural networks can be applied to render novel views, they fall short of producing accurate and realistic {LiDAR} patterns because the renderers rely on explicit 3D reconstruction and exploit game engines, that ignore important attributes of {LiDAR} points. We address this challenge by formulating, to the best of our knowledge, the first differentiable end-to-end {LiDAR} rendering framework, {LiDAR}-{NeRF}, leveraging a neural radiance field ({NeRF}) to facilitate the joint learning of geometry and the attributes of 3D points. However, simply employing {NeRF} cannot achieve satisfactory results, as it only focuses on learning individual pixels while ignoring local information, especially at low texture areas, resulting in poor geometry. To this end, we have taken steps to address this issue by introducing a structural regularization method to preserve local structural details. To evaluate the effectiveness of our approach, we establish an object-centric multi-view {LiDAR} dataset, dubbed {NeRF}-{MVL}. It contains observations of objects from 9 categories seen from 360-degree viewpoints captured with multiple {LiDAR} sensors. Our extensive experiments on the scene-level {KITTI}-360 dataset, and on our object-level {NeRF}-{MVL} show that our {LiDAR}-{NeRF} surpasses the model-based algorithms significantly.},
	number = {{arXiv}:2304.10406},
	publisher = {{arXiv}},
	author = {Tao, Tang and Gao, Longfei and Wang, Guangrun and Lao, Yixing and Chen, Peng and Zhao, Hengshuang and Hao, Dayang and Liang, Xiaodan and Salzmann, Mathieu and Yu, Kaicheng},
	urldate = {2024-02-23},
	date = {2023-07-14},
	eprinttype = {arxiv},
	eprint = {2304.10406 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huang_neural_2023,
	title = {Neural {LiDAR} Fields for Novel View Synthesis},
	url = {http://arxiv.org/abs/2305.01643},
	doi = {10.48550/arXiv.2305.01643},
	abstract = {We present Neural Fields for {LiDAR} ({NFL}), a method to optimise a neural field scene representation from {LiDAR} measurements, with the goal of synthesizing realistic {LiDAR} scans from novel viewpoints. {NFL} combines the rendering power of neural fields with a detailed, physically motivated model of the {LiDAR} sensing process, thus enabling it to accurately reproduce key sensor behaviors like beam divergence, secondary returns, and ray dropping. We evaluate {NFL} on synthetic and real {LiDAR} scans and show that it outperforms explicit reconstruct-then-simulate methods as well as other {NeRF}-style methods on {LiDAR} novel view synthesis task. Moreover, we show that the improved realism of the synthesized views narrows the domain gap to real scans and translates to better registration and semantic segmentation performance.},
	number = {{arXiv}:2305.01643},
	publisher = {{arXiv}},
	author = {Huang, Shengyu and Gojcic, Zan and Wang, Zian and Williams, Francis and Kasten, Yoni and Fidler, Sanja and Schindler, Konrad and Litany, Or},
	urldate = {2024-02-23},
	date = {2023-08-13},
	eprinttype = {arxiv},
	eprint = {2305.01643 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{noauthor_sync_nodate,
	title = {sync [Zotero Documentation]},
	url = {https://www.zotero.org/support/sync},
	urldate = {2024-02-23},
}

@inproceedings{li_durlar_2021,
	location = {Surrey / Online},
	title = {{DurLAR}: A High-Fidelity 128-Channel {LiDAR} Dataset with Panoramic Ambient and Reflectivity Imagery for Multi-Modal Autonomous Driving Applications},
	url = {https://3dv2021.surrey.ac.uk/},
	shorttitle = {{DurLAR}},
	abstract = {We present {DurLAR}, a high-fidelity 128-channel 3D {LiDAR} dataset with panoramic ambient (near infrared) and
reflectivity imagery, as well as a sample benchmark task using depth estimation for autonomous driving applications.
Our driving platform is equipped with a high resolution 128
channel {LiDAR}, a 2MPix stereo camera, a lux meter and
a {GNSS}/{INS} system. Ambient and reflectivity images are
made available along with the {LiDAR} point clouds to facilitate multi-modal use of concurrent ambient and reflectivity
scene information. Leveraging {DurLAR}, with a resolution
exceeding that of prior benchmarks, we consider the task of
monocular depth estimation and use this increased availability of higher resolution, yet sparse ground truth scene
depth information to propose a novel joint supervised/self-supervised loss formulation. We compare performance over
both our new {DurLAR} dataset, the established {KITTI} benchmark and the Cityscapes dataset. Our evaluation shows our
joint use supervised and self-supervised loss terms, enabled
via the superior ground truth resolution and availability
within {DurLAR} improves the quantitative and qualitative
performance of leading contemporary monocular depth estimation approaches ({RMSE} = 3.639, {SqRel} = 0.936).},
	eventtitle = {International Conference on 3D Vision},
	booktitle = {International Conference on 3D Vision, Surrey / Online, 1-3 Dec 2021 [Conference proceedings]},
	publisher = {{DU}},
	author = {Li, Li and Ismail, Khalid N. and Shum, Hubert P. H. and Breckon, Toby P.},
	urldate = {2022-10-26},
	date = {2021-12},
}

@misc{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2022-10-26},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@online{noauthor_envision_nodate,
	title = {Envision the Future},
	url = {https://velodynelidar.com/},
	abstract = {Welcome to Velodyne Lidar, provider of smart, powerful lidar technology solutions for autonomy, driver assistance, mapping, robotics and more.},
	titleaddon = {Velodyne Lidar},
	urldate = {2022-10-26},
	langid = {english},
}

@online{noauthor_alpha_nodate,
	title = {Alpha Prime},
	url = {https://velodynelidar.com/products/alpha-prime/},
	abstract = {Velodyne's Alpha Prime sensor delivers optimal long-range sensing for autonomous mobility, with a world-class combination of range, image clarity and {FOV}.},
	titleaddon = {Velodyne Lidar},
	urldate = {2022-10-26},
	langid = {english},
}

@online{noauthor_high-performance_nodate,
	title = {High-performance digital lidar: autonomous vehicles, robotics, industrial a...},
	url = {https://ouster.com/},
	shorttitle = {High-performance digital lidar},
	abstract = {Lidar sensors for low-cost use in vehicles, robots, and infrastructure. High performance \& reliable for any use case.},
	titleaddon = {Ouster},
	urldate = {2022-10-26},
	langid = {american},
}

@misc{agarwal_attention_2022,
	title = {Attention Attention Everywhere: Monocular Depth Prediction with Skip Attention},
	url = {http://arxiv.org/abs/2210.09071},
	doi = {10.48550/arXiv.2210.09071},
	shorttitle = {Attention Attention Everywhere},
	abstract = {Monocular Depth Estimation ({MDE}) aims to predict pixel-wise depth given a single {RGB} image. For both, the convolutional as well as the recent attention-based models, encoder-decoder-based architectures have been found to be useful due to the simultaneous requirement of global context and pixel-level resolution. Typically, a skip connection module is used to fuse the encoder and decoder features, which comprises of feature map concatenation followed by a convolution operation. Inspired by the demonstrated benefits of attention in a multitude of computer vision problems, we propose an attention-based fusion of encoder and decoder features. We pose {MDE} as a pixel query refinement problem, where coarsest-level encoder features are used to initialize pixel-level queries, which are then refined to higher resolutions by the proposed Skip Attention Module ({SAM}). We formulate the prediction problem as ordinal regression over the bin centers that discretize the continuous depth range and introduce a Bin Center Predictor ({BCP}) module that predicts bins at the coarsest level using pixel queries. Apart from the benefit of image adaptive depth binning, the proposed design helps learn improved depth embedding in initial pixel queries via direct supervision from the ground truth. Extensive experiments on the two canonical datasets, {NYUV}2 and {KITTI}, show that our architecture outperforms the state-of-the-art by 5.3\% and 3.9\%, respectively, along with an improved generalization performance by 9.4\% on the {SUNRGBD} dataset. Code is available at https://github.com/ashutosh1807/{PixelFormer}.git.},
	number = {{arXiv}:2210.09071},
	publisher = {{arXiv}},
	author = {Agarwal, Ashutosh and Arora, Chetan},
	urldate = {2022-10-26},
	date = {2022-10-17},
	eprinttype = {arxiv},
	eprint = {2210.09071 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_depthformer_2022,
	title = {{DepthFormer}: Exploiting Long-Range Correlation and Local Information for Accurate Monocular Depth Estimation},
	url = {http://arxiv.org/abs/2203.14211},
	doi = {10.48550/arXiv.2203.14211},
	shorttitle = {{DepthFormer}},
	abstract = {This paper aims to address the problem of supervised monocular depth estimation. We start with a meticulous pilot study to demonstrate that the long-range correlation is essential for accurate depth estimation. Therefore, we propose to leverage the Transformer to model this global context with an effective attention mechanism. We also adopt an additional convolution branch to preserve the local information as the Transformer lacks the spatial inductive bias in modeling such contents. However, independent branches lead to a shortage of connections between features. To bridge this gap, we design a hierarchical aggregation and heterogeneous interaction module to enhance the Transformer features via element-wise interaction and model the affinity between the Transformer and the {CNN} features in a set-to-set translation manner. Due to the unbearable memory cost caused by global attention on high-resolution feature maps, we introduce the deformable scheme to reduce the complexity. Extensive experiments on the {KITTI}, {NYU}, and {SUN} {RGB}-D datasets demonstrate that our proposed model, termed {DepthFormer}, surpasses state-of-the-art monocular depth estimation methods with prominent margins. Notably, it achieves the most competitive result on the highly competitive {KITTI} depth estimation benchmark. Our codes and models are available at https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox.},
	number = {{arXiv}:2203.14211},
	publisher = {{arXiv}},
	author = {Li, Zhenyu and Chen, Zehui and Liu, Xianming and Jiang, Junjun},
	urldate = {2022-10-26},
	date = {2022-03-27},
	eprinttype = {arxiv},
	eprint = {2203.14211 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_binsformer_2022,
	title = {{BinsFormer}: Revisiting Adaptive Bins for Monocular Depth Estimation},
	url = {http://arxiv.org/abs/2204.00987},
	doi = {10.48550/arXiv.2204.00987},
	shorttitle = {{BinsFormer}},
	abstract = {Monocular depth estimation is a fundamental task in computer vision and has drawn increasing attention. Recently, some methods reformulate it as a classification-regression task to boost the model performance, where continuous depth is estimated via a linear combination of predicted probability distributions and discrete bins. In this paper, we present a novel framework called {BinsFormer}, tailored for the classification-regression-based depth estimation. It mainly focuses on two crucial components in the specific task: 1) proper generation of adaptive bins and 2) sufficient interaction between probability distribution and bins predictions. To specify, we employ the Transformer decoder to generate bins, novelly viewing it as a direct set-to-set prediction problem. We further integrate a multi-scale decoder structure to achieve a comprehensive understanding of spatial geometry information and estimate depth maps in a coarse-to-fine manner. Moreover, an extra scene understanding query is proposed to improve the estimation accuracy, which turns out that models can implicitly learn useful information from an auxiliary environment classification task. Extensive experiments on the {KITTI}, {NYU}, and {SUN} {RGB}-D datasets demonstrate that {BinsFormer} surpasses state-of-the-art monocular depth estimation methods with prominent margins. Code and pretrained models will be made publicly available at {\textbackslash}url\{https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox\}.},
	number = {{arXiv}:2204.00987},
	publisher = {{arXiv}},
	author = {Li, Zhenyu and Wang, Xuyang and Liu, Xianming and Jiang, Junjun},
	urldate = {2022-10-26},
	date = {2022-04-03},
	eprinttype = {arxiv},
	eprint = {2204.00987 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
