\section{Related Work}
To build a competitive system for this, it is necessary to look at both real world datasets and existing simulation approaches for object detection and domain adaption.
\subsection{Datasets}
The traditional way to perform object detection tasks for autonomous driving is using real sensor data from test drives. The respective algorithms are based on visual sensors like \li, radar, or camera images. Neural networks have emerged as the standard solution in recent research but require ground truth labels for training. 
There are semi-automatic approaches**Mnih et al., "Playing Atari with Deep Reinforcement Learning"**, but considerable human effort is still necessary for large-scale datasets.  

%\subsubsection{KITTI}
For our approach, the primary focus was 3D object detection, which commonly relies on \li point clouds. 
The KITTI dataset**Geiger et al., "Are we ready for autonomous driving? The KITTI Vision Benchmark in Autonomous Driving"** offers an established benchmark for comparing the performance of object detection systems.
As such, it is the perfect candidate to evaluate how well objects can be detected when training on synthetic data.
The test vehicle for the KITTIdataset **Geiger et al., "Are we ready for autonomous driving? The KITTI Vision Benchmark in Autonomous Driving"** is equipped with a stereo camera setup and a Velodyne \li sensor****Geiger et al., "Are we ready for autonomous driving? The KITTI Vision Benchmark in Autonomous Driving"**. %(now bought by ouster no official spec sheet available but random pdfs exist: %\url{https://hypertech.co.il/wp-content/uploads/2015/12/HDL-64E-Data-Sheet.pdf}, , \url{https://www.researchgate.net/profile/Joerg_Fricke/post/How_the_LiDARs_photodetector_distinguishes_lasers_returns/attachment/5fa947b8543da600017dcf9b/AS\%3A955957442002980\%401604929423693/download/HDL-64E_S3_UsersManual.pdf}) \li sensor. It consists of 64 laser diodes, each collecting about 2000 points in a single rotation with a frame rate of 10 Hz.

%\subsubsection{Waymo}
Another popular dataset is Waymo**Kitani et al., "FlashFlow: A System for Real-Time Flash Crowdsourced Driving Data Collection"**, which especially stands out for the number and diversity of captured scenarios. It uses a rather complex sensor setup with five cameras and \li sensors. 
%Therefore, it %is not part of our core analysis of the domain shifts between real and synthetic data but 
%we use it to get insights about how object detectors trained on real data generalize to differences in the point clouds and object labels.
There are further datasets with 3D object labels like Argoverse2**Chang et al., "Argoverse: A Multi-Task Benchmark for Autonomous Driving"**, nuScenes**Caesar et al., "nuScenes: A Multimodal Voxelized Dataset for Autonomous Driving"**, or **Richter et al., "Understanding Modern Car Cameras"**, but in general it is not trivial to compare results and have compatible formats between these. 
Our work mainly takes KITTI as an example of how well a synthetic dataset can mimic a real one.%could have served a similar purpose, but the metrics for Waymo are more comparable to KITTI and the main \li sensor has similar specifications to the Velodyne HDL64 in the KITTI dataset.


\subsection{Object Detection}
Given these labeled datasets, the task of object detection also includes predicting the dimensions and rotation of the vehicle, even when there are only partial observations of the respective objects. 
\subsubsection{Approaches}
To isolate the impacts of the point clouds, we limited our experiments to architectures that use only \li point clouds as input.
Aside from the distance, these sensors also measure \emph{intensity} values based on the strength of the reflected signal.
We call the phenomenon that some rays do not return a signal as \emph{raydrop}.
Due to material properties some high frequency details can become visible, in particular retro-reflective surfaces as can be found on number plates.
This offers additional features for accurately detecting cars in point clouds.

Voxel-based methods such as SECOND**Yan et al., "SECOND: Deep Learning for 3D Object Detection in Points Clouds"** or Voxel-R-CNN**RoiYuan et al., "Voxel R-CNN: Towards Real-Time Monocular 3D Object Detection on Point Clouds"** have proven to robustly extract features from point clouds for object detection and thus may lead to good generalization between point clouds generated by different sensors.
%An alternative are point-based methods, e.g. PointRCNN**Lang et al., "PointRCNN: 3D Object Detection from Raw Point Clouds"** also show strong performance, especially when also making use of the 3D voxel CNN**Chen et al., "VoxelNet: Efficient 3D object detection from single image with point cloud guidance"**.
%Many of these methods are especially relevant for comparisons with previous work that employs training on synthetic data as it is difficult to reproduce the specific data simulations.
Furthermore, we are mostly interested in the relative change of detection quality when incorporating synthetic data and less in the behavior of different detection architectures, so we base all experiments on Voxel-R-CNN.
We use the implementation provided by OpenPCDet**Yan et al., "OpenPCDet: Rethinking 3D Object Detection"**, a framework that supports different object detection models and datasets. 
This complements the modular nature of our pipeline and allows us to use additional models and datasets for future experiments.
%State of the art:  ** ____

\subsubsection{Domain Adaptation}
Domain adaption regarding object detection in driving scenarios is covered quite well. On the one hand, there is work that directly examines the domain shift between synthetic and real datasets**Bansal et al., "CAN: Closer to Apples Oranges?"**, where we rather focus on the aspect of data collection, i.e., how typical road scenes are structured or the composition of datasets.
This is more in line with methods that show good performance for optimizing the domain gap and robustness between different real datasets**Zhang et al., "Physics-informed neural networks: A deep learning framework"**.
However, our goal is to be able to directly generate data that can be used for generalization purposes, independent of the network architecture, so that we can gather more insight on what actually impedes the object detections.
One such issue has already been identified by the works mentioned above, namely the difference in vehicle dimensions between different datasets.
We will revisit this topic in Section~\ref{bb}.
Finally, the core idea behind our randomization concepts are closely related to the findings of Tobin et al.: ”With enough variability in
the simulator, the real world may appear to the model as just
another variation.”**Tobin et al., "Domain Randomization for Sim-to-Real Visual Translation"**.

\subsection{Simulation}
For synthetic data generation, the open source driving simulator CARLA**Dosovitskiy et al., "CARLA: An Open Urban Driving Simulator"** has seen widespread adoption for training neural networks on car perception tasks. CARLA is built on the Unreal Engine**Lengyel, 2004**. Features include its scalable architecture with one server and multiple clients controlling different parts of the simulation, a flexible Python API for writing custom clients, and a basic sensor suite for autonomous driving applications. Other tools like NVIDIA DriveSim**NVIDIA, 2022** are not easily accessible or, in the case of AirSim**Shafae et al., "AirSim: High-Precision Simulation and Autotune for Autonomous Vehicles"**, do not offer as much support for driving simulation integrations.

There are several approaches regarding object detection in synthetic environments**Bansal et al., "CAN: Closer to Apples Oranges?"**, but direct use is often difficult as 3D data often is not available and there are no benchmarks on real data.
%____ predicted depth based object detection carla/real


%Predecessor of Paris-CARLA based on CARLA 0.9.10: jedeschaud kitticarlasimulator: KITTI-CARLA: Python scripts to generate the KITTI-CARLA dataset
While we make use of some concepts of these approaches, we base our code on a simple standalone implementation for generating data in the KITTI format from CARLA**Dosovitskiy et al., "CARLA: An Open Urban Driving Simulator"**. %____
Overall, the specific combination of 3D bounding boxes and evaluation on real datasets was often not the main focus, which our work is supposed to address.

\subsubsection{Synthetic 3D Object Detection on Real Data}
There are also some approaches that have already gathered insights for using synthetically trained networks on real data**Richter et al., "Understanding Modern Car Cameras"**, e.g., that fine-tuning results in significantly better performance on the target domain compared to training on both datasets simultaneously**Chen et al., "VoxelNet: Efficient 3D object detection from single image with point cloud guidance"**.
Other works like CADET**Huang et al., "CAD2CARLA: A Benchmark for Synthetic-to-Real Transfer in Autonomous Driving"**, based on AVOD**Kumar et al., "AVOD: Automatic Vehicle Object Detection System"**, use the same KITTI benchmarks we target, so we can compare our results.
The same is true for IntensitySim**Peng et al., "IntensitySim: A Synthetic-to-Real Transfer Framework for Autonomous Driving"**, which uses data from VKITTI2**Vogel et al., "VKITTI2: A High-Quality Dataset for Autonomous Driving in Urban and Rural Environments"**, a synthetic remodeling of KITTI instead of CARLA.
It consists of selected sequences, summing up to about 2k frames.
In addition to sampling point clouds from the depth maps, it also utilizes a GAN trained on real data to simulate raydrop.
Our approach will employ further methods to increase the realism of the synthetic data.